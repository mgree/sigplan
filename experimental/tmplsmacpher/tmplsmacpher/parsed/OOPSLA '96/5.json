{"article_publication_date": "10-01-1996", "fulltext": "\n Vortex: An Optimizing Compiler for Object-Oriented Languages Jeffrey Dean, Greg DeFouw, David Grove, \nVassily Litvinov, and Craig Chambers Department of Computer Science and Engineering University of Washington \nBox 352350, Seattle, Washington 981952350 USA {jdean, gdefouw, grove, vass, chambers}@cs.washington.edu \nAbstract Previously, techniques such as class hierarchy analysis and profile-guided receiver class prediction \nhave been demonstrated to greatly improve the performance of applications written in pure object-oriented \nlanguages, but the degree to which these results are transferable to applications written in hybrid languages \nhas been unclear. In part to answer this question, we have developed the Vortex compiler infrastructure, \na language-independent optimizing compiler for object-oriented languages, with front-ends for Cecil, \nC++, Java, and Modula-3. In this paper, we describe the Vortex compiler s intermediate language, internal \nstructure, and optimization suite, and then we report the results of experiments assessing the effectiveness \nof different combinations of optimizations on sizable applications across these four languages. We characterize \nthe benchmark programs in terms of a collection of static and dynamic metrics, intended to quantify aspects \nof the object-oriented- ness of a program. Introduction In recent years, it has been demonstrated that \nintra-and interprocedural static class analysis [Chambers &#38; Ungar 90, Plevyak &#38; Chien 94, Agesen \n&#38; Holzle 951, class hierarchy analysis [Dean et al. 95b], and profile-guided receiver class prediction \n[Hblzle &#38; Ungar 94, Grove et al. 951 can greatly improve the performance of dynamically-typed, purely \nobject-oriented languages such as Cecil [Chambers 92, Chambers 931, Self [Ungar &#38; Smith 871, and \nConcurrent Aggregates [Chien 931. These techniques have been highly effective in this context, since \nmessage sends are ubiquitous and expensive; even the most primitive operations in these languages are \nimplemented via user defined methods and dynamic dispatching. However, in statically-typed, hybrid object-oriented \nlanguages such as C++ [Stroustrup 911 and permission to make digital/hard copy of part or all of this \nwork for Personal or classroom use is granted without fee provided that QJPieS are not made or distributed \nfor profitor commercial advantage, the copyright noti@% the title of the publication and its date appear, \nand notice !S given that copying is by permissionof ACM, Inc. To copy otherwIse, to republish to post \non servers, or to redistribute to lists, requires prior Specific permission and/or a fee. Modula-3 [Nelson \n911, much of the normal execution of programs involves non-object-oriented constructs, and consequently \nthe incidence of dynamically-dispatched calls is much lower than in pure languages. Thus, is it unclear \nhow much overhead due to use of object-oriented features exists in programs written in these languages, \nand it is unclear how much benefit can be gained by applying advanced optimizations of the object-oriented \nfeatures to programs in these languages. Java [Gosling et al. 961, a language somewhere between C++ and \nCecil in terms of its purity and dependence on object-oriented features, offers another interesting point \nin the language design space, and therefore its need for advanced optimization techniques may be different \nthan both Cecil and C++. In this paper, we present a study of the effectiveness of advanced object-oriented-focused \noptimizations across a range of object-oriented languages (C++, Modula-3, Java, and Cecil). The study \nwas conducted using the Vortex compiler, a language-independent optimizing compiler for object-oriented \nlanguages that we have designed and implemented. By compiling all programs in these languages with a \ncommon optimizing back-end, we can ensure that each program receives comparable treatment and optimization \neffort. The next section of this paper reviews the optimization techniques included in Vortex. Section \n3 describes the structure of the Vortex compiler, focusing on the design of its intermediate language \nand reporting on our experience in building an optimizing compiler for a wide range of languages. In \nSection 4 we describe the benchmark programs used in this study. As part of this description, we define \na collection of static and dynamic metrics with which to characterize the programs, and we use these \nmetrics both to better convey the internal structure of the programs and also to attempt to predict when \ndifferent optimization techniques are likely to be most effective in improving an application s performance. \nSection 5 presents the performance results. Section 6 describes related work, and we offer our conclusions \nin Section 7. OOPSLA 96 CA, USA Q 1996 ACM 0-89791-788-x/98lOO10...$3.50 2 Background A compiler can \nreplace a dynamic dispatch with a static call whenever it can determine that a single method will be \ninvoked for all possible receiver classes of that call site. A message send that is replaced by a call \nin this fashion has been statically bound. Vortex uses five main techniques, described in the following \nsubsections, to statically bind message sends. Without additional programming environment support, interprocedural \noptimizations like class hierarchy analysis and cross-module inlining preclude rapid turnaround after \nincremental programming changes. Section 2.5 briefly describes the selective recompilation mechanism \nprovided by Vortex to allow whole-program optimizations and day-to-day application development to coexist. \n2.1 lntraprocedural Class Analysis Intraprocedural class analysis uses a standard iterative dataflow \napproach to compute for each program expression a set of classes such that any runtime value of the expression \nis guaranteed to be an instance of one of the classes in the computed set [Johnson 88, Chambers &#38; \nUngar 901. The analysis maintains a mapping from variables to sets of classes, and propagates this mapping \nthrough the procedure s control flow graph. By default, variables map to the set of all possible classes, \nbut literals and the results of object allocations (new) are mapped to singleton class sets. Class sets \nare combined with set-union at control flow merge points, and class sets are narrowed after a run-time \nclass test conditional branch. When a dynamically-dispatched message send is encountered, this mapping \nis consulted to determine if the receiver of the message is known to be a single class, or a bounded \nunion of classes. If only a single receiver class is possible, or if all classes in a union invoke the \nsame method, then the message send can be statically bound. Because intraprocedural class analysis has \nonly local knowledge, it cannot statically bind a message send if the receiver is known only to be an \ninstance of class C or some subclass of C, since an unknown subclass of C may provide an overriding definition \nof the target method. This limitation implies that knowing the static type of the receiver of a dynamically-dispatched \nmessage, as is the case in statically-typed languages like C++, Modula-3, and Java, is insufficient on \nits own to enable intraprocedural class analysis to statically bind the message send, since the possibility \nof an overriding method defined on a subclass cannot be ruled out. 2.2 Class Hierarchy Analysis Class \nhierarchy analysis [Dean et al. 95b] broadens the scope of the information available to the compiler \nby giving it access to all of the class and method declarations in the program. Given this global knowledge \nof the class hierarchy, the compiler can convert unbounded information of the form a variable holds an \ninstance of some (unknown) subclass of C into a bounded set of possible classes. Only bounded sets of \nclasses can provide useful information to the optimizer. Thus, class hierarchy analysis addresses one \nof the key weakness of intraprocedural class analysis by enabling the conversion of unbounded sets of \nclasses, derived from static type declarations or from the method s specialized formal parameters, into \nbounded sets of classes. 2.3 Receiver Class Prediction There are message sends that cannot be statically \nbound solely through static analysis; some message sends do invoke more than one method at run-time. \nHowever, it is still possible to transform message sends of this type into a form that allows inlining \nof at least a subset of the possible target methods. Receiver class prediction is a simple local code \ntransformation that converts a dynamic dispatch into a run-time type-case structure: one or more explicit \nin-line tests for particular expected classes, each of which branches when successful to a statically-bound \nor inlined version of the target method for that class, possibly followed by a final dynamic dispatch \nto handle any remaining unpredicted classes. Receiver class prediction can be driven either by information \nhard-wired into the compiler, as in early Smalltalk and Self implementations [Deutsch &#38; Schiffman \n84, Chambers &#38; Ungar 891, or by profile-derived class distributions [Hiilzle &#38; Ungar 94, Grove \net al. 951, or by static examination of the program s class hierarchy [Chambers et al. 961. We use the \nterm exhaustive class testing to refer to class-hierarchy-guided class testing, and projle-guided class \nprediction to refer to class testing based on dynamic profile information. If a large number of receiver \nclasses are possible at a call site, testing for individual classes can be very expensive. However, if \nthe number of target methods is low, then run-time subclass tests can be inserted instead of class identity \ntests, leading to a number of run-time tests on the order of the number of possible methods rather than \nthe number of possible classes. It is quite common for a method to contain multiple message sends to \na single receiver value; for example, several messages might be sent to self. If receiver class prediction \nis applied to each of these message sends, redundant class tests will be introduced. Splitting can eliminate \nthese redundant tests by duplicating paths through the control flow graph starting at merges after one \ntest and ending with the redundant test to be eliminated [Chambers &#38; Ungar 901. 2.4 Customization \nand Specialization F Customization and method specialization [Chambers &#38; Ungar 89, Lea 90, Dean et \nal. 95a] are techniques that also can be used to statically bind messages sent to self, by creating multiple \ncompiled copies of a single source method, each specialized to particular receiver classes. However, \nsince Vortex does not support customization or specialization for languages that use dispatch tables \nto implement message sends (e.g., C++ and Modula-3), we do not consider this technique in our study. \n 2.5 Selective Recompilation Because of their global scope, interprocedural optimizations like class \nhierarchy analysis and cross-module inlining can introduce non-local dependencies in the compiled code, \nthus preventing strict separate compilation. However, it is still possible to achieve rapid turnaround \nafter programming changes if the implementation keeps track of these dependencies and implements selective \nrecompilation. The Vortex compiler records the non-local effects of whole-program optimization in a dependency \ngraph, which is stored in a persistent program database. When pieces of the source program are modified, \nthe dependency graph is consulted to determine which object files must be recompiled. Our experience \nwith Vortex has been that incremental compilation is quite practical, and that most programming changes \nresult in very few additional files being recompiled that were not directly modified [Chambers et al. \n951. 3 Vortex Compiler Infrastructure The basic structure of the compiler is shown in Figure 1. Each \nof the different front-ends does whatever parsing and typechecking are appropriate for its input language, \nand then translates the input into the Vortex compiler s intermediate language (IL), which is described \nin Section 3.1. The IL representation of the program then proceeds through the various stages of Vortex \ns optimizer until code is generated; this process is outlined in Section 3.2. The usual way of adding \na new source language to the Vortex back-end is to reuse an existing public-domain or commercial front-end, \nand modify it to output the Vortex IL. For C++, we started with the Edison Design Group s C++ Front End \n[EDG], a commercial product used as the front-end for many industrial C++ compilers. For Modula-3, we \nmodified the front-end from the freely-available DEC SRC Modula-3 implementation [SRC]. For Java, we \nfirst use Sun s javac Java compiler program [JDK] (invoked with the -0 flag to perform as much optimization \nas possible in the existing front-end, to avoid overstating the benefits of Vortex s optimizations) to \ntranslate Java source programs into Java bytecodes, and then we modified the javap bytecode disassembler \nto convert to Vortex IL. Since no public-domain or commercial Cecil front- ends were available, we developed \none from scratch. 3.1 The Vortex Intermediate Language The Vortex intermediate language is the interface \nbetween the different language front-ends and the compiler back-end, and as such, it must allow the front-ends \nto describe the structure of a program in such a way that the optimizations described in Section 2 can \nbe performed. In particular, many existing compilers for object-oriented languages (such as the DEC SRC \nModula-3 implementation) translate input programs into a generic low-level form, where the object-oriented \nfeatures have been translated into sequences of standard imperative constructs. This enables traditional \nback-ends to cope with the new features, but it blocks analyses and optimizations that need higher-level \ninformation about the original language constructs. The Vortex intermediate language supports both traditional \nlower-level operations and selected higher-level operations making things such as message sends, field \naccesses, run-time type tests, and object creations explicit. Individual language front-ends can choose \nhigh-level or low- level translations for their language features. As Vortex optimization phases proceed, \nhigher-level constructs are successively expanded into lower-level constructs, until code geceration \ntakes place. The rest of this section describes the primary global declarations and the kinds of executable \nstatements supported by the Vortex IL 3.1.l Classes and the Inheritance Hierarchy Vortex represents the \nprogram s classes as a directed acyclic graph. The inheritance structure of the program is communicated \nto the back-end by declaring each class and the (possibly empty) list of superclasses from which it inherits. \nThe back-end supports multiple, non-replicating inheritance (virtual inheritance in C++ terms). We currently \nrely on the language front-end to handle other forms of inheritance, such as replicating (non-virtual) \ninheritance in C++, although the back-end could be extended to support other models of inheritance as \nwell. A sample class declaration in the Vortex IL is: class Square isa Rhombus, Quadrilateral; To implement \nlookup rules for operations such as method lookup and instance variable lookup, Vortex treats the inheritance \ngraph as a partial order, choosing the most specific match according to the partial order and reporting \nan ambiguity otherwise. This partial ordering implements the simple rule that children override parents, \nwhich was derived from the Cecil language semantics [Chambers 931. This can accommodate a wide range \nof langgages whose semantics either match these or that have static iypechecking rules that are more \nrestrictive. For example, C++ has a stricter treatment of ambiguities, but C++ s static typechecker ensures \nthat no programs with ambiguities according to C++ s rules produce IL code. Vortex s semantics are sufficient \nto handle the Modula-3 Java Front-ends -------------------mm-- Vortex IL Hiah-level Ootimizations static \nprofile-guided cross-module Traditional analyses optimizations inline expansion Optimizations SPARC code \ngen. Figure 1: Vortex inheritance rules for single-inheritance languages such as Java, Modula-3, and \nSmalltalk [Goldberg &#38; Robson 831, and to handle multiple-inheritance languages such as Cecil, C++, \nand Trellis [Schaffert et al. 851. However, some languages use other rules for method lookup (such as \nCLOS, which uses a left-to-right ordering of parents for resolving ambiguities [Bobrow et al. 88]), and \ncompiling such languages would require adding back-end support for more generally computing the required \npartial order from the inheritance graph. C++ and Modula-3 support parameterized classes and/or methods \n(templates in C++ and generic modules in Modula- 3). Currently, the front-ends for these languages expand \naway the templates, so Vortex sees only unparameterized classes and methods. This strategy is in keeping \nwith how other compilers implement these languages, but it does prevent Vortex from performing optimizations \nto share code across template instantiations where possible. 3.1.2 Instance Variables and Representations \nClasses can have many different possible representations, depending on how their instance variables are \nlaid out and other factors such as alignment, padding, and the sizes of numeric data types. Representation \ndecisions must be made at some level, and Vortex supports two strategies: either the front-end can lay \nout classes and structures completely, or they can provide information about instance variables to the \nback- end and allow the back-end to make layout decisions. Each approach has advantages and disadvantages. \nHaving the front- end perform class layout, which is the approach we have Vortex back end Proaram Database \nsource code interprocedural summaries recompilation dependencies  Architecture chosen for the C++ and \nModula-3 front-ends, preserves compatibility with separately-compiled code that uses the same layout \nrules and avoids having to encode class layout rules for these languages in the back-end. However, having \nthe back-end perform class layout, which is the approach we have chosen for the Cecil and Java front-ends, \nsupports higher-level analysis of object structures (such as better alias analysis) and opens the opportunity \nfor optimizations that rearrange data representations. (Recent work has shown that these can have substantial \nperformance impact in the context of both Modula- 3 [Femandez 9.51 and ML [Shao &#38; Appel95, Tarditi \net al. 961.) If the front-end makes representation decisions, objects are described as a generic array \nof bytes of a particular size. Low- level pointer operations are used to implement instance variable \naccesses, and object allocation is implemented in terms of malloc-type byte allocators. On the other \nhand, if the back-end makes representation decisions, then the front- end declares the instance variables \nof a class, including their representations, and instance variable accesses and object allocations are \nrepresented directly in the IL. Array accesses similarly have both low-level (pointer-based) and high-level \n(direct) forms. Some languages, including Cecil, C++, Java, and Trellis, allow an instance variable to \nbe declared immutable, and the Vortex IL supports this annotation on instance variable declarations. \nKnowing an instance variable is immutable allows the optimizer to retain its knowledge about the instance \nvariable s value, if any, across calls. For languages that use low-level accesses to instance variables, \nVortex IL allows load instructions to be marked as from immutable locations, enabling similar preservation \nof knowledge about that memory location. This low-level form works even if the memory location is only \ninvariant in some contexts. For example, in C++, vtbl words of objects are unchanged outside of their \nconstructor methods [Stroustrup 871. The Vortex IL supports global variable declarations. A declaration \nspecifies a representation and optionally the initial value for the variable (even if the variable is \nan aggregate data type). 3.1.3 Methods and Procedures The Vortex IL uses a notion of a generic function \n[Bobrow et al. 881 to unify the concepts of procedures, methods, and multi-methods. Each call or message \nmaps to a single generic function. A generic function contains a set of dynamically- overloaded methods, \nwith each method indicating where in the program s class hierarchy it is attached. A regular procedure \nis modeled as a singleton generic function whose sole method is attached to the (perhaps implicit) root \nof the inheritance graph, while several singly-and/or multiply-dispatched methods may be grouped together \nin one generic function. Static overloading as in C++ and Java is resolved by the front-end using a name \nmangling technique to encode the static argument types in the name of the message being sent; only dynamic \noverloading remains in the Vortex IL. Method lookup and inlining work uniformly over this generic function \nand method model. A procedure or method implementation is communicated to the back-end via a method definition \nin the Vortex IL, which specifies the method s name, a list of formal arguments and representations, \na return representation, and a body of code made up of Vortex IL statements (see Section 3.1.4 below). \nTo indicate where a method is placed in the inheritance hierarchy, a separate associate declaration is \nproduced by the front- end that names the generic function and the tuple of argument class specializers \nfor the method. Singly-dispatched methods will have non-any specializers only on the receiver argument; \nmulti-methods may specialize any of their arguments. To illustrate how method and associate fit declarations \n together, consider the following C++ class: class Square: Rhombus, Quadrilateral { virtual void drawtint \ncolor); 1;  If draw were introduced in the Shape base class, the front- end might generate the following \nIL declarations: class Square isa Rhombus, Quadrilateral; method draw_6Square_int(this,color):void I... \nILcodefordraw...} associate draw-6Square-int with draw_5Shape_int(@Square, @any);  All methods that \nare part of the same generic function share the same message name in their associate declarations. Initially, \nwe had a single construct in the IL that subsumed both method and associate declarations. However, we \nchanged this when we began work on the Modula-3 front-end, since Modula-3 allows arbitrary procedures \nto be used as methods, perhaps in multiple places. Also, giving each method a unique name separate from \nthe generic function s allows direct non-dispatched calls to methods to be represented easily. 3.1.4 \nIL Statements Executable code in the Vortex IL is represented as a series of three-address statements \n[Aho et al. 861, including the usual complement of arithmetic, logical, pointer, branching (conditional, \nunconditional, and indexed), and direct and indirect procedure call-related statements. In addition, \na number of higher-level object-oriented notions are reified in the Vortex IL, allowing the optimizer \nto more easily reason about them: Message sends. Message sends are a high-level operation in the Vortex \nIL that provide sufficient information for the back-end to perform optimizations like class hierarchy \nanalysis and receiver-class prediction. As an example, consider the following C++ code: Shape* s = . \n. . . s->draw(color); This send of draw might be translated into the following Vortex IL statement: \nsend draw-5Shape-int(s, color); Subsection 3.1.5 describes two strategies for how Vortex implements a \nsend statement. Instance variable accesses. For languages where the front-end generates instance variable \ndeclarations and expects the Vortex back-end to perform object layout, accesses to instance variables \nare represented with instructions that specify the object being accessed, the name of instance variable, \nand the name of the class where the instance variable was declared (in case there are instance variables \nwith the same name but declared in different classes). For example, consider the following Java code: \nRectangle r = . . . . r.upper-left = 100; This instance variable access is translated into the following \nVortex IL statement, assuming that the upper-left instance variable was introduced in the Rectangle class: \nr.upper-left@Rectangle := 100; . Object allocations. A simple new statement is supported to allocate \nobjects of a particular class. Static class analysis can determine the static type of the result of this \nstatement exactly. Run-time class tests. Vortex IL supports both class identity tests and subclass tests \nas explicit comparisons. For example: if s is-class Rectangle goto . . . . if s inherits-from Shape goto \n. . . . The Java front-end outputs these tests to implement dynamically-checked type casts. Although \nnot currently implemented in high-level form, the Modula-3 front-end could also use these run-time tests \nfor its TYPECASE and NARROW features. Subsection 3.1.6 describes implementation strategies for class \ntests. Type assertions. Type assertions communicate static class information from the front-end to the \nback-end. In general, a type assertion is a guarantee from the front-end to the back-end that a particular \nvalue holds an instance of a particular set of classes at a particular program point. Similarly to run-time \nclass tests, Vortex IL supports class identity and subclass assertions: assert-type I Rectangle; assert-type \ns inherits-from Shape; Class identity assertions are output by the front-ends when a language-level \noperation has just created a value that is known to be of a specific class (e.g., allocating a new object). \nSubclass assertions are output whenever the front-end has information about the static type of a value \nthat might be useful to the back-end optimizer (e.g., the static type of the arguments of a method or \nthe result of an instance variable access). 3.1.5 Implementing Message Sends Vortex currently supports \ntwo implementation strategies for message sends, one based on dispatch tables and one based on polymorphic \ninline caches (PICs) [HGlzle et al. 911. For Cecil and Java, we select the PIC-based implementation strategy: \ndynamically generate a piece of executable code per call site that linearly tests for the N most common \nreceiver classes, falling back to a hash table lookup if the cache overflows. The PIC-based strategy \nis general, since it does not require dispatch table layout algorithms, but it can be less efficient \nthan table-based implementations. For C++ and Modula-3, we implement message sends using the language \ns native table indexing strategy, and rely on the language front-end to generate the appropriate tables \n(i.e., virtual function tables in C++ and method suites in Modula-3). For the back-end to produce the \nright table indexing sequence when implementing a send in lower-level terms, we augment the send IL statement \nto include the offset of the class identifier in the receiver object (e.g., the location of the vtbl \npointer in the object in C++), as well as the entry number to use in the dispatch table. This information \nis sufficient to generate the appropriate code to call indirectly through the dispatch table. Keeping \nthe table generation in the front-ends has the advantage of not requiring the back-end to deal with the \ncomplexities of dispatch table layout (which is difficult to do in the presence of multiple inheritance), \nand it preserves compatibility so that we can call separately-compiled code from our generated code. \nHowever, it places some limitations on the optimizations that the back-end can perform. In particular, \nbecause dispatch tables are not exposed at a high enough level, the back-end cannot perform optimizations \nthat would require generating different dispatch tables. Examples of such optimizations include customization \nand specialization [Chambers &#38; Ungar 89, Lea 90, Dean et al. 95a] and converting a C++-style method \nfrom virtual to non-virtual to reduce the size of dispatch tables, when the back-end is able to detect \nthat the method is not overridden anywhere in the program. However, in the presence of class hierarchy \nanalysis, Vortex is able to streamline the virtual function calling sequence for C++: by detecting portions \nof the class hierarchy that do not use multiple inheritance, one pointer adjustment per call can be eliminated.* \n3.1.6 Implementing Run-Time Class Tests For most languages, implementing a class identity test is straightforward: \nload the class identifier from the object (at a language-specific offset) and test it against a constant \n(derived from the class in a language-specific way). However, in C++ this test is complicated by the \nfact that, in the presence of multiple inheritance, an object can have multiple different vtbl addresses, \neach for different statically-typed views of the object [Stroustrup 871. To support C++-style class identifiers \neffectively, the front-end annotates each class with a mapping from static type views to the name of \nthe vtbl constant stored at that offset. The back-end then uses the known static type of the object being \ntested to determine which vtbl constant to compare against. Efficient constant-time subclass testing \ncan be implemented in languages having only single inheritance with a load instruction or two followed \nby a pair of comparisons. In languages with multiple inheritance, subclass testing is more *Trampoline \nfunctions are another technique for eliminating this pointer adjustment in the common (non-multiple-inheritance) \ncase, but they incur a higher cost when multiple inheritance is actually used. complex. In our Vortex \nimplementation, we simply compute an NxN boolean matrix of subclass relations, where N is the number \nof classes in the program. The <i,j>th entry indicates whether or not the class with the unique id i \nis a subclass of the class with id j. Alternative encodings of the subclassing relationship are possible, \nrepresenting different time-space tradeoffs; Ait-Kaci et al. provide a useful overview of efficient lattice \noperations that discusses many of these alternatives [AK et al. 891. 3.1.7 Exceptions Many languages \nsupport some form of exceptional control flow: Modula-3 supports traditional exceptions, C++ and Java \nsupport object-based exceptions, and Cecil, like Smalltalk and Self, has non-local returns that allow \na nested closure to return directly from its lexically-enclosing method. The Vortex IL describes exceptions \nby treating calls and sends that can raise exceptions as control flow branches, with the exceptional \nreturn branch passing to an exception handler label. Vortex supports two implementation strategies for \nexceptions: one based on frame-by-frame propagation (using special return calling conventions and tests \nafter each procedure call that can raise an exception), and one based on the language s runtime system \ns setjmp/longjmp mechanism (where a setjmp is performed when entering a try block and a longjmp is invoked \nto raise an exception). Cecil and Java use the propagation-based method, while Modula-3 uses its own \nsetjmp/longjmp-based mechanism; none of our C++ programs use exceptions. Originally, we used a lower-level \nform of exceptions for Modula-3, with the front-end generating explicit calls to setjmp to mark the beginning \nof TRY blocks. However, the Vortex optimizer was performing incorrect optimizations because its dataflow \nanalysis did not understand the strange control flow that can be introduced in the presence of setjmp. \nTo avoid sacrificing optimization in the presence of exceptions, which is the normal strategy, we opted \nto introduce a high-level explicit form for specifying the possible control flow in the presence of exceptions, \nand Vortex s dataflow analyses are left unhindered. However, to safely compile programs that call setjmp \nexplicitly, Vortex s dataflow analyses should be modified to cope with its irregular control flow, under \nthe assumption that explicit calls to setjmp, other than for implementing exceptions, are likely to be \nrare. 3.2 Optimization Phases Vortex takes the IL representation of the program produced by one of its \nfront-ends and performs a series of analyses and transformations on its way to generating optimized target \ncode. As part of this process, high-level IL operations are lowered by expanding them into sequences \nof simpler operations. In this section we highlight some of the interesting stages in this transformation. \nA central piece of supporting infrastructure in this process is Vortex s iterative dataflow analysis \nframework. All of the analyses and transformations rely on this framework to manage the details of iterative \ndataflow: control flow graph traversal, merging dataflow information at control flow merges, fixed-point \nconvergence testing for loops, and graph transformations. Clients of the framework simply provide an \nanalysis closure that encapsulates the appropriate flow functions and an analysis-specific domain class \nthat implements the copy, meet, and reached-convergence methods. (Vortex s IDFA framework is similar \nin spirit to the Sharlit system developed by Tjiang and Hennessy [Tjiang &#38; Hennessy 921.) Analyses \nwritten separately using Vortex s framework can also be composed so that they run together simultaneously, \nthus alleviating potential phase-ordering problems. (Click and Cooper describe the theoretical conditions \nunder which the resulting combination of optimizations will produce better results than repeated applications \nof the original separate optimizations [Click &#38; Cooper 951.) The major phases in Vortex s compilation \nof a control flow graph are: Loop identification: A dominator-based algorithm identifies the loops and \nthe loop nesting structure of the procedure being compiled. (Vortex alternatively allows front-ends to \nprovide this information directly, avoiding recomputing it in the back-end.) Vortex s IDFA framework \ncurrently requires that control flow graphs be reducible (roughly, have a single entry node) [Aho et \nal. 861; the loop identification pass detects irreducible flow graphs and reports them as errors. Object-oriented \noptimizations: This phase utilizes the iterative dataflow analysis framework s composer interface to \nrun a number of separately-written optimization passes simultaneously, thus potentially arriving at a \nbetter final fixed-point due to the synergistic relationships among the passes. Intraprocedural class \nanalysis, class hierarchy analysis, profile-guided receiver class prediction, inlining, splitting, must-alias \nanalysis, and an enhanced common subexpression elimination (CSE) pass all run in parallel. In addition \nto standard CSE transformations, the enhanced CSE pass also performs constant and copy propagation, constant \nfolding, simplification of arithmetic operations, and elimination of redundant load and store operations. \nBecause inlining is included as an integral part of the combined pass, when a routine is inlined it is \nimmediately optimized. This allows the callee to be fully optimized in the context of its caller and \nfor the downstream code of the caller to benefit from any information gained by inlining the callee [Chambers \n&#38; Ungar 901. Closure optimizations: Partial dead code elimination delays closure object creations \nuntil absolutely necessary (hopefully removing them entirely from the common case paths) and environments \nare marked to be either heap-allocated or stack-allocated. This phase has no effect in languages lacking \nclosures or nested procedures. Lowering I: High-level operations like table sends and class tests are \nexpanded into an equivalent sequence of lower level operations. All accesses to variables defined in \nlexically-enclosing scopes are expanded into an explicit series of loads. Standard optimizations: A suite \nof traditional optimizations such as CSE (again, since new opportunities for optimization have been exposed \nduring the lowering phase), dead store elimination, and dead assignment elimination are applied. C code \ngeneration: If Vortex is generating C code, then code is produced now and compilation ends. The generated \nC code is portable across platforms with the same word size. Otherwise the following additional stages \noccur: Lowering II: Remaining high-level nodes and complex operators are expanded. After this second \nlowering phase, most IL operations can be implemented in a single machine instruction. Standard optimizations \nrepeated. 8 Low-level optimizations: Graph coloring-based intraprocedural register allocation and instruction \nscheduling. 8 Assembly code generation. 3.3 Experience Vortex originally was a Cecil-specific compiler, \nbut over the last year or so we reworked it to be more language-independent. Starting with Cecil first \nhad both benefits and drawbacks: The original Cecil compiler already supported a set of sophisticated \noptimizations aimed at a powerful set of language features, such as multiple dispatching, multiple inheritance, \nand closures. Since the other languages we compile mostly have subsets of Cecil s object-oriented language \nfeatures, only a few small changes were needed for the Vortex compiler; associate declarations are one \nenhancement of the original Cecil compiler, and we had to generalize Cecil s exception handling support \nto cover what is used in Modula-3 and Java. . The original Cecil compiler was geared towards a language \nwith a type-safe, pure data model. For Vortex, we needed to do significant retrofitting to support non-word-sized \ndatatypes, uncontrolled pointers, structures and arrays with non-scalar elements, and so on. Both Modula-3 \nand Java had nicely-modularized existing front-ends that presented a good interface for compiling into \nthe Vortex IL: DEC SRC Modula-3 supports a separate code-generator interface, which we added a new implementation \nof, and Java defines a bytecoded intermediate language that we translated in a straightforward way into \nthe Vortex IL. DEC SRC Modula-3 s code-generator interface includes only low- level operations, so we \nhad to augment the interface with high- level operations; Java s bytecodes are already at the appropriate \nlevel for Vortex. EDG s C++ front-end was also reasonably modular, although we had to start with an annotated \nparse tree and implement intermediate code generation to the Vortex IL. The complexity and size of the \nC++ language made this a more difficult task than the other two languages, but the real obstacle to gathering \nand compiling C++ applications with Vortex is that there is no well-defined C++ language in common use: \neach program we gathered would only compile on a subset of compilers, or used compiler-specific extensions, \nor used a version of the language that was different than what EDG s front-end expected. This situation \nmakes it quite difficult to do compiler research for c++. A final difficulty is that some C compilers \ncannot cope with the C code Vortex produces. We had to go to some lengths to produce C code that did \nnot have functions or basic blocks that were too long or to produce files that had too many global symbols. \nMachine-generated programs have quite different characteristics than human-generated programs, and we \nencountered the same kinds of problems as other researchers in trying to compile our output. 4 Metrics \nfor Describing Object-Oriented Programs A program s structure and the degree to which it uses object-oriented \nlanguage features, such as inheritance and message sends, have a profound impact on the effectiveness \nof high- level optimizations such as class hierarchy analysis, exhaustive class testing, and profile-guided \nreceiver class prediction. Therefore, before we present our experimental assessment of these techniques, \nwe first define several metrics for describing object-oriented programs and use them to characterize \nour benchmark suite. These metrics attempt to quantify interesting properties of the program s internal \nstructure as well as predict how much an application can be expected to benefit from a particular optimization. \n4.1 Metric Definitions 951. Due to space constraints, we only use the first class We considered a number \nof different metrics for characterizing our applications. After evaluating how well they captured the \nunderlying program structure and usage of object-oriented language features, we selected the following \nmetrics as being the most illuminating: Number of immediate Parents: Measures the number of immediate \nparents of each class in the program; indicates the degree to which multiple inheritance is utilized. \nNumber of Immediate Children: Measures the number of classes that directly inherit from each class in \nthe program; indicates the branching factor (breadth) and bushiness of the class hierarchies. . Maximum \nDistance to Root of Inheritance Hierarch.y: The longest path from each class to the root of its inheritance \nhierarchy; indicates the depth of the class hierarchies used by the program. Number of Applicable Methods: \nMeasures the number of applicable methods at a dynamically-dispatched call site, optionally weighted \nby the execution frequency of the call site. Class hierarchy analysis works well when a large number \nof call sites have a single applicable method, while exhaustive class testing applies when a few methods \nare possible at a call site. Class Test EfJiciency: Measures the fraction of calls at a call site that \ngo to the most common receiver class at that call site, weighted by the execution frequency of the call \nsite. This histogram will always be a subset of the one representing the dynamic number of applicable \nmethods at a call site. Profile-guided receiver class prediction can work well if the most common class \nat a call site is much more common than other classes; similarity between this histogram and the histogram \nfor the dynamic number of applicable methods implies that dynamically-important call sites are dominated \nby a single receiver class. Average Cycles Between Message Sends: Measures the average number of cycles \nelapsed between message sends. As this value increases, we expect the overall performance impact of the \noptimizations to decrease. Bieman and Zhao also used the first three of these metrics (and some additional \nones) in their study of inheritance in C++ applications [Bieman &#38; Zhao 951. They utilized the metrics \nto assess the amount of code reuse through inheritance in large C++ programs; in contrast, we are interested \nin characterizing how the structure of the inheritance hierarchy affects the need for optimization. In \nprevious work, we applied several metrics to measure the peakedness and stability of the profile data \nused to drive profile-guided receiver class prediction [Grove et al. same metric here. 4.2 Applying the \nMetrics We applied the metrics defined in the previous section to a number of medium-to-large applications \nwritten in Cecil, Java, Modula-3, and C++. Table 1 summarizes several distinguishing language features. \nTable 1: Language Characteristics I II I I I I Cecil Pure Dynamica Yes Yes Java Mostly Mostly Yesb Yes \npure static IModula-3 IIHybrid IStatic IYes INO c++ IlHybrid 1 Static I NO Yes I a. Cecil allows mixing \nstatically-and dynamically-typed code, and running the static typechecker is optional. As a result, the \noptimizer ignores static type declarations and ensures type-safety through dynamic checks where needed. \n b. final methods cannot be overridden, although they can override other methods, c. Java supports multiple \nsubtyping although only single code inheritance. Our number-of-parents metric indicates the total number \nof supertypes and superclasses of a class.  Several of these language characteristics have a large \nimpact on the effectiveness of the object-oriented optimizations. Because of the much higher frequency \nof message sends in pure languages compared to hybrid languages, the impact of the object-oriented optimizations \nis much more dramatic. Static type declarations are excellent fodder for class hierarchy analysis, so \none would expect it to be more effective in statically-typed languages. Table 2 describes the application \nsuite and presents the results of applying the metrics to the programs. We use histograms to visually \ndisplay the metrics; the height of each bar represents the percentage of all elements whose metric value \ncorresponds to the bar s x-coordinate For C++, we also examined a version of two of the benchmarks where \nwe hand-modified the programs to make all methods virtual, to explore an alternative programming style; \nthese two benchmarks are identified with an -av suffix. All of our benchmarks are substantial in size, \nwith all at least 10,000 lines and most over 20,000. The Cecil programs have the largest and deepest \nclass libraries, with javac and 91 92 ? 9 : :POWWUpwWs!a JO # d - f :: .- 93  m2tom3 also having deep \nclass hierarchies, but all the programs except prover have at least a hundred classes and 400 methods. \nCecil programs used multiple inheritance a moderate amount, and javac used multiple subtyping some, but \nthe C++ programs made little if any use of multiple inheritance. Examination of the static number of \ncall sites with only one applicable method would suggest that class hierarchy analysis could be effective \nin most of the benchmarks, but the version weighted by dynamic execution frequency indicates a somewhat \nlower expected benefit; eon and ktsim-av buck this trend. As expected, the C++ all-virtual programs have \nmuch greater number of call sites amenable to class hierarchy analysis than the regular versions of these \nbenchmarks. About half the benchmarks have significant numbers of messages with 2-3 applicable methods, \nindicating that exhaustive class testing could be beneficial (although Vortex does not support this technique \nfor C++ or Modula-3 yet). Since the histograms reporting the efficiency of profile-guided class testing \nare usually similar to the dynamic number of applicable method histograms, profile-guided receiver class \nprediction looks promising. The number of cycles between dispatches suggests that Cecil, Java, and the \nall-virtual C++ programs can expect the greatest impact from the optimizations, while Modula-3 programs \ncan expect the least benefits from Vortex s message-level optimizations. With Cecil, optimization of \nother run-time overhead between dispatches, such as closure creations and the extra cost of multi-method \ndispatching, will increase the observed impact of Vortex s optimizations. 5 Performance Studies To assess \nthe performance impact of Vortex s object-oriented optimizations, we compare the following configurations: \n. native: Program is compiled by a native language compiler. For C++ programs we use g++ version 2.63 \nwith options -02 -finline-functions -msupersparc; for Modula-3 programs we use the DEC SRC Modula-3 compiler \n(which uses a modified version of gee version 2.6.3 as its back-end) with the -02 option. For Java the \nnative configuration runs the java interpreter on the program s precompiled .class files. There is no \nseparate native configuration for Cecil programs, since Vortex is currently the only Cecil compiler. \nbase: Program is compiled by Vortex with the collection of traditional optimizations described in Section \n3.2 but without any inlining or optimization of dynamically-dispatched messages or class tests. (For \nC++ and Modula-3, the combined CSE pass is not fully functional, so the base configuration in these languages \nuses only gee optimizations with no Vortex- level optimizations.) inline: base is augmented with cross-module \ninlining. intra: inline is augmented with intraprocedural class analysis, hard-wired class prediction \nfor common messages (for Cecil programs only), and splitting. intra+CHA: intra is augmented with class \nhierarchy analysis. intra+CHA+exh: intra is augmented with class hierarchy analysis and exhaustive class \ntesting. intra+profile: intra is augmented with profile-guided receiver class prediction. intra+CHA+profile: \nintra is augmented with class hierarchy analysis and profile-guided receiver class prediction. intra+CHA+exh+profile: \nintra is augmented with class hierarchy analysis, exhaustive class testing, and profile-guided receiver \nclass prediction. In all Vortex configurations, C code was generated and then compiled by gee version \n2.6.3 with options -02 -msupersparc. By producing C code, and compiling it with the same compiler used \nin the native configuration for C++ and Modula-3, we attempt to minimize distortions produced by aspects \nof Vortex compilation that are orthogonal to the high-level optimizations we are interested in studying. \nFor the C++ and Modula-3 programs, with the exception of prover (13%) and porky (39%), all base times \nwere within 8% of their native time. The large performance difference between the native and base configurations \nfor porky are explained by porky s frequent assignments of small 4 and 8 byte structures. Currently, \nVortex does not attempt to optimize structure assignments, compiling them down to calls to bcopy. The \ngee compiler compiles down small structure assignments to a sequence of inline loads and stores. For \nModula-3, our base configuration is losing some performance relative to native due to compilation to \nC; nested procedures in particular are not implemented as efficiently in base as in native. For Java, \nthe native (interpreted) version of javac was 6 times slower than Vortex s base configuration, demonstrating \nthe performance advantage of compilation. All run-time measurements are CPU times (user time + system \ntime) gathered on a lightly-loaded SPARCStation-20/6 1 with 128MB RAM, taking the median of 11 runs. \nVariations in CPU time from run to run of 10% are normal for this machine. Figure 2 presents selected \napplication speedup data; the complete set of application execution times and the dynamic number of message \nsends during each program execution can be found in Appendix A. Java instr-sched typechecker vortex \njavac c++ Modula-3 ixx ixx-av ktsim ktsim-av eon m2tom3 prover m3fe Figure 2: These graphs show application \nspeedup relative to the base configuration of the applica- base-av tion. For the two all-virtual C++ \nprograms, speedups are relative to the base configuration of the m inline original application, and an \nadditional bar, base-av, is shown. The impact of augmenting the i, i- ! I-CHA CHA, and i-CHA-exh configurations \nwith profile-guided class prediction is shown by the additional 0 i-CHA-exh height of the cross-hatched \nportion of each bar. For some configurations, the impact of profile data x + profile was negligble, and \nthus is not visible above. As expected, Vortex s optimizations had the largest impact on the Cecil programs, \nimproving their performance by an order of magnitude over the base configuration. Profile-guided class \nprediction was the single most important optimization for Cecil, but each technique resulted in a non-trivial \nimprovement. Although less spectacular than the Cecil results, many of the C++ and Java programs obtained \nspeedups on the order of 25-35%; m2tom3 showed the largest speedup of the Modula-3 programs, gaining \nabout 5% from cross-module inlining and an additional 4% from the object-oriented optimizations. In the \nnon-Cecil programs, which optimizations were most effective varied from one application to the next. \nThe.ktsim-av benchmark should have equaled the performance of ktsim after applying class hierarchy analysis, \nbut instead it remained substantially slower. Part of the difference can be blamed on the large number \nof replicated virtual function implementations and tables replicated throughout the program, hurting \nboth instruction and data cache performance. However, the performance gains obtained by applying these \noptimizations does not come without cost; both compile time and compiled code space Compile time increases \ncan the direct cost of performing indirect cost of increasing increasing the amount of phases. For these \nprograms, compile time increase was typically increase as a result. be broken into two components: the \nanalysis/optimization and the the amount of inlining, and thus time spent in other compilation we found \nthat the majority of the attributable to other compilation phases being faced with larger control flow \ngraphs after inlining. In the worst case, observed for a subset of the non- Cecil programs, compilation \ntime doubled between base and it-dine, and doubled again between inline and any of the other more optimized \nconfigurations. However, Vortex is a research compiler, designed for ease of extension rather than compilation \nefficiency, and so the magnitude of the compile time increases may not be indicative of what would be \nseen if these techniques were implemented in a production compiler. In contrast, code space costs were \nmodest. For the C++ programs, compiled code space grew by 3% to 20% over the base configuration. For \nthe other three languages, code space changes ranged from -10% to +4% over base. The failure of prover \nand m3fe to benefit more substantially from these optimizations was foreshadowed by the average cycles \nbetween message sends metric (1522 and 2433 respectively): those two Modula-3 programs spend little of \ntheir time performing dynamic dispatching. Overall, the dynamic number of applicable methods metric was \na good predictor of the effectiveness of class hierarchy analysis. The three programs which saw the smallest \nreduction in message sends (m3fe, porky, and eon), also had the smallest dynamic percentage of call sites \nwith a single applicable method. However, this metric was not a perfect predictor; ktsim looked very \nsimilar to eon, but a large fraction of its message sends were eliminated by class hierarchy analysis. \nThe class test efficiency metric was suggestive of the effectiveness of receiver class prediction, but \nwas not always accurate because Vortex only inserts class tests based on profile information when the \ntarget method is small enough to be inlined, and the metric does not acount for target method size. In \naddition to improving the performance of existing applications, high-level optimizations like class hierarchy \nanalysis and receiver class prediction can reduce the need for programmers to hand-optimize their programs. \nFor example, the authors of ktsim also provided us with a version of the program that they had hand-optimized \nby manually removing all virtual function calls from the common execution paths. While running faster \nthan the original native configuration, this hand-optimized version performed slightly worse than the \noriginal, flexible version optimized automatically with class hierarchy analysis. Related Work Several \nother projects have worked on implementing and assessing advanced optimizations for hybrid languages: \nCalder &#38; Grunwald, Aigner &#38; Holzle, Bacon &#38; Sweeney, and Pande &#38; Ryder have looked at \noptimizing C++, and Fernandez and Diwan, Moss, &#38; McKinley have looked at optimizing Modula-3. Calder \nand Grunwald consider several ways of optimizing dynamically-dispatched calls in C++ [Calder &#38; Grunwald \n941. They examined some characteristics of the class distributions of several C++ programs and found \nthat although the potential polymorphism was high, the distributions seen at individual call sites were \nstrongly peaked, suggesting that profile-guided receiver class prediction would pay off. However, they \nonly simulated the effects of converting indirect calls to direct calls (ignoring other benefits,such \nas inlining), and they did not test this hypothesis by implementing class prediction in a compiler. Aigner \nand Hijlzle implemented a prototype system to compare class hierarchy analysis and profile-guided receiver-class \nprediction for C++ programs [Aigner &#38; Htilzle 961. Their system works by first combining a C++ program \ncomposed of multiple source files to produce a single, monolithic C++ file. This file is then fed into \ntheir optimizer, which works as a source-to-source transformation, producing another C++ file (that has \nbeen optimized with some combination of class hierarchy analysis and profile-guided receiver-class prediction). \nFinally, this single file is compiled with a native C++ compiler to produce the final program executable. \nOne major difference with the Vortex approach is that they leave inlining to be done by the host C++ \ncompiler, rather than performing it in conjunction with class hierarchy analysis and class prediction. \nThis has the effect of preventing their preprocessor from analyzing the to-be-inlined callee in the context \nof its call site, and analyzing the rest of the caller using static class information derived from the \ncallee. This lack of integration also biases their results on the effects of combining class hierarchy \nanalysis with profile-guided receiver class prediction because the bodies of methods that are inlined \nvia class hierarchy analysis do not benefit from class prediction, thus making class hierarchy analysis \nappear to be less effective than it actually would be in a fully integrated implementation. On the two \nbenchmarks we have in common (ixx and porky), Vortex was able to eliminate a larger percentage of the \nvirtual functions calls and achieved marginally better speedups. Finally, their system does not support \nselective recompilation, since their source-to-source transformation starts by combining the entire program \ninto a single C++ source file. Bacon and Sweeney examined the effectiveness of three purely static techniques \nfor replacing virtual function calls with direct calls in C++, one used when there was only one method \nwith a particular name (and argument type signature) in the program, class hierarchy analysis, and rapid \ntype analysis (an extension of class hierarchy analysis that prunes unreachable classes and methods in \nparallel with computing the possible targets of call sites) [Bacon &#38; Sweeney 961. For two of their \nfive non-trivial benchmarks, ranging in size from 5,000 to 17,500 lines, rapid type analysis statically-bound \nmore calls than class hierarchy analysis. They did not look at the effectiveness of run-time class testing \n(driven by either class hierarchy analysis or execution profiles), nor did they report bottom-line impact \non execution time due to their transformations. They do report that rapid type analysis shrinks the size \nof the generated programs substantially. Pande and Ryder apply aggressive interprocedural context-sensitive \npointer analysis to statically-bind virtual function call sites in C++ [Pande &#38; Ryder 941. Their \nresults (in terms of the number of dynamically-dispatched call sites that are statically bound) appear \ngood, but their current benchmark suite is made up of programs of less than 1000 lines, so it is unclear \nhow the quality and cost of their analysis will scale to larger, more realistic C++ programs. Also, they \ndo not report the bottom-line run-time performance impact of their optimizations. Fernandez developed \nan optimizing Modula-3 system that performs class hierarchy analysis, inlining, and procedure specialization \nat link-time [Fernandez 951. Her system delays all code generation until link time: the compiler does \na simple translation of each source file to an intermediate representation, and the linker combines these \nIR files and generates code for the entire program. Her optimizations were able to statically-bind between \n2% and 79% of the indirect calls and to reduce the number of instructions executed by 3- 11% in a suite \nof Modula-3 benchmark programs (compared to the output of an unoptimizing compiler). Because the linking \nstep has become a bottleneck, her system performs only limited, basic-block level optimization; there \nis no comparison of her approach to that achieved using a more optimizing compiler. Our benchmark suites \nshare the prover and m3fe programs. Femandez reports that her version of class hierarchy analysis removes \nroughly 79% of the indirect calls in prover and 56% in m3fe, while we see only 20% and 0% drops, respectively, \nfor our class hierarchy analysis. However, the DEC SRC ModuIa-3 front-end by default implements all cross-module \nprocedure calls, even non-dispatched calls, as indirect jumps. To construct a more reasonable baseline \nsystem, we modified our Modula-3 front-end to implement regular Modula3 procedure calls using direct \njumps. Since Fernandez uses the unmodified Modula-3 front-end, her indirect jump reductions appear to \ninclude those for the trivially-optimizable cross-module procedure calls, overstating the effectiveness \nof her system at optimizing dynamically-dispatched method calls. Diwan, Moss, and McKinley developed \na whole-program optimizer for Modula-3 incorporating class hierarchy analysis (which they call type hierarchy \nanalysis), intraprocedural and interprocedural class analysis, and a simple, monovariant heap analysis \nto compute class sets for instance variables [Diwan et al. 961. They analyzed these algorithms on a range \nof Modula- 3 programs, and their results indicate that class hierarchy analysis obtains nearly all of \nthe benefit of the more powerful analyses (ignoring the possibility of sends to NULL, a distinguished \nobject in Modula-3 that results in an error whenever a message is sent to it). They present performance \nresults indicating less than a 2% performance increase, in part because their system does no optimizations \nbased on the information computed by the analyses other than converting message sends into direct calls; \nin particular, they do not perform inlining. Our bottom-line speedup results for Modula- 3 programs echo \ntheir limited gains, although our speedups appear slightly larger on the one benchmark (m2tom3) we have \nin common. Diwan s system also does not support selective recompilation after programming changes. Agesen \nand HGlzle compared the effectiveness of the Cartesian Product interprocedural class analysis algorithm \n[Agesen 951 with profile-guided receiver class prediction [Hiilzle &#38; Ungar 941 for Self programs \n[Agesen &#38; HSlzle 95, Agesen &#38; Hiilzle 961. In work related to our work in this paper, they extrapolated \ntheir results to a hypothetical hybrid language, Self++, by ignoring the influence of messages sent to \nobjects that would be primitive datatypes in a hybrid language, concluding that the contributions made \nby both optimization techniques (in terms of percentage of dispatches eliminated) would be substantially \nreduced in hybrid languages. Our work can be viewed as real experimental data for a range of real languages \nto support their general prediction about hybrid languages. 7 Conclusions Our studies using Vortex have \nprovided some initial data on how well advanced optimizations for object-oriented languages impact the \nperformance of sizeable programs written in a variety of languages, ranging from a very pure language \n(Cecil) to low-level hybrid languages (C++), with Java and Modula-3 providing intermediate points in \nthe language design space. As shown previously, programs in dynamically-typed, purely object-oriented \nlanguages can speed up by an order of magnitude through the application of optimizations for message \nsends and closures. New results in this study show that programs written in statically-typed, hybrid \nlanguages can also achieve significant speed-ups, on the order of 25-35%, with little cost in compiled \ncode space (although Vortex s research prototype nature leads to long compilation times). Our study benefits \nfrom using a common optimizing compiler back-end applying essentially the same suite of optimizations \nuniformly across all languages, providing a level playing field on which to compare the effectiveness \nof optimizations across programs and languages. Vortex has been a good infrastructure for research on \noptimizations for object-oriented languages. By translating different languages into a common intermediate \nlanguage that still retains high-level information such as the location of message sends, optimizations \ncan be written once rather than separately for each language. The supporting dataflow analysis frameworks \nin Vortex make it much easier to add new optimizations. Vortex s dependency mechanisms to support selective \nrecompilation help to make interprocedural or References whole-program optimizations practical in a normal \nprogram development environment. In the future, we hope to add additional language front-ends (such as \nSmalltalk and ML) and additional optimizations (such as interprocedural class analysis, alias analysis, \nand data representation and layout optimizations) to Vortex. It may seem that Vortex is an ideal platform \nfor comparing the performance of languages, for instance assessing how the performance of Cecil compares \nto C++. However, such cross- language comparisons are extremely difficult to perform well. First, realistically \nlarge programs need to be written in each language being compared; to date, we know of only small benchmarks \nof less than 1,000 lines that have been translated into several different object-oriented languages. \nSecond, a decision must be made as to whether identical algorithms and language features should be used \nin the different languages (adopting a least-common-denominator style), or whether the programs should \nbe written using the best or most common programming style for each language. The former strategy may \nproduce more closely comparable results, but the latter strategy may better reflect the expected performance \nof the typical program in each language. We have not tried to perform cross-language comparisons to this \nextent. If such benchmarks were developed, however, Vortex might be an excellent test-bed for uniformly \noptimizing each of the language-specific versions. Acknowledgments This research is supported in part \nby an NSF grant (number CCR- 9503741), an NSF Young Investigator Award (number CCR-9457767), an NSF Research \nInitiation Award (number CCR-9210990), a grant from the Office of Naval Research (contract number NOO014-94-l-1136), \nand gifts from Sun Microsystems, IBM, Xerox PARC, Pure Software, and Edison Design Group. We would also \nlike to especially thank the groups responsible for developing the basis for several of our language \nfront-ends: Edison Design Group for donating their C++ Front End product, Digital Equipment Corporation \ns System Research Center for producing DEC SRC s Modula-3 implementation, and Sun Microsystems for producing \nthe Java Development Kit. Urs HGlzle provided useful feedback on an earlier version of this paper. We \nare grateful to Gerald Aigner, David Detlefs, Amer Diwan, Mary Femandez, Urs Halzle, Dennis Lee, Ted \nRomer, and Peter Shirley for providing us with benchmark programs. An initial release of the Vortex compiler \nand its Cecil front-end is currently available. We expect that the next release of the system, planned \nfor late 1996, will include the C++, Modula-3, and Java front-ends. More information about the Vortex \ncompiler is available via the World Wide Web at: http://www.cs.washington.edu/research/projectslcecil \n [Agesen &#38; Hiilzle 951 Ole Agesen and Urs Hiilzle. Type Feed- back vs. Concrete Type Analysis: A \nComparison of Optimiza- tion Techniques for Object-Oriented Languages. In OOPSLA 95 Conference Proceedings, \npages 91-107, Austin, TX, October 1995. [Agesen &#38; Hiilzle 961 Ole Agesen and Urs HGlzle. Dynamic \nvs. Static Optimization Techniques for Object-Oriented Languag-es. Theory and Practice ofobject Systems, \nl(3), 1996. [Agesen 951 Ole Agesen. The Cartesian Product Algorithm: Sim- ple and Precise Type Inference \nof Parametric Polymorphism. In Proceedings ECOOP 95, Aarhus, Denmark, August 1995. Springer-Verlag. [Aho \net al. 861 Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools. \nAddison-Wes-ley, Reading, MA, 1986. [Aigner &#38; HGlzle 961 Gerald Aigner and Urs HGlzle. Eliminating \nVirtual Function Calls in C++ Programs. In Proceedings ECOOP 96, Linz, Austria, August 1996. Springer-Verlag, \n[AK et al. 891 Hassan Ait-Kaci, Robert Boyer, Patrick Lincoln, and Roger Nasr. Efficient Implementation \nof Lattice Opera- tions. ACM Transactions on Programming Languages and Systems, 1 l(l):1 15-146, January \n1989. [Bacon &#38; Sweeney 961 David F. Bacon and Peter F. Sweeney. Fast Static Analysis of C++ Virtual \nFunction Calls. In OOPS-LA 96 Conference Proceedings, San Jose, CA, October 1996. [Bieman &#38; Zhao \n9.51 James M. Bieman and Josephine Xia Zhao. Reuse Through Inheritance: A Quantitative Study of C++ Soft- \nware. In Proceedings of the Symposium on Software Reusabil-ity. ACM SIGSOFT, August 1995. Software Engineering \nNotes. [Bobrow et al. 881 D. G. Bobrow, L. G. DeMichiel, R. P. Gabriel, S. E. Keene, G. Kiczales, and \nD. A. Moon. Common Lisp Ob- ject System Specification X3J13. SIGPL4N Notices, 28(Spe-cial Issue), September \n1988. [Calder &#38; Grunwald 941 Brad Calder and Dirk Grunwald. Reduc- ing Indirect Function Call Overhead \nin C++ Programs. In Con- ference Record of POPL 94: 21st ACM SIGPUN-SIGACT Symposium on Principles of \nProgramming Languages, pages 397-408, Portland, Oregon, January 1994. [Chambers &#38; Ungar 891 Craig \nChambers and David Ungar. Cus- tomization: Optimizing Compiler Technology for Self, A Dy- namically-Typed \nObject-Oriented Programming Language. SIGPLAN Notices, 24(7):146-160, July 1989. In Proceedings of the \nACM SIGPLAN 89 Conference on Programming Lan-guage Design and Implementation. [Chambers &#38; Ungar 901 \nCraig Chambers and David Ungar. Iter- ative Type Analysis and Extended Message Splitting: Optimiz- ing \nDynamically-Typed Object-Oriented Programs. SZGPLAN Notices, 25(6): 150-164, June 1990. In Proceedings \nof the ACM SIGPLAN 90 Conference on Programming Language Design and Implementation. [Chambers 921 Craig \nChambers. Object-Oriented Multi-Methods in Cecil. In 0. Lehrmann Madsen, editor, Proceedings ECOOP 92, \nLNCS 615, pages 33-56, Utrecht, The Netherlands, June 1992. Springer-Verlag. [Chambers 931 Craig Chambers. \nThe Cecil Language: Specifica- tion and Rationale. Technical Report TR-93-03-05, Depart-ment of Computer \nScience and Engineering. University of Washington, March 1993. [Chambers et al. 951 Craig Chambers, Jeffrey \nDean, and David Grove. A Framework for Selective Recompilation in the Pres- ence of Complex Intermodule \nDependencies. In 17th Interna-tional Conference on Software Engineering, Seattle, WA, April 1995. [Chambers \net al. 961 Craig Chambers, Jeffrey Dean, and David Grove. Whole-Program Optimization of Object-Oriented \nLan-guages. Technical Report TR-96-06-02, Department of Com- puter Science and Engineering. University \nof Washington, June 1996. [Chien 931 Andrew A. Chien. Concurrent Aggregates (CA): Sup-porting Modularity \nin Massively-Parallel Programs. MIT Press, Cambridge, MA, 1993. [Click&#38;Cooper 951 Cliff Click and \nKeith D. Cooper. Combining Analyses, Combining Optimizations. ACM Transactions on Programming Languages \nand Systems, 17(2): 18 l-l 96, March 1995. [Dean et al. 95a] Jeffrey Dean, Craig Chambers, and David \nGrove. Selective Specialization for Object-Oriented Languag-es. SIGPLAN Notices, pages 93-102, June 1995. \nIn Proceed-ings of the ACM SIGPLAN 95 Conference on Programming Language Design and Implementation. [Dean \net al. 95b] Jeffrey Dean, David Grove, and Craig Cham-bers. Optimization of Object-Oriented Programs \nUsing Static Class Hierarchy Analysis. In Proceedings ECOOP 9.5, Aar- hus, Denmark, August 1995. Springer-Verlag. \n[Deutsch &#38; Schiffman 841 L. Peter Deutsch and Allan M. Schiff- man. Efficient Implementation of the \nSmalltalk- System. In Conference Record of the Eleventh Annual A CM Symposium on Principles of Programming \nLanguages, pages 297-302, Salt Lake City, Utah, January 1984. [Diwan et al. 961 Amer Diwan, Eliot Moss, \nand Kathryn McKin-ley. Simple and Effective Analysis of Statically-typed Object-Oriented Programs. In \nOOPSLA 96 Conference Proceedings, San Jose, CA, October 1996. [EDG] C++ Front End 2.28 Provided by Edison \nDesign Group, Inc. http://www.edg.com. [Fernandez 951 Mary Fernandez. Simple and Effective Link-time \nOptimization of Modula-3 Programs. SIGPLlN Notices, pages 103-I 15, June 1995. In Proceedings of the \nACM SIGPL4 N 95 Conference on Programming Language Design and Implemen- tation. [Goldberg &#38; Robson \n831 Adele Goldberg and David Robson. Smalltalk-80: The Lanaguge and its Implementation. Addis-ion-Wesley, \nReading, MA, 1983. [Gosling et al. 961 James Gosling, Bill Joy, and Guy Steele. The Java Language Specification. \nAddison-Wesley, Reading, MA, 1996. [Grove et al. 951 David Grove, Jeffrey Dean, Charles Garrett, and \nCraig Chambers. Profile-Guided Receiver Class Prediction. In OOPSU 95 Conference Proceedings, pages 108-123, \nAustin, TX, October 1995. [Holzle &#38; Ungar 941 Urs Holzle and David Ungar. Optimizing Dynamically-Dispatched \nCalls with Run-Time Type Feed-back. SIGPLAN Notices, 29(6):326-336, June 1994. In Pro-ceedings of the \nACM SIGPLAN 94 Conference on Programming Language Design and Implementation. [Holzle et al. 911 Urs Holzle, \nCraig Chambers, and David Ungar. Optimizing Dynamically-Typed Object-Oriented Languages With Polymorphic \nInline Caches. In P. America, editor, Pro-ceedings ECOOP 91, LNCS 512, pages 21-38, Geneva, Swit- zerland, \nJuly 15-19 1991. Springer-Verlag. [JDK] Java Development Kit. Sun Microsystems Inc. http://ja-va.sun.coml. \n[Johnson 881 Ralph Johnson. TS: AN Optimizing Compiler for Smalltalk. In Proceedings OOPSL4 88, pages \n18-26, Novem-ber 1988. Published as ACM SIGPLAN Notices, volume 23, number 11. [Lea 901 Doug Lea. Customization \nin C++. In Proceedings of the 1990 Usenix C++ Conference, San Francisco, CA, April 1990. [Nelson 911 \nGreg Nelson. Systems Programming with Modula-3. Prentice Hall, Englewood Cliffs, NJ, 1991. [Pande &#38; \nRyder 941 Hemant D. Pande and Barbara G. Ryder. Static Type Determination for C++. In Proceedings of \nSixth USENIX C++ Technical Conference, 1994. [Plevyak &#38; Chien 941 John Plevyak and Andrew A. Chien. \nPre-cise Concrete Type Inference for Object-Oriented Languages. In Proceedings OOPSLA 94, pages 324-340, \nPortland, OR, October 1994. [Schaffert et al. 851 Craig Schaffert, Topher Cooper, and Carrie Wilpolt. \nTrellis Object-Based Environment, Language Refer-ence Manual. Technical Report DEC-TR-372, Digital Equip-ment \nCorporation, November 1985. [Shao &#38; Appel95] Zhong Shao and Andrew Appel. A type-based compiler foor \nStandard ML. SIGPLAN Notices, pages 116-129, June 1995. In Proceedings of the ACM SIGPLAN 9.5 Con- ference \non Programming Language Design and Implementa-  tion. [SRC] DEC SRC Modula-3 Implementation. Digital \nEquipment Corporation Systems Research Center. http://www.re-search.digital.com/SRC/modula-3/html/home.html. \n[Stroustrup 871 Bjarne Stroustrup. Multiple Inheritance for C++. In In Proceedings of the European Unix \nUsers Group Confer-ence 87, pages 189-207, Helsinki, Finland, May 1987. [Stroustmp 911 Bjarne Stroustrup. \nThe C++ Programming Lun-guage (second edition). Addision-Wesley, Reading, MA, 1991. [Tarditi et al. 961 \nDavid Tarditi, Greg Morrisett, Perry Cheng, Chris Stone, Bob Harper, and Peter Lee. TIL: A Type-Directed \nCompiler for ML. SIGPLAN Notices, pages 181-192, May 1996. In Proceedings of the ACMSIGPLAN 96 Conference \non Programming Language Design and Implementation. [Tjiang &#38; Hennessy 921 Steven W. K. Tjiang and \nJohn L. Hen- nessy. Sharlit -A Tool for Building Optimizers. SIGPLANNo-rices, 27(7):82-93, July 1992. \nIn Proceedings of the ACM SIGPLAN 92 Conference on Programming Language Design and Implementation. [Ungar \n&#38; Smith 871 David Ungar and Randall B. Smith. Self: The Power of Simplicity. In Proceedings OOPSLA \n87, pages 227-242, December 1987. Appendix A Raw Data Table 3: Execution Time (seconds) / Program ( \nnative base ( inline intra 28.90 32.75 I 32.25 1I ,. _.. m3fe 21.90 22.78 22.91 1 22.28 1 21 jo l,!,;s; \n@i f ixx 0.86 0.92 0.80 0.79 0.70 ;,jy< ,; + ktsim 79.70 85.49 15.49 76.27 eon 76.22 82.60 10.54 65.39 \nPOW I 9.75 I 13.56 ( 13.00 I 13.13 I 1 ixx-av 0.88 1 0.94 1 0.83 1 0.85 1 0.7 1 Table 4: Dynamic Number \nof Message Sends (x1000) ( I Program 1 I native 1 I base ( I inline / I intra ( I izg~ / I Ef tra+ IA+ \nFxh intra+ profile intra+ CHA+ profile intra+ CHA+ exh+ profile i instr sched I typecheck I vortex 1,736 \nt 798 prover I 1,255 1 1,255 m3fe 564 564 564 564 564 /xx 96 96 96 96 56 I :.s;ii iii:iF ( I ktsim I \n13,070 I 13,070 I 13,070 I 13,070 I 1,041 \\ .I:;: ?iiilr, I eon 7,401 7,401 7,401 7,401 (jgfj1 1 , \ng;$:; ,_,I.__ _ ,; perky 3.700 3.700 3.700 3.697 1 ixx-av ) 387 1 387 1 387 ) 386 1 56 I ktsim-av 1 336,276 \n1 336,276 1 336,276 1 336,276 1 7,883 I_ : ::; I>, 4 24,729 1 6,697 1,. 8 ,:   \n\t\t\t", "proc_id": "236337", "abstract": "Previously, techniques such as class hierarchy analysis and profile-guided receiver class prediction have been demonstrated to greatly improve the performance of applications written in pure object-oriented languages, but the degree to which these results are transferable to applications written in hybrid languages has been unclear. In part to answer this question, we have developed the Vortex compiler infrastructure, a language-independent optimizing compiler for object-oriented languages, with front-ends for Cecil, C++, Java, and Modula-3. In this paper, we describe the Vortex compiler's intermediate language, internal structure, and optimization suite, and then we report the results of experiments assessing the effectiveness of different combinations of optimizations on sizable applications across these four languages. We characterize the benchmark programs in terms of a collection of static and dynamic metrics, intended to quantify aspects of the \"object-orientedness\" of a program.", "authors": [{"name": "Jeffrey Dean", "author_profile_id": "81100248818", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, Washington", "person_id": "PP39034418", "email_address": "", "orcid_id": ""}, {"name": "Greg DeFouw", "author_profile_id": "81100059206", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, Washington", "person_id": "P99371", "email_address": "", "orcid_id": ""}, {"name": "David Grove", "author_profile_id": "81100575938", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, Washington", "person_id": "PP39049261", "email_address": "", "orcid_id": ""}, {"name": "Vassily Litvinov", "author_profile_id": "81100612632", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, Washington", "person_id": "PP31049952", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, Washington", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/236337.236344", "year": "1996", "article_id": "236344", "conference": "OOPSLA", "title": "Vortex: an optimizing compiler for object-oriented languages", "url": "http://dl.acm.org/citation.cfm?id=236344"}