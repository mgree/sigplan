{"article_publication_date": "10-01-1996", "fulltext": "\n A Flexible Operation Execution Model for Shared Distributed Objects* Saniya Ben Hassen Irina Athanasiu \nHenri E. Bal Vrije Universiteit Polytechnical University Vrije Universiteit Amsterdam Bucharest Amsterdam \nsaniya@cs.vu.nl irina@cs.pub.ro balQcs.vu.nl Abstract Many parallel and distributed programming mod- \nels are based on some form of shared objects, which may be represented in various ways (e.g., single-copy, \nreplicated, and partitioned objects). Also, many different operation execution strute- gies have been \ndesigned for each representation. In programming systems that use multiple rep- resentations integrated \nin a single object model, one way to provide multiple execution strategies is to implement each strategy \nindependently from the others. However, this leads to rigid systems and provides little opportunity for \ncode reuse. In- stead, we propose a flexible operation execution model that allows the implementation \nof many different strategies, which can even be changed at runtime. We present the model and a distributed \nimplementation of it. Also, we describe how var- ious execution strategies can be expressed using the \nmodel, and we look at applications that ben- efit from its flexibility. Introduction Shared objects \nhave become a popular model for parallel and distributing programming. From a *This research was supported \nin part by a PIONIER grant from the Netherlands Organization for Scientific Re-search (N.W.O.) and by \nthe TEMPUS Project DISCO. permission b make digital/hard copy of part or all Of this work for Personal \nor classroom use is granted without fee provided that COpi?s are not made or distributed for profit or \ncommercial advantage, $e cppyrlght noti@% the title of the publication and its date appear, and notIce \n1s given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, reqUireS prior specific Permission and/or a fee. COPSLA 96 CA, USA Q 1996 \nACM 0-89791-788-W96/0010...$3.50 user s point of view, a shared object is accessed as if it were stored \nin a shared memory common to all processors. Programming systems use many different representations to \nefficiently implement this simple view on a distributed-memory system. In the simplest case, an object \nis just stored on one machine and accessed over the network by other machines. This approach is used \nfrequently in distributed operating systems that are based on the client/server model [31] and in many \ncon-current object-oriented languages. More complicated methods are also used to im-plement shared objects \nefficiently. Several sys-tems replicate shared objects to reduce the com-munication overhead [4, 22, \n241. Other systems partition objects over multiple machines and ex-ecute operations on these partitions \nin a data-parallel way [12, 331. In addition to the three different representa-tion schemes (single-copy, \nreplication, partition-ing), many different execution strategies exist for operations on shared objects. \nAn execution strat-egy determines how an operation is executed to guarantee the appropriate operation \nand consis-tency semantics. In general, several execution strategies exist for each of the three representa-tion \nschemes. With replicated objects, for exam-ple, a read-only operation can be executed locally. An operation \nthat changes the object can either update all copies or invalidate all but one copies. Likewise, for \npartitioned objects there are many different strategies. In fact, even for the simplest representation \n(single-copy), there is a choice be-tween sending the operation to the object s site (remote object invocation) \nor moving the object to the caller s site (object migration). With the advent of this large variety of \nim-plementation methods for shared objects, it be-comes important to develop flexible models that can \nexpress many representation schemes and exe-cution strategies. In contrast, most programming systems \nhave representation schemes and execu-tion strategies built in, and new ones can only be added by significant \nchanges to the compiler and the runtime system. Flexibility is particularly important for systems that \nintegrate multiple representation schemes and execution strategies in one model. For ex-ample, in an \nearlier paper we have shown how to integrate single-copy objects and replicated ob-jects transparently \nto the user [3]. Other sys-tems integrate partitioned objects with replicated and single-copy objects, \nresulting in powerful pro-gramming models that support both task and data parallelism [12, 331. Implementing \nsuch sys-tems on top of a flexible framework is much easier than implementing each strategy separately \nfrom scratch. Several flexible execution models are presented in the literature, but they often focus \non single-copy (client-server) models, where a client re-quests a service from a server [lo, 261. In \nthis paper, we discuss an execution model that is de- signed to support single-copy, replicated, and \npar-titioned objects, as well as many different exe-cution strategies for each of these representation \nschemes. The model was developed with two goals in mind. First, it is flexible and eases the addition \nof new execution strategies. Second, it allows ex-ecution strategies to be modified at runtime, de-pending \non the dynamic behavior of objects and without changing the code of the operations or the objects. We \nhave made a prototype implementation of the execution model on top of the Amoeba1 dis- tributed operating \nsystem [30]. The most im- portant component of the implementation is a runtime system (RTS) called Hawk, \nwhich is in- tended to support coarse-grained and medium- grained parallel applications. Hawk provides \nprimitives for object creation, partitioning, repli- URL http://www.am.cs.vu.nl/ cation, and invocation. \nHawk is used together with the Orca RTS to support mixed task-and data-parallelism. The prototype supports \npas-sive objects; parallelism is expressed using pro-cesses (for task-parallelism) and data-parallel \nop-erations. Also, the prototype does not support object-oriented features like inheritance and dy-namic \nbinding, and neither does it support fault-tolerant (fail-safe) applications. In addition to the Hawk \nRTS, we have built a prototype compiler for our model by extending the Orca compiler with support for \npartitioned objects. The remainder of the paper is organized as fol-lows. Section 2 briefly describes \nthe shared object model we use. Section 3 gives an overview of the operation execution model of Hawk. \nSection describes the implementation of the model. In Section 5, we show how the model can be used to \nimplement operations on shared objects. Sec-tion 6 illustrates how the modification of execu-tion strategies \nat runtime can improve the perfor-mance of an application and increase the poten-tial for code reuse. \nSection 7 gives an overview of related work. Finally, Section 8 provides conclu-sions and future work. \n  2 Shared Objects in Hawk In Hawk, a shared object encapsulates shared data structures and the operations \non the data, so it is an instance of an Abstract Data Type (ADT). The objects in our model thus are pas- \nsive and do not contain an active thread of con- trol. Parallelism is expressed through the creation \nof multiple processes. These processes may ac- cess shared objects, even if they are on different processors \nin a distributed-memory system. Pro- cesses thus communicate by applying ADT oper- ations to shared objects. \nAs an example, Figure 1 shows the definition of a process type proc, using the Orca syntax. Processes \nof this type take an object of the pre- defined ADT IntObject as a shared (i.e., call-by- reference) \nparameter and repeatedly increment and print the integer stored in the object. Two processes of this \ntype are created, which both URL http://www.cs.vu.nl/orca. share the counter object. As should be clear \nfrom the example, processes in our model are active entities, whereas objects merely contain (shared) \ndata. Shared objects can be classified in three cat-egories: single-copy objects, replicated objects, \nand partitioned objects. The state of a single-copy object is stored on one processor, which is responsible \nfor servicing all invocations on the ob-ject. The state of a rephted object is replicated on several \nprocessors, Access to the object is per- formed on one or more of the replicas. For ex-ample, when a \nreplicated object is updated, the update may be applied to all replicas (update pro-tocol). Alternatively, \nit may be applied to one replica only while the others are invalidated (in-validation protocol). The \nstate of a partitioned object is partitioned and stored on several pro-cessors. Each partition is owned \nby one proces-sor, on which the master copy of the partition resides. Some partitions may be replicated \nand may be implemented using either an update or an invalidation protocol. Partitions can be accessed \nsequentially or in parallel. Single-copy and replicated shared objects have been extensively covered \nin the literature [2, 3, 4, 14, 22, 241. The model we use is based on that of Orca. With this model, \nan ADT oper-ation is always applied to a single object (con-sequently, calls to shared objects may not \nbe nested). This simple model is general enough for the majority of parallel applications. (Sev-eral \npeople have implemented a large number of realistic applications in Orca, such as N-body simulation, \nMonte Carlo simulation, combinato-rial search, retrograde analysis, image skeletoniza-tion, dynamic programming, \nautomatic test pat-tern generation, and many others.) The Orca model guarantees that every opera-tion \nis executed indivisibly, so the execution of concurrent operations is equivalent to some serial execution \nof these operations. In other words, the model provides sequential consistency among in-vocations from \ndifferent processes. In Figure 1, for example, if the two processes simultaneously execute increment0 \non the counter object, the processes will always get different values, since the operation is executed \nindivisibly. The Orca model can be implemented efficiently, even on machines without shared memory. The \nmost difficult issue is how to implement indivisi-ble operations on replicated objects. Several so-lutions \nto this problem exist. The solution we use is based on totally ordered group communica-tion [15]. All \noperations that update a replicated object are multicast and executed (identically) on all processors; \nthe total ordering of the multicast messages guarantees sequential consistency [15]. The Orca model and \nits implementation are de-scribed in more detail in (2, 3, 41. The partitioned objects used in this paper \nare a logical extension to the Orca model. We have de-veloped this more general model to support data-parallel \nprogramming with shared objects. Our extended system supports all three categories of shared objects \nand thereby allows mixed task-and data-parallelism using a single programming model. We describe the \nmodel briefly below; a more detailed description can be found in [12]. A partitioned object encapsulates \na data struc-ture as a multi-dimensional array of elements. The state of an object is accessed only through \neither parallel or sequential operations. Within an operation, an element is addressed by indices. As \nan example, Figure 2 shows a partitioned ob-ject grid of N by M real numbers on which a parallel operation \nsor() implements one itera-tion of Successive Over-Relaxation (SOR) with red/black partitioning [28]. \nThis operation up-dates the elements of a grid using a function of their neighbors values. It is called \nrepeatedly, as shown in Figure 2, until the grid reaches a stable state. From the user s point of view, \na purdkd oper-ation is applied to all elements of an object in parallel whereas a sequential operation \nis applied sequentially to the entire state of the object. In the example of Figure 2, sor0 is a data-parallel \noperation. During the execution of sor0, con-ceptually, there is one thread of control per ele-ment, \nin which only that element (designated by G [row, co11 ) can be updated; other elements may only be read. \nThe operation execution follows the owner-computes rule. The RTS ensures that old values of the data \nstructure are not overwrit-ten before all processors have ended applying the PROCESS proc(counter: SHARED \nIntObject); tmp: integer; BEGIN REPEAT tmp := counter$ ncremento ; # apply increment0 operation writeln(tmp); \nUNTIL tmp > 1000; END: PROCESS OrcaMainO; # the main (first) process counter: IntObject # the shared \nobject BEGIN # create processes on CPUs 1 and 2 FORK proc(counter) ON(l); FORK proc(counter) ON(Z) : \nEND;  Figure 1: Example of the interaction between processes and shared objects. OBJECT IMPLEMENTATION \ngrid[l ..N:integer,l..M:integer]; G: real;  PARALLEL OPERATION[row, co11 sor(c:color): REDUCE real \nWITH max: ax3 , diff: real; BEGIN diff := 0;  IF iscolor(row,col)=c AND row>1 AND col>l AND row<N AN \ncol<M THEN avg := (G[row-1,coll + G[row+l,col ] + G[row,col-11 + G[row,col+ll ) /4; diff := ABS(avg - \nG[row,coll); G[row,col] := G[row,coll + OMEGA * (avg - G[row,coll FI; RETURN diff; END; END; PROCESS \nOrcaMainO; SORgrid: grid[l..50,1. 001; . . . BEGIN SORgrid$init( . . . ); # initialize the grid data \nREPEAT rmaxdiff := SORgrid$sor(red); # invoke data parallel operation for bmaxdiff := SORgrid$sor(black); \n# red phase and then black phase UNTIL max(rmaxdiff,bmaxdiff) < epsilon; END. Figure 2: Example of a \nparallel operation on a partitioned object. updates on temporary copies of their elements. As in Orca, \nan operation is applied to a sin-gle object and each (entire) operation is executed indivisibly. Internally, \na single data-parallel op- eration consists of many concurrent actions (one per element) and accesses \ndata distributed over multiple machines. If multiple data-parallel op- erations are invoked simultaneously, \nthese will be executed in a way equivalent to some serial exe-cution of the given data-parallel operations. \nSo, our model guarantees sequential consistency se-mantics between multiple operations, and owner-computes \nsemantics within a single data-parallel operation. For efficiency, the programmer can group the elements \nof a partitioned object into partitions. These partitions are in turn distrib&#38;ed over multi-ple processors. \nThe distribution assigns a unique owner to each partition. The owner of a parti-tion retains a master \ncopy of it that is always consistent; copies of that partition on other pro-cessors are secondary copies. \nAs an example, Fig-ure 3 shows how the SORgrid object can be par-titioned row-wise and distributed in \nblocks over three processors. The processors are allocated an equal number of consecutive rows. Common \ndis-tributions such as block and cyclic distributions (independently along different dimensions) may \nbe specified using predefined directives. Others may be specified using low-level directives that distribute \npartitions arbitrarily over a set of pro- cessors. The partitioning and distribution of an object are \northogonal to its specification and implemen-tation. Consequently, they may be changed dy-namically without \nchanging the state and opera-tions of the object. The set of partitions along with all implementation \ninformation handled by each processor is called a fragment, just as a copy of a replicated object is \ncalled a replica. (The word fragment was first used in [20].) A suitable partitioning and distribution \nof an object gives the user some control over data place-ment and load balancing without having to worry \nabout communication and synchronization. Addi-tionally, it increases the granularity of operations and \ndata transfers. During the execution of a par- allel operation, each processor applies the opera- User \ns Directives SORgrid: grid[6] [6]; SORgrid$$partition(ROWWISE); SORgrid$$distribute(BLOCK) ;  Grid \nDistribution Row Owner 0 PO 1 PO 23 PlPl 4 P2 5 P2 Figure 3: Row-wise partitioning and block distribu-tion \nof a two dimensional partitioned object. There are 6 rows (therefore, 6 partitions) and 3 processors. \nEach processor is allocated 2 consecutive rows. tion to the partitions it owns. If a processor needs \nto read an element it does not own, it must re-quest its (old) value from the owner. Rather than transferring \none element at a time, the RTS trans-fers the entire partition the element belongs to. (HPF compilers \nlike Paradigm [5] achieve a some- what similar effect using an optimization called message vectorization.) \nThe RTS transfers partitions between proces-sors transparently to the user. The RTS takes care of reducing \nconsistency checking overhead, resolving data dependencies, and overlapping communication and computation \nto hide commu-nication latency 1111. To summarize, our object model provides single-copy and replicated \nobjects (as in Orca) as well as partitioned objects. It supports task paral-lel programming through the \ncreation of multiple processes that communicate through shared ob-jects (see Figure 1). In addition, \nit supports data parallel programming through parallel operations on partitioned objects (see Figure \n2).  3 A Flexible Model for Opera-tion Execution The goal of our model is to support multiple ob-ject \nrepresentation schemes and execution strate-gies. For example, Hawk uses different execution strategies \ndepending on whether an operation is sequential or parallel. Execution strategies may also differ depending \non the semantics, replication scheme, and usage of the object invoked. As an- other example, for an operation \nthat simply reads the state of a fully replicated object, the RTS ex-ecutes the operation on the local \nreplica. To obtain a flexible execution model, the key idea is to distinguish multiple phases during \nthe execution of an operation. Each phase may be im- plemented using one of several mechanisms. We have \nidentified five consecutive phases, each with a specific function: invocation, prefetching, execution, \nreturn, and commit. An execution strategy differs from another by the implementa-tion of one or more \nof these phases. Below, we explain the function of each phase in the execu-tion model. These functions \nare summarized in Table 1. The first phase is executed on the caller s side. The remaining phases are \nexecuted on one or more processors holding a fragment or a replica of an object. 1. Invocation. This \nphase determines on which processors an operation must be ex-ecuted. For example, when an operation of \na partitioned object is called, one or more processors holding a fragment of the object may need to be \nnotified of the call. The invocation could be carried out locally. It could also be sent over the network \nto other processors. In the latter case, this phase also determines which communication pro-tocol to \nuse. For example, for a replicated object, an invocation message may be mul-ticast to all processors \nholding a replica of the object using totally ordered group com-munication [15]. For a single-copy object, \nit may use point-to-point communication. All processors that are notified of the invocation carry on \nwith the execution of the operation, and execute the subsequent phases. 2. Prefetching. The execution \nof an opera-tion may require data that are not available on the invoking processor. Accesses to these \ndata thus define a set of dependencies. The simplest way to resolve such dependencies is to fetch remote \ndata on demand, during the execution phase of the operation. However, for many applications it may be \nmore efficient to prefetch data. For example, computation and communication can sometimes be over-lapped \nby issuing a nonblocking request to read remote data before the execution phase begins. In other cases, \nit is even possible to have the owner of the data send the data in advance, without using request messages \n(sender-initiated transfer). If many proces-sors need the data, collective communication can be used \nto decrease the communication overhead. So, in many cases it is desirable to fetch consistent copies \nof data before the execution begins. As we will explain in Sec- tion 4, the decision of which data to \nprefetch for a given object is taken by the RTS, based on information provided by the compiler or by \nthe user. 3. Execution. The execution phase consists of two parts: data consistency checking and actual \nexecution of the operation. The RTS checks whether the data it is about to access are locally consistent. \nIf the data are not con-sistent and are not being transferred, it ini-tiates their transfer from their \nowner. When and how partitions are fetched differs from one implementation of the execution phase to \nanother. The RTS applies the operation on the local fragment or replica depending on the semantics of \nthe operation. 4. Return. The return phase also consists of two parts: preparing the final result and \nno-tifying the caller of its availability. Once the operation has been executed, the partial re-sults \ngenerated by the individual executions must be combined into the final result and re-turned to the calling \nprocess. The return val-ues provided by the processors during a par-allel operation may be combined using \ngather  Phase Description Invocation Determines which processors receive the invocation of an operation \nand how. Prefetching May prefetch the data needed for the execution of an operation. Execution Executes \nthe actual code of an operation. May fetch some data. Return Determines how the results are prepared \nand returned by the processors. Commit Validates and invalidates the different parts of an object. Table \n1: Summary of the phases of operation execution. or reduce operations. The result of a sequen- tial \noperation is always prepared by one pro-cessor and returned to one processor. During the return phase, \nthe processors use a com-bining function provided by the user. 5. Commit. Once an operation is executed, \nHawk must ensure that any updates on the state of the object are performed atomically and must prevent \naccess to stale data. The commit phase determines how processors must be synchronized at the end of an \nop-eration to ensure its atomicity. It also de-termines which parts of the object s state must be validated \nor invalidated. For exam-ple, for a parallel operation, after the return phase, all processors reach \na barrier. When they leave the barrier, each one commits the changes on the partitions it owns and invali-dates \nthe replicas of other partitions it holds. Different update protocols can also be imple- mented during \nthis phase. In Hawk, there are various implementations for each phase. Each phase is carried out indepen-dently \nof the others by a handler. The same han-dler may be used in the implementation of sev-eral strategies. \nAdditionally, the implementation of one phase may be changed by substituting one handler for another \nwhile the other handlers of the execution strategy need not always be changed. With the model just described, \nHawk allows a user to modify an execution strategy without modifying the operations or the object. This \nis useful if an execution strategy affects the per-formance of an application. In such a case, the user \nmay choose an execution strategy that exe-cutes an operation more efficiently. Furthermore, new execution \nstrategies can be added easily to the system by reusing the mechanisms available for existing strategies \nor by providing new han-dlers. There are, however, some restrictions in the way phase handlers are combined. \nHawk cur-rently does not check whether handlers are used consistently, but it could do so at least for \nthe handlers it provides. This is discussed in the next section. 4 Implementation of the Model Hawk \nincludes a set of predefined handlers that are sufficient to implement single-copy, replicated, and partitioned \nobjects. Therefore, it uses a sin-gle framework to support task-and data-parallel programming using shared \nobjects. In this sec-tion, we first give an overview of Hawk. Second, we describe the different handlers \nthe system cur-rently provides for each phase of an operation ex-ecution. Finally, we briefly discuss \nthe prototype implementation. 4.1 Overview Hawk keeps track of classes, objects, and opera-tions in descriptors \nstored by each processor in a fragment (see Figure 4). Each descriptor, dis-cussed below, is given a \nsystem-wide unique iden-tifier and is replicated on all processors. A class descriptor contains the shape \nof a par-titioned object (number of dimensions and size of the elements) and an operation descriptor \nfor each one of its operations. Replicated and single-copy objects are implemented as particular cases \nof partitioned objects. A replicated object is a one- dimensional partitioned object where each parti-tion \nimplements a replica. A single-copy object is a one-dimensional array with only one element. OPl Phase \nhandlers Operation code PDG . . . Size of dimensions Partitioning  State Distribution State Opl descriptor \n(actual) d . . . Op2 descriptor (actual) - I . . . I Figure 4: The set of descriptors for classes, \nobjects, and operations on each processor constitutes a fragment. An operation descriptor contains pointers \nto will see, the PDG is used by the RTS to decide five handlers, each corresponding to one of the which \npartitions to prefetch. We briefly discuss execution phases described in the previous sec-how the PDG \nis created for static patterns. tion. The descriptor also contains a pointer to the Recall that our model \nallows the programmer operation code, i.e., a procedure that implements to specify (or even change) the \npartitioning of the body of the operation. For objects that are an object dynamically, so the compiler \ndoes not not partitioned and for sequential operations, the know how an object will be partitioned. Hence, \noperation code is just the body of the operation. the compiler may be able to determine the depen- For \na parallel operation, the operation code ap-dencies between elements, but not between parti- plies the \nbody of the operation to every element tions. As a simple example, consider the opera- of a partition. \nFor example, the operation code of tion sor 0 in which the compiler can determine sor.0 in Figure 2 consists \nof one procedure that that, to update an element, the values of its four applies the operation to each \nelement of a parti-neighbors are needed. The compiler conveys this tion, as shown in Figure 5. (This \ncode is generated information to the RTS. The RTS knows how the by the Orca compiler, but simplified \nto increase grid object is partitioned, so it can use the com- the readability.) piler information to \nconstruct the PDG at run time [ll]. For example, if the grid contains sixFinally, an operation descriptor \ncontains a Par- rows and is partitioned row-wise, the RTS buildstition Dependency Graph (PDG) [ll], which \nspec-the PDG shown in Figure 6. This PDG showsifies the dependencies between the partitions of that each \nrow (partition) depends on its neigh-an object during the execution of the operation. boring rows. The \nPDG is computed when an ob-The PDG is generated by the compiler if the op-ject is instantiated or whenever \nits partitioning iseration uses a simple, static access pattern. For changed. more complicated, dynamic \npatterns we also al-low the programmer to provide the PDG. As we PDGs are a variant of the inspector/executor \nvoid SorOpCode(instance_p instance, int part-num, void **args) { /* args[O] contains the first argument. \nAt the end of the execution of sor(), args[l] contains the return value. */ . . . color = (boolean *)args[O]; \nmaxdiff = (double *)args[l]; *maxdiff = 0.0; /* Initialize return value. */ for each element [row,coll \nin partition part-num if iscolor(row,col)=(*color) &#38;&#38; row>1 &#38;&#38; colzl &#38;&#38; row<N \n&#38;&#38; col<M { instance->temp-G[row,col] = (instance->G[row-l,col] + G[row+l,col] + instance->G[row,col-11 \n+ G[row,col+l])/4.0; diff=fabs(instance->temp_G[row,col] -instance->G[row,col]); /* max returns the maximum \nvalue in the first argument */ max(maxdiff,&#38;diff); Figure 5: Operation code for the parallel operation \nsor0. model [27]. In the latter model, the execution phase of a data-parallel statement is preceded \nby an identification of its communication require-ments: the inspector generates a list of elements to \nbe sent and received by each processor. Dur-ing the execution of the statement, the executor fetches \nremote data using the information gen-erated by the inspector. The major difference between this model \nand ours lies in the granu-larity of data transfers. In our model, only en-tire partitions can be transfered \nfrom one proces-sor to another and a processor may hold mul-tiple partitions. In systems using the original \ninspector/executor model, such as CHAOS [27], each processor holds exactly one partition and el-ements \nmay be transfered individually. At the time an object is instantiated, the RTS creates an object descriptor \nin which it stores a pointer to the class descriptor, the size of each di-mension of the object, the \npartitioning, the distri-bution, the status of each partition (whether the local value is consistent \nor not), and a copy of the default operation descriptor for each operation de-fined on the object. In \nthe current prototype, each processor allocates a contiguous area of memory large enough for the entire \nstate of the object. The advantage of this approach is that it simpli- fies the address arithmetic needed \nfor accessing el-ements. The disadvantage is that it wastes mem-ory space and thus may be inappropriate \nfor ap-plications with large data sets. This problem may be alleviated by using virtual memory (since \nun-used parts of the object s state would be swapped to disk), although the amount of available swap \nspace would still limit the size of the data set. Also, virtual memory is not supported on all par- allel \nplatforms. Yet another alternative would be to only allocate memory regions for the partitions that are \nneeded locally, and address these regions using a table of pointers. This would require at least an extra \nlevel of pointer indirection during address computations. The handlers in the operation descriptors of \nan object may be changed dynamically and atomi-cally by calling a primitive change-handlerso. A handler \nmay be defined in the RTS or may be user-defined. New execution strategies can thus be added to the runtime \nsystem. For example, the following call atomically changes the prefetching handler of the operation sor \n0 on object SORgrid to a NOOP built-in handler. The other handlers are not changed, as indicated by the \nNULL argu-ments. handler-p handlers [5] ; handlers[INVOCATION] = NULL; handlers [PREFETCHING]= NOOP; \nhandlers [EXECUTIONI = NULL; handlers [RETURN] = NULL; handlers CCOMMITI = NULL; change-handlers(SDRgrid, \nsor, handlers); Grid Partitioning and Distribution PDG for sor() cm P2 P3 P4 P5 Processor 0 Figure \n6: The PDG for SOR with The call to change-handlers 0 is broadcast to all processors using totally ordered \ngroup com-munication to guarantee atomicity of the call and consistency of the fragments. After an object \nhas been partitioned and dis-tributed, Hawk spawns invocation threads on ev-ery processor holding a fragment \nof the object. The threads are connected to each other and to client processes by communication channels \nto send invocation messages and partitions over the network. A client process isslles an operation by \ncalling its invocation handler and blocks until the operation ends. When an invocation thread receives \nan invocation, it calls the handlers for the prefetching, execution, return, and com-mit phases specified \nin the operation descriptor sequentially. Each handler might require some information that is either \nstored in the different descriptors linked to the object or given as an ar-gument to the handler. 4.2 \nPhase handlers All handlers available in Hawk are summarized in Table 2. Below, we give a brief description \nof each one. If during any of the five phases, no action needs to be taken by a processor, a NOOP handler \nmay be called. Invocation Phase. An invocation can be LO-CAL, UNICAST t0 one processor, or MULTICAST \nt0 all processors. (All communication protocols de-scribed in this paper are reliable protocols.) A III \nProcessor 1 0 Processor 2 a row-wise partitioned grid. LOCAL invocation handler just calls the remaining \nphase handlers locally. No invocation message is sent over the network. An invocation sent to a re- mote \nprocessor is UNICAST. The object identifier, the identifier of the operation called, and its argu- ments \nare marshaled into a message that is sent point-to-point to the destination processor. At the receiver \ns side, the invocation is unmarshaled and the remaining phase handlers are called. The MULTICAST invocation \nhandler marshals the invo-cation into a message and multicasts it to all pro- cessors using a totally \nordered multicast protocol. At the receivers side, the message is unmarshaled and the remaining phase \nhandlers are called. Prefetching Phase. There are three protocols our RTS uses to transfer partitions \nbetween pro-cessors: a receiver-initiated protocol, a sender-initiated protocol, and a collective protocol \n[ll]. With the receiver-initiated protocol, the proces-sor that needs to access a remote partition ini-tiates \nthe transfer. We use this strategy, for ex-ample, to fetch a new replica of an object whose local copy \nhas been invalidated. With the sender-initiated protocol, the owner of the partition initi-ates the transfer. \nThe SENDER handler is shown in Figure 7. It traverses the PDG of an oper-ation and sends all partitions \nneeded by remote processors to execute the operation. With this sender-initiated protocol, a processor \nneed not send requests to resolve dependencies of an opera-tion. Instead, it simply blocks until the \npartitions are sent by their respective owners. If multiple 1 Phase Handlers / Implementation of the \nphase Invocation Prefetching Execution Return Commit LOCAL UNICAST MULTICAST SENDER RECEIVER COLLECTIVE \nSEQ-NONREP SEQ-REP PAR-BLOCKING PAR-NONBLOCKING UNICAST LOCAL COLLECTIVE COMMIT-ALL COMMIT-OWNED INVALIDATE \nTable 2: Summary  LoCalcall Remote call Totally ordered multicast call Sender-initiated transfer Receiver-initiated \ntransfer Collective transfer Sequential operation executed on one processor Sequential operation only \nexecuted on all processors Parallel operation, no overlap of camp. and comm. Parallel operation, overlap \nof camp. and comm. Only callee returns result Result prepared and needed locally Result returned collectively \nCommits all partitions on each processor Commit partitions owned by each processor All processors except \nthe callee invalidate all partitions. of the handlers provided by Hawk. processors need the same partition, \nthe SENDER handler may multicast the partition rather than send it point-to-point multiple times. Finally, \nthe collective protocol is useful to applications that transfer a large part of the object state. Execution \nPhase. The handler used in the ex-ecution phase depends on the semantics of the op-eration. For a sequential \noperation, there are two different execution strategies: SEQ-REP (for se-quential opersition and replicated \nexecution) and SEQ-NONREP (for sequential operation and non-replicated execution). The SEQ-REP handler \nfirst checks if the dependencies have been resolved dur-ing the prefetching phase. If they have not, \nit initiates the transfer of the partitions it needs to access and blocks until they are received. Second, \nit applies the operation code once. All processors execute this phase of the operation. The SEQ-NONREP \nhandler executes the same steps as the SEQ-REP handler. However, only one processor (the sender of the \ninvocation) executes the code of the operation. For a parallel operation, there are two exe-cution strategies \ncalled PAR-NONBLOCKING and PAR-BLOCKING. The PAR-NONBLOCKING han-dler overlaps computation and communication \nto hide the cost of transferring partitions over the network. It loops twice over the partitions PO,Pl,PZ,.*. \nowned by a processor (see Figure 7). During the first iteration, it executes the code of the operation \non the partitions pi that have only local dependencies (i.e., those partitions pi for which the processor \nowns all partitions pi depends on). It marks other partitions as unprocessed. During the second iteration, \nfor each unprocessed partition pi, the handler waits until its dependen- cies are locally consistent \n(it assumes that their transfer has been initiated during the prefetch-ing phase by the owner of the \npartitions) and then executes the operation code on pi. The PAR-BLOCKING handler does not overlap communica-tion \nand computation: it iterates once over the partitions po,pl,p2, . . ., in that order, and blocks waiting \nfor the dependencies to be resolved before executing the operation on each pi. Return Phase. A return \nphase can be han-dled by one processor or collectively. In the UNI-CAST return handler, only the callee \nprepares the result of the operation and returns it to the caller. A UNICAST handler may be used, for \nexample, for single-copy objects. The LOCAL handler im-plements a particular case of the previous strat-egy \nwhere the results are prepared locally and re-turned locally. During a COLLECTIVE return, all void SENDER(int \nsender, instances instance) ( for every owned partition p of instance for every partition p-i that depends \non p if owner(p-i)!=me and p was not already sent to owner(p-i) /* owner(p) and owner of (p-i) synchronize \nhere. */ /***/ send p to owner(p-i); void PAR-NONBLOCKING(int sender, instances instance, void **args) \ni for every owned partition p-i { /* The execution of this block overlaps the communication of partitions \nbetween the processors. */ if p-i has only local dependencies SorOpCode(instance, p-i, args); else mark \np-i as unprocessed; for every owned partition p-i I if p-i is marked as unprocessed { for every partition \np p-i depends on /* owner(p) and owner(p-i) synchronize here. */ /***/ if owner(p)!=me wait until p is \nlocally consistent; SorOpCode(instance, p-i, args); Figure 7: Simplified algorithms for the SENDER and \nPAR-NONBLOCKING handlers. The statements marked by a commented * show where processors synchronize when \nexchanging partitions. processors holding a fragment of the object gen-erate partial results of the operation \nand combine them using a combining function. Either one or all processors receive the result of the operation \nexecution. This COLLECTIVE handler is used for parallel operations. Commit Phase. For the commit phase, \neach operation of an object is classified as a read or a write operation. A read operation does not up-date \nthe state of the object; a write operation does update the state (and may also read the state). During \na write operation, each processor uses a temporary write copy of its partitions on which all updates \nare first carried out. The modifica-tions are committed atomically after all updates have been performed \nop the temporary copies on all processors. For write operations, either all par- titions are potentially \nupdated (sequential opera-tions) or only owned partitions are updated (par-allel operations). The COMMIT-ALL \nhandler must be executed by all processors. It first reaches a barrier. Then, it commits the changes \nof all partitions that were updated. The COMMIT-OWNED handler also reaches a barrier at start. Then, \nit commits the changes on all owned partitions and invalidates others. In the INVALIDATE handler, only \none pro-cessor (the callee) commits its partitions while other processors invalidate theirs.  4.3 Prototype \nImplementation We have implemented a prototype of Hawk on top of Amoeba [31] on a network of single-processor \nboards connected by a 10 Mbps switched Eth-ernet. The processor boards are SPARCclassic clones running \nat 50 MHz with 32 MB of RAM each. The current implementation of Hawk has some restrictions. First, it \ndoes not support opera-tions that block on guards or condition variables, which are used in some languages \n[4]. This restric-tion is due to the implementation of the model rather than to the model itself. Second, \nour model does not support object-oriented features such as inheritance and dynamic binding of op- Single-Copy \nObjects SEQ-NONREP Table 3: Implementation of single-copy objects. erations. Third, the model only supports \npassive objects and uses other constructs (task-and data-parallel constructs) to express parallelism. \nFi-nally, the RTS does not check whether the han-dlers provided by the user for a particular opera-tion \nare compatible. For example, if the invoca-tion phase of an operation is UNICAST the return phase cannot \nbe COLLECTIVE. Although Hawk could indicate some inconsistencies when built-in handlers are used, it \ncannot do so for user-defined handlers. The overhead for using a flexible model as op-posed to a more \nrigid model is small. First, it con-sists of calling the phase handlers through an ad-ditional level \nof indexing to allow dynamic mod-ification of execution handlers. Second, a more rigid model would avoid \nthe calls to NOOP han-dlers when a phase need not be implemented. In our prototype, this overhead varies \nfrom less than 7.4% for operations that require no communica-tion and synchronization between the processors \nto 1% for other operations.   5 Implementing Shared Objects Using the Execution Model In this section \nand the next we study the flexi-bility of our execution model. We first show that the model allows us \nto implement each category of shared objects discussed in Section 2. In Sec-tion 6 we will look at the \nimplementation of two applications using our model. 5.1 Single-Copy Objects The implementation of an \noperation on a single- copy object is shown in Table 3. A call to the operation is unicast to the processor \nthat holds the state of the object, say processor p, and is en- tirely executed there. There is no need \nfor data prefetching since the state of the object on p is al- ways consistent. After p ends the execution \nof the operation, it unicasts the result back to the caller. In the operation code, updates on the object \nare performed directly on the state of the object and need not use a temporary copy. Therefore, there \nis no commit of the updates. 5.2 Replicated Objects Table 4 shows how replicated objects using either \nan update or an invalidation protocol may be im- plemented. For conciseness, we will only explain the \nimplementation of write operations using an update protocol (see the last column in Table 4). The other \nstrategies can be derived in a similar way. Using an update protocol, the state of an ob-ject must always \nbe consistent on all processors holding a replica of the object. A call to a write operation on the object \nis broadcast to all proces- sors. There is no prefetching required since the state of the object is consistent \non all processors. During the execution phase, the operation code is executed on all processors. Because \nthe ob-ject is fully replicated, the processor that issued the operation must have a local copy of the \nob-ject and only that processor needs the result of the operation. Therefore, the result is computed \nand returned locally. Total ordering of invocation messages ensures that replicas are updated in the \nsame order on all processors and all updates are performed on the state of the object without us-ing \ntemporary copies. Consequently, no barrier and no commit are necessary during the commit phase. 5.3 \nPartitioned Objects Table 5 shows the strategies used most often for the implementations of operations \non partitioned objects. As in the previous section, for concise-ness, we will discuss the implementation \nof only one type of operation. Consider a write paral-lel operation. The operation must be executed Replicated \nObjects Handler Phase Invalidation Protocol r Update Protocol Read Operation Write Operation ~ Read Operation \nWrite Operation Invocation I LOCAL MULTICAST LOCAL MULTICAST Prefetching i RECEIVER RECEIVER NOOP NOOP \nExecution SEQ-NONREP SEQ-NONREP SEQ-NONREP SEQ-REP Return LOCAL LOCAL LOCAL LOCAL Commit NOOP INVALIDATE \nNOOP NOOP Table 4: Implementation of replicated objects using an update or an invalidation protocol. \nr  Phase Invocation Prefetching Execution Return Commit Sequential Read LOCAL RECEIVER SEQ-NONREP LOCAL \nNOOP  Partitioned Operations Write MULTICAST COLLECTIVE SEQ-REP LOCAL NOOP Objects Handler Parallel \nRead MULTICAST SENDER PAR-NONBLOCKING COLLECTIVE NOOP Operations Write MULTICAST SENDER PAR-EONBLOCKING \nCOLLECTIVE COMMIT-OWNED Table 5: Typical implementation of Partitioned Objects Operations. by all processors \nholding a fragment of the ob-ject. As for replicated objects, to ensure that concurrent calls to operations \non the same object are executed in the same order on all processors, the invocation is multicast to all \nprocessors us-ing totally ordered group communication. The prefetching phase uses a sender-initiated \ntrans-fer protocol (it is the most efficient in many cases). During the execution phase, each processor \nap-plies the operation code to its partitions and at-tempts to overlap communication and computa-tion. \nThe return phase is executed collectively to combine individual results generated by each processor. \nFinally, during the commit phase, ev-ery processor commits the partitions it owns and invalidates all \nothers.   Applications In this section, we look at two applications, Fast Fourier Transform (FFT) and \nthe Narrow-band Tracking Radar (NTR), that were implemented using replicated and partitioned objects. \nThe first application illustrates the benefits of chang- ing the execution strategy of an operation at \nrun-time and the performance improvement an appli-cation may gain by defining new handlers. The second \napplication illustrates how the model pro-vides more opportunity for code reuse at the ap-plication level. \n 6.1 Fast Fourier Transform FFT maps n complex values to n other complex values using a linear transformation. \nWe have im-plemented FFT by encapsulating the complex val-ues in a partitioned object matrix. The transfor-mation \nis applied by calling a parallel operation transf orm0 iteratively. Using a MULTICAST invocation, one \nprocess calls transpose0 iteratively to perform the transformation. This requires sending a message over \nthe network for each call. Multicasting the invocation message can be avoided by having all processors \nholding a fragment of the matrix ob-ject issue a collective call to the operation. Using a collective \ninvocation, there must be an implicit agreement between all processors holding a frag-ment of matrix \nto issue all the invocations on the object in the same order locally. Therefore, to implement a collective \ninvocation, the user may write a COLLECTIVE handler that calls the re-maining phase handlers locally \nand have all pro-cessors issue all invocations on the object. A col- lective invocation does not require \nsending a mes- sage over the network, thus, generates less over-head. Figure 8 illustrates the implementation \nof MULTICAST versus COLLECTIVE inVOCatiOnS. (a) MULTICAST invocation (b) Collective invocation PO Pl \nP2 P4 Pl P2 P4 Invocation Phase Local invocation on each processor i 1 1 1 pl I I I Figure 8: MULTICAST \nand COLLECTIVE invocation strategies. In (a), one processor multicasts a message to 4 processors. Then, \neach execute the remaining phases of the operation locally. In (b), there is an implicit agreement between \nthe 4 processors to issue a call to the operation collectively and start the execution of the remaining \nphases locally. During the execution phase, the overlap of communication and computation is not always \nef-fective. First, there is an overhead related to addi- tional thread switches. Second, data prefetching \nrequires a higher network bandwidth because the same amount of data needs to be transferred in a shorter \ntime [ll]. This higher bandwidth may not be available, especially if the amount of data to be transferred \nis large. The user may define a new execution handler PAR-CONTROL that at-tempts to prefetch partitions \nin a controlled man-ner, without requiring additional network band-width. More precisely, if a processor \nowns parti-tions pr,p~,. . ., while executing an operation on a partition pi, the processor transfers \nthe parti-tions needed for the execution of the operation on ~i+l. Therefore, each processor prefetches \nde-pendencies for one partition at a time rather then for all of them at once. This new handler uses \nhigh-level primitives available in Hawk to consult the dependency graph of an operation and fetch partitions. \nTable 6 shows the handlers of three execu-tion strategies we used for transform0 in FFT. The first variant \nof the program fetches all data needed for an operation during the prefetching phase and overlaps computation \nand communica-tion. The second variant does not prefetch data and does not overlap communication and \ncom-putation. The third variant prefetches only one partition at a time during the execution of the operation. \nFigure 9 shows the performance of each vari-ant of FFT on 215, 216, and 217 complex num-bers executed \non eight processors. For 216 and 217 complex numbers, full prefetching is not effec-tive and partial \nprefetching shows better perfor-mance [ll]. This behavior can be explained by the fact that FFT is a \ncommunication intensive appli-cation. For small input matrix sizes, fetching all data during the prefetching \nphase and overlapping of computation and communication decreases the transformation time. As the size \nof the matrix in-creases, the data transferred between processors becomes larger and prefetching all \ndata simulta-neously requires high bandwidth. It may even be counter-effective for large input sizes \nwhereas Three Variants of Parallel FFT Invocation COLLECTIVE COLLECTIVE COLLECTIVE Prefetching SENDER \nNOOP NOOP Execution PAR-NONBLOCKING PAR-BLOCKING PAR-CONTROL Return COLLECTIVE COLLECTIVE COLLECTIVE \nCommit COMMIT-OWNED COMMIT-OWNED COMMIT-OWNED Table 6: Possible handlers of the operation FFT. Each column \ndescribes the set of handlers used for one variant of the application. COMMIT-OWNED Table 7: Handlers \nfor the two objects in NTR. prefetching partitions one at a time still reduces the transformation time \n[ll]. Using our execution model, a compiler, the RTS, or a programmer may define a new exe-cution handler \nand change the execution strat-egy of the operation according to the size of the input matrix. The RTS \ncould even consult the PDG of the operation to determine the amount of data to be transferred by all \nprocessors to decide on the appropriate execution strategy. Changing the execution strategy at runtime \nis a relatively cheap operation in our system. The implementa-tion uses a totally-ordered group message \n(broad-cast) to inform all processors of the change. On Amoeba, this typically takes less than 2 msec \n[15]. 6.2 Narrow-band Tracking Radar This application is part of the CMU Task Paral-lel Program Suite \n[8]. It is not meant to show the advantages of using different execution strategies (this was illustrated \nin Section 6.1). Rather, we use it to show that the flexibility of the model allows us to reuse the matrix \nobject described in Section 6.1 for NTR even though the appli-cation requires a different execution strategy \nfor transf orm0. The program applies transformations to mul- tiple sets of n complex values. Each input \nset is encapsulated in the matrix object described above. The program consists of multiple work-ers, \neach one applying FFT in parallel on a dis-tinct matrix. A single-copy ticket object keeps track of which \nset has been processed. Whenever a worker becomes idle, it requests the number of the next input set \nto be transformed by invoking the ticket object. The handlers used for the execution of opera- tions \non ticket and matrix are shown in Table 7. Although we use the same matrix object as in the previous \napplication, the execution strategies for transform0 are different. In FFT, the calls to transform0 are \nissued collectively, whereas in this application they are multicast by one pro-cessor to all processors \nholding a fragment of the matrix. A collective invocation may be used only if the order in which the \ninvocations on the matrix are issued is deterministic, which is not the case with this application, because \ncalls to the matrix objects are issued by multiple worker processes that are synchronized by another \nobject. There-fore, the processors executing the operation can-not determine in which order the collective \ninvo-cations must be issued. This application shows 20000 zi5 15000 8 .v,-z E E .-t-% 5 ::c? 10000 5000 \n0 Input Figure 9: Performance of the FFT program that our model allows a programmer to reuse the same \nobject in different applications and adapt the execution strategy or the object s operations depending \non the characteristics of the applica-tions.  7 Related Work There are three categories of systems that \nuse exe-cution models related to ours: systems that imple-ment single-copy objects, replicated (or cached) \nobjects, and distributed objects. Below, we dis-cuss each one in turn and compare it with our execution \nmodel. 7.1 Execution Models for Single-Copy Objects Several early systems focus on the implementation \nof Remote Procedure Calls (RPCs) [6] or single-copy object invocation (for example, 17, 181). In these \nsystems, RPC or object invocation is per- formed in several steps: marshaling, invocation, execution, \nand return. The systems described, however, were not designed to support modifica-tion of the implementation \nof RPC or invocations dynamically. I Full Prefetch ._.. No Prefetch ------ Partial Prefetch __  2 +16 \n2**17 Set Size using several prefetching strategies. Shapiro described the prozy model for struc-turing \ndistributed object invocation in [26]. Each object is implemented by a server and one or more proxies. \nTo invoke a remote object, an application must first obtain a proxy which is responsible for servicing \nrequests on the client side and forward-ing them to a remote server transparently to the user. More recently, \nHamilton, Powell, and Mitchell used a mechanism based on s~bcontructs [lo] in the Spring operating system. \nThis mecha-nism allows the programmer to have some control over the implementation of operations on objects. \nFirst, on the client side, there are szlbcontract op-erations for marshaling, invocation, unmarshal- \ning, and parameter model, the phases tations. Similarly, may vary from one passing. In our invocation \ncan have different implemen- subcontract implementations object to another. Second, on the server side, \nthere are subcontracts for object creation, processing of incoming calls, and object revocation. Our \nmodel was designed for shared objects that may be implemented as single-copy, replicated, or partitioned \nobjects whereas the models described above were designed for the implementation of single copy objects. \nHowever, there are many sim-ilarities in the division of operation execution into several phases in our \nmodel and the division of subcontracts in subcontract operations in Spring.  7.2 Execution Models for \nReplicated Objects Several research projects have shown that repli-cation of shared data is important \nfor many ap-plications [4, 24, 25, 91. The main problem with data replication is data consistency. This \nprob-lem occurs in any system that replicates or caches shared data, such as Distributed Shared Memory \n(DSM) systems and cache-based shared-memory multiprocessors. Several interesting solutions have been \ndeveloped, ranging from all-hardware to mostly-hardware and all-software [13]. The DASH multiprocessor \n[17], for example, is a hard- ware solution; it caches shared data and uses a di- rectory to keep track \nof which processors contain replicas of a cache line. If a cache line is modified, all copies are invalidated, \nusing the directory. Our system can be regarded as an all-software DSM system. All mechanisms (including \nthe hit/miss check and all protocol control messages) are entirely handled in software, without any spe-cial \nhardware support. Also, our system uses ob-jects (rather than fixed-size pages or cache lines) as the \nunit of replication. Finally, unlike most other (hardware or software) DSM systems, we support an update \nprotocol in addition to an in-validation protocol. 7.3 Execution Models for Distributed Objects In \n[20], Makpangou et al. present a model for Fragmented Objects (FOs). An FO has two ab-stract views: the \ninvoker s view and the designer s view. An abstract interface defines the opera-tions one or more clients \nmay perform on an FO and hides its distribution. The designer sees an FO as a collection of fragments. \nA group inter-face defines the operations that may be performed on individual fragments or on all fragments \nfrom an operation of the abstract interface. The frag-ments are connected by a connective object. There \nare predefined connective objects that implement point-to-point and group communication proto-cols with \nvarious semantics. The designer of the FO maps operations into send and receive primi-tives to fragments. \nThe invocation protocol may be changed according to the access semantics of the object by modifying the \nconnective object. A similar model is also described in 1231. The lat-ter model additionally allows the \nspecification of the result gathering at the end of an invocation. These systems, however, do not allow \nthe exe-cution strategy of operations on fragments to be modified as our system does. Additionally, they \ndo not support parallel and sequential execution of operations. In [29], Sturman and Agha describe a \nproto-col description language for the implementation of multiple failure semantics in the actor model \n[l]. A fault-tolerant actor is implemented by a set of actors, each of which can take different roles. \nA role defines the behavior of an object and how it interacts with the other actors. Each role has two \nbasic methods: an in method and an out method. A message sent by an actor triggers the execution of the \nout method and the message received by an actor triggers the execution of the in method. To customize \nthe failure semantics of an object, a user specifies the different roles played in the protocol (e.g., \na server role and a backup role for a primary backup protocol) and their in and out methods. The failure \nsemantics of an object can be changed by modifying the roles of the actors that implement these semantics. \nThe language developed in [29] was specifically designed to cus-tomize failure semantics. The last execution \nmodel we discuss is pre-sented in [16] by Kiczales and also in [21] by Manola. This model is a reflective \n[32, 191 one. Each object stores information about its own im-plementation in a meta-object that may \nbe mod-ified dynamically. The set of operations or meth-ods on the meta-object (also called a meta-object \nprotocol) and a description of the control flow that ties the operations define the implementation of \nthe object. During the execution of an operation, first, the meta-object determines which set of op- \nerations must be executed. Second, it determines the flow that controls the execution of the opera-tions. \nFinally, it calls the operations. This model allows the inter-operability of objects of different object \nmodels by encapsulating each object model in a meta-object. To the best of our knowledge, this execution \nmodel is the most general one de-scribed in the literature and could also be used for the implementation \nof shared objects.  8 Conclusions and Future Work We have presented a flexible execution model for \noperations on shared objects. The execution of an operation is divided into five consecutive phases. \nSeveral handlers exist for the implementation of each phase. The execution strategy of an opera-tion \nmay be changed by substituting handlers in one or more phases. Using our model, the strategy used for \nthe execution of an operation is defined on a per instance basis and may be modified at runtime. Furthermore, \nthe RTS may be extended easily to implement new execution strategies by defining new handlers or by using \na new combi-nation of existing ones. We presented two appli-cations for which this flexibility proved \nuseful for experimental purposes, for improving the perfor-mance of a program, and for code reuse. A \nchallenging extension to this work is to de-velop heuristics that a compiler can use to choose the right \nstrategy for each operation on an ob-ject. With the current version of Hawk, in some cases, the user \nmust provide the necessary infor-mation for the system to call the appropriate han-dler. In a high-level \nlanguage, the compiler should take full advantage of the flexibility of the model transparently to the \nuser. We are extending the Orca compiler to support single-copy, replicated, and partitioned objects \nfor mixed task-and data-parallelism. This compiler will use simple heuris-tics to take advantage of the \nmodel. For example, if the compiler detects that there are no depen-dencies in an operation, it may omit \nthe prefetch-ing phase. More complex heuristics will be devel- oped in the future. Acknowledgments. \nWe are grateful to Satoshi Matsuoka for his many useful comments on our work, which helped to improve \nthe paper significantly. In addition, we would like to thank Raoul Bhoedjang, Koen Lan-gendoen, Tim Riihl, \nMaarten van Steen, and Kees Verstoep for their help during the preparation of this paper. References \nG.A. Agha. Actors: A Mode2 of Concurrent PI Computation in Distributed Systems. MIT Press, 1986. H.E. \nBal. Programming Distributed Systems. PI Prentice Hall, 1991. H.E. Bal and M.F. Kaashoek. Object distri- \nPI bution in Orca using compile-time and run-time techniques. In Proc. of the Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, pages 162-177, Washington, D.C., October 1993. H.E. \nBal, M.F. Kaashoek, and A.S. Tanen- PI baum. Orca: a language for parallel pro-gramming on distributed \nsystems. IEEE nans. on Software Engineering, 18(3):190-205, March 1992. P. Banerjee, J.A. Chandy, M. \nGupta, E.W. PI Hodges, J.G. Holm, A. Lain, D.J. Palermo, S. Ramaswamy, and E. Su. The Paradigm compiler \nfor distributed-memory multicom-puters. IEEE Computer, 28(10):37-47, Oct. 1995. A.D. Birrell and B.J. \nNelson. Implement- PI ing remote procedure calls. ACM Trans. on Computer Systems, 2(1):39-59, February \n1984. A.P. Black and Y. Arsty. Implementing loca- 171 tion independent invocation. IEEE Trans. on Parallel \nand Distributed Systems, 1(1):107-119, January 1990. [8] P. Dinda, T. Gross, D. O Hallaron, E. SegaII, \nJ. Stichnoth, J. Subhlock, J. Webb, and B. Yang. The CMU task parallel program suite. Technical Report \nCMU-CS-94-131, School of Computer Science, Carnegie Mel-lon University, Pittsburgh, PA, 1994. [9] B. \nFalsafi, A.R. Lebeck, S.K. Reinhardt, I. Schoinas, M.D. Hill, J.R. Larus, A. Rogers, and D.A. Wood. Application-specific \nproto-cols for user-level shared memory. Supercom-puting 94,, pages 380-389, Nov. 94. [lo] G. Hamilton, \nM.L. Powell, and J.G. Mitchell. Subcontract: a flexible base for distributed programming. Operating Systems \nReview, 27(5):69-79, December 1993. Proc. of the 14th ACM Symp. on Operation Systems Principles. [ll] \nS. Ben Hassen. Prefetching strategies for par- titioned shared objects. In Proc. of the 29th Hawaii IntI. \nConference on System Sciences, pages 261-271, Maui, Hawaii, January 1996. [12] S. Ben Hassen and H.E. \nBal. Integrating task and data parallelism using shared objects. In 10th ACM Intl. Conference on Supercomput- \ning, Philadelphia, Pennsylvania, May 1996. [13] K.L. Johnson, M.F. Kaashoek, and D.A. Wallach. CRL: High-performance \nAll-Software Distributed Shared Memory. In 15th ACM Symp. on Operating Systems Principles, pages 213-228, \nCopper Moun-tain, CO, December 1995. [14] E. Jul, H. Levy, N. Hutchinson, and A. Black. Fine-grained \nmobility in the Emerald sys-tem. ACM Trans. on Computer Systems, 6(1):109-133, February 1988. [15] M.F. \nKaashoek. Group Communication in Distributed Computer Systems. PhD thesis, Vrije Universiteit, Amsterdam, \n1992. [IS] G. Kiczales. Towards a new model of ab-straction in the engineering of software. In IMSA 92 \nWorkshop on Reflection and Meta-level Architectures, 1992. [17] D. Lenoski, J. Laudon, K. Gharachor-loo, \nW. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M.S. Lam. The Stan- ford Dash Multiproce ssor. IEEE \nComputer, 25(3):63-79, March 1992. [18] B. Liskov. Distributed programming in AR- GUS. CACM, 31(3):300-312, \nMarch 1988. [19] Pattie Maes. Concepts and experiements in computational reflection. In Proc. of the \nConference on Object-Oriented Program-ming, Systems, Languages, and Applications, pages 147-155, Orlando, \nFlorida, October 1987. [20] M. Makpangou, Y. Gourhant, J-P Le Narzul, and M. Shapiro. Fragmented objects \nfor dis- tributed abstractions. In T. L. Casavant and M. Singhal, editors, Readings in Distributed Computing \nSystems. IEEE Computer Society Press, July 1994. .] F. Manola. Meta-object protocol concepts for a Rise \nobject model. Technical Report TR-0244-12-93-165, GTE Laboratories, Inc, December 1993. !] W.G. O Farrell, \nF.Ch. Eigler, I. Kalas, and G.V. Wilson. ABC++ User Guide. Technical report, IBM Canada, Toronto, 1995. \n[23] P. Pardyak. Group communication in an object-based environment. In pd Intl. Work-shop on Object \nOrientation in Operating Systems, pages 106-116, Dourdon, France, September 1992. [24] M.C. Rinard, D. \nJ. Scales, and M.S. Lam. Jade: A high-level, machine independent lan-guage for parallel programming. \nIEEE com-puter, June 1993. [25] D.J. Scales and M.S. Lam. The Design and Evaluation of a Shared Object \nSystem for Distributed Memory Machines. Proc. 1st Symp. on Operating System Design and Implementation, \npages 101-114, November 1994. [26] M. Shapiro. Structure and encapsulation in distributed systems: the \nProxy principle. In  Intl. Conference on Distributed Computing Systems, pages 198-204, Cambridge, MA, \nMay 1986. [27] S.D. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das, and J. Saltz. Run-time and compile-time \nsupport for adaptive irreg-ular problems. In Supercomputing 94, pages 97-106. IEEE, 1994. [28] J. Stoer \nand R. Bulirsh. Introduction to Numerical Analysis. Springer-Verlag, New York, NY, 1983. [29] D.C. Sturman \nand G.A. Agha. A protocol description language for customizing failure semantics. In 13th Intl. Symp. \non Reliable Distributed Systems, 1994. [30] A.S. Tanenbaum, M.F. Kaashoek, R. van Re-nesse, and H.E. \nBal. The Amoeba distributed operating system-a status report. Com-puter Communications, 14, July/August \n1991. [31] A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, A.J. Jansen, \nand G. van Rossum. Experiences with the Amoeba distributed operating sys-tem. CACM, 33:46-63, December \n1990. [32] T. Watanebe and A. Yonezawa. Reflection in an object-oriented concurrent language. In Proc. \nof the Conference on Object-Oriented Programming, Systems, Languages, and Ap-plications, pages 306-315, \nSeptember 1988. [33] E.A. West and A.S. Grimshaw. Braid: Inte-grating task and data parallelism. In \nSymp. on the Frontiers of Massively Parallel Com-putation, McLean, Virginia, February 1995. \n\t\t\t", "proc_id": "236337", "abstract": "Many parallel and distributed programming models are based on some form of shared objects, which may be represented in various ways (e.g., single-copy, replicated, and partitioned objects). Also, many different operation <i>execution strategies</i> have been designed for each representation. In programming systems that use multiple representations integrated in a single object model, one way to provide multiple execution strategies is to implement each strategy independently from the others. However, this leads to rigid systems and provides little opportunity for code reuse. Instead, we propose a flexible operation execution model that allows the implementation of many different strategies, which can even be changed at runtime. We present the model and a distributed implementation of it. Also, we describe how various execution strategies can be expressed using the model, and we look at applications that benefit from its flexibility.", "authors": [{"name": "Saniya Ben Hassen", "author_profile_id": "81100559659", "affiliation": "Vrije Universiteit , Amsterdam", "person_id": "P259803", "email_address": "", "orcid_id": ""}, {"name": "Irina Athanasiu", "author_profile_id": "81100586001", "affiliation": "Polytechnical University, Bucharest", "person_id": "P116861", "email_address": "", "orcid_id": ""}, {"name": "Henri E. Bal", "author_profile_id": "81100348137", "affiliation": "Vrije Universiteit, Amsterdam", "person_id": "PP14124679", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/236337.236341", "year": "1996", "article_id": "236341", "conference": "OOPSLA", "title": "A flexible operation execution model for shared distributed objects", "url": "http://dl.acm.org/citation.cfm?id=236341"}