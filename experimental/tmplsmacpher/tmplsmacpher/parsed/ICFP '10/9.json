{"article_publication_date": "09-27-2010", "fulltext": "\n Lazy Tree Splitting Lars Bergstrom Mike Rainey John Reppy Adam Shaw University of Chicago {larsberg,mrainey,jhr,ams}@cs.uchicago.edu \nAbstract Nested data-parallelism (NDP) is a declarative style for program\u00adming irregular parallel applications. \nNDP languages provide lan\u00adguage features favoring the NDP style, ef.cient compilation of NDP programs, \nand various common NDP operations like paral\u00adlel maps, .lters, and sum-like reductions. In this paper, \nwe describe the implementationofNDPinParallelML(PML),partoftheMan\u00adticore project. Managing the parallel \ndecomposition of work is one of the main challenges of implementing NDP. If the decomposi\u00adtion creates \ntoo manysmall chunks of work, performance will be eroded by too much parallel overhead. If, on the other \nhand, there aretoofewlarge chunksofwork, therewillbetoomuch sequential processing and processors will \nsit idle. Recently the technique of Lazy Binary Splitting was proposed for dynamic parallel decomposition \nof work on .at arrays, with promising results.We adapt Lazy Binary Splitting to parallel pro\u00adcessing \nof binary trees, which we use to represent parallel arrays in PML.We call our technique LazyTree Splitting \n(LTS). One of its main advantages is its performance robustness: per-program tuning is not required to \nachieve good performance across varying plat\u00adforms.We describeLTS-based implementations of standard NDP \noperations, and we present experimental data demonstrating the scalabilityofLTS acrossa rangeof benchmarks. \nCategories and Subject Descriptors D.3.0[Programming Lan\u00adguages]: General; D.3.2 [Programming Languages]: \nLanguage Classi.cations Concurrent, distributed, and parallel languages; D.3.4[Programming Languages]: \nProcessors Run-time environ\u00adments General Terms Languages, Performance Keywords nested-data-parallel \nlanguages, scheduling, compilers, and run-time systems 1. Introduction Nested data-parallelism (NDP) \n[BCH+94]isa declarativestylefor programming irregular parallel applications. NDP languages pro\u00advide language \nfeaturesfavoring the NDP style, ef.cient compila\u00adtion of NDP programs, and various common NDP operations \nlike parallel maps, .lters, and sum-likereductions. Irregular parallelism is achievedby thefact that \nnested arrays need nothave regular, or Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright c . 2010ACM \n978-1-60558-794-3/10/09... $10.00 Matthew Fluet Rochester InstituteofTechnology mtf@cs.rit.edu rectangular, \nstructure; i.e., subarrays may have different lengths. NDP programming is supported by a number of different \nparallel programming languages [CLP+07, GSF+07], including our own Parallel ML(PML) [FRRS08]. On its \nface, implementing NDP operations seems straightfor\u00adward because individual array elements are natural \nunits for creat\u00ading tasks, which are small, independent threads of control.1 Cor\u00adrespondingly, a simple \nstrategy is to spawn off one task for each array element. This strategy is unacceptable in practice, \nas there is a scheduling cost associated with each task(e.g., the cost of plac\u00ading the task on a scheduling \nqueue) and individual tasks often per\u00adform only small amounts of work. As such, the scheduling cost of \na given task might exceed the amount of computation it performs. If scheduling costs are too large, parallelism \nis not worthwhile. One commonwaytoavoidthispitfallistogrouparray elements into.xed-size chunksof elementsandspawnataskforeach \nchunk. Eager Binary Splitting (EBS), a variant of this strategy, is used by Intel s TBB [Int08, RVK08] \nand Cilk++ [Lei09]. Choosing the right chunk size is inherently dif.cult, as one must .nd the middle \nground between undesirable positions on either side. If the chunks are too small, performanceisdegradedby \nthe high costsof the associated scheduling and communicating. By contrast, if the chunks are too big, \nsome processors go unutilized because there aretoofew taskstokeepthemallbusy. One approach to picking \nthe right chunk size is to use static analysis to predict task execution times and pick chunk sizes ac\u00adcordingly \n[TZ93].Butthis approachis limitedbythefactthat tasks can run for arbitrarily different amounts of time, \nand these times are dif.cult to predict in speci.c cases and impossible to predict in general. Dynamic \ntechniques for picking the chunk size have the advantage that they can base chunk sizes on runtime estimates \nof system load. Lazy Binary Splitting (LBS) is one such chunk\u00ading strategy for handling parallel do-all \nloops [TCBV10]. Un\u00adlike the two aforementioned strategies, LBS determines chunks au\u00adtomatically and without \nprogrammer (or compiler) assistance and imposes only minor scheduling costs. This paper presents an implementation \nof NDP that is based on our extension of LBS to binary trees, which we call LazyTree Splitting (LTS).LTS \nsupports operations that produce and consume trees where tree nodes are represented as records allocated \nin the heap.We are interested in operations on trees because Manticore, the system that supports PML, \nuses ropes [BAP95], a balanced binary-tree representation of sequences, as the underlying represen\u00adtation \nof parallel arrays. Our implementation is purely functional in that it works with immutable structures, \nalthough some imperative techniques are used under the hood for scheduling. 1We do not address.attening \n(or vectorizing)[Kel99, Les05] transforma\u00adtions here, since the techniques of this paper apply equally \nwell to .attened or non-.attened programs.  LTS exhibitsperformance robustness;i.e., it provides scalable \nparallel performance across a range of different applications and platforms without requiring any per-application \ntuning. Perfor\u00admance robustness is a highly desirable characteristic for a parallel programming language, \nfor obvious reasons. Prior to our adoption ofLTS, we used EagerTree Splitting (ETS), a variation of EBS. \nOur experiments demonstrate that ETS lacks performance robust\u00adness: the tuning parameters that control \nthe decomposition of work are very sensitive to the given application and platform. Further\u00admore, we \ndemonstrate that the performance ofLTS comparesfa\u00advorably to that of (ideally-tuned) ETS across our benchmark \nsuite. 2. Nested data parallelism Inthis sectionwegiveahigh-level descriptionofPMLand discuss the runtime \nmechanisms we use to support NDP. More detail can be found in our previous papers [FRR+07, FFR+07, FRRS08]. \n2.1 Programming model PMListhe programming language supportedbythe Manticoresys\u00adtem.2 Our programming \nmodelis based ona strict,but mutation\u00adfree, functional language (a subset of Standard ML [MTHM97]), which \nis extended with support for multiple forms of parallelism. We provide .ne-grain parallelism through \nseveral lightweight syn\u00adtactic constructs that serveas hints to the compiler and runtime that the program \nwill bene.t from executing the computation in paral-lel.For this paper, we are primarily concerned with \nthe NDP con\u00adstructs, which are based on those foundinN ESL [Ble90b, Ble96]. PML provides a parallel array \ntype constructor(parray)and operations to map, .lter, reduce, and scanthese arrays in parallel. Like \nmost languages that support NDP, PML includes comprehen\u00adsion syntax for maps and.lters,but for this paper \nwe omit the syn\u00adtactic sugar and restrict ourselves the the following interface: type a parray val range \n: int * int -> int parray val mapP : ( a -> b) -> a parray -> b parray val filterP : ( a -> bool) -> \na parray -> a parray val reduceP : ( a * a -> a) -> a -> a parray -> a val scanP : ( a * a -> a) -> a \n-> a parray -> a parray The function range generates an array of the integers between its two arguments, \nmapP, filterP, and reduceP have their usual meaning,exceptthattheymaybeevaluatedin parallel,and scanP \nproduces a pre.x scan of the array. These parallel-array operations have been used to specify both SIMD \nparallelism thatis mapped onto vector hardware (e.g., Intel s SSE instructions) and SPMD parallelism \nwhere parallelism is mapped onto multiple cores; this paper focuses on exploiting the latter. As a simple \nexample, the main loop of a ray tracer generating an image of width w and height h can be written fun \nraytrace (w, h) = mapP (fn y => mapP (fn x => trace (x, y)) (range (0,w-1))) (range (0,h-1)) This parallelmap \nwithinaparallelmapisanexampleof nested data parallelism. Note that the time to compute one pixel depends \non the layout of the scene, because the ray cast from position (x,y) might pass through a subspace that \nis crowded with re.ective ob\u00adjects or it might pass through relatively empty space. Thus, the amount \nof computation across the trace(x,y) expression (and, therefore, across the inner mapP expression) may \ndiffer signi.\u00adcantly dependingonthelayoutofthe scene.Arobust techniquefor 2Manticore may support other \nparallel languages in the future. balancing the parallel execution of this unbalanced computation is \nthe primary contribution of this paper. 2.2 Runtime model The Manticore runtime system consists of a \nsmall core written in C, which implements a processor abstraction layer, garbage collection, and a few \nbasic scheduling primitives. The rest of our runtime system is written in BOM, a PML-like language. BOM \nsupports several mechanisms, such as .rst-class continuations and mutable data structures, that are useful \nfor programming schedulers but are not in PML. Further details on our system may be found elsewhere [FRR08, \nRai09, Rai07]. A task-scheduling policy determines the order in which tasks execute and the assignments \nfrom tasks to processors. OurLTS is built on top of on a particular task-scheduling policy called work \nstealing [BS81, Hal84]. In work stealing, we employ a group of workers, one per processor, that collaborate \non a given computa\u00adtion. The idea is that idle workers which have no useful work to do bear mostof the \nscheduling costs andbusyworkers whichhave useful work to do focus on .nishing that work. We use the following, \nwell-known implementation of work stealing [BL99, FLR98]. Each worker maintains a deque (double\u00adended \nqueue) of tasks, represented as thunks. When a worker reaches a point of potential parallelism in the \ncomputation, it pushes a task for one independent branch onto the bottom of the deque and continuesexecuting \nthe other independent branch. Upon completion of the executed branch, it pops a task offthe bottom of \nthe deque and executes it. If the deque is not empty, then the task is necessarily the most recently \npushed task; otherwise all of the local tasks have been stolen by other workers and the worker must steal \nataskfromthetopof some otherworker s deque. Potential victims are chosen at random from a uniform distribution. \nThiswork-stealing scheduler canbe encapsulatedinthe follow\u00ading function, which is part of the runtime \nsystem core: val par2 : (unit -> a) * (unit -> b) -> a * b When a worker P executes par2 (f,g), it pushes \nthe task g onto the bottom of its deque3 and then executes f(). When the computation of f() completes \nwith result rf , P attempts to pop g from its deque. If successful, then P will evaluate g() to a result \nrg and return the pair (rf ,rg). Otherwise, some other worker Q has stolen g, so P writes rf into a shared \nvariable and looks for other work to do. When Q .nishes the evaluation of g(), then it will pass the \npair of results to the return continuation of the par2 call. The scheduler also provides a generalization \nof par2 to a list of thunks. val parN : (unit -> a) list -> a list This function can be de.ned in terms \nof par2,but we use a more ef.cient implementation that pushes all of the tasks in its tail onto the deque \nat once.  2.3 Ropes In our Manticore system, we use ropes as the underlying represen\u00adtation of parallel \narrays. Ropes, originally proposed as an alterna\u00adtive to strings, are persistent balanced binary trees \nwith seqs, con\u00adtiguousarraysofdata,attheirleaves[BAP95].Forthe purposesof this paper,we viewthe rope \ntype as having the following de.nition: datatype a rope = Leaf of a seq | Cat of a rope * a rope although \nin our actual implementation there is extra information in the Cat nodes to support balancing. Read from \nleft to right, the 3Strictly speaking, it pushes a continuation that will evaluate g().  data elementsattheleavesofarope \nconstitutethedataofaparallel array it represents. Since ropes are physically dispersed in memory, they \nare well\u00adsuited to being built in parallel, with different processors simul\u00adtaneously working on different \nparts of the whole. Furthermore, the rope data structure is persistent, which provides, in addition to \nthe usual advantages of persistence, two special advantages related to memory management. First, we can \navoid the cost of store-list operations [App89], which would be necessary for maintaining an ephemeral \ndata structure. Second,aparallel memory manager,such as the one used by Manticore [FRR08], can avoid \nmaking memory management a sequential bottleneck by letting processors allocate and reclaim ropes independently. \nAs a parallel-array representation, ropes have several weak\u00adnesses as opposed to contiguous arrays of, \nsay, unboxed doubles. First, rope random access requireslogarithmic time. Second,keep\u00ading ropes balanced \nrequires extra computation. Third, mapping over multiple ropes is more complicated than mapping over \nmul\u00adtiple arrays, since the ropes may have different shape. In our per\u00adformance study in Section 5, we \n.nd that these weaknesses are not crippling by themselves, yet we know of no study in which NDP implementations \nbased on ropes are compared side by side with implementations based on alternative representations, such \nas con\u00adtiguous arrays. The maximum length of the linear sequence at each leaf of a rope is controlled \nby a compile-time constant M. At run-time, a leaf contains a number of elements n such that 0 = n = M. \nIn general,rope operationstrytokeepthesizeeachleafascloseto M as possible, although some leaves will \nnecessarily be smaller.We do not demand that a rope maximize the size of its leaves. Relaxing the perfect \nbalance requirement reducesexcessivebal\u00adancing, yet maintains the asymptotic behavior of rope operations. \nOur rope-balancing policyis a relaxed, parallel version of the se\u00adquential policy used by Boehm, et al. \n[BAP95]. The policy of Boehm, et al. is as follows.Foragiven rope r of depth d and length n,the balancing \ngoal isd =.log2 n.+2. This property is enforced by the function val balance : a rope -> a rope which \ntakes a rope r and returns a balanced rope equivalent to r (returning r itself if it is already balanced). \nIn our rope-balancing policy, only those ropes that arebuilt se\u00adrially are balanced by balance, i.e., \nthe serial balancing process only ever takes place within a given chunk. There is no explicit guaranteeonthe \nbalanceofarope containing subropesthatarebuilt by different processors.For sucha rope, the amountof rope \nimbal\u00adance is proportional to the distribution of work across processors rather than the size of the \nrope itself. As we discuss in Section 5, across all our benchmarking results, balancing has minimal impact \non performance. Asnotedabove,rope operationstrytokeepthesizeofeachleaf as close to M as possible. Inbuilding \nropes, rather than using the Cat constructor directly, we de.ne a smart constructor: val cat2 : a rope \n* a rope -> a rope If cat2 is applied to two small leaves, it may coalesce them into a single larger \nleaf. Note that cat2 does not guarantee balance, although it will maintain balance if applied to two \nbalanced ropes of equal size.We also de.nea similar function val catN : a rope list -> a rope which returns \nthe smart concatenation of its argument ropes. We sometimes needafast, cheap operation for splittinga \nrope into multiple subropes.For this reason, we provide val split2 : a rope -> a rope * a rope which \nsplits its rope parameter into two subropes such that the size of these ropes differsby at most one.We \nalso de.ne val splitN : a rope * int -> a rope list which splits its parameter into n subropes, where \neach subrope has the same size, except for one subrope that might be smaller than the others. We sometimes \nuse val length : a rope -> int which takes a rope r and returns the number of elements stored in the \nleaves of r.4 The various parallel-array operations described in Section 2.1 are implementedby analogous \noperations on ropes. Sections3and 4describes the implementation of these rope-processing operations in \ndetail. 3. The Goldilocks problem In NDP programs, computations are divided into chunks, and chunks of \nwork are spawned in parallel. Those chunks might be de.ned by subsequences (of arrays, for example, or, \nin our case, ropes) or iteration spaces (say, k to some k + n). The choice of chunk size in.uences performance \ncrucially. If the chunks are too small, there will be too much overhead in managing them; in ex\u00adtreme \ncases, the bene.ts of parallelism will be obliterated. On the other hand, if they are too large, there \nwill not be enough paral\u00adlelism, and some processors may run out of work. An ideal chunk\u00ading strategy \napportions chunks that are neither too large nor too small,butare,likeGoldilocks sthirdbowlof porridge, \njustright. Some different chunking strategies are considered in the sequel. 3.1 Fragile chunking strategies \nA fragile chunking strategy is prone either to creating an exces\u00adsive number of tasks or to missing signi.cant \nopportunities for par\u00adallelism. Let us consider a two simple strategies, T -ary decom\u00adposition and structural \ndecomposition, and the reasons that they are fragile. In T -ary decomposition, we split the input rope \ninto T = min(n, J \u00d7 P ) chunks, where n is the size of the input rope, J isa.xed compile-time constant, \nand P is the number processors, andspawnataskforeach chunk.Forexample,inFigure1(a),we showthe T -arydecompositionversionofthe \nrope-map operation. 5 In computations where all rope elements take the same time to pro\u00adcess, such as \nthose performed by regular af.ne (dense-matrix) sci\u00adenti.c codes, the T -ary decomposition will balance \nthe work load evenly across all processors because all chunks will take about the same amount of time. \nOn the other hand, when rope elements cor\u00adrespond to varying amounts of work, performance will be fragile \nbecause some processors will get overloaded and others underuti\u00adlized. Excessive splitting is also a \nproblem. Observe that for i lev\u00adels of nesting and suf.ciently-large ropes, the T -ary decomposition \ncreates (J \u00d7 P )i tasks overall, which can be excessive when either i or P get large. To remedy the imbalance \nproblem, we might try structural de\u00adcomposition, in which both children of a Cat node are processed in \nparallel and the elements of a Leaf node are processed sequen\u00adtially.We show the structuralversionof \nthe rope-map operationin Figure1(b). Recallthatthe maximumsizeofaleafis determinedby 4In our actual implementation, \nthis operation takes constant time, as we cache lengths in Cat nodes. 5In this and subsequent examples, \nwe use the function mapSequential with type ( a -> b) -> a rope -> b rope which is the obvious sequential \nimplementation of the rope-map operation. \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Nested data-parallelism (NDP) is a declarative style for programming irregular parallel applications. NDP languages provide language features favoring the NDP style, efficient compilation of NDP programs, and various common NDP operations like parallel maps, filters, and sum-like reductions. In this paper, we describe the implementation of NDP in Parallel ML (PML), part of the Manticore project. Managing the parallel decomposition of work is one of the main challenges of implementing NDP. If the decomposition creates too many small chunks of work, performance will be eroded by too much parallel overhead. If, on the other hand, there are too few large chunks of work, there will be too much sequential processing and processors will sit idle.</p> <p>Recently the technique of Lazy Binary Splitting was proposed for dynamic parallel decomposition of work on flat arrays, with promising results. We adapt Lazy Binary Splitting to parallel processing of binary trees, which we use to represent parallel arrays in PML. We call our technique <i>Lazy Tree Splitting</i> (LTS). One of its main advantages is its performance robustness: per-program tuning is not required to achieve good performance across varying platforms. We describe LTS-based implementations of standard NDP operations, and we present experimental data demonstrating the scalability of LTS across a range of benchmarks.</p>", "authors": [{"name": "Lars Bergstrom", "author_profile_id": "81470655259", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P2338157", "email_address": "", "orcid_id": ""}, {"name": "Mike Rainey", "author_profile_id": "81330497308", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P2338158", "email_address": "", "orcid_id": ""}, {"name": "John Reppy", "author_profile_id": "81100590527", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P2338159", "email_address": "", "orcid_id": ""}, {"name": "Adam Shaw", "author_profile_id": "81330498457", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P2338160", "email_address": "", "orcid_id": ""}, {"name": "Matthew Fluet", "author_profile_id": "81100181338", "affiliation": "Rochester Institute of Technology, Rochester, NY, USA", "person_id": "P2338161", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863558", "year": "2010", "article_id": "1863558", "conference": "ICFP", "title": "Lazy tree splitting", "url": "http://dl.acm.org/citation.cfm?id=1863558"}