{"article_publication_date": "09-27-2010", "fulltext": "\n Abstracting Abstract Machines * DavidVan Horn Northeastern University dvanhorn@ccs.neu.edu Abstract \nWe describe a derivational approach to abstract interpretation that yields novel and transparentlysound \nstatic analyses when applied to well-established abstract machines. To demonstrate the tech\u00adnique and \nsupport our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine \ns machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations \nof themselves. The resulting analyses bound temporal ordering of program events; predict return-.ow and \nstack-inspection behavior; and approximate the .ow and eval\u00aduation of by-need parameters.For all of these \nmachines, we .nd that a series of well-known concrete machine refactorings, plus a technique we call \nstore-allocated continuations, leads to machines that abstract into static analyses simply by bounding \ntheir stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic \nlanguage features, including tail calls, conditionals, side effects, exceptions, .rst-class continuations, \nand evengarbage collection. Categories and Subject Descriptors F.3.2[Logics and Meanings of Programs]: \nSemantics of Programming Languages Program analysis, Operational semantics; F.4.1[Mathematical Logic \nand Formal Languages]: Mathematical Logic Lambda calculus and related systems GeneralTerms Languages, \nTheory Keywords abstract machines, abstract interpretation 1. Introduction Abstract machines such as \nthe CEK machine and Krivine s ma\u00adchine are .rst-order state transition systems that represent the core \nof a real language implementation. Semantics-based program anal\u00adysis, on the other hand, is concerned \nwith safely approximating intensional properties of such a machine as it runs a program. It seems natural \nthen to want to systematically derive analyses from machines to approximate the core of realistic run-time \nsystems. Our goal is to develop a technique that enables direct abstract interpretations of abstract \nmachines by methods for transforming a given machine description into another that computes its .nite \napproximation. * Supportedbythe National ScienceFoundationundergrant 0937060tothe Computing Research \nAssociation for the CIFellow Project. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright c &#38;#169; \n2010ACM 978-1-60558-794-3/10/09... $10.00 Matthew Might University of Utah might@cs.utah.edu We demonstrate \nthat the technique of refactoring a machine with store-allocated continuations allows a direct structural \nab\u00adstraction1 by bounding the machine s store. Thus, we are able to convert semantic techniques used \nto model language features into static analysis techniques for reasoning about the behavior of those \nvery same features.Byabstracting well-knownmachines, our tech\u00adnique delivers static analyzers that can \nreason about by-needevalu\u00adation, higher-order functions, tail calls, side effects, stack structure, exceptions \nand .rst-class continuations. The basic idea behind store-allocated continuations is not new. SML/NJ \nhas allocated continuations in the heap for well over a decade[28]. At .rst glance, modeling the program \nstack in an ab\u00ad stract machine with store-allocated continuations would not seem to provide anyreal bene.t. \nIndeed, for the purpose of de.ning the meaning of a program, there is no bene.t, because the meaning \nof the program does not depend on the stack-implementation strat\u00adegy. Yet, a closer inspection .nds that \nstore-allocating continua\u00adtions eliminate recursion from the de.nition of the state-space of the machine.With \nno recursive structure in the state-space, an ab\u00adstract machine becomes eligible for conversion into \nan abstract in\u00adterpreter through a simple structural abstraction. To demonstrate the applicability of \nthe approach, we derive abstract interpreters of: a call-by-value .-calculus with state and control \nbased on the CESK machine of Felleisen and Friedman[13],  a call-by-need .-calculus based on a tail-recursive, \nlazy vari\u00adant of Krivine s machine derived by Ager, Danvy and Midt\u00adgaard[1], and  a call-by-value .-calculus \nwith stack inspection based on the CM machine of Clements and Felleisen[3];  and use abstractgarbage \ncollection to improve precision[25]. Overview In Section2, webegin with the CEK machine and attempta \nstruc\u00adtural abstract interpretation,but .nd ourselves blockedby two re\u00adcursive structures in the machine: \nenvironments and continuations. We make three refactorings to: 1. store-allocate bindings, 2. store-allocate \ncontinuations, and 3. time-stamp machine states;  resulting in the CESK, CESK*, and time-stamped CESK* \nma\u00adchines, respectively. The time-stamps encode the history (context) of the machine sexecution andfacilitate \ncontext-sensitive abstrac\u00adtions. We then demonstrate that the time-stamped machine ab\u00adstracts directly \ninto a parameterized, sound and computable static analysis. 1 Astructural abstraction distributes component-, \npoint-, and member-wise. In Section3, we replay this process (slightly abbreviated) with a lazy variant \nof Krivine s machine to arrive at a static analysis of by-need programs. In Section4, we incorporate \nconditionals, sideeffects,excep\u00adtions, .rst-class continuations, andgarbage collection. In Section6,we \nabstracttheCM (continuation-marks) machine to produce an abstract interpretation of stack inspection. \nIn Section7,we widenthe abstract interpretations withasingle\u00adthreaded global storeto accelerate convergence.For \nsomeof our analyzers, this widening results in polynomial-time algorithms and connects them back to known \nanalyses. 2. From CEK to the abstract CESK* In this section, we start with a traditional machine for \na program\u00adming language based on the call-by-value .-calculus, and gradu\u00adally derive an abstract interpretation \nof this machine. The outline followed in this section covers the basic steps for systematically deriving \nabstract interpretersthat we follow throughout the rest of the paper. To begin, consider the following \nlanguage of expressions:2 e . Exp ::= x | (ee) | (.x.e) x . Var a set of identi.ers. Astandard machineforevaluatingthis \nlanguageistheCEKma\u00adchine of Felleisen and Friedman[12], and it is from this machine we derive the abstract \nsemantics a computable approximation of the machine s behavior. Most of the steps in this derivation \ncorre\u00adspond to well-known machine transformations and real-world im\u00adplementation techniques and most \nof these steps are concerned only with the concrete machine;a very simple abstraction is em\u00adployed only \nat the very end. The remainder of this section is outlined as follows: we present the CEK machine, to \nwhich we add a store, and use it to allo\u00adcate variable bindings. This machine is just the CESK machine \nof Felleisen and Friedman[13].From here, we furtherexploit the store to allocate continuations, which \ncorresponds to a well-known im\u00adplementation technique used in functional language compilers[28]. Wethen \nabstractonly the storeto obtainaframework for the sound, computable analysis of programs. 2.1 The CEK \nmachine Astandard approach to evaluating programs is to rely on a Curry\u00adFeys-style Standardization Theorem, \nwhich says roughly: if an expression e reduces to e' in, e.g., the call-by-value .-calculus, then e reduces \nto e' in a canonical manner. This canonical manner thus determinesastate machine forevaluating programs:astandard \nreduction machine. Tode.ne suchamachine for our language, we de.neagrammar ofevaluation contexts and \nnotionsof reduction(e.g., \u00dfv). An eval\u00aduation contextis anexpression witha hole in it.For left-to-right \nevaluation order, we de.ne evaluation contexts E as: E ::= [ ] | (Ee) | (vE). 2Fine print on syntax:As \nis often the case in program analysis where se\u00admantic values are approximated using syntactic phrases \nof the program un\u00adder analysis, we would like to be able to distinguish different syntactic oc\u00adcurrences \nof otherwise identical expressions within a program. Informally, this meanswewantto trackthe source locationofexpressions.Formally, \nthis is achieved by labeling expressions and assuming all labels within a program are distinct: e . Exp \n::= x\u00a3 | (ee)\u00a3 | (.x.e)\u00a3 i . Lab an in.nite set of labels. However,we judiciously omit labels whenever \ntheyare irrelevant and doing so improves the clarity of the presentation. Consequently, theyappear only \nin Sections 2.7 and7, which are concerned with k-CFA. .' . -.CEK (x, ., .) (v, .', .) where .(x) = (v, \n.') ((e0e1), ., .) (e0, ., ar(e1, ., .)) (v, ., ar(e, .', .)) (e, .', fn(v, ., .)) (v, ., fn((.x.e), \n.', .)) (e, .'[x . (v, .)], .) Figure 1. The CEK machine. An expression is either a value or uniquely \ndecomposable into an evaluation context and redex. The standard reduction machine is: '' E[e] -.\u00dfv E[e], \nif e\u00dfv e. However, this machine does not shed much light on a realistic implementation. At each step, \nthe machine traverses the entire source of the program looking for a redex. When found, the redex is \nreduced and the contractum is plugged back in the hole, then the process is repeated. Abstract machines \nsuch as the CEK machine, which are deriv\u00adable from standard reduction machines, offer an extensionally \nequivalentbut more realistic modelofevaluation thatis amenable to ef.cient implementation. The CEK is \nenvironment-based; it uses environmentsand closuresto model substitution.It representseval\u00aduation contexts \nas continuations, an inductive data structure that models contextsinan inside-out manner.Thekeyideaof \nmachines such as the CEK is that the whole program need not be traversed to .nd the next redex, consequently \nthe machine integrates the pro\u00adcess of plugging a contractum into a context and .nding the next redex. \nStatesof the CEK machine[12]consistofa control string(an expression), an environment that closes the \ncontrol string, and a continuation: . . S= Exp \u00d7 Env \u00d7 Kont v . Val ::= (.x.e) . . Env = Var ..n Val \n\u00d7 Env . . Kont ::= mt | ar(e, ., .) | fn(v, ., .). States are identi.ed up to consistent renaming of \nbound variables. Environments are .nite maps from variables to closures. Envi\u00adronment extension is written \n.[x . (v, .')]. Evaluation contexts E are represented (inside-out)by continua\u00adtions as follows: [] is \nrepresented by mt;E[([]e)] is represented by ar(e', ., .) where . closes e' to represent e and . represents \nE; E[(v[])] is represented by fn(v', ., .) where . closes v' to represent v and . represents E. The transition \nfunction for the CEK machine is de.ned in Fig\u00adure1(we follow the textbook treatment of the CEK machine[11, \npage 102]). The initial machine state for a closed expression e is givenby the inj function: inj CEK \n(e)= (e, \u00d8, mt). Typically, an evaluation function is de.ned as a partial function from closed expressions \nto answers: ' evalCEK (e)=(v, .) if inj (e) -. CEK (v, ., mt). Thisgivesanextensionalviewofthe machine, \nwhichis useful, e.g., toprovecorrectnesswith respecttoa canonicalevaluation function such as one de.ned \nby standard reduction or compositional valu\u00adation. However for the purposes of program analysis, we are \ncon\u00adcerned more with the intensional aspects of the machine. As such, we de.ne the meaning of a program \nas the (possibly in.nite) set of reachable machine states: evalCEK (e)= {. | inj (e) -. CEK .}.  Deciding \nmembership in the set of reachable machine states is not possible due to the halting problem. The goal \nof abstract interpretation, then, is to construct a function, aval , that is a CEK sound and computable \napproximation to the evalCEK function. Wecando thisbyconstructingamachine thatis similarin struc\u00adturetotheCEK \nmachine:itis de.nedbyan abstract state transition relation (-. ) . S \u00d7 S , which operates over abstract \nstates, CEK S , which approximate the states of the CEK machine, and an ab\u00adstraction map a :S . S that \nmaps concrete machine states into abstract machine states. The abstract evaluation function is then de.ned \nas: aval (e)= {. | a(inj (e)) -. . }. CEK CEK 1. We achieve decidability by constructing the approximation \nin such a way that the state-space of the abstracted machine is .nite, which guarantees that for any \nclosed expression e, the set aval(e) is .nite. 2. We achieve soundness by demonstrating the abstracted \nma\u00adchine transitions preserve the abstraction map, so that if . -. . ' and a(.) . . , then there exists \nan abstract state . ' such that . -. . ' and a(. ' ) . . ' .  A .rst attempt at abstract interpretation: \nA simple approach to abstracting the machine s state space is to apply a structural abstract interpretation,which \nlifts abstraction point-wise, element\u00adwise, component-wise and member-wise across the structure of a \nmachine state(i.e., expressions, environments, and continuations). The problem with the structural abstraction \napproach for the CEK machine is that both environments and continuations are recursive structures. As \na result, the map a yields objects in an abstract state-space with recursive structure, implying the \nspace is in.nite. It is possible to perform abstract interpretation over an in.nite state-space,butit \nrequiresawidening operator.Awidening operator accelerates the ascent up the lattice of approximation \nand must guarantee convergence. It is dif.cult to imagine a widening operator, other than the one that \njumps immediately to the top of the lattice, for these semantics. Focusing on recursive structure as \nthe source of the problem, a reasonable course of action is to add a level of indirection to the recursion \nto force recursive structure to pass through explicitly allocated addresses. In doing so, we will unhinge \nrecursion in a program s data structures and its control-.ow from recursive structure in the state-space. \nWeturnour attentionnexttotheCESK machine[10,13],since the CESK machine eliminates recursion from one \nof the structures in the CEK machine: environments. In the subsequent section (Sec\u00adtion 2.3), we will \ndevelop a CESK machine with a pointer re.ne\u00adment (CESK*)that eliminates the other source of recursive \nstruc\u00adture: continuations. At that point, the machine structurally abstracts via a single point of approximation: \nthe store.  2.2 The CESK machine The states of the CESK machine extend those of the CEK machine to include \na store, whichprovidesalevelof indirectionforvariable bindings to pass through. The store is a .nite \nmap from addresses to storable values and environments are changed to map variables to addresses. Whenavariable \nsvalueis looked-upby the machine, it is now accomplished by using the environment to look up the variable \ns address, which is then used to look up the value. To bind a variable to a value, a fresh location in \nthe store is allocated and mapped to the value; the environment is extended to map the variable to that \naddress. ' . -.CESK . (x, ., s, .) (v, . ' , s, .) where s(.(x)) = (v, . ' ) ((e0e1), ., s, .) (e0, ., \ns, ar(e1, ., .))(v, ., s, ar(e, . ' ,.)) (e, . ' , s, fn(v, ., .))(v, ., s, fn((.x.e),. ' ,.)) (e, . \n' [x . a],s[a . (v, .)],.)where a/. dom(s) Figure 2. The CESK machine. The state space for the CESK machine \nis de.ned as follows: . . S= Exp \u00d7 Env \u00d7 Store \u00d7 Kont . . Env = Var ..n Addr s . Store = Addr ..n Storable \ns . Storable = Val \u00d7 Env a, b, c . Addr an in.nite set. States are identi.ed up to consistent renaming \nof bound variables and addresses. The transition function for the CESK machine is de.nedinFigure2(wefollowthetextbook \ntreatmentoftheCESK machine[11, page 166]). The initial statefora closedexpressionisgivenbythe inj func\u00adtion, \nwhich combines the expression with the empty environment, store, and continuation: inj CESK (e)= (e, \n\u00d8, \u00d8, mt). The evalCESK evaluation function is de.ned following the tem\u00adplate of the CEK evaluation \ngiven in Section 2.1: evalCESK (e)= {. | inj (e) -. CESK .}. Observe that for any closed expression, \nthe CEK and CESK ma\u00adchines operatein lock-step: each machine transitions,by the corre\u00adsponding rule, \nif and only if the other machine transitions. Lemma 1 (Felleisen,[10]). evalCESK (e) . evalCEK (e). Asecond \nattempt at abstract interpretation: With the CESK ma\u00adchine, half the problem with the attempted na\u00a8ive \nabstract interpre\u00adtation is solved: environments and closures are no longer mutually recursive. Unfortunately, \ncontinuations still have recursive struc\u00adture.We could crudely abstracta continuation intoa setof frames, \nlosingall senseof order,but thiswould leadtoastatic analysis lack\u00adingfaculties to reason about return-.ow:every \ncallwould appear to returntoevery other call.Abetter solutionistorefactor contin\u00aduations as we did environments, \nredirecting the recursive structure through the store. In the next section, we explore a CESK machine \nwith a pointer re.nement for continuations. 2.3 The CESK* machine To untie the recursive structure associated \nwith continuations,we shiftto store-allocated continuations. The Kont component of the machine is replaced \nby a pointer to a continuation allocated in the store. We term the resulting machine the CESK* (control, \nenvironment, store, continuation pointer) machine. Notice the store now maps to denotable values and \ncontinuations: . . S= Exp \u00d7 Env \u00d7 Store \u00d7 Addr s . Storable = Val \u00d7 Env + Kont . . Kont ::= mt | ar(e, \n., a) | fn(v, ., a). The revised machine is de.ned in Figure3and the initial ma\u00adchine state is de.ned \nas: inj CESK* (e)= (e, \u00d8, [a0 . mt],a0).  . -.CESK* . ', where . = s(a),b /. dom(s) (e0, ., s[b . ar(e1, \n., a)],b)(v, ., s, a)if . = ar(e, . ' ,c) (e, . ' ,s[b . fn(v, ., c)],b)if . = fn((.x.e),. ' ,c) (e, \n. ' [x . b],s[b . (v, .)],c) Figure 3. The CESK* machine. The evaluation function (not shown) is de.ned \nalong the same lines as those for the CEK (Section 2.1)and CESK (Section 2.2) machines. Like the CESK \nmachine, it is easy to relate the CESK* machine to its predecessor; from corresponding initial con.gura\u00adtions, \nthese machines operate in lock-step: Lemma 2. evalCESK* (e) evalCESK (e). Addresses, abstraction and \nallocation: The CESK* machine, as de.ned in Figure3, nondeterministically chooses addresses when it allocatesa \nlocationinthe store,but because machines are iden\u00adti.ed up to consistent renaming of addresses, the transition \nsystem remains deterministic. Looking ahead, an easy way to bound the state-space of this machine is \nto bound the set of addresses.3 But once the store is .nite, locations may need to be reused and when \nmultiple values are to reside in the same location; the store will have to soundly approximate this by \njoining the values. In our concrete machine, all that matters about an allocation strategy is that it \npicks an unused address. In the abstracted ma\u00adchine however, the strategy may have to re-use previously \nallo\u00adcated addresses. The abstract allocation strategy is therefore cru\u00adcial to the design of the analysis \nit indicates when .nite resources should be doled out and decides when information should deliber\u00adately \nbe lost in the service of computing within bounded resources. In essence, the allocation strategy is \nthe heart of an analysis (allo\u00adcation strategies corresponding to well-known analyses are given in Section \n2.7.) For this reason, concrete allocation deserves a bit more atten\u00adtion in the machine. An old idea \nin program analysis is that dy\u00adnamically allocated storage can be represented by the state of the computation \nat allocation time[18,22,Section 1.2.2]. That is, allo\u00ad cation strategies can be based on a (representation) \nof the machine history. These representations are often called time-stamps. A common choice for a time-stamp, \npopularized by Shiv\u00aders[29],isto representthe historyofthe computationas contours, .nite strings encoding \nthe calling context.We presenta concrete machine that uses general time-stamp approach and is parameter\u00adized \nby a choice of tick and alloc functions.We then instantiate tick and alloc to obtain an abstract machine \nfor computing a k\u00adCFA-style analysis using the contour approach.  2.4 The time-stamped CESK* machine \nThe machine states of the time-stamped CESK* machine include a time component, which is intentionally \nleft unspeci.ed: t, u . Time . . S= Exp \u00d7 Env \u00d7 Store \u00d7 Addr \u00d7 Time. The machine is parameterized by \nthe functions: tick :S . Time alloc :S . Addr. 3A .nite number of addresses leads to a .nite number of \nenvironments, which leads to a .nite number of closures and continuations, which in turn, leads to a \n.nite number of stores, and .nally, a .nite number of states. . -.CESK* . ', where . = s(a),b = alloc(.),u \n= tick(t) t (x, ., s, a, t) (v, . ' , s, a, u) where (v, . ' )= s(.(x)) ((e0e1), ., s, a, t) (e0, ., \ns[b . ar(e1, ., a)], b, u)(v, ., s, a, t)if . = ar(e, ., c) (e, ., s[b . fn(v, ., c)], b, u)if . = fn((.x.e),. \n' ,c) (e, . ' [x . b],s[b . (v, .)], c, u) Figure 4. The time-stamped CESK* machine. The tick function \nreturns the next time; the alloc function allocates a fresh address fora binding or continuation.We requireof \ntick and alloc that for all t and ., t c tick(t) and alloc(.) ./s where . = ( , ,s, , ). The time-stamped \nCESK* machineis de.nedin Figure4. Note that occurrences of . on the right-hand side of this de.nition \nare implicitly bound to the state occurring on the left-hand side. The initial machine state is de.ned \nas: inj CESK* (e)= (e, \u00d8, [a0 . mt],a0,t0). t Satisfying de.nitions for the parameters are: Time = Addr \n= Z a0 = t0 =0 tick( ,, , ,t) = t +1 alloc( ,, , ,t) = t. Under these de.nitions, the time-stamped CESK* \nmachine oper\u00adates in lock-step with the CESK* machine, and therefore with the CESK and CEK machines as \nwell. Lemma 3. evalCESK* (e) evalCESK* (e). t The time-stamped CESK* machine forms the basis of our \nab\u00adstracted machine in the following section. 2.5 The abstract time-stamped CESK* machine As alluded \nto earlier, with the time-stamped CESK* machine, we now have a machine ready for direct abstract interpretation \nvia a single point of approximation: the store. Our goal is a machine that resembles the time-stamped \nCESK* machine, but operates over a .nite state-space and it is allowed to be nondeterministic. Once the \nstate-space is .nite, the transitive closure of the transition relation becomescomputable, and this transitiveclosure \nconstitutes a static analysis. Buried in a path through the transitive closure is a (possibly in.nite) \ntraversal that corresponds to the concrete execution of the program. The abstracted variant of the time-stamped \nCESK* machine comes from bounding the address space of the store and the number of timesavailable.Bybounding \nthese sets, the state-space becomes .nite,4but for the purposesof soundness, an entryin the store may \nbe forced to hold several values simultaneously: s . S= Addr ..n P (Storable). Store Hence, stores now \nmap an address to a set of storable values rather than a single value. These collections of values model \napproxima\u00adtion in the analysis. If a location in the store is re-used, the new value is joined with the \ncurrent set of values. When a location is dereferenced, the analysis must consider any of the values \nin the set as a result of the dereference. The abstract time-stamped CESK* machine is de.ned in Fig\u00adure5. \nThe (non-deterministic) abstract transition relation changes little comparedwiththe concrete machine.Weonlyhavetomodify \nit to account for the possibility that multiple storable values (which 4Syntactic sets like Exp are in.nite,but \n.nite for anygiven program. Sd . -. E* . ', where . . s (a),b = alloc( .,.),u = tick(t, .) CESK t (x, \n., s, a, t) (v, . ' , ) . s, a, u) where (v, . ' s(.(x)) ((e0e1), ., s, a, t) (e0, ., s U [b . ar(e1, \n., a)], b, u)(v, ., s, a, t)if . = ar(e, . ' ,c) (e, . ' ,s U [b . fn(v, ., c)], b, u)if . = fn((.x.e),. \n' ,c) (e, . ' [x . b],s U [b . (v, .)], c, u) Figure 5. The abstract time-stamped CESK* machine. includes \ncontinuations) may reside together in the store, which we handle by letting the machine non-deterministically \nchoose a par\u00adticularvaluefromthesetatagiven store location. The analysis is parameterized by abstract \nvariants of the func\u00adtions that parameterized the concrete version: dS tick : S \u00d7 Kont . Time, alloc \n: S \u00d7 Kont . Addr. In the concrete, these parameters determine allocation and stack behavior. In the \nabstract, they are the arbiters of precision: they determine when an address gets re-allocated, how manyaddresses \nget allocated, and which values have to share addresses. Recall that in the concrete semantics, these \nfunctions consume states not states and continuations as theydo here. This is because in the concrete, \na state alone suf.ces since the state determines the continuation. But in the abstract, a continuation \npointer within a state may denote a multitude of continuations; however the tran\u00adsition relation is de.ned \nwith respect to the choice of a particular one.We thus pair states with continuations to encode the choice. \nThe abstract semantics computes the set of reachable states: aval E* (e)= {. |(e, \u00d8, [a0 . mt],a0,t0) \n-. E* . }. CESKCESK tt  2.6 Soundness and computability The .niteness of the abstract state-space ensures \ndecidability. Theorem 1 (Decidability of the Abstract CESK* Machine). . . aval E* (e) is decidable. CESK \nt Proof. The state-space of the machine is non-recursive with .nite sets at the leaves on the assumption \nthat addresses are .nite. Hence reachability is decidable since the abstract state-space is .nite. Wehaveendeavoredtoevolvethe \nabstract machine graduallyso that its .delityin soundly simulating the original CEK machine is both intuitive \nand obvious. But to formally establish soundness of the abstract time-stamped CESK* machine, we use an \nabstraction function, de.ned in Figure6, from the state-space of the concrete time-stamped machine into \nthe abstracted state-space. The abstraction map over times and addresses is de.ned so that the parameters \nStick are sound simulations of the alloc and dparameters alloc and tick, respectively.We also de.ne the \npartial order () on the abstract state-space as the natural point-wise, element-wise, component-wise \nand member-wise lifting, wherein the partial orders on the sets Exp and Addr are .at. Then, we can prove \nthat abstract machine s transition relation simulates the concrete machine s transition relation. Theorem \n2 (Soundness of the Abstract CESK* Machine). If . -.CEK . ' and a(.) . , then there exists an abstract \nstate ' '' . ', suchthat . -.E* . and a(. ) . . CESK t Proof. By Lemmas1,2,and3,itsuf.cestoprove soundness \nwith respect to -.CESK* . Assume . -.CESK* . ' and a(.) . . tt a(e, ., s, a, t)=(e, a(.),a(s),a(a),a(t)) \n[states] a(.)= .x.a(.(x)) [environments] G a(s)= .a. {a(s(a))} [stores] a(a)= a a((.x.e),.)=((.x.e),a(.)) \n[closures] a(mt)= mt [continuations] a(ar(e, ., a)) = ar(e, a(.),a(a)) a(fn(v, ., a)) = fn(v, a(.),a(a)), \nFigure 6. The abstraction map, a :SCESK* . CESK. t S E* t Because . transitioned, exactly one of the \nrules from the de.nition of (-.CESK* ) applies.Wesplitby caseson these rules.The rule t for the second \ncase is deterministic and follows by calculation. For the the remaining (nondeterministic) cases, we \nmust show an abstract state exists such that the simulation is preserved. By examining the rules for \nthese cases, we see that all three hinge on the abstract store in . soundly approximating the concrete \nstore in ., which follows from the assumption that a(.) . . 2.7 A k-CFA-like abstract CESK* machine \nIn this section, we instantiate the time-stamped CESK* machine to obtain a contour-based machine; this \ninstantiation forms the basis of a context-sensitive abstract interpreter with polyvariance like that \nfound in k-CFA[29]. In preparation for abstraction, we instantiate the time-stamped machine using labeled \ncall strings. Inside times, we use contours (Contour), which are .nite strings of call site labels that \ndescribe the current context: d . Contour ::= E | \u00a3d. The labeled CESK machine transition relation must \nappropri\u00adately instantiate the parameters tick and alloc to augment the time\u00adstamp on function call. \nNext, we switch to abstract stores and bound the address space by truncating call string contours to \nlength at most k (for k-CFA): - d . Contourk iffd . Contour and |d|= k. Combining these changes, we \narrive at the instantiations for the concreteand abstract machinesgivenin Figure7, wherethevalue LdJk \nis the leftmost k labels of contour d. Comparison to k-CFA: Wesay k-CFA-like rather than k-CFA because \nthere are distinctions between the machine just de\u00adscribed and k-CFA: 1. k-CFA focuses on what .ows where \n; the ordering between states in the abstract transition graph produced by our machine produces what \n.ows where and when. 2. Standard presentations of k-CFAimplicitly inline a global ap\u00adproximation of \nthe store into the algorithm[29]; ours uses one store per state to increase precision at the cost of \ncomplexity. In terms of our framework, the lattice through which classical k-  CFAascendsisP (Exp \u00d7 \nEnv \u00d7 Addr) \u00d7 SStore,whereas our analysis ascends the lattice P Exp \u00d7 Env \u00d7 S. Store \u00d7 Addr We can explicitly \ninline the store to achieve the same complex\u00adity, as shownin Section7. 3. On function call, k-CFAmerges \nargument values together with previous instances of those arguments from the same context; our minimalist \nevolution of the abstract machine takes a  Time =(Lab + ) \u00d7 Contour Addr =(Lab + Var) \u00d7 Contour t0 =( \n,E) tick(x, , , ,t) = t tick((e0e1)\u00a3 ,,,, ( ,d)) =((\u00a3, d) (\u00a3, d), if s(a)= ar( ,, ) tick(v, ,s,a, (\u00a3, \nd)) = ( , \u00a3d), if s(a)= fn( ,, ) alloc(((e \u00a3 0e1),,,, ( ,d)))=(\u00a3, d) alloc((v, ,s,a, ( ,d)))=(\u00a3, d) if \ns(a)= ar(e \u00a3 ,, ) alloc((v, ,s,a, ( ,d)))=(x, d) if s(a)= fn((.x.e),, ) d tick((x, , , ,t),.)= t d\u00a3 tick(((e0e1),,,, \n( ,d)),.)=(\u00a3, d) ( (\u00a3, d), if . = ar( ,, ) d tick((v, , s, a, (\u00a3, d)),.)= ( , L\u00a3dJk), if . = fn( ,, ) \nS\u00a3 alloc(((e0e1),,,, ( ,d)),.)=(\u00a3, d) Ss, a, ( ,d)),.)=(\u00a3, d) if . = ar(e \u00a3 alloc((v, , ,, ) S alloc((v, \n, , s, a, ( ,d)),.)=(x, d) if . = fn((.x.e), ) Figure 7. Instantiation for k-CFAmachine. higher-precision \napproach: it forks the machine for each ar\u00adgument value, rather than merging them immediately. 4. k-CFAdoes \nnot recover explicit information about stack struc\u00adture; our machine contains an explicit model of the \nstack for every machine state. 3. Analyzing by-need with Krivine s machine Even though the abstract machines \nof the prior section have advan\u00adtages over traditional CFAs, the approach we took (store-allocated continuations)yields \nmorenovel resultswhenappliedinadifferent context: a lazy variant of Krivine s machine. That is, we can \ncon\u00adstruct an abstract interpreter that both analyzes and exploits lazi\u00adness. Speci.cally, we present \nan abstract analog toa lazy and prop\u00aderly tail-recursivevariantof Krivine s machine[19,20]derivedby Ager, \nDanvy, and Midtgaard[1]. The derivation from Ager et al. s machine to the abstract interpreter follows \nthe same outline as that of Section2:we applyapointer re.nementbystore-allocating con\u00ad tinuations and \ncarry out approximation by bounding the store. The by-need variant of Krivine s machine considered here \nuses the common implementation technique of store-allocating thunks and forced values. When an application \nis evaluated, a thunk is created that willcompute the value of the argument when forced. When a variable \noccurrence is evaluated, if it is bound to a thunk, the thunkis forced(evaluated)andthe storeis updatedtothe \nresult. Otherwise if a variable occurrence is evaluated and bound to a forced value, that value is returned. \nStorable values include delayed computations (thunks) d(e, .), and computedvalues c(v, .), which are \njusttagged closures. There are twocontinuation constructors: c1(a, .) is inducedbyavariable occurrence \nwhose binding has not yet been forced to a value. The address a is where we want to write the given value \nwhen this continuation is invoked. The other: c2(a, .) is induced by an . -.LK . ' (x, ., s, .) if s(.(x)) \n= d(e, . ' ) (e, . ' , s, c1(.(x),.))if s(.(x)) = c(v, . ' ) (v, . ' , s, .)((e0e1), ., s, .) (e0, ., \ns[a . d(e1,.)], c2(a, .))where a/. dom(s) (v, ., s, c1(a, .)) (v, ., s[a . c(v, .)],.)((.x.e), ., s, \nc2(a, .)) (e, .[x . a], s, .) Figure 8. The LK machine. 'd S . -.LK* . , where . . s (a),b = alloc( .,.),u \n= tick( .,.) t (x, ., s, a, t) (e, . ' ,s U [b . c1(.(x),a)], b, u) if s (.(x)) 3 d(e, . ' ) (x, ., s, \na, t) (v, . ' , s, a, u) if s (.(x)) 3 c(v, . ' ) ((e0e1), ., s, a, t) (e0, ., s ' , b, u)where c = alloc( \n.,.), S s ' (v, ., = s U [c . d(e1,.),b . c2(c, a)] s, a, t) (v, . ' ,s U [a ' . c(v, .)], c, u)if . \n= c1(a ' ,c) ((.x.e), ., s, a, t) (e, . ' [x . a ' s, c, u) ], if . = c2(a ' ,c) Figure 9. The abstract \nLK* machine. application expression, which forces the operator expression to a value. The address a \nis the address of the argument. The concrete state-space is de.ned as follows and the transition relationis \nde.nedin Figure8: . . S= Exp \u00d7 Env \u00d7 Store \u00d7 Kont s . Storable ::= d(e, .) | c(v, .) . . Kont ::= mt \n| c1(a, .) | c2(a, .)  Whenthe control componentisavariable,the machine looksup its stored value, which \nis either computed or delayed. If delayed, a c1 continuation is pushed and the frozen expression is put \nin control. If computed, the value is simply returned. When a value is returned to a c1 continuation, \nthe store is updated to re.ect the computed value. When a value is returned to a c2 continuation,its \nbody is put in control and the formal parameter is bound to the address of the argument. We now refactor \nthe machine to use store-allocated continua\u00adtions; storable values are extended to include continuations: \n. . S= Exp \u00d7 Env \u00d7 Store \u00d7 Addr s . Storable ::= d(e, .) | c(v, .) | . . . Kont ::= mt | c1(a, a) | c2(a, \na). It is straightforward to perform a pointer-re.nement of the LK ma\u00adchine to store-allocate continuations \nas done for the CESK machine in Section2.3and observethelazyvariantofKrivine smachineand its pointer-re.ned \ncounterpart (not shown) operate in lock-step: Lemma 4. evalLK (e) evalLK* (e). After threading time-stamps \nthrough the machine as done in Section 2.4 and de.ning tick and S dalloc analogously to the de.\u00adnitions \ngiven in Section 2.5, the pointer-re.ned machine abstracts directly to yield the abstract LK* machinein \nFigure9.  The abstraction map for this machine is a straightforward struc\u00adtural abstraction similar \nto that given in Section 2.6 (and hence omitted). The abstracted machine is sound with respect to the \nLK* machine, and therefore the original LK machine. Theorem 3 (Soundness of the Abstract LK* Machine). \nIf . -.LK . ' and a(.) . , then there exists an abstract state . ' , suchthat . -. LK* . ' and a(. ' \n) . ' . t Optimizing the machine through specialization: Ager et al. opti\u00admizetheLK machinebyspecializing \napplication transitions.When the operand of an application is a variable, no delayed computa\u00adtion needs \nto be constructed, thus avoiding the construction of space-leaky chains of thunks. Likewise, when the \noperand is a .-abstraction, we can store the corresponding closure as a com\u00adputed value rather than as \na delayed computation. Both of these optimizations, which conserve valuable abstract resources, can be \nadded with no trouble, as shown in Figure 10. Sd . -.LK * . ', where . . s (a),b = alloc( .,.),u = tick(t) \n((ex), ., s, a, t) (e, ., s U [b . c2(.(x),a)], b, u)((ev), ., s, a, t) (e0, ., s U [b . c(v, .),c . \nc2(b, a)], c, u)where c = alloc( .,.) S Figure 10. The abstract optimized LK* machine. Varying the machine \nthrough postponed thunk creation: Ager et al. alsovarytheLK machineby postponingthe constructionofa delayed \ncomputation from the point at which an applicationis the control string to the point at which the operator \nhas been evaluated and is being applied. The c2 continuation is modi.ed to hold, rather than the address \nof a delayed computation, the constituents of the computation itself: . . Kont ::= mt | c1(a, a) | c2(e, \n., a). The transitions for applications and functions are replaced with thosein Figure11. This allocates \nthunks whenafunctionis applied, rather than when the control string is an application. As Ager et al. \nremark, each of these variants gives rise to an abstract machine. From each of these machines, we are \nable to systematically derive their abstractions. 4. State and control We have shown that store-allocated \ncontinuations make abstract interpretation of the CESK machine and a lazy variant of Krivine s machine \nstraightforward. In this section, we want to show that the tight correspondence between concrete and \nabstract persists after the addition of language features such as conditionals, side effects,exceptions \nand continuations.We tackle each feature, and present the additional machinery required to handle each \none. In most cases, the path from a canonical concrete machine to pointer\u00adre.ned abstraction of the machine \nis so simple we only show the abstracted system. In doing so, we are arguing that this abstract machine-oriented \napproach to abstract interpretation represents a .exible and viable framework forbuilding abstract interpreters. \n 4.1 Conditionals, mutation, and control To handle conditionals, we extend the language with a new syn\u00adtactic \nform, (if eee), and introduce a base value #f, rep\u00adresenting false. Conditional expressions induce a \nnew continua\u00adtion form: if(e0' ,e 1' , ., a), which represents the evaluation context Sd . -.E. ', where \n. . s (a),b = alloc( .,.),u = tick(t) LK * ((e0e1), ., s, a) (e0, ., s U [b . c2(e1, ., a)],b) ((.x.e), \n., s, a) (e, .[x . b],s U [b . d(e ' ,. ' )],c) if . = c2(e ' ,. ' ,c) Figure 11. The abstract thunk \npostponing LK* machine. Sd . -. E* . ', where . . s (a),b = alloc( .,.),u = tick(t) CESK t ((if e0 e1 \ne2), ., s, a, t) (e0, ., s U [b . if(e1,e2, ., a)], b, u)(#f, ., s, a, t) (e1,. ' , s, c, u)if . = if(e0,e1,. \n' ,c) (v, ., s, a, t) (e0,. ' , s, c, u)if . = if(e0,e1,. ' ,c), and v = #f ((set! xe), ., s, a, t) (e, \n., s U [b . set(.(x),a)], b, u)(v, ., s, a, t) (v ' , ., s U [a ' . v], c, u)if . = set(a ' ,c) where \nv ' . s (a ' ) ((.x.e), ., s, a, t) (e, .[x . b],s U [b . c], c, u)if . = fn(callcc,. ' ,c) S where c \n= alloc( .,.) (c, ., s, a, t) (a, ., s, c, u)if . = fn(callcc,. ' ,a ' ) (v, ., s, a, t) (v, ., s, c, \nu)if . = fn(c, . ' ,a ' ) Figure 12. The abstract extended CESK* machine. E[(if [] e0 e1)] where . closes \ne0 ' to represent e0, . closes e1 ' to represent e1, and a is the address of the representation of E. \nSide effects are fully amenable to our approach; we introduce Scheme s set! for mutatingvariables using \nthe (set! xe) syntax. The set! form evaluates its subexpression e and assigns the value to the variable \nx. Although set! expressions are evaluated for effect, we follow Felleisen et al. and specify set! expressions \nevaluateto thevalueofx beforeitwas mutated[11,page166].The evaluation context E[(set! x [])] is represented \nby set(a0,a1), where a0 is the address of x s value and a1 is the address of the representation of E. \nFirst-class control is introduced by adding a new base value callcc which rei.es the continuation as \na new kind of applicable value. Denoted values are extended to include representations of continuations. \nSince continuations are store-allocated, we choose to representthembyaddress.Whenan addressis applied,it \nrepresents the application of a continuation (rei.ed via callcc)to a value. The continuation at that \npoint is discarded and the applied address is installed as the continuation. The resulting grammar is: \ne . Exp ::= ... | (if eee) | (set! xe) . . Kont ::= ... | if(e, e, ., a) | set(a, a) v . Val ::= ... \n| #f | callcc | a. We show only the abstract transitions, which result from store\u00adallocating continuations, \ntime-stamping, and abstracting the con\u00adcrete transitions for conditionals, mutation, and control. The \n.rst three machine transitions deal with conditionals; here we follow the Scheme tradition of considering \nall non-false values as true. The fourth and .fth transitions deal with mutation. ' . -.CESHK . (v, \n., s, ., .)((throw v), ., s, hn((.x.e),. ' ,. ' ,.),.)(e, . ' [x . a],s[a . (v, .)], ., . ' )where a/. \ndom(s) ((catch ev), ., s, ., .) (e, ., s, hn(v, ., ., .), mt) Figure 13. The CESHK machine. The remaining \nthree transitions deal with .rst-class control. In the .rst of these, callcc is being applied to a closure \nvalue v. The value v is then called with the current continuation , i.e., v is applied to a value that \nrepresents the continuation at this point. In the second, callcc is being applied to a continuation (address). \nWhen this value is applied to the rei.ed continuation, it aborts the current computation, installs itself \nas the current continuation, and puts the rei.ed continuation in the hole . Finally, in the third, a \ncontinuation is being applied; c gets thrown away, and v gets plugged into the continuation b. In all \ncases, these transitions result from pointer-re.nement, time-stamping, and abstraction of the usual machine \ntransitions.  4.2 Exceptions and handlers To analyze exceptional control .ow,we extend the CESK machine \nwith a register to hold a stack of exception handlers. This models a reduction semantics in which we \nhave two additional kinds of evaluation contexts: E ::= [ ] | (Ee) | (vE) | (catch Ev) F ::= [ ] | (Fe) \n| (vF ) H ::= [ ] | H[F [(catch Hv)]], and the additional, context-sensitive, notions of reduction: '' \n' (catch E[(throw v)] v ) . (vv), (catch vv ) . v. H contexts represent a stack of exception handlers, \nwhile F con\u00adtexts represent a local continuation, i.e., the rest of the compu\u00adtation (with respect to \nthe hole) up to an enclosing handler, if any. E contexts represent the entire rest of the computation, \nincluding handlers. The language is extended with expressions for raising and catchingexceptions.A new \nkindof continuationis introducedto represent a stack of handlers. In each frame of the stack, there is \na procedure for handling an exception and a (handler-free) continua\u00adtion: e . Exp ::= ... | (throw v) \n| (catch e (.x.e)) . . Handl ::= mt | hn(v, ., ., .) An . continuation representsa stackofexception handler \ncontexts, i.e., hn(v ' , ., ., .) represents H[F [(catch [] v)]], where . represents H, . represents \nF , and . closes v ' to represent v. The machine includes all of the transitions of the CESK ma\u00adchine \nextended with a . component; these transitions are omitted for brevity. The additional transitions are \ngiven in Figure 13. This presentation is based on a textbook treatment of exceptions and handlers[11, \npage 135].5 The initial con.gurationisgivenby: inj CESHK (e)= (e, \u00d8, \u00d8, mt, mt). 5To be precise, Felleisen \net al. present the CHC machine, a substitution based machine that uses evaluation contexts in place of \ncontinuations. Deriving the CESHK machine from it is an easy exercise. . -.CESHK* . ', where . = s(h),. \n= s(a),b /. dom(s) (v, ., s, h, a) (v, ., s, h ' ,a ' )if . = hn(v ' ,. ' ,a ' ,h ' ), and . = mt ((throw \nv), ., s, h, a) (e, . ' [x . b],s[b . (v, .)],h ' ,a ' )if . = hn((.x.e),. ' ,a ' ,h ' ) ((catch ev), \n., s, h, a) (e, ., s[b . hn(v, ., a, h)], b, amt) Figure 14. The CESHK* machine. S . -. E* . ', where \n. . s (h),. . s (a),b = alloc( ., ., .), CESHK t d u = tick(t) (v, ., s, h, a, t) (v, ., s, h ' ,a ' \n,u)if . = hn(v ' ,. ' ,a ' ,h ' ), and . = mt ((throw v), ., s, h, a, t)if . = hn((.x.e),. ' ,a ' ,h \n' ) (e, . ' [x . b],s U [b . (v, .)],h ' ,a ' ,u)((catch ev), ., s, h, a, t)(e, ., s U [b . hn(v, ., \na, h)], b, amt,u) Figure 15. The abstract CESHK* machine. In the pointer-re.ned machine, the grammar \nof handler contin\u00aduations changes to the following: . . Handl ::= mt | hn(v, ., a, h), where h is used \nto range over addresses pointing to handler con\u00adtinuations. The notation amt means a such that s(a)= \nmt in concrete case and mt . s (a) in the abstract, where the intended store should be clear from context. \nThe pointer-re.ned machine is given in Figure 14. After threading time-stamps through the machine as \ndone in Section 2.4, the machine abstracts as usual to obtain the machine in Figure 15. The only unusual \nstep in the derivation is to observe that some machine transitionsrelyonachoiceof two continuations from \nthe store; a handler and a local continuation. Analogously to Section 2.5, we extend dalloc to take two \ncontinuation tick and Sarguments to encode the choice: d tick : S \u00d7 Handl \u00d7 Kont . Time, S alloc : S \n\u00d7 Handl \u00d7 Kont . Addr. 5. Abstract garbage collection Garbage collection determines when a store location \nhas become unreachable and can be re-allocated. This is signi.cant in the ab\u00adstract semantics because \nan address may be allocated to multiple valuesdueto .nitenessofthe address space.Withoutgarbagecol\u00adlection,thevaluesallocatedtothis \ncommon addressmustbejoined, introducing imprecision in the analysis (and inducing further, per\u00adhaps spurious, \ncomputation).By incorporatinggarbage collection in the abstract semantics, the location may be proved \nto be un\u00adreachable and safely overwritten rather than joined, in which case no imprecision is introduced. \nLike the rest of the features addressed in this paper, we can incorporate abstract garbage collection \ninto our static analyzers ' * . . -.CESK (e, ., s, a) (e, ., {(b, s(b))| b . L},a)if (LLs(e, .) . LLs(s(a)), \n{a},s) -. GC (\u00d8, L,s) Figure 16. The GC transition for the CESK* machine. by a straightforward pointer-re.nement \nof textbook accounts of concretegarbage collection, followedbya .nite store abstraction. Concretegarbage \ncollectionis de.nedin termsofaGC machine that computes the reachable addresses in a store[11, page 172]: \n(G, B,s) -.GC ((G .LLs(s(a)) \\ (B.{a})), B.{a},s) if a .G. This machine iteratesoverasetof reachablebutunvisited \ngrey locations G. On each iteration, an element is removed and added to the set of reachable and visited \nblack locations B. Anynewly reachable and unvisited locations, as determined by the live loca\u00adtions function \nLLs, are added to the greyset. When there are no grey locations, the black set contains all reachable \nlocations. Ev\u00aderything elseisgarbage. The live locations function computes a set of locations which maybeusedinthe \nstore.Its de.nitionwillvarybasedonthepartic\u00adular machine beinggarbage collected,but the de.nition appropriate \nfor the CESK* machine of Section 2.3 is LLs(e)= \u00d8 LLs(e, .)= LLs(.|fv(e)) LLs(.)= rng(.) LLs(mt)= \u00d8 LLs(fn(v, \n., a)) = {a} . LLs(v, .) . LLs(s(a)) LLs(ar(e, ., a)) = {a} . LLs(e, .) . LLs(s(a)). We write.|fv(e) \nto mean . restricted to the domain of free vari\u00adables in e.We assume the least-.xed-point solution in \nthe calcula\u00adtion of the function LL in cases where it recurs on itself. The pointer-re.nement of the \nmachine requires parameterizing the LL function with a store used to resolve pointers to continua\u00adtions.A \nnice consequenceof this parameterizationisthat we can re-use LL for abstract garbage collection by supplying \nit an ab\u00adstract store for the parameter. Doing so only necessitates extending LL to the case of sets \nof storable values: [ LLs(S)= LLs(s) s.S The CESK* machine incorporatesgarbage collectionbya tran\u00adsition \nrule that invokes the GC machine as a subroutine to remove garbage from the store (Figure 16). Thegarbage \ncollection transi\u00adtion introduces non-determinism to the CESK* machine because it applies to anymachine \nstate and thus overlaps with the existing transition rules. The non-determinism is interpreted as leaving \nthe choice of when to collectgarbageuptothe machine. The abstract CESK* incorporates garbage collection \nby the concrete garbagecollection transition, i.e.,we re-use the de.nition in Figure 16 with an abstract \nstore, s , in place of the concrete one. Consequently,itis easytoverify abstractgarbage collection approximates \nits concrete counterpart. The CESK* machine may collectgarbage at any point in the computation, thus \nan abstract interpretation must soundly approx\u00adimate all possible choices of when to trigger a collection, \nwhich the abstract CESK* machine does correctly. This may be a useful analysis of garbage collection,howeveritfailstobea \nuseful anal\u00adysis with garbage collection: for soundness, the abstracted machine must consider the casein \nwhichgarbageis never collected, imply\u00ading no storage is reclaimed to improve precision. However, we \ncanleverage abstractgarbage collectionto reduce the state-space explored during analysis and to improve \nprecision and analysis time. This is achieved (again) by considering proper\u00adties of the concrete machine, \nwhich abstract directly; in this case, wewantthe concrete machineto deterministically collectgarbage. \nDeterminism of the CESK* machine is restored by de.ning the transition relation asanon-GC transition \n(Figure3)followedbythe GC transition (Figure 16).This state-space of this concrete machine is garbage \nfree and consequentlythe state-spaceofthe abstracted machineis abstractgarbage free. Inthe concrete semantics,anice \nconsequenceofthis propertyis that although continuations are allocated in the store, theyare deal\u00adlocated \nas soon as theybecome unreachable, which corresponds to whentheywouldbepoppedfromthestackina non-pointer-re.ned \nmachine. Thus the concrete machine really manages continuations like a stack. Similarly, in the abstract \nsemantics, continuations are deallo\u00adcated as soon as theybecome unreachable, which often corresponds \ntowhentheywouldbe popped.Wesay often,becauseduetothe .niteness of the store, this correspondence cannot \nalways hold. However, this approach gives a good .nite approximation to in\u00ad.nitary stack analyses that \ncan always match calls and returns. 6. Abstract stack inspection In this section, we derive an abstract \ninterpreter for the static analy\u00adsisofa higher-order language with stack inspection.Following the outlineof \nSection2and3, we start from the tail-recursiveCM ma\u00adchineofClementsand Felleisen[3], performa pointer \nre.nement on continuations, then abstract the semantics by a parameterized bounding of the store. 6.1 \nThe .sec-calculus and stack-inspection The .sec-calculus of Pottier, Skalka, and Smith is a call-by-value \n.-calculus modelof higher-order stack inspection[26].We present the language as given by Clements and \nFelleisen[3]. All code is statically annotated with a given set of permissions R, chosen from a .xed \nset P.A computation whose source code was statically annotated witha permission may enable that permis\u00adsion \nfor the dynamic extent of a subcomputation. The subcompu\u00adtation is privileged so long as it is annotated \nwith the same per\u00admission, and every intervening procedure call has likewise been annotated with the \nprivilege. e . Exp ::= ... | fail | (grant Re) |(test Ree) | (frame Re) Afail expression signalsanexceptionifevaluated;byconvention \nitis usedto signala stack-inspectionfailure.A(frame Re) eval\u00aduates e as the principal R, representing \nthe permissions conferred on e given its origin.A(grant Re) expression evaluates as e but with the permissions \nextended with R enabled.A(test Re0 e1) expression evaluates to e0 if R is enabled and e1 otherwise. Atrusted \nannotator consumes a program and the set of permis\u00adsions it will operate under and inserts frame expressions \naround each .-bodyand intersectsallgrantexpressionswiththissetofper\u00admissions.We assume all programshave \nbeen properly annotated. Stack inspection can be understood in terms of an OK predi\u00adcate on an evaluation \ncontexts and permissions. The predicate de\u00adtermines whether the given permissions are enabled for a subex\u00adpression \nin the hole of the context. The OK predicate holds when\u00adever the context can be traversed from the hole \noutwards and, for each permission, .nd an enabling grant context without .rst .nding a denying frame \ncontext. . -.CM . ' (fail, ., s, mt\u00d8) ((frame Re), ., s, .) (e, ., s, .[R . deny])((grant Re), ., s, \n.) (e, ., s, .[R . grant]) ( (e0, ., s, .) if OK(R, .),((test Re0 e1), ., s, .) (e1, ., s, .) otherwise \nOK(\u00d8,.) OK(R, mtm) .. (R n m -1(deny)= \u00d8) . OK(R, fnm(v, ., .)) (R n m -1(deny)= \u00d8) . OK(R, ar m(e, ., \n.)) .. OK(R \\ m -1(grant),.) Figure 17. The CM machine and OK predicate.  6.2 The CM machine The CM \n(continuation-marks) machine of Clements and Felleisen isaproperly tail-recursiveextended CESK machine \nfor interpreting higher-order languages with stack-inspection[3]. In the CM machine, continuations are \nannotated with marks [4], which, for the purposes of stack-inspection, are .nite maps from permissions \nto {deny, grant}: . ::= mtm | ar m(e, ., .) | fnm(v, ., .). We write.[R . c] to mean update the marks \non . to m[R . c]. The CM machine is de.ned in Figure 17 (transitions that are straightforward adaptations \nof the corresponding CESK* transi\u00adtions to incorporate continuation marks are omitted). It relies on \nthe OK predicate to determine whether the permissions in R are enabled. The OK predicate performs the \ntraversal of the context (represented as a continuation) using marks to determine which permissions have \nbeen granted or denied. The semantics of a program is given by the set of reachable states from an initial \nmachine con.guration: inj CM (e)= (e, \u00d8, [a0 . mt \u00d8],a0).  6.3 The abstract CM* machine Store-allocating \ncontinuations, time-stamping, and bounding the store yields the transition system given in Figure 18. \nThe notation s (a)[R . c] is used to mean [R . c] should update some continuation in s (a), i.e., s (a)[R \n. c]= s [a . s (a) \\{.}.{.[R . c]}], for some . . s (a). It is worth noting that continuation marks are \nupdated, not joined, in the abstract transition system. The S OK* predicate (Figure 18)approximates the \npointer re.ne\u00ad ment of its concrete counterpart OK, which can be understood as tracingapaththroughthestore \ncorrespondingtotraversingthecon\u00adtinuation. The abstract predicate holds whenever thereexists sucha path \nin the abstract store that would satisfy the concrete predicate: Consequently,in analyzing (test Re0 \ne1), e0 is reachable only when the analysis can prove the OK * predicate holds on some path through the \nabstract store. It is straightforward to de.ne a structural abstraction map and verify the abstract CM* \nmachineisa sound approximationof its concrete counterpart: Theorem 4 (Soundness of the Abstract CM* Machine). \nIf . -.CM . ' and a(.) . , then there exists an abstract state . ' , ' '' suchthat . -. CM * . and a(. \n) . . t . -.* . ' CM (fail, ., s, a) (fail, ., s, amt) ((frame Re), ., s, a) (e, ., s (a)[R . deny],a)((grant \nRe), ., s, a) (e, ., s (a)[R . grant],a) ( (e0, ., if Ss, a), s, a) OK* (R, ((test Re0 e1), ., s, a) \n(e1, ., otherwise. s, a) S OK* (\u00d8, s, a) S-1 OK* (R, s, a ) .. (R n m (deny)= \u00d8) if s (a) 3 mtm S-1 OK* \n(R, s, a ) .. (R n m (deny)= \u00d8) . if s (a) 3 fnm(v, ., b) S-1(grant), OK* (R \\ m s,b) or s (a) 3 ar \nm(e, ., b) Figure 18. The abstract CM* machine. 7. Widening to improve complexity If implemented na\u00a8ively, \nit takes time exponential in the size of the input program to compute the reachable states of the abstracted \nmachines. Consider the size of the state-space for the abstract time\u00adstamped CESK* machine: |Exp \u00d7 Env \n\u00d7 S Store \u00d7 Addr \u00d7 Time| = |Exp|\u00d7|Addr||Var| \u00d7|Storable||Addr| \u00d7|Addr|\u00d7|Time|. Without simplifying any \nfurther, we clearly have an exponential number of abstract states. To reduce complexity, we can employwidening \nin the form of Shivers s single-threaded store[29].To usea single threaded store, wehaveto reconsiderthe \nabstractevaluation function itself. Instead of seeing it as a function that returns the set of reachable \nstates, it isafunctionthat returnsasetofpartialstatesplusasingleglobally approximating store, i.e., aval \n: Exp . System, where: System = P (Exp \u00d7 Env \u00d7 Addr \u00d7 Time) \u00d7 S Store. We compute this as a .xed point \nof a monotonic function,f: f : System . System f(C, s ) = (C ' ,s '' ) where .\u00af ' '' '' Q =(c ,s ): c \n. C and (c, s ) -. (c ,s ) = inj (e)(c0,s 0) ~ .\u00af ' ''' C = C . c :(c, ) . Q .{c0} G s '' = s U s ' , \n( ,s ).Q  so that aval(e) = lfp(f). The maximum number of iterations of the function f times the cost \nof each iteration bounds the complex\u00adity of the analysis. Polynomial complexityfor monovariance: It is \nstraightforward to compute the cost of a monovariant (in our framework, a 0CFA\u00adlike ) analysis with this \nwidening. In a monovariant analysis, envi\u00adronments disappear; a monovariant system-space simpli.es to: \nSystem0 = P (Exp \u00d7 Lab \u00d7 Lab.) addresses fn conts ar conts z }| {z }| {z }| { \u00d7 ((Var + Lab) . (Exp \u00d7 \nLab)+ (Exp \u00d7 Lab)+Lam).  If ascended monotonically, one could add one new partial state each time or \nintroduce a new entry into the global store. Thus, the maximum number of monovariant iterations is: |Exp|\u00d7|Lab|2 \n+1 + |Var + Lab|\u00d7 (2|Exp \u00d7 Lab| + |Lam|), which is cubic in the size of the program. 8. Related work \nThe study of abstract machines for the .-calculus began with Landin s SECD machine[21], thetheory of \nabstract interpreta\u00ad tion with the POPL papers of the Cousots [6,7], and static analy\u00ad sis of the .-calculus \nwith Jones s coupling of abstract machines and abstract interpretation[17]. All three have been active \nar\u00ad eas of research since their inception,but only recently have well known abstract machines been connected \nwith abstract interpreta\u00adtionbyMidtgaardand Jensen[23,24].We strengthenthe connec\u00ad tion by demonstrating \na general technique for abstracting abstract machines. Abstract interpretation of abstract machines: \nThe approxima\u00adtion of abstract machine states for the analysis of higher-order lan\u00adguages goes back to \nJones[17], who argued abstractions of regular tree automata could solve the problem of recursive structure \nin en\u00advironments.We re-invoked that wisdom to eliminate the recursive structureof continuationsby allocating \nthemin the store. Midtgaard and Jensen presenta 0CFAfora CPS .-calculus lan\u00adguage[23].The approach is \nbased on Cousot-style calculationalab\u00ad stract interpretation[5], appliedtoa functional language.Likethe \npresent work, Midtgaard and Jensen start with an off-the-shelf abstract machine for the concrete semantics \n(in this case, the CE machine of Flanagan, et al. [14]) and employa reachable-states model. Theythen \ncompose well-known Galois connections to re\u00adveala 0CFA with reachabilityin the styleofAyers[2].6 TheCE \nmachine is not suf.cient to interpret direct-style programs, so the analysis is specialized to programs \nin continuation-passing style. Later work by Midtgaard and Jensen went on to present a similar calculational \nabstract interpretation treatment of a monomorphic CFAfor an ANF .-calculus[24].The concrete semantics \nare based on reachable statesoftheCaEK machine[14].The abstract seman\u00ad tics approximatethe control stack \ncomponentofthe machinebyits top element, which is similar to the labeled machine abstraction given in \nSection 2.7 when k =0. Although our approach is not calculational like Midtgaard and Jensen s, it continues \nin their tradition by applying abstract inter\u00adpretation to off-the-shelf tail-recursive machines. We \nextend the application to direct-style machines for a k-CFA-like abstraction that handles tail calls, \nlaziness, state, exceptions, .rst-class contin\u00aduations, and stack inspection.Wehaveextended return .ow \nanal\u00adysis to a completely direct style (no ANF or CPS needed) within a framework that accounts for polyvariance. \nHarrison gives an abstract interpretation for a higher-order lan\u00adguage with control and state for the \npurposes of automatic paral\u00adlelization[15]. Harrison maps Scheme programs into an impera\u00ad tive intermediate \nlanguage, which is interpreted on a novel abstract machine. The machine uses a procedure string approach \nsimilar to that given in Section 2.7 in that the store is addressed by proce\u00ad dure strings.Harrison s \n.rst machine employs higher-ordervalues to represent functions and continuations and he notes, the straight\u00adforward \nabstraction of this semantics leads to abstract domains 6Ayers derived an abstract interpreter by transforming \n(the representation of) a denotational continuation semantics of Scheme into a state transition system \n(an abstract machine), which he then approximated using Galois connections[2]. containing higher-order \nobjects (functions)over re.exive domains, whereas our purpose requires a more concrete compile-time repre\u00adsentationof \nthevalues assumedbyvariables.We therefore modify the semantics suchthat its abstraction resultsin domains \nwhich are both .nite and non-re.exive. Because of the re.exivity of deno\u00adtable values, a direct abstraction \nis not possible, so he performs closure conversion on the (representation of) the semantic func\u00adtion. \nHarrison then abstracts the machine by bounding the proce\u00addure string space (and hence the store) via \nan abstraction he calls stack con.gurations, which is represented by a .nite set of mem\u00adbers, each of \nwhich describes an in.nite set of procedure strings. To prove that Harrison s abstract interpreter is \ncorrect he argues that the machine interpreting the translation of a program in the intermediate language \ncorresponds to interpreting the program as written in the standard semantics in this case, the denotational \nsemantics ofR3RS. On the other hand, our approach relies on well known machines with well known relations \nto calculi, reduction semantics, and other machines[10,8]. These connections, coupled with the strong \nsimilarities between our concrete and abstract ma\u00adchines, result in minimal proof obligationsin comparison. \nMore\u00adover, programs are analyzed in direct-style under our approach. Abstract interpretation of lazy \nlanguages: Jones has analyzed non-strict functional languages[17,16],but thatwork has only fo\u00ad cused \non the by-name aspect of laziness and does not address mem\u00adoization as done here. Sestoft examines .ow \nanalysis for lazy lan\u00adguages and uses abstract machines to prove soundness[27]. In par\u00ad ticular, Sestoft \npresents a lazy variant of Krivine s machine similar to thatgivenin Section3and proves analysisis sound \nwith respect to the machine. Likewise, Sestoft uses Landin s SECD machine as the operational basis for \nproving globalization optimizations cor\u00adrect. Sestoft s work differs from ours in that analysis is developed \nseparately from the abstract machines, whereas we derive abstract interpreters directly from machine \nde.nitions.Fax\u00b4en usesa type\u00adbased .ow analysis approach to analyzing a functional language with explicit \nthunks and evals, which is intended as the intermedi\u00adate language fora compilerofa lazy language[9].In \ncontrast, our approach makes no assumptions about the typing discipline and an\u00adalyzes source code directly. \nRealistic languagefeatures and garbage collection: Static an\u00adalyzers typically hemorrhage precision in \nthe presence of excep\u00adtions and .rst-class continuations: theyjump to the top of the lattice of approximation \nwhen these features are encountered. Conversion to continuation-and exception-passing style can handle \nthese fea\u00adtures without forcing a dramatic ascent of the lattice of approxima\u00adtion[29].The cost of this \nconversion, however,is lost knowledge both approaches obscure static knowledge of stack structure, by \ndesugaring it into syntax. MightandShivers introducedtheideaofusing abstractgarbage collection to improveprecision \nand ef.ciencyin .owanalysis[25]. Theydevelopagarbage collecting abstract machine fora CPS lan\u00adguageandproveit \ncorrect.Weextend abstractgarbage collection to direct-style languages interpreted on the CESK machine. \nStatic stack inspection: Most work on the static veri.cation of stack inspection has focused on type-based \napproaches. Skalka and Smith present a type system for static enforcement of stack\u00adinspection[30]. Pottier \net al. present type systems for enforcing stack-inspection developed via a static correspondence to the \ndy\u00adnamic notion of security-passing style[26]. Skalka et al. present type and effect systems that use \nlinear temporal logic to express regular properties of program traces and show how to statically en\u00adforce \nboth stack-and history-based security mechanisms[31]. Our approach, in contrast, is not typed-based and \nfocuses only on stack\u00adinspection, although it seems plausible the approach of Section6 extends to the \nmore general history-based mechanisms. 9. Conclusions and perspective We have demonstrated the utility \nof store-allocated continuations by deriving novel abstract interpretations of the CEK, a lazy vari\u00adant \nof Krivine s, and the stack-inspecting CM machines. These abstract interpreters are obtained by a straightforward \npointer re\u00ad.nement and structural abstraction that bounds the address space, making the abstract semantics \nsafe and computable. Our technique allows concrete implementation technology to be mapped straight\u00adforwardlyintothatof \nstatic analysis,whichwe demonstratedbyin\u00adcorporating abstractgarbage collectionand optimizationstoavoid \nabstract space leaks, both of which are based on existing accounts of concrete GC and space ef.ciency. \nMoreover, the abstract inter\u00adpreters properly model tail-calls by virtue of their concrete coun\u00adterparts \nbeing properly tail-call optimizing. Finally, our technique uniformlyscalesupto richer language features.Wehave \nsupported thisbyextending the abstract CESK machine to analyze condition\u00adals, .rst-class control,exception \nhandling, and state.We speculate that store-allocating bindings and continuations is suf.cient for a \nstraightforward abstraction of most existing machines. Acknowledgments: We thank Matthias Felleisen, \nJan Midtgaard, and SamTobin-Hochstadt for discussions and suggestions.Wealso thank the anonymous reviewers \nfortheir close reading and helpful critiques; their comments have improved this paper. References [1] \nMads S. Ager, Olivier Danvy, and Jan Midtgaard. A functional correspondence between call-by-need evaluators \nand lazy abstract machines. Information Processing Letters, 90(5):223 232, June 2004. [2] AndrewE.Ayers. \nAbstract analysis and optimization of Scheme. PhD thesis, Massachusetts InstituteofTechnology, 1993. \n[3] John Clements and Matthias Felleisen.Atail-recursive machine with stack inspection. ACMTrans. Program. \nLang. Syst., 26(6):1029 1052, November 2004. [4] John Clements, Matthew Flatt, and Matthias Felleisen. \nModeling an algebraic stepper. In ESOP 01: Proceedings of the 10th European Symposium on Programming \nLanguages and Systems, pages 320 334, 2001. [5] Patrick Cousot. The calculational design of a generic \nabstract interpreter. In M. Broyand R. Steinbr\u00a8 uggen, editors, Calculational System Design. 1999. [6]Patrick \nCousot and Radhia Cousot. Abstract interpretation:Auni.ed lattice model for static analysis of programs \nby construction or approximation of .xpoints. In Conference Record of the Fourth ACM Symposium on Principles \nof Programming Languages, pages 238 252, 1977. [7] Patrick Cousot and Radhia Cousot. Systematic design \nof program analysis frameworks. In POPL 79: Proceedings of the 6thACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Languages, pages 269 282, 1979. [8] Olivier Danvy. An Analytical Approachto Program as \nData Objects. DSc thesis, Department of Computer Science, Aarhus University, October 2006. [9] Karl Fax\u00b4Optimizing \nlazy functional programs using .ow en. inference. In Static Analysis, pages 136 153. 1995. [10] Matthias \nFelleisen. The Calculi of Lambda-v-CS Conversion: A Syntactic Theory of Control and State in Imperative \nHigher-Order Programming Languages. PhD thesis, Indiana University, 1987. [11] Matthias Felleisen, Robert \nB. Findler, and Matthew Flatt. Semantics Engineering with PLTRedex. August 2009. [12] Matthias Felleisen \nand DanielP. Friedman. Control operators, the SECD-machine, and the lambda-calculus. In 3rdWorking Conference \non theFormal DescriptionofProgramming Concepts, August 1986. [13] Mattias Felleisen and D.P. Friedman. \nA calculus for assignments in higher-order languages. In POPL 87: Proceedings of the 14th ACM SIGACT-SIGPLAN \nSymposium on Principles of Programming Languages, pages 314+, 1987. [14] Cormac Flanagan, Amr Sabry,BruceF. \nDuba, and Matthias Felleisen. The essence of compiling with continuations. In PLDI 93: ProceedingsoftheACM \nSIGPLAN1993 ConferenceonProgramming Language Design and Implementation, pages 237 247, June 1993. [15] \nWilliams L. Harrison. The interprocedural analysis and automatic parallelization of scheme programs. \nLISP and Symbolic Computation, 2(3):179 396, October 1989. [16] N. Jones and N. Andersen. Flow analysis \nof lazy higher-order functional programs. Theoretical Computer Science, 375(1-3):120 136, May 2007. [17] \nNeil D. Jones. Flow analysis of lambda expressions (preliminary version). In Proceedings of the 8th Colloquium \non Automata, Languages and Programming, pages 114 128, 1981. [18] Neil D. Jones and Steven S. Muchnick. \nA .exible approach to interprocedural data .ow analysis and programs with recursive data structures. \nIn POPL 82: Proceedings of the 9thACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,pages \n66 74, 1982. [19] Jean-Louis Krivine. Un interpr\u00b4eteurdu lambda-calcul. 1985. [20] Jean-Louis Krivine. \nA call-by-name lambda-calculus machine. Higher-Order and Symbolic Computation,20(3):199 207, September \n2007. [21] Peter J. Landin. The mechanical evaluation of expressions. The ComputerJournal, 6(4):308 320, \n1964. [22] Jan Midtgaard. Control-.ow analysis of functional programs. Tech\u00adnical Report BRICS RS-07-18,DAIMI, \nDepartment of Computer Science, University of Aarhus, December 2007. To appear in revised forminACM Computing \nSurveys. [23] Jan Midtgaard and Thomas Jensen. A calculational approach to control-.ow analysisby abstract \ninterpretation. In Mar\u00b4ia Alpuente and Germ\u00b4 anVidal, editors, SAS, volume 5079 of Lecture Notes in Computer \nScience, pages 347 362, 2008. [24] Jan Midtgaard and ThomasP. Jensen. Control-.ow analysis of function \ncalls and returns by abstract interpretation. In ICFP 09: Proceedingsof the 14thACM SIGPLAN International \nConference on Functional Programming, pages 287 298, 2009. [25] MatthewMight and Olin Shivers. Improving \n.owanalyses via GCFA: Abstractgarbage collection and counting. In ICFP 06: Proceedings of the Eleventh \nACM SIGPLAN International Conference on Functional Programming, pages 13 25, 2006. [26] Franc\u00b8ois Pottier, \nChristian Skalka, and Scott Smith. A systematic approach to static access control. ACMTrans. Program. \nLang. Syst., 27(2):344 382, March 2005. [27] Peter Sestoft. Analysis and ef.cient implementation of functional \nprograms. PhD thesis, University of Copenhagen, October 1991. [28] Zhong Shao and Andrew W. Appel. Space-ef.cient \nclosure representations. In LFP 94: Proceedings of the 1994 ACM Conference on LISP and Functional Programming, \npages 150 161, 1994. [29] Olin G. Shivers. Control-Flow Analysis of Higher-Order Languages. PhD thesis, \nCarnegie Mellon University, 1991. [30] Christian Skalka and Scott Smith. Static enforcement of security \nwith types. In ICFP 00: Proceedings of the .fthACM SIGPLAN International Conference on Functional Programming, \npages 34 45, September 2000. [31] Christian Skalka, Scott Smith, and David Van Horn. Types and trace \neffects of higher order programs. Journal of Functional Programming, 18(02):179 249, 2008.   \n\t\t\t", "proc_id": "1863543", "abstract": "<p>We describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to well-established abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict return-flow and stack-inspection behavior; and approximate the flow and evaluation of by-need parameters. For all of these machines, we find that a series of well-known concrete machine refactorings, plus a technique we call store-allocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, first-class continuations, and even garbage collection.</p>", "authors": [{"name": "David Van Horn", "author_profile_id": "81337494657", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2338146", "email_address": "", "orcid_id": ""}, {"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2338147", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863553", "year": "2010", "article_id": "1863553", "conference": "ICFP", "title": "Abstracting abstract machines", "url": "http://dl.acm.org/citation.cfm?id=1863553"}