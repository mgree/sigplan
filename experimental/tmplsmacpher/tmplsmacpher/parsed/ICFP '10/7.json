{"article_publication_date": "09-27-2010", "fulltext": "\n The Reduceron Recon.gured Matthew Naylor Colin Runciman University of York, UK {mfn,colin}@cs.york.ac.uk \nAbstract The leading implementations of graph reduction all target conven\u00adtional processors designed \nfor low-level imperative execution. In this paper, we present a processor specially designed to perform \ngraph-reduction. Our processor the Reduceron is implemented using off-the-shelf recon.gurable hardware. \nWe highlight the low\u00adlevel parallelism present in sequential graph reduction, and show how parallel memories \nand dynamic analyses are used in the Re\u00adduceron to achieve an average reduction rate of 0.55 function \nap\u00adplications per clock-cycle. Categories and Subject Descriptors C.1.3 [Processor Architec\u00adtures]: Other \nArchitecture Styles High-level language architec\u00adtures; D.3.4 [Programming Languages]: Processors Run-time \nenvironments; I.1.3 [Symbolic and Algebraic Manipulation]: Lan\u00adguages and Systems Special-Purpose Hardware \nGeneral Terms Design, Experimentation, Performance Keywords Graph Reduction, Recon.gurable Hardware 1. \nIntroduction Ef.cient evaluation of high-level functional programs on conven\u00adtional computers is a big \nchallenge. Sophisticated techniques are needed to exploit architectural features designed for low-level \nim\u00adperative execution. Furthermore, conventional computers have lim\u00aditations when it comes to running \nfunctional programs. For exam\u00adple, memory bandwidth is limited to serial communication in small units. \nEvaluators based on graph reduction perform intensive con\u00adstruction and deconstruction of expressions \nin memory. Each such operation requires sequential execution of many machine instruc\u00adtions, not because \nof any inherent data dependencies, but because of architectural constraints in conventional computers. \nAll this motivates the idea of computers specially designed to meet the needs of high-level functional \nlanguages -much as GPUs are designed to meet needs in graphics. This is not a new idea. In the 80s and \n90s there was a 15-year ACM conference series Functional Programming Languages and Computer Architecture. \nIn separate initiatives, there was an entire workshop concerned with graph-reduction machines alone [Fasel \nand Keller 1987], and a major computer manufacturer built a graph-reduction prototype [Scheevel 1986]. \nBut the process of constructing exotic new hard\u00adware was slow and uncertain. With major advances in compilation \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright c &#38;#169; 2010 ACM 978-1-60558-794-3/10/09. \n. . $10.00 for ever bigger, faster and cheaper mass-market machines, the idea of specialised hardware \nfor functional languages went out of fash\u00adion. Recon.gurable Hardware Today, the situation is quite different. \nField-programmable gate arrays (FPGAs) have greatly reduced the effort and expertise needed to develop \nspecial-purpose hardware. They contain thousands of parallel logic blocks that can be con.g\u00adured at will \nby software tools. They are widely-available and are an advancing technology that continues to offer \nimproved perfor\u00admance and capacity. The downside of FPGA applications is that they typically have much \nlower maximum clocking frequencies than corresponding directly-fabricated circuits this is the price \nto pay for recon.gura\u00adbility. To obtain good performance using an FPGA, it is therefore necessary to \nexploit signi.cant parallelism. The Reduceron In this paper, we present a special-purpose ma\u00adchine for \nsequential graph reduction the Reduceron imple\u00admented on an FPGA. We build upon our previous work on \nthe same topic [Naylor and Runciman 2007] by presenting a new design that exhibits a factor of .ve performance \nimprovement. A notable feature of our new design is that each of its six semantic reduction rules is \nperformed in a a single-clock cycle. All the necessary memory transactions required to perform a reduction \nare done in parallel. The Reduceron performs on average 0.55 hand-reductions per clock-cycle. A hand-reduction \nis a reduction that programmer would perform in by-hand evaluation trace of a program; it includes function \napplication and case analysis, but not machine-level reductions such as updating and unwinding. Another \nnotable development in our new design is the use of two dynamic analyses enabling update avoidance and \nspeculative evaluation of primitive redexes, both of which lead to signi.cant performance improvements. \nOn conventional computers, the run\u00adtime overhead of these dynamic analyses would be prohibitive, but \non FPGA they are cheap and simple to implement. Contributions In summary, we give: \u00a72 a precise description \nof the Reduceron compiler, including re\u00ad .nements to the Scott encoding of constructors, used for com\u00ad \npiling case expressions, addressing various ef.ciency concerns; \u00a73 an operational semantics of the template \ninstantiation machine underpinning the Reduceron implementation; \u00a74 a detailed description of how each \nsemantic reduction rule is implemented in a single clock-cycle using an FPGA; \u00a75 extensions to the semantics \nto support (1) dynamic sharing anal\u00adysis, used to avoid unnecessary heap updates, and (2) dynamic detection \nof primitive redexes, enabling speculative reduction of such expressions during function-body instantiation; \n\u00a76 a comparative evaluation of the Reduceron implementation against other functional language implementations. \n e ::= xe (Application) | case e of { xa } (Case Expression) | let { xb } in e (Let Expression) | n \n(Integer) | x (Variable) | p (Primitive) | f (Function) | C (Constructor) | <fx> (Case Table) a ::= C \nxx -> e (Case Alternative) b ::= x = e (Let Binding) d ::= f xx = e (Function De.nition) Figure 1. Core \nsyntax of F-lite. 2. Compilation This section de.nes a series of re.nements that take programs written \nin a lazy functional language called F-lite to a form known as template code which the Reduceron can \nexecute. 2.1 Source Language F-lite is a core lazy functional language, close to subsets of both Haskell \nand Clean. The syntax of F-lite is presented in Figure 1. Case Expressions Case expressions are in a \nsimpli.ed form that can be produced by a pattern match compiler such as that de.ned in [Peyton Jones \n1987]. Patterns in case alternatives are constructors applied to zero or more variables. All case expressions \ncontain an alternative for every constructor of the case subject s type. Primitives The meta-variable \np denotes a primitive function sym\u00adbol. All applications of primitive functions are fully saturated. \nThe Reduceron implements only a small set of primitive operations, not the full set of a conventional \nprocessor e.g. we have no .oating\u00adpoint operations. Primitives used in this paper include: (+), (-) \nand (<=). Main Every program contains a de.nition main = e where e is an expression that evaluates to \nan integer n; the result of the program is the value n. Case Tables Notice the unusual case-table construct \n<fx>. Case tables are introduced during compilation see \u00a72.4. Examples Here are two example function \nde.nitions. The .rst concatenates two lists and the second computes triangular numbers. append xs ys \n= case xs of { Nil -> ys ; Cons x xs -> Cons x (append xs ys) } trin= case (<=)n1of { False -> (+) (tri \n((-) n 1)) n ; True -> 1 }  2.2 Terminology Application Length The length of an application e1 \u00b7\u00b7\u00b7 en \nis n. For example, the length of the application append xs ys is three. Compound and Atomic Expressions \nApplications, case expres\u00adsions and let expressions are compound expressions. All other ex\u00adpressions \nare atomic. Flat Expression A .at expression is an atomic expression or an application e1 \u00b7\u00b7\u00b7 en in which \neach ei for i in 1 \u00b7\u00b7\u00b7 n is an atomic expression. For example, append xs ys is a .at expression, but \ntri ((-) n 1) is not. Expression Graph A let expression let { x1 = e1 ; \u00b7\u00b7\u00b7 ; xn = en } in e is an expression \ngraph exactly if e is a .at expression and each ei for i in 1 \u00b7\u00b7\u00b7 n is a .at expression. Expression graphs \nare restricted A-normal forms [Flanagan et al. 1993]. Constructor Index and Arity Each constructor C \nof a data type with m constructors is associated with a unique index in the range 1 \u00b7\u00b7\u00b7 m. More precisely, \nthe index of a constructor is its position in the alphabetically sorted list of all constructors of that \ndata type. For example, the standard list data type has two constructors: Cons has index 1 and Nil has \nindex 2. A constructor with index i is denoted Ci, and the arity of a constructor C is denoted #C. 2.3 \nPrimitive Applications In a lazy language, an application of a primitive function such as (+), (-) or \n(<=) requires special treatment: the integer arguments must be fully evaluated before the application \ncan be reduced. One simple approach is to transform binary primitive applications by the rule pe0 e1 \n. e1 (e0 p) (1) with the run-time reduction rule ne . en (2) for any fully evaluated integer literal \nn. To illustrate this approach, consider the expression (+) (tri 1) (tri 2). By compile\u00adtime application \nof rule (1), the expression is transformed to tri 2 ((tri 1) (+)). At run-time, reduction is as follows. \ntri 2 ((tri 1) (+)) { tri 2 evaluates to 3 } = 3 ((tri 1) (+)) { Rule (2) } = (tri 1) (+) 3 { tri 1 evaluates \nto 1 } = 1 (+) 3 { Rule (2) } = (+) 1 3 After transformation by rule (1), tri looks as follows. trin \n=case 1(n(<=))of { False -> n (tri (1 (n (-))) (+)) ; True -> 1 } In \u00a75, we present more ef.cient techniques \nfor dealing with primi\u00adtive applications.  2.4 Case Expressions This section describes how case expressions \nare compiled. First we recall the Scott encoding recently rediscovered by Jansen [Jansen et al. 2007]. \nThen we make a number of re.nements to this encoding. The Scott/Jansen Encoding The .rst step of the \nencoding is to generate, for each constructor Ci of a data type with m construc\u00adtors, a function de.nition \nCi x1 \u00b7\u00b7\u00b7 x#Ci k1 \u00b7\u00b7\u00b7 km = ki x1 \u00b7\u00b7\u00b7 x#Ci The idea is that each data constructor Ci is encoded as a function \nthat takes as arguments the #Ci arguments of the constructor and m continuations.The function encoding \nconstructor Ci passes the constructor arguments to the ith continuation. For example, the list constructors \nare transformed to the following functions. Consxxscn= cx xs Nil cn=n Now case expressions of the form \ncase e of { C1 xx1 -> e1 ; \u00b7\u00b7\u00b7 ; Cm xxm -> em }  are transformed to e (alt1 xv1 x 1) \u00b7\u00b7\u00b7 (altm xvm x \nm) where xvi are the free variables occurring in the ith case alternative and each alti for i in 1 \u00b7\u00b7\u00b7 \nm has the de.nition alti xvi x i = ei For example, the append function is transformed to append xs ys \n= xs (consCase ys) (nilCase ys) consCase ys x xs = Cons x (append xs ys) nilCase ys = ys Notice that \nthe application of nilCase could be reduced at compile time. This is a consequence of constructor Nil \nhaving arity 0. Larger Example Now let us look at a slightly larger example: an evaluator for basic arithmetic \nexpressions. eval xy e=caseeof { Addnm-> (+)(eval xyn) (eval xy m); Negn ->(-)0(evalxyn); Subnm-> (-)(eval \nxyn) (eval xy m); X -> x; Y -> y; } After transformation, and in-lining the nullary cases, we have: \neval xy e=e(addxy) (neg xy)(subxy) xy add xynm=(+) (eval xyn)(evalxym) neg xyn =(-)0(evalxyn) sub xynm=(-) \n(eval xyn)(evalxym) Look at the large body of eval: it contains three nested function applications and \nseveral repeated references to x and y. In typi\u00adcal functional language implementations, large function \nbodies are more expensive to construct than small ones. Re.nement 1 Rather than partially apply each \ncase-alternative function to the free variables it refers to, we can de.ne every alternative function \nalike to take all free variables occurring in any alternative. A case alternative can simply ignore variables \nthat it does not need. So, let us instead transform case expressions to e alt1 \u00b7\u00b7\u00b7 altm xv where xv is \nthe union of the free variables in each case alternative, and each alti for i in 1 \u00b7\u00b7\u00b7 m has the de.nition \nalti x i xv = ei Each case-alternative function now takes the constructor arguments followed by the free \nvariables, rather than the other way around. To illustrate, append now looks as follows. append xs ys \n= xs consCase nilCase ys consCase x xs ys = Cons x (append xs ys) nilCase ys = ys And eval becomes eval \nxy e=eadd neg subxCase yCase xy add nmxy=(+)(evalxyn)(eval xym) neg n xy=(-)0(evalxyn) sub nmxy=(-)(evalxyn)(eval \nxym) xCase xy=x yCase xy=y The new bodies of append and eval contain no nested function applications \nand no repeated references. An apparent disadvantage is that we have had to introduce functions for the \n0-arity construc\u00adtor cases nilCase, xCase, and yCase. But our next re.nement prepares the way to recover \nthe cost of applying these functions. Re.nement 2 We now have a large row of contiguous constants in \nthe body of eval. To allow these constants to be represented ef\u00ad.ciently (see \u00a72.7) we place them in \na case table. Case expressions are transformed to e <alt1, \u00b7\u00b7\u00b7 , altm> xv and each constructor Ci is \nencoded as Ci x1 \u00b7\u00b7\u00b7 x#Ci t = (t ! i) x1 \u00b7\u00b7\u00b7 x#Ci where t!i returns the ith element of case table t. \nRe.nement 3 An evaluator can handle constructors more ef.\u00adciently than general function de.nitions. We \ncould introduce the following reduction rule for constructors. Ci e1 \u00b7\u00b7\u00b7 e#Ci t . (t ! i) e1 \u00b7\u00b7\u00b7 e#Ci \nThis rule replaces a constructor with a case-alternative function by looking up the case table using \nthe constructor s index. However, the rule also drops the t argument. As a result, an implementation \nwould have to slide the constructor arguments down the stack. A reduction rule that does not require \nargument sliding is Ci e1 \u00b7\u00b7\u00b7 e#Ci t . (t ! i) e1 \u00b7\u00b7\u00b7 e#Ci t (3) To account for the fact that t has not \nbeen dropped, the case\u00adalternative functions take the form: alti x i t xv = ei The .nal version of append \nis append xs ys = xs <consCase, nilCase> ys consCase x xs t ys = Cons x (append xs ys) nilCase t ys = \nys The t argument is simply ignored by the case alternatives. The .nal version of tri is tri n = 1 (n \n(<=)) <falseCase, trueCase> n falseCase t n = n (tri (1 (n (-))) (+)) trueCase t n = 1 In \u00a73.3 and \u00a74.5 \nwe will see how these re.nements enable ef.cient choices to be made at the implementation level.  2.5 \nIn-lining Our de.nition of append is no longer directly recursive. This is a consequence of splitting \nthe case alternatives off as new function de.nitions. However, direct recursion is easily recovered: \nsimply in-line the de.nition of append in the body of consCase. consCase x xs t ys = Cons x (xs <consCase, \nnilCase> ys) This transformation motivates the following general in-lining rule: in-line saturated applications \nof functions that have .at bodies. In\u00adlining a .at expression e is often a big win because it eliminates \na reduction and e is often no larger than the application it replaces.  2.6 Expression Graphs It is \nconvenient for implementation purposes to make the graph structure of function bodies explicit by transforming \nthem to ex\u00adpression graphs (\u00a72.2). This is achieved by three rewrite rules. (1) Lift nested applications \ninto let bindings e1 \u00b7\u00b7\u00b7 (ei) \u00b7\u00b7\u00b7 en . let { x = ei } in e1 \u00b7\u00b7\u00b7 x \u00b7\u00b7\u00b7 en where ei is an application or \na let expression, and x is a fresh variable. (2) Lift let expressions out of let bodies. let { xb0 } \nin (let { xb1 } in e) . let { xb0 ; xb1 } in e  > data Atom = > FUN Arity Int --Function with arity \nand address > | ARG Int --Reference to a function argument > | PTR Int --Pointer to an application > \n| CON Arity Int --Constructor with arity and index > | INT Int --Integer literal > | PRI String --Primitive \nfunction name > | TAB Int --Case table Figure 2. Syntax of atoms in template code. (3) Lift let expressions \nout of let bindings. let { \u00b7\u00b7\u00b7 ; x = let { xb } in e0 ; \u00b7\u00b7\u00b7 } in e1 . let { \u00b7\u00b7\u00b7 ; xb ;x = e0 ; \u00b7\u00b7\u00b7 } \nin e1 These rules assume no variable shadowing. To illustrate, the de.ni\u00adtion of falseCase becomes: falseCase \nt n = let{x0= tri x1 (+);x1= 1x2;x2= n(-)} innx0 It is easy to see the number and length of applications \nin an expres\u00adsion graph. For example, falseCase contains four applications and its longest application, \ntri x1 (+), has length three.  2.7 Template Code We are now very close to the template code that can \nbe executed by the Reduceron. We shall de.ne template code as a Haskell data type, paving the way for \nan executable semantics to be de.ned in the next section. To highlight the semantics, each semantic de.ni\u00adtion \nis pre.xed with a > symbol. In template code, a program is de.ned to be a list of templates. > type Prog \n= [Template] A template represents a function de.nition. It contains an arity,a spine application and \na list of nested applications. > type Template = (Arity, App, [App]) > type Arity = Int The spine application \nholds the let-body of a de.nition s expres\u00adsion graph and the nested applications hold the let-bindings. \nAppli\u00adcations are .at and are represented as a list of atoms. > type App = [Atom] An atom is a small, \ntagged piece of non-recursive data, de.ned in Figure 2. The following paragraphs de.ne how programs are \ntranslated to template code. Functions Given a list of function de.nitions f0 x 0 = e0, \u00b7\u00b7\u00b7 ,fn x n = \nen each function identi.er fi occurring in e0 \u00b7\u00b7\u00b7 en is translated to an atom FUN #fi where #f is the \narity of function f. Arguments In each de.nition fx0 \u00b7\u00b7\u00b7 xn = e, each variable xi occurring in e is translated \nto an atom ARG i. Let-Bound Variables In each expression graph let { x0 = e0 ; \u00b7\u00b7\u00b7 ; xn = en } in e each \nxi occurring in e, e0 \u00b7\u00b7\u00b7 en is translated to an atom PTR i. Integers, Primitives, and Constructors An \ninteger literal n,a primitive p, and a constructor Ci are translated to atoms INT n, PRI p, and CON #Ci \ni respectively. Case Tables Given a list of function de.nitions f0 x 0 = e0, \u00b7\u00b7\u00b7 ,fn x n = en each case \ntable <fi, \u00b7\u00b7\u00b7 fj > occurring in e0 \u00b7\u00b7\u00b7 en is translated to an atom TAB i. We assume that the functions \nin each case table are de.ned contiguously in the program. Example The template code for the program \nmain = tri 5 tri n = let x = n (<=) in 1 x <falseCase, trueCase> n falseCase t n = let {x0=tri x1(+);x1 \n=1x2;x2=n(-)}inn x0 trueCase t n = 1 is as follows. > tri5 :: Prog > tri5 = [ (0, [FUN 1 1, INT 5], []) \n> , (1, [INT 1, PTR 0, TAB 2, ARG 0], > [[ARG 0, PRI \"(<=)\"]]) > , (2, [ARG 1, PTR 0], > [[FUN 1 1, PTR \n1, PRI \"(+)\"], > [INT 1, PTR 2], > [ARG 1, PRI \"(-)\"]]) > , (2, [INT 1], []) ] 3. Operational Semantics \nThis section de.nes a small-step operational semantics for the Re\u00adduceron. There are two main reasons \nfor presenting a semantics: (1) to de.ne precisely how the Reduceron works; and (2) to high\u00adlight the \nlow-level parallelism present in graph reduction that is ex\u00adploited by the Reduceron. We have found it \nvery useful to encode the semantics directly in Haskell. Before we commit to a low-level implementation, \nwe can assess the complexity and performance of different design decisions and optimisations. At the \nheart of the semantic de.nition is the small-step state transition function > step :: State -> State \nwhere the state is a 4-tuple comprising a program, a heap, a reduc\u00adtion stack, and an update stack. > \ntype State = (Prog, Heap, Stack, UStack) The heap is modelled as a list of applications, and can be indexed \nby a heap-address. > type Heap = [App] > type HeapAddr = Int An element on the heap can be modi.ed using \nthe update function. > update :: HeapAddr -> App -> Heap -> Heap > update i a as = take i as ++ [a] ++ \ndrop (i+1) as The reduction stack is also modelled as a list of nodes, with the top stack element coming \n.rst and the bottom element coming last. > type Stack = [Atom] > type StackAddr = Int There is also an \nupdate stack. > type UStack = [Update] > type Update = (StackAddr, HeapAddr) The meaning of a program \np is de.ned by run p where > run :: Prog -> Int > run p = eval initialState > where initialState = (p, \n[], [FUN 0 0], []) > eval (p, h, [INT i], u) = i > eval s = eval (step s) The initial state of the evaluator \ncomprises a program, an empty heap, a singleton stack containing a call to main, and an empty update \nstack. The main template has arity 0 and is assumed to be the template at address 0. To illustrate, run \ntri5 yields 15. In the following sections, the central step function is de.ned.  3.1 Primitive Reduction \nThe prim function applies a primitive function to two arguments supplied as fully-evaluated integers. \n> prim :: String -> Atom -> Atom -> Atom > prim \"(+)\" (INT n) (INT m) = INT (n+m) > prim \"(-)\" (INT n) \n(INT m) = INT (n-m) > prim \"(<=)\" (INT n) (INT m) = bool (n<=m) The comparison primitive returns a boolean \nvalue. Both boolean constructors have arity 0; False has index 0 and True has index 1. > bool :: Bool \n-> Atom > bool False = CON 0 0 > bool True = CON 0 1 3.2 Normal Forms The number of arguments demanded \nby an atom on top of the reduction stack is de.ned by the arity function. > arity :: Atom -> Arity > \narity (FUN n i) = n > arity (INT i) = 1 > arity (CON n i) = n+1 > arity (PRI p) = 2 To reduce an integer, \nthe evaluator demands one argument as shown in rewrite rule (2). And to reduce a constructor of arity \nn, the evaluator requires n +1 arguments (the constructor s arguments and the case table) as shown in \nrewrite rule (3). The arity of an atom is only used to detect when a normal form is reached. A normal \nform is an application of length n whose .rst atom has arity = n. Some functions, such as case-alternative \nfunctions, are statically known never to be partially-applied, so they cannot occur as the .rst atom \nof a normal form. Such a function, say with address n, can be represented by the atom FUN 0 n.  3.3 \nStep-by-Step Reduction There is one reduction rule for each possible type of atom that can appear on \ntop of the reduction stack. Unwinding If the top of the reduction stack is a pointer x to an application \non the heap, evaluation proceeds by unwinding: copying the application from the heap to the reduction \nstack where it can be reduced. We must also ensure that when evaluation of the application is complete, \nthe location x on the heap can be updated with the result. So we push onto the update stack the heap \naddress x and the current size of the reduction stack. > step (p, h, PTR x:s, u) = (p, h, h!!x ++ s, \nupd:u) > where upd = (1+length s, x) Updating Evaluation of an application is known to be complete when \nan argument is demanded whose index is larger than n, the difference between the current size of the \nreduction stack and the stack address of the top update. If this condition is met, then a normal form \nof arity n is on top of the reduction stack and must be written to the heap. > step (p, h, top:s, (sa,ha):u) \n> | arity top > n = (p, h , top:s, u) > where > n = 1+length s -sa > h = update ha (top:take n s) h Integers \nand Primitives Integer literals and primitive functions are reduced as described in \u00a72.3. > step (p, \nh, INT n:x:s, u) = (p, h, x:INT n:s, u) > step (p, h, PRI f:x:y:s, u) = (p, h, prim f x y:s, u) Constructors \nConstructors are reduced by indexing a case table, as described in \u00a72.4. > step (p, h, CON n j:s, u) \n= (p, h, FUN 0 (i+j):s,u) > where TAB i = s!!n There is insuf.cient information available to compute \nthe arity of the case-alternative function at address i+j. However, an arity of zero can be used because \na case-alternative function is statically known not to be partially applied (\u00a73.2). Function Application \nTo apply a function f of arity n, n +1 elements are popped off the reduction stack, the spine application \nof the body of f is instantiated and pushed onto the reduction stack, and the remaining applications \nare instantiated and appended to the heap. > step (p, h, FUN n f:s, u) = (p, h , s , u) > where > (pop, \nspine, apps) = p !! f > h = h ++ map (instApp s h) apps > s = instApp s h spine ++ drop pop s Instantiating \na function body involves replacing the formal param\u00adeters with arguments from the reduction stack and \nturning relative pointers into absolute ones. > instApp :: Stack -> Heap -> App -> App > instApp s h \n= map (inst s (length h)) > inst :: Stack -> HeapAddr -> Atom -> Atom > inst s base (PTR p) = PTR (base \n+ p) >instsbase(ARG i) =s!! i > inst s base a = a 4. Implementation We now re.ne the semantic de.nition \nto an actual implementation that runs on an FPGA. Speci.cally, our target is a mid-range Xil\u00adinx Virtex-5 \nreleased in 2008. Our guiding design principle is to perform as much reduction as possible in each clock-cycle. \nOur implementation performs each semantic reduction rule in a single clock-cycle, and clocks at a modest \nbut respectable frequency for processor-like FPGA designs. 4.1 Low-Level Parallelism Below we motivate \nthree main opportunities for parallelism that we exploit in our implementation. Parallel Memories The \nstate of the reduction machine comprises four independent memory regions: the program, the heap, the \nre\u00adduction stack and the update stack. Most reduction rules refer to and modify more than one memory \nregion. For example, the reduc\u00adtion rule for unwinding writes to both the reduction stack and the update \nstack. If the four memory regions are implemented as four separate memory units then they can be accessed \nin parallel, avoid\u00ading contentions that would arise if they were all stored in a single memory unit. \nWide Memories Many of the reduction rules involve transferring applications to and from memory. If a \nmemory only allows one atom to be accessed at a time, transferring a single application involves several \nmemory accesses. If memories are wide enough to allow a whole application to be accessed at a time, transferring \nan application needs only a single memory access. Parallel Instantiation The reduction rule for function \napplication involves instantiating each application in a function body and ap\u00adpending it to the heap. \nEach atom in an application can be instanti\u00adated in parallel, as indicated by the use of map in the de.nition \nof instApp. The wide heap then allows the instantiated application to be written in one memory access. \nFurther, each application in a function body can also be instantiated in parallel, as indicated by the \nuse of map in semantic rule for function application. If more than one application can be appended to \nthe heap at a time, parallel instantiation of applications is possible.  4.2 Bounded Template Instantiation \nMaximum Application Length Ideally, we would have a wide enough data bus to transfer any entire application \nin one go. How\u00adever, this is an impossibility without some upper bound on the length of an application. \nTherefore, we introduce a bound, MaxAp\u00adpLen, on the number atoms that can occur in an application. To \ndeal with an application whose length is larger than MaxAp\u00adpLen, we split it into two or more smaller \nones. For example, if MaxAppLen is 3 the application f abcde can be bracketed ((f ab) cd) e resulting \nin three applications rather than one. An alternative way to bound application length is to split appli\u00adcations \ninto chunks that are aligned contiguously in memory, with the .nal chunk especially tagged by an end-marker. \nThis approach [Naylor and Runciman 2007] is more ef.cient in some cases, but it cannot be expressed as \na core-language transformation. Maximum Spine Length Spine applications are special because, during function \napplication, they are written to the stack, not the heap. So it is .ne for spine applications to have \na different maxi\u00admum length: MaxSpineLen. Maximum Applications per Template Ideally, all applications \nin a template would be instantiated in parallel. To allow for such an implementation, we introduce a \nbound, MaxAppsPerBody, on the maximum number of applications that can occur in a template body. To deal \nwith templates containing more applications than MaxAppsPerBody, we employ a technique called template \nsplitting. Template Splitting We explain template splitting by example. Consider the following template, \nrepresenting the falseCase function occurring in the tri5 program de.ned in \u00a72.7. (2, [ARG 1, PTR 0] \n--Spine , [ [FUN 1 1, PTR 1, PRI \"(+)\"] --Application 1 , [INT 1, PTR 2] --Application 2 , [ARG 1, PRI \n\"(-)\"] ] ) --Application 3 It contains one spine application and three nested applications. If MaxAppsPerBody \nis two then this template is split into two sub\u00adtemplates. The .rst sub-template (0, [FUN 0 4] --Intermediate \nspine , [ [FUN 1 1, PTR 1, PRI \"(+)\"] --Application 1 , [INT 1, PTR 2] ] ) --Application 2 replaces the \noriginal template in the tri5 program. The second sub-template is appended to the program at the next \nfree program address: address four in the case of the tri5 program. (2, [ARG 1, PTR (-2)] --Spine , [ \n[ARG 1, PRI \"(-)\"] ]) --Application 3 The spine of the .rst sub-template is simply a call to the second \nsub-template. There are three important points to note: The .rst sub-template contains three applications, \nwhich is still larger than MaxAppsPerBody. However, at the implementation level, we do not count a spine \napplication of the form [FUN 0 f] as an application: it can be interpreted simply as jump to template \nf , and does not entail any heap or stack accesses.  In the second sub-template, each atom of the form \nPTR n is replaced by PTR (n-2) to account for the fact that instantiation of the .rst sub-template will \nhave increased the size of the heap by two.  The arity of the .rst sub-template is set to zero: no elements \nare popped from the stack since they may be required by the second sub-template.  Choosing the Bounds \nWe must choose the values of the bounds MaxAppLen, MaxSpineLen, and MaxAppsPerBody carefully: mak\u00ading \nthem too low prevents useful parallelism; making them too Memory Unit Element Bits/Element Elements Program \nTemplate 234 1k Heap App 77 32k Reduction Stack Atom 18 8k Update Stack Update 28 4k Case-Table Stack \nAtom 18 4k Copy Space App 77 16k Table 2. Size and type of each parallel memory unit. high wastes resources. \nOur choices are informed by experiment. Table 1 shows the performance effect of varying each parameter \nin turn non-varying parameters are effectively de.ned as in.nity. The reduction count and heap usage \n.gures are normalised across the varying parameter and averaged across a range of benchmark programs \n(see \u00a76.1). The measurements are obtained using a PC implementation of the operational semantics. The \nreduction count represents the number of times that the step function is applied in the de.nition of \neval1. The chosen bounds are: MaxAppLen =4, MaxSpineLen =6, and MaxAppsPerBody =2. The measurements suggest \na MaxAp\u00adpLen of three is preferable to four due to better heap usage; the choice of four is motivated \nby another implementation parameter the arity limit introduced in \u00a74.3. A MaxSpineLen of .ve would \nnot be much worse than six, but the choice of six does not cost much extra at the implementation level. \nA MaxAppsPerBody of two is motivated by the fact that three would not be much better and that two .ts \nnicely with the dual-port memories available on the FPGA.  4.3 Memory Layout Our Xilinx Virtex-5 FPGA \ncontains 296 dual-port block RAMs each with a capacity of 18 kilobits giving a total on-chip RAM ca\u00adpacity \nof 5,328 kilobits. Each block RAM is dual-port allowing two independent accesses per clock-cycle. The \ndata-bus and address\u00adbus widths of each block-RAM are con.gurable. Possible con.g\u00adurations include 1k-by-18bit \nand 16k-by-1bit, and a range of pos\u00adsibilities in-between. Two 18 kilobit block RAMs can be merged to \ngive further possible con.gurations ranging from 1k-by-36bit to 32k-by-1bit. For simplicity, our implementation \nuses FPGA block RAMs only; no off-chip RAMs are used. This represents a tight constraint on the amount \nof memory available to the implementation. (The possibility of introducing off-chip memories is discussed \nin \u00a77.) Memory Structure The parallel memory units, each built out of block RAMs, are listed in Table \n2 along with their capacities and the type of element stored at every addressable location. Note that \nthere are uniform sizes for every program template and for every heap application. The two memory units \nat the bottom of the table are introduced in \u00a74.5 and \u00a74.6 respectively. Wide Memories The wide heap \nmemory is implemented by con\u00adcatenating the data-busses of 77 32k-by-1bit block RAMs and merging their \naddress-busses. This is done on both ports of each block RAM, making a dual-port heap. Similarly, the \nwide program memory is implemented using 13 1k-by-18bit block RAMs, but this time the dual-port capability \nis not needed. Stack Memories We store the top N stack elements in special\u00adpurpose stack registers. In \nany given clock-cycle, the stack imple\u00admentation allows: the top N elements to be observed; and up to \nN elements to be popped off; and up to N elements to be pushed on. If pushing and popping occur the same \nclock-cycle, the pop is 1 Constructor reductions are not counted, anticipating the optimisation pre\u00adsented \nin \u00a74.5.  MaxAppLen Reductions Heap MaxSpineLen Reductions Heap MaxAppsPerBody Reductions 2 1.00 1.00 \n2 1.00 1.00 1 1.00 3 0.84 1.00 3 0.82 0.76 2 0.89 4 0.83 1.30 4 0.76 0.67 3 0.85 5 0.82 1.57 5 0.71 0.60 \n4 0.85 6 0.82 1.89 6 0.70 0.57 Table 1. Effect of application-length, spine-length, and applications-per-template \nbounds on reduction count and heap usage. performed before the push. Simultaneous access to the top N \nele\u00adments of the stack is achieved by a crossbar switch. It requires over 2,000 logic gates, but this \nis less than 1% of our FPGA s logic-gate capacity. There is a lot of parallelism in a crossbar, so the \ninvest\u00adment is worth it. Further hardware-level implementation details of the stack implementation are \navailable in [Memo 27]. Arity Limit The stack implementation is parameterised by N, but requires N to \nbe a power of two. For the update stack, N is de.ned to be 1 since reading and writing multiple values \nis of no bene.t. For the reduction stack, there are three considerations to take into account, bearing \nin mind the aim of single-cycle reduction: (1) only the top N stack elements are available in any clock-cycle, \nhence the maximum number of arguments that be taken by a function is N - 1; (2) the maximum length of \na partially-applied function application, or normal-form, is therefore N - 1; and (3) the choice of N \nshould allow a normal form of length N - 1 to be written onto the heap in a single clock cycle. As two \napplications of length MaxAppLen can be written to the dual-port heap per clock-cycle, and MaxAppLen \nis four, a sensible choice for N is eight since a normal form of length seven can be bracketed perfectly \ninto two applications of length four. To deal with functions taking more than N -1 arguments, an ab\u00adstraction \nalgorithm can be used [Turner 1979]. We have developed a minor variant [Memo 12] of an abstraction algorithm \nbased on director strings [Dijkstra 1980, Kennaway and Sleep 1988] which uses a more coarse-grained combinator \nset than Turner s algorithm.  4.4 One Reduction per Clock-Cycle Heap and program memory units have the \nfollowing two properties. If a memory location x is read in clock-cycle n, the value at address x becomes \navailable on the memory s data bus on clock-cycle n +1.  If a value is written to memory location x \nin clock-cycle n, the new value at address x is not apparent until clock-cycle n +1.  The top stack \nelements are always observable without any clock\u00adcycle delay. Now we show how each reduction rule in \nthe semantics can be performed in a single clock-cycle, with reference to the following two invariants. \nInvariant 1: If the top of the reduction stack is of the form PTR x then the application at heap address \nx is currently available on the heap memory s data bus. Invariant 2: If the top of the reduction stack \nis of the form FUN nf then the template at program address f is currently available on the program memory \ns data bus. Unwinding The top of the reduction stack has the form PTR x. So the application currently \non the heap s data bus, say app, is the application at heap address x (Invariant 1). The following memory \ntransactions are performed in parallel in a single clock-cycle: the application app is pushed onto the \nreduction stack;  an update (n, x) is pushed onto the update stack where n is the size of the reduction \nstack before modi.cation; and  the .rst atom of app is the new top of the reduction stack and is used \nto lookup heap and program memory in order to maintain Invariants 1 and 2. Updating The update stack \ns data bus is used to determine if an update is required, and if so, at what heap address x. If an update \nis required, then a normal form is available on the reduction stack s data bus. The following memory \ntransactions are performed in parallel in a single clock-cycle: if the normal form has length less than \nor equal to four it is written to the heap at address x;  if the normal form has length larger than \nfour, it is bracketed into two applications of maximum length four one of which is written to the heap \nat address x, and the other of which is appended to the heap;  the top element of the update stack is \npopped; and  a program lookup is performed to preserve Invariant 2.  A heap lookup to preserve Invariant \n1 is not necessary since the top of the reduction stack cannot possibly be of the form PTR x if an update \nis being performed. So updating requires at most two heap accesses, which can be done parallel thanks \nto dual-port memory. Integers, Primitives, and Constructors Each of these reduction rules involves a \npure stack manipulation, and each straightfor\u00adwardly consumes a single clock-cycle. Function Application \nThe top of the reduction stack has the form FUN nf. So the template of f, say t, is available on the \ndata bus (Invariant 2). There are two cases to consider. Case 1: If t contains a spine application of \nthe form [FUN 0 f], then: up to two nested applications in t are instantiated and appended to the heap; \n the atom FUN 0 f is written to the top of the reduction stack; and  function f is looked-up in program \nmemory in order to preserve Invariant 2.  Case 2: If t is of some other form, then: zero or one nested \napplications in t are instantiated and ap\u00adpended to the heap;  the spine application in t is instantiated \nand written to the reduction stack; and  the .rst element of the instantiated spine is used to lookup \nheap and program memory to preserve Invariants 1 and 2.  In Case 1, a heap lookup to preserve Invariant \n1 is not required: the top of the stack is known to be a FUN, not a PTR. Thus in each case, at most two \nheap access are required.  4.5 The Case-Table Stack Constructor reduction modi.es only the top element \nof the reduc\u00adtion stack by adding the index of the constructor to the address of a case table. This addition \nis almost cheap enough to be implemented in combinatorial logic (i.e. in zero clock-cycles) without affecting \nthe critical path delay of the circuit. The problem is that the case table must be fetched from a variable \nposition on the stack. This requires a multiplexer, making the combinatorial logic more ex\u00adpensive. \n To solve this problem, we introduce a new stack memory to store case tables. When unwinding an application \ncontaining a case table, the case table is pushed onto the case-table stack. When per\u00adforming constructor \nreduction, the case table of interest is always in the same position: the top of the case-table stack. \nTable 3 shows the impact of various optimisations on clock\u00adcycle count and heap usage across a range \nof benchmark programs. The in-lining strategy de.ned in \u00a72.5 and the case-table optimisa\u00adtion both result \nin signi.cant performance gains on average. The other optimisations in Table 3 are introduced in \u00a75. \n 4.6 Garbage Collection Our implementation employs a simple two-space stop-and-copy garbage collector \n[Jones and Lins 1996]. Although a two-space collector may not make the best use of limited memory resources, \nit does have the attraction of being easy to implement. In particular, the algorithm is easily de.ned \niteratively so that no recursive call stack is needed. 4.7 Hardware Description The Reduceron is described \nentirely in around 2,000 lines of York Lava [Naylor et al. 2009], a hardware description language embed\u00added \nin Haskell. A large proportion of the description deals with garbage collection and the bit-level encoding \nof template code; the actual reduction rules account for less than 400 lines. The Reduceron description \nis quite different to other reported Lava applications. It combines structural and behavioural descrip\u00adtion \nstyles. Behavioural description brings improved modularity to our description. We associate each reduction \nrule with the memory transactions it performs, rather than associating each memory unit with all the \nmemory transactions performed on it. So each reduc\u00adtion rule can be expressed in isolation. The behavioural \ndescription language, called Recipe and in\u00adcluded with York Lava, takes the form of a 300 line Lava library. \nIt provides mutable variables, assignment statements, sequential and parallel composition, conditional \nand looping constructs, and shared procedure calls. In addition, it uses the results of a simple timing \nanalysis, implemented by abstract interpretation, to enable optimisations.  4.8 Synthesis Results Synthesising \nour implementation on a Xilinx Virtex-5 LX110T (speed-grade 1) yields an FPGA design using 14% of available \nlogic slices and 90% of available block RAMs. The maximum clock frequency after place-and-route is 96MHz. \nBy comparison, Xilinx distributes a hand-optimised RISC soft-processor called the MicroBlaze that clocks \nat 210MHz on the same FPGA. However, as the Reduceron performs a lot of computation per clock-cycle, \n96MHz seems respectable. Furthermore, the MicroBlaze supports up to .ve pipeline stages, whereas the \nReduceron is not pipelined. 5. Optimisations This section presents several optimisations, de.ned by a \nseries of progressive modi.cations to the semantics de.ned in \u00a73. A theme of this section is the use \nof cheap dynamic analyses to improve performance. 5.1 Update Avoidance Recall that when evaluation of \nan application on the heap is com\u00adplete, the heap is updated with the result to prevent repeated evalu\u00adation. \nThere are two cases in which such an update is unnecessary: (1) the application is already evaluated, \nand (2) the application is not shared so its result will never be needed again. We identify non-shared \napplications at run-time, by dynamic analysis. Argument and pointer atoms are extended to contain an \nextra boolean .eld. > data Atom = \u00b7\u00b7\u00b7 | ARG Bool Int | PTR Bool Int | \u00b7\u00b7\u00b7 An argument is tagged with \nTrue exactly if it is referenced more than once in the body of a function. A pointer is tagged with False \nexactly if it is a unique pointer; that is, it points to an application that is not pointed to directly \nby any other atom on the heap or reduction stack. There may be multiple pointers to an application containing \na unique pointer, so the fact that a pointer is unique is, on its own, not enough to infer that it points \nto a non-shared application. To identify non-shared applications, we maintain the invariant: Invariant \n3: A unique pointer occurring on the reduction stack points to a non-shared application. A pointer that \nis not unique is referred to as possibly-shared. Unwinding The reduction rule for unwinding becomes > \nstep (p, h, PTR sh x:s, u) = (p, h, app++s, upd++u) > where > app = map (dashIf sh) (h!!x) > upd = [(1+length \ns, x) | sh &#38;&#38; red (h!!x)] If the pointer on top of the stack is possibly-shared, then the ap\u00adplication \nis dashed before being copied onto the stack by mark\u00ading each atom it contains as possibly-shared. This \nhas the effect of propagating sharing information through an application. > dashIf sh a = if sh then \ndash a else a > dash (PTR sh s) = PTR True s >dasha= a If the pointer on top of the stack is unique, \nthe application it points to must be non-shared according to Invariant 3. An update is only pushed onto \nthe update stack if the pointer is possibly-shared and the application is reducible. An application is \nreducible if it is saturated or its .rst atom is a pointer. > red :: App -> Bool > red (PTR sh i:xs) \n= True > red (x:xs) = arity x <= length xs Updating When an update occurs, the normal-form on the stack \nis written to the heap. The normal-form may contain a unique pointer, but the process of writing it to \nthe heap will duplicate it. Hence the normal-form on the stack is dashed. > step (p, h, top:s, (sa,ha):u) \n> | arity top > n = (p, h , top:dashN n s, u) > where > n = 1+length s -sa > h = update ha (top:take \nn s) h > dashN n s = map dash (take n s) ++ drop n s It is unnecessary to dash the normal-form that is \nwritten to the heap, but there is no harm in doing so: the application being updated is possibly-shared, \nand a possibly-shared application will anyway be dashed when it is unwound onto the stack. Function Application \nWhen instantiating a function body, shared arguments must be dashed as they are fetched from the stack. \n> inst s base (PTR sh p) = PTR sh (base + p) > inst s base (ARG sh i) = dashIf sh (s!!i) > inst s base \na = a  Program Baseline Time Heap +In-lining Time Heap +Case Stack Time Heap +Update Avoid. Time Heap \n+In.x Prims. Time Heap +PRS Time Heap Adjoxo 1.00 1.00 0.85 0.80 0.71 0.80 0.54 0.80 0.43 0.49 0.36 0.41 \nBraun 1.00 1.00 0.84 0.93 0.63 0.93 0.46 0.93 0.43 0.88 0.42 0.88 Cichelli 1.00 1.00 0.93 0.97 0.77 0.97 \n0.56 0.97 0.42 0.36 0.41 0.33 Clausify 1.00 1.00 0.79 0.59 0.59 0.59 0.48 0.59 0.41 0.42 0.41 0.42 CountDown \n1.00 1.00 0.95 0.97 0.86 0.97 0.70 0.97 0.49 0.53 0.31 0.33 Fib 1.00 1.00 1.28 2.33 1.21 2.33 0.96 2.33 \n0.75 2.00 0.35 0.33 KnuthBendix 1.00 1.00 0.81 0.66 0.63 0.66 0.48 0.66 0.43 0.58 0.40 0.49 Mate 1.00 \n1.00 0.83 0.45 0.67 0.45 0.50 0.45 0.43 0.29 0.40 0.25 MSS 1.00 1.00 0.92 1.00 0.84 1.00 0.61 1.00 0.38 \n0.51 0.24 0.03 OrdList 1.00 1.00 0.73 0.67 0.55 0.67 0.42 0.67 0.42 0.67 0.42 0.67 PermSort 1.00 1.00 \n0.77 0.77 0.62 0.77 0.48 0.77 0.42 0.69 0.42 0.69 Queens 1.00 1.00 0.75 0.54 0.68 0.54 0.51 0.54 0.40 \n0.39 0.21 0.11 Queens2 1.00 1.00 0.82 0.92 0.67 0.92 0.55 0.92 0.50 0.77 0.50 0.73 SumPuz 1.00 1.00 0.95 \n1.06 0.80 1.06 0.60 1.05 0.50 0.74 0.48 0.63 Taut 1.00 1.00 0.90 0.99 0.70 0.99 0.56 0.99 0.51 0.90 0.50 \n0.87 While 1.00 1.00 0.93 0.95 0.77 0.95 0.58 0.95 0.50 0.81 0.49 0.80 Average 1.00 1.00 0.88 0.92 0.74 \n0.92 0.57 0.92 0.47 0.69 0.40 0.50 Table 3. Impact of optimisations on clock-cycle count and heap usage \nacross a range of programs. Performance Table 3 shows that, overall, update avoidance offers a signi.cant \nrun-time improvement. On average, 88% of all updates are avoided across the 16 benchmark programs. Just \nover half of these are avoided due to non-reducible applications, and just under half of them are avoided \ndue to non-shared reducible applications. The average maximum update-stack usage drops from 406 to 11. \n 5.2 In.x Primitive Applications For every binary primitive function p, we introduce a new primitive \n*p, a version of p that expects its arguments .ipped. > prim ( * :p) n m = prim p m n Any primitive function \np can be .ipped. > flip ( * :p) = p > flip p = * :p Now we translate binary primitive applications by \nthe rule pmn . mpn (4) In place of the existing reduction rules for primitives and integers, we de.ne: \n> step (p, h, INT m:PRI f:INT n:s, u) = > (p, h, prim f m n:s, u) > step (p, h, INT m:PRI f:x:s, u) = \n> (p, h, x:PRI (flip f):INT m:s, u) If both arguments are already evaluated, the primitive is applied. \nIf only the .rst argument is evaluated, then the arguments are swapped and the primitive is .ipped. Note \nthat compilation rule (4) could just as sensibly be pmn . n *pm (5) In the interest of ef.ciency, the \nchoice between (4) and (5) is in\u00adformed for each primitive application by compile-time knowledge of whether \nm or n is expected to be already-evaluated. Example Consider the steps needed to evaluate (+) e0 e1. \nUsing the approach to primitive reduction of \u00a73.1, the application is trans\u00adlated to e1 (e0 (+)) at compile \ntime. At run-time, in four successive clock-cycles: an integer reduction is required after e1 is evaluated; \n an unwinding is required to fetch argument e0 (+) from heap;  an integer reduction is required after \ne0 is evaluated; and  a primitive reduction is required.  With the new approach, the application is \ntranslated to e0 (+) e1 at compile-time. At run-time, in two successive clock-cycles: an integer reduction \nis required after e0 is evaluated; and  a primitive reduction is required after e1 is evaluated.  Also \nnote that e0 (+) e1 comprises one application whereas e1 (e0 (+)) comprises two, so the former is cheaper \nto instanti\u00adate. Table 3 shows run-time and heap-usage improvements brought by the new approach.  5.3 \nSpeculative Evaluation of Primitive Redexes Consider evaluation of the expression tri 5. Application \nof tri yields the expression case (<=) 5 1 of { False -> (+) (tri ((-) 5 1)) 5 ; True -> 1 } which contains \ntwo primitive redexes: (<=)5 1 and (-) 51. This section introduces a technique called primitive-redex \nspecu\u00adlation (PRS) in which such redexes are evaluated during function body instantiation. For example, \napplication of tri instead yields case False of { False -> (+) (tri 4) 5 ; True -> 1 } The bene.t is \nthat primitive redexes need not be constructed in memory, nor fetched again when needed. Even if the \nresult of a primitive redex is not needed, reducing it is no more costly than constructing it. We identify \nprimitive redexes at run-time, by dynamic analysis. Register File To support PRS, we introduce a register \n.le to the reduction machine, for storing the results of speculative reductions. > type RegFile = [Atom] \nThe body of a function may refer to these results as required. > data Atom = \u00b7\u00b7\u00b7 | REG Bool Int An atom \nof the form REG bi contains a reference i to a register, and a boolean .eld b that is true exactly if \nthere is more than one reference to the register in the body of the function. The instantiation functions \ninst and instApp are modi.ed to take the register .le r as an argument, and the following equation is \nadded to the de.nition of inst.  > inst s r base (REG sh i) = dashIf sh (r !! i) Waves The primitive \nredexes in a function body are evaluated in a series of waves. To illustrate, consider (+)1 ((+) 23). \nIn the .rst wave of speculative evaluation, (+) 2 3 would be reduced to 5; in the second wave, (+) 1 \n5 would be reduced to 6. More speci.cally, a wave is a list of independent primitive redex candidates. \nA primitive redex candidate is an application which may turn out at run-time to be a primitive redex. \nSpeci.cally, it is an application of the form [a0, PRI p, a1] where a0 and a1 are INT, ARG or REG atoms. \n> type Wave = [App] Templates are extended to contain a list of waves in which no application in a wave \ndepends on the result of an application in the same or a later wave. > type Template = (Arity, App, [App], \n[Wave]) Given the reduction stack, the heap, and a series of waves, PRS pro\u00adduces a possibly-modi.ed \nheap, and one result for each application in each wave. > prs :: Stack -> Heap -> [Wave] -> (Heap, RegFile) \n> prs s h = foldl (wave s) (h, []) > wave s (h,r) = foldl spec (h,r) . map (instApp s r h) If a primitive \nredex candidate turns out to be a primitive redex at run-time, it is reduced, and its result is appended \nto the register .le. Otherwise, the candidate application is constructed on the heap, and a pointer to \nthis application is appended to the register .le. > spec (h,r) [INT m,PRI p,INT n] = (h, r++[prim p m \nn]) > spec (h,r) app = (h++[app], r++[PTR False (length h)]) Function Application Since applications \nin a function body may refer to the results in the PRS register .le, PRS is performed before instantiation \nof the body. The new rule is: > step (p, h, FUN n f:s, u) = (p, h , s , u) > where > (pop, spine, apps, \nwaves) = p !! f > (h , r) = prs s h waves > s = instApp s r h spine ++ drop pop s > h = h ++ map (instApp \ns r h ) apps The template splitting technique outlined in \u00a74.2 is modi.ed to deal with waves of primitive \nredex candidates. Each wave is split into a separate template. If a wave contains more than MaxAppsPerBody \napplications, it is further split in order to satisfy the constraint. Strictness Analysis PRS works well \nwhen recursive call sites sustain unboxed arguments2. For example, if a call to tri is passed an unboxed \ninteger then, thanks to PRS, so too is the recursive call. However, if the initial call is passed a boxed \nexpression, primitive redexes never arise, e.g. the outer call in tri (tri 5) is passed a pointer to \nan application, inhibiting PRS. A basic strictness analyser in combination with the worker\u00adwrapper transformation \n[Gill and Hutton 2009] alleviates this prob\u00adlem. Each initial call to a recursive function is replaced \nwith a call to a wrapper function. The wrapper applies a special primitive to force evaluation of any \nstrict integer arguments before passing them on to the recursive worker. Performance Table 3 shows how \nPRS cuts run-time and heap\u00adusage over the range of benchmark programs. On average, the maximum stack \nusage drops from 811 to 104, and 85% of primitive redex candidates turn out to be primitive redexes. \n2 An unboxed integer is an integer literal INT n as opposed to a pointer PTR x to an expression of type \ninteger. GHC -O2 Clean Hand reds. Program Lines Run-time Run-time per Cycle Adjoxo 108 0.18 0.26 0.65 \nBraun 51 0.35 0.29 0.46 Cichelli 200 0.17 0.12 0.52 Clausify 132 0.33 0.27 0.54 CountDown 120 0.42 0.26 \n0.62 Fib 10 0.14 0.14 0.90 KnuthBendix 551 0.37 0.21 0.47 Mate 393 0.10 0.16 0.52 MSS 47 0.17 0.11 0.65 \nOrdList 46 0.64  0.43 PermSort 39 0.43 0.37 0.43 Queens 49 0.17 0.38 0.72 Queens2 62 0.34 0.31 0.40 \nSumPuz 158 0.23 0.21 0.46 Taut 97 0.32 0.16 0.46 While 96 0.27 0.19 0.49 Average 135 0.29 0.23 0.55 \nTable 4. Normalised run-times of GHC and Clean compiled code running on an Intel Core 2 Duo E8400 PC \nclocking at 3GHz. Run\u00adtimes are relative to 1.00, the run-time of Reduceron running on a Xilinx Virtex-5 \nFPGA clocking at 96MHz (over 30x slower). 6. Comparative Evaluation This section evaluates the Reduceron \nin the context of previous and current work on functional language implementation. 6.1 Benchmark programs \nThe performance of the Reduceron is measured using a set of 16 benchmark programs named in Table 4. The \nprograms, though small (the largest is 551 lines), are diverse and fairly representative of functional \nprograms in general. For details of the programs, including source code, see [Naylor et al. 2009]. 6.2 \nPrevious work on the Reduceron Compared to our previous work on the Reduceron presented in [Naylor and \nRunciman 2007], the implementation described in this paper reduces the number of clock-cycles required \nto run the bench\u00admark programs by an average factor of 6.4. As the previous imple\u00admentation clocks at \n111MHz, and the new one at 96MHz on the same FPGA, the raw speed-up factor is 5.5. The gains are mainly \ndue to the combined impact of improved case-expression compila\u00adtion, single-cycle reduction, and the \noptimisations listed in Table 3. But another factor is that the new implementation performs spine\u00adless \nevaluation [Burn et al. 1988]. During function application, the spine of a function body is only written \nonto the stack, reducing heap contention and heap usage. The spine is only ever written to the heap during \nupdating, and even then, only if it is a possibly\u00adshared normal-form. Spineless evaluation also avoids \nthe problem of indirection chains, and is more modular in the sense that it allows function application \nto be conceptually separated from updating.  6.3 State of the Art A run-time performance comparison \nof the Reduceron against state-of-the-art functional language implementations running on a 3GHz Intel \nCore 2 Duo PC is shown in Table 43. Given the speed-up over our previous implementation of the Re\u00adduceron, \nwe had hoped that the performance of our new implemen\u00ad 3 The Clean-compiled version of the OrdList program \ndoes not terminate due to a bug in the Clean compiler.  tation would approach that of the PC implementations. \nHowever, new GHC optimisations and the use of a 3GHz Core 2 Duo instead of a 2.8GHz Pentium-4 have signi.cantly \nboosted the PC results. (The Dhrystone MIPS (DMIPS) per MHz of the Core 2 Duo is al\u00admost twice that of \nthe Pentium-4 [Longbottom 2009].) It would be interesting to compare the Reduceron against GHC or Clean \ncompiled programs running on an FPGA soft-processor such as the Xilinx MicroBlaze. Unfortunately, this \nexperiment would be quite an undertaking since the run-time system of GHC or Clean would need to be ported \nto the FPGA environment. We can, however, point out that the Core 2 Duo achieves almost three times as \nmany DMIPs per MHz as the Xilinx MicroBlaze [MicroBlaze], and clocks 14 times faster. So the performance \nratio for this conven\u00adtional benchmarking is around 42. The performance ratio between the Reduceron and \nthe PC is an order of magnitude less. Table 4 also shows hand-reductions per clock-cycle. A hand\u00adreduction \nis the application of a function or a primitive function; it includes applications of functions introduced \nby case compila\u00adtion, but does not include updating, unwinding, integer reduction, constructor reduction, \nor applications of functions introduced by template splitting.  6.4 Modern Processors Modern microprocessors \nare the product of almost half a century of intensive engineering. Instruction pipelines with tens of \nstages have helped achieve clock frequencies in the region of 3-4GHz. Tech\u00adniques such as dynamic branch \nprediction, out of order execution, and caching have enabled high utilisation of such deep pipelines. \nThe Reduceron represents a different kind of processor: a vector processor. Rather than process one word \nat a time, it processes sev\u00aderal in parallel. It is not pipelined, so the sophisticated techniques needed \nto keep rapidly-clocked pipelines busy are not needed. 6.5 The G-machine In [Peyton Jones 1987], template \ninstantiation is presented as a simple .rst step towards a more sophisticated approach to graph reduction \nbased on the G-machine. So why is the Reduceron based on template instantiation and not the G-machine? \nThe G-machine approach aims to generate good code for con\u00adventional hardware, exploiting its strengths \nand avoiding its weak\u00adnesses. We base the Reduceron on template instantiation precisely because it does \nnot make assumptions about the target hardware. The G-machine executes a sequential stream of .ne-grained \nin\u00adstructions, many of which could in fact be executed in parallel. The FPGA negates the assumption that \nsuch a sequential stream of in\u00adstructions is necessary to avoid interpretive overhead. 6.6 Manipulating \nBasic Values One aspect of reduction that the G-machine approach aims to opti\u00admise is the processing \nof basic values such as integers. In particu\u00adlar, avoiding construction of strictly-needed primitive \napplications in memory can lead to large performance gains. For example, if a function body has the form \nf ((+) x 1) and f is strict then construction of (+) x1 on the heap can be avoided and instead reduced \nimmediately. The Reduceron can also avoid construction of primitive applica\u00adtions to good effect (\u00a75.3). \nHowever, it discovers suitable primitive applications at run-time and evaluates them speculatively. The \nRe\u00adduceron allows construction of (+) x1 to be avoided regardless of whether or not f is strict, but \nonly if x, at run-time, takes the form INT i. So the conditions under which construction of primitive \nap\u00adplications can be avoided are quite different between the two ap\u00adproaches. As discussed in \u00a75.3, strictness \nanalysis can aid PRS. But strictness analysis alone, without some mechanism for reduc\u00ading primitive redexes \ncheaply, is of little use to the Reduceron. PRS provides such a mechanism. 6.7 The SKIM Machine SKIM \nis a microcoded processor designed speci.cally to perform combinator reduction [Stoye 1985]. Stoye writes \nthat a combina\u00adtor reducer coded on an 8-MHz 68000 goes at about one thirtieth of the speed of SKIM, \nand was considerably harder to write than SKIM s microcode . One interesting aspect of SKIM is its use \nof one-bit reference counts. Stoye observes that such reference counts can be stored in the pointer to \nan application rather than in the application itself, making useful information about an application \navailable without the expense of dereferencing a pointer. A reference count bit indi\u00adcates whether the \npointer is a unique application pointer or mul\u00adtiple application pointer. This information is used to \ngood effect in SKIM by allowing the space pointed to by a unique pointer to be reused during reduction \nrather than discarded. On average about 70% of discarded cells are immediately reused. SKIM s successful \nuse of reference-count bits partly motivated the development of the dynamic sharing analysis presented \nin \u00a75.1. We have precisely speci.ed the modi.cations needed to implement dynamic sharing analysis in \na general graph reduction machine. We also discuss two important details not mentioned by Stoye: (1) \nthe subtle case in which an update can cause a unique pointer to be\u00adcome non-unique; and (2) Invariant \n3, an important key to under\u00adstanding why the technique actually works. We use the results of the analysis \nnot for storage reclamation (which would complicate the machinery for template instantiation), but for \nupdate avoidance. 6.8 Static versus Dynamic Analysis Sharing Analysis The idea to avoid updates by identifying \nnon\u00adshared applications is discussed in [Burn et al. 1988], including trade-offs between static and dynamic \nsharing analysis. The authors write that dynamic sharing analysis has the advantage of greater precision \nbut that in general we strongly suspect that the cost of dashing greatly outweighs the advantages of \nprecision when com\u00adpared to [static analysis] . In the Reduceron, dynamic sharing anal\u00adysis (dashing) \nhas no time cost: it is implemented in combinatorial logic that is not on the Reduceron s critical path. \nIt is precise and simple to implement, requiring only minor modi.cations to three of the Reduceron s \nreduction rules. Primitive Redex Analysis Primitive redexes can also be detected by static or dynamic \nanalysis. In our experience, a dynamic ap\u00adproach is simple and cheap to implement in hardware, and works \nquite well. As an alternative, we are currently trying a static analy\u00adsis to determine expressions whose \nevery instance at run-time will be a primitive redex. The analysis can be combined with specialisa\u00adtion \nto increase the incidence of such expressions. Eliminating the logic and memory capacity needed to handle \nfailed PRS candidates could signi.cantly boost performance.  6.9 The Big Word Machine A prototype machine \nsimilar in spirit to the Reduceron is Augusts\u00adson s Big Word Machine (BWM) [Augustsson 1992]. The BWM \nis a graph reduction machine with a wide word size, four pointers long, allowing wide applications to \nbe quickly built on, and fetched from, the heap. Augustsson likens the BWM to a VLIW (very long instruction \nword) machine [Hennessy and Patterson 1992], designed for functional languages rather than scienti.c \ncomputing. Like the Reduceron, the BWM has a crossbar switch attached to the stack allowing complex rearrangements \nto be done in a single clock-cycle. The BWM also uses the Scott encoding to imple\u00adment case expressions \nand constructors. Unlike the Reduceron, the BWM works on an explicit instruction stream rather than by \ntemplate instantiation. The BWM was never actually built. Some simulations were performed but Augustsson \nwrites the absolute performance of the machine is hard to determine at this point .  7. Conclusions \nand Future Work Considering their relatively low clocking frequencies, FPGA appli\u00adcations must exploit \nsigni.cant parallelism to achieve high perfor\u00admance. In the context of sequential graph-reduction, we \nhave taken this idea to its natural limit: each reduction rule is performed within one clock-cycle. Furthermore, \nupon synthesis our design achieves a respectable clock frequency compared to similar FPGA designs for \nthe same device. It is therefore quite hard to see how the Reduc\u00aderon s reduction rules could be performed \nmore quickly. On the other hand, there is a lot of scope to reduce the num\u00adber of reductions performed \nin a given program run. To this end, update avoidance and speculative evaluation of primitive redexes \nare both effective, making use of simple and precise dynamic anal\u00adyses. These dynamic analyses would \nhave a prohibitive run-time overhead on a PC, but have no such overhead on an FPGA. Compared to state-of-the-art \nfunctional language implementa\u00adtions running on a PC, the Reduceron implemented on a FPGA is on average \naround a factor of four slower. This difference may be disappointing, but it is an order of magnitude \nsmaller than the typical performance gap between PC-based hard-processors and FPGA-based soft-processors. \nFuture Work The main limitation of the current Reduceron im\u00adplementation is the small amount of heap \nmemory it provides. Could the heap be implemented using a larger, off-chip memory unit? We believe it \ncould, without loss of performance, and with\u00adout signi.cant modi.cation to the existing design. Two possible \noptions are: (1) the use of low-latency memory technologies such as RLDRAM, ZBT RAM, and QDR SRAM, commonly \nused by FPGA applications that require access to large amounts of mem\u00adory; and (2) the use of buffers \nor caches, implemented using on\u00adchip block RAM. Functional languages offer much scope for parallel evaluation \nof expressions. On conventional architectures there is a high cost for operations such as locking and \nreleasing expressions under eval\u00aduation, so the bene.ts of parallel evaluation are offset by signi.cant \ncommunication overheads. It would be interesting to see if special\u00adpurpose hardware could be used to \novercome such overheads. Mul\u00adtiple Reducerons could be synthesised to FPGA, coordinated for parallel \ngraph reduction [Clack 1999]. One of the main features of FPGAs that we are not exploiting is that they \ncan be con.gured on a per-program basis. One option would be to allow programmers to express, as part \nof their pro\u00adgram, custom FPGA logic that accelerates execution of that pro\u00adgram. Such logic would act \nas co-processor to the Reduceron, and could itself be suitably described in the functional source language. \nThe future development and competitiveness of special-purpose processors for graph reduction remains \nquestionable. But within a few years, just as plug-in GPU cards are already used for high-performance \ngraphics, we d like to see FPU cards for high\u00adperformance applications of functional languages. We hope \nour work on the Reduceron makes a small advance in that direction. Acknowledgments This work was supported \nby the Engineering and Physical Sciences Research Council of the UK under grant EP/G011052/1. Thanks \nto Xilinx for donating the FPGA used in this work, and to Satnam Singh, Gabor Greif, and the anonymous \nICFP reviewers for their helpful suggestions. References [Augustsson 1992] L. Augustsson. BWM: A Concrete \nMachine for Graph Reduction. In Proceedings of the 1991 Glasgow Workshop on Functional Programming, pages \n36 50, Springer, 1992. [Burn et al. 1988] G. L. Burn, S. L. Peyton Jones and J. D. Robson. The Spineless \nG-machine. In Proceedings of the 1988 Conference on Lisp and Functional Programming, pages 244-258, ACM, \n1988. [Clack 1999] C. Clack. Realisations for non-strict languages. In Research Directions in Parallel \nFunctional Programming, pages 149187. Springer, 1999. [Dijkstra 1980] E. W. Dijkstra. A mild variant \nof Combinatory Logic. EWD735, 1980. [Fasel and Keller 1987] J. H. Fasel and R. M. Keller, editors. Graph \nReduction, Proceedings of a Workshop. Springer LNCS 279, 1987. [Flanagan et al. 1993] C. Flanagan, A. \nSabry, and B. F. Duba, and M. Felleisen. The essence of compiling with continuations. In PLDI 93: Proceedings \nof the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation, pages 237 247, \nACM, 1993. [Gill and Hutton 2009] A. Gill and G. Hutton. The Worker/Wrapper Transformation. JFP, volume \n18, part 2, pages 227 251, 2009. [Hennessy and Patterson 1992] J. Hennessy and D. Patterson. Computer \nArchitecture; A Quantitative Approach. Morgan Kaufmann, 1992. [Jansen et al. 2007] J. M. Jansen, P. Koopman, \nR. Plasmeijer. Ef.cient Interpretation by Transforming Data Types and Patterns to Functions. In Trends \nin Functional Programming, volume 7, pages 157 172, 2007. [Jones and Lins 1996] R. Jones and R. Lins. \nGarbage Collection: Algo\u00adrithms for Automatic Dynamic Memory Management. Wiley, 1996. [Kennaway and Sleep \n1988] R. Kennaway and R. Sleep. Director strings as combinators. ACM Transactions on Programming Languages \nand Systems, volume 10, number 4, pages 602 626, 1988. [Longbottom 2009] R. Longbottom. Dhrystone Benchmark \nResults On PCs, November 2009. (http://www.roylongbottom.org.uk/ dhrystone%20results.htm) [Naylor and \nRunciman 2007] M. Naylor and C. Runciman. The Reduc\u00aderon: Widening the von Neumann bottleneck for graph \nreduction using an FPGA. In IFL 07, pages 129 146. Springer LNCS 5083, 2008. [Naylor et al. 2009] M. \nNaylor, C. Runciman, and J. Reich. Reduceron home page. (http://www.cs.york.ac.uk/fp/reduceron/) [Memo \n9] M. Naylor. F-lite: a core subset of Haskell, 2008. (http: //www.cs.york.ac.uk/fp/reduceron/memos/Memo9.txt) \n[Memo 12] M. Naylor. An algorithm for arity-reduction, 2008. (http: //www.cs.york.ac.uk/fp/reduceron/memos/Memo12.lhs) \n[Memo 27] M. Naylor. Design of the Octostack, 2009. (http://www.cs. york.ac.uk/fp/reduceron/memos/Memo27.lhs) \n[MicroBlaze] Xilinx. MicroBlaze Soft Processor v7.20, April 2009. (http://www.xilinx.com/tools/microblaze.htm) \n[Peyton Jones 1987] S. L. Peyton Jones. The Implementation of Functional Programming Languages, Prentice \nHall, 1987. [Peyton Jones 1992] S. L. Peyton Jones. Implementing lazy functional languages on stock hardware: \nthe Spineless Tagless G-machine. Journal of Functional Programming, volume 2, pages 127-202, 1992. [Scheevel \n1986] M. Scheevel. NORMA: a graph reduction processor. In Proceedings of the 1986 Conference on LISP \nand Functional Programming, pages 212-219. ACM, 1986. [Stoye 1985] W. Stoye. The Implementation of Functional \nLanguages using Custom Hardware. PhD Thesis, University of Cambridge, 1985. [Turner 1979] D. A. Turner. \nA New Implementation Technique for Applicative Languages. Software Practice and Experience, volume 9, \nnumber 1, pages 31 49, 1979. [Weicker 1984] R. Weicker. Dhrystone: A Synthetic Systems Programming Benchmark. \nCommunications of the ACM, volume 27, number 10, pages 1013-1030, 1984.    \n\t\t\t", "proc_id": "1863543", "abstract": "<p>The leading implementations of graph reduction all target conventional processors designed for low-level imperative execution. In this paper, we present a processor specially designed to perform graph-reduction. Our processor -- the Reduceron -- is implemented using off-the-shelf reconfigurable hardware. We highlight the low-level parallelism present in sequential graph reduction, and show how parallel memories and dynamic analyses are used in the Reduceron to achieve an average reduction rate of 0.55 function applications per clock-cycle.</p>", "authors": [{"name": "Matthew Naylor", "author_profile_id": "81337492174", "affiliation": "University of York, York, United Kingdom", "person_id": "P2338151", "email_address": "", "orcid_id": ""}, {"name": "Colin Runciman", "author_profile_id": "81100654458", "affiliation": "University of York, York, United Kingdom", "person_id": "P2338152", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863556", "year": "2010", "article_id": "1863556", "conference": "ICFP", "title": "The reduceron reconfigured", "url": "http://dl.acm.org/citation.cfm?id=1863556"}