{"article_publication_date": "09-27-2010", "fulltext": "\n VeriML: Typed Computation of Logical Terms inside a Language with Effects Antonis Stampoulis Zhong \nShao Department of Computer Science Yale University New Haven, CT 06520-8285 {antonis.stampoulis,zhong.shao}@yale.edu \nAbstract Modern proof assistants such as Coq and Isabelle provide high de\u00adgrees of expressiveness and \nassurance because they support for\u00admal reasoning in higher-order logic and supply explicit machine\u00adcheckable \nproof objects. Unfortunately, large scale proof develop\u00adment in these proof assistants is still an extremely \ndif.cult and time\u00adconsuming task. One major weakness of these proof assistants is the lack of a single \nlanguage where users can develop complex tac\u00adtics and decision procedures using a rich programming model \nand in a typeful manner. This limits the scalability of the proof devel\u00adopment process, as users avoid \ndeveloping domain-speci.c tactics and decision procedures. In this paper, we present VeriML a novel language \ndesign that couples a type-safe effectful computational language with .rst\u00adclass support for manipulating \nlogical terms such as propositions and proofs. The main idea behind our design is to integrate a rich \nlogical framework similar to the one supported by Coq inside a computational language inspired by ML. \nThe language design is such that the added features are orthogonal to the rest of the computational language, \nand also do not require signi.cant additions to the logic language, so soundness is guaranteed. We have \nbuilt a prototype implementation of VeriML including both its type-checker and an interpreter. We demonstrate \nthe effectiveness of our design by showing a number of type-safe tactics and decision procedures written \nin VeriML. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory \nGeneral Terms Languages, Veri.cation 1. Introduction In recent years, there has been a growing interest \nin formal veri.ca\u00adtion of substantial software code bases. Two of the most signi.cant examples of this \ntrend is the veri.cation of a full optimizing com\u00adpiler for a subset of the C language in the CompCert \nproject [Leroy 2009], as well as the veri.cation of the practical operating sys\u00adtem microkernel seL4 \n[Klein et al. 2009]. Both of these efforts use powerful proof assistants such as Coq [Barras et al. 2010] \nand Is\u00adabelle [Nipkow et al. 2002] which support higher-order logic with Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, \nMaryland, USA. Copyright c &#38;#169; 2010 ACM 978-1-60558-794-3/10/09. . . $10.00 explicit proof objects. \nOther veri.cation projects have opted to use .rst-order automated theorem provers; one such example is \nthe cer\u00adti.ed garbage collector by Hawblitzel and Petrank [2009]. Still, the actual process of software \nveri.cation requires signif\u00adicant effort, as clearly evidenced by the above developments. We believe \nthat a large part of this effort could be reduced, if the under\u00adlying veri.cation frameworks had better \nsupport for extending their automation facilities. During a large proof development, a number of user-de.ned \ndatatypes are used; being able to de.ne domain\u00adspeci.c decision procedures (for these) can signi.cantly \ncut back on the manual proof effort required. In other cases, different pro\u00adgram logics might need to \nbe de.ned to reason about different parts of the software being veri.ed, as is argued by Feng et al. \n[2008] for the case of operating system kernels. In such cases, developing au\u00adtomated provers tailored \nto these logics would be very desirable. We thus believe that in order to be truly extensible, a proof \ndevelopment framework should support the following features: Use of a well-established logic with well-understood \nmetathe\u00adory, that also provides explicit proof objects. This way, the trusted computing base of the veri.cation \nprocess is kept at a minimum. The high assurance offered by developments such as CompCert and seL4 owes \nlargely to this characteristic of the proof assistants they are developed on.  Being able to programmatically \nre.ect on logical terms (e.g., propositions) so that we can write a large number of procedures (e.g., \ntactics, decision procedures, and automated provers) tai\u00adlored to solving different proof obligations. \nFacilities such as LTac [Delahaye 2000], and their use in developments like Chli\u00adpala et al. [2009], \ndemonstrate the bene.ts of this feature.  An unrestricted programming model for developing these pro\u00adcedures, \nthat permits the use of features such as non-termination and mutable references. The reason for this \nis that even sim\u00adple decision procedures might make essential use of impera\u00adtive data structures and \nmight have complex termination argu\u00adments. One such example are decision procedures for the theory of \nequality with uninterpreted functions [Bradley and Manna 2007]. By enabling an unrestricted programming \nmodel, port\u00ading such procedures does not require signi.cant re-engineering.  At the same time, being \nable to provide certain static guaran\u00adtees and rich type information to the programmer. Terms of a formal \nlogic come with rich type information: proof objects have types representing the propositions that they \nprove, and propositions themselves are deemed to be valid according to some typing rules. By retaining \nthis information when pro\u00adgrammatically manipulating logical terms, we can specify the behavior of the \nassociated code. For example, we could stati\u00adcally specify that a tactic transforms a propositional goal \ninto   Figure 1. Schematic comparison of the structure of related approaches guaranteesstaticnon-termination \nmutable an equivalent one, by requiring a proof object witnessing this equivalence. Another guarantee \nwe would like is the correct handling of the binding constructs that logical terms include (e.g. quanti.cation \nin propositions), so that this common source of errors is statically avoided. A framework that combines \nthese features is not currently avail\u00adable. As a result, existing proof assistants must rely on a mix \nof languages (with incompatible type systems) to achieve a certain degree of extensibility. In this paper, \nwe present VeriML a novel language design that aims to support all these features and provide a truly \nextensible and modular proof development framework. Our Traditional LCF no no yes (maybe) LTac no yes \nno yes Re.ection-based yes no no yes Beluga, Delphin yes yes no no VeriML yes yes yes yes Table 1. Comparison \nof different approaches based on features paper makes the following new contributions: As far as we \nknow, VeriML is the .rst proof framework that successfully combines a type-safe effectful computational \nlan\u00adguage with .rst-class support for manipulating rich logical terms such as propositions, proofs, and \ninductive de.nitions.  An important feature of VeriML is the strong separation of roles played by its \nunderlying logic language and computational language. The logic language, .HOLind, supports higher-order \nlogic with inductive de.nitions (as in Coq), so it can both serve as a rich meta logic and be used to \nde.ne new object logics/languages and reason about their meta theory. All proof objects in VeriML can \nbe represented using .HOLind alone. The computational language is used only for general-purpose programming, \nincluding typed manipulation of logical terms. This is in sharp contrast to recent work such as Beluga \n[Pientka and Dun.eld 2008] and Delphin [Poswolsky and Sch\u00a8urmann 2008] where meta-logical proofs are \nrepresented using their computational languages. Maintaining soundness of such proofs when adding imperative \nfeatures to these languages would be non-trivial, and would put additional burden (e.g. effect anno\u00adtations) \non general-purpose programming.  We present the complete development of the type system and operational \nsemantics for VeriML, as well as their associated meta-theory . We also show how to adapt contextual \nmodal type theory to work for a rich meta-logic such as .HOLind.  We have built a prototype implementation \nof VeriML and used it to write a number of type-safe tactics and decision proce\u00addures. We use these examples \nto demonstrate the applicability of our approach and show why it is important to support type\u00adsafe handling \nof binders, general recursion, and imperative fea\u00adtures such as arrays and hash tables in VeriML.  The \nrest of the paper is organized as follows. We .rst give a high-level overview of VeriML (Sec 2) and then \nuse examples to explain the basic design (Sec 3); we then present the logic language, the computational \nlanguage, and their meta-theory (Sec 4-5); .\u00adnally, we describe the implementation and discuss related \nwork. 2. Overview of the language design We will start off by presenting a high-level overview of our \nframe\u00adwork design. The .rst choice we have to make is the formal logic that we will use to base our framework. \nWe opt to use a higher\u00adorder logic with inductive de.nitions and explicit proof objects. This gives us \na high degree of expressivity, enough for software veri.cation as evidenced by the aforementioned large-scale \nproof developments, while at the same time providing a high level of as\u00adsurance. Furthermore, we allow \na notion of computation inside our logic, by adding support for de.ning and evaluating total recur\u00adsive \nfunctions. In this way, logical arguments that are based solely on such computation need not be explicitly \nwitnessed in proof ob\u00adjects, signi.cantly reducing their sizes. This notion of computation must of course \nbe terminating, in order to maintain soundness. Be\u00adcause of this characteristic, our logic satis.es what \nwe refer to as the Poincar\u00b4e Principle (abbreviated as P.P.), following the de.nition in Barendregt and \nGeuvers [1999]. Last, we choose to omit cer\u00adtain features like dependent types from our logic, in order \nto keep its metatheory straightforward. We will see more details about this logic in Section 4. We refer \nto propositions, inhabitants of inductive types, proof objects, and other terms of this logic as logical \nterms. Developing proofs directly inside this logic can be very tedious due to the large amount of detail \nrequired; because of this, proof development frameworks provide a set of computational functions (tactics \nand decision procedures) that produce parts of proofs, so that the proof burden is considerably lessened. \nThe problem that we are interested in is the design of a computational language, so that such functions \ncan be easily and effectively written by the user for the domains they are interested in, leading to \na scalable and modular proof development style. As we have laid out in the introduction, we would like \na number of features out of such a computational language: being able to programmatically pattern match \non propositions, have a general\u00adpurpose programming model available, and provide certain static guarantees \nto the programmer. Let us brie.y consider how computational approaches in exist\u00ading proof assistants \nfare towards those points. A schematic com\u00adparison is given in Figure 1, while Table 1 compares existing \nap\u00adproaches based on these points. A standard and widely available approach [Slind and Norrish 2008, \nHarrison 1996, Nipkow et al. 2002, Barras et al. 2010] is to write user-de.ned tactics and deci\u00adsion \nprocedures inside the implementation language of the proof assistant, which is in most cases a member \nof the ML family of languages. This gives to the user access to a rich programming model, with non-terminating \nrecursion and imperative data struc\u00adtures. Still, the user has to deal with the implementation details \nof the framework, and no static guarantees are given whatsoever. All logical terms are essentially identi.ed \nat the ML type level, leading to an untyped programming style when programming with them.  Another approach \nis the use of LTac [Delahaye 2000] in the Coq proof assistant: a specialized tactic language that allows \npattern matching on propositions, backtracking proof search and general recursion. This language too \ndoes not provide any static guarantees, and has occasional issues when dealing with binders and variables. \nAlso, the programming model supported is relatively poor, without support for rich data structures or \nimperativity. An interesting approach is the technique of proof-by-re.ection [Boutin 1997], where the \ncomputational notion inside the logic itself is used in order to create certi.ed decision procedures. \nWhile this approach gives very strong static guarantees (total correctness), it does not support non-termination \nor imperative data structures, limiting the kind of decision procedures we can write. Also, the use of \na mix of languages is required in this technique. In order to combine the bene.ts of these approaches, \nwe pro\u00adpose a new language design, that couples a general-purpose pro\u00adgramming language like ML with \n.rst-class support for our logical framework. Furthermore we integrate the type system of the logic inside \nthe type system of the computational language, leading to a dependently typed system. Logical term literals \nwill thus retain the type information that can be statically determined for them. More\u00adover, a pattern \nmatching construct for logical terms is explicitly added, which is dependently typed too; the type of \neach branch depends on the speci.c pattern being matched. We use dependent types only as a way to provide \nlightweight static guarantees. For example, we can require that a function receives a proposition and \nreturns a proof object for that proposition, ruling out the possibil\u00adity of returning an invalid proof \nobject because of programmer error. Our approach therefore differs from other dependently-typed frameworks \nlike Agda [Norell 2007] or HTT [Nanevski et al. 2006], as we are not necessarily interested in reasoning \nabout the cor\u00adrectness of code written in our computational language. Also, pro\u00adgramming in such systems \nincludes an aspect that amounts to proof development, as evidenced e.g. in the Russell framework [Sozeau \n2007]. We are interested in how proof development itself can be au\u00adtomated, so our approach is orthogonal \nto such systems. Our notion of pattern matching on propositions would amount to typecase-like constructs \nin these languages, which in general are not provided. Dependently-typed frameworks for computing with \nLF terms like Beluga [Pientka and Dun.eld 2008] and Delphine [Poswolsky and Sch\u00a8 urmann 2008], are not \nideal for our purposes, because of the lack of imperative features and the fact that encoding a logic \nlike the one we describe inside LF is dif.cult and has not been demonstrated yet in practice. The reason \nfor this is exactly our logic s support for the Poincar\u00b4 e principle. Still, we draw inspiration from \nsuch frameworks. In order to type-check logical terms, and make sure that bind\u00ading is handled correctly, \nwe need information about the free vari\u00adables context that they depend on. Not all logical terms manipu\u00adlated \nduring evaluation of a program written in our language need to refer to the same context; for example, \nwhen pattern matching a quanti.ed proposition like .x : Nat.P, the variable P might re\u00adfer to an extra \nvariable compared to the original proposition that was being matched. Therefore, in order to guarantee \nproper scop\u00ading of variables used inside the logical terms, the type system of our computational language \ntracks the free variable context of the logical terms that are manipulated. This is done by using the \nmain idea of contextual modal type theory [Nanevski et al. 2008, Pientka 2008]. We introduce a notion \nof contextual logical terms, that is, logical terms that come packaged together with the free variables \ncontext they depend on. Our computational language manipulates such terms, instead of normal logical \nterms, which would also need some external information about their context. A notion of context polymorphism \nwill also need to be introduced, in order to write code that is generic with respect to variable contexts. \n3. Programming Examples In this section we will present a number of programming exam\u00adples in our computational \nlanguage in order to demonstrate its use as well as motivate some of our design choices, before pre\u00adsenting \nthe full technical details in later sections. Each example demonstrates one particular feature of the \ncomputational language. The examples we will present are successive versions of a tac\u00adtic that attempts \nto automatically derive intuitionistic proofs of propositional tautologies (similar to Coq s tauto tactic \n[Barras et al. 2010]) and a decision procedure for the theory of equality, along with the data structures \nthat they use. Note that we use a some\u00adwhat informal style here for presentation purposes. Full details \nfor these examples can be found as part of our implementation at http://flint.cs.yale.edu/publications/veriml.html. \n3.1 Pattern matching We will start with the automatic tautology proving tactic which is structured as \nfollows: given a proposition, it will perform pattern matching in order to deconstruct it, and attempt \nto recursively prove the included subformulas; when this is possible, it will return a proof object of \nthe given proposition. Some preliminary code for such a function follows, which handles only logical \nconjunction and disjunction, and the True proposition as a base case. Note that we use a monadic do notation \nin the style of Haskell for the failure monad (computation with the ML option type), and we use the syntax \ne1||e2 where both expressions are of option type in order to choose the second expression when the .rst \none fails. We use the holcase \u00b7 of \u00b7\u00b7\u00b7 construct to perform pattern matching on a logical term. Also, \nwe use the notation (\u00b7) to denote the lifting of a logical term into a computational language term. Under \nthe hood, this is an existential package which packages a logical term with the unit value. Here we have \nused it so that our function might return proof objects of the relevant propositions. We have not written \nout the details of the proof objects themselves to avoid introducing unnecessary technical details at \nthis point, but they are straightforward to arrive at. tauto P = holcase P of P1 . P2 . do pf1 . tauto \nP1; pf2 . tauto P2; (\u00b7\u00b7\u00b7 proof of P1 . P2 \u00b7\u00b7\u00b7) | P1 . P2 . (do pf1 . tauto P1; (\u00b7\u00b7\u00b7 proof of P1 . P2 \n\u00b7\u00b7\u00b7)) || (do pf2 . tauto P2; (\u00b7\u00b7\u00b7 proof of P1 . P2 \u00b7\u00b7\u00b7)) | True . Some (\u00b7\u00b7\u00b7 proof of True \u00b7\u00b7\u00b7) | P' . \nNone We assign a dependent type to this function, requiring that the proof objects returned by the function \nprove the proposition that is given as an argument. This is done by having the pattern matching be dependently \ntyped too; we will see the details of how this is achieved after we describe our type system. We use \nthe notation LT(\u00b7) to denote lifting a logical term into the level of computational types; thus the function \ns type will be: .P : Prop.option LT(P)  3.2 Handling binders and free variables Next we want to handle \nuniversally quanti.ed propositions. Our procedure needs to go below the quanti.er, and attempt to prove \nthe body of the proposition; it will succeed if this can be done para\u00admetrically with regards to the \nnew variable. In this case, the pro\u00adcedure we have described so far will need to be run recursively on \na proposition with a new free variable (the body of the quanti.er). To avoid capture of variables, we \nneed to keep track of the free variables used so far in the proposition; we do this by having the function \nalso take the free variables context of the proposition as an argument. Also, we annotate logical terms \nwith the free variables context they refer to. Thus we will be handling contextual terms in our computational \nlanguage, i.e., logical terms packaged with the free variables context they refer to. We use f to range \nover contexts (their kind is abbreviated as ctx), and the notation [f]Prop to de\u00adnote propositions living \nin a context f; a similar notation is used for other logical terms. The new type of our function will \nthus be:  .f : ctx..P : [f]Prop.option LT([f] P) tauto f P = holcase P of P1 . P2 . do pf1 . tauto f \nP1; pf2 . tauto f P2; (\u00b7\u00b7\u00b7 proof of P1 . P2 \u00b7\u00b7\u00b7) |.x : A.P . do pf . tauto (f, x : A) P; (\u00b7\u00b7\u00b7 proof of \n.x : A.P \u00b7\u00b7\u00b7)| \u00b7\u00b7\u00b7 Here the proof object pf returned by the recursive call on the body of the quanti.er \nwill depend on an extra free variable; this dependence is to be discharged inside the proof object for \n.x : A.P. Let us now consider the case of handling a propositional impli\u00adcation like P1 . P2. In this \ncase we would like to keep information about P1 being a hypothesis, so as to use it as a fact if it is \nlater en\u00adcountered inside P2 (e.g. to prove a tautology like P . P). The way we can encode this in our \nlanguage is to have our tauto procedure carry an extra list of hypotheses. The default case of pattern \nmatch\u00ading can be changed so that this list is searched instead of returning None as we did above. Each \nelement in the hypothesis list should carry a proof object of the hypothesis too, so that we can return \nit if the hypothesis matches the goal. Thus each element of the hy\u00adpothesis list must be an existential \npackage, bundling together the proposition and the proof object of each hypothesis: hyplist = .f : ctx.list \n(SH : [f]Prop.LT([f]H)) The type list is the ML list type; we assume that the standard map and fold left \nfunctions are available for it. For this data structure, we de.ne the function hyplistWeaken which lifts \na hypotheses list from one context to an extended one; and the function hyplistFind which given a proposition \nP and a hypotheses list, goes through the list trying to .nd whether the proposition P is included, returning \nthe associated proof object if it is. For hyplistWeaken we only give its desired type; we will see its \nfull de.nition in Section 5, after we have introduced some details about the logic and the computational \nlanguage. hyplistWeaken : .f : ctx..A : [f] Prop.hyplist f . hyplist (f, x : A) hypMatch : .f : ctx..P \n: [f]Prop.(SP' : [f]Prop.LT([f]P')) . option LT([f]P) hypMatch f P hyp = let (P' , pf ) = hyp in holcase \nP of P' . Some pf | . None hyplistFind : .f : ctx..P : [f] Prop.hyplist f . option LT([f]P) hyplistFind \nf P hl = fold left (.res..hyp.res || hypMatch P hyp) None hl Note that we use the notation (\u00b7, \u00b7) as \nthe introduction form for existential packages, and let (\u00b7, \u00b7) = \u00b7 in \u00b7\u00b7\u00b7 as the elimination form. The \ntauto tactic should itself be modi.ed as follows. When trying to prove P1 . P2, we want to add P1 as \na hypothesis to the current hypotheses list hl. In order to provide a proof object for this hypothesis, \nwe introduce a new free variable pf1 representing the proof of P1 and try to prove P2 recursively in \nthis extended context using the extended hypotheses list. Note that we need to lift all terms in the \nhypotheses list to the new context before being able to use it; this is what hyplistWeaken is used for. \nThe proof object returned for P2 might mention the new free variable pf1. This extra dependence is discharged \nusing the implication introduction axiom of the underlying logic, yielding a proof of P1 . P2. Details \nare shown below. tauto : .f : ctx..P : [f] Prop.hyplist f . option LT([f]P) tauto f P hl = holcase P \nof ... | P1 . P2 . let hl = hyplistWeaken f P1 hl in let hl = cons (P1, ([f, pf1 : P1]pf1)) hl in do \nx . tauto (f, pf1 : P1) P2 hl ; (\u00b7\u00b7\u00b7 proof of P1 . P2 \u00b7\u00b7\u00b7) | . hyplistFind f P hl  3.3 General recursion \nWe have extended this procedure in order to deconstruct the hy\u00adpothesis before entering it in the hypothesis \nlist (e.g. entering two different hypotheses for P1 . P2 instead of just one, etc.), but this extension \ndoes not give us any new insights with respect to the use of our language so we do not show it here. \nA more interesting modi.cation we have done is to extend the procedure that searches the hypothesis list \nfor the current goal, so that when trying to prove the goal G, a hypothesis like H'. G can be used, making \nH' a new goal. This is easy to achieve: we can have the hyplistFind procedure used above be mutually \nrecursive with tauto, and have it pattern-match on the hypotheses, calling tauto recursively for the \nnewly generated goals. Still, we need to be careful in order to avoid recursive cycles. A naive implementation \nwould be thrown into an endless loop if a proof for a proposition like (A . B) . (B . A) . A was attempted. \nThe way to solve this is to have the two procedures maintain a list of already visited goals, so that \nwe avoid entering a cy\u00adcle. Using the techniques we have seen so far, this is easy to en\u00adcode in our \nlanguage. This extension complicates the termination argument for our tactic substantially, but since \nwe are working in a computational language allowing non-termination, we do not need to formalize this \nargument. This is a point of departure compared to an implementation of this tactic based on proof-by-re.ection, \ne.g. similar to what is described in Chlipala [2008]. In that case, the most essential parts of the code \nwe are describing would be written inside the computational language embedded inside the logic, and as \nsuch would need to be provably terminating. The complicated termination argument required would make \nthe effort required for this extension substantial. Compared to programming a similar tac\u00adtic in a language \nlike ML (following the traditional LCF approach), in our implementation the partial correctness of the \ntactic is estab\u00adlished statically. This is something that would otherwise be only achievable by using \na proof-by-re.ection based implementation.  3.4 Imperative features The second example that we will \nconsider is a decision procedure that handles the theory of equality, one of the basic theories that \nSMT solvers support. This is the theory generated from the axioms: .x.x = x .x, y.x = y . y = x .x, y, \nz.x = y . y = z . x = z In a logic like the one we are using, the standard de.nition of equality includes \na single constructor for the re.exivity axiom; the other axioms above can then be proved as theorems \nusing inductive elimination. We will see how this is done in the next section. Usually this theory is \nextended with the axiom: .x, y.x = y . fx = fy which yields the theory of equality with uninterpreted \nfunctions (EUF). We have implemented this extension but we will not con\u00adsider it here because it does \nnot add any new insights. To simplify our presentation, we will also assume that all terms taking part \nin equations are of a .xed type A.  We want to create a decision procedure that gets a list of equa\u00adtions \nas hypotheses, and then prove whether two terms are equal or not, according to the above axioms and based \non the given equa\u00adtions. The standard way to write such a decision procedure is to use a union-.nd data \nstructure to compute the equivalence classes of terms, based on the given equations. We will use a simple \nalgo\u00adrithm, described in Bradley and Manna [2007], which still requires imperative features in order \nto be implemented ef.ciently. Terms are assigned nodes in a tree-like data structure, which gets usually \nimplemented as an array. Each equivalence class has one representative; each node representing a term \nhas a pointer to a parent term, which is another member of its equivalence class; if a term s node points \nto itself, then it is the representative of its class. We can thus .nd the representative of the equivalence \nclass where a term belongs by successively following pointers, and we can merge two equivalence classes \nby making the representative of one class point to the representative of the other. We want to stay as \nclose as possible to this algorithm, yet have our procedure yield proof objects for the claimed equations. \nWe choose to encode the union-.nd data structure as a hash table; this table will map each term into \na (mutable) value representing its parent term. Since we also want to yield proofs, we need to also store \ninformation on how the two terms are equal. We can encode such a hash table using the following type \n(assuming that terms inhabit a context f): eqhash = array (SX : [f]A.SX' : [f]A.LT([f]X = X')) We can \nread the type of elements in the array as key-value pairs, where the key is the .rst term of type A, \nand the value is the existential package of its parent along with an appropriately typed proof object. \nImplementing such a hash-table structure is straightforward, provided that there exists an appropriate \nconstruct in our compu\u00adtational language to compute a hash value for a logical term. We can have dependent \ntypes for the get/set functions as follows: eqhashGet : .X : [f]A.eqhash . option (SX' : [f]A.LT([f] \nX = X')) ' eqhashSet : .X : [f]A..X' : [f]A..pf : [f]X = X.eqhash . unit The .nd operation for the union-.nd \ndata structure can now be simply implemented using the following code. Given a term, we need to return \nthe representative of its equivalence class, along with a proof of equality of the two terms. We look \nup a given term in the hash table, and keep following links to parents until we end up in a term that \nlinks to itself, building up the equality proof as we go; if the term does not exist in the hash table, \nwe simply add it. .nd : .X : ([f] A).eqhash . SX' : ([f]A).LT([f]X = X') .nd X h = (do x . eqhashGet \nX h; ' let (X, pf :: X = X') = x in holcase X of '' X .(X, pf) '' | . let (X, pf :: X' = X'') = .nd \nX' h in '' '' (X, (\u00b7\u00b7\u00b7 proof of X = X\u00b7\u00b7\u00b7))) || (let self = (X, \u00b7\u00b7\u00b7 proof of X = X \u00b7\u00b7\u00b7) in (eqhashSet \nX self); self) The union operation is given two terms along with a proof that they are equal, and updates \nthe hash-table accordingly: it uses .nd to get the representatives of the two terms, and if they do not \nmatch, it updates the one representative to point to the other. ' union : .X : [f]A..X' : [f]A..pf : \n[f]X = X.eqhash . unit union XX' pf h =  letXrep, pf1 :: X = Xrep= .nd X h in ' '' letXrep, pf2 :: \nX= Xrep= .nd X' h in holcase Xrep of ' Xrep . () '' | . let upd =X\u00b7\u00b7\u00b7 proof of Xrep = X\u00b7\u00b7\u00b7in rep,rep \neqhashSet Xrep upd A last function is needed, which will be used to check whether in the current hash \ntable, two terms are equal or not. Its type will be: areEqual? : .X : ([f]A)..X' : ([f]A).eqhash . option \nLT([f]X = X') Its implementation is very similar to the above function. In the implementation we have \nseen above, we have used an im\u00adperative data-structure with a dependent data type, that imposes an algorithm-speci.c \ninvariant. Because of this, rich type information is available while developing the procedure, and the \ntype restric\u00adtions impose a principled programming style. At the same time, a multitude of bugs that \ncould occur in an ML-based implementa\u00adtion are avoided: at each point where a proof object is explicitly \ngiven in the above implementation, we know that it proves the ex\u00adpected proposition, while in an ML-based \nimplementation, no such guarantee is given. Still, adapting the standard implementation of the algorithm \nto our language is relatively straightforward, and we do not need to use fundamentally different data \nstructures, as we would need to do if we were developing this inside the compu\u00adtational language of a \nlogic (since only functional data structures could be used). 4. The logic language .HOLind We will now \nfocus on the formal logic that we are using. We use a higher-order logic with support for inductive de.nitions \nof data\u00adtypes, predicates and logical connectives; such inductive de.nitions give rise to inductive elimination \naxioms. Also, total recursive functions can be de.ned, and terms of the logic are identi.ed up to evaluation \nof these functions. Our logical framework also consists of explicit proof objects, which can be viewed \nas witnesses of derivations in the logic. This framework, which we call .HOLind, is based on .HOL as \npresented in Barendregt and Geuvers [1999], extended with inductive de.nitions and a reduction relation \nfor total recursive functions, in the style of CIC [Barras et al. 2010]. Alternatively, we can view this \nframework as a subset of CIC, where we have omitted universes other than Prop and Type, as well as polymorphism \nand dependent types in Type. Logical consistency of CIC [Werner 1994] therefore implies logical consistency \nof our system. Still, a simpler metatheory based on reducibility candidates is possible. We view this \nlogical framework as a common core between proof assistants like Coq and the HOL family, that is still \nexpress\u00adible enough for many applications. At the same time, we believe that it captures most of the \ncomplexities of their logics (e.g. the no\u00adtion of computation in CIC), so that the results that we have \nfor this framework can directly be extended to them. The syntax of our framework is presented in Figure \n2. The syntactic category d includes propositions (which we denote as P) and predicates, as well as objects \nof our domain of discourse: terms of inductively de.ned data types, as well as total functions between \nthem. Inductive de.nitions come from a de.nitions environment .; total functions are de.ned by primitive \nrecursion (using the Elim(\u00b7,\u00b7) construct). Terms of this category get assigned kinds of the syntactic \ncategory K, with all propositions being assigned kind Prop. Inductive datatypes are de.ned at this level \nof kinds. We  (sorts) s ::= Type | Type ' (kinds) K ::= Prop | cK | K1 . K2 | x (domain obj./props.) \nd,P ::= d1 . d2 |.x : K.d | .x : K.d | d1 d2 | cd | x | Elim(cK,K ') (proof objects) p ::= x | .x : P.p \n| p1 p2 | .x : K.p | p d | cp | elim cK | elim cd (HOL terms) t ::= s | K | d | p (logic variables env.) \nF ::= | F, x : t (de.nitions env.) . ::= ---. | ., Inductive cK : Type := {cd : K} --. - -. Figure 2. \nSyntax of the base logic language .HOLind Typing for domain objects, propositions and predicates: x : \nK . FF f P1: Prop F f P2: Prop DP-VAR DP-IMPL F f x : K F f P1 . P2: Prop F,x : K f P : Prop F, x : K \nf d : K ' DP-FORALL DP-LAM F f.x : K.P : Prop F f .x : K.d : K . K ' F f d1: K . K ' F f d2: K DP-APP \nF f d1 d2: K ' Typing for proof objects: x : P . FF, x : P f p : P' F f P . P' : Prop PO-VAR PO-IMPI \nF f x : P F f .x : P.p : P . P ' F f p1: P . P ' F f p2: P PO-IMPE F f p1 p2: P' F, x : K f p : P' F \nf.x : K.P' : Prop PO-FORALLI F f .x : K.p : .x : K.P ' F f p : .x : K.P ' F f d : K PO-FORALLE F f p \nd : P'[d/x] F f p : PP =\u00df. P' PO-CONVERT F f p : P' Figure 3. Main typing judgements of .HOLind (selected \nrules) can view Prop as a distinguished datatype, whose terms can get extended through inductive de.nitions \nof predicates and logical connectives. Kinds get assigned the sort Type, which in turn gets assigned \nthe (external) sort Type ', so that contexts can include variables over Type. The last syntactic category \nof our framework is p, representing proof objects, which get assigned a proposition as a type. We can \nthink of terms at this level as corresponding to different axioms of our logic, e.g. function application \nwill witness the implication elimination rule (modus-ponens). We include terms for performing proof by \ninduction on inductive datatypes and inductive predicates. Using the syntactic category t we represent \nterms at any level out of the ones we described; at the level of variables we do not distinguish between \nthese different levels. To see how inductive de.nitions are used, let us consider the case of natural \nnumbers. Their de.nition would be as follows: Inductive Nat : Type := zero : Nat | succ : Nat . Nat. \nThis gives rise to the Nat kind, the zero and succ constructors at the domain objects level, and the \nelimination axiom elim Nat at the proof object level, that witnesses induction over natural numbers, \nhaving the following type: .P : Nat . Prop.P zero . (.x : Nat.Px . P (succ x)) ..x : Nat.Px Similarly \nwe can de.ne predicates, like equality of natural num\u00adbers, or logical connectives, through inductive \nde.nitions at the level of propositions: Inductive (=Nat)(x : Nat) : Nat . Prop := re. : x = Nat x. Inductive \n(.)(AB : Prop) : Prop := conj : A . B . A . B. From the de.nition of =Nat we get Leibniz equality as \nthe elimi\u00adnation principle, from which the axioms mentioned in the previous section are easy to prove. \nelim (=Nat) : .x : Nat..P : Nat . Prop.Px ..y : Nat.x = Nat y . Py Last, recursive functions over natural \nnumbers can also be de.ned through the Elim(Nat,K) construct: Elim(Nat,K) n fz fs proceeds by performing \nprimitive recursion on the natural number n given fz : K and fs : Nat . K . K, returning a term of kind \nK. For example, we can de.ne the addition function for Nat as: ' plus = .x,y : Nat.Elim(Nat,Nat) xy (.x \n, rx ' : Nat.succ rx ' ) Functions de.ned through primitive recursion are permitted to return propositions \n(where K = Prop), something that is crucial in order to prove theorems like .x : Nat.zero = succ x. We \npresent the main typing judgements of .HOLind in Fig 3. These judgements use the logic variables environment \nF. To sim\u00adplify the presentation, we assume that the de.nitions environment . is .xed and we therefore \ndo not explicitly include it in our judgements. We have not included its well-formedness rules; these \nshould include the standard checks for positivity of inductive def\u00adinitions and are de.ned following \nCIC (see for example [Paulin-Mohring 1993]). Similarly we have omitted the typing rules for the elim \nconstructs. We can view this as a standard PTS with sorts S = {Prop,Type, Type '}, axioms A = {(Prop, \nType), (Type,Type ')} and rules R = {(Prop,Prop,Prop), (Type,Prop,Prop), (Type,Type,Type)}, ex\u00adtended \nwith inductive de.nitions and elimination at the levels we described earlier. In later sections we follow \nthis collapsed view, using the single typing judgement F f t : t ' for terms of all levels. Of interest \nis the PO-CONVERT typing rule for proof objects. We de.ne a limited notion of computation within the \nlogic lan\u00adguage, composed by the standard \u00df-reduction for normal \u00df-redeces and by an additional .-reduction \n(de.ned as in CIC), which per\u00adforms case reduction and evaluation of recursive function applica\u00adtions. \nWith this rule, logical terms (propositions, terms of induc\u00adtive datatypes, etc.) that are \u00df.-equivalent \nare effectively identi.ed for type checking purposes. Thus a proof object for the proposi\u00adtion 2 =Nat \n2 can also be seen as a proof object for the proposition 1 +1 =Nat 2, since both propositions are equivalent \nif they are eval\u00aduated to normal forms. Because of this particular feature of having a notion of computation \nwithin the logic language, .HOLind fol\u00adlows the Poincar\u00b4 e principle, which we view as one of the points \nof departure of CIC compared to HOL. We have included this in our logic to show that a computational \nlanguage as the one we propose in the next section is still possible for such a framework. 4.1 Extension \nwith meta-variables As we have mentioned in the previous section, our computational language will manipulate \nlogical terms living in different contexts.  (contextual terms) T ::=[F]t (meta-variables env.) M ::= \n| M, X : T (substitution) s ::= | s, t K ::= \u00b7\u00b7\u00b7 | X/s d,P ::= \u00b7\u00b7\u00b7 | X/s p ::= \u00b7\u00b7\u00b7 | X/s Figure 4. Syntax \nextension of .HOLind with contextual terms and meta-variables (context env.) W ::= | W, f : ctx F ::= \n\u00b7\u00b7\u00b7 | F, f s ::= \u00b7\u00b7\u00b7 | s, idf Figure 5. Syntax extension of .HOLind with parametric contexts In order \nto be able to type-check these terms properly, we introduce a new class of terms T called contextual \nterms, which package a logical term along with its free variables environment. We write a contextual \nterm as [F]t where t can mention variables out of F. We identify these terms up to alpha equivalence \n(that is, renaming of variables in F and t). Furthermore, we need a notion of contextual variables or \nmeta\u00advariables. Our computational language will provide ways to ab\u00adstract over such variables, which \nstand for contextual terms T . We denote meta-variables as X and use capital letters for them. To use \na meta-variable X inside a logical term t, we need to make sure that when it gets substituted with a \ncontextual term T =[F']t ', the re\u00adsulting term t[T /X] will still be properly typed. Since t ' refers \nto different free variables compared to t, we need a way to map them into terms that only refer to the \nsame variables as t. This mapping is provided by giving an explicit substitution when using the vari\u00adable \nX. The syntax of our logic is extended accordingly, as shown in Figure 4. Since logical terms t now include \nmeta-variables, we need to refer to an additional meta-context M. Thus the main typing judge\u00adment of \nthe base logic, F f t : t ' is extended to include this new en\u00advironment, resulting in a typing judgement \nof the form M; F f t : t ' . Existing typing rules ignore the extra M environment; what is inter\u00adesting \nis the rule for the use of a meta-variable. This is as follows: X : T . M T =[F']t M; F f s : F' M; F \nf X/s : t[s/F'] We use the typing judgement M; F f s : F' to check that the provided explicit substitution \nprovides a term of the appropriate type under the current free variable context, for each one of the \nfree variables in the context associated with the meta-variable X. The judgment M; F f s : F' is de.ned \nbelow. M; F f s : F' M; F f t : t '[s/F'] M; F f : M; F f (s, t) : (F' , x : t ') A little care is needed \nsince there might be dependencies between the types of the elements of the context. Thus when type-checking \na substitution against a context, we might need to apply part of the substitution in order to get the \ntype of another element in the context. This is done by the simultaneous substitution [s/F] of variables \nin F by terms in s. To simplify this procedure, we treat the context F and the substitution s as ordered \nlists that adhere to the same variable order. W; M; F f t : t ' Typing for logical terms W; M f T : \nT ' Typing for contextual terms W; M; F f s : F' Typing for substitutions W; M f F Well-formedness for \nlogical variables contexts Figure 6. Summary of extended .HOLind typing judgements To type a contextual \nterm T =[F]t, we use the normal typing judgement for our logic to type the packaged term t under the \nfree variables context F. The resulting type will be another contextual ' term T associated with the \nsame free variables context. Thus the only information that is needed in order to type-check a contextual \nterm T is the meta-context M. The judgement M f T : T ' for typing contextual terms will therefore look \nas follows: T =[F]t M f F M;F f t : t ' M f T : [F]t ' We use the judgement M f F to make sure that F \nis a well\u00adformed context, i.e. that all the variables de.ned in it have a valid type; dependencies between \nthem are allowed. ' M f F M; F f t : t M f M f F, x : t Last, let us consider how to apply the substitution \n[T /X] (where T =[F]t) on X inside a logical term t '. In most cases the substi\u00adtution is simply recursively \napplied to the subterms of t '. The only special case is when t ' = X/s. In this case, the meta-variable \nX should be substituted by the logical term t; its free variables F are mapped to terms meaningful in \nthe same context as the original term t ' using the substitution s. Thus: (X/s)[T /X]= t[s/F], when T \n=[F]t For example, consider the case where X : [a : Nat,b : Nat]Nat, t ' = plus (X/1,2) 0 and T =[a : \nNat,b : Nat]plus ab. We have: t '[T /X]= plus ((plus ab)[1/a, 2/b]) 0 = plus (plus 12) 0 =\u00df. 3 The above \nrule is not complete: the substitution s is still permitted to use X based on our typing rules, and thus \nwe have to re-apply the substitution of T for X in s. Note that no circularity is involved, since at \nsome point a substitution s associated with X will need to not refer to it the term otherwise would \nhave in.nite depth. Thus the correct rule is: (X/s)[T /X]= t[(s[T /X])/F], when T =[F]t  4.2 Extension \nwith parametric contexts As we saw from the programming examples in the previous section, it is also \nuseful in our computational language to be able to specify that a contextual term depends on a parametric \ncontext. Towards that effect, we extend the syntax of our logic in Figure 5, introduc\u00ading a notion of \ncontext variables, denoted as f, which stand for an arbitrary free variables context F. These context \nvariables are de\u00ad.ned in the environment W. The de.nition of the logical variables context F is extended \nso that context variables can be part of it; thus F contexts become parametric. In essence, a context \nvariable f in\u00adside a context F serves as a placeholder, where more free variables can be substituted; \nthis is permitted because of weakening. We extend the typing judgements we have seen so that the W environment \nis also included; a summary of this .nal form of the judgements is given in Figure 6. The typing judgement \nthat checks well-formedness of a context F is extended so that context variables de.ned in the context \nW are permitted to be part of it:  W; M f Ff : ctx . W W; M f F, f With this change, meta-variables \nX and contextual terms T can refer to a parametric context F, by including a context variable f at some \npoint. Explicit substitutions s associated with use of meta\u00advariables must also be extended so that they \ncan correspond to such parametric contexts; this is done by introducing the identity substi\u00adtution idf \nfor each context variable f. The typing rule for checking a substitution s against a context F is extended \naccordingly: W; M; F f s : F' f . F W; M; F f (s, idf) : (F' , f) When substituting a context F for \na context variable f inside a logical term t, this substitution gets propagated inside the subterms of \nt. Again the interesting case is what happens when t corresponds to a use of a meta-variable (t = X/s). \nIn that case, we need to replace the identity substitution idf in the explicit substitution s by the \nactual identity substitution for the context F. This is done using the idsubst(\u00b7) function: idf[F/f]= \nidsubst(F) idsubst( )= where: idsubst(F, x : t)= idsubst(F), x idsubst(F, f)= idsubst(F), idf With the \nabove in mind, it is easy to see how a proof object for P1 . P2 living in context f can be created, when \nall we have is a proof object X for P2 living in context f, pf1 : P1. [f].y : P1.(X/(idf, y)) This can \nbe used in the associated case in the tautology prover example given earlier, .lling in as the term (\u00b7\u00b7\u00b7 \nproof of P1 . P2 \u00b7\u00b7\u00b7).  4.3 Metatheory We have proved that substitution of a contextual term T for a \nmeta\u00advariable X and the substitution of a context F for a context variable f, preserve the typing of \nlogical terms t. The statements of these substitution lemmas are: Lemma 4.1 If M, X0: T, M ' ; F f t \n: t ' and M f T0: T , then M, M '[T0/X0]; F[T0/X0] f t[T0/X0] : t '[T0/X0]. Lemma 4.2 If M, M ' ; F, \nf0, F'f t : t ' and W;M; F f F0, then M, M '[F0/f0]; F, F0, F'f t[F0/f0] : t '[F0/f0]. These are proved \nby straightforward mutual structural induc\u00adtion, along with similar lemmas for explicit substitutions \ns, con\u00adtextual terms T and contexts F, because of the inter-dependencies between them. The proofs only \ndepend on a few lemmas for the core of the logic that we have described, namely the standard simul\u00adtaneous \nsubstitution lemma, weakening lemma, and preservation of \u00df.-equality under simultaneous substitution. \nDetails are provided in the extended version of this paper [Stampoulis and Shao 2010]. These extensions \nare inspired by contextual modal type theory and the Beluga framework; here we show how they can be adapted \nto a different logical framework, like the one we have described. Compared to Beluga, one of the main \ndifferences is that we do not support .rst-class substitutions, because so far we have not found abstraction \nover substitutions in our computational language to be necessary. Also, context variables are totally \ngeneric, not constrained by a context schema. The reason for this is that we will not use our computational \nlanguage as a proof meta-language, so coverage and totality of de.nitions in it is not needed; thus context \nK ::= *| K1 . K2 t ::= unit | int | bool | t1 . t2 | t1 + t2 | t1 \u00d7 t2 | \u00b5a : K.t |.a : K.t | a | array \nt | .a : K.t | t1 t2 | \u00b7\u00b7\u00b7 e ::= () | n | e1 + e2 | e1 = e2 | true | false | if e then e1 elsee2 | .x \n: t.e | e1 e2 | (e1, e2) | projie | injie | case(e, x1.e1, x2.e2) | fold e | unfold e | .a : K.e | e \nt '' | .x x : t.e | mkarray(e,e ') | e[e '] | e[e '] := e | l | error | \u00b7\u00b7\u00b7 G ::= | G, x : t | G, a : \nK S ::= | S, l : array t Figure 7. Syntax for the computational language (ML fragment) K ::= \u00b7\u00b7\u00b7 | .x \n: T.K | .f : ctx.K t ::= \u00b7\u00b7\u00b7 | .X : T.t | SX : T.t | .f : ctx.t | Sf : ctx.t | .X : T.t | t T | .f : \nctx.t | tF ' e ::= \u00b7\u00b7\u00b7 | .X : T.e | eT |(T, e)| let (X, x) = e in e ' | .f : ctx.e | e F |(F, e)| let \n(f, x) = e in e | holcase T of (p1 . e1)\u00b7\u00b7\u00b7(pn . en) p ::= cd | p1 . p2 |.x : p1.p2 | .x : p1.p2 | p1 \np2 | x | X/s | Elim(cK, K ') | cK | Prop Figure 8. Syntax for the computational language (new constructs) \nschemata are not necessary too. Last, our F contexts are ordered and therefore permit multiple context \nvariables f in them; this is mostly presentational. 5. The computational language Having described the \nlogical framework we are using, we are ready to describe the details of our computational language. The \nML frag\u00adment that we support is shown in Figure 7 and consists of alge\u00adbraic datatypes, higher-order \nfunction types, the native integer and boolean datatypes, mutable arrays, as well as polymorphism over \ntypes. For presentation purposes, we regard mutable references as one-element arrays. We use bold face \nfor variables x of the compu\u00adtational language in order to differentiate them from logical vari\u00adables. \nIn general we assume that we are given full typing deriva\u00adtions for well-typed terms; issues of type \nreconstruction are left as future work. The syntax for the new kinds, types and expressions of this language \nis given in Figure 8, while the associated typing rules and small-step operational semantics are given \nin Figures 9 and 10 respectively. Other than the pattern matching construct, typing and operational semantics \nfor the other constructs are entirely standard. We will describe them brie.y, along with examples of \ntheir use. Functions and existentials over contextual terms Abstraction over a contextual logical term \n(.X : T.e) results in a dependent function type (.X : T.t), assigning a variable name to this term so \nthat additional arguments or results of the function can be related to it. Still, because of the existence \nof the pattern matching construct, such logical terms are runtime entities. We should therefore not view \nthis abstraction as similar to abstraction over types or type indexes in other dependently typed programming \nlanguages; rather, it is a construct that gets preserved at runtime. Similarly, existential packages \nover contextual terms are also more akin to normal tuples. To lift an arbitrary HOL term to the computational \nlanguage, we can use an existential package where the second member is of unit type. This operation is \nvery common, so we introduce the following syntactic sugar at the type level, and for the introduction \n ' W;M f T : T W;M, X : T ;S; G f e : t W;M; S;G f e : .X : T.t W; M f T ' : T W;M;S;G f .X : T.e : \n.X : T.t W; M;S;G f eT ' : t[T ' /X] W;M f T ' : T W;M; S;G f e : t[T ' /X] W;M;S;G f e : SX : T.t W; \nM, X ' : T ; S;G, x : t[X ' /X] f e ' : t' X ' . fv(t') '' W; M;S;G f T , e : SX : T.t W; M;S; G f let \nX , x = e in e ' : t' W, f : ctx;M;S; G f e : t W;M; S;G f e : .f : ctx.t W;M f F wf W;M f F wf W;M; \nS;G f e : t[F/f] W;M;S; G f .f : ctx.e : .f : ctx.t W;M; S;G f e F : t[F/f] W;M;S;G f(F, e) : Sf : ctx.t \nW;M; S;G f e : Sf : ctx.t W,f' : ctx;M; S;G, x : t[f' /f] f e ' : t' f' . fv(t') W;M;S;G f let f' , x \n= e in e ' : t' ' W;M f T : TT ' =[F]t ' W; M; F f t ' : Type .i,M; F f (pi . t ') . Mi W; M, Mi;S; \nG f ei : t[[F] pi/X] W; M;S; G f holcase T of (p1 . e1)\u00b7\u00b7\u00b7(pn . en) : t[T /X] Figure 9. Typing judgement \nof the computational language (selected rules) v ::= .X : T.e |(T, v)| .f : ctx.e |(F, v) | \u00b7\u00b7\u00b7 E ::= \n| E T |(T, E)| let (X, x) = E in e '| E F |(F, E)| let (f, x) = E in e ' | \u00b7\u00b7\u00b7 sM ::= | sM, X . T \u00b5 ::= \n| \u00b5,l . [v1, \u00b7\u00b7\u00b7 ,vn] '' \u00b5,e -. \u00b5 ,e \u00b5,E[error] -. \u00b5,error \u00b5,(.X : T.e) T ' -. \u00b5,e[T ' /X] \u00b5, let (X, \nx) = (T, v) in e ' -. \u00b5,e '[T /X][v/x] ' \u00b5, E[e] -. \u00b5 ,E[e '] T =[F]t F f unify(p1,t)= sM \u00b5,(.f : ctx.e) \nF -. \u00b5,e[F/f] \u00b5,let (f, x) = (F, v) in e ' -. \u00b5,e '[F/f][v/x] \u00b5,holcase T of (p1 . e1)\u00b7\u00b7\u00b7(pn . en) -. \n\u00b5,e1[sM] T =[F]t F f unify(p1,t)= . \u00b5, holcase T of -. \u00b5, error \u00b5,holcase T of (p1 . e1) \u00b7\u00b7\u00b7(pn . en) \n-. \u00b5, holcase T of (p2 . e2) \u00b7\u00b7\u00b7(pn . en) Figure 10. Operational semantics for the computational language \n(selected rules) and elimination operations: LT(T )= SX : T.unit (T ) = (T, ())'' let (X) = e in e = \nlet (X, ) = e in e An example of the use of existential packages is the hyplistWeaken function of Section \n3.2, which lifts a list of hypotheses from one context to an extended one. This works by lifting each \npackage of a hypothesis and its associated proof object in turn to the extended context. We open up each \npackage, getting two contextual terms referring to context f; we then repackage them, having them refer \nto the extended context f, x : A: hypWeaken : .f : ctx..A : [f]Prop.(SH : [f]Prop.LT([f]H)) . (SH : [f, \nx : A] Prop.LT([f, x : A]H)) hypWeaken f A hyp = let (X, x1) = hyp in ' let (X, ) = x1 in ([f, x : A](X/idf)), \n[f, x : A](X' /idf) hyplistWeaken : .f : ctx..A : [f]Prop.hyplist f . hyplist (f, x : A) hyplistWeaken \nf A hl = map hypWeaken hl Functions and existentials over contexts Abstraction over con\u00adtexts works as \nseen previously: we use it in order to receive the free variables context that further logical terms \nrefer to. Existential packages containing contexts can be used in cases where we can\u00adnot statically determine \nthe resulting context of a term. An exam\u00adple would be a procedure for conversion of a propositional formula \nto CNF, based on Tseitin s encoding [Tseitin 1968]. In this case, a number of new propositional variables \nmight need to be introduced. We could therefore give the following type to such a function: cnf :: .f \n: ctx..P : [f]Prop.Sf' : ctx.LT([f,f'] Prop) Erasure semantics are possible for these constructs, since \nthere is no construct that inspects the structure of a context. Type constructors At the type level we \nallow type constructors abstracting both over contexts and over contextual terms. This is what enables \nthe de.nition of the hyplist type constructor in Section 3.2. Similarly we could take advantage of type \nconstructors to de.ne a generic type for hash tables where keys are logical terms of type [f]A and where \nthe type of values is dependent on the key. The type of values is thus given as another type constructor, \nand the overall type constructor for the hashtable should be: hash : (.X : [f]A.*) .* = .res : (.X : \n[f] A.*).array (SX : [f]A.res X) Implementation of such a data structure is possible, because of a built-in \nhashing function in our computational language, that maps any logical term to an integer. Some care is \nneeded with this construct, since we want terms that are \u00df.-equivalent to generate the same hashing value \n(a-equivalence is handled implicitly by using deBruijn indices). To that effect we need to reduce such \nterms to full \u00df.-normal forms before computing the hash value. Static and dynamic semantics of pattern \nmatching The last new construct in our computational language is the pattern matching construct for logical \nterms. Let us .rst describe its typing rule, as seen in Figure 9. First we make sure that the logical \nterm to be pattern matched upon (the scrutinee) is well-typed. Furthermore, only logical terms that correspond \nto propositions, predicates or ob\u00adjects of the domain of discourse are allowed to be pattern matched \nupon; we thus require that the scrutinee s kind is Type. Patterns can be viewed as normal logical terms \n(of syntactic levels K and d of the logic) that contain certain uni.cation variables; we will discuss \ntheir typing shortly. Uni.cation variables are normal meta\u00advariables that are newly introduced. The result \ntype of the pattern matching construct is dependent on the scrutinee, enabling each branch to have a \ndifferent type depending on its associated pattern.  At runtime, patterns are attempted to be uni.ed \nagainst the scru\u00adtinee in sequence, and only the .rst succeeding branch is evalu\u00adated; an error occurs \nwhen no pattern can be matched. Uni.cation merely checks whether the pattern and the scrutinee match \nup to \u00df.\u00adequivalence; if they do, it returns a substitution for the uni.cation variables, which gets \napplied to the body of the branch. Higher-order uni.cation in a setting like the logic we are de\u00adscribing \nis undecidable. We therefore restrict the patterns allowed to linear patterns where uni.cation variables \nare used at most once. For ef.ciency reasons, we also impose the restriction that when we use a uni.cation \nvariable in a certain context, it must be applied to the identity substitution of that context. These \nrestrictions are im\u00adposed using the pattern typing judgement M; F f (p . t) . M ' . This judgement checks \nthat p is a valid pattern corresponding to a logical term of type t, and outputs the uni.cation variables \nenvi\u00adronment M ' that p uses. We further check that patterns are terms in normal form; that is, patterns \nshould only be neutral terms. The details for this judgement, as well as the dynamic semantics of the \nuni.cation procedure, are given in the extended version of this pa\u00adper [Stampoulis and Shao 2010]. Revisiting \nthe example of the tautology prover from Section 3.1, the pattern matching would more accurately be written \nas follows. Note that we also use a return clause in order to specify the result type of the construct. \nThough this is verbose, most of the contexts and substitutions would be easy to infer from the context. \nholcase P return option LT(P) with P1/idf . P2/idf . \u00b7\u00b7\u00b7 |.x : A/idf.P' /(idf, x) . \u00b7\u00b7\u00b7 Metatheory We \nhave studied the metatheory for this language, and have found it relatively straightforward using standard \ntech\u00adniques. Preservation depends primarily on the substitution lemmas: Lemma 5.1 (Substitution of contextual \nterms into expressions) If W; M, X : T ; S; G f e : t and W; M f T ' : T, then W; M; S[T ' /X]; G[T ' \n/X] f e[T ' /X] : t[T ' /X]. Lemma 5.2 (Substitution of contexts into expressions) If W, f : ctx; M, \nX : T ; S; G f e : t and W; M f F wf, then W; M[F/f]; S[F/f]; G[F/f] f e[F/f] : t[F/f]. A lemma detailing \nthe correct behavior of pattern uni.cation is also needed, intuitively saying that by applying the substitution \nyielded by the uni.cation procedure to the pattern, we should get the term we are matching against: Lemma \n5.3 (Soundness of uni.cation) If F f unify(p,t)= sM , W; M f [F]t : T0 and W; M f (p . T0) . M ' then \nW; M f sM : M ' and ([F] p)[sM/M ']=\u00df. T. Theorem 5.4 (Preservation) If ; ; S; f e : t, ; ; S; f \u00b5 and \n'' ' \u00b5,e -. \u00b5 ,e then there exists a S'. S so that ; ; S' ; f e : t ' and ; ; ; S' ; f \u00b5. '' Proof by \nstructural induction on the step relation \u00b5, e -. \u00b5 ,e , made relatively simple for the new constructs \nby use of the above lemmas. The proofs for common constructs do not require special provisions and follow \nstandard practice [Pierce 2002]. Progress depends on the following canonical forms lemma: Lemma 5.5 (Canonical \nforms) If ; ; S; f v : t then: If t = .X : T.t', then v = .X : T.e. ' If t = SX : T.t', then v = (T , \nv ') and ; f T ' : T. If t = .f : ctx.t', then v = .f : ctx.e. If t = Sf : ctx.t', then v = (F' , v ') \nand ; f F' wf. ... Theorem 5.6 (Progress) If ; ; S; f e : t then either e is a value, or, for every \u00b5 \nsuch that ; ; S; f \u00b5, \u00b5, e -. error, or there exists ' '' e and \u00b5' such that \u00b5, e -. \u00b5 , e. The proof \nis a straightforward structural induction on the typing derivation of e. The only cases for the error \nresult come from non-exhaustive pattern matching or out-of-bounds access in arrays. More details about \nthe proofs can be found in the extended version of this paper [Stampoulis and Shao 2010]. We have found \nthese proofs to be relatively orthogonal to proofs about the type safety of the basic constructs of the \ncomputational language. From type safety we immediately get the property that if an expression evaluates \nto an existential package containing a proof object p, then it will be a valid proof of the proposition \nthat its type re.ects. This means that, at least in principle, the type checker of the logic language \ndoes not need to be run again, and decision procedures and tactics written in our language always return \nvalid proof objects. In practice, the compiler for a language like this will be much larger than a type \nchecker for the logic language so we will still prefer to use the second as our trusted base. Furthermore, \nif we are only interested in type checking proof objects yielded from our language, our trusted base \ncan be limited to a type checker for the base logic language, and does not need to include the extensions \nwith meta-variables and parametric contexts. 6. Implementation and examples We have created a prototype \nimplementation of a type-checker and interpreter for VeriML, along with an implementation of the higher\u00adorder \nlogic we use. Readers are encouraged to download it from http://flint.cs.yale.edu/publications/veriml.html. \nThe implementation is about 4.5kLOC of OCaml code, and gives a VeriML toplevel to the user where examples \ncan be tested out. Concrete syntax for VeriML is provided through Camlp4 syntax extensions. The implementation \nof the logic can be used in isolation as a proof checker, and is done using the PTS style; thus generaliza\u00adtion \nto a theory like CIC is relatively straightforward. Binding is represented using the locally nameless \napproach. Both at the level of the logic, as well as at the level of the computational language, we allow \nnamed de.nitions. In the computational language we al\u00adlow some further constructs not shown here, like \nmutually recursive function de.nitions, as well as a printing function for HOL terms, used primarily \nfor debugging purposes. We are currently working on a code generator that translates well-typed programs \nof our computational language into normal OCaml code for ef.ciency. This translation essentially is a \ntype\u00aderasure operation, where the annotations needed to support our dependent type system are removed \nand we are thus left with code similar to what one would write for an LCF-style theorem prover. We are \ninvestigating the possibility of emitting code for existing frameworks, like Coq or HOL.  We have used \nthis implementation to test two larger examples that we have developed in VeriML; these are included \nin the lan\u00adguage distribution. The .rst example is an extension of the deci\u00adsion procedure for the theory \nof equality given in Section 3, so that uninterpreted functions are also handled. Furthermore, we use \nthis decision procedure as part of a version of the tauto tactic that we showed earlier. Equality hypotheses \nare used in order to create the equivalence class hash table; terms contained in goals are then viewed \nup to equivalence based on this hash table, by using the functions provided by the decision procedure. \nThe second example is a function that converts a proposition P into its negation normal form P', returning \na proof object wit\u00adnessing the fact that P' implies P. Such proof objects are not built manually. They \nget produced by a version of the tauto tactic with all the extensions that we described in Section 3, \nalong with han\u00addling of the False proposition. This tactic is enough to prove all the propositional tautologies \nrequired in NNF conversion. 7. Related work There is a large body of existing related work that we should \ncompare the work we described here to. We will try to cover other language and framework designs that \nare similar in spirit or goals to the language design we have described here. The LTac language [Delahaye \n2000, 2002] available in the Coq proof assistant is an obvious point of reference for this work. LTac \nis an untyped domain-speci.c language that can be used to de.ne new tactics by combining existing ones, \nemploying pattern match\u00ading on propositions and proof contexts. Its untyped nature is some\u00adtimes viewed \nas a shortcoming (e.g. in [Nanevski et al. 2010]), and there are problems with handling variables and \nmatching under binders. Our language does not directly support all of the features of LTac. Still, we \nbelieve that our language can serve as a kernel where missing features can be developed in order to recover \nthe practicality that current use of LTac demonstrates. Also, our lan\u00adguage is strongly typed, statically \nguarantees correct behavior with regards to binding, and gives access to a richer set of program\u00adming \nconstructs, including effectful ones; this, we believe, enables the development of more robust and complex \ntactics and decision procedures. Last, our language has formal operational semantics, which LTac lacks \nto the best of our knowledge, so the behaviour of tactics written in it can be better understood. The \ncomparison with the LCF approach [Gordon et al. 1979] to building theorem provers is interesting both \nfrom a technical as well as from a historical standpoint, seeing how ML was origi\u00adnally developed toward \nthe same goals as its extension that we are proposing here. The LCF approach to building a theorem prover \nfor the logic we have presented here would basically amount to build\u00ading a library inside ML that contained \nimplementations for each axiom, yielding a term of the abstract thm datatype. By permitting the user \nto only create terms of this type through these functions, we would ensure that all terms of this datatype \ncorrespond to valid derivations something that depends of course on the type safety of ML. Our approach \nis different in that the equivalent of the thm datatype is dependent on the proposition that the theorem \nshows. Coupled with the rest of the type system, we are able to specify tactics, tacticals, and other \nfunctions that manipulate logical terms and theorems in much more detail, yielding stronger static guaran\u00adtees. \nEssentially, where such manipulation is done in an untyped manner following the usual LCF approach, it \nis done in a strongly typed way using our approach. We believe that this leads to a more principled and \nmodular programming paradigm, a claim that we aim to further substantiate with future work. In recent \nyears many languages with rich dependent type sys\u00adtems have been proposed, which bear similarity to the \nlanguage design we proposed here; unfortunately they are too numerous to cover here but we refer the \nreader to three of the most recent and relevant proposals [Norell 2007, Fogarty et al. 2007, Chen and \nXi 2005] and the references therein. The way that our approach con\u00adtrasts with languages like these is \nthat we are not primarily inter\u00adested in certifying properties of code written in our language. We rather \nview our language as a foundation for an extensible proof assistant, where proofs about code written \nin other (however richly typed or untyped) languages can be developed in a scalable manner. Of the above \nlanguages, we believe Concoqtion [Fogarty et al. 2007] is the one that is closest to our language, as \nit embeds the full CIC universe as index types for use inside a version of the ML language. Our language \ndoes the same thing, even though only a subset of CIC is covered; the point of departure compared to \nConcoqtion is that our language also includes a computational-level pattern matching construct on such \nterms. Thus logical terms are not to be viewed only as index types, and actually have a runtime representation \nthat is essential for the kind of code we want to write. Pattern matching for logical terms would amount \nto a typecase-like construct in languages like Concoqtion, a feature that is generally not available \nin them. It is interesting to contrast our framework with computational languages that deal with terms \nof the LF logical framework [Harper et al. 1993], like Beluga [Pientka and Dun.eld 2008] and Delphin \n[Poswolsky and Sch\u00a8 urmann 2008]. Especially Beluga has been an inspiration for this work, and our use \nof meta-variables and con\u00adtext polymorphism is closely modeled after it. LF provides good support for \nencoding typing judgements like the ones de.ning our logic; in principle our logic could be encoded inside \nLF, and lan\u00adguages such as the above could be used to write programs manip\u00adulating such encodings with \nstatic guarantees similar to the ones provided through our language. In practice, because of the inclu\u00adsion \nof a notion of computation inside our logic, for which LF does not provide good support, this encoding \nwould be a rather intricate exercise. The \u00df.-reduction principles would have to be encoded as relations \ninside LF, and explicit witnesses of \u00df.-equivalence would have to be provided at all places where our \nlogic alludes to it. We are not aware of an existing encoding of a logic similar to the one we describe \ninside LF, and see it as a rather complicated endeavour. This situation could potentially be remedied \nby a framework like Licata et al. [2008] that considers adding computation to a subset of LF; still, \nthis framework currently lacks dependent types, which are essential for encoding the judgements of our \nlogic in LF. Even if this encoding of our logic was done, the aforementioned computational languages \ndo not provide the imperative constructs that we have considered. Last, writing procedures that could \nbe part of a proof assistant, even for simpler logics, has not been demonstrated yet in these languages, \nas we do here. Another framework that is somewhat related is Hoare Type The\u00adory (HTT) and the associated \nYNot project [Chlipala et al. 2009, Nanevski et al. 2010]. This framework attempts to extend the pro\u00adgramming \nmodel available in the Coq proof assistant with sup\u00adport for effectful features like mutable references. \nThis is done by axiomatically extending Coq s logic with support for a state\u00adful monad; essentially, \nimperative computational features are in\u00adtegrated inside the logical framework. Our approach, instead, \nin\u00adtegrates the logical framework inside a computational language, keeping the two as orthogonal as possible. \nThus, it does not require any signi.cant metatheoretic additions to the logic. Additional fea\u00adtures in \nour computational language like concurrency could be eas\u00adily added, as long as they are type safe. In \nprinciple, one could use HTT in conjunction with the standard proof-by-re.ection technique in order to \nprogram decision procedures in an imperative style in\u00adside the proof assistant. We are not aware of a \ndevelopment based on this idea; for example, even though a decision procedure for the EUF theory is proved \ncorrect in Nanevski et al. [2010], it is not evi\u00addent whether this procedure can be used as a tactic \nin order to prove further goals. We would be interested in attempting this approach in future work. \n 8. Future work and conclusion There are many directions for extending the work we have pre\u00adsented here. \nOne is to investigate how to replicate most of the func\u00adtionality of a language like LTac inside our \nlanguage, by providing ways to perform pattern matching with back-tracking, and pattern matching on contexts. \nFurthermore, we want to explore issues of type and term inference in our language in order to limit its \ncur\u00adrent verbosity, especially with respect to context quanti.cation and instantiation. Last, the context \nmanipulation currently allowed by our language is relatively limited, e.g. with respect to contraction \nof variables out of contexts. We are investigating how such limita\u00adtions can be lifted, without complicating \nthe language design or its metatheory. We have described VeriML, a new language design that intro\u00adduces \n.rst-class support for a logical framework modeled after HOL and CIC inside a computational language \nwith effects. The language allows pattern matching on arbitrary logical terms. A de\u00adpendent type system \nis presented, which allows for strong speci\u00ad.cations of effectful computation involving such terms. We \nhave shown how tactics and decision procedures can be implemented in our language, providing strong static \nguarantees while at the same time allowing a rich programming model with non-termination and mutable \nreferences. Acknowledgments We thank anonymous referees for their comments on this paper. This work is \nsupported in part by NSF grants CCF-0811665, CNS\u00ad0915888, and CNS-0910670. References Henk P. Barendregt \nand Herman Geuvers. Proof-assistants using dependent type systems. In A. Robinson and A. Voronkov, editors, \nHandbook of Automated Reasoning. Elsevier Sci. Pub. B.V., 1999. B. Barras, S. Boutin, C. Cornes, J. Courant, \nY. Coscoy, D. Delahaye, D. de Rauglaudre, J.C. Filli enez, H. Herbelin, et al. atre, E. Gim\u00b4The Coq proof \nassistant reference manual (version 8.3), 2010. S. Boutin. Using re.ection to build ef.cient and certi.ed \ndecision proce\u00addures. Lecture Notes in Computer Science, 1281:515 529, 1997. A.R. Bradley and Z. Manna. \nThe calculus of computation: decision proce\u00addures with applications to veri.cation. Springer-Verlag New \nYork Inc, 2007. C. Chen and H. Xi. Combining programming with theorem proving. In  Proceedings of the \ntenth ACM SIGPLAN international conference on Functional programming, page 77. ACM, 2005. A. Chlipala. \nCerti.ed Programming with Dependent Types, 2008. URL http://adam.chlipala.net/cpdt/. Adam J. Chlipala, \nJ. Gregory Malecha, Greg Morrisett, Avraham Shinnar, and Ryan Wisnesky. Effective interactive proofs \nfor higher-order imper\u00adative programs. In Proceeding of the 14th ACM SIGPLAN international conference \non Functional programming, pages 79 90. ACM, 2009. D. Delahaye. A tactic language for the system Coq. \nLecture notes in computer science, pages 85 95, 2000. D. Delahaye. A proof dedicated meta-language. Electronic \nNotes in Theoretical Computer Science, 70(2):96 109, 2002. X. Feng, Z. Shao, Y. Guo, and Y. Dong. Combining \ndomain-speci.c and foundational logics to verify complete software systems. In Proc. 2nd IFIP Working \nConference on Veri.ed Software: Theories, Tools, and Experiments (VSTTE 08), volume 5295 of LNCS, pages \n54 69. Springer, October 2008. S. Fogarty, E. Pasalic, J. Siek, and W. Taha. Concoqtion: indexed types \nnow! In Proceedings of the 2007 ACM SIGPLAN symposium on Partial evaluation and semantics-based program \nmanipulation, pages 112 121. ACM New York, NY, USA, 2007. M.J. Gordon, R. Milner, and C.P. Wadsworth. \nEdinburgh LCF: a mecha\u00adnized logic of computation. Springer-Verlag Berlin, 10:11 25, 1979. R. Harper, \nF. Honsell, and G. Plotkin. A framework for de.ning logics. Journal of the ACM, 40(1):143 184, 1993. \n J. Harrison. HOL Light: A tutorial introduction. Lecture Notes in Computer Science, pages 265 269, 1996. \n C. Hawblitzel and E. Petrank. Automated veri.cation of practical garbage collectors. ACM SIGPLAN Notices, \n44(1):441 453, 2009. G. Klein, K. Elphinstone, G. Heiser, J. Andronick, D. Cock, P. Derrin, D. Elkaduwe, \nK. Engelhardt, R. Kolanski, M. Norrish, et al. seL4: Formal veri.cation of an OS kernel. In Proceedings \nof the ACM SIGOPS 22nd symposium on Operating systems principles, pages 207  220. ACM, 2009. X. Leroy. \nFormal veri.cation of a realistic compiler. Communications of the ACM, 52(7):107 115, 2009. D.R. Licata, \nN. Zeilberger, and R. Harper. Focusing on binding and com\u00adputation. In Logic in Computer Science, 2008. \nLICS 08, pages 241 252, 2008. A. Nanevski, G. Morrisett, and L. Birkedal. Polymorphism and separation \nin hoare type theory. In Proceedings of the eleventh ACM SIGPLAN in\u00adternational conference on Functional \nprogramming, pages 62 73. ACM New York, NY, USA, 2006.  Aleksandar Nanevski, Frank Pfenning, and Brigitte \nPientka. Contextual modal type theory. ACM Trans. Comput. Log., 9(3), 2008. Aleksandar Nanevski, Viktor \nVafeiadis, and Josh Berdine. Structuring the veri.cation of heap-manipulating programs. In Proceedings \nof the 37th ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages, pages 261 274. ACM, \n2010. T. Nipkow, L.C. Paulson, and M. Wenzel. Isabelle/HOL : A Proof Assistant for Higher-Order Logic, \nvolume 2283 of LNCS, 2002. Ulf Norell. Towards a practical programming language based on dependent type \ntheory. Technical report, Goteborg University, 2007. C. Paulin-Mohring. Inductive de.nitions in the system \nCoq; rules and properties. Lecture Notes in Computer Science, pages 328 328, 1993. Brigitte Pientka. \nA type-theoretic foundation for programming with higher\u00adorder abstract syntax and .rst-class substitutions. \nIn Proceedings of the 35th ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages, pages \n371 382. ACM, 2008. Brigitte Pientka and Joshua Dun.eld. Programming with proofs and explicit contexts. \nIn Proceedings of the 10th international ACM SIGPLAN con\u00adference on Principles and practice of declarative \nprogramming, pages 163 173. ACM New York, NY, USA, 2008. B.C. Pierce. Types and programming languages. \nThe MIT Press, 2002. A. Poswolsky and C. Sch\u00a8urmann. Practical programming with higher-order encodings \nand dependent types. Lecture Notes in Computer Science, 4960:93, 2008. K. Slind and M. Norrish. A brief \noverview of HOL4. In TPHOLs, pages 28 32. Springer, 2008. M. Sozeau. Subset coercions in Coq. Types for \nProofs and Programs, pages 237 252, 2007. A. Stampoulis and Z. Shao. VeriML: Typed computation of logical \nterms inside a language with effects (extended version). Technical Report YALEU/DCS/TR-1430, Dept. of \nComputer Science, Yale Uni\u00adversity, New Haven, CT, 2010. URL http://flint.cs.yale.edu/ publications/veriml.html. \nG.S. Tseitin. On the complexity of derivation in propositional calculus. Studies in constructive mathematics \nand mathematical logic, 2(115\u00ad125):10 13, 1968. Benjamin Werner. Une Th\u00b4eorie des Constructions Inductives. \nPhD thesis, A L Universit\u00b4e Paris 7, Paris, France, 1994.   \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Modern proof assistants such as Coq and Isabelle provide high degrees of expressiveness and assurance because they support formal reasoning in higher-order logic and supply explicit machine-checkable proof objects. Unfortunately, large scale proof development in these proof assistants is still an extremely difficult and time-consuming task. One major weakness of these proof assistants is the lack of a single language where users can develop complex tactics and decision procedures using a rich programming model and in a typeful manner. This limits the scalability of the proof development process, as users avoid developing domain-specific tactics and decision procedures.</p> <p>In this paper, we present VeriML - a novel language design that couples a type-safe effectful computational language with first-class support for manipulating logical terms such as propositions and proofs. The main idea behind our design is to integrate a rich logical framework - similar to the one supported by Coq - inside a computational language inspired by ML. The language design is such that the added features are orthogonal to the rest of the computational language, and also do not require significant additions to the logic language, so soundness is guaranteed. We have built a prototype implementation of VeriML including both its type-checker and an interpreter. We demonstrate the effectiveness of our design by showing a number of type-safe tactics and decision procedures written in VeriML.</p>", "authors": [{"name": "Antonis Stampoulis", "author_profile_id": "81470653770", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P2338228", "email_address": "", "orcid_id": ""}, {"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P2338229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863591", "year": "2010", "article_id": "1863591", "conference": "ICFP", "title": "VeriML: typed computation of logical terms inside a language with effects", "url": "http://dl.acm.org/citation.cfm?id=1863591"}