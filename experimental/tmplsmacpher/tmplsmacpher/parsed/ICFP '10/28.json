{"article_publication_date": "09-27-2010", "fulltext": "\n Scrappingyour Inef.cient Engine:UsingPartialEvaluationto Improve Domain-Speci.c Language Implementation \n Edwin C. Brady Kevin Hammond School of Computer Science, University of St Andrews, St Andrews, Scotland. \n Email: eb,kh@cs.st-andrews.ac.uk Abstract Partial evaluation aims to improve the ef.ciency of a program \nby specialising it with respect to some known inputs. In this pa\u00adper, we show that partial evaluation \ncan be an effective and, un\u00adusually, easy to use technique for the ef.cient implementation of embedded \ndomain-speci.c languages.We achieve this by exploit\u00ading dependent types and by following some simple \nrules in the de.nition of the interpreter for the domain-speci.c language.We present experimental evidence \nthat partial evaluation of programs in domain-speci.c languages can yield ef.cient residual programs \nwhose performanceis competitivewiththeirJavaandCequivalents and which are also, through the use of dependent \ntypes, veri.ably resource-safe. Using our technique,it followsthataveri.ably cor\u00adrect and resource-safe \nprogram can also be an ef.cient program. Categories and Subject Descriptors D.3.2[Programming Lan\u00adguages]: \nLanguage Classi.cations Applicative (functional) Lan\u00adguages; D.3.4[Programming Languages]:Processors \nCompilers General Terms Languages,Veri.cation, Performance Keywords DependentTypes,Partial Evaluation \n1. Introduction This paper reconsiders the use of partial evaluation [20] for imple\u00admenting embedded \ndomain-speci.c languages (EDSLs) [19].Par\u00adtial evaluation is a well-known technique that aims to improve \nthe ef.ciency of a program by automatically specialising it with re\u00adspect to some known inputs. Embedded \ndomain-speci.c languages embed specialist languages for some problem domain in a general\u00adpurpose host \nlanguage.Byreusing features from the host language, EDSLs can be implemented much more rapidly than their \nstan\u00addalone equivalents, and can take advantage of compiler optimisa\u00adtions and other implementation effort \nin the host language. A common approach to EDSL implementation in functional languages such as Haskell \nis to design an abstract syntax tree (AST) which captures the required operations and properties of the \ndo\u00admain [14, 24], and then either to implement an interpreter on this ASTortouseacode generatortargetinge.g.C \norLLVM[23].Di\u00adrectly interpreting a syntax tree is a simple, lightweight approach, and it is relatively \nstraightforward to verify the functional correct- Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright \nc &#38;#169; 2010ACM978-1-60558-794-3/10/09... $10.00. ness of the interpreter. However,the resulting \nimplementation is un\u00adlikely to be ef.cient. In contrast, code generation gives an ef.cient implementation,butitis \nhardertoverifyits correctness, harderto addnewfeaturestotheEDSL,andcanbe hardertoexploit features of \nthe host language. In this paper, we consider how partial evaluation can be used to achieve an EDSL implementation \nthat is simple to write, straight\u00adforward to verify, and ef.cient. While the theoretical bene.ts of partial \nevaluation have been extensively covered in the literature e.g.[8,16,20,37], there areveryfew practical \nexamples (thework of Seefriedetel.onPantheon[34]isa notableexception). Thisis because it can be dif.cult \nto use partial evaluation effectively several issues must be dealt with, including binding-time improve\u00adments, \nfunction calls, recursion, code duplication, and management of side-effects. Since these issues must \nusuallybe dealt withby the applications programmer, they limit the practical bene.ts of par\u00adtialevaluation \nand so limit its widespread adoption.We argue here that partial evaluation can be a highly effective \ntechnique for im\u00adplementing ef.cient EDSLs, allowing us to specialise the EDSL interpreterwith respecttotheEDSL \nsource program.Wealsoargue that this approach allows us to reason easily about the correctness of our \nimplementation. 1.1 Contributions Our main contributionisa new studyofthe practical limitsof par\u00adtialevaluationasa \ntechniquefor realistic EDSL implementation.It is folklorethatwehaveachoice betweenwritingveri.ably correct, \nbut inef.cient, code and more ef.cient, but potentially incorrect, code. Sadly, at this point in time, \npragmatic software developers will generally make the latter choice. In this paper, we make the followingkeyclaim \nand supportit withexperimentalevidence: There is no need for correctness to be at the expense of ef.ciency. \nWe make the following speci.c contributions: we give experimental evidence that by partially evaluating \nan interpreter for the EDSL, the EDSL implementation compares favourably with Java, and is not signi.cantly \nworse than C;  we give concrete rules for de.ning an interpreter for an EDSL soastogainthe maximum bene.tfrompartialevaluation; \n we describe the implementation of realistic, state-aware EDSLs using dependent types.  Although the \ntechniques we describe apply equally to other de\u00adpendently typed languages, we will use IDRIS1 as a host \nlan-guage.IDRIS isa pure functional programming language with full\u00adspectrum dependent types. Throughout \nthis paper, we will identify 1http://www.idris-lang.org/  the features of IDRIS that assist both with \nEDSL implementation and with partial evaluation, and will also identify how the presence of dependent \ntypes affects standard partial evaluation techniques. In particular, in a language with full dependent \ntypes, there is a different view of the phase distinction between compile-time and run-time. Traditionally, \nthis distinction is observed syntactically between types (which are easily erasable) and values. With \nde\u00adpendent types, the distinction is semantic, between compile-time values (which are erasable by analysing \ndata types [6]) and run\u00adtime values. This affects several aspects of the implementation, which we will \nidentify here. Partial evaluation has been known for many years, and Futa\u00admura s paper [16] on interpreter \nspecialisation is now 39 years old. However, the technique is still not widely applied, because sev\u00aderal \nproblems arise when putting it into practice.We have found that these problems are either easily handled \nor simply do not arise when embeddingDSLsinIDRIS,largelyduetoitstype system. 1.2 Research Motivation: \nResource-Aware Programming The underlying motivation for our research is a desire to reason about extra-functionalproperties \nof programs, thatis howprograms behave in terms of their usage of .nite resources such as memory and \nexecution time, and how that behaviour affects the end-user, e.g. through proper management of .les and \nexceptions. Ensuring that essential extra-functional properties are met is vitally impor\u00adtantto writing \nsoftware thatis usefulin practice.Infact,arespected industrial language designer once con.ded to us that \nnobody in in\u00addustry really cares whether a program does what it s supposed to what they are really concerned \nwith is how the program behaves when it is run 2. This concern is re.ected in the major software failures \nthat we see reported in the national and international press. Many of the most signi.cant problems with \nsoftware behaviour boildownto poor usageof resources: software canfail becauseof buffer over.ows, because \nof deadlocks, because of memory leaks, because of inadequate checking (e.g. that a .le handle has the \ncor\u00adrectmode), becauseitfailstomeethard real-time deadlines,andin manyother ways that are not direct \nconsequences of the functional properties of the software. The EDSL approach, using a dependently-typed \nhost language, allows us to de.ne notations in which extra-functional resource usage properties are stated \nexplicitly in a program s type. Since we are primarily concerned with easeof reasoningand veri.cation,we \ntake the simplest possible approach to EDSL implementation, via an interpreter. Our hypothesis is that \n.rst de.ning an interpreter and then specialising it with respect to EDSL programs is ef.cient enough \nfor practical purposes. The de.nition of ef.cient enough is, of course, open to interpretation and it \nmay be hard to evaluate whetherithasbeenmet.Forthe purposesofthispaper,wewillcall a program ef.cient \nenough if it is similar in speed and memory footprint to an equivalent hand-written Java program 2. Idris \nand itsType Theory IDRIS is an experimental functional programming language with dependent types, similar \nto Agda [29] or Epigram [26]. It has a Haskell-like syntax,butisevaluated eagerly.It compilestoCvia a \nsupercombinator compiler.IDRIS has monadicI/Ointhestyleof Hancock and Setzer [18], and a simple foreign \nfunction interface. It is implemented on top of the IVOR theorem proving library [7], giving direct access \nto an interactive tactic-based theorem prover. This section explores the relationship between type checking \nand evaluationin dependently-typed languages, such asIDRIS. 2Joe Armstrong, Erlang designer, personal \ncommunication. 2.1 The CoreType Theory Thetypetheory underlyingIDRISis implementedbytheIVORthe\u00adorem proving \nlibrary.IDRIS providesa front-end (syntactic sugar), a back-end (a compiler), some primitive types and \nan interface to the IVOR theorem proving tools. I DRIS programs consist of functions over inductive data \ntypes, de.ned by dependent pattern\u00admatching.Terms, t, are de.ned as follows: t ::= Set Type of types \n| x Variables |\\x :t => t Abstraction | (x : t) -> t Function space | tt Function application | c Data \nconstructor | D Inductivefamily The type theory possesses full-spectrum dependent types, where the de.nition \nof terms also captures the de.nition of types. As a convention, we use T to stand for a term which is \nto be used as a type.Afunction de.nition consistsofa type declaration, followed bya numberof pattern-matching \nclauses.We use p to stand for a term which is to be used as a pattern: f :(x1 : T1) -> (x2 : T2 x1) -> \n... -> (xn : Tn x1 x2 ... xn-1) -> Tx1 x2 ... xn f p1,1 p2,1 ... pn,1 = t1 f p1,2 p2,2 ... pn,2 = t2 \n... f p1,m p2,m ... pn,m = tm The type of each argument may be predicated on the values of previous \narguments, and the return type may be predicated on the valuesofanyofthearguments.Apattern, pi,j , may \nbe an arbitrary term,but can onlybe matchedifitis eitheravariable orisin head normal form.  2.2 Type \nChecking DependentTypes Since full dependent types may include values, which may need to be reduced to \nnormal form, type checking exploits the following rules3, where Set represents the type of types: G f \nA, B : Set G f A r\u00df A' G f B r\u00df A' G f A . B G f A, B : Set G f x : A G f A . B G f x : B The context \nG records the types and values of all names in scope, as is standard. The .rst rule de.nes the conversion \nrelation. If two terms A and B have a common redex A', then they are \u00df\u00adconvertible. The second rule states \nthat \u00df-convertible terms can be interchanged within types.Twokeyimplicationsof these rules are: 1. Implementing \na type checker for a dependently-typed language requires an evaluator which can compute under binders. \n 2. In order to ensure termination of type checking (and therefore decidability), we must distinguish \nterms for which evaluation de.nitely terminates, and those for which it may not.  Wetakea simplebuteffective \napproachto termination checking: any functions that do not satisfy a simple syntactic constraint on recursive \ncalls will not be reduced by the type checker. The con\u00adstraint we use is that each recursive call must \nhave an argument that is structurally smaller than the input argument in the same po\u00adsition, and that \nthese arguments must belong to a strictly positive data type.We check for totality by additionally ensuring \nthat the patterns cover all possible cases. 3The full type-checking rules for core IDRIS may be found \nelsewhere [8]. The type checker follows standard methods for implementingadependently typed lambda calculus \n[11, 25].  E[Set] = Set = x E[x] E[\\x : T => t] = \\x :E[T ] => E[t]E[(x : T1) -> T2] =(x : E[T1]) -> \nE[T2]E[t1 t2] F[E[t1] t2] = = c E[c] = D E[D] F[(\\x :T => t1) t2] = E[t1[t2/x]]F[f Tt] = E[f(e)]if f \nTp = e de.ned in G Yes f = MAT CH(Tp, E[Tt]) = t F[t] MATCH(c Tp)(c Tt)= MAT CH(Tp,Tt) MATCHxt = Yes \n[t/x] MATCH = No Figure 1. Sketch of the evaluation function E[\u00b7]  2.3 Evaluation The evaluator used \nby the type checker implements \u00df-reduction and pattern matching. A sketch of the evaluation function \nis given in Figure 1. To keep the presentation simple, the sketch E[\u00b7] uses a substitution-based approach. \nIn practice, however, for ef.\u00adciencyreasons we will use an environment and de Bruijn indexed variables. \nIn the evaluator we useTt to denote a telescope of argu\u00adments t1 ... tn.We de.ne a function F[\u00b7], which \nevaluates a func\u00adtion application, using \u00df-reduction and pattern-matching de.ni\u00adtions, where possible, \nand anoverallevaluation function E[\u00b7] which effectively implements structural closure of function application. \nMATCHimplements pattern matching of an argument against a pat\u00adtern, returning a substitution if matching \nsucceeds, with MAT CH being the obvious lifting across a telescope of arguments. 2.4 Implicit Arguments \nIDRIS adds a layer of syntactic sugar to the core type theory, including implicit arguments. Where the \nvalue of an argument can be determined from its type, or from another argument, it can be omitted.Forexample,vectors \n(lists with the lengthexpressedin the type) are de.ned as follows: data Nat = O|SNat; infixr 5 ::; data \nVect : Set -> Nat -> Set where VNil : Vect A O | (::):A->VectAk->Vect A(Sk); The VNil constructor has \nan implicit argument A, and the :: constructor has twoimplicit arguments, A and k. Written out in full, \nthe types are: VNil : (A:Set) -> Vect A O (::) : (A:Set) -> (k:Nat) -> A -> Vect A k -> Vect A (S k); \nSimilarly, we can leave arguments implicit in function de.nitions: vadd : Vect Int n -> Vect Int n -> \nVect Int n; vadd VNil VNil = VNil; vadd (x :: xs) (y :: ys) = (x + y) :: (vadd xs ys); Written in full, \nusing the pre.x form of ::, this would be: vadd : (n:Nat) -> Vect Int n -> Vect Int n -> Vect Int n; \nvadd O (VNil Int) (VNil Int) = VNil Int; vadd (S k) ((::) Int k x xs) ((::) Int k y ys) = (::) Int k \n(x + y) (vadd k xs ys); Thevaluesfor implicit arguments,in types, patterns and function applications, \nare inferred from explicit values by uni.cation. For each implicitargumentIDRIS addsaplace holder()tothetermin \nthe type theory, whichis then .lledinbyIVOR s type checker.Itis important to consider fully explicit \nterms when type checking and reasoning about meta-theory.Aprogrammer,however, need notbe concerned that \nthese additional arguments affect performance: the back end erases computationally irrelevant information \n[6, 9]. In the case of Vect and vadd above, all implicit arguments are erased from the de.nition of Vect, \nand implicit arguments to vadd are marked as unused, so a dummy value is passed instead. This is an instance \nof the phase distinction between compile\u00adtime and run-time affecting the implementation. Implicit argu\u00adments, \nalthoughtheyareavailableat run-time, are normally present primarilyfortype correctnessand unusedat run-time.Ina \nfunction f :(x : T ) -> T ' we consider an argument xi unused if both of the following conditions hold: \n1. It is implicit (i.e. its value can be determined by the value of another argument at compile-time \nby uni.cation). 2. It is not used on the right hand side of the de.nition, except in an unused argument \nposition.  The .rst condition ensures that the argument s value will not affect case distinction another \nargument suf.ces. The second condition ensures that it will not be used for case distinction elsewhere. \n 2.5 Type Checking andPartialEvaluation There is an important implication of the typing rules: If a \nlanguage has full dependent types, it requires an evaluator. This evaluator could take several forms, \nbut it must provide compile-time \u00df-reduction and pattern matching to implement con\u00adversion, from which \nit is simple to extend to full normalisation. So, if we have a function f with statically-known arguments \ns and dynamic arguments d, we can create a specialised version (the residual program)by normalising anexpressionof \nthe form: \\d=>f sd Astandard example (e.g. [38]) is thepower function: power : Int -> Nat -> Int; power \nx O=1; power x (S k)=x *power xk; We can specialise the power function for a particular exponent. For \nexample, if the .rst argument x is dynamic, and the second argument has the statically-knownvalue S (S \n(S (S O))) i.e. 4 the IDRIS built-in evaluator, using \u00df-reduction and pattern matching alone, gives \nus the residual program: \\ x => power x (S (S (S (S O)))) ==> \\ x : Int => x*(x*(x*(x*1))) : Int -> Int \n In the restof this paper, we will makeextensive useof thisbuilt-in evaluator, and introduce some simple \nmethods that can be used to control its behaviour in order to make partial evaluation effective. 3. Embedding \nLanguages in Idris In this section, we demonstrate how we can implement EDSLs ef.ciently using partial \nevaluation by showing the implementation ofa simpleexpression language embeddedinIDRIS.  data Ty = TyInt \n| TyFun Ty Ty; interpTy : Ty -> Set; interpTy TyInt = Int; interpTy (TyFun A T) = interpTy A -> interpTy \nT; data Fin : Nat -> Set where fO : Fin (S k) |fS:Fink -> Fin (Sk); using (G:Vect Ty n) { data Expr \n: (Vect Ty n) -> Ty -> Set where Var : (i:Fin n) -> Expr G (vlookup i G) | Val : (x:Int) -> Expr G TyInt \n| Lam : Expr (A::G) T -> Expr G (TyFun A T) |App :Expr G(TyFun A T) ->Expr GA-> Expr G T | Op : (interpTy \nA -> interpTy B -> interpTy C) -> Expr GA -> Expr G B-> Expr GC; } Figure 2. The Simple Functional Expression \nLanguage, Expr. 3.1 ASimple Expression EDSL, Expr Acommon introductory example for dependently-typed \nlanguages isa well-typed interpreter[2,8,31], wherethetype system ensures that only well-typed source \nprograms can be represented and inter\u00adpreted. Figure2 de.nesa simple functional expression language, \nExpr, with integer values and operators. The using notation indi\u00adcates that G is an implicit argument \nto each constructor, with type Vect Ty n. Terms of type Expr are indexed by i) a context (of type Vect \nTy n), which records types for the variables that are in scope; and ii) the type of the term (of type \nTy). Thevalid types(Ty) are integers(TyInt)or functions(TyFun).We de.ne terms to rep\u00adresent variables(Var), \ninteger values(Val), lambda-abstractions (Lam), function calls(App), and binary operators(Op).Types may \neitherbe integers(TyInt)or functions(TyFun), and are translated toIDRIS types using interpTy. Our de.nition \nof Expr also states its typing rules, in some con\u00adtext, by showing how the type of each term is constructed. \nFor example, Val : (x:Int) -> Expr G TyInt indicates that lit\u00aderalvalueshaveinteger types(TyInt),andVar \n: (i:Fin n) -> Expr G (vlookup i G) indicates that the type of a variable is obtainedby lookingup i in \ncontext G.For anyterm, x, we can read x : Expr G T as meaning x has type T in the context G . Expres\u00adsions \nin this representation are well-scoped, as well as well-typed. Variables are represented byde Bruijn \nindices, which are guaran\u00adteedtobe boundedbythesizeofthe context,using i:Fin n in the de.nition of Var.A \nvalue of type Fin n is an element of a .nite set of n elements, whichwe useasareferenceto oneof n variables. \nIn order toevaluate this language, we will need tokeep track of the values of all variables that are \nin scope. Environments, Env, allowustolinkavectoroftypeswith instancesofthosetypes.They are indexed by \na vector of types Vect Ty n. Each element in the environment corresponds to an element in the vector: \ndata Env : Vect Ty n -> Set where Empty : Env VNil | Extend : (res:interpTy T) -> Env G -> Env (T :: \nG); We provide operations to lookup/update an environment, corre\u00adsponding to lookup and update on vectors: \nvlookup : (i:Fin n) -> Vect A n -> A; envLookup : (i:Fin n) -> Env G -> interpTy (vlookup i G); update \n: (i:Fin n) -> A -> Vect A n -> Vect A n; updateEnv : Env G -> (i:Fin n) -> interpTy T -> Env (update \ni T G); interp : Env G -> Expr G T -> interpTy T; interp env (Var i) = envLookup i env; interp env (Val \nx) = x; interp env (Lam sc) = \\x => interp (Extend x env) sc; interp env (App f a) = interp env f (interp \nenv a); interp env (Op op l r) = op (interp env l) (interp env r); Figure 3. Expression Language Interpreter \ndata Ty = TyInt | TyBool | TyFun Ty Ty; interpTy TyBool = Bool; data Expr : (Vect Ty n) -> Ty -> Set \nwhere ... | If : Expr G TyBool -> Expr GA-> Expr G A-> Expr GA; interp env (If v t e) = if (interp env \nv) then (interp env t) else (interp env e); Figure 4. Booleans and If construct The full interpreter \nfor Expr is given in Figure 3. Note that its re\u00adturn type depends on the type of the expression to be \ninterpreted. This is a signi.cant bene.t of dependent types for language im\u00adplementation there is no \nneed to tag the result of the interpreter with a type, because its type is known from the input program. \n 3.2 Example Programs We can now de.ne some simple example functions. We de.ne each function to work \nin an arbitrary context G, which allows it to be applied in anysubexpression in any context. Our .rst \nexample function adds its integer inputs using theI DRIS + primitive. add : Expr G (TyFun TyInt (TyFun \nTyInt TyInt)); add = Lam (Lam (Op (+) (Var (fS fO)) (Var fO))); We can useadd to de.ne the double function: \ndouble : Expr G (TyFun TyInt TyInt); double = Lam (App (App add (Var fO)) (Var fO)); Now, runDouble applies \ndouble to an argument: runDouble : Expr VNil TyInt; runDouble = App double (Val 21); We run this program \nby interpreting it in an empty environment: interp Empty runDouble ==> 42 :Int Running the interpreter \nyields a host language representation of the EDSL program. In the example above, the program had type \nTyInt, so the value returned was of type Int. The value to be returned is computed from the representation \ntype of the expres\u00adsion.So,ifwe weretoevaluate somethingwitha functiontype,we would obtain anIDRIS function.For \nexample, for add: interp Empty add ==> \\ x: Int => \\x0: Int => x+x0 : Int -> Int -> Int  3.3 Control \nstructures andrecursion TomakeExpr more realistic, we will add booleanvalues and an If construct,and \nattemptto writea recursive function.Ourextensions are shownin Figure4.We can now de.neafactorial function: \n fact : Expr G (TyFun TyInt TyInt); fact = Lam (If (Op (==) (Val 0) (Var fO)) (Val 1) (Op (*) (Var fO) \n(App fact (Op (-) (Var fO) (Val 1))))); Unfortunately, we cannot specialise an interpreter with respect \nto this de.nition: it is not structurally recursive, and if we try to eval\u00aduate it, the recursive call \nto fact will unfold forever. Evaluation of the interpreter with this de.nition will only terminate if \nit is given a concrete argument and the recursive call is evaluated lazily. 4. Partial Evaluation In \ngeneral,apartialevaluator produces specialisedversionsof func\u00adtions where some arguments are statically \nknown.Weare interested in the instance where the function to specialise is an EDSL inter\u00adpreter, and \nthe statically known argument is an input program. As we have seen, evaluating the interpreter with speci.c \ninput pro\u00adgrams yields specialised versions of those programs in the host language. This is, of course, \nnot surprising: it is the .rst Futamura projection [16] specialising an interpreter for given source \ncode yieldsanexecutable.However,twoproblems ariseifwe simply use the standard evaluator: 1. Recursive \nprograms with dynamically known inputs cannot be specialised since recursive calls are unfolded arbitrarily \ndeeply; 2. Evaluating completely, unfolding every function de.nition, can lead to loss of sharing. \n The .rst problem is illustrated by fact, in which the number of times to unfold the recursive de.nition \nis dynamically known. The second problem is illustrated by an application of double to a complex expression: \ndoubleBig : Expr VNil TyInt; doubleBig = App double complexFn; Assuming complexFn evaluates to complexExpr, \nevaluating doubleBig gives: complexExpr + complexExpr The large expression complexFn is evaluated twice \nin the residual code, whereitwouldmake more sensetoevaluateit once,andthen pass its result to the evaluated \nversion of double. This is a similar problem to that which arises when inlining functions [32] there \nis needless duplication of work in the evaluated code. 4.1 APartialEvaluatorfor Idris We can signi.cantly \nimprove the results of partial evaluation by taking care with function application.Asketchofthe partialeval\u00aduator, \nP[\u00b7], is given in Figure 5. This mostly follows the standard evaluator rulesgiven previously,but differsin \nthe F ' [\u00b7] function used to evaluate function de.nitions. The partial evaluator P[\u00b7] is implemented \nrelativetoacontext G,but unlikethe regularevaluator E[\u00b7] it updates G during evaluation. When applying \na function f, we separate its arguments Tt into those which are statically known (i.e. known at compile-time), \nTts, and those which are dynamic, Ttd (i.e. which will be known at run-time).InIDRIS, we make this distinction \nthrough programmer annotations. These annotations de.ne which functions should be partially evaluated, \nand which arguments of those functions should be treated as static.For interp,we declare the function \nas follows: interp : Env G -> Expr G T [static] -> interpTy T; The [static] annotation on the expression \nargument indicates thattheexpressionmaybe staticinanyapplicationof interp. Ad\u00additionally, any implicit \nargument which can be uniquely inferred P Set] = Set P = x P\\xx] :T => t] = \\x : P[T ] => P[t] P (x : \nT1) -> T2] =(x : P[T1]) -> P[T2] P t1 t2] = F ' [P[t1] t2] P = c c] = D P[ D] F ' (\\x : T => t1) t2] \n= P[t1[t2/x]]F ' = (f,Tts)Ttd [ f Tt] T' if f : T . T f Tp = e de.ned in G Yes f = MAT CH(Tps, P[Tts]) \n add (f,Tts) : f(TTd) . f(TT' ) (f,Tts) f(Tpd)= P[f(e)] to G F ' = t [t] MATCH(c Tp)(c Tt)= MAT CH(Tp,Tt) \nMATCHxt = Yes [t/x] MATCH = No Figure 5. Partial Evaluator from a static argument may also be static. \nIn this case, T can be uniquely inferred from the expression. G may not be considered static, because \nit can also be inferred from the (dynamic) environ\u00adment.We therefore de.ne the static arguments in an \napplication to be thoseinaposition annotatedas [static] or uniquely inferrable fromanargumentinaposition \nannotatedas [static],and in head normal form, or a global de.nition. The evaluator constructs a new version \nof f, (f,Tts), that is specialised with the static arguments Tts, reusing this de.nition if it already \nexists. The type of (f,Tts) is constructed by specialising the type according to the statically known \nvalues these values may appear in the dynamic portion of the type. This new de.nition is added, permanently, \nto the global context G. Effectively, this caches the intermediate result of a computation with speci.c \nstatic arguments.In thisway,we abstractaway multiple callstoagiven specialised de.nition. In particular, \nthis means that specialising the interpreter with fact will resultina recursiveIDRIS program. The method \nwe use for specialising function applications, namely extending the environment with cached versions \nof par\u00adtially evaluated functions, is a standard technique [13]. The new de.nitions are well-typed and \npreservethe semantics of the original program. However, some care is required in the presence of fully \nexplicit dependently typed programs, because the values of im\u00adplicit arguments may make a de.nition less \ngeneric than required. When constructing a new function (f,Tts) we aim to produce the mostgeneric de.nition \npossible.We achieve thisby replacing any implicit, unused arguments witha place holder before adding \nthe de.nition, as demonstrated by the example below. Example Factorial When the interpreter is partially \nevaluated with fact as a static argument, according to the scheme in Figure 5, we obtain: interp : (n:Nat) \n-> (T:Ty) -> (G:Vect Ty n) -> Env G -> Expr G T -> interpTy T fact : (n:Nat) -> (G:Vect Ty n) -> Expr \nG (TyFun TyInt TyInt) For the remainder of this section, we write applications in fully ex\u00adplicit form. \ninterp has implicit arguments for the expression type and context, and fact has implicit arguments for \nits initial context.  Partiallyevaluating the interpreter happens as follows. The type and expression \narein staticargument positions,sotheevaluator creates a new de.nition interpfact that has been specialised \naccording to these arguments and adds it to the context: interp O (TyFun TyInt TyInt) VNil Empty (fact \nO VNil) ==> interpfact O VNil Empty The new de.nition replaces arguments in implicit positions in the \noriginal application with place holders: interpfact : (n:Nat) -> (G:Vect Ty n) -> Env G -> Int -> Int; \ninterpfact n G e = interp _ _ _ _ (fact _ _); Type checking this leads to the following de.nition with \nthe im\u00adplicit arguments .lled in: interpfact : (n:Nat) -> (G:Vect Ty n) -> Env G -> Int -> Int; interpfact \nn G e = interp n (TyFun TyInt TyInt) G e (fact n G); Thenextstepisto partiallyevaluatethe de.nitionof \ninterpfact. Eventually, this reaches another call to interp: interpfact : (n:Nat) -> (G:Vect Ty n) -> \nEnv G -> Int -> Int; interpfact nGe=\\ x: Int => if (0==x) then 1 else interp (S n) (TyFun TyInt TyInt) \n(TyInt::G) (Extend x e) (fact (S n) G) (x-1); When creating a specialised version of interp there is \nalready a suitable de.nition of interpfact that can be reused: interpfact : (n:Nat) -> (G:Vect Ty n) \n-> Env G -> Int -> Int; interpfact nGe=\\ x: Int => if (0==x) then 1 else interpfact (S n) (TyInt::G) \n(Extend x e) (x-1); This de.nition builds a context and an environment, which are unused (as de.ned in \nSection 2.4). IDRIS notes this and replaces these arguments with dummy place holder values: interpfact \n: (n:Nat) -> (G:Vect Ty n) -> Env G -> Int -> Int; interpfact ___=\\ x: Int => if (0==x) then 1 else interpfact \n_ _ _ (x-1);  4.2 WhyTaglessness Matters As mentioned above, one important feature of an interpreter \nthat has been de.ned in this style is that there is no need to tag the return value with its type, because \nthe type can be computed in advance. This is a common feature of interpreters in dependently\u00adtyped languages \n[2, 8, 31]. Effectively, we use the type checker for the host language to check the terms in the object \nlanguage, so that there is no need to check types dynamically. This leads us to a concrete rule, to be \nfollowed by the EDSL author, for maximising the effect of partial evaluation: Rule 1: Index the EDSL \nrepresentationby its type, toavoid needing to tag the result. The tag elimination problem, in which an \nevaluator aims to move type checking of intermediate results in the interpreter from run\u00adtime to compile-time, \nhas been extensively studied in the partial evaluation literature [10, 21, 39, 40]. The presence of dependent \ntypes simpli.es the implementation of a tagless interpreter greatly, inthatweareabletowriteitina naturalstyle(avoiding,forexam\u00adple, \ncontinuation passing style), with no post-processing required. data Ty = TyUnit | TyBool | TyLift Set; \ninterpTy : Ty -> Set; interpTy TyBool = Bool; interpTy TyUnit = (); interpTy (TyLift A) = A; data Imp \n: Ty -> Set where ACTION : IO a -> Imp (TyLift a) | RETURN : interpTy a -> Imp a | WHILE : Imp TyBool \n-> Imp TyUnit -> Imp TyUnit |IF :Bool ->Impa-> Imp a->Impa | BIND : Imp a -> (interpTy a -> Imp b) -> \nImp b Figure 6. ASimple Imperative EDSL,Imp. interp : Imp a [static] -> IO (interpTy a); interp (ACTION \nio) = io; interp (RETURN val) = return val; interp (WHILE add body) = while (interp add) (interp body); \ninterp (IF v thenp elsep) = if v then (interp thenp) else (interp elsep); interp (BIND code k) = do { \nv <-interp code; interp (k v); }; Figure 7. Interpreter for Imp. 5. EDSLs with State The Expr language \nand its interpreter demonstrate our general approach to EDSL implementation: 1. De.ne the data type for \nthe EDSL in the host language; 2. Write an interpreter to evaluate this type, in the host language; \n 3. Specialise the interpreter with respect to concrete programs.  In practice, however, the languages \nthat interest us will not be as simple as Expr. Real programs have state, theymay communicate acrossanetwork,theymayneedtoreadandwrite.les, \nallocateand free memory, or spawn new threads and processes.Tobe usablein practice we need to ensure \nthat we can deal with such issues. In this section, weimplementa simple EDSL for .le manipulation, which \ndemonstrates how our approach can deal with external state, side effects such as I/O, and imperative \nfeatures in general. 5.1 ASimple Imperative EDSL, Imp Figure6describesasimple imperative EDSL, Imp,which \nincludes a meansof embedding arbitraryI/O operations(ACTION)and pure values(RETURN), WHILE and IF statements, \nand a monadic bind operation(BIND)for sequencing statements.Types inImp are the unit type, TyUnit, for \noperations which do not return any value, such as writing to a .le; a boolean type, TyBool for intermediate \nresults in control structures, and lifted host language types, TyLift, which allow host language functions \nand arbitrary I/O actions to be embeddedin EDSL programs. Figure7gives an interpreter.  5.2 AFile Management \nEDSL, File Sofar, wehave indexed our language representationsby the type of the programs theyrepresent. \nThis allows us to exploit the host language s type checker to ensure that EDSL programs are well-typed.Wecan \ntakethis idea much further withadependently-typed host language such as IDRIS, and index EDSL representations \nnot only by the program s type, but also by other properties such as the states of the resources that \nit uses. To demonstrate this, we implement an EDSL for .le management, designed so that type\u00adcorrect \nprograms have the following informally-stated properties:  data File : Vect FileState n -> Vect FileState \nn -> Ty -> Set where ... --Updated control structures | WHILE : (File ts ts TyBool) -> (File ts ts TyUnit) \n-> (File ts ts TyUnit) | IF : (a:Bool) -> (File ts ts b) -> (File ts ts b) -> (File ts ts b) --File \nmanagement operations | OPEN : (p:Purpose) -> (fd:Filepath) -> (File ts (snoc ts (Open p)) (TyHandle \n(S n))) | CLOSE : (i : Fin n) -> (OpenH i (getPurpose i ts) ts) -> (File ts (update i Closed ts) TyUnit) \n| GETLINE : (i:Fin n) -> (p:OpenH i Reading ts) -> (File ts ts (TyLift String)) | EOF : (i:Fin n) -> \n(p:OpenH i Reading ts) -> (File ts ts TyBool) | PUTLINE : (i:Fin n) -> (str:String) -> (p:OpenH i Writing \nts) -> (File ts ts (TyUnit)); Figure 8. The File-Handling Language, File Files must be open before \nthey are read or written;  Files that are open for reading cannot be written, and .les that are open \nfor writing cannot be read;  Only open .les can be closed;  All .les must be closed on exit.  Our \n.rst attempt extends Imp. The language de.nition is given in Figure 8, and its interpreter in Figure \n9. The interpreter returns a pair of the modi.ed environment, and the value resulting from interpretation, \nwhere (S&#38; T) isIDRIS notation fora pairof types S and T. Each command in the language has a more \nor less direct translation into a host language function. We index programs in File over their input \nand output states, as well as their type. This state is a vector that holds information about whether \n.le handles are open or closed: data Purpose = Reading | Writing; data FileState = Open Purpose | Closed; \ndata File : Vect FileState n -> Vect FileState n -> Ty -> Set where Environments carry concrete .le handles, \nif the .le state indicates that a .le is open: data FileHandle : FileState -> Set where OpenFile : (h:File) \n-> --actual file FileHandle (Open p) | ClosedFile : FileHandle Closed; data Env : Vect FileState n -> \nSet where Empty : Env VNil | Extend : (res:FileHandle T) -> Env G -> Env (T :: G); Since interpreting \nFile threads an environment through the pro\u00adgram, we also modify the interpreter so that imperative constructs \nsuch as while loops manage the environment. Since the type of WHILE indicates that the test and body \nof the loop cannot modify an environment, we call the interpreter recursively and discard the resulting \nenvironment: interp : Env ts -> File ts ts T [static] -> IO (Env ts &#38; interpTy T); ... interp env \n(WHILE test body) = do { while (ioSnd (interp env test)) (ioSnd (interp env body)); return (env, II); \n}; --File operations interp env (OPEN p fpath) = do { fh <-fopen (getPath fpath) (pMode p); return (addEnd \nenv (OpenFile fh), bound); }; interp env (CLOSE i p) = do { fclose (getFile p env); return (updateEnv \nenv i ClosedFile, II); }; interp env (GETLINE i p) = do { str <-fread (getFile p env); return (env, str); \n}; interp env (EOF i p) = do { e <-feof (getFile p env); return (env, e); }; interp env (PUTLINE i str \np) = do { fwrite (getFile p env) str; fwrite (getFile p env) \"\\n\"; return (env, II); }; Figure 9. Interpreter \nfor File. ioSnd : IO (a&#38;b) -> IOb; ioSnd p = do { p <-p; return (snd p ); }; interp env (WHILE test \nbody) = do { while (ioSnd (interp env test)) (ioSnd (interp env body)); return (env, II); }; Types inFile \ninclude the types from Imp,extended witha .le han\u00addle type, TyHandle, to carry the number of available \n.le handles. data Ty = TyUnit --unit type | TyBool --booleans | TyLift Set --host language type | TyHandle \nNat; --a file handle These types can be converted into host language types using an interpTy function, \nas before. In the interpreter, we will need to lookup concrete .le handles from the environment, so it \nis conve\u00adnient to use elements of .nite sets as a concrete representation. interpTy : Ty -> Set; interpTy \nTyBool = Bool; interpTy TyUnit = (); interpTy (TyLift A) = A; interpTy (TyHandle n) = Fin n; A program \nwhich is correct with respect to its .le management operations willbegin andend with no open .le handles.We \nuse FileSafe T as a notational convenience to represent a safe pro\u00adgram which returns a value of type \nT: FileSafe T = File VNil VNil T; We usehandles to declare, in advance, the number of .le handles we \nwill create, also as a notational convenience. The function allClosed returns a list of closed .le handles, \nso that handles requires a program which closes all of the .le handles it opens: handles : (x:Int) -> \nFile VNil (allClosed x) T -> FileSafe T; copyLine : Filepath -> Filepath -> FileSafe TyUnit; copyLine \ninf outf = handles 2  do { fh1 <-OPEN Reading inf; fh2 <-OPEN Writing outf; str <-GETLINE fh1 ?; PUTLINE \nfh2 str ?; CLOSE fh2 ?; CLOSE fh1 ?; }; Figure 10. Simple File program. interpCopyLine : Filepath -> \nFilepath -> IO (); interpCopyLine inf outf = IODo (fopen (getPath inf) \"r\") (\\ x : Ptr => IODo (fopen \n(getPath outf) \"w\") (\\ x0 : Ptr => IODo (freadStr x) (\\ x1 : String => IODo (fputStr x0 x1) (\\ x2 : () \n=> IODo (fputStr x0 \"\\n\") (\\ x3 : () => IODo (fclose x0) (\\ x4 : () => IODo (fclose x) (\\ x5 : () => \nIOReturn (Empty, II)))))))); Figure 11. Simple File program (interpreted).  5.3 Example 1: Copying a \nline Our .rst example is a very simple program which reads a line from one .le and writes it to another \n(Figure 10). IDRIS provides rebindable do-notation, which here uses the BIND and RETURN operators provided \nby File: do using (BIND, RETURN) { ... } GETLINE, PUTLINE and CLOSE each take an additional argument \nas a proof that the .le is open. Since the .les are known statically, these proofs are all straightforward. \nIDRIS provides hooks to the IVOR theorem prover [7] to allow these proofs to be completed, as well as \na means to implement decision procedures. As before, we can specialise the interpreter with respect to \nthis program, and ob\u00adtain a version which calls the I/O operations directly, as in Figure 11. Despite \nadding an environment for external resources, and us\u00ading side-effecting I/O operations, this partial \nevaluation proceeds smoothly. Since the interpreter returns a pair of the .nal environ\u00admentandavalue,the \nspecialisedversion returnsan emptyenviron\u00adment.Thisisasingle constructor,sonotexpensive,butitcaneasily \nbe removed with a call to ioSnd, which can itself be specialised. Input/Output implementation The reasonwedonothaveanydif.cultieswithpartialevaluationof \nI/O is because the evaluator does not execute I/O operations itself, but rather constructs anI/O tree \nexplaining which operations will be executed at run-time. Like Haskell, IDRIS provides an IO monad. This \nis implemented in the style of Hancock and Setzer [18], where anI/O operation consistsofa command followedbya \ncontinuation that de.nes how to process the response to that command: data IO : Set -> Set where IOReturn \n: a -> (IO a) | IODo : (c:Command) -> (Response c -> IO a) -> (IO a); IDRIS de.nes default Command and \nResponse structures which al\u00adlowsimple interactionwiththe outsideworld,pluscallstoCfunc\u00adtions.We de.nea \nbind operation for sequencing I/O operations: bind : IO a->(a -> IO b)->IOb; bind (IOReturn a) k = k \na; bind (IODo c p) k = IODo c (\\x => (bind (p x) k)); copy : Filepath -> Filepath -> FileSafe TyUnit; \ncopy i o = handles 2 do { fh1 <-OPEN Reading i; fh2 <-OPEN Writing o; WHILE (do { e <-EOF (handle fh1) \n?; return (not e); }) (do { str <-GETLINE (handle fh1) ?; PUTLINE (handle fh2) str ?; }); CLOSE (handle \nfh1) ?; CLOSE (handle fh2) ?; }; Figure 12. Copying a .le line by line. We considerIO to be an EDSL for \ndescribing interaction with the operating system, and the run-time system to be its interpreter. This \ngives a clean separation between pure values and external operations, and means we can treat IDRIS evaluation \nas pure, even with side-effecting code. WhyPartial EvaluationWorked As with the functional language example \nin Section 3.1, partial evaluation of our example program above yielded a residual pro\u00adgram without any \ntrace of the interpreter or environment. Firstly, in the host language, I/O operations remain pure, so \nthere is no need to treat them specially. Secondly, we followed a simple rule: Rule 2: The interpreter \nmust only pattern match on the EDSL program to be translated. The reason for this rule is that the EDSL \nprogram is the only static argument, i.e. the only thing we know at compile-time. Everything else (includingtheenvironmentandanyadditionalargumentstothe \nEDSL program)is dynamic, i.e. unknown until the programis run. Therefore, we write the interpreter so \nthat it does not need to match dynamicvalues. Auxiliary functions may match them (indeed, they may need \nto), as long as we follow another rule: Rule 3: Auxiliary functions which match on dynamic data must \nnot be passed EDSL code unless it has been inter\u00adpreted .rst. These rules are necessary to eliminate \nany trace of the interpreter in the residual code, in that they prevent situations which will cause partialevaluation \nto get stuck,but they are not suf.cient to guarantee the best results from partial evaluation, as we \nwill see.  5.4 Example2: Copyinga .le In orderto increaseexpressivity, File also includes while-loops \nand conditional expressions. For example, Figure 12 shows how our previous example can be extended to \ncopyan entire .le. Partial Evaluation First attempt Unfortunately, there is a problem. Using our initial \nimplementa\u00adtionofthe interpretertoevaluatethe copy program, the specialised program includes the fragment \nshown in Figure 13. At .rst, partial evaluation removesallofthe interpreterandenvironmentoverhead. After \ntranslating the while-loop, however, the residual code still carriesanenvironment.Wehavefaithfully followed \nour three rules, sowhydoes this happen?To understand this, we observe that there are several instances \nof bind in the residual program, including the following call: bind (while (IODo (feof x) ...) ...). \nHowever, bind matches on its .rst argument, so can be reduced only when its .rst argument is known. Since \nwhile will never be reduced by the partial evaluator, because it could loop forever, re\u00adductioncannot \ncontinue!Butweknow,bothfromthetypeof WHILE  interpCopy : Filepath -> Filepath -> IO (); interpCopy i \no = IODo (fopen (getPath i) \"r\") (\\ x : Ptr => IODo (fopen (getPath o) \"w\") (\\ x0 : Ptr => bind (bind \n(bind (bind (bind (while (IODo (feof x) {-... while loop translation omitted ... -} (\\ k0 : () => IOReturn \n(Extend (OpenFile (FHandle x)) (Extend (OpenFile (FHandle x0)) Empty), II))) {-... further residual code \nomitted ... -} Figure 13. First (unsuccessful) attempt at partial evaluation. and the behaviour of the \ninterpreter, exactly what the result of the loop will be (namely, a unit value and an unchanged environment): \nWHILE : File ts ts (TyLift Bool) -> File ts ts TyUnit -> File ts ts TyUnit interp env (WHILE test body) \n= do { while (ioSnd (interp env test)) (ioSnd (interp env body)); return (env,II); }; We should be able \nto persuade the evaluator to continue with what we know will be the result of the loop. The solution \nis to observe that bind can reduceiftheI/O operationisin constructor form.We therefore include a lifting \noperation in the Command type: data Command : Set where ... | IOLift : (IO a) -> Command; We also introduce \nan alternative bind operation: ibind : (IO a) -> (a -> (IO b)) -> (IO b); ibind c p = IODo (IOLift c) \np; Using ibind allows evaluation to continue as long as either the result of the operation is known \nstatically, or it is unused. If we have an expression bind ck, for some arbitrary c which is not in constructor \nform, evaluation cannot proceed. On the other hand, using ibind to bind the result of the c which we \nknow will not reduce, expands and evaluates as follows: bind (ibind c p) k ==> bind (IODo (IOLift c) \np) k ==> IODo (IOLift c) (\\x => bind (p x) k) Evaluation then proceeds with the inner bind, with x standing \nfor the value returned by the action c. As long as x is never used in p, as with WHILE where we already \nknow the environment will be unchanged, the inner bind can be reduced. We change the interpreter for \nWHILE as below, and note that the variable x is unused in the continuation: interp env (WHILE test body) \n= ibind (while (ioSnd (interp env test)) (ioSnd (interp env body))) (\\x => return (env, II)); Partial \nEvaluation Successfully After this change, specialising the copy program yields the residual program \nshown in Figure 14, with the environment eliminated entirely.To understand what has happened, observe \nthat IO itself is an EDSL describing execution, interpreted by the run-time system, and bind is a program \ntransformation operation. We can only interpCopy : Filepath -> Filepath -> IO (); interpCopy i o = IODo \n(fopen (getPath i) \"r\") (\\ x : Ptr => IODo (fopen (getPath o) \"w\") (\\ x : Ptr => IODo (IOLift (while \n(IODo (feof x) (\\ x1 : Int => IOReturn (x1==0))) (IODo (freadStr x) (\\ x2 : String => IODo (fputStr x0 \nx2) (\\ x3 : () => IODo (fputStr x0 \"\\n\") (\\ x4 : () => IOReturn II))))) (\\ x5 : () => IODo (fclose x) \n(\\ x6 : () => IODo (fclose x0) (\\ x7 : () => IOReturn (Empty, II))))))); Figure 14. Second (successful) \nattempt at partial evaluation. successfully partially evaluate an EDSL program if the program itself \nis statically known,andwehavebroken thisbyallowing bind to transform a program dynamically, based on \nthe environment. Using ibind, the environment can be discarded statically. This leads to a further rule: \nRule 4: Ensure that EDSL program construction, genera\u00adtion and transformation can be evaluated statically. \nAdding IOLift allows more work to be done statically by giv\u00ading bind a constructor form to evaluate in \nstatic position where it would not otherwise be available. Therefore this rule has a conse\u00adquence speci.c \nto IDRIS programs using the Command/Response I/O system: Consequence: The resultof interpretingacontrol \nstructure should be bound with ibind rather than the default bind.  5.5 Anote on modularity It is worth \nobserving that the safety of the File EDSL requires that only File, its constructors and interp are exposed \nas .le manip\u00adulation operations. If this were not the case, an EDSL programmer wouldbeabletobypassthesafety \nmechanismsgivenbytheEDSL by invoking fopen, fread and other .le manipulation functions directly. This \ncan be achieved, as normal, by not exporting these names to the EDSL programmer. 6. Experimental Results \nTo assess the value of our partial evaluation approach, we imple\u00admented several example programs as EDSLs, \nand measured their execution time and memory footprint before and after partial eval\u00aduation. The example \nprograms that we used were: fact : In Expr, thefactorial program, repeatedly calculating the sum of allfactorials \nfrom 1! to 20!.We implemented this both using direct recursion (as previously described) and using tail \nrecursion, and timed both 20,000 and 2,000,000 iterations. sumlist : In Expr extended with list processing, \ncalculating the sum of a list of 10,000 elements (iterating 20,000 times). copy :InFile, copying the \ncontentsofa large .le, lineby line. copy dynamic :InFile,copying the contents of several large .les, \nreading the .le names from another .le. copy store :InFile,copyingthe contentsofalarge.lebystoring the \nentire contents in memory. sort file :InFile, sortingthe contentsofalarge.leusinga tree sort, and writing \na new .le.  Program Idris (gen.) Idris (spec) Java C(gcc -O3) Time (s) Space (kb) Time (s) Space (kb) \nTime (s) Space (kb) Time (s) Space (kb) fact (20K tail-recursive) 8.598 1892 0.017 816 0.081 11404 0.007 \n292 fact (2M tail-recursive) 877.2 1900 1.650 816 1.937 11388 0.653 292 fact (2M recursive) 538.7 1888 \n3.154 816 N/A N/A N/A N/A sumlist (10K elements) 1148.0 155616 3.181 1604 4.413 12092 0.346 504 copy \n1.974 1944 0.589 1896 1.770 12764 0.564 296 copy dynamic copy store sort file 1.763 7.650 7.510 1940 \n59872 42228 0.507 1.705 5.205 1900 51488 42180 1.673 3.324 2.610 12796 46364 32560 0.512 1.159 1.728 \n304 24276 15832 Java andC versions implemented iteratively Table 1. Experimental Results Wewould,of \ncourse,prefertousereal programs,asmightbefound in the no.b suite for Haskell [30], for example. Since \nIDRIS is a new, experimental, language, however, such a suite does not yet exist, and we have therefore \ntried to implement a variety of simple benchmarks covering both functional and imperative features, as \nwell as examples which use the host language extensively. In order to compare our approach with mainstream \nprogram\u00adming methods, we implemented equivalent programs in Java (ver\u00adsion 1.5.0) andC(gcc 4.0.1, using \n-O3), following the same al\u00adgorithms asfar as possible and appropriate. Clearly, these results should \nbe treated with some caution, since comparing different lan\u00adguage implementations does not always produce \ncompletely fair results: different languages are optimised for different tasks; differ\u00adent algorithms \nwork better in some languages than others; and, to some extent, we are also comparing library implementations. \nNev\u00adertheless, the results provide an indication of the feasibility of our approach for more realistic \ntasks. The source code for ourexamples is available at http://www.cs.st-and.ac.uk/~eb/icfp10/. Analysis \nof our Results Table1gives absolute run times for ourexample programs. These results were obtained on \nan Apple MacBook Pro with a 2.8GHz Intel Core2Duo processor and 4Gb memory, running Mac OSX 10.5.8, using \ntime -l. The times are the reported CPUtimes (i.e. the actual processing time, rather than wall clock \ntime or system time), and the space is the maximum resident set size (i.e. the maximum portion of the \nprocess s memory held in RAM). For each example, specialising the interpreter provides both a signi.cant \nspeedup and a reduction in space usage. The speedup is particularly dramatic for Expr programs. There \nare two likely reasons for this (established using Apple s Shark pro.ler4): .rstly, Expr is far more \n.ne-grained than File, in that it has syntactic forms for variables, values and application, whereas \nFile takes advantage of host language constructs; and secondly,recursivecalls in non-specialised Expr \nprograms must be evaluated lazily, with some associated overhead, in order to avoid expanding the abstract \nsyntax tree inde.nitely. The speedup is less dramatic, but still signi.cant, for programs in File,even \nforsort file which spends much of its time evaluating host language code. It is worth noting that, for \nFile, the main bene.t of partial evaluation is in improved execution time rather than reducing space \nusage. In most cases, the results of specialising the interpreter pro\u00adducesa program that runsfaster \nthan itsJava equivalent and also uses signi.cantly less memory. For sort file, this is not the case, \nhowever, because the Java version of tree sort allows in\u00adplace update (this is safe since we discard \nthe intermediate trees) whereas the I DRIS version, being purely functional, does not. Few of the programs \nare close to the ef.ciency of the C equivalents, 4http://developer.apple.com/tools/sharkoptimize.html \nIdris (gen.) Idris (spec.) Program Time Space Time Space fact (tail-rec, 2M) 992.15 10904 1.642 816 sumlist \n709.6 70824 3.161 1604 copy 2.048 10976 0.587 10916 copy dynamic 1.847 10948 0.521 10920 copy store 7.576 \n57944 1.708 57936 sort file 7.593 49840 5.223 40604 Table 2. Space and time results with default heap \nsize 10 Mb due to the overhead of the run-time system, but in every case the results are well within \nan order of magnitude, and in one case(copy dynamic),the partiallyevaluated interpreteris actually slightlyfaster \nthantheC version.Itisworth noting thattheIDRIS compiler and run-time system are at an early stage of \ndevelopment, and do not yet apply well-known optimisations that have been used in production systems.Forexample, \ndeforestation [17, 43] might improve the performance of sort file and similar programs that build and \ndestroyintermediate structures. Our results are therefore highly encouraging. 6.1 Garbage Collection \nIDRIS compiles to C, using the Boehm-Demers-Weiser conserva\u00adtivegarbage collector [5] with an initial \nheap size of 1Mb. As an experiment, we increased the initial heap size to 10Mb,hypothe\u00adsising that thiswould \nleadtofaster runtimesduetofewer callsto thegarbage collector,attheexpenseofalarger memory footprint. \nThe results are shown inTable 2. Infact, theysuggest little more than that it is dif.cult to predict \nthe effect of changing garbage collector parameters. Increasing the heap-size has little positive ef\u00adfect \non either execution time or heap usage, other than sumlist where there is a signi.cant bene.t for the \ngeneric version. In gen\u00aderal, with a larger heap, while the programs collect less frequently, each collection \ntakes longer. Further (informal) experiments with a hand-written allocator for IDRIS suggest that we \ncould signi.\u00adcantly improveexecution timesby implementinga special-purpose collector, with speci.c knowledgeof \ntheI DRIS memory structure.  6.2 ALarger Example Network Protocols Given our encouraging results, we \nhave begun research into imple\u00admenting veri.ed network protocols, using the EDSL approach and partialevaluation.Wehave \nimplementeda simple transport proto\u00adcol [4] as a DSL embedded in IDRIS. Space does not permit a full \nexplanation,but the DSL encodesvalid transitionsofa state ma\u00adchine, where transitions represent actions \nsuch as sending a packet toaremote machine and receiving an acknowledgment, and its rep\u00adresentationis \nparameterisedover startandend states. Programs are therefore guaranteed to use valid transitions, and \nterminate in an  Program Idris (gen.) Idris (spec.) Time Space Time Space protocol (20K packets) 0.990 \n24285 0.751 24011 Table 3. Space and time results for Network Protocol expected state. The state machine \nhandles error conditions such as timeout and dropped packets.Table3gives CPU time and space usage for \na test run sending 20,000 packets to a remote machine. Once again, specialisation improves the run time. \nPro.ling suggests that much of the time spent in this example, at this stage of devel\u00adopment, is involved \nin validating packet data. Nevertheless, a sig\u00adni.cant interpreter overhead is eliminated. 7. RelatedWork \nDomain-speci.c languages are a recognised technique for improv\u00ading programmer productivitybyproviding \nappropriate notation and abstractions for a particular problem domain [42]. Recently, the Embedded Domain \nSpeci.c Language (EDSL) approach, in which a DSL is implemented by embedding in a host language, has \nbeen gaining in popularity, with Haskell a popular choice as the host language[3,14,24].A common approachtoexecuting \nthese lan\u00adguages is to use a code generation library such as LLVM [23, 41]. We prefer to use partial \nevaluation, for two reasons: .rstly, if we aim for veri.cation, a general purpose partial evaluator need \nonly be veri.ed once, rather than verifying a specialised code generator foreverycompiled program;andsecondly,wecanproduceef.cient \nlanguage implementations more rapidly. Partial evaluation [20] has been studied for manyyears, along \nwith related methods such as multi-stage programming as imple\u00admentedin e.g. MetaOCaml [38],Template Haskell \n[35], or Conco\u00adqtion [15]. The idea that an interpreter can be specialised to gener\u00adate an ef.cient executable \nhas been known since at least 1971 [16]. It may therefore seem surprising that the technique is not more \nwidely used5. However, there are several issues that must be ad\u00addressed when using partial evaluation \nin a general setting, e.g. tag elimination [39], termination analysis [1], code duplication [36], and \nit is also dif.cult to use with imperative programming and side effects [12]. In contrast, our approach \nis speci.cally targeted towardstheef.cientexecutionof EDSLsina dependently-typed purely functional host \nlanguage, where partial evaluation is effec\u00adtively \u00df-normalisation.We thereforeavoid manyofthe complexi\u00adties \ninvolved with general solutions. Our approach makes extensive use of tagless interpreters. An alternative \napproach is to use combinators, rather than data con\u00adstructors, tobuild an object language, which can \nthen be partially evaluated [10]. Combined with representation of stronger invari\u00adants [22], it is possible \nthat ef.cient, correct EDSLs could be im\u00adplementedinamore mainstream functional language.However,the \nstronger the invariants required, the more likely it is that a stronger type system with full dependent \ntypes will be needed. Finally, supercompilation [27, 28] is closely related to partial evaluation. This \ntechnique aims to reduce abstraction overhead through compile-time evaluation. We believe that it may \nbe par\u00adticularly effective when combined with tagless interpreters, and we hope to explore it in future \nwork. 5One example where partial evaluation has been applied to a realistic problem isPantheon [34], \nan implementation ofPan [14] usingTemplate Haskell. 8. Conclusions and FurtherWork The underlying motivation \nfor our research is to be able to rea\u00adson about extra-functional properties of programs, while ensur\u00ading \ngoodef.ciency.Wehave found thatif EDSLs are based ona dependently-typed host language, then theypresentapromising \nba\u00adsis for such reasoning. Moreover, we have established the feasibil\u00adity of producing ef.cient enough \nimplementations this way. Our methods apply notonlyto EDSLs embeddedinI DRIS,butwould also apply to those \nembedded in any language with a suitably rich type system,ifextendedwiththe [static] annotation and caching \npartial evaluator described in Section 4.1. This includes languages such as Agda, Coq and Concoqtion, \nor even GHC with recent ex\u00adtensions such as GADTs and open type families [33]. Given the current trend \nfor EDSL development in Haskell, our results sug\u00adgest that extending GHC s compile-time evaluation machinery \nto support full partial evaluation would be bene.cial. In particular, through developing several EDSL \nprograms using a variety of language features, we have found that dependently\u00adtyped languages such as \nIDRIS represent a sweet spot where partialevaluationis particularlyeffective.With minimal modi.ca\u00adtion \nto the evaluator, and minimal annotations to direct the process ofevaluation,theef.ciencyof our programs \ncomparesfavourably with similar hand-written programsinJava(infact, theyare gener\u00adallyfasterwithbettermemoryusage),andisnot \nsigni.cantlyworse thanC(generally around 2 3 times slower, and about3times the memory usage). Since we \nhave not yet applied standard optimisa\u00adtion techniques, this is highly promising. The main reason partial \nevaluation is so effective in our setting is that we can state precisely what the type of a residual \nprogram should be, and can allow that type to vary according to the input program.This removesanyneedfortaggingof \nintermediatevalues. I/O and side effects also pose no problem because we distinguish computation and \nexecution. Code duplication, which might arise as a result of carelessly expanding de.nitions, is easily \navoided by caching the result of function applications. There are some obvious limitations to our approach \nwhich we hopeto addressin futurework.In particular,anEDSLwithhigher\u00adorder functions would still retain \nan interpreter in the residual pro\u00adgram, because higher-order functions accept functions as dynamic arguments, \nviolating our Rule 4. Possible solutions include the use of multi-stage languages, defunctionalisation \nor run-time type-safe transformation rules. Amore serious limitation is that generated programs can only \nbe as ef.cient as the underlying host language constructs. For purely functional EDSLs this is not a \nproblem, as there are equiva\u00adlent constructsinIDRIS,butitwouldbea problemfora language with, e.g., mutable \nlocal variables. For the resource-safe EDSLs that currently interest us, such as those for safe network \nprotocols, we do not anticipate that this will be an issue,but if it is, then it maybe possibleto removetheoverhead,forexampleby \ncompiling environments specially. Toconclude, partialevaluationinadependently-typed language enables \ninef.cient EDSL interpretation engines to be scrapped, so achieving both ef.ciency and veri.ability. \nWe hope that our techniques and the new opportunities afforded by dependently\u00adtyped languages will leadtoarenewed \ninterestin partialevaluation. Acknowledgments Thisworkwas partlyfundedby the Scottish Informatics and \nCom\u00adputer Science Alliance (SICSA),byEU Framework7ProjectNo. 248828(ADVANCE)andbyEPSRCgrant EP/F030657(Islay).We \nthank our colleagues, notablyWilliam Cook, James McKinna and Anil Madhavapeddy for several helpful discussions, \nand the anony\u00admous reviewers for their constructive suggestions on this paper.  References [1] P. H. \nAndersen and C. K. Holst. Termination analysis for of.ine partial evaluation of a higher order functional \nlanguage. In Proc. SAS 96: Intl. Symp. on Static Analysis, pages 67 82. Springer, 1996. [2] L. Augustsson \nand M. Carlsson. An exercise in dependent types: A well-typed interpreter. In Workshop on Dependent Types \nin Programming, Gothenburg, 1999. Available from http://www. cs.chalmers.se/~augustss/cayenne/interp.ps. \n[3]L. Augustsson,H. Mansell,andG.Sittampalam.Paradise:atwo-stage DSL embedded in Haskell. In Proc. ICFP \n2008: International Conf. on Functional Programming, pages 225 228.ACM, 2008. [4] S. Bhatti, E. Brady, \nK. Hammond, and J. McKinna. Domain speci.c languages (DSLs) for network protocols. In InternationalWorkshop \non Next Generation Network Architecture (NGNA2009), 2009. [5] H.-J. Boehm, A. J. Demers, Xerox Corporation \nSilicon Graphic, and Hewlett-Packard Company. A garbage collector for C and C++. http://www.hpl.hp.com/personal/Hans \nBoehm/gc/, 2001. [6] E. Brady. Practical Implementation of a Dependently Typed Functional Programming \nLanguage. PhD thesis, University of Durham, 2005. [7] E. Brady. Ivor, a proof engine. In Implementation \nand Application of Functional Languages 2006, volume 4449 of LNCS, pages 145 162. Springer, 2007. [8] \nE. Brady and K. Hammond. A veri.ed staged interpreter is a veri.ed compiler. In Proc. GPCE 06: Conf. \non Generative Programming and Component Engineering, 2006. [9] E. Brady, C. McBride, and J. McKinna. \nInductivefamilies need not store their indices. In S. Berardi, M. Coppo, andF. Damiani, editors, Types \nfor Proofs and Programs 2003, volume 3085, pages 115 129. Springer, 2004. [10] J. Carette, O. Kiselyov, \nand C.-c. Shan. Finally tagless, partially evaluated:Tagless staged interpreters for simpler typed languages. \nJ. Funct. Program., 19(5):509 543, 2009. [11] T. Coquand. An algorithm for type-checking dependent types. \nScience of Computer Programming, 26(1-3):167 177, 1996. [12] S. Debois. Imperative program optimization \nby partial evaluation. In Proc. PEPM 04:ACM Symp. onPartial Evaluation and Semantics-Based Program Manipulation, \npages 113 122.ACM, 2004. [13] B. Delaware and W. R. Cook. Generic operations and partial evaluation using \nmodels, 2009. Draft. [14] C. Elliott, S. Finne, and O. De Moor. Compiling embedded languages. J. Funct. \nProgram., 13(3):455 481, 2003. [15] S.Fogarty, E.Pasalic, J. Siek, andW.Taha. Concoqtion: indexed types \nnow! In Proc. PEPM 07:ACM Symp. onPartial Evaluation and Semantics-Based Program Manipulation, pages \n112 121, 2007. [16] Y. Futamura. Partial evaluation of computation process an approach to a compiler-compiler. \nSystems, Comps., Controls, 2(5):45 50, 1971. [17] A. Gill. Cheap deforestation for non-strict functional \nlanguages. PhD thesis, University of Glasgow, January 1996. [18] P. Hancock and A. Setzer. Interactive \nprograms in dependent type theory. InP. Clote and H. Schwichtenberg, editors, Proc. CSL 2000: 14th Ann. \nConf. of EACSL,Fischbau, Germany, 21 26Aug 2000, LNCS 1862, pages 317 331. 2000. [19]P. Hudak. Building \ndomain-speci.c embedded languages. ACM Computing Surveys, 28A(4), December 1996. [20]N.Jones,C.Gomard,andP. \nSestoft. Partial Evaluation andAutomatic Program Generation. Prentice Hall International, 1993. [21] \nN. D. Jones. Challenging problems in partial evaluation and mixed computation. New Gen. Comput., 6(2-3):291 \n302, 1988. [22] O. Kiselyov and C.-c. Shan. Lightweight monadic regions. In Proc. Haskell 08:ACM SIGPLAN \nSymp. on Haskell, pages 1 12, 2008. [23] C. Lattner. LLVM: An infrastructure for multi-stage optimization. \nMaster s thesis, Computer Science Dept., University of Illinois at Urbana-Champaign, December 2002. [24]S.Lee,M.M.T. \nChakravarty,V.Grover,andG.Keller. GPUkernels as data-parallel array computations in Haskell. In Workshop \non Exploiting Parallelism using GPUs and other Hardware-Assisted Methods (EPAHM 2009), 2009. [25]A.L\u00a8oh,C. \nMcBride,andW. Swierstra.Atutorial implementationofa dependently typed lambda calculus, 2010. To appear \nin Fundam. Inf. [26] C. McBride and J. McKinna. The view from the left. Journal of Functional Programming, \n14(1):69 111, 2004. [27] N. Mitchell. Transformation and Analysis of Functional Programs. PhD thesis, \nUniversityofYork, June 2008. [28] N. Mitchell and C. Runciman. A supercompiler for core Haskell. In Implementation \nand Application of Functional Languages 2007, volume 5083 of LNCS, pages 147 164. Springer, May 2008. \n[29] U. Norell. Towards a practical programming language based on dependent type theory. PhD thesis, \nDepartment of Computer Science and Engineering, Chalmers University of Technology, SE-412 96 G\u00a8oteborg, \nSweden, September 2007. [30] W. Partain. The no.b benchmark suite of Haskell programs. In J. Launchbury \nand P. Sansom, editors, Functional Programming, Workshops in Computing, pages 195 202. Springer, 1992. \n[31] E.Pasal. \u00b4ic,W.Taha, andT. Sheard. Tagless staged interpreters for typed languages. In Proc. 2002 \nInternational Conf. on Functional Programming (ICFP 2002).ACM, 2002. [32] S. Peyton Jones and S. Marlow. \nSecrets of the Glasgow Haskell Compiler inliner. Journal of Functional Programming, 12(4):393 434, September \n2002. [33] T. Schrijvers, S. Peyton Jones, M. Chakravarty, and M. Sulzmann. Type checking with open type \nfunctions. In International Conf. on Functional Programming (ICFP 2008), pages 51 62,NewYork,NY, USA, \n2008.ACM. [34] S. Seefried, M. Chakravarty, and G.Keller. Optimising embedded DSLs usingTemplate Haskell. \nIn Proc. GPCE 04: Conf. Generative Prog. and Component Eng., LNCS. Springer, 2004. [35] T. Sheard and \nS. Peyton Jones. Template metaprogramming for Haskell. In ACM HaskellWorkshop, pages 1 16, Oct. 2002. \n[36] K.Swadi,W.Taha,O. Kiselyov, andE.Pasalic.Amonadic approach for avoiding code duplication when staging \nmemoized functions. In Proc. PEPM 06:ACM Symp. onPartial Evaluation and Semantics\u00adbased Program Manipulation, \npages 160 169, 2006. [37] W.Taha. Multi-stage Programming: Its Theory and Applications. PhD thesis, Oregon \nGraduate Inst.of Science andTechnology, 1999. [38]W.Taha.AGentle Introduction to Multi-stage Programming, \n2003. Available fromhttp://www.cs.rice.edu/~taha. [39]W.TahaandH. Makholm.Tag elimination or type specialisation \nis a type indexed effect. In Subtyping and Dependent Types in Programming, APPSEMWorkshop, 2000. [40]W.Taha,H. \nMakholm, andJ. Hughes. Tag elimination and jones\u00adoptimality. In PADO 01: Proceedings of the Second Symposium \non Programs as Data Objects, pages 257 275, London, UK, 2001. Springer-Verlag. [41] D.Terei. Low level \nvirtual machine for Glasgow Haskell Compiler. Bachelor s Thesis, Computer Science and Engineering Dept., \nThe UniversityofNew SouthWales, Sydney, Australia, 2009. [42] A.van Deursen,P. Klint, andJ.Visser. Domain-speci.c \nlanguages an annotated bibliography. http://homepages.cwi.nl/~arie/ papers/dslbib/, 2000. [43]P.Wadler. \nDeforestation:Transforming programsto eliminate trees. Theoretical Computer Science, 73:231 248, 1990. \n     \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Partial evaluation aims to improve the efficiency of a program by specialising it with respect to some known inputs. In this paper, we show that partial evaluation can be an effective and, unusually, easy to use technique for the efficient implementation of embedded domain-specific languages. We achieve this by exploiting dependent types and by following some simple rules in the definition of the interpreter for the domain-specific language. We present experimental evidence that partial evaluation of programs in domain-specific languages can yield efficient residual programs whose performance is competitive with their Java and C equivalents and which are also, through the use of dependent types, verifiably resource-safe. Using our technique, it follows that a verifiably correct and resource-safe program can also be an efficient program</p>", "authors": [{"name": "Edwin C. Brady", "author_profile_id": "81319488859", "affiliation": "University of St Andrews, St Andrews, United Kingdom", "person_id": "P2338223", "email_address": "", "orcid_id": ""}, {"name": "Kevin Hammond", "author_profile_id": "81100001518", "affiliation": "University of St Andrews, St Andrews, United Kingdom", "person_id": "P2338224", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863587", "year": "2010", "article_id": "1863587", "conference": "ICFP", "title": "Scrapping your inefficient engine: using partial evaluation to improve domain-specific language implementation", "url": "http://dl.acm.org/citation.cfm?id=1863587"}