{"article_publication_date": "09-27-2010", "fulltext": "\n Regular, Shape-polymorphic, Parallel Arrays in Haskell Gabriele Keller Manuel M. T. Chakravarty Roman \nLeshchinskiy Simon Peyton Jones Ben Lippmeier Computer Science and Engineering Microsoft Research Ltd \nUniversity of New South Wales, Australia Cambridge, England {keller,chak,rl,benl}@cse.unsw.edu.au {simonpj}@microsoft.com \n Abstract We present a novel approach to regular, multi-dimensional arrays in Haskell. The main highlights \nof our approach are that it (1) is purely functional, (2) supports reuse through shape polymorphism, \n(3) avoids unnecessary intermediate structures rather than relying on subsequent loop fusion, and (4) \nsupports transparent parallelisation. We show how to embed two forms of shape polymorphism into Haskell \ns type system using type classes and type families. In par\u00adticular, we discuss the generalisation of \nregular array transforma\u00adtions to arrays of higher rank, and introduce a type-safe speci.ca\u00adtion of array \nslices. We discuss the runtime performance of our approach for three standard array algorithms. We achieve \nabsolute performance com\u00adparable to handwritten C code. At the same time, our implementa\u00adtion scales \nwell up to 8 processor cores. Categories and Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language \nConstructs and Features Concurrent program\u00adming structures; Polymorphism; Abstract data types General \nTerms Languages, Performance Keywords Arrays, Data parallelism, Haskell 1. Introduction In purely functional \nform, array algorithms are often more elegant and easier to comprehend than their imperative, explicitly \nloop\u00adbased counterparts. The question is, can they also be ef.cient? Experience with Clean, OCaml, and \nHaskell has shown that we can write ef.cient code if we sacri.ce purity and use an im\u00adperative array \ninterface based on reading and writing individual array elements, possibly wrapped in uniqueness types \nor mon\u00adads [10, 11, 13]. However, using impure features not only obscures clarity, but also forfeits \nthe transparent exploitation of the data par\u00adallelism that is abundant in array algorithms. In contrast, \nusing a purely-functional array interface based on collective operations such as maps, folds, and permutations \nemphasises an algorithm s high-level structure and often has an obvious parallel implementation. This \nobservation was the basis Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright c &#38;#169; 2010 ACM \n978-1-60558-794-3/10/09. . . $10.00 for previous work on algorithmic skeletons and the use of the Bird-Meertens \nFormalism (BMF) for parallel algorithm design [17]. Our own work on Data Parallel Haskell (DPH) is based \non the same premise, but aims at irregular data parallelism which comes with its own set of challenges \n[16]. Other work on byte arrays [7] also aims at high-performance, while abstracting over loop-based \nlow\u00adlevel code using a purely-functional combinator library. We aim higher by supporting multi-dimensional \narrays, more functionality, and transparent parallelism. We present a Haskell li\u00adbrary of regular parallel \narrays, which we call Repa1 (Regular Par\u00adallel Arrays). While Repa makes use of the Glasgow Haskell Com\u00adpiler \ns many existing extensions, it is a pure library: it does not require any language extensions that are \nspeci.c to its implementa\u00adtion. The resulting code is not only as fast as when using an imper\u00adative array \ninterface, it approaches the performance of handwritten C code, and exhibits good parallel scalability \non the con.gurations that we benchmarked. In addition to good performance, we achieve a high degree of \nreuse by supporting shape polymorphism. For example, map works over arrays of arbitrary rank, while sum \ndecreases the rank of an arbitrary array by one we give more details in Section 4. The value of shape \npolymorphism has been demonstrated by the language Single Assigment C, or SAC [18]. Like us, SAC aims \nat purely functional high-performance arrays, but SAC is a specialised array language based on a purpose-built \ncompiler. We show how to embed shape polymorphism into Haskell s type system. The main contributions \nof the paper are the following: An API for purely-functional, collective operations over dense, rectangular, \nmulti-dimensional arrays supporting shape poly\u00admorphism (Section 5).  Support for various forms of constrained \nshape polymorphism in a Hindley-Milner type discipline with type classes and type families (Section 4). \n An aggressive loop fusion scheme based on a functional repre\u00adsentation of delayed arrays (Section 6). \n A scheme to transparently parallelise array algorithms based on our API (Section 7)  An evaluation \nof the sequential and parallel performance of our approach on the basis of widely used array algorithms \n(Section 8).  Before diving into the technical details of our contributions, the next section illustrates \nour approach to array programming by way of an example. 1 Repa means turnip in Russian.  2. Our approach \nto array programming A simple operation on two-dimensional matrices is transposition. With our library \nwe express transposition in terms of a permutation operation that swaps the row and column indices of \na matrix: transpose2D :: Elt e => Array DIM2 e -> Array DIM2 e transpose2D arr = backpermute new_extent \nswap arr where swap (Z :.i :.j) = Z :.j :.i new_extent = swap (extent arr) Like Haskell 98 arrays, our \narray type is parameterised by the array s index type, here DIM2, and by its element type e. The index \ntype gives the rank of the array, which we also call the array s dimensionality, or shape. Consider the \ntype of backpermute, given in Figure 1. The .rst argument is the bounds (or extent) of the result array, \nwhich we obtain by swapping the row and column extents of the input array. For example transposing a \n3 \u00d7 12 matrix gives a 12 \u00d7 3 matrix.2 The backpermute function constructs a new array in terms of an \nexisting array solely through an index transformation, supplied as its second argument, swap: given an \nindex into the result matrix, swap produces the corresponding index into the argument matrix. A more \ninteresting example is matrix-matrix multiplication: mmMult :: (Num e, Elt e) => Array DIM2 e -> Array \nDIM2 e -> Array DIM2 e mmMult arr brr = sum (zipWith (*) arrRepl brrRepl) where trr = transpose2D brr \narrRepl = replicate (Z :.All :.colsB :.All) arr brrRepl = replicate (Z :.rowsA :.All :.All) trr (Z :.colsA \n:.rowsA) = extent arr (Z :.colsB :.rowsB) = extent brr The idea is to expand both rank-two argument arrays \ninto rank\u00adthree arrays by replicating them across a new dimension, or axis, as illustrated in Figure \n2. The front face of the cuboid represents the array arr, which we replicate as often as brr has columns \n(colsB), producing arrRepl. The top face represents trr (the transposed brr), which we replicate as often \nas arr has rows (rowsA), producing brrRepl. As indicated by the .gure, the two replicated arrays have \nthe same extent, which corresponds to the index space of matrix multiplication: (AB)i,j =Snk=1Ai,kBk,j \nwhere i and j correspond to rowsA and colsB in our code. The summation index k corresponds to the innermost \naxis of the repli\u00adcated arrays and to the left-to-right axis in Figure 2. Along this axis we perform \nthe summation after an elementwise multiplication of the replicated elements of arr and brr by zipWith \n(*). A naive implementation of the operations used in mmMult would result in very bad space and time \nperformance. In particular, it would be very inef.cient to compute explicit representations of the replicated \nmatrices arrRepl and brrRepl. Indeed, a key principle of our design is to avoid generating explicit representations \nof the intermediate arrays that can be represented as the original arrays combined with a transformation \nof the index space (Section 3.2). A rigorous application of this principle results in aggressive loop \nfusion (Section 6) producing code that is similar to imperative code. As a consequence, this Haskell \ncode has about the same performance as handwritten C code for the same computation. Even 2 For now, just \nread the notation (Z :.i :.j) as if it was the familiar pair (i,j). The details are in Section 4 where \nwe discuss shape polymorphism. extent :: Array sh e -> sh sum :: (Shape sh, Elt e, Num e) => Array (sh \n:. Int) e -> Array sh e zipWith :: (Shape sh, Elt e1, Elt e2, Elt e3) =>(e1->e2 ->e3) -> Array sh e1 \n-> Array sh e2 -> Array sh e3 backpermute :: (Shape sh, Shape sh , Elt e) => sh -> (sh -> sh) -> Array \nsh e -> Array sh e Figure 1. Types of library functions Figure 2. Matrix-matrix multiplication illustrated \nmore importantly, we measured very good absolute speedup, \u00d77.2 for 8 cores, on multicore hardware. For \nthe C code, this can only be achieved through considerable additional effort or by employing special-purpose \nlanguage extensions such as OpenMP [22] which often have dif.culties with more complex programs. 3. Representing \narrays The representation of arrays is a central issue in any array library. Our library uses two devices \nto achieve good performance: 1. We represent array data as contiguously-allocated ranges of unboxed values. \n 2. We delay the construction of intermediate arrays to support constant-time index transformations and \nslices, and to combine these operations with traversals over successive arrays.  We describe these two \ntechniques in the following sections. 3.1 Unboxed arrays In Haskell 98 arrays are lazy: each element \nof an array is evaluated only when the array is indexed at that position. Although conve\u00adnient, laziness \nis Very Bad Indeed for array-intensive programs: A lazy array of (say) Float is represented as an array \nof point\u00aders to either heap-allocated thunks, or boxed Float objects, de\u00adpending on whether they have \nbeen forced. This representation requires at least three times as much storage as a conventional, contiguous \narray of unboxed .oats. Moreover, when iterating through the array, the lazy representation imposes higher \nmem\u00adory traf.c. This is due to the increased size of the individual elements, as well as their lower \nspacial locality.  In a lazy array, evaluating one element does not mean that the other elements will \nbe demanded. However, the overwhelm\u00adingly common case is that the programmer intends to demand the entire \narray, and wants it evaluated in parallel.   We can solve both of these problems simultaneously using \na Haskell-folklore trick. We de.ne a new data type of arrays, which we will call UArr, short for unboxed \narray . These arrays are one-dimensional, indexed by Int, and are slightly stricter than Haskell 98 arrays: \na UArr as a whole is evaluated lazily, but an attempt to evaluate any element of the array (e.g. by indexing) \nwill cause evaluation of all the others, in parallel. For the sake of de.niteness we give a bare sketch \nof how UArr is implemented. However, this representation is not new; it is well established in the Haskell \nfolklore, and we use it in Data Parallel Haskell (DPH) [5, 16], so we do not elaborate the details. class \nElt e where data UArr e (!) :: Array e -> Int -> e ...more methods... instance Elt Float where data \nUArr Float = UAF Int ByteArray# (UAF max ba) ! i | i < max = F# (indexByteArray ba i) | otherwise = error \n\"Index error\" ...more methods... instance (Elt a, Elt b) => Elt (a :*: b) where data UArr (a :*: b) = \nUAP (UArr a) (UArr b) (UAP ab) !i =(a!i:*:b!i) ...more methods... Here we make use of Haskell s recently \nadded associated data types [4] to represent an array of Float as a contiguous array of unboxed .oats \n(the ByteArray#), and an array of pairs as a pair of arrays. Because the representation of the array \ndepends on the element type, indexing must vary with the element type too, which explains why the indexing \noperation (!) is in a type class Elt. In addition to an ef.cient underlying array representation, we \nalso need the infrastructure to operate on these arrays in parallel, using multiple processor cores. \nTo that end we reuse part of our own parallel array library of Data Parallel Haskell. This provides us \nwith an optimised implementation of UArr and the Elt class, and with parallel collective operations over \nUArr. It also requires us to represent pairs using the strict pair constructor (:*:), instead of Haskell \ns conventional (,).  3.2 Delayed arrays When using Repa, index transformations such as transpose2D (discussed \nin Section 2) are ubiquitous. As we expect index transformations to be cheap, it would be wrong to (say) \ncopy a 100Mbyte array just to transpose it. It is much better to push the index transformation into the \nconsumer, which can then consume the original, unmodi.ed array. We could do this transformation statically, \nat compile time, but doing so would rely on the consumer being able to see the index transformation. \nThis could make it hard for the programmer to predict whether or not the optimisation would take place. \nIn Repa we instead perform this optimisation dynamically, and offer a guarantee that index transformations \nperform no data movement. The idea is simple and well known: just represent an array by its indexing \nfunction, together with the array bounds (this is not our .nal array representation): data DArray sh \ne = DArray sh (sh -> e) With this representation, functions like backpermute (whose type signature appeared \nin Figure 1) are quite easy to implement: backpermute sh fn (Array sh ix1) = Array sh (ix1 . fn) We \ncan also wrap a UArr as a DArray: wrap :: (Shape sh, Elt e) => sh -> UArr e -> DArray sh e wrap sh uarr \n= Array sh idx where idx i = uarr ! toIndex sh i When wrapping an DArray over a UArr, we also take the \noppor\u00adtunity to generalise from one-dimensional to multi-dimensional arrays. The index of these multi-dimensional \narrays is of type sh, where the Shape class (to be described in Section 4) includes the method toIndex \n:: Shape sh => sh -> sh -> Int. This method maps the bounds and index of an Array to the correspond\u00ading \nlinear Int index in the underlying UArr.  3.3 Combining the two Unfortunately, there are at least two \nreasons why it is not always bene.cial to delay an array operation. One is sharing, which we discuss \nlater in Section 6. Another is data layout. In our mmMult example from Section 2, we want to delay the \ntwo applications of replicate, but not the application of transpose2D. Why? We store multi-dimensional \narrays in row-major order (the same layout Haskell 98 uses for standard arrays). Hence, iterating over \nthe second index of an array of rank 2 is more cache friendly than iterating over its .rst index. It \nis well known that the order of the loop nest in an imper\u00adative implementation of matrix-matrix multiplication \nhas a dra\u00admatic effect on performance due to these cache effects. By forcing transpose2D to produce its \nresult as an unboxed array in mem\u00adory we call this a manifest array instead of leaving it as a de\u00adlayed \narray, the code will traverse both matrices by iterating over the second index in the inner loop. Overall, \nwe have the following implementation: mmMult arr brr = sum (zipWith (*) arrRepl brrRepl) where trr = \nforce (transpose2D brr) --New! force! arrRepl = replicate (Z :.All :.colsB :.All) arr brrRepl = replicate \n(Z :.rowsA :.All :.All) trr (Z :.colsA :.rowsA) = extent arr (Z :.colsB :.rowsB) = extent brr We could \nimplement force by having it produce a value of type UArr and then apply wrap to turn it into a DArray \nagain, providing the appropriate memory layout for a cache-friendly traversal. This would work, but we \ncan do better. The function wrap uses array indexing to access the underlying UArr. In cases where this \nindex\u00ading is performed in a tight loop, GHC can optimise the code more thoroughly when it is able to \ninline the indexing operator, instead of calling an anonymous function encapsulated in the data type \nDArray. For recursive functions, this also relies on the constructor specialisation optimisation [15]. \nHowever, as explained in Coutts et al. [6, Section 7.2], to allow this we must make the special case \nof a wrapped UArr explicit in the datatype, so the optimiser can see whether or not it is dealing directly \nwith a manifest array. Hence, we de.ne regular arrays as follows: data Array sh e = Manifest sh (UArr \ne) | Delayed sh (sh -> e) We can unpack an arbitrary Array into delayed form thus: delay :: (Shape sh, \nElt e) => Array sh e -> (sh, sh -> e) delay (Delayed sh f) = (sh, f) delay (Manifest sh uarr) = (sh, \n\\i -> uarr ! toIndex sh i)  infixl 3 :. dataZ =Z data tail :. head = tail :. head type DIM0 = Z type \nDIM1 = DIM0 :. Int type DIM2 = DIM1 :. Int type DIM3 = DIM2 :. Int class Shape sh where rank :: sh -> \nInt size :: sh -> Int --Number of elements toIndex :: sh -> sh -> Int --Index into row-major fromIndex \n--representation :: sh -> Int -> sh --Inverse of toIndex <..and so on..> instance Shape Z where ... instance \nShape sh => Shape (sh:.Int) where ... Figure 3. De.nition of shapes This is the basis for a general traverse \nfunction that produces a delayed array after applying a transformation. The transformation produced with \ntraverse may include in\u00addex space transformations or other computations: traverse :: (Shape sh, Shape \nsh , Elt e) => Array sh e -> (sh -> sh ) -> ((sh -> e) -> sh -> e ) -> Array sh e traverse arr sh_fn \nelem_fn = Delayed (sh_fn sh) (elem_fn f) where (sh, f) = delay arr We use traverse to implement many \nof the other operations of our library for example, backpermute is implemented as: backpermute :: (Shape \nsh, Shape sh , Elt e) => sh -> (sh -> sh) -> Array sh e -> Array sh e backpermute sh perm arr = traverse \narr (const sh) (. perm) We discuss the use of traverse in more detail in Sections 5 &#38; 7. 4. Shapes \nand shape polymorphism In Figure 1 we gave this type for sum: sum :: (Shape sh, Num e, Elt e) => Array \n(sh:.Int) e -> Array sh e As the type suggests, sum is a shape-polymorpic function: it can sum the rightmost \naxis of an array of arbitrary rank. In this section we describe how shape polymorphism works in Repa. \nWe will see that combination of parametric polymorphism, type classes, and type families enables us to \ntrack the rank of each array in its type, guaranteeing the absence of rank-related runtime errors. We \ncan do this even in the presence of operations such as slicing and replication which change the rank \nof an array. However, bounds checks on indices are still performed at runtime tracking them requires \nmore sophisticated type system support [20, 24]. 4.1 Shapes and indices Haskell s tuple notation does \nnot allow us the .exibility we need, so we introduce our own notation for indices and shapes. As de.ned \nin Figure 3, we use an inductive notation of tuples as heterogenous snoc lists. On both the type-level \nand the value-level, we use the in\u00ad.x operator (:.) to represent snoc. The constructor Z corresponds \nto a rank zero shape, and we use it to mark the end of the list. Thus, a three-dimensional index with \ncomponents x, y and z is written (Z:.x:.y:.z) and has type (Z:.Int:.Int:.Int). This type is the shape \nof the array. Figure 3 gives type synonyms for common shapes: a singleton array of shape DIM0 represents \na scalar value; an array of shape DIM1 is a vector, and so on. The motivation for using snoc lists, rather \nthan the more con\u00adventional cons lists, is this: we store manifest arrays in row-major order, where the \nrightmost index is the most rapidly-varying when traversing linearly over the array in memory. For example, \nthe value at index (Z:.3:.8) is stored adjacent to that at (Z:.3:.9). This is the same convention adopted \nby Haskell 98 standard arrays. We draw array indices from Int values only, so the shape of a rank-n array \nis: Z :. Int :. \u00b7\u00b7\u00b7 :. Int , ___ n times In principle, we could be more general and allow non-Int indices, \nlike Haskell s index type class Ix. However, this would complicate the library and the presentation, \nand is orthogonal to the contri\u00adbutions of this paper; so we do not consider it here. Nevertheless, shape \ntypes, such as DIM2 etc, explicitly mention the Int type. This is for two reasons: .rstly, it simpli.es \nthe transition to using the Ix class if that is desired; and secondly, in Section 4.4 we discuss more \nelaborate shape constraints that require an explicit index type. The extent of an array is a value of \nthe shape type: extent :: Array sh e -> sh The corresponding Haskell 98 function, bounds, returns an \nupper and lower bound, whereas extent returns only the upper bound. Repa uses zero-indexed arrays only, \nso the lower bound is always zero. For example, the extent (Z:.4:.5) characterises a 4 \u00d7 5 array of rank \ntwo containing 20 elements. The extent along each axis must be at least one. The shape type of an array \nalso types its indices, which range between zero and one less than the extent along the same axis. In \nother words, given an array with shape (Z:.n1:.\u00b7\u00b7\u00b7 :.nm), its index range is from (Z:.0:.\u00b7\u00b7\u00b7 :.0) to \n(Z:.n1 -1:.\u00b7\u00b7\u00b7 :.nm - 1). As indicated in Figure 3, the methods of the Shape type class determine properties \nof shapes and indices, very like Haskell s Ix class. These methods are used to allocate arrays, index \ninto their row-major in-memory representations, to traverse index spaces, and are entirely as expected, \nso we omit the details. 4.2 Shape polymorphism We call functions that operate on a variety of shapes \nshape poly\u00admorphic. Some such functions work on arrays of any shape at all. For example, here is the \ntype of map: map :: (Shape sh, Elt a, Elt b) =>(a-> b) -> Array sh a -> Array sh b The function map \napplies its functional argument to all elements of an array without any concern for the shape of the \narray. The type class constraint Shape sh merely asserts that the type variable sh ought to be a shape. \nIt does not constrain the shape of that shape in any way.  4.3 At-least constraints and rank generalisation \nWith indices as snoc lists, we can impose a lower bound on the rank of an array by .xing a speci.c number \nof lower dimensions, but keeping the tail of the resulting snoc list variable. For example, here is the \ntype of sum:  sum :: (Shape sh, Num e, Elt e) => Array (sh:.Int) e -> Array sh e This says that sum \ntakes an array of any rank n = 1 and returns an array of rank n - 1. For a rank-1 array (a vector), sum \nadds up the vector to return a scalar. But what about a rank-2 array? In this case, sum adds up all the \nrows of the matrix in parallel, returning a vector of the sums. Similarly, given a three-dimensional \narray sum adds up each row of the array in parallel, returning a two-dimensional array of sums. Functions \nlike sum impose a lower bound on the rank of an array. We call such constraints shape polymorphic at-least \ncon\u00adstraints. Every shape-polymorphic function with an at-least con\u00adstraint is implicitly also a data-parallel \nmap over the unspeci.ed dimensions. This is a major source of parallelism in Repa. We call the process \nof generalising the code de.ned for the minimum rank to higher ranks rank generalisation. The function \nsum only applies to the rightmost index of an array. What if we want to reduce the array across a different \ndimension? In that case we simply perform an index permutation, which is guaranteed cheap, to bring the \ndesired dimension to the rightmost position: sum2 :: (Shape sh, Elt e, Num e) => Array (sh:.Int:.Int) \ne -> Array (sh:.Int) e sum2 a = sum (backpermute new_extent swap2 a) where new_extent = swap2 (extent \na) swap2 (is :.i2 :.i1) = is :.i1 :.i2 In our examples so far, we have sometimes returned arrays of a \ndif\u00adferent rank than the input, but their extent in any one dimension has always been unchanged. However, \nshape-polymorphic func\u00adtions can also change the extent: selEven :: (Shape sh, Elt e) => Array (sh:.Int) \ne -> Array (sh:.Int) e selEven arr = backpermute new_extent expand arr where (ns :.n) = extent arr new_extent \n= ns :.(n div 2) expand (is :.i) = is :.(i * 2) As we can see from the calculation of new_extent, the \narray re\u00adturned by selEven is half as big as the input array, in the rightmost dimension. The index calculation \ngoes in the opposite direction, selecting every alternate element from the input array. Note carefully \nthat the extent of the new array is calculated from the extent of the old array, but not from the data \nin the array. This guarantees we can do rank generalisation and still have a rectangular array. To see \nthe difference, consider: filter :: Elt e => (e -> Bool) -> Array DIM1 e -> Array DIM1 e The filter \nfunction is not, and cannot be, shape-polymorphic. If we .lter each row of a matrix based on the element \nvalues, then each new row may have a different length. This gives no guar\u00adantee that the resulting matrix \nis rectangular. In contrast, we have carefully chosen our shape-polymorphic primitives to guarantee the \nrectangularity of the output.  4.4 Slices and slice constraints Shape types characterise a single shape. \nHowever, some collective array operations require a relationship between pairs of shapes. One such operation \nis replicate, which we used in mmMult. The data All = All data Any sh = Any type family FullShape ss \ntype instance FullShape Z = Z ... FullShape (Any sh) = sh ... FullShape (sl :. Int) = FullShape sl :. \nInt ... FullShape (sl :. All) = FullShape sl :. Int type family SliceShape ss type instance SliceShape \nZ = Z ... SliceShape (Any sh) = sh ... SliceShape (sl :. Int) = SliceShape sl ... SliceShape (sl :. All) \n= SliceShape sl :. Int class Slice ss where sliceOfFull:: ss -> FullShape ss -> SliceShape ss fullOfSlice:: \nss -> SliceShape ss -> FullShape ss instance Slice Z where ... instance Slice (Any sh) where ... instance \nSlice sl => Slice (sl :. Int) where sliceOfFull (fsl :. _) (ssl :. _) = sliceOfFull fsl ssl fullOfSlice \n(fsl :. n) ssl = fullOfSlice fsl ssl :. n instance Slice sl => Slice (sl :. All) where sliceOfFull (fsl \n:. All) (ssl :. s) = sliceOfFull fsl ssl :. s fullOfSlice (fsl :. All) (ssl :. s) = fullOfSlice fsl ssl \n:. s replicate :: ( Slice sl, Elt e , Shape (FullShape sl) , Shape (SliceShape sl)) => sl -> Array (SliceShape \nsl) e -> Array (FullShape sl) e replicate sl arr = backpermute (fullOfSlice sl (extent arr)) (sliceOfFull \nsl) arr slice :: ( Slice sl, Elt e , Shape (FullShape sl) , Shape (SliceShape sl)) => Array (FullShape \nsl) e -> sl -> Array (SliceShape sl) e slice arr sl = backpermute (sliceOfFull sl (extent arr)) (fullOfSlice \nsl) arr Figure 4. De.nition of slices function replicate takes an array of arbitrary rank and replicates \nit along one or more additional dimensions. Note that we cannot uniquely determine the behaviour of replicate \nfrom the shape of the original and resulting arrays alone. For example, suppose that we want to expand \na rank-2 array into a rank-3 array. There are three ways of doing this, depending on which dimension \nof the resulting array is to be duplicated. Indeed, the two calls to replicate in mmMult performed replication \nalong two different dimensions, corresponding to different sides of the cuboid in Fig\u00adure 2.  It should \nbe clear that replicate needs an additional argument, a slice speci.er, that expresses exactly how the \nshape of the result array depends on the shape of the argument array. A slice speci.er has the same format \nas an array index, but some index positions may use the value All instead of a numeric index. data All \n= All In mmMult, we use replicate (Z:.All:.colsB:.All) arr to indicate that we replicate arr across the \nsecond innermost axis, colsB times. We use replicate (Z:.rowsA:.All:.All) trr to specify that we replicate \ntrr across the outermost axis, rowsA times. The type of the slice speci.er (Z :.All :.colsB :.All) is \n(Z :.All :.Int :.All). This type is suf.ciently expressive to determine the shape of both the original \narray, before it gets replicated, and of the replicated array. More precisely, both of these types are \na function of the slice speci.er type. In fact, we derive these shapes using associated type families, \na recent extension to the Haskell type system [3, 19], using the de.nition for the Slice type class shown \nin Figure 4. A function closely related to replicate is slice, which ex\u00adtracts a slice along multiple \naxes of an array. The full types of replicate and slice appear in Figure 4. We chose their argu\u00adment \norder to match that used for lists: replicate is a generali\u00adsation of Data.List.replicate, while slice \nis a generalisation of Data.List.(!!). Finally, to enable rank generalisation for replicate and slice, \nwe add a last slice speci.er, namely Any, which is also de.ned in Figure 4. It is used in the tail position \nof a slice, just like Z, but gives a shape variable for rank generalisation. With its aid we can write \nrepN which replicates an arbitrary array n times, with the replication being on the rightmost dimension \nof the result array: repN :: Int -> Array sh e -> Array (sh:.Int) e repN n a = replicate (Any:.n) a 5. \nRectangular arrays, purely functional As mentioned in Section 3, the type class Elt determines the set \nof types that can be used as array elements. We adopt Elt from the library of unboxed one-dimensional \narrays in Data Parallel Haskell. With this library, array elements can be of the basic numeric types, \nBool, and pairs formed from the strict pair constructor: dataa :*:b=!a:*:!b We have also extended this \nto support index types, formed from Z and (:.), as array elements. Although it would be straightforward \nto allow other product and enumeration types as well, support for general sum types appears impractical \nin a framework based on regular arrays. Adding this would require irregular arrays and nested data parallelism \n[16]. Table 1 summarises the central functions of our library Repa. They are grouped according to the \nstructure of the implemented array operations. We discuss the groups and their members in the following \nsections. 5.1 Structure-preserving operations The simplest group of array operations are those that apply \na trans\u00adformation on individual elements without changing the shape, array size, or order of the elements. \nWe have the plain map function, zip for element-wise pairing, and a family of zipWith functions that \napply workers of different arity over multiple arrays in lockstep. In the case of zip and zipWith, we \ndetermine the shape value of the result by intersecting the shapes of the arguments that is, we take \nthe minimum extent along every axis. This behaviour is the same as Haskell s zip functions when applied \nto lists. The function map is implemented as follows: map ::(a-> b)->Arraysha->Array shb map f arr = \ntraverse arr id (f .) The various zip functions are implemented in a similar manner, although they also \nuse a method of the Shape type class to compute the intersection shape of the arguments. 5.2 Reductions \nOur library, Repa, provides two kinds of reductions: (1) generic reductions, such as foldl, and (2) specialised \nreductions, such as sum. In a purely sequential implementation, the latter would be implemented in terms \nof the former. However, in the parallel case we must be careful. Reductions of an n element array can \nbe computed with par\u00adallel tree reduction, providing log n asymptotic step complexity in the ideal case, \nbut only if the reduction operator is associative. Un\u00adfortunately, Haskell s type system does not provide \na way to ex\u00adpress this side condition on the .rst argument of foldl. Hence, the generic reduction functions \nmust retain their sequential seman\u00adtics to remain deterministic. In contrast, for specialised reductions \nsuch as sum, when we know that the operators they use meet the associativity requirement, we can use \nparallel tree reduction. As outlined in Section 4.3, all reduction functions are de.ned with a shape \npolymorphic at-least constraint and admit rank gener\u00adalisation. Therefore, even generic reductions, with \ntheir sequential semantics, are highly parallel if used with rank generalisation. Rank generalisation \nalso affects specialised reductions, as they can be implemented in one of the following two ways. Firstly, \nif we want to maximise parallelism, then we can use a segmented tree re\u00adduction that conceptually performs \nmultiple parallel tree reductions concurrently. Alternatively, we can simply use the same scheme as for \ngeneral reductions, and perform all rank one reductions in par\u00adallel. We follow the latter approach and \nsacri.ce some parallelism, as tree reductions come with some sequential overhead. In summary, when applied \nto an array of rank one, generic re\u00adductions (foldl etc.) execute purely sequentially with an asymp\u00adtotic \nstep complexity of n, whereas specialised reductions (sum etc.) execute in parallel using a tree reduction \nwith an asymptotic step complexity of log n. In contrast, when applied to an array of rank strictly greater \nthan one, both generic and specialised reduc\u00adtions use rank generalisation to execute many sequential \nreductions on one-dimensional subarrays concurrently.  5.3 Index space transformations The structure-preserving \noperations and the reductions transform array elements, whereas index space transformations only alter \nthe index at which an element is placed that is, they rearrange and possibly drop elements. A prime \nexample of this group of operations is reshape, which imposes a new shape on the elements of an array. \nA precondition of reshape is that the size of the extent of the old and new array is the same, meaning \nthat the number of elements stays the same: reshape :: Shape sh => sh -> Array sh e -> Array sh e reshape \nsh (Manifest sh ua) = assert (size sh == size sh ) $ Manifest sh ua reshape sh (Delayed sh f) = assert \n(size sh == size sh ) $ Delayed sh (f . fromIndex sh . toIndex sh )  Structure-preserving operations \nmap :: (Shape sh, Elt a, Elt b) Apply function to every array element. =>(a-> b)->Arraysha ->Array shb \nzip :: (Shape sh, Elt a, Elt b) Elementwise pairing. => Array sh a -> Array sh b -> Array sh (a :*: b) \nzipWith :: (Shape sh, Elt a, Elt b, Elt c) Apply a function elementwise to two arrays. =>(a-> b->c)->Array \nsha ->Array shb->Arraysh c (the resulting shape is the intersection) (Other map-like operations: zipWith3, \nzipWith4, and so on) Reductions foldl :: (Shape sh, Elt a, Elt b) Left fold. => (a -> b -> a) -> a -> \nArray (sh:.Int) b -> Array sh a (Other reduction schemes: foldr, foldl1, foldr1, scanl, scanr, scanl1 \n&#38; scanr1) sum :: (Shape sh, Elt e, Num e) => Array (sh:.Int) e -> Array sh a Sum an array along its \ninnermost axis. (Other speci.c reductions: product, maximum, minimum, and &#38; or) Index space transformations \nreshape :: (Shape sh, Shape sh , Elt e) => sh -> Array sh e -> Array sh e Impose a new shape on the same \nelements. replicate :: (Slice sl, Shape (FullShape sl), Shape (SliceShape sl)) Extend an array along \nnew dimensions. => sl -> Array (SliceShape sl) e -> Array (FullShape sl) e slice :: (Slice sl, Shape \n(FullShape sl), Shape (SliceShape sl)) Extract a subarray according to a slice => Array (FullShape sl) \ne -> sl -> Array (SliceShape sl) e speci.cation. (+:+) :: Shape sh => Array sh e -> Array sh e -> Array \nsh e Append a second array to the .rst. backpermute :: (Shape sh, Shape sh , Elt e) Backwards permutation. \n=> sh -> (sh -> sh) -> Array sh e -> Array sh e backpermuteDft :: (Shape sh, Shape sh , Elt e) Default \nbackwards permutation. => Array sh e -> (sh -> Maybe sh) -> Array sh e -> Array sh e unit ::Elte=>e->ArrayZe \nWrap a scalar into a singleton array. (!:) :: (Shape sh, Elt e) => Array sh e -> sh -> e Extract an element \nat a given index. General traversal traverse :: (Shape sh, Shape sh , Elt e) Unstructured traversal. \n=> Array sh e -> (sh -> sh ) -> ((sh -> e) -> sh -> e ) -> Array sh e force :: (Shape sh, Elt e) => Array \nsh e -> Array sh e Force a delayed array into manifest form. extent :: Array sh e -> sh Obtain size in \nall dimensions of an array. Table 1. Summary of array operations The functions index and fromIndex are \nmethods of the class Shape from Figure 3. The functions replicate and slice were already discussed in \nSection 4.4, and unit and (!:) are de.ned as follows: unit::e->Array Ze unit = Delayed Z . const (!:) \n:: (Shape sh, Elt e) => Array sh e -> sh -> e arr !: ix = snd (delay arr) ix A simple operator to rearrange \nelements is the function (+:+); it appends its second argument to the .rst and can be implemented with \ntraverse by adjusting shapes and indexing. In contrast, general shuf.e operations, such as backwards \nper\u00admutation, require the detailed mapping of target to source indices. We have seen this in the example \ntranspose2D in Section 2. An\u00adother example is the following function that extracts the diagonal of a \nsquare matrix. diagonal :: Elt e => Array DIM2 e -> Array DIM1 e diagonal arr = assert (width == height) \n$ backpermute width (\\x -> (x, x)) arr where _ :. height :. width = extent arr Code that uses backpermute \nappears more like element-based array processing. However, it is still a collective operation with a \nclear parallel interpretation. Backwards permutation is de.ned in terms of the general traverse as follows: \nbackpermute :: sh -> (sh -> sh) -> Array sh e -> Array sh e backpermute sh perm arr = traverse arr (const \nsh) (. perm) The variant backpermuteDft, known as default backwards per\u00admutation, operates in a similar \nmanner, except that the target index is partial. When the target index maps to Nothing, the corresponding \nelement from the default array is used. Overall, backpermuteDft can be interpreted as a means to bulk \nupdate the contents of an array. As we are operating on purely functional, immutable arrays, the original \narray is still available and the re\u00adpeated use of backpermuteDft is only ef.cient if large part of the \narray are updated on each use.  5.4 General traversal The most general form of array traversal is traverse, \nwhich sup\u00adports an arbitrary change of shape and array contents. Nevertheless, it is still represented \nas a delayed computation as detailed in Sec\u00adtion 3.3. Although for ef.ciency reasons it is better to \nuse speci.c functions such as map or backpermute, it is always possible to fall back on traverse if a \ncustom computational structure is required. For example, traverse can be used to implement stencil-based \nrelaxation methods, such as the following update function to solve the Laplace equation in a two dimensional \ngrid [14]: ' u(i, j)=(u(i - 1,j)+ u(i +1,j)+ u(i, j - 1) + u(i, j + 1))/4 To implement this stencil, \nwe use traverse as follows: stencil :: Array DIM2 Double -> Array DIM2 Double stencil arr = traverse \narr id update where _ :. height :. width = extent arr update get d@(sh :. i :. j) = if isBoundary i j \nthen get d else (get (sh :. (i-1) :. j) + get (sh :. i :. (j-1)) + get (sh :. (i+1) :. j) + get (sh \n:. i :. (j+1))) / 4  isBoundary i j = (i==0)||(i>=width -1) ||(j== 0)||(j >=height-1) As the shape \nof the result array is the same as the input, the second argument to traverse is id. The third argument \nis the update function that implements the stencil, while taking the grid boundary into account. The \nfunction get, passed as the .rst argument to update, is the lookup function for the input array. To solve \nthe Laplace equation we would set boundary condi\u00adtions along the edges of the grid and then iterate stencil \nuntil the inner elements converge to their .nal values. However, for bench\u00admarking purposes we simply \niterate it a .xed number of times: laplace :: Int -> Array DIM2 Double -> Array DIM2 Double laplace steps \narr = go steps arr where go s arr |s==0 =arr | otherwise = go (s-1) (force $ stencil arr) The use of \nforce after each recursion is important, as it ensures that all updates are applied and that we produce \na manifest array. Without it, we would accumulate a long chain of delayed compu\u00adtations with a rather \nnon-local memory access pattern. In Repa, the function force triggers all computation, and as we will \ndiscuss in Section 7, the size of forced array determines the amount of paral\u00adlelism in an algorithm. \n6. Delayed arrays and loop fusion We motivated the use of delayed arrays in Section 3.2 by the desire \nto avoid super.uous copying of array elements during index space transformation, such as in the de.nition \nof backpermute. However, another major bene.t of delayed arrays is that it gives by\u00addefault automatic \nloop fusion. Recall the implementation of map: map ::(a-> b)->Arraysha->Array shb map f arr = traverse \narr id (f .) Now, imagine evaluating (map f (map g a)). If you consult the de.nition of traverse (Section \n3.3) it should be clear that the two maps simply build a delayed array whose indexing function .rst indexes \na, then applies g, and then applies f. No intermediate arrays are allocated and, in effect, the two loops \nhave been fused. Moreover, this fusion does not require a sophisticated compiler transformation, nor \ndoes it require the two calls of map to be statically juxtaposed; fusion is a property of the data representation. \nGuaranteed, automatic fusion sounds too good to be true and so it is. The trouble is that we cannot \nalways use the delayed representation for arrays. One reason not to delay arrays is data layout, as we \ndiscussed in Section 3.3. Another is parallelism: force triggers data-parallel execution (Section 7). \nBut the most immediately pressing problem with the delayed representation is sharing. Consider the following: \nlet b= mapfa in mmMult b b Every access to an element of b will apply the (arbitrarily-expensive) function \nf to the corresponding element of a. It follows that these arbitrarily-expensive computations will be \ndone at least twice, once for each argument of mmMult, quite contrary to the programmer s intent. Indeed, \nif mmMult itself consumes elements of its arguments in a non-linear way, accessing them more than once, \nthe computa\u00adtion of f will be performed each time. If instead we say: let b = force (map f a) in mmMult \nb b then the now-manifest array b ensures that f is called only once for each element of a. In effect, \na manifest array is simply a memo table for a delayed array. Here is how we see the situation: In most \narray libraries, every array is manifest by default, so that sharing is guaranteed. However, loop fusion \nis dif.cult, and must often be done manually, doing considerable violence to the structure of the program. \n In Repa every array is delayed by default, so that fusion is guaranteed. However, sharing may be lost; \nit can be restored manually by adding calls to force. These calls do not affect the structure of the \nprogram.  By using force, Repa allows the programmer tight control over some crucial aspects of the \nprogram: sharing, data layout, and par\u00adallelism. The cost is, of course, that the programmer must exercise \nthat control to get good performance. Ignoring the issue altogether can be disastrous, because it can \nlead to arbitrary loss of sharing. In further work, beyond the scope of this paper, we are developing \na compromise approach that offers guaranteed sharing with aggres\u00adsive (but not guaranteed) fusion. 7. \nParallelism As described in Section 3.1, all elements of a Repa array are demanded simultaneously. This \nis the source of all parallelism in the library. In particular, an application of the function force \ntriggers the parallel evaluation of a delayed array, producing a manifest one. Assuming that the array \nhas n elements and that we have P parallel processing elements (PEs) available to perform the work, each \nPE is responsible for computing n/P consecutive elements in the row-major layout of the manifest array. \nIn other words, the structure of parallelism is always determined by the layout and partitioning of a \nforced array. The execution strategy is based on gang parallelism and is described in detail in [5]. \n Let us re-consider the function mmMult from Section 3.3 and Figure 2 in this light. We assume that \narr is a manifest array, and know that trr is manifest because of the explicit use of force. The rank-2 \narray produced by the rank-generalised application of sum corresponds to the right face of the cuboid \nfrom Figure 2. Hence, if we force the result of mmMult, the degree of available parallelism is proportional \nto the number of elements of the resulting array 8 in the .gure. As long as the hardware provides a \nsuf.cient number of PEs, each of these elements may be computed in parallel. Each involves the element-wise \nmultiplication of a row from arr with a row from trr and the summation of these products. If the hardware \nprovides fewer PEs, which is usually the case, the evaluation is evenly distributed over the available \nPEs. Let s now turn to a more sophisticated parallel algorithm, the three-dimensional fast Fourier transform \n(FFT). Three-dimensional FFT works on one axis at a time: we apply the one dimensional FFT to all vectors \nalong one axis, then the second and then the third. Instead of writing a separate transform for each \ndimension, we implement one-dimensional FFT as a shape polymorphic function that operates on the innermost \naxis. We combine it with a three\u00addimensional rotation, rotate3D, which allows us to cover all three axes \none after another: fft3D :: Array DIM3 Complex --roots of unity -> Array DIM3 Complex --data to transform \n-> Array DIM3 Complex fft3D rofu = fftTrans . fftTrans . fftTrans where fftTrans = rotate3D . fft1D rofu \nThe .rst argument, rofu, is an array of complex roots of unity, which are constants that we wish to avoid \nrecomputing for each call. The second is the three-dimensional array to transform, and we require both \narrays to have the same shape. We also require each dimension to have a size which is a power of 2. If \nthe result of fft3D is forced, evaluation by P PEs is again on P consecutive segments of length n 3/P \nof the row-major lay\u00adout of the transformed cube, where n is the side length of the cube. However, the \nwork that needs to be performed for each of the el\u00adements is harder to characterise than for mmMult, \nas the computa\u00adtions of the individual elements of the result are not independent and as fft1D uses force \ninternally. Three-dimensional rotation is easily de.ned based on the func\u00adtion backpermute which we discussed \npreviously: rotate3D :: Array DIM3 Complex -> Array DIM3 Complex rotate3D arr = backpermute (Z:.m:.k:.l) \nf where (Z:.k:.l:.m) = extent arr f (Z:.m:.k:.l) = (Z:.k:.l:.m) The one-dimensional fast Fourier transform \nis more involved: it requires us to recursively split the input vector in half and apply the transform \nto the split vectors. To facilitate the splitting, we .rst de.ne a function halve that drops half the \nelements of a vector, where the elements to pick of the original are determined by a selector function \nsel. halve :: (sh:.Int -> sh:.Int) -> Array (sh:.Int) Complex -> Array (sh:.Int) Complex halve sel arr \n= backpermute (sh :. n div 2) sel arr where sh:.n = extent arr By virtue of rank generalisation, this \nshape polymorphic function will split all rows of a three-dimensional cube at once and in the same manner. \nThe following two convenience functions use halve to extract all elements in even and odd positions, \nrespectively. evenHalf, oddHalf :: Array (sh:.Int) Complex -> Array (sh:.Int) Complex evenHalf = halve \n(\\(ix:.i) -> ix :. 2*i) oddHalf = halve (\\(ix:.i) -> ix :. 2*i+1) Now, the de.nition of the one-dimensional \ntransform is a direct encoding of the Cooley-Tukey algorithm: fft1D :: Array (sh:.Int) Complex -> Array \n(sh:.Int) Complex -> Array (sh:.Int) Complex fft1D rofu v | n > 2 = (left +^ right) :+: (left -^ right) \n| n == 2 = traverse v id swivel where (_:.n)= extent v swivel f (ix:.0) = f (ix:.0) + f (ix:.1) swivel \nf (ix:.1) = f (ix:.0) -f (ix:.1) rofu = evenHalf rofu left = force . fft1D rofu . evenHalf $ v right \n= force . (*^ rofu) . fft1D rofu . oddHalf $ v (+^) = zipWith (+) (-^) = zipWith (-) (*^) = zipWith (*) \n All the index space transformations that are implemented in terms of backpermute, as well as the elementwise \narithmetic op\u00aderations based on zipWith produce delayed arrays. It is only the use of force in the de.nition \nof left and right that triggers the parallel evaluation of subcomputations. In particular, as we force \nthe recursive calls in the de.nition of left and right separately, these calls are performed in sequence. \nThe rank-generalised input vector v is halved with each recursive call, and hence, the amount of available \nparallelism decreases. However, keep in mind that by virtue of rank generalisation we perform the one-dimensional \ntransform in parallel on all vectors of a cuboid. That is, if we apply fft3D to a 64 \u00d7 64 \u00d7 64 cube, \nthen fft1D still operates on 64 * 64 * 2 = 8192 complex numbers in one parallel step at the base case, \nwhere n =2. 8. Benchmarks In this section, we discuss the performance of three programs pre\u00adsented in \nthis paper: matrix-matrix multiplication from Section 3.3, the Laplace solver from Section 5.4 and the \nfast Fourier transform from Section 7. We ran the benchmarks on two different machines: a 2x Quad-core \n3GHz Xeon server and  a 1.4GHz UltraSPARC T2.  The .rst machine is a typical x86-based server with \ngood single\u00adcore performance but frequent bandwidth problems in memory\u00adintensive applications. The bus \narchitecture directly affects the scal\u00adability of some of our benchmarks, namely, the Laplace solver, \nFigure 5. Performance on the Xeon  Repa GCC 1 thread fastest 4.2.1 parallel Matrix mult 1024\u00d71024 3.8s \n4.6s 0.64s Laplace 300\u00d7300 0.70s 1.7s 0.68s FFT 128\u00d7128\u00d7128 0.24s 8.8s 2.0s Repa GCC 1 thread fastest \n4.1.2 parallel Matrix mult 1024\u00d71024 53s 92s 2.4s Laplace 300\u00d7300 6.5s 32s 3.8s FFT 128\u00d7128\u00d7128 2.4s \n98s 7.7s Figure 6. Performance on the SPARC which cannot utilise multiple cores well due to bandwidth \nlimi\u00adtations. The SPARC-based machine is more interesting. The T2 proces\u00adsor has 8 cores and supports \nup to 8 hardware threads per core. This allows it to effectively hide memory latency in massively multi\u00adthreaded \nprograms. Thus, despite a signi.cantly worse single-core performance than the Xeon it exhibits much better \nscalability which is clearly visible in our benchmarks. 8.1 Absolute performance Before discussing the \nparallel behaviour of our benchmarks, let us investigate how Repa programs compare to hand-written C \ncode when executed with only one thread. The C matrix-matrix multipli\u00adcation and Laplace solver are straightforwardly \nwritten programs, while the FFT uses FFTW 3.2.2 [8] in estimate mode. Figure 5 shows the single threaded \nresults together with the fastest running times obtained through parallel execution. For ma\u00adtrix multiplication \nand Laplace on the Xenon, Repa is slower than C when executed sequentially, but not by much. FFTW uses \na .nely tuned in-place algorithm, which is signi.cantly faster but more complicated than our own direct \nencoding of the Cooley-Tukey al\u00adgorithm. We include the numbers with FFTW for comparative pur\u00adposes, \nbut note that parallelism is no substitute for a more ef.cient algorithm. Compared with the Xenon, the \nresults on the SPARC (Figure 6) are quite different. The SPARC T2 is a throughput machine, de\u00adsigned \nto execute workloads consisting of many concurrent threads. It has half the clock rate of the Xenon, \nand does not exploit in\u00adstruction level parallelism. This shows in the fact that the single threaded \nC programs run about 10x slower than their Xenon coun\u00adterparts. The SPARC T2 also does not perform instruction \nreorder\u00ading or use speculative execution. GHC does not perform compile time scheduling to account for \nthis, which results in a larger gap be\u00adtween the single threaded C and Repa programs than on the Xenon. \nWe have also compared the performance of the Laplace solver to an alternative, purely sequential Haskell \nimplementation based on unboxed, mutable arrays running in the IO Monad (IOUArray). This version was \nabout two times slower than the Repa program, probably due to the overhead introduced by bounds checking, \nwhich is currently not supported by our library. Note, however, that bounds checking is unnecessary for \nmany collective opera\u00adtions such as map and sum, so even after we introduce it in Repa we still expect \nto see better performance than a low-level, imperative implementation based on mutable arrays. Figure \n7. Matrix-matrix multiplication, size 1024x1024.  8.2 Parallel behaviour The parallel performance of \nmatrix multiplication is show in Fig\u00adure 7.3 Each point shows the lowest, average, and highest speedups \nfor ten consecutive runs. Here, we get excellent scalability on both machines. On the Xeon, we achieve \na speedup of 7.2 with 8 threads. On the SPARC, it scales up to 64 threads with a peak speedup of 38. \nFigure 8 shows the relative speedups for the Laplace solver. This program achieves good scalability on \nthe SPARC, reaching a speedup of 8.4 with 14 threads but performs much worse on the Xeon, stagnating \nat a speedup of 2.5. As Laplace is memory bound, we attribute this behaviour to insuf.cient bandwidth \non the Xeon machine. There is also some variation in the speedups from run to run, which is more pronounced \nwhen using speci.c numbers of threads. We attribute this to scheduling effects, in the hardware, OS, \nand GHC runtime system. Finally, the parallel behaviour of the FFT implementation is shown in Figure \n9. This program scales well on both machines, achieving a relative speedup of 4.4 on with 7 threads on \nthe Xeon and 12.7 on 14 threads on the SPARC. Compared to the Laplace solver, this time the scalability \nis much better on the Xenon but practically unchanged on the SPARC. Note that FFT is less mem\u00adory intensive \nthan Laplace. The fact that Laplace with a 300 \u00d7 300 matrix does not scale as well on the Xenon as it \ndoes on the SPARC 3 Yes, those really are the results in the .rst graph of the .gure.  Figure 8. Laplace \nsolver, 1000 iterations. supports our conclusion that this benchmark suffers from lack of memory bandwidth. \nFor Laplace with a 400 \u00d7 400 matrix on the SPARC, we suspect the sharp drop off after 8 threads is due \nto the added threads contending for cache. As written, the implementation from Section 5.4 operates on \na row-by-row basis. We expect that changing to a block-wise algorithm would improve cache-usage and reduce \nthe bandwidth needed. 9. Related Work Array programming is a highly active research area so the amount \nof related work is quite signi.cant. In this section, we have to restrict ourselves to discussing only \na few most closely related approaches. 9.1 Haskell array libraries Haskell 98 already de.nes an array \ntype as part of its prelude which, in fact, even provides a certain degree of shape polymor\u00adphism. These \narrays can be indexed by arbitrary types as long as they are instances of Ix, a type class which plays \na similar role to our Shape. This allows for fully shape-polymorphic functions such as map. However, \nstandard Haskell arrays do not support at-least constraints and rank generalisation which are crucial \nfor imple\u00admenting highly expressive operations such as sum from Section 4.3. This in.exibility precludes \nmany advanced uses of shape polymor\u00adphism described in this paper and makes even unboxed arrays based \non the same interface a bad choice for a parallel implementation. Figure 9. 3D Fast Fourier Transform, \nsize 128x128x128 Partly motivated by the shortcomings of standard arrays, nu\u00admerous Haskell array libraries \nhave been proposed in recent years. These range from highly specialised ones such as ByteString [7] to \nfull-.edged DSLs for programming GPUs [12]. However, these li\u00adbraries do not provide the same degree \nof .exibility and ef.ciency for manipulating regular arrays if they support them at all. Our own work \non Data Parallel Haskell is of particular relevance in this con\u00adtext as the work presented in this paper \nshares many of its ideas and large parts of its implementation with that project. Indeed, Repa can be \nseen as complementary to DPH. Both provide a way of writing high-performance parallel programs but DPH \nsupports irregular, arbitrarily nested parallelism which requires it to sacri.ce perfor\u00admance when it \ncomes to purely regular computations. One of the goals of this paper is to plug that hole. Eventually, \nwe intend to in\u00adtegrate Repa into DPH, providing ef.cient support for both regular and irregular arrays \nin one framework. 9.2 C++ Array Libraries Due to its powerful type system and its wide-spread use in \nhigh\u00adperformance computing, C++ has a signi.cant number of array li\u00adbraries that are both fast and generic. \nIn particular, Blitz++ [23] and Boost.MultiArray [1] feature multidimensional arrays with a re\u00adstricted \nform of shape polymorphism. However, our library is much more .exible in this regard and also has the \nadvantage of a natural parallel implementation which neither of the two C++ libraries pro\u00advide. Moreover, \nthese approaches are inherently imperative while we provide a purely functional interface which allows \nprograms to be written at a higher level of abstraction.  9.3 Array Languages In addition to libraries, \nthere exist a number of special-purpose array programming languages. Of these, Single Assignment C (SAC) \n[18] has exerted the most in.uence on our work and is the closest in spirit as it is purely functional \nand strongly typed. SAC provides many of the same bene.ts as Repa: high-performance arrays with shape \npolymorphism, expressive collective operations and extensive optimisation based on with-loops, a special-purpose \nlanguage construct for creating, traversing and reducing arrays. It also comes with a rich library of \nstandard array and matrix opera\u00adtions which Repa has not yet acquired. However, Repa has the advantage \nof being integrated into a mainstream functional language and not requiring speci.c compiler support. \nThis allows Repa programs to utilise the entire Haskell infrastructure and to drop down to a very low \nlevel of abstraction if required in speci.c cases. This, along with strong typing and purity, are also \nthe advantages Repa has over other array languages such as APL, J and Matlab [2, 9, 21]. Acknowledgements. \nWe are grateful to Arvind for explaining the importance of delaying index space transformations and thank \nSi\u00admon Winwood for comments on a draft. We also thank the anony\u00admous ICFP 10 reviewers for their helpful \nfeedback on the paper. This research was funded in part by the Australian Research Coun\u00adcil under grant \nnumber LP0989507. References [1] The Boost Multidimensional Array Library, April 2010. URL http://www.boost.org/doc/libs/1_42_0/libs/multi_ \narray/doc/user.html. [2] C. Burke. J and APL. Iverson Software Inc., 1996. [3] M. M. T. Chakravarty, \nG. Keller, and S. Peyton Jones. Associated type synonyms. In ICFP 05: Proceedings of the Tenth ACM SIGPLAN \nInternational Conference on Functional Programming, pages 241 253, New York, NY, USA, 2005. ACM Press. \nISBN 1-59593-064-7. doi: http://doi.acm.org/10.1145/1086365.1086397. [4] M. M. T. Chakravarty, G. Keller, \nS. Peyton Jones, and S. Marlow. Associated types with class. In POPL 05: Proceedings of the 32nd ACM \nSIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 1 13. ACM Press, 2005. ISBN 1-58113-830-X. \ndoi: http://doi.acm.org/10.1145/1040305.1040306. [5] M. M. T. Chakravarty, R. Leshchinskiy, S. Peyton \nJones, G. Keller, and S. Marlow. Data Parallel Haskell: a status report. In DAMP 2007: Workshop on Declarative \nAspects of Multicore Programming. ACM Press, 2007. [6] D. Coutts, R. Leshchinskiy, and D. Stewart. Stream \nfusion: From lists to streams to nothing at all. In Proceedings of the ACM SIGPLAN International Conference \non Functional Programming (ICFP 2007). ACM Press, 2007. [7] D. Coutts, D. Stewart, and R. Leshchinskiy. \nRewriting Haskell strings. In Practical Aspects of Declarative Languages 8th International Sym\u00adposium, \nPADL 2007, pages 50 64. Springer-Verlag, Jan. 2007. [8] M. Frigo and S. G. Johnson. The design and implementation \nof FFTW3. Proceedings of the IEEE, 93(2):216 231, 2005. Special issue on Program Generation, Optimization, \nand Platform Adaptation . [9] A. Gilat. MATLAB: An Introduction with Applications 2nd Edition. John Wiley \n&#38; Sons, 2004. ISBN 978-0-471-69420-5. [10] J. H. v. Groningen. The implementation and ef.ciency \nof arrays in Clean 1.1. In W. Kluge, editor, Proceedings of Implementation of Functional Languages, 8th \nInternational Workshop, IFL 96, Selected Papers, number 1268 in LNCS, pages 105 124. Springer-Verlag, \n1997. [11] J. Launchbury and S. Peyton Jones. Lazy functional state threads. In Proceedings of Programming \nLanguage Design and Implementation (PLDI 1994), pages 24 35, New York, NY, USA, 1994. ACM. [12] S. Lee, \nM. M. T. Chakravarty, V. Grover, and G. Keller. GPU kernels as data-parallel array computations in haskell. \nIn EPAHM 2009: Workshop on Exploiting Parallelism using GPUs and other Hardware-Assisted Methods, 2009. \n[13] X. Leroy, D. Doligez, J. Garrigue, D. R\u00b4emy, and J. Vouillon. The Ob\u00adjective Caml system, release \n3.11, documentation and user s manual. Technical report, INRIA, 2008. [14] J. Mathews and K. Fink. Numerical \nMethods using MATLAB, 3rd edition. Prentice Hall, 1999. [15] S. Peyton Jones. Call-pattern specialisation \nfor Haskell programs. In Proceedings of the International Conference on Functional Program\u00adming (ICFP \n2007), pages 327 337, 1997. [16] S. Peyton Jones, R. Leshchinskiy, G. Keller, and M. M. T. Chakravarty. \nHarnessing the multicores: Nested data parallelism in Haskell. In R. Hariharan, M. Mukund, and V. Vinay, \neditors, IARCS Annual Con\u00adference on Foundations of Software Technology and Theoretical Com\u00adputer Science \n(FSTTCS 2008), Dagstuhl, Germany, 2008. Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik, Germany. URL \nhttp: //drops.dagstuhl.de/opus/volltexte/2008/1769. [17] F. A. Rabhi and S. Gorlatch, editors. Patterns \nand Skeletons for Parallel and Distributed Computing. Springer-Verlag, 2003. [18] S.-B. Scholz. Single \nassignment C ef.cient support for high-level array operations in a functional setting. Journal of Functional \nPro\u00adgramming, 13(6):1005 1059, 2003. [19] T. Schrijvers, S. Peyton-Jones, M. M. T. Chakravarty, and M. \nSulz\u00admann. Type checking with open type functions. In Proceedings of ICFP 2008 : The 13th ACM SIGPLAN \nInternational Conference on Functional Programming, pages 51 62. ACM Press, 2008. [20] W. Swierstra and \nT. Altenkirch. Dependent types for distributed arrays. In Trends in Functional Programming, volume 9, \n2008. [21] The International Standards Organisation. Programming Language APL. ISO standard 8485, 1989. \n[22] The OpenMP Architecture Review Board. OpenMP Application Pro\u00adgram Interface, 2008. URL http://www.openmp.org/specs. \n[23] T. L. Veldhuizen. Arrays in Blitz++. In Proceedings of the 2nd In\u00adternational Scienti.c Computing \nin Object Oriented Parallel Environ\u00adments (ISCOPE 98). Springer-Verlag, 1998. ISBN 978-3-540-65387\u00ad 5. \n [24] H. Xi. Dependent ML: an approach to practical programming with dependent types. Journal of Functional \nProgramming, 17(2):215 286, 2007.    \n\t\t\t", "proc_id": "1863543", "abstract": "<p>We present a novel approach to regular, multi-dimensional arrays in Haskell. The main highlights of our approach are that it (1) is purely functional, (2) supports reuse through shape polymorphism, (3) avoids unnecessary intermediate structures rather than relying on subsequent loop fusion, and (4) supports transparent parallelisation.</p> <p>We show how to embed two forms of shape polymorphism into Haskell's type system using type classes and type families. In particular, we discuss the generalisation of regular array transformations to arrays of higher rank, and introduce a type-safe specification of array slices.</p> <p>We discuss the runtime performance of our approach for three standard array algorithms. We achieve absolute performance comparable to handwritten C code. At the same time, our implementation scales well up to 8 processor cores.</p>", "authors": [{"name": "Gabriele Keller", "author_profile_id": "81100011375", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P2338212", "email_address": "", "orcid_id": ""}, {"name": "Manuel M.T. Chakravarty", "author_profile_id": "81408595395", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P2338213", "email_address": "", "orcid_id": ""}, {"name": "Roman Leshchinskiy", "author_profile_id": "81330494188", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P2338214", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd, Cambridge, United Kingdom", "person_id": "P2338215", "email_address": "", "orcid_id": ""}, {"name": "Ben Lippmeier", "author_profile_id": "81488641994", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P2338216", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863582", "year": "2010", "article_id": "1863582", "conference": "ICFP", "title": "Regular, shape-polymorphic, parallel arrays in Haskell", "url": "http://dl.acm.org/citation.cfm?id=1863582"}