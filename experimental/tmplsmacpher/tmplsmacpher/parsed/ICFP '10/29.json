{"article_publication_date": "09-27-2010", "fulltext": "\n Rethinking Supercompilation Neil Mitchell ndmitchell@gmail.com Abstract Supercompilation is a program \noptimisation technique that is par\u00adticularly effective at eliminating unnecessary overheads. We have \ndesigned a new supercompiler, making many novel choices, includ\u00ading different termination criteria and \nhandling of let bindings. The result is a supercompiler that focuses on simplicity, compiles pro\u00adgrams \nquickly and optimises programs well. We have benchmarked our supercompiler, with some programs running \nmore than twice as fast than when compiled with GHC. Categories and Subject Descriptors D.3 [Software]: \nProgram\u00adming Languages General Terms Languages Keywords Haskell, optimisation, supercompilation 1. Introduction \nConsider a program that counts the number of words read from the standard input in Haskell (Peyton Jones \n2003) this can be compactly written as: main = print . length . words =< getContents Reading the program \nright to left, we .rst read the standard input as a string (getContents), then split it in to words (words), \ncount the number of words (length), and print the result (print). An equivalent C program is unlikely \nto use such a high degree of abstraction, and is more likely to get characters and operate on them in \na loop while updating some state. Sadly, such a C program is three times faster, even using the advanced \noptimising compiler GHC (The GHC Team 2009). The abstractions that make the program concise have a signi.cant \nrun\u00adtime cost. In a previous paper (Mitchell and Runciman 2008) we showed how supercompilation can remove \nthese abstractions, to the stage where the Haskell version is faster than the C version (by about 6%). \nIn the Haskell program after optimisation, all the inter\u00admediate lists have been removed, and the length \n. words part of the pipeline is translated into a state machine. One informal description of supercompilation \nis that you sim\u00adply run the program at compile time . This description leads to two questions what happens \nif you are blocked on information only available at runtime, and how do you ensure termination? An\u00adswering \nthese questions provides the design for a supercompiler. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, \nUSA. Copyright c . 2010 ACM 978-1-60558-794-3/10/09. . . $10.00 1.1 Contributions Our primary contribution \nis the design of a new supercompiler (\u00a72). Our supercompiler has many differences from previous supercom\u00adpilers \n(\u00a75.1), including a new core language, a substantially dif\u00adferent treatment of let expressions and entirely \nnew termination criteria. The result is a supercompiler with a number of desirable properties: Simple \nOur supercompiler is designed to be simple. From the descriptions given in this paper a reader should \nbe able to write their own supercompiler. We have written a supercompiler following our design which \nis available online1. Much of the code (for example Figure 2) has been copied verbatim into our implementation. \nThe supercompiler can be implemented in under 300 lines of Haskell. Fast compilation Previous supercompilers \nhave reported compi\u00adlation times of up to .ve minutes for small examples (Mitchell and Runciman 2008). \nOur compilation times are under four sec\u00adonds, and there are many further compile time improvements that \ncould be made (\u00a74.2). Fast runtime We have benchmarked our supercompiler on a range of small examples \n(\u00a74). Some programs optimised with our supercompiler, and then compiled with GHC, are more than twice \nas fast than when compiled with GHC alone. We give examples of how our supercompiler performs (\u00a72.3.2), \nincluding how it subsumes list fusion and specialisation (\u00a73), and what happens when the termination \ncriteria are needed (\u00a72.6.4).  2. Method This section describes our supercompiler. We .rst present \na Core language (\u00a72.1), along with simpli.cation rules (\u00a72.2). We then present the overall algorithm \n(\u00a72.3), which combines the answers to the following questions: How do you evaluate an open term? (\u00a72.4) \n What happens if you can t evaluate an open term further? (\u00a72.5)  How do you know when to stop? (\u00a72.6) \n What happens if you have to stop? (\u00a72.6.3)  Throughout this section we use the following example: \nrootgf x = map g (mapf x) map f [] =[] map f (x : xs)= fx : mapf xs Our supercompiler always optimises \nthe function named root. The root function applies map twice the expression mapf x 1 http://hackage.haskell.org/package/supero \n type Var = String --variable/function names type Con = String --constructor names data Exp = App Var \n[Var] --function application | Con Con [Var] --constructor application | Let [(Var, Exp)] Var --let expression \n| Case Var [(Pat, Exp)] --case expression | Lam Var Exp --lambda expression type Pat = Exp --restricted \nto Con Figure 1. Core Language produces a list that is immediately consumed by map g. A good supercompiler \nshould remove the intermediate list. 2.1 Core Language Our Core language for expressions is given in \nFigure 1, and has much in common with Administrative Normal Form (Flanagan et al. 1993). We make the \nfollowing observations: We require variables in many places that would normally permit expressions, \nincluding let bodies and application. A standard Core language (such as from Tolmach (2001)) can be translated \nto ours by inserting let expressions.  Our let expression is non-recursive bindings within a let expression \nare bound in order. For example, we allow let x = y; y = C in C but not let x = y; y = x in C.  We don \nt have default patterns in case expressions. These can be added without great complexity, but are of \nlittle interest when describing a supercompiler.  We assume programs in our Core language are well-typed \nusing Hindley-Milner, in particular that we never over-apply a constructor or perform case analysis on \na function. While most Haskell programs can be translated to our Core language, the typing restriction \nmeans some features are not supported (GADTs, existential types).  Function application takes a list \nof arguments, rather than just a single argument the reasons are explained in \u00a72.8.1. We use an application \nwith no arguments to represent just a variable.  Variables may be either local (bound in an expression), \nor global (bound in a top-level environment). We require that all global variables occur as the .rst \nargument of App.  When comparing expressions we always normalise local vari\u00adable names and the order \nof let bindings.  We de.ne the arity of a variable to be the number of arguments that need to be applied \nbefore reduction takes place. For our pur\u00adposes, it is important that for a variable with arity n, if \nless than n arguments are applied, no evaluation occurs. We approximate the arity of bound variables \nusing the number of lambda arguments at the root of their expression, for primitives we use a known arity \n(e.g. integer addition has arity 2), and for all other variables we use arity 0. In our example map has \narity 2, root has arity 3, and f, g and x have arity 0. We write expressions using standard Haskell syntax \n(e.g. let for Let, case for Case etc.). Rewriting the map/map example in our Core language gives: root \n= .gfx . let v1 = mapf x v2 = map g v1 in v2 map = .fx . case x of [] . let v1 = [] in v1 y : ys . let \nv1 = fy v2 = mapf y v3 = (:) v1 v2 in v3 Our Core language can be rather verbose, so we sometimes use \na superset of our Core language, assuming the expressions are translated to our Core language when necessary. \nFor example, we might write map as: map = .fx . case x of [] . [] y : ys . fy : mapf ys  2.2 Simpli.ed \nCore We now de.ne a simpli.ed form of our Core language. When work\u00ading with Core expressions we assume \nthey are always simpli.ed, and after constructing new expressions we always simplify them. We require \nall expressions bound in the top-level environment con\u00adsist of a (possibly empty) sequence of lambda \nexpressions, fol\u00adlowed by a let expression. We call the .rst let expression within a top-level de.nition \nthe root let. Within a let we remove any bindings that are not used and ensure all bound variables are \nunique. We also require that all expressions bound at a let must not have the following form: App v \n[], where v is bound at this let we can remove the binding by inlining it.  App v vs, where v is bound \nto a Con the App can be replaced with a Con of higher arity.  App v vs, where v is bound to App w ws \nand the arity of w is higher than the length of ws the App can be replaced with an App with more arguments. \n App v vs, where v is bound to a Lam the App can be replaced with the body of the lambda, with the \n.rst variable substituted.  Case v w, where v is bound to a Con the Case can be replaced with the appropriate \nalternative.  Let bs v the bindings can be lifted into the let, renaming variables if necessary.  \nAs an example, we can simplify the following expression: let v1 = f v2 = Con x v3 = v2 y v4 = let w1 \n= y in v1 w1 v5 = case v3 of Con a b . v4 a in v5 To give: let v4 = fy v5 = v4 x in v5 If the arity of \nf was known to be 2, this would further simplify to: let v5 = fyx in v5 2.2.1 Simpli.er Non-Termination \nSadly, not all expressions have a simpli.ed form. Take the follow\u00ading example:  type Env = Var . Maybe \nExp data Tree = Tree {pre :: Exp, gen :: [Var] . Exp, children :: [Tree]} manager :: Env . [(Var, Exp)] \nmanager env = assign (.atten (optimise env e)) where Just e = env \"root\" optimise :: Env . Exp . Tree \noptimise env = f [] where fhe | terminate (S) he = ghe (stop h e) | otherwise = g (e : h) e (reduce env \ne) ghe (gen, cs)= Tree e gen (map (fh) cs) reduce :: Env . Exp . ([Var] . Exp, [Exp]) reduce env = f \n[] where fhe = case step env e of | terminate (<) he . stop h e ' Just e' . f (e : h) eNothing . split \ne .atten :: Tree . [Tree] .atten = nubBy (.t1 t2 . pre t1 = pre t2) . f [] where f seen t = if pre t \n. seen then [] else t : concatMap (f (pre t : seen)) (children t) assign :: [Tree] . [(Var, Exp)] assign \nts = [(ft, gen t (map f (children t))) | t . ts] where ft = fromJust (lookup (pre t) names) names = zip \n(map pre ts) functionNames Figure 2. The manager function. data U = MkU (U . Bool) e = let f = .x . case \nx of MkU y . yx in f (MkU f) This program encodes recursion via a data type, and any attempt to apply \nall the simpli.cation rules will not terminate. We are aware of two solutions: 1) We could avoid performing \ncase elimination on contravariant data types (Peyton Jones and Marlow 2002); 2) We could avoid simplifying \ncertain expressions, using the size measure from the HOSC supercompiler (Klyuchnikov 2010). We have chosen \nto leave this problem unsolved. The problem only occurs for contrived programs which encode recursion \nvia a data type, and it is a problem shared with GHC, which will also non-terminate when compiling this \nexample. The later stages of our supercompiler do not rely on the simpli.cations having been performed, \nso either solution could be applied in future.  2.3 Manager Our supercompiler is based around a manager, \nthat integrates the answers to the questions of supercompilation. The manager itself has two main purposes: \nto create recursive functions, and to ensure termination (assuming the simpli.er terminates). In our \nexperience the creation of recursive functions is often the most delicate part of a supercompiler, so \nwe deliberately include all the details. The code for our manager is given in Figure 2, making use of \na some auxiliary functions whose types are given in Figure 3. We .rst give an intuition for how the manager \nworks, then describe each part. Our supercompiler takes a source program, and generates a target program. \nFunctions in these programs are distinct target expressions cannot refer to source functions. The source \nand tar\u00ad step :: Env . Exp . Maybe Exp --\u00a72.4 split :: Exp . ([Var] . Exp, [Exp]) --\u00a72.5 type History \n=[Exp] (<), (S) :: Exp . Exp . Bool --\u00a72.6 terminate :: (Exp . Exp . Bool) . History . Exp . Bool --\u00a72.6 \nstop :: History . Exp . ([Var] . Exp, [Var]) --\u00a72.6.3 Figure 3. Auxiliary de.nitions for Figure 2. get \nprogram are equivalent, but hopefully the target program runs faster. We use the type Env to represent \na mapping from source function names to expressions, allowing a result of Nothing to in\u00addicate a primitive \nfunction. The manager .rst builds a tree (the type Tree), where each node has a source expression (pre) \nand an equivalent target expression. The target expression may call target functions, but these functions \ndo not yet have names. Therefore, we store target expressions as a generator that when given the function \nnames produces the target expression (gen), and a list of trees for the functions it calls (children). \nWe then .atten this tree, ensuring identical functions are only included once, and assign names to each \nnode before generating the target program. If a target function is recursive then the initial tree will \nbe in.nite, but the .attened tree will always be .nite due to the termination scheme de.ned in \u00a72.6. \nmanager: This function puts all the parts together. Reading from right to left, we .rst generate a potentially \nin.nite tree by optimis\u00ading the expression bound to the function root, we then .atten the tree to a .nite \nnumber of functions, and .nally assign names to each of the result functions. optimise: This function \nconstructs the tree of result functions. While the tree may be in.nite, we demand that any in.nite path \nfrom the root must encounter the same pre value more than once. We require that for any in.nite sequence \nof distinct expressions h, there must exist an i such that terminate (S)(take i h)(h !! i + 1) returns \nTrue (where (!!) is the Haskell list indexing operator). If we are forced to terminate we call stop, \nwhich splits the expression into several subexpressions. We require that stop h only produces subexpressions \nwhich pass the termination test, so that when f is applied to the subexpressions they all call reduce. \nIf the termination criteria do not force us to stop, then we call reduce to evaluate the expression. \nreduce: This function optimises an expression by repeatedly eval\u00aduating it with calls to step. If we \ncan t evaluate any further we call split. We use a local termination test to ensure the evaluation ter\u00adminates. \nWe require that for any in.nite sequence of expressions h, there must exist an i such that terminate \n(<)(take i h)(h !! i + 1) returns True. Note that this termination criteria is more restrictive than \nthat used by optimise. .atten: This function takes a tree and extracts a .nite number of functions from \nit, assuming the termination restrictions given in optimise. Our .atten function will only keep one tree \nassociated with each source expression. These trees may have different tar\u00adget expressions if one resulted \nfrom a call to stop, while another resulted from a call to reduce but all are semantically equivalent. \nassign: This function assigns names to each target function, and constructs the target expressions by \ncalling gen. We assume the function functionNames returns an in.nite list of function names.  2.3.1 \nNotation Values of type ([Var] . Exp, [Exp]) can be described by taking an expression and removing some \nsubexpressions (indicated by [ ] ). The .rst component is a function to insert variables where subex\u00adpressions \nwere removed, and the second component is a list of the removed subexpressions. Before removing each \nsubexpression, we insert an applied lambda for all variables bound in the expression but free in the \nsubexpression. As an example: .gf . map g ([[mapf xs]]) We .rst insert a lambda for the variable f: .gf \n. map g (([[.f . mapf xs]]) f) We then remove the subexpression. The .rst component of the re\u00adsult is \na function that when given [\"name\" ] returns the expression: .gf . map g (name f) And the second component \nis the singleton list containing the expression: .f . mapf xs 2.3.2 Example Revisiting our initial example, \nmanager .rst calls optimise with: .gfx . map g (mapf x) The termination history is empty, so we call \nreduce, which calls step repeatedly until we reach the expression: .gfx . let v = case w of [] . [] y \n: ys . gy : map g ys w = case x of [] . [] z : zs . fz : mapf zs in v The step function now returns \nNothing, since we cannot eval\u00aduate further without the result of x. We therefore call split, which results \nin (using the notation from \u00a72.3.1): .gfx . case x of [] . [ let v = ...; w = ...; x = [] in v] z : \nzs . [ let v = ...; w = ...; x = z : zs in v] Looking at the .rst child expression, where x = [], the \nsimpli\u00ad.cation rules from \u00a72.2 immediately produce [] as the result. The second child starts as: .gf \nzzs . let x = z : zs w = case x of [] . [ ]; z : zs . fz : mapf zs v = case w of [] . [ ]; y : ys . gy \n: map g ys in v Which simpli.es to: .gf zzs . let y = fz ys = mapf zs q = gy qs = map g ys v = q : qs \nin v Calling step produces Nothing, as the root of this expression is a constructor (:) which can t be \nevaluated. We therefore call split which results in: force :: Exp . Maybe Var force (Case v )= Just v \nforce (App v )= Just v force = Nothing next :: Exp . Maybe Var next (Lam x)= next x next (Let bind v)= \nlast (Nothing : fv) where fv = case lookup v bind of Nothing . [] Just e . Just v : maybe [] f (force \ne) Figure 4. Function to determine the next evaluated binding. .gf zzs . let q =[ g (fz)]] qs =[ map \ng (mapf zs)]] v = q : qs in v When optimising g (fz) we get no optimisation, as there is no available \ninformation. To optimise map g (mapf zs) we repeat the exact same steps we have already done. However, \nthe .atten function will spot that both Tree nodes have the same pre expression (modulo variable renaming), \nand reduce them to one node, creating a recursive function. We then assign names using assign. For the \npurposes of display (not optimisation), we apply a number of simpli.cations given in \u00a72.7. The end result \nis: rootgf x = case x of [] . [] z : zs . g (fz): rootgf zs The .nal version has removed the intermediate \nlist, with no additional knowledge about the map function or its fusion rules.  2.4 Evaluation Evaluation \nis based around the step function. Given an expression, step either replaces a variable with its associated \nvalue from the environment and returns Just, or if no suitable variable is found returns Nothing. We \nalways replace the variable that would be evaluated next during normal evaluation. To determine which \nvariable would be evaluated next, we de.ne the functions force and next in Figure 4. The function force \ndeter\u00admines which variable will be evaluated next given an expression either a case scrutinee or an applied \nvariable. The function next determines which variable bound at the root let will be evaluated next, by \nfollowing the forced variables of the let bindings. Looking at the original example: .gfx . let v1 = \nmapf x v2 = map g v1 in v2 The function next returns Just v2. Calling force on the expres\u00adsion mapg v1 \nreturns map, but since map is not bound at the root let we go no further. Therefore, to evaluate this \nexpression we will start by evaluating v2, and thus map. To perform an evaluation step we insert a fresh \nvariable w1 bound to the body of map, and replace the map variable in v2 with w1. This transformation \nresults in: .gfx . let v1 = mapf x w1 = .fx . case x of [ ] . [ ] y : ys . f y : map f ys v2 = w1 g v1 \nin v2  Simpli.cation immediately removes the lambda at w1, replacing v2 with a case expression on v1. \nMore generally, we match any expression with the following pattern: .free . let s = fw1 wn v1 = e1 vn \n= en in v where Just e ' = env f We use s to represent the next binding to be evaluated, as re\u00adturned \nby next. We allow any other variables v1 .. vn to be present, bound to expressions e1 .. en. Given this \ncon.guration we can rewrite to: .free . let s ' = e ' s = s ' w1 wn v1 = e1 vn = en in v As always, after \ngenerating a new expression we immediately apply the simpli.cation rules (\u00a72.2).  2.5 Evaluation Splitting \nIf evaluation cannot proceed, we split to produce a target expres\u00adsion, and a list of child expressions \nfor further optimisation. When splitting an expression there are three concerns: Permit further optimisation: \nWhen we split, the current expres\u00adsion cannot be evaluated using the rules described in \u00a72.4. We therefore \naim to place the construct blocking evaluation in the target expression, allowing the child expressions \nto be optimised further. No unbounded loss of sharing: An expensive variable binding cannot be duplicated \nin a way that causes it to be evaluated multiple times at runtime. The target program cannot remove sharing \npresent in the source program, or it would run slower. Keep bindings together: If we split variables \nbound at the same let expression into separate child expressions, we reduce the poten\u00adtial for optimisation. \nIf the expression associated with a variable is not available when evaluating, the evaluation will be \nforced to stop sooner. We aim to make child expressions as large as possible, but without losing sharing. \nWe split in one of three different ways, depending on the type of the next expression to be evaluated \n(as described in \u00a72.4). We now describe each of the three ways to split, in each case we start with an \nexample, then de.ne the general rule. We use the [ ] notation described in \u00a72.3.1. 2.5.1 Case Expression \nIf the next expression is a case expression then we make the target a similar case expression, and under \neach alternative we create a child expression with the case scrutinee bound to the appropriate pattern. \nFor example, given: .x . let v = case x of [] . [] y : ys . add y ys in v We split to produce: .x . \ncase x of [] . [ let x = [] v = case x of [] . [ ]; y : ys . add y ys in v] y : ys . [ let x = y : ys \nv = case x of [] . [ ]; y : ys . add y ys in v] Looking more closely at the second child, we start with \nthe expression: .y ys . let x = y : ys v = case x of [] . [ ]; y : ys . add y ys in v This expression \nimmediately simpli.es to: .y ys . let v = add y ys in v More generally, if s is the next expression to \nevaluate: .free . let s = case x of p1 . e1' ; pm . e ' m v1 = e1 vn = en in v After split it becomes: \n.free . case x of p1 . [ let x = p1 s = e ' 1 v1 = e1 vn = en in v] pm . [ let x = pm s = em ' v1 = \ne1 vn = en in v]  2.5.2 Lambda If the next binding to be evaluated is a lambda, then we place a lambda \nin the target program. The key point when splitting a lambda is that we do not reduce sharing. Consider \nthe following example: .x . let v1 = fx v2 = expensive v1 s = .y . add v2 y in s The add function takes \ntwo arguments, but only has one so far. We cannot move the argument y upwards to form .xy . ..., as this \naction potentially duplicates the expensive computation of v2. Instead, we create child expressions for \nevery variable binding, and for the body of the lambda: .x . let v1 =[ fx] v2 =[ expensive v1] s = .y \n. [ add v2 y] in s Unfortunately, we have now split the bindings for v1 and v2 apart, when there is \nno real need. We therefore move binding v1 under v2, because it is only referred to by v2, to give: .x \n. let v2 =[ let v1 = fx in expensive v1] s = .y . [ add v2 y] in s We will now optimise the body of v2, \nand the body of the lambda, which will be able to evaluate add. More generally, given:  .free . let \ns = .x . e ' v1 = e1 vn = en in v We rewrite: .free . let s = .x . [ e ' ] v1 =[ e1] vn =[ en] in v \nWe then repeatedly move any binding vi under vj if either: 1) vi is only used within the body of vj; \nor 2) the expression bound to vi is cheap. We de.ne an expression to be cheap if it is a constructor, \nor an application to a variable v with fewer arguments than the arity of v (a partial application). The \nintention of moving bindings is to increase sharing, which can be done provided we don t duplicate work \n(condition 1), or if the work duplicated is bounded (condition 2).  2.5.3 Anything Else The .nal rule \napplies if the next expression is not a case expression or a lambda, including a constructor, a variable, \nand an application of a variable not bound in the environment. We do not deal with variables bound in \nthe environment, as these are handled by step. Given the example: .xy . let v1 = expensive x v2 = v1 \nx v3 = Con v2 yv2 in v3 We turn each binding into a child, apart from the next binding to be evaluated: \n.xy . let v1 =[ expensive x] v2 =[ v1 x] v3 = Con v2 yv2 in v3 We then perform the same sharing transformation \nas for lambda expressions, noting that v1 is only used within v2, to give: .xy . let v2 =[ let v1 = expensive \nx in v1 x] v3 = Con v2 xv2 in v3 More generally, given an expression: .free . let s = e ' v1 = e1 vn \n= en in v We rewrite to: .free . let s = e ' v1 =[ e1] vn =[ en] in v We then repeatedly move any binding \nvi under vj according to the criteria given in \u00a72.5.2.  2.6 Termination The termination rule is responsible \nfor ensuring that whenever we proceed along a list of expressions we eventually stop. The intuition is \nthat each expression has a set of bindings at the root let, and each of these bindings has a name indicating \nwhere it came from in the source program. Compared to all earlier expressions in a list, each root let \nmust contain either different names, or fewer names. In this section we .rst describe the terminate, \n< and S func\u00adtions from a mathematical perspective, then how we apply these functions to expressions. \nFinally, we show an example of how these rules ensure termination. 2.6.1 Termination Rule Our termination \nrules are de.ned over bags (also known as mul\u00adtisets) of values drawn from a .nite alphabet S. A bag \nof values is unordered, but may contain elements more than once. We de.ne our rules as: x < y = set(x) \n= set(y) . #x< #y x S y = x = y . x < y We use set(x) to transform a bag to a set, and # as the cardi\u00adnality \noperator to take the number of elements in a bag. A sequence x1 ...xn is well-formed under < if for all \nindices i<j . xj <xi (and respectively for S). The following sequences are well-formed under both S and \n<: [a, aaaaab, aaabb, b] [abc, ab, accc, a] [aaaaabbb, aaab, aab] The following sequences are well-formed \nunder S, but not under <: [aaa, aaa] [aabb, ab, ab] The following sequences are not well-formed under \nS or <: [abc, abcc] [aa, aaa] We de.ne the terminate function referred to in Figure 3 as: terminate :: \n(Exp . Exp . Bool) . History . Exp . Bool terminate (<) he = not (all (e<) h) The terminate function \nreturns False if given a well-formed sequence (h), adding the expression e will keep the sequence well\u00adformed. \nLemma: Any well-formed sequence under < is .nite Given a .nite alphabet S, any well-formed sequence under \n< is .\u00adnite. Consider a well-formed sequence x1 .... We can partition this sequence into at most 2S subsequences \nusing set equality. Consider any subsequence y1 .... For any two elements in this subsequence, set(yi) \n= set(yj ) will be false, due to the partitioning. There\u00adfore, for the sequence to be well-formed, i<j \n. #yj < #yi. Therefore there can be at most #y1 +1 elements in any particular subsequence. Combined with \na .nite number of subsequences, we conclude that any well-formed sequence is .nite. Lemma: Any well-formed \nsequence under S has a .nite number of distinct elements. Given a .nite alphabet S, any well-formed sequence \nunder S has a .nite number of distinct elements. For a sequence to be well-formed under S but not < it \nmust have elements which are duplicates. If we remove all duplicates we end up with a well\u00adformed sequenced \nunder <, which must be .nite. Therefore there must be a .nite number of distinct elements.  2.6.2 Tracking \nNames Every expression in the source program is assigned a name. A name is a triple, (f , e, a) where \nf is a function name, e is an expression index and a is an argument count. We label every expression \nin the source program with f being the function it comes from and e being a unique index within that \nfunction. The argument count a for constructors and applications is the number of arguments, and for \nall other expressions is 0. When manipulating expressions, we track names:  When renaming a bound variable, \nor substituting one variable for another, we do not change any names.  If we move a subexpression, we \nkeep the name already assigned to that subexpression.  If we increase the number of arguments to an \napplication or constructor, we increase the argument count of that expression. For example, let v = Cx; \nw = vy in w being transformed to let w = Cxy in w would have the new name for w set to the old name of \nv, but with an argument count of 2 instead of 1.  When splitting on a case we introduce a new constructor \n(see \u00a72.5.1), for this constructor we use the name assigned to the pattern from the case alternative. \n We map an expression to a bag of names by taking the names of all expressions bound at the root let. \nLemma: For any source program, there are a .nite number of names All subexpressions are assigned expression \nindices in advance, so there are only a .nite number of function name/index values. We only increase \nthe argument count when increasing the number of arguments applied to a constructor or application, which \nis bounded by the arity of that constructor or the source function. Therefore, there are only a .nite \nnumber of names. Lemma: A bag of names represents a .nite number of expressions Given a bag of names, \nthere are only a .nite number of expressions that could have generated it. We .rst assume that when simplifying \nan expression we always normalise the free variables naming the let body v1, and naming all other variables \nas they are reached from v1. Each name refers to one particular subexpression, but may have different \nvariable names. A .nite number of subexpressions can only be combined to produce a .nite number of expressions, \nif we ignore variable names, which the normalisation handles. Lemma: The termination properties required \nby \u00a72.3 are satis.ed The termination properties in \u00a72.3 are satis.ed by the lemmas in this section. We \nhave shown that the alphabet of names, S, is .nite. For terminate (S) we have shown that there can only \nbe a .nite number of distinct name bags, and that each name bag can only correspond to a .nite number \nof expressions, therefore there are a .nite number of distinct expressions. For terminate (<) we have \nshown that there can only be a .nite number of name bags.  2.6.3 Termination Splitting If we are forced \nto terminate we call stop, which splits the ex\u00adpression into several subexpressions. We require that \nstop h e only produces subexpressions which are not forced to terminate by terminate (S) h. We trivially \nsatisfy this requirement by using the termination criteria when de.ning stop. Given an expression: .free \n. let v1 = e1 vn = en in v We .rst split every variable bound at the let, to give: .free . let v1 =[ \ne1] vn =[ en] in v We now move variable vi under vj using the same conditions as split, described in \n\u00a72.5.2. In addition, we do not merge vi under vj if the resulting expression bound to vj would violate \nthe termination criteria terminate (S) h. Combined with with the property that all initial children are \nsingleton name bags, which trivially satisfy S for any expression, our merge restriction ensures no children \nviolate the termination criteria. As a heuristic, we attempt to move variable v before w if the name \nassociated with v occurs fewer times in the original expres\u00adsion. In most expressions that are growing, \nand therefore hit the termination criteria, there will be some name that keeps repeating. By favouring \nless frequent names we hope to keep together subex\u00adpressions that are not growing. This heuristic has \nno effect on the correctness or termination, but sometimes gives better optimisation.  2.6.4 Example \nMany simple example programs (such as map/map) never trigger the termination criteria. The standard example \nof a function that does require termination is reverse, which can be written in a simpli.ed form as: \nroot xs = rev [] xs rev acc xs = case xs of [] . acc y : ys . rev (y : acc) ys The rev function builds \nup an accumulator argument, which will be equal to the size of xs. To specialise on the accumulator argument \nwould require an in.nite number of specialisations. To supercompile this program, the optimise function \nstarts with an empty termination history and the expression rev [] xs, and calls reduce, resulting in: \n.xs . case xs of [] . [[[ ]]] y : ys . [ rev (y : []) ys] Focusing on the second alternative, we now \nadd rev [] xs to the termination history, and continue optimising rev (y : []) ys. This leads to the \nsequence of expressions: .x1 . rev [] x1 .x1 x2 . rev (x1 : []) x2 .x1 x2 x3 . rev (x1 : x2 : []) x3 \n... We can rewrite these expressions in our core language, with annotations for names: .x1 . let v1 = \n(root, 2, 0) [] v2 = (root, 1, 0) rev v1 x1 in v2 .x1 x2 . let v1 = (root, 2, 0) [] v2 = (rev , 2, 0) \nx1 : v1 v3 = (rev , 1, 0) rev v2 x2 in v1 .x1 x2 x3 . let v1 = (root, 2, 0) [] v2 = (rev , 2, 0) x2 : \nv1 v3 = (rev , 2, 0) x1 : v2 v4 = (rev , 1, 0) rev v3 x3 in v1  Under S the .rst two expressions create \na well-formed se\u00adquence, but the .rst three expressions do not. The .rst expression is permitted because \nthe history is empty. The second expression is permitted because it has a different set of names from \nthe .rst. The third expression has the same set of names as the second, and has a higher cardinality. \nTherefore, when optimising, we call stop on the third expression. After calling stop we get: .x1 x2 x3 \n. let v2 =[ let v1 = (root, 2, 0) [] v2 = (rev , 2, 0) x2 : v1 in v2] v4 =[ let v3 = (rev , 2, 0) x1 \n: v2 v4 = (rev , 1, 0) rev v3 x3 in v4] in v1 Part of the accumulator has been bound to v2, and separated \nfrom the main expression. Continuing to optimise we get the se\u00adquence: .x1 . rev [] x1 --reduce .x1 x2 \n. rev (x1 : []) x2 --reduce .x1 x2 x3 . rev (x1 : x2 : []) x3 --stop .x1 x2 x3 . rev (x1 : x2) x3 --reduce \n.x1 x2 x3 x4 . rev (x1 : x2 : x3) x4 --stop .x1 x2 x3 . rev (x1 : x2) x3 --reduce ... --repeat the last \n2 lines As required, we have a .nite number of distinct expressions, and end up with a recursive function \nin the target program.  2.7 Post-processing Our split function is structured to produce only one simple \nexpres\u00adsion per target function for example a target function will never contain two constructors. While \nmost opportunities to remove in\u00adtermediate structure have been exploited, the target program will usually \ncontain lots of small functions. We can eliminate many of these functions by inlining all functions which \nare only called once. For example, given the source program: root x = x : x :[] After supercompilation, \nwe get the target program: root x = x : fx fx = x : nil nil = [] We can then inline all functions that \nare only called once: root x = x : x :[] It is important that the only optimisation intended from this \npost-processing is the reduction of function call overhead. This use of inlining is substantially different \nfrom other compilers (Peyton Jones and Marlow 2002), where inlining is used to bring expres\u00adsions together \nto trigger other optimisations. 2.8 Alternative Designs In this section we describe some possible design \nalternatives for our supercompiler. 2.8.1 Binary Application The .rst version of this supercompiler had \nbinary application, rather than vector application. The App Var [Var] constructor was replaced by a combination \nof Var Var and App Var Var. The reason for originally choosing binary application is that it is closer \nto other Core languages, and the simpli.cation does not need to track arity information. There were three \nmain reasons for moving to vector application: Vector application simpli.es splitting with primitive \nfunctions, by providing the arity information directly.  Vector application makes it easier to identify \npartial applica\u00adtions when increasing sharing (see \u00a72.5.2).  Vector application reduces the number of \nnames in an expres\u00adsion, improving the time taken to compile.  2.8.2 Alternative Termination Orderings \nOur original termination rule was: x < y = x .set y . x .bag y Both this rule and the one described in \n\u00a72.6.1 can be proved us\u00ading the same argument. We switched to use our new rule because it is simpler, \nfollows more directly from the proof, and can be im\u00adplemented very ef.ciently. Choosing a termination \nrule is a tricky business no termination rule can be the best for all programs, so there is always scope \nfor experimentation. 2.8.3 Recursive Lets Our Core language does not include recursive lets. Recursive \nlets bound to functions can be ef.ciently removed using lambda-lifting (Johnsson 1985). Recursive lets \nbound to values can be removed, but doing so may cause the program to run arbitrarily slower (Mitchell \n2008). Alternatively, we can take functions with value recursive lets and make them primitives, losing \noptimisation po\u00adtential, but preserving complexity. In practice, the most common function with a value \nrecursive let is repeat, and our supercompiler is nearly always able to fuse away the list generated \nby repeat. 2.8.4 Common Subexpression Elimination Common Subexpression Elimination (CSE) involves detecting \nwhen a program will compute two identical expressions, and re\u00adducing them both to a single shared expression. \nOur Core language is well suited to CSE two variables can be merged if they have the same bound expression. \nThe advantage of CSE is that performance can be increased, sometimes asymptotically. The disadvantages \nare that CSE can introduce space leaks (Chitil 1998), and the additional sharing may stop variables from \nbeing moved when splitting. We have not investigated the performance impact of CSE on supercom\u00adpilation, \nbut think it is a worthwhile area for future research. 2.8.5 Inlining Simple Functions The GHC compiler \ninlines many non-recursive functions during the simpli.cation phase (Peyton Jones and Marlow 2002). It \nis cer\u00adtainly possible that our simpli.cation rules could be extended to inline some functions, such \nas id, provided no new names were introduced (and thus termination was unaffected). Another alter\u00adnative \nwould be to inline simple functions in the source program before supercompilation started (such as otherwise \nand (.)). The primary motivation for inlining simple functions would be to re\u00adduce the complexity of \nthe main supercompilation phase, and avoid inopportune termination splits. We have deliberately not performed \nany inlining other than in the step function, as there is a risk that doing so would hide weaknesses \nin our supercompiler. However, we think simple inlining would be worth investigating.   3. Comparison \nto Other Optimisations Supercompilation naturally subsumes many other optimisations, including constructor \nspecialisation (Peyton Jones 2007) and defor\u00adestation (Gill et al. 1993; Wadler 1990). However, there \nare some optimisations that supercompilation (in the form presented here) does not address in particular \nstrictness analysis and unboxing (Peyton Jones and Launchbury 1991), and the generation of native code. \nIn order to bene.t from these optimisations we use GHC to compile the resulting Core after supercompilation \n(The GHC Team 2009).  We now give an example where our supercompiler massively outperforms GHC, and \ndiscuss the optimisations being performed. Our example is: root n = map square (iterate (+1) 1) !! n \nwhere square x = x * x Running this program with n = 400000, GHC takes 0.149 sec\u00adonds, while our supercompiler \ncombined with GHC takes 0.011 seconds. Running for larger values of n is infeasible as the GHC only variant \nover.ows the stack. After optimising with our super\u00adcompiler, then compiling with GHC, the resulting \ninner-loop is: go :: Int# . Int# . Int # go x y = case x of 0 . y * y . go (x - 1) (y + 1) All the intermediate \nlists have been removed, there are no func\u00adtional values, all the numbers have been unboxed and all arithmetic \nis performed on unboxed values (GHC uses Int# as the type of unboxed integers). Supercompilation has \nfused all the intermedi\u00adate lists and specialised all functional arguments, leaving GHC to perform strictness \nanalysis and unboxing. The program compiled with GHC alone is much less ef.cient. GHC uses programmer \nsupplied rewrite rules to eliminate interme\u00addiate lists (Peyton Jones et al. 2001), which fuses the map/iterate \ncombination. Unfortunately, GHC does not contain a rule to fuse the input list to the (!!) operator. \nThe GHC rules match speci.c function names in the source program, meaning that rede.ning map locally \nwould inhibit the fusion. In contrast, our supercompiler does not rely on rules so is able to fuse the \nfunctions regardless of their names, and is able to perform fusion on data types other than lists. GHC \nspecialises the resulting map/iterate combination with the square function, but fails to specialise with \nincrement passing (+1) as a higher-order function. GHC can specialise functions to particular data values \nusing constructor specialisation, but does not currently do the same transformation for functional arguments. \nTo allow specialisation, some functions are written in a particular style: foldrf zxs = go xs where go \n[] = z go (y : ys)= fy (go ys) In this de.nition, provided lambda-lifting is not performed, the function \nfoldr is considered non-recursive. GHC can inline non\u00adrecursive functions, allowing the de.nition of \nfoldr to be replicated in an expression where f is known, eliminating the functional ar\u00adgument. In contrast, \nour supercompiler has specialised all the func\u00adtions to their functional arguments, even when written \nin a natural style. GHC fails to eliminate all the lists and higher-order functions, which in turn means \nthe integers are not detected as strict, and are not unboxed. In contrast, our supercompiler has reduced \nthe program suf.ciently for everything to be unboxed.  4. Benchmarks In this section we run our supercompiler \nover a range of bench\u00admarks drawn from other papers. The results are given in Table 1. The benchmarks \nwe use are: Program Lines Compile time Runtime Memory Size append 8 0.1 + 0.6 0.85 0.84 1.00 bernouilli \n148 2.4 + 1.3 1.04 0.96 1.02 charcount 32 0.1 + 0.6 0.14 0.01 0.99 digits-of-e2 100 2.0 + 0.8 0.40 0.45 \n0.99 exp3 8 39 0.5 + 0.8 0.93 1.00 1.08 factorial 12 0.1 + 0.6 0.98 1.00 1.00 linecount 43 0.2 + 0.6 \n0.01 0.01 0.98 primes 58 0.1 + 0.6 0.58 0.81 0.99 raytracer 26 0.1 + 0.6 0.56 0.44 1.00 r.b 16 0.1 + \n0.7 0.77 1.01 0.98 sumsquare 45 1.2 + 0.9 0.38 0.23 1.03 sumtree 27 0.1 + 0.6 0.14 0.01 1.00 tak 19 0.1 \n+ 0.6 0.79 1.01 0.98 tree.ip 26 0.1 + 0.6 0.57 0.45 1.01 wordcount 62 0.3 + 0.7 0.19 0.30 1.00 x2n1 36 \n0.1 + 0.8 0.90 0.99 1.00 Program is the name of the program as given in \u00a74. Lines is the number of lines \nof code in the original program, including library de.nitions, but excluding primitives. Compile time \nis the number of seconds to compile the program (a + b), including both compilation with our supercompiler \n(a) and the subsequent compilation with GHC (b). The .nal three columns are relative to ghc -O2 being \n1.00, with a lower number being better. Runtime is how long the optimised program takes to run. Memory \nis the amount of memory allocated on the heap. Size is the size of the optimised program on disk. Table \n1. Benchmark results. sumsquare is the introductory example used in the stream fusion paper (Coutts \net al. 2007).  charcount, linecount and wordcount are taken from our previous supercompiler work (Mitchell \nand Runciman 2008), and word\u00adcount is used as the example in \u00a71. For the purpose of bench\u00admarking, we \nhave removed the actual IO operations, leaving just the actual computation.  append, factorial, tree.ip, \nsumtree and raytracer have been used to benchmark other supercompilers (Jonsson and Nordlander 2009), \nand originate from papers on deforestation (Kort 1996; Wadler 1990).  bernouilli, digits-of-e2, exp3 \n8, primes, r.b, tak and x2n1 are all taken from the Imaginary section of the no.b benchmark suite (Partain \net al. 2008).  We have manually translated all the examples from their source language to our Core language. \nWe have taken care to ensure that we have not simpli.ed the programs in translation in particular we \nhave inserted explicit dictionaries for all examples that require type classes (Wadler and Blott 1989), \nand have translated list\u00adcomprehensions to concatMap as described by the Haskell report (Peyton Jones \n2003). For comparison purposes we compiled all the benchmarks with GHC 6.12.1 (The GHC Team 2009), using \nthe -O2 optimisation setting. For the supercompiled results we .rst ran our supercom\u00adpiler, then compiled \nthe result using GHC. To run the benchmarks we used a 32bit Windows machine with a 2.5GHz processor and \n4Gb of RAM. 4.1 Comparison to GHC The benchmarks are nearly all faster than GHC, with some pro\u00adgrams \nrunning substantially faster than GHC alone. The improve\u00adment in speed is usually accompanied by either \na similar memory usage, or a substantial reduction. The resulting executables are all very close in size \nto compilation with GHC alone partly because the run-time system accounts for a substantial proportion \nof the ex\u00adecutable size.  Numerical Computation Some of the benchmarks mainly test numerical performance \n for example factorial, x2n1 and tak. In these benchmarks we have been able to inline some of the func\u00adtions \neven though they are recursive, which has been equivalent to a small amount of loop unrolling, and has \nsometimes improved ex\u00adecution speed. Complete Elimination Some of the benchmarks allow us to com\u00adpletely \neliminate most intermediate values for example char\u00adcount, sumtree and raytracer. In these cases the \nexecution time and memory are both substantially reduced. Most of these benchmarks have previously been \nused to test supercompilers, and our super\u00adcompiler performs the same optimisations. Partial Elimination \nSome of the benchmarks have a combination of data structures and numerical computation for example primes, \ndigits-of-e2 and exp3 8. In these benchmarks we perform speciali\u00adsation, and remove some intermediate \nvalues, but due to the nature of the benchmarks not all intermediate values can be eliminated. In digits-of-e2 \nwe are able to fuse long pipelines of list fusions. In exp3 8 most of our performance increase comes \nfrom eliminating intermediate values of the data type data Nat = Z | S Nat. Bernouilli The benchmark \non which we do worst is bernouilli. The bernouilli program seems reasonably similar in terms of list \noperations to other benchmarks such as primes, but our supercom\u00adpiler is unable to outperform GHC the \nexact reasons are still un\u00adclear. Interestingly, both our previous supercompiler and the stream fusion \nwork also failed to outperform GHC on this benchmark, so the reason may be that GHC does a particularly \ngood job on this benchmark.  4.2 Compilation Speed In the benchmarks presented, our supercompiler always \ntakes under four seconds to compile. We have given the compilation times as two .gures the time taken \nto run the supercompiler, followed by the time taken to compile the result with GHC. In all cases, the \nresulting GHC compilation time is dominated by the linker. Compared to our previous supercompiler, where \ncompile times ranged from a few seconds to .ve minutes, our new supercompiler is substantially faster. \nWhile we have designed our supercompiler with compilation speed in mind, we haven t focused on optimising \nthe compiler all functions are implemented as simply as possible. Pro.ling shows that 80% of the compilation \ntime is spent simplifying expressions, as described in \u00a72.2. Our simpli.cation method is currently written \nas a transformation that is applied until a .xed point is reached we believe the simpli.cation can be \nimplemented in one pass, leading to a substantial reduction in compile time. We have implemented the \ntermination check exactly as de\u00adscribed in \u00a72.6.1, traversing and comparing the entire history at each \nstep. For our termination check it is simple to change the his\u00adtory to a mapping from a name bag to an \ninteger (being the highest permitted cardinality) reducing the algorithmic complexity. We could also \noptimise the representation of names, using a single in\u00adteger for both the function name and subexpression \nindex.  5. Related Work We .rst describe related work in the area of supercompilation, par\u00adticularly \nwhat makes our supercompiler unique. We then describe some work from other areas, particularly work from \nwhich we have used ideas. 5.1 Supercompilation Supercompilation was introduced by Turchin (1986) for \nthe Refal programming language (Turchin 1989). Since this original work, there have been many suggestions \nof both termination strategies and generalisation/splitting strategies (Leuschel 2002; S\u00f8rensen and Gl\u00a8 \nuck 1995; Turchin 1988). The original supercompiler main\u00adtained both positive and negative knowledge \n(Secher and S\u00f8rensen 2000), however our implementation uses only positive knowledge (what a variable \nis, rather than what it cannot be). More recently, su\u00adpercompilation has started to converge towards \na common design, described in detail by Klyuchnikov (2009), but which has much in common with the underlying \ndesign present in other papers (Jons\u00adson and Nordlander 2009; Mitchell and Runciman 2008). Compared to \nan increasingly common foundation, our super\u00adcompiler is radically different. We have changed many of \nthe in\u00adgredients of supercompilation (the treatment of let, termination cri\u00adteria, how the termination \nhistories are used), but have also changed the way these ingredients are combined (the manager). In particu\u00adlar, \nmany of our choices would not work if applied in isolation to another supercompiler for example the \ntermination criteria relies on the treatment of let. 5.1.1 Let Expressions Compared to other supercompilers, \nour Core language requires many more let expressions. Previous supercompilation work has tended to ignore \nlet expressions if let is mentioned the usual strategy is to substitute all linear lets and residuate \nall others. At the same time, movement of lets can have a dramatic impact on performance: carefully designed \nlet-shifting transformations give an average speedup of 15% in GHC (Peyton Jones et al. 1996). Our previous \nwork inlined all let bindings that it could show did not lead to a loss of sharing (Mitchell and Runciman \n2008). Unfortunately, where a let could not be removed, there was a substantial performance penalty. \nBy going to the opposite extreme we are forced to deal with let bindings properly, making our new supercompiler \nboth simpler and more robust.  5.1.2 Termination Criteria The standard termination criteria used by \nsupercompilers is the homeomorphic embedding (Leuschel 2002). The homeomorphic embedding is a well-quasi \nordering, from Kruskal s Tree Theo\u00adrem (Kruskal 1960). The criteria requires that for every in.nite se\u00adquence \ne1,e2 ... there exist indicies i<j such that ej . ei. The intuition of homeomorphic embedding is that \nx . y holds if by removing nodes from y you cannot obtain x. Our termination rule uses similar ideas \nto a well-quasi ordering, but with a very different comparison relation. We are unaware of any other \nsupercompilers that have assigned names to expressions, or that have used a bag based termination rule \n(most use tree orderings, or sometimes cost models/budgets). Without our particular treatment of expressions \nas a set of let bindings, and our particular simpli.cation rules, it is not possible to use our termination \nrule. For example, if we ever inline let bindings then subexpressions would be changed internally, and \na single name for each subexpression would no longer be suf.cient. In some cases, our rule is certainly \nless restrictive than the homeomorphic embedding. The example in \u00a72.6.4 would have stopped one step earlier \nwith a homeomorphic embedding. Under a fairly standard interpretation of variable names and let expressions, \nwe can show that our rule is always less restrictive than the home\u00adomorphic embedding although other \ndifferences in our treatment of expressions mean such a comparison is not necessarily meaning\u00adful. However, \nwe did not choose our termination criteria to permit more expressions it was chosen for both simplicity \nand compila\u00adtion speed.  We use two separate termination histories, one in reduce and another in optimise \n an idea suggested by Mitchell (2008), but not previously implemented. By separating the termination \nhistories we gain better predictability, as reduce is not dependent on which functions have gone before. \nAdditionally, the histories are kept sub\u00adstantially smaller, again improving compile-time performance. \nBy splitting termination checks we also reduce the coupling between the separate aspects of supercompilation, \nallowing us to present a simpler manager than would otherwise be possible. As a result of the changes \nto termination and the Core language, the operation for splitting when the termination check fails is \nradi\u00adcally different. In particular, we can use almost identical operations when either evaluation fails \nto continue, or the termination check fails.  5.2 Partial evaluation There has been a lot of work on \npartial evaluation (Jones et al. 1993), where a program is specialised with respect to some static data. \nPartial evaluation works by marking all variable bindings within a program as either static or dynamic, \nusing binding time analysis, then specialises the program with respect to the static bindings. Partial \nevaluation is particularly appropriate for optimis\u00ading an interpreter with respect to the expression \ntree of a particular program, automatically generating a compiler, and removing inter\u00adpretation overhead. \nThe translation of an interpreter into a com\u00adpiler is known as the First Futamura Projection (Futamura \n1999), and can often give an order of magnitude speedup. Supercompilation and partial evaluation both \nremove abstrac\u00adtion overhead within a program. Partial evaluation is more suited to completely removing \nstatic data, such as an expression tree which is interpreted. Supercompilation is able to remove intermediate \ndata structures, which partial evaluation cannot usually do. 5.3 Deforestation Deforestation (Wadler \n1990) removes intermediate trees (most commonly lists) from computations. This technique has been ex\u00adtended \nin many ways, including to encompass higher-order defor\u00adestation (Marlow 1996). In many cases the gains \nfrom supercompi\u00adlation are just particular forms of deforestation. Probably the most practically applied \nwork on deforestation uses GHC s rewrite rules to optimise programs (Peyton Jones et al. 2001). Shortcut \ndeforestation rewrites many de.nitions in terms of foldr and build, then combines foldr/build pairs (Gill \net al. 1993) to deforest lists. Stream fusion works similarly, but relies on stream/unstream rules (Coutts \net al. 2007). All these schemes are only able to optimise functions written in terms of the correct primitives, \nwhich have had fusion rules de.ned. The advantage of supercompilation is that it applies to many types \nand functions, without any special effort from the program author. 5.4 Lower Level Optimisations Our \noptimisation works at the Core level, but even once ef.cient Core has been generated there is still some \nwork before ef.cient machine code can be produced. Key optimisations include strict\u00adness analysis and \nunboxing (Peyton Jones and Launchbury 1991). In GHC both of these optimisations are done at the core \nlevel, using a core language extended with unboxed types. After this lower level core has been generated, \nit is then compiled to STG machine in\u00adstructions (Peyton Jones 1992), from which assembly code is gen\u00aderated. \nThere is still work being done to modify the lowest levels to take advantage of the current generation \nof microprocessors (Mar\u00adlow et al. 2007). We rely on GHC to perform all these optimisations after our \nsupercompiler generates a target program. The GRIN approach (Boquist and Johnsson 1996) uses whole program \ncompilation for Haskell. It is currently being implemented in the jhc compiler (Meacham 2008), with promising \ninitial results. GRIN works by .rst removing all functional values, turning them into case expressions, \nallowing subsequent optimisation. The inter\u00admediate language for jhc is at a much lower level than our \nCore language, so jhc is able to perform detailed optimisations that we are unable to express.  6. \nConclusions and Future Work We have described a novel supercompiler, with a focus on sim\u00adplicity, which \ncan compile our benchmarks in a few seconds, and in some benchmarks offers substantial performance improvements \nover GHC alone. We see two main avenues for future work: in\u00adcreasing the range of benchmarks, and improving \nthe runtime per\u00adformance. 6.1 More Benchmarks In order to run more benchmarks we need to automatically \ntrans\u00adlate Haskell to our Core language. In previous papers we used the Yhc compiler to generate Core \n(Golubovsky et al. 2007), but sadly Yhc is not maintained and no longer works. Given that our super\u00adcompiler \nrelies on GHC to perform strictness analysis and generate native code, it seems sensible to use GHC to \ngenerate our Core language perhaps as a compiler plug-in, or working on external Core, or integrated \ninto the compiler. Our supercompiler processes the whole program in one go, which naturally leads to \nquestions of scalability. In the tests we have run we have not had a problem with compilation time, but \nit is something to be aware of as benchmarks increase in size. We believe that our supercompiler could \nbe sped up massively, using some of the techniques mentioned in \u00a74.2. In addition, we could split programs \ninto separate components by de.ning some functions to be primitive although this will remove optimisation \npotential. 6.2 Runtime Performance Our performance results are good, but there are always opportu\u00adnities \nto improve. We currently rely on GHC s strictness analysis to run after we have optimised the program, \nbut by integrating a strictness analysis we may be able to do better. The most common uses of GHC s rules \nengine, particularly list/stream fusion, are au\u00adtomatically performed by our supercompiler. However, \nsome trans\u00adformations such as replacing head . sort with minimum, are too complex to automatically infer. \nIt may be of bene.t to integrate a rules engine in to our supercompiler. In some cases the author of \na program has a particular idea about some intermediate data structure they expect to be eliminated. \nIf these structures remain in the optimised program the performance penalty is sometimes dramatic. Perhaps \na user could mark some values they expect to be removed, and then be warned if they remain.  6.3 Conclusions \nSupercompilation is a powerful technique which generalises many of the transformations performed by optimising \ncompilers. We were initially drawn to supercompilation for two reasons. Firstly, all intermediate values \nhave the potential to be eliminated, regard\u00adless of their type or the functions which operate on them. \nSecondly, supercompilation is a single-pass optimisation, avoiding the tricky problem of ordering compiler \nphases for best optimisation. With these advantages supercompilation has the potential to drastically \nsimplify an optimising compiler, while still achieving great per\u00adformance. Our supercompiler builds on \nthese advantages, rethink\u00ading supercompilation to make it simpler and improve compilation times.  Acknowledgements \nI would like to thank Max Bolingbroke, Jason Reich, Simon Peyton Jones, Colin Runciman and Peter Jonsson \nfor helpful discussions. Thanks to Ketil Malde for providing further inspiration to continue researching \nsupercompilation. Thanks to Max Bolingbroke, Mike Dodds and the anonymous referees for helpful comments \non earlier drafts.   References Urban Boquist and Thomas Johnsson. The GRIN project: A highly opti\u00admising \nback end for lazy functional languages. In Proc IFL 96, volume 1268 of LNCS, pages 58 84. Springer-Verlag, \n1996. Olaf Chitil. Common subexpressions are uncommon in lazy functional languages. LNCS, 1467:53 71, \n1998. Duncan Coutts, Roman Leshchinskiy, and Don Stewart. Stream fusion: From lists to streams to nothing \nat all. In Proc ICFP 07, pages 315 326. ACM Press, October 2007. Cormac Flanagan, Amr Sabry, Bruce Duba, \nand Matthias Felleisen. The essence of compiling with continuations. In Proc PDLI 93, volume 28(6), pages \n237 247. ACM Press, New York, 1993. Yoshihiko Futamura. Partial evaluation of computation process an \nap\u00adproach to a compiler-compiler. Higher-Order and Symbolic Computa\u00adtion, 12(4):381 391, 1999. Andrew \nGill, John Launchbury, and Simon Peyton Jones. A short cut to deforestation. In Proc FPCA 93, pages 223 \n232. ACM Press, June 1993. Dimitry Golubovsky, Neil Mitchell, and Matthew Naylor. Yhc.Core from Haskell \nto Core. The Monad.Reader, 1(7):45 61, April 2007. Thomas Johnsson. Lambda lifting: transforming programs \nto recursive equations. In Proc. FPCA 85, pages 190 203. Springer-Verlag, 1985. Neil Jones, Carsten Gomard, \nand Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall International, \n1993. Peter Jonsson and Johan Nordlander. Positive supercompilation for a higher order call-by-value \nlanguage. In POPL 09, pages 277 288. ACM, 2009. Ilya Klyuchnikov. Supercompiler HOSC 1.0: under the hood. \nPreprint 63, Keldysh Institute of Applied Mathematics, Moscow, 2009. Ilya Klyuchnikov. Supercompiler \nHOSC 1.1: proof of termination. Preprint 21, Keldysh Institute of Applied Mathematics, Moscow, 2010. \nJ Kort. Deforestation of a raytracer. Master s thesis, University of Amster\u00addam, 1996. Joseph Kruskal. \nWell-quasi-ordering, the tree theorem, and Vazsonyi s conjecture. Transactions of the American Mathematical \nSociety, 95(2): 210 255, 1960. Michael Leuschel. Homeomorphic embedding for online termination of symbolic \nmethods. In The essence of computation: complexity, analysis, transformation, pages 379 403. Springer-Verlag, \n2002. Simon Marlow. Deforestation for Higher-Order Functional Programs. PhD thesis, University of Glasgow, \n1996. Simon Marlow, Alexey Rodriguez Yakushev, and Simon Peyton Jones. Faster laziness using dynamic \npointer tagging. In Proc. ICFP 07, pages 277 288. ACM Press, October 2007. John Meacham. jhc: John s \nHaskell compiler. http://repetae.net/ john/computer/jhc/, 2008. Neil Mitchell. Transformation and Analysis \nof Functional Programs. PhD thesis, University of York, 2008. Neil Mitchell and Colin Runciman. A supercompiler \nfor core Haskell. In Selected papers from IFL 2007, volume 5083 of LNCS, pages 147 164. Springer-Verlag, \nMay 2008. Will Partain et al. The nofib Benchmark Suite of Haskell Programs. http://darcs.haskell.org/nofib/, \n2008. Simon Peyton Jones. Implementing lazy functional languages on stock hardware: The spineless tagless \nG-machine. JFP, 2(2):127 202, 1992. Simon Peyton Jones. Haskell 98 Language and Libraries: The Revised \nReport. Cambridge University Press, 2003. Simon Peyton Jones. Call-pattern specialisation for Haskell \nprograms. In Proc. ICFP 07, pages 327 337. ACM Press, October 2007. Simon Peyton Jones and John Launchbury. \nUnboxed values as .rst class citizens in a non-strict functional language. In Proc FPCA 91, volume 523 \nof LNCS, pages 636 666. Springer-Verlag, August 1991. Simon Peyton Jones and Simon Marlow. Secrets of \nthe Glasgow Haskell Compiler inliner. JFP, 12:393 434, July 2002. Simon Peyton Jones, Will Partain, and \nAndre Santos. Let-.oating: Moving bindings to give faster programs. In Proc. ICFP 96, pages 1 12. ACM \nPress, 1996. Simon Peyton Jones, Andrew Tolmach, and Tony Hoare. Playing by the rules: Rewriting as a \npractical optimisation technique in GHC. In Proc. Haskell 01, pages 203 233. ACM Press, 2001. Jens Peter \nSecher and Morten S\u00f8rensen. On perfect supercompilation. In Proceedings of Perspectives of System Informatics, \nvolume 1755 of LNCS, pages 113 127. Springer-Verlag, 2000. Morten S\u00f8rensen and Robert Gl\u00a8uck. An algorithm \nof generalization in positive supercompilation. In Logic Programming: Proceedings of the 1995 International \nSymposium, pages 465 479. MIT Press, 1995. The GHC Team. The GHC compiler, version 6.12.1. http://www. \nhaskell.org/ghc/, December 2009. Andrew Tolmach. An external representation for the GHC core lan\u00adguage. \nhttp://www.haskell.org/ghc/docs/papers/core.ps. gz, September 2001. Valentin Turchin. The concept of \na supercompiler. ACM Trans. Program. Lang. Syst., 8(3):292 325, 1986. Valentin Turchin. The algorithm \nof generalization in the supercompiler. In Partial Evaluation and Mixed Copmutation, pages 341 353. North-Holland, \n1988. Valentin Turchin. Refal-5, Programming Guide &#38; Reference Manual. New England Publishing Co., \nHolyoke, MA, 1989. Philip Wadler. Deforestation: Transforming programs to eliminate trees. Theoretical \nComputer Science, 73:231 248, 1990. Philip Wadler and Stephen Blott. How to make ad-hoc polymorphism \nless ad hoc. In Proc. POPL 89, pages 60 76. ACM Press, 1989.   \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Supercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.</p>", "authors": [{"name": "Neil Mitchell", "author_profile_id": "81337491713", "affiliation": "Cambridge, United Kingdom", "person_id": "P2338225", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863588", "year": "2010", "article_id": "1863588", "conference": "ICFP", "title": "Rethinking supercompilation", "url": "http://dl.acm.org/citation.cfm?id=1863588"}