{"article_publication_date": "09-27-2010", "fulltext": "\n A Play on Regular Expressions Functional Pearl Sebastian Fischer Frank Huch Thomas Wilke Christian-Albrechts \nUniversity of Kiel, Germany {sebf@,fhu@,wilke@ti.}informatik.uni-kiel.de Abstract Cody, Hazel, and Theo, \ntwo experienced Haskell programmers and an expert in automata theory, develop an elegant Haskell program \nfor matching regular expressions: (i) the program is purely func\u00adtional; (ii) it is overloaded over arbitrary \nsemirings, which not only allows to solve the ordinary matching problem but also supports other applications \nlike computing leftmost longest matchings or the number of matchings, all with a single algorithm; (iii) \nit is more powerful than other matchers, as it can be used for parsing every context-free language by \ntaking advantage of laziness. The developed program is based on an old technique to turn regular expressions \ninto .nite automata which makes it ef.cient both in terms of worst-case time and space bounds and actual \nperformance: despite its simplicity, the Haskell implementation can compete with a recently published \nprofessional C++ program for the same problem. Categories and Subject Descriptors D.1.1 [Programming \nTech\u00adniques]: Applicative (Functional) Programming; F.1.1 [Computa\u00adtion by Abstract Devices]: Models \nof Computation (Automata) General Terms Algorithms, Design Keywords regular expressions, .nite automata, \nGlushkov con\u00adstruction, purely functional programming CAST CODY pro.cient Haskell hacker HAZEL expert \nfor programming abstractions THEO automata theory guru ACT I SCENE I. SPECIFICATION To the right: a \ncoffee machine and a whiteboard next to it. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright \n&#38;#169; 2010 ACM 978-1-60558-794-3/10/09. . . $10.00 To the left: HAZEL sitting at her desk, two of.ce \nchairs nearby, a whiteboard next to the desk, a laptop, keyboard, and mouse on the desk. HAZEL is looking \nat the screen, a browser window shows the project page of a new regular expression library by Google. \nCODY enters the scene. CODY What are you reading? HAZEL Googlejustannouncedanewlibraryforregularexpression \nmatching which is in the worst case faster and uses less memory than commonly used libraries. CODY How \nwould we go about programming regular expression matching in Haskell? HAZEL Well, let s see. We d probably \nstart with the data type. (Opens a new Haskell .le in her text editor and enters the following de.nition.) \ndata Reg = Eps --e | Sym Char --a | Alt Reg Reg --a|\u00df | Seq Reg Reg --a\u00df | Rep Reg --a *  THEO (a computer \nscientist, living and working three .oors up, strolls along the corridor, carrying his coffee mug, thinking \nabout a dif.cult proof, and searching for distraction.) What are you doing, folks? HAZEL WejuststartedtoimplementregularexpressionsinHaskell. \nHere is the .rst de.nition. THEO (picks up a pen and goes to the whiteboard.) So how would you write \n((a|b) * c(a|b) * c) * (a|b) * , which speci.es that a string contains an even number of c s? CODY That \ns easy. (Types on the keyboard.) ghci> let nocs = Rep (Alt (Sym a ) (Sym b )) ghci> let onec = Seq nocs \n(Sym c ) ghci> let evencs = Seq (Rep (Seq onec onec)) nocs  THEO Ah. You can use abbreviations, that \ns convenient. But why do you have Sym in front of every Char? That looks redun\u00addant to me. HAZEL Haskell \nis strongly typed, which means every value has exactly one type! The arguments of the Alt constructor \nmust be of type Reg, not Char, so we need to wrap characters in the Sym constructor. But when I draw \na regular expression, I leave out Sym, just for simplicity. For instance, here is how I would draw your \nexpression. (Joins THEO at the whiteboard and draws Figure 1.) CODY How can we de.ne the language accepted \nby an arbitrary regular expression? Seq Rep Rep Seq Alt Seq Seq Rep Rep Alt Alt a b c a b c a \n b Figure 1. The tree representation of the regular expression ((a|b) * c(a|b) * c) * (a|b) * which \nmatches all words in {a, b, c} * with an even number of occurrences of c THEO As a predicate, inductively \non the structure of your data type. (Writes some formal de.nitions to the whiteboard: seman\u00adtic brackets, \nGreek letters, languages as sets, etc.) HAZEL (goes to the keyboard, sits down next to CODY.) Ok, this \ncan be easily coded in Haskell, as a characteristic function. List comprehensions are fairly useful, \nas well. (Writes the following de.nition in her text editor.) accept :: Reg . String . Bool accept Eps \nu = null u accept (Sym c) u = u =[c] accept (Alt p q) u = accept p u . accept q u accept (Seq p q) u \n= or [accept p u1 . accept q u2 | (u1, u2) . split u] accept (Rep r) u = or [and [accept r ui | ui . \nps] | ps . parts u] THEO Letmesee. split produces all decompositions of a string into two factors, and \nparts stands for partitions and produces all decompositions of a string into an arbitrary numbers of \nfactors. CODY Wait! We need to be careful to avoid empty factors when de.ning parts. Otherwise there \nis an in.nite number of possible decompositions. HAZEL Right. But split must also produce empty parts \nand can be de.ned as follows. (Continues writing the Haskell program.) split :: [a] . [([a], [a])] split \n[] = [([], [ ])] split (c : cs) = ([], c : cs) : [(c : s1, s2) | (s1, s2) . split cs] The function parts \nis a generalization of split to split words into any number of factors (not just two) except for empty \nones. CODY That s tricky. Let s use list comprehensions again. (Sits down on one of the empty chairs, \ngrabs the keyboard and ex\u00adtends the program as follows:) parts :: [a] . [[[a]]] parts [] = [[]] parts \n[c] = [[[c]]] parts (c : cs)= concat [[(c : p): ps, [c]: p : ps] | p : ps . parts cs] We split a word \nwith at least two characters recursively and either add the .rst character to the .rst factor or add \nit as a new factor. THEO Why do you write [a] and not String. HAZEL That s because we want to be more \ngeneral. We can now work with arbitrary list types instead of strings only. THEO That makes sense to \nme. CODY Maybe, it s good to have a separate name for these lists. I think Hazel used the term words \nthat s a good term. Let s stick to it. THEO I want to check out your code. (Sits down as well. Now all \nthree build a small crowd in front of the monitor.) ghci> parts \"acc\" [[\"acc\"],[\"a\",\"cc\"],[\"ac\",\"c\"],[\"a\",\"c\",\"c\"]] \nghci> accept evencs \"acc\" True  THEO Aha. (Pauses to think for a moment.) Wait a second! The number \nof decompositions of a string of length n +1 is 2n . Blindly checking all of them is not ef.cient. When \nyou convert a regular expression into an equivalent .nite-state automaton and use this automaton for \nmatching, then, for a .xed regular expression, the run time of the matching algorithm is linear in the \nlength of the string. HAZEL Well, the program is not meant to be ef.cient. It s only a speci.cation, \nalbeit executable. We can write an ef.cient program later. What I am more interested in is whether we \ncan make the speci.cation a bit more interesting .rst. Can it be generalized, for instance? THEO (staring \nout of the window.) We can add weights. HAZEL Weights? SCENE II. WEIGHTS HAZEL, CODY, and THEO are still \nsitting around the laptop. HAZEL What do you mean by weights? THEO Remember what we did above? Given \na regular expression, we assigned to a word a boolean value re.ecting whether the word matches the given \nexpression or not. Now, we produce more complex values semiring elements. HAZEL What s an example? Is \nthis useful at all? THEO A very simple example is to determine the length of a word or the number of \noccurrences of a given symbol in a word. A more complicated example would be to count the number of matchings \nof a word against a regular expression, or to determine a leftmost longest matching subword. CODY That \nsounds interesting, but what was a semiring, again? HAZEL If I remember correctly from my algebra course \na semiring is an algebraic structure with zero, one, addition, and multipli\u00adcation that satis.es certain \nlaws. (Adds a Haskell type class for semirings to the Haskell .le.) class Semiring s where zero, one \n:: s (.), (.) :: s . s . s Here, zero is an identity for ., one for ., both composition operators are \nassociative, and . is commutative, in addition. THEO That s true, but, moreover, the usual distributivity \nlaws hold and zero annihilates a semiring with respect to multiplications, which means that both zero \n. s and s . zero are zero for all s. HAZEL These laws are not enforced by Haskell, so program\u00admers need \nto ensure that they hold when de.ning instances of Semiring. CODY Ok, .ne. I guess what we need to do \nis to add weights to the symbols in our regular expressions. THEO (sipping coffee) Right. CODY So let \ns make a data type for weighted regular expressions. THEO (interjects.) Cool, that s exactly the terminology \nwe use in formal language theory.  CODY The only change to what we had before is in the symbol case; \nwe add the weights. We can also generalize from charac\u00adters to arbitrary symbol types. (Writes the following \ncode.) data Regw cs = Epsw | Symw (c . s) | Altw (Regw cs)(Regw cs) | Seqw (Regw cs)(Regw cs) | Repw \n(Regw cs)  HAZEL Aha! A standard implementation for the function attached to some character would compare \nthe character with a given character and yield either zero or one: sym :: Semiring s . Char . Regw Char \ns sym c = Symw (.x . if x = c then one else zero) Using sym, we can translate every regular expression \ninto a weighted regular expression in a canonical fashion: weighted :: Semiring s . Reg . Regw Char s \nweighted Eps = Epsw weighted (Sym c)= sym c weighted (Alt p q)= Altw (weighted p)(weighted q) weighted \n(Seq p q)= Seqw (weighted p)(weighted q) weighted (Rep p)= Repw (weighted p)  THEO How would you adjust \naccept to the weighted setting? HAZEL I replace the Boolean operations with semiring operations. (Goes \non with entering code.) acceptw :: Semiring s . Regw cs . [c] . s acceptw Epsw u = if null u then one \nelse zero acceptw (Symw f) u = case u of [c] . fc; . zero acceptw (Altw pq) u = acceptw pu . acceptw \nqu acceptw (Seqw pq) u = sum [acceptw pu1 . acceptw qu2 | (u1, u2) . split u] acceptw (Repw r) u = sum \n[prod [acceptw rui | ui . ps] | ps . parts u] THEO How do you de.ne the functions sum and prod? HAZEL \nThey are generalizations of or and and, respectively: sum, prod :: Semiring s . [s] . s sum = foldr (.) \nzero prod = foldr (.) one And we can easily de.ne a Semiring instance for Bool: instance Semiring Bool \nwhere zero = False one = True (.) =(.) (.) =(.)  THEO I see. We can now claim for all regular expressions \nr and words u the equation accept r u = acceptw (weighted r) u. CODY Ok, but we have seen matching before. \nTheo, can I see the details for the examples you mentioned earlier? THEO Let me check on your algebra. \nDo you know any semiring other than the booleans? CODY Well, I guess the integers form a semiring. (Adds \na corre\u00adsponding instance to the .le.) instance Semiring Int where zero =0 one =1 (.) = (+) (.) =(*) \n THEO Right, but you could also restrict yourself to the non\u00adnegative integers. They also form a semiring. \nHAZEL Let s try it out. ghci> let as = Alt (Sym a ) (Rep (Sym a )) ghci> acceptw (weighted as) \"a\" :: \nInt 2 ghci> let bs = Alt (Sym b ) (Rep (Sym b )) ghci> acceptw (weighted (Seq as bs)) \"ab\" :: Int 4 It \nseems we can compute the number of different ways to match a word against a regular expression. Cool! \nI wonder what else we can compute by using tricky Semiring instances. THEO I told you what you can do: \ncount occurrences of symbols, determine leftmost matchings, and so on. But let s talk about this in more \ndetail later. There is one thing I should mention now. You are not right when you say that with the above \nmethod one can determine the number of different ways a word matches a regular expression. Here is an \nexample. (Uses again the inter\u00adactive Haskell environment.) ghci> acceptw (weighted (Rep Eps)) \"\" :: \nInt 1  CODY The number of matchings is in.nite, but the program gives us only one. Can t we .x that? \nTHEO Sure, we can, but we would have to talk about closed semir\u00adings. Let s work with the simple solution, \nbecause working with closed semirings is a bit more complicated, but doesn t buy us much. HAZEL (smiling) \nThe result may not re.ect our intuition, but, due to the way in which we de.ned parts, our speci.cation \ndoes not count empty matchings inside a repetition. It only counts one empty matching for repeating the \nsubexpression zero times. ACT II Same arrangement as before. The regular expression tree previously \ndrawn by CODY, see Figure 1, is still on the whiteboard. HAZEL and CODY standing at the coffee machine, \nnot saying anything. THEO enters the scene. SCENE I. MATCHING THEO Good morning everybody! How about \nlooking into ef.cient matching of regular expressions today? HAZEL Ok. Can t we use backtracking? What \nI mean is that we read the given word from left to right, check at the same time whether it matches the \ngiven expression, revising decisions when we are not successful. I think this is what algorithms for \nPerl style regular expressions typically do. CODY But backtracking is not ef.cient at least not always. \nThere are cases where backtracking takes exponential time. HAZEL Can you give an example? CODY If you \nmatch the word a n against the regular expression (a|e)n a n, then a backtracking algorithm takes exponential \ntime to .nd a matching.1 HAZEL You re right. When trying to match a n against (a|e)n a n one can choose \neither a or e in n positions, so all together there are 2n different options, but only one of them picking \ne every time leads to a successful matching. A backtracking 1 By xn Cody means a sequence of n copies \nof x. Seq Seq Rep Rep Rep Rep Seq Alt Seq Alt Seq Seq Seq Seq Rep Rep Rep Rep Alt Alt Alt Alt a \nb c a b c a b a b c a b c a b (a) Result after reading b (b) Result after reading bc Figure \n2. Marking positions in the regular expression ((a|b) * c(a|b) * c) * (a|b) * while matching algorithm \nmay pick this combination only after having tried all the other options. Can we do better? THEO An alternative \nis to turn a regular expression into an equiva\u00adlent deterministic .nite-state automaton. The run time \nfor sim\u00adulating an automaton on a given word is linear in the length of the word, but the automaton can \nhave exponential size in the size of the given regular expression. CODY That s not good, because then \nthe algorithm not only has exponential space requirements but additionally preprocessing takes exponential \ntime. HAZEL Can you give an example where the deterministic automa\u00adton has necessarily exponential size? \nTHEO Supposeweareworkingwiththealphabetthatcontainsonly a and b. If you want to check whether a word \ncontains two occurrences of a with exactly n characters in between, then any deterministic automaton \nfor this will have 2n+1 different states. CODY Why? THEO Because at any time while reading a word from \nleft to right the automaton needs to know for each of the previous n characters whether it was an a or \nnot in order to tell whether the entire word is accepted. HAZEL I see. You need such detailed information \nbecause if there was an a exactly n +1 positions before the current position and the next character is \nnot an a, then you have to go to the next state, where you will need to know whether there was an a exactly \nn positions before the current position, and so on. THEO Exactly! And here is a formal proof. Suppose \nan automa\u00adton had less than 2n+1 states. Then there would be two distinct words of length n +1, say u \nand v, which, when read by the automaton, would lead to the same state. Since u and v are dis\u00adtinct, \nthere is some position i such that u and v differ at position i, say u carries symbol a in this position, \nbut v doesn t. Now, consider the word w which starts with i copies of b, followed by one occurrence of \na (w = bi a). On the one hand, uw has the above property, namely two occurrences of a s with n charac\u00adters \nin between, but vw has not, on the other hand, the automa\u00adton would get to the same state for both words, \nso either both words are accepted, or none of them is a contradiction. CODY Interesting. And, indeed, \na regular expression to solve this task has size only linear in n. If we restrict ourselves to the alphabet \nconsisting of a and b, then we can write it as follows. (Grabs a pen from his pocket and a business card \nfrom his wallet. Scribbles on the back of the business card. Reads aloud the following term.) (a|b) * \na(a|b)n a(a|b) * HAZEL Can we avoid constructing the automaton in advance? CODY Instead of generating \nall states in advance, we can generate the initial state and generate subsequent states on the .y. If \nwe discard previous states, then the space requirements are bound by the space requirements of a single \nstate. THEO And the run time for matching a word of length n is in O(mn) if it takes time O(m) to compute \na new state from the previous one. HAZEL That sounds reasonably ef.cient. How can we implement this idea? \nTHEO Glushkov proposed a nice idea for constructing non-deter\u00administic automata from regular expressions, \nwhich may come in handy here. It avoids e-transitions and can probably be im\u00adplemented in a structural \nfashion. HAZEL (smiling) I think we would say it could be implemented in a purely functional way. CODY \n(getting excited) How are his automata constructed? THEO A state of a Glushkov automaton is a position \nof the given regular expression where a position is de.ned to be a place where a symbol occurs. What \nwe want to do is determinize this automaton right away, so we should think of a state as a set of positions. \nHAZEL What would such a set mean? THEO The positions contained in a state describe where one would get \nto by trying to match a word in every possible way. HAZEL I don t understand. Can you give an example? \nTHEO Instead of writing sets of positions, I mark the symbols in the regular expression with a box. This \nis more intuitive and allows me to explain how a new set of positions is computed from an old one. CODY \nLet s match the string bc against the regular expression which checks whether it contains an even number \nof c s. THEO Well, I need to draw some pictures. CODY Then let s go back to Hazel s of.ce; we can probably \nuse what we wrote on the whiteboard yesterday. The three move to the left side of the stage, get in front \nof the whiteboard, where Figure 1 is still shown. THEO Initially, no symbol is marked, i.e., the initial \nstate is the empty set. We then mark every occurrence of b in the regular expression that might be responsible \nfor reading the .rst char\u00ad Seq Seq Rep Rep Rep Rep Seq Alt Seq Alt Seq Seq Seq Seq Rep Rep Rep Rep Alt \nAlt Alt Alt a a c b b c a b a b c a a c b b (a) Result after reading bcc (b) Result after \nreading bccc Figure 3. Shifting symbol marks in repetitions of the regular expression ((a|b) * c(a|b) \n* c) * (a|b) * acter b of the word. (Draws two boxes around the .rst and last b in the regular expression \ntree, see Figure 2(a).) HAZEL There are two possibilities to read the .rst character b, which correspond \nto the two positions that you marked. The last b in the regular expression can be marked because it follows \na repetition which accepts the empty word. But the b in the middle cannot be responsible for matching \nthe .rst character because it follows the .rst c which has not yet been matched. THEO Exactly! If we \nnow read the next character c we shift the mark from the .rst b to the subsequent c .(Does so, which \nleads to Figure 2(b).) CODY And the mark of the other b is discarded because there is no possibility \nto shift it to a subsequent c . THEO Right, you got the idea. We have reached a .nal state if there is \na mark on a .nal character. CODY When is a character .nal? THEO When no other character has to be read \nin order to match the whole regular expression, i.e., if the remaining regular expres\u00adsion accepts the \nempty word. HAZEL I think we can elegantly implement this idea in Haskell! Instead of using sets of positions \nwe can represent states as regular expressions with marks on symbols just as you did on the whiteboard. \n(They move to the desk, CODY sits down right in front of the keyboard, HAZEL and THEO take the two other \nof.ce chairs.) CODY Ok, let s change the data type. We .rst consider the simple version without semirings. \n(Opens the .le from the previous scene, adds a data type for regular expressions with marked symbols. \nThey use this .le for the rest of the scene.) data REG = EPS | SYM Bool Char | ALT REG REG | SEQ REG \nREG | REP REG  HAZEL Let s implement the shift function. We probably want it to take a possibly marked \nregular expression and a character to be read and to produce a possibly marked regular expression. THEO \nThat s not enough. In the beginning, no position is marked. So if we just shift, we ll never mark a position. \nA similar problem occurs for subterms. If, in the left subexpression of a sequence, a .nal position is \nmarked, we want this to be taken into account in the right subexpression. HAZEL We need a third parameter, \nm, which represents an addi\u00adtional mark that can be fed into the expression. So the argu\u00adments of shift \nare the mark, a possibly marked regular expres\u00adsion, and the character to be read: shift :: Bool . REG \n. Char . REG The result of shift is a new regular expression with marks. CODY The rule for e is easy, \nbecause e doesn t get a mark: shift EPS = EPS THEO And a symbol in the new expression is marked if, \n.rst, some previous symbol was marked, indicated by m=I True, and, second, the symbol equals the character \nto be read. shift m (SYM x) c = SYM (m . x =Ic) x HAZEL We treat both arguments of a choice of regular \nexpressions the same. shift m (ALT pq) c = ALT (shift m p c)(shift m q c) Sequences are trickier. The \ngiven mark is shifted to the .rst part, but we also have to shift it to the second part if the .rst part \naccepts the empty word. Additionally, if the .rst part contains a .nal character we have to shift its \nmark into the second part, too. Assuming helper functions empty and .nal which check whether a regular \nexpression accepts the empty word or contains a .nal character, respectively, we can handle sequences \nas follows: shift m (SEQ pq) c = SEQ (shift m p c) (shift (m . empty p . .nal p) qc) We haven t talked \nabout repetitions yet. How do we handle them? THEO Let s go back to the example .rst. Assume we have \nal\u00adready read the word bc but now want to read two additional c s. (Stands up, grabs a pen, and changes \nthe tree on the white\u00adboard; the result is shown in Figure 3(a).) After reading the .rst c the mark is \nshifted from the .rst c in the regular expression to the second c as usual because the repetition in \nbetween ac\u00adcepts the empty word. But when reading the second c, the mark is shifted from the second c \nin the expression back to the .rst one! (Modi.es the drawing again, see Figure 3(b) for the result.) \nRepetitions can be read multiple times and thus marks have to be passed through them multiple times, \ntoo. CODY So we can complete the de.nition of shift as follows: shift m (REP r) c = REP (shift (m . .nal \nr) rc) We shift a mark into the inner expression if a previous character was marked or a .nal character \nin the expression is marked. HAZEL Ok, let s de.ne the helper functions empty and .nal.I guess, this \nis pretty straightforward. (Types the de.nition of empty in her text editor.) empty :: REG . Bool empty \nEPS = True empty (SYM )= False empty (ALT pq)= empty p . empty q empty (SEQ pq)= empty p . empty q empty \n(REP r)= True No surprises here. How about .nal?(Goes on typing.) .nal :: REG . Bool .nal EPS = False \n.nal (SYM b )= b .nal (ALT pq)= .nal p . .nal q .nal (SEQ pq)= .nal p . empty q . .nal q .nal (REP r)= \n.nal r  CODY (pointing to the screen) The case for sequences is wrong. It looks similar to the de.nition \nin shift, but you mixed up the variables p and q.(Takes the keyboard and wants to change the de.nition.) \nHAZEL No, stop! This is correct. .nal analyzes the regular expres\u00adsion in the other direction. A .nal \ncharacter of the .rst part is also a .nal character of the whole sequence if the second part accepts \nthe empty word. Of course, a .nal character in the sec\u00adond part is always a .nal character of the whole \nsequence, as well. CODY Got it. Let s wrap all this up into an ef.cient function match for regular expression \nmatching. (Continues typing.) The type of match is the same as the type of accept our previously de.ned \nspeci.cation. match :: REG . String . Bool If the given word is empty, we can check whether the expression \nmatches the empty word using empty: match r [] = empty r If the given word is a nonempty word c : cs \nwe mark all symbols of the given expression which may be responsible for matching the .rst character \nc by calling shift True r c. Then we subsequently shift the other characters using shift False. match \nr (c : cs)= .nal (foldl (shift False)(shift True r c) cs)  THEO Why has the argument to be False? CODY \nBecause, after having processed the .rst character, we only want to shift existing marks without adding \nnew marks from the left. Finally, we check whether the expression contains a .nal character after processing \nthe whole input word. HAZEL That is a pretty concise implementation of regular expres\u00adsion matching! \nHowever, I m not yet happy with the de.ni\u00adtion of shift and how it repeatedly calls the auxiliary functions \nempty and .nal which traverse their argument in addition to the traversal by shift. Look at the rule \nfor sequences again! (Points at the shift rule for sequences on the screen.) shift m (SEQ pq) c = SEQ \n(shift m p c) (shift (m . empty p . .nal p) qc)  data REGw cs = REGw {emptyw :: s, .nalw :: s, regw \n:: REw cs} data REw cs = EPSw | SYMw (c . s) | ALTw (REGw cs)(REGw cs) | SEQw (REGw cs)(REGw cs) | REPw \n(REGw cs) epsw :: Semiring s . REGw cs epsw = REGw {emptyw = one, .nalw = zero, regw = EPSw } symw :: \nSemiring s . (c . s) . REGw cs symw f = REGw {emptyw = zero, .nalw = zero, regw = SYMw f } altw :: Semiring \ns . REGw cs . REGw cs . REGw cs altw pq = REGw {emptyw = emptyw p . emptyw q, .nalw = .nalw p . .nalw \nq, regw = ALTw pq} seqw :: Semiring s . REGw cs . REGw cs . REGw cs seqw pq = REGw {emptyw = emptyw p \n. emptyw q, .nalw = .nalw p . emptyw q . .nalw q, regw = SEQw pq}  repw :: Semiring s . REGw cs . REGw \ncs repw r = REGw { emptyw = one, .nalw = .nalw r, regw = REPw r} matchw :: Semiring s . REGw cs . [c] \n. s matchw r [] = emptyw r matchw r (c : cs)= .nalw (foldl (shiftw zero \u00b7 regw)(shiftw one (regw r) c) \ncs) shiftw :: Semiring s . s . REw cs . c . REGw cs shiftw EPSw = epsw shiftw m (SYMw f) c =(symw f) \n{.nalw = m . fc} shiftw m (ALTw pq) c = altw (shiftw m (regw p) c)(shiftw m (regw q) c) shiftw m (SEQw \npq) c = seqw (shiftw m (regw p) c) (shiftw (m . emptyw p . .nalw p)(regw q) c) shiftw m (REPw r) c = \nrepw (shiftw (m . .nalw r)(regw r) c) Figure 4. Ef.cient matching of weighted regular expressions There \nare three calls which traverse p and one of them is a recursive call to shift. So, if p contains another \nsequence where the left part contains another sequence where the left part contains another sequence \nand so on, this may lead to quadratic run time in the size of the regular expression. We should come \nup with implementations of empty and .nal with constant run time.  CODY We need to cache the results \nof empty and .nal in the inner nodes of regular expressions such that we don t need to recompute them \nover and over again. Then the run time of shift is linear in the size of the regular expression and the \nrun time of match is in O(mn) if m is the size of the regular expression and n the length of the given \nword. THEO That s interesting. The run time is independent of the num\u00adber of transitions in the corresponding \nGlushkov automaton. The reason is that we use the structure of the regular expres\u00adsion to determine the \nnext state and .nd it without considering all possible transitions. HAZEL And the memory requirements \nare in O(m), because we discard old states while processing the input. The three are excited. THEO sits \ndown again to join CODY and HAZEL for generalizing the implementation previously developed: they use \nsemirings again and im\u00adplement the idea of caching the results of empty and .nal. The result is presented \nin Figure 4. HAZEL First of all, we have to add .elds to our regular expressions in which we can store \nthe cached values for empty and .nal. This can be done elegantly by two alternating data types REGw and \nREw. THEO Okay, but what are the curly brackets good for. CODY This is Haskell s notation for specifying \n.eld labels. You can read this de.nition as a usual data type de.nition with a single constructor REGw \ntaking three arguments. Furthermore, Haskell automatically de.nes functions emptyw, .nalw, and regw to \nselect the values of the corresponding .elds. THEO I understand. How can we go on then? CODY We use smart \nconstructors, which propagate the cached values automatically. When shifting marks, which are now weights, \nthrough the regular expression these smart construc\u00adtors will come in handy (epsw, symw, altw, seqw, \nand repw). THEO And again the curly brackets are Haskell s syntax for con\u00adstructing records? HAZEL Right. \nFinally, we have to modify match and shift such that they use cached values and construct the resulting \nregular expression by means of the smart constructors. The rule for SYMw introduces a new .nal value \nand caches it as well. THEO And here the curly brackets are used to update an existing record in only \none .eld. Two more weeks of Haskell pro\u00adgramming with you guys, and I will be able to write beautiful \nHaskell programs. Disbelief on HAZEL s and CODY s faces. They all laugh. SCENE II. HEAVY WEIGHTS HAZEL, \nCODY, and THEO sitting relaxed on of.ce chairs, facing the audience, holding coffee mugs. CODY I have \na question. Until now we compute a weight for a whole word according to a regular expression. Usually, \none is interested in matching a subword of a given word and .nding out information about the part that \nmatches. HAZEL Like the position or the length of the matching part. CODY Can we use our approach to \nmatch only parts of a word and compute information about the matching part? THEO I think so. How about \nputting something like (a|b) * around the given regular expression? This matches anything before and \nafter a part in the middle that matches the given expression. HAZEL Yes, that should work. And we can \nprobably zip the input list with a list of numbers in order to compute information about positions and \nlengths. Let s see. (Scratches her head with one hand and lays her chin into the other.) After a few \nseconds, the three move with their of.ce chairs to the desk again, open the .le from before in an editor, \nand continue typing. submatchw :: Semiring s . REGw (Int, c) s . [c] . s submatchw rs = matchw (seqw \narb (seqw r arb)) (zip [0 ..] s) where arb = repw (symw (. . one))  CODY I see! arb is a regular expression \nthat matches an arbitrary word and always yields the weight one. And r is a regular expression where \nthe symbols can access information about the position of the matched symbol. HAZEL Exactly! THEO How \ncan we create symbols that use the positional informa\u00adtion? HAZEL For example, by using a subclass of \nSemiring with an additional operation to compute an element from an index: class Semiring s . Semiringi \ns where index :: Int . s  CODY We can use it to de.ne a variant of the sym function: symi :: Semiringi \ns . Char . REGw (Int, Char) s symi c = symw weight where weight (pos, x) | x = c = index pos | otherwise \n= zero  THEO Ok, it yields zero if the character does not match, just like before, but uses the index \nfunction to compute a weight from the position of a character. And if we use symi we get regular expressions \nof type Regw (Int, Char) s, which we can pass to submatchw. Now, we need some instances of Semiringi \nthat use this machinery! HAZEL How about computing the starting position for a nonempty leftmost subword \nmatching? We can use the following data types: data Leftmost = NoLeft | Leftmost Start data Start = NoStart \n| Start Int NoLeft is the zero of the semiring, i.e., it represents a failing match. Leftmost NoStart \nis the one and, thus, used for ignored characters: instance Semiring Leftmost where zero = NoLeft one \n= Leftmost NoStart  CODY Let me try to de.ne addition. NoLeft is the identity for .. So the only interesting \ncase is when both arguments are con\u00adstructed by Leftmost. NoLeft . x = x x . NoLeft = x Leftmost x . \nLeftmost y = Leftmost (leftmost x y) where leftmost NoStart NoStart = NoStart leftmost NoStart (Start \ni)= Start i leftmost (Start i) NoStart = Start i leftmost (Start i)(Start j)= Start (min i j) The operation \n. is called on the results of matching a choice of two alternative regular expressions. Hence, the leftmost \nfunc\u00adtion picks the leftmost of two start positions by computing their minimum. NoStart is an identity \nfor leftmost. HAZEL Multiplication combines different results from matching the parts of a sequence \nof regular expressions. If one part fails, then the whole sequence does, and if both match, then the \nstart position is the start position of the .rst part unless the .rst part is ignored: NoLeft . = NoLeft \n. NoLeft = NoLeft Leftmost x . Leftmost y = Leftmost (start x y) where start NoStart s = s start s = \ns  THEO We need to make Leftmost an instance of Semiringi. I guess we just wrap the given position in \nthe Start constructor. instance Semiringi Leftmost where index = Leftmost \u00b7 Start  HAZEL Right. Now, \nexecuting submatchw in the Leftmost semir\u00ading yields the start position of the leftmost match. We don \nt have to write a new algorithm but can use the one that we de\u00ad.ned earlier and from which we know it \nis ef.cient. Pretty cool. THEO Let me see if our program works. (Starts GHCi.) I ll try to .nd substrings \nthat match against the regular expression a(a|b) * a and check where they start. ghci> let a = symi a \nghci> let ab = repw (a altw symi b ) ghci> let aaba = a seqw ab seqw a ghci> submatchw aaba \"ab\" :: \nLeftmost NoLeft ghci> submatchw aaba \"aa\" :: Leftmost Leftmost (Start 0) ghci> submatchw aaba \"bababa\" \n:: Leftmost Leftmost (Start 1) Ok. Good. In the .rst example, there is no matching and we get back NoLeft. \nIn the second example, the whole string matches and we get Leftmost (Start 0). In the last example, there \nare three matching subwords \"ababa\" starting at position 1, \"aba\" starting at position 1, and \"aba\" starting \nat position 3 and we get the leftmost start position. CODY Can we extend this to compute also the length \nof leftmost longest matches? HAZEL Sure, we use a pair of positions for the start and the end of the \nmatched subword. data LeftLong = NoLeftLong | LeftLong Range data Range = NoRange | Range Int Int The \nSemiring instance for LeftLong is very similar to the one we de.ned for Leftmost. We have to change the \nde.nition of addition, namely, where we select from two possible matchings. In the new situation, we \npick the longer leftmost match rather than only considering the start position. If the start positions \nare equal, we also compare the end positions: First, they only sketch how to implement this. ... LeftLong \nx . LeftLong y = LeftLong (leftlong x y) where leftlong ... leftlong (Range i j)(Range k l) | i < k . \ni = k . j ? l = Range i j | otherwise = Range k l CODY And when combining two matches sequentially, \nwe pick the start position of the .rst part and the end position of the second part. Pretty straightforward! \n... LeftLong x . LeftLong y = LeftLong (range x y) where range ... range (Range i )(Range j)= Range i \nj THEO We also need to de.ne the index function for the LeftLong type: instance Semiringi LeftLong where \nindex i = LeftLong (Range i i) And again, we can use the same algorithm that we have used before. The \nlight fades, the three keep typing, the only light emerges from the screen. After a few seconds, the \nlight goes on again, the sketched Semiring instance is on the screen. HAZEL Let s try the examples from \nbefore, but let s now check for leftmost longest matching. ghci> submatchw aaba \"ab\" :: LeftLong NoLeftLong \nghci> submatchw aaba \"aa\" :: LeftLong LeftLong (Range 0 1) ghci> submatchw aaba \"bababa\" :: LeftLong \nLeftLong (Range 1 5) The three lean back in their of.ce chairs, sip their coffee, and look satis.ed. \nSCENE III. EXPERIMENTS CODY and HAZEL sit in front of the computer screen. It s dark by now, no daylight \nanymore. CODY Before we call it a day, let s check how fast our algorithm is. We could compare it to \nthe grep command and use the regular expressions we have discussed so far. (Opens a new terminal window \nand starts typing.) bash> for i in seq 1 10 ; do echo -n a; done | \\ ...> grep -cE \"^(a?){10}a{10}$\" \n1 HAZEL What was that? CODY That was a for loop printing ten a s in sequence which were piped to the \ngrep command to print the number of lines matching the regular expression (a|e)10 a 10 . HAZEL Aha. Can \nwe run some more examples? CODY Sure. (Types in more commands.) bash> for i in seq 1 9 ; do echo -n a; \ndone | \\ ...> grep -cE \"^(a?){10}a{10}$\" 0 bash> for i in seq 1 20 ; do echo -n a; done | \\ ...> grep \n-cE \"^(a?){10}a{10}$\" 1 bash> for i in seq 1 21 ; do echo -n a; done | \\ ...> grep -cE \"^(a?){10}a{10}$\" \n0 HAZEL Ah. You were trying whether nine a s are accepted they are not and then checked 20 and 21 a \ns. CODY Yes, it seems to work correctly. Let s try bigger numbers and use the time command to check how \nlong it takes. bash> time for i in seq 1 500 ;do echo -n a;done |\\ ...> grep -cE \"^(a?){500}a{500}$\" \nCODY and HAZEL stare at the screen, waiting for the call to .nish. A couple of seconds later it does. \n 1 real 0m17.235s user 0m17.094s sys 0m0.059s  HAZEL That s not too fast, is it? Let s try our implementation. \n(Switches to GHCi and starts typing.) ghci> let a = symw ( a ==) ghci> let seqn n = foldr1 seqw . replicate \nn ghci> let re n = seqn n(altw a epsw ) seqw seqn na ghci> :set +s ghci> matchw (re 500) (replicate \n500 a ) True (5.99 secs, 491976576 bytes) CODY Good. We re faster than grep and we didn t even compile! \nBut it s using a lot of memory. Let me see. (Writes a small program to match the standard input stream \nagainst the above expression and compiles it using GHC.) I ll pass the -s option to the run-time system \nso we can see both run time and memory usage without using the time command. bash> for i in seq 1 500 \n; do echo -n a; done | \\ ...> ./re500 +RTS -s match ... 1 MB total memory in use ... Total time 0.06s \n(0.21s elapsed) ... Seems like we need a more serious competitor! HAZEL I told you about Google s new \nlibrary. They implemented an algorithm in C++ with similar worst case performance as our algorithm. Do \nyou know any C++? CODY Gosh! The light fades, the two keep typing, the only light emerges from the screen. \nAfter a few seconds, the light goes on again. HAZEL Now it compiles! CODY Puuh. This took forever one \nhour. HAZEL Let s see whether it works. CODY C++ isn t Haskell. They both smile. HAZEL We wrote the \nprogram such that the whole string is matched, so we don t need to provide the start and end markers \n^ and $. bash> time for i in seq 1 500 ;do echo -n a;done |\\ ...> ./re2 \"(a?){500}a{500}\" match real \n0m0.092s user 0m0.076s sys 0m0.022s Ah, that s pretty fast, too. Let s push it to the limit: bash> time \nfor i in seq 1 5000 ;do echo -n a;done |\\ ...> ./re2 \"(a?){5000}a{5000}\" Error ... invalid repetition \nsize: {5000} CODY Google doesn t want us to check this example. But wait. (Furrows his brow.) Let s \ncheat: bash> time for i in seq 1 5000 ;do echo -n a;done |\\ ...> ./re2 \"((a?){50}){100}(a{50}){100}\" \nmatch real 0m4.919s user 0m4.505s sys 0m0.062s  HAZEL Nice trick! Let s try our program. Unfortunately, \nwe have to recompile for n = 5000, because we cannot parse regular expressions from strings yet. They \nrecompile their program and run it on 5000 a s. bash> for i in seq 1 5000 ; do echo -n a; done |\\ ...> \n./re5000 +RTS -s match ... 3 MB total memory in use ... Total time 20.80s (21.19s elapsed) %GC time \n83.4% (82.6% elapsed) ... HAZEL The memory requirements are quite good but in total it s about .ve \ntimes slower than Google s library in this example. CODY Yes, but look at the GC line! More than 80% \nof the run time is spent during garbage collection. That s certainly because we rebuild the marked regular \nexpression in each step by shiftw. HAZEL This seems inherent to our algorithm. It s written as a purely \nfunctional program and does not mutate one marked regular expression but computes new ones in each step. \nUnless we can somehow eliminate the data constructors of the regular expression, I don t see how we can \nimprove on this. CODY A new mark of a marked expression is computed in a tricky way from multiple marks \nof the old expression. I don t see how to eliminate the expression structure which guides the computation \nof the marks. HAZEL Ok, how about trying another example? The Google li\u00adbrary is based on simulating \nan automaton just like our algo\u00adrithm. Our second example, which checks whether there are two a s with \na speci.c distance, is a tough nut to crack for automata-based approaches, because the automaton is exponen\u00adtially \nlarge. THEO curiously enters the scene. CODY Ok, can we generate an input string such that almost all \nstates of this automaton are reached? Then, hopefully, caching strategies will not be successful. THEO \nIf we just generate a random string of a s and b s, then the probability that it matches quite early \nis fairly high. Note that the probability that it matches after n +2 positions is one fourth. We need \nto generate a string that does not match at all and is suf.ciently random to generate an exponential \nnumber of different states. If we want to avoid that there are two a s with n characters in between, \nwe can generate a random string and additionally keep track of the n +1 previous characters. Whenever \nwe are exactly n +1 steps after an a, we generate a b. Otherwise, we randomly generate either an a or \na b. Maybe, we should... THEO s voice fades out. Apparently, he immerses him\u00adself in some problem. CODY \nand HAZEL stare at him for a few seconds, then turn to the laptop and write a program genrnd which produces \nrandom strings of a s and b s. They turn to THEO. CODY Theo, we re done! THEO Ohhh, sorry! (Looks at \nthe screen.) CODY We can call genrnd with two parameters like this: bash> ./genrnd 5 6 bbbaaaaabbbbbbbabaabbbbbbbaabbbbbbabbaabbb \n The result is a string of a s and b s such that there are no two a s with 5 characters in between. The \ntotal number of generated characters is the product of the incremented arguments, i.e., in this case \n(5 + 1) * (6+1) = 42. THEO Ok. So if we want to check our regular expression for n = 20 we need to use \na string with length greater than 220 106 . Let s generate around 2 million characters. HAZEL Ok, let \ns check out the Google program. bash> time ./genrnd 20 100000 | ./re2 \".*a.{20}a.*\" While the program \nis running CODY is looking at a process monitor and sees that Google s program uses around 5 MB of memory. \nno match real 0m4.430s user 0m4.514s sys 0m0.025s Let s see whether we can beat this. First, we need \nto compile a corresponding program that uses our algorithm. They write a Haskell program dist20 which \nmatches the standard input stream against the regular expression .*a.{20}a.*. Then they run it. bash> \n./genrnd 20 100000 | ./dist20 +RTS -s no match ... 2 MB total memory in use ... Total time 3.10s (3.17s \nelapsed) %GC time 5.8% (6.3% elapsed) ... HAZEL Wow! This time we are faster than Google. And our pro\u00adgram \nuses only little memory. CODY Yes, and in this example, the time for garbage collection is only about \n5%. I guess that s because the regular expression is much smaller now, so fewer constructors become garbage. \nTHEO This is quite pleasing. We have not invested any thoughts in ef.ciency at least w.r.t. constant \nfactors but, still, our small Haskell program can compete with Google s library. HAZEL Whatotherlibrariesarethereforregularexpressionmatch\u00ading? \nObviously, we cannot use a library that performs back\u00adtracking, because it would run forever on our .rst \nbenchmark. Also, we cannot use a library that constructs a complete au\u00adtomaton in advance, because it \nwould eat all our memory in the second benchmark. What does the standard C library do? CODY No idea. \nJust as above, the light fades out, the screen being the only source of light. CODY and HAZEL keep working, \nTHEO falls asleep on his chair. After a while, the sun rises. CODY and HAZEL look tired, they wake up \nTHEO. CODY (addressing THEO) We wrote a program that uses the stan\u00addard C library regex for regular \nexpression matching and checked it with the previous examples. It s interesting, the per\u00adformance differs \nhugely on different computers. It seems that different operating systems come with different implementa\u00adtions \nof the regex library. On this laptop an Intel MacBook running OS X 10.5 the regex library outperforms \nGoogle s library in the .rst benchmark and the Haskell program in the second benchmark both by a factor \nbetween two and three, but not more. We tried it on some other systems, but the library was slower there. \nAlso, when not using the option RE2::Latin1 in the re2 program it runs in UTF-8 mode and is more than \nthree times slower in the second benchmark. THEO Aha. ACT III SCENE I. INFINITE REGULAR EXPRESSIONS \nHAZEL sitting at her desk. THEO and CODY at the coffee machine, eating a sandwich. CODY The benchmarks \nare quite encouraging and I like how ele\u00adgant the implementation is. THEO Ilikeourworkaswell,althoughitisalwaysdif.culttowork \nwith practitioners. (Rolls his eyes.) It is a pity that the approach only works for regular languages. \nCODY I think this is not true. Haskell is a lazy language. So I think there is no reason why we should \nnot be able to work with non\u00adregular languages. THEO How is this possible? (Starts eating much faster.) \nCODY Well, I think we could de.ne an in.nite regular expression for a given context-free language. There \nis no reason why our algorithm should evaluate unused parts of regular expressions. Hence, context-free \nlanguages should work as well. THEO That sinteresting.(Finishes his sandwich.) Let s go to Hazel and \ndiscuss it with her. THEO jumps up and rushes to HAZEL. CODY is puzzled and follows, eating while walking. \n THEO (addressing HAZEL) Cody told me that it would also be possible to match context-free languages \nwith our Haskell pro\u00adgram. Is that possible? HAZEL It might be. Let s check how we could de.ne a regular \nexpression for any number of a s followed by the same number n of b s ({abn | n ? 0}). CODY Instead \nof using repetitions like in a * b *, we have to use recursion to de.ne an in.nite regular expression. \nLet s try. ghci> let a = symw ( a ==) ghci> let b = symw ( b ==) ghci> let anbn = epsw altw seqw a (anbn \nseqw b) ghci> matchw anbn \"\" The program doesn t terminate. ^C THEO It doesn t work. That s what I \nthought! HAZEL You shouldn t be so pessimistic. Let s .nd out why the program evaluates the in.nite regular \nexpression. CODY I think the problem is the computation of .nalw. It traverses the whole regular expression \nwhile searching for marks it can propagate further on. Is this really necessary? HAZEL You mean there \nare parts of the regular expression which do not contain any marks. Traversing these parts is often super\u00ad.uous \nbecause nothing is changed anyway, but our algorithm currently evaluates the whole regular expression \neven if there are no marks. CODY We could add a .ag at the root of each subexpression indi\u00adcating that \nthe respective subexpression does not contain any mark at all. This could also improve the performance \nin the .nite case, since subexpressions without marks can be shared instead of copied by the shiftw function. \nTHEO I d prefer to use the term weights when talking about the semiring implementation. When you say \nmarks you mean weights that are non-zero. CODY Right. Let me change the implementation. CODY leaves. \nSCENE II. LAZINESS THEO and HAZEL still at the desk. CODY returns. CODY (smiling) Hi guys, it works. \nI had to make some modi.ca\u00adtions in the code, but it s still a slick program. You can check out the new \nversion now. HAZEL What did you do? CODY First of all, I added a boolean .eld active to the data type \nREGw. This .eld should be False for a regular expression without non-zero weights. If a weight is shifted \ninto a sub\u00adexpression the corresponding node is marked as active. shiftw m (SYMw f) c = let .n = m . \nfc in (symw f) {active = .n = zero, .nalw = .n}  HAZEL So the new .eld is a .ag that tells whether there \nare any non-zero weights in a marked regular expression. We need an extra .ag because we cannot deduce \nthis information from the values of emptyw and .nalw alone. An expression might contain non-zero weights \neven if the value of .nalw is zero. CODY Right. The smart constructors propagate the .ag as one would \nexpect. Here is the modi.ed de.nition of seqw, the other smart constructors need to be modi.ed in a similar \nfashion: seqw :: Semiring s . REGw cs . REGw cs . REGw cs seqw pq = REGw {active = active p . active \nq, emptyw = emptyw p . emptyw q, .nalw = .nala p . emptyw q . .nala q, regw = SEQw pq} HAZEL What is \n.nala? CODY It s an alternative to .nalw that takes the active .ag into account. .nala :: Semiring s \n. REGw cs . s .nala r = if active r then .nalw r else zero HAZEL How does this work? CODY It blocks \nthe recursive computation of .nalw for inactive regular expressions. This works because of lazy evaluation: \nif the given expression is inactive, this means that it does not contain any non-zero weights. Thus, \nwe know that the result of computing the value for .nalw is zero. But instead of computing this zero \nrecursively by traversing the descendants, we just set it to zero and ignore the descendants. HAZEL This \nonly works if the value of the active .eld can be accessed without traversing the whole expression. This \nis why we need special constructors for constructing an initial regular expression with all weights set \nto zero. CODY Yes, for example, a constructor function for sequences with\u00adout non-zero weights can be \nde.ned as follows: seq p q = REGw {active = False, emptyw = emptyw p . emptyw q, .nalw = zero, regw = \nSEQw pq} The difference to seqw is in the de.nition of the active and .nalw .elds, which are set to False \nand zero, respectively. HAZEL Ok, I guess we also need new functions alt and rep for initial regular \nexpressions where all weights are zero. CODY Right. The last change is to prevent the shiftw function \nfrom traversing (and copying) inactive subexpressions. This can be easily implemented by introducing \na wrapper function in the de.nition of shiftw: shiftw :: (Eq s, Semiring s) . s . REGw cs . c . REGw \ncs shiftw mrc | active r . m= zero = stepw m (regw r) c | otherwise = r where stepw is the old de.nition \nof shiftw with recursive calls to shiftw. The only change is the de.nition for the SYMw case, as I showed \nyou before. 2 THEO Ok, .ne (tired of the practitioners conversation). How about trying it out now? CODY \nOk, let s try a nbn with the new implementation. We only have to use the variants of our smart constructors \nthat create inactive regular expressions. ghci> let a = symw ( a ==) ghci> let b = symw ( b ==) ghci> \nlet anbn = epsw alt seq a (anbn seq b) ghci> matchw anbn \"\" True ghci> matchw anbn \"ab\" True ghci> matchw \nanbn \"aabb\" True ghci> matchw anbn \"aabbb\" False  THEO Impressive. So, what is the class of languages \nthat we can match with this kind of in.nite regular expressions? HAZEL I guess it is possible to de.ne \nan in.nite regular expression for every context-free language. We only have to avoid left recursion. \nCODY Right. Every recursive call has to be guarded by a symbol, just as with parser combinators. THEO \nIsee.ThenitisenoughifthegrammarisinGreibachnormal form, i. e., every right-hand side of a rule starts \nwith a symbol. CODY Exactly. But, in addition, regular operators are allowed as well, just as in extended \nBackus-Naur form. You can use stars and nested alternatives as well. HAZEL Pretty cool. And I think we \ncan recognize even more lan\u00ad guages. Some context-sensitive languages should work as well. THEO How should \nthis be possible? HAZEL By using the power of Haskell computations. It should be possible to construct \nin.nite regular expressions in which each alternative is guarded by a symbol and remaining expressions \ncan be computed by arbitrary Haskell functions. Let s try to nbn specify an in.nite regular expression \nfor the language ac n n (more precisely, {a nbn c | n ? 0}), which as we all know is not context-free. \nTHEO A .rst approach would be something like the following. (Scribbles on the whiteboard.) e | abc | \naabbcc | aaabbbccc | ... CODY Unfortunately, there are in.nitely many alternatives. If we generate them \nrecursively, the recursive calls are not guarded by a symbol. But we can use distributivity of choice \nand sequence. HAZEL Ah! Finally, we are using an interesting semiring law: e | a(bc | a(bbcc | a(bbbccc \n| a(...)))) Now every in.nite alternative of a choice is guarded by the sym\u00adbol a. Hence, our algorithm \nonly traverses the corresponding subexpression if the input contains another a. CODY So, let s see! \nWe .rst de.ne functions to generate a given number of b s and c s. (Typing into GHCi again.) ghci> let \nbs n = replicate n (symw ( b ==)) ghci> let cs n = replicate n (symw ( c ==)) Then we use them to build \nour expression. (Continues typing.) 2 These modi.cations not only allow in.nite regular expressions, \nthey also affect the performance of the benchmarks discussed in Act II. The .rst benchmark runs in about \n60% of the original run time. The run time of the second is roughly 20% worse. Memory usage does not \nchange sigini.cantly. ghci> let bcs n = foldr1 seq (bs n ++ cs n) ghci> let a = symw ( a ==) ghci> let \nabc n = a seq alt (bcs n) (abc (n+1)) ghci> let anbncn = epsw alt abc 1  THEO Fairly complicated! Can \nyou check it? CODY enters some examples. ghci> matchw anbncn \"\" True ghci> matchw anbncn \"abc\" True ghci> \nmatchw anbncn \"aabbcc\" True ghci> matchw anbncn \"aabbbcc\" False  THEO Great, it works. Impressive! SCENE \nIII. REVIEW The three at the coffee machine. HAZEL Good that you told us about Glushkov s construction. \nTHEO We ve worked for quite a long time on the regular expres\u00adsion problem now, but did we get somewhere? \nHAZEL Well, we have a cute program, elegant, ef.cient, concise, solving a relevant problem. What else \ndo you want? CODY What are we gonna do with it? Is it something people might be interested in? THEO I \n.nd it interesting, but that doesn t count. Why don t we ask external reviewers? Isn t there a conference \ndeadline coming up for nice programs (smiling)? CODY and HAZEL (together) ICFP. THEO ICFP? CODY Yes, \nthey collect functional pearls elegant, instructive, and fun essays on functional programming. THEO \nBut how do we make our story a fun essay? The three turn to the audience, bright smiles on their faces! \n EPILOGUE Regular expressions were introduced by Stephen C. Kleene in his 1956 paper [Kleene 1956], \nwhere he was interested in charac\u00adterizing the behavior of McCulloch-Pitts nerve (neural) nets and .nite \nautomata, see also the seminal paper [Rabin and Scott 1959] by Michael O. Rabin and Dana Scott. Victor \nM. Glushkov s paper from 1960, [Glushkov 1960], is another early paper where regu\u00ad lar expressions are \ntranslated into .nite-state automata, but there are many more, such as the paper by Robert McNaughton \nand H. Yamada, [McNaughton and Yamada 1960]. Ken Thompson s pa\u00adper from 1968 is the .rst to describe \nregular expression matching [Thompson 1968]. The idea of introducing weights into .nite automata goes \nback to a paper by Marcel P. Sch\u00fctzenberger, [Sch\u00fctzenberger 1961]; weighted regular expressions came \nup later. A good reference for the weighted setting is the Handbook of Weighted Automata [Droste et al. \n2009]; one of the papers that is concerned with several weighted automata constructions is [Allauzen \nand Mohri 2006]. The paper [Caron and Flouret 2003] is one of the papers that focuses on Glushkov s construction \nin the weighted setting. What we nowadays call Greibach normal form is de.ned in Sheila A. Greibach s \n1965 paper [Greibach 1965]. Haskell is a lazy, purely functional programming language. A historical overview \nis presented in [Hudak et al. 2007]. There are several implementations of regular expressions in Haskell \n[Haskell Wiki]. Some of these are bindings to existing C libraries, others are implementations of common \nalgorithms in Haskell. In comparison with these implementations our approach is much more concise and \nelegant, but can still compete with regard to ef.ciency. The experiments were carried out using GHC version \n6.10.4 with -O2 optimizations. The Google library can be found at http://code.google.com/ p/re2/, the \naccompanying blog post at http://google-opensource. blogspot.com/2010/03/re2-principled-approach-to-regular. \nhtml. REFERENCES C. Allauzen and M. Mohri. A uni.ed construction of the Glushkov, follow, and Antimirov \nautomata. In R. Kralovic and P. Urzyczyn, editors, Mathematical Foundations of Computer Science 2006 \n(MFCS 2006), Star\u00e1 Lesn\u00e1, Slovakia, volume 4162 of Lecture Notes in Computer Science, pages 110 121. \nSpringer, 2006. P. Caron and M. Flouret. From Glushkov WFAs to rational expressions. In Z. \u00c9sik and \nZ. F\u00fcl\u00f6p, editors, Developments in Language Theory, 7th International Conference (DLT 2003), Szeged, \nHungary, volume 2710 of Lecture Notes in Computer Science, pages 183 193. Springer, 2003. M. Droste, \nW. Kuich, and H. Vogler. Handbook of Weighted Automata. Springer, New York, 2009. V. M. Glushkov. On \na synthesis algorithm for abstract automata. Ukr. Matem. Zhurnal, 12(2):147 156, 1960. S. A. Greibach. \nA new normal-form theorem for context-free phrase structure grammars. J. ACM, 12(1):42 52, 1965. Haskell \nWiki. Haskell regular expressions. http://www.haskell.org/ haskellwiki/Regular_expressions. P. Hudak, \nJ. Hughes, S. L. Peyton-Jones, and P. Wadler. A history of Haskell: being lazy with class. In Third ACM \nSIGPLAN History of Pro\u00adgramming Languages Conference (HOPL-III), San Diego, California, pages 1 55. ACM, \n2007. S. Kleene. Representation of events in nerve nets and .nite automata. In C. Shannon and J. McCarthy, \neditors, Automata Studies, pages 3 42. Princeton University Press, Princeton, N.J., 1956. R. McNaughton \nand H. Yamada. Regular expressions and state graphs for automata. IEEE Transactions on Electronic Computers, \n9(1):39 47, 1960. M. O. Rabin and D. Scott. Finite automata and their decision problems. IBM journal \nof research and development, 3(2):114 125, 1959. M. P. Sch\u00fctzenberger. On the de.nition of a family of \nautomata. Informa\u00adtion and Control, 4(2 3):245 270, 1961. K. Thompson. Programming techniques: Regular \nexpression search algo\u00adrithm. Commun. ACM, 11(6):419 422, 1968.  \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Cody, Hazel, and Theo, two experienced Haskell programmers and an expert in automata theory, develop an elegant Haskell program for matching regular expressions: (i) the program is purely functional; (ii) it is overloaded over arbitrary semirings, which not only allows to solve the ordinary matching problem but also supports other applications like computing leftmost longest matchings or the number of matchings, all with a single algorithm; (iii) it is more powerful than other matchers, as it can be used for parsing every context-free language by taking advantage of laziness.</p> <p>The developed program is based on an old technique to turn regular expressions into finite automata which makes it efficient both in terms of worst-case time and space bounds and actual performance: despite its simplicity, the Haskell implementation can compete with a recently published professional C++ program for the same problem.</p>", "authors": [{"name": "Sebastian Fischer", "author_profile_id": "81330491065", "affiliation": "Christian-Albrechts University of Kiel, Kiel, Germany", "person_id": "P2338234", "email_address": "", "orcid_id": ""}, {"name": "Frank Huch", "author_profile_id": "81100099102", "affiliation": "Christian-Albrechts University of Kiel, Kiel, Germany", "person_id": "P2338235", "email_address": "", "orcid_id": ""}, {"name": "Thomas Wilke", "author_profile_id": "81332535409", "affiliation": "Christian-Albrechts University of Kiel, Kiel, Germany", "person_id": "P2338236", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863594", "year": "2010", "article_id": "1863594", "conference": "ICFP", "title": "A play on regular expressions: functional pearl", "url": "http://dl.acm.org/citation.cfm?id=1863594"}