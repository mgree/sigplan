{"article_publication_date": "09-27-2010", "fulltext": "\n Specifying and Verifying Sparse Matrix Codes * Gilad Arnold Johannes H\u00f6lzl Ali Sinan K\u00f6ksal University \nof California, Berkeley Technische Universit\u00e4t M\u00fcnchen \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne arnold@cs.berkeley.edu \nhoelzl@in.tum.de alisinan.koksal@ep..ch Rastislav Bod\u00edk Mooly Sagiv University of California, Berkeley \nTel Aviv University bodik@cs.berkeley.edu msagiv@acm.org Abstract Sparse matrix formats are typically \nimplemented with low-level imperative programs. The optimized nature of these implementa\u00adtions hides \nthe structural organization of the sparse format and complicates its veri.cation. We de.ne a variable-free \nfunctional language (LL) in which even advanced formats can be expressed naturally, as a pipeline-style \ncomposition of smaller construction steps. We translate LL programs to Isabelle/HOL and describe a proof \nsystem based on parametric predicates for tracking relation\u00adship between mathematical vectors and their \nconcrete representa\u00adtions. This proof theory automatically veri.es full functional cor\u00adrectness of many \nformats. We show that it is reusable and extensible to hierarchical sparse formats. Categories and Subject \nDescriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations Specialized application lan\u00adguages; \nF.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs Mechanical \nVeri.ca\u00adtion General Terms Languages, Veri.cation 1. Introduction Sparse matrix formats compress large \nmatrices with a small number of nonzero elements into a more compact representation. The goal is to both \nreduce memory footprint and increase ef.ciency of op\u00aderations such as sparse matrix-vector multiplication \n(SpMV). More than .fty formats have been developed; the reason for this diversity is that a format may \nimprove memory locality in a given memory * Research supported by Microsoft (Award #024263) and Intel \n(Award #024894) funding and by matching funding by U.C. Discovery (Award #DIG07-10227). Additional support \ncomes from Par Lab af.liates National Instruments, NEC, Nokia, NVIDIA, Samsung, and Sun Microsystems. \nThis work was done while visiting Stanford University supported in part by grants NSF CNS-050955 and \nNSF CCF-0430378 with additional sup\u00adport from DARPA. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright \nc &#38;#169; 2010 ACM 978-1-60558-794-3/10/09. . . $10.00 hierarchy, expose parallelism that .ts the \nhardware, and tailor the layout to the operations that will be performed on the matrix. The development \nof a sparse matrix format is nontrivial; formats exploit algebraic properties such as commutativity, \nassociativity and zero; have to judiciously choose between linear and random access to ar\u00adray data to \nimprove cache locality, memory bandwidth and use of vector instructions. Sparse codes are used heavily \nin scienti.c ap\u00adplications, simulations and data mining, as well as other domains. It is expected that \nmore formats will be designed to support future (parallel) platforms. Our goal is to simplify their development. \nSparse matrix codes are typically implemented using imperative languages like C and Fortran. This gives \nprogrammers control over low-level details of the computation, allowing them to create optimized implementations. \nHowever, imperative implementations obfuscate the structure of the format because logically independent \nsteps of sparse matrix construction are fused, resulting in code with loop nests that contain complex \narray indirections, in-place data mutation and other low-level optimizations. Not only is the code hard \nto read, it is also challenging to verify. In fact, we failed to verify the functional correctness of \neven simple formats using several state-of-the-art tools. The key reason was that describing the properties \nof the format construction expressed using such low-level implementations required complex invariants \nthat were hard to formulate. Consequently, we sought to raise the level of abstraction in programming \nsparse matrix formats. We describe a new approach to implementing and verifying sparse matrix codes. \nThe main idea is to specify sparse codes as functional programs, where a computation is a sequence of \nhigh\u00adlevel transformations on lists. We then use Isabelle/HOL to verify full functional correctness of \nprograms. We identify a little language (LL) for specifying a variety of sparse matrix formats. LL is \na strongly typed, variable-free func\u00adtional programming language in the spirit of FP [1]. It is also \nin.u\u00adenced by such languages as APL, J, NESL and Python, but favors simplicity and ease of programming \nover generality and terseness. LL provides several built-in functions and combinators for opera\u00adtions \nover vectors and matrices common in sparse formats. LL is restricted by design, lacking custom higher-order \nfunctions, recur\u00adsive de.nitions, and a generic reduction operator. These limitations of LL, as well \nas its purely functional semantics, facilitate auto\u00admatic veri.cation of sparse codes. The contributions \nof this paper can be summarized as follows. We design a variable-free functional language for sparse \nmatrix codes. We show how interesting and complex sparse formats can be naturally and concisely expressed \nin LL (Section 3).  We describe a powerful proof method for automatic veri.cation of sparse matrix \ncodes using Isabelle/HOL [11] (Section 4).  We evaluate the reusability of proof rules in our theory \nand its extensibility to proving additional formats. We show that our language and veri.er can accommodate \ncomplex formats including Jagged Diagonals (JAD) and Coordinate (COO), as well as hierarchical formats \nincluding Sparse CSR (SCSR), register-and cache-blocking schemes (Section 5). As far as we know, this \nis the .rst successful attempt in proving full functional correctness of operations on such formats. \n We are currently writing a compiler which automatically generates ef.cient low-level code from LL programs. \n 2. Overview This section outlines our solutions for implementing and verifying sparse matrix programs. \nWe demonstrate our language using the JAD sparse format, and the proof system using the CSR sparse format. \nThese formats are introduced properly in Section 3; in this section, we will make do with an informal \noverview of the formats and the examples shown in Fig. 1. 2.1 Sparse matrix codes in the LL language \nSparse matrix formats are usually constructed with a sequence of transformations. For example, a JAD \nsparse matrix is constructed in three steps, by (i) compressing each row in the dense matrix; (ii) sorting \ncompressed rows by their length; and (iii) transposing the rows. Ef.cient imperative implementations \nusually fuse these distinct steps, which complicates code comprehension and mainte\u00adnance. We de.ne a \nsmall functional language that keeps these steps separate. The fusion, necessary for performance, will \nbe performed by a data-parallel compiler (which is under development and out\u00adside the scope of this paper). \nLet us compare the characteristics of imperative and functional implementations of JAD format construction. \nConsider .rst the C code that compresses a dense matrix M into the JAD format, represented by arrays \nP, D, J,and V. The low-level code reads and writes a single word at a time, relies heavily on array indirections \n(i.e., array accesses whose index expressions are themselves array accesses), and explicitly spells out \nloop boundaries. The code does not distinguish the three construction steps provides little insight into \nthe JAD format: lenperm (M, P); /* obtain row permutation */ for (d=k=0;d<n;d++) { kk=k; for (i=0;i<n;i++) \n{ for (j =nz=0;j<m;j++) if (M[P[i]][j]) if (++nz > d) break; if (j < m) { J[k] = j; V[k] = M[P[i]][j]; \nk++; }} if (k == kk) break; D[d]=k;} Contrast the C code with this LL program, which is a composition \nof three functions corresponding to the steps in JAD construction. The function composition operator \nis ->. def jad: csr -> lenperm -> (fst, snd -> trans) LL is a functional language rooted in the variable-free \nstyle of FP/FL [1], which means that functions do not refer to their 01 a 000 P: [1302] R: [1 3 3 5] \nB bc 00 C D: [3 5] @A J: [00113] 0000 J: [01013] V: [abcde] 0 d 0 e V: [bdace] (a) Dense matrix. (b) \nJAD sparse format. (c) CSR sparse format. Figure 1. Two sparse matrix formats. Shown are imperative \nrepresentations; their LL counterparts are in Figures 2 and 4. arguments by name; instead, they transform \na single, unnamed input parameter. For example, if the input to a function is a pair, then a function \nextracts the .rst element using the built-in function fst. LL is strongly typed and datatypes include \nnumbers, Boolean values, pairs and lists. Vectors are represented by lists, and matrices by lists of \n(row) vectors. Compressed matrix representations use a variety of nested data structures built of lists \nand pairs. The three steps of JAD construction in LL are visualized in Fig. 2, which shows the dense \nmatrix, the resulting JAD matrix, as well as the intermediate values of JAD construction. Notice that \nthe JAD representation in LL (the result in Fig. 2) is more abstract than the JAD format in C (Fig. 1(b)). \nWhere LL formats rely on lists of lists, the C formats linearize the outer list and create explicit indexing \nstructures to access the inner lists. LL thus frees the programmer from reasoning about these optimized \ndata structure layouts, eliminating dependence on explicit array indirection. The .rst step compresses \nrows by invoking the constructor for the sparse format CSR. In the second step, the function lenperm \nsorts the compressed rows by decreasing length: def lenperm: [(len, (#, id))] -> sort -> rev -> [snd] \n-> unzip Here, the syntax [f] denotes a map that applies the function f over the elements of the input \nlist: len, # and id return the length of the current element, the position index of that element in the \nlist, and the element itself (identity), respectively. The third-step function (fst, snd -> trans) takes \na pair and produces a pair in which .rst element is unchanged and the second element is transposed. In \nsummary, LL lifts an intricate imperative computation into a cleaner functional form, exposes high-level \nstages and the .ow of data from one stage to another, and encourages the programmer to think about invariants \nover intermediate results. These bene.ts are not merely due to the use of functional programming. We \nbe\u00adlieve that they are equally attributed to our careful selection of a very simple subset of functional \nlanguage features, designed with the sparse matrix domain in mind. In particular, LL does not sup\u00adport \nlambda abstractions, which encourages expressing computa\u00adtions as pipelines of functions. LL also excludes \nde.nitions of re\u00adcursive functions and a general fold operator, both of which are compensated for by \na versatile set of built-ins (e.g., zip and sum) and combinators for handling lists (e.g., map and filter). \nThese restrictions contribute to our ability to automatically verify LL pro\u00adgrams because they sidestep \nthe need to infer induction invariants, a hard task for automated tools. The LL language is introduced \nin detail in Section 3. We have recently developed compiler for LL that relies on opti\u00admization techniques \npioneered in NESL [3] and later generalized in Data Parallel Haskell [4]. Thanks to LL s simplicity, \nwe were able to simplify the compilation and indentify more opportunities for optimization. Initial results \nindicate that code generated for real\u00adworld formats such as register-blocked CSR (see Section 5.2) runs \nas fast as a hand-optimized code and scales well to multiple cores. `\u00b4 \u00b41 1302 0` ` \u00b4C 0  1 000 `\u00b4 \n1302 0` \u00b41 ` \u00b4 B c 00 C ` \u00b4 \u00ab B C lenperm (fst, snd -> trans) B csr B C 0000 @ A \u00b7 ---. A -------. \n-----------------. B C ` `\u00b4 (1,c) (3,e) ` \u00b4 0 e A (0,b) (1,c) (1,d) (3,e) \u00b4 (0,a)  @ @`\u00b4 \u00b7 Figure \n2. The three steps of JAD format construction. Shown are the dense matrix, the JAD matrix, and the two \nintermediate values. 2.2 Verifying sparse matrix codes There are at least two arguments for full functional \nveri.cation of sparse matrix codes. First, classical static typing is insuf.cient for static bug detection \nbecause these programs contain array in\u00addirection, whose memory safety would be typically guaranteed \nonly with run-time safety checks. Dependent type systems may be able to prove memory safety but, in our \nexperience, the necessary dependent-type predicates would need to capture invariants nearly as complex \nas those that we encountered during full functional veri\u00ad.cation. For example, to prove full functional \ncorrectness, one may need to show that a list is some permutation of a subset of values in another list; \nto prove memory safety, one may need to show that the values in a list are smaller than the length of \nanother list. It thus seemed to us that with a little extra effort, we can use theorem de.ne suitable \nrepresentation relations for the objects that arise in sparse matrix programs. We use Isabelle/HOL as \nour underlying theorem prover. We em\u00adbed LL functions in Isabelle using typed .-calculus and Isabelle \nlibraries. Our proofs deploy two techniques: (a) term simpli.ca\u00adtion, which rewrites subterms in functions \ninto simpler, equivalent ones; and (b) introduction, which substitutes a proof goal with a certain term \nfor alternative goal(s) that do not contain the term, and whose validity implies the validity of the \noriginal goal. In our ex\u00adample, term simpli.cation unfolds the de.nitions of csrmv and csr and applies \nstandard rules for simplifying function application and composition, map and .lter operations on lists, \nand extraction of elements from pairs. This results in the goal [enum -> [snd != 0 ? ] ->[snd x[fst]] \n-> sum](A) * proving to extend safety to full functional correctness. The second reason for full functional \nveri.cation is synthesis of mc B \u00b7 y sparse matrix programs, including the discovery of new formats. \nIn inductive synthesis, which is conceptually a search over a space of plausible (i.e., potentially semantically \nincorrect) implementations, a full functional veri.er is a prerequisite for synthesis because it is an \narbiter of correctness of the selected implementation. Synthesis, however, is outside the scope of this \npaper. Before settling on the design presented in this paper, we set as our goal the full functional \nveri.cation of imperative sparse code, in the style presented in Section 2.1. However, even the simple \nCSR format turned out to be rather overwhelming. We attempted to verify its correctness in multiple ways: \n(i) manually with Hoare\u00adstyle logic, both with .rst-order predicates and inductive predi\u00adcates; (ii) \nwith ESC/Java [6]; (iii) with TVLA [13]; and (iv) using a SAT-based bounded model checker. The results \nwere unsatisfactory either because it took weeks to develop the necessary invariants (i, ii), the abstraction \nwas too complex for us to manage (iii), or be\u00adcause the checker scaled poorly (iv). Eventually, we concluded \nthat we needed to verify sparse codes at a higher level of abstraction (and separately compile the veri.ed \ncode into ef.cient low-level code). Turning our attention to functional programs allowed us to replace \nexplicit loops over arrays with maps and a few .xed re\u00adductions over lists, which in turn simpli.ed the \nformulation and encapsulation of inductive invariants. Let us the simple CSR format to give the rationale \nfor the design of our proof system. Suppose that A and x are concrete language objects that, respectively, \ncontain dense representations of a mathematical matrix B and a vector y. We want to prove that the product \nof the CSR-compressed A with x produces an object that is a valid (dense) representation of the vector \nB \u00b7 y. Note that the product is CSR-speci.c. Formally, our veri.cation goal is csrmv(csr(A),x) mc B \n\u00b7 y (1) The LL function on the left enumerates each row of A into a list of (column index, value) pairs, \nthen .lters out pairs whose second element is zero ([snd != 0 ? ]). For the remaining pairs, it multiplies \nthe second (nonzero) component with the value of x at the index given by the .rst component ([snd * x[fst]]). \nFinally, it sums the resulting products (sum). So far, simpli.cation has done a good job. To carry out \nthe next step of the proof, we observe that the missing zeros do not affect the result of the computation, \nso we would like to simplify the left-hand-side by rewriting away the .l\u00adter ([snd != 0 ? ]); this would \neffectively desparsify the left\u00adhand side, moving it closer to the mathematical right-hand-side. Unfortunately, \nstandard simpli.cation available in prover libraries cannot perform the rewrite; we would need to add \na rule tailored to this format. The hypothetical rule, shown below, would match p with snd != 0 and f \nwith snd * x[fst]. .y. \u00acp(y) -. f(y)=0 [p ?]-> [f] -> sum = [f] -> sum The rule would achieve the desired \nsimpli.cation but we refrain from adding such a rule because it would take a considerable effort to prove \nit. Additionally, the rule would of little use in cases where the LL operations appear in just a slightly \nsyntactically different way. We will instead rely on introduction which, by substituting the current \ngoal with a set of goals, isolates independent pieces of rea\u00adsoning. Introduction rules tend to be more \ngeneral than simpli.ca\u00adtion rules because they are concerned with a single construct from the current \ngoal. Also, the validity of introduction rules is easier to establish. Our .rst introduction rule substitutes \nin the goal (1) the whole The goal expresses the relationship between a mathematical object result vector \nwith a single element of that vector. In effect, this removes the outermost map from the LL function \non the left-hand and its concrete counterpart with the representation relation a kc b, which states \nthat the concrete object a represents the mathematical side. Semi-formally, the rule for map can be stated \nas follows: vector b:for all i<k, a[i] equals bi and the lengths of a and b length of A is m .i<m.f(A[i])= \nBi are k. In the course of the proof, we may need to track relationships [f](A) mc B on various kinds \nof concrete objects; one of our contributions is to (2)  In goal (1), f matches the entire chain of \nenum -> ... -> sum and the new subgoals are (i) length of A is m (ii) .i<m.  enum -> [snd != 0 ?] -> \nX [snd * x[fst]] -> sum (A[i])= Bi,j \u00b7 yj j<n We now need a second introduction step to remove the summa\u00adtion \non both sides of the equality: instead of requiring equivalence between sums of sequences of numbers, \nwe will require equiva\u00adlence between the values in the sequences themselves. In order for such a rule \nto be general enough, we need to permit arbitrary permutations of the values in a sequence to prove programs \nthat exploit associativity and commutativity of addition. A hypotheti\u00adcal rule may look as follows, where \n[xi|p(xi)]i=a,...,a+d denotes a construction of an ordered list of elements out of xa,...,xa+d that satisfy \np. ' .n= n, permutation P. ' n f(A[i]) c [Bi,j | Bi,j=0]j=P0,...,Pn-1 X sum(f(A[i])) = Bi,j j<n This \nrule is problematic for two reasons. First, it is more com\u00adplex than what we may want to prove. For example, \nthe premise constructs a .ltered and permuted mathematical vector on the right\u00adhand side (via list comprehension), \nrather than keeping the mathe\u00admatical object untouched. This might hinder our ability to link our proof \ngoal to the original input matrix in the assumptions of the the\u00adorem. Second, the rule is not as general \nas we would like because a concrete representation may contain zeros. k Our approach is to enrich the \nrepresentation relation (a c b). This relation uses plain equality to relate single elements from the \ntwo vector objects, which limits its applicability to more subtle mappings. To express a relation where, \nsay, each element in a con\u00adcrete representation equals the corresponding vector element mul\u00adtiplied by \nsome value, we parameterize the representation relation with an inner relation that describes how individual \nelements repre\u00adsent their mathematical counterparts. Individual elements need not be scalars; they could \nbe, recursively, lists. Therefore, inner rela\u00adtions could be parameterized by further inner relations. \nOur domain proof theory for sparse matrices is novel in two ways. First, we de.ne common representation \nrelations that occur in our domain. Our infrastructure is powerful because we (a) in\u00adsist on relaxing \ninvariants as much as possible (e.g., zeros may still be present in a compressed representation); (b) \nencapsulate many quanti.cations and implications in the representation rela\u00adtions (e.g., universal quanti.cation \non all indexes of a vector, exis\u00adtence of a permutation); (c) include necessary integrity constraints \nin the representation relations (e.g., lengths must match). The repre\u00adsentation relations we de.ne include \nindexed list (ilist), where the element at position i represents the ith vector element; value list (vlist), \nin which all nonzero values are represented; and associative list (alist), which contains index-value \npairs. These representation relations raise the level of abstraction and focus theory develop\u00adment on \nthese prevalent data representation. The use of represen\u00adtation relations also prevents oversimpli.cation \nof proof terms by concealing their internal conjuncts from Isabelle s simpli.er. The second novelty is \nparameterizing the inner predicate, which describes how the vector elements represent their mathematical \ncounterparts. In the case of a vector of numbers, we use equal\u00adity. For matrices, the inner relation \nrelates a single row to its con\u00adcrete indexed-list representation (ilist); technically, the inner rela\u00adtion \npredicate is a parameter to the (outer) representation predi\u00adcate for the whole matrix. In addition to \nreducing the number of rules, parameterization helps with syntactic matching and substitu\u00adtion of inner \ncomparators during introduction. For example, with a parameterized relation, an introduction rule for \nmap similar to that in Eq. (2) can be written more generally and concisely: the conclusion of the rule \ncontains an indexed-list representation rela\u00adtion where the concrete object is the term [f](x) (i.e., \nmap with an arbitrary function f over x) and the inner representation rela\u00adtion is some arbitrary predicate \nP our parameter. The premise of the rule is again an indexed-list representation relation where the concrete \nobject is x and the inner representation relation is .iab.P (i, a, f(b)). Fortunately, Isabelle can match \nand substi\u00adtute terms that contain parameters such as P (as well as f and x); these rules can thus be \napplied automatically. The representation relations are described in Section 4. Sec\u00adtion 5 evaluates \nwhether they improve reuse of rules and thus sim\u00adplify theory development; we argue that the principles \nused in our approach are crucial for proofs on nested data representations.It may be interesting to apply \nsuch parameterized representation re\u00adlations also in other domains.  3. High-Level Sparse Matrix Programming \nSparse matrix codes can often be decomposed into sequences of high-level transformations. This section \ndescribes LL and its use for expressing such computations naturally and concisely. 3.1 Introduction to \nLL The LL language constructs are presented in Fig. 3. The semantics of each construct is shown, either \nby translation to Isabelle/HOL .-calculus and standard library for lists [11], or by de-sugaring to simpler \nLL constructs. The language includes (a) general functions such as identity, equality, constants, conditional \nbranching, and a name binding form used for assigning names to components of an input value; (b) construction \nof pairs/tuples and extraction of values from pairs; (c) pipeline-and application-style composition, \nas well as a curried application operator; (d) standard arithmetic operators and comparators; (e) Boolean \nlogic operators; and (f) list handling functions (e.g., distribution of values onto lists, zipping, enumeration, \nconcatenation) and combinators (map, .lter, and a uni.ed comprehension syntax). 3.2 Speci.cation of \nsparse codes using LL Compressed sparse rows (CSR). This format compresses each row by storing nonzero \nvalues together with their column indexes. The resulting sequence of compressed rows is not further com\u00adpressed, \nso empty (all zero) rows are retained. This enables random access to the beginning of each row, but requires \nlinear traversal to extract a particular element out of a row. CSR is widely used be\u00adcause it is relatively \nsimple and entails good memory locality for row-wise computations such as SpMV. Implementing CSR in C, \nshown below,1 is not trivial. Traversal of the dense matrix (construction) or the compressed rows (SpMV) \nis done with nested loops. Single values are copied (construction) or extracted (SpMV) through array \nindirection. Compressed row boundaries need to be stored and observed. That said, the resulting SpMV \ncode is rather ef.cient as the inner product of each row is incrementally accumulated, using very few \ninstructions and avoid\u00ading unnecessary memory accesses. Applying CSR construction to the 4-by-4 matrix \nin Fig. 1(a) yields the data structure in Fig. 1(c). 1 For brevity, we omit memory allocation and initialization \nand assume that matrix dimensions are known at compile-time. id .x. x eq (=), neq (!=) .(x, y).x = y, \n.(x, y).x = y n, true, false .y. n, .y. true, .y. false f ? g | h .x. if fx then gx else hx l1,...,lk \n= f: g ` .(x1,...,xk).g[li/.y. xi](x1,...,xk) \u00b4 . f (f) f (f1,f2,...,fk) .x. (f1 x, f2 x,...,fk x) fst, \nsnd .(x, y).x, .(x, y).y f -> g g . f g(f1,...,fk) (f1,...,fk)-> g g f (f, id) -> g add (+), sub (-), \nmul (*), div (/), mod (%) .(x, y).x + y, .(x, y).x - y,... leq (<=), lt (<), geq (>=), gt (>) .(x, y).x \n= y, .(x, y).x < y,... sum (/+), prod (/*) foldl (op +) 0, foldl (op *) 1 and (&#38;&#38;), or (||) .(x, \ny).x . y, .(x, y).x . y neg (!) .x. \u00acx conj (/&#38;&#38;), disj (/||) foldl (op .) True, foldl (op .) \nFalse len length rev rev sub (f[g]) .(v, i).v ! i subseq (f[g]) .(v, s). map (.i. v ! i) s distl, distr \n.(x, v). map (.y. (x, y)) v, .(v, x). map (.y. (y, x)) v zip, unzip unsplit zip, .l. (map fst, map snd) \nenum .v. zip [0 .. < length v] v concat concat infl .(d, n, v). foldr (.(i, x) v. v[i := x]) v (replicate \nnd) gather .xs. map (.k. (k, map snd (.lter (.(k ' ,v).k = k ' ) xs))) (remdups (map fst xs)) sort sort_key \nfst trans .v. [map (.v. v ! i)(takeWhile (.v. i < length v) v) . i . [0 .. < if v =[] then 0 else length \n(v !0)]] map f map f filter f .lter f [l1,...,ln = f: g ? h] filter (l1,...,ln = f: g); map (l1,...,ln: \nh) Figure 3. LL constructs and their translation to Isabelle/HOL. Here, f, g and h denote functions, \nn a number, and l a label. Alternative in.x, pre.x or mix.x notation is shown in parentheses. f defaults \nto id. Value naming is optional, f and h default to id and g to true. 0` \u00b41 010 1 a 000 (0,a)(1, 0) \n(2, 0) (3, 0) (0,a) [ a 000] [ (0,a) ] b c 0 0 0 0 0 0 0 d 0 e B (0,b)(1,c) ` \u00b4 `\u00b4 \u00b7 (1,d)(3,e) C CB \n(0,b) (1,c) (2, 0) (3, 0) C B BC @ A A@ (0, 0) (1, 0) (2, 0) (3, 0) @A (a) (b) (c) Figure 4. Conceptual \nphases in CSR construction. /* CSR construction. */ /* CSR SpMV. */ for (i=k=0; i<m; i++) { for (i=k=0; \ni<m; i++) for (j=0; j<n; j++) for (y[i]=0; k<R[i]; k++) if (M[i][j] != 0) { y[i] += V[k] x[J[k]]; * J[k] \n= j; V[k] = M[i][j]; k++; } R[i]=k;} Fig. 4 shows the high-level stages in CSR compression men\u00adtioned \nabove. Given (a), each row is enumerated with column in\u00addexes, resulting in (b). Pairs containing a zero \nvalue are then .l\u00adtered, yielding (c). A data.ow view of such a computation is shown in Fig. 5. Notice \nhow similar it is to the following LL function. def csr: [enum -> [snd !=0?]] Using name binding in comprehensions \nmay improve clarity. [ bc 00] [ (0,b)(1,c) ] [ (0,b)(1,c)(2, 0)(3, 0) ] b true [0000] [] [0 d 0 e ][ \n(2,d)(3,e) ]  [ bc 00] (0,b) [ (0,b)(1,c) ] Figure 5. Data.ow view of high-level CSR construction. [enum \n-> [j, v: v !=0?]] Alternatively, one can use an explicit enumeration operator inside comprehensions. \nThe following variant appears more integrated , but in fact entails the exact same semantics. [[v: v \n!= 0 ? (#, v)]]  0` \u00b41 . (0,a) .. 01 . . .. aq B (0,b)(1,c) ` \u00b4 `\u00b4 \u00b7 (1,d)(3,e) C `\u00b4 `\u00b4 `\u00b4 . BC A \n: (0,b)(1,c) J : 01 V : bc . B bq + cr 0 dr + et C `\u00b4 @A BC .. (b, q)(c, r) BC . . @A . .. . ` \u00b4` \n\u00b4` \u00b4 . x : qrst x : qrst x : qrst (a) (b) (c) (d) (e) Figure 6. Conceptual phases in CSR SpMV. A more \nverbose variant uses Python-style comprehension. This variant is de-sugared to the original de.nition. \ndef csr(A): [[(j, v) for j, v in enum(r) if v!=0] for r in A] Fig. 6 shows the stages in CSR SpMV. Each \ncompressed row is multiplied separately, as shown in (b). First, column indexes are separated from nonzero \nvalues as in (c). They are used to retrieve corresponding values from x, pairing them with their respective \nrow values as in (d). Finally, values in pairs are multiplied and the products are summed, yielding the \ninner-product in (e). This maps to the following LL function. def csrmv(A, x): A-> [J, V = unzip: (V, \nx[J]) -> zip -> [mul] -> sum] Here, too, it is possible to write a more integrated variant that bun\u00addles \nmultiplication with the extraction of single values. Although semantically equivalent, the resulting \ncode is less amenable to vec\u00adtorization due to the use of word-level operations. A->[[j,v:v x[j]] -> \nsum]* Jagged diagonals (JAD). This format deploys a clever compres\u00adsion scheme that allows handling of \nsequences of nonzeros from multiple rows, taking advantage of vector instructions. The ith nonzero values \nfrom all rows are laid out consecutively in the com\u00adpressed format, constituting a jagged diagonal . \nSince nonzeros are distributed differently in each row, column indexes need to be stored as well. These \nsteps can be thought of as per-row compres\u00adsion (as shown above for CSR), followed by transposition to \ninvert the direction of compressed rows and ith-element columns. However, packing ith elements in a predetermined \norder e.g., from the .rst to the last row induces a problem: one needs to account for compressed rows \nthat are shorter than other rows that succeed them.2 This is addressed by adding a sorting step between \nrow compression and transposition, in which rows are ordered by decreasing number of nonzeros. The sort \npermutation is stored with the resulting diagonals, so the correct order of rows can be restored. These \nconceptual steps in JAD compression are visualized in Fig. 2 and the LL implementation is shown in Section \n2.1. Fig. 7 shows the high-level steps in JAD SpMV. (b) is obtained by computing, for each diagonal, \nthe cross-product of its induced vector of values with the elements of x corresponding to their column \nindexes. These are transposed to obtain the lists of products in each (nonzero) row as in (c). Products \nin corresponding rows are summed, obtaining (d). In (e), each inner product is paired with 2 Transposition \ninverts columns up to the .rst missing element, below which all other elements are omitted. In this respect \nit is lossy and the equality A = ATT only holds for matrices whose rows are sorted by length. its row \nindex, which are then in.ated to obtain the dense result vector in (f). The following LL function implements \nthese steps. def jadmv((P, D), x): (P, D -> [unzip -> snd * x[fst]] -> trans -> [sum]) -> zip -> infl(0, \nm, id) Other formats. Two additional standard formats are Coordinate (COO) and Compressed Sparse Columns \n(CSC) [10]. COO is a highly-portable compression in which nonzeros are stored together with their row \nand column indexes in a single, arbitrarily ordered sequence. Construction can be implemented in LL as \nfollows. def coo: [i = #: [v: v != 0 ? (i, #, v)]] -> concat COO SpMV is less straightforward: one needs \nto account for the fact that nonzeros of a particular row might be scattered along the compressed list. \nIt is necessary to gather those values prior to computing the inner-product. This is expressed as follows. \ndef coomv (A, x): A-> gather -> [(fst, snd -> [j, v: v x[j]] -> sum)] -> * infl(0, m, id)  A CSC representation \nis obtained by compressing the nonzero values in the column direction, instead of row direction as in \nCSR. In C, it is done by swapping the order of the loops iterating over the dense matrix, and storing \nthe row index with the nonzero values. In LL, it amounts to prepending a transposition to CSR construction. \ndef csc: trans -> csr Like COO, CSC SpMV calls for a gather operation prior to sum\u00adming row cross-products. \ndef cscmv: zip -> [cj, xj: cj -> [i, v: (i, v xj)]] -> * concat -> gather -> [(fst, snd -> sum)] -> \ninfl(0, m, id)  Here, too, the fact that data layout is not in line with the compu\u00adtation entailed by \nmatrix-vector multiplication calls for additional steps to massage the result into a proper vector form. \nIn addition to the above formats, LL can naturally and suc\u00adcinctly describe hierarchical compression. \nThis includes Sparse CSR (SCSR) and different block variants of all of the above. These will be described \nand studied in Section 5.  4. Verifying Sparse Codes using Isabelle/HOL We make use of Isabelle s rich \ninfrastructure in implementing a proof method for sparse matrix codes. This includes the simpli.er and \na powerful tactical language, which is used to combine existing proof methods in forming new ones. All \nparts of our proofs are checked from .rst principles by a small LCF-style kernel. `\u00b4 01 1 0` \u00b41 P \n: 1302 0` bq cr dr et aq \u00b4 \u00b410 \u00ab + cr \u00ab + cr 1, B C ` \u00b4 cr C B C B` \u00b4C B @ A `\u00b4 D : @ + et AB C \n+ et A@3, ` \u00b4@ 0 A ` (1,c) (3,e) `\u00b4 0, + et x : qrst (a) (b) (c) (d) (e) (f) Figure 7. Conceptual phases \nin JAD SpMV. 4.1 Translating LL to Isabelle/HOL Fig. 3 constitutes a shallow embedding [16] of LL in \nIsabelle/HOL, a standard technique when the goal is to verify correctness of pro\u00adgrams in some language. \nIn this approach, the functions and types of an object language (LL) are written directly in the language \nof the theorem prover (typed .-calculus). Subsequent logical formulas relate to these translated programs \nas ordinary HOL objects, which allows to leverage existing support for proving properties of them. The \nCSR implementation in Section 3.2 translates to the following de.nitions, which will be used in our proofs. \ncsr =(.lter(.(j, v).v =0)) . enum and csrmv (A, x)= map (listsum . map (.(x, y).x * y) . unsplit zip \n. (3) (.(J, V ). (V, map (.i. x ! i) J)) . map unzip We now pose the veri.cation theorem: when A index-represents \nthe m \u00d7 n-matrix A ' and x the n-vector x ' , the result of CSR SpMV applied to a CSR version of A and \nto x represents the m-vector that is equal to A ' \u00b7 x ' . ilistM mnA ' A . ilistv nx ' x -. ilistv m \n(.i. Sj<n.A ' ij * x ' j)(csrmv (csr A, x)) (4) The remainder of this section presents the formalism \nand explains the reasoning used in proving this goal.  4.2 Formalizing vector and matrix representations \nWe begin by formalizing vectors and matrices in HOL. Mathemat\u00adical vectors and matrices are formalized \nas functions from indexes to values, namely nat . a and nat . nat . a, respectively; note that the . \ntype constructor is right-associative, hence a ma\u00adtrix is a vector of vectors. Dimensions are not encoded \nin the type itself, and values returned for indexes exceeding the dimensions can be arbitrary, which \nmeans that many functions can represent the same mathematical entity. Concrete representations of dense \nand sparse vectors/matrices are derived from the LL implementa\u00adtion and consist of lists and pairs. Commonly \nused representations include indexed lists, value lists and associative lists, all of which are explained \nbelow. We introduce representation relations (de.ned as predicates in HOL) to link mathematical vectors \nand matrices with different con\u00adcrete representations, for three reasons. First, in proving correctness \nof functions we map operations on concrete objects to their mathe\u00admatical counterparts. This is easy \nto do for indexed list representa\u00adtions but gets unwieldy with others. We hide this complexity inside \nthe de.nitions of the relations. Second, predicates can be used to enforce integrity constraints of the \nrepresentation. For example, an associative list representation requires that index values are unique; \nor the lengths of a list of indexed list representations need to be .xed. Third, for some representations \n(e.g., value list) there exists no injective mapping from concrete objects to abstract ones, forc\u00ading \nus to use relations rather than representation functions.Using relations across the board yields a more \nconsistent and logically lightweight framework. An indexed list representation of an n-vector x ' by \na list x is captured by the ilist predicate. Note that we refrain from .xing vector elements to a speci.c \ntype (e.g., integers) and instead use type parameters a and \u00df to denote the types of inner elements of \nthe mathematical and concrete vectors, respectively. ilist :: nat . (nat . a . \u00df . bool) . (nat . a) \n. [\u00df] . bool ilist nP x ' x .. (length x = n) . (.i<n.P i (x ' i)(x ! i)) The parameter P is a relation \nthat speci.es the representation of each element in the vector. For ordinary vectors, it is equality \nof elements. However, P turns useful for matrix representation, as we can use arbitrary relations to \ndetermine the representation of inner vectors. We introduce abbreviations for the common cases of indexed \nlist representations. ilistv nx ' x .. ilist n (.j. op =) x ' x ilistM mnA ' A .. ilist m (.i. ilistv \nn) A ' A An associative list representation is central to sparse matrix codes as it is often used in \nvector compression. It is captured by the alist predicate. alist :: nat . (nat . a . \u00df . bool) . (a set) \n. (nat . a) . [(nat,\u00df)] . bool alist nP Dx ' x .. distinct (map fst x) . (.(i, v) . set x.P i (x ' i) \nv . i<n) . (.i<n.x ' i . D -. .v. (i, v) . set x) Here, distinct is a predicate stating the uniqueness \nof indexes (i.e., keys) in x. Each element in an associative list must relate to the re\u00adspective vector \nelement, also requiring that index values are within the vector length. Finally, each element in the \nvector that is not a default value (speci.ed by the set of values D) must appear in the representing \nlist. Note that a set of default values accounts for cases where more than one such value exists, as \nin the case of nested vec\u00adtors where each function mapping the valid dimensions to zero is a default \nvalue. Also note that alist does not enforce a particular or\u00adder on elements in the compressed representation, \nnor does it insist that all default values are omitted. Sometimes concrete objects contain only the values \nof the el\u00adements in a given vector, without mention of their indexes. This value list representation \noften occurs prior to computing a cross\u00ador dot-product. It is captured by the vlist predicate, which \nstates that the list of values can be zipped with some list of indexes p to form a proper associative \nlist representation. (The length restriction ensures that no elements are dropped from the tail of x.) \n vlist :: nat . (nat . a . \u00df . bool) . (a set) . (nat . a) . [\u00df] . bool vlist nP Dx ' x .. .p. length \np = length x . alist nP Dx ' (zip px) Additional representations can be incorporated into our theory. \nFor example, when a matrix is compressed into an associative list, a dual-index representation relation \ncan be de.ned similarly to alist.  4.3 Proving correctness of sparse matrix computations We prove Eq. \n(4) using term rewriting and introduction rules. In\u00adtroduction rules are used whenever further rewriting \ncannot be ap\u00adplied. An introduction rule is applied by resolution: applying the rule Gx . Hy -. Fxy to \nthe goal Fab yields two new subgoals, Ga and Hb. The theorem in Eq. (4) makes the following two assumptions, \nilistM mnA ' A (5) ilistv nx ' x (6) which are added to the set of available introduction rules as true \n-. ... . The conclusion of Eq. (4) is our initial proof goal, ilistv m (.i. Sj<n.A ' ij * x ' j)(csrmv \n(csr A) x) (7) Simplifying the goal. We begin by applying Isabelle s simpli.er using Eq. (3) and standard \nrules for pairs, lists, arithmetic and Boolean operators. This removes most of the function abstractions, \ncompositions and pair formations due to the translation from LL. Our new goal is analogous to Eq. (1) \nin Section 2.2. ilistv m (.i. Sj<n.A ' ij * x ' j) (map (.r. listsum (map (.v. snd v * x ! fst v) (.lter \n(.v. snd v =0)(enum r)))) A) (8) Solving the entire goal using rewriting alone calls for simpli.\u00adcation \nrules that are too algorithm-speci.c. For example, the rule (.x . set xs. \u00ac Px -. fx =0) (9) -. listsum \n(map f (.lter Pxs)) = listsum (map fxs) allows further simpli.cation of Eq. (8), but fails for all formats \nthat introduce more complex operations between map and .lter. Introduction rules on representation relations. \nConsider the equation in the conclusion of Eq. (9). We know that it holds when the two lists, xs and \n.lter Pxs, value-represent the same vec\u00adtor. By introducing rules, describing when it is allowed to apply \nmap, .lter and enum operations to value list representations, we prove that the result of listsum in \nEq. (8) equals the mathematical dot-product. Fig. 8 shows the introduction rules used in proving Eq. \n(4). Ap\u00adplication of introduction rules is syntax directed, choosing rules whose conclusion matches the \ncurrent goal. Given Eq. (8), the prover applies ILIST-MAP, which moves the map from the repre\u00adsenting \nobject into the inner representation relation, followed by ILIST-LISTSUM, which substitutes listsum with \nan equivalent no\u00ad 3 The predicate P and the vector z ' are arbitrary, they just help to state that x \nis a list of length m. tion of value-represented rows. This results in ` ' ' ilist m .ir r. vlist n \n(.j. op =) {0} r (map (.v. snd v * x ! fst v) \u00b4 (.lter (.v. snd v =0)(enum r))) (.ij. A ' ij * x ' j) \nA Further simpli.cation is not possible at this point, nor can we modify the vlist relation inside ilist. \nLuckily, ILIST-VLIST matches our goal, lifting the inner vlist to the outermost level and permitting \nto further operate on the concrete parameters of vlist. Note that ILIST-VLIST has two assumptions, resulting \nin new subgoals ilist m ?Q ?B ' A (10) and .i<m. vlist n (.j. op =) {0} (.j. A ' ij * x ' j) ` map (.v. \nsnd v * x ! fst v) \u00b4 (.lter (.v. snd v =0)(enum (A ! i))) (11) In Eq. (10), ?Q and ?B ' are existentially \nquanti.ed variables. They do not get instantiated when we apply ILIST-VLIST, and the subgoal Eq. (10) \nmerely certi.es that A has length n. Therefore, the prover is allowed to instantiate them arbitrarily \nand Eq. (10) is discharged by the assumption Eq. (5). The rules VLIST-MAP, ALIST-FILTER and ALIST-ENUM \ncan now be applied to Eq. (11). Note that applying them amounts to the effect of simpli.cation using \nEq. (9). However, they can be applied regardless of the way in which the three operations map, .lter \nand enum are intertwined. Therefore, they are applicable in numerous cases where the context imposed \nby Eq. (9) is too restrictive. The ALIST-FILTER rule forces us to prove that .lter only re\u00admoves default \nvalues, in the form of the following new subgoals, .i<m. .j<n. .vv ' . \u00ac snd (j, v)=0 . v ' = v * x ! \nj -. v ' .{0} (12) .i<m. '' '' ilist n (.jv v.v = v * x ! j)(.j.A ij * xj)(A ! i) Fortunately, subgoal \nEq. (12) is completely discharged by the sim\u00adpli.er. The remaining goal is solved using the ILIST-MULT, \nILIST\u00adNTH,and ILISTv. ILISTM , as well as the assumptions Eq. (5) and Eq. (6). 4.4 Automating the proof \nThe above proof outline already dictates a simple proof method. Is\u00adabelle s tactical language [15] provides \nus with ample methods and combinators that can be used to implement custom proof tactics. Our proof method \nis implemented as follows. 1. The simpli.er attempts to rewrite the goal until no further rewrites are \napplicable, returning the new goal. If no rewrite rule could be applied, it returns an empty goal. 2. \nThe resolution tactic attempts to apply each of the introduction rules and returns a new goal state for \neach of the matches. It is possible that more than one rule matches a given goal, e.g.  ' ILIST-MAP \nand ILIST-NTH both match ilist n (.i v ' v. v = y ! i) x ' (mapf x), resulting in a sequence of alternative \ngoal states to be proved. Invoking the proof method leads to a depth-.rst search on the combination \nof the two sub-methods. It maintains a sequence of goal states, initially containing only the main goal. \nAfter each ilist n (.iab.P ia (fb)) x ' x ILIST-MAP ilist nP x ' (map fx) ' '' ilist m (.ir r. vlist \nn (.j.op =) {0} r (fr)) AA ILIST-LISTSUM '' ' ilist m (.ir r.r = listsum (fr)) (.i. S j<n.A ij) A ilist \nmQB ' A .i<m. vlist n (Pi)(Di)(f (A ' i) i)(g (A ! i) i) ILIST-VLIST ' '' ilist m (.ir r. vlist n (Pi)(Di)(fr \ni)(gri)) AA alist nP Dx ' x alist m (.i r ' r.P ir ' (f (i, r))) Dx ' x (.i<n. .vv ' . \u00ac Q (i, v) . Piv \n' v -. v ' . D) VLIST-MAP ALIST-FILTER vlist mP Dx ' (map fx) alist nP Dx ' (.lter Qx) '' ' ilist n (.iv \nv.v = fiv) xz ' ''' ilist mPx x ilist n (.iv v.v = giv) yz ALIST-ENUM ILIST-MULT ' '' '' alist mP Dx \n(enum x) ilist n (.iv v.v = fiv * giv)(.i.x i * yi) z '' ' ilistv mx y ilist mPz x ilistM mnA A i<m ILIST-NTH3 \nILISTv . ILISTM ''' ' ilist m (.iv v.v = y ! i) xx ilistv n (Ai)(A ! i) Figure 8. Introduction rules \nused in the proof of CSR SpMV. k =0 '' ' ilistv n (.i. block k 1(.i j.A (i * k + i ))) A ILIST-CONCAT_VECTORS \n ilistv (n * k) A ' (concat_vectors kA) l =0 ilistv (m * l) x ' x ILIST-BLOCK_VECTOR '' ' ilistv m \n(.i. block l 1(.i j.x (i * l + i ))) (block_vector mlx) k =0 l =0 ilistM (m * k)(n * l) A ' A ILIST-BLOCK_MATRIX \n '''' ' ilistM mn (.i j. block kl (.i j.A (i * k + i )(j * l + j ))) (block_matrix mnklA) Figure 9. Introduction \nrules used for proving blocked format operations. successful application of either sub-method, the result \nis prepended to the head of the sequence. A failure at any level causes the search to backtrack and continue \nwith the next available goal state. When the top element of the goal state sequence is empty, the main \ngoal has been discharged and the proof is complete.  5. Evaluation In this section we evaluate programmability \nof sparse codes in LL and the extensibility of our veri.cation method to new formats. 5.1 Verifying \nadditional sparse formats We examine to what extent our prover design allows us to verify ad\u00additional \nformats without adding excessively many rules. Recall that our initial implementation of the prover for \nCSR SpMV (Section 4) insisted on minimizing reliance on format-speci.c rules, avoiding duplication of \nlogic, and keeping representation relations general, for example by keeping the type of the value stored \nin the matrix parametric. In this section, we extend our prover to verify several formats that are strictly \nmore complex than CSR. Our experience indicates that our prover can overcome varia\u00adtions in both format \nconstruction and matrix-vector multiplication. The variations were both syntactic (i.e., due to syntactic \nsugar) and structural (i.e., inducing a different data.ow structure). This bene.t is thanks to Isabelle \ns simpli.er, which successfully canonicalizes these differences, requiring only minor tweaks to the prover \ns rule base. Therefore, we consider below only a single implementation for each format and argue that \nthe single variant represents a larger class of similar implementations. Jagged Diagonals (JAD). A prominent \nfeature of JAD s proof goal is the double use of transpose, once during compression (jad) and once during \nmultiplication (jadmv). This form can be simpli\u00ad.edtotheIsabelle takeWhile list operator on the premise \nthat com\u00adpressed rows are sorted by length prior to being transposed. The form is matched by a rewrite \nrule for transpose (transpose xs). Adding introduction rules for in., takeWhile, rev and sort_key was \nsuf.cient for our veri.er to complete the proof. The ability to prove full functional correctness of \nJAD SpMV documents the strength of our prover; no other veri.cation frame\u00adwork that we know of can (i) \nhandle the complex data transforma\u00adtions in JAD compression, and (ii) prove correctness of arithmetic \noperations on the resulting sparse representation (see Section 6). Coordinate (COO). As mentioned in \nSection 3.2, the COO for\u00admat is challenging because it associates matrix values with both row and column \ncoordinates, and also because it requires concate\u00adnation and gather operations. It turns out that the \nCOO pair coordi\u00adnates do not call for a new representation relation. In fact, thanks to how the functions \ncoo and coomv are composed, we need to han\u00addle the pair coordinates only between concatenation (in coo)and \ngather (in coomv). The simpli.er moves these two functions to\u00adgether; therefore, we introduce a rule \nto relate the representation of the input and output of gather (concat xs), allowing the prover to automatically \ncomplete the proof. ALIST-GATHER-CONCAT vlist n (.i. vlist m (.j ab.a = snd b . i = fst b) {0}) {x..j<m.xj \n=0} Mxs alist n (.i. vlist m (.i.op =) {0}) {x..j<m.xj =0} M (gather (concat xs)) Compressed Sparse Columns \n(CSC). As CSC exhibits a peculiar use of concatenation and gather operations, it is handled similarly \nto COO. In contrast to COO, the input list to concat represents a transposed matrix, hence we use a rule \nsimilar to ALIST-GATHER-CONCAT, but with a transposed matrix M. How many introduction rules did we need \nto prove our sparse formats? In total, 24 rules were needed, including both introduc\u00adtion and simpli.cation \nrules. Introduction rules were typically used to (i) reason about some language construct such as map, \nsum and filter, in the context of a certain representation (e.g.,rules ILIST-MAP, ILIST-LISTSUM, ALIST-FILTER \nin Fig. 8); and (ii) formal\u00adize algebraic operations on vector and matrix representations, such as extracting \nan inner representation relation (ILIST-VLIST)and substituting a vector representation with a matrix \nrepresentation (ILISTv . ILISTM ). Most operators were handled by a single in\u00adtroduction rule; a few \n(e.g., map) required one rule per representa\u00adtion relation. To quantify rule reuse in our prover, we \nsummarize the reuse of the 24 rules that were needed for proving .ve sparse formats (see Fig. 10). On \naverage, fewer than 19% of rules used by a particular format are speci.c to this format, while over 66% \nof these rules are used by at least three additional formats, a signi.cant level of reuse. Even of the \nrules needed for more complex formats (CSC and JAD), only up to a third are format-speci.c. On the other \nhand, format-speci.c rules tend to be harder to prove, as indicated by the average number of lines of \nIsar code required to prove the rules. A detailed examination reveals that two rules for handling a gather-concat \nsequence (used in CSC and COO) account for over a hundred lines each. We believe that these rules can \nbe refactored for better reuse of simpler lemmas and greater automation. Note that most of the effort \nin proving JAD was invested in stating and proving simpli.cation of transpose-transpose composition. \nFig. 10 does not account for these rules, as they are quite general and were implemented as an extension \nto Isabelle s theory of lists.  5.2 Case study: hierarchical compression formats This subsection evaluates \nexpressiveness of the LL language. This question is motivated by the absence from LL of some powerful \nconstructs, such as .rst-class functions and folds. We show that LL can express advanced formats even \nwithout these constructs. Sparse CSR (SCSR). The SCSR format extends CSR with an\u00adother layer of compression. \nSCSR compresses the list of (com\u00adpressed) rows, by .ltering out empty rows (i.e., those rows that have \nonly zero-valued elements). The remaining rows are associ\u00adated with their row index. Implementing SCSR \nin LL amounts to obtaining the CSR for\u00admat, which compresses individual rows, followed by compression \nof the resulting list of compressed rows. Again, LL manages to ex\u00adpress format construction as a pipeline \nof stages. def scsr: csr -> [len != 0 ? (#, id)] The corresponding SpMV implementation needs to account \nfor the row indexes. It must also in.ate the resulting sparse vector into dense format: def scsrmv(A, \nx): A-> [i, r: r -> unzip -> snd * x[fst] -> sum -> (i, id)] -> infl(0, m, id) Alternatively, we can \nreuse SpMV for CSR: A -> unzip -> (fst, csrmv(snd, x)) -> zip -> infl(0, m, id)  SCSR demonstrates the \nability of our prover to peel off the additional compression layer and prove correctness of the overall \nresult, while requiring only two rules in addition to those needed by CSR (see Fig. 10). Next, we investigate \ntwo optimizations for SpMV register blocking and cache blocking designed to improve temporal local\u00adity \nof the vector x at two different levels of the memory hierarchy. The locality is improved by reorganizing \nthe computation to oper\u00adate on smaller segments of the input matrix, which in turn allows the reuse of \na segment of x. Register blocking. This optimization is useful when the nonzero values in the matrix \nappear in clusters [14]. The idea is to place the cluster of nonzeros in a small dense matrix. To obtain \nregister\u00adblocked format, instead of compressing single-cell values (i.e., numbers), a matrix is partitioned \ninto uniformly sized rectangular blocks. These dense blocks form the base elements for compres\u00adsion: \na block is .ltered away if all its elements are zeros; if the block is nonzero, it is represented as \na dense matrix. The size of these blocks is chosen so that the corresponding portion of the vec\u00adtor x \ncan reside in registers during processing of a block. Register blocking can be applied to all sparse \nformats described in Section 3.2. The 2 \u00d7 2 blocked representation of Fig. 1(a) can be seen in Fig. 11(a). \nApplying CSR compression to this blocked matrix results in the register-blocked CSR format in Fig. 11(b). \nTo construct the register-blocked CSR (RBCSR) in LL, we .rst blockify the dense matrix with the block \nfunction, which transforms a dense matrix A of size mr \u00d7 nc to an m \u00d7 n matrix of r \u00d7 c dense blocks. \nNext, we pair these blocks with their column indices using [enum], and .lter out the all-zero blocks. \ndef rbcsr(A): block(r, c, A) -> [enum -> [snd -> [[neq 0] -> disj] -> disj ? ]] In SpMV of an r \u00d7 c-RBCSR \nmatrix A and a dense vector x, we .rst bind the names B and l to each dense block and its index, respectively, \nand perform dense matrix-vector multiplication (densemv) on each block and the corresponding c-sub-vector \nof x. Reuse degree #Rules Avg. LOC CSR SCSR COO CSC JAD Total % 1 2 3 4 5 11 3 1 5 4 28.3 6.3 6.0 6.4 \n2.3 1 1 3 4 1 5 4 2 3 4 4 5 3 3 4 3 1 5 4 11 6 3 20 20 18.3 10.0 5.0 33.3 33.3 Figure 10. Reusability \nanalysis of our sparse-matrix code prover.  0 \u00ab \u00ab 1 0 . \u00ab.\u00ab 1 a 0 00 a 0 0, B bc 00 CB bc C BCB C \u00ab \n\u00ab . \u00ab. . \u00ab.\u00ab @A@ A 0000 00 00 0, 1, 0 d 0 e 0 d 0 e (a) (b) Figure 11. Example dense and sparse 2 \u00d7 2 \nblocked matrix representations. The latter is obtained by breaking x into a list of c-vectors using theorem \nfollows. block(c, x) and selecting the appropriate sub-vector. The result vectors in a row block are \nsummed, and the .nal result is obtained by concatenating the result sub-vectors from all row blocks. \ndef rbcsrmv(A, x): A-> [[l, B: (B, block(c, x)[l]) -> densemv] -> sum] -> concat Our prover allowed \nus to easily extend proofs to blocked for\u00admats because our matrices are of parametric type; the prover \ncan work with matrices of numbers as well as with matrices whose el\u00adements are matrices. Parameterization \nof matrices was expressed with Isabelle/HOL type classes, which are used to restrict types in introduction \nrules. We use a theory of .nite matrices [12]. Here, too, the size of a matrix is not encoded in the \nmatrix type (denoted a matrix) but it is required that matrix dimensions are bounded. To represent matrices \nas abstract values, we introduce the matrix conversion function: matrix :: nat . nat . (nat . nat . a) \n. a matrix The .rst two parameters specify the row and column dimensions, respectively. The third parameter \nis the abstract value encoded into the matrix. Implementing functions on compressed matrices necessitates \na few more conversion functions: block_vector :: nat . nat . [a] . [a matrix] block_matrix :: nat . nat \n. nat . nat . [[a]] . [[a matrix]] concat_vectors :: nat . [a matrix] . [a] The operation block_matrix \nmnklA transforms the object A,representingan mk \u00d7 nl-matrix, into an object representing an m \u00d7 n-matrix \nof k \u00d7 l-blocks; block_vector mkx transforms the list x of length nk into a list of nk-vectors; concat_vectors \nkx is the inverse operation, unpacking the k-vectors in x. This code shows register-blocked CSR code \nin Isabelle. rbcsr mnklA = map (.lter (.(i, v).v =0) . enum) (block_matrix mnklA) rbcsrmv mnkl (A, x)= \nconcat_vectors k (map (listsum . (map (.v.snd v * block_vector nlx ! fst v))) A) We require that block \ndimensions are greater than zero and properly divide the respective matrix and vector dimensions. The \ncorrectness k =0 . l =0 . ilistM (m * k)(n * l) A ' A . ilistv (n * l) x ' x -. ilistv (m * k)(.i. Sj<n \n* l. A ' ij * x ' j) (rbcsrmv mnkl (rbcsr mnklA, x)) (13) After adding the introduction rules in Fig. \n9 and a few rewrite rules for matrix, the prover automatically proves Eq. (13). Cache blocking. The idea \nin cache blocking is to reduce cache misses for the source vector x when it is too large to entirely \n.t in the cache during SpMV. We consider static cache blocking [7]. The sparse matrix is partitioned \ninto rectangular sub-matrices of size r \u00d7 c. While in register blocking these sub-matrices were kept \ndense, in the cache-blocked format they are compressed. Our cache blocking scheme differs from the one \nin [7] in that we only allow cache blocks to start at column indices which are multiples of c; this restriction \nleads to suboptimal compression. We believe that this restriction can be relaxed by augmenting LL with \na blocking function that creates optimally placed blocks. Notice that the construction of a cache-blocked \nmatrix is very similar to the construction for register blocking. The only differ\u00adence is the additional \ncompression applied to each block. The LL code for the CSR compressed cache-blocked matrix, whose blocks \nare stored in CSR format, is shown below. def cbcsr(A): block(r, c, A) -> [enum -> [snd -> [[neq 0] \n-> disj] -> disj ? ] -> [l, B: (l, B -> csr)]] The corresponding cache-blocked SpMV in LL: def cbcsrmv(A, \nx): A-> [[l, B: (B, block(c, x)[l]) -> csrmv] -> sum] -> concat In the cache-blocked SpMV, we again \nnotice the similarity to the register-blocked SpMV. The two codes are identical except for the function \nused for multiplying a block by a vector. It is somewhat desirable to factor out these inner multiplications \n(densemv and csrmv) but this is not possible in LL. The reason is that LL does not support lambda abstraction, \nwhich would allow reuse of code common to register-and cache-blocked versions. We have refrained from \nenriching LL with .rst-order functions for now because this allows for simpler veri.cation and gives \nus broad veri.cation cov\u00aderage. We do not consider the absence of lambda abstraction a signi.cant disadvantage \nbecause even optimized LL programs are small. In the future, we may decide to extend LL with a template \nmechanism that will be used to instantiate such hierarchical com\u00adposition, allowing code reuse. The \nveri.cation of cache-blocked sparse formats was not yet implemented. We expect that the amount of work \nwill not be sub\u00adstantial, based on our experience with other hierarchical formats.  6. Related Work \nSpecifying sparse matrix code Bernoulli [8, 9] is a system that synthesizes ef.cient low-level implementations \nof matrix opera\u00adtions given a description of the sparse format using relational al\u00adgebra. This is impressive \nand permits rapid development of fast low level implementations. However, the functionality of the sys\u00adtem \nwas limited and it had limited impact. Instead, we are express\u00ading formats using a functional programming \nlanguage which can be mechanically veri.ed. We believe that function-level programming provides the right \nlevel of abstraction for expressing the desired transformations. Moreover, LL can be embedded in existing \ncall\u00adby-value functional programming languages. Compiling LL into low-level code is a work in progress. \nThe synthesizer by Bik et al. [2] produces ef.cient implemen\u00adtations by replacing A[i, j] in dense-matrix \ncode with a representa\u00adtion function that maps to the corresponding sparse element; pow\u00aderful compiler \noptimizations then yield ef.cient code. Verifying sparse matrix code We are not aware of previous work \non verifying full functional correctness of sparse matrix codes. We are not even aware of work that veri.ed \ntheir memory safety with\u00adout explicitly provided loop invariants. Our own attempts at veri\u00ad.cation included \nESC/Java, TVLA and SAT-based bounded model checking, neither of which was satisfactory. Furthermore, \nneither of these tools was capable of proving higher-order properties like the ones we currently prove. \nThis led us to raising the level of abstrac\u00adtion and deferring to purely functional programs where loops \nare replaced with comprehensions and specialized reduction operators. Higher order veri.cation Duan et \nal. [5] veri.ed a set of block ciphers using the interactive theorem prover HOL-4. They proved that the \ndecoding of an encoded text results in the original data. Their proofs are mostly done using inversion \nrules, namely rules of the form f (f-1 x)= x, and algebraic rules on bit-word identities. For the block \nciphers used by AES and IDEA special rules where needed. The domain of block cipher veri.cation does \nnot seem to require more complicated rules than bit-word identities. 7. Conclusion In this paper we \nshowed how to raise the level of abstraction for sparse matrix programs from imperative code with loops \nto functional programs with comprehensions and limited reductions. We also developed an automated proof \nmethod for verifying a diverse range of sparse matrix formats and their SpMV operations. This was accomplished \nby introducing relations that map a sparse representation to the abstract (mathematical) one. Through \na clever de.nition of these representation relations we were able to build a reusable set of simpli.cation \nand introduction rules, which could be applied to a variety of computations. We are currently working \non the problem of compiling the func\u00adtional code into an ef.cient C code. Deploying techniques from pre\u00addecessor \nfunctional and data parallel languages, we already exhibit promising performance results with real-world \nsparse formats.  References [1] J. Backus. Can programming be liberated from the von Neumann style? \nA functional style and its algebra of programs. Communications of the ACM (CACM), 21(8):613 641, 1978. \n [2] A. J. C. Bik, P. Brinkhaus, P. M. W. Knijnenburg, and H. A. G. Wijshoff. The automatic generation \nof sparse primitives. ACM Transactions on Mathematical Software, 24(2):190 225, 1998. [3] G. E. Blelloch. \nProgramming parallel algorithms. Communications of the ACM (CACM), 39(3):85 97, 1996. [4] M. M. T. Chakravarty, \nR. Leshchinskiy, S. P. Jones, G. Keller, and S. Marlow. Data Parallel Haskell: a status report. In Workshop \non Declarative Aspects of Multicore Programming (DAMP), pages 10 18, New York, NY, USA, 2007. ACM. [5] \nJ. Duan, J. Hurd, G. Li, S. Owens, K. Slind, and J. Zhang. Functional correctness proofs of encryption \nalgorithms. In Logic for Programming, Arti.cial Intelligence and Reasoning (LPAR), pages 519 533, 2005. \n[6] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and R. Stata. Extended static \nchecking for Java. In Programming Languages Design and Implementation, pages 234 245, 2002. [7] E.-J. \nIm. Optimizing the performance of sparse matrix-vector multiplication. PhD thesis, University of California, \nBerkeley, 2000. [8] V. Kotlyar and K. Pingali. Sparse code generation for imperfectly nested loops with \ndependences. In International Conference on Supercomputing (ICS), pages 188 195, 1997. [9] V. Kotlyar, \nK. Pingali, and P. Stodghill. A relational approach to the compilation of sparse matrix programs. In \nEuro-Par, pages 318 327, 1997. [10] N. Mateev, K. Pingali, P. Stodghill, and V. Kotlyar. Next-generation \ngeneric programming and its application to sparse matrix computations. In International Conference on \nSupercomputing (ICS), pages 88 99, 2000. [11] T. Nipkow, L. C. Paulson, and M. Wenzel. Isabelle/HOL: \nA Proof Assistant for Higher-Order Logic, volume 2283 of Lecture Notes in Computer Science. Springer-Verlag, \n2002. [12] S. Obua. Flyspeck II: The Basic Linear Programs. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, \n2008. [13] M. Sagiv, T. W. Reps, and R. Wilhelm. Parametric shape analysis via 3-valued logic. ACM Transactions \non Programming Languages and Systems (TOPLAS), 24(3):217 298, 2002. [14] R. W. Vuduc. Automatic performance \ntuning of sparse matrix kernels. PhD thesis, University of California, Berkeley, 2004. [15] M. Wenzel. \nThe Isabelle/Isar Implementation. Technische Universit\u00e4t M\u00fcnchen. http://isabelle.in.tum.de/doc/implementation.pdf. \n[16] M. Wildmoser and T. Nipkow. Certifying machine code safety: Shallow versus deep embedding. In International \nConference on Theorem Proving in Higher-Order Logics, pages 305 320, 2004.  \n\t\t\t", "proc_id": "1863543", "abstract": "<p>Sparse matrix formats are typically implemented with low-level imperative programs. The optimized nature of these implementations hides the structural organization of the sparse format and complicates its verification. We define a variable-free functional language (LL) in which even advanced formats can be expressed naturally, as a pipeline-style composition of smaller construction steps. We translate LL programs to Isabelle/HOL and describe a proof system based on parametric predicates for tracking relationship between mathematical vectors and their concrete representations. This proof theory automatically verifies full functional correctness of many formats. We show that it is reusable and extensible to hierarchical sparse formats.</p>", "authors": [{"name": "Gilad Arnold", "author_profile_id": "81331488163", "affiliation": "University of California, Berkeley, Berkeley, CA, USA", "person_id": "P2338207", "email_address": "", "orcid_id": ""}, {"name": "Johannes H&#246;lzl", "author_profile_id": "81470647032", "affiliation": "Technische Universit&#228;t M&#252;nchen, Munich, Germany", "person_id": "P2338208", "email_address": "", "orcid_id": ""}, {"name": "Ali Sinan K&#246;ksal", "author_profile_id": "81470653567", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland", "person_id": "P2338209", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "Univeristy of California, Berkeley, Berkeley, CA, USA", "person_id": "P2338210", "email_address": "", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2338211", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863581", "year": "2010", "article_id": "1863581", "conference": "ICFP", "title": "Specifying and verifying sparse matrix codes", "url": "http://dl.acm.org/citation.cfm?id=1863581"}