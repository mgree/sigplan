{"article_publication_date": "09-27-2010", "fulltext": "\n Distance Makes the Types Grow Stronger A Calculus for Differential Privacy Jason Reed Benjamin C. Pierce \nUniversity of Pennsylvania Abstract We want assurances that sensitive information will not be disclosed \nwhen aggregate data derived from a database is published. Differ\u00adential privacy offers a strong statistical \nguarantee that the effect of the presence of any individual in a database will be negligi\u00adble, even when \nan adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms \nto be differ\u00adentially private one at a time; we propose to streamline this process with a functional \nlanguage whose type system automatically guar\u00adantees differential privacy, allowing the programmer to \nwrite com\u00adplex privacy-safe query programs in a .exible and compositional way. The key novelty is the \nway our type system captures function sensitivity, a measure of how much a function can magnify the dis\u00adtance \nbetween similar inputs: well-typed programs not only can t go wrong, they can t go too far on nearby \ninputs. Moreover, by in\u00adtroducing a monad for random computations, we can show that the established de.nition \nof differential privacy falls out naturally as a special case of this soundness principle. We develop \nexamples including known differentially private algorithms, privacy-aware variants of standard functional \nprogramming idioms, and compo\u00adsitionality principles for differential privacy. Categories and Subject \nDescriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations specialized application lan\u00adguages \nGeneral Terms Languages Keywords Differential Privacy, Type Systems 1. Introduction It s no secret that \nprivacy is a problem. A wealth of information about individuals is accumulating in various databases \n patient records, content and link graphs of social networking sites, book and movie ratings, ... and \nthere are many potentially good uses to which it could be put. But, as Net.ix and others have learned \n[26] to their detriment, even when data collectors try to release only anonymized or aggregated results, \nit is easy to publish information that reveals much more than was intended, when cleverly combined with \nother data sources. An exciting new body of work on differ\u00adential privacy [6, 7, 12 15, 27] aims to address \nthis problem by, Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright c &#38;#169; 2010 ACM 978-1-60558-794-3/10/09. \n. . $10.00 .rst, replacing the informal goal of not violating privacy with a technically precise and \nstrong statistical guarantee, and then offer\u00ading various mechanisms for achieving this guarantee. Essentially, \na mechanism for publishing data is differentially private if any con\u00adclusion made from the published \ndata is almost exactly as likely if any one individual s data is omitted from the database. Methods for \nachieving this guarantee can be attractively simple, usually involv\u00ading taking the true answer to a query \nand adding enough random noise to blur the contributions of individuals. For example, the query How many \npatients at this hospital are over the age of 40? is intuitively almost safe safe because it aggregates \nmany individuals contributions together, and almost because if an adversary happened to know the ages \nof every pa\u00adtient except John Doe, then answering this query would give them certain knowledge of a fact \nabout John. The differential privacy methodology rests on the observation that, if we add a small amount \nof random noise to its result, we can still get a useful idea of the true answer to this query while \nobscuring the contribution of any single individual. By contrast, the query How many patients are over \nthe age of 40 and also happen to be named John Doe? is plainly problematic, since it is focused on an \nindividual rather than an aggregate. Such a query cannot usefully be privatized: if we add enough noise \nto obscure any individual s contribution to the result, there won t be any signal left. So far, most \nof the work in differential privacy concerns speci.c algorithms rather than general, compositional language \nfeatures. Although there is already an impressive set of differentially private versions of particular \nalgorithms [6, 18], each new one requires its own separate proof. McSherry s Privacy Integrated Queries \n(PINQ) [25] are a good step toward more general principles: they allow for some relational algebra operations \non database tables, as well as certain forms of composition of queries. But even these are rel\u00adatively \nlimited. We offer here a higher-order functional program\u00adming language whose type system directly embodies \nreasoning about differential privacy. In this language, we can implement Mc\u00adSherry s principles of sequential \nand parallel composition of differ\u00adentially private computations, and many others besides, as higher\u00adorder \nfunctions. This provides a foundational explanation of why compositions of differentially private mechanisms \nsucceed in the ways that they do. The central idea in our type system also appears in PINQ and in many \nof the algorithm-by-algorithm proofs in the differential privacy literature: the sensitivity of query \nfunctions to quantitative differences in their input. Sensitivity is a sort of continuity property; a \nfunction of low sensitivity maps nearby inputs to nearby outputs. To give precise meaning to nearby, \nwe equip every type with a metric a notion of distance on its values. Sensitivity matters for differential \nprivacy because the amount of noise required to make a deterministic query differentially pri\u00advate is \nproportional to that query s sensitivity. The sensitivity of both queries discussed above is in fact \n1: adding or removing one patient s records from the hospital database can only change the true value \nof the query by at most 1. This means that we should add the same amount of noise to How many patients \nat this hospital are over the age of 40? as to How many patients are over the age of 40, who also happen \nto be named John Doe? This may appear counter-intuitive, but actually it is just right: the privacy of \nsingle individuals is protected to exactly the same degree in both cases. Of course, the usefulness of \nthe results differs: knowing the answer to the .rst query with, say, a typical error margin of \u00b1100 could \nstill be valuable if there are thousands of patients in the hospital s records, whereas knowing the answer \nto the second query (which can only be zero or one) \u00b1100 is useless. (We might try making the second \nquery more useful by scaling its answer up numerically: Is John Doe over 40? If yes, then 1,000, else \n0. But this query has a sensitivity of 1,000, not 1, and so 1,000 times as much noise must be added, \nblocking our sneaky attempt to violate privacy.)  To track function sensitivity, we give a distance-aware \ntype sys\u00adtem. This type system embodies two important connections be\u00adtween differential privacy and concepts \nfrom logic and type the\u00adory. First, reasoning about sensitivity itself strongly resembles lin\u00adear logic \n[4, 16], which has been widely applied in programming languages. The essential intuition about linear \nlogic and linear type theories is that they treat assumptions as consumable resources. We will see that \nin our setting the capability to sensitively depend on an input s value behaves like a resource. This \nintuition recurs through\u00adout the paper, and we sometimes refer to sensitivity to an input as if it is \ncounting the number of uses of that input. The other connection comes from the use of a monad to inter\u00adnalize \nthe operation of adding random noise to query results. We include in the programming language a monad \nfor random compu\u00adtations, similar to previously proposed stochastic calculi [29, 30]. Since every type \nhas a metric in our setting, we are led to ask: what should the metric be for the monad? We .nd that, \nwith the right choice of metric, the de.nition of differentially private functions falls out as a special \ncase of the de.nition of function sensitivity for functions, when the function output happens to be monadic. \nThis observation is very useful: while prior work treats differen\u00adtial privacy mechanisms and private \nqueries as separate things, we see here that they can be uni.ed in a single language. Our type system \ncan express the privacy-safety of individual queries, as well as more complex query protocols (see Section \n5) that repeatedly in\u00adteract with a private database, adjusting which queries they perform depending \non the responses they receive. To brie.y foreshadow what a query in our language looks like, suppose \nthat we have the following functions available: over 40 : row . bool size : db -R .lter :(row . bool) \n. db -db add noise : R -OR The predicate over 40 simply determines whether or not an indi\u00advidual database \nrow indicates that patient is over the age of 40. The function size takes an entire database, and outputs \nhow many rows it contains. Its type uses a special arrow -, related to the lin\u00adear logic function type \nof the same name, which expresses that the function has sensitivity of 1. The higher-order function .lter \ntakes a predicate on database rows and a database; it returns the subset of the rows in the database \nthat satisfy the predicate. This .ltering operation also has a sensitivity of 1 in its database argument, \nand again -is used in its type. Finally, the function add noise is the differential privacy mechanism \nthat takes a real number as input and returns a random computation (indicated by the monad O) that adds \nin a bit of random noise. This function also has a sensitivity of 1, and this fact is intimately connected \nto privacy properties, as explained in Section 4. With these in place, the query can be written as the \nprogram .d : db. add noise (.lter over 40 d): db -OR. As we explain in Section 4, its type indicates \nthat it is a differ\u00adentially private computation taking a database and producing a real number. Its runtime \nbehavior is to yield a privacy-preserving noised count of the number of patients in the hospital that \nare over 40. We begin in Section 2 by describing a core type system that tracks function sensitivity. \nWe state an informal version of the key metric preservation theorem, which says the execution of every \nwell-typed function re.ects the sensitivity that the type system as\u00adsigns it. Section 3 gives examples \nof programs that can be imple\u00admented in our language. Section 4 shows how to add the probability monad, \nand Section 5 develops further examples. In Section 6 we state the standard safety properties of the \ntype system, give a formal statement of the metric preservation theorem, and sketch its proof. The remaining \nsections discuss related work and offer concluding remarks. 2. A Type System for Function Sensitivity \n 2.1 Sensitivity Our point of departure for designing a programming language for differential privacy \nis function sensitivity. A function is said to be c\u00adsensitive (or have sensitivity c) if it can magnify \ndistances between inputs by a factor of at most c. Since this de.nition depends on the input and output \ntypes of the function having a metric (a notion of distance) de.ned on them, we begin by discussing a \nspecial case of the de.nition for functions from R to R, where we can use the familiar Euclidean metric \ndR(x, y)= |x - y| on the real line. We can then formally de.ne c-sensitivity for real-valued functions \nas follows. De.nition A function f : R . R is said to be c-sensitive iff dR(f(x),f(y)) = c \u00b7 dR(x, y) \nfor all x, y . R. A special case of this de.nition that comes up frequently is the case where c =1.A \n1-sensitive function is also called a nonex\u00adpansive function, since it keeps distances between input \npoints the same or else makes them smaller. Some examples of 1-sensitive functions are f1(x)= xf2(x)= \n-xf3(x)= x/2 f4(x)= |x| f5(x)=(x + |x|)/2 and some non-examples include: f6(x)=2x and f7(x)= x 2. The \nfunction f6, while not 1-sensitive, is 2-sensitive. On the other hand, f7 is not c-sensitive for any \nc. ' PROPOSITION 2.1. Every function that is c-sensitive is also c\u00adsensitive for every c' = c. For example, \nf3 is both 1/2-sensitive and 1-sensitive. So far we only have one type, R, with an associated metric. \nWe would like to introduce other base types, and type operators to build new types from old ones. We \nrequire that for every type t that we discuss, there is a metric dt (x, y) for values x, y . t . This \nrequirement makes it possible to straightforwardly generalize the de.nition of c-sensitivity to arbitrary \ntypes. De.nition A function f : t1 . t2 is said to be c-sensitive iff dt2 (f(x),f(y)) = c \u00b7 dt1 (x, y) \nfor all x, y . t1. The remainder of this subsection introduces several type op\u00aderators, one after another, \nwith examples of c-sensitive functions on the types that they express. We use suggestive programming\u00adlanguage \nterminology and notation, but emphasize that the discus\u00adsion for now is essentially about pure mathematical \nfunctions we do not yet worry about computational issues such as the possi\u00adbility of nontermination. \nFor example, we speak of values of a type in a way that should be understood as more or less synonymous \nwith mere elements of a set in Section 2.2 below, we will show how to actually speak formally about \ntypes and values.  First of all, when t is a type with associated metric dt , let !rt be the type whose \nvalues are the same as those of t, but with the metric scaled up by a factor of r. That is, we de.ne \nd!rt (x, y)= r \u00b7 dt (x, y). One role of this type operator is to allow us to reduce the concept of c-sensitivity \nto 1-sensitivity. For we have PROPOSITION 2.2. A function f is a c-sensitive function in t1 . t2 if and \nonly f it isa 1-sensitive function in !ct1 . t2. Proof Let x, y : t1 be given. Suppose dt1 (x, y)= r. \nThen d!ct1(x,y) = cr. For f to be c-sensitive as a function t1 . t2 we must have dt2 (f(x),f(y)) = cr, \nbut this is exactly the same condition that must be satis.ed for f to be a 1-sensitive function !ct1 \n. t2. We can see therefore that f6 is a 1-sensitive function !2R . R, and also in fact a 1-sensitive \nfunction R . !1/2R. The symbol ! is borrowed from linear logic, where it indicates that a resource can \nbe used an unlimited number of times. In our setting an input of type !rt is analogous to a resource \nthat can be used at most r times. We can also speak of !8, which scales up all non-zero distances to \nin.nity, which is then like the original linear logic !, which allows unrestricted use. Another way we \ncan consider building up new metric-carrying types from existing ones is by forming products. If t1 and \nt2 are types with associated metrics dt1 and dt2 , then let t1 . t2 be the type whose values are pairs \n(v1,v2) where v1 . t1 and v2 . t2. In the metric on this product type, we de.ne the distance between \ntwo pairs to be the sum of the distances between each pair of components: ' '' dt1.t2 ((v1,v2), (v1,v2)) \n= dt1 (v1,v 1)+ dt2 (v2,v 2) the differences between their components instead the sum. Even though the \nunderlying set of values is essentially the same, we regard choosing a different metric as creating a \ndistinct type: the type t1 &#38; t2 consists of pairs (v1,v2), (written differently from pairs of type \nt1 . t2 to further emphasize the difference) with the metric ' '' dt1&#38;t2 ((v1,v2), (v1,v2)) = max(dt1 \n(v1,v 1),dt2 (v2,v 2)). Now we can say that f15(x, y)= (x, x) is a 1-sensitive function R . R . R &#38; \nR. More generally, &#38; lets us combine outputs of different c-sensitive functions even if they share \ndependency on common inputs. PROPOSITION 2.3. If f : t . t1 and g : t . t2 are c-sensitive, then .x.(f \nx,g x) is a c-sensitive function in t . t1 &#38; t2. Next we would like to capture the set of functions \nitself as a type, so that we can, for instance, talk about higher-order functions. Let us take t1 -t2 \nto be the type whose values are 1-sensitive functions f : t1 . t2. We have already established that the \npresence of !r means that having 1-sensitive functions suf.ces to express c-sensitive functions for all \nc, so we need not specially de.ne an entire family of c-sensitive function type constructors: the type \nof c-sensitive functions from t1 to t2 is just !ct1 -t2. We de.ne the metric for -as follows: dt1 t2 \n(f, f ' ) = max dt2 (f(x),f ' (x)) x.t1 This is chosen to ensure that -and . have the expected curry\u00ading/uncurrying \nbehavior with respect to each other. We .nd in fact that curry(f)= .x..y.f (x, y) uncurry(g)= .(x, y).g \nxy are 1-sensitive functions in (R. R -R) . (R -R -R) and (R -R -R) . (R. R -R), respectively. We postulate \nseveral more type operators that are quite familiar from programming languages. The unit type 1 which \nhas only one inhabitant (), has the metric d1((), ()) = 0. Given two types t1 and t2, we can form their \ndisjoint union t1 + t2, whose values are either of the form inj1 v where v . t1, or inj2 v where v . \nt2. Its metric is 8 >< >: ' '' dt1 (v0,v 0) if v = inj1 v0 and v = inj1 v0; With this type operator we \ncan describe more arithmetic opera\u00ad ' tions on real numbers. For instance, ' '' if v = inj2 v0 and v \ndt1+t2 (v, v )= dt2 (v0,v 0) = inj2 v 0; 8 otherwise. f8(x, y)= x + yf9(x, y)= x - y ( are 1-sensitive \nfunctions in R. R . R, and Note that this de.nition creates a type that is an extremely disjoint union \nof two components. Any distances between pairs of points f11(x, y)=(y, x) f10(x, y)=(x, y) within the \nsame component take the distance that that component (x, y) if x<y speci.es, but distances from one component \nto the other are all f12(x, y)=(x + y, 0) cswp(x, y)= in.nite.(y, x) otherwise Notice what this means \nfor the type bool in particular, which are 1-sensitive functions in R . R . R . R. We will see the usefulness \nof cswp in particular below in Section 3.6. However, f13(x, y)=(x \u00b7 y, 0) f14(x, y)=(x, x) are not 1-sensitive \nfunctions in R.R . R.R. The function f14 is of particular interest, since at no point do we ever risk \nmultiplying x by a constant greater than 1 (as we do in, say, f6 and f13) and yet the fact that x is \nused twice means that variation of x in the input is effectively doubled in measurable variation of the \noutput. This intuition about counting uses of variables is re.ected in the connection between our type \nsystem and linear logic. This metric is not the only one that we can assign to pairs. Just as linear \nlogic has more than one conjunction, our type theory admits more than one product type. Another one that \nwill prove useful is taking distance between pairs to be the maximum of we de.ne as usual as 1+1. It \nis easy to write c-sensitive functions from bool to other types, for the in.nite distance between the \nvalues true and false licenses us to map them to any two values we like, no matter how far apart they \nare. However, it is conversely hard for a nontrivial function to bool to be c-sensitive. The function \ngtzero : R . bool, which returns true when the input is greater than zero, is not c-sensitive for any \n.nite c. This can be blamed, intuitively, on the discontinuity of gtzero at zero. Finally, we include \nthe ability to form (iso)recursive types \u00b5a.t whose values are of the form fold v, where v is of the \ntype [\u00b5a.t/a]t , and whose metric we would like to give as '' This de.nition, however, is not well-founded, \nsince it depends on a metric at possibly a more complex type, due to the substitution  [\u00b5a.t/a]t . It \nwill suf.ce as an intuition for our present informal discussion, since we only want to use it to talk \nabout lists (rather than, say, types such as \u00b5a.a), but a formally correct treatment of the metric is \ngiven in Section 6.1. With these pieces in place, we can introduce a type of lists of real numbers, listreal \n= \u00b5a.1+ R . a. (The reader is invited to consider also the alternative where . is replaced by &#38;; \nwe return to this choice below in Section 3.) The metric between lists that arises from the preceding \nde.nitions is as follows. Two lists of different lengths are at distance 8 from each other; this comes \nfrom the de.nition of the metric on disjoint union types. For two lists [x1,...,xn] and [y1,...,yn] of \nthe same length, we have n X dlistreal([x1,...,xn], [y1,...,yn]) = |xi - yi|. i=1 We now claim that there \nis a 1-sensitive function sort : listreal -listreal that takes in a list of reals and outputs the sorted \nversion of that same list. This fact may seem somewhat surprising, since a small variation in the input \nlist can lead to an abrupt change in the permutation of the list that is produced. However, what we output \nis not the permutation itself, but merely the values of the sorted list; the apparent point of discontinuity \nwhere one value overtakes another is exactly where those two values are equal, and their exchange of \npositions in the output list is unobservable. Of course, we would prefer not to rely on such informal \nar\u00adguments. So let us turn next to designing a rigorous type system to capture sensitivity of programs, \nso that we can see that the 1\u00adsensitivity of sorting is a consequence of the fact that an implemen\u00adtation \nof a sorting program is well-typed.  2.2 Typing Judgment Type safety for a programming language ordinarily \nguarantees that a well-typed open expression e of type t is well-behaved during execution. Well-behaved \nis usually taken to mean that e can accept any (appropriately typed) value for its free variables, and \nwill evaluate to a value of type t without becoming stuck or causing runtime errors: Well-typed programs \ncan t go wrong. We mean to make a strictly stronger guarantee than this, namely a guarantee of c-sensitivity. \nIt should be the case that if an expression is given similar input values for its free variables, the \nresult of evaluation will also be suitably close i.e., Well-typed programs can t go too far. To this \nend, we take, as usual, a typing judgment G f e : t (expressing that e is a well-formed expression of \ntype t in a context G) but we add further structure the contexts. By doing so we are essentially generalizing \nc-sensitivity to capture what it means for an expression to be sensitive to many inputs simultaneously \n that is, to all of the variables in the context rather than just one. Contexts G have the syntax G \n::= \u00b7| G,x :r t for r . R>0 .{8}. To have a hypothesis x :r t while constructing an expression e is to \nhave permission to be r-sensitive to variation in the input x: the output of e is allowed to vary by \nrs if the value substituted for x varies by s. We include the special value 8 as an allowed value of \nr so that we can express ordinary (unconstrained by sensitivity) functions as well as c-sensitive functions. \nAlgebraic operations involving 8 are de.ned by setting 8\u00b7 r = 8 (except for 8\u00b7 0=0) and 8 + r = 8. This \nmeans that to be 8-sensitive is no constraint at all: if we consider the de.nition of sensitivity, then \n8-sensitivity permits any variation at all in the input to be blown up to arbitrary variation in the \noutput. A well-typed expression x :c t1 f e : t2 is exactly a program that represents a c-sensitive computation. \nHowever, we can also consider more general programs x1 :r1 t1,...,xn :rn tn f e : t in which case the \nguarantee is that, if each xi varies by si, then the P result of evaluating e only varies by i risi. \nMore carefully, we state the following metric preservation theorem for the type sys\u00adtem, which is of \ncentral importance. The notation [v/x]e indicates substitution of the value v for the variable x in expression \ne as usual. THEOREM 2.4 (Metric Preservation). Suppose G f e : t . Let sequences of values (vi)1=i=n \nand (vi' )1=i=n be given. Suppose for all i . 1,...,n that we have 1. f vi : ti 2. dti (vi,v i' )= si \n 3. xi :ri ti . G.  If the program [v1/x1] \u00b7\u00b7\u00b7 [vn/xn]e evaluates to v, then there exists ' '' a v such \nthat [v1/x1] \u00b7\u00b7\u00b7 [vn/xn]e evaluates to v ', and X dt (v, v ' ) = risi. i We give a more precise version \nof this result in Section 6. 2.3 Types The complete syntax and formation rules for types are given in \nFigure 1. Essentially all of these types have already been mentioned in Section 2.1. There are type variables \na, (which appear in type variable contexts .) base types b (drawn from a signature S), unit and void \nand sum types, metric-scaled types !rt , and recursive types \u00b5a.t . There are the two pair types . and \n&#38;, which differ in their metrics. There are two kinds of function space, -and ., where t1 -t2 contains \njust 1-sensitive functions, while t1 . t2 is the ordinary unrestricted function space, containing the \nfunctions that can be programmed without any sensitivity requirements on the argument. As in linear logic, \nthere is an encoding of t1 . t2, in our case as !8t1 -t2, but it is convenient to have the built\u00adin type \nconstructor . to avoid having to frequently introduce and eliminate !-typed expressions. 2.4 Expressions \nThe syntax of expressions is straightforward; indeed, our language can be seen as essentially just a \nre.nement type system layered over the static and dynamic semantics of an ordinary typed func\u00adtional \nprogramming language. Almost all of the expression formers should be entirely familiar. One feature worth \nnoting (which is also familiar from linear type systems) is that we distinguish two kinds of pairs: the \none that arises from ., which is eliminated by pattern\u00admatching and written with (parentheses), and the \none that arises from &#38;, which is eliminated by projection and written with (angle brackets). The \nother is that for clarity we have explicit introduction and elimination forms for the type constructor \n!r. e ::= x | c | () |(e, e)| (e, e) let(x, y)= e in e | pie | .x.e | ee |inji e | (case e of x.e | x.e) \n|!e | let !x = e in e |unfoldt e | foldt e Just as with base types, we allow for primitive constants \nc to be drawn from a signature S.  2.5 Typing Relation To present the typing relation, we need a few \nalgebraic operations on contexts. The notation sG indicates pointwise scalar multiplica\u00adtion of all the \nsensitivity annotations in G by s. We can also de.ne addition of two contexts (which may share some variables) \nby \u00b7 + \u00b7 = \u00b7 (G,x :s t ) + (.,x :r t )= (G+.),x :r+s t (G,x :r t )+.= (G+.),x :r t (x . .) G + (.,x :r \nt )= (G+.),x :r t (x . G)  t ::= a | b | 1 | \u00b5a.t | t + t | t . t | t &#38; t | t -t | t . t | !rt .,a \n: type f t : type .,a : type f a : type . f 1 : type . f \u00b5a.t : type b : type . S. f t : type r . R>0 \n. {8} . f b : type . f !rt : type . f t1 : type . f t2 : type * .{+, &#38;, ., -, .} . f t1 *t2 : type \nFigure 1. Type Formation r = 1. f e1 : t1 G f e2 : t2 var .I G,x :r t f x : t .+G f (e1,e2): t1 . t2 \n'' G f e : t1 . t2 .,x :r t1,y :r t2 f e : t .E '' .+ rG f let(x, y)= e in e : t G f e1 : t1 G f e2 : \nt2 G f e : t1 &#38; t2 &#38;I &#38;E G f e1,e2 : t1 &#38; t2 G f pi e : ti ' .,x :r t1 f e1 : t ' G f \ne : t1 + t2 .,x :r t2 f e2 : t +E ' .+ rG f case e of x.e1 | x.e2 : t ' G f e : ti G,x :1 t f e : t +I \n-I ' G f inji e : t1 + t2 G f .x.e : t -t ' ' . f e1 : t -t G f e2 : t G,x :8 t f e : t -E .I ' ' .+G \nf e1 e2 : t G f .x.e : t . t ' . f e1 : t . t G f e2 : t G f e : t .E !I ' .+ 8G f e1 e2 : tsG f !e :!st \n'' G f e :[\u00b5a.t/a]t G f e :!st .,x :rs t f e : t \u00b5I !E '' G f fold e : t .+ rG f let !x = e in e : t \n\u00b5a.t G f e : t \u00b5E G f unfold e :[\u00b5a.t/a]t \u00b5a.t Figure 2. Typing Rules The typing relation is de.ned by \nthe inference rules in Figure 2. Every occurrence of r and s in the typing rules is assumed to be drawn \nfrom R>0 .{8}. Type-checking is decidable; see Section 6 and the appendix1 for more details. In short, \nthe only novelty is that lower bounds on the annotations in the context are inferred top\u00addown from the \nleaves to the root of the derivation tree. The rule var allows a variable from the context to be used \nas long as its annotation is at least 1, since the identity function is c\u00adsensitive for any c = 1 (cf. \nProposition 2.1). Any other context G is allowed to appear in a use of var, because permission to depend \non a variable is not an obligation to depend on it. (In this respect our type system is closer to af.ne \nlogic than linear logic.) 1 Available at http://www.cis.upenn.edu/~bcpierce/papers/dp.pdf ' e1 . .x.e \ne2 . v [v/x]e . v .x.e . .x.e ' () . () e1 e2 . v e1 . v1 e2 . v2e1 . v1 e2 . v2 e1,e2 . v1,v2 (e1,e2) \n. (v1,v2) '' e . (v1,v2)[v1/x][v2/y]e . ve . v1,v2 '' let(x, y)= e in e . vpie . vi ' e . v e . ve . \ninji v [v/x]ei . v ' fold e . fold v inji e . inji v case e of x.e1 | x.e2 . v tt e . fold v '' t e . \nve . !v [v/x]e . e unfold e . v '' !e . !v let !x = e in e . v t Figure 3. Evaluation Rules In the rule \n.I, consider the role of the contexts. G represents the variables that e1 depends on, and captures quantitatively \nhow sensitive it is to each one. . does the same for e2. In the conclusion of the rule, we add together \nthe sensitivities found in G and ., precisely because the distances in the type t1 . t2 are measured \nby a sum of how much e1 and e2 vary. Compare this to &#38;I, where we merely require that the same context \nis provided in the conclusion as is used to type the two components of the pair. We can see the action \nof the type constructor !r in its introduc\u00adtion rule. If we scale up the metric on the expression being \ncon\u00adstructed, then we must scale up the sensitivity of every variable in its context to compensate. The \nclosed-scope elimination rules for ., +, and ! share a common pattern. The overall elimination has a \nchoice as to how much it depends on the expression of the type being eliminated: this is written as the \nnumber r in all three rules. The cost of this choice is that context G that was used to build that expression \nmust then be multiplied by r. The payoff is that the variable(s) that appear in the scope of the elimination \n(in the case of .E, the two variables x and y, in +E the xs one in each branch) come with permission \nfor the body to be r-sensitive to them. In the case of !E, however, the variable appears with an annotation \nof rs rather than r, re.ecting that the !s scaled the metric for that variable by a factor of s. We note \nthat -I, since -is meant to capture 1-sensitive functions, appropriately creates a variable in the context \nwith an annotation of 1. Compare this to .I, which adds a hypothesis with annotation 8, whose use is \nunrestricted. Conversely, in .E, note that the context G used to construct the argument e2 of the function \nis multiplied by 8 in the conclusion. Because the function e1 makes no guarantee how sensitive it is \nto its argument, we can in turn make no guarantee how much e1 e2 depends on the variables in G. This \nplays the same role as requirements familiar in linear logic, that the argument to an unrestricted implication \ncannot depend on linear resources.  2.6 Evaluation We give a big-step operational semantics for this \nlanguage, which is entirely routine. Values, the subset of expressions that are allowed as results of \nevaluation, are de.ned as follows. v ::= () |(v, v)| (v, v) | .x.e | inji v | foldt v | !v The judgment \ne'. v says that e evaluates to v. The complete set of evaluation rules is given in Figure 3.  3. Examples \nWe now present some more sophisticated examples of programs that can be written in this language. We \ncontinue to introduce new base types and new constants as they become relevant. For readability, we use \nsyntactic sugar for case analysis and pattern matching \u00b4 a la ML. 3.1 Fixpoint Combinator Because we \nhave general recursive types, we can simulate a .x\u00adpoint combinator in pretty much the usual way: we \njust need to be a little careful about how sensitivity interacts with .xpoints. Let t0 = \u00b5a.a . (t -s). \nThen the expression Y = .f.(.x..a.f ((unfoldt0 x) x) a) (foldt0 (.x..a.f ((unfoldt0 x) x) a)) has type \n((t -s) . (t -s)) . (t -s). This is the standard call-by-value .xed point operator (differing from the \nmore familiar Y combinator by the two .a \u00b7\u00b7\u00b7 a eta-expansions). It is easy to check that the unfolding \nrule f (Yf) v'. v0 Y fv'. v0 is admissible whenever f is a function value .x.e. We could alternatively \nadd a .xpoint operator .xf.e to the language directly, with the following typing rule: G,f :8 t -s f \ne : t -s 8G f .xf.e : t -s This rule re.ects the type we assigned to Y above: uses of .x can soundly \nbe compiled away by de.ning .xf.e = Y (.f.e). The fact that f is added to the context annotated 8 means \nthat we are allowed to call the recursive function an unrestricted number of times within e. The context \nG must be multiplied by 8 in the conclusion because we can t (because of the .xpoint), establish any \nbound on how sensitive the overall function is from just one call to it. In the rest of the examples, \nwe write recursive functions in the usual high-level form, eliding the translation in terms of Y . 3.2 \nLists We can de.ne the type of lists with elements in t as follows: t list = \u00b5a.1+ t . a We write [] \nfor the nil value foldt list inj1() and h :: tl for foldt list inj2(h, tl), and we use common list notations \nsuch as [a, b, c] for a :: b :: c :: []. Given this, it is straightforward to program map in the usual \nway. map :(t -s) . (t list -s list) map f [] = [] map f (h :: tl)=(fh) :: map f tl The type assigned \nto map re.ects that a nonexpansive function mapped over a list yields a nonexpansive function on lists. \nEvery bound variable is used exactly once, with the exception of f; this is permissible since f appears \nin the context during the typechecking of map with an 8 annotation. Similarly, we can write the usual \nfold combinators over lists: foldl :(t . s -s) . (s . t list) -s foldl f (init, []) = init foldl f (init, \n(h :: tl)) = foldl f (f(h, init), tl) foldr :(t . s -s) . (s . t list) -s foldr f (init, []) = init foldr \nf (init, (h :: tl)) = f (h, foldr f (init, tl)) Again, every bound variable is used once, except for \nf, which is provided as an unrestricted argument, making its repeated use acceptable. The fact that the \ninitializer to the fold (of type s) together with the list to be folded over (of type t list) occur to \nthe left of a -is essential, capturing the fact that variation in the initializer and in every list element \ncan jointly affect the result. Binary and iterated concatenation are also straightforwardly implemented: \n@: t list . t list -t list @ ([],x)= x @(h :: tl, x)= h :: @ (tl, x) concat : t list list -t list concat \n[] = [] concat (h :: tl)=@(h, concat tl) If we de.ne the natural numbers as usual by nat = \u00b5a.1+ a z \n= foldnat inj1() sx = foldnat inj2 x then we can implement a function that .nds the length of a list \nas follows: length : t list -nat length [] = z length (h :: tl)= s (length tl) However, this implementation \nis less than ideal, for it con\u00adsumes the entire list in producing its answer, leaving further com\u00adputations \nunable to depend on it. We can instead write length : t list -t list .nat length [] = ([],z) length (h \n:: tl)= let(tl ' ,e)= length tl in(h :: tl ' ,se) which deconstructs the list enough to determine its \nlength, but builds up and returns a fresh copy that can be used for further pro\u00adcessing. Consider why \nthis function is well-typed: as it decomposes the input list into h and tl, the value of h is only used \nonce, by in\u00adcluding it in the output. Also, tl is only used once, as it is passed to the recursive call, \nwhich is able to return a reconstructed copy tl ', which is then included in the output. At no point \nis any data duplicated, but only consumed and reconstructed.  3.3 &#38;-lists Another de.nition of lists \nuses &#38; instead of .: we can say t alist = \u00b5a.1+ t &#38; a. (the a in alist is for ampersand ). To \ndistinguish these lists visually from the earlier de.nition, we write Nil for foldt alist inj1() and \nCons p for foldt list inj2 p. Recall that &#38; is eliminated by projection rather than pattern\u00admatching. \nThis forces certain programs over lists to be imple\u00admented in different ways. We can still implement \nmap for this kind of list without much trouble. amap :(t -s) . (t alist -s alist) amap f Nil = Nil amap \nf (Cons p)= Cons(f (p1p), map f (p2p)) This function is well-typed (despite the apparent double use of \np in the last line!) because the &#38;I rule allows the two components of an &#38;-pair to use the same \ncontext. This makes sense, because the eventual fate of an &#38;-pair is to have one or the other of \nits components be projected out. The fold operations are more interesting. Consider a na\u00a8ive im\u00adplementation \nof foldl for alist afoldl :(t &#38; s -s) . (s &#38; t alist) -s alist afoldl fp = case p2p of x. p1p \n| x. afoldl f (f(p1x, p1p),p2x)  where we have replaced . with &#38; everywhere in foldl s type to get \nthe type of afoldl. This program is not well-typed, because p1p is still used in each branch of the case \ndespite the fact that p2p is case-analyzed. The +E rule sums together these uses, so the result has sensitivity \n2, while afoldl is supposed to be only 1-sensitive to its argument of type s &#38; t alist. We would \nlike to case-analyze the structure of the second com\u00adponent of that pair, the t alist, without effectively \nconsuming the .rst component. The existing type system does not permit this, but we can soundly add a \nprimitive2 analyze : s &#38;(t1 + t2) -(s &#38; t1)+(s &#38; t2) that gives us the extra bit that we \nneed. The operational behavior of analyze is simple: given a pair value (v, inji v ' ) with v : s and \nv ' : ti, it returns inji (v, v ' ). With this primitive, a well-typed implementation of afoldl can be \ngiven as follows: unf :(s &#38; t alist) -(s &#38; (1+ t &#38; t alist)) unf p = (p1p, unfoldt alist \np2p) afoldl :(t &#38; s -s) . (s &#38; t alist) -s alist afoldl fp = case analyze (unf p) of x :(s &#38; \n1).p1x | x :(s &#38;(t &#38; t alist)). afoldl f (f (p1p2x, p1x),p2p2x)  3.4 Sets Another useful collection \ntype is .nite sets. We posit that t set is a type for any type t , with the metric on it being the Hamming \nmetric dt set(S1,S2)= ||S1 D S2|| where D indicates symmetric difference of sets, and ||S|| the car\u00addinality \nof the set S; the distance between two sets is the number of elements that are in one set but not the \nother. Note that there is no obvious way to implement this type of sets in terms of the list types just \npresented, for the metric is different: two sets of different size are a .nite distance from one another, \nbut two lists of different size are in.nitely far apart. Primitives that can be added for this type include \nsize : t set -R set.lter :(t . bool) . t set -t set setmap :(s . t ) . t . s set -t set n, ., \\ : t set \n. t set -t set split :(t . bool) . t set -t set . t set where size returns the cardinality of a set, \nn returns the intersection of two sets, . their union, and \\ the difference. Notably, for these last \nthree primitives, we could not have given them the type t set &#38; t set -t set. To see why, consider \n{b}.{c, d} = {b, c, d}and {a}.{c, d, e} = {a, c, d, e}. We have d({b}, {a})=2 and d({c, d}, {c, d, e})=1 \non the two inputs to ., but on the output d({b, c, d}, {a, c, d, e})=3, and 3 is strictly larger than \nmax(2, 1). The functions set.lter and setmap work mostly as ex\u00adpected, but with a proviso concerning \ntermination below in Sec\u00adtion 3.5. We note that size is a special case of a more basic summation primitive: \nsum :(t . R) . t set -R 2 The reader may note that this primitive is exactly the well-known dis\u00adtributivity \nproperty that the BI, the logic of bunched implications [28], no\u00adtably satis.es in contrast with linear \nlogic. We conjecture that a type system based on BI might also be suitable for distance-sensitive computations, \nbut we leave this to future work, because of uncertainties about the decidabil\u00adity of typechecking and \nBI s lack of exponentials, that is, operators such as !, which are important for interactions between \ndistance-sensitive and -insensitive parts of a program. P The expression sum fS returns s.S clip(f (s)), \nwhere clip(x) returns x clipped to the interval [-1, 1] if necessary. This clipping is required for sum \nto be 1-sensitive in its set argument. Otherwise, an individual set element could affect the sum by an \nunbounded amount. We can then de.ne size S = sum (.x.1) S. The operation split takes a predicate on t \n, and a set; it yields two sets, one containing the elements of the original set that satisfy the predicate \nand the other containing all the elements that don t. Notice that split is 1-sensitive in its set argument; \nthis is because if an element is added to or removed from that set, it can only affect one of the two \noutput sets, not both. By using split repeatedly, we can write programs that, given a set of points in \nR, computes a histogram, a list of counts in\u00addicating how many points are in each of many intervals. \nFor a simple example, suppose our histogram bins are the intervals (-8, 0], (0, 10],..., (90, 100], (100, \n8). hist' : R . Rset -Rset list hist' cs = if c = 101 then [s] else let(y, n)= split(.z.c = z) in y :: \nhist' (c + 10) n hist : Rset -Rlist hist s = map size (hist' 0 s) Here we are also assuming the use \nof ordinary distance\u00adinsensitive arithmetic operations such as = : R . R . bool and +: R . R . R. We \nsee in the next section that comparison operators like = cannot be so straightforwardly generalized to \nbe distance sensitive. 3.5 Higher-Order Set Operations and Termination Some comments must be made on \nthe termination of the higher\u00adorder functions set.lter, setmap, and setsplit. Consider the expres\u00adsion \nset.lter fs for s of type t set and an arbitrary function f : t . bool. If f diverges on some particular \ninput v : t , then the presence or absence of v in the set s can make set.lter fs diverge or terminate. \nThis runs afoul of the claim of theorem 2.4 that two metrically similar computations should together \nevaluate to metrically nearby values. A way of avoiding this problem is to adopt primitives for which \n2.4 can still be proved: we can ensure dynamically that the function argument (set.lter, setmap, and \nset\u00adsplit) terminates by imposing a time limit on the number of steps it can run over each element of \nthe set. Whenever a function exceeds its time limit while operating on a set element x, it is left out \nof the .lter or of the current split as appropriate, and in the case of setmap, a default element of \ntype t is used. One alternative is to weaken theorem 2.4 to state that if two computations over metrically \nrelated inputs do both terminate, then their outputs are metrically related. This weakened result is \ncon\u00adsiderably less desirable for our intended application to differential privacy, however. A .nal option \nis to statically ensure the termination of the function argument. This seems to combine the best features \nof both of the other choices, but at the price of greater dif.culty of program analysis.  3.6 Sorting \nWhat about distance-sensitive sorting? Ordinarily, the basis of sort\u00ading functions is a comparison operator \nsuch as =t : t \u00d7 t . bool. However, we cannot take =R: R. R -bool as a primitive, be\u00adcause = is not 1-sensitive \nin either of its arguments: it has a glaring discontinuity. (Compare the example of gtzero in Section \n2.1) Al\u00adthough (0,E) and (E, 0) are nearby values in R . R if E is small (they are just 2E apart), nonetheless \n=R returns false for one and true for the other, values of bool that are by de.nition in.nitely far apart. \n Because of this we instead take as a primitive the conditional swap function cswp : R.R -R.Rde.ned \nin Section 2.1, which takes in a pair, and outputs the same pair, swapped if necessary so that the .rst \ncomponent is no larger than the second. We are therefore essentially concerned with sorting networks, \n[5] with cswp being the comparator. With the comparator, we can easily implement a version of insertion \nsort. insert : R -Rlist -Rlist insert x [] = [x] insert x (h :: tl)= let(a, b)= cswp (x, h) in a :: (insert \nb tl) sort : Rlist -Rlist sort [] = [] sort (h :: tl)= insert h (sort tl) Of course, the execution time \nof this sort is T(n 2). It is an open question whether any of the typical T(n log n) sorting al\u00adgorithms \n(merge sort, quick sort, heap sort) can be implemented in our language, but we can implement bitonic \nsort [5], which is T(n(log n)2), and we conjecture that one can implement the log\u00addepth (and therefore \nT(n log n) time) sorting network due to Ajtai, Koml\u00b4edi [2]. os, and Szemer\u00b4  3.7 Finite Maps Related \nto sets are .nite maps from s to t , which we write as the type s-t . A .nite map f from s to t is an \nunordered set of tuples (s, t) where s : s and t : t , subject to the constraint that each key s has \nat most one value t associated with it: if (s, t) . f and (s, t ' ) . f, then t = t '. One can think \nof .nite maps as SQL databases where one column is distinguished as the primary key. This type has essentially \nthe same metric as the metric for sets, ds.t (S1,S2)= ||S1 D S2||. By isolating the primary key, we can \nsupport some familiar relational algebra operations: fmsize :(s-t) -R fm.lter :(s . t . bool) . (s-t) \n-(s-t ) mapval :(t1 . t2) . (s-t1) -(s-t2) join :(s-t1) . (s-t2) -(s-(t1 . t2)) The size and .lter functions \nwork similar to the corresponding operations on sets, and there are now two different map operators, \none that operates on keys and one on values. The join operation takes two maps (i, si)i.I1 and (i, s \n' i)i.I2 , and outputs the map (i, (s1,s2))i.I1nI2 . This operation is 1-sensitive in the pair of input \nmaps, but only because we have identi.ed a unique primary key for both of them! For comparison, the cartesian \nproduct \u00d7 on sets the operation that join is ordinarily derived from in relational algebra is not c-sensitive \nfor any .nite c, for we can see that ({x}. X) \u00d7 Y has |Y | many more elements than X \u00d7 Y . McSherry also \nnoted this issue with unrestricted joins, and deals with it in a similar way in PINQ [25]. Finally, we \nare also able to support a form of GroupBy aggre\u00adgation, in the form of a primitive group :(t . s) . \n!2t set -(s-(t set)) which takes a key extraction function f : t . s, and a set S of values of type t \n, and returns a .nite map which maps values y . s to the set of s . S such that f(s)= y. This function \nis 2-sensitive (thus the !2) in the set argument, because the addition or removal of a single set element \nmay change one element in the output map: it takes two steps to represent such a change as the removal \nof the old mapping, and the insertion of the new one. 4. A Calculus for Differential Privacy We now describe \nhow to apply the above type system to expressing differentially private computations. There are two ways \nto do this. One is to leverage the fact that our type system captures sensitiv\u00adity, and use standard \nresults about obtaining differential privacy by adding noise to c-sensitive functions. Since Theorem \n2.4 guaran\u00adtees that every well-typed expression b :c db f e : R (for a type db of databases) is a c-sensitive \nfunction db . R, we can apply Proposition 4.1 below to obtain a differentially private function by adding \nthe appropriate amount of noise to the function s result. But we can do better. In this section, we show \nhow adding a probability monad to the type theory allows us to directly capture differential privacy \nwithin our language. 4.1 Background First, we need a few technical preliminaries from the differential \nprivacy literature [14]. The de.nition of differential privacy is a property of random\u00adized functions \nthat take as input a database, and return a result, typically a real number. For the sake of the current \ndiscussion, we take a database to be a set of rows , one for each user whose privacy we mean to protect. \nThe type of one user s data that is, of one row of the database is written row. For example, row might \nbe the type of a single patient s complete medical record. The type of databases is then db = row set; \nwe use the letter b for elements of this type. Differential privacy is parametrized by a number E, which \ncontrols how strong the privacy guarantee is: the smaller E is, the more privacy is guaranteed. It is \nperhaps just as well to think about E as a measure rather of how much privacy can be lost by allowing \na query to take place. We assume from now on that we have .xed E to some particular appropriate value. \nInformally, a function is differentially private if it behaves statis\u00adtically similarly on similar databases, \nso that any individual s pres\u00adence in the database has a statistically negligible effect. Databases b \nand b ' are considered similar, written b ~ b ' if they differ by at most one row in other words if ddb(b, \nb ' ) = 1. The standard def\u00adinition [15] of differential privacy for functions from databases to real \nnumbers is as follows: De.nition A random function q : db . R is E-differentially private if for all \nS . R, and for all databases b, b ' with b ~ b ' , E we have Pr[q(b) . S] = ePr[q(b ' ) . S]. We see \nthat for a differentially private function, when its input database has one row added or deleted, there \ncan only be a very small multiplicative difference (eE) in the probability of any out\u00adcome S. For example, \nsuppose an individual is concerned about their data being included in a query to a hospital s database; \nperhaps that the result of that query might cause them to be denied health insurance. If we require that \nquery to be 0.1-differentially private (i.e., if E is set to 0.1), then they can be reassured that the \nchance of them being denied health care can only increase by about 10%. (Note that this is a 10% increase \nrelative to what the probability would have been without the patient s participation in the database. \nIf the probability without the patient s data being included was 5%, then including the data raises it \nat most to 5.5%, not to 15%!) It is straightforward to generalize this de.nition to other types, by using \nthe distance between two inputs instead of the database similarity condition. We say: De.nition A random \nfunction q : t . s is E-differentially private if for all S . s, and for all v, v ' : t , have Pr[q(v) \n. S] = Edt (v,v')' ePr[q(v ) . S]. Although we will use this general de.nition below in Lemma 4.2, for \nthe time being we continue considering only functions db . R.  One way to achieve differential privacy \nis via the Laplace mech\u00adanism. We suppose we have a deterministic database query, a func\u00adtion f : db \n. R of known sensitivity, and we produce a differ\u00adentially private function by adding Laplace-distributed \nnoise to the result of f. The Laplace distribution Lk is parametrized by k intuitively, a measure of \nthe spread, or amount , of noise to be 1 -|x|/k added. It has the probability density function Pr[x]= \n2k e. The Laplace distribution is symmetric and centered around zero, and its probabilities fall off \nexponentially as one moves away from zero. It is a reasonable noise distribution, which is unlikely to \nyield values extremely far from zero. The intended behavior of the Laplace mechanism is captured by the \nfollowing result: PROPOSITION 4.1 ([15]). Suppose f : db . R is c-sensitive. De.ne the random function \nq : db . R by q = .b.f(b)+ N, where N is a random variable distributed according to Lc/E. Then q is E-differentially \nprivate. That is, the amount of noise required to make a c-sensitive function E-private is c/E. Stronger \nprivacy requirements (smaller E) and more sensitive functions (larger c) both require more noise. Note \nthat we must impose a global limit on how many queries can be asked of the same database: if we could \nask the same query over and over again, we could eventually learn the true value of f with high probability \ndespite the noise. If we exhaust the privacy budget for a given database, the database must be destroyed. \nThis data-consuming aspect of differentially private queries was the initial intuition that guided us \nto the linear-logic-inspired design of the type system.  4.2 The Probability Monad We now show how to \nextend our language with a monad of random computations. Formally, the required extensions to the syntax \nare: Types t ::= \u00b7\u00b7\u00b7 | Ot Expressions e ::= \u00b7\u00b7\u00b7 | return x | let Ox = e in e ' Values v ::= \u00b7\u00b7\u00b7 | d We \nadd Ot , the type of random computations over t . Expres\u00adsions now include a monadic return, which deterministically \nal\u00adways yields x, as well as monadic sequencing: the expression let Ox = e in e ' can be interpreted \nas drawing a sample x from the random computation e, and then continuing with the compu\u00adtation e '. We \npostpone discussing the typing rules until after we have established what the metric on Ot is, and for \nthat we need to understand what its values are. For simplicity, we follow Ramsey and Pfeiffer [30] in \ntaking a rather denotational approach, and think of values of type Ot as literally being mathematical \nprobability distributions. A more strictly syntactic presentation (in terms of, say, pseudo-random number \ngenerators) certainly is also possible, but is needlessly technical for our present discussion. In what \nfollows, a probability distribution d is written as (pi,vi)i.I , a multiset of probability- The typing \nrules for the monad are as follows: G f e : t . f e : Ot G, x :8 t f e ' : Ot ' OI OE 8G f return e : \nOt . + G f let Ox = e in e ' : Ot ' The introduction rule multiplies the context by in.nity, because \nnearby inputs (perhaps surprisingly!) do not lead to nearby deter\u00administic probability distributions. \nEven if t and t ' are close, say dt (t, t ' )= E, still return t has a 100% chance and return t ' has \na 0% chance of yielding t. The elimination rule adds to\u00adgether the in.uence . that e may have over the \n.nal output dis\u00adtribution to the in.uence G that e ' has, and provides the variable x unrestrictedly \n(with annotation 8) to e ', because once a differen\u00adtially private query is made, the published result \ncan be used in any way at all. We add the following cases to the operational semantics: e'. v return \ne'. (1,v) e1 '. (pi,vi)i.I .i . I. [vi/x]e2 '. (qij ,wij )j.Ji let Ox = e1 in e2 '. (piqij ,wij )i.I,j.Ji \nWe see that return creates the trivial distribution that always yields v. Monadic sequencing considers \nall possible values vi that e could evaluate to, and then subsequently all the values that e ' could \nevaluate to, assuming that it received the sample vi. The probabilities of these two steps are multiplied, \nand appropriately aggregated together. Combining the type system s metric preservation property with \nLemma 4.2, we .nd that typing guarantees differential privacy: COROLLARY 4.3. The execution of any closed \nprogram e such that f e :!nt -Os is an (nE)-differentially private function from t to s. 5. Differential \nPrivacy Examples Easy examples of E-differentially private computations come from applying the Laplace \nmechanism at the end of a deterministic computation. We can add a primitive function add noise : R -OR \nwhich adds Laplace noise L1/E to its input. According to Propo\u00adsition 4.1, this is exactly the right \namount of noise to add to a 1\u00adsensitive function to make it E-differentially private. For a concrete \nexample, suppose that we have a function age : row . int. We can then straightforwardly implement the \nover-40 count query from the introduction. over 40 : row . bool. over 40 r = age r> 40. count query \n: row set -OR count query b = add noise (set.lter over 40 b)  P value pairs. We write d(v) for the probability \n((pi,vi)i.I )(v)= Notice that we are able to use convenient higher-order functional pi of observing v \nin the distribution d. {i | vi=v} programming idioms without any dif.culty. The function over 40 The \nmetric on probability distributions is carefully chosen to is also an example of how ordinary programming \ncan safely comparison >. max x.t ln d1(x) d2(x) \u00ab \u00ab .... 1 dOt (d1,d2)= E Other deterministic queries \ncan be turned into differentially pri-The de.nition measures how multiplicatively far apart two distri\u00ad \nbutions are in the worst case, as is required by differential privacy. We can then easily see by unrolling \nde.nitions that ' LEMMA 4.2. A 1-sensitive function t . Os is the same thing as hist query : row set \n-(OR) list ' an E-differentially private random function t . s. hist query b = map add noise (hist (setmap \nage b)) This takes a database, .nds the age of every individual, and com\u00adputes a histogram of the ages. \nThen we prescribe that each item in the output list every bucket in the histogram should be inde\u00adpendently \nnoised. This yields a list of random computations, while what we ultimately want is a random computation \nreturning a list. But we can use monadic sequencing to get exactly this: seq :(OR) list -O(Rlist) seq \n[] = return [] seq (h :: tl)= let Oh ' = h in let Otl ' = seq tl in return(h ' :: tl ' ) hist query \n: row set -O(Rlist) hist query b = seq (hist query ' b) In the differential privacy literature, there \nare explicit de.nitions of both the meaning of sensitivity and the process of safely adding enough noise \nto lists of real numbers [15]. By contrast, we have shown how to derive these concepts from the primitive \nmetric type Rand the type operators \u00b5, 1, +, ., and O. We can also derive more complex combinators on \ndifferentially private computations, merely by programming with the monad. We consider .rst a simple \nversion3 of McSherry s principle of sequential composition [25]. LEMMA 5.1 (Sequential Composition). \nLet f1 and f2 be two E\u00addifferentially private queries, where f2 is allowed to depend on the output of \nf1. Then the result of performing both queries is 2E\u00addifferentially private. In short, the privacy losses \nof consecutive queries are added to\u00adgether. This principle can be embodied as the following higher\u00adorder \nfunction: sc :(t1 -Ot2) . (t1 -t2 . Ot3) . (!2t1 -Ot3) ' '' sc f1 f2 t1 = let !t1 = t1 in let Ot2 = f1 \nt1 in f2 t1 t2 It takes two arguments are the functions f1 and f2, which are both E-differentially private \nin a data source of type t1 (and f2 additionally has unrestricted access to the t2 result of f1), and \nreturns a 2E-differentially private computation. McSherry also identi.es a principle of parallel composition: \nLEMMA 5.2 (Parallel Composition). Let f1 and f2 be two E\u00addifferentially private queries, which depend \non disjoint data. Then the result of performing both queries is E-differentially private. This can be \ncoded up by interpreting disjoint with .. pc :(t1 -Ot2) . (s1 -Os2) . (t1 . s1) -O(t2 . s2) ' pc fg (t, \ns)= let Ot ' = ft in let Os = gs in return(t ' ,s ' ) In McSherry s work, what is literally meant by \ndisjoint is disjoint subsets of a database construed as a set of records. This is also possible to treat \nin our setting, since we have already seen that split returns a .-pair of two sets. For a .nal, slightly \nmore complex example, let us consider the privacy-preserving implementation of k-means by Blum et al. \n[6]. Recall that k-means is a simple clustering algorithm, which works as follows. We assume we have \na large set of data points in some space (say Rn), and we want to .nd k centers around which they cluster. \nWe initialize k provisional centers to random points in the space, and iteratively try to improve these \nguesses. One iteration consists of grouping each data point with the center it is closest to, then taking \nthe next round s set of k centers to be the mean of each group. 3 McSherry actually states a stronger \nprinciple, where there are k different queries, all of different privacy levels. This can also be implemented \nin our language. We sketch how this program can be implemented, taking data points to be of the type \npt = R.R. The following helper functions are used: assign : pt list . pt set -(pt . int) set partition \n:(pt . int) set -pt set list totx, toty : pt set -R zip : t list . s list . (t . s) list These can be \nwritten with the means primitives we have described; assign takes a list of centers and the dataset, \nand returns a version of the dataset where each point is labelled by the index of the center it s closest \nto. Then partition di\u00advides this up into a list of sets, us\u00ading split. The functions totx and toty compute \nthe sum of the .rst and second coordinates, respec\u00adtively, of each point in a set. This Figure 4. k-Means \nOutput can be accomplished with sum. Finally, zip is the usual zipping operation that combines two lists \ninto a list of pairs. With these, we can write a function that performs one iteration of private k-means: \niterate :!3pt set -Rlist . O(Rlist) iterate b ms = let !b ' = b in let b '' = partition (assign ms b \n' ) tx = map (add noise . totx) b '' ty = map (add noise . toty) b '' t = map (add noise . size) b '' \nstats = zip (zip (tx, ty),t) in seq (map avg stats) It works by asking for noisy sums of the x-coordinate \ntotal, y\u00adcoordinate total, and total population of each cluster. These data are then combined via the \nfunction avg: avg : ((OR. OR) . OR) -O(R. R) avg ((x, y),t)= let Ox ' = x in let Oy ' = y in ' '' let \nOt = t in return (x /t ' ,y /t ' ) We can read off from the type that one iteration of k-means is 3E\u00addifferentially \nprivate. This type arises from the 3-way replication of the variable b ''. We can use monadic sequencing \nto do more than one iteration: two iters :!6pt set -Rlist . O(Rlist) two iters b ms = let !b ' = b in \niterate !b ' (iterate !b ' ms) This function is 6E-differentially private. Figure 4 shows the result \nof three independent runs of this code, with k =2, 6E =0.05, and 12,500 points of synthetic data. We \nsee that it usually manages to come reasonably close to the true center of the two clusters. We have \nalso developed appropriate additional primitives and program\u00adming techniques to make it possible (as \none would certainly hope!) to choose the number of iterations not statically but at runtime, but space \nreasons prevent us from discussing them here. 6. Metatheory In this section we address the formal correctness \nof the program\u00adming language described above. First of all, we can prove appro\u00adpriate versions of the \nusual basic properties that we expect to hold of a well-formed typed programming language. LEMMA 6.1 \n(Weakening). If G f e : t , then G+. f e : t .  .v : t1.[v/x]e1 ~r [v/x]e2 : t2 .v : t1.[v/x]e1 ~r [v/x]e2 \n: t2 .x.e1 ~r .x.e2 : t1 . t2 .x.e1 ~r .x.e2 : t1 -t2 '' ' v1 ~r1 v1 : t1 v2 ~r2 v2 : t2 v ~r v : t '' \n' (v1,v2) ~r1+r2 (v1,v 2): t1 . t2 !v ~rs !v :!st '' v1 ~r v1 : t1 v2 ~r v2 : t2 '' () ~r () : 1 v1,v2 \n~r v1,v : t1 &#38; t2 2 '' v ~r v :[\u00b5a.t/a]tv ~r v : ti '' fold v ~r fold v : \u00b5a.t inji v ~r inji v : \nt1 + t2 .v1 : t.e1 . v1 ..v2.e2 . v2 . v1 ~r v2 : t e1 ~r e2 : t Figure 5. Metric Relation THEOREM 6.2 \n(Substitution). If G f e : t and .,x :r t f e ' : t ', then .+ rG f [e/x]e ' : t ' . THEOREM 6.3 (Preservation). \nIf f e : t and e'. v, then f v : t . Note that the weakening lemma allows both making the context larger, \nand making the annotations numerically greater. The sub\u00adstitution property says that if we substitute \ne into a variable that is used r times, then G, the dependencies of e, must be multiplied by r in the \nresult. The preservation lemma is routine; if we had pre\u00adsented the operational semantics in a small-step \nstyle, a progress theorem would also be easy to show. 6.1 De.ning the Metric Up to now, the metrics on \ntypes have been dealt with somewhat informally; in particular, our de.nition of distance for recursive \ntypes was not well founded. We now describe a formal de.nition. It is convenient to treat the metric \nnot as a function, but rather as a relation on values and expressions. The relation v ~r v ' : t (resp. \ne ~r e ' : t) means that values v and v ' (expressions e and e ') of type t are at a distance of no more \nthan r apart from each other. The metric on expressions is de.ned by evaluation: if the values that result \nfrom evaluation of the two expressions are no farther than r apart, then the two expressions are considered \nto be no farther than r apart. This relation is de.ned coinductively on the rules in Figure 5. By this \nwe mean that we de.ne v ~r v ' : t to be the greatest relation consistent with the given rules. A relation \nis said to be consistent with a set of inference rules if for any relational fact that holds, there exists \nan inference rule whose conclusion is that fact, and all premises of that rule belong to the relation. \nIntuitively, this means that we allow in.nitely deep inference trees. Note that ~r never appears negatively \n(i.e., negated or to the left of an implication) in the premise of any rule, so we can see that closure \nunder the rules is a property preserved by arbitrary union of relations, and therefore the de.nition \nis well-formed.  6.2 Metric Preservation Theorem Now we can state the central novel property that our \ntype system guarantees. We introduce some notation to make the statement more compact. Suppose G= x :s1 \nt1,...x :sn tn. A substitution s for G is a list of individual substitutions of values for variables \nin G, written [v1/x1] \u00b7\u00b7\u00b7 [vn/xn].A distance vector . is a list r1,...,rn such that every ri is in R=0 \n.8. We say s ~. s ' :G when, for every [vi/xi] . s and [vi' /xi] . s ', we have vi ~ri vi ' : ti. In \nthis case we think of s and s ' as being . apart : the distance vector . tracks the distance between \neach corresponding pair of values. We de.ne the dot product of a distance vector and a context as follows: \nif . is r1,...,rn, and G is as above, then P n . \u00b7 G= i=1 risi. THEOREM 6.4 (Metric Preservation). Suppose \nG f e : t . Sup\u00adpose s, s ' are two substitutions for G such that s ~. s ' :G. Then we have se ~.\u00b7G s \n' e : t. A straightforward proof attempt of this theorem fails. If we try to split cases by the typing \nderivation of e, a problem arises at the case where e = e1 e2. The induction hypothesis will tell us \nthat se1 is close to s ' e1, and that se2 is close to s ' e2. But the de.nition of the metric at function \ntypes (whether . or - the problem arises for both of them) only quanti.es over one value how then can \nwe reason about both se2 and s ' e2? This problem is solved by using a step-indexed metric logical relation \n[1, 3] which represents a stronger induction hypothesis, but which agrees with the metric. We defer further \ndetails of this argument to the appendix. 7. Related Work The seminal paper on differential privacy is \n[15]; it introduces the fundamental de.nition and the Laplace mechanism. More general mechanisms for \ndirectly noising types other than R also exist, such as the exponential mechanism [24], and techniques \nhave been developed to reduce the amount of noise required for repeated queries, such as the median mechanism \n[31]. Dwork [13] gives a useful survey of recent results. Girard s linear logic [16] was a turning point \nin a long and fruitful history of investigation of substructural logics, which lack structural properties \nsuch as unrestricted weakening and contrac\u00adtion. A key feature of linear logic compared to earlier substructural \nlogics [20] is its ! operator, which bridges linear and ordinary rea\u00adsoning. Our type system takes its \nstructure from the af.ne variant of linear logic (also related to Ketonen s Direct Logic [19]), where \nweakening is permitted. The idea of counting, as we do, multiple uses of the same resource was explored \nby Wright [32], but only integral numbers of uses were considered. The study of database privacy and \nstatistical databases more generally has a long history. Recent work includes Dalvi, R\u00b4e, and Suciu s \nstudy of probabilistic database management systems [11], and Machanavajjhala et al. s comparison of different \nnotions of privacy with respect to real-world census data [22]. Quantitative Information Flow [21, 23] \nis, like our work, con\u00adcerned with how much one piece of a program can affect another, but measures this \nin terms of how many bits of entropy leak during one execution. Provenance analysis [8] in databases \ntracks the input data actually used to compute a query s output, and is also capable of detecting that \nthe same piece of data was used multiple times to produce a given answer [17]. Chaudhuri et al. [10] \nalso study auto\u00admatic program analyses that establish continuity (in the traditional topological sense) \nof numerical programs. Our approach differs in two important ways. First, we consider the stronger property \nof c\u00adsensitivity, which is essential for differential privacy applications. Second, we achieve our results \nwith a logically motivated type sys\u00adtem, rather than a program analysis. 8. Conclusion We have presented \na typed functional programming language that guarantees differential privacy. It is expressive enough \nto encode examples both from the differential privacy community and from functional programming practice. \nIts type system shows how dif\u00adferential privacy arises conceptually from the combination of sensi\u00adtivity \nanalysis and monadic encapsulation of random computations.  There remains a rich frontier of differentially \nprivate mecha\u00adnisms and algorithms that are known, but which are described and proven correct individually. \nWe expect that the exponential mecha\u00adnism should be easy to incorporate into our language, as a higher\u00adorder \nprimitive which directly converts McSherry and Talwar s no\u00adtion of quality functions [24] into probability \ndistributions. The me\u00addian mechanism, whose analysis is considerably more complicated, is likely to be \nmore of a challenge. The private combinatorial op\u00adtimization algorithms developed by Gupta et al. [18] \nuse different de.nitions of differential privacy which have an additive error term; we conjecture this \ncould be captured by varying the notion of sen\u00adsitivity to include additive slack. We believe that streaming \nprivate counter of Chan et al. [9] admits an easy implementation by coding up stream types in the usual \nway. We hope to show in future work how these, and other algorithms can be programmed in a uniform, privacy-safe \nlanguage. Acknowledgments Thanks to Helen Anderson, Jonathan Smith, Andreas Haeberlen, Adam Aviv, Daniel \nWagner, Michael Hicks, Katrina Ligett, Aaron Roth, and Michael Tschantz for helpful discussions. This \nwork was supported by ONR Grant N00014-09-1-0770 Networks Opposing Botnets (NoBot) . References [1] A. \nAhmed. Step-indexed syntactic logical relations for recursive and quanti.ed types. In of Lecture Notes \nin Computer Science, volume 3924, pages 69 83, 2006. [2] M. Ajtai, J. Koml\u00b4edi. os, and E. Szemer\u00b4Sorting \nin c log n parallel steps. Combinatorica, 3(1):1 19, March 1983. ISSN 0209-9683. [3] A. W. Appel and \nD. McAllester. An indexed model of recursive types for foundational proof-carrying code. ACM Trans. Program. \nLang. Syst., 23(5):657 683, 2001. ISSN 0164-0925. [4] A. Barber. Dual intuitionistic linear logic. Technical \nReport ECS\u00adLFCS-96-347, University of Edinburgh, 1996. [5] K. E. Batcher. Sorting networks and their \napplications. In AFIPS 68 (Spring): Proceedings of the April 30 May 2, 1968, spring joint computer conference, \npages 307 314, New York, NY, USA, 1968. ACM. [6] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical \nprivacy: the sulq framework. In PODS 05: Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium \non Principles of database sys\u00adtems, pages 128 138, New York, NY, USA, 2005. ACM. [7] A. Blum, K. Ligett, \nand A. Roth. A learning theory approach to non\u00adinteractive database privacy. In STOC 08: Proceedings \nof the 40th annual ACM symposium on Theory of computing, pages 609 618, New York, NY, USA, 2008. ACM. \n[8] P. Buneman, S. Khanna, and T. Wang-Chiew. Why and where: A characterization of data provenance. In \nJ. Bussche and V. Vianu, editors, Database Theory ICDT 2001, volume 1973 of Lecture Notes in Computer \nScience, chapter 20, pages 316 330. Springer Berlin Heidelberg, Berlin, Heidelberg, October 2001. [9] \nT.-H. H. Chan, E. Shi, and D. Song. Private and continual release of statistics. Cryptology ePrint Archive, \nReport 2010/076, 2010. http://eprint.iacr.org/. [10] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity \nanalysis of programs. SIGPLAN Not., 45(1):57 70, 2010. ISSN 0362-1340. [11] N. Dalvi, C. R\u00b4e, and D. \nSuciu. Probabilistic databases: diamonds in the dirt. Commun. ACM, 52(7):86 94, 2009. [12] C. Dwork. \nThe differential privacy frontier (extended abstract). In Theory of Cryptography, Lecture Notes in Computer \nScience, chap\u00adter 29, pages 496 502. 2009. [13] C. Dwork. Differential privacy: A survey of results. \n5th International Conference on Theory and Applications of Models of Computation, pages 1 19, 2008. [14] \nC. Dwork. Differential privacy. In Proceedings of ICALP (Part, volume 2, pages 1 12, 2006. [15] C. Dwork, \nF. Mcsherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory \nof Cryptography Conference, 2006. [16] J. Girard. Linear logic. Theoretical Computer Science, 50(1):1 \n102, 1987. [17] T. J. Green, G. Karvounarakis, and V. Tannen. Provenance semir\u00adings. In PODS 07: Proceedings \nof the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 31 40, \nNew York, NY, USA, 2007. ACM. [18] A. Gupta, K. Ligett, F. McSherry, A. Roth, and K. Talwar. Differen\u00adtially \nprivate combinatorial optimization. Nov 2009. [19] J. Ketonen. A decidable fragment of predicate calculus. \nTheoretical Computer Science, 32(3):297 307, 1984. ISSN 03043975. [20] J. Lambek. The mathematics of \nsentence structure. American Mathe\u00admatical Monthly, 65(3):154 170, 1958. [21] G. Lowe. Quantifying information \n.ow. In In Proc. IEEE Computer Security Foundations Workshop, pages 18 31, 2002. [22] A. Machanavajjhala, \nD. Kifer, J. Abowd, J. Gehrke, and L. Vilhuber. Privacy: Theory meets practice on the map. In ICDE 08: \nProceedings of the 2008 IEEE 24th International Conference on Data Engineering, pages 277 286, Washington, \nDC, USA, 2008. IEEE Computer Soci\u00adety. [23] S. McCamant and M. D. Ernst. Quantitative information .ow \nas net\u00adwork .ow capacity. In PLDI 08: Proceedings of the 2008 ACM SIG-PLAN conference on Programming \nlanguage design and implementa\u00adtion, pages 193 205, New York, NY, USA, 2008. ACM. [24] F. McSherry and \nK. Talwar. Mechanism design via differential privacy. In FOCS 07: Proceedings of the 48th Annual IEEE \nSymposium on Foundations of Computer Science, pages 94 103, Washington, DC, USA, 2007. IEEE Computer \nSociety. [25] F. D. McSherry. Privacy integrated queries: an extensible platform for privacy-preserving \ndata analysis. In SIGMOD 09: Proceedings of the 35th SIGMOD international conference on Management of \ndata, pages 19 30, New York, NY, USA, 2009. ACM. [26] A. Narayanan and V. Shmatikov. Robust de-anonymization \nof large sparse datasets. In SP 08: Proceedings of the 2008 IEEE Sym\u00adposium on Security and Privacy, \npages 111 125, Washington, DC, USA, 2008. IEEE Computer Society. ISBN 978-0-7695-3168-7. doi: http://dx.doi.org/10.1109/SP.2008.33. \n[27] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. \nIn STOC 07: Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 75 84, \nNew York, NY, USA, 2007. ACM. [28] P. O Hearn and D. Pym. The logic of bunched implications. Bulletin \nof Symbolic Logic, 5(2):215 244, 1999. [29] S. Park, F. Pfenning, and S. Thrun. A monadic probabilistic \nlanguage. In In Proceedings of the 2003 ACM SIGPLAN international workshop on Types in languages design \nand implementation, pages 38 49. ACM Press, 2003. [30] N. Ramsey and A. Pfeffer. Stochastic lambda calculus \nand monads of probability distributions. In In 29th ACM POPL, pages 154 165. ACM Press, 2002. [31] A. \nRoth and T. Roughgarden. The median mechanism: Interactive and ef.cient privacy with multiple queries, \n2010. To appear in STOC 2010. [32] D. Wright and C. Baker-Finch. Usage Analysis with Natural Reduc\u00adtion \nTypes. In Proceedings of the Third International Workshop on Static Analysis, pages 254 266. Springer-Verlag \nLondon, UK, 1993.    \n\t\t\t", "proc_id": "1863543", "abstract": "<p>We want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. <i>Differential privacy</i> offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacy-safe query programs in a flexible and compositional way.</p> <p>The key novelty is the way our type system captures <i>function sensitivity</i>, a measure of how much a function can magnify the distance between similar inputs: well-typed programs not only can't go wrong, they <i>can't go too far</i> on nearby inputs. Moreover, by introducing a monad for random computations, we can show that the established definition of differential privacy falls out naturally as a special case of this soundness principle. We develop examples including known differentially private algorithms, privacy-aware variants of standard functional programming idioms, and compositionality principles for differential privacy.</p>", "authors": [{"name": "Jason Reed", "author_profile_id": "81541655256", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P2338177", "email_address": "", "orcid_id": ""}, {"name": "Benjamin C. Pierce", "author_profile_id": "81100303310", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P2338178", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863568", "year": "2010", "article_id": "1863568", "conference": "ICFP", "title": "Distance makes the types grow stronger: a calculus for differential privacy", "url": "http://dl.acm.org/citation.cfm?id=1863568"}