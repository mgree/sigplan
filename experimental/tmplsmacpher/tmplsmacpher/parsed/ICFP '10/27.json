{"article_publication_date": "09-27-2010", "fulltext": "\n Total Parser Combinators Nils Anders Danielsson School of Computer Science, University of Nottingham, \nUnited Kingdom nad@cs.nott.ac.uk Abstract A monadic parser combinator library which guarantees termination \nof parsing, while still allowing many forms of left recursion, is described. The library s interface \nis similar to those of many other parser combinator libraries, with two important differences: one is \nthat the interface clearly speci.es which parts of the constructed parsers may be in.nite, and which \nparts have to be .nite, using dependent types and a combination of induction and coinduction; and the \nother is that the parser type is unusually informative. The library comes with a formal semantics, using \nwhich it is proved that the parser combinators are as expressive as possible. The implementation is supported \nby a machine-checked correct\u00adness proof. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: \nApplicative (Functional) Programming; E.1 [Data Struc\u00adtures]; F.3.1 [Logics and Meanings of Programs]: \nSpecifying and Verifying and Reasoning about Programs; F.4.2 [Mathematical Logic and Formal Languages]: \nGrammars and Other Rewriting Systems Grammar types, Parsing General Terms Languages, theory, veri.cation \nKeywords Dependent types, mixed induction and coinduction, parser combinators, productivity, termination \n1. Introduction Parser combinators (Burge 1975; Wadler 1985; Fairbairn 1987; Hutton 1992; Meijer 1992; \nFokker 1995; R\u00a8ojemo 1995; Swier\u00adstra and Duponcheel 1996; Koopman and Plasmeijer 1999; Lei\u00adjen and Meijer \n2001; Ljungl\u00a8of 2002; Hughes and Swierstra 2003; Claessen 2004; Frost et al. 2008; Wallace 2008, and \nmany others) can provide an elegant and declarative method for implementing parsers. When compared with \ntypical parser generators they have some advantages: it is easy to abstract over recurring grammatical \npatterns, and there is no need to use a separate tool just to parse something. On the other hand there \nare also some disadvantages: there is a risk of lack of ef.ciency, and parser generators can give static \nguarantees about termination and non-ambiguity which most parser combinator libraries fail to give. This \npaper addresses one of these points by de.ning a parser combinator library which ensures statically that \nparsing will terminate for every .nite input string. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 10, September 27 29, 2010, Baltimore, Maryland, USA. Copyright \nc &#38;#169; 2010 ACM 978-1-60558-794-3/10/09. . . $10.00 The library has an interface which is very \nsimilar to those of classical monadic parser combinator libraries. For instance, con\u00adsider the following \nsimple, left recursive, expression grammar: term ::= factor | term + factor factor ::= atom | factor \n* atom atom ::= number | ( term ) We can de.ne a parser which accepts strings from this grammar, and \nalso computes the values of the resulting expressions, as fol\u00adlows (the combinators are described in \nSection 4): mutual term = factor | . term > = . n1 . tok + > = . . factor > = . n2 . return (n1 + n2) \nfactor = atom | . factor > = . n1 . tok * > = . . atom > = . n2 . return (n1 * n2) atom = number | tok \n( > = . . . term > = . n . tok ) > = . . return n The only visible difference to classical parser combinators \nis the use of ., which indicates that the de.nitions are corecursive (see Section 2). However, we will \nsee later that the parsers types con\u00adtain more information than usual. When using parser combinators \nthe parsers/grammars are often constructed using cyclic de.nitions, as above, so it is natural to see \nthe de.nitions as being partly corecursive. However, a purely coin\u00adductive reading of the choice and \nsequencing combinators would allow de.nitions like the following ones: p = p | . p i . i p= p> = . x \n. . return (fx) For these de.nitions it is impossible to implement parsing in a total way (in the absence \nof hidden information): a de.ning characteris\u00adtic of parser combinator libraries is that non-terminals \nare implicit, encoded using the recursion mechanism of the host language, so (in a pure setting) the \nonly way to inspect p and pi is via their in.nite unfoldings. The key idea of this paper is that, even \nif non-terminals are implicit, totality can be ensured by reading choice inductively, and only reading \nan argument of the sequencing operator coinduc\u00adtively if the other argument does not accept the empty \nstring (see Section 3). To support this idea the parsers types will contain in\u00adformation about whether \nor not they accept the empty string.  The main contributions of the paper are as follows: It is shown \nhow parser combinators can be implemented in such a way that termination is guaranteed, using a combination \nof induction and coinduction to represent parsers, and a variant of Brzozowski derivatives (1964) to \nrun them.  Unlike many other parser combinator libraries these parser combinators can handle many forms \nof left recursion.  The parser combinators come with a formal semantics. The implementation is proved \nto be correct, and the combinators are shown to satisfy a number of laws.  It is shown that the parser \ncombinators are as expressive as possible (see Sections 3.5 and 4.5).  The core of the paper is Sections \n3 and 4. The former section introduces the ideas by using recognisers (parsers which do not return any \nresult other than the string matched or the string did not match ), and the latter section generalises \nto full parser combinators. Related work is discussed below. As mentioned above the parser type is de.ned \nusing mixed in\u00adduction and coinduction (Park 1980). This technique is explained in Section 2, and discussed \nfurther in the conclusions. Those read\u00aders who are not particularly interested in parser combinators \nmay still .nd the paper useful as an example of the use of this technique. The parser combinator library \nis de.ned in the dependently typed functional programming language Agda (Norell 2007; Agda Team 2010), \nwhich will be introduced as we go along. The library comes with a machine-checked1 proof which shows \nthat the imple\u00admentation is correct with respect to the semantics. The code which the paper is based \non is at the time of writing available from the author s web page. 1.1 Related work There does not seem \nto be much prior work on formally veri\u00ad.ed termination for parser combinators (or other general parsing \nframeworks). McBride and McKinna (2002) de.ne grammars in\u00adductively, and use types to ensure that a token \nis consumed before a non-terminal can be encountered, thereby ruling out left recursion and non-termination. \nDanielsson and Norell (2008) and Koprowski and Binsztok (2010) use similar ideas; Koprowski and Binsztok \nalso prove full correctness. Muad`Dib (2009) uses a monad anno\u00adtated with Hoare-style pre-and post-conditions \n(Swierstra 2009) to de.ne total parser combinators, including a .xpoint combina\u00adtor whose type rules \nout left recursion by requiring the input to be shorter in recursive calls. Note that none of these other \napproaches can handle left recursion. The library de.ned in this paper seems to be the .rst one which \nboth handles (many forms of) left recur\u00adsion and guarantees termination for every parser which is accepted \nby the host language.2 It also seems fair to say that, when com\u00adpared to the other approaches above, \nthis library has an interface which is closer to those of classical parser combinator libraries. In the \nclassical approach the ordinary general recursion of the host language is used to implement cyclic grammars; \nthis library uses ordinary corecursion (restricted by types, see Section 3). There are a number of parser \ncombinator libraries which can handle various forms of left recursion, but they all seem to come 1 Note \nthat the meta-theory of Agda has not been properly formalised, and Agda s type checker has not been proved \nto be bug-free, so take words such as machine-checked with a grain of salt. 2 Danielsson and Norell (2009) \nde.ne a parser using a specialised version of the library described in this paper. This version of the \nlibrary can handle neither left nor right recursion, and is restricted to parsers which do not ac\u00adcept \nthe empty string. A brief description of the parser interface is provided, but the implementation of \nthe backend is not discussed. with some form of restriction. The combinators de.ned here can handle many \nleft recursive grammars, but not all; for instance, the de.nition p = p is rejected statically. Lickman \n(1995) de\u00ad.nes a library which can handle left recursion if a tailor-made .xpoint combinator, based on \nan idea due to Philip Wadler, is used. He proves (informally) that parsers de.ned using his com\u00adbinators \nare terminating, as long as they are used in the right way; the argument to the .xpoint combinator must \nsatisfy a non\u00adtrivial semantic criterion, which is not checked statically. Johnson (1995) and Frost et \nal. (2008) de.ne libraries of recogniser and parser combinators, respectively, including memoisation \ncombina\u00adtors which can be used to handle left recursion. As presented these libraries can fail to terminate \nif used with grammars with an in\u00ad.nite number of non-terminals for instance, consider the gram\u00admar { \npn ::= p1+n | n . N}, implemented by the de.nition pn = memoise n (p (1 + n)) and users of the libraries \nneed to en\u00adsure manually that the combinators are used in the right way. The same limitations apply to \na library described by Ljungl\u00a8 of (2002). This library uses an impure feature, observable sharing (Claessen \nand Sands 1999), to detect cycles in the grammar. Claessen (2001) mentions a similar implementation, \nattributing the idea to Magnus Carlsson. Kiselyov (2009) also presents a combinator library which can \nhandle left recursion. Users of the library are required to anno\u00adtate left recursive grammars with something \nresembling a coinduc\u00adtive delay constructor. If this constructor is used incorrectly, then parsing can \nterminate with the wrong answer. Baars et al. (2009) represent context-free grammars, including semantic \nactions, in a well-typed way. In order to avoid problems with left recursion when generating top-down \nparsers from the grammars they implement a left-corner transformation. Neither correctness of the transformation \nnor termination of the generated parsers is proved formally. Brink et al. (2010) perform a similar exercise, \ngiving a partial proof of correctness, but no proof of termination. In Section 4.5 it is shown that the \nparser combinators are as expressive as possible every parser which can be implemented using the host \nlanguage can also be implemented using the com\u00adbinators. In the case of .nite token sets this holds even \nfor non\u00admonadic parser combinators using the applicative functor inter\u00adface (McBride and Paterson 2008); \nsee Section 3.5. The fact that monadic parser combinators can be as expressive as possible has al\u00adready \nbeen pointed out by Ljungl\u00a8of (2002), who also mentions that applicative combinators can be used to parse \nsome languages which are not context-free, because one can construct in.nite grammars by using parametrised \nparsers. It has also been known for a long time that an in.nite grammar can represent any language, decidable \nor not (Solomon 1977), and that the languages generated by many in.nite grammars can be decided (Mazurkiewicz \n1969). However, the result that monadic and applicative combinators have the same expressive strength \nfor .nite token sets seems to be largely un\u00adknown. For instance, Claessen (2004, page 742) claims that \nwith the weaker sequencing, it is only possible to describe context-free grammars in these systems . \nBonsangue et al. (2009, Example 2) represent a kind of regular expressions in a way which bears some \nsimilarity to the representa\u00adtion of recognisers in Section 3. Unlike the de.nition in this paper their \nde.nition is inductive, with an explicit representation of cy\u00adcles: \u00b5x.e, where e can contain x. However, \noccurrences of x in e have to be guarded by what amounts to the consumption of a token, just as in this \npaper. In Sections 3.3 and 4.2 Brzozowski derivative operators (Brzo\u00adzowski 1964) are implemented for \nrecognisers and parsers, and in Sections 3.4 and 4.3 these operators are used to characterise recog\u00adniser \nand parser equivalence coinductively. Rutten (1998) performs similar tasks for regular expressions. \n 2. Induction and coinduction The parser combinators de.ned in Sections 3 and 4 use a combina\u00adtion of \ninduction and coinduction which may at .rst sight seem be\u00adwildering, so let us begin by discussing induction \nand coinduction. This discussion is rather informal. For more theoretical accounts of induction and coinduction \nsee, for instance, the works of Hagino (1987) and Mendler (1988). Induction can be used to de.ne types \nwhere the elements have .nite depth . A simple example is the type of .nite lists. In Agda this data \ntype can be de.ned by giving the types of all the constructors: data List (A : Set) : Set where [] : \nList A :: : A . List A . List A This de.nition should be read inductively, i.e. all lists have .nite \nlength. Functions with underscore in their names are operators; marks the argument positions. For instance, \nthe constructor :: is an in.x operator. Set is a type of small types. Coinduction can be used to de.ne \ntypes where some elements have in.nite depth. Consider the type of potentially in.nite lists (colists), \nfor instance: data Colist (A : Set) : Set where [] : Colist A :: : A .8 (Colist A) . Colist A (Note \nthat constructors can be overloaded.) The type function 8 : Set . Set marks its argument as being coinductive. \nIt is simi\u00adlar to the suspension type constructors which are used to implement non-strictness in strict \nlanguages (Wadler et al. 1998). Just as the suspension type constructors the function 8 comes with delay \nand force functions, here called (sharp) and b (.at): : {A : Set}. A .8 A b : {A : Set}.8 A . A Sharp \nis a tightly binding pre.x operator; ordinary function appli\u00adcation binds tighter, though. (Flat is an \nordinary function.) Note that {A : Set}. T is a dependent function space; the argument A is in scope \nin T. Arguments in braces, {...}, are implicit, and do not need to be given explicitly as long as Agda \ncan infer them from the context. Agda is a total language. This means that all computations of inductive \ntype must be terminating, and that all computations of coinductive type must be productive. A computation \nis productive if the computation of the next constructor is always terminating, so even though an in.nite \ncolist cannot be computed in .nite time we know that the computation of any .nite pre.x has to be terminating. \nFor types which are partly inductive and partly coinductive the inductive parts must always be computable \nin .nite time, while the coinductive parts must always be productively computable. To ensure termination \nand productivity Agda employs two basic means for de.ning functions: inductive values can be destructed \nus\u00ading structural recursion, and coinductive values can be constructed using guarded corecursion (Coquand \n1994). As an example of the latter, consider the following de.nition of map for colists: map : .{AB}. \n(A . B) . Colist A . Colist B map f [] = [] map f (x :: xs) = fx :: map f (b xs) (Note that the code \n.{AB}. ... means that the function takes two implicit arguments A and B; it is not an application of \nA to B.) Agda accepts this de.nition because the corecursive call to map is guarded: it occurs under \nthe delay constructor , without any non\u00adconstructor function application between the left-hand side and \nthe corecursive call. It is easy to convince oneself that, if the input colist is productively computable, \nthen the (spine of the) output colist must also be. Let us now consider what happens if a de.nition uses \nboth induction and coinduction. We can de.ne a language of stream processors (Carlsson and Hallgren 1998; \nHancock et al. 2009), taking colists of As to colists of Bs, as follows: data SP (AB : Set) : Set where \nget : (A . SP A B) . SP A B put : B .8 (SP A B) . SP A B done : SP A B The recursive argument of get \nis inductive, while the recursive ar\u00adgument of put is coinductive. The type should be read as the nested \n.xpoint .X.\u00b5Y.(A . Y) + B \u00d7 X + 1, with an outer greatest .x\u00adpoint and an inner least .xpoint.3 This \nmeans that a stream proces\u00adsor can only read (get) a .nite number of elements from the input before having \nto produce (put) some output or terminate (done). As a simple example of a stream processor, consider \ncopy, which copies its input to its output: copy : .{A}. SP A A copy = get (. a . put a ( copy)) Note \nthat copy is guarded (lambdas do not affect guardedness). The semantics of stream processors can be de.ned \nas follows: [] : .{AB}. SP A B . Colist A . Colist B [ get f ] (a :: as) = [ fa ] (b as) [ put b sp ] \nas = b :: [ b sp ] as = [] [] ([] is a mix.x operator.) In the case of get one element from the input \ncolist is consumed (if possible), and potentially used to guide the rest of the computation, while in \nthe case of put one output element is produced. The de.nition of [] uses a lexicographic combination \nof guarded corecursion and structural recursion: In the second clause the corecursive call is guarded. \n In the .rst clause the corecursive call is not guarded, but it preserves guardedness : it takes place \nunder zero occurrences of rather than at least one (and there are no destructors in\u00advolved). Furthermore \nthe stream processor argument is struc\u00adturally smaller: fx is strictly smaller than get f for any x. \n This ensures the productivity of the resulting colist: the next output element can always be computed \nin .nite time, because the number of get constructors between any two put constructors must be .nite. \nAgda accepts de.nitions which use this kind of lexicographic combination of guarded corecursion and structural \nrecursion. For more information about Agda s criterion for accepting a program as total, and more examples \nof the use of mixed induction and coinduction in Agda, see Danielsson and Altenkirch (2010). It may be \ninteresting to observe what would happen if get were made coinductive. In this case we could de.ne more \nstream processors, for instance the following one: sink : .{AB}. SP A B sink = get (. . sink) On the \nother hand we could no longer de.ne [] as above (suitably modi.ed), because the output of [ sink ] as \nwould not be produc\u00adtive for in.nite colists as. In other words, if we make more stream processors de.nable \nsome functions become impossible to de.ne. 3 At the time of writing this interpretation is not correct \nin Agda (Altenkirch and Danielsson 2010), but the differences are irrelevant for this paper.  3. Recognisers \nThis section de.nes a small embedded language of parser combi\u00adnators. To simplify the explanation the \nparser combinators de.ned in this section can only handle recognition. Full parser combinators are described \nin Section 4. The aim is to de.ne a data type with (at least) the following basic combinators as constructors: \nfail, which always fails; empty, which accepts the empty string; sat, which accepts tokens satisfy\u00ading \na given predicate; | , symmetric choice; and \u00b7 , sequencing. Let us .rst consider whether the combinator \narguments should be read inductively or coinductively. An in.nite choice cannot be decided (in the absence \nof extra information), as this is not possible without inspecting every alternative, so choices will \nbe read inductively. The situation is a bit trickier for sequencing. ii Consider de.nitions like p = \np \u00b7 por p = p\u00b7 p. If pi accepts the empty string, then it seems hard to make any progress with these \nde.nitions. However, if pi is guaranteed not to accept the empty string, then we know that any string \naccepted by the recursive occurrence of p has to be shorter than the one accepted by p \u00b7 pi or pi\u00b7 p. \nTo make use of this observation I will indicate whether or not a recogniser is nullable (accepts the \nempty string) in its type, and the left (right) argument of \u00b7 will be coinductive iff the right (left) \nargument is not nullable. Based on the observations above the type P of parsers (recog\u00adnisers) can now \nbe de.ned for a given token type Tok: mutual data P : Bool . Set where fail : P false empty : P true \nsat : (Tok . Bool) . P false | : .{n1 n2}. Pn1 . Pn2 . P (n1 . n2) \u00b7 : .{n1 n2}. 8I n2 )Pn1 . 8I n1 \n)Pn2 . P (n1 . n2) 8I )P : Bool . Bool . Set 8I false )Pn =8 (Pn) 8I true )Pn = Pn Here P true represents \nthose recognisers which accept the empty string, and P false those which do not: fail and sat do not \naccept the empty string, while empty does; a choice p1 | p2 is nullable if either p1 or p2 is; and a \nsequence p1 \u00b7 p2 is nullable if both p1 and p2 are. The de.nition of the sequencing operator makes use \nof the mix.x operator 8I )P to express the conditional coinduction discussed above: the left argument \nhas type 8I n2 )Pn1, which means that it is coinductive iff n2 is false, i.e. iff the right argument \nis not nullable. The right argument s type is symmetric. The conditionally coinductive type 8I )P comes \nwith corre\u00adsponding conditional delay and force functions: ? : .{bn}. Pn . 8I b )Pn ? {b = false} x = \nx ? {b = true} x = x b?: .{bn} . 8I b )Pn . Pn b? {b = false} x = b x b? {b = true} x = x (Here {b = \n...} is the notation for pattern matching on an implicit argument.) We can also de.ne a function which \nreturns true iff the argument is already forced: forced?: .{bn} . 8I b )Pn . Bool forced? {b = b}= b \nIn addition to the constructors listed above the following con\u00adstructors are also included in P: nonempty \n: .{n}. Pn . P false cast : .{n1 n2}. n1 = n2 . Pn1 . Pn2 The nonempty combinator turns a recogniser \nwhich potentially accepts the empty string into one which de.nitely does not (see Section 3.1 for an \nexample and 3.2 for its semantics), and cast can be used to coerce a recogniser indexed by n1 into a \nrecogniser indexed by n2, assuming that n1 is equal to n2 (the type n1 = n2 is a type of proofs showing \nthat n1 and n2 are equal). Both nonempty and cast are de.nable in terms of the other combinators in the \ncase of cast the de.nition is trivial, and nonempty can be de.ned by recursion over the inductive structure \nof its input but due to Agda s reliance on guarded corecursion it is convenient to have them available \nas constructors. 3.1 Examples Using the de.nition above it is easy to de.ne recognisers which are both \nleft and right recursive, for instance the following one: left-right : P false left-right = left-right \n\u00b7 left-right Given the semantics in Section 3.2 it is easy to show that left-right does not accept any \nstring. This means that fail does not necessarily have to be primitive, it could be replaced by left-right. \nAs examples of ill-de.ned recognisers, consider bad and bad2: bad : P false bad2: P true bad = bad bad2 \n= bad2 \u00b7 bad2 These de.nitions are rejected by Agda, because they are neither structurally recursive \nnor guarded. They are not terminating, either: an attempt to evaluate the inductive parts of bad or bad2 \nwould lead to non-termination, because the de.nitions do not make use of the delay operator . As a more \nuseful example of how the combinators above can be used to de.ne derived recognisers, consider the following \nde.nition of the Kleene star: mutual * : P false . P true p * = empty | p + + : P false . P false p \n+= p \u00b7 (p *) (The combinator | binds weaker than the other combinators.) The recogniser p * accepts \nzero or more occurrences of whatever p accepts, and p + accepts one or more occurrences; this is easy \nto prove using the semantics in Section 3.2. Note that this de.nition is guarded, and hence productive.4 \nNote also that p must not accept the empty string, because if it did, then the right hand side of p + \nwould have to be written p \u00b7 p *, which would make the de.nition unguarded and non-terminating if p * \nwere unfolded, then no delay operator would ever be encountered. By using the nonempty combinator one \ncan de.ne a variant of * which accepts arbitrary argument recognisers: * : .{n}. Pn . P true p * = nonempty \np * For more examples, see Section 4.6. 4 The call to p + is not guarded in the de.nition of p *, but \nall that matters for guardedness is calls from one function to itself. If p + is inlined it is clear \nthat p * is guarded.  3.2 Semantics The semantics of the recognisers is de.ned as an inductive family. \nThe type s . p is inhabited iff the token string s is a member of the language de.ned by p: data . : \n.{n}. List Tok . Pn . Set where ... The semantics is determined by the constructors of . , which are \nintroduced below. The values of type s . p are proofs of language membership; the constructors can be \nseen as inference rules. To avoid clutter the declarations of bound variables are omitted in the constructors \ntype signatures. No string is a member of the language de.ned by fail, so there is no constructor for \nit in . . The empty string is recognised by empty: empty : [] . empty (Recall that constructors can be \noverloaded.) The singleton [ t ] is recognised by sat f if ft evaluates to true (Tb is inhabited iff \nb is true): sat : T (ft) . [ t ] . sat f If s is recognised by p1, then it is also recognised by p1 | \np2, and similarly for p2: |-left : s . p1 . s . p1 | p2 |-right : s . p2 . s . p1 | p2 If s1 is recognised \nby p1 (suitably forced), and s2 is recognised by p2 (suitably forced), then the concatenation of s1 and \ns2 is recognised by p1 \u00b7 p2: \u00b7 : s1 . b? p1 . s2 . b? p2 . s1 + s2 . p1 \u00b7 p2 If a nonempty string is \nrecognised by p, then it is also recog\u00adnised by nonempty p (and empty strings are never recognised by \nnonempty p): nonempty : t :: s . p . t :: s . nonempty p Finally cast preserves the semantics of its \nrecogniser argument: cast : s . p . s . cast eq p It is easy to show that the semantics and the nullability \nindex agree: if p : Pn, then [ ] . p iff n is equal to true (one direction can be proved by induction \non the structure of the semantics, and the other by induction on the inductive structure of the recogniser; \ndelayed sub-parsers do not need to be forced). Given this result it is easy to decide whether or not \n[ ] . p; it suf.ces to inspect the index: nullable?: .{n} (p : Pn) . Dec ([] . p) Note that the correctness \nof nullable? is stated in its type. An element of Dec P is either a proof of P or a proof showing that \nP is impossible: data Dec (P : Set) : Set where yes : P . Dec P no : \u00ac P . Dec P Here logical negation \nis represented as a function into the empty type: \u00ac P = P ...  3.3 Backend Let us now consider how the \nrelation . can be decided, or al\u00adternatively, how the language of recognisers can be interpreted. No \nattempt is made to make this recogniser backend ef.cient, the focus is on correctness. (Ef.ciency is \ndiscussed further in Section 4.2.) The backend will be implemented using so-called derivatives (Brzozowski \n1964). The derivative Dtp of p with respect to t is the remainder of p after p has matched the token \nt; it should satisfy the equivalence s . Dtp . t :: s . p. By applying the derivative operator D to t1 \nand p, then to t2 and Dt1 p, and so on for every element of the input string s, one can decide if s . \np is inhabited. The new recogniser constructed by D may not have the same nullability index as the original \none, so D has the following type signature: D : .{n} (t : Tok)(p : Pn) . P (D-nullable t p) The function \nD-nullable decides whether the derivative accepts the empty string or not. Its extensional behaviour \nis uniquely con\u00adstrained by the de.nition of D; its de.nition is included in Figure 1. The derivative \noperator is implemented as follows. The combi\u00adnators fail and empty never accept any token, so they both \nhave the derivative fail: Dt fail = fail Dt empty = fail The combinator sat f has a non-zero derivative \nwith respect to t iff ft is true: Dt (sat f ) with ft ... | true = empty ... | false = fail (Here the \nwith construct is used to pattern match on the result of ft.) The derivative of a choice is the choice \nof the derivatives of its arguments: Dt (p1 | p2) = Dtp1 | Dtp2 The derivatives of nonempty p and cast \neq p are equal to the derivative of p: Dt (nonempty p) = Dtp Dt (cast eq p) = Dtp The .nal and most interesting \ncase is sequencing: Dt (p1 \u00b7 p2) with forced? p1 | forced? p2 ... | true | false = Dt p1 \u00b7 ? (b p2) ... \n| false | false = Dt (b p1) \u00b7 ? (b p2) ? ... | true | true = Dt p1 \u00b7 p2 | Dtp2 ? ... | false | true = \nDt (b p1) \u00b7 p2 | Dtp2 Here we have four cases, depending on the indices of p1 and p2: In the .rst two \ncases the right argument is not forced, which implies (given the type of \u00b7 ) that the left argument is \nnot nullable. This means that the .rst token accepted by p1 \u00b7 p2 (if any) has to be accepted by p1, so \nthe remainder after accepting this token is the remainder of p1 followed by p2.  In the last two cases \np1 is nullable, which means that the .rst token could also be accepted by p2. This is re.ected in the \npresence of an extra choice Dtp2 on the right-hand side.  In all four cases the operator ? is used to \nconditionally delay p2, depending on the nullability index of the derivative of p1; the implicit argument \nb to ? is inferred automatically. The derivative operator D is total: it is implemented using a lexicographic \ncombination of guarded corecursion and structural recursion (as in Section 2). Note that in the .rst \ntwo sequencing cases p2 is delayed, but D is not applied recursively to b p2 because p1 is known not \nto accept the empty string.  D-nullable : .{n}. Tok . Pn . Bool D-nullable t fail = false D-nullable \nt empty = false D-nullable t (sat f ) = ft D-nullable t (p1 | p2) = D-nullable t p1 . D-nullable t p2 \nD-nullable t (nonempty p) = D-nullable t p D-nullable t (cast p) = D-nullable t p D-nullable t (p1 \u00b7 \np2) with forced? p1 | forced? p2 ... | true | false = D-nullable t p1 ... | false | false = false ... \n| true | true = D-nullable t p1 . D-nullable t p2 ... | false | true = D-nullable t p2 Figure 1. The \nindex function D-nullable. The index function D-nullable uses recursion on the inductive structure of \nthe recogniser. Note that D-nullable does not force any delayed recogniser (it does not use b). Readers \nfamiliar with de\u00adpendent types may .nd it interesting that this de.nition relies on the fact that . is \nde.ned by pattern matching on its right ar\u00adgument. If . were de.ned by pattern matching on its left argu\u00adment, \nthen the type checker would no longer reduce the open term D-nullable (b p1) t . false to false when \nchecking the de.nition of D. This problem could be .xed by using an equality proof in the de.nition of \nD, though. It is straightforward to show that the derivative operator D satis.es both directions of its \nspeci.cation: D-sound : .{nst}{p : Pn}. s . Dtp . t :: s . p D-complete : .{nst}{p : Pn}. t :: s . p \n. s . Dtp These statements can be proved by induction on the structure of the semantics. Once the derivative \noperator is de.ned and proved correct it is easy to decide if s . p is inhabited: .?: .{n} (s : List \nTok)(p : Pn) . Dec (s . p) [] .? p = nullable? p t :: s .? p with s .? Dtp ... | yes s.Dtp = yes (D-sound \ns.Dtp) ... | no s./Dtp = no (s./Dtp . D-complete) In the case of the empty string the nullability index \ntells us whether the string should be accepted or not, and otherwise .? is recur\u00adsively applied to the \nderivative and the tail of the string; the speci.\u00adcation of D ensures that this is correct. (Note that \ns.Dtp and s./Dtp are normal variables with descriptive names.) As an aside, note that the proof returned \nby .? when a string matches is actually a parse tree, so it would not be entirely incorrect to call these \nrecognisers parsers. However, in the case of ambiguous grammars at most one parse tree is returned. The \nimplementation of parse in Section 4.2 returns all possible results.  3.4 Laws Given the semantics above \nit is easy to prove that the combinators satisfy various laws. Let us .rst de.ne that two recognisers \nare equivalent when they accept the same strings: : .{n1 n2}. Pn1 . Pn2 . Set p1 p2 = p1 . p2 \u00d7 p2 . \np1 Here \u00d7 is conjunction, and . encodes language inclusion: . : .{n1 n2}. Pn1 . Pn2 . Set p1 . p2 = \n.{s}. s . p1 . s . p2 It is straightforward to show that is an equivalence relation, if the de.nition \nof equivalence relation is generalised to accept indexed sets. (Such generalisations are silently assumed \nin the remainder of this text.) It is also easy to show that is a congruence i.e. that it is preserved \nby all the primitive recogniser combinators and that . is a partial order with respect to . The following \nde.nition provides an alternative, coinductive characterisation of equality: data c {n1 n2} (p1: Pn1)(p2: \nPn2) : Set where :: : n1 = n2 . (. t .8 (Dtp1 c Dtp2)) . p1 c p2 Two recognisers are equal iff they agree \non whether the empty string is accepted, and for every token the respective derivatives are equal (coinductively). \nNote that the values of this data type are in.nite proofs5 witnessing the equivalence of the two parsers. \nNote also that this equality is a form of bisimilarity: the transitions are of the form (n,t) p - -. \nDtp, where p : Pn. It is easy to show that and c are equivalent. When proving properties of recognisers \none can choose the equality which is most convenient for the task at hand. For an example of a proof \nusing a coinductively de.ned equality, see Section 4.4. The type of the sequencing combinator is not \nquite right if we want to state properties such as associativity, so let us introduce the following variant \nof it: 0 : .{n1 n2}. Pn1 . Pn2 . P (n1 . n2) ? 0{n1 = n1} p1 p2 = p1 \u00b7 ? {b = n1} p2 (Agda does not \nmanage to infer the value of the implicit argument b, but we can still give it manually.) Using the combinator \n0 it is easy to prove that the recognisers form an idempotent semiring: p | p pp1 | p2 p2 | p1 fail \n| p pp1 | (p2 | p3) (p1 | p2) | p3 p 0 empty pp1 0 (p2 0 p3) (p1 0 p2) 0 p3 empty 0 p pp1 0 (p2 \n| p3) p1 0 p2 | p1 0 p3 fail 0 p fail (p1 | p2) 0 p3 p1 0 p3 | p2 0 p3 p 0 fail fail It is also easy \nto show that the order . coincides with the natural order of the join-semilattice formed by | : p1 . \np2 . p1 | p2 p2 By using the generalised Kleene star * from Section 3.1 one can also show that the recognisers \nform a *-continuous Kleene algebra (Kozen 1990): p1 0 (p2 *) 0 p3 is the least upper bound of the set \n{ p1 0 (p2 i) 0 p3 | i . N}, where p i is the i-fold repetition of p: I )-nullable : Bool . N . Bool \nI n zero )-nullable = I n suc i )-nullable = : .{n}. Pn . (i : N) . P (I n i )-nullable) p zero = \nempty p suc i = p 0 (p i) (Here zero and suc are the two constructors of N. Note that Agda can .gure \nout the right-hand sides of I )-nullable automatically, given the de.nition of ; see Section 4.6.) \n 3.5 Expressive strength Is the language of recognisers de.ned above useful? It may not be entirely \nobvious that the restrictions imposed to ensure totality 5 If Tok is non-empty.  do not rule out the \nde.nition of many useful recognisers. Fortu\u00adnately this is not the case, at least not if Tok, the set \nof tokens, is .nite, because then it can be proved that every function of type List Tok . Bool which \ncan be implemented in Agda can also be realised as a recogniser. For simplicity this will only be shown \nin the case when Tok is Bool. The basic idea is to turn a function f : List Bool . Bool into a grammar \nrepresenting an in.nite binary tree, with one node for every possible input string, and to make a given \nnode accepting iff f returns true for the correspond\u00ading string. Let us .rst de.ne a recogniser which \nonly accepts the empty string, and only if its argument is true: accept-if-true : . b . Pb accept-if-true \ntrue = empty accept-if-true false = fail Using this recogniser we can construct the in.nite binary tree \nusing guarded corecursion: grammar : (f : List Bool . Bool) . P (f []) grammar f = cast (lemma f )( ? \n(sat id) \u00b7 grammar (f . :: true ) | ? (sat not) \u00b7 grammar (f . :: false) | accept-if-true (f [])) Note \nthat sat id recognises true, and sat not recognises false. The following lemma is also used above: lemma \n: . f . (false . f [ true ] . false . f [ false ]) . f [] = f [] The .nal step is to show that, for any \nstring s, fs = true iff s . grammar f . The only if part can be proved by induction on the structure \nof s, and the if part by induction on the structure of s . grammar f . Note that the in.nite grammar \nabove has a very simple struc\u00adture: it is LL(1). I suspect that this grammar can be implemented using \na number of different parser combinator libraries. As an aside it may be interesting to know that the \nproof above does not require the use of lemma. The following left recursive grammar can also be used: \ngrammar : (f : List Bool . Bool) . P (f []) grammar f = grammar (. xs . f (xs + [ true ])) \u00b7 ? (sat id) \n| grammar (. xs . f (xs + [ false ])) \u00b7 ? (sat not) | accept-if-true (f []) This shows that nonempty \nand cast are not necessary to achieve full expressive strength, because neither grammar nor the backend \nrely on these operators. Finally let us consider the case of in.nite token sets. If the set of tokens \nis the natural numbers, then it is quite easy to see that it is impossible to implement a recogniser \nfor the language { nn | n . N}. By generalising the statement to it is impossible that p accepts in.nitely \nmany identical pairs, and only identical pairs and/or the empty string (where an identical pair is a \nstring of the form nn) one can prove this formally by induction on the structure of p (see the accompanying \ncode). Note that this restric\u00adtion does not apply to the monadic combinators introduced in the next section, \nwhich have maximal expressive strength also for in.\u00adnite token sets. 4. Parsers This section describes \nhow the recogniser language above can be extended to actual parser combinators, which return results. \nConsider the monadic parser combinator bind, > = : The parser p1 > = p2 successfully returns a value \ny for a given string s if p1 parses a pre.x of s, returning a value x, and p2 x parses the rest of s, \nreturning y. Note that p1 > = p2 accepts the empty string iff p1 accepts the empty string, returning \na value x, and p2 x also accepts the empty string. This shows that the values which a parser can return \nwithout consuming any input can be relevant for determining if another parser is nullable. This suggests \nthat, in analogy with the treatment of recognisers, a parser should be indexed by its initial set the \nset of values which can be returned when the input is empty. However, some\u00adtimes it is useful to distinguish \ntwo grammars if the number of parse trees corresponding to a certain string differ. For instance, the \nparser backend de.ned in Section 4.2 returns twice as many re\u00adsults for the parser p | p as for the parser \np. In order to take account of this distinction parsers are indexed by their return types and their initial \nbags (or multisets), represented as lists: mutual data Parser : (R : Set) . List R . Set1 where ... \n(Set1 is a type of large types; Agda is predicative.) The .rst four combinators have relatively simple \ntypes. The return combinator is the parser analogue of empty. When accept\u00ading the empty string it returns \nits argument: return : .{R} (x : R) . Parser R [ x ] (Note that [ ] is the return function of the list \nmonad.) The fail parser, which mirrors the fail recogniser, always fails: fail : .{R}. Parser R [] (Note \nthat [ ] is the zero of the list monad.) The token parser accepts any single token, and returns this \ntoken: token : Parser Tok [] This combinator is not as general as sat, but a derived combinator sat is \neasy to de.ne using token and bind, see Section 4.6. The analogue of the choice recogniser is | : | : \n.{R xs1 xs2}. Parser R xs1 . Parser R xs2 . Parser R (xs1 + xs2) The initial bag of a choice is the union \nof the initial bags of its two arguments. The bind combinator s type is more complicated than the types \nabove. Consider p1 > = p2 again. Here p2 is a function, and we have a function f : R1 . List R2 which \ncomputes the initial bag of p2 x, depending on the value of x. When should we allow p1 to be coinductive? \nOne option is to only allow this when fx is empty for every x, but I do not want to require the user \nof the library to prove such a property just to de.ne a parser. Instead I have chosen to represent the \nfunction f with an optional function f : Maybe (R1 . List R2),6 where nothing represents . . [ ], and \nto make p1 coinductive iff f is nothing. The same approach is used for xs, the initial bag of p1: > = \n: .{R1 R2}{xs : Maybe (List R1)} {f : Maybe (R1 . List R2)}. 8I f )Parser R1 (.atten xs) . ((x : R1) \n. 8I xs )Parser R2 (apply f x)) . Parser R2 (bind xs f ) The helper functions .atten, apply and bind, \nwhich interpret nothing as the empty list or the constant function returning the 6 The type Maybe A has \nthe two constructors nothing : Maybe A and just : A . Maybe A.  .atten : {A : Set}. Maybe (List A) . \nList A .atten nothing = [] .atten (just xs) = xs apply : {AB : Set}. Maybe (A . List B) . A . List B \napply nothing x = [] apply (just f ) x = fx bind : {AB : Set}. Maybe (List A) . Maybe (A . List B) . \nList B bind xs nothing = [] bind xs (just f ) = bindL (.atten xs) f Figure 2. Helper functions used in \nthe type signature of > = . Note that there is a reason for not de.ning bind using the equation bind \nxs f = bindL (.atten xs)(apply f ); see Section 4.6. empty list, are de.ned in Figure 2; bind is de.ned \nin terms of bindL, the standard list monad s bind operation. The function 8I )Parser is de.ned as follows, \nmutually with Parser: 8I )Parser : {A : Set}. Maybe A . (R : Set) . List R . Set1 8I nothing )Parser \nR xs =8 (Parser R xs) 8I just )Parser R xs = Parser R xs (8 works also for Set1.) It is straightforward \nto de.ne a variant of b? for this type. It is not necessary to de.ne ?, though: instead of conditionally \ndelaying one can just avoid using nothing. Just as in Section 3 two additional constructors are included \nin the de.nition of Parser: nonempty : .{R xs}. Parser R xs . Parser R [] cast : .{R xs1 xs2}. xs1 bag \nxs2 . Parser R xs1 . Parser R xs2 Here bag stands for bag equality between lists, equality up to permutation \nof elements; the cast combinator ensures that one can replace one representation of a parser s initial \nbag with another. Bag equality is de.ned in two steps. First list membership is encoded inductively as \nfollows: data .{A : Set} : A . List A . Set where here : .{x xs}. x . x :: xs there : .{x y xs}. y . \nxs . y . x :: xs Two lists xs and ys are then deemed bag equal if, for every value x, x is a member \nof xs as often as it is a member of ys: bag : .{R}. List R . List R . Set xs bag ys = .{x}. x . xs . \nx . ys Here A . B means that there is an invertible function from A to B, so A and B must have the same \ncardinality. 4.1 Semantics The semantics of the parser combinators is de.ned as a relation .\u00b7 , such \nthat x . p \u00b7 s is inhabited iff x is one of the results of parsing the string s using the parser p. This \nrelation is de.ned in Figure 3. Note that values of type x . p \u00b7 s can be seen as parse trees. The parsers \ncome with two kinds of equivalence. The weaker one, language equivalence ( ), is a direct analogue of \nthe equiv\u00adalence used for recognisers in Section 3.4: : .{R xs1 xs2}. Parser R xs1 . Parser R xs2 . Set1 \np1 p2 = .{xs}. x . p1 \u00b7 s . x . p2 \u00b7 s data .\u00b7 : .{R xs}. R . Parser R xs . List Tok . Set1 where return \n: x . return x \u00b7 [] token : t . token \u00b7 [ t ] |-left : x . p1 \u00b7 s . x . p1 | p2 \u00b7 s |-right : x . p2 \n\u00b7 s . x . p1 | p2 \u00b7 s > = : x . b? p1 \u00b7 s1 . y . b? (p2 x) \u00b7 s2 . y . p1 > = p2 \u00b7 s1 + s2 nonempty : \nx . p \u00b7 t :: s . x . nonempty p \u00b7 t :: s cast : x . p \u00b7 s . x . cast eq p \u00b7 s Figure 3. The semantics \nof the parser combinators. To avoid clut\u00adter the declarations of bound variables are omitted in the construc\u00adtors \ntype signatures. Here A . B means that A and B are equivalent: there is a function of type A . B and \nanother function of type B . A. We immedi\u00adately get that language equivalence is an equivalence relation. \nAs mentioned above language equivalence is sometimes too weak. We may want to distinguish between grammars \nwhich de.ne the same language, if they do not agree on the number of ways in which a given value can \nbe produced from a given string. To make the example given above more concrete, the parser backend de.ned \nin Section 4.2 returns one result when the empty string is parsed using return true (parse tree: return), \nand two results when return true | return true is used (parse trees: |-left return and |-right return). \nBased on this observation two parsers are de.ned to be parser equivalent ( ~) if, for all values and \nstrings, the = respective sets of parse trees have the same cardinality: ~ = : .{R xs1 xs2}. Parser R \nxs1 . Parser R xs2 . Set1 ~ p1 = p2 = .{xs}. x . p1 \u00b7 s . x . p2 \u00b7 s From its de.nition we immediately \nget that parser equivalence is an equivalence relation. Parser equivalence is strictly stronger than \nlanguage equivalence: the former distinguishes between return true and return true | return true, while \nthe latter is idempotent. Just as in Section 3.2 the initial bag index is correct: index-correct : .{R \nxs x}{p : Parser R xs}. x . p \u00b7 [] . x . xs Note the use of . : the number of parse trees for x matches \nthe number of occurrences of x in the list xs. One direction of the inverse can be de.ned by recursion \non the structure of the semantics, and the other by recursion on the structure of . . From index-correct \nwe easily get that parsers which are parser equivalent have equal initial bags: same-bag : .{R xs1 xs2}{p1: \nParser R xs1}{p2: Parser R xs2}. ~ p1 = p2 . xs1 bag xs2 Similarly, language equivalent parsers have \nequal initial sets.  4.2 Backend Following Section 3.3 it is easy to implement a derivative operator \nfor parsers: D : .{R xs} (t : Tok)(p : Parser R xs) . Parser R (D-bag t p) The implementation of the \nfunction D-bag which computes the derivative s initial bag can be seen in Figure 4. Both D and D-bag \nuse analogues of the forced? function from Section 3:  D-bag : .{R xs}. Tok . Parser R xs . List R D-bag \nt (return x) = [] D-bag t fail = [] D-bag t token = [ t ] D-bag t (p1 | p2) = D-bag t p1 + D-bag t p2 \nD-bag t (nonempty p) = D-bag t p D-bag t (cast eq p) = D-bag t p D-bag t (p1 > = p2) with forced? p1 \n| forced?i p2 ... | just f | nothing = bindL (D-bag t p1) f ... | just f | just xs = bindL (D-bag t p1) \nf + bindL xs (. x . D-bag t (p2 x)) ... | nothing | nothing = [] ... | nothing | just xs = bindL xs \n(. x . D-bag t (p2 x)) Figure 4. The index function D-bag. Note that its implementation falls out almost \nautomatically from the de.nition of D. forced?: .{A R xs m} . 8I m )Parser R xs . Maybe A forced? {m \n= m}= m forced?i : .{AR1 R2: Set}{m}{f : R1 . List R2}. ((x : R1) . 8I m )Parser R2 (fx)) . Maybe A forced?i{m \n= m}= m The non-recursive cases of D, along with choice, nonempty and cast, are easy: Dt (return x) = \nfail Dt fail = fail Dt token = return t Dt (p1 | p2) = Dtp1 | Dtp2 Dt (nonempty p) = Dtp Dt (cast eq \np) = Dtp The last case, > = , is more interesting. It makes use of the combinator return*, which can \nreturn any element of its argument list: return* : .{R} (xs : List R) . Parser R xs return* [] = fail \nreturn*(x :: xs) = return x | return* xs The code is very similar to the code for sequencing in Section \n3.3: Dt (p1 > = p2) with forced? p1 | forced?i p2 ... | just f | nothing = Dt p1 > = (. x . b(p2 x)) \n... | nothing | nothing = Dt (b p1)> = (. x . b(p2 x)) ... | just f | just xs = Dt p1 > = (. x . p2 x) \n| return* xs > = (. x . Dt (p2 x)) ... | nothing | just xs = Dt (b p1)> = (. x . p2 x) | return* xs > \n= (. x . Dt (p2 x)) ? There are two main differences. One is the absence of . The other difference can \nbe seen in the last two cases, where p1 is potentially nullable (it is if xs is nonempty). The corresponding \nright-hand sides are implemented as choices, as before. However, the right choices are a bit more involved \nthan in Section 3.3. They correspond to the cases where p1 succeeds without consuming any input, returning \none of the elements of its initial bag xs. In this case the elements of the initial bag index of p1 are \nreturned using return*, and then combined with p2 using bind. The implementation of D-bag is structurally \nrecursive, while the implementation of D uses a lexicographic combination of guarded corecursion and \nstructural recursion, just as in Section 3.3. It is straightforward to prove the following correctness \nproperty: D-correct : .{R xs x s t}{p : Parser R xs}. x . Dtp \u00b7 s . x . p \u00b7 t :: s Both directions of \nthe inverse can be de.ned by recursion on the structure of the semantics, with the help of index-correct. \nGiven the derivative operator it is easy to de.ne the parser backend: parse : .{R xs}. Parser R xs . \nList Tok . List R parse {xs = xs} p [] = xs parse p (t :: s) = parse (Dtp) s The correctness of this \nimplementation follows easily from index\u00adcorrect and D-correct: parse-correct : .{R xs x s}{p : Parser \nR xs}. x . p \u00b7 s . x . parse p s Both directions of the inverse can be de.ned by recursion on the structure \nof the input string. Note that this proof establishes that a parser can only return a .nite number of \nresults for a given input string (because the list returned by parse is .nite) in.nitely ambiguous grammars \ncannot be represented in this framework. As mentioned in Section 4.1 we have parse (return true | return \ntrue) [] = true :: true :: [] . It might seem reasonable for parse to remove duplicates from the list \nof results. However, the result type is not guaranteed to come with decidable equality (consider functions, \nfor instance), so such .ltering is left to the user of parse. The code above is not optimised, and mainly \nserves to illustrate that it is possible to implement a Parser backend which guarantees termination. \nIt is not too hard to see that, in the worst case, parse is at least exponential in the size of the input \nstring. Consider the following parser: p : Parser Bool [] p = fail > = .(b : Bool) . fail The derivative \nDtp is p | p, for any token t. After taking n deriva\u00adtives we get a parser with 2n - 1 choices, and all \nthese choices have to be traversed to compute the parser s initial bag. The parser p may seem contrived, \nbut similar parsers can easily arise as the result of taking the derivative of more useful parsers. It \nmay be possible to implement more ef.cient backends. For in\u00adstance, one can make use of algebraic laws \nlike fail > = p ~fail (see Section 4.4) to simplify parsers, and perhaps avoid the kind of behaviour \ndescribed above, at least for certain classes of parsers. Exploring such optimisations is left for future \nwork, though. =  4.3 Coinductive equivalences In Section 3.4 a coinductive characterisation of recogniser \nequiva\u00adlence is given. This is possible also for parser equivalence: ~ data = c {R xs1 xs2} (p1: Parser \nR xs1) (p2: Parser R xs2) : Set where :: : xs1 bag xs2 . ~ (. t .8 (Dtp1 = c Dtp2)) . ~ p1 = c p2 Two \nparsers are equivalent if their initial bags are equal, and, for every token t, the respective derivatives \nwith respect to t are equivalent (coinductively). Using index-correct and D-correct it is ~ easy to show \nthat the two de.nitions of parser equivalence, = ~ and = c , are equivalent. By replacing the use of \n. in the de.nition of bag equality with . we get set equality instead. If, in turn, the use of bag~ equality \nis replaced by set equality in = c , then we get a coinduc\u00adtive characterisation of language equivalence \n( ).  By using the coinductive characterisations of equivalence I have proved that all primitive parser \ncombinators preserve both language and parser equivalence, i.e. the equivalences are congruences.  4.4 \nLaws Let us now discuss the equational theory of the parser combinators. Many of the laws from Section \n3.4 can be generalised to the setting of parser combinators. To start with we have a commutative monoid \nformed by fail and | : ~ p1 | p2 = p2 | p1 ~ fail | p = p ~ (p1 | p2) | p3 = p1 | (p2 | p3) If language \nequivalence is used this monoid is also idempotent: p | p p We also have a monad, with fail as a left \nand right zero of bind, and bind distributing from the left and right over choice: ~ return x > = p = \npx ~ p > = return = p p1 > (. x . p2 x = = > p2) = = > p3) ~(p1 = > p3 ~ fail > = p = fail ~ p > = \n(. . fail) = fail ~ p1 > = (. x . p2 x | p3 x) = p1 > = p2 | p1 > = p3 ~ (p1 | p2)> = p3 = p1 > = p3 \n| p2 > = p3 Unlike in Section 3.4 there is no need to de.ne a special variant of > = to state the laws \nabove: if the types of the argument parsers are given (as for > =-left-identity below), then Agda automatically \ninfers that bind s implicit arguments xs and f should have the form just something. Analogues of most \nof the laws from Section 3.4 are listed above. However, assuming that the token type is inhabited, it \nis not possible to .nd a function f : .{R xs}. Parser R xs . List (List R) and a Kleene-star-like combinator \n* : .{R xs} (p : Parser R xs) . Parser (List R)(fp) such that return [] | (p > = . x . p * > = . xs . \nreturn (x :: xs)) p * holds for all p. (Here is de.ned as in Section 3.4.) The reason is that p may be \nnullable, in which case the inequality above implies that xs . p * \u00b7 [ ] must be satis.ed for in.nitely \nmany lists xs, whereas parse-correct shows that a parser can only return a .nite number of results. (A \ncombinator * satisfying the inequality above can easily be implemented if it is restricted to non-nullable \nargument parsers.) Before leaving the subject of equational laws, let me take a moment to explain how \none of the laws above the left identity law for bind can be proved. Assume that we have already proved \nsome of the other laws, along with the following property of bindL: bindL-left-identity : {AB : Set} \n(x : A)(f : A . List B) . bindL [ x ] f bag fx I have found the coinductive characterisations of the \nequivalences to be convenient to work with, so I have proved the law roughly as follows: > =-left-identity \n: {R1 R2: Set}{f : R1 . List R2} (x : R1)(p : (x : R1) . Parser R2 (fx)) . = c return x > = p ~px > \n=-left-identity {f = f} xp = bindL-left-identity x f :: . t . ( ~ Dt (return x > = p) = c fail > = p \n| return* [ x ] > = (. x . Dt (px)) ~ = c fail | return x == c > (. x . Dt (px)) ~return x > (. x . Dt \n(px)) ~ = = c Dt (px) D) (To avoid clutter the proof above uses the equational reason\u00ad ~~ ing notation \n... = c ... = c ... D, and the sub-proofs for the individual steps have been omitted.) The proof has \ntwo parts. First bindL-left-identity is used to show that the initial bags of return x > = p and px are \nequal, and then it is shown, for every token t, that Dt (return x > = p) and Dt (px) are equivalent. \nThe .rst step of the latter part uses a law relating D and > = , the sec\u00ad = c ond step uses the left \nzero law (fail > = p ~fail) and the right identity law for choice (p | fail ~p), the third step uses \nthe left = c identity law for choice (fail | p ~p), and the last step uses the = c coinductive hypothesis. \nThe proof as written above would not be accepted by Agda, because the coinductive hypothesis is not guarded \nby constructors (due to the uses of transitivity implicit in the equational reasoning notation). However, \nthis issue can be addressed (Danielsson 2010). For details of how all the properties above have been \nproved, see the code accompanying the paper. 4.5 Expressive strength This subsection is concerned with \nthe parser combinators ex\u00adpressiveness. By using bind one can strengthen the result from Section 3.5 \nto arbitrary sets of tokens: every function of type List Tok . List R can be realised as a parser (if \nbag equality is used for the lists of results). The grammar is similar to the con\u00adstruction in Section \n3.5: grammar : .{R} (f : List Tok . List R) . Parser R (f []) grammar f = token > = (. t . grammar (f \n. :: t)) | return*(f []) The function grammar satis.es the following correctness property: grammar-correct \n: .{Rxs} (f : List Tok . List R) . x . grammar f \u00b7 s . x . fs One direction of the inverse can be de.ned \nby induction on the structure of the semantics, and the other by induction on the struc\u00adture of the input \nstring. If we combine this result with parse-correct we get the expressiveness result: maximally-expressive \n: .{R} (f : List Tok . List R) {s}. parse (grammar f ) s bag fs Assume for a moment that the primitive \nparser combinators in\u00adcluded sat and applicative functor application (McBride and Pater\u00adson 2008) instead \nof token and bind. Then, for .nite sets of tokens, we could have de.ned grammar roughly as in Section \n3.5. This means that, for .nite sets of tokens, the inclusion of the monadic bind combinator does not \nprovide any expressive advantage; the applicative functor interface is already suf.ciently expressive. \nThis comparison does not take ef.ciency into account, though.  4.6 Examples Finally let us consider \nsome examples, along with some practical remarks.  Let us start with the left recursive grammar in the \nintroduction. Note that it does not require any user annotations, except for the three uses of . Agda \ninfers all the type signatures and all the im\u00adplicit arguments, including several functions, automatically. \nAgda s inference mechanism is based on uni.cation (a variant of pattern uni.cation (Pfenning 1991)), \nand an omitted piece of code is only .lled in if it can be uniquely determined from the constraints pro\u00advided \nby the rest of the code. In general there is no guarantee that implicit arguments can be omitted, and \nit is not uncommon that the exact form of a de.nition affects how much can be inferred. Consider the \nde.nition of bind in Figure 2. It is set up so that bind xs nothing evaluates to the empty list, even \nif xs is a neutral term. If bind had instead been de.ned by the equation bind xs f = bindL (.atten xs)(apply \nf ), then the example in the introduction would have required man\u00adual annotations: the example gives \nrise to the constraint xs = bind (just xs) nothing, which with the alternative de.nition of bind reduces \nto xs = bindL xs (. . []), and Agda cannot solve this uni.cation problem. As an example of a de.nition \nfor which the initial bag is not inferred automatically, consider the following de.nition of sat: sat \n: .{R}. (Tok . Maybe R) . Parser R sat {R = R} p = token > = . t . ok (pt) where ok-bag : Maybe R . List \nR ok-bag nothing = ok-bag (just x) = ok : (x : Maybe R) . Parser R (ok-bag x) ok nothing = fail ok (just \nx) = return x The parser sat p matches a single token t iff pt evaluates to just x, for some x; the value \nreturned is x. The initial bag function ok-bag is not inferred by Agda. However, the right-hand sides \nof ok-bag, and the initial bag of sat, are inferred. The example in the introduction uses the derived \ncombinators tok and number. The parser tok, which accepts a given token, is easy to de.ne using sat (assuming \nthat equality of tokens can be decided using the function == ): tok : Tok . Parser Tok tok t = sat (. \nti. if t == ti then just ti else nothing) Given a parser for digits (which is easy to de.ne using sat) \nthe parser number, which accepts an arbitrary non-negative number, can also be de.ned: number : Parser \nN number = digit + > = return . foldl (. nd . 10 * n + d) 0 Here foldl is a left fold for lists, and \np + parses one or more ps (as in Section 3.1). The examples above are quite small; larger examples can \nalso be constructed. For instance, Danielsson and Norell (2009) construct mix.x operator parsers using \na parser combinator library which is based on some of the ideas described here. 5. Conclusions A parser \ncombinator library which handles left recursion and guar\u00adantees termination of parsing has been presented, \nand it has been established that the library is suf.ciently expressive: every .nitely ambiguous parser \non .nite input strings which can be implemented using the host language can also be realised using the \ncombinators. I believe that the precise treatment of induction and coinduction which underlies the de.nition \nof the parser combinators gives a good framework for understanding lazy programs. To take one example, \nClaessen (2004) de.nes the following parser data type using Haskell: data Pi sa = SymbolBind (s . Pi \nsa) | Fail | ReturnPlus a (Pi sa) He notes that it is isomorphic to the stream processor type used in \nFudgets (Carlsson and Hallgren 1998), and that this isomorphism inspired the view of the parser combinators \nbeing parsing process combinators . However, in a total setting I would de.ne these two types differently. \nThe stream processors were de.ned in Section 2, with an inductive get constructor and a coinductive put \nconstructor. I .nd it natural to de.ne Pi in the opposite way: data Pi (SA : Set) : Set where symbolBind \n: (S .8 (Pi SA)) . Pi SA fail : Pi SA returnPlus : A . Pi SA . Pi SA The reason for the difference is \nthat the types are used differently. Stream processors are interpreted using [], and parsers using parsei, \nwhich works with .nite lists: i parse: .{SA}. Pi SA . List S . List (A \u00d7 List S) parsei (symbolBind f \n)(c :: s) = parsei (b (fc)) s i parsei (returnPlus xp) s = (x, s) :: parsep s parsei = [] The de.nition \nof [] in Section 2 would not be total if get were coinductive, because then we could not guarantee that \nthe result\u00ading colist would be productive. On the other hand, if returnPlus were coinductive and symbolBind \ninductive, then parsers like the one used in the proof of maximal expressiveness in Section 4.5 could \nnot be implemented (consider the case when the argument to grammar is . . [ ]). The use of lazy data \ntypes and general recursion in Haskell is very .exible for instance, Carlsson and Hallgren (1998) use \ntheir stream processors in ways which would not be accepted if the type SP were used in Agda but I .nd \nit easier to understand how and why programs work when induction and coinduction are separated as in \nthis paper. The use of mixed induction and coinduction has been known for a long time (Park 1980), but \ndoes not seem to be well-known among functional programmers. It is my hope that this paper provides a \ncompelling example of the use of this technique. Acknowledgements I would like to thank Ulf Norell for \nprevious joint work on total parsing, and for improving Agda s uni.cation mechanism. Another person who \ndeserves thanks is Thorsten Altenkirch, with whom I have had many discussions about mixed induction and \ncoinduction. Thorsten also suggested that I should allow left recursive parsers, which I might otherwise \nnot have tried, and gave feedback which improved the presentation; such feedback was also given by several \nanonymous reviewers. Finally I would like to acknowledge .nancial support from EPSRC and the Royal Swedish \nAcademy of Sciences funds (EPSRC grant code: EP/E04350X/1). References The Agda Team. The Agda Wiki. \nAvailable at http://wiki.portal. chalmers.se/agda/, 2010. Thorsten Altenkirch and Nils Anders Danielsson. \nTermination checking in the presence of nested inductive and coinductive types. Note support\u00ading presentation \ngiven at the Workshop on Partiality and Recursion in Interactive Theorem Provers, Edinburgh, UK, 2010. \n Arthur Baars, S. Doaitse Swierstra, and Marcos Viera. Typed transforma\u00adtions of typed grammars: The \nleft corner transform. In Preliminary Pro\u00adceedings of the Ninth Workshop on Language Descriptions Tools \nand Applications, LDTA 2009, pages 18 33, 2009. Marcello Bonsangue, Jan Rutten, and Alexandra Silva. \nA Kleene theo\u00adrem for polynomial coalgebras. In Foundations of Software Science and Computational Structures, \n12th International Conference, FOS-SACS 2009, volume 5504 of LNCS, pages 122 136, 2009. Kasper Brink, \nStefan Holdermans, and Andres L\u00a8oh. Dependently typed grammars. In Mathematics of Program Construction, \nTenth Interna\u00adtional Conference, MPC 2010, volume 6120 of LNCS, pages 58 79, 2010. Janusz A. Brzozowski. \nDerivatives of regular expressions. Journal of the ACM, 11(4):481 494, 1964. William H. Burge. Recursive \nProgramming Techniques. Addison-Wesley, 1975. Magnus Carlsson and Thomas Hallgren. Fudgets Purely Functional \nProcesses with applications to Graphical User Interfaces. PhD thesis, Chalmers University of Technology \nand G\u00a8oteborg University, 1998. Koen Claessen. Embedded Languages for Describing and Verifying Hard\u00adware. \nPhD thesis, Chalmers University of Technology, 2001. Koen Claessen. Parallel parsing processes. Journal \nof Functional Program\u00adming, 14:741 757, 2004. Koen Claessen and David Sands. Observable sharing for functional \ncircuit description. In Advances in Computing Science ASIAN 99, volume 1742 of LNCS, pages 62 73, 1999. \nThierry Coquand. In.nite objects in type theory. In Types for Proofs and Programs, International Workshop \nTYPES 93, volume 806 of LNCS, pages 62 78, 1994. Nils Anders Danielsson. Beating the productivity checker \nusing embedded languages. In Workshop on Partiality and Recursion in Interactive Theorem Provers, Edinburgh, \nUK, 2010. Nils Anders Danielsson and Thorsten Altenkirch. Subtyping, declarative\u00adly: An exercise in mixed \ninduction and coinduction. In Mathematics of Program Construction, Tenth International Conference, MPC \n2010, volume 6120 of LNCS, pages 100 118, 2010. Nils Anders Danielsson and Ulf Norell. Structurally recursive \ndescent parsing. Unpublished note, 2008. Nils Anders Danielsson and Ulf Norell. Parsing mix.x operators. \nTo appear in the proceedings of the 20th International Symposium on the Implementation and Application \nof Functional Languages (IFL 2008), 2009. Jon Fairbairn. Making form follow function: An exercise in \nfunctional programming style. Software: Practice and Experience, 17(6):379 386, 1987. Jeroen Fokker. \nFunctional parsers. In Advanced Functional Programming, volume 925 of LNCS, pages 1 23, 1995. Richard \nA. Frost, Rahmatullah Ha.z, and Paul Callaghan. Parser combina\u00adtors for ambiguous left-recursive grammars. \nIn PADL 2008: Practical Aspects of Declarative Languages, volume 4902 of LNCS, pages 167 181, 2008. Tatsuya \nHagino. A Categorical Programming Language. PhD thesis, University of Edinburgh, 1987. Peter Hancock, \nDirk Pattinson, and Neil Ghani. Representations of stream processors using nested .xed points. Logical \nMethods in Computer Science, 5(3:9), 2009. R. John M. Hughes and S. Doaitse Swierstra. Polish parsers, \nstep by step. In ICFP 03: Proceedings of the eighth ACM SIGPLAN international conference on Functional \nprogramming, pages 239 248, 2003. Graham Hutton. Higher-order functions for parsing. Journal of Functional \nProgramming, 2:323 343, 1992. Mark Johnson. Memoization in top-down parsing. Computational Linguis\u00adtics, \n21(3):405 417, 1995. Oleg Kiselyov. Parsec-like parser combinator that handles left recursion? Message \nto the Haskell-Cafe mailing list, December 2009. Pieter Koopman and Rinus Plasmeijer. Ef.cient combinator \nparsers. In IFL 98: Implementation of Functional Languages, volume 1595 of LNCS, pages 120 136, 1999. \nAdam Koprowski and Henri Binsztok. TRX: A formally veri.ed parser interpreter. In Programming Languages \nand Systems, 19th European Symposium on Programming, ESOP 2010, volume 6012 of LNCS, pages 345 365, 2010. \nDexter Kozen. On Kleene algebras and closed semirings. In Mathematical Foundations of Computer Science \n1990, volume 452 of LNCS, pages 26 47, 1990. Daan Leijen and Erik Meijer. Parsec: Direct style monadic \nparser combina\u00adtors for the real world. Technical Report UU-CS-2001-35, Department of Information and \nComputing Sciences, Utrecht University, 2001. Paul Lickman. Parsing with .xed points. Master s thesis, \nUniversity of Cambridge, 1995. Peter Ljunglof.\u00a8Pure functional parsing; an advanced tutorial. Licentiate \nthesis, Department of Computing Science, Chalmers University of Tech\u00adnology and G\u00a8oteborg University, \n2002. Antoni W. Mazurkiewicz. A note on enumerable grammars. Information and Control, 14(6):555 558, \n1969. Conor McBride and James McKinna. Seeing and doing. Presentation (given by McBride) at the Workshop \non Termination and Type Theory, Hind\u00b0as, Sweden, 2002. Conor McBride and Ross Paterson. Applicative programming \nwith effects. Journal of Functional Programming, 18:1 13, 2008. Erik Meijer. Calculating Compilers. PhD \nthesis, Nijmegen University, 1992. Paul Francis Mendler. Inductive De.nition in Type Theory. PhD thesis, \nCornell University, 1988. Muad`Dib. Strongly speci.ed parser combinators. Post to the Muad`Dib blog, \n2009. Ulf Norell. Towards a practical programming language based on depen\u00addent type theory. PhD thesis, \nChalmers University of Technology and G\u00a8oteborg University, 2007. David Park. On the semantics of fair \nparallelism. In Abstract Software Speci.cations, volume 86 of LNCS, pages 504 526, 1980. Frank Pfenning. \nUni.cation and anti-uni.cation in the calculus of construc\u00adtions. In Proceedings of the Sixth Annual \nIEEE Symposium on Logic in Computer Science, pages 74 85, 1991. J.J.M.M. Rutten. Automata and coinduction \n(an exercise in coalgebra). In CONCUR 98, Concurrency Theory, 9th International Conference, volume 1466 \nof LNCS, pages 547 554, 1998. Niklas R\u00a8ojemo. Garbage collection, and memory ef.ciency, in lazy func\u00adtional \nlanguages. PhD thesis, Chalmers University of Technology and University of G\u00a8 oteborg, 1995. Marvin Solomon. \nTheoretical Issues in the Implementation of Programming Languages. PhD thesis, Cornell University, 1977. \nS. Doaitse Swierstra and Luc Duponcheel. Deterministic, error-correcting combinator parsers. In Advanced \nFunctional Programming, volume 1129 of LNCS, pages 184 207, 1996. Wouter Swierstra. A Hoare logic for \nthe state monad. In Theorem Proving in Higher Order Logics, 22nd International Conference, TPHOLs 2009, \nvolume 5674 of LNCS, pages 440 451, 2009. Philip Wadler. How to replace failure by a list of successes; \na method for ex\u00adception handling, backtracking, and pattern matching in lazy functional languages. In \nFunctional Programming Languages and Computer Ar\u00adchitecture, volume 201 of LNCS, pages 113 128, 1985. \nPhilip Wadler, Walid Taha, and David MacQueen. How to add laziness to a strict language, without even \nbeing odd. In Proceedings of the 1998 ACM SIGPLAN Workshop on ML, 1998. Malcolm Wallace. Partial parsing: \nCombining choice with commitment. In IFL 2007: Implementation and Application of Functional Languages, \nvolume 5083 of LNCS, pages 93 110, 2008.   \n\t\t\t", "proc_id": "1863543", "abstract": "<p>A monadic parser combinator library which guarantees termination of parsing, while still allowing many forms of left recursion, is described. The library's interface is similar to those of many other parser combinator libraries, with two important differences: one is that the interface clearly specifies which parts of the constructed parsers may be infinite, and which parts have to be finite, using dependent types and a combination of induction and coinduction; and the other is that the parser type is unusually informative.</p> <p>The library comes with a formal semantics, using which it is proved that the parser combinators are as expressive as possible. The implementation is supported by a machine-checked correctness proof.</p>", "authors": [{"name": "Nils Anders Danielsson", "author_profile_id": "81309480746", "affiliation": "University of Nottingham, Nottingham, United Kingdom", "person_id": "P2338221", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1863543.1863585", "year": "2010", "article_id": "1863585", "conference": "ICFP", "title": "Total parser combinators", "url": "http://dl.acm.org/citation.cfm?id=1863585"}