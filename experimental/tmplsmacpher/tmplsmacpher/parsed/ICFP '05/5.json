{"article_publication_date": "09-12-2005", "fulltext": "\n Dynamic Optimization for Functional Reactive Programming using Generalized Algebraic Data Types Henrik \nNilsson School of Computer Science and Information Technology, University of Nottingham Henrik.Nilsson@cs.nott.ac.uk \nAbstract A limited form of dependent types, called Generalized Algebraic Data Types (GADTs), has recently \nbeen added to the list of Haskell extensions supported by the Glasgow Haskell Compiler. Despite not being \nfull-.edged dependent types, GADTs still offer consid\u00aderably enlarged scope for enforcing important code \nand data invari\u00adants statically. Moreover, GADTs offer the tantalizing possibility of writing more ef.cient \nprograms since capturing invariants statically through the type system sometimes obviates entire layers \nof dy\u00adnamic tests and associated data markup. This paper is a case study on the applications of GADTs \nin the context of Yampa, a domain\u00adspeci.c language for Functional Reactive Programming in the form of \na self-optimizing, arrow-based Haskell combinator library. The paper has two aims. Firstly, to explore \nwhat kind of optimizations GADTs make possible in this context. Much of that should also be relevant \nfor other domain-speci.c embedded language imple\u00admentations, in particular arrow-based ones. Secondly, \nas the actual performance impact of the GADT-based optimizations is not ob\u00advious, to quantify this impact, \nboth on tailored micro benchmarks, to establish the effectiveness of individual optimizations, and on \ntwo fairly large, realistic applications, to gauge the overall impact. The performance gains for the \nmicro benchmarks are substantial. This implies that the Yampa API could be simpli.ed as a num\u00adber of \npre-composed primitives that were there mainly for per\u00adformance reasons are no longer needed. As to the \napplications, a worthwhile performance gain was obtained in one case whereas the performance was more \nor less unchanged in the other. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: Functional \nProgramming; D.3.2 [Programming Lan\u00adguages]: Language Classi.cations functional languages, data.ow languages; \nD.3.3 [Programming Languages]: Language Con\u00adstructs and Features data types and structures, polymorphism \nGeneral Terms Languages, Performance Keywords Functional programming, Haskell, arrows, combinator library, \ndomain-speci.c languages, DSEL, reactive programming, FRP, Yampa, synchronous data.ow languages, GADT \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n05 September 26 28, 2005, Tallinn, Estonia 1. Introduction Combinator libraries have proven to be a \nvery effective way of tailoring a functional programming language to address particular programming problems, \nsuch as parsing [30, 29], pretty-printing [17, 7], graphical user-interfaces [3], or even entire application \ndo\u00admains, such as animation [12], vision [25], or robotics [23]. Part of their attraction is that programming \nwith a well-designed combina\u00adtor library can be very much like using a customized programming language \nthat provides features and abstractions highly appropriate for the task at hand, but at a fraction of \nthe implementation effort re\u00adquired for a stand-alone language implementation. Indeed, the term Domain-Speci.c \nEmbedded Languages (DSEL) has been coined to refer to such libraries [15]. An equally important aspect \nof their attractiveness is that such libraries often can be implemented in a way that achieves levels \nof performance that are acceptable for many practical purposes. However, this may require quite sophisticated \nlibrary implementa\u00adtions that carry out static and/or dynamic analysis and optimiza\u00adtion, and thus a \nbit more effort on behalf of the library imple\u00admentor. A good example is Swierstra s and Duponcheel s \nself\u00adanalyzing parser combinators [29]. Another example is Yampa [21]1, a domain-speci.c language for \nFunctional Reactive Pro\u00adgramming (FRP) in the form of a self-optimizing Haskell combi\u00adnator library, \nwhich provides the setting for this paper. Unfortunately, as many combinator library implementors have \nobserved, the Hindley-Milner-based type systems of typical, mod\u00adern functional languages, such as SML \nor Haskell 98, often tend to get in the way once one try to pursue the optimization approach in earnest. \nFor example, this was noted in the Yampa paper, where it was pointed out that the ef.cient implementation \nof a particularly obvious optimization would require a simple form of dependent types [21, p. 62]. Baars \nand Swierstra [1] and Hughes [19] discuss the problems that the standard Haskell 98 system causes for \nop\u00adtimizing DSEL implementations in more general terms. Addition\u00adally, they show how certain commonly \nimplemented extensions of the Haskell 98 type system can be put to clever use to work around some of \nthe limitations of the standard type system. Alas, for rea\u00adsons to be discussed (see section 6), neither \nof these approaches would be ideal for Yampa. To give a concrete example, let us consider a simpli.ed \nversion of the problem encountered in the optimizing Yampa implementa\u00adtion. The central abstraction in \nYampa is that of a Signal Function. A signal function represents a simple, synchronous process, map\u00adping \nan input signal to an output signal. However, for this example, one can think of signal functions just \nas a plain function. The type of a signal function is written SF a(, where a is the type of the input \nand ( is the type of the output. Yampa is structured using John Hughes arrow framework [18]. This is \nan an abstract data Copyright c &#38;#169; 2005ACM1-59593-064-7/05/0009...$5.00. 1 Then called AFRP for \nArrowized Functional Reactive Programming type interface for function-like types, particularly suitable \nfor types that represent process-like computations, such as Yampa s signal functions. Two central arrow \ncombinators are arr, that constructs a (pure) arrow from an ordinary function (often referred to as lift\u00ading), \nand >>>, that forms a new arrow by composing two arrows, similar to ordinary function composition. In \nthe context of Yampa, the type signatures of these two combinators are arr ::(a ->b)->SFab (>>>) :: SF \nab ->SF bc -> SFa c In addition, the arrow framework speci.es a set of algebraic laws that all instances \nof the framework must satisfy. One of the require\u00adments is that the lifting of the identity function \nid must be the iden\u00adtity of arrow composition. That is, for an arbitrary arrow f: arr id >>> f = f = \nf >>> arr id It would of course be great if these two algebraic identities could be exploited in the \nde.nition of >>>, as this would eliminate the overhead of an arrow composition and of applying the identity \nfunction to the input or output of f. In an attempt to implement this for Yampa, one could imagine introducing \na constructor for signal functions that represents arr id: data SF ab =... | SFId --Represents arr id \n| ... The type SF is then made abstract by hiding all of its constructors, and an appropriately typed \nconstant is added to the API as the only way of constructing a signal function represented by SFId: identity \n:: SF a a identity = SFId Note that the constructor SFId itself has the more general type SF ab. The \nprogrammer would be asked to use identity in place of arr id if he or she wishes to take advantage of \nthe optimizations. The above algebraic identities can now we exploited in the de.\u00adnition of >>>. For \nexample, the following fragment of the de.nition of >>> captures the .rst of the two algebraic identities \nabove: (>>>) :: SF ab ->SF bc -> SFa c ... SFId >>> sf = sf ... The problem is now obvious. The de.ning \nequation above is only well-typed if the type of the second argument sf is the same as the overall return \ntype of >>>. That would be the case if the type vari\u00adables a and b always were instantiated to the same \ntype when the .rst argument is SFId. And indeed, as long as the only way to intro\u00adduce SFId is through \nthe type-constrained constant identity, that will be the case. Unfortunately, the type of the constructor \nSFId itself, which is all that matters when type checking the equation above, is too general to enforce \nthat constraint. In fact, the Haskell 98 type system does not provide any way to give SFId a suf.ciently \nconstrained type, nor any way to exploit such information in indi\u00advidual branches of function de.nitions \nor case expressions if it were possible. However, the recent addition of Generalized Algebraic Data Types \n(GADTs) [26] to the list of Haskell extensions supported by the Glasgow Haskell Compiler (GHC) gives \nprogrammers a lot more freedom. GADTs are a limited form of dependent types, offering a considerably \nenlarged scope for capturing and thus en\u00adforcing important code and data invariants statically. In particular, \nGADTs are just what is needed to address the problem discussed above since the key idea is to allow constructors \nthat have more spe\u00adci.c types than usual, and to take that extra type information into account in individual \ncase branches. GADTs would no doubt also offer an interesting alternative to the methods described by \nBaars and Swierstra [1] and Hughes [19]. This paper is a case study on the applications of GADTs in the \ncontext of Yampa. It has two aims. Firstly, to explore what kind of optimizations that GADTs make possible \nin this context. It turned out that GADTs are expressive enough to enable the implementation of optimizations \nwell beyond what was originally envisioned. Much of this should also be relevant for other domain\u00adspeci.c \nembedded language implementations, in particular arrow\u00adbased ones. Secondly, to quantify the actual performance \nimpact those optimizations can have, as the performance gains of GADT\u00adbased optimizations are not as \nclear-cut as it might .rst appear. The problem is that the optimizations do add to the size and complexity \nof the combinator library. This could have a negative performance impact which might offset the gains \nfrom the optimizations. With this in mind, performance .gures are given both for small, tailored, benchmarks, \nto establish the effectiveness of individual optimizations, and for two fairly large, realistic applications, \nto gauge their overall impact. The performance gains for the small benchmarks are substantial. This is \nencouraging in itself, and also implies that the Yampa API could be simpli.ed as a number of pre-composed \nprimitives that were there mainly for performance reasons are no longer needed. As to the applications, \nthe .rst was written well before GADTs were added to GHC and thus without GADT-based optimizations in \nmind. The other is mainly an event processing application, as it turned out that GADTs enabled a num\u00adber \nof interesting optimizations in that area. A worthwhile perfor\u00admance gain was obtained for the event \nprocessing one, whereas the performance was more or less unchanged for the other. The rest of the paper \nis organized as follows. Section 2 gives the necessary background on arrows, Yampa, and GADTs. Section \n3 reviews the the current, simply optimized, Yampa implementation, and shows that these optimizations \ndo have a positive performance impact that is suf.ciently large to make a substantial difference at the \nsystem level for non-trivial applications. This was always the assumption, but had not been con.rmed \nby measurements before. Section 4 then shows how GADTs can be used to optimize Yampa further. The effectiveness \nof the GADT-based optimization efforts is evaluated in section 5. Related work is discussed in section \n6. Section 7, .nally, gives conclusions. 2. Technical Background The introduction brie.y outlined the \narrow framework, Yampa, Generalized Algebraic Data Types, and their respective roles in this paper. This \nsection gives a somewhat more thorough presentation of these topics in the interest of making this paper \nself-contained. Nothing here is new, and a reader who is familiar with any (or all) of these can probably \nskim or even skip past the subsection(s) in question without loss of continuity. 2.1 Arrows The arrow \nframework, introduced by John Hughes [18], is an ab\u00adstract data type interface for function-like types. \nIt is particularly suitable for types that represent process-like computations, and that is why it is \ninteresting in the context of Functional Reactive Pro\u00adgramming and Yampa. Since arrows can be be seen \nas computa\u00adtions, arrows are related to monads, but arrows are more general in that they can handle more \nkinds of computations. A type constructor of arity two, together with three operations, arr, >>>, and \nfirst, form an arrow, provided certain algebraic laws hold (see below). In Haskell, this, except for \nthe algebraic requirements, is captured by the following class de.nition: class Arrow a where (a) arr \nf (b) a1 >>> a2 Figure 1. The core arrow combinators. arr ::(b->c)->abc (>>>) ::a bc ->a cd -> abd first \n:: a b c -> a (b,d) (c,d) The operator arr lifts an ordinary function to an arrow. Any ar\u00adrow that can \nbe constructed in that way is called pure. The operator >>> composes arrows in series, similar to ordinary \n(albeit reversed) function composition. The operator first is a widening opera\u00adtion that converts an \narrow from type b to type c to one operating on pairs, processing the .rst component of the pair through \nthe ar\u00adrow, but leaving the second component of the pair unchanged. This combinator is crucial when combining \narrows to work on more than one input, as it allows part of the input to be routed past an arrow for \nindividual processing later. Ordinary functions is the canonical example of an arrow. In that case, arr \nis just the identity function, >>> is reversed function composition, and first applies a function to \nthe .rst component of a pair. Another important operator is loop: a .xed-point operator used to express \nrecursive arrows or feedback [22]. Since not all arrow instances support such an operation, it is a method \nof a separate class: class Arrow a => ArrowLoop a where loop ::a (b, d)(c, d) -> abc An intuitive, and \nin the context of this paper, fully adequate, way to think about arrows, is as boxes with an input and \nan output, en\u00adcapsulating a mechanism for computing the output from the input, possibly making use of \nsome form of state in the process. Figure 1 uses that idea to illustrate the arrow combinators graphically. \nOther arrow combinators can be de.ned in terms of these primi\u00adtives. Commonly used derived combinators \nare second, the mirror image of first, and *** and &#38;&#38;&#38;, two forms of parallel arrow composition: \nsecond :: Arrow a => a b c -> a (d,b) (d,c) (***) :: Arrow a =>a bc -> ade -> a (b,d) (c,e) (&#38;&#38;&#38;) \n:: Arrow a =>a bc -> abd -> ab(c,d) It is a remarkable fact that using only the three basic arrow opera\u00adtors \nand loop, it is possible to express any conceivable network of interconnected arrows. As to the algebraic \nlaws, Hughes lists 9 laws for the three basic operators in his paper, and Paterson lists a further 6 \nlaws for loop. The following are the laws that are directly exploited for optimization purposes later \nin this paper: (f >>> g)>>> h = f >>> (g >>> h) (1) arr (f >>> g) = arr f >>>arr g (2) arr id>>> f = \nf (3) f = f >>> arr id (4) That is, composition is associative, is preserved by arr, and arr id is an \nidentity for composition. Note that the fact that functions are arrows is exploited in the formulation \nof law 2. 2.2 Yampa Functional Reactive Programming (FRP) is about describing reac\u00adtive systems as functions \nmapping signals (time-varying values) to signals. The nature of these signals depends on the application \ndo\u00admain, but they could represent input from sensors, video streams, or control signals for motors and \nother actuators. FRP has been ap\u00adplied to a number of domains, for example robotics (Frob) [23, 24], \nvisual tracking (FVision) [25], and graphical user interfaces (Fruit) [8]. FRP grew out of Conal Elliott \ns and Paul Hudak s work on Functional Reactive Animation [12]. Since then, the basic FRP framework has \nbeen implemented in a number of different ways. However, synchrony and support for both continuous and \ndiscrete time are common to most of them. There are thus close connections to, on the one hand, synchronous \ndata.ow languages like like Esterel [2], Lustre [5, 13], and Lucid Synchrone [6, 27], and to, on the \nother hand, hybrid automata [14] and languages for hybrid modeling and simulation, like Simulink [20]. \n2.2.1 Basics Yampa is an arrow-based implementation of FRP [21, 16]. Yampa takes the idea of describing \nsystems as functions on signals quite literally and provides Signal Functions as the central abstraction. \nThe type of a signal function mapping a signal of type a onto a signal of type ( is written SF a(. Intuitively: \nSF a( . Signal a -Signal ( where Signal a . Time -a for some suitable type Time representing continuous \ntime. How\u00adever, only signal functions are .rst-class entities in Yampa: signals only exist indirectly, \nthrough signal functions. This distinguishes Yampa from earlier FRP implementations. To ensure that signal \nfunctions are executable, they are required to be causal: the output of a signal function at time t must \nbe uniquely determined by the input signal on the interval [0,t]. Note that time here is local time, \nmeasured from the time at which a signal function is applied to its input signal, or switched in using \nYampa terminology. If the output at time t is determined solely by the input at time t, the signal function \nis said to be stateless, otherwise it is said to be stateful. A simple example of a stateful signal function \nis integral: integral :: SF Double Double de.ned by t y(t)= x(T)dT 0 where x(t) is the input signal and \ny(t) is the output signal. If more than one input or output signal are needed, tuples are used for a \nor ( since a signal of tuples is isomorphic2 to a tuple of signals. A Yampa system consists of a number \nof interconnected signal functions, operating on the system input and producing the system output. The \nsignal functions operate in parallel, sensing a common rate of time .ow. This is why Yampa is a synchronous \nlanguage. 2 More or less: the fact that the unde.ned value (.) belongs to every type in Haskell complicates \nthe picture slightly. Of course, when it comes to implementation, Yampa can only approximate the conceptually \ncontinuous signal model since the signals necessarily are evaluated for only a discrete set of sample \npoints. In Yampa, these points need not be equidistant, but it is the same set for all continuous-time \nsignals in a system. Another way to think of a signal function is thus as a simple synchronous process, \npossibly encapsulating some internal state, that at each tick of a global clock reads a value from the \ninput, processes it, updates any internal state, and outputs the result. This is indeed close to how \nYampa (and other FRP-based systems) are implemented. Signal functions are arrows, and Yampa makes the \nsignal func\u00adtion type SF an instance of the classes Arrow and ArrowLoop. The Yampa instance of the combinator \narr lifts an ordinary function to a signal function by applying the function pointwise to the input signal. \nThe result is a stateless signal function since the instanta\u00adneous value of the output signal at any \npoint in time only depends on the instantaneous input value at that same time. Any stateless signal function \ncan be constructed in this way, and stateless signal functions are thus pure arrows. The Yampa instance \nof composi\u00adtion, >>>, is what one would expect: at all points in time, the input to the composed signal \nfunction is fed into the .rst subordinate signal function, the resulting output is fed into the second \nsubordi\u00adnate signal function, and the resulting output from the second signal function is the overall \noutput of the composed signal function. The other basic arrow combinators are de.ned in a similar manner. \n 2.2.2 Events In FRP, the domain of a signal can, conceptually, be either be continuous or discrete. \nIn the former case, the signal is de.ned at every point in time. In the latter case, the signal is a \npartial function, only de.ned at discrete points in time. Such a point of de.nition is called an event. \nIn Yampa, this distinction has been deliberately blurred to make it easier to mix and match continuous\u00adtime \nand discrete-time signals. The notion of discrete-time signals is captured by lifting the range of continuous-time \nsignals using an option type called Event, similar to Haskell s Maybe type.3 This type has two data constructors: \nNoEvent, representing the absence of a value; and Event, representing the presence of a value, also called \nan event occurrence. In Haskell notation: data Event a = NoEvent | Event a A discrete-time signal carrying \nelements of type a can thus be thought of as a function of type Signal (Event a). Yampa provides a rich \nset of functions for operating pointwise on events. In fact, the type Event is abstract in the current \nYampa implementation, so events cannot be manipulated except through these operations. Some examples \nare: tag :: Event a -> b -> Event b lMerge :: Event a -> Event a -> Event a rMerge :: Event a -> Event \na -> Event a filterE :: (a -> Bool) -> Event a -> Event a Note that these all are ordinary functions. \nThey have to be lifted (using arr) in order to process discrete-time signals. The func\u00adtion tag tags \nan event with a new value, replacing the old one. lMerge and rMerge allow two discrete signals to be \nmerged point\u00adwise. In case of simultaneous event occurrences, lMerge favors the left event (.rst argument), \nwhereas rMerge favors the right event (second argument). filterE, .nally, suppresses events which do \nnot satisfy the Boolean predicate supplied as the .rst argument. 3 Note that Event is used as the name \nfor both the type constructor and a data constructor, which is not uncommon in Haskell code. A clearer \nname for the type constructor might have been MaybeEvent, but that, among other problems, is a bit verbose. \nAdditionally, Event is an instance of Functor, allowing fmap to be used on events: fmap :: Functor f \n=>(a -> b) ->f a-> fb Yampa also provides stateful signal functions for generating and processing events. \nFour important examples are: edge :: SF Bool (Event ()) hold :: a -> SF (Event a) a accumBy ::(b -> a-> \nb)-> b -> SF (Event a) (Event b) accum :: a -> SF (Event (a -> a)) (Event a) The signal function edge \ngenerates an event whenever the in\u00adput changes from False to True. A signal function whose out\u00adput signal \nis of type Event a for some type a is called an event source. The signal function hold converts a discrete-time \nsignal to a continuous-time one by holding on to the value carried by the last input event. The signal \nfunction accumBy processes events by applying a function to incoming events and an internal state. An \noutput event is generated in response to each input event. It is tagged with the result of the function \napplication, which also be\u00adcomes the new internal state. The signal function accum is similar. 2.2.3 \nDynamic System Structure The structure of a Yampa system may evolve over time. These structural changes \nare known as mode switches. This is accom\u00adplished through a family of switching primitives that use events \nto trigger changes in the connectivity of a system. The simplest such primitive is switch: switch :: \nSF a (b,Event c) -> (c->SF a b) -> SFa b switch switches from one subordinate signal function into another \nwhen a switching event occurs. The .rst argument of switch is the signal function that initially is active. \nIt outputs a pair of signals. The .rst de.nes the overall output while the initial signal function is \nactive. The second signal carries the event that will cause the switch to take place. Once the switching \nevent occurs, switch applies its second argument to the value with which the event is tagged and switches \ninto the resulting signal function. Note that the switching constructs are what apply a signal func\u00adtion \nto a signal argument, or, if one prefers, spawns a process, with the exception of the initial top-level \nsignal function that gets started by other means. A signal function in itself is just inert code.4 Yampa \nalso includes parallel switching constructs that maintain dynamic collections of signal functions connected \nin parallel. Sig\u00adnal functions can be added to or removed from such a collection at runtime in response \nto events; see .gure 2. The .rst class status of signal functions in combination with switching over \ndynamic collections of signal functions makes Yampa an unusually .exible language for describing hybrid \nsystems and sets it apart from typi\u00adcal synchronous data.ow and simulation languages [21].  2.3 Generalized \nAlgebraic Data Types Generalized Algebraic Data Types (GADTs) have recently been added to the list of \nHaskell extensions supported by the Glasgow Haskell Compiler (GHC) [26]. The idea of GADTs is not new. \nA number of variations have been proposed under a number of differ\u00adent names, such as guarded recursive \ndata types and equality qual\u00adi.ed types. In particular, GADTs are closely related to inductive families \nthat have been studied in the dependent types community for a very long time. However, this is the .rst \ntime these ideas have 4 This is in contrast to some earlier FRP implementations that didn t make a sharp \ndistinction between signals and signal functions. Figure 2. System of interconnected signal functions \nwith varying structure been made available to a wider audience in the context of a very mature implementation \nof a reasonably well-established language. The key idea of GADTs is to allow the type of each data type \nconstructor to be stated separately, and to take that extra type information into account in individual \ncase branches when taking data structures apart, during pattern matching. Let us return to the example \nfrom the introduction. GADTs allow us to de.ne a constructor representing arr id that has the expected, \nmore speci.c type SF aa as opposed to SFa b: data SF a b where ... SFId ::SF aa ... But apart from this, \nthere is nothing special with SFId, and its type is a perfectly normal Haskell type. The fun starts when \nwe do case analysis on signal functions. The following fragment of the de.nition of >>>, that re.ects \narrow law 3 (section 2.1), is exactly as given in the introduction: (>>>) :: SF ab ->SF bc -> SFa c ... \nSFId >>> sf = sf ... The difference is that the type checker now is going to accept the de.nition since \nit knows that the type of the .rst argument in case it is constructed using SFId must be SF a a. That \nis, the types a and b must in fact be equal, and thus the second argument sf in fact has type SFa c, \nwhich is exactly the return type. The equation is therefore well-typed.  3. Basic Yampa Implementation \nThis section outlines the implementation of the current version of Yampa. This has been described before \n[21], and is only included here to make the present paper self-contained. The current Yampa version performs \nsome basic dynamic optimizations, for example, it exploits arrow law 2 (section 2.1), replacing arrow \ncomposition by simple function composition, but as noted in the introduction, there are a number of obvious \noptimizations that were not imple\u00admented because the Haskell 98 type system got in the way. However, \nbefore looking at the current version, a very simple implementation that does not do any dynamic optimization \nat all will be described. This is done in part for explanatory purposes, and in part to provide a baseline \nfor establishing the effectiveness of the basic optimizations, as that up until know had been taken on \nfaith. For example, while replacing arrow composition by sim\u00adple function composition indeed does seem \nlike an obvious win in a system like Yampa, where arrow composition is an expensive operation compared \nto function composition, the merit of simplic\u00adity should never be underestimated. Simplicity does translate \ninto smaller function de.nitions, less case analysis, and possibly a bit more compact representation \nof signal functions, all of which could have a positive impact on the performance. This section is thus \ncon\u00adcluded by a simple comparison of the performance of the current implementation and the totally unoptimized \none. 3.1 The Unoptimized Implementation The Yampa implementation uses a continuation-based signal func\u00adtion \nrepresentation, originally inspired by the implementation of the Fudgets graphical user interface toolkit \n[4]. There are also sim\u00adilarities to the residual behaviors implementation of Fran [10]. We start by \nconsidering the unoptimized version. Each signal function is essentially represented by a transition \nfunction. This takes as arguments the amount of time passed since the previous time step and the current \ninstantaneous value of the input signal. It returns a transition: a pair of a (possibly) updated representation \nof the signal function, the continuation, along with the current value of the output signal: type DTime \n= Double data SF ab= SF {sfTF :: DTime -> a -> Transition a b} type Transition a b = (SF a b, b) The \ncontinuation encapsulates any internal state of the signal func\u00adtion. The type synonym DTime is the type \nused for the time deltas. They are assumed to be strictly greater than 0. We will return to the question \nwhat the initial time delta should be below. The function reactimate is responsible for animating a sig\u00adnal \nfunction. It runs in an in.nite loop. At each point in time, reactimate reads an input sample and the \ntime from the external environment (typically via an I/O action), feeds this sample value and the amount \nof time that passed since the previous sampling to the signal function s transition function, and then \nwrites the re\u00adsulting output sample to the environment (also typically via an I/O action). The loop then \nrepeats, but uses the continuation returned from the transition function on the next iteration, thus \nensuring that any internal state is maintained properly. As a .rst example of a signal function implementation, \nlet us consider the combinator arr: arr ::(a -> b)-> SF ab arr f= sf where sf= SF {sfTF =\\_ a ->(sf, \nf a)} It is obvious that arr constructs a stateless signal function since the returned continuation \nis exactly the signal function being de.ned, i.e. it never changes. Now let us consider serial composition. \nSince each of the signal functions being composed could be stateful, we have to make sure to combine \ntheir continuations into an updated continuation for the composed arrow: (>>>) :: SFa b-> SF bc ->SF \nac (SF {sfTF = tf1}) >>> (SF {sfTF = tf2})= SF {sfTF = tf} where tf dt a = (sf1 >>> sf2 , c) where (sf1 \n, b) = tf1 dt a (sf2 , c) = tf2 dt b Note how the de.nition corresponds to the intuitive semantics given \nin section 2.2.1. Also note how the same time delta is fed to both subordinate signal functions, thus \nensuring synchrony. When a transition function is invoked for the very .rst time, there is no meaningful \ntime delta to feed in since there is no prior invocation of that transition function. For that reason, \nsignal func\u00adtions are actually represented by a slightly simpler transition func\u00adtion that only expects \na single argument: the initial input value. Once an initial input sample has been fed in, the signal \nfunctions makes a transition to a running state, where the transition func\u00adtion has the more general \nform described above. Thus, the real type de.nitions are as follows: data SF ab =SF {sfTF :: a -> Transition \na b} data SF ab = SF {sfTF :: DTime -> a -> Transition a b} type Transition a b = (SF a b, b) The type \nSF is an internal type, hidden from the user. The real de.nitions of arr and >>> are also slightly more \ncomplicated, as the de.nitions above are internal and really called something else, and what the user \nsees are essentially wrappers around these internal de.nitions. 3.2 The Simply Optimized Implementation \nWe now turn our attention to the current, simply optimized version of Yampa. The central idea is to represent \nstateless signal functions, i.e., signal functions constructed by arr, using a special construc\u00adtor. \nThis makes it possible to recognize such signal functions for example when composing signal functions, \nallowing arrow law 2 to be exploited: arr (f >>> g) = arr f>>> arr g Additionally, we introduce a special \nconstructor for the signal function constant, conceptually de.ned through constant :: b -> SF a b constant \nb = arr (const b) This is done because the following laws hold, suggesting what ought to be worthwhile \noptimizations: sf >>> constant c = constant c constant c >>> arr f = constant (f c) A number of useful \nidentities involving constant also hold for other arrow combinators, like first and &#38;&#38;&#38;, \nand for some Yampa-speci.c ones like switch. The utility of handling constant specially thus goes beyond \nserial arrow composition. Here is the new de.nition of SF : data SF ab = SFConst { sfTF :: DTime -> a \n-> Transition a b, sfCVal :: b } | SFArr { sfTF :: DTime -> a -> Transition a b, sfAFun :: a -> b } | \nSF {sfTF :: DTime -> a -> Transition a b} Note that all constructors has a sfTF component. This enables \nsignal functions to be handled uniformly in cases where one do not wish to differentiate between the \ndifferent kinds. The internal versions of constant and arr are then de.ned as follows: sfConst :: b -> \nSF a b sfConst b = sf where sf = SFConst { sfTF = \\_ _ -> (sf, b), sfCVal = b } sfArr :: (a-> b) ->SF \na b sfArr f =sf where sf = SFArr { sfTF =\\_a->(sf,fa), sfAFun = f } Implementing the internal version \nof >>> is straightforward, if a bit tedious: cpAux _ sf2@(SFConst {}) = sfConst (sfCVal sf2) cpAux sf1@(SFConst \n{}) sf2 = cpAuxC1 (sfCVal sf1) sf2 cpAux sf1@(SFArr {}) sf2 = cpAuxA1 (sfAFun sf1) sf2 cpAux sf1 sf2@(SFArr \n{}) = cpAuxA2 sf1 (sfAFun sf2) cpAux sf1 sf2 = SF {sfTF = tf} where tf dt a = (cpAux sf1 sf2 , c) where \n(sf1 , b) = (sfTF sf1) dt a (sf2 , c) = (sfTF sf2) dt b The functions cpAuxC1, cpAuxA1 and cpAuxA2 are \nspecialized versions of cpAux that exploit that the form of the .rst or second of the arrows being composed \nis known, and that it thus would be wasteful to pattern match on those over and over again. We give cpAuxA1 \nas an example. Note the second equation that implements the optimization suggested by arrow law 2: cpAuxA1 \n_ (SFConst{sfCVal=c}) = sfConst c cpAuxA1 f1 (SFArr {sfAFun=f2}) = sfArr (f2 . f1) cpAuxA1 f1 (SF {sfTF \n=tf2}) = SF {sfTF =tf} where tf dt a = (cpAuxA1 f1 sf2 , c) where (sf2 , c) = tf2 dt (f1 a) Recall that \nYampa, through the use of switching, allows system with dynamic structure to be described (see section \n2.2.3). This means that a signal function at some point could become a to\u00adtally different signal function, \nperhaps something very simple, like arr f. For example: switch (...) (\\_ -> arr f) If the above code \nfragment was used in, say, an arrow composition, it might, at the time of the switch, become possible \nto eliminate that arrow composition completely, e.g.: arr g >>> switch (...) (\\ -> arr f) switch =. arr \ng>>> arr f = arr (f. g) Yampa does implement this, as can be seen from the code for com\u00adposition above \n(cpAuxA1), but it means that the various arrow com\u00adbinators constantly have to monitor the subordinate \nsignal functions to see if they have changed shape in such a way that an optimiza\u00adtion has become possible. \nThe dynamic system structure is why optimizations have to be performed dynamically in Yampa, while the \nsystem is running. A single optimization phase before the sig\u00adnal processing proper starts would not \nbe good enough. See section 6 for some further discussion. Figure 3. Screenshot of Space Invaders 3.3 \nEffectiveness of the Simple Optimizations From comparing the code of the unoptimized and the simply opti\u00admized \nimplementations of Yampa, it should be clear that the simply optimized version is quite a bit bigger, \nand that it potentially spends more time on case analysis than the unoptimized version since there simply \nare more cases to consider. All of this could have a negative performance impact, and that price would \nhave to be paid regard\u00adless of whether any actual optimization opportunities turn up or not. Thus, as \nalways, the question is if the gains really outweigh the costs? In particular, what is the bottom line \nin the context of fairly large, realistic applications? Are there suf.ciently many op\u00adtimization opportunities, \nand are any gains large enough to justify the extra implementation complexity? To begin answering such \nquestions, a simple, initial experi\u00adment was carried out. The performance of two reasonably large and \ndemanding application were measured using the two Yampa implementations to see if any signi.cant performance \ndifferences emerged. Obviously, since only two applications are involved, any far-reaching conclusions \nbased on the outcome must be avoided. But a positive outcome should still be reassuring. The Yampa Space \nInvaders game [9] was chosen as one of the benchmark application since it was considered representative \nof typical applications. The screenshot in .gure 3 should give an indication as to its complexity. Since \nthe objective was to measure the impact of the optimizations on the execution of signal functions, the \ngame was modi.ed by disabling the graphical rendering as that accounted for a signi.cant part of the \noverall run time. Further modi.cations were made to run the game for a .xed number of cycles over a .xed \nlength of simulated time on .xed input. The other benchmark is a high-level model of a MIDI5 Event Processor \n(MEP), patterned after hardware devices like the Yamaha MEP46. Such a device allows a stream of MIDI \nevents (like note on and off) to be transformed to achieve effects like splitting, layering, and arpeggios \n(by adding delayed messages). Yampa s synchronous nature and event processing capabilities make it very \nwell suited for this type of application. The Yampa MEP was 5 Musical Instrument Data Interface 6 http://www.yamaha.co.jp/manual/english/ \nBenchmark TU [s] TS [s] TS/TU Space Invaders 0.95 0.86 0.91 MEP 19.39 10.31 0.53 Table 1. Benchmark \nperformance. Averages over .ve runs. programmed to carry out typical split and layering duties, including \nadding arpeggiated notes if the velocity attribute of a note-on event indicates that the keyboard key \nhas been played hard. The measurements were carried out on a 1.6 GHz Pentium lap\u00adtop running Linux. The \nspeedstep facility was turned off to ensure that the CPU speed was not changed during the measurements. \nTwo versions of the each benchmark were compiled, one using an unop\u00adtimized version of Yampa like the \none presented in section 3.1, the other using the simply optimized version. The benchmarks were then \nrun with a known initial heap size (8 MByte). The size was chosen large enough so that most time would \nbe spent executing actual Yampa code as opposed to doing garbage collection, but also small enough so \nthat really bad space behavior of either implemen\u00adtation would have an impact. The average execution \ntime was mea\u00adsured over .ve good runs using the Linux time command by adding the reported user and system \ntimes. A run was consid\u00adered good if 90 % or more of the CPU time was spent on executing the benchmarks, \nindicating that not much else that could skew the results was going on. The GHC runtime system was instructed \nto print out the time spent on executing code and doing garbage col\u00adlection. About 5 % of the overall \nrun time was spent on garbage collection in each case. Thus there were no signi.cant differences in this \nrespect. The results are given in table 1. T stands for total execution time. The subscript U refers \nto the unoptimized implementation, whereas S refers to the simply optimized one. The results do indeed \nsuggest that the simple optimizations have a substantial positive impact at the system level, in particular \nthe results for MEP where the optimized version runs almost twice as fast. The Space Invaders benchmark \ninvolves a lot of numerical .oating point and vector computations, and a closer inspection reveals that \na large part of the overall time is spent there. Considering this, a 10 % shorter execution time is a \nfairly good performance improvement.  4. Optimizing Yampa Using GADTs This section explores how GADTs \ncan be employed to implement dynamic optimizations in a Yampa that go beyond what is possible in plain \nHaskell 98. General arrow optimizations are considered .rst, then Yampa-speci.c ones, particularly improving \non event processing. 4.1 Optimizing Pure Arrows Let us return to the problem of optimizing arrow composition \nwith the identity arrow. One approach was outlined in section 2.3, and integrating that into the current \nYampa implementation as de\u00adscribed in section 3.2 is mostly a matter of just adding the con\u00adstructor \nSFId to the type SF . However, when the emerging opti\u00admization opportunities are implemented for the \nvarious arrow com\u00adbinators, there is a lot of code duplication related to composition of pure arrows, \nof which there now are three varieties: constant, identity, and all others. Additionally, for Yampa-speci.c \nopti\u00admizations, it is necessary to consider further cases, as will be dis\u00adcussed later. A better approach \nis to factor out the description of the functions to be lifted, write a single function for dealing with \ncomposition of such descriptions, and .nally add a single signal function con\u00adstructor SFArr for pure \narrows, that incorporates this description (instead of just a function about which nothing is known). \nUsing GADTs, a data type describing the three kinds of func\u00adtions can be de.ned as follows: data FunDesc \na b where FDI :: FunDesc a a FDC :: b -> FunDesc a b FDG :: (a -> b) -> FunDesc a b FDI represents the \nidentity function id, FDC b represents the func\u00adtion const b, and FDG f represents a general function \nf. Note how the types of the function descriptions match the types of the func\u00adtions being described. \nNext, a way to recover the described function from a function description is needed: fdFun :: FunDesc \na b -> (a -> b) fdFun FDI = id fdFun (FDC b) = const b fdFun (FDG f) = f Note how GADTs come into play \nin the FDI case. The equation is well-typed only because FDI has type FunDesc a a, meaning that types \na and b are equal in that case, which in turns makes it legitimate to return id as the result. Composition \nof function descriptions can now be de.ned. Again, note how GADTs come into play wherever FDI is used. \nfdComp :: FunDesc a b -> FunDesc b c -> FunDesc a c fdComp FDI fd2 = fd2 fdComp fd1 FDI = fd1 fdComp \n(FDC b) fd2 = FDC ((fdFun fd2) b) fdComp _ (FDC c) = FDC c fdComp (FDG f1) fd2 = FDG (fdFun fd2 . f1) \nIt is also worth to note how pleasingly natural and direct the de.ni\u00adtion is. 4.2 Optimizing Arrow Composition \nArrow composition is associative. Thus it would be desirable if op\u00adtimization of composition was performed \nas well as possible re\u00adgardless of the bracketing. This is particularly important when ar\u00adrow notation \n[22] is being used since the user then does not always have control over the bracketing. However, the \ncurrent Yampa im\u00adplementation is not that well-behaved. For example, if sf is some general signal function, \nthen (arr f >>> arr g) >>> sf would be optimized to arr (g .f) >>> sf eliminating one costly arrow composition, \nbut arr f >>> (arr g >>> sf) would not be optimized, except that the implementation of >>> is slightly \nmore ef.cient for composition with a pure arrow (see section 3.2). To address this problem, a constructor \nrepresenting arrow com\u00adpositions of the form arr f>>> sf >>> arr g is introduced. The idea is due to \nHughes [19]. The point is that com\u00adposition becomes observable, allowing re-bracketing if necessary, \nand that pure arrows to either side always can be absorbed . Here is the new de.nition of SF . SFArr \nis the single con\u00adstructor for pure arrows that was discussed in the previous section. SFCpAXA represents \npre and post composition with pure arrows: data SF a b where SFArr :: (DTime -> a -> Transition a b) \n-> FunDesc a b ->SF a b SFCpAXA :: (DTime -> a -> Transition a d) -> FunDesc a b ->SF b c -> FunDesc \nc d ->SF a d SF :: (DTime -> a -> Transition a b) ->SF a b Note that a signal function constructed using \nSFCpAXA includes the representations of the three subordinate signal functions. This is what makes it \npossible to change the bracketing structure. However, they play no direct role when making a transition: \nthat is the sole responsibility of the transition function, as before. It will construct an updated version \nof the composed signal function, where the two subordinate pure arrows of course are the same as before, \nbut where the signal function in the middle possibly has been updated. Let us now look at the implementation \nof composition. Like in the simply optimized implementation, unnecessary construction and destruction \nof signal function representations is avoided by having specialized functions dealing with cases where \none or more of the subordinate signal functions is something simple, like a pure arrow, constant, or \neven identity. However, there are now more cases to consider, and we will thus only look at some fragments \nof the de.nition to convey the general ideas. We begin with composition of two arbitrary signal functions. \nA naming convention is used where X stands for arbitrary signal function (could be anything) and A stands \nfor an arbitrary pure ar\u00adrow. Thus the function dealing with composition of arbitrary signal functions \nis called cpXX, and the .rst few lines of its de.nition are as follows: cpXX :: SF a b-> SF bc -> SF \na c cpXX (SFArr _ fd1) sf2 = cpAX fd1 sf2 cpXX sf1 (SFArr _ fd2) = cpXA sf1 fd2 As can be seen, if it \nturns out that either the .rst or the second argument is a pure arrow, one of two dedicated functions \nthat are optimized for those cases are invoked to carry out further analysis. Here is the de.nition of \ncpAX that handles the case where the .rst argument is a pure arrow. cpAX :: FunDesc a b-> SF bc ->SF \na c cpAX FDI sf2 = sf2 cpAX (FDC b) sf2 = cpCX b sf2 cpAX (FDG f1) sf2 = cpGX f1 sf2 If the .rst argument \nis the identity function, then the entire com\u00adposition can be immediately optimized to just sf2. GADTs \nare needed to make this case well-typed. Otherwise, further process\u00ading is delegated to functions that \nanalyze the second argument to spot further optimization possibilities and, if none is found, are de\u00adsigned \nto implement composition as ef.ciently as possible given the knowledge they have about the .rst argument \n(constant or a pure function). If two signal functions constructed by SFCpAXA are composed, then a re-bracketing \nstep is performed with the aim of fusing the two pure arrows in the middle. Algebraically, we have: (arr \nf >>> sf1 >>> arr g) >>> (arr f >>> sf2 >>> arr g ) = arr f >>> ((sf1 >>> arr (f . g)) >>> sf2) >>> arr \ng The code is as follows: cpXX (SFCpAXA _ fd11 sf12 fd13) (SFCpAXA _ fd21 sf22 fd23) = cpAXA fd11 (cpXX \n(cpXA sf12 (fdComp fd13 fd21)) sf22) fd23 Finally, if no optimizable case has been detected, the basic \ncomposition is performed. Note that the updated representation of the composed signal function is constructed \nrecursively using cpXX, ensuring that sf1 and sf2 will be analyzed during the next time step to detect \nany emerging optimization possibilities. cpXX sf1 sf2 = SF tf where tf dt a = (cpXX sf1 sf2 , c) where \n(sf1 , b) = (sfTF sf1) dt a (sf2 , c) = (sfTF sf2) dt b Other arrow combinators are optimized in similar \nways. For example, first identity is optimized to identity. More ad\u00advanced optimizations involving e.g. \nfirst and >>> have not yet been attempted. For instance, Hughes identify optimizing first f >>> first \ng to first (f >>> g) as troublesome due to the type system really getting in the way. GADTs should solve \nthis. The main dif.culty is structuring the code in a way that keeps the number of cases that have to \nbe considered manageable. 4.3 Optimizing Event Processing As was discussed in section 2.2.2, Yampa deliberately \nblurs the dis\u00adtinction between continuous-time signal and discrete-time signals by layering the latter \non top of the former through the Event op\u00adtion type. This is quite convenient from a programming perspective, \nbut does mean that it is hard to implement pure event-processing as ef.ciently as one could hope for. \nThe involved signal functions gets invoked at every single time step, even though everything re\u00admain \nconstant between events. Thus, if events are relatively sparse, the overhead is considerable. In some \nearlier FRP implementations, where discrete and continuous signal functions were separate con\u00adcepts, \nthere was greater scope for optimization. In this section, we will see how GADTs allow some optimizations \nof event-processing to be reintroduced. Consider the following composition of pure arrows: f :: Event \na -> Event b g :: Event b -> Event c arr f>>> arr g Normally events occur relatively sparsely. Thus, \nfor the most part, the output from f is going to be NoEvent, and the result of g ap\u00adplied to NoEvent \nis going to be computed over and over again. That seems a bit wasteful. If the assumption that events \noccur sparsely holds, it would be better to compute g NoEvent only once, and reuse that value whenever \nthe output from f is NoEvent. The func\u00adtion g would then only have to be invoked on event occurrences. \nThis can be achieved by introducing a function description for event-processing functions, allowing the \ncomposition of such functions to be handled specially. However, instead of representing functions of \ntype Event a->Event b as the example above would suggest, we chose to represent functions of type Event \na->b to cover more functions. Nothing is lost by doing this: if two such functions are composed, the \nresult type of the .rst function must in fact be Event, and GADTs allow that fact to be exploited. The \ntype FunDesc from section 2.2.2 is thus extended as fol\u00adlows: data FunDesc a b where ... FDE :: (Event \na ->b) -> b -> FunDesc (Event a) b The .rst argument to FDE is the event-processing function in ques\u00adtion. \nThe second argument is the result of applying that function to NoEvent. The function fdComp is then extended \nto handle compo\u00adsitions involving event-processing functions: fdComp :: FunDesc a b -> FunDesc b c -> \nFunDesc a c ... fdComp (FDE f1 f1ne) fd2 = FDE (f2 . f1) (f2 f1ne) where f2 = fdFun fd2 fdComp (FDG f1) \n(FDE f2 f2ne) = FDG f where fa =case f1 aof NoEvent -> f2ne f1a -> f2 f1a The second equation re.ects \nthe discussion above. Note how f2 only gets invoked on events. Also note the crucial role played by GADTs \nfor making the code well-typed. In particular, the type re.nement due to GADTs is what makes the case \nanalysis of f1 a possible. The only question now is how to get FDEs into play. The other two special \nfunction descriptions, FDI and FDC, are introduced by providing the special signal functions identity \nand constant in the Yampa API, and asking the user to use those in preference to writing arr id and arr \n(const x). Similarly we could intro\u00adduce a special version of arr, say arrE :: Event a -> b, and ask \nthe user to use that wherever possible. However, while not ideal, identity and constant work because \nthey are fairly natural sig\u00adnal functions to have in the API anyway, and because it is only those two. \nAsking the user to remember to use arrE where possible is a bit too much. Moreover, when the arrow notation \nis used, many arrs get introduced by the translation into plain Haskell, outside the control of the user. \nOne way to resolve this dilemma is to employ a GHC-speci.c trick. A rewriting rule is speci.ed that instructs \nthe compiler to rewrite arr f to arrE f whenever the type of f is Event a ->b7. The rule is remarkably \nsimple, exploiting that the GHC rewriting rules only apply when the types match: {-# RULES \"arrPrim/arrEPrim\" \narrPrim = arrEPrim #-} This solution is good because it is totally transparent to the user. However, \nthe price paid is that the Yampa implementation gets tied even harder to GHC. An alternative might be \nto use the class sys\u00adtem, but the only solution in that direction explored by this author necessitated \nchanging the Arrow class slightly and enabling both 7 Note that GHC rewrite rules could not be used to \nimplement the optimiza\u00adtions described in this paper in general since the optimizations have to be carried \nout dynamically as explained in section 3.2. overlapping and incoherent instances. Altogether not very \nappeal\u00ading. In the worst case, should rewriting rules not be available, all is not lost since optimization \nof stateful event processing, described in the following, only relies on GADTs. Event-based optimization \nin the spirit above can also be imple\u00admented for stateful event-processing signal functions like accumBy \nand hold. Both of these, and many other Yampa event processors, can be seen as instances of a general \nstateful event-processing sig\u00adnal function that we can call ep: ep ::(c -> a-> (c,b,b)) -> c-> b -> SF \n(Event a) b The argument of type c is the initial sate, the argument of type b is the initial quiescent \noutput (when the input is NoEvent), and the function argument is the function that gets invoked on the \nvalue of any incoming event and the current state to compute the updated state, the output at the point \nof the event, and the new quiescent output. For example, hold can be de.ned as follows: hold :: a -> \nSF (Event a) a hold a_init = ep f () a_init where f_ a= ((), a,a) The point of this is that composition \nof ep with ep or with pure arrows, both event-processing ones and others, results in a signal function \nthat can be expressed in terms of a single ep, in a manner similar to the composition of pure, event-processing \narrows, thus allowing an arrow composition to be optimized away. The result is that pipelines of event-processing \nsignal functions execute very ef.ciently since the latter signal functions in the pipeline only get invoked \non events. All that is needed to achieve this is to introduce a constructor that represents ep, and extend \nthe implementation of arrow composition accordingly: data SF a b where ... SFEP :: (DTime -> Event a \n-> Transition (Event a) b) -> (c-> a-> (c, b, b)) ->c ->b -> SF (Event a) b Note that GADTs are again \nneeded here. The code for composition is in many ways very similar to the stateless case discussed above. \nThe details are omitted. 4.4 Optimizing Simple Stateful Signal Processing Many simple, stateful continuous \nsignal functions, like edge or pre (an in.nitesimal delay), can be expressed in terms of a common underlying \nprimitive signal function in much the same way as the stateful event processors discussed above. If that \nunderlying signal function is designed in such a way that it composes nicely with pure arrows, stateful \nevent processors, and itself, this opens up many opportunities for optimization. There is a quite large \ndesign space here. The current design employs a function of type c -> a -> Maybe (c, b) to compute the \nnew internal state and the output given the present internal state an input. If the result is Nothing, \nthis indicates that both the state and output should stay as they were. The new constructor is called \nSFSScan: data SF a b where ... SFSScan :: (DTime -> a -> Transition a b) -> (c-> a-> Maybe (c, b)) -> \nc-> b -> SF a b Benchmark TS [s] TG [s] 1 0.41 0.00 2 0.74 0.22 3 0.45 0.22 4 1.29 0.07 5 1.95 0.08 6 \n1.48 0.69 7 2.85 0.72 Table 3. Micro benchmark performance. Averages over .ve runs. Note that GADTs \nare not absolutely essential for expressing SFSScan in itself: existential quanti.cation could have been \nused. However, GADTs are needed when coding composing with e.g SFEP. The details of coding the various \ncompositions are omitted, but again similar to what has been discussed before.  5. Evaluation of the \nGADT-based Optimizations In the previous section we saw how GADTs made possible a num\u00adber optimizations, \nboth general and Yampa-speci.c ones. However, we also saw that the size and complexity of the implementation \ngrew. That could offset any gains from the optimizations. This sec\u00adtion presents some benchmark results \nin an attempt to determine whether the optimizations are worthwhile. Extensive testing was conducted \non a number of micro bench\u00admarks designed to evaluate whether optimizations based on the arrow laws, \nevent processing, and composition of simple stateful signal functions worked as intended. A few representative \nones are considered in the following: see table 2. The procedure and test\u00ading conditions were as described \nin section 3.3, except that the benchmarked implementations now are the simply optimized one (YampaS) \nand the one with GADT-based optimizations (YampaG ). The benchmarks were executed for a .xed simulated \ntime period. Appropriate input (continuous or events) was generated using suit\u00adable signal functions. \nHowever, the time for generating the input was excluded from the benchmark times in order to highlight \nhow much longer the execution times became when the input was pro\u00adcessed through the benchmarks of table \n2. The average execution times for the micro benchmarks are given in table 3. TS and TG are the execution \ntimes for YampaS and YampaG respectively. The amount of time spent on garbage col\u00adlection was monitored, \nbut it was small in all cases. The measured times thus represent time spent on executing Yampa code. \nAs can be seen from the times for benchmark 1, YampaG, as expected, succeeds in eliminating the overhead \nof composition with identity completely. Studying benchmarks 2 and 3, we see that YampaG is insensitive \nto parenthesization order, whereas YampaS is not (sf is the input generator and is not included in the \nmeasured times). Moreover, YampaG appears to be a little faster. In the case of benchmarks 4 to 7, one \nshould keep in mind that the implementations of the involved stateful signal functions are very different \nin YampaS and YampaG . While it certainly is good that YampaG is a lot faster here, the most important \nis to study how the execution times change as further signal functions are added to a pipeline. Comparing \nthe times for benchmarks 4 and 5 shows that the overhead of adding an additional stateful event processor \n(here accumBy) to an event processing pipeline in YampaG is al\u00admost negligible. Similarly, the results \nfor benchmarks 6 and 7 show that the overhead of adding stateless and stateful event processing signal \nfunctions to some stateful continuous-time signal processing is also very small. GADTs have thus enabled \nef.cient fusing of a variety of stateful and stateles signal functions. However, it should Benchmark \nCode Description 1 identity >>> identity Composition of identity 2 (sf >>> arr (*1)) >>> arr (*2) Left-biased \nparenthesization 3 sf >>> (arr (*1) >>> arr (*2)) Right-biased parenthesization 4 accumBy count >>> hold \n0 Stateful event processing 5 accumBy count >>> accumBy (+) 0 >>> hold 0 Stateful event processing 6 \narr (>) >>> edge Stateful and stateless signal processing 7 arr (>) >>> edge >>> arr ( tag (+1)) >>> \naccum 0 Stateful and stateless signal and event processing Table 2. Micro benchmarks. Benchmark TS [s] \nTG [s] TG/TS Space Invaders 0.86 0.88 1.02 MEP 10.31 9.36 0.91 Table 4. Application benchmark performance. \nAverages over .ve runs. be pointed out that the event density was very low in these experi\u00adments. The \nresults may thus be a bit too optimistic. Anyway, these results are important. The present Yampa API \ncontains a number of pre-composed signal functions, such as accumHoldBy and edgeTag, implemented as primitives \nfor rea\u00adsons of ef.ciency. However, benchmark results 4 to 7 indicate that there is no need for such \nprimitives in YampaG. The GADT-based optimizations make it is possible to simplify both the Yampa imple\u00admentation, \nand, more importantly, the Yampa API, in this respect. The above results are good, but perhaps not wholly \nunexpected: the benchmarks are small and set up in a way that let the opti\u00admizations have maximal impact. \nWhat about the effect on larger application like Space Invaders and MEP (see section 3.3)? Table 4 shows \nthe results. As can be seen, the execution times for Space In\u00advaders are almost exactly the same. This \napplication does quite a bit of .oating-point and vector computations, and a closer inspection reveals \nthat that is where most time is spent. After the basic opti\u00admizations as implemented in YampaS have been \nperformed, there just is not so many more (easy) optimization opportunities. This is a bit disappointing, \nbut at least it suggests that the size and com\u00adplexity of YampaG does not have any major negative impact. \nThe MEP fares better, showing a worthwhile speedup. Of course, one would hope that, given that this is \nan event-centered application and the effort put into optimizing event processing. However, it should \nbe emphasized that the MEP is non-trivial, in\u00advolving many stateful signal functions that cannot be expressed \nin terms of the composable primitives. It is not the case that the MEP trivially reduces to just a single \nor a few signal functions.  6. Related Work Dynamic optimization of domain-speci.c embedded languages \nit not new. This was brie.y discussed in the introduction, one ex\u00adample being Swierstra s and Duponcheel \ns self-analyzing parser combinators [29]. Optimization in FRP or FRP like systems has also been attempted \na number of times, e.g. various implementa\u00adtions of Fran [10]; Pan, a compiled embedded language implemen\u00adtation \n[11]; the highly optimized FranTk implementation, employ\u00ading unsafePerformIO behind the scenes [28]; \nand, of course, the current Yampa implementation [21]. However, most closely related to the optimization \napproach pre\u00adsented here is Hughes work on self-optimizing arrow-based com\u00adbinator libraries [19]. There \nare two parts to Hughes approach. First, a generalized optimization framework for arrows based on arrow \ntransformers is introduced. This allows any arrow instance to be transformed into a self-optimizing version \nutilizing the arrow laws. Moreover, the run-time overhead is small, since the optimiza\u00adtions are carried \nout once and for all up front: the transformed ar\u00adrow is run to produce an optimized untransformed arrow. \nSecond, Hughes makes clever use of (commonly implemented) extensions to the Haskell 98 type system to \nallow optimizing e.g. composi\u00adtion with the identity arrow. Now that GADTs are available, they could \nhave been used instead, allowing more optimizations, but at the cost of being tied to GHC for the time \nbeing. This is no surprise: both Hughes and Nilsson et al. [21, p. 62] note that some form of dependent \ntypes is what really is wanted in this context. The main question then is why the idea of an optimizing \nar\u00adrow transformer was not used for Yampa? That would arguably have been more elegant than optimizing \nthe the individual arrow combinator instances individually. There are two reasons for this choice. First, \noptimizations of event processing is very important but rather Yampa-speci.c. There does not seem to \nbe much point in inventing a Yampa-speci.c arrow transformer for transforming the Yampa arrow. A layered \napproach might have been possible, where the basic Yampa arrow .rst was transformed using Hughes general \noptimizing arrow transformer, and then further transformed by an arrow transformer addressing only event \nprocessing. That does seem rather complicated, however. The second reason is that Yampa needs to consider \noptimization at every time step since Yampa allows systems with dynamic structure to be described. This \nwas discussed in section 3.2, where an example showing how a sig\u00adnal function eventually might become \nsomething simple, like a pure arrow, was given, at which point one would like any optimiza\u00adtions that \nthis enables to be carried out. The overhead of an arrow transformer approach can thus not be reduced \naway once and for all, and the approach simply seemed too costly. 7. Conclusions This paper showed how \nGADTs make possible a number of use\u00adful dynamic optimizations for Yampa, an arrow-based, domain\u00adspeci.c \nlanguage for Functional Reactive Programming. Some of these optimizations are applicable for arrow-based \nlibraries in gen\u00aderal, whereas others are speci.c to Yampa. A set of small bench\u00admarks showed that these \noptimizations can boost the performance signi.cantly in some cases. At the level of of complete applications, \nthe gains ranged from modest but worthwhile to none. However, the optimization possibilities have not \nyet been ex\u00adhausted. In particular, optimization of combinations of serial (>>>) and parallel (first, \n***, . . . ) arrow composition has not yet been attempted. GADTs are expressive enough to cope with such \nexten\u00adsions: the dif.cult part is to manage the combinatorial growth of cases for optimization that need \nto be considered. All in all, despite the lack of major system-wide performance gains, and without taking \nthe possible impact of further optimiza\u00adtions into account, the GADT-based Yampa implementation is still \nan improvement on the old one. The gains at the code fragment level are important as they allow a more \nconcise and principled API, without pre-composed signal functions that are there only for performance \nreasons. The GADT-based implementation is also able to exploit that arrow composition is associative, \nmaking it in\u00adsensitive to how arrow composition is parenthesized. The measure\u00adments thus far suggest \nthat these bene.ts were obtained without an overall negative performance impact that could have been \nthe result of the general increase of the size and complexity of the core of the Yampa implementation. \nOn a general note, GADTs were found to be intuitive to use and pleasingly expressive. Among other applications, \nthey should be quite useful for dynamic optimizations in a wide range of em\u00adbedded domain-speci.c languages. \nAt present, they do, however, have a bit of a bolted on feel. For instance they cannot be used together \nwith Haskell s labeled-.eld notation (compare the stylistic difference between the de.nitions of SF in \nsections 3.2 and 4.2 for an example). Acknowledgments The author would like to thank the anonymous reviewers \nfor their thorough and constructive comments. References [1] Arthur I. Baars and S. Doaitse Swierstra. \nType-safe, self inspecting code. In Proceedings of the 2004 ACM SIGPLAN Haskell Workshop (Haskell 04), \npages 69 79. ACM Press, 2004. [2] G. Berry and G. Gonthier. The Esterel synchronous programming language: \ndesign, semantics, implementation. Science of Computer Programming, 19(2):217 248, 1992. [3] Magnus Carlsson \nand Thomas Hallgren. Fudgets: A graphical user interface in a lazy functional language. In Conference \non Functional Programming Languages and Computer Architecture (FPCA 93), pages 321 330, Copenhagen, Denmark, \nJune 1993. ACM Press. [4] Magnus Carlsson and Thomas Hallgren. Fudgets Purely Functional Processes with \nApplications to Graphical User Interfaces. PhD thesis, Department of Computing Science, Chalmers University \nof Technology, 1998. [5] P. Caspi, D. Pilaud, N. Halbwachs, and J. A. Plaice. LUSTRE: A declarative language \nfor programming synchronous systems. In Proceedings of the 14th ACM Symposium on Principles of Programming \nLanguages, New York, NY, 1987. ACM. [6] Paul Caspi and Marc Pouzet. Lucid Synchrone, a functional extension \nof Lustre. Submitted for publication, 2000. [7] Olaf Chitil, Colin Runciman, and Malcolm Wallace. Freja, \nHat and Hood a comparative evaluation of three systems for tracing and debugging lazy functional programs. \nIn Markus Mohnen and Pieter Koopman, editors, Proceedings of the 12th International Workshop on Implementation \nof Functional Languages (IFL 2000), Aachen, Germany, September 2000, volume 2011 of Lecture Notes in \nComputer Science, pages 176 193. Springer-Verlag, 2001. [8] Antony Courtney and Conal Elliott. Genuinely \nfunctional user interfaces. In Proceedings of the 2001 ACM SIGPLAN Haskell Workshop, Firenze, Italy, \nSeptember 2001. [9] Antony Courtney, Henrik Nilsson, and John Peterson. The Yampa arcade. In Proceedings \nof the 2003 ACM SIGPLAN Haskell Workshop (Haskell 03), pages 7 18, Uppsala, Sweden, August 2003. ACM \nPress. [10] Conal Elliott. Functional implementations of continuous modelled animation. In Proceedings \nof PLILP/ALP 98. Springer-Verlag, 1998. [11] Conal Elliott, Sigbj\u00f8rn Finne, and Oege de Moor. Compiling \nembed\u00added languages. In Semantics, Applications, and Implementation of Program Generation (SAIG 2000), \nvolume 1924 of Lecture Notes in Computer Science, pages 9 27, Montreal, Canada, September 2000. Springer-Verlag. \n[12] Conal Elliott and Paul Hudak. Functional reactive animation. In Proceedings of ICFP 97: International \nConference on Functional Programming, pages 163 173, June 1997. [13] N. Halbwachs, P. Caspi, P. Raymond, \nand D. Pilaud. The synchronous data.ow programming language LUSTRE. Proceedings of the IEEE, 79(9):1305 \n1320, 1991. [14] Thomas A. Henzinger. The theory of hybrid automata. In Proceedings of the 11th Annual \nIEEE Symposium on Logics in Computer Science (LICS 1996), pages 278 292, 1996. [15] Paul Hudak. Modular \ndomain speci.c languages and tools. In Proceedings of Fifth International Conference on Software Reuse, \npages 134 142, June 1998. [16] Paul Hudak, Antony Courtney, Henrik Nilsson, and John Peterson. Arrows, \nrobots, and functional reactive programming. In Johan Jeuring and Simon Peyton Jones, editors, Advanced \nFunctional Programming, 4th International School 2002, volume 2638 of Lecture Notes in Computer Science, \npages 159 187. Springer-Verlag, 2003. [17] John Hughes. The design of a pretty-printing library. In J. \nJeuring and E. Meijer, editors, Advanced Functional Programming, volume 925 of Lecture Notes in Computer \nScience, pages 53 96. Springer Verlag, LNCS 925, 1995. [18] John Hughes. Generalising monads to arrows. \nScience of Computer Programming, 37:67 111, May 2000. [19] John Hughes. Programming with arrows. In Advanced \nFunctional Programming, 2004. To be published by Springer Verlag in their LNCS series. [20] The MathWorks, \nInc. Using Simulink Version 4, June 2001. http://www.mathworks.com [21] Henrik Nilsson, Antony Courtney, \nand John Peterson. Functional reactive programming, continued. In Proceedings of the 2002 ACM SIGPLAN \nHaskell Workshop (Haskell 02), pages 51 64, Pittsburgh, Pennsylvania, USA, October 2002. ACM Press. [22] \nRoss Paterson. A new notation for arrows. In Proceedings of the 2001 ACM SIGPLAN International Conference \non Functional Programming, pages 229 240, Firenze, Italy, September 2001. [23] John Peterson, Greg Hager, \nand Paul Hudak. A language for declarative robotic programming. In Proceedings of IEEE Conference on \nRobotics and Automation, May 1999. [24] John Peterson, Paul Hudak, and Conal Elliott. Lambda in motion: \nControlling robots with Haskell. In Proceedings of PADL 99: 1st International Conference on Practical \nAspects of Declarative Languages, pages 91 105, January 1999. [25] John Peterson, Paul Hudak, Alastair \nReid, and Greg Hager. FVision: A declarative language for visual tracking. In Proceedings of PADL 01: \n3rd International Workshop on Practical Aspects of Declarative Languages, pages 304 321, January 2001. \n[26] Simon Peyton Jones, Geoffrey Washburn, and Stephanie Weirich. Wobbly types: type inference for generalized \nalgebraic data types. Submitted to POPL 05, July 2004. [27] Marc Pouzet, Paul Caspi, Pascal Couq, and \nGr\u00b4egoire Hamon. Lucid Synchrone v2.0 tutorial and reference manual. http://www-spi.lip6.fr/lucid-synchrone/ \nlucid_synchrone_2.0_manual.ps, April 2001. [28] Meurig Sage. FranTk: A declarative GUI system for Haskell. \nIn Proceedings of the ACM SIGPLAN International Conference on Functional Programming (ICFP 2000), September \n2000. [29] S. D. Swierstra and Luc Duponcheel. Deterministic, error-correcting combinator parsers. In \nJohn Launchbury, Erik Meijer, and Tim Sheard, editors, Advanced Functional Programming, volume 1129 of \nLecture Notes in Computer Science, pages 184 207. Springer-Verlag, 1996. [30] Philip Wadler. How to replace \nfailure with a list of successes. In Conference on Functional Programming Languages and Computer Architecture \n(FPCA 85), volume 201 of Lecture Notes in Computer Science, pages 113 128. Springer-Verlag, 1985.  \n \n\t\t\t", "proc_id": "1086365", "abstract": "A limited form of dependent types, called Generalized Algebraic Data Types (GADTs), has recently been added to the list of Haskell extensions supported by the Glasgow Haskell Compiler. Despite not being full-fledged dependent types, GADTs still offer considerably enlarged scope for enforcing important code and data invariants statically. Moreover, GADTs offer the tantalizing possibility of writing more efficient programs since capturing invariants statically through the type system sometimes obviates entire layers of dynamic tests and associated data markup. This paper is a case study on the applications of GADTs in the context of Yampa, a domain-specific language for Functional Reactive Programming in the form of a self-optimizing, arrow-based Haskell combinator library. The paper has two aims. Firstly, to explore what kind of optimizations GADTs make possible in this context. Much of that should also be relevant for other domain-specific embedded language implementations, in particular arrow-based ones. Secondly, as the actual performance impact of the GADT-based optimizations is not obvious, to quantify this impact, both on tailored micro benchmarks, to establish the effectiveness of individual optimizations, and on two fairly large, realistic applications, to gauge the overall impact. The performance gains for the micro benchmarks are substantial. This implies that the Yampa API could be simplified as a number of \"pre-composed\" primitives that were there mainly for performance reasons are no longer needed. As to the applications, a worthwhile performance gain was obtained in one case whereas the performance was more or less unchanged in the other.", "authors": [{"name": "Henrik Nilsson", "author_profile_id": "81100060854", "affiliation": "University of Nottingham", "person_id": "PP39025384", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086374", "year": "2005", "article_id": "1086374", "conference": "ICFP", "title": "Dynamic optimization for functional reactive programming using generalized algebraic data types", "url": "http://dl.acm.org/citation.cfm?id=1086374"}