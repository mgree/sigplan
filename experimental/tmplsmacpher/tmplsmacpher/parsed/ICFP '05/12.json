{"article_publication_date": "09-12-2005", "fulltext": "\n Simple, partial type-inference for System F based on type-containment DidierR\u00b4emy INRIA-Rocquencourt \nhttp://pauillac.inria.fr/ remy Abstract We explore partial type-inference for System F based on type-containment.We \nconsiderboth cases ofa purely func\u00adtional semantics and a call-by-value stateful semantics. To enabletype-inference, \nwe require higher-rankpolymorphism tobe user-speci.ed viatype annotations on source terms. We allow implicit \npredicativetype-containment and explicit impredicativetype-instantiation.We obtain a core language that \nisboth as expressive as System F and conservative over ML. Its type system has a simple logical speci.cation \nand a partial type-reconstruction algorithm that are both very close to the ones for ML.We then propose \na surface language where some annotations maybe omitted and rebuiltby some algorithmically de.ned but \nlogically incomplete elaboration mechanism. Categories and Subject Descriptors D.3.3[Language Constructs \nand Features]: Polymorphism General Terms Design, Reliability, Languages, Theory, Veri.cation Keywords \nType Inference, System F, Polymorphism, Type Reconstruction, Type Containment, Elaboration Introduction \nML-stylepolymorphism has often been considered as a lo\u00adcal optimal, o.ering one of the best compromises \nbetween simplicity and expressiveness. It is at the heart of two suc\u00adcessful families of languages, Haskell \nand ML. In the last two decades, both Haskell and ML type systems evolved in sur\u00adprisingly di.erent directions \nwhile retaining their essence. However, programmers are starting to feel restricted as programs become \nbigger and advanced programming pat\u00adterns are used. The demand for .rst-class polymorphism hasbeen increasing. \nFirst-classpolymorphism is not only sometimes useful it is quickly becoming unavoidable! The reference \ncalculus for .rst-classpolymorphism is Sys\u00adtem F. Unfortunately, full type inference is undecidable in \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copiesbear this notice and the full citation on the .rst page.To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n05 September 26-28, 2005 Tallinn, Estonia. Copyright c . 2005 ACM 1-59593-064-7/05/0009. . . $5.00. \nSystem F [Wel94]. Adding .rst-classpolymorphism to ML should not sacri.ce type inference, one of the \nattractive as\u00adpects of ML. One approach to keeping type inference is to reduce it to second-order uni.cation \n[Pfe88]. However, even so only provides with a semi-algorithm. Moreover, unintu\u00aditive explicit marks \nfor type abstractions and applications are still required. An alternate approach is to explicitly annotate \nsource terms with their types, as in Church s presentation of System F. This turns type inference into \na simple type checking algorithm. However, type annotations are quickly a burden to write and often become \nobtrusive, even for simple ML programs. Of course, one wishes to have simultaneously the expres\u00adsiveness \nof System F, the decidability of its explicitly typed version, and the convenience of ML-like type inference, \nif not full type inference. The idea is thus to bring System F and ML closer. This can be approached \nfrom two oppo\u00adsite directions. Starting with explicitly typed System F, one mayperform some partialtype \ninference so as to alleviate the need for some (but not all) type annotations. Or con\u00adversely, starting \nwith ML, one may allow some explicit type annotations so as to reach most or all of System F programs. \nThe .rst approachis known as localtype inference [PT00, OZZ01]. By de.nition, these solutions cover all \nof System F. However, they are not yet conservative over ML, that is, there remain ML programs that fail \nto type without annota\u00adtions. In fact, local type inference allows getting rid of many silly and annoying \ntype annotations, but fails to make all of them redundant, so some unintuitive annotations remain needed \n[HP99]. Here, we focus on the second approach. By construction, it leads to conservative extensions of \nML. The simplest method to extend ML with .rst-class poly\u00admorphism is to use boxed polymorphism. The \nidea is to em\u00adbed .rst-classpolymorphictypesinto simpletypes using ex\u00adplicit injection and projection \nfunctions. These coercions can be automatically attached to data constructors via algebraic data type \nde.nitions, which makes them simple and some\u00adtimes transparent to the user. This solution was originally \nproposed for existential types [LO94] and then applied to universal types [R\u00b4em94]. Boxed polymorphism \nis extremely simple,because ML never sees second-ordertypes. The draw\u00adback is that boxes are rigid: they \nneed to be declared and explicitly inserted.Furthermore, the history of the construc\u00adtionofapolymorphic \nvalueis recorded in itstype, as the stacking of boxes. Odersky and La\u00a8ufer [OL96] later observed that \nsome types need not be boxed. They proposed a type system where the user can write .z : . a.a . a. t, \nmaking the .-bound variable z available within t at type . a.a . a, as if it were let-bound. Another \nkey feature in the work of Odersky and La\u00a8ufer is to generalize the instance relation of ML, allowing \ninstantiation of toplevel quanti.ers as in ML but also of inner quanti.ers by recursively traversing \narrow types, co-variantly on the right-hand side to instan\u00adtiate their codomains and contra-variantly \non the left-hand side to generalize their domain. This allows to keep types aspolymorphic aspossible \nand only instantiate themby need. For instance, .z : . a.a . a..y. z may be typed as . ..(. a.a . a) \n. . . (. a.a . a) but still used with type . ..(. a.a . a) . . . (. \u00df.(\u00df . \u00df). (\u00df . \u00df)). There is a restriction, \nhowever, that is essential to allow simple type-inference: when a type variable such as a above is instantiated, \nit can only be replaced with a monotype t. This built-in restriction to predicativepolymorphism is ac\u00adtually \nquite severe, as we shall see. Fortunately, it can be circumvented by keeping boxed types, which allow \nmore ex\u00adplicit but impredicative type instantiation, coexisting with implicit predicative polymorphism \n[OL96]. Peyton-Jones et al. have recently extended Odersky and La\u00a8ufer s proposal to propagate type annotations \nin source programs [PVWS05a], so that an external type annotation, such as .z. t :(. a.a . a) . t, behaves \nas if the argu\u00adment z was also annotated with type . a.a . a. Their type system, which we hereafter refer \nto as PVWS, hasbeen in\u00adgeniously tuned. It is also quite involved. The authors claim that: adding higher-rank \npolymorphism to ML is so simple that every implementation of ML-style type inference should have it.They \narguethatveryfewchangesneedtobemadeto adapt an implementation of MLtype inference to predicative higher-rankpolymorphism. \nWhile we accept their claim, we .nd their implementation-based demonstration unconvinc\u00ading and their \nspeci.cation too algorithmic and unintuitive. Indeed, PVWS mixes several features that are often con\u00adsidered \ncon.icting: (1) ML-like type inference; (2) a form of contravariant subsumption, and (3) propagation \nof user\u00adprovided annotations. Type systems that mix (1) and (2) usually exploit explicit constraints \nto keep principal types, which PVWS does not; (3) enforces a strict order in which typecheckingmustbeperformed, \nwhichusually standin the way when inferring principal types. Such an unusual combi\u00adnation of features \ndoes not imply misconception, of course and indeed, PVWS has been carefully designed. However, it should \nthen be studied with a lot of attention [PVWS05b]. Contributions In this paper, we investigate partial \ntype inference based on type containment and .rst-ordertyping constraints. Our pri\u00admary goal is to bring \nfurther evidence to Peyton-Jones and Shields claim and thus to provide more insight to PVWS. Another \ngoal is also to explore the design space and, in par\u00adticular, how far one can go within an ML-like type-inference \nframework. We have at least four di.erent contributions: A preliminary, independent result is the soundness \nof a variant of F. for a call-by-value stateful semantics. Our main contri\u00adbution is the core language \nFML that brings down System F to the level of ML it has a simple logical speci.cation and a sound and \ncomplete .rst-order type inference algorithm. A secondary contribution is the surface language and F? \nML our proposal to split partial type inference in F? into a ML composition oftwo separate orthogonal \nphases: an algorith\u00admic elaboration process into FML, followed by type inference into FML. Our approach \nleaves room for variations in the de.nition oftype-containment relations, which controls the expressiveness \nof the core language, and in the elaboration process. A side contribution is also the exploration of \nthe design space: our two-phase decomposition has advantages but also some limitations. 1. System F(.)and \nsome instances System F is the canonical system for .-calculus with second\u00adorderpolymorphism. However, \nit maybe presented intwo ways. In Church s view, terms are explicitly typed, including type annotations \non .-abstractions, and type applications. Therefore, type-checking only needs to verify that user\u00adprovided \ntype information is correct with respect to the typing rules. On the opposite, in Curry s view, terms \nare untyped and type-checking must infer the types that should be assigned to formal parameters in .-abstractions \nas well as the places where type abstractions and type applications should be inserted. The two views \ncan be reconciled by considering type inference for terms in Curry s style as elaborating an explicitly \ntyped term in Church s style. 1.1 Terms and types We assume given a .nite or denumerable collection of \ncon\u00adstants (ranged over by c) and a denumerable collection of variables (ranged over by z and y). Constants \nand variables form identi.ers, which we write x. Terms, ranged over by t, are those of the untyped .-calculus, \ni.e., identi.ers x, ab\u00ad ' stractions .z. t, or applications tt. Application has higher ' prioritythan \nabstraction and is left-associative i.e. .z..z. t ''' '''' ttstands for .z. (.z. ((tt)t)). We assume \na denumerable collection of type variables (ranged over by a). Types, ranged over by s, are type vari\u00adables, \narrow types s1 . s2, andpolymorphic types . a.s. The . symbol acts as a binder for a in s. Free type \nvari\u00adables in a type s, which we write ftv(s), are de.ned acc\u00adcordingly. We always consider types equal \nmodulo renam\u00ading ofbound-type variables. The scope of .-quanti.cation extends to the right as much as \npossible and arrows are right-associative. That is, . a.s1 . s2 . s3 stands for . a. (s1 . (s2 . s3)). \nA type variable a occurs positively in a. If a occurs positively (resp. negatively) in s, then it occurs \npositively (resp. negatively) in s' . s and . \u00df.s when \u00df is distinct from a, and negatively (resp. positively) \nin s . s'. We write ftv+(s)(resp.ftv-(s)) the sets of type variables that occurpositively (resp. negatively) \nin s.We write ftv (s)the set ftv+(s)\\ ftv-(s), which we call non-negative free type variables. A relation \nR on types is structural if it is re.exive, transitive and closed under the following properties: if \ns1 R s2, then . a.s1 R. a.s2 (S-All), s2 . s R s1 . s (S-Contra)and s . s1 R s . s2 (S-Cov). A congruence \nis a structural equivalence relation. Let be the smallest congruence that allows commu\u00adtation of adjacent \nbinders, i.e. . a. . \u00df.s . \u00df.. a.s, and removal of redundant binders, i.e. . a.s s whenever a is not \nfree in s. Most relations on types we will consider will be subrelations of . Therefore, we could usually \ntreat types equal modulo . However, we prefer not to do so and treat explicitly when needed. A canonical \nform for \u00adequivalence is one without redundant quanti.ers and where adjacent quanti.ers are listed in \norder of apparition. Canon\u00adical forms are unique (up to renaming ofbound-type vari\u00adables, but we treat \nsuch types as equal). We write canon(s) for the canonical form of s. Subterms of types in canonical forms \nare also in canonical form. Figure 1. Typing rules for F( ) Figure 2. Containment rules for =. Sub Trans \n' ''' \u00afa.s) \u00df/. ftv(. \u00afs ss s . a.s\u00af . \u00df.s\u00af[\u00afs/a\u00af] s s '' Arrow All '' ' s1 s1 s2 s2 s s '' ' s1 . s2 \ns1 . s2 . a.s . a.s Distrib . a.s . s ' (. a.s).. a.s ' f=F. However, both languages have the same set \nof typable terms. About Rule Distrib It is actuallypossible to limit uses of Rule Distrib to cases where \na/. ftv(s) and further\u00admore a . ftv-(s ' ) without a.ecting the type-containment =. relation . We refer \nto these two limited use of Dis\u00adtrib as Distrib-Right and Distrib-Right-Neg, respectively. Moreover, \nRule Distrib-Right is reversible, i.e. . a.s ' . a.s . s ' whenever a/. ftv(s). So we actually have s \n. . a.s ' =. . a.s . s ' whenever a/. ftv(s). Types can be put in prenex-form byrepeatedly applying the \nrewriting rule s .. a.s ' . . a.s . s ' when a/. ftv(s) in any type context (the side-condition can always \nbe satis.ed modulo appropriate renaming). We write prf(s)for the prenex-form equivalent to s, whichis \nunique up to .We also write prf(G) for prf(\u00b7)composed with the mapping G. For any judgment G fF(=.) t \n: s, the judgment prf(G) fF(=.) t : prf(s) also holds. Moreover, there exists a derivation of this judgment \nthat uses only types in prenex-form in typing judgments. However, subderivations of =.-type-containment \njudgments may require the use of types not in prenex-form, e.g. in ap\u00adplications of Rule Sub. 1.3 Expressiveness \nof F(=.) Coercion functions allow the extrusion of quanti.ers and deep type-specialization. This is convenient \nin a type in\u00adference context because instantiation may be performed a posteriori. For instance, the term \ntK equal to .z..y. y can be assigned either type . a.a .. \u00df.\u00df . \u00df, giv\u00ading the sub-expression .y. y a \npolymorphic type, or type . a,\u00df.a . \u00df . \u00df. Coercions makebothtypes inter-convertible. Here, either one \nis actually a principal type of tK in F(=.), i.e. all other types may be obtained by type\u00adcontainment \n[Mit88]. By comparison, there is no type of tK in System F of which all other types would be =F-instances. \nFor this reason, it has been suggested that F(=.)would be a better candidate for type inference [Mit88] \nbefore full type inference was shown to be undecidable. The example above shows that some terms have \nmore types in F(=.). One may also expect more terms to have types in F(=.), although we are not aware \nof any term of F(=.) that has been proved not to be in F. However, the additional expressivepower,ifany,does \nnot seemtobevery signi.cant. 1.4 Soundness of F( ) A typing relation G fX t : s and a reduction relation \n-. on expressions enjoythe subject reduction propertyif reduction preserves typing. That is, if G fX \nt : s and t -. t ' , then G fX t ' : s. Of course, this result depends on the Var Inst x : s . GG f t \n: s ' s ' s G f x : s G f t : s Gen Fun G f t : s a/. ftv(G) G,z : s2 f t : s1 G f t : . a.s G f .z. \nt : s2 . s1 App G f t1 : s2 . s1 G f t2 : s2 G f t1 t2 : s1 We write a\u00affor tuples of variables a1,..an \n(and more generally e\u00affor a sequence of elements of the syntactic class e)and . \u00af a.s for . a1. .. . \nan.s.We write t for types that do not contain any universal quanti.ers, called monotypes; we write . \nfor types that do not have any outermost universal quanti.ers. We write s[\u00afs/a\u00af] for the simultaneous \ncapture\u00adfree substitution of types s\u00affor variables a\u00afin s. 1.2 The generic system F( ) Let F( ) be the \ntype system for the .-calculus de.ned in Figure 1 and parameterized by a binary relation on types called \ntype containment. A typing context G is a .nite mapping from program identi.ers to types. The empty environment \nis written \u00d8. We write G,x : s for the environment that binds x to s and other program variables as G. \nWe write x : s . G to mean that G maps x to s. We write ftv(G) for Sftv(s). Typing judgments are triples \nx:s.G of the form G f t : s. We write t . F( ) if (and only if) there exists a typing environment G and \na type s such that G f t : s. We may write G fX t : s when we need to remind that typing judgments refer \nto the type system X. Rules Var, App, Fun are the same as in the simply typed .-calculus, except that \ntypes mayhere have quanti.ers. Rule Gen allows generalizingthetypeofan expressionoveratype variable that \ndoes not appear in the environment. The really interesting Rule is Inst, which allows replacing the type \nof an expression with one of its instances, where the notion of instance is determined by the relation \n. System F is obtainedbytaking =F for , which is de.ned as the smallest relation that satis.es the unique \naxiom Sub of Figure 2. (We use to range over arbitrary relations, while symbols =, =F , =. , etc. denotes \nspeci.c relations.) As early as 1984, Mitchell proposed to replace the instance relation =F by a more \ngeneral relation =., called type con\u00adtainment, which is de.ned as the smallest relation satisfying the \nrulesof Figure2 [Mit88].He also showed thata term t is typable in F(=.)if and only if there exists a \nterm t ' typable in F that is .-equal to t. That is, F(=.)is the typed closure F. of F by .-conversion, \nhence F(=.) is usually called . It follows that the relation s =. s ' holds if and only if there exists \na term t of type s . s ' in F that .-reduces to the identity [Mit88]. Such terms are called coercion \nfunctions. We write = for the kernel of . Note that =. is a structural relation. It is also a subrelation \nof . Conversely, =F is not structural and it is not a subrelation of . We write =F for the composition \n=F . , which is also equal to .=F . (but not to .=F). The relation =F is still not structural, but it \nis a subrelation of . The language F(=F ) is not System F per se: its typing relation f=F is larger than \n particular choice of X and of the reduction relation -.. In the following, we will consider several \ntype systems F() where is a subrelation of =. (or =.-). Type soundness for all of these systems will \nthen follow from type soundness for F(=.). For an e.ect-free semantics and in the absence of con\u00adstants, \nthe relation -. is the transitive closure of the one\u00adstep reduction C[(.z. t1) t2] -. C[t1[t2/z]] for \narbitrary contexts C. Subject reduction is known to hold in System F. It then easily follows from the \nde.nition of F(=.)in terms of F that subject reduction also holds in F(=.). Subject reduction is only \nhalfway key totype soundness. It remains to verify that any well-typed program that is not a value canbe \nfurther evaluated, that is, the progress Lemma. Progress is trivial in the absence of constants. Otherwise, \none can show thatboth subject reduction and progress lemmas hold under some hypotheses ensuring subject \nreduction and progress for reduction involving constants.  1.5 A stateful sound variant of F(=.) For \nsake of simplicity, we focus on the treatment of the store and references rather than general side-e.ecting \noperations. It is well-known that combiningpolymorphism and mutable store requires some care. Because \nSystem F(=.) is highly polymorphic, we must be even more careful. In this section we de.ne a variant \nFv(=.-) of F(=.) that is safe when equipped with a call-by-value semantics with side e.ects. One way \nto ensure type soundness in the presence of side e.ects is to give type abstraction a call-by-name semantics. \nThat is,apolymorphic expressionmustbe reevaluated every time it is specialized. In an explicitly typed \ncalculus, poly\u00admorphism is introduced by a type abstraction .a.t, which freezing the evaluation of t \nand every use of polymorphism is obtained by a type application t[s], which evaluates the whole expression \nt (after occurrences of ahavebeen replaced by s). This call-by-name semantics ofpolymorphism a.ects all \nexpressions, whether or not their evaluation mayproduce side-e.ects.(Even in the absence of side e.ects, \nthis modi\u00ad.es sharing of expressions and may change the complexity of computation signi.cantly.) A variant \nof this solution is to restrict generalization to values, whichis known as the value\u00adrestriction [Wri95]. \nMore precisely, we may de.ne a class of non-expansive expressions that includes at least values and variables, \nbut may also include some other expressions a formal de.nition is givenbelow. This solution is now in \nuse in most implementations of ML, although manyof them still use a more restrictive de.nition of non-expansiveness \nfor no good reason. We shall thus, as in ML, restrict Rule Gen so that it only applies when the expression \nt is non-expansive. How\u00adever, this restriction alone is not sound in F(=.), because it canbebypassedbytype-containment.For \nexample, consider the expression tm de.ned as .y. tr where tr is ref (.z. z). As in ML, tm canbe assignedtype \n. a.\u00df . ref (a . a). In\u00adtuitively, this is correct since then an application tm t0 will then have the \nmonomorphic type ref (a . a), or more pre\u00adcisely, will have type ref (t . t)for any type t. The key is \nthat tm should not have type \u00df .. a. ref (a . a), since otherwise tm t0 would return a reference cell \nthat could be treated asapolymorphic value. Thistype seems tobe dis\u00adallowed, since the value restriction \nprohibits generalization of the type of tr. Unfortunately, type containment allows replacing the correct \ntype . a.\u00df . ref (a . a) of tm with the unsound type \u00df .. a. ref (a . a). The problem can be traced back \nto rule Distrib-Right-Neg, which allows bypassing the value restriction, and is thus unsound in the presence \nof side-e.ects. The solution is thus both to restrict rule Gen to non\u00adexpansive expressions and invalidate \nRule Distrib-Right-Neg. Instances or Rule Distrib that are derivable from other rules are both harmless \nand useful. This is the case in particular when type variable a only occurs positively in ftv(s). We \nrefer to this special instance of Distrib as Distrib-Right-Pos. It is actually remarkable, that Rule \nDistrib-Right-Pos, which is valid in Fv(=.-)leads to the enhanced value restriction as de.ned by Garrigue \n[Gar04], rather than the standard value-restriction [Wri95]. Store semantics We assume given a denumerable \ncollection of memory loca\u00adtions m .M and restrict constants to the following cases: - c ::= c + | c Constants \nc + ::= m Constructors c - ::=(ref \u00b7)| (!\u00b7)| (\u00b7 := \u00b7) Destructors Each constant c comes with a .xed \narity a(c). The arity of memory locations is 0. The arities of!\u00b7, ref \u00b7, and \u00b7 := \u00b7 are, respectively \n0, 1, and 2, as suggested by the notation. By lack of space, we cannot describe the call-by-value semantics \nwith store. We refer the reader to an extended version of this paper [R\u00b4em05] or to a detailed presentation \nof type-constraints [PR05]. Typing rules with side e.ects Non-expansive expressions u .U are de.ned as \nfollows: u ::= z | .z. t | (.z1 . ...zk . u)u1 ... uk | c + u1 ... uk,k = a(c) | c - u1 ... uk, k<a(c) \n By construction, reduction of non-expansive expressions cannot extend the store. Remark that non-expansive \nexpressions may contain free variables. Substituting non\u00adexpansive expressions for free variables in \nnon-expansive expressions yields again non-expansive expressions. We introduce a new invariant type symbol \nref of arity 1 to classify expressions that evaluate to store locations. Invariance mean that an occurrence \nof a variable in a type ref s isbothpositive and negative and thus never in ftv (s). Moreover, a structural \nrelation must now validate the following rule: Ref '' ss ss ref s ref s ' We write =.- for the smallest \nrelation that satis.es rules Sub, Trans, Arrow, All of Figure 2 and rule Ref. We also restrict generalization \nto non-expansive expres\u00adsions or to non-negative type variables. Genv G f t : s a/. ftv(G) t .U . a . \nftv (s) G f t : . a.s Let Fv( ) be the type system F( ) where Rule Gen has been replacedbyRuleGenv.Types \nfor constants are de.ned by an initial environment G0 that contains exactly: ref \u00b7 : . a. a . ref a !\u00b7 \n: . a.ref a . a \u00b7 := \u00b7 : . a.ref a . a . a With these restrictions, we still have G0 fFv(=.-) tm : . \na. unit . ref (a . a), but not G0 fFv(=.-) tm : unit . . a. ref (a . a)any longer. Theorem 1. The language \nFv(=.-)with references is sound. By lackof space, we refer to [R\u00b4em05] for a more formal state\u00adment of \nthis result and its proof. Type soundness is shown ina standard way by combinationof subject reduction \nand progress lemmas.  1.6 Predicative fragments F(=F p) and F(=.p) The predicative fragment of System \nF is the system F(=F p) obtained by restricting the instance relation =F so that only monotypes can be \nsubstituted for type variables1. That is, =F p is the smallest relation satisfying the following rule: \nSubp \u00afa.s) \u00df/. ftv(. \u00af. \u00af\u00df.s[\u00afa] a.s . \u00aft/\u00af The predicative fragment of F(=.) can be de.ned either (1) \nas the typed-closure of F(=F p)by .-conversion, or (2) as F(=.p)where=.p is the smallest relation satisfying \nall rules of Figure 2 except Rule Sub, which is replaced by Rule Subp. Fortunately, de.nitions (1) and \n(2) are equivalent. The type system F(=.p)is safe, as it is a subset of F(=.). This does not imply subject \nreduction. However, one may show independently that subject reduction holds in both F(=F p)and in F(=.p). \nReplacing Distrib by Distrib-Right in the de.nition of =.p does not change the relation itself. This \nis not a con\u00adsequence of the similar result for the impredicative relation =. . Indeed, further replacing \nDistrib-Right by Distrib\u00adRight-Neg would lead to a weaker relation than =.p. Hence, in the case of a \ncall-by-value stateful semantics the language Fv(=.p -)could be safely enriched with a version of Distrib \nthat requires a in ftv (s ' )\\ ftv(s). We refer to this Rule as Distrib-Right-Pos.  1.7 Expressiveness \nof the predicative fragments The restriction of Systems F or F(=.) to their predicative fragments comes \nwith a signi.cant loss of expressiveness. It means thatpolymorphism is limited to simple types. That \nis, programs can manipulate values of several types that di.er in their monomorphic parts but must have \nthe same polymorphic shape. For instance, int . int and bool . bool can be two specializations of a predicative \ntype, but (. a.a . a). (. a.a . a)and int . int cannot. The restriction of rule Sub to rule Subp suggests \na large family of counter-examples. For instance, a polymorphic function of type . a.s . s ' can onlybe \nused attypes s[t/a] where t is a monotype. Thus, one may need as many copies of the function as there \nare di.erent shapes s at which the function needs to be used. As a particular case, the apply function \n.f..z. fz does not have a most general type, but as many types as there are polymorphic shapes for the \ntype of the function f. This is a general situation that also applies to iterators such as iter, map, \nfold, etc. Unsurprisingly, the well-known encoding of existential types with universal types in System \nF is also severely lim\u00adited in the predicative fragments. A value v of an existential type .a.s canbe \nencoded asa function . \u00df.(. a.s.\u00df). \u00df. 1 One mayalso considera strati.cationof predicative fragments \nsee [Lei91] Figure 3. Type containment rules for =.p - Refl a a Arrow s ' 1 s1 s1 . s2 s2 s ' 2 s ' 1 \n. s ' 2 All-I s s ' a /. ftv(s) All-E s[t/a] . s . a.s ' . a.s . The use of value v under the name \nx inside some term t is encoded as the application of v to .x . t. This application can be typed in F, \ngiving x thepolymorphic type . a.s . s ' s ' and instantiating \u00df with . In the predicative fragment, \nhowever, s ' must be a monotype. Hence, the restriction of existential types to the predicative fragment \nonly allows ex\u00adpressions that manipulate (encoded) existential types to re\u00adturn monotypes. Moreover, \nthe type hidden by the abstrac\u00adtion may only be a monotype. In explicit System F, the expression packt \nasz : . \u00df.s, where s[s ' /\u00df]is the type of t, is encoded as .a..f : . \u00df.s. f s ' t where s ' is the abstract \npart of the type. Hence, s ' must be a monotype t in F(=F p). As a corollary, encodings of objects [BCP99] \ndo not work well in the predicative fragments: the object state cannotbe hidden any longer as soon as \nit references an object. Indeed, we may exhibit terms than are typable in F but not in the predicative \nfragment F(=F p). One such term (.z. zy z)(.z. zy z) is due to Pawel Urzyczyn, according to Leivant [Lei91]. \nStrati.ed versions of System F are more expressive than System F(=F p)[Lei91]. However, they would not \nease type inference. 2. Type inference in the language FML Full type inference is undecidable in both \nSystem F [Wel94] and F(=.)[Wel96] thetype-containment relationis it\u00ad =. self undecidable [TU02, Wel95]. \nTo the best of our knowl\u00adedge it is not known whether type inference is decidable for the predicative \nfragments. However, Harper and Pfenning remarked that the reduction of second-order uni.cation to type \ninference for System F with explicit placeholders for type abstractions and applications also applies \nto the pred\u00adicative fragment [Pfe88]. Still, we do not know whether full type inference for F(=.p)is \ndecidable, even though predica\u00adtive type containment =.p is itself decidable [PVWS05b]. We also consider \nthe restriction =.p - of =.p obtained by removing Rule Distrib from the de.nition of =p. . That =.- is, \nwe de.ne p as the smallest relation that satis.es rules Subp, Trans, Arrow and All. A consequence of \nthe removal of rule Distrib is that =.p - has a very simple syntax-directed presentation given by rules \nof Figure 3. Lemma 1. =.p - is also the smallest relation that satis.es the rules of Figure 3. Lemma \n2 (Stability of =p.- by substitution). If s =p.- s ' then s[\u00aft/a\u00af]=.p - s ' [\u00aft/a\u00af]. Actually, the two \nrelations =.p and =.p - coincide on types in prenex form, as shown by the following lemma due to Peyton-Jones, \nVytiniotis, Weirich and Shields [PVWS05b]. s ' Lemma 3 (Peyton-Jones et al.). s =.p if and only if prf(s)=.p \n- prf(s ' ). This provides us with an algorithm for testing =.p by .rst projectingbothtypes to their \nprenex forms and then test\u00ading for =.p - (or by deriving a direct algorithm from this composition [PVWS05b]. \nAlthough the restriction =.p - of =.p is primarily introduced to ensure soundness in Fv(=.p -), we may \nalso consider the language F(=.p -)in the absence of side-e.ects. One advantageis thattype inference \nfor F(=.p -) is closer to type inference for ML, especially in its treatment of generalization. Moreover, \nworking with =.p - is slightly more convenient than =.p for type inference. We start with this simpler \ncase and will consider F(=p.) and Fv(=p.-) in sections 2.5 and 2.6. 2.1 Terms with type annotations: \nFML() We leave the relation a parameter of FML() so asto share some of the developments between =.p - \nand FML(=.p). However, theintentionis that thetype-containment relation be predicative, i.e. a subrelation \nof FML(=.p). Thus, thetype-containment relations thatwe shall con\u00adsider are decidable, and so is -constraint \nresolution, as we shall see below. However, this is not su.cient to per\u00adform type inference. In order \nto avoid guessing polymorphic types, we now extend terms with type annotations. An an\u00adnotation is a type \nscheme s whose free variables a\u00afare locally bound; we write . \u00af a.s. While s represents the explicit \n(usu\u00adally second-order) type information, the existentially bound variables a\u00afrepresent the implicit \ntype information, which will be inferred. That is, type annotations are partial, which is important to \nleave room for inference. As a particular case, the trivial annotations . a.a let us infer monomorphic \ntypes, as in ML.We require thattype annotationsbe closed. This is only for the sake of simplicity. One \ncould easily al\u00adlow type annotations to have free type variables, provided these are explicitly bound \nsomewhere in the program. This mechanism, sometimes known as scopedtype variables, is dis\u00adcussed by Peyton \nJones and Shields [PS03] and by Pottier and R\u00b4emy [PR]. We let . range over type annotations. The syntax \nof FML( )is as follows: t ::= x | .z. t | t1 (t2 : .)| let z =(t1 : .)in t2 As usual, expressions maybe \nidenti.ers x, abstractions .z. t, or applications t1 (t2 : .). However, arguments of applica\u00adtions must \nnow always be explicitly annotated (although annotations may be trivial). The intuition for annotating \nthe argument of applications is to avoid guessing the type of the argument. Indeed, knowing the type \nof the result of an application does not tell anything about the type of its argument. We also allow \nlet-bindings let z =(t1 : .) in t2, which mean (.z. t2)(t1 : .), but which will be typed in a special \nway, as in ML. As for applications, an annotation is required for the expected type of t1, since it cannot \nbe deduced from the expected type of the whole expression.  2.2 Typing rules Typing rules for FML( )are \ndescribed in Figure 4. They use judgments of the formG f t : s as for F( ). These judgments should be \nread as checking judgments, where G, t and s are given. Rules Var, Inst, Gen, and Fun are all taken from \nF( ). However, as mentioned above, the intention is that in the right premise of Rule Inst be chosen \nas an instance of =. p, so as to ensure predicativity. That is, thepolymorphic structure ofpolymorphic \ntypes can onlybe instantiatedby monotypes. Note that, as opposed to ML, rule Fun allows its argument \nto be polymorphic. Despite the appearance, Figure 4. Typing rules for FML() Var Inst Gen Fun AppA G f \nt1 : s2[\u00aft/\u00af\u00df]. s1 G f t2 : s2[\u00aft/\u00af\u00df] G f t1 (t2 : . \u00af: s1 \u00df.s2) LetA G f t1 : . a.s\u00af1[\u00aft/\u00afG,z : . \u00aft/\u00df\u00af]f \nt2 : s2 \u00df] a.s1[\u00af (t1 : . \u00af G f let z = \u00df.s1)in t2 : s2 this does not imply guessing polymorphism, as \nlong as the expected type s2 . s1 is given. Rule AppA (the subscript is to remind the annotation ) is \nnot quite the one of F( ). We must of course take the annotation of the argument into account: the type \nof t2, which is also the domain of the type of t1, must be an instance of the annotation. The annotation \navoids guessing the polymorphic parts of the expected type of t1 and t2 that cannot be deduced from the \nexpected type of the application. When the annotation is the trivial one, s2[\u00aft/a\u00af] must be a monotype \nt2, to be guessed not a problem, since it is a monotype. More generally, only monotypes t\u00afneed to be \nguessed in applications. Rule LetA is taken from ML, except that the annotation on t1 is used to build \nthe expected type of t1, up to .rst-order instantiation, much as in Rule AppA. Expressiveness. Apart \nfrom annotations, the language FML( )is as expressive asF( )for any relation .Formally, let the type \nerasure of a term t be the .-term obtained by dropping all type annotations and replacing all let-bindings \nlet z = t1 in t2 by(.z. t2)t1. Then, we have t . F( )if and only if there exists an expression t ' . \nFML( )whose erasure is t. Syntactic sugar. While annotations are always required in applications and \nlet bindings, the trivial annotations may be used. This is not exactly an absence of annotation, since \nthe annotation is mandatory and a trivial annotation always stands for a monomorphic type. In particular, \nan expression where all annotations are trivial will be typed as in ML. For convenience, we may allow \nlet z = t1 in t2 and t1 t2 as syntactic sugar for let z =(t1 : . a.a) in t2 and t1 (t2 : . a.a), respectively. \nThe language FML( ) is a conservative extension to ML. That is, ifGis an ML environment and t is an ML \nexpression, i.e. both without anynon trivial annotation, thenG fML t : t if and only if G fFML(.) t : \nt. The language does not allow arbitrary expressions to carry type annotations. However, we can de.ne \n(t : .) as syntactic sugar for let z =(t : .)in z. Example. The expression .z. z is well-typed, and can \nbe given type t . t, for any type t, as in ML. It may also be given types . a.a . a,(. a.a). (. a.a), \n. \u00df.((. a.a). \u00df). ((. a.a). \u00df), etc. none of whichis more general than the other in FML(=.p -). In F(=.p), \nthe .rst one isbetter than the second-one; (In F(=.), the .rst-one is better than all other ones on this \nparticular example, but this is accidental.) Hence FML(=.p -)does not have principal types in the usual \nsense.However, aswe shall seebelow,it hasa principaltype for any givenpolymorphic shape. Figure 5. Syntax-directed \ntyping rules FA for FML(=.p -) Var-Inst-Rho s =.- x : s . G p . Gen Fun G f x : . AppA-Rho G f t1 : s2[\u00aft/\u00df\u00af]. \n.1 G f t2 : s2[\u00aft/\u00df\u00af] G f t1 (t2 : . \u00af: .1 \u00df.s2) LetA-Gen-Rho G f t1 : s1[\u00aft/\u00df\u00af] G,z : .\\G.s1[\u00aft/\u00df\u00af]f \nt2 : .2 G f let z = \u00df.s1)in t2 : .2 (t1 : . \u00af As in ML, the expression .z. zz does not have any mono\u00adtype \nt. However, we may explicitly provide the information that we expect a type of the form (. a.a . a) . \nt . t for some t. Then, the expression is well-typed (and a best type of that shape may be inferred). \nWe have G f.- FML(=p ) .z. zz : . \u00df.(. a.a . a). \u00df . \u00df. Typing problems. Let . range over .rst-order \nsubstitu\u00adtions, simply called substitutions for short, which map type variables to monotypes. We may \ndistinguish four di.erent problems: 1. Typability: Given a term t,do there exista contextG and a type \ns such that G f t : s? 2. Typing inference: Given a term t, what are the pairs of a context G and a \ntype s such that G f t : s? 3. Type inference: Given a term t and a context G, what are the types s \nand substitutions . such that G. f t : s? 4. Type checking: Given a term t, a context G, and a type \ns, what are the substitutions . such that G. f t : s.?  Note that our de.nition of typechecking still \nallows .rst\u00adorder inference, which is somehow non standard. Neither typing inference, nor type inference \nproblems have principal solutions in general in FML(=p.), nor in FML(=p.-). However, type checking problems \ndo.  2.3 Syntax-directed typing rules FA As usual, typing judgments need not use all the .exibility \nallowed by the typing rules. That is, typing derivations can be rearranged to form derivations that follow \ncertain patterns. In particular, derivations that are driven by the syntax of the conclusion, called \nsyntax-directed, are the key to type checking and type inference. Figure 5 presents an equivalent set \nof rules for deriving typing judgments in FML(=.p -).We write .\\G.s for thetype . (ftv(s)\\ ftv(G)).s. \nRules Gen and Fun are unchanged. All syntax-directed rules but Gen assign .-types rather that types to \nterms. As opposed to ML, the Gen rule is not removed in the syntax\u00addirected system. It is the only rule \nthat may(and thus must) be used .rst to derive judgments whose conclusion mentions a type scheme that \nis not a .-type. Hence, it can be used either as the last rule in a derivation, or in three other places \n(and only there): immediately above the left-premise of a Let rule, the premise of a Fun rule or the \npremise of another Gen. Strictly speaking, Rule Gen is not syntax-directed, unless we take the expected \ntype-scheme as part of the syntax. Itwouldbepossible andobvious to inlineitin the premises ofboth rules \nFun and Let, but thiswouldbe more verbose and less readable. Rule Var-Inst-Rho behaves as Var followed \nby Inst; Rule AppA-Rho is a restriction of Rule AppA so as to conclude .-types only (hence the -Rho su.x). \nRule LetA-Gen-Rho behaves as Let (restricted to .-types) with a sequence of Gen above its left premise. \nAs in ML, this is the most interesting rule. The type scheme required for t1 is the one requested by \nthe annotation, but where monomorphic parts may be instantiated. Hence, its free types variables that \ndo not appear in G can be generalized in the type assigned to z while typechecking t2. This is the only \nplace where we introduce polymorphism that is not explicitly given (in FA, Rule Gen may only be used \nforpolymorphism that is explicitly given).We do so, much as in ML, and without really guessingpolymorphism \nwejusthaveitfor free!Shapesofall otherpolymorphictypes in the premises are never inferred and just taken \nfrom some corresponding type in the conclusion. If we change all type annotations into . \u00df.\u00df in syntax\u00addirected \nrules, we obtain exactly the syntax-directed rules of ML (Gen becomes useless if the expected type is \na simple type). Thus, we depart from ML only by allowing second\u00adorder polymorphic types in annotations \nand typing judg\u00adments. The instantiation, let-polymorphism, and inference mechanism remain the same. \nIn particular, type inference (as de.ned above) relies on .rst order-uni.cation, which we shall see now. \nFirst, let us state a series of useful standard Lemmas. Lemma 4 (Substitution). If G fFA t : s, then \nthere exists a derivation of G. fFA t : s. of the same length for any substitution .. =.- We writeG \n' Gifdom(G) = dom(G ' )andG ' (z)=.- G(z) p p for each z in dom(G), Lemma 5 (Strengthening). If G fFA \nt : s and G ' =.- p G, then G ' fFA t : s. The next two lemmas establish the correspondence between \nthe two presentations of the typing rules. Lemma 6 (Soundness of syntax-directed rules). Rules Var\u00adInst-Rho, \nAppA-Rho, and LetA-Gen-Rho are derivable in FML(=.p -), (i.e. they may be replaced by chunk of deriva\u00adtions \nusing rules of FML(=.p -)). Lemma 7. (Completeness of syntax-directed rules) Rules Inst, AppA, and LetA \nare admissible in FA, (i.e. adding them to the rules to the de.nition of FA does not allow to derive \nmore judgments). This direction is more interesting and a bit trickier than the previous one. There are \na few subtleties. Admissibility of Rule Inst would not be true if =.- were replaced by =.p p in both \nsystems. Admissibility of Rule AppA requires to follow an application of Rule AppA-Rho with a sequence \nof instances of Rule Gen. This would not be true if Gen were replaced by Genv in both systems see Rule \nAppA v in Figure 9. Corollary 8. The syntax-directed rules and the original rules derive the same judgments. \n 2.4 Type inference via type constraints Syntax-directed typing rules open the path to a type infer\u00adence \nalgorithm. They show that a typing judgment for an expression can only hold if some judgment for its \nimmedi\u00adate subexpressions also hold. Moreover, thetype judgments for sub-expressions are entirely determined \nfrom the original Figure 6. Syntax of constraints Figure 7. Solving =-constraints C ::= true | false \n| t = t | s = s | C . C |. a.C |. a.C | x . s | def x : .a\u00af[C].s in C Abbreviations: (.a\u00af[C].s). . = \n. \u00af\u00af. ftv(.) a.(C . s = .) a/ let x : .a\u00af[C].s in C ' =(. \u00afa[C].s in C ' a.C). (def x : .\u00af) let G,x : \n.a\u00af[C].s in C ' = let G in letx : .a\u00af[C].s in C ' let \u00d8 in C = C judgment except for some instantiationsoftypevariablesby \nmonotypes. This is just as in ML and suggests an underlying .rst-order type-inference mechanism. Typing \nconstraints are a general yet simple and intuitive framework to de.ne type inference algorithms for ML-like \nlanguages [PR05]. Below, we present a brief summary of this approach and extend it in a straightforward \nmanner to cover type inference for FML( ). We refer the reader to [PR05] for a more thorough presentation. \nThe syntax of type constraints is given in Figure 6. We use letter C to range over type constraints. \nDefault atomic constraints are true, false, equalityconstraints t = t, and subtyping constraints s = \n. We build constraints s ' by conjunction C . C ' , existential quanti.cation . a.C, or universal quanti.cation \n. a.C. Finally, we use two special forms of constraints forpolymorphism elimination x . .and polymorphism \nintroduction def x : .a\u00af[C].. in C ' . In order to interpret constraints, we need a model in which we \nmay interpret free type variables. As for ML, we take the set of closed ground types (monotypes without \ntype variables) for our model. (Here, we must assume at least one type constructor of arity 0, e.g. int.) \nWe use letter t to range over ground types. A ground assignment f is a map from all type variables to \nelements of the model. We also see f as a mapping from types to types, by letting (s1 . s2)f = s1f . \ns2f and(. a.s)f = . a.(sfa)where fa is the restriction of f to dom(f)\\{a}. (Consistently with the notation \nfor substitutions, we write sf for the application of f to s.) We write f I C to mean that C is valid \nin environment f. We take f I true and fI false for any f. We take f I t = t ' if and only if tf =.p \n- t ' fandWe take f I s = s ' if and only if sf =.p - s ' f. Technically, we could identify the equation \nt = t ' with the inequation t = t ' , since they coincide. However, for sake of exposition, we prefer \nto explicitly transform subtyping constraints into equations when both sides are monotypes. We take f \nI C . C ' if and only if f I C and f I C ' . We take f I . a.C if and only if there exists a ground type \nt such that f[a . t] I C (we write f[a . t] for the environment that maps a to t and otherwise coincides \nwith f). We take f I . a.C if and only if for every ground type t, we have f[a . t] I C. A constraint \nC entails a constraint C ' if for every ground assignment f, such that f I C we also have f I C ' . We \nthen write C I C ' . Contraints C and C ' are equivalent if both C I C ' and C ' I C holds. Def-constraints \ndef x : .a\u00af[C].s in C ' can be interpreted as C ' [.a\u00af[C].s/x](of course, the resolution of constraints \nwill avoid this substitution). The e.ect of this substitution is to expose constraints of the form .a\u00af[C].s \n. .. These are t = t ' -. t = t ' s1 . s2 = a -. . a1a2.(s1 . s2 = a1 . a2 . a = a1 . a2) a = s1 . s2 \n-. . a1a2.(a1 . a2 = s1 . s2 . a = a1 . a2) s1 . s2 = s1 ' . s2 ' -. s1 ' = s1 . s2 = s2 ' . a.s = . \n-. . a.(s = .) a/. ftv(.) s =. a.s ' -. . a.(s = s ' ) a/. ftv(s) Figure 8. Constraint generation rules \n[ x : .] = x . . [ .z. t : a] = . \u00df2\u00df1.([[.z. t : \u00df2 . \u00df1] . a = \u00df2 . \u00df1) \u00df1,\u00df2 a = [ .z. t : s2 . s1] \n= let z : s2 in [ t : s1] [ t1 (t2 : . \u00af: .1] \u00df.([[t1 : s2 . .1] . [ t2 : s2]]) \u00df.s2)= . \u00af\u00af \u00df/. ftv(.1) \n[ let z = \u00df.s1) (t1 : . \u00afin t2 : .2] = let z : .\u00df\u00af[[[t1 : s1]]].s1 in [ t2 : .2] [ t : . a.s] = . a.[ \nt : s]  syntactic sugar for . \u00afa/ a.(C . s = .), provided \u00af. ftv(.). Hence, the scope in a constrained \ntype scheme .a\u00af[C].s is both C and s. In fact, rather that using def x : .a\u00af[C].. in C ' directly, we \noften use let x : .a\u00af[C].. in C ' as an abbreviation for . a.C\u00af. def x : .a\u00af[C].. in C ' . The solver \nof [PR05] need only be extended to solve sub\u00adtyping constraints. These can easily be reduced to equality \nconstraints by repeatedly applying the rules of Figure 7. (We have left it implicit in the .rst and second \nrules that variables a1 and a2 should not appear free on the left-hand side). Eachrule preserves the \nmeaning of constraints, indeed. When no rule applies, we are left with equality constraints of the form \nt = t ' , which are treated as .rst-order uni.\u00adcation constraints (the predicate = may be interpreted \nas equality on monotypes). Hence, rules of Figure 7 are meant tobe added to rules for solving .rst-order \nconstraints aswell as structural rules for rearranging constraints (see [PR05]). Indeed, the rules of \nFigure7 can alreadybe found in [OL96]. Lemma 9. The rewriting rules of Figure 7 preserve con\u00adstraint \nequivalence. Once the (exposed) subtyping constraints havebeen re\u00adsolved, the remaining constraints are \nequality constraints between monotypes, which can then be resolved by a uni\u00ad.cation algorithm, as in \n[PR05]. The whole simpli.cation process ends with a constraint in solved from, which deter\u00admines a most \ngeneral solution to the initial problem. Remark thatatype substitution . may always be represented as \nthe type constraint .a.dom(.)(a = a.). Constraint generation rules We now describe a type inference algorithm \nby turning type checking problems into type constraint problems, which can then be simpli.ed as described \nabove. Constraint generation rules are given in Figure 8. They take as input a type expression t and \nan expected type s and return a constraint that describes the conditions under which t may be assigned \ntype s. Maybe surprisingly, Figure 9. Syntax-directed typing rules for Fv ) ML(=.p - Var-Inst-Rho Genv \nFun v AppA G f t1 : s2[\u00aft/\u00afa..1 \u00df].. \u00afG f t2 : s2[\u00aft/\u00af\u00af. ftvt1 t2(.1) \u00df] a/G f t1 (t2 : . \u00df.s\u00af2): . \u00af \na..1 LetA-Genv G f t1 : s1[\u00aft/\u00df\u00af] G,z : . t1\\G.s1[\u00aft/\u00afa..2 \u00af. ftvt1,t2(.2) \u00df]f t2 : . \u00afa/G f let z = \n\u00df.s1)in t2 : . \u00af (t1 : . \u00afa..2 constraint generation does not take a typing context G: if the program \nt has free program identi.ers x, so will the constraint [ t : s]]. Then, the constraint may only be valid \nwhen placed in a context of the form let G in \u00b7 that will assign types to free type variables of t. Most \nconstraint generation rules can be read straightfor\u00adwardly and follow syntax-directed typing rules. An \nidenti.er x has type . is and only if . is an instance of the type of x.A function .z. t has type a if \nand only if it has type \u00df2 . \u00df1 where a is of the form \u00df2 . \u00df1 for some types \u00df1,\u00df2;a function .z. t \nhas a type scheme s2 . s1 if and only if t has type s1 assuming z has type scheme s2. The decompo\u00adsition \nof applications is also straightforward.To decompose a let-binding, we .rst generate a constraint for \ntypechecking its argument; this constraint is used to build a constrained type-schemebygeneralizing alltypevariables \n\u00df\u00afthat are free in s1, but free in G. After simpli.cation of [ t1 : s1] some variables of \u00df\u00afwill in general \nbe further constrained, maybe so that they can only be monotypes. Generalizing them is not a problem, \nsince the resolution algorithm will take care of such dependences, which is one of the elegance of using \ntype constraints. Finally, an expression t has a type . a.s if and only if t has type s for any instance \nof a. A type checking problem G f t : s is equivalent to the constraint let G in [ t : s] (which is syntactic \nsugar for a sequence of let constraints, as described in Figure 6). This is stated very precisely and \nconcisely by the fol\u00adlowing two lemmas. This uses a correspondence mapping an idempotent substitution \n. to a solved type constraint a = a.. Va.dom(.) Lemma 10 (soundness). If . I let G in [ t : s] , then \nG. f t : s.. Lemma 11 (completeness). If G. f t : s., then . I let G in [ t : s] . We can read backthese \ntwo theorems into a more traditional (but not really simpler) formulation. Corollary 12. Given a typing \nenvironment G, a program t, and a type scheme s, a substitution . is a solution to the type checking \nproblem G f t : s if and only if it is a solution to the constraint let G in [ t : s] .  2.5 Type inference \nwith side-e.ects As we have seen in Section 1.5, choosing a call-by-value semantics and extending the \nlanguage with side-e.ects also implies some changes in the typing rules. Let Fv ) be ML(=.p - the language \nwhose expressions and typing rules are the same as those of FML( ), except for Rule Gen, which is Figure \n10. New constraint generation rules for Fv ) ML(=.p - [ t1 (t2 : . \u00af: . \u00af= \u00df.s2)a..1] . \u00af \u00df.([[t1 : s2 \n.. a..\u00af1] . [ t2 : s2]]) a\u00af= \u00d8 if t1 t2 .U [ let z =(u1 : . \u00afa..2] \u00df.s1)in t2 : . \u00af= let z : .\u00df\u00af[[[u1 \n: s1]]].s1 in [ t2 : . \u00af\u00af= \u00d8 if t2 .U a..2] a [ let z = \u00df.s1)in t2 : . \u00af= (t1 : . \u00afa..2] . \u00afa..2]]) t1 \n/ \u00df.([[t1 : s1] . let z : s1 in [ t2 : . \u00af.U [ u : . a.s] = . a.[ u : s]  replaced by Rule Genv . We \nmay extend the de.nition of non-expansive expressions given in Section 1.5 with let z = u1 in u2. The \nrelation =.p - is a subrelation of =.-. Hence, Fv ML(=.p -)is sound. Preparing for type inference, we \nmust also review the syntax-directed typing rules. Of course, we should change Gen to Genv. Since rule \nGenv is more restrictive, only non\u00adnegativetypevariablesmaybe generalizedaposterioriinthe type of an \nexpansive expression. As a consequence, the result of the application (see Rule AppA)may have apolymorphic \ntype but only if thispolymorphism comes from the codomain s1 of the type of t1 or from non-negative free \ntype variables in s1. Hence, we change Rule AppA-Rho for Rule AppA v to allow exactly those variables \na\u00afthat do not belong to ftvt1 t2(.1) in the type of the application others may still be generalized afterward. \nThe notation ftv \u00aft(s) where \u00aft is a sequence of expressions stands for ftv(s) if\u00aft .U (i.e. all expressions \nof \u00aft are non-expansive) and ftv (s) otherwise. Similarly, we change Rule LetA-Gen-Rho to LetA-Genv which \nallows a\u00aftobepolymorphic in the type of t2. The sequence of expressions t1,t2 used in the superscript \nof ftv controls the of let =(t1 : . \u00af expansiveness z \u00df.s1) in t2. Consistently with the restriction \nof generalization, we have also replaced .\\G.s1[t/\u00df\u00af]by . t1\\G.s1[\u00aft/\u00df\u00af]. The notation . t\\G.s stands \nfor . ftvt(s)\\ ftv(G).s. That is, generalizable variables in the type of t1 are ftv(s)\\ ftv(G) as before \nif t1 is non-expansive, and ftv (s)\\ ftv(G) otherwise. Constraint generation Let us .rst consider the \nsimpler case of the standard value restriction. That is, we ignore non-negative type variables which \namounts to taking the empty set for ftv (s). In this case, ftv \u00aft(s)isftv(s)if all expressions of\u00aft are \nnon-expansive and is empty otherwise. It is then straightforward to extend constraint genera\u00adtion: we \nneed not add new forms of constraints but only test for expansiveness during constraint generation and \ngenerate di.erent constraints for non-expansive applications and let\u00adbindings. The modi.ed rules are \nsummarized in Figure 10 (capture-avoiding side-conditions have been omitted); these should replace the \ncorresponding rules in Figure 8. Con\u00adstraint resolution then proceeds asbefore. Lemmas6,7, 10, and 11 \nextend to FMLv (=p.-)with standard value restriction. By lack of space, we refer to the full version \n[R\u00b4em05] for a discussion of the enhanced value restriction, which requires enriching the language of \ntype-constraints. 2.6 Type inference for FML(=.p) Unsurprisingly, rules of Figure 5 do not form a syntax\u00ad \n=.- directed presentation of FML(=.p): after replacing p by =. p in Rule Var-Inst-Rho they would de.ned \na typing relation that does not satisfy Rule Inst.Fortunately, we may easily reduce type inference for \nFML(=.p) to type inference for FML(=.p -) by .rst putting typing problems into prenex\u00adforms. More precisely, \nlet prf(. \u00df.s\u00af)be . \u00af \u00df.prf(s)andprf(t) bea copy of t where all annotations . have be replaced by their \nprenex-forms prf(.). Lemma 13. Judgments G fFML(=.) t : s and prf(G) f.- p) FML(=p prf(t): prf(s)are \nequivalent. This shows that the choice between =.p - and =.p is unim\u00adportant in predicative systems. \n 2.7 Recovering impredicativity An important limitation of FML(=.p -) is its predicativity. Fortunately, \nthere is an easy way to recover impredicativity by adding an explicit impredicative decidable fragment \nof type-containment, say I to the implicit predicative type\u00adcontainment relation =.p - or =.p, say P. \nIndeed, to obtain full impredicativity, i.e., in the end, the expressiveness of F, it su.ces that I be \na larger relation than =F. We choose the relation =F , which isbetter suited fortype inference, as it \ndisregards the order of quanti.ers. Formally, we may simply introduce a collection of coer\u00adcion functions \n(i.e. functions that evaluate to the identity) (: . \u00af\u00df.s1 . s2 for all pairs(s1,s2) \u00df.s1 Cs2)with types. \n\u00af \u00af in I and with \u00df equal to ftv(s1). ftv(s2). As for annota\u00adtions,we requirecoercionstobe closed, although \nthisisonly a matter of simpli.cation. This is the purpose of the exis\u00adtentially bound variables whose \nscope extends to both sides of the coercion.We refer to this language as FML( I, P). We may here take \nadvantage of constrained type schemes and be slightly more .exible. Let us introduce new forms of constraints \ns1 s2 whose meaning is given by f I s1 s2 if and only if s1fs2f (for some given relations ). Then, we \nmay assign( \u00df.s1 Cs2)the constrained type scheme : . \u00af.\u00df\u00af[s1 I s2].s1 . s2. This has two advantages: \n.rst, it internalizes the veri.cation of the I type-containment; second, it allows to provide types s1 \nand s2 up to some monomorphic instantiation. For example, taking =F for I, the coercion ( : . \u00df.. a.a \n. \u00df . aCs . t . s) is now valid for any (closed) type t and type scheme s; .nding out that \u00df must actually \nbe t can be left to type inference. This extension is safe by construction, as long as I is a subrelation \nof =., since it amounts to giving the identity a valid type in F(=.). Regarding type inference, it only \nremains to solve I type-containment constraints so that the coercioniswell-de.ned and Rule Var-Inst-Rhoapplies. \nLet us now focus on the particular case of FML(=F p ), ,=.- which we shall abbreviate as FML.Type constraints \ns1 =F s2 can be reduced to canon(s1) =F canon(s2). So we are left to solving constraints of the form \ns1 s2. Re\u00ad =F =F . \u00af...[\u00afa]\u00af. ftv(. \u00afcon\u00adcall that all instances of the relation are of the form a.. \n=F . \u00afs/\u00afwith . /a..). Thus, the \u00afstraint . \u00af...2 with free type variables a..1 =F . \u00af\u00df is equiv\u00adalent \nto the existence of type schemes s\u00afsuch that .2 and .1[\u00afs/a\u00af] are equal modulo a-conversion with variables \n.\u00afnot in ftv(. \u00af a..1). That is, it is equivalent to the equality . \u00afa.(.1 = .2) where \u00afover type schemes \nand a\u00af ... \u00afa range \u00af and \u00df range over monotypes. This is a (restricted) form of uni.cation modulo a-conversion \nand under a mixed pre.x (but no \u00df-reduction is ever involved). Solving such con\u00adstraints is folklore \nknowledge. We refer the reader to the full version for details. Coercion functions are meant to be applied \nto terms. Formally,an application requiresatype annotation, whichin this case maybe exactly the domainof \nthe coercion.We may avoid repeating the annotationbyletting the syntactic sugar (t : . \u00afstand ( : . \n\u00af\u00df.s1). \u00df.s1 Cs2) for \u00df.s1 Cs2)(t : . \u00afWhena coercionisin applicationposition,it still needs an annotation, \neven though the coercion may already carries all the necessary type information. Hence, we may see t1 \n(t2 : . \u00af\u00df.sCs ' : \u00df.s1 Cs2)as syntactic sugar for t1 ((t2 : . \u00af). \u00af). \u00df.s ' There still seem to be \nsome redundancies in coercions since the polymorphic shape of the expected type s2 ' must be provided \nwhentypecheckinga coercion(t : . \u00af \u00df.s1 Cs2). However, s2 ' is only known up to =.p -. That is, in general, \nwe only have s2 = s2' . If we were to take s2 ' for s2, we would then be letf with =F .=.p - constraints. \nHowever, we have not explored yet the resolution of such constraints. Expressiveness The set of raw termstypable \nin FML( I, P ) is exactly F( I . P ). Hence, FML contains terms of both F and F(=.p -) but not all terms \nof F(=.). One may in fact generalize the idea of explicit coercionsby providingtyping evidence for any \ncoercion, which can be done by writing coercions in FML.We could thus also allow coercions of the form( \n: t) where t isa term of FML that .-reduces to the identity. This way, we would .nally reach all of F(=.). \nHow\u00adever, writing coercion functions explicitly is rather heavy. An alternative way to recover impredicativity \nis to use semi-explicitpolymorphism [GR97], which simpli.es signif\u00adicantly here, as types of the host \nlanguage are already of arbitrary rank. 3. Propagation of annotations in F? ML We consider the language \nFML. However, we .rst treat co\u00adercions as constants, as if we were in FML(=.p -). We shall discuss special \nelaboration of coercions in Section 3.5 and elaboration in Fv )in Section 3.4. ML(=.p - Many type annotations \nmay seem redundant in FML. For instance, consider the expression let f =(.z. zz : sid . sid) in f (.y. \ny : sid), which is well typed, but becomes ill-typed if we remove the annotation on the application. \nHowever, one mayargue that given the type of f, it is obvious what the annotation on the application \nshould be. We .rst show that only the polymorphic skeletons of annotations, where the monomorphic leaves \nare stripped o., actually matter. We refer to them as shapes. Computation on shapes is simpler than computation \non types, because shapes are closed.We exploit this to proposea preprocessing step that propagates shapes \nbefore typechecking in FML. 3.1 Shapes Let shapes be closed polymorphic types extended with a unary type \nconstructor r. Shapes are considered modulo the absorbing equation r. r= r. A shape is in canonical form \nwhen it contains the minimum number of occurrences of r. The shape of a polymorphic type s, written 1sl \nis obtained from s by replacing all free type variables of s by r. We use S to range over shapes. Shapes \ncapture the polymorphic structure of types. For example, consider the type s0 equal to . a1.(. a2.(a1 \n. a2) . (\u00df0 . \u00df0)) . (\u00df1 . \u00df2). Its shape 1sl is . a1.(. a2.(a1 . a2) . (r . r)) . (r . r), which can \nbe put in its canonical form . a1. (. a2. (a1 . a2). r) . r. By extension, the shape of an annotation \n1. \u00afA shape \u00df.sl is simply 1sl. S may be read back as a type annotation, written LSJ. By de.nition LSJ \nis . \u00afand a.s if S is in canonical form syntactically equal to s[r/a\u00af], and each variable of a\u00afoccurs \nexactly once in s. For example, the annotation L1slJ is .\u00df1,\u00df2 . a1. (. a2. (a1 . a2). \u00df1) . \u00df2. We sometimes \nneed to strip o. the front quanti.ers of a shape S. However, the resulting type may contain free type \nvariables and must be reshaped. We write Sbfor 1.l where LSJ is of the form . \u00afa... \u00df.. \u00af Our interest \nfor shapes is that only shapes of annotations matter for type inference. If we write L1tlJ for the term \nt where each annotation . has been replaced by L1.lJ, this property is precisely capturedby the following \nlemma. Lemma 14. If G f t : s then G f L1tlJ : s. Sinceshapes areaprojectionoftypes, one couldprojecttyp\u00ading \nrules as well, and obtain a shaping relation that would assign shapes to programs. By construction, one \nwould then expect a property such as ifG f t : s then 1Glf t : 1sl. However, such a result would not \nbe very useful, since we already have an algorithm for typechecking. Conversely, one could hope for some \nkind of shape\u00adinference algorithm and by combination with typecheck\u00ading at a given shape to solve type \ninference problems at unknown shapes. However, shape-inference is of course not quite realistic if we \nde.ne it as .nding every shape for which atype couldbe inferred.However,we maygiveup complete\u00adness, or \neven soundness, and look for some obvious shape for which a type might be inferred. Incompleteness may \nnot be a problem in the context of type reconstruction where we attempt to rebuild missing type annotations \nin an obvious manner and accept to fail if there is no way to do so. Since type reconstruction is not \ngoing to be complete with respect to some simple logical speci.cation, let it be at least simple and \nintuitive! Thus, we shall now allow unannotated application nodes t1 t2 and let-binding nodes let z = \nt1 in t2 here, we mean no annotation at all and not the trivial annotation . \u00df.\u00df, which we may still \nwrite, but explicitly. Since the expected shape may nowbe missing, itbecomes convenient to simul\u00adtaneously \nallow explicit annotations on formal parameters of functions, which we write .z : .. t. Precisely, let \nexpressions of the language F? ML be those of FML extended with the three constructions above. Finally, \nwe may now de.ne typecheck\u00ading in F? ML( )by elaboration into terms ofFML computing on shapes to .ll \nin the missing annotations. This is a form of (incomplete) shape inference that returns both a shape \nand an elaborated term of FML. We write shape inference using judgments of the form G f. t : S. t ' where \nthe . is there to remember that S is inferred. Because ML is a superset F? of FML, there are still cases \nwhere the expected type, hence the expected shape, are known. In such cases, elaboration may be turned \ninto a checking mode: it then uses the given shape and elaborates subterms from which it can return an \nelaborated term. Hence, we recursively de.ne a judgment G f. t : S. t ' . Here, G, t and S are all given \nand t ' is re\u00adturned. The . sign indicates that S is only checked. The idea of mixing checking and inference \nmodes is taken from Peyton Jones and Shields [PVWS05a] but goes backto older works such as local type \ninference [PT00]. The direction of arrows is taken from [PVWS05a].  3.2 Elaboration The two elaboration \njudgments are de.ned by the set of rules of Figure 11. We used variable E to range over . and .. This \nallows factoring some rules that could otherwise be written as pairs of rules. Figure 11. Elaboration \nrules for F? ML Var-C Var-I x : S ' . G x : S. G G f. x : S. x G f. x : S. x Fun-C Fun-I '' G,z : S2 \nf. t : S1 . t G,z : .f. t : S. t '' G f. .z. t : S2 .S1 . .z. t G f. .z. t : ..S . .z. t FunA-C FunA-I \n'' G,z : 1s1f. t : S1 . t G,z : 1s1f. t : S. t G f. .z : . \u00df.s\u00af. t : S2 .S1 \u00df.s G f. .z : . \u00af. t : 1s1 \n.S . .z. let z =(z : . \u00afin t ' . .z. let z =(z : . \u00af \u00df.s)\u00df.s)in t ' Let-E '' G f. t1 : S1 . t1 G,z : \nS1 f t2 : S2 . t2 ' G f let z = t1 in t2 : S2 . let z =(t1 : lS1J)in t ' 2 LetA-E '' G f. t1 : 1s1. t1 \nG,z : 1s1f t2 : S2 . t2 ' G f let z = \u00df.s)in t2 : S2 . let z =(t1:. \u00afin t ' (t1:. \u00af\u00df.s) 2 AppA-C G f. \nt1 : 1s1 . S . t ' 1 G f. t2 : 1s1 . t ' 2 '' G f. t1 (t2 : . \u00af: S. t1 (t \u00df.s) \u00df.s)2 : . \u00af AppA-I '' \nG f. t1 : S. t Sb= S2 .S1 G f. t2 : 1s1. t 12 '' G f. t1 (t2 : . \u00df.s\u00af): S1 . t 2 : . \u00af 1 (t \u00df.s) App-C \n'' G f. t1 : S. t Sb= S2 .S ' G f. t2 : S2 ' . t 11 2 '' G f. t1 t2 : S1 . t1 (t2 : lS2J) App-I '' G \nf. t1 : S. t Sb= S2 .S1 G f. t2 : S2 ' . t 12 '' G f. t1 t2 : S1 . t1 (t2 : lS2J) Rule Var-C checks \nthat there is a binding for x and ignores the shape of x. Of course, a certain relation should holdbetween \nS ' and S.However, this willbeveri.edbylater typechecking in FML, so we may just ignore this condition. \nRule VarA-I reads the shape from the environment G. Note that the best known shape of x is S and not \nan instance of S, which would be weaker. In Rule Fun-C the given shape of the conclusion provides us \nwith both the shape of the parameter z and the expected shape of t.We can thus elaborate the premise \nin checking mode. Rule Fun-I is just the opposite, since nothing is known from the conclusion. We thus \nuse shape r for the parameter and elaborate the premise in inference mode. In Rule FunA-C we actually \nhave extra information since the shape of the parameter is known fromboth the annotation in the conclusion \nand the shape of the conclusion. Again, some relationbetween 1sl and S2 should hold. We use 1sl, which \nis explicitly given and simply ignore S2 during elaboration. Still, the let-binding in the elaborated \nterm, whichstands for an additional pure type-annotation, ensures that the correct relation between s \nand S2 will be veri.ed. Rule FunA-I, is similar except that the premise is called in inference instead \nof checking mode. Cases for let-bindings (Let-E and LetA -E) are all very similar. The left-premise is \ncalled in inference mode when there is no annotation on t1; the right-premise is called in inference \nmode when the conclusion is itself in inference mode. The important detail is that in all cases, variable \nz is bound to the best-known-shape 1sl or S1, whether it is taken from the annotation or inferred. In \nparticular, one cannot generalize the shape in any meaningful way. Indeed, the type inferred for t1 during \ntypechecking may have free variables that could later be generalized during typechecking. However, we \ncannot tell during elaboration, so we may only assume for the shape of z the best known shape of t1. \nOf course, typechecking may later dobetter! An annotated application is elaborated inchecking mode (Rule \nAppA-C)by calling both premises in checking mode, since all necessary shapes are known. In inference \nmode (AppA-I), the left-premise must be called in inference mode because the expected shape of t1 is \nnot entirely known. However, only the range of the inferred shape S1 matters and is used in the elaboration \n(remember that Sb is S stripped o. its toplevel quanti.ers and reshaped). Again the domain S2 should \nbe related to s in some way, but we may ignore it here. Rules App-I and App-C deal with the inference \nmode. Both premises must be called in inference mode, since none of the expected shapes is ever entirely \nknown. The annotation S2 is however taken from the left-hand side. This choice is somehow arbitrary, \nbut it seems to usually work better in practice. The di.erence between inference and checking modes is \nthat the return shape S1 is given in checking mode, while it is the range of Sb in inference mode. (In \nchecking mode the range S1 ' of Sb, which is ignored, should be related to S1 in some way.) By construction, \nelaboration alwayskeep existing anno\u00adtations, so it is the identity on terms of FML and idempotent for \nterms of F? ML. Variations In typing rules for unannotated applications, we have somehow made arbitrary \nchoices, taking the anno\u00adtation from the domain of the inferred shape of the func\u00adtion and discarding \nthe inferred shape of the argument. In [R\u00b4em05], we propose other elaboration rules for appli\u00adcations \nthat may also sometimes take the shape of the ar\u00adgument for building the annotation.  3.3 Typechecking \nin F? ML Of course, elaboration may return ill-typed programs, since shapes are only an approximation \nof types. Hence the pro\u00adgrams resulting from elaboration mustbe submitted totype inference in FML. Actually, \nwell-typedness in F? is simply de.nedby means of elaboration in FML. ML Definition 1. Let G f t : s if \nand only if there exists an ' '' expression t such that 1Glf t : 1sl. t and G f t : s. By construction, \nF? is sound. Elaboration preserves the ML type erasure, hence the semantics of terms. Elaboration rules \nare given in syntax-directed form and are deterministic. They straightforwardly de.ne an algo\u00adrithm that, \ngiven G and t as input, returns the shape of t and an elaborated term t ' . Thus, type inference problems \nin F? can be solved as follows: given G and t, let elabo- ML ration compute both a term t ' and a shape \nS such that G f. t : S. t ' \u00df.s be LSJ; infer a principal sub\u00ad ; let . \u00afstitution . for the type checking \nproblem G f t ' : s in FML and returns the substitution . and the type .\\G..s.. By construction this \ntype inference algorithm is sound and complete with respect to De.nition 1,because propagation is deterministic \nand type-checking in FML is sound and com\u00adplete for the logical speci.cation of FML. One may argue that \nDe.nition 1does nothavea logical .avor. Indeed, we agree! However, we sustain the claim that it is simple \nand easy to understand. Ill-typed programs may be explainedby providingboth the elaborated program and \nan explanation of why it does not typecheck in FML. Either the elaborated program is not as expected, \nand the user should easily see whysince elaboration is simple or he should understand the type error \nwith respect to the elaborated program. 3.4 Elaboration with coercions So far, we have elaborated coercions \nas constants. How\u00adever, we may take advantage of elaboration to allow par\u00adtial type information on explicit \ncoercions as well. Let us allow to omit any side of an explicit coercion, i.e. to write (t : . \u00af\u00df.s1 \nC)and(t : C)respectively. Elab\u00ad \u00df. Cs2),(t : . \u00aforationmust returna fullyspeci.ed coercionby.llingin \nthe missing part. We do this in the obvious ways, turning a co\u00adercion into a simple type annotation or \nsimply discarding it when insu.cient information is available. By lack of space, we refer to the fullversion \nfor the corresponding elaboration rules. Conversely,we may also allow elaboration to insert coer\u00adcions \nthat are not user-speci.ed. However, in general, this would amountto guessingpolymorphism.Sowe shouldonly \ninsert obvious coercions. In particular, when otherwise elab\u00adoration would lead to a typechecking error. \nThere are such opportunities in rules Var-C, FunA-C, and several appli\u00adcation rules. We say that the \nannotation S1 is compatible with the annotation S2, which we write S1 =.p - S2, if there exists type \nschemes s1 and s2 of shape S1 and S2 such that s1 =.p - s2.For instance,(. a.a). r is not compatible \nwith (. a.a . a). r. We may restrict rule Var-C to cases where the shape S ' of x is r or is compatible \nwith the expected shape S. Otherwise,we mayelaborate x as if itwere(x : C). Similarly, we mayrestrict \nFunA-C to cases where S2 ' is r or compatible with 1sl. Otherwise, we may elaborate .z : . \u00af. t as if \nit \u00df.swere .z. let z =(z : . \u00afin t. Rules for applications \u00df. Cs) have several places where elaboration \nmay obviously lead toatypechecking failure. We mayintroduce a coercion on the function-side or on the \nargument-side. Or on both sides, but this would require guessing an intermediate type scheme at whichboth \ncoercions would meet. Further investigation is necessary to .nd a reasonable and useful strategy for \ninserting coerions around applications. 3.5 Elaboration with an imperative semantics Actually, elaboration \ndoes not depend on the di.erences between FML(=.-) and Fv ).To safely elaborate pro\u00ad p ML(=.p - grams \nfor FMLv (=p.-), we only need to replace typechecking in FML(=.-)by typechecking in Fv ). We write Fv \n? for p ML(=.p - ML the corresponding language. This language is safe by con\u00adstruction, since elaboration \npreserves the semantics and the .naltypechecking ensure type-safety. Of course,type-safety is not su.cient \nto make Fvp ) an interesting language. ML(=.- Onepotential problem is that elaborated programs would \nthentoo oftenbe rejectedbecause of thevalue-restriction. For instance, consider an expression t1 of the \nform(.z. .y. y)t0 where t0 is a well-typed expansive expression. Let sid be . a.a . a. ML t1 : sid and \nIn F? we have both f. f. t1 : sid. ML , we do not have f. any more, In Fv ? t1 : sid because this will \n.rst infer a monomorphic type and fail to generalize at the end. Fortunately, we still have f. t1 : sid. \nNote that if t0 were non-expansive, then t1 would also be non-expansive and no annotation would be needed \njust to emphasize the bene.ts of using a larger class of non\u00adexpansive expressions than just values and \nvariables. As another example consider the expression t2 equal to let f =(.y. y : sid)in (.z. f)t0. Here, \nwe even have f. t2 : sid because thepolymorphic shape of f can be inferred. One may wonder why an explicit \nannotation on f is needed, since F? can infer that .y. y haspolymorphic type sid. Indeed, the problem \nis that elaboration isperformed before type inference. A solution is to use incremental elaboration as \ndescribed in Section 3.6 ML  3.6 Incremental elaboration Annotating let-bound expressions withMLtypes \nhas shown to be sometimes useful. This may seem surprising, since thesetypes can actuallybe inferred. \nThe reason is that elab\u00adoration, which uses annotations, is performed before type\u00adchecking and does not \nsee inferred let-boundpolymorphism. Incremental elaboration is a solution to this problem. In\u00adstead ofperforming \nelaboration of the whole program fol\u00adlowedby typecheckingwe may elaborate andtypecheckeck programs by \nsmaller parts, such as toplevel phrases. Con\u00adsider, for instance, the expression let z1 = t1 in ... let \nzn = tn in t.It canbe seen asa successionof n+1 phrases, each of which but the last one augmenting the \ninitial environment with a binding zk : sk where sk is the type inferred for tk. Assuming such a mechanism, \nno ML-like annotation would ever be useful on the toplevel bindings of \u00afz. We may push this idea further \nand apply it to local bindings as well using the tree-structure of let-bindings to order the sequence \nof small elaboration followed by typechecking steps. How\u00adever, this is getting (slightly) more complicated \nand loosing the simplicity of the two-step mechanism so that at the end the user may not so easily understand \nthe elaboration pro\u00adcess. Incremental elaboration at the level of toplevel phrases seems to be a good \ncompromise. 4. Related works and conclusions We have already widely discussed related works in the in\u00adtroduction \nand in particular the most closely related one, PVWS, that inspired this work. Another close work, devel\u00adoped \nin parallel with ours, is a recent proposal by Peyton-Jones et al. [VWP05], which we refer to below as \nVWP. It contributes a signi.cant improvement over PVWS. Leaving impredicativity aside, FML(=.p -)andPVWS \nhave similar, but incomparable, expressiveness. That is, both have the capability to propagate annotations \nin both di\u00adrections. However, they also di.er in small details and there are examples that one can type \nand the other cannot. Since our elaboration is de.ned as a simple preprocessing step, it can easily be \nmodi.ed, e.g. to match PVWS more closely, but not entirely. We have chosen to elaborate applications \nwithout prop\u00adagation of information from the function to the argument (e.g. App-I), as opposed to PVWS. \nOur choice is more sym\u00admetric andkeeps the .ow of elaborationbottom-up, whichis more intuitive for the \nuser to guess the elaborated program. Of course,we lose some information thisway, at thebene.ce of more \npredictable elaboration andtypechecking. Actually, the result of elaboration is then passed to typechecking, \nso some information may still .ow sideway, e.g. from the func\u00adtion side to the argument side. However, \nsuch information does not depend on underneath .rst-order uni.cation nor on the order in whichtypechecking \nisperformed. This is actu\u00adally another limit of our separation of elaboration andtype\u00adchecking. Incremental \nelaboration re-introduces some order\u00ading in which typechecking must be performed, but in a con\u00adtrolled \nand intuitive manner. The language VWP shares a lot with PVWS including the monolithic algorithmic speci.cation \nof typechecking. However, VWP also improves over PVWS in at least two signi.cant ways. First, checking \nand inference modes are carried on types rather than on typing judgments, much as for colored local type \ninference [OZZ01]. This provides with a more precise control of these modes. For instance, it allows \nto specify that some part of a type is known and to be checked while some other part is still unknown \nand to be inferred a capability that we missed in the elaboration of applications. However, as a result, \nVWP is also quite involved and it seems quite di.cult to guess in advance whether a program is typable \nwithout running the algorithm, which can hardly be done mentally or even on paper. Second, VWP also allows \nfor impredicative polymor\u00adphism. The treatment of impredicativepolymorphism seems better integrated in \nVWP. However, it is also deeply hid\u00adden into algorithmic inference rules and an ad-hoc multi\u00adargument \ntypechecking rule for applications. This makes it di.cult to understand the interaction between impredica\u00adtive \ninstantiation and predicative type-containment, since both seem to be left implicit, which clearly cannot \nbe the case. The monolithic type inference process of VWP seems to be more powerful in propagating known \ninformation than However, it simultaneously mixes complicated F? ML. orthogonal concepts in hard-wired \nalgorithmic rules, and as PVWS,it lacksa simplespeci.cation. We see at least three directions for future \nimprovements. Improving the expressiveness of the core language is to be favored. Finding a stronger \nrelation than =.p for which .rst\u00adorder constraintswouldbe easily solvable.In particular solv\u00ading constraints \nof the form =.p - .=F .=.p - would enable explicit impredicative instantiation up to predicative con\u00adtainment \nby taking this relation for I. We should also ex\u00adplore properties of elaborations in more details, which \nwe may then advantageously exploit. Finally, elaboration could probably be made more accurate by introducing \nvariables ranging over type schemes to represent unknown informa\u00adtion and avoid assigning them r a priori. \nThis could also help with the elaboration of impredicative coercions. Hope\u00adfully, such solutions may \navoid the need for more incremen\u00adtal elaboration, which couldbe confusing and less intuitive for the \nuser. However, we might then lose our original goal to keep type inference within the resolution of .rst-order \nconstraints. If we are to lose this simplicity, we should also compare with MLF[LBR03, LB04], which was \ndesigned so as to allow impredicative instantiation from thebeginning, and does it rather well, by comparison \nwith impredicative instantiation in F? ML. Conversely, MLF does not have type-containment and does not \nallow instantiations but this does not =.p seem to be a problem at all in practice. Already present \nin MLF is the idea of elaboration to propagate annotations from interfaces to abstractions inside expressions, \nalthough elaboration used for MLF remains a rather trivial process. MLF has obviously a more powerful \ntype inference engine F? and seems toperform better than ML. However its meta\u00adtheory is also more involved. \nConclusions In summary, we have proposed a core language FML with .rst-order type inference with predicative \ntype containment and explicit impredicativetype-instantiation. A more con\u00advenient surface language F? \nsome ML may be used to alleviate repetitions oftype annotations in source programs. It is de\u00ad.nedby a \nsimple elaboration procedure into the core lan\u00adguage. We believe that the decomposition of type inference \ninto a simple elaboration procedure that propagates second-order type annotations followedby a .rst-order \nML-liketype in\u00adference mechanism is a good compromise between a logical and an algorithmic presentation. \nAt least, it clearly sepa\u00adrates the algorithmic elaboration process, which is complete by de.nition, \nbut incomplete by nature, from the logical order-independent speci.cation oftype inference. As observed, \nsmall variations in the logical speci.cation, e.g. in the type-containment relation or the application \nof generalization may result in rather di.erent type inference algorithms. This con.rms, if it were necessary, \nthat algorith\u00admicspeci.cationsoftype inference are fragile.Of course, al\u00adgorithmic elaboration is a remedy. \nThe goal remains to .nd expressivetype systems with simple logicalspeci.cations for which we have complete \ninference algorithms. Acknowledgments I would like to particularly thank Fran\u00b8cois Pottier for many fruitful \ndiscussions regarding thiswork.I am also grateful to Dimitrios Vytiniotis, Stephanie Weirich, and Simon \nPeyton Jones for their useful feedback concerning this paper and, more generally, for discussions on \ntype inference with type\u00adcontainment. References [BCP99] Kim B. Bruce, Luca Cardelli, and Benjamin C. \nPierce. Comparing object encodings. Information and Computation, 155(1/2):108 133, November 1999. [Gar04] \nJacques Garrigue. Relaxing the value restriction. In International Symposium on Functional and Logic \nProgramming, volume 2998 of Lecture Notes in Computer Science, Nara, April 2004. Springer-Verlag. [GR97] \nJacques Garrigue and DidierR\u00b4emy. Extending ML with semi-explicit higher-orderpolymorphism. InTakayasu \nIto and Mart\u00b4in Abadi, editors, Theoretical Aspects of Computer Software, volume 1281 of Lecture Notes \nin Computer Science, pages 20 46. Springer-Verlag, September 1997. [HP99] Haruo Hosoya and Benjamin C. \nPierce. How good is local type inference? Technical Report MS-CIS-99-17, University of Pennsylvania, \nJune 1999. [LB04] Didier Le Botlan. MLF: Une extension de ML avec polymorphisme de second ordre et instanciation \nimplicite. PhD thesis, University of Paris 7, June 2004. (english version). [LBR03] Didier Le Botlan \nand Didier R\u00b4emy. MLF: Raising ML to the power of system-F. In Proceedings of the Eighth ACM SIGPLAN \nInternational Conference on Functional Programming, pages 27 38, August 2003. [Lei91] Daniel Leivant. \nFinitely strati.ed polymorphism. Information and Computation, 93(1):93 113, July 1991. [LO94] Konstantin \nL\u00a8aufer and Martin Odersky. Polymor\u00adphic type inference and abstract data types. ACM Transactions on \nProgramming Languages and Sys\u00adtems, 16(5):1411 1430, September 1994. [Mit88] John C. Mitchell. Polymorphic \ntype inference and containment. Information and Computation, 76:211 249, 1988. [OL96] Martin Odersky \nand Konstantin L\u00a8aufer. Putting type annotations towork. In ACM Symposium on Principles of Programming, \npages 54 67, St. Petersburg, Florida, January 21 24, 1996. ACM Press. [OZZ01] Martin Odersky, Christoph \nZenger, and Matthias Zenger. Colored local type inference. ACM SIGPLAN Notices, 36(3):41 53, March 2001. \n[Pfe88] Frank Pfenning.Partialpolymorphictype inference and higher-order uni.cation. In Proceedings of \nthe ACM Conference on Lisp and Functional Programming, pages 153 163. ACM Press, July 1988. [PR] Fran\u00b8cois \nPottier and Didier R\u00b4emy. The essence of ML type inference. Extended version of [PR05] (in preparation). \n[PR05] Fran\u00b8cois Pottier and Didier R\u00b4emy. The essence of ML type inference. In Benjamin C. Pierce, editor, \nAdvanced Topics in Types and Programming Languages, chapter 10, pages 389 489. MIT Press, 2005. [PS03] \nSimon Peyton Jones and Mark Shields. Lexically\u00adscoped type variables. Submitted to ICFP 2004, March 2003. \n[PT00] Benjamin C. Pierce and David N. Turner. Local type inference. ACM Trans. Program. Lang. Syst., \n22(1):1 44, 2000. [PVWS05a] SimonPeyton Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Mark Shields. \nPractical type inference for arbitrary-rank types. Submitted to the Journal of Functional Programming, \nJune 2005. [PVWS05b] SimonPeyton Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Mark Shields. Practical \ntype inference for arbitrary-rank types. technical appendix. Private communication, June 2005. [R\u00b4em94] \nDidier R\u00b4emy. Programming objects with ML-ART: An extension to ML with abstract and record types. In \nMasami Hagiya and John C. Mitchell, editors, Theoretical Aspects of Computer Software, volume 789 of \nLecture Notes in Computer Science, pages 321 346. Springer-Verlag, April 1994. [R\u00b4em05] Didier R\u00b4emy. \nSimple, partial type-inference for System F based on type-containment. Full version, September 2005. \n[TU02] Jerzy Tiuryn andPawel Urzyczyn. The subtyping prob\u00adlem for second-order types is undecidable. \nInformation and Computation, 179(1):1 18, 2002. [VWP05] Dimitrios Vytiniotis, Stephanie Weirich, and \nSimon Peyton Jones. Boxy type inference for higher-rank types and impredicativity. Available electronically \nat , April 2005. [Wel94] Joe B. Wells. Typability and type checking in the second-order .-calculus are \nequivalent and undecidable. In Proceedings of the Ninth Annual IEEE Symposium on Logic in Computer Science \n(LICS), pages 176 185, 1994. [Wel95] Joe B.Wells. The undecidabilityof Mitchell s subtyping relation. \nTechnical Report 95-019, Computer Science Department, Boston Univiversity, December 1995. [Wel96] Joe \nB. Wells. Typability is undecidable for F+eta. Technical Report 96-022, Computer Science Depart\u00adment, \nBoston Univiversity, March 1996. [Wri95] Andrew K.Wright. Simple imperativepolymorphism. Lisp and Symbolic \nComputation, 8(4):343 355, 1995.  \n\t\t\t", "proc_id": "1086365", "abstract": "We explore partial type-inference for System F based on type-containment. We consider both cases of a purely functional semantics and a call-by-value stateful semantics. To enable type-inference, we require higher-rank polymorphism to be user-specified via type annotations on source terms. We allow implicit predicative type-containment and explicit impredicative type-instantiation. We obtain a core language that is both as expressive as System F and conservative over ML. Its type system has a simple logical specification and a partial type-reconstruction algorithm that are both very close to the ones for ML. We then propose a surface language where some annotations may be omitted and rebuilt by some algorithmically defined but logically incomplete elaboration mechanism.", "authors": [{"name": "Didier R&#233;my", "author_profile_id": "81100311096", "affiliation": "INRIA-Rocquencourt", "person_id": "P745792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086383", "year": "2005", "article_id": "1086383", "conference": "ICFP", "title": "Simple, partial type-inference for System F based on type-containment", "url": "http://dl.acm.org/citation.cfm?id=1086383"}