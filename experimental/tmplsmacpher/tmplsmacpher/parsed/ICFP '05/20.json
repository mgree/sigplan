{"article_publication_date": "09-12-2005", "fulltext": "\n Fast Narrowing-Driven Partial Evaluation *  for Inductively Sequential Programs J. Guadalupe Ramos \n Josep Silva Germ\u00b4an Vidal I.T. La Piedad, Av. Tecnol\u00b4ogico 2000, DSIC, Tech. University of Valencia, \nLa Piedad, Mich., M\u00b4exico Camino de Vera s/n, E-46022 Valencia, Spain guadalupe@dsic.upv.es {jsilva,gvidal}@dsic.upv.es \n Abstract Narrowing-driven partial evaluation is a powerful technique for the specialization of (.rst-order) \nfunctional and functional logic pro\u00adgrams. However, although it gives good results on small programs, \nit does not scale up well to realistic problems (e.g., interpreter spe\u00adcialization). In this work, we \nintroduce a faster partial evaluation scheme by ensuring the termination of the process of.ine. For this \npurpose, we .rst characterize a class of programs which are quasi\u00adterminating, i.e., the computations \nperformed with needed narrow\u00ading the symbolic computation mechanism of narrowing-driven partial evaluation \nonly contain .nitely many different terms (and, thus, partial evaluation terminates). Since this class \nis quite restric\u00adtive, we also introduce an annotation algorithm for a broader class of programs so that \nthey behave like quasi-terminating programs w.r.t. an extension of needed narrowing. Preliminary experiments \nare encouraging and demonstrate the usefulness of our approach. Categories and Subject Descriptors F.3.2 \n[Logics and Mean\u00adings of Programs]: Semantics of Programming Languages partial evaluation, program analysis; \nI.2.2 [Arti.cial Intelligence]: Auto\u00admatic Programming program transformation General Terms algorithms, \nperformance, theory Keywords narrowing, quasi-termination, of.ine partial evaluation 1. Introduction \nGiven a program and an initial call (usually containing some known data), the aim of a partial evaluator \nis the construction of a new, residual program specialized for this call. The essential component of \nmany partial evaluators is a technique to compute a .nite repre\u00adsentation of the generally in.nite computation \nspace for the ini\u00adtial call, so that a (hopefully more ef.cient) residual program can * This work has \nbeen partially supported by the EU (FEDER) and the Spanish MEC under grants TIN2004-00231 and HU 2003-0003, \nby the Generalitat Valenciana GRUPOS03/025, and by the ICT for EU-India Cross-Cultural Dissemination \nProject ALA/95/23/2003/077-054. This research was done while visiting the T.U. Valencia (Spain), funded \nby grants SEIT-ANUIES 20020V and DGEST beca-comisi\u00b4on (M\u00b4exico). Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 05 September 26 28, 2005, Tallinn, Estonia. \nCopyright c . 2005 ACM 1-59593-064-7/05/0009. . . $5.00. be extracted from this representation. For instance, \ngiven a pro\u00adgram P and an initial function call, f(t,x), where t is a known input data and x is a free \nvariable, a trivial partial evaluator may simply return the residual program P ' = P.{ft(x)= f(t, x)} \ncontaining a specialized version ft of function f. While the correct\u00adness of this trivial partial evaluator \nis obvious, it is also clear that no ef.ciency improvement can be achieved. A challenge in partial evaluation \nis the de.nition of techniques for constructing nontrivial yet .nite representations of the computation \nspace of a program so that ef.cient residual programs can be extracted. Narrowing-driven partial evaluation \n(NPE) is a powerful spe\u00adcialization technique for rewrite systems [2], i.e., for the .rst\u00adorder component \nof many functional (logic) languages like Haskell [40] or Curry [21]. Higher-order features can still \nbe modeled by using an explicit application operator, i.e., by defunctionaliza\u00adtion [42]; this strategy \nis used in several implementations of lazy functional logic languages, like the Portland-Aachen-Kiel \nCurry System (PAKCS [27]) and the M\u00a8unster Curry Compiler (MCC [38]). Although NPE can be seen as a traditional \npartial evaluation scheme for program specialization, it can also achieve more pow\u00aderful optimizations \nlike deforestation [46], elimination of higher\u00adorder functions (represented in a .rst-order setting by \ndefunction\u00adalization), etc. A narrowing-driven partial evaluator is currently in\u00adtegrated into the PAKCS \nenvironment for Curry (an experimental evaluation can be found in [1]). At the core of the NPE scheme \nwe .nd a method to construct a .nite representation of a (usually) in.nite computation space. To be precise, \ngiven a rewrite system R and a term t, NPE constructs a .nite representation of all possible reductions \nof t and any of its instances if it contains variables in R, and then extracts a new, often simpler and \nmore ef.cient, rewrite system. Since t may contain variables, some form of symbolic computation is required. \nIn NPE, a re.nement of narrowing [43] is used to perform symbolic computations, being needed narrowing \n[9] the strategy that presents better properties (as shown in [5]). In general, the narrowing space of \na term may be in.nite. However, even in this case, NPE may still terminate when the original program \nis quasi-terminating [19] w.r.t. the considered narrowing strategy, i.e., when only .nitely many different \nterms modulo variable renaming are computed. The reason is that the (partial) evaluation of multiple \noccurrences of the same term (modulo variable renaming) in a computation can be avoided by inserting \na call to some previously encountered variant (a technique known as specialization-point insertion in \nthe partial evaluation literature). Partial evaluators fall in two main categories, online and of.ine, \naccording to the time when termination issues are addressed. On\u00adline partial evaluators are usually more \nprecise since more infor\u00admation is available. For instance, the original NPE scheme (which follows the \nonline approach) considers a variant of the Kruskal tree condition called homeomorphic embedding [34] \nto ensure the termination of the process [4]: if a term embeds some previous term in the same narrowing \ncomputation, some form of generaliza\u00adtion usually the most speci.c generalization operator is applied \nand partial evaluation is restarted with the generalized terms. How\u00adever, this extra precision comes \nat a cost: the homeomorphic em\u00adbedding tests, together with the associated generalizations, make NPE \nvery expensive and, thus, it does not scale up well to realistic problems like interpreter specialization \n[30] or compiler generation by self-application [22]. In this work, we propose a faster NPE scheme by \nensuring ter\u00admination of.ine. Of.ine partial evaluators usually proceed in two stages: the .rst stage \nreturns a program that includes annotations to guide the partial computations (e.g., to identify those \nfunction calls that can be safely unfolded); then, the second stage the proper partial evaluation only \nneeds to obey the annotations and, thus, it is generally much faster than online partial evaluators. \nLet us remark that, in the NPE framework, the so-called static/dynamic distinction is hardly present. \nIndeed, in a functional logic setting, one can require the (nondeterministic) evaluation of terms contain\u00ading \nfree variables at runtime. Therefore, in contrast to traditional binding-time analysis, the .rst stage \nof our of.ine partial evalua\u00adtion scheme ensures termination even if all arguments are dynamic (i.e., \nunknown). Contributions. The main contributions of this work are the fol\u00adlowing. First, we identify a \nclass of quasi-terminating rewrite sys\u00adtems, called nonincreasing, by providing a suf.cient condition. \nThis is an interesting result on its own since no previous charac\u00adterization appears in the literature. \nUnfortunately, this class is too restrictive and, thus, we also introduce an algorithm that takes an \nin\u00adductively sequential program a much broader class and returns an annotated program. Then, we de.ne \nan extended needed nar\u00adrowing relation, generalizing needed narrowing, in which anno\u00adtated subterms are \ngeneralized. We prove that computations with this relation are quasi-terminating for annotated inductively \nse\u00adquential programs and, thus, it forms an appropriate basis for en\u00adsuring termination of NPE of.ine. \nFinally, we explain how our new developments can be integrated into the NPE scheme and prove its correctness \nand termination. Preliminary experiments with a proto\u00adtype implementation of the new partial evaluation \nmethod are en\u00adcouraging and demonstrate the usefulness of our approach. Plan of the paper. This paper \nis structured as follows. Section 2 presents an informal overview of our approach to the partial eval\u00aduation \nof functional (logic) programs. Then, after providing some preliminary de.nitions in Sect. 3, we introduce \nthe characteriza\u00adtion of nonincreasing programs in Sect. 4. Section 5 presents an algorithm for annotating \ninductively sequential programs, together with an extension of needed narrowing that exploits program \nanno\u00adtations to ensure quasi-termination. Section 6 describes the scheme of the complete of.ine NPE method \nand includes a summary of the experimental evaluation. Finally, Sect. 7 includes a comparison with related \nwork and concludes. Proofs of technical results can be found in [41].  2. Partial Evaluation In this \nsection, we present an informal overview of our approach to the partial evaluation of functional (logic) \nprograms. In our setting, the input for the partial evaluator are a rewrite system a typical .rst-order \nfunctional program and an initial function call, which usually contains some known data (the so\u00adcalled \nstatic data). Consider, for instance, the following rewrite system: inc(x) . add(succ(zero),x) add(zero,y) \n. y add(succ(x),y) . succ(add(x,y)) where natural numbers are built from zero and succ. We can partially \nevaluate this program w.r.t. the initial term inc(x)in order to obtain a direct de.nition for function \ninc (i.e., by specializing function add to have a .xed .rst argument succ(zero)). Both online and of.ine \npartial evaluators should construct some form of symbolic execution tree. It is symbolic because terms \nmay contain free variables and, thus, a non-standard, symbolic execu\u00adtion mechanism if often required. \nFurthermore, we get a tree struc\u00adture since the evaluation of function calls containing free variables \ngenerally require nondeterministic evaluation steps. The construction of such a symbolic execution tree \nis explicit in some partial evaluation techniques (like, e.g., positive supercom\u00adpilation [44] or narrowing-driven \npartial evaluation [2]). In some other techniques, the construction of a symbolic execution tree is only \nimplicit. For instance, many partial evaluators for functional programs (see, e.g., [32]) include an \nalgorithm that iteratively (1) takes a function call, (2) performs some symbolic evaluations, and (3) \nextracts from the partially evaluated expression the set of pend\u00ading function calls the so-called successors \nof the initial function call to be processed in the next iteration of the algorithm. Observe that, if \nwe add an arrow from each term to its set of successors, we would also obtain a sort of symbolic execution \ntree. In order to perform symbolic computations in a functional con\u00adtext, an extension of the standard \nsemantics is required for evaluat\u00ading terms with free variables. Here, the choice of narrowing [43] as \nsymbolic computation mechanism arise naturally since it combines functional reductions with the instantiation \nof free variables (see the next section for a formal de.nition). Moreover, in the setting of functional \nlogic programming, the same operational principle can be used for performing both standard and symbolic \ncomputations [2] (similarly to the partial evaluation of logic programs, where SLD-resolution is used \nfor both standard and symbolic computa\u00adtions [36]). For instance, the symbolic execution tree for the \ninitial call inc(x)w.r.t. the program above is as follows (the selected function call is underlined): \ninc(x) t add(succ(zero),x) t succ(add(zero,x)) t succ(x) Here, no instantiation of free variables was \nnecessary; therefore, we get a deterministic evaluation. The associated residual program can easily be \nextracted from the root-to-leaf computations of the symbolic execution tree. In the example above, we \nget the single rule inc(x) . succ(x) In practice, partial evaluators often include a sort memoization \ntechnique to avoid the repeated evaluation of the same term (mod\u00adulo variable renaming). Consider the \nfollowing de.nition: inc ' (x) . add(x,succ(zero)) Although the symbolic execution tree for inc ' (x)is \nin.nite: inc ' (x) t add(x,succ(zero)) {x7.zero}7 {x7.succ(y)} 7 777777 succ(zero) succ(add(y,succ(zero))) \n 8 a partial evaluator would terminate in this example since the func\u00adtion call add(y,succ(zero))is a \nvariant of add(x,succ(zero)). In the tree above, the arrows issuing from add(x,succ(zero))are labeled \nwith the computed substitution by narrowing, i.e., a sub\u00adstitution such that, when applied to add(x,succ(zero)), \nallows a reduction step with the standard semantics. Here, the associated residual program is the following: \ninc ' (x) . add(x,succ(zero)) add(zero,succ(zero)) . succ(zero) add(succ(y),succ(zero)) . succ(add(y,succ(zero))) \nIn this case, we have a residual rule associated to the .rst evaluation step and two residual rules associated \nto each nondeterministic step (here, the associated bindings are applied to the left-hand sides of the \nrules). The .niteness of the symbolic execution tree can be guaranteed when symbolic computations are \nquasi-terminating, i.e., when only .nitely many different terms modulo variable renaming are ob\u00adtained. \nNote that, even if the considered program is terminating w.r.t. the standard semantics, the symbolic \nexecution mechanism may give rise to both non-terminating and non-quasi-terminating computations. Consider \nthe following function de.nition: double(x) . prod(x,succ(succ(zero))) prod(zero,y) . zero prod(succ(x),y) \n. add(prod(x,y),y) Given the initial call double(x), the associated symbolic tree is in\u00ad.nite (we use \nsucc 2(zero)as a shorthand for succ(succ(zero))): double(x) t prod(x,succ 2(zero)) {x7.{x7.succ(y)} \nzero} 777777777   zero add(prod(y,succ 2(zero)),succ 2(zero)) {y7.zero} 7777777 add(zero,succ 2(zero)) \n{y7.succ(z)} 7777777 t 2 2 2 succ (zero) add(add(prod(z,succ (zero)),succ (zero))) 8 In order to always \nensure the .niteness of symbolic execution trees, one should consider a generalization operation on terms. \nThe deci\u00adsion on which terms should be generalized can be taken in a pre\u00adprocessing stage (the case of \nof.ine partial evaluation) or during partial evaluation itself (as in online partial evaluation). Online \npar\u00adtial evaluators are usually more precise since more information is available for deciding whether \ngeneralization is necessary or not. In contrast, of.ine partial evaluators are less precise but generally \nmuch faster since the partial evaluation stage should only follow the annotations given by a pre-processing \nanalysis (the so-called binding-time analysis). In the example above, termination can be guaranteed by \ngener\u00adalizing the second call to function prodas follows: double(x) t prod(x,succ 2(zero)) {x7.{x7.succ(y)} \nzero} 777777777  zero add(prod(y,succ 2(zero)),succ 2(zero)) 7 77777 add(w,succ 2(zero)) prod(y,succ \n2(zero)) {w7.zero}7 {w7.succ(z)} 77  77 7 7 succ 2(zero) succ(add(z,succ 2(zero))) Now, the symbolic \nexecution tree is kept .nite since all the leaves are values (i.e., they do not contain function calls, \nlike zero and succ 2(zero)) or contain a function call that is a variant of a previ\u00adous function call \nin the tree (the case of prod(y,succ 2(zero))and add(z,succ 2(zero)), which are variants of prod(x,succ \n2(zero)) and add(w,succ 2(zero)), respectively). From this symbolic execution tree, the following residual \npro\u00adgram can be extracted: double(x) . prod(x,succ 2(zero)) prod(zero,succ 2(zero)) . zero prod(succ(y),succ \n2(zero)) . add(prod(y,succ 2(zero))) add(zero,succ 2(zero)) . succ 2(zero) add(succ(z),succ 2(zero)) \n. succ(add(z,succ 2(zero))) In the remainder of this paper, we present a systematic approach to the of.ine \npartial evaluation of inductively sequential systems.  3. Foundations Term rewriting [11] offers an \nappropriate framework to model the .rst-order component of many functional (logic) programming languages.1 \nTherefore, in the remainder of this paper we follow the standard framework of term rewriting for developing \nour results. 3.1 The Source Language A set of rewrite rules (or oriented equations) l . r such that l \nis a nonvariable term and r is a term whose variables appear in l is called a term rewriting system (TRS \nfor short); terms l and r are called the left-hand side and the right-hand side of the rule, respectively. \nGiven a TRS R over a signature F, the de.ned symbols D are the root symbols of the left-hand sides of \nthe rules and the constructors are C = F\\D. We restrict ourselves to .nite signatures and TRSs. We denote \nthe domain of terms and constructor terms by T(F, V)and T(C, V), respectively, where V is a set of variables \nwith FnV = .. A TRS Ris constructor-based if the left-hand sides of its rules have the form f(s1,...,sn)where \nsi are constructor terms, i.e., si .T (C,V), for all i =1, ...,n. The set of variables appearing in a \nterm tis denoted by Var(t). A term t is linear if every variable of V occurs at most once in t. Ris left-linear \n(resp. right-linear) if 1 Nevertheless, higher-order features can be modeled by using an explicit application \noperator, i.e., by defunctionalization [42]. l (resp. r) is linear for all rules l . r .R. The de.nition \nof f in R is the set of rules in R whose root symbol in the left-hand side is f. A function f .D is left-linear \n(resp. right-linear) if the rules in its de.nition are left-linear (resp. right-linear). The root symbol \nof a term t is denoted by root(t). A term t is operation-rooted (resp. constructor-rooted) if root(t) \n.D (resp. root(t) .C). As it is common practice, a position p in a term t is represented by a sequence \nof natural numbers, where o denotes the root position. Positions are used to address the nodes of a term \nviewed as a tree: t|p denotes the subterm of t at position p and t[s]p denotes the result of replacing \nthe subterm t|p by the term s. A term t is ground if Var(t)= .. A term t is a variant of term t ' if \nthey are equal modulo variable renaming. A substitution s is a mapping from variables to terms such that \nits domain Dom(s)= {x .V| x s(x)}is .nite. The identity = substitution is denoted by id. A substitution \ns is constructor, if s(x) is a constructor term for all x .Dom(s). Term t ' is an instance of term t \nif there is a substitution s with t ' = s(t).A uni.er of two terms s and t is a substitution s with s(s)= \ns(t). In the following, we write on for the sequence of objects o1,...,on. Inductively sequential TRSs \n[6] are a subclass of left-linear constructor-based TRSs. Essentially, a TRS is inductively sequen\u00adtial \nwhen all its operations are de.ned by rewrite rules that, recur\u00adsively, make on their arguments a case \ndistinction analogous to a data type (or structural) induction. Inductive sequentiality is not a limiting \ncondition for programming. In fact, the .rst-order compo\u00adnent of many functional (logic) programs written \nin, e.g., Haskell, ML or Curry, are inductively sequential.2 Also, the class of induc\u00adtively sequential \nprograms provides for optimal computations both in functional and functional logic programming [6, 9]. \nEXAMPLE 1. Consider the following rules which de.ne the less\u00ador-equal function on natural numbers (built \nfrom zero and succ): zero . y . true succ(x) . zero . false succ(x) succ(y) . x . y . This function is \ninductively sequential since its left-hand sides can be hierarchically organized as follows: . . zero \n. m . n m =. succ(x). zero . succ(x)m =. . . succ(x)succ(y) . where arguments in a box denote a case \ndistinction (this is similar to the notion of de.nitional tree in [6]). 3.2 Semantics The evaluation \nof terms w.r.t. a TRS is formalized with the notion of rewriting.A rewrite step is an application of \na rewrite rule to a term, i.e., t .p,R s if there exists a position p in t, a rewrite rule R =(l . r) \nand a substitution s with t|p = s(l) and s = t[s(r)]p (p and R will often be omitted in the notation \nof a reduction step). The instantiated left-hand side s(l)is called a redex. A term t is called irreducible \nor in normal form if there is no term s with t . s. We denote by .+ the transitive closure of . and by \n. * its re.exive and transitive closure. Given a TRS R and a term t, we say that t evaluates to s iff \nt . * s and s is in normal form. Functional logic programs mainly differ from purely functional programs \nin that function calls may contain free variables. In or\u00adder to evaluate such terms containing variables, \nnarrowing nonde\u00ad 2 Curry also accepts overlapping inductively sequential systems. This class extends \ninductively sequential systems with a disjunction operator which introduces additional don t-know nondeterminism. \nNevertheless, the nice properties of inductive sequentiality carry over to overlapping systems too. terministically \ninstantiates the variables such that a rewrite step is possible [25]. Formally, t p,R,s t ' is a narrowing \nstep iff p is a nonvariable position of t and s(t) .p,R t ' (we sometimes omit p, R and/or s when they \nare clear from the context). s is very often the most general uni.er3 of t|p and the left-hand side of \n(a variant of) R, restricting its domain to Var(t). As in proof pro\u00adcedures for logic programming, we \nassume that the rules of the TRS always contain fresh variables if they are used in a narrow\u00ading step. \nWe denote by t0 s * tn a sequence of narrowing steps t0 s1 ... sn tn with s = sn . \u00b7\u00b7\u00b7 . s1 (if n =0 \nthen s = id). Due to the presence of free variables, a term may be reduced to different values after \ninstantiating these variables to different terms. Given a narrowing derivation t0 s * tn, we say that \ntn is a computed value and s is a computed answer for t0. EXAMPLE 2. Consider the following de.nition \nof function + : zero + y . y (R1) succ(x)+ y . succ(x + y)(R2) Given the term x + succ(zero), narrowing \nnondeterministically performs the following derivations: x + succ(zero)succ(zero) o,R1,{x7.zero} x + \nsucc(zero) o,R2,{x7.succ(y1)} succ(y1 + succ(zero)) succ(succ(zero)) 1,R1,{y17.zero} x + succ(zero)succ(y1 \n+ succ(zero)) 1,R2,{y17.succ(y2)} succ(succ(y2 + succ(zero))) o,R2,{x7.succ(y1)} succ(succ(succ(zero))) \n... 1.1,R1,{y27.zero} Therefore, x + succ(zero)nondeterministically computes the fol\u00adlowing values (here, \nwe use succ n as a shorthand for n applica\u00adtions of function succ): succ(zero)with answer {x . zero}, \n succ 2(zero)with answer {x . succ(zero)},  succ 3(zero)with answer {x . succ 2(zero)}, etc.  As in \nlogic programming, narrowing derivations can be represented by a (possibly in.nite) .nitely branching \ntree. Formally, given a TRS R and an operation-rooted term t,a narrowing tree for t in Ris a tree satisfying \nthe following conditions: (a) each node of the tree is a term, (b) the root node is t, and (c) if s is \na node of the tree then, for each narrowing step s p,R,s s ', the node has a child s ' and the corresponding \narc in the tree is labeled with (p,R,s). In order to avoid unnecessary computations and to deal with \nin.nite data structures, a demand-driven generation of the search space has been advocated by a number \nof lazy narrowing strate\u00adgies [23, 37, 39]. Due to its optimality properties w.r.t. the length of derivations \nand the number of computed solutions, needed nar\u00adrowing [9] is currently the best lazy narrowing strategy. \nWe say that s p,R,s t is a needed narrowing step iff s(s) .p,R t is a needed rewrite step in the sense \nof Huet and L\u00b4evy [29], i.e., in every computation from s(s)to a normal form, either s(s)|p or one of \nits descendants must be reduced. Here, we are interested in a particular needed narrowing strategy, denoted \nby . in [9, Def. 13], which is based on the notion of a de.nitional tree [6] (a hierarchical structure \ncontaining the rules of a function de.nition, which is used to guide the needed narrowing steps). This \n3 Some narrowing strategies (e.g., needed narrowing) compute uni.ers which are not the most general, \nsee below. strategy is basically equivalent to lazy narrowing [39] where nar\u00adrowing steps are applied \nto the outermost function, if possible, and inner functions are only narrowed if their evaluation is \ndemanded by a constructor symbol in the left-hand side of some rule (i.e., a typical call-by-name evaluation \nstrategy). The main difference is that needed narrowing does not compute the most general uni\u00ad.er between \nthe selected redex and the left-hand side of the rule but only a uni.er. The additional bindings are \nrequired to ensure that only needed computations are performed (see, e.g., [9]) and, thus, needed narrowing \ngenerally computes a smaller search space. EXAMPLE 3. Consider again the rules de.ning function . of \nExample 1. In a term like t1 . t2, needed narrowing proceeds as follows: First, t1 should be evaluated \nto some head normal form (i.e., a free variable or a constructor-rooted term) since all three rules de.ning \n. have a non-variable .rst argument. Then, 1. If t1 evaluates to zero then the .rst rule is applied. \n 2. If t1 evaluates to succ(t1' )then t2 is evaluated to head normal form: (a) If t2 evaluates to zero \nthen the second rule is applied. (b) If t2 evaluates to succ(t2' )then the third rule is applied. (c) \nIf t2 evaluates to a free variable, then it is instantiated to a constructor-rooted term, here zero or \nsucc(x) and, depending on this instantiation, we proceed as in cases (a) or (b) above.  3. Finally, \nif t1 evaluates to a free variable, needed narrowing instantiates it to a constructor-rooted term (zero \nor succ(x)). Depending on this instantiation, we proceed as in cases (1) or  (2) above. Let us note \nthat needed narrowing is only de.ned on operation\u00adrooted terms, i.e., a needed narrowing derivation stops \nwhen a head normal form (a value in our context) is obtained. This is not a restriction since the evaluation \nto normal form can be reduced to a sequence of head normal form computations (see [26]). A precise de.nition \nof inductively sequential TRSs and needed narrowing is not necessary in this work (the interested reader \ncan .nd detailed de.nitions in [6, 9]). In the following, we use needed narrowing to refer to the particular \nstrategy . in [9, Def. 13].  4. Ensuring Quasi-Termination w.r.t. Needed Narrowing In the NPE framework \n[2], narrowing is used as a symbolic compu\u00adtation mechanism to perform partial computations. Roughly \nspeak\u00ading, given a program Rand an initial term t, partial evaluation pro\u00adceeds by constructing a narrowing \ntree for t in R with the addi\u00adtional constraint of not evaluating terms that are variants of pre\u00advious \nterms in the computation. Therefore, the termination of the NPE process can be ensured when all narrowing \ncomputations are quasi-terminating. Analogously to Holst [28], we say that a com\u00adputation is quasi-terminating \nwhen it only contains .nitely many different terms (modulo variable renaming). The most recent instance \nof the NPE scheme is based on needed narrowing [5]. However, while the original NPE scheme guarantees \nthat computations are quasi-terminating online (by applying appro\u00adpriate termination tests and generalization \noperators), in this work we introduce a suf.cient condition for TRSs so that needed nar\u00adrowing computations \nare always quasi-terminating. First, we need the following preparatory de.nitions: DEFINITION 4 (graph \nof functional dependencies). Given a TRS R, its graph of functional dependencies, in symbols G(R), con\u00adtains \nnodes labeled with the function symbols in D and there is an arrow from node f to node g iff there is \na call to g from the right-hand side of some rule in the de.nition of f. DEFINITION 5 (cyclic, noncyclic \nfunction). Let R be aTRS. A function f .D is cyclic if node f belongs to a cycle in G(R)and it is noncyclic \notherwise. EXAMPLE 6. Consider the following TRS R: f(s(x),y) . g(x,y) g(x,s(y)) . h(x,y) h(0,y) . y \nh(s(x),y) . c(i(x),h(x,y)) i(x) . x where f,g,h,i .D are de.ned functions and 0,s,c .C are con\u00adstructor. \nThe associated graph of functional dependencies, G(R), is as follows:  fghi Thus, functions f, g, and \ni are noncyclic, while h is cyclic. Clearly, noncyclic functions cannot introduce nonterminating (nor \nnon-quasi-terminating) computations as long as the cyclic func\u00adtions do not introduce them. Thus, we \nturn our attention to cyclic functions. Following [13], the depth of a variable x in a constructor term \nt, in symbols dv(t,x), is de.ned as follows: dv(c(tn),x)=1+max(dv(tn,x)) if x .Var(c(tn)) dv(c(tn),x)= \n-1 if x .Var(c(tn)) dv(y,x)=0 if x = y and y .V dv(y,x)= -1 if x = y and y .V where c .C is a constructor \nterm with arity n . 0. The following de.nition introduces the notion of nonincreasing function, i.e., \na function that always consume its parameters or leave them unchanged: DEFINITION 7 (nonincreasing function). \nLet R be a left-linear, constructor TRS. A function f .D is nonincreasing iff each rule f(sn). r in the \nde.nition of f ful.lls the following conditions: 1. the right-hand side does not contain nested de.ned \nfunction symbols (i.e., de.ned function symbols that occur inside other de.ned function symbols), and \n 2. dv(si,x)dv(tj,x)for all operation-rooted subterms g(tm)  . in r, where i .{1, ...,n}, x .Var(si), \nand j .{1, ...,m}. EXAMPLE 8. A function de.ned by the single rule f(x,y,s(z)). c(g(x),h(z)), with s,c \n.C and f,g,h .D, is nonincreasing since the following relations hold: dv(x,x)=0 0= dv(x,x) . dv(x,x)=0 \n-1= dv(z,x) . dv(y,y)=0 -1= dv(x,y) . dv(y,y)=0 -1= dv(z,y) . dv(s(z),z)=1 -1= dv(x,z) . dv(s(z),z)=1 \n0= dv(z,z) . i.e., variable x is just copied, variable y vanishes, and (the depth of) variable z decreases. \nAnalogously to [19], we say that a TRS is quasi-terminating for a set of terms T w.r.t. needed narrowing \niff all needed narrowing derivations issuing from the terms in T are quasi-terminating. Now, we give \na suf.cient condition for quasi-termination: DEFINITION 9 (nonincreasing TRS). Let R be an inductively \nse\u00adquential TRS. Ris nonincreasing iff all functions f .D are right\u00adlinear and either noncyclic or nonincreasing. \nThe restriction to inductively sequential TRSs is not really neces\u00adsary (i.e., left-linear, constructor \nTRSs would suf.ce) but we im\u00adpose this condition because needed narrowing is only de.ned for this class \nof TRSs. On the other hand, right-linearity is not only necessary to guarantee quasi-termination (see \nbelow) but also for ensuring that no repeated computations are introduced by function unfolding. THEOREM \n10. If R is a nonincreasing TRS, then R is quasi\u00adterminating for any linear term w.r.t. needed narrowing. \nWe note that there is no clear relation between quasi-termination w.r.t. needed narrowing and related \nconditions in term rewriting. Consider, for instance, the following TRS: f(0,y) . yf(s(x),y) . f(x,s(y)) \nwhich is not nonincreasing, where 0,s .C and f .D. This TRS is trivially terminating w.r.t. rewriting \nsince the .rst parameter of function f strictly decreases with each recursive call. However, it is not \nquasi-terminating w.r.t. needed narrowing as witnessed by the following (in.nite) computation: f(x,y) \n{x7.s(x ' )} f(x ' ,s(y)) {x ' 7.s(x '' )} f(x '' ,s(s(y))) ... Related notions like size-change termination \n[33] (adapted to TRSs in [45]) are equally not useful for ensuring (quasi-)termination w.r.t. needed \nnarrowing, since they only ensure that some parameter decreases (but not all of them), which is not useful \nin our context where all parameters may be unknown (i.e., free variables). Right\u00adlinearity is an essential \nrequirement even for the simplest functions. Consider, for example, the following nonincreasing functions: \nf(0,y).yf(s(x),y).f(x,y) g(x). f(x,x) where 0,s .C and f,g .D. This is not a nonincreasing TRS since \nfunction g is not right-linear. Thus, quasi-termination w.r.t. needed narrowing is not ensured: g(x) \nid f(x,x) {x7.s(x ' )} f(x ' ,s(x ' )) '' '' {x ' 7.s(x '' )} f(x ,s(s(x ))) ... Clearly, the use of \nneeded narrowing is also crucial, i.e., quasi\u00adtermination for other narrowing strategies (e.g., innermost \nnarrow\u00ading) is not guaranteed. For instance, given the following quasi\u00adterminating TRS: f(x) . g(h(x)) \nh(0) . 0 g(x) . xh(s(x)) . s(h(x)) needed narrowing is quasi-terminating while an innermost strategy \nwould produce the following non-quasi-terminating derivation: f(x) id g(h(x)) {x7.s(x ' )} g(s(h(x ' \n))) {x ' 7.s(x '' )} g(s(s(h(x '' )))) ... The closest characterizations to ours have been presented \nby Wadler [46] and Chin and Khoo [13]. Wadler introduced the notion of treeless functions in order to \nensure the termination of deforesta\u00adtion [46]. Treeless functions are a subclass of our nonincreasing \nfunctions where, additionally, all function calls in the right-hand sides of the rules can only have \nvariable arguments. Chin and Khoo [13] introduced the class of nonincreasing consumers and proved that \nany set of mutually recursive functions that are nonincreasing consumers can be transformed into an equivalent \nset of treeless functions, so that deforestation can be applied. This characteriza\u00adtion differs from \nours mainly in two points. Firstly, Chin and Khoo only require linear function calls in the right-hand \nsides of the rules (rather than being linear the entire right-hand sides, as we impose). This relaxed \nde.nition, however, is not safe in our context. Con\u00adsider, e.g., the following nonincreasing consumers \naccording to Chin and Khoo [13]: f(x). c(g(x),x) g(s(x)). g(x) h(c(s(x),y)). x where c,s .C and f,g,h \n.D. Here, given the initial term h(f(x)), needed narrowing has an in.nite derivation which is not quasi-terminating: \nh(f(x)) id h(c(g(x),x)) {x7.s(x ' )} h(c(g(x ' ),s(x ' ))) {x ' 7.s(x '' )} h(c(g(x '' ),s(s(x '' )))) \n... And, secondly, Chin and Khoo do not accept nested function calls in the right-hand side of any program \nrule. In contrast, we accept ar\u00adbitrary (linear) terms in the right-hand sides of noncyclic functions, \nwhich allows us to cope with a wider range of functions. 5. From Online to Of.ine NPE The current formulation \nof the NPE scheme ensures termination online (see, e.g., [1, 2, 4]), i.e., appropriate termination tests \nand generalization operators are used during partial evaluation to guar\u00adantee that only a .nite number \nof distinct terms (modulo vari\u00adable renaming) are computed. As mentioned before, this scheme achieves \nsigni.cant optimizations but it is also very expensive (in terms of both time and space consumption) \nand, thus, it does not scale up well to realistic problems. In order to remedy this situa\u00adtion, in this \nsection we introduce a faster NPE method which en\u00adsures termination of.ine by including a pre-processing \nstage based on the notion of nonincreasing TRS. In principle, a naive NPE method could restrict the source \npro\u00adgrams to nonincreasing TRSs and, then, apply neither termination tests nor generalizations during \npartial evaluation since needed nar\u00adrowing computations would be quasi-terminating (cf. Theorem 10). \nThis would give rise to a very fast NPE tool only equality tests modulo variable renaming would be required \nbut, unfortunately, the class of acceptable programs would be too restrictive. Therefore, we now consider \nthe class of programs for which NPE is originally de.ned: inductively sequential programs a much broader \nclass of TRSs and de.ne an algorithm that anno\u00adtates the expressions which may cause the non-quasi-termination \nof needed narrowing computations. These annotations will be later used by an extended needed narrowing \nrelation in order to gener\u00adalize problematic subterms. We let F = F .{ }, where .F is a fresh symbol. \nGiven a TRS R, a term t is annotated by replac\u00ading t by (t). The following auxiliary functions will be \nuseful to manipulate annotated terms: gen(x)= x if x .V gen(h(tn)) = h(gen(tn)) if h .F, n . 0 gen( (t)) \n= y where y .V is a fresh variable i.e., given an annotated term t .T (F , V), the expression gen(t).T \n(F, V)returns a generalization of t by replacing anno\u00adtated subterms by fresh variables. aterms(x) = \nif x .V . . aterms(h(tn)) = ni=1 aterms(ti) if h .F, n . 0 aterms( (t)) = {t}.aterms(t) Here, the expression \naterms(t) .T (F , V) returns the set of annotated subterms (possibly containing annotations) in t . T(F \n, V). Let us illustrate the use of functions genand aterms with some simple examples: gen(f(x,g(h(y)))) \n= f(x,g(h(y))) gen(f(x, (g(h(y))))) = f(x,w) gen(f(x, (g( (h(y)))))) = f(x,w) aterms(f(x,g(h(y)))) = \n{} aterms(f(x, (g(h(y))))) = {g(h(y))} aterms(f(x, (g( (h(y)))))) = {g( (h(y))),h(y)} The following de.nition \nintroduces our transformation to annotate inductively sequential programs. Intuitively speaking, we annotate \nthose arguments of the topmost operation-rooted subterms in the right-hand sides of the rules that either \ncontain de.ned function symbols or break the nonincreasing property; then, every annotated subterm is \ntreated similarly to the original right-hand side (thus, nested annotations are possible); .nally, all \nrepeated occurrences but one of the same variable are annotated. Formally, DEFINITION 11 (ann(R)). Let \nR = {li . ri | i =1,. ..,k} be an inductively sequential TRS over F. The annotated TRS, ann(R), over \nF is given by the set of rules {li . ri ' | i = 1, ...,k}where ri' , i =1,. ..,k, is computed as follows: \n1. If root(li)is a noncyclic function, then ri ' is obtained from ri by annotating all occurrences of \nthe same variable but one (e.g., the leftmost one), so that gen(ri' )is a linear term. 2. If root(li)is \ncyclic, then ri ' is obtained from qs(li,ri)by anno\u00adtating the least number of variables such that gen(t)becomes \nlinear for all t .{qs(li,ri)}.aterms(qs(li,ri)). The de.ni\u00adtion of auxiliary function qsis shown in Fig. \n1.  Intuitively speaking, auxiliary function qsignores constructor sym\u00adbols until an operation-rooted \nsubterm f(t1,...,tn) is found. Then, for each argument ti, it proceeds (by calling qs ' ) as follows: \nif ti is a constructor term and all variables ful.ll the nonincreas\u00ading property, then ti remains unchanged; \notherwise, the considered subterm, ti, is annotated and the process is restarted for ti. Trivially, for \nany nonincreasing TRS R, we have ann(R)= R. Furthermore, if R is an inductively sequential TRS so is \nann(R), since the left-hand sides of the rules are not modi.ed. Note that De.nition 11 is nondeterministic \nsince it does not .x which variable occurrence should not be annotated when there are repeated occurrences \nof the same variable. In some cases, this decision can dramatically affect the result of a partial evaluation \n(see Sect 6.2). This situation could be improved in some cases by allowing the programmer to choose the \n(static) variable that should not be annotated. EXAMPLE 12. Consider the following inductively sequential \npro\u00adgram R: f(0,y) . y f(s(x),y) . g(x,f(x,s(y))) g(x,y) . g(y,x) where f,g .D and 0,s .C. The annotated \nTRS, ann(R), is as follows: f(0,y) . y f(s(x),y) . g(x, (f(x, (s(y))))) g(x,y) . g(y,x) Observe that \nrepeated occurrences of x in the second rule should not be annotated since aterms(g(x, (f(x, (s(y))))))= \n{f(x, (s(y))),s(y)} and, hence, gen(t)is linear for all t .{g(x, (f(x, (s(y))))),f(x, (s(y))),s(y)} i.e., \ng(x,w1), f(x,w2), and s(y)are linear terms, where w1 and w2 are fresh variables. Since partial computations \nare computed in the NPE scheme by means of needed narrowing, we now extend this relation in order to \ngeneralize annotated subterms (thus ensuring the termination of the partial evaluation process). DEFINITION \n13 (generalizing needed narrowing). Let R be an annotated inductively sequential TRS over F . The generalizing \nneeded narrowing relation, in symbols . , is de.ned as the least relation satisfying (needednarrowing) \ns p,R,s t if root(s).D and s .T (F, V) s . s t (generalization) t .{s}.aterms(s) if root(s).D and s .T \n(F, V) s . gen(t) (decomposition) s = c(t1,. ..,tn) . i .{1, ...,n} if root(s).C s . C ti A generalizing \nneeded narrowing derivation s . s * t is thus com\u00adposed of proper needed narrowing steps (for operation-rooted \nterms with no annotations), generalizations (for annotated terms), and constructor decompositions (for \nconstructor-rooted terms with no annotations), where s is the composition of the substitutions la\u00adbeling \nthe proper needed narrowing steps. Note that, since needed narrowing only computes a head normal form \n(i.e., a variable or a constructor-rooted term), the decomposition rule is required to ensure that all \ninner functions (if any) are eventually partially eval\u00aduated. Some examples of generalizing needed narrowing \ncomputa\u00adtions can be found in the next section. We also note that our generalization step is somehow \nequivalent to the splitting operation of conjunctive partial deduction (CPD) of logic programs [17]. \nWhile CPD considers conjunctions of atoms, we deal with terms possibly containing nested function symbols. \nTherefore, .attening a nested function call is basically equivalent to splitting a conjunction (in both \ncases some information is lost). The next result shows the correctness of the annotation algo\u00adrithm. \nTHEOREM 14. Let R be an inductively sequential TRS and t a linear term. Every generalizing needed narrowing \nderivation for t in ann(R)is quasi-terminating. 6. The Of.ine NPE Method in Practice In this section, \nwe .rst describe the complete of.ine NPE method based on annotated programs and generalizing needed narrowing. \nThen, we illustrate the new scheme by means of some selected ex\u00adamples. Finally, we provide a summary \nof the experiments con\u00adducted with a prototype implementation of the method which show the advantages \nof our approach in comparison with the previous online NPE method. 6.1 Overview of the Of.ine NPE Method \nIn our of.ine approach to NPE, given an inductively sequen\u00adtial TRS R, the .rst stage consists in computing \nthe annotated TRS: ann(R). Then, the second stage the proper partial evalu\u00adation takes the annotated \nTRS, ann(R), together with an initial   .. . t if t .V is a variable qs(l,t)= c(qs(l,tn)) if t = \nc(tn), c .C, and n . 0 f(t '' n) if t = f(tn), f .D, and ti = qs ' (l,ti) for all i =1, ...,n, n . 0 \n. t if t .T(C, V)is a constructor term and dv(pi,x)dv(t,x)for all x .Var(pi), i =1,...,n . qs ' (f(pn),t)= \n(qs(f(pn),t)) otherwise Figure 1. Auxiliary functions qsand qs ' (linear) term, t, constructs the (.nite) \ngeneralizing needed nar-(nondeterministic) mapping ren. is de.ned as follows: rowing tree for t in ann(R), \nand extracts the residual partially evaluated program. Essentially, residual programs are extracted by \nproducing a so\u00ad s if s .V c(ren.(tn)) if s = c(tn),c .C, and n . 0 T rowing step st in the considered \ngeneralizing needed nar\u00ad . s called resultant, s(s) . rann(t), for each proper needed nar-ren.(s)= . \n' (.(t)) if there exists a term t . rowing tree, where function rann simply removes the occurrences \n. ..... ..... such that s = .(t)and . ' = of  in a term. In general, however, the left-hand sides \nof these {x . ren.(.(x))|x .Dom(.)} rules need not be of the form f(s1,...,sn), where si are construc\u00adtor \nterms, since they may contain nested de.ned function symbols. Therefore, a renaming of terms is often \nmandatory. The following de.nitions from [5] formalize the notion of renaming: DEFINITION 15 (independent \nrenaming [5]). An independent re\u00adnaming . for a set of terms T is a mapping from terms to terms de.ned \nas follows: for all term t . T, .(t)= t if t = f(xn), where f .D and xn are different variables, and \n .(t)= ft(xn), otherwise, where xn are the distinct variables of t in the order of their .rst occurrence \nand ft .D is a fresh function symbol.  Observe that function calls whose arguments are different variables \nare not renamed since it is not necessary. EXAMPLE 16. Consider the set of terms T = {f(x,y),g(h(x),y),s(c(x),x)} \n where f,g,h, s .D are de.ned functions and c .C is a con\u00adstructor symbol. Then, the following mapping \n. is an independent renaming for T: . = { f(x, y) . g(h(x),y) . s(c(x), x) . f(x, y), g ' (x, y), s ' \n(x) } Basically, given the annotated program ann(R)and a linear term t, the partial evaluation stage \nproceeds by constructing the general\u00adizing needed narrowing tree for t in ann(R), where, additionally, \na test is included to check whether a variant of the current term has already been computed and, if so, \nstop the derivation. The quasi\u00adtermination of generalizing needed narrowing computations (The\u00adorem 14) \nguarantees that the tree thus constructed is .nite. Once the tree is built, we compute an independent \nrenaming .for the set of terms {s |s . s t}, i.e., for the terms to which a proper needed narrowing step \nis applied. While the mapping . suf.ces to rename the left-hand sides of resultants, the right-hand sides \nrequire a more elaborated mapping, ren., that recursively replaces each call in the term by a call to \nthe corresponding renamed function. Formally, DEFINITION 17 (renaming mapping [5]). Let T be a .nite \nset of EXAMPLE 18. Consider the set of terms T and the independent renaming of Example 16. Given the \nterm g(h(x),f(a,s(c(b),b))), where a,b .C are constructor symbols, function ren. returns the renamed \nterm g ' (x,f(a,s ' (b))). Now, the of.ine NPE method can be formalized as follows: DEFINITION 19 (of.ine \nNPE). Let Rbe an inductively sequential TRS and f(xn)a linear term4 with f .D. The of.ine NPE of R w.r.t. \nf(xn)is obtained as follows: 1. First, we compute the annotated TRS ann(R). 2. Then, we construct a \n(.nite) generalizing needed narrowing tree, t, for f(xn) in ann(R), where each derivation stops whenever \nit reaches a constructor term or an operation-rooted term that is a variable renaming of some previous \nterm in the same (or a previous) derivation. 3. Finally, the residual TRS contains a (renamed) rule \n s(.(s)). ren.(rann(s ' )) for each proper needed narrowing step s . s s ' in t. Here, . is an independent \nrenaming of {s |s . s s ' . t}. For simplicity, in the de.nition above, we extract a resultant from each \nsingle needed narrowing step. Clearly, more re.ned algo\u00adrithms for extracting resultants from a generalizing \nneeded narrow\u00ading tree are possible; e.g., in many cases, one can extract a single resultant associated \nto a sequence of narrowing steps rather than to a single narrowing step. In fact, the implemented system \nfollows such a re.ned strategy. Now we state the correctness and termination of this partial evaluation \nmethod. THEOREM 20. Let Rbe an inductively sequential TRS and f(xn) a linear term with f .D. The algorithm \nof Def. 19 always terminates computing an inductively sequential TRS R ' such that needed narrowing computes \nthe same results for f(xn)in R and in R ' . 6.2 Selected Examples In this section, we illustrate the \nof.ine NPE method presented so far by means of some selected examples. 4 This is not a restriction since \none can consider an arbitrary term t by simply adding a new function de.nition f(xn) . t to R, where \nxn are terms and . an independent renaming of T. Given a term s, the the different variables of t. main(x) \nid t pow(x,s(s(0))) id t x \u00d7 (pow(x,s(0)))  777 7777  x \u00d7 w1 pow(x,s(0)) {x7.0}{x7.s(w2)} // / id / \nt / 0 w1 + (w2 \u00d7 w1) x \u00d7 (pow(x,0)) / / // //// / / w1 + w3 w2 \u00d7 w1 x \u00d7 w5 pow(x,0) {w17.0}{w17.s(w4)} \n// / id / t / w3 s(w4 + w3) s(0) C t w4 + w3 Figure 2. Generalizing needed narrowing tree for main(x) \n Program specialization. Our .rst example illustrates the use of the of.ine NPE method for program specialization. \nConsider the following TRS which has been annotated according to Def. 11: main(x) . pow(x,s(s(0))) pow(x,0) \n. s(0) pow(x,s(n)) . x \u00d7 (pow(x,n)) 0\u00d7 m . 0 s(n)\u00d7 m . m + (n \u00d7m) 0+m . m s(n)+m . s(n + m) Given the \ninitial term main(x), we construct the generalizing needed narrowing tree depicted in Fig. 2. Then, the \nassociated residual TRS contains the following rules: main(x) . pow2(x) pow2(x) . x \u00d7pow1(x) pow1(x) \n. x \u00d7pow0(x) pow0(x) . s(0) together with the original de.nitions of \u00d7 and + . The consid\u00adered independent \nrenaming is as follows: . = { main(x) . main(x), pow(x,s(s(0))) . pow2(x), pow(x,s(0)) . pow1(x), pow(x, \n0) . pow0(x), x \u00d7y . x \u00d7 y, x +y . x + y } Furthermore, these four rules can easily be simpli.ed by \nusing a standard post-unfolding transition compression [32] as follows: main(x). x \u00d7 (x \u00d7 s(0)) since \nfunctions pow2, pow1, and pow0 are only intermediate func\u00adtions (i.e., there is only one call to any \nof them). This simple exam\u00adple shows that, despite the annotation of some subterms, the spe\u00adcialization \npower of the original (online) NPE is not lost in our of.ine approach. Deforestation. Our second example \nis concerned with Wadler s deforestation to eliminate intermediate data structures [46]. Here, lenapp(x,y) \nid t len(app(x,y)) {x7.[]} 77 {x7.z:zs} 77 77 77 len(y) len(z : app(zs,y)) / {y7.[]}{y7.w:ws} / / id \n/ / t  / 0 s(len(ws)) s(len(app(zs,y))) CC tt len(ws) len(app(zs,y)) Figure 3. Generalizing needed narrowing \ntree for lenapp(x,y) we consider the following TRS R: lenapp(x,y) . len(app(x,y)) len([]) . 0 len(x : \nxs) . s(len(xs)) app([],y) . y app(x : xs,y) . x : app(xs,y) where lenapp(x,y)computes the length of \nthe concatenation of lists x and y. This function is not ef.cient since an intermediate data structure \n(the concatenation of x and y) is built. Since R is already nonincreasing, we have that ann(R)= R. Given \nthe ini\u00adtial term lenapp(x,y), we construct the generalizing needed nar\u00adrowing tree depicted in Fig. \n3. By using the following independent renaming: . = { lenapp(x,y) . lenapp(x,y), len(app(x,y)) . la(x,y), \nlen(y) . len(y), len(z : app(zs,y)) . la2(z,zs,y) } the associated residual TRS is as follows: lenapp(x,y) \n. la(x,y) la([],y) . len(y) la(z : zs,y) . la2(z, zs,y) la2(z,zs,y) . s(la(zs,y)) together with the original \nde.nition of function len. As in the previous example, a simple post-unfolding transformation would eliminate \nthe intermediate function la2. Note that the residual TRS is completely deforested (i.e., no intermediate \nlist is built). Higher-order removal. Our last example consist in the elimina\u00adtion of higher-order functions. \nIn some programming languages, higher-order features are defunctionalized [42, 47], i.e., they are expressed \nby means of a .rst-order program with an explicit ap\u00adplication operator.5 For instance, the following \nTRS, which has al\u00adready been annotated according to Def. 11, includes the de.nition of the well-known \nhigher-order function map: minc(x) . map(inc0,x) map(f,[]) . [] map(f,x : xs) . apply( (f),x): map(f,xs) \ninc(x) . s(x) apply(inc0,x) . inc(x) 5 As in the language Curry, we do not allow the evaluation of higher-order \ncalls containing free variables as functions (i.e., such calls are suspended to avoid the use of higher-order \nuni.cation). A more .exible strategy can be found in [10]. minc(x) id t map(inc0,x) {x7.[]} 77 {x7.y:ys} \n 777777 [] apply( (inc0),y): map(inc0,ys) C 777 C 77777 apply( (inc0),y) map(inc0,ys) / / /// apply(w,y) \ninc0 Figure 4. Generalizing needed narrowing tree for minc(x) Here, we used the explicit application \noperator apply together with the partial function application inc0 (a constructor symbol). Observe that, \nin this example, we have annotated the leftmost occurrence of variable f in the third program rule. This \nis essential to obtain a .rst-order de.nition for map(inc0,x). Indeed, by an\u00adnotating the second occurrence \nof variable f, the original program is basically returned by the partial evaluator. Given the initial \nterm minc(x), the generalizing needed nar\u00adrowing tree of Fig. 4 is built. Note that apply(w,y)is not \nfurther reduced because, as mentioned before, this higher-order call con\u00adtains a free functional variable \nand, thus, its evaluation suspends (which means that the original de.nition of apply should also be included \nin the residual program). Given the following independent renaming: . = { minc(x) . minc(x), map(inc0,ys) \n. mapinc(ys), inc(y) . inc(y), apply(w,y) . apply(w,y) } the residual TRS computed by of.ine NPE is \nas follows: minc(x) . mapinc(x) mapinc([]) . [] mapinc(y : ys) . apply(inc0,y): mapinc(ys) inc(y) . s(y) \napply(inc0,y) . inc(y)  Finally, by using a simple post-unfolding simpli.cation we get the following \nTRS: minc(x) . mapinc(x) mapinc([]) . [] mapinc(y : ys) . s(y): mapinc(ys) where the explicit application \noperator apply is no longer needed. We note that this transformation often achieves signi.cant speedups \nin practice (see, e.g., [1]). 6.3 Experimental Evaluation The of.ine NPE method outlined in Sect. 6.1 \nhas been imple\u00admented in the declarative multi-paradigm language Curry [21]. The sources of the partial \nevaluator and a detailed explanation of the benchmarks considered below are publicly available from http://www.dsic.upv.es/users/elp/german/offpeval/. \nThe of.ine NPE tool is purely declarative and accepts Curry programs containing additional features like \nhigher-order func\u00adtions, several built-in functions, etc. Table 1 shows the results of some benchmarks: \nackermann: This is the well-known Ackermann s function spe\u00adcialized for an input argument greater than \nor equal to 10. allones: The aim of this benchmark is to automatically produce a new function that transforms \nall elements of a list into 1 by .rst computing the length of the original list and, then, constructing \na new list of the same length whose elements are 1. This is a typical deforestation example [46]. fliptree: \nAnother typical deforestation example. Here, the aim is to .ip a tree structure twice so that the original \ntree is ob\u00adtained; no static values are provided. foldr.allones: The goal of this benchmark is the specializa\u00adtion \nof a function that concatenates a number of lists and, then, transforms all elements into 1. The original \nfunction is de.ned by means of the higher-order combinator foldr. The specializa\u00adtion considers that \none of the lists is known. foldr.sum: In this benchmark, we produce a specialized function to sum the \nelements of a list (with a given pre.x) by using the higher-order function foldr. fun inter: This benchmark \nconsists in the specialization of sim\u00adple functional interpreter for a given program. gauss: Our goal \nin this benchmark is the specialization of the well-known Gauss function to consider natural numbers \ngreater than or equal to 5. kmp matcher: A naive pattern matcher specialized for a given pattern. This \nbenchmark is known as the KMP-test [15]. power: The specialization shown in Section 6.2 for a .xed expo\u00adnent \nof 6. For each benchmark, we show the size (in bytes) of each program (codesize), the time for executing \nthe previous online NPE tool (onlineNPE), the time for executing the new of.ine NPE tool de\u00adscribed so \nfar (offlineNPE), where we show both the time for analyzing and annotating the original program (ann) \nand for per\u00adforming partial computations and extracting the residual program (mix), as well as the speedups \nachieved by the programs special\u00adized with each technique (speedup1 and speedup2); speedups are given \nby orig/spec, where orig and spec are the absolute run times of the original and specialized programs, \nrespectively. Times are expressed in milliseconds and are the average of 10 executions on a 2.4 GHz Linux-PC \n(Intel Pentium IV with 512 KB cache). Run\u00adtime input goals were chosen to give a reasonably long overall \ntime. The programs were executed with the Curry to Prolog compiler of PAKCS [27]. As it can be seen in \nTable 1, we have reduced the partial eval\u00aduation time to approximately 25% of the original NPE tool, \nwhich means that our main goal has been achieved. As for the speedups, we note that most of the benchmarks \nwere specialization prob\u00adlems (rather than optimization problems), which explains the good results achieved \nby our of.ine NPE tool. Let us remark, how\u00adever, that the new method is not able to pass the so-called \nKMP\u00adtest [15] (see benchmark kmp matcher). There are two main requirements for passing the KMP test: \na good propagation of information and a powerful termination analysis that avoids too much generalization. \nWhile our of.ine scheme propagates infor\u00admation as well as the previous online approach (which does pass \nthe KMP test), our (implicit) termination analysis is much simpler. It would be interesting to check \nwhether a mixed online/of.ine approach could be useful here. Our partial evaluator deals well with arithmetic \nfunctions (benchmark ackermann), with the sim\u00adpli.cation of higher-order calls (benchmarks foldr.allones \nand foldr.sum), and with a simple functional interpreter (benchmark fun inter), where speedups are not \nshown since the execution time of the specialized programs is zero (i.e., the input program to the interpreter \nhas been fully evaluated).   Table 1. Benchmark results benchmark codesize onlineNPE speedup1 offlineNPE \nspeedup2 (offline) (bytes) (ms.) (online) ann (ms.) mix (ms.) ackermann 1496 20290 1.006 100 590 4.750 \nallones 1191 180 1.065 50 200 1.050 fliptree 1861 1940 0.985 100 240 0.977 foldr.allones 2910 3633 1.024 \n120 430 2.034 foldr.sum 3734 6797 1.311 170 3340 1.293 fun inter 4266 28955  160 5190 gauss 1241 11090 \n1.040 100 757 1.013 kmp matcher 3222 11670 5.346 157 9410 1.219 power 1693 160 3.087 110 280 1.012 Average \n2402 9413 1.858 119 2271 1.668  7. Related Work and Discussion Despite the relevance of narrowing as \na symbolic computation mechanism, we .nd in the literature very few works devoted to analyze its termination. \nFor instance, Dershowitz and Sivakumar [20] de.ned a narrowing procedure that incorporates pruning of \nsome unsatis.able goals. Similar approaches have been presented by Chabin and R\u00b4ety [12], where narrowing \nis directed by a graph of terms, and by Alpuente et al. [3], where the notion of loop\u00adcheck is introduced. \nAlso, Antoy and Ariola [8] introduced a sort of memoization technique for functional logic languages \nso that, in some cases, a .nite representation of an in.nite narrowing space is achieved. All these techniques \nare online, since they use informa\u00adtion about the term being narrowed. On the other hand, Christian [14] \nintroduced a characterization of TRSs for which narrowing terminates. Basically, it requires the left-hand \nsides to be .at, i.e., all arguments are either variables or ground terms. None of these works considered \nquasi-termination nor presented a method to an\u00adnotate TRSs so that termination is enforced. Other related \nworks come from the extensive literature on par\u00adtial evaluation. Within the logic programming paradigm, \nDecorte et al. [18] studied the quasi-termination of tabled logic programs in order to port specialization \ntechniques from standard logic programs to tabled ones. They introduced the characterization of quasi-acceptable \nprograms and proved that this class of programs guarantees quasi-termination. However, determining whether \na program is quasi-acceptable is not easy to check (the authors sketched how standard termination analysis \ncould be extended). Within the functional setting, Holst [28] introduced a suf.cient condition for quasi-termination \nin order to ensure the termination of partial evaluation (which was then used by Glenstrup and Jones \n[24] to de.ne a BTA algorithm ensuring the termination of of.ine partial evaluation). Holst also presented \na static analysis based on abstract interpretation in order to check the suf.cient condition for quasi-termination. \nSimilarly to [18], the presented conditions are based on the semantics and, thus, are generally dif.cult \nto analyze. In contrast, our approach relies on a simple syntactic character\u00adization which is generally \nless precise but very easy to check. In fact, the closest approaches to our work are the syntactic character\u00adizations \ngiven by Wadler [46] and Chin and Khoo [13], which have been already discussed in Sect. 4. In summary, \nwe have introduced a novel characterization for TRSs that ensures the quasi-termination of needed narrowing \ncomputations. This is a dif.cult problem of independent interest that has not been tackled before. Since \nthe considered class of TRSs is too restrictive, we then considered inductively sequential programs a \nmuch broader class and introduced an algorithm that annotates those subterms which may cause the non-quasi\u00adtermination \nof needed narrowing. We also introduced a general\u00adizing extension of needed narrowing which is guided \nby program annotations. Finally, we described how our new developments can be used to de.ne a correct \nand terminating NPE scheme that en\u00adsures termination of.ine. Preliminary experiments conducted on a wide \nvariety of programs are encouraging and demonstrate the usefulness of our approach. Although we considered \ninductively sequential systems as pro\u00adgrams and needed narrowing [9] as operational semantics, our de\u00advelopments \ncould easily be extended to overlapping inductively sequential systems and inductively sequential narrowing \n[7]. The main difference is that overlapping systems allow the use of an ex\u00adplicit disjunction operator \nwhich introduces additional don t-know nondeterminism. In this context, introducing a function with a \ndis\u00adjunction in the right-hand side, e.g., f(x) . t1 or t2, is basically equivalent to writing the following \nsingle rules: f(x) . t1 f(x) . t2 Since our termination characterization mainly depends on how the function \nparameters change from the left-to the right-hand side of a rule, the treatment of disjunctions in overlapping \nsystems presents no additional problems; basically, a disjunction operator could be considered as a constructor \nsymbol. Positive supercompilation [44] shares many similarities with NPE since driving, the symbolic \ncomputation mechanism of posi\u00adtive supercompilation, is equivalent to needed narrowing on com\u00adparable \nprograms. Therefore, our results could easily be transferred to the setting of positive supercompilation. \nRegarding future work, one of the most recent approaches to ensure the (quasi-)termination of functional \nprograms is based on size-change graphs [33] (which have been already used in the con\u00adtext of partial \nevaluation in [31]). An interesting topic for future work is thus the use of size-change graphs for de.ning \nmore pre\u00adcise though computationally more expensive annotation algo\u00adrithms. On the other hand, our algorithm \nfor annotating TRSs is in\u00addependent of the term considered for partial evaluation. This means that a \nTRS only needs to be annotated once and, then, it can be par\u00adtially evaluated w.r.t. different terms \nwithout computing new anno\u00adtations. However, it also means that we are not exploiting the known structure \nof the term considered for partial evaluation. Hence, it would be interesting to study the combination \nof our .rst stage with traditional binding-time analysis. Here, our functional logic setting poses new \ndemands for binding-time analysis due to the use of logical variables and nondeterministic functions. \nFor this pur\u00adpose, we plan to investigate techniques for the binding-time analy\u00adsis of logic programs \nwithin the partial deduction technique (like, e.g., [16, 35]).  Acknowledgments We would like to thank \nthe anonymous referees for many helpful comments and suggestions.  References [1] E. Albert, M. Hanus, \nand G. Vidal. A Practical Partial Evaluation Scheme for Multi-Paradigm Declarative Languages. Journal \nof Functional and Logic Programming, 2002(1), 2002. [2] E. Albert and G. Vidal. The Narrowing-Driven \nApproach to Func\u00adtional Logic Program Specialization. New Generation Computing, 20(1):3 26, 2002. [3] \nM. Alpuente, M. Falaschi, M.J. Ramis, and G. Vidal. Narrowing Approximations as an Optimization for Equational \nLogic Programs. In Proc. of PLILP 93, pages 391 409. Springer LNCS 714, 1993. [4] M. Alpuente, M. Falaschi, \nand G. Vidal. Partial Evaluation of Functional Logic Programs. ACM TOPLAS, 20(4):768 844, 1998. [5] \nM. Alpuente, M. Hanus, S. Lucas, and G. Vidal. Specialization of Functional Logic Programs Based on Needed \nNarrowing. Theory and Practice of Logic Programming, 5(3):273 303, 2005. [6] S. Antoy. De.nitional trees. \nIn Proc. of the 3rd Int l Conference on Algebraic and Logic Programming (ALP 92), pages 143 157. Springer \nLNCS 632, 1992. [7] S. Antoy. Optimal Non-Deterministic Functional Logic Compu\u00adtations. In Proc. of \nthe Int l Conference on Algebraic and Logic Programming (ALP 97), pages 16 30. Springer LNCS 1298, 1997. \n [8] S. Antoy and Z.M. Ariola. Narrowing the Narrowing Space. In In Proc. of PLILP 97, pages 1 15. Springer \nLNCS 1292, 1997. [9] S. Antoy, R. Echahed, and M. Hanus. A Needed Narrowing Strategy. Journal of the \nACM, 47(4):776 822, 2000. [10] S. Antoy and A. Tolmach. Typed Higher-Order Narrowing without Higher-Order \nStrategies. In Proc. of FLOPS 99, pages 335 352. Springer LNCS 1722, 1999. [11] F. Baader and T. Nipkow. \nTerm Rewriting and All That. Cambridge University Press, 1998. [12] J. Chabin and P. R\u00b4ety. Narrowing \ndirected by a graph of terms. In Proc. of the 4th Int l Conf. on Rewriting Techniques and Applications \n(RTA 91), pages 112 123. Springer LNCS 488, 1991. [13] W.N. Chin and S.C. Khoo. Better Consumers for \nProgram Specializations. Journal of Functional and Logic Programming, 1996(4), 1996. [14] J. Christian. \nSome termination criteria for narrowing and E\u00adnarrowing. In Proc. of CADE-11, pages 582 588. Springer \nLNCS 607, 1992. [15] C. Consel and O. Danvy. Partial Evaluation of Pattern Matching in Strings. Information \nProcessing Letters, 30:79 86, 1989. [16] S.J. Craig, J. Gallagher, M. Leuschel, and K.S Henriksen. Fully \nAu\u00adtomatic Binding Time Analysis for Prolog. In Proc. of LOPSTR 04. Springer LNCS, 2005. To appear. [17] \nD. De Schreye, R. Gl\u00a8uck, J. J\u00f8rgensen, M. Leuschel, B. Martens, and M.H. S\u00f8rensen. Conjunctive Partial \nDeduction: Foundations, Control, Algorihtms, and Experiments. Journal of Logic Programming, 41(2&#38;3):231 \n277, 1999. [18] S. Decorte, D. De Schreye, M. Leuschel, B. Martens, and K.F. Sagonas. Termination Analysis \nfor Tabled Logic Programming. In Proc. of LOPSTR 97, pages 111 127. Springer LNCS 1463, 1997. [19] N. \nDershowitz. Termination of Rewriting. Journal of Symbolic Computation, 3(1&#38;2):69 115, 1987. [20] \nN. Dershowitz and G. Sivakumar. Goal-Directed Equation Solving. In Proc. of 7th National Conf. on Arti.cial \nIntelligence, pages 166 170. Morgan Kaufmann, 1988. [21] M. Hanus (ed.). Curry: An Integrated Functional \nLogic Language. Available at: http://www.informatik.uni-kiel.de/~mh/curry/. [22] Yoshihiko Futamura. \nPartial Evaluation of Computation Process An Approach to a Compiler-Compiler. Higher-Order and Symbolic \nComputation, 12(4):381 391, 1999. Reprint of article in Systems, Computers, Controls 1971. [23] E. Giovannetti, \nG. Levi, C. Moiso, and C. Palamidessi. Kernel Leaf: A Logic plus Functional Language. Journal of Computer \nand System Sciences, 42:363 377, 1991. [24] A.J. Glenstrup and N.D. Jones. BTA Algorithms to Ensure Termination \nof Off-Line Partial Evaluation. In Proc. of the 2nd Int l Andrei Ershov Memorial Conference on Perspectives \nof System Informatics, pages 273 284. Springer LNCS 1181, 1996. [25] M. Hanus. The Integration of Functions \ninto Logic Program\u00adming: From Theory to Practice. Journal of Logic Programming, 19&#38;20:583 628, 1994. \n[26] M. Hanus and C. Prehofer. Higher-Order Narrowing with De.nitional Trees. Journal of Functional Programming, \n9(1):33 75, 1999. [27] M. Hanus (ed.), S. Antoy, M. Engelke, K. H\u00a8oppner, J. Koj, P. Niederau, R. Sadre, \nand F. Steiner. PAKCS 1.6.0: The Portland Aachen Kiel Curry System User Manual. Technical report, University \nof Kiel, Germany, 2004. [28] C.K. Holst. Finiteness Analysis. In Proc. of Functional Programming Languages \nand Computer Architecture, pages 473 495. Springer LNCS 523, 1991. [29] G. Huet and J.J. L\u00b4evy. Computations \nin orthogonal rewriting systems, Part I + II. In J.L. Lassez and G.D. Plotkin, editors, Computational \nLogic Essays in Honor of Alan Robinson, pages 395 443, 1992. [30] N.D. Jones. Transformation by Interpreter \nSpecialisation. Science of Computer Programming, 52:307 339, 2004. [31] N.D. Jones and A. Glenstrup. \nPartial Evaluation Termination Analysis and Specialization-Point Insertion. ACM TOPLAS, 2005. To appear. \n[32] N.D. Jones, C.K. Gomard, and P. Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall, \nEnglewood Cliffs, NJ, 1993. [33] C.S. Lee, N.D. Jones, and A.M. Ben-Amram. The Size-Change Principle \nfor Program Termination. In ACM Symposium on Principles of Programming Languages (POPL 01), volume 28, \npages 81 92. ACM press, 2001. [34] M. Leuschel. Homeomorphic Embedding for Online Termination of Symbolic \nMethods. In The Essence of Computation, Complexity, Analysis, Transformation. Essays Dedicated to Neil \nD. Jones, pages 379 403. Springer LNCS 2566, 2002. [35] M. Leuschel, J. J\u00f8rgensen, W. Vanhoof, and M. \nBruynooghe. Of.ine specialisation in Prolog using a hand-written compiler generator. TPLP, 4(1-2):139 \n191, 2004. [36] J.W. Lloyd and J.C. Shepherdson. Partial Evaluation in Logic Programming. Journal of \nLogic Programming, 11:217 242, 1991. [37] R. Loogen, F. L\u00b4opez-Fraguas, and M. Rodr\u00b4iguez-Artalejo. A \nDemand Driven Computation Strategy for Lazy Narrowing. In Proc. of PLILP 93, pages 184 200. Springer \nLNCS 714, 1993. [38] W. Lux. M\u00a8unster Curry v0.9.8: User s Guide. Technical report, University of M\u00a8unster, \nGermany, 2004. [39] J.J. Moreno-Navarro and M. Rodr\u00b4iguez-Artalejo. Logic Programming with Functions \nand Predicates: The language Babel. J. Logic Programming, 12(3):191 224, 1992. [40] Simon Peyton-Jones, \neditor. Haskell 98 Language and Libraries The Revised Report. Cambridge University Press, 2003. [41] \nJ.G. Ramos, J. Silva, and G. Vidal. Fast Narrowing-Driven Partial Evaluation for Inductively Sequential \nSystems. Technical report, Technical University of Valencia, 2005. Available at http://www.dsic.upv.es/~gvidal. \n[42] J.C. Reynolds. De.nitional interpreters for higher-order programming languages. Higher-Order and \nSymbolic Computation, 11(4):363 297, 1998. Reprinted from the proceedings of the 25th ACM National Conference \n(1972). [43] J.R. Slagle. Automated Theorem-Proving for Theories with Simpli.ers, Commutativity and Associativity. \nJournal of the ACM, 21(4):622 642, 1974. [44] M.H. S\u00f8rensen, R. Gl\u00a8uck, and N.D. Jones. A Positive Supercompiler. \nJournal of Functional Programming, 6(6):811 838, 1996. [45] R. Thiemann and J. Giesl. Size-Change Termination \nfor Term Rewriting. In Proc. of RTA 03, pages 264 278. Springer LNCS 2706, 2003. [46] P.L. Wadler. Deforestation: \nTransforming programs to eliminate trees. Theoretical Computer Science, 73:231 248, 1990. [47] D.H.D. \nWarren. Higher-Order Extensions to Prolog Are they needed? Machine Intelligence, volume 10. Ellis Horwood, \n1982.  \n\t\t\t", "proc_id": "1086365", "abstract": "Narrowing-driven partial evaluation is a powerful technique for the specialization of (first-order) functional and functional logic programs. However, although it gives good results on small programs, it does not scale up well to realistic problems (e.g., interpreter specialization). In this work, we introduce a faster partial evaluation scheme by ensuring the termination of the process <i>offline</i>. For this purpose, we first characterize a class of programs which are <i>quasi-terminating</i>, i.e., the computations performed with needed narrowing&#8212;the symbolic computation mechanism of narrowing-driven partial evaluation&#8212;only contain finitely many different terms (and, thus, partial evaluation terminates). Since this class is quite restrictive, we also introduce an annotation algorithm for a broader class of programs so that they behave like quasi-terminating programs w.r.t. an extension of needed narrowing. Preliminary experiments are encouraging and demonstrate the usefulness of our approach.", "authors": [{"name": "J. Guadalupe Ramos", "author_profile_id": "81100093098", "affiliation": "I.T. La Piedad, La Piedad, Mich., M&#233;xico", "person_id": "P745800", "email_address": "", "orcid_id": ""}, {"name": "Josep Silva", "author_profile_id": "81339528446", "affiliation": "Tech. University of Valencia, Valencia, Spain", "person_id": "PP14031990", "email_address": "", "orcid_id": ""}, {"name": "Germ&#225;n Vidal", "author_profile_id": "81100420507", "affiliation": "Tech. University of Valencia, Valencia, Spain", "person_id": "PP14148602", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086394", "year": "2005", "article_id": "1086394", "conference": "ICFP", "title": "Fast narrowing-driven partial evaluation for inductively sequential programs", "url": "http://dl.acm.org/citation.cfm?id=1086394"}