{"article_publication_date": "09-12-2005", "fulltext": "\n High-level Views on Low-level Representations Iavor S. Diatchki Mark P. Jones Rebekah Leslie OGI School \nof Sci. &#38; Eng. at OHSU OGI School of Sci. &#38; Eng. at OHSU Portland State University diatchki@cse.ogi.edu \nmpj@cse.ogi.edu rebekah@cs.pdx.edu Abstract This paper explains how the high-level treatment of datatypes \nin functional languages using features like constructor functions and pattern matching can be made to \ncoexist with bitdata.Weuse this term to describe the bit-level representations of data that are required \nin the construction of many different applications, includ\u00ading operating systems, device drivers, and \nassemblers. We explain our approach as a combination of two language extensions, each of which could \npotentially be adapted to any modern functional language. The .rst adds simple and elegant constructs \nfor manip\u00adulating raw bit.eld values, while the second provides a view-like mechanism for de.ning distinct \nnew bitdata types with .ne-control over the underlying representation. Our design leverages poly\u00admorphic \ntype inference, as well as techniques for improvement of quali.ed types, to track both the type and the \nwidth of bitdata struc\u00adtures. We have implemented our extensions in a small functional language interpreter, \nand used it to show that our approach can handle a wide range of practical bitdata types. Categories \nand Subject Descriptors D.3.2 [Language Classi.ca\u00adtions]: Applicative (functional) languages; D.3.3 [Language \nCon\u00adstructs and Features]: Data types and structures General Terms Design, Languages Keywords Data representation, \nbit manipulation, bitdata, bit\u00ad.elds, pattern matching, views, polymorphism, quali.ed types 1. Introduction \nAlgebraic datatypes promote a high-level view of data that hides low-level implementation details. Storage \nfor new data structures is allocated automatically by applying constructor functions, while pattern matching \nprovides a way to inspect values without concern for their machine-level representation. By abstracting \nfrom such details, we can obtain code that is more succinct and easier to reuse. Many applications, however, \nrequire the use of data that is stored in bit .elds and accessed as part of a single machine word. Standard \nexamples can be found in operating system APIs; in the control register formats used by device drivers; \nand in programs like assemblers that work with machine code instruction encodings. We will refer to examples \nlike this collectively as bitdata. In this paper, we explain how a modern functional language like ML \nor Haskell can be extended with mechanisms for spec- Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 05 September 26 28, 2005, Tallinn, Estonia. Copyright c 2005 \nACM 1-59593-064-7/05/0009. . . $5.00. ifying and using bitdata. Our design provides .ne-control over \nthe choice of low-level representations while also supporting the higher-level programming notations \nand constructs that are associ\u00adated with strongly typed, algebraic datatypes. The original motiva\u00adtion \nfor this work grew out of two ongoing projects using Haskell for device driver and operating system implementation, \nrespec\u00adtively. Although there are occasional differences in syntax and se\u00admantics, the examples in this \npaper still follow the basic conven\u00adtions of Haskell. We have strived, however, for a general approach \nthat is broadly compatible with any modern functional language, and also for a .exible approach that \nis useful in many different application domains. Our goal in this paper is to treat .xed-width bitdata \ntypes of the kind that can potentially be stored in a single machine register. This emphasis distinguishes \nour work from ap\u00adproaches, some of which will be described in Section 5, that focus instead on handling \nvariable length streams of bitdata, as used in applications like multimedia and compression codecs, and \nmachine language instruction stream encoders. 1.1 Examples of Bitdata Much of the time, the speci.c bit \npatterns that are used in bitdata encodings are determined by external speci.cations and standards to \nwhich the application developer must conform. For example, an operating system standard might .x the \nencoding for the set of .ags that are passed to a system call, while the datasheet for a hardware device \nspeci.es the layout of the .elds in each control register. In the general case, bit-level encodings may \nuse tag bits that is, speci.c patterns of 0s and 1s in certain positions to distinguish between different \ntypes of value, leaving the bits that remain to store the actual data. For example, some bits in the \nencoding of a machine code instruction might identify a particular opcode, while others specify the operands. \nWe will now describe a small collection of examples to illustrate some of the challenges of dealing with \nbitdata. PCI Device Addresses PCI is a high performance bus standard that is widely used on modern PCs \nfor interconnecting chips, ex\u00adpansion boards, and processor/memory subsystems. Individual de\u00advices in \na given system are identi.ed by a 16 bit address that con\u00adsists of three .elds: an eight bit bus identi.er, \na .ve bit device code, and a three bit function number. We can represent the lay\u00adout of these .elds in \na block diagram that speci.es the name and width (as a subscript) of each .eld, from which we can infer \nthe corresponding positions of each .eld. bus (8) dev (5) fun (3) We draw diagrams like this with the \nmost signi.cant bit on the left and the least signi.cant bit on the right. With this encoding, function \n3 of device 6 on bus 1 is represented by the 16 bit value that is written 0x0133 in hexadecimal or as \n00000001 00110 011 in binary, using spaces to show .eld boundaries. Timeouts in the L4 Microkernel L4 \nwas developed as a minimal, .exible, and high performance operating system kernel [13]. The most recent \nversion of the L4 speci.cation includes a detailed ABI (application binary interface) that describes \nthe format for system call arguments and results [17]. For example, one of the parameters in the interprocess \ncommunication (IPC) system call is a timeout period, which speci.es how long the sender of a message \nshould wait for a corresponding receive request in another process. Simplifying the details a little, \nthere are three forms of timeout value, as shown in the following diagrams: now =0 period =2em \u00b5s never \n= 8 0 0 1 (5) 0 (10) e (5) m (10) 0 (16) There are two special values here: A now timeout speci.es that \na send operation should abort immediately if no recipient is al\u00adready waiting, while a timeout of never \nspeci.es that the sender should wait inde.nitely. All other time periods are expressed using a simple \n(unnormalized) .oating point representation that can en\u00adcode time periods, at different levels of granularity, \nfrom 1\u00b5supto (210 - 1)231 \u00b5s (a period slightly exceeding 610hours). There is clearly some redundancy \nin this encoding; a period of 2\u00b5s can be represented with m =2and e =0or with m =1and e =1. Moreover, \nthe representations for now and never overlap with the representations for general time periods; for \nexample, a sixteen bit period with e =0and m =0must be interpreted as never and not as the 0\u00b5s time that \nwe might calculate from the formula 2em\u00b5s. While this detail of the encoding may seem counter-intuitive, \nit was likely chosen because many programs use only never timeouts, and most machines can test for this \nspecial case a zero word very quickly with a single machine instruction. One .nal point to note is that \nthe most signi.cant bit in all of these encodings is zero. In fact the L4 ABI also provides an interpretation \nfor timeouts with the most signi.cant bit set, which it uses to indicate an absolute rather than a relative \ntime. Because there are places in the ABI where only relative times are permitted, we prefer to treat \nthese as a separate type. Instruction Set Encodings for the Z80 The Zilog Z80 is an 8 bit microprocessor \nwith a 16 bit address bus that was .rst released in 1976, and continues to .nd uses today as a low-cost \nmicrocon\u00adtroller [19]. The Z80 has 252 root instructions, each of which is represented by a single byte \nopcode. There are, of course, 256 pos\u00adsible byte values, and the four bytes that do not correspond to \nin\u00adstructions are used instead as pre.xes to access an additional 308 instructions. For example, the \n0xCB pre.x byte signals that the next byte in the instruction stream should be interpreted as a bit manip\u00adulation \ninstruction using one of four different formats: 00 s (3) r (3) 01 r (3) n (3) 10 r (3) n (3) 11 r (3) \nn (3) SHIFT s, r BIT r, n RES r, n SET r, n n r s 000 B RLC 001 C RRC 010 D RL 011 E RR 100 H SLA 101 \nL SRA 110 (HL) 111 A SRL The most signi.cant two bits in each case are tag bits that serve to distinguish \nbetween shift, bit testing, bit setting, and bit resetting instructions, respectively. The remaining \nportion of each byte is split into two three-bit .elds, each of which speci.es either a bit number n, \na register/operand r, or a shift type s as shown by the table on the right. Note that there are three \ndistinct types for n, r, and s, all of which are encoded in just three bits, and that the appropriate \ninterpretation of the lower six bits in each byte is determined by the value of the two tag bits. 1.2 \nThe Perils of Bit Twiddling From a high-level, it is clear that bitdata structures can have quite a lot \nin common with the sum-of-products algebraic datatypes that are used in modern functional languages: \nwhere necessary, each encoding uses some parts of the data to distinguish between different kinds of \nvalue (the sum), each of which may use other bits for zero or more data .elds (the product). In practice, \nhowever, programmers usually learn to manipu\u00adlate bitdata using so-called bit twiddling techniques that \ninvolve combinations of shifts, bitwise logical operators, and carefully chosen numeric constants. Some \nof the more common idioms of this approach include clearing the ith bit in a word x using x &#38;= ~(1 \n<< i), or extracting the most signi.cant byte from a 32 bit word w using (w >> 24) &#38; 0xff. With experience, \nexam\u00adples like these can become quite easy for programmers to recognize and understand. However, in general, \nbit twiddling leads to code that is hard to read, debug, and modify. One reason for this is that bit \ntwiddling code can overspecify and obfuscate the semantics of the operation that it implements. Our two \nexamples show how a simple operation, such as clearing a single bit, can be obscured behind a sequence \nof arguably more complex steps. As a result, human readers must work harder to read and understand the \neffect of this code. Compilers too must rely on sophisticated optimization and instruction selection \nschemes to recover the intended semantics and, where possible, substitute more direct implementations. \nBit twiddling idioms can also result in a loss of type informa\u00adtion, and hence reduce the bene.ts of \nstrong typing in detecting certain kinds of program error at compile-time. The bit pattern that is used \nto program a device register may, for example, place con\u00adceptually different types of value in different \n.elds. Bit twiddling, however, usually bypasses this structure, treating all data homo\u00adgeneously as some \nkind of machine word, with few safeguards to ensure that .elds are accessed at the correct offset, with \nthe correct mask, and with appropriately typed contents. Some of the most widely used systems programming \nlanguages, notably C/C++ and Ada, do provide special syntax for describing and accessing bit .elds, and \nthese go some way to addressing the problems of raw bit twiddling. In C/C++, however, the primary pur\u00adpose \nof bit .elds is to allow multiple data values to be packed into a single machine word, and speci.c details \nof data layout, includ\u00ading alignment and ordering, can vary from one implementation to the next. As a \nresult, different C/C++ compilers will, in general, require different renderings of the same bitdata \nstructure to achieve the correct layout. Ada improves on this by allowing programmers to provide explicit \nand more portable representation speci.cations for user-de.ned datatypes. These languages, however, do \nnot typ\u00adically provide direct mechanisms for dealing with tag bits, or for using them to support pattern-matching \nconstructs that automate the task of distinguishing between different forms of data. In practice, programs \nthat make signi.cant use of bitdata often de.ne symbolic constants (representing .eld offsets and masks, \nfor example) and basic functions or macros that present a higher-level (and possibly more strongly typed) \ninterface to speci.c bit twid\u00addling operations. This approach also isolates portability concerns in a \nsoftware layer that can be rewritten to target different compilers or platforms. In effect, this amounts \nto de.ning a simple, domain\u00adspeci.c language for each application which can work quite well in a single \nprogram, but involves duplication of effort in identify\u00ading, implementing, and learning to use the set \nof abstractions that are provided. These problems can be addressed by designing a sin\u00adgle domain-speci.c \nlanguage that can be used across multiple ap\u00adplications. The PADS and SLED systems (described in Section \n5) follow this approach using separate programming and data descrip\u00adtion languages. One of the goals \nof the current paper is to investi\u00adgate a different point in the design space that treats data description \nas an integral part of the programming language by providing gen\u00aderal and .exible programming constructs \nfor working with bitdata. 1.3 Low-level Representations for Bitdata In this section, we consider how \nissues of data representation should in.uence our design of bitdata language extensions. Sup\u00adpose, for \nexample, that we want to write programs that manipulate PCI addresses, as described in Section 1.1. If \nthe language had al\u00adready been extended with an appropriate collection of Intn types representing integers \nof different bit widths, then it would be pos\u00adsible to represent PCI addresses with a standard Haskell \ndata type: data PCIAddr = PCIAddr { bus :: Int8, dev :: Int5, fun :: Int3 } Ideally, we might hope that \na smart enough compiler would gen\u00aderate code using 16 bit values to represent values of type PCIAddr \nwith exactly the same layout that was suggested by the earlier dia\u00adgram. For Haskell, at least, this \nis impossible because every type including PCIAddr as well as each of its component types Int8, Int5, \nand Int3 has an additional bottom element, .. It follows, therefore, that the semantics of PCIAddr has \nmore values than can be represented in 16 bits. This speci.c problem can (almost) be ad\u00addressed by inserting \nstrictness annotations in front of each of the component types, as in the following variation: data PCIAddr \n= PCIAddr { bus :: !Int8, dev :: !Int5, fun :: !Int3 } Given this de.nition, it is conceivable that a \ncompiler might be able to infer that it is safe to use our preferred sixteen bit representation for PCI \naddresses. (Technically, this would require a lifted semantic domain to account for the remaining bottom \nelement in this modi\u00ad.ed PCIAddr type.) However, there is nothing in the semantics of Haskell to guarantee \nthis, and, to the best of our knowledge, no existing Haskell (or ML) compiler even attempts it. The representation \nthat a compiler chooses for a given data type is usually only important when values of that type must \nbe commu\u00adnicated with the outside world. Using either of the previous repre\u00adsentations for PCIAddr, we \ncould de.ne functions like the follow\u00ading to marshal back and forth between external and internal rep\u00adresentations \nof PCI addresses (we take some liberties with syntax here, using >> and << for the corresponding shift \noperators of C and &#38; and | for bitwise and and or operators, respectively): toPCIAddr :: Int16 -> \nPCIAddr toPCIAddr addr = PCIAddr { bus = (addr >> 8) &#38; 0xff, dev = (addr >> 3) &#38; 0x1f, fun=addr \n&#38;7} fromPCIAddr :: PCIAddr -> Int16 fromPCIAddr pci = (pci.bus << 8) | (pci.dev << 3) | pci.fun In \neffect, functions like these might be used to package bit twid\u00addling code in a single place so that a \nhigher-level PCIAddr repre\u00adsentation can be used exclusively in the remaining portions of the Haskell \ncode. In theory, at least, if we follow this approach, then the details of the representation that the \nHaskell compiler chooses for PCIAddr would not be signi.cant. In practical terms, however, there are \nsome serious conse\u00adquences. For example, it would be unfortunate if we ended up using these functions \nto import PCI addresses into the functional world, and then export them out again, without ever having \nmade any use of their structure. In that case, all the resources that are spent in decoding, storing, \nand then re-encoding would be wasted. This is one example of the overhead that we inevitably incur if \nwe try to maintain multiple representations of a single type. Furthermore, while it is not dif.cult to \nwrite functions like toPCIAddr and fromPCIAddr, it is tedious work, especially when dealing with more \ncomplex examples. It is also error prone be\u00adcause there is nothing to ensure that the functions we de.ne \nare mutual inverses. Clearly, it would be better if we could arrange to have the code for these functions \ngenerated automatically by a care\u00adfully designed tool that would guarantee the required behavior. The \nderiving mechanism of Haskell cannot be used for this purpose because standard datatype de.nitions do \nnot provide information about layout. It is possible to generate the necessary code from speci.cations \nin a separate interface de.nition language (IDL) that provides these details. Instead of using an external \nIDL, we ex\u00adtend the programming language to enable programmers to describe the layout of data directly. \nIn this way the compiler can use the same underlying representation for both the internal and external \nforms of PCIAddr. The functions toPCIAddr and fromPCIAddr could be implemented as identity functions \nand their uses optimized away in the back end of the compiler. Our conclusion from this discussion is \nthat we are likely to in\u00adcur signi.cant overheads if the representation that is used inside the functional \nlanguage differs from the representation that is used externally. Given that many bitdata formats are \noften determined by third parties, we cannot expect to change them to match the representations used \nby our compilers; instead, the only option is to change our compilers so that they will use the external \nrepre\u00adsentation directly. In this respect, we share the goal of data-level interoperability that guided \nthe design of PADS [4] and of Blume s foreign function interface for ML [3]. However, neither of these \nsystems deals with the construction and matching of bit-level struc\u00adtures that motivates our work. We \nalso consider that it is bene.cial to maintain strong typing distinctions between different kinds of \nbitdata values. In this way, we are able to catch more type errors at compile-time while allowing compiler-generated \nanalogues of the from and to functions to be used where necessary as explicit type conversions, without \nincurring runtime overhead. 1.4 Describing Bitdata Layout We have concluded that our compilers will \nneed to use the same external representation for bitdata as everybody else. But how will such a compiler \ndetermine what that external representation might be? Looking again at the examples in Section 1.1, it \nis easy to construct the following algebraic datatypes for the L4 time type and for the Z80 encoding \nof bit twiddling instructions: data Time = Now | Period { e::Int5, m::Int10 } | Never data BitOp = Shift \n{shift::S, reg::R} | BIT {reg::R, n::Int3} | RES {reg::R, n::Int3} | SET {reg::R, n::Int3} dataS =RLC|RRC|RL|RR|SLA|SRA|SRL \ndataR =A|B|C|D|E|H|L|MemHL However, it is also easy to see that there is not enough information in these \nde.nitions for a compiler, no matter how sophisticated it might be, to infer the corresponding external \nrepresentations. For example, there are no indications in these de.nitions that Time values should be \nrepresented using 16 bits; that Now and Never should be coded as special cases of Period; that the bit \npattern 110 should not be used in the encoding of S; or that the bit pattern 111, rather than 000, should \nbe used to represent A. While a compiler might be able to infer the appropriate layout for PCIAddr, the \ngeneral case requires a more expressive notation for specifying bitdata representations. One possibility \nwould be to adopt a separate (and perhaps language-neutral) interface de.nition language (IDL) for describing \nlow-level encodings of data. In this scenario, we would also need a compiler for compiling IDL de\u00adscriptions \ninto stub code that could be used to import the bitdata types and operations into the functional language \nas foreign types. 1.5 Our Approach In this paper, we present a new approach to specifying and working \nwith bitdata that is structured as two language extensions. These ex\u00adtensions allow programmers to capture \nessential details of low-level data formats directly within the functional language. This approach is \nattractive because it enables tighter integration of bitdata with the rest of the language than would \nbe possible using an external IDL alone, including stronger type checking, the possibility of check\u00ading \nfor non-exhaustive and/or overlapping pattern matches, and the potential for more aggressive and effective \noptimization. Our .rst extension supports basic bit-level manipulation with a family of primitive Bit \nn types, a syntax for bit literals, and a # operator that can be used both for concatenating bit values \nand, in the context of pattern matching, splitting bit values. The following examples give a brief .avor \nof the programs that we can write with this language extension: mkPCIAddr :: Bit 8 -> Bit 5 -> Bit 3 \n-> Bit 16 mkPCIAddr bus dev fun = bus # dev # fun bitOpType :: Bit 8 -> Bit 2 bitOpType (tag # args) \n= tag Programs like this can be parsed, type checked, and executed using hobbit (a higher-order language \nwith bit-level data), which is a prototype interpreter that we have built to test and evaluate our bit\u00addata \nextensions. The following extract shows how the mkPCIAddr and bitOpType functions might be used in an \ninteractive session with hobbit (the > character is the hobbit prompt): > show (mkPCIAddr 1 6 3) \"B0000000100110011\" \n> show (bitOpType 0x7f) \"B01\" The output from these examples also shows the syntax that is used for \nbit literals; an initial B followed by a sequence of binary digits. Our second extension adds a mechanism \nfor de.ning new bit\u00addata types that are distinguished from their underlying representa\u00adtion. In special \ncases, layout can be inferred from the way that the type is de.ned. For example, our system will infer \nthe intended bit representation of a PCIAddr from the following de.nition: bitdata PCIAddr = PCIAddr \n{ bus::Bit 8, dev::Bit 5, fun::Bit 3 } In general, it is necessary to specify layout explicitly by annotating \neach constructor with an appropriate as clause. The following de.nition shows how Time can be described \nin this notation. bitdata Time = Now as B0 # 1 # (0::Bit 10) | Period {e::Bit 5, m::Bit 10} as B0 # e \n# m | Never as 0 Note that the representation for Never is written simply as 0; the fact that a sixteen-bit \nzero is required here is inferred automatically from the other two as clauses. The encoding of Z80 bit \ntwiddling instructions can be described in a similar way. In this case, we specify the appropriate bit \npatterns for each of the constructors in the enumeration types S and R. bitdata BitOp = Shift { shift::S, \nreg::R } as B00 # shift # reg | BIT { reg::R, n::Bit 3 } as B01 # reg # n | RES { reg::R, n::Bit 3 } \nas B10 # reg # n | SET { reg::R, n::Bit 3 } as B11 # reg # n bitdata S = RLC as B000 | RRC as B001 | \nRL as B010 | RR as B011 | SLA as B100 | SRA as B101 | SRL as B111 bitdata R =AasB111|BasB000|CasB001|D \nasB010 | E as B011 | H as B100 | L as B101 | MemHL as B110 With these de.nitions, we can construct byte \nvalues for Z80 in\u00adstructions using expressions like Shift{shift=RRC, reg=D} and SET{n=6, reg=A}, but \nattempts to construct encodings using ar\u00adguments of the wrong type as in SET{n=6, reg=B010} are treated \nas type errors, even where values might otherwise be con\u00adfused because their representations have the \nsame number of bits. Our system also includes generic toBits and fromBits opera\u00adtors that can be used \nto convert arbitrary bitdata to and from its un\u00adderlying bit-level representations. These are generalizations \nof the toPCIAddr and fromPCIAddr operations described in Section 1.3. The following example shows how \nthe .rst of these function can be used to inspect the bit pattern for one particular Z80 instruction: \n> show (toBits (SET{n=6, reg=A})) \"B11111110\" The remaining sections of this paper present our approach \nin more detail, beginning with an overview of the language design, includ\u00ading its type system, in Section \n2. An extended example, demonstrat\u00ading some of the more advanced features of our design, is presented \nin Section 3. Details of our current implementation are described in Section 4, while Section 5 documents \nrelated work. We conclude with ideas for future work in Section 6.  2. Language design This section \nprovides a more thorough description of our design, covering features for bit manipulation in Section \n2.1, and mecha\u00adnisms for de.ning new bitdata types in Section 2.2. New syntax and typing rules are presented \nas we go along. The core of our design is the small functional language de\u00adscribed in Figure 1. However, \nour extensions are largely indepen\u00addent of the details of this language, and it should be quite easy \nto integrate them with other functional languages. As usual, a program consists of a number of (possibly \nrecursive) declarations. e = x variable . = *|... kind | ee application s = .a. \u00afscheme \u00afp .t | e :: \nt type sig. t = tt |a|c type | ... p = ... predicate Figure 1. The core language The type system is based \non the Hindley-Milner system [14], extended with quali.ed types [8]. We use kinds (.) to classify different \ntypes. Types that contain values are classi.ed by *, and function kinds classify type constructors. The \ncore language should have at least the following type constants: Bool : * Boolean value (.): *.*.* function \nspace We use schemes (s) to type expressions that have multiple simple types (t ). Such expressions are \nsaid to be polymorphic. When a polymorphic expression appears in a particular context, the type variables \n(a) in the schema are instantiated to concrete types. In certain situations, it is convenient to impose \nrestrictions on how type schemes may be instantiated. We do this by qualifying type schemes with predicates \n(p). An expression of a quali.ed type may only appear in contexts where the type variables are replaced \nby concrete types that satisfy the predicates. In the following sections, we present a set of rules for \nsolving predicates. The rules are of the form . fp, which states that, from the set of assumptions .,we \ncan conclude that the predicate p holds. 2.1 Bit Manipulation We introduce a new type constant called \nBit that we use to type bit sequences. Bit is a type constructor that, given a natural number, produces \nthe type of bit sequences of the corresponding length. To complete the de.nition of Bit we introduce \na new kind N, that is inhabited by the natural numbers. Bit : N .* bit sequence 0, 1, 2,... : N natural \nnumber For example, bytes are 8-bit sequences and have type Bit 8. In this paper we focus on manipulating \nbit sequences that will .t in the registers of a CPU or a hardware device. It is therefore desirable \nto restrict the lengths of bit sequences that can be used in a program. An elegant way to achieve this, \nwithout compromising the generality of the system, is to use quali.ed types. We introduce a predicate, \ncalled Width, that can only be solved for natural numbers that are valid bit sequence widths. n =max \nWidth : N .Prop [Width] . fWidth n The kind Prop classi.es predicates. In the rule Width, . is a set \nof assumptions and n is a natural number. The rule states that, to solve the predicate Width n, the number \nn should be smaller than max, which might be chosen as the size of the largest hardware register. In \nthis way, implementations that are optimized to work on a particular machine will reject programs that \nattempt to create bit sequences that are too large. An implementation may instead allow the use of arbitrary \n(.xed-size) bit sequences by choosing to solve arbitrary Width predicates. Now we can introduce standard \noperators on Bit values: (&#38;) : .a. Width a . Bit a .Bit a .Bit a (==) : .a. Width a . Bit a .Bit \na .Bool Other common operations on bit sequences include logical and arithmetic operations (e.g., disjunction \nand addition), relational operations (e.g., equality tests and comparisons), and sequence manipulation \n(e.g., concatenation and splitting). Readers familiar with the Haskell class system may think of these \nconstants as the methods of a class called Width. Furthermore, both signed and unsigned versions of Int, \nand the basic operations on these types, are easy to de.ne using Bit types. 2.1.1 Literals One way to \nintroduce a bit sequence in a program is to use a binary literal. This notation is useful when a bit \nvector is used as a name, for example, to identify a device, a vendor, or perhaps a particular command \nthat needs to be sent to a device. A binary literal is written as a B followed by a number in base two. \nAn n digit binary literal belongs to the type Bit n, as long as n is a valid width. Leading zeros are \nimportant because they affect the type of the literal. Here are some examples of binary literals: >:tB11 \nBit 2 > :t B011 Bit 3 > :t B000000000000000000000000000000000 FAIL 33 is not a valid width This example \nuses the :t <expr> command in hobbit to show the type of an expression. In our implementation, the largest \nal\u00adlowed width is 32, so the last example is not type correct. Binary literals may be used in both expressions \nand patterns. Indeed, we can think of Bit n as an algebraic data type that has the n-digit binary literals \nas constructors. The only exception is the case when n =0, where the name of the constructor is NoBits.To \nbe consistent, we could have used the name B for the inhabitant of this type but we found that this can \nbe confusing. It is often convenient to think of bit sequences as numbers and we introduce numeric literals \nto accommodate this. An interesting challenge is to allow numeric literals for all types of the form \nBit n, without introducing a baroque notation. We do this by overload\u00ading the notation for octal, hexadecimal, \nand decimal literals, as in the design of Haskell [11]. The trick is to introduce a new primitive function \nfromLit : .a. Width a .Bit max .Bit a (correspond\u00ading to fromInteger in Haskell). A numeric literal n \nin the text of a program, can then be treated as syntactic sugar for the constant fromLit applied to \nthe value n of type Bit max. Bit sequences represent numbers using the standard two s complement encoding. \nUsually, the type of an overloaded literal can be inferred from the context where it is used. If this \nis not the case, programmers can use a type signature to indicate the number of bits they need. Nu\u00admeric \nliterals may also be used in patterns and will match only if the argument is a value that is the same \nas the literal. Here are some examples that illustrate how literals work: >:t 2 (Width a) => Bit a >:t \n2&#38;B11 Bit 2 Notice that, when used on its own, the number 2 has a polymorphic type the system is \ntelling us that 2 has type Bit a for any a that is a valid width. However, if used in a particular context, \n2 will be converted to the correct length using the function fromLit. 2.1.2 Joining and Splitting Bit \nSequences Another common programming task is joining and splitting bit se\u00adquences. The usual way of doing \nthis is to use shift and mask op\u00aderations to get bits into the correct positions. This is a complicated \nway to achieve a conceptually simple task, and it is all too easy to shift a bit too much, or to use \nthe wrong bit mask. To make this task simpler, we introduce the operator (#) to join sequences. One way \nto type this operator is to use an addition operator at the type level, and to use the type (#) : .a\u00df. \nBit a .Bit \u00df .Bit (a + \u00df).At .rst, this seems quite attractive because it captures our intuition of what \n(#) does. However the situation is more complex than it ap\u00adpears, especially in a system that supports \ntype inference. Having the (+) operator at the type level makes it more dif.cult to decide if two types \nare the same: checking for structural equivalence is not suf.cient. There are also situations in which \nit is not clear what type should be inferred. Consider, for example, the following de.nition: maskxy= \n(x # y)&#38; B101 This mask function may be applied to any two bit sequences whose lengths add up to \n3 but we cannot express this using an addition operator at the type level. For this reason, we choose \ninstead to introduce an addition predicate: (_ + _ = _): N .N .N .Prop n1 + n2 = n3 . fWidth ni i .{1, \n2, 3}[Add] . fn1 + n2 = n3 The predicate n1 + n2 = n3 is a relation between natural numbers that holds \nwhen the sum of the .rst two numbers is the same as the third number. We also require that all the numbers \nare valid bit sequence widths. This trick of representing functions with relations should be very familiar \nto Prolog programmers and to users of Haskell s class system. Now we can give the join operator the following \ntype: (#) : .a\u00df.. (a + \u00df = .) .Bit a .Bit \u00df .Bit . Here are some examples that use this operator: > :t \nmask (a+b =3)=> Bita->Bitb-> Bit 3 > show (B100 # B111) \"B100111\" The .rst line shows the type that \nthe system inferred for mask. The second line shows the result of joining together two sequences. We \nuse pattern matching to split bit sequences. The split pattern has the form p1 # p2. This pattern matches \nbit vectors whose most signi.cant part matches p1, and least signi.cant part matches p2. For example, \na function to get the upper 16 bits of a 32 bit quantity could be written like this: upper16 :: Bit 32 \n-> Bit 16 upper16 (x # _) = x Note that (#) patterns do not specify how to split a value into two parts, \nbut simply what the two parts should match. How the sequence will be split depends on the types of the \nsub-patterns p1 and p2. These types may be determined using type inference, or explicit signatures in \nthe patterns. For example, if we de.ne another function called upper that is the same as upper16, but \nwe omit the type signature we get the following type: > :t upper (a+b =c)=> Bitc->Bit a An interesting \npoint about this type is that the order of the variables in the predicate a + b = c is important. If \nwe were to switch a and b around we would get the type of the function that accesses the lower bits of \na bit sequence. As an example of a situation where we need to use a signature in a pattern, consider \nthe function that extracts the bus component of a PCI address: pciBus :: Bit 16 -> Bit 5 pciBus ((dev \n:: Bit 8) # bus # fun) = bus If we omit the annotation on the dev pattern, the system fails with the \nfollowing error: FAIL Cannot solve goals: ?a + ?b = 16, ?c + 5 = ?a The system needs to split a 16 bit \nquantity into two parts: one of width a (dev # bus), and one of width b (fun). It also has to split the \na component into two parts: one that is c bits wide (dev), and one that is 5 bits wide (bus). There is \nnot enough information in the program to determine how this splitting should be done, which is why we \nget the type error. Signature patterns resemble the explicit types on functions in the presentation of \nthe lambda calculus \u00e0 la Church. We do not cur\u00adrently allow type variables in signature patterns but \nthis restriction could be lifted using scoped type variables [12].  2.2 User-De.ned Bitdata Types In \nthe context of systems programming, bit sequences are often used as representations for values that have \nmore structure. To enable programmers to capture this extra structure we introduce bitdata declarations \nto the language (Fig. 2). The grammar is speci.ed using extended BNF notation: non-terminals are in italics, \nand terminals are in a bold font; constructs in brackets are optional, while constructs in braces may \nbe repeated zero or more times. The syntax resembles data declarations in Haskell, because this is a \ncommon way to specify structured data. However, while there are many similarities between data and bitdata \ndeclarations, there are also important differences. For example, the type de.ned by a bitdata declaration \nis not the free algebra of its constructors (see Section 2.2.4). Instead, the type provides a kind of \nview [18] on the underlying bit sequences. We use constructors to construct and recognize bit sequences, \nwhile .elds provide a means to access or update contiguous sub-sequences. bdecl = bitdata con = cdecl \n{| cdecl} type decl. cdecl = con { [fdecls] } [as layout][if expr] constr. decl. fdecls = fdecl {, fdecl} \n.eld decl. fdecl = label [= expr] :: t layout = layout # l.eld | layout :: t .eld layout l.eld = lit \n| _ | ( layout ) Figure 2. The syntax of user de.ned data types To illustrate how bitdata declarations \nwork, we present some de.nitions for a device driver for a NE2000 compatible network card [15]. The details \nof how the hardware works are not important; our goal is to illustrate the features of bitdata declarations. \nAs a .rst example, consider a type that de.nes a number of commands: bitdata RemoteOp = Read as B01 | \nWrite as B10 | SendPacket as B11 This is essentially an enumeration type. The de.nition introduces a \nnew type constant RemoteOp and three constructors Read, Write, and SendPacket. The as clauses specify \na bit pattern for each con\u00adstructor, which will be used to construct values and to recognize them in \npatterns. All of the constructors in a given bitdata dec\u00adlaration must have the same number of bits in \ntheir representation. The following examples use these constructors: > :t Read RemoteOp > show Read \"B01\" \n> Read &#38; B00 FAIL Type mismatch: RemoteOp vs. Bit 2 The last example emphasizes the point that, \neven though Read is represented with the bit sequence 01, it is not of type Bit 2. There is a close relation \nbetween the bit sequence types Bit n and bitdata types like RemoteOp, captured by the following con\u00adstants: \ntoBits : .a\u00df. BitRep a\u00df . a .Bit \u00df fromBits : .a\u00df. BitRep a\u00df . Bit \u00df .a The function toBits converts \nvalues into their bit vector represen\u00adtations, while the function fromBits does the opposite, turning \nbit sequences into values of a given type. We may think of toBits as a pretty-printer, and fromBits as \na parser, that use bits instead of characters. These functions are very useful when a program\u00admer needs \nto interact with the outside world. The function toBits is used when data is about to leave the system, \nand the function fromBits is used when data enters the system. Not all types in the language may be converted \nto bit sequences. We use the predicate BitRep to restrict the contexts where the functions toBits and \nfromBits may appear: .fWidth n BitRep :*.N .Prop [Bit] .fBitRep (Bit n)n The predicate BitRep t n states \nthat the type t is represented with n bits. We can represent Bit n types in n bits, as long as n is a \nvalid width, as described in the rule Bit. We may also solve BitRep predicates for types de.ned with \nbitdata declarations. These declarations introduce new assumptions to the system that enable us to solve \nthe predicate directly by assumption. We can use the :a command to list the assumptions that are in scope: \n>:a BitRep RemoteOp 2 So far, we have de.ned only one type, so there is only one as\u00adsumption. For readers \nfamiliar with Haskell we note that the au\u00adtomatic introduction of assumptions is like deriving an instance \nof the BitRep class for each bitdata declaration. A critical design decision that shows up in the type \nof fromBits is that we do not include the possibility of failure: fromBits will always produce a value \nof the target type, even if the input sequence does not correspond to anything that may be created using \nthe constructors of that type. We call such values junk, and they will not match any constructor pattern \nin a function de.nition. Programmers may, however, use variable or wildcard patterns to match these values. \nConsider, for example, de.ning a function that will present human readable versions of the values in \nthe RemoteOp type: showOp Read = \"Read\" showOp Write = \"Write\" showOp SendPacket = \"SendPacket\" showOp \n_ = \"Unknown\" Now we can experiment with different expressions: > showOp (fromBits B01) \"Read\" > showOp \n(fromBits B00) \"Unknown\" > show (toBits (fromBits B00 :: RemoteOp)) \"B00\" The .rst example recognizes \nthe bit-pattern for Read. The second example does not match any of the constructors as none of them are \nrepresented with B00. The last example illustrates that we can convert a bit sequence into a value of \ntype RemoteOp and then back into the original bit sequence without loss of information. An alternative \ndesign decision would be to adopt a checked se\u00admantics for fromBits in which an exception is signaled \nwhen an unmatched bit pattern is passed in as an argument. We chose to use the unchecked semantics because \nit is simple, has practically no overhead (in hobbit, it is implemented as an identity function), and \ndoes not rely on support for exceptions in the core language. However, our design has all the machinery \nthat would be needed to generate checked versions of fromBits or even to derive isJunk predicates that \ncould be used to test for junk at runtime. Clearly, programmers must allow for the possibility that data \nvalues ob\u00adtained from the \"real-world\" using fromBits may not be valid. The choice between the different \ndesigns that we have sketched here is about .nding the most appropriate and/or convenient way to handle \nthis. For hobbit, we have chosen an approach that re\u00adquires programmers to deal with invalid data, where \nappropriate, by including equations with wildcards in function de.nitions, as in the code for showOp. \nWe expect that fromBits and toBits are inverses of each other, in the sense that the equation toBits \n(fromBits n)=n holds. This is useful because it enables programmers to propagate junk values to other \nparts of the system without changing them. Saying that toBits and fromBits are inverses suggests that \nthe equation fromBits (toBits n)=n also holds. But what do we mean by equality in this case? We use operational \nequivalence we consider two expressions to be the same if we can replace the one with the other in any \npiece of program. Because expressions of bitdata types are represented with bit patterns then two expressions \nare the same if they are represented with the same bit pattern: x =y = toBits x =toBits y. Using this \nde.nition for equality, we can see that the second equation follows from the .rst. The type RemoteOp \ncaptures only a fragment of the DMA com\u00admands available on NE2000 cards. The full set of DMA commands \nis described in the following de.nition: bitdata DMACmd = Remote { op :: RemoteOp } as B0 # op | AbortDMA \nas B1 # _ This de.nition uses some more features of bitdata declarations. In general, constructors may \nhave a number of .elds that describe sub-components of the value. For example, the constructor Remote \nhas one .eld called op of type RemoteOp. Only types for which the BitRep predicate can be solved may \nbe used in .eld declarations. The reason for this is that the .elds become a part of the represen\u00adtation \nof the value, and so we need to be able to come up with a bit pattern for them. As we already saw, RemoteOp \nis a type de.ned with a bitdata declaration, and so the above declaration is valid. To construct values \nwith .elds, programmers may use the no\u00adtation C{li =ei}, where C is the name of a constructor, li are \nits .elds, and ei are the values for the .elds. The order of the .elds is not signi.cant. There is also \na corresponding pattern C{li =pi}that may be used to check if a value matches the C constructor and the \n.elds match the patterns pi. The following de.nition is a function that will change remote read commands \ninto remote write commands and leave all other DMA commands unchanged: readToWrite (Remote { op=Read \n}) = Remote { op=Write } readToWrite x = x The .elds of a constructor in a bitdata declaration may con\u00adtain \ndefault values. The default value for a .eld is written after the .eld name, and should be of the same \ntype as the .eld. If a pro\u00adgrammer does not initialize a .eld while creating a value with a particular \nconstructor, then the .eld will be initialized with the de\u00adfault value for the .eld. If the .eld does \nnot have a default value, then the program is invalid and the system will report a compile time error. \n2.2.1 The as Clause The syntax of as clauses is more general than what we have seen so far. A layout \nspeci.cation may contain literals, .eld names, and wildcards (_), separated by #. Field names must appear \nexactly once, but can be in any order. Type signatures are also permitted in the as clause. The representation \nfor a constructor is obtained by placing the elements in the layout speci.cation sequentially, with the \nleft-most component in the most signi.cant bits of the repre\u00adsentation. For example, the layout speci.cation \nfor the constructor Remote says that we should place 0 in the most signi.cant bit and that we should \nplace the representation for the .eld op next to it: > show (Remote { op = Read }) \"B001\" The as clause \nis also used to derive tests that will recognize val\u00adues corresponding to the constructor. The matching \nof a pattern C{li =pi}proceeds in two phases: .rst, we see if the value is a valid C-value, and then \nwe check that the listed .elds match their corresponding patterns. The tests to recognize C-values check \nif the bits of a value corresponding to literals in the as clause match. For example, to check if a value \nis a Remote-value we need to check that the most signi.cant bit is 0. Wild-cards in the layout speci.cations \nrepresent don t care bits. They do not play a role in pattern matching. For value con\u00adstruction they \nhave an unspeci.ed value. The only constraint on a concrete implementation is that the don t care bits \nfor a partic\u00adular constructor are always the same. This is necessary to make toBits a proper function. \nFor example, the AbortDMA constructor only speci.es that the most signi.cant bit of the command should \nbe 1 and the rest of the bits are not important. Constructors that have no as clause are laid-out by \nplacing their .elds sequentially, as listed in the declaration. This is quite con\u00advenient for types that \ndo not contain any fancy layout. Following this rule, the representation of constructors with no .elds, \nand no as clause, is simply NoBits, the value of type Bit 0. Such exam\u00adples are not common, but this \nbehavior has some surprising conse\u00adquences. Consider, for example, the de.nition: bitdata MyBool = MyFalse \n| MyTrue This is legal, but it is probably not what the user intended: both constructors end up being \nrepresented with NoBits and are thus the same. Our implementation examines bitdata declarations for constructors \nwith overlapping representations, and warns the pro\u00adgrammer to alert them of potential bugs, like MyBool \nabove. 2.2.2 Records So far, we have seen how to access the .elds of constructors using pattern matching. \nThis approach is convenient in many situations but also has a drawback: if many functions need to access \nthe .elds of a value, each of them will have to pattern match to .rst ensure that the value has the expected \nformat. Instead, we would like to have a mechanism that enables us to check that a value is of a particular \nform once, and then we should be able to simply access the .elds without any additional overhead. To \nachieve this, bitdata declarations introduce a type constant for constructors that have .elds. Consider, \nfor example, a fragment of an encoding for the Z80 bit twiddling instructions: bitdata Instr = LD { dst::Reg, \nsrc::Reg } as B01 # dst # src | HALT as 0x76 ... This de.nition introduces not just the type Instr, but \nalso a type called Instr.LD. In general, the type of a constructor with .elds C, in a bitdata declaration \nT,is T.C .T. For example, LD is of type Instr.LD .Instr. Like other constructors, constructors with .elds \nmay be used in both patterns and expressions. A pattern of the form Cp, will examine a value by using \nthe tests derived from the as clause of C. If the tests succeed, the value will be promoted to type T.C, \nand then matched against the pattern p. Notice that the values of type T and T.C have the exact same \nrepresentation, but values of type T.C are guaranteed to conform to the format speci.ed by the constructor \nC. This is guaranteed because pattern matching is the only way to obtain a value of type T.C. In particular, \nthere is no way to solve the BitRep predicate for T.C types, and so we cannot use the function fromBits \nto create a T.C value from raw bits. The types T.C are record types that contain the .elds of the constructor. \nTo capture this idea, we introduce another predicate: (l :: _) ._ : *.*.Prop The predicate (l :: t1) \n.t2 states that t2 is a record type that has a .eld of type t1. This idea is not new and has been used \nin the design of various record systems [7, 5]. The only way to solve such predicates is by assumption. \nThese assumptions are introduced by bitdata declarations: one for each .eld of each constructor. For \nexample, if we use the :a command to list all assumptions that are in scope for the types we de.ned so \nfar, we get the following list: (our implementation prints (l :: t1) .t2 as t2 has l :: t1) >:a BitRep \nDMACmd 3 DMACmd.Remote has op :: RemoteOp BitRep RemoteOp 2 BitRep MyBool 0 BitRep Instr 8 Instr.LD has \ndst :: Reg Instr.LD has src :: Reg BitRep Reg 3 This allows for the same .eld names to appear in different \ncon\u00adstructors, even if they belong to different bitdata declarations. There are two families of constants, \nindexed by label names, that we use to manipulate records: access l : .a\u00df. (l :: a) .\u00df . \u00df .a update \nl : .a\u00df. (l :: a) .\u00df . a .\u00df .\u00df The constant access l is used to get the .eld l of a record. A more concise \nnotation for this operation is e.l, which is merely a short-hand for access l e. The constant update \nl is used to re\u00adplace the value of the .eld l in a record. We have syntactic sugar for updates as well: \n{r |l1 = e1, l2 = e2}is an abbreviation for update l2 e2 (update l1 e1 r). Notice that this differs from \nHaskell s notation for manipulating records. As an example of how to use the record operations, consider \nde.ning a function that will set the source of the LD instruction to a particular register, and will \nleave other instructions unchanged: setSrc (LD r) x = LD { r | src = x } setSrci _ =i > :t setSrc Instr \n-> Reg -> Instr > show (LD { src = A, dst = B }) \"B01000111\" > show (setSrc (LD { src = A, dst=B}) C) \n\"B01000001\" Another way to examine record values is to use record patterns. These patterns are of the \nform {p |li = pi}. A value matches a record pattern if it matches the pattern p, and its .elds li match \nthe patterns pi. For example, we may use a record pattern in com\u00adbination with a constructor pattern \nto match on a constructor and a .eld, and at the same time to name the record of the constructor: fromHL \n(LD { r | src = MemHL }) = r.dst fromHL _ = MemHL  2.2.3 The if Clause In some complex situations the \npattern derived from the layout of a value is not suf.cient to recognize that the value was created with \na particular constructor. Occasionally it may be necessary to examine the values in the .elds as well. \nFor example, the LD instruction should never contain the register MemHL as both its source and its destination. \nIn fact, the bit pattern corresponding to such a value is instead used for the HALT instruction: > show \n(LD { src = MemHL, dst = MemHL }) \"B01110110\" > show HALT \"B01110110\" One way to deal with complex de.nitions \nis to include an ex\u00adplicit guard [11] in any de.nition that pattern matches on LD. This approach works \nbut it is error prone because it is easy to forget the guard. To avoid such errors, a bitdata de.nition \nallows pro\u00adgrammers to associate a guard with each constructor by using an if clause with a Boolean expression \nover the names of that con\u00adstructor s .elds. The expression is evaluated after the tests derived from \nthe as clause have succeeded and before any .eld patterns are checked. If the expression evaluates to \nTrue, then the value is recognized as matching the constructor, otherwise the pattern fails. For example, \nthis is how we could modify the de.nition of Instr to document the overlap between LD and HALT: bitdata \nInstr = LD { dst::Reg, src::Reg } as B01 # dst # src if not (isMemHL src &#38;&#38; isMemHL dst) | HALT \nas 0x76 ... instrName (LD _) = \"Load\" instrName HALT = \"Halt\" We use the function instrName to experiment \nwith this feature: > instrName (LD { src = A, dst = MemHL }) \"Load\" > instrName (LD { src = MemHL, dst \n= MemHL }) \"Halt\" > instrName HALT \"Halt\" As the second example illustrates, the if clause is used only \nin pat\u00adtern matching and not when values are constructed. We made this design choice because it is simple, \nand avoids the need for partiality or exceptions, which arise when the if clause is used during value \nconstruction. The cost of this choice is minimal because program\u00admers may de.ne smart constructors to \nvalidate the .elds before constructing a bitdata record. 2.2.4 Junk and Confusion! Standard algebraic \ndatatypes enjoy two important properties that are sometimes referred to as no junk and no confusion [6], \nboth of which are useful when reasoning about the behavior of functional programs. The former asserts \nthat every value in the datatype can be written using only the constructor functions of the type, while \nthe latter asserts that distinct constructors construct distinct values. In the language of algebraic \nsemantics, which is where these terms originated, the combination of no junk and no confusion implies \nthat the semantics of a datatype is isomorphic to the initial algebra generated by its constructor functions. \nIn more concrete terms, no junk states that every bit pattern corresponds to some conceptual value, while \nno confusion tells us that there is no overlap between the bit patterns for different constructors. For \nbitdata types, we can only hope to avoid junk and confusion if the total number of representable values, \nN, is a power of two. In any other case, if 2n-1 < N < 2n, we need to use an encoding with at least n \nbits, and either accept some junk (i.e., some bit patterns that do not correspond to any of the N possible \nvalues), or else some the N values will be represented by more than one bit pattern. In other cases, \neven when it is technically possible to avoid both junk and confusion, a designer might still opt for \na representation that sacri.ces one or both of these properties because it simpli.es the tasks of encoding \nand decoding. The Time type in Section 1.5, for example, includes both junk (because the most signi.cant \nbit can never be set) and confusion (because the Now and Never cases overlap with the Period case). We \ncannot avoid the potential for junk and confusion in bitdata, but we can at least take steps to warn \nprogrammers about potential errors and pitfalls that they can cause. We have implemented a prototype \nstatic analysis for hobbit that captures the set of all possible bit patterns in each bitdata type using \nan ordered binary decision diagram (OBDD). Testing for junk in this setting amounts to testing the OBDD \nfor the propositional constant true. Testing for confusion is accomplished by comparing pairs of OBDDs \nfor individual constructors within a bitdata de.nition. The results of these tests are used to trigger \nappropriate warning diagnostics, and, from our experience to date, this seems to work well in practice. \nIt may also be possible to infer orderings between the represen\u00adtations of different constructors, and \nto use these results to check for non-exhaustive or overlapping pattern matches in arbitrary user\u00adde.ned \nfunctions. This remains as a topic for future work.  3. Extended Example: Flexpages in L4 In this section, \nwe present an extended example from L4 [17] to illustrate the use of our language on a real world problem. \nSeveral operations in L4 manipulate regions of a process virtual address space. The L4 speci.cation introduces \na type of .expages to describe these regions in an architecture-independent manner. fpage (b, 2s) complete \n nilpage 0 (32) In general, a .expage contains a base address for the region of memory, a size equal \nto log2(number of bytes in region), and a set of permissions that specify what operations can be performed \non the region. The set of permissions contains three bits: read (r), write (w), and execute (x). Complete \nand nilpage values are special cases: complete represents the entire virtual address space, and nilpage \nrepresents an empty region of memory. The size of the region described by a .expage is restricted to \nwhole numbers of physical pages on the target machine, and the starting address must be 2s-aligned. It \nis up to a particular imple\u00admentation to enforce this condition. For the IA32, the size of the .expage \nmust be greater than or equal to 12. We characterize .ex\u00adpages using a type called Fpage and with permissions \nrepresented by the Perms type. bitdata Fpage = Fpage { base::Bit 22, size::Bit 6, perms::Perms } as base \n# size # B0 # perms if (base mod (2 ^ size) == 0) &#38;&#38; (size >= 12) | Complete { perms = nullPerms \n:: Perms } as 1 # B0 # perms | Nilpage as 0 bitdata Perms = Perms { r::Bit 1, w::Bit 1, x::Bit 1 } nullPerms \n=Perms{r =0,w =0,x =0} The de.nitions of Complete and Nilpage follow directly from the layout in the \nspeci.cation. The representation of regular .expages is more involved, because we capture the validity \nrestrictions in the type. We use an if clause to guarantee that a .expage will only match the Fpage constructor \nif its base .eld is aligned and its size is greater than or equal to 12. Note there is junk in this type: \nan Fpage with an invalid size or incorrect alignment will not match any constructor. The L4 speci.cation \nstates that such a .expage should be treated as a Nilpage. We capture this, and eliminate junk from the \ntype, by reformulating the de.nition of Nilpage to catch invalid .expages. bitdata Fpage = ... | Nilpage \n{ bits = 0 :: Bit 32 } While there is no longer junk in the type, we have introduced confusion: now Nilpage \nwill match any .expage, including a valid or complete one. To obtain the desired behavior for .expages, \nwe must take care when pattern matching on Fpage values: the Nilpage constructor should always come last. \nIn addition, the representation of a Nilpage is no longer guaranteed to be zero. This is acceptable in \nthe context of L4, but it is a choice that might not be right in all situations. The discussion of the \nFpage datatype exempli.es the reasoning behind some of our language design decisions, such as allowing \njunk and confusion. Of course it is undesirable to use a speci.\u00adcation that includes invalid values and \neven worse to use an im\u00adplementation that constructs such values. Unfortunately, these sit\u00aduations cannot \nalways be avoided. An implementation of L4 must adhere to the speci.cation, which contains datatypes, \nlike .expage, that can describe meaningless values. Often .expages come from untrusted user-level programs, \nand there is no way to guarantee that these applications only construct valid values. A frequent operation \non a .expage is the computation of the region s ending address, the base address plus the size. The types \nof base and size do not match, so the (+) operator, which has type Bit n . Bit n . Bit n, is insuf.cient. \nZero-extending size to Bit 22 before applying (+) recti.es this problem, but it is not an ideal solution. \nAdding quantities of different widths is a common occurrence, and manually zero-extending the smaller \nquantity is tedious and clutters the code. An alternative approach is to create a more polymorphic addition \noperation that automatically zero extends both arguments to the expected result type. add ::(b+a =d,e+c \n=d) =>Bita ->Bitc ->Bitd addxy =(0# x)+(0# y) We can use add without a type annotation whenever the width \nof the result type is evident from the context in which add appears. If the result type cannot be determined \nfrom the context, we must attach a type annotation to the expression containing add. We use add to de.ne \na subset operation on .expages. A .ex\u00adpage fp1 is a subset of another .expage fp2 if the region speci.ed \nby fp2 completely contains the region speci.ed by fp1. subset (Fpage fp1) (Fpage fp2) = (fp1.base >= \nfp2.base) &#38;&#38; (end fp1 <= end fp2) where end fp = (add fp.base fp.size)::Bit 22 Only the case \nfor normal, valid .expages is shown. If the starting and ending addresses of the .rst argument, fp1, \nare both within the region of memory described by the second argument, fp2, then fp1 is a subset of fp2. \nWe check if fp2 contains the starting address of fp1 using a simple comparison of the base .elds. Next, \nwe check if the ending address for fp1 is less than or equal to the ending address for fp2. We de.ne \na local function end, which calculates the ending address of a .expage using add. In this case, the result \ntype of add cannot be determined from the context alone, so we annotate the call to add with the desired \nresult type. 4. Implementation The ideas described in this paper are implemented in a prototype system \nwritten in Haskell. The system is about 3500 lines of code, approximately 1000 of which are in the parser. \nThe implementation played an important part in the development of our ideas, as the process of building \nthe prototype revealed a number of details that we had not noticed before. Having the prototype also \nenabled us to experiment with concrete examples, which revealed both problems with the design (that we \nthen had to .x), and unexpected features, that we had not realized would be possible at .rst. The implementation \nconsists of four phases: parsing and static analysis in the front end, and a simpli.er and interpreter \nin the back end. The simpli.er eliminates patterns (by turning them into se\u00adquences of case statements \nand tests) and produces translated pro\u00adgrams that can be executed using a standard interpreter for lambda\u00adcalculus \nwith constants. We will therefore focus our attention in the rest of this section on the treatment of \nthe type system, which is the most complex part of our implementation. 4.1 Type Inference Type inference \nin our system is based on a modi.ed version of Milner s algorithm W [14]. The modi.cations are to account \nfor quali.ed types, and are described elsewhere [8]. In the absence of quali.ed types the two algorithms \nwork in the same way. To implement quali.ed types we need a way to solve predicates. To do that we use \nthe rules described in Section 2. We also add some simpli.cation rules [9], described in Figure 3. They \ndo not make the system any more expressive, in the sense that the same programs may be typed with or \nwithout these rules. The rules are useful, because they provide alternative ways to solve Width predicates: \nwe can discharge a Width predicate even if the argument type is not a natural number. This makes it possible \nto infer simpler types. For example, using the rule WRep we can simplify the type (.a\u00df. (BitRep a\u00df, Width \n\u00df) .t ) to (.a\u00df. BitRep a\u00df .t ) . ft1 + t2 = t3 i .{1, 2, 3} . fBitRep t1 t2 [WRep] . fWidth ti . fWidth \nt2 Figure 3. Simpli.cation rules Readers familiar with the Haskell class system may think of these simpli.cation \nrules as specifying that Width is a super-class of the BitRep and (_ + _ = _) classes. 4.2 Evidence \nfor Predicates As we have already seen, in the presence of quali.ed types, the system needs to solve \npredicates to make sure that type schemes may be instantiated at the given concrete types. A common way \nto implement such systems is to have a predicate solver that constructs proof objects as evidence that \na predicate is true. We can think of a value that has a quali.ed type p . t as a function that takes \nthe evidence that the predicate p holds as an argument, and then produces a result of type t . While \nthe system performs type inference, it automatically constructs evidence for predicates, and applies \nvalues of quali.ed types to the evidence they need. In our system, we have four different predicate forms. \nThe evi\u00addence for each of these depends on the concrete data representation chosen in a particular implementation. \nThe evidence we chose for our system is described in Table 1. It is quite general and can ac\u00adcommodate \ndifferent data representations. We chose to implement all bit values with 32 bit words. However there \nare other options: for example, we could have implemented all values with up to 8 bits as bytes, values \nwith between 8 and 16 bits as words, and values with between 16 and 32 bits as long words. There are \nalso implemen\u00adtation choices that can use simpler evidence, based on particular assumptions about how \ndata is represented. Predicate form Evidence (x :: t1) .t2 {offset :: Int, width :: Int} Width t {width \n:: Int } BitRep t1 t2 {width :: Int } t1 + t2 = t3 {upper :: Int, lower :: Int} Table 1. Evidence for \npredicates The evidence for record manipulation tells us the offset and number of bits that we need to \naccess. The evidence for Width and BitRep tells us how many bits are necessary to represent some value. \nAddition predicates are used when we split or join bit vectors together. The evidence tells us the widths \nof t1 (the upper or most signi.cant bits) and t2 (the lower or least signi.cant bits). 4.3 Improvement \nImprovement [9] is a technique for inferring types from predicates. Given a set of predicates, we examine \nthem and compute an im\u00adproving substitution. The substitution does not change the set of predicates, \nbut can replace some of the type variables with con\u00adcrete types. This in turn may enable the system to \nsolve some of the predicates, which could not be solved before because the types were unknown. A popular \nlanguage feature based on improvement is the addition of functional dependencies[10] to the Haskell class \nsystem. Figure 4 shows the functional dependencies that we used in our type checker as well as some additional \nrules for improvements that cannot be captured using functional dependencies. a + \u00df = . | (a, \u00df) . ., \n(a, .) . \u00df, (\u00df, .) . a (l :: a) . \u00df | \u00df . a BitRep a\u00df | a . \u00df impr{0+ t1 = t2} = mgu t1 t2 impr{t1 +0= \nt2} = mgu t1 t2 impr{t1 + t2 =0} = mgu t1 0 . mgu t2 0 impr{t1 + t2 = t1} = mgu t2 0 impr{t1 + t2 = t2} \n= mgu t1 0 impr{BitRep (Bit t1) t2} = mgu t1 t2 Figure 4. Functional Dependencies and Improvement Rules \nTo see how improvement works in practice, consider the ex\u00adpression toBits B00. The constant B00 is of \ntype Bit 2, and the constant toBits is of type .a. BitRep a\u00df . a . Bit \u00df. The type inference algorithm \nwill infer the type Bit \u00df, provided that the system can solve the predicate BitRep (Bit 2) \u00df. Notice \nthat we cannot use any of the rules to solve this predicate because we do not know what type \u00df stands \nfor. One possibility is to infer the type .\u00df. BitRep (Bit 2) \u00df . Bit \u00df. This type is not wrong, but is \nunnec\u00adessarily complicated we know that the type Bit 2 is represented with 2 bits. To get a more intuitive \ntype, we use the improvement rules and determine that \u00df is not a free type variable, but should be replaced \nby the type 2. Then we can use the rule Bit to discharge the predicate, and infer the expected type Bit \n2.  5. Related Work Cryptol Our use of the (#) notation comes from Cryptol, and our Bit types are a \nspecial case of Cryptol s more general sequences: for example, Bit 32 in our system corresponds to the \n[32] Bit type in Cryptol. However, our approach differs from Cryptol be\u00adcause we have a different application \ndomain in mind: we are inter\u00adested in helping programmers develop systems software. The op\u00aderations that \nare common in that setting are not based on sequence manipulation (which is one of Cryptol s main strengths) \nbut are much more like operations on ordinary data types, which is what our design introduces. The work \non Cryptol is orthogonal to our own in principle, we could add more operators for sequence ma\u00adnipulation. \nBluespec Bluespec is a language for programming FPGAs [1]. There does not seem to be a freely available \nimplementation that we could experiment with, but the language speci.cation describes a number of ideas \nsimilar to ours. Bluespec supports bit vectors, much like the ones we described, but there are no operations \nto join or split bit vectors. Bluespec supports user de.ned data types and records, which provides convenient \nways to pattern match on data, and to access .elds in a record. However, because Bluespec s main goal \nis to program FPGAs, there is no need for the data types to conform to a particular prede.ned ABI. Bluespec \ndoes not allow programmers to specify explicit data layouts, and instead leaves the compiler to derive \nbit representations for data automatically in a prede.ned manner. SLED The Speci.cation Language for \nEncoding and Decoding (SLED) is a domain speci.c language for working with streams of machine language \ninstructions [16]. Encoding and decoding ma\u00adchine instructions requires bit manipulation, so it is interesting \nto compare SLED s design to our own. An important difference is that SLED processes streams of instructions, \nwhile our approach deals only with .xed-width values. In our design, obtaining the data from a buffer \nor a device is separate from decoding and processing the data, while in SLED these two concepts are uni.ed \nin a single match construct. Another difference is that SLED is a speci.cation language that (at least \nin principle) is independent of the language in which the application is written, while with bitdata \nthe speci.\u00adcation and the application are written in the same language. The bene.t of the SLED approach \nis that it is not necessary to extend the host language in which the application is written. The bene.t \nof our approach is that we can obtain stronger static checking, and potentially gain some opportunities \nfor optimization. The reason for this is that a language independent preprocessor can only take into \nconsideration the SLED parts of a program, and the language speci.c tools only get to process a translated \nspeci.cation. Using a single language does not have these drawbacks and, in addition, enables us to have \nmore accurate types. From a language perspective, there are interesting differences between the mechanisms \nthat SLED provides for specifying bit\u00adlevel representations, and those that are provided by our bitdata \nde.nitions. In SLED, the position of a .eld within a bitdata struc\u00adture is speci.ed by giving a range \nof bit positions, and a language of patterns including simple range/equality constraints on indi\u00advidual \n.elds as well as constructs for conjunction, disjunction, and sequencing of patterns is used to describe \nbinary representations. This is strictly more expressive than our approach, although we have found that \nthe layout speci.cations ( as clauses) in our bit\u00addata de.nitions provide a convenient and .exible way \nto accom\u00adplish the same thing in many practical examples. SLED also allows the use of equations that \nrelate constructor values with bit .elds using a small language that includes linear arithmetic as well \nas a few specialized operators such as sign extension. There is no corre\u00adsponding feature in our system, \nbut we have certainly encountered situations where this would be useful as a way to allow the data that \nis seen through bitdata views to be computed rather than simply ex\u00adtracted directly from the underlying \nbitstream. PADS PADS is a declarative data description language [4] for processing large streams of data \nfrom ad hoc sources, such as web server logs. This is a separate problem from our goal of providing tools \nthat enable easy manipulation of .xed size machine registers, but there are still some similarities with \nour work. In both cases, for example, the programmer describes data in a high level language, rather \nthan directly writing parsers and pretty printers by hand. A PADS speci.cation describes the physical \nlayout and semantic properties of a data source, so it is analogous to a bitdata declaration, but on \na different scale. Like ours and many other designs, a PADS speci.cation can contain unions (sums), structs \n(products), types, and constraints. DataScript DataScript is a language that uses types to describe the \nphysical layout of binary .le formats [2]. DataScript provides primitive types, set types, variants (sums), \nrecords (products), and arrays. Set types include bitmask types, which enable programmers to describe \nenumerations with speci.c bit representations in a fash\u00adion similar to bitdata. An alternative in a sum \ntype is chosen based on constraints written in a speci.cation. DataScript speci.cations are language \nindependent, so processing a speci.cation with dif\u00adferent language bindings can produce code for different \nlanguages. In this respect it resembles the input to parser generator tools like Yacc. The exact representation \nof the parsed .les depends on the particular language binding. For example, in one of the Java bind\u00adings \n[2] arrays are represented as lists. Unlike bitdata, DataScript does not provide mechanisms for de.ning \nfunctions by pattern matching; DataScript s design is based on the Java language, which does not support \nthis feature. Views Views [18] provide the ability to pattern match on an ab\u00adstract type as if it were \nan algebraic datatype. This is accomplished by de.ning a view type, which speci.es a set of functions \nfor converting values of the abstract type into the view type. Values of the view type are used in pattern \nmatching, but the programmer cannot construct them outside of a view type declaration (in fact, the proposed \ncompilation scheme advocates that these values are never constructed at all). One can regard bitdata \ndeclarations as de.ning views on a spe\u00adci.c class of types, namely Bit n types. Limiting the scope in \nthis way allows us to provide considerably more powerful functionality. In Wadler s work, view types \nare phantom types which can only be used in pattern matching. In contrast, a bitdata declaration creates \na new type that may appear in type signatures and whose values may appear on both the left and right \nhand sides of an equation. In ad\u00addition, our compiler automatically generates the marshaling func\u00adtions. \nThe application of these functions imposes no performance penalty because they are the identity, where \nas view transformation functions may perform arbitrary computation. 6. Future Work Beyond registers \nIn this paper we focused on data whose rep\u00adresentation .ts in the registers of a machine. This is suf.cient \nfor many systems level programming tasks, but not all. For example, when implementing a network protocol \nstack, programmers often think of a sequence of bytes as having some particular structure. It seems plausible \nthat our ideas may be extended to handle such larger data items. However there are important details \nthat need to be worked out, mostly to do with representing data in memory. For example, we need to be \ncareful about the order in which bytes are stored in memory. Other important details include working \nwith references, and interactions with automatic garbage collection. Optimizations As we already discussed, \nwe have a working im\u00adplementation of all the ideas in the paper. A component that is miss\u00ading from the \nimplementation is a good optimizer to get ef.cient bit-twiddling code. Using the higher-level constructs \nwe described should make it easier for a compiler to spot opportunities for opti\u00admization. It would also \nbe useful to consider optimizations to our implementation of pattern matching: we would like to examine \nthe patterns occurring in the equations of a function, and combine them into an ef.cient test on values. \nOne promising approach is to use binary decision diagrams (BDDs) to calculate minimal representa\u00adtions \nfor groups of patterns. More static checking As we discussed in Section 2.2.4, program\u00admers may de.ne \ntypes with both junk and confusion. In some cir\u00adcumstances this is inevitable because a program has to \nconform to a prede.ned ABI, but an implementation may still warn program\u00admers about such anomalies. To \nachieve this, it would be useful to develop algorithms that can detect overlapping or potentially re\u00addundant \npattern matches in function de.nitions. It seems that this problem is related to pattern matching compilation, \nand we expect that the two problems may have similar solutions. Generalizing data types Several features \nof bitdata declara\u00adtions are orthogonal to bit manipulation, and hence may have more general interest. \nAllowing record declarations to contain default values makes sense in any language that supports records. \nSimi\u00adlarly, if clauses are not speci.c to bitdata and might also be use\u00adful in data declarations. A .nal \nobservation idea is that as clauses are also not speci.c to bitdata. Instead of specifying that a value \nshould be represented with a particular bit pattern, as clauses could specify a value of a different \ntype to represent the constructor.  Acknowledgments This work was supported, in part, by the National \nScience Foun\u00addation award number 0205737, ITR: Advanced Programming Languages for Embedded Systems. We \nwould especially like to thank Norman Ramsey, Andrew Tolmach, Thomas Hallgren, and the anonymous reviewers \nfor their comments and suggestions. References [1] Lennart Augustsson, Jacob Schwartz, and Rishiyur \nS. Nikhil. Bluespec Language de.nition. Sandburst Corporation, 2002. [2] Godmar Back. Datascript -a \nspeci.cation and scripting language for binary data. In Proceedings of the ACM Conference on Generative \nProgramming and Component Engineering Proceedings (GPCE 2002), pages 66 77, October 2002. [3] Matthias \nBlume. No-Longer-Foreign: Teaching an ML compiler to speak C natively . In BABEL 01: First workshop on \nmulti-language infrastructure and interoperability, September 2001. [4] Kathleen Fisher and Robert Gruber. \nPADS: a domain-speci.c language for processing ad hoc data. In PLDI 05: Proceedings of the 2005 ACM SIGPLAN \nconference on Programming language design and implementation, pages 295 304, 2005. [5] Benedict R. Gaster \nand Mark P. Jones. A polymorphic type system for extensible records and variants. Technical report, University \nof Nottingham, 1996. [6] J.A. Goguen, J.W. Thatcher, E.G. Wagner, and J.B. Wright. Initial algebra semantics \nand continuous algebras. JACM, 24(1):68 95, 1997. [7] Robert W. Harper and Benjamin C. Pierce. Extensible \nrecords without subsumption. Technical Report CMU-CS-90-102, School of Computer Science, Carnegie Mellon \nUniversity, Feburary 1990. [8] Mark P. Jones. Quali.ed Types Theory and Practice. Cambridge University \nPress, 1994. [9] Mark P. Jones. Simplifying and improving quali.ed types. Tech\u00adnical Report YALEU/DCS/RR-1040, \nYale University, New Haven, Connecticut, USA, June 1994. [10] Mark P. Jones. Type classes with functional \ndependencies. In ESOP 2000: European Symposium on Programming, March 2000. [11] Simon Peyton Jones, editor. \nHaskell 98 Language and Libraries, The Revised Report. Cambridge University Press, 2003. [12] Simon Peyton \nJones and Mark Shields. Lexically scoped type variables. March 2004. [13] Jochen Liedtke. On \u00b5-kernel \nconstruction. In 15th ACM Symposium on Operating System Principles, December 1995. [14] Robin Milner. \nA theory of type polymorphism in programming. Journal of Computer and System Sciences, 17(3):348 375, \nDecember 1978. [15] National Semiconductor. DP8390D/NS32490 NIC Network Interface Controller, July 1995. \n[16] Norman Ramsey and Mary F. Fernandez. Specifying representations of machine instructions. ACM Transactions \non Programming Languages and Systems, 19(3):492 524, 1997. [17] L4ka Team. L4 eXperimental Kernel Reference \nManual, January 2005. Available online from http://l4ka.org/. [18] Philip Wadler. Views: a way for pattern \nmatching to cohabit with data abstraction. In 14 th ACM Symposium on Principles of Programming Languages. \n[19] Zilog, Inc. Z80-CPU, Z80A-CPU Technical Manual, 1977. Informa\u00adtion about the Z80 is also available \nfrom www.z80.info.  \n\t\t\t", "proc_id": "1086365", "abstract": "This paper explains how the high-level treatment of datatypes in functional languages&#8212;using features like constructor functions and pattern matching&#8212;can be made to coexist with <i>bitdata</i>. We use this term to describe the bit- level representations of data that are required in the construction of many different applications, including operating systems, device drivers, and assemblers. We explain our approach as a combination of two language extensions, each of which could potentially be adapted to any modern functional language. The first adds simple and elegant constructs for <i>manipulating</i> raw bitfield values, while the second provides a view-like mechanism for defining distinct new bitdata types with fine-control over the underlying <i>representation</i>. Our design leverages polymorphic type inference, as well as techniques for improvement of qualified types, to track both the type and the width of bitdata structures. We have implemented our extensions in a small functional language interpreter, and used it to show that our approach can handle a wide range of practical bitdata types.", "authors": [{"name": "Iavor S. Diatchki", "author_profile_id": "81100345874", "affiliation": "OHSU", "person_id": "P396557", "email_address": "", "orcid_id": ""}, {"name": "Mark P. Jones", "author_profile_id": "81100557950", "affiliation": "OHSU", "person_id": "PP31047626", "email_address": "", "orcid_id": ""}, {"name": "Rebekah Leslie", "author_profile_id": "81100575103", "affiliation": "Portland State University", "person_id": "PP14198850", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086387", "year": "2005", "article_id": "1086387", "conference": "ICFP", "title": "High-level views on low-level representations", "url": "http://dl.acm.org/citation.cfm?id=1086387"}