{"article_publication_date": "09-12-2005", "fulltext": "\n * Witnessing Side-Effects Tachio Terauchi EECS Department University of California, Berkeley tachio@cs.berkeley.edu \n Abstract We present a new approach to the old problem of adding side effects to purely functional languages. \nOur idea is to extend the language with witnesses, which is based on an arguably more pragmatic motivation \nthan past approaches. We give a semantic condition for correctness and prove it is suf.cient. We also \ngive a static checking algorithm that makes use of a network .ow property equivalent to the semantic \ncondition. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; \nD.3.3 [Programming Languages]: Language Constructs and Features General Terms Design, Languages, Theory \nKeywords Functional languages, side-effects 1. Introduction Adding side-effects to a purely functional \nlanguage is a well-known problem with a number of solutions [7, 9, 11, 15, 6, 1] with mon\u00adads [10, 11, \n7, 9] being arguably the most popular. In this paper, we propose a new approach to this old problem by \nattacking it from a different angle. Instead of starting from a language theo\u00adretic point of view, we \nstart by introducing a simple programming feature called witnesses so that programs can explicitly order \nside\u00adeffects. This feature is motivated by a pragmatic observation and is straightforward. The catch \nis that, because it is so simple, it actu\u00adally does not guarantee that a program is correct (i.e., that \nit can be viewed as a purely functional program). Instead, we argue that the feature makes it easy for \nprogrammers to write correct programs. We then formally state a natural semantic condition that is suf.\u00adcient \nto guarantee correctness and give a static checking algorithm. The result is a new framework for guaranteeing \ncorrectness of side\u00adeffects in purely functional programs. Besides arguably being more intuitive to programmers, \nour ap\u00adproach is more expressive than previous approaches. In particular, our approach does not force \nside-effects to occur in a sequential * This research was supported in part by NASA Grant No. NNA04CI57A; \nNSF Grant Nos. CCR-0234689, CCR-0085949, and CCR-0326577. The information presented here does not necessarily \nre.ect the position or the policy of the Government and no of.cial endorsement should be inferred. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 05 September \n26 28, 2005, Tallinn, Estonia. Copyright c . 2005 ACM 1-59593-064-7/05/0009. . . $5.00. Alex Aiken Computer \nScience Department Stanford University aiken@cs.stanford.edu order. For example, a program is allowed \nto read from a reference cell in two unordered contexts as well as write to two different cells in two \nunordered contexts. Besides providing new insights into the old problem of .tting side effects into functional \nlanguages for conventional von Neu\u00admann architectures, our work is motivated by the emergence of commercial \nparallel computer architectures (e.g., chip-multiprocessors or multi-core chips) that encourage parallel \nprogramming. It is well-known that the explicit dependence property of functional languages makes parallelization \neasier for both programmers and compilers.1 However, frequent use of side-effects, namely manual destructive \nmemory/resource updates, are believed to be important for programming high-performance parallel applications \nin prac\u00adtice. Hence a functional way to add side-effects without imposing parallelism-destroying sequentiality \nmay be of practical interest for exploiting parallelism within these new architectures. 1.1 Contributions \nand Overview This paper makes the following contributions: A simple language feature called witnesses \nthat can be used to order side-effects. (Section 2)  A semantic condition called witness race freedom \nfor correct usage of witnesses and a proof of its suf.ciency. (Section 3)  An automatic algorithm for \nchecking the afore-mentioned se\u00admantic condition that makes use of a network .ow property. (Section 4) \n  The semantic condition is intuitive in the sense that it is directly motivated by the implications \nof writing race-free programs. The automatic algorithm is derived as a type inference algorithm for a \nsubstructural type system. The type system and its inference problem are somewhat subtle and interesting \nin their own right. Section 5 discusses related work. Section 6 concludes.  2. Preliminaries We need \na precise de.nition of what it means for side-effects to be correct within a functional language. A helpful \nidea is to show that a program s semantics is independent of a class of functional program transformation \nrules. However, there is no consensus on the right set of transformations. For example, the transformation \n(let x = e in e ' )= (e ' [x/e])for x/. free(e)is not always true in systems based on linear-types. (Here \nfree(e)is the set of free variables of e.) To de.ne correctness, we .x a set of program transformations \nexpressive enough to model different functional reduction strate\u00ad 1 But not easy, since there are other \nchallenging issues such as selecting the right granularity of parallelism, but these issues apply equally \nto other languages and many solutions such as data-parallel operators and thread annotations already \nexist. ' ' ' e := x | i | .x.e | e e | let x = e in e | e . e | pi(e) ' ' | write e1 e2 e3 | read e e \n| ref e | join e e | Figure 1: The syntax of the language .wit . gies, including call-by-value, call-by-need \n(i.e., lazy-evaluation), and parallel evaluation.2 So, for example, a program that is invari\u00adant under \nthis set of transformations evaluates to the same result regardless of whether the evaluation order is \ncall-by-value or call\u00adby-need. In parallel evaluation, invariance implies that a program is deterministic \nunder any evaluation schedule. For ease of exposition, we include the afore-mentioned program transformations \ndirectly in the semantics as non-deterministic re\u00adduction rules. The correctness criteria then reduces \nto showing that a program is con.uent with respect to this semantics. Also, for the purpose of exposition, \nwe restrict side-effects to imperative opera\u00adtions on .rst class references. Figure 1 gives the syntax \nof .wit , a simple functional language with side-effects and witnesses. Note .wit has the usual features \nof a functional language: variables x, integers i, function abstractions .x.e, function applications \nee ' , variable bindings let x = e in e ' , pairs e . e ' , and projections pi(e)where i =1or i =2. Bindings \nlet x = e in e ' can be recursive, i.e., x . free(e). Three expres\u00adsion kinds work with references: reference \nwrites write e1 e2 e3, reference reads read ee ' , and reference creations ref e. A read read ee ' has \na witness parameter e ' along with a reference pa\u00adrameter e such that it does not read the reference \ne until it sees the witness e ' . (Section 3 de.nes the formal meaning of seeing the witness. ) Similarly, \na write write e1 e2 e3 writes the expression e2 to the reference e1 after it sees the witness e3. After \ncompletion of the read, read ee ' returns a pair of the read value and a wit\u00adness. Similarly, write e1 \ne2 e3 returns a witness after the write. In general, any side-effect primitive returns a witness of performing \nthe corresponding side-effect; in the case of .wit , the side-effect primitives are just write e1 e2 \ne3 and read ee ' . Before describing the formal semantics of .wit , we describe novel properties of .wit \ninformally by examples. Programs in .wit can use witnesses to order side-effects. For ex\u00adample, the following \nprogram returns 2regardless of the evaluation order because the read requires a witness of the write: \nlet x = (ref 1) in let w = (write x 2 )in read xw (The symbol is used for dummy witnesses.) On the other \nhand, .wit does not guarantee correctness. For example, the following .wit program has no ordering between \nthe read and the write and hence may return 1or 2depending on the evaluation order: let x = (ref 1) in \nlet w = (write x 2 )in read x The expression kind join ee ' joins two witnesses by waiting until it sees \nthe witness e and the witness e ' and returning a wit\u00adness. For example, the following program returns \nthe pair 1 . 1 regardless of the evaluation order because the write waits until it sees witnesses of \nboth reads: let x = (ref 1) in let y = (read x )in let z = (read x )in let w = (write x 2(join p2(y)p2(z)))in \np1(y). p1(z) Note that the two reads may be evaluated in any order. In general, witnesses are .rst class \nvalues and hence they can be passed to and 2 The results in Section 3 are general enough for most other \nfunctional transformations too, but the static checking algorithm requires a more stringent de.nition. \nE := D .{a . E}| []| Ee | eE | E . e | e . E ' '' | pi(E)| write Eee | write eE e | write ee E | read \neE | read Ee | ref E | join Ee | join eE App (S,E[(.x.e)e ' ]). (S,E[e[a/x]].{a . e ' }) Let (S,E[let \nx = e in e ' ]) . (S,E[e ' [a/x]].{a . e[a/x]}) Pair (S,E[pi(e1 . e2)]). (S,E[ei]) Write (S,E[write le \n]). (S[l . a],E[ ].{a . e}) Read (S,E[read l ]). (S,E[S(l). ]) Ref (S,E[ref e]). (S .{l . a},E[l].{a \n. e}) Join (S,E[join ]). (S,E[ ]) Arrive (S,E[a].{a . e}). (S,E[e].{a . e}) where e . V GC (S,D . D \n' ). (S,D) where ../dom(D ' ). dom(D ' )n free(D)= \u00d8 Figure 2: The semantics of .wit . returned from \na function, captured in function closures, and even written to and read from a reference. Witnesses are \na simple feature that can be used to order side-effects in a straightforward manner. In the rest of this \nsection, we describe the semantics of .wit so that we can formally de.ne when a .wit program is correct, \ni.e., when it is con.uent. Figure 2 shows the semantics of .wit , which is de.ned via reduction rules \nof the form (S,D). (S ' ,D ' ) where S,S ' are reference stores and D,D ' are expression stores. A reference \nstore is a function from a set of reference locations l to ports a, and an expression store is a function \nfrom a set of ports to expressions. Here, expressions include any expression from the source syntax extended \nwith reference locations and ports. Given a program e, evaluation of e starts from the initial state \n(\u00d8,{. . e}) where the symbol . denotes the special root port. Ports are used for evaluation sharing.3 \nThe reduction rules are parametrized by the evaluation contexts E. For an expression e, free(e)is the \nset of free variables, ports, and reference locations of e. For an expression S store D, free(D)= dom(D). \nfree(e). e.ran(D) We brie.y describe the reduction rules from top-to-bottom. The rule App corresponds \nto a function application. For functions F ''' ' and F , F . F denotes F . F if dom(F)n dom(F )= \u00d8 and \nis unde.ned otherwise. Thus App creates a fresh port a and stores e ' at a. The rule Let is similar. \nThe rule Pair projects the ith element of the pair. The rule Write creates a fresh port a, stores the \nexpression e ' at the port a, and stores the port a at the reference location l. We use S[l . a] as a \nshorthand for {l ' . S(l ' )| l ' . dom(S). l ' = l}.{l . a}. Note that we use the dummy witness symbol \n as the run-time representation of any witness because only its presence is important to the semantics, \ni.e., operationally, a witness is like a data.ow token in data.ow machines. The rule Read reads from \nthe reference location l and, as noted above, returns the value paired with a witness. The rule Ref creates \na fresh location land a fresh port a, initializes a to the expression e and lto the port a. The rule \nJoin takes two witnesses and returns one witness. The rule Arrive might look somewhat unfamiliar. Here \nV is the set of safe to duplicate expressions. Partly for the sake of the static-checking algorithm to \nbe presented later, we .x V to values generated by the following grammar: v := x | i | a | | l | v . \nv ' | .x.e 3 In the literature, top-level let-bound variables often double as variables and ports. Arrive \nsays that if e is safe to duplicate, then we can replace a by e; we say a safe to duplicate expression \nhas arrived at port a. In essence, while standard operational semantics for functional languages [12, \n8] implicitly combine Arrive with other rules, we separate Arrive for increased freedom in the evaluation \norder. Lastly, the rule GC garbage-collects unreachable (from the root port .) portions of the expression \nstore. Here is an example of a .wit evaluation: (\u00d8,{. . (.x.read x )ref 1}) . ({l . a},{. . (.x.read \nx )l,a . 1}) Ref . ({l . a},{. . read a ' ,a . 1,a ' . l}) App . ({l . a},{. . read l ,a . 1,a ' . \nl}) Arrive . ({l . a},{. . a . ,a . 1,a ' . l}) Read . ({l . a},{. . 1. ,a . 1,a ' . l}) Arrive . \n({l . a},{. . 1. }) GC  The semantics is non-deterministic and therefore also allows other reduction \nsequences for the same program. For example, we may take an App step immediately instead of .rst creating \na new refer\u00adence by a Ref step: (\u00d8,{. . (.x.read x )ref 1}) . (\u00d8,{. . read a ,a . ref 1}) App Before \nde.ning con.uence, we point out several important properties of this semantics. Firstly, note that the \nevaluation con\u00adtexts E do not extend to subexpressions of a . abstraction .x.e, i.e., we do not reduce \nunder .. The evaluation contexts also do not extend to subexpressions of an expression let x = e in e \n' , but e and e ' may become available for evaluation via applications of the Let rule. As with call-by-value \nevaluation or call-by-need eval\u00aduation, evaluation of an expression is shared. For example, in the program \n(.x.x. x)e, the expression e is evaluated at most once. The semantics of .wit has strictly more freedom \nin evaluation order than both call-by-value and call-by-need. In particular, call\u00adby-need evaluation \ncan be obtained by using the same reduction rules but restricting the evaluation contexts to the following \nE := D .{. . E}| []| Ee | pi(E)| write Eee ' | write leE | read Ee | read lE | join Ee | join E Call-by-value \nevaluation can be obtained by adding the following contexts to the evaluation contexts of the call-by-need \nevaluation E := ... | (.x.e)E | E . e | v . E | write lE | ref E |let x = E in e in addition to restricting \nthe rule App to the case e ' . V, the rule Let to the case e . V , the rule Pair to the case e1,e2 . \nV, the rule Write to the case e ' . V, and the rule Ref to the case e . V.4 Note that both lazy writes \nand strict writes are possible in .wit . It is important to understand that we are only concerned with \nside-effects via references, and hence we are not concerned about issues like the number of ports that \nare created during an evaluation. Having de.ned the semantics, we can formally de.ne when a .wit program \nis con.uent. To this end, we de.ne observational equivalence as the smallest re.exive and transitive \nrelation D D ' on expression stores satisfying: D D[a/a ' ]where a/. free(D)  D D[l/l ' ]where l/. \nfree(D)  That is, expression stores are observationally equivalent if they are equivalent up to consistent \nrenaming of free ports and reference locations. Let .* be a sequence of zero or more .. 4 Strictly speaking, \nthe context let x = E in e is not in the semantics of .wit . But .wit can simulate the behavior via a \nLet step and then reducing e[a/x] which is now in an evaluation context. DEFINITION 1 (Con.uence). A \nprogram state (S,D)is con.uent if for any two states (S1,D1)and (S2,D2)such that (S,D) .* (S1,D1)and \n(S,D).* (S2,D2), there exist two states (S1' ,D 1' ) and (S2' ,D 2' ) such that (S1,D1) .* (S1' ,D 1' \n), (S2,D2) .* (S2' ,D 2' ), and D1 ' D2' . A program e is con.uent if its initial state (\u00d8,{. . e})is \ncon.uent. Note that the de.nition does not require any relation between ref\u00aderence location stores S1 \n' and S2' . So, for example, a program that writes but never reads would be con.uent. As shown before, \n.wit contains programs that are not con.uent. Indeed, the difference between call-by-need and call-by-value \nis enough to demonstrate non-con.uence: (.x.read x )(let x = (ref 1) in let y = (write x 2 )in x) The \nabove program evaluates to the pair 1 . under call-by-need and to the pair 2. under call-by-value. No \nfurther reductions can make the two states observationally equivalent. (Here we implicitly read back \nthe top-level expression from the root port instead of showing the actual expression stores for brevity.) \nWe have shown earlier that witnesses can aid in writing correct programs by directly ordering side-effects. \nWitnesses are .rst class values and hence can be treated like other expressions. For exam\u00adple, the program \nbelow captures a witness in a function which itself returns a witness to ensure that reads and writes \nhappen in a correct order: let x = ref 1 in let w = write x 2 in let f = .y.read xw in let z = (f 0). \n(f 0) in let w = write x 3join p2(p1(z))p2(p2(z))in z Note that a witness of the .rst write is captured \nin the function f. Hence both reads from the two calls to f see a witness of the write. A witness of \neach read is returned by f, and the last write waits until it sees witnesses from both reads. Therefore, \nthe result of the program is (2. ).(2. )regardless of the evaluation order. Note that the two calls to \nf, and thus the reads in the calls, can occur in either order. 3. Witness Race Freedom As discussed \nin Section 2, witnesses aid in writing correct programs in the presence of side-effects but do not enforce \ncorrectness. In this section, we give a suf.cient condition for guaranteeing con.uence. The guideline \nfor writing a correct program should be intuitively clear at this point: we ensure that reads and writes \nhappen in some race-free order by partially ordering them via witnesses. We now make this intuition more \nprecise. First, we formally de.ne what we mean by the phrase side-effect Asees a witness of side effect \nB that we have used informally up to this point. Intuitively, a trace graph is a program trace with all \ninformation other than reads, writes, and witnesses elided. There are three kinds of nodes: read nodes \nread(l), write nodes write(l), and the join node join. Read and write nodes are parametrized by a reference \nlocation l. There is a directed edge (A,B)from node A to node B if B directly sees a witness of A. A \ntrace graph (V,E) is constructed as the program evaluates a modi.ed semantics: Write (S,E[write leA]). \n(S[l . a],E[B].{a . e}) V:=V .{B} where B is a new write(l)node E:=E .{(A,B)} Read (S,E[read lA]). (S,E[S(l). \nB]) V:=V .{B} where B is a new read(l)node E:=E .{(A,B)} Join (S,E[join AB]). (S,E[C]) V:=V .{C} where \nC isa new join node E:=E .{(A,C),(B,C)} Note that we now use nodes as witnesses instead of . The line \nbe\u00adlow each reduction rule shows the graph update action correspond\u00ading to that rule. The other rules \nremain unmodi.ed and hence have no graph update actions. An evaluation starts with V = E = \u00d8 and performs \nthe corresponding graph update when taking a Write step, a Read step, or a Join step. Notice that a trace \ngraph and the annotated semantics are only needed to state the semantic condition for correctness and \nare not needed in the actual execution of a .wit program. We can now de.ne what it means for a node Ato \nsee a witness of a node B, a notion we have used informally until now. DEFINITION 2. Given a trace graph, \nwe say that a node A sees a witness of a node B if there is a path from B to A in the trace graph. We \nwrite B . A. The following is a trivial observation: THEOREM 1. If B . A in a trace graph, then the side \neffect corresponding to B must have happened before the side effect corresponding to A in the evaluation \nthat generated the trace graph. Clearly, any trace graph is acyclic. Having de.ned trace graphs and the \n. relation, we are now ready to state the semantic condition for correctness. Before show\u00ading the condition \nformally, we informally motivate it by making analogies to the conventional programming guideline for \nwriting correct concurrent programs: prevent race conditions. We .rst note that a program could produce \ndifferent trace graphs depending on the choice of reductions, even when those trace graphs are from terminating \nevaluations. Furthermore, it is not nec\u00adessarily the case that such a program is non-con.uent. Therefore, \ninstead of trying to argue about con.uence by comparing different trace graphs, we shall de.ne a condition \nthat can be checked by observing each individual trace graph in isolation. Let us write A : nodetype \nas a shorthand for a node A of type nodetype. If we have A : read(l)and B : write(l), then we want either \nA . B or B . A to ensure that A always happens before B or B always happens before A because otherwise \nwe may get a read-write race condition due to non-determinism. Also, for any A : read(l), if there are \ntwo nodes B1,B2 : write(l)such that neither B1 . B2 nor B2 . B1 (so we do not know which write occurs \n.rst) and A could happen after both B1 and B2, then we want C : write(l)such that C . A, B1 . C and B2 \n. C, because otherwise the read at A might depend on whether the evaluation chose to do B1 .rst or B2 \n.rst, i.e., we have another kind of race-condition. Perhaps somewhat surprisingly, satisfying these two \nconditions turns out to be suf.cient to ensure con.uence. We now formalizes this discussion. For B : \nwrite(l), we use the ! shorthand B . A if for any C : write(l)such that C . A and C = B, we have C . \nB. Now, for any A : read(l), there exists ! at most one B : write(l)such that B .A. Note that the second \ncondition above is equivalent to requiring that for any A : read(l), either there is no B : write(l) \nsuch that B . A or there is a ! B : write(l)such that B .A A1 A2 A1 A2   ! !! !  B1 B2B1B2 (a)(b) \n Figure 3: Possible orderings between pairs of reads and writes in a witness race free trace graph. DEFINITION \n3 (Witness Race Freedom). We say that a trace graph (V,E)is witness race free if for every location l, \n for every A : read(l) . V and B : write(l) . V, either A . B or B . A, and  for every A : read(l). \nV, either there is no B : write(l). V such that B . A or there is a B : write(l) . V such that  ! B \n.A. We say that a program e is witness race free if every trace graph of e is witness race free. THEOREM \n2. If e is witness race free then e is con.uent. The proof appears in our companion technical report \n[13]. While witness race freedom is a suf.cient condition, it is not necessary. For example, if for each \nreference location writes happen to never change the location s value, then the program is trivially \ncon.uent regardless of the order of reads and writes. Another example is a program using implicit order \nof evaluation: e.g., in .wit , expressions are not reduced under . so a function body is evaluated only \nafter a call. Hence a program that stores a function in a reference location, reads the reference location \nto call the function, and then writes in the same reference location from the body of the called function \nis con.uent because the write always happens after the read despite the write not seeing the witness \nof the read. Nevertheless, witness race freedom is almost complete in a sense that if the only way to \norder two side-effects is to make one see a witness of the other, and if we cannot assume anything about \nwhat expressions are written and how the contents are used, then it is the weakest condition guaranteeing \ncorrectness. In particular, if the trace graphs are the only information available about a program, then \nwitness race freedom becomes a necessary condition. The result in this section can be extended to most \nother func\u00adtional program transformations, because witness race freedom is an entirely semantic condition. \nHowever, the static checking algo\u00adrithm described in Section 4 is not as forgiving, which is why we have \nrestricted the set of program transformations to that of the se\u00admantics of .wit . For example, the algorithm \nis unsound for general call-by-name reduction. 4. Types for Statically Checking Witness Race Freedom \nWhile the concept of witnesses is straightforward, it may neverthe\u00adless be desirable to have an automated \nway of checking whether an arbitrary .wit program is witness race free. Witness race freedom may be checked \ndirectly by checking every program trace, which is computationally infeasible. Instead, we exploit a \nspecial prop\u00aderty of witness-race-free trace graphs to design a sound algorithm that can ef.ciently verify \na large subset of witness-race-free .wit programs. The key observation is that any witness-race-free \ntrace graph contains for each reference location la subgraph that we shall call Figure 4: A read-write \npipeline with bottlenecks for a reference location l. a read-write pipeline with bottlenecks. We shall \ndesign an algorithm that detects these subgraphs instead of directly checking the witness race freedom \ncondition. Consider a witness-race-free trace graph. Suppose there are A1,A2 : write(l)and B1,B2 : read(l)such \nthat A1 = A2, A1! B1 and A2! B2. Due to witness race freedom, it must be the case that B2 A1 or A1 B2. \nIf the former is the case, we have the relation as depicted in Figure 3 (a). Suppose that the latter \nis the case. Then, since A2! B2, it must be the case that A1 A2. Consider A2 and B1. Due to witness race \nfreedom again, it must be the case that either A2 B1 or B1 A2. But if A2 B1, then since A1! B1, it must \nbe the case that A2 A1. But this is impossible since A2 A1 A2 forms a cycle. So it must be the case that \nB1 A2, and we have the relation as depicted in Figure 3 (b). Further reasoning along this line of thought \nreveals that for a witness-race-free trace graph, for any reference location l, the nodes in the set \nX = {A : write(l) |.B : read(l).A ! B} are totally ordered (with as the ordering relation), and that \nthese nodes partition all read(l)nodes and write(l)nodes in a way depicted in Figure 4 where X = {A1,...,An}. \nIn the .gure, each Ri and Wi is a collection of nodes. No Ri contains a write(l)node and no Wi contains \na read(l)nodes. Each Ai is one write(l)node. An arrow from X to Y means that there is a path from each \nwrite(l) node or read(l)node in X to each write(l)node or read(l)node in Y, except if a Wi contains no \nsuch node, then there is a path from each read(l) . Ri to Ai. Each Ri for i =1 must contain at least \none read(l). Arrows just imply the presence of paths, and hence there can be more paths than the ones \nimplied by the arrows, e.g., paths to/from nodes that are not in the diagram, paths to and from nodes \nin the same collection, and even paths relating the collections in the diagram such as one that goes \ndirectly from Ri to Ai, bypassing Wi. The graph in Figure 4 can be described formally as a subgraph of \nthe trace graph satisfying certain properties. DEFINITION 4. Given a trace graph (V,E)and a reference \nloca\u00adtion l, we call its subgraph Gl a read-write pipeline with bottle\u00adnecks if Gl consists of collections \nof nodes R1,R2,..., Rn and W1,W2,..., Wn with the following properties: S n {A | A : read(l). V}. i=1 \nRi, S n {A | A : write(l). V}. i=1 Wi,  R1, ..., Rn, W1, ..., Wn, restricted to write(l) nodes and \nread(l)nodes are pairwise disjoint,  for each A : read(l). Ri and B : write(l). Wi, AB,  for each Ri \nsuch that i =1, there exists at least one A : read(l). Ri, and  there exists A : write(l) . Wi for all \ni = n such that for all  B : read(l). Ri+1 and all C : write(l). Wi, A ! B and CA. Note that each collection \nRi and Wi corresponds to the collection of nodes marked by the same name in Figure 4 but with each e \n:= | | x | i | .x.e | e e ' | let x = e in e ' | e . e ' | pi(e) write e1 e2 e3 | read e e ' | ref e \ne ' | join e e ' | letreg x e Figure 5: The syntax of .reg wit . E := | D. {a . E} | []| E e | e E | \nE . e | e . E pi(E)| write E e e ' | write e E e ' | write e e ' E | read eE | read Ee | ref Ee | ref \neE | join Ee | join eE App (R,S,E[(.x.e)e ' ]) . (R,S,E[e[a/x]].{a . e ' }) Let (R,S,E[let x = e in e \n' ])  . (R,S,E[e ' [a/x]].{a . e[a/x]}) Pair (R,S,E[pi(e1 . e2)]). (R,S,E[ei]) Write (R,S,E[write le \n]  . (R,S[l . a],E[ ].{a . e}) Read (R,S,E[read l ]). (R,S,E[S(l). ]) Ref (R,S,E[ref er])  . (R,S .{l \n. a},E[l].{a . e}) Join (R,S,E[join ]). (R,S,E[ ]) LetReg (R,S,E[letreg xe])  . (R.{r},S,E[e[a/x]].{a \n. r}) Arrive (R,S,E[a].{a . e}). (R,S,E[e].{a . e}) where e . V  GC (R,S,D. D ' ). (R,S,D) where ../dom(D \n' ). dom(D ' )n free(D)= \u00d8 Figure 6: The semantics of .reg . wit node Ai included in the collection \nWi. The bottlenecks are the Ai s. Note that a trace graph (V,E)actually contains a read\u00adwrite pipeline \nwith bottlenecks per each reference location l as a subgraph Gl (but the subgraphs may not be disjoint \nbecause the paths may involve other locations and share join nodes). The following theorem formalizes \nour earlier informal discus\u00adsion. THEOREM 3. A trace graph (V,E)is witness race free if and only if it \nhas a read-write pipeline with bottlenecks for every l. The proof appears in our companion technical \nreport [13]. The following is an immediate consequence: COROLLARY 1. A .wit program e is witness race \nfree if and only if every trace graph of e has a read-write pipeline with bottlenecks for every reference \nlocation l. 4.1 Regions Corollary 1 reduces the problem of deciding whether a program e is witness race \nfree to the problem of deciding if every trace graph of e has a read-write pipeline with bottlenecks \nfor every reference location l. Therefore it suf.ces to design an algorithm for solving the latter problem. \nBut before we do so, we make a slight change to .wit to make the problem more tractable. In .wit , there \nis a read\u00adwrite pipeline with bottlenecks for each reference location l, but distinguishing dynamically \nallocated reference locations individu\u00adally is dif.cult for a compile-time algorithm. Therefore, we add \nregions to the language so that programs can explicitly group refer\u00adence locations that are to be tracked \ntogether. Figure 5 shows .reg , .wit extended with regions. The syntax wit contains two new expression \nkinds: letreg xe which creates a new region and ref ee ' which places the newly created reference in \nregion e ' ; ref ee ' replaces ref e. Figure 6 gives the semantics of .reg wit which differs from .wit \nin two small ways. First, a state now contains a set of regions R. We use symbols r,r ' ,ri, etc. to \ndenote regions. Regions are safe to duplicated, i.e., r . V. The R s are used only for ensuring that \nthe newly created region r at a LetReg step is fresh. (We overload the symbol . such that R.R ' = R.R \n' if R n R ' = \u00d8 and, is unde.ned otherwise.) Note that evaluation contexts E do not extend to the subexpressions \nof letreg xe. The second difference is that a Ref step now takes a region r along with the initializer \ne to indicate that the newly created reference location lbelongs to the region r. Note that the semantics \ndoes not actually associate the reference location l and the region r, and therefore grouping of reference \nlocations via regions is entirely conceptual.5 Regions force programs to abide by witness race freedom \nat the granularity of regions instead of at the granularity of individual reference locations. That is, \ninstead of read(l)nodes and write(l) nodes, we use read(r)nodes and write(r)nodes. Formally, a trace \ngraph for .reg is constructed by the following graph construction wit semantics: Write (R,K,S,E[write \nleA]) . (R,K,S[l . a],E[B].{a . e}) V:=V .{B} where B isa new write(K(l))node E:=E .{(A,B)} Read (R,K,S,E[read \nlA]). (R,K,S,E[S(l). B]) V:=V .{B} where B isa new read(K(l))node E:=E .{(A,B)} Join (R,K,S,E[join AB]). \n(R,K,S,E[C]) V:=V .{C} where C is a new join node E:=E .{(A,C),(B,C)} Ref (R,K,S,E[ref er]) . (R,K .{l \n. r},S .{l . a},E[l].{a . e}) Note that these reduction rules use an additional function K which is a \nmapping from reference locations to regions. The mapping K starts empty at the beginning of evaluation. \nOther reductions rules are unmodi.ed except that the function K is passed from left to right in the obvious \nway. Since there is less information available in a .reg trace graph wit than in a .wit trace graph, \nthe witness race freedom condition is more conservative for .reg . That is, we still need the condition \nwit that for any A : write(r) and B : read(r), either AB or BA. But we need to tighten the second condition \nso that for any A : read(r)if there are B1,B2 : write(r)such that B1 A and B2 A, then either B1 B2 or \nB2 B1. This condition is strictly more conservative than for .wit , which only requires some C : write(r)such \nthat C ! A in such a situation. The reason for this conservativeness is that we do not know from a trace \ngraph of .reg wit whether B1 and B2 both write to the same reference location. Formally, witness race \nfreedom for .reg can be de.ned as fol\u00ad wit lows. DEFINITION 5 (Witness Race Freedom for .reg ). We say \nthat a wit .reg wit trace graph (V,E)is witness race free if for every region r, for every A : read(r). \nV and B : read(r). V, either AB or BA, and  for every A : read(r) . V and B1,B2 : write(r) . V such \nthat B1 Aand B2 A, we have B1 B2 or B2 B1.  THEOREM 4. If a .reg program e is witness race free, then \ne is wit con.uent. 5 Regions are traditionally coupled with some semantic meaning such as memory management \n[14, 5]. It is possible to extend .reg to do similar wit things with its regions. Proof (Sketch): For \nany evaluation of e, carry out the same reduc\u00adtion sequence with the trace graph building action of .wit \n, i.e., the trace graph Ggenerated is at the granularity of reference locations. Then it is easy to see \nthat if Gsatis.es the above two conditions, G also satis.es the two conditions of Theorem 2. 0 It is \neasy to see that De.nition 5 is the weakest possible re\u00adstriction to the original witness race freedom \nunder the region ab\u00adstraction because for any .reg trace graph that is not witness race wit free, one \ncan easily .nd a non-con.uent program that produces the graph. .reg In a witness-race-free trace graph \nfor wit , the read-write pipeline with bottlenecks for a region r consisting of the collec\u00adtions (R1,..., \nRn,W1,..., Wn)has the following property: each set {A | A : write(r). Wi} for i = n can be totally ordered \n(with as the ordering relation). The theorem below is immediate from Corollary 1 under this additional \nproperty. THEOREM 5. A .reg program e is witness race free if and only if wit every trace graph of e \nhas a read-write pipeline with bottlenecks for every region r. This additional property helps in designing \na static checking algo\u00adrithm. 4.2 From Network Flow to Types Now our goal is to design an algorithm \nfor statically checking if every trace graph of a .reg program e has a read-write pipeline with wit \nbottlenecks for every region r. Our approach exploits a network .ow property of read-write pipelines \nwith bottlenecks. Consider a trace graph as a network of nodes with each edge (A,B)able to carry any \nnon-negative .ow from A to B. (Recall edges are directed.) As usual with network .ow, we require that \nthe total incoming .ow equal the total outgoing .ow for every node in the graph. Now, let us add a virtual \nsource node AS and connect it to every node B by adding an edge (AS,B). We assign incoming .ow 1 to AS. \nThen it is not hard to see that if there exists a read\u00adwrite pipeline with bottlenecks for the region \nr then there exists .ow assignments such that every read(r)node and write(r)node gets a positive .ow \nand every A : write(r). Wi for i = n gets a .ow equal to 1. It turns out that the converse also holds. \nThat is, given a trace graph, if there is a .ow assignment such that each read(r)node and write(r)node \ngets a positive .ow and each A : write(r)such that there exists B : read(r)with AB gets a .ow equal to \n1, then there is a read-write pipeline with bottlenecks for the region r. By Theorem 5, this implies \nthat there exists such a .ow assignment for every region r if and only if the trace graph is witness \nrace free. Because edges in a trace graph are traces of witnesses, our idea is to assign a type to a \nwitness such that the type contains .ow assignments for each (static) region. We use this idea to design \na type system such that a well-typed program is guaranteed to be witness race free. Formally, a witness \ntype W is a function from the set of static region identi.ers RegIDs to rational numbers in the range \n[0,1], i.e., W : RegIDs . [0,1]. The rational number W(.)indicates the .ow amount for the static region \nidenti.er .in the witness type W. We use the notation {.1 . q1,...,.n . qn} to mean a witness type W \nsuch that W(.)= qi if . = .i for some 1 = i = n and W(.) =0 otherwise. (We use the symbols q, qi, q ' \n, etc for non-negative rational numbers, including those larger than 1.) The rest of the types are de.ned \nin Figure 7, including types q include integer types int, function types t . t ' , pair types t . t \n' , reference types ref (t,t ' ,.), and region types reg(.). The non\u00ad q negative rational number q in \nt . t ' represents the number of times the function can be called. We allow the symbols q,q ' , etc q \n'' ' t := int | t . t | t . t | ref (t,t ,.)| reg(.)| W Figure 7: The type language. G;W . e : tt = t \n' G(x)= t Sub Var G;W . e : t ' G;W . x : t Int Dummy G;W . i : int G;W . : \u00d8 G;W1 . e : W2 Source \nG;W1 +W3 . e : W2 + W3 G,x:t;W . e : t ' Abs q G\u00d7 q;W \u00d7 q . .x.e : t . t ' q ' G '' ' G;W . e : t . t \n;W . e : tq = 1 App ' ' ' G+G ' ;W +W . ee : t ' '' G ' G;W . e : t ;W . e : t Pair ' '' G+G ' ;W + \nW . e . e : t . t G;W . e : t1 . t2 Proj G;W . pi(e): ti G '' ' G;W . e : t ;W . e : reg(.) Ref G+G \n' ;W +W ' . ref ee ' : ref (t,t,.) G1;W1 . e1 : ref (t,t ' ,.) G2;W2 . e2 : t ' G3;W3 . e3 : WW(.)= 1 \n Write G1 +G2 +G3;W1 + W2 + W3 . write e1 e2 e3 : W G;W1 . e : ref (t,t ' ,.) G ' ' ;W2 . e : WW(.)> \n0 Read G+G ' ;W1 + W2 . read ee ' : W . t G,x:reg(.);W + {. . q}. e : tq = 1 ./. free(G). free(W). free(t) \n LetRegion G;W . letreg xe : t G;W1 . e : W G;W2 . e ' : W ' Join G+G ' ;W1 + W2 . join ee ' : W + \nW ' G;W . e ' [(let x = e in x)/x]: te . V+ LetA G;W . let x = e in e ' : t ' '' G ' G,x:t;W . e : t \n,x:t;W . e : t t = t \u00d78 if x . free(e)  LetB ' '' G+G ' ;W +W . let x = e in e : t Figure 8: Type judgment \nrules. to take the valuation 8 to imply that the function can be called arbitrarily many times. We use \nthe following arithmetic relation: q+ 8 = 8, q\u00d78 = 8 for q =0, and 0\u00d78 =0. Figure 8 shows the main type \njudgment rules. Our type system belongs to the family of substructural type systems, which includes linear \ntypes. We discuss the rules from top-to-bottom and left-to\u00ad reg(.)+reg(.)= reg(.) int + int = int '' \nq qq+q ' ' ' t . t +t . t = t . t t1 . t2 + t3 . t4 =(t1 + t3). (t2 + t4) ref (t1,t,.)+ref (t2,t,.)= \nref (t1 +t2,t,.) W + W ' = {. . W(.)+W ' (.)| . . RegIDs} reg(.)\u00d7 q = reg(.) int \u00d7 q = int '' qq \u00d7q \nt . t ' \u00d7 q = t . t ' t . t ' \u00d7 q =(t \u00d7 q). (t ' \u00d7 q) ref (t,t ' ,.)\u00d7 q = ref (t \u00d7 q,t ' ,.) W \u00d7 q = \n{. . W(.)\u00d7 q | . . RegIDs} Figure 9: Arithmetic over types. right, except for the rule Sub which we \ndefer to the end. The rules Var and Int are standard. The rule Dummy gives a dummy witness an empty witness \ntype; note that \u00d8(.) =0 for any static region identi.er .. The rule Source uses additive arithmetic over \ntypes de.ned in Figure 9. The rule adds W3 amount of .ow from the virtual source nodes (i.e., nodes AS \nfrom the .rst paragraph of this section) to W2. In the type judgment relation G;W . e : t, the witness \ntype W represents the .ow the expression e receives from the virtual source nodes. Therefore, the rule \nSource says that assuming that we took W1 .ow from the virtual source nodes in the precondition, we are \nnow taking W3 more. In the rule Abs, we multiply the left hand side of the judgments by the number of \ntimes that the function can be used. Multiplication over type environments Gis de.ned as follows: (G,x:t)\u00d7 \nq =(G\u00d7 q),x:(t \u00d7 q) So for example, if the . abstraction .x.e captures a witness as a free variable \ny and that G(y)= W, then (G\u00d7 q)(y)= W \u00d7 q. Thus if the function body requires W amount of .ow in the \nwitness, then we actually require W\u00d7qamount of .ow because the function may be called q times. In the \nrule App, the precondition q = 1says that the number of times the function can be used must be at least \n1. The left hand side of the two judgments in the precondition are added so that we can compute the combined \n.ow required for the expressions e and e ' . Addition over type environments is de.ned as follows: (G,x:t)+(G \n' ,x:t ' )=(G+G ' ),x:(t + t ' ) The rules Pair and Proj are self-explanatory. In a reference type ref \n(t,t ' ,.), the static region identi.er . identi.es the region where the reference belongs while the \ntype t is the read type of the reference and the type t ' is the write type of the reference. Initially \nthe read and write types are the same as seen in the rule Ref. The rule Write matches the type of the \nto-be-assigned expression e2 with the write type of the reference while the rule Read uses the read type \nof the reference type. Note that we require W(.)= 1at Write and W(.)> 0at Read; both correspond to the \n.ow requirement for writes and reads. The reason read-type/write\u00adtype separation is subtle. Consider \nthe following expression where the expressions e1 and e3 are witnesses and the expression e2 is a region: \nlet x = (ref e1 e2)in let w = (write xe3 )in read xw Suppose we just have read types so that the type \nsystem uses read types at instances Write as well as at instances Read. Then the type system is unsound \n(even without Sub) for the following reason. The type system may assign some .ow W to the occurrence \nof ' '' t1 = t1 t2 = t2 q = q t = t q' q ' ' t1 . t2 = t1 . t2 t1 = t1 ' t2 = t2 ' t1 . t2 = t1 ' . t2 \n' t1 = t1 ' t2 = t2 ' ref (t1,t2,.)= ref (t1' ,t 2' ,.) W(.)= W(.)for all . . RegIDs W = W ' Figure \n10: Subtyping. the variable x at the write and some .ow W ' to the occurrence of the variable x at the \nread. But there is no constraint to force W = W ', so the type system can let W ' >W while keeping the \nsum W + W ' .xed, i.e., we get more .ow from a reference than what was assigned to the reference. Separating \nread and write types prevents this problem because addition and multiplication do not act on write types. \nThe rule LetRegion introduces a fresh static region identi.er .. The witness type {. . q} represents \nthe virtual source node for the new region. We constrain q = 1 to ensure that we do not use more than \n1unit total from the source. The rule Join combines two witnesses by adding their types. There are two \nrules, LetA and LetB, for the expression kind let x = e in e ' . LetA is less conservative and should \nbe used whenever x occurs more than once in e ' and e . V+ where V+ is the smallest set such that V+ \n= V .{let x = e in x | e . V+}. This rule corresponds to the usual substitution interpretation of let\u00adbased \npredicative polymorphism with the value restriction. LetB is used if e/. V+ or x occurs at most once \nin e ' . Here, free(t)is the set of static region identi.ers in the type t where free(W)= {. | S W(.)=0}, \nand free(G)= free(t). t.ran(G) Finally, we come return to Sub. The subtyping relation is de\u00ad.ned in Figure \n10. As usual, argument types of function types are contravariant. Write types of reference types are \nalso contravariant; this treatment of reference subtyping is identical to that of a type\u00adbased formulation \nof Andersen s points-to analysis [4]. Intuitively, the rule Sub expresses the observation that the .ow \ngraph property may be relaxed so that the sum of the outgoing .ow can be less than the sum of the incoming \n.ow, i.e., if we could .nd a .ow as\u00adsignment satisfying the required .ow constraints at reads and writes \nunder this relaxed condition, then we still have a read-write pipeline with bottlenecks. We say that \na .reg program e is well-typed if \u00d8;\u00d8. e : t for wit some type t. The following theorem states that the \ntype system is sound. THEOREM 6. If a .reg program e is well-typed, then it is witness wit race free. \nThe proof, which uses the network .ow property, appears in the companion technical report [13]. We point \nout a few of the positive properties of this type system. If a program contains no reads or writes and \ncan be typed by a standard Hindley-Milner polymorphic type system, then it can also be typed by our type \nsystem; for example, we may use the quali.er 8 for all function types and use 0 for all .ows. In general, \nwe can give the 8 quali.er to the function type of any function that does not capture a witness (directly \nor indirectly). We can also assign .ow 0 to any .ow for a region r that does not .ow into a side\u00adeffect \nprimitive operating on the region r. Fresh(int)= int \u00df Fresh(s.s)= Fresh(s). Fresh(s) where \u00df is fresh \nFresh(s . s ' )= Fresh(s). Fresh(s) Fresh(ref (s,s ' ,.))= ref (Fresh(s),Fresh(s ' ),.) Fresh(reg(.))= \nreg(.) Fresh(I)= {. . a | . . I} where a is fresh Figure 11: Fresh. G,W .b e I : t,C G,W +Fresh(I).a \ne I : t +Fresh(I),C G,W .b e s : t,C s/. type I G,W .a e s : t,C Figure 12: Type inference .a. The type \nsystem is quite expressive. In particular, it is able to type all of the examples that were used as correct \nprograms up to this point in the paper (with straightforward modi.cation to translate .wit programs into \n.reg ). In fact, assuming that for wit each region r, write(r) nodes in each collection Wi are totally \nordered, the type system is complete for the .rst-order fragment (i.e., no higher order functions) with \nno recursion and no storing of witnesses in references. That is, such a program e is witness race free \nand well-typed by a standard Hindley-Milner type system if and only if it is well-typed by our type system. \nWe also show later in Section 5 that the type system is more expressive than past approaches. The limitations \nof the type system are the standard ones: let\u00adbased predicative polymorphism, .ow-insensitivity of reference \ntypes, and unsoundness under call-by-name semantics; the latter is a typical limitation of a non-linear \nsubstructural type system. Another limitation is that the type system enforces for each region r that \nwrite(r) nodes in every collection Wi are totally ordered whereas witness race freedom permits an absence \nof ordering for the case i = n; we believe that this is a minor limitation.  4.3 Inference We next present \na type inference algorithm. By Theorem 6, this results in an automatic algorithm for statically checking \nwitness race freedom. At a high-level, our type system is a standard Hindley-Milner type system with \nsome additional rational arithmetic constraints. Therefore we could perform inference by employing a \nstandard type inference technique to solve all type-structural constraints while generating rational \narithmetic constraints on the side, and then solving the generated arithmetic constraints separately. \nUn\u00adfortunately, the arithmetic constraints may be non-linear since they involve the multiplication of \nvariables. Because there is no ef.cient algorithm for solving general non-linear rational arithmetic \ncon\u00adstraints, we need to dive into lower-level details of the type system. Let us separate type inference \ninto two phases. The .rst phase carries out type inference after erasing all rational numbers from the \ntype system. That is, the types inferred in this phase are: s := int | s.s ' | s . s ' | ref (s,s ' ,.)| \nreg(.)| I where a type I is a subset of RegIDs. Intuitively, each type I rep\u00adresents the non-0 domain \nof some witness type W. The .rst phase can be carried out by a standard Hindley-Milner type inference, \nal\u00ad t = Fresh(s) {x:t},\u00d8.b x s : t,\u00d8 \u00d8,\u00d8.b i : int,\u00d8\u00d8,\u00d8.b : \u00d8,\u00d8 G,W .a e : t,C \u00df is fresh \u00df G\u00d7 \u00df,W \u00d7 \n\u00df .b .x.e : G(x). t,C G1,W1 .a e1 : t1,C1 G2,W2 .a e2 : t2,C2 t = Fresh(s) t ' = Fresh(s ' ) \u00df is fresh \n\u00df C = C1 .C2 .{t1 = t . t ' ,\u00df = 1,t2 = t} s.s ' ' G1 +G2,W1 + W2 .b e1 e2 : t,C G1,W1 .a e1 : t1,C1 \nG2,W2 .a e2 : t2,C2 G1 +G2,W1 + W2 .b e1 . e2 : t1 . t2,C1 .C2 G,W .a pi(e): t,C t1 = Fresh(s) t2 = Fresh(s \n' ) s1 .s2 ) G,W .b pi(e : ti,C.{t = t1 . t2} G1,W1 .a e1 : t1,C1 G2,W2 .a e2 : t2,C2 reg(.) G1 +G2,W1 \n+ W2 .b ref e1 e2 : ref (t1,t2,.),C1 .C2 G1,W1 .a e1 : t1,C1 G2,W2 .a e2 : t2,C2 G3,W3 .a e2 : t3,C3 \nt = Fresh(s) t ' = Fresh(s ' ) C = C1 .C2 .{t1 = ref (t,t ' ,.),t2 = t ' ,t3(.)= 1} G =G1 +G2 +G3 W = \nW1 + W2 + W3 ref (s,s ' ,.) G,W .b write e 1 e2 e3 : t3,C G1,W1 .a e1 : t1,C1 G2,W2 .a e2 : t2,C2 t = \nFresh(s) t ' = Fresh(s ' ) C = C1 .C2 .{t1 = ref (t,t ' ,.),t2(.)> 0} ref (s,s ' ,.) G1 +G2,W1 +W2 .b \nread e1 e2 : t . t2,C G,W .a e : t,C reg(.) G,W .b letreg xe : t,C.{W(.)= 1} G1,W1 .a e1 : t1,C1 G2,W2 \n.a e2 : t2,C2 G1 +G2,W1 + W2 .b join e1 e2 : t1 + t2,C1 .C2 G1,W1 .a e1 : t1,C1 G2,W2 .a e2 : t2,C2 x/. \nfree(e1) C = C1 .C2 .{t1 = G2(x)} G1 +G2,W1 + W2 .b let x = e1 in e2 : t2,C G1,W1 .a e1 : t1,C1 G2,W2 \n.a e2 : t2,C2 x . free(e1) C = C1 .C2 .{t1 = G1(x)\u00d78,t1 = G2(x)} G1 +G2,W1 + W2 .b let x = e1 in e2 \n: t2,C Figure 13: Type inference .b. beit with regions, which is no harder than type variables. We omit \nthe details of this phase. We may safely reject the program if the .rst phase fails. Otherwise we annotate \neach subexpression e by its inferred type s: e s. In the second phase, we use the annotated pro\u00adgram \nto generate the appropriate rational arithmetic constraints via bottom-up type-inference. Let e be an \nannotated program. Then the generated constraints for e is C where G,W .a e : t,C for some G, W, and \nt. The second-phase type inference rules are separated into two kinds, .a (Figure 12) and .b (Figure \n13), which must occur in strictly interleaving manner. The purpose of .a is to account for the type judgment \nrule Source, whereas .b accounts for all other rules. We should note that, strictly speaking, types t \nappearing in the algorithm are different from the ones in the type judgment rules. That is, instead of \nrational numbers, the types t in the algorithm are quali.ed by rational number variables a,\u00df,., etc. \nAlso the domain of a witness type W is not the entire RegIDs set but only some subset of it. In other \nwords, a witness type W is a partial function from RegIDs to rational number variables. We re-de.ne the \naddition of witness types as follows to re.ect the change: W +W ' = {. . W(.)+W ' (.)| . . dom(W). . \n. dom(W ' )} .{. . W(.)| . . dom(W). ./. dom(W ' )} .{. . W ' (.)| . . dom(W ' ). ./. dom(W)} We also \nre-de.ne the addition of type environments: G+G ' = {x:G(x)+G ' (x)| x . dom(G). x . dom(G ' )} .{x:G(x)| \nx . dom(G). x/. dom(G ' )} .{x:G ' (x)| x . dom(G ' ). x/. dom(G)} Note that we omit annotations when \nthey are not used (i.e., we say e instead of e s, etc.). There are only two cases for .a. The .rst case \nis for expressions that were given a witness type I in the .rst phase. In this case, we add Fresh(I)to \nt and W to account for a possible application of Source. Fresh is de.ned in Figure 11. The second case \nis for expressions that were not given a witness type. In this case, we simply pass the result of the \nsubderivation .b up. We discuss a few representative .b rules. Note that .b rules are syntax directed. \nIn the case of a variable x s, we create a fresh t from s and pass {x:t},\u00d8.b x s : t,\u00d8 up to the parent \nderivation. (Recall our type inference is bottom-up.) The case for integers and dummy witnesses are trivial. \nIn the case of an abstraction .x.e, we multiply G and W passed from the subderivation by \u00df. In the s.s \n' case of a function application e1 e2, we add the constraints \u00df {t1 = t . t ' ,\u00df = 1,t2 = t} to connect \narguments and returns as well as requiring \u00df to be at least 1. Note that the type rule Sub is implicitly \nincorporated in the constraints. In the case ref (s,s ' ,.) of write e1 e2 e3, we add the constraint \nt2(.) = 1 to match the type rule Write. Note that the .rst phase guarantees reg(.) that . . dom(t3). \nIn the case letreg xe, the constraint W(.)= 1 is effective only when . . dom(W)as ./. dom(W) implies \nthat the region was not used at all. Note that there is no case corresponding to the type rule LetA. \nPrior to running the algorithm, we replace each occurrence of the expression let x = e in e ' in the \nprogram by the expression e ' [(let x = e in x)/x]whenever e . V and x occurs more than once in e ' . \nAs an example, consider the following program (a .reg version wit of the last example from Section 2): \nletreg r let x = ref 1r in let w = write x 2 in let f = .y.read xw in let z = (f 0). (f 0) in let w = \nwrite x 3join p2(p1(z))p2(p2(z))in z Suppose the .rst phase assigns r the type reg(.). Assume each let-bound \nvariable is treated monomorphically. The second phase generates the following constraints for the let-bound \nexpressions (slightly simpli.ed for readability): {r:reg(.)};\u00d8.a ref 1r : ref (int,int,.),\u00d8 G;{. . .1 \n+ .2}.a write x 2 : {. . .1 +.2},{.1 = 1} where G= {x:ref (int,int,.)} \u00df1 G;W .a .y.read xw : int . int \n.{. . a1 + .3},C where G= {x:ref (int,int,.),w:{. . a1 \u00d7 \u00df1}} and W = {. . .3 \u00d7 \u00df1} and C = {a1 + .3 \n> 0} G;\u00d8.a (f 0). (f 0) : t,C \u00df2 +\u00df3 where G= {f:int . int .{. . a2}} and t =(int .{. . a2}). (int .{. \n. a2}) and C = {\u00df2 = 1,\u00df3 = 1} G;{. . .4}.a write x 3... : {. . a3 + a4 + .4},C where G= {x:ref (int,int,.),z:t} \nand t =(int .{. . a3}). (int .{. . a4}) and C = {a3 + a4 +.4 = 1} The .nal constraints, after some simpli.cation, \nis as follows: .1 = 1,1 = .1 + .2 + .3 \u00d7 \u00df1 + .4,\u00df1 = \u00df2 + \u00df3, \u00df2 = 1,\u00df3 = 1,.1 + .2 = a1 \u00d7 \u00df1,a1 +.3 \n> 0, a1 +.3 = a2,a3 + a4 +.4 = 1,a2 = a3,a2 = a4 Note that the constraints are satis.able, e.g., by the \nsubstitution \u00df1 =2 \u00df2 = \u00df3 = .1 =1 a1 = a2 = a3 = a4 =0.5 .2 = .3 = .4 =0 In general, a program e is \nwell-typed if and only if the con\u00adstraints C generated by type inference are satis.able. So it suf.ces \nto show that the satisfaction problem for any C generated by the type inference algorithm can be solved. \nTo this end, we .rst note that because of the .rst phase, any constraint t = t ' .C can be reduced to \na set of rational arithmetic constraints of the form p = p ' where p,p ' are rational polynomials. The \ntroublesome non-linearity comes from G\u00d7 \u00df and W\u00d7 \u00df in the .x.ecase. Let us focus our attention on the \nset B of variables used in such multiplications. (Note that we have used \u00df instead of ajust for this \ncase in the pseudo-code to make it clear that these variables are special.) We can show that the following \nholds: THEOREM 7. Let p I p ' .C where I . {=,>}. If \u00df .B occurs in the polynomial p, then it must be \nthe case that I ==, p = \u00df, and that the polynomial p ' consists only of symbols in the set {+,\u00d7,1,8} \n.B. Proof (Sketch): Let G,W .a e : t,C. For any t ' . G, \u00d7 and + only appear at the top-level, i.e., \nnot within argument and return types of a function type. Secondly, we can show by induction that within \nthe type t, \u00d7 only appears in negative positions. More precisely, for any p . Pos(t), the polynomial \np contains no \u00d7 where Pos is de.ned as follows: p Pos(t . t ' )= Neg(t). Pos(t ' ).{p} Pos(t . t ' )= \nPos(t). Pos(t ' ) Pos(ref (t,t ' ,.))= Pos(t). Neg(t ' ) Pos(W)= ran(W) Neg(int)= Pos(int)= Neg(reg(.))= \nPos(reg(.))= \u00d8 p Neg(t . t ' )= Pos(t). Neg(t ' ) Neg(t . t ' )= Neg(t). Neg(t ' ) Neg(ref (t,t ' ,.))= \nNeg(t). Pos(t ' ) Neg(W)= \u00d8 Third, for any + that appears in a positive position of t, i.e. in some \np . Pos(t), the polynomial p does not contain any \u00df .B. Then the result follows from inspection of the \nsubtyping rules. 0 The theorem implies that we can compute all assignments to the variables in B by computing \nthe minimum satisfying assignment for C ' = {\u00df = p | \u00df . B} . C. It is easy to see that such an assignment \nalways exists. (Recall that the range is non-negative.) This problem can be solved in quadratic time \nby an iterative method in which all variables are initially set to 0, and at each iteration the new values \nfor the variables are computed by taking the maximum of the right hand polynomials evaluated at the current \nvalues. It is possible to show that if the minimum satisfying assignment for a variable \u00df is some q< \n8, then the iterative method .nds q for \u00df in 2|C ' | iterations. Hence any variable changing after the \n2|C ' |th iteration can be safely set to 8. All variables are then guaranteed to converge within 3|C \n' | iterations. Because each iteration examines every constraint, the overall time is at most quadratic \nin the size of C ' . Substituting the computed assignments for B in C results in linear rational constraints, \nwhich can be solved ef.ciently by a linear programming algorithm.  5. Related Work Adding side-effects \nto a functional language is an old problem with many proposed solutions. Here we compare our technique \nagainst two of the more prominent approaches: linear types [15, 6, 1] and monads [10, 11, 7, 9]. In linear \ntypes, there is an explicit world program value (or one world per region for languages with regions) \nthat conceptually represents the current program state. By requiring each world have a linear type, the \ntype system ensures that the world can be updated in place. The linear type system can be expressed in \nour type system by restricting every .ow to 1, every witness to contain only one .ow, and designating \none dummy witness to serve the role of the starting witness (or for regions, one dummy witness per region). \nThus our approach is more expressive than such an approach. Note that this also implies that every function \ntype can be restricted to have either 1 or 8 as the quali.er. It is easy to see that the program is well-typed \nunder this restriction if and only if it is well\u00adtyped by the linear type system. The restriction limits \nprograms to manipulate witnesses only in a linear fashion. In practice, this implies that there can be \nno parallel reads, no dead witnesses, no redundant witnesses, and no duplication of values containing \nwitnesses. Our approach can implement monadic primitives as follows (for concrete comparison, we use \nstate monads [9]): newVar = .x..y.let z = (ref xp2(y))in y. z readVar = .x..y.let z = (read xp1(y))in \n(p2(z). p2(y)). p1(z) writeVar = .x..w..y.let z = (write xwp1(y))in (z . p2(y)). w >>= = .f..g..x.let \ny = (fx)in gp2(y)p1(y) returnST = .x..y.y. x runST e = letreg xp2(e ( . x)) The idea behind these de.nitions \nis to implement each state monad of the type ST (a,t) as a function that takes a witness and the region \na as arguments and returns a witness, the region a, and a value of the type t. It is easy to see that \nif a state monad program is well-typed by the monadic type system, then it is also well-typed with our \ntype system using the above de.nitions for the monadic primitives. Thus, our approach is more expressive \nthan monads. In practice, a monadic approach shares essentially the same limitations as linear types; \nfor example, side-effects are restricted to a linear, sequential order. (In fact, it is not too hard \nto see that we can actually implement monadic primitives with the linear types restriction with only \nslightly longer code.) On the other hand, a monadic type system has an engineering advantage as it only \nrequires Hindley-Milner type inference. In addition to the above technical differences, our approach \ndiffers from previous approaches in its design motivation. That is, while our language feature, witnesses, \nis motivated by a pragmatic observation, understanding the motivation behind linear types and monads \n(i.e., not just knowing how to use them) arguably requires an appreciation of their underlying theory. \nThe technique used in our type system has some resemblance to fractional permissions [2] which can guarantee \nnon-interference in imperative programs. Indeed, it may be possible to give an interpretation of witnesses \nas some relaxation of permissions or capabilities [3] (after promoting permissions or capabilities to \n.rst class values) in situations where the program is witness race free.  6. Conclusions We have presented \na new approach to adding side-effects in purely functional languages based on witnesses. We have stated \na natural semantic correctness condition called witness race freedom and proposed a type-based approach \nfor statically checking witness race freedom.  References [1] P. Achten, J. H. G. van Groningen, and \nR. Plasmeijer. High level speci.cation of I/O in functional languages. In Proceedings of the 1992 Glasgow \nWorkshop on Functional Programming, pages 1 17. Springer-Verlag, 1993. [2] J. Boyland. Checking interference \nwith fractional permissions. In R. Cousot, editor, Static Analysis, Tenth International Symposium, volume \n2694 of Lecture Notes in Computer Science, pages 55 72, San Diego, CA, June 2003. Springer-Verlag. [3] \nK. Crary, D. Walker, and G. Morrisett. Typed memory management in a calculus of capabilities. In Proceedings \nof the 26th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 262 275, \nSan Antonio, Texas, Jan. 1999. [4] M. F\u00a8ahndrich, J. S. Foster, Z. Su, and A. Aiken. Partial online cycle \nelimination in inclusion constraint graphs. In Proceedings of the 1998 ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 85 96, Montreal, Canada, June 1998. [5] D. Grossman, G. Morrisett, \nT. Jim, M. Hicks, Y. Wang, and J. Cheney. Region-based memory management in Cyclone. In Proceedings of \nthe 2002 ACM SIGPLAN Conference on Programming Language Design and Implementation, Berlin, Germany, June \n2002. [6] J. C. Guzman and P. Hudak. Single-threaded polymorphic lambda calculus. In Proceedings, Fifth \nAnnual IEEE Symposium on Logic in Computer Science, pages 42 51, Philadelphia, Pennsylvania, June 1990. \n[7] S. L. P. Jones and P. Wadler. Imperative functional programming. In Proceedings of the 20th Annual \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 71 84, Charleston, South Carolina, \nJan. 1993. [8] J. Launchbury. A natural semantics for lazy evaluation. In Pro\u00adceedings of the 20th Annual \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 144 154, Charleston, South \nCarolina, Jan. 1993. [9] J. Launchbury and A. Sabry. Monadic state: Axiomatization of type safety. In \nProceedings of the 1997 ACM SIGPLAN International Conference on Functional Programming, pages 227 238, \nAmsterdam, The Netherlands, June 1997. [10] E. Moggi. Notions of computation and monads. Information \nand Computation, 93(1):55 92, 1991. [11] M. Odersky, D. Rabin, and P. Hudak. Call by name, assignment, \nand the lambda calculus. In Proceedings of the 20th Annual ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, pages 43 56, Charleston, South Carolina, Jan. 1993. [12] G. D. Plotkin. Call-by-name, \ncall-by-value and the lambda-calculus. Theoretical Computer Science, 1:125 159, 1975. [13] T. Terauchi \nand A. Aiken. Witnessing side-effects. Technical report, Computer Science Division, EECS Department, \nUniversity of California, Berkeley. [14] M. Tofte and J.-P. Talpin. Implementation of the typed call-by-value \n.-calculus using a stack of regions. In Proceedings of the 21st Annual ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages, pages 188 201, Portland, Oregon, Jan. 1994. [15] P. Wadler. Linear \ntypes can change the world! In Proceedings of the third ACM SIGPLAN International Conference on Functional \nProgramming, pages 63 74, Baltimore, Maryland, Sept. 1998. \n\t\t\t", "proc_id": "1086365", "abstract": "We present a new approach to the old problem of adding side effects to purely functional languages. Our idea is to extend the language with \"witnesses,\" which is based on an arguably more pragmatic motivation than past approaches. We give a semantic condition for correctness and prove it is sufficient. We also give a static checking algorithm that makes use of a network flow property equivalent to the semantic condition.", "authors": [{"name": "Tachio Terauchi", "author_profile_id": "81100563652", "affiliation": "University of California, Berkeley", "person_id": "P348279", "email_address": "", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University", "person_id": "PP39041079", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086379", "year": "2005", "article_id": "1086379", "conference": "ICFP", "title": "Witnessing side-effects", "url": "http://dl.acm.org/citation.cfm?id=1086379"}