{"article_publication_date": "09-12-2005", "fulltext": "\n Modular Veri.cation of Concurrent Assembly Code with Dynamic Thread Creation and Termination Xinyu Feng \nand Zhong Shao Department of Computer Science, Yale University New Haven, CT 06520-8285, U.S.A. {feng, \nshao}@cs.yale.edu Abstract Proof-carrying code (PCC) is a general framework that can, in principle, verify \nsafety properties of arbitrary machine-language programs. Existing PCC systems and typed assembly languages, \nhowever, can only handle sequential programs. This severely limits their applicability since many real-world \nsystems use some form of concurrency in their core software. Recently Yu and Shao proposed a logic-based \ntype system for verifying concurrent assembly pro\u00adgrams. Their thread model, however, is rather restrictive \nin that no threads can be created or terminated dynamically and no sharing of code is allowed between \nthreads. In this paper, we present a new formal framework for verifying general multi-threaded assembly \ncode with unbounded dynamic thread creation and termination as well as sharing of code between threads. \nWe adapt and general\u00adize the rely-guarantee methodology to the assembly level and show how to specify \nthe semantics of thread fork with argument pass\u00ading. In particular, we allow threads to have different \nassumptions and guarantees at different stages of their lifetime so they can co\u00adexist with the dynamically \nchanging thread environment. Our work provides a foundation for certifying realistic multi-threaded pro\u00adgrams \nand makes an important advance toward generating proof\u00adcarrying concurrent code. Categories and Subject \nDescriptors F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams; \nD.2.4 [Software Engineering]: Software/Program Veri.ca\u00adtion correctness proofs, formal methods; D.3.1 \n[Programming Languages]: Formal De.nitions and Theory semantics; D.4.5 [Operating Systems]: Reliability \nveri.cation General Terms Languages, Veri.cation Keywords Concurrency Veri.cation, Proof-Carrying Code, \nRely-Guarantee, Dynamic Thread Creation 1. Introduction Proof-carrying code (PCC) [28] is a general framework \nthat can, in principle, verify safety properties of arbitrary machine-language programs. Existing PCC \nsystems [29, 6, 2] and typed assembly lan\u00adguages (TAL) [27, 26], however, can only handle sequential \npro\u00adgrams. This severely limits their applicability since most real-world Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 05 September 26 28, 2005, Tallinn, Estonia. \nCopyright c &#38;#169; 2005 ACM 1-59593-064-7/05/0009. . . $5.00. systems use some form of concurrency \nin their core software. Cer\u00adtifying low-level concurrent programs is an important task because it helps \nincrease the reliability of software infrastructure and is cru\u00adcial for scaling the PCC and TAL technologies \nto realistic systems. As an important .rst step, Yu and Shao [42] at last year s ICFP proposed a certi.ed \nformal framework (known as CCAP) for specifying and reasoning about general properties of concur\u00adrent \nprograms at the assembly level. They applied the invariance proof technique for verifying general safety \nproperties and the rely-guarantee methodology [23] for decomposition. They intro\u00adduced a notion of local \nguarantee for supporting thread-modular veri.cation even inside the middle of an atomic instruction se\u00adquence. \nTheir thread model, however, is rather restrictive in that no threads can be created or terminated dynamically \nand no shar\u00ading of code is allowed between threads; both of these features are widely supported and used \nin mainstream programming languages such as C, Java, and Concurrent ML [34]. Certifying dynamic thread \ncreation and termination turns out to be a much harder problem than we had originally anticipated [42]. \nDynamic thread creation and termination imply a changing thread environment (i.e., the collection of \nall live threads in the system other than the thread under concern). Such dynamic environment cannot \nbe tracked during static veri.cation, yet we still must some\u00adhow reason about it. For example, we must \nensure that a newly cre\u00adated thread does not interfere with existing live threads, but at the same time \nwe do not want to enforce non-interference for threads that have no overlap in their lifetime. Using \none copy of code to create multiple threads also complicates program speci.cation. Existing work on the \nveri.cation of concurrent programs al\u00admost exclusively uses high-level calculi (e.g., CSP [21], CCS [25], \nTLA [24]). Also, existing work on the rely-guarantee methodology for shared-memory concurrency only supports \nproperly nested con\u00adcurrent code in the form of cobegin P11 ...1Pn coend (which is a language construct \nfor parallel composition where code blocks P1, ..., Pn execute in parallel and all terminate at the coend \npoint). They do not support dynamic thread creation and termination. Modularity is also needed to make \nveri.cation scale. Existing work on the rely-guarantee methodology supports thread modular\u00adity, i.e., \ndifferent threads can be veri.ed separately without looking into other threads code. However, they do \nnot support code reuse very well. In CCAP, if a procedure is called in more than one thread, it must \nbe veri.ed multiple times using different speci.cations, one for each calling thread. We want a procedure \nto be speci.ed and veri.ed once so it can be reused for different threads. In this paper, we propose \na new framework for supporting certi.ed multi-threaded assembly programming (CMAP). CMAP is based on \na realistic abstract machine which supports dynamic thread creation with argument passing (the fork operation) \nas well as termination (the exit operation). Thread join can also be implemented in our language using \nsynchronization. Our approach builds on previous work on type systems and program veri.cation but makes \nthe following important new contributions:   The fork/join thread model is more general than cobe\u00adgin/coend \nin that it supports unbounded dynamic thread cre\u00adation, which poses new challenges for veri.cation. To \nour knowledge, our work is the .rst to successfully apply the rely\u00adguarantee method to verify concurrent \nprograms with dynamic thread creation and termination. Our CMAP framework pro\u00advides a foundation for \ncertifying realistic multi-threaded pro\u00adgrams and makes an important step toward generating concur\u00adrent \ncerti.ed code.  The presence of dynamic threads makes it impossible to track the actual live threads \nduring veri.cation. This poses great challenge in enforcing the rely-guarantee condition. To solve this, \nwe collect all dynamic threads into a single environment (i.e., the dynamic thread queue) and reason \nabout the envi\u00adronment s assumption and guarantee requirements as a whole. Although the dynamic thread \nqueue cannot be tracked stati\u00adcally, we can update and approximate the environment s as\u00adsumption and \nguarantee at each program point. In fact, we can unify the concepts of the current running thread s assump\u00adtion/guarantee \nwith its environment s guarantee/assumption. As we will demonstrate in Sections 3 and 4, making this \nwork in a formal framework (i.e., CMAP) is not trivial and it constitutes our main technical contribution. \n To ensure that the dynamic thread environment is well-formed, we enforce the invariant that the active \nthreads in the system never interfere with each other. We maintain this invariant by following the approach \nused for type checking the dynamic data heap [27]. By combining the type-based proof techniques with \nthe rely-guarantee based reasoning, we get a simple, extensi\u00adble, and expressive framework for reasoning \nabout the .exible fork/join thread model.  We allow one copy of thread code to be activated multiple \ntimes at different places. Different incarnations may have different behavior, depending on the value \nof the thread argument. This allows us to support unbounded thread creation.  We show how to maintain \nthread-modular reasoning even in the presence of dynamic thread creation and termination. Unlike CCAP, \nwe allow each code segment to be speci.ed indepen\u00addently of threads. Our work provides great support \nfor code\u00adand veri.cation sharing between threads.  We have also solved some practical issues such as \nthread ar\u00adgument passing and the saving and restoring of thread-private data at context switches. These \nissues are important for realistic multi-threaded programming but as far as we know have never been discussed \nin existing work.  We have developed CMAP and proved its soundness using the Coq proof assistant [37]. \nThe implementation in Coq is avail\u00adable for download [10]. Our work makes an important advance toward \nbuilding a complete PCC system for multi-threaded pro\u00adgrams. Without formal systems such as CMAP, we \ncannot formally reason about concurrent assembly code. Still, more work must be done before we can construct \na fully practical system. For exam\u00adple, a highly desirable goal is a high-level language with concise \nhuman-readable annotations that can be automatically compiled into CMAP programs and proofs. We leave \nthis as future work. In the rest of this paper, we .rst give an overview of the rely\u00adguarantee-based \nreasoning and a detailed explanation of the key challenges in verifying multi-threaded assembly code \n(Section 2). We then give an informal description of our approach to address these problems in Section \n3, and present our work on CMAP with A G 1 1 S0 S1S 2 T1  T2 SS 0 1S 2 G A 2 2 time Figure1. \nRely-guarantee-basedreasoning A G 11 S S2 S'2 T 1 0  g  S T2 S'1 S S 1 3 G A 2 2 time Figure \n2. R-G in a non-preemptive setting formal semantics in Section 4. We use a few examples to illustrate \nCMAP-based program veri.cation in Section 5. Finally we discuss related work and conclude.  2. Background \nand Challenges 2.1 Rely-Guarantee-Based Reasoning The rely-guarantee (R-G) proof method [23] is one of \nthe best\u00adstudied approaches to the compositional veri.cation of shared\u00admemory concurrent programs. Under \nthe R-G paradigm, every thread is associated with a pair (A, G), with the meaning that if the environment \n(i.e., the collection of all of the rest threads) satis.es the assumption A, the thread will meet its \nguarantee G to the environment. In the shared-memory model, the assumption A of a thread describes what \natomic transitions may be performed by other threads, while the guarantee G of a thread must hold on \nevery atomic transition of the thread. They are typically modeled as predicates on a pair of states, \nwhich are often called actions. For instance, in Figure 1 we have two interleaving threads T1 and T2. \nT1 s assumption A1 adds constraints on the transition (S0,S1) made by the environment (T2 in this case), \nwhile G1 de\u00adscribes the transition (S1,S2), assuming the environment s transi\u00adtion satis.es A1. Similarly \nA2 describes (S1,S2) and G2 describes (S0,S1). We need two steps to reason about a concurrent program \ncon\u00adsisting of T1 and T2. First, we check that there is no interference between threads, i.e., that each \nthread s assumption can be satis\u00ad.ed by its environment. In our example, non-interference is sat\u00adis.ed \nas long as G1 . A2 (a shorthand for .S, S\".G1(S, S\") . A2(S, S\")), and G2 . A1. Second, we check that \nT1 and T2 do not lie, that is, they satisfy their guarantee as long as their assumption is satis.ed. \nAs we can see, the .rst step only uses the speci.cation of each thread, while the second step can be \ncarried out indepen\u00addently without looking at other threads code. This is how the R-G paradigm achieves \nthread-modularity.  2.2 R-G in Non-Preemptive Thread Model CMAP adopts a non-preemptive thread model, \nin which threads yield control voluntarily with a yield instruction, as shown in Fig\u00adure 2. The preemptive \nmodel can be regarded as a special case of Variables: Initially: nat[100] data; data[i]= ni,0 = i< 100 \nmain1 : main2 : data[0]:= f(0); nat i := 0; fork(chld,0); while(i < 100){ . data[i]:= f(i); . . fork(chld,i); \ndata[99]:= f(99); i := i + 1; fork(chld,99); }... ... void chld(int x){data[x]:= g(x,data[x]); } Figure \n3. Loop: high-level program the non-preemptive one, in which an explicit yield is used at every program \npoint. Also, on real machines, programs might run in both preemptive and non-preemptive settings: preemption \nis usually im\u00adplemented using interrupts; a program can disable the interrupt to get into non-preemptive \nsetting. An atomic transition in a non-preemptive setting then corre\u00adsponds to a sequence of instructions \nbetween two yields. For in- T1 T2 T1 T2T3 T1 T2T3 A A A B B C B C ... D C  D  D E (a) (b) (c) Figure \n4. Interleaving of threads to by the .rst argument. The second argument of fork is passed to the function \nas argument. The thread main1 does this in sequential code while main2 uses code with a loop. We assume \nthat the high level code runs in preemptive mode. In other words, there is an implicit yield at any program \npoint. It is easy to see that both versions are well-behaved as long as the function g has no side effects, \nand all other threads in the rest of the system do not update the array of data. However, the simple \napproach used in [42] and [13] even cannot provide a speci.cation for such trivial code. 1. Figure 4 \n(a) illustrates the execution of main1 (time goes down\u00adwards). When doing data initialization (at stage \nA-B, meaning 2) corresponds to an atomic transition of thread T1. A dif.culty in modeling concurrency \nin no other threads in the environment (say, T2) can change the ar\u00adsuch a setting is that the effect \nof an atomic transition cannot ray of data. However, the composition of the main thread s en\u00adbe completely \ncaptured until the end. For example, in Figure 2, the vironment changes after a child thread is created. \nThe assump\u00ad 1) should satisfy G stance, in Figure 2 the state pair (S2,Sfrom point A to point B), the \nmain thread needs to assume that transition (S1,S 2. But when we reach the inter-tion used at A-B is \nno longer appropriate for this new environ\u00ad mediate state S, we have no idea of what the whole transition \n(i.e., ment since the .rst child thread will write to data[0]. And the 1)) will be. At this point, neither \n(S1,S) nor (S,S1) need satisfy G2. Instead, it may rely on the remaining commands (the commands between \ncomm and yield, including comm) to com\u00adplete an adequate state transition. In CCAP [42], a local guaran\u00adtee \ngis introduced for every program point to capture further state changes that must be made by the following \ncommands before it is safe for the current thread to yield control. For instance, the local guarantee \ng attached to comm in Figure 2 describes the transition 1). (S1,S (S,S environment will keep changing \nwith the execution of the main thread. How can we specify the main1 thread to support such a dynamic \nthread environment? One possible approach is that the main thread relaxes its as\u00adsumption to make exceptions \nfor its child threads. However, it is hard to specify the parent-child relationship. Another approach \nis to use something like the program counter in the assumption and guarantee to indicate the phase of \ncomputation. This means the speci.cation of the main thread is sensitive to the imple\u00admentation. Also \nthe program structure of the main thread has  2.3 Challenges for Dynamic Thread Creation To prove safety \nproperties of multi-threaded programs, the key problem is to enforce the invariant that all executing \nthreads must not interfere with each other. As mentioned in Section 2.1, threads do not interfere (or \nthey satisfy the non-interference or interference-free property) only if each thread s assumption is \nim\u00adplied by the guarantee of all other threads1. For languages that do not support dynamic thread creation, \nthe code for each thread cor\u00adresponds to exactly one executing thread. Using the rely-guarantee method, \nwe can assign an assumption and guarantee to each thread code and enforce non-interference by checking \nall of these as\u00adsumptions and guarantees, as is done in [42] and [13]. However, the following example \nshows that this simple approach cannot sup\u00adport dynamic thread creation and and multiple incarnation \nof the thread code. In Figure 3, the high-level pseudo code (using C-like syntax) shows the code for \n(two versions of) a main thread and child threads. The main thread initializes 100 pieces of data using \nsome function f, and distributes them to 100 child threads that will work on their own data (by applying \na function g) in parallel. The fork function creates a child thread that will execute the function pointed \n1 We will formalize the Non-Interference property in Section 4.4. to be exposed to the speci.cation of \nthe child threads, which compromises modularity. The worst thing is that this approach simply won t work \nfor the version main2. 2. Since multiple child threads are created, we must make sure that there is no \ninterference between these children. It is easy for the above example since we can let the assumption \nand guarantee of the chld code be parameterized by its argument, and require Gi . Aj given i .j. However, \nthis approach = cannot be generalized for threads that have dummy arguments and their behavior does not \ndepend on their arguments at all. In this case Gi = Gj and Ai = Aj for any i and j. Then requiring Gi \n. Aj is equivalent to requiring Gi . Ai, which cannot be true in general, given the meaning of assumptions \nand guarantees described in section 2.1. Do we need to distinguish these two kinds of threads and treat \nthem differently? And how do we distinguish them? 3. Another issue introduced by dynamic thread creation, \nbut not shown in this example program, is that the lifetimes of some threads may not overlap. In the \ncase shown in Figure 4 (b), the lifetimes of T2 and T3 do not overlap and we should not statically enforce \nnon-interference between them. Again, how can we specify and check the interleaving of threads, which \ncan be as complex as shown in Figure 4 (c)? In the next section we ll show how these issues are resolved \nin our development of CMAP.  3. Our Approach In the rest of this paper, to distinguish the executing \nthread and the thread code, we call the dynamically running thread the dynamic thread and the thread \ncode the static thread . In Figure 3 the function chld is the static child thread, from which 100 dynamic \nchild threads are activated. As explained in Section 2.3, the approach that requiring non\u00adinterference \nof static threads is too rigid to support dynamic thread creation. Our approach, instead, enforces the \nthread non\u00adinterference in a lazy way. We maintain a dynamic thread queue which contains all of the active \nthreads in the system. When a new thread is created, it is added to the dynamic thread queue. A thread \nis removed from the queue when its execution terminates. We also require that, when specifying the program, \neach static thread be assigned an assumption/guarantee pair. However, we do not check for non-interference \nbetween static thread speci.cations. Instead, each dynamic thread is also assigned an assumption and \nguarantee at the time of creation, which is an instantiation of the correspond\u00ading static thread speci.cation \nwith the thread argument. We require that dynamic threads do not interfere with each other, which can \nbe checked by inspecting their speci.cations. Our approach is very .exible in that each dynamic thread \ndoes not have to stick to one speci.cation during its lifetime. When its environment changes, its speci.cation \ncan be changed accordingly. As long as the new speci.cation does not introduce interference with other \nexisting dynamic threads, and the subsequent behavior of this thread satis.es the new speci.cation, the \nwhole system is still interference-free. In this way, we can deal with the changing environment resulting \nfrom dynamic thread creation and termina\u00adtion. Problem 1 in Section 2.3 can be solved now. If the lifetimes \nof two threads do not overlap they will not show up in the system at the same time. Therefore we do not \nneed to check for interference at all. Also, since each dynamic thread has its own speci.cation, we no \nlonger care about the speci.cation of the corresponding static thread. Therefor problems 2 and 3 shown \nin Section 2.3 are no longer an issue in our approach. 3.1 Typing The Dynamic Thread Queue We de.ne the \ndynamic thread queue Q as a set of thread identi.ers ti, and the assignment T of assumption/guarantee \npairs to dynamic threads as a partial mapping from ti to (Ai,Gi) 2. The queue Q is well-typed with regard \nto T if: Q = dom(T), where dom(T) is the domain of T;  threads in Q do not interfere, i.e., .ti,tj \n.ti .tj .(Gi .  = Aj); and each dynamic thread ti is well-behaved with regard to (Ai,Gi), i.e., if Ai \nis satis.ed by the environment, ti sex\u00adecution does not get stuck and satis.es Gi. Therefore, the invariant \nwe need to maintain is that during the execution of the program, for the queue Q at each step there exists \na T such that Q is well-typed with regard to T. In fact, we do not require T to be part of the program \nspeci.cation. We only need to ensure that there exists such a T at each step, which may be changing. \nThe content of the thread queue keeps changing, so how can we track the set of threads in the queue by \na static inspection of the program? Here we follow the approach used for type-checking the 2 This is \na temporary formulation to illustrate our basic idea. We will use different de.nitions in our formal \ndevelopment of CMAP in Section 4. dynamic data heap [27], which is dynamically updated by the store instruction \nand extended by the alloc instruction. We can ensure our invariant holds as long as the following conditions \nare satis.ed: At the initial state (when the program starts to run) we can .nd a T to type-check the \ninitial Q. Usually the initial Q only contains the main thread, which will start to execute its .rst \ninstruction, so we can simply assign the assumption/guarantee in the speci.cation of the static main \nthread to the dynamic main thread.  For each instruction in the program, assume that before the ex\u00adecution \nof the instruction there is a T such that Q is well typed. Then as long as certain constraints are satis.ed \nto execute the instruction, there must exist a T. that can type check the result\u00ading Q\". For most instructions \nwhich do not change the content of the thread queue, this condition can be trivially satis.ed. We are \nonly interested in the fork and exit operation which will change the content of Q.  For the exit instruction, \nthe second condition can also be satis.ed by the following lemma which can be trivially proven. Lemma \n3.1 (Thread Deletion) If Q is well-typed with regard to T, then for all t . Q we know Q \\{t}is well-typed \nwith regard to T \\{t}. For the fork instruction, things are trickier. We need to ensure that the new \nchild thread does not interfere with threads in the parent thread s environment. We also require that \nthe parent thread does not interfere with the child thread. The following lemma ensures the .rst requirement. \n Lemma 3.2 (Queue Extension I) Suppose Q is well-typed with regard to T and the current executing thread \nis t.If T(t)=(A,G);  a new thread t. is created by t;  (A\",G\") is the instantiation of the corresponding \nstatic thread speci.cation by the thread argument;  A.A. and G. .G;  then Q. .{t\"}is well-typed with \nregard to T\"{t. . (A\",G\")}, where Q. = Q \\{t}and T. =T \\{t}. Here Q. is the environment of the current \nthread t. Since t does not interfere with its environment (because Q is well-typed), we know that its \nassumption A is an approximation of what the envi\u00adronment can guarantee (Ge), and similarly that Gis \nan approxima\u00adtion of the environment s assumption (Ae). By this interpretation, we can unify the concepts \nof the current running thread s assump\u00adtion/guarantee with its environment s guarantee/assumption. To \nen\u00adsure the new thread t. does not interfere with t s environment, we need G. .Ae and Ge .A\", which can \nbe derived from G. .G and A.A. . Still, we need to ensure that thread t does not interfere with t\". As \nmentioned above, A and G are approximations of Ge and Ae, respectively. Since the environment is extended \nwith the child thread, the guarantee G. e for the new environment is Ge .G. and the assumption for the \nnew environment Ae . is Ae .A\". We want to change Aand Gcorrespondingly to re.ect the environment change. \nFirst, the following lemma says that the speci.cation of a dynamic thread can be changed during its lifetime. \nLemma 3.3 (Queue Update) Suppose Q is well-typed with regard to T and that the current executing thread \nis t.If T(t)=(A,G); G\". .Gand A.A\"\"; \"\",G\"\"); the subsequent behavior of the current thread satis.es \n(A . (A\"\" ,G\"\" then Q is well-typed with regard to T{t )}. Now we can change the speci.cation of the \nparent thread tto let it re.ect the change of the environment. Lemma 3.4 (Queue Extension II) Suppose \nQ is well-typed with regard to T and the current executing thread is t.If T(t)=(A,G);  a new thread \nt. is created by t;  (A\",G\") is the instantiation of the corresponding static thread speci.cation by \nthe thread argument;  (A.A\") .(G. .G);  the remainder behavior of the thread t also satis.es (A . G\",G.A\"); \n then Q .{t\"}is well-typed with regard to T{t. . (A\",G\"),t . (A.G\",G.A\")}. If tlater creates another \nthread t\"\", because the speci.cation of t already re.ects the existence of t\", by Lemma 3.2 we know that \nt\". will not interfere with t. as long as its speci.cation satis.es the constraints. Therefore we do \nnot need to explicitly check that t. and t\". are activated from the same static thread or that multiple \nactivations of a static thread do not interfere with each other. These lemmas are used to prove the soundness \nof CMAP. They are somewhat similar to the heap update and heap extension lem\u00admas used in TAL s soundness \nproof [27]. People familiar with the traditional rely-guarantee method may feel this is nothing but the \nparallel composition rule used to support nested cobegin/coend. However, by combining the invariant-based \nproof technique used by type systems and the traditional rely-guarantee method, we can now verify multi-threaded \nassembly program with a more .exible program structure than the cobegin/coend structure. In particular, \nprograms which are not properly nested, as shown in Figure 4(c) and the main2 program in Figure 3, can \nbe supported in our sys\u00adtem. This is one of the most important contributions of this paper. 3.2 Parameterized \nAssumption/Guarantee The assumptions and guarantees are interfaces between threads, which should only \ntalk about shared resources. As we allow mul\u00adtiple activations of static threads, the behavior of a dynamic \nthread may depend on its arguments, which is the thread s private data. Therefore, to specify a static \nthread, the assumption and guarantee need to be parameterized over the thread argument. In our thread \nmodel, the .at memory space is shared by all threads, and as in most operating systems, the register \n.le is saved at the moment of context switch. Therefore the register .le is thread-private data. The \nthread argument is stored in a dedicated register. Rather than letting the assumption and guarantee be \nparameter\u00adized over the thread argument, we let them be parameterized over the whole register .le. This \nmakes our speci.cation language very expressive. For instance, we allow the dynamic thread to change \nits speci.cation during its lifetime to re.ect change in the envi\u00adronment. If the thread has private \ndata that tracks the composition of the environment, and its speci.cation is parameterized by such data, \nthen its speci.cation automatically changes with the change of the data, which in turn results from the \nchange of the thread en\u00advironment. This is the key technique we use to support unbounded dynamic thread \ncreation, as shown in the program main2 in Fig\u00adure 3.  (a) (b) Figure 5. Code sharing between different \nthreads 3.3 Support of Modular Veri.cation The rely-guarantee method supports thread modularity well, \ni.e., code of one thread can be veri.ed independently without inspecting other threads code. However, \nit does not have good support for code reuse. In CCAP, each thread has its own code heap and there is \nno sharing of code between threads. As Figure 5(a) shows, if an instruction sequence is used by multiple \nthreads, it has multiple copies in different threads code heaps, each copy veri.ed with regard to the \nspeci.cations of these threads. Based on the rely-guarantee method, the thread modularity is also supported \nin our system. In addition, using our lazy checking of thread non-interference, and by the queue update \nlemma, we can allow instruction sequences to be speci.ed independently of their calling thread, thus \nachieving better modularity. As shown in Figure 5(b), we assign an assumption/guarantee pair to the speci.cation \nof each instruction sequence, and require the instruction sequence be well-behaved with regard to its \nown as\u00adsumption/guarantee. Similar to threads, the instruction sequence is well-behaved if, when it is \nexecuted by a dynamic thread, its exe\u00adcution is safe and satis.es its guarantee, as long as other dynamic \nthreads in the environment satisfy the assumption. The instruction sequence only needs to be veri.ed \nonce with respect to its own speci.cation, and can be executed by different threads as long as certain \nconstraints are satis.ed. Intuitively, it is safe for a dynamic thread t with speci.cation (Ai,Gi) to \nexecute the instruction sequence labeled by f as long as executing it does not require a stronger assumption \nthan Ai, nor does it violate the guarantee Gi. Therefore, if the speci.cation of f is (A,G), t can call \nf as long as Ai . A and G . Gi. The intuition is backed up by our queue update lemma.   4. CMAP The \nlanguage CMAP is based on an untyped low-level abstract machine supporting multi-threaded programs with \ndynamic thread creation and argument passing. The type system of CMAP uses the calculus of inductive \nconstructions (CiC) [32] to essentially support reasoning in higher-order predicate logic. 4.1 The Abstract \nMachine Figure 6 shows the de.nition of our abstract machine. A CMAP program (corresponding to a complete \nmachine state) is made up of an updatable state S (which is made up of the shared memory M and the register \n.le R), a dynamic thread queue Q, two shared code heaps C (for basic code blocks) and T (for thread entries), \nand the current instruction sequence I of the currently executing thread. Here C and T can be merged, \nbut conceptually it is cleaner to have them separated because the speci.cation of T and is different \nfrom that of C (See Section 4.3: for T we do not need to specify a local guarantee). Memory is a partial \nmapping from ((M, R), Q, T, C, I) -.P if I = then P = fork h, r; I\"\" ((M, R), Q{t . (R\", I\")}, T, C, \nI\"\") where I\" = T(h), t .dom(Q), t = R(rt), and R\" = {r0 . ,..., r15 . , rt . t, ra . R(r)}yield; I\"\" \n((M, R\"), (Q{R(rt) . (R, I\"\")}) \\{t}, T, C, I\") where t .dom(Q) and (R\", I\")= Q(t) or t = R(rt) and (R\", \nI\")=(R, I\"\") exit ((M, R\"), Q \\{t}, T, C, I\") where t .dom(Q) and (R\" , I\")= Q(t) jd f ((M, R), Q, T, \nC, I\") where I\" = C(f) bgt rs, rt, f; I\"\" ((M, R), Q, T, C, I\"\") if R(rs) =R(rt), \" ((M, R), Q, T, C, \nI\") otherwise, where I= C(f) beq rs, rt, f; I\"\" ((M, R), Q, T, C, I\"\") if R(rs)= R(rt), \" ((M, R), Q, \nT, C, I\") otherwise, where I= C(f) \"\") c; I\"\" for remaining (Next(c, (M, R)), Q, T, C, Icases of c Figure \n8. Operational semantics of CMAP (Program) P ::= (S, Q, T, C, I) (State) S ::= (M, R) * (Memory) M ::= \n{l . w} * (RegFile) R ::= {r . w} (Register) r ::= r0 |r1 |... |r15 |rt |ra * (CdHeap) C ::= {f . I} \n(Labels) f, l ::= n(nat nums) (WordVal) w ::= n(nat nums) * (TEntries) T ::= {h . I} * (TQueue) Q ::= \n{t . (R, I)} (THandles) h ::= n(nat nums) (ThrdID) t ::= n(nat nums) (InstrSeq) I ::= c; I |jd f|exit \n(Commd) c ::= yield |fork h, r|add rd, rs, rt |sub rd, rs, rt |movi rd, w|bgt rs, rt, f|beq rs, rt, f \n|ld rd, rs(w) |st rd(w), rs Figure 6. The abstract machine if c = then Next(c, (M, R)) = add rd, rs, \nrt (M, R{rd . R(rs)+ R(rt)}) sub rd, rs, rt (M, R{rd . R(rs) -R(rt)}) movi rd, w (M, R{rd . w}) ld rd, \nrs(w)(M, R{rd . M(R(rs)+ w)}) where (R(rs)+ w) .dom(M) st rd(w), rs (M{(R(rd)+ w) . R(rs)}, R) where \n(R(rd)+ w) .dom(M) Figure 7. Auxiliary state update function memory locations to word-sized values. The \nregister .le R maps registers to word-sized values. In our machine, there are 16 general purpose registers \n(r0 -r15) and two special registers (rt and ra) that hold the current thread id and the thread argument. \nCode heaps map code labels to instruction sequences, which are lists of instructions terminated by a \njd or exit instruction. Code labels pointing to thread entries are also called thread handles. T maps \nthread handles to thread entries (instruction sequences from which a thread starts to execute). Thread \nentries are also called static threads. The current instruction sequence I plays the role of the program \ncounter of the current executing thread. For simplicity, we just model the queue of ready threads, which \nis the dynamic thread queue Q that maps the dynamic thread id to an execution context of a thread. The \ndynamic thread id t is a natural number generated randomly at run time. The thread execution context \nincludes the snapshot of the register .le and the program point where the thread will resume its execution. \nNote that Q does not contain the current executing thread, which is different from the dynamic thread \nqueue used in Section 3. The instruction set of CMAP just contains the most basic and common assembly \ninstructions. It also includes primitives fork, exit and yield to support multi-threaded programming \nwhich can be viewed as system calls to a thread library. We do not have a join instruction because thread \njoin can be implemented using synchro\u00adnization. Readers who are eager to see a CMAP program can take \na quick look at Figure 10 in Section 5.1 (ignore the speci.cations and annotations in program for the \ntime being), which is the CMAP implementation of programs main2 and chld shown in Figure 3. The execution \nof CMAP programs is modeled as small-step transitions from one program to another, i.e., P P\". Figure \n8 -. de.nes the program transition function. The primitive fork creates a new thread using the static \nthread h, and passes the value R(r) to it as the argument. The new thread will be assigned a fresh thread \nid and placed in the dynamic thread queue waiting for execution. The current thread continues with the \nsubsequent instructions. At a yield instruction, the current thread will give up the control of the machine. \nIts execution context is stored in Q. The scheduler will pick one thread non-deterministically from the \nthread queue (which might be the yielding thread itself), restore its execution context, and execute \nit. The exit instruction terminates the execution of the current thread and non-deterministically selects \na thread from the thread queue. Here we have the implicit assumption that there is always an idle thread \nin the thread queue that never changes the state or terminates, ensuring that the thread queue will never \nbe empty. Semantics for the rest of the instructions are standard. The next state function de.ned in \nFigure 7 describes the effects of some instructions on the state.  4.2 The Meta-Logic To encode the \nspeci.cation and proofs, we use the calculus of inductive constructions (CiC) [37, 32], which is an extension \nof the calculus of constructions (CC) [7] with inductive de.nitions. CC corresponds to Church s higher-order \npredicate logic via the Curry-Howard isomorphism. CiC has been shown strongly normalizing [38], hence \nthe cor\u00adresponding logic is consistent. It is supported by the Coq proof as\u00adsistant [37], which we have \nused to implement CMAP. In the remainder of this paper, we will mainly use the more fa\u00admiliar mathematical \nand logical notations, instead of strict CiC or Coq representation. We use Prop to denote the type of \nall propo\u00adsitions. No knowledge of CiC and Coq is required to understand them. 4.3 Program Speci.cations \nThe veri.cation constructs of CMAP are de.ned in Figure 9. The program speci.cation F is a global invariant \n(Inv), a static thread (ProgSpec) F ::=(Inv, ., .) (ThrdSpec) . ::= {h . .}* (ThrdType) . ::= (p, A, \nG) (CdHpSpec) . ::= {f . (p, g, A, G)}* (ActTSpec) T ::= {t . (p, A, G)}* (Invariant) Inv . Mem . Prop \n(Assertion) p . State . Prop (Assumption) A . RegFile .Mem .Mem . Prop (Guarantee) G, g . RegFile .Mem \n.Mem . Prop Figure 9. Veri.cation constructs of CMAP speci.cation ., and a code heap speci.cation .. \nThe invariant Inv is a programmer speci.ed predicate, which implies a safety property of concern. It \nmust hold throughout the execution of the program. The static thread speci.cation . contains a speci.cation \n. for each static thread in T. Each . contains a precondition p to invoke this thread, and an assumption \nA and guarantee G for this thread with regard to the environment at its creation time. A code heap speci.cation \n. assigns a quadruple (p, g, A, G) to each instruction sequence. The assertion p is the precondition \nto execute the code sequence. The local guarantee g, as introduced in Section 2.2, describes a valid \nstate transition it is safe for the current thread to yield control only after making a state transition \ndescribed by g. As explained in Section 3.3,, we assign a pair of A and G to each instruction sequence \nas part of its speci.cation. The instruction sequence can be veri.ed with regard to its own speci.cation \nwithout knowing which thread executes it. Here the A and G re.ect knowledge of the dynamic thread environment \nat the time the instruction sequence is executed. The global invariant Inv is a CiC term of type Mem \n. Prop, i.e., a predicate over memory. Inv does not specify the register .le, which contains thread-private \ndata and keeps changing. In contrast to Inv, assertions (p) are predicates over the whole state. Assumptions \nand guarantees (e.g., A, G and g) are CiC terms with type RegFile . Mem . Mem . Prop, which means predicates \nover a register .le and two instances of memory. Assumptions and guarantees specify the behavior of threads \nby describing the change of shared memory. As mentioned in Section 3.2, they are parameterized over the \nregister .le, which contains the private data of threads. We also de.ne the speci.cation T of active \nthreads in the thread queue Q. Within T, each triple (p, A, G) describes a dynamic thread at its yield \npoint (or at the beginning if it has just forked). The assertion pgives the constraint of the state when \nthe thread gets control back to execute its remaining instructions. The assumption and guarantee used \nby the thread at the yield point are given by A and G. As we said in Section 3.1, the A and G of each \ndynamic thread may change during the lifetime of the thread. Notice that T is not part of the program \nspeci.cation. It is used only in the soundness proof. 4.4 Inference Rules We use the following judgement \nforms to de.ne the inference rules: F; T; (p, g, A, G) f P (well-formed program) F; T; (g, S) f Q (well-formed \ndynamic threads) F f T (well-formed static threads) F f C (well-formed code heap) F; (p, g, A, G) f I \n(well-formed instr. sequence) Before introducing the inference rules, we .rst de.ne some shorthands in \nTable 1 to simplify our presentation. Well-formed programs. The PROG rule shows the invariants that need \nto be maintained during program transitions. Representation De.nition S(r) R(r) where S=(M, R) Inv . \np .(M, R).Inv M. p (M, R) \"\" g SM g RMM , where S=(M, R) \"\" \" ASM , GSM similar to g SM p. c .S.p (Next(c, \nS)) \"\" g. c .S..M. g (Next(c, S)) M A. c, G. c similar to g. c \"\" p . p .S.p S. p S p . g .R, M.p (M, \nR) . g RMM \" \" \"\"\" p . g . g .S, M.p S. g SM . g SM \" \" \"\"\" A. A .R, M, M.ARMM . A RMM \" \" \"\"\" G. G .R, \nM, M.GRMM . G RMM Table 1. Assertion de.nitions and syntactic sugar (Inv, ., .)=F (M, R)= S t = R(rt) \nF f T F f C (Inv . p) S F; (p, g, A, G) f I F; T; (g, S) f Q NI(T{t . (p, A, G)}, Q{t . (R, I)}) (PROG) \nF; T; (p, g, A, G) f (S, Q, T, C, I) The well-formedness of a program is judged with respect to the program \nspeci.cation F, the dynamic thread speci.cation T, and the speci.cation of the current executing thread \n(p, g, A, G). Compared with the triples in T, we need the local guarantee ghere to specify the transition \nwhich the current thread must make before it yields control. In the .rst line, we give the composition \nof the program speci\u00ad.cation, the current state and the current thread id. Here we use a pattern match \nrepresentation, which will be used in the rest of this paper. We require the code, including the thread \nentry points T and the code heap C, always be well-formed with respect to the program speci.cation. Since \nF, T and C do not change during program transitions, the check for the .rst two premises in line 2 can \nbe done once and for all. The next premise shows the constraints on the current state S: it must satisfy \nboth the global invariant Inv and the assertion p of the current thread. The last premise in line 2 essentially \nrequires it be safe for the current thread to execute the remainder instruction sequence I. Premises \nin line 3 require the well-formedness of dynamic threads in Q, which is checked by the rule DTHRDS, and \nthe non\u00adinterference between all the live threads. Non-interference. The non-interference macro NI(T, \nQ) re\u00adquires that each dynamic thread be compatible with all the other. It is formally de.ned as: \" .ti, \ntj . dom(T)..M, M. \"\" ti = tj . Gi Ri MM . Aj Rj MM, where ( , Ai, Gi)= T(ti), ( , Aj, Gj )= T(tj ), \nRi = Q(ti) and Rj = Q(tj ). As explained in Section 3, here we enforce the non-interference by a lazy \ncheck of speci.cations T of dynamic threads in Q, instead of checking the speci.cation . of all static \nthreads in T. Well-formed dynamic threads. The rule DTHRDS ensures each dynamic thread in Q is in good \nshape with respect to the speci.\u00adcation T, the current program state S, and the transition g that the \ncurrent thread need do before other threads can take control. (Rk, Ik)= Q(tk), (pk, Ak, Gk)= T(tk), .tk \n.dom(Q) \"\" \" (Inv, pk 0 Ak, Rk) .M.g RMM .pk (M, Rk) (Inv, ., .); (pk, Gk, Ak, Gk) fIk (Inv, ., .); T; \n(g, (M, R)) fQ (DTHRDS) The .rst line gives the speci.cation of each thread when it reaches a yield point, \nand its execution context. The .rst premise in line 2 requires that if it is safe for a thread to take \ncontrol at certain state, it should be also safe to do so after any state transition satisfying its assumption. \nNote that the state transition will not affect the thread-private data in Rk. The second premise describes \nthe relationship between the lo\u00adcal guarantee of the current thread and the preconditions of other threads. \nFor any transitions starting from the current state (M, R), as long as it satis.es g, it should be safe \nfor other threads to take control at the result state. Line 3 requires that for each thread its remainder \ninstruction sequence must be well formed. Note we use Gk as the local guar\u00adantee because after yield, \na thread starts a new atomic transition described by its global guarantee. Well-formed static threads. \nThis rule checks the static thread entry is well formed with respect to its speci.cation in .. (pk, Ak, \nGk)= .(hk), Ik = T(hk), .hk .dom(T) .R.(Inv, pk 0 Ak, R)(Inv, ., .); (pk, Gk, Ak, Gk) fIk (Inv, ., .) \nfT (THRDS) The .rst line gives the speci.cation for each static thread in T. We implicitly require dom(.) \n= dom(T). The .rst premise in line 2 says that if it is safe to invoke the new thread at certain state, \nit should also be safe to delay the invocation after any transition satisfying the assumption of this \nthread. The initial instruction sequence of each thread must be well\u00adformed with respect to the thread \nspeci.cation. This is required by the last premise. Well-formed code heap. A code heap is well formed \nif every in\u00adstruction sequence is well-formed with respect to its corresponding speci.cation in .. dom(.) \n= dom(C)(Inv, ., .); .(f) fC(f), .f .dom(.) (Inv, ., .) fC (CDHP) Thread creation. The FORK rule describes \nconstraints on new thread creation, which enforces the non-interference between the new thread and existing \nthreads. \"\" (p , A, G \" )= .(h) \"\"\"\" \"\" ) .A \"\" \"\" ) A.AG .G (A.GG .(G.A ' RR \" .R, R. (R(rt)= R \" (rt) \n.R(r)= R \" (ra)) .((Inv .p)=.g =.p \" ) \"\" \"\" ) fI (Inv, ., .); (p, g, A , G (Inv, ., .); (p, g, A, G) \nffork h, r; I (FORK) As explained in Section 3.1, the parent thread can change its speci.cation to re.ect \nthe change of the environment. To maintain the non-interference invariant, constraints between speci.cations \nof the parent and child threads have to be satis.ed, as described in Lemma 3.4. Here we enforce these \nconstraints by premises in line 2, where (A . G \"\" ) . A \" is the shorthand for: \"\"\" \" .(M, R)..M, M \n..R.(Inv .p)(M, R) .R(rt)= R \" (rt) \"\"\" \"\"\"\"\" .R(r)= R \" (ra) .(A.G \"\" ) RM M .ARMM , and G \" . (G . \nA \"\" ) for: \"\"\" \" .(M, R)..M, M ..R.(Inv .p)(M, R) .R(rt)= R \" (rt) \"\"\"\"\" \"\" \"\"\" .R(r)= R \" (ra) .GRMM \n.(G.A ) RM M . Above non-interference checks use the extra knowledge that: the new thread id is different \nwith its parent s, i.e., R(rt) . = R \" (rt); the argument of the new thread comes from the parent s register \nr, i.e., R(r)= R \" (ra); the parent s register .le satis.es the precondition, i.e., (Inv . p)(M, R). \nIn most cases, the programmer can just pick (A . . G \") and (G . A.\") as A \"\" G \" A \" and G \"\" respectively, \nwhere .and .are instantiations of G \" and A \" using the value of the child s argument. The premise in \nline 3 says that after the current thread completes the transition described by g, it should be safe \nfor the new thread to take control with its new register .le (R \" ), whose relationship between the parents \nregister .le R is satis.ed. The last premise checks the well-formedness of the remainder instruction \nsequence. Since the fork instruction does not change states, we need not change the precondition pand \ng. Yielding and termination. .R. (Inv, p 0 A, R)(Inv .p) .g (Inv, ., .); (p, G, A, G) fI (Inv, ., .); \n(p, g, A, G) fyield; I (YIELD) The YIELD rule requires that it is safe for the yielding thread to take \nback control after any state transition satisfying the assump\u00adtion A. Also the current thread cannot \nyield until it completes the required state transition, i.e., an identity transition satis.es the lo\u00adcal \nguarantee g. Lastly, one must verify the remainder instruction sequence with the local guarantee reset \nto G. (Inv .p) .g (EXIT) (Inv, ., .); (p, g, A, G) fexit The rule EXIT is simple: it is safe for the \ncurrent thread to terminate its execution only after it .nishes the required transition described by \ng, which is an identity transition. Type-checking other instructions. c .{add rd, rs, rt, sub rd, rs, \nrt, movi rd, w}, rd .{rt, ra} \" (Inv .p) .(Inv .p ) .c (Inv .p) .(g \" .c) .g \"\" \" A.A \" .c G \" .c .G \n(Inv, ., .); (p , g , A, G \" ) fI (Inv, ., .); (p, g, A, G) fc; I (SIMP) The rule SIMP covers the veri.cation \nof instruction sequences starting with a simple command such as add, sub or movi.We require that the \nprogram not update registers rt and ra. In these \"\" , A \" , G \" cases, one must .nd an intermediate precondition \n(p, g ) under which the remainder instruction sequence I is well-formed. The global invariant Inv and \nthe intermediate assertion p \" must hold on the updated machine state, and the intermediate guarantee \ng \" applied to the updated machine state must be no weaker than the current guarantee gapplied to the \ncurrent state. Since A and G are parameterized over R, which will be changed by the instruction c, one \nmay change A and G to ensure the assumption does not become stronger and the guarantee does not become \nweaker. \"\" \" (p , g , A, G \" )= .(f) \" \"\"\" (Inv .p) .p (Inv .p) .g .g A.AG .G (JD) (Inv, ., .); (p, g, \nA, G) fjd f The JD rule checks the speci.cation of the target instruction sequence. As mentioned before, \neach instruction sequence can have its own speci.cation, independent of the thread that will jump to \nit. It is safe for a thread to execute an instruction sequence as long as executing the instruction sequence \ndoes not require a stronger assumption than the thread s assumption A, nor does it break the guarantee \nG of the thread. Inference rules for memory operations are quite similar to the SIMP rule. Here we also \nneed ensure the memory address is in the domain of the data heap. c = ld rd, rs(w) rd .{rt, ra} .M..R.(Inv \n.p)(M, R) .((R(rs)+ w) .dom(M)) (Inv .p) .(Inv .p \" ) .c (Inv .p) .(g \" .c) .g \"\"\" \" .S, M, M.A(S.R) \nMM .A (Next(c, S).R) MM \"\" \"\" .S, M, M.G (Next(c, S).R) MM .G(S.R) MM \"\" \" (Inv, ., .); (p , g , A, G \n\" ) fI (LD) (Inv, ., .); (p, g, A, G) fc; I c = st rd(w), rs .M..R.(Inv .p)(M, R) .((R(rd)+ w) .dom(M)) \n(Inv .p) .(Inv .p \" ) .c (Inv .p) .(g \" .c) .g \"\" (Inv, ., .); (p , g , A, G) fI (ST) (Inv, ., .); (p, \ng, A, G) fc; I Rules for conditional branching instructions are similar to the JD rule, which are straightforward \nto understand. \"\"\" \"\" (p , g , A, G \" )= .(f) A.AG .G \" .S.(Inv .p) S.(S(rs) > S(rt)) .(p S) \"\" .S..M.(Inv \n.p) S.(S(rs) > S(rt)) .(g SM \" ) .(g SM \" ) \"\" .S.(Inv .p) S.(S(rs) =S(rt)) .(p S) \" \"\" .S..M.(Inv .p) \nS.(S(rs) =S(rt)) .(g SM \" ) .(g SM \" ) \"\" \"\" (Inv, ., .); (p , g , A, G) fI (Inv, ., .); (p, g, A, G) \nfbgt rs, rt, f; I (BGT) \"\"\" \"\" (p , g , A, G \" )= .(f) A.AG .G \" .S.(Inv .p) S.(S(rs)= S(rt)) .(p S) \n\"\" .S..M.(Inv .p) S.(S(rs)= S(rt)) .(g SM \" ) .(g SM \" ) \"\" .S.(Inv .p) S.(S(rs)= S(rt)) .(p S) \" \"\" \n.S..M.(Inv .p) S.(S(rs)= S(rt)) .(g SM \" ) .(g SM \" ) \"\" \"\" (Inv, ., .); (p , g , A, G) fI (Inv, ., .); \n(p, g, A, G) fbeq rs, rt, f; I (BEQ) 4.5 Soundness of CMAP The soundness of CMAP inference rules with \nrespect to the opera\u00adtional semantics of the machine is established following the syntac\u00adtic approach \nof proving type soundness [39]. From the progress and preservation lemmas, we can guarantee that given \na well\u00adformed program under compatible assumptions and guarantees, the current instruction sequence will \nbe able to execute without getting stuck . Furthermore, any safety property derivable from the global \nn invariant will hold throughout the execution. We de.ne P -. P \" as the relation of n-step (n = 0) program \ntransitions. The soundness of CMAP is formally stated as Theorem 4.3. Lemma 4.1 (Progress) F=(Inv, ., \n.). If there exist T, p, g, A and G such that F; T; (p, g, A, G) f ((M, R), Q, T, C, I), then (Inv M), \nand there exists a program P.such that ((M, R), Q, T, C, I) -. P.. Lemma 4.2 (Preservation) If F; T; \n(p, g, A, G) f P and P -. P., where P =(S, Q, T, C, I) and P.=(.S, Q., T, C, .I), then there exist T., \np., g., A.and G.such that F; .p, .A., G.) f P.. T; (.g, Theorem 4.3 (Soundness) F=(Inv, ., .). If there \nexist T, p, g, A and G such that F; T; (p, g, A, G) f P0, then for any n = 0, there exist M, R, n Q, \nT, C and I such that P0 -. ((M, R), Q, T, C, I) and (Inv M). The proofs for these two lemmas and the \nsoundness theorem are given in Appendix A. We have also implemented the complete CMAP system [10] in \nthe Coq proof assistant so we are con.dent that CMAP is indeed sound and can be used to certify general \nmulti-threaded programs.  5. Examples 5.1 Unbounded Dynamic Thread Creation In Figure 3 we showed a \nsmall program main2 which spawns child threads within a while loop. This kind of unbounded dynamic thread \ncreation cannot be supported using the cobegin/coend structure. We show how such a program is speci.ed \nand veri.ed using our logic. To simplify the speci.cation, we trivialize the function f and g and let \nf(i)= i and g(x, )= x +1. We assume the high-level program works in a preemptive mode. Figure 10 shows \nthe CMAP implementation, where yield instruc\u00adtions are inserted to simulate the preemption. This also \nillustrates that our logic is general enough to simulate preemptive thread model. To verify the safety \nproperty of the program, the programmer need .nd a global invariant and speci.cations for each static \nthread and the code heap. In Figure 10 we show de.nitions of assertions that are used to specify the \nprogram. Proof sketches are also in\u00adserted in the program. For ease of reading, we use named variables \nas short-hands for their values in memory. The primed variables represent the value of the variable after \nstate transition. We also introduce the shorthand [r] for R(r). The following formulae show the speci.cations \nof static threads, and the initial memory and instruction sequence. Inv =True . ={main . (True, A0, G0), \nchld . (p, A, G)} \" . ={loop . (p , G1, A1, G1), cont . (p3, G3, A3, G3)} Initial M={data . ,..., data \n+99 . } Initial I =movi r0, 0; movi r1, 1; movi r2, 100; jd loop For each child thread, it is natural \nto assume that no other threads will touch its share of the data entry and guarantee that other threads \ndata entry will not be changed, as speci.ed by A and G. For the main thread, it assumes at the beginning \nthat no other threads in the environment will change any of the data entries (A0). Specifying the loop \nbody is not easy. We need .nd a loop invariant (p \" , G1, A1, G1) to attach to the code label loop.At \n.rst glance, A1 and G1 can be de.ned the same as A3 and G3, respectively. However, this does not work \nbecause our FORK rule requires G . G1, which cannot be satis.ed. Instead, our A1 and G1 are polymorphic \nover the loop index r0, which re.ects the composition of the changing thread environments. At the point \nthat a new child thread is forked but the value of r0 has not been changed to re.ect the environment \nchange, we explicitly change the assumption and guarantee to A2 and G2. When the value of r0 is increased, \nwe cast A2 and G2 back to A1 and G1.  5.2 The Readers-Writers Problem Our logic is general enough to \nspecify and verify general proper\u00adties of concurrent programs. In this section, we give a simple solu\u00adtion \nof the readers-writers problem and show that there is no race conditions. This example also shows how \nP/V operations and lock primitives can be implemented and speci.ed in CMAP. Figure 11 shows the C-like \npseudo code for readers and writers. Note that this simple code just ensures that there is no race con\u00ad \np = 0 = [ra] <100 A = data[ra]= data \" [ra] G =.i.(0 = i<100 . i=[ra]) . (data[i]= data \" [i]) A0 =.i.0 \n= i<100 . (data[i]= data \" [i]) G0 = True A1 =.i.(0 = i<100 . i= [r0]) . (data[i]= data \" [i]) G1 =.i.(0 \n= i<100 . i<[r0]) . (data[i]= data \" [i]) A2 =.i.(0 = i<100 . i>[r0]) . (data[i]= data \" [i]) G2 =.i.(0 \n= i<100 . i= [r0]) . (data[i]= data \" [i]) A3 = True G3 =.i.0 = i<100 . (data[i]= data \" [i]) \" p = 0 \n= [r0] <100 . [r1]=1 . [r2] = 100 p3 = [r0]= 100 main : -{(True,A0 ,G0 )} movi r0 ,0 movi r1 ,1 movi \nr2 ,100 chld : -{(p,A,G)} jd loop movi r0 ,data \" loop : -{(p ,G1 ,A1 ,G1 )} yield beq r0 ,r2 ,cont \nadd r0 ,r0 ,ra yield yield st r0 (data),r0 ld r1 ,r0 (0) yield yield fork chld,r0 movi r2 ,1 \" -{(p \n,G1 ,A2 ,G2 )} yield yield add r1 ,r1 ,r2 add r0 ,r0 ,r1 yield \" -{(p ,G1 ,A1 ,G1 )} st r0 (0),r1 yield \nexit jd loop cont : -{(p3 ,G3 ,A3 ,G3 )} ... Figure 10. Loop: the CMAP program Variables : reader(int \nx){ int[100] rf,wf; while(true){ int cnt,writ,l,v; lock acq(l); Initially : if (cnt = 0){rf[i]= wf[i]= \n0,0 = i <100; P(writ); cnt = 0 . writ = 1 . l = 0; } cnt := cnt + 1; rf[x]:= 1; writer(int x){ lock rel(l); \nwhile(true){ read v ... P(writ); lock acq(l); wf[x]:= 1; rf[x]:= 0; write v ... cnt := cnt - 1; if(cnt \n= 0){ wf[x]:= 0; V(writ); V(writ); } } lock rel(l); } }} Figure 11. Readers &#38; writers : the high-level \nprogram ditions. It does not ensure fairness. Veri.cation of liveness prop\u00aderties is part of our future \nwork. We assume that 100 readers and writers will be created by a main thread. The main thread and its \nspeci.cation will be very similar to the main program shown in the previous section, so we omit it here \nand just focus on the readers and writers code. The array of rf and wf are not necessary for the implementation. \nThey are introduced as auxiliary variables just for speci.cation and veri.cation purpose. Figure 13 shows \nthe CMAP implementation of the high-level pseudo code. Yielding is inserted at all the intervals of the \natomic operations of the high-level program. The lock primitives and P/V operations, as shown in Figure \n12, are implemented as program macros parameterized by the return label. They will be instantiated  \n acq(f): -{}p writ(f): -{} yield yield movi r0 ,0 movi r0 ,0 movi r1 ,l movi r1 ,writ ld r2 ,r1 (0) \nld r2 ,r1 (0) bgt r2 ,r0 ,acq beq r2 ,r0 ,p writ st r1 (0),rt st r1 (0),r0 jd f jd f rel(f): -{}v writ(f): \n-{}yield movi r1 ,1 movi r0 ,0 movi r2 ,writ st r0 (l),r0 st r2 (0),r1 jd f jd f  Figure 12. Lock &#38; \nsemaphore primitives JD rel(reader) reader : -{(p1 ,g,Ar ,Gr )}JD acq(cont 1) cont 3 : -{(p7 ,g,Ar ,Gr \n)}yield movi r0 ,0 cont 1 : -{(p2 ,g,Ar ,Gr )}yield movi r0 ,0 movi r1 ,cnt ld r2 ,r1 (0) yield beq r2 \n,r0 ,getw jd inc cnt st ra(rf),r0 yield movi r1 ,cnt ld r2 ,r1 (0) yield movi r3 ,1 sub r2 ,r2 ,r3 st \nr1 (0),r2 yield getw : -{(p3 ,g,Ar ,Gr )}JD p writ(inc cnt) beq r2 ,r0 ,relw -{(p8,g,Ar,Gr)} inc cnt \n: -{(p4 ,g,Ar ,Gr )} relw : -{(p9 ,g,Ar ,Gr )} yield yieldmovi r1 ,cnt JD v writ(cont 4) ld r2 ,r1 (0) \ncont 4 : -{(p8 ,g,Ar ,Gr )} yield JD rel(reader) movi r3 ,1 add r2 ,r2 ,r3 st r1 (0),r2 writer : -{(p10 \n,g,Aw ,Gw )} yield JD p writ(cont 5) movi r1 ,1 st ra(rf),r1 cont 5 : -{(p11 ,g1 ,Aw ,Gw )} -{(p5,g,Ar,Gr)} \nmovi r1 ,1 JD rel(cont 2) st ra(wf),r1 yield cont 2 : -{(p6 ,g,Ar ,Gr )} ... yield yield ... movi r0 \n,0yield st ra(wf),r0 -{(p6,g,Ar,Gr)} -{(p11 ,g2 ,Aw ,Gw )} JD acq(cont 3) JD v writ(writer) Figure 13. \nReaders &#38; writers : the CMAP program and inlined in the proper position of the code. We introduce \na pseudo instruction JD to represent the inlining of the macro. We de.ne the global invariant and reader/writer \ns assumptions and guarantees in Figure 14. The code heap speci.cations are em\u00adbedded in the code as annotations, \nas shown in Figure 13. Speci\u00ad.cations of lock primitives and P/V operations are given at places they \nare inlined. De.nitions of assertions and local guarantees used in code heap speci.cations are shown \nin Figure 14. The assertion inv1 says that out of the critical section protected by the lock, the value \nof the counter cnt is always consistent and re.ects the number of the readers that can read the value \nof v; inv2 says at one time there is at most one writer that can change the value of v; while inv3 and \ninv4 states the relationship between the counter cnt, the semaphore variable writ and the actual number \n inv1 =l =0.rf[i]=cnt inv2 =wf[i]=1 ii inv3 =i wf[i]=1.(cnt =0.writ =0) inv4 =writ =1.cnt =0 Inv =inv1 \n.inv2 .inv3 .inv4 idr =.i.rf[i]=rf \" [i] idr1(i)=.j.i =j .rf[j]=rf \" [j] idr2(i)=rf[i]=rf \" [i] idw \n=.i.wf[i]=wf \" [i] idw1(i)=.j.i =j .wf[j]=wf \" [j] idw2(i)=wf[i]=wf \" [i] \"\" Ar =idr2([ra]).(rf[ra]=1.v \n=v ).(l =[rt].cnt =cnt ) \" Gr =idw .idr1([ra]).v =v \" .(l .{[rt], 0}.cnt =cnt ) \" Aw =idw2([ra]).(wf[ra]=1.v \n=v ) \" Gw =idr .idw1([ra]).((writ =0.wf[ra]=0).v =v ) g =.R, M, M \" .M =M \" g1 =.R, M, M \" .M{wf[ra]. \n1}=M \" g2 =.R, M, M \" .M{writ . 1}=M \" p1 =rf[ra]=0.l =[rt] p2 =rf[ra]=0.l =[rt] p3 =p2 .cnt =0 p4 =p2 \n.writ =0 p5 =rf[ra]=1.l =[rt] p6 =rf[ra]=1.l =[rt].cnt > 0 p7 =rf[ra]=1.l =[rt].cnt > 0 p8 =l =[rt].rf[ra]=0 \np9 =l =[rt].writ =0.cnt =0.rf[ra]=0 p10 =wf[ra]=0 p11 =wf[ra]=0.writ =0.cnt =0 Figure 14. Program speci.cations \nof writers that can write the data, which must be maintained to ensure the mutual exclusive access between \nreaders and writers. The program invariant Inv, which is the conjunction of the four, must be satis.ed \nat any step of the execution. The assumptions and guarantees of readers and writers are pa\u00adrameterized \nby their thread id (in rt) and thread argument (in ra). The reader assumes (Ar) that if it has the right \nto read the data v (i.e., rf[ra]=1), nobody can change the data. Also, if it owns the lock, nobody else \ncan change the value of the counter. Finally, it assumes its auxiliary variable rf[ra]will never be changed \nby oth\u00aders. Readers guarantee (Gr) that they will never change the shared data v and auxiliary variables \nof other threads, and it will not re\u00advise the counter unless it owns the lock. Writers assumption (Aw) \nand guarantee (Gw) are de.ned in a similar way to ensure the non\u00adinterference.  5.3 More Examples Yu \nand Shao [42] have shown by examples that CCAP is expres\u00adsive enough to verify general properties of \nconcurrent programs, like mutual exclusion, deadlock freedom, and partial correctness. CMAP, as an extension \nof CCAP with dynamic thread creation, is more expressive than CCAP (it is straightforward to translate \nthe CCAP code and veri.cation to PCC packages in CMAP). There\u00adfore, CMAP also applies for those CCAP \nexamples. In Appendix B, we give a variation of the GCD program shown in [42], where collaborating threads \nare forked by a main thread. We also show how thread join can be implemented and reasoned in CMAP.  \n6. Related Work and Conclusion We have presented a certi.ed programming framework for verify\u00ading multi-threaded \nassembly code with unbounded dynamic thread creation. Our work is related to two directions of research: \ncon\u00adcurrency veri.cation and PCC. The rely-guarantee method [23, 35, 36, 1, 13, 42] is one of the best \nstudied technique for compositional concurrent program veri.cation. However, most of the work on it are \nbased on high-level languages or calculi, and none of them sup\u00adport unbounded dynamic thread creation. \nOn the other hand, many PCC frameworks [28, 27, 2, 18, 41, 8, 42] have been proposed for machine/assembly \ncode veri.cation, but most of them only support sequential code. The only intersection point of these \ntwo directions is the work on CCAP [42], which applies the R-G method at the assembly level to verify \ngeneral concurrency properties, like mu\u00adtual exclusion and deadlock-freedom. Unfortunately, CCAP does \nnot support dynamic thread creation either. Recently, O Hearn and others [31, 4, 3] applied separation \nlogic to reason about concurrent programs. Bornat et al. [3] showed how to verify the race-free property \nof a program solving the readers and writers problem, which is similar to the program presented in this \npaper. However, their work heavily depends on the higher\u00adlevel language features, such as resources and \nconditional critical regions. As other traditional work on concurrency veri.cation, they only support \nnested cobegin/coend structure. It is not clear how their technique can be applied to support assembly \ncode veri.cation with unbounded dynamic thread creation. A number of model checkers have been developed \nfor concur\u00adrent software veri.cation, but most of them only check programs with a .xed .nite number of \nthreads [22, 9, 19, 5, 16, 14]. The CIRC algorithm [20] supports unbounded number of threads, but it \ndoesn t model the dynamic thread creation and the changing thread environment. Qadeer and Rehof [33] \npointed out that veri.cation of concurrent boolean program with unbounded parallelism is de\u00adcidable if \nthe number of context switches is bounded, but they do not directly verify dynamic thread creation either. \nGiven a context bound k, they reduce a dynamic concurrent program P to a pro\u00adgram Q with k+1 threads \nand verify Q instead. 3VMC [40] sup\u00adports both unbounded number of threads and dynamic thread cre\u00adation, \nbut it is not based on the rely-guarantee method and does not support compositional veri.cation. Many \ntype systems are also proposed to reason about concur\u00adrent programs [11, 12, 15, 17]. Unlike CMAP, which \nuses high\u00adorder predicate logic to verify general program properties, they are designed to automatically \nreason about speci.c properties of pro\u00adgrams, like races, deadlocks and atomicity. Also, they do not \ndi\u00adrectly generate proofs about program properties. Instead, proofs are implicitly embedded in their \nsoundness proofs. CMAP extends previous work on R-G method and CCAP with dynamic thread creation. We \nunify the concepts of a thread s as\u00adsumption/guarantee and its environment s guarantee/assumption, and \nallow a thread to change its assumption and guarantee to track the changing environment caused by dynamic \nthread creation. Code segments in CMAP can be speci.ed and veri.ed once and used in multiple threads \nwith different assumptions and guaran\u00adtees, therefore CMAP achieves better modularity than CCAP. Some \npractical issues, such as argument passing at thread creation, thread local data, and multiple invocation \nof one copy of thread code, are also discussed to support practical multi-threaded programming. CMAP \nhas been developed using the Coq proof assistant, along with a formal soundness proof and the veri.ed \nexample programs. The goal of our work is to provide an explicit and general frame\u00adwork (with smaller \nTCB) such that code and speci.cations at the higher level can be compiled down to the assembly level. \nAlthough directly specifying and proving CMAP programs may be daunting, it is simpler at the higher level. \nAlso, there are common concur\u00adrent idioms that would permit the assumption/guarantee to be gen\u00aderated \nautomatically during compilation. For example, for critical sections protected by lock, the assumption \nis always like nobody else will update the protected data if I am holding the lock... (see Figure 14). \nAnother scenario is illustrated by the example shown in Figure 3 and 10, where each thread is exclusively \nresponsible for a single piece of global data. In these scenarios, the assumption is always like nobody \nelse will touch my share of data and the guar\u00adantee is like I will not update other threads data . Automatically \ngenerating assumptions/guarantees and proofs for common idioms, and compiling higher level concurrent \nprograms and speci.cations down to CMAP, will be our future work. The thread primitives in CMAP are higher-level \npseudo instruc\u00adtions. They can be replaced by system calls to certi.ed thread li\u00adbraries, which is part \nof our ongoing work. Also we separate issues in multi-threaded programming from the embedded code pointer \nproblem which is addressed in a companion paper [30]. Applying that framework to our thread model will \nbe part of the future work. 7. Acknowledgment We thank Andrew McCreight and anonymous referees for sugges\u00adtions \nand comments on an earlier version of this paper. Rodrigo Ferreira helped implement the CMAP language \nand its soundness proof in the Coq proof system. This research is based on work sup\u00adported in part by \ngrants from Intel and Microsoft, and NSF grant CCR-0208618. Any opinions, .ndings, and conclusions contained \nin this document are those of the authors and do not re.ect the views of these agencies. References [1] \nM. Abadi and L. Lamport. Conjoining speci.cations. ACM Trans. on Programming Languages and Systems, 17(3):507 \n535, 1995. [2] A. W. Appel. Foundational proof-carrying code. In Proc. 16th Annual IEEE Symposium on \nLogic in Computer Science, pages 247 258. IEEE Computer Society, June 2001. [3] R. Bornat, C. Calcagno, \nP. W. O Hearn, and M. J. Parkinson. Permission accounting in separation logic. In Proc. 32th ACM Symp. \non Principles of Prog. Lang., pages 259 270, 2005. [4] S. D. Brookes. A semantics for concurrent separation \nlogic. In Proc. 15th International Conference on Concurrency Theory (CONCUR 04), volume 3170 of LNCS, \npages 16 34, 2004. [5] S. Chaki, E. Clarke, A. Groce, S. Jha, and H. Veith. Modular veri.cation of software \ncomponents in C. In ICSE 03: International Conference on Software Engineering, pages 385 395, 2003. [6] \nC. Colby, P. Lee, G. Necula, F. Blau, M. Plesko, and K. Cline. A certifying compiler for Java. In Proc. \n2000 ACM Conf. on Prog. Lang. Design and Impl., pages 95 107, New York, 2000. ACM Press. [7] T. Coquand \nand G. Huet. The calculus of constructions. Information and Computation, 76:95 120, 1988. [8] K. Crary. \nToward a foundational typed assembly language. In Proc. 30th ACM Symp. on Principles of Prog. Lang., \npages 198 212, 2003. [9] C. Demartini, R. Iosif, and R. Sisto. dSPIN: A dynamic extension of SPIN. In \nProc. 5th and 6th International SPIN Workshops on Theoretical and Practical Aspects of SPIN Model Checking, \npages 261 276, London, UK, 1999. Springer-Verlag. [10] R. Ferreira and X. Feng. Coq (v8.0) implementation \nfor CMAP language and the soundness proof. http://flint.cs.yale. edu/publications/cmap.html, Mar. 2005. \n[11] C. Flanagan and M. Abadi. Object types against races. In CONCUR 99 Concurrency Theory, volume 1664 \nof LNCS, pages 288 303. Springer-Verlag, 1999. [12] C. Flanagan and M. Abadi. Types for safe locking. \nIn Proceedings of the 8th European Symposium on Programming Languages and Systems, pages 91 108. Springer-Verlag, \n1999. [13] C. Flanagan, S. N. Freund, and S. Qadeer. Thread-modular veri.cation for shared-memory programs. \nIn Proc. 2002 European Symposium on Programming, pages 262 277. Springer-Verlag, 2002. [14] C. Flanagan \nand S. Qadeer. Thread-modular model checking. In Proc. 10th SPIN workshop, pages 213 224, May 2003. [15] \nC. Flanagan and S. Qadeer. A type and effect system for atomicity. In Proceedings of the ACM SIGPLAN \n2003 conference on Programming Language Design and Implementation, pages 338 349, 2003. [16] D. Giannakopoulou, \nC. S. Pasareanu, and H. Barringer. Assumption generation for software component veri.cation. In ASE 02: \nAuto\u00admated Software Engineering, pages 3 12, 2002. [17] D. Grossman. Type-safe multithreading in cyclone. \nIn Proceedings of the 2003 ACM SIGPLAN International workshop on Types in Languages Design and Implementation, \npages 13 25, 2003. [18] N. A. Hamid, Z. Shao, V. Trifonov, S. Monnier, and Z. Ni. A syntactic approach \nto foundational proof-carrying code. In Proc. Seventeenth Annual IEEE Symposium on Logic In Computer \nScience (LICS 02), pages 89 100. IEEE Computer Society, July 2002. [19] K. Havelund and T. Pressburger. \nModel checking Java programs using Java path.nder. Software Tools for Technology Transfer (STTT), 2(4):72 \n84, 2000. [20] T. A. Henzinger, R. Jhala, and R. Majumdar. Race checking by context inference. In PLDI \n04: Programming Language Design and Implementation, pages 1 13, 2004. [21] C. A. R. Hoare. Communicating \nsequential processes. Communica\u00adtions of the ACM, 21(8):666 677, 1978. [22] G. J. Holzmann. Proving properties \nof concurrent systems with SPIN. In Proc. 6th International Conference on Concurrency Theory (CONCUR \n95), volume 962 of LNCS, pages 453 455. Springer, 1995. [23] C. B. Jones. Tentative steps toward a development \nmethod for interfering programs. ACM Trans. on Programming Languages and Systems, 5(4):596 619, 1983. \n[24] L. Lamport. The temporal logic of actions. ACM Trans. on Programming Languages and Systems, 16(3):872 \n923, May 1994. [25] R. Milner. A Calculus of Communicating Systems. Springer-Verlag New York, Inc., 1982. \n[26] G. Morrisett, K. Crary, N. Glew, D. Grossman, R. Samuels, F. Smith, D. Walker, S. Weirich, and S. \nZdancewic. TALx86: a realistic typed assembly language. In 1999 ACM SIGPLAN Workshop on Compiler Support \nfor System Software, pages 25 35, Atlanta, GA, May 1999. [27] G. Morrisett, D. Walker, K. Crary, and \nN. Glew. From System F to typed assembly language. In Proc. 25th ACM Symp. on Principles of Prog. Lang., \npages 85 97. ACM Press, Jan. 1998. [28] G. Necula. Proof-carrying code. In Proc. 24th ACM Symp. on Principles \nof Prog. Lang., pages 106 119. ACM Press, Jan. 1997. [29] G. Necula and P. Lee. The design and implementation \nof a certifying compiler. In Proc. 1998 ACM Conf. on Prog. Lang. Design and Impl., pages 333 344, New \nYork, 1998. [30] Z. Ni and Z. Shao. Certi.ed assembly programming with embedded code pointers. Technical \nreport, Dec. 2004. http://flint.cs. yale.edu/publications/ecap.html. [31] P. W. O Hearn. Resources, concurrency \nand local reasoning. In Proc. 15th International Conference on Concurrency Theory (CONCUR 04), volume \n3170 of LNCS, pages 49 67, 2004. [32] C. Paulin-Mohring. Inductive de.nitions in the system Coq rules \nand properties. In Proc. TLCA, volume 664 of LNCS, 1993. [33] S. Qadeer and J. Rehof. Context-bounded \nmodel checking of concurrent software. In TACAS 05: Tools and Algorithms for the Construction and Analysis \nof Systems, pages 93 107, 2005. [34] J. H. Reppy. CML: A higher concurrent language. In Proc. 1991 Conference \non Programming Language Design and Implementation, pages 293 305, Toronto, Ontario, Canada, 1991. ACM \nPress. [35] E. W. Stark. A proof technique for rely/guarantee properties. In Proc. 5th Conf. on Foundations \nof Software Technology and Theoretical Computer Science, volume 206 of LNCS, pages 369 391, 1985. [36] \nK. St\u00f8len. A method for the development of totally correct shared\u00adstate parallel programs. In Proc. 2nd \nInternational Conference on Concurrency Theory (CONCUR 91), pages 510 525, 1991. [37] The Coq Development \nTeam. The Coq proof assistant reference manual. The Coq release v7.1, Oct. 2001. [38] B. Werner. Une \nTh\u00b4PhD thesis, A eorie des Constructions Inductives. L Universit\u00b4e Paris 7, Paris, France, 1994. [39] \nA. K. Wright and M. Felleisen. A syntactic approach to type soundness. Information and Computation, 115(1):38 \n94, 1994. [40] E. Yahav. Verifying safety properties of concurrent Java programs using 3-valued logic. \nIn Proc. 28th ACM Symp. on Principles of Prog. Lang., pages 27 40, New York, NY, USA, 2001. ACM Press. \n[41] D. Yu, N. A. Hamid, and Z. Shao. Building certi.ed libraries for PCC: Dynamic storage allocation. \nIn Proc. 2003 European Symposium on Programming, LNCS Vol. 2618, pages 363 379, 2003. [42] D. Yu and \nZ. Shao. Veri.cation of safety properties for concurrent assembly code. In Proc. 2004 ACM SIGPLAN Int \nl Conf. on Functional Prog., September 2004. A. The Soundness of CMAP Lemma A.1 (Progress) F=(Inv,.,.). \nIf there exists T, p, g, Aand Gsuch that F; T; (p,g,A,G) f ((M,R),Q,T,C,I), then (Inv M), and there exists \na program P.such that ((M,R),Q,T,C,I) -. P.. Proof sketch: By induction over the structure of I. (Inv \nM) holds by the assumption and the inversion of the rule PROG. In the cases where Istarts with add, sub, \nmovi, fork, exit,or yield, the program can always make a step by the de.nition of the operational semantics \n(recall in our abstract machine we always assume there is an idle thread in Q, therefore the instructions \nfork and exit can go through). In the cases where Istarts with ld and st, the side conditions for make \na step, as de.ned by the operational semantics, are established by the rules LD and ST. In the cases \nwhere Istarts with bgt or beq, or where Iis jd, the operatinal semantics may fetch a code block from \nthe code heap; such a code block exists by the inversion of the rule CDHP. . Lemma A.2 (Preservation) \nIf F; T; (p,g,A,G) f Pand P-. P., where P=(S,Q,T,C,I) and P.=(.S,Q.,T,C,.I), then there exist T., p., \ng., A.and G.such that F; .p,.A.,G.) f P.. T; (.g, Proof sketch. By the assumption F; T; (p,g,A,G) f Pand \nthe inversion of the rule PROG, we know that 1. (Inv,.,.) = F, (M,R)= S, t = S(rt); 2. F f T; 3. F \nf C; 4. (Inv . p) S; 5. F; (p,g,A,G) f I; 6. F; T; (g,S) f Q; 7. NI(T{ t . (p,A,G)} ,Q{ t . (R,I)} \n) (see Section 4.4 for the de.nition of NI).  We prove the lemma by induction over the structure of \nI. Here we only give the detailed proof of cases where Istarts with fork and yield. Proof for the rest \ncases are trivial. Case I= fork h,r; I \"\" . By the operational semantics, we know that .S= S, Q= Q{ \nt . \"\" \"\"\" (R,I)} , and .I= I , where R = { r0 . ,...,r15 . ,rt . \"\"\" \" t ,ra . S(r)} , I = T(h), t ..dom(Q) \nand t .= t. According to 5 and the inversion of the FORK rule, we know that there exist \"\"\"\"\" \"\" p , \nA, G, A and G such that \"\" f.1 (p , A, G \" )=.(h); \"\"\"\" \"\" ).A \" \"\" f.2 A.A , G .G, (A.G \", and G .(G.A \n); \"\" f.3 .(M, R)..R..M.(Inv .p)(M, R).R(rt)=R \" (rt).R(r)= \" \"\" R \" (ra).g RMM .p (M, R \" ); \"\" \"\" )fI \n\"\" f.4 (Inv, ., .);(p, g, A , G Then we let .. (p ,G pg A, T=T{ t ,A )} , .= p, .= g, = A and G.= G \n\"\" . According to the rule PROG, to prove F; .p,.A.,G.) f P., T; (.g, we need prove the following: F \nf Tand F f C. They trivially follow 2 and 3. Since these two conditions never change, we will omit the \nproof of them in the following cases.  (Inv . p) S.By 4.  \"\"\"\" \"\" F; (p,g,A ,G ) f I . By f.4.  F; \n.) f .. By 6 and the inversion of the rule DTHRDS,  T; (g,SQwe need check the following for the new \nthread t \" : \"\"\" \"\"\" \"\"\"\"\" . M,M .(Inv . p )(M,R) . ARMM . \" \"\"\" p (M ,R). By 2 and the inversion of \nthe rule THRDS. \"\"\"\" \" (Inv,.,.); (p ,G,A,G) f I. By 2 and the inversion of the rule THRDS. \"\" \"\" \"\"\"\" \n . M .g SM . p (M ,R). By 4, and f.3. NI(T.{ t . (p,A ,G )} ,Q{ t . (R,I )} ). By 4, 7, f.2, and Lemma \n3.4. Case I= yield; I \"\" . By the operational semantics, we know that .S=(M,R \" ), Q.= \"\" (Q{ R(rt) \n. (R,I )} ) \\{ t} , and .I= I \" , where t . dom(Q) and \"\" \"\"\"\" (R,I)= Q(t) or t = R(rt) and (R,I)=(R,I \n). If t = R(rt), the yield instruction is like a no-op instruction. The proof is trivial. So we just \nconsider the case that t . dom(Q) and (R \" ,I \" )= Q(t). T=(T{ Rp \" g \" We let .(rt) . (p,A,G)} ) \\{ \nt} , .= p , .= G, A= A \" , and G.= G, where (p ,A,G)= T(t). According to the rule PROG, to prove F; .p,.A., \n.) f ., we need prove: T; (.g, GP \"\" \" (Inv . p )(M,R). By 4 we know that Inv (M,R) (note that Inv is \nindependent of R). By 6 and the inversion of the DTHRDS rule, we know that \"\" \"\" \"\"\"\" . M .g SM . p (M \n,R), also by 4, 5 and the inversion of the YIELD rule we get g SM. Therefore we can prove p \" (M,R \" \n). \"\"\"\" F; (p ,G,A,G) f I \" . By 6 and the inversion of DTHRDS.  F; .,(M,R)) f Q.  T; (G To prove \nthis, we only check the following for the yielding thread: \"\"\" \" \"\"\"\"\" . M,M .(Inv . p)(M,R) . ARM M \n. p(M ,R). This follows 5 and the inversion of the rule YIELD. (Inv,.,.); (p,G,A,G) f I \"\" . This follows \n5 and the in\u00adversion of the rule YIELD. \"\"\"\" \"\" \"\" ). . M .G R MM . p (R,M By7 weknow \"\"\"\"\"\"\"\" \"\"\" . \nM,M .GRMM . ARM M . Therefore, \"\" \"\"\"\" we only need prove: . M .ARMM . p (M ,R).By 5, the inversion of \nthe rule YIELD, and 4 we get it. NI(T.{ t . (p ,A,G)} ,Q{ t . (R,I)} ), which follows 7. . Theorem A.3 \n(Soundness) F=(Inv,.,.). If there exists T, p, g, Aand Gsuch that F; T; (p,g,A,G) f P0, then for any \nn = 0, there exist M, R, n Q, T, Cand Isuch that P0 -. ((M,R),Q,T,C,I) and (Inv M). Proof sketch. Given \nthe progress and the preservation lemmas, this theorem can be easily proved by induction over n. However, \ninstead of proving (Inv M), we need strengthen the induction hypothesis and prove F; .p,.A.,G.) f ((M,R),Q,T,C,I) \nT; (.g,for some T., p., g., A.and G. . .  B. Partial Correctness of A Lock-Free Program Figure 16 and \n17 give the CMAP implementation of the GCDalgo\u00adrithm shown in Figure 15, where two threads collaborate \nto com\u00adpute the greatest common divisor of two numbers. This is a lock\u00adfree algorithm, i.e., no synchronization \nis required to ensure the non-interference even if atomic operations are machine instruc\u00adtions. To show \nthe lock-free property, we insert yield instructions at every program point of the chld thread (some \nyield instructions are omitted in the main thread for clarity). In Figure 18 we show de.nitions of assertions \nused to specify the program. The following formulae show the speci.cations of static threads, and the \ninitial memory and instruction sequence. Inv =True . ={main . (True, A0, G0), chld . (p2 .p8, Ag, Gg)} \nInitial M={a . , b . , flag . , (flag +1).  } Initial I =jd begn Variables: nat a,b; nat[2] flag; Main \n: a := a; b := \u00df; flag[0]:= 0; fork(gcd,0); flag[1]:= 0; fork(gcd,1); while (!flag[0]) yield; while (!flag[1]) \nyield; post proc ... void gcd(int x){ while (a = b){ if ((a >b)&#38;&#38;(x = 0)) a := a - b; else if \n((a <b)&#38;&#38;(x = 1)) b := b - a; } flag[x]:= 1; } Figure 15. GCD: high-level program main : -{(True,A0 \n,G0 )} next : -{p2 ,g,A1 ,G1 }jd begn yield begn : -{(True,G0 ,A0 ,G0 )} st r1 (1),r0 movi r0 ,a yield \nmovi r1 ,a movi r2 ,1 st r1 (0),r0 fork chld,r2 yield join1 : -{p2 . p3 ,g,A2 ,G2 }movi r0 ,\u00df yield movi \nr1 ,b ld r2 ,r1 (0) st r1 (0),r0 beq r0 ,r2 ,join1 yield join2 : -{p2 . p4 ,g,A2 ,G2 }movi r0 ,0 yield \nmovi r1 ,flag ld r2 ,r1 (1) st r1 (0),r0 beq r0 ,r2 ,join2 yield post : -{(p2 . p5 ,g,A2 ,G2 )}movi r2 \n,0 yield fork chld,r2 ... Figure 16. GCD: the main thread Inv is simply set to True. The partial correctness \nof the gcd algorithm is inferred by the fact that p2 is established at the entry point and Gg is satis.ed. \nTwo child threads are created using the same static thread chld. They use thread arguments to distinguish \ntheir task. Correspond\u00adingly, the assumption Ag and Gg of the static thread chld also use the thread \nargument to distinguish the speci.cation of different dy\u00adnamic copies. The child thread does not change \nits assumption and guarantee throughout its lifetime. Therefore we omit Ag and Gg in the code heap speci.cations. \nSince we insert yield instructions at every program point, every local guarantee is simply Gg, which \nis also omitted in the speci.cations. At the data initiation stage, the main thread assumes no threads \nin the environment changes a, b and the .ags, and guarantees nothing, as shown in A0 and G0 (see Figure \n18). After creating child threads, the main thread changes its assumption and guarantee to re.ect the \nchanging environment. The new assumption is just the disjunction of the previous assumption and the guarantee \nof the new thread (instantiated by the thread id and thread argument), similarly the new guarantee is \nthe conjunction of the previous guarantee and the new thread s assumption. This example also shows how \nthread join can be implemented in CMAP by synchronization. We use one .ag for each thread to indicate \nif the thread is alive or not. The assumptions and guaran\u00adtees of the main thread also take advantages \nof these .ags so that it can weaken its guarantee and strengthen its assumption after the child threads \ndie. chld : -{(p2 . p8 ,Ag ,Gg )}jd gcd gcd : -{p2 . p8 } yield movi r0 ,0 loop : -{p2 . p8 . p9 } yield \nld r1 ,r0 (a) yield -{p2 . p9 . p10 } ld r2 ,r0 (b) yield -{p2 . p9 . p11 } beq r1 ,r2 ,done yield \nbgt r1 ,r2 ,calc1 yield jd calc2 done : -{p1 . p2 . p8 } yield movi r0 ,1 yield st ra(flag),r0 exit \ncalc1 : -{p2 . p9 . p12 } yield beq r0 ,ra,cld1 yield jd loop cld1 : -{p2 . p9 . p13 } calc2 yield sub \nr3 ,r1 ,r2 yield st r0 (a),r3 yield jd loop : -{p2 . p9 . p14 } yield movi r3 ,1 yield beq r3 ,ra,cld2 \nyield jd loop cld2 : -{p2 . p9 . p15 } yield sub r3 ,r2 ,r1 yield st r0 (b),r3 yield jd loop Figure 17. \nGCD: the chld thread \" id1 = a = a id2 = b = b \" id3 = (flag[0] = flag \" [0]) . (flag[1] = flag \" [1]) \n G0 = True A0 = id1 . id2 . id3 \" p = id2 . (a >b . (GCD(a,b)= GCD(a ,b \" ))) . (a = b . id1) \"\" p \n= id1 . (a <b . (GCD(a,b)= GCD(a ,b \" ))) . (a = b . id2) Gg = (flag \" [ra]=0 . (([ra]=0 . p) . ([ra]=1 \n. p \" ))) \" .(flag \" [ra]=1 . (id1 . id2 . a = b \" )) .(flag[ra]=1 . S = S \" ) .(flag[1 - ra]= flag \" \n[1 - ra]) Ag = (flag \" [ra]= 0) . ((flag[ra]= flag \" [ra]) \" .(([ra]=0 . p ) . ([ra]=1 . p))) \" G1 = \nG0 . (flag \" [0] = 0 . ((flag[0] = flag \" [0]) . p )) A1 = A0 . ((flag[1] = flag \" [1]) . (flag \" [0] \n= 0 . p) \" .(flag \" [0] = 1 . (id1 . id2 . a = b \" ))) .(flag[0] = 1 . S = S \" )) G2 = G1 . (flag \" [1] \n= 0 . ((flag[1] = flag \" [1]) . p)) A2 = A1 . ((flag[0] = flag \" [0]) . (flag \" [1] = 0 . p \" ) \" .(flag \n\" [1] = 1 . (id1 . id2 . a = b \" ))) .(flag[1] = 1 . S = S \" )) g = S = S \" p1 = a = b p2 = GCD(a,b)= \nGCD(a,\u00df) p3 = flag[0] = 0 . (flag[0] = 1 . p1) p4 = (flag[1] = 0 . flag[1] = 1) . flag[0] = 1 . p1 p5 \n= flag[0] = 1 . flag[1] = 1 . p1 p6 = [ra]=0 . flag[0] = 0 p7 = [ra]=1 . flag[1] = 0 p8 = p6 . p7 p9 \n= [r0]=0 p10 = (p6 . [r1]= a) . (p7 . ([r1] = b =>[r1]= a)) p11 = (p6 . [r1]= a . ([r1] = [r2]=>[r2]= \nb)) .(p7 . [r2]= b . ([r1] = [r2]=>[r1]= a)) p12 = [r1] >[r2] . ([ra]=0 . (flag[0] = 0 . [r1]= a . \n[r2]= b)) p13 = p6 . [r1] >[r2] . [r1]= a . [r2]= b p14 = [r1] <[r2] . ([ra]=1 . (flag[1] = 0 . [r1]= \na . [r2]= b)) p15 = p7 . [r1] <[r2] . [r1]= a . [r2]= b Figure 18. GCD: Assertion De.nitions  \n\t\t\t", "proc_id": "1086365", "abstract": "Proof-carrying code (PCC) is a general framework that can, in principle, verify safety properties of arbitrary machine-language programs. Existing PCC systems and typed assembly languages, however, can only handle <i>sequential</i> programs. This severely limits their applicability since many real-world systems use some form of concurrency in their core software. Recently Yu and Shao proposed a logic-based \"type\" system for verifying concurrent assembly programs. Their thread model, however, is rather restrictive in that no threads can be created or terminated dynamically and no sharing of code is allowed between threads. In this paper, we present a new formal framework for verifying general multi-threaded assembly code with unbounded dynamic thread creation and termination as well as sharing of code between threads. We adapt and generalize the rely-guarantee methodology to the assembly level and show how to specify the semantics of thread \"fork\" with argument passing. In particular, we allow threads to have different assumptions and guarantees at different stages of their lifetime so they can coexist with the dynamically changing thread environment. Our work provides a foundation for certifying realistic multi-threaded programs and makes an important advance toward generating proof-carrying concurrent code.", "authors": [{"name": "Xinyu Feng", "author_profile_id": "81100140619", "affiliation": "Yale University, New Haven, CT", "person_id": "PP14059606", "email_address": "", "orcid_id": ""}, {"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Yale University, New Haven, CT", "person_id": "PP14127817", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086399", "year": "2005", "article_id": "1086399", "conference": "ICFP", "title": "Modular verification of concurrent assembly code with dynamic thread creation and termination", "url": "http://dl.acm.org/citation.cfm?id=1086399"}