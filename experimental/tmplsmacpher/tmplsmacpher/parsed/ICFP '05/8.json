{"article_publication_date": "09-12-2005", "fulltext": "\n AtomCaml: First-Class Atomicity via Rollback Michael F. Ringenburg * Dan Grossman Department of Computer \nScience &#38; Engineering, University of Washington, Seattle, WA 98195 {miker,djg}@cs.washington.edu \nAbstract We have designed, implemented, and evaluated AtomCaml, an ex\u00adtension to Objective Caml that \nprovides a synchronization primitive for atomic (transactional) execution of code. A .rst-class primitive \nfunction of type (unit-> a)-> a evaluates its argument (which may call other functions, even external \nC functions) as though no other thread has interleaved execution. Our design ensures fair scheduling \nand obstruction-freedom. Our implementation extends the Objective Caml bytecode compiler and run-time \nsystem to sup\u00adport atomicity. A logging-and-rollback approach lets us undo un\u00adcompleted atomic blocks \nupon thread pre-emption, and retry them when the thread is rescheduled. The mostly functional nature \nof the Caml language and the Objective Caml implementation s com\u00admitment to a uniprocessor execution \nmodel (i.e., threads are in\u00adterleaved, not executed simultaneously) allow particularly ef.cient logging. \nWe have evaluated the ef.ciency and ease-of-use of Atom-Caml by writing libraries and microbenchmarks, \nwriting a small application, and replacing all locking with atomicity in an existing, large multithreaded \napplication. Our experience indicates the per\u00adformance overhead is negligible, atomic helps avoid synchroniza\u00adtion \nmistakes, and idioms such as condition variables adapt reason\u00adably to the atomic approach. Categories \nand Subject Descriptors D.3.3 [Language Constructs and Features]: Concurrent programming structures General \nTerms Languages Keywords Atomicity, Transactions, Concurrent Programming, Objective Caml  1. Introduction \nConcurrency has been an important and widely-used programming idiom for decades, even on uniprocessors. \nProgrammers can mask latency by spawning new threads to handle I/O or other inef.cient tasks while other \nthreads continue to compute. They can also im\u00adprove the code structure and response time of interactive \napplica\u00adtions by spawning new threads to handle user requests while the original thread waits for new \nrequests. Unfortunately, concurrency * Supported in part by an ARCS fellowship sponsored by the Washington \nResearch Foundation. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 05 September 26 28, 2005, Tallinn, Estonia. Copyright c . 2005 ACM 1-59593-064-7/05/0009. \n. . $5.00. remains a common source of software errors, resulting in incorrect behavior, crashes, and \nsecurity vulnerabilities. For example, recent searches for race condition on the SecurityFocus [32] and \nUS CERT [37] websites yielded 994 and 537 hits respectively. These dif.culties are well-known and inspired \nmuch of the related work described in Section 3. Writing code in which threads communicate via mutable \nshared memory will never be easy, but there is an increasing belief that locks and condition variables \n(the most common concurrency prim\u00aditives in today s high-level languages) make matters worse: As low\u00adlevel \nprimitives they are dif.cult to reason about and provide only weak guarantees. For example, locks provide \ncorrect synchroniza\u00adtion only if every thread accessing shared resources acquires and releases locks \nat the correct times. Recently, several researchers have proposed replacing locks with atomicity (see, \ne.g., [14, 18, 16]). If a block of code is marked as atomic, the language implementation guarantees that \nthe code appears to execute without interleaving of other threads. Instead of disabling interrupts, the \nimplementation also ensures fair scheduling. Unlike locks, this guarantee is provided regardless of the \nbehavior of the other threads.1 Figure 1 contains an example of an atomic block and Section 2 reviews \nsome of the technical reasons to prefer atomicity. For atomicity to become the next-generation concurrency \nprim\u00aditive, now is the time to investigate the relevant questions in lan\u00adguage design, language implementation, \nand programming idioms. To do so, we have built AtomCaml, a prototype system that ex\u00adtends the Objective \nCaml bytecode system with atomicity. At the language level, the atomic construct is a .rst-class function \nof type (unit-> a)-> a that takes a function and executes it atomically. The block can contain arbitrary \ncode, including buffered output, ex\u00adceptions, and calls to Caml functions or native C code. AtomCaml \nis available for download from our website [1]. Our implementation uses logging and rollback to undo \nan un\u00adcompleted atomic s effects if the thread executing it is pre-empted. Like the Objective Caml implementation, \nwe assume a uniproces\u00adsor execution model i.e., that (shared-memory) threads are in\u00adterleaved, and not \nrun in parallel. Though support for multipro\u00adcessing is also important, we believe uniprocessors are \nan impor\u00adtant special-case that allows particularly ef.cient logging. Just as garbage collectors and \noperating systems use specialized tech\u00adniques for uniprocessors, atomicity implementations should too. \nOur evaluation includes microbenchmarks showing the low overhead of atomic, libraries demonstrating key \nprogramming id\u00adioms, a small application demonstrating atomic s ease-of-use, and a port of the PLANet \nactive-network implementation [22, 21] that removes all uses of locks. In particular, atomic is typically \neasier to 1 Some systems provide a slightly weaker guarantee, requiring that all shared memory accesses \nin all threads occur inside atomic sections for the atomicity property to hold. let withdraw amt = let \nwithdraw amt = Mutex.lock acctLock; Thread.atomic (fun () -> let oldBalance = readBalance () in let oldBalance \n= readBalance () in let newBalance = oldBalance -amt in let newBalance = oldBalance -amt in setBalance \nnewBalance; setBalance newBalance Mutex.unlock acctLock ) Figure 1. A withdraw function written with \nObjective Caml s Mutex library, and the same function written with atomic. The atomic code is just as \neasy to write, and provides a stronger synchronization guarantee. use than locks, but idioms using condition \nvariables require some care. In porting PLANet, we .xed concurrency bugs and noticed no signi.cant change \nin performance.2 The rest of this paper is organized as follows: Section 2 dis\u00adcusses some bene.ts of \nusing atomic rather than locks. Section 3 discusses related work including other approaches to providing \natomic execution of code. Section 4 describes the design of Atom-Caml from the programmer s perspective. \nWe present the function\u00adality we added to Caml, the guarantees it provides, and the inter\u00adesting language \ndesign questions that arose. Section 5 describes the implementation of AtomCaml, including our basic \nrollback ap\u00adproach, the implementation details, and some interesting imple\u00admentation choices. Section \n6 describes our experience using Atom-Caml. Section 7 concludes and presents directions for future work, \nincluding support for true shared-memory parallelism.  2. The Case for Atomic Concurrency errors are \nstill prevalent in modern software in part because locks are dif.cult to use correctly in complex systems. \nAc\u00adcesses to shared resources may be spread across many procedures and .les. If any access is not protected \nby the correct lock or locks, a race condition may occur. The race may result in incorrect be\u00adhavior \nby the unprotected code, or it may cause incorrect behavior by protected pieces of code that access the \nsame resource. In the latter case, the source of the error can be dif.cult to locate. Fur\u00adther compounding \nthe dif.culty, a thread may deadlock because another thread (often executing a different procedure) fails \nto re\u00adlease a lock. These complex interactions force programs to obey subtle program-wide invariants. \nOn the other hand, atomicity provides a key error-localization advantage: An atomic block will execute \nas though there was no in\u00adterleaving, even if code executing in other threads is poorly written. Conversely, \na thread executing an atomic block that fails to com\u00adplete in a timely manner may make little progress, \nbut fair schedul\u00ading ensures other threads will not be starved. With locks, a thread holding a lock too \nlong can starve other threads. Atomicity also has several other advantages over locks: As code evolves, \nit is possible to update any collection of data objects atomically without risking deadlock or changing \nexisting code to obey a new locking discipline.  Wrapping an abstract datatype s operations in atomic \nblocks can make the abstraction thread-safe without unneeded locking.  As Flanagan and Qadeer showed \n[14], atomicity is often con\u00adceptually what programmers achieve with existing synchroniza\u00adtion primitives; \nproviding atomicity directly makes program\u00adming easier.  Atomicity and locks peacefully coexist. It \nis trivial for program\u00admers to implement locks with the atomic primitive, but the implementation also \nneed not remove conventional lock imple\u00admentations.  2 The performance point is admittedly a tad weak \nsince a real active network would not run as bytecodes in user space. Atomicity does not eliminate the \ngranularity problem: Pro\u00adgrammers must still decide whether concurrent operations are .ne\u00adgrained (potentially \nincreasing performance and nondeterminism) or coarse-grained (which in the limit becomes sequential code). \nHowever, atomicity does make it easier to mix .ne-grained and coarse-grained operations; locking idioms \nrequire complex and error-prone hierarchical locking schemes.  3. Related Work Atomicity is not a new \nidea: Operating systems disable interrupts for short sequences. Databases group operations into atomic \ntrans\u00adactions. Distributed systems employ protocols to commit consistent updates. But as a general-purpose \nconcurrency primitive, atomicity has enjoyed a recent surge of interest in language design and im\u00adplementation. \nHarris et al. have pioneered most of the recent work on lan\u00adguage support for atomicity. Their design \nfor Java [18, 17] adds a statement form for atomic execution and uses software transac\u00adtional memories \n(STMs) [20, 33] to ensure threads commit consis\u00adtent views to shared memory. Their atomic blocks have \nguards that must hold before the atomic block executes. More recently, they developed a system for Haskell \n[19] in which a transactions monad provides atomicity and the composition (both sequential and alter\u00adnative) \nof smaller transactions. Both systems are extremely promis\u00ading and motivated our work considerably. Our \nwork complements or extends the work by Harris in sev\u00aderal ways. First, the Java system provides a weaker \nguarantee (atomic blocks appear atomic only to other atomic blocks) and the Haskell system relies on \nHaskell s purity for the stronger guar\u00adantee. Second, in AtomCaml atomic is a .rst-class function of \ntype (unit-> a)-> a (STM Haskell s atomic is also .rst-class, but has return type IO a rather than a). \nThird, we eschew the sophisticated data structures and commit protocols of STMs for simple logging and \nuniprocessor-based simpli.cations. Fourth, we develop an API for allowing calls to native code from atomic \n(in\u00adstead of raising a run-time exception). Fifth, we report new experi\u00adence with applications and concurrency \nidioms. Other language work has made related contributions. Welc et al. s transactional monitors [40] \nprovide a weaker guarantee (code guarded by a given monitor appears atomic to threads executing code \nguarded by the same monitor) in hopes of allowing more par\u00adallelism. They also investigate the overhead \nof techniques (such as write barriers) that we expect will be part of most atomicity imple\u00admentations. \nManson et al. [25] use a logging-and-rollback mecha\u00adnism similar to ours to add preemptible atomic regions \nto Real-time Java. Less recently, the Venari project [41] used locks to imple\u00adment serialized transactions \nin SML. The ARGUS language [24] provided atomicity for actions that share only objects with special atomic \ntypes that had to be accessed via handler calls. Flanagan, Qadeer, and Freund [14, 13, 11] have taken \nthe com\u00adplementary approach of checking or inferring that lock-based code is actually atomic. In this \nsense, atomic is not a term-level primi\u00adtive but rather a checked type annotation. Programmers must still \nlet add_to_bbuf bbuf item = Thread.atomic (fun () -> if (is_full_bbuf bbuf) then Thread.yield_r bbuf.out_ptr \nelse (); bbuf.buffer.(!(bbuf.in_ptr))<-item; advance bbuf bbuf.in_ptr) let remove_from_bbuf bbuf = Thread.atomic \n(fun () -> if (is_empty_bbuf bbuf) then Thread.yield_r bbuf.in_ptr else (); let ans = bbuf.buffer.(!(bbuf.out_ptr)) \nin advance bbuf bbuf.out_ptr; ans) Figure 2. Bounded buffer insertion and removal functions that use \nyield_r to implement conditional critical regions in AtomCaml. The advance function updates the associated \nreference (bbuf.in_ptr or bbuf.out_ptr). This update wakes any threads that were suspended due toa yield_r \non that reference. use locks, but a type system can help detect unintended atomicity violations. Their \nwork builds on an underlying data-race detector. Data-race detectors and deadlock detectors also help \n.nd bugs in locking code, but race-freedom is neither necessary nor suf.cient for atomicity [14]. The \ndetectors attempt to identify race conditions statically (e.g., [12, 10, 9, 4, 5, 2]) or dynamically \n(e.g., [31, 8, 7, 38]), so that the programmer can .x them. We use rollback to implement atomicity whereas \nsome re\u00adsearchers have investigated a variant of exceptions that reverts state in addition to transferring \ncontrol [26, 34]. Despite similar imple\u00admentations, such a language construct is fundamentally different \nin use. Our support for external calls is also slightly more power\u00adful. Logging and rollback has also \nbeen used to prevent priority inversion [39], allow safe thread termination [30], and automate software \ncheckpointing [6, 36]. Rollback without needing logging can support speci.c atomic code sequences on \nuniprocessors, in\u00adcluding heap allocation [35] and an implementation of locking [3]. In the context of \nfunctional languages, Concurrent ML [28] and the more recent kill-safe abstractions of Flatt et al [15] \nare ele\u00adgant concurrency systems based on asynchronous message passing rather than implicit communication \nvia shared memory. More gen\u00aderally, it is well-understood that the lack of mutation inherent to functional \nprogramming reduces the need for synchronization. Our logging approach exploits this fact: the expense \nof logging is pro\u00adportional to the number of mutations executed in an atomic block. Finally, hardware \nsupport for software transactions remains on\u00adgoing research. Most recently the TCC project [16] uses \nhardware buffers, consistency protocols, and (in the extreme case) locking the bus to implement atomicity. \nTransactions have also been used as an optimistic implementation of locking [27].  4. The Design of \nAtomCaml In this section, we discuss the design of the AtomCaml language. Section 4.1 describes the atomic \nprimitive itself. Section 4.2 dis\u00adcusses the yield_r primitive and how it can be used to implement conditional \ncritical regions. Section 4.3 presents the interface that lets programmers specify how external C functions \ninteract with atomic blocks. Finally, Section 4.4 describes design choices regard\u00ading the interaction \nbetween atomic blocks, exceptions, and input. As we see below, the only user-visible changes to Objective \nCaml are atomic, yield_r, and some additional facilities for interacting with external C code.3 In addition, \nour system is fully backwards compatible with Objective Caml all Objective Caml programs are equivalent \nAtomCaml programs. 4.1 The atomic Primitive The atomic primitive in AtomCaml is a .rst class function \nof type (unit-> a)-> a. The function takes a thunked block of code as its argument and executes it atomically. \nThe implementation 3 In the actual implementation, we put the primitives in the Thread module. ensures \nthat there will appear to be no interleaving during the block s execution. For example, to execute the \nfollowing code atomically: let totalWidgets = !blackWidgets + !blueWidgets in if totalWidgets > 0 then \nprint_string (pickWidget () ^ \" available\") else raise NoWidgets we can simply write: atomic (fun () \n-> let totalWidgets= !blackWidgets + !blueWidgets in if totalWidgets > 0 then print_string (pickWidget \n() ^ \" available\") else raise NoWidgets) As we describe in more detail in Section 5, AtomCaml provides \nthis atomicity guarantee with a rollback and retry-based approach. When the currently executing thread \nreaches an atomic block, it simply begins executing the block. If the thread is not pre-empted during \nthe execution of the block, then it has executed atomically, since truly parallel execution is not possible \nin Objective Caml or AtomCaml. On the other hand, if the thread is pre-empted in the middle of the atomic \nblock, we roll the block back (undoing its side effects), and retry the block the next time the thread \nexecutes. The atomic block can contain arbitrary Caml code, including function calls to Caml code or \nto external C code (see Section 4.3). The atomic block can also raise exceptions that are caught inside \nor outside the block; an exception that leaves the atomic block causes the atomic block to complete. \nIn addition, Objective Caml s built\u00adin buffered output (print_int, print_string, etc.) can appear inside \natomic blocks, but our current implementation raises an exception if a built-in input function is called \ninside an atomic block. It would also be possible to allow input inside an atomic block, but as we discuss \nin Section 4.4, it is unclear if it is a good idea. So far, our applications have not needed it. Under \nour semantics, a nested atomic block (for example, an atomic inside a function called from an atomic \nblock) is redun\u00addant. Our atomic sections appear to execute without interleaving. Because the enclosing \natomic block will (appear to) execute with\u00adout interleaving, all nested blocks will automatically (appear \nto) execute without interleaving regardless of whether or not they are labeled as atomic blocks. 4.2 \nConditional Critical Regions and yield_r Conditional critical regions (CCR s) are a useful concurrent \npro\u00adgramming idiom. For example, a function that adds an item to a bounded buffer must .rst check if \nthe buffer is full, and if so, wait for space to be available. In a lock-based implementation, we would \nlike to avoid acquiring the buffer s lock if the buffer is full. How\u00adever, we must also ensure that another \nthread does not .ll the buffer between when we check and when we add the new item. CCR s make this possible \nby allowing us to delay entry to a critical sec\u00adtion until a speci.ed condition (a guard expression) \nholds. Similar issues arise when programming with atomic. We must check whether the buffer is full inside \nthe same atomic block that adds the item. Otherwise another thread could .ll the buffer be\u00adtween when \nwe check and when we insert our new item. However, if it turns out that the buffer is in fact full, we \nwould like to termi\u00adnate the atomic block immediately and try it again later, when the buffer is no longer \nfull. Semantically, an explicit call to yield (already provided in Caml) suf.ces: yield causes the thread \nto be suspended. Because the atomic block has not completed, we will undo any effects and restart the \nblock when the thread next runs. After programming this idiom with yield, we noticed that the yielding \ncode is often waiting for a mutable reference to have its contents changed and it is useless to rerun \nthe thread until such change occurs. The yield_r primitive (of type a ref -> unit) lets a thread indicate \nexactly that: yield_r x suspends the thread and allows (but does not require) the scheduler to skip the \nsuspended thread whenever the contents of the reference bound to x are the same as when yield_r was called. \nThus our bounded buffer insert function can check if the buffer is full, and if so, simply execute a \nyield_r on the buffer s out pointer. Figure 2 shows a thread-safe bounded\u00adbuffer implementation using \natomic and yield_r. These are the same functions we use in the simpli.ed web cache application described \nin Section 6.3. Appendix A provides another example, using the primitives to implement a variant of condition \nvariables. Note that it is always correct to use yield instead of yield_r. 4.3 Calling External C Functions \nUnlike the prior work described in Section 3, AtomCaml allows programmers to call external C functions \nfrom inside atomic blocks. Programmers can specify three types of behavior when they declare an external \nfunction: If their code can be run without modi.cation in an atomic context (e.g., if it only modi.es \nlocal state), they can use a declaration of the form: external foo : type_of_foo = \"c_foo\" The C function \nc_foo will be invoked whenever foo is called. If the programmer has created a separate version of the \nexternal function that is safe to call from inside an atomic section, they can use a declaration of the \nform: external foo : type_of_foo = \"c_foo1 c_foo2\" When we call foo from a non-atomic context, the .rst \nC func\u00adtion (c_foo1 in this case) will be invoked, and when we call it from an atomic section, the second \n(c_foo2) will be invoked. AtomCaml provides facilities, described below, that make it simple to create \natomic-safe versions of many C functions. If a function should never be called in an atomic context (e.g., \nif the programmer has not created an atomic-safe version because they never expect the function to be \ncalled in an atomic context), they should use a declaration of the form: external foo : type = \"c_foo1 \nraise_on_atomic\" If foo is ever called inside an atomic block, a Sys_error exception will be raised with \nan argument indicating the name of the function that was improperly called. Fundamentally, there is only \none mechanism here: external foo : type_of_foo = \"c_foo\" is the same as external foo : type_of_foo = \n\"c_foo c_foo\" and is in fact implemented that way. The third option is the same as the second if raise_on_atomic \nis just a C function provided by the run-time system. It could be if we did not (for convenience) provide \nthe function name in the value that Sys_error carries. When a block rolls back, its side effects must \nbe reversed. So to create atomic versions of C functions, we let programmers specify actions that must \noccur when the atomic block rolls back. We also let them specify actions that must occur when the atomic \nblock successfully completes. The latter facility lets unreversible actions be delayed until it is known \nthat reversal is unnecessary. AtomCaml provides two functions to specify these actions: The caml_register_rollback_action(void(*reg_func) \n(void*), void* reg_env) function causes the function reg_func to be called with argument reg_env when \nthe cur\u00adrently executing atomic block rolls back.  The caml_register_commit_action(void (*reg_func) \n(void*), void* reg_env) function causes the function reg_func to be called with argument reg_env when \nthe cur\u00adrently executing atomic block completes.  The virtual machine thread scheduler will not pre-empt \nthreads in the middle of an external C function, so we do not need to worry about being interrupted between \nexecuting an action and registering the corresponding rollback action. Figure 3 shows how to use these \nfunctions to create atomic-safe versions of C functions that increment and delete heap-allocated counters. \nThe rollback action undoes any increments and the commit action completes any delete (which was delayed \nduring the atomic block). This interface suf.ces for implementing atomic-safe buffered output. The .rst \noutput can register an action that reverts the buffer to its original state if the atomic block rolls \nback. In addition, the atomic-safe version of the .ush function can register the .ush as an action to \nbe taken when the atomic block successfully completes.  4.4 Language Design Choices Several interesting \nlanguage-design questions arose during our work. We now consider the two most interesting: Should we \nal\u00adlow input inside atomic blocks? What do exceptions thrown from atomic blocks mean? Given our implementation \ns rollback-and-retry mechanism (see Sections 4.1 and 5), it is not dif.cult to allow input within atomic \nblocks, but for semantic and ef.ciency reasons (and the lack of a compelling need) we have made the policy \ndecision not to. To allow input, each input statement in an atomic block would simply read the next item \nfrom the buffer. If we rolled back, the items read would be put back into the buffer. Implementing this \napproach, however, would force all input functions (even the non-atomic ones) to see if input is already \navailable in a buffer because of another thread s rollback. Furthermore, we cannot statically bound the \nsize of the buffer, because we cannot predict how much data may need to be put back into it due to a \nrollback. (For output, the atomic functions buffers cannot be bounded, because we cannot .ush them until \nwe complete the block. However, the non-atomic functions buffers can be bounded, because we can .ush \nthem whenever. This asymmetry between input and output surprised us.) There are also semantic problems \nwith allowing input after out\u00adput in an atomic block: The input cannot depend on the output be\u00adcause \nwe delay the output until the atomic block completes. For ex\u00adample, the output could prompt a user for \ninformation and the input could be their response. Our atomic primitive is for shared-memory concurrency; \nit is unwise (and impossible, given our current choice to raise an exception if an input function is \ncalled inside an atomic block) to perform external communication atomically. Turning to exceptions, the \ninteresting case is when an exception thrown from an atomic block is not caught in the atomic block. \n struct Counter { void do_registration(counter c) { int val; // current value if(c->did_registration) \nreturn; int did_registration; // boolean c->did_registration = 1; int old_val; // pre-atomic value c->old_val \n= c->val; int pending_delete; // boolean caml_register_commit_action(commit_ctr,c); }; caml_register_rollback_action(rollback_ctr,c); \n} typedef struct Counter * counter; void commit_ctr(void *v) { void rollback_ctr(void *v) { counter \nc = (counter) v; counter c = (counter) v; if(c->pending_delete) c->val = c->old_val; delete_ctr(Val_int((int)c)); \nc->did_registration = 0; else c->pending_delete = 0; c->did_registration = 0; } } value inc_ctr_atomic(value \nv) { value delete_ctr_atomic(value v) { counter c = (counter) Int_val(v); counter c = (counter) Int_val(v); \ndo_registration(c); do_registration(c); return inc_ctr(v); c->pending_delete = 1; } return Val_unit; \n} Figure 3. An atomic-safe version of C code that increments and deletes heap-allocated counters. The \nfunctions caml_register_rollback_action and caml_register_commit_action register functions that are called \nif the currently exe\u00adcuting atomic block rolls back or completes, respectively. The code undoes increments \non rollbacks and delays deletes until an atomic block completes. new_ctr, inc_ctr, and delete_ctr are \nstraightforward and not shown. In our view, an exception is just a nonlocal control transfer and when \ncontrol transfers outside the atomic block, the atomic block commits successfully. This policy is semantically \nsimple and aligns with our view that atomic is nothing more and nothing less than a concurrency primitive. \nAnother possibility is to roll back an atomic block when an ex\u00adception occurs, which is tempting because \nexceptions often indi\u00adcate unexpected conditions. This is a fundamentally different ex\u00adception semantics: \nRather than just transfer control, an exception raise would also revert to a previous state. While such \nan operator has its advocates [26, 34], we consider it an orthogonal language feature that happens to \nenjoy a very similar implementation. From the perspective of atomicity, yield and yield_r allow a thread \nto abort an atomic block and retry it later. Quite differently, this variant of exception would abort \nan atomic block and not retry it (continuing as though the atomic block were empty). This feature has \nits own pitfalls, however. For example in AtomCaml this code will never fail, because the write to x \nwill not be undone even if f raises an exception: letx=ref 0 atomic (fun () -> x := 1; f() (* f may raise \nan exception *)) if !x = 0 then failwith \"huh\" else ... If exceptions had a rollback-and-do-not-retry \nsemantics, an excep\u00adtion thrown in f would cancel the write to x. The code would then fail because x \nwould be 0 when we reached the conditional.  5. The Implementation of AtomCaml We now discuss our strategy \nfor implementing the design described in the previous section. As described earlier, we implemented AtomCaml \nas an extension to bytecode-compiled Objective Caml. Section 5.1 describes our basic approach to guaranteeing \natomicity. Section 5.2 discusses why Objective Caml and other mostly func\u00adtional languages are a good \ntarget for this approach. Section 5.3 presents some details of our implementation. Finally, Section 5.4 \ndiscusses key implementation choices. 5.1 The Approach Atomicity via Logging and Rollback The ease and \ncorrectness of atomic is well-known; it is why operating system code disables interrupts for short code \nsequences. The problem with making atomic a language construct in a safe language is that we do not trust \ncode to re-enable interrupts, nor do we want draconian restrictions like disallowing function calls in \natomic blocks. We show how to provide particularly ef.cient support for atomicity on systems without \ntrue parallelism (e.g., uniprocessor systems and systems like Objective Caml where the run-time system \nrequires that at most one thread executes at any given time), without disabling interrupts or restricting \ncode. We designed our approach based on the belief that most atomic blocks will be short, and that most \ncode will execute outside of atomic blocks. The applications described in Section 6 con.rm these observations. \nThus we have designed a system where short atomic blocks execute with low overhead and where non-atomic \ncode is not slowed down compared to systems without atomicity. The key idea is this: We know exactly \none thread is executing at any time. When the currently executing thread reaches an atomic block, it \noptimistically begins executing the block. If the thread completes the block without being pre-empted \nby the scheduler, no further action is required the block executed atomically. Because atomic blocks \nare typically short, this case is the common one. If the scheduler does pre-empt a thread during an atomic \nblock, the thread rolls back to the beginning of the block and removes any evidence that the block was \npartially executed. The next time the thread executes, it will restart at the beginning of the block. \nGiven this general approach, several questions remain: In a language with side effects, how can we ensure \nthat the program behaves as if a rolled back block never executed? In particular, how do we handle writes \nto mutable data and output?  How do we ensure that an atomic block eventually completes successfully, \nwithout starving other threads?  How do we handle functions that may be used inside both atomic and \nnon-atomic contexts? Their behavior must be dif\u00adferent in these two cases, but we do not want to slow \ndown non-atomic code.  How do we ensure that our approach has low overhead?  The rest of this section \nanswers these questions in turn. To enable rollbacks, side effects (memory writes, output, mes\u00adsage sends, \netc.) in atomic blocks must be reversible. The essential uniprocessor optimization is that our system \nhas no need to log reads to memory. In atomic blocks, the system tracks updates to mutable locations \nwith a log that records the location and the pre\u00advious value. If rollback becomes necessary, it simply \nreverts every location in the log to its original value. The only exceptions to this rule are variables \nand memory that are local to the atomic block they do not need to be logged or reversed because the thread \nwill recreate them when it retries the block. In a garbage collected lan\u00adguage, the memory occupied by \nthese local variables will be re\u00adclaimed because there will be no references to them after the roll\u00adback. \nWe handle output and message sends by buffering, and by not allowing the buffers to .ush in the middle \nof an atomic block. Rollbacks can then remove the writes and sends from the appro\u00adpriate buffers.4 Input \nduring atomic blocks is an interesting policy question, discussed in Section 4.4. We also must ensure \nthat an atomic block will eventually be able to complete (provided, of course, that it does not enter \nan in.nite loop). To achieve this, the scheduler allocates extra time to a thread s next execution if \nit fails to complete an atomic block after two attempts. (Note that the block will have been given a \nfull time slice on the second attempt.) If the block still fails to complete, we can allocate even more \ntime on subsequent executions. This extra time may not be necessary the atomic block may complete faster \nbecause of work done by other threads. However, by allocating more time, the scheduler ensures that inherently \nlong atomic blocks will eventually be able to complete. To ensure fair scheduling (i.e., that we do not \nstarve other threads), the scheduler skips the thread for a number of rounds proportional to the extra \ntime allocated. The scheduler can also enforce an upper bound on the allocated time per thread execution, \nin order to prevent unresponsiveness. These policy issues may depend on an application and should be \ntunable just like garbage-collection parameters. To allow function calls inside atomic blocks, the system \nmust also log writes in functions called from inside an atomic block. We could test at function entry \nwhether the current thread is in an atomic context, but this test would slow down non-atomic code. Instead, \nthe compiler generates two versions of each function: an atomic version that logs side effects and buffers \noutput, and a non\u00adatomic version that executes normally. We can determine statically which version to \ncall: when we call the argument to an atomic, or when we make a call inside the atomic version of a function, \nwe call the atomic version. Otherwise, we call the non-atomic version. In a functional language, this \nrequires adding a second function pointer to each closure,5 and creating new versions of the apply bytecodes \nthat follow the new code pointers. We can also reduce the code size somewhat by statically identifying \nfunctions that are called only in atomic contexts, or only in non-atomic contexts, and generating only \nthe appropriate versions. We can also avoid duplicating purely functional code, because there will be \nno writes to log. Because non-atomic code is essentially unchanged and roll\u00adbacks are rare, the primary \nsource of overhead is logging writes. Logging occurs only inside atomic blocks (which are typically only \n4 Removing writes and sends from buffers is really just a special case of reversing memory writes. 5 \nWe can save this extra per-closure space, but at the expense of increasing the overhead of atomic function \ncalls. See Section 5.4. a small fraction of the code), and writes must already be tracked by the (generational) \ngarbage collector. Thus this overhead has little effect on the overall running time of the applications \ndiscussed in Section 6. Even if an atomic block has trouble completing (thus suf\u00adfering multiple rollbacks), \nonly the thread executing it slows down. This is in contrast to locking systems, where other threads \nwait for locks held by a thread executing a problematic critical section. Should it prove necessary, \nwe could investigate dynamic and static approaches to reduce rollback frequency. For example, a thread \ncan yield rather than start an atomic block near the end of its time slice (when it is less likely the \nblock will complete). The compiler can also use static analysis to shrink atomic blocks with right-and \nleft-movers at, respectively, the beginning and end of the block (see [13, 14, 23] for information about \nmovers).  5.2 Why Objective Caml? We chose Objective Caml as a starting point for our prototype for \nseveral reasons. First, as a mostly functional language, writes to mutable data structures in Caml are \nrelatively rare. Since these writes are our primary source of overhead,6 we expect that logs will remain \nquite small and our experience con.rms it. Second, Objec\u00adtive Caml already lacks support for true parallelism, \nso we exploit the same simpli.cations that the existing run-time system already does. Third, we can make \natomicity a fully .rst-class construct, i.e., a function of type (unit-> a)-> a. Without higher-order \nfunc\u00adtions this is clearly impossible. Moreover, in a purely functional language like Haskell, atomic \nis useful only as a monad [19]. Making atomic a function helps enormously in localizing the changes necessary \nto the implementation. We simply add the dec\u00adlaration external atomic: (unit-> a)-> a = \"atomic\" to the \nThread module, and the front end correctly parses and typechecks uses of atomic. In short, we changed \nnothing in the front end. Making atomic a function also simpli.es nested atomic blocks. As described \nin Section 4.1, nested atomics are redundant; such a call should just apply its argument because there \nis already a log set up. However, a single atomic construct may be used in both nested and non-nested \ncontexts. Treating atomic as a function solves this problem. We already have two versions of every function, \nand we already determine which one to call based on the context. The outermost atomic will always be \ncalled from a non-atomic context (otherwise it would not be the outermost), and nested atomics will always \nbe called from atomic contexts (otherwise they would not be nested). Thus the non-atomic version of atomic \ndoes the setup for logging and rollback, and calls the atomic version of the passed function, and the \natomic version is simply:7 let atomic th = th () Since we are already in an atomic context, we automatically \ncall the atomic version of th. We chose to change the bytecode compiler rather than the native compiler \nfor three reasons: (1) simplicity (it is a smaller system), (2) portability, and (3) a user-level thread \nscheduler (so we did not have to deal with the kernel). The disadvantage is that bytecode typically runs \nslower than native code, making our performance results less reliable. Our approach should work .ne with \nnative code and a system thread scheduler provided the latter provides 6 Recall that initialization writes \ndo not need to be logged, because we will lose all references to the data after a rollback. 7 As we discuss \nin Section 5.3.3, atomic is actually a C function built in to the virtual machine. However, the behavior \nof the atomic version of atomic is identical to the Caml code given here.  Figure 4. This .gure illustrates \nthe effect of rolling back an atomic block that modi.es a mutable variable x twice. The reference x initially \nholds the value 7338. The .rst update pushes an entry onto the stack containing the address of x and \nit s former value (7338). The second update pushes an entry containing x s address and its value after \nthe .rst update (6337). On rollback, we .rst pop the stack and reset x to 6337. We then pop the next \nelement and reset x back to its original value: 7338. a hook for executing code when a (lightweight, \ni.e., intraprocess) thread is pre-empted.  5.3 Implementation Details We implemented AtomCaml by modifying \nthe Objective Caml bytecode compiler, the run-time system (particularly the thread scheduler and the \ngarbage collector), and the Thread library. As mentioned earlier, we did not touch the front end of the \ncompiler. This section describes (at a high level) the modi.cations we made to implement AtomCaml. None \nof these modi.cations rely on any unusual properties of Caml or the Objective Caml vir\u00adtual machine. \nThus similar extensions should be possible in other mostly functional languages. 5.3.1 Logging and Rollback \nAs Section 5.1 describes, AtomCaml logs all non-initialization writes occurring inside atomic blocks \nand reverses them if the block is rolled back. To do so, the AtomCaml compiler uses alternate versions \nof bytecodes and primitives that might modify existing data. These alternate forms cause the virtual \nmachine to log the modi.cation. This approach is more ef.cient than setting a .ag when we enter an atomic \nblock and checking the .ag on every write, because the check would slow down non-atomic code. The log \nis a stack of modi.ed addresses and their previous values. Thus when we roll back, the .rst writes are \nthe last reversed. As Figure 4 illustrates, this ensures all locations revert to the value they held \nbefore the atomic block. If the log becomes large, we switch to a hashing scheme to avoid logging multiple \nwrites to the same address (see Section 5.4). We must also consider how logging and rollback interact \nwith garbage collection: If we lose the last reference to an object during an atomic block, we still \nmust not garbage collect it. Otherwise, after a rollback the program could have a reference to an object \nthat no longer exists. We must also ensure that the garbage collector updates the log if it moves any \nlogged data structures. Thus the log must be reachable to the garbage collector. 5.3.2 Code Duplication \nAtomCaml creates two copies of each function: an atomic version that logs writes and buffers output, \nand a non-atomic version that does not. Thus our compiled code size is roughly twice that of an equivalent \nObjective Caml program. The compiler indicates which function should be called by inserting different \napply bytecodes in atomic and non-atomic contexts. Since atomic is simply a function consist of the atomic \nfunction itself, and the atomic versions of all other functions. Thus, to compile AtomCaml code, the \nbytecode compiler merely needs to compile each function twice. The .rst compile generates the non-atomic \nversion of the function, and pro\u00adceeds exactly as a normal Objective Caml compilation would. The second \ncompile generates the atomic version, and is identical to the .rst except that instructions that call \nfunctions or modify mutable data are replaced with their alternate, atomic forms. Calls inside atomic \nblocks will then automatically call the atomic version. The virtual machine keeps track of these two \nfunction versions by adding an extra code pointer to every closure. This is depicted in Figure 5b. The \nnormal apply bytecodes follow the original code pointer, and the atomic apply bytecodes follow the new \ncode pointer. In Section 5.4, we discuss an alternate representation that eliminates this extra per-closure \nspace at the expense of slower function calls inside atomic blocks. 5.3.3 The atomic Function The atomic \nfunction lets the programmer pass in code that should be evaluated atomically. We implemented atomic \nas a C function that is built-in to the virtual machine runtime. It must be a part of the runtime because \nit is the one place where we transition from a non-atomic context to an atomic context. The compiler \nneed not (and in general does not) know if a function-call target is atomic. The version of atomic called \nfrom non-atomic contexts must perform the setup work necessary to enter an atomic section, call the argument \nfunction, and rollback the block if it failed to com\u00adplete. The function .rst sets up the modi.ed-memory \nlog. It then calls a new version of the Objective Caml callback function that fol\u00adlows the atomic code \npointer of the passed function. If the thread scheduler interrupts an atomic block,8 it throws a special \nrollback exception. If the callback returns this exception, atomic rolls back the modi.cations in the \nlog and executes any registered rollback actions (see Section 4.3). Otherwise, we empty the log and execute \nany registered commit actions. The version of atomic called from atomic contexts simply calls the passed \nfunction. As described in Section 4.1, nested atomics are redundant, so this is suf.cient.  5.4 Design \nChoices In this section, we consider key choices we made in the AtomCaml implementation. We .rst discuss \nthe representation of function 8 Objective Caml experts may recall that the virtual machine normally \npre\u00advents pre-emption during callbacks. AtomCaml allows pre-emption during that calls the atomic version \nof its argument, the atomic contexts callbacks, but only if all active callbacks are from the atomic \nfunction. Figure 5. a) Objective Caml function closures. b) AtomCaml closures with two code pointers. \nc) AtomCaml closures with a single code pointer. The atomic code pointer is placed at a .xed offset from \nthe non-atomic code. closures and an alternate representation (Section 5.4.1). We then describe some \noptimizations to the logging mechanism that we chose to implement, and the associated tradeoffs (Section \n5.4.2). 5.4.1 Closure Representations Function closures consist of a code pointer and an environment \ncontaining the function s free variables (see Figure 5a). The most straightforward extension adds a second \ncode pointer, as depicted in Figure 5b. This representation has the advantage of not adding any extra \nlevels of indirection for function calls. However, the closures become larger, so closure creation is \nmore expensive, even in non-atomic code. Alternately, we could place the atomic-code pointer at a .xed \noffset from the start of the non-atomic code. We would then need only one code pointer in the function \nclosure, as depicted in Fig\u00adure 5c. This representation avoids enlarging the closures, thus clo\u00adsure \ncreation is not slowed down. However, atomic functions calls will now have to follow two code pointers. \nThus non-atomic code is sped up (due to cheaper closure creation), at the expense of slowing down function \ncalls in atomic blocks. As Section 6.5 shows, our test applications that use atomic run slightly faster \n(or at the same speed) with the .rst representation; thus we chose it for our implementation. Objective \nCaml also uses special .attened representations for closures containing mutually recursive functions. \nBoth our ap\u00adproaches extend naturally to such closures. 5.4.2 Optimizing Logging and Rollback Generally, \nwe want to reduce the work done for each atomic write, even if it means more expensive rollbacks, because \nwe expect rollbacks to be rare. However, there are some cases where doing a little extra work when we \nlog a write can save lots of time on a rollback. In particular, if we update the same location many times, \nwe can save time on rollback by only logging the .rst update. However, checking for repeats has overhead; \nthus it only makes sense for atomic blocks that have an above-average chance of being rolled back, and \nthat perform enough repeated writes that they stand to gain by eliminating duplicates. In our current \nimplementation, we dynamically track the total number of writes to mutable data in a block to approximate \nboth quantities. A block with a large number of writes has a higher chance of repeated writes, and will \nlikely take longer to complete and thus have a higher chance of being rolled back. Thus, once the number \nof writes reaches our preset threshold (currently 50), we begin hashing newly logged addresses to check \nfor duplicates. We also begin incrementally hashing the previously logged addresses to check for any \nduplicates that have already been inserted. Another option we plan to try in the future would attempt \nto identify repeated writes statically. For instance, a static analysis could determine that the write \nto big_number in the following loop needs to be logged only once: atomic (fun () -> while (!big_number \n> 0) do big_number := !big_number -1; ... done)   6. Experience To understand the convenience and ef.ciency \nof AtomCaml, we have written or modi.ed multithreaded libraries and applications. Section 6.1 describes \ncommon idioms we encountered in existing code and external C libraries and how these idioms appear when \nusing atomic. Section 6.2 investigates the performance implica\u00adtions of our implementation on contrived \nmicrobenchmarks and non-atomic applications. Sections 6.3 and 6.4 describe more com\u00adplete case studies \ninvolving one new and one existing application. Section 6.5 compares closure representations. 6.1 Common \nIdioms with Atomic Usually, programming with atomic is much easier than program\u00adming with locks. With \nlocks, one typically uses patterns like the following (which is a much-used utility function copied verbatim \nfrom PLANet [22, 21]): let critical m thunk = try Mutex.lock m; let result = thunk() in Mutex.unlock \nm; result with e -> (Mutex.unlock m; raise e) It is often but not always meaning-preserving to replace \nthis func\u00adtion with: let critical m thunk = atomic thunk In many cases, a change like this one suf.ces. \nFor example, the Objective Caml standard library provides an implementation of the Concurrent ML primitives \n[28]. The implementation keeps private data structures consistent with a master lock and atomic works \njust as well. It took only a few minutes to change the library, which we had never seen before. Similarly, \ncode that wraps libraries such as hashtable implementations with a lock to implement a monitor-style \nabstraction is ideally suited for atomic. In one place in PLANet described below, making the simple change \nto atomic made a library more useful by avoiding a potential deadlock. However, occasionally using atomic \nwhere current practice would use locks is incorrect or a bad idea. We consider three such scenarios, \ntwo of which we encountered during our case studies. 6.1.1 Condition Variables Threads often communicate \nvia condition variables, which have an associated lock and support the wait, signal, and broadcast operations9 \n(see the Condition library in Objective Caml). A thread calling wait should hold the associated lock. \nwait then releases the lock and suspends the thread. Upon resumption, wait reacquires the lock before \nreturning. signal resumes one thread waiting on the condition variable and broadcast resumes all of them. \nTo our knowledge, prior work on atomic has not considered using this important idiom. Code using wait \ngenerally has this form, where lk is a lock and cv is a condition variable: critical lk (fun () -> e1; \nlet rec loop () = if e2 then e5 else (e3; wait cv lk; e4; loop ()) in loop ()) It is crucial that the \ncall to wait suspends the thread so that another thread may modify shared state such that e2 becomes \ntrue. But rolling back (as the atomic implementation of critical would do upon suspension) is incorrect \nif other threads need the effects of e1, e2, e3, or e4 to make progress. (If and only if these four expressions \nare pure does a conditional critical region [18] or yield_r suf.ce.) Although there are often simpler \nsolutions (it is rare that this pattern is used in its full generality), a general solution is almost \nthe following:10 let f() = if e2 then Some e5 else (e3; None) in let rec loop x = match x with None -> \n(wait cv; loop (atomic (fun () -> e4; f()))) | Somey->yin loop (atomic (fun () -> e1; f())) As needed, \nthis code evaluates the correct expressions atomically and it does not call wait (which we suppose takes \na condition variable but not a lock) from within atomic. Unfortunately it has a race condition: Between \nthe evaluation of e2 and wait cv, another thread could make e2 true and signal cv; the waiting thread \nwill never see a signal that precedes the wait. In the lock-based code, this race does not exist because \nthe waiting thread holds the 9 These operations are sometimes called wait, notify, and notifyAll. 10 \nNote it is also easy to abstract the pattern with a higher-order function taking thunks that evaluate \ne1, e2, e3, e4, and e5. condition variable s lock until it suspends and a signaling thread must hold \nthe lock. To solve the problem, the waiting thread must start listening for a signal inside atomic (with \nlisten) but suspend itself (with wait) outside atomic. The following interface and revised code suf.ces: \ntype condvar type channel val create : unit -> condvar (*signal a channel that hasn t yet been signaled*) \nval signal : condvar -> unit val broadcast : condvar -> unit val listen : condvar -> channel (*suspends \nunless/until channel is signaled*) val wait : channel -> unit type a attempt = Wait of channel | Go of \na let f() = if e2 then Go e5 else (e3; Wait (listen cv) in let rec loop x = match x with Wait ch -> (wait \nch; loop (atomic (fun () -> e4; f()))) | Goy->yin loop (atomic (fun () -> e1; f())) In an atomic section, \na thread can start listening for a signal, so a signal cannot be missed. The call to listen returns a \nchannel on which signals occur; the wait operation suspends unless the channel has been signaled. If \nanother thread quickly signals the channel, then the wait operation will simply not suspend. Our implementation \nof condition variables is about 20 lines of AtomCaml; Appendix A contains its entirety. Note our use \nof atomic above leads to nested atomic evaluations, but these pose no problem. 6.1.2 External Calls \n(e.g., I/O) If Caml code calls C code while holding a lock, then before replac\u00ading the lock acquisition/release \nwith atomic, we must do one of the following to ensure that we will be able to safely roll back the block: \n(1) modify the Caml code, (2) manually verify the C code (at least as it is being called) is safe for \natomic, (3) write an atomic ver\u00adsion of the C code using the API described in Section 4.3. Which approach \nis easiest depends on the situation. Modifying the Caml code is often not dif.cult. For example, this \nlock-based code closes or writes to a shared output channel held in a shared variable f that is guarded \nby lk: fun close () = critical lk (fun () -> match !f with None -> () | Some oc -> close_out oc; f := \nNone) fun output () = critical lk (fun () -> match !f with None -> () | Some oc -> output_string \"ICFP\") \nWe have modi.ed the C code implementing close_out and output_string to have no effect until an atomic \nblock com\u00adpletes. But suppose it was too dif.cult to provide this functionality for close_out, so we \nhad it raise an exception instead. Then the programmer can still just write: fun close () = let th = \natomic (fun () -> match !f with None -> (fun () -> ()) | Some oc -> (f:=None; (fun () -> close_out oc)) \nin th() fun output () = atomic (fun () -> match !f with None -> () | Some oc -> output_string \"ICFP\") \nOther times, we may know an operation is pure so we can move it outside of an atomic block. For example, \nthe Objective Caml library uses C code for parsing, but parsing a constant (and well-formed) string is \nnonetheless a pure operation at the application level. Determining if the C code is actually safe for \natomic is usually easy: Getter functions (e.g., pos_out, which returns an output\u00adchannel s current position) \nare typically safe and other functions typically are not. In the latter case, it may be possible to delay \neffects until an atomic block commits. 6.1.3 Long Critical Sections We have yet to .nd a situation where \nit seemed appropriate to hold a lock while performing a long computation but inappropriate to perform \nthe computation atomically. This could happen if the com\u00adputation was long enough to force the atomic \nblock to roll back repeatedly. In this situation, it would be better to use locks or an idiom simulating \nthem. We can have our cake and eat it too be\u00adcause our implementation of atomic is compatible with conven\u00adtional \nlock implementations such as Objective Caml s Mutex li\u00adbrary. Moreover, as a simple exercise we have \nwritten a library on top of atomic that provides simple locks, locks that check the re\u00adleasing thread \nis the acquiring thread, reentrant locks, and read\u00aders/writer locks. The simple lock implementation is \ntrivial: type simple_lock = bool ref let acquire lk = atomic (fun () -> if !lk then yield_r lk else lk \n:= true) let release lk = atomic (fun () -> lk := false)  6.2 Simple Benchmarks We created several small \ntests to evaluate the performance of var\u00adious aspects of AtomCaml. The tests were run on a 500MB, 2.26 \nGHz Pentium 4. Sections 6.3 and 6.4 present the overhead of two real applications. As described in Section \n5, our primary source of overhead in non-atomic code is closure creation. We measured this cost with \na loop that creates 100,000,000 functions. AtomCaml required 2.945 sec. to execute this test and Objective \nCaml required 2.457 (a 19.9% overhead).11 To see how this carries over to real applications, we compiled \nthe AtomCaml compiler (which contains no atomic blocks) with both AtomCaml and Objective Caml, and compared \nthe resulting compilers on two large Caml source .les from the AtomCaml distribution: parser.ml and ctype.ml. \nThe AtomCaml\u00adcompiled compiler took 1.6% longer to compile parser.ml (0.594 vs. 0.584 sec.) and 2.7% \nlonger to compile ctype.ml (1.015 vs. 0.988 sec.). We also compiled an interpreter for the language de\u00adscribed \nin [29] with both AtomCaml and Objective Caml. The AtomCaml version took 1.3% longer to run our largest \ntest (3.483 vs. 3.440 sec.). Thus for real non-atomic code, AtomCaml adds about a 2% overhead.12 These \nresults are summarized in Figure 6a. 11 Neither version experienced any major garbage collections. The \nnumber of minor collections was directly proportional to the size of the closures: 9155 for AtomCaml, \nand 6103 for Objective Caml. 12 There are situations where closure-creation overhead is more signi.cant. \nIn particular, a large application with many top-level functions creates many closures when the application \nstarts. If the application does little additional We also created three microbenchmarks to measure the \ncost of logging and rollback. Each has a loop that iterates 100,000 times. The .rst microbenchmark s \nloop body executes a series of writes (decrements of an int ref) followed by an empty atomic block. The \nsecond has the same writes inside the atomic block instead of outside; thus the difference with the .rst \nis the cost of logging. The third is like the second except the atomic block rolls back instead of completing;13 \nthus the difference with the second is the cost of rollback (slightly underestimated since the enclosing \nloop body never completes). We ran each benchmark with 0, 10, 50, and 100 writes of the same top-level \nreference. Figure 6b summarizes the results; the running time for the .rst benchmark is execution , the \nsecond execution plus logging , and the third execution plus logging plus rollback . Rollback was actually \nfaster with 100 writes than with 50 because our duplicate\u00adelimination logging optimization removed redundant \nwrites from the log. Appendix B contains the data for this graph. Finally, we compared the cost of executing \nan atomic block and executing the same code surrounded by a lock acquire and release. Our blocks consisted \nof a loop (implemented as a recursive function) that executes a variable number of writes. Our results \nare summarized in Figure 6c. With no writes, the atomic block took 2.15 times longer (0.118 sec. vs. \n0.055 sec. for 100,000 executions). With 5 writes, the atomic block took 2.08 times longer (0.156 vs. \n0.075 sec.), and with 10 writes, it took 1.82 times longer (0.182 vs. 0.100 sec.). The longer atomic \nblocks experienced less relative overhead (despite having to do more write-logging) because there are \n.xed costs associated with atomic entry and exit. For instance, we must allocate the log, perform a callback \nof the passed in function, and then check if a rollback is necessary. We evaluated the source of this \noverhead with another set of benchmarks that add callbacks to the lock acquire/release benchmarks described \nabove. We determined that the callback accounts for 23.3% of the overhead in the 0 write case, 19.8% \nin the 5 write case, and 17.7% in the 10 write case.  6.3 Small Application: Web Cache We also wrote \na simple web cache application in AtomCaml. The application is initially given a page to cache. It searches \nthat page for links and embedded objects (e.g., images), and caches them as well. If any of these objects \nare themselves HTML pages, we search them in turn. For simplicity, and for more consistent performance \nresults, our application only works with locally stored web pages. We implemented our web cache with \na simple thread-safe bounded buffer library we wrote in AtomCaml. We create a bounded buffer for each \nHTML page and a producer thread to scan the page and insert any linked or embedded objects into the buffer. \nWe also create a consumer thread to remove the objects from the buffer and cache them. If the object \nis an HTML page, the consumer creates a new bounded buffer and the two threads for it. Thus two threads \nshare each bounded buffer, and every consumer thread shares the cache. Figure 7 depicts this architecture. \nOur bounded buffer insert and remove functions are shown in Figure 2. Using atomic in these functions \nensures no thread ever sees a bounded buffer with a partially-inserted or partially-removed object. Using \nyield_r lets us rollback the atomic block if we attempt to insert into a full (or remove from an empty) \nbuffer, and to put the thread to sleep until the buffer is no longer full (or empty). work, the start-up \ncost can dominate. For situations like these, it may be better to use the alternative closure representation \ndescribed in Section 5.4. 13We use a call to yield at the end of the block to roll back. For a fair comparison, \nwe included a call to yield immediately after the atomic block in the other two microbenchmarks. AtomCaml \nObjective Caml Test Min Max Avg Min Max Avg Overhead Compile ctype.ml 1.007 sec 1.021 sec 1.015 sec 0.975 \nsec 0.996 sec 0.988 sec 2.7% Compile parser.ml 0.591 sec 0.597 sec 0.594 sec 0.578 sec 0.592 sec 0.584 \nsec 1.6% CDS Interpreter 3.466 sec 3.502 sec 3.483 sec 3.432 sec 3.456 sec 3.440 sec 1.3% Web Cache 7.582 \nsec 7.758 sec 7.689 sec 7.563 sec 7.787 sec 7.630 sec 0.8% PLANet ping 1.062 ms 1.709 ms 1.198 ms 1.024 \nms 1.398 ms 1.150 ms 4.2% send (in sec/MB) 0.0657 0.0683 0.0667 0.0710 0.0754 0.0736 ( 9.4%) receive \n(sec/MB) 0.0868 0.0925 0.0898 0.0820 0.0857 0.0839 7.0% (a) (c)  Block Lock acquire and release Atomic \nblock Overhead 0 Writes 0.055 sec 0.118 sec 115% 5 Writes 0.075 sec 0.156 sec 108% 10 Writes 0.100 sec \n0.182 sec 82% Figure 6. Results of tests evaluating AtomCaml s performance: Table (a) compares the performance \nof programs compiled with Objective Caml and with AtomCaml. Table (b) measures the overhead of logging \nand rollback for atomic blocks with 0, 10, 50, and 100 writes. Table (c) compares the execution time \nfor a short atomic block with the execution time for the same block plus a lock acquire and release. \nWe implemented the same application in Objective Caml using locks and ran both versions with the .rst \nauthor s homepage as input. The web cache has 177 lines of code and three atomic blocks. The AtomCaml \nversion took 7.689 seconds on average to .ll a 100MB cache. The Objective Caml version took 7.630 seconds. \nThus our overhead for this I/O-bound application was just 0.8%.  6.4 Large Application: PLANet PLANet \n[22] is a prototype active network system that allows arbitrary network topologies, extensible routers \nusing a domain\u00adspeci.c language for the extensions [21], and many other features. Separate threads interpret \nnetwork packets, perform router updates, generate performance statistics, etc. PLANet was last used with \nversion 2.2 of Objective Caml, so we .rst made minor changes so the code would run with version 3.08.1 \n(the version from which AtomCaml is derived).14 We then replaced all lock-based synchro\u00adnization with \nuses of atomic, though of course we could have left some uses of locks had we preferred. Although we \nmodi.ed all the code, we should note that the experiments we ran exercise some but not all of the system \ns synchronization. Most synchronization used the critical function described in Section 6.1 and was therefore \ntrivial to switch to atomic. Condition variables were used in several places; in all of them, the transforma\u00adtion \ndescribed earlier suf.ced. One place used Caml s Concurrent ML library; we changed this library not to \nuse locks and the actual PLANet code needed no change. The PLANet code also imple\u00admented readers/writer \nlocks on top of regular locks and used them to mediate access to a hashtable. We removed the readers/writer \nlocks and wrapped the hashtable operations in atomic blocks. At this point, we had .ve unsafe calls to \nC functions from within atomic blocks. We deemed one pure (so we moved it to before the atomic block), \none delayable (so we moved it to after the atomic 14 The most interesting change had nothing to do with \nchanges to Caml: as of January 10, 2004, the number of seconds since January 1, 1970 no longer .ts in \na signed 31-bit integer. block), and three requiring atomic-safe versions of the C code (which we implemented). \nTwo of the three functions performed output (which we buffered); the third killed a thread (which we \ndelayed until the block commits). The atomic-safe versions use the rollback and commit callbacks described \nin Section 4.3. In examining the code base, we also discovered three concur\u00adrency bugs. First, a clock \nmodule for performing periodic events would deadlock if one callback attempted to register another call\u00adback. \nNone of the PLANet code would do this, but the module s in\u00adterface suggests it is perfectly reasonable. \nSuch library-reentrancy bugs are probably quite common. Second, the readers/writer locks had a glaring \nbug: An inverted test would cause a write-lock ac\u00adquisition to block only if there were waiting readers \nand waiting writers. Third, the same library did not always allow simultaneous readers: When releasing \na write-lock when no writers were wait\u00ading, the library would signal only one waiting reader rather than \nbroadcasting to all of them. (So if one reader does not terminate, subsequent waiting readers will starve, \nwhich is not the desired be\u00adhavior.) In our opinion, PLANet is well-written code; it is just very dif.cult \nto write and test concurrent applications. Though it may be coincidence, our port to use atomic re\u00admoved \nall three bugs, and would have even if we had not noticed them: The deadlock was a thread waiting on \na lock it already held; this translates to a nested atomic block which is no problem. The readers-writer \nlocks are not used in the atomic version of PLANet. The resulting logging in the atomic version was small \neven though we did not make atomic blocks any smaller than the critical regions in the original code. \nWe instrumented the run-time system to record the size of the logs and never found more than 41 logged \nwrites in any atomic-block execution. As for performance, the PLANet publications already indicate that \nthe Objective Caml code s speed is typically lost in the noise: An active network with routers running \nin user space spends most of its time copying data from user-space to kernel-space and vice\u00adversa. Nonetheless, \nwe ran basic latency ( ping ) and throughput  Figure 7. The architecture of our simple web cache. A \nproducer thread scans an HTML page, inserting linked and embedded objects into a bounded buffer. A consumer \nthread removes objects from the buffer and caches them. If any object is an HTML page, a new producer, \nbounded buffer, and consumer are created. ( stream ) tests for a trivial two-node network (two machines, \neach with 2.8 GHz Pentium 4 processors and 1 GB of RAM). The latency test took on average 4.2% longer \nper self-routed ping in the AtomCaml-compiled version (1.198 ms vs. 1.150 ms). However, as we see in \nFigure 6, the variation between tests was much larger than this difference, so it is probably not particularly \nmeaningful. The throughput test measured the speed at which sender and receiver nodes process data. The \nAtomCaml compiled receiver node was 7.0% slower than the Objective Caml receiver, but the AtomCaml sender \nwas 9.4% faster. There are several reasons that could explain why the locks version of the code is actually \nslower. For example, a thread could be pre-empted while it holds a lock for some data structure that \nevery thread accesses. Or, the readers/writer lock bug that switching to atomic eliminated may have hurt \nperformance.  6.5 Alternate Closure Representation The results described above used the two-pointer \nclosure repre\u00adsentation depicted in Figure 5b. We also tested the single-pointer representation shown \nin Figure 5c. As expected, non-atomic code sped up slightly (0 2.5%): Compiling parser.ml and ctype.ml \ntook 0.586 and 1.012 seconds, respectively, and the interpreter test took 3.407 sec. Applications using \natomic were unchanged except for the ping test which slowed down by 7%: The web cache took 7.718 s, the \nping test took 1.278 ms, and the send and receive tests took 0.0670 and .0894 s/MB respectively.  7. \nConclusions and Future Work We have designed, implemented, and evaluated AtomCaml, an ex\u00adtension to bytecode-compiled \nObjective Caml that supports atomic critical sections. Our design adds two .rst-class primitives, atomic \nand yield_r, and provides support for calling external C functions within atomic blocks. The atomic function \ntakes a thunked block of code, evaluates it as if there was no interleaving of other threads, and returns \nthe result. The yield_r function suspends the current thread (triggering a rollback if it occurs inside \nan atomic block) and allows the scheduler to keep it suspended until the reference bound to its argument \nchanges. Our implementation uses logging and rollback to guarantee atomicity. Our approach is appropriate \nfor any system that, like Objective Caml, enforces a uniproces\u00adsor execution model (i.e., that does not \nallow multiple threads to execute simultaneously). We evaluated the ef.ciency and conve\u00adnience of AtomCaml \nby writing libraries, microbenchmarks, and a small application, and by porting a large multithreaded \napplication. The overhead for real applications was small, and replacing locks with atomicity removed \n(at least) three concurrency errors from the application we ported. The primary drawback to our current \nimplementation is the re\u00adquirement that threads not execute in true parallel. However, many language \nimplementations (e.g., Objective Caml and DrScheme) do not have true shared-memory parallelism and a \nfunctional program parallel and/or concurrent garbage collector. Even when multipro\u00adcessors become commonplace, \nwe believe many concurrent lan\u00adguages and applications will continue being run such that threads sharing \nmemory are interleaved but not truly parallel. Moreover, many desktop applications (e.g., editors) need \nconcurrency for re\u00adsponsiveness, but it is much less clear that they need the perfor\u00admance of parallel \ncomputing. In addition, we are extending our ideas to settings with true shared-memory parallelism. We \nhave begun preliminary work on our next project: AtomJava. AtomJava will bring the same strong atomicity \nguarantees present in AtomCaml to an object-oriented setting with support for true parallelism. Acknowledgments \nManuel F\u00a8 ahndrich and Shaz Qadeer provided helpful early dis\u00adcussions about our approach. Joao Dias, \nMatthew Fluet, Michael Hicks, and Leaf Petersen provided feedback on earlier drafts. Michael Hicks suggested \nwe look at PLANet.  References [1] AtomCAML. http://www.cs.washington.edu/homes/miker/atomcaml. [2] \nDavid Bacon, Robert Storm, and Ashis Tarafdar. Guava: A dialect of Java without data races. In ACM Conference \non Object-Oriented Programming, Systems, Languages, and Applications, pages 382 400, October 2000. [3] \nBrian N. Bershad, David D. Redell, and John R. Ellis. Fast mutual exclusion for uniprocessors. In 5th \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, pages \n223 233, 1992. [4] Chandrasekhar Boyapati, Robert Lee, and Martin Rinard. Ownership types for safe programming: \nPreventing data races and deadlocks. In ACM Conference on Object-Oriented Programming, Systems, Languages, \nand Applications, pages 211 230, November 2002. [5] Chandrasekhar Boyapati and Martin Rinard. A parameterized \ntype system for race-free Java programs. In ACM Conference on Object-Oriented Programming, Systems, Languages, \nand Applications, pages 56 59, October 2001. [6] Greg Bronevetsky, Daniel Marques, Keshav Pingali, Peter \nSzwed, and Martin Schulz. Application-level checkpointing for shared memory programs. In International \nConference on Architectural Support for Programming Languages and Operating Systems, pages 235 247, 2004. \n[7] Guang-Ien Cheng, Mingdong Feng, Charles E. Leiserson, Keith H. Randall, and Andrew F. Stark. Detecting \ndata races in Cilk programs that use locks. In ACM Symposium on Parallel Algorithms and Architectures, \npages 298 309, June 1998. [8] Jong-Deok Choi, Keunwoo Lee, Alexey Loginov, Robert O Callahan, Vivek Sarkar, \nand Manu Sridharan. Ef.cient and precise datarace detection for multithreaded object-oriented programs. \nIn ACM Conference on Programming Language Design and Implementation, needing such support would presumably \nalso need a state-of-the-art pages 258 269, June 2002. [9] Cormac Flanagan and Mart\u00b4in Abadi. Types for \nsafe locking. In European Symposium on Programming, volume 1576 of Lecture Notes in Computer Science, \npages 91 108. Springer-Verlag, 1999. [10] Cormac Flanagan and Stephen N. Freund. Type-based race detection \nfor Java. In ACM Conference on Programming Language Design and Implementation, pages 219 232, 2000. [11] \nCormac Flanagan and Stephen N. Freund. Atomizer: A dynamic atomicity checker for multithreaded programs. \nIn ACM Symposium on Principles of Programming Languages, pages 256 267, 2004. [12] Cormac Flanagan, K. \nRustan M. Leino, Mark Lillibridge, Greg Nelson, James B. Saxe, and Raymie Stata. Extended static checking \nfor Java. In ACM Conference on Programming Language Design and Implementation, pages 234 245, 2002. [13] \nCormac Flanagan and Shaz Qadeer. A type and effect system for atomicity. In ACM Conference on Programming \nLanguage Design and Implementation, pages 338 349, June 2003. [14] Cormac Flanagan and Shaz Qadeer. Types \nfor atomicity. In ACM International Workshop on Types in Language Design and Implementation, pages 1 \n12, January 2003. [15] Matthew Flatt and Robert Bruce Findler. Kill-safe synchronization abstractions. \nIn ACM Conference on Programming Language Design and Implementation, pages 47 58, Washington DC, June \n2004. [16] Lance Hammond, Brian D. Carlstrom, Vicky Wong, Ben Hertzberg, Mike Chen, Christos Kozyrakis, \nand Kunle Olukotun. Programming with transactional coherence and consistency (tcc). In International \nConference on Architectural Support for Programming Languages and Operating Systems, pages 1 13, 2004. \n[17] Tim Harris. Exceptions and side-effects in atomic blocks. In PODC Workshop on Concurrency and Synchronization \nin Java Programs, July 2004. [18] Tim Harris and Keir Fraser. Language support for lightweight transactions. \nIn ACM Conference on Object-Oriented Programming, Systems, Languages, and Applications, pages 388 402, \nOctober 2003. [19] Tim Harris, Simon Marlow, Simon Peyton Jones, and Maurice Herlihy. Composable memory \ntransactions. In ACM Symposium on Principles and Practice of Parallel Programming, 2005. [20] Maurice \nHerlihy, Victor Luchangco, Mark Moir, and William N. Scherer III. Software transactional memory for dynamic-sized \ndata structures. In ACM Symposium on Principles of Distributed Computing, pages 92 101, 2003. [21] Michael \nHicks, Pankaj Kakkar, Jonathan T. Moore, Carl A. Gunter, and Scott Nettles. PLAN: A packet language for \nactive networks. In ACM SIGPLAN International Conference on Functional Program\u00adming Languages (ICFP), \npages 86 93, 1998. [22] Michael Hicks, Jonathan T. Moore, D. Scott Alexander, Carl A. Gunter, and Scott \nM. Nettles. PLANet: An active internetwork. In IEEE Computer and Communication Society INFOCOM Conference, \npages 1124 1133, 1999. [23] Richard J. Lipton. Reduction: a method of proving properties of parallel \nprograms. Communications of the ACM, 18(12):717 721, December 1975. [24] Barbara Liskov and Robert Schei.er. \nGuardians and actions: Lin\u00adguistic support for robust, distributed programs. ACM Transactions on Programming \nLanguages and Systems, 5(3):381 404, 1983. [25] Jeremy Manson, Jason Baker, Antonio Cunei, Suresh Jagannathan, \nMarek Prochazka, Jan Vitek, and Bin Xin. Preemptible atomic regions for Real-time Java. Technical report, \nPurdue University, 2005. [26] V. Krishna Nandivada and Suresh Jagannathan. Dynamic state restoration \nusing versioning exceptions. Higher-Order and Symbolic Computation. To appear. [27] Ravi Rajwar and James \nR. Goodman. Transactional lock-free execution of lock-based programs. In 10th International Conference \non Architectural Support for Programming Languages and Operating Systems, pages 5 17, San Jose, CA, October \n2002. [28] John H. Reppy. Concurrent Programming in ML. Cambridge University Press, 1999. [29] Michael \nF. Ringenburg and Dan Grossman. Types for describing coordinated data structures. In ACM International \nWorkshop on Types in Language Design and Implementation, pages 25 36, January 2005. [30] Algis Rudys \nand Dan S. Wallach. Transactional rollback for language\u00adbased systems. In International Conference on \nDependable Systems and Networks, June 2002. [31] Stefan Savage, Michael Burrows, Greg Nelson, Patrick \nSobalvarro, and Thomas Anderson. Eraser: A dynmaic data race detector for multithreaded programs. ACM \nTransactions on Computer Systems, 15(4):391 411, 1997. [32] February 27, 2005. http://www.securityfocus.com/. \n[33] Nir Shavit and Dan Touitou. Software transactional memory. Distributed Computing, Special Issue(10):99 \n116, 1997. [34] Avraham Shinnar, David Tarditi, Mark Plesko, and Bjarne Steens\u00adgaard. Integrating support \nfor undo with exception handling. Tech\u00adnical Report MSR-TR-2004-140, Microsoft Research, December 2004. \n[35] Olin Shivers, James W. Clark, and Roland McGrath. Atomic heap transactions and .ne-grain interrupts. \nIn ACM International Conference on Functional Programming, pages 48 59, 1999. [36] Andrew P. Tolmach \nand Andrew W. Appel. A debugger for standard ML. Journal of Functional Programming, 5(2):155 200, 1995. \n[37] February 27, 2005. http://www.us-cert.gov/. [38] Christoph von Praun and Thomas R. Gross. Object \nrace detection. In ACM Conference on Object Oriented Programming, Systems, Languages, and Applications, \npages 70 82, October 2001. [39] Adam Welc, Antony L. Hosking, and Suresh Jagannathan. Preemption-based \navoidance of priority inversion for Java. In International Conference on Parallel Processing, 2004. [40] \nAdam Welc, Suresh Jagannathan, and Antony L. Hosking. Transac\u00adtional monitors for concurrent objects. \nIn European Conference on Object-Oriented Programming, 2004. [41] Jeannette M. Wing, Manuel F\u00a8ahndrich, \nJ. Gregory Morrisett, and Scott Nettles. Extensions to standard ML to support transactions. In ACM SIGPLAN \nWorkshop on ML and its Applications, 1992.  A. AtomCAML Condition Variable Library open Thread type \nchannel = bool ref type condvar = channel list ref (*a queue would be fairer perhaps*) let create () \n= ref [] let signal cv = atomic (fun () -> match !cv with [] -> () | hd::tl -> (cv := tl; hd := false)) \nlet broadcast cv = List.iter (fun r -> r := false) (atomic (fun () -> let ans = !cv in cv := []; ans)) \nlet listen cv = atomic (fun () -> letr =reftruein cv:= r:: !cv; r) let wait ch = atomic (fun () -> if \n!ch then yield_r ch else ()) B. Logging and Rollback Microbenchmarks Writes Execution Execution Logging \nExecution Logging &#38; Rollback 0 0.194 0.195 0.245 10 0.238 0.271 0.327 50 0.368 0.643 0.741 100 0.543 \n1.064 1.158  \n\t\t\t", "proc_id": "1086365", "abstract": "We have designed, implemented, and evaluated AtomCaml, an extension to Objective Caml that provides a synchronization primitive for atomic (transactional) execution of code. A first-class primitive function of type (unit-&gt;'a)-&gt;'a evaluates its argument (which may call other functions, even external C functions) as though no other thread has interleaved execution. Our design ensures fair scheduling and obstruction-freedom. Our implementation extends the Objective Caml bytecode compiler and run-time system to support atomicity. A logging-and-rollback approach lets us undo uncompleted atomic blocks upon thread pre-emption, and retry them when the thread is rescheduled. The mostly functional nature of the Caml language and the Objective Caml implementation's commitment to a uniprocessor execution model (i.e., threads are interleaved, not executed simultaneously) allow particularly efficient logging. We have evaluated the efficiency and ease-of-use of AtomCaml by writing libraries and microbenchmarks, writing a small application, and replacing all locking with atomicity in an existing, large multithreaded application. Our experience indicates the performance overhead is negligible, atomic helps avoid synchronization mistakes, and idioms such as condition variables adapt reasonably to the atomic approach.", "authors": [{"name": "Michael F. Ringenburg", "author_profile_id": "81100300563", "affiliation": "University of Washington, Seattle, WA", "person_id": "P578357", "email_address": "", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP43120334", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086378", "year": "2005", "article_id": "1086378", "conference": "ICFP", "title": "AtomCaml: first-class atomicity via rollback", "url": "http://dl.acm.org/citation.cfm?id=1086378"}