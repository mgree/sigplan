{"article_publication_date": "09-12-2005", "fulltext": "\n The Anatomy of a Loop A story of scope and control Olin Shivers * Georgia Institute of Technology \nshivers@cc.gatech.edu Abstract Writing loops with tail-recursive function calls is the equivalent of \nwriting them with goto s. Given that loop packages for Lisp-family languages have been around for over \n20 years, it is striking that none have had much success in the Scheme world. I suggest the reason is \nthat Scheme forces us to be precise about the scoping of the various variables introduced by our loop \nforms, something pre\u00advious attempts to design ambitious loop forms have not managed to do. I present \nthe design of a loop package for Scheme with a well\u00adde.ned and natural scoping rule, based on a notion \nof control dom\u00adinance that generalizes the standard lexical-scope rule of the .\u00adcalculus. The new construct \nis powerful, clear, modular and exten\u00adsible. The loop language is de.ned in terms of an underlying language \nfor expressing control-.ow graphs. This language itself has inter\u00adesting properties as an intermediate \nrepresentation. Categories and Subject Descriptors D.3.3 [Programming lan\u00adguages]: Language Constructs \nand Features control structures, frameworks, procedures, functions, and subroutines; F.3.3 [Log\u00adics and \nmeanings of programs]: Studies of program constructs control primitives and functional constructs; D.1.1 \n[Programming techniques]: Applicative (Functional) Programming; D.3.1 [Pro\u00adgramming languages]: Formal \nDe.nitions and Theory semantics and syntax General Terms Design, Languages, Performance, Theory Keywords \nFunctional programming, control structures, iteration, lambda-calculus, compilers, programming languages, \ncontinua\u00adtions, macros, Scheme 1. The case for loop forms In call-by-value functional languages such \nas ML or Scheme, we typically write loops using tail-recursive function calls. This is actually a terrible \nway to express program iteration, and it s not hard to see why. As was popularised by Steele [14, 15, \n16, 17], a tail call is essentially a goto that passes arguments. So writing * This work supported by \nthe NSF Science of Design program. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 05 September 26 28, 2005, Tallinn, Estonia. Copyright c . 2005 \nACM 1-59593-064-7/05/0009. . . $5.00. loops with tail calls is just writing them with gotos. Yet, it \nhas long been accepted in the programming-language community that goto is a low-level and obfuscatory \ncontrol operator, a position stated by Dijkstra s Goto considered harmful letter [5]. Consider, for example, \nthe loop shown in Figure 1, which has non-trivial control structure. The version on the left is written \nusing basic Scheme and tail calls, while the version on the right is written using a macro-based loop \nform. We loop over list l, binding elements of the list to variable x; the loop s .nal value is accumulated \nin result in reverse order. The body of the loop has two fast path tests. The .rst test allows us to \nskip the presumably expensive computations producing the intermediate values bound to y and z. The second \ntest allows us to skip the z computation. From a software-engineering perspective, this code fragment \nis a disaster. The fact that we are iterating over a list is expressed by code that is spread over the \nentire loop: the entry point bind\u00ading rest, the four distinct calls to loop (including the initial call \nproviding the beginning value l for rest), and the null? termi\u00adnation test. There s no clear distinction \nin the syntax between the loop parameter that drives the loop (rest), and the loop parameter that accumulates \nthe loop s .nal value (result). Since both driver and accumulator are lists, if we slip up and swap the \ntwo update forms in one of our four tail calls, we won t get a type error (at run time in Scheme, or \nat compile time in the analogous SML variant), so the bug would be tricky to catch. As loop variables \nproliferate in more complex loops, and loop size increases, separating looping calls from the binding \nsites, this error becomes easier and easier to make. The interactions between the control structure of \nthe fast\u00adpath tests and the environment structure imposed by lexical scope cause the loop s indentation \nto drift off to the right of the page. The technology we ll be developing in this paper allows us to \ncreate cleaner sub-languages that can be embedded within Scheme that are speci.cally designed to express \niteration. Using a loop form of this sort, we can write the same loop as shown on the right side of the \n.gure. Even without going into the details of the various clauses (a grammar for the loop form is shown \nin Figure 3), it s fairly clear to see the high-level structure of the loop. It s clear that we are iterating \nover a list, that we have two fast-path tests, that we are binding y and z to the results of intermediate \ncomputations, and that we are accumulating the results into a list. The fact that the rightward-drifting/nested \nindentation of the original code has aligned into a vertical column is a hint that our control and envi\u00adronment \nstructuring has been brought more closely into alignment. The real payoff from specialised loop forms \ncomes when we decide to alter the code. Suppose we wish to change our example so that the loop s input \nis provided in a vector instead of a list. Al\u00adthough this is conceptually a single element of the loop \ns structure, the relevant code that must be altered in the low-level tail-call ver\u00adsion of the loop is \nspread across the entire loop, interwoven with the rest of the code. Figure 2 shows the new loop; you \nare invited (letrec ((loop (. (rest result) (if (null? rest) (reverse result) (let ((x (car rest))) \n(if (p? x) (let ((y <verbose code using x>)) (if (q? <verbose code using y>) (let ((z <verbose expression>)) \n(loop (cdr rest) (cons <z expression> result))) (loop (cdr rest) result))) (loop (cdr rest) result))))))) \n(loop l ())) (loop (for x in l) (when (p? x)) (bind (y <verbose code using x>)) (when (q? <verbose code \nusing y>)) (bind (z <verbose expression>)) (save <z expression>)) Figure 1. A loop expressed directly \nwith tail calls and by means of a loop form. The loop form disentangles the distinct conceptual elements \nof the iteration. (let ((len (vector-length v))) (loop (for x in-vector v) (letrec ((loop (. (i result) \n(when (p? x)) (if (= i len) (bind (y <verbose code using x>)) (reverse result) (when (q? <verbose code \nusing y>)) (let ((x (vector-ref v i))) (bind (z <verbose expression>)) (if (p? x) (save <z expression>)) \n(let ((y <verbose code using x>)) (if (q? <verbose code using y>) (let ((z <verbose expression>)) (loop \n(+ i 1) (cons <z expression> result))) (loop (+ i 1) result))) (loop (+ i 1) result))))))) (loop 0 ()))) \n Figure 2. Using tail calls, small design changes induce large code changes; with loop forms, small design \nchanges require small code changes. to hunt for the eight distinct changes scattered across the .fteen \nlines of code. If, however, we use a dedicated loop form, the sin\u00adgle change to the loop s spec requires \na single change to the loop s code. The loop-form version allows us to modularly construct the loop by \nseparately specifying its conceptual elements. When we increase the complexity of our loops, the problems \nassociated with using direct tail calls to express iteration get worse. Multiple nested loops, Knuth-style \nexits occurring in the middle of an iteration, multiple iteration drivers or accumulators, and other \nreal-world coding demands all increase the confusing complexity of this approach. In short, tail calls \nand . forms make great machine code for expressing a computation at a low level. But they are not good \nsoftware-engineering tools for programmers to express an iteration. Additionally, the problem is not \nhandled by the use of higher\u00adorder functions such as map and fold to capture patterns of itera\u00adtion. \nWhile these functional control abstractions can capture simple loops, they don t scale gracefully as \nloop complexity grows. For ex\u00adample, map is .ne if we wish to accumulate a list...but not if some elements \nof the iteration don t add elements to the result (as is the case in our example above). We can use fold \nto iterate across a list...but what do we use when we wish to iterate across a list, and increment an \nindex counter, and sequence through a compan\u00adion vector in reverse order? Loop forms let us compose these \nloops easily by adding driver clauses in a modular fashion; functional ab\u00adstractions do not compose in \nthis way. For these reasons, programmers in the Lisp family of languages have long resorted to loop-speci.c \nforms of the kind employed on the right-hand side of the examples above. Common Lisp, Zetalisp, and Maclisp \nall had loop forms more or less along the lines of the one we showed. Common Lisp s form is a particularly \npowerful and baroque example of the genre. It s notable, however, that this style of loop macro has never \nmade much headway in the Scheme community.  2. The problem with loop macros The critical problem with \nloop macros is the dif.culty of providing a clear, well-de.ned de.nition of scope for the variables and \nex\u00adpressions introduced by the clauses. A careful reading of the Com\u00admon Lisp documentation for its loop \nform [18, paragraph 6.1.1.4] reveals the following somewhat embarrassing non-speci.cation (bold-face \nemphasis added): Implementations can interleave the setting of initial val\u00adues with the bindings. . . \n. One implication of this interleav\u00ading is that it is implementation-dependent whether the lex\u00adical environment \nin which the initial value forms . . . are evaluated includes only the loop variables preceding that \nform or includes more or all of the loop variables In the commentary that accompanies the speci.cation, \nPitman s discussion [12] makes the ambiguity explicit (again, bold-face em\u00adphasis added): These extremely \nvague phrases don t really say much about the environment, and since they don t say what goes into the \nlet or the lambda, or even how many let or loop ::= (loop lclause ...) lclause ::= (initial (vars init \n[step [test]]) ...) | (before exp ...) | (incr i from init [to .nal] [by step]) | (decr i from init [to \n.nal] [by step]) | (previous pvar var init1 ... initn) | (repeat n) | (for x in list [by step-fn]) \n| (for l on list [by step-fn]) | (for c in-string s [option ...]) | (for x in-vector v [option ...]) \n| (for i in-string-index s [option ...]) | (for i in-vector-index v [option ...]) | (for x in-file fname \n[reader reader]) | (for x from-port exp [reader reader ]) | (for x input [reader ]) (continued) | bclause \n| | | (after exp ...) (result exp1 ... expn) (save exp) bclause ::= | (while exp) | (until exp) | (when \nexp) | (unless exp) | (do exp1 ...) | (bind (vars1 exp1) ...) | (subloop lclause ...) | (if exp bclause \n[bclause]) ; Body clause: ; Controls loop ; termination ; Controls single ; iteration ; For side effect \n; Nested loop options ::= incr | decr | index i | from init |to .nal |by step Figure 3. Partial grammar \nfor the loop form necessarily partial, as the clause set is extensible by means of the Scheme macro system. \nlambda forms are involved, they don t really say much at all. Further, the vague statement on p8 85 about \nhow let+setq might be used to implement binding leaves an unusually large amount of latitude to implementations. \nThe source of the problem is that a single clause in a loop form can cause parts of the clause to be \ninserted into multiple places in the resulting code. For example, the initial clause in the loop form \n(loop ... (initial (i 0 (+ i 1))) ...) initialises loop variable i to 0, and then increments it on each \nfol\u00adlowing iteration. Thus, it introduces the expression 0 into the loop s prologue and the expression \n(+ i1) into the loop s update sec\u00adtion, which occurs at an entirely distinct position in the expanded \ncode. This makes it problematic to de.ne scope in terms of the order in which clauses occur in the loop \nform. If clause a occurs before clause b, a s late code fragment may come after b s early code fragment, \nbut before b s late code fragment. Thus clause or\u00adder cannot be used to establish some kind of scope \norder. Lack of a coherent, unambiguous scoping principle is not acceptable for a programming-language \nconstruct. This dif.culty in providing a clear speci.cation for variable scope is, I believe, the primary \nreason Lisp-style loop forms have not been provided in Scheme. When we wish to make our loop form extensible, \nby allowing programmers to de.ne their own clauses using Scheme s macro system, the problem becomes even \nmore dif.cult.  3. The essence of lexical scope The lexical-scoping rule of the .-calculus provides \ntwo important properties for programmers. First, it allows a programmer to select a variable name for \na local piece of code without needing global knowledge of the program. For example, if a programmer wishes \nto write a three-line loop somewhere in a program, he can select i as an iteration variable without having \n.rst to scan the entire pro\u00adgram to ensure that i is used nowhere else. We can have multiple variables \nwith the same name, locally disambiguating references to the shared name using the rule of lexical scope. \nWhile important for humans, this property is not a deep prop\u00aderty. Suppose we dispensed with it by de.ning \na variant of the .\u00adcalculus that required every variable declared in the program to be unique. This would \ncause no reduction in the computational power of the language; it would still be Turing-complete. A com\u00adpiler \nor other program-manipulation system would have no prob\u00adlems analysing or otherwise operating on programs \nwritten in such a language. The second property provided by classical lexical scoping, however, is a \nmore generally useful one: de.nitions (or bindings) control-dominate uses (or references). That is, in \na functional lan\u00adguage based on the .-calculus, we are assured that the only way program control can \nreach a variable reference is .rst to go through that variable s binding site. We have a simple static \nguarantee that program execution will never reference an unbound variable. If this seems simple and obvious, \nnote that it is not true of assem\u00adbler or Fortran: one can declare a variable in a Fortran program, then \nbranch around the initialising assignment to the variable, and proceed to a reference to the variable. \nFurther, if we are casting about for a general principle on which to base a language design, the simpler \nand more obvious, the better. 4. BDR scope In the previous section, we established that the lexical-scope \nrule of the classic .-calculus, which we ll call LC scope, implies an important property: it ensures \nthat binders dominate references. The key design idea that serves as the foundation for our extensible \nloop form is to invert cause and effect, taking the desired property as our de.nition of scope. This \ngives us a new scoping rule, which we call BDR scope. The BDR scoping principle can be informally de.ned \nin a graphical context. Suppose we have a control-.ow graph. Each vertex represents a unit of computation, \nor basic block; there is a designated start vertex v0; edges in the graph connect a vertex to its possible \ncontrol successors. We additionally decorate each edge e with a set of identi.ers vse. The total set \nof all identi.ers occurring on the edges of the entire control-.ow graph are called its loop variables. \nThe execution model is that when control reaches a vertex, we perform the computation associated with \nthat vertex. This computation culminates by (1) selecting one of the vertex s cfg ::= (let* ((l1 cfg1) \n...) cfg) | (letrec ((l1 cfg1) ...) cfg) | (go l) | (do proc (vars1 cfg1) ...) | (indep ((vars1 exp1) \n...) cfg) | (permute ((l1 cfg1) ...) cfg) | (permute/tail ((l1 cfg1) ...) cfg) ::= ident vars ::= (ident \n...) proc,exp ::= Scheme expression Figure 4. The CFG language out edges e, and (2) providing new values \nfor the loop variables vse on edge e. We update these variables to the new values, and control proceeds \nalong edge e to the next vertex. Note that this is a model that permits variable updating that is, the \nvalue associated with a variable can change over time. In our graphical model, the BDR scoping rule is \nthis: vertex v is in the scope of loop variable x if every path from start vertex v0 to v has some edge \nthat de.nes x. If there is some way to get to v without de.ning x, then x is not visible at node v. As \nwe ll see, BDR scope is purpose driven for its intended application: It is chosen for application in \nan iterative, .rst-order context.  It permits multiple updates to a given variable.  It integrates \nwith classic LC scope. That is, it allows us to specify the individual computations in the CFG s vertices \nwith fragments of a standard, LC-scoped language, and then embed the entire CFG itself as a fragment \nwithin a larger program written in that LC-scoped language. This is precisely what we want for our intended \nuse as an embedded Scheme macro.  Scope is now an ensemble property of the entire control-.ow graph. \nThus we can stitch together fragments of a CFG from multiple sources (in our case, multiple clauses of \na loop form), and then get a coherent scope story from the resulting graph.  In short, with the BDR \nrule, scope proceeds naturally from control. 5. The CFG language The larger design picture is that we \nde.ne our loop form by .xing a general iteration template; for our loop form, this template will have \neight blocks (e.g., an initialisation block, a per-iteration condi\u00adtional top-guard block, a body block, \nan update block, a .nalisation block, etc.). Each clause in a loop form will be a Scheme macro that expands \ninto a small set of graph fragments, with each fragment tar\u00adgeted to one of the eight blocks in the general \nloop template. The loop form will collect the fragments from all the loop clauses, and insert them into \nthe template, stitching them together into a com\u00adplete loop, whose control and environment structure \nare given by its explicit control structure and its associated, control-determined BDR scoping rule. \nThis graph is then translated by the macro into Scheme code. The .rst step in this design is to make \nconcrete a language for specifying control-.ow graphs in composable fragments. We do this with the CFG \nlanguage, shown in Figure 4. The CFG language has two independent name spaces, for binding code labels \nand loop variables: Code labels  name graph vertices;  are bound with let* and letrec,  referenced \nwith (go l),  and scoped with standard LC scope.  {x,y}{z}   cfg1 cfg2 Figure 5. Individual vertex \nencoded as CFG do form. Loop variables  name data values;  are de.ned on edge transitions,  referenced \nby the internal computations of the graph ver\u00adtices,  and scoped using BDR scope.  Note that labels \nare not .rst-class, expressible values. 5.1 Specifying individual vertices with do forms The basics \nof loop-variable handling are speci.ed with the CFG do form, which is used to de.ne a single vertex in \nthe CFG and its outgoing edges. If the vertex has n outgoing edges, then edge i has attached loop-variable \nset varsi and leads to cfgi. For example, Figure 5 shows, in graphical form, a do form that speci.es \na vertex with two exit edges. If the vertex s computation proc chooses to exit along its .rst edge, it \nmust provide two values which are used to update loop variables x and y before proceeding to cfg1.If \nit exits along its second edge, it must provide a single value, which is used to update z before proceeding \nto cfg2. The vertex s computa\u00adtion is expressed as a Scheme expression proc (typically a lambda expression). \nWhen control transfers to this vertex, the proc expres\u00adsion is evaluated in a Scheme lexical scope that \nsees the current bindings of all loop variables in the BDR scope of this vertex. This evaluation produces \na Scheme procedure, which is applied to n pro\u00adcedures, each representing one of the vertex s n exit edges. \nWhen the computation represented by proc is .nished, it proceeds by tail\u00adcalling one of these exit-edge \nprocedures, passing as parameters the new values for that edge s associated loop variables. It is an \nerror to call an exit procedure in a non-tail position relative to proc s appli\u00adcation.1 So, for example, \nif we want to encode a fragment of graph structure that updates x to be its absolute value and then jumps \nto the vertex named by graph label next, we can write the following CFG form: (do (. (e1 e2) (if (< x0) \n(e1 (-x)) (e2))) ((x) (go next)) ; Edge 1 (( ) (go next))) ; Edge 2 This assumes that the do form executes \nin a loop-variable scope that includes x, and a label scope that includes label next. 1 We could eliminate \nthis errorful possibility simply by specifying the exit procedures to be real Scheme continuations produced \nby call-with\u00adcurrent-continuation. Conceptually, that is what they are: continua\u00adtions. As a matter of \nengineering pragmatics, however, we can usually rely on a compiler to handle functional tail calls ef.ciently, \nwhile continuation creation and invocation remains a signi.cantly heavier weight operation for many Scheme \ncompilers. Since all this control structure happens behind the scenes, hidden by the surface syntax of \nloop-clause macros, it isn t visible to the application programmer in any event it is solely the province \nof the meta-programmer who designs and implements the loop clauses. cfg (let* ((lj cfg) ; Join point \n(la (do (. (e1) (f y) (e1)) (() (go lj)))) (lb (do (. (e1) (g x) (e1 (* x x))) ((y) (go lj))))) (do (. \n(e1 e2) (if (odd? x) (e1 (+x 1)) (e2))) ((y) (go la)) (() (go lb))))  Figure 6. A diamond control structure, \nrendered as a control-.ow graph and as a term in the CFG language. We specify a .nal or halting vertex \nin the control-.ow graph simply by means of a do form that has no successor edges. In this case, the \nScheme procedure halts the computation by returning in\u00adstead of ending in a tail call. For example, if \ncontrol ever reaches the vertex (do (. () 42)), the CFG computation .nishes, pro\u00adducing a .nal value \nof 42.  5.2 Composition: snapping together CFG fragments with label capture The CFG let* form provides \nsequential lexical scoping for loop labels analogously to the Scheme let* form; this permits us to construct \nDAGs in our control-.ow graph. Similarly, the letrec form allows circular binding of labels, so that \nwe can construct graphs containing cycles. What s important about the particulars of this syntax is that \nwe can use the LC scoping rule for loop labels to wire up fragments of graph structure. That is, if a \nCFG term represents a chunk of graph structure, then a free label referred to by some internal go form \nbut not bound by the term expresses a dangling edge for the subgraph. If we embed the term within an \nouter CFG term that binds the free label, we provide a connection for the dangling edge. For example, \nsuppose we construct three arbitrary CFG terms, using the convention that each term jumps to a free label \np (for proceed ) when it is done. Some CFG processor would like to wire the three terms together in series, \nso that when cfg1 jumps to p with a (go p) sub-form, we transfer control to cfg2; likewise, when cfg2 \njumps to p, we transfer to cfg3. We do this by inserting the three fragments into the label-capturing \ntemplate (let* ((p cfg3) (p cfg2)) cfg1) As we ll see, this is how the fragments of control structure \npro\u00adduced by the individual clauses of a loop form are snapped together into a complete loop. 5.3 Dominance \ntrees and LC label scope As a slightly more ambitious example, suppose we have a dia\u00admond control structure, \nas shown in Figure 6. The program tests loop variable x. If it is odd, we branch to the left child vertex \n(la\u00adbelled la in the textual form), updating variable y to x+1. That vertex performs the Scheme call \n(f y), presumably for side-effect, then jumps to the join point, labelled lj. If, however, x is even, \nthen control branches to the right child (labelled lb), doing no variable update. This vertex performs \nthe Scheme call (g x) for side-effect, then branches to the join point, binding y to the square of x \nat the lj jump. Note that the join point is in the scope of loop variable y, since all paths to it de.ne \ny. Consider the control-dominator tree for the graph in Figure 6. The immediate dominator of vertices \nla, lb and lj is the initial (odd? x) vertex. Note that this dominator tree is expressed di\u00adrectly by \nthe let* structure of the CFG term; this is ensured by the LC scoping used for code labels. If two vertices \nwish to jump to a common successor (such as the lj join point in our example), that successor must be \nbound to a label by a let* outside/above the two vertices. It is a general property of the CFG language \nthat the binding structure of the labels provides a conservative ap\u00adproximation to the control-dominator \ntree which means that al\u00adgorithms that process CFG terms don t need to bother performing complex dominance-frontier \ncalculations to determine the control\u00addominance relation. This is an intriguing contrast with other .rst\u00adorder, \n.at control representations, such as SSA.  5.4 Scope independence and control nondeterminance The core \nCFG forms let*, letrec, go and do capture the idea of textually expressing a control-.ow graph that can \nbe com\u00adposed in a structured way. The remaining indep, permute and permute/tail forms are included in \nthe language to allow us to compose graph structure in ways that insulates the parts from one another. \nThe indep form allows us to do parallel updates to loop variables. Each Scheme expression expi is evaluated \nin the loop\u00advar scope of the indep form. The expi expression must produce as many return values as there \nare loop variables in the varsi binding list. After all the expi expressions have been evaluated, we \nbind the values to the variables in the varsi binding lists. The different binding lists must be disjoint. \nThus no expi sees the variable updates made by any other expj. The permute forms allow for non-deterministic \ncontrol permu\u00adtation (Figure 7). When control reaches a (permute ((l1 cfg1) ...) cfg) form, the machine \nmay arbitrarily permute the sequence of cfgi terms. Then these terms are wired together in the permuted \nse\u00adquence; in cfgi, a reference to label li is connected to the next clause in the sequence; the .nal \nclause has its li label connected to the body of the permute form, cfg. Since variable scope in the CFG \nlanguage is a function of control structure, the relaxed control spec for a permute form has a corresponding \neffect on its environment structure. Since any clause can come .rst in the dynamic execution order of \nthe form, the updates performed by the other clauses do not contribute to the scope of cfgi but they \ndo contribute to the scope of the .nal cfg clause. However, note that if the entire permute form is in \nthe (let* ((join (permute ((l1 cfg1) ... (lk cfgk)) cfg)) (left (permute/tail ((l1 cfg1) ... (li cfgi)) \n (go join))) (right (permute/tail ((l1 cfg1) ... (lj cfgj )) (go join)))) ...)  cfgk lk   Figure \n8. Tail-permute allows interpermuting across join points. (permute ((l1 cfg1) ... Shuf.e l1 cfgi(ln \ncfgn)) . . on entry . cfg) ln Figure 7. The permute form allows its constituent cfgi terms to be executed \nin a non-deterministically permuted order before pro\u00adceeding to the .nal cfg term. Each cfgi term is \nthreaded forwards to the following terms by means of the associated li exit label. scope of loop variable \nx, then all the clauses are in the scope of x, and so an update to x performed by sub-graph cfgi will \nbe visible to all clauses that dynamically come after its execution. (This can be useful when two permutable \nupdates to the same variable commute with one another, such as adding two elements to a common set.) \nFinally, the permute/tail form allows for permutable se\u00adquences to extend across join points (Figure \n8). The body cfg of a permute/tail form is restricted to be (1) a permute or permute/tail form, (2) a \n(go l) form that references a le\u00adgal permute/tail body, or (3) a let* or letrec form whose body is a \nlegal permute/tail body. When control reaches a permute/tail form, we chase down the chain of permute/tail \nforms that begin with this one until we reach the terminating permute form. The clauses of all of these \nforms are then permuted together to assemble the sequence to be executed. The semantic speci.cation of \npermute and permute/tail does not mean that an implementation is required to .ip coins and shuf.e blocks \nof control structure at run time. A typical im\u00adplementation will freeze the sequence order of the relevant \nsub\u00adterms at compile time. However, even if the order of execution is so frozen, the variable scoping \nretains its permutation-restricted se\u00admantics. The reasons for the permutable semantics are (1) isolating \nsub-terms from each other s scope contributions and (2) allowing the higher-level macros that produce \nthe components to be given an order-independent control and scope semantics. We ll see how permute and \npermute/tail forms contribute to loop construction in following examples.  6. The LTK language and the \nloop template Recall the point of the CFG language: it allows separate clauses in the loop form we are \ndesigning to provide pieces of control structure that can be plugged into a master template to assemble \na complete loop. The general loop template is shown in Figure 9, along with the corresponding grammar \nfor the Loop Toolkit (LTK) language we use to specify pieces of control-.ow graph tagged with their destination \nin the template. By design contract, the macros that produce component pieces of CFG structure to be \ninserted into the loop template specify control linkages by generating CFG terms that have up to three \nfree labels: p, s and f. Jumping to a p label is used to proceed with normal execution of the loop; jumping \nto an s label is used to skip to the next iteration of the loop; jumping to an f label is used to .nish \nor terminate the loop. The semantics of the distinct components of the loop template are as follows: \nInit The init block contains the parts of the loop prologue that should execute unconditionally. Each \nsuch CFG form should have a single free label, p, used to string multiple init compo\u00adnents together; \njumps to s and f labels are not allowed. init-guard The init-guard block contains loop-initialisation \ncode which may cause the loop to terminate with no iterations at all. An init\u00adguard CFG term may have \nreferences to free labels p or f (but not s). The f references in the init-guard CFGs are connected to \nthe .nish block, while the p labels are used to continue executing the other init-guard terms before \nproceeding to the top-guard block. Top-guard The top-guard block contains fragments that perform the \nper\u00aditeration termination tests that should occur at the beginning of each iteration. The individual \ncfg terms are allowed to refer to free p and f labels, with the f labels jumping to the .nish block, \nand the p labels being used to string together the various top\u00adguard cfgs.  init init-guard top-guard \nbody update bottom-guard finish result  ltk ::= (init cfg) | (init-guard cfg) | (top-guard cfg) | (body \ncfg) | (update vars exp) | (bottom-guard cfg) | (finish cfg) | (result exp) Figure 9. The loop template \nand its corresponding LTK language. Dotted lines indicate permutable sequences. Body The body block contains \nthe fragments that are intended to comprise the per-iteration body of the loop. Besides p and f labels, \nthese fragments are also allowed to refer to s labels, which are connected to short-cut transfers to \nthe update block, causing execution to skip the rest of the body block. Update The update block is for \nperforming loop-variable updates. These independent update components are composed together using an \nindep CFG term, thus causing updates to be done in parallel. Bottom-guard The bottom-guard block is the \nlocation for bottom-of-the-loop per-iteration termination tests and conditional updates. Bottom\u00adguard \ncfgs are connected into the template using their p and f labels. Finish &#38; result The .nish block \nis for wrap-up code, and the result block is for the Scheme expression that provides the .nal value of \nthe loop to its containing Scheme expression. Finish cfgs may only refer to free p labels. There can \nbe only one result form in a complete LTK program. The three guard blocks init-guard, top-guard and bottom\u00adguard \nare constructed using permute and permute/tail to as\u00adsemble the block from its respective LTK components. \nThis means that scope-introducing guard clauses are order-independent, which is important to allow loop \nclauses to have position-independent scoping properties. To give an idea for how we can use the LTK language \nto de.ne loop clauses, consider the loop form s for and incr clauses. The loop clause (loop ... (for \nx in exp) ...) causes variable x to sequence through the elements of the list exp. We implement this \nby having the for clause expand into the pair of LTK forms: ;;; TMP fresh / X from FOR clause. (init \n(do (. (e) (e exp)) ((tmp) (go p)))) (top-guard (do (. (e1 e2) (if (pair? tmp) (e1 (car tmp) (cdr tmp)) \n(e2))) ((x tmp) (go p)) (() (go f)))) where the loop variable tmp is a fresh variable generated using \nScheme s hygienic macro system. This use of hygiene illustrates a general principle behind the de.nition \nof loop clauses. Private, intra-clause state, that must be communicated between, for exam\u00adple, the init \nand top-guard blocks of code produced by a single clause is managed by having the macro producing the \nLTK forms create a fresh variable (such as tmp) and insert it into both LTK forms. Because this variable \nis fresh, it cannot clash with or be ref\u00aderenced by code produced by any other loop clause. The same \nis true of the exit-edge Scheme variables bound by the proc subform in a do CFG term (e, e1 and e2 in \nthe example) these variables are typically fresh variables created by the macro that constructs the do \nterm around code taken from a loop-clause term, which was written by the original author. Thus these \nexit-edge variable bind\u00adings can t accidentally capture variable references in the user code. In contrast, \nthe loop variable x is inter-clause state presumably the reason it is being de.ned in the loop s for \nclause is so that some other clause can refer to it. We manage this kind of state, linking the producer \nfor clause with a consumer clause, by means of common reference to the same variable in this case, x. \nNotice that the for clause conditionally binds x. Since it only has a de.ned value if the source list \nis non-empty, its de.nition happens below a conditional split in the clause s CFG. Similarly, consider \na loop clause that steps a variable across an arithmetic sequence, such as (loop ... (incr i from (* \njj) to (-k 3)) ...)  (: ini1 ... ; init (let* ((f (: .n1 ... (do (. () res))))) ; finish &#38; result \n(permute/tail ((p ig1) ...) ; init-guard (letrec ((p (permute ((p tg1) ...) ; top-guard (let* ((s (indep \n((upvars1 upexp1) ...) ; update (permute/tail ((p bg1) ...) ; bottom-guard (go p))))) (: body1 ... (go \ns)))))) ; body (go p))))) Figure 10. Putting it all together: inserting the LTK clauses into the master \nloop template to assemble a complete CFG. Comments on right side indicate the LTK clauses providing the \nCFG terms inserted on each line. The (: cfg1 ...) form is syntactic sugar for the p-serialising form \n(let* ((p cfgn) ... (p cfg2)) cfg1). Note how label scope for each individual form is restricted to the \nallowed linkages for the LTK element. The incr clause creates a fresh identi.er hi and then expands into \nthe trio of LTK forms (init (do (. (e) (e(* j j) (-k3))) ((i hi) (go p)))) (top-guard (do (. (e1 e2) \n (if (< i hi) (e1) (e2))) (() (go p)) (() (go f)))) (update (i) (+ i 1)) As with the for clause above, \nthe incr clause s three LTK forms are stitched into the loop template by means of their references to \nfree p and f labels. 7. Rotating loop tests and permutable loop blocks We saw one possible de.nition \nof the (for var in list-exp) loop clause in the pair of init and top-guard LTK terms shown above. But \nthere is another completely reasonable de.nition we could use, which rotates the conditional test back \nalong the loop template from the top-guard block into the init-guard and bottom\u00adguard blocks, replicating \nthe code: ;;; (for var in list-exp) (init (do (. (e) (e list-exp)) ((tmp) (go p)))) (init-guard (do (. \n(e1 e2) (if (pair? tmp) (e1 (car tmp) (cdr tmp)) (e2))) ((var tmp) (go p)) (() (go f)))) (bottom-guard \n(do (. (e1 e2) (if (pair? tmp) (e1 (car tmp) (cdr tmp)) (e2))) ((var tmp) (go p)) (() (go f)))) We might \nwish to de.ne for clauses this way to put the per\u00aditeration test at the bottom of loop, thus allowing \nthe compiler to save one branch instruction when generating assembler code for the loop by combining \nthe termination test s conditional branch with the branch that jumps back to the top of the loop. This \nis a common compiler trick. The problem is that we ve changed the control-.ow graph. Con\u00adtrol structure, \nin our CFG model, determines environment structure. So our CFG change induces a change in scoping. In \nthe rotated def\u00adinition, var sde.nition is pulled up to the init-guard block, which places top-guard \ncode in the scope of var. It s a design problem that these two implementations of the for clause provide \ndifferent variable scope we d like the meaning of the loop to be invariant across the two de.nitions. \nWe can re\u00adstore semantic invariance by making the init-guard, top-guard and bottom-guard elements all \ninterpermutable. We do this by insert\u00ading the init-guard and bottom-guard elements into permute/tail \nforms; these two blocks transfer control to the top-guard block, which is constructed as a permute form. \nThat is, suppose the various clauses of a loop form produce n init-guard LTK terms (init-guard ig1) ... \n(init-guard ign). Then the init-guard block is (permute/tail ((p ig1) ; permuted ... ; init-guard (p \nign)) ; elements loop-top) ; top-guard permutes Note how the p labels are bound by the permute/tail form \nto capture the exit of each ig CFG term. In turn, if the var\u00adious top-guard LTK forms produced by the \nloop clauses are (top-guard tg1) ... (top-guard tgn), then the top-guard CFG block is rendered as (permute \n((p tg1) ; top-guard block ... (p tgn)) loop-body) ; body/update/bottom-guard blocks Within the loop \nbody, the bottom-guard elements are assembled into a permute/tail just as the init-guard terms are; the \ninit-guard and bottom-guard permute/tail forms share the same target, the top-guard permute form. This \nmeans that when control arrives at the top of the init-guard block, in principle, all the init-guard \nand top-guard terms are col\u00adlected together and permuted; we then execute the permuted se\u00adquence of tests. \nSo de.nitions introduced by these terms do not in\u00adtroduce scope visible by the other init-guard and top-guard \nclauses (although their de.nitions do introduce scope for the loop body it\u00adself). Similarly, when control \nreaches the bottom-guard block, all the bottom-guard and top-guard LTK terms are collected together and \ninterpermuted. Thus it doesn t matter if a conditional test oc\u00adcurs in the shared top-guard permute block, \nor is replicated back into the two init-guard and top-guard permute/tail pre.xes. A loop-clause implementor \ncan choose either implementation with no change in the speci.ed scope or control behavior. If all this \ninterpermuting seems a bit complex, bear in mind that this is simply the hidden low-level semantics providing \nthe building blocks we use to construct the higher-level control and environment fragments the loop clauses \nthat the programmer actually uses to construct a loop. These details ensure that these fragments .t to\u00adgether \nat the loop-clause level in a clean and modular way. In partic\u00adular, allowing for control permutation \nprovides order-independent scoping that simpli.es the semantics at the loop-clause level. The complete \ntemplate for the loop form is shown in Figure 10; this is the graphical template of Figure 9 rendered \nas a CFG term. E,[[ (let ((l1 cfg1)) cfg)]] . E,[ l1.cfg1] cfg E,[[ (letrec ((l1 cfg1) ...) cfg)]] . \n[] E,li((l1 cfg1)...) cfgi)]] .[[ (letrec cfg prim E,prim --.( i,(v1,...,vj ))1 =i=n j = ki (do prim \n((x1,1 ... x1,k1 ) cfg1) E,[... ] . E[ xi,j.vj ] ,cfgi [ ((xn,1 ... xn,kn ) cfgn)) E,[[ (permute () \ncfg)]] . E,cfg E,[[ (permute (b1...bn) cfg)]] . E,[[ (let ((li (permute (b1...bi-1 bi+1...bn) cfg))) \ncfgi)]] where bi =[[ (li cfgi)]] '' E,cfg . E,cfgcfg is let E,[[ (permute/tail (b1 ...) cfg)]] . or letrec. \n' E,[[ (permute/tail (b1 ...) cfg)]] (permute/tail (b1 ...) ' [[ ]] E,(permute/tail (b1 ...) . E,[[ \n(permute/tail (b1 ...b1 ' ...) cfg)]] cfg)) (permute/tail (b1 ...) '' [[ ]] E,(permute (b1 ...) . \nE,[[ (permute (b1 ...b1 ...) cfg)]] cfg)) Figure 11. The transition relation for the dynamic semantics \nof the CFG language. 8. Formal semantics By this point, we ve had enough informal description and seen \nenough examples to de.ne a simple small-step operational seman\u00adtics, giving a precise meaning to the \nCFG language. We can rep\u00adresent a machine con.guration as a term in the language plus an environment \nE giving the current values of the de.ned variables. Note that the environment gives the values of all \nvariables de.ned by prior execution, not just the ones in scope at the current control point we are not \nyet de.ning any notion of loop-variable scope. We don t need to de.ne the language in terms of an actual \ngraph, because joins and cycles in the graph structure are de.ned by means of labels, which are managed \nusing the standard scoping mecha\u00adnisms of the .-calculus. Thus a simple substitution model suf.ces to \nunroll the term on demand as execution proceeds through the control-.ow graph. Unrolling is managed by \nmeans of label substi\u00adtutions: a substitution [ l1,...,ln.cfg] is a map from .cfg1nlabels to CFG terms \nthat is the identity function at all but a .nite number of elements in its domain. Label substitution \nis lifted to CFG terms in the usual capture-avoiding way. We also need a set of rules to model the primitive \ncomputa\u00adtions sited at the graph nodes (which is provided by the Scheme code in the do and indep forms \nof the concrete CFG language). In our formal semantics, we model these computations with a prim --.relation \nthat relates an environment/primitive pair E,prim to an edge-index/value-vector pair ( i,(v1,...,vj )) \n. Such a rela\u00adtion means that if primitive computation prim is performed start\u00ading with variable context \nE,it .nishes by producing the vector of values (v1,...,vj ), and electing to proceed along exit edge \n#i.We only model do in this semantics; indep is a trivial variant. With these pieces in place, we can \nde.ne our transition re\u00adlation . with the schema shown in Figure 11. The relation is fairly simple. The \n.rst two rules handle let and letrec terms by completely standard substitution steps, with the latter \nunrolling the recursion once. The do rule simply .res the primitive com\u00adputation, then traverses the \nindicated edge, while making the in\u00addicated loop-variable updates. The only rules of any real interest \nare the remaining rules, for permute and permute/tail. These assemble permutable items from permute/tail \nchains, and non\u00addeterministically execute the elements. The rules make it clear how completely the language \nis focussed on control and environment manipulation. All the real computation prim is left to the --.relation; \nthe CFG terms provide the control and environment glue that connects the primitive computations. The \nrules also show how closely the CFG language is related to CPS; one indicator is their lack of recursion \nnote that the premise of the .rst permute/tail rule is not truly recursive, but simply a device for compactly \nwriting down what would otherwise be a pair of more verbose axiom schema (for permute/tail terms with \nlet and letrec bodies, respectively). 9. Types for scope Something is missing from our dynamic semantics: \nscope. It ap\u00adpears nowhere in the schema for the . relation. The semantics, as de.ned by these schema, \nsimply runs each primitive computation in the context of whatever de.nitions happen to have dynamically \noccurred during the execution that led to that primitive s particular do form. Sl =(s ' ,d ' )[ ] s \n' .s S, ..(go l) : s .l .d ' ' S =S[l1 .(d1,s1)] '' '' S , . .cfg :s S, . .cfg1 :s1 . ' =.[l1 .\u00d8] [ \n] S, ..(let ((l1 d1 s1 cfg1)) cfg) : s . '' =... .d1 d1 .s1 .i =... .varsi S, .1 .cfg1 : s1, ... [ \n] si =s ' .varsi S, ..(do s ' proc (vars1 cfg1) ...) : s s ' .s S ' =S[l1 .(d1,s1)] '' ''' S , . .cfg \n:s S , . .cfg1 : s1 . ' =.[l1 .\u00d8] [ ] '' =... .d1 S, ..(letrec ((l1 d1 s1 cfg1)) cfg) : s . d1 .s1 \nFigure 13. Scope-establishing type-judgement schema for core of type-annotated CFG language tcfg ::= \n(let* ((l1 s1 d1 tcfg1) ...) tcfg) | (letrec ((l1 s1 d1 tcfg1) ...) tcfg) | (go l) | (do s proc (vars1 \ntcfg1) ...) s, d ::= (ident ...) Figure 12. Syntax of core type-annotated CFG language. Recall our fundamental \nscoping principle: scope proceeds from control. The semantics we ve de.ned provides a control story; \nthus it implies one for scope, as well. The scope of a given do subterm in a CFG is simply the set of \nloop variables that are de.ned on every execution path to that subterm. With this de.nition, suitably \nformalised, we can then restrict the premise of the do rule so that a prim --.computation is only provided \nthe statically-guaranteed subset of the dynamic variable environment at that point. That is, we can run \nthe primitive computation in its proper scope. Once a complete CFG has been constructed, the task of \ntrans\u00adlating it into Scheme is mediated by an analysis step that deter\u00admines the scope of each program \npoint. The results of this analysis are used to annotate the CFG; these annotations guide the subse\u00adquent \ncompilation step into .nal Scheme code. We can think of this step as a type-inference step, where our \ntypes express the en\u00advironment structure of CFG terms. However, the calculation does not have the structure \nof standard control-dominance algorithms, because we can use the lexically explicit label-scoping structure \nto conservatively approximate the dominance tree. To simplify the presentation, we .rst present the type \nsystem for the core CFG language, without the permute, permute/tail and indep forms. The grammar of the \ntype-annotated language is shown in Figure 12. Essentially, we tag each label and do form in the language \nwith s and d identi.er sets: s: Vars de.nitely de.ned (all paths) at this control point. d: Vars that \nmight be de.ned (some path) between label l s binder (i.e., l s immediate dominator) and the labelled \ncontrol point. The basic type judgement depends on two type environments: S maps a label to the type \ninformation to which it is statically bound. For a given control point, . maps a free label to the set \nof variables that may have been de.ned on some path between that label s binding point and the given \ncontrol point. S : Lab .Scope \u00d7Defs . : Lab .Defs Scope = P(Ident) The basic type judgement establishing \nthe scope of an annotated term is S, . .tcfg : s. The type judgement is de.ned with the schemas of Figure \n13; the let* and letrec rules are simpli.ed to handle a single bound label, but the generalisation to \nthe full form is straightforward. We extend the type system to handle permutable sequences by extending \nthe s scope types to permit a pair of loop-variable sets (spre sperm ) to specify the type of a permute \nor permute/tail context. When a term has such a type, it means that the term is an element of a permute/tail \nchain. The spre set speci.es the scope that was visible at the start of the chain (and, hence, at each \nsub-term of all permute and permute/tail forms in the chain), while the sperm set speci.es the extra \nscope being accumulated by preceding elements of the chain to be eventually made visible to the term \nat the end of the chain. Examining the rules for the type system, it s not hard to see that it captures \nthe notion of variables that are de.ned on all paths to a term. To express this formally, we .rst extend \nthe type judgement to machine con.gurations, where the semantics is altered to operate upon typed terms. \nWe say that a machine con.guration is valid, written .E, tcfg,if [], [] .tcfg :dom(E). That is, a machine \ncon.guration is valid if its type that is, its scope is satis.ed by the current dynamic environment. \nThe key theorem, then, is a kind of preservation theorem: THEOREM 1. .E,tcfg .E,tcfg . E ' , tcfg ' ..E \n' , tcfg ' . The proof of the theorem is by induction on the justi.cation tree for the typing of the \ntcfg term, in the standard form for type\u00adpreservation proofs. It relies on another straightforward lemma, \nthat label substitutions preserve type; again, this can be shown by induction. From our theorem, and \nthe typing rule for do forms, it follows that if we execute a well-typed term in an initial empty environ\u00adment, \nthat whenever execution reaches a do form, every variable in the form s scope will have a de.nition in \nthe current environment. Thus we have a solid de.nition of static variable scope in terms of Defs = \nP(Ident) our original BDR principle. While beyond the scope of this paper, we can perform a similar formalisation \nto capture the meaning of the d declarations. 10. Type-inference and type-directed translation We infer \nthe type annotations for a complete CFG term by assign\u00ading a type variable to each position in the syntax \nwhere a s or d should be, then recursing once over the tree to collect set-inclusion constraints generated \nby the type schema, then propagating infor\u00admation around the constraints to .nd maximal types. Once we \nhave performed the type inference, it is fairly straight\u00adforward to translate the annotated term to Scheme \ncode. The com\u00adpiler (a Scheme macro) is guided by the s and d annotations. A labelled CFG point is translated \nto a Scheme variable let-bound to a procedure, using the following conventions: We assume the procedure \ns body is closed in a Scheme scope that sees all loop variables in the term s scope.  The procedure \nhas for its parameters all variables that might have been de.ned/updated between the time the procedure \nwas evaluated and the time it is applied. This is exactly the d set for the label.  A (go l) form is \ntranslated to a call to l s Scheme procedure, passing the current values of l s d set as parameters. \n To prevent variable capture, the Scheme variables used to name control points in the generated code \nare fresh names. Permutable sequences are compiled by means of maintaining a parallel set of fresh Scheme \nvariables for the loop variables, called shadow variables. If variable x is in the sperm scope of a permute \nchain, then a de.nition of x that happens during execution of a component of the chain is bound to x \ns shadow variable, so that subsequent elements of the chain do not see this binding. At the end of the \nchain, all the newly introduced scope is exposed by binding the newly introduced variables to their shadow-variable \nvalues. The translation from a typed CFG to pure-functional Scheme code is really just a kind of SSA \nconversion [9]. However, it s much simpler than standard SSA-conversion algorithms a few dozen lines \nof code suf.ce. One reason the translation is so simple is be\u00adcause, as we noted earlier, there is no \nneed to compute dominance trees. The dominance information directly encoded in the syntac\u00adtic structure \nand the type annotations provide all the information needed for the translation. These properties of \nthe CFG language make it an interesting possibility for a compiler intermediate repre\u00adsentation in other \nsettings. 11. Implementation The current implementation is a fairly ambitious Scheme macro, implemented \nusing a mix of high-level R5RS macros [10] and low\u00adlevel Clinger-Rees explicit renaming macros [3]. It \nis comprised of about 4500 lines of highly-commented code (about 2800 lines excluding blank lines and \ncomments). This breaks down, very roughly, as follows: 1000 de.nitions of individual loop clauses 350 \nloop.ltk macro core 115 ltk.cfg macro 280 CFG AST, sexp/AST parsing &#38; unparsing 320 CFG simpli.er \n890 type inference 420 tcfg.scheme compiler2 500 general utilities 2 I actually wrote three such compilers, \nof varying properties. The most complex version is 420 lines; the simplest, 136. Again, about half these \nlines counts are comments or blank lines. Somewhat unusually for a Scheme macro, the CFG-processing code \ndoes not operate on raw s-expressions. Instead, it parses the incoming s-expression into an AST comprised \nof records, then op\u00aderates on the AST. This is just reasonable software engineering: compilers of a certain \ncomplexity need real data structures for their intermediate representations. The entire system is strongly \nmodu\u00adlarised by using the various languages as hinge points. As most of the system is concerned with \ntranslating expressions in one non-Scheme language to another non-Scheme language (e.g., between loop \nclauses and LTK forms), the system makes tremendous use of CPS macros [8], a style of macro useage which \npermits a Scheme macro to target a non-Scheme language. Of the 83 macros de.ned in the source, 75 are \nCPS macros. I expect the line count to increase 50-100% before the pack\u00adage is released, for two reasons. \nFirst, the system works well when the programmer makes no errors. But static semantics and espe\u00adcially \nsyntax errors produce incomprehensible error messages. This is due to the fact that little syntax checking \nis actually programmed into the system; it mostly comes from the pattern-matching machin\u00adery of the various \nCPS macros that process the language. Worse, some errors won t manifest themselves until multiple transforma\u00adtions \nhave happened to the original form. This can be handled, again, by reasonable software engineering common \nto any com\u00adpiler: error checking must be performed as early as possible, and as high in the language \ntower as possible. It is frequently the case with robust, industrial-strength software systems for error-handling \ncode to dominate the line counts; the loop package is no different. Adding the code to provide careful \nsyntax checking and clear er\u00adror messages is tedious but straightforward implementation work; it will \nbe required to turn the initial version into a tool that is gen\u00aderally useable. Second, I have designed \nbut not implemented a facility for de\u00adscribing general accumulators. These are provided by means of BDR-scoped \nmacros, allowing loop-clause writers to implement object-oriented macros that obey a standard object \nprotocol for accumulators. The state of each accumulator is private to the de.n\u00ading loop clause, but \ncan be tightly integrated with the rest of the CFG loop state all the same, providing abstraction and \nmodularity without sacri.cing performance. 12. Quicksort example We ve focussed on the CFG language in \nthis paper, primarily be\u00adcause the underlying framework provided by the CFG language is the chief intellectual \ncontribution of the design. However, before .nishing, we should present at least one real example of \nthe top\u00adlevel loop language in use. Here is in-place vector quicksort, writ\u00adten in Scheme with the loop \npackage: (let recur ((l 0) (r (vector-length v))) (if (> (-rl) 1) (loop (initial (p (pick-pivot l r)) \n(i(-l 1)) (j r)) (subloop (incr i from i) (bind (vi (vector-ref v i))) (while (< vi p))) (subloop (decr \nj from j) (bind (vj (vector-ref v j))) (while (> vj p))) (until (<= j i)) (do (vector-set! v i vj) (vector-set! \nv j vi)) (after (recur l i) (recur (+ j 1) r))))) This example manages to pack a fair amount of control \nstructure into a few lines of code, containing one recursive function and three distinct loops. The loop \nform begins by initially binding three variables: the pivot value p, and the left and right indices of \nthe partition step, i and j. (The pivot selection is performed off-stage by a pick-pivot function.) The \nbody of the loop is comprised of four clauses: two subloop clauses, a do and an until clause. A subloop \nclause is used to perform a nested loop that shares loop variables with its containing loop (this is \neasy to arrange, given that our CFG notation handles general graph structure with no problem). The .rst \nsubloop clause steps i from left to right, skipping over indices whose element is less than the pivot \nvalue. On each iteration of the subloop, we increment i, then bind vi to the element of vector v at that \nindex, then compare vi and the pivot p.(A .ne point: the from keyword causes an incr or decr clause to \nskip the .rst value; the more common case of including the initial value is provided by the alternate \nkeyword :from. A similar variation with the to and to: keywords that provide the .nal value allows a \nprogrammer to specify closed, open, or half-open intervals. Thus the simple mnemonic: including the colon \nincludes the end point; leaving it off, omits the end point.) If vi =p, the subloop is terminated. (Note \none of the features of the loop design: a given loop can have multiple while/until termination tests; \neach test is performed where it occurs in the loop, interleaved with other body clauses.) Thus, the subloop \nterminates with i bound to the leftmost index of v whose element is greater than or equal to p. Similarly, \nthe second subloop steps j from right to left, terminating when j reaches an element of v that is less \nthan or equal to p. The next clause in the main loop body, the until clause, checks to see if the partition \nindices have overlapped; if so, the partition loop is done. If not, the loop continues on to the do clause. \nA do clause is a body element whose embedded Scheme code is simply executed for side effect. This particular \none swaps the two elements at the i and j indices, after which the loop proceeds to the next iteration. \n(Note that the swap code executes in the scope of the vi and vj bindings, since they occur before the \ntermination tests of their respective subloops.) When the partition loop is done, its after clause recurs \non the two segments of the partition. 13. Related work Iteration is one of the fundamental things programmers \ndo; as a result, there is a wealth of related work on de.ning language forms for specifying loops. Haskell \ns list comprehensions [19] are one alternative; Egner s SRFI-42 [6] provides some of its ideas in a call\u00adby-value \nsetting as eager comprehensions. However, SRFI-42 is a more limited design than the loop form presented \nhere. It does not permit the construction of arbitrarily complex control structure for its iterations. \nIt s not clear, for example, how one could use the system to construct more than one result (e.g., a \nlist and some derived summary integer). On the other hand, limited notations can frequently be clearer \nin their restricted domain of applicability. One of the charms of working with systems based on macros \nis their ability to incorporate further elements of specialised syntax. For example, suppose we wanted \nto add a new in-table keyword to the for clause allowing database queries, e.g.: (loop ... (for row-vars \nin-table sql-query) ...) We d like to be able to use some s-expression form of SQL in the query part \nof the driver clause, rather than encoding the query as a string. One bene.t is the increased static \nchecking we get for our uses of the specialised notation [4]; to quote a Perlis aphorism [11], The string \nis a stark data structure and everywhere it is passed there is much duplication of process. It is a perfect \nvehicle for hiding information. Macros are just little compilers, which makes them a powerful tool for \nprogrammers, allowing them to construct software systems by the composition of little languages [13]. \nTo compare, we could not, for example, embed SQL notation into a Haskell comprehension. Egner s comprehension \nsystem, on the other hand, does have this property of linguistic composability, because it is also built \nwith macros in an extensible manner. Water s LetS (later renamed OSS, later renamed simply Se\u00adries ) \nsystem [20, 21, 22, 18, 23] lets one write loops in an ex\u00adplicitly data-parallel style, rather like APL. \nThe package attempts to process the computation in an on-line manner, without allocat\u00ading intermediate \nbuffer structures. However, the reasoning required to ensure on-line processing can be tricky and subtle. \nThis critical element of the design, in fact, went through a somewhat tortured evolution in the history \nof the design, shifting over time from an initial design that was carefully restricted to guarantee on-line \npro\u00adcessing to a more general one that inserts temporary buffers into the produced code where the macro \nis unable to resolve the depen\u00addencies. Another issue with LetS/OSS iterations is that they don t nest. \nAPL [7] itself is an interesting model: it essentially replaces time-like loops with space-like aggregate \noperations on multi\u00addimensional arrays. This is a beautiful model; the cost of its el\u00adegance and simplicity, \nhowever, is the language s complete aban\u00addonment of any attempt to guarantee that computations can be \nper\u00adformed on-line, without allocating potentially enormous intermedi\u00adate collections. There have been \nmany designs for loop packages in the general style of the one presented here. Besides Common Lisp sof.cial \nloop facility [18], there is also the iterate package [1, 2], which is similar, if somewhat cleaner. \nThe top-level loop design presented here traces back to the Yale loop, designed and implemented for Maclisp \nat Yale in the early 1980 s. The Yale loop and the MIT iterate form are quite similar. These Lisp packages \ntypically are not pure-functional, that is, they expand into code that side-effects variables. This renders \nthem unsuitable in language contexts that do not permit variable assignment (i.e., most modern functional \nlanguages), and is a signi.cant barrier to optimisation even for those languages that do. One of the \ncontributions of the framework provided by the CFG language is that it provides a variable-update mechanism \nfor iteration state that nonetheless still permits a purely functional connection to the language with \nwhich it is mutually embedded. This is a subtle design point, worth noting. Classic Lisp loop packages \nall share the same issues with respect to variable scope discussed earlier, as well. 14. Conclusion The \nloop form presented here is an exercise in design that comes from following a single foundational idea: \ndescribing iteration structures using control-.ow graphs, and letting the control-.ow dictate the scoping. \nThe other novel elements of the system the compositional notation for encoding the control-.ow graphs, \nthe concept of permutable sequences, and the use of a type system to express scope all .ow from this \ncore idea. By hewing consis\u00adtently to this concept, we obtain a system that enjoys an underlying conceptual \nintegrity, and this is what enables the framework to be extensible. While the implementation is sizeable, \nprogrammers do not have to write multi-thousand line compilers themselves to take advan\u00adtage of the extensibility \nprovided by the macro-based framework. A programmer who is implementing, for example, a hash-table pack\u00adage, \ncan add an in-hash-table keyword for the loop for clause to his module with about ten lines of code; \nclients of the package can then easily iterate over hash tables with loop forms. By build\u00ading on the \nfoundation provided by the CFG language, loop-clause designers have a simple notational interface to \nthe semantic struc\u00adtures of control and scope that the notation encodes, as well as the (multi-thousand \nline) reasoning engines that process the notation. This is the power of the language towers approach \nto extensibil\u00adity. Note, also, that programmers typically do not need to be aware of the low-level scoping \ndetails of the CFG terms to which their loop forms translate. This is a crucial design criterion. It \ns not enough simply to have a scoping principle that is well de.ned in some formal sense. If a programmer \nwere required to mentally construct the full control-.ow graph for a given loop and then solve the associated \ndominance equations in his head, just to resolve a scope issue, the system would be well de.ned, but \nit would not be a useful tool for humans. Instead, loop programmers do need to be aware of the general \neight-block loop skeleton, because the scoping of various parts of loop clauses can be described in terms \nof this structure. For example, a variable .rst de.ned in the top-guard block of a loop is visible to \nthe loop s body block, but not its .nal block. In short, the large-grain control structure of the loop \nskeleton constrains the scoping of the loop clauses to simple structures. Again (to stress the design \nmessage of this paper), this is no accident: the CFG semantics was carefully designed to allow loop-clause \ndesigners to provide scope and control semantics for their clauses in terms of this simple, large-scale \nstructure.   Acknowledgements In August 2004, Olivier Danvy convened a group of researchers at the \nUniversity of \u00b0 Aarhus for an informal workshop on aspects of continuations and continuation-passing \nstyle representations. This work was stimulated and re.ned by discussion with the other participants: \nAndrzej Filinski, Mayer Goldberg, Julia Lawall and Olivier. In Boston, similar discussions with Alan \nBawden, Matthias Felleisen, Greg Morrisett and Mitch Wand over extended periods of time helped push this \nwork forwards. I am also grateful to Olivier for the invitation to spend the Fall of 2004 visiting at \nthe University of \u00b0 Arhus, which provided me the perfect balance of quiet re.ection and collegial interaction \nneeded to bring these ideas to completion. The CFG design was .rst presented as a talk at Daniel P. Fried\u00adman: \nA Celebration in December 2004; during the years that I ve been thinking about the de.nition of loop \nforms, I was guided by a remark Dan once made to me about the design of language forms: Scope is everything. \nThe entire structure of the CFG language proceeds from the focus provided by that single remark. The \ntop-level design of the loop form presented in this paper was strongly in.uenced by the Yale loop, designed \nfor Maclisp by the Tools Group graduate students and faculty in Yale s Computer Science Department in \nthe early 1980 s; among others, John Ellis and Chris Riesbeck were primary contributors to that design. \nI have drawn on a long email message that Ellis wrote during that time to make the case for loop forms \nand against low-level tail recursion in Section 1. References [1] Jonathan Amsterdam. The Iterate manual. \nAI Memo 1236, MIT AI Lab, October 1990. [2] Jonathan Amsterdam. Don t Loop, Iterate. Working Paper 324, \nMIT AI Lab. [3] William Clinger and Jonathan Rees. Macros that work. In Pro\u00adceedings of the ACM Conference \non Principles of Programming Languages, pp. 155 162, 1991. [4] Ryan Culpepper, Scott Owens and Matthew \nFlatt. Syntactic ab\u00adstraction in component interfaces. In Proceedings of the Fourth International Conference \non Generative Programming and Com\u00adponent Engineering (GPCE 05), September 2005, Tallinn, Estonia. [5] \nEdsger W. Dijkstra. GOTO statement considered harmful. Letter to the Editor. Comm. ACM 11(3), March 1968. \n[6] Sebastian Egner. Eager comprehensions in Scheme: The design of SRFI-42. In Proceedings of the ACM \nSIGPLAN 2005 Workshop on Scheme and Functional Programming (Scheme 2005), September 2005, Tallinn, Estonia. \nSee also http://srfi.schemers.org/ srfi-42/. [7] Leonard Gilman and Allen J. Rose. Apl: An Interactive \nApproach. John Wiley &#38; Sons Inc., Third edition, January 1984. [8] Erik Hilsdale and Daniel P. Friedman. \nWriting macros in continuation-passing style. In Proceedings of the Workshop on Scheme and Functional \nProgramming (Scheme 2000), pages 53 61, Montreal, Canada September 2000. Rice Technical Report 00\u00ad 368. \nSeptember 2000. [9] Richard A. Kelsey. A correspondence between continuation\u00adpassing style and static \nsingle assignment form. In Proceedings of the ACM SIGPLAN Workshop on Intermediate Representations, SIGPLAN \nNotices 30(3), pages 13 22, 1995. [10] Richard Kelsey, William Clinger and Jonathan Rees (Editors). Revised5 \nreport on the algorithmic language Scheme. SIGPLAN Notices, 1998. [11] Alan J. Perlis. Epigrams on programming. \nSigplan 17 #9, Septem\u00adber 1980. [12] Kent M. Pitman. Common Lisp Issue LOOP-INITFORM-ENVIRONMENT:PARTIAL-INTERLEAVING-VAGUE. \nMarch 1991. http://www.lisp.org/HyperSpec/Issues/ iss222-writeup.html [13] A universal scripting framework. \nOlin Shivers. In Concurrency and Parallelism, Programming, Networking, and Security, Lecture Notes in \nComputer Science #1179, pages 254 265, Editors Joxan Jaffar and Roland H. C. Yap, 1996, Springer. [14] \nGuy L. Steele Jr. and Gerald Jay Sussman. LAMBDA: The ulti\u00admate imperative. AI Memo 353, MIT AI Lab, \nMarch 1976. [15] Guy L. Steele Jr. LAMBDA: The ultimate declarative. AI Memo 379, MIT AI Lab, November \n1976. [16] Guy L. Steele Jr.. Debunking the expensive procedure call myth. AI Memo 443, MIT AI Lab, October \n1977. [17] Guy L. Steele Jr. RABBIT: A Compiler for SCHEME. Masters Thesis, MIT AI Lab, May 1978. Technical \nReport 474. [18] Guy L. Steele Jr. Common Lisp: The Language. Digital Press, Maynard, Mass., second edition \n1990. [19] Philip Wadler. List comprehensions (Chapter 7). In The Imple\u00admentation of Functional Programming \nLanguages, Editor Simon L. Peyton Jones, Prentice Hall, 1987. [20] Richard C. Waters. LetS: An expressional \nloop notation. AI Memo 680A, MIT AI Lab, February 1983. [21] Richard C. Waters. Obviously synchronizable \nseries expressions: Part I: User s manual for the OSS macro package. AI Memo 958A, MIT AI Lab, March \n1988. [22] Richard C. Waters. Obviously synchronizable series expressions: Part II: Overview of the theory \nand implementation. AI Memo 959A, MIT AI Lab, March 1988. [23] Richard C. Waters. Automatic transformation \nof series expressions into loops. ACM Transactions on Programming Languages and Systems, 13(1):52 98, \nJanuary 1991. See also AI Memos 1082 and 1083, MIT AI Lab, 1989.  \n\t\t\t", "proc_id": "1086365", "abstract": "Writing loops with tail-recursive function calls is the equivalent of writing them with goto's. Given that loop packages for Lisp-family languages have been around for over 20 years, it is striking that none have had much success in the Scheme world. I suggest the reason is that Scheme forces us to be precise about the scoping of the various variables introduced by our loop forms, something previous attempts to design ambitious loop forms have not managed to do.I present the design of a loop package for Scheme with a well-defined and natural scoping rule, based on a notion of control dominance that generalizes the standard lexical-scope rule of the &#955;-calculus. The new construct is powerful, clear, modular and extensible.The loop language is defined in terms of an underlying language for expressing control-flow graphs. This language itself has interesting properties as an intermediate representation.", "authors": [{"name": "Olin Shivers", "author_profile_id": "81100129912", "affiliation": "Georgia Institute of Technology", "person_id": "PP39028876", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086368", "year": "2005", "article_id": "1086368", "conference": "ICFP", "title": "The anatomy of a loop: a story of scope and control", "url": "http://dl.acm.org/citation.cfm?id=1086368"}