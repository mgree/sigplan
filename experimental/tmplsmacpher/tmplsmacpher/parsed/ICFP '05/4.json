{"article_publication_date": "09-12-2005", "fulltext": "\n Recursive Type Generativity Derek Dreyer Toyota Technological Institute at Chicago dreyer@tti-c.org \nAbstract Existential types provide a simple and elegant foundation for un\u00adderstanding generative abstract \ndata types, of the kind supported by the Standard ML module system. However, in attempting to extend \nML with support for recursive modules, we have found that the tra\u00additional existential account of type \ngenerativity does not work well in the presence of mutually recursive module de.nitions. The key problem \nis that, in recursive modules, one may wish to de.ne an abstract type in a context where a name for the \ntype already exists, but the existential type mechanism does not allow one to do so. We propose a novel \naccount of recursive type generativity that resolves this problem. The basic idea is to separate the \nact of gener\u00adating a name for an abstract type from the act of de.ning its under\u00adlying representation. \nTo de.ne several abstract types recursively, one may .rst forward-declare them by generating their names, \nand then de.ne each one secretly within its own de.ning expres\u00adsion. Intuitively, this can be viewed \nas a kind of backpatching se\u00admantics for recursion at the level of types. Care must be taken to ensure \nthat a type name is not de.ned more than once, and that cycles do not arise among transparent type de.nitions. \nIn contrast to the usual continuation-passing interpretation of existential types in terms of universal \ntypes, our account of type generativity suggests a destination-passing interpretation. Brie.y, instead \nof viewing a value of existential type as something that creates a new abstract type every time it is \nunpacked, we view it as a function that takes as input a pre-existing unde.ned abstract type and de.nes \nit. By leaving the creation of the abstract type name up to the client of the existential, our approach \nmakes it signi.cantly easier to link abstract data types together recursively. Categories and Subject \nDescriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; D.3.3 [Programming Languages]: \nLanguage Constructs and Features Recursion, Ab\u00adstract data types, Modules; F.3.3 [Logics and Meanings \nof Pro\u00adgrams]: Studies of Program Constructs Type structure General Terms Languages, Theory Keywords \nType systems, abstract data types, recursion, genera\u00adtivity, recursive modules, effect systems Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 05 September \n26 28, 2005, Tallinn, Estonia. Copyright c . 2005 ACM 1-59593-064-7/05/0009. . . $5.00. 1. Introduction \nRecursive modules are one of the most frequently requested exten\u00adsions to the ML languages. After all, \nthe ability to have cyclic de\u00adpendencies between different .les is a feature that is commonplace in mainstream \nlanguages like C and Java. To the novice program\u00admer especially, it seems very strange that the ML module \nsystem should provide such powerful mechanisms for data abstraction and code reuse as functors and translucent \nsignatures, and yet not allow mutually recursive functions and data types to be broken into sepa\u00adrate \nmodules. Certainly, for simple examples of recursive modules, it is dif.cult to convincingly argue why \nML could not be extended in some ad hoc way to allow them. However, when one considers the semantics \nof a general recursive module mechanism, one runs into several interesting problems for which the right \nsolutions are far from obvious. One problem involves the interaction of recursion and com\u00adputational \neffects. The evaluation of an ML module may in\u00advolve impure computations such as I/O or the creation \nof mu\u00adtable state. Thus, if recursion is introduced at the module level, it appears necessary to adopt \na backpatching semantics of recur\u00adsion (in the style of Scheme s letrec construct) in order to en\u00adsure \nthat the effects within recursive module de.nitions are only performed once. Under such a semantics, \na recursive de.nition letrec X =M in ... is evaluated by (1) binding X to an unini\u00adtialized ref cell, \n(2) evaluating M to a value V, and (3) backpatching the contents of X s cell with V, thereby tying the \nrecursive knot. As a matter of both methodology and ef.ciency, it is desirable to know statically that \nthis recursive de.nition is well-founded, i.e., that M will evaluate to V without requiring the value \nof X prematurely. In previous work [4], we studied this problem in detail and proposed a type-based approach \nto guaranteeing well-founded recursion. In this paper, we focus on a second, orthogonal problem with \nre\u00adcursive modules that we and other researchers have struggled with. This problem is of particular importance \nand should be of interest to a general audience because it concerns the interaction of two funda\u00admental \nconcepts in programming, recursion and data abstraction, and it is possible to understand and explore \nthe problem indepen\u00addently of modules. (In fact, that is precisely what we are going to do later in the \npaper.) To begin, however, we will use some infor\u00admal examples in terms of ML modules as a way of illustrating \nthe problem. 1.1 Mutually Recursive Abstract Data Types Suppose we want to write two mutually recursive \nML modules A and B, as shown in Figure 1. Module A (resp. B) de.nes a type t (resp. u) and a function \nf (resp. g) among other components. It is sealed with a signature SIGA (X) (resp. SIGB (X)) that hides \nthe de.nition of its type component.1 Note that the type of the function 1 We make use of parameterized \nsignatures here as a matter of syntactic convenience, although ML does not currently support them. signature \nSIGA(X) = sig type t valf: t->X.B.u*t ... end signature SIGB(X) = sig type u val g :X.A.t -> u* X.A.t \n... end signature SIG(X) = sig structure A : SIGA(X) structure B : SIGB(X) end structure rec X :> SIG(X) \n= struct structure A :> SIGA(X) = struct type t= int fun f(x:t) :X.B.u * t= let val (y,z) = X.B.g(x+3) \n(* Error 1 *) in (y,z+5) end (* Error 2 *) ... end structure B :> SIGB(X) = struct type u = bool fun \ng (x:X.A.t) : u * X.A.t = ...X.A.f(...)... ... end end Figure 1. Mutually Recursive Abstract Data Types \ncomponent in each module refers to the abstract type provided by the other module. The code here is clearly \ncontrived e.g., A.t and B.u are im\u00adplemented as int and bool but it serves to concisely demonstrate the \nkind of type errors that can arise very easily due to the interac\u00adtion of recursion and abstract types. \nThe .rst type error comes in the .rst line of the body of A.f. The function takes as input a vari\u00adable \nx of type t (which is de.ned to be int), and makes a recursive call to the function X.B.g, passing it \nx+3. The error arises because the type of X.B.g is X.A.t -> X.B.u * X.A.t, and X.A.t is not equivalent \nto int. To see this, observe that the variable X is bound with the signature SIG(X), whose A.t component \nis speci\u00ad.ed opaquely.2 The second type error, appearing in the next line of the same function, is similar. \nThe value z returned from the call to X.B.g has type X.A.t, but the function attempts to use z as if \nit were an integer. Both of these type errors are of course symptoms of the same problem: Alice, the \nprogrammer of module A, knows that X.A.t is implemented internally as int, because she is writing the \nimple\u00admentation! Yet this fact is not observable from the signature of X. The only simple way that has \nbeen proposed to address this prob\u00adlem is to reveal the identity of A.t in the signature SIGA(X) as transparently \nequal to int. This is not really a satisfactory solu\u00adtion, though, since it exposes the identity of A.t \nto the implementor of module B and essentially suggests that we give up on trying to impose any data \nabstraction within the recursive module de.nition. A more complex suggestion would be that we change \nthe way that recursive modules are typechecked. Intuitively, when we are 2 Incidentally, you may wonder \nhow it can be legal for the signature X is bound with to refer to X. This is achieved through the use \nof recursively dependent signatures, which were proposed by Crary, Harper and Puri [3] in theory, and \nimplemented by Russo [19] and Leroy [13] in practice. Subject to certain restrictions, they are not semantically \nproblematic, but they are beyond the scope of this paper. signature ORDERED = sig type t val compare \n: t * t -> order end signature HEAP = sig type item; type heap; val insert : item * heap -> heap ... \nend functor MkHeap : (X : ORDERED) -> HEAP where type item = X.t structure rec Boot : ORDERED = struct \ndatatype t = ...Heap.heap... fun compare (x,y) = ... end and Heap : (HEAP where type item = Boot.t) = \nMkHeap(Boot) Figure 2. Bootstrapped Heap Example typechecking the body of module A, we ought to know \nthat X.A.t is int, but we ought not know anything about X.B.u. When we are typechecking the body of module \nB, we ought to know that X.B.u is bool, but we ought not know anything about X.A.t. Additionally, when \ntypechecking B, we ought to be able to observe that a direct hierarchical reference to A.t is interchangeable \nwith a recursive reference to X.A.t. In the case of the module from Figure 1, such a typechecking algorithm \nseems fairly straightforward, but it becomes much more complicated if the recursive module body contains, \nfor instance, nested uses of opaque sealing. It is certainly possible to de.ne an algorithm that works \nin the general case the author s Ph.D. thesis formalizes such an algorithm [5] but it is not a pretty \nsight. Furthermore, we would really like to be able to explain what is going on using a type system, \nnot just an algorithm. 1.2 Recursion Involving Generative Functor Application Figure 2 exhibits another \ncommonly desired form of recursive module, one that is in some ways even more problematic than the one \nfrom Figure 1. In this particular example, the goal is to de.ne a recursive data type Boot.t of so-called \nbootstrapped heaps, a data structure proposed by Okasaki [17]. The important feature of bootstrapped \nheaps (for our purposes) is that they are de.ned recursively in terms of heaps of themselves, where heaps \nof themselves are created by applying the library functor MkHeap to the Boot module. The problem with \nthis de.nition, at least in the case of Standard ML semantics [14], is that functors in SML behave generatively, \nso each application of MkHeap produces a fresh abstract heap type at run time. The way this is typically \nmodeled in type theory is by treating the return signature of MkHeap as synonymous with an existential \ntype. Consequently, while Boot.t must be de.ned be\u00adfore MkHeap(Boot) can be evaluated, the type Heap.heap \nwill not even exist until MkHeap(Boot) has been evaluated and un\u00adpacked. It does not make sense in the \nML type system to de.ne Boot.t in terms of a type (Heap.heap) that does not exist yet. The usual solution \nto this problem is to assume that MkHeap is not generative, but rather applicative [12]. Under applicative \nfunc\u00adtor semantics, MkHeap(Boot) is guaranteed to produce the same heap type every time it is evaluated, \nand thus the de.nition of Boot.t is allowed to refer to the type MkHeap(Boot).heap stati\u00adcally, without \nhaving to evaluate MkHeap(Boot) .rst. This solution is certainly sensible if one is designing a recursive \nmodule exten\u00adsion to O Caml [11], for O Caml only supports applicative func\u00adtors. There are good reasons, \nhowever, for supporting generative functors. Their interpretation in type theory is simpler than that \nof applicative functors, and they provide stronger abstraction guaran\u00adtees that are desirable in many \ncases.3 It seems unfortunate that MkHeap is required to be applicative. 1.3 Overview Our exposition \nthus far begs the question: Is recursion fundamen\u00adtally at odds with type generativity? In this paper, \nwe will argue that the answer is no. We propose a novel account of type abstraction that resolves the \nproblems encountered in the above recursive mod\u00adule examples and provides an elegant foundation for understanding \nhow recursion can coexist peacefully with generativity. The basic idea is to separate the act of generating \na name for an abstract type from the act of de.ning its underlying representation. To de.ne several abstract \ntypes recursively, one may .rst forward\u00addeclare them by generating their names, and then de.ne each one \nsecretly within its own de.ning expression. Intuitively, this can be viewed as a kind of backpatching \nsemantics for recursion at the level of types! The upshot is that there is a unique name for each abstract \ntype, which is visible to everyone (within a certain scope), but the identity of each abstract type is \nonly known inside the term that de.nes it. This is exactly what was desired in both of the recursive \nmodule examples discussed above. While our new approach to type generativity is operationally quite different \nfrom existing approaches, it is fundamentally com\u00adpatible with the traditional interpretation of ADT \ns in terms of exis\u00adtential types. The catch is that, while existential types are typically understood \nvia the continuation-passing Church encoding in terms of universals4, we offer an alternative destination-passing \ninterpre\u00adtation. Brie.y, instead of viewing a value of existential type .a. A as something that creates \na new abstract type every time it is un\u00adpacked, we view it as a function that takes as input a pre-existing \nunde.ned type name \u00df and de.nes it, returning a value of type A (with \u00df substituted for a). How the function \nhas de.ned \u00df,how\u00adever, we do not know. By leaving the creation of the abstract type name \u00df up to the \nclient of the existential, our approach makes it signi.cantly easier to link abstract data types together \nrecursively. The rest of the paper is structured as follows. In Section 2, we discuss the details of \nour approach informally, and give examples to illustrate how it works. In Section 3, we de.ne a type \nsystem for re\u00adcursive type generativity as a conservative extension of System F.. In order to ensure \nthat abstract types do not get de.ned more than once, we treat type de.nitions as a kind of effect and \ntrack them in the manner of an effect system [9, 22]. The intention is that this type system may eventually \nserve as the basis of a recursive mod\u00adule language. In Section 4, we explore the expressive power of \nour destination-passing interpretation of ADT s. Finally, in Section 5, we discuss related work, and \nin Section 6, we conclude.  2. The High-Level Picture We will now try to paint a high-level picture \nof how our approach to recursive type generativity works. The easiest way to understand is by example, \nso in Section 2.1, we use the recursive module examples from Section 1 as a way of introducing the key \nconstructs of our language. In particular, we show how those examples would be encoded in our language \nand why, under this new encoding, they typecheck. Then, in Section 2.2, we also show how our approach \nmakes it possible to support separate compilation of recursive abstract data types. Lastly, in Section \n2.3, we discuss some of the subtler issues that we encounter in attempting to prevent bad cycles in type \nde.nitions. 3 For more details, see the discussion in Dreyer, Crary and Harper [6]. 4 See Section 2.2 \nfor details. SIGA = .a : T..\u00df : T. {f:a . \u00df \u00d7 a,...}SIGB = .a : T..\u00df : T. {g:a . \u00df \u00d7 a,...}SIG = .a : \nT..\u00df : T. {A:SIGA (a)(\u00df), B:SIGB (a)(\u00df)} new a . T,\u00df . T in letrec X : SIG(a)(\u00df) = {A =(let () = a := \nint in {f = ...}) : SIGA (a)(\u00df) defines a, B =(let () = \u00df := bool in {g = ...}) : SIGB (a)(\u00df) defines \n\u00df} in ... Figure 3. New Encoding of Example from Figure 1 2.1 Reworking the Examples Figure 3 shows \nour new encoding of the recursive module exam\u00adple from Figure 1. The .rst thing to notice here is that \nwe have dispensed with modules. SIGA, SIGB and SIG are represented here via the well-known encoding of \nML signatures as type operators in System F.. The idea is simply to view the types of a signature s value \ncomponents as being parameterized over the signature s ab\u00adstract type components. Correspondingly, the \nML feature of using where type to add type de.nitions to signatures is encodable in F. by type-level \nfunction application.5 (We employ this encoding here merely so that we can study the interaction of recursion \nand data abstraction at the foundational level of F., with which we as\u00adsume the reader is familiar. In \nthe future, we intend to scale the ideas of this paper to a more easily programmable language of re\u00adcursive \nmodules.) Starting on the fourth line, however, we see something that is not standard. (The underlined \nportions of the code indicate new features that are not part of F..) What the new declaration does is \ncreate two new type variables a and \u00df of kind T, the kind of base types. Throughout this example, you \ncan think of a as standing for A.t, and \u00df as standing for B.u, in the original example of Figure 1. What \ndoes it mean to create a new type variable ? Intuitively, you can think of it much like creating a reference \ncell in memory. Imagine that during the execution of the program you maintain a type store, mapping locations \n(represented by type variables) to types. Eventually, each location will get .lled in with a type, but \nwhen a type memory cell is .rst created (by the new construct), its contents are uninitialized.6 Formally \nspeaking, what the new declaration does is to insert a and \u00df into the type context with a special binding \nof the form a . T, which indicates that they have not yet been de.ned. We refer to such type variables \nas writable. Next, we make use of a letrec construct to de.ne A and B. For simplicity, the letrec construct \nemploys an unrestricted (i.e., potentially ill-founded) backpatching semantics for recursion.7 Speci.cally, \nwe allocate an uninitialized ref cell X in memory, whose type is rec(SIG(a)(\u00df)). In order to use X within \nthe body of the recursive de.nition i.e., in order to get a value of type SIG(a)(\u00df) without the rec one \nmust .rst dereference the 5 See Jones [10] for more examples of this encoding. Also, this is essentially \nhow the De.nition of SML interprets signatures [14]. 6 In fact, this is exactly how we are going to model \ntype creation in the dynamic semantics of our language in Section 3.4. 7 In principle, we believe it \nshould be straightforward to incorporate static detection of ill-founded recursion [4] into the present \ncalculus, but we have not yet attempted it. ORDERED = .a: T.{ compare:a\u00d7 a. order} HEAP = .a: T..\u00df: T.{ \ninsert:a\u00d7 \u00df. \u00df,...} \u00df. HEAPGEN = .a: T.. \u00df. T.unit -. HEAP(a)(\u00df) MkHeap : . a. T.ORDERED(a) . HEAPGEN(a) \nnew a. T,\u00df. T in letrec X : { Boot:ORDERED(a), Heap:HEAP(a)(\u00df)} = let Boot = (let () = a : (...\u00df...) \nin { compare = ...} )in let Heap = MkHeap [a](Boot)[\u00df]() in { Boot=Boot, Heap=Heap} in ... Figure 4. \nNew Encoding of Example from Figure 2 memory location by writing fetch(X). This fetch operation must \ncheck whether X s contents have been initialized and, if not, raise a run-time error. Finally, when the \nbody of the letrec is .nished evaluating, the resulting value (of type SIG(a)(\u00df)) is backpatched into \nthe location X. (There are good reasons to require the derefer\u00adencing of X to be explicit, as we will \nsee in Figure 5.) Now for the de.nition of module A: The .rst thing we do here is to backpatch the type \nname a with the de.nition int. The use of := notation is appropriate because at run time we can think \nof this operation as modifying the contents of the a location in the type store. At compile time, it \nresults in a change to the type context so that the typechecking of the remainder of A is done with the \nknowledge that a is equal to int. As a result, the type errors from Figure 1 disappear! Once we have \n.nished typechecking A, however, we want to hide the knowledge that a is int from the rest of the program. \nWe achieve this in the next line by sealing the de.nition of A. Al\u00adthough it is a bit hard to tell from \nFigure 3, the sealing construct has the form e : t defines a , where in this case t is SIGA(a)(\u00df). The \nsealing construct does two things simultaneously: it exports e at the type t, and it removes the de.nition \nof afrom the type con\u00adtext in which the rest of the program is typechecked.8 While the hiding of a s \nde.nition is obviously important, it is critical to un\u00adderstand that the ascription of the type t is \njust as important. In the case of module A, it is the type ascription SIGA(a)(\u00df) that ensures that A.f \nis exported at the type a. \u00df\u00d7 aand not, say, at the type int . \u00df\u00d7 int. Finally, there is the de.nition \nof B, which works similarly to the de.nition of A. Voil`a! To summarize, by distinguishing the point \nat which aand \u00df are created from the points at which they are de.ned, we have made it possible for all \nparties to refer to these types by the same names, but also for the underlying representation of each \ntype to be speci.ed and made visible only within its own de.ning expression. Let us move on to Figure \n4, which shows our new encoding of the bootstrapped heap example. As in the previous example, we de\u00ad.ne \ntwo abstract types here, aand \u00df, but now astands for Boot.t, and \u00dffor Heap.heap. The signatures ORDERED \nand HEAP are in pa\u00adrameterized form as expected, with the former parameterized over the type a of items \nbeing compared, and the latter parameterized over both the item type aand the heap type \u00df. The most unusual \n(and important) part of this encoding is the type that we require for the MkHeap functor. Under the standard \n8 Note that sealing is purely a compile-time notion at run time, the de.ni\u00adtion of a is not actually \nremoved from the type store. encoding of generative functors into F., we would expect MkHeap to have \nthe type . a: T.ORDERED(a) .. \u00df: T.HEAP(a)(\u00df) The type shown in Figure 4 differs from our expectations \nin two ways. First, while a is universally quanti.ed, the quanti.cation is written a. T. The reason for \nthis has to do with avoiding cycles in transparent type de.nitions, and we will defer explanation of \nit until Section 2.3. For the moment, read a. T as synonymous with a: T. Second, MkHeap s result type, \nHEAPGEN(a), is not an existential, but some weird kind of universal! \u00df. Indeed, HEAPGEN(a) = . \u00df. T.unit \n-. HEAP(a)(\u00df) is a uni\u00adversal type, but a very special one. Speci.cally, a function of this type requires \nits type argument to be a type variable that has not yet been de.ned (hence, the notation . \u00df. T). Moreover, \nwhen the function is applied, it will not only return a value of type HEAP(a)(\u00df), but also de.ne \u00df in \nthe process. We write \u00df. on the arrow type to indicate that the application of the function engenders \nthe effect of de.ning \u00df, but how it de.nes \u00df we cannot tell. The reason for de.ning HEAPGEN(a) in this \nfashion is that it allows us to come up with a name (\u00df) for the Heap.heap type ahead of time, before \nthe MkHeap functor is applied. In this way, it is possible for the de.nition of a(i.e., Boot.t) to refer \nto \u00df before \u00df has actually been de.ned. As we explained in Section 1.2, this is something that is not \npossible under the ordinary interpretation of HEAPGEN(a) as an existential type. The only other point \nof interest in this encoding is that a is de.ned by a new kind of assignment (: ). One can think of this \nassignment as analogous with datatype de.nitions in SML, just as := is analogous with transparent type \nde.nitions (type synonyms). The de.nition of a in Boot does not change the fact that a is an abstract \ntype, but it introduces fold and unfold coercions that allow one to coerce back and forth between a and \nits underlying de.nition. This form of type de.nition is necessary in order to break up cycles in the \ntype-variable dependency graph. We discuss this point further in Section 2.3. 2.2 Destination-Passing \nStyle and Separate Compilation The strange new universal type that we used to de.ne HEAPGEN(a) in the \nlast example can be viewed as a kind of existential type in sheep s clothing. Under the usual Church \nencoding of existen\u00adtial types in terms of universals, . a:K.t can be understood as shorthand for . \u00df: \nT.(. a:K.t . \u00df) . \u00df. This is quite similar to a. . a. K.unit -. t in the sense that a function of either \ntype has some type constructor aof kind K and some value of type t hidden inside it, but the function \ns type won t tell you what ais. The dif\u00adference is that the Church encoding is a function in continuation\u00adpassing \nstyle (CPS), whereas our new encoding is a function in destination-passing style (DPS) [24]. In Section \n4.2, we will make the DPS encoding of existentials precise. So, one may wonder, if our DPS universal \ntype is really an existential in disguise, why don t we just write, say, . a. K.A a. instead of . a. \nK.unit -. t? Why bother with the unit? The answer is that in some cases we want to write a function of \ntype a. . a. K.t1 -. t2 where a . FV(t1) that is, a function that takes as input a writable type name \na, together with a value whose type depends on a. In typical programming this does not come up often, \nbut with recursive modules it arises naturally, especially in the context of separate compilation. Figure \n5 illustrates such a situation. The goal here is to allow the recursive modules A and B from Figure 3 \nto be compiled separately. We have put the implementations of A and B inside of two separate functors \nSeparateA and SeparateB, represented as polymorphic functions. SeparateA takes \u00df(i.e., B.u) as its .rst \na. SeparateA : . \u00df: T.. a. T.rec(SIG(a)(\u00df)) -. SIGA(a)(\u00df) = .\u00df: T..a. T..X : rec(SIG(a)(\u00df)).... \u00df. SeparateB \n: . a: T.. \u00df. T.rec(SIG(a)(\u00df)) -. SIGB(a)(\u00df) = .a: T..\u00df. T..X : rec(SIG(a)(\u00df)).... new a. T,\u00df. T in letrec \nX : SIG(a)(\u00df) = { A = SeparateA [\u00df][a](X), B = SeparateB [a][\u00df](X)}in ... Figure 5. Separate Compilation \nof A and B from Figure 3 argument, a (i.e., A.t) as its second argument, and the recursive module variable \nX as its third argument. The type of SeparateA employs a DPS universal type to bind abecause SeparateA \nwants to take a writable A.t and de.ne it. Note, however, that \u00df is bound normally as \u00df: T.(SeparateB \nof course does the opposite, because it wants to de.ne \u00df, not a.) The important point here is that the \ntype of the argument X refers to both aand \u00df and therefore cannot be moved outside of the DPS universal.9 \nIf all we had was a a. DPS universal of the form . a. K.unit -. t, we would have no way of typing SeparateA \nand SeparateB . If it is so important to be able to write a function that takes a value argument after \nan a. K argument, it is natural to ask why we do not just offer two separate type constructs, . a. K.t \nand a. a. t1 -. t2, of which . a. K.t1 -. t2 would be the composition. The former construct would require \nits argument to be a writable variable, and the latter would be a standard sort of effectful function \ntype, in this case the effect being the de.nition of some externally\u00adbound type variable a. The reason \nwe do not divide up the DPS universal type in this way is that such a division would result in serious \ncomplications for a. our type system. The main complication is that, while t1 -. t2 looks like a standard \nsort of effect type, the effect in question is highly unusual. In particular, if f were a function of \nthat type, it could only be applied once because, for soundness purposes, we require that a type variable \na can only be de.ned once. Another a. way of saying this is that the type t1 -. t2 only makes sense while \nais writable. Meta-theoretically speaking, this becomes problematic from the point of view of de.ning \ntype substitution. If at some point in the program agets de.ned as t, and a s binding in the context \nchanges correspondingly from a. T to a: T = t, then we should be able to substitute t for free occurrences \nof a. But substituting t for a a. in t1 -. t2 does not make sense. In contrast, our type system has the \nproperty that well-formed types stay well-formed, regardless of whether their free type variables go \nfrom being writable to being de.ned. 2.3 Avoiding Cycles in Transparent Type De.nitions We have now \npresented all the key constructs in our language and shown how they can be used to support recursive \nde.nitions of generative abstract data types. In order to make this approach work, there are two points \nof complexity that our type system has to deal with. One involves making sure that writable type variables \nget de.ned once and only once. This is a kind of linearity property 9 Also important to the success of \nthis encoding is the fact that X must be explicitly dereferenced. Otherwise, the references to X in the \nlinking module would result in a run-time error. See Dreyer [4] for more discussion of this issue. and \nit is not fundamentally dif.cult to track using a type-and-effect system, as we explain in Section 3. \nThe other point concerns our desire to avoid cycles in trans\u00adparent type de.nitions. While our language \nis designed to permit recursive de.nitions of abstract types, we require that every cy\u00adcle in the type \ndependency graph must go through a datatype, i.e., one that was de.ned by a: A.10 We make this restriction \nbecause we want to keep the de.nition of type equivalence sim\u00adple. If we were able to de.ne a:= \u00df\u00d7 \u00df \nand \u00df:= a\u00d7 a, then we would need to support some form of equi-recursive types [1, 3]. In fact, since \nwe allow de.nitions of type constructors of higher kind, we would need to support equi-recursive type \nconstructors, whose equational theory is not fully understood. The mechanism we employ to guarantee that \nno transparent type cycles arise is slightly involved, but the reasoning behind it is straightforward \nto understand. Let us step through it. First of all, if ais de.ned by a: A, then clearly no restrictions \nare necessary. If, however, a is de.ned transparently by a:= A, then we must require at the very least \nthat A does not depend on a. By depend on, we mean that if all known type synonyms were expanded out, \nthen awould not appear in the free variables of the expanded A. Unfortunately, in the presence of data \nabstraction, this restric\u00adtion alone is not suf.cient. Suppose, for instance, that in our exam\u00adple from \nFigure 3 the type variable awere de.ned by A and \u00dfby B (instead of by int and bool). The de.nition of \naand the de.nition of \u00df each occur in contexts where the other variable is considered abstract. Consequently, \nthe restriction that A not depend on aand B not depend on \u00df would not prevent A from depending on \u00df and \nB from depending on a. How can our type system ensure that each de.nition does not contribute to a transparent \ncycle without peek\u00ading at what the other one is (and hence violating abstraction)? A simple, albeit conservative, \nsolution to this dilemma is to de\u00admand that, if ais de.ned by a:= A, then A may not depend on any abstract \ntype variables except those that are known to be datatypes. We will say that a type A obeying this restriction \nis stable. While this approach does the trick, it is rather limiting. For example, in ML, it is common \nto de.ne a type transparently in terms of an abstract type imported from another module (which may or \nmay not be known to be a datatype). The stability restriction, however, would prohibit such a type de.nition \ninside a recursive module. Therefore, to make our type system less draconian, we employ a modi.ed form \nof the above conservative solution, in which the restriction on transparent de.nitions is relaxed in \ntwo ways. First, in order to permit transparent de.nitions to depend on abstract types that are not datatypes, \nwe expand the notion of stability by allowing type variables to be considered stable if they are bound \nin the context as such. A stable type variable, bound as a. K, may only be instantiated with other stable \ntypes. We also introduce a new form of universal type, . a. K.t, describing functions whose type arguments \nmust be stable. We have already seen an instance where a stable universal is needed, namely in the type \nof the MkHeap functor from Figure 4. The reason for quantifying the item type a as a stable variable \nis that it enables the MkHeap functor to de.ne the heap type \u00df transparently in terms of a (e.g., \u00df:= \na list). If a were only bound as a: T, then \u00df would have to be de.ned as a datatype in order to ensure \nstability. Since MkHeap requires its item argument to be stable, it is imperative that the actual type \na to which it is applied be stable. In the case of the Boot module, ais de.ned as a datatype, so all \nis well. The second way in which we relax the restriction on transpar\u00adent type de.nitions is that, while \nwe require them to be stable, we 10 We use A and B here to denote type constructors of arbitrary kind, \nas opposed to t, which represents types of kind T. Type Variables a,\u00df Kinds K,L ::= T |1 |K1 \u00d7K2 |K1 \n.K2 Constructors A,B ::= a|b|()|(A1,A2)|piA | .a:K.A |A1(A2) Base Types b::= unit |A1 \u00d7A2 |A1 .A2 |rec(A) \n|.a:K.A |.a.K.A | a. .a.K.A1 -.A2 Eliminations E::= |piE|E(A) Type Contexts . ::= \u00d8|.,a:K |.,a.K |.,a.K \n| .,a:K=A |.,a:K A Type Effects .::= \u00d8|.,a:= A |.,a: A |.,a. Figure 6. Syntax of Types do not need them \nto be immediately stable. For example, say we have two writable type variables aand \u00df. It is clearly \nok to de.ne \u00df:= int, followed by a:= \u00df, but what about processing the de.ni\u00adtions in the reverse order? \nIf a:= \u00df comes .rst, then a s de.nition is momentarily unstable. Ultimately, though, the de.nitions are \nstill perfectly acyclic because a s de.nition is eventually stable. More\u00adover, there are situations where \nit is useful to have the .exibility of de.ning aand \u00df in either order (in particular, see Section 4.1). \nTo afford this .exibility, when typechecking a:= A, we allow A to depend on some set of writable variables \n{\u00dfi}not including a, so long as the \u00dfi are all backpatched with stable de.nitions by the time a is sealed \n(i.e., by the time e,in e: t defines a, has .n\u00adished evaluating). While this requirement is not strictly \nnecessary, it allows us to treat all sealed abstract types as stable, which in turn means that subsequent \ncode may depend on them freely, without any restrictions.  3. The Type System 3.1 Type Structure The \nsyntax of our type structure is shown in Figure 6. The base type constructors binclude all the usual \nF. base types, plus the new type constructs introduced in the examples of Section 2. The language of \nhigher type constructors and kinds is standard F., extended with products. Type eliminations E are used \nin the typing rules for fold s and unfold s (see the discussion of Rules 13 and 14 in Section 3.2). Type \ncontexts . include bindings for ordinary types (a:K), writable types (a.K), stable types (a.K), transparent \ntype synonyms (a:K=A) and datatypes (a:K A). We treat type contexts as unordered sets and assume implicitly \nthat all bound variables are distinct. We write writable(.) to denote the set of writable type variables \nbound in .. We write .(a) to denote the kind to which ais bound in .. Type contexts are permitted to \ncon\u00adtain cycles as long as those cycles are broken by a datatype binding. To be precise: De.nition 3.1 \n(Acyclic Type Contexts) We say that a type context . is acyclic if there is an order\u00ading of its domain \na1,\u00b7\u00b7\u00b7,an such that, for all i . 1..n,if ai :Ki =Ai .., then FV(Ai) .{a1,\u00b7\u00b7\u00b7,ai-1}. In this case, we \ncall a1,\u00b7\u00b7\u00b7,an an acyclic ordering of ..  De.nition 3.2 (Well-Formed Type Contexts) We say that a type \ncontext . is well-formed, written f. ok, if: 1. . is acyclic 2. (a:K=A .. . a:K A ..) .. fA:K  Value \nVariables x,y Values v::= x |() |(v1,v2) |.x:A.e |.a:K.e |.a.K.e |.a.K..x:A.e |foldA |unfoldA |foldA(v) \nTerms e,f ::= v |piv |v1(v2) |v[A] |v1[a](v2) |recA(x.e) |fetch(v) |let a=A in e|let x= e1 in e2 |new \na.K in e:A |a:= A |a: A |e:A defines a Value Contexts G ::= \u00d8|G,x:A Figure 7. Syntax of Terms The type \nwell-formedness judgment (. f A:K) referred to in part 2 of De.nition 3.2 is de.ned in the obvious way \n(some representative rules appear in Appendix A). The only thing that the type well-formedness judgment \nneeds to know from . is what the kinds of its bound variables are. It does not care whether type variables \nare writable or transparent, or even whether . is acyclic. Type equivalence is slightly more complicated, \ndue to the pres\u00adence of type synonyms. To account for these, we use the equiva\u00adlence judgment (. f A1 \n= A2 :K) de.ned by Stone in Sec\u00adtion 9.1 of Pierce s ATTAPL book [20].11 Our new base types, like the \nDPS universal type, do not at all complicate type equivalence, which Stone shows is relatively easy to \nprove decidable (assum\u00ading . is well-formed). Note that the type equivalence judgment treats datatype \nbindings (a:K A) no different from ordinary ab\u00adstract type bindings (a:K). See Appendix A for some representa\u00adtive \nequivalence rules. In order to de.ne what it means to be a stable type constructor, we .rst de.ne a useful \nauxiliary notion, which we call the basis of a type constructor. Intuitively, the basis of a type constructor \nA is the set of unstable abstract type variables on which A depends. This is determined by inductively \ncrawling through the type context. Stable types are precisely those types whose bases are empty. Formally: \nDe.nition 3.3 (Basis of a Type Constructor) Given a type constructor A and an acyclic context ., where \n FV(A) .dom(.), let basis.(A) be de.ned as {basis.(a) |a.FV(A)}, where basis.(a) is de.ned inductively \nas follows: . . \u00d8 if a:K A .. or a.K .. def basis.(a)= {a} if a.K .. or a:K .. . basis.(A) if a:K=A .. \nNote: assuming a1,\u00b7\u00b7\u00b7,an is an acyclic ordering of ., the above de.nition is sensible because basis.(ai) \nis de.ned only in terms of basis.(aj ) for j<i. De.nition 3.4 (Stable Type Constructor) We say that a \ntype constructor A (with kind K in acyclic context .) is stable, written . fA .K,if . fA:K and basis.(A) \n= \u00d8. The .nal and most interesting element of Figure 6 is the de.\u00adnition of type effects .. A type effect \nis an unordered set of type variable de.nitions (backpatchings). The domain of .is the set of type variables \nbeing de.ned in .(each entry in .must de.ne a dis\u00adtinct variable). A type variable acan either be de.ned \ntransparently 11 N.B. The language we are referring to is not the Stone-Harper singleton kind language \n[21]; it is just F. with \u00df-. equivalence, extended with support for type de.nitions in the context. Well-formed \nterms: .; G fe :A with . We write .; G fe:A as shorthand for .; G fe:A with \u00d8. x:A .G (1) (2) .; G fv1 \n:A1 .; G fv2 :A2 (3) .; G fv :A1 \u00d7A2 i .{1,2} (4) .; G fx:A .; G f() : unit .; G f(v1,v2) :A1 \u00d7A2 .; \nG fpiv :Ai . fA: T .; G,x:A fe:B .; G fv1 :A .B.; G fv2 :A (5) (6) .; G f.x:A.e :A .B .; G fv1(v2): B \n.,a:K; G fe:A .; G fv : .a:K.B. fA:K (7) (8) .; G f.a:K.e : .a:K.A .; G fv[A] : {a..A}B .,a.K; G fe:A \n.; G fv : .a.K.B. fA .K (9) (10) .; G f.a.K.e : .a.K.A .; G fv[A] : {a..A}B .,a:K fA: T .,a.K; G,x:A \nfe:B with a. (11) a. .; G f.a.K..x:A.e : .a.K.A -.B a. .; G fv1 : .a.K.A -.B.;G fv2 : {a..\u00df}A \u00df.K .. \n(12) .; G fv1[\u00df](v2): {a. .\u00df}B with \u00df. . fA =E{a}: T a:K B ... fA =E{a}: T a:K B .. (13) (14) .; G ffoldA \n: E{B}.A .;G funfoldA :A .E{B} . fA: T .; G,x: rec(A) fe:A with . .; G fv : rec(A) (15) (16) .; G frecA(x.e):A \nwith . .; G ffetch(v):A . fA:K .,a:K=A; G fe:B with . .; G fe1 :A1 with .1 .@ .1;G,x:A1 fe2 :A2 with \n.2 (17) (18) .; G flet a=A in e: {a.. .; G flet x= e1 in e2 :A2 with .1,.2 .A}B with {a.A}. .,a.K; G \nfe:A with .,a. a . FV(A) .FV(.) (19) .; G f(new a.K in e:A) : A with . a.K ... fA:K basis.(A) .writable(.) \n\\{a} a.K ... fA:K (20) (21) .; G fa:=A : unit with a:=A .;G fa: A: unit with a: A a.K ...;G fe:A with \n. .@ . fa.K .;G fe:B with . .@ . fA =B: T (22) (23) .; G f(e:A defines a):A with (.sealing a) .;G fe:A \nwith . Figure 8. Static Semantics (a:= A), by a datatype de.nition (a: A), or abstractly (a.). The last \nkind of de.nition only arises as the result of the sealing opera\u00adtion (e:A defines a), as discussed in \nthe next section. In our type system, it is useful to have a shorthand .@ . for applying the type effect \n. to .. The application results in a type context that re.ects the de.nitions in .. Assuming that dom(.) \n.writable(.), we de.ne .@ .as follows: def .@ . = (. \\{a.K |a.K .. . a.dom(.)}) .{a:K=A |a.K .. . a:= \nA ..}.{a:K A |a.K .. . a: A ..}.{a.K |a.K .. . a...} Note that variables that have been de.ned abstractly \n(a.) are classi.ed as stable in the new context. This is sound because a. arises from uses of sealing, \nand sealed types are always stable (see the end of Section 2.3). De.nition 3.5 (Well-Formed Type Effects) \nWe say that a type effect . is well-formed in type context ., written . f. ok, if: 1. dom(.) .writable(.) \n2. f.@ . ok 3. .a:= A ...basis.(A) .writable(.)  The .rst two conditions are straightforward. The third \ncondition checks that for all transparent de.nitions a:= A in ., the right\u00adhand side A does not depend \non any variables \u00df bound as \u00df:K. The reason for this is simple: if A depends on an unstable, non\u00adwritable \n\u00df, there is no way that A can eventually become stable via the backpatching of \u00df. Thus, since A is irrevocably \nunstable, there is no point in allowing the de.nition a:= A. 3.2 Term Structure The syntax of our term \nstructure is shown in Figure 7. After the exposition of Section 2, the new term constructs in our language \nshould all look familiar. A few minor exceptions: let a=A in e enables local transparent type de.nitions \ninside expressions. One can think of this as shorthand for {a. .A}e, that is, e with A substituted for \nfree occurrences of a. Also, instead of a letrec, we employ a self-contained recA(x.e) expression. One \ncan think of this as shorthand for letrec x:A= e in x. For simplicity, we require that all sequencing \nof operations be done explicitly with the use of a let expression (let x= e1 in e2). It is straightforward \nto code up standard left-to-right (or right-to\u00adleft) call-by-value semantics for function application, \netc., using a let. We say that a value context Gis well-formed under type context ., written .f G ok,if \nf . ok and .x :A. G. .f A: T. Figure 8 de.nes the typing rules for terms. Our typing judgment (.; G f \ne :A with .) is read: Under type context .and value context G, the term e has type Aand type effects \n.. We leave off the with . if . =\u00d8. Rules 1 through 8 are completely standard. Note that func\u00adtion bodies \nare not permitted to have type effects, i.e., to de.ne externally-bound type variables. If they were, \nwe would need to a. support effect types like A1 -. A2, which we argued in Sec\u00adtion 2.2 is a problematic \nfeature. Rules 9 and 10 for stable universals are completely analogous to the normal universal rules \n(7 and 8). Rules 11 and 12 for DPS universals are straightforward as well. The body of a DPS universal \nis required to de.ne its type argument, but that is the only type effect it is allowed to have since \nthat is the only effect written on its arrow. What if we want to write a function that takes multiple \nwritable type arguments and de.nes all of them? It turns out that such a function is already encodable \nwithin the language by packaging all the writable types together as a single writable type constructor \nof product kind. See Section 4.1 for details. Rules 13 and 14 for foldA and unfoldA require that the \ntype Athat is being folded into or out of is some type path E{a} rooted at a datatype variable a, whose \nunderlying de.nition is B. These coercions witness the isomorphism between E{a} and E{B}.12 Rules 15 \nand 16 for rec and fetch are completely straight\u00adforward. Notice that the body of a rec may have arbitrary \ntype effects. Also, the canonical forms of type rec(A) are variables. In the dynamic semantics (Section \n3.4), we use variables to model backpatchable memory locations. Rule 17 processes the let binding of \na =A by adding that type de.nition to the context when typechecking the let body. It substitutes Afor \na, however, in the result type and type effect. Note that there is no need to restrict Ato be stable \nbecause a s de.nition as Ais never hidden. Rule 18 for let x =e1 in e2 is slightly interesting in that \nthe type effect .1 engendered by e1 must be applied to the type context .before typechecking e2. Rule \n19 for new a .K in e :A, as a matter of simplicity, re\u00adquires e to de.ne and seal the new writable variable \na. However, a may not escape its scope by appearing in the free variables of the result type A or in \nany other type effects . that e might have. We ask for new to be annotated with its result type so that \nthe type\u00adchecking algorithm does not have to guess one (via normalization) that does not refer to a. \nRule 20 for transparent type de.nitions, a :=A, implements what we described at the end of Section 2.3. \nIn particular, the third premise, basis.(A). writable(.)\\{a}, allows Ato depend on any writable type \nvariables besides a. The rule does not check that A is immediately stable. Stability will be checked \nlater on, at the point when a is sealed (Rule 22). Rule 21 for datatype de.nitions simply checks that \nthe variable being de.ned is in fact writable. There is nothing else interesting to check because datatype \nde.nitions cannot introduce any cycles. Rule 22 concerns the sealing construct (e :A defines a). The \n.rst premise ensures that a is a writable variable (otherwise, e 12 For simplicity, we have made foldA \nand unfoldA into new canonical forms of the ordinary arrow type. In practice, one may wish to classify \nthese values using a separate coercion type, so as to indicate to the compiler that they behave like \nthe identity function at run time [23]. should certainly not be allowed to de.ne it!). The second premise \nchecks that e has the export type A along with some effects .. The third premise then checks that, once \n. has been applied to the type context ., a is stable. It is now safe to seal the de.nition of a. To \ndenote the sealing of a s de.nition in the type effect of the conclusion, we write . sealing a, de.ned \nas follows: De.nition 3.6 (Sealing a Type Variable in a Type Effect) Suppose . is a type effect whose \ndomain includes the type variable a. Let . = .1,.2, where dom(.2)= {a} (so a. dom(.1)). Then, we will \nwrite . sealing a as shorthand for .1,a .. Finally, Rule 23 allows for type conversion. The rule is slightly \ninteresting in that type conversion is done in the context .@. instead of .. This is because the type \nof e describes the value that e evaluates to. That value exists in the post-. world of .@., where more \ntype de.nitions may be available, so it is useful to allow type conversion to occur there. 3.3 Some Interesting \nProperties of the Static Semantics Here we discuss some useful properties of our type system. For any \ntype judgment J, we will use the notation .I J to signify that f . ok and . fJ. For any term judgment \nJ, we will use the notation .; G I J to signify that .f G ok and .; GfJ. First, we have a validity property, \nstating that well-formed terms have well-formed types and well-formed type effects. Proposition 3.7 (Validity) \nIf .; GI e :A with ., then .f A:T and . f . ok. Next, we have some substitution properties. Let a type \nsubstitu\u00adtion d be a total mapping from type variables to type constructors that behaves like the identity \non all but a .nite set of variables, called its domain, written dom(d). Let id stand for the identity \nsub\u00adstitution. We will write dA(resp. de, d.,or dG) to signify the result of performing the substitution \nd on the free variables of A(resp. e, .,or G) in the usual capture-avoiding manner. Note that the free \ntype variables of . include dom(.). We must actually de.ne the notion of well-formed substitution quite \ncarefully in order to make the theorems go through: De.nition 3.8 (Well-Formed Type Substitutions) We \nsay that a type substitution d maps .to .', written .' f d :., if: 1. dom(d). dom(.) 2. f . ok and f \n.' ok 3. .a .K. .. .a' .K. .'.a' =da 4. .a1 .K1 . .. .a2 .K2 . .. (a1 =a2). (da1 da2)  = 5. .a :K=A \n. .. (.' f da = dA:K) . (basis./ (da). basis./ (dA)) ' . .' 6. .a :K A. .. .a' :K A. (a' =da) . (.' \nf A' = dA:K)  7. .a .K. .. .' f da . K 8. .a :K . .. .' f da :K  Conditions 1, 2 and 8 are completely \nstraightforward. Conditions 3 and 6 require that d maps writable and datatype variables to type variables \nof the same classes. Condition 7 guarantees that d maps stable type variables to stable types (not necessarily \nvariables). Condition 4 ensures that d does not alias two writable variables that were distinct in the \noriginal .. This is critical, since writable variables may only be backpatched once. Condition 5 checks \nthat d maps transparent type variables to types that match their de.nitions. The second conjunct of this \ncondition may seem redundant, but it is not. For example, suppose a:T =unit .., \u00df.T .. ' , and d(a)=(.a \n' :T.unit)(\u00df). While it is certainly true that d(a) is equivalent to d(unit),itis not the case that basis./ \n(d(a)).basis./ (d(unit))because d(a) refers to \u00df(albeit in a useless way). This situation could potentially \nbe avoided by de.ning basis.(A) in a more sophisticated way (e.g., by .rst normalizing A, and then computing \nthe set of type variables that its normal form depends on). In any case, the reason we care about the \nsecond conjunct of Condition 5 is in order to obtain the following monotonicity prop\u00aderty, which is useful \nin proving various theorems. It says essen\u00adtially that, if a type A only depends on some set of writable \nvari\u00adables, then the basis of A cannot grow unexpectedly to include other variables when the type undergoes \na well-formed substitu\u00adtion. One obvious instance where this is important is in proving substitution \n(Proposition 3.12 below) for the construct a:=A (Rule 20). When we apply a substitution d to this construct, \nwe want to make sure that dAdoes not suddenly depend on da. If S is a set of type variables, let dS denote \n{da |a .S}. Proposition 3.9 (Monotonicity) If . ' fd :.and basis.(A).writable(.), then basis./ (dA).d(basis.(A)). \nWe can now state several type substitution properties: Proposition 3.10 (Substitution on Types) Suppose \n. ' fd :.. Then: 1. If . fA:K, then . ' fdA:K. 2. If . fA .K, then . ' fdA.K. 3. If . fA1 =A2 :K, then \n. ' fdA1 =dA2 :K. 4. If . fG ok, then . ' fdG ok.  Proposition 3.11 (Substitution on Type Effects) \nIf . ' fd :.and .f. ok, then . ' fd. ok and . ' @d. fd :.@.. Proposition 3.12 (Type Substitution on \nTerms) If . ' fd :.and .; G I e:A with ., then . ' ; dGfde: dA with d.. A similar property holds for \nvalue substitutions, but the de.nition of a well-formed value substitution is much more obvious. See \nAppendix B for details and statements of other useful properties. In proving type soundness, we have \nfound the following lemma to be very handy. We call it the use it or lose it lemma because it states \nthis simple strengthening property: if a term e is well-typed in a context where a type variable ais \nbound as writable, but edoes not use the fact that ais writable, then ewill also be well-typed in a context \nwhere ais not writable. Like the validity and substitution properties, this lemma is provable by straightforward \ninduction on derivations. Lemma 3.13 (Use It Or Lose It) If .,a.K; G I e :A with .and .,a:K f. ok, then \n.,a:K; G I e :A with .. One corollary of this lemma says that terms that have no type effects (most notably, \nvalues) remain well-typed even if type effects are applied to their context. This fact is important in \nshowing that the mutable value store maintained by our dynamic semantics remains well-formed throughout \nexecution. Corollary 3.14 ( Pure Terms Stay Well-Typed Under Effects) If .; G I e :Aand .f. ok, then \n.@.;GI e:A. Proof: Let . ' =(.\\{a.K|a.K..}).{a:K |a.K ..}.By Lemma 3.13, . ' ;GI e:A. It is easy to see \nthat .@.fid :. ' . Thus, the desired result follows by Proposition 3.12. . Another corollary says that \nif we have an expression ereferring to two writable type variables of the same kind, and eonly depends \non one of them being writable, then we can merge them into one writable type variable. As stated here, \nthis is exactly what we need in order to prove type preservation in the case of \u00df-reduction for DPS universal \ntypes (cf. Rule 11 in Figure 8, and Rule 28 in Figure 9). Corollary 3.15 (Merging Together Two Writable \nTypes) If . fG ok and \u00df.K .. and .,a.K; G,x:A I e:B with a., then .; G,x:{a..e :{a.\u00df}B with \u00df. .\u00df}A I \n{a.\u00df}.. Proof: Let .=. ' ,\u00df.K. By Lemma 3.13, . ' ,\u00df:K,a.K; G,x:A I e :B with a..It is easy to see that \n.f{a.' ,\u00df:K,a.K. Thus, the desired .\u00df}:. result follows by Proposition 3.12. . Finally, although we do \nnot provide a typechecking algorithm here, it is completely straightforward to write one that, given \nwell\u00adformed contexts .and G and a well-formed term e, synthesizes a unique type effect .for e, as well \nas a type Athat is unique up to type conversion under .@.. 3.4 Dynamic Semantics and Type Soundness We \nde.ne the dynamic semantics of our language in Figure 9 using an abstract machine semantics. A machine \nstate . is either of the form BlackHole or (.; .; C; e). The former arises when an attempt is made to \nfetch a recursive location whose contents have not yet been initialized. In the normal state, . is the \ncurrent type context (i.e., the type store), . is the current value store, C is the current continuation, \nand eis the expression currently being evaluated. In the language de.ned here, the only purpose of the \nvalue store is to support a backpatching semantics for recursion. It could naturally be extended to support \nother things, such as mutable references. A value store . binds variables to either values (v)or junk \n(?). Assuming x . dom(.), we write .(x) to denote the contents of location x in .. Mirroring the syntax \nof type effect application, we write .@x:=v to signify the store . ' with the ' '' property that dom(. \n)= dom(.), . (x)= v, and . (y)= .(y) for all y .dom(.),y =x. We de.ne well-formedness of value stores \nas follows: De.nition 3.16 (Run-Time Value Contexts) We say that a value context Gis run-time if it only \ncontains bindings of the form x:rec(A). De.nition 3.17 (Well-Formed Value Stores) We say that a value \nstore . is well-formed in . and has type G, written .f. :G, if: 1. .fG ok and Gis run-time 2. dom(.)=dom(G) \n 3. .x:rec(A).dom(G). either .(x)=? or .; Gf.(x):A  Continuations C are represented as stacks of continuation \nframes F. There are only two continuation frames. The .rst is let x= in e, which waits for x s binding \nto evaluate to a value vand then plugs vin for xin e. The second is recA(x. ), which Reductions: e . \ne ' Machine States . ::= (.; .; C; e) |BlackHole Value Stores .::= \u00d8|.,x..v |.,x..? Continuations C::= \n|C.F Continuation Frames F::= let x= in e|recA(x. ) (24) (25) pi(v1,v2) . vi (.x:A.e)(v) . {x..v}e \n(26) (27) (.a:K.e)[A] . {a.e (.a.K.e)[A] . {a.e .A}.A} (28) (29) (.a.K..x:A.e)[\u00df](v) . {a.x.v}e unfoldA(foldB(v)) \n. v .\u00df}{. (30) (31) let a=A in e. {a.e .A}e:A defines a . e Machine state transitions: . . . ' e. \ne ' (.; .; C; e) . (.; .; C; e ' ) (32) (33) (34) (.; .; C; let x= e1 in e2) . (.; .; C.let x= in e2; \ne1) (.; .; C.let x= in e; v) . (.; .; C; {x..v}e) (35) (36) x.dom(.) x.dom(.) (.; .; C; recA(x.e)) . \n(.; .,x.(.; .; C.recA(x. ); v) . (.; .@ x:= v; C; v) .?; C.recA(x. ); e) x.dom(.) .(x)= vx.dom(.) .(x)= \n? (37) (38) (.; .; C; fetch(x)) . (.; .; C; v) (.; .; C; fetch(x)) . BlackHole a .dom(.) (39) (.; .; \nC; new a.K in e:A) . (.,a.K; .; C; e) a.K .. a.K .. (40) (41) (.; .; C; a:= A) . (. @ a:= A; .; C; ()) \n(.; .; C; a: A) . (. @ a: A; .; C; ()) Well-formed continuations: .; G fC:A cont . fA: T (42) .; G fF:A \n. B with . .@ .;G fC:B cont (43) .; G fC:B cont . fA =B: T (44) .; G f :A cont .; G fC.F:A cont .; G \nfC:A cont Well-formed continuation frames: .; G fF:A . B with . . fA: T .; G,x:A fe:B with .x: rec(A) \n.G (45) (46) .; G flet x= in e:A . B with . .; G frecA(x. ):A . A with \u00d8 Figure 9. Dynamic Semantics \nwaits for the body of a recursive term to evaluate to a value v and then backpatches the recursive memory \nlocation xwith v. The typing judgments for continuations and continuation frames are shown in Figure \n9. The latter is slightly interesting in that a frame may have type effects. One can read the judgment \n(.; G f F :A . B with .) as: starting in type context ., the frame F takes a value of type A and returns \na value of type B while engendering the effects in .. Continuations Cmay of course have type effects \nas well, but they are irrelevant because we never return from a continuation. The dynamic semantics itself \nis entirely what one would expect given our discussion from Section 2. Like sealing in ML, sealing in \nour language is a static abstraction mechanism with no run-time signi.cance. The new construct, on the \nother hand, has the effect of creating a new entry in the type store at run time. Backpatching a writable \ntype is modeled by actually updating its entry in the type store. In short, the semantics is faithful \nto our intuition. That said, it is worth noting that, while the type store . is useful in de.ning the \ndynamic semantics and proving type soundness, it does not have any real in.uence on run-time computation. \nIn other words, the dynamic semantics of Figure 9 never consults the type store in order to determine \nthe identity of a type variable and make a transition based on that information. Consequently, there \nis no need in an actual implementation to construct and maintain the type store, and the operations for \ncreation and de.nition of abstract type variables may both be compiled as no-ops. We can now de.ne what \nit means to be a well-formed machine state and state the standard preservation and progress theorems \nleading to type soundness. The interesting part of the de.nition is that the expression e currently being \nevaluated may have type a1.,a2. [[ . a1 . K1,a2 . K2.A(a1)(a2) ----. B(a1)(a2)]] def a. = . a. K1 \u00d7 K2.A(p1a)(p2a) \n-. B(p1a)(p2a) [[ .a1 . K1,a2 . K2..x:A(a1)(a2).(e:B(a1)(a2)) ] def =.a. K1 \u00d7 K2..x:A(p1a)(p2a). new \na1 . K1,a2 . K2 in (let () = a:= ( a1,a2) in e) :B(p1a)(p2a) defines a [[ v1[a1][a2](v2): B(a1)(a2)]] \ndef = new a. K1 \u00d7 K2 in (let () = a1 := p1a in let () = a2 := p2a in v1[a](v2)) :B(a1)(a2) defines a1,a2 \n Figure 10. Encoding of Multiple-Argument DPS Universals effects ., so these effects must be incorporated \ninto the starting context of the continuation C . De.nition 3.18 (Well-Formed Machine States) We say \nthat a machine state . is well-formed, written f . ok,if either .= BlackHole,or .= (.; .; C ; e) and \nthere exist G, A and .such that: 1. . f . :G 2. .; G f e:A with . and .@ .;G fC :A cont  Theorem 3.19 \n(Preservation) If f . ok and . . . ' , then f . ' ok. De.nition 3.20 (Terminal States) A machine state \n. is terminal if it is of the form BlackHole or (.; .; ; v). De.nition 3.21 (Stuck States) A machine \nstate . is stuck if it is not terminal and there is no state . ' such that . . . ' . Theorem 3.22 (Progress) \nIf f . ok, then . is not stuck. (The progress theorem depends on a standard canonical forms lemma, which \nis given in Appendix B.) Corollary 3.23 (Type Soundness) If \u00d8 ; \u00d8f e :A, then the execution of (\u00d8; \u00d8; \n; e) never enters a stuck state. 4. Encodings in Destination-Passing Style 4.1 Multiple-Argument DPS \nUniversal Types It is likely that in practice one may wish to de.ne a function of DPS universal type \nthat takes multiple writable type arguments and de\u00ad.nes all of them. However, our language as presented \nin Section 3 appears to allow DPS universals to take only a single writable type argument. Figure 10 \nillustrates that in fact multiple-argument DPS universals can be encoded in terms of single-argument \nones. For simplicity, we take multiple-argument to mean two-argument, but the technique can easily be \ngeneralized to narguments. The idea is to encode a function taking two writable type argu\u00adments a1 and \na2 (of kinds K1 and K2) as a function taking one [[ . a. K.A]]DPS def a. = . a. K.unit -. A [[ pack [A,v] \nas . a. K.B]]DPS def =.a. K..(). (let () = a:= A in v) :B defines a [[ let [a,x]= unpack vin (e:A) ] \nDPS def = new a. K in (let x= v[a]() in e):A Figure 11. DPS Universal Encoding of Existentials writable \ntype argument a(of kind K1 \u00d7 K2). In Figure 10, we as\u00adsume the value argument and result types have the \nform A(a1)(a2) and B(a1)(a2), respectively, where a1,a2 . FV(A) . FV(B). In the introduction form, we \ndivide the single a into two writable variables a1 and a2 by creating those variables with a new and \nthen de.ning the original a in terms of them. For the elimination form, it is the reverse. We start with \ntwo writable vari\u00adables, and in order to apply the DPS universal we must package them up as one. This \nis achieved by simply creating a new a of the pair kind, and then de.ning the original writable variables \nas projections from it. For the elimination form to be well-typed, it is important of course that a1 \nand a2 be distinct. Also note that we make use of new and sealing constructs that create (or seal) multi\u00adple \nvariables. These are simply shorthand for several nested new s or sealings. In the encoding of both the \nintroduction and elimination forms, we rely heavily on the ability to de.ne a writable variable trans\u00adparently \nin terms of another writable variable, which is then subse\u00adquently de.ned in some stable way. This provides \ngood motivation for our policy that de.nitions of writable variables need not be sta\u00adble immediately, \nbut only by the time they are sealed (as discussed at the end of Section 2.3). 4.2 Existential Types \nIn Section 2.2, we argued that the special case of the DPS universal in which the value argument has \nunit type can be viewed as a kind of existential type. We now make that argument precise. Figure 11 shows \nhow existential types and their introduction and elimination forms may be encoded using that special \ncase of the DPS universal type. The caveat is that DPS universals are not capable of encoding arbitrary \nexistentials . a:K.A, but only what we call stable existentials, which we write . a. K.A. As the name \nsuggests, a value of stable existential type is a package whose type component is stable, and the standard \nCPS encoding of existentials can be trivially modi.ed to de.ne . a. K.A as shorthand for . \u00df: T.(. a. \nK.A . \u00df) . \u00df. To package type constructor A with value v, we write a DPS function that asks for a writable \nabstract type name a, and then returns v after de.ning a to be A. The data abstraction one nor\u00admally \nassociates with existential introduction is achieved here by our sealing construct. Note that A must \nbe stable in order for the encoding of pack to be well-typed, since A is used to de.ne the writable variable \na. To unpack an existential value v, we (the client) must .rst create a new writable type name a and \nthen pass it to v to be de.ned. A potential bene.t of the DPS encoding over the CPS encoding is that \nit allows the body eof the unpack to have arbitrary type effects, so long as they do not refer to a. \nIn the CPS encoding of unpack, e must be encapsulated in a function, so it is not allowed to de.ne any \nexternally-bound variables. The DPS encoding is encouraging because it means that our approach to recursive \ntype generativity is fundamentally compat\u00adible with the traditional understanding of generativity in \nterms of existential types. For instance, returning to the bootstrapped heap example from Figure 4, we \ncan now rewrite the type of MkHeap as . a . T. ORDERED(a) .. \u00df . T. HEAP(a)(\u00df) This looks just like the \nstandard F. interpretation of a generative functor signature, except that we have replaced the normal \ntype variable bindings by stable ones. It is not even necessary for the existential in the result type \nof MkHeap to be encoded in DPS a value of stable existential type (under any encoding) can always be \ncoerced to [[ . a . K. A]]DPS by .rst unpacking its components and then repacking them using the DPS \nencoding of pack. 5. Related Work As discussed in Section 1, there has been much work on extend\u00ading ML \nwith recursive modules, but a clear account of recursive type generativity has until now remained elusive. \nCrary, Harper and Puri [3] have given a foundational type-theoretic account of recur\u00adsive modules, but \nit does not consider the interaction of recursion with ML s sealing mechanism (opaque signature ascription). \nRusso has formalized and implemented recursive modules as an exten\u00adsion to the Moscow ML compiler [15]. \nUnder his extension, any type components of a recursive module that are referred to recur\u00adsively must \nhave their de.nitions made public to the whole module. Leroy has implemented recursive modules in O Caml \n[11], but has not provided any formal account of its semantics. With none of these approaches is it possible \nto implement the bootstrapped heap example using a generative MkHeap functor. In reaction to the dif.culties \nof incorporating recursive linking into the ML module system, others have investigated ways of re\u00adplacing \nML s notion of module with some alternative mechanism for which recursive linking is the norm and hierarchical \nlinking a special case. Ancona and Zucca s CMS calculus, in particular, has been highly in.uential and \nled to a considerable body of work on mixin modules [2]. However, it basically ignores all issues in\u00advolving \ntype components (and hence, data abstraction) in modules. More recently, Duggan has developed a language \nof recursive DLLs [7]. His calculus is not intended as the basis of a source\u00adlevel language, but rather \nas an interconnection language for dy\u00adnamic linking and loading of shared libraries. Based on his informal \ndiscussion, Duggan appears to address some of the problems of re\u00adcursive ADT s in a manner similar to \nthe typechecking algorithm we suggested in Section 1.1. It is dif.cult, though, to determine precisely \nhow his approach relates to ours because he is working in a relatively low-level setting. In addition, \nDuggan simpli.es the problem to some extent by not supporting full ML-style transparent type de.nitions, \nbut only a limited form of sharing constraint that is restricted to atomic types. Interestingly, the \nwork that seems most closely related to our approach comes from the Scheme community. Flatt and Felleisen \ndeveloped a recursive-module-like construct called units for use in MzScheme [16]. While MzScheme is \ndynamically-typed, their paper formalizes an extension of units to the statically-typed setting as well \n[8]. A unit has some set of imports and exports, which may include abstract types. Two units may be compounded \ninto one, with each unit s exports being used to satisfy the other s imports. While our approach differs \nfrom units in many details, there are considerable similarities in terms of expressive power. For instance, \na. one can think of the DPS universal type . a . K. A -. B as the type of a unit with a value import \nof type A, a value export of type B, and a type export a. (We model type imports separately, via standard \nuniversal quanti.cation.) The avoidance of transparent type cycles, which we handle by distinguishing \nbetween stable and unstable forms of universal quanti.cation, is dealt with in the unit language by means \nof unit signatures, which explicitly specify which export types of a unit depend on which import types. \nUltimately, the main distinction between our approach and units is that, while units do many things at \nonce, we have tried instead to isolate orthogonal concerns as much as possible. As a result, our language \nconstructs are more lightweight, and our semantics is eas\u00adier to follow. In contrast, the unit typing \nrules are large and com\u00adplex. Given that units were intended as a realistic, programmable language construct, \nthis complexity is understandable, but there are some other problems with units as well. In particular, \nthey lack support for ML-style type sharing, and their emphasis on exter\u00adnal linking forces one to program \nin a recursive analogue of fully functorized style. Nonetheless, we hope that our present account of \nrecursive type generativity will help draw attention to some of the interesting and novel features of \nunits that the existing work on recursive ML-style modules has heretofore ignored. Finally, unrelated \nto recursive modules, Rossberg [18] gives an account of type generativity that, like ours, provides a \nnew construct for creating fresh abstract types at run time. Rossberg s focus, however, is not on recursion \nbut on the interaction of data abstraction and run-time type analysis. Thus, his system requires one \nto de.ne an abstract type at the same point where it is created. 6. Conclusion In previous work with \nKarl Crary and Bob Harper [6], we gave an interpretation of ML-style modularity in which type generativity \nwas treated as a computational effect. We view the present work as a continuation and re.nement of that \ninterpretation. Speci.cally, while we still model the de.nition of new abstract types as a kind of effect, \nwe allow abstract types to be created and used before they are de.ned, thus making it possible to link \nsuch types recursively. One complaint that can be leveled against our approach is that the interpretation \nof type-level recursion in terms of backpatching is highly operational and may make it dif.cult to reason \nabout ab\u00adstraction guarantees. Admittedly, while it is possible to state a weak syntactic abstraction \nproperty e.g., that if a program contains a subterm e in which a writable abstract type is de.ned and \nsealed, then e may be replaced by another subterm e ' that de.nes the ab\u00adstract type in a different way \nit is not clear what a stronger ab\u00adstraction theorem would look like. This remains an important con\u00adsideration \nfor future work. Another interesting question is whether the full power of our language is useful, or \nonly a fragment of it is really needed for practical purposes. For example, our type system allows the \npro\u00adgrammer to de.ne types at run time based on information that is only dynamically available. If one \nis only interested in supporting second-class recursive modules, then the language we have pre\u00adsented \nhere is more powerful than necessary. In that case, it is worth considering whether there is a weaker \nsubset of the language that suf.ces and is easier to implement in practice. This question is of course \ntied in with the more general problem of scaling the ideas of this paper to the level of a module language. \nThe path from this paper to a full-blown module system is not immediate, primarily because the approach \nto data abstraction we have taken here is at least super.cially quite different from the way that type \nsystems for modules have traditionally been formalized. That said, we believe it should not be fundamentally \ndif.cult. Acknowledgments We would like to thank Aleks Nanevski for suggesting that we pur\u00adsue an idea \nalong these lines years ago, as well as the anonymous referees for giving such helpful and detailed comments. \nReferences [1] Roberto Amadio and Luca Cardelli. Subtyping recursive types. ACM Transactions on Programming \nLanguages and Systems, 15(4):575 631, 1993. [2] Davide Ancona and Elena Zucca. A primitive calculus for \nmodule systems. In International Conference on Principles and Practice of Declarative Programming (PPDP), \nvolume 1702 of Lecture Notes in Computer Science, pages 62 79. Springer-Verlag, 1999. [3] Karl Crary, \nRobert Harper, and Sidd Puri. What is a recursive module? In ACM Conference on Programming Language Design \nand Implementation (PLDI), pages 50 63, Atlanta, GA, 1999. [4] Derek Dreyer. A type system for well-founded \nrecursion. In ACM Symposium on Principles of Programming Languages (POPL), pages 293 305, Venice, Italy, \nJanuary 2004. [5] Derek Dreyer. Understanding and Evolving the ML Module System. PhD thesis, Carnegie \nMellon University, Pittsburgh, PA, May 2005. [6] Derek Dreyer, Karl Crary, and Robert Harper. A type \nsystem for higher-order modules. In ACM Symposium on Principles of Programming Languages (POPL), New \nOrleans, LA, January 2003. [7] Dominic Duggan. Type-safe linking with recursive DLL s and shared libraries. \nACM Transactions on Programming Languages and Systems, 24(6):711 804, November 2002. [8] Matthew Flatt \nand Matthias Felleisen. Units: Cool modules for HOT languages. In ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation (PLDI), pages 236 248, Montreal, Canada, June 1998. [9] David K. Gifford \nand John M. Lucassen. Integrating functional and imperative programming. In ACM Conference on LISP and \nFunctional Programming, Cambridge, MA, August 1986. [10] Mark P. Jones. Using parameterized signatures \nto express modular structure. In ACM Symposium on Principles of Programming Languages (POPL), pages 68 \n78, St. Petersburg Beach, FL, 1996. [11] Xavier Leroy. The Objective Caml system: Documentation and user \ns manual. http://www.ocaml.org/. [12] Xavier Leroy. Applicative functors and fully transparent higher\u00adorder \nmodules. In ACM SIGPLAN Symposium on Principles of Programming Languages (POPL), pages 142 153, San Francisco, \nCA, January 1995. [13] Xavier Leroy. A proposal for recursive modules in Objective Caml, May 2003. Available \nfrom the author s website. [14] Robin Milner, Mads Tofte, Robert Harper, and David MacQueen. The De.nition \nof Standard ML (Revised). MIT Press, 1997. [15] Moscow ML. www.dina.kvl.dk/~sestoft/mosml.html. [16] \nMzScheme. www.plt-scheme.org/software/mzscheme/. [17] Chris Okasaki. Purely Functional Data Structures. \nCambridge University Press, 1998. [18] Andreas Rossberg. Generativity and dynamic opacity for abstract \ntypes. In International Conference on Principles and Practice of Declarative Programming (PPDP), Uppsala, \nSweden, 2003. [19] Claudio V. Russo. Recursive structures for Standard ML. In International Conference \non Functional Programming (ICFP), pages 50 61, Florence, Italy, September 2001. [20] Christopher A. Stone. \nType de.nitions. In Benjamin C. Pierce, editor, Advanced Topics in Types and Programming Languages, chapter \n9. MIT Press, 2005. [21] Christopher A. Stone and Robert Harper. Extensional equivalence and singleton \ntypes. ACM Transactions on Computational Logic, 2005. To appear. [22] Jean-Pierre Talpin and Pierre Jouvelot. \nThe type and effect discipline. Information and Computation, 111(2):245 296, 1994. [23] Joseph C. Vanderwaart, \nDerek Dreyer, Leaf Petersen, Karl Crary, Robert Harper, and Perry Cheng. Typed compilation of recursive \ndatatypes. In ACM SIGPLAN Workshop on Types in Language Design and Implementation (TLDI), New Orleans, \nJanuary 2003. [24] Philip Wadler. Listlessness is Better than Laziness. PhD thesis, Carnegie Mellon University, \nPittsburgh, PA, August 1985. A. Representative Rules for Kinding and Equivalence of Type Constructors \nThe de.nition of well-formedness and equivalence for type con\u00adstructors in our language is entirely straightforward. \nHere we give some representative rules. Well-formed type constructors: . f A:K .,a:K f A1 :T .,a:K f \nA2 :T .(a)=K a. . f a :K .f.a.K.A1 -.A2 : T Type constructor equivalence: .f A1 = A2 :K a:K=A . . .f \na = A:K .,a:K f A1 = B1 :T .,a:K f A2 = B2 : T a. a. .f.a.K.A1 -.A2 =.a.K.B1 -.B2 : T .,a:K f A1 = B1 \n:K ' .f A2 = B2 :K . f (.a:K.A1)(A2)={a..B2}B1 :K ' .,a:K f A(a)= B(a):K ' a. FV(A). FV(B) .f A= B:K.K \n' B. Additional Meta-Theoretic Properties Proposition B.1 (Properties of Type Effect Application) Suppose \n.f . ok. Then: 1. .f A:Kif and only if .@. f A:K. 2. If . f A1 = A2 :K, then .@. f A1 = A2 :K. 3. If \nbasis.(A). basis.(B), then basis.@.(A). basis.@.(B).  4. If .@.f . ' ok,  ' '' then .f .,. ok and .@(.,. \n)=(.@.)@. . De.nition B.2 (Well-Formed Value Substitutions) We say that a value substitution . maps Gto \nG ' under ., written .; G ' f . :G, if: 1. dom(.). dom(G) 2. .f G ok and .f G ' ok 3. .x:A . G..; G \n' f .x:A  Proposition B.3 (Value Substitution on Terms) If .; G ' f . :Gand .; G f e:A with ., then \n.; G ' f .e:A with .. Lemma B.4 (Canonical Forms) Suppose .; GI v :Aand Gis run-time. Then: 1. If A=unit, \nthen v is of the form (). 2. If A=A1 \u00d7A2, then vis of the form (v1,v2). 3. If A=A1 .A2, then v is of \nthe form .x:B.eor foldB or unfoldB. 4. If A=.a:K.A, then v is of the form .a:K.e. 5. If A=.a.K.A, then \nv is of the form .a.K.e.  a. 6. If A=.a.K.A1 -.A2, then v is of the form .a.K..x:B.e.  7. If A=rec(A \n' ), then vis of the form x. 8. If A=E{a}, where a:K B . ., then v is of the form foldA/ (v ' ).  \n  \n\t\t\t", "proc_id": "1086365", "abstract": "Existential types provide a simple and elegant foundation for understanding generative abstract data types, of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so.We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first \"forward-declare\" them by generating their names, and then define each one secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of <i>types</i>. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among \"transparent\" type definitions.In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a <i>destination-passing</i> interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.", "authors": [{"name": "Derek Dreyer", "author_profile_id": "81100381796", "affiliation": "Toyota Technological Institute at Chicago", "person_id": "P414177", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086372", "year": "2005", "article_id": "1086372", "conference": "ICFP", "title": "Recursive type generativity", "url": "http://dl.acm.org/citation.cfm?id=1086372"}