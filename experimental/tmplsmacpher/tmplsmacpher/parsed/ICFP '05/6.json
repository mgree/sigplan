{"article_publication_date": "09-12-2005", "fulltext": "\n Combining Programming with Theorem Proving * Chiyan Chen Hongwei Xi Boston University {chiyan, hwxi}@cs.bu.edu \nAbstract fun append {a:type, m:nat, n:nat} Applied Type System (ATS) is recently proposed as a framework \nfor designing and formalizing (advanced) type systems in support of practical programming. In ATS, the \nde.nition of type equality involves a constraint relation, which may or may not be algorith\u00admically decidable. \nTo support practical programming, we adopted a design in the past that imposes certain restrictions on \nthe syntac\u00adtic form of constraints so that some effective means can be found for solving constraints \nautomatically. Evidently, this is a rather ad hoc design in its nature. In this paper, we rectify the \nsituation by presenting a fundamentally different design, which we claim to be both novel and practical. \nInstead of imposing syntactical restric\u00adtions on constraints, we provide a means for the programmer to \nconstruct proofs that attest to the validity of constraints. In particu\u00adlar, we are to accommodate a \nprogramming paradigm that enables the programmer to combine programming with theorem proving. Also we \npresent some concrete examples in support of the practi\u00adcality of this design. Categories and Subject \nDescriptors D.3 [Software]: Program\u00adming Languages General Terms Languages, Veri.cation Keywords ATS, \nApplied Type System, Dependent Types, Proof Erasure, Theorem Proving 1. Introduction The notion of type \nequality plays a pivotal r ole in type system design. However, the importance of this role is often less \nevident in commonly studied type systems. For instance, in the simply typed .-calculus, two types are \nconsidered equal if and only if they are syntactically the same; in the second-order polymorphic .-calculus, \ntwo types are considered equal if and only if they are a-equivalent; in the higher-order polymorphic \n.-calculus, two types are considered equal if and only if they are \u00df.-equivalent. The situation immediately \nchanges in the framework Applied Type System (ATS) [25, 27], and we now use a simple example to stress \nthis point. In Figure 1, we implement a function in ATS (via a form of syntax rather similar to that \nof Standard ML [12]), where ATS is * Partially supported by NSF grant no. CCR-0229480 Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 05 September 26 28, \n2005, Tallinn, Estonia. Copyright c C 2005 ACM 1-59593-064-7/05/0009. . . $5.00. (xs: list(a,m), ys: \nlist(a,n)): list(a,m+n) = case xs of | nil => ys (* the first clause *) | cons (x, xs) => (* the second \nclause *) cons (x, append (xs, ys)) Figure 1. A simple function in ATS a programming language with its \ntype system rooted in ATS. We use list as a type constructor; when applied to a type T and an integer \nI, list(T,I) forms a type for lists of length I in which each element is of type T . Also, the two list \nconstructors nil and cons are assigned the following types: nil : .a : type.list(a, 0) cons : .a : type..n \n: nat.(a, list(a, n)) . list(a, n + 1) The header of the function append indicates that append is as\u00adsigned \nthe following type: .a : type..m : nat..n : nat. (list(a, m), list(a, n)) . list(a, m + n) which means \nthat append returns a list of length m + n when applied to two lists of length m and n, respectively. \nNote that type is a built-in sort in ATS, and a static term of the sort type stands for a type. Also, \nint is a built-in sort for integers in ATS, and nat is an abbreviation of the subset sort {a : int | \na = 0} for all nonnegative integers. When type-checking the de.nition of append, we essentially need \nto generate the following two constraints: 1. .m : nat..n : nat.m =0 . n = m + n ' 2. .m : nat..n : nat..m: \nnat. '' m = m+1 . (m+ n)+1= m + n The .rst constraint is generated when the .rst clause is type\u00adchecked, \nwhich is needed for determining that the types list(a, n) and list(a, m + n) are equal under the assumption \nthat list(a, m) equals list(a, 0). Similarly, the second constraint is generated when the second clause \nis type-checked, which is needed for deter\u00ad ' mining that the types list(a, (m+ n) + 1) and list(a, m \n+ n) are ' equal under the assumption that list(a, m) equals list(a, m+ 1). Clearly, we need to impose \ncertain restrictions on the form of constraints allowed in practice so that an effective means can be \nfound to solve constraints. In ATS, we require that (arithmetic) constraints like those presented above \nbe linear,1 and we rely on a constraint solver based on the Fourier-Motzkin variable elimina\u00adtion method \n[6] to solve such constraints. While this is indeed a 1 More precisely, we require that an arithmetic \nconstraint can be turned into a linear integer programming problem. datasort mynat = Z | S of mynat datatype \nmyadd (mynat, mynat, mynat) = | {n: mynat} Bas (Z, n, n) | {m: mynat, n: mynat, s: mynat} Ind(S m,n,Ss)ofmyadd(m,n,s) \ndatatype mylist (type, mynat) = | {a: type} mynil (a, Z) | {a: type, n: mynat} mycons (a, S n) of (a, \nmylist (a, n)) // {...} : universal quantifier // [...] : existential quantifier fun myappend {a:type, \nm:mynat, n:mynat, s:mynat} (xs: mylist (a, m), ys: mylist (a, n)) : [s: mynat] (myadd (m, n, s), mylist \n(a, s)) = case xs of | mynil => (Bas, ys) | mycons (x, xs) => let val (pf, zs) = myappend (xs, ys) in \n(Ind pf, mycons (x, zs)) end Figure 2. A motivating example simple design, it can also be too restrictive, \nsometimes, in a situa\u00adtion where nonlinear constraints (e.g., .n : int. n * n = 0) need to be handled. \nFurthermore, such a design is inherently ad hoc in its nature. In this paper, we present a fundamentally \ndifferent design. We are to provide a means for the programmer to handle nonlinear con\u00adstraints by constructing \nexplicit proofs (while linear constraints are still solved by a constraint solver). For a simpler presentation, \nlet us assume for this moment that even the addition function on integers is not allowed in forming constraints. \nUnder such a restriction, we can still implement a list append function that is assigned a type capturing \nthe invariant that the length of the concatenation of two given lists xs and ys equals m + n if xs and \nys are of length m and n, respectively. For instance, this is achieved by the code in Figure 2, and we \nprovide some explanation for it as follows. In Figure 2, we .rst declare a datasort mynat for forming \nterms that can be used as type index expressions. We then declare a datatype constructor myadd such that \nmyadd forms a type myadd(s1,s2,s3) when applied to three static terms s1,s2,s3 of the sort mynat. The \nsyntax indicates that there are two value con\u00adstructors associated with myadd, which are given the following \ntypes: Bas Ind : : .a : mynat. myadd(Z, a, a) .a1 : mynat..a2 : mynat..a3 : mynat. myadd(a1, a2, a3) \n. myadd(S(a1), a2, S(a3)) Given static terms s1,s2,s3 of the sort mynat, it is easy to see that there \nexists a closed value of the type myadd(s1,s2,s3) if and only if |s1|+|s2| = |s3|, where we use |s| for \nthe number of occur\u00adrences of S in s (which is assumed to be closed). We next declare a datatype constructor \nmylist, which forms a type mylist(T,s) when applied to a type T and a term s of the sort mynat. Note \nthat the two constructors mynil and mycons are assigned the following types: mynil : .a : type. mylist(a, \nZ) mycons : .a : type..n : mynat. (a, mylist(a, n)) . mylist(a, S(n)) Thus, given a type T and a term \ns of the sort mynat, the type mylist(T,s) is for lists of length |s| in which each element is of type \nT . Lastly, we de.ne a function myappend, which is given the following type: .a : type..a1 : mynat..a2 \n: mynat. (mylist(a, a1), mylist(a, a2)) . .a3 : mynat.(myadd(a1,a2,a3), mylist(a, a3)) Note that we use \n' (...) in the concrete syntax to form tuple types as well as tuples, and the quote symbol ( ' ) is solely \nfor the purpose of parsing. Given two lists xs and ys of types mylist(T,s1) and mylist(T,s2), respectively, \nfor some type T and terms s1 and s2 of the sort mynat, myappend returns a pair (pf , zs) such that pf \nis a value of type myadd(s1,s2,s3) for some term s3 of the sort mynat and zs is a list of type mylist(a, \ns3); the value pf , which we call a witnessing value, essentially serves as a witness to the fact that \n|s3| = |s1| + |s2|, that is, the length of zs is the sum of the lengths of xs and ys. So far, what we \nhave described can already be implemented in Dependent ML (DML) [21]. Certainly, the programming style \nas is presented in Figure 2 is more involved than the usual functional programming style. However, this \nis not so much a concern as we expect to make only occasional use of this programming style. In particular, \nwe emphasize that the programmer can choose not to program in such a style by simply avoiding capturing \ncertain pro\u00adgram invariants. What is of real concern is the need for constructing witnessing values (e.g., \npf in the de.nition of myappend) at run\u00adtime. For instance, we have a realistic example (array subscripting \nfunction) where the underlying algorithm is O(1)-time but an in\u00advolved witnessing value takes O(n)-time \nto construct. This is sim\u00adply unacceptable in practice. The primary contribution of the paper lies in \nthe novel pro\u00adgramming language design we propose that allows programs to be combined with proofs while \nobviating the need for construct\u00ading witnessing values at run-time. With this design, we save not only \ntime but also space when evaluating programs (that contain proofs). More importantly, we become able \nto verify at compile\u00adtime the correctness of witnessing values, that is, these values in\u00addeed witness \nthe facts they are supposed to witness. Though we only combine programming with proofs from a particular \nproof system (based on intuitionistic predicate logic) in this paper, we stress that the design itself \nis general and .exible in its nature. For instance, we also support in ATS the construction of proofs \nbased a form of linear logic (closely related to separation logic [17] for establishing properties on \nmemory) [30]. In support of the practi\u00adcality of this design, we have .nished a running implementation \nof ATS [27] and written tens of thousands lines of code in ATS itself2, where a signi.cant part is involved, \neither directly or indirectly, with proof construction. At this point, we stress that this design for \ncombining program\u00adming with theorem proving is fundamentally different from the programming paradigm \n(as is supported in certain theorem prov\u00ading systems such as NuPrl [4] and Coq [7]) in which (total) \npro\u00adgrams are extracted out of proofs. In ATS, program construction may involve programming constructs \nsuch as general recursion and nonexhaustive pattern matching that are in principle not allowed in proof \nconstruction. The distinction between proofs and programs we propose is partly inspired by the distinction \nbetween logical parts and informative parts employed in extracting programs out of proofs in Coq [14]. \nHowever, there is also a profound difference: We allow proofs in programs but not programs in proofs \nwhile log\u00adical parts may contain informative parts and vice versa. In partic\u00ad 2 The library of ATS alone \nalready contains more than 20,000 lines of code in ATS at this moment. ular, we extract nothing out of \nproofs, which are simply erased at run-time. This will be further illustrated later with some concrete \nexamples. Also, we emphasize that combining programming with theorem proving is not just a simple matter \nof hooking up programming languages with (automated) theorem provers. After all, we have so far not seen \nit done elsewhere effectively in practice. Thus, we consider a design that actually supports practical \nprogramming with theorem programming to be an important contribution. The rest of the paper is organized \nas follows. In Section 2, we demonstrate an approach to combining programs with proofs in the design \nand formalization of a language .pf , setting up some ma\u00adchinery for further development. In order to \nreap the bene.ts of combining programs with proofs, we extend .pf to ..,. in Sec\u00ad pf tion 3 by introducing \nuniversally as well as existentially quanti.ed types. We then present a few examples in Section 4 to \ngive the reader some concrete feel as to how the approach to combining programs with proofs can be applied \nin practice. Lastly, we men\u00adtion some related work and conclude. There is a full version of the paper \navailable on-line [1] in which more details such as proofs and examples can be found.  2. Formal Development \nIn this section, we present a typed language .pf , formally demon\u00adstrating a design for combining programs \nwith proofs. The lan\u00adguage .pf , which is essentially built on top of the simply typed .-calculus, is \nnot intended for demonstrating some practical appli\u00adcations of combining programs with proofs as such \napplications are dif.cult to .nd until dependent types are introduced. The primary purpose of .pf is \nto set up the machinery needed for further devel\u00adopment. The syntax of .pf is given in Figure 3. There \nare proof terms and dynamic terms in .pf , and we are to present rules for assigning types to these terms. \nIn order to avoid potential confusion, the types for proof terms are called props. We use P for props, \nd for proof terms and v for proof values. Also, we use . for contexts in which proof variables are declared. \nThe rules for assigning props to proof terms are given in Figure 4, where we use a judgment of the form \n. f d : P to mean that d can be given the prop P under the context .. We use T for types, d for dynamic \nterms and v for dynamic values. There are two forms of dynamic variables: x and f; we use the names lam-variable \nand .x-variable to refer to x and f , respectively; the former is a value while the latter is not. We \nmay write xf to mean either a lam-variable or a .x-variable. Intuitively, a type of the form P * T is \nto be assigned to a value of the form (v, v) such that v is a proof value of prop P and v is a dynamic \nvalue of type T ; therefore, if a value of type P * T is produced, then we know that the prop P holds. \nAlso, a type of the form P . T is to be assigned to a value of the form lam x.v, which can only be of \nuse if a proof of prop P is made available. For those who are familiar with the recently proposed framework \nATS [25, 27], these two forms of types are closely related to but different from asserting types and \nguarded types in ATS. The typing judgment in .pf is of the form .; . f d : T , where we use . for contexts \nin which dynamic variables are declared, and the rules for deriving such typing judgments are given in \nFigure 5. We now assign dynamic semantics to dynamic terms. For doing so, we also need to assign dynamic \nsemantics to proof terms. As usual, we .rst introduce the notion of evaluation contexts in Figure 3. \nGiven a proof evaluation context E, we write E[d] for the proof term obtained from replacing the hole \n[] in E with d. Given a dynamic evaluation context E, we know that it contains a hole which is either \n[] or []; in the former case, we write E[d] for the dynamic term obtained from replacing [] in E with \nd; in (pr-var) .,x : P f x : P (pr-unit) . f () : 1 . f d1 : P1 . f d2 : P2  (pr-tup) . f(d1,d2) : \nP1 * P2 . f d : P1 * P2  (pr-fst) . f fst(d): P1 . f d : P1 * P2 (pr-snd) . f snd(d): P2 .,x : P1 f \nd : P2 (pr-lam) . f lam x.d : P1 . P2 . f d1 : P1 . P2 . f d2 : P1  (pr-app) . f app(d1,d2): P2 Figure \n4. The rules for assigning props to proof terms in .pf the latter case, we we write E[d] for the dynamic \nterm obtained from replacing [] in E with d. We next introduce proof redexes and dynamic redexes. DEFINITION \n2.1. We de.ne proof redexes and dynamic redexes as follows. fst((v1,v2)) is a proof redex, and its reduction \nis v1.  snd((v1,v2)) is a proof redex, and its reduction is v2.  app(lam x.d, v) is a proof redex, \nand its reduction is d[x . v].  let (x, x) = (v, v) in d is a dynamic redex, and its reduction is d[x \n. v][x . v].  app(lam x.d, v) is a dynamic redex, and its reduction is d[x . v].  let x = v in d is \na dynamic redex, and its reduction is d[x . v].  fst((v1,v2)) is a dynamic redex, and its reduction \nis v1.  snd((v1,v2)) is a dynamic redex, and its reduction is v2  app(lam x.d, v) is a dynamic redex, \nand its reduction is d[x . v].  let x = v in d is a dynamic redex, and its reduction is d[x . v].  \n.x f.d is a dynamic redex, and its reduction is d[f . .x f.d].  We leave out the details on the (standard) \nsubstitution involved in the above de.nition. Given d1 and d2 such that d1 = E[d] and d2 = E[d ' ] for \nsome proof redex d and its reduction d ' , we write d1 . d2 and say that d1 reduces to d2 in one step. \nGiven d1 and d2, we write d1 . d2 and say that d1 reduces to d2 in one step if (1) d1 = E[d1] and d2 \n= E[d2] for some d1 . d2 or (2) d1 = E[d] and d2 = E[d ' ] for some dynamic redex d and its reduction \nd ' . We use . * and . * for the re.exive and transitive closures of . and ., respectively. The type \nsoundness of .pf can be established in a standard manner, and some of the lemmas and theorems involved \nare given as follows. Please see [1] for details on proofs. LEMMA 2.2 (Substitution). We have the following: \n1. Assume that . f d1 : P1 and .,x : P1 f d2 : P2 are derivable. Then . f d2[x . d1]: P2 is also derivable. \nprops proof terms proof values proof var. ctx. types dynamic terms dynamic values dynamic var. ctx. proof \neval. ctx. dynamic eval. ctx. P ::= 1 | P1 * P2 | P1 . P2 d ::= x | () | (d1,d2)| fst(d) | snd(d) | \nlam x.d | app(d1,d2) v ::= x |(v1,v2)| lam x.d . ::= \u00d8| .,x : P T ::= 1 | P * T | P . T | T1 * T2 | T1 \n. T2 d ::= x | f | () | (d, d)| let (x, x) = d1 in d2 | lam x.v | app(d, d) | let x = d in d | (d1,d2)| \nfst(d) | snd(d) | lam x.d | app(d1,d2) | let x = d1 in d2 | .x f.d v ::= x |(v, v)| lam x.v |(v1,v2)| \nlam x.d . ::= \u00d8| .,x : T E ::= [] |(E, d)|(v, E)| fst(E) | snd(E) | app(E,d) | app(v, E) E ::= [] |([],d)|(v, \nE)| let (x, x) = E in d | app(E, d) | app(v, []) | let x = [] in d (E, d)|(v, E)| fst(E) | snd(E) | app(E,d) \n| app(v, E) | let x = E in d Figure 3. The syntax for .pf 2. Assume that . f d : P and .,x : P ;. f d \n: T are derivable. Then .; . f d[x . d]: T is also derivable. 3. Assume that .; . f d1 : T1 and .; .,x \n: T1 f d2 : T2 are derivable. Then .; . f d2[x . d1]: T2 is also derivable.  THEOREM 2.3 (Totality). \nAssume that \u00d8f d : P is derivable. Then d . * v holds for some proof value v of prop P . THEOREM 2.4 \n(Subject Reduction). Assume that \u00d8; \u00d8f d : T is derivable and d . d ' holds. Then \u00d8; \u00d8f d ' : T is also \nderivable. THEOREM 2.5 (Progress). Assume that \u00d8; \u00d8f d : T is derivable. Then either d is a value or \nd . d ' holds for some dynamic term d ' . We are now ready to establish a key property of .pf , which \nstates that proof terms cannot affect the dynamic semantics of a dynamic term. We .rst introduce an erasure \nfunction in Figure 6, which erases all syntax related to proof terms in a given dynamic term. The following \ntheorem indicates that the evaluation of a well-typed closed dynamic term d can be performed by simply \nevaluating the erasure of d, thus obviating the need for constructing proof values at run-time. THEOREM \n2.6. Assume that \u00d8; \u00d8f d : T is derivable. 1. If d . * v, then |d|. * |v|. 2. If |d|. * v, then d . \n* v ' for some dynamic value v ' such that |v ' | = v.  Note that Theorem 2.3 plays a crucial r ole \nin the proof of Theo\u00adrem 2.6.  3. Extension While the basic design for combining programs with proofs \nis already demonstrated in the formalization of .pf , it is nonetheless dif.cult to truly reap the bene.ts \nof this design given that the type system of .pf is simply too limited. We now extend .pf to .pf .,. \nwith universally as well as existentially quanti.ed types. Following the work in [25, 27], we present \nin the rest of this section an overview of this extension. Like an applied type system [25], that is, \na type system formed in the framework ATS, .pf .,. consists of a static component (statics) and a dynamic \ncomponent (dynamics). The (additional) syntax for .pf .,. is given in Figure 7. The statics itself is \na simply typed language and a type in it is called sort. We assume the existence of the following basic \nsorts: bool, int, prop and type; bool is the sort for truth values, and int is the sort for integers, \nand prop is the sort for props, and type is the sort for types. We use a for static variables, b for \ntruth values tt and ff, and i for integers. A term s in the statics is called a static term, and we write \nS f s : s to mean that s can be given the sort s under the context S, which assigns sorts to static variables. \nThe rules for assigning sorts to static terms are omitted as they are completely standard. In this presentation, \na static term s is either a static boolean term B of the sort bool, or a static integer I of the sort \nint, or a prop P of the sort prop, or a type T of the sort type. In practice, we allow the programmer \nto introduce new sorts through datasort declarations, which are rather similar to datatype declarations \nin ML. We assume some primitive functions cB and cI when forming static terms of the sorts bool and int; \nfor instance, we can form terms such as I1 + I2, I1 - I2, I1 = I2, \u00acB, B1 . B2, etc. We use B for a sequence \nof static boolean terms and S; B |= B for a constraint that means for any substitution T:S, if each static \nboolean term in B[T] equals tt then so does B[T]. Note that we use T:S to mean \u00d8f T(a) : S(a) holds for \neach a . dom(T) = dom(S). In practice, such a constraint relation is often determined by some automatic \ndecision procedure. We now brie.y explain some unfamiliar syntax of ..,. . pf B . T is called a guarded \ntype and B .T is called an asserting type. As an example, the following type is for a function from natural \nnumbers to negative integers: .a1 : int.a1 = 0 . (int(a1) ..a2 : int.(a2 < 0) . int(a2)) The guard a1 \n= 0 indicates that the function can only be applied to an integer that is greater than or equal to 0; \nthe assertion a2 < 0 means that each integer returned by the function is negative. The markers .+ (\u00b7), \n.- (\u00b7), .(\u00b7), .+(\u00b7), .-(\u00b7), .(\u00b7) are intro\u00adduced to establish a lemma needed for conducting inductive \nrea\u00adsoning on typing derivations. Please see [25] for further expla\u00adnation on this issue. In addition, \nwe introduce two type constructors bool and int; given a static boolean term B, bool(B) is the singleton \ntype in which the only value is the truth value of B; similarly, given an integer I, int(I) is the singleton \ntype in which the only value is the integer I. sorts s ::= bool | int | prop | type static contexts \nS ::= \u00d8| S,a : s static bool. terms B ::= b | cB(s1,...,sn) static int. terms I ::= i | cI (s1,...,sn) \nprops P ::= ... | B . P |.a : s.P | B . P |.a : s.P types T ::= ... | a | bool(B) | int(I) | B . T |.a \n: s.T | B . T |.a : s.T proof terms d ::= ... |.+ (d) |.- (d) |.+(d) |.-(d) |.(d) | let . (x)= d1 in \nd2 |.(d) | let .(x)= d1 in d2 dynamic terms d ::= ... | if(d1,d2,d3) | let . (x)= d in d | let .(x)= \nd in d | .+ (d) |.- (d) |.+(d) |.-(d) |.(d) | let . (x)= d1 in d2 |.(d) | let .(x)= d1 in d2 Figure \n7. The syntax for ..,. pf .; ., xf : T f xf : T (ty-var) . f d : P .; . f d : T .; . f (d, d) : P * T \n(ty-pr-tup-i) .; . f d1 : P * T1 .,x : P ;.,x : T1 f d2 : T2 (ty-pr-tup-e) .; . f let (x, x) = d1 in \nd2 : T2 .,x : P ;. f v : T (ty-pr-lam) .; . f lam x.v : P . T .; . f d : P . T . f d : P (ty-pr-app) \n.; . f app(d, d): T . f d : P .,x : P ;. f d : T (ty-pr-let) .; . f let x = d in d : T (ty-unit) .; \n. f () : 1 .; . f d1 : T1 .; . f d2 : T2 (ty-tup) .; . f(d1,d2) : T1 * T2 .; . f d : T1 * T2 (ty-fst) \n.; . f fst(d): T1 .; . f d : T1 * T2 (ty-snd) .; . f snd(d): T2 .; .,x : T1 f d : T2 (ty-lam) .; . \nf lam x.d : T1 . T2 .; . f d1 : T1 . T2 .; . f d2 : T1 (ty-app) .; . f app(d1,d2): T2 .; . f d1 : T1 \n.; .,x : T1 f d2 : T2 (ty-let) .; . f let x = d1 in d2 : T2 .; .,f : T f d : T (ty-.x) .; . f .x f.d \n: T Figure 5. The rules for assigning types to dynamic terms in .pf A judgment for assigning a prop to \na proof term is now of the form S; B;. f d : P , and the rules in Figure 4 need to be properly modi.ed. \nIntuitively, such a judgment means that .[T] f d[T] : P [T] holds for any substitution T:S such that \nB[T] holds for |xf | |(d, d)| |let (x, x) = d1 in d2| |lam x.v| |app(d, d)| |let x = d in d| |(d1,d2)| \n|fst(d)| |snd(d)| |lam x.d| |app(d1,d2)| |let x = d1 in d2| |.x f.d| = xf = |d| = let x = |d1| in |d2| \n= |v| = |d| = |d| = (|d1|, |d2|) = fst(|d|) = snd(|d|) = lam x.|d| = app(|d1|, |d2|) = let x = |d1| in \n|d2| = .x f.|d| Figure 6. The erasure function each B in B. Some additional rules for assigning props \nto proof terms are given in Figure 8. Similarly, a judgment for assigning a type to a dynamic term is \nnow of the form S; B; .; . f d : T , and the rules in Figure 5 need to be modi.ed properly. Some additional \nrules for assigning types to dynamic terms are given in Figure 9. Following the development of ATS, it \nis a standard routine to establish the type soundness of .pf .,.. Then we can prove a theorem in .pf \n.,. that corresponds to Theorem 2.6. In practice, we also need to allow the use of recursion in constructing \nproof terms. It is clear that we cannot support unrestricted general recursion as it would otherwise \nallow the construction of proof terms that are not normalizing and thus invalidate Theorem 2.3, which \nplays a crucial r ole in establishing Theorem 2.5. Instead, we follow the work in [23], providing a means \nfor the programmer to de.ne terminating proof terms by supplying a form of metrics. This point will be \nmade clear when we present some examples in the next section. Another issue in practice is the need for \nrecursive props (dataprops) and recursive types (datatypes), which are not present in ..,. for the pf \nsake of brevity. It should be understood that recursive props and recursive types can be readily added \ninto ..,. without dif.culty.3 pf We now use a simple example to illustrate some of these mentioned issues. \nIn Figure 10, we declare a prop constructor MUL, where the concrete syntax indicates that there are three \n(proof) value constructors associated with MUL, which are given the following 3 As for the de.nition \nof a recursive prop, we require that the de.ned prop itself have no negative occurrences in the de.nition. \nOtherwise, a nonterminating proof term can be constructed without using .xed-point operator. S; B, B;. \nf d : P (pr-.+) S; B;. f.+ (d): B . P S; B;. f d : B . P S; B |= B (pr-.-) S; B;. f.- (d): P S,a : \ns; B;. f d : P (pr-.+) S; B;. f.+(d): .a : s.P S; B;. f d : .a : s.P S f s : s (pr-.-) S; B;. f.-(d): \nP [a . s] S; B |= B S; B;. f d : P (pr-.+) S; B;. f.(d): B . P S; B;. f d1 : B . P1 S; B, B;.,x : P1 \nf d2 : P2 (pr-.-) S; B;. f let . (x)= d1 in d2 : P2 S f s : s S; B;. f d : P [a . s]  (pr-.+) S; B;. \nf.(d): .a : s.P S; B;. f d1 : .a : s.P1 S,a : s; B;.,x : P1 f d2 : P2 (pr-.-) S; B;. f let .(x)= d1 \nin d2 : P2 Figure 8. Some additional rules for assigning props to proof terms constant props: MULbas \n: .n : int.() . MUL(0, n, 0) MULind : .m : int..n : int. m > 0 . (MUL(m - 1, n, p - n) . MUL(m, n, p)) \nMULneg : .m : int..n : int. m < 0 . (MUL(-m, n, -p) . MUL(m, n, p)) Given integers I1,I2,I3, it is clear \nthat I1 * I2 = I3 holds if and only if MUL(I1,I2,I3) can be assigned to a closed (proof) value. In essence, \nMULbas, MULind and MULneg correspond to the following three equations in an inductive de.nition of the \nmultiplication function on integers: 0 * n =0; m * n =(m - 1) * n + n if m> 0; m * n = -((-m) * n) if \nm< 0. In Figure 10, lemma1 is de.ned as a proof function of the follow\u00ading prop: .m : nat..n : nat..p \n: int. MUL(n, m, p) . MUL(n, m +1,p + n) Note that we use the keyword prfun to declare a proof function. \nEssentially, lemma1 represents an inductive proof of n * m = p . n * (m +1) = p + n for all natural numbers \nm, n and integers p, where the induction is on n. In particular, the following two linear arithmetic \nconstraints, which can be easily veri.ed, are generated when the two clauses in the body of lemma1 are \ntype-checked: .n : nat..p : int.n =0 . (p =0 . 0= p + n) .m : nat..n : nat..p : int..n ' : int..p ' : \nint. '' '' n = n +1 . (p = p + m . p + n =(p + n )+(m + 1)) However, in order for lemma1 to represent \na proof, we need to show that lemma1 is a total function, that is, given pf of prop MUL(I2,I1,I3) for \nnatural numbers I1 and I2 and inte\u00adger I3, lemma1(pf ) is guaranteed to return a proof value of prop \nS; B; .; . f d1 : bool(B) S; B, B; .; . f d2 : T S; B, \u00acB; .; . f d3 : T  (ty-if) S; B; .; . f if(d1,d2,d3): \nT S; B;. f d : B . P S; B, B;.,x : P ;. f d : T  (ty-pr-.-) S; B; .; . f let . (x)= d in d : T S; B;. \nf d : .a : s.P S,a : s; B;.,x : P ;. f d : T  (ty-pr-.-) S; B; .; . f let .(x)= d in d : T S; B, B; \n.; . f d : T  (ty-.+) S; B; .; . f.+ (d): B . T S; B; .; . f d : B . T S; B |= B (ty-.-) S; B; .; . \nf.- (d): T S,a : s; B; .; . f d : T  (ty-.+) S; B; .; . f.+(d): .a : s.T S; B; .; . f d : .a : s.T S \nf s : s  (ty-.-) S; B; .; . f.-(d): T [a . s] S; B |= B S; B; .; . f d : T  (ty-.+) S; B; .; . f.(d): \nB . T S; B; .; . f d1 : B . T1 S; B, B; .; .,x : T1 f d2 : T2  (ty-.-) S; B; .; . f let . (x)= d1 in \nd2 : T2 S f s : s S; B; .; . f d : T [a . s] (ty-.+) S; B; .; . f.(d): .a : s.T S; B; .; . f d1 : .a \n: s.T1 S,a : s; B; .; .,x : T1 f d2 : T2  (ty-.-) S; B; .; . f let .(x)= d1 in d2 : T2 Figure 9. Some \nadditional rules for assigning types to dynamic terms MUL(I2,I1 +1,I3 + I2). Generally speaking, when \nimplement\u00ading a recursive proof function in ATS, the programmer is required to provide a metric that \ncan be used to verify the termination of the function. A thorough study on using such metrics for veri\u00adfying \nprogram termination can be found in [22, 23]. In the def\u00adinition of lemma1, (n) is the provided metric \nfor verifying that lemma1 is terminating; when lemma1 is applied to a value of prop MUL(I2,I1,I3), the \nlabel (I2) is associated with this call; in case a recursive call to lemma1 is made subsequently, the \nla\u00adbel associated with the recursive call is (I2 - 1) (since pf ' in the de.nition of lemma1 is given \nthe type MUL(n - 1, m, p - m)), which is strictly less than the label (I2) associated with the original \ncall; as a label associated with lemma1 is always a natural number, it is evident that lemma1 is terminating. \nTo show that lemma1 is total, we also need to verify that pattern matching in the de.nition of lemma1 \ncan never fail, which is a topic that is already studied elsewhere [20, 24]. dataprop MUL (int, int, \nint) = | {n:int} MULbas (0, n, 0) | {m:int,n:int,p:int | m > 0} MULind (m, n, p) of MUL (m-1, n, p-n) \n| {m:int,n:int,p:int | m > 0} MULneg (m, n, p) of MUL (~m, n, ~p) (* <n> is a termination metric *) \nprfun lemma1 {m:nat, n:nat, p:int} .<n>. (pf: MUL (n, m, p)): MUL (n, m+1, p+n) = case pf of MULbas => \nMULbas | MULind pf => MULind (lemma1 pf ) Figure 10. A dataprop for encoding integer multiplication \n 4. Examples We present a few examples in ATS to give the reader some concrete feel as to how combining \nprogramming with theorem proving can be put into practice. There are also a large number of more realistic \nexamples that can be found on-line [27]. Of course, we need a process to elaborate programs written in \nthe concrete syntax of ATS into the (kind of) formal syntax .,. of .pf . This is a rather involved process, \nand we unfortunately could not formally describe it in this paper and thus refer the interested reader \nto [26] for further details. Instead, we are to provide some (informal) explanation to facilitate the \nunderstanding of the concrete syntax we use. We use ieq, ipred, iadd, isub and imul for the equality \nfunc\u00adtion, the predecessor function, the addition function, the subtrac\u00adtion function and the multiplication \nfunction on integers, which are given the following types: ieq : .a1 : int..a2 : int. (int(a1), int(a2)) \n. bool(a1 = a2) ipred : .a : int. int(a) . int(a - 1) iadd : .a1 : int..a2 : int. (int(a1), int(a2)) \n. int(a1 + a2) isub : .a1 : int..a2 : int. (int(a1), int(a2)) . int(a1 - a2) imul : .a1 : int..a2 : int. \n(int(a1), int(a2)) . .a3 : int. MUL(a1, a2, a3) * int(a3) Note that imul is not given the following \ntype: .m : int..n : int. (int(m), int(n)) . int(m * n) as m * n, which is nonlinear, is not allowed to \nbe a type index in ATS. 4.1 List Concatenation The code for implementing concat is given in Figure 11, \nwhich concatenates a given list of lists together. We write ' (...) to form a tuple, where the quote \nsymbol ( ' ) is used solely for the purpose of parsing. Also, we use the bar symbol ( | ) as a separator \nto sepa\u00adrate proofs from programs. When given an argument xss of type list(list(T,I2),I1), the function \nconcat returns a pair (pf , res) such that pf is a proof value of prop MUL(I1,I2,I3) for some integer \nI3 and res is a list of type list(T,I3). Therefore, pf acts as a witness to certify that the length of \nres is I1 * I2. Now suppose fun concat {a:type, m:nat, n:nat} (xxs: list (list (a, n), m)) : (MUL (m, \nn, p) | list (a, p)) = case xxs of | nil => nil | cons (xs, xss) => let val (pf | res) = concat xss \nin (MULind (pf) | append (xs, res)) end Figure 11. An implementation of the list concatenation function \nfun fact1 {a:nat} (x: int a): Int = if xieq0then 1else let val (pf | r) = x imul fact1 (ipred x) in r \nend // <a1> is the termination metric prfun lemma2 {a1:nat, a2:nat, a3:int} .<a1>. (pf: MUL (a1, a2, \na3)): [a3 >= 0] () = case pf of MULbas => () | MULind pf => lemma2 pf fun fact2 {a:nat} (x: int a): Nat \n= if xieq0then 1else let val (pf | r) = x imul fact2 (ipred x) prval _ = lemma2 (pf) // proves r >= 0 \nin r end dataprop FACT (int, int) = | FACTbas (0, 1) | {n:int, r:int, r1:int | n > 0} FACTind(n,r) of \n(FACT(n-1,r1), MUL(n,r1,r)) fun fact3 {a:nat} (x: int a) : [r:int] (FACT (a, r) | int r) = if x ieq 0 \nthen (FACTbas | 1) else let val (pf1 | r1) = fact3 (ipred x) val (pf2 | r) = x imul r1 in (FACTind (pf1, \npf2) | r) end Figure 12. Some implementations of the factorial function we would like to assign concat \nthe following type: .a : type..m : nat..n : nat. list(list(a, n),m) ..p : int. MUL(n, m, p) * list(a, \np) which is obtained from replacing the prop MUL(m, n, p) with the prop MUL(n, m, p) in the above type \nassigned to concat. Then we need to replace MULind(pf ) in the de.nition of concat with lemma1(pf ), \nwhere lemma1 is de.ned in Figure 10. 4.2 Different Implementations of the Factorial Function We present \nthree implementations of the factorial function in Fig\u00adure 12 to make an interesting point. The function \nfact1 is given the following type: .a : nat. int(a) . Int where Int, de.ned to be .a : int. int(a), is \nthe type for all integers. This type simply means that fact1 is a function from natural numbers to integers. \nNote that we use the bar symbol (|) in the concrete syntax to separate proof terms from dynamic terms \nin a tuple. The function fact2 is given the following type: .a : nat. int(a) . Nat where Nat, de.ned \nto be .a : nat.int(a), is the type for all natural numbers. Hence, fact2 is a function from natural numbers \nto natural numbers. When implementing fact2, we need to prove that the product of two natural numbers \nis a natural number. This is done by the proof function lemma2 de.ned in Figure 12, which is assigned \nthe following prop: .a1 : nat..a2 : nat..a3 : int. MUL(a1,a2,a3) . (a3 = 0) . 1 where 1 is the unit prop. \nThe syntax .<a1>. in the header of the de.nition of lemma2 indicates that a1 is the metric (provided \nby the programmer) for establishing the termination of lemma2. Note that the keyword prval in the body \nof the function fact2 indicates pattern matching on proof values, which is erased before program execution. \nNext we declare a prop constructor FACT, which forms a prop FACT(I1,I2) when applied to two given integers \nI1 and I2; there is a closed value of prop FACT(I1,I2) if and only if I2 equals the factorial of I1. \nThe function fact3 is assigned the following type: .a1 : nat. int(a1) ..a2 : int.FACT(a1,a2) * int(a2) \nWhen applied to a value of type int(I1) for some natural number I1, fact3 returns a proof pf of prop \nFACT(I1,I2) for some integer I2 and a value of type int(I2). Of course, the proof is not actually constructed \nat run-time. Clearly, fact1, fact2, fact3 all implement the factorial function, but the types assigned \nto them become more and more accurate. Intuitively speaking, the programmer is given some freedom in \nATS to determine the extent of theorem proving to be involved based on the invariants that need to be \ncaptured. 4.3 Implementing the call-by-value evaluation for the pure untyped .-calculus In contrast \nto extracting programs out of proofs, we emphasize that dynamic functions such as fact1, fact2 and fact3 \nare not meant to correspond to any proofs in the .rst place. In particular, they are not assumed to be \nterminating (though fact1, fact2, fact3 are all terminating) and may incur effects (e.g., updating references, \nraising exceptions). This is crucial to practical programming. To further stress this point, we give \nan implementation of the call-by\u00advalue evaluation for the pure untyped .-calculus in Figure 13. We declare \na datasort tm such that each static term of the sort tm represents an untyped .-term. For instance, the \nstatic term TMlam(.x : tm.TMlam(.y : tm.TMapp(y, x))) repre\u00adsents .x..y.y(x). This representation strategy \nis referred to as higher-order abstract syntax [16]. We then declare a prop construc\u00adtor EVAL that forms \na prop EVAL(s1,s2) when applied to two static terms s1 and s2 of the sort tm; there exists a closed proof \nvalue of prop EVAL(s1,s2) if and only if the .-term represented by s1 evaluates to the .-term represented \nby s2. We then declare a type constructor EXP which forms a type EXP(s) for each term s of the sort tm \nsuch that an untyped .-term represented by s can be represented by a dynamic value of type EXP(s).4 We \nnext 4 This is a higher-order representation that is not adequate as there are dy\u00adnamic values of type \nEXP(s) that do not correspond to the .-term rep\u00ad datasort tm = TMlam of (tm -> tm) | TMapp of (tm, tm) \ndataprop EVAL (tm, tm) = | {f: tm -> tm} EVALlam (TMlam f, TMlam f) | {t1: tm, t2: tm, f1: tm -> tm, \nv2: tm, v: tm} EVALapp (TMapp (t1, t2), v) of (EVAL (t1, TMlam f1), EVAL (t2, v2), EVAL (f1 v2, v)) \ndatatype EXP (tm) = | {f: tm -> tm} EXPlam(TMlam f) of {t:tm} EXP t -> EXP(f t) | {t1: tm, t2: tm} EXPapp(TMapp \n(t1, t2)) of (EXP t1, EXP t2) fun evaluate {t: tm} (e: EXP t) : [t : tm] (EVAL (t, t ) | EXP t ) = case \ne of | EXPlam _ => (EVALlam | e) | EXPapp (e1, e2) => let val (pf1 | EXPlam f1) = evaluate e1 val (pf2 \n| v2) = evaluate e2 val (pf3 | v) = evaluate (f1 v2) in (EVALapp (pf1, pf2, pf3) | v) end Figure 13. \nAn implementation of the call-by-value evaluation for the pure untyped .-calculus implement a function \nevaluate of the following type: ' '' .a : tm.EXP(a) ..a : tm.EVAL(a, a ) * EXP(a ) When applied to a \nvalue of type EXP(s) for some static term s of the sort tm, evaluate can only return a value of type \nEXP(s ' ) such that a proof value of prop EVAL(s, s ' ) exists. However, it may never return as there \ncertainly exist .-terms that do not evaluate to any values. In other words, evaluate is not a total function \nand thus cannot in principle be extracted out of any (valid) proof. 4.4 Safe Matrix Subscripting We \nnow present a simple but realistic example. In Figure 14, we .rst implement a proof function lemma3. \nWe use the keyword prfun in the concrete syntax to indicate that a proof function is de.ned. Essentially, \nlemma3 proves the statement that i * col + col = row * col holds if col, row and i are natural numbers \nand i < row holds. In the de.nition of lemma3, the syntax .<row>. indicates that row is a metric supplied \nby the programmer; it can be easily ver\u00adi.ed that the metric decreases when a recursive call to lemma3 \nis made in the body of lemma3; this guarantees lemma3 is terminat\u00ading [23]. Also, it can be easily veri.ed \nthat pattern matching in the body of lemma3 is exhaustive, and thus lemma3 is a total function. We next \nshow in Figure 14 how a safe matrix subscripting function matrixSub is implemented. Given a type T and \nan integer I, we can form a type array(T,I) in ATS for arrays of size I resented by s. In Appendix, we \nare to present a more realistic implemen\u00adtation of the call-by-value evaluation of .-calculus, which \nmakes use of a .rst-order adequate representation for .-terms and avoids the need for sub\u00adstitution by \nforming closures. prfun lemma3 {row:nat, col:nat, size:int, i:nat, p:int | i < row} .<row>. (pf1: MUL(row,col,size), \npf2: MUL(i,col,p)) : [p + col <= size] unit = case pf1 of | MULind (pf1) => begin case pf2 of | MULbas \n=> let val _ = lemma2 (pf1) in () end | MULind pf2 => let val _ = lemma3 (pf1, pf2) in () end end typedef \nmatrix (a: type, row: int, col: int) = [size:int] (MUL (row,col,size) | int row, int col, array (a, size)) \nfun matrixSub {a:type, row:nat, col:nat, i:nat, j:nat | i < row, j < col} (M: matrix(a,row,col), i: int \ni, j: int j): a = let val (pf1 | row, col, A) = M val (pf2 | p) = i imul col // proves: p >= 0 prval \n_ = lemma2 (pf2) // proves: p + col <= row * col prval _ = lemma3 (pf1, pf2) in arraySub (A, p iadd \nj) end Figure 14. An example of combining programs with proofs in which each element is of type T . \nThe usual array subscripting function arraySub is given the following type: .a : type..n : nat..i : nat. \ni<n . ((array(a, n), int(i)) . a) Therefore, the index used to access an array is required to be within \nthe bounds of the array (we assume the index of a given array ranges from 0 until n - 1, where n is the \nsize of the array). In Figure 14, we de.ne a type constructor matrix; given a type T and two integers \nI1 and I2, matrix(T,I1,I2) is de.ned to be: .p : int. MUL(I1,I2,p) * int(I1) * int(I2) * array(T,p) which \nindicates that a matrix of dimension I1 by I2 is represented as an array of size I1 * I2. The function \nmatrixSub implemented in Figure 14 is assigned the following type as can be expected: .a : type..row \n: nat..col : nat..i : nat..j : nat. i < row . (j < col . ((matrix(a, row, col), int(i), int(j)) . a)) \nThe following two lines of code in the de.nition of matrixSub are uncommon, and we now provide some explanation. \nprval _ = lemma2 (pf2) // ... prval _ = lemma3 (pf1, pf2) // ... The use of the keyword prval is to indicate \nthat the pattern fol\u00adlowing it is to match a proof value. If we assume that pf 2 is of prop MUL(i, col, \np) for some integer p, then lemma2(pf 2) is of prop (p = 0) . 1; the code prval = lemma2 (pf2) essen\u00adtially \nelaborates into let . (x)= lemma2(pf 2) in ..., which means p = 0 can be assumed when we solve the constraints \ngener\u00adated in the scope of this let-binding; the typing rule involved here is (ty-pr-.-). Similarly, \nprval = lemma3 (pf1, pf2) essen\u00adtially elaborates into let .(x)= lemma3(pf 1, pf 2) in ...; the prop \nof lemma3(pf 1, pf 2) is (p + col = size) . 1, where we assume pf 1 is of prop MUL(row, col, size), and \nthus p + col = size can be assumed when we solve the constraints generated in the scope of this let-binding. \nLastly, we need to show that both p + j = 0 and p + j < size hold when calling arraySub(A, p iadd j); \nthe former constraint is easily proven since j is a natural number and p = 0 can be assumed; the latter \nconstraint is also easily proven since both p + col = size and j < col can be assumed. Hence, if the \npair of supplied indexes i and j are natural num\u00adbers satisfying i < row and j < col, accessing a matrix \nof dimen\u00adsion row by col via matrixSub is guaranteed to be safe. What is signi.cant here is that this \nproperty is captured in the type system of ATS. After matrix subscripting is properly handled, it is \nstraightfor\u00adward to implement various other functions (e.g., multiplication) on matrices. At this point, \nwe stress that the programmer may also decide to insert run-time array bound checks to implement the \nma\u00adtrix subscripting function matrixSub. By doing so, it is no longer necessary to construct the proof \nfunction lemma3, though this also means that the absence of illegal array subscripting can no longer \nbe guaranteed in the underlying type system.  5. Related Work and Conclusion A fundamental problem in \nprogramming is to .nd approaches that can effectively facilitate the construction of safe and reliable \nsoft\u00adware, and we have so far seen numerous attempts to address this problem. An interesting idea is \nto build a language based on Martin\u00adL\u00a8of s constructive type theory [9, 13] or its variants in which \nsoft\u00adware speci.cations can be formally stated and proven and an im\u00adplementation can be algorithmically \nextracted from the proof of a speci.cation to guarantee that the extracted implementation meets the speci.cation. \nWhile the practicality of such an idealistic ap\u00adproach to software development is yet to be proven, the \nnotion ex\u00adpressed in this approach of integrating software design and imple\u00admentation in a veri.ably \nconsistent manner is certainly inspiring. Constructive type theory, which was originally proposed by \nMartin-L\u00a8of primarily for the purpose of establishing a foundation for mathematics, requires pure reasoning \non programs (or proofs, more precisely). This requirement seems to have imposed a funda\u00admental limitation \non the use of constructive type theory in practical programming as pure reasoning on (large and realistic) \nprograms seems simply untenable. We have recently recti.ed the situation by formalizing a framework Applied \nType System (ATS) [25, 27], completely eliminating the need for pure reasoning on programs. Also, by \nintroducing the notion of conditional type equality, we have made ATS highly expressive in capturing \nprogramming in\u00advariants. For instance, we have already formally demonstrated that ATS can be used as \na basis to support in a typeful manner a variety of programming paradigms such as functional programming, \nim\u00adperative programming, object-oriented programming, modular pro\u00adgramming meta-programming, etc. In \nATS, a constraint relation is involved in determining type equality. In order to support effective constraint \nsolving, we adopted a design in the past that imposes certain restrictions on the syntac\u00adtic form of \nconstraints that are allowed in practice. For instance, arithmetic constraints are required to be linear \nin the current imple\u00admentation of ATS [27]. While this is a simple design, it is evidently ad hoc by \nits nature and can also be too restrictive, sometimes, in a situation where nonlinear constraints need \nto be handled. We have presented a different design in this paper. Instead of imposing restrictions, \nwe provide a means for the programmer to construct proofs that attest to the validity of constraints. \nIt is interesting to see a comparison between our design for com\u00adbining programs with proofs and theorem \nproving systems such as NuPrl [4] (based on Martin-L\u00a8 of s constructive type theory) and Coq [7] (based \non the calculus of construction [5]). In order to rea\u00adson effectively about program properties within \na type theory, the underlying functional language of a theorem proving system such as Coq or NuPrl is \noften required to be pure, making it dif.cult to support many realistic programming features (e.g., general \nre\u00adcursion, reference, exception). In general, programming in such a setting amounts to constructing \nproofs (of speci.cations) and pro\u00adgrams are automatically extracted out of the constructed proofs. This \nmeans that only total programs can be constructed in principle. Therefore, such a programming paradigm \ncan often be in.exible or even infeasible in many common situations. For instance, par\u00adtial functions \nsuch as evaluate mentioned in Section 4 are rather common in practice and they in principle cannot be \nextracted from any proofs. To a large extent, this argument also applies to Epi\u00adgram [11], a recently \ndeveloped functional programming language with a dependent type system based on UTT [8], and it is yet \nto be seen whether monads can be successfully employed to incorporate effects (including partiality of \nfunctions) into Epigram in support of practical programming. The sorts prop and type in ATS roughly correspond \nto the kinds Prop and Set in Coq [14]. However, there is a subtle difference. In .pf , types may contain \nprops but props may never contain types. For instance, P * T is a type and can never be a prop. On the \nother hand, terms of kind Prop may contain terms of kind Set in Coq and vice versa. This difference is \nprofound as it makes it possible to construct in ATS programs that may be nonterminating, raising exceptions \nor causing effects, which on the other hand is forbid\u00adden in Coq. Moreover, the design we have presented \nfor combin\u00ading programs with proofs is largely rooted in a programming lan\u00adguage, which can readily accommodate \nprogramming features such as general recursion and effects. Intuitively speaking, the design somewhat \nprovides the programmer with some .exibility in deter\u00admining the extent of theorem proving to be involved \naccording to the invariants that need to be captured. This design is fundamen\u00adtally different from program \nextraction. The theme of combining programs with proofs is also proposed in the design of the programming \nlanguage Oemga [19]. The type system of Oemga is largely built on top of a notion called equality constrained \ntypes (a.k.a. phantom types [3]), which are closely re\u00adlated to the notion of guarded recursive datatypes \n[28]. In Oemga, there seems no strict separation between programs and proofs. In particular, proofs need \nto be constructed at run-time, and thus the serious problem with proof construction at run-time as is \nmentioned in Section 1 does occur in Oemga. Also, an approach to simulating dependent types through the \nuse of type classes in Haskell is given in [10], which is casually related to proof construction in our \nde\u00adsign. However, this approach does not address the issue of proof erasure, which on the other hand \nis key in our design. Furthermore, there is currently no facility in Haskell for verifying the totality \nof a function and a proof function such as lemma1 cannot really be ad\u00adequately simulated. Please see \n[2] for a critique on the practicality of simulating dependent types in Haskell. Another line of related \nwork is the formation of a type system in support of certi.ed binaries [18], in which the idea of a com\u00adplete \nseparation between types and programs is also employed. Ba\u00adsically, the notions of type language and \ncomputational language in the type system correspond to the notions of statics and dynamics in ATS, respectively, \nthough the type language is based on the calcu\u00adlus of constructions extended with inductive de.nitions \n(CiC) [15]. However, the notion of a constraint relation in ATS does not have a counterpart in [18]. \nInstead, the equality between two types is de\u00adtermined by comparing the normal forms of these types. \nAlso, there seems so far no attempt to build a source programming language, and in particular, the theme \nof combining programs with proofs (which is done by the programmer) is not addressed there. In summary, \nwe have presented a novel design in this paper for combining programs with proofs in support of the use \nof (ad\u00advanced) types in capturing program invariants, opening a promis\u00ading avenue to making theorem proving \navailable for practical pro\u00adgramming. In support of the practicality of this design, we have .nished \na running implementation of ATS and tested a variety of examples. In particular, a large part of the \nlibrary of ATS involves the combination of programs and proofs as is described here. The presented design \nto support programming with theorem proving is both general and .exible, and we naturally expect to capture \nmore program properties by exploring other logics (in addition to intu\u00aditionistic logic) and proof systems \nand investigating whether they can be incorporated into ATS. As a matter of fact, we have already succeeded \nin developing a proof system (based on a form of linear logic) to reason about properties on memory and \nthen incorporated it into ATS [29] by following the design of combining program\u00adming with theorem proving. \nIn the future, we plan to study whether reasoning about concurrency and distribution can also be supported \nin a similar fashion. References [1] CHEN, C., AND XI, H. Combining Programming with Theorem Proving, \nNovember 2004. Available at: http://www.cs.bu.edu/~hwxi/ATS/PAPER/CPwTP.ps. [2] CHEN, C., ZHU, D., AND \nXI, H. Implementing Cut Elimination: A Case Study of Simulating Dependent Types in Haskell. In Proceedings \nof the 6th International Symposium on Practical Aspects of Declarative Languages (Dallas, TX, June 2004), \nSpringer-Verlag LNCS vol. 3057. [3] CHENEY, J., AND HINZE, R. Phantom Types. Technical Report CUCIS-TR2003-1901, \nCornell University, 2003. Available at http://techreports.library.cornell.edu:8081/ Dienst/UI/1.0/Display/cul.cis/TR2003-1901. \n[4] CONSTABLE, R. L., ET AL. Implementing Mathematics with the NuPrl Proof Development System. Prentice-Hall, \nEnglewood Cliffs, New Jersey, 1986. [5] COQUAND, T., AND HUET, G. The calculus of constructions. Information \nand Computation 76, 2 3 (February March 1988), 95 120. [6] DANTZIG, G., AND EAVES, B. Fourier-Motzkin \nelimination and its dual. Journal of Combinatorial Theory (A) 14 (1973), 288 297. [7] DOWEK, G., FELTY, \nA., HERBELIN, H., HUET, G., MURTHY, C., PARENT, C., PAULIN-MOHRING, C., AND WERNER, B. The Coq proof \nassistant user s guide. Rapport Technique 154, INRIA, Rocquencourt, France, 1993. Version 5.8. [8] LOU, \nZ. A unifying theory of dependent types: the schematic approach. Technical Report LFCS-92-202, University \nof Edinburgh, 1991. [9] MARTIN-L \u00a8 Bibliopolis, Naples, OF, P. Intuitionistic Type Theory. Italy, 1984. \n [10] MCBRIDE, C. Faking It. Journal of Functional Programming 12,4 &#38; 5 (July 2002), 375 392. [11] \nMCBRIDE, C., AND MCKINNA, J. The view from the left. Journal of Functional Programming 14, 1 (2004), \n69 111. [12] MILNER, R., TOFTE, M., HARPER, R. W., AND MACQUEEN, D. The De.nition of Standard ML (Revised). \nMIT Press, Cambridge, Massachusetts, 1997. [13] NORDSTR \u00a8 OM, B., PETERSSON, K., AND SMITH, J. M. Program\u00adming \nin Martin-L\u00a8 of s Type Theory, vol. 7 of International Series of Monographs on Computer Science. Clarendon \nPress, Oxford, 1990. [14] PAULIN-MOHRING, C. Extraction de programmes dans le Calcul des Constructions. \nTh`ese de doctorat d \u00b4etat, Universit\u00b4e de Paris VII, Paris, France, 1989. [15] PAULIN-MOHRING, C. Inductive \nDe.nitions in the System Coq: Rules and Properties. In Proceedings of the International Conference on \nTyped Lambda Calculi and Applications (Utrecht, The Netherlands, 1993), M. Bezem and J. de Groote, Eds., \nvol. 664 of Lecture Notes in Computer Science, pp. 328 345. [16] PFENNING, F., AND ELLIOTT, C. Higher-order \nabstract syntax. In Proceedings of the ACM SIGPLAN 88 Symposium on Language Design and Implementation \n(Atlanta, Georgia, June 1988), pp. 199 208. [17] REYNOLDS, J. Separation Logic: a logic for shared mutable \ndata structures. In Proceedings of 17th IEEE Symposium on Logic in Computer Science (LICS 02) (2002). \n[18] SHAO, Z., SAHA, B., TRIFONOV, V., AND PAPASPYROU, N. A Type System for Certi.ed Binaries. In Proceedings \nof 29th Annual ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 02) (Portland, OR, \nJanuary 2002), pp. 217 232. [19] SHEARD, T. Languages of the future. In Proceedings of the Onward! Track \nof Objected-Oriented Programming Systems, Languages, Applications (OOPSLA) (Vancouver, BC, October 2004), \nACM Press. [20] XI, H. Dead code elimination through dependent types. In The First International Workshop \non Practical Aspects of Declarative Languages (San Antonio, January 1999), Springer-Verlag LNCS vol. \n1551. [21] XI, H. Dependent ML. Available at http://www.cs.bu.edu/~hwxi/DML/DML.html, 2001. [22] XI, \nH. Dependent Types for Program Termination Veri.cation. In Proceedings of 16th IEEE Symposium on Logic \nin Computer Science (Boston, June 2001), pp. 231 242. [23] XI, H. Dependent Types for Program Termination \nVeri.cation. Journal of Higher-Order and Symbolic Computation 15, 1 (March 2002), 91 132. [24] XI, H. \nDependently Typed Pattern Matching. Journal of Universal Computer Science 9, 8 (2003), 851 872. [25] \nXI, H. Applied Type System (extended abstract). In post-workshop Proceedings of TYPES 2003 (2004), Springer-Verlag \nLNCS 3085, pp. 394 408. [26] XI, H. Dependent Types for Practical Programming via Applied Type System, \nSeptember 2004. Available at http://www.cs.bu.edu/~hwxi/academic/drafts/ATSdml.ps [27] XI, H. Applied \nType System, 2005. Available at: http://www.cs.bu.edu/~hwxi/ATS. [28] XI, H., CHEN, C., AND CHEN, G. \nGuarded Recursive Datatype Constructors. In Proceedings of the 30th ACM SIGPLAN Symposium on Principles \nof Programming Languages (New Orleans, LA, January 2003), ACM press, pp. 224 235. [29] XI, H., AND ZHU, \nD. Views, Types and Viewtypes, October 2004. Available at: http://www.cs.bu.edu/~hwxi/ATS/PAPER/VsTsVTs.ps. \n[30] XI, H., ZHU, D., AND LI, Y. Applied Type System with Stateful Views. Technical Report BUCS-2005-03, \nBoston University, 2005. Available at: http://www.cs.bu.edu/~hwxi/ATS/PAPER/ATSwSV.ps.  A. Implementing \nthe call-by-value evaluation for the pure untyped .-calculus via closures We present an implementation \nof the call-by-value evaluation for the pure untyped .-calculus in Figure 15. In this case, we use a \n.rst-order representation for .-terms that essentially uses deBrujin indexes to represent free variables. \nGiven a static term s of the sort tm, EXP0(s) is the type for the dynamic value that represents the .-term \nrepresented by s, and VAL(s) is the type for the closures that represents the .-term represented by s. \nThe following type is assigned to the function evaluate: ' .a : tm. EXP0(a) ..a : tm. EVAL0(a, a ' ) \n* VAL(a ' ) where EVAL0 is like EVAL in Section 4. datasort tm = TMlam of (tm -> tm) | TMapp of (tm, \ntm) datasort tms = TMSemp | TMSmore of (tms, tm) dataprop EVAL (tm, tm, int) = // the third index is \nneeded for form metrics | {f: tm -> tm} EVALlam (TMlam f, TMlam f, 0) | {t1: tm, t2: tm, f: tm -> tm, \nv1: tm, v2: tm, n1:nat, n2:nat, n3:nat} EVALapp (TMapp (t1, t2), v2, n1+n2+n3+1) of (EVAL (t1, TMlam \nf, n1), EVAL (t2, v1, n2), EVAL (f v1, v2, n3)) propdef EVAL0 (t:tm, t :tm) = [n:nat] EVAL (t, t , n) \ndataprop ISVAL (tm) = {f: tm -> tm} ISVALlam (TMlam f) // a prop definition prfun lemma1 {t:tm} (pf: \nISVAL (t)): EVAL0 (t, t) = // a value evaluates to itself case pf of ISVALlam => EVALlam // a lambda-term \ncan only be evaluated to a value prfun lemma2 {t: tm, t :tm, n:nat} .<n>. (pf: EVAL (t, t , n)): ISVAL \n(t ) = case pf of | EVALlam => ISVALlam | EVALapp (_, _, pf3) => lemma2 pf3 datatype IN (tm, tms) = \n// deBruijn indexes | {ts: tms, t: tm} INone (t, TMSmore (ts, t)) | {ts: tms, t: tm, t :tm} INshi (t, \nTMSmore (ts, t )) of IN (t, ts) datatype EXP (tms, tm) = // first-order representation for lambda-terms \n| {ts: tms, t: tm} EXPvar (ts, t) of IN (t, ts) | {ts: tms, f: tm -> tm} EXPlam (ts, TMlam f) of {t: \ntm} EXP (TMSmore (ts, t), f t) | {ts: tms, t1: tm, t2: tm} EXPapp (ts, TMapp (t1, t2)) of (EXP (ts, t1), \nEXP (ts, t2)) typedef EXP0 (t: tm) = EXP (TMSemp, t) // a type definition datatype VAL (tm) = // value \nrepresentation | {ts: tms, f: tm -> tm } VALclo (TMlam f) of (ENV (ts), {t: tm} EXP (TMSmore (ts, t), \nf t)) and ENV (tms) = // environment representation | ENVnil (TMSemp) | {ts: tms, t: tm} ENVcons (TMSmore \n(ts, t)) of (ISVAL t | VAL (t), ENV (ts)) fun eval {ts: tms, t: tm} (env: ENV (ts), e: EXP (ts, t)): \n[t :tm] (EVAL0 (t, t ) | VAL (t )) = case e of | EXPvar i => evalVar (env, i) | EXPlam body => (EVALlam \n| VALclo (env, body)) | EXPapp (e1, e2) => let val (pf1 | VALclo (env , body)) = eval (env, e1) val \n(pf2 | arg) = eval (env, e2) val (pf3 | v) = eval (ENVcons (lemma2 pf2 | arg, env ), body) in (EVALapp \n(pf1, pf2, pf3) | v) end and evalVar {ts: tms, t: tm} (env: ENV (ts), i: IN (t, ts)): (EVAL0 (t, t) \n| VAL (t)) = case i of | INone => let val ENVcons (pf | v, _) = env in (lemma1 pf | v) end | INshi i \n=> let val ENVcons (_ | _, env) = env in evalVar (env, i) end fun evaluate {t: tm} (e: EXP0 (t)) : [t \n:tm] (EVAL0 (t, t ) | VAL (t )) = eval (ENVnil, e) Figure 15. An implementation of the call-by-value \nevaluation for the pure untyped .-calculus via closures \n\t\t\t", "proc_id": "1086365", "abstract": "Applied Type System (<i>ATS</i>) is recently proposed as a framework for designing and formalizing (advanced) type systems in support of practical programming. In <i>ATS</i>, the definition of type equality involves a constraint relation, which may or may not be algorithmically decidable. To support practical programming, we adopted a design in the past that imposes certain restrictions on the syntactic form of constraints so that some effective means can be found for solving constraints automatically. Evidently, this is a rather <i>em ad hoc</i> design in its nature. In this design, which we claim to be both novel and practical. Instead of imposing syntactical restrictions on constraints, we provide a means for the programmer to construct proofs that attest to the validity of constraints. In particular, we are to accommodate a programming paradigm that enables the programmer to combine programming with theorem proving. Also we present some concrete examples in support of the practicality of this design.", "authors": [{"name": "Chiyan Chen", "author_profile_id": "81100119914", "affiliation": "Boston University", "person_id": "P414181", "email_address": "", "orcid_id": ""}, {"name": "Hongwei Xi", "author_profile_id": "81100625632", "affiliation": "Boston University", "person_id": "PP39051360", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1086365.1086375", "year": "2005", "article_id": "1086375", "conference": "ICFP", "title": "Combining programming with theorem proving", "url": "http://dl.acm.org/citation.cfm?id=1086375"}