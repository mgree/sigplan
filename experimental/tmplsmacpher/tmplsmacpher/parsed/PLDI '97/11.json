{"article_publication_date": "05-01-1997", "fulltext": "\n Aggressive Inlining Andrew Ayers Robert Gottlieb Richard Schooler Hewlett-Packard Massachusetts Language \nLaboratory 300 Apollo Drive Chelmsford, MA 01824 e-mail: {ayers,gottlieb,schooler}@ch.hp.com Abstract \nExisting research understates the benefits that can be obtained from inlining and cloning, especially \nwhen guided by profile information. Our implementation of inlining and cloning yields excellent results \non average and very rarely lowers performance. We believe our good results can be explained by a number \nof factors: inlining at the intermediate-code level removes most technical restrictions on what can be \ninlined; the ability to inline across files and incorporate profile information enables us to choose \nbetter inline candidates; a high-quality back end can exploit the scheduling and regis-ter allocation \nopportunities presented by larger subrou-tines; an aggressive processor architecture benefits from more \npredictable branch behavior; and a large instruc-tion cache mitigates the impact of code expansion. We \ndescribe the often dramatic impact of our inlining and cloning on performance: for example, the implementa-tions \nof our inlining and cloning algorithms in the HP-UX 10.20 compilers boost SPECint95 performance on a \nPA8000-based workstation by a factor of 1.32. 1 Introduction Procedure boundaries have traditionally \ndelimited the scope of a compiler s optimization capabilities. Indeed, it is no accident that optimizations \nwithin a procedu-ral scope are termed global optimizations. But as an optimizer s scope is limited, so \nis its power, and several techniques have been developed to extend optimizations to larger scopes. One \nsuch technique is inlining: direct incorporation of the code for a subroutine call into the calling proce-dure. \nAfter inlining, optimizations blocked or hindered Permission to make digital/hard copy of part or all \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advan-tage, the copyright notice, the title of the publication and its date \nappear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. PLDI \n97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/97/0006...$3.50 by the procedure call boundary can be \napplied straight-forwardly to the combined code of caller and cake with little or no loss of precision. \nAs a side benefit, the run time cost of a procedure call is also eliminated. Another common technique \nfor exploiting interprocedural infor-mation is cloning: the duplication of a &#38;lee so that its body \nmay be specialized for the circumstances existing at a particular call site or set of call sites. Mining \nis often considered to be a brute-force ap-proach to interprocedural optimization. Since many global \noptimizations are not linear time or linear space, and since instruction caches are of fixed capacity, \nthe code expansion caused by inlining is cause for some con-cern. Interprocedural analysis is usually \nproposed as a more tractable alternative to inlining with less drastic resource costs. However, it is \ndifficult to model many important analyses in an interprocedural setting, and many of the analyses degrade \nmarkedly in the usual case where not all program source is visible to the analyzer. Even if interprocedural \nanalysis is performed, effective use of this information will almost always require code expansion, since \nmany of the code transformations en-abled by an interprocedural analysis are impossible to safely express \nwithout some duplication of code in ei-ther the caller, the callee, or both. Our high-level intermediate-code \noptimizer, HLO, employs both inlining and cloning iu combination to achieve its optimization goals. Cloning \nis goal-directed: it is used to expose particularly important details about the calling context to the \ncallee. Inlining is used more liberally to allow traditional optimizations to affect a wider scope. HLO \ns inlining and cloning capabilities are uniquely powerful: it can inline or clone calls both within and \nacross program modules, can inline or clone independent of source language, can accommodate both user \ndirectives and profile directed feedback, and can inline or clone at almost every call site with very \nfew restrictions. The aggressive inlining and cloning done by HLO can substantially reduce the run time \nof a program. For ex-ample, on the six SPEC92 integer benchmarks, HLO s inlining and cloning boost the \noverall performance ra-tio by a factor of 1.24 on a PA8000 workstation, with a maximum speedup ratio \nof 2.02. For the eight SPEC95 integer benchmarks the results are even more dramatic, boosting overall \nperformance by a factor of 1.32 with a maximum speedup ratio of 1.80. The remainder of this paper is \norganized as follows: Section 2 describes the capabilities and structure of HLO; Section 3 presents data \nfrom a number of mea-surements; Section 4 describes related work, and Sec-tion 5 summarizes our results \nand describes opportuni-ties for future work. 2 HLO s Mining and Cloning 2.1 Compiler Infrastructure \nThe current generation of HP compilers communi-cate via a common intermediate language known as mode. \nLanguage front-ends produce ucode, and the common back end accepts ucode as input. HLO acts as a ucode-toucode \ntransformer interposed between the front and back ends of the compiler. By buffer-ing the ucode from \nthe front-end, HLO is able to per-form module-at-a-time optimizations. An alternative compile path allows \nthe ucode to be stored into special object files known as isoms. These files remain unopti-mized until \nlink time. When the linker is invoked and discovers isoms, it passes them en masse to HLO, which performs \noptimizations and then passes the files one-at- a-time to the back end, where real object files are pro \nduced. After all files have been optimized the linker is reinvoked on the real object files to build \nthe final exe-cutable. This path allows HLO to perform intermodule optimizations. The isom path is fully \nmake compatible. It also al-lows for the incorporation of profile information -for example, branch execution \ncounts -gathered by previ- ous training runs. The availability of profile information feeds the inlining \nand cloning heuristics, and enables a number of other profile-based optimizations (PBO) within the compiler \n[15, 9, 121. Figure 1 gives a picture of how all this fits together. 2.2 Structure of HLO Conceptually, \nHLO operates as something of a pipeline. The input stage translates the ucode into HLO s own internal \nrepresentation (IR), and builds up a compre hensive symbol table. A variety of classic optimizations \n(e.g. constant propagation) are performed on the IR at this time, mainly to reduce its size. After all \ncode has been input, a limited amount of inter-procedural anal-ysis is performed. HLO then inlines and \nclones in a manner we describe below. The output phase converts the HLO IR back into ucode and sends \nthe ucode on to the back end for intensive intraprocedural optimization and ultimately generation of \nobject code. HLO performs several passes of inlining and cloning. The main motivation for this multi-pass \nstructure is that it is quite difficult to anticipate the optimization impact of a particular inline \nor clone. If all inlining and cloning were done in a single pass HLO would not be able to focus in on \nparticular areas of interest revealed only after the first stage of inlining. Having multiple passes \nalso simplifies matters like cloning a recursive procedure with a pass-through parameter which might \nbe difficult to do correctly in a single pass. The overall algorithm is sketched in Figure 2. High-level \ncontrol of the inliner is done by giving the inliner a budget. This budget is an estimate of how much \ncompile time will increase because of inlin- ing. By default the inliner will try to lit compile-time \nincreases to 100% over no inlining. Note that because various optimization phases are nonlinear, a 100% \nin-crease in compile time does not imply that the inliier will double the size of the code. The HP-UX \nbackend optimizer contains several algorithms that are quadratic in the size of the routine being optimized, \nso we model this effect accordingly. For our compiler, then, code growth is typically on the order of \n20%. The budget can be adjusted in either direction by a variety of user controls. Once the overall budget \nhas been computed, the inliner computes the staging for the budget. This apportions the budget amongst \nthe various passes, ba-sically to ensure that not all of the budget is used up in the first pass. The \ncompiler then alternates cloning and inlining passes until either the budget is exhausted or a pass limit \nis reached. 2.3 Cloning Cloning begins with the selection of cloning sites. Each call site is examined \nin turn. The cloner first determines if the call site passes certain legality tests. For example, cloning \nis disallowed if there are gross type mismatches between caller and callee, or if the caller and callee \ndo not agree on the number of parameters to be passed. Next, the cloner determines if the caller supplies \nin-teresting information to the callee. For example, the caller might pass an integer 0 as the first \nactual param-eter. If the calliig context is sufficiently interesting, the callee s use of this context \nis queried next. If the callee can benefit from knowing about its formals what the caller knows about \nits actuals, the site is consid-ered to be one suitable for cloning. At this point, the We could clone \neven in such cases, but the idea is to try and preserve the behavior of even semantically incorrect programs. \n Linker  Sources Front Optimization End optimized Executable Front End Optimization Optimkation \n Figure1: (Top) A traditional path, supporting optimization, optimiza- compile intraprocedural and interprocedural \ntionwithin a source module. (Bottom) The path used in HP-UX compilers to support cross-module optimization \nand profile-based optimization. Inline and Clone(G) INPUT call graph G: (routines, edges) ALGORITHM \n// estimate current compile time cost current cost C = 0 FOREACH routine R IN G c= C + (sizeof (R)j2 \n/,I determine budget growth factor D = 1.2 budget B = C * D // determine staging of budget s co1 = C \n+ B * 0.2 . . . SClimit-11 = C + B  // inline and clone clone database D = ( } pass number P = 0 WHILE \n( C c B AND P < limit 1 Do c= Clone(G,SCPl ,C,D) c-Inline(G,SCPl,C) P =P+l Figure 2: Overall Inlining \nand Cloning algorithm cloner effectively intersects the information supplied by the caller and the information \nuseful to the cake to create a clone specification, or clone spec. In our cur-rent implementation, only \ncaller-supplied constants are considered interesting. Many other criteria are possi-ble: cloning to exploit \naliasing properties that hold at the call site, or the fact that certain arguments are ig-nored by the \ncallee, or that the caller ignores the return value, and so on. Our current implementation of the &#38;lee-side \nanal-ysis is relatively simplistic. Each parameter is consid- ered independently, only the abstract constancy \nor non- constancy of the parameter is considered, and we do not model interprocedural effects (pass-through \nconstants). Special emphasis is put on parameter values that reach the function position at an indirect \ncall site. Each inter-esting use of a parameter is weighed by an estimate of the importance of that use. \nWhen PBO data is present, the compiler computes the profile count of the block relativeto the routine \nentry; without such data it uses heuristics to guess at the relative importance. After finding an interesting \ncall site, the cloner could continueon building call clone specs for all suitable sites, but doing so \nmight lead to unnecessary prolif-eration of clones. Instead, once an interesting site has been found,the \ncloner uses the clone spec to try and greedily create a clone group: a set of call sites which can safely \ncall the clone described by the clone spec. This is done by examining each of the calls made to the callee \nto see if the calling context at the call site is com- patible with the clone spec created for the clone \ngroup. If so, the call site is included into the clone group. Once the clone group is completely formed, \nthe cloner then assesses the run-time benefit of making the clone. This calculation takes into account \nfactors like the estimated total number of calls that will call the clone instead of the original routine, \nand the value to the callee of the caller-specific context information. After all call sites have been \nexamined, the cloner has a collection of clone groups describing the particu-lar clones that could be \ncreated, and an estimate of the benefit of creating each clone. The cloner then ranks all clone groups \nby benefit and greedily creates clones and modifies call sites until the current allotment of the compile-time \ngrowth budget has been used up. Any clone groups that were not handled in this pass are dis-carded; they \nmay be recreated and cloned in a later pass. Creation of a clone is fairly straightforward. The IR for \nthe clonee is duplicated, and any formal parameters that are known from the calling context are turned \ninto routine-scope variables and initialized with appropriate constants in the clone s entry block. If \nthere are slight discrepancies in type, a type cast is inserted. The clone is always placed into the \nsame module as the clonee. If the caller isin another module and is passing symbolic information which \nis only visible in the caller s module (e.g. the address of a file-static procedure), this infor-mation \nmust be promoted to global scope and given a unique name that will not collide with any user-supplied \nname. As clones are created, the clone and associated clone spec are also recorded in a special database. \nThis database comes into play in later cloning passes, when it is possible that the cloner will reproduce \nthe same clone spec used to clone in an earlier pass, because in-tervening optimizations have sharpened \nthe information available at call sites which were previously not worth consideration. If a given clone \nexists in the database then it is simply reused; otherwise the clone must be created as described above. \nModification of the call sites in a clone group to in-voke the clone is also fairly straightforward. \nThe clone spec describes the signature of the new routine, so any parameters incorporated into the clone \nare edited from the actuals list. The call site is then modified to refer to the clone instead of the \noriginal routine. This modifica-tion in turn inspires changes in the call graph to reflect the new relationships \nbetween caller, clonee, and clone. In particular, if all calls to a clonee are replaced by calls to a \nclone, the clonee may become unreachable in the call graph and will be deleted. The cloner attempts to \nanticipate subsequent clonee deletion when estimating Clone(G,B,C,D) : returns C INPUT call graph G: \n(routines,edges) budget B current cost C clone database D ALGORITHM N setup FOREACH routine R IN G create \nparameter-usage descriptor P(R) FOREACH edge E IN G create calling-context descriptor S(E) // build clone \ngroups FOREACH edge E in G callee R = E.target IF ( clonable(R) end clonable(E) 1 THEN clone spec CS \n= intersect( S(E), P(R) > IF ( CS is nonempty > THEN clone group CC = (R,CS,E) FOREACH edge EJ incident \non R IF ( clonable(E') AND matches( S(E), CS ) > THEN add E to CG estimate benefit of CC  // select \nclones sort CGs by benefit; C' = C FOREACH clone group CG IN CGs cost x = (sizeof(R)j2 IF ( C' + X < \nB ) THEN accept CG; C' = C' + X // create clones and jix call sites FOREACH clone group CG IN accepted \nCGe IF ( ! lookup(D, R, CS> > THEN R' = make clone (R, CS) add database entry ( R, CS, R ) FOREACH \nedge E' IN CC change target of E' from R to R' /,/ optimize clones and recalibrate FOREACH newly created \nclone R' optimize(R)) C = C + (sizeof(R')j2 Figure 3: Cloning Pass the budget impact of a particular \nclone group or groups; in effect, a clone group that ensures that the clonee will be deleted is considered \nto have no compile time impact. 2.4 Mining The overall structure of an inlining pass is similar to cloning. \nThe inliner first considers all call sites for any legal, technical, pragmatic, or user-imposed restric-tions \non inlining. Illegal sites include those with gross type mismatches, varargs, or argument arity differences. \nTechnically restricted sites include those where infor-mation specific to the callee disagrees with information \nspecific to the caller. For example, the caller s IR may specify that reassociation of floating point \noperations is allowed, while the callee s IR may indicate that such re-associations are unacceptable. \nBy and large these kinds of restrictions are imposed to simplify the task of representation of this information. \nPragmatic concerns include issues like handling callees that use alloca to dynamically allocate space \non the stack, or inlining at a site where actual parameters describe overlapping re- gions of memory \nand the callee is allowed to assume that its formal parameters do not alias. User imposed restrictions \ncome from various command line options and pragmas. Once the set of viable inlining sites has been identi-fied, \nthey are assigned a runtime figure of merit. High-frequency call sites are given highest priority. Sites \nthat occur in blocks executed less frequently than the rou-tine entry block are assigned a penalty. This \nhelps to avoid inlining into a non-critical path; doing so might cause increases in register pressure \nwhich push spills into critical code paths and hurt performance. The inliner then walks over the inline \nsite list in pri- ority order. The compiktime impact of each site is considered, and if within the current \nbudget, theinline is accepted. Computation of the compile time effect is complicated by interactions \namong inlines. For exam-ple, if A calls B and B calls C, the cost of inlining B into A depends on whether \nor not C has been already been inlined into B. To model this dependence, the inliner keeps a schedule \nof the order in which it will perform all accepted inlines. By and large, the inliner attempts to work \nbottom-up over the call graph. To compute the cost of inlining B into A, a description of the in-line \nis first inserted into the schedule in the appropriate spot. If B is then determined to be the target \nof an ear-lier inline or inlines, the estimated size of B after those inlines have been performed is \nused to compute the cost of optimizing A. The inliner processes and accepts call sites greed-ily until \nits allotment of the budget is exhausted. At this point the remaining viable inline sites are discarded \nInline(G,B,C) : returns C INPUT call graph G: (routines,edges) budget B current cost C ALGORITHM // \nscreen inline candidates FOREACH edge E IN G IF ( inlineable(E) > THEN accept E; compute benefit(E) \n // select inline sites sort accepted E's by benefit C' = c FOREACH accepted edge E insert E into schedule \ncost x = (sizeof( E.target + E.source >I2 -(sizeof( E.target )I2 C' ' = C' C' = C' + x IF ( E.target \nis source in later inline ) THEN adjust C' for cascaded cost IF ( C' > B ) THEN remove E from schedule \nC = C  // perform inlines FOREACH scheduled edge E inline E.target into E.source  // optimize inlines \nand recalibmte FOREACH routine inlined into R' optimize@') c= C + (sizeof( Figure 4: Mining Pass GO8.e.SprfSso \n3166 022.li 1638 023.cqntoa 472 026.comprcss 200 072x 1373 085gcc 9942 099.go 2565 124.m88ksim 1876 126.gcc \n21241 129.compress 116 13OSi 1527 132.ijpeg 1644 134.ped 4501 147.voltcx 9478   = ZEi m cross module \nm within module recursive Figure 5: Static characteristics of call sites in the SPEC integer benchmarks. \nThe number at right is the total number of call sites in the code. (they may be reconsidered in a subsequent \npass of inlin- ing). The inliner then uses its schedule to carry out each inline in the list of accepted \ninlines. As with cloning, movement of code between modules may result in pro-gram entities being promoted \nto wider scopes. 3 Measurements 3.1 Characteristics of Call Sites Figure 5 illustrates some static information \nabout the 14 programs in the SPEC92 and SPEC95 suites. Each call site in these programs can be classified \ninto one of five categories: external, indirect, cross-module, within-module cross-routine, and recursive. \nExternal sites represent calls to library routines or to program modules not visible to the compiler. \nIn princi- ple it is possible to provide intermediate code versions of such libraries and modules to \nbroaden the scope of inlining or cloning even further, but the results reported in this paper are with \nstandard precompiled libraries, with one notable exception. The 072.SC benchmark in-cludes a special \ncurses library in which all curses calls do nothing. These calls (reported in our figure as cross-module \ncalls) would be ideal candidates for inlining, but they are eliminated before inlining because HLO s \ninter-procedural analysis determines that they have no side effect. At indirect sites the callee is computed \nat run time, so these sites are not directly amenable to inlining or cloning. It is possible to employ \nvarious techniques to try and resolve the target of indirect calls at compile time. For example, HLO \nwill aggressively clone at sites where the caller passes a pointer to a procedure and the callee uses \nthe value of a formal variable in an indi-rect call. Subsequent constant propagation of this code pointer \nto the call site will then provide the information needed to turn the indirect call into a direct call, \nwhich can then be inlined or cloned in a later pass. This sort of staged optimization would be much more \ndifficult to accomplish in a single inlining pass. The remaining are amenable to inlining and cloning. \nAs the figure shows, there are significant numbers of cross-module calls. The ability to inline these \ncross-module calls is crucial for good performance. 3.2 Transformations to SPEC Integer Programs Table \n1 shows more detail on the transformations done to a subset of the SPECint programs by HLO. There are \nseveral points worthy of further discussion. First, as more information is made available to the compiler, \nthe quality of the code improves. For instance, in 072.sc, the base performance level with iulining and \ncloning done per-module is 7.1 seconds. If the compiler is al-lowed to inline and clone cross-module, \nthe runtime drops to 6.3 seconds. If the compiler is allowed to make use of profile information, the \nruntime becomes 5.3 sec-onds. Finally, the combination of both cross-module iulining and cloning with \nprofile feedback gives a run time of 4.5 seconds. By and large, this monotonic im-provement property \nholds for almost all programs that we have examined. Another consequence of the increase in scope is \nthat compile time2 increases. Again looking at 072.sc, the base compile time is 862 seconds, while the \ncompile time with cross module inlining and profile feedback is 1786 seconds, approximately 100% larger \n(this time includes the time required for the instrumenting compile, train-ing run, and final compile). \nIn some cases, the compile time increases are a good deal larger; in others, smaller. The precise impact \nis often difficult to estimate because the analyses performed downstream are often quite sen-sitive to \nparticular sorts of code structures. 2All programs were compiled on an HP K400 workstation us-ing special \ndevelopmental versions of the HP-UX 10.20 compilers. The times shown are therefore 30-40% slower than \nwould be ob- tained by production compilers on the same hardware. Clone Compile 1 Run Benchmark Scope \nInlines Clones Repls Deletions Tim;26 1 Ti; 008,espresso 281 28 47 28 C 188 18 32 23 983 8.2 P 815 45 \n106 9 976 8.2 cP 297 17 47 7 1047 7.6 022.li 256 42 52 63 348 25.6 C 76 13 18 15 466 20.0 P 620 93 495 \n35 cP 90 23 256 10 072. SC 127 26 30 18 C 39 6 8 4 981 1 6.3 P 244 42 50 21 cP 106 12 17 6 085 .gcc 732 \n87 247 70 C 1008 230 760 193 13544 1 22.5 P 309 47 110 25 cP 641 349 2484 80 099. go 400 23 219 6 C 545 \n30 371 2 877 453.4 P 154 14 177 0 1013 436.3 cP 121 22 327 0 996 386.0 124.m88ksim 140 33 64 18 491 298.0 \nC 339 121 431 19 702 284.8 P 97 21 27 17 783 228.7 cp 80 49 132 7 147. vortex 253 17 67 9 C 841 121 2211 \n5 P 140 9 12 5 2028 1 373.7 cp 175 83 2142 2522 1 270.1 1 IL Table 1: Mine and clone information for \nselected benchmarks. Here c indicates cross-module compilation, p profile- based compilation. Baseline \nis a compiler with full inlining and cloning capabilities. 008.espresso m , , , , , 022.li - 023.eqntott \n026.compress I I I 072.sc I 085gcc SPECint!JZ I I I 099.go I 124.m88ksim 126.gcc I I I 129.compfcss I \n130X 132.ijpeg 134.pecl 3 I I I I I I I I I I I I I I I 147.vortex SPECint95 I I I I I I 116 I!8 2!0 \nm inline and clone V inline m clone Figure 6: Relative speedup of SPEC integer programs with inlining, \ncloning, or both, for the PA8000 worksta-tion. Baseline compile uses cross-module and profile-based optimization, \nplus peak options not affecting in-lining or cloning. Overall figures present the geometric mean speedup \nfor each benchmark suite. Data in this table also underscores the role of clone groups. Most clones are \nusable at more than one call site, and as the optimization scope widens, the ratio of call sites modified \nto clonescreated increases, indicat-ing that a given clone is being used at a larger number of sites \non average. The distribution tends to be quite skewed. A sizeable number of routines are also deleted \nduring compilation. These include both file-scope user routines and clones which are provably not callable \nbe-cause all calls have either been cloned or inlined. The data in table also indicates the synergistic \nben-efits of profile-based and cross-module optimizations. For instance, in 099. go, the cross-module \nprofile-based compilation actually does fewer inlines than the other compilations, yet produces a faster \nbinary. Compile times are shorter than the cross-module alone case, de-spite the need for a preliminary \ncompile with instru-mentation and a training run to produce the profile database. The data also shows \nthat for our compiler, profile-based optimization is usually more valuable than cross-module optimization. \nWe cannot yet say if this represents anything fundamental; it may simply be an indication of the relative \nmaturity of the profile-based optimization components. 3.3 Overall Performance Figure 6 shows the relative \nperformance of the 14 SPECint programs as measured on a PA8000-based [lo] K460 workstation running HP-UX \n10.20. The worksta-tion had two 180 MHz cpus and 256 MB of memory, 16way interleaved. Programs were compiled \nwith the HP-UX 10.20 C compiler. All compilations used inter-procedural optimization (t04 +Onolimit) \nand all the compiles incorporated profile information (tP) gathered from an instrumentation run done \non the specified train-ing data set. Each benchmark was compiled four sepa-rate times: with no inlining \nor cloning, with only inlin-ing or only cloning, and with both inlining and cloning. Each executable \nwas run three times on an unloaded workstation, and the best time reported was used. The data shows that \ninlining alone has the biggest impact on performance, though cloning is a vital con-tributor to both \n022. li and 13O.li (which are quite similar) and to 124.m88ksim. Cloning by itself does not yield significant \nperformance improvements, and on some benchmarks actually reduces performance slightly over what can \nbe obtained by inlining alone. Though we have several theories, we have not as yet been able to determine \nthe precise reason or reasons for the per-formance losses seen in some benchmarks when just cloning is \nused. What is it that happens in inlining that leads to these speedups? To try and answer this question \nwe ran several of the benchmarks through a PA8000 sim-ulator. Data for several of these sets of simulations \nare presented in Figure 7. In gathering this data, the simulator ran modified versions of the SPEC95 \ninte-ger benchmarks, with simplified input sets designed to closely mimic the behavior of the benchmark. \nThe simulation data shows that in several bench-marks inlining has resulted in dramatic drops in overall \nexecution time (as measured by cycles) and the num-ber of instructions retired by the processor. The \neffect on the CPI varies; in 130. li it falls dramatically, but in 147.vortex it rises; yet both benchmarks \nspeed up substantially. Not surprisingly, inlining and cloning both tend to increase the I cache miss \nrate and the total number of I cache misses. For the most part, however, inlining reduces the total number \nof I cache accesses, meaning Relative Cycles Cycles Per Instruction Relative I Cache Act. I Cache Miss \nRate xl000 Relative D Cache Act. 1.0 0.8 0.6 0.4 0.6 D Cache Miss Rate xl00 0.4 0.2 0.0 1.0 Relative \nBranches 0.8 0.6 0.4 Branch Miss Rate Key m m inline and 4one clone m m inline neither Figure 7: Simulation \nresults for the PA8000 running a modified versions of the SPEC integer benchmarks. Relative indicates \nthat the data is scaled relative to the run with neither inlining or cloning. that some of the increase \nin miss rate is due to the same number of I cache misses being amortized over fewer ac-cesses. In the \nlarger benchmarks, especially 126.gcc, the I cache miss rate more than doubles. The overall im-pact of \nthe inlining and cloning on I cache performance is unclear, and seems to depend on the particular dy-namic \nof the program in question. The number of D cache accesses is also dramatically decreased. This causes \nan increase in the data cache miss rate, again because a similar number of misses is spread over fewer \ntotal accesses. A big part of this dra-matic drop is the elimination of caller and callee register save \noperations at call sites that have been inlined. We believe that this indicates that the register allocation \nphase of the HP-UX compiler has little difficulty with the larger routines created by inlining and cloning, \nand that for the most part register pressure is not an issue. The number of branches overall is reduced. \nSince this number includes procedure calls this is also not surprising. The branches that remain appear \nto be-come more predictable. Since the PA8000 always mis-predicts procedure return branches, this may \nalso be a misleading statistic. However, we suspect that the pre-dictability of the remaining branches \nis also enhanced. Any possible improvement in branch behavior must be weighed off against an increase \nin the total number of branches, which may increase the rate of branch colli-sion in a branch prediction \ncache. 3.4 Validation of Heuristics There is no practical technique at compile time for de- termining \nthe optimum set of inlines or clones. Both the run time benefit and compile time costs must be estimated \n(see Ball [3] for an example). Furthermore, in any reasonably sized benchmark, there are a stag-gering \nnumber of ways to perform inlining and cloning. Chang, Mahlke, Chen, and Hwu [5] point out that a simplified \nversion of the problem is equivalent to the knapsack problem, which is known to be NP complete. Each \nsite can be either inlined or cloned, and the or-der of inlining and cloning may make a difference in \nthe final code. Inlining and cloning must thus be guided by heuris- tics. As with all heuristics it is \nimportant to verify that the decisions they make are reasonably sound ones. To this end we present the \ndata shown in Figure 8, which illustrates one technique for assessing the quality of heuristics used \nin HLO. As an experiment, we var-ied the budget provided to the inliner from a relatively small budget \nof 25 to a large budget of 1000. For each budget level we compiled the benchmark 022. li a num- ber of \ntimes. In each compile we artificially stopped the inliner after a certain number of inlines and/or clone \nre-placements. The resulting curves depict the incremen-tal benefit of each successive inline or clone \nreplace-ment. As can be seen, very few inlines or clones have an adverse impact on performance. Also, \nonce the bud-get has reached a sufficiently large value (100 in this case), there is no additional performance \nincrease with extra inlining. This property (that performance reaches an asymptote with increasing budget) \nis true of many of the programs we have studied. Our default bud-get of 100 was chosen to maximize the \nperformance of the benchmarks we studied without performing unnec-essary inlines. 3.5 Mining in Other \nCodes Inlining is also of potential benefit in any language that supports the notion of a procedure or \nsubroutine. At present, HLO is only capable of optimizing C, FOR-TRAN, and C++ programs. In this paper, \nwe do not report any data for the floating-point benchmarks in the SPEC suite. One rea-son for this is \nthat there is a significant barrier to in-lining in FORTRAN: it is difficult to aggressively rep-resent \nthe aliasing semantics of an inlined FORTRAN subroutine. By default, the compiler is free to assume that \nformal parameters do not alias each other nor any global variables. Representing this information in \nthe post-inlined code is tricky, and compilers (like ours) that cannot properly represent it are usually \nbetter off not inlining. The SPEC benchmarks tend to be small programs; 126. gee is the largest at around \n120,000 lines of code. A major challenge to effectively deploying aggressive inlining is the sheer size \nof production codes. We have recently been experimenting with compiling the 500,000 line performance \nkernel of an important application pro-gram, and have been amazed to find that significant speedups like \nwe see in some of the SPEC benchmarks can also be obtained in large production codes. 4 Related Work \nMany research and production systems have been capa-ble of inliing and cloning [l, 2, 14, 7, 5, 4, 11,6, \n8, 131. However, very few have reported consistently good re-sults in a mature compilation system, reported \nresults on moderately large well-known programs, reported the effects of aggressive inlining and cloning, \nor offered a de tailed analyses of the costs and benefits of inlining. Chang, Mahlke, Chen, and Hwu [5, \n111 describe a profile-based inliner for the IMPACT compiler. Their work is perhaps closest in spirit \nto ours, inlining across modules, and making use of profile information to select the best inlining sites. \nThe control algorithm makes a . . . . . . . 25 - loo --- 200 -. - 1000 19 - 17 - I5 -14 - I2 0 I 50 \nI loo I 150 .4 .tied \\ \\ \\ .A \\ \\ .- I I I I 300 350 4clo 450 Number of inlining and cloning operations \n Figure 8: Incremental benefit of inlines and clone replacements in 022.li, at various budget levels. \nsingle pass over the inlineable sites, ranking them by profile weight. As in our implementation, selected \nin-lines are then scheduled to be performed in roughly bottom-up call graph order. Overall control is \ngov-erned by a code growth budget. They report a mean speedup of 11% with a maximum speedup of 46%. Our \nwork differs in a few important aspects: we make use of cloning, perform multiple passes, rank inline \nsites both in terms of profile weight and relative execution frequency, and have profile information \nnot only on in-terprocedural arcs but also intraprocedural ones. Davidson and Holler [7] developed an \ninliner for C programs that operated at source level. They reported a mean speedup of about 12% and a \nmaximum speedup of about 35% on a variety of programs. They noticed a number of cases where inlining \ninduced register pressure limited performance. Their study was done without the benefit of profile information, \nwhich we believe to be crucial to getting good performance. Allen and Johnson [2] describe a C language \ninliner and give a good discussion of some of the motivations for inlining. However, we feel that the \ncommonly held no-tion (found in [2] and elsewhere) that an inliner should aim to inline only small functions \nto be untrue. Medium and large functions should be inlined if the remainder of the compile path is capable \nof aggressively handling large functions. Cooper, Hall, and Kennedy [S] describe a cloning algorithm \nwhich is a good deal more sophisticated than ours. Their analysis is interprocedural and relies on the \nactual values of constants passed to callees. Our use of clone specifications and clone groups mimics \nsome of the clone vector merging possible in their work. For reasons we do not yet completely understand, \nwe have found our implementation of cloning to be relatively ineffective in boosting performance. Dean \nand Chambers [S] describe an interesting sys-tem that is able to perform experiments to determine the \nactual benefits of an inline. Our system is handi-capped by having to rely on static estimates of the \nben-efit of an inline, and assessing an independent benefit of any particular inline is not easy, since \nsuch benefits depend upon all the other inlining decisions that have been made so far. For code under \ndevelopment in sit-uations where development builds make use of inlining, however, the idea of a database \nto record information about past inlining decisions is appealing. 5 Summary and Future Work Our experiences \nwith inlining and cloning in HLO demonstrate that aggressive inlining and cloning can give substantial \nand widespread improvements in the performance of programs, with some well-studied pro-grams like 022.li \nspeeding up by a factor of two. Our inliner differs from previous inliners described in the literature \nin that it is able to inline at almost any call site without restriction; it can inline cross-module \nand cross-language calls; it is uses profile feedback in conjunction with multiple passes to aggressively \ninline in the important parts of the program and adapt to the consequences of previous inlines and clones; \nand it keeps a budget that allows for a global assessment of compile time impact without artificially \nrestricting the amount of inlining in any one portion of the code. Our inliner was added to a mature \ncompiler that already contained an aggressive, state-of-the-art global optimizer. The fact that inlining \nproduces such im-pressive additional speedups is an indication that the gains we are seeing here are \nnot simply straw-man arti-facts where high-level optimizations eliminate problem- atic code from an the \ninliner s actions regions of code to able the full power performance-critical Though we are we have obtained \nprojects in mind. immature global optimizer. Instead, expose more significant and weighty the global \noptimizer, and thereby en-of the compiler to be trained on the portions of the application. pleasantly \nsurprised with the results so far, we have a number of future We want to apply aggressive inlining to \nlarge, production programs like the HP-UX kernel, database applications, and CAD tools. We also plan \nto improve the impact of cloning and remove the in-lining restrictions for FORTRAN codes. We are look-ing \nat techniques to make profiling less onerous, per-haps incorporating profile information from a variety \nof sources. We are also contemplating using aggressive outlining as a complement to aggressive inlining, \nto help further focus the global optimizer on the truly impor-tant stretches of code. Acknowledgments \nWe gratefully made by our HP language Benitez, Wei mar, Adam acknowledge the inspired contributions colleagues \non the HLO project and in other projects, especially Anne Holler, Manuel Hsu, La&#38;y Shah, Carl Burch, \nRajiv Ku-Matusiak, John Liu, Jonathan Springer, and Steve Rehrauer. We also appreciate the thoughtful \ncomments of the anonymous referees which helped to substantially improve this paper. References PI F. \nC. Allen and J. Cocke. A catalogue of optimiz- ing transformations. In Design and Optimization of Compilers, \nR. Ruskin, Ed., Prentice-Hall, En-glewood Cliffs, NJ, 1971, l-30. PI R. Allen and S. Johnson. Compiling \nC for vector-ization, parallelization, and inline expansion. Pro-ceedings of the ACM SIGPLAN 88 Conference \non Programming Language tion, 241-249. [31 J. Ball. Predicting the a procedure body. ACM 214-220, 1979. \n Design and Implementa- effects of optimization on  SIGPLAN Notices 14(8), PI P. Chini. Automatic Inlining. \nIBM Research Re-port RC 20286, November 1995. P. P. Chang, S. A. Mahlke, W. Y. Chen, and W. PI W. Hwu. \nProfile-guided automatic inline expansion for C programs. Software Practice and Experience 22(S), 349-369, \nMay 1992. PI K. D. Cooper, M. W. Hall, and K. Kennedy. A methodology for procedure cloning. Computer \nLanguages, 19(2), 105-117, February J. W. Davidson and A. M. Holler. 171 function inliner. Software \nPmctice 18(8), 775790, August 1988. 1993. A study of a C  and Experience PI J. Dean and C. Chambers. \nIraining compilers to make better inlining decisions. Technical Report 93-05-05, Department of Computer \nScience and Engineering, University of Washington, 1993. PI A. M. Holler. Compiler optimizations for \nthe PA-8000. COMPCON 1997 Digest of Papers, February 1997. [lOI D. Hunt. Advanced performance features \nof the 64-bit PA8000. COMPCON 1995 Digest of Papers, 123-128, March 1995. WI W. W. Hwu and P. P. Chang. \nInline function ex-pansion for compiling C programs. Proceedings of the ACM SIGPLAN 89 Conference on \nProgram-ming Language Design and Implementation, 246- 257. K. Pettis and R. C. Hansen. Profile guided \ncode WI positioning. In Proceedings of the ACM SIGPLAN 90 Conference on Programming Language Design \nand Implementation, 16-27. S. Richardson and M. Ganapathi. Interprocedural analysis versus procedure \nintegration. Information Processing Letters, 32(3), 137-142, August 1989. 1131 [I41 R. W. Schiefler. \nAn analysis of inline substitution for a structured programming language. Commu-nications of the ACM \n20(g), 647-654, September 1977. S. E. Speer, R. Kumar, and C. Partridge. Improv-ing UNIX Kernel Performance \nusing Profile Based Optimization. In USENIX 1994 Proceedings. P51  \n\t\t\t", "proc_id": "258915", "abstract": "Existing research understates the benefits that can be obtained from inlining and cloning, especially when guided by profile information. Our implementation of inlining and cloning yields excellent results on average and very rarely lowers performance. We believe our good results can be explained by a number of factors: inlining at the intermediate-code level removes most technical restrictions on what can be inlined; the ability to inline across files and incorporate profile information enables us to choose better inline candidates; a high-quality back end can exploit the scheduling and register allocation opportunities presented by larger subroutines; an aggressive processor architecture benefits from more predictable branch behavior; and a large instruction cache mitigates the impact of code expansion. We describe the often dramatic impact of our inlining and cloning on performance: for example, the implementations of our inlining and cloning algorithms in the HP-UX 10.20 compilers boost SPECint95 performance on a PA8000-based workstation by a factor of 1.32.", "authors": [{"name": "Andrew Ayers", "author_profile_id": "81100199268", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "P17687", "email_address": "", "orcid_id": ""}, {"name": "Richard Schooler", "author_profile_id": "81100550569", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "P242939", "email_address": "", "orcid_id": ""}, {"name": "Robert Gottlieb", "author_profile_id": "81100605564", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "P245013", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258928", "year": "1997", "article_id": "258928", "conference": "PLDI", "title": "Aggressive inlining", "url": "http://dl.acm.org/citation.cfm?id=258928"}