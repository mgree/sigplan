{"article_publication_date": "05-01-1997", "fulltext": "\n Code Compression Jens Ernst University of Arizona William Evans University of Arizona Christopher W. \nFraser Microsoft Research Steven Lucco Microsoft Todd A. Proebsting University of Arizona Abstract Current \nresearch in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This \nview has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of \nnetwork and disk speeds to CPU speeds grows exponentially. For example, we have seen the CPU idle for \nmost of the time during paging, so compressing pages can increase total performance even though the CPU \nmust decompress or interpret the page contents. Another profile shows that many functions are called \njust once, so reduced paging could pay for their interpretation overhead. This paper describes: . Measurements \nthat show how code compression can save space and total time in some important real-world scenarios. \n. A compressed executable representation that is roughly the same size as gzipped x86 programs and can \nbe in- terpreted without decompression. It can also be com- piled to high-quality machine code at 2.5 \nmegabytes per second on a 120MHz Pentium processor . A compressed wire representation that must be de- \ncompressed before execution but is, for example, roughly 21% the size of SPARC code when com-pressing \ngee. Forcon~spondence: { jens, todd,willl@cs.ariz.ona.edu, Dept of ComputerScience,Gould SimpsonBuilding, \nTucson, AZ 85721. (cwfraser, steveluc)@microsoft.com,OneMicnxoftWay, Redmond, WA 98052. Permission to \nmake digital/hard copy of part or all this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan-tage, the copyright notice, \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nACM, Inc. To copy otherwise, to republish, to post on servers. or to redistribute to lists, requires \nprior specific permission and/or a fee PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-89791s907-6/9710006...$3.50 \n Introduction Computer programs are delivered to the CPU via networks, disks, and caches, all of which \ncan be bottlenecks. In some important scenarios, it can be significantly faster to send compressed code \nthat is then interpreted or decompressed and executed. This fact is self-evident when delivering code \nover 28.8kbaud modems, but it can be true for faster networks, for paging from disk, and even for cache \nmisses if the decompressor is fast enough. We consider two im- portant bottlenecks: transmission and \nmemory. When transmission is a bottleneck, we want the best possi- ble compression, and we can afford \nto expand the com-pressed program before executing. We call such codes wire codes because a wire is the \nbottleneck. When memory is a bottleneck, the code -at least seldom used code -must be stored and interpreted \nin compressed form. Code includes jumps and calls, so we need random access to at least the basic blocks. \nIf some code must be compiled to run fast enough, the JIT (just in time) compila- tion rate must be very \nhigh. When both transmission and memory are bottlenecks, it may make sense to decompress a wire code \ninto a com- pressed interpretable form. The literature on general-purpose data compression [Bell et al] \noffers many techniques. Our tasks have been mainly to find combinations of techniques that suit the specialized \nproblem of compressing virtual machine (VM) code, and to determine how to generate compact automata that \naccu-rately predict the next VM operator or operand based on the current context, so that tokens common \nin the current con- text can be given the shortest encodings. This paper con-cerns only VM code, though \nsome of the techniques clearly apply to machine-specific code as well. This paper describes two code \ncompressors -the best that we ve found for each of our two scenarios. The compres- sors are quite different, \nbut both gather information about the common patterns that appear in the code, and both di- vide the \nstream of code into several smaller streams, one holding the operators and one holding the literal operands \nfor each operator (or class of related operators) that needs a literal operand. The compressors are: \n. A wire VM code that yields programs almost one-fifth the size of SPARC code. . An interpretable VM \ncode called Byte-coded RISC or BRISC, which is roughly 30% larger than the wire format but still about \nthe same size as non-interpretable gzipped x86 programs. We can interpret BRISC code with a typical 12x \ntime pen- alty while cutting working set size by over 40%. Alter- nately, we can compile BRISC at over \n2.5 megabytes per second, producing x86 machine code over 100 times faster than, for example, all commercial \nJIT compilers known to us. This high compilation rate permits us to recompile the program at each execution \nfor clients with no local disk cache. The delivery time from the network or disk can mask some or even \nall of the recompilation time, and the code runs within 1.08x of the speed of fully optimized ma-chine \ncode generated by Microsoft Visual C++ 5.0. BRISC can also trim memory requirements for large desktop \nappli- cations and compress programs to fit within the memory requirements of embedded systems. Both \ncodes support client-side and server-side compilation. Server-side compilation is necessary to efficiently \ndeliver large application programs. For example, existing JIT com- pilers must allocate registers on \nthe client, which is expen- sive and, for the best results, super-linear in the length of the input program. \nBy performing code optimization be- fore a program is downloaded, a mobile code system can dramatically \nreduce the time necessary to generate machine code on the client.  Design space No single code compressor \nsuits all applications. Rather, there is a design space or solution space of related methods. The trade-offs \ninvolve addressing the issues listed below. . Should one compress using byte-codes, arithmetic coding \n[Witten et al], or something between? At one extreme, byte-codes are the easiest to interpret directly, \nand branches naturally target byte boundaries. Nibble and Huffman codes can be decoded and addressed \nanalogously, but their units are 4-bit and l-bit fields, so the decoding overhead is higher. At the other \nextreme are arithmetic codes, which can compress better by coding for sequences longer than individual \nsymbols, but complicate direct interpretation. Arithmetic codes must be expanded before interpretation, \nthough we have used them successfully by decompressing a func- tion at a time. . Should the compressed \nrepresentation include a dic- tionary? Dictionaries allow the compressor to emit a series of dictionary \nindices, but the dictionary itself must be transmitted. Dictionaries can be: . static, that is, computed \nexactly once and reused for all subject programs. . semi-static, that is, computed once for each subject \nprogram but then used throughout that program. dynamic, that is, updated as the compressor and decompressor \nadvance through the subject pro-gram. . Should the dictionary coder (if any) use move-to-front (MTF) \nindexing [Bentley et al; Elias]? This technique starts by replacing sequence elements with their indi- \nces in a table that changes dynamically. The table s elements are ordered such that the first element \nwas the most recently accessed element; after each new access, the accessed element is moved to the front \nand all in- termediate elements are shifted down one place. A se- quence with high spatial locality tends \nto yield a se- quence of small indices, which should compress well. MTF coders act a bit like caching \nhardware, so there s probably some interaction with register-based interme- diate codes, since registers \ncan be regarded as a kind of cache. Should the compressor partition its input into separate streams? \nOperators and operands can benefit from dif- ferent compression schemes; finer partitionings are possible. \n. In programs, one important class of streams can be separated by parremizing the input [Proebsting; \nFraser and Proebsting]. Pattemization accepts an actual pro- gram and proposes specialized instructions \nthat might help compress that program. The patterns replace each combination of operands with wildcards. \nFor example, the code tree FetchInt(AddrLocal[4]) generates the patterns FetchInt(*) FetchInt(AddrLocal[*]) \nFetchInt(AddrLocal[4]) Regard patterns as specialized instructions. For exam- ple, the last (degenerate) \npattern above is specialized to fetch the value of the local at frame offset 4. That is, 4 is burned \ninto the specialized pattern. The middle pattern above takes an arbitrary offset, and the first pattern \nabove gets the address of the cell to fetch by popping the stack. All three push their result onto the \nstack. . Should the coder use finite-context or Markov model- ing, which uses the last few symbols to \npredict the next symbol more precisely? An order-N Markov model uses the last N symbols to predict the \nnext. The degen- erate order-0 model may use frequencies but no con- text. Also, the original representation \ncan influence the effec- tiveness of the compression techniques. In particular, should the input VM use \nregisters or a stack? That is, should the VM resemble a conventional target machine or a stack machine? \nStack machines have no register numbers to compress, but register machines permit the compiler s front \nend to invest more in, say, global register allocation and thus produce code that is typically faster \nand smaller. Some applications can accept sub-optimal performance, but there will always exist applications \nthat demand ambitious opti- mization.  A wire code We use the term wire-futmutfor codes that need not \nbe interpreted directly but can be at least partly decompressed into an interpretable form or even compiled \nbefore they are used. Thus, for example, one can simply gzip a file of in- termediate or object code, \nand the result is a wire-format code. gzip typically compresses code by a factor between two and three. \nYu [Yu] has recently described ways to tune general-purpose data compressors for use in software dis- \ntribution. His compressor outputs an average of 2.61 bits per input character, a factor of 3.07. Franz \nreports similar compressions using his slim binaries for load-time code generation [Franz and Kistler; \nFranz], though our numbers are not easily compared because he compresses full execu- tables, and we compress \nonly code segments. Our wire-format code achieves a factor of 4.9. We tried a lot of techniques, but \nthe best to date happens to be very simple: compile trees of VM code, pattemize out all liter- als, form \none stream for all patterns and one for containing the literal operands associated with each opcode or \nclass of related opcodes, IKE-code each stream, and gzip the re- sulting streams in isolation. To demonstrate: \n1. Compile the input program into trees. For example, we compile the C code int salttint j, int i) { \nif Cj > 0) { pepperti, j); j--; return j; into the ICC trees: ASGNI(ADDRLP8[72], SUBI(INDIRI(ADDRLP8[72l),CNSTC[ll)) \nLEI[1](INDIRI(ADDRLP8~68l),CNSTC[Ol) ARGI(INDIRI(ADDRLP8[721)) ARGI(INDIRI(ADDRLP8[681)) CALLI(ADDRGP[pepperl) \nASGNI(ADDRLP8[68], SUBI(INDIRI(ADDRLP8[68l),CNSTC[ll)) LABELV RETI(INDIRI(ADDRLP8[68])) A full description \nof the ICC IR appears elsewhere [Fraser and Hanson] but is not important to the discussion here. It suffices \nto note that the code is stack-based, that square brackets enclose literal operands, and that the base \ninter- mediate code has been augmented with a few operators with the suffixes 8 and 16 to flag literals \nthat fit in eight or sixteen bits. 2. Pattemize and form one stream holding the nested op- erator patterns \nand one for each type of operator that takes a literal operand. For example, the patternized operator \nstream for the sample above is: ASGNI(ADDRLP8[*], SUBI(INDIRI(ADDRLP8[*l),CNSTC[*l)) LEI[*](INDIRI(ADDRLP8[*l),CNSTC[*l) \nARGI(INDIRI(ADDRLP8[*1)) ARGI(INDIRI(ADDRLP8[*])) CALLI(ADDRGP[*]) ASGNI(ADDRLP8[*], SUBI(INDIRI(ADDRLP8[*l),CNSTC[*])) \nLABELV RETI(INDIRI(ADDRLP8[*])) The ADDRL.P8 stream is /72 72 68 72 68 68 68 6$ 3. Apply move-to-front \ncoding to each stream in isolation. For example, MTF coding transforms the ADDRLPl stream above to [o \n1 0 2 2 1 1 11, using the table m. Zero denotes a symbol not seen previously. 4. Huffman-code all MTF \nindices but no MTF tables. 5. gzip to produce the final, fully-compressed version of the original program. \nBefore applying gzip, ail MTF streams and tables are encoded in 1, 2, or 4-byte values (or strings for \nsymbolic names), as appropriate. For instance, each unique instance of a particular tree is encoded as \na se-quence of bytes, one per operator, emitted in prefix order. char literals are encoded as individual \nbytes, short literals as pairs, etc.  The table below compares the size of three conventional SPARC \ncode segments with our wire code. Conventional code Wire code uncom-Gtipped pressed ICC 3 15,636 75,928 \n64,475 gee 1,381,304 380,451 287,260 wep 61,036 15,936 16,013 So the wire format improves significantly \nover conven-tional encodings, dividing the input size by as much as 4.9 And it beats the gzipped version \nsignificantly as well, ex- cept for a small loss on the smallest input. All compilations above were done \nwith ICC, because it was the source of the wire byte-codes. A compiler with a more ambitious optimizer \nwould probably make the conventional code smaller, but it would probably do likewise for byte-codes too, \nif it were adapted to emit them.  An interpretable code Our wire format achieves unprecedented levels \nof code density by organizing semantically similar instruction com- ponents into separately compressed \nstreams. This method exploits the insight that byte-stream or word-stream com-pression techniques such \nas Lempel-Ziv [Lempel and Ziv; Ziv and Lempel] will miss the correlation among sub-byte and sub-word \nquantities in instructions. Such quantities include opcodes and various types of operands. For exam- \nple, LZ compression will inefficiently code simple instruc- tion semantics such as a call instruction \noften follows a move instruction, because the bits or bytes that represent opcodes are intermixed with \nbits or bytes that have other semantics, such as the destination register of the move is no. Our wire \nformat makes those semantics available to LZ compression by grouping opcodes and various types of operands \ninto separate streams. Because this strategy uses LZ compression, it requires linear decompression. Some \napplications, such as just-in-time machine code generation or working set reduction through direct interpretation \nof compressed code, require a randomly addressable, compact program representation. In this section, \nwe describe two simple techniques, operand specialization and opcode com- bination, that yield a dense, \nrandomly addressable program representation called BRISC. These techniques exploit the same stream-separation \ninsight as the tree compression method given above. However, instead of physically sepa-rating streams \nof instruction information, operand speciali- zation and opcode combination quantize the representation \nof these streams by packing them into a randomly accessi- ble stream of discrete byte codes. We conclude \nthis section by presenting and analyzing measurements of a production- quality virtual machine environment \n(OmniVM). These measurements demonstrate that BRISC supports just-in- time code generation at 2.5MB/sec \nwhile yielding code density that is competitive with the best packaged LZ com- pression programs. Our \nsystem, called Omniware, includes a compiler that converts high-level language programs into sequences \nof instructions for the Omniware virtual machine (OmniVM) [Lucco, PLDI96]. OmniVM has a RISC instruction \nset augmented with macro-instructions for common operations such as moving and initializing blocks of \ndata. The next section will describe how this input differs from the ICC intermediate representation \nused in the experiments related above. In brief, one can automatically at code generation time synthesize \nOmniVM RISC instructions from ICC IR and hence the two are interconvertible with respect to com- pression. \nHowever, the OmniVM programs measured in this section were highly optimized using a commercial compiler \nback end and so contain more information, such as register allocation decisions, than 1 cc IR. The Omniware \nsystem compresses fully linked executable programs containing OmniVM RISC instructions into pro- grams \ncontaining BRISC instructions. A server ships these across a network to client computers, which contain \nan implementation of the OmniVM. The OmniVh4 either in- terprets the BRISC instructions directly or converts \nthem to native machine code. The system works on several plat- forms, including x86Nr, SPARUSolaris 2.4, \nPowerPUNT, and PowerPC/MacOS. All measurements in this section were performed on a Pentium 12OMHZ proces- \nsor running NT 4.0. The processor was configured with 32 megabytes of memory. BRISC generation Because \nwe require BRISC to be interpretable, we con-strain its design to ensure that instructions occur on byte \nboundaries. Hence, where the split-stream compression techniques described above would use 2-3 bits per \nopcode, BRISC will always use 8 or 16 bits per opcode. To make up for the increased size of its opcodes, \nBRISC packs more information into each opcode. It does so through operand specialization and opcode combination. \n Operand specialization We briefly described operand specialization in the back- ground section, as \nburning in a particular value for one or more of the fields of a pattemized instruction. We now re- turn \nto the subject, describing operand specialization con-cretely in terms of the Omniware system. Consider \nthe OmniVM instruction Id.iw no, 4 ( sp) . The effect of this instruction is to load the 32-bit word \nat address sp+4 into register no.The . iw suffix on this instruction indicates that this is the 32-bit \ninteger version of the instruction. As it turns out, this particular instruction is the most frequently \noccurring instruction among our benchmark programs. To investigate possible specializations of this instruction, \nwe patternize it into the following set of patterns (ordered from least to most general): 1. ld.iw nO,l(sp) \n 2. ld.iw *,4(sp)  3. ld.iw n0,4(*)  4. ld.iw nO,*(sp)  5. ld.iw . ,4(*)  6. ld.iw *(s-p)  7. \nld.iw no:*(*)  8. ld.iw *,*(*)  The most general instruction pattern (8) is part of the base instruction \nset. When we write base instructions in patter- nized form as above, we place asterisks in all field \nposi- tions of the instruction, to indicate that the base instruction pattern can take on any legal field \nvalue in any field posi- tion. For example, writing the base integer register move instruction as mov. \ni *, *, indicates that each of the in- struction s fields can take on any value legal for the field s \ntype. In the case of this mov . i instruction, both of its fields can take on any value from nO through \nn15, because the OmniVM has 16 integer registers. Since Id. iw no, 4 (sp) is the most frequently occurring \ninput instruction occurring in our benchmarks, it makes sense to add to our dictionary of possible instruction \npat- terns some of the specialized forms of this instruction. By doing so, we avoid explicitly representing \ncommon oper-ands such as no or 4. The compression algorithm we de- scribe below performs operand specialization \none field at a time. For example when the compressor encounters the specific instruction (1) during an \ninput scan, it generates instruction patterns (5)-(T) as candidate dictionary entries. To arrive at a \ntwo-operand-specialized instruction pattern such as Id.iw no, 4 ( . ) , the compressor would first add \nld.iw no,*(*) or ld.iw . ,4(*) to the dictionary. It would then modify the input program to reflect the \npres- ence of this new instruction pattern. On a subsequent pass over the input program, the compressor \ncould add to the dictionary a more specialized version of this instruction pattern through incorporation \nof another field. To denote an input instruction that has been converted to use an operand- specialized \ninstruction pattern, we first write the instruction pattern encased in square brackets followed by a \nlist of the literal values to be substituted into the unspecified fields (denoted by asterisks) of the \ninstruction pattern. For exam- ple, if we have derived instruction pattern (5) from input instruction \n(l), then we would re-write the input instruction as [ld.iw . ,4(*)]:nO,sp.  Opcode combination The \ncompressor also generates candidate instruction pat-terns through opcode combination. In our system, \nevery adjacent pair of opcodes is a candidate for opcode combi- nation. For example, if the input program \ncontains the se- quence of instructions [ld.iw nO,*(*)]:4,sp; mov.i n2, no, the instruction pattern < \n[Id no, *(*) I , [mov. i , I > would become a candidate for addition into the base instruction set. We \ndenote with angle brackets instruction patterns resulting from opcode combination. Because BRISC is quantized, \nnot all instruction combina- tions make sense. If a combined instruction pattern leaves a trailing sub-byte \noperand, the compressor can defer combi- nation until further specialization has taken place (so that \nthe combined unspecified operands from the adjacent in- structions would pack neatly into a whole number \nof bytes). The compressor generates as candidate instruction patterns not only each pair of adjacent \ninstructions <i,p, but every possible pair consisting of a zero or one-field operand spe- cialization \nof i followed by a zero or one-field operand spe- cialization of i This ensures that operand specialization \nwon t compete with opcode combination by further spe-cializing an instruction before the combiner has \na chance to consider a less-specialized version. Opcode combination captures common code generation idioms. \nFor example, data movement instructions such as Id. iw and mov. i frequently occur to set up parameters \nbefore call instructions. This results in a quantized version of the tree construction shown in the previous \nsection.  BRISC generation algorithm The compressor begins with the base instruction set (cur- rently \n224 instruction patterns) and adds to it to create a dictionary of frequently occurring instruction patterns. \nTo find useful instructions to add to the dictionary, the com- pressor scans the input program several \ntimes, generating candidate instruction patterns and estimating their program size reduction P and their \ncost in decompressor memory usage W (W abbreviates working set ). The program size reduction P equals \nthe reduction in compressed program bytes that would occur if the candidate instruction pattern were \nadded to the dictionary minus the number of bytes needed to represent the instruction pattern in the \ndictionary. The decompressor for BRISC uses a table of native in- struction sequences for interpretation \nor native code gen- eration. The compressor estimates the decompressor s memory usage cost, W, for a \ndictionary entry by averaging the size in bytes of decompression table instruction se-quences for the \nPentium and PowerPC 601 chips. The benefit B of an instruction pattern equals P-W (of course, in abundant \nmemory situations we can set B equal to P). The compressor maintains a heap of candidate instructions, \nsorted by B. After each pass over the input program, the compressor removes the K best candidates from \nthe heap and adds them to the dictionary. Then, the compressor modifies the input program to reflect \nthe newly available instruction patterns. It first considers each pair of instruc- tions that can be \ncombined by a new opcode-combined instruction pattern. On each pass, there can only be one new instruction \npattern that applies to a particular pair. Af- ter it performs instruction combination, the compressor \nmodifies all instructions in the input program that could be represented more compactly using one of \nthe new instruc- tion patterns. To avoid undue overhead in updating the in- put program, the compressor \nmaintains a table that maps each base instruction pattern to a list of all input program instructions \nmatching that pattern. Similarly, to avoid gen- erating candidate instruction patterns that have already \nbeen generated, the compressor maintains a hash table of previously generated candidates, keyed by base \ninstruction patterns and specialized field values. The compressor ceases to hunt for useful patterns \nafter a pass that doesn t yield at least K patterns for which B is positive. Thus the compressor uses \na greedy algorithm for building the dictionary. The optimal algorithm would con- sider all possible dictionaries \nand their effect on compres- sion, but this would be prohibitively time-consuming. To perform dictionary \nencoding, the compressor uses an order- 1 semi-static Markov model so that all opcodes fit within 8 bits. \nIn other words, the compressor builds (and the decom- pressor can build, based on the dictionary) a table \nfor each possible instruction pattern I that enumerates the instruction patterns that can follow I in \nthe input. If more than 256 instructions can follow I, the compressor splits I into two instruction patterns. \nFor example, the dictionary for the OmniVM program implementing ICC contains 981 in-struction patterns. \nEach instruction pattern has at most 244 instruction patterns that can follow it. There is a special \ncontext in the Markov model for basic block beginnings (of various types) so that the BRISC program remains \ninter- pretable. Once the compressor has created a dictionary, it outputs the dictionary followed by \nthe modified input pro- gram that it has compressed during dictionary construction.  A BRISC compression \nexample The Omniware C++ compiler generates the following se- quence of OmniVM instructions for the example \nprogram introduced in the wire format discussion above. enter SP,SP,~~ spil1.i n4,16(sp) spil1.i ra,2O(sp) \nm0v.i n4,nO m0v.i n2,nl b1e.i n4,O,$L56 m0v.i nl,n4 m0v.i nO,n2 call 3ewer $L56: add.i nO,n4,-1 re1oad.i \nn4,16(sp) re1oad.i ra,20(sp) exit SP,SP,~~ rjr ra For this input program, the initial dictionary is \nthe set of baseinstructionsituses: (enter, spill.i, mov.i, ble.i,call,add.i,reload.i,exit, and rjr}.Be- \ncause this program is small, it affords little opportunity for useful instruction combination or specialization. \nHowever, we can use it to illustrate some of the steps of BRISC com- pression. We will consider just \nthe first three instructions of the program. Applying operand specialization to these three instructions \ngenerates following candidate specializations in the first pass of the BRISC algorithm: 1. [enter sp,*,*] \n[enter . ,sp,*l [enter . ,*,241 2. [spill.i n4,*(*)1 tspil1.i *,16(*)1 [spill.i *,*(sp)]  3. [spill.i \nra,*(*)] [spill.i *,20(*)1  Note that one candidate specialization of instruction 3, spil1.i *,*(sp), \nhas already been generated by ap- plying operand specialization to instruction 2. For each instruction, \nthe set of candidate instructions generated through operand specialization is called that instruction \ns operand-specialized set. If we add the corresponding base instruction pattern to the operand-specialized \nset for a given input instruction i, we construct the augmented operand- specialized set of candidate \ninstruction patterns for i. To apply opcode combination to instructions 1 and 2, we gen- erate the 16 \npairs of instruction patterns that can be formed by selecting one element from instruction l s augmented \noperand-specialized set of candidates and one element from instruction 2 s augmented operand-specialized \nset of candi- dates: <[enter sp,*, *],[spill.i n4,*(*)]> <[enter sp,*,*],[spill.i *,16(*)1> <[enter sp,*,*],[spill.i \n,*(sp)l> <[enter *,sp, . ],[spill.i n4,*(*)1> etc. Hence the total set of candidate instruction patterns \ngenerated by instructions 1 and 2 for our example program would be the 16 candidates generated through \nopcode com- bination and the 6 candidates generated through opcode specialization. Because the total \nset of base instruction patterns is only 224, however, the total number of candi- dates generated by \na large program remains manageable. For example, the total number of candidates tested in com- pressing \ngee-2.6.3 is 93,211. The final dictionary for gcc- 2.6.3 contains 1232 instruction patterns, including \nbase instruction patterns. To illustrate the operation of our cost-benefit metric, we will apply it \nto one of our candidate instructions, [enter ] . The file size cost of a dictionary entry for [en-ter \nsp,*, *I is 2 bytes, 1 byte to indicate the base in- struction, enter, 2 bits to indicate which field \nis special- ized, and 4 bits to set the specialized value for that field. The working set cost of a dictionary \nentry for [enter sp.. , * ] is dominated by the sequence of native instruc- tions that will be generated \nby the decompressor to gener- ate code for this instruction. For just-in-time conversion to Pentium instructions, \nthe instruction space required is 17 bytes; on a PowerPC 601, the instruction space required is 28 bytes. \nAveraging these yields W=25 for [enter sp,*,* ] . This instruction pattern saves one byte over the original \ninput program. One input instruction, [enter sp, sp ,24 I would be represented in 2 bytes instead of \n3 bytes, because the remaining field values, sp and 24, can be compacted into a single operand byte. \nHowever, the pro- gram size reduction P is this 1 byte saved minus the 2 bytes of dictionary entry. The \nbenefit B=P-W= -26 and hence we would not add this instruction pattern to the dictionary. sp, I. Because \nof their code-generation/interpretation table cost, W, none of the candidate instructions are suitable, \nand the program, as given, remains. For a large input, in contrast, the benefits of operand specialization \nand opcode combi- nation will outweigh the mstruction table costs. To illus- trate this, we applied the \ndictionary generated in com-pressing gee-2.6.3 to our example program. The resulting compressed program \nis listed below. <[enter-x4 sp,sp,*], [spill.i-x4 n4,*(sp)],  [spill.i-x4 ra,*(sp)]>: 6,4,5 <[mov.i \n*, nO],[mov.i *,nl]>: n4, n2 [ble.i *,O,*]: n4, $L56 <[mov.i nl,n4],[mov.i nO,n2]> call qepper SL56: \n [sub-lt32.i no,*,*]: n4, 1 epi The first instruction spans three lines. As before, angle brackets indicate \nopcode combination. Recall from above that if an instruction contains unspecified fields (denoted by \nasterisks), it will be followed by a colon and then a list of literal values to insert, in order, into \nthe unspecified fields. The -x4 suffix indicates that immediate values should be multiplied by four. \nThe final instruction of this sequence, epi,is a special-case macroinstruction. Its semantics are to \nexit the current func- tion, restoring callee-saved registers, restoring the frame and returning in the \nnormal fashion (using the r j r instruc-tion). epi is the only such instruction used in our compres- \nsor. All other dictionary entries are generated through ei- ther operand specialization or opcode combination. \nThe total number of bytes in the original input was 60. The compressed program totals 17 bytes, 7 bytes \nfor instruction opcodes and 10 bytes for packed literals. This compression ratio is better than the average \nfor our benchmark programs because function prologue and epilogue make up a greater proportion of this \nexample than in our benchmark pro-grams. Results The table below gives executable program sizes for \nseveral benchmark programs. The code sizes -for K=20 -are relative to Pentium chip executable programs \nproduced using Microsoft Visual C++ 5.0 (i.e. the size of the Visual C++ 5.0 output for each program \nis normalized to 1.0). This table shows that BRISC is competitive with gzip in code size. In addition, \nthe table gives the just-in-time native code generation speed for each program in megabytes per second \nof produced Pentium code. It also shows the run- time of the program relative to native Pentium code \npro- duced by Microsoft Visual C++ 5.0. The runtimes of the BRISC programs include the time necessary \nto generate the native code. Finally, the table also shows the runtime of each BRISC program when interpreted, \nagain relative to Pentium code produced by Microsoft Visual C++ 5.0. The runtime numbers for Microsoft \nWord97 are the average of three metrics: automatic document formatting time for an issue of Slate magazine, \npage-through time for Slate, and cold boot time (program not present in NT 4.0 disk cache). BRISC compression \nfor Word97 is somewhat less effective than for the other benchmark programs. This is due to an unusually \nlarge number of 16-bit operations in Word97. These results demonstrate that the BRISC code compres- sion \nalgorithm yields programs that are highly suitable for mobile code. They require no more network bandwidth \nthan gzipped native code, yet the OmniVM can generate native code from BRISC programs at over 2.5 megabytes \nper second on a Pentium 120MHz processor with 32 mega- bytes of memory. Hence, in a local area network, \nBRISC is a good mobile program representation choice. Over a mo- dem, the tree compression algorithm \ngiven above will do better at minimizing the latency between when a program is requested and when the \nprogram begins performing useful work on the client machine.  Reducing RISC abstract machines RISC designs \nare reduced but rarely minimal. Most have addressing modes and immediate instructions that are re- dundant; \nthat is, they could be simulated by simpler forms, such as load- and store-indirect and load-immediates. \nThe abbreviations make hardware implementations faster, but their value in abstract machines is less \nclear. It amounts to limited ad hoc code compression, but having two forms for, say, integer additions \n-one explicit and the other hidden in register-displacement addresses -might hurt the code com- pressor \nmore than it helps. Also, it makes the abstract ma- chine harder to implement We ran some experiments \nto try to answer this question. We wrote an OmniVM back end for ICC and then progres- sively de-tuned \nit by removing: . all immediate instructions except for one primitive: load-immediates, or . all addressing \nmodes except for two primitives: load- and store-indirect, or . both. Then we compiled ICC itself to \nuse the de-tuned compilers and compressed the results using the methods from the last section above. \nThe results are: Abstract machine variant Compressed size/native size RISC 0.54 minus immediates 0.56 \nminus register-displacement 0.57 minus both 0.59 These results suggest that a minimal abstract machine \ncom- presses nearly as well as one with typical ad hoc features for making programs smaller. Acknowledgments \nTodd Proebsting s work was supported in part by grants from IBM. the AT&#38;T Foundation. the NSF (CCR-9502397 \nand CCR- 9415932) and ARPA (N66001-96-C-8518 and DABJ-63-85-C- 0075). John Miller and Gideon Yuval of \nMicrosoft Research pro- vided helpful background and suggestions. Bibliography Timothy C. Bell, John \nG. Cleary, and Ian H. Witten. Text Com- pression. Prentice Hall, 1990. Jon Louis Bentley, Daniel D. Sleator, \nRobert E. Tarjan, and Vic- tor K. Wei. A locally adaptive data compression scheme. Commu- nications of \nthe ACM 29(4):520-540,486. Peter Elias. Interval and recency rank source coding: Two on-line adaptive \nvariable-length schemes. IEEE Transactions on Infor- mation Theory IT-33(l), 1987. M. Franz and T. Kistler. \nSlim binaries. TR 96-24, Dept of Infor- mation and Computer Science, University of California, Irvine, \n6/96. Also http://www.ics.uci.edt&#38;oberon/research.html and to appear in Communications of the ACM. \nM. Franz. Adaptive compression of syntax trees and iterative dynamic code optimization: Two basic technologies \nfor mobile- object systems. TR 97-04, Dept of Information and Computer Science, University of California, \nIrvine, 2/97. Christopher W. Fraser and David R. Hanson. A Retargerabfe C Compiler: Design and Implementation. \nAddison Wesley Long- man, 1995. Christopher W. Fraser and Todd A. Proebsting. Custom instruc- tion sets \nfor code compression. htto:Nwww.cs.arizona.edu/neonl&#38;odd/naners/nldi2.us, 10/95. T. Kistler and M. \nFranz. A tree-based alternative to Java byte- codes; TR 96-58, Dept of Information and Computer Science, \nUniversity of California, Irvine, 12196. A. Lempel and J. Ziv. On the complexity of finite sequences. \nIEEE Transactions on Information Theory 22( 1):75-8 1, lt76. Ali-Reza Adl-Tabatabai, Geoff Langdale, \nSteven Lucco and Rob- ert Wahbe. Efficient and language-independent mobile programs. PL.DI %: 127-136,6/96. \nTodd A. Proebsting. Optimizing an ANSI C interpreter with su- peroperators, POPLP5:322-332, l/95. Ian \nH. Witten, Radford M. Neal, and John G. Cleary. Arithmetic coding for data compression. Communications \nof the ACM 30(6):520-540,6/87. Tong L.ai Yu. Data compression for PC software distribution. Sofrware-Practice&#38;Experience26(11):1181-1195, \n11196. J. Ziv and A. Lempel. Compression of individual sequences via variable-rate coding. IEEE Transactions \non Information Theory 24(5):530-536,9/78.   \n\t\t\t", "proc_id": "258915", "abstract": "Current research in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This view has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of network and disk speeds to CPU speeds grows exponentially.For example, we have seen the CPU idle for most of the time during paging, so compressing pages can increase total performance even though the CPU must decompress or interpret the page contents. Another profile shows that many functions are called just once, so reduced paging could pay for their interpretation overhead.This paper describes:&amp;bull; Measurements that show how code compression can save space <i>and</i> total time in some important real-world scenarios.&amp;bull; A compressed executable representation that is roughly the same size as gzipped x86 programs and can be interpreted without decompression. It can also be compiled to high-quality machine code at 2.5 megabytes per second on a 120MHz Pentium processor&amp;bull; A compressed \"wire\" representation that must be decompressed before execution but is, for example, roughly 21% the size of SPARC code when compressing gcc.", "authors": [{"name": "Jens Ernst", "author_profile_id": "81100203888", "affiliation": "University of Arizona, Dept of Computer Science, Gould Simpson Building, Tucson, AZ", "person_id": "P138530", "email_address": "", "orcid_id": ""}, {"name": "William Evans", "author_profile_id": "81100485414", "affiliation": "University of Arizona, Dept of Computer Science, Gould Simpson Building, Tucson, AZ", "person_id": "PP31044304", "email_address": "", "orcid_id": ""}, {"name": "Christopher W. Fraser", "author_profile_id": "81100364566", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P47620", "email_address": "", "orcid_id": ""}, {"name": "Todd A. Proebsting", "author_profile_id": "81100592757", "affiliation": "University of Arizona, Dept of Computer Science, Gould Simpson Building, Tucson, AZ", "person_id": "P283229", "email_address": "", "orcid_id": ""}, {"name": "Steven Lucco", "author_profile_id": "81100431977", "affiliation": "Microsoft, One Microsoft Way, Redmond, WA", "person_id": "PP42051841", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258947", "year": "1997", "article_id": "258947", "conference": "PLDI", "title": "Code compression", "url": "http://dl.acm.org/citation.cfm?id=258947"}