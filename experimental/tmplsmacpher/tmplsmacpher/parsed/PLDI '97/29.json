{"article_publication_date": "05-01-1997", "fulltext": "\n Data-centric Multi-level Blocking Induprakas Kodukula, Nawaaz Ahmed, and Keshav Pingali Department of \nComputer Science, Cornell University, Ithaca, NY 14853. {prakas,ahmed,pingali}Qcs.cornell.edu Abstract \nWe present a simple and novel framework for generat- ing blocked codes for high-performance machines \nwith a memory hierarchy. Unlike traditional compiler tech-niques like tiling, which are based on reasoning \nabout the control flow of programs, our techniques are based on reasoning directly about the flow of \ndata through the memory hierarchy. Our data-centric transformations permit a more direct solution to \nthe problem of enhanc- ing data locality than current control-centric techniques do, and generalize easily \nto multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard \nbenchmarks from the problem do- main of dense numerical linear algebra. The simplicity and intuitive \nappeal of our approach should make it at- tractive to compiler writers as well as to library writers. \n Introduction Data reuse is imperative for good performance on mod- ern high-performance computers because \nthe memory architecture of these machines is a hierarchy in which the cost of accessing data increases \nroughly ten-fold from one level of the hierarchy to the next. Unfortu-nately, programs with good data \nreuse cannot be ob- tained for most problems by straight-forward coding of standard algorithms. In some \ncases, it is necessary to develop new algorithms which exploit structure in the underlying problem to \nreuse data effectively; a well-known example of this is Bischof and van Loan s WY algorithm for QR factorization \nwith Householder reflec- tions, which was developed explicitly for improving data reuse in orthogonal \nfactorizations [15]. Even when ex-isting algorithms are sufficient, reorganizing a program to reuse data \neffectively can increase its size by orders of magnitude, and make the program less abstract and less \nportable by introducing machine dependencies. In this paper, we describe restructuring compiler technology \nthat reorganizes computations in programs to enhance data reuse. We evaluate the performance of this \ntechnology in the problem domain of dense numer- ical linear algebra. This problem domain is appropriate \nbecause it is important in practice, and there are li-braries of hand-crafted programs with good data \nreuse This work WM supported by NSF grant CCR-9503199, ONR grant N00014-93-1-0103, and the Cornell Theory \nCenter. which can be used for comparisons with automatically generated code. Some of these programs are \ndiscussed in Section 2. The rest of this paper is organized as follows. In Section 3, we discuss current \nsolutions to the problem of developing software with good data reuse, including hand-crafted libraries \nlike LAPACK [2], and automatic compiler techniques such as tiling [24]. The technology described in this \npaper was motivated by the limitations of these approaches, and is introduced in Section 4. This technology \ndiffers from standard restructuring compiler technology like tiling because it is based on reasoning \nabout the data flow rather than the control flow of the program. In a sense that is made precise later \nin the pa- per, our approach is data-centric, and it should be con- trasted with existing compiler techniques \nfor promot- ing data reuse, which are control-centric. In Section 5, we present a general view of data-centric \ntransforma-tions, and show how to reazon about the correctness of such transformations. In Section 6, \nwe show how data-centric transformations can be combined together to produce new transformations, and \nuse these ideas to generate transformations to enhance data reuse in com- mon dense linear algebra benchmarks. \nIn Section 7, we describe performance results. Finally, we discuss ongo- ing work in Section 8. 2 Running \nExamples Figure 1 shows three important computational kernels that we will use to illustrate the concepts \nin this pa- per. Figure l(i) shows matrix multiplication in the so-called I-J-K order of loops. It is \nelementary for a compiler to deduce the well-known fact that all six permutations of these three loops \nare legal. This loop is called a perfectly nested loop because all assignment statements are contained \nin the innermost loop. Fig-ure l(ii, iii) show two versions of Cholesky factorization called right-looking \nand left-looking Cholesky factoriza-tion; both these loop nests are imperfectly nested loops. Both codes \ntraverse the matrix A a column at a time. In right-looking Cholesky, the columns to the right of the \ncurrent column are updated by the L-K loop nest, using the outer product of the current column, as shown \nin Figure 2(i). The L and K loops are called the update loops. Left-looking Cholesky performs lazy updates \nin the sense a column is updated only when it is visited by do I = l..N do J = l..N do K = l..I C[I,J] \n= C[I,Jl + A[I,KI * BIK,Jl (i) Matriz Multiplication do J = l..N Sl: A[J,Jl = sqrt (ACJ,JI) do I = J+l..N \nS2: A[I,Jl = AlI,Jl / A[J,Jl do L = J+l..N do K = J+l..L S3: A[L,K] = A[L,K J -A[L,JI * A[K,JI (ii) \nRight-looking Cholesky Factorization do J = l..N do L = J..N do K = l..(J-1) S3: A[L,JI = AfL,Jl -AIL,Kl*ACJ,KI \nSl:A[J,JI = sqrt(AfJ,Jl) do I = J+l..N S2: A[I,Jl = AII,JI / ACJ,JI (iii) Left-looking Cholesky Factorization \nFigure 1: Running examples -Matrix Multiply and Cholesky -J- -I---- (i) Right-looking Cholesky (ii) \nL&#38;ding Cholesky Figure 2: Pictorial View of Cholesky Factorization the outermost loop. The shaded \narea to the left of the current column in Figure 2(ii) shows the region of the matrix that is read for \nperforming this update.  Previous Work The numerical analysis community has used a layered approach \nto the problem of writing portable software with good data reuse. The general idea is to (a) find a set \nof core operations for which algorithms with good data reuse are known, (b) implement carefully hand-tuned \nimplementations of these algorithms on all plat- forms, and (c) use those operations, wherever possible, \nin writing programs for applications problems. From Amdahl s law, it follows that if most of the computa- \ntional work of a program is done in the core operations, the program will perform well on a machine with \na mem- ory hierarchy. While the implementations of these op- erations are not portable, the rest of the \nsoftware is machine-independent. In the context of dense numerical linear algebra, the core operation \nis matrix multiplication. The stan-dard algorithm for multiplying two n x n matrices per-forms n operations \non n data, so it has excellent data reuse. Most vendors provide so-called Level-3 BLAS routines which \nare carefully hand-optimized, machine-tuned versions of matrix multiplication. To exploit these routines, \nthe numerical analysis community has invested considerable effort in developing block-matriz algorithms \nfor standard dense linear algebra problems such as Cholesky, LU and QR factorizations. These block algorithms \noperate on entire submatrices at a time, rather than on individual matrix elements, and are rich in matrix \nmultiplications. The well-known LA- PACK library contains block matrix algorithms imple-mented on top \nof the BLAS routines, and is written for good data reuse on amachine with a two-level memory hierarchy \n[2]. The LAPACK library has been successful in prac- tice. However, it requires a set of machine-specific, \nhand-coded BLAS routines to run well. Since it is not a general-purpose tool, it cannot be used outside \nthe realm of the dense numerical linear algebra. It is also specifically written for a two-level memory \nhi-erarchy, and it must be re-implemented for machines with deeper memory hierarchies. Therefore, automatic \nprogram restructuring tools that promote data reuse through transformations provide an attractive alterna-tive. \nThe restructuring compiler community has devoted much attention to the development of such technology. \nThe most important transformation is Mike Wolfe s it-eration apace tiling (241, preceded by linear loop \ntram- formation if necessary [4, 17, 231. This approach is restricted to perfectly nested loops, although \nit can be extended to imperfectly nested loops if they are first transformed into perfectly nested loops. \nA loop in a loop nest is said to carry reuse if the same data is touched by multiple iterations of that \nloop for fixed outer loop iterations. For example, loop K in Figure l(i) carries reuse because for fixed \nI and J, all iterations of the loop touch C II, 31; similarly, loop I carries reuse be cause successive \niterations of the I loop touch B [K, J] . Loops that carry reuse are moved as far inside the loop nest \nas possible by using linear loop transformations; if two or more inner loops carry reuse and they are \nfully pemnutable, these loops are tiled [24]. Intuitively, tiling improves performance by interleaving \niterations of the tiled loops, which exploits data reuse in all those loops rather than in just the innermost \none. It is easy to ver- ify that all the three loops in the matrix multiplication code carry reuse and \nare fully permutable. Tiling all three loops produces the code shown in Figure 3 (for 25 x 25 tiles). \nThe three outer loops enumerate the iteration space tiles, while the three inner loops enu-merate the \niteration space points within a tile. In this case, iteration space tiling produces the same code as \nthe equivalent block matrix code [15]. Tiling can be applied to imperfectly nested loops if these loops \nare converted to perfectly nested loops through the use of code sinking [25]. Code sinking moves all \nstatements into the innermost loop, inserting appro- priate guards to ensure that these statements are \nexe- cuted the right number of times. There is no unique way to sink code in a given loop nest; for example, \nin left- do tl = 1 -. r(N/25)1 do t2 = 1 . . [(N/25)] do t3 = 1 . . [(N/25)] do It = (tl-1)*25 +l . . \nmin(tl*25,N) do Jt = (t2-1)*25 +l . . min(t2*25,N) do Kt = (t3-1)*25 +l . . min(t3*25,N) C[It,Jt] = C[It,Jtl \n+ A[It,Ktl * B[Kt,Jtl Figure 3: Blocked Code for matrix matrix multiplica-tion looking Cholesky, statement \nSl can be sunk into the I loop or into the L-K loop nest. Other choices arise from the possibility of \ndoing imperfectly nested loop trans-formations (especially loop jamming) during the code sinking process. \nDepending on how these choices are made, one ends up with different perfectly nested loops, and the resulting \nprograms after linear loop transfor-mations and tiling may exhibit very different perfor-mance. In right-looking \nCholesky for example, if Sl is sunk into the I loop, and the resulting loop is sunk into the L-K loop \nnest, we end up with a 4-deep loop nest in which only the update loops can be tiled, even if linear loop \ntransformations are performed on the per- fectly nested loop. If on the other hand, we jam the I and \nL loops together, and then perform sinking, we get a fully permutable loop nest; tiling this loop nest \nproduces code with much better performance. Simi-larly, it can be shown that for left-looking Cholesky, \nthe best sequence of transformations is to first sink Sl into the I loop, jam the I and L loops together \nand then sink Sl and S2 into K loop. Interactions between loop jamming/distribution and linear loop transformations \nhave been studied by McKinley et al [la]. However, no systematic procedure for exploring these options \nfor ob- taining perfectly nested loops from imperfectly nested loops is known. A somewhat different approach \nhas been taken by Cart and Kennedy [7]. By doing a detailed study of matrix factorization codes in the \nLAPACK library, they came up with a list of transformations that must be performed to get code competitive \nwith LAPACK code. These include strip-mine-and-interchange, preceded by index-set-splitting and loop \ndistribution to make the interchange legal [S]. Additional transformations such as unroll-and-jam [7] \nand scalar replacement are per-formed on this code to obtain code competitive with hand-blocked codes \nused in conjunction with BLAS [S]. However, it is unclear how a compiler can discover auto- matically \nthe right sequence of transformations to per- form; it is also unclear whether this approach can be generalized \nfor a machine with a multi-level memory hi- erarchy. Finally, there is a large body of work on determin-ing \ngood tile sizes [6, 14, 20, 22, 121. This research focuses on perfectly nested loops with uniform depen-dences \n(i.e. dependence vectors can be represented as distances). While this work is not directly comparable \nto ours, the detailed memory models used in some of this research [18, 22, 161 are useful in general \nfor estimating program performance.  4 Data-centric Transformations Since the goal of program transformation \nis to enhance data reuse and reduce data movement through the mem- ory hierarchy, it would seem advantageous \nto have a tool that orchestrates data movement directly, rather than as a side-effect of control flow \nmanipulations. The ultimate result of the orchestration is, of course, a transformed program with the \ndesired data reuse, but to get that program, the tool would reason directly about the de- sired data \nflow rather than about the control flow of the program. A useful analogy is signal processing. The in- \nput and the output of signal processing is a signal that varies with time, and in principle, all processing \ncan be done in the time domain. However, it is often more con- venient to take a Fourier transform of \nthe signal, work in the frequency domain and then take an inverse Fourier transform back into the time \ndomain. 4.1 Data Shackle In the rest of the paper, the phrase statement in&#38;ance refers to the execution \nof a statement for given values of index variables of loops surrounding that statement. Definition 1 \nA data shackle is a speci$cation in three parts. We choose a data object and divide it into blocks. We \ndetermine a sequence in which these block3 are UtouchedJP by the processor. For each block, we determine \na set of statement in-atancea to be performed when that block is touched by the proceaaor. However, we \nleave unspecified the order of enumeration of statement in.rtances within this Jet. We now look at this \nin detail. . The data objects of interest to us are multidimen- sional arrays. An array can be sliced \ninto blocks by using a set of parallel cutting plane3 with nor-mal n, separated by a constant distance \nd. Further slicing can be performed by using additional sets of planes inclined with respect to this \nset of planes. We define the catting planes mat&#38; as the matrix whose columns are the normals to the \ndifferent sets of cutting planes; the order of these columns is de- termined by the order in which the \nsets of cut-ting planes are applied to block the data. Figure 4 shows the blocking of a two-dimensional \narray with two set! of cutting planes; the cutting planes ma- I 1 0 Itrix is o I . . A bloik is Jassigned \nthe block co-ordinate (Zl,... ,z,,,) if it is bounded by cutting planes numbered z; -1 and z; from the \ni h set of cut-ting planes. The code that we generate schedules blocks by enumerating them in lexicographic \norder of block co-ordinates. One possible order for executing these statement instances is to use the \nsame order as the initial code. We leave the order unspecified because it permits us to join data shackles \ntogether to get finer degrees of control on the execution, as we will see in Section 6. . The final \nstep is to specify the statement instances that should be performed when a block is sched- uled. From \neach statement S, we choose a single reference R of the array that is being blocked. For now, we assume \nthat a reference to this array ap- pears in every statement. We will relax this condi- tion in Section \n5. When a block of data is scheduled, we execute all instances of S for which the data touched by ref- \nerence R is contained in this blocka. As mentioned before, the order in which these instances should \nbe done is left unspecified.   $ -2 + i- $ N ~ltllliflillilillliiil w . Figure 4: Cutting planes on \na data object The rationale for the term data shackle should now be clear. One thinks of an instrument \nlike a pantograph in which a master device visits the blocks of data in lexicographic order, while a \nslave device shackled to it is dragged along some trajectory in the iteration space of the program, It \nis also convenient to be able to refer to statement instances shackled to a particular block of data. \nSince a data shackle reorders computations, we must check that the resulting code respects dependences \nin the original program. Legality can be checked using standard techniques from polyhedral algebra, and \nis dis- cussed in Section 5. As an example of a data shackle, consider the matrix multiplication code \nof Figure l(i). Let us block matrix C as shown in Figure 4, using a block size of 25 X 25, and shackle \nthe reference C[I,Jl in Figure l(i) to this blocking. This data shackle requires that when a par- ticular \nblock of C is scheduled, all statement instances that write into this block of data must be performed \nby the processor. We can require these to be performed in program order of the source program. Naive \ncode for accomplishing this is shown in Figure 5. The two outer loops iterate over the blocks of C. For \nevery block of C, the entire original iteration space is visited, and every Becauseof other in statement \nstate- data references S, these ment instsnces that block. may touch data outside iteration is examined \nto see if it should be executed. If the location of C accessed by the iteration falls in the current \nblock (which is determined by the conditional in the code), that iteration is executed. It is easy to \nsee that in the presence of afllne references, the conditionals are all &#38;line conditions on the loop \nindices correspond- ing to the cutting plane sets and the initial loop index variables. do bl = 1 . . \nmw5)1 do b2 = 1 . . rw5)1 do I = 1 . . N do J = 1 . . N do K = 1 . . N if ((bl-I)+26 < I <= b1+25) \n&#38;I: ((b2-1)+25< J <= b2+25) CCI,J]-CCI.Jl+ A[I,K]* BCK,J] Figure 5: Naive code produced by blocking \nC for matrix matrix multiply The code in Figure 5 is not very efficient, and is sim- ilar to runtime \nresolution code generated when shared- memory programs are compiled for distributed-memory machines [21]. \nFortunately, since the conditionals are afllne conditions on surrounding loop bounds, they can be simplified \nusing any polyhedral algebra tool. We have used the Omega calculator [19] to produce the code shown in \nFigure 6. It is simple to verify that this code has the desired effect of blocking the C array since \nblocks of C are computed at a time by taking the product of a block row of A and a block column of B. \nThis code is not the same as the code for matrix matrix product in the BLAS library used by LAPACK since \nthe block row/column of A/B are not blocked themselves. Our data shackle constrains the values of the \nloop indices I and J in Figure l(i), but leaves the values of K uncon- strained. This problem is addressed \nin Section 6 where we discuss how data shackles can be combined. In the Cholesky factorization code, \narray A can be blocked in a manner similar to Figure 4. When a block is scheduled, we can choose to perform \nall statement in- stances that write to that block (in program order). In other words, the reference \nchosen from each statement of the loop nest is the left hand side reference in that statement. Using \npolyhedral algebra tools, we obtain the code in Figure 7. In this code, data shackling re-groups the \niteration space into four sections as shown in Figure 8. Initially, all updates to the diagonal block \nfrom the left are performed (Figure 8(i)), followed by a baby Cholesky factorization block [15] of the \ndiagonal (Figure 8(ii)). For each off-diagonal block, updates from the left (Figure 8(iii)) are followed \nby interleaved SC&#38; ing of the columns of the block by the diagonal block, and local updates(Figure \n8(iv)). Note that just as in the case of matrix matrix prod- uct, this code is only partially blocked \n(compared to LA- PACK code) -although all the writes are performed into a block when we visit it, the \nreads are not localized to blocks. Instead, the reads are distributed over the en- tire left portion \nof the matrix. As before, this problem is solved in Section 6. do tl = 1 . . [(N/25)] do t2 = i . . \n[(N/25)1 do It = (tl-I)*26 +I . . min(tl*ZS.N) do Jt = (t2-1)*25 +1 . . min(tZ*ZS,N) do K = 1 . . N C[It,Jt]= \nCCIt,Jtl + ACIt,Kl . BCK,Jtl , / I Figure 6: Simplified code produced by blocking C for matrix matrix \nmultiply 4.2 Discussion By shackling a data reference R in a source program statement S, we ensure that \nthe memory access made from that data reference at any point in program exe-cution will be constrained \nto the current data block. Turning this around, we see that when a block be-comes current, we perform \nall instances of statement S for which the reference It accesses data in that block. Therefore, this \nreference enjoys perfect self-temporal lo-cality (231. Considering all shackled references together, \nwe see that we also have perfect group-temporal locdity for this set of references; of course, references \noutside this set may not necessarily enjoy group-temporal lo-cality with respect to this set. As mentioned \nearlier, we do not mandate any particular order in which the data points within a block are visited. \nHowever, if all dimensions of the array are blocked and the block fits in cache (or whatever level of \nthe memory hierarchy is under consideration), we obviously exploit spatial local- ity, regardless of \nwhether the array is stored in column- major or row-major order. An interesting observation is that if \nstride-l accesses are mandated for a particular reference, we can simply use cutting planes with unit \nseparation which enumerate the elements of the array in storage order. Enforcing stride-l accesses within \nthe blocks of a particular blocking can be accomplished by combining shackles as described in Section \n6. The code shown in Figure 7 can certainly be ob- tained by a (long) sequence of traditional iteration \nspace transformations like sinking, tiling, index-set splitting, distribution etc. As we discussed in \nthe introduction, it is not clear for imperfectly nested loops in general how a compiler determines which \ntransformations to carry out and in what sequence these transformations should be performed. In this \nregard, it is important to understand the division of work between our data- centric transformation and \na polyhedral algebra tool like do tl = I. (n+63)/64 I* Apply updates from leit to diagonal block / do \nt3 = 1, 64+tl-64 do t6 = 64*tl-63, min(n,64*tl) do t7 = t6, min(n,64*tl) A(t7,t6)= A(t7,t6). ......... \n. ........ /* Cholesky factor ........ . / do t3 = 64*tl-63, min(64*tl,n) A(t3,t3) = sqrt(A(t3. t3)) \ndo t5 = t3+1, min(64*tl.n) A(tS,t3) = A(tS,tt)/ A(t3,t3) do t6 = t3+1, min(n,brl*ti) do t7 = t6, min(n,64*tl) \nA(t7,tS). A(t7,t6) . A(t7,t3) . ........ do t2 = tltl, (n+63)/64 /e Apply updates from left to oif-diagonal \nblock */ do t3 = I. 64*tl-64 do t6 -64*tl-63, 64+tl do t7 = 64+t2-63. min(n.64*tZ) A(t7,t6) = A(t7,t6) \n. A(t7,t3) . ........ /* Apply internal scale/updater to off-diagonal block */ do t3 = 64+tl-63, 64*tl \ndo t5 = 64*t2-63, min(64stZ.n) A(tS,t3)= A(tS,t3)/ A(t3,t3) do t6 = t3+1, 64+tl do t7 = 64+t2-63, min(n,64*tZ) \n  A(t7,t6) = A(t7,t6) . A(t7,t3) . A(t6,t.3) Figure 7: Data shackle applied to right-looking Cholesky \nfactorization PY 17 4 1 t3 0 t6 L (i)Updakdiqondbkckhmkfl (ii)Ckdakyfacbrdiagadkkck \\ t7   t3 t6 b% \nFigure 8: Pictorial View of Code in Figure 7 Omega. Enforcing a desired pattern of data accesses is \nobtained by choosing the right data shackle -note that the pattern of array accesses made by the code \nof Fig- ure 5, which is obtained directly from the specification of the data shackle without any use \nof polyhedral alge- bra tools, is identical to the pattern of array accesses made by the simplified code \nof Figure 6. The role of polyhedral algebra tools in our approach is merely to simplify programs, as \nopposed to producing programs with desired patterns of data accesses. 5 Legality Since data shackling \nreorders statement instances, we must ensure that it does not violate dependences. An instance of a statement \nS can be identified by a vec- tor i which specifies the values of the index variables of the loops surrounding \nS. The tuple (S,i) represents instance 1 of statement S. Suppose there is a depen-dence from (Sl,a) to \n(S2,g) and suppose that these two instances are executed when blocks b1 and ~YJ are touched respectively. \nFor the data shackle to be legal, either bl and bs must be identical, or bl must be touched before bz. \nIf so, we say that the data shackle respect8 that dependence. A data shackle is legal if it respects \nall dependences in the program. Since our techniques apply to imperfectly nested loops like Cholesky \nfactor- ization, it is not possible to use dependence abstractions like distance and direction to verify \nlegality. Instead, we solve an integer linear programming problem. 5.1 An Example To understand the general \nalgorithm, it is useful to con- sider first a simple example: in right-looking Cholesky factorization, \nwe formulate the problem of ensuring that the flow dependence from the assignment of A [J, Jl in Sl to \nthe use of A[J, Jl in S2 is respected by the data shackle from which the program of Figure 7 was generated3. \nWe first write down a set of integer inequal- ities that represent the existence of a flow dependence \nbetween an instance of Sl and an instance of S2. Let Sl write to an array location in iteration J, of \nthe J loop, and let S2 read from that location in iteration (JI,Ir) of the J and I loops. A. flow dependence \nexists if the fol- lowing linear inequalities have an integer solution [25]: Jr = J,, 1, = J, (same location) \nN>J,zl (loop bounds) NzJ,zl (loop bounds) (1) N 1 I, > J, + 1 (loop bounds) J, 2 Jw (read after write) \n Next, we assume that the instance of S2 is performed when a block (bll, 612) is scheduled, and the instance \nof Sl is done when block (br ,t~) is scheduled. Finally, we add a condition that represents the condition \nthat the dependence is violated in the transformed code. In other words, we put in a condition which \nstates that block The shackle WM produced by blocking the matrix A aa shown in Figure 4, and choosing \nthe left hand side references of all assignment statements in Figure l(ii) for shackling. (bll, blz) \nis touched strictly after (&#38;,&#38;z). These conditions are represented as: Writing iteration done \nin (bll, bll) bll * 25 -24 5 J, 5 bll * 25 bll* 25 -24 < J,,, < bll* 25 Reading iteration done in (hi, \nbzs) (2) bl*25-24< J,<b,*25 bs*25-24<I,<bs*25 Blocks visited in bad order (h < h) v ((bll = bzl) A (bn \n< &#38;a~)) If the conjunction of the two sets of conditions (1) and (2) has an integer solution, it \nmeans that there is a dependence, and that dependent instances are per-formed in the wrong order. Therefore, \nif the conjunc- tion has an integer solution, the data shackle violates the dependence and is not legal. \nThis problem can be viewed geometrically as asking whether a union of cer- tain polyhedra contains an \ninteger point, and can be solved using standard polyhedral algebra. This test can be performed for each \ndependence in the program. If no dependences are violated, the data shackle is legal. 5.2 General View \nof Legal Data Shackles The formulation of the general problem of testing for legality of a data shackle \nbecomes simpler if we first generalize the notion of blocking data. A data blocking, such as the one \nshown in Figure 4, can be viewed simply as a map that assigns co-ordinates in some new space to every \ndata element in the ar-ray. For example, if the block size in this figure is 25 x 25, array element (ar,as) \nis mapped to the co-ordinate ((ai diu 25) + 1, (as div 25) + 1) in a new two- dimensional space. Note \nthat this map is not one-to-one. The bottom part of Figure 9 shows such a map pictorially. The new space \nis totally ordered under lex- icographic ordering. The data shackle can be viewed as traversing the remapped \ndata in lexicographic order in the new co-ordinates; when it visits a point in the new space, all statement \ninstances mapped to that point are performed. Therefore, a data shackle can be viewed as a func- tion \nM that maps statement instances to a totally or-dered set (V, 4). For the blocking shown in Figure 9, \nC: @,I) + A maps statement instances to elements of array A through data-centric references, and T:A \n-+ V maps array elements to block co-ordinates. Concisely, M = ToC. Given a function M: (S , I) -+ (V, \n41, the transformed code is obtained by traversing V in increasing order, and for each element v E V,executing \nthe statement instances M- (v) in program order in the original pro- gram. Theorem 1 A map M:(S,I) -(V,+) \ngenerate3 legal code if the following condition ia aatidjied for every pair of dependent statements Sl \nand S2. . Introduce vector3 of unknown3 A and a that rep- resent inatancea atatementa Sl and S2 of dependent \n  mpectively.  j$py - Original Datawith new co-ordinates Dar0 Figure 9: Testing for Legality . Formulate \nthe inequalities that must be satisfied for a dependence to e&#38;at from instance u of state-ment Sl \nto instance s of statement S2. This is standard 1251. . Formulate the predicate M (S2 ,s) +M (Sl ,u). \nThe conjunction of these condition8 does not have an integer solution. Proof: Obvious, hence omitted. \nCl 5.3 Discussion Viewing blocking as a remapping of data co-ordinates simplifies the development of \nthe legality test. This remapping is merely an abstract mathematical device to enforce a desired order \nof traversal through the ar-ray; the physical array itself is not necessarily reshaped. For example, \nin the blocked matrix multiplication code in Figure 6, array C need not be laid out in block or-der to \nobtain the benefits of blocking this array. This is similar to the situation in BLASILAPACK where it \nis assumed that the FORTRAN column-major order is used to store arrays. Of course, nothing prevents us \nfrom reshaping the physical data array if the cost of con- verting back and forth from a standard representation \nis tolerable. Physical data reshaping has been explored by other researchers Ill, 31. Upto this point, \nwe have assumed that every state- ment in the program contains a reference to the array being blocked \nby the data shackle. Although this as-sumption in valid for kernels like matrix multiplication and Cholesky \nfactorization, it is obviously not true in general programs. Our current approach to this prob- lem is \nnaive but simple. If a statement does not con-tain a reference to the array being blocked by the data \nshackle, we simply add a dummy reference to that ar-ray (such as + O*B[I,Jl) to the right hand side of \nthe statement. The dummy reference is of course irrelevant for dependence analysis, and serves only to \ndetermine which instances of this statement are performed when elements of B are touched by the data \nshackle. The pre- cise expression used in the dummy reference is irrelevant for correctness, but a data \nshackle that is illegal for one choice of this expression may be legal if some other ex- pression is \nused (since that changes the order in which the statement instances are performed). This is clearly an \nissue that we need to revisit in the future, and we plan to use tools we developed for automatic data \nalign- ment to address this problem more carefully [S]. 6 Products of shackles We now show that there \nis a natural notion of taking the Cartesian product of a set of shackles. The motivation for this operation \ncomes from the matrix multiplication code of Figure 6, in which an entire block row of A is multiplied \nwith a block of column of B to produce a block of C. The order in which the iterations of this compu- \ntation are done is left unspecified by the data shackle. The shackle on reference C [I, J] constrains \nboth I and J, but leaves K unconstrained; therefore, the references A [I, Kl and B [K , Jl can touch \nan unbounded amount of data in arbitrary ways during the execution of the iter- ation shackled to a block \nof C [I, Jl ). Instead of C, we i can b ock A or B, but this still results in unconstrained references \nto the other two arrays. To get LAPACK- style blocked matrix multiplication, we need to block all three \narrays. We show that this effect can be achieved by taking Cartesian products. Informally, the notion \nof taking the Cartesian prod- uct of two shackles can be viewed as follows. The first shackle partitions \nthe statement instances of the origi- nal program, and imposes an order on these partitions. However, \nit does not mandate an order in which the statement instances in a given partition should be per- formed. \nThe second shackle refines each of these parti- tions separately into smaller, ordered partitions, with-out \nreordering statement instances across different par- titions of the first shackle. In other words, if \ntwo state- ment instances are ordered by the first shackle, they are not reordered by the second shackle. \nThe notion of a binary Cartesian product can be extended the usual way to an nary Cartesian product; \neach extra factor in the Cartesian product gives us finer control over the granularity of data accesses. \nA formal definition of the Cartesian product of data shackles is the following. Recall from the discussion \nin Section 5 that a data shackle for a program P can be viewed as a map M:(S ,I) + V, whose domain is \nthe set of statement instances and whose range is a totally ordered set. Definition 2 FOT any program \nP, let  Ml :(S,I)-+V1 Mz:(S,I)-+V2 1 be two data shackles. The Cartesian product Ml x I$ of these shackles \nis defined as the map whose domain U the set of statement instancea, whose range is the Cartesian product \nVIx V2 and whose values are defined as follows: for any statement instance (S,iJ, (MI x M2)(s,i) = < \nM~(S,i),h(s,i) > The product domain Vl x V2 of two totally or- dered sets is itself a totally ordered \nset under stan-dard lexicographic order. Therefore, the code gen-eration strategy and associated legality \ncondition are identical to those in Section 5. It is easy to see that for each point 3 x y in the product \ndomain Vl x V2, we perform the statement instances in the set (MI x Ma)- (q,uz) = MI- (VI) nMz- (vz). \n In the implementation, each term ir&#38;r nary Carte- sian product contributes a guard around each state-ment. \nThe conjunction of these guards determines which statement instances are performed at each step of execution. \nTherefore, these guards still consist of con- juncts of afline constraints. As with single data shack- \nles, the guards can be simplified using any polyhedral algebra tool. Note that the product of two shackles \nis always legal if the two shackles are legal by themselves. However, a product Ml x Ms can be legal \neven if M2by itself is illegal. This is analogous to the situation in loop nests where a loop nest may \nbe legal even if there is an inner loop that cannot be moved to the outermost position; the outer loop \nin the loop nest carries the dependence that causes difficulty for the inner loop. 6.1 Examples In matrix \nmultiplication, it is easy to see that shackling any of the three references (CCI,Jl ,ACI,Kl ,BCK,JI) \nto the appropriate blocked array is legal. Therefore, all Cartesian products of these shackles are also \nlegal. The Cartesian product MC x MA of the C and A shackles produces the code in Figure 3. It is interesting \nto note that further shackling with the B shackle (that is the product MC x MAx MB)does not change the \ncode that is produced. This is because shackling C [I, Jl to the blocks of C and shackling ACI ,Kl to \nblocks of A imposes constraints on the reference B[K, Jl as well. A similar effect can be achieved by \nshackling the references C [I, Jl and B[K,Jl, or AfI,Kl and BCK,Jl. A more interesting example is the \nCholesky code. In Figure l(ii), it is easy to verify that there are six ways to shackle references in \nthe source program to blocks of the matrix (choosing A[J, 31 from statement Sl, either A[I,Jl or A[J,JI \nfrom statement S2 and either A[L,Kl, AIL, 53 or ACK, ~1 from statement S3). Of these, only two are legal: \nchoosing ACJ,Jl from Sl, ACI,Jl from S2 and A[L,Kl from S3, or choosing A[J, Jl from Sl, A [J , J] from \nS2 and A CL, Jl from S3. The first shackle chooses references that write to the block, while the sec- \nond shackle chooses references that read from the block. Since both these shackles are legal, their Cartesian \nprod- uct (in either order) is legal. It can be shown that one order gives a fully-blocked left-looking \nCholesky, identi- cal to the blocked Cholesky algorithm in [15], while the other order gives a fully-blocked \nright-looking Cholesky. 6.2 Discussion Taking the Cartesian product of data shackles gives us finer \ncontrol over data accesses in the blocked code. As discussed earlier, shackling just one reference in \nmatrix multiplication (say CD, 51) does not constrain all the data accesses. On the other hand, shackling \nall three references in this code is over-kill since shackling any two references constraints the third \nautomatically. Taking a larger Cartesian product than is necessary does not affect the correctness of \nthe code, but it introduces un-necessary loops into the resulting code which must be optimized away by \nthe code generation process to get good code. The following obvious result is useful to de- termine how \nfar to carry the process of taking Cartesian products. We assume that all array access functions are \nlinear functions of loop variables (if the functions are afEne, we drop the constant terms); if so, they \ncan be written as F*l where F is the data access matrix [17] and L is the vector of iteration space variables \nof loops surrounding this data reference. Theorem 2 For a given statement S, let Fl,. . . , F,, be the \naccess matrices for the shackled data references in this statement. Let F,,+I be the access matti for \nan un- shackled reference in S. Assume that the data accessed by the shackled references are bowaded \nby block size pa-rameters. Then the data accessed by F,,+l is bounded by block size parameters iff every \nrow of F,,+I U spanned by the rows of Fl,... ,F,,. Stronger versions of this result can be proved, but \nit suffices for our purpose in this paper. For ex-ample, the access matrix for the reference C[I, Jl \nis 1 0 0 * Shackling this reference does not bound0 1 0 [ 1 the data accessed by row [ 0 0 1 ] of the \naccess ma- 0 0 1 trix o 1 o of reference B [K, 51. However, taking [ I the Cartesian product of this \nshackle with the shackle obtained from A[I,Kl constrains the data accessed by B [K, 51, because all rows \nof the corresponding access matrix are spanned by the set of rows from the access matrices of C [I, Jl \nand A[I ,Kl . Although the problem of generating good shackles automatically is beyond the scope of this \npaper, Carte-sian product is obviously viewed as a way of generat- ing new shackles from old ones. The \ndiscussion in this section provides some hints for generating good data shackles. How are the data-centric \nreferences chosen? For each statement, the data-centric references should be chosen such that there are \nno remaining uncon-strained references. How big should the Cartesian products be? If there is no statement \nleft which has an unconstrained reference, then there is no benefit to be obtained from extending the \nproduct. What is a correct choice for orientation of cutting planes P To a first order of approximation, \nthe ori- entation of the cutting planes is irrelevant as far as performance is concerned, provided the \nblocks have the same volume. Of course, orientation is important for legality of the data shackle. These \nmatters and related ones are currently being investigated and will be addressed in the forthcoming thesis \nof the first author of the paper.  6.3 Multi-level Blocking Cartesian products can be used in an interesting \nmanner to block programs for multiple levels of memory hierar-chy. For lack of space, we explain only \nthe high level do tl = 1, (n+63)/64 do t2 = 1, (n+63)/64 do t3 = 1, (n+63)/64 do t7 = 8*tl-7, min(8*tl,(n+7)/8) \ndo t8 = 8+t2-7, min(8*t2,(n+7)/8) do t9 = 8*t3-7, min(B*t3,(n+7)/8) do t13 = 8*t9-7, min(n,8*t9) do t14 \n= 8*t8-7, min(n,8*t8) do t15 = 8*t7-7, min(8*t7,n) C[ti3,t141+= A[t13,t151*B[t15, t141 Figure 10: Matrix \nmultiply blocked for two levels of memory hierarchy idea here. For a multi-level memory hierarchy, we \ngen- erate a Cartesian product of products of shackles where each factor in the outer Cartesian product \ndetermines blocking for one level of the memory hierarchy. The first term in the outer Cartesian product, \ncorre- sponds to blocking for the slowest (and largest) level of the memory hierarchy and corresponds \nto largest block size. Subsequent terms correspond to blocking for faster (and usually smaller) levels \nof the memory hierarchy. We have applied this idea to obtain multiply blocked versions of our running \nexamples in a straightforward fashion. It is unclear to us that tiling can be general- ized to multiple \nlevels of memory hierarchy in such a straightforward manner. Figure 10 demonstrates this idea for matrix \nmulti-plication. The outer Cartesian product for this example has two factors: the first factor is itself \na product of two shackles (on C[I, J] and A[I,Kl with block sizes of 64), and the second factor is also \na product of two shack- les (once again, on C [I, Jl and A CI , Kl , but block sizes of 8). As can be \nseen from the code, the first term of the outer Cartesian product performs a 64-by-64 matrix multiplication, \nwhich is broken down into several 8-by-8 matrix multiplications by the second term in this prod- uct. \nBy choosing smaller inner blocks (like 2-by-2) and unrolling the resulting loops, we can block for registers. \n  Performance We present performance results on a thin node of the IBM SP-2 for the following applications: \nordinary and banded Cholesky factorizations, QR factorization, the AD1 kernel and the GMTRY benchmark \nfrom the NAS suite. All compiler generated codes were compiled on the SP-2 using xlf -03. Figure 11 shows \nthe performance of Cholesky factor- ization. The lines labeled Input right-looking code show the performance \nof the right-looking Cholesky factor-ization code in Figure l(ii). This code runs at roughly 8 MFlops. \nThe lines labeled Compiler generated code show the performance of the fully blocked left-looking Cholesky \ncode produced by the Cartesian product of data shackles discussed in Section 6. While there is a dramatic \nimprovement in performance from the initial code, this blocked code still does not get close to peak \nperformance because the compiler back-end does not perform necessary optimizations like scalar replacement \nin innermost loops. A large portion of execution time -LAP*lnfWh8BLAS Mathmplad MO.0 -mI QtybyDGEW ----carpl*rpnM9dub4o-ti \n --- lnpnligM.bc&#38;rQccdl 3 1M.O 100.0 I ___.___._----_   P 50.0d,__._________..---.---.. : ~---_-------------------- \na?, 0 m.0 300.0 100.0 sm.0 em0 SiUd-d.3OMWW Figure 11: Cholesky factorization on the IBM SP-2 is spent \nin a few lines of code which implement ma trix multiplication, but which are optimized poorly by the \nIBM compiler. Replacing these lines manually by a call to the ESSL BLAS-3 matrix multiplication rou-tine \nimproves performance considerably, as is shown by the lines labeled Muttis Multiply replaced by DGEMM. \nFinally, the line labeled LAPACK with native BLAS is the performance of the Cholesky factorization routine \nin LAPACK running on the native BLAS routines in ESSL. The MFlops graph provides truth in advertis- ing \n-although the execution times of LAPACK and compiler-generated code with DGEMM are compara-ble, LAPACK \nachieves higher Mflops. This is because we replaced only one of several matrix multiplications in the \nblocked code by a call to DGEMM. On the posi- tive side, these results, coupled with careful analysis \nof the compiler-generated code, show that the compiler- generated code has the right block structure. \nWhat re-mains is to make the compilation of inner loops (by unrolling inner loops, prefetching data and \ndoing scalar replacement [l, 251) more effective in the IBM compiler. Figure 12 shows the performance \nof QR factorization using Householder reflections. The input code has poor performance, and it is improved \nsomewhat by block-ing. The blocked code was generated by blocking only columns of the matrix, since dependences \nprevent com-plete two-dimensional blocking of the array being fac-tored. As in the case of Cholesky factorization, \nre-placing loops that perform matrix multiplication with calls to DGEMM results in significant improvement, \nas shown by the line labeled Matriz Multiply replaced by DGEMM. This code beats the fully blocked code \nin LAPACK for matrices smaller than 200-by-200. The compiler-generated code uses the same algorithm as \nthe z!om - WAa(xlhrulk.BU5 , -u*rnm*plyr~.cadby~E~~ ----~.pnwMd~abmdm ---,rp*Ml-mc&#38; Figure 12: QR \nfactorization on the IBM SP-2 do i = 2, IL do k = 1, 11 Si: X(i,k) -= XC-l,k)*A(i,k)/B(i-l,kI enddo \ndo k = 1, n S2: B(i, k) -= A(i,k)*A(i,k)/B(i-1,k) enddo enddo (i) Inptrt code do tl = 1, II do t2 = \n1, n-l Sl: X(t2+l,tl)-=X(t2,tl)*A(t2+l,tl)/B(t2,tl) S2: B(t2+l,tl)-=A(t2+i,tl)*A(t2+l,tl)/B(t2,tl~ enddo \nenddo (ii) Transformed Code Figure 14: Effect of fusion + interchange on AD1 150.0 -LAPM(-*8do,aIchwmd \n-wkQm9mdoob(bnd~l0) ----conpi*r-ad-* ---LAmcueandd f ~00.0. ~~ -,~2?r!?TE~L~  -, I 1 I 1 I 50.0- __________..___-,,,__.._..._.__ \n. ..k... ~ 7-l pointwise algorithm for this problem; the LAPACK code on the other hand uses domain-specific \ninforma-,.,m1w.o zQo.0 300.0 400.0 sm.0 m.0 sk.dMhddc&#38;h@xN, tion about the associativity of Householder \nreflections to generate a fully-blocked version of this algorithm [9]. Figure 15: Banded Cholesky factorization \non the SP-2  Samok ADI loom on thin no&#38; of SP2 w- speeded up by a factor of 2. 1.0 Figure 14(i) \nshows the AD1 kernel used by McKinley et al in their study of locality improving transforma-tions [18]. \nThis code was produced from FORTRAN-90 by a scalarizer. The traditional iteration-centric ap-proach to \nobtain good locality in this code is to fuse the two k loops first, after which the outer i loop and \nthe fused k loop are interchanged to obtain unit-stride accesses in the inner loop. The resulting code \nis shown T-Co&#38; in Figure 14(ii). In our approach, this final code is ob- 1.a12tained by simply choosing \nB(i-1,k) as the data-centric nr qmd * c- laluti GmtrJ knchmuk on thin muk ofsP2  I.0 r-7 Figure 13: \n(i) Gmtry &#38; (ii) AD1 benchmarks on the IBM SP-2 Figure 13(i) shows the results of data shackling \nfor the Gmtry kernel which is a SPEC benchmark kernel from Dnasa7. This code performs Gaussian elimination \nacross rows, without pivoting. Data shackling blocked the array in both dimensions, and produced code \nsimilar to what we obtained in Cholesky factorization. As can be seen in this figure, Gaussian elimination \nitself was speeded up by a factor of 3; the entire benchmark was reference in both Sl and S2 and blocking \nB into blocks of size 1x1. When an element of B is touched, all statement instances from both loop nests \nthat touch this element must be performed; this achieves the effect of loop jam- ming. Traversing the \nblocks in storage order achieves perfect spatial locality, which achieves the effect of loop interchange \nafter jamming. As shown in Figure 13(ii), the transformed code runs 8.9 times faster than the in- put \ncode, when n is 1000. Since shackling takes no position on how the remapped data is stored, the techniques \ndescribed in Section 4 can be used to generate code even when the underlying data structure is reshaped. \nA good exam-ple of this is banded Cholesky factorization 1151. The banded Cholesky factorization in LAPACK \nis essen-tially the same as regular Cholesky factorization with two caveats: (i) only those statement \ninstances are per- formed which touch data within a band of the input ma- trix, and (ii) only the bands \nin the matrix are stored (in column order), rather than the entire input matrix. In our framework, the \ninitial point code is regular Cholesky factorization restricted to accessing data in the band, and the \ndata shackling we used with regular Cholesky factorization is applied to this restricted code. To obtain \nthe blocked code that walks over a data structure that stores only the bands, a data transformation is \napplied to the compiler generated code as a post processing step. As seen in Figure 15, the compiler \ngenerated code actu- ally outperforms LAPACK for small band sizes. As the band size increases however, \nLAPACK performs much better than the compiler generated code. This is be- cause, for large band sizes, \nLAPACK starts reaping the benefits of level 3 BLAS operations, whereas the xlf back-end (on the IBM SP-2) \nis once again not able to perform equivalent optimization8 for loops in our com- piler generated code \nwhich implement matrix multiply operations. 8 Ongoing Work There are several unresolved issues with \nour approach. In this section, we discuss these open issues and suggest plausible solutions when possible. \n As discussed in Section 4, a data shackle has three components -sets of cutting planes, shackled references \nand order of enumeration over blocks. In this paper, we have assumed that the specification of these \ncomponents is given to the compiler. Automating our data-centric approach fully requires the compiler \nto determine these components. We are working on this problem, basing our approach on the following observations. \nOne approach is to implement a search method that enumerates over plausible data shackles, evaluates \neach one and picks the best. When there are multiple data shackles that are legal for a program, we need \na way to determine the best one. This requires accurate cost models for the memory hierarchy, such as \nthe ones de- veloped by other researchers in this area [18, 221. If the search space becomes large, heuristics \nmay be useful to cut down the size of the search. Theorem 2 and the subsequent discussion suggests that \nto a first order of approximation, the orientation of cutting planes has little impact on performance, \nand can be selected to satisfy legality considerations alone. For all the bench- marks in the paper, \nwe found that walking over the blocked array in top to bottom, left to right order was adequate. This \norder of enumerating blocks has great appeal because it is simple and because we believe that this is \nthe natural order used by programmers. In gen- eral, of course, this order of traversing blocks may not \nbe legal (triangular back-solve is an example), but we believe that in most of those cases, traversing \nthe blocks bottom to top or right to left will be legal . Another assumption in our approach so far is \nthat a shackled ref- erence makes a single sweep through the corresponding array. This is adequate for \nproblems like matrix fac-torizations in which there is a definite direction to the This ia eimilar to \nloop reversal. underlying data flow of the algorithm, but it is obviously not adequate for problems \nlike relaxation codes in which an array element is eventually affected by every other element. To solve \nthis problem, we must make multi- ple passes over the blocked array. One possibility is the following: \nrather than perform all shackled statement instances when we touch a block, we can perform only those \ninstances for which dependence8 have been satis- fied. The array is traversed repeatedly till all instances \nare performed. Determination of good block sizes can also be tricky, especially for a multi-level memory \nhi-erarchy [lo]. This problem arises even in handwritten code -in this context, library writers are exploring \nthe use of training sets to help library code determine good block sizes [13]. We can adopt this solution \nif it proves to be successful. We have presented data shackling 8s an alternative restructuring technology \nthat avoids some of the prob- lems of current control-centric approaches. However, it is unclear to us \nwhether our approach can fully sub-sume loop transformation techniques. In the event that both these \napproaches are required for program restruc-turing, an important open question is to determine the interaction \nbetween them. Finally, we note that there are programs for which handwritten blocked codes exploit algebraic \nproperties of matrices. QR-factorization using Householder reflec-tions is an example [15]. It is unclear \nto us whether a compiler could or even should attempt to restructure programs using this kind of domain-specific \ninforma-tion. It is likely that the most plausible scenario is compiler blocking augmented with programmer \ndirec-tives for blocking such codes [9]. Acknowledgments: We would like to thank Rob Schreiber, Charlie \nvan Loan, Vladimir Kotlyar, Paul Feautrier and Sanjay Rajopadhye for stimulating dis-cussions on block \nmatrix algorithms and restructuring compilers. This paper was much improved by the feed- back we received \nfrom an anonymous good shepherd on the PLDI committee. References PI Ramesh C. Agarwal and Fred G. Gustavson. \nAlgo-rithm and Architecture Aspect8 of Producing ESSL BLAS on POWER&#38;. PI E. Anderson, 2. Bai, C. \nBischof, J. Dem-mel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, \nand D. Sorensen, editors. LAPACK Uaera Guide. Sec-ond Edition. SIAM, Philadelphia, 1995. Jennifer Anderson, \nSaman Amarsinghe, and Mon- ica Lam. Data and computation transformations for multiprocessors. In ACM \nSymposium on Prin-ciples and Practice of Parallel Programming, Jun 1995. 131 U. Banerjee. Unimodular \ntransformations of dou- ble loops. In Proceeding3 of the Workshop on Ad-uances in Languages and Compiler8 \nfor Parallel Processing, pages 192-219, August 1990. 141 David Bau, Induprakas Kodukula, Vladimir Kotl-yar, \nKeshav Pingali, and Paul Stodghil. Solving alignment using elementary linear algebra. In Pro- ceedings \nof the 7th LCPC Workshop, August 1994. Also available as Cornell Computer Science Dept. tech report TR95-1478. \nI51 Pierre Boulet, Alain Darte, Tanguy Risset, and Yves Robert. (Pen)-ultimate tiling? In INTE-GRATION, \nthe VLSI Journal, volume 17, pages 33-51. 1994. PI Steve Carr and K. Kennedy. Compiler blockability of \nnumerical algorithms. In Supercomputing, 1992. [71 Steve Carr and R. B. Lehoucq. Compiler blockabil- \nity of dense matrix factorizations. Technical report, Argonne National Laboratory, Ott 1996. PI Steven \nCarr and R. B. Lehoucq. A compiler-blockable algorithm for QR decomposition, 1994. PI L. Carter, J. Ferrante, \nand S. Flynn Hummel. Hi- PO1 erarchical tiling for improved superscalar perfor-mance. In International \nParallel Proceaaing Sym- poainm, April 1995. Michael Ciemiak and Wei Li. Unifying data and control transformations \nfor distributed shared memory machines. In SIGPLAN 1995 conference on Programming Languages Deaign and \nImplemen- tation, Jun 1995. Stephanie Coleman and Kathryn S. McKinley. Tile size selection using cache \norganization and data layout. In David W. Wall, editor, ACM SIGPLAN 95 Conference on Programming Language \nDeaign and Implementation (PLDI), volume 30(6) of ACM SIGPLAN Notices, pages 279-290, New York, NY, USA, \nJune 1995. ACM Press. WI Jim Demmel. Personal communication, Sep 1996. I131 Jack Dongarra and Robert \nSchreiber. Automatic blocking of nested loops. Technical Report UT-CS- 90-108, Department of Computer \nScience, Univer- sity of Tennessee, May 1990. WI Gene Golub and Charles Van Loan. Matriz Com-putationa. \nThe Johns Hopkins University Press, 1996. 1151 Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. \nThe cache performance and op- timizations of blocked algorithms. In Proceeding8 of the Fourth International \nConference on Archi-tectural Support for Programming Languages and Operating Systems, pages 63-74, Santa \nClara, Cal- ifornia, April 8-11, 1991. ACM SIGARCH, SIG-PLAN, SIGOPS, and the IEEE Computer Society. \nWI W. Li and K. Pingali. Access Normalization: Loop restructuring for NUMA compilers. ACM Trana-action8 \non Computer Systems, 1993. 1171 Kathryn S. McKinley, Steve Carr, and Chau-Wen P81 Tseng. Improving data \nlocality with loop transfor- mations. In ACM Transaction8 on Programming Languages and Syatema, volume \n18, pages 424-453. July 1996. W. Pugh. A practical algorithm for exact array de- pendency analysis. Comm. \nof the ACM, 35(8):102, August 1992. PI J. Ramanujam and P. Sadayappan. Tiling mul-tidimensional iteration \nspaces for multicomputers. Journal of Parallel and Distributed Computing, 16(2):108-120, October 1992. \nWI A. Rogers and K. Pingali. Process decomposition through locality of reference. In SIGPLAN89 con-ference \non Programming Languages, Design and Implemenlation, Jun 1989.  I211 Vivek Sarkar. Automatic selection \nof high order PA transformations in the IBM ASTI optimizer. Tech-nical Report ADTI-96-004, Application \nDevelop-ment Technology Institute, IBM Software Solu-tions Division, July 1996. Submitted to special \nis- sue of IBM Journal of Research and Development. M.E. Wolf and M.S. Lam. A data locality optimiz- \n1231 ing algorithm. In SIGPLAN 1991 conference on Programming Languages Design and Implementa- tion, \nJun 1991. M. Wolfe. Iteration space tiling for memory hierar-chies. In Third SIAM Conference on Parallel \nPro-ceasing for Scientific Computing, December 1987. 1241 M. Wolfe. High Performance Compilers for Paral-lel \nComputing. Addison-Wesley Publishing Com-pany, 1995. [251 Permission to make digital/hard copy of part \nor all this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for profit or commercial advan-tage, the copyright notice, the title of the publication \nand its date appear. and notice is given that copying is by permission of ACM, inc. To copy otherwise, \nto republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or \na fee, PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/97/0006...$3.50  \n\t\t\t", "proc_id": "258915", "abstract": "We present a simple and novel framework for generating blocked codes for high-performance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our data-centric transformations permit a more direct solution to the problem of enhancing data locality than current control-centric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers.", "authors": [{"name": "Induprakas Kodukula", "author_profile_id": "81100032409", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP39024093", "email_address": "", "orcid_id": ""}, {"name": "Nawaaz Ahmed", "author_profile_id": "81100287480", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP42050912", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258946", "year": "1997", "article_id": "258946", "conference": "PLDI", "title": "Data-centric multi-level blocking", "url": "http://dl.acm.org/citation.cfm?id=258946"}