{"article_publication_date": "05-01-1997", "fulltext": "\n Automatic Inline Allocation of Objects Julian Dolby dolbyacs.uiuc.edu Concurrent Systems Architecture \nGroup Department of Computer Science University of Illinois 1304 West Springfield Avenue Urbana, IL 61801 \n Abstract Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies \nprogramming by providing a consistent, abstract model of object behavior. But direct implementations \nintroduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, \nfunction specialization); in this paper, we introduce o6ject inlining, an optimization that automati-cally \ninline allocates objects within containers (as is done by hand in C++) within a uniform model. We present \nour technique, which includes novel program analyses that track how inlinable objects are used throughout \nthe program. We evaluated object inlining on several object-oriented bench-marks. It produces performance \nup to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined \ncodes. 1 introduction Traditional object-oriented languages (e.g. SmallTalk [13]) sport a simple, uniform, \nabstract programming model; all objects are accessed via references and all calls are dynamic message \nsends. This simplifies programming, as the pro- grammer faces a single model of object behavior, and \nbe- cause different portions of programs can be isolated behind opaque interfaces. This abstract model, \nhowever, introduces overhead: even simple field accesses become dynamic dis-patches, indirection through \nreferences is required to ac-cesses every object, and individual functions shrink [5]. Hy- brid languages \nlike C++ (111 provide multitudinous low-level features (e.g virtual vs non-virtual functions, inline \ndirectives, and explicit stack, inline or heap allocation) so programmers can manually reduce overhead \nby removing unused abstraction. However, the original notion of a clean, simple seman-tics is m-emerging \nin recent object-oriented languages (e.g. Java [27], NewtonScript [3], Cecil [7]). This leaves eliding \nabstraction overhead to aggressive implementations. Dy-namic dispatches are optimized statically by type \ninference Permission to make digital/hard copy of part or all this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for profit or commercial advan-tage, \nthe copyright notice, the title of the publication and its date appear, and notice is given that copying \nis by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute \nto lists, requires prior specific permission and/or a fee. PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/97/0006...$3.50 \n[l, 6, 21, 231, dynamically by inline caching [16] or with hybrid approaches like type feedback [17]. \nStatic or hybrid type analysis is combined with method specialization [lo, 241 to allow inlining, removing \nthe small functions common in object-oriented code. The combination of static type analysis and specializ* \ntion also permits inline allocation of objects within contain- ers, thereby eliminating many object references \nand reduc- ing object allocation. We present object inlining, the fhzt fully automatic optimization for \ndoing inline allocation in an object-oriented language. We exploit the power of anal- ysis and cloning \nthat can handle data flow through object state [23, 241 to inline allocate child objects even for poly- \nmorphic containers and to systematically exploit such allo- cation wherever the child objects are used. \nclass Point ( x-pas. J-pas; 1; class Rectangle (. lover-left, upper-right; 1; Figure 1: A Rectangle class \n To make the problem concrete, consider the example in Figure 1 . A direct implementation is shown in \nFigure 2(a); inlined allocation, as in C++, would produce Figure 2(b). Accesses to coordinates of rectangles \nare cheaper in the in- lined implementation, requiring one dereference fewer. Fur- thermore, cache performance \nis likely to benefit. Inline al- location also reduces object allocation costs, since the sub- objects \nare allocated with the container. This especially benefits languages like Java where objects have conceptu- \nally infinite extent, necessitating heap allocation in general. Note that methods such as Point: :area \nin Figure 4 could cache fields with inline allocation of Points since p and this could not be aliased \nwhen the method is called from Rectangle: : area. Inline allocation creates a correspondence between \ncon-tainer and containee that our analysis permits to be ex-ploited as alias information. An example \nuse of this is The syntax used in thin paper is reminiscent of C++, but we leave out explicit types to \nbetter illustrate our dynamic model (b) Figure 2: Two Rectangle Representations caching object fields \nin registers: more precise aliasing in- formation concomitantly enables more thoroughgoing regis-ter \nallocation of object state as noted above. Furthermore, treating inline allocation as a global program \ntransforma-tion, as we do, allows customized data layouts, such as par- allel arrays for an array of \nobjects. We evaluated object inlining on several object-oriented benchmarks, and found it making programs \nup to three times as fast as without inline allocation, and matching code with inline allocation done \nby hand. In subsequent sections, we first introduce our running example (Section 2) and provide background \n(Section 3). We next cover our analysis (Section 4) and program trans-formation (Section 5). We then \nevaluate our optimization (Section 6), discuss related work (Section 7) and conclude (Section 8). We \nconsider future directions in Section 9. Example Program Our examples of inline object allocation focus \non Points and Rectangles (see Figure 1); the code in this section will be used in our examples hereafter. \nTo illustrate the complexi- ties involved in automatic inline allocation, we introduce a subclass of \nRectangle (Figure 3). We will use the follow- ing methods while expounding various aspects of our opti- \nmization; significant methods are in Figure 4, and the main program is in Figure 5. class Parallelogram \n: Rectangle C upper-left; 1; Figure 3: Rectangle Subclass Figure 4 suggests complications of automatic \ninline allo- cation that require global program transformations: when the Point : : area method is called \nfrom Rectangle : : area, it must be specialized because, instead of two point objects, it will be called \nwith one rectangle with four inlined fields. Producing this information requires inter-procedural data-flow \nanalysis (Sections 3.2.1 and 4), and exploiting it is a global transformation. More difficulties arise \nwhen inlined objects are put into unrelated containers: we must call appropriate specialized methods \nwhen such objects are accessed via the unrelated container. For example, in Figure 5, the compiler must \ndetermine that the call to head in dorectangle returns a Point inlined into a Rectangle so that the appropriate \nspecialized version of abs can be called. This requires data- flow analysis that can flow properties \nthrough fields (Sec- Point: :area(p) I return absfx-pos -P.x-Pas) * abs (y-pos -p.y-pod; 1 Point: :absO \nC return sqrt (I-Pos*x-Pas + y-pos*y-pas) ; 1 Rectangle: : area0 c return lover,left.areafupper-right); \nI Figure 4: Methods tion 3.2.1). Our Concert [8] analysis framework is, to our knowledge, the only one \nthat can do this. do-rectangle (11, ur) i r = new Rectanglefll. ur); tout << r.area(); 11 = new List(r.loaer-left, \nnil); 12 = nev List(r.upper,right, nil); tout C< head(H) .absf) ; tout << head(12).absO; I main0 C pi \n= new Point(l.O.2.0); P2 = new Point(3.0,4.0); do-rectangle(pl. ~2); p3 = new Point3Dfl.0.2.0.3.0); p4 \n= new Point3D(4.0,5.0,6.0); do-rectangle (~3, ~4) ; 1 Figure 5: Main Program The main program (Figure \n5) illustrates our other anal- ysis diiculty. Inlining the points involves copying the fields from the \nconstructor arguments into the inlined Point fields within the Rectangle. Such copying would become problem- \natic were do-rectangle to be called with one aliased Point as both arguments, as it would change aliasing \nrelationships. In order to do inline allocation, we must ensure that this has not happened, that is, \naliasing relations are not changed so inlining is safe. In sum, our example illustrates the two challenges \nwe face: 1) finding and specializing uses of inlined objects and 2) ensuring that inline allocation does \nnot change aliasing relationships. 3 Background In this section, we discuss manual inline allocation \nin the context of C++. Then we present the compiler framework for our optimizations, focusing upon the \nsensitivity that it permits: its ability to handle data flow through object fields is vital to our analysis. \n 3.1 Manual lnline Allocation C++ provides a direct syntactic mechanism for specifying whether a given \nobject slot should be inlined: the type sys- tem explicitly denotes storage allocation. However, the \ntype system denotes other things too, so specifying an object as inline allocated (e.g. declaring lower-left \nto be of type point rather than point *) also determines other behav- iors (e.g. changes the meaning \nof assignment). Hence inline allocation is not simply a performance optimization. This can make specifying \ninline allocation awkward-for instance a list element conceptually contains a reference to its data- \nor even impossible due to semantic changes. Finally, this mechanism leaves the burden of deciding what \nto inline on the programmer. On the other hand, because we want to preserve a uniform model and our analysis \nopens up other optimization opportunities, our inline allocation is done au- tomatically by the compiler. \n3.2 The Concert Compiler This works was done in the Concert [8] compiler, so sub-sequent discussion of \nour optimization relies heavily on the framework of that compiler. Therefore, a brief overview is given \nof the two portions that we use: the analysis and cloning modules. 3.2.1 The Analysis Framework The Concert \ncompiler has a global analysis framework (see [23,22] for more detail) that does context sensitive flow \nanal- ysis. Context sensitivity adapts, in a demand-driven man-ner, to program structure. A method contour \n(261 represents execution environments for a method, i.e. it is the unit of context sensitivity. Method \ncontours can discriminate arbi-trary dataflow properties of its caller and creator. An object contour \nrepresents method contours of the statements that created a given object. For method contours: caller \n-the calling statement and contour. This covers arguments, allowing discrimination based upon data-flow \nproperties of caller and its arguments. creator -the object contour representing self. This per- mits \na limited form of alias analysis based upon prop- erties of the creation point s method contours. Contours \nare created on demand: they are created when the analysis needs to distinguish some property. The origi- \nnal use of this framework was type inference, which creates contours to distinguish type information. \nMethod contours are created for different sets of argument types; for poly- morphic fields, different \nobject contours are built for the containing object to differentiate the types in the field. The analysis \nframework includes a mechanism for distinguishing object contours with respect to uses of objects. Figure \n6 illustrates type analysis on part of the pro-gram in Figures 4 and 5. In Figures 6 and 7, con-tours \nare signified by [<callers>, <creators>]. The first graph is after one pass of analysis; the calls to \ndolectangle have different argument types, so two con-tours are created to distinguish them; on the other \nhand, since both calls to Rectangle: :area have the same types for their arguments, just one contour \nis created embrac-ing both call sites. Within Rectangle : : area the type mnint.) lnll, ml] pl 5 new \nPoint; (1) pz = new Point; (2) do-mlsngk(pl, ~2); (3) p3 = new Pointm (4) p4 = new Point3D; (5) ~-~tw&#38;d~3, \np4); (6) %~~lyk(ll. ur) IWh nW r = new Rectmgqll, ur): (7) cout << rrmo; (II) Figure 6: Pass One of \nRectangle: : lower-left is ambiguous, so the demand-driven specialization comes into play. Since the \ntype con-fusion is due to a field, the system creates object contours representing each creation statement \nof Rectangle to dii tinguish the types of Rectangle: :louer-left. This results in Figure 7. Figure 7: \nPass Two 3.2.2 The Cloning Framework The Concert compiler includes a mechanism for cloning based upon \ndata-flow properties discovered by analysis [24], which works on the contours. The mechanism uses a generic \nfunction that determines, for two given contours, whether they are compatible. Cloning is done upon both \nmethods and classes. For each method in the program, its method contours are grouped into sets of compatible \ncontours, and a copy of the method is generated for each set. Caller information from the contours is \nused to reconstruct the call graph. The origi- nal use of this mechanism is to eliminate dynamic dispatches \nfrom the program, and so it marks contours as incompatible if they have different types for the target \nof any call in the method. For example, the method Rectangle: :area in Figure 7 has two contours to discriminate \nthe type of the louerlef t field of Rectangle. In the program, however, there is only one method, requiring \nthat lower-left. area(upper-right) be dynamically dispatched. The cloning mechanism marks these two contours \nas incompatible as their types for the field lower-left of Rectangle differ; then, Rectangle : : area0 \nis duplicated and lower-left. area(upper-right) can be stat- ically dispatched in each one since the \ntype of lower-left is precise. Note that cloning Rectangle: :area creates problems in do-rectangle: there \nare now two possible methods to call for Rectangle: :area. The cloning framework includes an iterative \nmechanism to split caller methods when cloning a callee creates a dynamic dispatch, which in this case \nsplits dolectangle as well.2 Cloning is also done on classes: classes are split based upon the object \ncontours. As with methods, cloning works by grouping the contours based upon some discriminator function. \nCloning a class does not necessarily require cloning all associated methods. 4 Object Mining Analysis \nAs Section 2 illustrated, automatic inline allocation requires two analyses: first, all uses of fields \nin inlined objects must be found precisely so that the appropriate inlined field can be used instead; \nwe call this use specialization. Second, assignments to the reference field being removed must be found, \nand we must verify that converting these to assign- ments to update the inlined fields is safe; we call \nthis assign- ment specialization. 4.1 Use Specialization To determine from which object fields any given \nvalue in the program may have come, we tag values that result from field accesses and propagate these \ntags through the inter- procedural data-flow graph. The basic idea is that values are tagged with the \nnames of any fields from which they may have come, and these tags are transitive on field accesses to \nobjects that themselves were the result of a field access. More formally, our data-flow analysis framework \ngives us the following properties of each value in the program: Backs(v) are the values from which data \nflows into v Creators(v) is the set of object contours of v. We also define tags with which to mark the \nresults of accesses to fields; these tags keep track of the fields from which a given value might have \ncome. Fields themselves are represented by special values that denote the contents of the field. Note \nthat MakeTag can built nested tags for accesses to nested objects. NoField is a special tag for values \nthat did not flow from field accesses MalceTag(f, tag) returns the tag representing values that came \nfrom field f from an object whose origin is repre- sented by tag. Head(tag) returns the last field in \nthe given tag, i.e. Head(MakeTag(f, tag)) = f. Tags(v) is the set of tags associated with the value v. \nThe situation gets even worse here because the dispatch of Rectangle: :arm depends on the type of a slot, \nwhich would make the call graph unrealisable if doxe3anglo were not split. We have three different transfer \nfunctions for our data-flow analysis, corresponding to three cases: object creations, instance variable \naccesses and other operations. Object cre-ations are places where new objects are allocated (i.e. new \nstatements) and their result values get the special NoField tag. v = new Object a Tags(w) t-{NoField} \nInstance variable accesses append the accessed field onto the existing tags of the object being accessed; \nthus v gets a tag representing all the fields accessed to obtain this value. v = 0.f --7 Tags(v) t MakeTag(f, \nt) U tETa940) where f is the special value for the given field. Other values get their tags by propagation \nfrom all their reaching de&#38;ri- tions in the inter-procedural graph; the use of the Creators of the \ntag prevents extraneous propagation that would oth-erwise occur across dynamic dispatches.  Tags(v) \nt uiEBaeks(v)  t t E Tags(i)h -II Creators(Head(t)) n Creators(u) Marks from different slots, or from \nno slot, can converge whenever data-flow paths do, and the analysis uses the con- tours provided by the \nframework to split such paths. As method contours are created on demand, the analysis must assert when \ntwo calls to the same method require different contours; this happens when two objects with different \ntags are supplied at different call sites to the same argument. That is, cl can be in the contour of \ncs only if  Vi Tags(Arg(cl, i)) C Tags(Arg(c2, i)) where Arg(c,i) represents the ith argument of call \nc. An example situation that requires splitting is the two calls to abs in do-rectangle; it is illustrated \nin Figure 8. Figure 8: A Call Confluence We also use creator sensitivity to disambiguate values with \ndifferent tags that flow into an object contour s state. This arises in the two List objects created \nin do-rectangle, and is illustrated in Figure 9. The object contour must be split into new ones, one \nfor each distinct tag amongst the definitions. Thus, the definitions of contour o must be partitioned \ninto contours o,, such that VffcFiclds(o)vuir ?+,. v .~&#38;,~k.(~.f) $1 f (30n Ui,Uj E BdCS(O,.f)) * \nTagS(U;) = TagS(Uj) UsesAfter(e,v) is the set of possible uses of u in Caller(e) Once the definitions \nhave been partitioned thus, the anal-ysis framework will split the object contour if possible. Figure \n9: Field Confluences This analysis marks each value with an approximation of from which fields it may \nhave come. We use this information to specialize the program to work on inline-allocated objects: a field \ncan be inline allocated only if this analysis is able to distinguish exactly where the given field is \nused. That is, the tags of the given field must not be confused with tags from any other field. 4.2 Assignment \nSpecialization Copying the contents of an inlined object into its container potentially alters aliasing \nrelations, and we must ensure that such copying is safe. A common case is an object is initial- ized \nand then assigned to an inlined slot and from then on is accessed via the container (this happens in \nFigure 5). To handle such cases, we analyze to determine method argu-ments that can be passed by value; \nif the argument to the field s mutator3 method may be passed in by value, we can copy it into the inlined \nfields safely. Our analysis is defined in terms of contours, which rep- resent method calls, edges which \nmap arguments between caller and callee, and uses, which are primitive operations, in particular calls \nand assignments. The basic idea behind this analysis is that objects may be passed by value if they have \nnot previously been stored and are not subsequently used. We first define some local operations upon \nspecific values and contours that form the groundwork for our anal- ysis: Caller(e) is the calling use \nof inter-procedural edge e. CaIIee(e) is the called contour of inter-procedural edge e. CaIl(u, v)) yields \ntrue if the use u of value v is a call. Edge(u) is the inter-procedural edge corresponding to use u, \nassuming u is a call. Uses(v) is the set of uses of a value u in its contour; these uses represent calls \nand primitive actions like setting a variable. UsesBefore(e,v) is the set of possible uses of u in Caller(e) \nbefore the edge e. 3ali access to field go thru acce~sor functions in our model after the edge e. Map(v,e) \ngives the argument value in the called contour corresponding to the value v passed across edge e. LocalCreation \nreturns true if u is the product of a new operation within its contour. DontStore(u,v) is true if the \nuse u does not store v in an instance variable or a global variable. Given these operations, we define \na NoStore predicate that is true if a given value u is not stored in persistent state (all such state \nin our model is either an instance variable or a global variable). NoStoreCall(u, v) t Call(u)h  NoStore(Callee(Edge(u)),Map(Edge(u),v)) \nNoStoreUse(u, v) t -Call(u) A DontStore(u, v) NoStore(u, v) t NoStoreCall(u, v) V NoStoreUse(u, v)  \nNoStore(c, v) t V~ipiEUse~(v~NoStore(p;, v) Valuability for locally created objects (results of new \nop- erations within a given contour) can be checked by looking at all the uses of such objects within \nthe contour. Once an object is passed in by value, we can copy it, and hence it is, effectively, created \nlocally. The following helper predicate reflects this: CreatedLocally t  LocalCreation V CullByValue(v) \n We are now ready to define valuability: an argument is callable by value if it can be passed by value \nfrom each call site. PassBy Value and CallBy Value formalize the criterion set out above: PassByValue(p, \nv) t NoStore@;, V)A -UsesAfter&#38; v)f\\vPipiEUse.Before(p,v) CreatedLocally ( ) CallByValue(v) t b \npij {(pi,vi)IMop(Edge(pi),v;)=v}  PassByValue(pi, vi) A given field may be inlined only if the mutator \nmethod has its value argument passed in by value. 5 Object Mining Transformation Once analysis has determined \nwhich objects are inlinable, we transform the program. In discussing our transforma-tion, the inlined \nfield is the field being removed and the in- lined state is the fields being added to the container. \nFirst, the methods and classes needed for specialization must be created by cloning. Then classes must \nbe restructured to remove inlined fields and add new fields for inlined state; all uses of inlined fields \nmust be redirected to the container s new inlined state. Finally, definitions of inlined fields must \nbe altered to update inlined state instead. 5.1 Cloning Analysis has marked all values indicating from \nwhich fields they came. Thus, to specialize methods on inlined fields, we use the Concert cloning mechanism \n(Section 32.2) to separate method contours that come from differing fields. Thus, we get one clone of \neach method for each inlined field on which it could be called. This requires just checking whether, \nfor two given contours, the field from which self could have originated is the same. The cloning is illustrated \nin Figure 10  Before I I I i I r After Figure 10: Object Inlining Code Cloning Similarly, we clone \nclasses to handle polymorphic con-tainers. Suppose that a polymorphic field is inlinable ac-cording to \nour analysis; the type analysis (See Section 3.2.1) creates object contours for each type of the polymorphic \nfield (See Figure 7). To inline fields of multiple types, we must have distinct container classes, and \nso cloning splits object contours corresponding to different types of inlinable fields into different \nclasses. 5.2 Restructuring Classes When adding inlined fields to a class, we must avoid altering the \nlayout of other fields of the class. Consider a polymor- phic container in which an inlined field has \ndifferent types in different situations: it has been cloned, but we need not clone methods that do not \nuse the inlined field if we can preserve the layout of the other fields across all the cloned classes. \nFurthermore, the layouts of the container class and any subclasses must remain conforming. Both constraints \ncan be satisfied by replacing the inlined field with one field from the inlined class, and adding the \nrest of the fields at the end of the fields of the container class. This is illustrated in Figure 11 \nFigure 11: New Rectangle and Parallelogram 5.3 Use Specialization Field references whose object is from \nan inlined field must be redirected to the corresponding inlined state. This involves two steps: traversing \nall the methods called upon the inlined field, adjusting field references to use the new inlined state, \nand eliding accesses to the inlined field in methods on the containing object. The effect of this transform \nis illustrated in Figure 12. R--m#-mo _ / lo.bwwJm / I1 I up -lighti' w-f+ / / rrYrn* * R-ldc=m \\ i \nllrxJol: , to.% I ,+-3-E-A mrro; -L%!gG).~!: w=-o -~=~jpvJ Before ~;$$.gg&#38;,; /* / .  / P u After \nFigure 12: Code Changes for Rectangle and Point Inlining arrays (converting arrays of references to arrays \nof object states) presents a complication. Uses of the inlined field become array accesses to the inlined \nstate, and so we must remember what array index to use. When eliding ac-cesses to inlined array contents, \nwe pass the array index used along with the containing array to uses of the inlined field. This index \nvalue is then used when altering the accesses to the inlined field by turning them into array references. \nThis is illustrated in Figure 13. Figure 13: Code Changes for an Array of Points 5.4 Assignment Specialization \nWhen the inlined field is removed, assignments to it must be converted into assignments to the inlined \nstate. Analysis ensures this is safe, so we replace the original assignment to the inlined field with \ncopies into the inlined fields; we traverse methods on the containing object, changing each assignment. \nArray contents are special: copies are generated for each array element (each array has a field that \nrecords its size). 6 Evaluation To evaluate our optimization, we implemented it in the Con- cert compiler, \nand ran it with a suite of object-oriented benchmarks. We report on the effectiveness, costs and ben- \nefits of our optimization. Effectiveness covers whether our analysis can inline all slots for which it \nis appropriate; the benefits are performance gains. The costs are both compile time and code size; we \nquantify compile time by measuring the additional analysis sensitivity object inlining requires. We choose \nthe following benchmark programs to stress all aspects of our implementation: OOPACK is a set of kernels \nfrom KAI that demonstrate the compiler s ability to perform simple object-oriented optimizations. One \nkernel (the ComplexBenchmark) uses arrays of complex number objects; these numbers are inline allocated \nin C++, but would be references in Java or Lisp. Our transformation inlines these objects into their \ncontaining arrays. We include timings only for this kernel. Richards is an operating system simulator \nbenchmark. It has some array objects that can be inlined into con-taining objects; more interestingly, \nthe Task object has a private data pointer (declared as void * in C++ and accessed using castsj. use \ndifferent Various subclasses types in thii slot, and hence it cannot be declared in- lined in C++. Our \ntransformation inlines the private data independently for each subclass. Silo is an event-driven simulator \nbenchmark from the repository at Colorado4. Some wrapper objects for queues can be inlined into their \ncontainers, and list items (essentially cons cells) can be eliminated by com- bining them with their \ndata. The queue wrappers are inline allocated in C++, but the cons cells cannot be. polyover is the benchmark \nfrom [28]; it computes an over- lay of two polygon maps; it uses several algorithms em-ploying arrays \nand lists of polygons. Our transforma- tion inlines cons cells as in Silo, contents of arrays, and, most \ninterestingly, an array of cons cells. These cells stored references to each other, and hence inlining \nthem requires our analysis ability to flow thru data. The arrays are inline allocated in C++, but the \ncons cells cannot be. These benchmarks are all pm-existing C++ codes; since the Concert Compiler accepts \nICC++ [9], a C++-derived language, only minor changes were required for our compiler to accept them. \nThese were mostly declaration and array access syntax. However, the Concert Compiler assumes an object \nmodelin which all objects are accessed via references, so syntactic inline allocation is ignored. 6.1 \nAnalysis Effectiveness Given our approach of not (conceptually) destroying the containee, the only fundamental \nlimit on what objects our optimization can inline is the ability to add copies. Adding copies depends \nupon aliasing information, so is necessar-ity conservative. For each of the codes, we present four field \ncounts: the total number of fields which hold objects, the number that could ideally be inlined given \naliasing con-straints (determined by hand), the number of fields declared inline in C++, and the number \nfields our optimization in-lined. 18 Declaredso in C++ 2 15 Automakallyinlinad g 12 z 9 e z 6 3 0 Silo \nRirds oopack poiyomr Figure 14: Inlinable Field Counts Our analysis did as well or better than manual \ninline allocation on all codes; there was no field manually declared inline in C++ that our analysis \ndid not find inlmable. We did better than C++ on Silo, Richards and polyover. This is because we inlined \na polymorphic field in Richards and merged cons cells with their data in Silo and polyover. In both polyover \nand Silo, lists present problems. In Silo, our analysis cannot inline cons cells of the global event \nlist, because it cannot tell that a given event is in the list at most once; hence it thinks there is \npossible aliasing between the data of different elements, so the data and list element cannot be merged. \nIn polyover, a list cannot be blocked because it is constructed in a loop, and our analysis has no mechanism \nto distinguish loop iterations. The Richards code causes another difficulty: an array of pointers to \ntasks. The array is polymorphic (different ele- ments reference different types of task) and our analysis \ndoes not distinguish different array elements, simply representing the array state as an instance variable. \n6.2 Optimization Cost There are two obvious costs of object inlining: the extra sensitivity required \nby the analysis and the accretion of extra generated code from cloning. While the analysis does require \na good deal of extra sensitivity, and produces many more clones than without it, this does not translate \ninto more generated code. 6.2.1 Generated Code Size The Concert Compiler generates C++ code as a portable \nassembly language; for this reason, among others, our code is much larger than that of G++, but object \ninlining does not increase the size of the code, but rather the opposite, as Figure 15 shows. This table \nmeasures the size in kilobytes of stripped object files generated by G++. The reason is twofold: by removing \nobject allocations and heap references, object inlining shrinks the size of spe- Q, 240 = ~~fLcZWi*out \nlnlining 3210 t i5?%+58 ConcertWith lnlining i 180 % 150 $.8 120 ; 90 75 60 3 30 n po1-r Sib Oopack \nFigure 15: Object Inlining Code Expansion cialized methods compared to the original program. Fur-thermore, \nmost of the specialized methods are inlined, so the cloned methods are not generated by themselves any-way. \n6.2.2 Analysis Cost We present analysis sensitivity required as a metric of our analysis cost. As our \nanalysis works by creating different contours for uses of object fields, a good measure of its cost is \na count of the number of contours created with and with- out object inlining. Our compiler performs other \nanalyses, so more contours than methods are needed even without object inlining. Figure 16, presents \nthe number of method contours required per method in the program as a measure of precision required. \nConcert Without lnlining Concert With lnlining r\" 5.2 - iz 0. 4.5 - 2 2 3.8-E 0 3.1 - 0 g 2.4 -r g \n1.7- 1.0 m I porvover oopck RichardsSib Figure 16: Method Contours Required Our analysis framework also \ncreates object contours for handling data flow thru state; the object inlining analyses did not, for \nthese programs, require additional object con-tours. Even polygon overlay does not generate more object \ncontours, despite requiring analysis through object fields, because the fields used contain differing \ntypes, so the type inference mechanism splits the relevant object contours even when object inlining \ndoes not. The polygon overlay code also causes the largest increase in required sensitivity, mostly because \nessentially all the data structures in this program are transformed. 6.3 Performance To measure the performance \nimprovement brought about by object inlining, we compiled several our chosen object- oriented benchmark \nprograms using our Concert compiler both with and without object inlining; to provide calibra-tion, we \nalso compiled the same programs with G++; G++ was used with -02 for both the C++ programs and the Con- \ncert compiler generated code. Measurements were taken on a SparcStation 20/60, and are the average of \n10 runs. Figure 17 is normalized to the performance of Concert code without object inlining; it shows \nthat the Concert Sys- tem, without inlining, gives roughly similar performance to G++ except on polyover. \nOur use of C++ as a portable as-sembly language, unambitious array optimization, and, nat- urally, the \ncost of accessing uninlined objects are the major contributors when Concert is slower; a highly-tuned \nmemory allocator is the major reason Concert is faster in Silo. 1.8 m Concert Without lnlining m Concert \nWith lnlining .i t1.5 m G++ -02 xi .+ 0.9  E 0.6 s 0.3 (amy) (iisl, Figure 17: Object Inlining Performance \nThe performance gain of object-inlining is most dramatic on polyover: both the array and list versions \nare roughly three times as fast as without it. This code boasts the most aggressive use of inlining: \npolygons are inlined into arrays, tightening inner loops; result polygons are merged with the cons cells \nof their list, reducing dynamic allocation; and a list of cons cells is inline allocated, which also \ntightens loops. The combining of resultant polygons and cons cells produces tighter data-structures than \nthe C code, which is why the array version is faster than G++; the list version should be faster for \nthe same reason, but low-level code generation issues in the Concert compiler frustrate this. OOPACK \nis nearly twice as fast with inlining as without, and it ends up substantially faster than G++. This \nis due, in part, to inlining laying out the complex number array as parallel arrays (Fortran style) rather \nthan by object, which seems to improve cache performance for this code. The 14% gain for Silo comes primarily \nfrom reducing dynamic alloca-tion by merging cons cells with their data. Richards creates very few objects, \nand the 5% gain it shows derives partly from eliding pointer dereferences. Both Silo and Richards benefit \nfrom improved object field caching with object in-lining. Thus, object inlining overall makes code run \nup to three times as fast as without inline allocation and matches the performance of code with inline \nallocation specified by hand. 6.4 Discussion Our optimization manages many cases, from simple situa- \ntions where a C++ programmer would use inline allocation to more complex situations like eliding cons \ncells, which would, at a minimum, cause conceptual disruption to the code if a C++ programmer used inline \nallocation. poly-Over is a low-level C program with carefully tuned storage management resulting in messy \ncombinations of pointers. Our analysis is able to reproduce even such ugliness as that automatically. \nFurthermore, doing so provides substantial performance gains without bloating code size, indeed the code \nusually shrinks a little bit. The major limitation is, as one might expect, aliasing information needed \nto make inlining decisions. The limita- tions in Richards and polyover are implementation issues, better \nloop and array analysis respectively could solve these without fundamental difficulty. On the other hand, \nSilo is more problematic: to eliminate the cons cells of the event list would require strong aliasing \ninformation. Given the dif- ficulty of alias analysis, this represents a limitation on our optimization \ns efficacy. 7 Related Work The idea of rearranging data layout to improve performance is widespread, \nand the work most relevant to ours comes from three camps: the object-oriented realm, the functional \ncommunity and Fortran array optimization. Object-Oriented Systems The impetus for object inlining came \nfrom the ability to specify such inlining by hand in hybrid-object languages like C++ and Oberon-2 [14, \n20) that distinguish between objects and object references. But the idea of doing inline allocation automatically \nis by no means new; the Emerald [4, 181 object system was designed with this goal in mind. Indeed, they \nwanted to automat-ically generate specialized representations of objects and transform the code that \nused them. This is exactly what we do. However, their compiler was not capable of the requi- site analysis \n(Section 4). The compiler did do type inference, and they could inline allocate immediate types when \nthey had precise type information. They could do this because immediate types in Emerald, as in most \nobject-oriented lan-guages, are values-their contents are important but their identities are not-so they \ncould avoid our analyses by sim- ply copying in and out of the field. Functional Languages There has \nbeen much work in the functional community on unboxing, in which specialized rep- resentations are used \nto reduce storage and access overhead. Of particular relevance to our work is unboxing in the pres- ence \nof polymorphism [19, 151. In [Xi], Cordelia Hall and company present a transformation for Haskell that, \nupon being told what variables to unbox, generates specialized code to exploit the unboxing, even for \npolymorphic func-tions. They generate specialized function versions using a partial evaluator to propagate \nunboxedness through the program. The propagation of unboxedness resembles the propagation of tags our \nuse analysis defines; however, it only works for immediate types. In [25], Shao et al. unroll linked \nlists-essentially inline allocating tail pointers-in a functional subset of ML. Their analysis works \nusing refinement types (121 that distinguish odd and even length lists. These refined types are prop- \nagated using an abstract interpretation, with rules for the refined types generated by cons statements. \nAll functions that take list parameters are cloned and specialized with all possible combinations of \nrefinement types for their list parameters. Our analysis is similar is using an abstract in-terpretation, \nbut our field tags are more general, as they handle arbitrary object structures, rather than lists. Also \nour scheme, because our abstract interpretation is interpro- cedural, only analyzes specializations that \nare actually used, whereas theirs analyzes all possible combinations. Since this work was done for functional \nlanguages, their analysis can be more straightforward than ours: it need not handle structure assignment, \ni.e. data flow through con-tainers. This provides two simplifications: it eliminates the difficulties \ncreated by assigning inlined object into other con- tainers, and it obviates the need to prove inline \nallocation is safe. Fortran Optimizing array layout for cache performance (2, 291 also involves transforming \ndata layout. Arrays and loops that operate upon them are tiled; that is, the inner loops are strip-mined \nto operate upon small portions of the arrays that will fit in cache. Because array iterations cannot \nbe reordered arbitrarily-due to data dependence-the arrays are rearranged in memory so these small portions \nexhibit spatial locality. Altering a given array s layout naturally affects all code that uses it, and \nfinding all such uses is analogous to our use specialization analysis, although much simpler analyses \nsuffice for common Fortran code. This alteration of array layout is typically done only when it can be \nexpressed as an affine transformation on the array indices and the analysis is generally not context \nsensitive. 8 Conclusions We presented object inlining, an optimization that automat- ically inline allocates \nobjects, allowing us obtain similar ef- ficiency to a system with explicit inline allocation while pre- \nserving the programmability benefits of a uniform object model. To do this requires a novel analysis \nand transforma- tion mechanism that builds upon previous work on analysis and specialization of object-oriented \nprograms. Our eval- uation shows that our analysis can handle not only situa- tions where inline allocation \nis done in C++, but also situ- ations where such manual allocation would be conceptually disruptive. \nWe showed several object-oriented benchmarks running up three times as fast due to object inlining as \nop- posed to without it, and matching the performance of code with inlining specified by hand. 9 Future \nWork Turning our analysis loose on C++ demonstrates that it can, at least, discover inlining opportunities \nin the sort of places that C++ programmers specify them, and provides a basis for comparison in standard \nC++ compilers such as G++. However, the greater opportunity for automatic inline allo- cation is languages \nwith a more abstract object model that do not allow its explicit specification. These languages also \npose the greater challenge by promoting a more polymor-phic and dynamic kind of program. But our optimization \ns handling of polymorphic containers and cons cells suggest such dynamic codes will be doable. We intend \nto apply our optimization to such languages, Java in particular. Acknowledgments Others have contributed \nto this paper and the work it de- scribes. My advisor Professor Andrew Chien provided a great deal of \nhelpful feedback on the structure and con-tent of the paper; the Concert System has been the work of \nAndrew Chien, Hao-Hua Chu, Bishwaroop Ganguly, Vi-jay Karamcheti, John Plevyak and Xingbin Zhang as well \nas myself. In particular, the analysis framework upon which this work is based was largely designed and \nimplemented by John Plevyak. The research described in this paper was supported in part by DARPA Order \n#E313 through the US Air Force Rome Laboratory Contract F30602-96-1-0286, NSF grant MIP-92-23732, ONR \ngrants N00014-92-J-1961 and N00014- 93-I-1086, NASA grant NAG 1-613, and supercomputing re-sources at \nthe Jet Propulsion Laboratory. Support from In- tel Corporation, Tandem Computers, Hewlett-Packard, and \nMotorola is also gratefully acknowledged. References [I] 0. Agesen, J. Palsberg, and M. Schwartzbach. \nType inference of SELF: Analysis of objects with dynamic and multiple inheritance. In Proceedings of \nECOOP 93, 1993. [2] Jennifer M. Anderson, Saman P. Amarasinghe, and Monica S. Lam. Data and computation \ntransformations for multiprocessors. In Proceedings of Fifth Symposium on Principles and Practice of \nPamllel Programming, 1995. [3] Apple Computer Inc. The NewtonScript Progmm-ming Language, December 1995. \nAvailable online from ftp://ftpdev.info.apple.com/DeveloperServices/ NewtonDevelopment/DOCSPDF/NSCRIPTR.ZIP. \n[4] A. Black, N. Hutchinson, E. Jul, and H. Levy. Ob- ject structure in the emerald system. In Proceedings \nof OOPSLA 86, pages 78-86. ACM, September 1986. (51 Brad Calder, Dirk Grunwald, and Benjamin Zorn. Quantifying \ndifferences between C and C++ programs. Technical Report CU-CS-698-94, University of Col-orado, Boulder, \nJanuary 1994. [6] C. Chambers and D. Ungar. Iterative type analysis and extended message splitting. In \nProceedings of the SIG- PLAN Conference on Programming Language Design and implementation, pages 150-60, \n1990. [7] Craig Chambers. The Cecil language: Specification and rationale, version 2.0. Technical report, \nDepart-ment of Computer Science and Engineering, University of Washington, Seattle, Washington, March \n1995. [8] Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vi- jay Karamcheti, and Xingbin Zhang. Supporting \nhigh level programming with high performance: The illinois concert system. In Proceedings of the Second \nInter-national Workshop on High-level Parallel Programming Models and Supportive Environments, April \n1997. [9] Andrew A. Chien, Uday S. Reddy, John Plevyak, and Julian Dolby. ICC++ -a C++ dialect for high-performance \nparallel computation. In Proceedings of the 2nd International Symposium on Object Technolo-gies for Advanced \nSoftware, March 1996. [lo] Jeffrey Dean, Craig Chambers, and David Grove. Se-lective specialization for \nobject-oriented languages. In Proceedings of the ACM SIGPLAN 95 Conference on Progmmmin g Language Design \nand Implementation, pages 93-102, La Jolla, CA, June 1995. [ll] Margaret A. Ellis and Bjarne Stroustrup. \nThe Anno-tated C++ Reference Manual. Addison-Wesley, 1990. [12] Tim Freeman and Frank Pfenning. Refinement \ntypes for ml. In Proceedings of the 1991 ACM SIGPLAN Conference on Progmmming Language Design and Im-plementation, \nJune 1991. [13] Adele Goldberg and David Robson. Smalltalk-80: The language and its implementation. Addison-Wesley, \n1985. [14] H. Mossenbock. Object-Oriented Programming in Oberon-2. Springer-Verlag, 1993. (151 Cordelia \nHall, Simon L. Peyton-Jones, and Patrick M. Sansom. Functional Programming, Glasgow 1994, chapter Unboxing \nUsing Specialization. Workshops in Computing Science. Springer-Verlag, 1995. [16] Urs Hijlzle, Craig \nChambers, and David Ungar. Op-timizing dynamically-typed object-oriented languages with polymorphic inline \ncaches. In ECOOP 91 Confer-ence Proceedings. Springer-Verlag, 1991. Lecture Notes in Computer Science \n512. [17] Urs HGlzle and David Ungar. Optimizing dynamically-dispatched calls with run-time type feedback. \nIn Pro-ceedings of the 1994 ACM SIGPLAN Conference on Programming Language Design and Implementation, \npages 326-336, June 1994. [18] Norman C. Hutchinson. Ememld: An Object-Based Language for Distributed \nProgramming. PhD thesis, University of Washington, Department of Computer Science, Seattle, Washington, \n1987. TR-87-01-01. [19] Xavier Leroy. Unboxed objects and polymorphic typ-ing. In Proceedings of the \n19th Symposium on the Prin-ciples of Programming Languages, pages 177-188, 1992. [20] N. Wirth and J. \nGutknecht. Project Oberon: The De-sign of an Operating System and Compiler. Addison Wesley, 1992. [21] \nJ. Palsberg and M. Schwartzbach. Object-oriented type inference. In Proceedings of OOPSLA 91, pages 146 \n61, 1991. [22] John Plevyak. Optimrzation of Oblecl-Orzented and Concurrent Programs. PhD thesis, University \nof Illi- nois at Urbana-Champaign, Urbana, Illinois, 1996. [23] John Plevyak and Andrew A. Chien. Precise \nconcrete type inference of object-oriented programs. In Pro-ceedings of OOPSLA 94, Object-Oriented Programming \nSystems, Languages and Architectures, pages 324-340, 1994. [24] John Plevyak and Andrew A. Chien. Type \ndirected cloning for object-oriented programs. In Proceedings of Ihe Workshop for Languages and Compilers \nfor Parallel Computing, pages 566-580, 1995. [25] Zhong Shao, John H. Reppy, and Andrew W. Appel. Unrolling \nlists. In ACA4 Conference on Lisp and Func-tional Programming, June 1994. [26] Olin Shivers. Topics in \nAdvanced Language Implemen-tation, chapter Data- Flow Analysis and Type Recovery in Scheme, pages 47--88. \nMIT Press, Cambridge, MA, 1991. [27] Sun Microsystems Computer Corporation. The Java Language Specific&#38;ion, \nMarch 1995. Available at http://java.sun.comll.Oalpha:!/doc/java-whitepaper.ps. [28] Gregory V. Wilson \nand Paul Lu, editors. Parallel Pro-gramming Using C++. MIT Press, 1995. [29] Micheal E. Wolf and Monica \nS. Lam. A data locality optimizing algorithm. In Proceedings of the 1991 ACM SIGPLAN Conference on Programming \nLanguage De-sign and Implementalion, June 1991.  \n\t\t\t", "proc_id": "258915", "abstract": "Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, function specialization); in this paper, we introduce <i>object inlining</i>, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several object-oriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined codes.", "authors": [{"name": "Julian Dolby", "author_profile_id": "81100506419", "affiliation": "Concurrent Systems Architecture Group, Department of Computer Science, University of Illinois, 1304 West Springfield Avenue, Urbana, IL", "person_id": "PP18001962", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258918", "year": "1997", "article_id": "258918", "conference": "PLDI", "title": "Automatic inline allocation of objects", "url": "http://dl.acm.org/citation.cfm?id=258918"}