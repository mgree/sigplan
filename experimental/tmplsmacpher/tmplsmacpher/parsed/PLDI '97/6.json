{"article_publication_date": "05-01-1997", "fulltext": "\n Dynamic Feedback: An Effective Technique for Adaptive Computing * Pedro Diniz and Martin Rinard Department \nof Computer Science Engineering I Building University of California, Santa Barbara Santa Barbara, CA \n93 104-5 110 {pedro,martin}@cs.ucsb.edu Abstract This paper presents dynamic feedback, a technique \nthat enables computations to adapt dynamically to different execution environ- ments. A compiler that \nuses dynamic feedback produces several different versions of the same source code; each version uses \na dif- ferent optimization policy. The generated code alternately performs sampling phases and production \nphases. Each sampling phase mea- sures the overhead of each version in the current environment. Each \nproduction phase uses the version with the least overhead in the pre- vious sampling phase. The computation \nperiodically resamples to adjust dynamically to changes in the environment. We have implemented dynamic \nfeedback in the context of a par- allelizing compiler for object-based programs. The generated code uses \ndynamic feedback to automatically choose the best synchro- nization optimization policy. Our experimental \nresults show that the synchronization optimization policy has a significant impact on the overall performance \nof the computation, that the best policy varies from program to program, that the compiler is unable \nto stat- icdly choose the best policy, and that dynamic feedback enables the generated code to exhibit \nperformance that is comparable to that of code that has been manually tuned to use the best policy. We \nhave also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality \nbound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses \nthe best policy at every point during the execution. 1 Introduction The most efficient implementation \nof a given abstraction often de- pends on the environment in which it is used. For example, the best \nconsistency protocol in a software distributed shared memory system often depends on the access pattern \nof the parallel pro- gram [12]. The best data distribution of dense matrices in dis- tributed memory \nmachines depends on how the different parts of the program access the matrices [l, 2, 18, 211. The best \nconcrete data structure to implement a given abstract data type often de- pends on how it is used [14, \n221. The best algorithm to solve a . Pedro Diniz is sponsomd by the PRAXIS XXI pmpm administrated by \nJNICJ -Junta National de hwestigafao Qenthca e Tecnold&#38;a from Portugal, and holds a Fulbrigbt travel \ngrant. Martin Rinard is supported in par~ by an Alfred R Sloan Research Fellowship. Permission to make \ndigital/hard copy of part or all this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advan-tage, the copyright notice, the \ntitle of the publication and its date appear, and notice is given that copying is by permission of ACM, \nInc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior \nspecific permission and/or a fee. PLDI 97 Las Vegas, NV, USA 0 1997 ACM O-89791-907-6/97/0006...$3.50 \ngiven problem often depends on the combination of input and harcl- ware platform used to execute the \nalgorithm [S]. In all of these cases. it is impossible to statically choose the best implementation -the \nbest implementation depends on information (such as the input data, dynamic program characteristics or \nhardware features) that is either difficult to extract or unavailable at compile time. If a programmer \nhas a program with these characteristics, he or she is currently faced with two unattractive alternatives: \neither manu- ally tune the program for each environment. or settle for suboptimal pclfOl-IllanCe. We \nhave developed a new technique. dynunzic feedback, that en- ables programs to automatically adapt to \ndifferent execution envi- ronments. A compiler that uses dynamic feedback pmduces several different versions \nof the same code. Each version uses a different optimization policy. The generated code alternately performs \nsum- phg phases and production phases. During a sampling phase, the generated code measures the overhead \nof each version in the cur- rent environment by running that version for a fixed time interval. Each \nproduction phase then uses the version with the least overhead in the previous sampling phase. After \nrunning a production phase for a fixed time interval, the generated code performs another sam- pling \nphase. If the environment has changed, the generated code dynamically adapts by using a different version \nin the next produc- tion phase. We see dynamic feedback as part of a general trend towards adaptive computing. \nAs the complexity of systems and the capa- bilities of compilers increase, compiler developers will find \nthat they can automatically apply a large range of transformations, but have no good way of statically \ndetermining which transformations will deliver good results when the program is actually executed. The \nproblem will hecome even more acute with the emergence of new computing paradigms such as mobile programs \nin the Internet. The extreme heterogeneity of such systems will defeat any imple- mentation that does \nnot adapt to different execution environments. Dynamic feedback is one example of the adaptive techniques \nthat will enable compilers to deliver good performance in modem com- puting systems. This paper describes \nthe use of dynamic feedback in the context of a parallelizing compiler for object-based languages. The \ncom- piler generates parallel code that uses synchronization constructs to make operations execute atomically \n[33]. Our experimental results show that the resulting synchronization overhead can significantly degrade \nthe performance [lo]. We have developed a set of synchro- nization transformations and a set of synchronization \noptimization policies that use the transformations to reduce the synchronization overhead [lo]. Unfortunately, \nthe best policy is different for differ- ent programs, and may even vary dynamically for different parts \nof the same program. Furthermore, the best policy depends on infor- mation, such as the global topology \nof the manipulated data struc- tures and the dynamic execution schedule of the parallel tasks, that is \nunavailable at compile time. The compiler is therefore unable to statically choose the best synchronization \noptimization policy. Our implemented compiler generates code that uses dynamic feedback to automatically \nchoose the best synchronization opti-mization policy. Our experimental results show that dynamic feed- \nback enables the automatically generated code to exhibit perfor- mance comparable to that of code that \nhas been manually tuned to use the best policy.  1.1 Contributions This paper makes the following contributions: \nIt presents a technique. dynamic feedback, that enables sys- tems to automatically evaluate several different \nimplemen- tations of the same source code, then use the evaluation to choose the best implementation \nfor the current environment. It shows how to apply dynamic feedback in the context of a parallelizing \ncompiler for object-based programs. The gen- erated code uses dynamic feedback to automatically choose \nthe best synchronization optimization policy. It presents a theoretical analysis that characterizes the \nworst- case performance of systems that use dynamic feedback. This analysis provides, under certain assumptions, \na guaranteed optimality bound for dynamic feedback relative to a hypo- thetical (and unrealizable) optimal \nalgorithm that uses the best policy at every point during the execution. It presents experimental results \nfor the automatically gen-erated parallel code. These results show that the generated code exhibits performance \ncomparable to that of code that has been manually tuned to use the best synchronization op-timization \npolicy.  1.2 Structure The remainder of the paper is structured as follows. In Section 2, we briefly \nsummarize the analysis technique, commutativity analy- sis, that our compiler is based on. Section 3 \nsummarizes the issues that affect the performance impact of the synchronization optimiza- tion algorithms. \nSection 4 describes the implementation details of applying dynamic feedback to the problem of choosing \nthe best synchronization policy. Section 5 presents the theoretical analy- sis. Section 6 presents the \nexperimental results. We discuss related work in Section 7 and conclude in Section 8. 2 Commutativity \nAnalysis Our compiler uses commufativity undysis to automatically paral- lelize serial, object-based \nprograms. Such programs structure the computation as a sequence of opemtion.9on objects. The compiler \nanalyzes the program at this granularity to determine when opera- tions commute, or generate the same \nresult regardless of the order in which they execute. If all of the operations in a given compu- tation \ncommute, the compiler can automatically generate parallel code. This code executes all of the operations \nin the computation in parallel. Our experimental results indicate that this approach can effectively \nparallelize irregular computations that manipulate dy-namic, linked data structures such as trees and \ngraphs [33]. To ensure that operations execute atomically, the compiler aug- ments each object with a \nmutual exclusion lock. It then automati- cally inserts synchronization constructs into operations that \nupdate objects. These operations first acquire the object s lock, perform the update, then release the \nlock. The synchronization constructs ensure that the operation executes atomically with respect to all \nother operations that access the object.  3 Synchronization Optimizations We found that, in practice, \nthe overhead generated by the synchro- nization constructs often reduced the performance. We therefore \ndeveloped several synchronization optimization algorithms [lo]. These algorithms are designed for parallel \nprograms, such as those generated by our compiler, that use mutual exclusion locks to im- plement critical \nregions. Each critical region acquires its mutual exclusion lock, performs its computation, then releases \nthe lock. Computations that use mutual exclusion locks may incur two kinds of overhead: locking overhead \nand wuiting overhead. Lock- ing overhead is the overhead generated by the execution of con- structs that \nsuccessfully acquire. or release a lock. Waiting overhead is the overhead generated when one processor \nwaits to acquire a lock held by another processor. If a computation releases a lock, then reacquires \nthe same lock, it is possible to reduce the locking overhead by eliminating the re- lease and acquire. \nOur synchronization optimization algorithms statically detect computations that repeatedly release and \nreacquire the same lock. They then apply lock elimination trans$ormations to eliminate the intermediate \nrelease and acquire constructs [IO]. The result is a computation that acquires and releases the lock \nonly once. In effect, the optimization coalesces multiple critical regions that acquire and release the \nsame lock multiple times into a single larger critical region that includes all of the original critical \nregions. The larger critical region, of course, acquires and releases the lock only once. This reduction \nin the number of times that the computa- tion acquires and releases locks translates directly into a \nreduction in the locking overhead. Figures 1 and 2 present an example of how synchronization op-timizations \ncan reduce the number of executed acquire and release constructs. Figure 1 presents a program (inspired \nby the Bames- Hut benchmark described in Section 6) that uses mutual exclusion locks to make body : : \none-interaction operations execute atomically. Figure 2 presents the program after the application of \na synchronization optimization algorithm. The algorithm interproce- durally lifts the acquire and release \nconstructs out of the loop in the body : : interactions operation. This transformation reduces the number \nof times that the acquire and release constmcts are executed. An overly aggressive synchronization optimization \nalgorithm may introduce false exclusion. False exclusion may occur when a processor holds a lock during \nan extended period of computation that was originally part of no critical region. If another proces- \nsor attempts to execute a critical region that uses the same lock, it must wait for the first processor \nto release the lock even though the first processor is not executing a computation that needs to be in \na critical region. The result is an increase in the waiting overhead. Excessive false exclusion reduces \nthe amount of available concur-rency, which can in turn decrease the overall performance. The synchronization \noptimization algorithms must therefore me- diate a trade-off between the locking overhead and the waiting \nover- head. Transformations that reduce the locking overhead may in- crease the waiting overhead, and \nvice-versa. The synchronization optimization algorithms differ in the policies that govern their use \nof the lock elimination transformation: extem double interact(double,double); chss body { private : lock \nmutex; double pos,sum; public : void one-interaction(body *b); void interactions(body b[l, int n); 1; \n void body: :one-interaction(body *b) { double val = interact(this->pos, b->pos); mutex.acquire ( ) \n; sum = sum + val;  mutex.release ( ) ; 1 void body::interactions(body b[l, int n) { for (int i = 0; \ni < n; i++) { this->one-interaction(&#38;b[il); 1 I Figure 1: Unoptirnized Example Computation extem \ndouble interact(double,double); this body { private : lock mutex; double pos,sum; public: void one-interaction(body \n*b); void interactions(body b[l, int n); 1;  void body:: one-interaction(body *b) { double val = interact(this->pos, \nb->pos); sum = sum + val; 1 void body::interactions(body b[l, int n) { mutex.acquire ( 1 ; for (int \ni = 0; i c n; i++) { this->one-interaction(&#38;b[il); 1  mute.x.rekase ( ) ; 1 Figure 2: Optimized \nExample Computation . Original: Never apply the transformation -always use the default placement of acquire \nand release constructs. In the default placement, each operation that updates an object ac- quires and \nreleases that object s lock. . Bounded: Apply the transformation only if the new critical region will \ncontain no cycles in the call graph. The idea is to limit the severity of any false exclusion by limiting \nthe dynamic size of the critical region. . Aggressive: Always apply the transformation. In general, \nthe amount of overhead depends on complicated dy- namic properties of the computation such as the global \ntopology of the manipulated data structures and the run-time scheduling of the parallel tasks. Our experimental \nresults show that the synchroniza- tion optimizations have a large impact on tire performance of our \nbenchmark applications. Unfortunately, there is no one best policy. Because the best policy depends on \ninformation that is not avail- able at compile time, the compiler is unable to statically choose the \nbest policy.  4 Implementing Dynamic Feedback The compiler generates code that executes an alternating \nsequence of serial and parallel sections. Within each parallel section, the generated code uses dynamic \nfeedback to automatically choose the best synchronization optimization policy. The execution starts with \na sampling phase, then continues with a production phase. The parallel section periodically resamples \nto adapt to changes in the best policy. We next discuss the specific issues associated with implementing \nthis general approach. 4.1 Detecting Interval Expiration To obtain the optimality results in Section \n5, the generated code for the sampling phase must execute each policy for a fixed sampling time interval. \nThe production phase must also execute for a fixed production time interval, although the production \nintervals are typ- ically much longer than the sampling intervals. The compiler uses two values to control \nthe lengths of the sampling and production intervals: the target sampling interval and the target pmduction \ninterval. At the start of each interval, the generated code reads a timer to obtain the starting time. \nAs it executes, the code periodi- cally polls the timer: it reads the timer, computes the difference \nof the current time and the starting time, then compares the difference with the target interval. The \ncomparison enables the code to de tect when the interval has expired. Several implementation issues determine \nthe effectiveness of this approach: . Potential Switch Points: In general, it is possible to switch policies \nonly at specific potentid switch points during the execution of the program. The rate at which the potential \nswitch points occur in the execution determines the minimum polling rate, which in turn determines how \nquickly the gen- erated code responds to the expiration of the current interval. In all of our benchmark \napplications, each parallel section executes a parallel loop. A potential switch point occurs at each \niteration of the loop, and the generated code tests for the expiration of the current interval each time \nit completes an iteration. In our benchmark applications, the individual iterations of the loops are \nsmall enough so that each proces- sor can respond reasonably quickly to the expiration of the interval. \n . Polling Overhead: The polling overhead is determined in large part by the overhead of reading the \ntimer. Our cur- rently generated code uses the timer on the Stanford DASH machine. The overhead of accessing \nthis timer is approxi- mately 9 microseconds, which is negligible compared with the sizes of the iterations \nof the parallel loops in our bench- mark applications. . Synchronous Switching: The generated code switches \npoli- cies synchronously. When an interval expires, each proces- sor waits at a barrier until all of \nthe other processors detect that the interval has expired and arrive at the barrier. This strategy ensures \nthat all processors use the same policy dur- ing each sampling interval. The measured overhead therefore \naccurately reflects the overhead of the policy. Synchronous switching also avoids the possibility of \ninterference between incompatible policies. One potential drawback of synchronous switching is that each \nprocessor must wait for all of the other processors to detect the expiration of the current interval \nbefore it can proceed to the next interval. This effect can have a significant neg- ative impact on the \nperformance if one of the iterations of the parallel loop executes for a long time relative to the other \niterations and to the sampling interval. The combination of an especially bad policy (for example, a \nsynchronization op timization policy that serializes the computation) and itera- tions of the loop that \nexecute for a significant time relative to the sampling interval can also cause poor performance. . Timer \nPrecision: The precision of the timer places a lower bound on the size of each interval. The timer must \ntick at least once before the interval expires. In general, we do not expect the precision of the timer \nto cause any problems. Our generated code uses target sampling intervals of at least several milliseconds \nin length. Most systems provide timers with at least this resolution. All of these issues combine to \ndetermine the eflective sampling interval, or the minimum time from the start of the interval to the \ntime when all of the processors detect that the interval has expired and proceed to the next interval. \nThe theoretical analysis in Sec- tion 5 formulates some of the optimality results in terms of the effective \nsampling interval. 42 Stitching Policies During the sampling phase, the generated code. must switch quickly \nbetween different synchronization optimization policies. The cur- rent compiler generates three versions \nof each parallel section of code. Each version uses a different synchronization optimization policy. \nThe advantage of this approach is that the code for each pol- icy is always available, which enables \nthe compiler to switch very quickly between different policies. The currently generated code simply executes \na switch statement at each parallel loop iteration to dispatch to the code that implements the current \npolicy. The potential disadvantage is an increase in the size of the gen- erated code. Table 1 presents \nthe sizes of the text segments for several different versions of our benchmark applications. These This \npotential problem does not arise when using dynemic feedback to choose the best synctuonizndon opdmizedon \npolicy. All of the synchronization optimize- lion policies me compatiblez it is possible to concunrntly \nexecute different versions without tiecting the cxmzctness of the computation. We expect that in other \napptica- tions of dynamic feedback, however, the different policies may be iucompatible and tbe concurrent \nexecution of different versions may cause the computation to execute incorrectly. data are from the object \nfiles of the compiled applications before linking and therefore include code only from the applications \n-there is no code from libraries. The Serial version is the original serial program, the Original version \nuses the Original synchroniza- tion optimization policy and the Dynamic version uses dynamic feedback. \nIn general, the increases in the code size are quite small. This is due, in part, to an algorithm in \nthe compiler that locates closed subgraphs of the call graph that are the same for all opti- mization \npolicies. The compiler generates a single version of each method in the subgraph, instead of one version \nper synchronization optimization policy. Application 1 Version 1 Sizc(bytes) I Serial I 25,248 Barnes-Hut \n1 Chininal 1 31;152 Table 1: Executable Code Sizes (bytes) We also considered using dynamic compilation \n[3, 11, 241 to produce the different versions of the parallel sections as they were required. Although \nthis approach would reduce the amount of code present at any given point in time, it would significantly \nincrease the amount of time required to switch policies in the sampling phases. This alternative would \ntherefore become viable only for sit- uations in which the sampling phases could be significantly longer \nthan our set of benchmark applications would tolerate. Finally, it is possible for the compiler to generate \na single ver- sion of the code that can use any of the three synchronization opti- mization policies. \nThe idea is to generate a conditional acquire or release construct at all of the sites that may acquire \nor release a lock in any of the synchronization optimization policies. Each site has a flag that controls \nwhether it actually executes the construct; each acquire or release site tests its flag to determine \nif it should acquire or release the lock. In this scenario, the generated code switches policies by changing \nthe values of the flags. The advantage of this approach is the guarantee of no code growth; the disadvantage \nis the residual flag checking overhead at each conditional acquire or release site.  4.3 Measuring the \nOverhead To choose the policy with the least overhead, the generated code must first measure the overhead. \nThe compiler instruments the code to collect three measurements: Locking Overhead: The generated code \ncomputes the lock- ing overhead by counting the number of times that the com- putation acquires and releases \na lock. This number is com- puted by incrementing a counter every time the computation acquires a lock. \nThe locking overhead is simply the time required to acquire and release a lock times the number of times \nthe computation acquires a lock. Waiting Overhead: The current implementation uses spin locks. The hardware \nexports a construct that allows the com- putation to attempt to acquire a lock; the return value indi- \ncates whether the lock was actually acquired. To acquire a lock, the computation repeatedly executes \nthe hardware lock acquire construct until the attempted acquire succeeds. The computation increments \na counter every time an attempt to acquire a lock fails. The waiting overhead is the time re-quired to \nattempt, and fail, to acquire a lock times the number of failed acquires. . Execution Time: The amount \nof time that the computation spends executing code from the application. This time is measured by reading \nthe timer when a processor starts to ex- ecute application code, then reading the timer again when the \nprocessor finishes executing application code. The pro- cessor then subtracts the first time from the \nsecond time, and adds the difference to a running sum. As measured, the ex- ecution time includes the \nwaiting time and the time spent acquiring and releasing locks. It is possible to subtract these two sources \nof overhead to obtain the amount of time spent performing useful computation. Together, these measurements \nallow the compiler to evaluate the total overhead of each synchronization optimization policy. The total \noverhead is simply the lock overhead plus the waiting over- head divided by the execution time. The total \noverhead is therefore always between zero and one. The compiler uses the total over- head to choose the \nbest synchronization optimization policy -the policy with the lowest overhead is the best. One potential \nconcern is the instrumentation overhead. Our ex- perimental results indicate that this overhead has little \nor no effect on the performance. We measure the overhead by generating ver-sions of the applications \nthat use a single, statically chosen, syn- chronization optimization policy. We then execute these versions \nwith the instrumentation turned on and the instrumentation turned off. The performance differences between \nthe instrumented and uninstrumented versions are very small, which indicates that the instrumentation \noverhead has little or no impact on the overall per- formance. 4.4 Choosing Sampling and Production \nIntervals The sizes of the target sampling and production intervals can have a significant impact on \nthe overall performance of the generated code. Excessively long sampling intervals may degrade the perfor- \nmance by executing non-optimal versions of the code for a long time. But if the sampling interval is \ntoo short, it may not yield an accurate measurement of the overhead. In the worst case, an inac- curate \noverhead measurement may cause the production phase to use the wrong synchronization optimization policy. \nWe expect the minimum absolute length of the sampling inter- val to be different for different applications. \nIn practice, we have had little difficulty choosing default values that work well for our applications. \nIn fact, it is possible to make the target sampling inter- vals very small for all of our applications \n-the minimum effective sampling intervals are large enough to provide overhead measure- ments that accurately \nreflect the relative overheads in the pmduc- tion phases. To achieve good performance, the production \nphase must be long enough to profitably amortize the cost of the sampling phase. In practice., we have \nfound that the major component of the sam- pling cost is the time spent executing the non-optimal versions. \nSection 5 presents a theoretical analysis that characterizes how long the production phase must be relative \nto the sampling phase to achieve an optimality result. In our current implementation of dy- namic feedback, \nthe length of the parallel section may also limit the performance. Our current implementation always \nexecutes a sampling phase at the beginning of each parallel section. If a par- allel section does not \ncontain enough computation for a production phase of the desired length, the computation may be unable \nto suc- cessfully amortize the sampling overhead. It should be possible to eliminate this potential problem \nby generating code that allows sampling and production intervals to span multiple executions of the parallel \nphase. This code would still maintain separate sam- pling and production intervals for each parallel \nsection, but allow the intervals to contain multiple executions of the section. In practice, we have \nhad little difficulty choosing target produc- tion intervals that work well for our applications. All \nof our ap- plications perform well with target production intervals that range from five to 1000 seconds. \n 4.5 Early Cut Off and Policy Ordering In many cases, we expect that the individual sources of overhead \nwith be either monotonically nondecreasing or monotonically non-increasing across the set of possible \nimplementations. The lock- ing overhead, for example, never increases as the policy goes from Original \nto Bounded to Aggressive. The waiting overhead, on the other hand, should never decrease as the policy \ngoes from Original to Bounded to Aggressive. These properties suggest the use of an early cut off to \nlimit the number of sampled policies. If the Aggres- sive policy generates very little waiting overhead \nor the Original policy generates very little locking overhead, there is no need to sample any other policy. \nIt may therefore be possible to improve the sampling phase by trying extreme policies first, then going \ndirectly to the production phase if the overhead measurements indicate that no other policy would do \nsignificantly better. It may also be possible to improve the sampling phase by ordering the policies. \nThe generated code could sample a given policy first if it has done well in the past. If the measured \noverhead continued to be acceptable, the generated code could go directly to the production phase.  \n5 Theoretical Analysis In this section, we present a theoretical analysis of the worst-case performance \nof dynamic feedback. We compare dynamic feedback with an hypothetical, unrealizable algorithm that always \nuses the best policy. We start by observing that if there is no constraint on how fast the overhead of \neach policy may change, it is impossible to obtain a meaningful optimality result for any sampling algorithm \n-the overhead of each policy may change dramatically right after the sampling phase. We therefore impose \nthe constraint that changes in the overheads of the different policies are bounded by an expo- nential \ndecay function. We also assume that the values measured during the sampling phase accurately reflect \nthe actual overheads at the start of the production phase. Finally, we assume each produc- tion phase \nexecutes to completion, although it is possible to relax this assumption. The worst case for the dynamic \nfeedback algorithm relative to the optimal algorithm occurs when more than one policy has the lowest \noverhead during the sampling phase. In this case, the dy- namic feedback algorithm must arbitrarily select \none of the sam- pled policies with the lowest overhead for the production phase. The maximum difference \nbetween the performance of the dynamic feedback algorithm and the performance of the optimal algorithm \noccurs when the overhead of the selected policy increases at the maximum bounded rate and the overheads \nof the other policies de- crease at the maximum bounded rate. We analyze this scenario to derive a conservative \nbound on the worst-case performance of the dynamic feedback algorithm relative to the optimal algorithm. \nWe next establish some notation. The variable S is the effective sampling interval, P is the length of \nthe production interval and PO,... ,pi _ 1 are the N different policies. The computation starts with \na sampling phase. During this phase, the dynamic feedback algorithm executes each of the N policies for \nthe sampling interval S to derive overhead measurements ue, . . . , VN-1 for each of the N policies. \nThe overhead is the proportion of the total execution time spent executing lock constructs or waiting \nfor other processors to release locks. The overhead therefore varies between zero (if the computation \nnever executes a lock construct) and one (if the computation performs no useful work). In the worst case, \nmultiple policies have the same overhead v during the sampling phase, and v is the lowest sampled overhead. \nWithout loss of generality, we assume that the dynamic feed- back algorithm executes policy po during \nthe production interval. We also assume that, at any time t during the production phase, the policy overheads \nare bounded above by the exponential decay func- tion 1 + (u - l)e-A* (here X is the rate of decay). \nIn the worst case, the overhead function 00(t) of policy po actually hits this bound: o&#38;) = 1+ (u \n-l)P (1) We define the amount of useful work performed by a given pol- icy p; over given period of time \nT by: Work; = (1 -oi(t))dt JT 0 The dynamic feedback algorithm performs the following amount of work \nduring the production phase: Our worst-case analysis conservatively assumes that no useful work at all \ntakes place during the sampling phase. For the dynamic feed- back algorithm, Work,, is therefore the \ntotal amount of useful work performed during the SN + P units of time that make up the sam- pling and \nproduction phases. We now turn our attention to the optimal algorithm. When it begins executing, multiple \npolicies have the same lowest overhead V. We assume that, at any time t during the first P time units, \nthe policy overheads are bounded below by the exponential decay function vemxt. In the worst case, the \noverhead function of one of the policies, say policy pl , actually hits the bound: 01(t) = ueexf In this \ncase the optimal algorithm will execute policy pr for the first P time units, and the total useful work \nperformed during this interval is: Work; = p -$1 -e-xp) (5) We make the conservative assumption that, \nfor the next SN time units, the optimal algorithm executes a policy with no overhead at all -in other \nwords, that it performs SN units of work during this time period. We now compare the amountsof work performed \nby the worst- case dynamic feedback algorithm and the best case optimal algo- rithm over the time period \nP + SN: 1 -XP WorkP+SN -Wor~PfSN = SN + P + Xe 1 -; (6) We next discuss the conditions under which we \ncan obtain a guaranteed performance bound for the dynamic feedback algorithm relative to the optimal \nalgorithm. We start by defining a precise way to compare policies: Definition 1 Policy pi is at most \ne worse than policy pj over a time intervalT ifWork? -Work? 5 Te. Given a decay rate X, an effective \nsampling interval S, a num- ber of policies N and a desired performance bound e. Definition 1 yields \nthe following inequality, which determines if it is possible to choose a production interval P such that \nthe dynamic feedback algorithm is guaranteed to be most e worse than the optimal algo- rithm. If so, \nthe inequality also characterizes the values of P that are guaranteed to deliver the desired performance. \n (I -e)~ + tesAp 5 (e - 1)SN + r (7) Conceptually, several things are happening in this inequality. First, \nthe production interval P must be long enough to successfully amor- tize the sampling time SN. Second, \nthe production interval P must be short enough so that the dynamic feedback algorithm de- tects policy \noverhead changes quickly enough to avoid executing an inefficient policy for a long time. In other words, \nthe inequality bounds P both below and above. Finally, the decay rate X must be small enough so that \nthe dynamic feedback algorithm can perform enough work relative to the optimal algorithm to obtain the \nbound. In some cases, it is impossible to choose a P that satisfies the conditions. If it is possible \nto choose such a P. the inequality i&#38;n- tifies a feasible region in which P is guaranteed to satisfy \nthe con- ditions. Figure 3 graphically illustrates the range of feasible values for the production interval \nP using the following example values: S = 1.0, N = 2, X = 0.065 and e = 0.5. The inequality also pro- \n17 /I 10.5 0.5-r + (1 .am.ws)'(wp(-n.o55'x)) t A I 0 5 25 30 LAP(A 12 Figure 3: Feasible Region for Production \nInterval P vides insight into various relationships. As e increases, tbe range of feasible values for \nP also increases. As S increases, the range of feasible values for P decreases. We next show how to determine \nthe optimal value of P un-der our worst-case assumptions. We define the optimal value for P as the value \nthat minimizes the worst-case difference in work performed per unit time by the optimal and dynamic feedback \nal- gorithms. Equation 8 defines this difference. WorkP+SN-WorkP+SN SN+P+ e-XP-i 1 0 (8)  P+SN = P+>N \nFinding the root of the first derivative and solving for P yields the following equation. The value Popt \nthat satisfies this equation is the optimal value of P. It is possible to use numerical methods to solve \nfor Popt. + Aggressive I6 -w-Dynamic Feedback .: 14 + Bounded -: CXP(P+SN+$= i (9) t For the example \nvalues used in Figure 3, the optimal value of P is Popt FL?7.25.  6 Experimental Results This section \npresents experimental results that characterize how well dynamic feedback works for three benchmark applications. \nThe applications are Barnes-Hut [4], a hierarchical N-body solver, Water [38], which simulates water \nmolecules in the liquid state, and String [ 191, which builds a velocity model of the geology be- tween \ntwo oil wells. Each application is a serial C++ program that performs a computation of interest to the \nscientific comput- ing community. Barnes-Hut consists of approximately 1500 lines of code, Water consists \nof approximately 1850 lines of code, and String consists of approximately 2050 lines of code. We used \nour prototype compiler to parallelize each application. This paralleliza- tion is completely automatic \n-the programs contain no pragmas or annotations, and the compiler performs all of the necessary anal- \nyses and transformations. To compare the performance impact of the different synchronization optimization \npolicies, we used com- piler flags to obtain four different versions of each application. One version \nuses the Original policy, another uses the Bounded policy, another uses the Aggressive policy, and the \nfinal version uses dy-namic feedback. We report results for the applications running on a 16 processor \nStanford DASH machine [25] running a modified version of the IRIX 5.2 operating system. The programs \nwere compiled using the IRIX 5.3 CC compiler at the -02 optimization level. 6.1 Barnes-Hut Table 2 presents \nthe execution times for the different versions of Barnes-Hut. Figure 4 presents the corresponding speedup \ncurves. All experimental results rue for an input data set of 16,384 bod-ies. The static versions (Original, \nBounded and Aggressive) exe cute without the instrumentation required to compute the locking or waiting \noverhead. The Dynamic version (the version that uses dynamic feedback), must contain this instrumentation \nbecause it uses the locking and waiting overhead measurements to determine the best synchronization optimization \npolicy.* Version Processors 1 2 4 8 12 16 serial 147.8 - - - - - odginsl 217.2 111.8 56.59 32.61 20.78 \n15.84 Btnln&#38;d 191.7 97.25 49.22 26.98 19.62 15.12 k.@=i= 149.9 76.30 37.81 21.88 15.57 12.87 4acunic \n158.3 60.37 41.00 24.27 17.22 13.85 Table 2: Execution Iimes for Barnes-Hut (seconds) aStrictlv soe&#38;ian. \nthe lhmic version onlv needs to execute insmmmted code during theba;nptiag&#38;we. But because the ir&#38;tnmt&#38;on \novabead does not sig- nificanlty alTea he pesformmw tbcpdu&#38;onphaesimplyaccutcstberpmin-shlmlentedcodeRsrbebestdonintbepm4onssurplingpbase. \nThisapproach inhibilp code gmwh by eliminating the need to genmte instrumati and uninstm- roomed vasions \nof the code. -0 2 4 6 6 10 12 I4 16 Number of F rocessors Figure 4: Speedups for Barnes-Hut For this \napplication, the synchronization optimization policy has a significant impact on the overall performance, \nwith the Ag- gressive version significantly outperforming both the Original and the Bounded versions. \nThe performance of the Dynamic version is quite close to that of the Aggressive version. Table 3 presents \nthe locking overhead for the different versions of Barnes-Hut. The execution times ate correlated with \nthe locking overhead. For all versions except Dynamic, the number of executed acquire and release constructs \n(and therefore the locking overhead) does not vary as the number of processors varies. For the Dynamic \nversion, the number of executed acquire and release constructs in- creases slightly as the number of \nprocessors increases. The mnn- bers in the table for the Dynamic version are from an eight proces- sor \nrun. 1 Executed Acmite 1 Ab~hteLockinn 1 AndRelea~ehim 1 orclhead(~eco&#38;) Original 1 15,471,682 1 \n77.4 Bo&#38;ded 7,744,033 I 38.7 AgglCSSivt 49,152 0.246 Dynamic 72,050 0.360 Table 3: Locking Overhead \nfor Barnes-Hut Although the absolute performance varies with the synchro- nization optimization policy, \nthe performance of the different ver- sions scales at approximately the same rate. This indicates that \nthe synchronization optimizations introduced no significant false ex- clusion. The reason that this application \ndoes not exhibit perfect speedup is that the compiler is unable to parallelize one section of the computation. \nAt large numbers of processo rs the serial execn- tion of this section becomes a bottleneck [33]. To \ninvestigate how the overheads of the different policies change over time, we produced a version of the \napplication with small tar- get sampling and production intervals. We instrumented this ver- sion to \nprint out the measured overhead at the end of each sampling interval. Figure 5 presents this data from \nan eight processor run in the form of a time series graph for the main computationally in-tensive parallel \nsection, the FORCES section. Our benchmark ex-ecutes the FORCES section two times. The gap in the time \nseries lines corresponds to the execution of a serial section of the code. Figure 5 shows that the measured \noverheads stay relatively stable over time. 0.5 1 0.41 F0.3 - 7 Origina 8 zio.2 7 /------ B0undcd d \n: Aggressive 0 5 10 I5 20 25 ~ecutim Time @cconds) Figure 5: Sampled Overhead for the Barnes-Hut FORCES \nSection on Eight Processors We next discuss the characteristics of the application that relate to the \nminimum effective sampling interval for the FORCES sec-tion. The computation in this section consists \nof a single parallel loop. Table 4 presents the mean section size, the number of iter- ations in the \nparallel loop, and the mean iteration size. The mean section size is the mean execution time of the FORCES \nsection in the serial version, and is intended to measure the amount of use- ful work in the section. \nBecause the generated code cheeks for the expiration of sampling and production intervals only at the \ngran- ularity of the loop iterations, the sizes of the loop iterations have an important impact on the \nsize of the minimum effective sampling interval. Mean Section Size 1 Number of Iterations 1 Mean Iteration \nSize 69.14seconds 1 16,384 1 4.2 milliseconds Table 4: Statistics for the Barnes-Hut FORCES Section \nWe used the version with small target sampling and production intervals to measure the minimum effective \nsampling intervals for each of the different synchronization optimization policies. In this version, \nthe sampling and production intervals are as small as pos- sible given the application characteristics \n-in other words, the actual intervals are the same length as the minimum effective sam- pling intervals. \nWe instrumented this version to measure the length of each actual sampling interval, and used the data \nto compute the mean minimum effective sampling interval for each policy. Table 5 presents the data from \nan eight processor run. As expected, the mean minimum effective sampling intervals are larger than but \nstill roughly comparable in size to the mean loop iteration size. The differences in the mean minimum \neffective sampling intervals are correlated with the differences in lock overhead. As the lock over- \nhead increases, the amount of time required to execute each itera- tion also increases. Because none \nof the versions have significant waiting overhead, the increases in the amount of time required to execute \neach iteration translate directly into increases in the mean minimum effective sampling interval. We \nnext consider the impact of varying the target sampling and production intervals. For the performance \nnumbers in Table 2, the target sampling interval was set to ten milliseconds and the target production \ninterval was set to 1000 seconds. This target sampling interval was small enough to ensure that the minimum \neffective sampling interval, rather than the target sampling interval, deter- Version 1 Mean Minimum \nEffective Sampling Intetwl (milliseconds) Original 1 10 Bounded 7.8 Aggressive 6.5 Table 5: Mean Minimum \nEffective Sampling Intervals for the Barnes-Hut FORCES Section on Eight Processors mined the length of \neach actual sampling interval. A target produc- tion interval of 1000 was long enough to ensure that \neach parallel section finished before it executed another sampling phase. The ex- ecution of each parallel \nsection therefore consisted of one sampling phase and one production phase. Table 6 presents the mean \nexecution times of the FORCES sec- tion running on eight processors for several combinations of target \nsampling and production intervals. The performance is relatively insensitive to the variation in the \ntarget sampling and production intervals. Even when the target sampling and production inter-vals are \nidentical (which means the computation spends approxi- mately three times as long in the sampling phase \nas in the produc- tion phase), the section only runs approximately 20% slower than with the best combination. \nTarget Sampling TIUgetPiWhCliRoductionLoanal hervaI I second 5szccmds IOsecoads 1OOlsccoads 0.01 secalds \n9.138 9.058 9.058 9.025 0.1 seconds 9.691 9.178 9.122 9.220 l.OStXOOdS 10.784 9.834 9.726 9.670 Table \n6: Mean Execution Times for Varying Production and Sam- pling Intervals for the Barnes-Hut FORCES Section \non Eight Pro- cesms (seconds) 6.2 Water Table 7 presents the execution times for the different versions \nof Water. Figure 6 presents the corresponding speedup curves. All ex- perimental results are for an input \ndata set of 512 molecules. The static versions (Original, Bounded and Aggmssive) execute with- out the \ninstrumentation required to compute the locking or waiting overhead. The Dynamic version needs the instrumentation \nto ap ply the dynamic feedback algorithm, so this version contains the instrumentation. Vasion 1 1 1 \n1 2 1 4 1 8 1 12 1 16 Serial 1165.8 I - I - I - I - I - Table 7: Execution IImes for Water (seconds) \n For this application, the synchronization optimization policy has a significant impact on the overall \nperformance. For one pro- cessor, the Aggressive version performs the best. As the number of processors \nincreases, however, the Aggressive version fails to scale, and the Bounded version outperforms both the \nAggressive and the Original versions. As the performance results presented --t Bounded 16 --C Dynamic \nFeedback . . 14 + Original .- Aggressive ..:\" 12 - t 0 2 4 6 8 10 12 14 16 Number of Processors Figure \n6: Speedups for Water Mow indicate, false exclusion causes the poor performance of the Aggressive version. \nThe performance of the Dynamic version is very close to the performance of the Bounded version, which \nex- hibits the best performance. Table 8 presents the locking overhead for the different versions of \nWater. For the Original. Bounded and Dynamic versions, the execution times am correlated with the locking \noverhead. For all versions except Dynamic, the number of executed acquire and re- lease constructs (and \ntherefore the locking overhead) does not vary as the number of processors varies. For the Dynamic version \nat two processors and above, the number of executed acquire and re- lease constructs is very close to \nthe Bounded version, with a slight increase as the number of processors increases. At one proces-sor, \nthe Dynamic version executes approximately the same number of acquire and release constructs as the Aggressive \nversion. The numbers in the table for the Dynamic version are from an eight processor run. Table 8: Locking \nOverhead for Water We instrumented the parallel code to determine why Water does not exhibit perfect \nspeedup. Figure 7 presents the waiting prupor-tion, which is the proportion of time spent in waiting \noverhead.3 These data were collected using program-counter sampling to pro- file the execution [16,23]. \nThis figure clearly shows that waiting overhead is the primary cause of performance loss for this applica- \ntion, and that the Aggressive synchronization optimization policy generates enough false exclusion to \nseverely degrade the perfor- mance. Water has two computationally intensive parallel sections: the INTERF \nsection and the FYYIENG section. Figures 8 and 9 present %loreptecisely. the waitiagpp&#38;onisthewmoverall \npmcearors ofthe amount of rim that each processor spends waiting to acquire a lock held by another processor \ndivided by the execution time of the pgmm timzs tlx number of pmrsMKs executing the cQmputati00. 79 \n+ Aggressive 1.0 --t Bounded c 0 2 4 6 6 10 12 14 16 Figure 7: Waiting Proportion for Water 0.5 0.4 \n-8 g 0.3 d B 80.2 d wf sr* d -3 original 0.1 d 4 f-J d- BolNldCd 0 Ib ;o ;o 4b Execution Time (aeconda) \n;o Qo Figure 8: Sampled Overhead for the Water INTERF Section on Eight Processors 1 pq/ &#38; Aggnsriw \n 0.8 fJ+ 6 0.6 ii 0.4 ---A%#*- 0 0 Ib ;o ;o 4b At 6il Bxecutlon lime (secouds) 0.21 Figure 9: Sampled \nOverhead for the Water IVIENG Section on Eight Processors time series graphs of the measured overheads \nof the different syn- chronization optimization policies. For the INTERF section, the generated code \nwould be the same for the Bounded and Aggres- sive policies. The compiler therefore does not generate \nan Aggres- sive version, and the sampling phases execute only the Original and Bounded versions. A similar \nsituation occurs in the PCXENG sec- tion, except that in this case, the code would be the same for the \nOriginal and Bounded versions. As for Barnes-Hut, the overheads are relatively stable over time. The \ngaps in the time series graphs correspond to the executions of other serial and parallel sections. Tables \n9 and 10 present the parallel section statistics for the IN- TERF and POTENG sections. Tables 11 and \n12 present the mean minimum effective sampling intervals for the two sections. As ex- pected, the mean \nminimum effective sampling intervals for all of the versions except the Aggressive version in the PUI \nENG section are larger than but still roughly comparable to the corresponding mean iteration sizes. The \nmean minimum effective sampling in- terval for the Aggressive version in the POTENG section is sig- nificantly \nlarger than for the Original version. We attribute this difference to the fact that the Aggressive policy \nserializes much of the computation, which, as described in Section 4.1, increases the effective sampling \ninterval. Mean Section Size 1 Number of Iterations 1 Mean Iteration Size 20.80 seconds 1 511 1 40.7 \nmiltiscconds Table 9: Statistics for the Water INTERF Section Mean Section Size 1 Number of Iterations \n1 Mean Iteration Size 16.34seconds 1 511 1 32.0 milliseconds Table 10: Statistics for the Water PDl ENG \nSection Version Mean Minimum Effective Sampling Interval (milliseconds) chigird 93 Bounded 82 Table 11: \nMean Minimum Effective Sampling Intervals for the Wa- ter INTERF Section on Eight Processors Version \nMean Minimum Effective Sampling Interval (milliseconds) original 59 Aggnzssive 286 Table 12: Mean Minimum \nEffective Sampling Intervals for the Wa- ter PUIXNG Section For the performance numbers in Table 7, the \ntarget sampling interval was set to ten milliseconds and the target production in- terval was set to \n1000 seconds. This combination ensured that the execution of each parallel section consisted of one sampling \nphase and one production phase. Tables 13 and 14 present the execution times for the INTERF and POTENG \nsections running on eight pro- cessors for several combinations of target sampling and production intervals. \nFor the INTERF section, all of the combinations yield approximately the same performance. We attribute \nthis uniformity to the fact that the performance of the two versions in the section (the Original and \nBounded versions) is not dramatically different. For target production intervals of one and five seconds, \nthe per- formance of the POTENG section is quite sensitive to the choice of target sampling interval. \nThere is a dramatic difference in this sec- tion between the performance of the Aggressive and Original \nver-sions. In this case, one would intuitively expect the performance Table 13: Mean Execution limes \nfor Varying Production and Sam- pling Intervals for the Water INTERF Section on Eight Processors (seconds) \nTarget Sampling Target Production herval Interval I second 5 seconds 10 secoads loo0 seconds 0.01 seconds \n3.669 3.541 3.507 3.552 0.1 seconds 3.733 3.532 3.529 3.566 1 .o seconds 3.713 3.695 3.676 3.673 Target \nSampling Target F roduction herval Interval I second 5 seconds IO seconds 1OOOseeonds 0.01 seconds 3.32 \n2.624 2642 2.649 0.1 seconds 3.072 2.690 2:709 2.124 I .O seconds 4.184 3.482 3.479 3.489 Table 14: \nMean Execution limes for Varying Production and Sam- pling Intervals for the Water PCYI ENG Section on \nEight Processors (seconds) to increase with increases in the target production interval and de- crease \nwith increases in the target sampling interval. We address the ways in which the data fail to conform \nto this expectation. First, the execution times are virtually identical at target pro- duction intervals \nof five, ten and 1000 seconds. We attribute this uniformity to the fact that the execution of the POTENG \nsection always terminates in less than five seconds. Extending the target production interval beyond \nfive seconds therefore has no effect on the execution. Second, the execution times are virtually identical \nfor a target production interval of 5.0 seconds and target sampling intervals of 0.01 and 0.1 seconds. \nWe attribute these data to the fact that the execution of the PUIENG section always terminates in less \nthan five seconds and the fact that the minimum effective sampling in- terval for the Aggressive policy \nis greater than 0.1 seconds. Both of the executions in question consist of an Aggressive sampling inter- \nval whose length is the same in both executions. an Original sam-pling interval, then an Original production \ninterval during which the section completes its execution. Both executions spend almost identical amounts \nof time executing the Aggressive and Original versions. Finally, the execution time decreases for a target \nproduction in- terval of 1.0 seconds when the target sampling interval increases from 0.01 seconds to \n0.1 seconds. The effect is caused by the fact that the minimum effective sampling interval of the Original \nver-sion is smaller than 0.1 seconds, while the minimum effective sam- pling interval of the Aggressive \nversion is larger than 0.1 seconds. The program therefore spends a larger proportion of the sampling \nphase executing the more efficient Original version with a target sampling interval of 0.1 seconds than \nit does with a target sam- pling interval of 0.01 seconds. An effect associated with the end of the section \nexacerbates the performance impact. With a target sampling interval of 0.1 seconds, the section completes \nafter two sampling phases and two production phases. With a target sam- pling interval of 0.01 seconds, \nthe section performs less compu- tation in the Original sampling intervals, and it does not complete \nuntil after it has executed a third Aggressive sampling interval. The net effect of the increase in the \ntarget sampling interval is a signifi- cant reduction in the amount of time spent executing the inefficient \nAggressive sampling intervals.    6.3 String For String, the Bounded policy produces the same parallel \ncode as the Original policy. We therefore report performance results for only the Original, Aggressive \nand Dynamic policies. Table 15 presents the execution times for the different versions of String. Figure \n10 presents the corresponding speedup curves. All experi- mental results are for the Big Well input data \nset. The static ver- sions (Original and Aggressive) execute without the instrumenta- tion required to \ncompute the locking or waiting overhead; the Dy- namic version includes the instrumentation. Version \nI PIWXSSOCS 1 1 I 2 1 4 1 8 1 12 1 16 mIff:i:z:: 2337.7 2313.5 I -I -, m3:: 1 y; 2231.9 2244.3 I -I -1 \n;;::I 1 y::: 2254.8 2260.9 Table 15: Execution Times for String (seconds) Dynamic Feedback.. . . Number \nof Processors Figure 10: Speedups for String For String, the Aggressive policy completely serializes \nthe com- putation. This version therefore fails to scale at all. The execution time of the Dynamic version \nis comparable to the execution time of the Original version, with a small loss of performance at 12 and \n16 pCtNOrS. Table 16 presents the locking overhead for the different versions of String. For the Dynamic \nversion at two processors and above, the number of executed acquire and release constructs is slightly \nless than in the Original version. The number also increases slightly as the number of processors increases. \nAt one processor, the Dynamic version executes approximately six times fewer acquire and release constructs \nthan the Original version. The numbers in the table for the Dynamic version are from an eight processor \nrun. We instrumented the parallel code to determine why String does not exhibit perfect speedup. Figure \n11 presents the waiting propor- tion. This figure clearly shows that waiting overhead is the pri- mary \ncause of performance loss for this application, and that the Aggressive synchronization optimization \npolicy generates enough false exclusion to serialize the computation. Figure 12 presents time series \ngraphs of the measured over-heads of the different synchronization optimization policies for the Version \n1 Executed Acquire I AbsoluteLocking 1 And Release Pairs 1 Ovuhead &#38;co&#38;) Orininal I 30.286.596 \nI 151.43 A&#38;sive 2; 313 0.01156 Dynamic 30,016,913 150.08 Table 16: Locking Overhead for String 1.0 \n. g 0.6 - 2 &#38; 6o 0.6 z :+ 0.4 - 3 0.2 -    o.o- 0 2 4 6 6 10 12 14 16 Number of Processors Figure \n11: Waiting Proportion for String I- Aggressive 0.8- B u 0.6 d -2 $o.4 0.2- w--Original Oi 0 Iho 2&#38;l \n3&#38;l do ito Execution Tie (seconds) Figure 12: Sampled Overhead for the String PROJFWD Section on \nEight Processors MeanSectionSizc I Numterof Iterations 1 McanIteradonSize solsemnds 1 28.288 1 28.3 milliseconds \nTable 17: Statistics for the String PROJFWD Section Version Mean Minimum Elktive Sampling Interval . \n%v~ 54 Ilisccwds Agpessin 26o~Riscco* Table 18: Mean Minimum Effective Sampling Intervals for the String \nPROJFWD Section main computationally intensive parallel section, the PROJFWD sec- tion. We collected \nthese data by setting the target sampling and production intervals to one second, then instrumenting \nthe code to print out the measured overhead at the end of each sampling in- terval. As for Barnes-Hut \nand Water, the overheads are relatively stable over time. The gaps in the time series graphs correspond \nto the executions of other serial and parallel sections. Table 17 presents the parallel section statistics \nfor the PRO- JFWD section. Table 18 presents the mean minimum effective sam- pling intervals. The mean \nminimum effective sampling interval for the Original version is larger than but roughly comparable to \nthe iteration size. As in the POTENG section of Water, the Aggressive version is significantly larger \nthan for the Original version. The reason is the same: the Aggressive version serializes much of the \ncomputation. For the performance numbers in Table 15. the target sampling interval was set to ten milliseconds \nand the target production in- terval was set to 1000 seconds. This combination ensured that the execution \nof each parallel section consisted of one sampling phase and one production phase. Table 19 presents \nthe execution times for the PROJFWD section running on eight processors for several combinations of target \nsampling and production intervals. As ex- pected for a section with dramatic efficiency differences between \nthe versions, the performance increases with increases in the tar- get production interval and decreases \nwith increases in the target sampling interval. Target Sampling Target l mdwtioo btend btcwal I.sumd \n1 Sscconds 1 lOseuml8 ~1WOseconds 0.01 seconds I 140.6 I 117.1 I 114.7 I 112.54 112.60 112.96 Table 19: \nMean Execution limes for Varying Production and Sam- pling Intervals for the String PROJFWD Section on \nEight Proces- sors (seconds)  6.4 Discussion For each application, the best static synchronization optimization \npolicy is different from that of the other two applications. Furtber- more, the performance differences \nare significant -at 16 proces- sors, the best version of Barnes-Hut is approximately 209b faster than \nthe worst: for Water, the best is more than three times faster than the worst; for String, the best is \nmore than ten times faster than the worst. In all of these cases, dynamic feedback allows the Dy- namic \nversion to exhibit performance that is not only very close to that of the best static policy, but also \nalmost always better than that of the next best static policy. The compiler can therefore automat- ically \ngenerate robust code that performs well in a variety of envi- ronments, which eliminates the need for \nthe programmer to manu- ally tune the program to use the best synchronization optimization policy. 7 \nRelated Work Many researchers have developed systems that collect information about the dynamic chamcteristics \nof programs, then use that in- formation to improve the performance. We discuss several ap- proaches: \nprofiling, dynamic type feedback techniques for improv- ing the performance of object-oriented languages, \nadaptive execu- tion techniques and dynamic techniques for parallelizing loops. We also discuss dynamic \ncompilation, efficient implementations of par- allel function calls, and related work in synchronization \noptimiza-tion. 7.1 Profiling Profiling is a standard way to obtain infotmation about the dynamic characteristics \nof a program. In this approach, the program is in- strumented, then executed to collect profiling data. \nThe program can then be recompiled, with the profiling data used to guide policy decisions in the compiler. \nProfiling has been used in the context of object-oriented lan-guages to predict the most frequently occurring \nclass of the receiver object at a given call site [17]. This information is then used to drive optimizations \nthat inline methods based on predictions about the class of the receiver. Profiling has also been used \nto guide de- cisions to inline procedures in C programs [7]. to drive instruction scheduling algorithms \n[8], to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register \nallo- cation [28, 391, and to direct the compiler to frequently executed parts of the program so that \nthe compiler can apply further opti- mizations [ 131. Brewer [5] describes a system that uses statistical \nmodeling to automatically predict which algorithm will work best for a given combination of input and \nhardware platform. The different algo- rithms are implemented by hand, not automatically generated Born \na single specification. The system uses profiling to characterize the performance of the different algorithms \non the different hardware platforms. Dynamic feedback differs from profile-based feedback in that it \ncan adapt dynamically to the current execution environment, rather than hoping that the environment is \nsimilar to the environment in the profiling run of the program. Dynamic feedback can also ad- just to \nchanges that occur within a single execution. Profile-based approaches collect a single aggregate set \nof measurements for the entire execution, and can therefore miss environment changes that take place \nwithin a single execution. 7.2 Adaptive Execution Techniques Other researchers have recognized the need \nto use dynamic perfor- mance data to optimize the execution [9,35,36]. These approaches are based on \na set of control variables that parameter&#38; a given al- gorithm in the implementation. An example \nof a control variable is the prefetch distance in an algorithm that prefetcbes data accessed by a loop \n[36]. Typically, the programmer defines the set of ob- servable variables and a feedback function that \nuses the observable variables to produce valuesfor the control variables. changes in the values of the \nobservable variables propagate through the feed- back function to change the control variables, and the \nprogram re- sponds by modifying its behavior. IdealIy, the observable variables, control variables and \nfeedback function are defined so that the pm- gram maximizes its performance across a range. of dynamic \nenvi- ronments. While dynamic feedback is similar in spirit to these approaches, there is an important \ndifference. Dynamic feedback is a general technique designed to choose between several discrete, and \npoten- tially quite different, implementations. Other approaches are de- signed to tune one or more control \nvariables in the context of a single algorithm. 7.3 Dynamic Dispatch Optimizations In object-oriented \nlanguages, the method that is invoked at a given call site depends on the dynamic class of the receiver \nobject. The same call site may therefore invoke many different methods; the algorithm that determines \nwhich method to invoke is called the dynamic dispatch algorithm. Researchers have proposed several adaptive \noptimizations for improving the efficiency of dynamic dis- patch. The standard mechanism is to collect \ndata that indicates which methods tend to be invoked from which call sites, then to insert a type test \nthat checks for common types first [6]. Dynamic type feedback is designed to direct the compiler s at- \ntention to parts of the program that would benefit from optimiza- tion 1201. Once a method has been optimized, \nthe generated code continues to collect data that can be used to drivefurther optimiza- tions and reverse \npoor implementation choices. In this sense, dy- namic feedback is similar to dynamic type feedback in \nthat both techniques generate code that dynamically adapts to its execution environment. 7.4 Run-Time \nAnalysis and Speculative Execution In certain circumstances, a lack of statically available information \nmay prevent the compiler from parallelizing the program. Several systems address this problem by parallelizing \nprograms dynami- cally using information that is available only as the program runs. The inspector/executor \napproach dynamically analyzes the values in index arrays to automatically parallelize computations that \nac- cess irregular meshes [26, 371. The Jade implementation dynami- cally analyzes how tasks access datato \nexploit the concurrency in coarse-grain parallel programs [34]. Speculative approaches opti- mistically \nexecute loops in parallel, rolling back the computation if the parallel execution violates the data dependences \n[32]. A major difference between dynamic feedback and these run- time techniques is that dynamic feedback \nis designed to automati- cally choose between several implementations that deliver the same functionality. \nBach implementation is equally valid, and may very well perform the best in the current environment. \nIn all of the run- time techniques, the goal is clearly to parallelize the computation, but the compiler \nsimply lacks the information necessary to do so. It must therefore postpone the decision to apply the \noptimization until run-time, when the information is available. 7.5 Dynamic Compilation Dynamic compilation \nsystems enable the generation of code at run time [3, 11, 241. Because delaying the compilation until \nrun time provides the compiler with information about the concrete values of input parameters, the compiler \nmay be able to generate more efficient code. Existing research has focused on providing efficient mechanisms \nfor dynamic compilation. We see dynamic compilation as one way to generate the differ- ent implementations \nthat dynamic feedback samples to find a best implementation. The advantage would be the elimination of \npoten- tial code growth -the memory used to hold the generated code can be deallocated if the code will \nnot be executed for a signifi- cant period of time. The compiler could dynamically regenerate the code \nwhen the dynamic feedback algorithm needs to sample its performance. The major drawback would be the \noverhead required to per- form the compilation dynamically. This overhead would become less of a concern \nif the program executed sampling phases very infrequently -the dynamic compilation overhead would be \namor- tized away by the long production phases. 7.6 Synchronization Optimizations This paper applies \ndynamic feedback to the problem of choosing the best synchronization granularity.Our previous research \npro- duced analyses and transformations for reducing the synchroniza- tion overhead and the different \nsynchronization optimization poli- cies [lo]. Plevyak. Zhang and Chien have developed a similar syn- \nchronization optimization technique, access region -ion, for concurrent object-oriented programs [31]. \nBecause access region expansion is designed to reduce the overhead in sequential execu-tions of such \nprograms, it does not address tbe trade off between lock overhead and waiting overhead. The goal is simply \nto mini- mize the lock overhead. 7.7 Parallel Function Calls Several researchers have developed efficient \nimplementations for parallel function calls [ 15,27,30]. These implementations dynam- ically match the \namount of exploited parallelism to the amount of parallelism available on the parallel hardware platform \nby select- ing between an efficient sequential call and a full parallel call. The selection is based \non a dynamic measure of the difference between the currently exploited and available amounts of parallelism. \n 8 Conclusion This paper presents a new technique, dynamic feedback, that en- ables computations to \nadapt dynamicalIy to different execution en- vironments. A compiler that uses dynamic feedback produces \nsev- eral different versions of the same source code; each version uses a different optimization policy. \nDynamic feedback automatically chooses the most efficient version by periodically sampling the per- formance \nof the different versions. We have implemented dynamic feedback in the context of a parallelizing compiler \nfor object-based programs. The generated code uses dynamic feedback to automatically choose the best \nsyn- chronization policy. Our experimental results show that dynamic feedback enables the compiler to \nautomatically generate code that exhibits performance comparable to that of code that has been man- ually \ntuned to use the best synchronization optimization policy. We see dynamic feedback as part of a general \ntrend towards adaptive computing. As the complexity of systems and the capa- bilities of compilers increase, \ncompiler developers will find that they can automatically apply a large range of transformations, but \nhave no good way of statically determining which transformations will deliver good results when the program \nis actually executed. The problem will become even more acute with the emergence of new computing paradigms \nsuchas mobile programs in the Internet. The extreme heterogeneity of such systems will defeat any imple- \nmentation that does not adapt to different execution environments. Dynamic feedback is one example of \nthe adaptive techniques that will enable compilers to deliver good performance in modem com- puting systems. \n References [l] S.P. Atnarasinghe and M.S. Lam. Communication optimization and code generadon for distributed-memory \nmachines. In Pmccecfings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, \nSIGPLAN Notices 28(7). ACM, July 1993. [2] J.M. Anderson and M.S. Lam. Global optimizatious for Parallelism \nand locality machines. In Proceedings of the SIG- onscalable parallel PLAN 93 Conference on Programming \nLunguage Design and Imple- mentation, SIGPLAN Notices 28(7). ACM, July 1993. 131J. Auslander, M. Philipose, \nC. Chambers, S. Eggers, , and B. Bershad. Fast, effective dynamic compilation. In Proceedings of the \nSIGPLAN 96 Conference on Program Language Design and Implementation, Philadelphia PA, May 1996. [41 J. \nBarnes and P. Hut. A hierarchical O(NlogN) force-calculation al-gorithm. Nature, pages 446-449, December \n1976. 151 E. Brewer. High-level optimization via automated statistical model- ing. In Pmceedings of the \nFph ACM SIGPLAN Symposium on Prin- ciples and Practice of Parallel Programming, Santa Barbara, CA, July \n1995. WI C. Chambers and D. Ungar. Customization: Optimizing compiler technology for SELF, a dynamically-typed \nobject-oriented program-ming language. In Pmceedings of the SIGPLAN 89 Conference on Program Language \nDesign and Implementation, Portland, OR, June 1989. 171 P Chang, S. Mahlke, W. Chen, and W. Hwu. Profile-guided \nautomatic inline expansion for C programs. Software-Practice and Experience, 22(5):349-369, May 1992. \nPI W. Chen, S. MahIke, N. Warter, S. Anik. and W. Hwu. Profile-assisted instruction scheduling. htUMdOM1 \nJournal of Parallel Program- ming, 22(2):151-181, April 1994. 191 A. Cox and R. Fowler. Adaptive cache \ncoherency for detecting migra- tory shared data. In Proceedings of the 20th International Symposium on \nComputer Architecture, May 1993. r101 P. Diniz and M. Rinard. Synchronization transformations for parallel \ncomputing. In Pmceedings of the nenty-fourth Annual ACM Sym- posium on the Principles of Programming \nLanguages, Paris, France, January 1997. ACM. PII D. Engler. VCODE: A rctargetable. extensible, very fast \ndynamic code-generation system. In Proceedings of the SKPLAN 96 Con- ference on Program Language Design \nand implementation, Philadel-phia, PA, May 1996. WI B. Falsafi, A. Lebeck. S. Reinhardt, I. Schoinas, \nM. Hill, J. Lams. A. Rogers, and D. Wood. Application-specific protocols for user-level shared memory. \nIn Proceedings of Supercomputing 94, November 1994. [I31 M. Femandez. Simple and effective link-time \noptimization of modula- 3 programs. In Proceedings of the SIGPLAN 95 Conference on Pm- gram Language \nDesign and Implementation, La Jolla, CA, June 1995. v41 S. Freudenberger, J. Schwartz, and M. Sharir. \nExperience with the SETL optimizer. ACM Transactions on Progmmming Languages and Systems, 5(1):2645. \nJanuary 1983. WI S. C. Goldstein, K. E. S&#38;user, and D. E. Culler. Lazy Threads: Implementing a fast \nparallel call. Jountal of Parallel and Distributed Computing, 37( 1):5-20, August 1996. 1161 S. Graham, \nP. Kessler, and M. McKusick. gprofi a call graph exe- cution profiler. In Proceedings of the SIGPLAN \n82 Symposium on Compiler Construction, Boston, MA, June 1982. 1171 D. Grove, J. Dean, C. Garret, and \nC. Chambers. Profile-guided re-ceiver class prediction. In Proceedings of the Tenth Annual Confer- ence \non Object-Oriented Programming Systems, Languages and Ap- plications, Austin, TX, October 1995. WI M. \nGupta and P. BanerJee. Demonstration of automatic data par- titioning techniques for parallelizing compilers \non multicomputers. IEEE Transactions on Parallel and Distributed Systems, 3(2):179-193, March 1992. t191 \nJ. Harris, S. Lazamtos. and R. Michelena. Tomographic string inver- sion. In 60th Annual International \nMeeting, Society of Exploration and Geophysics, Extended Abstracts, pages 82-85. 1990. ml U. Holzle and \nD. Ungar. Optimizing dynamically-dispatch calls with run-time type feedback. In Proceedings of the SIGPLAN \n94 Conference on Pmgmm Language Design and Implementation. Or-lando, FL, June 1994. PII K. Kennedy and \nU. Kremer. Automatic data layout for High Perfor- mance Fortran. In Proceedings of Supercomputing 95, \nSan Diego, CA, November 1995. WI G. Kiczales. Beyond the black box: open implementation. IEEE Software, \n13(l), January 1986. [231 D. Knutb. An empirical study of FORTRAN pmgrams. Sofnuare-Practice and Experience. \n1:105-133, 1971. r241 P. Lee and M. Leone. Optimizing ML with run-time code genera-tion. In Proceedings \nof the SIGPLAN 96 Conference on Program Language Design and Implementation. Philadelphia. PA, May 1996. \n[251 D. Lenoski. The Design and Analysis of DASH: A Scalable Directory- Bnred Multiprocessor. PhD thesis, \nStanford, CA, February 1992. 1261 S. Leung and J. Zahorjan. Improving the performance of runtime par- \nallelization. In Proceedings of the Fourth ACM SIGPZAN Symposium on Principles and Pmctice of Parallel \nProgramming, pages 83-91, San Diego, CA, May 1993. ~271 E. Mohr, D. Kranz. and R. Halstead. Lazy task \ncreation: a technique for increasing the granularity of parallel programs. In Proceedings of the 1990 \nACM Conference on Lisp and Functional Pmgmmming, pages 185-197, June 1990. WI W. Morris. Ccg: a prototype \ncoagulating code generator. In Pmceed-ings of the SIGPLAN 91 Conference on Program Lnnguage Design and \nImplementation, Toronto. Canada, June 1991. [291 K. Pettis and D. Hansen. Profile guided code positioning. \nIn Pmceed-ings of the SIGPL4N 90 Conference on Program Lunguage Design and Implementation, White Plains, \nNY, June 1990. [301 J. Plevyak. V. Karamcheti. X. Zhang, and A. Chien. A hybrid exe- cution model for \ntine-grained languages on distributed memory mul- ticomputers. In Proceedings of Supercomputing 9.5. \nSan Diego, CA, November 1995. t311 I. Plevyak, X. Zhang. and A. Chien. Obtaining sequential effi-ciency \nfor concurrent object-oriented languages. In Proceedings of the Twenty-second Annual ACM Symposium on \nthe Principles of Pm- gmmming Languages. ACM, January 1995. [321 L. Rauchwerger and D. Padua The LRPD \ntest: speculative run-time parallelization of loops with privatization and reduction paralleliza-tion. \nIn Pmceedings of the SIGPIAN 95 Conference on Pmgram Lunguage Design and Implement&#38;on, La Jolla, \nCA, June 1995. [331 M. Rinard and P Diniz. Commutativity analysis: A new analysis framework for Pd- lelizing \ncompilers. In Proceedings of the SIGPLAN 96 Conference on Program Lunguage Design and Implementation. \nPhiladelphia, PA, May 1996. (httpz//www.cs.ucsb.edu/~&#38;paper/pldi%.ps). WI M. Rinard, D. Scales, and \nM. Lam. Heterogeneous Parallel Pmgram- ming in Jade. In Proceedings of Supercomputing 92, pages 245-256, \nNovember 1992. [351 T. Romer, D. Lee, B.Bershad, and J. Chen. Dynamic page mapping policies for cache \nconflict resolution on standard hardware. In Pm-ceedings of the First USENIX Symposium on Operating Systems \nDe- sign and Impkmentation, pages 255 -266, Monterey, CA, November 1994. [361 R. Saavedm and D. Park. \nImproving the effectiveness of soliwam prefetching with adaptiveexecution. In Proceedings of the I996 \nCon-ference on Parallel Algorithms and Compilation Techniques (PACT %, Boston, MA, October 1996. 1371J. \nSaltz, H. Berryman. and J. Wu. Multiprocessors and run-time com- pilation. Concurrency: Practice &#38; \nExperience, 3(6):573-592, De-cember 1991. [381 J. Singh, W. Weber, and A. Gupta. SPLASH: Stanford parallel \nappli- cations for shared memory. Computer Architecture News, 20(1):5-44, March 1992. [391 D. Wall. Global \nregister allocation at link time. In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. \nACM, June 1986. 84  \n\t\t\t", "proc_id": "258915", "abstract": "This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.We have implemented dynamic feedback in the context of a parallelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.", "authors": [{"name": "Pedro C. Diniz", "author_profile_id": "81377592829", "affiliation": "Department of Computer Science, Engineering I Building, University of California, Santa Barbara, Santa Barbara, CA", "person_id": "PP39071146", "email_address": "", "orcid_id": ""}, {"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "Department of Computer Science, Engineering I Building, University of California, Santa Barbara, Santa Barbara, CA", "person_id": "P192533", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258923", "year": "1997", "article_id": "258923", "conference": "PLDI", "title": "Dynamic feedback: an effective technique for adaptive computing", "url": "http://dl.acm.org/citation.cfm?id=258923"}