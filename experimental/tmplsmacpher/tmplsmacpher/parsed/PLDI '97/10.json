{"article_publication_date": "05-01-1997", "fulltext": "\n Interprocedural Dataflow Analysis in an Executable Optimizer David W. Goodwin goodwin@vssad.hlo.dec.cm \n Digital Equipment Corporation Abstract Interprocedural dataflow information enables link-time and post-link-time \noptimizers to perform analyses and code transformations that are not possible in a traditional compiler. \nThis paper describes the interprocedural dataflow analysis techniques used by Spike, a post-link-time \noptimizer for Alpha/NT executables. Spike uses dataflow analysis to su mmarize the register definitions, \nuses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations \nthat require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, \nthe time required to perform interprocedural dataflow analysis could potentially be unacceptably long, \nlimiting Spike s effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact \nrepresentation of a program s intraprocedural and interprocedural control flow that efficiently summarizes \nthe register definitions and uses that occur in the program. Experimental results are presented for the \nSPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation \nallows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 \ninteger benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 \nthousand basic blocks, interprocedural dataflow analysis requires just 12 seconds. 1 Introduction Link-time \nand post-link-time optimizers can perform analyses and code transformations over an entire program, enabling \noptimizations that are not practical in a traditional compiler. Spike is a post-link-time optimizer for \nAlpha/NT executables, implementing Digital s executable optimization technology [Srivastava94, Permission \nto make digital/hard copy of part or all this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan-tage, the copyright notice, \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires \nprior specific permission and/or a fee PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-69791-907-6197/0006...$3.50 \n Wilson961. Spike performs a variety of profile-driven optimizations including code restructuring to \nimprove instruction cache performance [PettisgO], Hot-Cold optimization [Cohn96], and near-optimal global \nregister allocation [Goodwin96]. Because the entire program is available, Spike can summarize the register \ndefinitions, uses, and kills that occur external to the code segment being optimized, and then use those \nsummaries to perform optimizations that are impossible or impractical in a traditional compiler. Figure \n1 shows examples of two types of optimizations that are possible when a summary of the defined, used, \nand killed registers is available. In each example, summary information is shown inside of square brackets. \nFigure l(a) and Figure l(b) show code segments where Spike uses the summary information to eliminate \ndead code. In Figure l(a), register Ra is used to return a value from a called procedure. However, the \nsummary information shows that Ra is not used on return to any caller and thus the instruction defining \nRa can be deleted. In Figure l(b), registers Ra and Rb are used to pass arguments to a called procedure \nbut the summary information shows that the called procedure does not use the argument passed in Ra. Thus, \nthe instruction defining Ra can be deleted. Because . . . def Ra . . . def Rb def Ra call [ used by \ncall = {Rb) ] ret [usedonretum=IZI] . . . 64 (b) save Rs . . . def Rt def Rs store Rt call [ killed \nby call = 0 ) call [ killed by call = 0 ] use Rs load Rt . . . use Rt restore Rs (cl (4 Figure 1: Examples \nof optimizations enabled by register summary information. the calling procedure and the called procedure \nmay be in separately compiled modules, these optimizations are not available to a typical compiler. Figure \nl(c) and Figure l(d) show examples where the register summary information enables Spike to reallocate \nregisters to increase performance. In both figures a value is live across a call instruction. In Figure \nl(c) the compiler has assigned the value to caller-saved register Rt and spilled Rt around the call. \nThe register summary information reveals that Rt is in fact not killed by the call, and so Rt does not \nneed to be spilled around the call instruction. In Figure l(d) the compiler has assigned the value to \ncallee-saved register Rs, requiring a save and restore of Rs around the function. The register summary \ninformation shows that caller-saved register Rt is not killed by the call. Thus, Spike can assign the \nvalue to Rt instead of Rs and delete the save and restore of Rs. For large PC applications, call overhead, \nincluding the overhead to save and restore callee-saved registers, is high, accounting for as much as \n16% of the total execution time [Cohn96]. As a result, reassigning registers so that saves and restores \nare eliminated can potentially provide a significant performance improvement. Spike can only perform \nthese and similar optimizations because the external register definitions, uses, and kills are indicated \nin the surmnary information. Preliminary results show that these optimizations consistently provide performance \nimprovements of 5%10% and in some cases provide improvements of as much as 20%. A routine is a sequence \nof instructions generated for a high-level language procedure or function, typically having a single \nentry and one or more exits. A register definition that reaches a routine or a register use that causes \nthe register to be live at a routine could be separated from the routine by arbitrarily complex control \nflow containing multiple levels of calls, loops, recursion, etc. Thus, generating a summary of the registers \ndefined, used, and killed external to each routine requires interprocedural dataflow analysis. Interprocedural \ndataflow analysis can be performed using a program s entire control-flow graph (CFG) [Srivastava93]. \nA CFG for an entire program is constructed by connecting the CFG representing each routine with additional \narcs representing calls and returns between the routines (e.g., see Figure 2). Because the time require \nto perform interprocedural dataflow analysis is typically proportional to the size of the graph being \nanalyzed [Khedkar94], reducing the size of the graph over which the dataflow analysis is performed decreases \nthe analysis time. Moreover, because Spike is designed to optimize large PC applications, the time required \nto perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike s \neffectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation \nof a program s intraprocedural and interprocedural control flow that efficiently summarizes the register \ndefinitions and uses that occur in the program, and that is significantly smaller than the program s \nCFG. Using the compact representation makes it practical for Spike to analyze programs containing millions \nof instructions and hundreds of thousands of basic blocks. The remainder of the paper is organized as \nfollows. Section 2 describes how Spike summarizes the registers defined, used, and killed external to \neach routine. Section 3 describes the compact representation used to represent a program and the interprocedural \ndataflow analyses that produce the summary information for each routine. Section 4 presents experimental \nresults, Section 5 reviews related work, and Section 6 summarizes the paper. 2 Summarizing Register \nInformation For many optimizations, Spike analyzes and transforms one routine at a time. To optimize \na routine, Spike requires a summary of the register definitions that occur before the routine is entered, \nthe register uses that occur after the routine exits, and the register definitions, uses, and kills that \noccur during calls made by the routine. Thus, for each routine, Spike requires the following dataflow \ninformation: live-at-enrry: the registers live at each entrance to the routine. five-at-exit: the registers \nlive at each exit from the routine. cull-wed: as seen by the caller, the registers that may be used in \na called routine before they are defined. cull-defined: as seen by the caller, the registers that must \nbe defined in a called routine. cull-killed: as seen by the caller, the registers that may be overwritten \nin a called routine. When analyzing a routine, Spike uses the call-used, call-defined, and call-killed \nsets to summarize the register uses, definitions, and kills that occur during a call. Spike replaces \neach call instruction to routine P with a cull-swnmary insrruction. The registers call-used by P are \nused by the call-summary instruction, the registers call-defined by P are defined by the call-summary \ninstruction, and the registers call-killed by P are killed by the call-summary instruction. Spike uses \na routine s MAY-USE, MUST-DEF, and MAY-DEF dataflow sets to conservatively estimate the registers call-used, \ncall-defined, and call-killed by the routine, respectively. A routine s MAY-USE set contains the registers \nthat may be used by the routine before being defined, the MAY-DEF set contains the registers that may \nbe defined by the routine, and the MUST-DEF set contains call Pz . ..,.-  ,..-... ._..__....- *......***---. \nPI p2 p3 Figure 2: Example routines showing register definitions and uses. the registers that must be \ndefined by the routine. Figure 2 shows the control-flow graphs for three routines, with calls and returns \nrepresented by dotted arcs. When analyzing routine Pr or P3 Spike uses the call-used, call-defined, and \ncall-killed sets of PZ to determine the register usage that occurs during the call to Pz. For P2, call-used \n= MAY-USE[Pz] = (Rl }, call-defined = MUST-DEF[P2] = (R2). and call-killed = MAY-DEF[P2] = (R2, R3 ). \nThus, the call-summary instruction that replaces a call to PZ uses Rl, defines R2, and kills R2 and R3, \nas shown in Figure 3. After replacing each call instruction in a routine with ................ idef RO,R$~. \n.. .: .......,...... i def ... ,............ . ! def ... i Q ,.. ., i use ... ! use ... j ~.____________: \n......_....... 1 ,....... ......, i useROi *_____.__...__: p2 p3 Figure 3: Routines from Figure 2 showing \nentry, exit and call-summary instructions. the corresponding call-summary instruction, Spike uses a live-at-entry \nset to summarize the register definitions that occur before a call to the routine, and uses a live-at-exit \nset to summarize the register uses that occur after a return from the routine. At each entry, Spike inserts \nan entry instruction that defines the registers live at that entry, as indicated by the corresponding \nlive-at-entry set. Similarly, at each exit, Spike inserts an exit instrwrion that uses the registers \nlive at that exit, as indicated by the corresponding live-at-exit set. Figure 3 shows the entry and exit \ninstructions inserted for each routine in Figure 2. The live-at-entry and liveat-exit sets include any \nregisters that could subsequently be used before being defined along any path from the entry or exit, \nincluding all possible return paths. Thus, in routine P2 live-at-entry = {RO, Rl ) and live-at-exit = \n{RO}. RO is in the live-at-entry and live-at-exit sets because a return path from P2 leads to a use of \nRO in PI.  3 Interprocedural Dataflow To generate the interprocedural dataflow sets described above, \nSpike uses a Program Summary Graph (PSG) similar to Callahan s [Callahan881 and a two phase approach \n[Srivastava93] that computes the call-used, call- defined, and call-killed sets for each routine in the \nfirst phase, and then computes the live-on-entry and live-on-exit sets in the second phase. This section \nfirst describes the construction of the program summary graph, and then describes the dataflow phases. \n 3.1 Program Summary Graph The program summary graph is a compact representation of a program s intraprocedural \ncontrol flow, composed of nodes and directed edges. Although it is conceptually similar to the PSG proposed \nin [Callahan88], the PSG used by Spike differs significantly in its construction and in its encoded information. \nEach PSG node represents a location in the program for which dataflow information is being collected. \nEach node records the MAY-USE, MAY-DEF, and MUST-DEF sets for the program location represented by the \nPSG node. Two nodes are connected by an edge if there is a possible control-flow path between the program \nlocations represented by those nodes. Each edge records a summary of the register definitions and uses \nthat occur along the control-flow paths represented by the edge. Spike examines each routine individually, \nproducing four types of PSG nodes to represent the routine: 1) an entry node is produced for each entrance \nto the routine, 2) an exit node is produced for each exit from the routine, and 3) a cull node and 4) \na return node are produced for each call instruction in the routine. Figure 4(a) shows the control-flow \ngraph for a routine containing four basic blocks and a single call instruction. To simplify the EBbetween \nthe nodes (MAY-DEF), and with the registers that are used before they are defined along some control-flow \n1 def R2 path between the nodes (MAY-USE). For each flow- summary edge E = (Nx, NY). Spike constructs \na subgraph 1J of the routine s control-flow graph containing the basic , 2: def R3 3: def RO blocks \nand arcs that are part of any path fi-om X to Y. Each EA NCALL use R3 call ECFt NRETURN EC 4: use R2 \nD N~xrr I I  (a) (b) Figure 4: Example CFG and corresponding PSG nodes and edges. presentation, the \nfollowing discussion assumes a basic block is ended by a call instruction (as usual, basic blocks are \nalso ended by branches). Figure 4(b) shows the PSG nodes produced to represent the entrance to the routine, \nthe exit from the routine, and the call instruction in the routine. Next, Spike produces two types of \nPSG edges to connect the nodes: 1) a cull-return edge connects each call node to the corresponding return \nnode, and 2) a flow- summary edge connects two nodes if there is a possible control-flow path between \nthe locations represented by the nodes. In Figure 4(b), the flow-summary edges are labeled EA, En, and \nEc, and the call-return edge is labeled Eta. Each flow-summary edge represents all possible control-flow \npaths between the locations represented by the nodes incident to the edge. Spike labels each flow-summary \nedge with the registers that are defined along all control-flow paths between the nodes (MUST-DEF), with \nthe registers that are defined along some control-flow path 1: 2: UBD=0 DEF= (R3)  Represented by EA \nRepresented (a> (b) basic block is labeled with the registers defined (DEF) in the basic block, and with \nthe registers that are used-before-defined (UBD) in the basic block. Figure 5 shows the subgraph of the \nCFG represented by each flow-summary edge in Figure 4(b). Node Nail. represents the call instruction \nat the end of basic block 3. Thus, basic block 3 is represented by flow-summary edge En but not by flow- \nsummary edge Ec. Basic block 4 is represented by both flow-summary edge E* and flow-summary edge EC because \nbasic block 4 is on a path from the routine s entrance (represented by node Nz,vrar) to the routine s \nexit (represented by node NEXIT), and on a path from the call instruction (represented by node NRE7uRN) \nto the routine s exit. Similarly, basic block 1 is represented by both flow-summary edge EA and flow-summary \nedge Ea. The MUST-DEF, MAY-DEF, and MAY-USE sets for each flow-summary edge are found using conventional \ndataflow techniques [Aho88] on the subgraph of the CFG represented by the flow-summary edge. The datailow \nequations are summarized in Figure 6. After the dataflow sets converge for flow-summary edge E = (Nx,Ny), \nthe MUST-DEF, MAY-DEF, and MAY-USE sets associated with location X indicate the registers that must be \ndefined, that may be defined, and that may be used along paths from X to Y. Thus, flow-summary edge E \nis labeled with the MUST-DEF, MAY-DEF, and MAY-USE sets associated with node Nx. For example, for the \nsubgraph in Figure 5(a), the dataflow sets at the entry to basic block 1 are found to be: MUST-DEFIN[l] \n= {R2, R3), MAY- by Ea Represented by Ec Cc) Figure 5: CFG subgraphs for the flow-summary edges in Figure \n4(b). Initialization: for each basic block B MAY-USEINIB] = MAY-USEom[B] = 0 MAY-DEFJB] = MAY-DEFom[B] \n= 0 MUST-DEF,,q[B] = MUST-DEFour[Bl = 0 Dataflow: for each basic block B MAY-USEN[B] = UBD[B] u (MAY-USEo&#38;B]-DEF[B]) \nMAY-DEF,&#38;3] = MAY-DEFour[B] u DEF[B] MUST-DEFm[B] = MUST-DEFour[B] u DEF[B] MAY-USEo&#38;3] = us \nMAY-USE&#38;S], V succ. S of B MAY-DEFou#3] = us MAY-DEFINIS], V succ. S of B MUST-DEFour[B] = nS MUST-DEFeq[S], \nV succ. S of B Figure 6: Dataflow equations to compute the MAY-USE, MAY-DEF, and MUST-DEF dataflow sets \nfor flow-summaryedge E=(Nx,NY). DEF&#38;l] = (R2, R3), and MAY-USE&#38;l] = {RI). Thus, the dataflow \nsets for flow-summary edge EA are assigned as shown in Figure 7. Figure 7 also shows the dataflow sets \nfor flow-summary edges &#38; and Ec. Each call-return edge represents all possible control-flow paths \nthat could be visited as a result of the call, and thus the MAY-USE, MAY-DEF, and MUST-DEF sets labeling \na call-return edge indicate the register definitions and uses that occur along all of these possible \npaths. Initially, Spike has no information about the possible control-flow paths that could be visited \nduring the call. Thus, each call-return edge is initialized with empty MUST-DEF, MAY-DEF, and MAY-USE \nsets.  3.2 Phase 1 Dataflow After constructing the PSG, Spike performs the first phase of dataflow \nto find the call-used, call-defined, and call-killed sets for each routine. Figure 8 summarizes the datatlow \nequations used for the first dataflow phase. The call-used, call-defined, and call-killed sets summarize \nthe MAY-USE = {RI } &#38;NTRY MAY-DEF = {RO.R2) MUST-DEF=( RO,R2) MAY-USE := (Rl) MAY-DEF = (R2,R3) MUST-DEF \n={R2,R3] MAY-DEF = 0 MUST-DE!F= 0 MAY-USE = {R2} MAY-DEF = 0 NEXIT MUST-DEF= 0 Figure 7: Complete PSG \nfor CFG in Figure 4(a) Initialization: for each PSG node N MAY-USE[Nj = MAY-DEF[N-j = MUST-DEFM = 0 Dataflow: \nfor each PSG edge E = (Nx, NY) MAY-USE[Nx] = MAY-USE[E] u (MAY-USE[Ny] -MUST-DEFE]) MAY-DEF[Nx] = MAY-DEF[Nv] \nu MAY-DEF(E] MUST-DEF[Nx] = MUST-DEF[Ny] u MUST-DEF[E] if Nx is an entry node for routine R then foreach \ncall-return edge Eta representing a call to R MAY-USEM = MAY-USE[Nx] MAY-DEF[Ec,J = MAY-DEF[Nx] MUST-DEF&#38;a] \n= MUST-DEF[Nx] Figure 8: Dataflow equations to compute the call-used, call-killed, and call-defined sets \nfor each routine. register definitions and uses that occur as a result of a call, including definitions \nand uses that occur in calls made by the called routine. Thus, dataflow information flows from the entry \npoint of the callee to the call instruction in the caller. To transmit dataflow information from a callee \nto a caller, Spike copies the MAY-USE, MAY-DEF, and MUST-DEF sets from an entry node to each call-return \nedge representing a call instruction that targets the routine represented by the entry node. When the \ndataflow converges, the MAY-USE, MAY-DEF, and MUST-DEF sets associated with each entry node indicate \nthe registers call-used, call-killed, and call-defined by the routine represented by the entry node, \nas describe in Section 2. Figure 9 shows the PSG for the routines in Figure 2. Dashed lines connect the \ncall-return edges in routines Pi and P3 to the entry node of routine Pz, indicating the paths along which \ndataflow information flows from the callee to the caller. After the first phase of dataflow completes, \nMAY-USE[Npi-smv] = 0, MAY-DEFINPi-eNIRy] = {RO,Rl,R2,R3), and MUST-DEFINPI-zNTRY] = {RO,Rl,R2); MAY-USE[NPZ-ENIRY] \n= {Rl), MAY-DEFINn-sNIRY] = {R2,R3 ), and MUST-DEF[N~Q-ENIRY] = (R2); and MAY-USE[Nm-smv] = 0, MAY-DEF[Np3. \nENlaY] = (Rl ,R2,R3 ), and MUST-DEF[NP3-nNIRy] = (RI&#38;!). Thus, for example, for any call to routine \nPi call-used = 0, call-defined = {RO,Rl,R2), and call-killed = {RO,Rl,R2,R3).  3.3 Phase 2 Dataflow \nSpike performs the second phase of dataflow to find the live-at-entry and live-at-exit sets for each \nroutine entry and exit. Figure 10 summarizes the dataflow equations used for the second dataflow phase. \nThe live-at-entry and live-at-exit sets summarize the register uses that could occur along all possible \nreturns from a routine. Thus, dataflow 0 NPI-ENTRY NPSENTRY 0 MAY-USE = 0 MAY-USE = 0 MAY-DEF = (RO,Rl \n) MAY-DEF = { Rl } .----~-*. *....... . . ..-.- . ...____* MUST-DEF={ RO,Rl } ._.-. , MUST-DEF=[ Rl ) \n-. ,/ N,,;; *. NPI-CALL . . %~LL~) MAY-USE = 0 .** MAY-USE = (Rl) *a._ MAY-USE = 0 . . ..____ -.a .-- \n--.-.._... * MAY-DEF = 0 MAY-DEF = 0 MAY-DEF = (R2,R3) MUST-DEF=( R2) MUST-DEF= 0 r MUST-DEF= 0 % MAY-USE \n= (RO} MAY-DEF = 0 MUST-DEF= 0 ) NPI-REIVRN NPLEXIT NPI-~xrr PI p2 Figure 9: PSG for routines in Figure \n information flows from the return point in the caller to the exit of the callee. To transmit dataflow \ninformation from a caller to a callee, Spike copies the MAY-USE set for a return node to each exit node \nrepresenting a routine exit that could return to the location represented by the return node. The MUST-DEF \nand MAY-USE sets computed for the call-return edges during the first dataflow phase summarize the register \ndefinitions and uses that occur during each call, and are retained for the second dataflow phase. When \nthe &#38;tafIow converges, the MAY-USE set associated with each entry node indicates the registers live-at-entry, \nand the MAY-USE set associated with each exit node indicates the registers live-at-exit. Figure 11 shows \nthe PSG for the routines in Figure 2 before the second dataflow phase. The MAY-DEF sets are not used \nduring the second dataflow phase and so are not shown. A dashed line connecting an exit node to a return \nnode indicates a possible return path along which MAY-USE information flows from the caller to the callee. \nInitialization: for each PSG node N MAY-USE[Nl = { 0) Dataflow: for each PSG edge E = (Nx, NY) MAY-USE[NxJ \n= MAY-USEIE] u (MAY-USE[Ny] -MUST-DEF[E]) if Nx is a return node then foreach exit node N,&#38; representing \na routine that could return to Nx MAY-USE[N&#38;,] = MAY-USE&#38;e J u MAY-USE[Nd Figure 10: Dataflow \nequations to compute live-at-entry and live-at-exit sets. b3-RETURN MAY-USE = 0 MAY-DFtF = 0 MUST-DEF= \n0 b3-EXV p3 2 before the first dataflow phase. 3.4 Callee-Saved Registers The Windows NT calling standard \nfor Alpha [CALLSTD] specifies a set of cullee-saved registers, registers that must be saved by a routine \nbefore they can be used, and that must be restored before the routine exits. As seen by the caller, a \ncallee-saved register is not used, killed, or defined by the called routine, because the called routine \nsaves and restores each callee-saved register it modifies. Thus, definitions and uses of callee-saved \nregisters within a routine should not propagate to any callers of the routine, and callee-saved registers \nshould not appear call-used, call-killed, or call-defined by any routine. During the first dataflow phase, \nafter computing the MAY-USE, MAY-DEF, and MUST-DEF sets for an entry node, Spike removes from those sets \nany callee-saved registers saved and restored by the corresponding routine, preventing callee-saved register \ndefinitions and uses within a routine from propagating to the callers. 3.5 Indirect Calls and Jumps \nMany programs contain indirect calls and jumps, used to implement multiway branches (e.g., for SWITCH \nstatements in C), virtual method invocations, calls to shared library routines, etc. If Spike cannot \ndetermine the target of an indirect call or jump, conservative assumptions must be made about the registers \nused, defined, and killed at the target. For multiway branches, Spike extracts the jump-table stored \nwith the program to find all possible targets of the jump and constructs the control-flow information. \nIf Spike cannot determine indirect jump, Spike conservatively registers are live at the jump s target. \nunknown target are assumed to obey graph using that the target(s) of an assumes that all Indirect calls \nto an the calling standard MAY-USE = (01   Muds.._.. _._._*____ . ;;iy=y~~~~~ S PI p2 Figure 11: \nPSG for routines in Figure 2 before the second dataflow phase. [CALLSTD]. Thus, Spike assumes that the \nregisters used to pass arguments are call-used, that the return-value registers are call-defined, and \nthat temporary registers are call-killed. While these assumptions have proven safe for all programs optimized \nto date, datatlow accuracy can be improved if additional information is provided to Spike by the compiler \nor linker. The compiler or linker has exact information about the registers assumed to be live at the \ntarget of each indirect jump, and about the registers assumed to be call-used, call-killed, and call-defined \nby each indirect call. Making this information available to Spike would ensure safe and accurate datatlow \ninformation. 3.6 PSG Branch Nodes Multiway branches inside of loops can potentially cause a large number \nof PSG edges to be produced for a routine, increasing the size of the PSG and thereby increasing dataflow \nanalysis time and memory usage. To prevent a multiway branch from producing a large number of PSG call \nA call B edges, Spike produces a branch de for the multiway branch. Figure 12(a) shows the control-flow \ngraph for a routine containing a 3-way branch with call instructions at each target of the branch. Because \nthere is a possible control-flow path from each call instruction to itself and to every other call instruction, \nan edge connects every return node to every call node in the PSG representing the routine, requiring \n9 flow-summary edges for the CFG in Figure 12(a). By inserting a branch node to represent the multiway \nbranch, the number of flow-summary edges is reduced to 6, as shown in Figure 12(b). In general, inserting \na branch node for an n-way branch potentially reduces the number of PSG edges from o(n2) to O(n). Results \nin Section 4 show that inserting a branch node at multiway branches can reduce the number of PSG edges \nby as much as 80%. A loop containing a large number of conditional (twoway) branches could potentially \nproduce a large number of PSG edges in the same manner as a multiway branch. However, the experimental \nresults show that inserting branch nodes for multiway branches is typically sufficient to prevent the \nproduction of a large number of PSG edges. (a) Figure 12: Example PSG edge reduction from using a branch \nnode. Full Name Description PC APP acad Autodesk AutoCad mechanical CAD excel Microsoft Excel 5.0 spreadsheet \nmaxeda OrCad MaxEDA 6.0 electronic CAD sqlservr Microsoft Sqlservr 6.5 database texim Welcom Software \nTexim 2.0 project manager ustation Bentley Systems Microstation mechanical CAD vc Microsoft Visual C \ncompiler backend winword Microsoft Word 6.0 word processing Table 1: Description of each PC application \nbenchmark. 4 Experimental Results The inter-procedural dataflow techniques presented in this paper \nare implemented in Spike. The techniques are evaluated using the SPEC95 integer benchmark suite and eight \nlarge PC applications compiled for Digital s Alpha architecture [ALPHA]. The PC benchmarks are described \nin Table 1. All results are collected on a Digital PC164LX containing a 466 Mhz 21164 processor and 128 \nMbytes of memory. Each program is optimized using the same highly optimizing back-end used for Digital \nUnix and VMS [Blickstein92]. Table 2 shows the size of each benchmark in machine instructions, and the \nnumber of basic blocks and routines contained in the benchmark. The basic block counts assume a basic \nblock is ended by a call instruction. With the exception of gee, the SPEC benchmarks are d unatically \nsmaller tlhan the PC appl ations. Suite Benchmark Routines Basic T SPECint95 compress 122 .w 1878 go \n462 ijpeg 393 Ii 491 m88ksim 383 per1 487 vortex 818 PC Applications acad 31766 excel 12657 maxeda 2126 \nsqlservr 3275 texim 1821 ustation 12101 vc 2154 winword 12252 For each benchmark, Table 2 also shows \nthe total time required to compute the dataflow information, and the total amount of memory required. \nExcept for gee, analysis time and memory usage for the SPEC integer benchmarks is insignificant. For \nthe large benchmarks analysis time and memory usage increase, but are still reasonable given the number \nof instructions and basic blocks in these applications. Figure 13 shows the fraction of the total dataflow \ntime spent in different stages of the analysis. The smaller benchmarks are not shown because the timer \nresolution prevented accurate timing of each phase. CFG Build indicates the fraction of time spent building \nthe CFG for each routine. Initiafizution indicates the fraction of time spent in general initialization \nand consists mainly of the time spent generating the DEF and UBD sets for each basic block. PSG Build \nindicates the fraction of time spent generating the PSG nodes and edges. Phase 1 and Phase 2 indicate \nthe fraction of time spent in the two phases of dataflow analysis. For each benchmark the fraction of \ntime spent in initialization and control-flow graph building is consistently 5060% of the total analysis \ntime. However, the fraction of time spent in the remaining stages varies considerably between the benchmarks. \nTable 3 shows several benchmark characteristics that influence the time required to construct the PSG \nnodes and edges for each routine, and the number of PSG nodes and PSG edges needed for each routine. \nThe number of calls in a routine and the number of entrances and exits to and from a routine, determine \nthe number of PSG nodes required for the routine. For example, maxeda has a large number of calls per \nroutine on average, and thus requires a Blocks Instructions Total Dataflow Memory Usage 03 Time (sec.) \n(Mbytes) 2546 13.5 0.05 .20 69588 297.6 1.90 6.38 12548 71.4 0.28 .88 6814 42.8 0.16 .56 6052 29.4 0.14 \n.56 8205 40.6 0.16 .58 19468 92.7 0.42 1.57 21880 110.0 0.59 2.85 339962 1734.7 12.04 41.11 301823 1506.3 \n8.95 28.04 84053 418.6 2.02 8.14 123607 754.9 3.34 10.17 50955 302.0 1.34 5.36 165929 916.4 5.21 16.61 \n82072 493.7 2.18 6.18 288799 1520.8 8.30 25.42 Table 2: Benchmark size, dataflow analysis time and memory \nusage. 1.00 benchmarks, branch nodes dramatically reduce the number 0.90 of PSG edges, while increasing \nthe number of PSG nodes c ilEt 0.20 I-1 *a u.LJI b a40 f 0.30 Y 0.20 0.10 zg%gjggjq E ST 3 s Bench~ \nFigure 13: Fraction of total time spent in different stages of the datatlow analysis. large number of \nPSG nodes for each routine. A routine containing a large number of branches is more likely to have multiple \ncontrol-flow paths connecting calls, entrances, and exits, and thus is likely to require more PSG edges \nthan a routine containing fewer branches. Moreover, to compute the MUST-DEF, MAY-DEF, and MAY-USE sets \nfor each PSG edge, Spike must construct and analyze a subgraph of the CFG, increasing the time required \nto build the PSG. For example, because acad has significantly fewer PSG edges per routine than vc, acad \nspends a smaller fraction of the total analysis time building the PSG than vc. Branch nodes significantly \nreduce the number of PSG edges required to represent a routine. Table 4 shows the reduction for each \nbenchmark, compared with a PSG constructed without branch nodes. For several by less than 1%. Suite Benchmark \nEntrances/ Exits/ Routine Routine SPECint95 compress 1.04 1.81 SC 1.00 1.62 go 1.01 1.71 ijpeg 1.02 1.49 \nli 1.01 1.37 m88ksim 1.02 1.75 per1 1.01 1.47 vortex 1.01 1.20 PC Applications acad 1.00 1.L4 excel 1.00 \n1.00 maxeda 1.00 1.12 sqlservr 1.02 1.30 texim 1.00 1.29 ustation 1.00 1.35 vc 1.03 1.10 winword 1.00 \n1.01 Using the PSG is an effective technique for reducing the size of the graph over which dataflow \nmust be computed. For each benchmark, Table 5 shows the number of nodes and edges in the PSG, and the \nnumber of basic blocks and control-flow arcs in the CFG, including arcs representing calls and returns. \nOn average the PSG has 30% fewer nodes than the CFG has basic blocks, and has nearly 40% fewer edges \nthan the CFG has arcs. Two benchmarks, acad and vortex,show uncharacteristic results. acad has a high \ndensity of call instructions, where on average there is a call every 2.1 basic blocks. Each call instruction \nrequires two PSG nodes, which along with the entry and exit nodes causes acad to have more PSG nodes \nthan basic blocks. The PSG for vortex contains more edges than there are arcs in the CFG due to the large \nnumber of branches inside loops, as discussed in Section 3.6. Typically, the PSG for a program contains \nsignificantly fewer nodes than the CFG contains basic blocks. Moreover, each PSG node occupies significantly \nless memory than a basic block. When using the CFG for interprocedural dataflow analysis, the amount \nof dataflow information that must be maintained in each basic block is approximately equal to the dataflow \ninformation contained in three PSG nodes. Each basic block contains the MAY-USEm, MAY-USEom, MAY-DEFm, \nMAY-DEFour, MUST-DEFm, and MUST-DEFom dataflow sets as well as the DEF and UBD sets indicating which \nregisters are defined and used-before-defined in the basic block. In contrast, a PSG node contains just \nthree dataflow sets, Calls/ Branches/ PSG Nodes/ PSG Edges/ Routine Routine Routine Routine 3.30 13.75 \n9.47 17.19 9.86 23.16 22.45 43.65 4.92 17.99 12.58 22.03 3.92 10.55 10.38 16.16 3.49 7.18 9.41 10.72 \n 4.66 13.47 12.14 16.39  9.34 25.55 21.27 40.73 8.97 15.00 20.19 50.11 5.02 4.58 12.18 14.36 8.42 12.98 \n18.88 26.66 15.45 20.25 32.96 46.33 10.48 22.60 23.31 38.94 11.24 13.90 24.91 34.47  5.03 6.86 12.42 \n15.76 9.11 24.47 20.5 1 36.58 8.10 13.02 18.25 24.64 Table 3: Benchmark characteristics influencing \nPSG size and construction time. PSG Edge PSG Node 5 Related Work Reduction Increase compress J.w go ijpeg \nli m88ksim per1 vortex acad excel maxeda sqlservr texim ustation vc 35.4% 0.4% 48.5% 0.5% 12.2% 0.2% \n17.1% 0.2% 1.3% 0.4% 1.2% 0.5% 73.6% 0.5% 4.7% 0.2% 1.8% 0.2% 4.1% 0.4% 0.9% 0.3% 80.0% 0.2% 3.6% 0.6% \n2.1% 0.2% 55.4% 0.8% 1 winword 1 0.3% 0.3% _ . . . Tab!le 4: PSG edge reduction provmed by branch nodes. \n MAY-USE, MAY-DEF, Figure 14 shows the interprocedural dataflow function of the number number of basic \nblocks, and MUST-DEF. total time required to perform the analysis for each benchmark, as a of instructions, \nas a function of the and as a function of the number of routines. The analysis time is well behaved, \nespecially with respect to the number of basic blocks, and shows low-order polynomial complexity. Figure \n15 shows the memory required to perform the interprocedural dataflow analysis for each benchmark. As \nwith analysis time, the memory required is well behaved and has low-order polynomial complexitv. -------, \n Spike uses the two-phase dataflow approach proposed in [Srivastava93] to precisely compute the meet-over-all-valid-paths \nsolution [Callahan88, Reps95, Sagiv95, SharirSl] for the live-at-entry and live-at-exit dataflow sets. \nA path through a called routine is valid if it returns to the site of the call. The first dataflow phase \ncomputes the registers defined and used by a call to each routine, not including any register use that \noccurs after a routine returns. During the second datafiow phase, the registers live before a call instruction \nare influenced registers used and defined during the call registers live after that call instruction, \nbut not that are live at the called routine s other return the live-at-entry and live-at-exit sets computed \nonly by the and by the by registers sites. Thus, during the second dataflow phase live over valid paths. \nWhile the program conceptually similar [Callahan88], it differs include only the registers that are summary \ngraph used by Spike is to the one proposed by Callahan significantly in its construction and in its \nencoded information. Callahan s PSG contains nodes and edges for each variable being analyzed, while \nSpike s PSG contains a single set of nodes and edges shared by all the registers. For each variable, \nCallahan builds the PSG to represent the portion of the CFG reachable by definitions of the variable. \nCallahan s PSG edges do not encode any variable definition or use information. Spike, on the other hand, \nbuilds the PSG to represent an entire routine and summarizes on the edges the register definitions and \nuses that occur on the control-flow paths represented by the edge. Suite Benchmark PSG Nodes PSG Edges \nBasic Blocks CFG Arcs Nodes I Edges I 00 (k) (k) (k) BasicBlock Arc SPECint95 compress SC go i&#38;23 \nli m88ksim per1 vortex PC Applications acad excel maxeda sqlservr texim ustation vc winword - _--- Table \n5: compartson 1.16 2.10 2.55 4.20 .45 .5a 42.16 81.97 69.59 125.91 .61 .65 5.81 10.18 12.55 21.95 .46 \nA6 4.08 6.35 6.81 11.39 A0 .56 4.62 5.27 6.05 10.74 .76 .49 4.65 6.28 8.21 14.02 .57 .45 10.36 19.84 \n19.47 33.72 .53 .59 16.51 40.99 21.88 39.95 .75 1.03 386.80 456.07 339.96 612.11 1.14 .75 238.91 337.48 \n301.82 544.41 .80 .61 70.08 98.50 84.05 151.55 .83 .6! 76.33 127.54 123.61 211.74 .62 .6G 45.36 62.77 \n50.96 90.79 .89 .69 150.27 190.76 165.93 294.47 .91 .65 44.17 78.80 82.07 146.34 .54 .54 223.56 301.84 \n288.80 508.20 .77 .59 --1 . . or Psti nooes ana edges to CFG basic blocks and arcs. e H -10 c f 3 0.1 \n100 1000 10000 ammo lalocQ0 100000M)  RanhaIBnkBlockllnabuctbnr Figure 14: Total interprocedural dataflow \nanalysis time for each benchmark as a function of number of routines, basic blocks, and instructions \nin the benchmark. The branch nodes described in Section 3.6 can potentially allow a quadratic number \nof PSG edges to be replaced with a linear number of edges. Thus, branch nodes have similarities to SSA \nform [Cytron89] and to earlier techniques [Reif82] that reduce the number of def- use chains used to \nconnect variable definitions and uses.   6 Conclusions The Spike executable optimizer uses interprocedural \ndataflow information to enable analyses and code transformations that are not possible in a traditional \ncompiler. Computing interprocedural dataflow information for large programs could potentially require \nunacceptable amounts of time and memory, severely restricting the applicability and effectiveness of \nSpike s optimizations. The techniques and results presented in this paper demonstrate that interprocedural \ndataflow analysis is practical even for large applications containing millions of machine instructions, \nand hundreds of thousands of basic blocks. To perform interprocedural dataflow analysis, Spike first \nconstructs a program summary graph to represent the program s intraprocedural and interprocedural control \nflow, and to represent the register definitions and uses that occur in the program. Next, Spike uses \ntraditional dataflow techniques to compute the sets of registers live at each routine entry and exit, \nand the sets of registers used, killed, and defined by each call. These dataflow sets provide a summary \nof the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform \noptimizations across call instructions and procedure boundaries. Spike s interprocedural dataflow analysis \nis evaluated using the SPEC95 integer benchmarks and eight large PC ~lwcxl 4 1 low 100 100 1000 loo00 \nlwooo lamoo 1oMxxKK) RoullllwlbdcBbclaII-B Figure 15: Memory usage for each benchmark as a function of \nnumber of routines, basic blocks, and instructions in the benchmark. applications. For the small benchmarks, \ninterprocedural dataflow analysis uses an insignificant amount of time, requiring less than two seconds \nfor each of the SPEC95 integer benchmarks. For the larger benchmarks, analysis time and memory usage \nincrease as a near-linear function of program size, remaining practical even for the largest Programs. \n 7 Acknowledgments The author would like to thank Robert Cohn and Geoff Lowney for their help implementing \nand testing interprocedural dataflow analysis in Spike.  8 References [Ah0881 A. Aho, R. Sethi, and \nJ. Ullman. Compilers: Principles, Techniques, and Tools. Addison-Wesley, 1988. [ALPHA] Alpha Architecture \nReference Manual. http://www.partner.digital.com/www-swdev/pages/ Home!T ECH/documents/alpha_cookbookbook/biblio.htm \n[Blickstein92] D. Blickstein. et al, The GEM optimizing compiler system, Digital Technical Journal, 4(4):121-136. \n[Callahan881 D. Callahan. The program summary graph and flow-sensitive interprocedural data flow analysis, \nin Proc. ACM SIGPLAN Con5 on Programming Language Design and Implementation 88, pp 47-56, June 1988. \nw==-m Alpha NT Calling Standard. http://www.partner.digital.com/www-swdev/pages/ HomeJlECH/documents/alpha_cookbooWbiblio.htm \n [Cohn961 R. Cohn and G. Lowney, Hot Cold optimization of large Windows/NT applications, to appear in \nProc. of the 29th Annual Intl. Symp. on Microarchitecture, Dec. 1996. [Cytron89] Ron Cytron, Jeanne Ferrante, \nBarry Rosen, Mark Wegman, and Kenneth Zadeck, An efficient method for computing static single assignment \nform. in ACM Symposium on Principles of Programming Languages, pp 25-35, January1989 [Goodwin961 D. Goodwin \nand K. Wilken, Optimal and near-optimal global register allocation using O-l integer programming, Software \n-Practice &#38; Experience, Vol. 26(8), pp. 929-965, August 1996. [Khedker94] U. Khedker and D. Dhamdhere, \nA generalized theory of bit vector data flow analysis, ACM Transactions on Programming Languages and \nSystems, Vol. 16(5), pp. 1472-1511, September 1994. pettis90] K. Pettis and R. Hansen, Profile guided \ncode positioning, in Proc. ACM SIGPUN Co@ on Programming Language Design and Implementation 90, pp 16-27, \nJune 1990 [Reit82] John Reif and Robert Tarjan, Symbolic program analysis in almost linear time. SIAM \nJournal of Computing, 1 1 ( 1 ), February 1982. [Reps951 T. Reps, S. Horwitz, and M. Sagiv, Precise interprocedural \ndataflow analysis via graph reachability, in Proc. 22nd ACM SIGPLAN-SIGACT Co&#38; on Principles of Programming \nLanguages, pp. 49-61,1995. [Sagiv95] M. Sagiv, T. Reps, and S. Horowitz, Precise interprocedural dataflow \nanalysis with applications to constant propagation, in Proc. of FASE 95: Colloquium on Formal Approaches \nin Sofnvare Engineering, May 1995. Lecture Notes in Computer Science, Vol. 915, Springer-Verlag, New \nYork, NY, pp. 651-665. [SharirSl] M. Sharir and A. Pnueli, Two approaches to interprocedural data flow \nanalysis. in Program Flow Analysis, Theory and Applications by S. Muchnick and N. Jones, 1981. [Srivastava93] \nA. Srivastava and D. Wall, A practical system for intermodule code optimization at link-time, Journal \nof Programming Languages l(I), pp. I-18, March 1993. [SrivastavaW] A. Srivastava and D. Wall, Link-time \noptimization of address calculation on a 64-bit architecture, in Proc. ACM SIGPLAN Con. on Programming \nLanguage Design and Implementation 94, pp. 49-60, Orlando, FL, June 1994. [Wilson961 L.S. Wilson, C.A. \nNeth, M.J. Rickabaugh, Delivering binary object modification tools for program analysis and optimization, \nvolume 8.1 of Digital Technical Journal, pp. 18-31, 1996.  \n\t\t\t", "proc_id": "258915", "abstract": "Interprocedural dataflow information enables link-time and post-link-time optimizers to perform analyses and code transformations that are not possible in a traditional compiler. This paper describes the interprocedural dataflow analysis techniques used by Spike, a post-linktime optimizer for Alpha/NT executables. Spike uses dataflow analysis to summarize the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations that require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, the time required to perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike's effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation of a program's intraprocedural and interprocedural control flow that efficiently summarizes the register definitions and uses that occur in the program. Experimental results are presented for the SPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation allows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 integer benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 thousand basic blocks, interprocedural dataflow analysis requires just 12 seconds.", "authors": [{"name": "David W. Goodwin", "author_profile_id": "81100263282", "affiliation": "Digital Equipment Corporation", "person_id": "PP31034553", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258927", "year": "1997", "article_id": "258927", "conference": "PLDI", "title": "Interprocedural dataflow analysis in an executable optimizer", "url": "http://dl.acm.org/citation.cfm?id=258927"}