{"article_publication_date": "05-01-1997", "fulltext": "\n A New Algorithm for Partial Redundancy Elimination based on SSA Form Fred Chow Sun Chan Robert Kennedy \nShin-Ming Liu Raymond Lo Peng Tu fchow@sgi.com Silicon Graphics Computer Systems 2011 N. Shoreline Blvd. \nMountain View, CA 94043 Abstract A new algorithm, SSAPRE, for performing partial redun-dancy elimination \nbased entirely on SSA form is presented. It achieves optimal code motion similar to lazy code mo-tion \n[KRS94a, DS93], but is formulated independently and does not involve iterative data flow analysis and \nbit vec-tors in its solution. It not only exhibits the characteristics common to other sparse approaches, \nbut also inherits the advantages shared by other SSA-based optimization tech-niques. SSAPRE also maintains \nits output in the same SSA form as its input. In describing the algorithm, we state theo- rems with proofs \ngiving our claims about SSAPRE. We also give additional description about our practical implementa- tion \nof SSAPRE, and analyze and compare its performance with a bit-vector-based implementation of PRE. We \ncon-clude with some discussion of the implications of this work. 1 Introduction The Static Single Assignment \nForm (SSA) has become a popular program representation in optimizing compilers, because it provides accurate \nuse-def relationships among the program variables in a concise form [CFR+Sl, Wo196, CCL+96]. Many efficient, \nglobal optimization algorithms have been developed based on SSA. Among these optimiza- tions are dead \nstore elimination [CFR+Sl], constant propa-gation [WZ91], value numbering [AWZSS, RWZ88, CS95a], induction \nvariable analysis [GSW95, LLCSG], live range computation [GWS94] and global code motion [Cli95]. All \nthese uses of SSA have been restricted to solving problems based on program variables, since the concept \nof use-def does not readily apply to expressions. Noticeably missing among SSA-based optimizations is \npartial redundancy elimination. Partial redundancy elimination (PRE) is a powerful op- timization algorithm \nfirst developed by Morel and Renvoise [MR79]. By targeting partially redundant computations in the program, \nit automatically removes global common sub-expressions and moves invariant computations out of loops. \nIt has since become the most important component in many global optimizers [Cho83, CHKW86, SKL88, BC94, \nCS95b]. In [KRS92, KRS94a], Knoop et al. formulated an alternative Permission to make digital/hard copy \nof part or all this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advan-tage, the copyright notice, the title of the publication \nand its date appear, and notice is given that copying is by permission of ACM. Inc. To copy otherwise, \nto republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/9710006...S3.50 placement strategy called \nlazy code motion that improves on Morel and Renvoise s results by avoiding unnecessary code movements, \nand by removing the bidirectional nature of the original PRE data flow equations. The result of lazy \ncode motion is optimal: the number of computations cannot be further reduced by safe code motion, and \nthe lifetimes of the temporaries introduced are minimized. In (DS93], Drechsler and Stadel gave a simpler \nversion of the lazy code motion algorithm that inserts computations on edges rather than in nodes. Optimizations \nbased on SSA all share the common char-acteristic that they do not require traditional iterative data \nflow analysis in their solutions. They all take advantage of the sparse representation of SSA. In a sparse \nform, informa- tion associated with an object is represented only at places where it changes, or when \nthe object actually occurs in the program. Because it does not replicate information over the entire \nprogram, a sparse representation conserves memory space. Information can be propagated through the sparse \nrepresentation in a smaller number of steps, speeding up most algorithms. To get the full benefit of \nsparseness, one must typically give up operating on all elements in the pro- gram in parallel, as in \ntraditional bit-vector-based data flow analysis. But operating on each element separately allows optimization \ndecisions to be customized for each object. There is another advantage of using SSA to perform global \noptimization. Traditional optimization techniques of- ten implement two separate versions of the same \noptimiza-tion: a global version that uses bit vectors in each basic block, and a simpler and faster local \nversion that performs the same optimization within a basic block. SSA-based op timization algorithms \ndo not need to distinguish between global and local optimizations. The same algorithm can handle both \nglobal and local versions of an optimization si-multaneously. The amount of effort required to implement \neach optimization can be correspondingly reduced. As was hinted at by Dhamdhere et d. in the conclusion \nof [DRZ92], developing a PRE algorithm based on SSA is difficult because an expression E can be redundant \nas the result of many different computations at different places of the same expression E , E , . . . \nwhose operands have differ- ent SSA versions from the operands of E. This is illustrated in Fig. l(a). \nIn such a situation, the us+def chain of SSA does little to help in recognizing that E is partially redun-dant. \nIt also does not help in effecting the movement of computations. Lacking an SSA-based PRE algorithm, \nopti-mizers that use SSA have to switch to bit-vector algorithms in performing PRE. To apply subsequent \nSSA-based opti- 01 + tl t al + bl a2 t h tz \\ a3 4- 4(al ,a21 a3 + 4(at,a2) t3 t 4th) t21 I a3 + bl \nt3 a. before PRB b.after PILE Figure 1: PRE in SSA form mizations, it is necessary to convert the results \nof PRE back into SSA form, and such incremental updates based on ar- bitrary modifications to the program \nare expensive [CSSSS]. We have developed an algorithm that performs PRE di-rectly on an SSA representation \nof the program (SSAPRE). Our algorithm is sparse because it does not require collecting traditional local \ndata flow attributes over the program and it does not require any form of iterative data flow analysis \nto arrive at its solution. Our algorithm works by constructing the SSA form of the hypothetical temporary \nh that could be used to store the result of each computation in the program. In the resulting SSA form \nof h, a def corresponds to a com- putation whose result may need to be saved, and a use cor- responds \nto a redundant computation that may be replaced by a load of h. Based on this SSA form of h, we can then \nap ply the analyses corresponding to PRE. The analyses allow us to identify additional defs of h, with \naccompanying com-putations, that need to be inserted to achieve optimal code motion. The final output \nis generated according to the up dated SSA graph of h: temporaries are introduced into the program to \nsave and reuse the values of computations. Since the algorithm works by modeling the SSA forms of the \nhy- pothetical temporaries, the real temporaries introduced are maintained with SSA properties, as in \nFig. l(b). The rest of this paper is organized as follows. Section 2 surveys related work aimed at improving \nthe efficiency of data flow analysis and PRE. Section 3 briefly introduces SSA form and gives an overview \nof the SSAPRE approach. Section 4 describes the SSAPRE algorithm in detail, while stating related lemmas \nwith proofs. Section 5 discusses the theoretical foundations of the SSAPRE algorithm, and verifies its \ncorrectness and optimality. Section 6 discusses some practical issues related to au efficient implementation \nof SSAPRE. Section 7 compares and contrasts the steps in SSAPRE with bit-vector-based PRE, and analyzes \nthe com- plexity of the SSAPRE algorithm. Section 8 provides mea surements that compare the time spent \nin performing PRE between a bit-vector-based implementation and an imple-mentation of SSAPRE. Section \n9 concludes by discussing the implications of thii work, and points out some promis-ing areas where similar \ntechniques can be applied using SSAPRE as a model. Related Work In recent years, we have seen development \nof different tech- niques aimed at improving the solution of data flow problems that are related to SSA \nor PRE. In [CCFSl], by generalizing SSA form, Choi et al. de-rived Sparse Evaluation Graphs as reduced \nforms of the orig- inal flow graph for monotone data flow problems related to variables. The technique \nmust construct a separate sparse graph per variable for each data flow problem, before solving the data \nflow problem for the variable based on the sparse graph. Thus, it cannot practically be applied to PILE, \nwhich requires the solution of several diierent data flow problems. In [DRZ92], Dhamdhere et al. observed \nthat in solving for a monotone data flow problem, it sufhces to examine only the places in the problem \nwhere the answer might be differ- ent from the trivial default answer 1. There are only three possible \ntransfer functions for a node: raise to T, lower to I, or identity (propagate unchanged). They proposed \nslot-wise analysis. For nodes with the identity transfer function, those that are reached by any node \nwhose answer is I will have I as their answer. By performing the propagation slot-wise, the method can \narrive at the solution for each variable in one pass over the control flow graph. Slotwise analysis is \nnot sparse, because it still performs the propagation with respect to the control flow graph of the program. \nThe ap-proach can be used in place of the iterative solution of any monotone data flow problem as formulated. \nIt can be used to speed up the data flow analyses in PRE. In [Joh94], Johnson proposed the use of Dependence \nFlow Graphs (DFG) as a sparse approach to speed up data flow analysis. The DFG of a variable can be viewed \nas its SSA graph with additional merge operators imposed to identify single-entry single-exit (SESE) \nregions for the variable. By identifying SESE regions with the identity transfer function, the technique \ncan short-circuit propaga-tion through them. Johnson showed how to apply hi tech-niques to the data flow \nsystems in Drechsler and Stadel s variation of Knoop et a1. s lazy code motion. Researchers at Rice University \nhave done work aimed at improving the effectiveness of PRE [BC94, CS95b]. The work involves the application \nof some SSA-based transfor-mation techniques to prepare the program for optimization by PRE. Their techniques \nenhance the results of PRE. Their implementation of PRE was based on Drechsler and Stadel s variation \nof Knoop et 01. ~ lazy code motion, and was unrelated to SSA. All prior work related to PRE has modeled \nthe problem as systems of data flow equations. Regardless of how efIi- ciently the systems of data flow \nequations can be solved, a substantial amount of time needs to be spent in scanning the contents of each \nbasic block in the program to initial-ize the local data flow attributes that serve as input to the data \nflow equations. Experience has shown that this often takes more time than the solution of the data flow \nequa- tions, so a fundamentally new approach to PRE that does not require the dense initialization of \ndata flow information is highly desirable. SSAPRE satisfies this property as it exploits sparseness. \n 3 Overview of Approach The input to SSAPRE is an SSA representation of the pro- gram. In SSA, each definition \nof a variable is given a unique version, and d&#38;rent versions of the same variable can be regarded \nas different program variables. Each use of a vari- able version can only refer to a single reaching \ndefinition. By virtue of the versioning, use-def information is built into the representation. Where \nseveral definitions of a variable, Ul,U?,..., a,,,, reach a confluence point in the control flow graph \nof the program, a I$ function assignment statement, an t qqUl,QZ,..., am), is inserted to merge them \ninto the definition of a new variable version an. Thus the semantics of single reaching definitions is \nmaintained. This introduc-tion of a new variable version as the result of 4 factors the set of usedef \nedges over confluence nodes, reducing the num- ber of usedef edges required to represent the program. \nIn SSA, the usedef chain for each variable can be provided by making each version point to its single \ndefinition. One im- portant property of SSA form is that each definition must dominate all its uses in \nthe control flow graph of the pro- gram if the uses at 4 operands are regarded as occurring at the predecessor \nnodes of their corresponding edges. We assume all expressions are represented as trees with leaves that \nare either constants or SSA-renamed variables. SSAPRE can be applied to program expressions indepen- \ndently, regardless of subexpression relationships. In Sec-tion 6, we describe a strategy that exploits \nthe nesting re-lationship in expression trees to obtain greater optimization efficiency under SSAPRE. \nIndirect loads are also candidates for SSAPRE, but since they reference memory and can have aliases, \nthe indirect variables have to be in SSA form in or- der for SSAPRE to handle them. Using the HSSA form \npresented in [CCL+961 allows SSAPRE to uniformly handle indirect loads together with other expressions \nin the pro- gram. SSAPRE consists of six separate steps: (1) @-Insertion, (2) Rename, (3) DownSafety, \n(4) WiJJBeAvaiJ, (5) Finalize and (6) CodeMotion. SSAPRE works by conducting a round of SSA construction \non the lexically identical expressions in the program whose variables are already in SSA form. Since \nthe term SSA cannot be meaningfully applied to expressions, we define it to refer to the hypothetical \ntemporary h that could be used to store the result of the expression. In the rest of this paper, we use \n4r to refer to a 4 in the SSA form of the hypothetical temporary to contrast it with a 4 for a variable \nin the original program. O-Insertion and Rename are the initial SSA construc-tion steps for expressions. \nThis round of SSA construction can use an approach similar to that described in [CFR+91], working on \nall expressions in the program simultaneously. Alternatively, au implementation may choose to work on \neach lexically identical expression in sequence. We describe such a sparse implementation in Section \n6. Assuming we are working on the expression a + b, whose hypothetical temporary is h. After the Rename \nstep, oc-currences of a + b corresponding to the same version of h must compute the same value. At this \nstage, the points of defs and uses of h have not yet been identified. Many 9 s inserted for h are also \nunnecessary. Later steps in SSAPRE will fix them up. Some @ operands can be determined to be undefined \n(I) after Rename because there is no avail-able computation of a + 5. These I-valued + operands will \nplay a key role in the later steps of SSAPRE, because inser- tions are performed only because of them. \nWe call the SSA graph for h alter Rename the dense SSA graph because it contains more a s than in the \nminimal SSA form (as defined in [CFR+Sl]). Expressions are letically identical if they apply exactly \nthe same operator to exactly the same operands; the SSA versions of the wri- ablea are ignored in matching \nexpressions. For example, 01 + bl and oa + ba are lexically identical expressions. Our SSA graph is similar \nto that described in [GSWQ5], which is formed from the use-def edges of nodes assigned the same SSA The \nsparse computation of global data flow attributes for a + 6 can be performed on the dense SSA graph for \nh. Two separate phases are involved. The first phase, Down-Safety, performs backward propagation to determine \nthe a s whose results are not fully anticipated with respect to a + b. The second phase is WiJiBeAwaiJ, \nwhich performs forward propagation to determine the @ s where the computation of a + 5 will be available \nassuming PRE insertions have been performed at the appropriate incoming edges of the ip s. Using the \nresults of WiUBeAvaiJ, we are ready to finalize the effects of PRE. The Finalize step inserts computation \nof a + 5 at the incoming edges of @ to ensure that the compu- tation is available at the merge point. \nFor each occurrence of a + b in the program, it determines if it is a def or use of h. It also links \nthe uses of h to their defs to form its precise SSA graph. Extraneous O s (see [CFR+Sl], p.359) are removed \nso that h is in minimal SSA form. The last step is to update the program to effect the code motion for \na + b as determined by SSAPRE. The CodeMo-tion step introduces the real temporary t to eliminate the \nredundant computations of a + 5. It walks over the pre-cise SSA graph of h and generates saves of the \ncomputation a+6 into t, giving each t its unique SSA version. Redundant computations of a + b are replaced \nby t. The a s for h are translated into 4 s for t in the native program representa-tion. 4 SSAPRE Algorithm \nIn this section, we describe the complete SSAPRE algo-rithm. As in p<RS92] and [DS93], we assume all \ncritical edges in the control flow graph have been removed by in- serting empty basic blocks at such \nedges. Thii allows us to model insertions as edge placements, even though we only insert at the ends \nof the predecessor blocks. We assume prior computation of the dominator tree (DT) and dominance frontiers \n(DF s) with respect to the control flow graph of the program. These data must have already been computed \nand used when the program was first put into SSA form [CFR+91]. Again, we base our discussion on the \nexpression a + 5 whose hypothetical temporary is h. We use the example program shown in Fig. 2 to illustrate \nthe various steps. Based on the algorithms we describe, we also state and prove various lemmas, which \nwe use in establishing the theorems about SSAPRE in Section 5. 4.1 The @Insertion Step A Q, for an expression \nis needed whenever d&#38;rent values of the same expression reach a common point in the pro-gram. There \nare two diierent situations that cause e s for expressions to be placed: Fit, when an expression appears, \nwe insert a @ at its iterated dominance frontiers (DF+), because the occurrence may correspond to a def \nof h. In Fig. 3, a + is inserted at block 3 due to a + b in block 1. The second situation that causes \ninsertion of a s is when there is a 4 for a variable contained in the expression, be cause that indicates \nan alteration of the expression reaches the merge point. We only need to insert a 0 at a merge point \nwhen it reaches a later occurrence of the expression, because otherwise the ip will not contribute to \nany optimiza- tion in PRE. In Fig. 3, the @ for h at block 8 is caused by the 4 for a in the same block. \nWe do not need to insert any '2 [ h t+(h, h) 1 I 3 Figure 2: Example Program (in SSA form) 9 at block \n10 even though it is a merge point, because there is no later occurrence of a + b after block 10. Both \ntypes of 9 insertions are performed together in one pass over the program, with the second type of ip \ninser- tion performed in a demand-driven way. We use the set DF-phis[i] to keep track of the a s inserted \ndue to DF+ of the occurrences of expression Ei. We use the set Varphis[;lljl to keep track of the 3 s \ninserted due to the occurrence of 4 s for the jthvariable in expression Ei. When we come across an occurrence \nof expression E;, we update DF-phi&#38; For each variable vj in the occurrence, we check if it is defined \nby a 4. If it is, we update Var-phis[i]lj], because a 0 at the block that contains the 4 for uj may contribute \nto op- timization of the current occurrence of Ei. The same may apply to earlier points in the program \nes well, so it is nec- essary to recursively check for updates to Var-phis[i]fi] for each operand in \nthe 4 for vj. After all occurrences in the program have been processed, the places to insert 9 s for \nEi are given by the union of DF-phis[i] with the Varphis[illjl s. The full algorithm for the +-Insertion \nstep is given in Fig. 4. By using this demand-driven technique, we take advantage of the SSA representation \nin the input program. Other algorithms for SSA 4 placement with linear time complexity can also be used \nto place Cp s [JPP94, SGSS]. We adapt the algorithm from [CFR+Sl] because it is easier to understand \nand implement. LEMMA 1 (Suficiency of G insertion) If B is a basic block where no expression 9 is inserted \nand the expression is par- tially anticipated at the entry to B, ezactly one evaluation of the expression \n(counting I as an evaluation) can reach the entry to B. Proof: Suppose at least two different evaluations \nof the Figure 3: Program after +-Insertion expression, $1 and $2, reach the entry to B. It cannot be \nthe case that $1 and $2 both dominate B; suppose without loss of generality that $1 does not dominate \nB. Now there exists a block BO that dominates B, is reached by $1 and $2, and lies in DF+($i) (n.b., \nBO may be B). If $1 is a computation of the expression, the +-Insertion step must have placed a @ in \nBo, contradicting the proposition that $1 reaches B. If on the other hand $1 is an assignment to an operand \nv of the expression (so I is among the values reaching B), there must be a $ for v in Bo by the correctness \nof the input SSA form. Hence when @-Insertion processed Bo, it must have placed a Q, there, once again \ncontradicting the proposition that $1 reaches B. cl 4.2 The Rename Step The Rename step assigns SSA \nversions to h in its SSA form. The version numbering we produce for h differs from the eventual SSA form \nfor the temporary t, but has the follow- ing two important properties. First, occurrences that have identical \nh-versions have identical values. Second, any con-trol flow path that includes two different h-versions \nmust cross au assignment to an operand of the expression or a + for h. We apply the SSA Renaming algorithm \nas given in [CFR+Sl], in which we conduct a preorder traversal of the dominator tree, but with the following \nmodification. In addition to a renaming stack for each variable in the pro-gram, we maintain a renaming \nstack for every expression; entries on these expression stacks are popped as we back up the blocks that \ndefine them. Maintaining the variable and expression stacks together allows us to decide efficiently \nwhether two occurrences of an expression should be given the same h-version. procedure @-Insertjon for \neach expression Ei do { r DFphB[i] t empty-set for each variable j in I% do Var-phis[illjl t {} 1 for \neach occurrence X of Ei in program do DF-phis[i] t DF-phis[i] U DF+(X) for each variable occurrence V \nin X do if (V is defined by 4) { j t index of V in X Set-var-phis(Phi(V), i, j) 1 1 for each expression \nEi do { for each variable j in Ei do DF-phis[ij c DF-phis(i] U Var-phis(illjl insert @ s for E; according \nto DF-phis[i] 1 end +-Insertion procedure Set-varphis(phi, i, j) if (phi $ Var-phi@]) { Var-phisli]lj \nt Var-phis[i]lil U {phi} for each operand V in phi do if (V is defined by 4) Set-varphis(Phi(V), i, j) \n 1 end Set-var-phis Figure 4: Algorithm for +-Insertion There are three kinds of occurrences of expressions \nin the program: (1) the expressions in the original program, which we call renl occurrences; (2) the \n9 s inserted in the &#38;Insertion step; and (3) 9 operands, which are regarded as occurring at the exits \nof the predecessor nodes of the corresponding edges. The Rename algorithm performs the following steps \nupon encountering an occurrence q of the expression Ei. If q is a a, we assign q a new version. Oth-erwise, \nwe check the current version of every variable in Ei (i.e., the version on the top of each variable s \nrename stack) against the version of the corresponding variable in the oc-currence on the top of Ei s \nrename stack. If all the variable versions match, we assign q the same version as the top of Ei s stack. \nIf any of the variable versions does not match, we have two cases: (a) if q is a real occurrence, we \nassign q a new version; (b) if q is a @ operand, we assign the spe- cial version I to that @ operand \nto denote that the value of E; is unavailable at that point. Finally, we push q on Ei s stack and proceed. \nFig. 5 shows the dense SSA graph that forms after h in our example has been renamed. This expression \nrenaming technique also takes advantage of the SSA representation of the program variables. The remaining \nsteps of the SSAPRE algorithm rely on the fact that + s are placed only where Ei is partially an-ticipated, \n(i.e., there is no dead Cp in the SSA graph of h). Dead a s can efficiently be identified by applying \nthe stan- dard SSA-based dead store elimination algorithm [CFR+Sl] on the SSA graph formed after renaming. \nFrom here on, we assume that only live 9 s are represented in the SSA form of h. LEMMA 2 (Correctness \nof version renaming) If two occur- rences are assigned the same version by Rename, the expres- Figure \n5: The dense SSA graph for a + b sion has the same value at those two occurrences. Proof: This lemma \nfollows directly from the fact that the Rename step assigns the same version to two occurrences of an \nexpression Ei only if all the SSA versions of their expres- sion operands match. We appeal to the single-assignment \nproperty and the correctness of the SSA renaming algorithm for variables [CFR+Sl] to complete the proof. \n0 LEMMA 3 (Versions capture all the redundancy) If two oc- currences &#38;, qbv are assigned versions \nx, y by Rename, ezactly one of the following holds: a no control flow path can reach from $J= to &#38;, \nwithout passing through a real (i.e., non-$) assignment to an operand of the expression (meaning that \nthere is no redundancy between the occurrences); or . there is a path (possibly empty, in which case \nx = y) in the SSA graph of use-def arcs from y to x (implying that any redundancy between &#38; and &#38; \nis exposed to the algorithm). Proof: Suppose there is a control flow path P from &#38; to &#38; that \ndoes not pass through any assignment to an operand of the expression. Our proof will proceed by induction \non the number of 9 s for the expression traversed by P. If P encounters no @, z = y establishing the \nbasis for our induction. If P hits at least one P, the last ip on P defines &#38;. Now we apply the induction \nhypothesis to that part of P up to the corresponding operand of that G. 0 4.3 The DownSafety Step One \ncriterion required for PRE to insert a computation is that the computation is down-safe (or anticipated) \nat the point of insertion [KRS94a]. In the dense SSA graph con-structed by Rename, each node either represents \na real oc-currence of the expression or is a @J. It can be shown that SSAPRE insertions are only necessary \nat + s, so down-safety only needs to be computed for them. Using the SSA graph, down-safety can be sparsely \ncomputed by backward propa-gation along the use-def edges. A Cp is not down-safe if there is a control \nflow path from that @ along which the expression is not evaluated before program exit or before being \naltered by redefinition of one of its variables. Except for loops with no exit, this can happen only \ndue to one of the following cases: (a) there is a path to exit along which the 9 result version is not \nused; or (b) there is a path to exit along which the only use of the @ result ver- sion is as an operand \nof a + that is not down-safe. Case (a) represents the initialization for our backward propagation of \ndown-safety; all other O s are initially marked down-safe. DownSafety propagation is based on case (b). \nSince a real occurrence of the expression blocks the case (b) propagation, the algorithm marks each Cp \noperand with a flag ha.s_reaJ-use when the path to the Cp operand crosses a real occurrence of the same \nversion of the expression. It is convenient to perform initialization of the case (a) downs&#38; and \ncomputation of the hasl-eaJ-use flags during a dominator-tree preorder pass over the SSA graph. Since \nRename conducts such a pass, we can include these calcu- lations in the Rename step with minimal overhead. \nIni-tially, all downsafe flags are true and all has-real-use flags are false. When Rename assigns a new \nversion to a real occurrence of expression Ei or encounters a program exit, it examines the occurrence \non the top of Ei s stack before pushing the current occurrence. If the top of stack is a 9 occurrence, \nRename clears that Cp sdownsafe flag because the version it defines is not used along the path to the \ncur- rent occurrence (or exit). When Rename assigns a version to a + operand, it sets that operand s \nhas-real-use flag if and only if a real occurrence for the same version appears at the top of the rename \nstack. Fig. 6 gives the DownSafety propagation algorithm. LEMMA4 (Correctness of downsafe) A 9 is marked \ndownsafe after DownSafety if and only if the expression is fully anticipated at that 0. Proof: We first \nnote that each @ marked not downsafe during Rename is indeed not down-safe. The SSA renaming algorithm \nhas the property that every definition dominates all its uses. Suppose that a + appears on the top of \nthe stack when Rename creates a new version or encounters a program exit. In the case where a program \nexit is encoun- tered, the Cp is obviously not down-safe because there is a path in the dominator tree \nfrom the @ to exit containing no use of the a. Similarly, if Rename assigns a new version to a real occurrence, \nit does so because some expression operand v has a different version in the current occurrence from its \nversion at the a. Therefore there exists a path in the domi- nator tree from the @ to t,he current occurrence \nalong which there is an assignment to V. Minimality of the input HSSA program implies, then, that any \npath from the @ to the current occurrence and continuing to a program exit must encounter an assignment \nto v before encountering an evalu- ation of the expression. Therefore the expression is not fully anticipated \nat the a. Next we make the observation that any ip whose downsafe flag gets cleared during the DownSafety \nstep is not down-safe, since there is a path in the SSA use-def graph procedure DownSafety for each expr-@ \nF in program do if (not downsafe( for each operand opnd of F do Reset-downsafe(opnd) end DownSafety procedure \nReset-down&#38;e(X) if (hasxeal-use(X) or X not defined by P) return F t @ that defines X if (not downsafe( \n return downsafe t false for each operand opnd of F do Reset-downsafe(opnd) end Reset-down&#38;e Figure \n6: Algorithm for DownSafety from an unused version to that @ where no arc in the path crosses any real \nuse of the expression value. Indeed one such path appears on the recursion stack of the Reset-down&#38;e \nprocedure at the time the downs&#38; flag is cleared. Finally, we need to show that all the a s that \nare not down-safe are so marked at the end of DownSafety. This fact is a straightforward property of \nthe depth-first search propagation performed by Reset-downsafe. 0  4.4 The WillBeAwail Step The WiJJBeAvaiJ \nstep has the task of predicting whether the expression will be available at each Cp result following \ninsertions for PRE. In the Finalize step, insertions will be performed at incoming edges corresponding \nto + operands at which the expression will not be available (without that insertion), but the ip s will-beavail \npredicate is true. The WiJJBeAtiJ step consists of two forward propaga-tion passes performed sequentially, \nin which we conduct sim-ple reachability search in the SSA graph for each expression. The first pass \ncomputes the can-be-avail predicate for each @ by first initializing it to true for all @ s. It then \nbegins with the boundary set of ip s at which the expression can-not be made available by any down-safe \nset of insertions. These are Cp s that do not satisfy the downsafe predicate and have at least one I-valued \noperand. The can-be-avail predicate is set to false and the false value is propagated from such nodes \nto others that are not down-safe and that are reachable along def-use arcs in the SSA graph, excluding \narcs at which has-real-use is true. Cp operands defined by @ s that are not can-beavail are set to I \nalong the way. Af-ter this propagation step, can-beavail is false for a @ if and only if no down-safe \nplacement of computations can make the expression available. The a s where can-be-avail is true together \ndesignate the range of down-safe program areas for insertion of the expression, plus areaS that are not \ndown-safe but where the expression is fully available in the original progrann3 The second pass works \nwithin the region computed by the first pass to determine the G s where the expression will be available \nfollowing the insertions we will actually make, which implicitly determines the latest (and final) insertion \nThe entry points to this region (the I-valued 0 operands) can be thought of as SSAPRE s earliest insertion \npoints. These may be later than the earliest insertion points in [KRS92] and [DS93] because their bit-vector \nschemes allow earliest insertion at non-merge blocks. points. The second pass is analogous to the computation \nof the predicate LATERIN in [DSSS). It works by propagat- ing the later predicate, which it initializes \nto true wherever can-be-avail is true. It then begins with the real occur-rences of the expression in \nthe program, and propagates the false value of later forward to those points beyond which insertions \ncannot be postponed (moved downward) without introducing unnecessary new redundancy. At the end of the \nsecond pass, will-be-avail for a @ is given by: willbeavail = canbeavail A -later. Fig. 5 shows the \nvalues of downsafe (ds), can-be-avail (cba), later and will-be-avail (wba) for the program example at \neach + for h. For convenience, we define a predicate to indi- cate those 9 operands where we will perform \ninsertions: We say insert holds for a @ operand if and only if the following hold: . the Cp satisfies \nwill-be-avail; and . the operand is I, or has-real-use is false for the operand and the operand is defined \nby a 9 that does not satisfy will-beavaiJ. Fig. 7 gives the WiJJBeAvaiJ propagation algorithms. As in \n[KR,S92], we use the term placement to refer to the set of points in the program where a particular expression \ns value is computed. LEMMA 5 (Correctness of can-be-avail) A @ satisfies can-be-avail if and only if \nsome safe placement of insertions makes the expression available immediately after the a. Proof: Let \nF be a 9 satisfying can-be-avail. If F satisfies downsafe, the result is immediate because it is safe \nto in- sert computations of the expression at each of F s operands. If F is not down-safe and satisfies \ncan-beavail, note that the expression is available in the unoptimized program at F because there is no \npath to F from a + with a I-valued operand along def-use arcs in the SSA graph. Now let F be a Cp that \ndoes not satisfy can-be-avail. When the algorithm reset this can-be-avail flag, the recur-sion stack \nof Reset-can-be-avail gives a path bearing witness to the fact that no safe set of insertions can make \nthe ex-pression available at F. 0 LEMMA 6 (Correctness of later) A can-beavail @ satisfies later after \nWillBeAvail if and only if there exists a compu-tationally optimal placement under which that 9 s result \nis not available immediately after the a. Proof: The set of e s not satisfying later after WiJJBeAvaiI \nis exactly the set of can-beavail ?p s reachable along def-use arcs in the SSA graph from has-real-use \noperands of can-bead1 ip s. Let P be a path in the def-use SSA graph from such a 9 operand to a given \nexpr-3 F with later(F) = false. We will prove by induction on the length of P that F must be made available \nby any computationally optimal placement. If F is not down-safe, the fact that F is can-beAvaiJ means \nall of F s operands must be fully available in the unoptimized program. They are therefore trivially \navailable under any computationally optimal placement, making the result of F available as well. In the \ncase where F is down-safe, if P contains no arcs there is a has-real-use operand of F. Such an operand \nmust procedure Compute-can-be-avail for each expr4 F in program do if (not downsafe and can-be-avail(F) \nand 3 an operand of F that is I) Reset-can-be-avail(F) end Compute-can-be-avail procedure Reset-can-be-avail(G) \ncan-beavaiJ(G) t false for each expr4 F with operand cqmd defined by G do if (not has-real-use(opnd)) \n{ set that ip operand to I if (not downs&#38;e(F) and can-beavail( Reset-can-be-avail(F)7 end -Reset-can-beavail \nprocedure Computedater for each expr-O F in program do later(F) c can-beavaiJ(F) for each expr-+ F in \nprogram do if (later(F)) and 3 an operand opnd of F such that (opnd # I and has-real-use(opnd))) Reset-later(F) \nend Compute-later procedure Reset-later(G) later(G) t false for each expr4 F with operand opnd defined \nby G do if (later(F)) Reset-later(F) end Reset-later procedure WiJJBeAvail Compute-can-beavail Compute-later \n end WiJJBeA vail Figure 7: Algorithm for WiJJBeAtiJ be fully available in the optimized program, so \nany inser-tion below F would be redundant with that operand, con-tradicting computational optima&#38;y. \nSince F is down-safe, that operand is already redundant with real occurrence(s) in the unoptimized program \nand any computationally opti-mal placement must eliminate that redundancy. The way to accomplish this \nis to perform insertions that make the expression fully available at F. If F is down-safe and P contains \nat least one arc, we ap- ply the induction hypothesis to the Q, defining the operand of F corresponding \nto the final arc on P to conclude that that operand must be made available by any computation- ally optimal \nplacement. As a consequence, any computa- tionally optimal placement must make F available by the same \nargument as in the basis step (previous paragraph). 0 The following lemma shows that the wiJJ-beavail \npred-icate computed by WiJJBeAvaiJ faithfully corresponds to availability in the program after insertions \nare performed for @ operands satisfying insert. LEMMA 7 (Correctness of will-beavail) The set of inser-tions \nchosen by SSAPRE together with the set of real occur- rences makes the ezpression available immediately \nafter a + if and only if that 0 satisfies will-beavail. Proof: We establish the if direction with a simple \ninduc- tion proof showing that if there is some path leading to a particular % in the optimized program \nalong which the ex-pression is unavailable, that + does not satisfy will-be-avail. Let Q(k) be the following \nproposition: For any expr-9 F, if there is a path P(F) of length k in the SSA def-use graph begin-ning \nwith I, passing only through Cp s that are not will-be-avail along arcs that do not satisfy has-real-use \nV insert, and ending at F, F is not will-beavail. Q(0) follows directly from the fact that no insertion \nis performed for any operand of F, since it is not marked will-be-avail. The fact that F has a I-valued \noperand im-plies that such an insertion would be required to make F available. Now to see Q(k) for k \n> 0, notice that Q(k -1) implies that the operand of F corresponding to the final arc of P(F) is defined \nby a Cp that is not will-be-avail, and there is no real occurrence of the expression on the path from \nthat defining 9 to the operand of F. Since we do not perform an insertion for that operand, F cannot \nsatisfy will-be-avail. To establish the only if direction, suppose expr-+ F does not satisfy will-beavail. \nEither F does not satisfy can-be-wail or F satisfies later. In the former case, F is not available in \nthe optimized program because the insertions performed by SSAPRE are down-safe. In the latter case, F \nwas not processed by Reset-Later, meaning that it is not reachable along def-use arcs from a Cp satisfying \nwill-bear&#38;l. Therefore, insertion above F would be required to make F s result available, but F is \nnot will-beavail so the algorithm performs no such insertion. 0  4.5 The Finalize Step The Finalize \nstep plays the role of transforming the SSA graph for the hypothetical temporary h to the valid SSA form \nthat reflects insertions and in which no @ operand is 1. The Finalize step performs the following tasks: \nIt decides for each real occurrence of the expression whether it should be computed on the spot or reloaded \nfrom the temporary. For each one that is computed, it also decides whether the result should be saved \nto the temporary. It sets two flags, reload and save, to represent these two pieces of information. For \n9 s where will-be-avail is true, insertions are per-formed at the incoming edges that correspond to Cp \noperands at which the expression is not available. Expression a s whose will-be-avail predicate is true \nmay become 4 s for t. 9 s that are not will-beavail will not be part of the SSA form for t, and links \nfrom will-be-avail Cp s that reference them are fixed up to refer to other (real or inserted) occurrences. \nExtraneous @ s are removed. Finalize creates a table Avail-de&#38; (for available definitions) for each \nexpression Ei to perform the first three of the above tasks. The indices into this table are the SSA \nversions for E; s hypothetical temporary h. Avail-defi[z] will point to the defining occurrence of Ei \nfor h,, which must be either: (a) a real occurrence, or (b) a + for which will-be-avail is true. Finalize \nperforms a preorder traversal of the domina- tor tree of the program control flow graph. In the course \nof this traversal it will visit each defining occurrence whose value will be saved to a version of the \ntemporary, t,, before it visits the occurrences that will reference t,; such a ref-erence is either: \n(a) a redundant computation that will be replaced by a reload oft,, or (b) a use of h, as a 9 operand \nthat will become a use of t, as a $ operand. Although the processing order of Finalize is modeled after \nthe standard SSA rename step [CFR+Sl], Finalize does not require any renaming stack because SSA versions \nhave already been as- signed. In the course of its traversal, Finalize will process occur-rences as follows: \n9 -If its will-beavail is false, nothing needs to be done. (An example of this is the + in block 3 of \nour running example. See Fig. 5.) Otherwise, we must be visiting h, for the first time. Set AvaiJdefi[z] \nto this 0. Real occurrence of Ei -If AvaiLdefi[z] is I, we are visiting h, the first time. If Avail-defi[z] \nis set, but that occurrence does not dominate the current occur-rence, the current occurrence is also \na definition of h,. (An example of this latter case is the first hz in block 9 of our example.) In both \nof these cases, we update Avail-defi[z] to the current occurrence. Otherwise, the current occurrence \nis a use of h,, and we set the save flag in the occurrence pointed to by Avail-de&#38;[z] and the reload \nflag of the current occurrence. Operand of 0 in a successor block4 -If will-beavail of the @ is false, \nnothing needs to be done. Otherwise if the operand satisfies insert, (e.g., operand hz in the Cp at block \n6 of our example), insert a computation of E, at the exit of the current block. If will-be-avail holds \nbut the operand does not satisfy insert, set the save flag in the occurrence pointed to by Avail-defi[z] \n(which cannot be 1), and update that Cp operand to re- fer to AvaiLdefi[z] (e.g. operand ha in the @ \nat block 8 of our example). The full algorithm to perform the above tasks is given in Fig. 8. The removal \nof extraneous a s, or SSA minimization, for h is not a necessary task as far as PRF is concerned. However, \nthe extraneous a s take up storage in the program representation, and may a&#38;t the efficiency of other \nSSA-based optimizations to be applied after PRE. Removing extraneous Cp s also requires changing their \nuses to refer to their replacing versions. SSA minimization can be imple- mented as a variant of the \n4 insertion step in SSA construc- tion [CFR+Sl, JPP94, SG95]. We initially mark all the 9 s as being \nextraneous. Applying the 4 insertion algorithm, we can find and mark the Cp s that are not extraneous \nbased on the iterated dominance frontier of the set of real assignments to h in the program (i.e., real \noccurrences with the save bit set plus the inserted computations). We then pass over all the extraneous \n9 s to determine a replacing version for each one. Whenever an extraneous ip defines version h, and has \nan operand using h, that is not defined by an extraneous @, y is the replacing version for 2. From such \na * we propagate the replacing version through all its uses: once the replacing version for a Cp is known, \nthe replacing version for every use of that @ becomes known (the replacing version of each use is the \nsame as the replacing version of the a) and we prop- agate recursively to all uses of that 9. It straightforward \n*Recall that + operands are considered as occurring at their cor- responding predecessor blocks. procedure \nFinalize-visit( block) for each occurrence X of Ei in block do { save(X) t false reload(X) t false 2 \nt version(X) if (X is a) { if (will-beavaiJ(X)) Avail-def[i][z] t X Else if (Avail-def[i][z] is I or \nAvail-def [i][z] does not dominate X) Avail-def[i][z] t X else if (Avail-def[i][z] is real) { save(Avaibdef[i][z]) \nt true reload(X) t true ibr each S in Succ(block)do { j + WhichPred(S, block) for each expr4 F in S \ndo if (wilLbeatil(F)) { i c WhichExpr(F) if (jthoperand of F satisfies insert) { insert Ei at the exit \nof block set, jthoperand of F to inserted occurrence 1 else { I t version(jthoperand of F) if (Avail-def[i][z] \nis real) { save(AvaiJ-def[i][z]) t true set jthoperand of F to AvaiJdef[i][z] 1 ? 1 for each K in ChiJdren(DT, \nblock) do Finalize-visit(K) end Finalize-visit procedure FinaJize for each version z of Ei in program \ndo Avail-def[i][z] t I Finalize-visit(Root(DT)) end Finalize Figure 8: Algorithm for FinaJize to see \nthat this method replaces all references to extraneous a s by references to non-extraneous occurrences. \nFig. 9 shows our example program at the end of the Fi- nalize step. LEMMA 8 (Corre&#38;ness of save/reload) \nAt the point of any reload, the temporary contains the value of the ezpression. Proof: This lemma follows \ndirectly from the Finalize algo-rithm and from the fact that Rename assigns versions while traversing \nthe SSA graph in dominator-tree preorder. In particular, Finalize ensures directly that each reload is \ndom- inated by its available definition. Because the live ranges of different, versions of h do not overlap, \neach reloaded occur-rence must refer to its available definition. 0 LEMMA 9 (Optimality of reload) The \noptimized program does not compute the eqreasion at any point where it is fully available. Figure 9: \nProgram after Finalize Proof: It is straightforward to check that the optimized program reloads the \nexpression value for any occurrence de-fined by a @ satisfying will-be-avail, and it reloads the ex-pression \nvalue for any occurrence dominated by another real occurrence of the same version. Therefore we need \nonly note that will-beavail accurately reflects availability in the opti- mized program (by Lemma 7) \nand that by the definition of insert we only insert for 6 operands where the insertion is required to \nachieve availability. 0 4.6 The CodeMotion Step Once the hypothetical temporary h has been put into \nvalid SSA form, the only remaining task is to update the SSA program representation to reflect. the results \nof PRE. This involves introducing the real temporary t for the purpose of eliminating redundant computations. \nThis task is straight- forward due to the fact that h is already in valid SSA form. The SSA form oft \nis a subgraph of the SSA form of h, since defs of h (including + s) with no use are omitted. The CodeMotion \nstep walks over the SSA graph of h. At a real occurrence, if save is true, it generates a save of the \nresult of the computation into a new version of t. If reload is true, it replaces the computation by \na use of t. At an inserted occurrence, it saves the value of the inserted computation into a new version \noft. At a Cp of h, it generates a corresponding r$ for t. Fig. 10 shows our example program at the end \nof the CodeMotion step. 4 4    tltal+h L a3 i- to t a3 + b1 9 7 t 1 a4 t d(az,aa) I &#38; w3.:, \nI* exit ii!7 10 Figure 10: Program after CodeMotion 5 Theoretical Results In this section we derive \nour main results about SSAPRE from the lemmas already given. THEOREM 1 SSAPRE chooses a safe placement \nof compu- tations; i.e., along any puth from entry to ezit ezactly the same values are wmputetf in the \noptimized program as in the originalprogram. Proof: Since insertions take place only at points satisfy- \ning downsafe, this theorem follows directly from Lemma 4. cl THEOREM 2 SSAPRE genemtes a reload of the \ncorrect ex-pressionvalue from temporary at a real occurrence point if and only if the expression value \nis availableat that point in the optimizedprogram. Proof: Thii theorem follows from the fact that reloads \nare generated only when the reloaded occurrence is dominated by a will-beavajJ 9 of the same version \n(in which case we appeal to Lemma 7 for the availability of the expression at the reload point), or by \na real occurrence of the same version that is marked save by Finalize. 0 THEOREM 3 SSAPRE generates a \nsave to temporary at a real occurrence or insertion point if and only if the following hold: . the expressionvalut \nis unavailable (in the optimized program) just before that point, and . the ezpression value is partially \nanticipated just after that point (i.e., there will be a use of the saved value). Proof: This theorem \nfollows directly from Lemma 9 and fromthe fact that the Finalize algorithm sets the save flag for a real \noccurrence only when that occurrence dominates a use of the same version by another real occurrence or \nby a + operand. In the former case the result is immediate, and in the latter case we need only appeal \nto the fact that the expression is partially anticipated at every 9 remaining after the Rename step. \n0 THEOREM 4 SSAPRE chooses a computationally optimal placement; i.e., no safe placement can result in \nfewer wm-putations along any path jrom entry to ezit in the control jfow graph. Proof: We need only show \nthat any redundancy remain-ing in the optimized program cannot be eliminated by any safe placement of \ncomputations. Suppose P is a control flow path in the optimized program leading from one computa- tion, \nQi, of the expression to another computation, $2, of the same expression with no assignment to any operand \nof the expression along P. By Theorem 2, the expression value cannot be available just before $2, so \n$2 is not dominated by a real occurrence of the same version (by Lemma 9) nor is it defined by a wiJJ_beavaiJ \n% (by Lemma 7). Because $1 and $2 do not have the same version and there is no as- signment to any expression \noperand along P, the definition of $2 ~ version must lie on P, and since it cannot be a real occurrence \nnor a will-be-avail %, it must be a + that is not will-be-avail. Such a ip cannot satisfy later because \none of its operands is reached by r+!~i, so it must not be down-safe. So no safe set of insertions could \nmake $2 available while eliminating a computation from P. 0 THEOREM 5 SSA PRE chooses a lifetime-optimal \nplacement; specifically,if p is the point just after an insertionmade by SSAPRE and C denotes any computationally \noptimal place- ment, C makes the expression available at p. fully Proof: This theorem is a direct consequence \nof Lemma 6 and Theorem 4. 0 THEOREM 6 SSAPRE produces minimal SSA form for the generated temporary. Proof: \nThis minimality result follows directly from the correctness of the dominance-frontier +-insertion algorithm. \nEach @ remaining after Finalize is justified by being on the iterated dominance frontier of some real \nor inserted occur-rence that will be saved to the temporary. 0 6 Practical Implementation Since SSAPRE \nis a sparse algorithm, an implementation can reduce the maximum storage needed to optimize all the expressions \nin the program by finishing the work on each expression before moving on to the next one. Under this \nscheme, the different lexically identical expressions that need to be worked on by SSAPRE are maintained \nas a workliit. If the expressions in the program are represented in tree form, we can also exploit the \nnesting relationship in expression trees to reduce the overhead in the optimization of large expressions. \nThere is also a more efficient algorithm for performing the Rename step of SSAPRE. In this section, we \ngive a brief description of these implementation techniques. 6.1 Worklist-driven PRE Under worklist-driven \nPRE, we add an initial pass, CoJJect-Occurrences, that scans the entire program and creates a worklist \nfor all the expressions in the program that need to be worked on by SSAPRE. For each element of the work- \nlist, we represent its occurrences in the program as a set of occurrence nodes. Each occurrence node \nprovides enough information to pinpoint the location of the occurrence in the program. Collect-Occurrences \nis the only pass that needs to look at the entire program. The six steps of SSAPRE oper-ate on each expression \nbased only on its occurrence nodes. The intermediate storage needed to work on each expression can be \nreclaimed when working on the next one. Collect-Occurrences enters only first order expressions into \nthe worklist. First order expressions contain only one operator. For example, in the expression (a + \nb) -c, a + b is the fist order expression and is entered into the worklist, but (a + b) -c is not initially \nentered into the worklist. After SSAPRE has worked on a + 6, any redundant occurrence of a + b will be \nreplaced by a temporary t. If PRE on a + b changes (a + b) -c to t -c, the CodeMotion step will en- ter \nthe new first order expression t -c as a new member of the worklist. Redundant occurrences of t -c, and \nhence redundancies in (a + b) - c, will be replaced when t-c is pro- cessed. If the expression (a + b) \n- c does not yield t - c when a + b is being worked on, a + b is not redundant, implying that (a + b) \n-c has no redundancy and can be skipped by SSAPRE. This approach deals cleanly with the interaction between \nthe optimizations of nested expressions and gains efficiency by ignoring the higher order expressions \nthat ex-hibit no redundancy. This strategy is hard to implement in bit-vector PRE, which typically works \non all expressions in the program simultaneously in order to take advantage of the parallelism inherent \nin bit-vector operations. In manipulating the sparse representation of each expres- sion, some steps \nin the algorithm need to visit the occurrence nodes in an order corresponding to a preorder traversal \nof the dominator tree of the control flow graph. For this pur- pose, we maintain the occurrence nodes \nfor a given expres- sion in the order of this preorder traversal of the dominator tree. As we mentioned \nin Section 4.2, there are three kinds of occurrences. Collect-Occurrences only creates the real oc- currence \nnodes. The O-Insertion step inserts new occurrence nodes that represent a s and ip operands. Under worklist-driven \nPRE, we need a fourth kind of occurrence nodes to indicate when we reach the program exits in the Rename \nstep. These etit occurrence nodes can be represented just once and shared by all expressions. Fig. 11 \nis a flow chart for our SSAPRE implementation. 6.2 Delayed Renaming The Rename algorithm described in \nSection 4.2 maintains version stacks for all the variables in the program in addition to the version \nstacks for the expressions. Apart from taking up additional storage, updating the variable stacks requires \nkeeping track of when the values of the variables change, which may incur significant overhead. The algorithm \nis not in line with sparseness, because in a sparse algorithm, the time spent in optimizing an expression \nshould not be affected by the number of times its variables are redefined. Also, un- der the worklist-driven \nimplementation of SSAPRE, we can no longer pass over the entire program in the Rename step, because that \nwould imply passing over the entire program once for every expression in the program. The solution of \nsFor higher order expressions that have redundancies, this ap-proach also has the secondary effect of \nconverting the expression tree essentially to triplet form. input HSSP program initial worklist I dense \nSSA graph t precise SSA graph f output HSSA program Figure 11: SSAPRE implementation flow chart both \nof these problems is to use a more efficient algorithm for renaming called delayed renaming. Recall the \npurpose of the variable stacks in the Rename step is to enable us to determine when the value of an avail- \nable expression is no longer current by checking if the ver-sions of all the variables are the same as \nthe current versions. At a real occurrence of the expression, we do not have to rely on the variable \nstacks, because the current versions of all its variables are represented in the expression. We only \nneed the variable stacks when renaming ip operands. To implement delayed renaming, the Rename step is \nre- placed by two separate passes. The first pass, Rename-l, is the same as Rename, except that it does \nnot use any vari- able stack. At a * operand, it optimistically assumes that its version is the version \non top of the expression stack. Thus, it can perform all its work based on the occurrence nodes of the \nexpression. Rename-l computes an initial version of the SSA graph for h that is optimistic and not entirely \ncor-rect. The correct renaming of 9 operands is delayed to the second pass, Rename-2, which relies on \nseeing a later real occurrence of the expression to determine the current ver-sions of the variables. \nSeeing a later real occurrence implies that at the earlier +, the expression is partially anticipated. \nThus, the versions of the @ operands are fixed up only for these @ s. Rename-2 works according to a worklist \nbuilt for it by Rename-I, which contains all the real occurrences that are defined by 0 s. From the versions \nof the variables at the merge block of a 0, it determines the versions of the variables at each predecessor \nblock based on the presence or absence of 4 s for the variables at that merge block. If they are different \nfrom the versions assumed at the ip operand in the Rename-l pass, Rename-2 invalidates the 9 operand \nby condition for data flow equations and the initialization of the local data 1 real real all corresponding \nvariables 2 real 9 operand have same versions 3 cp real defs of all variables in t i i cp /@onerand] \nX dominate the 9 1 Table 1: Assigning h-versions in Delayed Renaming resetting it to 1. Otherwise, the \n@ operand renamed by Rename-1 is correct. If the @ operand is also defined by Cp, it is added to the \nworklist so that the process can continue up the SSA graph. For example, Rename-l will initially set \nthe second operand of the 0 for h in block 8 of Fig. 5 to ha. Rename-Z resets it to 1. Table 1 gives \nthe rules for deciding when two occurrences should be assigned the same h-version in the absence of the \nvariable stacks. Rules 1 and 3 are applied in Rename-l, while rules 2 and 4 are applied in Rename-2. \nAn additional advantage of delayed renaming is that it allows us to determine the ip s that are not live \nwithout per-forming a separate dead store elimination phase. In delayed renaming, only the operands at \n9 s at which the expression is partially anticipated are fixed up. The remaining Cp s cor- respond to \ndead a s, and they can be marked for deletion.  7 Analysis While the formulation of the optimal code \nmotion algorithm in SSAPRE is self-contained, we can gain additional insight by comparing SSAPRE with \na slotwise implementation of lazy code motion. We can regard the @-Insertion and Re-name steps to construct \nthe SSA graph for the hypothetical temporary as corresponding to the initialization of data flow information; \nthese two steps are faster in SSAPRE because we take full advantage of the SSA form of the input pro-gram. \nWhile down-safety corresponds to the same attribute in lazy code motion, the correlation in the part \nthat involves forward propagation of data flow information is less direct. Since we have shown that our \nalgorithm yields the same results as lazy code motion, it is quite plausible that the forward propagation \nparts in SSAPRE and a slotwise im- plementation of lazy code motion can be proven essentially equivalent. \nBut because slotwise analysis propagates with respect to the control flow graph and SSAPRE propagates \nwith respect to the sparse SSA graph, the propagation in SSAPRE will take fewer steps. The SSA graph \nof the hypo- thetical temporary also allows SSAPRE to easily maintain the generated temporary in SSA \nform. The complexities of the various steps in SSAPRE can be easily established. Assuming the implementation \ndescribed in Section 6, the Rename, DownSafety, WillBeAtiI, Final-ize and CodeMotion steps are all linear \nwith respect to the sum of the number of nodes (u) and edges (e) in the SSA graph. The +-Insertion step \nis 0(v ) for insertion at domi-nation frontiers, but as we explained in Section 4.1, there are linear-time \nSSA h-placement algorithms that can be used to lower it to O(e). The second kind of Cp insertion due \nto variable 4 s is also linear using our demand-driven algo-rithm. Thus, for a program of size n, SSAPRE \ns total time is O(n(E + V)), where E and V are the number of edges and nodes in the control flow graph \nrespectively. This is pleasing given that SSAPRE replaces both the solution of flow attributes in bit-vector-based \nPRE. 8 Measurements We have implemented SSAPRE in WOPT, the global optimizer in the Silicon Graphics \nMIPSpro Compilers. The optimizer uses a variant of SSA called HSSA as its internal program representation \n[CCL+96]. The optimizer had used the bit-vector-based Morel and Renvoise algorithm [Cho83] to perform \nPRE, while it uses known SSA-based algorithms for its other optimizations. In Release 7.2 of the compiler, \nwe have re-implemented the PRE phase using SSAPRE, in-corporating the techniques we described in Section \n6. In this section, we compare their performance differences using the SPECint95 and SPECfp95 benchmark \nsuites. In terms of optimization results, measured by the run-ning time of the benchmarks, the differences \nbetween the two implementations of PRE are not noticeable. We are more in- terested in comparing the \noptimization efficiencies between the sparse approach and the bit-vector approach. Both im-plementations \nof PRE start out with an SSA representation of the program. The bit-vector-based PRE starts by deter- \nmining the local attributes and setting up the bit vectors for data flow analyses. Our bit vectors are \nrepresented as arrays of 64-bit words, and their operations are very efficient. The bit-vector-based \nPRE does not update the SSA representa- tion of the program; instead it encodes the effects of PRE in \nbit vector form until it is ready to emit the output program. Our timing for the bit-vector-based PRE \nincludes only the local attributes phase and the solution time of the PRE data flow equations. Correspondingly, \nwe omit the CodeMotion step from the SSAPRE timing and include only the CoUect- Occurrences pass and \nthe first five SSAPRE steps. Table 2 gives our timing results as measured on a 195 MHz RlOOOO Silicon \nGraphics Power Challenge. The benchmarks were compiled under the optimization level -02, which does not \ninvoke procedure inlining. The measurements in Table 2 show widely different re- sults across the various \nbenchmarks. In the SPECint95 benchmarks, SSAPRE ranges from 65% faster in per1 to 29% slower in go. In \nthe SPECfp95 benchmarks, SSAPRE is usually slower, sometimes by up to 2.8 times, as in the case of mgrid. \nWithout examining the sizes and character- istics of each benchmark s procedures in detail, we cannot \ncharacterize from these measurement results the situations in which our SSAPRE implementation is superior \nto our bit- vector implementation. Even so, we see that the efficiency of sparse implementation stands \nout mainly in large proce- dures. In small procedures, a sparse graph cannot be much simpler than the \ncontrol flow graph, so it is much harder to beat the performance of bit vectors that process 64 expres- \nsions at a time. The advantage of sparse implementations increases with procedure size. In large procedures, \nmany expressions do not appear throughout the procedure, and their sparse representations are much smaller \ncompared to the control flow graph. Despite the strong bias towards bit-vector-based PRE being faster \nin our set of measurements, we think SSAPRE is very promising. The time complexity of collecting local \nattributes is fI(n ). A number of techniques contribute to speeding up bit-vector data flow analysis, \nbut there is lit- tle promise of overcoming the cubic complexity of local at-tribute collection in the \nbit-vector approach. As data flow SPECint95 Benchmarks go m88ksim SC compress li iiwg per1 vortex Bit-vector \nPRE (Tl) 116900 4850 886360 SSAPRE (T2) 151260 4440 339160 Ratio T2/Tl 1.293 0.915 0.382 SPECfp95 Benchmarks \nj tomcatv swim su2cor hydro2d Bit-vector PRE (Tl) 1 40 170 500 7080 SSAPRE (T2) I 60 400 700 8780 Ratio \nT2/Tl 1 1.500 2.352 1.399 1.240 Table 2: Time (in msec.) spent in Partial Redundancy analysis have sped \nup, the time spent collecting local at-tributes has come to dominate: our bit-vector-based PRE spends \n51% of its time in its local attributes collection phase while optimizing our benchmarks. Because of \nthe cubic com- plexity, optimization efficiency is more of an issue in large procedures. With the trend \ntowards more inlining during compilation, large procedures will be more commonplace, and the efficiency \nadvantages of sparse implementation will become more obvious. There is still work to be done in tuning \nthe implementa- tion of SSAPRE. Using a characterization of the common sizes and forms of SSA graphs \nof the hypothetical temporary, we expect to improve the implementation of many parts of the algorithm \nto speed up SSAPRE s processing. Investi-gation into SSAPRE s wide compile-time performance dif-ferences \nrelative to bit-vector-based PRE may offer insights that lead to more efficient implementation. Conclusion \nand Further Work The SSAPRE algorithm presented in this paper performs PRE while taking full advantage \nof the SSA form in the in- put program and within its operation. It incorporates the advantages shared \nby all the other SSA-based optimization techniques: no separate phase to collect local attributes, no \ndata flow analysis involving bit vectors, sparse representa- tion, sparse computation of global attributes, \nand unified handling of each optimization s global and local forms. In actual implementation, by working \non one expression at a time, we can also lower the maximum storage requirement needed to optimize all \nthe expressions in the program, and also exploit the nesting relationship in expression trees to speed \nup the optimization of large expressions. SSAPRE enables PRE to be seamlessly integrated into a global \noptimizer that uses SSA as its internal represen-tation. Because the SSA form is updated as optimization \nprogresses, optimizations can be re-invoked as needed with- out incurring the cost of repeatedly rebuilding \nSSA. From an engineering point of view, SSAPRE permits a cohesive software implementation by making SSA \nand sparseness the theme throughout the optimizer. Previous uses of SSA were directed at problems related \nto variables. SSAPRE represents the first use of SSA to solve data flow problems related to expressions \nor operations in the program. This work shows that data flow problems for expressions can be modeled \nin SSA form by introducing hy- pothetical temporaries that store the values of expressions. Such an approach \nopens up new ways to solve many data flow problems by first formulating their solution in terms of the \nSSA graph of the hypothetical temporary. Candidates for this new approach are code hoisting and the elimination \nof load and store redundancies [Cho88, KRS94b]. We intend 100 12950 10340 98840 62950 60 5090 11200 34970 \n53000 0.600 0.393 1.083 0.353 0.841 mgrid wdu turb3d 500 5060 2420 1400 9450 5000 2.799 1.867 2.066 \nElimination in compiling wi fpwp wave5 37930 1450 94150 93960 1980 85800 2.477 1.365 0.911 SPECint95 \nand SPECfp95 to pursue such work in the near future. The SSAPRE approach can also incorporate techniques \ndeveloped in the context of classical PRE, such as the in-tegration of strength reduction into the PRE \noptimization phase [Cho83, Dha89, KRSSS]. We currently have a working prototype of SSAPRE that includes \nstrength reduction and linear function test replacement. Processing expressions one at a time also allows \nother possibilities for SSAPRE by customizing the handling of dif- ferent types of expressions. For example, \none might suppress PRE for expressions that are branch conditions because the branch instructions can \nevaluate the conditions without ex-tra cost. One might also move selected loop-invariant op-erations \nout of loops to points that are not down-safe be-cause they will not raise exceptions. Since SSAPRE works \nbottom-up with respect to an expression tree, it can reasso- ciate the expression tree when no optimization \nopportunity was found with the original form. This last possibility rep- resents a different approach \nfor addressing the code shape issue in PRE discussed in [BC94]. We intend to report on any interesting \nresults in future publications. Acknowledgement The authors would like to thank Ash Mnnshi, Ron Price \nand Ross Towle for their support of this work in the MIPSpro compilers. Peter Dahl and Mark Streich also \ncontributed to the work described in this paper. Lastly we thank the conference referees, whose comments \nhelped improve this paper. References [AWZSS] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality \nof values in programs. In Conference Record of the Fifteenth ACM Sym-posium on Principles of Programming \nLan-guages, pages l-11, January 1988. [BC94] P. Briggs and K. Cooper. Effective partial re-dundancy \nelimination. In Proceedings of the ACM SIGPLAN 94 Conference on Progmm-ming Language Design and Implementation, \npages 159-170, June 1994. [CCFSl] J. Choi, R. Cytron, and J. Ferrante. Auto-matic construction of sparse \ndata flow eval-uation graphs. In Conference Record of the Eighteenth ACM Symposium on Principles of Programming \nLanguages, pages 55-66, January 1991. [CCL+961 F. Chow, S. Chan, S. Liu, R. Lo, and M. Streich. Effective \nrepresentation of aliases and indirect memory operations in ssa form. In Proceedings [CFR+Sl] [CHKW86) \n[Cho83] [Cho88] [Cli95] [CSSSa] [CS95b] [CSSSS] [Dha89] [DRZ92] [DS93] [GSW95] [GWS94] of the Sixth Intema2ional \nConference on Com-piler Construction, pages 253-267, April 1996. R. Cytron, J. Ferrante, B. K. Rosen, \nM. N. Weg- man, and F. K. Zadeck. Efficiently computing static single assignment form and the control \nde- pendence graph. ACM %ns. on Programming Languages and Systems, 13(4):451490, Octo-ber 1991. F. Chow, \nM. Himelstein, E. Killian, and L. We- ber. Engineering a rise compiler. In Proceedings of IEEE COM% CON, \npages 132-137, March 1986. F. Chow. A portable machine-independent global optimizer -design and measurements. \nTechnical Report 83-254 (PhD Thesis), Com-puter Systems Laboratory, Stanford University, December 1983. \nF. Chow. Minimizing register usage penalty at procedure calls. In Proceedings of the ACM SIGPLAN 88 Conference \non Pmgmm-ming Language Design and Implementation, pages 85-94, June 1988. C. Click. Global code motion \nglobal value num- bering. In Proceedings of the ACM SIGPLAN 95 Conference on Programming Language De-sign \nand Implementation, pages 246-257, June 1995. K. Cooper and T. Simpson. See-based value numbering. Technical \nReport CRPC-TR95636-S, Dept. of Computer Science, Rice University., October 1995. K. Cooper and T. Simpson. \nValue-driven code motion. Technical Report CRPC-TR95637-S, Dept. of Computer Science, Rice University., \nOctober 1995. J. Choi, V. Sarkar, and E. Schonberg. Incre-mental computation of static single assignment \nform. In Proceedings of the Sixth International Conference on Compiler Construction, pages 223-237, April \n1996. D. Dhamdhere. A new algorithm for composite hoisting and strength reduction optimization (+ corrigendum). \nJournal of Computer Mathemat-ics, 27:1-14 (+ 31-32), 1989. D. Dhamdhere, B. Rosen, and K. Zadeck. How \nto analyze large programs efficiently and infor- matively. In Proceedings of the ACM SIGPLAN 92 Conference \non Programming Language De-sign and Implementation, pages 212-223, June 1992. K. Drechsler and M. Stadel. \nA variation of knoop, riithing and steffen s lazy code motion. SIGPLAN Notices, 28(5):29-38, May 1993. \nM. Gerlek, E. Stoltz, and M. Wolfe. Beyond in- duction variables: Detecting and classifying se-quences \nusing a demand-driven ssa form. ACM l+ans. on Programming Languages and Sys-tems, 17(1):85-122, January \n1995. M. Gerlek, M. Wolfe, and E. Stoltz. A reference chain approach for live variables. Technical Re- \nport CSE 94-029, Oregon Graduate Institute, April 1994. [ Joh94] [JPP94] [KRS92] [KRS93] [KFtS94a] [KRS94b] \n[LLC96] [MR79] [RWZSS] [SG95] [SKLS~] [wol96] wzg11 R. Johnson. Efficient program analysis using de- \npendence flow graphs. Technical Report (PhD Thesis), Dept. of Computer Science, Cornell University, August \n1994. R. Johnson, D. Pearson, and K. Pingali. The program structure tree: Computing control re-gions \nin linear time. In Proceedings of the ACM SIGPLAN 94 Conference on Progmm-ming Language Design and Implementation, \npages 171-185, June 1994. J. Knoop, 0. Riithing, and B. Steffen. Lazy code motion. In Proceedings of \nthe ACM SIG-PLAN 92 Conference on Programming Lan-guage Design and Implementation, pages 224-234, June \n1992. J. Knoop, 0. Riithing, and B. Steffen. Lazy strength reduction. Journal of Pmgmmming Languages, \n1(1):71-91, March 1993. J. Knoop, 0. Riithing, and B. Steffen. Opti-mal code motion: Theory and practice. \nACM l+ans. on Programming Languages and Sys-tems, 16(4):1117-1155, October 1994. J. Knoop, 0. Riithing, \nand B. Steffen. Par-tial dead code elimination. In Proceedings of the ACM SIGPLAN 94 Conference on Pm-gmmming \nLanguage Design and Implementa-tion, pages 147-158, June 1994. S. Liu, R. Lo, and F. Chow. Loop induction \nvariable canonicalization in parallelizing com-pilers. In Proceedings of the Fourth Intema-tional Conference \non Parallel Architectures and Compilation Techniques, pages 228-237, Octo-ber 1996. E. Morel and C. Renvoise. \nGlobal optimization by supression of partial redundancies. Comm. ACM, 22(2):96-103, February 1979. B. \nK. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant compu-tations. In Conference \nRecord of the Fifteenth ACM Symposium on Principles of Programming Languages, pages 12-27, January 1988. \nV. Sreedhar and G. Gao. A linear time algo-rithm for placing h-nodes. In Conference Record of the Eighteenth \nACM Symposium on Princi-ples of Progmmming Languages, pages 62-73, January 1995. B. Schwarz, W. Kirchg&#38;sner, \nand R. Landwehr. An optimizer for Ada -Design, experiences and results. In Proceedings of the ACM SIGPLAN \n88 Conference on Programming Language De-sign and Implementation, pages 175-184, June 1988. M. Wolfe. \nHigh Performance Compilers For Parallel Computing. Addison Wesley, 1996. M. Wegman and K. Zadeck. Constant \nprop-agation with conditional branches. ACM lkans. on Progmmming Languages and Sys-tems, 13(2):181-210, \nApril 1991.  \n\t\t\t", "proc_id": "258915", "abstract": "A new algorithm, SSAPRE, for performing partial redundancy elimination based entirely on SSA form is presented. It achieves optimal code motion similar to lazy code motion [KRS94a, DS93], but is formulated independently and does not involve iterative data flow analysis and bit vectors in its solution. It not only exhibits the characteristics common to other sparse approaches, but also inherits the advantages shared by other SSA-based optimization techniques. SSAPRE also maintains its output in the same SSA form as its input. In describing the algorithm, we state theorems with proofs giving our claims about SSAPRE. We also give additional description about our practical implementation of SSAPRE, and analyze and compare its performance with a bit-vector-based implementation of PRE. We conclude with some discussion of the implications of this work.", "authors": [{"name": "Fred Chow", "author_profile_id": "81100327963", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P86530", "email_address": "", "orcid_id": ""}, {"name": "Sun Chan", "author_profile_id": "81538405756", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP311678200", "email_address": "", "orcid_id": ""}, {"name": "Robert Kennedy", "author_profile_id": "81100450533", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP31082148", "email_address": "", "orcid_id": ""}, {"name": "Shin-Ming Liu", "author_profile_id": "81100526198", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P263984", "email_address": "", "orcid_id": ""}, {"name": "Raymond Lo", "author_profile_id": "81100147119", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP37024390", "email_address": "", "orcid_id": ""}, {"name": "Peng Tu", "author_profile_id": "81100439481", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP15032280", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258940", "year": "1997", "article_id": "258940", "conference": "PLDI", "title": "A new algorithm for partial redundancy elimination based on SSA form", "url": "http://dl.acm.org/citation.cfm?id=258940"}