{"article_publication_date": "05-01-1997", "fulltext": "\n Exploiting Hardware Performance Counters with Flow and Context Sensitive Profiling Glenn Ammons Thomas \nBall James R. Larus* Dept. of Computer Sciences Bell Laboratories Dept. of Computer Sciences University \nof Wisconsin-Madison Lucent Technologies University of Wisconsin-Madison ammons@cs.wisc.edu tballQresearch.bell-labs.com \nlarusQcs.wisc.edu Abstract A program profile attributes run-time costs to portions of a program s execution. \nMost profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, \nsuch as execution frequency or elapsed time to static, syn-tactic units, such as procedures or statements; \nsecond, they aggressively reduce the volume of information collected and reported, although aggregation \ncan hide striking differences in program behavior. This paper addresses both concerns by exploiting the \nhardware counters available in most modem processors and by incorporating two concepts from data flow \nanalysis-flow and context sensitivity-to report more context for measure- ments. This paper extends our \nprevious work on efficient path profiling to flow sensitive pro!Xng, which associates hardware performance \nmetrics with a path through a proce- dure. In addition, it describes a data structure, the calling context \ntree, that efficiently captures calling contexts for procedure-level measurements. Our measurements show \nthat the SPEC95 benchmarks execute a small number (3-28) of hot paths that account for 9-98% of their \nLl data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex \ndynamic behavior. 1 Introduction A program profile attributes run-time costs to portions of a program \ns execution. Profiles can direct a program-mer s attention to algorithmic bottlenecks or inefficient \ncode [Knu71], and can focus compiler optimizations on the *This research supported by: Wright Laboratory \nAvion-ics Directorate, Air Force Material Command, USAF, under grant #F33615-941-1525 and ARPA order \nno. B550; NSF NY1 Award CCR935 7779, with support from Hewlett Packard and Sun Microsystems; and NSF \nGrant MIP-9625558. The U.S. Government is authorized to reproduce and distribute reprints for Governmental \npurposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are \nthose of the authors and should not be interpreted as necessarily rep resenting the official policies \nor endorsements, either expressed or im-plied, of the Wright Laboratory Avionics Directorate or the U. \nS. Government Permission to make digital/hard copy of part or all this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan- tage, the copyright notice, the title of the publication and its date appear, and notice is given \nthat copying is by permission of ACM. Inc. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee. PLDl 97 Las Vegas, NV, USA 0 \n1997 ACM 0-89791-907-6/9710006...$3.50 parts of a program that offer the largest potential for im- provement \n[CMHSl]. Profiles also provide a compact sum-mary of a program s execution, which forms the basis for \nprogram coverage testing and other software engineering tasks [WHHSO, RBDL97]. Although program profiling \nis widely used, most tools report only rudimentary profiles that apportion execution frequency or time \nto static, syn-tactic units, such as procedures or statements. This paper extends profiling techniques \nin two new di-rections. The first exploits the hardware performance coun-ters becoming available in modem \nprocessors, such as In-tel s Pentium Pro, Sun s UltraSPARC, and MIPS s RlOOOO. These processors have \ncomplex microarchitectures that dy- namically schedule instructions. These machines are diffi- cult to \nunderstand and model accurately. Fortunately, these processors also contain readily accessible hardware \ncounters that record a wide range of events. For example, Sun s UltraSPARC processors [Sun961 count events \nsuch as in-structions executed, cycles executed, instruction stalls of various types, and cache misses \n(collectively, hardware per- formance metrics). Existing profiling systems do not exploit these counters \nand are limited to a few, simple metrics, such as instruction execution frequency [BL94], time in a proce- \ndure [GKM83, Sof93], or cache misses [LW94]. Our second extension increases the usefulness of program \nprofiles by reporting a richer context for measurements, by applying two concepts from static program \nanalysis-flow and context sensitivity. A flow sensitive profile associates a performance metric with \nan acyclic path through a proce- dure. A context sensitive profile associates a metric with a path through \na call graph. Most profiling systems are flow and context insensitive and attribute costs only to syntactic \nprogram components such as statements, loops, or proce-dures. Flow sensitive profiling enables programmers \nboth to find hot spots and to identify dependencies between them. For example, a flow insensitive measurement \nmight find two statements in a procedure that have high cache miss rates, whereas a flow sensitive measurement \ncould show that the misses occur when the statements execute along a common path, and thus are possibly \ndue to a cache conflict. Context sensitive profiling associates a metric with a se- quence of procedures \n(a calling contest) that are active dur- ing intervals of a program s execution. Labeling a metric with \nits calling context can separate measurements from dif- ferent invocations of a procedure. Without a \ncalling context, profiling tools can only approximate a program s context-dependent behavior. For example, \nprofiling systems such as gprof [GI<M83] or qpt [BL94] apportion the cost of a pro- cedure to its callers \nin proportion to the relative frequency of calls between each pair of procedures, although this cal- \nculation can produce misleading results [PF88]. Moreover, context is essential to elucidate hardware \nmea- surements. Many hardware events (such as instruction stalls and cache misses) are affected by execution \nrelationships among program components. Performance tools that as-sociate a metric with an isolated component \nof a program overlook these relationships. Paths capture temporal rela-tionships, by reporting the sequence \nof statements leading up to the behavior of interest. In addition, our measure-ments found that one important \nmetric, data cache misses, are heavily concentrated along a small number of paths in a few routines that \nhave complex dynamic behavior. Tools that report cache misses at the procedure or statement level cannot \nisolate these hot paths.  1.1 Contributions This paper describes how to instrument programs to record \nhardware performance metrics efficiently in a flow sensitive and/or context sensitive manner. Our contributions \nare three-fold: . Flow sensitive profiling extends our technique of path profiling, which previously \nrecorded only the execu-tion frequency of paths in a procedure s control flow graph [BL96]. This paper \ngeneralizes path profiling by associating hardware performance metrics with paths. . Context sensitive \nprofiling provides a calling context for flow sensitive (or other) procedure-level profiles. It uses \na run-time data structure, called the calling con-tezt tree (CCT), to label an arbitrary metric or set \nof metrics with its dynamic calling context. The CCT captures a program s calling behavior more precisely \nthan a call graph, but its size is bounded, unlike a com- plete dynamic call tree. Our techniques for \nconstruct- ing a CCT are general and handle indirect function calls, recursion, and non-local returns. \n. These two techniques may be combined, by using a CCT to record the calling context for paths within \na procedure. This combination provides an efficient approximation to interprocedural path profiling. \n 1.2 Measurements Using the Executable Editing Library (EEL) [LS95], we built a tool called PP (Path \nProtier) that instruments pro- gram executables to record flow sensitive and context sensi-tive profiles. \nPP records not only instruction frequency, but also fine-grain timing and event count information by \nac-cessing the hardware counters on UltraSPARC processors. The run-time overhead of flow sensitive and \ncontext sen-sitive profiling for the SPEC95 benchmarks is 60-80%, on average. Furthermore, the results \nshow that CCTs are quite compact in practice. Our measurements of the SPEC95 benchmarks show that these \nprograms contain a small number of hot paths, which each incur at least 1% of the Ll data cache misses. \nCollec-tive, 3-28 of these paths account for 59-98% of the misses in programs other than 099.go and 126.gcc. \nThese two prcr grams execute many more paths, but lowering the hot path threshold to 0.1%of the Ll misses \nfinds that approximately 1% of the paths (172 and 139, respectively) again account for 42 and 56% of \nthe misses. Most hot paths have above average miss rates. Moreover, hot paths are concentrated in a small \nnumber of routines (l-24), which incur most cache misses (44-99%) and execute roughly ten times as many \npaths as the cold routines. 1.3 Overview The paper is organized as follows. Section 2 summarizes our \nresult on intraprocedural path profiling and Section 3 shows how it can be generalized to perform flow \nsensitive profil- ing of hardware counters. Section 4 describes the CCT data structure, how it is built, \nand how it can be used to record context sensitive profiles. Section 5 describes the implemen- tation \nof PP, which uses both techniques to record a variety of hardware-specific metrics on Sun UltraSPARC \nprocessors. Section 6 presents experimental measurements that show that the overhead and perturbation \nof this technique are reasonable, and examines the hot path phenomenon. Sec-tion 7 describes related \nwork. 2 Efficient Path Profiling This section summarizes our previous work on intrapro- cedural path \nprofiling pL96], which this work extends. Given a procedure s control flow graph (CFG), our path profiling \nalgorithm: Assigns an integer label to every edge in an acyclic CFG such that the sum of integers is \nunique along each unique path from the entry to exit of a procedure. This labelling is also compact, \nas path sums fall in the range 0. . . n -1, where n is the number of potential paths from entry to exit. \nInserts simple instrumentation to track the path sum in an integer register at run-time. The path sum \ncan directly index an array of counters or be used as a key into a hash table of counters (if the number \nof potential paths is large). Transforms a CFG containing cycles (loops), which contain an unbounded \nnumber of potential paths, into an acyclic graph with a bounded number of paths. The algorithm can handle \nreducible and irreducible CFGs. Figure l(a) illustrates the technique in a simple graph containing six \nunique paths from A to F. Each path has a unique path sum, as shown in Figure l(b). Figure l(c) shows \na simple instrumentation scheme for tracking the path sum in a register r, while Figure l(d) shows an \noptimized instrumentation scheme (see [BL96] for details). To instrument a CFG for efficient path profiling, \nit must have a unique entry vertex ENTRY from which all vertices are reachable and a unique exit vertex \nEXIT that is reach-able from all vertices. The algorithm has a straightforward extension for CFGs that \ndo not meet this requirement. The description below breaks the algorithm in two parts: instrumentation \nof acyclic CFGs and a method of transform- ing a CFG containing cycles into an acyclic CFG. A 2 0 BO \nc 2 0 D 1 0 E F ft 0 60 Figure 1: Path profiling edge labelling Path Encoding ACDF ACDEF ABCDF ABCDEF \nABDF ABDEF 0 : 3 4 5 I I (b) l=o l=o- A IAI r+=2 B C B C I=2 r+=2 l= 4 4s 43    D ID1 r+=l r+=l \nE F E F CL&#38; da3 count[r]++ count1:m-+ w 69 and instrumentation. (a) An integer labelling with unique \npath sums; (b) The six paths and their path sums; (c) Simple instrumentation for tracking the path 2.1 \nPath Profiling of Acyclic CFGs Conceptually, the algorithm makes two linear-time traver- sals of the \nCFG, visiting vertices in reverse topological order in each pass. The two passes easily can be combined \ninto a single linear-time pass. In the first pass, each vertex w is labelled with an integer NP(v), which \ndenotes the number of paths from wto EXIT. These values are well-defined because the graph is acyclic. \nAs a base case, NP(EXZT) = 1. If v has successors wi...wUn (which are already labelled, since vertices \nare visited in re- verse topological order), NP(w) = NP(wl) + . . . + NP(wn). The second pass labels \neach edge e of the CFG with an integer Vd(e) such that: . Every path from ENTRY to EXIT generates a path \nsum in the range O...NP(ENTRY) -1. . Every value in the range O...NP(ENTRY) -1 is the path sum of some \npath from ENTRY to EXIT. No processing is required for the EXIT vertex in this pass, as it has no outgoing \nedges. Consider vertex u with suc-cessors WI . ..wn. Since the algorithm traverses the CFG in reverse \ntopological order, we may assume that for each Wi, all the edges reachable from wi have been labelled \nso that each path from wi to EXIT generates a unique path sum in the range O...NP(w;)-1. Given a vertex \ntt, let u s successors be totally ordered wi . ..w., (the order chosen is immaterial). The value associated \nwith edge ei = u + wi is simply the sum of the number of paths to EXIT from all successors w1 .,.w;-1: \n Val(e;) = Cfz: NP(wj). Figure 2 illustrates the labelling process for a vertex u with three successors, \nwi...ws. The paths from each wi to EXZT generate path sums in the range O...NP(wi)-1. The label on edge \nu + wi is 0, so paths from v to EXIT be-ginning with this edge will generate path sums in the range O...NP(wl) \n-1. The label on edge u + wz is NP(wl), so paths from u to EXIT beginning with this edge will gener- \nate path sums in the range NP(w~)...NP(wI)+NP(w~)-1, and so on. Path sums for paths from u to EXIT therefore \nlie in the range O...NP(wl) + NP(w2) + NP(w3) -1. sums; (d) Optimized instrumentation for tracking the \npath sums. I 0 . . . NP(w) + NP(wJ + NP(w ) -I i i . . ..-.... -..__-A..---_--__...L.-.-.; V Figure 2: \nEdge labelling phase. Given the edge labelling, the instrumentation phase is very simple. An integer \nregister r tracks the path sum and is initialized to 0 at the ENTRY vertex. Along an edge e with a non-zero \nvalue, r is incremented by Vd(e). At the EXIT vertex, r indexes an array to update a count (count[r]++) \nor serves as a hash key into a hash table of counters. For details on how to greatly reduce the number \nof points at which r must be incremented, see [BL96, Bal94].  2.2 Path Profiling of Cyclic CFGs Cycles \nin a CFG introduce an unbounded number of paths. Every cycle contains a backedge, as identified by a \ndepth- first search from ENTRY. The algorithm only profiles paths that have no backedges or in which \nbackedges occur as the first and/or last edge of the path. As a result, paths fall into four categories: \n. A backedge-free path from ENTRY to EXIT; . A bra&#38;edge-free path from ENTRY to V, followed by backedge \nv -+ w; . After execution of backedge v + w, a backedge-free path from w to z, followed by backedge z \n+ y; . After executing backedge LJ + w, a backedge-free path from w to EXIT. r;;l hw-cnt = 0 Figure \n3: Instrumentation for measuring a metric over a paths. On out-of-order processors, such as the UltraSPARC, \nis is neces- sary to read the hardware counter after writing it, to ensure that the write has completed. \nThe algorithm of the previous section only guarantees uniqueness of path sums for paths starting at \na single en-try point,. Paths with different starting points, such as the latter three categories, may \ncompute identical path sums. However, a simple graph transformation turns a cyclic CFG into an acyclic \nCFG and extends the uniqueness and compactness properties of path sums to all paths described above, \nregardless of whether or not they are in the same category. For each backedge b = u + w in the CFG, the \ntransfor- mation removes b from the CFG and adds two pseudo edges in its place: b,,,,* = ENTRY -+ w and \nbe,,d = v + EXIT. The resulting graph is acyclic and contains a bounded num-ber of paths. The pseudo \nedges represents paths that start and/or end with a backedge. For example, a path from ENTRY to vertex \nv, followed by backedge b = u + w in the original CFG translates directly to a path starting at ENTRY \nand ending with pseudo edge bend = v + EXIT. The path algorithm for acyclic CFGs runs on the trans formed \ngraph, labelling both original edges and pseudo edges. This labelling guarantees that all paths from \nENTRY to EXIT have unique paths sums, including paths containing pseudo edges. After this labelling, \ninstrumenta-tion is inserted as before, with one exception. Since the pseudo edges do not represent actual \ntransfers of control, they cannot. be instrumented. However, their values are in- corporated into the \ninstrumentation along a backedge: count [r+ENDI+; r=START, where END is the value of pseudo edge bend \nand START is the value of pseudo edge bat,,*. 3 Flow Sensitive Profiling of Hardware Metrics This section \nshows how to extend the path profiling algo- rithm to accumulate metrics other than execution frequency. \nThese hardware metrics may record execution time, cache misses, instruction stalls, etc. Although this \ndiscussion fo-cuses on the UltraSPARC s hardware counters [Sun96], sim- ilar considerations apply to \nother processors. 3.1 Tracking Metrics Along a Path Associating a hardware metric with a path is straightfor- \nward, as shown in Figure 3. At the beginning of a path, set the hardware counter to zero. At the end \nof the path, read the hardware counter and add its value into an accumulator associated with the path. \nSince paths are intraprocedural, hardware counters must be saved and restored at procedure calls. A system \ncan either save the counter before a call and restore it after the call, or save the counter on procedure \nentry and restore it before procedure exit. Our implemen- tation uses the latter approach to reduce code \nsize and to capture the cost of call instructions. The UltraSPARC provides an instruction that reads \nits two 32 bit hardware counters into one 64 bit general pur- pose register. Several additional instructions \nare necessary to extract the two values from the register. In total, our instrumentation requires thirteen \nor more instructions to increment two accumulators and a frequency metric for a path. Both counters can \nbe zeroed by writing a 64 bit reg-ister to the two hardware counters. However, because of the UltraSPARC \ns superscalar architecture, it, is necessary to read the counters immediately after writing them, to \nen- sure that the write completes before subsequent instructions execute. 3.2 Measurement Perturbation \nAn important problem with hardware counters is the pertur- bation caused by the instrumentation. As a \nsimple example, consider using hardware counters to record instruction fre-quency. Each instrumentation \ninstruction executed along a path is counted. Figure 3 shows an example, in which the profiling instrumentation \nintroduces two instructions along path A + B + D + F-one instruction for the read af- ter the write to \nthe hardware counters, and one instruction along B + D. Moreover, instrumenting an executable introduces \nfur-ther perturbation. For example, path profiling requires a free local register in each procedure. \nIf a procedure has no free registers, EEL spills a register to the stack, which re-quires additional \nloads and stores around instructions that originally used the register. These instructions affect a met- \nric. In addition, EEL s layout of the edited code can intro-duce new branches. For simple, predictable \nmetrics, such as instruction fre-quency, a profiling tool can correct for perturbation by us-ing path \nfrequency to subtract the effect of instrumentation code. For other metrics, however, it is very difficult \nto es-timate perturbation effects. For example, if the metric is processor stalls or cache misses, separating \ninstrumentation from the underlying behavior appears intractable. More-over, instrumentation that executes \nat the beginning or end of a path, outside the measured interval, can also cause cache conflicts that \nincrease a program s cache misses. We do not have a general solution for this difficult problem, which \nhas been explored by others [MRW92]. Section 6 contains mea-surements of the perturbation. 3.3 Overflow \nThe UltraSPARC s hardware counters are only 32 bits wide. A metric, such as cycle counts, can cause a \ncounter to wrap in a few seconds. However, our intraprocedural instrumen-tation measures call-free paths, \nand even 32 bit counters Ashok Singhal, Sun Microsystems. Personal communication. Oct. 1996. B C (b) \n Figure 4: (a) A dynamic call tree, (b) its corresponding call graph, and (c) its corresponding calling \ncontext tree. will not wrap on any conceivable path. Moreover, we accu- mulated metrics as 64 bit quantities, \nwhich will handle any program.  4 Context Sensitive Profiling This section describes the calling context \ntree (CCT), a data structure that compactly represents calling contexts, presents an algorithm for constructing \na CCT, and shows how a CCT can capture flow sensitive profiling results and other performance metrics. \n4.1 The Calling Context Tree The CCT offers an efficient intermediate point in the spec- trum of run-time \nrepresentations of calling behavior. The most precise but space-inefficient data structure, as shown \nin Figure 4(a), is the dynamic call tree (DCT). Each tree vertex represents a single procedure activation \nand can pre- cisely record metrics for that invocation. Tree edges rep- resent calls between individual \nprocedure activations. The size of a DCT is proportional to the number of calls in an execution. At the \nother end of the spectrum, a dynamic call graph (DCG) (Figure 4(b)), compactly represents calling behavior, \nbut at a great loss in precision. A graph vertex represents all activations of a procedure. A DCG has \nan edge X + Y iff there is an edge X + Y in the program s dynamic call tree. Although the DCG s size \nis bounded by the size of the program, each vertex accumulates metrics for (perhaps unboundedly) many \nactivations. This accumulation leads to the gprof problem, in which the metric recorded at a procedure \nC cannot be accurately attributed to C s callers. Furthermore, a call graph can contain infeasible paths, \nsuch as M + D + A + CY,which did not occur during the program s execution. The calling contest tree (CCT) \ncompactly represents all calling contexts in the original tree, as shown in Figure 4(c). It is defined \nby the following equivalence relation on a pair of vertices in a DCT. Vertices u and w in a DCT are equivalent \nif: . v and w represent the same procedure, and . the tree parent of v is equivalent to the tree parent \nof w, or u = w. The equivalence classes of vertices in a DCT define the ver- tex set of a CCT. Let Eq(z) \ndenote the equivalent class of vertex 2. There is an edge Eq(u) + Eq(w) in the CCT iff there is an edge \nu -+ w in the DCT. Figure 4(c) shows that the CCT preserves the two unique calling contexts associated \nwith procedure C (M + A + B A C (a) (b) CC) Figure 5: (a) A dynamic call tree containing recursive calls, \n(b) its corresponding call graph, and (c) its corresponding calling context tree. C and M + D + C). A \nCCT contains a unique vertex for each unique path (call chain) in its underlying DCT. Stated another \nway, a CCT is a projection of a dynamic call tree that discards redundant contextual information while \npreserving unique contexts. Metrics from identical contexts will be aggregated, again trading precision \nfor space. A CCT can accurately record a metric along different call paths- thereby solving the gprof \nproblem . Another important property of CCTs is that the sets of paths in the DCT and CCT are identical. \nAs defined above, the out-degree of a vertex in the CCT is bounded by the number of unique procedures \nthat may be called by the associated procedure. Thus, the breadth of the CCT is bounded by the number \nof procedures in a program. In the absence of recursion, the depth of a CCT also is bounded by the number \nof procedures in a program. However, with recursion, the depth of a CCT may be un-bounded. To bound the \ndepth of the CCT, we redefine ver- tex equivalence. Vertices u and w in a DCT are equivalent if: . u \nand w represent the same procedure, and . the tree parent of v is equivalent to the tree parent of w, \nor v = w, or there is a vertex u such that u represents the same procedure as u and w and u is an ancestor \nof both v and w.~ The modified second condition ensures that all occurrences of a procedure P below \nand including an instance of P in the DCT are equivalent. As a result of this new equivalence relation, \nthe depth of a CCT never exceeds the number of procedures in a program since each procedure occurs at \nmost once in any path from the root to a leaf of the CCT. The new equivalence relation introduces backedges \ninto the CCT, so it is not strictly a tree. However, CCTs never include cross-edges or forward edges. \nThis implies that tree edges are uniquely defined and separable from backedges. Unfortunately, backedges \n(which arise solely due to recur-sion) destroy the context-uniqueness property of a CCT with respect \nto a DCT. Figure 5(a) shows a dynamic call tree with recursive call- ing behavior and its corresponding \ncall graph and CCT. The recursive invocation of procedure A and its initial invocation are represented \nby the same vertex in the CCT. A space-precision trade-off in a CCT is whether to distin- guish calls \nto the same procedure from different call sites in a calling procedure. Distinguishing call sites requires \nmore 3The breadth of a DCT is unbounded due to loop iteration. A vertex is an ancestor of itself. struct \nList; struct CallRecord; typedef union {List* le; CallRecord+ cr; int offset) Cellee; typedef union \n{CallRecord* cr; int offset) ListElem; struct CallRecord C int ID; // procedure identifier CallRecord \n*parent; // tree parent int metrics[INJ ; // the metrics Callee children[l ; // the callees 1; struct \nList C // list of dynamic callees ListElem pr; List *next; 1; Figure 6: CCT data structures. space, \nbut is useful for path profiling, as distinct intraproce- dural paths reach the different call sites. \nOther applications of a CCT may aggregate a metric for a pair of procedures, and would not benefit from \nthis increased precision. The approach described below aSSumea that call sites are distin-guished; changes \nto combine them are minor. 4.2 Constructing a Calling Context Tree A CCT can be constructed during a \nprogram s execution, as we now describe. Each vertex of a CCT is referred to as a call record, although \ndifferent run-time activation records may share the same call record. Let CR be the call record associated \nwith the currently active procedure C. The principle behind building a CCT is simple: if D has just been \ncalled by C and is already represented by a record that is one of CR s children, use the existing call \nrecord. Otherwise, look for a call record for D in C s ancestors. If a record DR is found, the call to \nD is recursive, so create a pointer from CR to DR (a backedge), and make DR the current call record. \nIf 12 is not an ancestor of C, create a new call record DR for D and make it a child of CR. The first \ntime that a new callee is encountered, we pay the cost of traversing the parent pointers. However, at \nsub- sequent calls, a callee immediately finds its call record, re-gardless of whether the call is recursive. \nThe C structures in Figure 6 define the basic data types in a CCT. A CallRecord contains two scalar fields \nand two arrays: . ID is an int that identifies the procedure or function associated with the CallRecord. \nWe currently use a procedure s starting address its identifier, although more compact encodings are possible. \n. parent is a pointer to the CallRecord s tree parent. This pointer is NULL if the CallRecord represents \nthe root of the CCT. . metrics is an array of counters for recording met-rics. The size of this array \nis usually fixed for all CallRecords. When combining path profiling with call graph profiling, the size \nof the array depends on the number of paths in a procedure.    (p=j?&#38; 6 D I, 1 IO lrn,*l--f---fizpzJ \nFigure 7: A representation of the first two levels of the CCT from Figure 4. Dashed lines indicate pointers \nto lists and solid lines indicate pointers to call records. For simplicity, each cell is assumed to be \nfour bytes wide. . children is an array of callee slots, one for each call site in the procedure. Each \nslot may be a pointer to a list of children (for indirect calls), a pointer directly to one child, or \nits uninitialized value-an offset to the start of the call record. The low-order two bits of a slot form \na tag that discriminates among the three possible values. Figure 7 illustrates the CCT data structure \nfor the first two levels of the CCT in Figure 4, which includes the proce- dures M, A and D. Each call \nrecord in the structure begins with the corresponding procedure s ID. The root of the CCT is labeled \nwith the special identifier T, which corresponds to no procedure. The distinguished node is useful in \ntwo ways. First, our executable editing tool cannot insert the full in- strumentation in start, which \nis a program s entry point in UNIX. Second, if we extended our tool to handle signals, the CCT would \nneed multiple roots, as signal handlers represent additional entry points into a program. The call record \nfor the root does not accumulate metrics. There are several ways to build a CCT. Our approach al- lows \neach procedure to be instrumented separately, without knowledge of its callers or callees. Each procedure \ncreates and initializes its own call records. When a procedure exe-cutes a call, the caller passes a \npointer to the slot in its call record in which the callee should insert its call record. Our algorithm \ninserts instrumentation at the following points in a program in order to build a CCT: Start of program \nexecution. The initialization code allo- cates a heap for the CCT in a memory-mapped region. The region \nis demand paged, so physical memory is not allocated until it is actually used. The root call record \nis allocated and its callee slot is initialized to a list of one element, the offset back to the beginning \nof the call record. Finally, a SPARC global register, called the callee slot pointer (gCSP), is set to \nthe root s call record. Callers use this register to pass a pointer to a callee slot in the record down \nto the callee. Procedure entry. The instrumentation code first obtains a pointer to the procedure s call \nrecord. The code loads the value pointed to by the gCSP (the procedure s callee slot). The low-order \n2 bits of this value are a tag. If the tag is 0, the value is a pointer to a call record for this procedure. \nIf the tag is 1, this procedure has not been called from this context. Masking off the low-order two \nbits yields an offset from the beginning of the current call record (i.e., the caller s call record). \nThe code then searches the parent pointers, looking for an ancestral instance of the callee. If a record \nis found, this call is recursive and the old record is reused. If no record is found, the code allocates \nand initializes a new call record. Initialization sets the ID and children fields of the new record. \nCallee slots for direct calls are set to the tagged offset of the slot in the record. Slots for indirect \ncalls are set to a tagged pointer to a list containing a single entry, the tagged offset of the beginning \nof the call record. In both cases, the code stores a pointer to the found or allocated record in the \ncallee slot. Finally, if the tag is 2, the callee slot contains a pointer to a list of callees. If this \nprocedure has been called from this site before, its call record is in the list. The code moves its pointer \nto the front of the list, so it can be found more quickly next time. If there is no such pointer in the \nlist, the list s last element is the tagged offset to the caller s call record. This provides enough \ninformation to follow the par- ent pointers and find or create a call record, as before. The callee s \ncall record becomes the new current call record, which is held in a local register call 1CRP. The instru- \nmentation also saves the gCSP to the stack. If an exception or signal transfers control to instrumented \ncode, the normal exception mechanisms restore the 1CRP. If instead control transfers to uninstrumented \ncode, there is no 1CRP to re-store. Thus, exceptions to instrumented code are handled transparently, \nbut the exception handling mechanism must be changed to handle exceptions to uninstrumented code. Signals, \nwhich have resumption semantics, do not have this problem. Procedure ezit. The instrumentation restores \nthe old gCSP from the stack. If an instrumented procedure A calls an uninstrumented procedure B, and \nB calls the instrumented procedures C and D, saving and restoring the gC8P ensures that C and D are correctly \ncounted as children of A. Procedure call. The instrumentation sets the gCSP to the sum of the 1CRP and \nthe offset to the callee slot for this call site. Program ezit. Immediately before the program termi-nates, \nthe instrumentation writes the heap containing the CCT to a 6le from which the CCT can be reconstructed. \n4.3 Associating Metrics with a CCT The instrumentation code uses the metric fields in the cur- rent call \nrecord to capture metrics for the procedure invo-cation. The simplest metric is execution frequency, \nwhich just increments a counter in the CallRecord. To combine intraprocedural path profiles with calling \ncontext simply re-quires a minor change to keep a procedure s array of counters or hash table (Section \n2) in a CallRecord. Recording hardware metrics is slightly trickier. The least expensive approach is \nto record a hardware counter upon entry to a procedure, and to compute and accumu-late the difference \nupon exit. This approach has two prob- lems. First, it will not correctly measure functions that are \nnot returned to in the conventional manner (i.e., due to longjmp s or exceptions). Second, the measured \ninterval ex- tends over the entire function invocation, which may cause 32 bit counters to wrap. To avoid \nthese problems, the in-strumentation also reads the hardware counters along loop backedges. Although \nthis has higher instrumentation costs, it produces more information and mitigates or eliminates these \ntwo problems. 5 Implementation We implemented these algorithms in a tool called PP, which instruments \nSPARC binary executables to profile intrapro cedural paths and call paths. PP is built using EEL (Exe- \ncutable Editing Library), which is a C++ library that hides much of the complexity and system-specific \ndetail of editing executables [LS95]. EEL provides abstractions that allow a tool to analyze and modify \nbinary executables without being concerned with particular instruction sets, executable tile formats, \nor the consequences of deleting existing code and adding foreign code (i.e., instrumentation). 5.1 UltraSPARC \nHardware Counters The Sun UltraSPARC processors [Sun96] implement six-teen hardware counters, which record \nevents such as in-structions executed, cycles executed, instruction stalls, and cache misses. The architecture \nalso provides two program- accessible registers that can be mapped to two hardware counters, so a program \ncan quickly read or set a counter, without operating system intervention. Solaris 2.5 currently does \nnot save and restore these counters or registers on a context switch, so they measure all processes running \non the processor. In our experiments, we minimized other activity by running a process locked onto a \nhigh-numbered processor of a SMP Server (low-number processors service interrupts). The 32 bit length \nof these counters is not a problem as PP records the hardware metrics along acyclic, intraprocedural \npaths.  6 Experimental Results This section presents measurements of the SPEC95 bench-marks, made using \nPP. The benchmarks ran on a Sun Ultra- server E5000-12 167MHz UltraSPARC processors and 2GB of memory-running \nSolaris 2.5.1. C benchmarks were com- piled with gee (version 2.7.1) and Fortran benchmarks were compiled \nwith Sun s f77 (version 3.0.1). Both compilers used only the -0 option. In the runs, we used the ref \nin- put dataset. Elapsed time measurements are for the entire dataset. Hardware metrics are for the entire \nrun, or the last input file in the case of programs, such as gee, with multiple input files. 6.1 Run-Time \nOverhead The overhead of intraprocedural path profiling is low (an av- erage of 32% overhead on the SPEC95 \nbenchmarks, roughly twice that of efficient edge profiling [BL94]). Details are re- ported elsewhere \n[BL96]. The extensions described in this paper increase the run-time cost of profiling, but the over- \nheads remain reasonable. Table 1 reports the cost of three forms of profiling. Recording hardware metrics \nalong in-traprocedural paths (Flow and HW) incurs an average overhead of 80%. Recording hardware metrics \nalong with call graph context (Context and HW) incurs an aver-age overhead of 60%. Finally, recording \npath frequency (no hardware metrics) along with call graph context (Context and Flow) incurs an average \noverhead of 70%. 6.2 Perturbation PP s instrumentation code can perturb hardware metrics. Table 2 reports \na rough estimate of the perturbation for several metrics. The baseline in each case is the unin-strumented \nprogram, which we measured by sampling the UltraSPARC s hardware counters every six seconds (to avoid \nBase Flow and HW Context and HW Context and Flow Benchmark Time Time Overhead Time Overhead Time Overhead \n(54 (s-3 (x base) (se4 (x base) (==I (x base) 099.go 850.9 2517.0 3.0 1642.6 1.9 1949.2 2.3 124.mSSksin-1 \n551.8 1446.9 2.6 1347.6 2.4 1011.4 1.8 126.gcc 330.9 1461.5 4.4 1425.4 4.3 2984.5 9.0 129.com press 343.2 \n968.7 2.8 904.8 2.6 594.0 1.7 13O.li 479.5 1169.5 2.4 1270.4 2.6 961.2 2.0 132.ijpeg 766.2 1494.0 1.9 \n1438.9 1.9 1130.7 1.5 134.perl 333.2 906.7 2.7 790.5 2.4 976.7 2.9 147.vortex 648.1 1545.1 2.4 1672.4 \n2.6 1938.7 3.0 CINT96 Avg 538.0 1438.7 2.7 1311.6 2.4 1443.3 2.7 -lOl.tomcatY 505.9 670.1 1.3 586.7 1.2 \n652.2 1.3 102.&#38;m 693.0 786.7 1.1 766.9 1.1 793.5 1.1 103.su2cor 468.5 587.4 1.3 561.0 1.2 553.5 1.2 \n104.hydro2d 795.6 1490.8 1.9 961.0 1.2 1147.1 1.4 107.mgrid 877.5 1088.6 1.2 1001.5 1.1 962.1 1.1 llO.applu \n710.1 1517.6 2.1 911.1 1.3 1293.3 1.8 125.turb3d 1063.1 1847.6 1.7 1750.1 1.6 1452.2 1.4 14l.apsi 515.0 \n627.1 1.2 636.1 1.2 611.5 1.2 145.fPPPP 2090.4 2172.2 1.0 1978.4 0.9 2310.5 1.1 146.~~~~5 635.6 815.8 \n1.3 741.6 1.2 773.9 1.2 95 vg 835.5 1055.0 1.3   j il SPEC95 Avg 703.3 1284.1 1.8 1 1132.6 1.6 1227.6 \n1.7 Table 1: Overhead of profiling. Base reports the execution time of the uninstrumented benchmark. \nFlow and HW reports the cost of intraprocedural path profiling using hardware counters. Context and HW \nreports the cost of context sensitive profiling using hardware counters. Context and Flow reports the \ncost of context sensitive profiling using only frequency counts (no hardware counters). Cycles Insts \nDCache DCache ICache hllspredict Store FP F&#38;ad Write Miss Stalls Buffer Stalls Misses Misses Stalls \nstalls Benchmark F C F C F C F C F C F C F C F C 099.go 1.22 1.20 1.04 1.10 1.18 1.11 1.07 1.04 3.83 \n2.26 0.84 0.95 0.04 0.49 0.00 0.00 124.m88ksim 1.38 1.33 1.21 1.17 2.37 2.49 1.77 1.28 1.45 2.43 0.53 \n0.52 0.00 18.54 10.74 126.gcc 1.19 1.31 0.97 1.37 1.11 1.00 0.99 0.94 1.28 2.82 0.60 6.17 0.05 130.69 \n1.13 1442.89 129.compres.9 1.58 1.55 1.48 1.40 0.90 0.89 1.00 1.00 1.87 3.28 0.66 0.67 0.31 0.33 0.05 \n0.05 13O.li 1.57 1.50 1.33 1.25 1.24 1.23 1.19 1.29 7.41 3.78 0.99 0.79 38.60 21.65 1.76 1.45 132.ijpeg \n1.18 0.83 1.04 0.80 1.07 1.11 0.86 0.85 9.55 3.19 0.84 0.45 1.54 1.43 1.28 0.73 134.perl 1.77 1.49 1.52 \n1.18 1.77 2.04 1.62 1.77 2.26 1.67 1.10 1.19 0.27 0.18 6.22 3.98 147.vortex 1.46 1.34 1.30 1.14 1.33 \n1.84 0.75 1.12 2.09 2.13 1.30 1.53 0.31 0.78 0.00 0.00 mNTf.6 Avg 1.45 1.34 1.27 1.16 1.25 1.36 0.98 \n1.10 2.35 2.17 0.83 0.83 0.65 1.52 0.05 1.42 lOl.tomcatv 1.12 1.05 1.13 1.04 1.03 0.98 1.27 1.00 1.45 \n1.29 1.10 1.07 5.44 1.21 0.89 0.89 102.swim 1.06 1.07 1.05 1.05 1.11 1.11 1.27 1.27 1.20 1.22 0.99 0.99 \n0.85 0.90 1.00 1.00 103.su2cor 1.06 1.06 1.07 1.04 1.01 1.00 1.05 1.00 5.58 2.24 1.06 1 .oo 0.84 0.91 \n0.98 0.99 104.hydro2d 1.12 1.05 1.22 1.02 0.99 1.00 1.78 1.01 1.32 1.81 0.20 2.30 0.94 0.98 0.99 0.99 \n107.mgrid 1.09 1.05 1.03 1.03 1.01 1.01 0.84 0.87 1.27 1.37 1.01 2.34 0.66 1.00 1.02 1.02 llO.applu 1.25 \n1.03 1.21 1.06 1.00 0.98 1.26 1.00 1.11 1.02 1.11 1.02 1.08 1.01 0.97 1.00 125.turb3d 1.39 1.14 1.13 \n1.04 0.98 0.98 0.95 0.94 27.93 3.19 0.99 0.96 2.21 10.31 0.98 1.18 14l.apsi 1.02 0.98 1.08 1.02 0.99 \n1.00 0.98 1.02 2.04 3.23 1.02 1.87 0.54 0.86 0.97 1.02 145.fPPPP 0.96 0.90 1.00 1.00 0.97 1.00 0.94 0.97 \n0.44 0.40 0.97 0.97 0.05 0.02 0.86 0.88 146.wave5 1.07 0.98 1.11 1 .Ol 1.01 0.99 1.12 0.99 0.19 0.13 \n0.67 0.53 0.92 0.86 1.00 1.00 CFP95 Avg 1.10 1.01 1.09 1.03 1.00 1.00 1.05 0.97 0.61 0.44 0.96 1.12 0.60 \n0.93 0.94 0.96 SPEC95 Avg 1.19 1.10 1.14 1.06 1.00 1.00 1.04 0.98 0.93 0.76 0.86 0.90 0.60 0.94 1 0.94 \n0.96 Table 2: Perturbation of hardware metrics from profiling. F reports the ratio of each metric, recorded \nusing flow sensitive profiling (intraprocedural paths), to the corresponding metric in the uninstrumented \nprogram. C reports the ratio of a metric, recorded using context sensitive profiling (call graph paths), \nto the metric in the uninstrumented program. Avg Avg Out Hew .ht I Ml&#38;X I Call Site !s Benchmark \nSize Nodes Node Size Degree Avg MaX I Reolication I One Path 099x0 l.le7 26894 10.5 18.0 9308 3226 124&#38;8gksim \n1.2e6 2617 6.5 13.0 228 1868 126.gcc 2.le7 28863 10.2 25.0 1724 21867 129.compress 8.9e4 253 5.2 12.0 \n28 201 13O.h 1 .oe6 3227 8.9 18.0 356 2252 132.ijpeg 1.3e6 2311 9.2 18.0 147 1663 134.nerl 3.6e6 5517 \n7.3 16.0 539 4634 147.bortex 2.le8 257710 15.2 30.0 30511 231866 lOl.tomcatv 2.5e5 782 7.1 14.0 73 684 \n102.swim 3.9e5 950 6.4 12.0 89 3071 1533 767 103.su2cor 8.8e5 1867 5.9 14.0 118 5869 3127 1059 104.hydro2d \n1.8e6 3255 7.1 16.0 395 10488 4445 2343 107.mgrid 6.5e5 1272 6.2 12.0 269 3978 1702 773 1 lO.appiu 4.le5 \n1193 6.2 12.0 133 3768 2248 959 125.turb3d 1.4e6 4295 8.0 16.0 1305 9901 5409 2277 141 .apsi 8.5e6 12494 \n8.2 18.0 2161 53148 20572 6813 145.fPPPP 2.8e5 806 6.2 13.0 107 2439 1643 671 146.wave5 1.5e6 2580 7.8 \n15.0 632 7942 3753 1676 Table 3: Statistics for a CCT with intraprocedurai path information in the nodes. \nSize is size (in bytes) of the profile file. Nodes is the number of nodes in the CCT. Avg Node Size reports \nthe average size of an allocated call record (bytes). Avg Out Degree reports the average number of children \nof interior nodes. Height reports the average and maximum height of the tree. Max Replication reports \nthe maximum number of distinct call records for any routine in the CCT. Finally, Call Sites reports the \ntotal number of call sites in all call records. Used is call sites that were actually reached. One Path \nis call sites that were reached in a given call record by exactly one path from the procedure s entry. \nAll Paths Hot Paths Cold Paths Dense Paths I Snarse Paths Num Inst Miss Num Inst Miss Num- Inst Miss \nNum Inst Miss 26628 3.2elO l.le9 II 7 8.5% 13.1% 4 12.8% 8.2% 26617 78.7% 78.7% 1115 7.8elO 6.3e8 22 \n28.9% 69.6% 2 20.2Yo 3.1% 1091 50.9% 27.3% 126.gcc 11769 1.7e8 3.8e6 3 4.3% 8.0% 0 0% 0% 11766 95.7% \n92.0% 129.compress 249 Se10 1.3e9 11 34.0% 85.5% 4 19.6% 7.5% 234 46.4% 7.1% 13O.h 811 5.6elO l.le9 13 \n25.8% 58.7% 10 34.3% 21.7% 788 39.9% 19.6% 132.ijpeg 1284 3.6elO 1.6e8 11 37.3% 77.6% 1 3.2% 3.1% 1272 \n59.5% 19.2% 134merl 1427 2.5e10 6.7e8 20 27.3% 48.8% 8 14.4% 10.4% 1399 58.3% 40.8% 147.;ortex ] 2244 \n7.3elO 1.2e9 14 39.3% 59.3% 3 15.5% 10.3% 2227 45.3% 30.4% CINT95 Avg 12.6 25.7% 52.6% 4.0 15.0% 8.0% \n5674.2 59.3% 39.4% lOl.tomcatv 427 5.3elO 2.6e9 3 29.9% 62.7% 2 66.8% 34.9% 422 3.3% 2.4% 102.swim 377 \n2.7elO 2.5e9 1 24.9% 50.9% 2 72.9% 47.0% 374 2.3% 2.1% 103.su2cor 950 5.2elO 2.3e9 14 57.1% 83.3% 2 12.7% \n9.8% 934 30.2% 6.8% 104.hydro2d 1816 9.3elO 3.6e9 17 27.7% 53.9% 10 43.4% 28.4% 1789 28.9% 17.7% lO?.mgrid \n588 1.4ell 3e9 9 7.4% 17.8% 2 87.5% 73.5% 577 5.1% 8.7% 1 lO.applu 637 9.5elO 1.7e9 22 29.6% 74.1% 3 \n28.9% 8.2% 612 41.5% 17.7% 125.turb3d 672 1.7ell 3.5e9 12 14.1% 72.9% 6 24.9% 11.2% 654 60.9% 15.9% 14l.apsi \n1065 5elO 1.8e9 15 24.5% 68.5% 3 7.7% 4.8% 1047 67.8% 26.7% 145.fPPPP 1067 2.5ell 6.le9 5 51.3% 63.3% \n7 24.1% 16.2% 1055 24.6% 20.5% 146.wave5 934 6.3elO 3.le9 12 38.4% 62.9% 8 43.4% 24.5% 914 18.2% 12.6% \nCFP95 Avg 11.0 30.5% 61.0% 4.5 41.2% 25.9% 837.8 28.3% 13.1% SPEC95 Avg 11.7 28.4% 57.3% 4.3 29.6% 17.9% \n2987.3 42.1% 24.8% SPEC95 Avg - go, gee 12.6 31.1 63.1 4.6 32.5 19.7 961.8 36.4 17.2 Table 4: Ll data \ncache misses paths. Hot Paths are intraprocedural paths that incur at least 1% of a program s cache misses. \nCold paths are the other paths. Dense Paths are hot paths that have an above average miss rate. Sparse \npaths are hot paths with a below average miss rate. Num is the quantity of paths in the category. Inst \nis the instructions executed along these paths. Miss is the Ll data cache misses along the path. Hot \nProcedures Cold Procedures Dense Procedures Sparse Procedures Benchmark Num Path/Proc Misses Num Path/Proc \nMisses Num Path/Proc Misses 099.go 9 182.9 30.7% 8 405.9 39.3% 394 55.2 29.9% 124.mSSksim 12 18.0 77.2% \n3 14.3 16.9% 207 4.1 5.9% 126gcc 14 88.4 28.0% 10 155.1 16.1% 1055 8.5 55.9% 129.compress 2 22.5 91.6% \n4 6.5 7.9% 63 2.8 0.5% 13O.li 9 6.0 66.1% 6 9.5 27.0% 242 2.9 6.9% 132.ijpeg 9 42.2 86.9% 1 11.0 7.0% \n255 3.5 6.1% 134.perl 13 14.2 76.2% 6 16.7 14.1% 217 5.3 9.6% 147.vortex 17 11.7 66.4% 4 2.2 12.2% 611 \n3.3 21.5% CINT95 Avg 10.6 48.2 65.4% 5.2 77.7 17.6% 380.5 10.7 17.0% lOl.tomcatv 1 41.0 99.7% 0 0.0 0.0% \n142 2.7 0.3% 102.swim 1 13.0 51.9% 2 12.0 48.0% 137 2.5 0.1% 103.su2cor 8 32.4 90.1% 2 13.0 8.6% 196 \n3.4 1.4% 104.hydro2d 7 20.0 46.8% 5 171.4 49.0% 213 3.8 4.1% 107.mgrid 3 37.7 20.8% 2 7.0 78.9% 153 3.0 \n0.2% 1 lO.applu 3 25.7 71.0% 3 33.0 28.9% 134 3.4 0.1% 125.turb3d 11 7.5 80.7% 3 11.7 18.1% 171 3.2 1.2% \n14l.apsi 12 9.4 66.5% 5 12.8 26.1% 224 4.0 7.4% 145.fPPPP 2 8.0 51.0% 3 222.3 47.9% 135 2.8 1.1% 146.wave5 \n5 34.6 28.0% 3 32.3 68.5% 203 3.3 3.5% CFP95 Avg 5.3 22.9 60.6% 2.8 51.6 37.4% 170.8 3.2 1.9% SPEC95 \nAvg 7.7 34.2 62.8% 3.9 63.2 28.6% 264.0 6.5 8.6% -spEC95 Avg -go, gee 7.2 21.5 66.9% 3.2 36.0 28.7% 206.4 \n3.4 4.4% Table 5: Ll data cache misses per procedure. Hot Procedures incur at least 1% of a program s \ncache misses. Dense Procedures have above average miss ratios, while Sparse Procedures have below average \nmiss ratios. Cold Procedures incur fewer than 1% of a program s cache misses. Path/Proc is the average \nnumber of paths executed in procedures in each category. Misses is the fraction of misses incurred by \nprocedures in the category. counter wrap). These measurements also slightly perturbed the hardware counters, \nas the data collection process sus-pends the application process and briefly runs. Since the hardware \ncounters were set to only record user processes events, the kernel was not directly measured. However, \nker- nel code could indirectly affect metrics, for example by pol- luting the cache. The table reports \nthe ratio of each metric, collected with both flow and context sensitive profiling, to the uninstru- \nmentcd program. In some cases, instrumentation can im-prove performance, for example by spreading apart \nstores and reducing store buffer stalls. The average perturbation for ail metrics was small, though some \nprograms exhibited large errors. We have not yet isolated the cause of these results. However, it is \nencouraging that the two techniques, flow and context sensitive profiling, typically obtained sim-ilar \nresults. 6.3 CCT Statistics Table 3 contains information on the size of a CCT. This data structure increases \nin size by a factor of 2-3x when the CCT is built on a call site basis. The total size of the data structure \nis a few hundred thousand bytes for most of the benchmarks. However, the CCT can be quite large for programs, \nsuch as 147.vortex, that contain many call paths. The table also shows that a CCT is a bushy, rather \nthan tall, tree. In addition, the figures show that the routine with the most call records (Max Replication) \noften accounts for a large fraction of the tree nodes. The final three columns report the total number \nof call sites in the allocated call records, how many of these sites are used, and how many sites only \nare reached by one intrapro- cedural path. The latter figure is particularly interesting, because in \nthis case the combination of flow and context sensitive profiling produces as precise a result as complete \ninterprocedural path profiling. 6.4 Flow Sensitive Profiling This sections contains some sample measurements \nof hard- ware performance metrics along intraprocedural paths. Processor stalls have many manifestations-cache \nmisses, branch mispredicts, load latency, or resource contention-but, in general, they occur when operations \nwith long la- tencies cannot be overlapped or when excessive contention arises for resources. A processor \ns dynamic scheduling logic delays operations to preserve a program s semantics or re- solve resource \ncontention. Compiler techniques, such as trace scheduling, instruction scheduling, or loop transforma- \ntions, reorder instructions to reduce stalls. Most compilers operate blindly, and apply these optimizations \nthroughout a program, without an empirical basis for making tradeoffs. Our measurements show that this \nconventional approach is inefficient at best, as stalls are heavily concentrated on a very small number \nof paths, which we call hot paths. 6.4.1 Cache Misses, By Path As an example, Table 4 reports hot paths \nin the SPEC95 benchmarks, for the UltraSPARC s Ll data cache, which is an on-chip 16 Kb, direct mapped \ncache. Hot paths are in- traprocedural paths that incur at least 1% of the total cache misses-this threshold \nis a parameter to control the number of paths. Paths that are not hot are cold paths. A dense path is \na hot path with a miss ratio above the program s average miss ratio. A sparse path is a hot path with \na be- 7.1 Call-related Performance Measure- low average miss ratio. Sparse paths incur a large number \nof misses because they execute heavily, rather than because of poor data locality. Dense paths are more \ncommon than sparse paths, which is fortunate because it seems likely that dense paths are more likely \nto be optimized by a compiler. In the SPEC95 benchmarks, excepting 099.go and 126.gcc, l-22 (avg. 12) \ndense paths account for 51-86% (avg. 63%) of the Ll cache misses. Considering all hot paths (dense + \nsparse) increases the number of paths to 3-28, but also increase their coverage to 5998% (avg. 83%) of \nthe Ll misses. The programs 099.go and 126.gcc have a well-known rep utation for differing from the rest \nof the SPEC95 bench-marks. Our numbers corroborate this observation. They execute roughly an order of \nmagnitude more paths than the other programs and each path makes a less significant contri- bution to \nthe program s miss rate. It is therefore necessary to reduce the threshold for hot paths to 0.1%. This \nchange finds 139 hot paths in 099.go that account for 55% of its misses and 172 hot paths in 126.gcc \nthat account for 27% of its misses. In both cases, the number of hot paths is still around 1% of executed \npaths and a miniscule fraction of potential paths. 6.4.2 Cache Misses, By Procedure Another way to apportion \ncache misses is by procedure. Ta-ble 5 reports hot procedures in the SPEC95 benchmarks, for the Ll data \ncache. A hot procedure incurs at least 1% of the cache misses. A dense procedure is a hot procedure that \nhas an above-average miss ratio. A sparse procedure is a hot procedure with a below-average miss ratio. \nAgain, cache misses are heavily concentrated in a small portion of the program. From 1-24 (avg. 11.7) \nproce-dures account for 4499% (avg. 91%) of the cache misses. This time, 099.go, 126.gcc, and 147.vortex \nhave significantly lower coverage than the other programs. Again, lowering the threshold to 0.1% improves \ncoverage, so that 82 procedures cover 97% of the misses in 099.go, 157 procedures cover 89% of the misses \nin 126.gcc, and 75 procedures cover 96% of the misses in 147.vortex. 6.4.3 Implications for Profiling \nTable 5 also demonstrates that reporting cache misses by procedure may not help isolate the aspects of \na program s behavior that caused cache misses. Hot procedures execute many paths (an average of 34 and \n63, for dense and sparse procedures, respectively), so that knowing a procedure in-curs many cache misses \nmay not help isolate the path along which they occur. Moreover, collecting and reporting cache misses \nmeasurements at the statement level, in addition to being far more expensive than path profiling, does \nnot alle- viate this problem. In these benchmarks, the basic blocks along hot paths execute along an \naverage of 16 different paths (of all type). Path profiling offers a low-cost way to provide insight \ninto a program s dynamic behavior. 7 Related Work This section describes previous work related to flow \nand con- text sensitive profiling. ment Many profiling tools approximate context sensitive profiling \ninformation with heuristics. Tools such as gprof [GKM83] and qpt [BL94] use counts of the number of times \nthat a caller invokes a callee to approximate the time in a procedure attributable to different callers. \nBecause these profihng tools do not label procedure timings by context, information is lost that cannot \nbe accurately recovered. Furthermore, the tools also make naive assumptions when propagating timing information \nin the presence of recursion, which results in further inaccuracy. Abnormalities that result from approximating \ncontext sensitive information are well known [PF88]. Ponder and Fateman propose several instrumentation \nschemes to solve these problems. Their preferred solution associates proce-dure timing with a (caIler, \ncallee) pair rather than with a single procedure. This results in one level of context sen-sitive profiling. \nOur work generalizes this to complete con-texts. Pure Atria s commercial profiling system Quantify uses \na representation similar to a CCT to record instruction counts in procedures and time spent in system \ncalIs @3en96]. Details and overheads of this system are unpublished. This work goes further by incorporating \npath profiling and hardware performance metrics. 7.2 Context Sensitive Measurement Mech-anisms Call path \nprofiling is another approach to context sensi-tive profiling [Hal92, HG93]. It, however, differs substan- \ntiaIly in its implementation and overhead. Hall s scheme re+instruments and re-executes programs to collect \ncall path profiling of LISP programs in a top-down manner [Hal92]. Initially, the call-sites in the main \nfunction are instru-mented to record the time spent in each callee. Once mea-surements have been made, \nthe system re-instruments the program at the next deepest level of the call graph and re- executes it \nto examine particular behavior of interest, and so on. Since the amount of instrumentation is small, \nover-head introduced in a run can be quite low. However, iter- ative re-instrumentation and r-e-execution \ncan be impracti- cally expensive and does not work for programs with non-reproducible behavior. By contrast, \nour technique requires only one instrumentation and execution phase to record complete information for \nall calling contexts. Goldberg and Hall used process sampling to record con- text sensitive metrics for \nUnix processes [HG93]. By in-terrupting a process and tracing the call stack, they con-structed a context \nfor the performance metric. Beyond the inaccuracy introduced by sampling, their approach has two disadvantages. \nEvery sample requires walking the call stack to establish the context. Also, the size of their data StNC-ture \nis unbounded, since each sample is recorded along with its call stack. 7.3 Calling Context Trees CCTs \nare related to Sharir and Pnueli s call strings, which are sequences of calls used to label values for \ninterprocedural flow analysis [SPSl]. In interprocedural analysis, the need to bound the size of the \nrepresentation of recurs we programs and to define a distributive meet operator, made call strings impractical. \nCCTs, because they only need to capture a single execution behavior, rather than all possible behaviors, \nare a practical data structure. Jerding, Stasko and Ball describe another approach for compacting dynamic \ncall trees that proceeds in a bottom- up fashion [JSB97]. Using hash consing, they create a dag structure \nin which identical subtrees from the call tree are represented exactly once in the dag. In this approach, \ntwo activations with identical contexts may be represented by different nodes in the dag, since node \nequivalence is defined by the subtree rooted at a node rather than the path to a node.   Summary Flow \nand context sensitivity are used in data-flow analysis to increase the precision of static program analysis. \nThis paper applied these techniques to a dynamic program analysis-program profiling-where they improve \nthe precision of re- porting hardware performance metrics. Previous tools asso- ciated metrics with programs \nsyntactic components, which missed spatial or temporal interactions between statements and procedures. \nPaths through a procedure or call graph capture much of a program s temporal behavior and pro- vide the \ncontext to interpret hardware metrics. This paper showed how to extend our efficient path profiling algorithm \nto track hardware metrics along every path through a procedure. In addition, it described a sim- ple \ndata structure that can associate these metrics with paths through a program s call graph. Measurements \nof the SPEC95 benchmarks showed that the run-time overhead of flow and context sensitive profiling is \nreasonable and that these techniques effectively identify a small number of hot paths that are the profitable \ncandidates for optimization. The program paths identified by profiling have close ties to compilation. \nMany compiler optimizations attempt to re- duce the number of instructions executed on paths through \na procedure [MR81, MW95, BGS97]. In order to ensure profitability, these optimizations must not increase \nthe total instructions executed along all paths through a procedure or, more strongly, over any path \nthrough a procedure. In addition, these optimizations duplicate paths to customize them, which increases \ncode size, to the detriment of high is- sue rate processors. Much of the optimization s complexity, and \nmany of their heuristics, stems from the assumption that all paths are equally likely. Compilers can \nuse path profiles to identify portions of a program that would bene- fit from optimization, and as an \nempirical basis for making optimization tradeoffs. Acknowledgements Many thanks to Ashok Singhal of \nSun Microsystems for as- sistance in understanding and using the UltraSPARC hard-ware counters. Trishul \nChilimbi, Mark Hill, and Chris Lb provided helpful comments on this paper. References [Ba194] Thomas \nBall. EfRciently counting program events with support for on-line queries. ACM !IYansactions on Programming \nLanguages and Systems, 16(5):1399-1410, September 1994. [Ben961 [BGS97] [BL94] [BL961 [CMHSl] [GKM83] \n[Ha1921 [HG93] [JSB97] [Km711 (Lsw [LW94] [Mm11 [MRW92] [MW95] W-1 [RBDL97] [S&#38;3] [SP81] [Sun961 \n[WHHSO] Jim Bennett (PureAtria, Inc.). Personal communication, November 1996. Ft. Bodik, R. Gupta, and \nM. L. Soffa. Interprocedural con-ditional branch elimination. In Proceedings of the SIG-PLAN 97 Conference \non Programming Language De-sign and Implementation, June 1997. T. Ball and J. Fl. Larus. Optimally profiling \nand trac-ing programs. ACM pansactions on Programmrng Lan-guages and Systems, 16(3):1319-1360, July 1994. \nT. Ball and J. R. Larus. Efficient path profiling. In Pro-ceedings of MICRO 96, pages 46-57, December \n1996. P. P. Chang, S. A. Mahlke, and W-M. W. Hwu. Us-ing profile information to assist classic code optimiza-tions. \nSoftware-Pmctice and Ezperience, 21(12):1301-1321, December 1991. S. L. Graham, P. B. Kessler, and M. \nK. McKusick. An ex-ecution profiler for modular programs. Software-Practice and Ezperience, 13:671-685, \n1983. R. J. Hall. Call path profiling. In Proceedings of the 14th International Conference on Software \nEngineering (ICSE92), pages 296-306,1992. R. J. Hall and A. J. Goldberg. Call path profiling of monc- \ntonic program resources in UNIX. In Proceedings of the USENIX Summer 1993 Technrcal Conference, pages \nl-14., Cincinnati, OH, 1993. D. F. Jerding, J. T. Stasko, and T. Ball. Visualizing in-teractions in program \nexecutions. In Proceedings of 1997 International Conference on Softwan Engineering (to appear), May 1997. \nD. E. Knuth. An empirical study of FORTRAN pm grams. Software-Practice and Ezperience, 1(2):105-133, \nJune 1971. James R. Larus and Eric Schnarr. EEL: Machine-independent executable editing. In Proceedings \nof the SIGPLAN 95 Conference on Programming Language Design and Implementation (PLDI), pages 291-300, \nJune 1995. Alvin R. Lebeck and David A. Wood. Cache profiling and the spec benchmarks: A case study. \nIEEE Computer, 27(10):15-26, October 1994. E. Morel and C. Renvoise. Interprocedural elimination of partial \nredundancies. In S.S. Muchnick and N.D. Jones, editors, Program Flow Analysis: Theory and Applica-tions. \nPrentice-Hall, Englewood Cliffs, NJ, 1981. Allen D. Malony, Daniel A. Reed, and Harry A. G. Wi-jshoff. \nPerformance measurement instrusion and pertur-bation analysis. IEEE ZYansactions on Parallel and Dis- \n tributed Systems, 3(4):433-450, July 1992. F. Mueller and D. B. Whalley. Avoiding conditional branches \nby code replication. In Proceedings o,f the SIG-PLAN 95 Conference on Programming Language De-sign and \nImplementation, pages 56-66, June 1995. C. Ponder and R. J. ateman. Inaccuracies in program profilers. \nSoftware-Practice and Ezpenence, 18:459467, May 1988. T. Reps, T. Ball, M. Das, and J. R. Larus. The \nuse of program profiling for software maintenance with applick tions to the year 2000 problem. In Technical \nReport 1335, Computer Sciences Department, University of Wiscon-sin, Madison, WI, January 1997. Pure \nSoftware. Quantify User s Guide. 1993. Micha Sharir and Amir Pnueli. Two approaches to inter-procedural \ndata flow analysis. In Steven S. Muchnick and Neil D. Jones, editors, Program Flow Analysis: Theory and \nApphcatrons, pages 189-233. Prentice-Hall, 1981. Sun Microelectronics. UltraSPARC User s Manual, 1996. \nM. R. Woodward, D. Hedley, and M. A. Hennell. Ex-perience with path analysis and testing of programs. \nIEEE lRonaactions on Software Engineering, 6(3):278-286, May 1980. 96  \n\t\t\t", "proc_id": "258915", "abstract": "A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.", "authors": [{"name": "Glenn Ammons", "author_profile_id": "81546156556", "affiliation": "Dept. of Computer Sciences, University of Wisconsin-Madison", "person_id": "PP39040589", "email_address": "", "orcid_id": ""}, {"name": "Thomas Ball", "author_profile_id": "81100472343", "affiliation": "Bell Laboratories Lucent Technologies", "person_id": "PP39044398", "email_address": "", "orcid_id": ""}, {"name": "James R. Larus", "author_profile_id": "81100277326", "affiliation": "Dept. of Computer Sciences, University of Wisconsin-Madison", "person_id": "P132790", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258924", "year": "1997", "article_id": "258924", "conference": "PLDI", "title": "Exploiting hardware performance counters with flow and context sensitive profiling", "url": "http://dl.acm.org/citation.cfm?id=258924"}