{"article_publication_date": "05-01-1997", "fulltext": "\n Linear-time Subtransitive Control Flow Analysis Nevin Heintze* David McAllestert Abstract We present \na linear-time algorithm for bounded- type programs that builds a directed graph whose transitive closure \ngives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm \ncan be used to list all functions calls from all call sites in (op timal) quadratic time. More importantly, \nit can be used to give linear-time algorithms for CFA- consuming applications such as: effects analysis: \nfind the side-effecting ex-pressions in a program. k-limited CFA: for each call-site, list the functions \nif there are only a few of them (5 k) and otherwise output many . called-once analysis: identify all \nfunctions called from only one call-site.  Introduction The control-flow graph of a program plays a \ncen- tral role in compilation -it identifies the block and loop structure in a program, a prerequisite \nfor many code optimisations. For first-order lan-guages, this graph can be directly constructed from \na program because information about flow of control from one point to another is explicit in the program \n(we remark that in the context of tail-call optimization, some extra work may be needed; see [3]). For \nlanguages with higher-order functions, the situation is very different: the flow of control from one \npoint to another is not readily apparent from Bell Labs, 600 Mountain Ave, Murray Hill, NJ 07974, nchQbell-labs.com. \ntAT&#38;T Labs, 600 Mountain Ave, Murray Hill, NJ 07974, dmacOresearch.att.com. program text because \na function can be passed around as data and subsequently called from any- where iu the program. This \nlimits compiler op timisation. One way to address this problem is to perform control-flow onalysia (CFA) \nto deter-mine (an approximation) of the functions that may be called from each call site in a program \n[g, 9, 17, 161. (In fact, some form of CFA is used in most forms of analyses for higher-order lan- kw%=) \nMany different control-flow analyses have been developed (often independently), with vari- ations in: \n1. the treatment of contezt: does the analysis take into account the calling context of a function (polyvariant \ntreatment) or does it fold all activations of a function together (monovariant treatment)? 2. the treatment \nof dead-code: does the analy- sis take into account which pieces of a pro- gram can actually be called? \nDoes it take into account reduction order? 3. the treatment of data-constructors: what happens when \na function is stored in a list and then later extracted from the list -is the identity of a function \ntraced through re-cursive data-structures?  Despite these variations, a fundamental notion of CFA has \nemerged -the monovariant form of CFA defined over the pure lambda calculus. We call this analysis standard \nCFA (see Section 2 for a definition). Most other forms of CFA can be viewed as modification or extensions \nof standard CFA. Moreover, a number of connections have been established between standard CFA and a va- \nriety of type inference problems [15, 51. A number of different terminologies have developed for this \nconcept: a polyvariant analyses is often called context-sensitive , and sometimes polymorphic . A monovariant \nanalysis is often called context-insensitive , and sometimes monomorphic . Permission to make digital/hard \ncopy of part or all this work for appear, and notice is given that copying is by permission of ACM, personal \nor classroom use is granted without fee provided that Inc. To copy otherwise, to republish, to post on \nservers, or to copies are not made or distributed for profit or commercial advan-redistribute to lists, \nrequires prior specific permission and/or a fee. tage, the copyright notice. the title of the publication \nand its date PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/97/0006...$3.50 261 The usual algorithm \nfor standard CFA has O(n3) time complexity and O(n ) space complex- ity, where n measures the length \nof the input program. The conventional wisdom is that this algorithm cannot be improved because the algo-rithm \nis essentially performing a dynamic tran-sitive closure and the best known algorithm for dynamic transitive \nclosure is O(n3). Dynamic transitive closure differs from the usual transitive closure problem because \nnew basic edges may be added during the algorithm s execution; as a re-sult the usual techniques for \nobtaining sub-cubic algorithms for transitive closure cannot be ap plied in the dynamic case. The apparent \ncubic complexity of standard CFA has been a barrier to the use of CFA in com- pilers. In fact, although \na number of prototypes analyzers have been built, standard CFA has yet to find its way into a widely \nused compiler. Many implementors have instead used simpler but less accurate algorithms. For example, \nBondorf and Jorgensen [2] employ an equality-based algorithm for CFA because the equality-based flow \nanaly- sis can be done in almost-linear time whereas an inclusion-based analysis is expected to be at \nleast cubic. Another common choice is to use very simple approximations of the control-flow graph based \non known function applications. We re-mark that, in practice, the standard CFA algo-rithm (and its derivatives) \nrarely exhibit cubic behavior, but they are often non-linear. Recent work [6] indicates that it is unlikely \nwe can improve on the cubic-time complexity of stan- dard control-flow analysis, because it is as hard \nas the 2-NPDA acceptability problem. (Melski and Reps [12] have recently shown a similar kind of cubic-hardness \nresult for first-order set-based analysis with data-constructors.) In this paper we focus on bounded-type \nprograms. For monotyped (or simply typed) programs?, we simply bound the tree-size of a program s types \nby some con-stant. Equivalently, we could bound a program s order and a&#38;y, where arity is defined \nso that cur-rying increases argument count rather than order; for example, the usual curried version \nof integer map with type (Int -+ 1nt) + Int list + Int list has arity 2 and order 2. Bounded-type programs \ncapture the intuition that functions rarely have more than, say, 20 ar-guments or have order greater \nthan 3, and al-most never at the same time (while this holds for most hand-written programs, the case \nis rather less clear for automatically generated programs). This class of programs has been particularly \nuse-ful for understanding the observed linear behavior We discuss the notion of bounded-type for polymor- \nphically typed programs (in the sense of ML) in Section 5. of type inference for ML [13]; see [7, lo]. \nHow-ever, it cannot be used to control the complexity of the standard CFA algorithm, which is cubic even \nwhen type size is bounded. Section 10 illus- trates this with an example. The main result of this paper \nis a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure \ngives exactly the results of the standard (cubic-time) CFA algorithm. Our algorithm can be used to list \nall functions calIs from all call sites in (optimal) quadratic time. More importantly, for many ap plications \nthat consume (standard) control-flow information, we can adapt our algorithm to per-form the necessary \ncontrol-flow analysis and post- processing of the control-flow information all in linear-time. We illustrate \nthis by giving linear-time algorithms for: . effects analysis: find the side-effecting ex-pressions in \n. program. . k-limited CFA: for each call-site, list the functions if there are only a few of them (5 \nk) and otherwise output many . . called-once analysis: identify all functions called from only one call-site. \nOur algorithm is simple, incremental, demand-driven and easily adapted to polyvariant usage. Early experimental \nevidence suggests that this new algorithm is significantly faster than the standard algorithm. Section \n2 gives the definition of the standard control-flow problem. In Section 3, we present the linear-time \nalgorithm. The main insight of our algorithm is a decoupling of the transitive closure and edge addition \naspects of the stan-dard CFA algorithm (the standard CFA algo-rithm can be viewed as transitive closure \ninter-twined with edge addition due to newly discov-ered function applications). Section 4 uses types \nto show termination. Sections 5 shows termiua- tion for polymorphically typed programs. Sec-tion 6 extends \nthe basic algorithm to records and recursive datatypes (the treatment of recursive datatypes is somewhat \nless accurate than in other approaches such as set-based analysis [4]). Sec-tion 7 considers polyvariance. \nSections 8 and 9 show how our algorithm can be used to provide linear-time algorithms for some CFA-consuming \napplications. In Section 10, we provide some empirical evidence that our new algorithm has practical \nsignificance. Preliminary results sug-gest that the new algorithm is significantly faster than the standard \nalgorithm for ML programs. We also provide a comparison on some programs that exhibit worst-case cubic \nbehavior. Perhaps most important, the overhead of the new algo-rithm (i.e. the size of the constant ) \nappears to be small. In general terms, what we establish in this pa- per is a connection between control-flow \nanalysis and graph reachability. Recent work by Melski and Reps [12] has show a similar kind of connec- \ntion between (a limited class of) set constraints and context-free reachability. While the differ-ences \nare significant (our work deals with reacha- bility in a standard graph, whereas Mel&#38;i/Reps deals \nwith S-path reachability in a labeledgraph), the use of graphical constructs in analysis is be- coming \nincreasingly common. We also note that very recent (and independent) work by Mossin [ll] investigates \nideas similar to those in this paper (in particular, the use of types to bound control-flow graph construction). \n Control-Flow Analysis We define standard control-flow analysis on a variant of the X-calculus with \nlabeled abstrac-tions. Expressions e are defined by : e ::= 2 1 X x.e ) (el e2) where x is a variable \nand 1 is a label. Such la- bels allow us to trace a program s control flow. A progmm is a closed term \nin which each ab-straction has a unique label. Program evalu-ation is P-reduction, appropriately adapted \nto labeled expressions. Note that reduction pre-serves labels on abstractions: e.g. a single step of \n/3-reduction rewrites (X1x.(x x)) (X y.y)) into (X y.y) (1 y.y). There are no restrictions on the reduction \norder: the P-reduction can be applied at any subterm at any time. The purpose of control-flow analysis \nis to asso- ciate a set of labels L(e) with each sub-expression e of the program5 such that if e reduces \nto au abstraction labeled 1 during execution of the pro- gram, then 1 E L(e). In other words, control-flow \nanalysis gives a conservative approximation of the abstractions that can be encountered at each ex-pression \nduring execution of a program. We can now define standard control-flow anal- When discussing control-fkw \nof ML typable prc-grams, we shall assume the addition of a fix construct and a let construct. However, \nwe describe the core of our algorithm using just thin simple version of the X-calculus. Strictly speaking, \nthe association is with each occw-rence of a sub-expression: different occurrences can have different \nlabel sets. ysis as the least such association of label sets that satisfies the following two conditions: \n. for any abstraction X x.e, we have 1 E L(X x.e), and . for any application (el ea), if 1 E L(er) and \n1 labels the abstraction Xrx.e, then - L(x) > L(e2) for each occurrence of x bound by the abstraction \n1, and - L((el e2)) 2 L(e). The standard algorithm for computing this anal- ysis is essentially a least \nfixed-point computation: each label set is initially the empty set, and then the algorithm repeatedly \npicks a sub-expression at which one of the conditions is not satisfied, and minimally adds new labels \nto label sets to locally satisfy the condition. This process is guaranteed to terminate because there \nare only a finite num- ber of labels in a program, and the label sets are monotonically increasing. The \ncomplexity of this algorithm is O(n3) where n is the size (number of syntax nodes) of the input program: \ninformally, at most n labels may be added to the n label sets, and each of these n2 possible additions \nmay involve up to O(n) work. To give some intuition about how this kind of behavior can arise in practice, \nconsider the fol-lowing program fragment: fun f x -. . . . . . . (f xl) . . . . . . . . . . (f x2) *... \n Using the above algorithm, the label set co&#38;acted for x is the union of the label sets collected \nfor xl and x2. Since the number of calls to function f can linearly increase with program size, the in- \nformation collected for x can grow linearly -iu effect, x acts like a join point, combining infor- mation \nfrom diverse parts of a program. If x is applied in the body of f, then we must perform work proportional \nto the information colIected for x at this application site. Worse, if x is returned then ail of the \ninformation joined by x can flow back to the call sites of the function f. This join- point behavior \nis independent of type size and is observed in practice, particularly for extensions of standard CFA \nthat deal with data construc-tors (for which library functions such as append and map become common join-points). \nAlthough it rarely leads to cubic behavior, it can have a significant impact on analysis running time \nand space. Note that the standard CFA algorithm com-putes the label sets for each program expression \nall at once. Since each label set contains up to a labels, this represents O(n ) information, and takes \nO(n2) time just to output. So a linear time algorithm for compute all label sets! is out of the question. \nHowever, in compiler applications we rarely want to know the control-flow informa- tion for every node \nin the program. Instead we usually only want to know the functions that can be called from a (relatively \nfew) specific call sites. Alternatively, we may want to know whether only one function can be called \nfrom a particular site (e.g. for inhning or specialization applications). More generally, we may not \nbe interested in spe- cific control-flow information, but rather we may need to know about control-flow \nto answer other questions such as is this expression side-effect free? . We address the complexity of \nthese such questions in Sections 8 and 9; for now, we con- sider four basic control-flow questions: . \nGiven a label 1 and an expression occurrence e, is 1 E L(e)? . Given an expression occurrence e, compute \nL(e). . Given a label 1, compute all expression oc-currences e such that 1 E L(e). . Compute all label \nsets. The following table compares the complexity of our algorithm with the standard algorithm on these \nquestions, for bounded-type programs. Problem Std AIg. New Alg. Is 1 E L(e)? O(n3) On L(e) O(n3) O(n) \n{e : 1 E L(e)} 0 n3 O(n) Ail Label Sets 0 n3 O(d)  3 The Algorithm We begin by reformulating the definition \nof stan- dard control-flow information as a transition sys-tem between program nodes. Thii reformulation \nmakes explicit the connection with transitive clo-sure. For convenience we assume that programs are renamed \nto ensure that bound variables are distinct. Now, for a program P, construct the following deduction \nrules: (=s)X 2.e -L X x.e elxbwAI: e (if (el e2) in P) (APP-1) Fil-L;; 2 , (if (el e2) in P) (APP-2) \nel -e2 e2 -e3 el -e3 where the condition on the second and third rules if (er es) in P indicates that \nthere is instance of the respective rule for each occurrence of a term of the form (er es) in P. Intuitively, \nan edge el -* es indicates that anything (e.g. an ab-straction) we can derive from ea is also derivable \nfrom el. In the case where es is itself au abstrac- tion, this edge says that ea is one possible value \nfor er. The first rule is a boot-strapping rule that says that any abstraction leads to itself. The sec-ond \nand third rules deal with application and re- spectively say that if there is a transition from the operator \nof an application to an abstraction, then add a transition from the bound variable of the abstraction \nto the operand of the application (rule APP-1) and also add a transition from the entire application \nto the body of the abstraction (rule APP-2). The final rule is transitivity (we note that it is sufficient \nto restrict this rule to the case where es is an abstraction). Standard control-flow analysis can now \nbe redefined as fol- lows: given a program expression e, find all ab- stractions X x.e such that e -+ \nX z.e is derivable from the above rules. For example, consider (X z.(x x) (A x .x )). The above rules \nlead to: X x.(x 2) -X x.(x x) (ABS) (1) xt x .x -xt xt.xJ (W (2) x -X1 x .x (*p-l) (3) (X x.(z x) Xl \nx .x ) -, (x x) (-4=2) (4) 2 -XI xI 2 (*p-1) (5) (x x) -2 - W-2) (6) (X x.(z x) Y x .x ) -XI x~.x (TRANS:~,~,~) \nThe last rule follows from two applications of transitivity on (4), (6) and (5) (in that order). In effect, \nthe four deduction rules ABS, APP-1, APP-2 and TRANS define a dynamic transitive closure problem: ABS \nsets up some initial edges, TPANS is transitive closure, and APP-1 and APP-2 add new basic edges as the \ntransitive closure proceeds. We refer to this system as DTC because of its close connection to dynamic \ntransitive closure. Clearly we can use DTC as the basis of an iterative fixed-point algorithm for control-flow \nanalysis: we start with the empty set of transitions and add a tran-sition e + e if it follows from one \nof the above rules in the context of the set of transitions ob-tained thus far. This algorithm performs \nessen-tirdly the same steps as the algorithm based on the revious control-flow definition and is also \nO(n T) (in fact, the close correspondence between the two algorithms can be used to provide an easy proof \nof equivalence of the two definitions). Observe that in this algorithm, the transitive closure computation \nin intertwined with the ad-dition of new edges. However, this does not need to be the case; this is a \nkey insight of our algc+ rithm. To show this, we define a new transition system. First, define a set \nof nodes n by the fol- lowing grammar:  n ::= e ( dam(n) 1 mn(n) Intuitively, dam(n) represents the \ndomain of the node n, and mn(n) represents the range of the node n. If n corresponds to an abstraction, \nthen dam(n) is simply the argument (bound vari- able) of the abstraction; otherwise, dam(n) de notes \nthe collection of arguments of the abstrac- tions represented by n. Similarly, if n corresponds to an \nabstraction, then mn(n) is simply the re-sult (body) of the abstraction; otherwise, dam(n) denotes the \ncollection of results of abstractions represented by n. Now, define a transition system between nodes \nn as follows: (ifJ4lz.e in P) (ABS-1)  z -dom(2z.e) (if X 3z.e in P) (ABS-2) mn(iVc.e) + e (if(q Ed) \nin P) (APP-1)   dom(el) d es (if (el e2) in P) (APP-2) (el e2) - mn(el) nl -n2 (CLOSE-DOM) dom(n2) \n-+ dom(nl)  n1 -n2 (CLOSE-RAN)   ran(m) -mn(n2) Call this system LC (for linear closure ); it forms \nthe core part of our linear-time CFA al-gorithm. To illustrate LC, consider the program (X z.(z z) (X \nz .z )) used earlier in thii section. Applying the first four rules leads to:  z *= dom(X z.(z 2)) (1) \nmn(A z.(z 2)) ** (z z) (2)  z *z dom(X z .r ) (3) mn(X Z .z ) *-2 (4) APP-1 1 , , dom(X z.(z 2)) -+ \nX x .z (5) (X z.(z d) (X Z .z )) PP-2 mn(X z.(z z)) (6) dam(z) *pp-! z  (7) (z 23) PP-2 mn(2) (8) \nwhere the subscripts on the arrows indicate which rule is employed. Applying the last two rules leads \nto the following transitions (among others). dom(X z .z ) b dom(dom(XLz.(z z))) (9) mn(dom(X z.(t z))) \n+ mn(X z .z ) (10) dom(dom(A z.(z z))) -* dam(z) (11) mn(z) --+ mn(dom(X z.(z 2))) (12) (9) and (11) \nare by CLOSE-DOM; (10) aud (12) are by CLOSE-RAN. Combining these transitions, we can derive X z .z from \n(X z.(z z) (X1 z .z )): (xqd z) (P d.2 )) -mn(X z.(z 2)) by (6) -(x s) -mn(z) p [:I -mn(dom(X z.(z 2))) \nby (12) -mn(X z .z ) by (10) -2 by (4)  -dom(X z .z ) by (3) -dom(dom(A z.(z 2))) ---+ dam(z) : ,cj \nby (4 7 Zdom(X z.(z 2)) by (1)  -xl ct.z by (5) Compare this to DTC in which the transition Xi z .z \n-, (X%.(2 z) (X% .d)) was added by the deduction rules. In other words, what was a single transition \nstep in DTC has become a multi- step transition in LC. This relationship holds in general: the transitive \nclosure of LC corresponds to DTC in the following sense: Proposition 1 For all progmms P, and ezpres- \neions e and X z.e appearing in P, e -X z.e in DTC ifl or some nodes nit e + f nl -. . . --) nk -x XX! \nin Lc. What we have achieved, then, is a factorisa-tion of the algorithm into two separate parts: an \nedge-adding phase (which adds basic edges) and a transitive closure phase. However, note that the CLOSE-DOM \nand CLOSE-RAN rules areopenended: given any edge n + n (added by one of the other rules), the CLOSE-RAN \nrule says that we can add all edges of the form mnk(n) + mnk(n ) for all k 2 1, and similarly for the \nCLOSE-DOM rule. To control the application of the CLOSE-DOM and CLOSE-RAN rules, we make them demand \ndriven, as follows:  n1 - n2 n + dom(n2) (CLOSE-DO&#38;) dom(n2) + dom(nl) nl - n2 n + mn(n1) (CLOSE-RAN') \nmn(nl) - mn(n2) Thii means that CLOSE-DOM' can only applied if there is a transition whose right-hand-side \ncould immediately match with the left-hand-side of the added transition i.e. if it is needed . Similarly \nfor CLOSE-RAN'. Call this new system LC (that is,LC consists of ABS-1, ABS-2, APP-1, APP-2, CLOSE-DOM' \nand CLOSE-RAN'). LC is equivalent to LC in the following sense:  Proposition 2 For all progmms P, and \nezpres-Gons e and X x.e appearing in P: . There ezist nodes ni such that e d n1 -+ . . . -nk -X x.e in \nLC if . there ezirt nodes n: such that e b n: -+ . . . -n:, -+ X x.e in LC. This modification of LC into \nLC improves the termination properties of the system (dis-cussed further in the next section). It also \nintro- duces an element of demand-driven/incremental behavior. For example, suppose that we have a function \nX Z.E. The rules introduce edges mn(X z.z) + E and z -+ dom(X z.z). At this stage, these are the only \nedges involving these nodes. Eventually, if and when the function is used, we may invoke the CLOSE-DOM' \nand CLOSE-RAN' rules. For example, if the entire program is ((X z.z X y.y) e), then the CLOSE-DOM' and \nCLOSE-RAN rules will add mn(mn(X z.z)) + m (z) and mn(z) + mn(dom(X z.z)), amongst others. In other words, \nwe only explore the parts of the type of an expression that are actually needed.  4 Termination The \nLC system can be viewed as an algorithm: given a program P, add all of the edges specified by the rules \n(this is best represented as a graph). However, this procedure does not terminate in general. We now \nshow that the algorithm: . terminates for typed programs (either sim-ply typed or polymorphically typed). \n. is fast (linear) for bounded-type programs (either simply typed or polymorphically typed)- In essence, \nwe shall use the types as a template for the nodes that need to be considered. To illus- trate this, \nsuppose e is a program expression with (non-polymorphic) type (r~ -+ m) + q + r4, then we need to consider \nsix new nodes, one for each proper subexpression of the type: dam(e), mn(e), dom(dom(e)), mn(dom(e)), \ndom(mn(e)) and mn(mn(e)). In general, the number of new nodes that must be added corresponds to the size \nof the type trees of program nodes. We show how this idea can be applied to programs with polymorphic \ntypes in the next section (Section 5); for the moment we shall consider programs with monotypes. Bounding \nthe number of nodes guarantees termination, because the inference rules ABS-1, ABS-2, APP-1, APP-2 generate \na fixed (program- dependent) number of rules, and CLOSE-DOM' and CLOSE-BAN' can add at most one new rule \nfor each rule/node pair. In the case of programs with k-bounded types, the size of these type trees is \nbounded by k, and hence we can obtsin a lin-ear bound on the number of rules that must be added. (Note \nthat, in general, the tree-&#38;e of a program can be exponential in program size -for programs that \nexhibit this behavior, our pro-posed algorithm would be a poor choice compared with the standard cubic-time \nalgorithm. For un-typed (or recursively typed programs), there is no bound, and our algorithm may not \nterminate.) To make the behavior of our algorithm on bounded-type program more precise, fix on some constant \nk, and define pk to be the class of mono- typed programs whose whose types are bounded by k. In system \nLC we have: Proposition 3 There ezistu a constant ck such that when LC is applied to a program P in pk, \nLC constructs at most ck . IPI edges where lPl denote8 the size of progmm P. Hence, we obtain the following \nlinear-time al-gorithms for bounded-type monotyped programs: Algorithm 1 Input: A program P, label 1 \nand a program sub-expression e. Output: Is I E L(e)? 1. Apply LC to P. 2. Use graph reachability to \ndetermine whether 2 is reachable from e.  Algorithm 2 Input: A program P and a program sub-expression \ne. Output: L(e) 1. Apply LC to P. 2. Use graph reachability to find all nodes reachable from e.  3. \nOutput the labels of abstractions in these reachable nodes.  We can also obtain an O(n2) algorithm \nfor com- puting all label sets (i.e. complete CFA informa- tion) by repeatedly applying Algorithm 2 to \nall program sub-expressions. The key part of our new algorithm is the use of the type tree at each program \nnode to limit, the number of edges that must be added during the analysis. Proposition 4 bounds the number \nof nodes using the maximum size type trees that can appear at any program node. This provides a rather \nloose bound. We could obtain tighter bounds by observing that the work done by the algorithm at each \nnode is proportional to the type tree at that node. Hence the total work done by the algorithm is proportional \nto the sum of the type tree sizes at each node. In other words, it is proportional to k,,, . IPI, where \nk,,, is the av-erage size of the type trees at each node in the program. One of the principal concerns \nof our im- plementation was the size of this constant: would it be prohibitive? Early results indicate \nthat this is not the case: the constant is quite small, typi- cally around 2 or 3. Note that the algorithm \nitself does not, ac- tually look at the types during its execution. Rather, the types are used only to \nestablish ter-mination (and the linear-time complexity bounds in the bounded-type case). In other words, \nour algorithm only needs to know that the (appropri- ate) types exist -it does not need to know what \nthey are. This simplifies implementation -we do not need to transmit the types from type in- ference \nto our algorithm. 5 Polymorphic Types Thus far we have considered monomorphic prcr grams and we have \nassumed a bound k on the size of the monotypes in a program. Now consider polymorphically typed programs \n(in the sense of ML) and suppose our expression language is ex- tended with an appropriate let construct. \nThere are at least two notions of bounded size poly-morphically typed programs. McAllester [lo] de-fines \nthat a polymorphically typed program P has bounded type size if there is some constant k such that the \ntree-size of the monotypes of each expres- sion in the let-expansion of P a.ll have size 5 k. Alternatively, \nmotivated by Henglein s ML pro- grams with small types [7], we can define that a polymorphically typed \nprogram P has bounded type size if there is some constant k such that the types (including polytypes) \nof expressions in P all have tree-size bounded by 5 k. Unfortunately, the two definitions are not equivalent \n. We shall use McAllester s defini-tion. Suppose that we have a polymorphically typed programs (according \nto McAllester s defi-nition). For monotyped program, we bound the running of the algorithm by using the \nmono-types of expressions to provide a template for the nodes that need to be considered during the edge- \nadding phase. The situation is similar for poly- morphically typed programs, except that we use the induced \ncollection of monotypes in the let-expansion of a program to provide a collection of templates for bounding \nthe behavior of our algo-rithm. Note, again, that our algorithm does not actually need to have the types \n(and in particu- lar, our algorithm does not need to construct the let-expansion of the program!); we \njust use their existence to prove termination. For example, in the program fun id x = x val y = ((id \nid) id) 1 the induced monotypes for id are int --* int, (int + int) + (int + int) and ((int + int) + \n(int + int)) + ((int -+ int) -4 (int -+ int)). In essence, we can set up a correspondence be-tween each \nnode n added during the edge-adding step and a position in some type tree for the based node of n (recall \nthat nodes in the edges- adding phase are built from basic nodes by ap-plying dom(-) and m (e)). To show \nthat our algorithm adds only a Consider the program consisting of n functions where the Arst function \nfo is just the identity function, and Ji.+l is defined to be AZ.(fi 1.) t. This program has bounded type \nusing Henglein s deflnition, but the monotypesin the let-expansion of the program have exponential tree \nsize. bounded number of edges for bounded type size polymorphically typed programs, it therefore suf- \nfices to show that the number of distinct posi-tions in the type trees of the monotypes in the let-expanded \nversion of the program. This is im- mediate, since the sizes of the monotypes in the let-expanded program \nare bounded by some con- stant k, and hence there is at most a total of 2k positions in these types. \nWe have thus established that our algorithm runs in linear-time on bounded-type (in the sense of McAllester) \npolymorphic programs. Note that although we have addressed programs with poly- morphic types, the algorithm \nitself is still mono- variant (context insensitive). Making the algo-rithm polyvariant is a separate \nissue, and is con- sidered in Section 7. CFA for ML Thus far, we have worked in the context of a simple \nversion of the lambda calculus (with just abstraction and application). We now extend the algorithm to \nrecursion, records and (recur-sive) datatypes. First consider fix: since we have worked with simply typed \nlambda terms with only abstraction and application, there is no re- cursion. To address this, consider \nadding a con-struct letrec f = Xrz.ei in ez. It is simple to ex-tend the linear-time algorithm for this \nconstruct: for each instance of this construct, we add tran- sitions: letrec f = X z.el in e2 + e2 f \n-X z.el  Next consider records. Suppose we add con-structs (el, . . . , e,) and projj, j = l..n, for \nrecord creation and accessing. We extend the algorithm by adding proj, as a node operator (i.e. it has \nthe same status as dom and ran in the last section, and has its own closure rule, similar to CLOSE- RJw* \nThen, for each expression (ei, . . . , e,), we add transitions proj,((el,.. . ,e,)) -ej, j = l..n, and \neach program expression projj(e) is treated as a node. If the program has bounded types, then only a \nbounded number of nodes need be considered, and so the extended algorithm is linear-time. (Note, however, \nthat for programs with records, bounded-order and bounded-arity no longer implies bounded type size.) \nNext consider recursive data types. One possi- bility is to ignore recursive data types: whenever a function \nis put in a recursive data structure and then extracted, we lose all information about the function (i.e. \nwe obtain the set of all abstraction labels). The rational for this is that functions are rarely put \nin recursive data structures, and so we can obtain most of the important informa-tion about control-flow \nin a program by ignoring recursive data types. However, many generalized forms of CFA track data-constructors \nin the same way as they track functions: the advantage here is that not only do they give better control-flow \ninforma-tion, but they also give information about the shape of first-order values. We consider a simi-lar \napproach. First, we extend the node opera-tors dom and ran with additional operators just as we did for \nrecords. Specifically, we add one operator ( de-constructor ) for each argument of each data-constructor \n: for an n-ary constructor c, we would add I+;, . . . , c$). Then we add ap- propriate (demand-driven) \nclosures rules for de- constructors. Finally, for each expression e of form c(ei, . . . , en), we add \ntransitions c,;(e) + e,, j = l..n. Unfortunately, as formulated, we have no way of bounding the size \nof the nodes we must con-sider. In fact, the monadic monotone closure problem can be mapped into this \nanalysis prob- lem, and Neal [14] has shown that monadic mono-tone closure is essentially as hard as \nthe 2NPDA acceptability problem, a well-studied problem for which the best known algorithm is O(n3) and \nhas not been improved since Aho, Hopcroft and Ull- mann s early work [l] in 1968. Melski and Reps [12] \nhave recently obtained a similar result in their work on set-based analysis and context-free reach-ability. \nTo reduce this complexity, we consider two al- ternatives, both of which reduce the accuracy of the analysis \nso that it is less accurate than, for example, mono-variant set-based analysis. The basic idea is to \nuse a finite node congruence that bounds the number of nodes that are considered (the algorithm considers \nonly one node from each congruence class) at the expense of reducing anal- ysis accuracy. First, note \nthat each node can be associated with a type. In particular, each node of the form c,;(n) can be associated \nwith the type of the ith argument of the constructor c. Let r(n) denote the type thus associated with \nA (we For simplicity,we view an ML datatype declaration an a definition of a collection of of multi-a&#38;y \ndata-constructors. *This is a generalization of transitive closure that in-cludes two (or more) monotone \nnode functions f and g such that if n is a node, then f(n) and g(n) are nodes, and if n + n then f(n) \n+ iin ) and-&#38;k) -g(n ). Given a set of ednee between nodes. and two nodes n and n appearing in this \nset, does n w n follow from the standard transitive closure rule and the additional f and g rules? shall \nonly be interested in the case where r(n) is a datatype). We now define two node congruences (by con- \n gruence, we mean that if ni E nz then dom(nl) z dom(nz), etc.). The first congruence, ~1, is defined \nto be the least congruence such that ni 11 ns whenever r(ni) = ~(7~2) and both are datatypes. The second \ncongruence differs from the first in that only nodes that have the same base node and involve a de-constructor \nare con-sidered equivalent. To this end, observe that any node can be written in the form o(n) where \nn is a basic node and a is a sequence of dom, rsn, de-constructors, etc. We say that n is the base node \nof a(n). Now, define ~2, to be the least congruence such that nr 12 ns whenever (a) r(nl) = r(nz) and \nboth are datatypes and (b) ni and ns both have the same base node and in- volve a de-constructor. This \nsecond congruence is finer than the first, and it leads to a strictly more accurate analysis. To illustrate \nthese construc-tions, suppose that we have the program frag-ment cons (2, cons (1, nil) 1. Let e denote \nthis expression, and let e denote the sub-expression cons (1, nil). Using ~2, we generate the follow- \ning edges (we include some of the rules that may be generated by the closure rules): e -+ cons(2, e \n) e -cons(1, nil) car(e) d 2 cdr(e) + e car(e ) --+ 1  cdr(e ) -nil car(cdr(e)) -car(e ) (CAR-CLOSE) \ncdr(e) -cdr(e ) (CDR-CLOSE) noting that in the last line, c&#38;(e) ~2 cdr(cdr(e)). Now, if we use 11 \ninstead of Es, then e, e , cdr(e), cdr(e ) would all be in the same equivalence class, and so, for example, \nthere would be edges to both 1 and 2 from car(e). For bounded type programs, 11 generates O(n) congruence \nclasses, and this leads to a linear-time analysis algorithm. In contrast, for bounded type programs, \n~2 generates up to O(n ) congruence classes, and hence leads to a quadratic-time analysis algorithm. \nHowever, if in addition to bounded types, we assume that nest-ing levels of datatypes are bounded, then \n~2 gives a linear-time algorithm. We define nesting levels of datatypes as follows: label a datatype \ndefini-tion that does not mention other datatypes with 0, and label any other datatype definition with \nthe maximum of the labels of all datatypes it uses, plus 1. We are currently investigating the tradeoffs \nbetween these two approaches. In particular, how much more accurate is the second approach? Note that \nthe first approach is akin to statically fixing the set of allowed functions that can ap pear in a particular \nslot in a data-constructor; the second approach allows more dynamic behavior. We also plan to investigate \nwhether the bounded nesting-level assumption for datatypes is realistic.  7 Polyvariance So far, we \nhave described a monovariant al-gorithm. We now describe polyvariant exten-sions to our algorithm that \nare analogous to let- polymorphism. Consider a program P. At a very naive level, consider just let-expanding \nP and an- alyzing the resulting (probably very large) pro-gram. Our intent is to develop an analysis \nwhose end result is equivalent to doing a monomorphic analysis of the let-expanded P, without doing the \nexplicit let-expansion. Instead, we analyze the function once, and build a summary of the anal-ysis of \nits code body. The resulting parameter-ized and simplified graph can then be instantiated (copied) at \nthe points of the function where it is mentioned, much like polymorphic type inference in ML. One of \nthe strengths of our algorithm is that the simplification/parameterization steps can be easily carried \nout -they correspond to graph reachability and simplification steps. To illus-trate the basic issues, \nlet e be the code fragment X z.((X y.z) nil). If we look at e in isolation, the LC rules introduce edges: \nran(X y.z) -2 y + dom(X y.z) dom(X y.z) -nil ((X g.z) nil) b ran(XL y.z) ran(e) -((i4 v.z) nil) z -dam(e) \n To simplify/parameterize this graph fragment, we first isolate the critical nodes, which are those nodes \nthat may be used by surrounding pro- gram text, in this case ran(Xr z.((X y.z) nil)) and dom(X z.((X \ny.z) nil)). Next, we do a graph reachability from these two nodes (and here we must generalize reachable \nso that if n is reach- able, then so is dam(n) and run(n)). Any non-reachable nodes (such as nil) can \nnow be removed. Finally, we can compress the graph and remove intermediate nodes (such as z). In this \ncase, we are left with just mn(e) + dam(e). For a let-bound function, we can first ana-lyze the function \nusing the above technique to obtain a simplified graph fragment representing the analysis of the function; \nthen, we make copies of this graph fragment for each place the function is used. In practice, we rarely \nwant to blindly du- plicate graph fragments for all functions in this way (in general, this duplication \ncould lead to an exponential control-flow analysis). Rather, we focus on functions where polyvariance \npays off. For example, we could look at the types of the program and determine which functions are poly- \nmorphic. Alternatively, we could first perform a simple monovariant analysis, and then use that information \nto control a subsequent polyvariant analysis (see e.g. [4]). Note that we could force our polyvariant \nal-gorithm to be linear-time by restricting polyvari-ante so that there is some global bound on the number \nof times each graph fragment is effectively duplicated (if one graph fragment is duplicated inside another \ngraph fragment, then any dupli- cation of the enclosing graph fragment must be counted as duplication \nof the enclosed fragment).  Linear-time Effects Analysis Suppose that we want to use CFA information \nto drive an effects analysis. We could, for example, run the standard CFA algorithm, build the list of \nfunctions that can be called from each call-site, and then iterate over this information to deter-mine \nwhich expressions have side-effects. For sim- plicity, assume that all side-effecting primitives are \nfully applied and that the language consists only of applications and abstractions. We start by marking \napplications of side-effecting primi- tives as side-effecting. Then, we mark an appli-cation (er ez) \nas side-effecting if either el or ez are marked side-effecting or if the CFA says that er could be a \nside-effecting function. Note that this analysis has complexity at least quadratic in the program size, \nbecause it uses a representa-tion of control-flow information that is quadratic in program size. Using \nour algorithm, we can obtain a proce-dure that computes exactly the same effects in- formation as the \nprocess just described, but runs in linear-time. The basic idea is that we color all applications that \ninvolve side-effecting operations with red, and then propagate coloring as follows: (a) a node (ei ez) \nis colored red if either el, ez or mn(er) are red; (b) a node mn(e) is colored red if there is an edge \nmn(e) + e and e is red. Rule (a) corresponds to the condition used in the previous paragraph; rule (b) \nis a (limited) form of transitive closure for coloring. This is clearly a linear-time procedure (it is \njust a graph reach-ability problem). 9 Linear-time k-limited CFA In many applications of CFA, we are \nonly in-terested in knowing information about call sites where a small number of functions can be called \n(e.g. one, two or three functions). If more func-tions than that may be called, then we might not be \ninterested in knowing the exact details of the functions, because the optimization we have in mind may \nbe intractable or inappropriate. Exam-ples of these kinds of applications include inlining and specialization. \nWe can use our algorithm to obtain a linear-time procedure for computing this information as follows. \nFirst, we annotate with each node with a value that is either a small set or the token Umanyn. We start \nby annotating nodes corre-sponding to functions with the singleton set con-taining just that function, \nand all other nodes with the empty set. Then, we propagate infor-mation back along edges: if a node n \nhas edges to nodes with sets Sr, . . . , Sj, then we update the annotation at n with Sr U*- *USj if this \nis a srnsll set (size 5 k), and many otherwise. Each up date can be done in constant time, each node \ncan be updated at most a constant number of times, and hence if we only propagate changes, we can obtain \na linear-time algorithm for computing Jr-limited CFA.  10 Experimental Results We present some preliminary \nresults from a proto- type implementation of the linear-time algorithm. This prototype implements the \nbasic linear-time CFA algorithm, with extensions for datatypes and records; however certain aspects of \nML have not yet been implemented, and as a result the benchmarks we have been able to run are limited. \nOur implementation is essentially a naive im- plementation of the algorithm described in this paper. \nA number of improvements could be made (such as taking advantage of the many nodes that have only one \noutgoing edge). We expect con-siderable improvement in the performance of the prototype as we better \nunderstand how the al-gorithm behaves in practice. It is also likely that we can exploit a number of \ngraph implementation techniques. We compare the performance of the linear-time algorithm with an implementation \nof set- based analysis (SBA) [4], run in monovariant mode (a generalization of the standard CFA al-gorithm). \nAll results are for a 150 MHz MIPS R4400 processor with 512 MBytes; all timings are user time in seconds \nand represent the fastest of 10 runs of the benchmark. Each benchmark con-sists of analyzing the example \nprogram and writ- ing out the control flow information for all non- trivial applications (i.e. applications \nof the form (ei ez) where ei is not a function identifier or an abstraction). The first set of results \nare for a parameterized benchmark that illustrates the cubic behavior of the standard CFA algorithm. \nThe benchmark of size 1 consists of: fun fs x = x fun bs x -x fun fl x -x fun bl x = x val xl = bl(fs \nfl) val yl = (bs bl) fl and the benchmark of size n consists of the first two lines of the above code \nand n copies of the last four lines, with f 1, bl, xl and yl appropriately renamed. The following table \npresents results for a variety of sizes of this benchmark. The cubic-time behavior of SBA is clear (the \ntable not only give runtimes, but also a measure of the units of work involved, since cache effects and \noptimiza- tions in the implementation of SBA itself mean that timings are somewhat misleading). The last \nthree columns describe the behavior of our new algorithm: the first two of them give the results for \nthe linear-time LC algorithm, and the last one shows the quadratic cost of querying all non- trivial \napplications (there are O(n) of them and each has cost O(n)). Note that the graph build-ing phase (which \nconsists of a simple linear-time pass over the program text) appears to be slightly non-linear (e.g. \n0.029 x 2 # 0.082). This may be due to cache effects, timing inaccuracies, or to inefficiencies introduced \nin the implementation of the prototype (e.g. lists are currently used to rep resent some aspects of the \ngraph s structure). Next, we give the results from two standard SML benchmarks, the life program, and \nthe lexer generator. These results indicate that our pre-liminary implementation of the linear-time algo-rithm \nis 2.5 -3 times faster than SBA. Perhaps more significant is the number of nodes gener-ated by the linear-time \nalgorithm. The number of nodes in the build phase of the analysis is es- sentially the same as the number \nof syntax nodes in the program. The key quantity is the number of nodes added during the close phase \nof the algorithm: this gives a measure of the number of times rules such as CLOSE-DOM and CLOSE-RAN are \napplied. The results suggest that the number of nodes added in the close phase is typically no more than \nthe number of nodes in the build phase, although more benchmarks are clearly needed. We remark that the \ntiming results probably overstate the cost of the linear-time algorithm. Additional measurements have \nshown that the cost of the analysis time for the linear-algorithm is now dominated by the cost of just \ntraversing the intermediate representation: for the lexgen example, this cost accounted for up to 198 \nms out of the total 368 ms for the benchmark, and for life it was 65 ms out of 83 ms. 11 Conclusion \nWe have introduced the notion of a subtransitive control-flow graph: this is a graph whose transi- tive \nclosure represents control-flow information. The key advantage of this graph is that we can develop O(n) \nalgorithms (where n measures the size of the graph) for many control-flow consum-ing applications. Our \nmain result is a linear-time algorithm for bounded-type programs that builds a sub-transitive control-flow \ngraph whose transitive clo-sure gives exactly the results of the standard (cubic-time) CFA algorithm. \nOur algorithm can be used to list sll functions caIls from aU call sites in (optimal) quadratic time. \nHowever, we argue that the all calIs from all call-sites view of control-flow analysis is unsuitable \nfor investi- gating the complexity of analyses. In particular, we show that by directly using the sub-transitive \ngraph (instead of using the quadratic sized all calls from all call-sites representation), we can develop \nlinear-time algorithms for many CFA-consuming applications. Examples include effects analysis and L-limited \nCFA. We leave the ques- tion of the generality of this approach to future work. In particular, are there \nnatural CFA-consuming applications that require the entire all caIls from alI call-sites information, \nor can we adapt our techniques to all CFA-consuming applications? We note that linear-time algorithms \nfor other forms of control-flow analysis have previously been proposed. In effect, these algorithms re-place \ncontainment by unification in the definition of control-flow information, and as a result com-pute information \nthat is strictly less accurate that standard CFA. Our paper shows that this loss of information is not \nnecessary to obtain linear-time algorithms. Table 1: hnear-time al8orithm benchmark prog. size SBA total \nburld close (lines) (time) time time nodes trme nodes hfe 150 0.201 0.083 0.069 1429 0.013 564 lexnen \n1180 1.090 0.368 0.217 3624 0.150 2651 Table 2: Our algorithm could potentially be combined with the \nstandard cubic-time CFA algorithm to obtain a hybrid algorithm that terminates for ar- bitrary programs \nbut is linear for bounded-type programs. Another area of future work involves extending the analysis \nto make use of evaluation- order information. References A. Aho, J. Hopcroft, and J. UIImann, Time and \ntape complexity of pushdown automaton languages , Ifl~ormation and Control, vol. 13, no. 3, pp. 186-206, \n1968. PI A. Bondorf and J. Jorgensen, Efficient PI analysis for realistic off-line partial evabia-tion \n, Journal of finctionol Progmmming, Vol. 3, No. 3, July 1993. S. Debray and T. Proebsting, Inter-procedural \nControl Flow Analysis of First Order Programs with Tail Call Optimiza-tion , draft, May 1996. (http://www.cs.arizona.edu/people/ \ndebrayf papers/cfa.ps) [31 N. Heintze, Set-Based Analysis of ML [41 Programs , ACM Conference on Lisp \nand Fkmctional Programming, pp 306-317, 1994. N. Heintze, Control-Flow Analysis and Type Systems , Static \nAnalysis Sympo-sium, 1995, pp 189-206. [51 [ 51 N. Heintze and D. McAllester, On the Cubic-Bottleneck \nof Subtyping and Flow Analysis IEEE Symposium on Logic in Computer Science, 1997, to appear. F. HengIein, \nType Inference with Polymor- phic Recursion , Transactions on Progmm- [71 ming Languages and Systems, \nVol. 15, No. 2, pp. 253-289, 1993. N. Jones, Flow Analysis of Lambda Ex- PI pressions , Symp. on Functional \nLanguages and Computer Architecture, pp. 6674, 1981. N. Jones, Flow Analysis of Lazy Higher- Order Functional \nPrograms , in Abstmct Interpretation of Declamtive Languages, S. Abramsky and C. Hankin (Eds.), EIIis \nHor- wood, 1987. PI D. M&#38;Rester, Inferring Recursive Data Types , draft manuscript, July 1996. PO1 \nC. Mossin, Control Flow Analysis for Higher-Order Typed Programs , draft Ph.D. thesis, DIKU, University \nof Copen- PI  hagen, December, 1996. E. MeIski and T. Reps, Interconvertibility of Set Constraints and \nContext-Free Lan-guage Reachability , PEPM 97, to appear. WI R. Milner, M. Tofte and R. Harper, The Definition \nof Standard ML , MIT Press, P31 1990. R. Neal, The computational complex-ity of taxonomic inference , \nunpublished manuscript, 18 pages, 1989, (ftp://ftp.cs.utoronto.ca/pub/radford/ taxc.ps.Z). P41 J. PaIsberg \nand P. O Keefe, A Type Sys- 1151 tem Equivalent to Flow Analysis , POPL-95, pp. 367-378, 1995. J. Palsberg \nand M. Schwartzbach, Safety Analysis versus Type Inference Injorma-tion and Computation, Vol. 118, No. \n1, pp. 128-141, April 1995. WI 0. Shivers, Control Flow Analysis in Scheme , Proc. 1988 A CM Conj. on \nPro-gmmming Language Design and Zmplemen- tation, Atlanta, pp. 164-174, June 1988. P l   \n\t\t\t", "proc_id": "258915", "abstract": "We present a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give linear-time algorithms for CFA-consuming applications such as:&amp;bull; effects analysis: find the side-effecting expressions in a program.&amp;bull; <i>k</i>-limited CFA: for each call-site, list the functions if there are only a few of them (&amp;le; k) and otherwise output \"many\".&amp;bull; called-once analysis: identify all functions called from only one call-site.", "authors": [{"name": "Nevin Heintze", "author_profile_id": "81100251839", "affiliation": "Bell Labs, 600 Mountain Ave, Murray Hill, NJ", "person_id": "P208265", "email_address": "", "orcid_id": ""}, {"name": "David McAllester", "author_profile_id": "81100488875", "affiliation": "AT&T Labs, 600 Mountain Ave, Murray Hill, NJ", "person_id": "PP43121220", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258939", "year": "1997", "article_id": "258939", "conference": "PLDI", "title": "Linear-time subtransitive control flow analysis", "url": "http://dl.acm.org/citation.cfm?id=258939"}