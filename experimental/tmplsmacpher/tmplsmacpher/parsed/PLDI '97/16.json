{"article_publication_date": "05-01-1997", "fulltext": "\n Efficient Formulation for Optimal Modulo Schedulers Alexandre E. Eichenberger Edward S. Davidson ECE \nDepartment EECS Department North Carolina State University University of Michigan Raleigh, NC 27695-7911 \nAnn Arbor, MI 48109-2122 alexe@eos.ncsu.edu davidson0eecs.umich.edu Abstract Modulo scheduling algorithms \nbased on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling \nheuristics. While recent advances have broadened the scope for which the optimal approach is applicable, \nthis approach increasingly suffers from large execution times. In this paper, we propose a more efficient \nformulation of the modulo scheduling space that significantly decreases the execution time of solvers \nbased on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 \nwhen 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register \nrequirements using the more efficient formulation instead of the traditional formulation. Experimental \nevidence further indicates that significantly larger loops can be scheduled under realistic machine constraints. \n1 Introduction Current research compilers for VLIW and superscalar machine8 focus on exposing more of \nthe inherent paral-lelism in an application to obtain higher performance by better utilizing wider issue \nmachines and reducing the schedule length of a code. There is generally insufficient parallelism within \nindividual basic blocks and higher levels of parallelism can be obtained by also exploiting the instruction \nlevel parallelism among successive basic blocks. Modulo scheduling [1][2][3] is a technique that exploits \nthe instruction level parallelism present among Permission to make digital/hard copy of part or all this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advan-tage, the copyright notice, the title of the publication and its date \nappear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. PLDI \n97 Las Vegas, NV, USA 0 1997 ACM 0-89791-907-6/97/0006...$3.50 the iterations of a loop by overlapping \nthe execution of consecutive loop iterations. It mre8 the same schedule for each iteration of a loop \nand it initiates successive iterations at a constant rate, i.e. one initiation interval (II clock cycles) \napart. In order to efficiently exploit the available instruc-tion level parallelism, modulo scheduling \nalgorithms must take into account the constraint8 of the target processor, such a8 the latencies of the \noperations, the number of available resources, and the size of the reg-ister files. In addition to satisfying \nthe above machine constraints, several potentially conflicting objective8 are typically considered, such \nas minimizing the initiation interval of the modulo schedule, minimizing the sched- ule length of a loop \niteration, and minimizing the reg-ister requirement8 of the resulting modulo schedule. Because of the \npotentially conflicting nature of these above objectives, and to investigate the best feasible schedule8 \nfor a given loop iteration and set of machine constraints, modulo scheduling algorithm8 based on op- \ntimal solvers have been proposed. These algorithms, referred to aa optimal modulo schedulers are only \nopti- mal with respect to their objective functions. Exam-ples of objective function8 found in the literature \nare minimum II, minimum schedule length among all minimum-11 modulo schedule8, and minimum regis-ter \nrequirements among all miniium-ll modulo sched-ules. Note also that the code is generally assumed to \nbe optimized (including transformations such as load-store elimination, strength reduction, and loop \nunrolling) and no further code transformations are performed while scheduling. Recent advances in optimal \nmodulo scheduling algo rithms have broadened the scope of the machines for which this approach is applicable: \nfor example, ma-chines with arbitrary patterns of resource usages can be handled using the formulation \nproposed in [4] or [5]. The formulation in [5] may also map each operation to a given instance of a re8ource \n(e.g. map a multiply op-eration to one of the multiply functional units). Recent advances have also refined \nthe secondary objectives that may be minimized; for example, the actual register re-quirements of a modulo \nschedule (i.e. the maximum number of live variable8 at any cycle of the schedule, referred to as MaxLive) \nmay be minimized using the formulation proposed in [4]. While the optimal modulo scheduling approach \nmay be further extended to, for example, architecture8 with clusters of functional units or scheduling \nmodels that integrate spill code generation or loop transformations, such extensions will only be practical \nif loops of rea- sonable size may be solved in a reasonable amount of time. Unfortunately, the present \nstate-of-the-art op-timal modulo scheduler8 baaed on integer linear pro-gramming solver8 has not yet \nreached this stage, when accounting for all the machine constraints, even when scheduling for traditional \nmachines and without con-sidering spill code generation or loop transformations. A8 recently illustrated \nby Ruttenberg et al [6], a mod- ulo scheduler that minimizes the register requirement8 among all minimum-11 \nmodulo schedule8 has difficulty finding solution8 in reasonable time for medium-sized loops when precisely \nmodeling the machine constraints. However, they show that if simplifying assumptions are made (by minimizing \nan approximation of the register requirement8 and by ignoring the memory bank con-tentions present on \nthe actual machine), the resulting schedules, although optimal under the simplifying as-gumptions, are \nfrequently slower on the actual machine and have often higher register requirement8 than the schedules \nobtained by a carefully tuned heuristic. A8 a result, we may conclude from their study that modulo schedulers \nbased on optimal solvers provide useful in-sight only if they are both efficient, to solve problems of \nreasonable size, and precise, to find solutions that are relevant to the actual machine. In this paper, \nwe address this major concern by proposing a more efficient formulation of the modulo scheduling space, \nwhich can be used by integer lin-ear programming solvers (IP solvers) to significantly de-crease the \ncomputation time required to find an optimal schedule under realistic machine constraints. Compared to \nthe traditional formulation of the modulo scheduling space used in previous work [4][5] [S] [7], this \nnovel formu- lation decreases the number of branch-and-bound nodes visited by the IP solver by two orders \nof magnitude, on average, when searching for schedules with minimum register requirements among all minimum-11 \nmodulo schedules, for the 782 loops (out of 1327 loops from the Perfect Club, SPEC-89, and the Liver-more \nFortran Ker-nels) that were successfully scheduled by both the tra-ditional and the proposed formulations. \nThis decrease results, in turn, in a reduction of the total computation time of the solver by a factor \nof 8.6, from 870.2 to 101.0 seconds. Furthermore, this more efficient formulation en-ables us to increase \nthe number of loops successfully scheduled for minimum register requirements among all minimum-11 modulo \nschedule8 from 782 to 917 loops. When searching for a minimum II modulo schedule only, up to 1179 or \n88.8% of the 1327 loops can be sched- uled in reasonable time (which was set to 15 minutes per loop in \nour experiments). Note also that these result8 are all obtained when scheduling for the Cydra 5 ma- chine, \na machine with complex resource requirements. Using the proposed technique in conjunction with a commercial \ninteger linear programming solver, one should be able to build in a few day8 an optimal mod-ulo scheduler, \nwhich can be used to evaluate and fine tune the performance of modulo scheduling heuristics. In this \npaper, we use our formulation to evaluate the performance of the schedule8 produced by the Iterative \nModulo Scheduler [3] [8] as well as the register require ments of the schedule8 produced by the Stage \nSchedul- ing heuristics [9][10] used in conjunction with Iterative Modulo Scheduler. In this paper, we \npresent the background concepts and the traditional formulation of the modulo schedul-ing space in Sections \n2 and 3, respectively. We derive a more efficient formulation of the dependence constraint8 in Section \n4. We evaluate the benefits of our technique in Section 5 and conclude in Section 6.  2 Backgrounds \non Modulo Scheduling In this section, we present the background concepts used in modulo scheduling. The \nexample target ma-chine is a hypothetical processor with three fully-pipelined general-purpoee functional \nunits. The mem-ory latency and the add/sub latency is one cycle, and the mult latency is four cycles. \nWe selected these val-ues to obtain a concise example; however, our method works independently of the \nnumber8 of functional units, resource constraints, and latencies. Example 1 This example [ll] illustrates \nthe schedul- ing constraints and the register requirement8 of a modulo8cheduled loop. This kernel is: \nyCi1 = x Cil 2- x Cil - a, where the value of x Cil is read from memory, squared, decremented by x Cil \n+a, and stored in y Cil , as shown in the dependence graph of Figure la. The vertices of the dependence \ngraph correspond to operations and the edge8 correspond to virtual registers. The value of each virtual \nregister is defined by a unique operation and once its value has been defined, it may be a) Data dependence \ngraph b) Schedule (k2) d) Lifetimes $3 \\ * + -9 portion of the lifetime Additional lifetime Schedule \nof one iteration / fst c) MRT (M) e) Register requirements WZO vrl vr2 vr3 Time: ;,, 2:::; 1 y I et \n1 : Lifetime Figure 1: Modulo schedule for the kernel y Cil = x [il 2-x [il -a with minimum register \nrequirements. used by several operations. In this paper, a virtual reg-ister is reserved in the cycle \nwhere its define operation is scheduled, and remains reserved until it becomes free in the cycle following \nits last-use operation. The lifetime of a virtual register is the set of cycles during which it is reserved. \nThe scheduler places each operation of an iteration so that both resource constraints and dependence \ncon-straints are fulfilled. Figure lb illustrates a schedule with an ZZ of 2 for the kernel of Example \n1 on the tar- get machine. In this schedule, the load, mult, add, sub and store operations of the iteration \nstarting at time 0 are respectively scheduled at time 0, 1, 2,5, and 6. The schedule can be divided into \nstages of ZZ cycles each. In this example, the above operations are respectively scheduled in stage 0, \n0, 1, 2, and 3. The module reservation table (MRT) associated with a schedule is obtained by collapsing \nthe schedule for an iteration to a table of ZZ rows, using wraparound. Fig-ure lc illustrates the MRT \nassociated with the sched-ule of Figure lb. The resource constraints of a modulo schedule are satisfied \nif and only if the packing of the op- erations within the ZZ rows of the MRT does not exceed the resources \nof the machine. For our target machine, the resource constraints allow up to 3 operations of any kind \nto be issued in each row of the MRT. The initiation interval is bounded by the minimum initiation interud \n(MZZ) [ 11, which is a lower bound on the smallest feasible value of ZZ for which a modulo schedule can \nbe found. This lower bound is constrained either by critical resources being fully utilized or by critical \nloop-carried dependence cycles. Note that the MZZlower bound is not a tight lower bound as they may be \nno feasible modulo schedules that achieves MZZ, possibly due to the presence complex resource patterns \nor to the interference between resource and dependence constraints [8]. The virtual register lifetimes \nassociated with this it- eration are presented in Figure Id. The register require- ments can also be \ncomputed by collapsing Figure Id to ZZ rows, with wraparound, as shown in Figure le. We see that exactly \n7 virtual registers are live in the first row and 7 in the second. Thus the register require ments, which \nare determined by the row with the maxi- mum number of live values, and referred to as MazZ,iue [12], \nare thus 7 in this example.  3 Backgrounds on Optimal Modulo Scheduling In this section, we first present \nthe traditional formula-tion that is used by optimal modulo schedulers based on integer linear programming \nsolvers. We then present a general framework used by optimal modulo schedulers to find a minimum-ZZ modulo \nschedule. The traditional formulation used by optimal modulo schedulers consists of two types of variables \nper oper-ation: one type describing the MRT row and one type describing the stage associated with each \noperation. The traditional formulation consists of three types of scheduling constraints: the assignment \nconstraints, the dependence constraints, and the resource constraints. The formulation of the variables \nand the first two types of constraints (presented in Sections 3.1 and 3.2) were proposed by Govindarajan \net al [7], and the last type of constraints (shown in Section 3.3) was proposed by us in [4]. Relating \nthe more efficient formulation investigated in this paper to the traditional formulation, the more efficient \nformulation reuses the same variables, assign-ment constraints, and the resource constraints as the traditional \nformulation; however, the more efficient for- mulation differs in its formulation of the dependence constraints, \nwhich will be presented in Section 4. In this work, we represent a loop by a dependence graph G = {V, \nEsched, Ereg}, where the set of vertices V represents operations and the sets of edges Esched and E,,, \ncorrespond, respectively, to the scheduling depen- dences and the register dependences among operations. \nA scheduling edge enforces a temporal relationship be+ tween dependent operations or between any operations \nthat cannot be freely reordered, such as load and store operations to ambiguous memory locations. A schedul- \ning edge from operation i to operation j, 20 iterations later, is associated with a latency li,j and \na dependence distance wi,j = w. A register edge corresponds to a data flow dependence carried in a register. \n3.1 Assignment Constraints Consider a loop with N operations and an initiation interval of II. We represent \na schedule for this loop by a II x N binary matrix, called A, where o,i = 1 if and only if operation \ni is scheduled in row T of the MRT and 0 otherwise. The first condition that a valid modulo schedule \nmust satisfy is that each operation is scheduled exactly once in the MRT: II-1 G,i = vi E [OJV) (1) c \n1 r=ll Equation (1) defines all the assignment constraints, i.e. the constraints that assign each operation \nto exactly one row of the MRT. 3.2 Dependence Constraints While the A matrix defines the row in which \neach op- eration is scheduled, we must also select the stage in which each operation is placed. We represent \nthe stage numbers by k, an integer vector of dimension N, where Ici is the stage number in which operation \ni is sched- uled. Matrix A and vector k uniquely define the cycle in which each operation is scheduled. \nWe now introduce two derived parameters that char-acterize the MRT row and the time at which each op- \neration is scheduled, which are defined as follows: II-1 TOWi = r * G,i timei = ki * II+ TOWi (2) c r=l \nNote that since o,i = 1 precisely in the row in which operation i is scheduled, and is 0 otherwise, TOWi \nis correctly computed and satisfies the following property: ruwi E [O,ZI). A modulo schedule must enforce \nall the scheduling dependences of its dependence graph. A dependence be-tween operation i and operation \nj, wi,j iterations later, is fulfilled if operation j is scheduled at least li,j cycles after operation \ni: (timej + W&#38;j * II) -timei 1 Ei,j (3) Substituting Equation (2) into Inequality (3) results in \nthe following inequality: II-1 C r * (a,,j -a,i) + (kj -IEi) * 11 2 k,j - r=l Wi,j * II v(i,i) E Eached \n(4) Inequality (4) defines all the dependence constraints of a modulo schedule for a given initiation \ninterval II with respect to the dependence distances Wi,j and de- pendence latencies Ei,j of a dependence \ngraph G.  3.3 Resource Constraints The third condition that a valid modulo schedule must satisfy is \nthat no cycle of the schedule consumes more resources than are available in the machine. In this paper, \nwe use the constraints derived in [4]: N-l cc a(r-e)modlI,i 5 Mq i=O ~EResi,, Vq E Q, T E [O,II) (5) \nwhere Q is the set of resource types, Mq is the number of resources of type q, and c E Resi,, indicates \nthat op-eration i uses a resource of type q exactly c cycles after being issued. Note that for machines \nwhere a mapping from each operation s resource usages to resource in-stances cannot be trivially found, \nthe formulation pro-posed by Altman et al [5] should be used. A derivation of Inequality (5) as well \nas a precise definition of the machines for which Inequality (5) is applicable is found in [lo].  3.4 \nOptimal Module Scheduling Framework The traditional formulation of the modulo scheduling space is based \non the assignment, dependence, and re-source constraints as defined by Constraints (l), (4), and (5), \nrespectively. In this formulation, and for a given II, each variable (i.e. each element of the A matrix \nand k vector) is only multiplied by a constant factor; thus an integer linear programming solver (IP \nsolver) can be used to find a solution. However, since the primary objective is to minimize the initiation \nin-terval, a schedule with minimum II is obtained by solv- ing a series of integer programming problems \nuntil the smallest II with a feasible solution is found. A traditional framework of optimal modulo sched-uler \nfor minimum II based on IP solvers is thus defined as follows. First, the minimum initiation interval \n(MI4 [8] is computed, and the tentative II is set to ML Sec-ond, the integer linear programming system \nfor the ten- tative II, given loop iteration, and given target machine is constructed. Third, an IP solver \nis used to solve the system, possibly minimizing a secondary objective func-tion such as the schedule \nlength of the loop iteration or the register requirements of the resulting modulo sched- ule. If the \nIP solver fails to find a feasible solution, the tentative II is incremented by one, and the second and \nthird steps are repeated; otherwise, the solution found is optimal. 4 Structured Formulation of the Dependence \nConstraints Solving an integer linear programming system can be implemented by iteratively solving a \nlinear program-ming model where additional constraints are introduced (and removed) to force each integer \nvariable to an inte- ger value without omitting from the solution space any optimal (integer) solution \n[13]. A branch-and-bound al-gorithm is used to determine which parts of the search space to consider, \nand each branch-and-bound node is evaluated by solving a linear programming model with the original constraints \naugmented by some additional constraints that force variables to integer values. A key aspect for formulating \nan efficient integer program is to find a formulation that results in fewer branch-and-bound nodes, a \ngoal that can be achieved in part by structuring the problem so that the linear pro- gramming solver \nnaturally results in an integer solution for as many integer variables as possible. Recent work by Chaudhuri \net al [14] on the structure of the schedul- ing problem has shown techniques to formulate efficient integer \nlinear program for scheduling straight-line (non-loop, nonbranching) code. One beneficial property that \ncan be derived from their theoretical results is defined here as follows: Definition 1 (O-l-Structured \nConstraints) A constraint is defined as O-l-structured if each variable appears at most once, multiplied \nby either a 0, +l, or -1 constant coeficient. By eztension, a formulation is defined as O-l-structured \nif each of its constraints are structured. Note that the assignment constraints defined by Equation (1) \nand the resource constraints defined by Inequality (5) satisfy this property, whereas the depen-dence \nconstraints defined by Inequality (4) do not sat-isfy this property, since the elements of the k vector \nare multiplied by II and the elements of the binary A matrix are multiplied by r E [0, II). In this section, \nwe investigate a more efficient for-mulation of the dependence constraints that can be used instead of \nConstraints (4) in the optimal modulo scheduling framework described in Section 3.4. Experi- mental evidence \nshown in Section 5 indicates that when this O-l-structured formulation of the dependence con-straints \nis used, the number of branch-and-bound nodes visited by the IP solver is decreased by two orders of \nmagnitudes, on average, for a benchmark of 782 loops scheduled for minimum register requirements among \nall minimum-11 modulo schedules. The basic idea for this reformulation is due to Chaudhuri et al [14] \nwhich has such a reformulation for straight line (nonloop, non-branching) code. The adaptation of this \nidea to modulo schedules for loop code is, however, not straightforward and substantially different in \ndetail, as is the proof of the validity of this adaptation. In the remainder of this section, we derive \na O-l-structured formulation of the dependence constraint as-sociated with scheduling edge (i, j) in \nthree steps. First, we assume in Section 4.1 that we know the scheduling time of operation i (i.e. timei \nis known and constant) and derive a structured constraint that precisely deter-mines the scheduling times \nfor operation j that satisfy scheduling edge (i, j). Second, we show in Section 4.2 how to extend this \nresult without assuming the value of timei to be known and constant. Third, we de-rive in Section 4.3 \nthe final structured formulation of the dependence constraints. Derivations in Sections 4.1 and 4.2 are \noriginal to our work, and the technique proposed by Chaudhuri et al for straight line code is adapted \nto loop code in Section 4.3. 4.1 Case with known timei Recall that a dependence constraint associated \nwith scheduling edge (i, j) enforces the scheduling depen-dence between operation i and operation j, \nWi,j it- erations later, and was defined in Inequality (3) as timej + Wi,j * II -timei > 1i.j. Since \neach term in the dependence constraint has an integer value, we may reformulate this constraint as a \nstrict inequality, i.e. timej +wi,j *II -timei > li,j -1. We may thus write: ti77lej + Wif * II > timei \n+ li,j -1 (6) The left hand side of Inequality (6) corresponds to the times where the value produced \nby operation i can legally be used by operation j (wi,j iterations later). For conciseness, we refer \nto this value as time, (for the time of use) in the remainder of Section 4. Using the relations timej \n= kj * II + rmj and time, = k, * II + row,, we may thus define the stage and row of time, as, respectively, \nku = kj + wi,j rawu = TOWj (7) Similarly, the right hand side of Inequality (6) corre- sponds to the \nlatest time in which the value produced by i is forbidden from use. We refer to this value as timef (for \nthe last forbidden time) in the remainder of Section 4. Using the relations timei = iii * II + ToWi and \ntimef = kf * II + rowj, we may thus define the stage and row of timef as, respectively, rOWi + Zi,j -1 \n kf = ki + II J = (TWi + lij -1) mod II  rmf (8) Note that since we assume here that the value of \ntimei is known and constant, by extension, the values of kf and rowf are known and constant as well. \nUsing the definitions from Equations (7) and (S), we may write the dependence constraint expressed in \nInequality (6) as: ku*II+row, > kf *II+rowj (9) We may transform Inequality (9) to isolate the two \nstage numbers, k, and kf: k, -kf > roLvf ;Irowy Interestingly, we can show that the row difference \nrowf -row, has values in the range (-II, II) since row numbers have by definition values in the range \n[0, II). Consequently, the right hand side of Inequality (10) has values in the range (- 1,l). Therefore, \nwe can guarantee that when the integer valued stage difference k, -kf is 1 (or larger), the dependence \nconstraint is satisfied. Sim-ilarly, we can guarantee that when the stage difference k, -kf is -1 (or \nsmaller), the dependence constraint is violated. We may thus write: (k, > kf) + dep. is satisfied (11) \n(k, < kf) + dep. is violated (12) Consequently we are able to determine the schedul- ing times of operation \nj that satisfy the dependence constraint when k, # kf. Otherwise kf = k,, i.e. both the use time of the \nvalue produced by operation i and the latest forbidden time for that value occur in the same stage of \nthe schedule, and thus the specific values of row and rowf must be take into account to deter-mine whether \ntimej is feasible. To evaluate this case, let kf = ku and substitute kf for k, in Inequality (9), obtaining \nthe following depen- dence constraint: rmu > rc7wf (13) Inequality (13) simply states that operation \nj cannot be assigned to any of the rows z E [0, row,] when kf = k,. Reformulating Inequality (13) using \nthe binary a,,j variables, we obtain: 0.%=c %,j = (14) 2=0 where the value of the sum in Equation (14) \nis referred to as z. Because of the assignment constraints, we know that only one a,,j variable is equal \nto 1, and all other a,,j variables are equal to 0. Thus, we know that z is either 0 or 1, depending on \nthe row in which operation j is scheduled. Consequently, we can clearly see that Equation (14) is satisfied \nif and only if operation j is not assigned to any of the rows in the range [0, rmuf]. As a result, Inequality \n(13) and Equation (14) are equivalent. We may thus write: (Ic, = kf) &#38; (Z = 0) * dep. is satisfied \n(15) (ku = kf) &#38; (Z # 0) =+ dep. is violated (16) To summarize our findings, we have shown that op-eration \nj satisfies the scheduling edge (i, j) if either Fte-lation (11) is satisfied (i.e. k,, > kf) or Relation \n(15) is satisfied (i.e. k, = kf and z = 0). Otherwise, we have shown by Relations (12) and (16) that \nthe dependence constraint is violated. We may now combine the two disjoint Relations (11) and (15) by \nformulating the following constraint:  kf-k,+z 5 0 (17) Inequality (17) is equivalent to the union of \nBela-tions (11) and (15) because when k, > kf , Inequal-ity (17) holds regardless of the value of z since \nz E [0, l] and when k, = kf , Inequality (17) holds precisely when % = 0. Using the definitions in Equations \n(7), (8), and (14) 4.3 Final Formulation of the Structured Depen-in Inequality (17), we obtain the following \nconstraint: dence Constraints IWWi + li,j - Ici + --+ II (rowi+li,j-l)modll G,j 0 z=o we derived constraint \ndeter- for given (expressed terms ki rowi), scheduling for j in of and that the edge j). constraint O-l-structured \nthat vari- (i.e. and appear once are multiplied +l, and constant 4.2 Case with unknown time1 We now \nextend the previous result to the case where the scheduling time of operation i, timei, is not assumed \nto be known. Since the row in which operation i is sched- uled is also unknown, we must eliminate rowi \nfrom In- equality (18). The main idea used in this section is to substitute rwi by an arbitrary row T \nE [O,II), and transform the right hand side of Inequality (18) such that the new inequality is only constraining \nwhen op eration i is effectively scheduled in row r. Our claim is that the following inequality: (r+fi,j-l)modlI \nc &#38;z,j I 1 -G,i vr E [O, II) (19) 2=0 is equivalent to Inequality (18) when r = ToWi for some row \nr E [0,11) and is trivially satisfied other-wise. The sketch of a proof is as follows. Consider a row \nT E [0, II). If operation i is scheduled in row r , by definition a+l,i = 1, and thus the right hand \nside of the Inequality (19) with r = T is 0. This inequal-ity is thus identical to Inequality (18) since \nin this case T = T = rmi. Otherwise, if operation i is not sched-uled in row T , by definition a+,i = \n0, and thus the right hand side of the Inequality (19) with r = r is 1. This right hand side value is \njust large enough to ensure that this inequality is trivially satisfied. A detailed proof is provided \nin [lo]. While we may simply formulate structured dependence constraints for each dependence edge (i, \nj) E Esfhed using Inequality (19), we may further tighten the for-mulation of the scheduling space using \nan observation made by Chaudhuri et al [14] for dependent operations in straight line (nonloop, nonbranching) \ncode. Consider operation i, with latency 1, that produces a value used by operation j. When operation \ni is assigned to cycle t, or any subsequent cycles [14], operation j must be assigned in a cycle t 2 \nt + I. Using a similar observation here, we may replace o,i in the right hand side of Inequality (19) \nby the sum of the o,,i variables over z E [r,II). Thus, we obtain the following final form of the structured \ndependence constraints: II-1 (r+li,j-l)modlI G,i + a,,j + ki -kj 5 Wi,j - c c 2=r 2=0 + 1 VT E [O, II), \nhi) E Esched (20) At first, it may appear counterintuitive to replace Inequality (4) with Inequality \n(20) in order to obtain a more efficient formulation of the problem, since it corresponds to replacing \neach constraint with II new constraints. The crucial point, however, is that the new constraints are \nO-l-structured, since each variable, namely each k and each element of A appears at most once, and is \nmultiplied by only +l, 0, or -1 coefficients. 5 Measurements In this section, we evaluate four modulo \nscheduling al-gorithms based on integer linear programming formula-tion. MinReg Modulo Scheduler. This \nscheduler finds a schedule with the minimum II over all modulo schedules, and with the minimum register \nrequirements among such schedules. It uses the integer programming model based on Constraints (l), (4), \nand (5) when eval- uating the traditional formulation, and Constraints (l), (51, and (20) when evaluating \nthe structured formula-tion of the modulo scheduling space. The formulation of the secondary objective \nfunction (for minimum register requirements) is based on the O-l-structured formula-tion found in [4]. \nRecall that this algorithm achieves the minimum feasible register requirements for a given loop iteration, \ninitiation interval, and set of machine constraints.     ,000  1 l i z ,oo lo -[ i 1 0.1 Figure \n2: Average number of branch-and-bound nodes visited by the CPLEX solver. MinBuff Modulo Scheduler. This \nscheduler finds a schedule with minimum II, and the minimum buffer requirements among all such schedules. \nRecall that un- like registers, buffers are reserved for integer multiples of II cycles. When considering \nthe traditional formu-lation of the modulo scheduling space, we use Con-straints (4), (l), and (5,) as \nwell as the formulation of the minimum-buffer secondary objective function found in [7]. When evaluating \nthe structured formulation, we use Constraints (l), (5), and (20); in addition, we refor- mulate the \nminimum-buffer objective function as pro- posed by DuPont de Dinechin [15] in order to obtain a O-l-structured \nformulation . Although the MinBuff algorithm minimizes buffers, in our comparisons we al- ways present \nthe actual register requirements associated with these schedules. MinLife Modulo Scheduler. This scheduler \nfinds a schedule with the maximum steady-state throughput, and the minimum cumulative length of the lifetimes \namong all such schedules. When considering the tradi- tional formulation of the modulo scheduling space, \nwe use Constraints (l), (5), and (4,) as well as the for-mulation of the minimum-lifetime secondary objective \nfunction found in [16]. When evaluating the structured formulation, we use Constraints (l), (5), and \n(20); we As formulated in [7], the minimum-buffer objective function uses additional variables that are \ndefined by constraints which are not O-l-structured. By transforming the problem using a tech- nique \nproposed in [15], however, we may define these additional variables using constraints of the same type \nas Constraints (20). As a result, we obtain a formulation that is entirely O-l structured. also modify \nthe formulation of the objective function (as devised for the MinBuff Modulo Scheduler) in order to obtain \na formulation that is entirely O-l structured. In our comparisons, we also present the actual register \nrequirements associated with these schedules. NoObj Modulo Scheduler. Thii scheduler simply finds a schedule \nwith minimum II, without minimizing any secondary objective function. It uses the same for-mulation of \nthe modulo scheduling space as the MinReg Modulo Scheduler, and simply returns the first schedule that \nit finds. In this study, we use a obtained from the Perfect and the Livermore Fortran by the Cydra 5 \nFortran store elimination, recurrence conversion. The resource benchmark of 1327 loops Club [17], SPEC-89 \n[18], Kernels [19], as compiled compiler [20] after load-back-substitution, and IF- requirements of the \nCy- dra 5 [21] are precisely modeled, using the reduced ma-chine description produced in [22]. Note \nthat we only model, for a given loop, the resources that are used by at least two operations, since the \nother resources, if any, pose no resource conflicts. Furthermore, we limit the schedule length of the \nmodulo schedules sought to 20 cycles beyond the minimum schedule length, in order to achieve schedules \nwith high transient performance. We use here the CPLEX solver, a widely available com-mercial integer \nlinear programming solver, and never search for a schedule for more than 15 minutes per loop on a HP-9000/715 \nworkstation. This time limit is arbi- trary, and was fixed to complete the scheduling of the 1327-100~ \nbenchmark in reasonable time. In our first experiment, we investigate the benefits of using the structured \nformulation instead of the tra-ditional formulation of the dependence constraints. We present data for \nthe 653 loops that were successfully scheduled (using no more than 15 minutes per loop) by each algorithm \nand formulation of the modulo schedul-ing space. We present in Figure 2 the average number of branch-and-bound \nnodes visited by the solver when us-ing either the structured or the traditional formulation. Recall \nthat the CPLEX solver used in our experiment explores branch-and-bound nodes when it must force variables \nto integral values. Note also that the Y-axis of Figure 2 is logarithmic. The first observation is that \nthe four schedulers ben-efit significantly from using the structured formulation of the dependence constraints \nwhich, for example, de-creases the average number of branch-and-bound nodes visited by the MinLife and \nMinReg Modulo Scheduler by a factor of 167.4 and 124.5, respectively. The second observation is that \nthe three algorithms that result in the lowest average number of branch-and-bound nodes are the NoObj, \nMinReg Modulo Schedulers with the structured formulation of the dependence constraints. Because of this \nsignificant decrease in number of visited branch-and-bound nodes, schedulers based on the structured \ndependence constraints run significantly faster; for example, the structured MinReg Modulo Scheduler \ncompletes in 11.6% of the time (i.e. 101.0 instead of 870.2 seconds) when scheduling all the loops successfully \nscheduled by the MinReg Modulo Sched-uler with unstructured dependence constraints. Conse-quently, the \nIP solver can schedule a larger fraction of the loops in the benchmark suite when using the struc-tured \ndependence constraints; e.g. 1179 versus 1084 loops for the NoObj Modulo Scheduler, 898 versus 859 loops \nfor the MinLife Modulo Scheduler, and 917 ver- sus 782 loops for the MinReg Modulo Scheduler. We thus \nexclusively employ the structured dependence con-straints in the remainder of this section. In our second \nexperiment, we investigate the perfor- mance of each modulo scheduling algorithm with the structured \nformulation of the dependence constraints in a benchmark containing all successfully scheduled loops. \nWe present the performance characteristics of the four modulo scheduling algorithms with the struc-tured \nformulation in Table 1. In addition to the number of branch-and-bound nodes, the table also lists data \nthe numbers of variables and constraints prior to any simplifications that might be performed by the \nCPLEX solver, as well as the number of simplex iterations per-formed by the CPLEX solver, the number \nof operations, N, and the initiation interval, II. For comparison, we provide the same data for the four \nalgorithms with the traditional formulation of the dependence constraints in Table 2; however, we only \nconsider the structured formulation in the discussion below. First, observe the distribution of the data \nin Table 1. As indicated by the low median values, relative to the average numbers, there is a large \nnumber of simple loops in the benchmark suites. However, the solvers clearly succeed in solving some \nrather large problems, as indi- cated by the large max values. The second observation is that the NoObj \nModulo Scheduler processes loops with significantly larger av-erage numbers of operations, II, variables, \nand con-straints than the three other algorithms. It also handles the loops with the largest maximum \nnumber of opera- tions and II, i.e. 80 operations and 118 cycles, respec-tively. Note also that since \nit simply returns the first valid integral solution, the number of visited branch-and-bound nodes directly \ncorresponds to the nodes that are needed by the solver to force all variables to integer values. As indicated \nin the table, very few branch-and- bound nodes are required (0 node in 73.9% of the loops, 7.93 nodes \non average, and and never more than 337 nodes) compared to the traditional formulation (0 node in 37.4% \nof the loops, 249.64 nodes on average, and and never more than 29746 nodes) This data confirms again \nthe benefit of structured formulations. The third observation is that the MmReg Modulo Scheduler processes \nloops that are nearly as large as those the MinLife Modulo Scheduler can process, and significantly larger \nthan those of the MinBuff Modulo Scheduler, even though MinReg Modulo Scheduler pre-cisely minimizes \nthe register requirements. In our third experiment, we use the NoObj Modulo Scheduler to investigate \nthe performance of the Itera-tive Modulo Scheduler proposed by Rau [8]. Since the Iterative Modulo Scheduler \nresults in schedules with op- timal throughput for 1274 (or 96.0%) of the 1327 loops in the benchmark \nsuite, we only need to investigate and compare the remaining 53 loops. Among these 53 loops, the NoObj \nModulo Scheduler finds 2 loops where II (that was obtained by the Itera- tive Modulo Scheduler) can be \ndecreased by 2 (but not 3) cycles, i.e. the scheduler finds a schedule with an II decreased by 2 cycles \nand shows that decreasing II by 3 cycles is infeasible. The scheduler also finds 6 loops where II can \nbe decreased by 1 (but not 2) cycles. It furthermore shows that the II of 22 of these loops can-not be \ndecreased even by 1 cycle. Finally, the scheduler does not complete the remaining 23 loops within the \n15 minute execution time limit. Thus, the NoObj Mod-ulo Scheduler succeeds in finding schedules with \nbetter throughput for 8 (or 15.1%) of the 53 interesting loops. Using the NoObj Modulo Scheduler, we \nhave thus shown that the Iterative Modulo Scheduler actually Table 1: Measurement8 with structured scheduling \nconstraints. Measurements: min free I median average max NoObj Modulo-Sched: (1179 loops) Variables \n4.00 0.3% 33.00 183.12 3880.00 Constraints 8.00 0.3% 67.00 298.66 5400.00 Branch-and-bound nodes 0.00 \n74.0% 0.00 8.02 337.00 Simplex iterations 0.00 37.1% 11.00 345.63 20645.00 II 1.00 32.0% 2.00 7.48 118.00 \nN 2.00 0.4% 9.00 13.95 80.00 MinBuff Modulo-Sched: (762 loops) Variables 6.00 0.5% 20.00 36.58 748.00 \nConstraints 9.00 0.5% 33.00 76.11 1922.00 Branch-and-bound nodes 0.00 51.0% 0.00 276.52 21551.00 Simplex \niterations 0.00 0.1% 10.00 1167.72 144568.00 II 1.00 49.3% 2.00 2.71 42.00 N 2.00 0.7% 5.00 6.81 36.00 \nMinLife Modulo-Sched: (898 loops) Variables 8.00 0.4% 48.00 169.50 5846.00 Constraints 11.00 0.4% 72.00 \n230.79 6934.00 Branch-and-bound nodes 0.00 72.7% 0.00 299.37 27142.00 Simplex iterations 1.00 18.3% 29.00 \n2181.56 221039.00 II 1.00 36.3% 2.00 5.84 118.00 N 2.00 0.6% 1 7.00 9.04 41.00 MinReg Modulo-Sched: (917 \nloops) Variables 7.00 0.4% 36.00 119.67 5810.00 Constraints 10.00 0.4% 57.00 169.24 6975.00 Branch-and-bound \nnodes 0.00 63.5% 0.00 124.71 10711.00 Simplex iterations 0.00 24.5% 23.06 2539.64 167504.00 II 1.00 41.1% \n2.00 4.63 118.00 N 2.00 0.5% 7.00 8.39 41.00 Measurements: min freq median average max NoObj Modulo-Sched: \n(1084 loops) Variables 4.00 0.4% 27.00 115.95 3630.00 Constraints 8.00 0.4% 40.00 74.12 792.00 Branch-and-bound \nnodes 0.00 37.4% 4.00 249.64 29746.00 Simplex iterations 0.00 35.6% 10.00 1731.96 424846.00 II 1.00 35.0% \n2.90 6.50 118.00 N 2.00 0.5% 8.00 10.76 52.00 MinBuff ModuloSched: (762 loops) Variables 6.00 0.5% 20.00 \n49.39 2520.00 Constraints 9.00 0.5% 26.00 43.36 698.00 Branch-and-bound nodes 0.00 52.1% 0.00 790.17 \n26730.00 Simplex iterations 0.00 30.3% 3.00 1668.74 65961.00 II 1.00 49.6% 2.00 3.93 118.00 N 2.00 0.7% \n5.00 6.47 21.00 MinLife Modulo-Sched: (859 loops) Variables 6.00 0.5% 28.00 60.13 2520.00 Constraints \n9.00 0.5% 37.00 52.13 698.60 Branch-and-bound nodes 0.00 49.6% 1.00 711.44 29262.00 Simplex iterations \n0.00 26.9% 4.00 1984.99 88811.00 II 1.00 43.9% 2.00 4.37 118.00 N 2.00 0.6% 7.00 7.51 29.00 MinR.eg Modulo-Sched: \n(782 loops) Variables 7.00 0.5% 26.00 77.59 4978.00 Constraints 10.00 0.5% 32.00 77.32 3273.00 Branch-and-bound \nnodes 0.00 51.4% 0.00 518.32 26524.00 Simplex iterations 0.00 28.5% 3.00 4228.82 322537.00 zz 1.00 48.2% \n2.00 3.67 118.00 N  2.00 0.6% 5.00 6.81 25.00 Table 2: Measurements with traditional scheduling constraints. \n203 finds a schedule with maximum throughput for 22 more loops than previously known, i.e. it results \nin sched- ules with maximum throughput for 1296 (or 97.7%) of the 1327 loops in the benchmark suite. \nIn addi- tion to these 22 loops, the NoObj Modulo Scheduler finds 8 more loops with maximum throughput \n(with a higher throughput than the schedule found by the Iter- ative Modulo Scheduler); thus we have \nnow schedules with maximum throughput for 1304 (or 98.3%) of the loops in the benchmark suite. At this \npoint, it is un- known whether the remaining 23 loops have suboptimal throughput. To summarize the findings \nof this section, the algo- rithms based on structured formulation are clearly more efficient than the \ntraditional formulations. The sec-ond result is that the NoObj Modulo Scheduler clearly schedules the \nlargest fraction of the loops in the bench- mark suite. This result is not unexpected since this formulation \ndoes not minimize any objective function. The third and surprising result is that using the Min-Reg scheduler \nis nearly as efficient as using the MinLife scheduler, and is much more efficient than using the MinBuff \nscheduler, in spite of the fact it precisely min-imizes MaxLive. Conclusions In this paper, we contribute \na more structured formula-tion of the modulo scheduling solution space. This more efficient formulation \naddresses a major concern with modulo schedulers that are based on integer linear pro- gramming solvers \n[6], which is their traditionally high execution time. Experimental evidence (gathered for a benchmark \nsuite of 1327 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels, compiled for the \nCydra 5 machine) indicates that the number of branch-and-bound nodes is decreased on average by a factor \nof 103, when using the structured instead of the traditional formulation of the MinReg Modulo Sched-uler \non the 782 loops successfully scheduled by both formulations. It results, in turn, in a decrease in the \nto- tal execution time by a factor of 8.6, from 870.2 to 101.0 seconds. Also, using the more efficient \nrepresentation enables us to successfully schedule a larger fraction of the 1327 loops; for example the \ncoverage increases from 58.9% to 69.1% for the MinReg Modulo Scheduler and from 81.7% to 88.8% for the \nNoObj Modulo Scheduler. Also, significantly larger loops are successfully sched-uled: for example, the \nmaximum number of operations in a loop increases from 25 to 41 operations for the MinReg Modulo Scheduler \nand from 52 to 80 for the NoObj Modulo Scheduler. These results are obtained when scheduling for the \nCydra 5 machine, a machine with complex resource requirements. Using this more efficient formulation \nof the mod-ulo scheduling solution space, we may carry through relevant performance evaluations of existing \nscheduling heuristics. For example, we have analyzed the perfor-mance of the Iterative Modulo Scheduler \nproposed by Rau [3][8]. While this algorithm was known to achieve the minimum initiation interval (i.e. \nMII) in 96.0% of the 1327 loops in the benchmark suite, we can show that it actually achieves MII in \n97.7% of the loops. Fur-thermore, our algorithm finds schedules with lower II for 8 of the 31 remaining \nloops. We have also analyzed the register requirements of the stage scheduling heuris-tics proposed in \n[9][10] used in conjunction with the It- erative Modulo Scheduler. Using the MinReg Modulo Scheduler, \nwe find schedules with lower register require- ments for 23.6% of the 1327 loops. By evaluating the stage \nscheduling heuristics with the MinLife and the MinBuff Modulo Schedulers, a schedule with lower reg- \nister requirements is found in 18.5% and 4.5% of the loops, respectively. However, the heuristic finds \na sched- ule with lower register requirements in 3.2% and 12.3% of the loops, respectively. These results \nconfirm that optimal algorithms must be both precise and efficient to provide useful insights.  Acknowledgments \nThe authors would like to thank B. Ramakrishna Rau for his many useful suggestions and for providing \nthe input data set. We are grateful to Vinod Kathail for his explanation of the resource constraints \nof the Cy-dra 5 machine. We also appreciate the suggestions of the referees which significantly improved \nthe quality of the paper. This work was supported in part by the Office of Naval Research under grant \nnumber NO0014 93-l-0163 and by Hewlett-Packard.  References ill B. R. Rau and C. D. Glaeser. Some scheduling \ntech-niques and an easily scheclulable horizontal architecture for high performance scientific computing. \nFourteenth Annual Workshop on Microprogramming, pages 183-198, October 1981. P. Y. Hsu. Highly Concurrent \nSc&#38;r Processing. PhD thesis, Department of Electrical and Computer Engi- PI neering, University of \nIllinois, Urbana, IL, 1986. B. R. Rau. Iterative Modulo Scheduling: An algorithm for software pipelining \nloops. Proceedings of the 27th 131 Annual International Symposium on Microarchitecture, pages 63-74, \nNovember 1994. PI A. E. Eichenberger, E. S. Davidson, and S. G. Abra- ham. Optimum modulo schedules for \nminimum register requirements. Proceedings of the International Conjer-ence on Supercomputing, pages \n31-40, July 1995. [5] E. R. Altman, R. Govindarajan, and G. R. Gao. Scheduling and mapping: Software \npipelining in the presence of structural hazards. In Proceedings of the ACM SIGPLAN 95 Conference on \nProgramming Language Design and Implementation, pages 139-150, 1995. [6] J. R. Ruttenberg, G. R. Gao, \nand A. Stoutchinin. Soft-ware pipelining showdown: Optimal vs. heuristic meth- ods in a production compiler. \nProceedings of the ACM SIGPLAN OB Conference on Programming Language Design and Implementation, pages \nl-11, May 1996. [7] R. Govindarajan, E. R. Altman, and G. R. Gao. Minimizing register requirements under \nresource-constrained rate-optimal software pipelining. Proceed-ings of the 27th Annual International \nSymposium on Microarchitecture, pages 85-94, November 1994. [8] B. R. Rau. Iterative Modulo Scheduling. \nInternational Journal of Parallel Pmgmmming, 24(1):2-64, 1996. [9] A. E. Eichenberger and E. S. Davidson. \nStage schedul- ing: A technique to reduce the register requirements of a modulo schedule. Proceedings \nof the 28th Annual International Symposium on Microarchitecture, pages 180-191, November 1995. [lo] \nA. E. Eichenberger. Modulo Scheduling, Machine Rep-resentations, and Register-Sensitive Algorithms. PhD \nthesis, University of Michigan, Department of Electri- cal Engineering and Computer Science, Ann Arbor, \nMI, 1996. [ll] A. E. Eichenberger, E. S. Davidson, and S. G. Abra-ham. Minimum register requirements \nfor a modulo schedule. Proceedings of the 27th Annual International Symposium on Microarchitecture, pages \n75-84, Novem-ber 1994. [12] R. A. Huff. Lifetime-sensitive modulo scheduling. Pro-ceedings of the ACM \nSIGPLAN 93 Conference on Pro-gramming Language Design and Implementation, pages 258-267, June 1993. [13] \nG. L. Nemhauser and L. A. Wolsey. Integer and Com-binatorial Optimixation. Wiley, New York, 1988. [14] \nS. Chaudhuri, R. A. Walker, and J. E. Mitchell. An-alyzing and exploiting the structure of the constraints \nin the ILP approach to the scheduling problem. IEEE Tkansactions on Very Large Scale Integration Systems, \n2(4):456-471, December 1994. [15] B. DuPont de Dinechin. Parametric computation of margins and of minimum \ncumulative register lifetime dates. Proceedings of the 9th International workshop on Languages and Compilers \nfor Pamllel Computing, 1996. [la] B. DuPont de Dinechin. Simplex scheduling: More than lifetime-sensitive \ninstruction scheduling. Proceedings of the International Conference on Parallel Architecture and Compiler \nTechniques, pages 327-330, 1994. [17] M. Berry et al. The Perfect Club Benchmarks: Effec-tive performance \nevaluation of supercomputers. The International Journal of Supercomputer Applications, 3(3):5-40, Fall \n1989. [18] J. Uniejewski. SPEC Benchmark Suite: Designed for today s advanced system. SPEC Newsletter, \nFall 1989. [19] F. H. McMahon. The Livermore Fortran Kernels: A computer test of the numerical performance \nrange. Technical Report UCRL-53745, Lawrence Livermore National Laboratory, Livermore, California, 1986. \n[20] J. C. Dehnert and R. A. Towle. Compiling for the Cydra 5. In The Journal of Supercomputing, volume \n7, pages 181-227, 1993. [21] G. R. Beck, D. W. L. Yen, and T. L. Anderson. The Cydra 5 mini-supercomputer: \nArchitecture and imple- mentation. In The Journal of Supercomputing, vol-ume 7, pages 143-180, 1993. \n[22] A. E. Eichenberger and E. S. Davidson. A reduced mul- tipipeline machine description that preserves \nschedul-ing constraints. Proceedings of the ACM SIGPLAN 96 Conference on Pmgmmming Language Design and \nIm- plementation, pages 12-22, May 1996.   \n\t\t\t", "proc_id": "258915", "abstract": "Modulo scheduling algorithms based on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling heuristics. While recent advances have broadened the scope for which the optimal approach is applicable, this approach increasingly suffers from large execution times. In this paper, we propose a more efficient formulation of the modulo scheduling space that significantly decreases the execution time of solvers based on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 when 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register requirements using the more efficient formulation instead of the traditional formulation. Experimental evidence further indicates that significantly larger loops can be scheduled under realistic machine constraints.", "authors": [{"name": "Alexandre E. Eichenberger", "author_profile_id": "81100575094", "affiliation": "ECE Department, North Carolina State University, Raleigh, NC", "person_id": "P14433", "email_address": "", "orcid_id": ""}, {"name": "Edward S. Davidson", "author_profile_id": "81100100296", "affiliation": "EECS Department, University of Michigan, Ann Arbor, MI", "person_id": "PP14044916", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258933", "year": "1997", "article_id": "258933", "conference": "PLDI", "title": "Efficient formulation for optimal modulo schedulers", "url": "http://dl.acm.org/citation.cfm?id=258933"}