{"article_publication_date": "05-01-1997", "fulltext": "\n Componential Set-Based Analysis Cormac Flanagan Matt hias Felleisen cormacQcs.rice.edu matthias@cs.rice.edu \nRice University* Abstract analysis a whole analysis is to and oriented languages. the is for programs, \nit ates of flow that quadratically the of program. paper componential analysis, is and larger without \nloss accuracy set-based The of analysis a of results cerning systems, a re- and decision concerning equivalence \nconstraint Experimental validate practicality the 1 Effectiveness Set-Based Rice s program environment \nvides static MrSpidey, analyzes program using results this checks soundness all primitives If primitive \nmay due a of invariant, highlights program so the can the fault before the Using graph- explanation of \nthe can whether fault really or the correctness is the capabilities. program is constraint-based similar \nHeintze s analysis The consists two phases: deriva-phase, which derives describing data relationships \nthe *This work was partially supported by NSF grants CCR-0633748 and CCR-0610758, and a Lodieska Stockbridge \nVaughan Fellowship. Permission to make digital/hard copy of part or all this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan-tage, the copyright notice. the title of the publication and its date appear, and notice is given \nthat copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee. PLDI 97 Las Vegas, NV, USA 0 \n1997 ACM 0-89791-907-6197/0006...$3.50 program, and a solution phase, during which MrSpidey solves the \nconstraints. The solution conservatively ap proximates the set of values that may be returned by each \nprogram expression. In practice, MrSpidey has proven highly effective for pedagogic programming, which \nincludes programs of several hundred to a couple of thousand lines of code. It becomes less useful, however, \nfor debugging larger programs due to limitations in the underlying analy-sis, which has an O(n3) worst-case \ntime bound. The constant on the cubic element is small, but it becomes dominant for programs of several \nthousand lines. The bottleneck is due to the excessive size of the con- straint systems that describe \na program s data flow rela- tionships. If we could simplify these constraint systems without affecting \nthe data flow relationships that they denote, then we could reduce the analysis times. That is, by first \nsimplifying the constraint system for each program component (e.g. module or package), we could solve \nthe combined system of constraints in less time. Furthermore, if we saved each simplified constraint \nsys-tem in a constraint file, then we could exploit those saved constraints in future runs of the analysis \nto avoid reprocessing components that have not changed. The simplification of constraint systems raises \nboth interesting theoretical and practical questions. On the theoretical side, we need to ensure that \nsimplification preserves the observable behavior of a constraint sys-tem. In this paper, we provide a \ncomplete characteri-zation of observable behavior and, in the course of this development, establish a \nclose connection between this observable equivalence of constraint systems and the equivalence of regular \ntree grammars (RTGs). Exploit-ing this connection, we develop a complete algorithm for deciding the equivalence \nof constraint systems. Unfor-tunately, the algorithm is PSPACEhard. Fortunately, a minimized constraint \nsystem is only optimal but not necessary for practical purposes. The A number of researchers, including \nReynolds [la], Jones and Muchnick [14], Heintze (111, Aiken [2], and Cousot and Coueot [3] previously \nexploited the relationship between RTGa and the leasst so- lution of a constraint system. We present \nan additional result, namely a connection between RTGs and the observable behavior (i.e., the en-tire \nsolution space) of constraint systems. practical question finding algo- for constraint that make more \nTo this we the between minimiza- problems RTGs constraint to a of for RTGs the of constraint Based these \nalgorithms, develop componen- or variant set-based Experimental verify effectiveness the plification \nand corresponding of analysis. simplified systems typ-at an of smaller the systems, these in result significant \nin speed the We that of theoretical practical as as techniques carry to constraint-based such the type \nof et [2], et object-oriented system or or et subtyping algorithms 17, The proceeds follows. 2 scribes \nidealized language. 3 4 the underpinnings the analy- Section introduces constraint tion and 6 7 how algorithms \nin realistic analysis tem. 8 related and 9 directions future  2 The Source Language For simplicity, \nwe derive our analysis for a X-calculus-like language with constants and labeled expressions. It is straightforward \nto extend the analysis to a realistic language including assignments, recursive data struc-tures, objects \nand modules along the lines described in an earlier report [7]. Expressions in the language are either \nvariables, val-ues, function applications, let-expressions, or labeled expressions: see figure 1. We \nuse labels to identify those program expressions whose values we wish to predict. Values include basic \nconstants and functions. Functions have identifying tags so that MrSpidey can reconstruct a call-graph \nfrom the results of the analysis. We use let-expressions to introduce polymorphic bindings, and hence \nrestrict these bindings to syntactic values [23]. We work with the usual conventions and terminology \nof the X,-calculus when discussing syntactic issues. In 2componential a. of or pertaining to components; \nspec. (Ling.) designating the analysis of distinctive sound units or grammatical elements into phonetic \nor semantics components (New Shorter Ozford Englrsh Drctionary, Clarendon Press, 1993) syntax: .I1 E \nA = x 1 1, ) (Af AI) 1 AZ (Expressions) 1 (let (s .!I) AI) v E Value = b 1 (Xtx.M) (Valuee) x E Var.9 \n= {r,y,Z,...} (Variables) b E BConst (Basic constants) t E Tag (Function tags) I E Label (Expression \nlabels) Evaluator: eval : A0 -Value U {I} eual(M) = V if M c* V Reduction Rules: &#38;[ ((X XM) V) ] \n----) E[ af[x H V] ] (P ) EL (let (x VI JW 1 -E[ M[x H V] ] (Olet ) &#38;IV 1 -E[ I ] (unlabd) Evaluation \nContexts: E = [ 1 I (E W I (V f) I (let (x 8) J4 I E Figure 1: The source language A: syntax and semantics \nparticular, the substitution operation M[z t V] re-places all free occurrences of z within M by V, and \nho denotes the set of closed terms, also called programs. We specify the meaning of programs via the \nreduc- tion semantics based on the rules described in figure 1. The reduction rules 0 and /31et are conventional, \nand the unlabel rule removes the label from an expression once its value is needed.  3 Set-Based Analysis \nConceptually, set-based analysis consists of two phases: a specification phase and a solution phase.3 \nDuring the specification phase, the analysis tool derives con-straints on the sets of values that program \nexpressions may assume. These constraints describe the data flow relationships of the analyzed program. \nDuring the solu- tion phase, the analysis produces finite descriptions of the potentially infinite sets \nof values that satisfy these constraints. The result provides an approximate set of values for each labeled \nexpression in the program. 3.1 The Constraint Language To simplify the derivation of the constraint \nsimplifica-tion algorithms, we formulate our constraint language in terms of type selectors, instead \nof the more usual Cousot and Cousot showed that set-based analysis can alter-natively be formulated as \nan abstract interpretation computed by chaotic iteration [3]. type constructors: T E SetExp = (Y 1 c \n1 dam(T) 1 rng(T) a, p, 7 E Set Var > Label c E Const = BConst U Tag A set expression 7 is either a \nset variable; a constant; or one of the selector expressions dam(r) or rng(T). By using selector expressions, \nwe can specify each quan-tum of the program s data flow behavior independently; using constructors would \ncombine several of these quanta into one constraint. The meta-variables a, /3,~ range over set variables, \nand we include program labels in the collection of set variables. Constants include both ba-sic constants \nand function tags. A constraint C is an inequality 71 5 ~2 relating two set expressions. Intuitively, \neach set expression denotes a set of run- time values, and a constraint [~1 5 74 indicates that the value \nset denoted by ~1 is contained in the value set denoted by 72. A constraint system S is a collection \nof constraints. A simple constraint system is a collection of simple constraints, which have the form: \nClB I aSP 1 aSdom(B) I vda) 5P I dom(d I a <w(P) ID In some cases, we are interested in constraints \nthat only mention certain set variables. The restriction of a con- straint system to a collection of \nset variables E is: S 1~ = {C E S I C only mentions set variables in E}  3.2 Semantics of Constraints \nA set expression denotes a collection of values, which is represented as a triple X = (C,D, R). The first \ncom-ponent C E P(Const)* is a set of basic constants and function tags, and represents a set of run-time \nvalues (relative to a given program) according to the relation V in C: binC iff bEC (Xtz.M) in C iff \nt E C The second and third components of X denote the pas- sible argument values (dom) and result values \n(rng) of functions in X, respectively. Since these two compo-nents also denote value sets, the appropriate \nmodel for set expressions is the solution of the equation? P denotes the power-set constructor. The \nset P is equivalent to the set of all infinite binary trees with each node labeled with an element of \nP( Coast). This set can be formally defined aa the 6e.t of total functions f : {dm,rng}* + P(Const), \nend the rest of the development can be adapted mutan-dis mutotis (161. For clarity, we present our results \nusing the more intuitive notation instead. We use the functions const : D -P( Con&#38;) and dom, wag \n: 2) -2) to extract the respective components of an element of 2). We order the elements of D according \nto a relation that is contravariant in the argument component, since the information about argument values \nat an applica-tion needs to flow backwanl along data-flow paths to the formal parameter of the corresponding \nfunction defini-tions. Thus (Cl, D1, RI) L (C2, DI, Rz) if and only if Cl G CZ, DZ C DI, and RI E R2. \nThe set 2, forms a complete lattice under this ordering, with top and bottom elements being the solutions \nto the equations T = (Co&#38;, I, T) and I = (0, T, 1), respectively. The semantics of set expressions \nis defined with re-spect to a set environment p, which maps each set vaxi- able to an element of V. We \nextend the domain of set environments from set variables to set expressions in the natural manner: p \n: SetExp - D P(C) = ((4, T, 4 ddom(T)) = dam (~(4) Phd~)) = rtlg (P(T)) An environment p satisfies a \nconstraint C = [TI 5 Q] (written p + C) if ~(7-1) 5 472). Similarly, p satisfies S, or p is a solution \nof S (written p j= S) if p /= C for each C E S. The solution space of a constraint system S is Soln(S) \n= {p 1 p b S}. A constraints set Sl entails S2 (written S1 + &#38;) iff SoIn E SoZn(&#38;), and Sl is \nobservably equivalent to &#38;, (written Sl Y &#38;) iffS1 /=&#38;and&#38;k&#38;. The restriction of \na solution space to a collection of variables E is: Soln(S) 1~ = {p I 3p E SoZn(S). Va E E. p(a) = p \n(a)} We extend the notion of restriction to entailment and observable equivalence of constraint systems: \n. If SoZn(&#38;) 1~ G Soln(S2) IE, then SI entails SZ with respect to E (written Sl t=~ &#38;). . If \nSl +E S2 and S2 FE Sl then that Sl and &#38; are observably equivalent with respect to E (written SIYE&#38;). \n 3.3 Deriving Constraints The specification phase of set-based analysis derives constraints on the sets \nof values that program expres-sions may assume. Following Aiken et al. and Palsberg and O Keefe, we formulate \nthis derivation as a subtype system [2, 161. The derivation proceeds in a syntax-directed manner according \nto the constraint derivation rules presented ru{x:a}~z:a,0 (vaf) rl-b:a,{b~a} (mist) rI-M:a,S (label) \nI ~M1:a,SU{a~I} Cabs)r k (X*s.M): a,SU {t 5 a, do+) 5 al, a2 < q(a)}  rk Mi :pi,Si (aPP) I I-(Ml M2) \n: a,&#38; US2 U {Pz I dom(Pl), q(A) 50) rkv:a,sv A = Vars(Sv) \\ (FV[rng(I )] U Label) ru{5-vA.ia,S;))~M:p,S \nr I- (let (2 V) M) : P,S li, is a substitution of fresh varsforA (ind)r u (z : VA. (a,&#38;)} I- z : \n$(a), $(Sv) Figure 2: Constraint derivation rules. in figure 2. Each rule infers a judgement of the form \nI I- M : cy,S, where the set variable context r maps the free variables of M either to set variables \nor constraint schemes (see below); a names the value set of M; and the constraint system S describes \nthe data flow rela- tionships of M, using a. The rules (uar) and (con&#38;) are straightforward. The \nrule (label) records the value set of a labeled expression in the appropriate label. The rule (abs) for \nfunctions records the function s tag, and also propagates values from the function s domain into its \nformal parameter and from the function s body into its range. The rule (app) for applications propagates \nvalues from the argu- ment expression into the domain of the applied function and from the range of that \nfunction into the result of the application expression. The rule (let) produces a constraint schema 0 \n= VA. (a, S) for polymorphic, let-bound values [20,2,23]. The set variable a names the result of the \nexpression, the constraint system S describes the data %ow rela- tionships of the expression, and the \nset A contains those internal set variables of the constraint system that must be duplicated at each \nreference to the let-bound vari-able via the rule (in&#38;). We use FV[rng(I )] to denote the free set \nvariables in the range of F. The free set vari- ables of a schema u = VA. (a,S) are those in S but not \nin A, and the free variables of a set variable is simply the set variable itself.  3.4 Set Based Analysis \nEvery constraint system admits the trivial solution pT where pT(a) = T, and T, = (Con.&#38;, T,, Td). \nSince T, represents the set of aIZ run-time values, this solution is highly approximate and utterly useless. \nFortunately, constraint systems typically yield many additional so-lutions that more accurately characterize \nthe value sets of program expressions. For example, consider the program P = (Xtz.z), which yields the \nconstraint system: {t 5 a~, dam(w) i az, a2 I rng(ap)} In addition to the trivial solution described \nabove, this constraint system admits a number of other solutions, including: Pl = {aP I+ ({q,-bQaz t-b \n1) pz = {a~ c-) ({t)J,T),a, -T) The solution pr more accurately describes the program s run-time value \nsets than pz. Yet these two solutions are incomparable under the ordering E (pointwise ex-tended to environments), \nsince it models the flow of values through a program, but does not rank environ-ments according to their \naccuracy. Therefore we introduce a second ordering E, on 2) that properly ranks environments according \nto their ac-curacy. This ordering is covariant in the domain posi-tion, i.e., (Cl, DI, RI) C, (Cz, D2, \nR2) if and only if Cl EC2,Dl LsD2, mdR1 LR2. Under this ordering, a constraint system S has both a maximal \nsolution (pT above) and a minimal solution. The minimal solution exists because the greatest lower bound \ntl, with respect to 5. of two solutions is also a solution [ll]. We use LeastSoln(S) to denote this least \nsolution, and define set-based analysis as the function that extracts the basic constants and function \ntags for each labeled expression from LeastSoZn(S). Definition 3.1. (sba) If 0 I- P : o,S, then: sba(P)(l) \n= wnst(LeastSoZn(S)(Z)) The solution sba(P) conservatively approximates the value sets for each labeled \nexpression. Theorem 3.2 (Correctness of &#38;a) IfP++*E[ V ] then V in da(P)(Z). This result follows \nfrom a subject reduction proof along the lines of Wright and Felleisen [22] and Pals-berg [15] and is \ncontained in a related report [S].  3.5 Computing the Least Solution To compute sba(P), we close the \nconstraint system for P under the rules 8 described in figure 3. Intuitively, 4 Observable Equivalence \nof Constraints Cl0 PlY Cl-Y (31) Q 5 -3(B) 05-i a I mg(Y) (92) don@) I Q Plr dam(r) 5 Q (93) a I mpm \nmgw I Y (YlY (s4) Q I do@) dam(P) I Y aSr (55) Figure 3: The inference rule system 8. these rules infer \nall the data flow paths in the pro-gram, and propagate values along those paths. Specif-ically, the rules \n(si), (ss), and (8s) propagate infor-mation about constants, function domains and func-tion ranges forward \nalong the data flow paths of the program. These data flow paths are described by con- straints of the \nform ,# 5 -y. The rule (ad) constructs the data flow paths from actual to formal parameters for each \nfunction call, and the rule (ss) similarly constructs data flow paths from function bodies to corresponding \ncall sites. We write S l-o C if S proves C via the rules 0, and use Q(S) to denote the closure of S under \n8, i.e., the set {C ] S l-o C}. MrSpidey uses a worklist algorithm to compute the closure of S under \n8 efficiently. The worklist keeps track of all eligible inference rules whose antecedents are in S but \nwhose consequent may not be in S. The algorithm repeatedly removes an inference rule from the worklist, \nadds its consequent to S, if necessary, and then adds to the worklist all inference rules that are made \neligible by the addition of that consequent. The process iterates until the worklist is empty, at which \npoint S is closed under 0. The complete algorithm can be found in an earlier technical report [7]. This \nclosure process propagates all information con-cerning the possible constants for labeled expressions \ninto constraints of the form c 5 1. Hence, we can infer sba(P) from 0(S) according to the following theorem. \nTheorem 3.3 If P E A and 0 I- P : a, S then: sbu(P)(Z) = {c 1 [c 5 I] E Q(S)} The traditional set-based \nanalysis we have just described has proven highly effective for programs of up to a cou- ple of thousand \nlines of code. Unfortunately, it is useless for larger programs due to its nature as a whole program \nanalysis and due to the size of the constraint systems it produces, which are quadratic in the size of \n(large) programs. Storing these constraint systems in memory is beyond the capabilities of most machines. \n To overcome this problem, we develop algorithms for simplifying constraints systems. Applying these \nsimpli- fication algorithms to each program component signifi-cantly reduces both the time and space \nrequired by the overall analysis. The following subsection shows that constraint sim-plification does \nnot affect the analysis results provided the simplified system is observably equivalent to the original \nsystem. Subsection 4.2 presents a complete proof-theoretic formulation of observable equivalence, and \nsubsection 4.3 exploits this formulation to develop an algorithm for deciding the observable equivalence \nof constraint systems. The insights provided by this de- velopment lead to the practical constraint simplification \nalgorithms of section 5. 4.1 Conditions on Constraint Simplification Let us consider a program P containing \na program com-ponent M. Suppose the constraint derivations for M concludes l? I- M : a,&#38;, where Si \nis the constraint system for M. Our goal is to replace Si by a simpler constraint system without changing \nh(P). Since the constraint derivation process is composi- tional, the constraint derivation for the entire \nprogram concludes 0 l- P : /I, SC U&#38;, where SC is the constraint system for the context surrounding \nM. The combined constraint system SC u SI describes the space of solu- tions for the entire program, \nwhich is the intersection of the two respective solution spaces: SoIn(Sc U SI) = SoZn(Sc)fl Soin and \nhence Soln(Si) describes at least all the properties of Si relevant to the analysis. However, SoZn(Si) \nmay describe solutions for set variables that are not relevant to the analysis of P. In particular, . \n&#38;a(P) only references the solutions for labels; and . the only interactions between SC and Si are \ndue to the set variables {a} u FV[mg(l?)]. Thus the only properties of Si relevant to the analysis is \nthe solution space for its external set variables E = LabeZu {a} u FV[mg(r)] For our original problem, \nthis means that we want a constraint system SZ whose solution space restricted to E is equivalent to \nthat of Sr restricted to E: SoZn(&#38;) IE = SoZn(S2) IE; or, with the notation from section 3, Sr and \nSZ are observably equivalent on E: Sl YE s2 We can translate this compaction idea into an addi- tional \nrule for the constraint derivation system: lT&#38; M:a,Sl Sl YE Sz where E = Label U FV[mg(r)] U {a} \n( ) r&#38; M:a,S2 This rule is admissible in that any derivation (denoted using IX) in the extended constraint \nderivation system produces information that is equivalent to the informa- tion produced by the original \nanalysis. Lemma 4.1 If 0 l-e P : a, S, then: ha(P)(Z) = const(LeastSoZn(S)(Z))  4.2 Proof-Theoretic \nCharacterization Since the new derivation rule (2) involves the semantic notion of observably equivalent \nconstraint systems, it cannot be used directly. To make this rule useful, we must first reformulate the \nobservable equivalence rela-tion as a syntactic proof system. The key properties of the observational \nequivalence relation are reflections of the properties of the ordering relation (CI) and the functions \ndom and mg, respec-tively. We can reify these properties into a syntactic proof system via the following \ninference rules: 71 I 7-7-5 72 (tmm,)   a<a Cd=) 71 L 72 Kl 5 K2 (compat)m&#38;1) I W(E2) dom(K2) \n< dom(Kr) where we restrict K to non-constant set expressions to avoid inferring useless tautologies: \nK ::= a 1 dam(K) 1 mg(K) Many of the inferred constraints lie outside of the orig- inal language of \nsimple constraints. The extended lan-guage of compound constraints is: C ::= C</clK</c While this proof \nsystem obviously captures the prop erties of E, it does not lend itself to an efficient imple Q 5 m3w \nPlc a 5 m5(K) a I dam(P) PLK a 2 dom(tc) (compo=3) a > dam(P) 0 5 K (wmw=4) a 1 dam(K) 71 I c-t a I \nT2 71 I 3 61 SK2 W(Q) I W(~2) dmh) I domh) Figure 4: The inference rule system Q. mentation. Specifically, \nchecking if two potential an-tecedents of (trmv ) contain the same set expression T involves comparing \ntwo potentially large set expres-sions. Hence we use an alternative proof system that can easily be implemented, \nyet infers the same con-straints as the above. The alternative system consists of the inference rules \n!I! described in Figure 4, together with the rules 0 from Figure 3. The rules (composeI...l) replace \na reference to a set variable by an upper or lower (non-constant) bound for that variable, as appropriate. \nThe rule (trans) of @ provides a weaker characteriza-tion of transitivity than the previous rule (tmns \n), but the additional rules compensate for this weakness. The proof system 8 U !J! is sound and complete \nin that it infers all true compound constraints. Lemma 4.2 (Soundness and Completeness of 0s) For a simple \nconstraint system S and compound con-straint C, S l-oe C if and only ifs + C. This lemma implies that \n* o(S), which denotes the closure of S with respect to @Uq, contains exactly those (compound) constraints \nthat hold in all environments in SoZn(S). For a collection of external set variables E, W(S) 1~ contains \nall (compound) constraints that hold in all environments in SoZn(S) 1~. Lemma 4.3 s EE @e(s) IE. We could \nuse this result to define a proof-theoretic equivalent of restricted entailment as follows: sl I&#38; \ns2 8 *e(&#38;) IE > *e(&#38;) IE and then show that Si I-&#38; Sz if and only if S1 FE S2. However, \na variant of the above definition yields a relation that is easier to compute. Specifically, suppose \nW(Sr) ]E contains the constraint [mg(Tl) 5 mg(Q)] inferred by (compat). Then, since V~s(ri)U Vars(~2) \nE E, the corresponding antecedent [rr I 721 is also in W(S) ]E, and therefore: q@(s) IE \\ {mg(Tl) 5 md72)) \nzE q@(s) IE Put because does eliminate variables, (compat)-consequent qQ(S) is by antecedent. we II \nQJ {compat} this implies W(S) ZE ]E. we the lemma.  4.4 YE IE. lemmas and provide basis introduce equivalents \nrestricted tailment observable . sl k&#38; s2 iff I@ IE > n@(s2) (ET . =&#38; iff l-go and I-&#38; The \nrelations characterize en-and equivalence. Theorem (Soundness Completeness) S1 S2 and if /=E 2. =&#38; \nif only Si Ss.  4.3 Deciding Observable Equivalence The relation =&#38; completely characterizes the \nmodel- theoretic observable equivalence relation %E, but for an implementation of the extended constraint \nderivation system we need a decision algorithm for =&#38;. Given Si and Sz closed under 8, this algorithm \nneeds to verify that \\k(Si) ]E = g(S2) ]E. The naive approach to enumerate and to compare the two con-straint \nsystems does not work, since they are infinite. For example, if S = {a 5 rng(a)}, then Q(S) is the infinite \nset {a 5 mg(a), a 5 rng(rng(a)), . . .}. Fortunately, the infinite constraint systems inferred by k exhibit \na regular structure, which we exploit to decide observable equivalence as follows. First, we gen- erate \nregular grammars describing the upper and lower bounds for each set variable. Second, we extend these \ngrammars to regular tree grammars (RTGs) describ-ing oZZ constraints in II(&#38;) ]E and II(&#38;) ]E, \nexcluding those constraints inferred via compat, which we cannot describe in this manner. Third, we use \nthese RTGs to decide entailment by checking if 9(&#38;) ]E 2 II(&#38;) ]E via an adaptation of an RTG \ncontainment algorithm. To decide observable equivalence, we simply check en-tailment in both directions. \nThese steps are described in more detail below. Regular Grammars: Our first step is to describe the lower \nand upper non-constant bounds for each set variable. Technically, we want to describe the following two \nlanguages of types: {K 1 [tc 5 a] E Q(S) and Vars(n) C E} {K ( [a 5 K] E rSr(S) and Vurs(n) c E} for \neach set variable a. Both languages are generated by a regular grammar G,(S,E). The grammar con-tains \nthe non-terminals au and aL, for each a in S, which generate the above lower and upper bounds of a, respectively. \nThe productions of the grammar are determined by S and +. To illustrate this idea, suppose S contains \n[a I mdP)l. Th en, for each upper bound n of p, the rule (compose,) infers the upper bound mg(K) of a. \nSince, by induction, j3 s upper bounds are generated by &#38;, the production au H rng&#38;) generates \nthe corre-sponding upper bounds of a. More generally, the collec- tion of productions {au I-+ rng(Pu) \n] [a 5 rng@)] E S} describes all bounds inferred via (compose,). Bounds inferred via the remaining (compose) \nrules can be de- scribed in a similar manner. Bounds inferred via the rule (r@ez) imply the pro- duction \nrules au I+ a, aL H a for a E E. The rule (compat) cannot generate constraints of the form [K 5 a] or \n[a 5 K]. Finally, consider the rule (tmna), and suppose this rule infers an upper bound r on a. This \nbound must be inferred from an upper bound r on /3, based on the antecedent [a 5 p]. Hence the pro- ductions \n{au I+ /3~ ] [a 2 /3] E S} generate all upper bounds inferred via (tmns). In a similar fashion, the productions \n(0~ w a~ ] [a 5 p] E S} generate all lower bounds inferred via (tmns). Definition 4.6. (Regular Grammar \nG,(S,E)) Let S be a simple constraint system and E a collection of set variables. The regular grammar \nG,(S, E) consists of the non-terminals {aL,au I a E Vara(S)} and the following productions: au ++ a, \naL I-# a VacE w-Pu,P~++m V [a I PI E S au ++ dom(PL) V [a 5 dam(p)] E S   au t+ rg(Pu) V [a L wdP)l \nE S PL cf dom(au) V [dam(a) < p] E S PL H 9daL) V k-x(a) 5 PI E S I The grammar G,(S, E) describes two \nlanguages for each set variable: the upper and lower non-constant bounds. Specifically, if I+; denotes \na derivation in the grammar G, and Cc(z) denotes the language (7 1 z I-+: 7) generated by a non-terminal \nz, then the following lemma holds. Lemma 4.7 I G = G,(S, E), then: LG(aL) = {K 1 [K 5 a] E 9(S) and \nVurs(ti.) C E} &#38;(aU) = {K 1 [a 5 K] E !k(S) and Vars(~) C E} Proof: We prove each containment relation \nby induc- tion on the appropriate derivation. I Regular Tree Grammars: The grammar G,(S,E) does not describe \nall constraints in II(S) 1~. In partic- ular, it does not describe constraints of the form [c 5 71 and \nconstraints inferred by (trans) or (corn@). To represent the constraint system n(S) IE, we extend the \ngrammar G,(S, E) to a regular tree grammar Gt(S, E). It combines upper and lower bounds for set variables \nin the same fashion as the (trans) rule, and also generates constraints of the form [c 5 T] where appropriate. \nDefinition 4.8. (Regular Ike Grammar Gt (S, E)) The RTG Gt(S, E) extends the grammar G,(S, E) with the \nroot non-terminal Rand the additional productions: RI-+ [a Iwl V a E h-s(S) R ++ [c I WI v [c I a] E \ns where (. 5 -1 is viewed as a binary constructor. I The grammar Gt(S,E) describes all constraints in \nnI(s) iE* Lemma 4.9 If G = Gt(S,E), then II(S) 1~ = &#38;(R). Before we can exploit the grammar representation \nof II(S) IE, we must still prove that the closure under 8 U IIU (cornpot) can be performed in a sequential \nmanner. The following lemma justifies this staging of the closure algorithm. Lemma 4.10 For any simple \nconstraint system S: W(S) = XB(e(S)) = compat(II(0(S))) The Entailment Algorithm: We can check entail- \nment based on lemmas 4.9 and 4.10 as follows. Given SI and &#38;, we close them under 0 and then have: \nsz +Te Sl w QQ(S2) IE 2 l-w&#38;) IE by defn I-&#38; w !J!(Q(&#38;)) IE 2 lI(0(&#38;)) IE by lemma 4.10 \nq(s2) (E 2 n(h) IE as sj = Q(Sj) z comput(II(S2) 1~) 2 II(&#38;) 1~ by lemma 4.10 e=s compat(&#38;,(R)) \n2 CG, (R) by kmma 4.9 where Gi = Gt(&#38;, E) The Entailment Algorithm In the following, Pn, denotes \nthe finite power-set constructor. Let: G1 = G,(&#38;,E) Li = {CXL 1 a E VCte(Si)} G2 = Gt(S2,E) Uj = \n{au 1 a E VOW(&#38;)} Assume G l and Gz are pre-processed to remove e-transitions. For C E Pfim(L2 x \nCr2), define: L(C) = th 5 ml I (aL,PcJ) E c, QL -cJn CT,, Pv -cs TU) The relation 7&#38;z, ,s2 [., ., \n+, ~1 is defined as the largest relation on Ll x Ul x PII.(Lz x U2) x Pfipr,.(L2x U2) such that if: Rs, \n$1 bL 9 Pu 9 c, Dl aL - GI x PrJ -0, y then one of the following cases hold: 1. L([X 5 yl) c qc u D). \n 2. X = mg(+), Y = m&#38; ;) and %l,~,[a L,P;,C,D l, where: D'={(ri,6;) I (TL,&#38;)ECUD, -rL -cz mg(r;hb \n-CT2 mg(at)) 3. X = dom(a;), Y = dam(q) and R~,,s~[p~,c$,,C, D ], where: The computable entailment ndation \nS2 I-&#38; Sl holds if and only if Vfx E Vars(S1): Figure 5: The computable entailment relation kie The \ncontainment question &#38;G,(R) > t&#38;(R) can be decided via an RTG containment algorithm. To decide \nthe more difficult question: we adapt an RTG containment algorithm to allow for constraints inferred \nvia (cornput) on L&#38;(R). The extended algorithm is presented in Figure 5. It first computes the largest \nrelation 7Es,,s2 such that 72~~ ,sz [CEL, &#38;J, C, D] holds if and only if: Nm 5 Pull G =wW(C)) U W) \nwhere a~, /3~ describe collections of types; C, D de- scribe collections of constraints; and L([a, 5 \n&#38;J]) de- notes the language {[TL 2 7~1 I a~ H* TL, &#38;J H* 7~). The first case in the definition \nof 72 uses an R!I G con- tainment algorithm to detect if L&#38;L 5 &#38;J]) E L(C) U C(D). The two remaining \ncases handle constraints of the form [rng(ai) 5 mg(&#38;)] or [dom(a;) < don@)], and allow for inferences \nvia (compat). The relation R can be computed by starting with a maximal relation (true at every point), \nand then iteratively setting en-tries to false as required by figure 5, until the largest relation satisfying \nthe definition is reached. Based on this relation, the algorithm then defines a computable entailment \nrelation l-5s on constraint sys-tems. This relation is equivalent to l-5. Theorem 4.11 &#38; l-g 2% if \nand only if S-Z I-&#38; SI. The entailment algorithm takes exponential time, since the size of R is exponential \nin the number of set variables in Ss. Although faster algorithms for the entailment may exist, these \nalgorithms must all be in PSPACE, because the containment problem on NFA s, which is PSPACEcomplete [l], \ncan be polynomially re-duced to the entailment problem on constraint systems. By using the entailment \nalgorithm in both direc-tions, we can now decide if two constraint systems are observable equivalent. \nThus, given a constraint system, we can find a minimal, observably equivalent system by systematically \ngenerating a11 constraint systems in order of increasing size, until we find one observably equivalent \nto the original system. Of course, the pro-cess of computing the minimal equivalent system with this \nalgorithm is far too expensive for use in practical program analysis systems. Practical Constraint Simplification \n Fortunately, to take advantage of the rule (Z) in a pro- gram analysis tool, we do not need a completely \nmini-mized constraint system. Any simplifications in a con- straint system produces corresponding reductions \nin the overall analysis time. For this purpose, we exploit the connection between constraint systems \nand RTGs. By Lemmas 4.4 and 4.9, any transformation on constraint systems that preserves the language: \nalso preserves the observable behavior of S with respect to E. Based on this observation, we transform \na vari- ety of existing algorithms for simplifying RTGs to al-gorithms for simplifying constraint systems. \nIn the fol- lowing subsections, we present the four most promising algorithms found so far. We use G \nto denote Gt(S,E), and we let X range over non-terminals and p over paths, which are sequences of the \nconstructors dom and rg. Each algorithm assumes that the constraint system S is closed under 0. Computing \nthis closure corresponds to propagating data flow information locally within a program component. This \nstep is relatively cheap, since program components are typically small (less than a few thousand lines \nof code). 5.1 Empty Constraint Simplification A non-terminal X is empty if &#38;(X) = 0. Similarly, a \nproduction is empty if it refers to empty non-terminals, and a constraint is empty if it only induces \nempty pro-ductions. Since empty productions have no effect on the language generated by G, an empty constraint \nin S can be deleted without changing S s observable behavior. To illustrate this idea, consider the program \ncompo-nent P = (Xgy.((Xfz.l) y)), where f and g are function tags. Although this example is unrealistic, \nit illustrates the behavior of our simplification algorithms. Analyz-ing P according to the constraint \nderivation rules yields a system S containing ten constraints. Closing S un-der 0 yields an additional \nthree constraints. Figure 6 displays the resulting constraint system O(S), together with the corresponding \ngrammar Gt(S(S), {aP}). An inspection of this grammar shows that the set of non- empty non-terminals \nis: Five of the constraints in 0(S) are empty, and are re-moved by this simplification algorithm, yielding \na sim- plified system of eight non-empty constraints. 5.2 Unreachable Constraint Simplification A non-terminal \nX is unreachable if there is no pro-duction R H [y 5 Z] or R H [Z 5 Y] such that &#38;(Y) # 0 and 2 -+z \np(X). Similarly, a production is unreachable if it refers to unreachable non-terminals, and a constraint \nis unreachable if it only induces un-reachable productions. Unreachable productions have no effect on \nthe language &#38;(R), and hence unreach-able constraints in S can be deleted without changing the observable \nbehavior of S. In the above example, the reachable non-terminals are al~, a (~ and agr~. Three of the \nconstraints are un- reachable, and are removed by this algorithm, yielding a simplified system with five \nreachable constraints.  5.3 Removing +Constraints A constraint of the form [a 2 /3] E S is an c-cowhint. \nSuppose a fZ E and the only upper bound on a in S is the c-constraint [a 5 p], i.e., there are no other \ncon-straints of the form a 5 7, rng(a) 5 7, or 7 5 dam(a) in S. Then, for any solution p of S, the set \nenvironment Constraints f 5 Qf dom(af) 2 az 1 5 or o1 5 mg(af) mg(of) 5 aa au 5 or or < dom(af) 9 I \nQP dom(aP) 5 a* oa 5 mg(op) R a=L R 0 ~ @L aru a ~ R @L CPU Production w [f5a ul H dom(ofu) -P~~ Ul -mg(ofu) \n-mg(~fL) w ffvu I-+ dom(afn) -bI~pul H dom(aP~) ++ mg(oPu) Rules dL H Non-empty 1 5 or CYYL oy 5 0 9 \nI op dom(aP) 5 au aa 5 mg(aP) Reachable 1 5 al 9 I op CP 5 mg(op) a a1 1 < 5 5 02 aa 00 CPU al R H H \nw CPU aaU P<hl CPL aaL H H O L alL a a1 1 < 5 5 a2 o aa a1 1 5 5 aa aa aPL H ap aPu H ffp Figure 6: \nThe original constraint system, grammar and simplified constraint systems for P = (&#38; v.((xfz.l) 9)) \np defined by: p(6) if 6 f a PW = p(p) if 6 =CY { is also a solution of S. Therefore we can replace all \noccurrences of a in S by p while still preserving the ob- servable behavior S&#38;r(S) 1~. This substitution \ntrans-forms the constraint [CY5 p] to the tautology [/3 5 /?I, which can be deleted. Dually, if [(Y 5 \n/3] E S with ,f3 $Z E and /3 having no other lower bounds, then we can re-place /3 by Q, again eliminating \nthe constraint [o 5 /3]. To illustrate this idea, consider the remaining con-straints for P. In this \nsystem, the only upper bound for the set variable a is the e-constraint [o 5 a ]. Hence this algorithm \nreplaces all occurrences of cxl by (Y , which further simplifies this constraint system into: (15 (Y \n,oo < mg(crP),g 5 aP} This system is the smallest simple constraint system observably equivalent to \nthe original system O(S). 5.4 Hopcroft s Algorithm The previous algorithm merges set variables under \ncer-tain circumstances, and only when they are related by an e-constraint. We would like to identify \nmore general circumstances under which set variables can be merged. To this end, we define a valid unifier \nfor S to be an equivalence relation -on the set variables of S such that we can merge the set variables \nin each equivalence class of -without changing the observable behavior of S. Using a model-theoretic \nargument, we can show that an equivalence relation -is a valid unifier for S if 1. Use a variant of Hopcroft \ns algorithm [12] to compute an equivalence relation N on the set variables of S that satisfies the following \nconditions: (a) Each set variable in E is in an equivalence class by itself. (b) If [a 5 P] E S then \nVa m a 30 N @ such that  [a 5 P ] E S. (c) If [a < mg(/3)] E S then Va N a 3p N p such that [a I mg(P \n)l E S. (d) If [rng(a) < S] E S then Va N a $3 N p such that [mda ) I P l E S. (e) If [a 5 dom(/3)] E \nS then Va N a VP N 0 such that [a < dam(@)] E S. 2. Merge set variables according to their equivalence \nclass. Figure 7: The Hopcroft algorithm for all solutions p E S&#38;r(S) there exists another solu-tion \np E Soln(S) such that p agrees with p on E and p (a) = p (p) for all (Y - p. A natural strategy for generating \np from p is to map each set variable to the least upper bound of the set variables in its equivalence \nclass: P (4 = Ll Pb ) a -0 Figure 7 describes sufficient conditions to ensure that p is a solution of \nS, and hence that -is a valid unifier for S. To produce an equivalence relation satisfying these conditions, \nwe use a variant of Hopcroft s O(n lg n) time algorithm [12] for computing an equivalence relation on \nstates in a DFA and then merge set variables according to their equivalence class6  5.5 Simplification \nBenchmarks To test the effectiveness of the simplification algorithms, we extended MrSpidey with the \nfour algorithms that we have just described: empty, unreachable, e-removal, and Hopcroft. Each algorithm \nalso implements the pre- ceding simplification strategies. The first three algo-rithms are linear in \nthe number of non-empty constraints in the system, and Hopcroft is log-linear. We tested the algorithms \non the constraint systems for nine program components on a 167MHz Spare Ul-tra 1 with 326M of memory, \nusing the MzScheme byte code compiler [lo]. The results are described in fig- ure 8. The second column \ngives the number of lines in each program component, and the third column gives the number of constraints \nin the original (unsimplified) constraint system after closing it under the rules 0. The remaining columns \ndescribe the behavior of each simpli- fication algorithm, presenting the factor by which the number of \nconstraints was reduced, and the time (in milliseconds) required for this simplification. The results \ndemonstrate the effectiveness and effi-ciency of our simplification algorithms. The resulting constraint \nsystems are typically at least an order of magnitude smaller than the original system. The cost of these \nalgorithms is reasonable, particularly consid-ering that they were run on a byte code compiler. As expected, \nthe more sophisticated algorithms are more effective, but are also more expensive. 6 Componential Set-Based \nAnalysis Equipped with the simplification algorithms, we return to our original problem of developing \na componential set-based analysis. The new analysis tool processes pro grams in three steps. 1. For each \ncomponent in the program, the analy-sis derives and simplifies the constraint system for that component \nand saves the simplified system in a constraint file, for use in later runs of the analy- sis. The simplification \nis performed with respect to the external variables of the component, excluding expression labels, in \norder to minimize the size of the simplified system. Thus, the simplified system A similar development \nbased on the definition p (a) = W(a ) I 0 -0 1 results in an alternative algorithm, which is less effective \nin practice. only needs to describe how the component inter-acts with the rest of the program, and the \nsimpli- fication algorithm can discard constraints that are only necessary to infer local value set invariants. \nThese discarded constraints are reconstructed later as needed. This step can be skipped for each program \ncom-ponent that has not changed since the last run of the analysis, since its constraint file can be \nused instead. The analysis combines the simplified constraint systems of the entire program and closes \nthe com-bined collection of constraints under 8, thus prop- agating data flow information between the \ncon-straint systems for the various program compo nents. Finally, to reconstruct the full analysis results \nfor the program component that the programmer is focusing on, the analysis tool combines the con-straint \nsystem from the second step with the un-simplified constraint system for that component. It closes the \nresulting system under 8, which yields appropriate value set invariants for each labeled expression in \nthe component. The new analysis can easily process programs that consist of many components. For its \nfirst step, it elim- inates all those constraints that have only local rele-vance, thus producing a small \ncombined constraint sys-tem for the entire program. As a result, the analysis tool can solve the combined \nsystem more quickly and using less space than traditional set-based analysis [ll]. Finally, it recreates \nas much precision as traditional set-based analysis as needed on a per-component basis. The new analysis \nperforms extremely in an inter-active setting because it exploits the saved constraint files where possible \nand thus avoids reprocessing many program components unnecessarily. We implemented four variants of this \nanalysis. Each analysis uses a particular simplification algorithm to simplify the constraint systems \nfor the program com-ponents. 6.1 Benchmarks We tested the componential analyses with five bench- mark \nprograms, ranging from 1,200 to 17,000 lines. For comparison purposes, we also analyzed each benchmark \nwith the standard set-based analysis that performs no simplification. The analyses handled library functions \nin a context-sensitive, polymorphic manner according to the constraint derivation rules (let) and (inst) \nto avoid merging information between unrelated calls to these em Y unren vol HOP ,ft -n Definition lines \nsize Factor time factor time +-factor time 5 221 3 <lO 6 30 13 30mP reverse 6 287 4 <IO a 10 20 30 substring \n8 579 12 10 64 10 96 20 qsort 41 1387 15 <lO 15 50 66 unify 89 2921 10 10 11 120 65 150 hopcroit 201 \n8429 25 10 42 100 124 200 check 237 21854 4 50 4 370 168 510 escher-iish 493 30509 187 10 678 40 678 \n80 scanner 1209 59215 3 180 17 2450 57 2120 Figure 8: Behavior of the constraint simplification algorithms. \nfunctions. The remaining functions were analyzed in a context-insensitive, monomorphic manner. The results \nFile Program size are documented in figure 9. I# lines) Analvsis @Y-) The third column in the figure \nshows the maximum scauner stondanl 572K size of the constraint system generated by each analysis, (1253) \n189K empty and also shows this size as a percentage of the constraint unreachable 39K system generated \nby the standard analysis. The analy- t-removal 28K ses based on the simplification algorithms produce \nsig-Hopcroft 25K nificantly smaller constraint systems, and can also an-zodiac standard 1634K alyze more \nprograms, such as sba and poly, for which (3419) empty 62K (9%) I 34.1 I 8.1 328K unreachable 21K (3%) \n28.8 4.5 169K the standard analysis exhausted heap space. e-removal 13K (2%) 28.8 3.8 147K The fourth \ncolumn shows the time required to ana-  Hopcroft 136K lyze each program from scratch, without using \nany ex- nucleic standard 2882K isting constraint files. The analyses that exploit con- (3432) empty \n90K (27%) 52.8 17.8 592K straint simplification yield significant speed-ups over unreachable 68K (20%) \n48.4 14.6 386K the standard analysis because they manipulate much c-removal 56K (1%) 48.3 13.1 330K \nsmaller constraint systems. The results indicate that, Hopcroft 56K (1%) 60.9 13.2 328K for these benchmarks, \nthe wemoval algorithm yields sba standard >5M . . . *    -l---L the best trade-off between efficiency \nand effectiveness (11560) ewtv 65.5 1351K of the simplification algorithms. The additional sim-unreachable \n43.3 920K e-removal 42.2 770K plification performed by the more expensive Hopcrojt Hopcroft ;;; (<;%ji \n1781 41.1 716K algorithm is out-weighed by the overhead of running PlY standard . the algorithm. The \ntradeoff may change as we analyze (17661) empty >5M . . . . larger programs. unreachable Ll201K (<4%) \n259.6 .... 1517K To test the responsiveness of the componential anal- t-removal 68K (<l%) 239.6 13.3 \n1038K yses in an interactive setting based on an analyze-debug- Hopcroft 38K (<l%) 254.1 10.9 907K edit \ncycle, we reanalyzed each benchmark after chang- * indicates the analysis exhausted heap space ing a \nrandomly chosen component in that benchmark. Figure 9: Behavior of the componential analyses. The re-analysis \ntimes are shown in the fifth column of figure 9. These times show an order-of-magnitude im-provement \nin analysis times over the original, standard analysis, since the saved constraint files are used to \nmodified, e.g., when a bug is identified and eliminated, avoid reanalyzing all of the unchanged program \ncom-using separate analysis substantially improves the us- ponents. For example, the analysis of zodiac, \nwhich ability of MrSpidey. used to take over two minutes, now completes in un-The disk-space required \nto store the constraint files der four seconds. Since practical debugging sessions is shown in column \nsix. Even though these files use using MrSpidey typically involve repeatedly analyzing a straight-forward, \ntext-based representation, their size the project each time the source code of one module is is typically \nwithin a factor of two or three of the corre sponding source file. These times exclude scanning and parsing \ntime. Relative time of smart oolvmorohic an~vses Mono. Program ewtv unreachable e-removal Hopcroftanalysis \n39% 36% 35% 38% 42% browse 233 2.5s 76% 76% 76% 81% 75% splay II WPY 265 7.9s 75% 73% 70% 72% 83% check \n281 50.1s 21% 23% 14% 14% 23% graphs 621 2.8s 85% 85% 62% 87% 82% boyer 624 4.3s 46% 46% 49% 50% 40% \nmatrix 744 7.5s 64% 57% 51% 52% 45% maze 857 6.2s 64% 59% 58% 61% 54% nbody 880 39.6s 57% 25% 25% 26% \n28% nucleic 3335 * . 243s * 42s . 42s . 44s . 36s * indicates the copy analysis exhausted heap space, \nand the table contains absolute times for the other analyses Figure 10: Times for the smart polymorphic \nanalyses, relative to the copy analysis. Efficient Polymorphic Analysis The constraint simplification \nalgorithms also enables an efficient polymorphic, or context-sensitive, analysis. To avoid merging information \nbetween unrelated calls to functions that are used in a polymorphic fashion, a poly- morphic analysis \nduplicates the function s constraints at each call site. We extended MrSpidey with five poly- morphic \nanalyses. The first analysis is copy, which duplicates the constraint system for each polymorphic reference \nvia a straightforward implementation of the rules (let) and (in&#38;).* The remaining four analyses are \nsmaft analyses that simplify the constraint system for each polymorphic definition. We tested the analyses \nusing a standard set of bench- marks (131. The results of the test runs are documented in figure 10. \nThe second column shows the number of lines in each benchmark; the third column presents the time for \nthe copy analysis; and columns four to seven show the times for each smart polymorphic analysis, as a \npercentage of the copy analysis time. For comparison purposes, the last column shows the relative time \nof the original, but less accurate, monomorphic analysis. The results again demonstrate the effectiveness \nof our constraint simplification algorithms. The smart analyses that exploit constraint simplification \nare al-ways significantly faster and can analyze more programs than the copy analysis. For example, while \ncopy ex-hausts heap space on the nucleic benchmark, all smart analyses successfully analyzed this benchmark. \nAgain, it appears that the c-removal analysis yields the best trade-off between efficiency and effectiveness \nof the simplification algorithms. This analysis provides the additional accuracy of polymorphism without \nmuch we 8h30 implemented a polymorphic analysis that re-analyzes 8 definition 8t each reference, but \nfound its performance to be compa- rable to, and sometimes worse than, the copy analysis. additional \ncost over the coarse, monomorphic analysis. With the exception of the benchmarks browse, splay and graphs, \nwhich do not re-use many functions in a polymorphic fashion, this analysis is a factor of 2 to 4 times \nfaster than the copy analysis, and it is also capable of analyzing larger programs. 8 Competitive Work \nFiihndrich and Aiken [S] examine constraint simplifi-cation for an analysis based on a more complex con-straint \nlanguage. They develop a number of heuristic algorithms for constraint simplification, which they test \non programs of up to 6000 lines. Their fastest approach yields a factor of 3 saving in both time and \nspace, but is slow in absolute times compared to other analyses. Pottier [17] studies an ML-style language \nwith a sub- type system based on constraints, and and presents an incomplete algorithm for deciding entailment \non con-straint systems. He proposes some ad hoc algorithms for simplifying constraints, but does not \npresent results on the cost or effectiveness of these algorithms. Eifrig, Smith and Trifonov [S, 211 \ndescribe a subtyp- ing relation between constrained types that are simi-lar to our constraint systems, \nand they present an in-complete decision algorithm for subtyping. They de-scribe three algorithms for \nsimplifying constraint sys-tems, two of which which are similar to the empty and .+removal algorithms, \nand the third is a special case of the Hopcroft algorithm. They do not present results on the cost or \neffectiveness of these algorithms. Duesterwald et al [4] describe algorithms for simpli- fying data flow \nequations. These algorithms are similar to the e-removal and Hopcrojt algorithms. Their ap-proach only \npreserves the greatest solution of the equa- tion system and assumes that the control flow graph is known. \nit be to programs a manner to pro- with contro l-flow such first-class and methods. paper not results \nthe or of algorithms.  Future Work All our constraint simplification algorithms preserve the observable \nbehavior of constraint systems, and thus do not affect the accuracy of the analysis. If we were willing \nto tolerate a less accurate analysis, we could choose a compressed constraint system that does not preserve \nthe observable behavior of the original, but only entails that behavior. This approach allows the use \nof much smaller constraint systems, and hence yields a faster analysis. A promising approach for deriving \nsuch approximate constraint systems is to rely on a programmer-provided sign&#38;we describing the behavior \nof each program com-ponent, and to derive the new constraint system from that signature. After checking \nthe entailment condi-tion to verify that signature-based constraints correctly approximates the behavior \nof the module, we could use those constraints in the remainder of the analysis. Since the signature-based \nconstraints are smaller than the derived ones, this approach could significantly reduce analysis times \nfor large projects. We are investigating this approach for developing a typed module language on top \nof Scheme.  References [I] AHO, A., 3. HOPCROFT AND J. ULLMAN. The Design and Analysis of Computer Algorithms. \nAddison-Wesley, Read-ing, Mass., 1974. [2] AIKEN, A., WIMMERS, E. L., AND LAKSHMAN, T. K. Soft typing \nwith conditional types. In Pmcecdings of the ACM Sigplan Conference on Principle8 of Progmmming Lan-guages \n(1994), pp. 163-173. [3]COLJSOT, P., AND COUSOT, R. Formal language, grammar, and set-constraint-based \nprogram analysis by abstract inter-pretation. In Pmccedings of the 1995 Conference on Pnnc-tionalPrvgmmming \nand Architecture Computer (1995), pp. 170-181. [4] DUESTERWALD, E., GUPTA, R., AND SOFFA, M. L. Reducing \nthe cost of data flow analysis by congruence partitioning. In International Conference on Compiler Construction \n(April 1994). [5] EIFRIG, J., SMITH, S., AND TRIFONOV, V. Sound polymor-phic type inference for objects. \nIn Conference on Object-Oriented Prvgmmming Systems, Languages, and Applicn- tions (1995). [6] F~HNDRICH, \nM., AND AIKEN, A. Making set-constraint based program analyses scale. Technical Report UCB/CSD-96-917, \nUniversity of California at Berkeley, 1996. [7] FLANAGAN, C., AND FELLEISEN, M. Set-based analysis for \nfull Scheme and its use in soft-typing. Technical Report TR95-254, Rice University, 1995. [S] FLANAGAN, \nC., AND FELLEISEN, M. Modular and polymor- phic set-based analysis: Theory and practice. Technical Re-port \nT&#38;96-266, Rice University, 1996. [9] FLANAGAN, C., FLATT, M., KRISHNAMURTHI, S., WEIRICH, S., AND \nFELLEISEN, M. Finding bugs in the web of pro-gram invariants. In Pmceedings of the ACM Conference on \nProgramming Language Design and Implementation (1996), pp. 23-32. (lo] FLATT, M. &#38;Scheme Reference \nManual. Rice University. [ll] HEINTZE, N. Set-based analysis of ML programs. In Pru-ceedings of the ACM \nConference on Lisp and Jbnctional Pmgmmming (1994), pp. 306317. [12] HOPCROFT, J. E. An n log n algorithm \nfor minimizing the states of a finite automaton. The Theory of Machines and Computations (1971), 189-196. \n[13] JAGANNATHAN, S., AND WRIGHT, A. K. Effective flow analy- sis for avoiding run-time checks. In Pmt. \n2nd International Static Analysis Symposium, LNCS 983 (September 1995), Springer-Verlag, pp. 207-224. \n[14] JONES, N., AND MUCHNICK, S. A flexible approach to inter- procedural data flow analysis and programs \nwith recursive data structures. In Conference Reconl of the NinthAnnual ACM Symposium on Principles of \nPrvgnamming Languages (January 1982), pp. 66-74. [15] PALSBERG, J. Closure analysis in constraint form. \n%nsoe-tions on Programming Languages and Systems 17, 1 (1995), 47-62. [16]PALSBERG, J., AND O KEEFE, \nP. A type system equip lent to flow analysis. In Proceedings of the ACM SIGPLAN 95 Conference on Principles \nof Pnqmmming Languages (1995), pp. 367-378. [17] POTTIER, F. Simplifying subtyping constraints. In Proaed-inge \nof the 1996 ACM SIGPLAN International Conference on finctional Pmgmmming (1996), pp. 122-133. [18] REYNOLDS, \nJ. Automatic computation of data set defintious. Information Processing 68 (1969), 456-461. [19] SHIVERS, \n0. Control-flour Analysis of Higher-Order Lan-guages, or Taming Lambda. PhD thesis, CarnegieMellon University, \n1991. [20] TOFTE, M. Type inference for polymorphic references. In-formation and Computation 89, 1 (November \n1996), l-34. [21] TRIFONOV, V., AND SMITH, S. Subtyping constrained types. In Third International Static \nAndyrio Symposium (LNCS 11.15) (1996), pp. 349-365. [22] WRIGHT, A., AND FELLEISEN, M. A syntactic approach \nto type soundness. Information and Computation 115, 1 (1994), 38-94. [23] WRIGHT, A. K. Simple imperative \npolymorphism. Lisp and SynboticComputation 8, 4 (Dec. 1995), 343-356.  \n\t\t\t", "proc_id": "258915", "abstract": "Set based analysis is a constraint-based whole program analysis that is applicable to functional and object-oriented programming language. Unfortunately, the analysis is useless for large programs, since it generates descriptions of data flow relationships that grow quadratically in the size of the program.This paper presents componential set-based analysis, which is faster and handles larger programs without any loss of accuracy over set-based analysis. The design of the analysis exploits a number of theoretical results concerning constraint systems, including a completeness result and a decision algorithm concerning the observable equivalance of constraint systems. Experimental results validate the practically of the analysis.", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "Rice University", "person_id": "PP14187273", "email_address": "", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "Rice University", "person_id": "PP39037684", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258937", "year": "1997", "article_id": "258937", "conference": "PLDI", "title": "Componential set-based analysis", "url": "http://dl.acm.org/citation.cfm?id=258937"}