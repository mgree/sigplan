{"article_publication_date": "05-01-1997", "fulltext": "\n Two for the Price of One: Composing Partial Evaluation and Compilation Michael Sperber Peter Thiemann \nWilhelm-Schickard-Institut fiir Informatik Universitat Tiibingen Sand 13, D-72076 Tiibingen, Germany \n{sperber,thiemaun}@informatik.uni-tuebingen.de Abstract One of the flagship applications of partial evaluation \nis com- pilation and compiler generation. However, partial eval-uation is usually expressed as a source-to-source \ntransfor-mation for high-level languages, whereas realistic compilers produce object code. We close this \ngap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application \nof several meta-computation techniques to build the system, both in theory aud in practice. The composition \nis an application of deforestation or fusion. The result is a run-time code generation system built from \nexisting components. Its applications are numerous. For example, it allows the language designer to perform \ninterpreter-based experiments with a source-to-source ver-sion of the partial evaluator before building \na realistic com-piler which generates object code automatically. Keywords semantimdirected compiler generation, \npartial evaluation, compilation of higher-order functional lan-guages, run-time code generationy  Introduction \n Both partial evaluation and run-time code generation (RTCG) are program optimization techniques that \nhave re- ceived considerable attention in the language implementa- tion community. The literature describes \npartial evalua-tion as an optimizing program transformation based on con- stant propagation and memoization \nof generated procedures. Most partial evaluators perform a source-to-source transfor-mation. In RTCG, \non the other hand, the focus is on dynam- ically generating code fragments at run time and executing \nthem. Recently, researchers have begun to notice that par-tial evaluation terminology and techniques \nare suitable for specifying and implementing RTCG: Offline partial evalua-tion generates the output (or \nresidual) program by stitch- ing together prefabricated fragments of the source pro-gram, inserting constants \nin pre-determined places. Generic techniques for RTCG assemble pre-fabricated fragments of Permission \nto make digital/hard copy of part or all this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan- tage, the copyright \nnotice. the title of the publication and its date appear, and notice is given that copying is by permission \nof ACM.  Inc. To copy otherwise, to republish, to post on servers. or to object code (usually called \ntemplates) in a similar man-ner [9,10,39,40]. Since then, systems have been developed that implement \nRTCG with the help of partial evaluation tools [9, lo]. No-tably, the binding-time analysis, which is \na vital part of ev- ery of&#38;e partial evaluator, can automatically determine a proper staging of computations \nand thus guide the construc- tion of the object code templates. However, most implementations of partial \nevaluation sys- tems work at the source level; with these systems, to obtain object code on-the-fly, \nit is necessary to employ reflection through eval procedures or similar means, thereby incur-ring significant \ncompilation or interpretation overhead [SO]. Partial evaluation systems which directly generate object \ncode exist; however, their implementation usually involves hand-written code generation primitives [15,27]. \nThe imple- mentation of many RTCG systems is complex and requires ad-hoc modifications to existing compilers \nor rewriting of components from scratch. We describe how to obtain a portable partial evaluation system \nfor Scheme (511 which directly generates object code in an essentially automatic way, by composing a \npartial eval-uation system with a compiler. For the composition, no knowledge of the internal workings \nof the compiler (or even the nature of the output code) or the partial evaluator is necessary. We achieve \nthe composition itself automatically through a specialization technique-deforestation [63]. Our methods \nallow for the automatic generation of run-time code generation systems. Our system is modular by construction: \nIt is possible to modify the partial evaluation system without keeping in mind object code generation \nissues, and to replace the com- piler without any changes to the partial evaluation system or even glue \ncode (which is generated automatically.) In fact, in our implementation, compiler and partial evaluation \nsystem were developed independently of each other. As it turns out, the components of our system naturally \ncomplement each other: The partial evaluation system al-ready generates code in a subset of Scheme especially \nsuited for compilation and on-the-fly code generation. This allows us to use a simple compiler and to \ncircumvent several aual- ysis steps in the compiler of the underlying Scheme system. To summarize, the \ncontributions of our work are the fol- lowing: . We present a formalization of the specialization phase \nof an of&#38;e partial evaluator for a higher-order core language resembling Scheme. redistribute to \nfists, requires prior specific permission and/or a fee. PLDI 97 Las Vegas, NV. USA 0 1997 ACM O-89791 \n-907-6/97/0006...S3.50 We show how to convert a recursive-descent compiler automatically into a set of \ncode generation combina-tors. We show how to automatically compose the combina- tors with a partial evaluator \nusing deforestation. We have implemented our techniques in the frame-work of an existing partial evaluation \nsystem [59] and Scheme 48 [32], a byte-code implementation of Scheme. We have compiled realistic programs \nand run bench-marks that confirm the expected benefits of the system. Our work has a number of immediate \napplications: . For straightforward use in partial evaluation, our sys-tem avoids the compilation step \nfor residual programs, thus speeding up the turnaround cycle in experimental applications of partial \nevaluation. . The system facilitates the automatic construction of true compilers: It maps a language \ndescription (an in- terpreter) to a compiler that directly generates low-level object code. . Our system \nallows the creation and execution of cus- tomized code at run time, thereby performing some classic jobs \nof RTCG systems. a The system makes realistic incremental specialization feasible which not only allows \nfor the implementation of dynamically evolving programs, but can also avoid termination problems in partial \nevaluation [60]. Since our implementation is based on a byte-code lan-guage system, we have not addressed \nnative-code genera-tion issues such as register allocation or code optimization. We believe these issues \nto be largely orthogonal to our tech- nique. Overview Because the implementation of our system was surprisingly \nsimple, we give the history of the implementa- tion process in Sec. 2. Section 3 gives some background \non partial evaluation and program generator generators (PGG s). Section 4 introduces the concrete partial \nevalu-ation framework used in our current work. Section 5 de- scribes the theoretical background of our \nwork. In Sec. 6, we present the techniques we used in the implementation. Benchmarks are the subject \nof Sec. 7. Section 8, finally, gives an account of related work.  Run-Time Code Generation Made Easy \nThe script for our RTCG production turns out to be sur- prisingly simple. Cast Peter and Mike, two programmers. \nSetting Autumn 1996, a crowded university office in Tiibingen, Germany, containing two desks with work- \nstations. Each of the workstations has installed a language sys- tem, in this case Scheme 48 [32], and \na partial eval-uator [30], or, better, a program generator generator (PGG) [36,59], which works on the \nsame language sys- tem. The acts of the production correspond to the technical descriptions in Sections \n6.1-6.4.  2.1 Act 1: Write a Compiler Mike writes a simple compiler for the language system. The compiler \nneeds to handle the output language of the partial evaluation process. As Peter, the author of the PGG \ncan tell, that language happens to be in A-normal form (ANF), a small subset of Scheme especially suitable \nfor effective com- pilation. Mike chooses to chop down the stock Scheme 48 com-piler to a compiler for \nprograms in ANF and to introduce a few obvious optimizations possible through that. The new compiler \nuses exactly the same syntax dispatch and code generation conventions as the normal compiler, and is \nthere- fore seamlessly integrated with the base language system. In principle, this step can be avoided \ngiven a suitable ANF compiler. 2.2 Act 2: Annotate the Compiler Peter obtains the compiler from M&#38;e \nand decoratesit with binding-time annotations. Peter resists learning about the internal workings of \nthe compiler and the code-generation issues involvedeven though Mike assures him it is all very simple. \nHowever, Peter just needs to know where the com-piler generates code-not how. Peter merely prefixes code- \ngenerating expressions in the compiler by an operator that causes them to be delayed until code-generation \ntime. The syntax dispatch remains unchanged. Even when Peter is finished, he has no idea how the code \nworks. Neither does Mike, poring over the code, understand the subtleties of Pe-ter s annotations. 2.3 \nAct 3: Implement the Annotations Peter writes macros that will simply ignore the annotations, so the \nresult is still usable as an ordinary compiler. He writes a second set of macros which turn the compiler \nfunctions into combinators. These combinators will replace counter-parts in the PGG normally responsible \nfor producing output code in the source language. The new combinators directly produce object code. Both \njobs turn out to be reasonably straightforward.  2.4 Act 4: Test and Debug Peter and Mike get together \nat a terminal to hook up the PGG with the new code generation combmators. Tension is high because the \ncomponents have been developed sepa- rately, have not been tried in combination, debugging sup port at \nthe object code level is minimal, and the paper dead- line is approaching. Will they make it? Right off, \nPeter and Mike find a number of embarrassing bugs in the binding-time analysis of the PGG, and even more \nembarrassing bugs in Mike s original compiler. They also have to resolve some integration issues that \nhave to do with the fact that the new program generators manipulate object code generators rather than \nsource expressions. To their surprise and slight disappointment, there are no conceptual problems, and \nno hackery whatsoever is neces- sary to ensure that the compiler is usable both as a normal source code \ncompiler and as a generator for the code gener- ation combinators. Curtain ..- M ..-I iet (z MI) Mz) \nV E Values 1 [gz$ M3) C E Constantsl...Mn) E Variables 1 (0 M:...M,) 5 E Primitives ..-   V ..-c \n1 z 1 (Xzl . . . zn.M) Figure 1: Abstract syntax of Core Scheme expressions (CS) ..- M ..- 1 Let (z \nV) M) 1 rt;$fl Mz) l...v,) i ;~$.(..~~ vt --.W) W 1 (let (z (O>l...V,,)) M) ..- V ..-clzl (XZl...Z,.M) \nFigure 2: Syntax of CS terms in A-normal form  Partial Evaluation and PGG s Partial evaluation [8,30] \nis an automatic program special-ization technique. It operates on a source program and its known (&#38;tic) \ninput. The specialized residual program takes the remaining (dynumic) input and delivers the same result \nas the source program applied to the whole input. Often, the residual program is faster than the source \nprogram. An attractive feature of partial evaluation is the ability to construct generating extensions. \nA generating extension for a program p with two inputs s-inp and d-inp is a pro- gram p-gen which accepts \nthe static input s-inp of p and produces a residual program pa-inp. The residual program accepts result \nas minate. the dynamic input d-inp and produces the same BPB s-inp d-inp, provided both p and ps-inp \nter- Bp-gell] s-inp = ps-inp d-inp = result kin s-inp d-inp = result  A generating extension results \nfrom applying a program-generator generator (PGG) to p. A PGG can result from double self-application \nof a partial evaluator as described by the third Futamura projection [21,30,62], or from direct implementation \n[36,59]. Specialiiing Core Scheme The building blocks of the system are a PGG for a large sub- set \nof Scheme [59] and the Scheme 48 byte code implementa- tion of Scheme [32]. Our PGG has several design \nproperties which make it especially suitable for our goals. None of these properties were designed with \nobject-code generation in mind-they arise naturally from other requirements. For our presentation, we \nuse an abstract syntax for Core Scheme (CS) [20] shown in Fig. 1. We have omitted top-level definitions \nfor brevity s sake. During work on a partial evaluator for an ML-style lan-guage that performs side e&#38;&#38;s \nat specialization time [16], Dussart, Lawall, and the second author discovered that a specialiier must \noutput code in a restricted subset of Scheme that explicitly serializes computations to ensure correctness \n--essentially A-normal form (201 (ANF) as shown in Fig. 2. Thus, ANF is the natural target language of \nthe PGG. Figure 3 shows such a specialiser restricted to Core Scheme. In the definition of the specializer, \nwe employ ACS ( Annotated Core Scheme ). ACS has additional variants of primitive operations, let expressions, \nlambda abstractions, applications, and conditionals (annotated with superscript D, for dynamic) that \ngenerate code. Additionally, there is a lift construct that coerces first-order values to code. Under-lining \nindicates code generation. The superscript produces fresh variables. Multiple occurrences of, say, z \ndenote the same variable. The specializer employs continuation-based partial eual-uation [3,7,38] to \ngenerate code in ANF. Whenever a piece of code denoting a serious computation (a non-value) is constructed, \nit is wrapped in a let expression with a fresh variable which is used in place of the piece of code. \nThis happens in the rules for primitive operations and applicb tions. The let wrapping is not necessary \nfor values (con-stants, variables, and abstractions). The specializer performs some other transformations \nthat are necessary in a compiler. It desugars input programs to Core Scheme, performs lambda lifting \n[29] and assignment elimination. We do not show the parts of the specializer that deal with memoization, \nsince they are standard [30,60] and not relevant to the present work. Our compiler makes essential use \nof the transformations already done by the specializer. It is a straightforward recursive-descent compiler \nwhich passes around source ex- = Xk.kc  SUCIIP = Xk.k(p[xl) a4iP S[(O El . . . GJp = Xk.S[E&#38;(Xyl. \n. . . S[En)p(Xyn.k([O]yl . . . yn))) S[(XXl . . . xn.E)]p = Xk.(Xyl . . . yn.5 _ _. ?W$;;l) S[(O Eo \nEI . . . En)lp S[(let (z El) Ez)]P S[(ifO El E2 E3)lp = = = Xk.S[E&#38;(Xf.S[Elj Xk.S[E&#38;(Xy.C---Xk.SCE&#38;(Ay.( \n. . . . S[En]p(Xy,.fyl ~U~mW~lk) ifrDY (WW) WWW . . . y,k)) SKlift ~ 31~ S((OD El . . . En)]p S[(XDxl \n. . . x,,.E)]p S[(QD Eo El . . . En)]p S[(let (x El) J&#38;)]p Sl[(ifoD El E2 E3)lp = = = = = = ~k.SUJWbk(y)) \nXk.S[E$(Xyl. . . . S(En]p(Xyn.(!&#38; (x0 (0 Xk.k((Xx: 9.. x~.Sl[EBp[xP/xi](Xy.y))) Xk.S%Eo~p(Xy.S]El]p(Xyl.. \n. . S[&#38;]p(Xy,.(!&#38; Xk.SCEllp(Xy.S[Ez]p[y/x]k) Xk.S[E$(Xy~.(Z! YI (S[E2Bpk) @[E&#38;k))) yl . . \n. yn)) kx ))) (x0 (P y YI . . . yn)) kx ))) Fimre 3: Specializer which generates ANF output code pressions, \na compile-time environment mapping names to stack and environment locations, and a stack depth neces-sary \nto correctly generate code for the Scheme 48 virtual machine. 5 Automatic Composition For the theoretical \nbasis of our work, we draw from ideas from numerous disciplines: Algebraic syntax representation, compositional \ncompilers, and deforestation are the key ideas crucial to composing a special&#38;r with a compiler. \nMore over, ANF serves as a convenient means of communication between the specializer and the compiler. \n5.1 Syntax For our purposes we regard standard syntax and annotated syntax as algebraic datatypes. Therefore, \nboth can be de- tied as least fixpoints of functions over sets of expressions. The functions are derived \nfrom the grammar of CS (with + denoting disjoint union, x denoting Cartesian product of sets, and List(X) \ndenoting the set of finite lists over X). Figure 4 shows the definition. We use the symbolic tags Syntax \n= MkSyntax(Syntax) where  MkSyntax(X) = wnst Constants constants + var Variables identifiers + lam \n(List(Variables) x X) lambda abstractions + let (Variables x X x X) let expressions + if(XxXxX) conditionals \n + app (X x List(X)) applications + prim (Primitives x List(X)) primitive operations  Figure 4: Algebraic \nDefinition of Syntax cons&#38; var, lam, if, app, and prim as indicators in which summaud of the disjoint \nunion a value in MkSyntax(X) lies. Thii allows us to use pattern matching as syntactic sugar in defining \nfunctions on MkSyntax(X). Given MkSyntax, we can convert every function f : Y + 2 to a function MkSyntax(f) \n: MkSyntax(Y) + M kSyntax(2) by MkSyntax(f)(const c) = wnst c MkSyntax(f)(var (x)) = vor (2) MkSyntax(f)(lam \n(XI.. .x,,,y)) = lam (x1 . ..x*.fy) MkSyntax(f)(let (x, brlr ~2)) = 1st (=, farl, f~a) MkSyntax(f)(V \n(~1, ~2, ~33)) = if (fY1, fY2, far31 MkSyntax(f)(wv (Y,YI . . . ~~1) = am UY, far1 . . . fynn) MkSyntax(f)(prim \n(0, II.. . yn)) = prim (0, fyi . . . fsm) Analogously, we define AnnSyntax = MkAnnSyntax(AnnSyntax), \n the function MkAnnSyntax, and its action on functions f : Y + 2. The additional tags are lift, dlam, \ndif, dapp, and dprim. Technically, MkSyntax and MkAnnSyntax are functors over Set. 5.2 Compositionality \nOne of the fundamental ideas of denotational semantics [53] is the description of the meaning of a programming \nlanguage phrase by a compositional recursive definition: The meaning of an expression is a function of \nthe meanings of its subex- pressions. For the language CS we cau therefore describe the se-mantics of \nCS by defining suitable domains and functions ev-wnst, ev-var, ev-lam, ev-let, ev-if, ev-app, and ev-prim. \n(A denotational implementation to follow Espinoea s termi-nology [19, p. Ill.) They are parameters to \na generic recur-sion schema that traverses CS expressions (see Fig. 5) where we write 7% for the tuple \n(ev-wrist, . . . , ev-prim). This recur-sion schema is a catamorphism for Syntax [43]. Apart from compositional \nsemantics, catamorphiams are also useful for describing compilers and specializers [59]. For a compiler, \nthe functions WC are compilation functions for each single construct. For a special&#38;r, the functions \nEs are specialization functions. In this case we have to use an cata.~s(ZC)(M) = case A4 of C =k-eu-cond(c) \n* ev-uar(v) b Zl...Z,.M) * ewlam(z1 . . . zn, catacs(Z5)(M)) (let (5 MI) Mz) * ev-let(2, cutacs(ZC)(M~), \ncatacs(ZU)(M2)) (if0 MI MZ MS) =S eu-iflcatws(T??)(M~), catacs(Z$(M2), catucs(ZC)(M.~)) eu-app(cataa@)(M), \ncatam(ZC cotocs(?%)(M,,)) eu-prim(O, catacs(ZG)(M~). . . catam( Figure 5: Generic recursion schema for \nCS extended schema &#38;aAcs(~S)(-) which has additional pa-rameters and uses for the annotated versions \nof the syntax constructors. For example the specialization function for (iRID MI M2 MS) is ev-di&#38;(al, \n22,23) = A~.XIC.~I~(X~I.(EQ 91 (rzpk) (zspk))). It is obtained from the explicit recursive definition \nin Fig. 3 by systematic transformation. 5.3 From Recursive Deflnition to Implicit Recur-sion The transformation \nof recursive definitions into definitions using catamorphisms is non-trivial in general [37]. In our \nspecific case, standard techniques from partial evaluation [30] suffice to transform the explicit recursive \ndefinition of the compiler into the specialized compilation functions EC. Our aim is to generate ZEE \nautomatically from the given recursive definition by specializing it with respect to the different syntactic \nconstructs. Two things have to be done: 1. The syntactic dispatch has to be performed at special- ization \ntime. 2. The recursive calls to the (explicitly recursive) compi- lation function on the syntactic subcomponents \nof the construct have to be removed (replaced by the iden-tity).  For the removal of recursive calls, \nthe definition of catocs(ZC)(J already takes care of the recursion. Hence all syntactic subcomponents \nhave already been compiled when EC is applied. For the specialization combinators Es, such a transformation \nhas been carried out and proved correct by the second author [59]. From now on we assume that the compiler \nis given in the form cata~s(?%c)(J and the specializer in the form c&#38;ACS(i?i&#38;)(-). The types \nof these functions are interest-ing and important for us. =c : MkSyntax(Code) + Code ~kS (eve > C) : \nSyntax + Code FES : MkAnnSyntax(Syntax) + Syntax CdaACS(=S>(-) : AnnSyntax -+ Syntax 5.4 Deforestation \nDeforestation is a program transformation for functional programs that removes intermediate data structures \nby sym- bolic composition [63]. Suppose a function g produces some intermediate data structures that \nis immediately consumed by function f in f(g(z)). Deforestation (if applicable) re-sults in a function \nh such that Vz.h(z) = f(g(z)) where the computation of h does not involve the construction of in- termediate \ndata. Deforestation can also be expressed in a calculational form where it amounts to the application \nof a so-called fusion or promotion theorem [41,43]. Specialized to our situation, the fusion theorem \nstates the following: vx.vg. mtacs(Zc)(eu-Xs@)) = eu-XcOs(MkAnnSyntax(mtucs@c)(J)(~)) 7M. catacs(evc)(cotaAcs(~s)(M)) \n= ~bex(=cos)(M) Here, X ranges over the syntax constructor tags of anno- tated expressions and 3 is \nan argument vector for X, but with all arguments of type AnnSyntax replaced by argu-ments of type Syntax, \ni.e., X(g) : MkAnnSyntax(Syntax). That is, on the left side of the premise the specializer has recursively \nspecialized AnnSyntax to Syntax and is now about to specialize the next constructor of AnnSyntax. Now, \neu-Xs@) : MkSyntax(Syntax) performs this specialization step and we can apply catacs(Z%)(J = Xz.cat~s(E~~)(r) \nto it to compile it to Code. On the right side of the premise we use ev-Xcos, a func- tion that specializes \nand compiles the annotated construct X. Here, we first compile the components of 5 of type Syn-tax to \nobtain a value of type MkAnnSyntax(Code). This is a suitable argument for eu-Xcos since mx.s : MkAnnSyntax(Code) \n+ Code c&#38;aAc~(~~o.+7)(-) : AnnSyntax + Code The remaining M in the conclusion ranges over AnnSyntax. \nNow the task is: Given 7%~ and iZc find ?%,.s such that the premise of the fusion theorem holds. In thii \nway we can derive that, for example, eu-difcOs(a, a, 22) = Xp.Xlc.zlp(Xyl.eu-r(~l, z2pk z33pk)). Now \nwe know what we have to do to obtain GcOs: we only have to replace the syntax constructor X in the de&#38;i- \ntion of Z@ by the respective call to function ev-Xc from EC. In practice, we parameterize Z&#38; over \nthe (standard) syn-tax constructors and provide alternative implementations for them: one that constructs \nsyntax and another one that corresponds to EC. 6 Implementation The theory presented in the previous \nsection translates into practice smoothly. This section describes the concrete im-plementation of the \ncompiler and its fusion with the PGG in the context of the Scheme 48 system. 6.1 Step 1: Write a Compiler \nIn principle, it is possible to simply use the stock Scheme 48 byte-code compiler which passes a compile-time \ncontinua-tion to identify tail-calls. However, the target code of the specialization engine is in ANF. \nANF, as shown in Fig. 2, already makes control flow explicit. Only those function ap- plications wrapped \nin a let are non-tail calls; all others are jumps. Hence, the propagation of a compile-time continu-ation \nis unnecessary, and it is sensible to make do with a drastically cut-down version of the compiler. Removing \nthe compile-time continuation simplifies the compiler, and also speeds up later code generation, as it \ncould not be removed by fusion. The compiler is integrated with the Scheme 48 system. In particular, \nit uses its native syntax representation and dii- patch mechanism. The output of the compiler is an abstract \nrepresentation of the byte code for the Scheme 48 virtual machine, essentially a stack machine with direct \nsupport for closures and continuations [32]. Here is the compilator for if: (define-compilator if syntax-type \n(lambda (node cenv depth) (let ((exp (node-form node)) (alt-label (make-label) 1) (sequentially ;; Test \n(compile-trivial (cadr exp) cenv) (instruction-using-label (arm5 op jump-if-false) alt-label) ;; Consequent \n(compile (caddr exp) cenv depth) ; ; Alternative (attach-label alt-label (compile (cadddr exp) cenv depth)) \n1))) The compilator takes three parameters: node a node of a syntax tree representing a conditional, \ncenv a compile-time environment, and depth the current depth of the stack. Compile and compile-trivial \ncompile the subexpressions of the conditional. A compilator constructs object code by using a number \nof constructors: Sequentially arranges byte-code instruc-tions in sequence; make-label, instruction-using-label, \nand attach-label serve to create the jump code typical for compiling conditionals. These constructors \nreturn an ab-stract representation of object code. Scheme 48 internally relocates the representation, \nresolves labels, and generates the actual byte code. The relocation step is inherent in the Scheme 48 \narchitecture; an alternative implementation would generate the object code directly, using backpatching \nfor resolving labels. The compiler utilizes the def ine-compilator procedure to create an entry in a \nsyntax dispatch table compilators. It uses the native syntax dispatch mechanism of the Scheme 48 compiler: \n(define (define-compilator name type proc) (operator-define! compilators name type proc)) Compilators \nis then used by a procedure compile which is the top-level dispatcher of the compiler. (define (compile \nexp cenv depth) (let ((node (classify exp cenv))) ((operator-table-ref compilatora (node-operator-id \nnode)) node cenv depth))) Compile compiles serious expressions. An analogous mechanism creates the compile-trivial \nprocedure. 0.2 Step 2: Annotate the Compiler The annotation of the compiler functions is a straightfor-ward \nprocess that requires no deep understanding of the code. The compilation combinators have to perform \nthe syntactic dispatch at generation time (of the combiitors) and copy the remaining code verbatim. The \nannotated com-pilator for if is as follows: (def ine-compilator if syntax-type (lambda (node cenv depth) \n(let ((exp (node-form node))) (-let ((alt-label (-make-label))) (- sequentially ;; Test (compile-trivial \n(cadr exp) cenv) (-instruction-using-label (-lift-literal (enum op jump-if-false)) alt-label) ;; Consequent \n(compile (caddr exp) cenv depth) ;; Alternative (-attach-label alt-label (compile (cadddr exp) cenv \ndepth) ) 1) 1) ) In an annotated program all constructs starting with an underline_ perform code generation. \nThe remaining parts are executed at generation time. Of the parameters to the compilator, only the structure \nof node is known at generation time. The subexpressions of node as well as cenv and depth are unknown. \nA correct annotation prescribes code generation for every value that depends on an unknown value. The \nunderlined constructs have the following meaning: b -let generates a let expression; . . generates a \nprocedure call, for example, (, make-label) generates a call to procedure make-label; a -lift-literal \nturns a generation-time constant into code. Conceivably, even this annotation could have been per- formed \nautomatically by an appropriate binding-time anal-ysis, possibly at the expense of changing the representation \nof abstract syntax in the compiler. 6.3 Step 3: Implement the Annotations Since the annotated compiler \nneeds to serve both as a stand- alone compiler and as a generator for the code generation combinators, \nthere are two diierent implementations of the annotations. 6.3.1 Annotations for the Compiler For the \ncompiler, the annotations need to disappear again. Scheme macros [51] do the job: (define-syntax _ (syntax-rules \n0 (Cm arg . ..I (WI . ..)))) (define-syntax -let (syntax-rules 0 ((-let stuff . ..I (let stuff . . .)))) \n (define-syntax -lift-literal (syntax-rules 0 ((-lift-literal 2) 2))) 6.3.2 Code Generation Combinators \nProducing code generation combinators from the compiler is also straightforward. The idea is to create \nalternative ver-sions of the annotation macros that produce Scheme source expressions for the combinators, \nto print these into a file, and load them when needed. The macro _ constructs a function call by taking \nthe name f ct of the function literally and processing the argu- ments recursively. (define-syntax _ \n(syntax-Iules 0 ((-fct arg . ..I ffct ,arg . ..)))) The Jet macro constructs a let. expression from \nthe vari- able name v and the processed header e. The body is con- structed in an environment where v \nis bound to the symbol v. This binding is produced by a generation-time let ex-pression. (define-syntax \n-let (syntax-rules 0 ((Jet ((v e)) body) (LET ((v ,e)) ,(let ((v VI) body))))) In general, such a macro \ncannot reuse the variable name v but rather needs to generate fresh names [59]. This is not necessary \nin our implementation. The -lift-literal macro (for numbers, etc) is merely present for conceptual reasons \nas numbers, for instance, are self-quoting in Scheme. To generate code for constants like lists, another \nmacro -lift-quote inserts the proper quoting. (define-syntax -lift-literal (syntax-rules 0 ((-lift-literal \n2) 2) >)  The calls to compile and compile-trivial are discarded as explained in Sec. 5.4. (define-syntax \ncompile (syntax-rules 0 ((compile rug . ..I (.arg . ..))>I Finally, an alternative implementation of \ndefine-compilator generates procedure definitions for output code constructors. The generating extensions \nproduced by the PGG call the procedures to generate residual code. (define-syntax define-compilator (syntax-rules \n(quote if call let-trivial let-serious begin) ((define-compilator if ncd-f un) (define (make-residual-if \ntest then alt) ,(constNct-serious-body ncd-f un (make-node (get-operator if) (IF TEST TEEN ALT))))) ((define-compilator \ncall ncd-fun) (define 6nake-residual-call f . rugs) ,(constNct-serious-body ncd-f un (make-node (get-op \nerator call) (F . ARGS))))) ;; . . . 1) (define (construct-serious-body ncd-fun node) (lambda (cenv \ndepth) ,(ncd-fun node cenv depth))) Each of the compilators cabs construct-serious-body. It accepts a \nfunction of three arguments, a known node, un-known cenv, and depth as described above. The other ar-gument \nnode is statically constructed in the respective part of the def ine-compilator macro. The make-residual-... \nfunctions generated by the above process serve as direct replacements of the syntax con-struction functions \n(in, _ _ -), (9 _ 3, etc, in the speciaiizer. 6.4 Step 4: Test and Debug There were no problems in putting \nthe system together. During the integration we discovered bugs mostly in the special&#38;r and in the \ncompiler which were independent of the conceptual issues. The only problem to be resolved has to do with \nthe du- ality between variable names and their compilators: During ordinary specialization there are \ntwo kinds of ob- jects: static values and pieces of code. Naive application of our approach replaces \nthe pieces of code by compilation functions. However, the compiiator for lambda abstractions requires \na lit of the names of its free variables; references to them are compiled differently from regular variables \nfor the Scheme 48 VM. Therefore, our systems passes the names of variables by default and converts them \nto compilation functions when necessary. 7 Benchmarks Our experiments largely confirm the expectations \nto RTCG technology, but also point to some possible improvements to our implementation. For our benchmarks, \nwe used two standard examples for compilation by partial evaluation: an interpreter for a small first-order \nfunctional language called MIXWELL, and one for a small lazy functional language called LAZY, both taken \nfrom the Similix distribution [4]. The MIXWELL in- terpreter is 93 lines long and was run on a 62~line \ninput program, the LAZY interpreter has 127 lines of code and was run on a 26line input program. We used \nScheme 48 0.46 on a Pentium/SO laptop with 24 Megabytes of RAM running l+eeBSD 2.1.5. All timings are \ncumulative over a large number of runs, and are in seconds. source code object code MIXWELL 3.072 3.770 \nLAZY 1.832 3.451 Figure 6: Generation speed Figure 6 shows timings for generating both Scheme source \nand object code directly for compilers generated from the interpreters, in both casea on medium-sized \ninput pro-grams. Object code generation is up to a factor of 2 slower than generating source, since Scheme \n48 uses a higher-order representation for the object code that still needs to be con- verted to actual \nbyte codes-that conversion is also part of the timings. Hence, a future step would be emitting byte code \ndirectly or using a more efficient intermediate repro-sentation. Figure 7: Compilation times for the \nspecialization output Still, loading the generated source code back into the Scheme system is by far \nmore expensive than direct object code generation, as in Fig. 7. Here, we used our own ANF compiler, \nnot the (slower) stock Scheme 48 compiler. To fully appreciate the timing data, note that in order to \npro- duce object code for a specialized program from an ordinary specializer, we have to add the timings \nfor source code gen- eration in Fig. 6 and the compilation times in Fig. 7. 1 BTA Load Generate Compile \nMIXWELL 1 2.730 4.026 0.652 0.964 LAZY 1 2.253 3.217 0.568 0.604 Figure 8: Using RTCG for normal compilation \nOne of our future objectives is to create a Scheme sys- tem where the stock compiler works through run-time \ncode generation. For normal compilation, the system takes all inputs to a program as dynamic. Figure \n8 shows timings for preliminary experiments in that direction: The BTA col-umn shows the time needed \nfor binding-time analysis and creation of the object code generator, Load is the time needed for loading \n(and compiling) the object code genera- tor, and Generate the time for running it. Compile is the time \nneeded to load and compile the original interpreter using the stock Scheme 48 compiler. 8 Related Work \n8.1 A-Normal Form Compilation with ANF [ZO] captures the essence of continuation-baaed compilation [2,31,34,57]. \nWe build upon that work to construct the simple ANF compiler. Using ANF (or monadic normal form) for \ncompilation is also put forward by Hatcliff and Danvy [25] and by Sabry and Wadler [521. Danvy s work \n[11,12,14] uses type-directed partial evalu-ation for semantics-directed compilation. His system wraps \nlet expressions around code that denotes computations in order to avoid code duplication. As a result, \nhe also ob- tains programs in ANF. The type-directed partial evaluator is also a suitable candidate for \ncomposition with a compiler in the same way as shown in this work. Partial evaluation [8,30] is an automatic \nprogram trans-formation that specializes programs with respect to parts of the input which are known \nin advance. Continuation-based partial evaluation [3,38] is the enabling technology that makes our specializer \nsuited to generate code in ANF. The partial evaluator that we use is the ANF version of Con- se1 and \nDanvy s [7] initial approach to improve the results of partial evaluation by CPS transformation. The \noriginal application of our specializer is specialization of MLstyle programs which can perform operations \non references at spe- cialization time [17]. 8.2 Partial Evaluation Holst [27] describes a system called \nAMIX, a partial evalu-ator that generates code for a stack machine directly. The motivation behind this \nsystem is similar to ours, with two notable differences: The AMIX system was written from scratch with \nthe generation of stack code in mind and it is offline in the sense that it produced stack code in some \nrepresentation that had to be fed to a separate interpreter. In contrast, our system results from the \nsystematic compo-sition of existing parts and it produces code for immediate execution by the run-time \nsystem. Annotation functions similar to the ones shown in Sec. 2.3 are considered in work on writing \nPGG s by hand 123,591. Deforestation is well-known and well-investigated in the functional programming \ncommunity because it is a powerful tool for program optimization [5,6,22,24,37,44,45, 54-56,58,63]. Symbolic \ncomposition is an important technique in the CERES compiler generator system [61]. One of the key steps \nin CERES is the composition of a language definition considered as a compiler with a fixed compiler to \na low-level language. However, the technical details of thii composi-tion are not spelled out. Also, \nin CERES, the result of the composition is the entire compiler, whereas our composition generates bricks \nfrom which the generated compiler will be built. Another application of composition in CERES is the creation \nof parts of the system itself. 8.3 Run-Time Code Generation Run-time code generation has received revived \ninterest since the early 90 s when techniques became available to perform RTCG cheaply. Previously, RTCG \nhappened mainly in the context of reflective language systems, notably Lisp, and usually involved prohibitive \ninterpretive overhead or compi- lation time. Since then, it has been applied to code optimiza- tion [33], \nefficient dynamic implementation of programming languages [26], optimization of bitmap graphics [15,47], \nop erating system optimization [42] and other tasks specifically suited to RTCG [9,10,39,40]. These present \nonly selections; a complete overview of the field would exceed the scope of this paper. Of particular \ninterest is the work relating RTCG to psr- tial evaluation. The FABIUS system of Lee and Leone [39,40] \nperforms RTCG of native code for ML. FABIUS comes with its own application specific code generator, whereas \nwe have reused the Scheme 48 code generation machinery. Both systems compile annotated source code to \nyield a program that generates code at run-time. The systems dier in the way they generate code. FABIUS \nproduces object code at run-time and performs various standard optimizations at compile-time and at run-time. \nFor example, it resolves the issue of register allocation for run-time generated code at compile-time \nwhereas it addresses instruction selection at run-time. Our system abstracts from all these issues since \nthe emphasis is on the composition itself. Our approach is complementary in that it could benefit from \nFABIUS S tech- niques to improve the efficiency and quality of code genera- tion Consel s group has implemented \nthe Tempo system for C [9, lo] which generates code templates which are copied and instantiated at run \ntime. Their system interacts with the GNU C compiler, but requires low-level modifications in the code \ngenerator to support templates. Draves [15] has applied partial evaluation to low-level code, and imple- \nmented a PGG for an intermediate representation to obtain efficient code for computer graphics. His system \noperates purely on the intermediate code; only rudimentary support for higher-level programming is provided. \n9 Assessment and F uture Work Unfortunately, researchers have applied the term run-time code generation \nto a variety of different situations. There is no clear cut border between ordinary compilation and RTCG. \nOn the ordinary end of the spectrum, compila-tion is ofFine as in traditional compilers. On the other \nend of the spectrum, there are online systems like DCG [18] or the Synthesis kernel [42] that compile \ncode for immediate execution. The rest of the spectrum is not empty: pro-gramming languages lie ML, Scheme, \nor Smalltalk have a read-eval-print loop that accepts function definitions that are compiled and the \ncode is immediately available for exe- cution. Hence, they are essentially online compilers. Obvi-ously, \nthe trade-of% involved at each point of the spectrum are different. Typical situations for RTCG seem \nto be the interleav- ing of compilation with execution of the compiled code, the generation of object \ncode without explicitly running a com- piler, and combinations of those two. Recent applications have \nfocused on using RTCG for on-the-fly code optimiza- tions in high-performance systems by staging computations \nat run time. For this to be effective, it is clearly necessary to deal with native code generation and \nthe problem that this incurs, such as register allocation and various optimization techniques. As our \nwork focuses on the high-level aspects of gaining an RTCG system, the issues we address are largely orthog- \nonal to recent work in the RTCG field. Instead, they are a direct continuation of previous work in the \npartial evalu-ation community. In some sense we are aiming to provide the missing link to systems like \nFABIUS. As such, our imple- mentation is geared more towards applications well-known in partial evalution \nwhich benefit from RTCG rather than the other way around. Clearly, work remains to apply our approach \nto realistic RTCG systems which generate native code. The applicability of partial evaluation methodology \nas a framework for on-the-fly code generation has already been demonstrated [9,10,40]. Hence, the following \nissues need to be addressed: . Our method needs to be applied to a compiler gen-erating native code. \nA first step in our present sys-tem would be circumventing the intermediate repre-sentation which would \nspeed up code generation signif- icantly. . Typical current RTCG applications must be reformu- lated \nin the context of incremental specialization [60]. The performance of those systems needs to be com-pared \nto hand-written ones. . As mentioned in Sec. 7, the major obstacle to replacing the a stock compiler \nby an RTCG system is that the code generators still have to be loaded-and thus com-piled by the stock \ncompiler-before they can be exe-cuted. The obvious way to speed up that process is to apply the system \nto itself, and generate the generating extensions as object code themselves. 10 Conclusion We have demonstrated \nthat modern program transforma-tion techniques lie deforestation and partial evaluation are powerful \ntools in the hands of the programmer. We have successfully composed a partial evaluator with a compiler \nwithout rewriting everything from scratch. Instead we have reused code from the compiler and from the \nspecialiier and have generated the glue code automatically (by partial eval-uation) from an annotated \nversion of the original compiler. Our work is an attempt to bridge the gap between partial evaluation \nand run-time code generation methodologies. Acknowledgments We would like to thank Richard Kelsey for \nexplaining the internals of the Scheme 48 system. References [l] ACM. Proc. 1991 ACM finctional Programming \nLanguages and Computer Architecture, Cambridge, September 1991. [2] Andrew W. Appel. Compiling with Continuations. \nCambridge University Press, 1992. [3] Anders Bondorf. Improving binding-times without ex-plicit CPS conversion. \nIn Symp. Lisp and Jbnctional Programming 92, pages l-10, San Francisco, Ca., June 1992. ACM. [4] Anders \nBondorf. Similix 5.0 Manual. DIKU, University of Copenhagen, May 1993. [5] Wei-Ngan Chin. Safe fusion \nof functional expressions. In 7th Conf. Lisp and finctional Programming, pages 11-20, San Francisco, \nCa., 1992. ACM. [S] Wei-Ngan Chin. Safe fusion of functional expressions II: Further improvements. Journal \nof finctional Pro-gramming, 4(4):x, October 1994. (71 Charles Consel and Olivier Danvy. For a better \nsupport of static data flow. In FPCA1991 [l], pages 496-519. (81 Charles Con&#38; and Olivier Danvy. \nTutorial notes on partial evaluation. In Symposium on Principles of Pro- gramming Languages 93, pages \n493-501, Charleston, January 1993. ACM. [9] Charles Consel, Luke Hornof, Francois N&#38;l, Jacques Noye, \nand Nicolae Volanschi. A uniform approach for compile-time and run-time specialization. In Danvy et al. \n[13], pages 54-72. LNCS. [lo] Charles Consel and Francois N&#38;l. A general approach for run-time specialization \nand its application to C. In POPL1996 [50], pages 145-156. [ll] Olivier Danvy. Pragmatics of type-directed \npartial evai- uation. In Danvy et al. (131, pages 73-94. LNCS. [12] Olivier Danvy. Type-directed partial \nevaluation. In POPL1996 [50], pages 242-257. [13] Olivier Danvy, Robert Gliick, and Peter Thiemann, ed-itors. \nDagstuhl Seminar on Partial Evaluation 1996. Springer-Verlag, 1996. LNCS. [14] Olivier Danvy and Rem? \nVestergaard. Semantics-based compiling: A case study in type-directed partial evaI-uation. Technical \nReport RS-96-13, Basic Research in Computer Science, May 1996. [15] Scott Draves. Compiler generation \nfor interactive graphics using intermediate code. In Dauvy et al. [13], pages 95-114. LNCS. [16] Dirk \nDumart and Peter Thiemann. Imperative func-tional specialization. Berichte des Wilhelm-Schickard- Instituts \nWSI-96-28, Universitat Tiibingen, July 1996. [17] Dirk Dussart and Peter Thiemann. Partial evama-tion \nfor higher-order languages with state. Berichte des Wilhelm-Schickard-Instituts WSI-96-Xx, Univer-sitat \nTiibingen, November 1996. [18] Dawson R. Engler and Todd A. Proebsting. DCG: An efficient, retargetable \ndynamic code generation system. In Conference on Architectural Support for Pmgmm-ming Languages and Opemting \nSystems (ASPLOS VI), pages 263-272. ACM Press, 1994. [19] David A. Espinosa. Semantic Lego. PhD thesis, \nColumbia University, New York, New York, 1995. [20] Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias \nFelleisen. The essence of compiling with con-tinuations. In Proc. of the ACM SIGPLAN 93 Con-ference on \nProgramming Language Design and Imple-mentation, pages 237-247, Albuquerque, New Mexico, June 1993. ACM \nSIGPLAN Notices, 28(6). [21] Yoshihiko Futamura. Partial evaluation of computation process-an approach \nto a compiler-compiler. Systems, Computers, Controls, 2(5):45-50, 1971. [22] Andrew Gill, John Launchbury, \nand Simon L. Peyton Jones. A short cut to deforestation. In Arvind, editor, Proc. finctional Progmmming \nLanguages and Com-puter Architecture 1993, pages 223-232, Copenhagen, Denmark, June 1993. ACM Press, \nNew York. (231 Robert Gliick and Jesper Jorgensen. Efficient multi-level generating extensions for program \nspecialization. In PLILP1995 (491. LNCS. [24] Geoffrey W. Hamilton. Higher order deforestation. In Kuchen \nand Swierstra [35], pages 213-227. [25] John HatclifI and Olivier Danvy. A generic account of continuation-passing \nstyles. In Pmt. 21st Symposium on Principles of Programming Languages, pages 458-471, Portland, OG, January \n1994. ACM Press. [26] Urs HGlzle and David Ungar. Opimiiing dynamicahy-dispatched calls with run-time \ntype feedback. In Con-ference on Progmmming Language Design and Imple-mentation 94, pages 326335, Orlando, \nJune 1994. ACM. [27] Carsten Kehler Ho&#38;. Language triplets: The AMIX approach. In Dines Bjerner, \nAndrei P. Ershov, and Neil D. Jones, editors, Partial EvaIuation and Mized Computation, pages 167-186, \nAmsterdam, 1988. North- Holland. [28] Prvc. 1996 ACM International Conference on fine-tional Programming, \nPhiladelphia, May 1996. ACM. [29] Thomas Johnsson. Lambda lifting: Transforming pro-grams to recursive \nequations. In Proc. finctional Prv-gmmming Languages and Computer Architecture 1985. Springer-Verlag, \n1985. LNCS 201. [30] Neil D. Jones, Carsten K. Gomard, and Peter S&#38;oft. Partial Evaluation and Automatic \nProgram Generation. Prentice-Hall, 1993. [31] Richard KeIsey and Paul Hudak. Realistic compilation by \nprogram transformation. In Pmt. 16th ACM Sym-posium on Principles of Programming Languages, 1989. [32] \nRichard A. Kelsey and Jonathan A. Rees. A tractable Scheme implementation. Lisp and Symbolic Computa-tion, \n7(4):315-335, 1995. [33] David Keppel, Susan J. Eggers, and Robert R. Henry. A case for runtime code \ngeneration. Technical Report 91-11-04, University of Washington, Seattle, WA, 1991. [34] David Kranz, \nRichard KeIsey, Jonathan A. Rees, Paul Hudak, Jim Philbin, and Norman Adams. Orbit: An optimizing compiler \nfor Scheme. In Proc. Symposium on Compiler Construction, pages 219-233, 1986. ACM SIGPLAN Notices, vol \n21(7). [35] Herbert Kuchen and Doaitse Swierstra, editors. PTV-gmmming Language Implementation and Logic \nPro-gramming (PLILP 96), volume 1140 of Lecture Notes in Computer Science, Aachen, Germany, September \n1996. Springer-Verlag. [36] John Launchbury and Carsten Kehler Holst. Hand-writing cogen to avoid problems \nwith static typing. In Draft Proceedings, Fourth Annual Glasgow Work-shop on finctional Progmmming, pages \n210-218, Skye, Scotland, 1991. Glasgow University. (371 John Launchbury and Tim Sheard. Warm fusion: \nDe-riving build-cata s from recursive definitions. In Peyton Jones [46], pages 314-323. (381 Julia LawaIl \nand Olivier Danvy. Continuation-based partial evaluation. In Proceedings of the Conference on Lisp and \nFunctional Pmgmmming, pages 227-238, Orlando, Florida, 1994. ACM. 1391 Peter Lee and Mark Leone. Optimizing \nML with run-time code generation. In PLDIl996 [48], pages 137-148. SIGPLAN Notices, 31(5). [40] Mark \nLeone and Peter Lee. Lightweight run-time code generation. In Peter S&#38;oft and HaraId Sondergaard, \neditors, Workshop Partial Evaluation and Semantics-Based Program Manipulation 94, pages 97-106, Or-lando, \nFla., June 1994. ACM. [41] Grant Malcolm. Homomorphisms and promotability. In J. L. A. van de Snepscheut, \neditor, Mathematics of Pro-gram Construction, pages 335-347, 1989. LNCS 375.  [42] Henry Massahn. Eficient \nImplementation of lh-damental Operating System Services. PhD thesis, Columbia University, 1992. [43] \nErik Meijer, Maarten Fokkinga, and Ross Paterson. Functional programming with bananas, lenses, en-velopes \nand barbed wire. In FPCA1991 [l], pages 124- 144. [44] Kristian Nielsen and Morten Heine Sprrensen. \nCall-by-name CPS-translation as a binding-time improve-ment. In Alan Mycroft, editor, Proc. International \nStatic Analysis Symposium, SAS 95, pages 296-313, Glasgow, Scotland, September 1995. Springer-Verlag. \nLNCS 983. [45] Kristian Nielsen and Morten Heine Sorensen. Defor-estation, partial evaluation, and evaluation \norders. In PLILP1995 [49]. LNCS. [46] Simon Peyton Jones, editor. Functional Programming Languages and \nComputer Architecture, La Jolla, CA, June 1995. ACM Press, New York. [47] Rob Pike, Bart Locanthi, and \nJohn Reiser. Trade-offs for bitmap graphics on the Blit. Software-Pmctive E4 Ezperience, 15:131-151, \n1985. [48] Proc. 1996 ACM Conference on Progmmming Lan-guage Design and Implementation, Philadelphia, \nMay 1996. ACM. SIGPLAN Notices, 31(5). [49] Pnx. of the Seventh Programming Language Implemen-tation \nand Logic Prqmmming 1995. Springer-Verlag, 1995. LNCS. [50] Pmt. 23rd Symposium on Principles of Programming \nLanguages, St. Petersburg, Fla., January 1996. ACM Press. [51] Revised4 report on the algorithmic language \nScheme. Lisp Pointers, IV(3):1-55, July-September 1991. [52] Amr Sabry and Philip Wadler. A reflection \non call-by- value. In ICFP1996 [28], pages 13-24. [53] David A. Schmidt. Denotational Semantics, A Method- \nology for Software Development. Allyn and Bacon, Inc, Massachusetts, 1986. [54] Helmut Seidl. Integer \nconstraints to stop deforesta-tion. In Hanne Riis Nielson, editor, Pmt. 6th European Symposium on Progmmming, \nLinksping, Sweden, April 1994. Springer-Verlag. LNCS 1058. [55] Morten Heine Sorensen. A grammar-based \ndata-flow analysis to stop deforestation. In Sophie Tison, ed-itor, Proc. l%ees in Algebra and Pmgmmming, \npages X$-~~8%linburgh, UK, April 1994. Springer-W&#38;g. [56] Morten Heine Sprrensen, Robert Gliick, \nand Neil D. Jones. Towards unifying partial evaluation, deforestil-tion, supercompilation, and GPC. In \nDonald San-nella, editor, Pmt. 5th European Symposium on Prv-gmmming, pages 485-500, Edinburgh, UK, April \n1994. Springer-Verlag. LNCS 788. [57] Guy L. Steele. Rabbit: a compiler for Scheme. Techni-cal Report \nAI-TR-474, MIT, Cambridge, MA, 1978. [58] Akihiko Takano and Erik Meijer. Shortcut deforesta-tion in \ncalculational form. In Peyton Jones [46], pages 306-313. [59] Peter Thiemann. Cogen in six lines. In \nICFP1996 [28], pages 180-189. [60] Peter Thiemann. Implementing memoization for partial evaluation. In \nKuchen and Swierstra [35], pages 198- 212. [61] Mads Tofte. Compiler Generators. What They Can Do, What \nThey Might Do, and What They Will Probably Never Do. Springer-Verlag, 1990. [62] VaIentin F. Turchin. \nA supercompiler system based on the language RefaI. SIGPLAN Notices, 14(2):46-54, February 1979. [63] \nPhilip L. WadIer. Deforestation: Transforming pro-grams to eliminate trees. Theoretical Computer Sci-ence, \n73(2):231-248, 1990.   \n\t\t\t", "proc_id": "258915", "abstract": "One of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a source-to-source transformation for high-level languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several meta-computation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a run-time code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreter-based experiments with a source-to-source version of the partial evaluator before building a realistic compiler which generates object code automatically.", "authors": [{"name": "Michael Sperber", "author_profile_id": "81100100127", "affiliation": "Wilhelm-Schickard-Institut f&#252;r Informatik, Universit&#228;t T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "PP14044834", "email_address": "", "orcid_id": ""}, {"name": "Peter Thiemann", "author_profile_id": "81100458917", "affiliation": "Wilhelm-Schickard-Institut f&#252;r Informatik, Universit&#228;t T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "PP39043747", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258935", "year": "1997", "article_id": "258935", "conference": "PLDI", "title": "Two for the price of one: composing partial evaluation and compilation", "url": "http://dl.acm.org/citation.cfm?id=258935"}