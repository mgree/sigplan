{"article_publication_date": "05-01-1997", "fulltext": "\n Data Distribution Support on Distributed Shared Memory Multiprocessors Rohit Chandra, Ding-Kai Chen, \nRobert Cox, Dror E. Maydan, Nenad Nedeljkovic Jennifer M. Anderson Silicon Graphics Computer Systems \nDigital Western Research Lab Mountain View, CA Palo Alto, CA Abstract Cache-coherent multiprocessors \nwith distributed shared memory are becoming increasingly popular for parallel comput-ing. However; obtaining \nhigh pe$ormance on these machines mquires that an application execute with good data locality. In addition \nto making efiective use of caches, it is often necessary to distribute data structures across the local \nmemories of the pro-cessing nodes, thereby reducing the latency ofcache misses. We have designed a set \nofabstractions for performing data distribution in the context of explicitly parallel programs and implemented \nthem within the SGZ MZPSpro compiler system. Our system incorporates many unique features to enhance \nboth pto- grammability and performance. We address the former by pro- viding a very simple ptvgmmming \nmodel with extensive support for emor detection. Reganiing performance, we carefully design the user \nabstractions with the wuierlying compiler optimitations in mind, we incorporate several optimization \ntechniques to gen- erate eJEcient code for accessing distributed data, and we pro- vide a tight integration \nof these techniques with other optimizations within the compiler Our initial experience sug-gests that \nthe directives are easy to use and can yield substantial performance gains, in some cases by as much \nas a factor of 3 over the same codes without distribution. 1 Introduction Cache-coherent shared memory \nmultiprocessors are attractive for parallel programming since they provide a uniform view of memory, \nwith inter-processor communication specified implic- itly through load and store operations to shared \nmemory loca- tions. To enable scaling beyond bus-based machines, modern multiprocessors typically contain \na large number of processing nodes each with one or more processors and a portion of main memory connected \nthrough a scalable interconnection network. This class of machines is termed CC-NUMA (cache-coherent \nnon-uniform memory access) and includes several commercially available systems, such as Convex Exemplar, \nSequent SliNG, and SGI Origin-2000. Although global memory is uniformly accessible by all the pro- cessors, \nremote memory latencies are typically much larger than local memory latencies (e.g., 2-3 times on the \nOrigin-2000). Permission to make digital/hard copy of part or all this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan-tage, the copyright notice, the title of the publication and its date appear, and notice is given \nthat copying is by permission of ACM. Inc. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee. PLDI 97 Las Vegas, NV, USA 0 \n1997 ACM 0-89791~907-6/9710006...$3.50 While processor caches can exploit temporal locality on both local \nand remote data, many applications, such as those without temporal reuse or with working sets larger \nthan the cache, are unable to benefit from cache locality alone. To obtain high per- formance on such \napplications, it is often necessary to distribute the data structures in the program so that the cache \nmisses of each processor are more likely to be satisfied from local rather than remote memory. In this \npaper we describe a set of data distribution abstractions for CC-NUMA multiprocessors. We have designed \nthese abstractions as a set of directives that allow the programmer to manually control the distribution \nof array data structures in explicitly parallel programs. We provide a small set of abstrac- tions that \nare easy to use. yet expressive enough for real applica- tions. Our directives are integrated with existing \nmechanisms for exploiting loop-level parallelism. Furthermore, the directives are designed keeping in \nmind the compiler s ability to generate effi- cient code for accesses to distributed data. Taken together, \nour abstractions enable the programmer to exploit loop-level paral- lelism and exercise fine control \nover both data distribution and computation scheduling. We have implemented these directives in the SGI \nMIPSpro7.1 commercial compiler system targeting the Origin-2000 multiprocessor. The primary concern when \ndistributing data on CC-NUMA architectures is that physical placement of data must be per- formed in \nunits of an operating system page (16 KB on the Ori- gin-2000). This can lead to false sharing at the \npage level when multiple data items that we wish to place in local memories of distinct processors happen \nto lie within the same page. In such situations it becomes necessary to move data objects within the \nvirtual address space of the process so that they now belong to distinct pages. However, moving data \nwithin the virtual address space of a process is not always legal and must be done carefully to be correct. \nPerforming these transformations safely and effi- ciently is a key component of our approach. The main \ncontributions of this paper are the following. We present a set of abstractions for controlling loop \nscheduling and data distribution on CC-NUMA architectures, and describe the implementation of these abstractions \nwithin a production com-piler. The unique aspects of our system include extensive error-detection features, \nsupport for separate compilation across multi- ple files, optimization techniques to generate efficient \ncode for distributed arrays, and tight integration of data distribution opti-mizations with other loop-level \noptimixations in the compiler. The rest of the paper is organized as follows. Section 2 provides an overview \nof the Origin-2000 architecture. Section 3 contains a detailed description of our directives for parallelism \nand data dis- tribution. In Section 4 we describe the implementation of these directives within the compiler \nand the runtime system. Section 5 describes our scheme for automatically propagating distribution directives \nacross subroutine calls. Section 6 outlines our support for error detection. Section 7 presents the optimizations \nper-formed by the compiler to efficiently support these constructs. We present performance results on \nsome applications in Section 8, discuss related work in Section 9, and offer conclud- ing remarks in \nSection 10. 2 Overview of the Origin-2000 Figure 1 shows the high-level architecture of the Origin-2000 \n[LL97]. %o 195 MHz MIPS RlOOOO processors together with a portion of the shared memory are connected \nthrough a hub, and multiple such nodes are connected together in a hypercube through a switch-based interconnect.. \nEach processor has sepa- rate on-chip instruction and data caches (32KB each, 32-byte line size), and \na unified off-chip cache (typically l-4MB, 128-byte line size), all two-way set associative. The hub \nmaintains cache coherence across processors using a directory-based invalidation protocol. The latency \nof a miss to local memory is about 70 pro- cessor cycles, while a miss to the remote memory of another \npro- cessor ranges from I10 to 180 cycles. The operating system on the Origin-2000 supports data alloca- \ntion at the granularity of a physical page (16Kbytes). It provides a default first-touch page allocation \npolicy where a page is allo- cated from the local memory of the processor incurring the page fault, as \nwell as an optional round-robin policy where pages are allocated in a round-robin fashion across processors. \n 3 Programming Model In this section we provide a detailed description of our program- ming model [SGI96]. \nWe first give a brief overview of the direc- tives to exploit loop-level parallelism. We then describe \nour data distribution directives, with particular emphasis on our approach to overcoming the page granularity \nlimitations. Finally, we describe the mechanisms to control the scheduling of parallel loops. 3.1 Existing \nDirectives for Loop Parallelization The code segment below illustrates our commonly used directive for \nspecifying parallelism. c$doacross local(i) shared(n, A) doi= I,n A(i) = 2*i en&#38;o  The a&#38;cross \ndirective specifies that all the iterations of the i loop can be executed concurrently. The local and \nshared clauses specify that each iteration should have a local instance of the variable i, while the \nvariables A and n should be shared across all the iterations and can be accessed directly through shared \nmem- ory references. It is assumed that all iterations are fully concur- rent with an implicit barrier \nat the end of the doacross loop. The . . . I Switch-Based Interconnect Figure l. Origin-2000 ArchitectureOverview. \npartitioning of iterations across processors may be controlled with an optional schedtype clause, and \nadditional synchroniza-tion, if required, must be explicitly specified by the programmer using constructs \nsuch as locks and barriers. We also support the parallel execution of nested loops through the nest clause \non a doacross directive. For instance, in the fol- lowing example all iterations in the (i,jj iteration \nspace can be executed concurrently. c$a oacross nest (i,j) local (i,j) shared (m,n,B) do i = 1, n do \nj = I.m B(j.i) = i+j enaifo enaiio  3.2 Extensions for Data Distribution Our data distribution support \nfocuses on regular distributions of arrays in Fortran. Our directives are similar to the basic data dis- \ntribution directive in HPF (High Performance Fortran) [Lov93, KLS+94, HPM-971, as shown below. real*8 \nA(m, n, . . .) c$distribute A (<dist>, <dist>, . . .) <dist> may be. one of block, cyclic, cyclic(<expr>), \nor *, with the same meaning as in HPF. Distribution may be specified for both global and local arrays, \nincluding dynamically sized local arrays. There are no restric- tions on the size or the assigned number \nof processors for an array dimension. The number of processors in each distributed dimension is determined \nat program start-up time, which enables the same executable to run with different number of processors. \nFurthermore, distribute directive can contain an optional onto clause specifying how the total number \nof processors should be assigned across multiple distributed array dimensions. We consider two possible \nways of supporting array distribution. The simple approach attempts to map each processor s portion of \nthe distributed array onto physical pages allocated from within the local memory of that processor (regular \ndistribution). This approach is simple and easy to implement, but is limited by the underlying page granularity. \nThe aggressive approach reorga-nizes the layout of the array within the virtual address space of the \nprocess, thereby overcoming the page-granularity limitations and guaranteeing the desired distribution \n(reshaped distribution). However, changing the layout of an array is not always legal since it may violate \nstorage layout assumptions in the program. Furthermore, it can incur additional array addressing overhead. \nThe choice between these two implementations depends on the size and layout of individual portions of \nthe distributed array and can vary for different arrays within the same program. For instance, in the \nexample POiPI :p2; , 1 1 I 1 1 real *8 A (1000, IOOO) , 1 * I 1 1 c$distribute A ( *, block) # 1 * # \n1 1 I 1 1 I 4 I I 1 1 an individual portion is a single contiguous piece of size 8*10 % bytes, assuming \nFortran style column-major layout and P proces- sors. Since an individual portion may be much larger \nthan a page, regular distribution may be sufficient for this array. On the other hand, in the example \nI PO__--------- real*8 A (lC#, IOOO) PI c$distribute A (block *) __-_---------P2 .--__------- an individual \nportion is still 8*106/P bytes, but due to the col- umn-major layout of the array each contiguous piece \nis only 8*103/p bytes, significantly less than a page. In such situations reshaped data distribution \nis desirable. As we can see, the choice between regular and reshaped distribu- tion may depend on array \nbounds and the number of processors, which are typically symbolic values not known at compile time. Rather \nthan leave the implementation choice to the compiler, we allow the programmer to choose between regular \nand reshaped distributions using the directives &#38;distribute c$distribute-reshape A program can contain \nboth distribute and distribute-reshape arrays. A particular array, however, must be declared either dis-tribute \nor distribute-reshape (or neither, of course) for the dura- tion of the program, and cannot be dynamically \nswitched between the two kinds of distributions. 3.2.1 Restrictions on Data Reshaping Array reshaping \nis legal only under certain conditions related to storage and sequence assumptions about that array and \nto passing arrays as subroutine arguments. First, a reshaped array cannot be equivalenced to another \narray, either explicitly through an equivalence statement or implicitly through multiple declarations \nof a common block. If an array in a common block is reshaped, then each declaration of that common block \nmust (a) contain an array at the same offset within the com- mon block, (b) declare that array with the \nsame number of dimensions, each of the same size, and (c) specify the same reshaped distribution for \nthe array. The second restriction concerns passing a reshaped array as an argument to a subroutine. If \nthe entire array is passed as an argu- ment, then the number of dimensions and the size of each dimen- \nsion in the actual and the formal parameter must match exactly. On the other hand, passing an element \nof a reshaped array is treated as passing a portion of a distributed array (the size. and shape of the \nportion depend on the array distribution, as illus- trated by the example below). The called procedure \ntreats the incoming parameter as a non-distributed (i.e., standard Fortran) array. The declared bounds \non the formal parameter are required not to exceed the size of the distributed array portion passed in \nas the actual argument. For instance, in the example real*8A(lOOO) c$distribute-reshape A (cycfic(5)) \ndo i=l,lt390,5 call mysub (A(i)) enddo end subroutine mysub (X) real *8 X(5) . . . end the main program \ncalls subroutine mysub once for each portion (5 elements) of the reshaped array A. The declared size \nof the for- mal parameter X in mysub can therefore be at most 5 elements. Given these restrictions, it \ncan be difficult for the programmer to correctly reshape arrays in a large application. We make this \ntask easier in multiple ways. First, the programmer does not need to specify reshaped distributions on \nformal parameters of subrou- tines; distribute-reshape directives need to be supplied only at array definition \npoints, and the compiler automatically propa-gates them down a call chain (Section 5). Second, we provide \nextensive error-detection support (both compile-time and mnt- ime) to enforce the consistency of reshaped \narrays (Section 6). Finally, we provide a rich set of intrinsics for traversing the indi- vidual portions \nof a distributed array [SG196]. Taken together, these features make it significantly easier to distribute \ndata in large applications.   3.3 Dynamic Data Redistribution Dynamic data redistribution may be useful \nwhen an application needs a different distribution on the same array in two distinct phases of the program. \nWe therefore provide the following redis-tribute directive 1. We treat call sub(A) as passing the entire \narray A, and call sub(A(O)J and call sub (A(i)) as passing an element ofA to sub. Original loop (with \naffinity for a distributed array of size n): c$doacross afjhity (i) = &#38;ta(A(s*i+c)) do i=LB. UB, \nstep  Block distribution: (b = dop = 0, P-l do i = max(LB Pb c min(UB,[(p+ )sb-C- ]),step fT]), - \n Cyclic distribution: (s=l. the expressions for s>l are omitteq for brevity) dop=O,P-1 doi=LB+ ((p-LB-c)modP),UB,P \n  Block-cyclic: (cyclic(k)) (UB)s+c+l -1 kP 1 min(UB, (P+l)k-c-1 +kPj s Figure 2. Implementing Affinity \nScheduling. c$redist bute A (<dist>, <dist>, . . .) 4 Implementation of Basic Features where <dist> may \nbe one of block, cyclic, cyclic(<expr>), or *. We now describe the implementation of our progmmming Redistribute \nis an executable statement that has global effect. We model. We focus on reshaped arrays and give only \na brief over- do not allow redistribution of reshaped arrays - as discussed later, view of the implementation \nof the other features. dynamic reshaping severely restricts compiler optimizations of reshaped data distributions \nand can result in inefficient code.  4.1 Affinity Scheduling 3.4 AfGnity Scheduling We implement affinity \nscheduling by transforming the loop into a doubly (or triply) nested loop, where the outermost loop Along \nwith data distribution we allow the user to control the traverses the processors in the distributed dimension, \nand the scheduling of the iterations of a parallel loop across processors inner loop(s) traverse each \nprocessor s elements of the distrib- through an optional affinity clause on a doacross directive (see \nuted array. Our transformations, similar to those described by Section 3.1). Hiranandani et al. [HKM+93], \nare reproduced in Figure 2. real *8 A(n) &#38;distribute A(block)  4.2 Regular Distributions c$doacross \nlocal (i) shared(n, A) afinity (i)=data(A(i)) do i=l,n Regular data distribution only affects the underlying \npage alloca- A(i) = i*i tion. Its implementation, therefore, is simply an operating system e&#38;do call \nto allocate the physical pages for each portion of the distrib- uted array from within the local memory \nof the corresponding The affinity clause specifies that iteration i should execute on the processor. \nThis system call is the only OS support required toprocessor that contains element A(i) of the distributed \narray A implement both regular and reshaped data distribution, and it (the distribution can be either \nregular or reshaped). Simple linear overrides the default first-touch page allocation policy. expressions \nsuch as A(p*i+q) are also allowed, but p and q must be literal constants, with p non-negative. This system \ncall for page allocation is generated within the com- piled code for local arrays. For common block arrays \nthe object In summary, a programmer can use the distribute and file is annotated with information about \narray dimensions anddistribute-reshape directives to distribute the key arrays in the their specified \ndistributions. At program start-up time the runtime program, along with the afiity clause on ~YXUYOSS \nto loops library reads this information and makes the appropriate operat- ensure that each loop iteration \nand the data referenced in that ing system call. Finally, a redistribute directive also translates iteration \nare collocated on the same processor. into a runtime call to remap the pages for the array. 4.3 Reshaped \nDistributions The implementation of the distribute-reshape must guarantee the desired distribution and \nis allowed to change the layout of the array in the virtual address space. Rather than simply padding \nthe array portions up to the next page boundary, we transform a reshaped array into a processor-array \nwith each element of the processor-array in turn pointing to the array elements belonging to that processor \n(see Figure 3). For a single reshaped dimension a reference A(i) is transformed as shown in Table 1 (the \ntransformed reference is shown in C syntax). Our transformation of a reference is similar to that described \nby Anderson, Amarasinghe, and Lam [AAL95]. The first dimension of each reference is the index into the \nprocessor array (the processor containing element i), while the remaining dimensions index into the processor \ns portion of the array to locate element i. In this table N is the size of the array dimension, P is \nthe number of processors assigned to that dimension, b is the size of a processor portion for a block \ndistribution, and k is the chunk size for a cyclic(k) distribution. The transformation for a reshaped \narray distributed in multiple dimensions is a simple composition of this basic scheme. ~~ Table 1. Itansformation \nof Reshaped Array References This implementation scheme allows us to manage storage for reshaped arrays \nin a space-efficient fashion. Since each proces- sor s portion of a distributed array can be allocated \nindepen-dently of the other portions, each processor allocates a pool of storage from the shared heap, \nmaps the pages for this pool of storage from within its local memory, and allocates its potion of each \nreshaped array from this pool of memory. We can therefore avoid padding the ends of each portion up to \na page boundary. As shown in Table 1, a transformed reference to a reshaped array contains integer divide \nand remainder operations (div and mod). These operations are extremely expensive on modem micropro- cessors \n- our optimizations for reducing their impact is the sub- ject of Section 7.   5 Propagating Reshape \nDirectives As mentioned in Section 3, when a reshaped array is passed as an argument to another subroutine, \nwe automatically propagate the distribute-reshape directive to the called subroutine. There are two main \nissues m supporting this feature. The first is that it should work correctly with separate compilation, \nso that we can propagate the directive to subroutines that may be defined in other, separately compiled \nfiles. We do so in a way that is sim- ilar to a C++ template instantiation mechanism [Str94]. Briefly, \nfor each user source file, the compiler maintains a shadow file Processor Original Array , , , PO : Pl \n: P2 : P3 Reshaped Array : : : Array i Be! Ezi: P3 by I ,I -I Figure 3. Transformation of a Reshaped \nArray. into which it inserts an entry each time a reshaped array is passed as an argument to a subroutine. \nWhen the linker is called with all the object files, it first invokes a pre-linker, which examines all \nthe object files and the corresponding shadow files, and propa- gates each directive to the called subroutine. \nSince the pre-linker is called with all the object files, it has a global view of the user program and \ncan successfully propagate directives across files. The second issue is that a subroutine may be invoked \nfrom multi- ple places with a different distribution on the same parameter. Compiling the subroutine \nto dynamically handle multiple incom- ing distributions may result in substantial runtime overhead. Instead, \nwe clone a copy of the subroutine for each distinct com- bination of distribute-reshape directives on \nits parameters. Although this results in code expansion, the generated code is more efficient, since \neach cloned copy of the subroutine can be optimized at compile time for the particular combination of \nincoming distributions. Furthermore, in practice we expect the number of distinct distributions to be \nsmall. The actual cloning of the subroutine is implemented as follows. The compiler updates the shadow \nfile with (a) the name of each subroutine defined in the file (along with any distribute-reshape directives \npropagated into the subroutine), and (b) the name of each subroutine call in the file that contained \na reshaped array as an actual argument. The pre-linker examines all the shadow files, matching subroutine \ninvocations with subroutine definitions with respect to distribute-reshape directives on the parameters. \nFor each invocation without a matching definition the pre-linker inserts a request into the shadow file \nfor the desired subroutine instance and invokes the compiler again on that file. The com- piler in turn \nexamines the corresponding shadow file and creates the requested clones (if any) of each subroutine. \nFinally, we avoid unnecessary cloning by removing requests from the shadow file for each definition that \ndoes not have a matching call. This is useful when the user removes a subroutine invocation from the \nprogram, leaving a now redundant request in the shadow file from the previous compilation. Overall, this \nmechanism allows us to transparently clone multi- ple instances of a subroutine, one for each incoming \ncombination of distribute-reshape directives on the subroutine parameters. We do so by transparently \nreinvoking the compiler at link time to compile a new clone of a subroutine. The first compilation of \na program can potentially result in several recompilations as the disrribute-reshape directives are propagated \nall the way down the call graph of the program. However, subsequent compilations incur a recompilation \nonly if a new cloning request is generated.  6 Error-Detection Support Our error-detection support is \ngeared towards enforcing the restrictions on reshaped arrays outlined in Section 3.2.1. The mechanisms \nfor error detection include compile-time, link-time, and runtime checks. Among the compile-time checks, \ndisallow- ing the equivalencing of reshaped arrays is straightforward and performed during the compilation \nof each subroutine. Checking that common blocks are declared consistently across all files is performed \nat link time -we annotate the shadow file (described in Section 5) with an entry for every declaration \nof a common block, along with the information (shape, size, and distribution) about each reshaped array \n(if any) in that common block. When the pre-linker is invoked, it reads all the entries and verifies \nthat all declarations of each common block (one for each subroutine that uses it) are consistent with \neach other. Specifically, it verifies that each reshaped array in a common block appears at the same \noffset within the common block and with the same shape, size, and distribution in all declarations of \nthat comnton block. This rule flags a link-time error only for inconsistent common blocks with reshaped \narrays -common blocks without reshaped arrays are not affected. Our optional runtime checks are useful \nwhen a reshaped array (or portion thereof) is passed as an argument to a subroutine. They are used to \nverify that the shape and size of the declared forma1 parameter are consistent with those of the incoming \nactual argu- ment. These checks are implemented as follows. At each subrou- tine invocation with a reshaped \narray (or a portion thereof) passed as an argument, we take the address being passed in and use it as \nan index into a runtime hash table to store information about the actual argument. For the entire array \nwe store the shape and size of the array, while for an array portion we store just the size of that portion. \nUpon entry to each subroutine, we take the incoming value for each parameter and use it as an index into \nthe hash table described above. If an entry is found then the incom- ing argument is either a reshaped \narray or a portion thereof. In either case, we compare the information found in the hash table with the \ndeclared shape and size of the formal parameter, gener- ating a nmtime error in case of a mismatch. The \noverhead for the runtime checks includes (i) adding an entry to the hash table each time a reshaped array \nis passed as an argu- ment to a subroutine (and removing the entry upon return from the subroutine), \nand (ii) performing a lookup of the hash table at subroutine entry for each parameter to the subroutine \n(we opti- mize the second part to some extent by performing this lookup only for formal parameters that \nare declared to be arrays). Overall, these runtime checks are extremely useful in catching errors in \nreshaped distributions. Such errors are otherwise extremely difficult to detect, since they are not easily \ndistin-guished from other algorithmic or coding errors. 7 Optimizing Reshaped Array References As we \nsaw earlier in Section 4.3, computing the address of an element of a reshaped array requires expensive \ndiv and mod operations - on a R10000 processor a 32-bit integer divide takes about 35 cycles. Optimizing \nthese operations is crucial for high performance and is the subject of this section. Given a reference \nto an element of a distributed array, the div operation determines which processor contains the element, \nwhile the mod operation determines the offset within that proces- sor s portion of the array. When the \nreference is known to be local to the processor then the div operation can simply be replaced by the \nprocessor number. When the offsets within each processor s portion are strictly increasing then the mod \noperation can be replaced by an addition. Our basic optimization approach, therefore, consists of loop \ntiling and Reeling to create inner loops that reference a single portion of the reshaped array and do \nnot require any div and mod operations; this is similar to the approaches proposed by Anderson, Amarasinghe, \nand Lam [AAL95] and Hiranandani et al. [HKM+93]. Furthermore, sev-eral additional optimizations are necessary \nto ensure that other compiler optimizations are not adversely affected by reshaped arrays; we describe \neach of these below. 7.1 Tiling and Peeling for Reshaped Arrays The tiling transformations to create \nkernels over portions of a distributed array are exactly the same as those presented for affinity scheduling \nin Section 4.1. The original loop is trans-formed into either two (for block and cyclic distributions) \nor three (for cyclic(k) distributions) loops. In either case, the outer loop, called the processor tile \nloop, indexes through the proces- sors in the distributed dimension, while the inner loop(s) traverse \nthe elements within a portion of the distributed array A. We illustrate these with a few examples. real*8 \nA(n) c$distribute-reshape A(block) do i = I, n A(i) = i em20 Before optimization, this loop has the form \ndoi= I,n A[ti][i%b] = i enddo After the tiling optimization, the loop is transformed into dop = 0. P-l \nlb = MAX((p*b+l), I) ub = MIN(((p+l)*b). n) locafjuiex = lb % b do i = lb, ub A[p][local&#38;dex] = i \nlocal-index = local-index + I enddo en&#38;o As we can see, the mod operation has been moved out of the \ninnermost loop so that we now need only P rather than n mod operations, while the div operation has disappeared \naltogether. If there are references to elements in the neighboring portion of a distributed array, then \nwe peel iterations from the inner loop. For instance, in the code do i = 2, n-1 A(i) = (A(&#38;I) + A(i) \nt A(i+l)Y3 eddo we peel one iteration from each end of the inner loop, which again results in an innermost \nloop without any div or mod opera- tions. dop=O,P-1 lb = MAX(p*b, 2) ub = MIN(((p+l)*b-I. n-1) f(lb LE. \nub) then A[(lb-lyb][(lb-l)%b] = (A[lbIb][lb%b] + A[(lb-lyb][(lb-l)%b] + A[(lb+l)/b][(lb+l)%b])/3 endif \nif(lb+l .LE. ub-I) local&#38;uiex = lb % b do i= Ib+l, ub-I A[p][local-index] = (A[p][local-index-l] \n+ A[p][local-index] + A[p][local-index+l]Y3 local-index = local-index + I enddo if (lb .L?Y ub) then \n ANub-lyb][(ub-I)%b] = (A[ub/b][ub%b] + A[(ub-iJfb][(ub-l)%b] + A[(ub+l)/b][(ub+l)%b])/3 endif enddo \n We increase the applicability of the above transformations in three ways. First, besides parallel loops \nwith data affinity, we apply them to other loops that reference reshaped arrays, such as serial loops \nand parallel loops without user-declared affinity. For each loop we examine the reshaped array references \nin that loop and use a simple heuristic to determine the desired tiling and peeling of the loop that \nwill result in the fewest div and mod operations. However, this optimization is not always possible: \nwhereas tiling for reshaped arrays is always legal for (a) parallel loops, and (b) serial loops with \nblock distributions (since the iter- ations are still executed in the same order as the original loop), \nfor cyclic or cyclic(k) distributions this transformation changes the order in which the iterations are \nexecuted and is therefore subject to data dependence constraints. Second, for loops such as doi= I,n \nA(i+c*k) = (c is a constant and k is a loop-invariant variable) we skew the loop by (c*k). This converts \nreferences like A(i+c*k) to A(i), which enables subsequent tiling and peeling. Third, having tiled a \nloop based on one array, we can simulta-neously optimize references to other reshaped arrays that match \nthe first array in size and distribution. These optimizations are successful only when the index expres- \nsions of reshaped references are of the simple form s*i+c, with s and c being literal constants. More \ncomplex index expressions are not optimized and always incur the indexing overhead. 7.1.1 Loop Interchange \nWe can tile multiple loops in a nest if they reference reshaped arrays. In the code real*8 A(n, n) c$distribute-reshape \nAtblock, block) doj= 1, N doi=l,N A(ij) = i+j enddo enddo since A is distributed in both dimensions, \nwe tile each of the j and i loops, generating a loop structure as follows: do pj = 1, Pi-1 doj=... do \npi = I, Pi-1 do i = . . . . . . enddo enddo enddo enddo We try to interchange the j and pi loops so \nthat the processor tile loops @i and pj) are outermost and the actual data loops (i and j) are innermost. \nWe can then hoist div and mod operations within the pi loop out of the j loop, which results in fewer \nsuch opera- tions. This interchange is always legal for parallel loops within the doacross-nest directive \n(see Section 3. l), but is subject to the same legality constraints as normal loop interchange for sequen- \ntial loops. 7.2 Hoisting and CSE of Array Index Expressions After performing the optimizations described \nabove, we exam- ined the generated code for some small examples, but found that the address computation \nfor reshaped arrays was still inefficient. There were three main reasons. The first problem was the inability \nof the scalar optimizer to per- form code hoisting and common subexpression elimination (CSE) across \nthe index expressions of reshaped arrays. A reshaped reference generates indirect loads (from the processor \narray) and div and mod operations, all of which are, in general, unsafe operations that cannot be speculated. \nAs a result, they cannot be moved out of an if clause or a do loop. Since these operations are always \nsafe in the context of reshaped arrays, we fixed this problem by hoisting them out of loop nests and \ncondi- tional statements directly during the transformation of reshaped array references. The second \nproblem was poor CSE across index expressions in the presence of subroutine calls. For instance, the \nindex expres- sion for a reshaped array may contain references to the block size 6 of a distributed dimension. \nAlthough 6 is initialized at the array definition point and never modified, the compiler must assume \nthat the subroutine call could modify 6 and must reload it from memory after each call. We solved this \nproblem by marking such variables as constant within the compiler internal representation. The third \nproblem was that subsequent optimizations of the transformed code were affected by the indirect loads \ngenerated for accesses to reshaped arrays, since an indirect memory refer- ence can in general be aliased \nto any other data in the program. We solved this problem by marking the base symbol of the array as aliased \nonly to other references using the same base array.  7.3 DIV/MOD using Floating-Point Arithmetic While \nan integer divide takes about 35 cycles on the MIPS RlOOOO processor and is not pipelined, the corresponding \nfloat-ing-point operation takes 11 cycles. We therefore simulate the integer divide in software using \nthe floating-point unit. In addi- tion to reducing the cost of the basic div and mod, the software scheme \noften enables additional hoisting of the reciprocal of the operands. 7.4 Integration with Other Optimizations \nWithin the overall MIPSpro7.1 compiler, we process reshaped arrays as part of the loop-nest optimizer. \nThe order of the optimi- zations is as follows: 1. Loop skewing, tiling, interchange, and peeling for \nreshaped arrays. 2. Regular loop-nest optimizations (e.g., fusion, fission, inter- change, cache and \nregister tiling). 3. Transformation of reshaped array references, including opti- mization of reshaped \nreferences in the inner loops, and hoist- ing indirect loads and div and mod operations. 4. CSE across \nindex expressions of reshaped arrays.  The primary benefit of this approach is that the loop transforma- \ntions for reshaped arrays are done early, presenting the code in a convenient form for the regular loop-level \noptimizer. Since the processor tiles are in place and often interchanged to be the out- ermost within \na loop nest, the loop-nest optimizer can perform single processor optimizations as it would on normal, \nsingle pro- cessor code. Furthermore, by delaying the transformation of reshaped arrays references we \nmaintain them in a reasonable form during the regular loop-level transformations.  8 Performance Results \nWe present performance results from one benchmark application (LU from the NAS 2.1 parallel benchmark \nsuite) and two compu- tation kernels (matrix transpose and 2-D convolution). Our results are obtained \non a l28-processor Origin-2000 with a 4MB secondary cache per processor and 16 GB of memory. In addition \nto measuring the overall performance, we use the hardware counters on the MIPS RlOOOO to analyze our \nresults [ZLT+96]. 8.1 LU We converted the original MPI version of the LU benchmark to a shared memory \nversion using doucrcxs directives for parallelism and distribute-reshape directives for data distribution. \nThe pri- mary data structures are two Cdimensional arrays that we dis- tribute in a (*,block,block, *) \nfashion, based on the parallel partitioning of the program. I I Optimization lime (sets) Reshape, no \noptimizations 83.91 Reshape, tile and peel 53.26 Reshape, tile and peel, hoist 46.23 Original code without \nreshaping 45.71 Table 2. Effect of Reshape Optimizations. We first evaluate just the basic effectiveness \nof our reshaped array addressing optimizations. We do so by comparing the code with and without reshaping \nrunning on a single processor, so that we can focus on just the reshaped array addressing overhead. As \nshown in Table 2, the basic code with reshaping, compiled at -03 but without optimizations for reshaped \narrays, ran very poorly. Tiling and peeling for reshaped arrays helped tremendously, fol-lowed by hoisting \nof indirect loads and div and mod operations, which enabled CSE across index expressions. Most importantly, \nthe final version of the code ran nearly as efficiently as the origi- nal code without reshaping. Figure \n4 shows the relative performance of four versions of LU on the class C input (the arrays are of size \n(5,166,166,166)). Two instances are without any data distribution directives: one uses the first-touch \npolicy and the other uses round-robin page place- ment. (Data is initialized in parallel in this application.) \nThe other two instances are with regular and reshaped data distribu- tion directives respectively. As \nshown by the results, the performance of this application is determined by bandwidth rather than latency \nconsiderations. Since all four versions spread the data across the machine (although differently), they \nall achieve good performance. The parallel initialization of data is quite effective, with first-touch \noutperforming both round-robin and regular distribution (the lat- ter two exhibit nearly identical performance). \nFurthermore. only reshaping can effectively provide the desired (*,block,block, *) distribution and obtains \nthe best performance on 64 processors, although the improvements over first-touch are modest (6%). All \nfour instances exhibit superlinear speedup because (a) the large data set size (360MB) exceeds the amount \nof memory on a single node (about 250MB) resulting in remote memory refer- ences even in the uniprocessor \ncase, and (b) the larger aggregate 80 . . Linear Speedup t -A-Reshaped * Round Robin 64 + Regular +-First \nTouch t .' ,:' ,. , B 48 .: 3 i/ ,.. ,.. ,: 8. .: v) ,.: 32 ,:. _: .: .: _.. ,: 16 . ... .: .., _., \nOk* 0 16 32 48 64 Number of Processors NAS LU (Class C) Figure 4. Performance of NAS-LU (Class C). cache \nat high processor counts improves the cache hit rate. We counted the cache misses using the hardware \ncounters on the pro- cessor, and found that the total number of secondary cache misses decreased by a \nfactor of three from 1 to 16 processors. 8.2 Matrix Transpose We next examine the performance of a parallel \nmatrix transpose, in which we iterate several times over the following loop nest. c$distribute A (*.block), \nEfblock, *j c$doacross locul(i,jJ do i=l,m do j=l.m A(j,i) = B(i,jj enddo enddo Figure 5 presents the \nspeedup of four versions of the code over a serial version on an input of size 5000x5000. The four versions \nare first-touch allocation, round-robin placement, regular distri- bution, and reshaped distribution. \nData initialization is per-formed serially in this application. The matrix with the (block, *) distribution \ncannot be distributed properly without reshaping; consequently both first-touch policy and regular distribution \nresult in most of the data being allocated from within the mem- ory of one or two nodes. These nodes \nbecome a bottleneck for accesses to this matrix, which results in extremely poor perfor- mance. Round-robin \nplacement outperforms these two versions by distributing the data uniformly across the nodes and better \nuti- lizing the network bandwidth. The reshaped version obtains the best performance: by reshaping even \nthe row-distributed matrix so that each processor s portion is contiguous in memory we are able to ensure \nthat almost all misses are satisfied locally. A secondary effect is the reduction in TLB misses with \nreshaping: since the reshaped version uses all the data in a page, it uses much fewer pages compared \nto the 140 Linear Speedup +-Reshaped -4 Round Robin 120 + Regular -e-First Touch 100 t 0 0 16 32 48 \n64 80 Number of F mcessors Matrix Transpose 5000x5000 Figure 5. Performance of Matrix Ikauspose. other \nversions. Using the hardware performance counters, we found that at 32 processors the round-robin version \nspends about 15% of its time in TLB misses, while the reshaped version needs less than half that time. \nAt moderate processor counts the reshaped version outperforms the round-robin version by 30-508. At a \nlittle over 64 processors the reshaped version starts to improve superlinearly as the appli- cation gets \nincreasing benefits from cache reuse. The two arrays require a total of 400 MB of memory while our system \nhas 4 MB of secondary cache per processor. At large processor counts this data begins to fit within the \nsecondary caches, and the number of cache misses decreases substantially for the reshaped version (a \ntotal of 244M misses compared to 702M for the round-robin ver-sion at P=96). One might expect similar \neffects for the other ver- sions, but random cache interference will prevent them from fully utilizing \nthe cache until much larger processor counts. Data reshaping ensures that each processor s portion is \ncontiguous in virtual memory, and the OS page-coloring algorithm tries to ensure that contiguous virtual \naddresses do not map into conflict- ing physical addresses. As a result, cache interference is greatly \nreduced for the reshaped version of the code.  8.3 2-D Convolution We next examine the performance of \na 2-D convolution. As shown in the code below, we exploit either one or two levels of parallelism with \ndistributions of (*,block) and (bfock,bfock) respectively. c$distribute A( *, block), B( *, block) c$doacmss \nlocal (i,j) af/iiry(i) = data(A(ij)) do j=2,n-I do i=2,n- I A(i,j)=(B(i-I j)+B(ij-I)+B(i,j)+B(ij+l)+B(i+l,j))6 \nend do end do  80 0 16 32 48 64 80 96 Number of Processors 2D Convolution 1OOOx1ooO, (*,blcck) Figure \n6. Performance The following exploits two levels of parallelism c$disttibute A(block, block). B(block, \nblock) c$ainzcmss nest (i,i) local (i,jj a@&#38;y(j,i)=&#38;ta (A(i,j)) do j=2,n- I do i=2,n-I A(ij)=(B(i-I,j)+B(i,j-l)+B(i,j)+B(i,j+l)+B(i+l,j))/5 \nend do en&#38;o We present results with the first-touch policy, round-robin page placement, regular \ndistribution, and reshaped distribution. Due to serial initialization all thedatais placed on just a \nfew nodes with the first-touch scheme. Our results are relative to the serial ver- sion of the code and \npresented on two different input sets: 1000x1000 (Figure 6) and 5000x5000 (Figure 7). With a single level \nof parallelism we obtain successive improve- ments over first-touch allocation with regular, round-robin, \nand then reshaped distribution. Regular distribution benefits are due to memory locality alone. On the \nsmaller input round-robin placement outperforms regular distribution for increasing num-ber of processors, \neven though the cache miss behavior remains unchanged. We believe this is because the round-robin policy \nresults in more uniform page distribution and better bandwidth utilization as compared to regular distribution. \nFor the 1000x1000 case the performance of regular distribution actually becomes chaotic at large processor \ncounts - each processor s por- tion becomes progressively smaller, increasing page-level false sharing. \nSince a page requested by multiple processors is simply allocated from within the local memory of the \nprocessor to last request the page, this often results in a poor distribution. Reshap- ing reduces degradation \ndue to edge effects at page bound- the aries and achieves the best performance. However, these edge effects \nare less important for larger problem sizes -for the 5000x5000 case (the left graph in Figure 7) regular \ndistribution performs as well as reshaped distribution. With 96 processors the application data (400MB) \nfits completely within the processor 48 32 16 0 0 16 32 48 64 80 % Number of Processors 2D Convolution \n1OOOx1OCQ (block.block) of 2-D Convolution (1OOOx1000). caches andboth round-robin and regular distribution \noutperform the reshaped version (which incurs some reshaping overhead). When we exploit two levels of \nparallelism both first-touch and regular distribution perform equally poorly, since with two-dimensional \nblocks the array layout suffers severely from false sharing over both cache lines and pages. Round-robin \nplacement is an improvement due to better bandwidth utilization, but reshaping is the only option for \nsuch distributions and, as expected, performs much better. For larger processor counts in the 5000x5000 \ncase, two-level parallelism outperforms the single level due to better communication/computation ratio. \nFinally, the data set size (16MB for the smaller input, 400MB for the larger input) exceeds the 4MB cache \nof a single processor, which leads to superlinear speedups for higher processor counts with larger aggregate \ncache size.   8.4 Summary Overall, these performance results illustrate the need for provid- ing both \nregular and reshaped distribution. As demonstrated by the convolution code on the bigger input, regular \ndistribution is perfectly adequate when the individual portions of a distributed array are large. Reshaped \ndistributions, on the other hand, are useful when data needs to be distributed at a finer granularity, \nsuch as the (block, block) distribution in the convolution code. Besides improving memory locality, reshaping \nthe layout of an array can also improve cache behavior by improving spatial locality and reducing false \nsharing across cache lines.  9 Related Work Several approaches have been proposed in the literature \nto improve data locality, including operating-system-based page migration, compiler-based data distribution, \nas well as program- ming languages such as HPF. 160 Linear Speedup 44 -A-Reshaped * Round Robin 128 \n+ Regular 112 -a-First Touch t s- % 3 5 80 ,.. Number of Processors 2D Convolution SOOOxSooO, (*,block) \nFigure 7. Performance Operating-system-based approaches [VDG+96] use mntime sta-tistics, such as TLB \nor cache misses, to identify the processor incurring the most cache misses to a page and migrate the \npage to the local memory of that processor. While transparent to the user, the main limitations of this \napproach are that it migrates data at the granularity of an entire page, incurs nmtime overhead and must \ntherefore be conservative, and infers application behavior only indirectly through per-page statistics. \nCompiler-based approaches [AAL95, AnL93, GuB92, KeK95, SSP+95] to data distribution are typically integrated \nwith the automatic detection of loop-level parallelism. These approaches are attractive since they are \nentirely transparent to the user and, in contrast to OS-guided approaches, perform data distribution \nbased on a static analysis of the data reference patterns in the program. However, many codes are not \namenable to such auto- matic compiler analysis and require explicit programmer inter- vention. Finally, \nprogramming languages such as HPF [Lov93, KLS+94, HPF2-971. Fortran-D [HKK+91], and Vienna Fortran [CMZ92], \nprovide a variety of data distribution directives. We limit our comparison to HPF, the most widely known \nof these languages. HPF was originally designed as an alternative to message pass- ing for programming \ndistributed address space machines such as the IBM SP-2. In HPF the user typically writes a serial program \nand annotates it with data distribution information. Based on this distribution, the compiler partitions \nand schedules the program for parallel execution and manages the communication and syn- chronization \nbetween processors. Programming in the message passing paradigm can be quite difficult, and the primary \nattrac- tion of HPF is a familiar shared memory programming model with all the communication managed \nautomatically by the imple- mentation. Our model is similar to HPF in many respects: our data distribu- \ntion directives are similar to those provided in HPF, our doacross directive for expressing loop-level \nparallelism is the same as the 160 - Linear Speedup 44 -A-Reshaped 128 -+ Round Robin + Regular 112 \n-+ First Touch 2 %- B 5 80- 64- 48 32 16 0 0 16 32 48 64 80 % Number of Processors 2D Convolution SOOOxSOOO, \n(block.block)   of 2-D Convolution (5OOOx5000). independent directive in HPF, while our @r&#38;y clause \ncorm-sponds to the on directive, recently introduced in HPF-2.0 [HPM-971. Furthermore, our restrictions \nrelated to storage and sequence association on reshaped arrays are similar to those imposed on distributed \narrays in HPF. In contrast to HPF, however, our model was designed for cache- coherent shared address \nspace machines and therefore differs in several fundamental ways. When programming in HPF on a dis- tributed \naddress space machine, data distribution is necessary to obtain parallel execution. In contrast, on a \nCC-NUMA machine data distribution is performed as an optional optimization over and above exploiting \nparallelism. On such machines, because of global cache-coherent shared memory, traditional multiprocess- \ning codes written with loop-level parallel directives continue to run correctly in parallel without requiring \ndata distribution. Because remote latencies are relatively low (compared with those on distributed address \nspace machines), these programs often run well without any modifications. Data distribution on these \nmachines is purely a performance enhancement to over- come the additional penalty of a remote reference \nover a local reference. For our abstractions, therefore, a simple programming model and an efficient \nimplementation were absolutely critical: if we were not careful, the overhead of managing data distribution \ncould very quickly outweigh any gains from memory locality. The differing goals are reflected in our \nspecific designs. First, we provide two distinct types of data distribution, regular and reshaped, whereas \nHPF provides only reshaped distributions. We believe it is important to have both types of distributions \non CC- NUMA architectures: distributions that do not have page granu- larity problems can use regular \ndistribution, thereby avoiding the legality restrictions and the array addressing overhead of reshaped \narrays, while distributions that suffer from page granu- larity limitations can use reshaping. Second, \nwe provide only the core data distribution constructs, and omit most of the advanced HPF features. For \ninstance, we do not support dynamic redistribution of reshaped data. Also, we provide a simpler model \nfor passing reshaped arrays as argu- ments to subroutines. HPF allows mismatched distributions on actual \nand formal parameters, and it provides three different kinds of distribution directives on formal array \nparameters. The implementation must remap the data (if necessary) at tuntime if the directive is prescriprive, \nthe called subroutine to compile accept any incoming distribution if the directive is transcriptive, \nand assert the specified distribution of the actual parameter if the directive is descriptive. In contrast, \nwe automatically propagate distributions down the call chain, cloning routines as necessary. An HPF implementation \ncannot always propagate distributions at compile time since HPF permits dynamic redistribution of data. \nBecause of these simplifications in our programming model, the distributions of all reshaped arrays are \nalways known at compile time. This makes it much more likely that we will optimize the implementation \nof reshaped array references.  10 Conclusions In this paper we have presented a set of abstractions \nfor distribut- ing data on CC-NUMA multiprocessors. Our abstractions are easy to use and provide a simple \nmodel for data reshaping. Our unique error-detection features and propagation of distribution directives \nacross subroutine calls enable the user to safely use reshaped distributions. Efficiency has been a prime \nconcern, and we have carefully avoided features that could hinder compiler optimizations of data distribution \n(such as dynamic data reshap- ing). We have implemented these abstractions within the SGI MIPSpro7.1 \ncompiler and incorporated several optimization techniques to improve the efficiency of the generated \ncode. Our initial experience has been encouraging: the abstractions allow the user to focus on identifying \nthe desired distribution based on the characteristics of the application, and leave the implementa- tion \ndetails to the compiler. Acknowledgments: We thank Jeff McDonald who helped us with the LU application, \nChau-Wen Tseng who participated in the initial stages of this work, and Seema Hiranandani and the anonymous \nreferees who offered many useful comments on ear- lier drafts of this paper. Bibliography [AAL95] J. \nM. Anderson, S. P Amarasinghe and M. S. Lam. Data and Computation Transformations for Multiprocessors. \nIn Proceedings of rhe Fifrh ACM SIGPL4N Symposium on Principles and Practice of Parallel Programming, \npages 166-178, July 1995 [AnL93] J. M. Anderson and M. S. Lam. Global Optimizations for Parallelism and \nLocality on Scalable Parallel Machines. In Proceedings of SIGPLAN 93 Conference Programming L4ww Design \nand Tzplementation, pages 112-125, June 1993. [GuB92] M. Gupta andP Banejee. Demonstration of Automatic \nData Partitioning Techniques for Parallelizing Compilers on Multicomputers. In Transactions on Parallel \nand Distributed Systems, 3(2), pages 179- 193, March 1992. [HPF2-971 High Performance Fortran Language \nSpecification, Version 2.0, January 1997. Available by anonymous ftp from softlibriceedu: /pub/HPF. [HKK+91] \nS. Hiranandani, K. Kennedy, C. Koelhel, U. Kremer, and C-W. Tseng. An Overview of the Fortran-D Programming \nSystem. In LJ. Banejee. D. Gelemter, A. Nicolau, and D. Padua, editors, Languuges and Compilers for Parallel \nComputing, Springer-Verlag, August 1991. [HKM+93] S. Hiranandani, K. Kennedy, J. Mellor-Crummey. and \nA. Sethi. Advanced Compilation Techniques for Fortran D. Technical Report CRPCTR93338, Center for Research \non Parallel Computation, October 1993. [KeK95] K. Kennedy and U. Kremer. Automatic Data Layout for High \nPerformance FORTRAN. In Proceedings of Supetcomputing 95, December 1995. [KLS+94] C. H. Koelbel, D. B. \nLoveman, R. S. Schreiber, G. L. Steele Jr., and M. E. Zosel. The High Performance Fortran Handbook. MIT \nPress, Cambridge MA, 1994. [LL97] J. Laudon and D. Lenoski. The SGI Origin: A ccNUMA Highly Scalable \nServer. In Proceedings of the 24th Annual International Symposium on Computer Architecture, May 1997. \n[Lov93] D. B. Loveman. High Performance Fortran. In IEEE Parallel and Distributed Technology, Systems \n&#38; Applications, l(l), pages 25-42, February 1993. [SSP+95] T. J. ShefIler. R. Schreiber, W. Pugh, \nJ. R. Gilbert and S. Chatterjee. Efficient Distribution Analysis via Graph Contraction. In Proceedings \nof the Eighth Workshop on Languages and Compilers for Parallel Processing, August 1995. [SGI96] Silicon \nGraphics, MIPSpro Fortran-Programmer s Guide, Document number 007-236 l-004,1996. [Str94] B. Stroustrup. \nThe Design and Evolution of C++. Addison Wesley, Reading MA, 1994. [VDG+96] B. Verghese, S. Devine, A. \nGupta, and M. Rosenblum. Operating System Support for Improving Data Locality on CC-NUMA Compute Servers. \nIn Proceedings of the 7th International Conferences on Architectural Support for Programming Languages \nand Operating Systems (ASPLOS VII), pages 219-289, October 1996. [ZLT+96] M. Zagha, B. Larson, S. Turner, \nM. Itzkowitz. Performance Analysis Using the MIPS RlOOOO [CMZ92] B. Chapman, P Mehrotra, and H. Zima. \nProgramming Performance Counters. In Supercomputing 96, in Vienna Fortran. Scienrijic Programming, 1 \n(l), pages Pittsburgh PA, November 1996. 3 l-50, Fall 1992.  \n\t\t\t", "proc_id": "258915", "abstract": "Cache-coherent multiprocessors with distributed shared memory are becoming increasingly popular for parallel computing. However, obtaining high performance on these machines mquires that an application execute with good data locality. In addition to making efiective use of caches, it is often necessary to distribute data structures across the local memories of the processing nodes, thereby reducing the latency of cache misses.We have designed a set of abstractions for performing data distribution in the context of explicitly parallel programs and implemented them within the SGI MIPSpro compiler system. Our system incorporates many unique features to enhance both programmability and performance. We address the former by providing a very simple programmming model with extensive support for error detection. Regarding performance, we carefully design the user abstractions with the underlying compiler optimizations in mind, we incorporate several optimization techniques to generate efficient code for accessing distributed data, and we provide a tight integration of these techniques with other optimizations within the compiler Our initial experience suggests that the directives are easy to use and can yield substantial performance gains, in some cases by as much as a factor of 3 over the same codes without distribution.", "authors": [{"name": "Rohit Chandra", "author_profile_id": "81409593520", "affiliation": "Silicon Graphics Computer Systems, Mountain View, CA", "person_id": "P248192", "email_address": "", "orcid_id": ""}, {"name": "Ding-Kai Chen", "author_profile_id": "81451596162", "affiliation": "Silicon Graphics Computer Systems, Mountain View, CA", "person_id": "P67347", "email_address": "", "orcid_id": ""}, {"name": "Robert Cox", "author_profile_id": "81542211256", "affiliation": "Silicon Graphics Computer Systems, Mountain View, CA", "person_id": "PP14107867", "email_address": "", "orcid_id": ""}, {"name": "Dror E. Maydan", "author_profile_id": "81100138889", "affiliation": "Silicon Graphics Computer Systems, Mountain View, CA", "person_id": "P70229", "email_address": "", "orcid_id": ""}, {"name": "Nenad Nedeljkovic", "author_profile_id": "81100331038", "affiliation": "Silicon Graphics Computer Systems, Mountain View, CA", "person_id": "P208186", "email_address": "", "orcid_id": ""}, {"name": "Jennifer M. Anderson", "author_profile_id": "81452613603", "affiliation": "Digital Western Research Lab, Palo Alto, CA", "person_id": "PP14046415", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258945", "year": "1997", "article_id": "258945", "conference": "PLDI", "title": "Data distribution support on distributed shared memory multiprocessors", "url": "http://dl.acm.org/citation.cfm?id=258945"}