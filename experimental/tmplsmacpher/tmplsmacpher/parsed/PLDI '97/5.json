{"article_publication_date": "05-01-1997", "fulltext": "\n Reverse Interpretation + Mutation Analysis = Automatic Retargeting Christian S. Collberg Department \nof Computer Science The University of Auckland Private Bag 92019 Auckland, New Zealand. collberg@cs.auckland.ac.nz \nAbstract There are three popular methods for constructing highly retargetable compilers: (1) the compiler \nemits abstract ma-chine code which is interpreted at run-time, (2) the compiler emits C code which is \nsubsequently compiled to machine code by the native C compiler, or (3) the compiler s code-generator \nis generated by a back-end generator from a formal machine description produced by the compiler writer. \nThese methods incur high costs at run-time, compile-time, or compiler-construction time, respectively. \nIn thii paper we will describe a novel method which promises to significantly reduce the effort required \nto retar- get a compiler to a new architecture, while at the same time producing fast and effective compilers. \nThe basic idea is to use the native C compiler at compiler construction time to discover architectural \nfeatures of the new architecture. From this information a formal machine description is produced. Given \nthis machine description, a native code-generator can be generated by a back-end generator such as BEG \nor burg. A prototype Automatic Architecture Discovery Unit has been implemented. The current version \nis general enough to produce machine descriptions for the integer instruction sets of common RISC and \nCISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86. The tool is completely \nautomatic and requires minimal input from the user: principally, the user needs to provide the internet \naddress of the target machine and the command- lines by which the C compiler, assembler, and linker are \ninvoked. 1 Introduction An important aspect of a compiler implementation is its re- tayetabiiity.For \nexample, a new programming language whose compiler can be quickly retargeted to new hard-ware/operating \nsystem combinations is more likely to gain widespread acceptance than a language whose compiler re-quires \nextensive retargeting effort. In this paper we will briefly review the problems as-sociated with two \npopular approaches to building retar-getable compilers (C Code Code Generation (CCCG), and Permission \nto make digital/hard copy of part or all this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan-tage, the copyright notice, \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nACM. Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires \nprior specific permission and/or a fee. PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-69791-907-619710006...$3.50 \nSpecification-Driven Code Generation (SDCG)), and then propose a new method (Self-F&#38;targeting Code \nGeneration (SRCG)) which overcomes these problems. 1.1 C Code Code Generation The back-end of a CCCG \ncompiler generates C code which is compiled by the native C compiler. If care has been taken to produce \nportable C code, then targeting a new archi-tecture requires no further action from the compiler writer. \nFurthermore, any improvement to the native C compiler s code generation and optimization phases will \nautomatically benefit the compiler. A number of compilers have achieved portability through CCCG. Examples \ninclude early versions of the SRC Modula-3 compiler [2] and the ISE Eiffel com-piler [7]. Unfortunately, \nexperience has shown that generating truly portable C code is much more diicult than it might seem. Not \nonly is it necessary to handle architecture and operating-system specific differences such as word-size \nand alignment, but also the idiosyncrasies of the C compilers themselves. Machine-generated C code will \noften exercise the C compiler more than code written by human program- mers, and is therefore more likely \nto expose hidden problems in the code-generator and optimizer. Other potential prob- lems are the speed \nof compilation and the fact that the C compiler s optimizer (having been targeted at code pro-duced by \nhumans) may be ill equipped to optimize the code emitted by our compiler. Further complications arise \nif there is a large semantic gap between the source language and C. For example, if there is no clean \nmapping from the source language s types to C s type, the CCCG compiled program will be very diffi- cult \nto debug. CCCG-based compilers for languages supporting garbage collection face even more difficult problems. \nMany collec-tion algorithms assume that there will always be a pointer to the beginning of every dynamically \nallocated object, a requirement which is violated by some optimizing C com-pilers. Under certain circumstances \nthis will result in live objects being collected. Other compelling arguments against the use of C as \nan intermediate language can be found in [3]. In some CCCG compilers the most expensive part of compila-tion \nis compiling the generated C code. For this reason both SRC Mod&#38;-3 and ISE Eiffel8re moving away \nfrom CCCG. ISE Eiffel now UBBSa bytecode interpreter for fast turn-around time and reserves the CCCG-based \ncompiler for final code generation. SRC Modula-3 uow supports at least two SDCG back-ends, based on gee \nand burg. 1.2 Specification-Driven Code Generation The back-end of a SDCG compiler generates intermedi-ate \ncode which is transformed to machine code by a specification-driven code generator. The main disadvantage \nis that retargeting becomes a much more arduous process, since a new specification has to be written \nfor each new ar- chitecture. A gee [17] machine specification, for example, can be several thousand lines \nlong. Popular back-end gener- ators such as BEG [6] and burg [9] require detailed descrip- tions of the \narchitecture s register set and register classes, as well as a set of pattern-matching rules that provide \na map ping between the intermediate code and the instruction set. See Figure 15 for some example rules \ntaken from a BEG machine description. Writing correct machine specifications can be a difficult task \nin itself. This can be seen by browsing through gee s machine descriptions. The programmers writing these \nspec- ifications experienced several different kinds of problems: Documentation/Software Errors/Omissions \nThe most serious and common problems seem to stem from documentation being out of sync with the actual \nhardware/software implementation. Examples: . . . the manual says that the opcodes are named movsx.. \n. , but the assembler . . . does not accept that. (i386) WARNING! There is a small i860 hardware limitation \n(bug?) which we may run up against . . . we must avoid using an addu instruction to perform such comparisons \nbecause . . . This fact is documented in a footnote on page 7-10 of the . . Manual (i8601. Lack of Understanding \nof the Architecture Even with the access to manuals, some specification writers seemed uncertain of exactly \nwhich constructs were legal. Examples: Is this number right? (mips) , Can this ever happen on i386? \n(i3861, Will divxu ahways work here? (i386) . Hardware/Software Updates Often, updates to the hardware \nor systems software are not immediately re-flected by updates in the machine specification. Exam-ple: \nThis has not been updated since version 1. It is certainly wrong. (ns32k) . I Lack of Time Sometimes \nthe programmer knew what needed to be done, but simply did not have the time .to implement the changes. \nExample: This INSV pattern is wrong. It should . . . Fixing this is more work than we care to do for \nthe moment, because it means most of the above patterns would need to be rewritten, . . . (Hitachi H8/300) \n. I Note that none of these comments are gee specific. Rather, they express universal problems of writing \nand maintaining a formal machine specification, regardless of which machine- description language/back-end \ngenerator is being targeted. 1.3 Self-Retargeting Code Generation In this paper we will propose a new \napproach to the design of retargetable compilers which combines the advantages of the two methods outlined \nabove, while avoiding most of their drawbacks. The basic idea is to use the native C compiler to discover \narchitectural features of the new target machine, and then to use that information to automatically produce \na specification suitable for input to a back-end generator. We will refer to this method as Self-Retargeting \nCode Gen- eration (SRCG). More specifically, our system generates a number of small C programs which \nare compiled to assembly-code by the native C compiler. We will refer to these codes collectively as \nsamples, and individually as C code aamplea and aaaembly-code samples. The assembly-code samples are \nanalyzed to extract in-formation regarding the instruction set, the register set and register classes, \nthe procedure calling convention, available addressing modes, and the sizes and alignment constraints \nof available data types. The primary application of the architecture discovery unit is to aid and speed \nup manual retargeting. Although a complete analysis of a new architecture can take a long time (several \nhours, depending on the speed of the host and tar- get systems and the link between them), it is still \nl-2 orders of magnitude faster than manual retargeting. However, with the advent of SRCG it will alzo \nbecome possible to build self-retargeting compilers, i.e. compilers that can automatically adapt themselves \nto produce native code for any architecture. Figure 1 shows the structure of such a compiler ac for some \nlanguage A\". Originally de signed to produce code for architectures Al and A2, ac is able to retarget \nitself to the A3 architecture. The user only needs to supply the Internet address of an A3 machine and \nthe command lines by which the C compiler, assembler, and linker are invoked. The architecture discovery \npackage will have other po-tential uses as well. For example, machine-independent tools for editing of \nexecutables (EEL [13]), decompilation (Ci-fuentes [4]), and dynamic compilation (DCG [8]) all need access \nto architectural descriptions, and their retargeting would be simplified by automatic architecture discovery. \n2 System Overview and Requirements For a system like this to be truly useful it must have few re- quirements \n-of its users as well as of the target machines. The prototype implementation has been designed to be \nas automatic as possible, to require as little user input as pos- sible, and to require the target system \nto provide as few and simple tools as possible: We require a user to provide the intemet address of the \ntarget machine and the command-lines by which the C compiler, assembler, and linker are invoked. For \na wide range of machines all other information is deduced by the system itself, without further user \ninteraction. We require the target machine to provide an assembly- code producing C compiler, an assembler \nwhich flags illegal assembly instructions, a linker, and a remote execution facility such as rsh. The \nC compiler is used to provide assembly code samples for us to analyze; the assembler is used to deduce \nthe syntax of the as- sembly language; and the remote execution facility is used for communication between \nthe development and target machines. Obviously, other widely available languages such ae FORTRAN will \ndo eaually well. The manner in which error8 are reported ia unimportant; &#38;seem-bleru which simply \ncrash on the first error are quite acceptable for our purposes. al, Run BEG Figure 1: The structure \nof a self-retargeting compiler ac for some language A. The back-end generator BEG and the architecture \ndiscovery system are ntegrated into ac. The user can tell ac to retarget itself to a new architecture \nA3 by giving the Internet address of an A3 machine and the command lines by which the C compiler, assembler, \nand linker are invoked: ac -retargot -ARCH A3 -HOST kea.cs.auckland.ac.nz -CC cc -S -g -0 %O XI -AS as \n-0 X0 XI -LD .a.. If these requirements have been fulfilled, the architecture ing the statement a=b+c. \nThe compiler will not be able to discovery system will produce a BEG machine description optimize these \njumps away since they depend on variables completely autonomously. hidden within Init. Two assembly-code \nlabels correspond- The architecture discovery system consists of five ma-ing to Begin and End will effectively \ndelimit the instructions jor components (see Figure 2). The Generator generates C of interest. These \nlabels will be easy to identify since they code programs and compiles them to assembly-code on the each \nmust be referenced at least three times. The printf target machine. The Lezer extracts and tokenizes \nrelevant statement ensures that a dead code elimination optimization instructions (i.e. corresponding \nto the C statements in the will not remove the assignment to a. sample) from the assembly-code. The Preprocessor \nbuilds a data-flow graph from each sample. The Eztmctor uses 3.1 Tokenizing the Input this graph to \nextract the semantics of individual instructions and addressing modes. The Synthesizer, finally, gathers \nthe Before we can start parsing the assembly code samples, we collected information together and produces \na machine de- must try to discover as much as possible about the syntax scription, in our case for the \nBEG back-end generator. accepted by the assembler. Fortunately, most modern as-sembly languages seem \nto be variants of a standard nota-tion: there is at most one instruction per line; each instruc- The \nGenerator and Lexer tion consists of an optional label, an operator, and a lit of comma-separated arguments; \ninteger literals are prefixed The Generator produces a large number of simple C by their base; comments \nextend from a special comment-code samples. Samples may contain arithmetic and logi- character to the \nend of the line; etc. cal operations %iain() {int b=6 ,c=6,a=b+c;} , conditionals We use two fully automated \ntechniques for discovering mainO{int b=5,c=6.a=7; if (b<c)a=8;} , and procedure the details of a particular \nassembler: we can textually scancalls iin(){int b=S,a; a=P(b);)l. We would prefer to the assembly code \nproduced by the C compiler or we can generate a minimal set of samples, the smallest set such draw conclusions \nbased on whether a particular assemblythat the resulting assembly code samples would be easy to program \nis accepted or rejected by the assembler. For exam- analyze and would contain all the instructions produced \nby ple, to discover the syntax of integer literals (Which basesthe compiler. Unfortunately, we cannot \nknow whether a par- are accepted? Which prefixes do the different bases use? ticular sample will produce \ninteresting code combinations for Are upper, lower, and/or mixed case hexadecimal literalsa particular \nmachine until we have tried to analyze it. We accepted?) we compile main(){int a=1286;}l and scan the \nmust therefore produce as many simple samples as possible. resulting assembly code for the constant 1235, \nin all the com- For example, for subtraction we generate: ra=b-cl, a=a-mon bases. To discover the comment-character \naccepted by Ii , a=b-al, ra=a-a7, a=b-b , a=7-b1, TB=b-i , a=7-a , and the assembler we start out with \nthe assembly code produced a=a-71. This means that we will be left with a large number from main0 {} \n, add an obviously erroneous line preceded of samples, typically around 150 for each numeric type sup \nby a suspected comment character, and submit it to theported by the hardware. The samples are created \nby simply assembler for acceptance or rejection. instantiating a small number of templates parameterized \non These techniques can be used for a number of similar type (int,float,etc.) and operation (+,-,etc.). \ntasks. In particular, we discover the syntax of addressing The samples are compiled to assembly code \nby the native modes and registers, and the types of arguments (literals,C compiler and the Lexer extracts \nthe instructions relevant labels, registers, memory references) each operator can take. to our analysis. \nThis is non-trivial, since the relevant in- We also use assembler error analysis to discover the accepted \nstructions often only make up a small fraction of the ones ranges of integer immediate operands. On the \nSPARC, forproduced by the C compiler. example, we would detect that the add instruction s imme-Fortunately, \nit is possible to design the C code samples diate operand is restricted to [-409640951. to make it easy \nto extract the relevant instructions and Some assembly languages can be quite exotic. Theto minimize \nthe compiler s opportunities for optimizations Tera[Fi], for example, uses a variant of Scheme as its \nas-that could complicate our analyses. In Figure 3 a sepa-sembly language. In such cases our automated \ntechniquesrately compiled procedure Init initializes the variables a, will not be sufficient, and we \nrequire the user to provide ab, and c, but hides the initialization values from the com- translator into \na more standard notation. piler to prevent it from performing constant propagation. The main routine \ncontains three conditional jumps to two labels Begin and End, immediately preceding and follow- r .align \n2 Generate w (4 lw $9-g, 12o t($sp z) a sub $sp, 1280-0 . . . . . . . Lex lw $10'*~,1161it($sp'*~) Compile \n main0 ( IV $9, 12ONsp) mu1 $ll\"g,$!Y~,$lOr*~ int a,b,c; IV $10, 116($sp) * SW $11\"~,124y$sp\"g)  a=b*c; \nmu1 $11, $9, $10 SW $11, 124($sp) 1 Preprocess r-l . . . . . . . + (4 add $sp, 128 (9 L RULE Mult \nSYn-T(Creg/23<-mul<-Creg/2,reg/31, EX- the- Reg.b Reg.c -> Reg.a;  e(*, int, [ tract size COST I; \n e(reg,int,Carg(I)l), EMIT { e(reg,int,Carg(2)l) printf(\"mul\", . ..) ; CI: I I)>. ~ Figure 2: An overview \nof the major components of the architecture discovery system. The Generator produces a large number of \nsmall C programs (a) and compiles them to assembly on the target machine. The Lexer analyzes the raw \nassembly code (b) and extracts and tokenizes the instructions that are relevant to our further analyses \n(c). The Preprocessor deduces the signature of all instructions, and builds a data-flow graph (d) from \neach sample. The semantics of individual instructions (e) are deduced from this graph, and from this \ninformation, finally, a complete BEG specification (f) is built. tst1 21 /* init.h */ /* jeql Ll extern \nADD 6Ll.a 6Ll.b 6Ll.c jbr m int zl ,z2,z3, */ Ll:tstl 22 z4,z5,z6; #include init .h jeql L3 extern main \n0 { jbr [L41 void InitO ; int a, b, c; L3:tstI 23 InitfLa, &#38;b, &#38;cl ; jeql L5 /* init.c */ if \n(~1) goto Begin; jbr IL2]int zl,z2.z3, if (22) goto End; L5:tstl 24 z4,z5,z6; if (23) goto Begin; jeql \nL6 void Init(n,o,p) if (24) goto End; jbr . int *n,*o,*p; if (25) goto Begin; L6:tstl z5if (~6) goto \nEnd;{ jeql L7 zl=z2=z3=1; Begin : jbr m z4=zS=z6=1; a=b+c; L7:tstl 26 *n=-1 ; End: *o-313; *p=109; \nprintf ( %i\\n , a> ; jeql L2 exit (0) ; jbr L4 1 El 1 L2:addl3 -12(fp),-8(fp),-4(fp) L4: (4 (b) (c) \nFigure 3: A C Code sample and the resulting assembly-code sample for the VAX. The relevant instruction \n(add13) can be easily found since it is delimited by labels L2 and L4, corresponding to Begin and End, \nrespectively. 4 The Preprocessor The samples produced by the lexical phase may contain ir- regularities \nthat will make them difficult to analyze directly. Some problems may be due to the idiosyncrasies of \nthe archi- tecture, some due to the code generation and optimization algorithms used by the C compiler. \nIt is the task of the Pre- processor to identify any problems and convert each sample into a standard \nform (a data-jlow gmph) which can serve as the basis for further analysis. The data-flow graph makes \nexplicit the exact flow of in- formation between individual instructions in a sample. This means that, \nfor every instruction in every sample, we need to know where it takes its arguments and where it deposits \nits result(s). There are several major sources of confusion, some of which are illustrated in Figure \n4. For example, an instruction operand that does not ap-pear explicitly in the assembly code, but is \nhardwired into the instruction itself, is called an implicit argument. They occur frequently on older \narchitectures (on the x86, cltd (Figure 8) takes its input argument and delivers its result in register \n%eax), as well as more recent ones when proce- dure call arguments are passed in registers (Figure 4(a)). \nIf we cannot identify implicit arguments we obviously cannot maino{int b,c,a=b+c;} II mainO{int b,c,a=P(b,c);) \n-11 main()jint-a=P(34);) -11 main(){int-a=503<<a;} movl -12 (%ebp) . %eax Id Id call =oP st [%f p+-0x81 \n,%oO C%fp+-Orcl ,X01 .mll1,2 100, CXf p+-0x41 (4 push1 %eax - ldq $1, iM(Ssp) movl -8(%ebp) ,%eax call \nP,i add1 $1. 0, $2 push1 %eax mov 34,100 ldil $3, 503 call P st x00, C%fp-41 811 $3, $2, $4 add1 $8,%esp \nadd1 $4, 0, $4 movl xeax: , %e= s t q $4, 184&#38;p) movl %eax,-4(%ebp) (b) (4 (4 Figure 4: Examples \nof compiler- and architecture-induced irregularities that the Preprocessor must deal with. On the SPARC, \nprocedure actuals are passed in registers %oO, Xol, etc. Hence these are implicit input arguments to \nthe call instruction in (a). In (b), the x86 C compiler is using register Xeax for three independent \ntasks: to push b, to push c, and to extract the result of the function call. The SPARC mov instruction \nin (c) is in the call instruction s delay slot, and is hence executed befoR the call. In (d), finally, \nthe Alpha C compiler generated a redundant instruction raddl $4, 0, $4 . accurately describe the flow \nof information in the samples. As shown in Figure 4(b), a sample may contain several distinct uses of \nthe same register. Again, we need to be able to detect such register reuse or the flow of information \nwithin the sample can not be identified. 4.1 Mutation Analysis Static analysis of individual samples \nis not sufficient to ac-curately detect and repair irregularities such as the ones shown in Figure 4. \nInstead we use a novel dynamic tech-nique (called Mutation Analysti) which compares the exe-cution result \nof an original sample with one that has been slightly changed: Figure 5 lists the available mutations. \n4.2 Eliminating Redundant Instructions To illustrate this idea we will consider a trivial, but ex-tremely \nuseful, analysis, redundant instruction elimination. An instruction is removed from a sample and the \nmodified sample is assembled, linked, and executed on the target ma- chine. If the mutated sample produces \nthe same result as the original one, the instruction is removed permanently. This process is repeated \nfor every instruction of every sample. Even for trivial mutation like this, we must take spe- cial care \nfor the mutation not to succeed by chance. As illustrated in Figure 6, countering random successes often \ninvolves judicious use of register clobbering. The result of these mutations is a new set of simplified \nsamples, where redundant instructions (such as move Rl, RT, rno19, radd Rl, 0, Rll) have been eliminated. \nThese samples will be easier for the algorithms in Section 5 to analyze. They will also be less confusing \nto further mutation analyses. We will next consider three more profound preprocessing tasks (Live-Range \nSplitting, Implicit Argument Detection, and Register Definition/Use Computation) in detail. 4.3 Splitting \nRegister Live-Ranges Some samples (such as Figure 4(b)) will contain several rm- related references to \nthe same register. To allow further analysis, we need to split such register references into dis- tinct \nregions. Figure 7 shows how we can use the rename and clobber mutations to construct regions that contain \nthe smallest set of registers that can be renamed without chang- ing the semantics of the sample. Regions \nare grown back- wards, starting with the last use of a register, and continuing until the region also \ncontains the corresponding definition of that register. To make the test completely reliable the new \nregister is clobbered just prior to the proposed region, and each mutated sample is run several times \nwith different clob- bering values. 4.4 Detecting Implicit Arguments Detecting implicit arguments is \ncomplicated by the fact that some instructions have a variable number of implicit input arguments (cf. \ncall in Figure 4(a,c)), some have a vari-able number of implicit output arguments (the x86 s idivl returns \nthe quotient in &#38;sax and the remainder in /lo&#38;), and some take implicit arguments that are both \ninput and output. The only information we get from running a mutated sample is whether it produces the \nsame result as the origi- nal one. Therefore all our mutations must be correctness preserving , in the \nsense that unless there is something spe-cial about the sample (such as the presence of an implicit argument), \nthe mutation should not affect the result. So, for example, it should be legal to move an instruction \nIs before an instruction 11 as long as they (and any inter- mediate instructions) do not have any registers \nin common. Therefore the mutation in Figure 8(c) should succeed, which it does not, since %eax is an \nimplicit argument to idivl. The algorithm runs in two steps. We first attempt to prove that, for each \noperator 0 and each register R, 0 is independent of R, i.e. R is not an implicit argument of 0 (See Figure \n8(b)). For those operator/register-pairs that fail the first step, we use moue mutations to show that \nthe registers are actually implicit arguments (Figure 8(c)). Note that while this statement does not \nhold for arbitrary codes (where, for example, aliasing may be present), it does hold for our simple samples. \n(2) OPZ A:. A:, Ai (1) OPl A:, RI, A;  (3) OPS RI, A;, A; (I) Oh A;, RI, A; (2) 0~2 A:, A;, A; move((l> \n,after. (2)) (3) OP3 RI, A;. A; Original sample (1) OPr A:, RI,  (3) OP3 RI, A;, delete( (2) 1 Figure \n5: This table liits the available mutations: A: A; (1) OPl A:, RI, (2) OP2 A:, A:, nov -13,Rl (3) OP3 \nRI. A;, clobber(Rl,after,(2)) (1) OPl A:, RI, (2) OP2 A:, A;, (3) OPS R2, A;, rename(Rl,R2,(3))  A: \nA; A; A:  A; A;  we can move, copy, and delete (I) OPl A:. RI. A; (2) OP2 A:. A:, A;  (3) OP3 RI. \nA:, A: (1 ) OPl A:, RI, A; copy((l) ,after, (3)) (I) OPr A:, R3, A: (2) OP2 A:, A;, A;  (3) OPs ~3, \nA;, A: renameAll(R1, R3) instructions, and we can rename and clobber (overwrite) registers. The Ais \nare operands not affected by the mutations. To avoid a mutation succeeding (producing the same value \nas the original sample) by chance, we always try several variants of the same mutation. A mutation is \nsuccessful only if all variants succeed. Two variants may, for example, differ in the values used to \nclobber a register, or the new register name chosen for a rename mutation. Original Sample (I) ldq $1. \n184(S~p) (2) add1 $1, 0. $2 (3) ldil $3, 503 (4) (111 $3. $2, $4 (5) add1 $4, 0. $4 (6) ztq $4, \n184($sp)  (4 Cdelete((l))l (2) add1 $1, 0, $2 (3) ldil $3, 503 (4) 811 $3, $2, $4 (5) add1 $4, 0, \n$4 (6) stq $4. 184ftsp)  (b) fclobber($l,before, .-., delete((l))] ldiq $1. ldiq $2. ldiq $3, ldiq \n$4, (2) add1 $1, (3) ldil $3.  (1))) -28793 2558 137 -22136 0, $2 503 (4) 111 $3. $2. $4 (5) add1 \n$4. 0. $4 (6) stq $4, 184($rp) (cl  Cclobber(Sl,before,(l)), ..., delete((i))] ldiq $1. 234 ldiq $2, \n-45256 ldiq $3, 33135 ldiq 94. 97 (2) add1 $1, 0. $2 (3) ldil $3. 503 (4) all $3. $2. 14 (5) add1 \n$4, 0, $4 (6) stq $4, lM(S8p) (4  Figure 6: Removing redundant instructions. In this example we are \ninterested in whether instruction (1) is redundant. In (b) we delete the instruction; assemble, link, \nand execute the sample; and compare the result with that produced from the original sample in (a). If \nthey are the same, (1) is redundant and can be removed permanently. Unfortunately, this simple mutation \nwill succeed if register $1 happens to contain the correct value. To counter this possibility, we must \nclobber all registers with random values. To make sure that the clobbers themselves do not initialize \na register to a correct value, two variant mutations ((c) and (d)) are constructed using different clobbering \nvalues. Both variants must succeed for the mutation to succeed. 4.5 Computing Definition/Use Once implicit \nregister arguments have been detected and made explicit and distinct register uses have been split into \nindividual live ranges, we are ready to attempt our linal pre- processing task. Each register reference \nin a live range has to be analyzed and we have to determine which references are pure uses, pure definitions, \nand use-definitions. In-structions that both use and define a register are common on CISC machines (e.g. \non the VAX addl2 5,x-F increments RI by 5), but less common on modem RISC machines. The first occurrence \nof a register in a live-range must be a definition; the last one must be a use. The intermediate occurrences \ncan either be pure uses or use-definitions. For example, in the following x86 multiplication sample, \nthe first reference to register %edx (%edxr) is a pure definition, the second reference (%edxz) a use-definition, \nand the last one (%edxs) a pure use: main 0 { movl -8(%ebp) ,%edxr int a,b,c; imull -12(%ebp) ,%edxz \na=b*c; movl %edxs , -4(%ebp) 1 Figure 9 shows how we use the copy and rename muta- tions5 to create a \nseparate path from the first definition of a sNecessary register clobbers have been omitted for clarity. \nregister RI to a reference of RI that is either a use or a use- definition. If the reference is a use/definition \nthe mutation will fail, since the new value computed will not be passed on to the next instruction that \nuses RI. 4.6 Building the Data-Flow Graph The last task of the Preprocessor is to combine the gathered \ninformation into a data-flow graph for each sample. This graph will form the basis for all further analysis. \nThe data-flow graph describes the flow of information between elements of a sample. Information that \nwas im-plicit or missing in the original sample but recovered by the Preprocessor is made explicit in \nthe graph. The nodes of the graph are assembly code operators and operands and there is an edge A + B \nif B uses a value stored in or computed by A. For ease of identification, each node is labeled Nf, where \nN is the operator or operand, i is the instruction number, and a is N s argument number in the instruction. \nIn Figure 10(b), for example, $11; refers to the use of register $11 as the first argument to the third \ninstruction (mul). QLI .a is a data descriptor [ll] referring to the variable a at static nesting level \n1. Given the information provided by the various mutation analyses, building the graph for a sample S \nis straightfor- [renama(%eax,Xebx,(4)), [rename(%eax,Xebx,(4)), Crename(%ear,%ebx.(4)), clobber(%ebx,before,(4) \nrename(Xeax,%ebx,(3)), rename(%eax,%ebx,(3)), clobber(%ebx,before,(3) rename(%eax,%ebx,(2)), clobber(%ebx,before,(2)] \n(1) movl -12(Xebp), Xeax (1) movl -12CXebp)t %aax (1) movl -lZ(Xebp), %eax (1) movl -12CXebp)Xeax (2) \npU6h1 %66X (2) pUBhl%-(2) push1 %eax movl -123. %ebx (3) movl -B(%ebp) , %e6x (3) movl -8 (Xebp) , %sax \nmovl -123, Xebx (2) pU6hl (4) pU6hl %eax movl -123, %ebx (3) mov (3) movl (5) call P (4) pu6hl (4) \nPUB (4) purhl (6) add1 $8, %e6p (5) call P (5) call P (5) call P  (7) movl %eax,%edx (6) add1 $8, \n%e6p (6) add1 $8, %eBp (6) add1 $8. %e6p (8) movl %edx,-4(%ebp) (7) movl %eax,%edx (7) movl %eax, %edx \n(7) movl %eax.%edx  (8) movl Xedx,-4CXebp) (8) movl %edx,-4CXebp) (8) movl %edx.-I(%ebp) (4 W (4 (4 \nFigure 7: Splitting the sample from Figure 4(b). (a) shows the sample after the references to Xeax in \n(7) and (8) have been processed. Mutation (b) will fail (i.e. produce a value different from the original \nsample), since the region only contains the use of Xeax, not its definition. The region is extended until \nthe mutated sample produces the same result as the original (c), and then again until the results differ \n(d). Figure 4( e) shows the sample after regions have been split and registers renamed. crenalue(%ecx,%eax, \nCl)), Original Sample rename(%ecx,%eax,(2))1 (1) moo1 -8(Xebp),Lcx (1) movl -8(%ebp), %ebx (1) movl \n-8(%ebp),Xecx (2) movl %ecx,Xeax (2) movl %ecx.%eax  (3) cltd (3) cltd (3) cltd (4) idivl -lZ(%sbp) \n(5) movl %eax,-4(%ebp) (4) idivl -iZ(%ebp) (5) movl Xeax,-4(%ebp) (4) idivl -12(%6bp) (5) movl Xeru,-4(r.ebp) \n(4 (b) (4 Figure 8: Detecting implicit arguments. (a) is the original x86 sample. The (b) mutation will \nsucceed, indicating that cltd, idivl, and movl are independent of %ecx. The (c) mutation will fail, since \nXeax is an implicit argument to idivl. ward. A node is created for every operator and operand that occurs \nexplicitly in S. Instructions that were determined to be redundant (Section 4.2) are ignored. Extra nodes \nare created for all implicit input and output arguments (Section 4.4). Finally, based on the information \ngathered through live-range splitting (Section 4.3) and definition-use analysis (Section 4.5), output \nregister nodes can be con-nected to the corresponding input nodes. Note that once all samples have been \nconverted into data-flow graphs, we can easily determine the signatures of individual instructions. This \nis a first and crucial step to- wards a real understanding of the machine. From the graph in Figure 10(d), \nfor example, we can conclude that cltd is a register-to-register instruction, and that the input and \noutput registers both have to be Xeax. 5 The Extractor The purpose of the Extractor is to analyse from \nthe data- flow graphs and extract the function computed by each indi- vidual operator and operand. In \nthis section we will describe two of the many techniques that can be employed: Graph Matching which is \na simple and fast approach that works well in many cases, and Reverse Interpretation which is a more \ngeneral (and much slower) method. 5.1 Graph Matching To extract the information of interest from the \ndata-flow graphs, we need to make the following observation: for a binary arithmetic sample a = b @ c, \nthe graph will have the general structure shown in Figure 11(c). That is, the graph will have paths fi \nand PC originating in QLI. b and OLl. c and intersecting at some node P. Furthermore, there will be paths \nP,, and Pa originating in P and OLI . a that intersect at some node Q. P,, may be empty, while all other \npaths will be non-empty. P marks the point in the graph where @ is performed. The paths fi and PC represent \nthe code that loads the r-values of b and c, respectively. Similarly, Pa represents the code that loads \na s l-value. Q, being the point where the paths computing b @ c and a s l-value meet, marks the point \nwhere the value computed from b 8 c is stored in a. 5.2 Reverse Interpretation Graph Matching is fast \nand simple but it fails to analyze some graphs. Particularly problematic are samples that per- form multiplication \nby a constant, since these are often ex-panded to sequences of shifts and adds. The method we will describe \nnext, on the other hand, is completely general but suffers from a worst-case exponential time complexity. \nIn the following, we will take an interpreter I to be a function I :: Se16 x Prog x Envi. + EnvLIT,$ \nSam is a mapping from instructions to their semantic inter-pretation, Env a mapping from memory locations, \nregisters, etc., to values, and prog is the sequence of instructions to be executed. The result of the \ninterpretation is a new environ- ment, with (possibly) new values in memory and registers. The example \nin Figure 12(a) adds 5 to the value in memory location 10 @[lo]) and stores the result in memory location \n20. Ccopy((i> .after, Cl)), Ccopy((l),after,(l)), Original Sample copy((2) ,after, (1 )). rename(Rl,R2,((1 \n), (2)))l rename(Rl,R2,((1 ),(2 ),(3)))1 (1) OPr ..a, Rl , ... (1) OPi ..., RID, ... \\LI OPi ** RID, \n.-. (I ) OP1 *... RZD, .*. (1) ur1 ..a. (1 ) OPi . . . . R2 , ... (2) 0ps . . . , Rlv or v/r@?, . . \n. OPs **., n1--- (2 ) OP3 *a., R2u D, ... (2) OPs ..-, R2, ..a Op3 . . ., Rl Or o/D?, . . . (2) OP3 \n.a., R1\u00b0 D. ... (3) OP3 . . (3) Op3 . . ., RI Oru/D?, . . . OP4 ** -. a.. (3) OP3 . . .. R2, ..a (4) \n. . .. RI , (4) OP4 ..*, Rl , -3. (4) OP4 ..a, Rio. 0.. (4 I (b) I (cj Figure 9: Computing definition/use \ninformation. The fkst occurence of Ri in (a) is a definition (D), the last one a use The references to \nRI in (2) and (3) could be either uses, or use-definitions (U/D). The mutation in (b) will succeed iff \n(2) pure use. In (c) we assume that (b) failed, and hence (2) is a use-definition. (c) will succeed iff \n(3) is a pure use. /* MULOL1.a QL1.b QL1.c */ mainO{ int b=313,c=lOQ,a=b*c; 1 (I) lw $9. 12O(Ssp) (2) \nlv $10, 116wsp) (3) mu1 $11, $9, $10 (4) sv $11, 124CSsp)  (4 /* DIV 9Ll.a QL1.b 0Li.c */ mainO{ int \nb,c,a=b/c; 1 movl -8 (%ebp) ,%ecx movl %ecx,%eax cltd idivl -12(%ebp) movl %eax,-4(%ebp) (4 (4 Figure \n10: MIPS multiplication (a-b) and x86 division (c-d) samples and their corresponding data-flow graphs. \nRectangular boxes are operators, round-edged boxes are operands, and ovals represent source code variables. \nNote that, in (d), the implicit arguments to cltd and idivl are explicit in the graph (they are drawn \nunshaded). Also note that the edges %eaxi + idivlt and idivlg + %eaxi indicate that idivl reads and modifies \n%eax. All the graph drawings shown in this paper were generated automatically as part of the documentation \nproduced by the architecture discovery system. A reverse interpreter R, on the other hand, is a function \nlikelihood of an instruction having a particular semantics) that, given a program and an initial and \nfinal environment, can be used to implement an effective reverse interpreter. will return a semantic \ninterpretation that turns the initial The idea is simply to interpret each sample, choosing environment \ninto the final environment. R has the signature (non-deterministically) new interpretations of the operators \nand operands until every sample produces the required re-R :: Semi. x Envimx Prog x Env,t + Sem,,,,r. \nsult. The reverse interpreter will start out with an empty semantic mapping (Semi. = {}), and, on completion, \nwill re- In other words, R extends Semi. with new semantic inter- turn a Sat mapping each operator and \naddressing mode pretations, such that the program Prog transforms Envh to to a semantic interpretation. \nhvout * In the example in Figure 12(b) the reverse inter-The reverse interpreter has a small number of \nseman-preter determines that the add instruction performs addi- tic primitives (arithmetic, comparisons, \nlogical operations, tion. loads, stores, etc.) to choose from. RISC-type instruc-tions will map more \nor less directly into these primitives, 5.2.1 The Algorithm but they can be combined to model arbitrarily \ncomplex machine instructions. For example, add13e+.,.,.(a, b, c) = We will devote the remainder of this \nsection to a detailed dii storc(a, odd( load(b), load(c))) models the VAX add in- cussion of reverse \ninterpretation. Particularly, we will show struction, and madd,+,,,,,(a, b, c) = add (a, mul (b, c)) \nthe how a probabilistic search strategy (based on expressing the Figure 11: Data-flow graphs after graph \nmatching. (a) is MIPS multiplication, (b) is x86 division, and (d) is VAX addition. In (a), P = mu$, \nQ = SW!. Hence, on the MIPS, Iv is responsible for loading the r-values of b and c, mu1 performs multiplication, \nand SW stores the result. {add(t, y) = z + 9; load(u) = M[a]; store(a, b) = ~[a] t b), [store(lO, add(load(lO), \n5))0, {M[lO] = 7, M[20] = 9) {load(o) = I$]; store(a, b) = !![a] tb}, {add(z, y) = z + y; (b) R {wol \n= 7,~]20] = 91, + load(o) = H[a];[store(20, add(load(lO), S))], store(a, b) = $1 t b} {M[lO] = 7,M[20] \n= 12) Figure 12: (a) shows the result of an interpreter I evaluating a program [store(20, add(load(lO), \nS))], given an environment (I@O] = 7,H[20] = 9). The result is a new environment in which memory location \n20 has been updated. (b) shows the result of a reverse interpretation. R is given the same program and \nthe same initial and resulting environments as I. R also knows the semantics of the load and store instructions. \nBased on thii information, R will determine that in order to turn Envy into E~v,~, the add instruction \nshould have the semantics add(z, y) = z + y. MIPS multiply-and-add. Figure 14 lists the most important \nprimitives, and in Section 5.2.3 we will discuss the choice of primitives in detail. The example in Figure \n13 shows the reverse interpreta- tion of the sample in Figure lO(a-b). The data-flow graph has been converted \ninto a list of instructions to be inter- preted. In this example, we have already determined the semantics \nof the su and Iv instructions and the d,,,, reg-ister+offset addressing mode. All that is left to do \nis to fix the semantics of the mu1 instruction such that the result- ing environment contains g[QLl.a] \n= 34117. The reverse interpreter does this by enumerating all possible semantic interpretations of mul, \nuntil one is found that produces the correct Env.,t . Before we can arrive at an effective algorithm, \nthere are a number of issues that need to be resolved. First of all, it should be clear there will be \nan infinite number of valid semantic interpretations of each instruction. In the example in Figure 13, \nIII&#38;+,,, could get any one of the semantics mul,+,,.(a, b) = a * b, mul,+r,,(a, b) = a * b * 1, mul,+,,,(a, \nb) = ba * a/b, etc. Since most machine instructions have very simple semantics, we should strive for \nthe simplest (shortest) interpretations. Secondly, there may be situations where a set of sam-ples will \nallow several con6icting interpretations. To see this, let S=kain(){int b=?,c=i,a=b*c;)7 be a sample, \nand let the multiplication instruction generated from S be named mul. Given that Envi. = {b = 2, c = \nI} and Env,.t = {b = 2, c = 1, a = 2}, the reverse interpreter could reason-ably conclude that mul(a, \nb) = a/b, or even mul(a, b) = a -b + 1. A wiser choice of initialization values (such as b=34117, c=lOQ), \nwould avoid thii problem. A Monte Carlo algorithm can help us choose wise initialization value&#38; gen-erate \npairs of random numbers (a, b) until a pair is found for which none of the interpreter primitives (or \nsimple combi- nations of the primitives) yield the same result. Thirdly, the reverse interpreter might \nproduce the wrong result if its arithmetic is different from that of the target ar- chitecture. We use \nenquire [16] to gather information about word-sizes on the target machine, and simulate arithmetic in \nthe correct precision. A further complication is how to handle addressing mode calculations such as ai \nt d,,,. t (120, $sp) which are used in calculating variable addresses. These typically rely on stack- \nor frame pointer registers which are initialized out- side the sample. How is it possible for the interpreter \nto determine that in Figure 13 OLi .a, QLI. b, and OLl .c are Thanks to John Hamer for pointing this \nout. {d+=,,(a,b) = loadAddr(add(a,b)),lw,+.(a) = load(a), swecr,.(a,b) = store (a,b)}, {M[QLi.b] = \n313,H[QLl.c] = log}, {.&#38;.,.(a,b) = Zoodr(ddr(odd(a,b)), ~ iv.+.(a) = load(a), awe+&#38;, 6) = 9 tore \n(a, b), m&#38;+r,,(a, b) = mtbl (a, b)} Figure 13: Reverse interpretation example. The data-flow graph \nin Figure 10(a) has been broken up into seven primitive instructions, each one of the form result t operator \nt (argumenta). amf+,,C represents the register+o&#38;+et addressing mode. The oi s are pseudo-registers \nwhich hold the result of address calculations. To distinguish between instructions with the same mnemonic \nbut different semantics (such as 5ddl $I,%ecg and add1 -8(%ebp),%e&#38;? on the x86), instructions are \nindexed by their signatures. Iv=+., for example, takes an address as argument and returns a result in \na register. addressed as 124 + $sp, 120 + $sp, 116 + $sp, respectively, for some unknown value of $ap? \nWe handle this by initial- izing every register not initialized by the sample itself to a unique value \n($8~ t Is,). The interpreter can easily de- termine that a symbolic value 124+&#38;., must correspond \nto the address OLl . a after having analyzed a couple of samples such as rmainO{int a=1462;}l. However, \nthe most difficult problem of all is how the reverse interpreter can avoid combinatorial explosion. We \nwill address this issue next. 5.2.2 Guiding the interpreter Reverse interpretation is essentially an \nexhaustive search for a workable semantics of the instruction set. Or, to put it differently, we want \nthe reverse interpreter to consider all possible semantic interpretations of every operator and ad- dressing \nmode encountered in the samples, and then choose an interpretation that allows all samples to evaluate \nto their expected results. As noted before, there will always be an infinite number of such interpretations, \nand we want the interpreter to favor the simpler ones. Any number of heuristic search methods can be \nused to implement the reverse interpreter. There is, however, one complication. Many search algorithms \nrequire a frt-ness function which evaluates the goodness of the current search position, based on the \nresults of the search so far. This information is used to guide the direction of the con- tinued search. \nUnfortunately, no such fitness function can exist in our domain. To see this, let us again consider the \nexample interpretation in Figure 13. The interpreter might guess that mult+r,r(a, b) = mu2 (a, odd(lOO,b)), \nand, since 313 * 100 + 109 = 31409 is close to the real solu- tion (34117) the fitness function would \ngive this solution a high goodness value. Based on this information, the inter- preter may continue along \nthe same track, perhaps trying nml,+l,~(a,C) = nnrl (a, add(llO,b)). This is clearly the wrong strategy. \nIn fact, an unsuc-cessful interpretation (one that fails to produce the correct Env.,.*) gives us no \nnew information to help guide our further search. Fortunately, we can still do much better than a com-pletely \nblind search. The current implementation is based on a probabiltitic best-first smmh. The idea is to \nassign a l&#38;e&#38;hood (or priority) to each possible semantic interpre-tation of every operator and \naddressing mode. The inter-preter will consider more likely interpretations (those that have higher priority) \nbefore less likely ones. Note the differ- ence between likelihoods and fitness functions: the former \nare static priorities that can be computed before the search starts, the latter are evaluated dynamically \nas the search proceeds. Let I be an instruction, S the set of samples in which I occurs, and R a possible \nsemantic interpretation of I. Then the likelihood that I will have the interpretation R is L(S, I, R) \n= clM(S, I, R)+coP(S, R)+csG(I, R)+cdV(I, R) where the G S are implementation specific weights and M, \nP, G, and N are functions defined below. M(S, I, R) This function represents information gathered from \nsuccessful (or even partially successful) graph matchings. Let S be the MIPS multiplication sam-ple in \nFigure 11(a). After graph matching we know that the operators and operands along the fi path will be \ninvolved in loading the value of 9Ll. b. Therefore M(S, Iv,+., load) will be very high. Similarly, since \nthe paths from QLI. b and 0Ll.c convene in the muli node, mu1 is highly likely to perform a multiplication, \nand therefore M(S,mul,+~,,, nul ) will also be high. When available, this is the most accurate information \nwe can come by. It is therefore weighted highly in the L(S, I, R) function. P(S, R) The semantics of \nthe sample itself is another impor- tant source of information, particularly when combined with an understanding \nof common code generation id- ioms. As an example, let S= iiin()(int b,c,a=b*c) . Then we know that the \ncorresponding assembly code sam-ple is much more likely to contain load, store, mu2 , add, or shiftlett \ninstructions, than (say) a diu or a branch. Hence, for this example, P(S, mu2) > P(S, add) B- P(S, branch). \n G(I,R) The signature of an instruction can provide some clues as to the function it performs. For example, \nif I takes an address argument it is quite likely to perform a load or a store, and if it takes a label \nargument it probably does a branch. Similarly, an instruction (such as swc+=,. in Figure 11(a) or addl3,+.,.,. \nin Fig- ure 11(d)) that returns no result is likely to perform (some sort of) store operation. N(I,R) \nFinally, we take into account the name of the in- struction. This is based on the observation that if \nI s mnemonic contains the string add or plus it is more likely to perform (some sort of) addition than \n(say) a left shift. Unfortunately, this information can be highly inaccurate, so N(I, R) is given a low \nweight- ing. For many samples these heuristics are highly successful. Of-ten the reverse interpreter \nwill come up with the correct se-mantic interpretation of an instruction after just one or two tries. \nIn fact, while previous versions of the system relied exclusively on graph matching, the current implementation \nnow mostly uses matching to compute the M(S, I, R) func-tion. There are still complex samples for which \nthe reverse interpreter will not find a solution within a reasonable time. In such cazes a time-out function \ninterrupts the interpreter and the sample is discarded. 5.2.3 Primitive Instructions The instruction \nprimitives used by the reverse interpreter largely determine the range of architectures that can be an- \nalyzed. A comprehensive set of complex primitives might map cleanly into a large number of instruction \nset archi-tectures, but would slow down the reverse interpreter. A smaller set of simple primitives would \nbe easier for the re- verse interpreter to deal with, but might fail to provide a semantic interpretation \nfor some instructions. As can be seen from Figure 14, the current implementation employs a small, RISC-like \ninstruction set, which allows us to handle current RISC% and CISCs. It lacks, among other things, con- \nditional expressions. This means that we currently cannot analyze instructions like the VAX s arithmetic \nshift (ash), which shifts to the left if the count is positive, and to the right otherwise. In other \nwords, the reverse interpreter will do well when analyzing an instruction set that is at the same or \nslightly higher level than its built-in primitives. However, dealing with micro-code-like or very complex \ninstructions may well be beyond its capabilities. The reason is our need to always find the shorted semantic \ninterpretation of every inetruc- tion. This means that when analyzing a complex instruction we will have \nto consider a very large number of short (and wrong) interpretations before we arrive at the longer, \ncorrect one. Since the number of possible interpretations grows ex- ponentially with the length of the \nsemantic interpretation, the reverse interpreter may quickly run out of space and time. Although very \ncomplex instructions are currently out of favor, they were once very common. Consider, for exam-ple, \nthe VAX s polynomial evaluation instruction TOLY or the HP 2100 [lo) series computers alter-skip-group. \nThe latter contains 19 basic opcodes that can be combined (up to 8 at a time) into very complex statements. \nFor exam- ple, the statement rCLA,SEZ,CME,SLA,INK will clear A, skip if E=O, complement E, skip if LSB(A)=O, \nand then increment A. 6 The Synthesizer The Synthesizer collects all the information gathered by pre- \nvious phases and converts it into a BEG specification. If the discovery system is part of a self-retargeting \ncompiler, the machine description would be fed directly into BEG and the resulting code generator would \nbe integrated into the compiler. If the discovery system is used to speed up a manual compiler retargeting \neffort the machiie description could first be refined by the compiler writer. The main difficulty of \nthis phase is that there may not be a simple mapping from the intermediate code in-structions emitted \nby the compiler into the machine code instructions. As au example, consider a compiler which emits an \nintermediate code instruction BranchEQ(a, 8, L) = IF a = b COT0 L. Using the primitives in Fig-ure 14, \nthe semantics of BranchEQ can be described as brTrue (isEQ(compore (or, az)), L). This, incidentally, \nis the exact semantics we derive for the MIPS beq instruc-tion. Hence, in this case, generating the appropriate \nBEG pattern matching rule is straight-forward. However, on most other machines the BranchECJ instruc- \ntion has to be expressed as a combination of two ma-chine code instructions. For example, on the Alpha \nwe derive cmpeq(a, b) = isEQ( compare (a, b)) and bne(a, L) = brTrue (a, L). To handle this problem, \na special Synthesizer phase (the Combiner) attempts to combine machine code instructions to match the \nsemantics of intermediate code instructions. Again, we resort to exhaustive search. We consider any combination \nof instructions to see if combin- ing their semantics will result in the semantics of one of the instructions \nin the compiler s intermediate code.7 Any such combination results in a separate BEG pattern matching \nrule. See figure Figure 15(d) for an example. Depending on the complexity of the machine description \nlanguage, the Synthesizer may have to contend with other problems as well. BEG, for example, has a powerful \nway of describing the relationship between different addressing modes, so called chain rules. A chain \nrule expresses under which circumstances two addressing modes have the same semantics. The chain-rules \nin Figure 15&#38;c) express that the SPARC s register+offset addressing mode is the same as the register \nimmediate addressing mode when the offset is 0. To construct the chain-rules we consider the semantics \nSA and Sz of every pair of addressing modes A and B. For each pair, we exhaustively assign small constants \n(such as 0 or 1) to the constant arguments of SA and Sz, and we as- sign registers with hardwired values \n(such BS the SPARC s QCI) to SA and Se s register arguments. If the resulting se-mantics Si and Sk are \nequal, we produce the corresponding chain-rule. 7 Discussion and Summary It is interesting to note that \nmany of the techniques pre-zented here have always been used manually by compiler This ia somewhat akin \nto Maesalin s [14] superoptimizer. The difference is that the superoptimizer attempts to find a ~mollest \npro-gram, whereas the Combiner looks for any combination of instruc-tions with the required behavior. \nThe back-end generator is then responsible for selecting the best (cheapest) instruction sequence at \ncompile-time. SIGNATURE SEMANTICS COMMENTS add (!I x I) --t 1 add(a,b) =a+b Also sub, mul, div, and \nmod. cabs II + I ohs (a) =/ a 1 Also neg, not and move. and (I x I) -+ I and(a,b) =aAb Also or, zor, \nshiftLeft, and shijtRight. ignore1 (I x I) + P ignore1 (a, b) = b Ignore first argument. Also ignore3. \n compare (I x 1) + @ compare (a, b) = (a < b, a = b, a > b) Return the result of comparing a and L. Example: \ncompare (5,7) = (T, F, P). isLE c -+ B isLE(a) = a # (-, --) a) Return true if a represents a less-than-or-equal \ncondition. Also isEQ, isLT, etc. brTrue (B x L) brTrue (a. b) = if a then PC 4-b Branch on true. Also \nbrFalse. w No operation. load A+ I load(a) = H[a] Load an integer from memory. store (A x I) store(a,b) \n= M[a] + b Store an integer into memory. lootii t Lit + I loadlit (a) = a Load an integer literal. 1oadAddr \nAddr + A ZoadAddr(a) = a Loada memory address. Figure 14: Reverse interpreter primitives. Available types \nare Int (I), Boo1 (B), Address (A), Label(L),and Condition Code (C). M[ ] is the memory. T is True and \nF is False. C is an array of booleans representing the outcome of a comparison. While the current implementation \nonly handles integer instructions, future versions will handle all standard C types. Hence the reverse \ninterpreter will have to be extended with the corresponding primitives. NONTFWUNALS AddrI'lods4 ADRHODE \n (4 CONDATTRIBUTES (ix&#38;l-1 : INTEGER) (reg4-1 : Register); RULE Rsgister.al -> AddrHods4.rss; ('4 \n COST 0; EVAL{res.intrl-1 := al;} := 0;) EHIT{res.reg'l-1 RULE AddrXode4.al -> Registsr.ras; (c) \n CONDITION{(al.int4-10)); COST 0; EnIT{ros := al.reg4-I;) =  RULE BranchEQ Label.al Registsr.aZ IntConstant.a3 \n; CONDITION((a3.val>=-4096) AND (a3,va1<=4095)}; '3) COST 2; EHIT{print \"cmp\", a2 \",' a3.val; print \nWY', \"L\" al.lab; print \"nap\"} RULE Nult Rsgister.a3(Rsg-00)  Registsr.a4(Regal)-> Rsgistsr.res(Reg-00); \n(4 COST 15; TARGET a3; MITtprint \"call .mul. 2\"; print \"nap\"} Figure 15: Part of a BEG specification \nfor the SPARC, generated automatically by the architecture discovery system. (a) shows the declaration \nof the register+offset addressing mode. (b) and (c) are chain-rules that describe how to turn a register+offset \naddressing mode into a register (when the offset is 0), and vice versa. In (d) a comparison and a branch \ninstruction have been combined to match the semantics of the intermediate code instruction BraachE@ Note \nhow the architecture discovery system has detected that the integer argument to the cmp instruction has \na limited range. (e), finally, describes the SPARC s software multiplication routine .mul . Note that \nwe have discovered the implicit input (x00 and x01) and output argument (x00)tothe call instruction. \nwriters. The fastest way to learn about code-generation 7.1 Generality techniques for a new architecture \nis to compile some small C What range of architectures can an architecture discoveryor FORTRAN program \nand examine the resulting assembly system possibly support? Under what circumstances mightcode. The architecture \ndiscovery unit automates this task. it fail? One of the major sources of problems when writing ma-As \nwe have seen, our analyzer consists of three major chine descriptions by hand is that the documentation \nde-modules: the Lexer, the Preprocessor, and the Extractor. scribing the ISA, the implementation of \nthe ISA, the as-Each of them may fail when attempting to analyze a partic- sembler syntax, etc. is notoriously \nunreliable. Our system ular architecture. The Lexer assumes a relatively standard bypasses these problems \nby dealing directly with the hard- assembly language, and will, of course, fail for unusual lan- ware \nand system software. Furthermore, our system makes guages such as the one used for the Tera. The Extractorit \ncheap and easy to keep machine descriptions up to date may fail to analyze instructions with very complex \nseman-with hardware and system software updates. tics, since the reverse interpreter (being worst-case \nexponen- We will conclude this paper with a discussion of the gen- tial) may simply Pun out of time. \n enzlity, completeness, and implementation status of the ar- The Preprocessor s task is essentially to \ndetermine howchitecture discovery system. pairs of instructions communicate with each other within a \nsample. Should it fail to do so the data-flow graph cannot be built, and that sample cannot be further \nanalyzed. There are basically four different ways for two instructions A and B to communicate: Explicit \nregisters A assigns a value to a general purpose register R. B reads this value. Implicit registers A \nassigns a value to a general purpose register R which is hardwired into the instruction. B reads thii \nvalue. Hidden registers A and B communicate by means of a special purpose register which is hidden within \nthe CPU and not otherwise available to the user. Examples include condition codes and the lo and hi registers \non the MIPS. Memory A and B communicate via the stack or main memory. Examples include stack-machines \nsuch as the Burroughs B6700. The current implementation handles the first two, some spe- cial cases (such \nas condition codes) of the third, but not the last. For this reason, we are not currently able to analyze \nextreme stack-machines such as the Burroughs B6700. Furthermore, there is no guarantee that either CCCG \nor SDCG will work for all architecture/compiler/language combinations. We have already seen that some \nC compil- ers will be unsafe as CCCG back-ends for languages with garbage collection. SDCG-based compilers \nwill also fail if a new ISA has features unanticipated when the back-end generator was designed. Version \n1 of BEG, for example, did not support the passing of actual parameters in registers, and hence was unable \nto generate code for RISC machines. Version 1.5 rectified this. 7.1.1 Completeness and Code Quality The \nquality of the code generated by an SRCG compiler will depend on a number of things: The quality of the \nC compiler. Obviously, if the C compiler does not generate a particular instruction, then we will never \nfind out about it. The semantic gap between C and the target language. The architecture may have instructions \nthat directly support a particular target language feature, such as exceptions or statically nested procedures. \nSince C lacks these features, the C compiler will never produce the corresponding instructions, and no \nSRCG compiler will be abIe to make use of them. Note that this is no different from a CCCG-based compiler \nwhich will have to synthesize its own static links, exceptions, etc. from C primitives. The completeness \nof the sample set. There may be instructions which are part of the C compiler s vocab-ulary, but which \nit does not generate for any of our simple samples. Consider, for example, an architec-ture with long \nand short branch instructions. Since our branching samples are currently very small (typi-cally, mainO{int \na,b,c; if (b<c) a=9;)1), it is un- likely that a C compiler would ever produce any long branches. The \npower of the architecture discovery system. If a particular sample is too complex for us to analyze, \nwe will fail to discover instructions present only in that sample. The quality of the back-end generator. \nA back-end generated by BEG will perform no optimization, not even local common subexpression elimination. \nR.e- gardless of the quality of the machine descriptions we produce, the code generated by a BEG back-end \nwill not be comparable to that produced by a production compiler. It is important to note that we are \nnot trying to reverse engineer the C compiler s code generator. This is a task that would most likely \nbe beyond automation. In fact, if the C compiler s back-end and the back-end generator use different \ncode generation algorithms, the codes they generate may bear no resemblance to each other. 7.2 Implementation \nStatus and Future Work The current version of the prototype implementation of the architecture discover \nsystem is general enough to be able to discover the instruction sets of common RISC and CISC ar-chitectures. \nIt has been tested on the integers instruction sets of five machines (Sun SPARC, Digital Alpha, MIPS, \nDEC VAX, and Intel x86), and has been shown to generate (almost) correct, machine specifications for \nthe BEG back-end generator. The are= in which the system is deficient relate to modules that are not \nyet implemented. For exam- ple, we currently do not test for registers with hardwired values (register \n%gO is always 0 on the Spare), and so the BEG specification fails to indicate that such registers are \nnot available for allocation. In this paper we have described algorithms which deduce the register sets, \naddressing modes, and instruction sets of a new architecture. Obviously, there is much additional in-formation \nneeded to make a complete compiler, information which the algorithms outlined here are not designed to \nob- tain. As an example, consider the symbol table information needed by symbolic debuggers ( I. stabs \nentries). Furthermore, to generate code for a procedure we need to know which information needs to go \nin the procedure header and footer. Typically, the header will contain in-structions or directives that \nreserve space on the runtime stack for new activation records. To deduce this information we can simply \nobserve the differences between the assembly code generated from a sequence of increasingly more com-plex \nprocedure declarations. For example, compiling %t P(){}l, int P(>{int a;}- , Tint P(){int a,b;}l, etc., \nwill result in procedure headers which only differ in the amount, of stack space allocated for activation \nrecords. Unfortunately, things can get more complicated. On the VAX, for example, the procedure header \nmust contain a register mask containing the registers that are used by the procedure and which need to \nbe saved on procedure entry. Even if the architecture discover system were able to deduce these requirements, \nBEG has no provision for expressing them. sAt this point we are targeting integer instruction sets exclusively, \nsince they generally exhibit more interesting idiosyncrasies than float-ing point instruction sets. 7.2.1 \nHardware Analysis There has been much work in the past on automati-cally determining the runtime characteristics \nof an archi-tecture implementation. This information can be used to guide a compiler s code generation \nand optimization passes. Baker [l] describes a technique ( scheduling through self-simulation,,), in \nwhich a compiler determines a good sched- ule for a basic block by executing and timing a few alterna- \ntive instruction sequences. Rumor [12] has it that SunSoft uses a compiler-construction time variant \nof this technique to tune their schedulers. The idea is to derive a good sched- uling policy by running \nand timing a suite of benchmarks. Each benchmark is run several times, each time with a dif- ferent set \nof scheduling options, until a good set of options has been found. In a similar vein, McVoy s lmbench \n[15] program mea-sures the sizes of instruction and data caches. This informa- tion can be used to guide \noptimlzations that increase code size, such as inline expansion and loop unrolling. Finally, Pemberton \ns enquire [16] program (which de-termines endian-ness and sizes and alignment of data types) is already \nin use by compiler writers. Parts of enquire have been included into our system. It is our intention \nto include more of these techniques in future versions of the architecture discovery system. At the present \ntime only crude instruction timings are performed. More detailed information would not be useful at this \npoint, since BEG would be unable to make use of it. 7.2.2 Current Status The system is under active development. \nThe implemen-tation currently consists of 10000 non-blank, non-comment lines of Prolog, 900 lines of \nshell scripts (mostly for com-municating with the machine being analyzed), 1500 lines of AWK (for generating \nthe C code samples and parsing the resulting assembly code), and 800 lines of makefiles (to in- tegrate \nthe diierent phases). Acknowledgments Thanks to Peter Fenwick and Bob Doran for valuable infor- mation \nabout legacy architectures, and to the anonymous referees for helping me to greatly improve the presentation. \nReferences [l] Henry G. Baker. Precise instruction scheduling without a precise machine model. Computer \nArchitecture News, 19(6), December 1991. [2] Digital Systems Research Center. Src modula-3: Re-lease \nhistory. http://vuv.resea.rch.digital.com/ SRC/modula-3/html/history.html, 1996. [3] David Chase and \nOliver Ridoux. C as an interme-diate representation. comp.compilers article numbers 90-08-046 and 90-08-063, \nAugust 1990. Retrieve from http://iecc.com/compilers/article.html. [4] Cristina Cifuentes and K. John \nGough. Decompilation of binary programs. Software -Practice # Ezperience, 25(7):811-829, July 1995. [5] \nTera Computer Company. Major system characteristics of the Tera supercomputer, November 1995. http://uvv.tera.com/hardvare-overviev.htrd. \n[6] Helmut Emmelmann, Friedrich-Wilhelm Schrijer, and Rudolf Landwehr. Beg -a generator for efficient \nback ends. In SIGPLAN 89 Conference on Pmgmmming Language Design and Implementation, pages 227-237, 1989. \n[7] Interactive Software Engineering. ISE Eiffel in a nutshell. http: //eiff el . com/eiff cl/nutshell. \nhtml, 1996. [8) D. R. Engler and Todd A. Proebsting. DCG: An effi- cient retargetable dynamic code generation \nsystem. In International Conference on Architectuml Support for Progmmming Languages and Opemting Systems, \nOcto-ber 1994. [9] Christopher W. Fraser, Robert R. Henry, and Todd A. Proebsting. BURG -fast optimal \ninstruction selection and tree parsing. SIGPLAN Notices, 24(7):6&#38;76, April 1992. [lo] Hewlett-Packard. \nA Pocket Guide to Hewlett-Packanl Computera, 1968. [ll] Richard C. Holt. Data descriptors: A compile-time \nmodel of data and addressing. ACh4 fiansactions on Programming Languages and Systems, 9(3):367-389, 1987. \n[12] David Keppel. Compiler back-ends. comp.compilers article number 95-10-136, October 1995. Retrieve \nfrom http://iecc.com/contpilers/article.html. [13] James R. Lams and Eric Schnarr. EEL: Machine-independent \nexecutable editing. In PLDI 95, pages 291-300, La Jolla, CA, June 1995. [14] Harry Massalin. Superoptimizer \n-a look at the smallest program. In Proceedings Second International Confer-ence on Architechtuml Support \nfor Programming Lan-guages and Operating Systems (ASPLOS II), Palo Alto, California, October 1987. [15] \nLarry McVoy and Carl Staelin. lmbench: Portable tools for performance analysis, 1996. In USENIX Annual \nTechnicul Conference, San Diego, California, January 1996. [lS] Steven Pemberton. Enquire 4.3, 1990. \n[17] Richard M. Stallman. Using and Porting GNU CC. Free Software Foundation, Inc., 2.7.2 edition, Novem- \nber 1995. http://wv.gnu.ai.mit.edn/doc/doc.html.  \n\t\t\t", "proc_id": "258915", "abstract": "There are three popular methods for constructing highly retargetable compilers: (1) the compiler emits abstract machine code which is interpreted at run-time, (2) the compiler emits C code which is subsequently compiled to machine code by the native C compiler, or (3) the compiler's code-generator is generated by a back-end generator from a formal machine description produced by the compiler writer.These methods incur high costs at run-time, compile-time, or compiler-construction time, respectively.In this paper we will describe a novel method which promises to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at <i>compiler construction time</i> to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native code-generator can be generated by a back-end generator such as BEG or burg.A prototype <i>Automatic Architecture Discovery Unit</i> has been implemented. The current version is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86. The tool is completely automatic and requires minimal input from the user: principally, the user needs to provide the internet address of the target machine and the command-lines by which the C compiler, assembler, and linker are invoked.", "authors": [{"name": "Christian S. Collberg", "author_profile_id": "81100590418", "affiliation": "Department of Computer Science, The University of Auckland, Private Bag 92019, Auckland, New Zealand", "person_id": "P46557", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258922", "year": "1997", "article_id": "258922", "conference": "PLDI", "title": "Reverse interpretation + mutation analysis = automatic retargeting", "url": "http://dl.acm.org/citation.cfm?id=258922"}