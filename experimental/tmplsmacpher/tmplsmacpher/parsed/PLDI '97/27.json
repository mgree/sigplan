{"article_publication_date": "05-01-1997", "fulltext": "\n Fine-grain Multithreading with Minimal Compiler Support- A Cost Effective Approach to Implementing Efficient \nMultithreading Languages Kenjiro Taura* and Akinori Yonezawa Department of Information Science, Faculty \nof Science, University of Tokyo, 7-3-l Hongo, Bunkyo-ku Tokyo 113 Japan {tau,yonezaua}Ois.s.u-tokyo.ac.jp \nAbstract It is difficult to map the execution model of multithread- ing languages (languages which support \nfine-grain dynamic thread creation) onto the single stack execution model of C. Consequently, previous \nwork on efficient multithreading uses elaborate frame formats and allocation strategy, with com-pilers \ncustomized for them. This paper presents an alterna- tive cost-effective implementation strategy for \nmultithread- ing languages which can maximally exploit current sequen- tial C compilers. We identify \na set of primitives whereby ef- ficient dynamic thread creation and switch can be achieved and clarify \nimplementation issues and solutions which work under the stack frame layout and calling conventions of \ncur- rent C compilers. The primitives are implemented as a C library and named StackThreads. In StackThreads, \na thread creation is done just by a C procedure call, max-imizing thread creation performance. When a \nprocedure suspends an execution, the context of the procedure, which is roughly a stack frame of the \nprocedure, is saved into heap and resumed later. With StackThreads, the compiler writer can straightforwardly \ntranslate sequential constructs of the source language into corresponding C statements or expres- sions, \nwhile using StackThreads primitives as a blackbox mechanism which switches execution between C procedures. \n 1 Introduction Many parallel programming languages support dynamic cre-ation of threads. Example language \nconstructs include fu- tures (141, asynchronous message passing between concur-rent objects[l, 321, fork-join \n[22], parallel blocks and loops [4,6], and implicit parallelism [21]. Implementation of parsl- lel languages \nwith dynamic thread creation (which we here- after refer to as multithreading languages) must achieve \neffi- cient multithreading without sacrificing good sequential per- formance. Compiling multithreading \nlanguages into C and exploiting optimizations performed by the C compiler is an attractive choice for \nobtaining sequential performance. It . JSPS Research Fellow Permission to make digital/hard copy of \npart or all this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advan-tage, the copyright notice, the title of the publication \nand its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, \nto republish. to post on servers, or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 97 Las Vegas, NV, USA 0 1997 ACM 0-69791-907-6/97/0006...$3.50 has been challenging, however, \nbecause the execution models of multithreading languages are not naturally mapped onto the single stack \nexecution model of C. The time and space overhead of allocating a separate stack for each thread is prohibitively \nlarge, hence existing user-level thread libraries [8, 171 cannot straightforwardly be used for implementing \nmultithreading languages. Consequently, most of the previ- ously published efficient implementations \nof multithreading languages adopt a custom frame management and generate either assembly or assembly-like \nC code in which frame man- agement and context switch code sequences are inlined. In such implementations, \na thread creation typically allocates only a single frame from a general free storage and a context switch \njust saves live registers on the frame and transfers control to the restarting thread (9). While this \napproach achieves very fast thread creation and context switch, there are several disadvantages and potential \npitfalls. First, the compiler development cost is very high, because they must design runtime data structures \nfrom scratch for threaded execution and the compiler must perform low-level analyses such as liverange \nanalysis to emit inlined context switch sequences. Second, sequential performance is sacrificed un-less \nthere is substantial effort on optimization. They must implement many sequential optimizations when generating \nassembly. Even when generating C code, C compilers often fail to optimize assembly-like C code because \nof its highly complex and unstructured representation of computation. For example, restarting a computation \ninside a loop requires a goto statement into the body of the loop, which is likely to disable optimization \nby the backend. This paper presents an attractive alternative for efficient implementation of multithreading \nlanguages. The mech-anism is provided as a low-level C library, called Stack-Threads. By low-level, we \nmean that only primitive frame management mechanisms are defined by the library. Sup porting higher-level \nabstractions on top of the base primi- tives is left for language designers and implementors. Sec-tion \n5 demonstrates several example abstractions built on top of StackThreads. By library, we mean that most \nof the work needed for multithreading is done under cover of the library, without requiring extensive \ncooperation from the code generator. More precisely, the generated code sim- ply calls a few library \nroutines when a thread blocks. The library routine performs all the work needed to switch to another \nthread. In particular, the context-saving sequence does not have to be inlined in the generated C code. \nHence, with StackThreads, sequential constructs can be straight- forwardly compiled into corresponding \nC statements or ex- pressions which may call a library routine when evaluation can no longer continue. \nThis reduces development cost and enhances the chances for the backend optimization. Unlike traditional \nthread libraries, StackThreads meets the performance requirement of multithreading languages- small creation \noverhead. In StackThreads, starting a new thread, including parameter passing, takes only a few in- structions. \nIn fact, starting a new thread which executes the body of a C function f is just a procedure call to \nf (possibly with extra parameters, depending on the implemented lan-guage construct). In the case where \na thread blocks, mecha- nisms are provided to (1) save the context of the C procedure and resume the \ncaller of a procedure, and (2) later restart a blocked thread from a saved context. Unlike previous im- \nplementations of multithreading languages in which these or similar mechanisms are implemented on top \nof a customized frame management protocol, StackThreads implements the mechanisms which work with any \nC code satisfying the few conditions described in Section 4.6. That is, the generated C code uses conventional \nC stack frames and procedure linkage conventions including parameter passing, result value pass- ing, \nand even callee save registers. The primary contribution of this paper is to identify a set of primitives \nwhich are re- quired for efficient multithreading and implementable under the stack frame format and \ncalling conventions of current se- quential compilers. We have successfully used StackThreads in the \nimplementation of a concurrent object-oriented lan-guage ABCL/f[29] running on APlOOO+ [15] distributed \nmemory multicomputer. The rest of the paper is organized as follows. Section 2 reviews previous work \non software im- plementations of multithreading. Section 3 summarizes the stack frame layout and code \ngeneration conventions of C procedures which StackThreads mechanism relies on. Sec-tion 4 details how \nour mechanism works. Section 5 demon- strates several higher-level language constructs built on top of \nStackThreads. Section 6 reports performance numbers and Section 7 summarizes the work and states conclusions. \n  Related Work This section reviews previous efficient multithreacling schemes. We limit our focus \nto schemes which are implemented on conventional CPUs and do not discuss those which are im- plemented \non multithreaded architectures. All of them ei- ther involved custom frame management and procedure link- \nage conventions or restrict the concurrency model so that they can be implemented without a general multithreading \nmechanism. Table 1 lists these works in roughly chronologi- cal order with their supported concurrency \nmodels and code generation schemes. A concurrency model is general if they implement a general multithreading \nmodel and restricted otherwise. By general multithreading model, we mean that created threads are guaranteed \nto be scheduled eventually, at least when it becomes the only runnable thread. A code generation scheme \nis native if it generates assembly, assembly-like C if it generates C with inlined frame man-agement \nand context switch code sequences, and simple C if it can simply run on top of C s stack frame management. \nMost work, including ours, do not guarantee any stronger .vense of fairness. They only guarantee that \nneither the runtime nor the compiler add dependencies between threads which are otherwise independent. \nMost notably, schemes which adopt simple C code gen-eration, leapfrogging [31] and lazy RPC [lo], do \nnot support a fully general concurrency model. A distinguishing feature of StackThreads is that it allows \nsimple C code generation while implementing a general multithreading model. Below we classify previous \nwork into three categories and describe each work in more detail. The three categories include those \nwhich use a simple task pool for thread management, those which adopt more elaborate and complicated \nthread man-agement schemes, and those which simply run on top of C s stack frame management at the cost \nof a restricted concur-rency model. 2.1 Thread Management by Simple Task Pool There are many multithreading \nimplementations which use a custom procedure linkage convention and frame manage- ment strategy, and \ngenerate assembly or assembly-like C code in which frame management and context switch code are inlined. \nThe simplest management scheme allocates ac-tivation frames from a general free storage (e.g., free list) \non an invocation-by-invocation basis, so that each thread does not have to have a stack. All runnable \nthreads are stored into a task pool. A thread creation only allocates a thread control block, which does \nnot have to have a stack, and stores it into the task pool. When a thread blocks, it simply schedules \nanother thread from the task pool. TAM [26] and threads implemented in SML/NJ [7] fall into this category. \nTAM allocates an activation frame for each paral- lel invocation of a function or a loop body, from a \nfree list. SML/NJ (71 implements threads using the callcc primitive. Since SML/NJ allocates all activation \nframes from the heap [2], callcc is implemented simply by capturing the pointer to the current frame \nand saving callee-save registers. Hence SML/NJ s thread management using callcc effectively im- plements \nsimple heap-based frame management very effi-ciently. This strategy necessarily uses a custom frame format \nas well as calling convention, and calling legacy libraries writ- ten in C needs special setup procedures \nsuch as a stack switch. In addition to such software engineering issues, threads in this category have \nsome performance disadvan-tages. First, thread creation incurs expensive operations such as allocating \nframes from a general free storage and enqueueing a frame into a task pool. Second, they usually have \nno chances to pass the result value of an invocation via a register. The result value is always written \ninto memory, because the callee in general does not know when the caller is scheduled. Since registers \nare volatile storage, returning the result value via a register requires making some assump- tions on \nthe scheduling order and exploiting them. Third, they have no provisions for using &#38;lee-save registers \non a thread creation. All contexts are saved by the caller prior to a thread creation. This is again \nbecause no scheduling order is assumed between a parent thread and a child thread.  2.2 More Elaborate \nThread Management Schemes More elaborate schemes are based on the observation first stated by Mohr, Kranz, \nand Halstead [18, 111. The obser- vation is that a thread creation merely has to leave mini- mal information \nto perform a real fork retroactively when / Concurrency 1 Code Generation ( Primary Target 1 Threads \nin SML/NJ [7] 1 LTC [ll, 181 TAM [9] ABCL/APlOOO [28] Leapfrogging [31] Olden [25] Concert [24] Lazy \nThreads [12] Lazy RPC [lo]  Model 1 Scheme Language general j native SML/NJ [2] general native Multilisp \n[ 141 general native Id [21] general assembly-like C ABCL restricted simple C Multilisp restricted native \nOlden general assembly-like C CA [5] &#38; ICC++ [6] general native restricted simple C ParSubC [lz \nTable 1: List of previous efficient implementations of multithreading it turns out to be really needed. \nMore precisely, when we create a thread which evaluates the body of we continue 61 the evaluation of \nf just as a sequential call. Mechanisms are provided to resume the continuation of f without wait- ing \nfor the completion of f in case f is blocked. If f is not blocked at all, the cost of a thread creation \nis roughly that of a procedure call + writing a descriptor to indicate a poten- tial fork point. This \nbasic structure-minimal fork overhead and retroactive work generation-can be ubiquitously found in many \nworks which follow 112, 24, 25, 281, in different con- texts with further clarifications and improvements. \nThis basic idea opens the door to several improvements over the simple task-pool approaches to managing \nthreads. First, allocating a frame of a new thread from a stack is more efficient than allocating from \na general free storage and en- queueing it to a task-pool. Moreover, this process is very similar to \na procedure call in sequential languages, giving us an opportunity to express a thread fork in C s procedure \ncall. Second, since the scheduling order is fixed and LIFO, they can return the result value via a register \nwhen a thread ter- minates without blocking, because we know the next thread to run will be the caller. \nProtocols must be devised for re- turning the result value after a thread is blocked. Third, we have \nan opportunity to exploit &#38;lee-save registers even on a thread creation. This is again because, when \nthe callee terminates without blocking, we know the caller is scheduled immediately after the callee, \nenabling the callee to restore the callee-save registers for the caller. The execution scheme of the \npresent work is based on the same observation and most close to that of the authors previous work [ZS] \nand the hybrid execution model of Concert [24]. The difference between our work and any prior work is \nwhere the mecha- nism is implemented. Existing schemes use a custom frame management protocol and do \nnot allow the direct reuse of a sequential compiler substrate-optimizing C compilers. On the other hand, \nStackThreads deals with a conventional C stack. Compilers of higher-level languages can straightfor-wardly \nmap sequential computation onto C and a context switch simply calIs library routines at runtime. Differences \nbetween schemes in this category lie in how to deal with blochng-situation where the current thread can \nno longer proceed. When a thread which evaluates a procedure f blocks, [24] and [28] simply save the \nstack frame of the procedure and resume the frame just below the current frame. Both works implement \nthis mechanism by generating assembly-like C code. LazyThreads [12] takes a similar but different approach \n 2A thread creation in LTC leaves a descriptor in a task pool so that another processor can steal it. \nThin is a cost of dynamic load distribution and not a cost of multithreading per se. to frame management \nand thread suspension. Frames are allocated in the unit of what they call stacklet. A stacklet is a contiguous \nregion which can hold several frames, but is much smaller than the typical stack size. A blocked thread \nresumes a caller without copying the stack frame to heap. Instead, each procedure checks if space is \navailable on top of the current frame. If not, a new stacklet is allocated, regardless whether the call \nis sequential or parallel. Imple-mentation of LazyThreads necessarily has to design runtime data structure \nfrom scratch; the implementation was done by modifying the GNU C compiler.  2.3 Simple C Code Generation \nwith Re-stricted Concurrency Model There are attempts which implement multithreading lan-guages on top \nof the stack frame management mechanism of C. The basic idea is we continue execution of a single thread \nas far as we can and when a thread is blocked, we grow the stack by other schedulable work, rather than \nunwinding the stack. In this way we can hold contexts of multiple threads in a single stack without unwinding \noperation, which cannot be naturally expressed in C. Leapfrogging [31] implements Multilisp s future \nconstruct. A thread which evaluates an expression e is created by a fu- ture expression (future e>. When \na processor encounters a future expression, the processor continues to evaluate the continuation of the \nexpression, leaving a descriptor of e in a shared task pool3 A task is blocked when it needs the value \nof an undetermined future. When a processor executing a task T encounters an undetermined future F, the \nprocessor now steals work from the shared task pool, but only steals one which is a subtask of F (including \nF itself). The stolen subtask (call it F ) is evaluated on top of the current stack. This strategy can \nbe naturally expressed in C s stack frame management mechanism. The processor simply fetches F and calls \na procedure which evaluates F . An implication is that the blocked thread T can only be resumed after \nthe evaluation of F finishes. This scheduling is not always safe; it is safe as long as determining the \nvalue of F requires de- termining the value of F , because in this case resuming T, which we know requires \nthe value of F, transitively re-quires determining the value of F . To summarize, leapfrog- ging can \nevaluate two tasks in a single stack as long as the lower thread has been blocked and is dependent on \nthe upper thread (assuming a stack grows upwards). Evaluating mul- 3Uaing a shared task pool does not \nimply leapfrogging assumes shared memory. The shared task pool can actually be implemented as a logically \nshared, but physically distributed data structure. tiple tasks which may be independent requires correspond- \ningly many stacks. The concurrency model of leapfrogging is more restrictive than the general multithreading \nmodel in several ways. First, it does not support speculative compu-tation. As the authors pointed out \nin 1311, if F is specula- tive and does not contribute to the value of F, leapfrogging causes a deadlock \nwhich should not occur in a general mul- tithreading model. Second, it only supports l-producer-N-consumers \nsynchronization via future and does not support general synchronization primitives. More specifically, \nit as- sumes that a blocked thread knows the resumer thread, the thread which is going to resume it. \nThis condition holds in stylized uses of futures, but does not hold for general synchronization primitives \nsuch as mutexes and condition variables. Finally, it does not encourage eager movement of tasks or application-specific \ntask placement, which are particularly important on distributed-memory parallel ma-chines. This is again \nbecause independent tasks may not coexist on a single stack. A new task can be evaluated only when the \ncurrent task is blocked or an empty stack becomes available. Lazy RPC [lo] is based on the same idea. \nThe difference is that, when a task blocks, the pro- cessor steals any task in the task pool. The discussion \nand restriction above also apply for Lazy RPC. StackThreads, on the other hand, implements general multithreading \nwith a single stack (or a constant number of stacks) per processor. The key mechanism is stack unwinding, \nin which we can speculatively evaluate independent tasks in a single stack and revoke speculative decisions \nwhen the topmost task is blocked. Unlike leapfrogging or Lazy RPC, StackThreads moves stack frames, thus \n1s incompatible with C programs or C compiler optimizations which assume they do not move. The primary \nuse of StackThreads is therefore as a compiler target, rather than for user-level libraries for C programs. \n3 Procedure Linkage Convention of C Pro- cedures 3.1 Stack Frame Layout This section summarizes procedure \nlinkage and code genera- tion conventions of C procedures, which are necessary for un- derstanding the \ndetails of StackThreads. In particular, un-derstanding the details of the procedure return mechanism- \nhow a procedure returns to the caller so that the caller con- tinues execution-gives us opportunities \nfor saving/restoring the context of procedures in an unusual way. The back-ground includes stack frame \nlayout, register usage conven- tion, how the linkage between a caller and a callee is main- tained, and \nwhen/where registers are saved. Figure 1 shows a typical stack frame layout of a C pro- cedure. The figure \nillustrates a stack frame for a procedure f and its parent P, assuming the stack grows downwards. The \nlowest and the highest addresses of the stack frame are pointed to by the stack pointer (SP) and frame \npointer (FP), respectively. A stack frame for f holds: . local and temporary variables of f, . callee-save \nregisters for P, and . the link to P, which is the return address and the frame pointer of P. Incoming \nparameters not allocated to registers are stored in the caller s frame, so that the caller does not have \nto Stack Pointer off j , 1 Stack Growth Figure 1: A typical stack frame layout of a C procedure: Parameters \nare in the frame of the caller. Local variables, temporary variables, callee save registers, and links \nto the caller (i.e., return address and FP of the caller) are in the frame of the current procedure. \nknow the frame layout of the called procedure. The offset of the incoming parameters from the callee \ns frame pointer is constant across all procedures, so that the caIlee does not have to know the caller. \nIt is the callee s responsibility to restore the SP and FP of the caller upon procedure return.  3.2 \nRegister Usage Convention The register usage convention for a processor classifies CPU registers into \ntwo categories. One is callee-save registers, which the caller assumes are preserved across a procedure \ncall, and the other is caller-save registers, which the caller assumes are destroyed across a procedure \ncall. In order for a procedure to return to the correct place with the correct register state, a stack \nframe saves the return address, the frame pointer of the parent, and the callee-save registers which \nit destroys. When a procedure returns, it restores the values of the frame pointer, stack pointer, and \nthe callee- save registers and jumps to the return address. The caller then continues execution, assuming \nFP, SP, and callee-save registers have the original values and other registers do not. In other words, \nFP, SP, and callee-save registers constitute f s contezt-information which must be restored when f is \nrescheduled. The basic idea behind any thread library is that whenever we fork or switch a thread, we \nsave enough information so that we can restore the context from the point where we reschedule the thread. \nSince the point where we reschedule the thread is usually unknown, ordinary thread libraries save all \ncontexts, including all callee-save registers in the calling convention, of the current thread whenever \na fork or a switch occurs. StackThreads, on the other hand, does not save all context on a thread fork-when \na procedure f is forked, StackThreads saves exactly the same amount of context as an ordinary procedure \ncall to f. 4.52 and FP are also assumed to be preserved across a procedure call. For our purpose,however, \nwe consider them as special registers and distinguish them from regular cake-save registers. This makes \na thread fork efficient but raises a difficult question against the implementation of StackThreads, be-cause \nthe callee-save registers for a procedure may be spread into an unknown number of frames and registers. \nSuppose a procedure f is using four callee-save registers {A, B, C, D}, calls a procedure g that uses \n{A, B}, which in turn calls a procedure h that uses {B, C}. Where is the relevant context for f? When \nh is active (i.e., its frame is on the top of the stack), A and B are saved in g s stack frame, C in \nh s stack frame, and finally, D still m the register! To save f s context and resume it later, we must \nfind where they are. Informa-tion is not present in stack frames as to which &#38;lee-save registers \nare used by a procedure or where they are saved. Even if it is present, interpreting information and \nrestoring registers would make context switch prohibitively slow. The way in which we handle callee-save \nregisters instead relies on an assumption about the code generation style of C compil- ers. The assumed \ncode generation style of C compilers is that a procedure saves callee-save registers at the entry (or \nprologue) of the procedure and restores them at the exit (epilogue) of the procedure, all at once. In \nother words, a procedure does not incrementally save them depending on the control path. The assumption \nis true of all the compil- ers we know of including GNU C compilers. The program- mer s manual of Mips \n1161 explicitly states that callee-save registers are saved at entry. This assumption validates an interesting \ntechnique for blocking a thread and resuming the parent in a consistent state, called epilogue code threading, \nwhich is further described in Section 4.3.  3.3 Summary: Where is Context? When a procedure invocation \nI is inactive (i.e., its stack frame is not at the top of the stack), the relevant context for I consists \nof: (1) the local and temporary variables of I, (2) the incoming parameters of I, (3) the stack pointer \nand frame pointer of I s frame, (4) and the callee-save registers of I. Locals and temporaries are in \nI s frame; incoming parameters are in the frame of Z s caller. The frame pointer is saved in the frame \nof the direct child of I (unless the direct child does not save it at all). The stack pointer is the \nframe pointer of the direct child of I, hence it may still be in the register or savedin the frame of \nthe grandchild of the thread. Finally, callee-save registers are hard to locate. The next section further \ndetailshow to locate the first three constituents and how to capture the callee-save registers.  4 StackThreads: \nFramework and Implemen- tation 4.1 Overview The basic execution scheme of StackThreads is simple and \nhas already been published elsewhere by the authors [30]. When we fork a new thread which evaluates a \nprocedure f, we call f just as a sequential call. When f blocks, it can resume its caller by moving its \nframe from the stack to the heap and unwinding the stack. Since the caller can be rescheduled even if \nthe callee blocks, we effectively create a new thread of control in this sequence. When f is later rescheduled, \nthe context is restored on top of the stack and control transfers to the point where f blocked. What \nneeds to be clarified is which part of a stack frame and registers must be saved/restored and how to \ncorrectly capture them from conventional C stack frames. 4.2 Creating Threads by C Procedure Calls Suppose \nwe wish to fork a new thread which evaluates the body of a C procedure f. The parent of the thread just \ncalIs f, passing parameters in exactly the same way as normal C procedure calls. If f successfully terminates \nits execution without blocking, the result value is obtained as the return value of the procedure call. \nIn StackThreads, however, con- trol may return to the caller even if f does block, in which case the \nreturn value from f is unspecified. Once f has blocked, it does not make sense for f to return the result \nvalue by means of C s return statement, as the caller may no longer be scheduled immediately after the \nreturn. Hence, it is often necessary for a thread to tell the caller whether it terminated or blocked, \nand, if blocked, the location through which these two threads thereafter communicate. In particu- lar, \nso-called sequential calls must be implemented using this kind of protocol, if the calls may be blocked. \nStackThreads does not define any fixed protocol for this, based on the observation that an appropriate \nprotocol is often language dependent and sometimes unnecessary. The protocol is, for example, unnecessary \nin pure Actor-based languages where all method invocations aredone via an asynchronous mes-sage and the \nresult value is passed via another asynchronous message. Section 5 shows example protocols for passing \nthe result value for a future-like communication primitive.  4.3 Resuming the Parent by Epilogue Code \nThreading StackThreads provides a way in which a thread saves its context into heap and resumes the frame \njust below the cur- rent frame (the current parent of the thread). The current parent is the original \ncaller (creator) when the thread blocks for the first time. When a thread A was blocked and another thread \nB later resumes A, B becomes the current parent for A. Notice that StackThreads by no means forces the \nrun-time system of the language to resume its current parent immediately when a thread blocks. It may \nspin a while, try to find other work locally or from the network, or even run the garbage collector when \nappropriate, and the right ac-tion cannot be determined by thread libraries alone. This is the reason \nwhy StackThreads supports the parent resuming as a library, rather than as a built-in response to a thread \nblocking. Suppose a thread P forked f, which now wishes to re-sume P again. It allocates a heap frame \nof appropriate size by calling library function allot-ctxt and then calIs switch-to-parent to trigger \nthe actual context switch, pass- ing the allocated frame to switch-to-parent. The proce-dure switch-to-parent \nfills the heap frame with the con-text of f and resumes the current parent of f. When f is resumed later, \ncontrol returns to f as if switch-to-parent returned normally. The allocation and the actual context \nswitch are separated because the context switch code needs to perform a language-specific action on the \ncontext (e.g., storing the pointer to the context somewhere for later re-sumption). The typical context \nswitch sequence is to first allocate a heap frame, save the pointer to the frame some-where to implement \nthe language construct, and then calI svitch-to-parent. This interface also allows the language   \n P (C p MO,...,, svitchtogarent 4 f + P handler which cqm-es Figure 2: Control flow and stack frame \nlayout when P forked f, which called suitch-to-parent to block f. The control is at the point denoted \nby the current position and we now resume P. We copy local variables, temporaries, and pa- rameters of \nf to heap within switch-to-parent and capture callee-save registers in a special handler that saves all \ncallee- save registers. Dotted line indicates the control path along which we resume P. implementors \nto reuse the same memory for saving context over multiple suspensions in a single procedure. Let us see \nhow the procedure svitch-to-parent works. For now, let us make an assumption for simplicity, which we \nwill relax later, that f directly calls suitch-to-parent, thus switch-to-parent knows the frame of the \nblocking thread is just below the current frame. Stack frames and the control flow when svitch-to-parent \nis just called di-rectly by f are illustrated in Figure 2. Thick lines denote prologue or epilogue code \nsequences of procedures. To later restart j as if switch-to-parent returned to f, we have to (1) capture \nand save the state of f at the point when f called switch-to-parent (C. in the figure), and (2) trans-fer \ncontrol to the return address of f (Rf in the figure), with the values of stack/frame pointers and callee-save \nreg- isters at the point when P called f (Cj in the figure). The state of f consists of local variables \nin the frame, incom-ing parameters of f, and &#38;lee-save registers. For saving locals, we need the \nhighest and the lowest address of the local variable save area in the frame of f, and for incom- ing \nparameters, we require its size (its offset is a constant through all procedures). Saving callee-save \nregisters is more complex. Since we do not know which callee-save regis-ters switch-to-parent destroys, \nthe only feasible way to capture the callee-save registers at C. is to actually run the epilogue code \nof svitch-to-parent and then capture callee-save registers there.5 We achieve this by modifying the return \naddress of svitch-to-parent so that it jumps to a special handler routine after running the epilogue \nof svitch-to-parent. The special handler routine saves all &#38;lee-save registers defined by the register \nusage convention and then jumps to the epilogue code off. The epilogue code off then restores the call-save \nregisters f uses and returns to P. The control path along which we save callee-save reg- isters for f \nand resume P is indicated by the dotted line in 5W e might know which callee-save registers switch-to-parent \nis us- ing by looking at assembly code generated from it, or by writing it in assembly in the first place. \nIn general, however, switch-to-parent is called indirectly from f. In that case, restoring only callee-save \nregisters destroyed by switch-to-parent does not restore callee-save registers correctly. -1 I J ----I \n < The handler which capt- all cake-save registers Figure 3: Control path along which we resume the current \nparent off (general case). Return addresses of every parents but the direct child of f (b in the figure) \nare redirected to the epilogue code of its parent. The return address of b is redirected to the special \nhandler which saves all calle+save registers. Figure 2. Notice that the control path is equivalent to \nthe regular control path, except that the rest of the procedure body of f is just skipped. We have so \nfar assumed that svitch-to-parent is di- rectly called from f. Let us relax this assumption and now consider \na general case where svitch-to-parent may be called indirectly from a procedure that is called from f. \nSuppose P forked f, which called a function b, which fi-nally decides to call svitch-to-parent to block \nf, perhaps from another procedure which is called from 6. The gen-eralized situation is illustrated in \nFigure 3. The semantics we implement is that f later restarts computation as if b returns to f. In other \nwords, switch-to-parent saves the context of f and resumes P, while aborting all computation from svitch-to-parent \nback to f. StackThreads allows switch-to-parent to be called in- directly for the sake of language implementors. \nIf it were callable only from the toplevel of the thread itself, a thread must always inline a code sequence \nwhich determines whether the thread continues or blocks. In such cases, inlined se-quences sometimes \nbecome unpleasantly long, so we wish to inline only frequent cases in which a thread can continue with \na small overhead and leave other cases in a separate procedure. To implement the above semantics, we \nmodify the return address of all procedures from switch-to-parent back to b. Every frame but b is redirected \nto the epilogue code of its caller and b is redirected to the handler which saves all callee- save registers. \nThe control flow from svitch-to-parent to P is threaded through all the epilogue sequences in the call \nchain, as is indicated by the dotted line in the Figure 3.  4.4 Resuming a Blocked Thread by Call Chain \nReconstruction Suppose a thread A satisfies the condition whereby a blocked thread f can now restart \nexecution. Thread A can restart f by calling restart-thread(c), where c is the heap context filled by \nsvitch-to-parent. The basic operations are as fol- lows; build local variables and incoming parameter \nregions 6For example, in our implementation of ABCL/f, checking whether or not to block at a potential \nblocking point includes four conditionals. Such inlined decisions increase code size and obscure the \nbackend C compiler. A restart-thread f (I )aUocate stack frame forf, (2)copy bcal variablesand param%en \noi (3)rephoe the link to the parent off. (4)save all calke-save registers, (5)twOre callee-save registers \noff, and (6)jutq to the restarting pointR,. restore callee save registerssaved at (4) l~+hvaUd) epiloguecode \n Figure 4: Control path along which we restart a blocked thread f. After building a stack frame for f \non top of the stack (1 and 2), we replace the return address and the FP of the parent at (3) so that \nf returns to restart-thread. We then save calleesave registers (4), restore callee-save reg-isters captured \nwhen f was blocked (5), and jump to the restarting point. The epilogue off is no longer valid. Hence \nrestart-thread restores callee-save registers saved at (4) by itself. for f on top of the stack, restore \n&#38;lee-save registers for f, set FP and SP to the new frame location, and jump to the restarting point. \nCare must be taken so that, after f finishes or blocks again, f correctly returns to restart-thread with \nthe correct callee-save register state. Since we directly jump into the middle of f, f does not run the \nregular prologue sequence. This in turn implies that executing the epilogue sequence off does not return \nto restart-thread, but to the original caller of f, with values of callee-save registers, SP, and FP \nfor the original caller. Our solution for this consists of two parts. First, restart-thread overwrites \nthe slots of f s frame which hold the link to the caller, so that f returns to restart-thread with the \ncorrect values of FP and SP. More specifically, it overwrites the slots which save the FP of the caller \nand the return address. Second, restart-thread saves c&#38;lee-save registers before restarting f, and \nrestores callee-save registers by itself after f returns. To summarize, restarting a blocked thread involves \nthe following steps: (1) allocate a stack frame for f, (2) copy local variables and incoming parameters \nonto the stack, (3) replace the link to the parent (return address and the FP of the parent) with the \nlink to restart-thread, so that f returns to restart-thread, (4) save all callee-save reg-isters, (5) \nrestore all &#38;lee-save registers captured when f was blocked, and (6) jump to the restarting point. \nSince f returns to restart-thread with invalid callee-save regis- ter state, restart-thread restores \nall callee-save registers saved at (4) after f returned. The control path along which we restart f and \nj eventually returns to restart-thread is illustrated in Figure 4. 4.5 Limitations and Discussion While \nStackThreads aims to support as wide a range of pro- gramming practices in C as possible, there are certain \nlimi- tations. First, StackThreads fundamentally relies on the fact that stack frames are mobile, or \nmore precisely, stack- allocated data are accessed relative to the FP or SP. This is not the case for \nall C programs and compilers, of course. Stack-Threads prohibits taking the address of stack-allocated \nob-jects and assuming the address remains valid across context switches. Worse, even if a procedure does \nnot explicitly take the address of stack data, an optimizer can cache such an address in a general purpose \nregister and use the ad- dress throughout a procedure. This is done for aggregate data (arrays or structures) \nallocated on the stack. Over-all, StackThreads discourages allocating any aggregate data structure on \nstack. We believe this is not a fatal restriction for garbage-collected languages. Second, the reader \nmight realize that there are alterna- tive choices as to which primitives StackThreads supports. It supports \nsaving the context of a single stack frame (recall that in Section 4.3, when a thread f calls switch-to-parent \nindirectly through a procedure call chain, only the frame of f is saved and other frames in the call \nchain are simply discarded). A consequence is that a sequential call to a possibly blocking procedure \nmust check a flag after the pro- cedure returns and cascade the unwinding operation if the return value \nis not present. We investigated the possibility of supporting a sequential call by the library, so that \nin a sequential call, the control transfer to the caller implies the presence of the return value. We \nfinally concluded this is hard to implement on some calhng conventions. To support the above sequential \ncall semantics, the library must unwind multiple stack frames in a single library call and later move \nthem to another location, maintaining the call chain between frames. Moving multiple frames together \nwould be simple if stack frames would be linked with only relative addresses. Unfortunately, however, \nthis is not the case in some proce-dure linkage conventions including Spare; each frame stores the absdute \naddress of the frame pointer of the caller. Tore construct call chains in such conventions, each frame \nwould have to supply information about the frame before making a sequential call. On Spare, this takes \nabout 10 instructions per frame, nullifying the advantage that the value present check is unnecessary. \nOur current implementation instead imposes no overhead before a call and the overhead is paid after the \ncall by checking a flag and cascading unwinding operations as necessary. Finally, StackThreads cannot \nutilize live variable infor- mation to minimize context switch cost. We save and re-store the entire \nregion which holds all local variables for a procedure. Although this is potentially a problem, the per- \nformance evaluation in Section 6 indicates that other factors are more dominating.  4.6 Clarifying Assumptions \nand Machine- Specific Details The assumptions made about the code generation style of C compilers for \nStackThreads to work are: . Scalar local variables or parameters on the stack are accessed relative to \nSP or FP. . Each procedure has only a single prologue and epilogue sequence. Furthermore, we need four \npieces of information about C procedures. They are: 0 size of parameters, int f (I, y. 2) c if (x > \n0) C return . . . ; 3 else { . . . = &#38;&#38;EPILOGUE; switch-to-parent (. . . ); 3 EPILOGUE: ; Figure \n5: The skeleton of a StackThreads procedure. The la- bel EPILOGUE is put at the end of the procedure \nand captured by &#38;&#38; operator. EPILOGUErefers to the epilogue sequence of the procedure. . epilogue \ncode address, . return address location, where we find the return ad- dress of a procedure. and . caller \nFP location, where we find the FP of the caller of a procedure (this is necessary only when a proce- \ndure maintains the link to the caller by the absolute addressees of the caller s frame). Our implementation \nhas been built using the unmodi- fied GNU C compiler version 2.7.2 under the Spare version 8 calling \nconvention [27]. As the register usage convention, we used the -mflat option, which does not use register \nwin- dows. This convention retains interoperability with legacy C binaries that use register windows. \nIn our environment, the size of parameters is obtained via the built-in function __builtin_args_info(O), \nwhich returns the number of pa- rameters of the current procedure in words. The epilogue code address \nis obtained by putting a label at the end of a procedure and capturing its address using the It&#38; \noperator, as shown in Figure 5. The location of the return address and caller FP are less straightforward. \nThe stack layout of the GNU C compiler is shown in Figure 6. It locates them at the lowest two words \nof the local variables save area, putting the caller FP at the bottom and the return address next. The \noffset of this area from both the FP and the SP varies from one procedure to another. Fortunately, however, \nthe space just below the local variables area is for stack allocation via alloca and we can obtain the \naddress of the bottom word by requesting zero(!) bytes using the alloca (i.e., alloca(0)) before making \nany other alloca request. To sum up, when a thread calls a procedure which may eventually call switch-to-parent, \nit passes a descrip- tor which holds (1) the result of alloca(O), (2) the address of the epilogue code \nof that procedure, and (3) the result of --builtin-args-info(O) for the procedure. When such a procedure \ncalls another procedure which may eventually call switch-to-parent, it passes (l), (2), and the link \nto the previous descriptor to the next procedure. We provide In GNU C, O&#38;L returns the address of \nlabel L. The reality is slightly more complex. There may be one word gap between the address returned \nby alloca(0) and the address where the caller FP ie saved. In that caee, the returned addresa contains \nnothing. Fortunately, we can diatinguish these two cases by reading the one word above the retUrned address \nand testing if this word contains the address of a text segment or an address in the stack. parameters \nI I arg size FP Locals &#38; T temporaries Locals I, Return address SP IStack Growth Figure 6: Stack \nframe layout of GNU C Compiler on Spare with -mflat option (ignore register windows). The two words just \nabove the alloca region are the caller s FP and return address. macros which set up the above information. \nIn the exam-ples below, we call the former macro SET-THREAD-DESC and the latter macro SET-LIIJK,DESC \n. 5 Implementing Higher Level Abstractions on Top of StackThreads 5.1 Remote Read Figure 7 illustrates \na simple example in which a procedure suspends its computation when it needs to read remote data. It \nfirst checks whether the data is local. If the data is remote, it allocates a context, sends a request \nmessage to an owner node, and calls switch-to-parent to suspend the procedure. The protocol is defined \nso that the data is put in the global variable R when the procedure is later reac-tivated. Prior to blocking, \na descriptor of the stack frame is filled by SET-THREAD-DESC( td) and passed to aJ.loc-ctxt and switch-to-parent \nto carry the information necessary to save and restore the stack frame.  5.2 Fork-Join Consider the \nsimple fork-join protocol illustrated in Fig-ure 8. A master thread (master) forks a number of child \nthreads (task)and waits for all the created threads to fin- ish. The master and all the children share \na counter (a sync object) which keeps the number of unfinished tasks. The master forks child threads \nsimply by calling the C proce-dure task in a loop and then calls join-children to wait for their completion. \nAssume each task may block during 1: { 2: . . . 3: 4: static stxuct thread-desc tdCl1; 5: /* try to \nread DATA to x */ 6: if (is-local (data)) i 7: x = data->-&#38;.; 8: 3 else I 9: char * c; 10: SETJHREAD-DESC(td) \n; 11: c = allot-ctxt (td); 12: remote-read-request (data, c>; 13: switch-to-parent ltd. c> ; 14: /* returns \nhere when unblocked */ 15: x = R->-al; 16: 3 17: /* X has now the right value */ 18: 19: . . . 20: 21: \nEPILOGUE: ; 22: 3 Figure 7: A simple code fragment which blocks the current procedure if data is remote. \nIf remote, it fills a frame de- scriptor, aUocates context, sends a remote read request, and switches \nto its parent. its computation. The master blocks if some tasks are still unfinished when the master \ntries join-children. There are basically two scenarios. If no children actually block, ev-ery procedure \ncall to task simply decrements the counter before returning to the master (line 13). The master then \nchecks the counter in join-children, to find the counter is already zero, and falls through (line 42). \nIf, on the other hand, any child is blocked, the counter may not be zero when the master checks it in \njoin-children. In that case, the master allocates the heap context, writes the context to the counter, \nand calls switch-to-parent to block itself (line 43-51). When the last child finishes, it will find the \ncontext written in the counter and restart the master (line 16-18). This example illustrates how to design \nand imple- ment synchronizing operations (i.e., operations which may trigger a context switch) on top \nof StackThreads primitives in a simple case. In this example, join-children is the syn- chronizing operation. \nIn general, a thread which calls a syn- chronizing operation must call SET~TIiREAD~DESC( td) , which \nputs information of the current frame in the area pointed to by td, and pass td to the synchronizing \noperation. The synchronizing operation checks the synchronizing condition and when it decides to block, \ncalls SET-LINK-DESC(Ink, td), which fills the area pointed to by Ink with information about the current \nframe and links Ink to td.  5.3 Return Value Passing Protocols StackThreads aliows a thread to return \na value via the nor- mai C return sequence when the thread terminates without blocking. Once a thread \nblocks, however, it does not make sense for a thread to return a value in this way, because the original \ncaller may not be present just below the cur-rent frame. This is inconvenient when building future-like \n1: typedef struct sync c 2: int count; /* t of unfinished tasks */ 3: char * wait; /* waiting context \n*/ 4: 3 * sync-t; 5: 6: void task (s) 7: sync-t s; 8: I 9: . . . do work. assume ve may 10: block during \ncomputation . . . . 11: 12: /* I am now finished */ 13: s->count--; 14: /* if everybody has finished \nand the 15: master is waiting, wake up the master / 16: if (s->count == 0 &#38;A s-Wait) C 17: restart-thread \n(s-await); 18: 3 19: EPILOGUE: ; 20: 3 21: 22: void master (n> 23: int n; 24: { 25: static struct thread-desc \ntdfll ; 26: 27: /* fork B child tasks */ 28: sync-t s -make-sync (n); 29: for (i = 0; i < n; i++) task \n(8); 30: 31: /* vait for everybody to finish */ 32: SET-THREAD-DESC(td>; join-children (8, td); 33: /* \ncontinue vork . . . . / 34: 35: EPILOGUE: ; 36: 3 37: 38: /* TD = descriptor of RASTER */ 39: void join-children \n(s, td) 40: sync-t s; thread-desc-t td; 41: I 42: if (s->count > 0) C 43: /* vhen there are unfinished \n44: tasks, ve svitch to parent */ 45: struct thread-deec InkCll ; char * c; 46: SET-LIBK-DESC(lnk, td); \n47: /* allocate context, vrite it to S, 48: and SWITCH-TO-PARENT */ 49: c = rilloc-ctxt (Ink); 50: s-Wait \n= c; 51: svitch-to-parent (Ink. c); 52: 3 53: EPILOGOE: ; 54: 3 Figure 8: A simple fork-join protocol. \nThe sync object (s) counts the number of unfinished tasks. The master blocks on a sync object by leaving \nthe context in it. A finished task decrements the counter and the last task wakes up the master if the \nmaster is waiting. primitive or even a sequential call. Here, we sketch how to express such abstractions \nin StackThreads, using two previ- ously published schemes as examples. 5.3.1 Potentially Blocking Sequential \nPro- cedure Call-Concert Hybrid Execu- tion Model Figure 9 shows a variant of lazy context creation scheme \nin the Concert hybrid execution model [24], changing unim-portant details for the presentation. Lazy \ncontext creation defines how a potentially blocking procedure passes its re-sult value to the caller, \ngiven that control may return to the caller even if the callee has not finished. Suppose procedure j \ncalls procedure g, which may block. If g finishes without blocking, g clears the global flag blocked \nand returns the re- sult value via C s return statement (line 52-54). If g blocks, on the other hand, \ng allocates a heap context, sets the flag to point to the context, and switch to j. After control re-turns \nto j, j tests the flag and cascades blocking if the flag is non-zero (line 18-33). To maintain the call \nchain between j and g after g blocks, j links g s context to j s context before blocking (line 27). When \ng is later resumed and fi- nally finishes, g checks if another context is linked from g s context and \nif one is, resumes it. The return value is written in j s context (line 47-50).  5.3.2 First Class Communication \nChannel Protocol In the authors previous work (301, we have proposed an im- plementation scheme for efficient \nfirst-class communication channels. The computation model has no inherent notion of sequential calls. \nAll procedure calls are, at least concep- tually, asynchronous calls. Threads communicate and syn- chronize \nvia communication channels. To return the result value of an invocation, each procedure, in addition \nto nor-mal parameters, takes an extra parameter, called the reply channel. A future call is expressed \nby a channel creation, a procedure call which passes the new channel as the re-ply channel, and receiving \nthe result later from the reply channel. A sequential call is just a special case of a future call. Given \nthat thread creation is sufficiently fast, the key question is how to implement channels efficiently. \nWhen a thread passes a new empty channel to a new lo- cal thread, it merely sets a special flag value \nwhich indicates an empty channel, without allocating memory for it. When the callee writes a value to \nan empty channel which is not yet assigned to a heap location, it merely writes the value to a register \nand sets the flag of the channel to indicate it is full. When a procedure terminates leaving one value \non the reply channel, it simply returns the value as the return value of the procedure. When a procedure \nis blocked, on the other hand, the reply channel is converted to a boxed representation-a heap memory \nis allocated for it-and the flag is set to the pointer to it. This boxing operation is also performed \nwhen the reply channel escapes from the callee s context; for example when it is passed to a remote proces-sor \nor stored into a data structure. When the callee tries to receive the result value, it checks the 5ag \nand if the flag indicates full, the value is simply extracted as the return The original lazy context \nallocation does the reverse; value return is written into memory, while the flag is returned to the \nprocedure. 1: typedef struct ctxt 2: C 3: char * ctxt; /* StackThreads context */ 4: int /* result vdlue \n. result-val; . 5: struct ctxt * cant; /* link to caller .. 6: 3 * ctxt-t; 7: 8: ctxt-t blocked; 9: 10: \nvoid f 0 11: c 12: static struct thread,desc td[l]; 13: int r; 14: 15: /* code template vhich calls 16: \nmay-block procedure G */ 17: r = g 0; 18: if (blocked) I 19: /* G has blocked, thus F 20: also blocks \n*/ 21: char . c; ctxt-t f-ctxt; 22: ctxt-t g-ctxt = blocked; 23: SET-THREAD-DESC(td); 24: c = allot-ctxt \n(td); 25: f-ctxt -make-ctxt (cl; 26: /* link G s ctxt -> F s ctxt */ 27: g-ctxt->cont = f-ctxt; 28: /* \ntell the caller of F that 29: F has blocked */ 30: blocked = f-ctxt; 31: svitch-to-parent (td, c); 32: \nr = f-ctxt-aresult-val; 33: 3 34: printf ( result = Xd\\n , r>; 35: . . . 36: 37: EPILOGUE: ; 3 ;;; 40: \nint g 0 41: I 42: . . . computation of G takes place, 43: during vhich G may block . . . . 44: 45: /* \nreturn sequence of may-block procedure */ 46: if (g-ctxt->cont) C 47: /* C has blocked. 48: Write result \nvalue and vake up F. */ 49: g-ctxt->cont-Bresult-vaI = result; 50: restart-thread (g-ctxt->cont-Bctxt); \n51: 3 else I 52: /* C has never blocked */ 53: blocked = 0; 54: return result; 55: 3 56: EPILOGUE: ; \n57: 3 Figure 9: A variant of the Concert lazy context allocation scheme. When a procedure g blocks, it \nallocates a context and sets the global variable blocked to it. After g returns, the caller (j) checks \nthe flag and blocks if it is not zero. The cant field of the g s context is set to the f s context so \nthat g can later wake up f when g finishes. Category ] Description # of Instructions 1. Allocate context \n1 l-l. Calculate context size I 33 1-2. MALLOC 28 1-3. Initialization 27 2. Switch to parent 2-1. Return \naddress modification 22 2-2. Copy locals and temporaries 16 + 3.251 2-3. Copy parameters 10 + 5p 2-4. \nSet buffer for &#38;lee-save registers 11 2-5. Epilogue 6  3. Save callee-save regs 3-l. Save all callee-save \nregisters 7+r  3-2. Epilogue of the thread 11 (depends on the thread) Others 15 Total (w/ context allocation) \n186 + 3.251+ 5p + r Total (w/o context allocation) 98+3.251+5p+r Table 2: Breakdown of the blocking \ncost in # of instructions. Parameters 1, p, and T refer to the number of locals and temporaries, incoming \nparameters, and callee save registers, respectively. In 2-4., we set the address of the buffer to a global \nvariable, in which callee save registers are written in 3-1. 1 Descrmtion 9 of instructions 1. Setup \n21 2. Copy locals and temporaries 16 + 3.251 3. Copy parameters 10+5p  4. Reinstall caller links 13 \n 5. Copy callee-save registers to stack 1 + 2T 6. Check if it should free the context 6 7. Swap callee-save \nregisters with stack 1 + 2T Total 68 + 3.251+ 52, + 4r  Table 3: Breakdown of the resuming cost in \n# of instructions. Parameters 1, p, and r refer to the number of locals and temporaries, incoming parameters, \nand callee save registers, respectively. value of the procedure. A notable point of this model is its \ngenerality and simplicity. There are no inherent notions of sequential calls or even asynchronous calls. \nMany primitives can be constructed from threads and communication chan-nels, including result value passing \nand mutual exclusion. Efficiency of frequent cases is maintained by efficient thread creation and the \nelaborate representation of communication channels.   6 Performance Evaluation 6.1 Micro Benchmark \nIn StackThreads, a thread creation is just a procedure call. Thus the overhead for a thread creation \nis that of a proce- dure call on the target machine + whatever necessary for implementing a specific \nlanguage construct. Therefore the interesting numbers are the cost of blocking and resuming. Table 3 \nbreaks down the cost of blocking and resuming in cases where switch-to-parent is directly called from \nthe blocked thread. Numbers are given as instruction counts on Spare. The overhead depends on the number \nof local variables of the procedure (1), the number of incoming pa-rameters of the procedure (p), and \nthe number of callee save registers in the convention (Y-). The cost of blocking also de- pends on whether \nwe must allocate a fresh context or can reuse the context of previous blocks of this procedure. In the \nregister usage convention of Spare where r = 14 and a typical procedure where 1 = 16 and p = 3, a block \ncosts 267 instructions (when we allocate a fresh context) or 179 in- structions (when we reuse a context) \nand a resuming costs 191 instructions. Copying locals and parameters accounts for one third of the total \ninstruction count for a block or a resume. In the simple benchmark program in which a thread repeats \nblocking and resuming, the time taken for a block-resume pair was 2.31 ps on 159Mhz HyperSparc pro-cessor. \nThis is comparable to the result reported by Plevyak et.al (241. As is already discussed in Section 4.5, \nour scheme totally ignores live variable information, copying all locals and parameters between stack \nand heap. Nevertheless, these figures show that this possible limiting factor is not a big problem in \npractice. Exploiting live variable information would make 1 (or p) the number of live local variables \n(or parameters) at the point, rather than the total number of slots allocated for local variables (or \nparameters) of the pro- cedure. Even assuming 1 = p = 0 would save at most one third of the blocking \nor the resuming cost. We also note that almost all instructions for context switch are shared in a li- \nbrary. In the benchmark program, the inlined sequence for blocking is only nine instructions. All other \ninstructions are shared by all context switch sites or are necessary anyhow (such as the epilogue sequence). \nThis is difficult to achieve if we inline context switch sequences to exploit live variable information. \n 6.2 Application Benchmark 6.2.1 Evaluation Settings We measured performance of three applications \nsummarized in Table 4. Application BH [3] RNA (191 CKY [23] . . Application BH RNA CKY 1 Description \n1 Barnes-Hut Nbody simulation Parallel tree search CFG parser by CKY algorithm 1 FK/SW fork a thread \nat: 1 each visit at BH tree node each visit to a node in the search tree each computation of Si,j Table \n4: Benchmark Applications TSQ TFK Tsw # of forks (F) # of syncs (S) # of blocks (B) 5,224 5,321 6,338 \n1,298,124 1,323,554 68,360 1,095 1,101 1,395 160,411 5,803 6,602 6,746 14,560 Table 5: Execution time \nfor each version (T.90, TFK, and Tsw) points, and actual blockings (F, S, and B, ;&#38;ectively). - BH: \nBarnes-Hut Nbody simulation [3, 131. In parallel pro- grams, all processors share a global data structure \n(called BH-tree). Particles are partitioned into pro-cessors and each processor calculates acceleration \nof the assigned particles sequentially. We only measured the force-calculation phase of the algorithm. \nRNA: Parallel RNA secondary structure prediction by com- binatorial search + pruning [19]. Parallelism \nis simply extracted by parallel recursion until the depth of the recursive call reaches a given limit. \nThe limit is given by the user at runtime. The appropriate value for the limit depends on input data \nand pruning conditions, thus is not predictable at compile time. CKY: CFG parser by a parallel CKY algorithm \n[20, 231. For an input sentence of length n, we calculate sets Si,j (0 5 i < j 5 n) and each Si,j is \ncalculated by a separate thread (i.e., we create l/2 n(n + 1) threads). Thread which computes Si,j requires \nSi,k and sk,j for each k s.t. i < k < j and blocks if data has not been produced when necessary. For \neach application, we first wrote a pure sequential al- gorithm in C++ and compared it with ones which \nare aug- mented with calls to StackThreads primitives. Augmented versions are still executed on a single \nprocessor (HyperSparc 15OMhz). More specifically, . the augmented versions fork a new thread at each \npoint where it would be required in a true parallel/distributed execution. . the augmented versions check \na synchronization con-dition at each point. where it would be required in a true parallel/distributed \nexecution. . the augmented versions fork a new thread at each se- quential call to a potentially blocking \nprocedure. A potentially blocking procedure is a procedure which may check a synchronization condition \nin its body. The last item is due to the fact that StackThreads does not support a sequential call to \npotentially blocking procedures. It must be constructed via a thread creation + explicit syn-chronization/communication \nbetween procedures. The re-ply value from a potentially blocking procedure is obtained via a first class \nchannel protocol presented in Section 5.3.2. 1 SW switches a thread when: I the node is not accessed \nrecently the depth of the node < D (a constant) Si,j itCCeSSt?S Si,k Or sk,j before computed 177,341 \n33,861 941,690 28,524 in milliseconds and the number of forks, potentially blocking Applications are \ncompiled with GNU C++ (g++) with the highest (-04) optimization level. We compared the fol- lowing three \nversions. SQ: True sequential execution in C++. No multithreading overhead is imposed. FK: SQ + the above \naugmentation. Overheads are imposed for fork and synchronization condition checking. Syn-chronizations \nactually never fail, thus the control flow is still equivalent to SQ. Points where a thread is forked \nand synchronization conditions is checked differ from one application to another. SW: Emulate execution \non distributed memory machines by blocking threads at synchronization points. The condition in which \na thread blocks differ from one ap- plication to another. In SW version of BH, a computation blocks when \nit accesses a BH-node which has not been accessed recently. When a force calculation starts, no BH-nodes \nhave been accessed re-cently. When a node is accessed, the computation is blocked and the node is marked \nrecently accessed. Accesses to node which are marked do not cause blocking. We clear all the marks every \n128 particles calculations. This effectively emulates a distributed memory execution where a thread is \nblocked when accessing a remote node and the remote node is cached for subsequent references. Clearing \nall the marks once in every 128 particles, emulates an execution where each processor is responsible \nfor 128 particles. Since each recursive call to a BH-node potentially encounters a remote node, each \nrecursive call is a potentially blocking procedure call. Hence we fork a thread at each recursive call \nboth in FK version and SW version. In SW version of RNA, a computation blocks at each recursion with \ndepth < D, where D is given at command line. This pessimistically emulates a distributed memory execution \nwhere recursions are distributed to remote pro-cessors until the recursion reaches a certain depth. In \ntrue distributed memory execution, the caller of a remote call is not blocked if it has other works when \nthe remote proces-sor is evaluating the remote call. Thus, this emulation is a pessimistic estimation \nof the execution on a true distributed memory machine in terms of switch frequency. A thread is forked \nat each recursive call both in FK version and SW version, regardless of the depth. Pyz ag qm bjx Figure \n10: Overhead of each version relative to true sequen- tial program. The original sequential CKY calculates \nSi,j (0 5 i < j 5 n) in such a way that Si,j with smaller j -i are computed before Si,j with larger j \n--i. This guarantees that neces-sary data has already been produced when necessary. FK attaches a thread \nfor each computation of Sij. SW version of CKY reverses the order in which Si,j are computed. This scheduling \nblocks all threads with j -i > 1 at least once. Points at which threads are created and conditions in \nwhich a thread blocks are summarized in Table 4. 6.2.2 Runtime Overhead Figure 10 shows the execution \ntime (relative to true sequen- tial execution) of the FK and SW version. FK estimates the overhead which \nappears when we execute the parallel program on a single processor. The sources of overhead in- clude \nforks, synchronization condition checks, and creation of synchronizing data structures. SW estimates \nthe fork and switch overheads which appear in true parallel execution on distributed memory parallel \ncomputers (other sources of overheads such as communication overhead are not in-cluded). In all the benchmarks, \nFK overhead is within 15% and SW overhead is within 30%. In CKY, the fork over- head is relatively high \nbecause accesses to Si,k and sk*j by the thread which computes Si,j, which are just an array reference \nin true sequential version, are performed by syn- chronizing accesses. The overhead also includes creation \nof synchronizing data structure for each Si,j. Table 5 shows the execution time of each version (call \nthem TSQ, TFK, and TSw, respectively) in milliseconds, # of forks (F), # of synchronizations (S), and \n# of blocking (B) which occurred in the benchmark. In other words, S is the number of potentially blocking \npoints, and B the number of actual context switches. Execution times are given in milliseconds on HyperSparc \n150Mhz processor. (T w -TPK)/B gives us a rough approximation of the cost o f a switch. Although they \nvary depending on applica- tion they are roughly between 5~s and 15~s.  7 Summary and Conclusion StackThreads \noffers a practical approach to implementing efficient multithreading languages. It supports very low \noverhead thread creation and efficient thread switching be-tween normally written C procedures. Unlike \nprevious im-plementations of efficient multithreading, it does not re-quire extensive cooperation from \nthe code generator, thus the compiler writer can map the sequential constructs of the language straightforwardly \nonto C, while using the low overhead multithreading support of StackThreads for paral- lel/concurrent \nprimitives. Performance measurement through three parallel applications shows encouraging results; fork \nand synchronization check overhead are never significant (< 15%), switch cost is comparable to one of \nthe best known results [24] which, unlike ours, needs a cooperating com-piler, and switch frequency in \nthese applications are low enough to make the overhead of switches acceptable in prac- tice (< 30%). \nAcknowledgements The presentation of this paper was substantially improved by a number of comments and \ncorrections by Robert H. Halstead Jr. Our thanks are due to Masahiro Yasugi for his comments on an early \nversion of the paper. We also thank to anonymous reviewers for their comments. The work were not possible \nwithout paral-lel computer APlOOO+ which was serviced by Fujitsu Lab- oratory. References 111 Gul A. \nAgha. Actors: A Model of Concurrent Compu-tation in Distributed Systems. The MIT Press, Cam-bridge, Massachusetts, \n1986. Andrew W. Appel. Compiling with Continuations. Cambridge University Press, 1992. PI Josh Barnes \nand Piet Hut. A hierarchical O(N log N) force-calculation algorithm. Nature, 324:446-449, 1986. PI K. \nMani Chandy and Carl Kesselman. CC++: A declarative concurrent object-oriented programming notation. \nIn Research Directions in Concurrent Object-Oriented Programming, chapter 11, pages 281-313. The MIT \nPress, 1993. 141 Andrew A. Chien. Concurrent Aggregates (CA). MIT Press, 1991. PI Andrew. A. Chien, U. \nS. Reddy, J. Plevyak, and PI J. Dolby. ICC++ -a C++ dialect for high perfor- mance parallel computing. \nIn Proceedings of the Sec- ond International Symposium on Object Technologies for Advanced Software, \n1996. Eric C. Cooper. Adding threads to standard ML. Tech- nical Report 90-186, Carnegie Mellon University, \nPitts- burgh, December 1990. I71 Eric C. Cooper and Richard P. Draves. C Threads. De-partment of Computer \nScience, Carnegie Mellon Uni-versity, 1987. 181 David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten \nvon Eicken, and John Wawrzynek. Fine-grain parallelism with minimal hardware support: A compiler-controlled \nthreaded abstract machine. In Pro-ceedings of the Fourth International Conference on AT-chitectural Support \nfor Programming Languages and Operating Systems, pages 166-175, 1991. PI [lo] Marc Feeley. Lazy remote \nprocedure call and its im- plementation in a parallel variant of C. In Proceedings of International Workshop \non Parallel Symbolic Lan-guages and Systems, number 1068 in Lecture Notes in Computer Science, pages \n3-21. Springer-Verlag, 1995. [ll] Mark Feeley. An Eficient and General Implementation of Futures on Large \nScale Shared-Memory Multiproces-sors. PhD thesis, Brandeis University, 1993. [12] Seth Copen Goldstein, \nKlaus Erik Schauser, and David Culler. Enabling primitives for compiling parallel lan- guages. In Workshop \non Languages, Compilers and Run- Time Systems for Scalable Computers, 1995. (131 Ananth Y. Grama, Vipin \nKumar, and Ahmed Sameh. Scalable parallel formulation of the Barnes-Hut method for n-body simulations. \nIn Proceedings of Supercomput-ing 94, pages 439-448, 1994. [14] Robert H. Halstead, Jr. Multilisp: A \nlanguage for concurrent symbolic computation. ACM Transactions on Programming Languages and Systems, \n7(4):501-538, April 1985. [15] Kenichi Hayashi, Tunehisa Doi, Takeshi Horie, Yoichi Koyanagi, Osamu Shiraki, \nNobutaka Imamura, Toshiyuki Shim&#38; Hiroaki Ishihata, and Tatsuya Shindo. APlOOO+: Architectural support \nof PUT/GET interface for parallelizing compiler. In Proceedings of Architectural Support for Programming \nLanguages and Operating Systems, pages 196-207, 1994. [16] Gerry Kane and Joe Heinrich. MIPS RISC Architec-ture. \nPrentice Hall, 1992. [17] Steve Kleiman, Devang Shah, and Bart Smaalders. Pro-gramming with Threads. \nSunSoft Press, 1996. 1181 Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. Lazy task creation: \nA techinque for increasing the granularity of parallel programs. IEEE fiansactions on Parallel and Distributed \nSystems, 2(3):264-280, July 1991. [19] Akihiro Nakaya, Kenji Yamamoto, and Akinori Yonezawa. RNA secondary \nstructure prediction us-ing highly parallel computers. Comput. Applic. Biosci. (CABZOS), 11, 1995. [ZO] \nAnton Nijholt. Parallel approaches to context-free lan-guage parsing. In Parallel Natural Language Processing, \npages 135-167. Ablex Publishing Corporation, 1994. [21] R. S. Nikhil and Arvind. Id: a language with \nimplicit parallelism. Technical report, Massachusetts Instituted of Technology, Cambridge, 1990. (221 \nRishiyur S. Nikhil. Parallel symbolic computing in Cid. In Takayasu Ito, Robert H. Halstead, Jr., and \nChristian Queinnec, editors, Proceedings of Intema-tional Workshop on Parallel Symbolic Languages and \nSystems, number 1068 in Lecture Notes in Computer Science, pages 217-242. Springer-Verlag, 1995. [23] \nTakashi Ninomiya, Kenjiro Taura, Kentaro Torisawa, and Jun ichi Tsujii. A scalable implementation of \npar- allel CKY algorithm in concurrent object-oriented lan-guage ABCL/f. In Proceedings of JSSST Workshop \non Object-Oriented Computing (WOOC), 1997. [24] John Plevyak, Vijay Karamcheti, Xingbin Zhang, and Andrew \nA. Chien. A hybrid execution model for fine grained languages on distributed memory multicomput- ers. \nIn Supercomputing 95, 1995. [25] A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. Sup- porting dynamic \ndata structures on distributed memory machines. ACM !i mnsactions on Programming Lan-guages and Systems, \n17(2):233-263, 1995. [26] Klaus Erik Schauser, David E. Culler, and Thorsten von Eicken. Compiler-controlled \nmultithreading for le- nient parallel languages. In Proceedings of 5th ACM Conference on Functional Programming \nLanguages and Computer Architecture, volume 523 of Lecture Notes in Computer Science, pages 50-72, Cambridge, \nMA, Au- gust 1991. [27] SPARC International Inc. The SPARC Architecture Manual, 1992. [28] Kenjiro Taura, \nSatoshi Matsuoka, and Akinori Yonezawa. An efficient implementation scheme of con- current object-oriented \nlanguages on stock multicom-puters. In Proceedings of the ACM SIGPLAN Sympo-sium on Principles d Practice \nof Parallel Programming (PPOPP), pages 218-228, 1993. [29] Kenjiro Taura, Satoshi Matsuoka, and Akinori \nYonezawa. ABCL/f: A future-based polymorphic typed concurrent object-oriented language -its design and \nimplementation -. In Proceedings of the DZMACS workshop on Specification of Parallel Algorithms, num-ber \n18 in Dimacs Series in Discrete Mathematics and Theoretical Computer Science, pages 275-292. Ameri-can \nMathematical Society, 1994. [30] Kenjiro Taura, Satoshi Matsuoka, and Akinori Yonezawa. StackThreads: \nAn abstract machine for scheduling fine-grain threads on stock CPUs. In Pro-ceedings of Workshop on Theory \nand Pm&#38;ice of Paral-lel Programming, number 907 in Lecture Notes on Com- puter Science, pages 121-136. \nSpringer Verlag, 1994. [31] David B. Wagner and Bradley G. CaIder. Leapfrog-ging: A portable technique \nfor implementing efficient futures. In Proceedings of the ACM SZGPLAN Sympo-sium on Principles d Practice \nof Parallel Programming (PPoPP), pages 208-217, 1993. [32] Akinori Yonezawa, Jean-Pierre Briot, and Etsuya \nShibayama. Object-oriented concurrent programming in ABCL/l. In OOPSLA 86 Conference Proceedings, pages \n258-268, 1986.  \n\t\t\t", "proc_id": "258915", "abstract": "It is difficult to map the execution model of multithreading languages (languages which support fine-grain dynamic thread creation) onto the single stack execution model of C. Consequently, previous work on efficient multithreading uses elaborate frame formats and allocation strategy, with compilers customized for them. This paper presents an alternative cost-effective implementation strategy for multithreading languages which can maximally exploit current sequential C compilers. We identify a set of primitives whereby efficient dynamic thread creation and switch can be achieved and clarify implementation issues and solutions which work under the stack frame layout and calling conventions of current C compilers. The primitives are implemented as a C library and named StackThreads. In StackThreads, a thread creation is done just by a C procedure call, maximizing thread creation performance. When a procedure suspends an execution, the context of the procedure, which is roughly a stack frame of the procedure, is saved into heap and resumed later. With StackThreads, the compiler writer can straightforwardly translate sequential constructs of the source language into corresponding C statements or expressions, while using StackThreads primitives as a blackbox mechanism which switches execution between C procedures.", "authors": [{"name": "Kenjiro Taura", "author_profile_id": "81100136693", "affiliation": "Department of Information Science, Faculty of Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku Tokyo 113 Japan", "person_id": "P159861", "email_address": "", "orcid_id": ""}, {"name": "Akinori Yonezawa", "author_profile_id": "81100439203", "affiliation": "Department of Information Science, Faculty of Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku Tokyo 113 Japan", "person_id": "PP15032269", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258944", "year": "1997", "article_id": "258944", "conference": "PLDI", "title": "Fine-grain multithreading with minimal compiler support&#8212;a cost effective approach to implementing efficient multithreading languages", "url": "http://dl.acm.org/citation.cfm?id=258944"}