{"article_publication_date": "05-01-1997", "fulltext": "\n Module-Sensitive Program Specialisation Dirk Dussart* Rogardt Heldalt John HughesS Abstract We present \nan approach for specialising large programs, such as programs consisting of several modules, or libraries. \nThis approachis based on the idea of using a compiler genera-tor (cogen) for creating generating extensions. \nGenerating extensions are specialisers specialised with respect to some input program. When run on some \ninput data the gener-ating extension produces a specialised version of the input program. Here we use \nthe cogen to tailor modules for spe- cialisation. This happens once and for all, independently of all \nother modules. The resulting module can then be used as a building block for generating extensions for \ncomplete pro- grams, in much the same way as the original modules can be put together into complete programs. \nThe result of run- ning the final generating extension is a collection of residual modules, with a module \nstructure derived from the original program. 1 Introduction Programmers are often faced with an uncomfortable \nchoice between generality and efficiency -writing a general pro- gram which can solve a wide class of \nproblems may save work in the future, but on the other hand a specific program for the particular problem \nin hand may be considerably more efficient. Over the last decade program specialisation has emerged as \na way out of this dilemma. Given a suitable specialiser, the programmer can write one general program \nsolving a class of problems, and automatically generate from it an efficient special purpose program \nfor each particular problem to be solved. Most real programs are structured as a collection of mod- ules, \nonly some of which are particular to the program con- *Home page: http://w~.cs.kuleuven.ac.be/Ndirkd/, \nemail ad-dress: dirkdOcs.kuleuven.ac.be, Supported hy the Belgian National Fund for Scientific Research \n(N.F.W.O.). This project wan started while visiting Chalmers University, Sweden. Thanka to Chahnera for \ntheir support. Home page: http://aww.cr.chalmers.se/Nheldal/, email addrew heldalOce.chalmers.se, Supported \nby chalmers. *Home page: http://www.ca.chalmers.ae/-rjmh/, email address: rjmhBcs.chalmers.se, Supported \nby the Swedish Board for Technical Research (TFR). Permission to make digital/hard copy of part or all \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advan- tage, the copyright notice, the title of the publication and its date \nappear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists. requires prior specific permission and/or a fee. PLDI \n97 Las Vegas, NV, USA  0 1997 ACM 0-89791-907-6/97/0006...$3.50 cerned. The remainder belong to libraries, \nand indeed, it is not unusual for a program to consist of relatively little new code, which makes use \nof very large and comprehensive libraries to provide most of its functionality. The tension between generality \nand efficiency is particularly pronounced for library modules: on the one hand, they must be general \nsince they should be applicable in a wide variety of different programs, on the other hand they must \nbe efficient for the same reason. Program specialisation ought therefore to be particularly valuable \nfor library modules. Unfortunately, today s specialisers completely ignore the module structure of programs. \nEither the program to be specialised is provided in the form of one monolithic mod-ule, and the generated \nprogram is created as one module. Or calls to functions defined in another module are regarded as primitive \ncalls -for example, Similix [Bon931 allows ex-ternal functions to be defined as primitives. Calls to \nsuch functions are either fully reduced, when all arguments are available at specialisation time, or \notherwise left unchanged. Thus such functions are never specialised. There is simply no way to perform \nfunction specialisation without exposing the definitions to the rest of the program. While this has worked \nwell to date, as specialisation is applied to larger and more realistic programs it has a number of disadvantages: \nThe input to the specialiser, consisting of the source code of the program plus all libraries it uses, \nmay be unreasonably large. As a result, specialisation may take an excessively long time, or even be \nimpossible. The source code of all library modules must. be available to the specialiser. For libraries \nwhich are commercial products, this is not a realistic assumption. The genera&#38;d programs may be unreasonably \nlarge: too large, in fact, to be analysed and compiled by the available program analysers and compilers. \nIn this paper we address these problems, by introducing a method for module-sensitive program speck&#38;&#38;ion \n. We address the first problem by preprocessing each module to prepare it for specialisation, converting \nit to its generating e&#38;n&#38;on. This can be done independently of the other mod- ules, so for example \nthe library modules can be preprocessed in advance. Preprocessed modules can be special&#38;d much faster \nthan source modules, and so (assuming the libraries have already been preprocessed) the cost of specialising \nan individual program can be substantially reduced. This ap-proach also addresses the second problem: \nsince only the generating extensions of library modules are needed for spe- cialisation, the source code \nof the library need not be avail- able. Finally, we address the third problem by using the module structure \nof the original program to suggest a struc- ture for the specialised code. Background Although program \nspecialisation can take a variety of forms, the most common approach is partial evaluation [X%89, JGS93] \n. The basic idea is to specialise a program by fixing the values of some of its inputs, and then simplify \nthe program by performing all static computations that de-pend only on the known inputs. The remaining \ndynamic computations are simply copied into the residual program. The classic example is the power function, \npowernz=ifn=lt.henzeIsezxpower(n-1)s which we can specialise by fixing n to be 3, for example, yielding \npower, x = x X (x X x) If we think of the static inputs as specifying a particular problem instance \nin a class, and the dynamic inputs aa the data for that particular instance, then partial evaluation \ngenerates a program solving the specified problem. In this case we generate a cubing program given the \nspecification (3) of a particular instance of the general exponentiation problem. In this example we \nunfolded the recursive calls of power, but this is not necessary. We could equally well have gener- ated \nseveral specialised versions of power, a technique known as polyvariant specialisation: power, x = x \nx power2 x power, 2 = x x power, x power, x = x Partial evaluators process specialisation requests in \nbreadth-first order or in depth-first order. A breadth-first specialiser queues specialisation requests \nin a pending -list. Then usually a done list is used to keep track of the spe- cialisations already created. \nEntries are removed from the pending list, the new version created, and added to the done list; when \nthe pending list becomes empty then specialisa-tion is complete. In contrast, a depth-first specialiser \nprocesses requests immediately by suspending the current specialisation. This specialisation is only \nresumed after all subsequent requests are processed. Since requests are processed immediately, no pending \n-list is needed. Both techniques lead to equivalent residual programs. When we write a general program \nfor a class of problems, we expect to need to solve many problems in that class. In other words, we expect \nto specialise the general program many times. This process can be accelerated by specialising the specialiser \nitself [Fut71]. Call the specialiser miz, and suppose we specialise a program P to input x by running \n mixPx=P, This section just introduces the basic concepts of &#38;line poly-variant partial evaluation, \nbinding-time analysis, generating exten-sion, and the handwritten cogen approach. It can be skipped by \nthe initiated. If we expect to do this often, we can construct mtip, the specialisation of mix to build \nspecialisations of P, and then construct each P, by mixp x = P, m&#38;p is called the genemting extension \nof P, since it gener- ates specialisations of P. Generating extensions are obtained by running mixmixP \n To obtain compact programs as a result, it is important that the first mizis able to simplify many tests \nin the second -in particular the tests that choose between treating operations in P aa static or dynamic. \nAn obvious way for a specialiser to decide whether an operation should be static is to inspect its operands: \nthey must either be known or unknown at spe- cialisation time, and if they are known then the operation \ncan be performed. But when we specialise miz, all the data is unknown! As a result, static and dynamic \nvalues cannot be distinguished by inspection, so such tests cannot be sim- plified away. The generating \nextension mixp must therefore take all possibilities into account, which means that its code must be \nlarge. This motivated the development of ofline partial evaluation, in which programs to be specialised \nare annotated to make the binding-time (static or dynamic) of each expression explicit. An off&#38;e \nmiz can decide whether to perform or residualise an operation purely by inspect ing the annotations on \nthe program; the data need not be considered. The annotations on P are available when its generating \nextension is created, and this permits the gener- ation of a compact result. Binding-time annotations \ncan be placed by hand, but in practice are normally inserted by a binding-time analyser. Generating extensions \nturn out to have a surprisingly natural structure. Apart from some house-keeping code to keep track of \nwhich definitions have been specialised, they contain, for each function f in P, a function mk-f to create \na specialised version of f's body. Static computations remain unchanged, while dynamic ones are replaced \nby operations to build abstract syntax trees. For example, the power func-tion above might give rise \nto the generating extension mk-power n x = if n = 1 then x else r&#38;X x (mk-power (n -1) 2) Here n \nis an integer parameter as before, but x will be bound to the abstract syntax tree of power s second \nargu-ment -a piece of code. The function mk-x creates an abstract syntax tree node for a multiplication. \nClearly, call- ing mk-power 3 x would create the body of power, in the example above. The simple relationship \nbetween source programs and their generating extensions has led some researchers to write programs, usually \ncalled cogen[HLSl, BW94], that construct generating extensions directly, rather than going via a par- \ntial evaluator. This approach can be simpler, because the cogen only does syntax manipulation [BW94]. \nSince self-application of a partial evaluator can be difficult to achieve, we have chosen to follow this \napproach in our application. 3 Language Of Interest We will consider the specialisation of a simple higher-order \nfunctional language, essentially a subset of Haskell, whose Program ::= Module Module ::= module Id where \n[import Iq Def Def ::= Id Id = E E ::= Nat]Id(PrimE*]ifEthenEelzeE] IdE IXId-,EIEQE Figure 1: Syntax \nof Programs. syntax appears in Figure 1. A program is a collection of modules. For simplicity we assume \nthat a module exports all of its definitions, and that there are no cyclic module de- pendencies. A module \ncontains a set of definitions of named functions; named functions are not implicitly curried, and may \nonly appear in named function applications together with all of their arguments. Anonymous functions \nare cre-ated by )r-expressions and are first-class values; they may be applied using the @I operator. \nWe distinguish between named and anonymous functions because the former will be specialised polyvariantly, \nwhereas the latter can only be unfolded. Specialisers such as Similix make a similar dis-tinction. In \nthe expression part of the language we have natural number constants, variables, primitive operations \n(such as addition), conditional expressions, function calls, lambda-expressions and applications of anonymous \nfunc-tions. The language is polymorphically typed, using the standard Hindley-Milner type system. Reducing \nthe Cost of Specialisation Our basic motivation is to reduce the cost of specialising large programs \nby, so far as possible, processing each module independently. Ideally one would specialise one module \nat a time, independently of all of the others. Unfortunately, this is impossible: consider two modules, \none defining power, and the other calling power with various arguments. We cannot specialise the module \ndefining power until we know which specialisations are needed; that is, which calls appear in the other \nmodule. Regretfully, we accept the necessity of specialising an entire program in one go. However, as \nwe have seen, specialisation can be acceler- ated by using the generating extension approach. But gen-erating \nextensions have a very simple structure: some house- keeping code, plus a generating version of each \nfunction in the original program -such as ml-power above. These gen- erating versions can be derived \njust from the source of the functions whose specialisations they generate, without refer-ence to the \nrest of the program. We can therefore construct the generating extension of a large program one module \nat a time: from each module in the source, we construct a corre- sponding module in the generating extension \ncontaining the generating versions of the functions in the source module. Now, when a large library is \nused in many programs, we can construct the generating extensions of the library mod-ules in advance, \nand even compile them. To specialise a program that uses the library, we just need to construct and compile \nthe generating extensions of the non-library mod-ules, link the code of all the generating extensions \ntogether, and then run the result. Neither the original source of the library modules, nor the source \ncode of their generating ex-tensions, needs to be available. The generating extension of a module is \ncomparable in size to the original source, and the same should be true of its compiled code. The generating \nextension of an entire program P should therefore be a code file comparable in size to the code of P \nitself. We assume that enough memory is available to run programs of this size, even if it is impossible \nto compile (or specialise) such a large program in one go. Running a generating extension is always faster \nthan run-ning the corresponding specialiser, because there is no need to inspect and interpret the source \ncode of the program to be specialised. But in our case, there is a further potential speed-up. A specialiser \nmust read, parse, and analyse every definition in a program before it can begin specialisation. Even \nfunctions which are not used incur a cost, therefore. In contrast, when a generating extension is used \ninstead, the cost-per-source-definition is very low: just the cost of linking and loading the code for \nthe corresponding gener-ating version. General purpose libraries often define very many functions, only \na few of which are used in any partic- ular application. Our approach is especially well suited to such \ncases, because only those functions which are actually specialised incur any significant cost. 4.1 Binding-Time \nAnalysis The fly in the ointment here is binding-time analysis. Nor-mally BTA is performed by first classifying \nthe inputs of the entire program as static or dynamic, and then propagat- ing this information throughout \nthe program. Any quantity which depends on a dynamic input is itself classified as dy- namic. In our \ncontext, this approach is unusable: we need to know the binding-times in a module when we construct its \ngenerating extension. At this time we don t even know which program(s) the module will eventually be \nused in, let alone the binding-times of their inputs. Thus we cannot derive the binding-times in a module \nfrom its uses. Moreover, most BTAs in use are monovariant, they as-sign a fixed binding-time to each \noperation. But in a very large program, it is likely that the same function may be applied to actual \nparameters with different binding-times at different calls. Especially in a setting, where we have to \ngenerate the binding-annotated version in absence of any information about the uses, the monovariant \napproach is rather unrealistic. In contrast with the monolithic setting we cannot make the step to a \npolyvariant BTA, again for lack of modularity. A polyvariant BTA can assign more than one property per \nfunction by distinguishing between all different uses and reanalysing the function for each dif- ferent \nuse. Since we don t know the uses before we know how the module is used, the use of a polyvariant BTA \nis out of the question. Our solution, borrowing from Henglein and Mossin s work [HM94], is to perform \nthe binding-time analysis sym-bolically, without knowing the actual binding-time proper-ties but by using \nbinding-time variables. This is possible because the set of computations we perform on binding-time properties \nis always the same. In essence, Henglein and Mossin s binding-time analysis is a way to factorise a polyvariant \nBTA into two parts: a symbolic or property-independent part, which is the same for all possible binding- \ntime inputs and can be reused, and a specialisation or property-dependent part. In the case of our power \nexample, we can note that the binding-time property for the condi-tional is always the property assigned \nto the test n = 1. The binding-time property for the test, is always computed  Progmm ::= Module Module \n::= module Id where [import Id] Def Def ::= Id {A?> Id =E E E ..- Nat I Id I PrimB E I Id {B } E I ifB \nE then E else E I XZd+EIE&#38;%I[T+T]E  B ..- SlDlIdlBuB T ..- BjT-BT Figure 2: Syntax of Annotated \nPrograms. by taking the lub of static (the binding time of 1) and the property for n, which can be symbolically \nreduced to the property of n. The result of the property-independent part of the analysis is an annotated \nprogram, with sym-bolic annotations. These annotated programs can then be specialised with respect to \nactual binding-time properties, yielding polyvariantly annotated programs [Bul93]. This extra specialisation \nstep can be avoided by letting the spe-cialiser create the annotated versions on the fly, on demand. \nThis means that we only have to create the versions that are actually needed. For this reason we choose \nto annotate the operations in a function definition without fixing them to be static or dynamic, and \nwe can then call the function at a variety of different binding-times. The syntax of annotated programs \nis defined in Figure 2. Named functions may have additional binding-time param-eters, enclosed in curly \nbraces. Binding times may be vari-ables, static (S) or dynamic (D), or the least upper bound of two binding \ntimes in the ordering induced by S < D. The equality sign in a function definition is annotated with \na binding-time to indicate whether the definition should be unfolded or special&#38;d to create a residual \nversion. The annotations on primitive operations and conditionals just indicate when they should be performed. \nEvery expression can be assigned a binding-time type. Expressions of base type are given a simple binding-time \nas their binding-time type, while anonymous functions are given binding-time types of the form a +b p. \nThe binding-time on the arrow indicates whether the function is static, and so should be unfolded during \nspecialisation, or dynamic, and so should appear in the residual program. Dynamic function types must \nhave purely dynamic arguments and results: all the binding-times in o and p must also be dy-namic. The \nnotation [a -, /?]e is a binding-time coercion, con-verting e which should have binding-time type a into \nan equivalent quantity with binding-time p. We do not anno-tate constants or X-expressions with binding-times: \nthese always denote static quantities, and when dynamic versions are required we insert a coercion. For \nexample, the power function could be annotated as follows: power {t u} n x =t ir( n= [S+t]l then [IL+ \n(t Llu)]x else [u + (t U u)]xxtUU power {t u} (n -* [S + t]l) x Here t and u are the binding-times of \nn and x respectively. If t is S, that is, n is static, then the definition can be unfolded and the conditional \nsimplified away; thus  power {S D} 3 x -x x (x x x) If t is D, so n is dynamic, then attempting to \nunfold the recursive call would lead to non-termination. The definition must be specialised instead; \nfor example power {D S} n 2 q power2 n where  power2 n = if n = 1 then 2 else 2 x power, (n -1) Therefore \nwe annotate the equality sign in the definition with a t, so that it is unfolded only if n is static. \nNotice that the multiplications can only be performed if both n and x are static. Notice also that numerous \ncoercions are needed to ensure that the binding-times are consistent. In this example, the binding-time \nparameters of power were just the binding-times of its ordinary parameters, but this need not always \nbe the case. In principle we could pass power all the binding-time annotations in its body as parameters, \nbut in practice the annotations are related, and so we can get away with fewer parameters without losing \nflexibility. Our annotations are closely based on Henglein and Moss-in s [HM94],with extensions made \nin collaboration with Dus-sart [DHM95]. They give a type system to verify that pro-grams are well annotated, \nin which functions are assigned types polymorphic in their binding-times. For example, power would be \nassigned the type Vt, u.t + u + t LIu. Hen-glein and Mossin show that every function has a principal \nbinding-time type, from which all others can be derived by instantiation. In general, the binding-time \ntypes in Dussart et al [DHM95], and also in this paper may contain type qual-ifications [Jon92]. This \nmeans that the types may contain binding-time restrictions or constraints. For example, con-sider the \nfollowing alternative type for the power function: Vt,u.{t 5 u} * t -+ u ---* u, where t 5 u means that \nthe function can only be called in case the binding time of the first argument is smaller than or equal \nto the binding time of the second argument. Dussart, Henglein and Mossin give an efficient algorithm \nfor inferring principal types. This al-gorithm can be used as a binding-time analysis, to infer the annotations \non function definitions. To infer the binding-times in a module, it needs to know the binding-time types \nof functions in imported modules, but it does not need any information about the uses of the module being \nanalysed. It is thus ideal for our purposes. Previous work on this binding-time analysis has been in \nthe context of a simply typed language. But simple types are far too restrictive for a language with \nmodules: imagine placing the map function in a library, but restricting it to work only on one type of \nlist! We have therefore extended the analysis to handle Hindley-Milner typed programs. The only annotation \nwhich this analysis cannot infer is that on function definitions, which determines whether they should \nbe unfolded or residualised. As we have seen, these annotations must depend on the binding-time parameters, \nand so they cannot be placed by hand in the original source code (in which these parameters do not appear). \nGood tech-niques for making special&#38;-or-unfold decisions automati-cally for higher-order languages \nare still an active topic of research -see for example Andersen and Holst [AH961 -but we have chosen \nto follow a rather conservative strat-egy first used in Similix. Similix introduces a new named non-unfoldable \nfunction for every dynamic conditional ex-pression in the source program, thus guaranteeing that any \ninfinite unfolding at specialisation time cannot be avoided by a conditional test at run time. Any program \nwhose special- isation fails to terminate, must fail to terminate when run. Since we want to preserve \na closer relationship between func- tions in the source and residual programs, we instead classify a \nfunction as non-unfoldable if its body contains a dynamic conditional, which has a very similar effect. \nConcretely, we annotate the equality sign in a function definition with the least upper bound of the \nbinding-times on the conditional expressions in its body; if any of these is dynamic, then the function \nwill not be unfolded. This also needs to be reflected in the result type of the function -if the function \nis consid- ered non-unfoldable, then the result of the function needs to be classified dynamic. For this \nreason, these annotations are also part of the annotations of the function s result value. Once a module \nhas been analysed, we write the binding- time types of the functions it exports to a binding-time inter-face \nfile. When analysing modules which import this one, we read their interface files and use the information \nto analyse calls of imported functions. This is just the same mecha-nism that many compilers use to typecheck \nacross module boundaries. Naturally, we are obliged to analyse modules in a suitable order, so that interface \nfiles are written before they are read. This is one reason why we cannot support cyclic module dependencies. \n4.2 Polymorphic Generating Extensions The result of the binding-time analysis is an annotated program, \nwhere the annotations might be symbolic values. Since we leave the creation of specialised annotated \nversions until specialisation time, our generating extensions need to include extra binding-time computations. \nAs a result we cannot exploit the binding-times to simplify generating ex-tensions. Look back at the \ndefinition of mk-power in section 2: we exploited the knowledge that n was static and x was dynamic to \nperform arithmetic on n directly, and generate residual code for operations on x. With variable binding-times, \nwe have to pass binding-time parameters in the gen- erating extension also. To avoid a code explosion, \nwe model each operation op in the source by an operation mk-op in the generating extension, with an extra \nbinding-time parameter to determine whether the operation is performed statically or dynamically. For \nexample, the generating version of power is shown in Figure 3. Looking first at mk-power-body, which \nconstructs speciahsed versions of puwer s body, we see that it corre sponds quite directly to the annotated \ndefinition of power. Each operation is replaced by a function call with a binding- time parameter to \ndetermine whether the operation should be performed or residuated. The function mk-power constructs specialisations \nof calls of power, and so needs to decide whether or not the call should be unfolded. This decision is \ntaken by mk-resid: if its first parameter is S then the call is unfolded. The other parameters of mk-resid \nare: . A triple which is used to identify the specialised version of power which is created when the \nfirst parameter is D.  mk-power t u n x = mk-resid t ( power , [t, u], [n, x]) (mk-power-body t u n \nx) (x[n , x ] + mk-power-body t u n x ) mk-power-body t u n x = mk-if t (mk-= t n (coerce S t (mk-n 1))) \n(coerce u (t Uu) x) (mk-x (t LI u) (coerce 21 (t U u) 5) (mk-power t u (mk--t n (coerce S t (mk-n 1))) \nxl) Figure 3: Generating Extension of power. . The result of unfolding the call, to be used if the first \nparameter is S. . A function to create the body of a specialised version, if the first parameter is D. \nThe first time a particular triple is encountered, mk-mid allocates a new residual function name for \nthat version and adds it to the pending list, returning a call of the newly defined function. On subsequent \noccasions, mk-resid just reuses the previously generated residual function. Haskell s lazy evaluation \nmeans that mk-r&#38;d s third p&#38; rameter, the result of unfolding the call, is not evaluated unless \nit is actually needed. ........ In a strict. one would need to replace this parameter by a function with \na dummy argument, to achieve the same effect. The function mk-resid has side-effects to be able to keep \ntrack of the specialisations already created. Since Haskell is purely functional, we use a monad [WadSO] \nto support side-effecting operations. When a residual call is created, the actual parameters are split \ninto their static and dynamic parts as usual. Of course, only the static parts are compared with previously \ngenerated specialisations to see whether a new version is required. The dynamic parts are replaced by \nnew formal parameter names -the formal parameters of the generated definition. These new names are passed \nto the fourth parameter of mk-resid, when it is called to create the new definition s body. For example, \nif we call mk-power D S 13 2 (where e represents residual code for e), then mk-resid is invoked with \nthe triple ( power , [D, s], [L13 , 21). When the dynamic ac-tual parameter 13 is replaced by a fresh \nvariable name n , then the list of actual parameters becomes [Ln , 21. It is this list that is passed \nto mk-power-body, so that references to n in the generated code are replaced by references to n . The \ngenerated definition has one formal parameter, namely n . Following Similix [Bon93], we represent static \nfunctions by closures containing the bound variable and body of the associated X-expression, and its \nenvironment. This is snf?% dent information for Similix to unfold the function at the point of call. \nBut when a generating extension calls a static function, of course we want to avoid needing to interpret \nits body. We have therefore added an extra field to static closures: a function which generates specialisations \nof the closure s body. Static functions may refer to dynamic values via the en-vironment they contain. \nWhen a static function is passed as a parameter to a residual function, these dynamic val-ues must be \npassed even in the residual program. For example, if z and ys are bound to dynamic values then map (Xx \n-+ x + z) ys must be specialised to mapxz z ys, where  m4pXz 2 ys = if null ys then 0 else (head ys \n+ t ) : mupx, z (toil ys) A slight complication is that when the static function is un- folded, we have \nto make sure that the residual code refers to the new formal parameter names. We use the same solution \na3 Similix. It is a simple matter now to write cogen by hand , that is, to write a program which automatically \nconstructs gen-erating extensions of this form. But because binding-times are variable, we do not obtain \nall the benefits usually asso-ciated with this approach. We are not able to use simpler representations \nin our generating extensions than in a cor-responding partial evaluator. When parameters are known to \nbe static, then they can be represented in a generating extension just by ordinary values -look back \nat parameter n of mk-power in section 2. Since we don t know binding- times when our generating extensions \nare created, we have to represent all values by types that can model both static and dynamic values -just \nas a partial evaluator must. This is the price we pay for making the generating extension as compact \nas possible. A similar price is paid in the imple-mentation of polymorphically typed functional languages, \nwhere arguments to polymorphic functions are required to be boxed. Also there, specialisation (i.e. monomorphic \nex-pansion) is a way to avoid the problem. Modularising Specialised Programs Even if we speed up specialisation \nusing generating exten-sions, we must still expect the resultof specialising a large program to be another \nlarge program. This specialised pro- gram must then be compiled. If it is produced in the form of one \nlarge module, this may simply be impossible. Therefore we break the residual program up into modules \nalso, each of which can hopefully be compiled reasonably fast. An obvious idea is to give the specialised \nprogram the same module structure aa the original one, and simply to place all the specialisations of \na function f in a module cor-responding to the one it is defined in. Unfortunately life is not quite \nso simple: we cannot in general use exactly the same module structure as the original program. Consider \nfor example the (partly annotated) program module A where f x =* . . . module E where import A  m0A.L~~ \n5-!2* ) import B hz=* 9(2x*4 where g is unfolded, but neither f nor h is. When h is specialised then \na residual version hz=f(2xt+l) will be placed in module C, while the residual version of f will be placed \nin module A. But C does not import A in the original program, and so if we simply re-use the original \nmodule structure, then f will not be in scope at the point where it is used. Our solution is to construct \nthe imports for each generated module by examining the code in the mod- ule, and ensuring that all modules \nreferred to are imported. At least when we specialise first-order programs, the only modules that can \nbe referred to are higher up in the import hierarchy, so that the generated module dependencies are guaranteed \nto be acyclic. It is possible that some residual modules may be empty. This can happen, for example, \nif none of the functions in the source module are needed for a particular specialisation. It can also \nhappen if all calls to functions in that module are unfolded. We simply avoid generating empty modules, \nand also of course their corresponding imports. Higher-order functions introduce an awkward complica-tion. \nConsider the program module A where map f ax =* if null x5 then 0 else f&#38; (head zs) : mop f (tail \n2s) module B where import A gz=Dx+Dl  h zs =* map (Xx + g x) zs where once again, we have inserted \nonly the interesting an-notations. If we specialise this program, placing specialisa- tions of each function \nin the same module that the function is defined in, then the specialisation of map to the static function \n(Xz -* g x) is placed in module A, with the result: module A where m4ps x5 = if null 23 then 0 else gQ(heod \nas) : mop, (toil 29) module B where import A 9x =x+1  h w = mops xs Now module A refers to g, defined \nin module B! In a sense this isn t surprising, since when we pass functions across module boundaries \nwe are effectively passing code, and so we can end up with arbitrary interdependencies in the resid- \nual programs. This is particularly pronounced for programs written in continuation-passing style, where \nthe calling con- text of a function is passed as an extra parameter. In the above example, we cannot \ninsert an import of B into module A, because that would lead to a cyclic module dependency. Instead, \nwe choose to move the specialisation of mop into module B. Indeed, there are other reasons to abandon \nthe idea that all specialisations of a function should appear in the same module. In the case of very \ncommon functions such as map it would lead to very large residual modules indeed. The functional programmer \nthinks of mop as a kind of looping construct; naturally we expect the code for a loop to appear in the \nsame module that the loop is used in -we certainly do not expect the code for all loops to be gathered \ninto one and the same module. If the function g in the example had not been defined in module B, but \nimported from a third module C, then no existing module would have been a natural home for the spe- cialisation \nmap,. Certainly mupg could be placed in module B: both map and g are in scope there. But suppose there \nis another module D which also imports A and C. Module D may well contain onother call map g, which after \nspecialisa- tion should refer to the same residual function -which was placed in module B! Importing \nB into D in such cases could easily produce cyclic module dependencies. We do not wish to place a duplicate \nversion of map, in module D. Instead, we create a new module A n C to contain mupg, which we can import \ninto both B and D, and any other modules that require it. We may need to generate combinations of any \nnumber of modules. The functions placed in A1 17 . . . A&#38; are those which require names from all \nof the modules concerned to be in scope. We do not need to include two modules one of which is imported \ndirectly or indirectly into the other in the original program. Nevertheless, in theory we might need \nto generate exponentially more residual modules than there are modules in the source. In practice we \nexpect the vast majority to be empty. This is the strongest reason why we must avoid generating empty \nmodules, and why we detect emptiness dynamically rather than looking for a conservative static approximation. \nAssigning functions to modules is an intrinsically depth- first problem. We can only assign a function \nto a mod-ule, if we know in which modules the functions it calls are placed. This suggest using a depth-first \nspecialisation strat-egy, which unfortunately may lead to very many specialisa- tions being active simultaneously, \nand may in turn require a great deal of space. As we want to keep memory con-sumption to an absolute \nminimum -we want to specialise large programs -we instead use a breadth-first strategy, which delays \nthe construction of further specialised func-tions (in a pending list) until the current specialisation \nis complete. Our experiments show that this strategy is con- siderably more space efficient. But to place \nthe specialisa- tion of the caller we have to know which modules the called functions (that still need \nto be generated) will be placed in. Hence we have to decide which modules to place a specialised function \nin when we discover the first call to it -that is, before the body of the specialised function is constructed. \nWithout constructing the body of a specialised function, we cannot know which modules are referred to \nfrom the body, but we can determine which modules might be re- ferred to. A call to be specialised consists \nof a function name, and values of static parameters which may be static closures. We collect all the \nfunction names which occur free in the call: the function to be specialised itself, the function names \nwhich occur free in the bodies of any static closures, and the function names which occur free in the \nenvironments of static closures (which can of course contain further static closures). The result of \nspecialisation can only refer to spe- cialisations of functions in this set. We therefore take the set \nof modules that these functions are defined in, remove any which are imported into others, and then place \nthe new specialisation in a combination of the remaining modules. For example, if the source program \nis module Power where  powernx=D ifn=l then x else x X power (n -1) x module Twice where twice j x =* \nf@l(fox) module Main where import Power import Twice main y = twice (Xx + power 3 x) y (where we have \nannotated the definitions non-unfoldable by hand) then the residual program has the following structure: \nmodule Power where  power, x = x x power2 x power, 2 = x x power1 x  power, x = x module PowerTwice \nwhere import Power twicepower 2 = power, (power, 5) module Main where import PowerTwice main 2 = twicepower \nx Module PowerTwice is a combination of two modules from the source program. Note that the module structure \nof the residual program is quite different from that of the source. As we have seen, we cannot decide \nwhich modules a gen- erated module should import until ufier we have generated its code. Since import \nstatements appear at the beginning of a module this compels us to use two passes: the first pass generates \nmodule bodies in temporary files, and the second pass generates module headers and imports, and then \ncopies the module bodies after them. Alternatively, we could have used depth-first specialisation, but \nas pointed out earlier this can require a lot more memory. 6 implementation We have implemented these \ntechniques in a prototype cogen for the little language used in this paper. It is implemented in Haskell, \nand both the generating extensions and residual programs are runnable Haskell programs. The cogen is \naround 800 lines of new code, which ex-cludes general purpose libraries for pretty-printing and pars- \ning. Of this, the cogen proper is less than 100 lines - wgen is very simple. In contrast the polymorphic \nbinding-time analyser is over 500 lines! Runnable generating extensions are produced by link-ing together \nthe modules produced by wgen with libraries providing the basic mechanisms of specialisation, and gen- \nerating versions of the language primitives. This common code amounts to around 300 lines of Haskell. \nThe compiled code of the generating extension of a module is four to five times larger than the code \nof the original module, at least for small programs. We attribute the expansion to the code needed to \npass and manipulate binding-time parameters, to the insertion of binding-time coercions, and to the fact \nthat simple primitive operations with short code in the source program become function calls in the generating \nextension. On the bright side, the size of the generating extension is linear in the size of the source \nprogram, so the expansion should not be substantially greater for large programs. The little language \nwe handle has proved useful for devel- oping and presenting the techniques, but is not sufficiently rich \nto write interesting larger programs in. As a result, a demonstration in practice of the value of our \nmethods as programs become larger must await a m-implementation for a more realistic language. Related \nWork pieces, but can be treated as a collection of modules. Once a This work was inspired by earlier \nwork by the two last au-thors (HH97]. In that work they show how to perform sep- arate compilation with \na partial evaluator, by specialising an interpreter for a modular language to one module at a time. They \nuse interface files to communicate intermodule dependency information to subsequent specialisations. \nThe resulting modules are the compiled versions of the original modules. They only treat a first-order \nlazy language. This work can be seen as the logical next step in the Futamura hierarchy [JGS93], where \nwe replace the interpreter by a specialiser. The special&#38;d modules then become generat-ing extensions \nor specialised specialisers. Where Heldal and Hughes use a partial evaluator, we use a cogen. 8 Further \nWork We have shown how the cogen approach can be used for specialising modular programs. In the cogen \napproach, the generating extension usually generates residual code in the same language as the original \nprogram was written in. This means that in order to run the residual modules we need to go through a \ncompile and link phase again. It also means that the generating extension reveals part of the original \nsource code: indeed the source of a function can be recon- structed by calling its generating extension \nwith completely dynamic arguments. This would be a problem for commer- cial products. In future work \nwe want to address this prob- lem by constructing generating extensions that produce na-tive code directly \n-partial evaluators which do so already exist. This also paves the way for applying our ideas in run- \ntime code generation (LL94]. In this context we would no longer need to divide the residual program into \nmodules. Inspired by the previous work of the last two authors [HH97] we also plan to investigate how \nto create a cogen which allows both programs and their input to be expressed in terms of modules. An \nexample application is a self-interpreter for a modular language. Here the interpreter can be written \nas a collection of modules, and the input program might also consist of a collection of modules. We have \nstudied a language with a simple module sys-tem. It would be interesting to see if our techniques can \nbe extended to handle parameterised modules, such as those found in ML. One problem here is that the \nuser would prob- ably need to supply a binding-time signature for the pa-rameter modules, just as an \nML programmer must supply a type signature -since our binding-analysis is a form of type inference. Binding-time \nanalysis would thereby become less automatic. Another interesting challenge is extending the placement \nalgorithm to work in the presence of both imported and parameter modules. To demonstrate that our techniques \nwork well for large programs, it will be essential to implement a cogen for a real programming language, \nin which large programs are available as test data! We hope to carry out this experiment for Haskell. \n9 Conclusion We have presented a module-sensitive approach to of-fline program specialisation, especially \nsuited for specialising large programs. A clear advantage over existing specialis- ers is that programs \ndo not have to be treated as monolithic module is added to a software system, it can be analysed and \ntailored for specialisation once and for all. For the analysis we only require that all imported modules \nhave been anal- ysed. The resulting module, or rather generating extension of a module, can then be treated \nas a black-box which can be linked with other generating extensions. An essential ingredient of this \ntechnology is a symbolic or polymorphic binding-time analyser [DHM95], which sup ports modularity by \nfactorising the analysis into a property- independent and a property-dependent part. In this paper we \nhave shown how to construct polymorphic generating ex-tensions given only the results of the property-independent \nbinding-time analysis. This results in very concise gener-ating extensions, in which little binding-time \ncomputations need to be performed at specialisation-time. Speciahsation is performed by linking the generating \nextensions together and running them. The residual program is broken into a set of residual modules, \nwhich has positive effects on the time taken to compile it. We can only hope to be able to specialise \nlarge programs if we can reduce the memory consumption at specialisation time to a bare minimum. We have \ngone to some lengths to ensure that only one specialisation need be active at a time, and that specialisations \ncan be written to files as soon as they are constructed. We have implemented our methods for a polymorphic \nhigher-order functional language, which although simple, in- cludes the most essential features of languages \nsuch as ML and Haskell. References [AH%1 Peter Holst Andersen and Carsten Kehler Holst. Termination Analysis \nfor Offline Partial Evalu-ation of a Higher Order Fmmtional Language. In Proceedings of the Third International \nStatic Analysis Symposium (SAS), 1996. [Bon931 Anders Bondorf. Similix 5.0 Manual. DIKU, Uni-versity \nof Copenhagen, Denmark, May 1993. In-cluded in Similix distribution, 82 pages. [Bu193] M. A. Bulyonkov. \nExtracting polyvariant bind-ing time analysis from polyvariant specializer. In ACM, editor, Proceedings \nof the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation. PEPM g3, \npages 59-85, New York, NY, USA, June 1993. ACM Press. [BW94] Lars Birkedal and Morten Welinder. Hand-writing \nprogram generator generators. In PLILP, MadridSpain, September 1994. Springer-Verlag. [DHM95] Dirk Dussart, \nFritz Henglein, and Christian Mossin. Polymorphic Recursion and Subtype Qualifications: Polymorphic Binding-Time \nAnal-ysis in Polynomial. In Alan Mycroft, editor, SAS 95: 2nd Int l Static Analysis Symposium, volume \n983 of Lecture Notes in Computer Sci-ence, pages 118-135, Glasgow, Scotland, Septem-ber 1995. Springer-Verlag. \nFuw Y. Futamura. Partial evaluation of computation process -an approach to a compiler-compiler. In [HH97] \n[Hw [HM94] (JGS93] [ Jon921 [JSS89]  (LL941 w=w Systems, Computers, Controls, volume 2, pages 721-728, \n1971. Rogardt HeldaI and John Hughes. Partial EvaI-uation and Separate Compilation. In Charles Consel, \neditor, ACM SIGPLAN Conference on Partial Evaluation and Semantics-Based Program Manipulation (PEPM 97) \nAmsterdam, June, 1997. C. K. Holst and J. Launchbury. Handwriting cogen to avoid problems with static \ntyping. In Dmjt Proceedings, Fourth Annual Glasgow Work- shop on Rmctional Programming, Skye, Scotland, \npages 210-218. Glasgow University, 1991. FXtz Henglein and Christian Mossin. Polymor-phic Binding-Time \nAnalysis. In Donald Sannella, editor, ESOP 94: European Symposium on Pro-gramming, volume 788 of Lecture \nNotes in Com- puter Science, pages 287-301. Springer-VerIag, April 1994. N. D. Jones, , C. K. Gomard, \nand P. Sestoft. Par-tial Evaluation and Automatic Program Genem-lion. Prentice-Hall, 1993. Mark P. Jones. \nA Theory of Qualified Types. In ESOP 92: European Symposium on Progmm-ming, volume 582 of Lecture Notes \nin Com-puter Science, Rennes, fiance, February 1992. Springer-Verlag. Neil D. Jones, Peter Sestoft, and \nHaraId Sondergaard. Mix: A self-applicable partial evaI-uator for experiments in compiler generation. \nLisp and Symbolic Computation, 2(1):9-50, 1989. DIKU Report 91/12. Mark Leone and Peter Lee. Lightweight \nRun-Time Code Generation. In Proceedings of the 1994 ACM SIGPLA N Workshop on Partial Eval-uation and \nSemantics-Based Progmm Manipula-tion, pages 97-106. Technical Report 94/9, De-partment of Computer Science, \nUniversity of Mel- bourne, June 1994. P. Wadler. Comprehending Monads. In Pro-ceedings of the 1990 ACM \nConference on Lisp and finctional Programming, pages 61-77, Nice, lhnce, 1990.  \n\t\t\t", "proc_id": "258915", "abstract": "We present an approach for specialising large programs, such as programs consisting of several modules, or libraries. This approach is based on the idea of using a compiler generator (cogen) for creating generating extensions. Generating extensions are specialisers specialised with respect to some input program. When run on some input data the generating extension produces a specialised version of the input program. Here we use the cogen to tailor modules for specialisation. This happens once and for all, independently of all other modules. The resulting module can then be used as a building block for generating extensions for complete programs, in much the same way as the original modules can be put together into complete programs. The result of running the final generating extension is a collection of residual modules, with a module structure derived from the original program.", "authors": [{"name": "Dirk Dussart", "author_profile_id": "81100619759", "affiliation": "Chalmers University, Sweden", "person_id": "P67462", "email_address": "", "orcid_id": ""}, {"name": "Rogardt Heldal", "author_profile_id": "81100274808", "affiliation": "", "person_id": "P247706", "email_address": "", "orcid_id": ""}, {"name": "John Hughes", "author_profile_id": "81100166325", "affiliation": "", "person_id": "PP31080508", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258934", "year": "1997", "article_id": "258934", "conference": "PLDI", "title": "Module-sensitive program specialisation", "url": "http://dl.acm.org/citation.cfm?id=258934"}