{"article_publication_date": "05-01-1997", "fulltext": "\n Near-optimal Intraprocedural Branch Alignment Cliff Young+, David S. Johnson* , David R. Karger*, and \nMichael D. Smith+ +Harvard University *AT&#38;T Labs *Massachusetts Institute of Technology Abstract \nBranch alignment reorders the basic blocks of a program to minimize pipeline penalties due to control-transfer \ninstructions. Prior work in branch alignment has produced useful heuristic methods. We present a branch \nalignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks \naverages within 0.3% of a provable optimum. We compare the control penalties and running times of our \nalgorithm to an older, greedy approach and observe that both the greedy method and our method are close \nto the lower bound on control penalties, suggesting that greedy is good enough. Sur- prisingly, in actual \nexecution our method produces programs that run noticeably faster than the greedy method. We also report \nresults from training and testing on different data sets, validating that our results can be achieved \nin real-world usage. Training and testing on different data sets slightly reduced the benefits from both \nbranch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits \nremain. 1 Introduction On modem pipelined microprocessors, control-transfer instructions (CT&#38;), such \nas conditional branches and unconditional jumps, often incur execution time penalties. The instruction \nfetch mechanism in these processors is best able to supply a steady stream of instructions to the proces- \nsor datapath when the instructions are ordered sequentially. A CTI that redirects the fetch mechanism \ncauses breaks in the sequential fetching of instructions, and in pipelined implementations, these breaks \ntemporarily starve the pipe- line datapath of instructions because the effects of a CTI are often not \nknown until late in the pipeline. For example, in the pipeline for the Digital Alpha 21164 microprocessor \n[4], the outcome of a conditional branch instruction is not known until the end of the sixth stage of \nthe pipeline, even though the next fetch address must be ready by the end of the first stage to maintain \nan uninterrupted flow of instruc- tions to the datapath. Several studies have shown that the compiler \ncan improve processor performance by reordering the pieces of a program so that breaks in the sequential \nfetching of instructions happen less often [2,9,19,23]. These studies use only greedy heuristics to guide \nthe reordering of the program blocks. A natural question to ask is: how good is the best possible reordering? \nDo the greedy approaches extract all possible Permission to make digital/hard copy of part or all this \nwork for persons1 or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advan- tage, the copyright notice, the title of the publication and its date \nappear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. PLDI \n97 Las Vegas, NV, USA 0 1997 ACM O-89791-907-6/97/0006...$3.5b benefit from code reordering? And if not, \nis it computation- ally feasible to find the ordering that maximizes the achiev- able benefit? In this \npaper, we reduce a limited form of the reordering problem to the Directed Traveling Salesman Problem \n(DTSP), allowing us to use DTSP analysis and solution techniques. Mathematically provable lower bounds \non DTSP costs give us the lowest control penalty that any branch alignment can hope to achieve. We also \napply recently developed powerful local search heuristics for DTSP that efficiently produce layouts that \noften meet the lower bounds. By comparing our near-optimal layouts with those produced by greedy methods, \nwe find that the greedy techniques capture a significant portion of the total potential pipeline penalty \nimprovement, but not all of it. In general, the compile-time reordering of program blocks is referred \nto as code placement. Good code placement tech- niques can reduce instruction cache misses as well as \npipe- line penalties due to CTIs. For example, placing the most likely follower of an instruction as \nits layout successor improves performance because the following instruction will be on the same or the \nnext cache line and will not incur any pipeline penalties when it is reached. Calder and Grun- wald [2] \ninvestigated basic block ordering techniques solely for the purpose of reducing pipeline penalties due \nto CTIs. They refer to this limited version of the code placement problem as the brunch alignment problem. \nFor purposes of discussion, we refer to CTIs generically as branch instruc-tions, unless it is important \nto the discussion to make a dis- tinction between specific types of CTIs, and we refer to the pipeline \npenalties due to branches as control penalties. mispredict penalty I 1 misfetch penalty I I target address \nbranch condition known known branch instr. fetch buffer &#38; decode multi-issue slotting r;i;;x execute \none execute tW0 register write back next instr. fetch buffer &#38; decode 0.0 next instr. fetch .a. t \n next fetch address needed Figure 1. Pipeline diagram for the Digital Alpha 21164 microprocessor: It \nhas a misfetch penalty of 1 cycle and a conditional branch mispredict penalty of 5 cycles. Control penalties \nare typically classified into two major sources: misprediction or misfetch penalties. Misprediction penalties \noccur on a conditional or multiwayl branch when the processor incorrectly predicts the instruction that \nfol- lows the branch. As shown in Figure 1, the Alpha 21164 has a mispredict penalty of five cycles since \nthe true direction of a conditional branch is known by the end of the sixth pipe- line stage. Misfetch \npenalties occur when the processor can- not determine the target address of a branch, even if it correctly \npredicts the branch, in time to fetch the target instruction from the cache. Figure 1 shows that the \nAlpha 21164 has a misfetch penalty of one cycle because the earli- est that the predicted target address \nis available is at the end of the second pipeline stage. This misfetch penalty occurs on any branch instruction \nthat redirects the instruction stream, e.g. a jump or a correctly-predicted taken branch. A number of \nhardware techniques reduce control penalties. Branch history tables [25] cache recent conditional branch \ndirections, reducing mispredict penalties. Similarly, branch target buffers [16] cache recent branch \ntarget addresses, reducing misfetch penalties. Like hardware-based branch techniques, compiler-based \nbranch alignment techniques use past program behavior as a predictor of its future actions. To obtain \nthis information, compilers can profile the program of interest to determine the most likely target of \neach branch in the program. Profile- based optimizations require good profiles to be effective. This \nis well-known in the static branch prediction literature, where researchers have gone to great lengths \nto show that 1. A jump to a target address contained in a source register operand is an example of a \nmultiway branch. useful, general training data sets can be found [5]. Disap- pointingly, we found that \nin the code placement and branch alignment literature, only Pettis and Hansen [23] present results using \ndifferent training and testing data sets. In this per, we show how to encode branch alignment as a DTS \np2p and then apply a suite of DTSP solution and analysis methods to the problem. In our experiments, \nwe also present performance results that use different training and testing data sets. These numbers \ncorrespond to the performance numbers that can be expected to hold in actual practice, rather than the \noptimistic results achieved by ideal training data sets. The next section explains our reduction of the \nbranch align- ment problem to a DTSP. The appendix describes the cur- rent state of the art in solutions \nto the DTSP Section 3 describes our experimental methodology, while Section 4 presents our results and \ndiscusses our findings. Section 5 summarizes the related work in code placement. The last section comments \non our work and outlines future research.   2 Branch Alignment and Its Reduc- tion to a DTSP We now \ndescribe the basics behind compile-time branch alignment and our transformation of this problem into \na standard DTSP. We assume that we are trying to optimize a 2. DTSP is a version of the traveling salesman \nproblem, where the goal is to find a minimum cost walk through a set of cities, given the distances between \nall pairs of cities. In the directed case, the distance from city A to city B is not the same as the \ndistance from city B to city A. program s layout in memory for a particular training input, with the \nhope that this will give a layout that also performs well for other inputs (as shown by our results in \nSection 4.2). Once the program input is fixed, the resulting execu- tion trace is fixed as well. 2.1 \nBasics of branch alignment An intraprocedural branch alignment is essentially a permu- tation of the \nbasic blocks of each procedure in the program, implemented with the appropriate inversions of conditional \nbranches and insertions or deletions of unconditional jumps to ensure that program semantics are maintained. \nGiven a processor model and a control-flow graph (CFG) weighted with execution frequencies on edges (the \nfrequencies are derived from the training input), a compiler estimates the number of penalty cycles that \nwill accrue from a particular permutation. This estimation is based on the execution-time penalty of \nhaving a particular basic block succeed another in the program layout. In some cases, this cost includes \nthe penalty due to the addition of new unconditional jumps at the end of basic blocks that no longer \nfall into their layout successor block. Globally minimizing these penalties is the goal of a branch alignment \nmethod. It is important to keep clear the distinction between one block succeeding another in the program \nlayout and one block following another in the execution trace. To avoid confusion, we will use the term \nsucceed when referring to the program layout and the termfollow when describing the execution trace. \nWe will always use the term CFG succes- sor when referring to the relations between nodes in the CFG. \nGreedy branch alignment approaches generate their guess at a good permutation of a procedure s basic \nblocks by consid- ering the edges in the CFG in some priority order, typically decreasing order of execution \nfrequency. As each edge is considered, two checks are made to see if the two blocks at the ends of this \nedge can be laid out consecutively. The first check passes as long as there is not some other block already \nsucceeding the block at the head of the edge, and there also is not some other block already preceding \nthe block at the tail of this edge. The second check verifies that adding this edge will not create a \nlayout cycle. If both checks pass, then the layout is updated. If either check fails, the algorithms \nmove on to the next priority edge. Once all the edges have been considered, the algorithms add uncon- \nditional jumps and invert conditional branches where neces- sary to complete the layout and maintain \nprogram semantics. Pettis and Hansen [23] and Calder and Grunwald [2] each give a more detailed description \nof these greedy branch alignment algorithms. Greedy algorithms generally work because they take the most \ncommon CFG successor A of a particular block and try to place it immediately after the block. If this \nlayout posi- tion is unavailable, it is because A has already been claimed by some other block that is \nfollowed by A more frequently. The basic drawback of greedy approaches is that they rely entirely on \nlocal information, and here they are further handicapped by the fact that they use frequencies rather \nthan cost models based on the target machine. Using more accu- rate cost models and looking at the problem \nfrom a global point of view could well lead to permutations with lower overall cost. 2.2 Reduction to \nDTSP In general, we wish to find a linear ordering of the program blocks that minimizes the total number \nof penalty cycles caused by mispredicts and misfetches. Consider any layout of the program blocks. The \ntotal number of penalty cycles is just the sum over all blocks of the number of penalty cycles occurring \nat the branch at the end of each block. We assume a fixed program trace and an architectural model where \nthe number of penalty cycles occurring at the end of a block B depends only on which block succeeds B. \n3 We therefore build a graph whose vertices are the program blocks. For blocks B and B , we place a directed \nedge (B,B? with cost c(B,B ) weighted by the number of penalty cycles that occur at B in a layout where \nB succeeds B. Note that this graph is a complete directed graph, with edges between every pair of vertices. \nEven if block B never branches to block B , we still put an edge from B to B with a cost that accounts \nfor any fixup branches that have been added. We also add a dummy block representing the end of the layout. \nThus, this graph is not the CFG. Now consider a particular walk through this graph-that is, a traversal \nof the vertices (blocks) in some linear order B,,...,B,. In order to visit these blocks, we traverse \na sequence of edges (B,, B2),(Bz,B3) ,..., (BnmI,B,,). The cost of this walk is defined to be the sum \nof the traversed edge costs, namely c(B~,B~)+c(B~,B,)+...+c(B,,-~,B,,). Since c(B,B 3 is the number of \npenalty cycles that occur at B if B succeeds B, we see that if we lay out the blocks in the order the \nwalk visits them, the total number of penalty cycles caused by the layout is equal to the cost of the \nwalk above. All that is left is the rule for assigning edge costs. We are given penalties for four different \nbranch outcomes. Let pn be the number of penalty cycles on a branch that is predicted taken and is taken \nand pm be the number of penalty cycles on a branch that is predicted not taken and is taken. We define \nPTN and PNN similarly. For the Alpha pipeline model in the introduction, pn is the misfetch penalty, \nand ,DTN and pNTare the mispredict penalty. We now consider a particular block B and the branch that \nterminates it. For any block B , let CsB-be the number of times that at block B the processor correctly \npredicts that block B will follow block B, and let res, be the number of times we branch from B to B \n when the processor incorrectly predicts that a different block will follow B. Observe that when B succeeds \nB, CBS p counts the 3. For example, machines that predict backwards branches will take and forward branches \nwill fall through (BTFNT architectures) do not meet this assumption. Benchmark Abbr. Description Data \nSets Abbr. 02dcompress corn Lempel-Ziv compressor SPEC ref input (program text) MPEG movie data in st \nOlS.doduc dad nuclear reactor thermohy- draulic simulation SPEC ref input SPEC small input l-e sm 023.eqntott \neqn translates boolean equa- tions to truth tables fixed to floating point encoder SPEC ref input fx \nip 008.espresso esp booleanfunction minimizer SPEC ti ref input SPEC tial ref input ti t1 089.su2cor \nsu2 ;ta&#38;ical mechanics calcu- spEc ref input SPEC short input l-e sh 022.li xli Lisp interpreter \nLisp impl. of Newton s method 7 queens problem ne q7 Table I : Descriptions of benchmarks and data sets \nexamined. Branch Sites Touched 56 56 657 651 309 303 1,458 1,440 318 316 295 367 Executed Branch Instructions \n 11.8M 135.4M 77.6M 13.4M 46.5M 335.8M 87.OM 157.2M 168.3M 13.1M O.lM 42.OM number of times that the \nprocessor correctly predicts the branch will fall through, but when B does not succeed B, C ,,- counts \nthe number of times that the processor correctly predicts that the branch to B will be taken. Since the \nnum- bers C ,, L and 1,, I depend only on the program s CFG and not on any particular layout of that \nprogram, it follows that if we select block X to succeed B, the total penalty at the end of block B is: \nd4 m = C,,PNN + I,,P*, + c (C,,.P, + I,,*P,,) B #X As argued previously, if we use this penalty count \nas the cost of the corresponding edge (E, X), then minimizing the total penalty time corresponds to finding \nthe minimum cost walk through the resulting graph. Section 3.3 presents the specific values used to compute \nedge costs. By the above reduction, an optimal solution to the con-structed DTSP will yield the branch \nalignment with mini- mum possible control penalty. Similarly, near-optimal solutions to the DTSP will \nyield near-optimal solutions to the branch alignment problems and lower bounds on the optimal solution \nvalue for the DTSP are lower bounds on the minimum possible control penalty. Our approach to obtaining \nnear-optimal solutions and lower bounds exploits a second reduction, one that takes arbitrary instances \nof the DTSP to the special case of the symmetric TSP (STSP), an approach whose power has only recently \nbeen recognized [ 1 I]. In this study, we compute near-opti- mal branch alignments by applying the iterated \n3-Opt algo-rithm for the STSP [lo] to a symmetrized version of the above DTSP, and compute lower bounds \non the penalty by computing the Held-Karp lower bound [6, 71 for that ver- sion. In general, our results \nimply that both the tours and the lower bounds typically come within 0.3% of the value of the optimal \nsolution. For more details and background on these approaches, please see the Appendix.  3 Experimental \nMethodology In this section, we describe our compilation and evaluation environments. Since we are examining \nthe practicality of different branch alignment techniques, we present two kinds of measurements: compile \ntimes and program execution times. A summary of the compile times for the different branch alignment \ntechniques is contained in this section. Program execution times are given in Section 4. Except for the \ngeneration of the DTSP solutions, we performed all compilations and performance evaluations on a Digital \nEquipment AlphaStation 500/266 running Digital UNIX version 3.2. This machine contains a 266MHz Alpha \n21164 microprocessor, 2 MB of third-level cache, and 128 MB of main memory. 3.1 Benchmarks As shown in \nTable 1, we report results for a subset of the SPECint92 and SPECfp92 benchmarks. SPEC92 bench-marks \ntouch a relatively small number of branch sites in the program text, but they execute a reasonable number \nof branch instructions (we will revisit xli.ne, the shortest-run- ning data set by far, in our cross-validation \nstudy). For each benchmark, we list two data sets; in the cross-validation study we report the name of \nthe testing data set and train with the other data set.  3.2 Compilation environment and compile- time \ncosts We compiled our programs using version 1.1.2 of the SUIF research compiler [30], including math-suif \n1.1.2 exten- sions [26]. SUIF is a research compiler that has been used extensively to study automatic \nparallelization, interproce-dural loop detection, and pointer analysis. The math-suif extensions support \nmachine-specific optimizations such as register allocation, instruction scheduling, and the branch alignment \noptimizations performed in this paper. We also used version 1.0 of the HALT instrumentation tool [33] \nto add branch and procedure call instrumentation to our bench- mark programs. As mentioned above, we \nperformed all aspects of our compilation process on the AlphaStation 500/ 266, except for the DTSP solver. \n In this study we were primarily interested in the ultimate savings obtainable by the DTSP approach, \nrather than in the immediate construction of an integrated tool. With this in mind, it was more convenient \nto run the DTSP codes at AT&#38;T Labs where they were originally developed and could most easily be \nadapted to this application. The runs were performed on single 250 Mhz R4400 MIPS processors in an SGI \nPower Challenge. These processors might be expected to run about half as fast as the AlphaStation. The \nrunning times were substantial, but not out of line with those for the other parts of the compilation \nprocess. More- over, there is reason to believe that much faster times would be possible in a production \nversion, as explained below. Table 2 summarizes the time spent compiling and profiling under our system. \nCompile times were collected using the UNIX time(l) command; this command has a resolution of 1110th \nof a second. The Intermediate Representation col- umn gives the time to compile from C or FORTRAN source \nto a math-suif intermediate form suitable for producing instrumented or optimized programs. The Instrumented \nProgram column shows the time to transform this interme- diate form into a profiling executable. The \nGreedy Pro- gram column lists the time to produce an executable pro- gram using the greedy layout algorithm. \nThe TSP Matrix column shows the time to transform the profile data into DTSP problem matrices. The TSP \nSolver column lists the time to solve the DTSP representations of our branch align- ment problems; recall \nthat the solver runs on a different machine so times are not necessarily comparable to other columns. \n(The time for computing lower bounds on the optimal solution is not included, since such computations \nare not needed for solution generation, and are used here solely for analytical purposes.) The TSP Program \ncolumn gives the time to build an executable using the solution tours. Lastly, the Profiling Run Time \ncolumn shows the time it took to run the profiling executable on the data set. The times in Table 2 confum \nthat SUIF is a research com- piler, not intended for daily production use. SUIF reads and writes files \nto disk between compiler passes. Our TSP solver performs even larger amounts of file I/O, and the times \nreported above for it are almost half system time, a half that could be avoided in a production environment \nby revising the code. The user time for the TSP solver could probably also be substantially reduced by \ntailoring the code more directly for the application at hand. Thus the time for the TSP solver is unlikely \nto be the ultimate bottleneck. I I Compile Time (seconds) I - corn st 33.4 12.5 7.5 4.4 36.5 7.7 86.S \ndod re 1288.8 507.1 185.2 100.0 418.0 190.3 72.S eqn ip 89.9 42.4 31.0 16.6 141.9 34.1 210.C esp tl 520.8 \n241.1 164.1 98.9 634.9 162.7 98.2 su2 re 210.1 85.9 40.9 25.1 178.3 40.8 218.6 xli q7 163.4 83.9 58.4 \n36.8 314.1 58.3 29.4 Table 2: Compilation and profiling times for the worst data set for each benchmark \n Even with the large times reported in Table 2, however, tire given approach might be useful at high \nlevels of optimiza- tion or in applications with large numbers of users, assum- ing the code improvements \nit yields are significant. 3.3 Control penalties For our evaluation, we used a set of control penalties \nbased upon the Alpha 21164 microarchitecture. In this model, a misfetch costs one cycle, and a conditional \nbranch mispre- diction costs five cycles. Table 3 summarizes the control penalties for the Alpha 21164 \nmodel in our experiments. Note that the penalty values vary based on the kind of branch at the end of \nthe basic block. Although the equation in Section 2.2 does not show that the penalties depend on B, it \nis simple to generalize the equation. To produce DTSP edge costs, we need to weight the CFG edge frequencies \nfrom profiling with the different control penalties. As outlined theoretically in Section 2.2, this amounts \nto accounting for all of the penalties that occur due to control instructions at the ends of basic blocks. \nAll prior compile-time branch alignment techniques assume that con- ditional branches are statically \npredicted4 when evaluating different layouts. To simplify our presentation and calcula- tions of CBBZ \nand IBB; we make the same assumption, but nothing in our approach forces us to use it. So the values \nof CBB and IBB I are chosen as if the processor always predicts the most common CFG successor of a basic \nblock. For all possible cases, Table 3 lists the relevant pxx values. There are two cases to consider \nwhere a basic block has a single CFG successor. In the desirable case, the basic block falls through \nto its CFG successor with no penalty and no PrediCtiOn, SO Only PNN = 0 applies. In the undesirable case, \nthe CFG successor is not placed immediately after this block, so an unconditional branch must be inserted. \nAgain we can always predict correctly so only pn applies. 4. For each conditional branch, the processor \nalways predicts that this branch will he taken or will fall through. Control Formulaic Block-ending Control \nEvent Penalties Term for (cycles) Penalty no branch 0 PNN unconditional branch 2 PTT fall through to \n(common) o PNN following block branch to (common) 1 conditional PTT following block branch mispredict \nfollowing PNT 1 5block (any layout) PTN fall through to (common) 0 PNN following block branch through \nPTTY register branch to any other CFG 3 PNT v successor PTN Table 3: A summary of the control penalties \nin our 21144 machine model. Note that fixup unconditional branches count as separate basic blocks; our \nalgotithm accounts for their cost. Because we have inserted the unconditional branch, pm equals 2 to \naccount for the cost of the branch in addition to the one cycle penalty for the misfetch. Conditional \nand multiway branches also use the formula in Section 2.2, employing the pxx values from Table 3. Some \nlayouts may not place either CFG successor of a conditional branch block as its layout successor. In \nsuch cases we insert a new unconditional branch as the fall-through successor of the conditional branch; \nthis unconditional branch is treated as a new basic block and its costs are totaled appropriately. Note \nthat for the reduction to DTSP, it would not do to add a new city (basic block) to the tour (CFG), so \nwe attach the penalties for this fixup basic block to the DTSP edge that required it be created.   \n Results In this section, we compare the performance of our bench- mark applications under three different \nbranch alignment layouts: original, greedy, and DTSP. We compare both the number of control-penalty cycles \nresulting from each layout and the total running time of each benchmark program. Sec- tion 4.1 presents \nresults where we train and test on the same data set, while Section 4.2 cross-validates aligned programs \nunder different training and testing data sets. Cross-valida- tion gives insights into the degradation \nfrom ideal to practi- cal training examples. esp tl 250.6M 186.8M 6.11 7.4x10-3 re 217.8M 206.1M 110.82 \n15.0x10- su2 sh 15.5M 14.8M 11.70 13.3x10-3 ne 0.2M O.lM 0.04 2.7~10-~ xli cl7 57.6M 22.7M 1.99 0.7x10-3 \n Table 4: Original control penalties, theoretical lower bounds on control penalties, and running timesfor \neach of our benchmarks and data sets. Control penalty cycles were estimated using the methodol- ogy described \nin the previous section. Running times were collected using the real-time clock on our AlphaStation 500/ \n266; this clock has a resolution of one millisecond5. We ran each benchmark 5 times to warm the buffer \ncache, and then took the arithmetic mean of the next 10 running times. We present both kinds of measurements \nnormalized against the statistics for the original layout. Raw statistics for the con- trol penalties \nand running times under the original layout are listed in Table 4. 4.1 Same training and testing data \nset The left graph in Figure 2 shows the compilercomputed control penalties under the Alpha 21164 model \nfor each of our benchmarks and data sets. The vertical axis has been normalized so that the control penalties \nfor the original branch alignment layout correspond to a value of 1.0. These penalties are from training \nand testing on the same data set, so these results are best-case for the DTSP algorithm and likely to \nbe best-case for the greedy algorithm. No layout can achieve lower control penalty than the lower bounds. \nFrom Figure 2, it appears that the bulk of the possible branch alignment benefit is conferred by running \nthe greedy algorithm alone. The greedy heuristic removes a mean of 5. To collect times, we run a separate \nprogram that reads the real-time clock before and after the program being timed. This means that we count \nsome bookending time in all of our mea- surements. However, tests in single-user mode show that this \naddi- tional measurement time is always less than 30 milliseconds and very stable. Because all programs \nincur this measurement over-head, we have ignored this effect and not compensated for it in our results. \n Control Penalties Execution Times Brrhnu(rmdMlls* Bendwwkndmtasd Figure 2. Results from training and \ntesting on the same data set. All values are normalized against the originalpro- gram, where no layout \noptimizations have been applied. In both graphs, the greedy method appears in black and the TSP method \nappears in white. In the control penalties graph, the TSP lower bound appears in gray. Note that the \nver- tical scale of the execution times graph does not start at the origin. 33% of the control penalty \nfrom running a particular bench- mark, while the TSP-based aligner removes a mean of 36%. The lower bound \nshows that the best we can do is to remove 36% of control penalties. Our TSP-based aligner is within \n0.3% of the lower bounds, so it is possible, if costly, to find nearly-optimal tours. The right side \nof Figure 2 compares the running times of the laid-out programs running on the Alpha 21164 machine described \nabove. On average, running times improved by 1.19% under greedy layout and 2.01% under TSP-based layout. \nThe running times generally follow the trends antici- pated by the compiler estimates, except for the \nsu2cor runs, where the greedy layout performs better than the TSP-based layout. For su2.r-17, the TSP-based \nalgorithm gives margin- ally worse performance than the original program layout. SuZcor has a very low \nratio of control penalties to execution time compared to the other benchmarks. In SuZcor, poten-tial \nfor benefit from branch alignment is relatively small, and our rearrangement may have changed other factors \nthat contribute to execution time. It is interesting to note that our two floating-point bench- marks, \ndoduc and sdcor, show very different results from branch alignment. Aligning doduc with any algorithm \nremoves 2/3 of control penalties, while aligning sdcor has virtually no effect. From the calculated control \npenalties, there appears to be very little difference between the greedy and TSP-based methods. Both \nmethods achieve control penalties very close to the lower bound. However, this does not translate directly \nto differences in performance, where the TSP-based method leads to a much larger improvement in execution \ntime than the greedy method. To explore this unexpected behavior, we used IPROBE, a tool that analyzes \nprogram behavior using the Alpha perfor- mance counters. We found a correlation between branch alignment \nand cache effects: good branch alignments also appear to be good for caching. Basic block placement tech- \nniques confer benefits in cache behavior that are not mod- eled by control penalties (for example, fewer \nbranches taken may cause fewer cache misses). These unmodeled caching benefits are responsible for the \nlarger-than-expected differ-ences in execution times. This suggests that we should update the weights \nto reflect caching costs.  4.2 Cross-validation Estimating control penalties becomes more complex with \ndifferent training and testing data sets. In such calculations, one must set up the layout from the training \ndata set, then evaluate the control penalties in that layout using the edge frequencies from the testing \ndata set. This may mean that the predicted direction of a conditional branch reverses, or that the most \ncommon target of a multiway branch changes. In Figure 3, we show the results of cross-validating different \ntraining and testing data sets under the greedy and TSP- based layout techniques. As expected, other \ndata sets are generally not as good as training and testing with the same data set. We experimented with \na number of data sets; in general the ones that ran for a very short time or touched few static branch \nsites gave the worst cross-validation per-formance. For example, xli.ne runs for a very short time; it \nturns out to be a poor training set for the longer-running xfi.q7, while the reverse is not true. This \ncorresponds to our experiences in branch prediction [31], where it is very important to find good training \ninputs. Under cross-validation, greedy layout removes a mean of 3 1% of the control penalties computed \nby the compiler, and leads to a 1.06% mean reduction in running time. Compared to the results in Section \n4.1, this is basically no change in computed control penalty, and not quite as good an improvement in \nrunning time. With the TSP-based layout, cross-validated results show a 34% reduction in computed control \npenalties and a 1.66% reduction in running time. These are both slightly worse than our earlier results, \nwith some dilution of execution time benefits. . tsp cross greedy self H greedy cross 0 tsp self . $ \nL E 1.00 0.80 5 0.60 3p 0.40 g 0.20 2 0.00 com.in comst dod.re dodsm eqn.fx eqn.ip ec3p.C esp.H Benchmark \nand Data Set su2.re su2.sh xkne xli.q7 com.in com.st dod.19 dod.sm eqn.fx eqn.ip eSp.ti 6Sp.U su2.m \nsu2.sh xti.ne xli.q7 Benchmark and Data Set Figure 3. Results from training and testing on different \ndata sets. The upper graph shows control penalties; the lower graph shows execution times. All values \nare normalized against the original program, where no layout optimizations have been applied. The black \nand white bars are repeatedfrom Figure 2. The cross-validated values for the greedy layout algorithm \nappear with diagonal stripes. The cross-validated values for the TSP-based layout algorithm appear in \ngray. Note that the vertical scales of the execution times graphs do not start at the origin. Cross-validation \nshows mild dilution of control penalty and timing results compared to training with ideal inputs. This \ndilution does not change the relative effectiveness of the methods and preserves the bulk of the benefits \ndue to branch alignment. As in profiled static branch prediction, it is important to choose good training \ndata sets. Cross-valida- tion reduced some of the gap between the execution times of the greedy and TSP-based \nlayouts, but the small gap in con- trol penalties still suggests that other benefits from branch alignment \nare contributing to performance.  Related Work Branch alignment is a special case of code placement \ntech- niques. These techniques reorder the pieces of a program to reduce both control penalties and cache \nmisses. McFarling [19], Hwu and Chang [9], Pettis and Hansen [23] did some of the earliest work on greedy \ncode placement using profile data. McFarling focuses on cache effects and uses the pro- file information \nto determine what fetches to exclude from a direct-mapped instruction cache. Hwu and Chang examine the \ncombined effect of code-expanding optimizations and basic block placement, concluding that they could \nachieve instruction cache miss rates close to those of fully-associa- tive caches using direct-mapped \ncaches and compile trans- formations. Pettis and Hansen use profiles of program runs to reorder code \nat two levels: procedure ordering and basic block ordering. Their greedy heuristic places ptocedures \nor basic blocks near each other based on edge weights in the call and control flow graphs. Pettis and \nHansen s primary focus was to improve the instruction cache miss rate, but their basic block ordering \ntechnique also works quite well to reduce control penalties. Since their work appears to be the basis \nfor many existing commercial code-placement tools, we have used their technique as a basis for our greedy \nimplementation. More recently, Torellas et al. [28] have investigated code placement to improve operating \nsystem performance. Their paper describes an algorithm that parti- tions the second-level cache into \nseparate sections for criti- cal code, sometimes-used code, and seldom-or-never-used code. Calder and \nGrunwald [2], on the other hand, directly address the branch alignment problem. They improved upon the \ngreedy heuristic described by Pettis and Hansen in two ways. First, Calder and Grunwald expose the details \nof the underlying microarchitecture to better estimate the cost of control penalties. We also consider \nthe specifics of the microarchitecture when calculating the costs on the edges in our DTSP. Second, they \npropose an alternative greedy heu- ristic that exhaustively searches all orders of the basic blocks touched \nby the 15 most frequently-executed edges in the CFG. They claim that this heuristic produces slightly \nbetter layouts and runs in a few minutes. A number of commercial tools exist for code placement. Speer, \nKumar, and Partridge [27] describe the benefits of code placement on the UNIX Kernel in HP-UK 9.0. IBM \ns FDPR/2 [3] performs interprocedural basic block placement on AIX executables. NTOM and OM are examples \nof pro- grams that perform code placement on Digital Alpha machines [29]. Finally, we mention just a \nfew other static techniques that reduce control penalties. Static branch prediction hints [ 181 allow \nthe compiler to direct hardware fetching. Static corre- lated branch prediction [15, 311 and conditional \nbranch removal [22] take advantage of correlated or redundant information along paths in the program \nto make branches more predictable or remove them entirely. Global instruc- tion schedulers like trace \nschedulers [17] or superblock schedulers [8] do not directly address branch penalties, but they indirectly \nlower branch penalties by trying to identify and linearize commonly-executed portions of the program. \n Conclusions and Future Work We exhibited a reduction from the branch alignment prob- lem to the Directed \nTraveling Salesman Problem. Using this reduction, we applied DTSP analysis techniques to place a lower \nbound on the control penalties experienced by aligned programs. We also used a DTSP solver to produce \nlayouts that approach the lower bound and meet it in many cases. We observed that the greedy method also \nproduces layouts that approach the lower bound in expected control penalties, but that the running times \nof the programs under TSP-based layout were better than the running times of the same pro- grams and \ndata sets under greedy layout. We cross-validated our results using different training and testing data \nsets for each benchmark. As in static branch prediction, it turns out to be important to choose good \ntrain- ing data sets. As expected, training and testing with differ- ent data sets gives worse results \nthan training and testing with the same data set. However, cross-validation did not change the relative \nbenefits of the greedy and TSP-based methods, and most of the benefits due to performing branch alignment \nremained. The methodology we use, reduction to a well-understood theoretical problem, is more important \nthan the specifics of our reduction or the performance gains that we report. Find- ing more reductions \nthat better model the underlying hard- ware can be an endless task. For example, we could perform a trace-driven \nsimulation of the branch prediction hardware in the target machine to derive more accurate frequencies \nof correct and incorrect predictions6. But having chosen a par- ticular reduction as sufficiently accurate, \nthe lower bounds can give us insight into whether further refinement is worth- while. Further, efficient \nsolvers make nearly optimal solu- tions possible, if not cheap. We would have preferred to run our algorithm \non larger, longer-running benchmarks, including those in SPEC95. We plan to do so as soon as math-suif \nmatures enough to correctly process the SPEC95 suite. We am intrigued by the mismatch in differences \nbetween expected control penalties and actual execution times. Other branch alignment benefits, such \nas improved cache locality, contribute to the larger-than-expected differences in execu- tion times. \nWe would like to analyze and model these cach- ing effects in the future. We also would like to investigate \napplying our method to other machine models, and we would like to try to generalize our method to the \ninterproce- dural code placement problem. 7 Acknowledgments This research was sponsored in part by grants \nfrom AMD, Digital Equipment, Hewlett-Packard, and Intel. Cliff Young is funded by an IBM Cooperative \nFellowship. David R. Karger is supported by a National Science Foundation Career Award, grant number \nCCR-9624239. Michael D. Smith is supported by a National Science Foundation Young Investigator award, \ngrant number CCR-9457779. We com- puted HK bounds using code adapted by David Applegate and Bill Cook \nfrom an article they wrote with Bixby and Chvatal [1], and we thank them for making this code avail- \nable to us. 8 References [l] D. Applegate, R. Bixby, V. Chvatal, and W. Cook, Finding cuts in the TSP \n(A preliminary report), Report No. 95-05, DIMACS, Rutgers University, Piscataway, NJ. [2] B. Calder and \nD. Grunwald. Reducing Branch Costs via Branch Alignment, Ptw. Sixth Id. Conj on Architec- tural Support \nfor Programming Languages and Operating Systems, pp. 242-25 1, October 1994. 6. Note that such a simulation \nwould not he completely accu- rate, due to aliasing effects [32] that would change under the new layout. \nHowever, aliasing is a second-order effect, so there would still be value to such a trace-based simulation. \n[3] J.-H. Chow, et al. FDPR/2: A Code Instrumentation and Restructuring Tool for OS/2 Executables, CASCON \n95, Toronto, Canada, October 1995. [4] Digital Semiconductor. Alpha 21164 Microprocessor Hardware Reference \nManual, Digital Equipment Corpora- tion, Maynard, Massachusetts, April 1995. Order No. EC- QAEQB-TE. \n[5] J. Fisher and S. Freudenberger. Predicting Condi- tional Branch Directions From Previous Runs of \na Pro-gram, Proc. Fijth Intl. Con$ on Architectural Support for Programming Languages and Operating Systems, \npp. 85- 95, October 1992. [6] M. Held and R. M. Karp, The traveling-salesman problem and minimum spanning \ntrees, Operations Research 18 (1970), 1138-I 162. [7] M. Held and R. M. Karp, The traveling-salesman \nproblem and minimum spanning trees: Part JL Math. Pro- gramming 1(1971), 6-25. [8] W. Hwu, et al. The \nSuperblock: An Effective Tech- nique for VLIW and Superscalar Compilation, The Journal of Supercomputing, \nKluwer Academic Publishers, 1993. [9] W. Hwu and P Chang. Achieving High Instruction Cache Performance \nwith an Optimizing Compiler, Ptoc. 16th Annual Intl. Symp. on Computer Architecture, pp. 242- 251, May \n1989. [lo] D. S. Johnson and L. A. McGeoch, The Traveling Salesman Problem: A Case Study in Local Optimization, \nin Local Search in Combinatorial Optimization, (E. Aarts and J. K. Lenstra, eds.), John Wiley &#38; Sons \n(to appear 1997). Preliminary draft available at http:// www.research.att.com/-dsj/papers/TSPchapter.ps. \n[ 1 l] D. S. Johnson and L. A. McGeoch, in preparation. [I21 D. S. Johnson, L. A. McGeoch, and E. E. \nRothberg, Asymptotic Experimental Analysis for the Held-Karp Traveling Salesman Bound, Proc. 7th Ann. \nACM-SIAM Symp. on Discrete Algorithms, SIAM, Philadelphia, 1996, 341-350. Preliminary draft available \nat ftp://dimacs.rut-gers.edu/pub/dsjltemp/soda.ps. [13] P. C. Kanellakis and C. H. Papadimitriou, Local \nsearch for the asymmetric traveling salesman problem, Operations Research 28 (1980), 1086-1099. [I41 \nR. M. Karp, A patching algorithm for the non-sym- metric traveling salesman problem, SIAM J. Comput. \n9 (1979) 561-573. [15] A. Krall, Improving Semi-static Branch Prediction by Code Replication, Proc. ACM \nSIGPLAN 94 Conf on Prog. Lang. Design and Implementation, Jun. 1994. [16] J. Lee and A. Smith, Branch \nPrediction Strategies and Branch Target Buffer Design, Computer 17 (1984), 6- - ) LL. [ 171 P. Lowney, \nS. Freudenberger, T. Karzes, W. Lichten- stein, R. Nix, J. O Donnell, and J. Ruttenberg. The Multi- flow \nTrace Scheduling Compiler, The Journal of Supercomputing, Kluwer Academic Publishers, 1993. [ 181 S. \nMcFarling and J. Hennessy, Reducing the Cost of Branches, Proc. of 13th Annual Intl. Symp. on Computer \nArchitecture, June 1986. [19] S. McFarling. Program Optimization for Instruction Caches, Ptoc. Third \nlntl. Conf: on Architectural Support for Programming Languages and Operating Systems, pp. 183-191, April \n1989. [20] 0. Martin, S. W. Otto, and E. W. Felten, Large-step Markov chains for the traveling salesman \nproblem, Com-plex Systems 5 (1991), 299-326. [21] D. L. Miller and J. F. Pekny, Exact solution of large \nasymmetric traveling salesman problems, Science 251 15 (February 1991), 754-761. [22] F. Mueller and \nD. Whalley, Avoiding Conditional Branches by Code Replication, Proc. ACM SlGPLU7 95 Con&#38; on Programming \nLanguage Design and Implementa- tion, pp. 56-66, June 1995. [23] K. Pettis and R. Hansen. Profile Guided \nCode Posi- tioning, Pmt. ACM SIGPLAN 90 Conf: on Programming Language Design and Implementation, pp. \n16-27, June 1990. [24] N. B. Repetto, Upper and Lower Bounding Proce-dures for the Asymmetric Tmveling \nSalesman Ptvblem, Ph.D. Thesis, GSIA, Carnegie-Mellon University, Pitts-burgh, 1994. [25] J. Smith, A \nStudy of Branch Prediction Strategies, Proc. 8th Annual Intf. Symp. on Computer Architecture, June 1981. \n[26] M. Smith. Extending SUIF for Machine-dependent Optimizations, Ptoc. First SUIF Compiler Workshop, \nStan- ford, CA, pp. 14-25, January 1996. [27] S. Speer, R. Kumar, and C. Partridge. Improving UNIX Kernel \nPerformance Using Profile Based Optimiza- tion, 1994 Winter USENIX Cot&#38; pp. 18 1-188, January 1994. \n[28] J. Torrellas, C. Xia, and R. Daigle. Optimizing Instruction Cache Performance for Operating System \nInten- sive Workloads, Pmt. First Intl. Symp. on High-Petjk-mance ComputerArchitecture, pp. 360-369, \nJanuary 1995. [29] L. Wilson. C. Neth, and M. Rickabaugh. Delivering Binary Object Modification Tools \nfor Program Analysis and Optimization, Digital Technical Journal, 8 (1996). 18-3 1. [30] R. Wilson, R. \nFrench, C. Wilson, S. Amarasinghe, J. Anderson, S. Tjiang, S. Liao, C. Tseng, M. Hall, M. Lam, and J. \nHennessy. SUB? An Infrastructure for Research on Parallelizing and Optimizing Compilers, ACM SZGPLAN \nNotices, 29 (1994). 3 l-37. [31] C. Young and M. Smith, Improving the Accuracy of Static Branch Prediction \nUsing Branch Correlation, Proc. 6th Annual Intl. Co@ on Architectural Support for Prog. Lang. and Operating \nSystems, October 1994. [32] C. Young, N. Gloy, and M. Smith. A Comparative Analysis of Schemes for Correlated \nBranch Prediction, Proc. 22nd Annual Id Symp. on Computer Architecture, Jun. 1995. 1331 C. Young and \nM. Smith. Branch Instrumentation in SUIF, Proc. First SUIF Compiler Workshop, Stanford, CA, pp. 139-145, \nJanuary 1996. [34] W. Zhang and R. E. Korf, Performance of linear- space search algorithms, Artificial \nIntelligence 79 (1996), 241-292.  9 Appendix: Algorithms for the Directed Traveling Salesman Problem \nIn the Directed Traveling Salesman Problem (DSTP) as dis- cussed in this paper, one is given cities and \nfor each ordered pair of cities a distance. The goal is to find a permutation of the cities that minimizes \nthe total length of the path from city to city. This permutation can also be viewed as a walk through \nthe cities. In practice, this problem appears to be much harder than its symmetric variant (the STSP) \nin which one asks for a mini- mum length tour (Hamiltonian cycle) rather than path, even though each \nversion reduces to the other via NP-complete- ness transformations. For the symmetric case, branch-and- \ncut algorithms have successfully solved non-trivial instances with over 7,000 cities [l], and the iterated \nLin- Kemighan algorithm typically finds solutions within 1% of optimal in reasonable time for instances \nwith as many as 100,000 cities [lo]. For the directed case, the best of the published optimization codes \n[21] was unable to solve real-world instances with only a few dozen cities, although it could solve instances \nwith random distance matrices having as many as 500,000 cities. Aiding it in the latter was the fact \nthat for such ran- dom instances, the optimal tour length equaled the assign- ment problem (AP) lower \nbound, i.e, the minimum length collection of disjoint directed cycles that covers all the cit- ies. The \nmost widely used approximation algorithms for this problem are also designed to exploit small gaps between \nthe AP bound and the optimal tour length, as they are based on patching together cycle covers into tours \n[14, 341. Unfortu- nately, a majority of the instances arising in the branch alignment problem do not. \nhave this property. For instance, in esp. tl, although 71 of the 179 relevant procedures yield instances \nwith the AP bound equal to the optimal tour, the median gap for the remaining 108 is 30% and for 15 instances \nthe optimal is over 10 times as long as the AP bound. Thus more sophisticated techniques are needed. \nIt has recently been observed [l l] that one can get surprisingly good results for a wide variety of \nDTSP instances by using the standard NP-completeness transformation to convert an instance of the DTSP \nto an equivalent one for the STSP, and then applying an appropriately chosen algorithm from that domain. \nWe use an efficient implementation of the iterated 3-Opt algorithm for the STSP [lo]. This algorithm, \nbased on an idea originally proposed by Martin, Otto, and Felten [20], returns the best tour found over \na succession of itera- tions , where each iteration consists of running the 3-Gpt local search algorithm \nto exhaustion and then making a ran- domly-chosen 4-Opt move [20]. Our DTSP to STSP trans- formation \nreplaces each city by a pair of cities, with the edge between them locked into the tour (such locks being \na feature of our code for iterated 3-Gpt). Experiments with this approach to the DTSP suggest that it \nis competitive with AP-based approaches when the AP bound is close to opti- mal, and outperforms them \notherwise [ 111. It also appears to outperform the algorithm of Kanellakis and Papadimitriou [13], a \nvariable-depth local search algorithm that works directly with the DTSP, this conclusion based on compari- \nsons to results reported for Repetto s implementation of that algorithm [ 241. Iterated 3-Opt is a randomized \nalgorithm, and can be further randomized by the choice of the starting tour. Thus it pays to run it more \nthan once, and then take the best tour found. For the purpose of this study, we ran it 10 times on each \ninstance, 5 times using randomized Greedy starts, 4 times using randomized Nearest Neighbor starts, and \nonce using the original ordering given by the compiler. Each run consists of 2N iterations, where N is \nthe number of cities in the original DTSP. We then output the best tour found. In practice, this number \nof runs should not always be neces- sary, and indeed on 128 of the 179 procedures in esp.tl it was found \non all 10 runs. To evaluate the quality of the tours, we compute the Held- Karp lower bound on optimal \ntour length [6, 71, a much more sophisticated bound than the AP bound mentioned above. This bound equals \nthe solution to the linear program- ming relaxation of the standard integer programming for-mulation \nof the STSP, and has been empirically shown to lie quite close to the true optimal for a wide range of \nsymmetric instance classes [12]. The quality of the bound appears to carry over to the cases under consideration \nhere, as shown from the results presented in the main body of this paper. For each of the programs covered \nin this paper, the sum of the HK lower bounds was never more than 0.9% below the total lengths of the \ntours found, and the average was less than 0.3%. The worst gap between tour and bound for any individual \nprocedure instance was 14%, which occurred twice. In one case the best tour found was optimal, in the \nother the optimal tour lay somewhere between the HK bound and the best found.  \n\t\t\t", "proc_id": "258915", "abstract": "Branch alignment reorders the basic blocks of a program to minimize pipeline penalties due to control-transfer instructions. Prior work in branch alignment has produced useful heuristic methods. We present a branch alignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks averages within 0.3% of a provable optimum. We compare the control penalties and running times of our algorithm to an older, greedy approach and observe that both the greedy method and our method are close to the lower bound on control penalties, suggesting that greedy is good enough. Surprisingly, in actual execution our method produces programs that run noticeably faster than the greedy method. We also report results from training and testing on different data sets, validating that our results can be achieved in real-world usage. Training and testing on different data sets slightly reduced the benefits from both branch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits remain.", "authors": [{"name": "Cliff Young", "author_profile_id": "81536654856", "affiliation": "Harvard University", "person_id": "PP33034629", "email_address": "", "orcid_id": ""}, {"name": "David S. Johnson", "author_profile_id": "81342499043", "affiliation": "AT&T Labs", "person_id": "PP77026858", "email_address": "", "orcid_id": ""}, {"name": "Michael D. Smith", "author_profile_id": "81100077421", "affiliation": "Harvard University", "person_id": "PP39026087", "email_address": "", "orcid_id": ""}, {"name": "David R. Karger", "author_profile_id": "81100337812", "affiliation": "Massachusetts Institute of Technology", "person_id": "PP15029551", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258915.258932", "year": "1997", "article_id": "258932", "conference": "PLDI", "title": "Near-optimal intraprocedural branch alignment", "url": "http://dl.acm.org/citation.cfm?id=258932"}