{"article_publication_date": "09-09-2012", "fulltext": "\n A Generic Abstract Syntax Model for Embedded Languages Emil Axelsson Chalmers University of Technology \n emax@chalmers.se Abstract Representing a syntax tree using a data type often involves hav\u00ading many \nsimilar-looking constructors. Functions operating on such types often end up having many similar-looking \ncases. Different languages often make use of similar-looking constructions. We pro\u00adpose a generic model \nof abstract syntax trees capable of represent\u00ading a wide range of typed languages. Syntactic constructs \ncan be composed in a modular fashion enabling reuse of abstract syntax and syntactic processing within \nand across languages. Building on previous methods of encoding extensible data types in Haskell, our \nmodel is a pragmatic solution to Wadler s expression problem . Its practicality has been con.rmed by \nits use in the implementation of the embedded language Feldspar. Categories and Subject Descriptors D.2.11 \n[Software Archi\u00adtectures]: Languages; D.2.13 [Reusable Software]: Reusable li\u00adbraries; D.3.2 [Language \nClassi.cations]: Extensible languages; D.3.3 [Language Constructs and Features]: Data types and struc\u00adtures \nKeywords the expression problem, generic programming, embed\u00added domain-speci.c languages 1. Introduction \nIn 1998, Philip Wadler coined the expression problem :1 The Expression Problem is a new name for an old \nprob\u00adlem. The goal is to de.ne a datatype by cases, where one can add new cases to the datatype and new \nfunctions over the datatype, without recompiling existing code, and while retaining static type safety \n(e.g., no casts). This is not just a toy problem. It is an important matter of making software more \nmaintainable and reusable. Being able to extend ex\u00adisting code without recompilation means that different \nfeatures can be developed and veri.ed independently of each other. Moreover, it gives the opportunity \nto extract common functionality into a li\u00adbrary for others to bene.t from. Having a single source for \ncommon functionality not only reduces implementation effort, but also leads to more trustworthy software, \nsince the library can be veri.ed once and used many times. 1 http://www.daimi.au.dk/~madst/tool/papers/expression.txt \n Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Our motivation \nfor looking at the expression problem is highly practical. Our research group has developed several embedded \ndomain-speci.c languages (EDSLs), for example, Lava [5], Feldspar [3] and Obsidian [8]. There are several \nconstructs and operations that occur repeatedly, both between the languages and within each language. \nWe are interested in factoring out this com\u00admon functionality in order to simplify the implementations \nand to make the generic parts available to others. A modular design also makes it easier to try out new \nfeatures, which is important given the experimental state of the languages. In addition to the requirements \nstated in the expression problem, a desired property of an extensible data type model is support for \ngeneric traversals. This means that interpretation functions should only have to mention the interesting \ncases. For example, an analysis that counts the number of additions in an expression should only have \nto specify two cases: (1) the case for addition, and (2) a generic case for all other constructs. Our \nvision is a library of generic building blocks for EDSLs that can easily be assembled and customized \nfor different domains. Modular extensibility (as stated in the expression problem) is one aspect of this \nvision. Support for generic programming is another important aspect, as it can reduce the amount of boilerplate \ncode needed to customize interpretation functions for speci.c constructs. This paper proposes a simple \nmodel of typed abstract syntax trees that is extensible and supports generic traversals. The model is \npartly derived from Swierstra s Data Types ` a la Carte (DTC) [18] which is an encoding of extensible \ndata types in Haskell. DTC is based on .xed-points of extensible functors. Our work employs the extensibility \nmechanism from DTC, but uses an application tree (section 2.2) instead of a type-level .xed-point. Given \nthat DTC (including recent development [4]) already provides extensible data types and generic traversals, \nour paper makes the following addi\u00adtional contributions (see also the comparison in section 10): We \ncon.rm the versatility of the original DTC invention by using it in an alternative setting (section 3). \n Our model provides direct access to the recursive structure of the data types, leading to simpler generic \ntraversals that do not rely on external generic programming mechanisms (section 4).  We explore the \nuse of explicit recursion in addition to prede\u00ad.ned recursion schemes (sections 5, 6 and 7), demonstrating \nthat generic traversals over extensible data types are not re\u00adstricted to prede.ned recursive patterns. \n  Our model is available in the SYNTACTIC library2 together with a lot of utilities for EDSL implementation \n(section 9). It has been successfully used in the implementation of Feldspar [3] (sec\u00ad tion 9.1), an \nEDSL aimed at programming numerical algorithms in time-critical domains. ICFP 12, September 9 15, 2012, \nCopenhagen, Denmark. Copyright &#38;#169; 2012 ACM 978-1-4503-1054-3/12/09 $15.00. 2 http://hackage.haskell.org/package/syntactic-1.0 \n  The code in this paper is available as a literate Haskell .le.3 It has been tested using GHC 7.4.1 \n(and the mtl package). A number of GHC-speci.c extensions are used; see the source code for details. \n2. Modeling abstract syntax It is common for embedded languages to implement an abstract syntax tree \nsuch as the following: data Expr1 a where Num1 :: Int . Expr1 Int Add1 :: Expr1 Int . Expr1 Int . Expr1 \nInt Mul1 :: Expr1 Int . Expr1 Int . Expr1 Int  Expr1 is a type of numerical expressions with integer \nliterals, addi\u00adtion and multiplication. The parameter a is the type of the semantic value of the expression; \ni.e. the value obtained by evaluating the expression. (For Expr1 , the semantic value type happens to \nalways be Int, but we will soon consider expressions with other semantic types.) Evaluation is de.ned \nas a simple recursive function: evalExpr1 :: Expr1 a . a evalExpr1 (Num1 n) = n evalExpr1 (Add1 a b) \n= evalExpr1 a + evalExpr1 b evalExpr1 (Mul1 a b) = evalExpr1 a evalExpr1 b* The problem with types such \nas Expr1 is that they are not extensi\u00adble. It is perfectly possible to add new interpretation functions \nin the same way as evalExpr1 , but unfortunately, adding new construc\u00adtors is not that easy. If we want \nto add a new constructor, say for subtraction, not only do we need to edit and recompile the de.ni\u00adtion \nof Expr1 , but also all existing interpretation functions. Another problem with Expr1 is the way that \nthe recursive structure of the tree has been mixed up with the symbols in it: It is not possible to traverse \nthe tree without pattern matching on the constructors, and this prevents the de.nition of generic traversals \nwhere only the in\u00adteresting constructors have to be dealt with. We are going to deal with the problem \nof generic traversal .rst, and will then see that the result also opens up for a solution to the extensibility \nproblem. 2.1 Exposing the tree structure One way to separate the tree structure from the symbols is \nto make symbol application explicit: data Expr2 a where Num2 :: Int . Expr2 Int Add2 :: Expr2 (Int . \nInt . Int) Mul2 :: Expr2 (Int . Int . Int) App2 :: Expr2 (a . b) . Expr2 a . Expr2 b  Here, Add2 and \nMul2 are function-valued symbols (i.e. symbols whose semantic value is a function), and the only thing \nwe can do with those symbols is to apply them to arguments using App2 . As an example, here is the tree \nfor the expression 3+4: ex1 = App2 (App2 Add2 (Num2 3)) (Num2 4) What we have gained with this rewriting \nis the ability to traverse the tree without necessarily mentioning any symbols. For example, this function \ncomputes the size of an expression: sizeExpr2 :: Expr2 a . Int sizeExpr2 (App2 s a) = sizeExpr2 s + sizeExpr2 \na sizeExpr2 = 1 _ 3 http://www.cse.chalmers.se/~emax/documents/axelsson2012generic.lhs *Main> sizeExpr2 \nex1 3  However, even though we have achieved a certain kind of generic programming, it is limited to \na single type, which makes it quite uninteresting. Luckily, the idea can be generalized. 2.2 The AST \nmodel If we lift out the three symbols from Expr2 and replace them with a single symbol constructor, \nwe reach the following syntax tree model: data AST dom sig where Sym :: dom sig . AST dom sig (:$) :: \nAST dom (a :. sig) . AST dom (Full a) . AST dom sig infixl 1 :$  The AST type is parameterized on the \nsymbol domain dom, and the Sym constructor introduces a symbol from this domain. The type (:.) is isomorphic \nto the function arrow, and Full a is isomorphic to a: newtype Full a = Full {result :: a} newtype a:. \nb = Partial (a . b) infixr :.  As will be seen later, these types are needed to be able to distinguish \nfunction-valued expressions from partially applied syntax trees. The AST type is best understood by looking \nat a concrete example. NUM is the symbol domain corresponding to the Expr1 type: data NUM a where Num \n:: Int . NUM (Full Int) Add :: NUM (Int :. Int :. Full Int) Mul :: NUM (Int :. Int :. Full Int) type \nExpr3 a = AST NUM (Full a) Expr3 is isomorphic to Expr1 (modulo strictness properties). This correspondence \ncan be seen by de.ning smart constructors corre\u00adsponding to the constructors of the Expr1 type: num :: \nInt . Expr3 Int add :: Expr3 Int . Expr3 Int . Expr3 Int mul :: Expr3 Int . Expr3 Int . Expr3 Int num \n= Sym . Num add a b= Sym Add :$ a:$ b mul a b= Sym Mul :$ a:$ b  Symbol types, such as NUM are indexed \nby symbol signatures built up using Full and (:.). The signatures of Num and Add are: Full Int Int :. \nInt :. Full Int  The signature determines how a symbol can be used in an AST by specifying the semantic \nvalue types of its arguments and result. The .rst signature above speci.es a terminal symbol that can \nbe used to make an Int-valued AST, while the second signature speci.es a non-terminal symbol that can \nbe used to make an Int-valued AST node with two Int-valued sub-terms. The Num constructor also has an \nargument of type Int. However, this (being an ordinary Haskell integer) is to be regarded as a parameter \nto the symbol rather than a syntactic sub-term. A step-by-step construction of the expression a + b \nillustrates how the type gradually changes as arguments are added to the symbol: a, b :: AST NUM (Full \nInt) Add :: NUM (Int :. Int :. Full Int) Sym Add :: AST NUM (Int :. Int :.Full Int) Sym Add :$ a :: \nAST NUM (Int :. Full Int) Sym Add :$ a :$ b :: AST NUM (Full Int) We recognize a fully applied symbol \nby a type of the form AST dom (Full a). Because we are often only interested in com\u00adplete trees, we de.ne \nthe following shorthand: type ASTF dom a = AST dom (Full a) In general, a symbol has a type of the form \nT(a :. b:. ... :. Full x) Such a symbol can be thought of as a model of a constructor of a recursive \nreference type Tref of the form T ref a . T ref b . ... . T ref x Why is Full only used at the result \ntype of a signature and not the arguments? After all, we expect all sub-terms to be complete syntax trees. \nThe answer can be seen in the type of (:$): (:$) :: AST dom (a :. sig) . AST dom (Full a) . AST dom sig \n The a type in the .rst argument is mapped to (Full a) in the second argument (the sub-term). This ensures \nthat the sub-term is always a complete AST, regardless of the signature. The reason for using (:.) and \nFull (in contrast to how it was done in Expr2 ) is that we want to distinguish non-terminal symbols from \nfunction-valued terminal symbols. This is needed in order to model the following language: data Lang \na where Op1 :: Lang Int . Lang Int . Lang Int Op2 :: Lang (Int . Int . Int) Here, Op1 is a non-terminal \nthat needs two sub-trees in order to make a complete syntax tree. Op2 is a function-valued terminal. \nThis distinction can be captured precisely when using AST: data LangDom a where Op1 :: LangDom (Int \n:. Int :. Full Int) Op2 :: LangDom (Full (Int . Int . Int)) type Lang a = AST LangDom (Full a) Without \n(:.) and Full, the distinction would be lost.  2.3 Simple interpretation Just as we have used Sym and \n(:$) to construct expressions, we can use them for pattern matching: eval NUM :: Expr3 a . a eval NUM \n(Sym (Num n)) = n eval NUM (Sym Add :$ a :$ b) = eval NUM a + eval NUM b eval NUM (Sym Mul :$ a :$ b) \n= eval NUM a eval NUM b * Note the similarity to evalExpr1 . Here is a small example to show that it \nworks: *Main> eval NUM (num 5 mul num 6) 30  For later reference, we also de.ne a rendering interpretation: \nrender NUM :: Expr3 a . String render NUM (Sym (Num n)) = show n render NUM (Sym Add :$ a:$ b) = \"(\" \n++ render NUM a++ \" +\" ++ render NUM b ++ \")\" render NUM (Sym Mul :$ a:$ b) = \"(\" ++ render NUM a ++ \n\" \" ++ render NUM b ++ \")\" * A quick intermediate summary is in order. We have shown a method of encoding \nrecursive data types using the general AST type. The encoding has a one-to-one correspondence to the \norigi\u00adnal type, and because of this correspondence, we intend to de.ne languages only using AST, without \nthe existence of an encoded ref\u00aderence type. However, for any type (ASTF dom), a corresponding reference \ntype can always be constructed. So far, it does not look like we have gained much from this exercise, \nbut remember that the goal is to enable extensible languages and generic traversals. This will be done \nin the two following sections. 3. Extensible languages In the quest for enabling the de.nition of extensible \nlanguages, the AST type has put us in a better situation. Namely, the problem has been reduced from extending \nrecursive data types, such as Expr1 , to extending non-recursive types, such as NUM. Fortunately, this \nproblem has already been solved in Data Types `a la Carte (DTC). DTC de.nes the type composition operator \nin Listing 1, which can be seen as a higher-kinded version of the Either type. We demonstrate its use \nby de.ning two new symbol domains: data Logic a where --Logic expressions Not :: Logic (Bool :. Full \nBool) Eq :: Eq a . Logic (a :. a:. Full Bool) data If a where --Conditional expression If :: If (Bool \n:. a:. a:. Full a)  These can now be combined with NUM to form a larger domain: type Expr a = ASTF (NUM \n:+: Logic :+: If) a A corresponding reference type (which we do not need to de.ne) has all constructors \nmerged at the same level: data Expr ref a where Num :: Int . Expr ref Int Add :: Expr ref Int . Expr \nref Int . Expr ref Int ... Not :: Expr ref Bool . Expr ref Bool ... If :: Expr ref Bool . Expr ref a \n. Expr ref a . Expr ref a Unfortunately, the introduction of (:+:) means that constructing expressions \nbecomes more complicated:4 not :: Expr Bool . Expr Bool not a = Sym (InjR (InjL Not)) :$ a cond :: Expr \nBool . Expr a . Expr a . Expr a cond c tf = Sym (InjR (InjR If)) :$ c:$ t :$ f  data (dom1 :+: dom2) \na where InjL :: dom1 a . (dom1 :+: dom2) a InjR :: dom2 a . (dom1 :+: dom2) a infixr :+: Listing 1: \nComposition of symbol domains (part of DTC interface) class (sub :<: sup) where inj :: sub a . sup a \nprj :: sup a . Maybe (sub a) instance (expr :<: expr) where inj = id prj = Just instance (sym :<: (sym \n:+: dom)) where inj = InjL prj (InjL a) =Just a prj = Nothing _ instance (sym1 :<: dom) . (sym1 :<: (sym2 \n:+: dom)) where inj = InjR . inj prj (InjR a) =prj a prj = Nothing _ --Additional instance for AST instance \n(sub :<: sup) . (sub :<: AST sup) where inj = Sym . inj prj (Sym a) = prj a prj = Nothing _ Listing 2: \nSymbol subsumption (part of DTC interface) The symbols are now tagged with injection constructors, and \nthe amount of injections will only grow as the domain gets larger. For\u00adtunately, DTC has a solution to \nthis problem too. The (:<:) class, de.ned in Listing 2, provides the inj function which automates the \ninsertion of injections based on the types. The .nal instance also takes care of injecting the Sym constructor \nfrom the AST type. We can now de.ne not as follows: not :: (Logic :<: dom) . ASTF dom Bool . ASTF dom \nBool not a = inj Not :$ a  The prj function in Listing 2 is the partial inverse of inj. Just like inj \nallows one to avoid a nest of InjL /InjR constructors in construction, prj avoids a nest of injection \nconstructors in pattern matching (see section 3.2). The instances of (:<:) essentially per\u00adform a linear \nsearch at the type level to .nd the right injection. Overlapping instances are used to select the base \ncase. The remaining constructs of the Expr language are de.ned in List\u00ading 3. Note that the types have \nnow become more general. For ex\u00ad ample, the type (.) :: (NUM :<: dom) . ASTF dom Int . ASTF dom Int . \nASTF dom Int 4 Here we override the not function from the Prelude. The Prelude function will be used \nquali.ed in this paper. num :: (NUM :<: dom) . Int . ASTF dom Int (.) :: (NUM :<: dom) . ASTF dom Int \n. ASTF dom Int . ASTF dom Int (8) :: (NUM :<: dom) . ASTF dom Int . ASTF dom Int . ASTF dom Int (=) :: \n(Logic :<: dom, Eq a) . ASTF dom a . ASTF dom a . ASTF dom Bool condition :: (If :<: dom) . ASTF dom \nBool . ASTF dom a . ASTF dom a . ASTF dom a  num = inj . Num a . b =inj Add :$ a:$ b a 8 b =inj Mul \n:$ a:$ b a = b =inj Eq :$ a:$ b condition ctf=inj If :$ c:$ t:$ f infixl 6 . infixl 7 8 Listing 3: Extensible \nlanguage front end says that (.) works with any domain dom that contains NUM. Infor\u00admally, this means \nany domain of the form ... :+: NUM :+: ... Expressions only involving numeric operations will only have \na NUM constraint on the domain: ex2 :: (NUM :<: dom) . ASTF dom Int ex2 =(num 5 . num 0) 8 num 6  This \nmeans that such expressions can be evaluated by the earlier function evalNUM , which only knows about \nNUM: *Main> eval NUM ex2 30  Still, the type is general enough that we are free to use ex2 together \nwith non-numeric constructs: ex3 = ex2 = ex2 The class constraints compose as expected: *Main> :t ex3 \nex3 :: (Logic :<: dom, NUM :<: dom) . ASTF dom Bool That is, ex3 is a valid expression in any language \nthat includes Logic and NUM. 3.1 Functions over extensible languages The evaluation function evalNUM \nis closed and works only for the NUM domain. By making the domain type polymorphic, we can de.ne functions \nover open domains. The simplest example is size, which is completely parametric in the dom type: size \n:: AST dom a . Int size (Sym _) = 1 size (s :$ a) =size s+ size a  *Main> size (ex2 :: Expr3 Int) 5 \n*Main> size (ex3 :: Expr Bool) 11  But most functions we want to de.ne require some awareness of the \nsymbols involved. If we want to count the number of additions in an expression, say, we need to be able \nto tell whether a given symbol is an addition. This is where the prj function comes in: countAdds :: \n(NUM :<: dom) . AST dom a . Int countAdds (Sym s) | Just Add . prj s= 1 | otherwise = 0 countAdds (s \n:$ a) = countAdds s + countAdds a In the symbol case, the prj function attempts to project the symbol \nto the NUM type. If it succeeds (returning Just) and the symbol is Add, 1 is returned; otherwise 0 is \nreturned. Note that the type is as general as possible, with only a NUM constraint on the domain. Thus, \nit accepts terms from any language that includes NUM: *Main> countAdds (ex2 :: Expr3 Int) 1 *Main> countAdds \n(ex3 :: Expr Bool) 2 We have now ful.lled all requirements of the expression problem:  We have the \nability to extend data types with new cases, and to de.ne functions over such open types.  We can add \nnew interpretations (this was never a problem).  Extension does not require recompilation of existing \ncode. For example, the NUM, Logic and If types could have been de.ned in separate modules. The function \ncountAdds is completely in\u00addependent of Logic and If. Still, it can be used with expressions containing \nthose constructs (such as ex3 ).  We have not sacri.ced any type-safety.   3.2 Pattern matching The \nencoding we use does come with a certain overhead. This is particularly visible when doing nested pattern \nmatching. Here is a function that performs the optimization x +0 . x: optAddTop :: (NUM :<: dom) . ASTF \ndom a . ASTF dom a optAddTop (add :$ a :$ b) | Just Add . prj add , Just (Num 0) . prj b =a optAddTop \na = a (This function only rewrites the top-most node; in section 6.2, we will see how to apply the rewrite \nacross the whole expression.) Note the sequencing of the pattern guards. An alternative is to use the \nViewPatterns extension to GHC instead: optAddTop ((prj. Just Add) :$ a :$ (prj. Just (Num 0))) = a optAddTop \na = a While view patterns have the advantage that they can be nested, doing so tends to lead to long \nlines. For this reason, it is ofter preferable to use a sequence of pattern guards. 4. Generic traversals \nWe will now see how to de.ne various kinds of generic traversals over the AST type. In this section, \nwe will only deal with fold\u00adlike traversals (but they are de.ned using explicit recursion). In sections \n5 and 7, we will look at more general types of traversals. According to Hinze and L\u00a8 oh [9], support \nfor generic programming consists of two essential ingredients: (1) a way to write overloaded functions, \nand (2) a way to access the structure of values in a uniform way. Together, these two components allow \nfunctions to be de.ned over a (possibly open) set of types, for which only the interesting cases need \nto be given. All other cases will be covered by a single (or a few) default case(s). We have already \nencountered some generic functions in this paper. For example, size works for all possible AST types, \nand countAdds works for all types (AST dom) where the constraint (NUM :<: dom) is satis.ed.5 For size, \nall cases are covered by the default cases, while countAdds has one special case, and all other cases \nhave default behavior. An important aspect of a generic programming model is whether or not new interesting \ncases can be added in a modular way. The countAdds function has a single interesting case, and there \nis no way to add more of them. We will now see how to de.ne functions for which the interesting cases \ncan be extended for new types. We begin by looking at functions for which all cases are interesting. \n4.1 Generic interpretation The interpretation functions evalNUM and renderNUM are de.ned for a single, \nclosed domain. To make them extensible, we need to make the domain abstract, just like we did in countAdds. \nHowever, we do not want to use prj to match out the interesting cases, because now all cases are interesting. \nInstead, we factor out the evaluation of the symbols to a user-provided function. What is left is a single \ncase for Sym and one for (:$): evalG :: (. a .dom a . Denotation a) . (. a .AST dom a . Denotation a) \nevalG f(Sym s) =fs evalG f(s :$ a) = evalG f s $evalG fa type family Denotation sig type instance Denotation \n(Full a) = a type instance Denotation (a :. sig) = a . Denotation sig The Denotation type function strips \naway (:.) and Full from a signature. As an example, we let GHCi compute the denotation of (Int :. Full \nBool): *Main> :kind! Denotation (Int :. Full Bool) Denotation (Int :. Full Bool) :: * = Int . Bool \n Next, we de.ne the evaluation of NUM symbols as a separate func\u00adtion: evalSym NUM :: NUM a . Denotation \na evalSym NUM (Num n) = n evalSym NUM Add = (+) evalSym NUM Mul = (*)  5 One can argue that these functions \nare not technically generic, because they only work for instances of the AST type constructor. However, \nbecause we use AST as a way to encode hypothetical reference types, we take the liberty to call such \nfunctions generic anyway. Because this de.nition only has to deal with non-recursive sym\u00adbols, it is \nvery simple compared to evalNUM . We can now plug the generic and the type-speci.c functions together \nand use them to evaluate expressions: *Main> evalG evalSym NUM ex2 30  Our task is to de.ne an extensible \nevaluation that can easily be extended with new cases. We have now reduced this problem to making the \nevalSymNUM function extensible. The way to do this is to put it in a type class: class Eval expr where \neval :: expr a . Denotation a instance Eval NUM where eval (Num n) = n eval Add = (+) eval Mul = (*) \n instance Eval Logic where eval Not = Prelude.not eval Eq = (==) instance Eval If where eval If = .ctf \n. if c then t else f  Now that we have instances for all our symbol types, we also need to make sure \nthat we can evaluate combinations of these types using (:+:). The instance is straightforward: instance \n(Eval sub1, Eval sub2) . Eval (sub1 :+: sub2) where eval (InjL s) =eval s eval (InjR s) =eval s  We \ncan even make an instance for AST, which then replaces the evalG function: instance Eval dom . Eval (AST \ndom) where eval (Sym s) = eval s eval (s :$ a) =eval s$ eval a Now everything is in place, and we should \nbe able to evaluate expressions using a mixed domain: *Main> eval (ex3 :: Expr Bool) True   4.2 Finding \ncompositionality One nice thing about eval is that it is completely compositional over the application \nspine of the symbol. This means that even par\u00adtially applied symbols have an interpretation. For example, \nthe par\u00adtially applied symbol (inj Add :$ num 5) evaluates to the deno\u00adtation (5 +). We call such interpretations \nspine-compositional. When making a generic version of renderNUM we might try to use the following interface: \nclass Render expr where render :: expr a . String  However, the problem with this is that rendering \nis not spine\u00adcompositional: It is generally not possible to render a partially applied symbol as a monolithic \nstring. For example, a symbol representing an in.x operator will join its sub-expression strings differently \nfrom a pre.x operator symbol. A common way to get to a spine-compositional interpretation is to make \nthe renderings of the sub-expressions explicit in the interpretation. That is, we use ([String] . String) \nas interpretation: class Render expr where renderArgs :: expr a . ([String] . String) render :: Render \nexpr . expr a . String render a = renderArgs a []  Now, the joining of the sub-expressions can be chosen \nfor each case individually. The following instances use a mixture of pre.x (Not), in.x (Add, Mul, Eq) \nand mix.x rendering (If): instance Render NUM where renderArgs (Num n) [] = show n renderArgs Add [a,b] \n= \"(\" ++ a ++ \" + \" ++ b ++ \")\" renderArgs Mul [a,b] = \"(\" ++ a ++ \" \" ++ b ++ \")\" *  instance Render \nLogic where renderArgs Not [a] = \"(not \" ++ a ++ \")\" renderArgs Eq [a,b] = \"(\" ++ a ++ \" == \" ++ b ++ \n\")\" instance Render If where renderArgs If [c,t,f] = unwords [\"(if\", c, \"then\", t, \"else\", f ++ \")\"] \n Although convenient, it is quite unsatisfying to have to use refutable pattern matching on the argument \nlists. We will present a solution to this problem in section 6. The instance for AST traverses the spine, \ncollecting the rendered sub-terms in a list that is passed on to the rendering of the symbol: instance \nRender dom . Render (AST dom) where renderArgs (Sym s) as = renderArgs s as renderArgs (s :$ a) as = \nrenderArgs s (render a:as) Note that the case for (:$) has two recursive calls. The call to renderArgs \nis for traversing the application spine, and the call to render is for rendering the sub-terms. The Render \ninstance for (:+:) is analogous to the Eval instance, so we omit it. This concludes the de.nition of \nrendering for extensible languages. *Main> render (ex2 :: Expr Int) \"((5 + 0) 6)\" * The functions eval \nand render do not have any generic default cases, because all cases have interesting behavior. The next \nstep is to look at a function that has useful generic default cases.  4.3 Case study: Extensible compiler \nWill now use the presented techniques to de.ne a simple compiler for our extensible expression language. \nThe job of the compiler is to turn expressions into a sequence of variable assignments: *Main> putStr \n$ compile (ex2 :: Expr Int) v3 = 5 v4 = 0 v1 = (v3 + v4) v2 = 6 v0 =(v1 v2) * Listing 4 de.nes the \ntype CodeGen along with some utility func\u00adtions. A CodeGen is a function from a variable identi.er (the \nresult location) to a monadic expression that computes the program as a list of strings.6 The monad also \nhas a state in order to be able to generate fresh variables. Listing 5 de.nes the fully generic parts \nof the compiler. Note the similarity between the types of compileArgs and renderArgs. One difference \nbetween the Compile and Render classes is that Compile has a default implementation of its method. The \ndefault method assumes that the symbol represents a simple expression, and uses renderArgs to render \nit as a string. The rendered expression is then assigned to the result location using (=:=). The instances \nfor AST and (:+:) are analogous to those of the Render class. Finally, the compile function takes care \nof running the CodeGen and extracting the written program. The code in Listings 4 and 5 is completely \ngeneric it does not mention anything about the symbols involved, apart from the as\u00adsumption of them being \ninstances of Compile. In Listing 6 we give the speci.c instances for the symbol types de.ned earlier. \nBecause NUM and Logic are simple expression types, we rely on the default behavior for these. For If, \nwe want to generate an if statement rather than an expression with an assignment. This means that we \ncannot use the default case, so we have to provide a speci.c case. A simple test will demonstrate that \nthe compiler works as intended: ex4 = condition (num 1 = num 2) (num 3) ex2 *Main> putStr $ compile (ex4 \n:: Expr Int) v2 = 1 v3 = 2 v1 = (v2 == v3) if v1 then v0 = 3 else v6 = 5 v7 = 0 v4 = (v6 + v7) v5 = 6 \nv0 =(v4 v5) * 5. Implicit and explicit recursion So far, our functions have all been de.ned using explicit \nrecursion. But there is nothing stopping us from de.ning convenient recursion schemes as higher-order \nfunctions. For example, the AST instances for renderArgs and compileArgs (see section 4) both perform \nthe same kind of fold-like bottom-up traversal which can be captured by the general combinator fold: \nfold :: . dom b .(. a . dom a . [b] . b) . (. a . ASTF dom a . b) fold f a =go a [] where go :: . a \n. AST dom a . [b] . b go (Sym s) as =fsas go (s :$ a) as = go s (fold f a : as)  Note, again, the two \nrecursive calls in the case for (:$): the call to go for traversing the spine, and the call to fold for \nfolding the sub-terms. Despite the traversal of the spine, fold should not be confused with a spine fold \nsuch as gfoldl from Scrap Your Boilerplate [11]. Rather, we are folding over the whole syntax tree, and \ngo is just used to collect the sub-results in a list. This way of using ordinary lists to hold the result \nof sub-terms is also used in the Uniplate library [15] (see the para combinator). 6 Thanks to D\u00b4evai \nGergely for the technique of parameterizing the compiler on the result location. type VarId = Integer \ntype ResultLoc = VarId type Program = [String] type CodeMonad = WriterT Program (State VarId) type CodeGen \n= ResultLoc . CodeMonad () freshVar :: CodeMonad VarId var :: VarId . String (=:=) :: VarId . String \n. String indent :: Program . Program freshVar = do v . get; put (v+1); return v var v = \"v\" ++ show \nv v=:= expr = var v++ \"=\"++ expr indent = map (\" \" ++) Listing 4: Extensible compiler: interpretation \nand utility functions class Render expr . Compile expr where compileArgs :: expr a . ([CodeGen] . CodeGen) \ncompileArgs expr args loc = do argVars . replicateM (length args) freshVar zipWithM ($) args argVars \ntell [loc =:= renderArgs expr (map var argVars)] instance Compile dom . Compile (AST dom) where compileArgs \n(Sym s) args loc = compileArgs s args loc compileArgs (s :$ a) args loc = do compileArgs s (compileArgs \na [] : args) loc instance (Compile sub1, Compile sub2) . Compile (sub1 :+: sub2) where compileArgs (InjL \ns) = compileArgs s compileArgs (InjR s) = compileArgs s compile :: Compile expr . expr a . String compile \nexpr = unlines $ flip evalState 1 $ execWriterT $ compileArgs expr [] 0 Listing 5: Extensible compiler: \ngeneric code instance Compile NUM instance Compile Logic instance Compile If where compileArgs If [cGen,tGen,fGen] \nloc = do cVar . freshVar cGen cVar tProg . lift $ execWriterT $ tGen loc fProg . lift $ execWriterT $ \nfGen loc tell $ [unwords [\"if\", var cVar, \"then\"]] ++ indent tProg ++ [\"else\"] ++ indent fProg Listing \n6: Extensible compiler: type-speci.c code  As a demonstration, we show how to rede.ne render and compile \nin terms of fold: render2 :: Render dom . ASTF dom a . String render2 = fold renderArgs compile2 :: \nCompile dom . ASTF dom a . String compile2 a = unlines $ flip evalState 1 $ execWriterT $ fold compileArgs \na 0 Here, renderArgs and compileArgs are only used as algebras (of type (dom a . [...] . ...)), which \nmeans that the Render and Compile instances for AST are no longer needed. Despite the usefulness of functions \nlike fold, it is important to stress that our traversals are by no means restricted to fold-like patterns. \nWe can fall back to explicit recursion, or de.ne new custom recursion schemes, whenever needed. As an \nexample of a function that does not suit the fold pattern, we de.ne term equality. The generic code is \nas follows: class Equality expr where equal :: expr a . expr b . Bool instance Equality dom . Equality \n(AST dom) where equal (Sym s1) (Sym s2) = equal s1 s2 equal (s1 :$ a1) (s2 :$ a2) = equal s1 s2 &#38;&#38; \nequal a1 a2 equal = False instance (Equality sub1 , Equality sub2 ) . Equality (sub1 :+: sub2) where \nequal (InjL s1) (InjL s2) = equal s1 s2 equal (InjR s1) (InjR s2) = equal s1 s2 equal = False  And, \nonce the generic code is in place, the type-speci.c instances are trivial; for example: instance Equality \nNUM where equal (Num n1) (Num n2) = n1 == n2 equal Add Add = True equal Mul Mul = True equal = False \n We see that term equality comes out very naturally as an explicitly recursive function. Expressing this \nkind of recursion (simultaneous traversal of two terms) in terms of fold is possible, but quite tricky \n(for a general method, see the generic version of zipWith in reference [12]). In section 7, we will see \nanother example where explicit recursion is useful. 6. Regaining type-safety The use of a list to hold \nthe interpretation of sub-terms (used by, for example, renderArgs and fold) has the problem that it loses \ntype information about the context. This has two problems: The algebra function can never know whether \nit receives the ex\u00adpected number of arguments (see the refutable pattern matching in implementations \nof renderArgs).  All intermediate results are required to have the same type and cannot depend on the \ntype of the individual sub-expressions.  We can make the problem concrete by looking at the local function \ngo that traverses the spine in fold: go :: . a. AST dom a . [b] . b go (Sym s) as =fsas go (s :$ a) as \n= go s (fold fa : as)  Now, consider folding an expression with Add as its top-level sym\u00adbol: fold f \n(Sym Add :$ x :$ y), for some algebra f and sub\u00adexpressions x and y. This leads to the following unfolding \nof go: go (Sym Add :$ x:$ y)[] = go (Sym Add :$ x) [fold f y] = go (Sym Add) [fold f x, fold f y]  In \nthis sequence of calls, go is used at the following types: go :: AST dom (Full Int) . [b] . b go :: AST \ndom (Int :. Full Int) . [b] . b go :: AST dom (Int :. Int :. Full Int) . [b] . b We see that the type \nof the term gradually changes to re.ect that sub-terms are stripped away; the number of arrows (:.) deter\u00admines \nthe number of missing sub-terms. However, the type of the list remains the same, even though its contents \ngrows in each it\u00aderation. This is the root of the problem with fold. What we need instead is a list-like \ntype we will call it Args, indexed by a sym\u00adbol signature, and with the property that the number of arrows \nde\u00adtermines the number of elements in the list. With such a list type, the go function will get a type \nof this form: go :: . a. AST dom a . Args a . ... Speci.cally, in the last recursive call in the above \nexample, go will have the type: go :: AST dom (Int :. Int :. Full Int) . Args (Int :. Int :. Full Int) \n . ...  The .rst argument is an expression that is missing two sub-terms, and the intention is that \nthe second argument is a two-element list containing the result of folding those particular sub-terms. \n  6.1 Typed argument lists A de.nition of Args that ful.lls the above speci.cation is the following: \ndata Args c sig where Nil :: Args c (Full a) (:*) :: c (Full a) . Args c sig . Args c(a :. sig) infixr \n:*  Here we have added a parameter c which is the type constructor for the elements. The elements are \nof type c (Full a) where a varies with the position in the signature. Each cons cell (:*) imposes an \nadditional arrow (:.) in the signature, which shows that the number of elements is equal to the number \nof arrows. Here is an example of a list containing an integer and a Boolean, using Maybe as type constructor: \nargEx :: Args Maybe (Int :. Bool :. Full Char) argEx = Just (Full 5) :* Just (Full False) :* Nil  The \nreason for making the elements indexed by Full a rather than just a is to be able to have lists with \nexpressions in them. It is not possible to use (ASTF dom) as the type constructor c because ASTF is a \ntype synonym, and, as such, cannot be partially applied. But because the elements are indexed by Full \na, we can instead use (AST dom) as type constructor. Lists of type Args (AST dom) are used, for example, \nwhen using recursion schemes to transform expressions as we will do in the following section.  6.2 Type-safe \nfold We are now ready to de.ne a typed version of fold: typedFold :: . dom c .(. a .dom a . Args c a \n. c (Full (Result a))) . (. a . ASTF dom a . c (Full a)) typedFold f a = go a Nil where go :: . a . \nAST dom a . Args c a . c (Full (Result a)) go (Sym s) as =fsas go (s :$ a) as = go s (typedFold fa :* \nas) Note the close correspondence to the de.nition of the original fold. The Result type function simply \ngives the result type of a signature: type family Result sig type instance Result (Full a) = a type instance \nResult (a :. sig) = Result sig *Main> :kind! Result (Int :. Full Bool) Result (Int :. Full Bool) :: \n * = Bool  The Args list ensures that the algebra will always receive the ex\u00adpected number of arguments. \nFurthermore, the elements in the Args list are now indexed by the type of the corresponding sub\u00adexpressions. \nIn particular, this means that we can use typedFold to transform expressions without losing any type \ninformation. As a demonstration, we de.ne the function everywhere that applies a function uniformly across \nan expression. It corresponds to the com\u00adbinator with the same name in Scrap Your Boilerplate [11]: everywhere \n:: (. a . ASTF dom a . ASTF dom a) . (. a . ASTF dom a . ASTF dom a) everywhere f = typedFold (.s . f \n. appArgs (Sym s)) appArgs :: AST dom sig . Args (AST dom) sig . ASTF dom (Result sig) appArgs a Nil \n= a appArgs s (a :* as) = appArgs (s :$ a) as   The algebra receives the symbol and its transformed \narguments. The general function appArgs is used to apply the symbol to the folded arguments, and f is \napplied to the newly built expression. We can now use everywhere to apply optAddTop from section 3.2 \nbottom-up over a whole expression: *Main> render (ex3 :: Expr Bool) \"(((5 + 0) 6) == ((5 + 0) 6))\" *Main> \nrender $ everywhere optAddTop (ex3 ::Expr Bool) ** \"((5 6) == (5 6))\" ** For the cases when we are not \ninterested in type-indexed results, we de.ne a version of typedFold with a slightly simpli.ed type: newtype \nConst a b = Const { unConst :: a} typedFoldSimple :: . dom b .(. a .dom a . Args (Const b) a . b) . \n(. a . ASTF dom a . b) typedFoldSimple f = unConst . typedFold (.s . Const . f s)  Using typedFoldSimple, \nwe can .nally de.ne a version of Render that avoids refutable pattern matching (here showing only the \nNUM instance): class Rendersafe sym where renderArgssafe :: sym a . Args (Const String) a . String  \ninstance Rendersafe NUM where renderArgssafe (Num n) Nil = show n renderArgssafe Add (Const a :* Const \nb :* Nil) = \"(\" ++ a++ \" + \"++ b ++ \")\" renderArgssafe Mul (Const a :* Const b :* Nil) = \"(\" ++ a++ \" \n\"++ b++ \")\" * rendersafe :: Rendersafe dom . ASTF dom a . String rendersafe = typedFoldSimple renderArgssafe \n 7. Controlling the recursion All generic recursive functions that we have seen so far have one aspect \nin common: the recursive calls are .xed, and cannot be overridden by new instances. The recursive calls \nare made in the instances for AST and (:+:), and these are not affected by the instances for the symbol \ntypes. To have full freedom in writing generic recursive functions, one needs to be able to control the \nrecursive calls on a case-by-case basis. This can be achieved by a simple change to typedFold: simply \ndrop the recursive call to typedFold and replace it with the unchanged sub-term: query :: . dom ac .(. \nb . (a ~ Result b) . dom b . Args (AST dom) b . c (Full a)) . ASTF dom a . c (Full a) query fa = go \naNil where go :: (a ~ Result b) . AST dom b . Args (AST dom) b . c (Full a) go (Sym a) as =faas go \n(s :$ a) as = go s (a :* as) In typedFold, the function f is applied across all nodes, which is why \nit is polymorphic in the symbol signature. In the case of query, f is only used at the top-level symbol, \nwhich is why we can allow the constraint (a ~ Result b) (the scope of a is now the whole de.nition). \nThis constraint says that the top-most symbol has the same result type as the whole expression. By reducing \nthe required polymorphism, we make query applicable to a larger set of functions. We note in passing \nthat typedFold can be de.ned in terms of query, but leave the de.nition out of the paper. One example \nwhere query is useful is when de.ning generic context-sensitive traversals. As a slightly contrived example, \nimag\u00adine that we want to change the previously de.ned optimization everywhere optAddTop so that it is \nperformed everywhere, except in certain sub-expressions. Also imagine that we want each symbol to decide \nfor itself whether to perform the optimization in its sub\u00adterms, and we want to be able to add cases \nfor new symbol types in a modular way. Because we need to be able to add new cases, we use a type class: \nclass OptAdd sym dom where optAddSym :: sym a . Args (AST dom) a . AST dom (Full (Result a)) (The need \nfor the second parameter will be explained shortly.) The idea is that the class method returns the optimized \nexpression given the top-level symbol and its sub-terms. However, we do not want to use optAddSym as \nthe algebra in typedFold. This is because typedFold traverses the expression bottom-up, and when the \nfunction is to join the results of a symbol and its sub-terms, it is already too late to decide that \ncertain sub-terms should remain unoptimized. Rather, we have to let optAddSym receive a list of unoptimized \nsub-terms, so that it can choose whether or not to recurse depending on the symbol. We can now use query \nto lift optAddSym to operate on a complete syntax tree: optAdd :: OptAdd dom dom . ASTF dom a . ASTF \ndom a optAdd = query optAddSym Before we de.ne instances of the OptAdd class we need a default implementation \nof its method: optAddDefault :: (sym :<: dom, OptAdd dom dom) . sym a . Args (AST dom) a . AST dom (Full \n(Result a)) optAddDefault s = appArgs (Sym (inj s)) . mapArgs optAdd This function calls optAdd recursively \nfor all arguments and then applies the symbol to the optimized terms. The mapArgs function is used to \nmap a function over an Args list: mapArgs :: (. a . c1 (Full a) . c2 (Full a)) . (. a .Args c1 a . Args \nc2 a) mapArgs f Nil = Nil mapArgs f (a :* as) = f a :* mapArgs f as  In the optimization of NUM, we \nmake a special case for addition with zero, and call the default method for all other cases. The optimization \nof Logic uses only the default method. instance (NUM :<: dom, OptAdd dom dom) . OptAdd NUM dom where \noptAddSym Add (a :* zero :* Nil) | Just (Num 0) . prj zero = optAdd a optAddSym s as = optAddDefault \ns as instance (Logic :<: dom, OptAdd dom dom) . OptAdd Logic dom where optAddSym = optAddDefault  Now, \nto show the point of the whole exercise, imagine we want to avoid optimization in the branches of a conditional. \nWith the current setup, this is completely straightforward: instance (If :<: dom, OptAdd dom dom) . OptAdd \nIf dom where optAddSym If (c :* t :* f :* Nil) = appArgs (Sym (inj If)) (optAdd c :* t :* f :* Nil) \n This instance chooses to optimize only the condition, while the two branches are passed unoptimized. \nThe instance for (:+:) concludes the de.nition of optAdd: instance (OptAdd sub1 dom, OptAdd sub2 dom) \n. OptAdd (sub1 :+: sub2) dom where optAddSym (InjL a) = optAddSym a optAddSym (InjR a) = optAddSym a \n The purpose of the second parameter of the OptAdd class is to let instances declare constraints on \nthe whole domain. This is needed, for example, to be able to pattern match on the sub-terms, as the NUM \ninstance does. As a nice side effect, it is even possible to pattern match on constructors from a different \nsymbol type. For example, in the If instance, we can pattern match on Num simply by declaring (NUM :<: \ndom) in the class context: instance (If :<: dom, NUM :<: dom, OptAdd dom dom) . OptAdd If dom where optAddSym \nIf (cond :* t :* f :* Nil) | Just (Num 0) . prj t = ...  8. Mutually recursive types Many languages \nare naturally de.ned as a set of mutually recursive types. For example, the following is a language with \nexpressions and imperative statements: type Var = String  data Expr a where Num :: Int . Expr Int Add \n:: Expr Int . Expr Int . Expr Int Exec :: Var . Stmt . Expr a  data Stmt where Assign :: Var . Expr \na . Stmt Seq :: Stmt . Stmt . Stmt The purpose of the Exec construct is to return the contents of the \ngiven variable after executing the imperative program. Assign writes the result of an expression to the \ngiven variable. In the AST model, it is not directly possible to group the symbols so that only some \nof them are available at a given node. However, it is possible to use type-level tags to achieve the \nsame effect. In the encoding below, the types in the symbol signatures are tagged with E or S depending \non whether they represent expressions or statements. data Ea --Expression tag data S --Statement tag \n  data ExprDom a where NumSym :: Int . ExprDom (Full (E Int)) AddSym :: ExprDom (E Int :. E Int :.Full \n(E Int)) ExecSym :: Var . ExprDom (S :. Full (E a))  data StmtDom a where AssignSym :: Var . StmtDom \n(E a :. Full S) SeqSym :: StmtDom (S :. S:. Full S) type Expr enc a = ASTF (ExprDom :+: StmtDom) (E a) \ntype Stmt enc = ASTF (ExprDom :+: StmtDom) S  For example, ExecSym has the signature (S :. ...), which \nmeans that its argument must be one of the symbols from StmtDom, since these are the only symbols that \nresult in Full S. Because the tags above re.ect the structure of the Expr and Stmt types, we conclude \nthat Exprenc and Stmtenc are isomorphic to those types. Following this recipe, it is possible to model \narbitrary mutually recursive syntax trees using AST. 9. The SYNTACTIC library The abstract syntax model \npresented in this paper is available in the SYNTACTIC library, available on Hackage7. In addition to \nthe AST type and the generic programming facilities, the library provides various building blocks for \nimplementing practical EDSLs: Language constructs (conditionals, tuples, etc.)  Interpretations (evaluation, \nequivalence, rendering, etc.)  7 http://hackage.haskell.org/package/syntactic-1.0  Transformations \n(constant folding, code motion, etc.)  Utilities for host-language interaction (the Syntactic class \n[2, 16], observable sharing, etc.)  Being based on the extensible AST type, these building blocks are \ngeneric, and can quite easily be customized for different languages. A particular aim of SYNTACTIC is \nto simplify the implementa\u00adtion of languages with binding constructs. To this end, the library provides \nconstructs for de.ning higher-order abstract syntax, and a number of generic interpretations and transformations \nfor lan\u00adguages with variable binding.  9.1 Practical use-case: Feldspar Feldspar [3] is an EDSL for \nhigh-performance numerical compu\u00ad tation, in particular for embedded digital signal processing appli\u00adcations. \nVersion 0.5.0.18 is implemented using SYNTACTIC. Some details about the implementation can be found in \nreference [2]. A demonstration of the advantage of a modular language im\u00adplementation is given in reference \n[16], where we show how to add monadic constructs and support for mutable data structures to Feldspar \nwithout changing the existing implementation. As a concrete example from the implementation, here is \na func\u00adtional for loop used for iterative computations: data Loop a where ForLoop :: Type st . Loop \n( Length --# iterations :. st --initial state :. (Index . st . st) --step function :. Full a ) --final \nstate The .rst argument is the number of iterations; the second argument the initial state. The third \nargument is the step function which, given the current loop index and state, computes the next state. \nThe third argument is of function type, which calls for a way of embedding functions as AST terms. SYNTACTIC \nprovides different ways of doing so, but the nice thing and a great advantage of using SYNTACTIC is that \nthe embedding of functions is handled completely independently of the de.nition of ForLoop. Feldspar \nhas a back end for generating C code. It is divided in two main stages: (1) generating an intermediate \nimperative representa\u00adtion (used for low-level optimization, etc.), and (2) generating C code. It is \nworth noting that the .rst of these two stages uses the same basic principles as the compiler in section \n4.3. 10. Related work Data Types ` a la Carte [18] (DTC) is an encoding of extensible data types in \nHaskell. Our syntax tree model inherits its extensi\u00adbility from DTC. Bahr and Hvitved [4] show that DTC \nsupports generic traversals with relatively low overhead using the Foldable and Traversable classes. \nOur model differs by providing generic traversals directly, without external assistance. Given that instances \nfor said type classes can be generated automatically (as Bahr and Hvitved do), the difference is by no \nmeans fundamental. Still, our method can generally be considered to be more lightweight with slightly \nless encoding overhead. The original DTC paper only con\u00adsidered untyped expressions. Bahr and Hvitved \nextend the model to account for typed syntax trees (as all trees in this paper are). This change also \nlets them handle mutually recursive types in essentially the same way as we describe in section 8. 8 \nhttp://hackage.haskell.org/package/feldspar-language-0.5.0.1  The DTC literature has focused on using \nrecursion schemes rather than explicit recursion for traversing data types. Although exam\u00adples of explicit \nrecursion exist (see the render function in refer\u00adence [18]), the combination of explicit recursion and \ngeneric traver\u00ad sals appears to be rather unexplored. In this paper we have shown how to support this \ncombination, demonstrating that generic traver\u00adsals over extensible data types are not restricted to \nprede.ned recur\u00adsive patterns. L\u00a8 ammel and Ostermann [13] give a solution to the expression problem \nbased on Haskell type classes. The basic idea is to have a non-recursive data type for each constructor, \nand a type class representing the open union of all constructors. Interpretations are added by introducing \nsub-classes of the union type class. This method can be combined with existing frameworks for generic \nprogramming.9 One drawback with the approach is that expression types re.ect the exact structure of the \nexpressions, and quite some work is required to manage these heterogeneous types. Yet another method \nfor de.ning fully extensible languages is Fi\u00adnally Tagless [7], which associates each group of language \ncon\u00ad structs with a type class, and each interpretation with a seman\u00adtic domain type. Extending the language \nconstructs is done by adding new type classes, and extending the interpretations is done by adding new \ninstances. In contrast to DTC and our model, this technique limits interpretations to compositional bottom-up \ntraver\u00adsals. (Note, though, that this limit is mostly of practical interest. With a little creativity, \nit is possible to express even apparently non\u00adcompositional interpretations compositionally [10].) There \nexist a number of techniques for data-type generic program\u00adming in Haskell (see, for example, references \n[11, 14]). An exten\u00ad sive, though slightly dated, overview is given in reference [17]. However, these \ntechniques do not qualify as solutions to the ex\u00adpression problem, as they do not provide a way to extend \nexisting types with new constructors. Rather, the aim is to de.ne generic algorithms that work for many \ndifferent types. The spine view [9] is a generic view for the Scrap Your Boilerplate [11] style of generic \nprogramming. The Spine type has strong similarities to our AST type. The main difference is that Spine \nis a one-layer view, whereas AST is a complete view of a data type. This means that the Spine type is \nnot useful on its own it merely provides a way to de.ne generic functions over other existing types. \nIt should be pointed out that the one-layer aspect of Spine is a good thing when it comes to ordinary \ngeneric programming, but it does mean that Spine alone cannot provide a solution to the expression problem. \nSo, although Spine and AST rely on the same principle for generic constructor access, they are different \nin practice, and solve different problems. Another use of a spine data type is found in Adams Scrap Your \nZippers [1], which de.nes a generic zipper data structure. The Left data type similar to our AST holds \nthe left siblings of the current position. Just like for AST, its type parameter speci.es what arguments \nit is missing. The Right data type reminiscent of our Args holds the right siblings of the current position, \nand its type parameter speci.es what arguments it provides. This similarity suggests that it might be \npossible to implement a similar generic zipper for the AST type. Outside the Haskell world, an interesting \napproach to implementing EDSLs is Modelyze [6]. The Modelyze language is speci.cally de\u00ad signed to be \na host for embedded languages. It has built-in support for open data types, and such types can be traversed \ngenerically by pattern matching on symbolic applications in much the same way as our countAdds example \n(section 3.1). However, generic traversals 9 See slides by L\u00a8ammel and Kiselyov Spin-offs from the Expression \nProblem http://userpages.uni-koblenz.de/~laemmel/TheEagle/ resources/xproblem2.html. require resorting \nto dynamic typing (for that particular fragment of the code), which makes the approach slightly less \ntype-safe than ours. 11. Discussion In this paper we have focused on the AST model and the basic prin\u00adciples \nfor programming with it. To remain focused, we have left out many details that are important when implementing \nan embedded language but still not fundamental to the underlying syntax model. Such details include how \nto deal with variable binding and syntactic annotations. The SYNTACTIC library has support for these \naspects (with varying degree of stability), but it is important to stress that all of this extra functionality \ncan be implemented on top of the ex\u00adisting AST type. So while SYNTACTIC is still developing, the AST \nmodel appears to be rather mature. One important aspect of extensible syntax that we have not treated \nin this paper is the ability to ensure that certain constructs are present or absent at certain passes \nin a compiler. Bahr and Hvitved have demonstrated how to do this with Data Types ` a la Carte, using \na desugaring transformation as example. The example is directly transferable to our model. Our experience \nwith implementing Feldspar has shown that, while the resulting code is quite readable, developing code \nusing SYN-TACTIC can be quite hard due to the heavy use of type-level pro\u00adgramming. In the future, we \nwould like to look into ways of hiding this complexity, by providing a simpler user interface, and, for \nex\u00adample, using Template Haskell to generate the tricky code. How\u00adever, we do not expect these changes \nto affect the underlying AST type. Our syntax tree encoding imposes a certain run-time overhead over \nordinary data types. Although we have not investigated the extent of this overhead, we have not noticed \nany performance problems due to the encoding in the Feldspar implementation. Still, the performance impact \nshould be investigated, as it may become noticeable when dealing with very large programs. 12. Conclusion \n Our goal with this work is to make a library of generic building blocks for implementing embedded languages. \nAny such attempt is bound to run into the expression problem, because the library must provide extensible \nversions of both syntactic constructs and inter\u00adpretation functions. The AST model provides a pleasingly \nsimple and .exible basis for such an extensible library. Its distinguishing feature is the direct support \nfor generic recursive functions no ad\u00additional machinery is needed. For extensibility, some extra machin\u00adery \nhad to be brought in, but the overhead is quite small compared to the added bene.ts. Even though our \nmodel comes with conve\u00adnient recursion schemes, it is by no means restricted to .xed traver\u00adsals. The \nuser has essentially the same freedom as when program\u00adming with ordinary data types to de.ne general \nrecursive traversals. Acknowledgments This work has been funded by Ericsson, the Swedish Founda\u00adtion \nfor Strategic Research (SSF) and the Swedish Basic Research Agency (Vetenskapsr\u00b0 adet). The author would \nlike to thank the fol\u00adlowing people for valuable discussions, comments and other in\u00adput: Jean-Philippe \nBernardy, Koen Claessen, D\u00b4 evai Gergely, Patrik Jansson, Oleg Kiselyov, Anders Persson, Norman Ramsey, \nMary Sheeran, Josef Svenningsson, Wouter Swierstra and Meng Wang. The anonymous reviewers also helped \nimproving the paper. References [1] M. D. Adams. Scrap your zippers: a generic zipper for heterogeneous \ntypes. In Proceedings of the 6th ACM SIGPLAN workshop on Generic programming, WGP 10, pages 13 24. ACM, \n2010. [2] E. Axelsson and M. Sheeran. Feldspar: Application and implementa\u00adtion. In Lecture Notes of \nthe Central European Functional Program\u00adming School, volume 7241 of LNCS. Springer, 2012. [3] E. Axelsson, \nK. Claessen, G. D\u00b4evai, Z. Horv\u00b4ath, K. Keijzer, B. Ly\u00adckeg\u00b0ard, A. Persson, M. Sheeran, J. Svenningsson, \nand A. Vajda. Feldspar: A domain speci.c language for digital signal processing algorithms. In 8th ACM/IEEE \nInternational Conference on Formal Methods and Models for Codesign (MEMOCODE 2010), pages 169 178. IEEE \nComputer Society, 2010. [4] P. Bahr and T. Hvitved. Compositional data types. In Proceedings of the seventh \nACM SIGPLAN workshop on Generic programming, WGP 11, pages 83 94. ACM, 2011. [5] P. Bjesse, K. Claessen, \nM. Sheeran, and S. Singh. Lava: Hardware Design in Haskell. In ICFP 98: Proceedings of the Third ACM \nSIG-PLAN International Conference on Functional Programming, pages 174 184. ACM, 1998. [6] D. Broman \nand J. G. Siek. Modelyze: a gradually typed host language for embedding equation-based modeling languages. \nTechnical Report UCB/EECS-2012-173, EECS Department, University of California, Berkeley, Jun 2012. [7] \nJ. Carette, O. Kiselyov, and C.-c. Shan. Finally tagless, partially evaluated: Tagless staged interpreters \nfor simpler typed languages. Journal of Functional Programming, 19(05):509 543, 2009. [8] K. Claessen, \nM. Sheeran, and B. J. Svensson. Expressive array con\u00adstructs in an embedded GPU kernel programming language. \nIn Pro\u00adceedings of the 7th workshop on Declarative aspects and applications of multicore programming, \nDAMP 12, pages 21 30. ACM, 2012. [9] R. Hinze and A. L\u00a8oh. Scrap Your Boilerplate Revolutions. In Mathematics \nof Program Construction, volume 4014, pages 180 208. Springer, 2006. [10] O. Kiselyov. Typed tagless \n.nal interpreters. In Lecture Notes of the Spring School on Generic and Indexed Programming (to appear). \n2010. [11] R. L\u00a8ammel and S. P. Jones. Scrap your boilerplate: a practical design pattern for generic \nprogramming. In Proceedings of the 2003 ACM SIGPLAN international workshop on Types in languages design \nand implementation, TLDI 03, pages 26 37. ACM, 2003. [12] R. L\u00a8ammel and S. P. Jones. Scrap more boilerplate: \nre.ection, zips, and generalised casts. In Proceedings of the ninth ACM SIGPLAN in\u00adternational conference \non Functional programming, ICFP 04, pages 244 255. ACM, 2004. [13] R. L\u00a8ammel and K. Ostermann. Software \nextension and integration with type classes. In Proceedings of the 5th international conference on Generative \nprogramming and component engineering, GPCE 06, pages 161 170. ACM, 2006. [14] J. P. Magalh oh. A generic \nderiving aes, A. Dijkstra, J. Jeuring, and A. L\u00a8mechanism for Haskell. In Proceedings of the third ACM \nHaskell symposium on Haskell, Haskell 10, pages 37 48. ACM, 2010. [15] N. Mitchell and C. Runciman. \nUniform boilerplate and list processing. In Proceedings of the ACM SIGPLAN workshop on Haskell workshop, \nHaskell 07, pages 49 60. ACM, 2007. [16] A. Persson, E. Axelsson, and J. Svenningsson. Generic monadic \nconstructs for embedded languages. In 23rd International Symposium on Implementation and Application \nof Functional Languages, IFL 2011, volume 7257 of LNCS, 2012. [17] A. Rodriguez, J. Jeuring, P. Jansson, \nA. Gerdes, O. Kiselyov, and B. C. d. S. Oliveira. Comparing libraries for generic programming in Haskell. \nIn Proceedings of the .rst ACM SIGPLAN symposium on Haskell, Haskell 08, pages 111 122. ACM, 2008. [18] \nW. Swierstra. Data types `a la carte. Journal of Functional Program\u00adming, 18(4):423 436, 2008.  \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Representing a syntax tree using a data type often involves having many similar-looking constructors. Functions operating on such types often end up having many similar-looking cases. Different languages often make use of similar-looking constructions. We propose a generic model of abstract syntax trees capable of representing a wide range of typed languages. Syntactic constructs can be composed in a modular fashion enabling reuse of abstract syntax and syntactic processing within and across languages. Building on previous methods of encoding extensible data types in Haskell, our model is a pragmatic solution to Wadler's \"expression problem\". Its practicality has been confirmed by its use in the implementation of the embedded language Feldspar.</p>", "authors": [{"name": "Emil Axelsson", "author_profile_id": "81337487449", "affiliation": "Chalmers University of Technology, Gothenburg, Sweden", "person_id": "P3804378", "email_address": "emax@chalmers.se", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364573", "year": "2012", "article_id": "2364573", "conference": "ICFP", "title": "A generic abstract syntax model for embedded languages", "url": "http://dl.acm.org/citation.cfm?id=2364573"}