{"article_publication_date": "09-09-2012", "fulltext": "\n An Error-Tolerant Type System for Variational Lambda Calculus Sheng Chen, Martin Erwig, Eric Walkingshaw \nSchool of EECS, Oregon State University {chensh,erwig,walkiner}@eecs.oregonstate.edu Abstract Conditional \ncompilation and software product line technologies make it possible to generate a huge number of different \nprograms from a single software project. Typing each of these programs in\u00addividually is usually impossible \ndue to the sheer number of possi\u00adble variants. Our previous work has addressed this problem with a type \nsystem for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for \nintroducing and or\u00adganizing variation. Although our type inference algorithm is more ef.cient than the \nbrute-force strategy of inferring the types of each variant individually, it is less robust since type \ninference will fail for the entire variational expression if any one variant contains a type error. In \nthis work, we extend our type system to operate on VLC expressions containing type errors. This extension \ndirectly supports locating ill-typed variants and the incremental development of vari\u00adational programs. \nIt also has many subtle implications for the uni.\u00adcation of variational types. We show that our extended \ntype system possesses a principal typing property and that the underlying uni.\u00adcation problem is unitary. \nOur uni.cation algorithm computes par\u00adtial uni.ers that lead to result types that (1) contain errors \nin as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation \nto determine the overhead of this extension compared to our previous work, to demonstrate the improvements \nover the brute-force approach, and to explore the effects of various error distributions on the inference \nprocess. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations \napplicative (functional) lan\u00adguages; F.3.3 [Logics and Meanings of Programs]: Studies of Pro\u00adgram Constructs \n type structure Keywords error-tolerant type systems; variational lambda calcu\u00adlus; variational type \ninference; variational types 1. Introduction The source code of many software projects can be used to \ngenerate a huge number of distinct programs that run on different platforms and provide different sets \nof features. Current research on software product lines (SPLs) [20] and feature-oriented software develop\u00adment \n[2] provide processes and tools for the development of mas\u00adsively con.gurable software, suggesting that \nthe variability of soft\u00adware systems will only continue to grow. Unfortunately, basic pro\u00adgram veri.cation \ntools, such as type systems, are not equipped to Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 12, September 9 15, 2012, Copenhagen, Denmark. Copyright \n&#38;#169; 2012 ACM 978-1-4503-1054-3/12/09 $15.00. deal with variation on this scale. Notions of type \ncorrectness are de\u00ad.ned in terms of single programs only, but generating all program variants and testing \neach one individually is usually impossible due to the sheer number of variants that can be generated. \nThe problem of type checking variational software is an active area of research [11, 12, 25]. Most of \nthis work comes out of the SPL community and is therefore highly pragmatic, tool-oriented, and focused \non imperative languages. Our work on this problem, begun in [5], distinguishes itself in several ways. \nMost signi.cantly, while other approaches consider only type checking of programs in explicitly typed \nlanguages, we solve the more general problem of type inference for implicitly typed languages. Our approach \nbe\u00adgins by establishing a simple functional language, the variational lambda calculus (VLC), for studying \nvariational software; it intro\u00adduces a notion of variational types for typing variational programs; it \ndevelops a formal type system that associates variational types with VLC expressions; and it presents \nan algorithm that infers these types. By addressing the problem from a more theoretical and fun\u00addamental \nperspective, we believe our results are more reusable and extensible than others. Variational types are \nalso a general contri\u00adbution to type theory that have other potential applications; for ex\u00adample, they \nmay be useful for more .exibly typing metaprograms. A subtle difference between the problems of checking \nexplicitly typed programs and inferring types in implicitly typed programs is that, in general, a type \nerror encountered during inference prevents inference in the rest of the program. This means that while \nour solution is more general, it is less robust. A type error in a single variant will cause the entire \ninference process to fail. In this work we extend our type system and inference algorithm to allow for \ntype errors at arbitrary positions in the inferred variational type. This extension directly supports \nthe location of ill-typed variants, and the ability to incrementally develop variational programs by \nleaving some variational branches unde.ned or incomplete while other variants are extended and .eshed \nout. While the focus in [5] is on establishing a broad foundation for formal work on typing and other \nstatic analyses of variational programs, here we focus on solving a speci.c problem of practical importance. \nSolving this problem is surprisingly challenging and leads to many interesting theoretical results, summarized \nin Section 1.2. 1.1 Motivation In this section we will brie.y motivate this work by way of a simple \nexample. We also motivate and explain our choice of VLC as a formal foundation for typing variational \nprograms. In general, there are three competing approaches to managing variational software, each with \ntheir own strengths and weaknesses. Compositional approaches rely on language features like mixins [4] \nor aspects [16] to modularize features that may or may not be in\u00adcluded in a generated variant. This \napproach is mostly used in con\u00adjunction with object-oriented programming languages. Metapro\u00adgramming-based \napproaches rely on staged computations to gen\u00aderate program variants through the use of macros; this \nis especially common in functional languages, for example, MetaML [24] and the Lisp family. Finally, \nannotative approaches rely on a separate annotation language to embed static variation directly within \nthe source code. The C Preprocessor (CPP) is by far the most widely used annotative variation tool. One \nof the advantages of the annota\u00adtive approach is that it is mostly independent of the object language \nand so can be applied across paradigms (and even in documentation and other non-source code). CPP annotations \nare frequently seen in large-scale Haskell programs, for example, GHC [9].  Although all three approaches \nare worthy of study, we choose the annotative approach here because it makes the variation in a program \nexplicit, allowing us to directly traverse and manipu\u00adlate the variation structure. This is not the case, \nfor example, in metaprogramming approaches, where variability is captured only implicitly in the de.nition \nand use of macros. While our annotation language is much less powerful than metaprogramming systems, \nit allows us to support a much more general form of type-safe varia\u00adtion than is possible in, for example, \nMetaML. Consider two different ways to implement a function in Haskell to .nd values in a lookup list \nof type [(a,b)]. In the .rst, we return a value of type Maybe b, possibly containing the .rst value in \nthe lookup list associated with a given key of type a. find x ((k,v):t) | x == k = Just v | otherwise \n= find x t find _ [] = Nothing In the second, we return a list of type [b], containing all of the values \nin the lookup list associated with the key. findx((k,v):t)|x==k =v:findxt | otherwise = find x t find_[] \n=[] Based on a notation developed in [8], we can represent the varia\u00adtion between these two function \nimplementations by annotating the program in-place. First, we declare a new dimension of variation, Res, \nrepresenting variation in the function s result. Then we indi\u00adcate the speci.c variation points in the \ncode using choices that are bound to the Res dimension. dim Res(fst,all) in find x ((k,v):t) | x == k \n= Res(Just v, v:find x t) | otherwise = find x t find_[] = Res(Nothing,[]) The Res dimension declaration \nabove states that we can select one of two tags in the dimension: fst, to return the .rst found value, \nor all, to return all found values. The two choices in the body of the function are synchronized with \nthese tags. For example, if we select the fst tag in the Res dimension (written Res.fst), the .rst alternative \nin each of the two choices in the Res dimension will also be selected, producing the .rst function de.nition \nabove. The types inferred in a variational program are also variational. For our find function, we infer \nthe following variational type which also contains a choice in the Res dimension.1 find :: a -> [(a,b)] \n-> Res(Maybe b,[b]) Using the variational type inference algorithm we have developed in [5] we can infer \ntypes like the above. A successfully inferred variational type indicates that all variants of the program \nare type correct. Since the typing information of shared code is reused (and for other reasons), a type-correctness \nresult can be obtained much more ef.ciently in the expected case than the brute-force strategy of generating \nall variants and type checking them separately. For 1 To keep the following discussion simpler, we omit \nthe Eq type class constraint on a. large variational programs with many dimensions of variation, the \nef.ciency gains can make type checking all variants tractable, when otherwise it would not be. However, \nvariational type inference has a hidden cost relative to the brute-force strategy. While variational \ntype inference is more ef.cient at detecting errors, it is less useful for locating errors. To demonstrate, \nsuppose we add a new dimension of variation to our find function, Arg, that captures variation between \nlooking up values based on an example key (as above) or looking up values based on a predicate on keys. \nWe name the tags corresponding to these possibilities val and pred, respectively. dim Arg(val,pred) in \ndim Res(fst, all) in find Arg(x, p) ((k,v):t) | Arg(x == k,pk) = Res(Just v, v:find x t) | otherwise \n= find Arg(x,p) t find_[] = Res(Nothing,[]) Since we can make our selections in the Res and Arg dimensions \nindependently, this new expression represents four total program variants. We expect variational type \ninference to infer the following variational type for our new implementation of find. find :: Arg(a, \n(a -> Bool)) -> [(a,b)] -> Res(Maybe b,[b]) But there is an error in the above de.nition that causes \nvariational type inference to fail. The error is that the variable x is unbound in findx t if we select \nArg.pred and Res.all. This can be easily .xed by replacing x with the choice Arg(x,p). The problem is \nthat the type inference algorithm presented in [5] provides no hint at the location of this error it \njust fails, indicating that there is an error. The brute-force strategy is more robust. By type checking \neach variant individually, we can determine exactly which variant(s) contain type errors and infer types \nfor those that are type correct. Of course, the brute-force strategy scales just as poorly for error \nlocation as it does for type checking (although it might be able to be used strategically, if one can \ncorrectly guess the variants that contain errors). In this paper we extend variational type inference \nto return par\u00adtially correct variational types that is, variational types contain\u00ading errors. For example, \nthe errorful variational type of our find function can be written as follows, where . is a special type \nthat indicates a type error at that location in the type. find :: Arg(a, (a -> Bool)) -> [(a,b)] -> Res(Maybe \nb,Arg([b], .)) This type indicates that there is a type error in the result type of the function if \nthe second tag is chosen from each dimension (Arg.pred and Res.all). This extension therefore directly \nsupports the location of type errors in variational programs without resorting to the brute\u00adforce strategy \nof typing variants individually. Similarly, it supports type inference on incomplete variational programs \nprograms in which only some variants are in a complete and type-correct state a quality which is needed \nfor incremental development. The addition of error types is a non-trivial extension to the type system \nand inference algorithm presented in [5]. In particular, there are many subtle implications for the uni.cation \nof variational types. In the case of an unbound variable, as above, the location of the error is obvious. \nHowever, often there are many possible candidates for the type error, depending on how we infer the surrounding \ntypes. The goal is to assign errors such that as few variants as possible are considered ill-typed, that \nis, to .nd a type that is most\u00adde.ned. This goal is in addition to the usual goal of inferring the most \ngeneral type possible. It is not obvious whether these two qualities of types are orthogonal. In this \npaper we will show that they are, and we present an inference algorithm that identi.es most\u00adde.ned, most-general \ntypes.  1.2 Contributions and Rest of Paper In the next section we brie.y introduce the syntax and \nsemantics of VLC, developed in [5], which is the formal foundation of this work. The structure of the \nrest of the paper is described relative to the major contributions of this work, which are: 1. The extension \nof our variational type system to support the typing of programs in which not all variants are well typed. \nThe extension of the types themselves is discussed in Section 3, and the extension of the typing rules \nin Section 5. A type preservation theo\u00adrem (Theorem 1) in Section 5 formally establishes the relationship \nbetween a variational type identi.ed by our type system and the set of types or type errors produced \nby the brute-force force approach. 2. The concept of typing patterns, de.ned in Section 4, that in\u00addicate \nwhich variants of a variational program are well-typed, and an associated more-de.ned relation for comparing \nthem. We use these in Section 6 to prove several results about the problem of uni\u00adfying variational types \ncontaining type errors. Most signi.cantly, we show that for any uni.cation problem, there is a mapping \nthat produces the most-de.ned result type (Theorem 2), and that among such mappings, there is a unique \nmapping that produces the most\u00adgeneral result type (Theorem 3). 3. A uni.cation algorithm on variational \ntypes with type er\u00adrors, given in Section 7, that produces uni.ers that result in most\u00adde.ned, most-general \ntypes. This is the core component of a type inference algorithm that implements the type system presented \nin this paper, given in Section 8. We show that both algorithms are sound (Theorems 4 and 6) and complete \n(Theorem 5 and 7). 4. A theoretical and experimental evaluation of these algo\u00adrithms. In Section 7, \nwe show that uni.cation of variational types with errors does not increase the complexity of unifying \nvariational types. In Section 9, we conduct experiments that demonstrate that the overhead to support \nerror-tolerant type inference is minor and that our algorithm offers signi.cant performance improvements \nover the brute-force approach. The evaluation results also reveal an interesting relationship between \nthe distribution of type errors in an expression and the time it takes to infer a type for that expression. \n Finally, in Section 10 we discuss related work and offer conclu\u00adsions and directions for future work \nin Section 11. The following table provides a short overview of the notation used throughout the paper. \nIt is meant as an aid to .nd de.nitions faster (\u00a7 indicates the section(s) containing the de.nition). \nSyntactic Categories \u00a7 Operations \u00a7 Expressions (e) 2.1 Types (T ) 3 Typing patterns (P) 4 Environments \n(G, .) 5 Mappings (.) 6.2 Partial uni.ers (.) 6.2 Qual. type vars (aA B) 7.1 Selection LeJD.t , LT JD.i \n2.2, 3 Semantics [[\u00b7]] 2.2 Masking P <T 4 Pattern union P1 . P2 4 Type matching T1 N T2 4 Arrow lifting \n.(T ) 5 Decision to selectors .e(q) 5 Relationships \u00a7 Results \u00a7 Equivalence T1 = T2 3 De.nedness P1 .P2 \n4 More general .1 c .2 6.1 Type preservation 5 Principal patterns 6.2 unify sound &#38; complete 7.2 \ninfer sound &#38; complete 8 2. Variational Lambda Calculus While the example from the previous section \nwas presented in Haskell, here and in our previous work on typing variational func\u00adtional programs we \nconsider a simpler language, the variational lambda calculus (VLC). VLC is a conservative extension of \nlambda calculus with constructs for introducing and organizing static vari\u00adation. Constraining the problem \nto VLC allows us to focus on the fundamental problem of typing variational programs and to present our \nsolution as clearly and simply as possible. In [5] we describe how the variational type system can be \nextended to incorporate other, more advanced language features. In this section we brie.y describe the \nsyntax and semantics of VLC. 2.1 Syntax VLC is based on our previous work on the choice calculus [8]. \nThe choice calculus is a fundamental representation of variation in arbitrary tree structures (such as \na program s abstract syntax tree), designed to serve as a general foundation for theoretical research \nin the .eld of variation management. The key features of the choice calculus were already introduced \nin the previous section, namely, choices and dimensions.2 Choices specify a point of variation in a tree, \nwhile dimensions are used to synchronize and scope related choices. The syntax of VLC is given below. \nThe .rst four constructs in the syntax de.nition correspond to lambda calculus extended with constant \nvalues, while the dimension and choice constructs are from the choice calculus. If a VLC expression contains \nno dimension or choice constructs, we call the expression plain. e ::= c Constant | x Variable | .x.e \nAbstraction | e e Application | dim D(t,t) in e Dimension | D(e, e) Choice Note that every dimension \nmust contain exactly two tags and all choices must contain exactly two alternatives. This is a constraint \nmade for presentation purposes only. Variation in dimensions with n tags can be easily simulated by n \n- 1 binary dimensions. More fundamental syntactic constraints are that the tags associated with one dimension \nmust be different (so that they can be uniquely referred to for selection), and that every choice must \noccur within scope of a corresponding dimension declaration.  2.2 Semantics A VLC expression de.nes \na set of named variants a set of plain lambda calculus expressions identi.ed by the selections that must \nbe performed to produce them. These variants are computed stat\u00adically. That is, the full semantics of \na VLC expression consists of two distinct stages: a selection stage that eliminates all dimen\u00adsions and \nchoices through tag selection, and an evaluation stage that evaluates the resulting plain lambda calculus \nexpression. When we speak of the semantics of a VLC expression in this paper, we refer only to the selection \nstage, which is brie.y described below (a more thorough treatment can be found in [8]). To select a particular \nplain expression from a VLC expression, we must repeatedly select tags from dimensions until we are left \nwith an expression with no dimensions or choices. We write LeJD.t for the selection of tag t from dimension \nD in expression e. Tag selection is performed by replacing in e the topmost-leftmost di\u00admension declaration \ndim D(t1,t2) in e1 with a version of e1 that is obtained by substituting choices bound by D with either \ntheir .rst or second alternatives (depending on whether t = t1 or t = t2). If e does not contain a dimension \nD, it remains unchanged. A decision is a sequence of dimension-quali.ed tags. A decision that produces \na plain expression is called a complete decision. The (selection) semantics [[e]] of an expression e \nis then a mapping from complete decisions to plain lambda calculus expressions. [[dim A(t1,t2) in A(.x.x,.y.dim \nB(t3,t4) in B(2, 3))]] = {([A.t1],.x.x),([A.t2,B.t3],.y.2),([A.t2,B.t4],.y.3)} 2 We omit here for simplicity \ntwo constructs for sharing since they do not affect the type system in any way.  Note that tags in dimension \nA always occur before tags in dimen\u00adsion B in the domain of the mapping. Also, note that dimension B \ndoes not appear at all in the .rst decision since it is eliminated by the selection of the tag A.t1. \n3. Partial Variational Types In Section 1 we motivated the use of variational types for typing variational \nprograms. In this section we extend this representation to support partial variational types, that is, \nvariational types that contain type errors. The extended representation is given below. T ::= t Constant \nType | a Type Variable | T . T Function Type | D(T,T ) Choice Type |. Error Type |T OK Type Constant \ntypes, type variables, and function types are as in other type systems plain types contain only these \nthree constructs. Non-plain types may also contain choice types. Choice types encode variation in types \nin the same way that choices encode variation in expressions, with the exception that dimension names \nin types are globally scoped (see [5] for the rationale). Choice types often correspond directly to choice \nexpressions; for example, the subexpression A(.x.true,3) might have the corresponding choice type A(a \n. Bool,Int). Since there are no tags at the type level, we extend selection to types by writing LT JD.i, \nwhere i .{1,2}, to represent selecting the ith alternative in all choices in dimension D. If T contains \nno such choices, then LT JD.i = T . We call D.i a selector and allow selections on types to be made in \nany order. The error type, ., represents a type error and can appear any\u00adwhere in a variational type. \nWe say that a variational type is partial if it contains one or more error types and complete otherwise. \nFinally, the symbol T is used to represent an arbitrary complete type that also contains no type variables, \nthat is, a type that is monomorphic and error-free. This abstraction is only used in typing patterns, \nwhich are described in the next section. Many syntactically different types can be considered equiva\u00adlent \nin that they represent essentially the same mapping from de\u00adcisions to plain types. Type equivalency \nis an important concept in typing variational programs. For example, usually when apply\u00ading a function \nof type T . T 1 to an argument of type T 11, we re\u00adquire that T = T 11, but this requirement is too strict \nin the vari\u00adational setting. Consider the expression succ A(1,2). The type of succ is Int . Int while \nthe type of the argument is A(Int,Int). Even though Int A(Int, Int), the expression should be con\u00ad = \nsidered well-typed because both variants (succ 1 and succ 2) are well-typed. Thus, we say that the two \ntypes are equivalent, written Int = A(Int,Int), and require only equivalency rather equality in well-typed \nfunction applications. Figure 1 gives the type equivalence relation in full. Most of the equivalence \nrules are straightforward. The FUN and CHOICE rules propagate equivalency across function types and choice \ntypes, the F-C rule commutes function types and choice types, and the two SWAP rules commute choice types \nin different dimensions. The three rules at the bottom of the .gure make the relation re.exive, symmetric, \nand transitive. The two interesting cases are C-IDEMP and the MERGE rules. The C-IDEMP rule captures \nthe property of choice idempotency, demonstrated in the example above. The MERGE rules capture the property \nof choice domination. For ex\u00adample, given the choice type D(D(T1,T2),T3), we say that the outer choice \ndominates the inner since there is no way to select type T2 the selection of the .rst alternative in \nthe outer choice implies the selection of the .rst alternative in the inner choice. Note that choice \ndomination only applies to nested choices in the same dimension. FUN Tl 1= T 1 Tl = Tr r Tl 1. Tl = T \n1. Tr r F-C D(T1, T2). D(T11 ,T21) = D(T1 . T11 ,T2 . T2 1) C-C-SWAP1 D1(D(T1,T2),T3)= D(D1(T1,T3),D1(T2,T3)) \nC-C-SWAP2 D1(T1,D(T2,T3)) = D(D1(T1,T2),D1(T1,T3)) C-C-MERGE1 C-C-MERGE2 D(D(T1, T2), T3)= D(T1,T3) D(T1,D(T2,T3)) \n= D(T1,T3) CHOICE C-IDEMP T1 = T1 1 T2 = T2 1 T1 = TT2 = T D(T1,T2)= D(T1 1 , T21) D(T1,T2)= T SYMM TRANS \nREFL T 1= T 11 T = T 1 T = T 1 T = T T = T 11T 1= T Figure 1: Variational type equivalence. In [5] we \nde.ne a normalization process that can be used to check if two types are equivalent; this can be trivially \nextended to variational types containing error types. A type is in normal form if (1) all function types \nare maximally distributed into choice types, (2) choice types are nested according to a .xed ordering \non dimen\u00adsion names, (3) the alternatives of each choice type are different, and (4) no choice type contains \nanother choice type of the same name. For example, the types B(Int,Int). A(Bool,.) and Int . A(Bool, \n.) are not in normal form, but A(Int . Bool,Int . .) is.  4. Typing Patterns A typing pattern is a variation \ntype consisting only of ., T, and choice types and is used to describe which variants of an expres\u00adsion \nare well-typed and which contain type errors. For example, the typing pattern P = A(T,B(T,.)) indicates \na type error in the vari\u00adant corresponding to the decision [A.2,B.2], and not in any other variants. \nA single typing pattern corresponds to an in.nite num\u00adber of partial variational types. Some types corresponding \nto P include: A(Int,B(Bool,.)), A(Int,Bool). B(Int,A(Bool, .)), and A(Int,B(Bool,.) . B(Int,.)). In these \nexamples, the con\u00adstant and function types are irrelevant all that matters is that se\u00adlecting [A.2,B.2] \nproduces a type containing errors, and that all other type variants are complete. Typing patterns are \nnot really types in the traditional sense, but rather an abstraction of variation types that indicate \nwhere the errors are in the variation space. They are useful for determining which types are more de.ned \nthan others (that is, which contain errors in fewer variants) and play a crucial role in the uni.cation \nof partial types (see Section 7). We con.ate the representation of variational types and typing patterns \nbecause they behave similarly and doing so allows us to reuse a lot of machinery. In the rest of this \nsection, we employ typing patterns to de.ne a few operations that will be used throughout the paper. \nWe begin by de.ning a re.exive, transitive relation for deter\u00admining which typing patterns are more de.ned \nthan others, given in Figure 2. All typing patterns are more de.ned than . and less de.ned than T. Note \nthat one typing pattern is not more de.ned  PP1 PP2PP P .T P PD(P1,P2) PP P1 P1 P1 P2 P11 P22 D(P1,P2) \nPD(P1,P2) D(P11 ,P21) Figure 2: The more-de.ned relation on typing patterns. than another by simply having \nfewer occurrences of error types. For example, the pattern A(B(.,T),B(T, .)) is trivially more de\u00ad.ned \nthan .. Next, we consider the masking of types with patterns. Given a pattern P and a type T , masking \nP <T potentially adds error types to T according to the position of error types in P. T <T = T . <T = \n. D(P1,P2) <T = D(P1 <LT JD.1,P2 <LT JD.2) For example, masking type Int . A(Bool,Int) with the typing \npattern A(T,.) yields the type A(Int . Bool, .). The intersection of two typing patterns P and P1, written \nP.P1 , is a pattern that is well-typed in exactly those variants that are well\u00adtyped in both P and P1. \nFor example, given patterns A(T,.) and B(., T), their intersection is A(B(.,T),.), which indicates that \nthe only well-typed variant corresponds to the decision [A.1,B.2]. Intersection is just a special case \nof masking, where the masked type is a typing pattern: P . P1 = P <P1 . The dual of intersection is pattern \nunion. The union of two typing patterns P and P1, written P . P1, is well-typed in those variants that \nare well-typed in either P or P1, or both. T. P = T .. P = P D(P1,P2). P = D(P1 .LPJD.1,P2 .LPJD.2) For \nexample, the union of A(T,.) and B(.,T) is A(T,B(.,T)). Note that the above de.nitions are all left-biased \nwith regard to the nesting order of choices and the structure of the resulting type. This bias can be \neliminated through the normalization process described in [5], which can be applied unaltered to typing \npatterns. In the typing process, we often need to check whether two types match, for example, to check \nthat the argument type of a function matches the type of the argument it is applied to. Rather than a \nsimple boolean response, we can use typing patterns to provide a more precise account, indicating in \nwhich variants the types match (T) and in which they do not (.). In the following de.nition of the variational \ntype matching operation N : T \u00d7T . P we assume both arguments are in normal form.3 T N T = T T1 . T1 \n1 N T2 . T 1 = T1 N T2 . T 1 T 1 21 N 2 T 1 D(T1,T2) N D(T11 ,T21) = D(T1 N T11 ,T2 N 2)D(T1, T2) N T \n= D(T1 N T, T2 N T )T N D(T1,T2) = D(T1, T2) N T . N T = T N . = . T N T 1 = . (otherwise) For example, \nmatching Int . A(Bool,.) N B(Int,.) . Bool produces the typing pattern A(B(T,.),.). This operation is \nused in the typing of applications, as we ll see in the next section. 3 Assumed for this presentation \nonly. Type matching is actually part of the uni.cation algorithm, whose arguments need not be in normal \nform. T-CON T-ABS T-VAR c is a constant of type t .,G;(x, T 1) f e : T G(x) = T .,G f c : t .,G f .x.e \n: T 1 . T .,G f x : T T-APP .,G f e1: T1 .,G f e2: T2 T2 1. T 1 = .(T1) P = T2 1 N T2 T = P <T 1 .,G \nf e1 e2: T T-DIM .;(D,D1),G f e : TD1 is fresh .,G f dim D(t1,t2) in e : T T-CHOICE ., G f e1: T1 .,G \nf e2: T2 .(D)= D1 .,G f D(e1,e2) : D1(T1,T2) Figure 3: Typing rules mapping VLC expressions to partial \ntypes. 5. An Error-Tolerant Type System The association of variational types with VLC expressions is \ndeter\u00admined by a set of typing rules, given in Figure 3. A VLC typing judgment has the form .,G f e : \nT , which states that expression e has type T in the context of environments . and G. Environ\u00adments are \nimplemented as stacks, where E;(k, v) means to push the mapping (k,v) onto environment E, and E(k)= v \nmeans that the topmost occurrence of k is mapped to v in E. The G environment maps variables to types \nand is the standard typing environment for lambda calculus. It is used as expected in the typing rules \nfor vari\u00adables and abstractions. The . environment maps expression-level dimension names to globally \nunique type-level dimension names. These mappings are added by the T-DIM rule and referenced by the T-CHOICE \nrule. The use of this environment also ensures that every choice is in scope of a corresponding dimension. \nThe focus here is on the T-APP rule for typing applications, ex\u00adtending it to support partial types. \nPreviously this rule required that the left argument be equivalent to a function type whose argument \ntype is uni.able with the type of the parameter value. In the pres\u00adence of partial types, we can relax \nthese requirements, introducing error types (rather than failing) when they are not satis.ed. There are \nessentially two ways that error types can be intro\u00adduced: (1) if we cannot convert the type of the left \nargument T1 into a function type T2 1. T 1, and (2) if T2 1 does not match the type of the parameter \nT2. The introduction of errors in the second case is handled by matching the two types using the N operation \nto pro\u00adduce a typing pattern P, then masking the result type T with P. In the .rst case, we employ a \nhelper function ., which lifts a function type to the top level, introducing error types as needed. .(T1 \n. T2)= T1 . T2 .(D(T1 . T1 1 , T2 . T21))= D(T1,T2). D(T11 ,T2 1) .(D(T1,T2))= .(D(.(T1),.(T2))) .(T \n)= ... (otherwise) For example, .(A(Int . Bool, Bool . Int))= A(Int,Bool). A(Bool, Int), while .(A(Int \n. Bool, Int)) must introduce error types to lift the function type to the top: A(Int,.) . A(Bool,.). \nTo illustrate the typing of an application, consider the expres\u00adsion e1 e2, where e1: A(Int . Bool,Bool \n. Bool) and e2: Int. Applying . to the type of e1 and simplifying the result type yields the type A(Int, \nBool). Bool. Matching A(Int,Bool) N Int pro\u00adduces the typing pattern A(T,.), which we use to mask the \nresult, A(T, .) <Bool, producing the type of the application: A(Bool,.).  The previous T-APP rule emerges \nas a special case of the gen\u00aderalized one. When e1 is a function type whose argument type matches the \ntype of e2, then matching returns T and masking doesn t alter the return type. The correspondence between \nvariational types and VLC expres\u00adsions is established inductively through the process of selection. Given \nthat e : T , if e is plain, then T is a plain type or .. If e is not plain, then we can select a tag \nfrom e to produce e1 : T 1, and T 1 can be obtained by a corresponding selection from T . The induc\u00adtive \nstep is captured in the following lemma, which can be proved by induction over typing derivations. LEMMA \n1 (Variation elimination). ., G f e : T =.. D,t : .,G fLeJD.t : LT J.e ([D.t]) Since tags are not present \nat the type level, and since expression\u00adlevel dimension names may differ from type-level ones, the func\u00adtion \n.e is a function derived from e that maps tag sequences to the set of corresponding type-level selectors. \nBy induction it follows that a sequence of selections that pro\u00adduces a plain expression can be used to \nselect a corresponding plain or error type. This results in the following theorem, where q is a list \nof dimension-quali.ed tags and s is a list of type-level selectors. THEOREM 1 (Type preservation). If \n\u00d8, G f e : T and (q,e1) . [[e]], then \u00d8,G f e1 : T 1 where .e(q)= s and (s,T 1) . [[T ]]. This theorem \ndemonstrates the soundness of the type systems since it establishes that from the type of a variational \nprogram we can obtain the type of each program variant it contains. We had similar type preservation \nresults in [5], but they applied to only well-typed variational programs. The results here are stronger \nsince they apply to any variational programs. 6. The Uni.cation of Partial Types Having extended the \ntype system to work with and produce partial types, we now turn to the more challenging problem of inferring \nvariational types containing type errors. By far the most dif.cult piece is partial type uni.cation. \nIn Section 6.1 we will describe the speci.c challenges posed. In particular, the uni.cation algorithm \nmust yield uni.ers that produce types that are both most-general and most-de.ned, two qualities that \nare not obviously orthogonal. In Section 6.2 we show that such uni.ers exist, and in Section 7 we present \nan algorithm for computing uni.ers. 6.1 Reconciling Type Partiality and Generality To support partial \ntype inference, we must extend variational type uni.cation to produce and extend mappings containing \nerror types, and to identify mappings that are somehow best. As a running example, consider the application \nee1 where e : T = A(Int,Bool). a and e1 : T 1 = B(Int, a). Usually we would .nd the most general uni.er \n(mgu) for the problem A(Int,Bool)=? B(Int,a), but in this case the two types are not uni.able since there \nis a type error in the [A.2,B.1] variant. So what should we map a to? The mapping we choose should be \nmost-general in the usual sense, but it should also be most-de.ned, yielding types with type errors in \nas few variants as possible. In this subsection we will explore the interaction of these two properties. \nIn Figure 4 we list several mappings we might choose to par\u00adtially unify T and T 1 in our example. In \nthe table, the type constants Bool, Char, and Int are shortened for space reasons. Each mapping is identi.ed \nby a .i, for example, .2 = {a . Int}. We also give the result of applying each mapping to each of the \ntwo types as Ti and Ti 1, the typing pattern Pi that results from matching the argument a. .i by c b. \nPi by c. .i by , c Figure 5: Orderings among patterns, result types, and mappings. type of Ti to Ti 1, \nand the result type generated by masking the result type of Ti with Pi. Note that we apply mappings by \nadjacency and use the functions arg and res to access, respectively, the argument and result types of \na function type. Figure 5 visualizes the more-general and more-de.ned relation\u00adships among mappings and \ntyping patterns. The relations are de\u00ad.ned for elements connected by lines, and the element higher in \nthe graph is considered more general or more de.ned. The .rst thing to note is that the standard more-general \nrelation, c, is not very helpful in selecting a mapping. A mapping . is more general than .1, written \n. c .1 if ..11 such that .1 = .11 . .. But this relationship is only de.ned on one pair of our .ve mappings: \n.5 c .4 (since {b . A(Int,Bool)} . .5 = .4). Since we are not restricted to mappings that are valid uni.ers, \nthere are many more possibilities, and many will not be ordered by the more-general relation. More useful \nis the more-de.ned relation (see Section 4) on the match-produced typing patterns, for which many relationships \nare de.ned, as seen in Figure 5b. Using this metric, we can rule out mappings .1, .2, and .3 because \nthey will produce types with errors in more variants than the mappings .4 and .5. The problem is that \n.4 and .5 produce the same pattern. The solution, of course, is to use both metrics together, as demonstrated \nin Figure 5c. The solid lines between mappings cor\u00adrespond to more-de.ned relations between the generated \ntyping patterns, and the dotted line corresponds to the more-general re\u00adlation between the mappings directly. \nThis reveals .5 as the most\u00adde.ned, most-general mapping. At this point it is not clear whether this \nconvergence was a quirk of our example, or whether these properties will always converge in this way. \nIn the next section we will tackle the general case, and show that a most-de.ned, most-general mapping \nalways exists.  6.2 Most-General Partial Uni.ers In Section 6.1, we have illustrated how uni.cation \nwith partial types requires the integration of two partial orderings of types, and c. In this section, \nwe introduce the necessary machinery that enables uni.cation to deal with this situation in general and \nproduce most general partial uni.ers. In the following we consider a general uni.cation problem of the \nform U = TL =? TR. For a given mapping ., we write U :: . for the typing pattern TL. N TR. that results \nfrom . and U. When we say that P is a typing pattern for U, we mean that there is some . such that P \n= U :: .. With vars(U) we refer to all type variables in U, and we use dom(.) to denote the domain of \n.. We use l.lU to normalize . with respect to the variables in U, that is, l.lU is obtained from . by \nrenaming type variables such that dom(l.lU )= vars(U). Finally, we extend selection to apply to uni.cation \nproblems and mappings, that is, LUJD.i = LTLJD.i =? LTRJD.i and L.JD.i = {(a,LT JD.i) | (a, T ) . .}. \nWe write .|V for the restriction of . by a set of variables V , which is de.ned as .|V = {(a,a.) | a \n. V }. The .rst three lemmas state that selection extends in a homo\u00admorphic way across several operations. \n T 1 = T 1.i = arg(Ti) N T 1 = Pi <res(Ti) .i Ti = T .i i Pi iRi 1 {a . Ch} A(In, Bo). Ch 2 {a . In} \nA(In, Bo). In 3 {a . Bo} A(In, Bo). Bo 4 {a . A(In,Bo)} A(In,Bo). A(In, Bo) 5 {a . B(b,A(In,Bo))} A(In,Bo). \nB(b,A(In, Bo)) Figure 4: Some mappings for T = A(Int,Bool). a and T 1 LEMMA 2. LTL N TRJD.i = LTLJD.i \nN LTRJD.i The proofs for this and the following lemmas, left out for brevity, proceed by applying the \nde.nition of the operation under consider\u00adation and then performing structural induction on types. LEMMA \n3. LTL . TRJD.i = LTLJD.i .LTRJD.i LTL . TRJD.i = LTLJD.i .LTRJD.i LP <T JD.i = LPJD.i <LT JD.i B(In, \nCh) A(B(T,.),.) A(B(Ch, .), .) B(In, In) A(T,.) A(In,.) B(In, Bo) A(B(T,.),B(.,T)) A(B(Bo,.),B(.,Bo)) \nB(In,A(In,Bo)) A(T,B(.,T)) A(In,B(.,Bo)) B(In,A(In,Bo)) A(T,B(.,T)) A(B(b,In), B(.,Bo)) = B(Int,a), with \nthe typing pattern and result types they produce. Third, if there is no relation between P21 and P11 \nor P12 and P22, we let U1 = LUJD.1, U2 = LUJD.2, .11 = .1|vars(U1), .12 = .1|vars(U2), .21 = .2|vars(U1) \nand .22 = .2|vars(U2). By induction, we can construct a mapping .31 from .11 and .21 for U1 such that \nU1 :: .31 = P11 . P21. Likewise, we can construct a mapping .32 from .12 and .22 for U2 such that U2 \n:: .32 = P12 .P22. We can now build .3 based on .31 and .32 as follows. For each type variable a . vars(U) \nwe de.ne .3 as follows. LTL . TRJD.i = LTLJD.i .LTRJD.i We also have a similar result for type substitution. \n.3(a)= LEMMA 4. LT .JD.i = LT JD.iL.JD.i . .. .. D(a.31, a.32) if a . vars(U1) . a . vars(U2) a.31 if \na . vars(U1) a.32 if a . vars(U2) The next lemma says that the computation of typing patterns can be \ndecomposed by using selection. LEMMA 5. LU :: .JD.i = LU :: L.JD.iJD.i = LUJD.i :: L.JD.i PROOF. The \nproof for the .rst part is as follows. Let P = U :: . and P1 = U :: L.JD.i, then LPJD.i = LTL. N TR.JD.i \n= LTL.JD.i N LTR.JD.i by Lemma 2 = LTLJD.iL.JD.i N LTRJD.iL.JD.i by Lemma 4 LP1JD.i = LTLL.JD.i N TRL.JD.iJD.i \n= LTLL.JD.iJD.i N LTRL.JD.iJD.i by Lemma 2 = LTLJD.iLL.JD.iJD.i N LTRJD.iLL.JD.iJD.i by Lemma 4 = LTLJD.iL.JD.i \nN LTRJD.iL.JD.i The proof for the second part is analogous. D LEMMA 6 (Typing patterns have a join). \nIf P1 and P2 are typing patterns for U, then so is P1 . P2. PROOF. Assume .1 and .2 are the mappings \nsuch that P1 = U :: .1 and P2 = U :: .2. The proof consists of several cases. For each case, we construct \na mapping .3 such that U :: .3 = P1 . P2, which we denote as P3. We show the proof for the case where \nP1 = D(P11,P12) and P2 = D(P21, P22) and there is no relation between P1 and P2. The proofs for other \ncases are simpler or can be transformed into this case. We assume that .1 and .2 are already normalized \nwith respect to U. We can consider several cases. First, if we assume P21 P11 and P12 P22, we let .3 \n= {(a, D(La.2JD.1,La.1JD.2)) | a . vars(U)}, for which we observe the following. U :: .3 = D(LU :: .3JD.1, \nLU :: .3JD.2) = D(LUJD.1 :: L.3JD.1,LUJD.2 :: L.3JD.2) Lemma 5 = D(LUJD.1 :: L.1JD.1,LUJD.2 :: L.2JD.2) \nconstruction = D(LU :: .1JD.1, LU :: .2JD.2) Lemma 5 = D(P21,P12) = P1 . P2 def. of . Second, the case \nfor P11 P21 and P22 P12 is analogous. Proving that U :: .3 = D(P31,P32) = D(P11,P12). D(P21, P22) is \nsimilar to the proof for the previous case. D Combining this lemma with the rule T P we can conclude \nthat for any uni.cation problem, there is an upper-bound typing pattern, which we call the principal \ntyping pattern. THEOREM 2 (Existence of principal typing patterns). For every uni.cation problem U there \nis a mapping . with P = U :: ., such thatP P1 for any other mapping .1 with P1 = U :: .1 . We call a \nmapping that leads to the principal typing pattern a partial uni.er and use . to denote partial uni.ers. \nWe call mappings that are not partial uni.ers non-uni.ers for short. Based on these de.nitions, the .rst \nexample in Section 6.1 has the principal typing pattern P4 and partial uni.ers .4 and .5. Theorem 2 only \nshows the existence of partial uni.ers, but does not say anything about how many partial uni.ers exist \nand how they are possibly related. It turns out that partial uni.ers can be compared with respect to \ntheir generality and for each uni.cation problem there is a most general partial uni.er (mgpu) of which \nall other partial uni.ers are instances. THEOREM 3 (Partial uni.cation is unitary). For every uni.cation \nproblem U there is one partial uni.er . of such that any other partial uni.er .1 for U is an instance \nof it, that is, . c .1 . The proof strategy is similar to that for Theorem 2, although more complex. \nGiven any two partial uni.ers, we can construct a new partial uni.er that is more general than the old \nones. 7. A Uni.cation Algorithm In this section we present a partial type uni.cation algorithm that identi.es \npartial uni.ers that produce most-general, most-de.ned types. This algorithm is a conservative extension \nof our algorithm for unifying complete variational types, presented in [5]. That is, when the types are \ncomplete and fully uni.able, we produce the same results as before. When the types to be uni.ed are partial \nand/or not uni.able, we produce partial uni.ers as described in the previous section. In Section 7.1 \nwe give a high-level overview of the process of unifying variational types, and in Section 7.2 we de.ne \nthe algorithm that makes up the core of this process.  7.1 Uni.cation of Variational Types The fundamental \ndifference between traditional type uni.cation [3] and variational uni.cation is the treatment of type \nvariables. Con\u00adsider the uni.cation problem A(Int,a)=? A(a, Bool). At .rst it may seem that these types \nare not uni.able blithe decomposition by alternatives yields the subproblems Int =? a and a =? Bool, \nbut a cannot map to both Int and Bool. However, there is a uni.er to the original problem: if we map \na to A(Int, Bool), then both types are equivalent to A(Int,Bool) by choice domination (see Section 3). \nDecomposition is essential to the uni.cation process, but decom\u00adposing by alternatives discards important \ncontext provided by the choice type. In our example, this context tells us that only one of the two a \ntype variables will be selected in any particular variant. As a solution, we encode the contextual information \nin the type variables themselves. A quali.ed type variable is a type variable marked by the choice type \nalternatives in which it is nested. We write aAB to indicate that type variable a is located in the .rst \nalter\u00adnative of a choice type in dimension A and the second alternative of a choice type in B. Throughout \nmost of the uni.cation process, type variables with different quali.cations are simply considered to \nbe different type variables, but we can use the contextual information to construct the .nal mappings \n(from unquali.ed type variables to variational types) through a process called completion. In the ex\u00adample \nabove, after quali.cation and decomposition we identify the mappings {aA . Int,aA . Bool} which completes \nto the .nal re\u00adsult {a . A(Int,Bool)}. Uni.cation thus consists of three main phases: (1) the uni.ca\u00adtion \nproblem U is translated into a corresponding quali.ed uni.ca\u00adtion problem Q, (2) Q is solved, and (3) \nthe solution to Q is com\u00adpleted to produce a solution to U. The .rst step of this process is trivial. \nWe simply traverse both types and qualify all of the type variables. Completion is also straightforward: \ngiven a list of map\u00adpings from quali.ed type variables, each aqi . Ti describes a leaf in a tree of nested \nchoice types that makes up the type T in the com\u00adpleted mapping a . T . We just iterate over the quali.ed \nmappings, lazily constructing and populating the resulting tree. The dif.cult part is of course solving \nthe quali.ed uni.ca\u00adtion problem. In addition to the traditional operations of match\u00ading and decomposition, \nquali.ed uni.cation relies on two addi\u00adtional operations. First, a choice type can be hoisted over another \nchoice type. For example, hoisting transforms A(T1,B(T2,T3)) into B(A(T1,T2), A(T1,T3)). Second, a type \nvariable can be split into a choice type between two quali.ed versions of that variable. For example, \nsplitting transforms a into A(aA,aA )). These operations manipulate the types being uni.ed so they can \nbe further matched or decomposed. For example, the problem A(Int,aA )=? B(bB, cB )cannot be directly \ndecomposed. However, if we split the variable aA into B(a AB ) and hoist this choice type to the top, \nwe get the AB,a new problem B(A(Int, aAB ),A(Int, a B)) =? B(bB,cB ), which can A be decomposed into \ntwo trivial subproblems. The full technical exposition of the uni.cation of complete variational types \nis provided in [5]. Signi.cantly, we also show that the uni.cation problem is decidable and unitary. \nIn the rest of this section we will develop the uni.cation of partial variational types.  7.2 Computing \nthe Most General Partial Uni.er In Section 6.2 we showed that for each partial uni.cation problem, there \nis a unique mgpu that produces the corresponding principal typing pattern. In this section, we show how \nto compute each of these by extending the process described in Section 7.1. We do this .rst by example, \nthen give the algorithm directly. Consider the uni.cation problem A(Int,a)=? B(Bool,b). We begin, as \ndescribed in Section 7.1, by transforming this into the corresponding quali.ed uni.cation problem shown \nat the top of A(Int,aA )=? B(Bool, bB ) .split A(Int,aA )=? B(Bool, A(bAB ,bA B )) .hoist A(Int,aA )=? \nA(B(Bool,bAB ),B(Bool, b B)) A Int =? B(Bool,bAB ) aA =?B(Bool, bA B ) *Int =? Bool * Int =? bA B Figure \n6: Quali.ed uni.cation resulting in a type error. Figure 6. Since the top-level choice names don t match, \nwe choose a type variable and apply the split-hoist strategy (.rst two steps) in order to decompose by \nalternatives (third step). This gives us the two subproblems at the fourth level from the top. When a \nplain type is uni.ed with a choice type, we can decompose it by unifying the plain type with each alternative. \nThis is demonstrated in the left branch, which yields two smaller subproblems, one of which, Int =? Bool, \nreveals a type error. This decomposition contains all of the information needed to construct both the \nmgpu and the principal typing pattern. We con\u00adstruct the mgpu by composing the mappings generated at \nthe end of every successful branch of the uni.cation process. In this case, there were two successful \nbranches, giving the following mgpu. {aA . B(Bool,b B),bAB . Int} A We construct the principal typing \npattern by observing which branches of the decomposition fail and succeed. In this case, the branch corresponding \nto the .rst alternative in both A and B failed, yielding the principal error pattern A(B(.,T),T). As \nthe .nal step, we use completion to produce the solution to the original (unquali.ed) uni.cation problem. \n{a . A(c, B(Bool,d)), b . B( f ,A(Int,d))} Figure 7 gives the partial uni.cation algorithm. It accepts \na qual\u00adi.ed uni.cation problem TL =? TR and returns a principal typing pattern P and a mgpu .. We show \nonly the cases that differ signi.\u00adcantly from the quali.ed uni.cation algorithm presented in [5]. The \nalgorithm relies on several helper functions. The function choices(T ) returns the dimension names of \nall choice types that occur in T . The function splittable returns the set of type variables that can \nbe split into a choice type. A variable is splittable if the path from itself to the root consists only \nof choice types (no function types). The function vars(T ) returns the set of quali.ed variables in a \ntype. Finally, the function sdims(vq, T ) returns the set of dimension names not present in q but present \nin the quali.cations of type variables that are more speci.c than vq. We say that up is more speci.c \nthan vq if u = v and p can be written as qp1 for some nonempty p1. For example, sdims(aA, aAB . Int)= \n{B}. We will work through the cases of the unify algorithm, from top to bottom. In the body of the algorithm \nand in these descriptions, TL and TR are used to refer to the .rst and second arguments to unify, respectively. \nWe .rst consider a couple of base cases. Attempting to unify any type and an error type yields an empty \nmapping and the fully unde.ned typing pattern .. This de.nes the propagation of errors. When unifying \ntwo plain types, we defer to the traditional robinson uni.cation algorithm [21]. If it succeeds, we return \nthe uni.er and the fully de.ned typing pattern T. If it fails, we return the empty mapping and .. When \nunifying a ground plain type g (a type that does not con\u00adtain choice types or type variables) with a \nchoice type, we just unify g with both alternatives. This is seen in the second decomposition  unify \n: T \u00d7 T . P \u00d7 . unify(., T ) = (.,\u00d8) unify(p, p1) | robinson(p, p1)= . = (.,\u00d8) | otherwise = (T,robinson(p, \np1)) unify(g,D(T1,T2)) = unify(D(g,g),D(T1,T2)) unify(D(T1, T2),D(T11 ,T21)) = (P1,.1) . unify(T1,T11) \n(P2,.2) . unify(T2,T21) return (D(P1,P2),.1 . .2) unify(D1(T1,T2),D2(T11 ,T21)) | D2 ./choices(TL) . \nsplittable(TL)= \u00d8. D1 ./choices(TR) . splittable(TR)= \u00d8 = unify(TL,D1(TR,TR)) unify(vq,T1 1. T21) | vq \n. vars(TR) = (.,\u00d8) | D . sdims(vq,TR) = unify(D(vDq,v Dq), TR) | otherwise = (T,{vq . TR}) unify(T1 \n. T2,T1 1. T21) = (P1,.1) . unify(T1,T11) (P2,.2) . unify(T2.1,T21.1) P . P1 . P2 return (P,.1 . .2) \nFigure 7: Partial uni.cation algorithm. in Figure 6. The .rst decomposition is by alternatives, which \nis per\u00adformed when unifying two choices in the same dimension; this is captured in the fourth case of \nunify. Note that we do not need to apply the mapping .1 to T2 and T21, as we might expect, because (vars(T1).vars(T11)) \nn(vars(T2).vars(T21)) = \u00d8due to type vari\u00adable quali.cation. We then compose the corresponding uni.ers \nand combine the error patterns with a choice type. The .fth case considers the uni.cation of two choice \ntypes in different dimensions with no splittable type variables. This is not fully uni.able and so would \nusually represent failure. However, with partial uni.cation we can proceed by attempting to unify all \ncombinations of alternatives in order to locate the variants that con\u00adtain errors. For example, A(Int,Bool)=? \nB(Int,Bool) produces the typing pattern A(B(T,.),B(.,T)). We reuse our existing ma\u00adchinery by duplicating \nTR and putting it in a choice type that will be decomposed by alternatives in the recursive execution \nof unify. Although we do not show all of the cases of unifying a quali.ed type variable against other \ntypes, we do show the trickiest case of unifying a type variable with a function type in the sixth case \nin Figure 7. There are there sub-cases to consider: (1) If vq occurs in TR, the uni.cation fails. (2) \nIf vq does not occur in TR but a more speci.c type variable vqr does, then some variants may still be \nwell-typed. So, we create a new uni.cation problem by adding a dimension D from r to the quali.cation \nof vq, then splitting the new variable vDq in the D dimension. (3) Finally, if v does not appear in any \nform in TR, then we simply map vq to TR. Note that the decomposition of the uni.cation problem is such \nthat if there is any vp in TR, then either vp = vq or vp is more speci.c than vq. Finally, we consider \nthe uni.cation of two function types. We unify the corresponding argument types and result types and \ncom\u00adpose the mappings. The resulting typing pattern is the intersection of the patterns of the two subproblems \nsince the result will be well\u00adtyped only if both the argument and result types agree. We conclude by \npresenting some important properties of the uni.cation algorithm. The .rst result is that the partial \nuni.cation algorithm is terminating through decomposition that eventually re\u00adsults in either the propagation \nof type errors, or calls to the robin\u00adson algorithm, which is terminating. There are two cases that do \nnot decompose, but rather grow the size of the types being uni\u00ad.ed, and so pose a threat to termination. \nThe .rst is the splitting of type variables. The second is the .fth case shown in Figure 7. Both of these \ncases introduce a new choice type and duplicate one of their arguments. These cases do not prevent termination, \nhow\u00adever, for two reasons. First, both cases are followed immediately by a decomposition that produces \ntwo subproblems smaller than the original problem. Second, the number of new choice types that can be \nintroduced is bounded by the overall number of dimensions in the uni.cation problem. This follows from \nthe property of choice domination and the fact that we eliminate a dimension from con\u00adsideration with \neach decomposition by alternatives. In [5], we did an in-depth time complexity analysis of the variational \nuni.cation algorithm. We showed that if the size of TL and TR are l and r respectively, then the time \ncomplexity of variational uni.cation is O(lr(l + r)). Since the computation of typing patterns in the \nuni.cation algorithm does not exceed the time for computing partial uni.ers, the run-time complexity \nis still O(lr(l + r)) for the partial uni.cation algorithm. The partial uni.cation algorithm is also \nsound and complete. These facts are expressed in the following theorems. We use unify1 to refer to the \nentire three-part uni.cation process described in Section 7.1 (quali.cation, quali.ed uni.cation, completion). \nTHEOREM 4 (Partial uni.cation is sound). Given the uni.cation problem T1 =? T2, if unify1(T1,T2)=(P,.), \nthen T1. N T2. = P. THEOREM 5 (Partial uni.cation is complete, most de.ned, and most general). Given \nthe uni.cation problem T1 =? T2, if T1. N T2. = P, then unify1(T1, T2)=(P1 ,.) such that P1 P and if \nP1= P then there exists some .1 such that . = .1. .. 8. Partial Type Inference Algorithm Although the \npartial uni.cation algorithm is quite complicated, the inference algorithm itself is simple. We de.ne \nit as an extension of algorithm W [6] and show the most interesting case below. infer : . \u00d7 G \u00d7 e . . \n\u00d7 T infer(.,G,e1 e2)= (.1,T1) . infer(.,G,e1) (.2,T2) . infer(.,G.1,e2) (P, .) . unify1(T1.2,T2.2 . a) \n{-a is a fresh variable -}R . P <a. return (. . .2 . .1,R) The algorithm takes three arguments: a dimension \nenvironment, a typing environment, and an expression. It returns a partial uni.er and the inferred partial \ntype. Traditionally, inferring the result of a function application consists of four steps: (1) infer \nthe type of the function, (2) infer the type of the argument, (3) unify the argument type of the function \nwith the type of the argument, and (4) instantiate the result type of the function with the returned \nuni.er. Our algorithm adds just one more step: we must mask the result type according to the typing pattern \nreturned by partial uni.cation, in order to introduce error types for the cases where traditional uni.cation \nwould fail. The remaining cases can be derived from the typing rules in Section 5. Variables and abstractions \nare treated as in W . For a dimension declaration we extend the dimension environment and recursively \ninfer the type of its scope. For a choice we infer the type of each alternative and build a corresponding \nchoice type.  Figure 8: Running time of prototype by error distribution. The following theorems state \nthat our type inference algorithm is sound and complete and has the principal typing property. In the \nfollowing, the symbol : represents a more-de.ned, more-general relation on variational types. That is, \nT 1: T means that for every corresponding pair of (plain) variants V 1 and V from T 1 and T , respectively, \neither V 1c V or V = .. THEOREM 6 (Type inference is sound). If infer(.,G,e)=(.,T ), then .,G. f e : \nT. THEOREM 7 (Type inference is complete and principal). If .,G. f e : T, then infer(.,G,e)=(.1 ,T 1) \nsuch that . = . . .1 for some . and T 1: T. These results mean that, for any syntactically correct VLC \nexpres\u00adsion, we can infer the most general type, containing type errors in as few variants as possible, \nall without type annotations. 9. Evaluation Variational type inference offers potentially huge ef.ciency \ngains over the brute-force strategy of typing each variant individually. The .rst opportunity is by sharing \nthe typing information of code common to multiple variants. For example, in the expression fA(e1,e2) \nwe need only type the function f once relative to e1 and e2. The second, more subtle opportunity is by \nreducing vari\u00adability in types through choice idempotency. This is often possi\u00adble because types are \nan abstraction of expressions. For example, in the expression A( f ,g) B(1,2), the argument type reduces \nto the plain type Int, reducing the variability in the application at the type level. While both of these \ncases are expected to be ubiquitous in practice, there are worst-case scenarios that fundamentally cannot \nbe typed faster than brute-force, for example, an expression with no sharing and in which every variant \nhas a different type, such as A(B(T1,T2), B(T3,T4)) where T1 ...T4 are all different. In this section \nwe empirically evaluate the ef.ciency of partial type inference in a variety of ways. To do this, we \nhave developed a prototype in Haskell that implements the contents of this paper. The prototype consists \nof three parts: a normalizer for variational types, the equational partial uni.cation algorithm described \nin Section 7, and the type inference algorithm described in Section 8. In the .rst experiment, we measure \nthe additional cost of the extensions described in this paper, relative to [5]. To measure this overhead \neffectively, we intentionally induced our worst-case performance through the cascading choice problem. \nCascading choices are long sequences of applications, where each expression is a choice in a different \ndimension. If the types of all alternatives are different, no solution can perform better than the brute-force \nstrategy. The overhead of our prototype on such examples (all well\u00adtyped, with between 14 and 21 dimensions) \nwas about 30% of the running time of the non-error-tolerant prototype described in [5]. In the second \nexperiment, we study how the distribution of er\u00adrors in an expression affects the ef.ciency of partial \ntype inference. The graph in Figure 8 shows the running time of the prototype on a cascading choice problem \nwith 21 dimensions, seeded with errors. The horizontal axis indicates the percentage of variants that \nwere seeded with type errors, and the different lines represent different distributions of these errors. \nErrors can be spread evenly through\u00adout the expression, clustered together, distributed randomly, or \nin\u00adtroduced at the end of the expression. An interesting phenomenon is that, while the running time at \n.rst increases as we introduce er\u00adrors (due to the costs of maintaining and applying error patterns), \nin three of the four curves the running time decreases sharply as the error density increases. This is \nbecause additional errors in\u00adtroduce opportunities for reduction through choice idempotency (D(., .) \n= .) that are usually denied in cascading choice expres\u00adsions. As expected, this feature is most pronounced \nwhen errors are clustered and least pronounced when they are spread evenly. When errors are introduced \nat the end of the expression, this opportunity never arises since all the work has already been done. \nFinally, in the third experiment, we demonstrate the ef.ciency and effectiveness of partial type inference \nin .nding type errors, relative to the brute-force approach (implemented as a prototype in the same way \nas our own). The results are presented in the table in Figure 9. Each row represents an arti.cially constructed \nexpression that varies in the indicated number of dimensions. The size of each expression is given by \nthe number of AST nodes. The expressions are constructed such that not all dimensions are independent \n(some dimensions are nested within choices), so the number of variants each expression represents is \nalso given. In each expression, we manually seeded the indicated number of errors according to two different \ndistributions: errors may be spread evenly throughout the expression or clustered together. Thus, each \nrow actually represents two expressions with different error distri\u00adbutions that are otherwise identical. \nErrors are counted relative to the variational expression, not the variants they occur in. For exam\u00adple, \nif the expression err produces a type error, then A(err,B(1,2))is considered to contain just one type \nerror even though that error is expressed in two variants ([A.2,B.1] and [A.2,B.2]). Finally, for each \nexpression we give the percentage of errors caught and total running time in seconds (run on a 2.8GHz \ndual core processor with 3GB RAM) of the brute-force approach and our inference algorithm, respectively. \nOften the problem is in\u00adtractable for the brute-force approach, so we cap the running-time at one hour \nand count the number of errors caught to this point. Because of this cap, and especially when errors \nare clustered, there is a potential for bias in which variants the brute-force algorithm sees before \nthe time limit is reached. To mitigate this, we ran each brute-force test 10 times, starting from random \nvariants, and aver\u00adaged the results. Note that for presentation reasons we do not list the percentage \nof errors found for our algorithm since this value is always 100%. Similarly, we do not list the running \ntime of the brute-force approach since this is the full 3600 seconds in all but a few cases, which are \nindicated by footnotes. From the results in Figure 9 we observe that our algorithm scales well as the \nsize, variability, and number of errors in an ex\u00adpression increases. Our algorithm is also more reliable \nfor detecting errors since the ability to completely type the expressions means that it is not sensitive \n(in this regard) to the distribution of errors, and we do not have to consider issues like which variant \nthe algo\u00adrithm starts with. Collectively, these results demonstrate the feasibility of error\u00adtolerant \ntype inference on large, complex expressions. In practice, Figure 9: A comparison of the performance \nof the brute-force approach and our inference algorithm on large expressions containing seeded type errors. \nThe errors are either spread evenly or clustered within the expression. Our algorithm caught 100% of \nthe errors in all cases, so we show only the time taken to do so. The running time of the brute-force \napproach was capped at one hour (3600 s). For cases that completed before this cap was reached, we give \nthe running time as a footnote. For cases that did not complete, we ran each test 10 times starting from \na random variant, and averaged the results.  size dims variants errors spread brute (%) vlc (s) clustered \nbrute (%) vlc (s) 702 3719 22 22 216 216 100 100 100a 0.62 14.90 1.09 100c 0.57 4.10 1.02 976 24 217 \n200 100b 0.71 100d 0.68 5327 24 217 200 0.50 2.26 39.85 2.17 8412 24 217 200 0.05 3.79 0.00 3.65 1163 \n27 221 400 27.05 0.76 4.90 0.71 1745 33 225 500 0.42 1.31 0.00 1.19 2079 37 229 500 0.04 1.44 2.98 1.33 \n3505 57 240 1000 0.08 1.82 0.21 1.74 9429 215 2165 1000 0.00 4.31 0.01 4.16 61345 213521 429586 1434 \n4983 10002 2892 23073 27455 2000 5000 10000 0.00 31.45 0.02 104.61 0.00 183.75 0.99 29.44 0.00 99.37 \n0.10 172.52 a 648 s b 2700 s c 639 s d 2645 s  we expect real software to be considerably less complex \n(from a variational perspective) than the expressions examined in this sec\u00adtion, and very unlikely to \ninduce worst-case scenarios. For exam\u00adple, some real-world studies have suggested an average choice nest\u00ading \ndepth of just 1.5 [13]. However, it is possible that variational complexity is arti.cially limited by \nthe inadequacy of current tools, which this work directly addresses. 10. Related Work The work presented \nhere builds on our previous work on typing variational programs [5]. In that work, we focused on establish\u00ading \nVLC as a foundation for work on typing variational programs (and other static analyses), introducing \nthe notion of variational types, and developing the fundamentals of variational typing and variational \ntype inference. In order to be practically useful, how\u00adever, variational typing must support many more \nfeatures. In [5] we demonstrated how this approach can be extended to support simple typing features \nlike sum types. The work presented in this paper represents a much more signi.cant and challenging extension \nto make the type system error-tolerant. This feature is critically impor\u00adtant for typing real-world variational \nprograms because it directly supports tasks like error location and incremental development. In general, \nour approach distinguishes itself from related work in the .eld of SPLs by representing variation more \ngenerally and at a .ner granularity, and by solving the more general problem of type inference rather \nthan the type-or de.nedness-checking of explicitly typed programs [11, 12, 25]. Choice types are similar \nto variant types [10], which are used to uniformly manipulate heterogeneous collection of types. A sig\u00adni.cant \ndifference between the two is that choices (at the expres\u00adsion level) contain all of the information \nneeded for inferring their corresponding choice type. Values of variant types, on the other hand, are \nassociated with just one label, representing one branch of the larger variant type. This makes type inference \nvery dif.cult. A common solution is to use explicit type annotations; whenever a variant value is used, \nit must be annotated with a corresponding variant type. Typing VLC does not require such annotations. \nChoice types are also reminiscent of union types [7]. A union type is an agglomeration of simpler types. \nFor example, a function f might accept the union of types Int and Bool. Function appli\u00adcation is then \nwell typed if the argument s type is an element of the union type (either Int or Bool). The biggest difference \nbetween union types and choice types is that union types are comparatively unstructured. In VLC, choices \ncan be synchronized, allowing func\u00adtions to provide different implementations for different argument \ntypes, or for different sets of functions to be de.ned in the con\u00adtext of different argument types. With \nunion types, an applied func\u00adtion must be able to operate on all possible values of an argument with \na union type. A major challenge in type inference with union types is union elimination, which is not \nsyntax directed and makes type inference intractable. Therefore, as with variant types, syntac\u00adtic markers \nare needed to support type inference. Although they share a name, our notion of partial types differs \nfrom the work of Thatte [26]. Thatte s partial types provide a way to type certain objects that are not \ntypable with simple types in lambda calculus, such as heterogeneous lists and persistent data. They are \nmore similar to our typing patterns. Thatte s untyped type O represents an arbitrary well-typed expression, \nsimilar to our T type, while his inclusion relationship on partial types (=) is similar to our more-de.ned \nrelationship on patterns ( ). Type inference with Thatte s partial types was proved decidable [14, 17], \na property that holds for our type system also. Top and bottom types in subtyping [19] are also similar \nto the types T and . used in typing patterns. Moreover, the subtyping relationship plays a similar role \nto that of on typing patterns. For example, all types are subtypes of the top type, which corresponds \nto the fact that all typing patterns are less or equally de.ned as T (similar for . and the bottom type). \nHowever, the role of these type bounds is quite different. The top and bottom types are introduced to \nfacilitate the proofs of certain properties and the design of type systems, for example, in bounded quanti.cation \n[18], whereas the T and . types are used as parts of larger patterns to track which variants are ill-typed, \nand to mask result types accordingly. Our work is also related to the work of Siek et al. on gradual \ntyping [22, 23]. The goal of that work is to integrate static and dynamic typing into a single type system. \nThey use the symbol ? to represent a type that is not known statically (that is, it is a dynamic type). \nThis is similar to our . type in partial types and typing patterns, particularly in the way it is used \nto determine a notion of informativeness. A type is less informative if it contains more ? types (or \nrather, if more of the type is subsumed by ? types). This relation is similar to an inverse of our more-de.ned \nrelation on typing patterns, where a pattern becomes less de.ned as it is subsumed by . types. The biggest \ndifference between this work and our own is that . types represent parts of a variational program that \nare statically known to be type incorrect, whereas the parts of a program annotated with ? types may \nstill be dynamically type correct. Also, their system isolates ? types as much as possible with respect \nto a plain type, while we allow . types to propagate outward in plain types, but contain . types to as \nfew variants as possible. This is best demonstrated by the fact that (if we extend the notion of de.nedness \nto partial types) . is equally de.ned as .. Int, but ? is strictly less informative than ? . Int.  Generating \ninformative error messages and determining the causes and locations of type errors has been extensively \nstudied in type systems [15, 28]. Our type system does not address this problem per se, as far as individual \nprogram variants is concerned. However, a partial type does indicate which variants contain type errors. \nThis information can be combined with traditional, single\u00advariant systems to improve error location in \nvariational programs. The uni.cation problem for equational theories that contain distributivity and \nassociativity is known to be undecidable [27]. However, it is decidable when an idempotency law is added \n[1]. Therefore, because of choice idempotency, our uni.cation problem is decidable. As we have shown, \nour problem is also unitary. This is important for implementing the type inference algorithm because \nit is necessary for a type system that has the principal typing property. 11. Conclusion and Future Work \nWe have presented a type system and inference algorithm for as\u00adsigning partial types to variational programs. \nWe have shown that the addition of error types and the resulting more-de.ned ordering for types integrates \nwell with a variational type system. Speci.\u00adcally, we were able to extend the uni.cation and type inference \nalgorithms to produce most-general partial types for variational lambda calculus expressions. These results \nare an important step to\u00adward providing type-system support for massively variational soft\u00adware, and \nfor the incremental development of variational programs. In future work we plan to explore incremental \nvariational type inference. We expect that programmers often work on only a small subset of variants \nat a time, and so there is huge opportunity for ef.\u00adciency gains by reusing the unchanged variational \ncontext in typing incremental changes. We expect this historical information to also be useful for producing \nmore precise error feedback. We also plan to investigate how variational typing can be used to support \nstrong but .exible typing in functional, staged programming languages. Acknowledgments This work is supported \nby the Air Force Of.ce of Scienti.c Re\u00adsearch under the grant FA9550-09-1-0229 and by the National Sci\u00adence \nFoundation under the grant CCF-0917092. References [1] S. Anantharaman, P. Narendran, and M. Rusinowitch. \nUni.cation Modulo ACUI Plus Homomorphisms/Distributivity. Journal of Au\u00adtomated Reasoning, 33:1 28, 2004. \n[2] S. Apel and C. K\u00a8astner. An Overview of Feature-Oriented Software Development. Journal of Object \nTechnology, 8(5):49 84, 2009. [3] F. Baader and W. Snyder. Uni.cation Theory. In A. Robinson and A. Voronkov, \neditors, Handbook of Automated Reasoning, chapter 8, pages 445 533. Elsevier Science Publishers, Amsterdam, \nNL, 2001. [4] D. Batory, J. N. Sarvela, and A. Rauschmayer. Scaling Step-Wise Re.nement. IEEE Trans. \non Software Engineering, 30(6):355 371, 2004. [5] S. Chen, M. Erwig, and E. Walkingshaw. Extending Type \nInference to Variational Programs. Technical Report, School of EECS, Oregon State University, 2012. Available \nat: http://eecs.oregonstate. edu/~erwig/ToSC/VLC-TypeSystem.pdf. [6] L. Damas and R. Milner. Principal \nType Schemes for Functional Pro\u00adgramming Languages. In 9th ACM Symp. on Principles of Program\u00adming Languages, \npages 207 208, 1982. [7] M. Dezani-Ciancaglini, S. Ghilezan, and B. Venneri. The Relevance of Intersection \nand Union Types. Notre Dame Journal of Formal Logic, 38(2):246 269, 1997. [8] M. Erwig and E. Walkingshaw. \nThe Choice Calculus: A Representa\u00adtion for Software Variation. ACM Trans. on Software Engineering and \nMethodology, 21(1):6:1 6:27, 2011. [9] GHC. The Glasgow Haskell Compiler. http://haskell.org/ghc. [10] \nK. Kagawa. Polymorphic Variants in Haskell. In ACM SIGPLAN Workshop on Haskell, pages 37 47. ACM, 2006. \n[11] C. K\u00a8astner, S. Apel, T. Th\u00a8um, and G. Saake. Type Checking Annotation-Based Product Lines. ACM \nTrans. on Software Engineer\u00ading and Methodology, 2012. To appear. [12] A. Kenner, C. K\u00a8astner, S. Haase, \nand T. Leich. TypeChef: Toward Type Checking #ifdef Variability in C. In Int. Workshop on Feature-Oriented \nSoftware Development, pages 25 32, 2010. [13] C. H. P. Kim, C. K\u00a8astner, and D. Batory. On the Modularity \nof Feature Interactions. In Int. Conf. on Generative Programming and Component Engineering, pages 19 \n23, 2008. [14] D. Kozen, J. Palsberg, and M. I. Schwartzbach. Ef.cient Inference of Partial Types. In \nJournal of Computer and System Sciences, pages 363 371, 1992. [15] B. S. Lerner, M. Flower, D. Grossman, \nand C. Chambers. Searching for Type-Error Messages. In ACM Conf. on Programming Language Design and Implementation, \npages 425 434, 2007. [16] M. Mezini and K. Ostermann. Variability Management with Feature-Oriented Programming \nand Aspects. ACM SIGSOFT Software Engi\u00adneering Notes, 29(6):127 136, 2004. [17] P. M. O Keefe and M. \nWand. Type Inference for Partial Types is Dec\u00adidable. In European Symp. on Programming, pages 408 417, \n1992. [18] B. C. Pierce. Bounded Quanti.cation with Bottom. Technical report, Computer Science Department, \nIndiana University, 1997. [19] B. C. Pierce. Types and Programming Languages. MIT Press, Cam\u00adbridge, \nMA, 2002. [20] K. Pohl, G. B\u00a8ockle, and F. van der Linden. Software Product Line Engineering: Foundations, \nPrinciples, and Techniques. Springer-Verlang, Berlin Heidelberg, 2005. [21] J. A. Robinson. A Machine-Oriented \nLogic Based on the Resolution Principle. Journal of the ACM, 12(1):23 41, 1965. [22] J. G. Siek and W. \nTaha. Gradual Typing for Functional Languages. In Scheme and Functional Programming Workshop, pages 81 \n92, 2006. [23] J. G. Siek and M. Vachharajani. Gradual Typing with Uni.cation-Based Inference. In Symp. \non Dynamic Languages, pages 7:1 7:12, 2008. [24] W. Taha and T. Sheard. MetaML and Multi-Stage Programming \nwith Explicit Annotations. Theoretical Computer Science, 248(1 2):211 242, 2000. [25] S. Thaker, D. Batory, \nD. Kitchin, and W. Cook. Safe Composition of Product Lines. In Int. Conf. on Generative Programming and \nComponent Engineering, pages 95 104, 2007. [26] S. Thatte. Type Inference with Partial Types. In Int. \nColloq. on Automata, Languages and Programming, pages 615 629, 1988. [27] E. Tiden and S. Arnborg. Uni.cation \nProblems with One-Sided Dist\u00adributivity. Journal of Symbolic Computation, 3(1-2):183 202, 1987. [28] \nM. Wand. Finding the Source of Type Errors. In ACM Symp. on Principles of Programming Languages, pages \n38 43, 1986.    \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Conditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the brute-force strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating ill-typed variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the brute-force approach, and to explore the effects of various error distributions on the inference process.</p>", "authors": [{"name": "Sheng Chen", "author_profile_id": "81548019885", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P3804300", "email_address": "chensh@eecs.oregonstate.edu", "orcid_id": ""}, {"name": "Martin Erwig", "author_profile_id": "81100586223", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P3804301", "email_address": "erwig@eecs.oregonstate.edu", "orcid_id": ""}, {"name": "Eric Walkingshaw", "author_profile_id": "81435610905", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P3804302", "email_address": "walkiner@eecs.oregonstate.edu", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364535", "year": "2012", "article_id": "2364535", "conference": "ICFP", "title": "An error-tolerant type system for variational lambda calculus", "url": "http://dl.acm.org/citation.cfm?id=2364535"}