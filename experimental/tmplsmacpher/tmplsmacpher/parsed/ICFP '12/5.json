{"article_publication_date": "09-09-2012", "fulltext": "\n Super.cially Substructural Types Neelakantan R. Krishnaswami Aaron Turon Derek Dreyer Deepak Garg MPI-SWS \nNortheastern University MPI-SWS MPI-SWS neelk@mpi-sws.org turon@ccs.neu.edu dreyer@mpi-sws.org dg@mpi-sws.org \n Abstract Many substructural type systems have been proposed for control\u00adling access to shared state \nin higher-order languages. Central to these systems is the notion of a resource, which may be split into \ndisjoint pieces that different parts of a program can manipulate in\u00addependently without worrying about \ninterfering with one another. Some systems support a logical notion of resource (such as permis\u00adsions), \nunder which two resources may be considered disjoint even if they govern the same piece of state. However, \nin nearly all ex\u00adisting systems, the notions of resource and disjointness are .xed at the outset, baked \ninto the model of the language, and fairly coarse\u00adgrained in the kinds of sharing they enable. In this \npaper, inspired by recent work on .ctional disjointness in separation logic, we propose a simple and \n.exible way of en\u00adabling any module in a program to create its own custom type of splittable resource \n(represented as a commutative monoid), thus providing .ne-grained control over how the module s private \nstate is shared with its clients. This functionality can be incorporated into an otherwise standard substructural \ntype system by means of a new typing rule we call the sharing rule, whose soundness we prove semantically \nvia a novel resource-oriented Kripke logical relation. Categories and Subject Descriptors D.3.1 [Programming \nLan\u00adguages]: Formal De.nitions and Theory; D.3.3 [Programming Languages]: Language Constructs and Features \nAbstract data types; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs General Terms \nLanguages, Design, Theory, Veri.cation Keywords Substructural type systems, separation logic, sharing \nrule, commutative monoids, .ctional disjointness, ADTs, hidden state, dependent types, capabilities, \nKripke logical relations 1. Introduction Over the past decade, many substructural type systems based \npri\u00admarily on variants of linear logic [20] and separation logic [34] have been proposed as a means of \nverifying critical semantic prop\u00aderties of higher-order stateful programs, ranging from basic mem\u00adory \nsafety to full functional correctness. These type systems and their key substructural elements go by \na variety of names e.g., typestate [38, 11], uniqueness [8], regions [39], capabilities [42], Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 12, September \n9 15, 2012, Copenhagen, Denmark. Copyright &#38;#169; 2012 ACM 978-1-4503-1054-3/12/09 $15.00. Hoare \ntypes [27] but one thing they all have in common is that they give programmers the ability to reason \nlocally about the ef\u00adfects of their code on the state of shared resources. The essence of this local \nreasoning is captured by the frame property: if an operation f consumes a resource satisfying the type \nor assertion A and produces one satisfying B,then f can also be seen to transform A.C to B .C,where C \nis an arbitrary frame representing assumptions about the greater ambient environment in which f is executed. \nThe . here denotes multiplicative (or sep\u00adarating ) conjunction, which ensures that the resource satisfying \nA . C can be split into disjoint pieces satisfying A and C,re\u00adspectively; since f only consumes the resource \nsatisfying A,itis guaranteed to leave the resource satisfying C untouched. As this discussion suggests, \na central element in substructural type systems is the notion of a resource, as well as the ability to \nsplit a resource into disjoint pieces. A resource, in essence, describes (1) the knowledge that the consumer \nof that resource has about the machine state, and (2) what rights they have to change the state. Two \nresources are then considered disjoint if they do not interfere with each other, that is: any operation \npermitted by the rights of one resource should not violate the knowledge of the other. In some substructural \ntype systems, such as those based on sep\u00adaration logic, resources take the form of entities such as heaps \nthat enjoy an immediate physical interpretation of disjointness. When an operation consumes a heap h, \nit has full access to h as a physical object: it knows what h is and has the right to mod\u00adify it as it \npleases. In other systems, resources take the form of permissions or capabilities , which are strictly \nlogical descrip\u00adtions of the knowledge and rights concerning some shared state. In particular, two logical \nresources may be considered disjoint even if they govern the same piece of state. For instance, fractional \npermissions [7] enable the full permission to a memory location (x .. v) which gives its consumer the \nknowledge that x cur\u00adrently points to v and the right to update x s contents to be split .5 .5 into two \nhalf permissions (x .. . v . x . v) which provide their respective consumers with the knowledge that \nx points to v but not the right to update it. These half permissions are logically disjoint because they \nensure that neither consumer can violate the other s knowledge that x points to v. However, in nearly \nall existing systems, the notions of resource and disjointness are .xed at the outset, baked into the \nmodel of the language, and fairly coarse-grained in the kinds of sharing they enable. This is unfortunate: \nideally, we would like to have a way of de.ning more .ne-grained custom logical notions of resource and \ndisjointness on a per-module or per-library basis. 1.1 Motivating Example: A Memory Manager To take \na concrete example, consider a module M implementing an explicit memory manager. M will of course maintain \nsome private data structure representing its free list, and it will expect a certain invariant A of that \ndata structure to hold whenever its methods are invoked. If M is simple enough that this invariant is \nthe only constraint needed on its methods, we can give it an interface like:  malloc : A -.X : Loc. \nptr X . cap X 1 . A free : .X : Loc. ptr X . cap X 1 . A -A Here, ptr X is a singleton type inhabited \nonly by the pointer X,and cap X 1 represents the knowledge that X points to a value of unit type 1, along \nwith the full capability to modify it. The invariant A is threaded through the pre-and post-conditions \nof the operations, but in some type systems it could even be hidden entirely [32]. However, the above \ninterface would not work for a more real\u00adistic memory manager that required the client to free only memory \npreviously allocated through the manager. For instance, in the Ver\u00adsion 7 Unix memory manager veri.ed \n(and debugged) recently by Wickerson et al. [43] the implementation internally maintains a chain of pointers \nto the cells preceding contiguous blocks of memory, both free and allocated. In order to preserve its \ninvariant that the blocks it maintains are contiguous, the manager must only permit its client to free \na block that the manager knows about (has marked as allocated in its internal chain). The type of the \nfree operation must therefore make the set of allocated blocks explicit, so that it can require the freed \nlocation to belong to that set. We can achieve this by parameterizing the manager s invariant A over \nthe set of allocated locations L, and revising its interface as follows: malloc : .L : LocSet.A(L) \u00ad \nfree : .X : Loc. ptr X . cap X 1 . A(L l{X}) .L : LocSet. .X : Loc. ptr X . cap X 1 . A(L l{X}) -A(L) \n Unfortunately, this latter interface is problematic if the memory manager is used by multiple client \nmodules that one would like to typecheck/verify independently. Each client module only really cares about \nthe locations that it allocates/frees, but because the global state of the memory manager i.e., the full \nset of allocated locations L is made explicit in the type A(L), each client will in fact be sensitive \nto interference from other clients. Consequently, each client will need to pollute its own interface \nwith explicit information about how it affects this global state, thereby leaking implementation details \nin the process. Ideally, we would like a way of giving each client its own local view of the global state. \nA simple way to provide such a local view would be to allow the memory allocator s invariant to somehow \nbe split up into (and reconstituted from) logically disjoint pieces: split : .L1,L2 : LocSet.A(L1 l L2) \n-A(L1) . A(L2) join : .L1,L2 : LocSet.A(L1) . A(L2) -A(L1 l L2) In particular, given A(L) for some initial \nL, we could use split to generate any number of copies of A(\u00d8), each of which could be passed to a separate \nclient, thus rendering each client completely oblivious to the existence of the others. Intuitively, \nthe splitting provided by the split operation is per\u00adfectly safe because L1 and L2 are disjoint sets, \nand so the only right granted to the owner of A(L1) namely, the right to free the loca\u00adtions in L1 cannot \npossibly violate the knowledge of the owner of A(L2) namely, that the locations in L2 are allocated. \nClearly, though, if we can support such split and join operations, then A(L) no longer means what it \ndid previously: rather than asserting that L is the global set of allocated locations, it now asserts \nmerely that L is some subset of them that the owner of A(L) has the right to free. In other words, we \nare treating sets of locations as a kind of split\u00adtable resource, and we are using this custom resource \nto control the knowledge and rights that any one client module has concerning the global, shared state \nof the memory manager. The question is: how can we put this intuition on a sound and .exible formal footing, \nthus enabling any module to develop its own custom notion of splittable resource in a safe, principled \nway?  1.2 Commutative Monoids to the Rescue! The goal of this paper is to show that the above example \nis but one instance of a simple and general pattern, and that there is a simple and general way of supporting \nsuch custom resource management within an otherwise standard substructural type system. The basic idea \nbehind our approach is inspired by some very recent work on separation logic, speci.cally Jensen and \nBirkedal s .ctional separation logic [21], Dinsdale-Young et al. s views [12], and Ley-Wild and Nanevski \ns subjective concurrent separation logic [25]. Although these developments are all motivated by dif\u00adferent \nconcerns (to be described in Section 5), a common thread running through them is the idea of accounting \nfor various custom notions of splittable resource along with their attendant notions of knowledge and \nrights in terms of commutative monoids. A commutative monoid is a set S equipped with a commutative, \nassociative composition operator (\u00b7): S \u00d7 S . S,and a unit element E . S such that .x . S. x \u00b7 E = x. \nIf one can cast one s notion of custom resource as a commutative monoid, then one can view the global, \nshared state as the composition rL \u00b7 rF of one s local resource rL with the resource rF of one s frame \n(i.e., one s environment). Owning the local rL gives one the knowledge that the global state must be \nsome extension of rL (i.e., it must equal rL \u00b7 rF for some frame resource rF ). It also gives one the \nright to update the global state however one likes, so long as the new global state satis.es rL \u00b7 rF \nfor some rL. In other words, one may change one s local resource to an arbitrary rL, so long as the change \nis frame-respecting, i.e., it leaves the frame resource rF alone. The notion of logical resource that \nwe suggested for the memory manager module in Section 1.1 is expressible very naturally as a commutative \nmonoid: sets of locations, with composition de.ned as disjoint union (l)and E = \u00d8. Furthermore, the malloc \nand free operations are both frame-respecting, due to their universal quanti.cation over the framing \nlocation set L. But many other notions of splittable resource are instances of commutative monoids as \nwell. In their work on .ctional separation logic (FSL) [21], Jensen and Birkedal thus propose a way of \nal\u00adlowing different modules in a program to specify their interfaces in terms of assertions such as A(L) \nin Section 1.1 about different, module-speci.c notions of resource, encoded as different commu\u00adtative \nmonoids. This ability to encode module-speci.c protocols governing shared state was in fact already present \nto a large extent in earlier work on deny-guarantee reasoning [15] and concurrent abstract predicates \n[13]; the main selling point of FSL in compar\u00adison is that it adopts a simpler and more abstract monoidal \nview of resource that is not bound up with concurrency-related concerns. (For a more detailed analysis, \nsee Section 5.) In this paper, we show how to lift the ideas of FSL and its predecessors from the .rst-order \nsetting of separation logic to the higher-order setting of a substructural type system.  1.3 Contributions \nWe make two main contributions: one syntactic, the other semantic. Our syntactic contribution is to propose \na new typing rule, which we call the sharing rule, that gives the author of a module .ne-grained control \nover how the module s private linear resources (e.g., the full capability to access its internal data \nstructures) are shared with its clients. In particular, as witnessed in our motivating example, the sharing \nrule allows the capabilities to access this shared state to be split (by .) into pieces that are logically \ndisjoint according to a custom commutative monoid of one s choosing. This means that our type system \nis only super.cially substructural: under the hood, the A and B in A . B may both be capabilities to \nread and write the very same shared state, albeit in ways that are guaranteed not to interfere with each \nother.  We present the sharing rule in the context of a fairly standard af.ne type system (Section 2), \nsupporting a combination of features from Dependent ML [45] and L3 [5]. While this language is not as \nexpressive as, say, Hoare Type Theory [27] which we would eventually like to target as well it is nevertheless \nrich enough to encode interesting examples (Section 3), while simple enough to focus our attention on \nthe sharing rule itself. Compared with the support for custom monoids in FSL, our sharing rule is more \n.exible because it enables types indexed by different commutative monoids to be freely intermingled using \nthe language s general-purpose . type. In contrast, FSL s indirect Hoare triples must be indexed explicitly \nby a particular monoid, and composing speci.cations that are indexed by different monoids requires additional, \nsomewhat inconvenient, machinery. However, in the face of arbitrary higher-order programs, our implementation \nof the sharing rule necessarily carries a dynamic cost, namely the use of a lock to protect updates to \nthe shared state from unsafe re-entrancy. We do not believe this imposes a serious practical restriction \non the use of the sharing rule in our sequential setting, but it is clear that a better approach is needed \nif we wish to scale to the concurrent setting. For special cases of the rule e.g., where the primitive \noperations on the shared state do not invoke unknown functions it is possible to show that locking is \nnot needed, but we leave a thorough examination of such optimizations to future work. We discuss another, \nmore complicated but potentially more scalable approach in Section 5. Our semantic contribution is a \nnovel step-indexed Kripke logical-relations model of shared state, which facilitates a clean semantic \nproof of the soundness of our sharing rule (Section 4). The structure of our Kripke model directly re.ects \nthe intuition behind the sharing rule. In particular, its possible worlds W which encode representation \ninvariants on shared state take the form of tuples of commutative monoids (think: one monoid for each \napplication of the sharing rule). Associated with each monoid is a resource predicate that says how to \ninterpret an element of the monoid as an invariant on some underlying resources e.g., in our motivating \nexample, how the full set of allocated locations L maps to an invariant on the memory manager s internal \nstate. Crucially, this resource predicate may describe invariants not only on the physical heap, but \nalso on logical resources (expressed as a tuple of elements of all the monoids in W ), thus enabling \napplications of the sharing rule to be soundly layered on top of one another. We conclude the paper with \na detailed comparison to related work (Section 5) and a discussion of future work (Section 6).  2. \nThe Core Language Our core calculus is an implicitly-typed version of af.ne F.,ex\u00adtended with domains \nthat index types. We call these domains sorts and their elements index terms. With a suf.ciently rich \nlanguage of index terms, and propositions and type-level quanti.cation over them, we retain much of the \n.exibility of dependent types for giv\u00ading rich type-based speci.cations for programs, without requiring \nthe index terms to coincide with program terms, thus avoiding the problematic issue of af.ne variable \noccurrences in types. Figure 1 lists the syntactic forms of the language, including sorts, index terms, \npropositions over index terms, kinds, types, terms, contexts (for the static semantics) and heaps (for \nthe dy\u00adnamic semantics). Judgments for checking well-formedness of kinds, index terms, propositions and \ncontexts, as well as logical inference, type equality and typing are listed in Figure 2, but we elide \nthe standard rules for inferring these judgments. Sorts, Index Terms, and Propositions Sorts, ranged \nover by the metavariable s, include mathematical domains such as natu\u00adral numbers, tuples, functions, \nlocations and sets, denoted by the Sorts s ::= N | 1 | s \u00d7 s | 2 | Loc | s . s | s. | seq s |P(s) | Q \n| ... Index Terms t ::= X | n | tt | . | ... Propositions P, Q ::= T| P . Q | P . Q |.| P . Q |.X : s. \nP |.X : s. P | t = u | t>u | ... Kinds . ::= .| s . . Types A ::= 1 | A . B | A -B | !A | ptr t | cap \ntA |.a : .. A |.a : .. A |.X : s :: P. A |.X : s :: P. A | | bool t | nat t | [A] | if(t, A, B) | a | \n.X : s. A | At Terms e ::= x |()|(e, e )| let (x, y) = e in e | .x. e | ee | !v | let !x = e in e | new(e) \n| getl e | e :=ll e | tt | . | if(e, e1,e2) | n | case(e, 0 . e1, s x . e2) | .x f(x).e | share(e, vi) \n| ee Eval E ::= [] |(E, e)|(v, E)| Contexts | let (x, y) = E in e | Ee | vE | !E | let !x = E in e | \nnew(E) | getE | getE v | E :=l e ee | v :=l E | v :=E v | if(E, e, e ) e | case(E, 0 . e1, s x . e2) \n| share(E, vi) Values v ::= () | (v, v )| .x. e | !v | \u00a3 | .x f(x).e | n | tt | . | | x Heaps h ::= \u00b7| \nh, \u00a3 : v Contexts Index/Type S ::= \u00b7| S,a : . | S,X : s Proposition . ::= \u00b7| .,P Unrestricted G ::= \u00b7| \nG,x : A Af.ne . ::= \u00b7| .,x : A Combined O ::= S;.;G;. Figure 1. Syntax S f A : . Well-kindedness S > \nt : s Well-sortedness S > P : prop Well-formedness of propositions S f . ok Well-formedness of propositional \ncontext S f G ok Well-formedness of hypothetical context S; . f P Logical entailment S; . f A = B : . \nType constructor equality O f e : A Well-typedness  Figure 2. Judgments metavariable t. Sorts are interpreted \nas plain mathematical sets and new sorts can be added if needed. For precise speci.cation of prop\u00aderties \nof index terms, we allow propositions of .rst-order logic over the index domains to appear in our types. \nThe standard judgment S; . f P means that P can be inferred from the assumptions in ., for all instances \nof the free variables in S. Types and Terms We use a standard af.ne type system, whose rules are shown \nin Figures 4 and 5. The natural presentation of typing has four contexts; to increase the legibility \nof the rules, we abbreviate these with a single symbol O, and de.ne notations for adding hypotheses and \nmerging contexts in Figure 3. As expected, the unit term () :1 types in a context with any set of resources \n.; the typing rule for (e1,e2) : A . B splits its  O,x : A = S;.;G;.,x : A if O=S; .; G; . O,x :! A \n= S;.;G,x : A;. if O=S; .; G; . O,a : . = S,a : .;.;G;. if O=S; .; G; . O,X : s = S,X : s;.;G;. if O=S; \n.; G; . O,P = S; .,P ;G;. if O=S; .; G; . O1, O2 = S;.;G;.1, .2 if O1 = S;.;G;.1 O2 = S;.;G;.2 Figure \n3. Context Manipulation Operations O f e : A x : A . G S; .; G; . f x : A x : A . . S; .; G; . f x : \nA O f() :1 O f v : A O f :[A] O1 f e1 : A O2 f e2 : B O1, O2 f(e1,e2) : A . B O1 f e : A . B O2,x : A, \ny : B f e : C O1, O2 f let (x, y) = e in e : C O,x : A f e : B O1 f e : A -B O2 f e : A O f .x. e : \nA -B O1, O2 f ee : B S; .; G; \u00b7f v : A O1 f e :!A O2,x :! A f e : C S; .; G; . f !v :!A O1, O2 f let \n!x = e in e : C O f e : A O f new(e): .X : Loc :: T. !ptr X . cap XA O f e : ptr t O f e : cap tA O, \nO f getl e : A . cap t 1 e O1 f e : ptr t O2 f e : A O3 f e : cap t 1 O1, O2, O3 f e :=ell e : cap tA \nS; .; G,f : A -B; x : A f e : B S; .; G; \u00b7f .x f(x).e : A -B O f tt : bool tt O f . : bool . O f n : \nnat n O f e : bool t O ,t = tt f e1 : C O ,t = . f e2 : C O, O f if(e, e1,e2): C O f e : nat t O ,t \n=0 f e1 : C O ,X : N,t = s X, x : nat X f e2 : C O, O f case(e, 0 . e1, s x . e2): C Figure 4. Typing \nRules resource context . into two disjoint parts for checking subterms e1 and e2; and, to type the af.ne \nfunction .x. e : A -B, we add the hypothesis x : A to the af.ne context to check the body e. The exponential \n!A is subject to a value restriction we can type terms !v at type !A only when v is a value. The intuition \nfor this restriction is that (following the standard af.ne interpretation) a term of type !A is duplicable, \nso the value it evaluates to must not depend on af.ne resources. If v were not a value, then its evaluation \ncould create new af.ne resources on which its result depended (e.g., the evaluation might allocate fresh \nmemory and return it). The base types bool t and nat u are singleton types, indexed by the Boolean sort \n2 and the natural number sort N, respectively. So, O f e : A O,a : . f v : B O,X : s, P f v : A O f v \n: .a : .. B O f v : .X : s :: P. A O f e : .X : s :: P. A O=S;.;G;. S > t : s S; . f [t/X]P O f e :[t/X]A \n O f e : .a : .. B O=S;.;G;. S f A : . O f e :[A/a]B O f e :[t/X]A O=S;.;G;. S > t : s S; . f [t/X]P \n O f e : .X : s :: P. A O f v : .X : s :: P. A O ,X : s, P, x : A f e : C X,x . FV(C) O, O f [v/x]e \n: C O=S;.;G;. S f A : . S,a : . f B : . O f e :[A/a]B O f e : .a : .. B O f v : .a : .. B O ,a : ., \nx : B f e : Ca . FV(C) O, O f [v/x]e : C O=S;.;G;. O f e : A S; . f A = B : . O f e : B O=S;.;G;. S; \n. f P . Q O,P f e : A O,Q f e : A O f e : A O=S;.;G;. S; . f.X : s. P O,X : s, P f e : A S f A : . \n O f e : A O=S;.;G;. S;. f. S f A : . O f e : A Figure 5. Typing Rules, Continued for example, the only \nvalue of type bool tt is tt and the only value of type nat 17 is 17. For access to shared memory, we \nintroduce the singleton type ptr f. A term of type ptr f evaluates to the location f. Additionally, we \nhave a capability type cap fA, which represents the permission to dereference the pointer f and obtain \na value of type A. Intuitively, f : ptr f is a freely duplicable pointer, which can be shared, but the \ncapability to use the pointer, of type cap fA, is af.ne, and can be shared only in a controlled manner \nusing our sharing rule. Since cap fA only represents a capability, the actual value of type cap fA is \ncomputationally irrelevant, and we write it as . The two types ptr t and cap tA are tied to each other \nby the typing rules for reading and writing memory. For example, the getel e operation (see Figure 4) \ndereferences a pointer e of type ptr t, but it requires the capability e of type cap tA.It returns a \npair of type (cap t 1) . A. The operational semantics of get f (Figure 6), removes the current value \nof f from the store, and replaces it with the value (). (It cannot also leave the contents of the pointer \nin place since doing so would violate any af.ne constraints on the contents. However, such behaviour \nis encodable for references containing a !A, which is duplicable.) The write  (h; let (x1,x2) = (v1,v2) \nin e) '.(h;[v1/x1,v2/x2]e) (h;(.x. e) v) '.(h;[v/x]e) (h; let !x =!v in e) '.(h;[v/x]e) (h; new(v)) '.(h \nl [\u00a3 : v]; (!\u00a3, )) (h l [\u00a3 : v]; get \u00a3) '.(h l [\u00a3 : ()]; (v, )) (h l [\u00a3 : ()]; \u00a3 := v) '.(h l [\u00a3 : v]; \n) (h;(.x f(x).e) v) '.(h;[.x f(x). e/f, v/x]e) (h; if(tt,e,e )) '.(h; e) (h; if(.,e,e )) '.(h; e ) (h; \ncase(0, 0 . e, s x . e )) '.(h; e) (h; case(s v, 0 . e, s x . e )) '.(h;[v/x]e ) (h; share(v, vi)) '.(h \nl [\u00a3 : .]; ( , !opi, !split, !join, !promote))where opi = .x. let (.ag, ) = get \u00a3 in let = \u00a3 := tt in \nif .ag then (.x f(x).f x) ()else let y = vi x in let = \u00a3 := .in y split = .x. ( , ) join = .x. promote \n= .x. ! () (h; e) '. h ; e () (h; E[e]) '. h ; E[e ] Figure 6. Operational Semantics operation f := v \ntakes a pointer f of type ptr f, new contents v , and a capability of type cap X 1. We generalize the \nidea of computational irrelevance by intro\u00adducing the irrelevant type [A], which is inhabited by the \ndummy value if there is some value inhabiting A (see the typing rule for [A] in Figure 4). The type \n[A] is employed gainfully in our shar\u00ading rule (Section 3). Our semantic model validates several equiv\u00adalances \non irrelevant types, including [cap tA] = cap tA and [A . B] = [B . A], which we use freely in our examples. \nPropositions over index domains are embedded in the type sys\u00adtem at quanti.ed types .X : s :: P. A and \n.X : s :: P. A.In\u00adtuitively, e : .X : s :: P. A means that for all terms t of sort s satisfying the proposition \nP , e has the type [t/X]A. The type .X : s :: P. A has the dual meaning. We also include an incon\u00adsistency \nrule (the last rule in Figure 5): if the propositional context . is inconsistent (derives false), then \nany term is well-typed in .. (The two prior rules give the rules for existentials and disjunctions.) \nIn addition, we also include type-level computation with indices with the if(t, A, B) type, which is \nequal to A if t is true, and B if t is false. There are no explicit introduction or elimination forms \nfor this type; we simply make use of the equality judgment. To assist in this, the typing for the term-level \nif-then-else construct adds the appropriate equality hypotheses about its index argument in the branches \nof the conditional. (Similar rules apply for the other index domains, but we suppress them for space \nreasons.) Kinds, ., in our language have the forms . (af.ne types) and s . . (dependent types). We include \ntype-level lambda\u00adabstraction .X : s. A, type-level application At and the universal and existential \npolymorphic types .a : .. A and .a : .. A.We could also include type constructor polymorphism, but we \nomit it for simplicity. We need a value restriction for all quanti.ed types because quanti.ers are implicitly \nintroduced and eliminated, and do not delay evaluation (unlike in explicit System F). Our choice of maximal \nimplicitness naturally makes typecheck\u00ading undecidable. It should be routine to add enough type and proof \nannotations to make typechecking decidable, and we chose the im\u00adplicit style both to make our examples \nmore readable, and to re\u00adduce the number of clauses in the term syntax. On the whole, our language is \na relatively conservative integration of the ideas of De\u00adpendent ML [45, 18] into an af.ne language with \na type structure S f A : s .. S; .; G; . f e :[At] S; . f monoids(E, (\u00b7)) .i. S; .; G; \u00b7f vi :[A/a]speci \nS; .; G; . f share(e, vi): .a : s ... [at] . !speci . !splitT . !joinT . !promoteT where speci = .X : \ns. .Y : s:: Pi.Bi . [a (ti \u00b7 X)] \u00ad i .Z : si :: Qi.Ci . [a (ti \u00b7 X)] where X, a . FV(Pi,Qi,Bi,Ci,ti,ti) \nsplitT = .X, Y : s. [a (X \u00b7 Y )] -[aX] . [aY ] joinT = .X, Y : s. [aX] . [aY ] -[a (X \u00b7 Y )] promoteT \n= .X : s :: X = X \u00b7 X. [aX] -![aX] monoids(E, (\u00b7)) = .X : s. E \u00b7 X = X . .X, Y : s. X \u00b7 Y = Y \u00b7 X . .X, \nY, Z : s. (X \u00b7 Y ) \u00b7 Z = X \u00b7 (Y \u00b7 Z) Figure 7. The Sharing Rule resembling that of L3 [5]. The primary \nnovelty in our language is encapsulated in the sharing rule, which lets us put user-de.ned log\u00adical resources \non a .rst-class footing. We describe this rule and its applications in the following section.  3. The \nSharing Rule A purely af.ne type discipline is too restrictive for most programs. In this section, we \ndescribe the sharing rule, our method for in\u00adtroducing controlled aliasing into an af.ne language. The \nintuition behind this rule is that if a library has a particular programmer\u00adde.ned notion of resource, \nand if all the operations the programmer exposes in the interface respect the frame property for that \nresource, then we can treat the library s concept of resource separation as an instance of our ambient \nnotion of separation: the tensor product. Concretely, suppose that we have a type A : s ..,repre\u00adsenting \nan af.ne capability indexed by a monoid s, along with an operation f : .X : s. A(Y1 \u00b7 X) -A(Y2 \u00b7 X). \nThe type of f as\u00adserts that it can take the (logical) resource Y1 to Y2,and thatin so doing, it preserves \nthe frame X. If we knew that f were the only operation transforming capabilities of the form A(t), then \nit would follow that we could split a capability A(X \u00b7 Y ) into two parts A(X) . A(Y ), and manipulate \nthem independently, since the only operation transforming capabilities of the form A(t) is f ,and f is \nparametric in the frame. By taking a value of type A(X) and using it to construct a new abstract type, \non which only frame-respecting operations are allowed, we can safely share an af.ne capability. The sharing \nrule, given in Figure 7, formalizes this idea. We assert the existence of a type constructor A : s ..,where \ns is a commutative monoid, and an initial resource e :[At], together with a family of frame-respecting, \nstate-passing operations vi, which take in an argument of type Bi and a state of type [A(ti \u00b7 X)], and \nreturn a result of type Ci and a state [A(ti \u00b7 X)].1 The full type of vi includes additional index quanti.cations, \nwhich are useful for asserting propositions that connect the input and initial state or output and .nal \nstate; in our examples, we suppress unused elements of this general type whenever we do not use them. \nThe sharing operator returns a new existential type, exporting the vi operations together with split, \njoin and promote operations. Splitting and joining allow treating the monoidal composition as a tensor \nproduct. The promote operator takes any resource value 1 Note that all the types of the form A(t) are \nin proof-irrelevance brackets this ensures that e represents a logical capability with no dynamic content, \nwhich turns out to be useful in the proof of soundness (Section 4). That said, it is possible to lift \nthis restriction at the cost of a more complex implementation of the sharing rule. See footnote 4 in \nSection 4.  indexed by an idempotent value (i.e., where X = X \u00b7 X), and returns a freely duplicable \nvalue. The type constraints on the operations vi statically ensure that .X : s. A(X) holds as an invariant \nat the beginning and ending of each call. However, if an operation vi is passed itself as an argument, \nwhether directly or indirectly, it may end up calling itself when its internal state does not satisfy \nthe invariant; this is the well-known problem of re-entrant calls [29, 44] in higher-order imperative \nprograms. One way to address this issue, embodied in Pottier s anti-frame rule [32], is to statically \ncheck that the invariant holds continuously, but this solution is often too restrictive [29]. We follow \nPilkiewicz and Pottier [29] in preventing reentrancy dynamically, using a lock. Thus, the operational \nsemantics of the sharing rule, given in Figure 6, is not a pure no-op, and shows how we rely on a combination \nof static and dynamic checking to enforce type safety. Sharing creates a .ag variable (the lock), and \nwraps each operator with code to test the lock and to diverge if it is already held. The remainder of \nthis section gives a series of examples using our sharing rule to introduce custom notions of resource, \nculminat\u00ading with an idealized memory allocator. Weak References Sharing enables us to model ML-style \nweak references of type !A that can be aliased. Suppose we have a location X : Loc, a duplicable pointer \nof type !ptr X and an af.ne capability of type cap X !A, and we wish to de.ne freely duplicable functions \nto dereference and assign the location X, with types !(1 -!A) and !(!A -1) respectively. The key idea \nis to use the share operator to allow these duplicable functions to close over the af.ne capability. \nFirst, we wrap the built-in operators getc l and l :=c v in the following functions get0 and set0 whose \nreturn types resemble those of the second argument of the construct share( , ). Typing these functions \nrequires the equivalence [cap X !A] = cap X !A, which our semantic model validates. get0 : .X : Loc. \n!ptr X -!([cap X !A] -!A . [cap X !A]) get0 = .!l. !.c. let (!v, c) = getl in let c =(l :=c !v) in (!v, \nc) c set0 : .X : Loc. !ptr X -!(!A . [cap X !A] -[cap X !A]) set0 = .!l. !. (!v, c) . let (!dummy,c) \n= getl in l :=c !v c For any expression e :!ptr X, get0 e and set0 e have types !([cap X !A] -!A . [cap \nX !A]) and !(!A . [cap X !A] \u00ad[cap X !A]), which essentially match the structure of the types speci in \nthe de.nition of the sharing rule. Next, we de.ne the monoid that encodes the logical state of the weak \nreference we are de.ning. Since the resource invariant for a weak reference is .xed and just states that \nthe reference points to something of type !A,we def def choose the unit monoid M =(1,E = () , (\u00b7)= .(x, \ny).()) ,and we interpret it by instantiating the capability operator A in Figure 7 def with C () = cap \nX !A. With these preliminaries, we can apply the sharing rule as follows: share ref (!l, c) = let (!g, \n!s) = (get0 !l, set0 !l) in share(c, g, s) share ref : .X : Loc. !ptr X . [cap X !A] \u00ad .a :1 ... [a ()] \n. !getType . !setType . !splitT . !joinT . !promoteT where getType def = .X :1. [aX] -!A . [aX] setType \ndef = .X :1. !A . [aX] -[aX] promoteT def = .X :1:: X = X \u00b7 X. [aX] -![aX] Finally, the unit monoid \nis idempotent by de.nition, so we can apply the promote operator to any value of type [at] (for any t \n:1). This allows us to construct the following function that, given a duplicable pointer and an af.ne \ncapability to it, returns two duplicable functions to read and write to it: MLref : .X : Loc. !ptr X \n. [cap X !A] -!(1 -!A) . !(!A -1) MLref (!l, c) = let (q, !get, !set,,, !promote) = share ref((!l, c)) \nin let !r = promote(q) in let deref =!(.(). let (v, ) = get(r) in v) in let setref =!(.a. let = set(a, \nr) in ()) in (deref , setref ) Monotonic Counters Next, we show how to construct shared monotonic counters \nthat can be freely incremented by all clients. Since clients can only increment the counter, the local \nknowledge of each client provides a lower bound on the counter s actual value. Suppose our counter is \nstored at a location X : Loc.Westart by de.ning a simple and standard increment function, next, that \ntakes as argument a pointer l of type !ptr X and a capability of type cap X !(nat n), increments the \ncounter, and returns n +1 and a capability of type cap X !(nat (n +1)). (We assume here that + is a primitive \noperation taking unrestricted values of type nat m1 and nat m2 and returning an expression of type !nat \n(m1 + m2).) next (!l, c) = let (!n, c)= getl in c let c =(l :=c n +1) in (n +1,c) next : .n. !ptr X . \ncap X !(nat n) -!nat (n +1) . cap X !(nat (n +1)) We wish to share the counter by passing the function \nnext as the second argument of the share( , ) operator. To do that, we must massage the type of next \ninto a compatible form, capturing the fact that, once the counter is aliased, its local knowledge only \nprovides def a lower bound on its value. We de.ne the monoid M =(N,E = def def 0, (\u00b7) =max), the type \nC(n)= cap X !(nat n) (to correspond to the type A in Figure 7) and observe that next can also be given \nthe following weaker type: next : nextType(C) nextType(a)= .Y, Z : N. !ptr X . [a(Z \u00b7 Y )] -.U : N :: \nU>Z. !nat U . [a(U \u00b7 Y )] The weaker type, nextType(C), only asserts that if the initial value of the \ncounter is max(Z, Y ), then its value after the next operation is max(U, Y ), for some U>Z. Intuitively, \nZ is the local context s initial lower bound on the counter, Y is the frame s lower bound on the counter, \nand U is the local context s lower bound on the counter after the increment operation. This weaker type \nis exactly in the form of the second argument of share( , ), so we can de.ne a counter sharing function \nthat creates an abstract, shared counter from a given capability to X : Loc and the next function. mkCnt \nc = share(c, next) mkCnt : . X : Loc,Y : N. [cap X !(nat Y )] \u00ad.a : N ... [aY ]. !nextType(a) . !splitT \n. !joinT . !promoteT Since max(x, x)= x, every element of our monoid is idempotent, so we can take any \ncounter and make it freely duplicable using the resulting function of type promoteT (as in the previous \nexam\u00adple). This permits multiple clients to make use of the same counter. Each client knows that its \nown use of the counter will yield mono\u00adtonically increasing elements, and does not have to worry about \ninterference with other clients of the counter. Fractional Permissions We provide an encoding of fractional \npermissions that is parametric in the underlying af.ne resource that we wish to share. Let s be an index \nsort, and let a : s .. be the type of an af.ne resource on top of which we want to layer a fractional \npermissions algebra. For example, to model fractional permissions over ref cells of type A, we could \nchoose s = Loc and aX = cap XA. We de.ne a sort of fractional (rational) numbers, called Frac, and a \nsort of fractional permissions over s, called FPerm(s): def Frac = {a . Q | 0 <a = 1} def FPerm(s)= {E, \n., Empty}.{(a, m) | a .S[Frac],m .S[s]}  A fractional permission is either E (essentially a 0 permission), \n. (for an invalid permission), Empty (denoting that there is no resource currently in place to be fractionally \nshared), or (a, m) (denoting fractional permission a to the resource represented by m). Here, S[s] denotes \nthe set of elements in the sort s. Fractional permissions form a monoid M with unit E and operation (\u00b7) \nde.ned on non-unit elements as follows: l (a + a ,m)0 <a + a = 1,m = m (a, m) \u00b7 (a ,m )= . otherwise \nl Empty x = E Empty \u00b7 x = . otherwise .\u00b7 x = . Next, we de.ne the af.ne type family FracTya which we \nactually share (this type family is called A in Figure 7). As required by the sharing rule, the type \nis indexed by the monoid FPerm(s). Here, void denotes the empty type .X : N :: .. 1. def def def FracTya \nE = void FracTya . = void FracTya Empty =1 l def am when a =1 FracTya (a, m)= void when a =1 Notice that \nthis type family is uninhabitable except at the extremes. This is important because, concretely, either \nthe whole resource will be available to the fractional permissions module as hidden state, or nothing \nwill be, even though the fractional permissions super.cially represent partial ownership. We now show \nhow to represent fractional permissions over the af.ne type a when a supports only one fractionally-shareable \nop\u00aderation, readonlyop,thatmaps aM to aM, possibly with aux\u00adiliary inputs and outputs (our construction \ngeneralizes very easily when there is more than one operation). Let this only operation, readonlyop, \nhave type ReadOnlyOp de.ned by: def ReadOnlyOp = .X : FPerm(s). .Y : s \u00d7 Frac :: P. \u00df (p1(Y )) . [FracTya \n((p2(Y ),M) \u00b7 X)] \u00ad.Z : s :: Q.. Z . [FracTya ((p2(Y ),M) \u00b7 X)] This type has been constructed speci.cally \nto match the spec type for the sharing rule, and thereby provide maximal generality. We quantify over \nan arbitrary fraction as the second component of Y . We can directly apply the share( , ) operator with \nthis opera\u00adtion as the second argument to obtain a shareable abstract type a but, to make the fractional \npermissions useful, we would also like to provide two operations that allow clients to exchange full \nre\u00adsources for full fractional permissions and vice versa. These two operations should have types de.ned \nbelow: def ToFrac = .X : s. [aX] . [a Empty] -[a (1,X)] def FromFrac = .X : s. [a (1,X)] -[aX] . [a Empty] \nAccordingly, we would like to pass to share( , ) two additional op\u00aderations of types ToFrac[FracTya/a \n] and FromFrac[FracTya/a ], respectively. Fortunately, given our de.nition of FracTya,these operations \ncan be trivially de.ned as .(x, ()).x and .x. (x, ()). Tying everything together, we now de.ne the following \nterm mkFrac, a generic (polymorphic) module for layering fractional permissions over a resource a. The \nmodule provides an empty frac\u00adtional permission at the outset, which can then be transferred to a full \npermission using ToFrac and back using FromFrac. (The type FracOp is de.ned as ReadOnlyOp with a in place \nof FracTya.) mkFrac = .!f. share((),f, .(x, ()).x, .x. (x, ())) mkFrac : .a : s ... .\u00df : s ... .. : s \n... !ReadOnlyOp -. a : FPerm(s) ... [a Empty] . !FracOp . !ToFrac . !FromFrac . !splitT . !joinT . !promoteT \n Memory Allocator We now give a stylized memory allocator with a non-monotonic resource invariant, inspired \nby (but much simpler than) Wickerson et al. s [43] proof of the Unix malloc function, and show how the \nallocator can be shared safely. The basic idea is that the memory allocator s free list is represented \nby an array, each entry of which contains a pair of a boolean .ag and a location; the .ag is true when \nthe location is free, and false when it has been allocated to a client. For free locations, the allocator \nalso owns a capability to access the memory of that location. For the allocated locations, it does not. \nTo formalize this idea, we .rst assume a family of types for af.ne arrays: arrA : Loc \u00d7 N \u00d7 (N . s) .. \nalengthA : .X : Loc,n : N,f : N . s. !ptr X . [arrA(X, n, f)] -!nat n . [arrA(X, n, f)] aswapA : .X \n: Loc,n : N,f : N . s, i : N,x : s :: i<n. !ptr X. !nat i . A(x) . [arrA(X, n, f)] -A(fi) . [arrA(X, \nn, .j. if(i = j, x, f(j)))] areadA : .X : Loc,n : N,f : N . s, i : N,x : s :: i<n. !ptr X. !nat i . \n(A(fi) -B . A(fi)) . [arrA(X, n, f)] -B . [arrA(X, n, f)] Here, A is a s-indexed type constructor, and \nthe index informa\u00adtion for the array of type arrA(X, n, f ) consists of its location X, its length n \nand a function f, such that for each i<n,the i-th ele\u00adment of the array contains a value of type A(fi).So \nf serves as a representation function for the array. To modify the array, we make use of a swapping operation \naswapA, which takes an array pointer, an index, a value, and a memory capability for the array, and uses \nit to replace the contents of that index. In the process, it also up\u00addates the representation function \nf. To read the array, we make use of a reading function areadA, which takes an array pointer, an in\u00addex, \nan array capability, and an observer function, which takes a value at the given location and returns \nthe array capability plus an observation of type B. To specialize this to the memory allocator ADT we \ndescribed brie.y above, we choose s =2 \u00d7 Loc,where 2= {tt, .} and A = contents, which is de.ned below: \ncontents :(2 \u00d7 Loc) .. contents(tt,X)=!bool tt . !ptr X . [cap X 1] contents(.,X)=!bool . . !ptr X . \n1 freelist : Loc \u00d7 N \u00d7 (N . (2 \u00d7 Loc)) .. freelist(X, n, f)= . :: inj(p2 . f). [arrcontents(X, n, f)] \n We de.ne the type freelist (the type of the free list of our memory allocator) as an array of contents, \nwith the invariant that the second projection of the representation function f be injective (i.e., the \narray has at most one entry for each location). The type operator contents takes a boolean b and a location \nX,where b re.ects whether X is free. The capability to access X is held in the array contents only for \nfree cells. Next, we use the function aswap to de.ne functions malloc at and free at to allocate and \nfree locations at a particular index in the free list, respectively. The function malloc at takes an \nindex i in the free list, which maps to location Y , a proof that the location is free (f (i)=(tt,Y )) \nand returns the capability of type [cap Y 1] stored in the location, swapping it with a unit value. free \nat does the opposite. .ag loc : .b :2,l : Loc. contents(b, l) -(!bool b . !ptr l) . contents(b, l) .ag \nloc (!b, !l, m) = ((!b, !l) , (!b, !l, m)) malloc at : .X, Y : Loc,n : N,f : N . (2 \u00d7 Loc),i : N :: \ni<n . f(i)=(tt,Y ). !ptr X. !nat i . freelist(X, n, f) -!ptr Y . [cap Y 1] . freelist(X, n, .j. if(i \n= j, (.,Y ),f(j))) malloc : .S : P(Loc).,X : Loc. !ptr X . [C(S)] -.Y : Loc. !ptr Y . [cap Y 1] . [C(S \n\u00b7{Y })] malloc(!a, m)= let (!n, m)= alength(!a, m) in let rec loop(m, !i)= if i<n then let ((!b, !l),m) \n= aread(!a, !i, .ag loc,m) in if(b, malloc at(!a, !i, m), loop(m, i +1)) else (.x f(x).f x) ()in loop(m, \n!0) free : .S : P(Loc).,X : Loc,Y : Loc. !ptr X . !ptr Y . [cap Y 1] . [C(S \u00b7{Y })] -[C(S)] free (!a, \n!l, c, m)= let (!n, m)= alength(!a, m) in let rec loop(c, m, !i)= let ((!b, !l ),m) = aread(!a, !i, .ag \nloc,m) in if(l = l, free at(!a, !i, c, m), loop(c, m, i +1)) in loop(c, m, !0) Figure 8. The Memory Allocator \nmalloc at(!a, !i, m)= let ((!b, !l),m) = aread(!a, !i, .ag loc,m) in let ((!b, !l, c),m) = aswap(!a, \n!i, (!., !l, ()) ,m) in (!l, c, m) free at : .X, Y : Loc,n : N,f : N . (2 \u00d7 Loc),i : N :: i<n . f(i)=(.,Y \n). !ptr X . !nat i . [cap Y 1] . freelist(X, n, f) -freelist(X, n, .j. if(i = j, (tt,Y ),f(j))) free \nat(!a, !i, c, m)= let ((!b, !l),m) = aread(!a, !i, .ag loc,m) in let ((!b, !l, ()),m) = aswap(!a, !i, \n(!tt, !l, c) ,m) in m Next, we consider the monoid P(Loc)., whose elements are sets of locations. The \nunit is the empty set E = \u00d8, and concatenation is de.ned by disjoint union l, with non-disjoint sets \ngoing to .. We use this monoid to de.ne the type C(S) for a given location X pointing to the head of \nthe free list: C(.)= void C(S)= .n : N,f : N . (2 \u00d7 Loc):: S = {l |.i<n.f(i)= (.,l)}. freelist(X, n, \nf) Intuitively, for S = ., C(S) is a free list whose allocated pointers coincide exactly with S. Using \nC(S), we can de.ne operations malloc and free (Fig\u00adure 8). The malloc operation traverses the free array \nuntil it .nds an unallocated element, updates the .ag, and returns that ele\u00adment. If the free list is \nfully allocated, then we go into an in.\u00adnite loop more realistic implementations would signal an er\u00adror \nor resize the free list. The free operation also iterates over the array until it .nds the element it \nwas passed as an argument, but it does not have to perform a bounds check as it iterates: the type C(S \n\u00b7{Y }) guarantees that the location Y will be found in the free list, and hence that i is always in bounds. \nNote that the type of the location comparison operation = used in free is .X, Y : Loc. ptr X . ptr Y \n-bool (X = Y ). More sophisticated versions of this pattern arise frequently in the implementation of \nfree lists, connection pools, and other re\u00adsource managers. The critical feature of our invariant is \nthat we can only free a piece of memory if it originally came from this mem\u00adory allocator in the .rst \nplace. Furthermore, it is a non-monotonic invariant, since the same piece of memory can go in and out \nof the free list, which means that the size of the free list in the predicate can grow and shrink as \nthe program executes. However, we can nevertheless share the memory allocator, since the frame conditions \non the speci.cations express the constraint that interference between different clients is benign up \nto partial { } j+1 def k< n, .j. . . Islandk Worldn = W =(k, .) .[0] = HIslandk { } def (M, \u00b7,E) comm. \nmonoid, Islandn = . =(M, \u00b7,E,I) I . M . ResPredn () def Heap., l, \u00d8,HIslandn = .h.{(W, E) | W . Worldn,h \n= .} { } def .W ; W. (W,r) . . ResPredn = . . ResAtomn =. (W ,r) . . { } def W .Worldn, .i. ai . W..[i].M, \n ResAtomn =(W, r) r =(a0,...,am-1),m = |W..| { } def .W ; W. (W, (r, v)) . V ValPred = V . ValAtom \n..r.(W, (r \u00b7 r ,v)) . V def ValAtom = {(W, (r, v)) |.n. (W, r) . ResAtomn } def t(k +1,.)=(k, l.Jk) def \nl(.1,...,.n)Jk =(l.1Jk,..., l.nJk) def l(M, \u00b7,E,I)Jk =(M, \u00b7,E,.a.lI(a)Jk) l.Jk def = {(W, r) . . | W.k \n<k} def ,...,.l ) ; (.1,...,.n)= n = n, .i = n. .= .i (.1ni def (k ,. ) ;j (k, .)= k = k - j, . ;l.Jkl \ndef (s, r): W = s = s0 \u00b7 ... \u00b7 sm-1,m = |W..|, .i . 0..m - 1. (tW,si) . W..[i].I((s \u00b7 r)[i]) Figure 9. \nPossible Worlds and Related De.nitions correctness, no client cares what allocations or deallocations \nother clients perform: mkAllocator : .X, n, f, S :: S = {l |.i<n.f(i)= (.,l)} . freelist(X, n, f) -.a \n: P(Loc). ... [a(S)] . !mallocType . !freeType !splitT . !joinT . !promoteT mkAllocator m = share(m, \nmalloc, free) The memory manager s state can be split up and shared among many different clients. The \nkey is to observe that for any state S,we know that a(S)= a(Sl\u00d8). Thus we can pass each client a copy \nof a(\u00d8), which it can use to allocate and free locally-owned memory without knowledge of the allocation \nbehavior of other clients.  4. The Semantic Model In this section, we justify the soundness of our type \nsystem. The main challenge, of course, is validating the sharing rule. We gain traction by characterizing \nthe behavior of well-typed terms through a step-indexed Kripke logical relation (SKLR). While SKLRs have \nbeen used previously to give clean semantic soundness proofs of related substructural calculi [5], ours \nis novel in its treatment of resources. We therefore begin by laying down some conceptual groundwork \nand terminology concerning resources. Physical vs. Logical Resources and the Global Store In the be\u00adginning, \nthere is the heap: it is a primitive, physical notion of split\u00adtable resource, and in the absence of \nsharing there is little more to say. The af.ne heap capability cap fA gives its owner i.e., the term \nthat consumes it full control over the location f and its contents, and the lack of sharing means that \nno other parts of the program may contain any knowledge about f or its contents at all. Each application \nof the sharing rule, however, introduces a new logical notion of splittable resource, represented as \na commutative monoid (M, \u00b7,E), which governs access to a piece of shared state. Control over resources \nof type M becomes a new type of af.ne capability (written [at] in the sharing rule in Figure 7), which \nmay be consumed by or transferred between different parts of the program just as heap capabilities can.2 \nUnlike the heap, which has a direct physical interpretation, M must be given an interpretation in terms \nof what invariants it imposes on the underlying shared state. Speci.cally, the capability [At] in Figure \n7 describes the invariant that holds of the shared state when the global store of M (i.e., the monoidal \ncomposition of all resources of type M that are currently in existence) is t. For those readers with \na Hoare-logic background, it may be helpful to think of this global store of M as a kind of ghost state \n[22] that instruments the physical heap state with extra logical information.  Atomic vs. Composite \nResources As a program executes, a new logical resource is created each time the sharing rule is executed, \nextending the resource set (which begins life with only the lone physical resouce of the heap). We will \nsay that a resource belonging to any one of these types is an atomic resource. Of course, a term may \nnaturally own many different atomic resources, as a result of being composed from multiple different \nsubterms. For example, it may own the heap capability cap f 1 to control location f, as well as the logical \ncapability [at] (where a is the abstract type constructor created by some application of the sharing \nrule). In this case, the term owns a physical heap resource ([f : ()]), as well as a logical resource \n(t) of the monoidal resource type that was created along with a. In general, a term may own resources \nof every type currently in existence (and later, when new types of resource are created, it can be implicitly \nviewed as owning the unit element of those re\u00adsources). We call such a combination of resources of all \nthe differ\u00adent atomic types a composite resource. Given that each atomic re\u00adsource is a commutative monoid, \nobserve that composite resources form a commutative monoid via the obvious product construction. For \nconvenience, we overload \u00b7 and write r1 \u00b7 r2 to denote the com\u00adponentwise composition of two composite \nresources r1 and r2. Composite resources are the fundamental currency of our model. Not only are they \nwhat terms consume and produce, but furthermore, when we apply the sharing rule to make some un\u00adderlying \n(af.ne) resource shareable, that underlying resource is a composite resource, and the invariant that \ngoverns it takes the form of a predicate on composite resources. Worlds and Islands Being a Kripke logical \nrelation, our model (presented below) is indexed by possible worlds. In previous Kripke models of ML-like \nlanguages, these worlds have been used to encode invariants on the physical heap. Here, since we support \nlogical as well as physical resources, we generalize worlds to en\u00adcode (1) the knowledge of what types \nof logical resources have been created by applications of the sharing rule, and (2) how to interpret \nthose logical resources as invariants on shared state. As de.ned in Figure 9, worlds are tuples of islands, \nwith each island describing a different type of resource.3 (Ignore the step indices k and n for now; \nwe explain them below.) An island com\u00adprises a commutative monoid (M, \u00b7,E), as well as a representation \ninvariant I that interprets elements of M into assumptions (com\u00adposite resource predicates) on the underlying \nshared state. Speci.\u00adcally, I(t) denotes the invariant that holds on the shared state when the global \nstore of the island s resource (M)is t. 2 Note: even if the sharing rule is instantiated twice with the \nsame monoid, it nevertheless generates two distinct types of logical resources. The dis\u00adtinction is enforced \nsyntactically by the fact that each application of the sharing rule creates a fresh, existentially-quanti.ed \ncapability constructor a;eveniftwo such a s (say, a1 and a2) are indexed by the same monoid, instantiations \n[a1 t] and [a2 t] will not be confused with each other. 3 Throughout, we use dot notation like W.k and \nW.. to project named components from structures, and indexing notation like .[i] to project the ith component \nfrom a tuple. The .rst island (island 0) is .xed to be the built-in island for physical heaps (HIsland). \nIts monoid is the standard partial commutative monoid on heaps, with disjoint union as composition and \nthe empty heap as unit, completed to a total monoid with a bottom element .. Its representation invariant \nI(h) is trivial it asserts no ownership of any underlying shared resource because there is none, but \nis only satis.ed if h is a heap and not .. In the other islands, the representation invariant I is more \nin\u00adteresting. First and foremost, it is world-indexed. For those read\u00aders familiar with recent SKLRs \n[16, 3], which employ similarly world-indexed heap invariants, the reason for this world-indexing will \nlikely be self-explanatory: it s needed to account for the pres\u00adence of higher-order state. For most \nother readers, it may appear completely mysterious, but it is also a technical point that the reader \nmay safely gloss over (by skipping the next paragraph). Brie.y, the reason for the world-indexing of \nthe resource pred\u00adicates is as follows: in proving the sharing rule (see the end of this section), we \nextend the world with a new island, and we want to de\u00ad.ne its I(t) to require (roughly) that the underlying \nshared resource of the island must justify the capability [At],where A is the capa\u00adbility constructor \nin the .rst premise of the sharing rule (Figure 7). But for arbitrary A, the question of whether some \n(composite) re\u00adsource r justi.es the capability [At] depends on what the current world W is when the \nquestion is asked, which might be at some point in the future when new invariants have been imposed by \nfu\u00adture islands. Such a situation would arise, for instance, were we to apply the sharing rule to create \na weak reference (Section 3) to a value of function type, which is (not coincidentally) the canonical \nexample of higher-order state. The solution is thus to parameterize the resource predicate I(t) over \nW , knowing that the W parameter will always be instantiated (in the de.nition of world satisfaction \nbelow) with the current world. This parameterization trick is by now a very standard move in the SKLR \nplaybook for building models of higher-order state [3, 17]. However, it is also a prime example of Wheeler \ns adage that all problems in computer science can be solved by another level of indirection, but that \nusually will create another problem. Indeed, an unfortunate consequence is that it causes a bad circularity \nin the construction of worlds that cannot be solved directly in sets. The step-indexed approach of Ahmed \net al. [1, 2, 3] handles this prob\u00adlem by stratifying the construction of worlds by n . N bounding the \nnumber of execution steps for which we observe the program, with n going down by 1 in the world parameter \nof the resource predicate. The details of this construction are entirely standard, as are the world approximation \n(L\u00b7Jk) and later (t) operators in Fig\u00adure 9, and interested readers are referred to the literature [3, \n17]. In any case, the resource predicates in the range of I are required to be monotonic: adding new \nislands to a world cannot invalidate the invariants of previous islands (see the de.nition of ResPred). \nFinally, when using a composite resource r with j atomic sub\u00adresources in the context of a future world \nwith j + k islands, we silently assume the atomic sub-resources of the last k islands are E. Local vs. \nShared Resources and World Satisfaction In reality, a term e executes under a global heap h. In our model, \nwe think of e as executing, logically, under the global composite store,which comprises all the resources \ncurrently in existence: speci.cally, it combines the global store of every atomic resource in existence, \nin\u00adcluding the heap (which is the 0-th island s resource). Some portion r of that global composite store \nis directly known to (and owned by) e itself we call this e s local resource while the remaining portion \ns constitutes the shared resource. The shared resource is so named because it is required to contain \nall the underlying shared resources needed to satisfy the representation invariants of all the islands \nin the world. (The local vs. shared terminology is bor\u00ad  def def K[.] =ValPred K[s . .] = S[s] .K[.] \ndef V[Bt]W = {(r, I[t].)} for B .{bool, nat, ptr} . def {} V[cap tA]W =(r \u00b7 [I[t]. : v], )(r, v) .V[A]W \n.. def V[1]W = {(r, ())} . def {}. l . V[A1 . A2]W =(r1 \u00b7 r2, (v1,v2))(ri,vi) .V[Ai]W def .W ; W. (r \n,v ) .V[A]W l . V[A -B]W =(r, v) . =. (r \u00b7 r ,v v ) .E[B]W l . def {} V[!A]W =(r l \u00b7 r, !v)(r, v) .V[A]W \n,r = }r \u00b7 r .. def l} V[.X:s ::P.A]W =V[A]W .[X . d] |= P ..[X .d] l} def V[.X:s ::P.A]W = V[A]W .[X \n. d] |= P ..[X .d] def V[.a : .. A].W =V[A]W V .K[.] .[a .V ] l} def V[.a : .. A]W = V[A]W V .K[.] ..[a \n.V ] def V[a]W = .(a) . def {} V[ [A] ]W =(r, ) .v. (r, v) .V[A]W .. def V[if(t,A,B)]W = if I[t]. = tt \nthen V[A]W else V[B]W . .. def V[.X : s. A]W = .d .S[s]. V[A]W ..[X .d] def V[At]W =(V[A]W )(I[t].) .. \ndef E[A]W = { (r, e) |.j< W.k, (s, r \u00b7 rF): W. . if h =(s \u00b7 r \u00b7 rF)[0], (h; e) '.j (h ; e ) '. then .W \n;j W, (s ,r \u00b7 rF): W with h =(s \u00b7 r \u00b7 rF)[0], (r ,e ) .V[A]W l } . Figure 10. Kripke Logical Relation \nrowed from Vafeiadis s work on concurrent separation logics [41], in which a closely analogous distinction \narises.) Formally, the relationship between the local and shared re\u00adsources is codi.ed by the world satisfaction \nrelation (s, r): W , de.ned in Figure 9, which asserts that s can be split into m com\u00adposite resources \nsi (one for each island of W ) such that si satis.es island i s representation invariant W..[i].I. Note \nthat the argument passed to I is (s \u00b7 r)[i]: this is correct because I s argument is sup\u00adposed to represent \nthe global store of the i-th island, which is pre\u00adcisely the i-th projection of the global composite \nstore, s \u00b7 r.Note also that the world parameter of each island s resource predicate is instantiated with \ntW , the current world W approximated one step-index level down. Kripke Logical Relation Logical relations \ncharacterize program behavior by induction over type structure, lifting properties about base type computations \nto properties at all types: a term at a com\u00adpound type is well-behaved if every way of eliminating it \nyields a well-behaved term at some simpler type. Kripke logical relations index logical relations by \na world W , which places constraints on the machine states under which terms are required to behave well. \nAlthough logical relations are often binary relations for proving program equivalences [30], it suf.ces \nin our case to de.ne unary predicates, since we are merely trying to prove safety [6]. Figure 10 presents \nour Kripke logical relation. We assume a se\u00admantics of sorts S[s], index terms I[t]. (where fv(t) . dom(.)) \nand propositions . |= P (where fv(P ) . dom(.)), all standard from multisorted .rst-order logic. From \nthe semantics of sorts, we can easily build a semantics of kinds K[.]. The value predicate V[A]W. is \nindexed by both a world W and a semantic environment ., and is satis.ed by pairs (r, v) of values v and \ntheir supporting (composite) resources r. Because the type system is af.ne, the re\u00adsource r may contain \nsome part that is irrelevant to v and in gen\u00aderal, if (r, v) .V[A]W. and W ; W then (r \u00b7 r ,v) .V[A]W. \nl , an assumption codi.ed in the de.nition of ValPred. This mono\u00adtonicity property means that the good \nbehavior of a term can de\u00ad def Env[\u00b7] = \u00d8 def Env[S,a : .] = {., a . V | . . Env[S],V .K[.]} def Env[S,X \n: s] = {., X . d | . . Env[S],d .S[s]} def U[\u00b7]W = \u00d8 . {} def . .U[G]W , (r, v) .V[A]W , .. U[G,x : A]W \n= ., x . (r, v) . r = r \u00b7 r L[\u00b7]W def = \u00d8 . def {} L[.,x : A]W = d, x . (r, v) d .L[.]W , (r, v) .V[A]W \n. .. def p(.)={r | x . dom(.),.(x)=(r, v)} def p(d)={r | x . dom(d),d(x)=(r, v)} def S; .; G; . 1 e : \nA = .W, . . Env[S],. .U[G]W ,d .L[.]W . .. . |=.=. (p(.) \u00b7 p(d),d(.(e))) .E[A]W . Figure 11. Semantics \nof Open Terms pend on certain islands and resources being present, but not on cer\u00adtain islands or resources \nbeing absent. The de.nition of the value predicate is essentially standard in particular, it is essentially \nan af.ne version of the model of L3 [5] out.tted with our monoidal worlds. One difference is our interpre\u00adtation \nof the exponential !A, which is inhabited by !v only when v can be supported by some idempotent portion \nof the resources that is, some part of the resources that permits the structural rule of contraction. \n(In L3, the heap is the only resource, so only the empty heap is idempotent.) Also, since universal and \nexistential types are introduced implicitly, they are given intersection and union seman\u00adtics, respectively. \nThe remaining differences are to do with indexed types e.g., the parameter indexing the base types bool, \nnat,and ptr must re.ect the particular value inhabiting the type and the computational irrelevance type \n[A], whose interpretation records the resources needed to justify A but not the value that inhabits it. \nThe term predicate E[A]W. captures the crucial property sup\u00adporting sharing: namely, that computations \nare frame-respecting. Suppose that a term e owns (composite) resource r.Toshow e is well-behaved, we \nquantify over an arbitrary frame resource rF rep\u00adresenting the resource of e s evaluation context. Together, \nr \u00b7 rF constitute the local resource, i.e., the portion of the global compos\u00adite store that the program \nbeing executed owns. We also quantify over some shared resource s such that (s, r \u00b7 rF): W .If e reduces \nto an irreducible term e starting from the global heap that is the 0-th projection of s \u00b7 r \u00b7 rF in j \nsteps, where j is less than the world s step-index W.k, then it must (1) leave the heap in a state described \nby a new global composite store s \u00b7 r \u00b7 rF, such that (2) (s ,r \u00b7 rF): W for some future world W of W \n(whose step\u00adindex is W.k - j), and (3) the .nal term e is in fact a value that, supported by the resource \nr , obeys the value predicate V[A]W. l . Note, however, that the frame resource rF must remain unchanged. \nThe logical predicates de.ned in Figure 10 only describe well\u00adbehaved closed terms. In Figure 11, we \nlift these to predicates on open terms in the standard way: namely, we consider e to be well\u00adbehaved \nat the type A under context O, written O 1 e : A,ifit is well-behaved (according to E[A]) for all well-behaved \nclosing instantiations of its free variables. These closing instantiations in\u00adclude both values and the \nresources supporting them; the p operator then multiplies together all the resources supporting a closing \nin\u00adstantiation. Note that the resources accompanying the instantiations of the unrestricted variables \nin G are required to be idempotent, so that they may be safely duplicated within the proof of soundness. \nSoundness of the Type System The main technical result of the paper is summed up in the following theorems: \n Theorem 1 (Fundamental Theorem of Logical Relations). If O f e : A,then O 1 e : A. Theorem 2 (Adequacy). \nIf \u00d8 1 e : A and (\u00d8; e) '.* (h; e ) '.,then e is a value. Corollary 3 (Soundness of the Type System). \nIf \u00d8f e : A and (\u00d8; e) '(h; e ) '.,then e is a value. .* The proof of Adequacy is almost trivial. The \nproof of the Fun\u00addamental Theorem essentially proceeds by showing that each rule in our type system is \nsemantically sound, i.e., that it holds if all the syntactic f s are replaced by semantic 1 s. The proofs \nfor most rules follow previous developments using SKLRs [3, 17, 16]. The most interesting new case, of \ncourse, is that of the sharing rule. The proof is quite involved, so here we will just offer a rough \nidea of how the proof goes, focusing on the most interesting technical con\u00adstructions. (For the full \ndetails, see the technical appendix [24].) As described above, the intuition behind our worlds W is that \neach island in W corresponds to an application of the sharing rule. Indeed, the proof that the sharing \nrule is semantically sound is the only part of our proof that involves extending a given input world \nW with a new island to form a future world W (as permitted in the de.nition of the logical term predicate). \nSupposing W already had n islands (0..n - 1), the new island will have index n. At .rst glance, it would \nseem we want to de.ne this new island to be (S[s], \u00b7,E,Isimple),where (S[s], \u00b7,E) is the monoid with \nwhich the sharing rule was instantiated, and the representation invariant Isimple is de.ned in terms \nof the A in the .rst premise of the rule (and whatever . we are given to interpret its free variables): \nIsimple(x)= {(W, r) |.v. (r, v) .V[A] W. (x)} in a moment). Finally, we give the following interpretation \nfor the abstract capability constructor a (returned by the share operation): { } [a] = .x .S[s]. (W, \n(r, ))r[n]= r + U(x) This essentially says that the owner of [ax] has control over a U(x) piece of the \nresource on island n. The two parameters to L are a technical trick we use to show that the shared operations \nof the ADT are frame-preserving . Speci.cally, the monoid we have de.ned has the property that if we \ncontrol L(y, E) of the resource, then the only possible resource r that the rest of the program could \nhave on island n, such that I(L(y, E)+ r) is satis.able, is U(y). To see how this is exploited in the \nsoundness proof, suppose that a client owns [at] (i.e., she controls a U(t) piece of island n s resource), \nand invokes one of the shared operations, whose type spec (see Figure 7) promises to transform [at] into \n[at ] for some t . (For simplicity, we ll ignore the frame X in the type of the operation. It does not \nadd any fundamental complication.) If the lock is held, the operation will diverge and there is nothing \nto show. If the lock is released, the de.nition of I guarantees that the rest of the global store on \nisland n must be of the form U(y) for some y, and that the island s shared resource r satis.es [A (t \n\u00b7 y)]. Here, U(y) represents the control the rest of the program has over the shared state of island \nn,and we must show that the operation we are about to execute respects it. Now, before invoking the underlying \noperation, we acquire the lock, we remove r from the shared resource so that we can transfer ownership \nof it to the operation, and this is the key point we replace the client s local U(t) resource with the \nresource L(y, E), thus updating the global store of island n to L(y, y).Whenwe invoke the underlying \noperation, we place the L(y, E) in its frame, which (by de.nition of the logical term predicate) it must \npreserve. This invariant stipulates that the shared resource of island n satis-Thus, when we get back \ncontrol from the operation (which must be .es the capability [Ax] when the island s global store is x. \nHowever, we must also take account of the lock f that the dy\u00ad namic semantics of share creates in order \nto protect against reen\u00ad trancy. Intuitively, when the lock f is released, the representation invariant \nof island n should be much like the above Isimple.But when the lock f is held, it means we are in the \nmiddle of a call to one of the operations returned by share, during which the represen\u00ad tation invariant \nmight not hold at all. The monoid of island n must in a state such that I is satis.able), the global \nstore of island n must still be L(y, y), of which the client controls L(y, E) and the rest of the program \ncontrols U(y). Also, the frame-preserving nature of the underlying operation s type tells us that it \nmust have returned us a resource r satisfying the capability [A (t \u00b7 y)]. We can then release the lock, \nreplace the client s L(y, E) resource with U (t ) (which is what the client expects to control when the \noperation is completed), and transfer ownership of r back to the island s shared therefore re.ect these \ntwo possibilities. We de.ne island n as (M, +,U (E),I), where (in ML notation) resource, which now satis.es \nI at the new global store, U(t \u00b7 y). But crucially, despite/because of all these shenanigans, the resource \nU(y) belonging to the rest of the program has been left untouched! type M = U of S[s] | L of S[s] \u00d7S[s] \n|., the composition operator (+) is the commutative closure of U(x)+ U(y)= U(x \u00b7 y) L( )+ L()=  5. Related \nWork . Dealing with Reentrancy: Locking vs. the Anti-Frame Rule As L(x, y)+ U (z)= L(x, y \u00b7 z) . += ., \nexplained in Section 3, our sharing rule uses a lock to protect and the representation invariant I is \nde.ned as against unsafe reentrancy, which can arise in our language due its support for shared, higher-order \nstate. Most prior separation logics have not had to deal with such a hard problem because they are W. \nI(U(x)) = {(W, r \u00b7 [f : .]) |.v. (r, v) .V[A] (x)} I(L(x, y)) = {(W, [f : tt]) | x = y} done in a .rst-order \nsetting, where the possibility of reentrancy \u00d8. is syntactically evident; and most prior substructural \ntype systems I(.)= The idea here is to distinguish between unlocked states U(x), where the lock f is \nreleased, and locked states L(x, y),where f is held. In the former case, I asserts that f points to . \nand that the rest of the island s shared resource r can satisfy [Ax], as required for invoking any of \nthe shared operations.4 In the latter case, I asserts that f points to tt and that x = y (we explain \nabout that 4 Note that if the sharing rule did not require A to represent a capability (i.e., to appear \nin proof-irrelevant brackets), then invoking any of the shared operations would require us to cough up \nthe actual value v witnessing Ax (whereas here, v is .-quanti.ed). This could be achieved by changing \nthe implementation of the sharing rule so that it maintains a private reference cell \u00a3 storing the current \nwitness v, and then updating I to also own [\u00a3 : v]. (e.g., L3 [5]) have not had to deal with it because \nthey don t support sharing/hiding of state. One exception is Pottier s work on the anti-frame rule [32], \nwhich does account for reentrancy in the presence of shared, higher-order state. The anti-frame rule \npermits a group of func\u00adtions to operate on a piece of hidden state described by an in\u00advariant C. Externally \nto the anti-frame rule, those functions may have type !(A -B), but internally they have roughly the form \n!(A . C -B . C) (but not quite, as we explain below). In a substructural setting, the rule therefore \ngives a way to export, e.g., an af.ne reference with a set of operations, without treating the operations \nthemselves as af.ne or forcing the client to thread the the af.ne reference capability through its code. \nThe restriction to a simple invariant has been subsequently relaxed to support hidden monotonic invariants \n[35], as well as monotonic observations about hidden state [29] (although to our knowledge the last exten\u00adsion \nhas not yet been proven sound).  Pottier s approach provides a more general solution to the reen\u00adtrancy \nproblem (of which our use of locks would constitute one mode of use), but this comes at the cost of signi.cant \nadditional complexity in the typing rule for hiding (i.e., the anti-frame rule) itself. In particular, \nthe . operator that Pottier employs in the type !(A . C -B . C) above is not a simple tensor, but rather \na tensoring operation, which propagates under . and ref types and comes equipped with a non-standard \nequational theory. Soundness proofs of the anti-frame rule using traditional syntactic techniques have \nconsequently required years of heroic effort [33]. That said, signi.cantly simpler semantic proofs of \nthe anti-frame rule have also been given using Kripke logical relations [35]. Based on this experience, \nwe chose to use a semantic model in our work, and have been very satis.ed with its simplicity. In this \npaper, we decided to isolate concerns by focusing on sharing and leaving an improved handling of reentrancy \nto future work. One possibility would be to consider synthesizing our shar\u00ading rule with the anti-frame \nrule, since they are complementary. The anti-frame rule offers a more general treatment of reentrancy, \nwhile the sharing rule offers a more general treatment of sharing. As demonstrated in our weak references \nexample, simple invari\u00adants may be encoded via the sharing rule using the unit monoid, and subsequently \nhidden. More novel, however, is our support for a variety of interesting uses of sharing involving both \nmonotonic state and non-monotonic state (e.g., the memory manager exam\u00adple). Furthermore, our use of \nmonoids lets clients divide, transfer, and recombine resources as they need, without restricting to a \none\u00adway increase in information as the anti-frame rule does. Fictions of Separation From the outset, \nsubstructural reasoning about state has relied on the notion of disjointly supported asser\u00adtions for \nlocal reasoning, but only gradually has the .exibility of that notion become clear. Early models of logically \n(but not phys\u00adically) separable resources like fractional permissions [7, 10] and trees [9] treat those \nresources as primitive, either baking them into the operational semantics or, in simple cases, relying \non a .xed in\u00adterpretation into an underlying heap. To handle higher-level notions of separation, Krishnaswami \net al. [23] embedded domain-speci.c separation logics into higher-order separation logic, and Dinsdale-Young, \nGardner, and Wheelhouse named the general phenomenon .ctional disjointness and justi.ed its support of \nlocal reasoning by employing data re.nement and axiomatic semantics [14]. Contemporaneously, concurrent \nabstract predicates (CAP, [13]) combined .ctional disjointness with several other important ideas the \ntwo most relevant being abstract predicates [28] and rights-as\u00adresources [15]. CAP allows the speci.cation \nof each module to include abstract predicates which, like the abstract data types in\u00adtroduced by our \nsharing rule, represent local knowledge and rights about a shared underlying resource. Hence, just as \nthe tensor . is the all-purpose notion of separation for us, so separating conjunc\u00adtion * is for CAP. \nOn the other hand, CAP is built on more speci.c and complex forms of knowledge and rights, inherited \nfrom deny\u00adguarantee [15] and intended for reasoning about concurrency. In very recent work, several groups \nof researchers have si\u00admultaneously proposed variants of commutative monoids as an abstract way to capture \n.ctional separation. Their original goals were quite distinct: Jensen and Birkedal s .ctional separation \nlogic (FSL) [21] is explicitly intended as a simple axiomatiza\u00adtion of .ctional disjointness within separation \nlogic; Dinsdale-Young et al. s views [12] are intended as a more abstract account of CAP (and compositional \nreasoning about concurrency in gen\u00aderal); and Ley-Wild and Nanevski s subjective concurrent separa\u00adtion \nlogic (SCSL) [25] is geared toward compositional reasoning about ghost state. The three frameworks also \nshare a shortcoming: the separating conjunction * of the assertion language is tied to a single, speci.c \nmonoid. With views and SCSL, this monoid is .xed at the outset, when the framework is instantiated. FSL, \nin contrast, is based on indirect Hoare triples parameterized by an interpretation map, which explicitly \nrecords a monoid together with its interpretation as a predicate on underlying resources. An interpretation \nmap is akin to an island in our model (Section 4), which means that the assertions within an indirect \nHoare triple must all be given in terms of a single abstract resource. While FSL enables interpretation \nmaps to be stacked in layers or combined as a product (resembling our worlds), such structure must be \nexplicitly managed within both assertions and proofs. Our sharing rule also employs commutative monoids \nfor .c\u00adtional separation, but it associates a different monoid with each ab\u00adstract data type it introduces. \nConsequently, our tensor product con\u00adstructor . implicitly mediates between all resources currently in \nexistence, both the physical resources and a dynamically-growing set of user-de.ned logical resources. \nTemporarily Structural Types Most substructural type systems are not completely substructural: they permit, \nby a variety of means, linear or af.ne types to coexist with unrestricted types. Keeping a strict distinction \nbetween the two kinds of types is crucial for ensuring the soundness of e.g. strong updates, but it is \nalso im\u00adpractical for large programs with complex data structures. There have been numerous proposals \nfor safely allowing the rules to be bent [37, 36], a well known example being F\u00a8 ahndrich and DeLine \ns adoption and focus [19]. At the root of these designs for temporar\u00adily structural types is the ability \nto revoke access to previously aliased data, providing a freshly linear view of that data. When un\u00adrestricted \naccess is later restored, however, there must be some way of ensuring that the aliases still have an \nappropriate type, and the simplest way of doing that is to keep the type .xed. Our sharing rule, on the \nother hand, does not commit to a par\u00adticular aliasing discipline. The abstract resources supported by \na shared underlying resource can be created and aliased to whatever extent their governing monoid allows, \nand can be strongly updated at any time without risk of invalidating non-local assertions. It re\u00admains \nto be seen whether our monoidal approach is .exible enough to recover the sophisticated rule-bending \nof the temporarily struc\u00adtural typing disciplines mentioned above. Per-Module Notions of Resources Two \nrecent languages Tov s Alms [40] and Mazurak and Zdancewic s F . [26] have been pro\u00adposed for general-purpose, \npractical programming with substruc\u00adtural types. The generality of these languages stems from their abil\u00adity \nto perform substructural sealing: they can seal an unrestricted value with an abstract type at a substructural \nkind, thereby prevent\u00ading clients from freely aliasing the value. Substructural sealing, like our sharing \nconstruct, provides a way to introduce per-module no\u00adtions of resource. But substructural sealing is \nused to impose a more restrictive interface on a less restrictive value, while sharing goes the other \nway around, allowing aliasing of af.ne resources. This difference is apparent in the work done by a typechecker \nin both cases: for substructural sealing, there is little to check, because it is always safe to tighten \nthe interface to a value; for sharing, the ex\u00adported operations must be shown to respect their frame. \nUltimately, these two forms of resource introduction seem complementary, and indeed, the language we \nhave presented supports both. Kripke Logical Relations Kripke logical relations have long been used to \nreason about state in higher-order, ML-like languages [31]. Ahmed et al. [5, 4] have given Kripke logical \nrelations for linear languages with state, using a simple notion of possible world corresponding to strict \nheap separation. The structure of our logical relation is quite similar to this earlier work, but the \nstructure of our worlds is signi.cantly different, since we must account for interaction between an unbounded \nnumber of abstract resource types, each of which is governed by a distinct monoid.  More recently, Ahmed \net al. [3] and Dreyer et al. [16] have given models for higher-order structural state based on the concept \nof transition systems, which facilitate the modeling of protocol\u00adbased uses of state, as well as the \nwell-bracketed state changes possible in languages without control. Since transition systems can be modeled \nas monoids, our current model fully supports transition systems as a mode of use. With a small extension \n(whose proof is in the appendix [24]), we can also model Dreyer et al. s public vs. private transitions \nfor reasoning about well-bracketed state changes, although proofs based on their techniques are arguably \nmore direct than ours. (We plan to report on this in future work.)  6. Conclusion and Future Work In \nthis paper, we have shown how to put programmer-de.ned re\u00adsource abstractions on the same footing as \nbuilt-in resources such as the heap, yielding a type system that permits the .exible use of aliased data \nwhile retaining the simple intuitions of substructural logic. To do so, we combined exciting new ideas \nfrom separation logic with classical type-theoretic techniques such as re.nement types and data abstraction. \nAn immediate direction for future work is to study how to optimize the sharing rule, both via the model \n(i.e., proving that locks are not needed for speci.c implementations), and via type\u00adtheoretic extensions \nthat we could use to avoid locking (e.g., via formalizing the concept of .rst-order data as a modality, \nor via a sharing modality [36]). Another natural direction for future work is to examine if our methods \nextend to full-blown value-dependent types (e.g., as in HTT [27]). This poses interesting questions, \nsince methods based on step-indexing have historically had challenges dealing with semantic equalities \n(as opposed to approximation), and our sharing rule deeply connects existential types and state.  References \n[1] A. Ahmed. Semantics of Types for Mutable State. PhD thesis, Princeton University, 2004. [2] A. Ahmed. \nStep-indexed syntactic logical relations for recursive and quanti.ed types. In ESOP, 2006. [3] A. Ahmed, \nD. Dreyer, and A. Rossberg. State-dependent representa\u00adtion independence. In POPL, 2009. [4] A. Ahmed, \nM. Fluet, and G. Morrisett. A step-indexed model of substructural state. In ICFP, 2005. [5] A. Ahmed, \nM. Fluet, and G. Morrisett. L3: A linear language with locations. Fundamenta Informaticae, 77:397 449, \n2007. [6] A. Appel, P.-A. Melli`es, C. Richards, and J. Vouillon. A very modal model of a modern, major, \ngeneral type system. In POPL, 2007. [7] J. Boyland. Checking interference with fractional permissions. \nIn SAS, 2003. [8] T. Brus, M. C. J. D. van Eekelen, M. van Leer, M. J. Plasmeijer, and H. P. Barendregt. \nClean: A language for functional graph rewriting. In FPCA, 1987. [9] C. Calcagno, P. Gardner, and U. \nZarfaty. Context logic and tree update. In POPL, 2005. [10] C. Calcagno, P. W. O Hearn, and H. Yang. \nLocal action and abstract separation logic. In LICS, 2007. [11] R. DeLine and M. F\u00a8ahndrich. Enforcing \nhigh-level protocols in low\u00adlevel software. In PLDI, 2001. [12] T. Dinsdale-Young, L. Birkedal, P. Gardner, \nM. Parkinson, and H. Yang. Views: Compositional reasoning for concurrency, 2012. Submitted for publication. \n [13] T. Dinsdale-Young, M. Dodds, P. Gardner, M. Parkinson, and V. Vafeiadis. Concurrent abstract predicates. \nIn ECOOP, 2010. [14] T. Dinsdale-Young, P. Gardner, and M. Wheelhouse. Abstraction and re.nement for \nlocal reasoning. In VSTTE, 2010. [15] M. Dodds, X. Feng, M. J. Parkinson, and V. Vafeiadis. Deny\u00adguarantee \nreasoning. In ESOP, 2009. [16] D. Dreyer, G. Neis, and L. Birkedal. The impact of higher-order state \nand control effects on local relational reasoning. In ICFP, 2010. [17] D. Dreyer, G. Neis, A. Rossberg, \nand L. Birkedal. A relational modal logic for higher-order stateful ADTs. In POPL, 2010. [18] J. Dun.eld. \nA Uni.ed System of Type Re.nements. PhD thesis, Carnegie Mellon University, 2007. [19] M. F\u00a8ahndrich \nand R. DeLine. Adoption and focus: Practical linear types for imperative programming. In PLDI, 2002. \n[20] J.-Y. Girard. Linear logic. TCS, 50(1):1 102, 1987. [21] J. Jensen and L. Birkedal. Fictional separation \nlogic. In ESOP, 2012. [22] C. B. Jones. The role of auxiliary variables in the formal development of \nconcurrent programs. In Re.ections on the work of C.A.R. Hoare, pages 167 188. Springer, 2010. [23] N. \nR. Krishnaswami, L. Birkedal, and J. Aldrich. Verifying event\u00addriven programs using rami.ed frame properties. \nIn TLDI, 2010. [24] N. R. Krishnaswami, A. Turon, D. Dreyer, and D. Garg. Super.cially substructural \ntypes (Technical appendix), 2012. URL: http://www.mpi-sws.org/~dreyer/papers/supsub/. [25] R. Ley-Wild \nand A. Nanevski. Subjective concurrent separation logic, 2012. Submitted for publication. [26] K. Mazurak, \nJ. Zhao, and S. Zdancewic. Lightweight linear types in System F..In TLDI, 2010. [27] A. Nanevski, G. \nMorrisett, and L. Birkedal. Hoare Type Theory, polymorphism and separation. JFP, 18(5&#38;6):865 911, \nSept. 2008. [28] M. J. Parkinson and G. M. Bierman. Separation logic and abstraction. In POPL, 2005. \n[29] A. Pilkiewicz and F. Pottier. The essence of monotonic state. In TLDI, 2011. [30] A. Pitts. Typed \noperational reasoning. In B. C. Pierce, editor, Advanced Topics in Types and Programming Languages, chapter \n7. MIT Press, 2005. [31] A. Pitts and I. Stark. Operational reasoning for functions with local state. \nIn HOOTS, 1998. [32] F. Pottier. Hiding local state in direct style: a higher-order anti-frame rule. \nIn LICS, 2008. [33] F. Pottier. Syntactic soundness proof of a type-and-capability system with hidden \nstate, 2011. Submitted for publication. [34] J. C. Reynolds. Separation logic: A logic for shared mutable \ndata structures. In LICS, 2002. [35] J. Schwinghammer, L. Birkedal, F. Pottier, B. Reus, K. St\u00f8vring, \nand H. Yang. A step-indexed Kripke model of hidden state. Mathematical Structures in Computer Science, \n2012. To appear. [36] R. Shi, D. Zhu, , and H. Xi. A modality for safe resource sharing and code reentrancy. \nIn ICTAC, 2010. [37] F. Smith, D. Walker, and G. Morrisett. Alias types. In ESOP, 2000. [38] R. E. Strom \nand S. Yemini. Typestate: A programming language concept for enhancing software reliability. IEEE Transactions \non Software Engineering, 12(1):157 171, 1986. [39] M. Tofte and J.-P. Talpin. Region-based memory management. \nInfor\u00admation and Computation, 132(2):109 176, 1997. [40] J. Tov. Practical Programming with Substructural \nTypes. PhD thesis, Northeastern University, 2012. [41] V. Vafeiadis. Modular .ne-grained concurrency \nveri.cation.PhD thesis, University of Cambridge, 2008. [42] D. Walker, K. Crary, and G. Morrisett. Typed \nmemory management via static capabilities. TOPLAS, 22:701 771, 2000. [43] J. Wickerson, M. Dodds, and \nM. Parkinson. Explicit stabilisation for modular rely-guarantee reasoning. In ESOP, 2010. [44] N. Wolverson. \nGame semantics for an object-oriented language.PhD thesis, University of Edinburgh, 2008. [45] H. Xi \nand F. Pfenning. Dependent types in practical programming. In POPL, 1999.  \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Many substructural type systems have been proposed for controlling access to shared state in higher-order languages. Central to these systems is the notion of a *resource*, which may be split into disjoint pieces that different parts of a program can manipulate independently without worrying about interfering with one another. Some systems support a *logical* notion of resource (such as permissions), under which two resources may be considered disjoint even if they govern the *same* piece of state. However, in nearly all existing systems, the notions of resource and disjointness are fixed at the outset, baked into the model of the language, and fairly coarse-grained in the kinds of sharing they enable.</p> <p>In this paper, inspired by recent work on \"fictional disjointness\" in separation logic, we propose a simple and flexible way of enabling any module in a program to create its own custom type of splittable resource (represented as a commutative monoid), thus providing fine-grained control over how the module's private state is shared with its clients. This functionality can be incorporated into an otherwise standard substructural type system by means of a new typing rule we call *the sharing rule*, whose soundness we prove semantically via a novel resource-oriented Kripke logical relation.</p>", "authors": [{"name": "Neelakantan R. Krishnaswami", "author_profile_id": "81548019176", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P3804303", "email_address": "neelk@mpi-sws.org", "orcid_id": ""}, {"name": "Aaron Turon", "author_profile_id": "81548019177", "affiliation": "Northeastern University, Boston, USA", "person_id": "P3804304", "email_address": "turon@ccs.neu.edu", "orcid_id": ""}, {"name": "Derek Dreyer", "author_profile_id": "81548019178", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P3804305", "email_address": "dreyer@mpi-sws.org", "orcid_id": ""}, {"name": "Deepak Garg", "author_profile_id": "81309499820", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P3804306", "email_address": "dg@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364536", "year": "2012", "article_id": "2364536", "conference": "ICFP", "title": "Superficially substructural types", "url": "http://dl.acm.org/citation.cfm?id=2364536"}