{"article_publication_date": "09-09-2012", "fulltext": "\n A Meta-Scheduler for the Par-Monad Composable Scheduling for the Heterogeneous Cloud Adam Foltzer Abhishek \nKulkarni Rebecca Swords Sajith Sasidharan Eric Jiang Ryan R. Newton Indiana University {afoltzer, adkulkar, \nraingram, sasasidh, erjiang, rrnewton} @indiana.edu Abstract Modern parallel computing hardware demands \nincreasingly spe\u00adcialized attention to the details of scheduling and load balancing across heterogeneous \nexecution resources that may include GPU and cloud environments, in addition to traditional CPUs. Many \nex\u00adisting solutions address the challenges of particular resources, but do so in isolation, and in general \ndo not compose within larger sys\u00adtems. We propose a general, composable abstraction for execution resources, \nalong with a continuation-based meta-scheduler that har\u00adnesses those resources in the context of a deterministic \nparallel pro\u00adgramming library for Haskell. We demonstrate performance bene\u00ad.ts of combined CPU/GPU scheduling \nover either alone, and of combined multithreaded/distributed scheduling over existing dis\u00adtributed programming \napproaches for Haskell. Categories and Subject Descriptors D.3.2 [Concurrent, Dis\u00adtributed, and Parallel \nLanguages] General Terms Design, Languages, Performance Keywords Work-stealing, Composability, Haskell, \nGPU 1. Introduction Ideally, we seek parallel code that not only performs well, but for that performance \nto be preserved under composition. Alas, this is not always the case even in serial code: implementations \nof functions f and g may be well-optimized individually, but if f . g is run inside a recursive loop, \nthe composition may, for example, exceed the machine s instruction cache. Nevertheless, sequential composition \nis far easier to reason about than parallel composition, which is the topic of this paper. Historically, \nthere have been many reasons for parallel codes not to compose. First, many parallel programming models \nare .at rather than nested i.e. a parallel computation may not contain an\u00adother parallel computation \n[9, 28]. Moreover, many parallel codes take direct control of hardware or operating system resources, \nfor example by using Pthreads directly. These programs, when Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 12, September 9 15, 2012, Copenhagen, \nDenmark. Copyright c &#38;#169; 2012 ACM 978-1-4503-1054-3/12/09. . . $10.00 composed, result in oversubscription, \nas has famously troubled OpenMP1 [3]. Yet the rising popularity of work-stealing schedulers (Section \n3) is a step forward for composability, at least on symmetric multipro\u00adcessors (SMPs). By abstracting \naway explicit thread management these schedulers enable mutually ignorant parallel subprograms to coexist \npeacefully without oversubscription, and are now available for a wide range of different languages, including \nHaskell [26], C++ [2, 19, 32], Java [18], and Manticore [15], as well as many others. New problems arise, \nhowever, namely: 1. Multiple schedulers for the same language are dif.cult to coor\u00addinate effectively \nand in a principled manner (e.g. TBB / Cilk / TPL [2, 19, 32], or even Haskell s sparks [26] and IO threads). \n 2. Non-CPU resources such as GPUs are competing for attention, and are not treated by existing schedulers. \n 3. Parallel work schedulers are themselves complex software ar\u00adtifacts (Section 3), typically non-modular \n[1], and dif.cult to extend.  The approach we take in this paper is to factor an existing work\u00adstealing \nimplementation into composable pieces. This addresses the complexity problem, but also leads the way \nto extensibility and interoperability even beyond the CPU. We describe a new system, Meta-Par2, which \nis an extensible implementation of the Par-monad library for Haskell [25]. The Par monad (Section 2) \nprovides only basic parallel operations: forking control .ow and communication through write-once synchroniza\u00adtion \nvariables called IVars. The extension mechanism we propose allows new variants of fork (e.g. to fork \na computation on the GPU), but remains consistent with the semantics of the original Par monad, in particular \nretaining deterministic parallelism. We present a set of these extensions, which we call Resources, that \naddress challenges posed by current hardware: (1) dealing with larger and larger multi-socket (NUMA) \nSMPs, (2) programming GPUs, and (3) running on clusters of machines. Further, we ob\u00adserve that from the \nperspective of a CPU scheduler, these Resources have much in common; for example, handling asynchronous \ncom\u00adpletion of work on a GPU or on another machine across the network presents largely the same problem. \nWe argue that Resources pro\u00advide a useful abstraction boundary for scheduler components, and show that \nthey compose into more sophisticated schedulers using a simple associative binary operator. Using a composed \nscheduler, a single program written for Meta-Par today can handle a variety of hardware that it might \nencounter 1 OpenMP: A popular set of parallel extensions to the C language widely used in the high-performance \ncomputing community. 2 http://hackage.haskell.org/package/meta-par  in the wild: for example, an ad-hoc \ncollection of machines some of which have GPUs while others do not. Hence the heterogeneous cloud: mixed \narchitectures within and between nodes. The primary contributions of this paper are: A novel design \nfor composable scheduler components (Section 4).  A demonstration of how to cast certain aspects of \nscheduler design aspects which go beyond multiplexing sources of work using Resources. One example is \nadding backoff to a scheduler loop to prevent excessive busy-waiting (Section 4.4).  An empirical evaluation \nof the Meta-Par scheduler(s), which includes evaluation of a number of recent pieces of common infrastructure \nin the Haskell ecosystem (network transports, CUDA libraries, and the like), as well as an in-depth case \nstudy of parallel comparison-based sorting implementations (Section 6.2).  The .rst, to our knowledge, \nuni.ed CPU/GPU work-stealing scheduler3 (Section 4.5), along with an empirical demonstra\u00ad tion that GPU-aware \nCPU-scheduling can outperform GPU\u00adoblivious (Section 6.3). With further validation, this princi\u00ad ple \nmay generalize beyond our implementation and beyond Haskell.  These results are preliminary, but encouraging. \nMeta-Par can provide a foundation for future work applying functional program\u00adming to the heterogeneous \nhardware wilderness. The reader is en\u00adcouraged to try the library, which is hosted on github and released \nvia Haskell s community package manager, Hackage: here, here, and here. 2. The Par Monad(s) Earlier work \n[25] introduced a Par monad with the following oper\u00adations: runPar :: Par a . a fork :: Par () . Par \n() new :: Par (IVar a) get :: IVar a . Par a put_ :: IVar a . a . Par () A series of fork calls creates \na binary tree of threads. We will call these Par-threads, to contrast them with Haskell s IO threads \n(i.e. user-level threads) and OS threads. Par threads do not return values hence the unit type in Par \n() instead they communicate only through IVars. IVars are .rst class, and an IVar can be read or written \nanywhere within the tree of Par-threads (albeit written only once). By blocking to read an IVar, Par-threads \ncan indeed be de\u00adscheduled and resumed, thereby earning the moniker thread . Ab\u00adstractly, IVars introduce \nsynchronization constraints that transform the tree describing the structure of the parallel computation \ninto a directed acyclic graph (DAG), as in Figure 1. DAGs are the stan\u00ad dard abstraction for parallel \ncomputations used in most literature on scheduling [5, 7, 8, 35]. The simple primitives supported by \nPar can be used to build up combinators capturing common parallelism patterns, and one extremely simple \nand useful combinator is spawn_, which provides futures: spawn_ :: Par a . Par (IVar a) spawn_p =doi \n. new fork (do x . p; put_ i x) return i 3 Though the idea has been discussed [17]. The original paper \n[25] has many more examples, and explains aspects of the design which we do not cover here, such as the \ndis\u00adtinction between put and put (weak-head-normal-form strictness vs. full strictness), and the reasoning \nbehind this design. The spawn_ abstraction is suf.cient to de.ne divide-and-conquer parallel algorithms \nby recursively creating a future for every sub\u00adproblem (a common idiom). We will use mergesort as a running \nexample of this style. Below we de.ne a mergesort on Vectors, a random-access, immutable array type commonly \nused in high\u00adperformance Haskell code. parSort :: Vector Int . Par (Vector Int) parSort vec = if length \nvec = seqThreshold then return (seqSort vec) else let n = (length vec) div 2 (left, right) = splitAt \nn vec in do leftIVar . spawn_ (parSort left) right . (parSort right) left . get leftIVar parMerge left \nright This function splits the vector to be sorted and uses spawn_ on the left half, giving rise to a \nbalanced binary tree of work to be run in parallel. A sequential sort is called once the length of the \nvector falls below a threshold. 2.1 Meta-Par Preliminary: Generalizing Par In later sections we introduce \nvariations on fork and spawn_ that correspond to alternate .avors of child computations, such as those \nthat might run on a GPU or over the network. Because a scheduler might have any combination of these \ncapabilities, there are many possible schedulers. Therefore, each scheduler will have a distinct variant \nof the Par monad (a distinct type), so that a subcomputation that depends on, say, a GPU capability cannot \nencounter a runtime error because it is combined with a scheduler lacking the capability. Thus we need \nto take a refactoring step that is common in Haskell library engineering4 introduce type classes to generalize \nover a collection of types that provide the same operations, in this case, multiple Par monads. class \nMonad m . ParFuture future m | m . future where spawn_ :: m a . m (future a) get :: future a . ma class \nParFuture ivar m . ParIVar ivar m | m . ivar where fork :: m() . m () new :: m (ivar a) put_ :: ivar \na . a . m () In the above classes we take an opportunity to separate levels of Par functionality. A given \nPar implementation may support just futures5(ParFuture class), or may support futures and IVars (ParIVar \nclass). The distinction in levels of capability will become more important as we introduce capabilities \nsuch as gpuSpawn and longSpawn (and classes ParGPU, ParDist) in Sections 4.5 and 4.5.2. In the above \nclass de.nitions, the type variable m represents the type of a speci.c Par monad that satis.es the interface \n(i.e. an instance). Some of the complexity above is speci.c to Haskell and may safely be ignored for \nthe reader of this paper. Namely, 4 A common example being the PrimMonad type class generalizing over \nIO and ST true external side effects and localized, dischargeable ones. 5 Indeed, we have a scheduler \nthat uses sparks [26] and supports only fu\u00ad tures. This allows us to compare the ef.ciency of our scheduling \nprimitives to those built in to the GHC runtime, using the former if desired.  the ParFuture and ParIVar \nclasses are multi-parameter type classes, both parameterized by a type variable ivar as well as m. This \nis necessary because two Par monads may require different representations for their synchronization variables. \nFinally, because the type for ivar is determined by the choice of Par monad, the above includes another \nadvanced feature of GHC type classes: a functional dependency, m . ivar. We do not simplify these classes \nfor the purpose of presentation, because they correspond exactly to those used in the released code. \n The writer of a reusable library should always use the generic functions, and never commit to a concrete \nPar monad. The .nal application is then free to decide which concrete implementation and therefore which \nheterogeneous execution capabilities to use. For the remainder of the paper, let us assume that all Par \nimplemen\u00adtations reside in their own distinct modules, Control.Monad.Par.Foo), each providing a concrete \ntype constructor named Par, as well as instances for the appropriate generic operations. These are the \nschedulers (plural) in our system, whereas Meta-Par itself is a meta\u00adscheduler not touched directly by \nusers, but instantiated to create concrete schedulers. For readability, we will informally write con\u00adcrete \ntype signatures, Par a, (referring to any valid concrete Par monad) rather than the more generic ParFuture \niv p => p a. 3. Work-Stealing Schedulers In work-stealing schedulers, each worker maintains a private \nwork pool, synchronizing with other workers only when local work is exhausted (the parsimony property \n[34]). Thus the burden of syn\u00ad chronizing and load-balancing falls on idle nodes. Like any parallel scheduler, \nwork-stealing schedulers map work items (e.g. forked Par-threads) onto P workers; workers are most often \nOS threads with a one-to-one correspondence to processor cores. As a work-stealing algorithm, the original \nimplementation of the Par monad [25] is rather standard and even simple. Yet schedulers that grow up \nfor example TBB, Cilk, or the GHC runtime become very complex, dealing with concerns such as the following: \n Idling behavior to prevent wasted CPU cycles in tight work\u00adstealing loops ( busy waiting ).  Managing \ncontention of shared data structures (backoff, etc.).  Interacting with unpredictable user programs \nthat can call into the scheduler (e.g. call runPar) from different hardware threads or in a nested manner. \n Multiplexing multiple sources of work.  Alas, in spite of this complexity, such schedulers typically \nhave monolithic, non-modular implementations [1, 19, 32]. Regard\u00ad ing work-source multiplexing in particular: \na typical work-stealing scheduler is described in pseudocode as an ordered series of checks against possible \nsources of work. For example, in the widely-used Threading Building Blocks (TBB) package, the reference \nmanual [4], Section 12.1, includes the following description of the task\u00ad scheduling algorithm: After \ncompleting a task t, a thread chooses its next task according to the first applicable rule below: 1. \nThe task returned by t.execute() 2. The successor of t if t was its last completed predecessor. 3. \nA task popped from the end of the thread s own deque. 4. A task with affinity for the thread. 5. A \ntask popped from approximately the beginning of the shared queue. 6. A task popped from the beginning \nof another randomly chosen thread s deque.   Figure 1. [Left] Meta-scheduling: scan a stack of work \nsources, always starting at the top. Work sources are heterogeneous, but all work is retrieved as unit \ncomputations in the Par monad (i.e. Par ()). [Right] Work DAGs formed by forks and gets; the circles \nat the leaves represent tasks bound for the resource with matching color. Six possible sources of work! \nAnd that is only for CPU scheduling. Rather than committing to a list like the above and hardcoding it \ninto the scheduler (the state of the art today), we construct sched\u00adulers that are composed of reusable \ncomponents. For example, a rough description of a distributed CPU/GPU scheduler may look like the following: \n1. Steal from CPU-local deque (try N times) else 2. Steal-back from GPU else 3. Steal from network \nelse 4. Goto step 1  This resembles a stack of resources. In fact, the purpose of this paper is to \ndemonstrate that scheduler composition need only be a simple associative binary operator. The familiar \nmappend oper\u00adation from Haskell s Monoid type class then suf.ces to combine Resources into compound [stacks \nof] Resources. 4. Meta-Scheduling: The Resource Stack The scheduler for Meta-Par is parameterized by \na stack of hetero\u00adgeneous execution resources, each of which may serve as a source of work. All workers \nparticipating in a Meta-Par execution (on all threads and all machines) run a scheduling loop that interacts \nwith the resource stack. Resource stacks are built using mappend, where (a mappend b) is a stack with \na on top and b on the bottom. Below, the type Resource is used for both singular and composed Resources. \nWe will use resource stack informally to refer to com\u00adplete, composed Resources. The division of labor \nin our design is between schedulers, Re\u00adsources, and the Meta-Par infrastructure (meta-scheduler). First, \nthe meta-scheduler itself: Creates worker threads, each with a work-stealing deque.  Detects nested \ninvocations of runPar and avoids re-initialization of the Resource (i.e. oversubscription)6.  Provides \nconcrete Par and IVar types that all Meta-Par-based schedulers use and repackage.  6 This ultimately \nrequires global mutable state via the well-known unsafePerformIO with NOINLINE pragma hack, both in Meta-Par \nand for some supplementary Resource data structures. See Figure 2  This Par provides blocking get operations \nvia a continua\u00adtion monad, using continuations to suspend Par-threads in the style of Haynes, Friedman, \nand Wand [16]. Further, each Resource may introduce: Additional (internal) data structures for storing \nwork, above and beyond the per-worker thread deques. These might contain work for an external device \nof a different type than Par ().  One or more fork-like operations appropriate to the resource. These \npush work into the per-resource data structures.  Finally, each scheduler contains: A new Par type \n(a newtype as described in Section 2.1),  a corresponding runPar, and  a composed Resource [stack] \n Thus a scheduler is a mere mashup of Resources, re-exporting components of Meta-Par and of constituent \nResources. In fact, schedulers can be created on demand with a few lines of code7. Typically, each scheduler \nand each Resource reside their own mod\u00adule. An example module implementing a Resource is shown in Fig\u00adure \n2, and an example module implementing a scheduler is shown in Figure 3. Because each Resource manages \nits own data structures, Meta-Par is not strictly just for work-stealing. For example, a Resource could \nchoose to ignore Meta-Par s spawn in favor of its own oper\u00adator with work-sharing semantics. Indeed, \neven the built-in work\u00adstealing behavior can be cast as a stand-alone Resource; however we choose to \ninclude it in the core of the system in order to keep the Meta-Par interface simpler. 4.1 Resource Internals \nA Resource presents an interface composed of two callbacks: a startup callback, and a work-searching \ncallback. type Startup = Resource . Vector WorkerState . IO () type WorkSearch = Int . Vector WorkerState \n. IO (Maybe (Par ())) data Resource = Resource { startup :: Startup, workSearch :: WorkSearch } The startup \ncallback is responsible for performing any work necessary to prepare a Resource, such as spawning worker \nthreads for SMP scheduling, or opening network connections for dis\u00adtributed coordination. A global barrier \nensures that no work com\u00admences until each Resource in the stack has completed initializa\u00adtion. The Resource \nargument to startup ties the knot to make the .nal composed Resource available when initializing any \nof its com\u00adponent Resources. WorkerState structures store each worker s work-stealing deque, along with \ncertain shared information such as the random number generator used for randomized work stealing. Each \nworker may have a single active Par-thread currently executing. When that Par-thread is .nished or blocks \non an IVar, the worker .rst tries to pop from the top of its work-stealing deque, 7 However, there is \none error prone aspect of scheduler composition. The newtype Par may use newtype-deriving to derive capabilities \nsuch as ParGPU corresponding to only the resources actually composed. A mis\u00admatch here could result in \na runtime error when a computation is run on an incompatible scheduler. An alternative would be constructing \nresource stacks explicitly at the type level (like a monad transformer stack), but this comes with signi.cant \ncomplications, including our reluctance to introduce lift operations. and if no work is found, invokes \nworkSearch. The arguments to workSearch provide the searcher s ID (just an Int) along with the global \nWorkerState vector. The former can be used to look up the local WorkerState structure in the latter. \nThe worker expects the workSearch to respond either with a unit of work (Just work), or with Nothing. \nResources, combined with mappend, form a non-commutative monoid so we can compose them using the Monoid \ntype class: 8 instance Monoid Startup instance Monoid WorkSearch instance Monoid Resource The Startup \ninstance is straightforward, where the empty ac\u00adtion does nothing, and composing two startups means to \nrun them in sequence with the same arguments. The interesting in\u00adstance is for WorkSearch, which must \nbe composed so that the work-.nding attempt runs the second workSearch only when the .rst workSearch \nreturns Nothing. instance Monoid WorkSearch where mempty = .__ . return Nothing mappend ws1 ws2 = .wid \nstateVec . do mwork . ws1 wid stateVec case mwork of Nothing . ws2 wid stateVec _ . return mwork In order \nto satisfy the axioms of a monoid, the empty Resource mempty does nothing no Meta-Par workers are ever \nspawned by its Startup, no work is ever found by its WorkSearch, and so no work can be computed if it \nis the only Resource. Meta-Par leaves it to non-empty implementations of Resources to decide how many \nand on which CPUs to spawn worker threads. The Meta-Par module itself, absent any Resources, provides \nvery little. It simply exposes primitives for spawning workers (handling exceptions, waiting for the \nstartup barrier, logging debugging info) and running entire Par computations with a particular Resource \ncon.guration: spawnWorkerOnCPU :: Resource . Int . IO () runMetaParIO :: Resource . Par a . IO a Meta-Par \ncommits to a speci.c concrete Par type (Control.Monad.Par.Meta.Par) for its internal implementation and \nfor the construction of new Resources and composed sched\u00adulers. This Par type allows arbitrary computation \nvia a MonadIO instance, which would put the Par-monad determinism guarantee at risk if exposed to the \nend user. Instead, the privileged Meta.Par is wrapped by the schedulers in newtype Par types that provide \nonly appropriate instances. For example, the SMP+GPU scheduler exports a Par monad that is an instance \nof ParFuture, ParIVar, and ParGPU, but not an instance of unsafe classes like MonadIO, or even classes \nfor other Meta-Par Resources (e.g., ParDist) not included in that particular scheduler.  4.2 CPU Scheduling: \nSingle-threaded and SMP To show that Meta-Par subsumes the previous implementation of Par-monad, we implement \nResources for serial execution and SMP. In section 6.1, we compare the performance against previous re\u00ad \nsults. The single-threaded Resource is the minimal Resource required for the meta scheduler to execute \nwork. Its startup creates a single worker on the current CPU, and its workSearch always returns Nothing, \nas the Resource has nowhere to look for more work. 8 For clarity, we use type here to present Startup \nand WorkSearch, but our implementation uses newtype to avoid type synonym instances.  module Control.Monad.Par.Meta.Resources.GPU \nwhere ... {-# NOINLINE gpuQueue #-} gpuQueue :: ConcurrentQueue (Par (), IO ()) gpuQueue = unsafePerformIO \nnewConcurrentQueue {-# NOINLINE resultQueue #-} resultQueue :: ConcurrentQueue (Par ()) resultQueue = \nunsafePerformIO newConcurrentQueue class ParFuture ivar m . ParGPU ivar m | m . ivar where gpuSpawn :: \n(Arrays a) . Acc a . m (ivar a) instance ParGPU IVar Par where gpuSpawn :: (Arrays a) . Acc a . Par (IVar \na) gpuSpawn comp = do iv . new let wrapCPU = put_ iv (AccCPU.run comp) wrapGPU = do ans . evaluate (AccGPU.run \ncomp) push resultQueue (put_ iv ans) liftIO (push gpuQueue (wrapCPU, wrapGPU)) return iv gpuProxy :: \nIO () gpuProxy = do --block until work is available (_, work) . pop gpuQueue --run the work and loop \nwork >> gpuProxy mkResource :: Resource mkResource = Resource { startup = . __ . forkOS gpuProxy workSearch \n= . __ . do mfinished . tryPop resultQueue case mfinished of Just finished . return (Just finished) Nothing \n. do mwork . tryPop gpuQueue fst fmap mwork } Figure 2. An Accelerate-based GPU Resource implementation \nmodule. {-# LANGUAGE GeneralizedNewtypeDeriving #-} module Control.Monad.Par.Meta.SMPGPU (Par, runPar) \nwhere ... resource = SMP.mkResource mappend GPU.mkResource newtype Par a = Par (Meta.Par a) deriving \n(Monad, ParFuture Meta.IVar, ParIVar Meta.IVar, ParGPU Meta.IVar, ... ) runPar :: Par a . a runPar (Par \nwork) = Meta.runMetaPar resource work Figure 3. A scheduler implementation module combining two Re\u00adsources. \nsingleThreadStartup resource _ = do cpu . currentCPU spawnWorkerOnCPU resource cpu singleThreadSearch \n_ _ = return Nothing The SMP Resource offers the same capability as the origi\u00adnal implementation of the \nwork-stealing Par-monad scheduler. Its startup spawns a Meta-Par worker for each CPU available to the \nHaskell runtime system. Its workSearch selects a stealee worker at random and attempts to pop from the \nstealee s work queue, looping a .xed number of times if the stealee has no work to steal. smpSearch myid \nstateVec = let WorkerState {rng} = stateVec ! myid getNext :: IO Int getNext = randomRange (0, maxCPU) \nrng loop :: Int . Int . IO (Maybe (Par ())) loop 0 _ = return Nothing loop n i | i == myid = loop (n-1) \n=<< getNext loop n i = let WorkerState {workpool} = stateVec ! i in do mtask . tryPopBottom workpool \ncase mtask of Nothing . loop (n-1) =<< getNext _ . return mtask in loop maxTries =<< getNext  4.3 CPU \nScheduling: NUMA Modern multi-socket, multi-core machines employ a shared mem\u00adory abstraction, but exhibit \nNon-Uniform Memory Access (NUMA) costs. This means that it is signi.cantly cheaper to access some memory \naddresses than others from a given socket, or NUMA node. Unfortunately, even if the memory allocation \nsubsystem correctly allocates into node-local memory, work-stealing can disrupt local\u00adity by moving work \nwhich depends on that memory to a differ\u00adent node. Thus NUMA provides an incentive for work-stealing \nalgorithms to prefer stealing work from cores within the same NUMA node. Although topology-aware schedulers \nhave been pro\u00adposed [6], most of the widely deployed work-stealing schedulers [2, 18, 19, 32] are oblivious \nto such topology issues. In the Meta-Par SMP setting, workers .rst try to pop work from their own queues \nbefore making more costly attempts to steal work from other CPUs. In the NUMA case, we support analogous \nbehavior: a worker .rst attempts to steal work from CPUs in its own NUMA node, and only moves on to attempt \nmore costly inter-node steals when no local work is available. Our NUMA-aware Meta-Par implementation \nnotably is a re\u00adsource transformer, rather than a regular Resource, and demon\u00adstrates the .rst-class \nnature of Resources. Instead of duplicating the work-stealing functionality of the SMP Resource, the \nNUMA Resource is composed of a subordinate SMP Resource for each NUMA node in the machine. Unlike the \nSMPs in Section 4.2, which may randomly steal from all CPUs, these subordinate SMP workSearches are restricted \nto steal only from CPUs in their respective node. With these subordinate Resources in place, the NUMA \nworkSearch .rst delegates to the SMP workSearch of the calling worker s local node. If no work is found \nlocally, it then enters a loop analogous to the SMP loop that calls all nodes SMP workSearches at random. \n 4.4 Another Resource Transformer: Adding Backoff An essential and pervasive aspect of practical schedulers \nis the ability to detect when little work is available for computation and back off from busy-waiting \nin the scheduler loop. Detecting a lack of available work may seem like a primitive capability that must \nbe built into the core implementation of the scheduler loop, but we can in fact implement backoff for \narbitrary Meta-Par Resource stacks as a Resource transformer.  The backoff workSearch does not alter \nthe semantics of the workSearch it transforms. Instead, it calls the inner workSearch, leaving both the \narguments and the return value unchanged. It does, however, observe the number of consecutive times that \na workSearch call returns Nothing for each Meta-Par thread (a counter kept in the WorkerState structure). \nWhen little work is available across the scheduler, these counts increase, and the back\u00adoff Resource \nresponds by calling a thread sleep primitive with a duration that increases exponentially with the count. \nWhen a workSearch again returns work for a thread, the count is reset, and the scheduling loop resumes \nwithout interruption.  4.5 Heterogeneous Resources -Blocking on foreign work A key motivation for composable \nscheduling is to handle different mixes of heterogeneous resources outside of the CPU(s). Work\u00ading with \na non-CPU resource requires launching foreign tasks and scheduling around blocking operations that wait \non foreign re\u00adsults (or arranging to poll for completion). Existing CPU work\u00adstealing schedulers have \nvarying degrees of awareness of blocking operations. Common schedulers for C++ (e.g. Cilk or TBB) are \ncompletely oblivious to all blocking operations ranging from block\u00ading in-memory data structures (e.g. \nwith locks) to IO system calls. Obliviousness means that while the scheduler attempts to maintain P worker \nthreads for P processors, fewer than P may be active at a given time. It is often suggested to use Haskell \ns IO threads directly to im\u00adplement Par-threads, as there is widespread satisfaction with how lightweight \nthey are. This would appear attractive, as the Glas\u00adgow Haskell Compiler (GHC) implements blocking operations \nat the Haskell thread layer (IO threads) using non-blocking system calls via the GHC event manager [30]. \nIO threads are even ap\u00ad propriately preempted when blocking on in-memory data struc\u00adtures, namely MVars. \nUnfortunately, GHC s IO threads are not lightweight enough for .ne-grained parallelism. They still require \nallocating large contiguous stacks and Par schedulers based on them cannot compete [25, 29]. Ultimately, \nthe lightest-weight approaches for pausing and resuming computations are based on continuation passing \nstyle (CPS). The relationship between CPS and coroutines or threading is old and well known [16], but \nhas increasingly been applied for concurrency and parallelism [11, 20, 21, 33, 37]. In the Par-monad, \nCPS is already a necessity for ef.cient blocking on IVars (Meta-Par uses the continuation-monad-transformer, \nContT). Using continua\u00adtions, we gain the ability to schedule around foreign work e.g. to keep the CPU \noccupied while the GPU computes for free. 4.5.1 Heterogeneous Resource 1: GPU Several embedded domain-speci.c \nlanguages (EDSLs) have been proposed to enable GPU programming from within Haskell [9, 24, 36]. In addition, \nraw bindings to the CUDA and OpenCL are avail\u00ad able [13, 27]. Accelerate and other EDSLs typically introduce \nnew types (e.g. Acc) for GPU computations as well as a run function much like Par, in fact. In Meta-Par, \nwe provide built-in support for launching Acceler\u00adate computations from Par computations: gpuSpawn :: \nArrays a . Acc a . Par (IVar a) --Asynchronous Acc computation, filling IVar when done: do gpuSpawn (Acc.fold \n(+) 0 (Acc.zipWith ... You might well ask why gpuSpawn is needed, given that both runPar and Accelerate \ns run are pure and should therefore be freely composable. Indeed, they are, semantically, but as discussed \nin the previous section, we do not want CPU threads to remain idle while waiting on GPU computations. \nNor can this be delegated to Haskell s foreign function interface itself, which quite reasonably assumes \nthat a foreign call does actual work on the CPU from which it is invoked! To avoid worker idleness, we \nfollow the approach of Li &#38; Zdancewic [21], making blocking resource calls only on proxy threads \nwhich stand in as an abstraction of the blocking resource. Par-monad workers communicate with these proxies \nvia channels; when a worker would otherwise make a blocking call, it instead places the corresponding \nIO callback in the appropriate channel, and returns a new IVar which will be .lled only when the operation \nis complete. (As usual, reading the IVar prematurely will save the current continuation and free the \nworker to execute other Par work.) The proxy runs in a loop, popping callbacks from its channel and executing \nthem. It writes the results to a channel read by the Par\u00admonad workers, who call put to .ll the IVar, \nwaking its waiting continuations with the result value. As shown in Figure 2, the Accelerate Resource \ns workSearch .rst checks the queue of results returned by the proxy, and if none are found, attempts \nto steal unexecuted Accelerate work for execution on a CPU backend in case the GPU is saturated.  4.5.2 \nHeterogeneous Resource 2: Distributed Execution We expose remote execution through another variant of \nspawn, called longSpawn, and follow CloudHaskell s conventions [14] for remote procedure calls and serialization: \nlongSpawn :: Serializable a . Closure (Par a) . Par (IVar a) In the type of longSpawn, the Serializable \nconstraint and Closure type constructor denote that a given unit of Par work and its return type can \nbe transported over the network. A Serializable value must have a runtime type representation via the \nData.Typeable class as well as serialization methods, both of which can be gener\u00adically derived by GHC \n[22]9. To employ longSpawn, the programmer uses a Template Haskell shorthand, making distributed calls \nonly slightly more verbose than their parallel counterparts: parVer = spawn_ (bar baz) distVer = longSpawn \n($(mkClosure bar) baz) In our implementation the Closure values contain both a lo\u00adcal version (a plain \nclosure in memory) and a serializable remote version of the computation. As with other resources in our \nwork\u00adstealing environment, longSpawned work is not guaranteed to hap\u00adpen remotely, it merely exposes \nthat possibility. One complication is that the above longSpawn requires that the user must further register \nbar with the remote execution environ\u00adment10: bar :: Int . Par Int bar x = ... remotable [ bar] A bigger \nlimitation is that functions like bar above are currently restricted to be monomorphic, which makes it \nvery dif.cult to 9 Generic serialization routines, however, are frequently much slower than routines \nspecialized for a type, so we provide ef.cient serialization routines for commonly-used types like Data.Vector \n10 Speci.cally, remotable is a macro that creates additional top level bind\u00adings with mangled names. \nThe mkClosure and mkClosureRec macros turn an ordinary identi.er into its Closure equivalent. Remotable \nfunc\u00adtions must be monomorphic, have Serializable arguments, return either a pure or a Par value, and \nonly have free variables de.ned at the top level.  de.ne higher-order combinators like parMap or parFold \nwhich are the bread and butter of the original Par-monad library. We share the hope of CloudHaskell s \nauthors that native support from the GHC compiler will improve this situation, and, if other volunteers \nare not forthcoming, plan to implement such native support ourselves in the future. Returning to our \nrunning example, a parallel merge sort could be augmented with both distributed and GPU execution with \nthe following two-line changes (assuming that gpuSort is a separate sorting procedure in the Accelerate \nEDSL): parSort :: Vector Int . Par (Vector Int) parSort vec = if length vec = gpuThreshold then gpuSpawn \n(gpuSort vec) >>= get else let n = (length vec) div 2 (l, r) = splitAt n vec indo lf . longSpawn ($(mkClosureRec \nparSort) l) r . parSort r l . get lf parMerge l r It may appear as though work never reaches the CPU. \nRecall, however, that gpuSort is effectively a hint; the work may end up on either the GPU or CPU. Ultimately, \nit will be possible for a gpuSort based on Accelerate to default to an ef.cient (e.g. OpenCL) implementation \nwhen the computation ends up on the CPU11. 5. Semantics The operational semantics of Par [25] remain \nnearly unchanged by Meta-Par. The amended rules appear in Appendix A. The only minor extension is that \nthere is more than one fork (e.g. fork1, fork2, ...) in the grammar, corresponding to the resources R1, \nR2, ... . Fortunately, this changes nothing important. The se\u00admantics do not need to model the Meta-Par \nscheduling algorithm (or any other Par scheduling algorithm). Rather, execution pro\u00adceeds inside a parallel \nevaluation structure in which any valid re\u00addex can be reduced at any time. Because these loose semantics \nare suf.cient to guarantee both determinism and deadlock/livelock freedom it is therefore safe for Meta-Par \nto use an arbitrary strategy for selecting between work-sources. Semantics of Scheduler Composition To \nensure correctness, at minimum we need a single guarantee from Resources: they must be lossless everything \npushed by a fork eventually is produced by a searchWork. Subsequently the following properties will hold: \n monoid laws: scheduler composition is an associative operation with an identity  commuting Resources \nin a stack will preserve correctness but may incur asymptotic differences in performance  These hold \nfor any scheduler which is a purely monoidal (mappended) composition of Resources as in Figure 2. Like \nother work-stealing schedulers, Meta-Par is designed for a scenario of .nite work; in.nite work introduces \nthe possibility of starvation (i.e. because other workers are busy, given piece of work may never execute). \nBecause runPar is used to schedule pure computation, fairness of scheduling Par-threads is semantically \nunimportant there are no observable effects other than the .nal value. (And the entire runPar completes \nbefore the value returned is in weak head normal form.) 11 Unfortunately, at the time of writing the \naccelerate distribution in\u00adcludes only an interpreter for CPU evaluation of Acc expressions. Time and \nSpace Usage A precise analysis of scheduler time and space usage is desirable, but is confounded both \nby (1) Meta-Par being parameterized by ar\u00adbitrary Resources and (2) by intrinsic dif.culty with the powerful \nclass of programming models that include user directed synchro\u00adnizations (i.e. reading IVars) [5, 7, \n34]. Nevertheless, while Meta-Par targets a general model, it can preserve good behavior of schedulers \nwhen certain conditions are met. For example, consider the class of programs that are strictly phased. \nThat is, given a resource stack R1 mappend R2 mappend R3, fork2 computations may call fork2, or fork1, \nbut not fork3. In a strictly phased program, all paths down the binary fork-tree proceed monotonically \nfrom deeper to shal\u00adlower Resources (i.e. fork3 to fork1). In general, this represents good practice; \nin a divide-and-conquer algorithm, the programmer should call longSpawn before spawn. In such a scenario, \nas long as the resources themselves manage work in a LIFO manner (a second requirement) the composed \nResource stack behaves the same way as a single, extended stack. We conjecture that existing analyses \nwould apply in this scenario [34], but do not treat the topic further here. 6. Evaluation In this section, \nwe analyze the performance of Meta-Par schedulers in several heterogeneous execution environments. First, \nwe com\u00adpare the performance of the Meta-Par SMP scheduler to the pre\u00adviously published Par-monad scheduler \nin both a multicore desk\u00adtop and many-core server environment. Then, we examine the per\u00adformance of parallel \nmerge sort on a multicore workstation with a GPU. Finally, we compare Meta-Par with a distributed Resource \nto other distributed Haskell implementations [14, 23]. 6.1 Traditional Par-Monad CPU Benchmarks Our \ngoal in this section is twofold: to compare the Meta-Par scheduler to the previously published scheduler \n[25], which we will call Trace (being based on the lazy trace techniques of [11, 21]), and  to analyze \nthe extent to which our results are contingent upon GHC versions and runtime system parameters, mainly \nthose affecting garbage collection.  The original work [25] studied a set of benchmarks on a 24\u00ad core \nIntel E7450. The benchmarks are all standard algorithms, so we refer the reader to the abbreviated description \nin that paper, rather than describing their purpose here. In this section we give results for blackscholes, \nnbody, mandel, sumeuler, and matmult, while omitting queens, coins, and minimax (which have fallen into \ndisrepair). We show our latest scaling numbers in Figure 4. These com\u00ad pare the Meta-Par scheduler against \nthe original Par-monad sched\u00aduler which does not suffer the overhead of indirection through Re\u00adsource \nstacks. The scaling results in Figure 4 come from our largest server platform: four 8-core Intel Xeon \nE7-4830 (Westmere) pro\u00adcessors running at 2.13GHz. Hyper-Threading was disabled via the machine s BIOS \nfor a total of 32 cores. The total memory was 64GB divided into a NUMA con.guration of 16GB per proces\u00adsor. \nThe operating system was the 64-bit version of Red Hat Enter\u00adprise Linux Server 6.2. SMP shows better \nscaling on blackscholes, but compromised performance on MatMult and mandel. Overall, we consider the \nperformance of SMP close enough to Trace. Clusterbench As with other garbage collected languages (e.g. \nJava) there are many runtime system parameters that affect GHC memory man\u00adagement and can have a large \neffect on performance12. These are the relevant runtime system options:  -A size of allocation area \n(.rst generation) -H suggested heap size -qa af.nity: pin Haskell OS threads to physical CPUs -qg enable \nparallel garbage collection (GC) for one or more generations -qb control which, if any, generations use \na load-balancing algorithm in their parallel GC Of course, varying all of these parameters results in \na combi\u00adnatorial explosion. Thus most performance evaluations for Haskell experiment by hand, .nd what \nseems like a reasonable compro\u00admise, and stick with that con.guration. To increase con.dence in our results \nwe wanted a more sys\u00adtematic approach. We decided to exhaustively explore a reasonable range of settings \nfor the above parameters (e.g. -A between 256K and 2M), resulting in 360 con.gurations. But when running \neach benchmark for 5-9 trials (and varying number of threads and sched\u00aduler implementation) each con.guration \nrequires between 324 and 1000 individual program runs and takes between 10 minutes and two hours. Thus \nexploring 360 con.gurations can take up to thirty days on one machine. To address this problem we created \na pro\u00adgram we call clusterbench that can run either in a dedicated cluster or search among a set of identically \ncon.gured workstations for idle machines to farm out benchmarking work. We used a collection of twenty \ndesktop workstations, each with an Intel Core i5-2400 (Westmere) running at 3.10 GHz with 4GB of memory \nunder 64-bit Red Hat Enterprise Linux Workstation 6.2. All of these workstations together were able to \ncomplete the 360 benchmark con.gurations in a few days. The results are summa\u00adrized in Table 5 and were \npleasantly surprising. In spite of some past problems with excessive variance in performance in response \nto GC parameters (in the GHC 6.12 era [29]), under GHC 7.4.1 we see remarkably little impact relative \nto our default settings, ex\u00adcept insofar as high settings of -H compromise performance signif\u00adicantly. \n 6.2 Case Study: Sorting We analyzed parallel merge sort performance on our GPU work\u00adstation platform, \nwhich has one quad-core Intel W3530 (Nehalem) running at 2.80GHz with 12GB and an NVIDIA Quadro 5000 \nGPU under 64-bit Red Hat Enterprise Linux Workstation 6.2. Our .rst comparison examines performance of \nthree task\u00adparallel (but not vectorized) CPU-only con.gurations, Figure 6. Each con.guration sorts Data.Vector.Storable \nvectors of 32-bit integers, and uses a parallel Haskell merge sort until falling below a .xed threshold, \nwhen one of three routines is called: Haskell: A sequential Haskell merge sort [12].  Cilk:A parallel \nC merge sort using the Cilk parallel runtime.  C: A sequential C quick sort called through the FFI. \nThis is the same sequential code as the Cilk algorithm employs at the leaves of its parallel computation. \n The pure Haskell sort does quite well, considering its limitation that while it is in-place at the \nsequential leaves (using the ST monad), it must copy the arrays during the parallel phase of the algorithm. \nIn general, ST and Par are effects that do not compose.13 12 See Table 2 in [29]. 13 However, in the \nfuture we are interested in exploring mechanisms to guarantee that slices of mutable arrays are passed \nlinearly to only one branch of a fork or the other, but not both.  Figure 4. Scaling behavior of original \nPar scheduler (top) vs. Meta-Par SMP scheduler on 32-core server platform. Error bars represent minimum \nand maximum times over .ve trials. Mergesort is memory bound beyond four cores. (The pure Cilk version \nlike\u00adwise has a maximum speedup less than .ve.) threads 1 2 3 4 --qa 0.00 -0.05 0.00 -0.43 0.00 -0.81 \n0.00 -0.97 -qb -qb0 -qb1 0.00 -0.16 -0.29 0.00 -0.35 -0.52 0.00 0.14 -0.21 0.00 0.75 0.31 -qg -qg0 -qg1 \n-0.06 0.00 -0.09 -3.34 0.00 -3.51 -7.09 0.00 -6.96 -10.37 0.00 -9.54 - 0.00 0.00 0.00 0.00 -H128M 5.08 \n-6.06 -4.91 -5.08 -H256M 6.49 2.22 -3.88 -7.51 -H512M 2.22 -5.85 -9.02 -14.42 -H1G -8.39 -24.76 -34.51 \n-42.10 -A256K -0.07 -0.11 -0.16 -0.17 -A512K 0.00 0.00 0.00 0.00 -A1M -0.05 -0.08 -0.09 -0.09 -A2M -0.07 \n-0.11 -0.14 -0.14 (Speedup/slowdown in percentages.) Figure 5. Effect of runtime system parameters on \nperformance variation. Each number represents the percentage faster or slower that the benchmark suite \nran given the particular setting of that parameter, and relative to the default setting of the parameter \n(i.e. the 0.00 row). Each percentage represents a geometric mean over all parameter settings other than \nthe selected one.   Figure 7. Median elapsed time (over 9 trials) to sort a random permutation of 224 \n32-bit integers with 4 threads. For all cases, the CPU threshold was 4096 elements. The GPU threshold \nwas 222 elements for static partitioning, and between 214 and 222 for dynamic partitioning. We include \nthe parallel Cilk routine for two reasons. First, it provides an objective performance comparison: cilksort.c \n14. It is not a world-class comparison-based CPU sorting algorithm that would require a much more sophisticated \nalgorithm including vectorization and a multi-way merge sort at sizes larger than the cache [10] but \nit is an extremely good sort, especially for its level of complexity. Second, by calling between Haskell \nand Cilk, two unintegrated schedulers, we explore a concrete example of oversubscription. When running \non a 4-core machine, the Cilk runtime dutifully spawns 4 worker threads, oblivious to the 4 Meta-Par \nworkers already contending for CPU resources. The overhead introduced by this contention makes overall \nperformance worse than both pure Haskell and hybrid Haskell/C merge sorts.  6.3 GPU Sorting Next, to \ndemonstrate the support of Meta-Par for heterogeneous resources, we examine merge sort con.gurations \nthat add a GPU Resource in addition to the parallel CPU sort of the last section. (We select the version \nthat bottoms out to a sequential C sort routine.) 14 http://bit.ly/xrgc8P Initially, we selected a sort \nroutine based on Accelerate (and coupled with an Accelerate-supporting Meta-Par Resource). How\u00adever, \ndue to temporary stability and performance problems15 we are instead making direct use of the CUDA SDK \nthrough the cuda package. Using this we call a GPU routine that is an adaptation of the NVIDIA CUDA SDK \nmergesort example that can sort vectors up to length 222. Since our benchmark input size of 224 is larger \nthan this limit, we always use a divide and conquer strategy to al\u00adlow subproblems to be computed by \nthe GPU. We examine three strategies for distributing the work between the CPU and GPU: static blocking: \nStatic partitioning of work (50% CPU, 50% GPU) where GPU calls block a Meta-Par worker. This work division \nwas selected based on the fact that our 4-core Cilk (CPU) and CUDA (GPU) sorts perform at very nearly \nthe same level.  static: Static partitioning of work (50% CPU, 50% GPU) with non-blocking GPU calls \nas described in section 4.5.  dynamic: Dynamic partitioning of work with CPU-GPU work stealing.  In \nall cases, adding GPU computation improves performance over CPU-only computation. Performance improves \nfurther by adding non-blocking GPU calls. The dynamic strategy, representa\u00adtive of the Accelerate Resource \nimplementation described in sec\u00adtion 4.5, pairs each spawn of a GPU computation with a CPU version of \nthat same computation, allowing the utilization of both resources to bene.t from work stealing. Figure \n7 shows that this yields better performance than a priori static partitioning of the work.  6.4 Comparison \nagainst other Distributed Haskells Meta-Par offers Resources for execution on distributed memory ar\u00adchitectures. \nWe have prototyped Resources based on different com\u00admunication backends: (1) haskell-mpi, Haskell bindings \nfor the Message Passing Interface (MPI), and (2) network-transport, an abstract transport layer for communication \nthat itself has multiple backends (e.g. TCP, linux pipes). All results below are from our network-transport/TCP \nResource. This results in lower perfor\u00admance than using MPI, but our MPI runs are currently unstable \ndue to a number of bugs. We evaluated the distributed performance on a cluster of 128 nodes, each with \ntwo dual-core AMD Opteron 270 processors running at 2GHz and 4GB of memory. The nodes were connected \nby a 10GB/s In.niband network and Gigabit Ethernet. Figure 8 shows a comparison between HdpH [23] and \nMeta-Par for the sumeuler benchmark which involved computing the sum of Euler s totient function between \n1 and 65536 by dividing up the work in 512 chunks. The tests were run up to 32 nodes since signi.cant \nspeedup was not observed beyond that for the above workload. The HdpH tests were run with 1 core per \nnode16, whereas the Meta-Par tests used all 4 cores on the node. Both tests measured the overall execution \ntime including the time required for startup and shutdown of the distributed instances. As seen in Figure \n8, Meta-Par performed nearly 4 times better than HdpH in this test, but HdpH continued scaling to a larger \nnum\u00ad 15 We are working with the Accelerate authors to resolve these issues. 16 In the original HdpH paper \n[23] the authors report not seeing further increases in performance when attempting multi-process (rather \nthan multi\u00adthreaded) parallelism within each node. Recent versions of HdpH, after the submission of this \npaper, have added better support for SMP parallelism. But we have not yet evaluated these; our attempt \nat using HdpH (r0661a3) with multiple threads per node led to a regression involving extremely variable \nruntimes.  Figure 8. Performance comparison and scaling behavior of Meta-Par (distributed) vs. HdpH. \nber of nodes: 30 rather than 16. The advantage in performance in this case was due entirely to the composed \nscheduler, which mixed .ne-grained SMP work-stealing with distributed work-stealing. For our chosen workload, \nthe lower bound in performance was lim\u00adited by the Meta-Par bootstrap time and the time to process a \nsin\u00adgle chunk sequentially. As a result, no signi.cant speedup was achieved beyond 16 nodes with a best \nexecution time of 9 seconds. HdpH ran using MPI in this example, and it also uses a more sophisticated \nsystem than Meta-Par for global work-stealing in which nodes prefetch work when their own work-pools \nrun low, not waiting for them to run completely dry. This helps hide the latency of distributed steals \nand may contribute to the better scaling seen in Figure 8. There is clearly much work left to be done. \nFortunately, HdpH and Meta-Par expose very similar interfaces, and we hope by standardizing on interfaces \n(type-classes) that it will possible for the community to incrementally develop and optimize a number \nof (compatible) distributed execution backends for Haskell. Distributed KMeans To compare directly against \nCloudHaskell we ported the KMeans benchmark. We have not yet run this benchmark on our cluster in\u00adfrastructure, \nbut we have run a small comparison with two work\u00aders (on the 3.10 GHz Westmere con.guration). Under this \ncon.g\u00aduration CloudHaskell takes 2540.9 seconds to process 600K 100\u00addimensional points in 4 clusters \nfor 50 iterations. Meta-Par, on the other hand took 175.8 seconds to accomplish the same. In this case \nwe believe the difference comes from (1) Meta-Par having a more ef.cient dissemination of the (>1GB) \ninput data, and (2) a high level of messaging overhead in CloudHaskell. In our microbenchmarks, CloudHaskell \nshowed a 10ms latency for sending small messages. 7. Related Work In the introduction we mentioned the \ntriple problem of scheduler s non-compositionality, complexity, and restriction to CPUs-only. Several \nsystems have attempted to solve one or more of these problems. For example, the Lithe [31] system addresses \nthe .rst the scheduler composition problem by instrumenting a number of different schedulers to support \nthe dynamic addition and subtrac\u00adtion of worker threads. Thereafter hardware resources themselves can \nbecome .rst class objects that are passed along in subroutine calls: that is, a subroutine inside one \nlibrary may receive a certain resource allocation that it may divvy among its own callees. Lithe is focused \non composing a-priori unrelated schedulers, whereas Meta-Par focuses on creating an architecture for \ncomposable het\u00aderogeneity. In the functional programming context, Manticore [15] also aims at scheduler \ndeconstruction, by providing a set of primitives used to construct many different schedulers. But, like \nLithe, Man\u00adticore only targets CPU computation. Further, to our knowledge Manticore has not been used \nto demonstrate advantages of simul\u00adtaneous use of different scheduling algorithms in the same applica\u00adtion \n(rather than between different applications). Li, Marlow, Peyton-Jones, and Tolmach s work on lightweight \nconcurrency primitives [20] is a wonderfully clear presentation of an architecture very similar to Meta-Par. \nThe core of their sys\u00adtem exposes a spartan set of primitives used by client libraries to implement callbacks, \nan arrangement much like our Resources. Their work focuses on the lower-level details of implementing \nHaskell concurrency primitives for CPUs, while Meta-Par extends the higher-level, deterministic Par-monad \nframework to heteroge\u00adneous environments, and it is encouraging to see similar architec\u00adtures yield good \nresults toward both goals. CloudHaskell [14] is a library providing Erlang-like functional\u00ad ity for Haskell. \nCloudHaskell offers a relatively large API in one package (messaging, monitors, serialization, task farming), \nand in our experiments was high-overhead. We found small messages in\u00adcurring a 10ms latency on a gigabit \nEthernet LAN. For these rea\u00adsons we ended up basing our own Meta-Par library on lower level communication \nlibraries rather than CloudHaskell. 8. Future Work and Conclusions While we have achieved some initial \nresults that show strong CPU/GPU and CPU/distributed integration, there remain many ar\u00adeas where we need \nto improve our infrastructure and apply it to more applications. In the process, we plan to continue \nto contribut\u00ading to low-level libraries for high-performance Haskell. (This work has resulted in both \nGHC17 and haskell-mpi bug .xes!) We also will work on Accelerate development until Accelerate CPU/GPU \nprograms can be written and run ef.ciently in Meta-Par. We want to ensure that Meta-Par is usable by \nthe community, and ultimately we regard it as a relatively thin layer in an ecosys\u00adtem of software including \nGPU and networking drivers, EDSLs, concurrent data structures, and so on. But by integrating disparate \ncapabilities in one framework, Meta-Par opens up interesting pos\u00adsibilities, such as automatically generating \ncode for separate phases of a recursive algorithm (e.g. distributed, parallel, sequential). 17 Atomic \ncompare-and-swap operations were missing a GC barrier.  References [1] Code for cilk runtime system. \nhttps://github.com/mirrors/ gcc/tree/cilkplus/libcilkrts. [2] Intel Cilk Plus. http://software.intel.com/en-us/ \narticles/intel-cilk-plus/. [3] Openmp article. http://intel.ly/9h7c7B. [4] Threading Building Blocks \nReference Manual, 2011. http:// threadingbuildingblocks.org/documentation.php. [5] N. S. Arora, R. D. \nBlumofe, and C. G. Plaxton. Thread scheduling for multiprogrammed multiprocessors. In Proceedings of \nthe tenth annual ACM symposium on Parallel algorithms and architectures, SPAA 98, pages 119 129, New \nYork, NY, USA, 1998. ACM. [6] S. Blagodurov, S. Zhuravlev, A. Fedorova, and A. Kamali. A case for numa-aware \ncontention management on multicore systems. In Pro\u00adceedings of the 19th international conference on Parallel \narchitectures and compilation techniques, PACT 10, pages 557 558, New York, NY, USA, 2010. ACM. [7] G. \nBlelloch, P. Gibbons, Y. Matias, and G. Narlikar. Space-ef.cient scheduling of parallelism with synchronization \nvariables. In Proceed\u00adings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, \npages 12 23, Newport, RI, jun 1997. [8] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, \nK. H. Randall, and Y. Zhou. Cilk: an ef.cient multithreaded runtime system. SIGPLAN Not., 30:207 216, \nAugust 1995. [9] M. M. Chakravarty, G. Keller, S. Lee, T. L. McDonell, and V. Grover. Accelerating haskell \narray codes with multicore gpus. In Proceedings of the sixth workshop on Declarative aspects of multicore \nprogram\u00adming, DAMP 11, pages 3 14, New York, NY, USA, 2011. ACM. [10] J. Chhugani, A. D. Nguyen, V. W. \nLee, W. Macy, M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and P. Dubey. Ef.cient implementation of sorting \non multi-core simd cpu architecture. PVLDB, 1(2):1313 1324, 2008. [11] K. Claessen. A poor man s concurrency \nmonad. J. Funct. Program., 9:313 323, May 1999. [12] D. Doel. The http://hackage.haskell.org/packagealgorithms \nfor vector arrays. vector-algorithms /vector-algorithms. package. Ef.cient [13] M. Dybdal. http://hackage.haskell.org/packagefor \nOpenCL. The /hopencl. hopencl Haskell package. bindings [32] J. Reinders. Intel Threading Building Blocks: \nOut.tting C++ for Multi-core Processor Parallelism. O Reilly Media, July 2007.  [14] J. Epstein, A. \nP. Black, and S. Peyton-Jones. Towards haskell in the cloud. In Proceedings of the 4th ACM symposium \non Haskell, Haskell 11, pages 118 129, New York, NY, USA, 2011. ACM. [15] M. Fluet, M. Rainey, J. Reppy, \nA. Shaw, and Y. Xiao. Manticore: a heterogeneous parallel language. In Proceedings of the 2007 workshop \non Declarative aspects of multicore programming, DAMP 07, pages 37 44, New York, NY, USA, 2007. ACM. \n[16] C. T. Haynes, D. P. Friedman, and M. Wand. Obtaining coroutines with continuations. Computer Languages, \n11(3.4):143 153, 1986. [17] C. Lauterback, Q. Mo, and D. Manocha. Work distribution methods on GPUs. \nUniversity of North Carolina Technical Report TR009-16. [18] D. Lea. A java fork/join framework. In Proceedings \nof the ACM 2000 conference on Java Grande, JAVA 00, pages 36 43, New York, NY, USA, 2000. ACM. [19] D. \nLeijen, W. Schulte, and S. Burckhardt. The design of a task parallel library. SIGPLAN Not., 44:227 242, \nOct. 2009. [20] P. Li, S. Marlow, S. Peyton Jones, and A. Tolmach. Lightweight concurrency primitives \nfor ghc. In Proceedings of the ACM SIGPLAN workshop on Haskell workshop, Haskell 07, pages 107 118, New \nYork, NY, USA, 2007. ACM. [21] P. Li and S. Zdancewic. Combining events and threads for scalable net\u00adwork \nservices implementation and evaluation of monadic, application\u00adlevel concurrency primitives. In Proceedings \nof the 2007 ACM SIG- PLAN conference on Programming language design and implementa\u00adtion, PLDI 07, pages \n189 199, New York, NY, USA, 2007. ACM. [22] J. P. Magalh aes, A. Dijkstra, J. Jeuring, and A. L\u00a8oh. A \ngeneric deriving mechanism for haskell. In Proceedings of the third ACM Haskell symposium on Haskell, \nHaskell 10, pages 37 48, New York, NY, USA, 2010. ACM. [23] P. Maier, P. Trinder, and H.-W. Loidl. Implementing \na High-Level Distributed-Memory parallel Haskell in Haskell, 2011. Submitted to IFL 2011. [24] G. Mainland \nand G. Morrisett. Nikola: embedding compiled gpu func\u00adtions in haskell. In Proceedings of the third ACM \nHaskell symposium on Haskell, Haskell 10, pages 67 78, New York, NY, USA, 2010. ACM. [25] S. Marlow, \nR. Newton, and S. Peyton Jones. A monad for deterministic parallelism. In Proceedings of the 4th ACM \nsymposium on Haskell, Haskell 11, pages 71 82, New York, NY, USA, 2011. ACM. [26] S. Marlow, S. Peyton \nJones, and S. Singh. Runtime support for multi\u00adcore haskell. In Proceedings of the 14th ACM SIGPLAN international \nconference on Functional programming, ICFP 09, pages 65 78, New York, NY, USA, 2009. ACM. [27] T. L. \nMcDonell. cuda. http://hackage.haskell.org/package/cuda. FFI binding to the CUDA interface for programming \nNVIDIA GPUs. [28] C. Newburn, B. So, Z. Liu, M. McCool, A. Ghuloum, S. Toit, Z. G. Wang, Z. H. Du, Y. \nChen, G. Wu, P. Guo, Z. Liu, and D. Zhang. Intel s array building blocks: A retargetable, dynamic compiler \nand embedded language. In Code Generation and Optimization (CGO), 2011 9th Annual IEEE/ACM International \nSymposium on, pages 224 235, april 2011. [29] R. Newton, C.-P. Chen, and S. Marlow. Intel Concurrent \nCollec\u00adtions for Haskell, March, 2011. MIT CSAIL Technical Report, MIT\u00adCSAIL-TR-2011-015. [30] B. O Sullivan \nand J. Tibell. Scalable i/o event handling for ghc. SIGPLAN Not., 45(11):103 108, Sept. 2010. [31] H. \nPan, B. Hindman, and K. Asanovi\u00b4c. Composing parallel software ef.ciently with Lithe. SIGPLAN Not., 45:376 \n387, June 2010. [33] T. Rompf, I. Maier, and M. Odersky. Implementing .rst-class poly\u00admorphic delimited \ncontinuations by a type-directed selective cps\u00adtransform. SIGPLAN Not., 44:317 328, Aug. 2009. [34] D. \nSpoonhower, G. E. Blelloch, P. B. Gibbons, and R. Harper. Beyond nested parallelism: tight bounds on \nwork-stealing overheads for par\u00adallel futures. In Proceedings of the twenty-.rst annual symposium on \nParallelism in algorithms and architectures, SPAA 09, pages 91 100, New York, NY, USA, 2009. ACM. [35] \nD. Spoonhower, G. E. Blelloch, R. Harper, and P. B. Gibbons. Space pro.ling for parallel functional programs. \nIn Proceedings of the 13th ACM SIGPLAN international conference on Functional programming, ICFP 08, pages \n253 264, New York, NY, USA, 2008. ACM. [36] J. Svensson, M. Sheeran, and K. Claessen. Obsidian: A domain \nspeci.c embedded language for parallel programming of graphics processors. In S.-B. Scholz and O. Chitil, \neditors, Implementation and Application of Functional Languages, volume 5836 of Lecture Notes in Computer \nScience, pages 156 173. Springer Berlin / Heidelberg, 2011. [37] D. Syme, T. Petricek, and D. Lomov. \nThe f# asynchronous program\u00adming model. In Proceedings of the 13th international conference on Practical \naspects of declarative languages, PADL 11, pages 175 189, Berlin, Heidelberg, 2011. Springer-Verlag. \n A. Appendix: Operational Semantics [Reproduced for convience in largely identical form to [25]] Figure \n9 gives the syntax of values and terms in our language. The only unusual form here is done M, which is \nan internal tool for the semantics of runPar. The main semantics for the language is a big-step operational \nsemantics written M . V meaning that term M reduces to value V in zero or more steps. It is entirely \nconventional, so we omit all its rules except one, namely (RunPar) in Figure 11. We will discuss (RunPar) \nshortly, but the important point for now is that it in turn depends on a small-step operational semantics \nfor the Par monad, written: P . Q. Here P and Q are states, whose syntax is given in Figure 9. A state \nis a bag of terms M (its active threads ), and IVars i that are either full, (M)i, or empty, ()i. In \na state, the .i.P serves (as is conventional) to restrict the scope of i in P . The notation P0 . * Pi \nis shorthand for the sequence P0 . ... . Pi where i>=0. States obey a structural equivalence relation \n= given by Fig\u00adure 10, which speci.es that parallel composition is associative and commutative, and scope \nrestriction may be widened or narrowed provided no names fall out of scope. The three rules at the bottom \nof Figure 10 declare that transitions may take place on any sub\u00ad state, and on states modulo equivalence. \nSo the . relation is inherently non-deterministic. The transitions of . are given in in Figure 11 using \nan evalua\u00adtion context E: E ::= [\u00b7] |E >>= M Hence the term that determines a transition will be found \nby look\u00ading to the left of >>=. Rule (Eval) allows the big-step reduction semantics M . V to reduce the \nterm in an evaluation context if it is not already a value. Rule (Bind) is the standard monadic bind \nsemantics. Rule (Fork) creates a new thread. Rules (New), (Get), and (PutEmpty) give the semantics for \noperations on IVars, and are straightforward: new creates a new empty IVar whose name does not clash \nwith another IVar in scope, get returns the value of a full IVar, and put creates a full IVar from an \nempty IVar. Note that there is no transition for put when the IVar is already full: in the implementation \nwe would signal an error to the programmer, but in the semantics we model the error condition by having \nno transition. Several rules that allow parts of the state to be garbage collected when they are no longer \nrelevant to the execution. Rule (GCReturn) allows a completed thread to be garbage collected. Rules (GCEmpty) \nand (GCFull) allow an empty or full IVar respectively to be garbage collected provided the IVar is not \nreferenced anywhere else in the state. The equivalences for . in Figure 10 allow us to push the . down \nuntil it encloses only the dead IVar. Rule (GCDeadlock) allows a set of deadlocked threads to be garbage \ncollected: the syntax E[get i] * means one or more threads of the given form. Since there can be no other \nthreads that refer to i, none of the gets can ever make progress. Hence the entire set of deadlocked \nthreads together with the empty IVar can be removed from the state. The .nal rule, (RunPar), gives the \nsemantics of runPar and connects the Par reduction semantics . with the functional reduc\u00adtion semantics \n.. Informally it can be stated thus: if the argument M to runPar runs in the Par semantics yielding a \nresult N, and N reduces to V , then runPar M is said to reduce to V . In order to express this, we need \na distinguished term form to indicate that the main thread has completed: this is the reason for the \nform done M. The programmer is never expected to write done M di\u00adrectly, it is only used as a tool in \nthe semantics. x, y . Variable i . IVar Values V ::= x | i | \\x |- > | M | return M | M >>= N | runPar \nM | forkn M | new | put iM | get i | done M Terms M, N ::= V | MN | \u00b7\u00b7\u00b7 States P, Q ::= M thread of computation \n| ()i empty IVar named i |(M)i full IVar named i, holding M | .i.P restriction | P | Q parallel composition \nFigure 9. The syntax of values and terms P | Q = Q | P P | (Q | R) = (P | Q) | R .x..y.P = .y..x.P .x.(P \n| Q) = (.x.P ) | Q, x/. fn (Q) P . Q P . Q P | R . Q | R .x.P . .x.Q P = P ' P ' . Q' Q' = Q P . Q Figure \n10. Structural congruence, and structural transitions. M = VM . V E[M] .E[V ] (Eval) E[return N| >>= \n|M] .E[MN] (Bind) E[forkn M] .E[return ()] | M (Fork) E[new] . .i.(()i |E[return i]), (New) i/. fn (E) \n(M)i |E[get i] .(M)i |E[return M] (Get) ()i |E[put iM] .(M)i |E[return ()] (PutEmpty) return M . (GCReturn) \n.i.()i . (GCEmpty) .i.(M)i . (GCFull) .i.(()i |E[get i] * ) . (GCDeadlock) . * (M >>= \\x.done x) done \nN, N . V runPar M . V (RunPar) Figure 11. Transition Rules    \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Modern parallel computing hardware demands increasingly specialized attention to the details of scheduling and load balancing across heterogeneous execution resources that may include GPU and cloud environments, in addition to traditional CPUs. Many existing solutions address the challenges of particular resources, but do so in isolation, and in general do not compose within larger systems. We propose a general, composable abstraction for execution resources, along with a continuation-based meta-scheduler that harnesses those resources in the context of a deterministic parallel programming library for Haskell. We demonstrate performance benefits of combined CPU/GPU scheduling over either alone, and of combined multithreaded/distributed scheduling over existing distributed programming approaches for Haskell.</p>", "authors": [{"name": "Adam Foltzer", "author_profile_id": "81548021662", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804352", "email_address": "acfoltzer@gmail.com", "orcid_id": ""}, {"name": "Abhishek Kulkarni", "author_profile_id": "81548021663", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804353", "email_address": "adkulkar@indiana.edu", "orcid_id": ""}, {"name": "Rebecca Swords", "author_profile_id": "81548021664", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804354", "email_address": "raingram@indiana.edu", "orcid_id": ""}, {"name": "Sajith Sasidharan", "author_profile_id": "81548021665", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804355", "email_address": "sasasidh@indiana.edu", "orcid_id": ""}, {"name": "Eric Jiang", "author_profile_id": "81548021666", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804356", "email_address": "erjiang@indiana.edu", "orcid_id": ""}, {"name": "Ryan Newton", "author_profile_id": "81341494555", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P3804357", "email_address": "rrnewton@indiana.edu", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364562", "year": "2012", "article_id": "2364562", "conference": "ICFP", "title": "A meta-scheduler for the par-monad: composable scheduling for the heterogeneous cloud", "url": "http://dl.acm.org/citation.cfm?id=2364562"}