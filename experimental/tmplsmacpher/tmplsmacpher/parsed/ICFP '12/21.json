{"article_publication_date": "09-09-2012", "fulltext": "\n Experience Report: Haskell in Computational Biology Noah M. Daniels Andrew Gallant Norman Ramsey Department \nof Computer Science, Tufts University {ndaniels, agallant, nr}@cs.tufts.edu Abstract Haskell gives computational \nbiologists the .exibility and rapid pro\u00adtotyping of a scripting language, plus the performance of native \ncode. In our experience, higher-order functions, lazy evaluation, and monads really worked, but pro.ling \nand debugging presented obstacles. Also, Haskell libraries vary greatly: memoization com\u00adbinators and \nparallel-evaluation strategies helped us a lot, but other, nameless libraries mostly got in our way. \nDespite the obstacles and the uncertain quality of some libraries, Haskell s ecosystem made it easy for \nus to develop new algorithms in computational biology. Categories and Subject Descriptors D.1.1 [Applicative \n(Func\u00adtional) Programming]; J.3[Biology andgenetics] Keywords memoization, stochastic search, parallel \nstrategies, QuickCheck, remote homology detection 1. Introduction Computational biologists write software \nthat answers questions about sequences of nucleic acids (genomic data) or sequences of amino acids (proteomic \ndata). When performance is paramount, software is usually written in C or C++. When convenience, read\u00adability, \nand productivity are more important, software is usually written in a dynamically typed or domain-speci.c \nlanguage like Perl, Python, Ruby, SPSS, or R. In this paper, we report on experi\u00adence using a third kind \nof language, Haskell: We had to reimplement an algorithm already implemented in C++, and the Haskell \ncode is slower. But the Haskell code was easy to write, clearly implements the underlying mathe\u00admatics \n(Section 3.1), was easy to parallelize, and performs well enough (Section 3.3). And our new tool solves \na problem that could not be solved by the C++ tool which preceded it.  Higher-order functions made it \nunusually easy to create and ex\u00adperiment with new stochastic-search algorithms (Section 3.2).  Haskell \nslowed us down in only one area: understanding and improving performance (Section 3.4).  Although the \n.rst two authors are computational biologists with little functional-programming experience, Haskell \nmade it easy for us to explore new research ideas. By contrast, our group s C++ code has made it hard \nto explore new ideas (Section 4).  The Haskell community offers libraries and tools that promise powerful \nabstractions. Some kept the promise, saved us lots of effort, and were a pleasure to use. Others, not \nso much. We couldn t tell in advance which would be which (Section 5.2).  Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 12, September 9 15, 2012, Copenhagen, \nDenmark. Copyright c &#38;#169; 2012 ACM 978-1-4503-1054-3/12/09. . . $10.00 2. The biology Proteins, \nby interacting with one another and with other molecules, carry out the functions of living cells: metabolism, \nregulation, sig\u00adnaling, and so on. A protein s function is determined by its struc\u00adture, and its structure \nis determined by the sequence of amino acids that form the protein. The amino-acid sequence is ultimately \nde\u00adtermined by a sequence of nucleic acids in DNA, which we call a gene. Given a gene, biologists wish \nto know the cellular function of the protein the gene codes for. One of the best known methods of discovering \nsuch function is to .nd other proteins of similar struc\u00adture, which likely share similar function. Proteins \nthat share struc\u00adture and function are expected to be descended from a common ancestor in biological \nterms, homologous and thus the problem of identifying proteins similar to a query sequence is called \nhomol\u00adogy detection. Computational biologists detect homologies by building algo\u00adrithms which, given \na query sequence, compare it with known proteins. When the known proteins have amino-acid sequences that \nare not too different from the query sequence, homology can be detected by a family of algorithms called \nhidden Markov mod\u00adels (Eddy 1998). But in real biological systems, proteins with sim\u00adilar structure and \nfunction may be formed from signi.cantly dif\u00adferent amino-acid sequences, which are not close in edit \ndistance. Our research software, MRFy (pronounced Murphy ), can detect homologies in amino-acid sequences \nthat are only distantly related. MRFy is available at mrfy.cs.tufts.edu.  3. The software Homology-detection \nsoftware is most often used in one of two ways: to test a hypothesis about the function of a single, \nnewly discovered protein, or to compare every protein in a genome against a library of known protein \nstructures. Either way, the software is trained on a group of proteins that share function and structure. \nThese proteins are identi.ed by a biologist, who puts their amino\u00adacid sequences into an alignment. This \nalignment relates individual amino acids in a set of homologous proteins. An alignment may be represented \nas a matrix in which each row corresponds to the amino-acid sequence of a protein, and each column groups \namino acids that play similar roles in different proteins (Figure 1). An alignment may contain gaps, \nwhich in Figure 1 are shown as dashes. A gap in row 2, column jindicates that as proteins evolved, either \nprotein 2 lost its amino acid in position j, or other proteins gained an amino acid in position j. If \ncolumn jcontains few gaps, it is considered a consensus column, and the few proteins with gaps probably \nlost amino acids via deletions. If column jcontains mostly gaps, it is considered a non-consensus column, \nand the few proteins without gaps probably gained amino acids via insertions. Once a protein alignment \nis constructed, it is used to train a hidden Markov model. A hidden Markov model is a probabilistic .nite\u00adstate \nmachine which can assign a probability to any query se\u00adquence. A protein whose query sequence has a higher \nprobability  Figure 1. A structural alignment of four proteins (C = 12) is more likely to be homologous \nto the proteins in the alignment. We write a query sequence as x1 ,...,xN , where each xi is an amino \nacid. The number of amino acids, N, can differ from the number of columns in the alignment, C. A hidden \nMarkov model carries probabilities on some states and on all state transitions. Both the probabilities \nand the states are determined by the alignment: For each column jof the alignment, the hidden Markov \nmodel has a match state Mj. The match state contains a table eMj (x) which gives the probability that \na homologous protein has amino acid x in column j.  For each column jof the alignment, the hidden Markov \nmodel has an insertion state Ij. The insertion state contains a table eIj (x) which gives the probability \nthat a homologous protein has gained amino acid x by insertion at column j.  For each column jof the \nalignment, the hidden Markov model has a deletion state Dj. The deletion state determines the proba\u00adbility \nthat a homologous protein has lost an amino acid by dele\u00adtion from column j.  The probabilities eMj \n(x) and eIj (x) are emission probabilities. A hidden Markov model also has distinguished begin and end \nstates. In our representation, each state contains a probability or a table of probabilities, and it \nis also labeled with one of these labels: data StateLabel = Mat | Ins | Del | Beg | End We use the Plan7 \nhidden Markov model, which forbids direct transitions between insertion states and deletion states (Eddy \n1998). Plan7 implies that there are exactly 7 possible transitions into the states of any column j. Each \ntransition has its own probability: A transition into a match state is more likely when column j is \na consensus column. Depending on the predecessor state, the probability of such a transition is aMj-1 \nMj , aIj-1 Mj , or aDj-1 Mj .  A transition into a deletion state is more likely when column j is a \nnon-consensus column. The probability of such a transition is aMj-1 Dj or aDj-1 Dj .  A transition into \nan insertion state is more likely when column j is a non-consensus column. The probability of such a \ntransition is aMj-1 Ij or aIj-1 Ij .  3.1 Computing probabilities using perspicuous Haskell Given a \nhidden Markov model, an established software package called HMMER (pronounced hammer ) can compute the \nproba\u00adbility that a new protein shares structure with the proteins used to train the model. The computation \n.nds the most likely path through the hidden Markov model. To make best use of .oating-point arith\u00admetic, \nthe software computes the logarithm of the probability of This model has begin and end states B and E, \nas well as four nodes, each containing an insertion state I, a match state M, and a deletion state D. \n Figure 2. A hidden Markov model (C = 4) each path, by summing the logs of the probabilities on the states \nand edges of the path (Viterbi 1967). The path that maximizes the log of the probability is the most \nlikely path. The computation is speci.ed on the left-hand side of Figure 3. A probability VjM(i) represents \nthe probability of the most likely path of the .rst i amino acids in the query sequence, terminating \nwith placement of amino acid xi in state Mj. Probabilities VjI(i) and VjD(i) are similar. The equations \nare explained very clearly by Durbin et al. (1998, Chapter 5) To be able to use Haskell, we had to reimplement \nthe standard algorithm for solving Viterbi s equations. Haskell made it possible for us to write code \nthat looks like the math, which made the code easy to write and gives us con.dence that it is correct. \nOur code represents a query sequence as an immutable array of amino acids. In idiomatic Haskell, we might \nrepresent an individual amino acid xi using a value of algebraic data type: data Amino = Ala | Cys | \nAsp | Glu | ... --not used But our models use a legacy .le format in which each amino acid is a small \ninteger used only to index arrays. We therefore chose newtype AA = AA Int Our legacy .le format also \nnegates the log of each probability, making it a positive number. The negated logarithm of a probability \nis called a score. newtype Score = Score Double Type Score has a limited Num instance which permits \nscores to be added and subtracted but not multiplied. In a real hidden Markov model, each probability \nis represented as a score. Our code implements a transformed version of Viterbi s equations which operates \non scores. The transformed equations are shown on the right-hand side of Figure 3. They minimize the \nscore (the negated log probability) for each combination of column j, amino acid xi, and state Mj, Ij, \nor Dj. A model is represented as a sequence of nodes; node j includes states Mj, Ij, and Dj, as well \nas the probabilities of transitions out of that node. Each node contains tables of emission scores  \n. ''M a + Vj-1 (i-1) . Mj-1 Mj . M 'M ' ''I . log aMj-1 Mj + Vj-1 (i-1) Vj (i)= eMj (xi) + min aIj-1 \nMj + Vj-1 (i-1) MI . eMj (xi) ''D Vj (i) = log + max log aIj-1 Mj + Vj-1 (i-1) a + Vj-1 (i-1) qxi . \nDDj-1 Mj log aDj-1 Mj + Vj-1 (i-1) . a ' + V 'M(i-1) . 'I ' Mj Ij j M Vj (i)= eIj (xi) + min ''I IeIj \n(xi) log aMj Ij + Vj (i-1) a + Vj (i-1) Vj (i) = log qxi + max IIj Ij log aIj Ij + Vj (i-1) . ''M a + \nVj-1 (i) . 'DMj-1 Dj M V (i) = min log aMj-1 Dj + Vj-1 (i) j ''D D a + Vj-1 (i) Vj (i) = max Dj-1 Dj \nlog aDj-1 Dj + VD j-1 (i) '' (x) 'MM ass = -log ass es(x)= -log esqx Vj (i)= -Vj (i) Figure 3. Viterbi \ns equations, in original and negated forms e ' and e ' . These tables are read by function eScore, whose \nMj Ij speci.cation is eScore sj i = e ' sj (xi). We place the transition probabilities into a record \nin which each .eld is labeled ss , where s and s form one of the 7 permissible pairs of state labels: \nnewtype TProb = TProb { logProbability :: Score } data TProbs = TProbs { m_m :: TProb, m_i :: TProb, \nm_d :: TProb , i_m :: TProb, i_i :: TProb , d_m :: TProb, d_d :: TProb } These scores are read by \nfunction aScore, whose speci.cation is aScore ss (j-1) = asj-1 s j . Scores can be usefully attached \nto many types of values, so we have de.ned a small abstraction: data Scored a = Scored { unScored :: \n!a, scoreOf :: !Score} (/+/) :: Score -> Scored a -> Scored a Think of a value of type Scored a as a \ncontainer holding an a with a score written on the side. The /+/ function adds to the score without touching \nthe container. Function fmap is also de.ned; it applies a function to a container s contents. Finally, \nwe made Scored an instance of Ord. Containers are ordered by score alone, so applying minimum to a list \nof scored things chooses the thing with the smallest (and therefore best) score. Armed with our models \nand with the Scored abstraction, we at\u00adtacked Viterbi s equations. The probability in each state is a \nfunc\u00adtion of the probabilities in its predecessor states, and all probabili\u00adties can be computed by a \nclassic dynamic-programming algorithm. This algorithm starts at the begin state, computes probabilites \nin nodes 1 through C in succession, and terminates at the end state. One of us implemented this algorithm, \nstoring the probabilities in an array. The cost was O(|N|\u00d7|C|); in MRFy, C and N range from several hundred \nto a few thousand. Another of us was curious to try coding Viterbi s equations di\u00adrectly as recursive \nfunctions. Like a recursive Fibonacci function, Viterbi s functions, when implemented na\u00a8ively, take \nexponential time. But like the Fibonacci function, Viterbi s functions can be memoized. For example, \nto compute Vj 'M(i) using the equation at the top right of Figure 3, we de.ne vee Mat ji. The equation \nadds e ' (xi), computed with eScore, to a minimum of sums. Mj ''s The sum of an a s term and a Vj-1 \n(i - 1) term is computed by s function avSum, in which the terms are computed by aScore and vee , respectively: \nvee Mat j i = fmap (Mat cons ) $ eScore Mat j i /+/ minimum (map avSum [Mat, Ins, Del]) where avSum prev \n= aScore prev Mat (j-1) /+/ vee prev (j-1) (i-1) What about the call to fmap (Mat cons )? This call \nperforms a computation not shown in Figure 3: MRFy computes not only the probability of the most likely \npath but also the path itself. Function (Mat cons ) adds M to a path; we avoid (Mat :) for reasons explained \nin Section 3.3 below. Function vee is the memoized version of vee . Calling vee produces the same result \nas calling vee , but faster: vee = Memo.memo3 (Memo.arrayRange (Mat, End)) (Memo.arrayRange (0, numNodes)) \n (Memo.arrayRange (-1, seqlen)) vee Functions Memo.memo3 and Memo.arrayRange come from Luke Palmer \ns Data.MemoCombinators package. The value numNodes represents C, and seqlen represents N. Memoization \nmakes vee perform as well as our classic dynamic\u00adprogramming code. And the call to Memo.memo3 is the \nonly part of the code devoted to dynamic programming. By contrast, stan\u00addard implementations of Viterbi \ns algorithm, such as in HMMER, spend much of their code managing dynamic-programming tables. Haskell \nenabled us write simple, performant code with little effort. Because the memoized version so faithfully \nresembles the equa\u00adtions in Figure 3, we retired the classic version.  3.2 Exploring new algorithms \nusing higher-order functions We use Viterbi s algorithm to help detect homologies in proteins with speci.c \nkinds of structure. When a real protein folds in three dimensions, amino acids that are far away in the \none-dimensional sequence can be adjacent in three-dimensional space. Some groups of such acids are called \nbeta strands. Beta strands can be hydrogen\u00adbonded to each other, making them stuck together. These beta \nstrands help identify groups of homologous proteins. MRFy detects homologous proteins that include hydrogen-bonded \nbeta strands; using prior methods, many instances of this problem are intractable. Beta strands require \nnew equations and richer models of protein structure. When column j of an alignment is part of a beta \nstrand and is paired with another column p(j), the probability of .nding amino acid xi in column j depends \non the amino acid x ' in col\u00adumn p(i). If x ' is in position i ' in the query sequence, Viterbi s equations \nare altered; for example, Vj 'M(i) depends not only on 'M 'M Vj-1 (i-1) but also on Vp(j) (i ' ). The \ndistance between jand p(j) can be as small as a few columns or as large as a few hundreds of columns. \nBecause Vj 'M(i) depends not only on nearby values but 'M also on Vp(j) (i ' ), dynamic programming cannot \ncompute the max\u00adimum likelihood quickly (Menke et al. 2010; Daniels et al. 2012). The new equations are \naccompanied by a new model. Within a beta strand, amino acids are not inserted or deleted, so a bonded \nEach shaded node represents a beta-strand position. Nodes con\u00adnected by dashed edges are hydrogen-bonded. \n  Figure 4. A Markov random .eld with two beta-strand pairs pair of beta strands is modeled by a pair \nof sequences of match states. Between beta strands, the model is structured as before. The combined model, \nan example of which is shown in Figure 4, is called a Markov random .eld. MRFy treats the beta strands \nin the model as beads which can slide along the query sequence. A positioning of the beta strands is \ncalled a placement. A placement s likelihood is computed based on frequencies of amino-acid pairs observed \nin hydrogen-bonded beta strands (Cowen et al. 2002). Given a placement, the maximum like\u00adlihood of the \nrest of the query sequence, between and around beta strands, is computed quickly and exactly using Viterbi \ns algorithm. This likelihood is conditioned on the placement. MRFy searches for likely placements stochastically. \nMRFy imple\u00adments random hill climbing, simulated annealing, multistart sim\u00adulated annealing, and a genetic \nalgorithm. These algorithms share much code, and MRFy implements them using higher-order func\u00adtions, \nexistentially quanti.ed types, and lazy evaluation. We describe MRFy s search abstractly: MRFy computes \na se\u00adquence of points in a search space. The type of point is existentially quanti.ed, but it is typically \na single placement or perhaps a popu\u00adlation of placements. Each point also has a Score; MRFy looks for \npoints with good scores. Ideally, MRFy would use the now-classic, lazy, modular technique advocated by \nHughes (1989), in which one function computes an in.nite sequence of points, and another function uses \na .nite pre.x to decide on an approximation. But because MRFy s search is stochastic, making MRFy s search \nmodular is not so easy. To illustrate the dif.culties, we discuss our simplest search: random hill climbing. \nFrom any given point in the search space, this search moves a random distance in a random direction. \nIf the move leads to a better point, we call it useful; otherwise it is useless. data Utility a = Useful \na | Useless (We also use Useful and Useless to tag points.) With luck, an in.nite sequence of useful \nmoves converges at a local optimum. MRFy s search path follows only useful moves; if a move is use\u00adless, \nMRFy abandons it and moves again (in a new random direc\u00adtion) from the previous point. Ideally, MRFy \nwould search by com\u00adposing a generator that produces an in.nite sequence of moves, a .lter that selects \nthe useful moves, and a test function that enu\u00admerates .nitely many useful moves and returns the .nal \ndestina\u00adtion. But a generator may produce an in.nite sequence of useless moves. (For example, if MRFy \nshould stumble upon a global op\u00adtimum, every move from that point would be useless.) Given an in.nite \nsequence of useless inputs, a .lter would not produce any values, and the search would diverge. We address \nthis problem by combining generate and .lter into a single abstraction, which has type SearchGen pt r. \nType vari\u00adable pt is a point in the search space, and r is a random-number generator. Rand r is a lazy \nmonad of stochastic computations: data SearchGen pt r = SG { pt0 :: Rand r (Scored pt) , nextPt :: \nScored pt -> Rand r (Scored pt) , utility :: Move pt -> Rand r (Utility (Scored pt)) } The monadic \ncomputation pt0 randomly selects a starting point for search; nextPt produces a new point from an existing \npoint. Because scoring can be expensive, both pt0 and nextPt use scored points, and they can reuse scores \nfrom previous points. To tell if a point returned by nextPt is useful, we call the utility function, \nwhich scrutinizes a move represented as follows: data Move pt = Move { older :: Scored pt , younger \n:: Scored pt , youngerCCost :: CCost } The decision about utility uses not only a source of randomness \nbut also the cumulative cost of the point, which we de.ne to be the number of points explored previously. \nThe cumulative cost of the current point is also the age of the search, and in simulated annealing, for \nexample, as the search ages, the utility function becomes less likely to accept a move that worsens the \nscore. Using these pieces, function everyPt produces an in.nite se\u00adquence containing a mix of useful \nand useless moves: everyPt :: RandomGen r => SearchGen pt r -> CCost -> Scored pt -> Rand r [CCosted \n(Utility (Scored pt))] everyPt sg cost startPt = do successors <-mapM (nextPt sg) (repeat startPt) \n tagged <-zipWithM costedUtility successors [succ cost..] let (useless, CCosted (Useful newPt) newCost \n: _) = span (isUseless . unCCosted) tagged (++) (CCosted (Useful startPt) cost : useless) <$>  everyPt \nsg newCost newPt where costedUtility pt cost =  utility sg move >>= \\u -> return $ CCosted u cost \nwhere move = Move { older = startPt, younger = pt , youngerCCost = cost } Both nextPt and utility are \nmonadic, but we can still exploit laziness: from its starting point, everyPt produces an in.nite list \nof randomly chosen successor points, then calls costedUtility to tag each one with a cumulative cost \nand a utility. We hope that if you look carefully at how successors is computed, you will understand \nwhy we separate pt0 from nextPt instead of using a single function that produces an in.nite list: We \ndon t want the in.nite list that would result from applying nextPt to many points in succession; we want \nthe in.nite list that results from applying nextPt to startPt many times in succession, each time with \na different source of randomness. Once the successors have been computed and tagged, span .nds the .rst \nuseful successor. In case there is no successor, everyPt also returns all the useless successors. If \nwe do .nd a useful suc\u00adcessor, we start searching anew from that point, with a recursive call to everyPt. \n(Because everyPt is monadic, the points accu\u00admulated so far are appended to its result using the <$> \noperator.) The most informative part of everyPt is last expression of the do block, which shows that \nthe result begins with a useful point, is fol\u00adlowed by a (possibly in.nite, possibly empty) list of useless \npoints, and then continues recursively with another call to everyPt.  The rest of the search uses Hughes \ns classic composition of gen\u00aderator and test function. Because our code is monadic, we use the monadic \ncomposition operator =<<, which is the bind operator with its arguments swapped: search :: RandomGen \nr => SearchGen pt r -> SearchStop pt -> Rand r (History pt) search strat test = return . test =<< everyPt \nstrat 0 =<< pt0 strat The test function has type SearchStop pt: type SearchStop pt = [CCosted (Utility \n(Scored pt))] -> History pt Type History pt retains only the useful points. (Internally, MRFy needs \nonly the .nal useful point, but because we want to study how different search algorithms behave, we keep \nall the useful points.) The de.nition of SearchStop reveals two forms of non-modularity which are inherent \nin MRFy s search algorithm. First, we need Utility, because if we omit the useless states, search might \nnot terminate. Second, we need CCosted, because some of our test functions decide to terminate based \neither on the cumulative cost of the most recent point or on the difference between costs of successive \nuseful points. Despite these non-modular aspects, the search interface provides ample scope for experiments. \nRandom hill climbing took 50 lines of code and one day to implement. Simulated annealing required only \na new utility function, which took 15 lines of code and half an hour to implement. (Hill climbing accepts \na point if and only if it scores better than its predecessor; simulated annealing may accept a point \nthat scores worse.) Our genetic algorithm uses very similar functions, except for nextPt: recombination \nof par\u00adent placements took forty lines of code and a full day to imple\u00adment. We re not entirely happy \nwith the way we re writing all the indi\u00advidual functions. In particular, SearchStop functions aren t \ncom\u00adposable; we can t, for example, combine two functions to say that we d like to stop if scores aren \nt improving or if we ve tried a thou\u00adsand points, whichever comes .rst. Eventually, we d like to have \ncombinator libraries for SearchStop and nextPt, at least.  3.3 Performance At each point in its search, \nMRFy calls vee several times. Our vee function computes a Scored [StateLabel], that is, an optimal path \nand its score. But at intermediate points in MRFy s search, MRFy uses only the score. Even though Haskell \nevalua\u00adtion is lazy, vee still allocates thunks that could compute paths. To measure the relevant overhead, \nwe cloned vee and modi.ed it to compute only a score, with no path. This change improved run time by \nnearly 50%. Could we keep the improvement without maintaining two versions of vee ? In Lisp or Ruby we \nwould have used macros or metapro\u00adgramming, but we were not con.dent of our ability to use Tem\u00adplate \nHaskell. Instead, we used higher-order functions. As shown in Section 3.1, vee does not use primitive \n(:) but instead uses an unknown function cons, which is passed in. To get a path, we pass in primitive \n(:); to get just a score, we pass in \\__ ->[]. This trick is simple and easy to implement, and it provides \nthe same speedup as the cloned and modi.ed code. But we worry that it may work only because of undocumented \nproperties of GHC s inliner, which may change. Even with this trick, MRFy s implementation of Viterbi \ns algo\u00adrithm is much slower than the C++ version in MRFy s predeces\u00adsor, SMURF. For example, on a microbenchmark \nthat searches for Speedup 7 6 5 4 3 2 1 0 Figure 5. MRFy s parallel speedup on an 8-bladed beta propeller \na structural motif of 343 nodes in a protein of 2000 amino acids us\u00ading only Viterbi s algorithm and \nno beta-strand information, MRFy takes 2.32 seconds and SMURF takes 0.29 seconds. But MRFy s job is not \nto run Viterbi s algorithm on large models; MRFy s job is to detect homologies to structures for which \nboth Viterbi s algorithm and SMURF s more complex algorithm are un\u00adsuited. MRFy can solve problems that \nSMURF cannot. For ex\u00adample, we tried both programs on a complex, 12-stranded beta sandwich model. The \nmodel contains 252 nodes, 97 of which ap\u00adpear in the 12 beta strands. MRFy computes an alignment in under \na minute, but SMURF allocates over 16GB of memory and does not terminate even after eight hours. We also \nbenchmarked MRFy using a model of an 8-bladed beta propeller. The model has 343 nodes, of which 178 appear \nin 40 beta strands. The segments between beta strands typically have at most 10 nodes. We used a query \nsequence of 592 amino acids, but each placement breaks the sequence into 41 pieces, each of which typically \nhas at most 20 amino acids. Because MRFy can solve the models between the beta strands independently, \nthis benchmark has a lot of parallelism, which Haskell made it easy to exploit. Using Control.Parallel, \nparallelizing the computation was as easy as substituting parmap rseq for map. Figure 5 shows speedups \nwhen using from 1 to 48 of the cores on a 48-core, 2.3GHz AMD Opteron 6176 system. Errors are estimated \nfrom 5 runs. After about 12 cores, where MRFy runs 6 times as fast as sequential code, speedup rolls \noff. By running 4 instances of MRFy in parallel on different searches, we hope to be able to use all \n48 cores with about 50% ef.ciency.  3.4 Awkward debugging and testing Our experience writing code and \ntrying new ideas was excel\u00adlent, as was the ease of parallelizing MRFy. Higher-order func\u00adtions, memoization, \nlaziness, and parallel strategies really worked. But we also encountered obstacles that prevented functional \npro\u00adgramming from working as well as we would have liked. The most signi.cant obstacles were in debugging \nand testing. We had a hard time diagnosing run-time errors. We expected some run-time errors; our group \ns legacy .le format is poorly docu\u00admented and hard to deal with. (When beta strands overlap and are doubly \npaired, even the invariants of the format are unclear.) But using Haskell, we found the errors hard to \ndiagnose. Calls to trace littered our code, even when relegated to wrapper functions. We didn t know \nabout the backtrace feature of GHC s pro.ler, and even after we learned about it, it didn t help: the \npro.ler can be used only on very small test cases, which didn t always trigger the errors. This same \nlimitation affected GHCi s debugger; in GHCi, our vee function is too slow to be runnable on nontrivial \ninputs. Moreover, GHCi s debugger can set breakpoints only at top-level functions or at speci.c column/line \npositions, which made debugging the mem\u00adoized vee function impractical. In August 2011, Lennart Au\u00adgustsson \nsaid that the biggest advantage of Strict Haskell is getting a stack trace on error, and Simon Marlow \nsaid that he may have .gured out how to track call stacks properly in a lazy functional language. We \ncan t wait.  Our dif.culties with debugging led to internal disagreements. The junior members of our \nteam wanted to apply the debugging skills they had honed through years of imperative programming. But \nthese skills did not transfer well to Haskell. The senior mem\u00adber of the team kept repeating that a proper \napproach to debugging Haskell code should involve QuickCheck. But mere exhortation was unhelpful. Only \nthe senior member of our team was able to use QuickCheck easily. In retrospect, we have identi.ed some \nobstacles that pre\u00advented the junior people from using QuickCheck. The examples and tutorials we found \nfocused predominantly on writing and testing properties using data types that al\u00adready implemented class \nArbitrary. We didn t understand the Arbitrary class very well, perhaps because the overloaded arbitrary \nvalue is not a function. (For programmers accus\u00adtomed to object-oriented dispatch on arguments, it is \nhard to grasp how Haskell s type-class system can .nd the proper ver\u00adsion of arbitrary using only the \ntype of a value.)  Our dif.culties were compounded by a weak understanding of monads. We were too baf.ed \nby QuickCheck s Gen monad to grasp its importance.  We continually overlooked the critical role of shrinking. \nAs a result, on the one or two occasions we did use QuickCheck, the counterexamples were too large to \nbe informative.  Because of these obstacles, we wrote thousands of lines of code without ever de.ning \nan instance of Arbitrary and therefore without looking hard at QuickCheck. After the fact, we were over\u00adwhelmed \nby the work involved in writing and testing instances of Arbitrary. The work got done only when the whole \nteam pitched in to meet the deadlines for this paper. At the last minute, QuickCheck did .nd a bug in \nour implementa\u00adtion of Viterbi s algorithm: we had omitted the score for the tran\u00adsition from the .nal \nnode of the hidden Markov model to the spe\u00adcial end state. Without QuickCheck, we probably wouldn t have \nknown anything was wrong.  4. Our previous experience compared Our Haskell code for hidden Markov models \nand Viterbi s algo\u00adrithm solves the same problems as existing C++ code. Other re\u00adsearchers using Haskell \nmay also have to reimplement code, but in computational biology, reimplementing existing algorithms is \nun\u00adremarkable. For example, both SMURF and HMMER also contain new implementations of hidden Markov models \nand Viterbi s algo\u00adrithm. When performance has mattered, members of our group, like other computational \nbiologists, have used C++. To compare our Haskell experience with our C++ experience, we discuss three \ntools: Matt (Menke et al. 2008) is used to create alignments like that shown in Figure 1. It comprises \nabout 12,000 lines of C++. The only external tools or libraries it uses are zlib and OpenMP, and its \ninitial development took two years.  SMURF (Menke et al. 2010) is used to detect homologous proteins \nin the presence of paired beta strands. It comprises about 9,000 lines of C++, of which about 1,000 lines \nare shared  with Matt. It uses no external tools or libraries, and its initial development took a year \nand a half. It uses multidimensional dynamic programming to exactly compute the alignments for which \nMRFy relies on stochastic search. As a result, in the presence of complex beta-strand topologies, SMURF \nis com\u00adputationally intractable. MRFy is used to detect homologous proteins in the presence of paired \nbeta strands; it effectively supplants SMURF. It com\u00adprises about 2,500 lines of Haskell, about 500 of \nwhich are devoted to tests, QuickCheck properties, and generators. Nei\u00adther Matt nor SMURF includes test \ncode. MRFy uses sev\u00aderal external tools and libraries, of which the most notable are Parsec, the BioHaskell \nbioinformatics library, and the li\u00adbraries Data.MemoCombinators, Control.Parallel, and Data.Vector. MRFy \ns initial development took about three months. Like much research software, all three tools were written \nin haste. We have experience modifying the older tools. We modi.ed Matt to use information about sequences \nas well as structure. The modi.cation added 2,000 lines of code, and it calls external sequence aligners \nthat we did not write. We thought the modi.cation would take three months, but it took most of a year. \nMatt uses such data structures as mutable oct-trees, vec\u00adtors, and arrays. It uses clever pointer arithmetic. \nThe mutable data structures were dif.cult to repurpose, and the pointer arith\u00admetic was too clever: nearly \nevery change resulted in new seg\u00adfaults. We had hoped to extend Matt further, with support for partial \nalign\u00adments, which we expected to require only a cosmetic manipulation of the output. But this feature \nwound up requiring deep information about Matt s data structures, and we had to give up. We believe we \ncould write an equivalent tool in Haskell, with most of Matt s per\u00adformance, in at most nine months. \nOur most painful experience was adding simulated evolution to SMURF (Daniels et al. 2012). Although simulated \nevolution represents a relatively minor enhancement, just understanding the existing code took several \nmonths. We built MRFy quickly, and we expect that higher-order functions will make MRFy easy to extend. \nEach new addition to MRFy s stochastic search has taken at most a day to implement. Haskell encourages \nhasty programmers to slow down. We have to get the types right, which makes it hard to write very large \nfunc\u00adtions. To get the code to typecheck, we have to write type sig\u00adnatures, which also serve as documentation. \nAnd once the types are accepted by the compiler, it is not much more work to write contracts for important \nfunctions. MRFy is still hasty work. Many types could be simpli.ed; we re sure we ve missed opportunities \nfor abstraction; and we know that MRFy s decomposition into modules could be improved. But despite being \nhasty program\u00admers, we produced code that is easy to understand and easy to ex\u00adtend. Our hastily written \nHaskell beats our hastily written Ruby and C++. Looking beyond our research group to computational biology \nmore broadly, our experience with other software is better. Little of it is written in functional languages, \nbut much of the software shared by the community is excellent. MRFy s training component was de\u00adrived \nfrom that of HMMER, and working with the HMMER code\u00adbase was pleasant; data structures and their mutators \nare well docu\u00admented. There is a BioHaskell library, part of which we use, but it is not nearly as complete \nas BioPython or BioRuby, which are heav\u00adily used in the community. We hope that tools for computational \nbiology in Haskell continue to mature.  5. What can you learn from our experience? If you are a computational \nbiologist and you are interested in func\u00adtional programming, you don t need extensive preparation to \nbe productive in Haskell. Two of us (Daniels and Gallant) are graduate students. Daniels has taken a \nseminar in functional programming, which included some Haskell; Gallant has taken a programming\u00adlanguages \ncourse which included signi.cant functional program\u00adming but no Haskell. Ramsey is a professor who has \nused Haskell for medium-sized projects, but his contributions to MRFy have been limited, mostly to post \nhoc refactoring and testing. 5.1 Obstacles to be overcome We had quite some dif.culty pro.ling, but we \nhope that this dif.\u00adculty may be mitigated by new pro.ling tools released early in 2012 with GHC 7.4. \nGHC assigns costs to cost centers (Sansom and Peyton Jones 1997), and in GHC 7.0, which we used for most \nof MRFy s development, cost-center annotations had to be added manually to nested functions. Although \nthese annotations made our code so ugly that we felt compelled to remove them, they did enable us to \nimprove the performance of vee as discussed in Section 3.3. GHC 7.4 provides more sophisticated pro.ling \ntools, which we look forward to using. Dif.culties using Cabal to enable pro.ling of installed libraries \nmay remain. Like other functional programmers, we have found that once we have our types right, our code \nis often right. But MRFy computes with arrays and array indices, and in that domain, types don t help \nmuch. Bounds violations lead to run-time errors, which we have not been able to identify any systematic \nway to debug. GHC s pro.ler can provide stack traces, but we found this information dif.cult to discover, \nand as noted above, there are obstacles to pro.ling. We re aware that debugging lazy functional programs \nhas been a topic of some research, but one of the biggest obstacles we encountered to using Haskell is \nthat we have had to abandon our old approaches to debugging. Ideally we would use QuickCheck to .nd bugs, \nbut as we mention in Section 3.4, we found obstacles. We have now overcome these obstacles, but we sorely \nregret not doing so earlier. In light of our experience, we will institute a new programming practice: \nwhenever we introduce a new data type, we will write the instance of Arbitrary right away, while relevant \ninvariants are still fresh in memory. When invariants are not enforced by Haskell s static type system, \nwe will write them as Haskell predicates. We can then immediately run QuickCheck on each predicate, to \nverify that our Arbitrary instance agrees with the predicate. For each predicate p we can also check \nfmap (all p . shrink) arbitrary. 5.2 Information that will help you succeed If you want to use Haskell \nin your research, we believe that you must have enough experience with functional programming that you \ncan build all the code you need, not only the code that is easy to write in a functional language. Implementing \nViterbi s equations in Haskell was pure joy. Writing an iterative search in purely functional style was \neasy. Transforming data in the HMMER .le format, without using mutable state the way the C++ code does, \nwas dif.cult. While the Haskell community offers many enticing tools, libraries, and packages, not all \nof them are worth using. Some are not ready for prime time, and some were once great but are no longer \nmain\u00adtained. The great packages, like Data.MemoCombinators and Par\u00adallel Strategies, are truly great. \nBut for amateurs, it s not always easy to tell the great packages from the wannabes and the has\u00adbeens. \nAnd even some of the great packages could be better docu\u00admented, with more examples. As in any endeavor, \naccess to experts helps. We would have been better off if our in-house expert had been an enthusiastic \nstudent and not a busy professor. But we have been surprised and pleased by the help available from faraway \nexperts on Stack Over.ow and on Haskell mailing lists. Although a local expert makes things easier, one \nis not absolutely necessary.  6. Conclusion A little knowledge of and a lot of love for functional program\u00adming \nenabled us to carry out a successful research project in a lan\u00adguage that computational biologists seldom \nuse. If you want to use Haskell or one of your graduate students wants to use Haskell you can succeed. \n  Acknowledgments Anonymous referees spurred us to think more deeply about laziness and to write more \ncarefully about performance. Josef Svenningsson suggested a correspondence between MRFy s search and \nstream fusion, which led us eventually to the Utility type. Koen Claessen instantly diagnosed an inappropriately \nstrict implementation of the Rand monad. We also thank Lenore Cowen, Kathleen Fisher, Ben Hescott, Brad \nLarsen, and Nathan Ricci. This work was funded in part by NIH grant 1R01GM080330.  References Lenore \nCowen, Philip Bradley, Matt Menke, Jonathan King, and Bonnie Berger. Predicting the beta-helix fold from \nprotein sequence data. Journal ofComputational Biology, 2002. Noah Daniels, Raghavendra Hosur, Bonnie \nBerger, and Lenore Cowen. SMURFLite: combining simpli.ed Markov random .elds with simulated evolution \nimproves remote homology de\u00adtection for beta-structural proteins into the twilight zone. Bioin\u00adformatics, \nMarch 2012. Rirchard Durbin, Sean Eddy, Anders Krogh, and Graeme Mitchi\u00adson. Biological Sequence Analysis: \nProbabilistic Models of Pro\u00adteins andNucleic Acids. Cambridge University Press, May 1998. Sean Eddy. \nPro.le hidden Markov models. Bioinformatics, 14: 755 763, 1998. John Hughes. Why functional programming \nmatters. The Com\u00adputerJournal, 32(2):98 107, April 1989. Matthew Menke, Bonnie Berger, and Lenore Cowen. \nMatt: local .exibility aids protein multiple structure alignment. PLoS Com\u00adputational Biology, 2008. \nMatthew Menke, Bonnie Berger, and Lenore Cowen. Markov ran\u00addom .elds reveal an N-terminal double beta-propeller \nmotif as part of a bacterial hybrid two-component sensor system. Pro\u00adceedings of the National Academy \nof Science, 2010. Patrick Sansom and Simon L Peyton Jones. Formally based pro\u00ad.ling for higher-order \nfunctional languages. ACM TOPLAS, 19 (2):334 385, 1997. Andrew Viterbi. Error bounds for convolutional \ncodes and an asymptotically optimum decoding algorithm. IEEE Transac\u00adtions on Information Theory, 13(2):260 \n269, April 1967. You can live to surf the Haskell wave, but if you slide off the crest, you drown.  \n \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Haskell gives computational biologists the flexibility and rapid prototyping of a scripting language, plus the performance of native code. In our experience, higher-order functions, lazy evaluation, and monads really worked, but profiling and debugging presented obstacles. Also, Haskell libraries vary greatly: memoization combinators and parallel-evaluation strategies helped us a lot, but other, nameless libraries mostly got in our way. Despite the obstacles and the uncertain quality of some libraries, Haskell's ecosystem made it easy for us to develop new algorithms in computational biology.</p>", "authors": [{"name": "Noah M. Daniels", "author_profile_id": "81548018883", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P3804348", "email_address": "ndaniels@cs.tufts.edu", "orcid_id": ""}, {"name": "Andrew Gallant", "author_profile_id": "81548018884", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P3804349", "email_address": "agallant@cs.tufts.edu", "orcid_id": ""}, {"name": "Norman Ramsey", "author_profile_id": "81100300481", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P3804350", "email_address": "nr@cs.tufts.edu", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364560", "year": "2012", "article_id": "2364560", "conference": "ICFP", "title": "Experience report: Haskell in computational biology", "url": "http://dl.acm.org/citation.cfm?id=2364560"}