{"article_publication_date": "09-09-2012", "fulltext": "\n Nested Data-Parallelism on the GPU Lars Bergstrom John Reppy University of Chicago University of Chicago \nlarsberg@cs.uchicago.edu jhr@cs.uchicago.edu Abstract Graphics processing units (GPUs) provide both \nmemory bandwidth and arithmetic performance far greater than that available on CPUs but, because of their \nSingle-Instruction-Multiple-Data (SIMD) ar\u00adchitecture, they are hard to program. Most of the programs \nported to GPUs thus far use traditional data-level parallelism, performing only operations that operate \nuniformly over vectors. NESL is a .rst-order functional language that was designed to allow programmers \nto write irregular-parallel programs such as parallel divide-and-conquer algorithms for wide-vector \nparallel computers. This paper presents our port of the NESL implementa\u00adtion to work on GPUs and provides \nempirical evidence that nested data-parallelism (NDP) on GPUs signi.cantly outperforms CPU\u00adbased implementations \nand matches or beats newer GPU languages that support only .at parallelism. While our performance does \nnot match that of hand-tuned CUDA programs, we argue that the nota\u00adtional conciseness of NESL is worth \nthe loss in performance. This work provides the .rst language implementation that directly sup\u00adports \nNDP on a GPU. Categories and Subject Descriptors D.3.0 [Programming Lan\u00adguages]: General; D.3.2 [Programming \nLanguages]: Language Classi.cations Applicative (Functional) Programming, Concur\u00adrent, distributed, and \nparallel languages; D.3.4 [Programming Languages]: Processors Compilers General Terms Languages, Performance \nKeywords GPU, GPGPU, NESL, nested data parallelism 1. Introduction Graphics processing units (GPUs) provide \nlarge numbers of par\u00adallel processors. For example, the NVIDIA Tesla C2050 has 14 multiprocessors, each \nwith 32 cores, for 448 total cores. This card provides over 1200 GFLOPS, far more than the approximately \n50 GFLOPS available from a typical Intel quad-core i7 processor. While the GPU cores provide very good \ninteger and .oating point throughput, they are very limited compared to a general-purpose CPU. They only \nachieve peak performance when all cores are ex\u00adecuting the same instructions at the same time. This model \nworks well for a wide variety of arithmetically intense, regular parallel problems, but it does not support \nirregular parallel problems problems characterized by abundant parallelism but that have non- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 12, September \n9 15, 2012, Copenhagen, Denmark. Copyright &#38;#169; 2012 ACM 978-1-4503-1054-3/12/09 $15.00. uniform \nproblem subdivisions and non-uniform memory access, such as divide-and-conquer algorithms. Most GPU programming \nis done with the CUDA [NVI11b] and OpenCL [Khr11] languages, which provide the illusion of C-style general-purpose \nprogramming, but which actually impose restrictions. There have been a number of efforts to support GPU \nprogramming from higher-level languages, usually by embedding a data-parallel DSL into the host language, \nbut these efforts have been limited to regular parallelism [CBS11, MM10, CKL+11]. The current best practice \nfor irregular parallelism on a GPU is for skilled programmers to laboriously hand code applications. \nThe literature is rife with implementations of speci.c irregular\u00adparallel algorithms for GPUs [BP11, \nDR11, MLBP12, MGG12]. These efforts typically require many programmer-months of effort to even meet the \nperformance of the original optimized sequential C program. GPUs have some common characteristics with \nthe wide-vector supercomputers of the 1980 s, which similarly provided high\u00adperformance SIMD computations. \nNESL is a .rst-order functional language developed by Guy Blelloch in the early 1990 s that was designed \nto support irregular parallelism on wide-vector machines. NESL generalizes the concept of data parallelism \nto nested data\u00adparallelism (NDP), where subcomputations of a data-parallel com\u00adputation may themselves \nbe data parallel [BS90, Ble96, PPW95, CKLP01]. For example, the dot product of a sparse vector (repre\u00adsented \nby index/value pairs) and a dense vector is a data-parallel computation that is expressed in NESL using \na parallel map com\u00adprehension (curly braces) and a parallel summation reduction func\u00adtion: function svxv \n(sv, v) = sum ({x * v[i] : (x, i) in sv}); Using this function, we can de.ne the product of a sparse \nmatrix (represented as a vector of sparse vectors) with a dense vector as: function smxv (sm, v) = { \nsvxv(row, v) : row in sm } This function is an example of a nested data-parallel computation, since its \nsubcomputations are themselves data-parallel computa\u00adtions of irregular size. As described, NDP is not \nwell-suited to execution on SIMD ar\u00adchitectures, such as wide-vector supercomputers or GPUs, since it \nhas irregular problem decomposition and memory access. Blel\u00adloch s solution to this problem was the .attening \ntransformation, which vectorizes an NDP program so that the nested array struc\u00adtures are .at and the \noperations are SIMD [BS90, Kel99, PPW95, Les05]. This compilation technique allows NDP codes to be run \non vector hardware, but it has not yet been applied to GPUs. In this paper, we describe a port of the \nNESL language to run on GPUs. Our implementation relies on the NESL compiler to apply the .attening transformation \nto the program, which produces a vectorized stack-machine code, called VCODE. We use a series Figure \n1. NESL implementation of Quicksort, demonstrating ir\u00adregular parallelism in a divide-and-conquer algorithm. \n function quicksort(a) = function quicksort (as) = if (#a<2) then a if all(#as<2) then as else let else \nlet p = a[#a/2]; ps = {a[#a/2] : a in as}; lt = {e in a | e<p}; lts = {{e in es | e<p}: p in ps; es in \nas}; eq = {e in a | e==p}; eqs = {{e in es | e==p}: p in ps; es in as}; gt = {e in a | e>p}; gts = {{e \nin es | e>p}: p in ps; es in as}; r= {quicksort(v) : v in [lt, eq, gt]}; rs = quicksort (flatten ([lts, \neqs, gts])); in r[0] ++ r[1] ++ r[2]; in rs; of code optimizers and libraries to transform this intermediate \nlanguage for ef.cient execution on GPUs. This paper makes the following contributions: 1. We demonstrate \nthat a general purpose NDP language, such as NESL, can be implemented ef.ciently on GPUs. By allowing \nirregular parallel applications to be programmed for a GPU using NESL, we effectively move the requirement \nfor highly\u00adskilled GPU programmers from the application space to the language-implementation space. \n2. We explain the performance requirements of modern GPU hardware and describe the techniques and data \nstructures that we developed to tune the performance of the underlying vector primitives required to \nimplement NDP on a GPU. 3. We demonstrate that our preliminary implementation provides performance better \nthan many of the .at data-parallel lan\u00adguages, which illustrates the potential of this compilation ap\u00adproach. \n The remainder of the paper is organized as follows. In the next section, we describe the programming \nand execution model using quicksort as an example. Then, we provide an overview of GPU hardware and the \nCUDA language. Section 4 is a detailed descrip\u00adtion of our implementation and the match between the requirements \nof NESL on the vector machine and the CUDA language and its associated libraries, focusing on the design \ndecisions required for high performance on GPUs. Since directly implementing the vec\u00adtor hardware model \nwas not suf.cient for our performance targets, Section 5 describes additional optimizations we perform. \nAfter we introduce several related systems in some depth in Section 6.2, we compare the performance of \nour system to these systems. Finally, we cover some related work and conclude. The source code for our \nimplementation and all benchmarks de\u00adscribed in this paper are available at: http://smlnj-gforge. cs.uchicago.edu/projects/neslgpu. \n2. NESL NESL is a .rst-order dialect of ML that supports nested data\u00adparallelism [BCH+94]. It provides \nthe ability to make data-parallel function calls across arbitrarily nested data sequences. A stan\u00addard \nexample of NDP computation in NESL is the quicksort algo\u00adrithm [Ble96], which is given in Figure 1. The \nrecursive calls to the quicksort function lead to irregular parallel execution in general. That is, the \ncompiler cannot know how the data will be partitioned, so it cannot statically allocate and balance the \nworkload. The NESL compiler supports such irregular NDP computations by transforming the code and data \ninto a representation where nested arrays have been .attened. The result of this transforma\u00adtion is VCODE, \nwhich is code for a stack-based virtual vector ma\u00adchine [BC90]. This language executes in an interpreted \nenviron\u00adment on the host machine, calling primitives written in the C Vec-Figure 2. Flattening-inspired \nimplementation of Quicksort, pro\u00adviding a high-level overview of the effect of the code and data transformations. \ntor Library (CVL) [BC93]. In this section, we explain the NESL compilation process using quicksort as \na running example. 2.1 Flattened quicksort Figure 2 shows an idealized version of this .attened program \nin syntax similar to that of NESL. This version is not actually emitted by the NESL compiler, but is \nhelpful for understanding the trans\u00adformation. Flattening transforms the quicksort function from operating \nover a single vector at a time into a new version that operates over a nested vector of vectors. In a \nlanguage such as C, this structure might be represented with an array of pointers to arrays. In the im\u00adplementation \nof NESL, however, nested vectors are represented in two parts. One part is a .at vector containing the \ndata from all of the different vectors. The other part is one or more segment descrip\u00adtors (one per level \nof nesting). Segment descriptors are vectors that contain the index and length information required to \nreconstruct the nested vectors from the .at data vector. To cope with the change in representation, the \nbody of quicksort must also be changed. Each operation that previously operated on a scalar value is \nlifted to oper\u00adate on a vector and vector operations are lifted to operate on nested vectors. The scalar \np, which was a scalar value holding the single pivot element from the input vector a in the original \nprogram becomes the vector ps, holding all of the pivots from each of the vectors inside of the nested \nvector as. The vectors that previously held the lt, eq, gt, and r vectors from a are now turned into \nnested vectors holding all of the vectors of corresponding elements from the vectors represented by as. \nThe termination condition for this function, which was previously that the vector a is of length less \nthan 2, becomes a check to ensure that all of the vectors in as have length less than 2. The flatten \noperator, which is used to combine the vectors for the recursive call to quicksort , is of type [[a]] \n. [a].It removes one level of nesting by appending the elements of each of the top-level vectors.  2.2 \nNESL runtime Theoutputofthe NESLcompileristhe.attenedprogramtranslated into a virtual-machine code called \nVCODE. The NESL runtime system can be viewed as consisting of a VCODE interpreter that runs on a CPU \nand a machine-speci.c vector library that runs on a device. In some cases, the vector device might be \nthe CPU, but in our case it is the GPU. Because most of the computational load is in the vector operations, \nperformance is dominated by the ef.ciency of the vector library. 2.2.1 VCODE VCODE is a stack-based \nlanguage that is intended to be run on a host computer with vector operations performed on a vector machine. \nFigure 3 contains a small fragment of the actual VCODE  FUNC QUICKSORT_13 function entry COPY 10 stack \nmanipulation CALL PRIM-DIST_37 function call COPY 21 COPY 12 CALL VEC-LEN_13 CONST INT 2 push the vector \n[2] COPY 12 CALL PRIM-DIST_6 COPY 12 POP 10 stack manipulation < INT element-wise integer comparison \n... Figure 3. A small section of the over 1500 lines of VCODE cor\u00adresponding to the original quicksort \nexample. This section is the beginning of the .attened version, which determines whether the lengths \nof all of the segments passed in are less than 2. generated by the NESL compiler from the quicksort program \nin Figure 1. We have annotated some of the instructions with their meaning. There are four categories \nof VCODE instructions: Vector instructions. These include both element-wise vector op\u00aderations (such \nas the < INT in Figure 3) as well as various re\u00adduction, pre.x-scan, and permutation operations. These \nopera\u00adtions are executed by the vector device. Stack instructions. These include various instructions \nfor permut\u00ading, copying, and discarding stack elements, and are executed on the interpreter. All references \nto live data are on the stack and the VCODE interpreter uses these manage space automatically (there \nare no memory-management instructions in VCODE). Stack values are represented by a device-speci.c handle \nto the data paired with type information. Underlying data values are not transferred between the CPU \nand device except when re\u00adquired for I/O. Control instructions. These include function entries calls, \nand re\u00adturns, as well as conditionals and the CONST instruction. These instructions are executed on the \nCPU and do not directly affect the device (except for the CONST instruction, which allocates a vector \non the device). Other instructions. VCODE includes instructions for I/O, system initialization and shutdown, \nand various other miscellaneous operations.  2.2.2 CVL The C Vector Library (CVL) is a set of C library \nfunctions callable from a host machine that implement the VCODE vector operations and is the primary \nhook between the host machine and hardware on which the program is executing. This library is well-documented \nin its manual [BC93] and has been ported to many of the high\u00adperformance parallel computers of the 1990 \ns. It also has a sequen\u00adtial C implementation for execution on scalar uniprocessors. In this work, we \nhave implemented a version of the CVL library in CUDA. 3. GPU hardware and programming model Graphics \nprocessing units (GPUs) are high-performance parallel processors that were originally designed for computer \ngraphics applications. Because of their high performance, there has been growing interest in using GPUs \nfor other computational tasks. To support this demand, C-like languages, such as CUDA [NVI11b] and OpenCL \n[Khr11] have been developed for general-purpose programming of GPUs. While these languages provide a \nC-like expression and statement syntax, there are many aspects of their programming models that are GPU-centric. \nIn this paper, we focus on the CUDA language and NVIDIA s hardware, although our results should also \napply to OpenCL and other vendors GPUs. A typical GPU consists of multiple streaming multiprocessors \n(SMP), each of which contains multiple computational cores. One of the major differences between GPUs \nand CPUs is that the mem\u00adory hierarchy on a GPU is explicit, consisting of a global mem\u00adory that is shared \nby all of the SMPs, a per-SMP local mem\u00adory, and a per-core private memory. An SMP executes a group of \nthreads, called a warp, in parallel, with one thread per com\u00adputational core. Execution is Single-Instruction-Multiple-Thread \n(SIMT), which means that each thread in the warp executes that same instruction. To handle divergent \nconditionals, GPUs execute each branch of the conditional in series using a bit mask to disable those \nthreads that took the other branch. An SMP can ef.ciently switch between different warps, which allows \nit to hide memory latency. GPUs have some hardware support for synchronization, such as per-thread-group \nbarrier synchronization and atomic mem\u00adory operations. 3.1 CUDA CUDA is a language designed by NVIDIA \nto write parallel pro\u00adgrams for their GPU hardware [NVI11b]. It can be viewed as an extension of C++ \nand includes support for code synthesis using tem\u00adplates. In CUDA, code that runs directly on the GPU \nis called a kernel. The host CPU invokes the kernel, specifying the number of parallel threads to execute \nthe kernel. The host code also speci.es the con.guration of the execution, which is a 1, 2, or 3D grid \nstruc\u00adture onto which the parallel threads are mapped. This grid structure is divided into blocks, with \neach block of threads being mapped to the same SMP. The explicit memory hierarchy is also part of the \nCUDA programming model, with pointer types being annotated with their address space (e.g., global vs. \nlocal).  3.2 Key programming challenges on GPUs The various features of the GPU hardware and programming \nmod\u00adels discussed above pose a number of challenges to effective use of the GPU hardware. Our implementation \nlargely addresses these challenges, which allows the programmer to focus on correctness and asymptotic \nissues instead of low-level hardware-speci.c im\u00adplementation details. Data transfer Communication between \nthe host CPU and GPU is performed over the host computer s interconnect. These intercon\u00adnects are typically \nhigh-bandwidth, but not nearly as high in band\u00adwidth as is available on the card itself. For example, \na PCI-E 2.0 bus provides 16 GB/s of bandwidth, but the bandwidth between the NVIDIA Tesla C2050 s SMPs \nand the global memory is 144 GB/s. Therefore, fast GPU programs carefully balance the number and timing \nof their data transfers and other communications between the host and device. Memory access Within a \nGPU kernel, access to the global mem\u00adory is signi.cantly slower than access to local or private memory. \nIn fact, naive kernels that rely excessively on global memory are often slower than native CPU speeds. \nFurthermore, GPU global memory performance is very sensitive to the patterns of memory accesses across \nthe warp [NVI11a]. Lastly, the memory characteristics dif\u00adfer between cards, so a kernel that is tuned \nfor one card may not run well on another. Divergence Threads within a warp must execute the same in\u00adstruction \neach cycle. When execution encounters divergent control .ow, the SMP is forced to execute both control-.ow \npaths to the point where they join back together. Thus care must be taken to reduce the occurrence of \ndivergent conditionals.  Figure 4. The NESL/GPU toolchain. The shaded portions repre\u00adsent our contributions. \nNo recursive calls Recursion is not, in general, permitted in GPU kernels.1 This limitation means that \nany program with recursive calls must be transformed into a non-recursive one. This can be done by CPS \nconverting and using a trampoline (as is done by the OptiX library [PBD+10]), by explicitly managing \nthe return stack on the GPU [YHL+09], or by managing control .ow on the host CPU. Our NESL implementation \ndoes the latter. 4. Implementation Figure 4 shows the structure of our implementation. It takes a NESL \nprogram (a.nesl) and compiles it to VCODE (a.vcode) using the preexisting NESL compiler from CMU. We \nthen optimize the VCODE to produce specialized CUDA kernels (a.cu) and fused VCODE (a.fcode) for the \nprogram. Finally, the code is executed by a modi.ed version of the VCODE host interpreter, using our \nown CUDA-based implementation of the CVL. The remainder of this section describes our implementation \nof the CVL, which consists of over 200 kernel functions, split up into segmented and unsegmented versions \nof element-wise operations, reductions, scans, permutations, and miscellaneous conversion and management \noperations. This implementation is the end product of many iterations of design and careful performance \npro.ling. Our implementation passes all of the single-precision NESL regression 1 Recursion is supported \non some newer cards, but only for kernels that may be called from other kernel functions not the host. \ntests.2 We leave a discussion of the VCODE optimizer and support\u00ading interpreter modi.cations to Section \n5. 4.1 Implementation basics CVL uses a .xed-sized heap and performs garbage collection when there is \ninsuf.cient space remaining for a requested allocation. All data values in our implementation are 32-bits \nlong float, integer, and bool values. The VCODE interpreter keeps ref\u00aderence counts to track the lifetime \nof vectors. We augmented each of our CUDA calls with information about opportunities to reuse a source \nvector for the destination values for cases where they are both the same length and there is no concurrency \nissue with reusing the storage space. This optimization results in a signi.cant reduc\u00adtion of the maximum \nmemory footprint. 4.2 Segment Descriptors As mentioned in Section 2, segment descriptors are used in \nthe .attened data representation to describe the original struc\u00adture of the data. For example, consider \nthe nested vector [[4], [5, 6, 7], [8, 9]]. This vector can be represented by the .at data vector [4, \n5, 6, 7, 8, 9] paired with the segment descriptor [1, 3, 2], which indicates that the .rst segment is \none element long, the sec\u00adond segment is three elements long, and the third segment is two elements long. \nAlthough simple, this representation is terrible for execution of segmented operations on GPUs. Many \nkernels need to know for a given thread which segment it is in. Using the representation described above \nwould mean that each thread would be scanning the segment descriptor to determine its segment, which \nwould result in excessive memory traf.c. To avoid this problem, we expand segment descriptors to the \nlength of the underlying data vector, with the ith element of the segment descriptor holding the segment \nnumber that the ith data element belongs to. For our example, the descriptor is [1, 2, 2, 2, 3, 3]. With \nthis representation, each thread has constant-time access to its seg\u00adment and the access pattern is optimal \nfor global memory band\u00adwidth. This representation can be further improved. Some kernels, such as the \nextraction and permutation kernels, need to know how far offset a given segment is within the data, which \nrequires knowing the length of the segment. For example, to extract the second ele\u00adment from the third \nsegment in the data vector, the kernel needs to compute the total number of elements from the .rst two \nsegments and then add two. Because these operations are common in a large number of kernels, we concatenate \nthe original segment lengths vector to the end of the segment descriptor vector. Returning to this section \ns example, the full descriptor is [1, 2, 2, 2, 3, 3, 1, 3, 2]. These representation choices are the result \nof experimentation with three different alternatives. The worst lengths only re\u00adsulted in individual \nkernel calls taking multiple seconds, even af\u00adter optimization. Using a .ags vector increased performance \nover the lengths-only representation and used less space (since multi\u00adple boolean .ags can be compacted \ninto single words), but were slower than our .nal representation because of the need to com\u00adpute the \nper-element segment index in many kernels. After settling on this format and carefully tuning each of \nthe different types of kernel calls, those same calls take no more than a couple of hun\u00addred microseconds, \neven on very-large input data. The remaining subsections discuss implementation details and trade-offs \nin these kernel calls.  4.3 Element-wise kernels The element-wise kernels perform a unary or binary \noperation uniformly across the elements of its arguments, independent of 2 Our implementation does not \nyet support double-precision codes.  their nesting structure. These kernels are the only ones that are \nunaffected by the choice of segment descriptor representation, but they do have the property that they \nare memory bound, since their inputs and output are stored in global memory. This property means that \nthey are very sensitive to the number of warps per thread block. With too many warps per block, warps \nwill be idle waiting on computing resources, whereas with too few warps per block, the SMP will be idle \nwaiting for memory operations to complete, This problem is the memory access challenge discussed in Section \n3.2. We tested block sizes between 32 and 4096 threads and found that blocks of 256 threads (8 warps) \nprovided the best overall per\u00adformance, by up to a factor of 8. At lower numbers of threads, the blocks \nwere too small to keep the hardware busy, since most of our kernels are memory-bound. Increasing beyond \n256 threads per block caused low occupancy rates. At 8 warps per block, this break\u00addown allows roughly \n12 cycles to compute the address and make a request for memory from each thread before all of the memory \nrequests have been issued and the .rst warp has been .lled and is ready to execute. These results agree \nwith those recommended by NVIDIA for memory-bound kernels [NVI11a]. In addition to the basic element-wise \nvector math operations, there is also an element-wise random number generation opera\u00adtion. While there \nare many random number generators available for GPUs, we have elected to execute this operation on the \nhost CPU instead. This strategy makes it easier to compare our GPU imple\u00admentation s results against \nCPU versions and the extra communi\u00adcation cost is not detrimental to performance, since the benchmarks \nonly use random number generation during initialization.  4.4 Scans and reductions Scan and reduction \noperators perform an associative binary opera\u00adtion over an input vector. The reduction operation results \nin a single value, whereas scan operations produce a vector containing all the intermediate results. \nOur implementation uses the Thrust implementation of scan and reduction [HB11], which also supports the \nsegmented scan operation directly. These implementations are based on the work by Sengupta et al. that \nproduced ef.cient versions of the scan primitives tuned for GPUs [SHZO07]. Thrust also allows custom \nbinary operators, so we also provide custom classes implementing the full set of binary operations available \nin the NESL language. Our segment descriptor format is directly usable as a .ag vector for the segmented \nscan primitives by design. While Thrust includes a native reduction operator, it does not include a segmented \nversion of reduction. Our segmented reduction is implemented by .rst performing a segmented inclusive \nscan and then performing a second kernel call to extract all of the reduction sums. This second kernel \ncall requires the locations of the .nal, reduced values from each of the segments. In our initial implementation \nof this kernel, we computed per warp the corresponding offsets for each of the segments. But, this \nwas far too slow on vectors with more than a few thousand segments, as each warp needed to read every \nelement from the lengths portion of the segment descriptor. Now, we perform a pre.x sum of the lengths \nportion of the segment descriptor so that each thread can directly get the resulting value. For example, \nif we had a thousand segments, each of length 10, in the .rst version there would be ceil(1000/32)=32 \nthreads accessing and adding up, on average, half of those numbers. Worse, nearly all of those threads \nwould access the .rst few elements, resulting in nearly the worst possible GPU memory access pattern. \nIn the .nal version, we ef.ciently compute a vector corresponding to each of the offsets for each thread \nindex and both avoid the multiple-access penalty and remove the portion of code from the start of each \nkernel where every other thread but the .rst was blocked, waiting for the .rst to compute all of the \noffsets for each of the threads in the warp. In many cases, the segmented scan and reduction operators \nare called on single-segment vectors (i.e., .at or non-nested vectors). Because of the overhead of creating \nand initializing extra data structures to handle multiple segments and the extra bounds check\u00ading in \nthe segmented operators, we wrote custom implementations of the segmented operations for the single-segment \ncase. These spe\u00adcialized implementations achieve a nearly 20% speedup over the general versions. 4.5 \nPermutation and vector-scalar operations Permutation operators shuf.e elements from their locations in \na source vector to a target vector, sometimes incorporating default values for unmapped or extra elements. \nVector-scalar operations extract or replace values in a vector or create a vector from a base value and \nan optional stride value. When a stride is provided, the resulting vector elements are equal to the sum \nof the base value and the product of the element index and the stride value. These operators rely on \nthe segment descriptor information that allows individual threads to avoid each reading the vector of \nlengths repeatedly. Similar to the previous section, this requirement means that we often perform a +-scan \nof the segment descriptor lengths to provide the per-segment offsets, sometimes for both source and target \nsegment descriptors if they differ and are required. The operations requiring these data are infrequent \nrelative to the cost in both memory and time of always computing these scanned segment lengths, so we \ndo not just maintain both formats. However, computing indexing information through composition of ef.cient \nscan-vector operations is critical for high-performance kernel code. In every case that we tried to compute \nindexed offsets in a kernel by hand on a per-warp or per-thread basis, there arose input data edge cases \nthat pushed execution times for each of those inef.cient kernel calls into the range of seconds.  4.6 \nMiscellaneous operations The general facilities of the implementation include memory al\u00adlocation, windowing, \nI/O, host/vector hardware data transfer, and timing operators. During the initialization process, we \ncurrently only select a single GPU the one with the estimated highest performance. There are also library \nfunctions that directly implement sorting. These rank operations take a vector of numbers and return \na vector of integer indices indicating the target index that the corresponding number would take on in \na sorted vector. We use the Thrust li\u00adbrary s radix sort to implement the rank operations. The segmented \nrank operation is implemented by multiple invocations of the un\u00adsegmented radix sort operation, once \nper segment. We do not rely on the rank operation in any of our benchmark code, but provide it for API \ncompatibility. The index operations are heavily used and .ll in vectors with default values. The values \nare not only constants, but can also support a stride, allowing the value at an index to be a function \nof an initialization value and a stride factor. Again, this is an operation that bene.ted from both the \nnew segment descriptor format that provided the index number associated with an element as well as an \nextra data pass where we generate the offset into the segment of each element. This offset allows the \ncomputation of the value at an index to just be a simple multiply and add. Similar to the scan and reduction \noperators, this operation is frequently called in its segmented form but with a single vector. There \nis a signi.cant performance gain by optimizing the case of a single segment since all elements share \nthe same stride and index and can be loaded from their .xed location in GPU memory once per warp and \nshared across all of the threads.  The .nal operation with an interesting implementation is the creation \nof a segment descriptor, which is performed even more often than the indexing operation once per vector, \nunless the segment descriptor can be reused from a previous vector. Vectors are immutable in the NESL \nimplementation, so segment descrip\u00adtors are frequently shared and reused in the generated VCODE. Creation \nof the segment descriptor takes a vector with the lengths of each segment. For example, the vector [1,3,3] \ndescribes an underlying vector of seven elements divided into three segments. We allocate space for the \nunderlying vector s length plus the length of the segment descriptor. In this case, that would be 10 \nelements. First, we .ll the portion of the vector that is the length of the un\u00adderlying data vector with \nzeros. In our example, there would now be a vector of seven zeros. Then, at the location of the start \nof each segment in the vector, we place the index of that segment. In this case, the beginning of the \nvector is now: [1,2,0,0,3,0,0].An inclusive max-scan carries the segment indices over each of the zero-initialized \nelements quickly, resulting in: [1,2,2,2,3,3,3]. Finally, we copy the original lengths input to the end \nof the vector for use in the numerous kernels that require it. The .nal segment descriptor is: [1,2,2,2,3,3,3,1,3,3]. \nThis operation is exe\u00adcuted as a single kernel invocation. There are many other more straightforward \nways to .ll in this data, such as having one thread per element that computes its cor\u00adrect index, but, \nas in the other examples that required the +-scan of the segment descriptor lengths, any kernel implementation \nthat requires a large number of kernel threads to touch the same piece of memory will perform so poorly \nthat an individual kernel call will take longer than the entire remainder of the benchmark. 5. Optimization \nA straightforward porting of the NESL implementation to GPUs suffers from some obvious performance issues. \nThe VCODE pro\u00adduced by the NESL compiler is sub-optimal. It includes many triv\u00adial utility functions \nand a signi.cant amount of stack churn. Fur\u00adthermore, a straightforward port of the CVL library to GPUs \nre\u00adquires an individual CUDA kernel invocation for each computa\u00adtional VCODE operation. This property \nadds signi.cant scheduling overhead and reduces memory locality, since the arguments and re\u00adsults of \nthe operations must be loaded/stored in global memory. For example, consider the following simple NESL \nfunction: function muladd (xs, ys, zs) = {x*y+z : x in xs; y in ys; z in zs}; Figure 5 shows the unoptimized \nVCODE as produced by the NESL compiler. From this .gure, we can see examples of the .rst two issues. \nThe code includes trivial utility functions (e.g., ZIP-OVER_8) and is dominated by stack manipulations; \nmany of which end up computing the identity. In addition, the multiplica\u00adtion is run as a separate operation \nfrom the addition, which means that the intermediate result (i.e., the x*y value) must be stored in global \nmemory and then reloaded to perform the addition. To improve performance of our system, we have implemented \nan optimizer that takes the VCODE produced by the NESL com\u00adpiler and produces an optimized program in \nan extension of VCODE that we call FCODE (for Fused vCODE). As shown in Figure 4, this optimization .ts \ninbetween the NESL compiler and the VCODE interpreter. We describe this optimizer in the rest of this \nsection and discuss its impact on performance in Section 6.4. 5.1 VCODE optimizations The VCODE optimizer \nconsists of .ve phases. The .rst two of these address inef.ciencies in the VCODE generated by the NESL \ncompiler. The .rst phase is the inliner, which visits functions in reverse topological order inlining \ncalls to other VCODE operations. FUNC MULADD1_7 CPOP 24 CPOP 24 CPOP 24 CALL ZIP-OVER_8 CALL ZIP-OVER_10 \nCOPY 13 CPOP 14 CPOP 14 CPOP 14 COPY 13 POP 10 * INT CPOP 13 CPOP 13 POP 10 + INT RET FUNC ZIP-OVER_8 \nCPOP 22 CPOP 22 POP 11 CPOP 12 CPOP 12 CPOP 12 RET FUNC ZIP-OVER_10 CPOP 23 CPOP 32 POP 12 CPOP 13 CPOP \n13 CPOP 22 RET Figure 5. Unoptimized VCODE for the muladd function The inliner does not inline recursive \nfunctions and uses a size metric to limit code growth. Our size metric is the number of computational \ninstructions in the function. Once we have performed inlining, the next phase converts the stack machine \ncode into an expression language with let-bound variables. In this representation, each computation is \nbound to a unique variable. Continuing with our example, the muladd func\u00adtion is represented as follows:3 \nfunction MULADD1_7 (p0, p1, p2, p3) let t033 = p1 * p2 let t034 = t033 + p3 in RET (p0, t034) This conversion \nhas the effect of compiling away stack manip\u00adulation instructions (i.e., POP, COPY, and CPOP). When we \ncon\u00advert back to the stack machine representation, we are careful to avoid redundant and unnecessary \nstack operations, so the .nal re\u00adsult is much more compact. For example, the resulting code for the muladd \nfunction is: FUNC MULADD1_7 CPOP 21 * INT CPOP 11  + INT RET   5.2 Fusion While the VCODE optimizations \nproduce much more compact pro\u00adgrams, they do not address the most signi.cant performance issue, which \nis the use of individual kernel invocations for each compu\u00adtational instruction. For example, in the \nmuladd code, a kernel invocation is used to perform the element-wise multiplication on the xs and ys \nto produce an intermediate result array. Then a sec\u00adond kernel invocation is used to add the result of \nthe .rst with the zs array. For element-wise operations, this pattern is extremely in\u00adef.cient, since \nwe incur kernel invocation overhead for relatively small kernels that are dominated by global memory \ntraf.c. As has been observed by others, the .attening approach to implementing NDP requires fusion to \nbe ef.cient [Kel99, Cha93]. VCODE does not have a way to express fused operators, so we must leave the \ncon.nes of the VCODE instruction set and extend the interpreter. 3 Parameter p0 is the segment descriptor \nfor the result vector.  Our VCODE optimizer identi.es element-wise computations that involve multiple \noperations and replaces them with synthesized su\u00adperoperators. This approach is similar to Proebsting \ns superoper\u00adators [Pro95] and Ertl s super instructions [Ert01], with the main difference being that \nwe introduce superoperators for any eligible subcomputation, independent of its frequency. In our implementation, \nfusion is a two-step process. The .rst step is the reducer, which replaces variables in argument positions \nwith their bindings. The reducer limits its efforts to element-wise operations and .at constants. The \nreducer also eliminates unused variables and function parameters. After reduction, the muladd function \nconsists of a single, multi-operation expression: function MULADD1_7 (p0, p1, p2, p3, p5) let t034 = \n(p1 * p3)+p5 in RET (p0, t034) The second step is to identify the unique fused expressions and to lift \nthem out of the program as superoperators, as is shown in the following code: fused OP0 ($0 : INT,$1: \nINT,$2: INT)= ($0 * $1)+$2 ... function MULADD1_7 (x025, x026, x027, x028) let x029 = OP0 (x026, x027, \nx028) in RET (x025, x029) The two bene.ts of the fused kernels over the optimized VCODE are reductions \nin the number of kernel calls and the num\u00adber of global memory loads and stores. For this example, we \nreduce the number of kernel calls by four for each dynamic instance of the MULADD1_7 function and we \nreduce the number of global loads and stores by one each per element of the argument vectors. Limitations \nOur implementation currently fuses only element\u00adwise operations. Fusing map-reduce combinations would \nfurther reduce the number of kernel calls and global memory accesses, but we have not yet implemented \nthis feature.  5.3 Code generation The .nal phase of the optimizer is code generation, which is re\u00adsponsible \nfor both converting the expression representation back to stack-machine format and generating CUDA C \ncode for the fused superoperators. First, the optimized VCODE is transformed to re\u00admove all identi.ed \nopportunities for fusion with a call to a super\u00adoperator. In this example, we produce the following code: \nFUNC MULADD1_7 FUSED 0 RET where the 0 is the ID of the fused operation that is being in\u00advoked. Then, \nwe generate custom CUDA kernels for each fused superoperator. The corresponding kernel for the fused \noperation in this example is shown in Figure 6.  5.4 Calling the custom kernels Alongside each of these \nCUDA kernels, we generate both a host C function and a set of data structures. The data structures contain \nsummary information about the kernel, such as its arity, parameter and return types, vector size information, \nand whether or not last\u00aduse input parameters can be reused as the output storage. We have modi.ed the \nVCODE interpreter to handle fused operators, such as FUSED 0 in the example. When the interpreter hits \na fused kernel call it runs code to perform the proper allocations, argument checks, and invoke the kernel \nthrough the host C function. __global__ void fused0Kernel (MAXALIGN *data, int dst, int s0, int s1, int \ns2, int len, int scratch) { int addr = blockDim.y * blockIdx.y + blockDim.x * blockIdx.x + threadIdx.x; \nif (addr < len) { int *pDst = (int*)(&#38;data[dst]); int *pSrc0 = (int*)(&#38;data[s0]); int *pSrc1 \n= (int*)(&#38;data[s1]); int *pSrc2 = (int*)(&#38;data[s2]); pDst[address] = pSrc0[addr] * pSrc1[addr] \n+ pSrc2[addr]; } } Figure 6. Kernel for fused operation OP0 6. Evaluation To evaluate the effectiveness \nof our approach to implementing NESL on GPUs, we compare the performance of our system to CPU-based systems \nthat use .attening to implement NDP and to GPU-based systems that support .at data parallelism. Speci.cally, \nwe compare with NESL running on a single CPU, Data Parallel Haskell (DPH) running on 8 cores, Copperhead \nrunning on a GPU, and hand-written CUDA code running on a GPU. We choose the best available implementation \nof each benchmark for each mea\u00adsured system. We take this approach because each platform varies in compiler \ntechnology, target hardware, and implementation tun\u00ading, resulting in unfair penalties to platforms with \ndifferent hard\u00adware tradeoffs or where benchmark authors were no longer main\u00adtaining tuned algorithms \nfor the evolving GPU hardware. While our implementation does not achieve the level of per\u00adformance of \nhand-tuned CUDA programs, our results are bet\u00adter than other high-level NDP and .at data-parallel programming \nlanguages. Furthermore, NESL programs are signi.cantly smaller than the corresponding hand-tuned CUDA \n(typically a factor of 10 smaller) and require no direct knowledge of GPU idiosyncrasies. Thus, we make \nGPUs applicable to a wider range of parallel appli\u00adcations and a wider range of programmers. 6.1 Experimental \nframework Our benchmark platform has an Intel i7-950 quad-core processor running at 3.06GHz, with hyper-threading \nenabled for a total of eight cores available for parallel execution. Our GPU is an NVIDIA Tesla C2050, \nwhich has 14 SMPs, each with 32 cores for a total of 448 cores, and 3GB of global memory. For most experiments, \nwe ran Linux x86-64 kernel version 3.0.0-15 with the CUDA 4.1 drivers, but for one test we used Microsoft \nWindows 7 (also with the CUDA 4.1 drivers). We report the wall-clock execution time of the core computa\u00adtion, \nexcluding initialization and result veri.cation times. We ex\u00adclude the latter, because those times differ \nwidely between different platforms. Each benchmark was run at least 20 times at each size con.guration \nand we report the mean.  6.2 Comparison systems We compare our NESL/GPU implementation with a number \nof different systems, which are described in this section. To illustrate the programming models of these \nsystems, we give code for the data-parallel computation of the dot product. In NESL, this code is: function \ndotp (xs, ys) = sum ({ x*y : x in xs; y in ys })  import Data.Array.Parallel import Data.Array.Parallel.Prelude.Double \nas D Figure 7. Data Parallel Haskell implementation of dot product 6.2.1 NESL/CPU We measured the CPU-based \nversion of NESL running on a single core of our test machine. For these experiments, we ran the output \nof our optimizer with fusion turned off (i.e., the VCODE after inlining and elimination of redundant \nstack operations). The CPU version of NESL uses an implementation of CVL that is quite ef.cient. The \ncode uses macros for loop unrolling and inlining, which exposes multiple adjacent operations that the \nC compiler optimizes into SSE instructions. 6.2.2 Data Parallel Haskell Data Parallel Haskell (DPH) \nalso uses .attening in its implemen\u00adtation of NDP [CLPK08]. This extension of the Glasgow Haskell Compiler \n(GHC) [GHC] implements a subset of the Haskell lan\u00adguage with strict evaluation semantics. On a per-module \nbasis, code restricted to this subset and using the appropriate types will be .at\u00adtened. DPH does not \nsupport GPU execution, but it does support parallel execution on multicore systems. For our benchmarks, \nwe report the DPH performance on all eight cores of our test machine.4 We measured the DPH implementation \nas of February 2012 and GHC Version 7.4.1. Dot product in DPH The DPH version of our dot-product exam\u00adple \nis shown in Figure 7. At its core, the last line is very similar to the NESL code for this example. \n6.2.3 Copperhead Copperhead [CGK11] is an extension of the Python language that provides direct language \nsupport for data parallel operations on GPUs. It is limited to element-wise operations and reductions \n(i.e., it does not support NDP computations). Sources intended to run on the GPU (and optionally also \nthe CPU) are annotated with an @cu tag, indicating that only the subset of Python that can be compiled \nto the GPU may occur in the following de.nitions. Using a special places keyword, those annotated de.nitions \ncan be executed either on the GPU or CPU. We are using the Copperhead sources as of February, 2012, available \nfrom: http://code.google.com/p/ copperhead/. These sources are compiled against Python 2.7.1, Boost 1.41, \nPyCuda 2011.1.2, and CodePy 2011.1. The Copper\u00adhead project is still under active development and the \ncompiler is not yet mature enough to implement all of the benchmarks used in this paper. Dot product \nin Copperhead The dot-product example is very straightforward to write in Copperhead and appears in Figure \n8. The @cu annotation on the dot_product function tells Cop\u00adperhead to compile the code to run on the \nGPU. When the dot_product function is invoked, its arguments are converted to CUDA representation and \ntransferred to the GPU. Likewise, the re\u00adsult of the function is transferred back to the CPU and converted \nto Python representation automatically. 4 We also measured the performance on four cores without hyper-threading, \nbut found that the eight-core performance was better. from copperhead import * @cu def dot_product(xs, \nys): return sum(map(lambda x,y: x*y, xs, ys)) Figure 8. Copperhead implementation of dot product #define \nN (2048*2048) #define THREADS_PER_BLOCK 512 __global__ void dot (int *xs, int *ys, int *res) { __shared__ \nint temp[THREADS_PER_BLOCK]; int index = threadIdx.x + blockIdx.x * blockDim.x; temp[threadIdx.x] = xs[index] \n* ys[index]; __syncthreads(); if (0 == threadIdx.x) { int sum=0; for (int i = 0; i < THREADS_PER_BLOCK; \ni++) sum += temp[i]; atomicAdd (res , sum); } } int main(void) { dot<<<N/THREADS_PER_BLOCK, THREADS_PER_BLOCK>>> \n(dev_xs, dev_ys, dev_res); } Figure 9. Basic CUDA implementation of dot product 6.2.4 CUDA We also measured \nhand-coded implementations of our benchmarks to provide a measure of the best possible performance that \none might expect for these benchmarks. These implementations rep\u00adresent signi.cant programmer effort, \nbut make very ef.cient use of the GPU. Dot product in CUDA Figure 9 gives CUDA kernel code for the dot \nproduct. This program performs basic blocking and uses fast shared memory on the GPU, but is slower than \nthe optimized version available in CUBLAS.5 Even a simple implementation in CUDA of this small example \nis signi.cantly more complicated and verbose than the code on any of the other platforms.  6.3 Benchmarks \nTable 1 summarizes our benchmark results. As would be expected, hand-coded CUDA code outperforms the \nother implementations on all benchmarks. But the NESL/GPU implementation is the second fastest system \non almost all of the benchmarks and also has the shortest programs. All of the NESL results reported \nin this section are for pro\u00adgrams optimized as described in Section 5. The NESL/CPU pro\u00adgrams do not \ninclude fused kernel operations because those were implemented only for the NESL/GPU backend. 5 For our \nperformance experiments, we measured the faster CUBLAS ver\u00adsion of dot product.  Lines of code Execution \ntime (ms) CUDA DPH Copper NESL/CPU NESL/GPU CUDA DPH Copper Dot Product 10,000,000 8 80 39 31 27 3.0 \n< 1.0 12 710 Sort 1,000,000 12 136 52 46 230 259 3.1 2,360 230 Black-Scholes 10,000,000 37 337 N/A N/A \n8,662 163 1.9 N/A N/A Convex Hull 5,000,000 25 unknown 72 N/A 1,000 283 269 807 N/A Barnes-Hut 75,000 \n225 1930 414 N/A 22,200 4,502 40 10,200 N/A Table 1. Lines of non-whitespace or comment code for the \nbenchmark programs, omitting extra testing or GUI code. Benchmark times are also reported, as the mean \nexecution times in milliseconds (ms). Smaller numbers are better for both code and time.  5000 1000 \n1000 500 100 500 Time in ms (log scale) Time in ms (log scale) 500100 50 50 100 100 50 50 10 5 10 5 10 \n10 5 5 1 1 Number of elements (log scale) 6.3.1 Dot product The dot product of two vectors is the result \nof adding the pairwise multiplications of elements from each of the vectors. This small benchmark is \ninteresting because it contains one trivial vector\u00adparallel operation (multiplying each of the elements) \nand one that requires signi.cant inter-thread communication (reducing those multiplied elements to a \nsingle value through addition). In the summary data in Table 1, each vector contains 10,000,000 32-bit \n.oating point. Figure 10 provides a more detailed breakdown for each system across many vector lengths. \nBecause the perfor\u00admance of these systems vary widely, we use a logarithmic scale for the time axis in \nthis plot. The superb CUDA performance is provided by a highly-tuned implementation of dot product from \nthe CUBLAS library. The NESL/GPU version of dot product also performs well. For vector sizes less than \n5,000,000, the NESL/GPU version .nishes faster than the .nest resolution of the timing APIs. This good \nperfor\u00admance is owed to the use of a highly-tuned parallel reduction opera\u00adtor for the addition operation \nand relatively low additional overhead around the element-wise multiplication. The poor Copperhead be\u00adhavior \nappears to be related to failing to take full advantage of the parallel hardware. 6.3.2 Sorting For \nthe sorting benchmarks, we selected the best algorithm for the particular platform, which was quicksort \nfor the CPU (DPH and NESL/CPU) and radix sort for the GPU (CUDA and Copper\u00adhead). The one exception is \nthat we measured quicksort for the NESL/GPU platform, since it runs nearly 10% faster than radix sort \nowing to lower CPU/GPU communication overhead. The sorting benchmarks in Table 1 measure the sorting \nof a vector of 1,000,000 0 Number of elements (log scale) random integers (the actual vectors differ \nbecause of differences in the random-number generation algorithms). Figure 11 provides a more detailed \nbreakdown for each system across many vector lengths. Copperhead and CUDA both make very effective use \nof the GPU and scale well across increasing problem sizes. The NESL/CPU version of quicksort performs \nbetter than the NESL/GPU version because of the reduced cost of vector operations. 6.3.3 Black-Scholes \nThe Black-Scholes option pricing model is a closed-form method for computing the value of a European-style \ncall or put option, based on the price of the stock, the original strike of the option, the risk-free \ninterest rate, and the volatility of the underlying in\u00adstrument [BS73]. This operation is numerically \ndense and trivially parallelizable. Reported numbers are for the pricing of 10,000,000 contracts. The \nNESL/GPU version is able to perform much faster than the NESL/CPU version because of the very aggressive \nfusion of the dense numeric operations. The CUDA version is a hand-optimized version included in the \nNVIDIA SDK. Figure 12 provides a more detailed breakdown for each system across many vector lengths. \n 6.3.4 Convex Hull The convex-hull benchmark results shown in Table 1 are the result of determining \nthe convex hull of 5,000,000 points in the plane. While this algorithm is trivially parallel, the parallel \nsubtasks are not guaranteed to be of equal work sizes, providing an example of irregular parallelism. \nThe NESL and DPH codes are based on the algorithm by Barber et al. [BDH96]. The convex-hull algorithm \n 10000 50000 10000 5000 100 Time in ms (log scale) Time in ms (log scale) 100 50 1000 500 10 10 5 \n5 1 1 1 0 Number of elements (log scale) Number of elements (log scale) Figure 12. Black-Scholes execution \ntimes (ms) for a range of Figure 14. Barnes-Hut execution times (ms) for a range of prob\u00adproblem sizes. \nSmaller times are better. Axes are log scale. lem sizes. Smaller times are better. Axes are log scale. \nData Parallel Haskell 2500 2500 for each system across many vector lengths. The number of itera\u00ad 500 \n250 500 tions is held constant at one. 250 Time in ms (log scale) 100 50 100 The CUDA version of Barnes-Hut \nwas implemented by 50 Burtscher and Pingali [BP11]. We use version 2.2 of their source, 5 which was \nthe newest available as of February 2012. All implementations exhibit roughly the same scalability. While \nthe NESL/GPU implementation is the second fastest implementa\u00adtion, it is still signi.cantly slower than \nthe CUDA version. The NESL/GPU implementation s runtime is split roughly 1/3 on mem\u00adory operations and \n2/3 on kernel calls. These memory operations are related to allocations and conditional branches. When \nthere is a conditional statement over a vector, .rst we perform the condi\u00adtional check. Then, we sum \nthe number of true elements from that conditional check and then transfer the integer (or integers, for \na Number of elements (log scale) segmented vector) back from the GPU to the CPU to allocate a memory \nblock of the correct size for each of the true and false el- ements. After that allocation, we copy the \ntrue elements into the true vector and the false ones into the false vector and then exe\u00adcute the clauses \nof the conditional on the appropriate vectors. These memory transfers back to the CPU and extra kernel \ncalls could be avoided by moving some of the allocation code onto the GPU in a lem sizes. Smaller times \nare better. Axes are log scale. written in CUDA is from ongoing work at the National University of Singapore \n[GCN+12]. Their gHull algorithm was originally future implementation. based on the quickhull algorithm, \nbut has been optimized for better performance on a GPU. Their algorithm also works with points in 3D, \nbut in our testing we constrained it to the 2D case for comparison with these other systems. It performs \nslightly better than the quickhull algorithm implemented in NESL and run on the GPU. This code was only \nmade available to us in binary form, for execution on Windows, so we measured its performance on our \nbenchmark machine under Windows 7 using the CUDA 4.1 drivers. Figure 13 provides a more detailed breakdown \nfor each system across many vector lengths.  6.3.5 Barnes-Hut (N-Body) The Barnes-Hut benchmark [BH86] \nis a hierarchical N-body prob\u00adlem solver; we measure the 2D version that uses a quadtree to ac\u00adcelerate \nthe computation. Each iteration has two phases. In the .rst phase, the quadtree is constructed from a \nsequence of mass points. The second phase then uses this tree to accelerate the computation of the gravitational \nforce on the bodies in the system. All versions of this benchmark shown in Table 1 run one iteration \nover 75,000 random particles. Figure 14 provides a more detailed breakdown  6.4 Effects of optimizations \nIn Section 5, we described a set of optimizations we perform on the code produced by the NESL compiler. \nTable 2 compares the execution time for the benchmarks across the optimizations on the NESL/GPU implementation, \nnormalized to the mean execution time of the baseline. In nearly all cases, these optimizations result \nin improvements. The one place where there is a decrease in performance is between the optimized and \nfused versions of quicksort. In the GPU version of quicksort, the vast majority of the remaining time \nspent in execution after optimization is in segmented sums and reductions that determine the number of \nelements in each of the less-than and greater-than partitions in order to shuf.e them for the recursive \ncalls. Because of this balance of work, the fusion of the element-wise operations that compare elements \ndoes not have a large enough effect to increase performance. While the mean execution time is slightly \nslower under fusion than the simple optimized version for this benchmark, the median of the fused  Fused \nDot Product 1.0 0.95 0.94 Sort 1.0 0.92 0.93 Black-Scholes 1.0 1.0 0.67 Convex Hull 1.0 0.95 0.91 Barnes-Hut \n1.0 0.94 0.90 Table 2. Performance bene.ts from VCODE optimization. Execu\u00adtion times are normalized to \nthe base (unoptimized) strategy, with smaller values being better. values is lower and a more rigorous \nstatistical analysis shows that the fused version is the same speed as the optimized one.6 One benchmark \nthat particularly bene.ts from fusion and the creation of kernels is Black-Scholes option pricing. This \nbench\u00admark is numerically dense and our optimizer aggressively reduces the number of kernel invocations, \ngenerating several kernels that each turn what would have been more than 10 separate GPU calls into a \nsingle call that both performs all of the operations and avoids intermediate writes to global memory. \n7. Related Work To the best of our knowledge, our work is the only example of an NDP language that is \ncompiled for GPUs, so we divide the related work into languages that support (non-nested) data parallelism \non GPUs and languages that use .attening to implement NDP on CPUs. 7.1 GPU Languages The work on languages \ntargeting GPUs is focused on regular data-parallel programs. These languages typically also provide li\u00adbrary \nfunctions with ef.cient parallel implementations of map and reduce. While many of the available languages \naddress some of the issues listed in Section 3.2, none of them address either the re\u00adcursive call issue \nor any of the memory, data, and thread issues with respect to irregular data-parallel programs. Barracuda \n[Lar11] and Single-Assignment C (SAC) for GPUs [GS06, GTS11], both provide techniques for compiling ap\u00adplicative \narray languages down to GPUs. Array languages have proven ideally suited for translation to the regular, \n.at parallelism available on the GPU. While these languages do not support paral\u00adlelizing over irregular \noperations, SAC includes a fusion pass that .nds operations whose results are based on the same index \nset and reduces those operations to a single pass generating multiple re\u00adsults. This fusion technique \nis not supported by our system, but it and other optimizations used in SAC may be useful for optimizing \nVCODE. Nikola and Accelerate provide support for compiling opera\u00adtions on .at vectors in Haskell programs \nto run on GPUs [MM10, CKL+11]. Similarly Copperhead, discussed in more detail in Sec\u00adtion 6.2, is a language \ntargeting GPUs based on Python [CGK11]. These languages add map, reduce, and other high-level operations \nto signi.cantly ease programming for GPUs and rely on the CPU to handle more complicated control-.ow. \nOptiX is an embedded domain-speci.c language and library that supports ray-oriented applications on GPUs \n[PBD+10]. These applications have a signi.cant recursive component, which OptiX handles by CPS conversion \nand a trampoline. 6 The Wilcoxon signed-rank test appropriate for nonparametric, indepen\u00addent data points \n provides 92% con.dence that the difference between the two distributions is zero.  7.2 CPU Languages \nData Parallel Haskell (DPH) is the culmination of many years of re\u00adsearch into expanding the .attening \ntransformation to handle both more datatypes and higher-order functions [CKLP01, LCK06]. The NESL language \ndoes not support datatypes and is only .rst-order. DPH also supports partial vectorization, which allows \nportions of a program to remain un.attened [CLPK08]. In NESL, the entire pro\u00adgram and all data are .attened. \nFinally, DPH implements a much wider range of fusion operations to remove the redundant alloca\u00adtions \nthan the implementation presented in Section 5.2. The Manticore project takes a different approach to \nnested data parallelism, implementing it without .attening and relying on ef.cient runtime mechanisms \nto handle load balancing is\u00adsues [BFR+10]. 8. Conclusion We have shown that with careful implementation \nof the library primitives, the .attening transformation can be used on NDP pro\u00adgrams to achieve good \nperformance on GPUs. By focusing our per\u00adformance tuning efforts on the VCODE implementation, we make \nit possible for a wide range of irregular parallel applications to get performance bene.ts from GPUs, \nwithout having to be hand ported and tuned. While performance does not match that of hand-tuned CUDA \ncode, NESL programs are a factor of 10 shorter and do not require expertise in GPU programming. We hope \nthat this work will be used as a better baseline for new implementations of ir\u00adregular parallel applications \nthan the usual sequential C programs. Better, of course, would be the integration of these classic com\u00adpilation \ntechniques for vector hardware into modern programming languages. 8.1 Future work The most obvious limitation \nof our approach is that communication with the host CPU is required for allocation of memory. This requirement, \nas described Section 6.3.5, results in many additional communications between the CPU and GPU merely \nto provide an address. Moving the memory allocation responsibility into the GPU kernels would remove \nmuch of this communication cost. This issue is addressed by the compilation model used by Chatterjee \nin his work porting NESL to the MIMD Encore Multimax [Cha93]. His implementation included size analysis \nof programs and full custom C code generation. Some of his techniques may be applicable to improving \nGPU performance of NDP. Acknowledgments Ben Lippmeier provided help understanding the benchmark results \nand current status of Data Parallel Haskell. Bryan Catanzaro untan\u00adgled the Copperhead dependency stack \nand provided insight into its current implementation and performance limitations. We thank the NVIDIA \nCorporation for their generous donation of both hardware and .nancial support. This material is based \nupon work supported by the National Science Foundation under Grants CCF-0811389 and CCF-1010568, and \nupon work performed in part while John Reppy was serving at the National Science Foundation. The views \nand conclusions contained herein are those of the authors and should not be interpreted as necessarily \nrepresenting the of.cial policies or endorsements, either expressed or implied, of these organizations \nor the U.S. Government. References [BC90] Blelloch, G. and S. Chatterjee. VCODE: A data-parallel inter\u00admediate \nlanguage. In FOMPC3, 1990, pp. 471 480.  [BC93] Blelloch, G. and S. Chatterjee. CVL: A C vector language, \n1993. [BCH+94] Blelloch, G. E., S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. Implementation \nof a portable nested data-parallel language. JPDC, 21(1), 1994, pp. 4 14. [BDH96] Barber, C. B., D. P. \nDobkin, and H. Huhdanpaa. The quickhull algorithm for convex hulls. ACM TOMS, 22(4), 1996, pp. 469 483. \n[BFR+10] Bergstrom, L., M. Fluet, M. Rainey, J. Reppy, and A. Shaw. Lazy tree splitting. In ICFP 10. \nACM, September 2010, pp. 93 104. [BH86] Barnes, J. and P. Hut. A hierarchical O(N log N) force calcu\u00adlation \nalgorithm. Nature, 324, December 1986, pp. 446 449. [Ble96] Blelloch, G. E. Programming parallel algorithms. \nCACM, 39(3), March 1996, pp. 85 97. [BP11] Burtscher, M. and K. Pingali. An ef.cient CUDA implemen\u00adtation \nof the tree-based Barnes Hut n-body algorithm. In GPU Computing Gems Emerald Edition, chapter 6, pp. \n75 92. Else\u00advier Science Publishers, New York, NY, 2011. [BS73] Black, F. and M. Scholes. The pricing \nof options and corporate liabilities. JPE, 81(3), 1973, pp. 637 654. [BS90] Blelloch, G. E. and G. W. \nSabot. Compiling collection-oriented languages onto massively parallel computers. JPDC, 8(2), 1990, pp. \n119 134. [CBS11] Cunningham, D., R. Bordawekar, and V. Saraswat. GPU pro\u00adgramming in a high level language \ncompiling X10 to CUDA. In X10 11, San Jose, CA, May 2011. Available from http: //x10-lang.org/. [CGK11] \nCatanzaro, B., M. Garland, and K. Keutzer. Copperhead: com\u00adpiling an embedded data parallel language. \nIn PPoPP 11, San Antonio, TX, February 2011. ACM, pp. 47 56. [Cha93] Chatterjee, S. Compiling nested \ndata-parallel programs for shared-memory multiprocessors. ACM TOPLAS, 15(3), July 1993, pp. 400 462. \n[CKL+11] Chakravarty, M. M., G. Keller, S. Lee, T. L. McDonell, and V. Grover. Accelerating Haskell array \ncodes with multicore GPUs. In DAMP 11, Austin, January 2011. ACM, pp. 3 14. [CKLP01] Chakravarty, M. \nM. T., G. Keller, R. Leshchinskiy, and W. Pfannenstiel. Nepal nested data parallelism in Haskell. In \nEuro-Par 01, vol. 2150 of LNCS. Springer-Verlag, August 2001, pp. 524 534. [CLPK08] Chakravarty, M. M. \nT., R. Leshchinskiy, S. Peyton Jones, and G. Keller. Partial vectorisation of Haskell programs. In DAMP \n08. ACM, January 2008, pp. 2 16. Available from http: //clip.dia.fi.upm.es/Conferences/DAMP08/. [DR11] \nDhanasekaran, B. and N. Rubin. A new method for GPU based irregular reductions and its application to \nk-means clustering. In GPGPU-4, Newport Beach, California, March 2011. ACM. [Ert01] Ertl, M. A. Threaded \ncode variations and optimizations. In EuroForth 2001, Schloss Dagstuhl, Germany, November 2001. pp. 49 \n55. Available from http://www.complang. tuwien.ac.at/papers/. [GCN+12] Gao, M., T.-T. Cao, A. Nanjappa, \nT.-S. Tan, and Z. Huang. A GPU Algorithm for Convex Hull. Technical Report TRA1/12, National University \nof Singapore, School of Computing, Jan\u00aduary 2012. [GHC] GHC. The Glasgow Haskell Compiler. Available \nfrom http: //www.haskell.org/ghc. [GS06] Grelck, C. and S.-B. Scholz. SAC A Functional Array Language \nfor Ef.cient Multi-threaded Execution. IJPP, 34(4), August 2006, pp. 383 427. [GTS11] Guo, J., J. Thiyagalingam, \nand S.-B. Scholz. Breaking the GPU programming barrier with the auto-parallelising SAC compiler. In DAMP \n11, Austin, January 2011. ACM, pp. 15 24. [HB11] Hoberock, J. and N. Bell. Thrust: A productivity-oriented \nlibrary for CUDA. In W. W. Hwu (ed.), GPU Computing Gems, Jade Edition, chapter 26, pp. 359 372. Morgan \nKaufmann Publishers, October 2011. [Kel99] Keller, G. Transformation-based Implementation of Nested Data \nParallelism for Distributed Memory Machines. Ph.D. dissertation, Technische Universit\u00a8 at Berlin, Berlin, \nGermany, 1999. [Khr11] Khronos OpenCL Working Group. OpenCL 1.2 Speci.cation, November 2011. Available \nfrom http://www.khronos. org/registry/cl/specs/opencl-1.2.pdf. [Lar11] Larsen, B. Simple optimizations \nfor an applicative array lan\u00adguage for graphics processors. In DAMP 11, Austin, January 2011. ACM, pp. \n25 34. [LCK06] Leshchinskiy, R., M. M. T. Chakravarty, and G. Keller. Higher order .attening. In V. Alexandrov, \nD. van Albada, P. Sloot, and J. Dongarra (eds.), ICCS 06, number 3992 in LNCS. Springer-Verlag, May 2006, \npp. 920 928. [Les05] Leshchinskiy, R. Higher-Order Nested Data Parallelism: Se\u00admantics and Implementation. \nPh.D. dissertation, Technische Universit\u00a8at Berlin, Berlin, Germany, 2005. [MGG12] Merrill, D., M. Garland, \nand A. Grimshaw. Scalable GPU graph traversal. In PPoPP 12, New Orleans, LA, February 2012. ACM, pp. \n117 128. [MLBP12] Mendez-Lojo, M., M. Burtscher, and K. Pingali. A GPU im\u00adplementation of inclusion-based \npoints-to analysis. In PPoPP 12, New Orleans, LA, February 2012. ACM, pp. 107 116. [MM10] Mainland, G. \nand G. Morrisett. Nikola: Embedding compiled GPU functions in Haskell. In HASKELL 10, Baltimore, MD, \nSeptember 2010. ACM, pp. 67 78. [NVI11a] NVIDIA. NVIDIA CUDA C Best Practices Guide, 2011. [NVI11b] NVIDIA. \nNVIDIA CUDA C Programming Guide, 2011. Available from http://developer.nvidia. com/category/zone/cuda-zone. \n[PBD+10] Parker, S. G., J. Bigler, A. Dietrich, H. Friedrich, J. Hoberock, D. Luebke, D. McAllister, \nM. McGuire, K. Morley, A. Robi\u00adson, and M. Stich. OptiX: a general purpose ray tracing engine. ACM TOG, \n29, July 2010. [PPW95] Palmer, D. W., J. F. Prins, and S. Westfold. Work-ef.cient nested data-parallelism. \nIn FoMPP5. IEEE Computer Society Press, 1995, pp. 186 193. [Pro95] Proebsting, T. A. Optimizing an ANSI \nC interpreter with superoperators. In POPL 95, San Francisco, January 1995. ACM, pp. 322 332. [SHZO07] \nSengupta, S., M. Harris, Y. Zhang, and J. D. Owens. Scan primitives for GPU computing. In GH 07, San \nDiego, CA, August 2007. Eurographics Association, pp. 97 106. [YHL+09] Yang, K., B. He, Q. Luo, P. V. \nSander, and J. Shi. Stack\u00adbased parallel recursion on graphics processors. In PPoPP 09, Raleigh, NC, \nFebruary 2009. ACM, pp. 299 300.      \n\t\t\t", "proc_id": "2364527", "abstract": "<p>Graphics processing units (GPUs) provide both memory bandwidth and arithmetic performance far greater than that available on CPUs but, because of their <i>Single-Instruction-Multiple-Data</i> (SIMD) architecture, they are hard to program. Most of the programs ported to GPUs thus far use traditional data-level parallelism, performing only operations that operate uniformly over vectors.</p> <p>NESL is a first-order functional language that was designed to allow programmers to write irregular-parallel programs - such as parallel divide-and-conquer algorithms - for wide-vector parallel computers. This paper presents our port of the NESL implementation to work on GPUs and provides empirical evidence that nested data-parallelism (NDP) on GPUs significantly outperforms CPU-based implementations and matches or beats newer GPU languages that support only flat parallelism. While our performance does not match that of hand-tuned CUDA programs, we argue that the notational conciseness of NESL is worth the loss in performance. This work provides the first language implementation that directly supports NDP on a GPU.</p>", "authors": [{"name": "Lars Bergstrom", "author_profile_id": "81548019882", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3804358", "email_address": "larsberg@cs.uchicago.edu", "orcid_id": ""}, {"name": "John Reppy", "author_profile_id": "81548019883", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3804359", "email_address": "jhr@cs.uchicago.edu", "orcid_id": ""}], "doi_number": "10.1145/2364527.2364563", "year": "2012", "article_id": "2364563", "conference": "ICFP", "title": "Nested data-parallelism on the gpu", "url": "http://dl.acm.org/citation.cfm?id=2364563"}