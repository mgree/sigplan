{"article_publication_date": "07-01-1992", "fulltext": "\n A Dynamic Scheduling Method for Irregular Parallel Programs Steven LUCCO* Computer Science Division, \n571 Evans Hall UC Berkeley, Berkeley CA, 94720 Abstract This paper develops a methodology for compiling \nand executing irregular parallel programs. Such programs implement parallel operations whose size and \nwork dis\u00adtribution depend on input data. We show a funda\u00admental relationship between three quantities \nthat char\u00adacterize an irregular parallel computation: the total available parallelism, the optimal grain \nsize, and the statistical variance of execution times for individual tasks. This relationship yields \na dynamic scheduling algorithm that substantially reduces the overhead of executing irregular parallel \noperations. We incorporated this algorithm into an extended Fortran compiler. The compiler accepts as \ninput a sub\u00adset of Fortran D which includes blocked and cyclic de\u00adcompositions and perfect alignment; \nit outputs Fortran 77 augmented with calls to library routines written in C. For irregular parallel operations, \nthe compiled code gathers information about available parallelism and task execution time variance and \nuses this informa\u00adtion to schedule the operation. On distributed memory architectures, the compiler encodes \ninformation about data access patterns for the runtime scheduling system so that it can preserve communication \nlocality. We evaluated these compilation techniques using a set of application programs including climate \nmodel\u00ad *Supportedin part by an IBM Fellowship. Email address: lucco@cs. Berkeley. EDU.Research sponsored \nin part by the De\u00adfense Advanced Research Projects Agency (DoD), monitored by Space and Naval Warfare \nSystems Comand under Contract NOO039-88-C-0292 Permission to copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0-89791 -476-7 /92/0006 /0200 \n. ..$1 .50 ing, circuit simulation, and x-ray tomography, that contain irregular parallel operations. \nThe results demonstrate that, for these applications, the dynamic techniques described here achieve near-optimal \neffi\u00adciency on large numbers of processors. In addi\u00adtion, they perform significantly better, on these \nprob\u00adlems, than any previously proposed static or dynamic scheduling algorithm.  1 Introduction The \naim of our research is to achieve efficient execu\u00adtion of parallel programs. Compiler research toward \nthis goal has focused on four main areas: discovery of parallelism [2, 3, 25], static scheduling [8, \n10, 24], linear transformation of iteration spaces to improve data lo\u00adcalit y or expose parallelism [31, \n33, 34], and improved compiler technology to support the first three activ\u00adities [7, 9]. In this paper \nwe focus on a particularly intractable class of parallel programs for which static techniques such as \nthese are not sufficient to gener\u00adate highly efficient code. This is not a deficiency of the static techniques, \nwhich are often a prerequisite for efficient execution, but a property of the programs themselves. Such \nprograms, which we call irregular, contain parallel operations whose size and work distri\u00adbution depend \non input data. Because of this property, a static schedule that performs well on one aet of in\u00adput data \nmay perform poorly on others. Further, even static schedules that use random task assignments to avoid \nbias will have only moderate efficiency if the in\u00addividual tasks have significant execution time variance. \nMany basic computational techniques, such as adap\u00adtive mesh refinement [5], time-step subdivision [1, \n20], tree traversal [12], and monte-carlo methods[30], yield irregular parallel programs. To execute \nsuch pro\u00ad 200 grams efficiently, we generate code that assigns tasks to processors dynamically. We \nrequire that a dynamic scheduling policy have two properties: adaptivity and statelessness. The latter \nproperty means that current scheduling decisions do not use information about past decisions; this implies \nthat the scheduling policy will function correctly under the addition and subtraction of processors, \nand the addition of new tasks to the set of tasks being scheduled. An adaptive scheduling pol\u00adicy gathers \nruntime information about the distribution of task execution times, and uses this information to improve \nthe efficiency of the schedule. No previously proposed dynamic scheduling policies have both these properties; \nhowever, we believe bo+h are necessary for efficient execution of irregular problems. In this paper, \nwe develop an adaptive, stateless dynamic scheduling algorithm and describe how we incorporated the algo\u00adrithm \ninto a compiler for shared and distributed mem\u00ad ory architectures. The remainder of the paper is organized \nas follows. Section 2 gives some examples of irregular programs and illustrates how different types of \nexecution time distributions can arise. Section 3 presents our basic compilation framework and describes \nhow we modi\u00adfied it to incorporate dynamic scheduling. Section 4 il\u00adlustrates some important properties \nof previously pro\u00adposed scheduling algorithms. In section 5.1, we prove a new property of multiprocessor \nscheduling and use this property to derive a relationship between opti\u00admal grain size, available parallelism \nand task execution time variance. In section 5.2, we develop our proba\u00adbilistic dynamic scheduling algorithm \nand extend it for execution on distributed memory machines. Finally, section 6 presents efficiency results \nfor a set of appli\u00adcation programs. Irregular Programs First we give some examples which illustrate \nhow ir\u00ad regular work distributions can arise in programs. We define a parallel operation as a set of \nN independent tasks, such as the iterates of a DOALL loop. In an irregular parallel operation, task execution \ntimes vary significantly and unpredictably. Consider the irregular Fortran loop shown in figure 1. The \nnumbers in brackets indicate execution costs for sequential sections of the loop. The conditional state\u00ad \nment is labeled with the probabilities of the true and false branches respectively. Even given this informa\u00ad \ntion it is impossible (except for very large N) to com\u00ad pute a schedule for this loop statically that \nis efficient in the average case. Suppose that we randomly assign tasks to proces\u00ad sors. Since the execution \ncosts of the tasks are them- DOALL 10 1=1 ,N IF (C< O.9,0.1}) THEN {200} ELSE <600003 ENDIF 10 CONTINUE \nFigure 1: A Sample Irregular Loop 100% v ./h$$&#38;atso% \u00ad./ @cio% -,/ / i!,~ . ./ / 20% \u00ad,/ o% I@ 101I@ \nI@ Id Figure 2: Efficiency of Static Assignment selves random, we can just assign successive contiguous \nblocks of N/p tasks to each processor (this is equiva\u00adlent to using the owner computes rule [14] for \na blocked decomposition). Figure 2 shows the efficiency of this method for executing our loop example, \ngiven different values of N/p. No static method can beat the average case behavior of random assignment. \nThis result is not limited to this example, but ap\u00adplies to any parallel operation with significant variance \nin task execution times. The expected finishing time of a chunk of N/p tasks is Np/p, where p is the \nmean task cost. However, given no restriction on task cost distribution, the best bound on the finishing \ntime of p such chunks is Np/p+ u~ where ISis the standard deviation of taak costs [11]. For an important \nclass of distributions (including normal, uniform, and expo\u00adnential), Kruskal [16] demonstrates the tighter \nbound Np/p + u<2N in p/p. However, either bound leaves room for substantial inefficiency. Since the expected \ninefficiency (the second term in these expressions) grows as the square root, while the expected completion \ntime grows linearly, random as\u00adsignment will be efficient for large N/p; however, we will demonstrate \nthat it is only more efficient than dy\u00adnamic methods when a << p (i.e. the tasks do not dif\u00adfer much \nin cost) or the scheduling overhead is much Figure 3: Execution Profile of Psirrfan Loop Nest. Z axis \nis microseconds; X and Y axes are iteration num\u00adbers. greater thanp. We abstracted our loop example from \none of the programs in our benchmark set, an x-ray tomogra\u00adphy application called Psirrfan. Psirrfan \nimplements anew algorithm for reconstructing an image where to\u00admographic data is only available for a \nlimited range of angles [13]. Some of Psirrfan s most expensive oper\u00adations are executed only when input \ndata is missing. Figure 3 shows an execution profile for an important loop nest involved in implementing \nthe image recon\u00adstruction. In addition, Psirrfan contains another type of irreg\u00adular operation. In this \ntype of operation, the body of one loop computes the bounds for another. In Psir\u00adrfan, this gives rise \nto a normal work distribution; we have also seen instances where computed bounds give rise to uniform \ndistributions. Another application program whose measurements are reported in section 6 is the UCLA General \nCircu\u00adlation Model [6]. Variants of this program are used widely to model trends in climate. We studied \na sec\u00adtion of this program, COMP3, that represents about 60% of the total execution time and is highly \nirregular. In this case, the irregularity arises during the simula\u00adtion, rather than directly from the \noriginal input data. COMP3 contains several physics modules which are only invoked when certain atmospheric \nconditions ex\u00adist. For example, in the cumulus clouds over Kenya, COMP3 models the thermal properties \nof water vapor; in the clear skies over the Sahara, it does not. To make matters worse, weather systems \ntravel large distantes across the Earth s surface, dragging along their special atmospheric conditions. \nAs with Psirrfan, these com\u00adputational irregularities are expresved as function calls and inner loops \nprotected by conditionals. The climate model has an additional important property. It updates its simulation \nstate using a two dimensional 5x5 stencil (over latitude and longitude). This means that any scheduling \nalgorithm must pre\u00adserve the program s communication locality by map\u00adping most adj scent grid elements \nto the same proces\u00adsor. 3 Context This section gives a brief overview of our extended For\u00adtran 77 compiler, \nand describes how we modified it to generate efficient code for irregular parallel programs, using the \nscheduling methods described in section 5. The compiler recognizes the following two exten\u00adsions for \nexplicitly specifying parallelism: DOALL and PARALLEL SECTION. DOALL denotes that all iter\u00adates of a \nloop can execute simultaneously. PARAL-LEL SECTION identifies sequences of program state\u00adments that can \nexecute independently. In addition, the compiler recognizes a subset of the Fortran D exten\u00adsions for \nspecifying data decomposition [14]. It han\u00addles BLOCKED and CYCLIC decompositions in one or two dimensions, \nand perfect alignment. The target architectures discussed in this paper are the Ncube\u00ad2 distributed memory \nmultiprocessor (512 processors) and the Cray Y/MP shared memory vector multipro\u00adcessor (8 processors). \nCompilation proceeds in four pipelined phases. In phase one, parallel operations are transformed to im\u00adprove \ndata locality and to reduce synchronization over\u00adhead. The compiler uses transformations including loop \ninterchange [4] and loop skewing [34] to expose coarse grained parallelism [32]. In addition, phase one \nperforms tiling transformations to improve the data locality of loop nests [31]. When compiling for distributed \nmemory machines, phase one also generates code to partition arrays among processors according to programmer-specified \ndecompositions. Following the convention of the Rice Fortran D compiler [14], our compiler uses the owner \ncomputes rule to make an initial assignment of loop it\u00aderations to processors. Under this rule, if an \narray ele\u00adment is the left hand side of an assignment statement, the processor holding that array element \ncomputes the right hand side of the statement. Our runtime scheduling system uses the distributed algorithm \ndescribed in section 5.2 to transfer owner\u00adship of array segments and their associated computa\u00adtions. \nThis algorithm operates under the assumption that data has been laid out to maximize communica\u00adtion locality. \nWhen re-assigning computations, the al\u00adgorithm avoids excessive data scattering by maintain\u00ading a minimum \ngrain size, Kmin. l{min determines the minimum number of iterations that can be trans\u00adferred as a block \nfrom one processor to another, This minimum grain size constrains the scheduler not to ex\u00adceed the bandwidth \nrequirements of the machine. On machines like the Ncube, which have a large message startup cost, the \nminimum grain size must also be large enough to keep the average number of messages per processor reasonably \nlow. Phase two of the compiler generates calls to the run\u00adtime communication library. It uses the dataflow \nalgo\u00adrithms described in [14] to compute send and receive sets for each parallel operation, and then \nuses this in\u00adformation to generate communication code. To sup\u00adport transfer of ownership, all messages \nfollow a for\u00adwarding protocol [19] to find their actual destinations. In phase three, the compiler generates \ncode blocks for each parallel operation. The code blocks are parametrized by grain size. Phase four gener\u00adates \nscheduling templates for each parallel operation. There are four types of scheduling templates: sequen\u00adtial, \nstatic, profile, and sample. The first of these executes the operation sequentially. The second uses \nthe owner computes rule on distributed memory ma\u00adchines, and allocates tasks to processors evenly on \nshared memory machines. The latter two use the dynamic scheduling algo\u00adrithms described below. They differ \nin how they gather information about the distribution of task execution times. Profile templates use \ninformation gathered from previous runs of the program to estimate execution time mean and variance [27], \nSample templates begin with a rough estimate of these two quantities and refine the estimates through \nruntime sampling of task execu\u00adtion times. On the Ncube and Cray Y/MP, the code performs sampling by \nreading a memory-mapped mi\u00adcrosecond clock. The scheduling algorithm preserves the output of sampled \ntasks, so that no repeated task execution is necessary. 4 Properties of Previous Ap\u00adproaches Researchers \nhave proposed many different approaches to scheduling parallel operations. Regular loops in scientific \nprograms have been scheduled using delay insertion [8] or combinatorial exploration of possible processor \nassignments [24]. These approaches are not applicable to irregular parallel operations because they require \nloop bounds and loop body execution times to be known at compile time. Sarkar and Hennessy have used \nexecution time esti\u00ad mates and a critical path algorithm to partition paral\u00ad lel programs statically \n[28]. Our algorithm for deter\u00ad mining a minimum grain size is similar to Sarkar s par\u00ad tition;g algorithm \nin that it balances communication costs against execution time estimates. Also, we use the algorithm \nreported in [27] to estimate execution time variance, Our scheduling algorithm is different in that it \nuses runtime information about execution time variance to determine the grain size at which a parallel \noperation is scheduled. A somewhat different approach to scheduling is load balancing[26]. Load balancing \nalgorithms run in paral\u00adlel with a computation; their goal is to re-assign tasks such that each processor \nends up with the same num\u00adber of pending tasks. There are two main types of load balancing algorithms: \nlocal and global. Local al\u00adgorithms attempt to achieve overall balance through communication among neighboring \nprocessors. In global algorithms, a central manager gathers load in\u00addices from each processor and directs \nthe transfer of tasks. These algorithms have two features which make them more applicable to distributed \noperating systems than to the scheduling of a single parallel program. First, they disregard communication \nrelationships be\u00adtween tasks. Second, the goal of these load balanc\u00ading algorithms is to maintain even \nnumbers of pending tasks on each processor. In contrast, the goal of the al\u00adgorithms presented here is \nto have all processors finish a parallel operation at the same time. Load balancing algorithms keep throughput \nhigh when parallelism is abundant, but lack the precision necessary to make a single parallel operation \nefficient. Yew and Tang have proposed a dynamic scheduling method called self-scheduling [29]. Self-scheduling \nfol\u00adlows the first available rule in assigning tasks to proces\u00adsors. Whenever a processor finishes executing \na task, it becomes available and requests a new task. Self\u00adscheduling produces even processor finishing \ntimes, even with uneven processor starting times. However, if the cost of scheduling a task is h then \nthe expected finishing time for N tasks is N(P + h)/p. Forp = h, the contribution of scheduling overhead \ndoubles the expected finishing time. Self-scheduling can be generalized so that processors are given \na chunk of K tasks whenever they become available. Kruskal and Weiss suggest a technique [16] for determining \nthe optimal value of K, given IV, p, u, and p. However, any method that uses a single chunk size K has \nexpected unevenness of Kp/2 in its processor finishing times. If we choose a large value of K, this unevenness \nwill be significant. If we choose a small value, we incur a large total scheduling overhead. We would \nlike to find a strategy that combines the low runtime overhead of large chunk sizes with the even finishing \ntimes of self-scheduling. In the next section, we investigate the finishing times of tapering methods, \nwhich use the first available rule but reduce runtime overhead by scheduling large chunks at the beginning \nof a parallel operation and successively smaller chunks as the computation proceeds. In theory, the smaller \nchunks should smooth out uneven finishing times left by the larger chunks. Polychronopoulis and Kuck \nsuggest a tapering method, guided-self-scheduling (GSS) [23], that chooses chunk size K~ = [l&#38;/pl \nwhere &#38; is the num\u00adber oft asks remaining after the i I t chunk has been scheduled (Rl = N). The \ngoal of this tapering rule is to smooth out uneven processor start times. It is op\u00adtimal when a = O, \nbut does not address the problem of variable task execution times.  5 Tapering Methods In this section, \nwe present our method for selecting a chunk size Ki, given the number of remaining tasks R~, number of \nprocessors p, and distribution(p,a). The method selects Ki using an expression of the form ~= (K~i.,.f(fi,K~in,~,h)), \nhere ~,. s the minimum chunk size and ~ is a function we will derive below. This expression yields GSS \nas a special case when u = O. The goal of any tapering method is to achieve opti\u00admally even finishing \ntimes while scheduling the small\u00adest possible number of chunks. When scheduling the it~ chunk, we would \nlike to pick the largest possible number of tasks Ki such that our expected maximum finishing time does \nnot increase. Define f inishi,j to be the time at which processor j has finished with any chunks numbered \nless than or equal to i. Let fmaxi = maxj (f inish~,j ), andZagic ~j (f maxi f inishi,j ) (iagO is the \ninitial unevenness in processor start times). If we schedule n chunks then fmax. is the finishing time \nof the computation, and lag. measures the total amount of processor idle time (the inefficiency) during \nthe computation (see figure 4 for an example). 5.1 The Fill Lemma First, we introduce a lemma that illustrates \na key prop\u00aderty of the first available rule. The goal of this lemma is to show how lagd+l depends on \nlagi. That is, given a particular unevenness in finishing times after schedul\u00ading chunk i, the lemma \ntells whether the scheduling of chunk i + 1 will cause smoothing or increase un\u00adevenness, and by how \nmuch. In the following, costi denotes the execution time of chunk i (note that the size of all chunks \ncould be 1, so this lemma applies to the scheduling of individual tasks). Lemma 1 If fmazi+l = fmaxi \nthen iagi+l == Jagi Costi+l. Otherwise, Iagi+l < (p l)costi+l. Proof. Case 1: f?71ClXi+l= fmax~: The \ncondition for this case states that the maximum Processor1 Processor2 Processor3 finl@h (5,2) :: : fmax(s) \nFigure 4: A Scheduling Scenario finishing time does not increase as a result of the scheduling of chunk \ni + 1. But, if this is true, there is some processor j such that f inishi+l ,j = f inishi,j + Costi+l \nand finishi~l,j < fma~i. If chunk i+ 1is assigned to processor j then for k # j, finishi+l, k = finish~,k. \nThus, lUgi+l = ~k#j(fma i finish~,k)+ f maxi (f inish~,j + Cdi+l) = lag~ cost~~~ . Case II: fmazi+l \n> fmazi: When the maximum finishing time, f maxi+l does increase, then there is some processor q such \nthat f inishedi,q = minj (f inishedi,j ) and f inishedi+l,q = finishe$,! + costi+l = fma~i+l. Since finishedi,q \nis the mmlmum finishing time after chunk i and only processor q s finish time changes as a result of \nthe scheduling of chunk i+ 1,~j (f maxi+l f inishj,j ) < (p l)costi+ln. We call Case I the fill-in \nrule, and Case II the excess rule. The point of this lemma is that, whenever the excess rule applies \n(i.e. fmUZi+l > fmazi), we can bound lagi+l independent of lagi. Corollary 1 Let 1 be the index of the \nh.st chunk for which fmaxl > fmaxl-l. Then we can bound lagn independent of lagili<l. Further, n lag. \n= lagl ~ COStj j=l+l Corollary 1 suggests that our analysis need only con\u00adsider the state of the computation \nfrom the last time the excess rule applies. Corollary 2 gives us a proce\u00addure for deciding when a chunk \ncan be the last chunk for which the excess rule applies. Corollary 2 If (iagi ~~=i+l costj ) <0 then \nchunk i can not be the lastchunk for which the excess rule applies, 5.2 Probabilistic Tapering We can \nuse the fill lemma as the basis for a probabilis\u00adtic tapering method. Given execution time variance, \nwe cannot know the exact time necessary to execute any task or group of tasks. Therefore, given a bound \nb that expresses the maximum allowable inefficiency, we seek a scheduling method that makes lagn < b \nwith high probability. We will express the new scheduling method as a rule for computing a set of chunk \nsizes Kl,.. ., K. such that, if tasks are handed out with these chunk sizes, lagn < b with high probability. \nWe denote as ~ill~ the term ~~=i+l Codj in Corol\u00adlary 2 and assert a final corollary to Lemma 1. This \ncorollary is what makes a probabilistic analysis of ta\u00adpering methods tractable. Corollary 3 lag. ~ maxj((p \n l)cost~ -~ill~) We can use this expression to limit the effect of each chunk K~ on lagn such that lagn \nis less than the given bound b with high probability. Suppose that at time i, there are &#38; tasks remaining \nand that our tapering method chooses to schedule a chunk con\u00adtaining Ki tasks. Then we can rewrite corollary \n3 as lag. s l)t(Ki) where t(Ki) and rnaxi((pt(lli+l)),f,(~+l) are random variables representing the \nactual time taken to execute Kj and Ri+l tasks, respectively. Let Z~= (P -l)t(Ki) t(Ri+I).We would like \nto find the largest value of Ki such that Pr[Zi > b]< c for a given bound b and probabilityy e. Using \nChebychev s inequality we find Pr[Zi S(Zi) > a] < u~,/a2, where U}i is the variance of Zi and ~(Zi) \nis the expected value of Zi. If we let a = b &#38;(Zi)j then we have Pr[Zi > b]< ~~i/(b g(Zi))2. Let \nPr[Zi > b]= ~. Substituting ~(Zi) = (~Ki I&#38;)p (using Ri+l = Ri Ki) andC&#38; = a2(((p 1)2 l)Ki \n+ ~) (where ~2 and p are the variance and mean of the original task cost distribution), we have ~ < ~2(((p \n 1)2 l)Ki + Ri)/(b (p~i Ri)P)2 (1) The partial derivative of (1) s right hand side with respect to \nKi is always positive. Thus, if we set the right hand side of (1) equal to c and solve for Ki, we will \nfind the largest Ki such that ((p l)cosi!i ~illi ) < b with probability at least 1 e. A Practical Algorithm \nChebychev s inequality is valid for all distributions. In practice, this means that it yields a needlessly \nconservative value for Ki. Note that both i!(Ki) and i!(~+l) are sums of individual task ;p-l)t(K[i]) \nvariance = 1 variance =4 Figure 5: Illustration of Ki selection. costs. By the Central Limit Theorem, \nthe sum of K independent variates approaches a normal distribution as K increases. For the distributions \nfound in irregular programs (normal, multinominal, uniform, exponential) this approach is rapid. Our \ncurrent runtime system uses the following method for choosing K~, based on the assumption that the distribution \nof Zi is normal. Using a table we can find a value a such that Pr[Zi S(Zi) > OWZJ < c for a given value \nof e. Put simply, we are guessing that Zi will not exceed its expected value by more that a standard \ndeviations. Since we want Pr[Zi > b]< c for some bound b, we have $(Zi) + auzi = b. Substituting for \nL (Zi) and az, as before and letting b= Owe have (Ri -pKi)~ = ~~<((~ -1)2-l)Ki + Ri (2) Let Ti = R~/p, \nand v. = au/IJ. Then (2) becomes T~\u00ad i= v~~ lntuitively i s less than Ti = Ri/p by an amount related \nto the variance of the work distribution. If we approximate ((p 1)2 1)/p2 as 1 and ~/p2 as O, then \nsolving for Ki yields Ki= T@-va~-(3) [ 1  Figure 5 shows how equation (3) maintains the in\u00advariant Pr[Zi \n> O] < e. Zi is the difference between two random variables (p l)i!(.l{i) and t(&#38;+l). Ex\u00adpression \n(3) sets the distance between the means of these two variables to be some number of standard de\u00adviations, \nspecifically a. When u = O then the expres\u00adsion sets the means to be the same value: (p l)Ri/P (like \nGSS). As the variance increases (3) increases the distance between the means to maintain the invariant. \nThus, for a given scheduling event i, we can ensure that Pr[Zi > 0] < e for any ~ by seIecting the corre\u00adsponding \nnumber of standard deviations, a, from a ta\u00adble. However, we would like to derive a single value of a \nI 1 ! 1 1 1 ! 1 , 0.0 0.4 0.S 1.2 1.6 2.0 2.4 alpha Figure 6: Effect of a. Lines labeled with IV/p. \nfor the entire parallel operation. To do this we require an expression that, given c, yields Pr[ma~ (Zi) \n> b]. It is an open question whether such an expression can be found; without it, determining the best \nvalue for a is analytically intractable. In practice, it is possible to discover a sufficiently accurate \nvalue for a empirically. Two parameters, the scheduling overhead (h) and the ratio of tasks to pro\u00adcessors \n(N/p) can affect the optimum value of a. Using a normal distribution, we measured the optimum value of \na over the entire possible range of both parameters, varying h from O to co and iV/p from 1 to co. Figure \n6 summarizes the results for some typical values of IV/p and with h= 1. Over all combinations of h and \niV/p, we found that the value a = 1.3 was within 3% of op\u00adtimum. All of the performance results reported \nin this paper were obtained using this single value for a. Figure 6 also indicates that a scheduling \nmethod us\u00ading (3) can withstand considerable inaccuracy in the value of fi, since Va in (3) is just the \nscaled coefficient of variation a 2. For example, we found that for all of the benchmark applications \ndiscussed in section 6, simply using (3) and setting Va = 3 yielded better per\u00adformance than the other \nscheduling methods tested. In all cases, runtime measurement of ~ further improved performance. Incorporating \nOverhead Equation (3) implicitly addresses overhead by selecting the largest number of tasks Ki that \nmeets the constraint Pr[Z~ > O] < c. If we explicitly account for overhead, we can improve our value \nfor I{i. We represent overhead through the pa\u00adrameter K~~n, the minimum chunk size. To determine ~{m~., \nwe use two additional parameters: .Kbandand K3Cb.~. We noted in section 3 that the compiler computes \nthe minimum chunk size, Kband, necessary to en- TAPER100% \u00ad,0::+ss 8W0 -~// </:/ @ / . //   ?? _,Lo/77- \n- EIv ~ 60% - ?! /:/ 40% - .+ 20% - .P , 0%~fy I 101 , llY ( IF llY N/p Figure 7: Binomial Distribution \nPr[60000]=0.1, Pr[200]=0.9 sure that communication between processors contain\u00ading logically adjacent \nchunks does not exceed the band\u00adwidth requirements of the machine (.Khand is zero on shared memory multiprocessors). \nThe other param\u00adeter) Ksched, is smallest chunk size such that the mean time to execute ~~c~ed tasks \nexceeds h, the overhead of scheduling a chunk. We set K~~. = rnin(Kband, Ksehed, N/p) . When Kmin = N/p, \nwe can t improve on a static schedule for the parallel operation. However, we can modify equation (3) \nsuch that it yields a dynamic scheduling method that outperforms static scheduling whenever Kmin < N/p. \nWe know that if all chunks contain K~i~ or more tasks, the average value for iag~ will be will be at \nleast K~i~P#/2. Hence, there is no reason to require 2; < 0 and therefore b (the bound on accept able \nlagn) = O~n the derivation above. If we set b= Kminpp/2, then equation (3) still holds, provided we set \nTi= * +Km~n /2. Our final expression for computing Ki becomes K~ = max K~i., ll+$-vod\u00ad1) ([ (4) Combining \nour techniques for selecting a and Kmin with equation (4) we have an algorithm for dynamic scheduling. \nWe call the algorithm TAPER. Figures 7 through 9 compare the performance of TAPER with guided self-scheduling \n(GSS), self-scheduling (SS), and static assignment (SA) for some synthetic work distri\u00adbutions. Even \nStarting Times The derivation and simu\u00adlation results given above demonstrate that TAPER yields a near-optimal \nschedule assuming only lago < N (i.e. processors can start at different times) and that task costs are \nindependent random variables. Further, method (called EVENSTART) in situations where all processors begin \nsimultaneously. This method uses DISTANCE for the first p chunks and TAPER there\u00adafter. Figure 10 illustrates \nhow EVEN START main\u00adtains a small unevenness in chunk finish times through\u00adout a parallel operation. \n Figure 8: Uniform Distribution on [0,10]. Overhead = 100%\u00ad . Y.l 80% . . * O.e~.OR S .\\* - G . . , s \ns , GSS SA ~ 60% - \\ \\ 40% - \\, \\>s 20%~ -0.5 0.0 0,5 1.0 1.5 2,0 : OveAead(inTasks) Figure 9: Performance \non Pr[lO] = 0.9, Pr[l] = 0.1 for different overheads. TAPER isstateIess. Adding tasks only increases \n~illi; removing processors only decreases (p l)t(~i). If the scheduling algorithm is given additional \ninfor\u00admation about processor starting times, it can do bet\u00adter. For example, suppose we know that processors \nwill execute the entire parallel operation and that all processors start at the sometime. Wecanuse this \nin\u00adformation to choose larger chunk sizes than TAPER. Let s~ be the time at which the ith chunk is assigned. \nLetD~=~ ~. This is the distance in tasks between Si and the expected finishing time of the computation. \nTaking into account the variance in possible finishing times yields I{i = max(K~~~, [D, -va~). We call \nthis method for selecting K~,the DISTANCE method. The difference between DISTANCE and TA-PER is greatest \nfor the first p chunks scheduled. Fur\u00adther, it is expensive to maintain globally meaning\u00adful values for \nSi. For these reasons, we use a hybrid ~ GSS TAPER SA . . v . \\ L \u00ad \\. k:~====:- v>. EvenStart , ,, \n0 2040 6080 100 120 140 164 Figure 10: As chunks are scheduled, lagi decreases. Determining the Distribution \nWhen profiling in\u00adformation is not sufficient to provide values for p and u, we need to discover these \nvalues at runtime. This is done by having each processor randomly select a few tasks from its large initial \nchunk and measure the execution times for these tasks. This information is accumulated as processors \nrequest later chunks. We therefore need a method to pick the first p chunk sizes. One technique is to \nuse Ki = N/2p for the first p chunks, since TAPER will always allocate more than half the work in the \nfirst p chunks. Hummel and Shoen\u00adberg have proposed a similar scheduling rule based on a different analysis \n[15]. This method is excessively conservative for large N/p. We chose instead to esti\u00admate that ~ = 3. \nThis ad hoc technique works well in practice ~ecause the estimate is quickly updated on each processor \nas sampling information accumulates. Nonindependent Task Costs The above discus\u00adsion assumes that task \nexecution times are indepen\u00addent random variables. If execution times are corre\u00adlated, then we have two \nchoices. First, we can ran\u00addomize the iteration space. That is we can randomly permute the tasks of a \nparallel operation so that we can use the independence assumption. This is possible for some programs \nlike Psirrfan, where there is little communication locality. For the COMP3 benchmark, communication requirements \nmake this transformation counterproductive. Second, we can do additional sampling of the task costs to \nbuild a cost function. A cost function esti\u00admates task execution times as a function of iteration number. \nWhen determining Ki we begin with the es\u00adtimate Ki = (Ri /pl. We then use the cost function to determine \nthe distribution over the particular set of Ki tasksselected. Given the distribution estimate (p,u), \nwe refine the value for Ki and obtain a new dis\u00adtribution estimate. To find the correct value for Ki \nwe must scale the value we obtain from (4) by s = pj /pC. In this expression, p$ is the global mean and \np. is the mean for the tasks in the current chunk. We have found the cost function technique to be ex\u00adtremely \neffective because most parallel operations have considerable distribution coherence. That is, succes\u00adsive \nexecutions of the same parallel operation (such as a parallel inner loop) will have almost the same cost \nfunction. Distribution coherence is a temporal local\u00adity property. Just as caches take advantage of mem\u00adory \naccess locality, we can take advantage of distribu\u00adtion coherence by cacheing our assignments of tasks \nto processors. On distributed memory machines, this cacheing takes the form of transferring blocks of \narray elements between processors. On shared memory ma\u00adchines, each processor stores its own task assignments. \nDistributed Memory On distributed memory ma\u00adchines, we can no longer model the effect of scheduling a \nchunk of tasks with a fixed overhead h. Each task may require a certain amount of data for its computa\u00adtion, \nso there will be a per-task as well as a per-chunk transfer cost. Further, we need to preserve communi\u00adcation \nlocality by maintaining a minimum chunk size, I<min, We solve this problem by beginning with some original \ndata decomposition and assigning tasks to pro\u00adcessors according to the owner computes rule. As we gain \ninformation about the work distribution, we refine the data decomposition. In the distributed algorithm \nthe p processors are log\u00adicall y connected as a binary tree with p leaves. Some of the processors act \nas both leaves and internal nodes of the tree. We modify TAPER so that it chooses chunk sizes in epochs \nof p chunks. Adding this con\u00adstraint to the derivation above vields the scheduling rule Kj = max(K mi., \n[~~ ~~~-1) (wher~ l{j is the number of tasks in the p chunks of the ~ h epoch). All processors start \nin epoch O. When a processor begins executing a chunk it sends its current epoch value (called a token) \nto its parent, which passes the token to its parent (possibly combining messages from both children). \nWhen the root receives p tokens from the same epoch, it increments the global epoch value and broadcasts \n(through the tree) a message to all pro\u00adcessors. The message tells the processors to increment GSS/ . \nSS-OFT 0.8 - *Y. --- BIDCXED CYCLIC ~,~ _ ;:2:7: ./  o..~ o 10QO 2000 3000 4000 50 Numberof Cells \nFigure 11: AMR on Ncube their epoch value and may also tell some processors to transfer a chunk of tasks \n(and their associated data) to another processor. Processors compete for the p chunks of each epoch. \nIf processor a can get two tokens of value i to the root before processor b can send one token of value \ni, then the root will re-assign processor b s chunk of size Ki to processor a, Processor b is then forced \nto re-interpret the chunk it is currently executing as be\u00adlonging to some later epoch (and thus containing \nfewer tasks). If most of the actual task cost is on a few pro\u00adcessors, this scheme will degenerate into \nthe central\u00adized TAPER algorithm. If task costs are independent then we expect most tasks to remain on \nthe processor owning them at the beginning of the parallel opera\u00adtion; thus, the algorithm reduces task \ntransfer costs and maintains communication locality. 6 Benchmarks This section gives performance results \nfor several real applications that contain irregular parallel operations. For each application we give \na brief description of its computational characteristics. The appendix gives graphs comparing the performance \nof GSS, SS, SS with optimal single chunk size (SS-OPT), static assignment (SA or BLOCKED and CYCLIC decompositions), \nand TAPER on each application. Efficiencies are reported against optimized sequential code. Cray results \nare for 8 processors, Ncube results are for 512 processors. To be fair to SS and GSS, we modified these \nmethods to use the distributed algorithm given in section 5.2. Our first application, AMR, is an adaptive \nvortex method for computing fluid flow [5]. The method uses a finer grid size wherever vortices are present. \nIts com\u00adputational structure is similar to our climate example, but it has more floating point operations \nper memory 1.0  T?== 0.9 / ,i0.4~ o 10IN 2(XXI 3000 4ci00 5L.1 NurnbaofCells Figure 12: AMR on Cray \nY/MP 0 TAPRR / /0./0 / ./O / / / , 0 2 4 6 8: ItaationNumber Figure 13: First few iterations of AMR \ninner loop on Ncube. access. This example illustrates the power of distribu\u00adtion coherence. For the first \nfew seconds of the short computation, TAPER builds the cost function. Then it reaches a point of maximum \nefficiency where the cost function needs only incremental improvement (see fig\u00adure 13). We have already \nintroduced the features of COMP3, a section of the UCLA General Circulation Model [6]. Note that, for \nsmall data sizes, no method can exe\u00adcute COMP3 efficiently because of the climate model s communication \nrequirements. We have also introduced Psirrfan, an x-ray tomography application. For Psir\u00adrfan, we include \na comparison with a representative local load balancing algorithm [26]. Our final application, EMU, is \na timing simula\u00adtor that is part of the MULGA circuit design system [1, 20]. Unlike the applications \nseen thus far, EMU has an exponential work distribution. EMU divides a ~ 0.8 i! ~ 0.7 Ii ~ 0.6 0.5 1, \n, I 0.4 5000 loIXlo 15000 2oooa Number ofCells Figure 14: COMP3 on Ncube I 0.2 t ,, I I,, 0 00 100 200 \n300 400 500 600 700 800 [ Detector Resolution Figure 15: Psirrfan on Ncube 1.0 TAPER ,~  ./ t B 0.8 \nY..nad Balwing I 0 ~ S 0 6 = i% n I 0.4} 02~ O 1002003034C0 5OO6OO7W8OO DetectorResolution Figure 16: \nPsirrfan Load Balancing Comparisoncircuit into regions; elements of a particular region are connected \nby pass transistors. For each region, EMU 1.0   References [1]B. Ackland, S. Lucco, T. London, and \nE. DeBenedic- I Ss .//- 0,0~ o 20M 401XI 6000 8000 lWOO 1209!l CwcnitRegim Figure 17: EMU on Ncube uses \na backward Euler integration to update voltage values. If the numerical method diverges, EMU subdi\u00advides \nthe timestep and re-integrates.  Summary In this paper, we introduced a methodology for compil\u00ading \nand executing irregular parallel programs. We de\u00adveloped a new dynamic scheduling algorithm for such \nprograms and extended the algorithm to work on dis\u00adtributed memory machines. We also extended the al\u00adgorithm \nto build cost functions for nonindependent task cost distributions, This modification was espe\u00adcially \nsuccessful because of distribution coherence. If a parallel operation has distribution coherence, suc\u00adcessive \nexecution instances of the operation will have nearly identical work distributions. We modeled the effect \nof communication locality through a minimum grain size K~~n; this method was sufficient for three real \napplications involving stencil communication. Fi\u00adnally, we developed a relationship between total avail\u00adable \nparallelism, optimal grain size, and execution time variance. We applied this relationship to the problem \nof choosing grain sizes for parallel operations. We plan to continue this work by incorporating the compiler \nand runtime techniques described above into the Rice Fortran D compiler [14]. We are currently in\u00adcorporating \nthese techniques into a coordination lan\u00adguage system [21, 22] and an object-oriented parallel language \n[18] that uses runtime techniques to optimize communication patterns [17]. We are also working on compilation \ntechniques for statically discovering cost functions and for proving distribution coherence at compile-time. \nFinally, we plan to make available, in conjunction with the Advanced Computing Research Facility at Argonne \nNational Labs, a benchmark suite of irregular programs. tis. CEMU: A Parallel Circuit Simulator, . In \nPr-o\u00ad ceedirzgs of the International Conference on Computer Design, October 1986. [2] F. Allen, M. Burke, \nP. Charles, R. Cytron, and J. Fer\u00adrante. An Overview of the PTRAN Analysis System for Multiprocessing, \n. Journal of Parallei and Dis\u00adtributed G ornputing, 5:617-640, October 1988. [3] J. R. Allen, D. Baumgartner, \nK. Kennedy, and A. Porterfield. PTOOL: A Semi-Automatic Parsl\u00adlel Programming Assistant, . In International \nCora\u00adference on Parallel Processing, pages 164-170, 1986. [4] J. R. Allen and K. Kennedy. Automatic Loop \nInter\u00adchange, . In Proceedings of the SIGPLAN Syrnposiurn on Compiler Construction, pages 233 246, June \n1984. [5] A. Almgren. A Fast Adaptive Vortex Method Using Local Corrections. PhD thesis, Center for Pure \nand Applied Mathematics, UC/Berkeley, 1991. [6] A. Arakawa and V. R. Lamb. Computational Design of the \nBasic Dynamical Processes of the UCLA Gen\u00aderal Circulation Model, . Methods in Computational Physics, \n17:173-265, 1977. [7] D. Callahan, K. Cooper, K. Kennedyj and L. Torczon. Interprocedural Constant Propagation, \n. In Proceed\u00adings of the SIGPLAN Symposium on Compiler Con\u00adstruction, pages 152 161, Palo Alto, 1986. \n[8] R. Cytron. Limited Processor Scheduling of Doacross Loops, . In Proceedings ICPP, pages 226-234, \n1987. [9] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and K. Zadeck. An Efficient Method of Computing \nStatic Single Assignment Form, . In ACM Conference ova Principles of Programming Languages, pages 25 \n35, 1989. [10] M. Girkar and C. Polychronopoulos. Partitioning Programs for Parallel Execution, . 1988 \nInternational Conference on Supercomputing (ICS), pages 217-229, July 1988. [11] E. Gumbel. The Maxima \nof the Mean of the Largest Value of the Range, . Annals of Mathematics and Statistics, 25, 1954. [12] \n0. Hansson and A. Mayer. Heuristic Search as Evi\u00addential Reasoning, . In Proceedings of the Fifth Work\u00adshop \non Uncertainty in AI, August 1989. [13] K. A. Heiskanen. Tomography with Limited Data in Fan Beam Geometry. \nPhD thesis, UC/Berkeley, 1990. [14] S. Hiranandani, K. Kennedy, and C.-W. Tseng. Com\u00adpiler Support for \nMachine-Independent Parallel Pro\u00adgramming in Fortran D, . Technical Report TR91\u00ad149, Rice University, \nMarch 1991. [15] S. F. Hummel, E. Schonberg, and L. E. Flynn. Fac\u00adtoring: A Practical and Robust Method \nfor Schedul\u00ading Parallel Loops, . Technical Report 74172, IBM Research Division, 1991. [16] C. Kruskal \nand A, Weiss. Allocating Independent Subtasks on Parallel Processors, . IEEE Transactions on Software \nEngineering, SE-11, October 1985, [17] S. Lucco. A Heuristic Linda Kernel for Hyper\u00adcube Multiprocessors, \n. In Proceedings of the Second Conference on Hgpercube Multiprocessors, September 1987. [18] S. Lucco. \nParallel Programming in a Virtual Object Space, . In Conference on Object-Oriented Program\u00adming Systems, \nLanguages, and Applications, October 1987. [19] S. Lucco and D. Anderson. Tarmac: A Language Sys\u00adtem \nSubstrate Based on Mobile Memory, . In Interna\u00adtional Conference on Distributed Computing Sgstems, 1990. \n[20] S. Lucco and K. Nichols. A Performance Analysis of Three Parallel Programming Methodologies in the \nContext of MOS Timing Simulation, . In Digest of Papers: IEEE Compcon, pages 205-210, 1987. [21] S. Lucco \nand O. Sharp. Delirium: An Embedding Coordination Language, . In Proceedings of Super\u00adcomputing 90, pages \n515 524, November 1.990. [22] S. Lucco and O. Sharp. Parallel Programming With Coordination Structures, \n. In ACM Conference on the Principles of Programming Languages, January 1991. [23] C. Polychronopoulis \nand D. Kuck. Guided Self-Scheduling: A Practical Scheduling Scheme for Par\u00adallel Supercomputers, . IEEE \nTransactions on Comp\u00aduters, C-36(12), December 1987. [24] C. Polychronopoulos and U. Banerjee. Speedup \nBounds and Processor Allocation for Parallel Pro\u00adgrams on a Multiprocessor, . Proceedings of the 1986 \nInternational Conference on Parallel Processing, pages 961-968, August 1986. [25] C. Polychronopoulos, \nM. Glrkar, M. R. Haghlghat, C. L. Lee, B. Leung, and D. Schouten. Parafrase-2: An Environment for Parallelizing, \nPartitioning, Syn\u00adchronizing, and Scheduling Programs on Multiproces\u00adsors, . In Proceedings of the International \nConference on Parallel Processing, volume II, pages 39 48, 1989. [26] L. Rudolph, M. Slivkin-Allalouf, \nand E. Upfal. A Simple Load BaJancing Scheme for Task Allocation in Parallel Machines, . In ACM Symposium \non Parallel Algorithms and Architectures, 1991. [27] V. Sarkar. Determining Average Program Execution \nTimes and their Variance, . In SIGPLA N Conference on Programming Language Design and Implementa\u00adtion, \nJune 1989. [28] V. Sarkar and J. Hennessey. Partitioning Parallel Programs for Macro Dataflow, . In ACM \nConference on Lisp and Functional Programming, pages 202 211, Cambridge, Mass., 1986. [29] P. Tang and \nP.-C. Yew. Processor Self-Scheduling for Multiple Nested Parallel Loops, . Proceedings of the 1986 International \nConference on Parallel processing, pages 528 535, August 1986. [30] S. Ulam and N. Metropolis. The Monte \nCarlo Method, . Journal of the American Statistics Asso\u00adciation, 44:335R, 1949. [31] M. E. Wolf and M. \nS. Lam. A Data Locality Op\u00ad timizing Algorithm, . In SIGPLAN Conference on Programming Language Design \nand Implementation, pages 30 44, June 1991. [32] M. E. Wolf and M. S. Lam. A Loop Transformation Theory \nand an Algorithm to Maximize Parallelism, . IEEE Transactions on Parallel and Distributed Sys\u00adtems, 2(4):452 \n471, October 1991. [33] M. Wolfe. Optimizing Supercompilers for Supercom\u00adputers. PhD thesis, University \nof Illinois at Urbana-Champaign, October 1982, [34] M. Wolfe. More Iteration Space Tiling, . In Proceed\u00adings \nof Supercomputing, pages 655-664, 1989. \n\t\t\t", "proc_id": "143095", "abstract": "<p>This paper develops a methodology for compiling and executing <italic>irregular</italic> parallel programs. Such programs implement parallel operations whose size and work distribution depend on input data. We show a fundamental relationship between three quantities that characterize an irregular parallel computation: the total available parallelism, the optimal grain size, and the statistical variance of execution times for individual tasks. This relationship yields a dynamic scheduling algorithm that substantially reduces the overhead of executing irregular parallel operations.</p><p>We incorporated this algorithm into an extended Fortran compiler. The compiler accepts as input a subset of Fortran D which includes blocked and cyclic decompositions and perfect alignment; it outputs Fortran 77 augmented with calls to library routines written in C. For irregular parallel operations, the compiled code gathers information about available parallelism and task execution time variance and uses this information to schedule the operation. On distributed memory architectures, the compiler encodes information about data access patterns for the runtime scheduling system so that it can preserve communication locality.</p><p>We evaluated these compilation techniques using a set of application programs including climate modeling, circuit simulation, and x-ray tomography, that contain irregular parallel operations. The results demonstrate that, for these applications, the dynamic techniques described here achieve near-optimal efficiency on large numbers of processors. In addition, they perform significantly better, on these problems, than any previously proposed static or dynamic scheduling algorithm.</p>", "authors": [{"name": "Steven Lucco", "author_profile_id": "81100431977", "affiliation": "", "person_id": "PP31085536", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143134", "year": "1992", "article_id": "143134", "conference": "PLDI", "title": "A dynamic scheduling method for irregular parallel programs", "url": "http://dl.acm.org/citation.cfm?id=143134"}