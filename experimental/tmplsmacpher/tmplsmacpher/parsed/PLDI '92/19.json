{"article_publication_date": "07-01-1992", "fulltext": "\n Lazy Code Motion Jens Knoop Oliver Ruthing$ Bernhard Steffen CAU Kielt CAU Kielt RWTH Aachen$ jk@informatik.uni-kiel. \ndbp.de or@informatik. uni-kiel.dbp.de bus@zeus.informatik. rwth-aachen.de Abstract We present a bit-vector \nalgorithm for the optimal and economical placement of computations within flow graphs, which is as eficient \nas standard uni-directional analyses. The point of our algorithm is the decom\u00adposition of the hi-directional \nstructure of the known placement algorithms into a sequence of a backward and a forward analysis, which \ndirectly implies the etll\u00adciency result. Moreover, the new compositional struc\u00adture opens the algorithm \nfor modification: two further uni-directional analysis components exclude any un\u00adnecessary code motion. \nThis laziness of our algorithm minimizes the register pressure, which has drastic ef\u00adfects on the run-time \nbehaviour of the optimized pro\u00adgrams in practice, where an economical use of registers is essential. \nTopics: data flow analysis, program optimization, par\u00adtial redundancy elimination, code motion, bit-vector \ndata flow analyses. 1 Motivation Code motion is a technique to improve the efficiency of a program by \navoiding unnecessary recomputations *Part of the work was done, while the author was supported by the \nDeutsche Forschungsgemeinschaft grant La 426/9-2. t In~titut ffir InfOmatik ~nd prakti~che Mathematik, \n Christian-Albrechts-Universit3t, Preufierstrafie 1-9, D-2300 Kiel 1. t The author is supported by the \nDeutsche Forschungsgemein\u00ad schaft grant La 426/11-1. sLehr~tuhl fur Infomatik II, Rheinisch-Westfali~&#38;e \nTe&#38;ni\u00ad sche Hochschule Aachen, Ahornstrat3e 55, D-51oo Aachen. Permission to copy without fee all \nor part of this material is granted provided that the copias are not made or distributed for direct commercial \nadvantaga, the ACM copyright notica and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, raquires a fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA e 1992 ACM 0-89791 \n-476-71921000610224 . ..$ 1.50 of a value at run-time. This is achieved by replac\u00ading the original computations \nof a program by auxil\u00adiary variables (registers) that are initialized with the correct value at suitable \nprogram points. In order to preserve the semantics of the original program, code motion must additionally \nbe sa$e, i.e. it must not in\u00adtroduce computations of new values on paths. In fact, under this requirement \nit is possible to obtain computationaily optimal results, i.e. results where the number of computations \non each program path can\u00adnot be reduced anymore by means of safe code mo\u00adtion (cf. Theorem 3.9). Central \nidea to obtain this optimality result is to place computations as early as possible in a program, while \nmaintaining safety (cf. [Dh2, Dh3, KS2, MR1, St]). However, this strategy moves computations even if \nit is unnecessary, i.e. there is no run-time gainl. This causes superfluous register pressure, which \nis in fact a major problem in practice. In this paper we present a lazy computationally op\u00adtimal code \nmotion algorithm, which is unique in that it e is as eficient as standard uni-directional analyses and \ne avoids any unnecessaryy register pressure. The point of this algorithm is the decomposition of the \nhi-directional structure of the known placement algo\u00adrithms (cf. Related Work below) into a sequence \nof a backward analysis and a forward analysis, which di\u00adrectly implies the efficiency result. Moreover, \nthe new compositional structure allows to avoid any unneces\u00adsary code motion by modifying the standard \ncomputa\u00adtionally optimal computation points according to the following idea: e Initialize as late as \npossible while maintaining computational optimality. Together with the suppression of initializations, \nwhich are only going to be used at the insertion point it\u00ad 1In (Dhslunnecessarycode motions are called \nredundant. self, this characterizes our approach of lazy code mo\u00adtion, Figure 1 displays an example, \nwhich is complex enough toillustrate the various features of the new ap\u00adproach. It will be discussed \nin more details during the development in this paper. For now just note that our algorithm is unique \nin performing the optimization dis\u00adplayed in Figure 2, which is exceptional for the follow\u00ading reasons: \nit eliminates the partially redundant com\u00adputations of a+b in node 10 and 16 by moving them to node 8 \nand 15, but it does not touch the computa\u00adtions of a+ b in node 3 and 17 that cannot be moved with run-time \ngain. This confirms that computations are only moved when it is profitable. 1 ~ 2 a:=c 1 4 3 x:= a+b \n4 5 6 89 i 1(J y:= a+b 11 12 JJ 14 16 z:=a+b 181 1 Figure 1: The Motivating Example Related Work In \n1979 Morel and Renvoise proposed a bit-vector al\u00adgorithm for the suppression of partial redundancies \n[MR1]. The hi-directionality of their algorithm became model in the field of bit-vector based code motion \n(cf. [Ch, Dhl, Dh2, Dh3, DS, JD1, JD2, Mo> MR2, So]). Bi-directional algorithms, however, are in general \ncon\u00adceptually and computationally more complex than uni\u00addirectional ones: e.g. in contrast to the uni-directional \ncase, where reducible programs can be dealt with in O(n /og(n)) time, where n characterizes the size \nof the argument program (e.g. number of statements), 2 a:=c i 3 x:= a+b 4 5 61 7 8 h:=a+b 9 J 10 y:=h \n11 12 J llj h:=a+b14 y:=h  16z:=h 181 I Figure 2: The Lazy Code Motion Transformation the best known \nestimation for hi-directional analyses is 0(n2) (cf. [Dh3]). The problem of unnecessary code motion is \nonly addressed in [Ch, Dh2, Dh3], and these proposals are of heuristic nature: code is unnecessarily \nmoved or redundancies remain in the program. In contrast, our algorithm is composed of uni\u00addirectional \nanalyses2. Thus the same estimations for the worst case time complexity apply as for uni\u00addirectional \nanalyses (cf.[AU, GW, HU1, HU2, Ke, KU1, Tal, Ta2, Ta3, Ull]). Moreover, our algorithm is con\u00adceptually \nsimple. It only requires the sequential com\u00adputation of the four predicates D-Safe, Earliest, Latest, \nand Isolated. Thus our algorithm is an ex\u00adtension of the algorithm of [St], which simply computes the \npredicates D-Safe and Earliest. The two new predicates Latest and Isolated prevent any unnec\u00adessary code \nmotion. 2 Preliminaries We consider variables v E V, terms t E T, and directed jlow graphs G= (N, E,s, \ne) with node set N and edge set E. Nodes n c N represent assignments of the 2Such an algorithm was first \nproposed in [St], which later on was interprocedurally generfllzed to programs with procedures, local \nvariables and formal parameters in [KS2]. Both algorithms reaKze an as early as possible placement. 225 \nform v := t and edges (r-n,n) ~ E the nondeterminis\u00adtic branching structure of G3. s and e denote the \nunique start node and end node of G, which are both assumed to represent the empty statement skip and \nnot to possess any predecessors and successors, respec\u00adtively. Every node n c N is assumed to lie on \na path from s to e. Finally, Succ(n)=ti { m I(n, m) c E} and precl(n)=df { m I(m, n) c E} denote the \nset of all successors and predecessors of a node n, respectively. For every node n = v:= t and every \nterm t c T \\V we define two local predicates indicating, whether t! is used or modified4: e Used(n, t \n)=d~ t E SubTerms(t) and 0 Thmsp(n, t )=dj v @ Var(t ) Here SubTerms(t) and Var(t ) denote the set of \nall subterms of t and the set of all variables occurring in t, respectively, Conventions: Following [MR1], \nwe assume that all right-hand-side terms of assignment statements contain at most one operation symbol. \nThis does not impose any restrict ions, because every assignment statement can be decomposed into sequences \nof assignments of this form. As a consequence of this assumption it is enough to develop our algorithm \nfor an arbitrary but fixed term here, because a global algorithm dealing with all program terms simultaneously \nis just the in\u00addependent combination of all the term algorithms . This leads to the usual bit-vector \nalgorithms that real\u00adize such a combination efficiently (cf. [He]). In the following, we fix the flow \ngraph G and the term tG T \\ V, in order to allow a simple, unparame\u00adterized notation, and we denote the \ncomputations of t occurring in G as original computations. Computationally Optimal  Computation I?oints \n In this section we develop an algorithm for the u early as possible placement, which in contrast to \npre\u00advious approaches is composed of uni-directional anal\u00adyses. Here, placement stands for any program \ntrans\u00adformation that introduces a new auxiliary variable h 3 We do not assume any structural restrictions \non G. In fact, every algorithm computing the fixed point solution of a uni-directional bit-vector data \nflow analysis problem may be used to compute the predicates D-Safe, Earliest, Latest, and Isolated (cf. \n[He]). However, application of the efficient tech\u00adniques of [AU, GW, HU1, HU Z, Ke, KU1, Tal, Ta2, Ta3, \nUll] requires that G satisfies the structural restrictions imposed by these algorithms. 1Flow ~aphs composed \nof basic blocks can be treated entirely in the same fashion replacing the predicate Used by the predicate \nAnt~oc (cf. [MR1]), indicating whether the computation of t is locally anticipatable at node n. for t,inserts \nat some program points assignments of the form h:= t,and replaces some of the original com\u00adputations \nof t by h provided that this is correct, i.e. that h represents the same value. Formally, two com\u00adputations \nof t represent the same value on apath if and only if no operand of t is modified between them. With \nthis formal notion of value equality, the correct\u00adness condition above is satisfied for a node n if and \nonly if on every path leading from s to n there is a last initialization of h at a node where t represents \nthe same vahse as in n. This definition of placement obviously leaves the freedom of inserting computations \nat node entries and node exists. However, one can easily prove that after the edge splitting of Section \n3.1 we can restrict our\u00adselves to placements that only insert computations at node entries. 3.1 critical \nEdges It is well-known that in completely arbitrary graph structures the code motion process may be blocked \nby critical edges, i.e. by edges leading from nodes with more than one successor to nodes with more than \none predecessor (cf. [Dh2, Dh3, DS, RWZ, SKR1, SKR2]). u) \\ b) \\ b / I 1 x:=a+b 2 3 y:= a+b @ Figure \n3: Critical Edges In Figure 3(a) the computation of a+ b at node 3 is partially redundant with respect \nto the computation of a + b ) at node 1. However, this partial redundancy cannot safely be eliminated \nby moving the computation of a + b to its preceding nodes, because this may introduce a new computation \non a path leaving node 2 on the right branch. On the other hand, it can safelv be elimina~ed after inserting \na synthetic node 4 in th~ critical edge (2, 3), as illustrated in Figure 3(b). We will therefore restrict \nour attention to programs having passed the following edge splitting transforma\u00adtion: every edge leading \nto a node with more than one predecessor has been split by inserting a synthetic node5. This simple transformation \ncert airily implies that all critical edges are eliminated. Moreover, it ;im\u00ad 5In order to keep the presentation \nof the motivating example simple, we omit synthetic nodes that are not relevant for the Lazy Code Motion \nTransformation. plifies the subsequent analysis, since it allows to obtain programs that are computationally \nand lifetime opti\u00admal (Optimalit y Theorem 4.9) by inserting all compu\u00adtations uniformly at node entries \n(cf. [SKR1, SKR2])6. 3.2 Guaranteeing Computational Opti\u00ad mality A placement is . safe, iff every computation \npoint n is an n-safe node, i.e.: a computation of t at n does not in\u00adtroduce a new value on a path through \nn. This property is necessary in order to guarantee that the program semantics is preserved by the placement \nprocess7. . earliest, iff every computation point n is an n\u00adearliest node, i.e. there is a path leading \nfrom s to n where no node m prior to n is n-safe and deli~ers the same value as n when computing t. \n A safe placement is . computaiionaihj optimal, iff the results of any other safe placement always require \nat least as many computations at run-time. In fact, safety and earliestness are already sufficient to \ncharacterize a comput ationally optimal placement: * A placement is computationally optimal if it is \nsafe and earliest, However, we consider the following stronger require\u00adment of safety which leads to \nan equivalent character\u00adization and allows simpler proofs: A placement is . down-safe, iff every computation \npoint n is an n\u00addown-safe node, i.e. a computation of t at n does not introduce a new value on a terminating \npath starting in n. Intuitively, this means that an initialization h:= t placed at the entry of node \nn is justified on ev\u00adery terminating path by an original computation occurring before any operand of \nt is modifieds. As safety induces earliestness, down-safety induces the notion of ds-earliest ness. \nThe following lemma states that it is unnecessary to distinguish between safety and down-safety in our \napplication, and that the notions of earliestness and ds-earliestness coincide. 6 Splitting critical \nedges only, would require a placement pro\u00adcedure whkh is capable of placing computations both at node \nentries and node exits. 7In ~artic~a, a safe placement does not change the Potential for run-time errors, \ne.g. division by O or overflow . 8 ~ [MR1] down-~&#38;ty is called anticipabihty, and the dual notion \nto down-safety, up-safety, is called availability. Lemma 3.1 A placement is 1. earliest if and only if \nit is ds-earliest 2. safe and earliest if and only if ii is down-safe and ds-earliest  Proof: ad 1): \nEarliestness implies ds-earliestness. Thus let us assume an n-ds-earliest computation point n # s. This \nrequires a path from s to n where no node m prior to n is n-down-safe and delivers the same value as \nin n when computing t,In particular, there is no original computation on this path before n that represents \nthe same value. Thus a node m prior to n on this path where a computation of t has the same value as \nin n cannot be n-up-safeg and consequently not n-safe eitherl . Hence node n is n-earliest. ad 2): Due \nto 1) it remains to show that a safe and ear\u00adliest placement is also down-safe. This follows directly \nfrom the fact that an n-earliest computation point n is not n-up-safe, which has been proved in 1), . \nLet n be a computation point of a down-safe and ear\u00adliest placement. Then the n-down-safety of n yields \nthat every terminating path p = (nl, ... n~) starting in n has a prefix q = (nl, . . ., nj) which satisfies: \na) Used(nj ) b) -Wsed(ni) for all i G {1,..., j 1} c) ~nsp(n~) for all i e {l, . ..jj 1} In the remainder \nof the paper the prefixes q are called safe-earliest jirst-use paths (SEFU-paths). They char\u00adacterize \nthe computation points of safe placements in the following way: Lemma 3.2 Let pi= be a safe and eariiest \nplacement, pls a safe placement and q=(nl, . . ..nj) a SEFU\u00ad path. Then we have: 1. p!= has no computation \nof t on (nz, ..., nj). 2. P1. has a computation of t on q.  Proof: In order to prove 1) let ni be \na computa\u00adtion point of pi~ with iG {2,..., j}. Then we are going to derive a contradiction to the earliestness \nof pl~. According to Lemma 3.1 we conclude that pl% is down-safe and, in particular, that ni is n-down-safe. \nMoreover, every predecessor m of ni is n-down-safe too: this is trivial in the case where ni has more \nthan one predecessor, because then they must all be syn\u00adthetic nodes. Otherwise, ni_ 1 is the only predecessor \nof n~. In this case its n-down-safety follows from the 9n-up-safety of a node is defined in analogy to \nn-down-safety. 10Note that ~ node is n-safe if and only if it is n-down-safeor n-up-safe. properties \na) c) of q and the n-down-safety of nl. Consequently, n~ is not n-ds-earliest and therefore due to \nLemma 3.1 not n-earliest either. 2) follows immediately from the n-earliestness of nl, and the safety \nof pl~. 1 As an easy consequence of Lemma 3.2 we obtain: Corollary 3.3 No computation point of a computa\u00adtionally \noptimal placement occurs outside of a SEFU\u00adpath. The central result of this section, however, is: Theorem \n3.4 A placement is computationa![y op-ti\u00admal if it is down-safe and eariiest. Proof: Applying Lemma 3.2, \nwhich is possible because of Lemma 3.1, we obtain that any safe placement has at least as many computations \nof t on every path p from s to e as the down-safe and earliest placement, In the following we will compute \nall program points that are n-down-safe and n-earliest.  3.2.1 Down-Safety The set of n-down-safe computation \npoints for t is characterized by the greatest solution of Equation Sys\u00adtem 3.5, which specifies a backward \nanalysis of G. Equation System 3.5 (Down-Safety) D-SAFE(n) = false ifn=e ( Used(n) V Tkansp(rt) A ~ D-SAFE(m) \notherwise [ ?7aCsucc(?a) Let D-Safe be the greatest solution of Equation Sys\u00adtem 3.5. Then we have (see \nFigure 4 for illustration): Lemma 3,6 (Down-Safe Computation Points) A node n is n-down-safe if and only \nif D-Safe(n) holds. Proof: only if : Let D denote the set of nodes that are n-down-safe and U the set \nof nodes satisfying Used. Then all successors of a node in D \\U are again in D. Thus a simple inductive \nargument shows that for all nodes n c D the predicate D-SAFE(n) remains con\u00adstantly true during the maximal \nfixed point iteration. if : This can be proved by a straightforward induction on the length of a terminating \npath starting in n. .  3.2.2 Earliestness Earliestness, which according to Lemma 3.1 is equiva\u00adlent \nto ds-earliestness, is characterized by the least so\u00adlution of Equation System 3.7, whose solution requires \na forward analysis of G. Equation System 3.7 (Earliestness) EARLIEST(n) = true ifn=s ~ (-llunsp(m) V \n?n@red(n) ~D-Safe(m) A EARLIEST(m) ) otherwise I Let Earliest denote the least solution of Equation \nSystem 3.7. Along the lines of Lemma 3.6 we can prove: Lemma 3.8 (Earliest Computation Points) A node \nn is n-earliest if and only if Earliest(n) holds. Figure 4 shows the predicate values of Earliest for \nour motivating example. It illustrates that Earliest is valid at the start node and additionally at those \nnodes that are reachable by a path where no node prior to n is n-down-safe and delivers the same value \nas n when computing t. Of course, computations cannot be placed earlier than in the start node, which \njus\u00adtifies Earliest(l) in Figure 4, Moreover, no node on the path (1, 4, 5, 7, 18) is n-down-safe. Thus \nEarli.est({2, 4,5,6,7, 18}) holds. Finally, evaluating t at node 1 and 2 delivers a different value as \nin node 3, which yields Earliest(3). D-Safe and Earliest induce the Safe-Earliest Trans\u00adformation: The \nSafe-Earliest Transformation e Introduce a new auxiliary variable h for t. . Insert at the entry of every \nnode n satisfying D-Safe and Earliest the assignment h:= t. . Replace every original computation of t \nin G by h. Whenever D-Safe(n) holds there is a node m on ev\u00adery path p from s to n satisfying D-Safe(m) \nand Earliest(m) such that no operand of t is modified between m and n. Thus all replacements of the Safe-Earliest \nTransformation are correct, which guarantees that the Safe-Earliest Transformation is a placement. Moreover, \naccording to Lemma 3.1, Lemma 3,6 and Lemma 3.8 the Safe-Earliest Transformation is down\u00adsafe and earliest. \nTogether with Theorem 3.4 this yields: 228 a 1 2 a.=c 1  Figure 4: The D-Safe and Earliest predicate \nvalues  Theorem3.9 (ComputationalOptimality) The Safe-Earliest Transformation is computationally optimal. \nFigure 5 shows the result of the Safe-Earliest Transfor\u00admation for the motivating example of Figure 1, \nwhich is essentially the same ae the one delivered by the al\u00adgorithm of Morel and Renvoise [MRl]ll. In \ngeneral, however, there may be more deviations, since their al\u00adgorithm inserts computations at the end \nof nodes, and it moves computations only, if they are partially avail\u00adable. Introducing this condition \ncan be considered as a first step in order to avoid unnecessary code motion. However, it limits the effect \nof unnecessary code mo\u00adtion only heuristically. For instance, in the example of Figure 5 the computations \nof node 10, 15, 16 and 17 would be moved to node 6, and therefore more than necessary. In particular, \nthe computation of node 1? cannot be moved with run-time gain at all. In the next section we are going \nto develop a pro\u00adcedure that completely avoids any unnecessary code mot ion. 11~ the ~x-ple of Figure \n1 the algorithm of [MR1] would not insert a computation at node 3. 11 6 h:= a+b 7 89 1( J 10/y:=h 11 \n12 13 i 14 15y:=h 16 z:=h 1 181 I I Figure 5: The Safe-Earliest Transformation 4 Suppressing Unrwcessary \n  Code Motion In order to avoid unnecessary code motion, computa\u00adtions must be placed as late as possible \nwhile main\u00adtaining computational optimality. We therefore define: A computationally optimal placement \npl is . latest, iff every computation point n is an n-latest node, i.e.: n is a computation point of \nsome computa\u00adtionally optimal placement and on every terminating path p starting in n any following computation \npoint of a compu\u00adtationally optimal placement occurs after an original computation on p. Intuitively, \nthis means no other computationally optimal placement can have later computation points on any path. \ne isolated, iff there is a computation point n that is an n-isolated node with respect to pl, i.e. on \nevery terminating path starting in a successor of n every original computation is preceded by a computation \nof pl. Essentially, this means that an initialization h:= t placed at the entry of n reaches a second \noriginal computation at most after passing a new initial\u00adizationof h. e lifetime optimal (or economic), \niff any other com\u00ad putationally optimal placement always requires at least as long lifetimes of the auxiliary \nvariables (registers). We have: Theorem 4.1 A computationally optimal placement is lifetime optimal if \nand only if it is latest and not isolated. F roofi We only prove the if -direction here, because the \nother implication is irrelevant for establishing our main result Theorem 4.9. The proof proceeds by contraposition, \nshowing that a computationally optimal but not lifetime optimal placement plCO is not latest or isolated. \nThis requires the consideration of a lifetime optimal placement plitO for comparison. The fact that pico \nis not lifetime optimal implies the existence of a SEFU-path q = (nl, ... nj) such that plCOhas an initialization \nin a node n. which precedes a computation of plltO inanode nlwith c~1.Now we have to distinguish two \ncases: Case 1: c <1. Applying property b) of q, we obtain that the path (n., ...nl-1) is free of original \ncomputa\u00adtions. Thus nc is not n-latest and consequently PICO not latest. Case 2: c =1. Obviously, this \nimplies c= 1= j, and that the computation of p/~tO in nl is an original one. Thus it remains to show \nthat ni is n-isolated with respect to plco, i.e. that on every terminating path starting in a successor \nn of nj the first original com\u00adputation is preceded by a computation of plco. We lead the assumption \nthat nj is not n-isolated, i.e. that there exists a path p = (n, . . . . m) without computations of plco \nbut with an original computa\u00adtion at m, to a contradiction. Under this assumption Lemma 3.2(2) delivers \nthat p is also free of computa\u00adtions of the Safe-Earliest Transformation, which yields that every node \nof p appears outside a SEFU-path. Thus, according to Corollary 3.3, p is also free of com\u00adputations of \npllte, which implies that p/ltO does not initialize h on either q or p. Moreover, because of the n-earliestness \nof nl, it is also impossible that h is initialized on every path from s to nl after the last modification \nof an operand of t.This contradicts the placement property of pllto, and therefore implies that nj is \nn-isolated with respect to plco as desired. 1 Intuitively, lifetime optimal placements can be ob\u00adtained \nby successively moving the computations from their earliest safe computation points in direction of the \ncontrol flow to later points aa long as computational optimality is preserved, This gives rise to the \nfollowing definition: A placement is . delayed, iff every computation point n is an n\u00addelayed node, i.e. \non every path from s to n there is a computation point of the Safe-Earliest Trans\u00adformation such that \nall subsequent original com\u00adputations of p lie in n. Technically, the computation of n-delayed computation \npoints is realized by determining the greatest solution of Equation System 4.2, which requires a forward \nanal\u00adysis of G. Equation System 4.2 (Delay) DELAY(n) = D-Safe(n) A Earliest(n) V false if 7?,=s ~ yUsed(m) \nA DELAY(m) otherwise { mcp! cd(n) Let Delay be the greatest solution of Equation System 4.2. Analogously \nto Lemma 3.6 we can prove: Lemma 4.3 A node n is n-delayed if and only if Delay(n) holds. Furthermore \nwe have: Lemma 4.4 1. A computationally optimal placement is delayed. .$2.A delayed placement is down-safe. \nProof: ad 1): We have to show that every computa\u00adtion point n of a computationally optimal placement \nis n-delayed. This can be deduced from Corollary 3.3 and property b) of SEFU-paths, which deliver that \nev\u00adery path froms to n goes through a computation point of the Safe-Earliest Transformation such that \nall sub\u00ad sequent original computations of p lie in n. ad 2): According to Lemma 3.6 and Lemma 4.3 it \nis enough to show that Delay(n) implies D-Safe(n) for every node n. This can be done by a straightforward \ninduction on the length of a shortest path from s to n. Based on the predicate Delay we define Latest \nby: Latest(n)=~~ Delay(n) A (Used(n) V -I Delay(m)) II mc$ticc(n) The predicate values of Delay and Latest \nare illus\u00adtrated for our motivating example in Figure 6. We have:  1 2 a:=c .....+  ~@:@;4y ~ ,.:.:.:. \n,.,. . :.,.:.,.,.,. Q 5 Figure 6: The Delay and Latest predicate values Lemma 4.5 A node n is n-latest \nif and only if Latest(n) holds. Proof: We will concentrate here on the part of the proof being relevant \nfor the proof of Theorem 4.9, which is the if -direction for nodes that occur as com\u00adputation points \nof some computationally optimal place\u00adment. This is much simpler than the proof for the gen\u00aderal case. \nThus let us assume a computation point n of a computationally optimal placement satisfying Latest. Then \nit is sufficient to show that on every terminating path p starting in n any following computation point \nof a computationally optimal placement occurs after an original computation on p. Obviously, this is \nthe case if n itself contains an original computation. Thus we can reduce our atten\u00adtion to the other \ncase, in which =Delay(m) must hold for some successor m of n. This directIy yields that n is a synthetic \nnode with m being its only successor, because m must have several predecessors. Now assume a terminating \npath p starting in m, and a node 1 on p that occurs not later than the first node with an original computation. \nThen it remains to show that i is not a computation point of a computationally optimal placement. Lemma \n3.2(1) implies that there does not exist any computation point of the Safe-Earliest Transformation prior \nto 1 on p, because n is n-delayed due to Lemma 4.3. Moreover Lemma 4.3 yields that m is not n\u00addelayed. \nThus 1 cannot be n-delayed either. Accord\u00ading to Lemma 4.4(1) this directly implies that i is not a computation \npoint of any computationally optimal placement, . As D-Safe and Earliest also Latest specifies a pro\u00ad \ngram transformation. We have: Lemma 4.6 The Latest Transformation is a compu\u00adtationaliy optimal placement. \nProof: Whereas the safety property holds due to Lemma 4.3 and Lemma 4.4, the proof that the Latest Transformation \nis a placement and that it is computa\u00adtionally optimal can be done simultaneously by show\u00ading that (1) \nevery SEFU-path cent ains a node satisfy\u00ading Latest and that (2) nodes satisfying Latest do not occur \noutside SEFU-paths. This is straightforward from the definition of Delay. . Figure 7 shows the result \nof the Latest Transformation for the motivating example. 1 2 a:=c 3 h:=a+b ~ x:=h @ 5 6 7 1 I 8 h := \na+b 9 i  10 y:=h 11 12 i 15;:= ;+b14  . = 16 z:=h Figure 7: The Latest Transformation The Latest \nTransformation is already more economic than any other algorithm proposed in the literature. However, \nit still contains unnecessary initializations of h in node 3 and 17. In order to avoid such unneces\u00adsary \ninitializations, we must identify all program points where an inserted computation would only be used \nin theinsertion node itself. This isachieved bydetermin\u00ading the greatest solution of Equation System \n4.7, which specifies a backward analysis of G. Equation System 4.7 (Isolation) ISOLATED(n) = ~ (Latest(m) \nV mesucc(n) =Used(m) A ISOLATED(m)) Let Isolated(n) be the greatest solution of Equation System 4.7. \nThen we can prove in analogy to Lemma 3.6; Lemma 4.8 A node n is n-isolated with respect to the Latest \nTransformation if and only if Isolated(n) holds. Figure 8 shows the predicate values of Isolated for \nthe running example. 4.1 Tile Optimal Program Transforma\u00adtion The set of optimal computation points for \nt in G is given by the set of nodes that are latest, but not iso\u00ad lated OCP =.~ { nI Latest(n) A yIsolated(n) \n} and the set of nodes that contain a redundant occur\u00adrence of t with respect to O CP is specified by \nRO =df { n I Used(n) A =( Latest(n) A Isolated(n))} Note that the Latest and with respect corresponding \noccurrences of t in nodes satisfying the Isolated predicate are not redundant to O CP, since the initialization \nof the auxiliary variable is suppressed. The Transformation O CP and RO induce the Lazy Code Motion Trans\u00adformation. \nThe Lazy Code Motion Transformation . Introduce a new auxiliary variable h for t. o Insert at the entry \nof every node in O CP the assignment h:= t. e Replace every original computation of t in nodes of RO \nby h. Our main result states that this transformation is op\u00adtimal: 8  10Y:=a+b 11 &#38;  ~ Theorem \n4.9 (Optimality Theorem) :~y :;,: ,.: :.,.,.,  The Lazy Code Motion Transformation is computation\u00ad,.,,..,,,, \naity and lifetime optimal, 12 ~;%; 13i i : . ,: j. j ,,. ,,,,,,,,,,. ,,,,,.,.,  b1!,+ $ Proof: Following \nthe lines of Lemma 4.6 we can prove that the Lazy Code Motion Transformation is a com\u00ad putationally optimal \nplacement. According to Lemma 4.5 and Lemma 4.8 it is also latest and not isolated (Lemma 4.8 can be \napplied since the Lazy Code Motion Transformation has the same computation points as the Latest Transformation.). \nThus Theorem 4.1 com\u00adpletes the proof. 1 The application of the Lazy Code Motion Transfor-Figure 8: The \nIsolated and Latest predicate values mation to the flow graph of Figure 1 results in the promised flow \ngraph of Figure 2. Conclusions We have presented a bit-vector algorithm for the tom\u00admutationally and \nlifetime optimal placement of com\u00adputations within flow graphs, which is as eficient as standard uni-directional \nanalyses. Important feature of this algorithm is its laziness: computations are placed as early as necessary \nbut as late as possible. This guar\u00adantees the lifetime optimality while preserving compu\u00adtational optimality. \nFundamental was the decomposition of the typi\u00adcally hi-directionally specified code motion procedures \ninto uni-directional components. Besides yielding clar\u00adity and reducing the number of predicates drasti\u00adcally, \nthis allows us to utilize the efficient algorithms for uni-directional bit-vector analyses. Moreover, \nit makes the algorithm modular, which supports future extensions: in [KRS] we present an extension of \nour lazy code motion algorithm which, in a similar fash\u00adion as in [JD 1, JD2], uniformly combines code \nmo\u00adtion and strength reduction, and following the lines of [KS1, KS2] a generalization to programs with \nproce\u00addures, local variables and formal parameters is straight\u00adforward. We are also investigating an \nadaption of the as early as necessary but as late as possible placing strategy to the semantically based \ncode motion algo\u00adrithms of [SKR1, SKR2]. References [AU] Aho, A, V,, and Unman, J. D. Node listings for \nreducible flow graphs. In Proceedings ?h STOC, 1975, 177-185. [Ch] Chow, F. A portable machine indepen\u00addent \noptimizer Design and measurements. Ph.D. dissertation, Dept. of Electrical Engi\u00adneering, Stanford University, \nStanford, Calif., and Tech. Rep. 83-254, Computer Systems Lab., Stanford University, 1983. [Dhl] Dhamdhere, \nD, M. Characterization of pro\u00adgram loops in code optimization. Comp. Lang. 8, 2 (1983), 69-76. [Dh2] \nDhamdhere, D. M. A fast algorithm for code movement optimization. SIGPLAN Not. 23, 10 (1988), 172-180. \n[Dh3] Dhamdhere, D. M. Practical adaptation of the global optimization algorithm of Morel and Renvoise. \nACM Trans. Program. Lang. Syst. 13, 2 (1991), 291-294. [DS] Drechsler, K. H., and Stadel, M. P. A solu\u00adtion \nto a problem with Morel and Renvoise s Global optimization by suppression of partial redundancies , ACM \nTrans. Syst. 10, 4 (1988), 635-640. Program. Lang. [GW] Graham, S. L., and Wegman, M. A fast and usually \nlinear algorithm for global flow anal\u00adysis. Journa/ of the ACM 29, 1 (1976), 172\u00ad202. [He] Hecht, M. \nS. Flow analysis of computer grams. Elsevier, North-Holland, 1977. pro\u00ad [HU1] Hecht, M. S., and Unman, \nJ. D. Analysis of a simple algorithm for global flow problems. In Proceedings Isi POPL, Boston, Massachusetts, \n1973, 207-217. [HU2] Hecht, M. S., and Unman, J. D. A simple al\u00adgorithm for global data flow analysis \nproblems. In SIAM J. Comput. 4,4 (1977), 519-532. [JD1] Joshi, S. M., and Dhamdhere, D. M. A com\u00adposite \nhoisting-strength reduction transforma\u00adtion for global program optimization part 1. Internat. J. Computer \nMath. 11, (1982), 21\u00ad41. [JD2] Joshi, S. M., and Dhamdhere, D. M. A com\u00adposite hoisting-strength reduction \ntransforma\u00adtion for global program optimization part II. Internat. J. Computer Math. 11, (1982), 111\u00ad126. \n[Kc] Kennedy, K. Node listings applied to flow analysis. In Proceedings Fd POPL, Alto, California, 1975, \n10-21. data Palo [KRS] Knoop, J., Riithing, O., and Steffen, B. strength reduction. To appear. Lazy [KS1] \nKnoop, J., and Steffen, B. The interprocedu\u00adral coincidence theorem. Aachener Informatik-Berichte Nr. \n9127, Rheinisch-Westfalische Technische Hochschule Aachen, Aachen, 1991. [KS2] Knoop, J., and Steffen, \nB. Efficient and opti\u00admal interprocedural bit-vector data flow analy\u00adses: A uniform interprocedural framework. \nTo appear. [KU1] Kam, J. B., and Unman, J. D. Global data flow analysis and iterative algorithms. Journal \nof the ACM 23, 1 (1976), 158-171. [KU2] Kam, J. B., and Unman, J. D. flow analysis frameworks. Acts (1977), \n309-317. Monotone data Informatica 7, [Me] Morel, E. Data flow analysis and optimization. In: Lorho, \nB. (Ed.). ods and tools for compiler construction, bridge University Press, 1984. global Meth-Cam\u00ad  \n[MR1] Morel, E., and Renvoise, C. Global opti\u00admization by suppression c)f partial redundan\u00adcies. Comrnun. \nof the ACM 22, 2 (1979), 96\u00ad 103. [MR2] Morel, E., and Renvoise, C. Interprocedural elimination of partial \nredundancies. In: Much\u00adnick, St. S., and Jones, N. ID. (Eds.). Program flow analysis: Theory and applications. \nPren\u00adtice Hall, Englewood Cliffs, NJ, 1981. [RWZ] Rosen, B. K., Wegman, M. N., and Zadeck,F. K. Global \nvalue numbers and redundant computations. In Proceedings It?h POP.L, San Diego, California, 1988, 12-27. \n[s0] Sorkin, A. Some comments on A solution to a problem with Morel and Renvoise s Global optimization \nby suppression of partial redun\u00addancies . ACM Trans. Program. Lang. Syst. 11, 4 (1989), 666-668. [St] \nSteffen, B. Data flow analysis as model check\u00ading. In Proceedings TA C S 91, Sendai, Japan, Springer-Verlag, \nLNCS 526 (1991), 346-364. [SKR1] Steffen, B., Knoop, J., ancl Ruthing, O. The value flow graph: A program \nrepresentation for optimal program transformations. In Pro\u00adceedings $d ESOP, Copenhagen, Denmark, Springer-Verlag, \nLNCS 432 (1990), 389-405. [SKR2] Steffen, B., Knoop, J., and Riithing, O. Efficient code motion and \nan adapti;n to strength reduction. In Proceedings 4~h TAP. SOFT, Brighton, United IKingdom, Springer-Verlag, \nLNCS 494 (1991), 394-415. [Tal] Tarjan, R. E. Applications of path compres\u00adsion on balanced trees. Journal \nof the ACM 26, 4 (1979), 690-715. [Ta2] Tarjan, R, E. A unifiecl approach to path problems. Journal of \nthe ACM 28, 3 (1981), 577-593. [Ta3] Tarjan, R. E. Fast algorithms for solving path problems. Journal \nof the .ACM 28, 3 (1981), 594-614. [Ull] Unman, J. D. Fast algorithms for the elimi\u00adnation of common \nsubexpressions. Act a Infor\u00admaiica 2, 3 (1973), 191-213.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>We present a bit-vector algorithm for the <italic>optimal</italic> and <italic>economical</italic> placement of computations within flow graphs, which is as <italic>efficient</italic> as standard uni-directional analyses. The point of our algorithm is the <italic>decomposition</italic> of the bi-directional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further uni-directional analysis components exclude any unnecessary code motion. This <italic>laziness</italic> of our algorithm minimizes the register pressure, which has drastic effects on the run-time behaviour of the optimized programs in practice, where an economical use of registers is essential.</p>", "authors": [{"name": "Jens Knoop", "author_profile_id": "81100197024", "affiliation": "", "person_id": "PP40024734", "email_address": "", "orcid_id": ""}, {"name": "Oliver R&#252;thing", "author_profile_id": "81100258003", "affiliation": "", "person_id": "PP31094851", "email_address": "", "orcid_id": ""}, {"name": "Bernhard Steffen", "author_profile_id": "81100210663", "affiliation": "", "person_id": "PP40024876", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143136", "year": "1992", "article_id": "143136", "conference": "PLDI", "title": "Lazy code motion", "url": "http://dl.acm.org/citation.cfm?id=143136"}