{"article_publication_date": "07-01-1992", "fulltext": "\n Compiling Dataflow Analysis of Logic Programs Jichang Tan I-Peng Lin Department of Computer Science \nand Information Engineering National Taiwan University Taipei, 10764 Taiwan jctan@csman.csie. ntu.edu.tw \nAbstract Abstract interpretation is a technique extensively used for global dataflow analyses of logic \nprograms. Existing implementations of abstract interpretation are slow due to interpretive or transforming \noverhead and the inefR\u00adciency in manipulation of global information. Since ab\u00adstract interpretation mimics \nstandard interpretation, it is a promising alternative to compile abstract interpre\u00adtation into the framework \nof the WAM (Warren Ab\u00adstract Machine) for better performance. In this paper, we show how this approach \ncan be effectively imple\u00admented in a low-cost manner. To evaluate the possible benefits of this approach, \na prototype dataflow analyzer has been implemented for inference of mode, type and variable aliasing \ninformation of logic programs. For a subset of benchmark programs in [15], it significantly improves \nthe performance by a factor of over 150 on the average. Introduction Like many conventional languages, \nthe performance of the logic programming language Prolog has been sig\u00adnificantly improved through compilation \n[21]. In par\u00adticular, the WAM (Warren Abstract Machine) [22] is a virtual machine that has emerged as \nthe de facto standard for the compilation and implementation of Prolog. The benefits of the WAM basically \nrest on local optimizations through a simple intra-procedural (clause-level) program analysis. However, \nsubstantial optimizations [12, 13, 15, 18, 23] all depend on inter\u00adprocedural information such as mode \n(the input and output direction of procedure arguments), type (the possible bindings of data-structures \nfor program vari- Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific \npermission. ACM SIGPLAN 92 PLD1-6/92/CA  @ 1992 ACM Q.8979J.476-7/921QQQ6 /Q~Q6... $J .5Q ables), and \nva~iable aliusing (the co-references of logical variables). The dataflow information is not only impor\u00adtant \nto enable more performance improvement of Pro\u00adlog, it also paves the way for efficient implementation \nof different classes of logic programs wtilch support Inde\u00adpendent And-Parallelism [23, 13], concurrent \nprocesses [2], or constraint satisfaction [10]. Instead of inferring various information about a pro\u00adgram \nthrough different analysis procedures, a technique called abstract interpretation can be used for global \ndataflow analyses of a programming language based on a unified model [4]. The idea is to execute a program \nto be analyzed over a finite abstract domain rather than the possibly infinite concrete domain over which \nthe language is defined. This includes the definition of a mapping from elements and operators of the \nstandard (i.e., concrete) domain to those of the abstract domain. Abstract interpretation is then carried \nout by a fix\u00adpoint execution of the abstracted program. The result of the execution gives information \nabout the original program over the specified abstract domain. For ab\u00adstract interpretation of logic \nprograms, there are two major issues. First, the unification procedure and vari\u00adable substitutions have \nto be redefined over the ab\u00adstract domain. For soundness of the dataflow analysis, there are certain \ncriteria to be met in their definitions. Second, termination and completeness of the control scheme must \nbe ensured. This means a different inter\u00adpretation strategy is required instead of the top-down depth-first \nmethod used by ordinary Prolog systems. Due to general interests and applications, both issues have been \nextensively studied [1, 2, 3], [5]-[16], [20]. To the best of our knowledge, all global dataflow an~ \nlyzers for logic programs have been implemented on top of Prolog. The implementation can be based on \na meta\u00adcircular interpreter [6, 17] or a program transformer [5, 23]. The mer!a-interp~eiing approach \ndirectly inter\u00adprets programs by using the redefined control scheme and unification procedure over an \nabstract domain. Instead of direct interpretation, the transforming ap\u00ad Query Answering   EzEl - \nProlog WAM WAM / Compiled Execution Program Compiler  Code n Dataflow Information  m - CompiledDataflow \nAnalysis Figure 1: Compiled Execution and Compiled Dataflow Analysis preach first partially evaluates \nthe programs over the abstract domain, and then runs transformed programs to do the abstract interpretation. \nFor efficiency, a dif\u00adferent control scheme can be incorporated [11, 12, 15]. However, both approaches \nare slow due to the inter\u00adpretive or transforming overhead and the inefficiency in manipulation of global \ndatatlow information. Since abstract interpretation mimics standard interpretation, it should be a promising \nalternative to compile abstract interpretation into a similar framework like the WAM for better performance. \nRather than reinventing a new virtual machine and a dedicated compiler for it, it is much cheaper to \nTewe the framework of the WAM and current compilers to the maximal extent. This idea turns out to be \nquite successful because there is almost no design in the WAM which is restricted to execution over the \ncon\u00adcrete domain. For a typical abstract domain such as the one considered in this paper, the instruction \nset of the WAM can be easily reinterpreted according to the redefined control scheme and unification \nprocedure. The outline of this approach is illustrated in Figure 1. We will use the term abst? act WAMfor \nthe rest of thk paper to refer to this reinterpretation of the WAM for dataflow analysis, and the term \nconc7ete or standa7d WAMfor the original framework of Prolog compilation. To evaluate the possible benefits \nof this approach, we have implemented a prototype datafiow analyzer for the inference of mode, type and \nvariable aliasing infc~r\u00admation of logic programs. For a subset of benchmark programs in [15], it significantly \nimproves the perfcm\u00admance by a factor of over 150 on the average. The most obvious source of performance \nimprove\u00adments is the removal of the overhead of interpretation and transformation incurred in other approaches. \nAs a comparison, the technique of compilation improved the performance of Prolog by a factor of about \n30 in War\u00adren s original work [21]. Of course, the WAM code does 107 not come for free. However, it \nappears that a transla\u00adtion to unification primitives like those of the WAM before dataflow analyses \nis indispensable for other ap\u00adproaches as well [15, 17]. Another important source of performance improvements \nis the efficiency in manipu\u00adlation of global dataflow information. The handling of global information \nis required in abstract interpretation while it is discouraged by existing implementations of Prolog. \nTherefore, concerns for its efficiency of imple\u00admentation have been explicitly addressed in the work \nof [23, 15, 17]. For our approach, it does not pose any problem because we use a conventional language \nlike C for the implementation. 2 Preliminaries 2.1 Warren Abstract Machine The basic idea of optimization \nis to generate special\u00adized code for different caaes of source programs. This works for Prolog as well \nas other conventional lan\u00adguages. Consider executing the head of a clause with two arguments in a Prolog \nprogram:  P(% [W)lq) t ,.. If it is directly carried out by an interpreter, a gened unification procedure \nwill be used for both arguments a and [f(V) [L] when this predicate is invoked. Since the interpreter \ncannot take advantage of knowledge of the source program, the information has to be gener\u00adated at run-time. \nDue to the abstract semantics of logic programs, this overhead can be considerably ex\u00adpensive. If the \nhead is compiled into the framework of the WAM, a sequence of unification primitives will be generated \nas the one given in Figure 2. We can see that there are specialized unification subroutines, i.e., the \nWAM instructions, defined for each type of term. For example, the instructions get .const, get-list, \nand get -struct are defined for constants, lists, and struc\u00adtures that appear in the head of a clause. \nFor a complex get.const a,Al % The first coming-in argument must unify with a . getJ.ist A2 % The second \nargument has to be a list. unify.var X3 % Keep the first subterm (car) of the list using a temporary \nregister. unify.var L % Unify its next subterm (cdr) with L. get-truct ~/1,X3 % The car of the list must \nbe an f/1 term. unifymxr V % And its subterm unifies with V. Figure 2: The WAM code instructions for \nthe head of the clause term such as [f(V) [L], the generated instructions will proceed in a breadth-jirsi \norder. For example, the two unif y_var instructions after the get-list instruction correspond to the \nunification of the first level subterms, and the third rmify-var instruction which is after the get struct \ninstruction corresponds to the unification of the second level subterm. Through this sequence of primitive \nunification subroutines, much of the overhead in the general unification procedure is eliminated. Based \non similar ideas, the instruction set of the WAM can be defined. According to Warren [22], they are classified \ninto get, ptd, mzify, procedural, and index\u00ading instructions. The put instructions are specialized get \ninstructions which are dedicated to term construc\u00adtion in the body of a clause. The procedural instruc\u00adtions \nare responsible for the control transfer and envi\u00adronment allocation associated with procedure calling. \nThe indezing instructions, the last group of instruc\u00adtions, control the access to different clauses of \na predi\u00adcate procedure. For other relevant details of the WAM, appropriate descriptions will be given \nin the following sections. 2.2 The Control Scheme for Abstract Interpretation Due to the requirement \nof compile-time completeness and termination, researchers have proposed several control schemes for abstract \ninterpretation of logic pro\u00adgrams [1, 3, 8, 9, 16, 20]. In thk paper, we only con\u00adsider the extension \ntable-based method [8] (a variant of OLDT Resolution [16]) because it is simple and can be easily incorporated \ninto the compilation approach presented. The basic idea of the extension table-based method is to employ \na dynamic programming technique to pro\u00adgram interpretation in which intermediate results are saved and \nlater reused to avoid redundant work and to prevent the execution from diving into an infinite loop. \nThe table is simply a memo structure contain\u00ading a set of pairs of tcalling pattern) (the description \nof the call) and its %uccess patternsn (the description of the successful return) which have been found \nso far. Instead of repeated computation, the extension table is consulted first whenever we try to interpret \na call\u00ading pattern. If any such solutions are found, they are returned directly; if the call has been \nmade earlier but no solution has been recorded, this call is considered failed. To ensure completeness, \nthe interpretation has to execute in multiple iterations such that another iter\u00adation will be initiated \nwhenever a new success pattern is found in one iteration. When the interpretation ter\u00adminates, the table \nwill provide approximate run-time information for each predicate in the program to be analyzed. The outline \nof an extension table-based abstract in\u00adterpretation can be described as follows. The inter\u00adpretation \nstarts from a given top-level calling pattern. Globally, it executes in the standard top-down depth\u00adfirst \norder. When a clause is successfully executed, the extension table is updated with the success pattern. \nHowever, the control is then transferred to the next clause of the predicate procedure by an artificial \nfail\u00adure such that all clauses will be explored. If there are multiple success patterns found for a calling \npattern, they may be summari~ed into a single pattern by tak\u00ading the least upper bound based on the abstract \ndo\u00admain used. By keeping at most one success pattern for a calling pattern, it also simplifies the control \nflow of the interpretation since any invocation of a predi cate always results in a deterministic return. \nThrough iterative deepening, the interpretation will eventually terminate for a finite abstract domain \nwhen least jix\u00adpoint of the dataflow analysis is reached. There are a few analyzers [5, 17, 23] which \nare baaed on this control scheme. However, the extension table is expensive to implement in Prolog because \nit is an inherently global data structure.  3 An Abstract Domain for Global Dataflow Analysis To infer \nrun-time information about a logic program, we classify all run-time terms into interesting subsets that \nform a complete lattice ordered by set inclusion. Each subset is represented by an abstract type in the \nabstract domain. An instance of an abstract type is re\u00adferred to as an abst?wct temn of that type. The \nabstract domain is briefly defined aa follows. . any represents the set of all terms, the top element \n(T) of the abstract domain. . nv represents all non-variable terms. . ground (or g, for short) represents \nthe set of ground terms. . const represents the set of constant terms. It is the union of two subsets \nwhkh are represented by atom and integer. . struct(~/rz, ~1, ... % ) represents the set of struc\u00adture \nterms which principal functor is j/n and each argument is oft ype a~ (1 ~ i ~ n) in the abstract domain. \nFor sake of clarity, the eet of terms will be written as j (al, ... aJ when there is no ambiguity. . \na-list represents the set of lists with a type param\u00adeter a. It is a precise type for the union of con\u00adstant \n D (the empty list) and [ala-list] (the struc\u00adture terms struct( . /2,cx, cv-list)) while [.l.] in gen\u00aderal \nremains to be an ordinary structure term. For example, glist stands for the set of terms 0, [g], [g, \ng], and so forth. Due to the extensive application of lists in logic programs, list-awareness is usually \nvery useful. . var represents the set of all variables. . empty represents the set of non-existing \nterm, the bottom element (J-) of the abstract domain.  To make the abstract domain finite, we impose \na term\u00addepth Testtiction for complex abstract terms in the analysis. Given an artificial limit k, the \nsubterms with depth h or greater in a complex term will be simpli\u00adfied to other simple elements in the \nabstract domain. Consequently the precision of the analysis for general recureive datatypes is traded \nfor a guarantee of analysis termination. The abstract domain presented here is actually a slightly simplified \nversion of the one used by Taylor s analyzer [17]. The dataflow analysis based on this ab\u00adstract domain \nimplements an inference of mode, type, and variable aliasing information in logic programs. This domain \nis considerably more complex than the one used by the Aquarius analyzer [15] whose run-time performance \nwill be compared to that of our analyzer in Section 6. 4 Compiling Abstract Unifications 4.1 Representation \nand Manipulation of Abstract Terms Sinceeachabstracttermrepresentsa set of termsin theconcretedomain,abstract \nunifications can be char\u00adacterized by set unification as follows. Let TX and T2 be two sets of terms. \nSet unification of T1 and T2 can be defined as: s.unify(!fl, T2) = {tIt= uni~(i!l,tz) : t1ET1At2ET2] \n AbstTact substitution can also be formalized as a set of concrete substitutions: {X/T3 = {X/t Itc T} \n where X is a variable occurred in T1 or T2, and T isa set of terms. More theoretical details can be \nfound in [1, 6]. Here are some examples for the abstract domain considered: . s.unify(any,ground) = ground. \n . s_unify(any, ~(X, Y)) = ~(any, any) with an abstract substitution {X/any, Y/any}. . s.unify(glist,[l?feadITaiq) \n= [glglist] with an abstract substitution {Head/g, Tail/glist}.  By these examples, we can see that \nthe instances of most abstract types, such as any or list, are similar to logical variables in that they \nbecome more specijk through unifications. It is therefore natural to repre\u00adsent these abstract terms \nlike variables such that each of them is encoded in a single word before unification, and can be instantiated \nto another type of term or a complex structure later. They will also be subscripted hereafter to stand \nfor different instances. Finally, the definition of abstract substitution should be extended to include \nsubstitutions for these abstract terms in ad\u00addition to that for variables. Instances of atom, integer \nand specific structure terms such as ~(atom), however, remain unchanged through unification because there \nis no smaller (i.e., more specific) element other than L in the abstract domain considered. Based on \nthe definitions, consider the abstract inter\u00adpretation of the head of the example clause in Section 2: \n  P(% vow])+ ... 109 % call p(atom, glistl); get.const a, Al 96 { (1) Succeeds for a N atom. 1 getlist \nA2 % glistl +--[gllglistz]; { (2.1) Get a [.1.]instance of glist. } unify.var X3 % x3 + gl; { The car \nof glktl } unify-var L % L + glistz; { The cdr of glktl } get-struct ~/1,X3 % X31 -f(g2); { (2.2) Get \nan ~(.) instance of g. } unify mar V %v+g2; { The argument of ~/1 } Figure 3: The WAM Code Reinterpreted \n over the abstract domain given a calling pattern p(atom, glistl). In breadth-first order, the abstract \nuni\u00adfication for this particular head can be decomposed into a sequence of three simple s-unify s: (1) \ns.unify(atom,atom) = atom with an empty substitution { }. (2.1) s-unify (glistl, ~[L]) = [g~lglistz] \nwith a substitution {glistl/[gl lglist2], L/glist2}. (2.2) s.unify(gl, f(V))= f(gz) with a substitution \n{gl/~(g2), V/g2}. Composing the substitutions, the head of the clause succeeds with an abstract substitution: \n{glistl/[~(gz)lglistQ], L/glistQ, V/gQ}\u00ad 4.2 Reinterpretation of the Instruction Set To continue the \nrefinement, the three s.unify s can be mapped into the sequence of WAM code instructions described in \nSection 2 as given in Figure 3. By this example, it is not difficult to define an appropriate reinterpretation \nof these WAM instructions over the abstract domain. For example, the algorithm to rein\u00adterpret get ~ist \nis outlined in Figure 4 where the fol\u00adlowing primitive operations about abstract terms are required. \n. Primary Approximation. Function Abs~pe(T) is defined to approximate a term l , either con\u00adcrete or \nabstract, to an abstract type regardless of its subterms. For example, A bsType(a) = atom, AbsType(glkt) \n= glkt, Abs~pe(f(V)) = ~(T) or simply denoted by struct(~/1). e Approximate Unijiabdity. Ignoring the \nsubterms, two terms T1 and T2 are unifiable, denoted by T1 m T2, if s.unify(Abs!@pe(T1), Absfipe(Tz)) \n# 0. For example, a w atom, glist N [.1.], and g R ~(.) have been used for the instructions get .const, \nget~ist, and getstruct ~/1, respectively, in abstract interpretation of the call p(atom, glistl). . Complex-Term \nInstantiation (ComplexTermInst). If an abstract term is approximately unifiable with a complex term, \nthe generation of the unification result will be necessary for subsequent subterm unifications. For example, \nthe term [gl lglk.t2] and ~(gQ) are complex-term instantiations of glistl and gz, generated by the instructions \nget list and get .-struct f/1, respectively. If we represent each run-time object by a tag and a value \nencoded in a computer word as in the standard WAM [22], the primary approximation function Ab\u00adsType simply \nreturns the tag of a simple object. For predefine abstract types (e.g. any or g), tabulation is sufficient \nto implement approximate unifiability and complex-term instantiation. For inferred data-types, such as \nglist, a type-description database will be neces\u00adsary. The instructions get-struct and get .const shown \nin the example can be reinterpreted in a similar way, while the interpretation of instruction unif y.var \nover the concrete domain can be completely reused over this abstract domain. 5 Compiling the Control \nScheme Consider a simple predicate procedure p and its WAM code as follows. p(x) + q, 7 (x). % clause \np.1 p(a). 96 clause p.2 p.1: get wgs o~p(X) call q put a~gs of r(X) execute r(X) getll,ist Ai s begin \nderef(~); { Sameas standardWAM interpretation} if concrete(~) then { The for atom, integer, struct, list, \nand var } concrete WAM code definition elsif~ w struct(~. /2) then { True for ground, nv, any and list} \nAi~+ tag(list,lif); Trail(Ai); { Generate a [,1.]instance of Ai on Heap 3 S i-ComplexTermInst(Ai, II); \n{ Set S pointer to the con-cell for subsequent} H i-H +2; Mode +-Read; { subterm unifications and proceed \nin read mode} else fail fi  end Figure 4: The Outline of th~eReinterpreted get-list Instruction p.2: \ngei! wgs ofp(a) proceed  The instruction proceed corresponds to a success\u00adful return of a clause, instruction \ncall corresponds to a predicate procedure invocation and instruction execute is equivalent to a call \nimmediately followed by a proceed (the last-call optimization). If we ap\u00adply the extension table-based \ninterpretation scheme de\u00adscribed in Section 2 to the source program, the result is a set of transformed \nprograms: p (X) + abtract(X, X@), if ezplored(p(X~))then 2ookupET(p(X.,)) else fzssert(ezplored(p(Xa \n))), p(X=) fio p(X) t q , r (X), tipo!atel?!Z@(X)), fail. p(a) + updatel?~p(atom)), fail. p(lhb) + lookupllqp(lhb)). \nwhere the predicate explored is true if a calling pat\u00adtern P has been explored in current iteration. \nThe predicate ZoolmpET(P) and updateET(P) are used to look up and update the extension table entry for \nP respectively. Every invocation of predicate p in the source program is replaced by an invocation of \nan artificially-introduced predicate p such that the cal\u00adculation of the calling pattern (through abstract) \nand the consulting of the extension table will be performed before exploration of p s clauses. The transformed \npredicate p is a deterministic procedure of the origi\u00adnal code such that it updates the extension table \nfor p and then fails to the next clause if a clause of p has been completely executed. When all clauses \nhave been explored, the summarized success pattern, if any, will be returned. This control scheme can \nbe incorporated into the ab\u00adstract WAM aa follows. Instruction call should be reinterpreted to execute \nlike the artificially-introduced predicate by calculating calling patterns, consulting the extension \ntable, or exploring the clauses. Instruc\u00adtion proceed should be reinterpreted as the part of code wpdateET, \nfail in the end of each transformed clause and lookupET when the original clauses are exhausted. Instruction \nexecute can simply be reverted to a call followed by a proceed. The reinterpretation technique can be \nbetter explained by the annotated WAM code sequence in Figure 5. In this discussion, we have intentionally \nomitted some relevant WAM instructions, including some proce\u00addural instructions and all indexing instructions, \nTheir reinterpretation is almost identical to that of a stan\u00addard WAM with few exceptions. For instance, \ncreation and reclamation of backtracking points would better be incorporated into instructions call and \nproceed rather than instructions try and trust. By such rein\u00adterpretation, the WAM code compiler and \nthe code it generates can be reused without any modHication. 6 Implementation and Performance Evaluation \nTo evaluate the benefits of the proposed approach, we have implemented a prototype analy~er. It is writ\u00adten \nin C and ported to several platforms. The input WAM code programs are generated by the PLM Pro\u00adlog compilerl \nfor a subset of benchmark programs [16]. The analyzer is about 2500 lines of code, includlng lots of \ncomments, debugging and conditional compilation statements. In comparison to the size of other analyz\u00aders \nreported (from 1200 lines [16] to over 3000 lines [18] 1TheoriginalcodewasfromPeterVanRoy [15]andmodiiied \nby Herve Touati for the Aquarius Project at UC Berkeley, 19s9. PI: get args ofp(X) call g % If eqdo~ed(q) \nthen ... else ... call q fi . put u? gs of 7=(x) call T(X) % ... if esplo~ed(r(Xa)) then ... else ... \ncall ~(Xm) fi . proceed % a updateliW(p(X)), fail. p.2: get args ofp(a) proceed % * updatel?ft (p(atom)), \nfail. % last clause of p + lookupEZ (p(lhA)). Figure 5: The Reinterpretation of the Control Instructions \nof Prolog code), the implementation cost of this an~ (A~gs) and predicates (P~eds) in each source program. \nlyzer is relatively low considering the precision of the Additional information includes the static code \nsize analysis. (Size) and the number of (abstract) WAM code exe\u00ad cuted (-llzec) at analysis-time. Neither \nthe times of our Important implementation features of the analyzer analyzer nor those of the Aquarius \nanalyzer (Aquarius)are briefly described as follows. For term abstraction include any preprocessing time. \nSince we use WAM before a predicate invocation, the constant for term\u00adcodes generated from the PLM compiler, \nthe compil~ depth restriction is set to 4, which is also used by tion times are also included in the \ntable (.PLlf). How\u00adTaylor s analyzer [17]. Like the analyzer reported in ever, the PLM compiler is not \nespecially fast by doing [23], this analyzer tries to keep complete aliasing in\u00admany optimizations which \ndo not appear justified as formation of variables (and also other abstract terms). far as our analyzer \nis concerned. The extension table is implemented as a linear list of (calling-pattern, success-pattern) \npairs. Multiple call-The result of the improvements in analysis time per\u00ading patterns are maintained \nfor a predicate while the formance is significant and encouraging. The speed-up success patterns for \neach of the calling patterns are factors range from 14 (zebra) to 575 (tak), and have lubbed (least-upper-bound \nis taken) together. This an (arithmetic) average of 152. The fluctuation of the implementation of the \nextension table is equivalent to speed-up factors is mainly due to the different abstract the aasert \ndatabase technique described in [23, 17]. domains and interpretation algorithms used. If the However, \nit is obviously more straightforward and effi-same abstract domain is used, our conservative esti\u00adcient \nto be implemented in C. It is worth mentioning mation is that the speedup would be over 100 for all that \nthe three-stack scheme used by the standard WAM benchmarks. As a prototype, the implementation was (i.e., \nthe Heap, the Stack, and the Trail) and most other meant to be simple. We believe that there is plenty \nof design considerations remain effective for reinterpreta-room left for more improvements in performance \nbased tion over the abstract domain considered. Neverthe-on better algorithms for abstract interpretation \nsuch less, it is possible to ignore some of the optimization as those described in [3, 20]. For a reference, \nthe mea\u00adtechniques designed for the concrete domain. In partic-surements of analysis time on several \nother platforms ular, the environment trimming technique which strives are given in Appendix A. This \ninformation should be for a quick stack reclamation appears to be overkill in useful for those who do \nnot have access to a Sun 3/60. this abstract WAM. The analysis time is measured on a Sun 3/60 to 7 Conclusions \ncompare with the performance of the Aquarius ana-Theoretically, the time complexities of most interesting \nlyzer [15] which is the only publicly available source dataflow analyses have been shown to be exponential \nfor both benchmark programs and analyzer efficiency. in the worst case [7]. In particular, the precise \ndataflow The Aquarius analyzer s times are running under Quin\u00adanalysis considered in this paper is an \nexample. In the tus Prolog version 2.0. The run-time performance of average case, however, it appears \nthat the analysis time the analyzer is typical among the analyzers reported is proportional to the number \nof arguments in the pro\u00ad(including [23] and [17]). Table 1 gives the results gram and the precision of \nthe abstract domain used. of the measurements of analysis time of our analyzer Thk is shown in the measurement \nresults presented (Our5), the speed-up factors (Speed-Z7p Factor), and in this paper and also the results \ngiven in [15, 23, 17]. the average of speed-up factors. A rough profile of the For a practical system, \nit becomes a design tradeoff be\u00adbenchmarks is given by the number of argument places tween time and precision \nof the analysis. More precise Benchmarks Arg$ Preds WAh code ours log lo 3 2 I&#38;!%.)_4.5 s 179 Exec \n749 q ops8 3 2 3.0 4.5 180 400 23:3 times10 3 2 3,0 4.5 186 971 48.4 dhride10 3 2 2.9 4.6 186 1043 50.7 \ntak 4 2 2.3 1.2 53 110 4.0 nreverse 5 3 2.2 1.6 99 479 26.7 qsort 7 3 3.4 2.5 164 763 44.0 query 7 5 \n4.2 4.3 264 626 25.8 zebra 9 5 3.5 7.5 271 1262 257.9 serialise 16 7 4.2 3.6 205 912 53.4 queens.8 16 \n7 6.0 3.1 117 324 16.5 average Table 1: The Efficiency of Dataflow Analyzers Speed-Up Factor 75 129 \n62 57 575 82 77 163 14 79 364 152 dataflow analysis can be used if the analyzer is more efficient.. \nThe compilation approach of dataflow analysis pre\u00adsented in this paper integrates two important ideas: \nthe efficient compilation framework of the WAM, and the similarity between abstract and concrete interpre\u00adtations. \nIt is more efficient than other implementation approaches for abstract interpretation such as meta\u00adinterpretation \nor program transformation because the interpretive or transforming overhead incurred is re\u00admoved. Moreover, \nthe manipulation of global dataflow information is straightforward and efficient with thb approach. Most \nof all, the approach can be incor\u00adporated into an existing compilation-based systeml in a low-cost manner. \nOur experience in implementing an abstract WAM and its excellent improvement in analysis-time performance \nhave demonstrated the sig\u00adnificance of the approach. It is probable that thb approach can also be effectively \napplied to different classes of dataflow analyses of logic programs or even to different classes of logic \nprogramming languages other than Prolog. Acknowledgements Martha Kosa, Jin-Kun Lin, and Tlng-Wei Hou \nhelped to proof-read this paper. Jieh Hsiang and Jin- Kun Lin always help to provid necessary information \npromptly. The theoretical work by Saumya K. Delway, David S. Warren, and Suzanne W. Dietrich helps us \nto do this work in a very simple way. We thank Pe\u00ad ter Van Roy for hls helpful comments and the results \nfrom the Aquarius project. The first author was mp\u00ad ported in part by the NSC Taiwan. References [1] \n[2] [3] [4] [5] [6] [7] [8] M. Bruynooghe and G. Jenssens, An Instance of Abstract Interpretation Integrating \nType and Mode Inferencing, In Proc. of 5th Id 2 Logic Pro\u00adgramming Conf.$ 1988. C. Codognet, P. Codognet, \nand M.-M. Corsini, Abstract Interpretation for Constraint Logic Languages, In Proc. of North American \nConf. on Logic Programming, 1990. B. Le Charlier, K, Musumbu and P. Van Hen\u00adtenryck, A Generic Abstract \nInterpretation Al\u00adgorithm and its Complexity Analysis, In Proc. of 8th Int 1 Conf. on Logic Programming, \nParis, 1991. P. Cousot and R. Cousot, Abstract Interpret&#38; tiom a Unified Lattice Model for Static \nAnalysis of Programs by Constructions of Fixed Points, In Conf. Rec. of ~th POPL, pp. 78-88, 1977. S. \nK. Debray and D. S. Warren, Automatic Mode Inference for Prolog Programs, In Proc. of 3rd Symp. on Logic \nProgramming, Salt Lake, 1986. S. K. Debray, Efficient Data Flow Analysis of Logic Programs, In Proc. \nof the l#h Symp. of Principles of Programming Languages, San Diego, 1988. S. K. Debray, The Mythical \nFree Lunch (Notes on the Complexity/Precision Tradeoff in Dataflow Analysis of Logic Programs), In Proc. \nof Int 1 Logic Programming Symp. 1991. S. W. Dietrich, Extension tables: memo rela\u00adtions in logic programming, \nIn Proc. of /th IEEE  Benchmarks Aquarius Ours Mac IIx /4Vux Vax DecS Ssl+ DecS SS2 3/60 3/60 TC 4.0 \n9100 8530 3100 5000 loglo 1 75 37 49 86 284 363 500 630 ops8 1 129 63 59 139 469 612 833 1034 times10 \n1 62 30 37 71 231 294 400 500 divide lO 1 57 28 34 65 215 266 372 453 tak 1 575 288 383 639 2091 3286 \n3833 5750 nreverse 1 82 41 56 108 297 333 595 579 qsort 1 77 38 45 95 281 318 548 540 query 1 163 84 \n60 183 618 894 1167 1556 zebra 1 14 5.7 9.4 16 55 63 95 107 serialise 1 79 39 47 94 296 375 538 656 queens.8 \n&#38; 1 364 + 152 1 182 76 .50 200 89 .58 448 177 1.2 1364 564 3.7 1935 794 5.21 2500 1035 6.8 3333 1376 \n9.0 Table2: The Speed Ratios on Various Platforms Int 1 Symp. on Logic Programming, San Francisco, [17] \nH. Tamaki and T. Sate, OLD Resolution with 1987. Resolution, In Proc. 3rd Int 1 Logic Programming Conf., \nLondon, 1986. LNCS 255. [9] K. Marriott and H. S@ndergaard, Bottom-Up Ab\u00ad stract Interpretation of Logic \nPrograms, In Proc. [18] A. Taylor, Removal of Dereferencing and Trailing of 5th Int 1 Conf. on Logic \nProgramming, 1988. in Prolog Compilation, In Proc. of 6ih Int 1 Conf. on Logic Programming, Lisbon, June \n1989. [10] K. Marriott and H. S#ndergaard, Analysis of Constraint Logic Programs, In Proc. of North \n[19] A. Taylor, LIPS on a MIPS: Results form a Pro- American Conf. on Logic Programming, 1990. log Compiler \nfor a RISC, Proc. of 7th Int 1 Conf. on Logic Programming, Jerusalem, 1990. [11] C. S. Mellish, The \nAutomatic Generation of Mode Declarations for Prolog Programs, DAI Research [20] A. Wamn, An Implementation \nTechnique for the Paper No. 163, 1981. Abstract Interpretation of Prolog, In Proc. of 5th Int 1 Conf. \non Logic Programming, 1988. [12] C. S. Mellish, Some Global Optimizations for a Prolog Compiler, In \nJour. of Logic Programming, [21] D. H. D. Warren, Applied Logic -its Use and Im\u00ad 1985:1:43-66. plementation \nas a Programming Tool, Ph. D. The\u00ad sis, Univ. Edinburgh, Scotland, 1977. [13] K. Muthukumar and M. Hermenegildo, \nDe\u00ad termination of Variable Dependence Information [22] D. H. D. Warren, An Abstract Prolog Instruction \nThrough Abstract Interpretation, In Proc. of Set, Tech. Note 309, SRI, October, 1983. North America Conf. \non Logic Programming, 1989. [23] R. Warren, M. Hermenegildo and S. K. Debray, On the Practicality of \nGlobal Flow Analysis of [14] R. A. O Keefe, Finite Fixed-Point Problems, In Proc. of dth Int 1 Conf. \non Logic Programming, Logic Programs, In Proc. of 5th Int 1 Conf. on Logic Programming, 1988. 1987. \n[15] P. Van Roy, A Prolog Compiler for the PLM , Report No. UCB/CSD 84/203, UC Berkeley, A Performance \nEvaluation on Various November 1984. Platforms [16] P. Van Roy and A. M. Despain, The Benefits of Table \n2 is supplied to compare the speed ratios for the Global Dataflow Analysis for an Optimizing Pro-Aquarius \nanalyzer [16] running on a Sun 3/60 work\u00adlog Compiler, In Proc. of North American Conf. station (Aqua~iw) \nand our implementation on eight on Logic Programming, 1990. different platforms: . Sun 3/60/SunOS 4.1.1 \n(Ours 3/60). . Apple Macintosh IIx with THINK C 4.0 (Mac llz,  TC 4.0). . DEC microVax 3100/VMS 5.4 \n(p Vaz 3100). . DEC Vax 8530/Ultrix-32 v3.O ( Van 8590). . DECstation 3100/Ultrix 4.2 (DecS 3100). . \nSun Spare station 1+/SunOS 4.1.1 (SS1-/-). . DECstation 5000/200 /Ultrix 4.2 (DecS 5000). . Sun Spare \nStation 2/SunOS 4.1.1 (SS2).  For simplicity, the figures of the analysis time on each platform have \nbeen omitted. They can be recalculated based on the figures given in Table 1. All timings of our analyzer \nhave been measured with a resolution of 0.1 mSec, and represent the average execution times over 100 \nto 1000 iterations. For Unix-based versions, the cc -0 option on the C compiler is used. The average \nspeed-indexes of the analyzer/platform config\u00adurations are given in the last row (hu!ez) of the table. \n  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Abstract interpretation is a technique extensively used for global dataflow analyses of logic programs. Existing implementations of abstract interpretation are slow due to interpretive or transforming overhead and the inefficiency in manipulation of global information. Since abstract interpretation mimics standard interpretation, it is a promising alternative to compile abstract interpretation into the framework of the WAM (Warren Abstract Machine) for better performance. In this paper, we show how this approach can be effectively implemented in a low-cost manner. To evaluate the possible benefits of this approach, a prototype dataflow analyzer has been implemented for inference of mode, type and variable aliasing information of logic programs. For a subset of benchmark programs in [15], it significantly improves the performance by a factor of over 150 on the average.</p>", "authors": [{"name": "Jichang Tan", "author_profile_id": "81100315265", "affiliation": "", "person_id": "P140018", "email_address": "", "orcid_id": ""}, {"name": "I-Peng Lin", "author_profile_id": "81100220623", "affiliation": "", "person_id": "PP31084609", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143123", "year": "1992", "article_id": "143123", "conference": "PLDI", "title": "Compiling dataflow analysis of logic programs", "url": "http://dl.acm.org/citation.cfm?id=143123"}