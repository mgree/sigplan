{"article_publication_date": "07-01-1992", "fulltext": "\n Remat erializat ion Preston Briggs Keith D. Cooper Linda Torczon Department of Computer Science* Rice \nUniversity Houston, TX 77251-1892 Abstract This paper examines a problem that arises during global register \nallocation rematerialization. If a value can\u00adnot be kept in a register, the allocator should recognize \nwhen it is cheaper to recompute the value (remateri\u00adalize it) than to store and reload it. Chaitin s \noriginal graph-coloring allocator handled simple instances of this problem correctly. This paper details \na general solution to the problem and presents experimental evidence that shows its import ante. Our \napproach is to tag individual values in the pro\u00adcedure s SSA graph with information specifying how it \nshould be spilled. We use a variant of Wegman and Zadeck s sparse simple constant algorithm to propagate \ntags throughout the graph. The allocator then splits live ranges into values with different tags. This \nisolates those values that can be easily rematerialized from val\u00adues that require general spilling. We \nmodify the base allocator to use this information when estimating spill costs and introducing spill code. \nOur presentation focuses on rematerialization in the context of Chaitin s allocator; however, the problem \narises in any global allocator. We believe that our ap\u00adproach will work in other allocators while the \ndetails of implementation will vary, the key insights should carry over directly. Introduction In the \npast decade, the literature on register allocation has focused largely on global allocation -methods \nthat factor information about the entire procedure into the decision process. Because the problem of \noptimal reg\u00adister allocation is NP-complete [19], compilers employ heuristic techniques to approximate \nits solution. In *This work has been supported by DARPA through ONR grant Nooo14-91-J-1989 and by the \nIBM Corporation. Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, tha ACM copyright notica and \nthe title of the publication and ita date appear, and notice is given that copying is by permiaaion of \nthe Aaaociation for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific \npermission. ACM SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0.89791-476-7/92/0006/031 1,..$1 .5(3 general, today \ns generation of global allocators produces good approximations; however, careful examination of the output \nof these allocators reveals that there is still room for improvement. This paper examines a specific \nproblem that arises in global register allocation rematerialization, When a value must be spilled, the \nallocator should recognize those cases when it is cheaper to recompute the value than to store and retrieve \nit from memory. While our discussion is set in the context of a Chaitin-style graph\u00adcoloring allocator \n[6, 5, 3, 1], the same questions seem to arise in all global allocators. Consider the code fragments \nshown in Figure 1 (the notation ~] means the contents of the memory location addressed by p ). Examining \nthe Source column, we note that p is constant in the first loop, but varying in the second loop, The \nregister allocator should take advantage of this situation. Imagine that high demand for registers in \nthe first loop forces p to be spilled; the kieaJ column shows the desired result. In the upper loop, \np is loaded just be\u00adfore it is needed (using some sort of load-immediate instruction). For the lower \nloop, p is loaded just before the loop, again using a load-immediate, The Ckitin column illustratea the \ncode that would be produced by a Chaitin-style allocator. The entire live range of p has been spilled \nto memory, with loads inserted before uses and stores inserted after definitions. The final column shows \ncode we would expect from a splitting allocator [8, 17, 16, 4]; the actual code might be worse. In fact, \nour work on remat erializat ion was motivated by problems observed during our own ex\u00adperiments with live \nrange splitting. Unfortunately, ex\u00adamples of this sort are not discussed in the literature on splitting \nallocators and it is unclear how best to extend these techniques to achieve the Ideal solution, We begin \nour discussion with a brief review of Chaitin-style allocators. Section 3 gives a high-level view of \nour approach to rematerialization. Section 4 describes the low-level modifications to the allocator re\u00adquired \nto support our approach. Experimental results are presented in Section 5. Subsequent sections suggest \nextensions and make comparisons with other work. Source Ideal 1 p +-Label p ~ Label y+y+~] I p* Label \n3 p+p+l 1 33 I Figure 1: Rematerialization Background The notion of modeling register allocation as \na graph coloring problem descends from very early work on stor\u00adage allocation [18]. The first actual \nimplement ation was done by Chaitin et al. in the PL.8 compiler [6]. Chow and Hennessy described a priority-based \nscheme built on a coloring paradigm [8]. Our own work has built on Chaitin s approach [3]. To distinguish \nour allocator from Ghaitin s, we call it the optimistic allocator. Throughout this paper, we assume that \nthe allocator works on either low-level intermediate code or assem\u00adbly code. Before allocation, the code \ncan reference an unlimited number of virtual registers. A single virtual register can have disconnected \nlifetimes in distinct parts of the procedure. Rather than map virtual registers di\u00adrectly onto physical \nregisters, the allocator discovers the distinct live ranges in a procedure and allocates them to physical \nregisters. To model register allocation as a graph coloring prob\u00adlem, the compiler first constructs an \ninterference graph G. Nodes in G represent live ranges; edges represent in\u00adterferences. Thus, there is \nan edge from node i to node j if and only if live range li interferes with live range ij; that is, they \nare simultaneously live at some point and cannot occupy the same register. Live ranges that in\u00adterfere \nwith 1~are its neighbors in the graph; the degree of li is the number of neighbors it has in the graph. \nTo find an allocation from G, the compiler looks for a k-coloring of G; that is, an assignment of k colors \nto the nodes of G such that neighboring nodes always have distinct colors. If we choose k to match the \nnumber of machine registers, then we can map a k-coloring for Chaitin Split ting J p + Label p -Label \nstore p store p 1 In reload p y+y+~] I3 reload p p+p+l store p I versus Spilling G into a feasible \nregister assignment for the underlying code. Because finding a k-coloring of an arbitrary graph is NP-complete, \nthe compiler uses a heuristic method to search for a coloring; it is not guaranteed to find a k\u00adcoloring \nfor all k-colorable graphs. If a k-coloring is not discovered, some live ranges are spilled; i.e., the \nvalues are kept in memory rather than registers. Spilling one or more live ranges changes both the code \nand the interference graph. The compiler proceeds by iteratively spilling some live ranges and attempting \nto color the resulting new graph. This process is guaran\u00adteed to terminate. In practice, this process \nconverges quickly [5, 3] (see also Table 2). Figure 2 illustrates the overall flow of the optimistic \nallocator. Renumber finds the live ranges and gives them unique names. It creates a new live range for \neach defini\u00adtion point and unions together the live ranges that reach each use point. Build constructs \nthe interference graph using the dual representations, a triangular bit-matrix and a set of adjacency \nvectors, advocated by Chaitin [5]. Coalesce attempts to combine live ranges. Two live ranges li and lj \nare combined, giving Iij, if the ini\u00ad tial definition of lj is a copy from [i and there is no interference \nbetween li and lj. This has sev\u00aderal beneficial effects; most importantly, it elimi\u00adnates the copy and \nreduces the total degree of the graph. Since some coalesces can preclude others, the allocator should \nwork inside out, examining deeply-nested blocks first. spill code { > t T ren urnber  build  coalesce \n spill costs  simplify  select Figure 2: The Optimistic Allocator SpiJl Costs estimates the cost \nassociated with spilling each live range. Spill cost is computed as the cost of the memory accesses required \nto spill the live range, each weight ed by 10d where d is the instruct ion s loop nesting depth. Simplify \nconstructs an ordering of the nodes. It re\u00admoves nodes with current degree less than k from G, pushes \nthem on a stack, and decrements the de\u00adgree of their neighbors. If all remaining nodes have k or more \nneighbors, it chooses a spill candidate, removes it from G, and pushes it on the stack. The metric for \npicking spill candidates is critical. Chaitin suggested choosing the node that mini\u00admizes spill cost \ndivided by degree [5]. Select assigns colors to the nodes of G in the order determined by simplify, \nSelect repeatedly pops a node from the stack and attempts to give it a color distinct from its colored \nneighbors. If no color is available for a node, it is left uncolored. If all nodes receive colors, allocation \nis complete. spill Code is invoked if select left a node uncolored. It converts each such node into a \ncollection of tiny live ranges by inserting a load or store at each use and definition. Detailed descriptions \nof these processes can be found in Chaitin s work and our earlier paper [5, 6, 3], Rematerialization \n Chaitin et al. discuss several ideas for improving the quality of spill code [6]. They point out that \ncertain values can be recomputed in a single instruction and that the required operands will always be \navailable for the computation. They call these exceptional values never-killed and note that such values \nshould be re\u00adcalculated instead of being spilled and reloaded. They further note that an uncoalesced \ncopy of a never-killed value can be eliminated by recomputing it directly into the desired register [6]. \nTogether, these techniques are termed rematerialization. In practice, opportunities for rematerialization \ninclude: immediate loads of integer constants and, on some machines, floating-point constants, computing \na constant offset from the frame pointer or the static data area pointer, loads from a known constant \nlocation in either the frame or the static data area, and loading non-local frame pointers from a display. \n The values must be cheaply computable from operands that are available throughout the procedure. It \nis important to understand the distinction between live rzmges and values. A live range may comprise \nsev\u00aderal values connected by common uses. In the Source column of Figure 1, p denotes a single live range \ncom\u00adposed from three values: the address Label, the result of the expression p + 1, and (more subtly) \nthe merge of those two values at the head of the second loop. Chaitin s allocator correctly handles rematerializa\u00adtion \nwhen spilling live ranges with a single value, but cannot handle more complex cases; e.g., the variable \np in Figure 1. Our task is to extend Chaitin s work to take advantage of rematerialization opportunities \nfor complex, multi-valued live ranges. Our approach is to tag each value with enough information to allow \nthe allocator to handle it correct 1y. To achieve this, we 1. split each live range into its component \nvalues, 2. propagate rematerialization tags to each value, and 3. form new live ranges from connected \nvalues having identical tags.  This approach allows correct handling of rematerializa\u00adtion, but introduces \nthe new problem of minimizing un\u00adnecessary splits. The following sections describe how to find values, \nhow to propagate tags, how to split the live ranges, and how to remove unproductive splits. 3.1 Discovering \nValues To find values, we construct the procedure s static single assignment (SSA) graph, a representation \nthat trans\u00adforms the code so that each use of a value references exactly one definition [11]. To achieve \nthis goal, the construction technique inserts special definitions called ~-nodes at those points where \ncontrol-flow paths join and different values merge, We actually use the pruned SSA, with dead ~-nodes \neliminated [7]. A natural way to view the SSA graph for a procedure is as a collection of values, each \ncomposed of a single definition and one or more uses. Each value s definition is either a single instruction \nor a @-node that merges two or more values. By examining the defining instruction for each value, we \ncan recognize never-killed values and propagate this information throughout the SSA graph. 3.2 Propagating \nRematerialization Tags To propagate tags, we use an analog of Wegman and Zadeck s sparse simple constant \nalgorithm [21], We modify their lattice slightly to represent the necessary rematerialization information. \nThe new lattice elements may have one of three types: T Top means that no information is known. A value \ndefined by a copy instruction or a ~-node has an initial tag of T. inst If a value is defined by an appropriate \ninstruction (never-killed), it should be rematerialized. The value s tag is simply a pointer to the instruction. \n1 Bottom means that the value must be spilled and restored. Any value defined by an inappropriate instruction \nis immediately tagged with J_. Additionally, their meet operation n is modified corre\u00adspondingly. The \nnew definition is: anyn T =any any n L=-l inst, n inStj = inst; if inst~ = instj in.st; 17 inst$ = 1 \nif inst, # instj Note that insti = instj compares the instructions on an operand-by-operand basis, Since \nour instructions have at most 2 operands, this modification does not affect the asymptotic complexity \nof propagation. During propagation, each value will be tagged with a particular inst or 1. Values defined \nby a copy instruc\u00adtion will have their tags lowered to inst or 1, depending on the value that flows into \nthe copy. Tags for values defined by qinodes will be lowered to inst if and only if all the values flowing \ninto the node have equivalent inst tags; otherwise, they are lowered to -L. This process tags each value \nin the SSA graph with either an instruction or l-. If a value s tag is 1, spilling that value requires \na normal, heavyweight spill. If, how\u00adever, its tag is an instruction ~ it can be rematerialized by issuing \nthe instruction specified by the tag. The tags are used in two phases of the allocator: spill costs uses \nthe tags to compute more accurate spill costs and spil~ code uses the tags to emit the desired code. \n 3.3 Inserting Splits After propagation, the +-nodes must be removed and values renamed to recreate \nan executable program. Consider the example in Figure 3. The Source column simply repeats the example \nintroduced in Figure 1. The SSA column shows the effect of inserting a qinode for p and renaming the \ndifferent values comprising p s live range. The Splits column illustrates the copies neces\u00adsary to distinguish \nthe different values without ~-nodes. The final column (Minimal) shows the single copy re\u00adquired to isolate \nthe never-killed value p. from the other values comprising p. We avoid the extra copy by not\u00ading that \npl and pz have identical tags after propagation (both are 1) and may be treated together as a single \nlive range plz. Similarly, two connected values with the same inst tagwould be combined into a single \nlive range. For the purposes of rematerialization, the copies are placed perfectly the never-killed \nvalue has been iso\u00adlated and no further copies have been introduced. The algorithm for removing ~-nodes \nand inserting copies is described in Section 4.1. In Section 6, we discuss the possibility of including \nall the copies suggested in the Splits column. 3.4 Removing Unproductive Splits Our approach inserts \nthe minimal number of copies re\u00adquired to isolate the never-killed values. Nevertheless, coloring can \nmake some of these copies superfluous. Re\u00adcall the Minimal column in Figure 3. If neither p. nor P12 \nare spilled and they both receive the same color, the copy connecting them is unnecessary, Because it \nhas a real run-time cost, the copy should be eliminated whenever possible, Of course, coalesce would \nremove all of the copies, losing the desired separation between values with different tags. So, we use \na pair of limited coalescing mechanisms to remove unproductive copies: Conservative coalescing is a straightforward \nmodifica\u00adtion of Chaitin s coalesce phase, Conceptually, we add a single constraint to coalesce only \ncombine two live ranges if the resulting single live range will not be spilled. Biased coloring increases \nthe likelihood that live ranges connected by a copy get assigned to the same reg\u00adist er. Concept uall \ny, select tries to assign the same color to two live ranges connected by a copy. Taken together, these \ntwo mechanisms remove most of the unproductive copies. Source SSA Splits Minimal i p +-Label p. + Label \n I y+?J+~~] ,i,g lo ;$ p12+ PO I 1/0 I ~~ 13 PI+ 4(P0, P2) pbp+l p~+pl+l p2*pl+l P1+P2 p12* plz+1 \nI I Iv I o () Figure 3: Introducing Splits 4 Implementation In our implementation, steps 3 and 4 are \nperformed dur\u00ading a single walk over the dominator tree. Using these Chaitin-style allocators can be \nextended naturally to techniques, renumber completely avoids the use of bit\u00adaccommodate our approach. \nThe high-level structure vectored flow anal ysis. Despite the apparent complexity depicted in Figure \n2 is unchanged, but a number of low\u00adof the algorithms involved, it is very fast in practice and level \nmodifications are required. The next sections dis\u00adrequires only a modest amount of space. cuss the enhancements \nrequired in renumber, coalesce, Because renumber already uses the SSA graph, onlyand select, modest changes \nare required to support rematerializa\u00adtion. The modified renumber has six steps: 4.1 Renumber Chaitin \ns version of renumber (termed getting the 1. Determine liveness at each basic block using a right number \nof names ) was based on clef-use chain\u00ad sparse data-flow evaluation graph. ing [6]. Long before our interest \nin rematerialization, 2. Insert #-nodes based on dominance frontiers, still we adopted an implementation \nstrategy for renumber avoiding insertion of dead ~-nodes. based on the pruned SSA graph. Conceptually, \nthe old implementation has four steps: 3. Renumber the operands in each instruction to refer to values. \nAt the same time, initialize the remate\u00adsparse data-flow evaluation graph [7]. 1. Determine liveness \nat each basic block using a  rialization tags for all values. 2. Insert #-nodes based on dominance \nfrontiers [11]. 4. Propagate tags using the sparse simple constant al- Avoid inserting dead +nodes. gorithm \nas modified in Section 3.2.  3. Renumber the operands in every instruction to refer 5. Examine each. \ncopy instruction. If the source and to values instead of the original virtual registers. At  destination \nvalues have identical inst tags,we can the same time, accumulate availability information union them \nand remove the copy. for each block, The intersection of live and avail is needed at each block to allow \nconstruction of a 6. Examine the operands of each ~-node. If an precise interference graph [6], operand \nvalue has the same tag as the result value, union the values; otherwise, insert a spJit (a distin\u00ad 4. \nForm live ranges by unioning together all the val\u00adguished copy instruction) connecting the values in \nues reaching each +node using a fast disjoint-set the corresponding predecessor block. union. The disjoint-set \nstructure is maintained while building the interference graph and coalesc-Steps 5 and 6 are performed \nin a single walk over the ing (where coalesces are further union operations). dominator tree. 4,2 Conservative \nCoalescing To prevent coalescing from removing the splits that have been carefully introduced in renumber, \nwe must limit its power. Specifically, it should never coalesce a split in\u00adstruction if the resulting \nlive range may be spilled. In normal coalescing, two live ranges li and Ij are com\u00adbined if ij is defined \nby a copy from !i and they do not otherwise interfere. In conservative coalescing, we add an additional \nconstraint: combine two live ranges con\u00adnected by a split if and only if lij has < k neighbors of significant \ndegree, where significant degree means a degree ~ k. To understand why this restriction is safe (indeed, \nit is conservative), recall Chaitin s coloring heuristic [5]. Before any spilling, nodes of degree < \nk are removed from the graph. When a node is removed, the degrees of its neighbors are reduced, perhaps \nallowing them to be removed. This process repeats until the graph is empty or all remaining nodes have \ndegree ~ k. Therefore, for a node to be spilled, it must have at least k neighbors with degree > k in \nthe initial graph. In practice, we perform two rounds of coalescing. Ini\u00adtially, all possible copies \nare coalesced (but not split instructions). The graph is rebuilt and coalescing is re\u00adpeated until no \nmore copies can be removed. Then, we begin conservatively coalescing split instructions. Again, we repeatedly \nbuild the interference graph and attempt further conservative coalescing until no more splits can be \nremoved. In theory, we should not intermix conservative coa\u00adlescing with unrestricted coalescing, since \nthe result of an unrestricted coalesce may be spilled. For example, li and lj might be conservatively \ncoalesced, only to have a later coalesce of tij with 1~ provoke the spilling of lij ~ (since the significant \ndegree of lij~ may be quite high). In practice, this may not prove to be a problem, per\u00admitting a simplification \nof the entire process. Conservative coalescing directly improves the alloca\u00adtion. Each coalesce removes \nan instruction from the resulting code a split instruction that was introduced by the allocator. In \nregions where there is little com\u00adpetition for registers (a region of low register pressure), conservative \ncoalescing undoes all splitting. It cannot, however, undo all of the non-productive splits by itself. \n 4.3 Biased Coloring The second mechanism for removing useless splits in\u00advolves changing the order in \nwhich colors are considered for assignment. Before coloring, the allocator finds part\u00adners values connected \nby splits. When select assigns a color to Ii, it first tries colors already assigned to one of Ii s partners. \nWith a careful implement at ion, this is no more expensive than picking the first available color; it \nreally amounts to biasing the spectrum of colors by previous assignments to ii s partners. The biasing \nmechanism can combine live ranges that conservative coalescing cannot. For example, li might have 2k \nneighbors of significant degree; but these neigh\u00adbors might not interfere with each other and thus might \nall be colored identically. Conservative coalescing can\u00adnot combine ii with any of its partners; the \nresulting live range would have too many neighbors of significant de\u00adgree. Biasing may be able to combine \n/i and its partners because it is applied after the allocator has shown that both live ranges will receive \ncolors. At that late point in allocation, combining them is a matter of choosing the right colors. By \nvirtue of its late application, the biasing mechanism uses a detailed level of knowledge about the problem \nthat is not available any earlier in the process for example, when coalescing is performed. To increase \nthe likelihood that biasing will match partners, we can add limited lookahead. When pick\u00ading a color \nfor ~i, if it has an uncolored partner Ij ~the allocator can look for a color that is still available \nfor lj. On average, Ii has a small number of partners; thus, we can add limited iookahead to biased coloring \nwithout increasing the asymptotic complexity of select, 5 Experimental Results To support our research, \nwe have written an optimiz\u00ading compiler for FORTRAN. The compiler is part of the ParaScope programming \nenvironment and includes sup\u00adport for interprocedural analysis and a variety of tradi\u00adtional optimizations \n[9]. We currently generate code for the IBM RT/PC and have experimental code generators for the Spare, \ni860, and RS/6000. To experiment with register allocation, we have built a series of allocators that \nare independent of any particular architecture [2], Our experimental allocators work with routines ex\u00ad \npressed in ILOC, a low-level intermediate language de\u00ad signed to allow extensive optimization. An ILOC \nrou\u00adtine that assumes an infinite register set is rewritten in terms of a particular target register \nset, with spill code added as necessary. The target register set is specified in a small table and may \nbe varied to allow convenient experiment ation with a wide variety of register sets. After allocation, \neach ILOC routine is translated into a complete C routine. Each C routine is compiled and the resulting \nobject files are linked into a complete pro\u00adgram. There are several advantages to this approach: e By \ninserting appropriate instrumentation during the translation to C, we are able to collect accu\u00adrat e, \ndynamic measurements. . Compilation to C allows us to test a single routine in the context of a complete \nprogram running with real data. . We are able to perform our tests in a machine\u00adindependent fashion, \npotentially using a variety of register sets.  LLE3: nop LLE3: LLA4: ldi r14 8 LLA4: r14 = (int) (8); \ni++; add r9 r15 rll r9 = r15 + rll; mvf fi5 fo fi5 = fo; c++; bc LO023 goto LO023; LO023: lddrr f14 r14 \nr9 LO023:f14 = *((double *) (r14 + r9)); l++; dabs f14 f14 f14 = fabs(f14); dadd f15 f15 f14 f15 = fi5 \n+ fi4; addi r14 r14 8 r14 = r14 + (8); a++; sub r? r10 r14 r7 = r10 -r14; br ge r7 M6 N? if (r? >= O) \ngoto N6; else goto N?; Figure4: ILOCand C Simply timing actual machine code is inherently machine-dependent \nand tends to obscure the effects of allocation, During the translation into C, we can add instrumentation \nto count the number oftimesanyspe\u00adcific ILOCinstruction is executed. For comparing regis\u00adter allocators, \nwe are interested in the number of loads, stores, copies, load-immediates, and add-immediates. Figure \n4showsa small sample ofILOC code and the corresponding C translation. Usually there is a one\u00adto-one mapping \nbetween the ILOC statements and the Ctranslations, though some additionalC isrequired for the function \nheader and declarations of the register variables; e.g., ri4 and f 1S. Also note the very simple instrumentation \nappearing immediately after several of the statements, Ofcourse, this code isverysimple, but the majority \nof ILOC is no more complex, 5.1 The Target Machine For the tests reported here, our target machine is \ndefined to have sixteen integer registers and sixteen floating-point registers. Each floating-point register \ncan hold a double-precision value, so no distinction is made between single-precision and double-precision \nval\u00adues once they are held in registers. Up to four integer registers may be used to pass arguments (recall \nthat ar\u00adguments are passed by reference in FORTRAN; there\u00adfore, the argument registers hold pointers \nto the ac\u00adtual values); any remaining arguments are passed in the stack frame, Function results are returned \nin an integer or floating-point register, as appropriate. Ten of each register class are designated as \ncallee-saves; the remaining six (including the argument registers) are not preserved by the callee. When \nreporting costs, we assume that each load and store requires two cycles; all other instructions are as\u00adsumed \nto require one cycle. Of course, these are only simple approximations of the costs on any real machine. \n 5.2 Spill Costs Since our instrumentation reports dynamic counts of all loads, stores, etc., we need \na mechanism for isolating the instructions due to allocation. A difficulty is that some spills are profitable. \nIn other cases, the allocator re\u00admoves instructions; e.g., copy instructions. Therefore, we tested each \nroutine on a hypothetical (huge ma\u00adchine with 128 registers, assuming this would give a nearly perfect \nallocation. The difference between the huge results and the results for one of the allocators targeted \nto our standard machine should equal the number of cycles added by the allocator to cope with insufficient \nregisters. 5.3 The Test Suite Our test suite is a collection of seventy routines con\u00adtained in eleven \nprograms. Eleven routines are from Forsythe, Malcolm, and Moler s book on numerical methods [13]. They \nare grouped into seven programs with simple drivers. The remaining fifty-nine routines are from the SPEC \nbenchmark suite [20]. Four of the SPEC programs were used: doduc (41 routines), f pppp (12 routines), \nmatrix30() (5 routines), and tomcatv (1 routine). The two other FORTRAN programs in the suite (spice \nand nasa7) require language extensions not yet supplied by our front-end. Table 1 summarizes the effect \nof our new approach to rematerialization. It compares two versions of the optimistic allocator that differ \nonly in their handling of never-killed values. The column labeled Optimistic gives data for a version \nthat uses Chaitin s limited ap\u00adproach to rematerialization. The column labeled Rema\u00adterialization gives \ndata for a version incorporating our new method. The table shows only routines where a difference was \nobserved. The first two columns give the program and subrou\u00adtine name. The third and fourth column give \nthe ob\u00adserved spill costs for the two allocators being compared. Cycles of Spill Code Percentage Contribution \nprogram routine Optimistic Rematerialization load store copy ldi addi total rkf45 fehl 68 50 26 7 7 \n27 seval spline 117 102 10 2 2 1 13 solve decomp 305 286 4 3 1 6 svd svd 1,977 1,966 1 0 o 1 zeroin \nzeroin 236 234 2 1 1 doduc bilan 1,046 966 5 3 8 bilsla 16 15 6 6 colbur 19 24 11 11 -5 -26 ddeflu \n335 375 5 7 1 1 12 debico 459 418 6 I, o I 1 I 2 I IIt 91 deseco 4,957 4,636 7 2 2 o 7 drepvi 218 \n175 4 14 0 2 20 drigl 32 31 3 3 heat 34 31 6 1 9 ihbtr 400 395 1 0 o 1 inideb 50 48 4 4 inisla 31 28 \n6 3 10 inithx 579 437 17 10 -2 25 integr 502 372 II 181 1 12 1 t 3 1 11 26 lectur 221 166 2 23 ii I \norgpar 39 35 II I1 5 II 3 I1 8 I 1 II II 10-. I paroi 1,433 1,383 8 0 11 4] 4 pastern 289 220 20] 10 \nI 13 -19 24 prophy. . 1.531 , 1.525 0 0 0 repvid 599 404 9 13 11 33 fpppp d2esp 35 34 6 -3 3 main twldrv \n210 11,311,624 199 11,198,058 ./, n u 0 5 .,-1 I 5 . matrix300 sgemm 9,905 8,398 12 6 3 tomcatv tomcatv \n367,995,733 355,039,258 4 * 0 o Table 1: Effects These costs are calculated from dynamic counts of \nin\u00adstructions as described earlier. The last column (total) gives the percentage improvement in spill \ncosts due to improved rematerialization large positive numbers in\u00addicate significant improvements. The \nmiddle columns show the contribution of each instruction type to the tot al. All percentages have been \nrounded to the nearest in\u00adteger. Insignificant improvements are reported as O and insignificant losses \nare reported as O. In cases where the result is zero, we simply show a blank. Since results are rounded, \na total entry may not equal the sum of the contributing entries. Consider the first row in Table 1. \nThis row presents results for the routine fehl from the program rkf45. The optimistic allocator generated \nan allocation requir\u00ading 68 cycles of spill code; the enhanced allocator re\u00adquired only 50 cycles. 26~0 \nof the savings came from having to execute fewer loads and 7% arose from fewer copies. There was a 7 \nZOdegradation due to more load\u00adimmediates. The total improvement was 27Y0. of Rematerialization From \nthe entire suite of 70 routines, we observed im\u00adprovements in 28 cases and degradations in only 2 cases. \nOne loss was very small (2 loads, 2 stores, and an extra copy); the other was somewhat larger. Improvements \nranged from tiny to reasonably large, with many greater than 20 ?ZO.Of course, adjusting the relative \ncosts of each instruction, especially loads and stores, will change the amount of improvement. As expected, \nwe see a pattern of fewer load instruc\u00adtions and more load-immediates. Typically, the number of stores \nand the number of copies are also reduced. The reduced number of copy instructions suggests that our \nheuristics for removing unhelpful splits are adequate in practice. Note that this reduction is obtained \nin spite of the extra copies int reduced by renumber.  5.4 Allocation Costs The improved support for \nrematerialization comes at a cost in allocation time. An extra pass over the code is required to initialize \nrematerialization tags before prop\u00ad remrid 1! toncatv twldrv Phase Old -New Old New Old New cfa ,00 \n.00 .00 .00 .01 .01 ren urn .03 .05 .06 .10 .57 .91 build .17 .17 .39 .43 10.27 8.81 costs .01 .01 ,02 \n.02 .16 .14 color .02 .02 .04 .04 1.16 1.21 spiJl .01 .01 .02 .02 .17 ,16 ren urn .02 .03 .02 .04 .10 \n,17 build .06 .05 .09 .12 .63 .83 costs .01 .01 .01 .01 .07 .06 color .01 .02 .02 .03 .14 .21 spill .01 \n,00 .01 .01 .03 .04 ren urn .01 .03 .02 .04 .10 .17 build .03 .06 .05 .09 .60 .82 costs .01 .01 .01 .01 \n.07 .06 color .01 .01 .02 .02 ,13 .21 spill .01 .01 renum .02 -IF build .05 .09 Costs .01 .01 color .01 \n.02 total ti.40 .49 .89 m 14.19 13.80 Table2: Allocation Times in Seconds agation and further time \nis required to propagate the tags throughout the routine. Finally, at least one extra pass is required \nto accomplish conservative coalescing. On the other hand, the build coalesce process may be slightly \nfaster since we are able to eliminate some copies during renumber (recall step 5 in Section 4.1). Table \n2 shows comparative timings for the two alloca\u00adtors on three routines from the SPEC suite, Times are \ngiven in seconds and were measured with a 100 hertz clock on an unloaded IBM RS/6000 Model 540. Each \nrun wss repeated 10 times and the results averaged. The first column shows the phase of allocation, where \ncfa stands for control-flow analysis and includes the time required to compute forward and reverse domina\u00adtors \nand dominance frontiers, buiki includes the entire build-coalesce loop, and coJor includes both simplify \nand select. Note that tomcat v required an additional round of spilling. For each routine, the Old column \ngives times required by the optimistic allocator with Chaitin s scheme and the IVew column gives times \nrequired by the same allocator wit h improved remat erialization. We selected three routines to illustrate \nperformance over a range of sizes. The first routine is repvid, from the program doduc, with 144 non-comment \nlines of FORTRAN. It compiles to a . text size of 1284 bytes using IBM s xlf compiler with full optimization. \nThe second routine is tomcat v, with 133 lines and a . text size of 3064 bytes. The largest routine is \ntvldrv from the program f pppp, with 881 lines and a . text size of 15,616 bytes, All three routines \nappear in Table 1. An obvious conclusion to draw from the data in Ta\u00adble 2 is that support for rematerialization \ncan require a small amount of additional compile-time. Occasion\u00adally, the new allocator may even be fsster, \nthough our experience suggests that twldrv is an exceptional case. The results in Table 2 also illuminate \na number of interesting details about the behavior of both allocators. The inital pass of the build coalesce \nloop dominates the overall cost of allocation (as noted by Chaitin). In comparison, additional iterations \nof the color\u00adspill process are quite inexpensive. In each case, the cost of renumber is higher for the \nIVew allocator, reflecting the cost of propagating rematerialization tags. In all but one case, the cost \nof the build coalesce loop is higher for the New allocator, due to the additional passes of conservative \ncoalescing, The very low costs of control-flow analysis illus\u00adtrates the speed and practicality of the \nalgorithm for calculating dominance frontiers [11], The higher cost of coloring in the first pass arises \nfrom the cost of choosing nodes to spill. While the cost of coloring is linear in the size of the graph, \nspill selection is 0(s. n), where s is the number of spill choices and n is the number of nodes. With \na large number of spills, this term dominates the cost of coloring. are pleased with the overall speed \nof both alloca\u00adtors. Our results appear to be slightly faster than the times reported by IBM s xlf compiler \nfor register al\u00adlocation and comparable to the times reported for op\u00adtimization. In an extensive comparison \nwith priority\u00adbased coloring, our allocators appeared much slower on very small routines, but much faster \non very large rou\u00adtines [2]. Of course, these speeds are not competitive with the fast, local techniques \nused in non-optimizing compilers [14, 15]; however, we believe that global opti\u00admization require global \nregister allocation, 6 Extensions Of course, rematerialization is not the only reason for splitting live \nranges; others have observed that splitting alive range can improve the allocation [12, 8]. A natural \nextension to the scheme described in Section 3 is to split at all ~-nodes, This lets the allocator pick \nand choose among all the values in the SSA graph. The machinery used to support rematerialization can \neasily handle additional splitting. While inserting copies at all #-nodes introduces addi\u00adtional splitting, \nit misses a particularly important csse. Consider the value p. in Figure 3. Because it is unmod\u00adified \nin the first loop, no @node is created at the loop header. Assume that additional code exists between \nthe definition of p. and the first loop. If this causes p. to be spilled, we would like the allocator \nto consider the loop body separately from the code that precedes it, even though they are part of the \nsame value in the SSA graph. If possible, the allocator should rematerialize PO in the loop header, where \nit will be defined exactly once. This suggests adding extra splits at the top of the loop. We have experimented \nwith a number of alternative splitting schemes. These include: 1. splitting all live ranges around all \nloops, 2. split ting all live ranges around outer loops, 3. splitting live ranges around the outermost \nloop where they are neither used nor defined, 4. splitting along the forward dominance frontiers (at \nall d-nodes), and 5. splitting based on both forward and reverse domi\u00adnance frontiers.  Conceptually, \nit seems easy to incorporate these ideas into our allocator; however, experience has shown that significant \nengineering is required. The complete results of our experiments with loop\u00adbased splitting are presented \nin Briggs thesis [2], Each scheme had several major successes; each had several equally dramatic failures. \nW bile the improvements are large enough to warrant further study, the failures are significant enough \nto discourage adoption in a pro duc\u00adtion compiler. Of course, we are holding the allocator to a high \nstandard the results of splitting are compared to the results presented in Section 5. Thus, we immedi\u00adately \nnotice both improvements and degradations. We intend to continue our search for a consistently prof\u00adit \nable approach. Related Worlc Our work extends the work described by Chaitin et al. and recalls an approach \nsuggested by Cytron and F errante. Chaitin et al. introduce the term rernateri\u00ad alization and discuss \nthe problem briefly [6]. Because their allocator cannot split live ranges, they handle only the simple \ncase where all definitions contributing to a live range are identical. Our work is a direct extension \nand is able to handle each component of a complete live range separately and correctly. Cytron and Fer\u00adrante \nsuggest splitting based on (the equivalent of) the SSA [10]. Their goal is minimal coloring in polynomial\u00adtime \n achieved at the cost introducing extra copies. There is no direct discussion of rematerialization; in\u00addeed, \nthe discussion of spilling is very sketchy. In con\u00adtrast, we are concerned primarily with quality of \nspill code. It is also interesting to compare our approach to other published alternatives, particularly \nthe splitting alloca\u00adtor of Chow and Hennessy and the hierarchical coloring allocator of Callahan and \nKoblenz [8, 4], The published work does not indicate how they handle rematerializa\u00adtion. It is possible \nthat they make no special provisions, trusting their splitting algorithm to do an adequate job. While \nwe can imagine constructing implementations of their techniques to allow more direct comparisons, it \nis unlikely that the results would accurately reflect the potential of their schemes. In any case, it \nmay be pos\u00adsible to modify their allocators to take advantage of our approach. 8 Summary The primary \ncontribution of this paper is a natural ex\u00adtension of Chaitin s i,deas on rematerialization, In par\u00adticular, \nwe show how to handle complex live ranges that may be completely or partially rematerialized. We de\u00adscribe \na technique for tagging the component values of a live range with correct remat erializat ion information. \nWe introduce heuristics, especially conservative coalesc\u00ading and biased coloring, that are required for \ngood re\u00adsults. Finally, we present experimental results that show the effectiveness and practicality \nof our extensions. 9 Acknowledgements Greg Chaitin, Ben Chase, John Cocke, Marty Hopkins, Bob Hood, Ken \nKennedy, Chuck Lins, Peter Markstein, Tom Murtagh, Randy Scarborough, Rick Simpson, Tom Spillman, and \nMatthew Zaleski have all contributed to this work through encouragement and enlightened dis\u00adcussion. \nOur colleagues on the ParaScope project at Rice have provided us with an excellent testbed for our ideas. \nTo all these people go our heartfelt thanks. References [1] David Bernstein, Dina Q. Goldin, Martin C. \nGolumbic, Hugo Krawczyk, Yishay Mansour, Itai Nahshon, and Ron Y. Pinter. Spill code mini\u00admization techniques \nfor optimizing compilers. SIG-PL4N Notices, 24(7):258 263, July 1989. Proceed\u00adings of the ACM SIGPLAN \n89 Conference on Pro\u00adgramming Language Design and Implementation. [2] Preston Briggs. Register Allocation \nvia Graph COL oring. PhD thesis, Rice University, April 1992. [3] Preston Briggs, Keith D. Cooper, Ken \nKennedy, and Linda Torczon. Coloring heuristics for regis\u00adter allocation. SIGPLAN Notices, 24(7):275-284, \nJuly 1989. Proceedings of the ACM SIGPLAN 89 Conference on Programming Language Design and Implementation. \n [4] David Callahan and Brian Koblenz. Register allo\u00adcation via hierarchical graph coloring. SIGPIAN \nNotices, 26(6):192 203, June 1991. Proceedings of the ACM SIGPLAN 91 Conference on Program\u00adming Language \nDesign and Implementation. [5] Gregory J. Chaitin. Register allocation and spilling via graph coloring. \nSIGPLAN Notices, 17(6):98\u00ad105, June 1982. Proceedings of the ACM SIGPLAN  82 Symposium on Compiler \nConstruction. [6] Gregory J. Chaitin, Marc A. Auslander, Ashok K. Chandra, John Cocke, Martin E. Hopkins, \nand Peter W. Markstein. Register allocation via color\u00ading. Computer Languages, 6:47 57, January 1981. \n[7] Jong-Deok Choi, Ron Cytron, and Jeanne Fer\u00adrante. Automatic construction of sparse data flow evaluation \ngraphs. In Conference Record of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, \npages 55 66, January 1991. [8] Fred C. Chow and John L. Hennessy. The priority-based coloring approach \nto register allo\u00adcation. ACM Transactions on Programming Lan\u00adguages and Systems, 12(4):501-536, October \n1990. [9] Keith D. Cooper, Ken Kennedy, and Linda Torc\u00adzon. The impact of interprocedural analysis and \noptimization on the JRn programming environ\u00adment. A Cikl Transactions on Programming Lan\u00adguages and Systems, \n8(4):491-523, October 1986. [10] Ron Cytron and Jeanne Ferrante. What s in a name? The value of renaming \nfor parallelism detec\u00adtion and storage allocation. In Proceedings of the 1987 International Conference \non Parallel Process\u00ading, pages 19 27, August 1987. [11] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, \nMark N. Wegman, and F. Kenneth Zadeck. Ef\u00adficiently computing static single assignment form and the control \ndependence graph. ACM 13-ans\u00adactions on Programming Languages and Systems, 13(4):451-490, October 1991. \n [12] Janet Fabri. Automatic storage optimization. SIG-PLAN Notices, 14(8):83-91, August 1979. Pro\u00adceedings \nof the ACM SIGPLA N 79 Symposium on Compiler Construction. [13] George E. Forsythe, Michael A. Malcolm, \nand Cleve B. Moler. Computer Methods for Mathemati\u00adcal Computations. Prentice-Hall, Englewood Cliffs, \nNew Jersey, 1977. [14] Christopher W. Fraser and David R. Hanson. A retarget able compiler for ANSI C. \nSTGPLAN No\u00adtices, 26(10):29 43, October 1991. [15] Christopher W, Fraser and David R, Hanson, Sim\u00adple \nregister spilling in a retargetable compiler, Soft\u00adware Practice and Experience, 22(1):85 99, Jan\u00aduary \n1992. [16] Rajiv Gupta, Mary Lou Soffa, and Tim Steele. Register allocation via clique separators. SIG-PLAN \nNotices, 24(7):264 274, July 1989. Proceed\u00adings of the ACM SIGPLAN 89 Conference on Pro\u00adgramming Language \nDesign and Implementation. [17] James R. Larus and Paul N. Hilfinger. Register allocation in the SPUR \nLisp compiler. SIGPLAN Notices, 21(7) :255 263, July 1986. Proceedings of the ACM SIGPLAN 86 Symposium \non Compiler Construction. [18] S. S. Lavrov. Store economy in closed opera\u00adtor schemes, Zhurnal Vychislitel \nnoi Matematiki i Matematicheskoi Fiziki, 1(4):687-701, 1961. En\u00adglish translation in U.S.S.R. Computational \nMath\u00adematics and Mathematical Physics 3, 1962. [19] Ravi Sethi. Complete register allocation problems. \nSIAM Journal on Computing, 4(3):226-248, 1975. [20] SPEC release 1.2, September 1990. Standards Per\u00adformance \nEvaluation Corporation. [21] Mark N. Wegman and F. Kenneth Zadeck. Con\u00adstant propagation with conditional \nbranches. ACM Transactions on Programming Languages and Sys\u00adtems, 13(2):181-210, April 1991.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>This paper examines a problem that arises during global register allocation &#8211; <italic>rematerialization</italic>. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.</p><p>Our approach is to tag individual values in the procedure's SSA graph with information specifying how it should be spilled. We use a variant of Wegman and Zadeck's <italic>sparse simple constant</italic> algorithm to propagate tags throughout the graph. The allocator then splits live ranges into values with different tags. This isolates those values that can be easily rematerialized from values that require general spilling. We modify the base allocator to use this information when estimating spill costs and introducing spill code.</p><p>Our presentation focuses on rematerialization in the context of Chaitin's allocator; however, the problem arises in any global allocator. We believe that our approach will work in other allocators&#8211;while the details of implementation will vary, the key insights should carry over directly.</p>", "authors": [{"name": "Preston Briggs", "author_profile_id": "81100277551", "affiliation": "", "person_id": "PP14104037", "email_address": "", "orcid_id": ""}, {"name": "Keith D. Cooper", "author_profile_id": "81100286556", "affiliation": "", "person_id": "P158954", "email_address": "", "orcid_id": ""}, {"name": "Linda Torczon", "author_profile_id": "81332513281", "affiliation": "", "person_id": "PP40035980", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143143", "year": "1992", "article_id": "143143", "conference": "PLDI", "title": "Rematerialization", "url": "http://dl.acm.org/citation.cfm?id=143143"}