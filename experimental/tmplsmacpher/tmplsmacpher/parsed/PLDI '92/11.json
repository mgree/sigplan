{"article_publication_date": "07-01-1992", "fulltext": "\n An Abstract Machine for CLP(7?) JOXAN JAFFAR* SPIRO MICHAYLOVt PETER J. STUCKEY$ ROLAND H. C. YAP*! \nAbstract An abstract machine is described for the CLP(7i!) pro\u00adgramming language. It is intended as a \nfirst step towards enabling CLP(7t) programs to be executed with efficiency approaching that of conventional \nlan\u00adguages. The core Constraint Logic Arithmetic Ma\u00adchine (CLAM) extends the Warren Abstract Machine \n(WAM) for compiling Prolog with facilities for handling rea 1 arithmetic constraints. The full CLAM includes \nfacilities for taking advantage of information obtained from global program analysis. Introduction \nThe CLP( R) language is an instance of the Constraint Logic Programming scheme (CLP) [3], a class of \nrule based constraint languages. CLP(77,) embodies con\u00adstraints over the domain of uninterpreted functors \nover real arithmetic terms. Appendix A contains an intro\u00adduction to CLP(%3). In this paper we present \nthe design of the Constraint Logic Arithmetic Machine (CLAM) for the efficient execution of CLP( R) programs. \n*IBM T.J. Watson Research Center, P. O, Box 704, York\u00adtown Heights, NY 10598, USA. f School Of Computer \nSczence, Carnegie Mellon university, Pittsburgh, PA 15213, USA. I Dept. of Computer Science, Umv. of \nMelbourne, Parkvdie, Vzctoria 3052, Australia. f On leave from Dept.of Computer Science, Monash Univer\u00ad \nsity, Clayton, Victoria 3168, Australta. Permission to copy without fee all or part of this material \nis granted provided that tha copies ara not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. ACM .SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0-89791 -476-7 /92/0006 /0128 \n. ..$1 .50 Abstract machines have been used for implementing programming languages for many reasons. \nPortability is one: only an implementation of the abstract machine needs to be made available on each \nplatform. Another is simply convenience: it is easier to write a native code compiler if the task is \nfirst reduced to compiling for an abstract machine that is semantically closer to the source language, \nThe best abstract machines sit at just the right point on the spectrum between the con\u00adceptual clarity \nof the high-level source language and the details of the target machine. In doing so they can often be \nused to express programs in exactly the right form for tackling the efficiency issues of a source language. \nFor example, the Warren Abstract Machine (WAM) [10] revolutionized the execution of PROLOG, since translating \nprograms to the WAM exposed many opportunities for optimization that were not apparent at the source \nlevel. An example from further afield is the technique of optimizing functional programs by first converting \nthem to Continuation Passing Style (CPS) [4]. CPS conversion has lead to highly optimiz\u00ading compilers \nfor Scheme and Standard ML. The bene\u00adfit from designing an appropriate abstract machine for a given source \nlanguage can be so great that even exe\u00adcuting the abstract instruction code by interpretation can lead \nto surprisingly efficient implementations of a language. For example, many commercial PROLOG systems \ncompile to WAM-like code. Certainly more ef\u00adficiency can be obtained from native code compilation, but \nthe step that made PROLOG usable was that of compiling to the WAM. While the WAM made PROLOG practical, \nglobal analysis shows the potential of making another major leap. For example, Taylor [8] and Van Roy \n[9] used fairly efficient analyzers to generate high quality native code. Based on certain examples, \nthey showed that the code quality was comparable to that obtained from a C compiler. In the case of CLP(7?), \nthe opportunities for obtaining valuable information from global analysis [7] are even greater than in \nPROLOG [1]. In what follows we describe the core CLAM, which includes a comprehensive set of basic instructions. \nThe full CLAM is described next, the focus being on spe\u00adcial instructions to take advantage of information \nob\u00adtained from analysis. The subsequent discussion of em\u00adpirics compares the efficiency of interpretive \ncode, core CLAM code, and optimized CLAM code.  CLP(R) Implementation Issues The central implementation \nissue in CLP(73) is based on an observation about programming methodology: while the expressive power \nof the language comes from its considerable generality, much of the execution of typical programs involves \nsolving simple constraints. These are often just PROLOG-like unifications, arith\u00admetic tests, simple \nevaluations and assignments. This suggests that the generality of the language should not be allowed \nto compromise the efficiency of the more ba\u00adsic features. In particular, PROLOG programs should run as \nefficiently as in a PROLOG system. Further\u00admore, it is desired that simple arithmetic should be per\u00adformed \nefficiently without resorting to a general con\u00adstraint solving mechanism. Another major implemen\u00adtation \nissue is incrementality. Throughout a computa\u00adtion, constraints are being added to or removed from a \ntypically large set of collected constraints. For effi\u00adciency reasons this process should be performed \nwith\u00adout constantly having to resolve all of the collected con\u00adst raints. The original CLP(7?) interpreter \n[5] satisfied these criteria in the sense that it executed PROLOG pro\u00adgrams at least as efficiently as \nother PROLOG inter\u00adpreters, and handled simple constraints directly with\u00adout using the constraint solver. \nThe interpreter was divided into a PROLOG-like inference engine, an in\u00adterface, and a constraint solver. \nThe inference engine, which was based on structure sharing, was responsi\u00adble for search, control and \nunification. The job of the interface was to directly execute arithmetic tests, as\u00adsignments and simple \nevaluations where possible. Oth\u00aderwise it broke up complex constraints into a simpler form to be passed \nto the constraint solver. The con\u00adstraint solver itself was partitioned into an equation solver based \non Gaussian elimination, an inequality solver which used a highly modified incremental sim\u00adplex algorithm, \nand a nonlinear handler which mam\u00adaged the delay and wakeup of nonlinear constraints, Clearly the desirable \nfeatures of the interpreter should be retained in a compiler-based system. Thus the objectives of the \ncore CLAM are as follows. . It should be based on the WAM, because of the latter s suitability for PROLOG. \n . It should provide a set of instructions directly in\u00adterfacing to the constraint solver. These being \nused to replace the functions of the interface in the interpreter. . The granularity of instructions \nshould be right in the sense that constraints can be broken up in a way corresponding to the structure \nof the con\u00adstraint solver.  Another objective of the full CLAM is to effectively make use of the kind \nof information that can be ob\u00adtained from a global analyzer. . A special datatype of mutable arithmetic \nvari\u00adables should be available. These assume fixed but changeable values, as opposed to normal (logical) \nvariables. . Simple evaluation of expressions based on mutable variables, and assignment to such variables, \nshould be supported. Mixed constraints, obtaining some coefficients and constants from the mutable vari\u00adables, \nshould be supported. . Optimization based on variables that will no lo~ger be used, and constraints \nthat will become redundant, should be expressible.  As is typical in PROLOG, we assume that optimizing \ncompilation is performed on a program in the context of a calling pattern which defines the set of allowed \nqueries. Thus, for one program, different code may be obtained for each different pattern. While the \nspecific nature of such patterns need not concern us here, they typically describe which arguments of \na predicate are groundl, and what their types are. Next we describe the CLAM, and as we shall see, the \nextension from the WAM to the CLAM has two es\u00adsential aspects. One is the generalization to arithmetic \nconstraints, and the other is specialization for low-level optimizations. 3 Core CLAM Presented here \nis a set of basic instructions which are sufficient to execute CLP(73) programs in general. In order \nto understand the design of the instructions, it IA variable is ground if it has One fixed v~ue. is necessary \nto discuss some key aspects of the con\u00adstraint solver. Mainly, we will describe basic instruc\u00adtions which \norganize constraints for the solver. Some specialized versions of these instructions are also de\u00adscribed. \nWe conclude with a short discussion of run\u00adtime structures. 3.1 Constraints A linear parametric form \nis co +CIVI +. . .+cnVn, where nz 1, each ca is a real number and each Vi is a distinct solver variable. \nA linear equation, say V= co+ CI WI + -.. + c~W~ equates a solver variable and a parametric form. The \nvariables Wz are known as parameters and V is a non-parameter. Linear inequalities are stored in the \nform lpf ~O or lpf >0 where lpj is a linear parametric form. The solver ensures that every solver variable \nappearing in an equation either is a parameter, or appears as a non-parameter in at most one equation. \nIt also ensures that every variable in the inequalities is parametric if it appears in an equation. The \nbasic instructions of the core CLAM build and manipulate parametric forms: e initpfz CO initializes a \nparametric form with constant cO. . addpf .va{lr}3 ci, Vi adds the term ci * Vi to the parametric form4. \nThe two versions of the instruction (var or val) correspond to whether Vi is a local variable ap\u00adpearing \nfor the first time (a new variable). If so, we need to create storage for the variable, and we may also \nsimplify constraint solving. The variable Vi may be a parameter, in which case c%is added to the coefficient \nof Vi in the parametric form, or already have a parametric representation lp f, in which case Ci * lpf \nis added. . solve_eqO signifies the end of the construction of the linear form. The equation co + CIVI+ \n..0 + cnVn = O is solved by the equation solver.  e solve_geO and solve-gtO Similar to the above, but \nforming an inequality or strict inequality instead of an equation. For example, consider the constraint \n5+ X Y = o, where X is a new variable. It could be compile~ 2The pf stands for parametric form. 3This \nbrace notation indicates addpf -val or addpf ..Var 4V, is either a register or a local variable in the \nstack; the distinction is not important for this paper. as shown in Figure 1. We also indicate how the \nin\u00adstructions are executed in the case where Y is non\u00adparametric, say Y = Z + 3.14 is in the solver. \nRecall that a constructed parametric form contains only parametric and new variables. The process of \nsolving an equation built using such a parametric form roughly amounts to (a) finding a parameter V in \nthe form to become non-parametric, (b) writing the equa\u00adtion into the normalized form V = lpf, (c) substituting \nout V using lpf in all other constraints, and finally (d) adding the new constraint V = lpf to the solver. \nSuppose there is a new variable in the equation to be compiled. Then it will always appear in the parametric \nform at runtime hence it can be chosen in step (a). Since this choice is made at compile time, much of \nthe work of step (b) can also be compiled away. Step (c) is not needed since the variable is new. Hence \nall we need is a new instruction for step (d), one for which the solver always returns true. A similar \nsimplification can be made for the compilation of inequalities which cent ain a new variable, The new \ninstructions: e solveno-fail.eq V e solveno.fail-ge V . solve_no_fail_gt V For example, the above constraint \n5+X Y = O, where X is new, can be better compiled into the instructions in Figure 2. Finally, there \nare a number of simple enhancements we can make to the instruction set to cater for commonly occurring \ncases. The cases where the constant is zero or the coefficient is 1 or 1 occur in the majority of instances, \nso special instructions can be expected both to keep down the code size and cut down on decode time. \nThis can be done using e initpf_O start a new linear form with constant O. e addpf.va{lr}_{+-} Vi add \na term consisting of a variable with coefficient *1. Nonlinear constraints are separated out at compile \ntime so that they appear in one of a few particular forms: X=YxZ,X =pow(Y, Z), X= abs(Y), X = sin(Y), \nX = COS(Y). ClLP(7il) delays the satisfiability of non\u00adlinear constraints until they become linear. To \ndetect when a nonlinear constraint becomes linear, the nonlin\u00adear constraint handler associates with \neach constraint a number of wakeup degrees representing the informa\u00adtion currently known about the variables \nappearing in the constraint. As variables become ground, a partic\u00ad initpf 5 lpf : 5 addpf.var 1, x Jpfi \n5+1*X addpf -val -1, Y ipf 1.86+1*X 1*2 solve.eqO SOJVG1.86+ 1* X l*Z=O Figure 1: Core CLAM code for \nthe constraint 5 + X Y = O initpf -5 ipf -5 addpf -val 1, Y ipf 1.86+1*z solveno_fail_eq X add X=-1.86 \n+1*Z Figure 2: Optimized CLAM code for the constraint 5 + X Y = O ular instance of a nonlinear constraint \npasses from one Solver identifiers degree to another until it reaches a point where it can Arithmetic \nvariables need to be represented differ\u00adbe awoken. For a full description see [6], ently from PROLOG \nvariables. In addition to the usual WAM cell data types, one more is required. Instructions are provided \nfor creating a nonlinear Cells of this type contain a solver identifier. Note constraint in any one of \nits degrees, so for example that the basic unification algorithm needs to be there are five instructions \nfor pow corresponding to the augmented to deal with this new type. various wakeup degrees: . pow_vvv \nVi, Vj , Vk for Vi = pow(Vj, vi) Tagged trail with variables Vi, Vj, In the WAM, the trail merely consists \nof a stack . pow-cvv Vj , Vk for c D ~OW(~, V,) of addresses to be reset on backtracking. For . pow-vcv \nVi, c, Vk for Vi = pow(c, V~) CLP(R), the trail is also used to store changes to . pow-vvc Vi, Vj, c \nfor Vi = pow(~, c) constraints. Hence a tagged value trail is required. . pow-eve cO, Vj, C2 for CO= \npow(~,cz) For example, X = pow(3, Y) is compiled into the in- Choice points struction pow-vcv X, 3, \nY. The remaining forms of a Choice points are expanded slightly so as to save pow constraint, e.g. 8 \n= pow(2, X), are equivalent to the high water mark for solver identifiers and in\u00ad linear equations and \nhence the exponentiation is evalu\u00ad equalities. ated at compile time, and replaced with a linear equa\u00adtion, \ne.g. X = 3. There is also a class of variants of these instructions, such as pow-Vcv, which allow a Linear \nform accumulator variable (the one indicated by V) to be initialized. The A linear constraint is built \nup using one instruc\u00adother nonlinear are handled similarly. tion for the constant term, and one for each \nlinear component. During this process, the partially con\u00adstructed constraint is represented in an accumula\u00adtor. \nOne of the solve instructions then passes the constraint to the solver. 3.2 Data Structures Some of \nthe data structures needed to support the CLAM are a routine extension of those for the WAM the usual \nregister, stack, heap and trail organiza-CLAM tion. The main new structures pertain to the solver. Their \ndescription is beyond the scope of this paper; see Above we described the core CLAM and associated [5]. \nAll that is needed here is that variables involved in runtime structures. Here we present specialized \nas well arithmetic constraints have a solver identifier, which as new instructions which implement some \nkey opti\u00ad is used to refer to that variable s location in the solver mization steps in arithmetic constraint \nsolving. Spe\u00ad data structures. cific kinds of global analysis are required in order to uti-The routine \nmodifications to the basic WAM archi-lize these instructions. A compiler for the core CLAM, tecture are: \nin contrast, requires no global analysis.  4.1 Modes One of the principle mechanisms for enhancing effi\u00adciency \nis to avoid invoking the fully general solver in cases where the constraints are simple. For exam\u00adple \nwhen they are equivalent to tests or assignments. Given some fixed calling pattern for a predicate, in\u00adcluding \ninformation about which variables are ground or unconstrained at call time, we can determine which constraints \ncan be executed as tests or assignments. Consider the following simple program, whose struc\u00adture is typical \nof recursive definitions in CLP(7i!): Sum(o, o). sum(N, X) :\u00ad N>=l, li =l i-1, x) = X -N, sum(!i , X \n). and consider executing the query sum(7, X). The first constraint encountered, N z 1, can be implemented \nsimply as a test since the value of N is known. The second constraint, N = N 1, can be implemented simply \nas an evaluation and assignment to N because IV is a new variable and the value of N is known. These \nobservations continue to hold when executing the resulting subgoal sum(6, X ) and for each subse\u00adquent \nsubgoal. Hence for any query SLUO(C, X), where c is a number, the constraint N ~ 1 is always a test, \nand N = iV 1 is always an evaluation/assignment. To take advantage of such constraints, a new data type, \nfp-val, is provided to represent arithmetic vari\u00adables whose value is known to be ground. (In CLP(R-) \nfp-vals are in fact stored in adjacent pairs of registers, or in adjacent stack locations.) The new instructions \nusing fp-vals are given in Figure 3. For example, the constraints N ~ 1, N = N 1 can be compiled into \nSubcf N, 1, Tmp j gef Tmp, cent fail cent: mvf Tmp, N Even when compilation into the above special instruc\u00adtions \nis not possible, some simplifications can be made. For example, consider the constraint V = 1*R+V. where \nthe values of I and V. will be known when the constraint is encountered. Since I will be known, it is \nclear at compile time that the constraint will be linear and hence there is no need to separate out the \nmultipli\u00adcation. Furthermore, the constraint becomes a linear equation on two variables since V. is also \nknown. We add new instructions for creating parametric forms using fp.val variables: initpffip FPi begin \na parametric form obtaining constant from an fp.val. addpf -fp-va{rl} FPi, Vi add a linear component \nobtaining the coefhcient from an fp-val. The constraint V = I*R+ V. can be compiled to initpffip Vo \naddpf fip-val I, R addpf _val_-v solve-eqO We finally remark that the instructions for nonlinear are \nalso augmented to make use of variables stored in fp-vals. 4.2 Redundancy An obvious way to enhance \nefficiency is to minimize the number of constraints in the solver. Toward this aim we want to eliminate \nredundant constraints, and an obvious starting point is to eliminate newly encoun\u00adtered constraints which \nare redundant. Unfortunately such constraints are rare. What is more common how\u00adever is that new constraints \nmake some earlier collected constraints redundant. We concentrate on linear con\u00adstraints in this subsection \nsince detecting redundancy involving nonlinear constraints is intractable. We now discuss two forms of \nredundancy: one asso\u00adciated with variables and the other with constraints. Redundancy of Variables We \nsay that a variable is redundant at a certain point in the computation if it will never appear again. \nThe elimination of these variables from the solver produces a more compact representation of the constraints \non the variables of interest. Eliminating these variables not only saves space, but also reduces solver \nactivity associ\u00ad ated with these variables when adding new constraints. Every variable becomes redundant \neventually; thus to gain the most compact representation we may project the constraints onto the remaining \nvariables. (For ex\u00adample, if the only variables of interest are S and T , the constraints T = TI, TI \n= TQ+TS T4,T4 TS = 1,T2 = S+2 can be better represented by T= S+1.) However, projection can be expensive, \nespecially when inequali\u00adties are involved. It is only worthwhile in certain cases, one of which is identified \nbelow. We note that identify\u00ading redundant variables could be achieved at runtime . litf c, FPi load \na numeric constant into an fp.val; . getf Vi, FPj convert a solver variable to an fp-val; . putf FPi, \nVj convert an fp-val to a solver variable; . Stf FPi, S put an fp.val on the stack frame (offset S); \n . ldf S, FPi read an fp.val from stack frame (offset S); . mvf FPi, FPj copy one fp.val to another; \n . addf FPi, FPj, FPk add fp-vals; similarly for mulf, subf, divf; . addcf FPi, c, FPk add a constant \nto an fp-val; similarly for mulcf, subcf, divcf; . j eqf FPi, L jump to label L if FPi is zero; . jgtf \nFPi, L jump to label L if FPi is positive. . j gef FPi, L jump to label L if FPi is nonnegative.  Figure \n3: Full CLAM instructions using fp-vals as part of general garbage collection. However, greater (3) init.pf \n-5 benefit can be obtained by utilizing CLAM instructions addpf _vale.+ ~tl !11 to remove these variables \nin a timely fashion. solveao_fail.eqO x ... ... Consider the sumo program above. After compiling away \nthe simple constraints as described above, the Notice that a different set of instructions is required \nfor following sequence of constraints, among others, arise the first equation from that required for \nthe remaining from executing the goal sum(7, X) : equations. Hence the first iteration needs to be un\u00ad \nrolled to produce the most efficient code. (1) x =x 7 xl, We conclude this subsection with a brief discussion \n (2) = (X-7)-6xl,, about implementation. Eliminating a variable X from  (3) = ((X-7) -6)-5 ... ... the \nconstraint solver is quite simple: if X is a non\u00adparametric variable, then we just remove the linear \nUpon encountering the second equation X = X -6 form associated with X (as in the above example). If, \nand simplifying into (2), note that the variable X however, X is a parameter, then to eliminate X we \nwill never occur in future. Hence equation (1) can be must (i) find an equation containing X, (ii) rewrite \nthe deleted. Similarly upon encountering the third equa-equation with X as the subject, (iii) substitute \nout X tion X = X 5 and simplifying into (3), the variable everywhere else using this equation, and \nfinally, (iv) re-X is redundant and so (2) can be removed. In short, move the equation. Thus when X is \na parameter, there only one equation involving X need be stored at any is a trade-off between having \none less equation and per\u00adpoint in the computation. We hence add the following forming the work toward \nthis aim (because this work instructions to the CLAM: is essentially equivalent to adding one new equation). \nFor example, removing an equation is not worthwhile . addpf-va{lr}e ci, Vi if execution immediately backtracks \nafter its removal. . addpf_va{lr]e_{+-} Vi . addpffip_va{lr}e FPi, Vi  The following illustrates a \ntypical execution se\u00adquence. Suppose the solver contained X = Y + 1, T= As before, these augment the \ncurrent parametric form U+ Y+landa newconstraint Y+ X T=Ois with an entry involving Vi, but now they \nindicate that encountered. Suppose further that it is known that Y the variable Vi is redundant and can \nbe eliminated does not appear henceforth and so can be eliminated. (hence the e ). Returning to the example \nabove, a A straightforward implementation would (a) write the possible sequence of new constraint into \nparametric form U Y =O, (b) (1) init-pf -7 substitute out U everywhere by Y, (c) add the new addpf _val-+ \nx constraint U = Y, and finally (d) using the informa\u00adsolveno-fail_eqO x tion that Y is redundant, process \nthe three resulting (2) init _pf -6 equations X= Y+l, T=2*Y+l, U= Yin orderto addpf vale-+ x eliminate \nY in the manner described above. A much  X:1 solvenofiail_eqO better implementation will (a) write \nthe new constraint into parametric form U Y =O, and (b) substitute out Y everywhere by U (instead of \nvice versa). program 1] Quintus 3. Ob I CLP(R) interpreter / CLP( R) compiler nrev 0.66 1.91 0.79 dnf \n0.61 1.89 0.78 zebra 1.33 2.58 1.57 Figure 4: Prolog benchmarks Redundancy of Constraints 4.3 Optimizing \nCompilation Of concern here are constraints which become redun\u00addant as a result of new constraints. One \nclass of this redundancy that is easy to detect is future redundancy defined in [7], where a constraint \nadded now will be made redundant in the future, but it does not effect execution in between these points. \nConsider the sum( ) program once again, and exe\u00adcuting the goal sum (N, X) using the second rule. We \nobtain the subgoal IJ>= 1, N = N-1, SUID(N , X ) and note that we have omitted writing the constraint \ninvolving X. Continuing the execution we have two choices: choosing the first rule we obtain the new \ncon\u00adstraint N = O, and choosing the second rule we obtain the constraint N ~ 1 (among others). In each \ncase the original constraint N ~ 1 is made redundant. The main point of this example is that the constraint \nN ~ 1 in the second rule should be implemented simply as a test5, and not added to the constraint store. \nWe hence add the new instructions . solveno.add-eqO b solveno-add.geO . solveno-add-gtO that behave like \nthe solve class of instructions, but which do not add the new constraint. In general this task involves \nsignificantly less work than the usual con\u00adstraint satisfiability check and addition since we do not \nhave to detect implicit equalities and may avoid sub\u00adstitutions. We finally remark that, in our experiments, \nimple\u00admenting future redundancy has lead to the most sub\u00adstantial efficiency gains compared to the other \nopti\u00ad mization discussed here. The main reason is that inequalities are prone to redundancy, and the \ncost of maintaining inequalities in general is already relatively high. Equations in contrast are maintained \nin a form that is essentially free of this kind of redundancy. 5In ~ener~, these tests are not just simple \nevahmtkm. The kinds of program analysis required to utilize the specialized CLAM instructions include \nthose familiar from PROLOG most prominently, detecting special cases of unification and deterministic \npredicates. Algo\u00adrithms for such analysis have become familiar; see [1, 2] for example. The extension \nto constraints involves no more than a straightforward extension to these algo\u00adrithms. Detecting redundant \nvariables and future redundant constraints can in fact be done without dataflow anal\u00adysis. One simple \nmethod involves unfolding the pred\u00adicate definition (and typically once is enough), and then, in the \ncase of detecting redundant variables, sim\u00adply inspecting where variables occur last in the un\u00adfolded \ndefinitions. For detecting a future redundant constraint, the essential step is determining whether the \nconstraints in an unfolded predicate definition im\u00adply the constraint being analyzed. Some further dis\u00adcussions \nappear in [7]. While we do not have a fully engineered analyzer, our experimental analyzer indicates \nthat many CLP(7?) programs can be analyzed quite effectively.   5 Some Empirics Throughout this section, \nall timings (in seconds) were obtained on an IBM RS 6000/530 workstation running AIX Version 3.0. The \nC compiler used throughout was the standard AIX C compiler with O level optimization. The systems tested \nare e The CLP(7?) interpreter, written entirely in C, whose inference engine uses standard PROLOG structure \nsharing techniques. e The CLP(7?) compiler system, executing core CLAM code. The CLAM code is interpreted, \nus\u00ading an emulator written in C. e An emulator, as above, for the full CLAM, execut\u00ading handwritten code. \n. Quintus PROLOG version 3.Ob, a widely-used commercial system. mortgage(P, T, I, R, B) :\u00adT>i, Ti =T-1, \nP >= o, P1 =p*I-R, mortgage(Pl, Tl, I, R, B). mortgage(P, T, I, R, B) :\u00adT=l, B=P* I-R. Figure5: Program \nfor reasoning about naortgage repayments QI : rnortgage(10000O, 360, 1.01, 1025, l?) Q2 : mortgage(P, \n360, 1.01, 1025, 12625.9) Q3 : R >0 A B~OA mortgage(P, 360, 1.01, R, B) 0< B A B <1030 A mortgage(10000O, \nT, 1.01, 1030, B) Q4 : Figure6: Four queries for the mortgage program double mgl(p, t, i, r) double \nmg2(t, i, r, b) double p,t, i, r; double t, i, r, b; {{ if (t == 1.0) return (p*i -r); double p; else \nif (t > 1.0 &#38;&#38;p >= 0.0) if (t == 1.0) return ((b + r)/i); return mgl(p*i -r, t -1.0, i, r); else \nif (t > 1.0) { else exit(l); p = (mg2(t -1.0, i, r, b) + r)/i; 1 if (p >= 0.0) return p; else exit(l); \nI else exit(l); 3 Figure7: Cfunctionsfor Ql andQ2 CLP(7?) CLP(7?) CLP(7?)~ull CLAM CLP(I?) c query interpreter \ncore CLAM but no redundancy full CLAM program QI 0.10 0.05 0.0042 0.0042 0.0010 Q2 2.08 1.78 0.0054 0.0054 \n0.0016 Q3 2.35 1.84 1.05 0.0700 nla Q4 4.20 2,05 1.78 0.0640 n/a Figure8: Timings for Mortgage program \n135 c Two C programs, for comparison with a CLP( R) program compiled into CLAM code. In Figure 4 we \ncompare CLP(7?) with Quintus. The programs chosen are a naive reverse benchmark (six times on a 150 element \nlist, built using a normal func\u00adtor rather than the list constructor), a program for con\u00adverting boolean \nformulae into disjunctive normal form, and a program which solves a standard logic puzzle by combinatorial \nsearch. The programs in themselves are not interesting, and hence the code is omitted. The important \npoint is that they test major aspects of a PROLOG inference engine. The purpose of this com\u00adparison is \nfirst to indicate the relative speeds of the in\u00adference engines of the two CLP(R) systems, and more importantly, \nto give evidence that the CLAM can be implemented without significantly compromising PRO-LOG execution \nspeed. The main part of this section deals with the con\u00adstraint aspects of the CLAM. We use the program \nin Figure 5 in four different ways, as shown in Figure 6, so as to utilize different constraint solving \ninstructions. Appendix C contains the core CLAM code for the mortgage program, and also the (full) CLAM \ncode ob\u00adtained for this program given the calling patterns of the first and third queries. The code for \nthe second (fourth) query is similar to that for the first (third). Given the calling pattern associated \nwith the first query, the program can be compiled as though it were a simple recursive definition. Similarly \nfor the sec\u00adond query, though a different recursive definition is ob\u00adtained. Figure 7 contains C functions \nwhich implement these two definitions. The third query essentially prop\u00adagates constraints, and hence \nit cannot be compiled so simply. Similarly for the fourth query which essentially carries out a search. \nIn Figure 8, timings are tabulated for the inter\u00adpreter, core CLAM and full CLAM. To separate the effects \nof the mode-based optimizations from those of the redundancy-based optimizations, Figure 8 also con\u00adtains \ntimings for full CLAM code that does not take advantage of redundancy. The first two timing columns illustrate \nthe benefit of compilation over interpretation. For the next two columns, the program is specialized \nw.r.t. the modes corresponding to the four queries. While we have not presented results for a comprehen\u00adsive \nset of benchmarks, the mortgage program is some\u00adwhat typical in structure for recursive definitions, \nIt was chosen because the four different classes of queries require most of the main components of the \nconstraint 6Similar measurements were carried out in [7] for this program and queries. Their focus was \nevaluating specific analyses; here it is the efficiency of specific CLAM code. solver. Similar tests \non a number of other programs, not presented here for space reasons, yielded similarly favorable results. \n 6 Conclusion We have described an abstract machine for the efficient execution of compiled CLP( R) programs. \nIn addition to providing a basis for executing CLP(7?) programs with an efficiency comparable to that \nof commercial Prolog compilers, it is suitable for taking advantage of global optimization techniques \nthat are currently on the leading edge of logic programming implementation techniques. References [1] \nS. K. Debray, Static Inference of Modes and Data Dependencies in Logic Programs , ACM Transactions on \nProgramming Languages and Systems 11 (3), 1989, pp 418-450. [2] S. K. Debray and D.S. Warren, Functional \nCom\u00adputations in Logic Programs , ACM Transac\u00adtions on Programming Languages and Systems 11 (3), 1989, \npp 451-481. [3] J. Jaffar and J-L. Lassez, Constraint Logic Pro\u00ad gramming , Proceedings 14 h ACM Symposium \non Principles of Programming Languages, Jan\u00aduary 1987, pp 111 119. [4] D. Kranz, R. Kelsey, J. Rees, \nP. Hudak, J. Philbin, and N. Adams, ORBIT: an optimizing compiler for Scheme , SIGPLAN 86 Symposium on \nCompiler Construction, June 1986, pp 219 233. [5] J. Jaffar, S. Michaylov, P.J. Stuckey and R.H.C. Yap \n, The CLP(7?) Language and System , ACM Transactions on Programming Languages and Systems, to appear, \nJuly 1992. [6] J. Jaffar, S. Michaylov and R.H.C. Yap, A Methodology for Managing Hard Constraints in \nCLP Systems , Proceedings A CM-SIGPLAN Conference on Programming Language Design and Implementation, \nJune 1991, pp 306 316. [7] N. J@rgensen, K. Marriott and S. Michaylov, Some Global Compile-time Optimizations \nfor CLP(7?) , Proceedings 1991 International Logic Programming Symposium , October 1991, pp 420-434. \n [8] A. Taylor, LIPS on a MIPS: Results from a Pro\u00adlog Compiler for a RISC , Proceedings 7th inter\u00adnational \nConference on Logic Programming, June 1990, pp 174 185 [Also Ph.D. thesis, CS, Univ. of Sydney, 1991]. \n[9] P. van Roy and A.M. Despain, The Benefits of Global Dataflow Analysis for an Optimizing Pro\u00adlog Compiler \n, Proceedings 1990 North Ameri\u00adcan Conference on Logic Programming, october 1990, pp 501-515 [Also Ph.D. \nthesis by Van Roy, CS/EE, UC Berkeley, 1990]. [10] D.H.D. Warren, An Abstract PROLOG Instruc\u00adtion Set \n, Technical note 309, AI Center, SRI In\u00adternational, Menlo Park, October 1983, Appendix A An Overview \nof CLP(7?) Real constants and real variables are both arithmetic terms. If t,tland t2are arithmetic terms, \nthen so are (t, +t~), (t, -t~), (t, *t~), (t~/t2), ah(t), sin(t), cm(t) and pow(tl, t2).Uninterpreted \nconstants and functors are like those in PROLOG. Uninterpreted constants am-i arithmetic terms are terms, \nand so is any vari\u00adable. If ~ is an n-ary uninterpreted functor, n ~ O, and tl,....tn are terms, then \nj(tl,....tn)is a term. If tland t2are arithmetic terms, then tl= t2,tl< t2 and tl< t2are all arithmetic \nconstraints. If at least one of tland t2is not an arithmetic term, then only the expression tl= t2is \na constraint. An atom is of the form p(tl,t2,...,tn)where p is a predicate symbol distinct from =, <, \nand s, and tl,....tnare terms. A CLP(7?,) program is defined to be a finite collection of rules: A. : \nal, a2, ..., ak where each ai, O ~ z ~ k, is either a constraint or an atom. A CLP(7Z) goal is of the \nform ? C I al, az,.,, , a~ where C is a conjunction of constraints and each CYi,1 s i < k, is either \na constraint or an atom. A derivation step for the above CLP(77,) goal selects the first atom or constraints \nal. If al is a constraint c,then ifc AC is satisfiable the new goal is ? C A C]az, . . ..clk. If the \nconstraints are unsatisfiable then execution (backtracks to a point where an alternate choice of matching \nrule is available and proceeds using this rule. If al is an atom P(sl,. ... Sn) and a rule in the program \nof the form p(tl,..., tn) : @l, @2,..., fll. The resulting new goal is of the form ? c/sl=tl, ....sn=tn31.1,...,pl,a \n1,....ak. Ideally the constraint solver would detect whenever the constraints are unsatisfiable. In practice, \nbecause of the intractability of detecting unsatisfiability of non\u00adlinear constraints, our implementation \nof CLP(R) only detects the satisfiability/unsatisfiability of all the non\u00adarithmetic and linear arithmetic \nconstraints in the goal, including nonlinear constraints that are effectively lin\u00adear in the context \nof the non-arithmetic and linear con\u00adstraints. A derivation sequence is a possibly infinite sequence \nof goals, starting with an initial goal, wherein there is a derivation step to each goal from the preceding \ngoal. A derivation sequence is successful if it is finite and its last goal contains only constraints. \nThe constraints in the last goal of successful derivation sequences are called answer constraints. Consider \nthe program in Figure 5, and the goal ?\u00ad1 mg(P, 2, 1.1, MP, B). Execution proceeds by first matching \nwith the first rule to obtain the new goal ?-12 >1, TI=2-l, P>=O, P1 = P*l.1 -MP, mortgage(Pl, Tl, 1.1, \nMP, B) . After four steps in which all the constraints are satisfi\u00adable, we obtain (ignoring trivial \nconstraints) ?-P >=0, PI =P*I. I -MPI mortgage (P 1, 1, 1.1, MP, B). The next derivation step matches \nthe first rule again, ?-P>= O, PI= P*l. I-MP II >1, Tl =l-1, P1 >= O, P1 = P1*l.1 -MP, mortgage (P 1 \n, T1 , 1.1, MP, B). In the next step, the constraint is unsatisfiable (1 > 1) and hence execution backtracks \nand the next matching rule is attempted, resulting in ?-P>= O, P1=P*l. I-MPI1 =1, B= PI*I. i -MP. After \ntwo more steps the constraints have been added to the solver and found to be satisfiable. Now as only \nsatisfiable constraints remain, the derivation is success\u00adful. The answer constraint after simplification \nis P = 0. 826446*B + 1. 73554*MP, P > 0. B Summary litf getf putf Stf Idf mvf addf addcf j eqf jgtf \njgef initpf initpf.O initpf-fp addpf .va{lr} addpf.va{lr}-{+-} addpf fip_va{rl} addpf.va{lr}e addpf-va{lr}e.{+-} \naddpfdp-va{lr]e solve-eqO solve_geO solve_gtO solvemo-fail-eq solve50-fail-ge solveao._fail_gt solveao-add-eqO \nsolveao_add_geO solve-no_add_gtO pow-vvv pow-cvv pow.vcv pow.vvc pow-eve  of main CLAM instructions \nc, FPi load numeric constant into fp-val Vi, FPj convert solver variable to fp_val FPi, Vj convert fp.val \nto solver variable FPi, S put fp_val on stack frame (offset S) S, FPi read fp-val from stack frame (offset \nS) FPi, FPj copy one fp.val to another FPi, FPj, FPk add fp-vals; similarly formulf, subf, divf FPi, \nc, FPk add a constantto an fp-val; similarly formulcf, subcf, divcf, subfc, divfc FPi, L jumptolabelL \nifFPiiszero FPi, L jump tolabel L ifFPi is positive FPi, L jump tolabel L if FPi is nonnegative co initialize \naccumulator lp~with constant CO initialize accumulator lpjwith constant O FPi initialize lpf with constant \nfrom fp-val ci, Vi add ci * Vi to lpf in accumulator Vi add linear component, with 1 or 1 coefficient \nFPi, Vi add linear component, wit h coefficient from fp-val ci, Vi like addpf-va{lr}, eliminating Vi \nif possible Vi like addpf -va{lr}_{+-} , eliminating Vi if possible FPi, Vi like addpf fip.va{lr} , eliminating \nV; if possible invoke equation solver on lpf = O invoke inequality sol~er on lpf ~ O invoke inequality \nsolver on lpf >0 v simply add V = lpf to solver v simply add V ~ lpf to solver v simply add V > lpf to \nsolver check lpf = O, do not add to solver check lpf ~ O, do not add to solver check lpf >0, do not add \nto solver Vi, Vj, Vk for Vi = pow(Vj, Vk) where V;, Vj, Vk are variables Vj, Vk for c = pow(~, V~) Vi, \nc, Vk for Vj = pcw(c, Vj) Vi, Vj, c for K = pow(~, c) cO, Vj, C2 for co = pow(Vj, C2) similar groups \nof instructions for cos, sin, abs and multiplication  C Example CLAM Code General Program mg try trust \nmgl initpf addpf _val_+ solve@O initpf addpf -val-\u00adaddpf var.+ solve-eqO initpf _O addpf _val_+ solve_geO \nmult_Vvv initpf-O addpf_val_+ addpf-val_\u00adaddpf-var-+ solve-eqO getvar getvar jump mg2 gettnum mult.Vvv \ninitpf_O addpf_val_+ addpf_val-\u00adaddpf-val-+ solve_eqO proceed mgl, 5 mg2 -1 T 1 T Tmpl P Tmp3, P, R Tmp3 \nTmp2 P, Tmp2 T, Tmpl mg 1, T Tmpl, P, R Tmpl B I I Specialized for QI mg subcf T, 1, Tmp jgtf Tmp, mgl \nj eqf Tmp, mg2 fail mgl mvf Tmp, T jgef P, cent fail cent mulf P, I, TIUp subf Tmp, R, P jump w mg2 mulf \np, I, T mp subf Tmp, R, B proceed  Specialized for Qs mg subcf T, 1, TZlp jgtf Tmp, mgl j eqf Tmp, \nmg2 fail mgl subcf T, 1, T jgtf T, contl fail contl initpf.O addpf_val-+ P solveao.add_geO initpf.O addpffip_val \nI, P addpf-val--R solveno-fail-eq Tmp3 getvar P, Tmp3 jump mg mg2 subcf T, 1, Tmp4 j eqf Tmp4, cont2 \nfail cont2 subfc O, I, Tmp2 initpf_O addpf_val-+ R addpffip-val Tmp2, P addpf-val.+ B solve-eqO proceed \n \n\t\t\t", "proc_id": "143095", "abstract": "<p>An abstract machine is described for the CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programming language. It is intended as a first step toward enabling CLP(<inline-equation> <f><sc>R</sc></f> </inline-equation>) programs to be executed with efficiency approaching that of conventional languages. The core Constraint Logic Arithmetic Machine (CLAM) extends the Warren Abstract Machine (WAM) for compiling Prolog with facilities for handling real arithmetic constraints. The full CLAM includes facilities for taking advantage of information obtained from global program analysis.</p>", "authors": [{"name": "Joxan Jaffar", "author_profile_id": "81100285914", "affiliation": "", "person_id": "PP39035997", "email_address": "", "orcid_id": ""}, {"name": "Peter J. Stuckey", "author_profile_id": "81100133272", "affiliation": "", "person_id": "PP14057424", "email_address": "", "orcid_id": ""}, {"name": "Spiro Michaylov", "author_profile_id": "81100197077", "affiliation": "", "person_id": "P266380", "email_address": "", "orcid_id": ""}, {"name": "Roland H. C. Yap", "author_profile_id": "81100576891", "affiliation": "", "person_id": "PP15036693", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143127", "year": "1992", "article_id": "143127", "conference": "PLDI", "title": "An abstract machine for CLP(<scp>R</scp>)", "url": "http://dl.acm.org/citation.cfm?id=143127"}