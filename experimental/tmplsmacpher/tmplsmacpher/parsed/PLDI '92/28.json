{"article_publication_date": "07-01-1992", "fulltext": "\n Simple and Efficient BURS Table Generation Todd A. Proebsting* University of Wisconsin-Madisont Abstract \nA simple and efficient algorithm for generating bottom\u00adup rewrite system (BURS) tables is described, \nA small prototype implementation produces tables 10 to 30 times more quickly than the best current techniques. \nThe algorithm does not require novel data structures or complicated algorithmic techniques. Previously \npub\u00adlished methods for on-the-fly elimination of states are generalized and simplified to create a new \nmethod, tri\u00adangle trimming, that is employed in the algorithm. 1 Int roduct ion Tree pattern matching \ncombined with dynamic pro\u00adgramming can be used in code generators to create lo\u00adcally optimal code for \nexpression trees [AGT89]. Code generators based on bottom-up rewrite system (BURS) theory can be extremely \nfast because all dynamic pro\u00adgramming is done when the BURS automaton is built. At compile-time, it is \nonly necessary to make two traversals of the subject tree: one bottom-up traversal to label each node \nwith a state that encodes all op\u00adtimal matches, and a second top-down traversal that uses these states \nto select and emit code. Fraser and Henry [FH9 lb] report that careful encodings can pro\u00adduce an automaton \nthat executes fewer than 50 VAX instructions per node to do both traversals. Two difficulties arise in \ncreating a BURS-style code generator: efficiently generating the states and state\u00adtransition tables, \nand creating an efficient encoding of *This work was supported by NSF grant CCR-8908355. tAuthor s address: \nDept. of Computer Sciences, 1210 W. Dayton St., Madkon, WI 53706. Email: todd@cs .wisc .edu - Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commarcisl advantaga, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to rapublish, requires a fee and/or specific permission. ACM SIGPLAN \n92 PLD1-6/92/CA e 1992 ACM 0-89791-476-71921000610331 . ..$l .50 . the automata for use in a compiler, \nA solution to the encoding problem is described by Fraser and Henry in [FH91b]. Because all potential \ndynamic programming decisions are done at table-generation time, they must be done efficiently. This \npaper describes a simple and efficient table generation algorithm whose implemen\u00adtation is an order of \nmagnitude faster than the best current systems. Simplicity has increased, not decreased, efficiency. \nEfficiency has been enhanced, and tables sizes kept small, by the development of a new technique, triangle \ntrimming, for eliminating most redundant states. Tri\u00adangle trimming is an uncomplicated optimization \nthat, for complex grammars, reduces both the table genera\u00adtion time and table sizes by over 50Y0. We \nalso describe optimizations that take advantage of special properties of BURS states. 2 Related Work \nNaively generating BURS states and state-transition tables fails because the tables become too large. \nA typical CISC machine description will generate over 1000 states,l Directly encoding the transition \ntable for a single binary operator would, therefore, require over 1,000,000 entries. Chase [Cha87] discovered \nthat many of the rows (and columns) of bottom-up pattern-matching tran\u00adsition tables are identical and \nproposed the idea of in\u00addex maps to encode much smaller tables. Index maps are vectors that map states \nof the automaton to rep\u00adresented states for indexing a transition table. States may share a given row \nor column of a transition ta\u00adble through a single indirection. These maps can be produced on-the-fly \nduring table generation so that no superfluous work will be performed. Pelegri-Llopart, the originator \nof BURS theory ([PLG88], [PL88]), incorporated Chase s ideas into 1The integer subset of a Motorola 68OOO \ngrammar has over 800 states (Figure 6). a system that added cost information for dynamic programming \nat table generation time. Balachan\u00addran, Dhamdhere, and Biswas [BDB90] also general\u00adized Chase s ideas \nto use cost information. Henry [Hen89] developed optimization techniques to limit the number of BURS \nstates produced during ta\u00adble generation. With fewer states, a smaller automw ton is produced more quickly. \nHis techniques are much more aggressive than the simple index map techniques, but at the cost of increased \ncomplexity. In [Hen89], Henry states, The table builder uses space and time voraciously, even though \nit uses very complex algo\u00adrithms designed to minimize these resources. Our al\u00adgorithm generalizes and \nsimplifies his work. Our sys\u00adtem can be directly compared to his on a variety of machine specifications, \nand shows a factor of 10 to 30 improvement in generation speed. Previous algorithms rely on bit-vectors \nthat encode information about pattern matching, and rely on aux\u00adiliary data structures to maintain cost \ninformation. ing new nonterminals. Putting the previous rules into canonical form gives the rules on \nthe right of Figure 1. 4 Algorithm to Generate BURS Tables Our method of computing the states and state \ntran\u00adsition tables is an uncomplicated work-list algorithm, This algorithm is outlined below in procedure \nMaino. Initially, the states corresponding to each leaf opera\u00adtor (arity = O) are computed, and added \nto the set of known states, States, and to the list of states to be pro\u00adcessed, WorkList. One by one, \nstates are removed from WorkList and processed. For each operator with arity greater than O, the state \nmust be examined to deter\u00admine what transitions are induced by that state when combined with each of \nthe already processed states. These transitions may create new states to be added to the WorkList, 1 \nprocedure Maino Ours has a simple and efficient unified structure that maintains both cost and pattern \nmatching information. BURS Model The input to a BURS code generator generator is a set of rules. Each \nrule indicates a tree pattern, a cost, 23456789 States = 0 WorkList = 0 ComputeLeaf.Stateso while WorkList \n# 0 do state = Pop( WorkList) V op E Operators do C omputeTransitions( op, state) V  a replacement symbol, \nand an action. The set of all 10 the rules is called the grammar. Figure 1 gives a small 11 end endend \nwhile procedure sample grammar (without actions), The replacement symbol is a nonterminal on the left \nof the rule the linearized tree pattern it derives is on the right. In the sample, goai, reg and addr \nare nonterminals. In addition to nonterminals, the grammar has operators of varying arities. In the sample, \nReg, Int, Fetch, and Plus are operators with respective arities of O, 0, 1, and 2. A BURS pattern matcher \nfinds a least-cost parse of a subject tree for the grammar that reduces to the goal nonterminal, Each \ntree node will be labeled with a state that encodes which rule is to be used when that node is to be \nreduced to a given nonterminal. 3.1 Normal Form Patterns To simplify the generation of BURS tables, all \npat\u00adterns are put into the canonical form introduced in [BDB90]. This form requires that all patterns \nbe of the form n a m where both n and m are nontermi\u00adnals, or of the form no ~ Op(nl, . . . . n~) where \nni are all nonterminals, k ~ O, and op is an operator. This canonical form does not reduce the expressive\u00adness \nof the grammars any set of rules not in canon\u00adical form can be put into canonical form by introduc\u00ad 4.1 \nData Structures Used to Generate BURS Tables The set of known states, States, is a table that main\u00adtains \na one-to-one mapping from individual states to non-negative integers. These integers are used as in\u00addices \ninto state transition tables via index maps. States in a BURS code generator encode three pieces of information \nat any node in a subject tree: the non\u00adterminals derived from patterns that match a rule at that node, \nthe relative costs of those nonterminals, and which rules generated each nonterminal (at a minimal cost). \nSuch triples are called items, and a collection of items describing a particular state is called an itemset. \nItemsets are implemented as arrays of{ cost, rule} pairs that are indexed by a nonterminal. Itemsets \nare, there\u00adfore, states. A cost of infinity (co) indicates that, in this state, no rule derives the given \nnonterminal. The empty state (0) has all costs equal to infinity. The relative costs are called delta \ncosts and are always nor\u00admalized so that the nonterminal with the lowest cost derivation has a delta \ncost of O. Costs within an item\u00adset are normalized by the routine NormalizeCostso be\u00adlow. Simple Grammar \nCanonical Form Rule# LHS RHS cost LHS RHS cost 1. goal + reg (o) goal - reg (o) 2. reg ~ Reg (o) reg \n~ Reg (o) 3. reg ~ Int (o) reg - Int (o) 4. addr ~ Plus(reg, Int) (o) addr - Plus(reg, n.1) (0) 4a. n.1 \n- Int (o) 5. reg d Fetch(addr) (2) reg ~ Fetch(addr) (2) 6. reg - Plus(reg, reg) (2) reg ~ Plus(reg, \nreg) (2) 7. reg ~ Plus(Plus(reg, reg), reg) (3) reg ~ Plus(n.2, reg) (3) 7a. I n.2 + Plus(reg, reg) (0) \nFigure 1: Simple Grammar and Its Canonical Form 1 procedure Normalize Costs(state) 2-deJta = minvi {state[i].~ost} \n3 V nc Nonterminalsdo 4 State[n].co9t= wkn?e[n]. cost delta 5 end V 6 end procedure 4.2 Chain Rules Itemsets \nare computed in a two-step process. Compute 1 ransitionso applies rules of the form n -op(. . .~ to generate \nnonterminals in the initial itemset. Next, the algorithm computes the closure of this set by applying \nchain rules. Chain rules are rules of the form n~ m where both n and m are non\u00adterminals. These rules \nmay introduce new nontermi\u00adnals into an itemset, or they may introduce cheaper ways of deriving nonterminals \nalready in the set. Find\u00ading the closure of the set is done by iteratively trying all the chain rules \nand repeatedly applying those that add new or cheaper nonterminals, until no changes are made, Chxwreo \nbelow implements this procedure, Be\u00adcause all costs are non-negative, and because a change is made only \nif a strictly less expensive derivation is found, this process must terminate. One nonterminal may be \nderived from another by zero or more chain rule applications. The least cost derivation is denoted n~ \nm. The cost of such least cost derivations, Cost(rz ~ m) , can be computed ef\u00adficiently using a shortest \npath algorithm. 1 procedure Closure(state) 2 repeat 3 vT: n~ W?such that m E Nontermincds do 4 cost = \nT. cost + state[m]. cost 5 if cost < state[n]. cost then 6 state[n] = { cost, T } i end if 8 end V 9 \nuntil no changesto state 10 end procedure 4.3 Computing States and Transitions The computation of the \nstates and the state transi\u00adtion tables begins by generating a state for each leaf operator (with arity \nof O) in the routine ComputeLeaf-Stateso. These leaf states must be combined as chil\u00addren of each non-leaf \noperator, and new states will be created. Each new state is added to the WorkList and will be subsequently \nprocessed to determine what tran\u00adsitions it induces. Computing the state to label each leaf is straight\u00adforward. \nRules with a right hand side of the given leaf operator generate nonterminals directly into the itemset. \nNormalizing the costs and finding the closure of the itemset completes the computation of the state corresponding \nto the leaf operator. For each dimension of a non-leaf operator,2 an in\u00addex map of represented states \nis maintained. Repre\u00adsented states are constructed from an itemset by re\u00adtaining only those nonterminals \nthat may contribute to a match in the given dimension for the given op\u00aderator ([Cha87], [BDB90]). Suppose \nthat, for a given grammar, there is no rule with a tree pattern for the binary operator, 0, that has \na left child of nonterminal n. In this case, we would project n out of any state when that state is to \nbe examined as a possible left child (in the I t dimension) of 0. Pr-ojecio will retain only those nonterminals \nin a given state that may be used in determining the tran\u00adsitions that may be induced by that state as \na given child of a particular operator. A represented state also discards the rule field of each item \nbecause that infor\u00admation does not affect transitions (only reductions). For each dimension, d, a table \nof represented states, op. reps[dl, is maintained that encodes a one-to-one mapping between those states \nand non-negative in\u00adtegers. Each dimension s op. map[dl table maintains a mapping from global states \nto represented states (op.map[dl[s] is the represented state to which s maps 2Each ~PeratOrof ~rity ~ \nhas a transition table of~ dimen\u00adsions. 1 procedure ComputeLeafStateso 2 V leaf E Leaves do 3 state = \n0 4 VT : n -+ leajdo 5 if T.cost < state[n]. cost then 6 state[n] = { T.cost, T } 7 end if 8 end V 9 \nNormalize Costs( state) 10 C/osw-e(state) 11 WorkList = Append( WorkList, state) 12 States = States U \n{state} 13 Ieaf.state = state 14 end V 15 end procedure 1 function Project(op, i, state) 2 pState = !3 \n3 V n E Nonternainals do 4 if3 r : m -op(nl,. ... n1,-l, n, n,+l,. ..jn OP,aTitY) then 5 // Nonterminal \nn may be used in the Z h dimension of op. 6 pState[n], cost= state[n]. cost 7 end if 8 end V 9 Norrna&#38;eCosts(pState) \n10 return pState 11 end procedure 1 procedure Compute Transitions(op, state) 2 V i E l.. op. aritgdo \n3 pState = Project(op, i, state) 4 op. nmp[i][state] = pState 5 if pState @ op. reps[i] then 6 op. reps[i] \n= op. reps[i] U {pState} 7 V($l, ..., St I, pshk, S8+I, . . . . sop, ar~ty ) such that each s, c op.repsv] \ndo 8 result = 0 9 Vr:n-op(m~ ,... ,mop,arztv) do 10 cost = r.cost + pState[m,]. cost + ~ ~#, Sj[mj]. \ncost 11 if cost < resdt[n]. cost then 12 result[n] = { cost, r } 13 end if 14 end V 15 Trirn(rewit) 16 \nNormalize Costs(resu!t) 17 if reszdt @ States then 18 Closure( result) 19 WorkList = Append( WorlcList, \nresult) 20 States = States U { result} 21 end if 22 op. transition[sl, . . . . s,_l, pState, s,+l, . \n. . . sOP,ar$tV]= restdt 23 end V 24 end if 25 end V 26 end procedure r map[2] e state 1; transition \n[1] table @ u Figure 2: Computing llansitions for (3(I,T) Using In\u00addex Maps. in the C#hdimension of \nop). Figure 2 illustrates the relationship between index maps and transition tables, Given the states \n1 and r for the children of binary operator L9,an indirection is used to lookup the state transition \nfor the 0 node. Transition tables are computed based on represen\u00adted states, not on the original states. \nThis reduces transition table size because many states may map to the same represented state. At tree-matching \ntime the cost of using this technique is the extra level of indirec\u00adtion necessary to compute transitions. \nCompute Tran\u00adsitions (given below) finds all the transitions that each new state induces when used in \ncombination with other known states for a given operator. Each represented state is checked to see if \nit has al\u00adready been processed. If the represented state has been previously processed, then no additional \nwork must be done. If the represented state is new, the transition table must be extended along the given \ndimension for all possible combinations of the represented states of other dimensions (along with this \nrepresented state). This is done by generating all such combinations and then searching for all applicable \nrules. Once these rules have been applied, the delta costs are normalized, and the itemset is closed. \nIf the generated state is new, then it is added to States and WorkList. (The postponement of Ciosureo \nuntil after the check for the state s existence in States is an optimization justified in 35.4. Z%irno, \nthe routine responsible for reducing the number of states produced, is discussed in $4,4.)  4.4 State \nTrimming Many of the states created by the Compute Transi\u00adtionso are nearly identical. The state-generation \nal\u00adgorithm will run faster if it can increase the likelihood that two created states will be identical. \nTwo states can often be made identical by trimming unessen\u00adtial nonterminals from the itemset. A nonterminal \nis unessential (in a particular state) if it can be proven that it will never be needed to produce a \nleast-cost cover of any subject tree. Henry devised two ad hoc techniques, sibling, and demand trimming \n[Hen89], to identify when one { cost, rule } item (representing a nonterminal) can be safely removed \nfrom a state because another item subsumes it. 4.4.1 Triangle Trimming By generalizing Henry s trimming \ntechniques, we have developed triangle t~imming for safely removing unessential nonterminals from an \nitemset. Triangle trimming considers all pairs of nonterminals in a par\u00adticular itemset and determines \nif, given their respec\u00adtive costs, one of the nonterminals can be removed. A nonterminal can be removed \nif, in all dimensions of all rules where it is applicable, the other nonterminal can be used in a different \nrule to generate the same resulting nonterminal at no greater cost. Informally, a nonterminal, i, can \nbe removed from an itemset if it can be shown that everywhere i can lead to a pattern match, another \nnonterminal, j, in the itemset can also lead to a comparable pattern match at no greater cost, Determining \nif j subsumes i requires comparisons that have a triangular shape (see Figure 3). For a given operator, \n0, and in a given dimension, d, two rules must be found such that both rules represent patterns for 0, \nand one rule, r, can employ i as its dth child, and the other rule, t, can employ j as its d h child. \n(It is not necessary that these rules use i and j directly they may use nonterminals that are derived \nfrom i and j via chain rules.) Since rule r reduces to nonterminal m., it must be shown that t can also \nproduce nr at no greater cost. We, therefore, start by assuming that rule r has matched. From this it \ncan be determined if rule t can also match. Rule t can also match if its children in dimensions other \nthan d can be derived via chain rules from the corresponding children of rule r. (All we are assuming \nis that r matches, therefore all we may as\u00adsume in determining if pt,~ exists for a match of rule t is \nwhether pt,~ may derived from pr,~ via chain rules.) Figure 3 shows how i and j, and the rules r and \nt must relate for j to subsume i. Once rule r is found to use i to derive n,, a rule must be found that \ncan employ j and can also derive nr. Notice that for any rule r that employs i, it is only necessary \nto find one such rule temploying j for j to subsume i, Subsumption is based not only on feasibility, \nbut also on costs. A nonterminal cannot be removed if its removal would force more expensive reductions \nto be found than had it been retained. For the pair of rules, r and t,in Figure 3, it is possible to \nremove i from 1i* Rule r: n. ~ O( ~~,1, . . . . p~,d, .-., f+,m,ty ) * ** T !1 !* ! Rule t: n~ 4 8( \n~~,1, .... ptld, .... pt,wity ) * T j Figure 3: Triangle Trimming Relationship (for j to subsume i) \nstate [i]. cost + r. cost + COSt(pr,.I 4-i) > state ~]. cost + t.cost + f%t(p~,d ~ j) + Cost(nr ~ nt) \n+ ~~#~ cost(p~,k ~ pr,?$) Figure 4: Inequality that must hold for i to be removed if j is present. the \nitemset containing j if the inequality in Figure 4 holds. The cost of using r is the sum of the cost \nof i, the cost of deriving pr,d from i, and the cost of r. Since our premise is only that rule r matches \nand that i and j are present in some itemset, the computation of the cost of using t with j to indirectly \nproduce nr will require not only the costs of t, j, and p~,d ~ j, but will also require the costs of \nderiving the other pt)kfrom Pr,k and the cost of deriving nr from nt. The inequality in Figure 4 is the \nbasis for finding the minimal cost difference between two nonterminals to allow one of them to be removed \nfor a given rule. In general, to safely remove i, it is necessary to examine all contexts in which i \ncan be used and find the cost difference that is sufficient to guarantee that i can be removed based \ncm the relative costs of i and j, The routine, Zliangieo, calculates this minimal difference for any \npair of nonterminals. (When it is impossible for nonterminal j to be used in place of i, regardless of \ntheir respective costs, Triangleo returns co.)  4.4.2 Chain Rule Trimming Two states are identical \nif they represent the same nonterminals at the same costs with each respective nonterminal generated \nby the same rule. Triangle trimming removes nonterminals from states whenever possible, thereby eliminating \nthe possibility that two states differ on the particular costs or rules involving those nonterminals. \nTo further minimize the number of states, it is necessary to bias the algorithm towards using the same \nrules whenever possible. Biasing the algorithm towards using chain rules whenever possible increases \nthe likelihood that two states will have used the same rules to derive a given nonterminal. This bias \ncan be forced by removing nonterminal entries from an itemset prior to closure when it can be determined \nthat Ciosw-eo will restore those nonterminals at an equal or lesser cost using chain rules. The routine, \nTrimo, uses both triangle and chain rule trimming to prune nonterminals from itemsets so that they will \nbe more likely to be identical, thereby reducing the size of the generated tables and the table generation \ntime. 5 Speed Optimizing Techniques The previous routines provide many opportunities for speed optimization. \nSome of the improvements are general techniques not specific to BURS table genera\u00adtion; other improvements \nrely on subtle knowledge of BURS table generation. 5.1 Attempt Cheaper Alternatives First It may appear \nthat the two sets of nested loops in Trimo could be jammed into a single pair of nested loops for improved \nefficiency, Both loops have the in\u00adtended side-effect of removing nonterminals from the states. Since \nthe loops iterate over only the nontermi\u00adnals that remain in the state, the second set of loops will \nnormally iterate fewer times than the first set, Be\u00adcause triangle trimming is an expensive operation \nrel\u00adative to chain rule trimming, it is more efficient to remove all possible nonterminals via chain \nrule trim\u00adming and then attempt triangle trimming only on the remaining nonterminals. 5.2 Precompute \nValues In the previous routines, many situations exist where values can be computed once and used many \ntimes. For instance, F rojecto requires the knowledge of which .thdimensiOnOfOper\u00ad nonterminals can appear \nin the z // Compute C, such that if state[i].cost 2 state[j].cost + C then i can safely be removed from \nstate. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 function T riangle(i,j) ifi= Goal then return 00 // Do \nnot remove the goal nonterminal, tempting as it may be. end if Max = co Vn E Nonterminals-{i} do if Mar< \nCost(n ~ j) -Cost(n ~ i) then Max = Cost(n ~ j) -Cost(n =$ i) end if end V V op IE Operators do V dc \nl..op.aritydo Vr:n, ~ op(pr,I,... ,p.,op, ar~tv) do C; = ~ (k9t(p.,di) ifC;c cothen end end V end V \nend V return Max end procedure LocalMin = Vt : nt+ c.,, = 6 j = Ck = m Of&#38;I,,.. ,p*, op, arity) do \nCost(% 4 W) cOS~(pt,d ~ ~) ~k+dco@t,k ~ C= Cr,t+ CJ+ Ch+ if C < LocalMin then Locahtfin = C end if end \nV if LocalMin > Max then Max = LocalMin end if if Pr,k) t.COSt T.COSt Ci procedure Z rim(.Wate) Vn \nG state do Vm c state (m# n) do C= Cost(n &#38; m) if state[n]. cost z state[m]. cost+ C then state[n]={w, \nL}// Remove n from state. end if end V end V Vn ~ state do Vm ~ state (m# n) do C= Triangle(n, m) if \nstate[n]. cost ~ state[m]. cost+ C then state[n] = { co, 1 } // Remove n from state. end if end V end \nV end procedure ator op. Because this list is invariant for a given rule set, it can be computed once \nand used repeatedly. Efficiency is also enhanced if the list of rules is parti\u00adtioned by the operator \nof the pattern, so that Compute\u00adTransitz onso will only iterate over the list of applicable rules. The \ncost of transitive closure rules (Cost(n &#38; m)) is precomputed advantageously since it is used often \nby Ttirno and !iWangleo.  5.3 Lazy Computations There are O(lV2) possible pairs of nonterminals that \nmay be used in a call to Z3iangleo, but in practice only very few pairs are ever used. Our original im\u00adplementation \nprecomputed the results of calling Trz\u00adangieo with all possible combinations of nonterminals and then \nused table lookup for these values. Using this strategy, Triangleo consumed over 75% of the ex\u00adecution \ntime generating tables for a VAX grammar. With 179 nonterminals in the (canonical form) gram\u00admar, Triangleo \nwas called 32041 times, but fewer than 1000 of those values were ever referenced! Changing the program \nto compute those values by need increased the speed tremendously. Once computed, these values are cached \nfor subsequent calls with the same argu\u00adments, 5.4 Defer Closure If two itemsets are equal before closure, \nthen they must be equal after closure. Because two itemsets are chain\u00adrule trimmed before closure, it \nis also the case that if two itemsets are equal after closure, they must have been equal before closure. \nBy maintaining both pre\u00adclosure and post-closure copies of an itemset in a table, we can check for the \nexistence of an itemset in the table by comparing their pre-closure representations, This allows the \nclosure computation to be deferred until it is known that the state is indeed new and must be added to \nthe table, 5.5 Itemset Equivalence Determining whether an itemset is already in a table of states is \nan expensive operation, and this test is done for every entry in every transition table. The integer \nsubset 68000 grammar required over 425,000 calls to determine itemset equivalence. Making itemset equiv\u00adalence \ntesting efficient is extremely important. For two itemsets to be equal, they must be equal for all of \ntheir items. Fortunately, two observations make testing for equivalence much more efficient: two itemsets \ncreated as members of transition tables for different operators can never be equal, and for any given \noperator it is only necessary to compare the entries corresponding to the left-hand sides of the rules \nfor that operator. By keeping a reference to the generating operator as part of an itemset s representation, \nmany itemsets can be determined to be unequal by recognizing that those entries differ. Should those \nentries be the same, it is only necessary to check that the nonterminal entries for the relevant nonterminals \nare equal for both itemsets. This check must be done after the states have been trimmed. The same routines \nare used to implement the global States table, and each of the local op. repso tables. These tables are \nimplemented as hash tables. Com\u00adputing the hash function is also made more efficient by examining only \nthe relevant nonterminals. Calling NonnahzeCostso after Zlimo, but before CVosureo, allows it to limit \nthe nonterminals it must inspect. Again, the same nonterminals that are rele\u00advant to determining itemset \nequivalence are those that must be normalized prior to a call to C/osureo. 5.6 Specialize Memory Allocation \nOur program allocates and deallocates an enormous amount of memory during the computation of the itemsets \nand transition tables, The primary source of allocation and deallocation of memory in the algo\u00adrithm \nis the tentative allocation of itemsets by Com\u00adpute Transitions and Projecto. Only after the itemset \nis allocated and computed can it be determined if an equivalent state has already been seen, thereby \nallow\u00ading the deallocation of the itemset. Redundant item\u00adsets really must be deallocated for a 68000 \ngrammar the program computed over 100,000 redundant item\u00adsets. Fortunately, knowledge of the the alloca\u00adtion/deallocation \npattern of particular data can lead to very efficient memory management [Han90]. This is the case with \nitemsets. Itemsets, after allocation! are computed and then either retained forever or im\u00admediately released. \nIt can never be the case, therefore, that two itemset deallocation occur sequentially with\u00adout an intervening \nallocation. This allows the creation of specialized deallocation and allocation routines for itemsets. \nThe deallocation routine simply maintains a reference to the last discarded itemset, and does not return \nthe space to the heap. Allocation checks this reference, and if the reference is not null, it returns \nthe reference to the previously deallocated value (and clears the reference); only if the reference is \nnull does the allocator request space from the heap. 5.7 Minimize space On a machine without enormous \namounts of RAM, it is important to avoid over-allocating memory and thrash\u00ading. The single biggest user \nof memory is the itemset representation for all of the computed states. Itemsets are kept as small as \npossible by minimizing the num\u00adber of nonterminals in the canonical form grammar. A naive translation \nof a grammar into canonical form may produce too many nonterminals if it creates dif\u00adferent nonterminals \nthat represent identical patterns. It is important (and easy) to reuse previously created nonterminals. \n6 Unprofitable Optimizations Two additional techniques were not implemented be\u00adcause either the speed-up \ndid not merit the additional complexity, or because the resulting code would only reduce the number of \nstates, without also speeding up the code. 6.1 Closure Speedup Because least-cost transitive chain rules \nare precom\u00adputed for use by Trzrno, they are available for speed\u00ading up the Closureo routine. CVosureo, \nhowever, rep\u00adresents less than 470 of the execution time of the pro\u00adgram, and using these transitive \nrules only speeds that routine by 1O-2OYO. 6.2 Post-pass State Minimization It is possible to further \neliminate states after they and the transition tables have been generated by isolating and removing states \nthat differ only in the respective costs of each constituent nonterminal. State minimiza\u00adtion for BURS \nis similar to DFA state minimization. Because state minimization is a post-pass, it cannot make the program \nfaster-it must make it slower.3 We decided the space savings was not worth the additional complexity \nor time and, therefore, did not attempt to add a state minimization pass. 7 Implementation Results Our \nalgorithm has been implemented in ANSI C [FHP91].4 The input has two parts: a description of the operators \n(including the arity and identifying value of each), and a list of grammar rules. The op\u00aderators are \nlimited to being nullary (leaf), unary, or binary. (The arity was limited because the intended 3HenrY[Hen@ \nfoundthattheadditional timeforthePOst\u00adpasswasnegligibleinh;ssystem. (<1Yo) 4The program,burg,isavailable \n viaanonymousftpfrom lcaese.wise.Thecompressedpub/burg.Z cs.edu.sharfileshar. holdsthecompletedistribution. \n Function I Lines (C/Yacc) I Table Generation I 1981 I Figure 5: Code size for our BURS table generator \napplication required only nullary, unary, and binary operators.) Each rule includes an arbitrarily complex \npattern, the nonterminal the pattern derives, its cost, and a unique external rule number (for identification), \nThe front end of the table generator puts the rules into canonical form. As output the program creates \nC!routines and tables for labeling and reducing a subject tree. The program can output either a simple \ntable-driven tree-labeler and reducer, or a hard-coded labeler and reducer. The hard-coded routines incorporate \nthe time and space saving techniques in [FH91b]. The entire program is under 4000 lines of code that \nsplits evenly between table generation routines and in\u00adput/output routines. Figure 5 gives the number \nof lines of code used to implement the table generator. Our program runs quickly on both simple and com\u00adplex \ninputs. We compare our system to Henry s table generator that was derived from the CodeGen system [Hen89]. \nHis system consists of over 20,000 lines of C code. It is not clear, however, how much of this code is \na direct consequence of algorithm design, and how much is an indirect consequence of the fact that his \nBURS system was derived from the much bigger CodeGen distribution. Figure 6 gives a description of 4 \nsample input gram\u00admars and the execution times for each system on each grammar. The first two grammars \n(used to generate code generators for lcc [FH91a]) are for the VAX and the MIPS R3000 RISC processor. \nTwo others that were developed as part of the CodeGen project are in\u00adteger (byte, word, and long) subsets \nof the VAX and Motorola 68000 processors. The timings were taken on a DECstation 5000 with 96Mb of RAM.5 \nThe differences in the number of generated states between the two systems for the CodeGen grammars can \nbe attributed to the presence of a state minimiza\u00adtion post-pass in Henry s system that is not present \nin our system. It should be noted that if triangle trimming is dzs\u00adabled, the number of states generated \nand the running times are about 100 200% higher than those reported here. 5Thetimingstowards aremoTefavorableourimplementation \nonmachineswithlimitedofRAM. amounts  Grammar Description Henry s System Generation Machine #Rules #Nonterms \n#States Time (seconds) vax 291 48 1017 467,7 MIPS 136 9 125 21.4 vax.bwl 524 179 493 146.8 mot.bwl 462 \n80 499 251.5 Figure 6: Timings 8 Conclusion The algorithm presented is a simple and efficient method \nof producing BURS tables. To the best of our [Cha87] knowledge our system is significantly faster than \nany other BURS system that does aggressive state trim\u00adming. The prototype implementation required fewer \nthan 2000 lines of C code for producing the BURS au\u00adtomata. It was able produce these tables over 30 \ntimes [FH91a] more quickly than the previous state of the art op\u00adtimizing system. Our system does not \nsacrifice table compaction optimizations to achieve this speed to the contrary, the compaction techniques \nincrease the over\u00adall speed of the implementation by reducing the num\u00ad [FH91b] ber of states that must \nbe examined, The algorithm employs only simple data structures and routines to generate these tables \nquickly. We be\u00adlieve that, to a large degree, this design simplicity in\u00adcreases efficiency. To further \nincrease speed, optimiza\u00ad [FHP91]tion that exploit the specific nature of BURS table generation were \nisolated and are described here. To reduce the number of states created a new tech\u00adnique of trimming \nstates, triangle trimming, has been developed to isolate nonterminals that can be removed from a state. \nThis trimming provides a many-fold re\u00ad [Han90] duction in the number of states and a commensurate speed-up \nin table generation. 9 Acknowledgements [Hen89] We would like to thank Robert Henry for making his system \navailable for comparison and explaining much of his early work in BURS table generation. [PL88] References \n[AGT89] Alfred V. Aho, Mahedevan Ganapathi, and Steven W. K. Tjiang. Code generation us\u00ading tree matching \nand dynamic program\u00ad [PLG88] ming, ACM Transactions on Programming Languages and Systems, 11(4):491-516, \nOc\u00ad tober 1989. [BDB90] A. Balachandran, D. M. Dhamdhere, and S. Biswas. Efficient retargetable code \ngenera- Our System Ratio Generation #States Time (seconds) 1015 14.4 32 125 0.6 36 586 15.5 9 838 14.4 \n14 tion using bottom-up tree pattern matching. Computer Languages, 15(3):127-140, 1990. David R. Chase. \nAn improvement to bottom\u00adup tree pattern matching. In Proceedings of the l~th Annual Symposium on Principles \nof Programming Languages, pages 168 177, 1987. Christopher W. Fraser and David R. Han\u00adson. A code generation \ninterface for ANSI c. Software-Practice and Experience, 21(9):963-988, September 1991. Christopher W. \nFraser and Robert R. Henry. Hard-coding bottom-up code generation ta\u00adbles to save time and space. Software \nPractice and Experience, 21(1):1 12, Jan\u00aduary 1991. Christopher W. Fraser, Robert R. Henry, and Todd \nA. Proebsting, BURG fast op\u00adtimal instruction selection and tree parsing. Technical Report 1066, University \nof Wis\u00adconsin, 1991. David R. Hanson. Fast allocation and deallo\u00adcation of memory based on object lifetimes. \nSofiware-Practice and Experience, 20(1):5\u00ad12, January 1990. Robert R. Henry. Encoding optimal pattern \nselection in a table-driven bottom-up tree\u00adpattern matcher. Technical Report 89-02-04, University of \nWashington, 1989. Eduardo Pelegri-Llopart. Rewrite Systems, Paitern Matching, and Code Generation. Phd \nThesis, Technical Report UCB/CSD 88/423, Computer Science Division, Univer\u00adsity of California, Berkeley, \n1988. Eduardo Pelegri-Llopart and Susan L. Gra\u00adham. Optimal code generation for expres\u00adsion trees: An \napplication of BURS the\u00adory. In Proceedings of the 15th Annual Sym\u00adposium on Principles of Programming \nLan\u00adguages, pages 294-308, 1988.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>A simple and efficient algorithm for generating bottom-up rewrite system (BURS) tables is described. A small prototype implementation produces tables 10 to 30 times more quickly than the best current techniques. The algorithm does not require novel data structures or complicated algorithmic techniques. Previously published methods for the on-the-fly elimination of states are generalized and simplified to create a new method, <italic>triangle trimming</italic>, that is employed in the algorithm.</p>", "authors": [{"name": "Todd A. Proebsting", "author_profile_id": "81100592757", "affiliation": "", "person_id": "P283229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143145", "year": "1992", "article_id": "143145", "conference": "PLDI", "title": "Simple and efficient BURS table generation", "url": "http://dl.acm.org/citation.cfm?id=143145"}