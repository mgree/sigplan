{"article_publication_date": "07-01-1992", "fulltext": "\n A Concurrent Compiler for Modula-2+ David B. Wortman, Michael D. Junkin Computer Systems Research Institute \nUniversity of Toronto 6 Kings College Road Toronto, Ontario, Canada M5S 1A4 wortman@csri. toronto.edu \nAbstract processing. Although we describe a particular prototype concurrent compiler in this paper, the \ntechniques we have In this paper we describe a collection of techniques for the developed are applicable \nto compilers for a broad range of design and implementation of concurrent compilers. We be\u00ad languages. \ngin by describing a technique for dividing a source program To provide a realistic environment for developing \nand into many streams so that each stream can be compiled con\u00ad evaluating our ideas we built a prototype \nconcurrent com\u00adcurrently. We discuss several compiler design issues unique piler [JW90] for the Modula-2+ \nsuperset of Modula-2 to concurrent compilers including source program partition\u00ad [Rov86]. Modula-2+ was \nchosen as the target language ing, symbol table management, compiler task scheduling partly for reasons \nof expediency (i.e. availability of an and information flow constraints. The application of our existing \ncompiler that served as a starting point) , but pri\u00adtechniques is illustrated by a complete design for \na concur\u00ad marily because Modula-2+ is a large modern programming rent Modula-2+ compiler. After describing \nthe structure of language that presented many challenges to the language this compiler we present an \nexperimental evaluation of the implementor. We felt that if we could produce an efficient compiler s \nperformance that demonstrates that significant compiler for Modula-2+ then it would be easy to handle \nlan\u00adimprovements in compilation time can be achieved through guages that are easier to compile concurrently \nsuch as Pas\u00adthe use of concurrency. cal and C. During the CCD project we implemented seven major variations \nof our concurrent compiler design to test various implementation strategies. The compiler described 1 \nIntroduction in this paper is the best of 7 compiler that represents the The Concurrent Compiler Development \n(CCD) Project at culmination of our efforts. the University of Toronto was a long term effort to ex-Our \npaper in the 1988 PLDI conference [SWJ+ 88] dis\u00ad plore issues in the design and implementation of concur-cussed \nthe narrow issues of semantic analysis and symbol rent compilers for modern programming languages. We \ntable management in concurrent compilers. A longer dis\u00ad developed techniques for structuring compilers \nto achieve cussion of this topic has been published elsewhere [SW9 1]. significant compilation time speedups \nthrough concurrent This paper describes the design and evaluation of a complete concurrent compiler for \nthe Modula-2+ superset of Modula\u00ad *Current address: IBM Canada Ltd., Toronto Laboratory, 844 Don Mills \nRd., North York, Ontario, Canada M3C 1V7 2. A more detailed discussion of our prototype compilers Permission \nto copy without fee all or part of this material is has been published as a technical report [JW90]. \ngranted provided that the copies are not made or distributed for direct commercial advantage, the ACM \ncopyright notice and the The research described in this paper differs from previous title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nwork in the area in a number of ways; Machinery. To copy otherwise, or to republish, requires a fea and/or \nspecific permission. e It describes a complete, implemented, concurrent com- ACM SIGPLAN 92 PLD1-6/92/CA \na 1992 ACM 0-89791 -476-7 /92/0006 /0068 . ..$1 .50 piler. In their comprehensive bibliography on Con\u00adcurrent \nCompilation [SB90] the authors identified only three implemented modern compilers for shared mem\u00adory \nmultiprocessors [Van87],[Sch79] and ours. The remaining work that they cite are theoretical studiles, \npaper designs or concern only one phase of compila\u00adtion (e.g. parallel parsing). It goes much further \nthan previous efforts in restruc\u00adturing compilers to take advantage of concurrent hard\u00adware. We have \ntaken a very aggressive approach to par\u00adtitioning compilation into separately executable tasks and to \nsplitting the source program into separately colm\u00adpilable sub-units. For example, Vandevoorde [Van137] \nsplit his compiler into a scanning phase and an ev\u00aderything else phase. He did not address the Doesn \nt Know Yet problem discussed in Section 2.2. The symbol table search mechanism (Skepticrd Han\u00addling) \npresented in Section 2.2 is a new approach to dealing with the problem of incomplete symbol tables that \narises in concurrent compilers. It supersedes our previous work [JW90, SW91]. The discussion of information \nflows that constrain con\u00adcurrent processing in Section 2.4 is a new approach to analyzing the performance \nof concurrent compilers. By careful design and optimization of the entire calm\u00adpiler, we have been able \nto achieve much better paral\u00adlel speedups than previous efforts. We have also been able to demonstrate \nthat compilers built using our tech\u00adniques perform better as the size of the program being compiled increases. \nThe closest comparable effort is Vandevoorde s parallel C compiler[Van87]. On ia 5 processor Firefly \nhe was able to achieve a speedupl of 2.5,, 3.3 on large programs and 1.0.. 2.0 on smdller ones. In the \nlimiting case of a best source program we have been able to demonstrate almost linear parallel speedup \n(see Figure. 4). Early in the CCD project we decided to concentrate our efforts on compilers for modem \nblock-structured procedur\u00ad al programming languages. Considering a broader class of languages would have \ndiluted our resources. We con\u00adcentrated on the design of compilers for shared memory multiprocessor systems \nwith modest numbers (i.e. tens not hundreds) of processors. We restricted ourselves to lan\u00adguages in \nwhich reserved words were used to determine the lexical structure of programs, This restriction allows \nus to partition programs for concurrent processing during lexical analysis, Languages which use keywords \nrather than re\u00adserved words to determine program structure usually can t be partitioned for concurrent \nprocessing until after syntax anal ysis thereby reducing the potential for achieving faster compilation \nthrough concurrency. The concurrent compilers that we have designed operate by splitting the source program \ninto separately compilable streams that correspond to scopes of declaration in the pro\u00adgram. Each stream \nis then processed through conventional parsing, semantic analysis and code generation phases, At the \nend of compilation the code generated for all of the streams is merged together to produce the compiler \noutput. The structure of the compiler is discussed in more detail in Section 3. We begin with a discussion \nof the major design issues that lead to this structure. 2 Compiler Design Issues During the CCD project \nwe identified four design issues that had a dominant effect on the performance of our concurrent compilers: \n1. Early splitting of the source program into separately compilable streams and late merging of separately \ncompiled object code into a a complete object mod\u00adule. 2. Management of the compiler symbol table and \nthe Doesn t Know Yet Problem. 3. Management and scheduling of the compiler tasks for each stream. 4. \nInter-scope flows of information that constrain concur\u00adrent processing.  There are many other small \ndesign details that mu~t be ham died well to achieve maximum compiler performance, but doing well on \nthese four issues is essential. 2.1 Source Splitting and Merging Opportunities for concurrent processing \nare increased by splitting the program being compiled into independently compilable streams as early \nin the compilation as possible. In compiling a Modula-2+ module, separately compilable streams arise \nfrom three sources: The body of the module being compiled. Procedures contained in the module body. \nInterfaces (definition modules) that are directly or in\u00addirectly imported by the module. The requirement \nthat reserved words determine source pro\u00adgram structure means that these streams can be identified by \na simple finite state recognize that processes the sequence of tokens produced during or just after lexical \nanalysis. A small amount of token stream lookahead may be necessary to resolve lexical tokens that have \nmultiple interpretations (e.g., PROCEDURE in Modula-2). In our concurrent com\u00adpiler design the procedure \nis the smallest program unit that is compiled concurrently. It is a straightforward exercise to generate \ncode for each procedure separately and to merge this code using simple concatenation. Most previous efforts \nto build concurrent compilers performed splitting during parsing and thereby limited opportunities for \nparallel pro\u00adcessing of the source program. Many paper designs for concurrent compilers proposed splitting \nthe source program being compiled at at-bitt-m-y points without adequate con\u00adsideration for the difficulties \nthat arbitrary splitting would cause for symbol table management and code generation. 2.2 Symbol Table \nManagement and the Doesn t Know Yet Problem Management of the compiler s symbol table is a critical issue \nin achieving good performance and correct seman\u00ad tic analysis in a concurrent compiler, The compilation \nof parts of a program concurrently introduces new problems in symbol table management that do not exist \nin conventional sequential compilers [SWJ+ 88, SW91]. In our compiler the units of compilation correspond \ndi\u00ad rectly to major scopes of declaration. The main reason for this design decision was to allow the \ncompiler symbol table to be organized on a scope basis [SGR79]. This has the de\u00adsirable effect of minimizing \ninter-stream dependencies and making the detection of incomplete tables easier. We use a separate symbol \ntable for each scope of declaration ( defini\u00adtion module, main module, procedure). These symbol tables \nare linked together to provided the correct scope ancestry path for resolving names. A problem unique \nto our style of concurrent compilers is the Doesn t Know Yet (DKY) problem [SWJ+ 88, Ses88, SW91]. In \na sequential compiler, a search of the symbol ta\u00adble for a given identifier either succeeds indicating \nthat the identifier is known or fails, indicating unequivocally that the identifier is not known to the \ncompiler (assuming a suitable scheme for handling allowable forward references). In a concurrent compiler \nthere is a third possible result from a search of the symbol table. The identifier may not be found because \nthe symbol table being searched is incomplete. The table that was being searched may be being constructed \nconcurrently by some other compiler task. A concurrent compiler must recognize when this Doesn t Know \nYet con\u00addition arises and deal with it in a way that doesn t violate the semantics of the language being \ncompiled. Symbol table search must correctly locate declared symbols and never fail to detect an undeclared \nsymbol. We experimented with a number of strategies for dealing with the DKY problem [Ses88, JW90]. The \ndifferences between these strategies are in the amount of delay incurred when a DKY occurs, the amount \nof concurrent processing that can be achieved, and the effort required to implement the strategy. The \nlist of strategies presented below is roughly ordered by decreasing DKY delay, increasing concurrent \nprocessing potential and increasing implementation effort. The major DKY strategies that we investigated \nare: Avoidance: delay the start of semantic analysis for a scope until the declaration analysis of its \nparent scope is complete. This order guarantees that symbol table searches originating in the scope will \nnever encounter an incomplete symbol table in an outer scope. Pessimistic Handling: symbol table search \nblocks and We assume that creation of symbol table entries is atomic with respect to symbol table search \nso that the possibility of symbol table search finding an incomplete symbol table entry does not arise. \n waits for table completion when it encounters an in\u00adcomplete symbol table in some outer scope. Skeptical \nHandling: symbol table search blocks and waits for table completion when it fails to find the identifier \nit is searching for in an incomplete symbol table in some outer scope, Optimistic Handling: symbol table \nsearch blocks md waits for completion of a symbol table entry or the completion of a symbol table (whichever \ncomes first) when it fails to find the identifier it is searching for in an incomplete symbol table in \nsome outer scope. When a symbol table search blocks, the embedded scheduler in our compiler (See Section \n2.3) is notified. It attempts to find another task to run, It will preferentially try to run the task \nwhich will resolve the DKY blockage. In our performance testing, the choice of a method for dealing with \nthe DKY problem caused a variation of about 11D70 in overall compiler performance. The Skeptical Handling \nalgorithm described in Figure 6 is our recommendation for the best compromise between compiler performance \nand ease of implementation. This algorithm incurs the cost c~fa duplicate search for identifiers that \nare found after a DKY blockage, but it gains performance by searching incomplete tables before incurring \na DKY blockage. The performance of the Skepticat Handling algorithm is analyzed in Section 4.3. Another \nsymbol table issue is the handling of builkin names, i.e., identifiers that are automatically provided \nby the compiler, These names are typically builtin input/output routines or mathematical routines like \nsin and sqrt, In a conventional compiler the typical way to implement builltin names is to create a global \nscope that is logically the panent scope of the module being compiled, The symbol table for this global \nscope is preinitialized by the compiler to con\u00adtain all builtin names. With this organization, the normal \nscope-chaining symbol table search will automatically find the builtin names if they have not been redeclared \nin Ithe program. This mechanism is not suitable for a concurrent compiler because it causes the first \nreference to a buil tin name to potentially incur DKY waits on all scopes out to the global scope. In \nparticular, the scope corresponding, to the module body can be quite large so an unnecessary DKY wait \non this scope can cause a significant delay. We took ad\u00advantage of the fact that builtin names could \nnot be redefined in Modula-2+ and treated builtin names as if they were de\u00adclared local to each scope. \nThis approach did not require any replication of symbol table entries, instead it was done by a simple \nmodification of the symbol table search mechanism. 2.3 Compiler Task Scheduling In our compiler structure, \nwe exploit parallelism on two lev\u00adels, First, as already mentioned, the compilation is split into streams \nwhich are processed in parallel. There is one stream for the main module, one for each procedure stream \ndefined therein and one stream for each definition module imported directly or indirectly by the module. \nSecond, we exploit parallelism within the processing of a stream by partitioning the stream into a number \nof tasks, which correspond to the traditional phases of compilation. The number and type of the tasks \na particular stream is partitioned into depends not only on the type of the stream but also on the version \nof the compiler. There are bet ween 2 and 5 tasks per stream (see Figure 5), The task types of the skeptical \nhandling compiler are described in section 3, The task types were chosen to partition the compilation \nas quickly as possible, in an effort to get all the compiler s worker threads busy, The early tasks are \nlarge in number but, in general, small, while the later ones, such as the statement analyzer/code generator \ntasks, are large but fewer in number. However, there is no need to partition these later tasks further \n(although there may be parallelism that can be exploited) because there are almost always enough of these \ntasks to ensure that all processors are fully utilized, 2.3.1 Tasks The task is the atomic unit of parallelism \nin our compilers. The execution of a given task is constrained by the rate the tasks on which it depends \nfor information are progressing (See sections 2,2 and 2.4). These constraints are embodied by events, \nthe concurrency mechanism used. An event is simply something that either has or has not occurred. A task \nwaits on an event if and only if it hasn t occurred. Events are discussed further in section 2.3.3 below. \n A task communicates with the tasks on which h depends, and which depend on it, as a producer/consumer \npair on a particular data structure. Each such data structure has asso\u00adciated with it one or more events, \nsignaled by the producer when the events occur. The signaling of such an event in\u00addicates that a particular \nportion of a shared data structure is complete and ready to be used. As an example, the Splitter task \nand the Lexor task of a main module stream communi\u00adcate via a lexical token queue, The elements in this \nqueue are blocks of tokens. Each block is associated with one event. When the Lexor fills a token block, \nthe block s event is signaled, indicating to the Splitter that it now may begin to read the tokens of \nthat block.  2.3.2 The Supervisors Approach The number of tasks in a compilation ranges from a dozen \nor so in a very small compilation to a few hundred in a large compilation. We concur with the results \nof Vande\u00advoorde and Roberts [VR88] that the best performance on the type of multiprocessors we are considering \nresults from carefully limiting the amount of concurrent activity in the system so as not to overwhelm \nmachine resources. If an attempt was made to execute all of these tasks concurrently saturation of the \noperating system and the virtual memory would make concurrent compilation effective only for small source \nmodules, Instead, we initiate one compiler process (Worker) for each real hardware processor. These workers \nare managed by a supervisor which oversees the assignment of tasks to workers. This approach, called \nSupervisors , is an extension of WorkCrews [VR88] that handles blockable tasks. At compiler initialization \ntime, the tasks of the main mod\u00adule stream are created by the compiler initialization thread, and then \na number of worker threads corresponding to the number of hardware processors are created. The initializa\u00adtion \nthread then blocks, waiting for the workers to perform the compilation. The workers begin by searching \nthe su\u00adpervisor s task queuing structure for tasks to perform. One begins the main module stream s Lexor \ntask, and the com\u00adpilation begins. 2.3,3 Events Events are classified into three categories. First, avoided \nevents are those events that must occur before the task(s) that will wait on them can begin. They are \ncalled this because we avoid waiting on the event by simply not assigning the task to a worker until \nthe event has occurred. The rationale is that the amount of work a task could do before being forced \nto wait on such an event is so little so as to be outweighed by the scheduling cost when the wait occurs, \nThe second type of event is the handled event. We allow tasks that may wait on handled events to begin, \nhoping that the amount of work they can do before being forced to wait on such an event outweighs the \nscheduling cost should the wait occur. If the wait does occur, the worker performing the task searches \nfor other tasks to execute, showing preference to those tasks whose execution will lead toward the event \noccurring. Lastly, barrier events are a special kind of handled event for which the worker executing \nthe waiting task is not rescheduled. The worker simply waits for the event to occur. Barrier events are \nonly used in the token streams. We know it is safe to simply wait because token consumers (the Split\u00adter \ntask and the parser tasks of definition module streams) are only started once their corresponding Lexor \ntasks have begun, and Lexor tasks never block, Thus, the possibility of deadlock is avoided. Our measurements \nshow that the delays due to workers waiting on barrier events are quite small in typical compilations, \nThe tasks and events of the different major versions of our compilers vary primarily in how DKY events \nare treated. In the Avoidance compiler, there is one DKY event per symbol table, and it is an avoided \nevent. This means that the tasks of a stream that depend on another stream s symbol table are not begun \nuntil that symbol table is complete. In the Pessimistic Handling compiler, there is again one DKY event \nper symbol table, but each is a handled event. Here, tasks are allowed to begin regardless of the completion \nstatus of the symbol tables on which they (may) depend, but must block as soon as they try to lookup \na symbol in an incomplete symbol table. These events are signaled when their corresponding symbol table \nis completed. A refinement of this strategy is presented in a Skeptical Handling compiler, where a searching \ntask is allowed to look into an incomplete symbol table and will block only if the symbol is not found. \nAs before, the blocked task is signaled when the symbol table is completed, and it attempts again to \nfind the symbol in the now complete table. The Optimistic Handling compiler implements a diffefient scheme. \nHere, there is one DKY event per symbol. In this method, when a task looks up a symbol, it waits on the \nsymbol s DKY event (should it be in the process of being defined), or creates an empty symbol with the \nnwme being searched for (if it isn t in the table) and waits on tlhat symbol s event, If the symbol is \nreally in that scope, the task is signaled and allowed to proceed. Otherwise, wh~en the table is completed, \nit is traversed and all unsignalled events (corresponding to symbols that are being looked up in the \ntable but are not defined in the scope) are signated, atlowing blocked tasks to continue searching, While \ntlhis method allows the maximum parallelism to be exploited, and indeed has the best self-relative speedup, \nthe overhead of maintaining so many events outweighs the advantages of the technique, 2.3.4 Worker Scheduling \nThe role of the Supervisor is to assign tasks to workers in such a manner as to minimize compilation \ntime. This is accomplished in two ways. First, free workers are {ls\u00adsigned tasks in an intelligent manner, \nattempting to select the tasks whose execution will benefit the most tasks. Sec\u00adond, handled events are \nimplemented in such a way that allows workers whose tasks are blocked on handled events perform other \ntasks. Task assignment is optimized in two ways. First, avoided events prevent a task that will have \nto wait almost immed\u00adiately from being selected for execution. Also, the Super\u00advisor implements the list \nof tasks waiting to be executed as a priority queue. In the Skeptical Handling compiler, the queues are \nsearched for new work in the following way: 1. Lexor tasks 2. Splitter task 3. Importer tasks 4. Definition \nModule Parser/Declarations Analyzer taslks  5. Module Parser/Declarations Analyzer task 6, Procedure \nParser/Declarations Analyzer tasks 7. Long Procedure Statement Analyzer/Code Generator tasks 8. Short \nProcedure Statement Analyzer/Code Generator tasks  Code is generated for long procedures before short \nones to avoid a long sequential tail at the end of the compilation, as one worker struggles to generate \ncode for one long proce\u00ad dure after finishing a number of short ones and atl the other workers are finished, \nWhen a task waits on a handled event, its associated worker does not simply block but instead is assigned \nanother task that it is eligible to execute, This task is always the one that signats the event on which \nthe worker is waiting, unless that task has been started by another worker. This leads to an interesting \nproblem, A task that is begun by a particular worker must be completed by that worker, even if it blocks \nduring execution. This is required by the underlying implementation of threads. Thus, the set of tasks \na blocked worker is eligible to execute consists of only those tasks that do not block on any event not \nyet signated by some task in the set of tasks being executed the worker (who are all currently blocked), \notherwise deadlock may occur. 2.4 Inter-scope Information Flows The implementation of block structured \nprogramming lan\u00adguages requires that information in the compilers symbol ta\u00adble be shared among different \nscopes. By processing scopes concurrently we introduce constraints on this sharing that do not exist \nin a sequential compiler. Delays waiting for this shared information can constitute a major impediment \nto concurrent processing, An example of an information flow constraint arises in the processing of procedure \nheadings in Modula-2+. A pro\u00adcedure heading contains the procedure s name, the name and specification \nfor each of the procedure s formal param\u00ad eters. The information in the procedure header needs to be \nshared between the parent scope in which the procedure is declared where it is used to validate and generate \ncode for calls on the procedure and the child scope (the body of the procedure) where it is used to \nvalidate and generate code for access to the procedure parameters, In sequential com\u00adpilers the sharing \nof this information occurs automatically given the usual order of program processing and symbol table \nconstruction, The information that is shared consists of the compiler symbol table entry for the procedure \nand the entries for its parameters. We investigated three alternatives for implementing this particular \ncase of sharing in our concurrent compiler: 1. Process the procedure heading in the parent scope and \ncopy the symbol table entries generated by this pro\u00adcessing into the symbol table for the child scope. \n 2. Process the procedure heading in the child scope and copy the symbol table entries produced into \nthe parent scope. 3. Process the parameter heading separately in the parent and child scopes taking \ncare to guarantee that identical symbol table entries are produced in both scopes,  In the first alternative, \nprocessing of the child scope should be delayed until the procedure header has been processed in the \nparent scope. This synchronization is in effect a DKY blockage from the child scope to its parameter \ndefinitions in the parent scope. Since this DKY is inevitable and since very little work can be done \non a procedure stream until the procedure s parameters have been processed, we found it more efficient \nto change this particular instance of a handled DKY event into an avoided event by delaying processing \nthe child scope until the parent scope had completely processed the procedure heading2. The second alternative \nis very dif\u00adficult to implement without introducing the possibility of a parent/child deadlock. The third \nalternative eliminates the information sharing between parent and child scopes. It is the cleanest solution, \nhowever in our evaluation it was about 3% slower than the first alternative due to redundant effort. \nProcedures with no parameterscould be startedearlier as a special case,butwe foundthemtobesoinfrequentthattheextraeffortwasnot \njustified 3 Compiler Structure The unit of compilation is a Modula-2+ module (M) that is physically \nrepresented as a definition module file (M.def) and an implementation module file (M.mod)3. The file \nM.def contains declarations for the interface between M and its clients. The file M.mod contains declarations \nfor constants, variables, types and procedures that are the im\u00adplementation of M. The task structure \nof our concurrent compiler is shown in Figure 5 The three columns in the figure show the compiler tasks \nthat are applied to the streams for definition modules, implementation modules4 and procedures respectively. \nThe tasks in the left column are applied to each definition module that is imported directly or indirectly \nby the module. The tasks in the right column are applied to each procedure in the module. We have used \nan unorthodox task division in the middle and back part of the compiler as a part of our strat\u00adegy for \ndealing with DKYs. One compiler task performs syntax analysis on the entire stream and semantic analysis \non declarations as would be done in a traditional sequential compiler. A parse tree is built for statements, \nbut semantic analysis of statements is deferred to a subsequent task that performs semantic analysis \non statements and then generates code for the statements. The symbol table for the declara\u00adtions is marked \ncomplete before the statment parse tree is built. Building the statement parse tree is sufficiently fast \nthat the added complexity of parsing statments separately was not considered. The rationale behind this \ndivision is that fast processing of the declaration parts of streams will assist in resolving DKY blockages \nby causing symbol ta\u00adbles to be completed earlier in the compilation. By the time the statement analysis \nand code generation tasks are ready to run there is usually many more parallel tasks available to run \nthan there are processors on which to run them, so we in\u00ad cur no loss in processing efficiency by combining \nstatement semantic analysis with code generation in a single task. The compilation of module M begins \nwith the lexical analysis of the file M.mod. The compiler optimistically anticipates the existence of \na file M.def and tries to start 3Weignoretherelativelyrareself containedprogrammodule 4theimporttaskfor \ntheimplementationmoduleisshownintbeleft columnfor convenience. processing this file as soon as possible. \nThe token stream produced by the lexical analysis phase is directed to the splitter and import tasks. \nThe import task searches the token stream for IMPORT declarations and starts anew stream for each imported \ndefinition module that it discovers, It starts a lexical analysis, importer, syntax and semantic analysis \nsequence for each new definition module stream. The token stream produced by the lexical analysis of \neach imported definition module is also fed into its import task to detect indirectly imported interfaces. \nA once-only table is used to guarantee that each definition module referenced in a compilation is processed \nexactly once. The splitter task searches for the reserved word PRO-CEDURE in the token stream of M.mod. \nIt creates a new stream for each procedure it detects and diverts the lexical tokens for the procedure \nto that stream. It then starts a syn\u00adtax analysis, semantic analysis, code generation sequence for that \nstream. The main module body which has now been striped of all embedded streams is processed through \nsyn\u00adtax analysis, semantic analysis and code generation, At the end of compilation, a merge task concatenates \nthe output of separate code generation streams to form the complete compiler result, Because the unit \nof merging is the code for an entire procedure, this concatenation can be done in any order and concurrently \nwith other compiler activity. 4 Compiler Performance We have conducted numerous experiments to evaluate \ntlhe performance of our prototype concurrent compilers. In this section we summarize the results of those \nexperiments, 4.1 Test Environment The concurrent compiler was written in Modula-2+. The compiler runs \non DEC Firefly prototype workstations [TSS88] under the Topaz operating system [MS87]. The Firefly used \nto obtain the results presented here was con\u00adfigured with 64 Mb of main memory and eight CWX pro\u00adcessors. \nThe compiler perfommnce results presented in this section should be view in the context of the known \nperfor\u00admance limitations of the Firefly hardware. At high levels of concurrent activity, memory bus saturation \neffects and fixed processor priorities for access to memory degrade the performance of all processors \n[TSS88]. Topaz provides a lightweight threads mechanism within a single address space. This mechanism \nwas used extensively within the compiler. The suite of 37 programs used to evaluate our compiler were \ntaken from a very large library of Modula-2+ soft\u00adware that was made available to us by the DEC Systems \nResearch Center. The programs in the test suite represent the efforts of a large number of authors and \ninclude a variety of programming styles. The programs in the test suite were chosen at the start of the \nCCD project to be a representative sample of the load that might be seen by a typical compiler. The programs \nin the test suite range from very small to very large. The gross characteristics of the test suite are \ngiven in Table 1. In this table imported Interfaces are the number if definition modules imported directly \nor indirectly by the module being compiled. Import Nesting Depth is the max\u00adimum length of the import \nnesting chain for each module. More details on the test suite are available in [SW91, JW90], 7 Attribute \nMinimum Median Maximum Module size (bytes) 2,371 13,180 336,312 Seq. Compile Time (see) 2.30 10.27 107,85 \nImported Interfaces 4 17 133 Import Nesting Depth 1 5 12 Number of Procedures 2 16 221 Number of Streams \n15 37 315 Table 1: Description of Test Suite  4.2 Speedup Results To evaluate the improvement in compilation \ntime that we were able to achieve through concurrent processing, we evaluated our concurrent compiler \nagainst a traditional se\u00adquential compiler for Modula-2+ and against itself. All test runs were made \non an otherwise idle Firefly workstation. The results presented are the average of 10 runs, The run\u00adto-run \nvariation in our timing results was very small, on the order of 1VO. Running on one processor, the concurrent \ncom\u00adpiler was 4.3% slower than the sequential compiler. This is due to the extra processing that was \nintroduced to achieve concurrency which is wasted on a single processor. Figure 1 shows the self-relative \nspeedup of our compiler as it was run on 1 through 8 processors. To test the limits of our (10 programs), \n5..10 seconds (8 programs), 10..30 seconds technique, we mechanically generated a synthetic module (Synth.mod) \nthat would have the best possible speedup for our technique, This module has been constructed so that \nit generates ample parallel work for the compiler and never in\u00adcurs a DKY blockage. The speedup results \nfor the synthetic module are shown in Figure 2. For comparison we have in\u00adcluded a reference line for \nlinear speedup and results for the human-authored module that had the best overall speedup in the test \nsuite. 6\u00ad mean Self 5\u00ad~ - ----max ........ min / . ,. / <--\u00ad. Relative ~ - / . / Speedup2 _ ,,+..,.................................... \n1\u00ad 1 I I I I I I I 1 2345678 Number of Processors Figure 1: Test Suite Self Relative Speedup 8 7 ........ \nVM.mod Self 6 linear 5 Relative d I Speedup 3 2 11 I II IIIIII 1 2345678 Number of Processors Figure \n2: Best Case Self Relative Speedup To get an indication of how our techniques scale as a function of \nthe size of module being compiled, we divided the programs in the test suite into approximate quartiles \nbased on their l-processor compilation times: 0..5 seconds 5Table3conminstbedatausedtogenerateFlgUres3,4~d \n5 (10 programs) and 30..109 seconds (9 programs). Figure 3 shows the self-relative speedup data plotted \nseparately for the programs in each quartile. From these plots we can see that the speedup obtainable \nthrough concurrent processing is limited for small programs, but increases as the size of program being \ncompiled increases. For programs with a se\u00adquential compile time of less than 10 seconds, a speedup of \n2.5 is obtained with 4 processors and there is no real advan\u00adtage in using more processors. It is evident \nfrom the first two compilations in Figure 6 that small modules don t generate enough parallel work to \nfully utilize the multiprocessor. For larger programs (sequential compile time greater than 10 seconds), \nthe speedup continues to increase although at a decreasing rate as more processors are used. We attribute \nthis to the greater amount of parallel work available in these programs. As a graphical illustration \nof our compilers oper\u00adation, we present the WatchTool snapshots in Figure 4. This figure shows processor \nactivity (vertical axis) as a function of time (horizontal axis) as a program from each of the quar\u00adtiles \nwas compiled on an eight processor Firefly. There is a ten second delay between compilations. For comparison \npurposes, the rightmost peak in this figure is a compilation of the synthetic module that was described \nabove. 6 . ... Q1 Self 5 ~ ---\u00ad  Q2 Q3 Q4 Relative ~ 1 Speedup 2 -\u00ad ......................,,.,,,,. ... \n 1 i 1 I I I I IIII 1 2345678  Number of Processors Figure 3: Speedup by Quaxtiles  4.3 Analysis of \nSkeptical Handling The skeptical handling symbol table lookup mechanism de\u00adscribed in Section 2.2 is \nan attempt to gain performance by Figure 4: WatchTool Snapshot searching incomplete symbol tables. To \nmeasure the effec\u00adtiveness of this strategy, we gathered performance data for this mechanism for one \ncompilation of our test suite. The lookup mechanism is applied in two circumstances: sermch for a simple \nidentifier occurring in a procedure or the ma~in module, and search for a qualified identifier (i.e. \nan identi\u00adfier explicitly associated with an imported definition mod\u00adule). The statistics for identifier \nsearch are given in Table 2. The Found when column indicates the conditions un\u00adder which the identifier \nwas found. Search indicates that the identifier was found during an outward search throuigh the scope \nparentage chain. After DKY indicates that tlhe identifier was found in a scope that was completed after \na DKY blockage. The scope column indicates the scope in which the identifier was found. Self is the scope \nof tlhe stream that initiated the search, other is some other explic\u00aditly designated initial search scope, \nouter is a scope thlat was accessed chaining outward through the scope parerLt\u00adage chain, WITH is the \nscope of a WITH statement and Builtin is the scope containing all identifiers predefincd by the compiler. \nThe completeness column indicates that state of the scope at the start of the search. The last two columns \ngiven the number and percentage of lookups that succeeded for the particular combination of Found, scope \nand completeness. The interesting cases in this table are the searches that found an identifier in an \nincomplete ta\u00adble. This is the case where Skeptical Hanclling has an edge over more conservative strategies. \nThe higher frequency (of search success in incomplete tables and DKY blockages in the case of qualified \nnames can be attributed to to the higher usage of qualified names in declarations. The majority of simple \nidentifier references arise from the statements in the bodies of procedures. By the time these are processed \nthere is a high probability that the symbol tables for outer scopes and definition modules have been \ncompleted. This table also shows that blockage due to the DKY conditicm is relatively rare. The low frequency \nof DKY blockages is one reason why the choice of a DKY strategy has only a small effect on overall compiler \nperformance.  4.4 An Activity View of Concurrent Compi\u00adlation In the early part of a compilation the \ndominant activity is the lexical, syntax and semantic analysis of definition modules. The definition \nmodules imported directly and indirectly by the main module form a tree. The need to resolve DKY blockages \nquickly and the task scheduling strategy used by our scheduler typically causes this tree to be processed \nin a bottom up order. At this point, the streams corresponding to the procedures in the main module have \nbeen identified by the splitter but processing of these streams is deferred until the corresponding procedure \nheadings have been pro\u00adcessed in the main module stream. Once processing of the main module stream reaches \nthe procedure headers, a large amount of parallel work becomes available as processing of many procedure \nstreams can be initiated. From this point on a high level of concurrent processing is possible through \nto the end of the compilation. Figure 7 illustrates the compiler activities in a typical concurrent compilation. \nThe horizontal axis is time, the vertical axis is processor number for an eight processor Firefly. The \nbars in this figure indicate different kinds of compiler activity. The dark gray bars at the left side \nare lexical analysis tasks. The light gray bars in the middle are parser/declaration semantic analysis \ntasks. The darker gray bars on the right side of the figure represent statement am\u00adysis/code generation \ntasks. The light gray bar for processor 7 is the splitter task. At the scale of this figure,, the importer \nand merge tasks are too small to be easily visible. The ac\u00adtivity lull in the center of this figure is \ncaused by delays waiting for DKYs to be resolved and by the delay involved in waiting for procedure headers \nto be processed in the main module body as described in Section 2.4. 77 Figure 4: WatchTool Snapshot \nsearching incomplete symbol tables. To measure the effec\u00adtiveness of this strategy, we gathered performance \ndata for this mechanism for one compilation of our test suite. The lookup mechanism is applied in two \ncircumstances: sermch for a simple identifier occurring in a procedure or the ma~in module, and search \nfor a qualified identifier (i.e. an identi\u00adfier explicitly associated with an imported definition mod\u00adule). \nThe statistics for identifier search are given in Table 2. The Found when column indicates the conditions \nun\u00adder which the identifier was found. Search indicates that the identifier was found during an outward \nsearch throuigh the scope parentage chain. After DKY indicates that tlhe identifier was found in a scope \nthat was completed after a DKY blockage. The scope column indicates the scope in which the identifier \nwas found. Self is the scope of tlhe stream that initiated the search, other is some other explic\u00aditly \ndesignated initial search scope, outer is a scope thlat was accessed chaining outward through the scope \nparerLt\u00adage chain, WITH is the scope of a WITH statement and Builtin is the scope containing all identifiers \npredefincd by the compiler. The completeness column indicates that state of the scope at the start of \nthe search. The last two columns given the number and percentage of lookups that succeeded for the particular \ncombination of Found, scope and completeness. The interesting cases in this table are the searches that \nfound an identifier in an incomplete ta\u00adble. This is the case where Skeptical Hanclling has an edge over \nmore conservative strategies. The higher frequency (of search success in incomplete tables and DKY blockages \nin the case of qualified names can be attributed to to the higher usage of qualified names in declarations. \nThe majority of simple identifier references arise from the statements in the bodies of procedures. By \nthe time these are processed there is a high probability that the symbol tables for outer scopes and \ndefinition modules have been completed. This table also shows that blockage due to the DKY conditicm \nis relatively rare. The low frequency of DKY blockages is one reason why the choice of a DKY strategy \nhas only a small effect on overall compiler performance.  4.4 An Activity View of Concurrent Compi\u00adlation \nIn the early part of a compilation the dominant activity is the lexical, syntax and semantic analysis \nof definition modules. The definition modules imported directly and indirectly by the main module form \na tree. The need to resolve DKY blockages quickly and the task scheduling strategy used by our scheduler \ntypically causes this tree to be processed in a bottom up order. At this point, the streams corresponding \nto the procedures in the main module have been identified by the splitter but processing of these streams \nis deferred until the corresponding procedure headings have been pro\u00adcessed in the main module stream. \nOnce processing of the main module stream reaches the procedure headers, a large amount of parallel work \nbecomes available as processing of many procedure streams can be initiated. From this point on a high \nlevel of concurrent processing is possible through to the end of the compilation. Figure 7 illustrates \nthe compiler activities in a typical concurrent compilation. The horizontal axis is time, the vertical \naxis is processor number for an eight processor Firefly. The bars in this figure indicate different kinds \nof compiler activity. The dark gray bars at the left side are lexical analysis tasks. The light gray \nbars in the middle are parser/declaration semantic analysis tasks. The darker gray bars on the right \nside of the figure represent statement am\u00adysis/code generation tasks. The light gray bar for processor \n7 is the splitter task. At the scale of this figure,, the importer and merge tasks are too small to be \neasily visible. The ac\u00adtivity lull in the center of this figure is caused by delays waiting for DKYs \nto be resolved and by the delay involved in waiting for procedure headers to be processed in the main \nmodule body as described in Section 2.4. 77 c\u00ad  ~ !r 4 I LEXICAL I IMPORT I ANALYSIS m [i LEXICAL \nANALYSIS m  # * + SYNTAX SYNTAX I ANALYSIS SYNTAX ANALYSIS ANALYSIS I  --. ----- DCL. DCL. DCL. SEMANTIC \nSEMANTICSEMANTIC ANALYSIS ANALYSIS L, STATEMENT STATEMENT SEMANTIC SEMANTIC ANALYSIS ANALYSIS . ----------. \n------------ CC)DE CODE GENERATOR GENERATOI ~ I Definition Implementation Procedure Module Module Figure5: \nCompiler Task Structure Found when ~ First try other complete 5,45 7 6,01 Search outer incomplete 3,228 \n3.55 First try incomplete 505 4.00 Search outer complete 12,87!) 14.18 First try complete 11,775 93.30 \n6!) AfterDKY outer complete 0.08 After DKY complete 341 2.70 First Try WITH complete 2,69!1 2.97 First \nTry Builtin complete 13,744 15.14 184 Never L 0.20 Table2: IdentifierLooku pStatistics 79 Min Mean Max \nSynth VM Q1 Q> 2 1,42 1.81 1.91 1.99 1.81 1.68 1.70 3 1.80 2.49 2.62 2.85 2.62 2.13 2.2 ? 4 1.91 3.07 \n3.43 3.57 3.43 2.34 2.57 5 1,94 3.58 4.07 4.26 4.07 2.38 2.76 6 1,99 3.93 4.67 5.18 4.67 2.42 2.85 7 \n1,86 4.20 5.29 6.01 5.29 2.36 2.91 8 1.95 4,34 5.47 6.67 5.32 2.43 2.89  r IN II Test Suite II BestCase \nII Chmrtiles Q3 1.81 2.50 3.05 3.56 3.88 4.03 4.19 -@\u00ad 1.84 2.55 3.24 3.88 4.37 4.83 5.02 Table 3: Summary \nof Speedup Data Record the completion state of the scope s symbol table Search the scope s symbol table \nfor the identifier. If identifier was found then exit with success. If the symbol table was initially \nincomplete then Wait for the symbol table to be completed. Search the scope s symbol table for the identifier. \nIf identifier was found then exit with success. If current scope is outermost scope then exit with failure \nelse Continue search in next outermost scope Figure6: Skeptical Handling Symbol TableLookup 8 H!!! \\ \n7 6 m 5 ,. 3 I 2 1 t!, ,. ,,\\ ,, low e 2f300 Ti]me (clock ticks) 3C)O0 4cko Lexical Import Analysis n \nSplitter 1 Parse/Del Analysis D Stint Processing Merge Figure 7: Ccmcurrent Compiler Processor Activity \n \n\t\t\t", "proc_id": "143095", "abstract": "<p>In this paper we describe a collection of techniques for the design and implementation of concurrent compilers. We begin by describing a technique for dividing a source program into many <italic>streams</italic> so that each stream can be compiled concurrently. We discuss several compiler design issues unique to concurrent compilers including source program partitioning, symbol table management, compiler task scheduling and information flow constraints. The application of our techniques is illustrated by a complete design for a concurrent Modula-2+ compiler. After describing the structure of this compiler's performance that demonstrates that significant improvements in compilation time can be achieved through the use of concurrency.</p>", "authors": [{"name": "David B. Wortman", "author_profile_id": "81100379705", "affiliation": "Computer Systems Research Institute, University of Toronto, 6 Kings College Road, Toronto, Ontario, Canada M5S 1A4", "person_id": "PP14134332", "email_address": "", "orcid_id": ""}, {"name": "Michael D. Junkin", "author_profile_id": "81332507659", "affiliation": "IBM Canada Ltd., Toronto Laboratory, 844 Don Mills Rd., North York, Ontario, Canada M3C 1V4 and Computer Systems Research Institute, University of Toronto, 6 Kings College Road, Toronto, Ontario, Canada M5S 1A4", "person_id": "P197543", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.120025", "year": "1992", "article_id": "120025", "conference": "PLDI", "title": "A concurrent compiler for Modula-2+", "url": "http://dl.acm.org/citation.cfm?id=120025"}