{"article_publication_date": "07-01-1992", "fulltext": "\n Avoiding Unconditional Jumps by Code Replication Frank Mueller and David B. Whalley Department of Computer \nScience, B-173 Florida State University Tallahassee, Florida 32306-4019 e-mail: whalleyQcs.fsu. edu Abstract \nThis study evaluates a global optimization technique that avoids unconditional jumps by replicating code. \nWhen implemented in the back-end of an optimizing compiler, this technique can be generalized to work \non almost all instances of unconditional jumps, including those generated from conditional statements \nand un\u00adstructured loops. The replication method is based on the idea of finding a replacement for each \nunconditional jump which minimizes the growth in code size, This is achieved by choosing the shortest \nsequence of instruc\u00adtions as a replacement. Measurements taken from a variety of programs showed that \nnot only the number of executed instructions decreased, but also that the total cache work was reduced \n(except for small caches) despite increases in code size. Pipelined and super\u00adscalar machines may also \nbenefit from an increase in the average basic block size. 1 Introduction Unconditional jumps occur often \nin programs. Depend\u00ad ing on the environment, execution frequencies between 4~o and 10~o have been reported \n[Pe77,C182]. Common programming constructs such as loops and conditional statements are translated to \nmachine code using un\u00ad conditional jumps, thus resulting in relatively compact code. Code size, however, \nhas become less important since the introduction of caches. For instance, inlining [Da88] and loop unrolling \n[He90] can obtain improve\u00ad ments while increasing the code size. This study describes a method of replacing \nuncondi\u00ad tional jumps uniformly by replicating a sequence of in- Permission to copy without fee all \nor part of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a faa andlor specific permission. ACM SIGPLAN 92 PLD1-6/92/CA e 1992 ACM 0-89791 \n-476-7 /92/0006 /0322 . ..$1 .50 structions from the jump destination. To perform this task, an algorithm \nis proposed which is based on the idea of following the shortest path within the control flow when searching \nfor a replication sequence. The effect of code replication is shown by capturing mea\u00adsurements from the \nexecution of a number of programs. The document is structured as follows. Section 2 gives an overview \nof research on related topics. Section 3 illustrates the advantages of using code replication for optimizing \nvarious programming constructs. Section 4 provides an informal description of the algorithms used to \nimplem ent code replication. Section 5 discusses the results of the implementation by comparing the \nmeasurements of numerous programs with and without code replication. Section 6 gives an overview of future \nwork and Section 7 summarizes the results,  2 Related Work Several optimization that attempt to improve \ncode by replicating instructions have been implemented. Loop unrolling [He90] replicates the body within \na loop. This reduces the number of compare and branch instructions that are executed. Also, more effective \nscheduling may be achieved for pipelined and multiple issue machines since some of the basic blocks comprising \nthe loop con\u00adtain more instructions after unrolling. Inlining, an optimization method studied by David\u00adson \nand Holler [Da88], results in replicated code when a routine is inlined from more than one call site. \nHwu and Chang [Hw89] used inlining techniques based on profiling data to limit the number of call site \nexpan\u00adsions and thereby avoid excessive growth. In general, a call to a non-recursive routine can be \nreplaced by the actual code of the routine body. The procedure call can be viewed as an unconditional \njump to the begin\u00adning of the body, and any return from the procedure can be viewed as an unconditional \njump back to the instruction following the call. Golumbic and Rainish [G090] used the method of = 1 \n1; while (i++<n) x[i-il = xCil; without replication with replication move 1 mto data reg. 1 (vartable \ni d[i]=i; d[O]=i; move addr. ofx[] into addr. reg. O a[O] =a[6] +x. ; d[i]=2; Li5 NZ=d[O] ?L[n] ; copy \ndata reg. 1 into data reg. O d[O]=d[i] ; PC=NZ>=0,L16; incr. indez into x[] a[o]=a[()]+i; a[O]=a[6]+x.+l; \nincr. data reg. 1 variablei) d[i]=d[i]+i; LOOO compare data reg. O wit 1 variable n NZ=d[O] ?L[n]; B[a[O]]=B[a[O]+l]; \nexit loop if greater/equal PC=NZ>=O,L16; a[Ol=a[Ol+i; move x[il into x[i-il B[a[O]]=B[a[O]+l] ; d[O]=d[i] \n; jump unconditionally to Li5 PC=L15; d[i]=d[i]+i; Li6 . . . NZ=d[01 ?L [n]; PC=NZ<O,LOOO; Li6 ... Table \n1: Exit Condition in the Middle of a Loop (RTLs for 68020) replicating parts of basic blocks for instruction \nschedul\u00ading to exploit potential parallelism for a super-scalar processor. In their approach, it suffices \nto copy the number of instructions needed to avoid a pipeline stall from the block following a conditional \nstatement. They expand natural loops similarly by replicating instruc\u00adtions from the top of the loop, \nnegating the branch condition, and inserting another unconditional jump. Their goal was to increase the \nnumber of instructions that can be issued simultaneously.  Mot ivation All replication techniques employed \nin the front-end of a compiler lack generality in reducing the number of unconditional jumps. Instances \nof unconditional jumps cannot always be detected due to interaction with other optimization and a lack \nof information about the tar\u00adget architecture. Consequently, front-end methods for code replication cannot \neliminate occurrences of uncon\u00additional jumps which are introduced by the optimiza\u00adtion phase of a compiler. \nThe optimization evaluated in this study, code repli\u00adcation, was accomplished by modifying the back-end \nof the optimizing compiler VPO (Very Portable Op\u00adtimizer) [Be88]. The algorithms to perform the opti\u00admization, \nexcept for a few small functions, are machine\u00adindependent. In general, RTLsl are searched for uncon\u00additional \njumps. By determining the jump destination and using control flow information, a subset of the basic \nblocks in the function can be replicated, replacing the unconditional jump. Such an optimization can \nbe ap\u00adplied to almost all occurrences of unconditional jumps. The following sections give examples of \ninst antes where 1Register Transfer Lists (RTLs) represent the effects of instructions of a target machine. \ncode replication can be applied to applications written in C. 3.1 Loops For while-loops, the front-end \nVPCC (Very Portable C Compiler) [Da89] generates intermediate code with an unconditional jump at the \nend of the loop. This un\u00adconditional transfer can be replaced by the instructions testing the termination \ncondition of the loop with the termination condition reversed. The intermediate code produced by the \nfront-end for for-loops with an unknown number of iterations in\u00adcludes an unconditional transfer of control \npreceding the loop to the instructions comprising the termina tion condition, a portion of code placed \nat the end of the loop. This unconditional jump can also be replaced by the code which tests for the \ninverse termination con\u00addition. Thus, the code checking the termination con\u00addition would appear before \nthe loop and at the end of the loop. Often, the replication of the termination condition of while and \nfor loops is performed by optimizing com\u00adpilers. But when the exit condition is placed in the middle \nof a loop, most compilers do not attempt a re\u00adplacement for the unconditional jump. An example for such \na situation is given in Table 1. Code replication is used to replace the unconditional jump by the RTLs \nbetween label L15 and the conditional branch. The conditional branch is reversed and a new loop header \nis int reduced at label LOOO. Afterwards, other opti\u00admization such as common sub expression elimination \nare applied. In this example, one unconditional jump per loop iteration is saved. The method proposed \nin this study handles these cases as well as unstructured loops, which are typically not recognized as \nloops by an optimizer. if (1>5) i= i/n; else i =1 *n; return(i); Without replication vlth replication \ncompare ~ and 5 NZ=L[a[6]+i.]?5; NZ=L[a[6]+i.]?5; branchifless/equal PC=NZ<=0,L22; PC=NZ<=0,L22; load \ni into data reg. O d[Ol=L[a[61+i.] ; d[O]=L[a[6]+i.] ; divide data reg. O by n d[O]=d[O]/L[a[6] +n.]; \nd[O]=d[O]/L[a[6] +n.]; store data reg. O into i L[a[6]+l. ]=d[O] ; L[a[6]+i. ]=d[O] ; jump unconditionally \nto L23 PC=L23 ; a [6] =UK; L22 PC=RT; load i into data reg. O d[O]=L[a[6]+l. ] ; L22 multiply data reg. \nO by n d [01 =d [01 *L [a[6] +n. 1 ; d[O]=L[a[6]+i. ] ; store data reg. O into i L[a[6]+i. ]=d[O] ; d[O] \n=d [01 *L [a[61 +n. ] ; L23 L[a[6]+i. ]=d[O] ; restore old frame pointer a [6]=UK; a [6]=UK; return from \nsubroutine PC=RT; PC=RT; Table 2: If-Then-Else Statement (RTLs for 68020) 3.2 Conditional Statements \nFor the if-then-else construct an unconditional jump is generated at the end of the then-part to jump \nover the else-part. This unconditional jump can also be elim\u00adinated by code replication. There are two \nexecution paths possible which are joined at the end of the if\u00adthen-else construct. The two execution \npaths can be separated completely or their joining can be at least deferred by replicating the code after \nthe if-then-else construct, so that the unconditional jump is replaced by the copied instructions. Table \n2 shows an example of replicating code where the two execution paths return from the function separately.z \nThe method of code replication used for conditional statements can also be applied to break and goto \nstatements, and conditional expressions in the C language (expr? expr: expr).  3.3 Sources for other \nOptimizations Code replication also creates new opportunities for global optimizations by modifying the \ncontrol flow of a function. The following paragraphs describe new op\u00adportunities for constant folding, \ninstruction selection, common sub expression elimination, and code mot ion. 3.3.1 Constant Folding of \nComparisons and Conditional Branches After applying code replication, sources for constant folding may \nbe introduced which did not exist before. For example, conditional branches, dependent on the comparison \nof two constants, may be introduced by ZNotice that nested if-then-else statements can cause code to \nbe replicated very often, thus resulting in an disproportional growth in code size relative to the original \ncode size. changing the control flow during code replication. De\u00adpending on the result of the constant \ncomparison, such a conditional branch can either be eliminated or re\u00adplaced by an unconditional jump. \nIn the later case, dead code elimination may remove instructions follow\u00ad ing the unconditional jump which \ncan no longer be reached. 3.3.2 Elimination of Instructions In conjunction with code replication, common \nsubex\u00adpression elimination can often combine instructions when an initial value is assigned to a register, \nfollowed by an unconditional jump. If the replication sequence uses the register, the use is replaced \nby the initial value so that the assignment to the register becomes redun\u00addant if there are no further \nuses or sets of the register. Similarly, instruction selection may combine instruc\u00adtions when the head \nof a loop is moved. For example, the second RTL in the replicated code of Table 1 is a simplification \nof the first and fifth RTLs in the code before replication.  3.3.3 Relocating the Preheader of Loops \nAfter code replication the execution of some instruc\u00adtions may be avoided for an execution path. For \nin\u00adstance, code motion is performed after an initial pass of code replication. This may result in a new \nlocation of the preheaders of loops. Thus, if a loop is not ex\u00adecuted because the conditional branch \npreceding the loop is taken, the instructions in the preheader (fol\u00adlowing that branch) would not be \nexecuted. This may result in considerable savings when loops are nested. An example is the RTL preceding \nlabel LOOO in the replicated code of Table 1 which is the loop preheader. Without Replication With Partial \nReplication With Loop Replication Figure 1: Interference with Natural Loops  4 JUMPS: An Algorithm \nfor Code Replication The task of code replication is to find a sequence of basic blocks to replace an \nunconditional jump. In or\u00adder to avoid algorithms with a high degree of complex\u00adity, it was decided to \nmake the initial assumption that only the shortest path between two basic blocks is ex\u00adamined. This constraint \nis motivated by the goal of limiting the size of code introduced by the replication process. The shortest \npath is determined with respect to the number of RTLs. Finding the shortest path in the control-flow \ngraph with n blocks is accomplished by using Warshall s algorithm for calculating the tran\u00adsitive closure \nof a graph ~a62] which has a complexity of 0(n3). First, all legal transitions between any two distinct \nbasic blocks are collected in a matrix, This initial pass creates a copy of the control flow graph but \nit excludes self-reflexive transitions and, optionally, other edges whose control flow is excluded explicitly. \nFor example, the replication of indirect jumps has not yet been implemented at this point. Then, the \nnon\u00adreflexive transitive closure is calculated for all nodes with respect to the shortest path. The transitivity \nre\u00adlation between two nodes is only recorded if it is the shortest connection found so far in terms of \nthe num\u00adber of RTLs in the traversed blocks [F162]. In the end, the matrix can be used to look up the \nshortest path between two arbitrary basic blocks in the table with\u00adout having to recalculate it after \neach replication. The algorithm JUMPS is divided into the following steps: 1. Initially, the matrix used \nto find the shortest se\u00adquence of basic blocks to replace an unconditional jump is set up. 2. In the \nsecond step, the basic blocks within a func\u00adtion are traversed sequentially and unconditional jumps are \nreplaced as follows. Either a sequence of blocks that ends with a return from the routine is replicated \n(~avoring returns), or a sequence of blocks is chosen linking the current block contain\u00ading the unconditional \njump with the block posi\u00adtionally following the unconditional jump (~avor\u00ading loops), In the latter case, \nthe last block to be replicated will fall through to the next block. At this point, heuristics can be \nused to make the choice between these two options. 3. If a collected block (i.e. a block chosen for \nreplica\u00ad  tion in the previous step) header of a natural loop previously was not inside blocks inside \nthis loop are was detected to be the and the block collected the same loop, then all included in the \nreplica\u00ad tion sequence in their positional order. The exam\u00ad ple in Figure 1 has an unconditional jump \nfrom block 2 to block 4 before replication. Without replicating block 6, the original loop would have \ntwo entry points and would be unstructured. 4. Once a sequence of basic blocks is replicated, the control \nflow is adjusted accordingly. A conditional branch is reversed in the replicated path if the path does \nnot follow the fall-through transition. New labels are introduced, and the destinations of conditional \nbranches are modified. In addition, all , \\ Initial Control Flow After Replication Adjusted Control \nFlow Figure 2: Partial Overlapping of Natural Loops unconditional jumps that are found in the repli-cation. \ncated code can he eliminated since the sequence of replicated blocks had to follow the control flow \nThe algorithm JUMPS is applied to a function for with fall-through transitions, each unconditional jump \nuntil no more unconditional jumps can be replaced. As a result of the replication5.The adjustment of \nthe control flow is extended, process, blocks which cannot be reached by the control once again to preserve \nthe structure of loops. flow anymore can sometimes occur. Therefore, dead When a replication is initiated \nfrom a block inside code elimination is invoked to delete these blocks. a loop, a portion of the loop \ncan be copied without introducing unnatural loops. In addition, the con\u00ad trol flow of all blocks in the \nloop which were not  5 Measurements copied but branch conditionally to a block which was copied, is \nchanged to the copied block. If Static and dynamic frequency measurements and in\u00ada block occurs twice \nin the replication sequence, struction cache performance measurements were taken forward branches (positionally \ndown) are favored from a number of well-known benchmarks, UNIX util\u00adover branches back to a previous \nblock. These ities, and one application (see Table 3). The code was modifications are needed to avoid \nthe introduct\u00adgenerated for the Motorola 68020/68881 processor and ion of natural loops which partially \noverlap and would complicate standard loop optimizations. An class Name Descriptionexample is given \nin Figure 2. There is an uncondi\u00adbanner banner generatortional transfer from block 3 to block 1 in the \ninitial cal calendar generatorcontrol flow. By changing the target of the condi\u00adcompact file compression \ntional branch in block 2 after replication, the intro\u00adderoff remove nroff constructsduction of partially \noverlapping loops is avoided in Utilities grep pattern searchthe adjusted control flow. od octal dump \n 6.Even with replication of entire natural loops, it is sort sort or merge files still possible for this \nalgorithm to introduce new Wc word count loops which are unstructured. Therefore, the con-bubblesort \nsort numbers trol flow graph is checked to determine if it is matmult matrix multiplication still reducible. \nThe replicated code is removed Benchmarks sieve iteration if it introduced a non-reducible flow graph. \nIn queens 8-queens problem this case, replication may be attempted using the quicksort sort numbers (iterative) \nlonger block sequence (see step 2). Reducibility User code mincost VLSI circuit partitioning has to be \npreserved to ensure that the same opti\u00admization can be applied to loops as wit bout repli-Table 3: Test \nSet of C Programs the Sun SPARC processor, a RISC architecture. For the SPARC processor, delay slots \nafter transfers of con\u00adtrol were filled, The standard code optimization tech\u00adniques such as branch chaining, \ninstruction selection, register coloring, common sub expression elimination, constant folding, code motion, \nstrength reduction, and constant folding at conditional branches were applied on the measured code, Library \nroutines could not be measured since the source code was not available to be compiled by VPO. Sun SPARC \nMotorola 68020 average std. deviation average std. deviation SIMPLE 3.74% 1.78% 5.08% 2.49% static LOOPS \n2.40% 1.9970 3.42% 2.83% JUMPS SIMPLE 0.03% 3.28% 0.12% 2.71% 0.04% 4,1470 0.15% 3.48~o dynamic LOOPS \n1.89% 2.56% 2.47% 3.36~o JUMPS 0.10% 0.30 %0 0.1370 0.43% Table 4: Percent of Instructions that are Unconditional \nJumps The measurements were collected by using EASE (Environment for Architectural Study and Experimen\u00adtation) \n[Da90-2] which is designed to measure the code produced by the optimizer VPO. When generating code, additional \ninstructions are inserted to capture measurements during the execution of a program. Each program was \ntested with three different sets of optimizations: . SIMPLE: Only the standard optimizations were performed, \n . LOOPS: Unconditional jumps preceding a loop or at the end of the loop are replaced by the ter\u00admination \ncondition of the loop and the replicated condition is reversed. Depending on the original layout of the \nloop, either one unconditional jump is removed at the entry point, or one unconditional jump is saved \nper loop iteration. This optimiza\u00adtion is often implemented in conventional optimiz\u00aders. . JUMPS: This \nis the algorithm discussed previ\u00adously. It is a generalized approach which attempts to replace any occurrences \nof unconditional jumps by replicating code.  5.1 Integration into an Optimizing Compiler The code replication \nalgorithms (J IJMPS and LOOPS) are integrated into the optimizing back-end of the VPO compiler in the \nfollowing manner. After performing initial branch optimizations such as branch chaining, code replication \nis performed to reduce the remaining number of unconditional jumps. When JUMPS is used for code replication, \nthe compile-time overhead for the replication process itself is minimal, but the following optimization \nstages process more RTLs. The impact of LOOPS on the compile time is minimal. Figure 3 summarizes the \norder in which the differ\u00adent optimization phases are invoked. Code replication is performed at an early \nstage so that the later op\u00adtimization can take advantage of the simplified con\u00adtrol flow. In order to \nreplace all unconditional jumps generated by constant folding at conditional branches or introduced by \nremote preheaders, code replication is reinvoked repeatedly. The final invocation of code replication \nreplaces those unconditional jumps which remained in the code because replication would have resulted \nin a non-reducible flow graph. branch chaining; dead code elimination; reorder basic blocks to minimize \njumps; code replication (either JUMPS or LOOPS); dead code elimination; instruction selection; register \nassignment; if (change) instruction selection; do { register allocation by register coloring; instruction \nselection; common subexpression elimination; dead variable elimination; code motion; strength reduction; \nrecurrences; instruct ion select ion; branch chaining; constant folding at conditional branches; code \nreplication (either JUMPS or LOOPS); dead code elimination; } while (change); filling of delay slots \nfor RISCS; Figure 3: Order of Optimizations  5.2 Static and Dynamic Behavior Table 4 shows the number \nof unconditional jumps rel\u00adative to the total number of instructions for the static and dynamic measurements. \nThe number of uncondi\u00adtional jumps is reduced by 40-42!%0 dynamically when LOOPS was applied, and with \ncode replication prac\u00adtically no unconditional jumps are left, Thus, code replication results in a reduction \nof instructions exe\u00adcuted by at least the number of unconditional jumps which could be avoided dynamically. \nThe few unconditional jumps left after code repli\u00adcation are due to indirect jumps, infinite loops, and \ninteractions with other optimization phases (such as code motion) that may introduce unconditional jumps. \nPaths containing indirect jumps are excluded from replication in the current implementation. Infinite \nloops do not provide any opportunity to replace the unconditional branch. And interactions with other \nop\u00adtimization phases are treated conservatively to avoid the potential of replication ad znfiniiunz. \nTable 5 illustrates the static and dynamic behavior of the programs. The columns SIMPLE indicate the \ntotal number of instructions and the other columns represent the change in the number of instructions \nrelative to the SIMPLE version of each program. The static change is proportional to the growth of the \ncode size. When LOOPS is applied, the number of instructions increases by only 2.6-4,0%. With generalized \ncode replication, on the other hand, an average of about 53910more instruc\u00ad tions are generated. The \ndecrease in the number of instructions executed for LOOPS is les s than half the decrease for JUMPS, \n For the SPARC about 1.5 more instructions are found between branches after code replication was ap\u00adplied \nand 5090 of the executed no-op instructions were eliminated. Thus, the opportunities for instruction \nscheduling may improve if code replication is applied for a pipelined machine. Also, in a multiple-issue \npro\u00adcessor more potential parallelism may be found [Ri72].  5.3 Impact on Instruction Caching The cache \nperformance was tested for cache sizes of lKb, 2Kb, 4Kb, and 8Kb. For each different cache size a direct-mapped \ncache with 16 bytes per line was sim\u00adulated. Both the miss ratio and the fetch cost were measured in \nthe experiment. The estimation of the fetch cost is based on the assumption that misses are ten times \nas expensive as hits. Thus, fetch cost is cal\u00adculated as follows: fetch cost = cache hits * cache access \ntime + cache misses * miss penalty where the cache access time is I time unit and the miss penalty is \n10 units of time. Context-switches were simulated by invalidating the entire cache every 10,000 units \nof time. The estimates for the cache access time, the miss penalty, and the context-switching interval \nwere adopted from Smith s cache studies [Sm82]. No\u00adtice that the overall fetch cost can decrease while \nthe miss ratio increases for the same program, This can be explained by the reduced number of instructions \nexe\u00adcuted after replication and illustrates the short-comings of the miss ratio as a measurement when \nthe code in a program changes. Table 6 shows the change of the miss ratio and fetch cost for varying \nsizes of dir-ect-mapped caches. Each set of measurements with the same configuration is related to the \ncorresponding values of the SIMPLE version. For example, for a lKb cache with context switches on the \nSPARC, the difference between the miss ratio of LOOPS and the miss ratio of the SIMPLE version was -0.05910, \na slight decrease of misses. The impact of context switching was minimal, and the miss ratio only increased \nslightly with context switching on. For small caches, code replication (JUMPS) may be outperformed by \nloop replication (LOOPS). A program may initially fit in the cache, but after code replication is applied, \nit might not fit anymore. Therefore, capac\u00adity misses can be introduced. For example, for a lKb cache \nabout 1~0 additional misses were caused by in\u00adstruction fetches. But for larger caches the miss ratio \nchanges only slightly. Code replication places instructions together which are likely to be executed \nin a sequence but increases the distance between conditional branch instructions and their branch destinations. \nNevertheless, the program s spatial locality can be improved by replicating code. Overall, code replication \nreduces the total number of instructions executed such that the average fetch cost is actually reduced \nexcept for small caches.  6 Future Work The algorithm for code replication could be extended to copy \nindirect jumps and, for some architectures, their jump tables. If the table has to be replicated, the \ntar\u00adget addresses within the jump table should be revised. In either case, the jump destinations do not \nneed to be copied. Thus, an indirect jump could terminate a replication sequence and provide yet another \nalter\u00adnative besides replication paths favoring returns and favoring loops (see step 2 of algorithm JUMPS). \nFurthermore, the increase in code size could be re\u00adduced by limiting the maximum length of a replication \nsequence to a specified number of RTLs. The improve\u00adments in the dynamic behavior of programs may drop \nslightly for this case while the performance of small caches should benefit. Sun SPARC 4 program static \ninstructions dynamic instructions executed SIMPLE LOOPS JUMPS SIMPLE LOOPS JUMPS cal 338 +3.25% +21.89% \n37,237 2.95% 3.15% quicksort 321 +5.61% +50.16% 836,404 2.86% 14.21% Wc 209 +0.96% +58.37% 540,158 -0.00% \n-1.96 %0 grep 968 +4.24% +79.34% 1,930,791 -0.04% 3.57% sort 1,966 +4.63% +89.17% 1,181,960 0.71% 10.49% \nod 1,352 +4.59% +95.19% 2,336,014 8.84% 10.22% mincost 1,068 +6.84% +30.99% 335,750 0.59Y0 3.91% bubblesort \n175 +7.43% +5.14% 29,071,668 0.05% 0.0770 matmult 218 +4.59% +3.67% 14,403,714 0.08 %0 0.28~o banner \n169 +7.69% +66.27% 2,565 1.68% 10.25~o sieve 93 +3.23% +3.23% 2,184,965 13.73% 13.73~o compact 1,491 \n+1.07% +75.18% 13,409,945 1.94% 4.86%  queens 114 +0.00% +7.89% 263,518 0.00% 0.03% deroff 7,987 +1.50% \n+204.98% 448,581 0.01% 3.13% average 1,176 +3.97% +56.53% 4,784,519 2.39% 5.71% Motorola 68020 program \nstatic instructions dynamic instructions executed SIMPLE LOOPS JUMPS SIMPLE LOOPS JUMPS cal 323 +3.72% \n+24.77% 36,290 3.09% 3.17% quicksort 245 +3.67% +37.96% 536,566 0.39% 3.96% Wc 173 +0.58% +56.65% 421,038 \n0.00% 5.32% grep 775 +3.35Y0 +80.90% 1,309,586 0,03% 3.44% sort 1,558 +3.98% +63.67% 902,075 1.49% 12.43% \nod 1,198 +2,92% +85.73% 1,980,808 9.45% 10.30% mincost 906 +3.20 % +35.98% 302,062 1.10% 5.13% bubblesort \n137 +3.65% +2.92% 20,340,231 18.92% 18.92% matmult 146 +3.42% +3.42% 4,891,507 0.21% 0.21% banner 177 \n+3.95% +55.93Y0 2,473 1.42% 13.34% sieve 70 +1.43% +1,43% 1,759,088 8.53% 8.53% compact 1,143 +0.70% \n+73.93% 10,602,159 1.54% 5.26% queens 94 +0.00% +12.77% 189,518 0.00% 0.05% deroff 5,730 +1.06% +155.17% \n360,051 0.03% -7.05% average 905 +2.55% +49.37% 3,116,675 3.30% 6.94% Table 5: Number of Static and Dynamic \nInstructions cache size lKb 2Kb 4KB 8Kb I I processor Icontext sw. LOOPS JUMPS ILOOPS JUMPS\\ LOOPS JUMPS \nILOOPS JUMPS Cache Miss Ratio Sun on 0.05% +1.07% 0.22% 0.07% +0.03% +0.25% +0.01% +0.11% SPARC off 0.03% \n+1.07% 0.22% 0.08% +0.03% +0.21% +0.0170 +0.07% ~ Motorola on +0.08% +1.26% +0.04% +0.75% +0.01% +0.09 \n% +0.OIYO +0.07% 68020 off +0.08% +1.25% +0.03% +0.70% +0.01% +0.05% +0.01% +0.03% Instruction Fetch \nCost sun on _273% +3,44% -3.80% -5.24% -2.26% 2.94% 2.40% 3.98% SPARC off -2.64% +3.68% 3.87% -5.33% \n2.24% 3.13% 2.47~o 4.30% Motorola on 3.07Y0 +1.69 %0 3.26Y0 0.63Y0 3.58Y0 5.13Y0 3.579Z0 5.30% 68020 \noff 3.04% +1.86% 3.28% 0.71% 3.61 %0 5.48% 3.60% 5.6670 Table 6: Percent Change in Miss Ratio and Instruction \nFetch Cost for Direct-Mapped Caches Conclusions A new global optimization method called code replica\u00adtion \nwas developed which can be applied to eliminate almost all unconditional jumps in a program. The re\u00adsulting \nprograms are executing 5,7-6.970 less instruc\u00adtions in average. The number of instructions between branches \nis increased by 1.5 instructions on the average on the SPARC, so that the opportunities for instruc\u00adtion \nscheduling for pipelined or multi-issue machines are improved. The cache work decreases by about 470 \nexcept for small caches. The static number of instruc\u00adtions increases by an average of 53910. The generalized \ntechnique of code replication should be applied in the back-end of highly optimizing compilers if the \nexecu\u00adtion time but not the program size is the major concern. The results of the test set also show \nthat the repli\u00adcation of only the branch condition at natural loops, commonly performed in optimizing \ncompilers, results in about 4570 of the dynamic instruction savings which can be achieved by generalized \ncode replication. References [Be88] M. E. Benitez, J, W. Davidson A Portable Global Optimizer and Linker, \nProceedings of the SIGPLAN 88 Symposium on Programming Lan\u00adguage Design and Implementation, Atlanta, \nGA, June 1988, pp. 329-338 [C182] D. W. Clark, H. M. Levy, Measurement and Analysis of Instruction Use \nin the VAX-11/780, Proceedings of the 9th Annual Symposium on Computer Architecture, April 1982, pp. \n9-17 [Da88] J. W. Davidson, A. M. Holler, A Study of a C Function Inliner, Software, Vol. 18, No. 8, \nAugust 1988, pp. 775-790 [Da89] J. W. Davidson, D. B. Whalley, Quick Com\u00adpilers Using Peephole Optimization, \nSoftware Practice and Experience, Vol. 19, No. 1, January 1989, pp. 195-203 [Da90-2] J. W. Davidson, \nD. B. Whalley, Ease: An Environment for Architecture Study and Exper\u00adimentation, Proceedings of the SIGMETRICS \n1990 Conference on Measurement and Modeling of Computer Systems, May 1990, pp. 259-260 [F162] R. W. Floyd, \nAlgorithm 97: Shortest Path, Communications of the ACM, Vol. 5, No. 6, June 1962, p. 345 [G090] M, C. \nGolumbic, V. Rainish, ]nstr~ction Scheduling beyond Basic Blocks, IBM Journal of Research Development, \nVol. 34, No. 1, January 1990, pp. 93-97 [He90] J. L. Hennessy, D, A. Patterson, Compufer Architecture: \nA Quantitative Approach, Morgan Kaufmann, 1990 (HW89] W. W. Hwu, P. P. Chang, In/ining Function Expansion \nfor Compiling C Programs, Proceed\u00adings of the ACM SIGPLAN 1989 Conference on Programming Language Design \nand Implementa\u00adtion, Vol. 24, No. 5, June 1989, pp. 246-257 [Pe77] B. L. Peuto, L. J. Shustek, An Instruction \nTim\u00ading Model of (7P U Performance, Proceedings of the 4th Annual Symposium on Computer Archi\u00adtecture, \nMarch 1977, pp. 165-178 [Ri72] E. M. Riseman, C. C. Foster, The Inhibition of Potential Parallelism by \nConditional Jumps, H2EE Transactions on Computers, Vol. 21, No. 12, De\u00adcember 1972, pp. 1405-1411 [Sm82] \nA. J. Smith, Cache Memories, Computing Sur\u00adveys, Vol. 14, No. 3, September 1982, pp. 473-530 [Wa62] S. \nWarshall, A Theorem on Boolean Matrices, Journal of the ACM, No. 9, 1962, pp. 11-12  \n\t\t\t", "proc_id": "143095", "abstract": "<p>This study evaluates a global optimization technique that avoids unconditional jumps by replicating code. When implemented in the back-end of an optimizing compiler, this technique can be generalized to work on almost all instances of unconditional jumps, including those generated from conditional statements and unstructured loops. The replication method is based on the idea of finding a replacement for each unconditional jump which minimizes the growth in code size. This is achieved by choosing the shortest sequence of instructions as a replacement. Measurements taken from a variety of programs showed that not only the number of executed instructions decreased, but also that the total cache work was reduced (except for small caches) despite increases in code size. Pipelined and superscalar machines may also benefit from an increase in the average basic block size.</p>", "authors": [{"name": "Frank Mueller", "author_profile_id": "81339518653", "affiliation": "", "person_id": "PP43142003", "email_address": "", "orcid_id": ""}, {"name": "David B. Whalley", "author_profile_id": "81100296923", "affiliation": "", "person_id": "P64341", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143144", "year": "1992", "article_id": "143144", "conference": "PLDI", "title": "Avoiding unconditional jumps by code replication", "url": "http://dl.acm.org/citation.cfm?id=143144"}