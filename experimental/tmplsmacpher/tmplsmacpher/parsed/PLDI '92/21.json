{"article_publication_date": "07-01-1992", "fulltext": "\n Abstractions for Recursive Pointer Data Structures: Improving the Analysis and Transformation of Imperative \nPrograms Laurie J. Hendren* School of Computer Science McGill University  Abstract Even though impressive \nprogress has been made in the area of optimizing and parallelizing programs with ar\u00adrays, the application \nof similar techniques to programs with pointer data structures has remained difficult. In this paper \nwe introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined \npointer data structures. Our approach is based on a mechanism for the Ab\u00adstract Description of Data Structures \n(ADDS), which makes explicit the important properties, such as dimen\u00adsionalit y, of pointer data structures. \nNumerous exam\u00adples demonstrate that ADDS definitions are both nat\u00adural to specify and flexible enough \nto describe complex, cyclic pointer data structures. We discuss how an abstract data structure descrip\u00adtion \ncan improve program analysis by presenting an analysis approach that combines an alias analysis tech\u00adnique, \npath matrix analysis, with information available from an ADDS declaration. Given this improved alias \nanalysis technique, we provide a concrete example of applying a software pipelining transformation to \nloops involving pointer data structures. Introduction and Motivation One of the key problems facing \nboth optimizing and parallelizing compilers for imperative programming languages is alias analysis, that \nis, detecting when two This work supported in part by FCAR, NSERC, and the McGill Faculty of Graduate \nStudies and Research. tDirect correspondenceto this author, jhummel@ics.uci.edu. i This work supported \nin part by NSF grant CCR8704367 ad ONR grant NOO01486K0215. Permission to copy without fee all or psrt \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM cc.pyright notice and the title of the publication and its data that copying is by \npermission of the Machinery. To copy otherwise, or and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA \n@l1992 ACM 0.89791 .476.7 /92/0006 Joseph Hummelt Alexandru Nicolau$ Dept. of Information and Computer \nScience UC-Irvine  distinct memory accesses may refer to the same phys\u00adical memory location. Alias \nanalysis is a critical com\u00adponent of such compilers, and the effectiveness of many compiler analysis \ntechniques and code-improving trans\u00adformations rely upon accurate alias analysis. Given the current trend \ntowards vector, RISC, and super\u00adscalar architectures, where optimizing compilers are even more important \nfor efficient execution, the need for more accurate analysis will grow. Scientific codes have often been \nthe target of optimiz\u00ading and parallelizing compilers. Such codes typically use arrays for storing data, \nand loops with regular in\u00addexing properties to manipulate these arrays. A good deal of work has been \ndone in the area of analysis and transformation in the presence of arrays and loops, and as a result \nnumerous techniques have been developed; for example, invariant code motion, induction variable elimination, \nloop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction \nscheduling strategies such as soft\u00adware pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, \ncodes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, \nand therefore there has not been as much progress in this area. This is problematic, since numer\u00adous \ndata structures in imperative programs linked\u00adlists and trees for example are typically built using recursively-defined \npointer data structures, Further\u00admore, such data structures are used not only in sym\u00adbolic processing, \nbut also in some classes of scientific codes (e.g. computational geometry [Sam90] and the so-called tree-codes \n[App85, BH86]). The goal of this paper is to: (1) investigate why the analysis of pointer data structures \nis more difficult than the analysis of array references, (2) suggest a new mechanism for describing pointer \ndata structures, and (3) show how better descriptions of such data struc\u00adtures lead to improved analysis \nand transformation of imperative programs. appear, and notice is given Aaaociation for Computing to republish, \nrequires a fee /0249...$1 .5(J  1.1 The Problem To motivate the problem, consider the two code frag\u00adments \nshown in Figure 11. The upper code fragment adds the element b [j ] to each element of array a. Since \narrays have the property that a [i] and a [j] refer to different locations if i # j, the compiler can \nimmediately determine from this code fragment that each iteration references a different location of \na. Fur\u00adther, arrays are usually statically allocated, and it is straightforward to determine that the \narrays a and b are different objects. Thus any reference to a will never refer to the same location as \nb [j 1, implying that b [j] is loop invariant. These observations allow the com\u00adpiler to perform a number \nof transformations, including (1) loading b [j] into a register before the loop begins, and (2) transforming \nthe loop by perhaps unrolling or applying software pipelining. fori= ito N a[i] = a[i] + b[j]; while \np 4> NULL { p->data = p->data + q->data; p = p->next; } Figure 1: Arrays versus linked-lists. Now consider \nthe lower code fragment in Figure 1, which operates on a linked-list. This code traverses a list p, and \nat each step adds the value q->data to the node currently being visited. In this case, the prop\u00aderties \nof the structure are not obvious from the code fragment. For example, unless we can guarantee that the \nlist is acyclic, we cannot safely determine that each iteration of the loop refers to a different node \np. This makes the application of loop transformations difficult. Secondly, since the nodes are dynamically \nallocated, it is much more difficult to determine if p and q ever refer to the same node. Hence the compiler \nwill be unable to detect if q->data is loop invariant. As a result, even though the code fragment performs \na similar function to the array-based version, traditional optimizing and parallelizing compilers will \nbe unable to apply similar transformations. 1.2. Previous Work There are currently three approaches \nfor dealing with the problem of alias analysis in the presence of pointer data structures: (1) assume \nthe most conservative case, that all pointers and nodes are potential aliases, (2) analyze other parts \nof the program in an attempt to 1Treat a, b, p, and q as local variables, where a and b are arrays of \nsize N and p and q are pointers. automatically discover the underlying properties of the data structures \n(and hence the relationship between all pointers and nodes), or (3) require code annotations. Although \napproach (1) is far more common (per\u00adhaps with (3) as a backup), there has been signifi\u00adcant work on \napproach (2). One class of solutions has been the development of advanced alias analysis techniques (also \ncalled structure estimation techniques) that attempt to statically approximate dynamically\u00adallocated \ndata structures with some abstraction. The most commonly used abstraction has been k-limited graphs [JM81], \nand variations on k-limited graphs [LH88a, LH88b, HPR89, CWZ90]. The major disad\u00advantage of these techniques \nis that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data \nstructures cannot be distinguished from data structures that truly contain cycles. The work by Chase \net al. [CWZ90] has addressed this prob\u00adlem to some degree; however, their method fails to find accurate \nstructure estimates in the presence of general recursion. This is a serious drawback since programs with \nrecursively-defined data structures often use re\u00adcursion as well. Another method, path matrix analy\u00adsis, \nwas designed to specifically deal with distinguish\u00ading tree-like data structures from DAG-like (shared) \nand graph-like (cyclic) structures [HN90, Hen90]. This analysis uses the special properties exhibited \nby tree\u00adlike structures to provide a more accurate analysis of list-like and tree-like structures even \nin the presence of recursion. However, it has the disadvantage that it cannot handle cyclic structures \n(even if the cyclic nature would not hamper optimizing or parallelizing transformations). Related approaches \ninclude those based on more traditional dependence analysis (e.g. [Gua88], which assumes that structures \ndo not have cy\u00adcles) and abstract interpretation techniques (e.g. Harri\u00adson in [Har89] presents a technique \ndesigned for list-like structures commonly used in Scheme programs, while more recently he has been developing \na uniform mech\u00adanism for handling both symbolic data and arrays us\u00ading the notion of generalized iteration \nspace [Har91]). In general, even though each of these methods work for certain classes of pointer data \nstructures, they un\u00addoubtable fail in the presence of general, possibly cyclic structures and recursion. \nCode annotations represent a compromise, since the programmer can specify what the compiler cannot de\u00adtermine. \nHowever, code annotations are often difficult to use, due to the varied kinds of information that must \nbe conveyed to the compiler. For example, the programmer may need to specify the data dependen\u00adcies [Lar89], \nthe transformation to apply [CON], or the distinctness of data [KKK90]. Also, the compiler typically \ntakes these annotations on blind faith, and so is unable to warn the programmer if a coding change invalidates \nan annotation; this is true for [Lar89, CON], while in [KKK90] it was suggested that the system could \nperform dynamic checks provided that the pr-o\u00adgrammer uniquely tags each node. Finally, such an\u00adnotations \nmust be repeated throughout the program to maximize performance.   Our Approach Based on our past \nexperience of developing alias anal\u00adysis techniques for tree-like structures, and the fail\u00adure of other \ntechniques to find accurate information for more general structures, we believe that a lack of appropriate \ndata structure descriptions is the most se\u00adrious impediment to the further improvement of anal\u00adysis techniques. \nAfter studying a wide range of imp\u00aderative pointer data structures, we have developed an approach for \ndescribing what we feel are the im\u00adportant properties of such structures, important in the sense of enabling \nnumerous optimizing and paralleliz\u00ading transformations. Current imperative programming languages provide \nno mechanisms for expressing this kind of information2. Our approach, called ADDS, provides the program\u00admer \nwith a mechanism for the Abstract Description of Data Structures. ADDS is a minor addition to most imperative \nprogramming languages, and was designed to: . be simple for the programmer to use, . minimize and localize \nprogram annotation, . describe complex pointer data structures, and . enable powerful transformations. \n  The properties expressed through such data structure descriptions are used to increase the accuracy \nand hence effectiveness of existing analysis techniques. This allows the application of powerful optimizing \nand par\u00adallelizing transformations, and may also lead to the de\u00advelopment of new pointer-specific transformations. \nAsking the programmer to specify some properties of his or her data structures should not be considered \na radical change in our way of thinking about program\u00adming in imperative programming languages. In the \npointer data structures domain, programmers already convey quite a bit of implicit information about \ntheir data structures. For example, consider the following two recursive type declarations: type BinTree \ntype TwoWayLL { int data; { int data; BinTree *left; TwoWayLL *next; BinTree *right; TwoWayLL *prev; \n3; 1; 2It should be noted that Larus in [Lar89] discussed a simi\u00adlar approach for Lisp; however, hk \napproach required code (not type) annotations, described only acyclic structures, and could not beusedincodefragmentswhichmightmodify \nthestructure. Even though these type declarations appear identical to the compiler (each declares a record \nwith three fields, one integer and two recursive pointers), the naming conventions imply very different \nstructures to readers of the program. In addition, each structure has some very nice properties which \nthe compiler could exploit. A binary tree naturally subdivides into two disjoint subtrees that can be \noperated on in parallel. A two\u00adway linked-list has the property that a traversal in the forward direction \nusing only the next field never visits the same node twice (likewise for traversals using only the prev \nfield); this property of never visiting the same node twice enables the parallelization of node process\u00ading \nalong the list. The idea of ADDS is simply to make this implicit information explicit to the compiler, \nPos\u00aditive side-effects may be increased human understand\u00ading of programs, and the compiler s ability \nto generate run-time checks to ensure proper use of dynamic data structures. Note that Fortran 90 has \ntaken a similar stance in its treatment of pointers to variables. Variables accessible through pointers \nmust be explicitly declared as either pointers or targets [MR90]. This simple declaration greatly improves \nthe accuracy of alias analysis in the presence of pointers. However, also note that the presence of a \ndescrip\u00adtion mechanism such as ADDS is not enough by itself. Side-effects in imperative programs often \nrearrange the components of a data structure, causing a temporary but intentional invalidation of the \nproperties we wish to exploit. Application of optimizing or parallelizing transformations (that rely \non these properties) during such a time would be incorrect, and intolerable. Hence some form of data \nstructure validation analysis, beyond alias analysis, is necessary not only to ensure correct\u00adness, but \nto enhance debugging as well. In the remainder of this paper we present our view\u00adpoint that: 1. recursive \npointer data structures, regardless of their overall complexity, typically contain sub\u00adstructures that \nexhibit regular properties that can be exploited for the purpose of program analysis and transformation, \nand 2. the ability to express these properties explicitly is an important first step towards the long-term \ngoal of accurate and efficient program analysis in the presence of cyclic pointer data structures.  \n The organization of the paper is as follows. In sec\u00adtions 3 and 4 we address the problem of expressing \nthe regular properties of recursive pointer data structures. In section 3 we present ADDS through a series \nof in\u00adtuitive, increasingly complex examples, and in section 4 we define the properties of ADDS data \nstructures more formally. In section 5 we discuss how the infor\u00admation provided by ADDS can be used to \nimprove the analysis and transformation of programs; in particular, we demonstrate how ADDS enables the \napplication of software pipelining to a list-traversal loop. Finally, in section 6 we present our conclusions. \n 3 ADDS -Abstract Description of Data Structures The transformation of codes involving data structures \nrequires knowledge about the properties exhibited by that structure, e.g. shape, size, and method of \nelement access. With arrays, these properties are readily iden\u00adtifiable. The shape of an array is fixed \nand declared at compile-t ime, its size is known at the time of crest ion (and often at compile-time), \nand locations are refer\u00adenced using integer indices. For example, given a state\u00adment S of the form i \n= i + c (c # O),the compiler is guaranteed that a [i] refers to different elements of a before and after \nthe execution of S. Contrast this with user-defined pointer data structures, in which none of these properties \nare made explicit. Unlike the array example, even a simple traversal statement T such as p = p->next \nwill prevent the compiler from determin\u00ading whether p denotes a different element after the execution \nof T, or the same element. In this section we shall present our mechanism ADDS for describing what we \nfeel are the important properties of recursive pointer data structures: shape, and a sense of direction. \nWe begin with an intuitive summary of these properties, and then demonstrate through a series of increasingly \ncomplex examples how such properties are captured using ADDS, We post\u00adpone a more formal definition of \nADDS to section 4. 3.1 Dimensions, Directions in Pointer Data Structures Suppose you have a recursive \npointer data structure and you wish to describe its shape. Consider the structure in its general form, \nand select a node as the origin -it doesn t matter which node you choose, though some choices make more \nsense than others (e.g. the root of a tree versus a leaf). Next, think of your structure as having dimensions, \n different paths em\u00adanating from the origin, with typically one dimension per path. Finally, select a \nnode n other than the ori\u00adgin, and for each recursive pointer field j in n, de\u00adcide which dimension f \ntraverses and in which direc\u00adtion. The forward direction implies traversing ~ moves one unit away from \nthe origin, and backward implies traversing ~ moves one unit back towards the origin. A field is limited \nto traversing one dimension in only one direction3. 3Thisrestrictioncanbe overcomeby theprogrammerwithout \ntoo muchdifficulty. By default, a structure has one dimension D, where it is assumed that all recursive \npointer fields traverse D in an unknown direction. The idea is to override this default and provide more \nspecific information. For example, we can specify that a singly linked-list has one dimension X, and \none recursive pointer field next that traverses X in the forward direction. As illustrated below, a two-way \nlinked-list is best described as hav\u00ading a single dimension X, with next traversing forward along X and \nprev traversing backward along X: To differentiate such a list from a more general DAG\u00adlike list, e.g. \n~ ~ we introduce the notion of a field f traversing uniquely forward, which implies that for any node \nn, at most one node n (n # n) points to n using ~. Syntactically, the ADDS declaration of a two-way linked-list \nwould then look like: type Two dayLL[X] { int data; TwoWayLL*next is uniquely forward along X; TwoWayLL*prev \nis backward along X; 3; A binary tree can be thought of as having two di\u00admensions left and right, but \nfor reasons soon to be apparent, we shall consider a binary tree as having only one dimension, doun. \nThe important property of a bi\u00adnary tree (and of trees in general) is that for any node n, all subtrees \nof n are disjoint. This information can be expressed by saying that the left and right fields combined \ntraverse down in a uniquely forward man\u00adner. More formally, left and right exhibit the prop\u00aderty that \nat most one left or one right points to any node n, but not both. The motivation for choosing one dimension \nto describe a binary tree is to support the notion of parent pointers, a field that refers from either \na left or right child back to its parent. Pictorially, we view a binary tree with parent pointers as: \n Its ADDS declaration would thus be: type PBinTree [down] { int data; PBinTree *left, *right is uniquely \nforward along down; PBinTree *parent i.s backward along down; };  Note that by declaring left and right \ntogether, we are expressing this notion of a combined traversal. The flexibility of ADDS is illustrated \nby more ex\u00adotic recursive pointer data structures. Typically such structures exhibit multiple dimensions, \nwhere dimen\u00adsions are either independent (disjoint) or depen\u00addent. For example, an orthogonal hst[Sta80], \nusedto implement sparse matrices, has two dependent dinnen\u00adsions X and Y (much like the two-dimensional \narray it represents): cl C2 C3 C4 across rl r2 up r3 r4 lVe say X and Y are dependent since one traversal \nalong X and another traversal along Y may lead to a common node or substructure. For example, traversing \nalong X from r4 and along Y from C3 may lead to the same node. However, even though the dimensions are \nde\u00adpendent, notice that orthogonal lists still possess regu\u00adlar properties. For example, traversing forward \nalong X, or forward along Y, is guaranteed never to visit the same node twice. Further, each row is disjoint, \nso that parallel traversals of different rows along X will never visit the same node (likewise for columns \nand the Y dimension). These properties are captured in the fol\u00adlowing ADDS declaration by declaring that \nthe fields across and down are uniquely forward: type OrthL [Xl [Y] { int data; OrthL *across is uniquely \nforward along X; OrthL *back i.s backward along X; OrthL *down is uniquely forward along Y; OrthL *up \nis backward along Y; 3; 253 Note that unless stated otherwise, dimensions (in this case X and Y) are \nconsidered dependent. Such conser\u00advative nature is intentional. A similar data structure that has two \nindependent di\u00admensions is a list of lists. Consider the following which illustrates a list of lists, \nincluding back pointers along each dimension: IY   IF--C-Q  1$= - Note that each node may be accessed \nby a forward traversal along either the X dimension or the Y di\u00admension, but not both; this is identical \nto the situation for binary trees. Hence X and Y are considered inde\u00adpendent, which is conveyed using \nI I in the following ADDS declaration: type LOLS [X] [Y] where X !I Y { int data; LOLS *across is uniquely \nforward along X; LOLS *back is backward along X; LOLS *down &#38; uniquely forward along Y; LOLS *UP \nis backward along Y; 3; An interesting three-dimensional structure that has both dependent and independent \ndimensions is the two-dimensional range tree [Sam90], used to answer queries such as find all points \nwithin the interval xl. . .x2 or find all points within the bounding rect\u00adangle (xl ,yl) and (x2,y2). \nAs illustrated below, it is a binary tree of binary trees, where the leaves of each tree are linked together \nto form a two-way linked list: leaves > The dimensions down and leaves are dependent; each leaf node \ncan be reached by a forward traversal along both the down dimension and the leaves dimension. However, \nobserve that sub is independent of both down and leaves. That is, any node that can be accessed by a \nforward traversal along sub, cannot be accessed by a forward traversal along down nor along leaves. This \nleads to the following declaration: type TwoDRT [down] [sub] [leaves] where sub 1Idown, sub I Ileaves \n{ int data; TwoDRT *left, *right is uniquely forward along down; TwoDRT *subtree is uniquely forward \nalong sub; TwoDRT *next is uniquely forward along leaves; TwoDRT *prev is backward along leaves; 3; \nNote that aunion (variant record) can beused tosave space by overlaying the storage for (left ,right) \nwith that of(next, prev). This can be done without aloss of expressive power. Lastly, let us consider \na common cyclic data struc\u00adture, the circular linked-list: nex This type of structure can beproblematic, \nsince a sin\u00adgle field next is used for what areessentially twopur\u00adposes, (1) traversing uniquely forward \nand (2) circling around. One declaration is thus: type CirL [X] { int data; CirL *next is unknown along \nX; 1; This is equivalent to saying nothing at all (the default), and the unknown nature of next prevents \nthe compiler from performing possible optimizing or parallelizing transformations (e.g. given the statement \np = q->next the compiler must conservatively assume that p and q are aliases). One solution is to provide \na more explicit declaration that more accurately reflects the properties of a circular list: type CirL \n[Xl < int data; int un_type; union { CirL *next is uniquely forward along X; CirL *around is unknown \nalong X; } Un; 3; Obviously, this may require some rather clumsy coding mactices. A second solution \nis to extend ADDS with another type of direction, circular , and then declare next as traversing in this \ndirection: type CirL [X] { int data; Ci.rL *next is circular along X; }; This solution is more desirable, \nsince the declaration does not require a change in the coding of the pro\u00adgram. However, one needs to \nknow the length of the list in order to perform accurate alias analysis, which is generally unavailable \nat compile-time. This implies information must be collected and maintained at run\u00adtime, which is not \nconsidered in this paper. 3.2 Speculative Traversability In all cases, a data structure declared using \nADDS is required to be speculatively traversable [HG92]. This property allows onetotraverse past the \nend ofad ata structure without causing a run-time error. It can be automatically supported by the compiler, \nand places no additional burden on the programmer (except good programming practices e.g. in C, they \nmust use the name NULLand not an arbitrary integer). This prop\u00aderty is analogous to computing an array \nindex outside the bounds of an array, but not actually using it. It is often useful when applying various \noptimizing and parallelizing transformations. 3.3 Summary of ADDS In summary, ADDS is a technique for \nabstractly de\u00adscribing the important properties of a large class of useful data structures. Once known, \nthese properties can be exploited by the compiler for analysis and trans\u00adformation purposes. By default, \na structure has one dimension D, and all recursive pointer fields traverse D in an unknown fashion. The \nprogrammer may refine this by describing (1) additional dimensions, (2) the in\u00adteraction between dimensions, \nand (3) how the various fields traverse the dimensions. As illustrated with our examples, the choice \nof dimensions and directions is quite intuitive, and the use of ADDS does not place a heavy burden upon \nthe programmer. It should be noted that a programming language and its compilers could directly support \ndata struc\u00adtures such as TwoWayLLand PBlnTree via predefined, abstract data types (e.g. see [Gro91] for \na tool that gen\u00aderates, from a context-free grammar description, ADTs for graph-like structures, and \n[S0190]for an implemen\u00adtation of parallelizable lists). However, a quick sur\u00advey of the literature (or \na data structures text [Sta80]) would reveal a wide variety of important pointer data structures. Implementations \nfor these structures can differ widely aa well. Thus, instead of trying to pre\u00addefine the most common \ntypes of pointer data struc\u00adtures (and constrain the implementation), we chose the opposite approach, \nnamely to develop a technique which allows the programmer to define and implement their 0W71data structures. \nWe believe ADDS is flexible enough to describe the important properties of nearly all pointer data structures, \nwhile providing the com\u00ad piler with the information necessary to enable powerful optimizing and parallelizing \ntransformations. Formal Properties of ADDS In this section we present an overview of the formal properties \nof ADDS data structures. These definitions are not crucial to the understanding of the ideas, but they \ndo show that our ADDS approach has some well\u00adspecified properties. For each definition we summarize the \nimportance of the property being defined. If the reader is comfortable with the intuitive definitions \npre\u00adsented in section 3, this section can be safely skipped. Let N be a data structure declaration with \nn re\u00adcursive pointer fields, n ~ 1. Let k be the number of programmer-specified dimensions over N, 1< \nk< n. The dimensions of N are denoted by dl, .... dk, and the recursive pointer fields of N are denoted \nby .fl , . . . . .f~. A recursive pointer field ~ may traverse along only one dimension d, and in only \none of three di\u00adrections: forward, backward, or unknown. pN denotes a dynamically-allocated node of type \nN, and pN. f isei\u00ad ther NULL4 or denotes the dynamically-allocated node of type N reached by traversing \nthe pointer stored in field f of pN. Traversing a series of non-NULL fields is denoted using a regular \nexpression notation, e.g. . pN(.f)2 denotes the node pN.f.f, . pN(.f)+ denotes any one of the nodes \npN.f, plv.f.f, . . . . . pN((.f) + (g))* denotes any one of the nodes pN, pN.f, pN.g, pN.f.f , pN.f.g, \npN.g.f, pN.g.g, . . . .  For any such regular expression R, R denotes a list of nodes which may or \nmay not cent ain duplicates. If this list is run-time finite (i.e. for all fields f in R, there exists \na node pN denoted by R such that pN .j is NULL), then there are no duplicates. Otherwise R denotes a \nrun-time infinite list of nodes, in which case there must be duplicates. All nodes pN can be thought \nof as forming a single data structure pDS of type N. pDS is considered well\u00ad behaved if the abstraction \ndefined for N is valid. In cases where pDS is not well-behaved (e.g. pDS is under modification) the abstraction \nmust be ignored at these points in the program 5. The definitions presented here assume a well-behaved \ndata structure. Def 4.1: ifN is speculatively traversable, then for all fields f and for all nodes pN, \nif pN.f = NULL then plV.j.f is a valid traversal and also yields NULL. 4TreatNULLasdenotinga typelessnode \nthat is dynamically\u00adallocated by the system at startup. 5The problem of ~jdatjon is discussed in section \n5. [Implication: legal to traverse past what would nor\u00ad mally be thought of as the end of a structure.] \n Def 4,2: if f traverses d in the forward direction, then for all nodes PN, the list of nodes denoted \nby pN(.f)* is run-time finite. [Implication: f is acyclic, traversing f never visits a node twice.] Def \n4.3: if f traverses din a uniquely forward direc\u00adtion, then Def 4.2 holds for f, and for all distinct \nnodes pN and pN , either 1. pN.f = pN .$ = NULL, or 2, pN,f + pN .f. [Implication: f is acyclic and list-like, \ntraversing f from different nodes never visits the same node.] Def 4.4: if f traverses d in an unknown \ndirection, then there may exist a node pN such that the list of nodes denoted by pN(.f)* is run-time \ninfinite. [Implication: f is potentially cyclic, traversing ~ is unpredictable and could visit a node \ntwice.] Def 4.5: if a field b traverses d in the backward di\u00adrection, then there exists a field f that \ntraverses d in the forward direction. Def 4.6: if f traverses d in a uniquely forward direc\u00adtion and \nb traversesd in the backward direction, then for all nodes pN, either: 1. pN.f = NULL, 2. pN.f.b = NULL, \nor  3. pN.f.b = pN. [Implication: traversing f then b forms a cycle, other\u00ad wise f or b is NULL.] Def \n4.7: if f and g (f #g) traverse d in a combined uniquely forward direction, then Def 4.3 holds for f, \nDef 4.3 holds for g, and for all distinct nodes pN and pN , 1. pN.f # pN .g Or pN.f = pN .g = NULL, and \n 2. pN.g # pN .f or pN.g = pN .f = NULL.  [Implication: $ and g are tree-like, they separate a structure \ninto disjoint substructures.] Def 4.8: if fields fl, fz, ,.., fm (2 < m < n) traverse d in a combined \nuniquely forward direction, then for all fi and fj, l<i, j<rn and i#j, Def 4.7 holds for fi and fj. [Implication: \ngeneralization of Def 4.7 the m fields separate a structure into m disjoint substructures.] Def 4.9: \n(a) let di and dj be dimensions of N (i # ~). If di and dj are independent, then for any field f traversing \ndi in the forward direction, for any field g traversing dj in the forward direction, and for all distinct \nnodes pN and pN , 1. pN.j # pN .g or pN.f = pN .g = NULL, and 2. pN.g # pN .f or pN.g = pN .f = NULL. \n (b) Further, for any field U$ traversing di in a uniquely forward direction and for any field b traversing \ndi in the backward direction, either 3. pN.uf = NULL, or 4. for all pN in the list denoted by pN.uf (g) \n, pN .b = pN or pN .b = NULL. [Implication: (a) d, and d, separate a structure into disjoint substructures, \n(b) uniquely forward/backward cycles hold across an independent dimension.] Def 4.10: let di and dj be \ndimensions of N (i # j). If di and dj are not independent, then they are dependent and Def 4.9 does not \nhold for di and d]. [Implication: traversal along di and dj could lead to the same node/substructure.] \n5 Analysis and Transformation using ADDS The principal goal of ADDS is to improve the analy\u00adsis of codes \nutilizing pointer data structures. As dis\u00adcussed in section 1.2, existing analysis-only approaches exhibit \nvarious limitations when faced with such struc\u00adtures. Our approach is to use the information avail\u00adable \nin the ADDS declarations to guide the analysis. This synergy between the abstract data structure de\u00adscriptions \nand the analysis technique provides a more general and more accurate approach. For example, by using \ninformation about the dimensionality and direc\u00adtion of field traversals, the abstraction approximations \nare freed from estimating needless cycles (such as those formed by the forward and backward directions \nalong the same dimension), and can therefore avoid making needless conservative approximations. Let us \nmake this idea more concrete. Analysis-only approaches begin with initial assumptions about what properties \nare important to estimate, and then tailor the approximation domain and analysis rules appropri\u00adately. \nFor example, by choosing k-limited graphs as the approximation domain, one is forced to conservatively \napproximate non-cyclic structures with cycles, which implies the approximation cannot distinguish tree-like \nstructures from cyclic ones. To further illustrate how the choice of analysis depends on the type of \ndata struc\u00adture under consideration, consider once again the fol\u00adlowing two data types, BinTree and TwoWayLL: \nWithout additional information, these types will ap\u00adpear identical to the compiler, and thus the same \nsort of analysis must be applied. Clearly however, there are some analyses that will be more appropriate \nfor one structure than the other; after all, binary trees exhibit much different properties than two-way \nlinked-lists. In the case of two-way linked-lists, the appropriate approximation for the paths between \ntwo nodes p and q is the number of next links from p to q. In addi\u00adtion, the analysis rules for traversing \nnext links from q (q = q->next) should lengthen paths between p and q, while analysis rules for traversing \nprev links from q (cl = Ci->prev) should shorten paths. Both the aPProx\u00adimation domain and the appropriate \nrules for traversals can be easily inferred from an ADDS declaration that says (1) there is one dimension, \n(2) the field next tra\u00adverses uniquely forward, and (3) the field prev tra\u00adverses backward. Now consider \nbinary trees. In this case, the appro\u00adpriate approximation of paths is that of going left, right or clown \n(where down is a conservative approx\u00adimation for going either left or right). Traversing along either \nthe left or right fields lengthens paths; there is no field for which a traversal shortens a path. This \ninformation can be expressed by an ADDS decla\u00adration that which states that left and right traverse uniquely \nforward along the same dimension. This in turn can be used to build an appropriate approxima\u00adtion domain, \nand set of analysis rules, for binary trees. Thus, the problem is that initial assumptions bias program \nanalysis. If the analysis starts with very weak assumptions, then the resulting analysis will be overly \nconservative; this is the current situation with struc\u00adture estimation techniques. If the initial assumptions \nare valid for only one class of data structures, then applying the analysis to other classes will lead \nto in\u00adaccurate and conservative analysis; this is the case of applying ordinary path matrix analysis \nto cyclic struc\u00adtures. Hence it appears crucial that in order to perform accurate analysis in the presence \nof a wide variety of data structures, we must begin with accurate informa\u00adtion about the structure. This \nis the essence behind ADDS, and our approach. In this section we show how the properties of ADDS data \nstructures can be used to improve the accuracy of program analysis, and thus enable the application of \npowerful optimizing and parallelizing transformations. In particular, we present in section 5.1 a new \napproach combining ADDS with an existing analysis technique. Then in section 5.2 we demonstrate a new \napplication of software pipelining made possible by our approach. type BinTree type TwoWayLL 5.1 Program \nAnalysis{ int data; { int data; BinTree *left; TwoWayLL*next; As discussed earlier, the presence of \na description mech- BinTree *right; TwoWayLL*prev; anism such as ADDS is not enough in itself to en\u00ad \n}; 1; able optimizing and parallelizing transformations in the presence of pointer data structures. Imperative \npro\u00adgrams routinely rearrange components of such a struc\u00adture, and it is during these points in a program \nthat the abstraction (or parts thereof) must be ignored by the compiler, Otherwise transformations may \nbe applied that are based on invalid assumptions, causing incor\u00adrect code generation. Hence some form \nof abstraction validation analysis is required to enable a given trans\u00adformation; this is in addition \nto more traditional alias analysis which is needed to ensure that the transfor\u00admation is safely applied. \nOur approach to the analysis problem is a combined one, in which safe analysis techniques are used in \ncon\u00adjunction with ADDS declarations, In particular, we are developing an approach to the static analysis \nof ADDS data structures that is an extension of path rna\u00adtriz analysis [HN90, Hen90], called general \npath matrix analysis. Path matrix analysis was originally designed to automatically discover and exploit \nthe properties of acyclic data structures. With the help of ADDS, gen\u00aderal path matrix analysis is capable \nof handling cyclic data structures as well. General path matrix analysis computes, at each pro\u00adgram point, \na path matrix PM which estimates the relationship between every pair of live pointer vari\u00adables; PM is \nthus a function of the current path matrix and the program statement under analysis. The entry PiW(p, \nq) denotes an explicit path or alias, if any, from the node pointed to by p to the node pointed to by \nq. The analysis does not attempt to express all possi\u00adble paths between two nodes, since cyclic data \nstruc\u00adtures would soon overwhelm the matrix. Instead, the paths explicitly traversed by the program are \ncaptured in the PM, while the remaining paths and aliases are deduced from the current state of the path \nmatrix and the ADDS declarations. 5.1.1 Abstraction Validation Analysis In order to validate an ADDS \ndeclaration, the effect of certain pointer statements on the path matrix must be compared with the original \nADDS declaration, In par\u00adticular, statements of the form p->f = q may change the shape of the data structure. \nThis in turn may re\u00adsult in a violation of the declared abstraction. However, this is generally not an \nerror in an imperative program, and so is not treated as one. Instead, we note that the abstraction is \ninvalid at this point in the program, and we do not perform any transformations that rely on the validity \nof the necessary ADDS properties. Though the actual process of validation is beyond the scope of this \npaper, the idea is as follows. The ADDS 6Of course, such a violation could in fact be an error. Warning \nthe programmer, or providing a compiler switch to enable these warnings, would be a useful debugging \ntool. declaration is encoded as a series of relationships be\u00adtween the various pointer fields of the \nnode. During analysis, if the path matrix ever denotes a relationship between two fields that is illegal, \nthis part of the ab\u00adstraction is deemed invalid and an entry is added to the path matrix encoding the \nviolation. Later, if another program statement fixes the relationship between these two fields, the entry \nis removed and the abstraction is once again considered valid. A common example of a temporary break \nin an ab\u00adstraction is the moving of a subtree from one node to another within a binary tree. Here is \na possible code fragment: dest->lef t = src->left; src->left = NULL; After analysis of the first statement, \nit is obvious that src and dest share a common subtree, even though this violates the disjointness property \nof a binary tree. However, the violation is immediately corrected, as is usually the case. 5.1,2 Alias \nAnalysis Alias analysis is best explained via example. Consider the following code fragment, in which \nthe pointer vari\u00adable M denotes the head of a two-way linked-list of nodes. Each node represents a point \nin 2D space, and contains its x and y coordinates. The code shifts the origin from (O, O) to (hd->x, \nO) by subtracting the x\u00adcoordinate of this first point from all remaining points: = hd->next; P ]M] \nply I while p <> NULL { P->X = P->X -hd->x; p= p->next; Ha 37 Alias Matrix AM If the compiler fails \nto discover that next traverses a list in the forward direction (i.e. that Def 4.2 holds for next ), \nthen its analysis of the above code will be overly conservative the compiler must assume that next is \ncyclic, and hence that hd and all values of p are poten\u00adtial aliases for the same node. The effect of \nthis conser\u00advative alias analysis is summarized above in alias ma\u00adtrix form7. The p entries are used \nto denote aliasing during the loop, where definite aliases are indicated by = and possible aliases are \nindicated by =?. Observe that all the entries denote some form of aliasing. In particular, the entry \nAM[hd, p ] indicates that hd is a possible alias for the iterative values of p. As discussed in section \n1.1, this prevents a number of useful loop transformations. Now suppose the programmer declared his or \nher linked-list using the ADDS declaration TwoWayLL, as TpleaSe note that this is just a convenient waY \nof expressing aliases, and is not the result of path matrix analysis. given in section 3.1. Assuming \ngeneral path matrix analysis determines that the structure abstraction is well-behaved (valid) at the \nstart of this code fragment, the compiler can use the acyclic nature of the next field (from Def 4.2) \nto infer that the statement p = p->next never visits the same node twice. As a result, general path matrix \nanalysis would produce the following path matrices, which denote (from top to bottom): just be\u00adfore the \nloop, after one iteration, and after the loop analysis has reached a fixed point. The important dif\u00adference \nbetween these path matrices and the previous alias matrix AM is the replacement of the =? entires with \nmore accurate information about paths. I I I &#38; hd p P hd = next+ next+ = P P next = An entry in \nthe path matrix like next+ indicates a path of one or more next links; all paths are either explic\u00aditly \nencoded within an entry, implicitly encoded within other entries and the ADDS declarations, or both, \nAll aliases are explicitly encoded, so an empty entry guar\u00adantees that the two pointers are not aliases, \nThus, we see that the ADDS declaration and general path ma\u00adtrix analysis have captured the desired property \nin the PM necessary for performing numerous optimizing and parallelizing transformations, namely that \nhd, p and p are never aliases, 5.2 Transformations Optimizing and parallelizing transformations come \nin many forms, including: e fine-grain transformations (e.g. improved instruc\u00adtion scheduling), . loop \ntransformations (e.g loop unrolling or soft\u00adware pipelining), and e coarse-grain transformations (e.g. \nparallel execu\u00adtion of code blocks). The application of such transformations typically re\u00adquires accurate \nalias analysis. As shown in the previ\u00adous subsection, ADDS and general path matrix anal\u00adysis provide \nsuch accuracy. This will clearly aid in fine-grain transformations where dependency analysis is crucial. \nI&#38;%en transforming code that operates on a data structure, loop transformations typically re\u00adquire \nthe structure to exhibit list-like properties, while coarse-grain transformations typically-require tree-like \nproperties. Such properties are all expressible using ADDS. Earlier work [HG92] has shown the applicability \nof loop unrolling [DH79] on scalar architectures. For ex\u00adample, a simple loop to initialize every node \nof a linked\u00adlist showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling \n(see [HG92] for more details and timings). In this section we present an ex\u00adample of a more powerful \nloop transformation, soft\u00adware pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Given the current trend \ntowards machines supporting higher degrees of parallelism, e.g. VLIW and super\u00adscalar, this type of optimization \noffers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, \nwhich manipulates a two\u00adway linked-list of points. The following is an equivalent loop body written in \npseudo-assembly code: S1 if p==NULLgoto done S2 load p->x, Rl S3 load hd->x, R2 S4 sub RI, R2, R3 S5 \nstore R3,p->x S6 load p->next ,p S7 goto SI As we saw earlier, if the compiler s analysis is overly \nconservative, then the compiler will incorrectly assume that hd and all values of p are potential aliases \nfor the same node. In this case the data dependency graph shown in Figure 2 is constructed, with false \nloop\u00adcarried dependencies from S5 to S2 and S3. The result is that the loop appears to exhibit very little \nparallelism between iterations. However, as we also discussed, if the data structure is declared as a \nTwoWayLL, general path matrix analysis will be able to determine that in fact hd and all the iterative \nvalues of p are never aliases. This will eliminate these false dependencies, and the loop now appears \nas a sequence of nearly independent, parallelizable iterations. To exploit this parallelism, we can apply \nsoftware pipelining. First we need to minimize the effect of the loop-carried dependence from S6 to S1. \nBy renaming, we can move S6 above S2, calling it S1 .6, and then replace S6 with a copy statement. The \nresult is the following semantically equivalent loop: Si if p==NULLgot o done S1.6 load p->next ,p S2 \nload p->x, Rl S3 load hd->x ,R2 S4 sub R1, R2, R3 S5 store R3,p->x S6 move P(>P S7 goto S1 The next \niteration of the loop can now begin as soon as S1.6 of the current iteration completes, allowing the \nFigure 2: Dependency graph for pseudo-code. loop bodies to nearly overlap in execution. But in ad\u00addition, \nsince the list must be speculatively traversable (by Def 4.1), it is safe to swap S1 and S1.6, further \nincreasing the amount of available parallelism. Finally, since hd and p are never aliases, we can deduce \nthat hd->x is loop invariant. Hence S3 can be moved out\u00adside and above the loop. The code below incorporates \nthe above-mentioned transformations, producing a semantically equivalent loop: so load hd->x , R2 SI \nload p->next ,p S2 if p==NULLgot o done S3 load p->x, Rl S4 sub RI, R2, R3 S5 store R3,p->x S6 move P \n$P S7 goto SI This loop can be pipelined as follows (the columns de\u00adnote different iterations, rows \ndenote statements exe\u00adcuting in parallel): S1 S2 S1 S3 S2 S1 S4 S3 S2 S1 c=== S5S4 S3S2 S5S4 S3 S5 S4 \nS5 The boxed statement represents the new parallel pipelined body of the loop. As a result, the code \nex\u00adhibits a theoretical speedup of 5. Note that the copy statement S6 is removed as part of the pipelining \npro\u00adcess, via (enhanced) copy propagation [NPW91]. In general, software pipelining can lead to even larger \nspeedups, depending on the characteristics of the 100P body. Obviously, the actual speedup also depends \nheavily on the target machine s architecture. 6 Conclusions As we have shown with numerous examples, \nmany recursively-definedpointer data structures exhibit im\u00adportant properties which compilers can exploit \nfor op\u00adtimization and parallelization purposes. These proper\u00adties are often known to the programmer conveyed \nim\u00adplicitly e.g. through the use of appropriate identifiers and yet unavailable to the compiler. This \nlack of in\u00adformation hinders the accuracy of alias analysis and thus restricts the transformations that \ncan be applied to codes using pointer data structures. In this paper we have proposed an abstract descrip\u00adtion \ntechnique, ADDS, which allows the programmer to state such properties explicitly. The description of \na recursive pointer data structure using ADDS is quite intuitive, and does not place an excessive burden \non the programmer. By combining ADDS and general path matrix analysis, we demonstrated that the result\u00ading \napproach enables more accurate and more general alias analysis, and hence the application of powerful \nop\u00adtimizing and parallelizing transformations. As we have seen, software pipelining is one such transformation. \nADDS represents an important first step in our long\u00ad term goal of efficient compiler analysis of codes \ninvolv\u00ad ing cyclic pointer data structures. ADDS is a simple extension to most any imperative programming \nlan\u00ad guage, and yet can lead to analysis and transforma\u00ad tions that are otherwise not possible using \ntraditional methods. With the increasing use of languages that support pointers and recursively-defined \npointer data structures, the import ante of such an approach will no doubt increase. References [AK87] \nRandy Allen and Ken Kennedy. Automatic trans\u00adlation of FORTRAN programs to vector form. ACM Trcmsactions \non Programming Languages and Sgstenw, 9(4), October 1987. [AN88a] A. Aiken and A. Nicolau. Optimal Loop \nParal\u00adlelization. In Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, \npages 308-317, June 1988. [AN88b] A. Aiken and A. Nicolau. Perfect Pipelining: A new loop parallelization \ntechnique. In Proceed\u00ad ings of the 1988 European Sumpositimon Program\u00ad ming. Springer Verlag Lecture \nNotes in Computer Science No.300, March 1988. [App85] Andrew W. Appel. An Efficient Program for Many-Body \nSimulation. SIAM J. Sci. Stat. Com\u00adput., 6(1):85 103, 1985. [ASU87] [BH86] [CON] [CWZ90] [DH79] [EN89] \n[Gro91] [Gua88] [Har89] [Har91] [Hen90] [HG92] [HN90] [HPR89] [JM81] Alfred V. Aho, Ravi Sethi, and \nJeffrey D. Unm\u00adan. Compilers: Principles, Techniques, and Toois. Addison-Wesley, 1987. Josh Barnes and \nPiet Hut. A Hierarchical O(NlogN) Force-Calculation Algorithm. Nature, 324:446 449, 4 December 1986. \nThe code can be obtained from Prof. Barnes at the University of Hawaii. CONVEX Computer Corporation. \nCONVEX C and FORTRAN Language Reference Manuals. 1990.  D.R. Chase, M. Wegman, and F.K. Zadek. Anal\u00advsis \nof Dointers and structures. In Proceedings .. of the SIGPLA N 90 Conference on Prograrnrni;g Language \nDesign and Implementation, pages 296\u00ad 310, 1990. J.J. Dongarra and A.R. Hinds. Unrolling looPs in FORTRAN. \nSoftware-Practice and Experience, 9:219-226, 1979. Kemal Ebcioglu and Toshio Nakat ani. A New Compilation \nTechnique for Parallelizing Loops Loops with Unpredictable Branches on a VLIW Architecture. In Proceedings \nof the Second Work\u00ad shop on Programming Languages and Compiiers for Paraliel Computing, Research Monographs \nin Parallel and Distributed Computing. MIT-Press, 1989. Josef Grosch. Tool support for data structures. \n Structured Programming, 12(1):31-38, January 1991. Vincent A. Guarna Jr. A technique for analyz\u00ading \npointer and structure references in parallel re\u00adstructuring compilers. In Proceedings of the Inter\u00adnational \nConference on Parallel Processing, vol\u00adume 2, pages 212 220, 1988. W. Ludwell Harrison III. The interprocedural \nanalysis and automatic parallelization of scheme programs. Lisp and Symbolic Computation, 2(3/4):179-396, \n1989. W. Ludwell Harrison III. Generalized iteration space and the paralleiization of symbolic pro\u00adgrams. \nIn Ian Foster and Evan Tick, editors, Pro\u00adceedings of the Workshop on Computation of Sym\u00adbolic Languages \nfor Parallel Computers. Argonne National Laboratory, October 1991. ANL-91/34. Laurie J. Hendren. Parallelizing \nPrograms with Recursive Data Structures. PhD thesis, Cornell University, April 1990. TR 90-1114. Laurie \nJ. Hendren and Guang R. Gao. Design\u00ad ing Programming Languages for Analyzability: A Fresh Look at Pointer \nData Structures. In Proceed\u00ad ings of the ~th IEEE International Conference on Computer Languages (to \nappear, also available as A CA PS Technical Memo 28, McGiii University), April 1992. Laurie J. Hendren \nand Alexandru Nicolau. Paral\u00ad lelizing Programs with Recursive Data Structures. IEEE Trans. on Parallel \nand Distributed Comput\u00ad ing, 1(1):35 47, January 1990. Susan Horwitz, Phil Pfeiffer, and Thomas Reps. \nDependence analysis for pointer variables. In Pro\u00ad ceedings of the SIGPLA N 89 Conference on Pro\u00ad gramming \nLanguage Design and Implementation, pages 28 40, June 1989. N. D. Jones and S. S. Muchnick. Program \nFlow Analysis, Theory and Applications, chapter 4, Flow Analysis and Optimization of LISP-like Structures, \npages 102 131. Prentice-Hall, 1981. [KKK90] [Kuc78] [Lam88] [Lar89] [LH88a] [LH88b] [Lov77] [MR90] [NPW91] \n[PW86] [RG82] [Sam90] [s0190] [Sta80] [ZC90] David Klappholz, Apostolos D. Kallis, and Xi\u00adangyun Kang. \nRefined C: An Update. In David Gelernter, Alexandru Nicolau, and David Padua, editors, Lanouaqes and \nComvilers for Parallel Comp~ting, ~age~ 331-357. Th} MIT Press, 1990. D.J. Kuck. The Structure of Computers \nand Comp\u00adutations: Volume I. Wiley, 1978. Monica Lam. Software Pipelining: An Effective Scheduling Technique \nfor VLIW Machines. In Proceedings of the SIGPLA N 1988 Conference on Programming Language Design and \nImplemerzta\u00adtion, pages 318-328, June 1988. James R. Larus. Restructuring Symboiic Pro\u00adgrams for Concurrent \nExecution on Multiproces\u00adsors. PhD thesis, University of California, Berke\u00adley, 1989. James R. Larus \nand Paul N. Hilfinger. Detecting conflicts between structure accesses. In Proceed\u00adings of the SIGPLA \nN 88 Conference on Program\u00adming Language Design and Implementation, pages 21-34, June 1988. James R. \nLarus and Paul N. Hilfinger. Restruc\u00adturing Lisp programs for concurrent execution. In Proceedings of \nthe A CM/SIGPLAN PPEALS 1988-Parallel Programming: Experience with Ap\u00adplications, Languages and Systems, \npages 100-110, July 1988. D.B. Loveman. Program Improvement by Source\u00adto-Source Transformation. Journal \nof the ACM, 24(1):121-145, January 1977. Michael Metcalf and John Reid. Fortran 90 Ex\u00ad plained. Oxford \nUniversity Press, 1990. A. Nicolau, R. Potasman, and H. Wang. Register Allocation, Renaming and their \nimpact on Fine\u00adgrain Parallelism. In Proceedings of the Fourth Workshop on Languages and Compilers for \nPar\u00adallel Computing, August 1991. David A. Padua and Michael J. Wolfe. Advanced compiler optimization \nfor supercomputers. Com\u00admunications of the ACM, 29(12), December 1986. B. R. Rau and C. D. Glaeser. Efficient \nCode Generation for Horizontal Architectures: Com\u00adpiler Techniques and Architectural Support. In Proceedings \nof the 9th Symposium on Computer Architecture, April 1982. Hanan Samet. The Design and Analysis of Spatial \nData Structures. Addison-Wesley, 1990. Jon A. Solworth. The PARSEQ Project: An Interim Report. In David \nGelernter, Alexandru Nicolau, and David Padua, editors, Languages and Compilers for Parallel Computing, \npages 490-510. The MIT Press, 1990. Thomas A, Standish, Data Structure Techniques. Addison-Wesley, 1980. \nHans Zima and Barbara Chapman. Supercompilers for Parallet and Vector Computers. ACM Press, 1990.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Even though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined pointer data structures.</p><p>We discuss how an abstract data structure description can improve program analysis by presenting an analysis approach that combines an alias analysis technique, <italic>path matrix</italic>, with information available from an ADDS declaration. Given this improved alias analysis technique, we provide a concrete example of applying a software pipelining transformation to loops involving pointer data structures.</p>", "authors": [{"name": "Laurie J. Hendren", "author_profile_id": "81100646110", "affiliation": "", "person_id": "P169482", "email_address": "", "orcid_id": ""}, {"name": "Joseph Hummell", "author_profile_id": "81332505759", "affiliation": "", "person_id": "P149056", "email_address": "", "orcid_id": ""}, {"name": "Alexandru Nicolau", "author_profile_id": "81100139899", "affiliation": "", "person_id": "PP39029364", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143138", "year": "1992", "article_id": "143138", "conference": "PLDI", "title": "Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs", "url": "http://dl.acm.org/citation.cfm?id=143138"}