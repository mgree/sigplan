{"article_publication_date": "07-01-1992", "fulltext": "\n Delinearization: an Efficient Way to Break Multiloop Dependence Equations Vadim Maslov Research Computing \nCenter, Moscow State University Moscow 119899, Russia maslov~vadik .srcc.msu.su Abstract Exact and efficient \ndata dependence testing is a key to success of loop-parallelizing compiler for computa\u00adtionally intensive \nprograms. A number of algorithms has been created to test array references contained in parameter loops \nfor dependence but most of them are unable to answer the following question correctly: Are references \nC(il + 10jl) and C (i2 + 10jz +5), o < il, iz < 4, 0 ~ jl,jz < 9 independent? The technique introduced \nin this paper recognizes that il, i2 and jl, j2 make different order contributions to the subscript index, \nand breaks dependence equation il+10jl =iz+ 10j2+5into twoequations il=iz+5 and 10jl = 10j2 which then \ncan be solved indepen\u00addently. Since resulting equations contain less variables it is less expensive to \nsolve them. We call this technique delinearization because it is reverse of the linearization much discussed \nin the literature. In the introduction we demonstrate that linearized references are used not infrequently \nin scientific FOR-TRAN and C codes. Then we present a theorem on which delinearization algorithm is based \nand the algo\u00adrithm itself. The algorithm is fairly simple and inex\u00adpensive. As a byproduct it tests equations \nit produces for independence as exactly as it is done by GCD-test and Banerjee inequalities combined. \nThe algorithm has been implemented at Moscow State University in a vec\u00adtorizer named VIC. Introduction \n The motivation for this work is to translate serial pro\u00adgrams written in FORTRAN-77 and C to intermedi- \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantaga, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires e fee and/or specific permission. ACM SIGPLAN \n92 PLD1-6/92/CA @ 1992 AcM 0.8979 ~-476-7 /92/0006 /0 J~2,.. $J ,50 ate code in which loop parallelism \nis explicitly encoded [DMM88, Mas90]. This code then can be prettyprinted in the form of FORTRAN-90 [Ame90] \nor Vector C [Li86] or used by compiler backend to generate code for vector/parallel computers. Currently \nthe translation is done automatically as in [ABC+ 87, AK87, KKP+81, Kus89, AGS91] though possibility \nof user participation in translation process as in [KMT90, IJT90] is consid\u00adered. To decide whether loop \niterations can be run in par\u00adallel or not the translator should know whether data are transferred bet \nween iterations or not. For example, the iterations of a loop in the fragment REAL D(O:9) DO 11=0,8 1 \nD(i+l)=D(i)*Q can not be run in parallel because every iteration uses memory location written on previous \niteration (the first iteration uses memory location computed before the loop). Parallelizing compiler \nachieves this conclusion by finding out that assignment statement data-depends on itself. Data dependence \nexists because there exist integer il, iz E [0, 8] such that il + 1 = i2 and there\u00adfore D(il + 1) and \nD(iz ) may use the same memory locations. Quite the opposite, references to D in the fragment REAL D(CI:9) \nDOli =0,4 i D(i)= D(i+5)*Q are independent becauae for all integer il, iz c [0, 4] we have il # iz + \n5. Therefore in this case iterations of the loop can be run in parallel. Finding out whether two references \nare dependent or not is in general case equivalent to integer program\u00adming. As is stated in [MHL91], \nbest algorithms known to solve this problem have complexity that depends on the value of loop bounds \nor the complexity is O(nO{ J) where n is the number of loop variables. Even for sim\u00adple cases these algorithms \nare too expensive. Existing techniques vs linearized multiloop ref\u00aderences. A number of dependence testing \nalgorithms have been created that sacrifice accuracy for efficiency [KKP+81, AK87, WB87, MHL91, GKT91]. \nHowever, even for the best algorithms this sacrifice is significant. Below we show that most existing \nalgorithms are un\u00adable to detect independence for a pair of linearized 2\u00adloop references. References \nto C in the program REAL C(O:99) DOii =0,4 DOij =0,9 C(i+lO*j)=C(i+10 *j+5) are independent because \nthe constrained equation il+Iojl =i2.+ loj2 +5 il, iz E [0,4] (1) { jl, j2 f= [0,9] has no integer solutions. \nHowever, this equation has real solutions. For example, il = iz = 2, jl = 4.5, jz = 4 satisfy the equation. \nThe existing techniques capable of handling multi\u00adloop dependence equations return dependent if there \nare real solutions, Therefore they can not prove in\u00addependence for this equation. The following tech\u00adniques \nbelong to this category: Banerjee inequalities [AK87, WB87], A-test [LYZ89], Fourier-Motzkin elim\u00adination \n[MHL91, DE73]. It should be noted, how\u00adever, that normalization (tightening) of constraints proposed \nin [Pug91] being applied to this problem to\u00adgether with Fourier-Motzkin elimination returns inde\u00adpendent \n. Nevertheless, it is better to avoid Fourier-Motzkin elimination because of its high cost [Tri85]. The \ninteger-exact techniques in this case are also ineffective because the equation containing four vari\u00ad \nables, each with different coefficients, is too compli\u00ad cated for these techniques. We found the following \ntechniques inadequate to disprove this dependence: Single Variable Per Constraint Test [MHL91, Ban88], \nAcyclic Test [MHL91], Simple Loop Residue Test [MHL91], Shostak algorithm [Sho81, BC86], GCD-test [AK87, \nBan88]. Our approach. Our approach is to break depen\u00ad dence equation (1) into two equations: lojl loj \nJ = o { i,~2 ~ [0)91 and il iz b=o il, iz E [0,4] { which then can be solved independently of each \nother by existing techniques. Let us prove that such a break can be done. To do it we make the following \nsubstitutions: A= 10jl loj2, B = il iz 5. Taking in account loop bounds gives us [l?! < 10, \\Al= O \nor ]Al ~ 10. Therefore A+.B=Ocan hold only if A= Oand13 =0 hold. We call our approach dehearization because \nit is reverse of linearization. For FORTRAN programs, lin\u00adearization is replacement of a reference A(iI, \n.... L) to an n-dimensional array A(O : HI, .... 0: Hn) with a ref\u00ad erence A(il +~~=z i~ ~&#38;l (.H/+ \n1)) to a 1-dimensional array A(() : (~~=1 (111 + 1)) 1). Array linearization is done by most compilers \nto map multidimensional ar\u00adrays to l-dimensional memory. Replacement of the above program fragment with \nREAL C(O:9,0:9) DOli =0,4 DOlj =0,9 1 C(i, j)= C(i+5, j) is delinearization in the literal sense of \nthe word two dimensions are introduced instead of one, We inter\u00ad pret delinearization more subtly as \nbreaking single dependence equation into smaller independently solved equations, For the cases when delinearized \ndependence equa\u00adtions show dependence we still have an advantage com\u00adpared to existing techniques because \nwe can calcu\u00adlate direction and distance vectors more exactly. In [MHL91] authors say that they can not \ndiscover that distance vector is (2,0) for the following fragment: DO10i=l,8 DO IOj=l,10 10 A(lO*i+j)=A(lO* \n(l+2)+j)+7 Using delinearization we are able to prove that distance vector is (2,0). One may think that \nlinearized references are very exotic and not worth of special consideration. This is not so. Next we \ndiscuss where and how often linearized references are used. Array references linearized by hand. First \nof all, linearized references can be used in programs as is . To find out how often linearized references \nare used in practice we analyzed 8 FORTRAN programs from RiCEPS benchmark suite [Por89]. We discovered \nthat array references linearized by hand are used in 6 out of 8 programs. Number of outermost loop nests \nthat con\u00ad tain linearized references for each examined program is given in Figure 1. We also found that \nlarge programs use linearization to arrange run-time dimensioning. The largest pro\u00ad grams examined, BOAST \nand CCM, use linearized ref\u00ad erences for this purpose. The amount of linearization Program Type BOAST \nReservoir Simulation CCM Atmospheric LINPACKD Linear Algebra QCD Quantum Chromodynamics SIMPLE Fluid \nFlow SPHOT Particle Transport TRACK Trajectory Plot WANALl Wave Equation Figure 1: Number of loop nests \ncontaining in these programs is so great that it may be impossi\u00adble to analyze them with acceptable precision \nwithout applying delinearization. Linearized references do not always appear explic\u00aditly. For example, \nin the the following fragment, de\u00adrived from a loop nest in BOAST, we have an induction variable controlled \nby 3 loops. IB =-1 DO 1 1=0,11-1 DO 1 J=O,JJ-I DO 1 K=O,KK-I IB=IB+I C(J)=C(J)+I i B(IB)=B(IB)+Q Existing \ntechniques treat it as controlled by only inner\u00admost loop. We can parallelize the last assignment state\u00adment \nwith respect to all 3 loops if we recognize that all loops control IB, replace IB with K+J*KK+I*KK*JJ: \nDO 1 1=0,11-1 DO 1 J=O,JJ-I DO 1 K=O,KK-I C(J)= C(J)+ I i B(K+J*KK+I*KK*JJ)= # B(K+J*KK+I*KK*JJ )+Q \nand delinearize references to B. Another way is to convert these 3 loops to one loop with variable IB \nand II* JJ*KK iterations. It doesn t work in general, because the loop nest can contain statements referencing \nnot only IB but 1, J, K too. In our example next to last assignment statement ref\u00aderences J. Should such \na conversion be done these refer\u00adences will become non-linear. This alternative method also doesn t work \nfor imperfectly nested loops. Array aliasing. In FORTRAN-77 array aliasing is caused by EQUIVALENCE, \nCOMMON statements and by association of dummy and actual parameters of procedure call. FORTRAN ANSI standard \n[Ame78] states that in time of association (aliasing) participat\u00ading arrays are considered to be linearized. \nThis require\u00adment allows programmers to associate arrays of differ\u00adent form and encourages compilers \nto linearize array references in order to handle all possible cases. Lines Loop nests containing linearized \nreferences 7000 >28 24000 >24 400 0 2000 2 1000 0 1000 2 4000 5 2000 4 linearized references for RiCEPS \n In the program fragment REAL A(O:9,0:9) REAL B(O:4,0:19) EQUIVALENCE (A, B) DOli =0,4 DOlj =0,9 1 A(i, \nj)=B(i,2*j+l) arrays A and B are associated by EQUIVALENCE statement. Since arrays have different form \nall the ref\u00aderences to them must be linearized: REAL C(O : 99) DOli =0,4 DOlj =0,9 1 C(i+lO*j )=C(i+10*j+5) \nApplying delinearization we prove independence. If we have references of different form to the same array, \nwe need not linearize all dimensions. It even can be harmful, In the program fragment REAL A(O:9,0:9,0:9,0:9) \nREAL B(O:4,0:19,0 :9,0:9) EQUIVALENCE (A, B) DOli =0,4 DOIJ =0,9 DOlk =0,9 DO 11=0,9 1 A(i, J,k, IFUN(lO))= \n# B(i,2*-j+l, k,l) linearization of i and j subscripts is necessary to make precise dependence testing \nfor aliased subarrays possi\u00adble. Adding to this group subscript k is possible be\u00adcause delinearization \nwill separate it back but it only wastes time. Linearization of subscript 1 make analy\u00adsis impossible \nbecause IFUN ( 10) ranges over unknown values and will spoil the whole index. So it is wise to linearize \n(and then delinearize) i and j subscripts and leave k and 1 subscripts as they are: REAL C(O:99,0:9,0:9) \nDOli =0,4 DOij =0,9 DOik =0,9 DO 11=0,9 i C(i+lO*-j,k, IFUN(lO))= # C(i+iO*j+5,k,l) C array references. \nConsidering C programs, we find several obvious cases when hand-linearization is likely to be used. First, \nsince only constant array boundaries are allowed in C one must linearize by hand references to dynamically-allocated \n(by malloc) mul\u00ad tidimensional arrays run-time dimensioning again. Second, to make analysis in the presence \nof pointers possible translator should treat pointer which is used to traverse some array as index in \nthe linearized version of that array. Consider the following C program fragment: float d[iOO] ; f lost \n*i, *j; for(j=d; j<=d+90; j+=iO) for(i=j; i<j+5; i++) *i=*(i+5) ; It uses pointers i and j to traverse \narray d. Since the program doesn t use these pointers for any other pur\u00adpose they can be replaced with \ninteger index variables: float d[iOO] ; int i,j; for(j=O; j<iO; j++) for(i=O; i<5; i++) d[j*iO+i]=d[j*iO+ \ni+5J ; The references to d then can be delinearized and finally we have float d[iO] [10]; int i,j; \nfor(j=O; j<iO; j++) for(i=O; i<5; i++) d[j] [i]=d[j] [i+5] ; which clearly demonstrates independence. \nIn fact it is possible to state that to perform precise dependence testing for C programs compiler has \nto as\u00adsume that all array references are linearized. On one hand, ANSI C standard requires value of subscript \nex\u00adpression to fit into corresponding subscript boundaries. On the other hand, subscript-out-of-range \ncheck is not performed by most C compilers and this requirement is unknown to majority of users. So we \nhave to linearize all the references in order to analyze correctly working programs which may be not \nstandard conforming.  2 Background In this section we give formal definition of dependence taken from \n[BC86, AK87, WB87]. Let us consider two FORTRAN-77 loop nests with no >0 common outer\u00admost loops (Figure \n2). DOxl = o, xl .,. DOZno = o, x., DO Xno+I = 0, x no+l ... DOZn, = o, x., S1 A(fl(~l, ....~nl). ...,fi(~l, \n....~n.)) a .. ENDDO ... ENDDO DO !/..+1 = o, Yno+l  ... DoYn, = o, Yn2 S2 ... = 4n(w, -wYn,),-. -.., \nw(w, -.,Yn2)). ENDDO ... ENDDO ENDDO ... ENDDO Figure 2: Loop nests for dependence definition Statements \nS1 and S2 are wrapped in nl and nz loops respectively. Since no outermost loops are common, Vi 6 [l, \nno] Xi = Yi. Each DO-loop is assumed to be normalized to run from O to its upper bound by 1. If source \nloops are not normalized they can be easily converted to the normalized form. We assume that loop upper \nbounds Xl, ....X., and Y1, ....Yn, are integer constants. If in practice we en\u00adcounter a loop bound Xi \n= F(zl, ..., q-1) we replace it with maxzl,,..,~i_l ~(~1, .... zi-1) 1. If a loop bound is an expression \nin variables of unknown value (for exam\u00adple, values supplied from the program input) we have to perform \nsymbolic calculations in the delinearization algorithm (see Section 4). For simplicity, we assume that \nall statements of the nest other than DO and ENDDO are assignment state\u00adments. It is so because in this \npaper we consider only data dependence and not control dependence. A is array with 1 dimensions. 1Replacing \niteration space by its rectangular extension is a trade-off necessary to make delinearization cost-effective. \nDe\u00adlinearization will proceed much better if we calculate the exact max and min for subscript functions \nin the exact iteration space (which in case of X, being linear function of q, ....Z,-I is a polyhedron). \nBut exact calculations will make delinearization too slow. Definition of dependence. Under such assump\u00adtions \nwe say that there is a dependence between state\u00adments S1 and S2 if and only if there exist inte\u00adger numbers \nal E [O,X1], .... an, E [0, X~l], /31 c [0, Yl], .... ,&#38; c [0, Yn,] such that: vi G [1,/] j$i(al, \n....anl) = gi(pl, ....pn.) (2) If there are no dependence between two references then these references \nare called independent. On an intuitive level, the existence of dependence between S1 and S2 means that \nthere exist instances of S1 and S2 that use the same memory locations. Direction and distance vectors. \nThe vector d= (dl, .... d~O)such that Vi c [1, no] < ifcr~</?~ di z z ifclizp~ > if@>/?$ { is called \nthe direction vector of the dependence [WO182]. Since there may exist more than one set of integer numbers \ncxi, ~~ that satisfy the system of dependence equations (2), there may exist more than one depen\u00addence \nbetween S1 and S2. Dependence which have the same direction vector are counted as one dependence. Dependence \nthat have different direction vectors can be summarized. For example, two dependence with direction vectors \n(=, <) and (=,=) can be represented by one dependence with direction vector (=, <). Sim\u00adilarly, (>) plus \n(=) is (~), (>) plus (<) is (#), (<) plus (=) plus (>) is (*). However not all direction vectors should \nbe summarized. For example, (<, =) and (=, <) dependence should not be replaced with a (<, <) dependence \nbecause this dependence have de\u00ad compositions that are not present in the original pair of dependence \n for example, (<, S) can be decomposed to (=,=) and (<,<). Distance vectors [Mur71, Lam74] are more exact \nver\u00adsion of direction vectors. The vector D = (al ~1, ....%. j?~.) is a distance vector for the above \ndefined dependence. In general case all dependence existing between two references may have different \ndis\u00adtance vectors. If a number of distinct distance vectors depends on the size of iteration space then \nit is said that there is no good distance vector. Distance-direction vectors. If some element of distance \nvector is not constant we can replace it with the corresponding element of direction vector thus con\u00adstructing \ndistance-direction vector. Such a vector car\u00adries all the information that is carried by direction and \ndistance vector combined. For example, in the program DO i=O, 5 DO j=0,8 1 A(i, j)=A(2i, j+l) direction \nvector of the only dependence is (~,>) and distance vector is (?, l). Combining these two vectors gives \nus distance-direction vector (~, 1). Non-direction vector constraints. Several tech\u00adniques were invented \nto provide dependence inform~ tion which is more precise than distance and direction vectors. Let us \nmention some of them. Wolf and Lam in [WL91] proposed generalization of distance and direction vectors \nin which each element of their vector is a range of integers. Lu and Chen in [LC90] treat set of distance \nvectors of dependence that hold between a pair of array refer\u00adences (they call them dependence vectors) \nas a space of vectors, not packing this space into single representing vector. Voevodin in [Voe91] proposed \nto draw dependence arcs in parametrized acyclic graph depicting computa\u00adtional history of at-line program \nfragment (called the al\u00adgorithm graph) rather than in conventional static state\u00adment graph, thus replacing \ndirection and dist ante vec\u00adtors with exact description of set of solutions. These techniques are flexible \nand exact. However they are expensive enough, so it is unclear whether they can be used in production \ncompilers. Dependence classification. Dependence that we obtain using the definition (2) are further \nclassified as true, anti, output, input [BC86, AK87, WB87]. Since this classification is done after establishing \nthe exis\u00adtence of dependence and calculating direction and/or distance vector and delinearization is \ndone before it we don t discuss this classification here. Linear subscript functions. Proving automati\u00adcally \nthat two references are independent is imprac\u00adtical unless subscript functions fi and gi are restricted \nto some class. Following a majority of researchers we restrict ~~ and g~ to a class of linear functions \nof loop variables. That is fi(X~, ... ,~.,) = ~o + ~aikxk (3) k=l n2 9i(Yl, . . ..yn. ) = b.+ ~ bikyk \n(4) lc=l where ai~ and b~kare integer constants. More general case when aik and bik are integer loop-invariant \nexpres\u00adsions is discussed in the Section 4. Example. Figure 3 contains FORTRAN-77 program adapted from \n[AK87] and table that lists pairs of de\u00adpendent references, dependence direction and distance\u00addirection \nvectors for this program. 156 REAL X(200), Y(200), B(IOO) REAL A(1OO,IOO), C(1OO,1OO) DO 30 i=l,100 \ns~ X(i) = Y(i) + 10 DO 20 j=l,99 s~ B(j) = A(j,20) DO 10 k=l,100 Sj A(j+l,k) = B(j) + C(j,k) 10 CONTINUE \nS4 Y(i+j) = A(j+l,20) 20 CONTINUE 30 CONTINUE Pair of Direction Distance-direc\u00adreferences vector tion \nvector SZ:B SZ:B (*, =) (*, o) S2:B S3:B (*, =) (*, 0 S3:AS3;A (*, =,=) (*, 0, q S3:AS2:A (*> <) (*, \n+1) S3:AS4:A (*, o) [:)=) S4:YSI:Y (<) Figure3: Program and itsdependences  3 Delinearization Let \nus consider one ofthe dependence equations from the system of equations (2): f($l,..., %,) = 9(W,..., \n%2) Taking in account (3), (4) and loop bounds wetrans\u00adform this equation to Co +clzl+. ..+cnzn=o (5) \n21 c [0,21], .... z~ c [O,zn] { where n= nl+nz, Zi are integer numbers, and co = all bI), ck = ak, %k=~k, \nZk=xk, l<k~nl ck = bk-~,, .%k Yk-nl,zk=yk-n,> w<k<n Theorem. The set of solutions for the constrained \nequation (5) coincides 2 with the Cartesian productof the set ofsolutions of constrained equation d(l+clzl \n+.. .+cmzm=o (6) 21 c [0,21], .... z~ c [O,zm] { and the set ofsolutions ofconstrained equation Do +C~+lZ~+l \n+...+CnZn= O (7) z~+l e [0, Zm+l], .... Zn c [0, Zn] { 2TW0 ~et~ ~oincide iff both sets are empty or \nboth sets have identical elements. if there exist integer numbers m, do, Do such that 772E [I, n], gcd(llo, \ncm+l, ....cm) > max(ldO+~~=le~Zkl, ]do+~fl=lc;z!cl) (8) and cl) = do+ D(). (9) By IzI we denote absolute \nvalue of x. By gcd(x, y,..., z) we denote greatest common divisor of Z,y, .... z. Functions c+ and c-are \ndefined by the fol\u00adlowing: c ifc~O O ifc~O c+ = O ifc<O c-= c ifc<O {{ This theorem gives us solid \nbase for performing delin\u00adearization correctly, On an intuitive level the theorem means that if variables \ntaking part in the equation can be divided in two groups such that minimal change of the left hand part \nof (5) caused by one group is greater than its maximal change produced by another group then these groups \nare separable and can be analyzed independently. Proof. It is quite obvious that for all B, C,v,G the \nstatements B< v< C and max(lBl, ICI) < G imply G < v < G or Ivl < G. Let us make the following substitutions \nin this statement: m B= do+~c; &#38; k=l k=l m v = do+ ~ c~z~, k=l G= gcd(~o, Cm+I, .... Cn). It is \neasy to prove that (to do it we can use the proof of Banerjee inequali\u00adties from [AK87]). Combining this \nwith the theorem condition we have: m Vzl E [0,21], .... z m E [O,ZnJ ldl)+~ck%kl < G, k=l (lo) Let \nus consider ~znl. =l~+*zm+l+ oo+G M is integer because DO, Cm+l, ..., Cn are all divisible by G according \nto definition of G. If M# O, then G~ Gil. Combining this with (10) gives us &#38;f # O =$ or Now it is \nrelatively easy to prove the theorem. If one of equations (6), (7) has solutions and another equation \nhas no solutions then by adding these equa\u00adtions we prove that equation (5) has no solutions, If both \nequations (6), (7) have no integer solutions then ul=do+~ckzk#O k=l and U2= DO+ ~ ck~k+o. k=m+l Combining \nthis with (11) gives us Ial I < Iu2 I and, therefore, al + C2 # O. Taking in account (9) we con\u00adclude \nthat equation (5) has no integer solutions. On the other hand, if left hand part of (5) is nonzero for \nanyzl, ..., z. then al + V2 # O. Therefore either al # O or U2 # O. That means that cartesian product \nof sets of solutions of equations (6) and (7) is empty. If both equations (6) and (7) have solutions \nthen by adding them we prove that equation (5) has solutions. On the other hand, if equation (5) has \ninteger solutions then U1 -t ~Z = O and according to (11), either u2. = O or lull < Iazl. It is possible \nonly if al = O and U2 = O. 0 Algorithm. Figure 4 is an overview of the delin\u00ad earization algorithm. This \nalgorithm breaks original dependence equation into smaller equations for indi\u00ad vidual dimensions using \nthe above theorem. The goal set for the algorithm is to produce as many dimensions (and equations) as \npossible. The number of produced dimensions is limited by the number of variables in the source equation \nbecause each resulting dimension must contain at least one variable and no variable can appear in more \nthan one dimension. The idea of the algorithm is based on the following observation: the condition (8) \nof the theorem implies that any of c1, ....c~ are less than any of Cm+l, ....cn. It means that dimensions \nproduced by the algorithm are ordered according to absolute value of coefficients standing before these \ndimensions variables. So the al\u00adgorithm begins with ordering coefficients of the source equation by their \nabsolute value. Then scanning or\u00ad dered coefficients from smaller ones to bigger ones the algorithm tries \nto establish that the barrier between already processed variables and variables that are still not processed \nis the barrier between dimensions. The algorithm applies the theorem correctly. The line IF max(lcrninl, \nIcrna%l) < gk THEN is a place where the condition of the theorem is checked. Indeed, on iteration k of \nthe loop right before that line we have: k-1 k-1 smin = c; 21,, smax = cIt+ -zI17 Ex l=kBeg l=kBeg do=r=comodgk, \n~o=co ? , G= gk= &#38;&#38;). Since Do= co (CIImod gk), Do is divisible by gk and therefore gcd(gk, \nDo) = gk. Taking in account these substitutions and making index substitution lkBe~ I--+ 1, ~kBeg+l ~ \n2, .... ~k.1 &#38;m, ~k wm+l, ~k+l % m+2, .... In * n gives us exactly the condition (8). If the condition \nof the theorem is satisfied we single out the equation r + ~?=-kl,e~ CI1 O. If cmin and zrI cmax for \nthis equation are consistent the existing tech\u00adniques are applied to the equation to compute the set \nof dependence it produces. Then smin, srnax, kBeg, co are tuned to cast away this equation. What does \nthe algorithm produce? The algo\u00adrithm produces the set of dependence that hold be\u00adtween two references \ndenoting each dependence with di\u00adrection vectors. Some dependence can be summed up without loss of precision \n(see the paragraph Distance\u00addirection vectors ). If precision is not important and time/storage are in \ngreat demand one can keep only one summary direction vector instead of several dis\u00adtinct vectors. On \nthe other hand if one wishes more precision being ready to sacrifice some storage, one can keep distance-direction \nvectors instead of direction vec\u00ad tors. Performing GCD and Banerjee tests on the fly . Besides separating \ndimensions the algorithm on the fly performs test for independence which has the same sharpness as combined \nGCD and (*, *, ....*) Banerjee tests performed on separated dimensions. Let us prove it. Variables cmin \nand cmax represent minimum and maximum values for left hand expression of equation (6). If for some subscript \n(which was just separated from the other subscripts) cmin >0 or cmaz <0 then co + ~~=-k1~e4 clz 1 # O \nfor all Z1. This proves indepen\u00ad dence for the separated dimension in the same way as it is done in Banerjee \ninequalities. On the first step of DO loop in the algorithm we have cmin =cmax =r=co mod gkO. Since COmod \ngk~ < gkO we execute IF statement body. The first statement of the IF body (also IF) is reduced to IF \nr > 0 or r <0 THEN RETURN 0 . That means that dependence is INPUT: dependence equation (5), that is, \nintegers co, Cl, ....cn. Z1, ....Zn. OUTPUT: set of dependence direction vectors (if this set is em~tv \nthen there is no de~endence). .. . , order ck, that ;s, compute 1~ such that Vi G [1, n 1] Icr,+l I \n~ lcr~ 1; Let kO be minimal number such that C1,O #O; Compute gk : Vk ~ [kO,n] gk = gcd(cz~ ,c~k+l, \n....c~.); 9n+.1 a ~; DirVecs = {(* , ....*)}; kBeg = ko; srnin = srnm = (); DOk=ko, n+l,l = COmod gk; \ncmin = smin+r; cmax=sma$+r; ~F max(lcmin[, lcmazl) < gk THEN IF cmin >0 or cmax <0 THEN RETURN O (independent); \nCompute set of direction vectors NV = (nvl, .... nvl) for dependence equation r + Z!=-&#38;c9 Crlzll \n= O using existing techniques [WB87, GKT91, MHL91]; DirVecs = {dv (l w Idv e DirVecs, nv c NV, h fl nv \n# 0}; smin = smaz = O; kBeg = k; co= co-r; ENDIF sm$n = smin + c~~ZI~; smmc = smax + c~~ZI~;  ENDDO \nRETURN DirVecs; Figure 4: Dependence testing algorithm based on delinearization possible only if r = \nO, that is only if co is divisible by but computations in them have to be performed sym\u00adgkO. Condition \nthat we extracted from the algorithm bolically. is classic GCD-test. So we have proved that GCD-test \nLet us consider an example of symbolic delineariza\u00adis equivalent to Banerjee test performed in the least \ntion. We try our algorithm on the FORTRAN program significant dimension. REAL A(O :N*N*Ii-1) DOli = o, \nN-2Example. Let us apply the algorithm to the follow- DOlj = o, x-l ing dependence equation: DOlk=O, \nN-2 i A(N*N*k+N* j +i) =A (N*N*k+ j +N*i+N*N+N) 100kl -100k2 + 10jl -10i2 +-il -jz 11(1= O { il, iz, \nkl, kz G [0,81, jI, j2 6 [0,9] which is to be delinearized to the following program: Values that algorithm \nvariables have on each itera- REAL A(O:N-l, O: N-l, O:N-1) tion right before applying the theorem and \nseparated DOli = O, N-2 equations are present at Figure 5. DOlj = O, N-1 DOlk=O, N-2 Efficiency. The \nalgorithm has linear time complexity i A(i, j,k)=A(j, i+l,k+l) O(n) where n is the number of variables \nin the source According to the algorithm we must delinearize the dif\u00ad equation, Sorting of ck takes more \nthan linear time, but ference between index expressions thus breaking depen\u00ad since n is usually small \n(n s 6), this time is negligible. dence equation into system of 3 equations. Here for The time needed \nto perform the algorithm is signif\u00adthe sake of simplicity we demonstrate delinearization icantly less \nthan the time needed to solve linearized for A( j, 1+1, k+l ) reference only. equation because there \nare fewer participating vari-Since N3 1 is upper bound of array A, N3 >1 and ables, The precision gains \nof delinearization are there\u00adtherefore N > 1. Knowing this we can do ordering fore almost free. of coefficients. \nThe results are: co =N2+N, c1= l,c2=N, cs=N2, gl=l, gs=N, gs =N2. Symbolics handling On the 1st iteration \nwe have: smin = smaz = O, r = (iV2 + iV) mod 1 = O. Since O <1 first iteration of the To analyze real \nlife programs we have to allow algorithm produces O = O dependence equation which Co, cl,..., Cn, 21, \n..., 2. from equation (5) to be not only doesn t change summary set dependence vectors. integer constants \nbut loop invariant integer expressions On the 2nd iteration we have: smin = O, smax = too. Extending \nclass of analyzable indexes in such a N-l, r=(N2+N) mod N=O. Since N-1< N way we can still apply the \ntheorem and the algorithm is true inequality for any N the barrier can be drawn k c~ smin smax 1-10 0 \n-1: 2 1 -9 0 -110 3 -lo -9 8 -110 4 10 -80 0 -110 .5 -1oo -80 90 -110 6 100 -800 0 -1oo 7 -800 800 -1oo \n1 Figure 5: Example of the between j and i dimensions. co remains the same after the iteration. On \nthe 3rd iteration we have: smin = O, smax = lV(IV-2), r = (112+ IV) mod iV2 = IV, max(AJ, N(lV 2) + iV) \n= N(f V 2) + fV = N2 -f-N. Since inequality N2 + N < jV2is also true for any N > 1we draw barrier bet \nween i and k dimensions. After the iteration co= N2. On the 4th iteration we have: smin = O, smax = N2(N \n2), r= N2 mod m = N2, max(...) < co. Since any number is less than infinity we always separate the last \ndimension. After the last iteration co = O always. This example demonstrates that operations used in \nthe algorithm are simple enough to be computed sym\u00adbolically. It should be also noted that for precise \nhan\u00addling of symbolics translator has to be able to keep and process predicates. 5 Conclusion We presented \ndependence testing method that under certain conditions breaks multiloop dependence equa\u00adtions into smaller \nequations. Since each resulting equa\u00adtion contains less variables than source equation it haa better \nchances to be solved exactly. Equations that we break originate from linearized references which are \nused rather oft en in the real programs. Delinearization algorithm is inexpensive and it proves independence \nof references on the fly with the same precision as GCD and Banerjee tests applied to each dimension. \nThe method makes computation of direction and dis\u00adtance vectors by other techniques more precise and \nless time-consuming. 6 Acknowledgements The author is grateful to Lev Korolyov and lgor Mashechkin from \nthe Computational Mathematics and Cybernetics department of Moscow State University and to Vladimir Repin \nand Paul Ruban from the Re\u00adsearch Computing Center of MSU for their guidance and advice. Thanks to everyone \nwho helped me to write this paper. Special thanks to Paul Havlak from gk ; 1 0 1 0 10 -lo 10 -lo 100 \n0 100 -1oo 00 delinearization Separated equation i~ j~=() 10j~ 10i~ 10=O 100kl 100k2 100= O algorithm \nwork Rice University for his thorough reading of this paper and numerous helpful suggestions.  References \n[ABC+ 87] F. Allen, M. Burke, P. Charles, R. Cytron, overview of the for multiprocess\u00ad the First Inter-Supercomputing. \nGreece, 1987. [AGS91] [AK87] [Ame78] [Ame90] [Ban88] [BC86] [DE73] and J. Ferrante. An PTRAN analysis \nsystem ing, In Proceedings of national Conference on Springer-Verlag, Athens, V. M. Andreyev, I. S. Golosov, \nand S. V. Sprogis. Fortran for MKP (in Rus\u00ad sian). Programmirov aniye (Programming), September 1991. \n J. R. Allen and K. Kennedy. Automatic translation of Fortran programs to vector form. ACM Transactions \non Programming Languages and Systems, 9(4):491-542, Oc\u00ad tober 1987, American National Standards Institute \nX3J3 Comittee. FORTRAN-77, 1978. American National Standards Institute X3J3 Comittee. FORTRAN-90, 1990. \nU. Banerjee. Dependence Analysis for Su\u00adpercomputing. Kluwer Academic Publish\u00aders, Boston, 1988. M. Burke \nand R. Cytron. Interprocedural dependence analysis and parallelization. In Proc. of the SIGPLAN 86 Symposium \non Compiler Construction, July 1986. G. Dantzig and B.C. Eaves. Fourier-Motzkin elimination and its dual. \nJour\u00adnal of Combinatorial Theory, A(14):288\u00ad297, July 1973. [DMM88] T. I. Daniyarhodzhayev, V. Yu. Maslov, \nand I. V. Mashechkin. Multilanguage pro\u00adgramming system intermediate language [GKT91] [IJT90] [KKP+81] \n[KMT90] [Kus89] [Lam74] [LC90] [Li86] [LYZ89] [Mas90] (in Russian). In Arhitektura EVM i Chislenniye \nMetodi (Computer Architecture and Numerical Methods}. Computational Mathematics Division, USSR Academy \nof Sciences, 1988. G. Goff, K. Kennedy, and C. Tseng. Practi\u00adcal dependence testing. In ACM SIGPLAN 91 \nConference on Programming Languages Design and Implementation, 1991. F. Irigoin, P. Jouvelot, and R. \nTriolet. Overview of the PIPS project. In Pro\u00ad ceedings of the International Workshop on Compilers for \nPara!lel Computers, Paris, France, November 1990. D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and \nM. Wolfe. Dependence graphs and compiler optimization. Confer\u00adence Record of the Eighth ACM Symposium \non Principles of Programming Languages, January 1981. K. Kennedy, K. S. McKinley, and C. Tseng. Interactive \nparallel programming using the ParaScope Editor. Technical Report TR90\u00ad 137, Dept. of Computer Science, \nRice Uni\u00ad versit y, October 1990. E. S. Kusikov. The algorithm for paralleliz\u00ading iterative loops (in \nRussian). Program\u00admirovaniye (Programming), March 1989. L. Lamport. The parallel execution of DO loops. \nCommunications of the ACM, 17(2):83-93, February 1974. L.-C. Lu and M. Chen. New loop transfor\u00admation \ntechniques for massive parallelism. Technical Report YALEU/DCS/TR-833, Department of Computer Science, \nYale University, October 1990. Kuo-Cheng Li. A note on the Vector C lan\u00adguage, SIGPLAN Notices, 21(1), \n1986. Z. Li, P. Yew, and C. Zhu. Data depen\u00addence analysis on multi-dimensional array references. In \nInternational Conference on Supercomputing, Crete, Greece, June 1989. Vadim Yu. Maslov. Program vectorizib \ntion in cross-translator system (in Rus\u00adsian). In Smeshanniye Vichis!eniya i Pre\u00adobrazovaniya Program \n(Mixed Calculations and Program Transformations), Novosi\u00ad birsk, USSR, 1990. Siberian Division of USSR \nAcademy of Sciences. 161  [MHL91] [Mur71] [Por89] [Pug91] [Sho81] [Tri85] [Voe91] [WB87] [WL91] [WO182] \nDror E. Maydan, John L. Hennesv, and Monica S. L~m. Efficient and exa~t data dependence analysis. In \nACM SIGPLAN 91 Conference on Programming Languages Design and Implementation, June 1991. Y. Muraoka. \nParallelism Exposure and Ex\u00adploitation in Programs. PhD thesis, Dept. of Computer Science, University \nof Illinois at Urbana-Champaign, February 1971. A. K. Porter field. Sofiware Methods for Im\u00adprovement \nof Cache Performance on Super\u00adcomputer Applications. PhD thesis, Rice University, May 1989. William Pugh. \nThe Omega Test: a fast and practical integer programming algo\u00adrithm for dependence analysis. In Super\u00adcomputing \n91, October 1991. R. Shostak. Deciding linear inequalities by computing loop residues. ACM Journal, 28(4):769-779, \n1981. R. Triolet. Interprocedural analysis for pro\u00adgram restructuring with Parafrase. Techni\u00adcal Report \nCSRD Rpt. No. 538, Dept. of Computer Science, University of Illinois at Urbana-Champaign, December 1985. \nV. V. Voevodin. Mathematical Founda\u00adtions of Parallel Computations (in Rus\u00adsian). Moscow University Publishing \nHouse, Moscow, 1991. M. Wolfe and U. Banerjee. Data depen\u00addence and its application to parallel pro\u00adcessing. \nInternational Journal of Parallel Programming, 16(2):137-178, 1987. Michael E. Wolf and Monica S. Lam. \nA data locality optimizing algorithm. In ACM SIGPLAN 91 Conference on Pro\u00adgramming Languages Design and \nImple\u00admentation, June 1991. M. J. Wolfe. Optimizing Supercompilers for Supercomputers. PhD thesis, University \nof Illinois, October 1982.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Exact and efficient data dependence testing is a key to success of loop-parallelizing compiler for computationally intensive programs. A number of algorithms has been created to test array references contained in parameter loops for dependence but most of them are unable to answer the following question correctly: Are references <italic>C</italic>(<italic>i</italic><subscrpt>1</subscrpt> + 10<italic>j</italic><subscrpt>1</subscrpt>) and <italic>C</italic>(<italic>i</italic><subscrpt>2</subscrpt> + 5), 0 &#8804; <italic>i</italic><subscrpt>1</subscrpt>, <italic>i</italic><subscrpt>2</subscrpt> &#8804; 4, 0 &#8804; <italic>j</italic><subscrpt>1</subscrpt>,<italic>j</italic><subscrpt>2</subscrpt> &#8804; 9 independent? The technique introduced in this paper recognizes that  <italic>i</italic><subscrpt>1</subscrpt>, <italic>i</italic><subscrpt>2</subscrpt> and <italic>j</italic><subscrpt>1</subscrpt>, <italic>j</italic><subscrpt>2</subscrpt> make different order contributions to the subscript index, and breaks dependence equation <italic>i</italic><subscrpt>1</subscrpt> + 10<italic>j</italic><subscrpt>1</subscrpt> = <italic>i</italic><subscrpt>2</subscrpt> + 10<italic>j</italic><subscrpt>2</subscrpt> + 5 into two equations <italic>i</italic><subscrpt>1</subscrpt> = <italic>i</italic><subscrpt>2</subscrpt> and 10<italic>j</italic><subscrpt>1</subscrpt> = 10<italic>j</italic><subscrpt>2</subscrpt> which then can be solved independently. Since resulting equations contain less variables it is less expensive to solve them. We call this technique  <italic>delinearization</italic> because it is reverse of the <italic>linearization</italic> much discussed in the literature.</p><p>In the introduction we demonstrate that linearized references are used not infrequently in scientific FORTRAN and C codes. Then we present a theorem on which delinearization algorithm is based and the algorithm itself. The algorithm is fairly simple and inexpensive. As a byproduct it tests equations it produces for independence as exactly as it is done by GCD-test and Banerjee inequalities combined. The algorithm has been implemented at Moscow State University in a vectorizer named VIC.</p>", "authors": [{"name": "Vadim Maslov", "author_profile_id": "81100602451", "affiliation": "", "person_id": "PP31085926", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143130", "year": "1992", "article_id": "143130", "conference": "PLDI", "title": "Delinearization: an efficient way to break multiloop dependence equations", "url": "http://dl.acm.org/citation.cfm?id=143130"}