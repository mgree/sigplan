{"article_publication_date": "07-01-1992", "fulltext": "\n Prototyping Fortran-90 Compilers for Massively Parallel Machines Marina Chen &#38;James Cowie* Department \nof Computer then-marina@cs.yale.edu Abstract Massively parallel architectures, and the languages used \nto program them, are among both the most dificult and the most rapidly-changing subjects for compilation. \nThis has created a demand for new compiier prototyp\u00ading technologies that allow novel styl. of compilation \nand optimization to be tested in a reasonable amount of time. Using formal speci$cation techniques, we \nhave pro\u00adduced a data-parallel Fortran-90 subset compiler for Thinking Machines ) Connection Machine/2 \nand Con\u00adnection Machine/5. The prototype produces code from initial Fortran-90 benchmarks demonstrating \nsustained performance superior to hand-coded *Lisp and compet\u00aditive with Thinking Machines CM Fortran \ncompiler. This paper presents some new specification techniques necessary to construct competitive, easily \nretargetable prototype compilers. 1 Introduction Existing compilers for massively parallel machines \nhave generally been constructed using traditional methods, combining generation from specification for \na few sub\u00adsets of the problem (typically the syntactic analysis) with hand-coded solutions for most others. \nOne such compiler is Thinking Machines CM Fortran [1], which targets the Connection Machine/2, a massively \nparal\u00adlel, SIMD supercomputer. CMF was awarded a 1990 Gordon Bell Prize honorable mention for compiled \ncode performance. *Supportfor thisworkisprovidedby theDefenseAdvanced ResearchProjectAgency,monitoredby \ntheArmyDirectorateof Contracting,undercontractDABT 63-91-C-0031. Permission to copy without fee all or \npart of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinary. To copy otherwise, \nor to republish, requires a fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA 01992 ACM 0-89791 \n-476-7192 /0006 /0094 . ..$1 .50 94 Science, Yale University cowie-james@)cs. yale.edu While CMF and \nsimilar compilers inarguably achieve production quality, they are difficult and ex\u00adpensive to construct, \nmaintain, and extend as a re\u00adsult of their target-specific, highly-tuned, hand-coded nature. By contrast, \ntraditional systems for gener\u00adating compilers from denotational semantics drew on the flexibility and \npower of formal specification to build and maintain systems that were truly machine\u00adindependent. They \nremain unacceptable for produc\u00adtion use, however, due to the generally poor perfor\u00admance of the code \nthey produce, and their size and speed. The MESS system [5] introduced (separable seman\u00adtic specification \nas an alternative to denotational se\u00admantics approaches. MESS used a two-level seman\u00adtic specification \nto separate high-level language prop\u00aderties from model detail. This enforced separation helped alleviate \nthe performance problems associated wit h compilers generated from formal descriptions. Such systems, \nhowever, were still limited in practice to sequential-execution, uniprocessor implementations of simple \nimperative languages. MESS-generated com\u00adpilers had no systematic framework for program trans\u00adformation, \nfocusing instead on simple two-pass trans\u00adlation. They lacked the necessary semantic features to express \nparallelism, and thus did not attempt to model or target parallel systems. Other research has shown the \nimportance of a unified intermediate representation in constructing optimizing compilers. The SUIF language \nof Tjiang, Wolf, et al. [7] successfully preserves high-level information for low\u00adlevel transformations, \nand begins to show how multiple levels of optimization concerns must be coordinated, but was not used \nin the context of formal compiler specification. Pingali et al use abstract interpretation techniques \nin their intermediate representation [6], and use a true imperative store and explicit formulation of \nloops. They are primarily concerned, however, with dataflow analysis, and do not have an abstraction \nthat unifies data and control parallelism, nor a treatment of back-end concerns. In this paper we propose \na methodology for com\u00adpiler design ~hat brings together lessons l~&#38;ned from previous compilers for \nmassively parallel machines with techniques from formal specification. Like the MESS system, our prototype \ncompilers define a semantic al\u00adgebra as their intermediate format, and use produc\u00adtion rule sets to define \nthe translation process from the user s syntactic forms to constructs of this algebra. Adding a framework \nfor intermediate program transfor\u00admation, support for multilayered target systems, a,nd a shape abstraction \nfor data parallel computation, we then show how to model massively parallel machines in a working compiler \nspecification. Finally, we present initial results from our prototype Fortran-90-Y compiler for the CM/2 \nand CM/5, After eight months of development, the prototype implemen\u00adtation generates code for multiple \nplatforms exhibit ing sustained performance superior to that of hand-coiled *Lisp and competitive with \nThinking Machines CM Fortran compiler. Furthermore, the prototype s space and time requirements are comparable \nto those of na\u00adtive language tools for the CM. We argue that compet\u00aditive performance, coupled with rapid \nprototyping a,nd target flexibility, have become persuasive arguments for developing compilers for massively \nparallel machines using formal specification techniques. 2 Problem Overview To clarify our motivations, \nwe introduce brief overviews of the data-parallel features of Fortran 90 and the slicewise programming \nmodel for the Connection Ma\u00adchine/2. We then break down the problems to be solved in a prototype compiler \nfor that machine, and de\u00adscribe the overall prototyping and optimization strate\u00adgies that need to be \ndeveloped as a result. 2.1 Source Language: Data Parallelism in Fortran 90 The Fortran 90 language [2] \ncontains many extensions to the old Fortran 77 standard that are of interest for data parallelism. Specifically, \nthe ability to refer to and compute directly with whole arrays and arbitri~ry subsections give the programmer \nthe ability to express program parallelism that would previously have been stated only implicitly or \nthrough the use of source\u00adlevel annotation. The wide selection of intrinsic pro\u00adcedures for examining, \nreshaping, or performing re\u00adductions over Fortran 90 arrays serve to replace many common transformations \nthat programmers were pre\u00adviously forced to approximate with serial syntax. For example, the Fortran \n77 code INTEGERK(128,64) , L(128) DO 10 1=1,128 L(I) =6 DO 20 J=I,64 K(I,J) =2*K(I,J) +5 20 CONTINUE \n10 CONTINUE could be replaced by the Fortran 90 assignments L=6 K = 2*K+5  Similarly, the loop DO 30 \n1=32 ,64 L(I) = L(I+64) DO 40 J=I,64 K(I, J) = K(I, J)**2 40 CONTINUE 30 CONTINUE can be rewritten in \nFortran 90 as L(32:64) = L(96:128) K(32:64,:) = K(32:64, :)**2 To some extent, then, our compiler has \nhigh-level parallelism pre-packaged for it by the user s choice of Fortran-90 constructs. We can therefore \nconcentrate on mat thing the source s parallelism to that of the tar\u00adget, and avoid the tangle of extracting \nparallelism from serial code. 2.2 Machine Model: Slicewise CM/2 Programming The Connection Machine Model \nCM/2 in slicewise mode 1 consists of up to 2,048 Slicewise Processing El\u00adements (nodes or PEs), each \nconsisting of 32 bit-serial processors coupled with one Weitek WTL3164 64-bit floating-point ALU. The \nPEs are connected by a 11\u00addimensional boolean hypercube with two wires along each dimension. The bit-serial \nnature of the processor set is hidden from the programmer, who sees a conven\u00adtional bit-parallel, word-serial \ninterface to memory and to the Weitek FPU. The programming language designed by the CM For\u00adtran group \nfor this PE abstraction is PEAC (Process\u00ading Element Assembly Code). PEAC allows the Weitek chip to be \nprogrammed as a four-wide vector processor; it also allows accesses to CM memory to be overlapped with \narithmetic operations, and supports the Weitek chained multiply-add instruction. Each slicewise processor \nsynchronously executes in\u00adstructions issued from the CM sequencer. Each instruc\u00adtion processes a vector \nof four 32-or 64-bit elements slices through local memory. PEAC code is only 1as opposedto the more traditionalfieldwise,or \nParismode. used to express purely elemental program segments; that is, under the normal programming \nmodel, there can be only pointwise dependencies between data that are aligned to the same geometry and \nused in the same PEAC routine. When an instruction sequence is ap\u00adplied serially to each of the (potentially \nvery many) elements local to each processor memory, the result is a virtual subgrid loop parameterized \nby a set of strided data pointers and a subgrid loop count. For computations that are not purely elemental, \nbut have some degree of locality or regularity, the CM run\u00adtime system provides a large set of special-purpose \nuniform communication primitives, efficient y imple\u00admented in microcode. When dependencies in user code \nexceed the restrictions of both the elemental and the special-purpose uniform styles, however, general \nrouter communications result, at significant performance ex\u00adpense. For example, the Fortran 90 code fragment \nreal, array (n, n) :: I 1(:,:) = 6.0 might be compiled as a single PEAC node routine stor\u00ading the 32-bit \nscalar value 6.0 to each local element serially, in batches of four. On the other hand, the Fortran 90 \nassignment I(:,2:n) = I(:,l:n-1) might be implemented by the CM runtime system call CMRT_cshif t, and \nthe negative-strided array section assignment 1(:, n:l:-2) = I(:,l:n:2) would be implemented via the \nrouter as general point\u00adto-point communication from I to 1. 2.3 Main Compiler Issues Clearly, the main \ntension in CM code arises between communication and computation. Specifically, given a section of user \ncode, an effective compiler must perform decompositions and transformations to minimize in\u00adterprocessor \ncommunication. Simultaneously, the code generation process should maximize in-processor per\u00adformance \nthrough efficient vectorization. There are, therefore, multiple levels of program optimization to consider. \nThe relatively high cost of data movement forces the compiler to address the high-level challenges of \ndata layout and communication. Simultaneously, those codes that are naturally computation-dominated, \neas\u00adily vectorized due to elemental locality, will benefit most from treatment of processor-level concerns. \nThese two optimization levels are not entirely or\u00ad thogonal. High-1evel compilation strategies for re\u00adducing \ninterprocessor communication, which typically group computations with similar layout, minimize con\u00adversion \nbetween rival layouts, and aggressive y unroll serial loops, all help to maximize the PEAC elemen\u00adtal \ncodeblock size as well. This in turn allows more efficient low-level coding. Node processor complexity, \nwhich continues to track advances in microprocessor technology, will continue to increase this pressure \nfor multilevel optimization. As the CM/ 1 bit-serial processor gave way conceptually to the Weitek-augmented \nabstraction of the CM/2, so too the CM/2 PE will eventually be replaced by the CM/5 s SPARC node, augmented \nwith optional vector hardware. For massively parallel architectures in gen\u00aderal, increases in node sophistication \nwill dramatically increase the need for compilers that can orchestrate strategies for cooperative optimization \nat many levels of target abstraction, from high-to low-level. These, then, are the primary concerns that \na realistic prototype compiler must satisfy: Because the project is primarily a prototyping sys\u00adtem, \nintended to serve as a testbed for research into optimization strategies, minimizing develop\u00adment time \nis critical, as is support for staged devel\u00adopment. Formal specification techniques help sup\u00adport attack \nstrategies for extremely rapid com\u00adpiler development.  The problem therefore requires a framework for \ncompilation that will help apply minimal effort to compiler scaffolding (front end data, serial code, \nand bookkeeping concerns) while optimizing the compiler s critical paths selecting appropriate forms \nof communication, efficient register alloca\u00adtion and effective vectorization of the node code.  The \nfinal goal is to produce compiled object code whose performance is competitive with traditional compilation \ntechniques for massively parallel ar\u00adchitectures, and, in many cases, with equivalent hand-coded solutions, \nfor a fraction of the total development cost. Simultaneously, generated com\u00adpilers must run in reasonable \nspace and time on available workstations, generally within an order of magnitude of hand-coded equivalents, \nif they are to be considered realistic for production use.   3 Theoretical Basis In the Fortran-90-Y \ncompiler we use formal specifica\u00adtion techniques to address the dual challenges of per\u00adformance and development \ntime, using an abstract se\u00admantic algebra with special operators for describing se\u00adFRONT END TARGET-INDEPENDENTCOMPILER(Section4.1) \nSPECIFICATION Optimization include RECOGNIZINGCOMMUNICATIONS DEAD CODE ELIMINATION LAYOUT SPECIFICATION \nLOOP UNROLLING SHAPEBLOCKING LOOP FUSION ,Hpg (AST) . TARGET-GENERIC  /yR)\\\\...------------------------ \n-----.-t ------ TARGET-SPECIFIC TARGET 2: TARGET 1: CM/5 NIR COMPILER CM/2 NIR COMPILER SPECIFICATION \nSPECIFICATION (Section5.3) (Sections5.1. 5.2) lr NODE HOST COMPILER Optimization includeCOMPILER (SPARC \n+ OPTIONAL HOST NODE VECTOR UNITS) VECTORIZATION COMPILER COMPILER (SPARC INSTRUCTIONCHAININGCONTROL \nREGISTERALLOCATIONSPARC VECTOR(SPARC) II(SLICEWISEPE) PROCESSOR) COMPILER DATAPATH 1 COMPILER m  Figure \n1: Fortran-90-Y specification structure rial and parallel iteration to formulate an appropriate machine \nmodel. These operators are collected in the central notation of the Fortran-90-Y compiler specification, \nYale Inter\u00admediate Representation (YR). YR serves as the basis for target-independent program transformation, \nas well as for specific compilation to the SPARC control pro\u00adcessor, the CM/2 slicewise node processor, \nthe CM/5 SPARC node processor, and the CM/2 and CM/5 as a whole. The following sections describe YR and \npro\u00advide some examples of computations expressed in it. Then we describe the actual structure of the \nprototype compiler, and show how compilation to, transformation over, and code generation from YR take \nplace. The overall structure of the compiler, and YR s role in it, are shown in Figure 1. 3.1 YR Core \nLanguage Core YR defines a series of semantic domains and sets of operators within each domain that model \nprogram actions. We will refer to these semantic domains as or\u00adthogonal facets of a semantic algebra \ndefined by the YR operators, to avoid overloading the more usual interpre\u00adtation of the term domain from \nthe parallel process\u00ading literature. A partial listing of YR semantic facets and their operators are \nshown in Figure 2. Together, the core facets define a space of scalar program behav\u00adiors, and YR constructs \nare programs for the abstract machine characterized by these behaviors. Generating a compiler for a specific \nmachine then reduces to the problem of compiling YR programs constructs of this semantic algebra -to \nnative code, by implementing the abstract machine for the intended target. 3.2 YR + Shapes Using only \nthe core YR operators, we could model im\u00adperative languages implemented serially, such as For\u00adtran 77, \nPascal, or Algol. A complete implementa\u00adtion of our operator set for a given target environment would \ncomprise a complete, working compiler. How\u00adever, for the data-parallel extensions of Fortran 90 we require \nthe addition of primitive shaping actions. User code that iterates over data or time, as well as target \nhardware that exhibits both parallelism and sequential restrictions, can be thought of as forms of serial \nor parallel iteration over abstract spaces. We represent such spaces in YR using shapes, a class of primitive \nsemantic operators that model iteration. The basis for shapes in the current work is the basic Carte\u00adsian \nproduct space, although future work may include tree, hypercube, or butterfly domains as shape primi\u00adtives, \nas suggested by previous research into domains and data fields [4]. To integrate these shape operators \ninto scalar YR, bridge actions are added to each of the other seman\u00adtic facets. The additional operators \nare marked with asterisks in figure 2. Types. To model array types, we add the type op\u00aderator df ield: \nS*T, which defines a new type whose shape is S and whose elements are each of type T. T may itself be \na dfield, which may be interpreted as a shape cross-product. Each value action has a declared or inferred \nshape and type, which can be checked stat\u00adically at compile time. Declarations, Because the existing \ndeclaration op\u00aderators allow identifiers to be bound to any ex\u00adpressible type, array declarations just \ntake the form DECL(i, dfield(S, T)). Values. We add a new value-producing operator, AVAR(i, S), which \nreferences array storage bound to identifier i through shape action S. Shape actions used as subscripts \ncan further specialize the declared shape of an array variable, or default to the declared shape using \nthe special shape action all. This allows us to construct array references that are implicitly parallel, \nShapes can also be filled in with integer coordinate val\u00adues using the LOCALLYvalue-constructor. Imperative \nactions. Finally, we model serial and parallel iterations over spatial and temporal domains with the \noperators DO(V, S, 1) and DOALL(V,S, I). These operators carry out the action I at each point in the \nshape action S with either serial or concurrent se\u00admantics, borrowing the storage referenced by the value \nset V for maintaining the loop variables. The body action I may itself be another DO-or DOALL-construct, \nproviding a way to inductively de\u00adfine loops over Cartesian product spaces. We can then begin to define \nsuch standard transformations as loop interchange and loop fusion. Moving data between like shapes is \nhandled normally as part of the general data motion operator MOVE,where the left-and right-hand types \nare dfields with match\u00ading shapes and elemental types, and the storage phase of the movement is executed \nunder an optional mask. TYPE FACET (T) integer.32 logical.32 float.32 float_64 dfield DECLARATIONFACET \n(D) DECL DECLSET INITIALIZED VALUE FACET (V) COMPUTE SVAR SCALAR FUNCALL AVAR LOCALLY IMPERATIVEFACET \n(I) PROGRAM SEQUENTIALLY CONCURRENTLY MOVE MMOVE IFTHENELSE WHILE REF.OUT COPY-OUT WITH SKIP DO DOALL \nSHAPE FACET (s) point gen-interval interval prod-dom dyn all operator signature T T T T S*T->T id* T->D \nD list -> D id* T*ll->D op*v*v -> v id -> V T*s-rep -> V id*(T*V)lLst -> V ld*S -> V S*lnt -> 1$ 1->1 \n I list ->1 I list ->1[ (V*V)list -> I (V*(V*V))l.ist -> I (v*I*I) -:, I (V*I) -> :[ V->1 V->1 D*1-> \n1 I V list*S*I -> I V list*S*I -> I int ->S S*S*~nt -:) S S*S-> s S list -> S V->s s interpret ation \n* Shape augmentation tocore YR 32-bit integer 32-bit logical single-precision floating point double-precision \nfloating point * afield of elements of the given type simple declaration multiple declarations declaration \nplus initial value arithmetic computation scalar variable scalar constant function call * array variable \n * coord arrav constructor  top-level program action sequential composition concurrent composition data \nmovement move under mask classical if-then-else classical while-construct passes call-by-reference parameter \npasses call-by-value parameter execute in extended environment defines (SEQUENTIALLY nil) * execute over \nthe given shape * ..in parallel * single point * strided interval * stride-1 interval *shape cross-product \n * dynamically determined shape * select all elements  Figure2: Partial Listing of YR Domains and \nOperators 99 The Fortran 90 code mentioned earlier, 4.1 Syntactic Analysis INTEGERK(128,64), L(128) \nL=6 K= 2*K+5  might be expressed in full YRas WlTH (DECLSET [DECL( k ,df ield {shape=prod.dom [interval(point \nI,point 128), interval(point I,point 64)], element=integer_32) ), DECL( 1 ,dfield {shape=interval(point \nI,point 64), element=integer-32})1 , SEQUENTIALLY [ ... MOVE[(SCALAR(integer_32, 6 ), AVAR( 1 ,all))], \n HOVE [(CORPUTE(bin Plus, [C014PUTE(bin Hul, [SCALAR(integer-32 , 2 ), AVAR( k ,all)]), SCALAR(integet_32 \n, 5 )]), AVAR( k , all))], ... 1); The program consists of a declaration section, wrapped around two \ndata movements composed se\u00adquentially. The first assigns the scalar integer value 6 to all elements of \narray 1 in parallel; the second com\u00adputes the function 2k+6 for all points in k simultane\u00adously, and \nmoves the result back into corresponding el\u00adementsofk. Note that the all operator isused for the array \nvariable subscript to specialize different shapes, with the meaning specified by context. All thus allows \nus to express elemental parallelism in data movement decoupled from the specific shape associated with \nthe array variables. The target-independent phases of the prototype corn\u00adpiler include modules for syntactic \nanalysis, initial translation to YR and static semantic analysis, and YR source-to-source optimization. \nEach phase is specified as a set of Standard ML modules. Syntactic analysis is performedby afront end \nsystem whose target is a generic AST format. The implemen\u00ad tation of syntactic analysis by way of formal \nspecifi\u00ad cation is well understood, and we elide further details here. The current front end in the Fortran-90-Y \npro\u00ad totype, for example, is a custom subset lexer/parser in the form of a ruleset guiding the accumulation \nof lex\u00ad ical tokens into simple tree form, and the recognition and tagging of anumber ofmultistatement \nconstructs. 4.2 ~tatic Semantic Analysis Following syntactic analysis, the resultant AST is matched \nagainst aset ofproduction rules that guide the construction ofavalid YR program and perform static semantic \nchecking. These rules are grouped accord\u00ading to the semantic facet within which their resultant program \naction is classified. The current prototype in\u00adcludes nine rule sets that govern the transformation of \nASTS into YR program fragments according to their parent semantic facets: right andleft-hand side expres\u00adsions(valuations), \ntypes and shapes, simple and block\u00adstructured statements (imperatives), and variable and constant declarations, \nThe rule sets are mutually recursive, and call each other to fill in the missing parts of parameterized \nac\u00adtions. For example, the DO operator for serial itera\u00adtion is parameterized by a list of value-resources \nthat may be borrowed to serve as loop variables, a shape over which the looping occurs, and a body instruc\u00adtion, \npossibly including the loop variables, to execute at each iteration. The production rule that creates \na DO\u00adstatement is just one clause of the rule set AST-toStmt. One pseudocode form of the rule to produce \na YR DO-statement from an AST encoding a simple lower\u00adbound, upper-bound loop with unit stride is AST-t \no_Stmt ( env, [[do /loopvar/=/lb/, /ub/ /body/]]) => let val bodyaction = AST_to_Stmt (body, env) and \nlvaraction = AST_to_LHS(env, loopvar) and lb_s.hape = AST_to_Shape(env, lb) and ub_shape = AST_to_Shape(env \n,ub) in DO([lvaraction] , interval (lb_shape ,ub_shape) , bodyact ion) end If the user s program contains \nsuch a simple DO\u00adloop, the AST fragment that encodes the loop will be matched against each rule in the \nruleset AST_to_Stmt sequentially. It will trigger this particular clause, which will in turn extract \nthe syntactic subtrees containing the loop bounds (point shapes), the loop index vari\u00adable, and the statement \nthat forms the loop body. The rulesets AST.t o.[Stmt, LHS, and Shape] are used ]re\u00adcursivelyto construct \ntheir semantic values. These val\u00adues are then combined under the DO-operator into a single entity, which \nis returned as the YR compilation of the original AST. Similarly, the LHS-rule that translates unqualified \nidentifiers to assignable variable actions is AST.to-LHS(env, [ [/vname/] ] ) => (case t ypeof (env, \nvname) of df ield{. . . } => AVAR(vname, all) I other => SVAR vname) Note that if the type associated \nwith the variable name in the static environment env is an array type (df ield), the rule assumes that \nthe whole-array refer\u00adence is intended, and produces the correct YR encoding for an array variable used \nas a left-hand side. Finally, the rule that handles simple assignment is just AST-to-Stmt (env, [ [/somelhs/=/somerhs/] \n1 ) => let val lhs = AST-to-LIiS (env, somelhs) and rhs > . AST-to-RHS(env, somerhs) in if types-agree \n(env,lhs , rhs)) andalso shapes=agree(env,lhs ,rhs ) then MOVE[(rhs ,lhs )] else raise AST_to_Stmt-Error \n( lilsmatch in assign ) end Here, the functions types~gree and shapes-agree lookuporderivethe typeandshapeofthe \nsuppliedYR valuation actions and trivially check them for agree\u00adment before allowing the compilation \nto proceed. 2 The final result from this phase should be a single imp\u00aderative action that describes the \nentire user computa\u00adtion, and has been statically type-and shape-checked. No attempts at program optimization \nhave been made. The YR action can then be passed through a series of optimizing transformations, or directly \nto a target\u00adspecific YR compiler for code generation. 4.3 YR Transformations The current Fortran-90-Y \noptimization stage performs a chain of source-to-source transformations over YR Nontrivial shapes, includlng \narrays and intervals with rnntime-determinatebounds, also exist in YR in the form of the dyn operator \nfor dynamic shapes. Certain shape checking operations may therefore be postponed until rnntime. code. \nThis framework supports program transforma\u00adtion within each semantic facet, propagating the effects of \ntransformations through the program by way of YR s bridging operators, where facets meet. The object \nis to produce programs in which computations over like shapes are blocked as much as possible, forming \ncom\u00adputation phases punctuated by communication. These shape-based program transformations allow us to \nexpress standard compilation techniques such as var\u00adious loop transformations, as well as specific goals \nsuch as exploiting communication patterns that are well\u00adsupported by the target. Each is implemented \nas a single Standard ML func\u00adtor, defining a transformation function whose type is YR. statement -> YR. \nstatement. Each has a set of input assertions that must be satisfied, or the trans\u00adformation fails, displaying \na warning of assertion fail\u00adure and passing the input program through unmodi\u00adfied. Like the static semantic \nphase, each transforma\u00adtion functicm is defined as a rule set partitioned along semantic facets. Generally, \neach transform is triggered by a particular context or YR construct, and so can be defined in just a \nfew hundred lines of SML code. The selection and ordering of these optimization modules are currently \nstatically determined at the time the compiler is built to satisfy whatever input asser\u00adtions the back \nend YR compilers may have, and to precondition code for good performance. 4.3.1 Constant propagation \nThe constant propagation module performs simple value tracking to simplify expressions in crucial con\u00adtexts. \nThis is primarily useful in statically resolving otherwise compile-time indeterminate subscript ranges, \nand thus can greatly extend the potential for later op\u00adtimization. For example, if the user codes B(2:npl) \n= A(2:npl) + B(l:n) and variable npl is clearly only used to store the value n+l for some parameter \nn, then rewriting the array expressions to include that information will resolve a dynamic shape checking \ndilemma at compile time. Simple value tracking is accomplished by passing over the input YR program with \nan environment that stores, for each variable, either a current constant val\u00aduation, an uninitialized \nmarker, or an unknown status. This simple tracking does not cross the boundaries of basic blocks. 4.3.2 \nDOALL reduction Unlike the Fortran 90 standard, but like most compil\u00aders that implement a subset of Fortran \n90 features, the 101 Fortran-90-Y prototype implements a FORALLstate\u00adment, though the YR operator is \nnamed DOALL.Per\u00adhaps its most significant use is to allow pointwise com\u00adputations in which the local \narray coordinates partici\u00adpate. The subset of FORALL expressions that involve only local array elements \nand coordinates are detected and reduced by this transformation module to whole\u00adarray expressions that \nuse the YR value-constructor LOCALLY.For example, the Fortran 90 statement forall (i=l:m, j=l:m) c A(i, \nj) = i.*B(i, j)+j where A and B were defined over the common sha~pes equal to (I :m)* ( I: m), would \nemerge from the static semantic phase as the YR fragment DOALL([SVAR i , SVAR j] , DOMAIN S , 140VE \n[COHPUTE(bin Add, [CO14PUTE(binHul, [AVAR(IIBII,prod_dom [dyn(SVAIt 1 ) ,dyn(SVAR j )] ) , SVAR i ] ) \n, SVAR j ]) , AVAR( A , prod_dom [dyn(SVAR i ), dyn(SVAR j )])])  Although theindividual shapes can \nbe inferred, the exact subscript values remain unspecified, leaving com\u00adplicated source and target expressions \nfor the M(DVE. The problematic subscript expressions each exactly match the list of variables that the \nDOALLborrows to store loop indices, triggering the DOALLreduction mod\u00adule. This transformation introduces \nparallel YR tem\u00adporaries of the same shape and distribution as A and B using the YR LOCALLYoperator, \nand reduces this YR fragment to the simpler pointwise computation MOVE [COMPUTE(bln Add, [COMPLJTE(binMul, \n[AvAR( BII ,all), LOCALLY(S,dimenslon 1)1 ) , LOCALLY(S,di.mension 2)1 ) , AVAR( A ,all)]  4.3.3 Communication \nintroduction Expressions that involve mixed communication ilnd computation must be broken into separate \npieces ex\u00ad ecuted serially, often with temporary storage to hold intermediate results. This module passes \nover the in\u00adput YR program, using a rule set to detect such ex\u00adpressions and perform the necessary splitting \ntransfor\u00admations. For example, if the module sees YR code expressing A = B + CSHIFT(B, DIM=2, SXIFT=2) \nsuch as MOVE [COMPUTE(bin Add, [AVAR(IIBII,all), PRIM( cshif t , AVAR( B , all), . ..)1). AVAR( A , \nall)] it converts this to multiple moves with a local declara\u00adtion WITH(DECL( tO ,dfield{shape=S, . . \n.}) , SEQUENTIALLY [MOVE [PRIM( cshift , AVAR( B , all), . ..). AVAR( tO , all)] , MOVE [COMPUTE(bin \nAdd, [AVAR( B ,all),AVARtO ,all)]ll)] ), AVAR( A ,all)]]) that computes the CSHIFTinto locally scoped \nstorage toofthe appropriate shape before performing the ad\u00addition and (presumably) releasing the local \nstorage. Later transformation phases may choose to bring some outside computations within the scope of \nthe generated temporary storage, increasing the length of the elemen\u00adtal code block at the expense of \nkeeping some allocated heap space longer than necessary.  4.3.4 Parallel loop fusion Finally, the loop \nfusion module passes over the YR program, looking for successive MOVEstatements with purely elemental \ncomputation and like shape. Those it finds itgroups together within asingle MOVEset. In the CM/2 YR implementation, \nthis results in these assign\u00adments being compiled into a single PEAC node routine, potentially resulting \nin increased vectorization. This is equivalent to fusing sequences of parallel loops, since in a synchronous \ncontext SEQUENTIALLYIDOALL(V,S,11),DOALL(V,S,12)1 DO DOAL --- ~ (A) is exactly equivalent to DO DOALL(V,S,SEQUENTIALLY[11,121) \n!3 1   5 Fortran-90-Y: Target-Specific Phase 5.1 CM2/YRCompiler The problemof compilinga validYR program \ninto codeforthe CM/2 is broken down into a hierarchy of YR compilers for different levels of target abstracticm, \nThe top-level abstraction (CM2/YR) models the CM/2 host and nodes together as a single machine, and then \npartitions input YR programs into YR subprograms for each half. The source YR program will have been \nrestructured by the optimization phase to consist of blocked compu\u00adtation and communication phases. The \nCM2/YR co]m\u00adpiler passes over this series of blocks, cutting out and compiling a list of the elemental \ncomputation phases. Then it patches the remaining program skeleton with the appropriate YR calling code. \nEach extracted comp\u00adutation phase is passed to the CM/2 node YR com\u00adpiler to be turned into a node procedure. \nThe remain\u00adder is transformed into native frontend code by the CM/2 front end compiler. 5.2 CM2/Host \nand CM2/Node YR Compilers The CM2/Host YR compiler translates the YR remain\u00adder progam into SPARC assembly \ncode plus CM run\u00adtime system library calls. DO-and MOVE-constructsover serial shapes become explicit \nfront-end iteration. Ref\u00aderences to front-end data, along with CM data used in a front-end context all \nbecome front-end code. Declara\u00adtive YR constructs become memory allocations, with their home determined \nby usage. Certain primitive function calls that represent communication intrinsic are replaced by calls \nto their CM runtime library im\u00adplementations. For each computation block being ex\u00adecuted remotely, the \ncompiler inserts calling code to push PEAC procedure arguments over the IFIFO to the node processors. \nThe Node/YR compiler then passes over the chain of extracted loopnests, transform\u00ading each straightforwardly \ninto a single PEAC node routine. An illustration of the code partitioning p]ro\u00adcess can be seen in Figure \n3. The host and node compilers in the current CM/2 implementation represent the point where true speci\u00adfication \nends. They compile YR fragments to native code directly, using hand-coded algorithms to achieve competitive \nperformance. The formalism of YR is still (D.1)  (B~-. D.1,D.2 (c)1 cloned have like (D.2) loopnest \nshapes , -. (A) 1~1 DO (A) (B) WI DOALL DO (c) DOALL El DO DOALL (B) (D) DOALL (D) DOALL. (c) \nDOALL a BD CM/2 HOST CODE cM/2 NODE CODE Figure 3: Decomposition to Host and Node Code present, however, \ninsofar as these implementations are organized internally at the top level as production rule sets. The \nCM2/Host compiler, for example, defines functions compile-[i, v, d] action that compile YR fragments \nfrom the imperative, valuation, and declar\u00adative semantic facets, respectively, to SPARC assem\u00adbly code. \nThe CM2/Node compiler performs a similar function, translating a narrow set of pointwise parallel loop \nconstructs into optimized slicewise PE assembly code. Pushing the level of specification down into the \nFE and PE compilers is an ongoing process, guided by this facetwise organization. Recall that earlier \ncompiler stages defined the trans\u00adlation of AST fragments to YR fragments, and syn\u00adthesized a single \nYR imperative program from them. The backend rulesets perform componentwise decom\u00adposition, rather than \nsynthesis, of the constructed YR imperative, producing an equivalent program in the target language. \nHaving these separate compilers for the host and nodes allows effort to be expended intelligently to \nbring the prototype up to speed. In codes that contain a high degree of data parallelism, the node processor \nand runtime libraries speeds are the limiting factor for per\u00ad formance. The SPARC front end can easily \nkeep up, feeding PEAC code and data through the sequencer and calling the runtime system. As problem \nsize in\u00adcreases, front end time comprises a negligible fraction of the overall execution profile. This \nallows SPARC front-end performance questions to be postponed. The current front-end YR implementation \nuses a simple memory-to-memory load/store model with little atten\u00adtion to effective register use or delay \nslot filling. Al\u00adlowing the host and node implementations to be fully separated gives us the freedom \nto filter out and focus on the performance-critical parts of the prototype com\u00adpiler instead. The prototype \nCM/2 node compiler is tuned for op\u00adtimizing the virtual subgrid loop over the local data in each processor. \nSince the role of the processing ele\u00adments is limited in this programming model to purely local, pointwise \ncomputations, the set of YR programs that the node compiler accepts can be restricted. Each computation \nburst is derived from computations over like shapes in the source program. The compiler there\u00adfore only \nneeds to be able to to process procedures whose body is a single loop containing a sequence of (optionally \nmasked) moves from the local points of source arrays to the corresponding points in the tar\u00adget. The \nnode YR compiler emits an optimized PE assembly code routine for each subgrid loop passed to it by the \nCM/YR code partitioned. After the host and node compilers have reduced their YR source programs to SPARC \nassembly code and PEAC, their output is then compiled and linked nor\u00admally, and can be executed on any \nCM/2 equipped with Weitek 64-bit FPUS. 5.3 CM5/YR Compiler The initial CM/5 YR compiler retains the majority \nof its structure and, therefore, its specification from the CM/2 version. To use the CM/5 in SIMD mode \nto run codes written for the CM/2, the Fortran-90-Y proto\u00adtype shares all code in common up to and including \nthe CM/2 whole-machine YR compiler, which splits a single program into host and node code. The CM/5 host \ncompiler required less than 200 lines of additional code to handle both machines front end requirements. \nA CM/5 node compiler that translates old node YR to regular C programs similarly allows CM/2 node code \nto compile transparently to the new machine. As a result, the total additional time required to construct \nthis preliminary CM/5 prototype compiler was less than two weeks. Compiling to a CM/5 augmented with \noptional vec\u00adtor hardware at each node will present more target lev\u00adels due to the increased node complexity. \nIn the new model a single YR program will be split three ways rather than two; one part will go to the \ncontrol pro\u00adcessor, as before; a second part will be executed on the SPARC node processor, and a third \npart will carry out floating point vector operations on the CM/5 vector datapaths. This structure is \nrepresented in the sec\u00adond back-end diagram of Figure 1. Most importantly, the new system will still \nbe able to take advantage of the machine-independent block-lengthening YR trans\u00adformations defined in \nthe common compiler modules.  5.4 Other Computation Moclels There are, in practice, no reasons why \nthe compiler should adhere to a single, restrictive programming model at the expense of flexibilityy. \nFor example, many codes would benefit from the ability to occasionally break the CM/2 s virtual processor \nruntime model, restricted to pointwise locality and subgrid looping. A more flexible model would allow \nthe compiler to pipeline communication and computation, or perform general neighborhood computations \ndirectly, using the full register set to store intermediate results and per\u00adforming physical communications \nas required. Likewise, a more flexible compiler will be able to take advantage of the MIMD features available \nin the CM/5 and other machines. Implementing new programming models only requires the specification of \nnew host and node YR compilers, and adjustments to the top-level compiler to make use of them. The YR \nsource trans\u00adformation stage would also benefit from extra modules to provide services from the runtime \nsystem previously taken for granted, such as explicit data layout.  6 Experimental Results The basic \nspecification methods were tested in the Fortran-90-Y compiler prototype, which we regard as realistic \nby the following standards: Development time. The current work began in July 1991, with initial research \ninto the slicewise CM programming model, the Fortran-90 language, and the CM Fortran compiler. Implementation \nof the pro\u00adtotype compiler actually began in mid-August. The CM/2 compiler began generating code for \nsimple test programs by late October, and was compiling bench\u00admark code by November. We gained access \nto a work\u00ading CM/5 in February, and began compiling simple codes to it as well shortly thereafter. Direction \nof Effort. By modeling the CM with a unified phase (CM/YR), we recognized that the ma\u00adchine taken as \na whole is still a useful abstract target for compilation. But by dividing the labor of compi\u00adlation \nbetween the host and node compilers, impos\u00ading restrictions on the constructs each accepts, and optimizing \nthe compiler s critical path first, we were able to develop a competitive system for multiple tar\u00adget \nmachines, and an interesting subset of the source language, in less than eight months. Compiler Performance. \nThe time and space re\u00adquirements for building and using generated compilers have been traditional obstacles \nto their acceptance fc,r production use. By constrast, on a moderately loaded Sun 4/390 with 32 MB of \nphysical memory, the cur\u00adrent prototype CM/2 compiler can be generated from specification source code \nin about ten minutes. During actual development, rebuilds generally take much less time -on the order \nof two or three minutes -due to specification modularity and Standard ML s separate compilation facilities. \nThe resultant CM/2 compiler image is just over one megabyte in size; this compares favorably with other \nlanguage tools available for the Connection Machine. Object files produced by the gene\u00adrated prototype \nsystem are, on average, close in size to those produced by CM Fortran. Compilation rates are generally \nwithin twenty percent of CMF as well.3 Object Code Performance. The initial benchmark was an updated \nFortran-90 version of a dusty deck code to implement a meteorological model, the shallow\u00adwater equations, \nor SWE. It has good locality, consist\u00ading of a series of circular shifts interspersed with blocks of \nlocal computation, and so represents an ideal prob\u00adlem for a SIMD, data-parallel machine like the CM/2!. \nA hand-coded *Lisp version of SWE running under fieldwise mode peaked at 1.89 gigaflops. The slicewise \nCM Fortran compiler (v1.1) reached an extrapolated 2.77 gigaflops. The prototype Fortran-90-Y compiler, \nafter the first eight months of development, produced code that attained a competitive sustained rate \nof 2.71 gigaflops. The initial CM/5 prototype, using untuned C program stubs for expressing node code, \nstill attains roughly 85% of the gigaflop rate of CM Fortran on the CM/5. Conclusions In this paper \nwe have presented a brief overview of an approach to parallelizing compiler design incorporat\u00ading concepts \nfrom both formal specification and tradi\u00adtional high-performance computing. The original moti\u00advation \nof this work was to demonstrate that developing scalable, portable, competitive compilers from specifi~\u00adcation \nfor massively parallel computing environments was feasible, in terms of reasonable development time and \ncompetitive performance. The achievements of the current Fortran-90-Y protc~\u00adtype implementation suggest \nthat these methodologies are on their way to being generally applicable to the real-world challenge of \ndeveloping compilers for con-i\u00adplex parallel targets at reasonable cost. sExclusive of finf@ time; as \nlinking can comprise two-thirds or more of overall compile time, practical compilation rates are thus \nalmost identical. 105 8 Acknowledgements The authors thank Young-Ii Choo for his advice re\u00adgarding earlier \nversions of YR, and Yu Hu for his many comments and technical advice. Also, thanks to Woody Liechtenstein, \nBob Millstein, and Gary Sabot of Think\u00ading Machines for their help with CM Fortran, CM/RT, and the slicewise \nprogramming model. Finally, special appreciation is due Lennart Johnsson, also of Think\u00ading Machines, \nfor his help in formulating the problems to be addressed and information on all aspects of the CM.  \nReferences [1] Cikl Fortran Reference Manual, 1991, [2] Fortran 90 Standard, 1991. [3] M. Bromley, S. \nHeller, T. McNerneyl and G. Steele Jr. Fortran at ten gigaflops: The connection ma\u00adchine convolution \ncompiler. ACM SIGPLAN 91 Conference on Programming Language Design and Engineering, 26(6):145-156, 1991. \n[4] Marina Chen, Young-ii Choo, and Jingke Li. The\u00adory and pragmatic of generating efficient parallel \ncode. In Parallel Functional Languages and Com\u00adpilers, chapter 7. ACM Press and Addison-Wesley, 1991. \n[5] Peter Lee. Realistic Compiler Generation. MIT Press, 1988. [6] K. Pingali, M. Beck, R. Johnson, M. \nMoudgill, and P. Stodghill. Dependence flow graphs: An alge\u00adbraic approach to program dependencies. In \nJ3igh\u00ad ieenth Annual ACM Symposium on Principles of Programming Languages, pages 67-78, 1991. [7] S. \nTjiang, M. Wolf, M. Lam, K. Piper, and J. Hen\u00adnessy. Integrating scalar optimizations and par\u00adallelism. \nIn Fourth Workshop on Languages and Compilers for Parallel Computing, pagesc1-c16, 1991.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Massively parallel architectures, and the languages used to program them, are among both the most difficult and the most rapidly-changing subjects for compilation. This has created a demand for new compiler prototyping technologies that allow novel style of compilation and optimization to be tested in a reasonable amount of time.</p><p>Using formal specification techniques, we have produced a data-parallel Fortran-90 subset compiler for Thinking Machines' Connection Machine/2 and Connection Machine/5. The prototype produces code from initial Fortran-90 benchmarks demonstrating sustained performance superior to hand-coded Lisp and competitive with Thinking Machines' CM Fortran compiler. This paper presents some new specification techniques necessary to construct competitive, easily retargetable prototype compilers.</p>", "authors": [{"name": "Marina Chen", "author_profile_id": "81451599623", "affiliation": "", "person_id": "PP39074541", "email_address": "", "orcid_id": ""}, {"name": "James Cowie", "author_profile_id": "81100273965", "affiliation": "", "person_id": "PP31073366", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143122", "year": "1992", "article_id": "143122", "conference": "PLDI", "title": "Prototyping Fortran-90 compilers for massively parallel machines", "url": "http://dl.acm.org/citation.cfm?id=143122"}