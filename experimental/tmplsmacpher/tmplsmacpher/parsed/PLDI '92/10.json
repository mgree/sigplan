{"article_publication_date": "07-01-1992", "fulltext": "\n Escape Analysis on Lists Young Gil Parktand Benjamin Goldberg Department of Computer Science Courant \nInstitute of Mathematical Sciences New York University 251 Mercer Street, New York, N.Y. 10012 Abstract \n Higher order functional programs constantly allocate objects dynamically. These objects are typically \ncons cells, closures, and records and are generally allocated in the heap and reclaimed later by some \ngarbage col\u00adlection process. This paper describes a compile time analysis, called escape analysis, for \ndetermining the lifetime of dynamically created objects in higher or\u00adder functional programs, and describes \noptimiza.tious that can be performed, based on the analysis, to im\u00adprove storage allocation and reclamation \nof such ob\u00adjects. In particular, our analysis can be applied to programs manipulating lists, in which \ncase optimiza\u00adtion can be performed to allow cons cells in spines of lists to be either reclaimed immediately \nor reused without incurring any garbage collection overhead. In a previous paper on escape analysis [10], \nwe had left open the problem of performing escape analysis on lists. Escape analysis simply determines \nwhen the argw ment (or some part of the argument) to a function call is returned by that call. This simple \npiece of in\u00adformation turns out to be sufficiently powerful to al\u00adlow stack allocation of objects, compile-time \ngarbage collection, reduction of run-time storage reclamation overhead, and other optimizatious that \nare possible *This research was funded in part by the Nationaf Science Foundation (CCR-8909634) and by \nDARPA/ONR (NoO014\u00ad90-1110). tpre~ent address School of Computer Science, University of Windsor, 401 Sunset \nAve, Windsor, Ontario, Canada N9B 3P4, Email: ypark@cs.uwindsor.ca Permission to copy without fee all \nor part of this material is granted provided that the copies are not mada or distributed for diract commercial \nadvantage, the ACM copyright notice and tha titla of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0-89791 \n-476-7 [92/0006/01 16,..$1.50 when the lifetimes of objects can be computed stati\u00adcally. Our approach \nis to define a high-level non-standard semantics that, in many ways, is similar to the stan\u00addard semantics \nand captures the escape behavior caused by the constructs in a functional language. The advantage of \nour analysis lies in its conceptual simplicity and portability (i.e. no assumption is made about an underlying \nabstract machine).  1 Introduction Higher order functional programs constantly allocate objects dynamically. \nThese objects are typically cons cells, closures, and records and are generally allocated in the heap \nand reclaimed later by some garbage col\u00adlection process. Garbage collection overhead can be reduced by \nperforming compile time analyses that al\u00ad the following optimizations to be performed: Sia ck allocation: \nObjects that would otherwise be allocated in the heap and then reclaimed us\u00ading garbage collection are \nallocated in an activa\u00adtion record on the stack and automatically (and cheaply) reclaimed when the activation \nrecord is popped off the stack. In-p/ace Reuse: When objects (such as cons cells) are no longer needed, \nthey can be reused directly by the program without invoking the garbage collector. A typical example \nof this is when a function constructs a new list by destructively modifying the cons cells that were \ncontained in an argument to that function. Block Allocation/Reclamation: A number of ob\u00adjects (such as \nthe cons cells of a list) are allo\u00adcated together in a contiguous block of memory. Eventually, the whole \nblock is put on the free list, rather than the individual objects, This al\u00adlows reclamation of larger \nsegments of memory, The first point reflects our inability to determine pre\u00adcisely, without actually \nrunning the program, which individual cells of a list might escape. To form a  spineb-w b-w  3rd? \nw!-T*** Figure 1: Spines of a List and reduces run-time overhead by avoiding the traversal of the individual \nobjects (in a mark\u00ad sweep collection, for instance). In this paper, we present a compile time analysis \nthat provides sufficient information to perform all of these optimizations on higher order functional \npro\u00adgrams. The analysis is called escape analysis. In a previous paper [10] we described an escape analysis \nfor non-list objects, such as closures, and left open the problem of performing the analysis in the pres\u00adence \nof lists. This paper solves that problem. Escape analysis answers a simple question: Given a function \napplication, does a parameter (or some part of a pa\u00adrameter) get returned in the result of the application? \nIf so, the parameter is said to escape from the applic\u00adation. If the escaping parameter is a list, we \nwou Id particularly like to know which spines of the paranle\u00adter escape. The spines of a list are a way \nof describing substructures of a list and are defined as follows: Definition 1 (Spines of a list) Given \na lid L and some i ~ 1, the top ith spine of L is de$ned as the set of cons cells accessible by a sequence \nof operations consisting of car and cdr where the number of occur\u00adrences of car is (i 1). Similarly, \ngiven a list L with d spines and some j >1, the bottom jth spine of L is dejined as the top (d j+ 1) \nspine of L. In Figure 1 we illustrate what we mean by the spines of a list. Naturally, an empty list \n(nil) and any non-list objects have no (or zero) spine. We havechosento analyzetheescapepropertiesof \n lists in terms of their spines for two reasons: . It is an approximation to the run-time behavior that \nallows a compile-time analysis. . It reflects the programming style commonly used for strongly typed \nlanguages, such as ML, in which lists are homogeneous (all elements have the same type) and functions \n(such as append, map, etc. ) often operate over complete spines of lists.  terminating compile-time \nescape analysis, one must choose an approximation of program behavior. The second point reflects our \nbelief that the spines are a good choice of approximation, since the cells of each spine of a list tend \nto be treated identically. Many functions, such as append, reduce, map, length, etc., operate on all \ncells of a spine. Many other functions have the form: fL= if predicate(car L) then,... else f (cdr L) \n or fLx =ifx =n then . .. else f (cdr L) (arith-op x) In general, it is impossible to determine at compile \ntime when the recursion will bottom out. One simply has to assume that all cells in the spine of the \nlist L will be visited. Consider, for example, the following program: let map f 1 = if (l=nil) then nil \nelse cons (f (car 1)) (map f (cdr 1)) ; pair x= [car X, car (cdr x)] ; in map pair [[1,2 ],[3,4], [5,6]] \n An escape analysis would allow us to determine the following properties of the program at compile time: \n1. The top spine of pair s parameter does not es\u00adcape from pair, only some elements do. 2. The top spine \nof map s parameter 1 does not es\u00adcape from map, and the elements of 1 escape from map to the extent they \nescape from the unknown function f. 3. In the call (map pair [[1,2], [3,4], [5,6]]), the top two spines \nof the second argument of map do not escape.  This means that (at least) the following optimizations \ncould be performed: . Stack Allocation: The spine of the list [ [1,2] , [3,41, [5,6]1and the spine of \neach el\u00ad ement of the list could be allocated in the acti\u00advation record for map. Thus, when map returns, \nthe cons cells of those spines would disappear. It seems strange to allocate a list in a stack, but there \nis really no reason not to (as long as we keep in mind the safety considerations outlined in [6]). e \nIn-place Reuse: If the parameter 1 of map is un\u00adshared (sharing information can also be obtained from \nescape information), we can recycle the cells of the spine of 1 within map to be used in the spine of \nthe result (since I s spine does not es\u00adcape). The call to cons inside map can reuse the first cell in \nthe spine of 1 for the result. pair can reuse the spine of x in its result. e Block Allocation/Reclamation: \nIf neither of the above optimization are used, then the top two spines of the list [ [I, 21, [3,41 , \n[5,6] 1 can be allocated in some block in the heap. When map finishes, that whole block can be placed \non the free list, freeing all the cons cells without travers\u00ading the list. This paper describes an escape \nanalysis on lists in higher-order, polymorphically-ty ped functional pro\u00adgrams, and describes optimizations \nand another anal\u00adysis (sharing analysis) that can be performed based on this analysis to improve storage \nallocation and reclamation of such objects. Our approach for lists could be applied to other data structures \nsuch as t{l\u00adples, trees, etc. Related Work There has been a number of papers describing anal\u00adyses for \noptimizing storage of lists and other struc\u00adtures. Most of these analyses have been first-order (i.e. \nnot accounting for higher order functions) or have analyzed first-order languages. Brooks, Gabriel, and \nSteele [3] describe an escape analysis for num\u00adbers, but do not extend it for arbitrary objects. Rug\u00adgieri \nand Murtagh [18] describe a lifetime analysis for a language with side-effects and complex data structures, \nbut, again, it is first order. Jones and Le Metayer [13] describe an algorithm, based on for\u00adward and \nbackward analysis, for detecting sharing of objects in first order functional languages, and de\u00adscribe \na method for reusing of cells based on the shar\u00ading analysis. We use only forward analysis providing, \nperhaps, a simpler conceptual framework for higher\u00adorder languages. Inoue, Seki, and Yagi [12] describe \nan analysis for functional languages to detect, and reclaim, run-time garbage cells based on formal lan\u00adguage \ntheory and grammars. The paper focuses only on the explicit reclamation of cons cells and it is un\u00adclear \nthat this approach could be extended for higher\u00adorder languages. Besides being higher order, the es\u00adcape \nanalysis described here is a more general lifetime analysis that can be applied to objects other than \nlists, and other optimizations are supported. Chase, Wegman, and Zadeck [7] describe an first-order anal\u00adysis \nfor LISP that constructs graphs representing pos\u00adsible list structures and analyzes the graphs for possi\u00adble \nstorage optimization. Our analysis, in contrast, concentrates on typed languages such as ML and ben\u00adefits \nfrom a type system that restricts the ways that lists can be created (for example, one cannot say f = \ncons (x, x)) and the sharing that can occur within a list. Orbit, an optimizing compiler for Scheme, \nuses a simple first-order escape analysis to stack allocate closures [15]. Other analyses for optimizing \nstorage allocation were proposed in [14, 5]. Deutsch [8] presents a lifetime and sharing analysis for \nhigher order languages. While the goals of the pa\u00adper seem to be similar to ours, the approach is very \ndifferent. The analysis consists of defining a low-level operational model for a higher order functional \nlan\u00adguage, translating a program into a sequence of op\u00aderations in this model, and then performing an \nanal\u00adysis to determine the lifetimes of dynamically cre\u00adated objects, The approach is also one of collecting \ninterpretation, in that it analyzes a whole program to infer properties of program points. Our approach \nis to define a high-level non-standard semantics that in many ways is similar to the standard semantics \nand captures the precise escape behavior caused by the constructs in a functional language. We then de\u00adfine \nan abstraction of these semantics which provides less precise information but which allows the analy\u00adsis \nto be performed at compile time. The advantage of our analysis lies in its conceptual simplicity and \nlower computational cost (compared to a collecting interpretation). Baker [2] describes an interesting \napproach to higher-order escape analysis of functional languages based on the type inference (unification) \ntechnique. The analysis sketched there provides escape informa\u00adtion on lists only, and might be extended \nto give infor\u00admation comparable to what our analysis gives. Our analysis, based on abstract interpretation, \nprovides escapement about other objects (closures) as well as lists. We also describe how to use escape \ninformation for various optimizations. The notion of spines of a list is used in [17] as a way of modeling \nthe location of dynamically created references in a list structure. 3 Escape Semantics 3.1 The Language \n-nlnl We clefine a simple, strict, monomorphically-typed, higher-order functional language. For lack \nof a better name, we will call this language nml, for not much of a language. The syntax of nml is defined \nas follows: cE Con Constants (including primitives) ={...l,O,l,l, true,false,lse, +, -, =, nil, cons, \ncar, cdr} xEIcl Identifiers ec Exp Expressions, defined by e ::= c I % I elez I lambda(x).e I if el \nthen ez else e3 I letreczl =el; . ..$n=en ine pr EPgm Programs, defined by pr ::=letreczl =el; . ..xn \n=en ine For convenience, we omit type declarations. Our analysis assumes that monomorphic type inference \n(perhaps requiring declarations) has already been performed. Subsequent ly, we will relax the monomor\u00adphic \ntyping restriction and show that our analysis works for a polymorphic version of nml. Also for convenience, \nwe will allow function definitions of the form~ Z1 . . . .z~ = e and assume that this is just syrl\u00adtactic \nsugar for ~ = lambda(%l). . . . lambda(zn ) .e. 3.2 An Exact Escape Semantics We first define a non-standard \nsemantics of nml, called the escape semantics, such that the meaning of an expression is exact information \nabout what es\u00adcapes in the result of the expression. Since nml is a higher order language, the result \nof an expression may be a function. Such a function, represented by a closure, has two important characteristics \nwith re\u00adspect to the escape semantics: 1. The closure is an object itself. We may be in\u00adterested in whether \nthe closure escapes or not, or we may be interested in another object that is captured (bound) within \nthe closure. Thus, the value of an expression returning a function must indicate whether an interesting \nobject has escaped, 2. A function value may be applied to arguments (which may themselves escape from \nthe applica\u00adtion). Therefore, in our non-standard semantics, the escape value of a function must include \nits behavior as a function.  Thus, the value of an expression in our non-standard escape semantics \nis an element of a non-standard es\u00adcape domain and must have two components. First, it must contain information \nabout what is contained within the result of the expression. Seconcl it must contain a function over \nthe values in the escape do\u00admain. This approach (a two component value) was used by Hudak and Young for \nperforming higher or\u00adder strictness analysis [11]. We perform escape analysis on each argument of a function \ncall separately. Thus, at any time we are only interested in whether or not a single object es\u00adcapes. \nOther objects may escape in the result of a function call, but are ignored by our analysis. An object \nis interesting if it is the one whose escape be\u00adhavior we are trying to determine. If a function has \nn parameters, then we perform escape analysis n times, each time treating a different parameter as interest\u00ading. \nThus, our escape semantics is defined in terms of interesting objects; The value of an expression in \nour escape semantics will indicate if some portion of, or all of, an interesting object is cent ained \nin the result. Given an expression, we want its corresponding value in the escape semantic domain to \ntell us how many spines (if any) of an interesting object are re\u00adturned by that expression. Values in \nour semantic domain De have two components; The first compo\u00adnent is an element of the basic escape domain \nBe, which is a domain of pairs ordered as follows: (o, o)~(l, o)~(l,l) g... ~(l,l)c(l,d)d) where dis \nsome integer constant, i.e. for a = (al, az) ~B. and b=(bl, b2)CB,, a~biffal<blanda2~ 62. The constant \nd is fixed for each program on which our escape analysis is performed. Each expression in the program \nreturns a value whose first component has the following meaning: . (1, i) : The bottom i spines of an \ninteresting ob\u00adject is contained in the value of the expression. (If an interesting object is not a list \nthen i will always be O, which means that an indivisible in\u00adteresting object is contained in the value \nof the expression. ) . (O, O) : No part of any interesting object is con\u00adtained in the value of the \nexpression.  The second component is a function in (D, -De). The escape semantic domains are defined \nas follows (in the style of [4]): @ it = B, x {err} @OOf = Be x {em} D:1472 = Be X (D: ---+D~) D; l,,t \n= (B. x {err})+ (D: xD: ) where err is a function (weaker than all others) that can never be applied. \nDe escape semantic domain = ZD: E71v, = I; --+ De escape environment Given an xc D, we use the notation \nz(l) and x(z) to refer to the first and second elements of c, respec\u00adtively. The two components of the \nfirst element of z are referred to as x(l)(l) and x(l)(a), respectively (i.e., x haa the form ((x(l)(l), \n$(1)(2)), z(2)). The escape semantic functions are: C : Con+D. E : Exp -+Enve + De  P: Pgrn+ D. The \nsemantic function C for constants is defined as follows: C [c] = ((O, O), err), where Ce{. .c, o, l,..., \ntrue, false, nil} C[C] = (( O, O), XC.(a(l), ~g.((o,o),er~))), where c ~ {+, ,=} C[consj = ((O, O), ~z.(x(l), \n~v. pair(~, u))) C[car] = ((O, O), kc. fst(z)) C[cdr] = ((O, O), Ax. snd(z)) where pair(z,y)=(z, y), \nfst($)=~(l), and snd(z)=a(z). The escape semantic function E for expressions and P for programs are defined \nas follows: E[c]enve = C[c] E[z]enve = env, [x] II[if el then ez else es]env, = if oracle(el) then E[ez]enve \nelse l?[es]env~ In order to return the actual escape value of each ex\u00adpression, we must be able to determine \nwhich branch of the conditional primitive if-then-else would be evaluated at run-time. Here, for convenience, \nwe in\u00adstead resort to an oracle to choose the appropriate branch of the if. This can be otherwise be \naccom\u00adplished by having the escape semantics directly com\u00adpute the standard meaning as well as escape \nmeaning of expressions. E[ele2]enve = (E[el]env,)~2) (E[ez]enve) E[lambda(x).e]enve = (V, Jy..E[e]env.[z \n+-+y]) where V = (0,0) I-I( U (en~e[z])~l)) ~eFnOn-1 j~ U(u(u P(l))) ~~~~ st p in (enue[z]) Here, p \nin (enu, [z]) denotes that p is an escape pair in enve [.z], Fn n- ist is the set of non-list type free \nidentifiers in lambda(z). e, and F [i t is the set of list type free identifiers in lambda(z). e). The \nescape value of a lambda expression reflects whether an interesting object is contained in the resulting \nclosure as a free identifier as well as its functional behavior when it is applied. Note that free identifiers \nare treated sepa\u00adrately according to whether they are of a list type or a non-list type. E[letrec .zl \n= el; . . .Zn = en; in e]enve = E[e]env~ where env~ = envc[zl * E[el]env~, . . . . x. ++ ll[en]env~] \nP~r] = l?~r]nuiienv. nullenv. is a escape environment that maps every identifier to the least element \nof its escape semantic domain.  3.3 Correctness One naturally wonders what the relationship between \nthe standard semantics and our non-standard escape semantics is. This is especially important if one \nwants to prove that the information provided by escape analysis is correct. However, it should be noticed \nthat our escape analysis strives to gain information about the run-time behavior of a certain implemen\u00adtation \nthat uses a stack and a heap and uses aliasing, rather than copying, of aggregate objects. Therefore, \nit is an operational semantics (perhaps couched in denotational selmantic terms) of which our escape \nse\u00admantics can be considered an abstraction. Although we do not, have space in this paper to provide \nthe op\u00aderational definition of our abstract machine, we can give such a definition and prove correctness \nproperties of our analysis with respect to that definition. To be complete, of course, we would have \nto prove that the abstract machine implements the standard semantic definition of the language.  3.4 \nAbstraction of the Escape Seman\u00adtics The escape semantics presented in the last section specifies exact \nescape information about functions. But, it is not suitable as a basis for compile time analysis because \nconditionals cannot be evaluated at compile time and the subdomain of escape values for lists is infinite. \nIn this section, we present a safe and computable but less complete abstraction of the exact escape se\u00admantics \nthat allows an approximation of the exact escape behavior to be found at compile time. We modify the \nmeanings of elements in the basic escape domain 1?,. We represent lists as finite objects by combining \nthe escape values of all the elements into a single value. We also modify the escape semantic functions \nfor constants and expressions to approxi\u00admate the escape behavior of expressions by assuming that both \nbranches of a conditional could be taken. The interpretation of elements of the basic escape domain \nB. is modified as follows: . (1, i) : The bottom i spines of an interesting ob\u00adject may be contained \nin the value of the expres\u00adsion. (If an interesting object is not a list then i will always be O.) . \n(O, O) : No part of any interesting object is con\u00adtained in the value of the expression.  The escape \nsemantic subdomain D; i t for lists of type ~ list is modified as follows: D; /iSt = D; The escape semantic \nfunction C for constants associ\u00adated with lists is modified as follows: C [nil listn = J-r (1, is the \nbottom element in D;) C [cons] = ((O, O), ~z.($tl], ~U.@U Y)) C[cars] = ((O, O), kz.subs(m)) where sub \n(z) = if (z(1)(2)= s) then (z(l)(l), z(I)(2) 1, z(2)) else z C[cdr] = ((O, O), h.z) The typed constant \nfunction cars is applied to a list that has s spines. The value of s may be arbitrary large, but fixed \nat compile-time. For each car in a program, s can be statically determined by type checking or inference \nbecause our language is stati\u00adcally typed. If that list contains the bottom n spines of an interesting \nobject then the first component of the list s escape value will be (1, n). There are two possible results \nwhen cars is applied: . Ifs = n, then the nth spine (from the bottom) of the interesting object is part \nof the top spine of the list. Thus, taking the car of the list returns an object containing at most n \n 1 spines of the interesting object. Thus the result should have the value (l, n 1). . If s > n, then \nthe nth spine (from the bottom) of the interesting object is not part of the top spine of the list. Thus, \napplying car to the list returns a list that could contain the n spines of the interesting object.  \n Notice that s cannot be less than n, since a list with s spines cannot contain a list with more than \ns spines. The escape semantic function E for expressions of if-then-else and lambda(x) . e is modifiecl \nas fol\u00adlows: ~[i.f el then ez else e3]enve = (E[ez]erzv,)(zl U (E[e3]enve) The abstraction for the conditional \nexpression if no longer makes an appeal to the oracle, but rather takes the least upper bound of the \nescape values of both branches. E[lambda(@).e]env, = ~(i~ey.E[e]env,[z w y]) V = (0,0) L.I( u (env.[zl)(l)) \nZCF Here, F is the set of all free identifiers in lambda(x) .e. Notice that this abstract definition \nis considerably simpler than the exact escape semantic function. 3.5 Safety and Termination The safety \nof interpretation under the abstract escape semantics with respect to the exact escape semantics means \nthat whenever an object escapes under the ex\u00adact escape semantics it escapes in the abstract escape semantics. \nThis can be proved by showing that, using the principle of abstract interpretation, the abstract escape \nsemantics is a safe abstraction of the exact escape semantics [16]. The termination of interpretation \nunder the ab\u00adstract escape semantics can be proved as follows: Since mnl functions are recursive, their \nvalues in the escape domain may be computed by the usual fixpoint iteration. The body of the functional \ncor\u00adresponding to a recursive function is composed of the monotonic least upper bound operator and other \nmonotonic operators. Thus each functional is mono\u00adtonic and its fixpoint can be found in finite time \nif the domain is finite. For each type ~, we defined a finite domain D:. When computing the fixpoint \nof a functional of type ~ ~ r, we need only iterate over values in the finite domain D;. Thus, the fixpoint \niteration terminates.  4 Escape Test on Lists We use the abstract escape semantics to infer escape properties \nof programs. Below we show how it is used to determine both the global and local escape proper\u00adties of \nprograms. Global escape analysis is performed on a function definition, and thus gives general infor\u00admation \nabout the escape properties of that function in any possible application. Local escape analysis de\u00adtermines \nthe escape behavior of a particular function call, ancl yields more specific results. 4.1 Global Escape \nTest In global escape analysis, we find escape information about a function f in an nml program that \nholds true for every possible application off. To do so, we apply the abstract escape semantic value \noff to arguments that cause the greatest escapement possible. Definition 2 (Worst-case escape function) \nFor each non-!ist type r, we define the function WT that corresponds to an nml function from whtch every \nar\u00adgument escapes: W = kZ1.(XlflJ, ~t2.(~1(1) u ~2(1)J ....kcm.(flz~(l),err) . . .)) form >1 and W = \nerr i=l for m = O, where m is the number of arguments that a function of type T can take before returmng \na prim\u00ad itive value. For each list type r list, W r i t is defined to be WT. Given a function ~ of n \narguments, the position i of an interesting parameter, and the abstract es\u00adcape semantic environment \nenve mapping f to an ele\u00adment of the abstract escape semantic domain De, the global escape test function \nG(f, i, enve ) determines how much of the ith parameter of f could possibly escape f globally. It is \ndefined as follows: G(f, i, enve) = (E[f z, . xn] env. [z, ++ Vi])(l) where yi = ((1, s,), W ),s,is \nthe number of spines of the ith parameter off (if it is a list type, otherwise s, is O), ri is the type \nof the ith parameter of f, and foi\u00adallj < n and j # i, y~ = ((O,O), W~~), r~ is the type of the jth parameter \nof f. Note that the whole ith argument with si spines is interesting, and any other argument is not interesting. \nThen, from the result of the global escape test function, we can conclude as follows: o If G(j, i, enve \n) = (0,0) then we conclude that none of the iih argument escapes f in any possi\u00adble application of j \nto n arguments. o If G(~, i,enve) = (1, k) then we conclude that, ifsi ~ 1then, the top (s; k) spines \nof the ith argument do not escape f in any possible ap\u00adplication off to n arguments, but the bottolm \nk spines of the ith argument could escape f in some application of f to n arguments. (If si = O then \nthe ith argument, which is not a list type, couid escape in some application off to n arguments.)  4.2 \nLocal Escape Test Generally, we would like to know if an argument, es\u00adcapes from a particular call to \na function f. This depends on the values of the arguments of that call. Given a function of n arguments \nin an application fe~ ... en, the position i of an interesting parame\u00adter, and the abstract escape semantic \nenvironment enve mapping f and the free identifiers within el through en to elements of the abstract \nescape se\u00admantic domain De, the local escape test function L(f, i,el, . . ..en. enve ) determines how \nmuch of the it~ parameter of f could escape f in the evaluation of fel . .. en. It is defined as follows: \n~(f) i ;;ij;:n> en%) = ... Xn] env. [zi + %])(l) where Zi = ((1, Si), (E[e~ j env~)(2J), s~ is the number \nof spines of e~ (if it is a list type, otherwise Si is O), and for all~< n and ~ # i, zj = ((O, 0), (E[ej] \nenv,)(z)). Similarly, we can determine local escape information from the result of the local escape test \nfunction. 4.3 Complexity The abstract interpretation framework for the es\u00adcape analysis uses a finite \nabstract domain of size (d+ 2) ~ 2. In the case of first-order functions, the order of time complexity \nis exponential in the number of arguments to the function being analyzed. In the case of higher-order \nfunctions, the complexity is much worse than exponential, and depends on the types of its arguments. \nThe time complexity of the escape analysis is comparable to that of strictness analy\u00adsis for higher-order \nfunctional languages with non-flat domains (lists) [19]  5 Polymorphic Invariance of Escape Analysis \nUp to this point, we have assumed that nml is a monomorphically-ty ped language. It remains to be shown \nthat escape analysis works on polymorphically\u00adtyped nml as well. We concentrate on parametric polymorphism. \nTo show this, we prove that escape analysis exhibits the property of polymorphic invari\u00adance [1]. This \nmeans that given a polymorphic func\u00adtion, escape analysis will return the same result on any two monotyped \ninstances of that function. Ac\u00adtually, escape analysis is polymorphically invariant when it is stated \nthe following way (essentially the converse of the way it was stated previously): Given a functzon application, \nhow many spines of an argu\u00adment are not returned an the result of the application. This is important \nbecause it is the portions of an ar\u00adgument that do not escape from the application that can be stack \nallocated or reused (if unshared). Theorem 1 (Polymorphic Invariance) Letf be a polymorphic function \nof arity n, and let f( and f! be any two monomorphic instances off. Assume that env~ and env~ are escape \nsemantic environments that map fl and f to elemenis of De, respectively. Then, forl<i <n, G(f , i, env~) \n= (O,O) # G(f , i, env~) = (O, O) and G(f , i, env~) = (1, k ) ~ G(f , i, env~) = (1,k ) such that s; \n k = s? kfl where s: and s; are the number of spines of the ith parameter off and F , respectively. \nProof : This is shown by structural and fixpoint induction on the expression e (the body off) [16]. . \nPolymorphic invariance indicates that to analyze a function definition in a polymorphic language, we \nneed only perform the analysis on the simplest mono\u00adtyped instance of that function. The result is then \napplicable to all possible instances of that function. Application of Escape Anal\u00adysis As mentioned in \nthe introduction, escape information can be used in a number of other analyses and opti\u00ad mization [16]. \nDue to space limitations, we will only sketch them. Several papers [8, 9] have been published on shar\u00ading \nanalysis of objects in higher order functional lan\u00adguages (and many papers for first-order languages). \nIt turns out that for strict languages (in which the evaluation order is obvious), sharing analysis of \nlists becomes easy in the presence of escape information. Theorem 2 (Sharing Information) Letf bea function \nwhich takes n arguments such that di is the number of spines of the ith parameter of f for i=l ., ,n, \nand let f return a list with df spines. If esci is the number of escaping sptnes of the iih pa\u00adrameter \nof f, for i = 1 . . . n (statically inferred by escape analysis), then 1. ail cons cells in the top (df \n max{min{escl, (dl -Ul)}, . . .,rnin{escn, (dn un)}}) spines of the result of (f el . . . en) are unshared \nwhen ui is the number of unshared spines of ei for 1< i~ n. 2. all cons cells in the top (dj rnax{escl, \n. . . . escn}) spines of the result of (f el , ., en) are un\u00adshared for any sex of arguments el, . . \n. en. Proof : 1. The number of spines of ei that are shared is (di Ui). The number of shared spines \nof e,, that could escape f is rnin{esci, (di Ui)}, In the re\u00adsult of (f el .. .en), the bottom maz{rnin{esci, \n(di Ui)} } spines will be shared. Thus, all cells in the top (dj -rnaz{rnin{escl, (d, -Ul)}, . . . . \nmin{escn, (dn -Un)} }) spines of the result are not shared. 2. Since we consider any set of arguments \nel, . . . .e~, and we have no sharing information of ei, we assume that Ui= O as the worst-case. Then, \nrnin{esci, (di O)} = esc~ because esci < (ti. Thus, all cells at top (df rnaz{escl, . . . . escn}) \nspines of the result are not shared. CI Both escape information and sharing information that are determined \nby escape analysis can be used for the in-place reuse optimization. Consider an ex\u00adpression of the form \nof (f el ., . ei ...en) where f is a function with n parameters, the ith parameter of f is a list type \nwith di spines, and there occurs some cons in the body of f. Let all cons cells at the top ui spines \nof the result of ei be unshared. Using the globai escape information of parameters of f, the in\u00adplace \nreuse of cons cells can be performed as follows: . If the bottom esci spines of the Z thparameter off \nescapes f globally then the expression can safely be transformed into (f el ... e; ...en) where f is \na new version of f which directly reuses cons cells in the top s = ~in{~i, (di -esci)}, spines of the \nith argument of f for new cons cells needed in the body of f. . Let f be defined as f xl ... Zn = ... \n(cons elez) .... If there is no further use of the i~h parameter Zi off after the evaluation of the subexpression \n(cons el ez) then a new version f of f which uses the in-place reuse op\u00adtimization can be defined as \nfollows: f q ...xn = ... (DCONS~i el e2) ... where DCONSis a destructive version of cons defined by DC13NSa \nb c = {p := a;car(a) := b; cdr(a) := c; return(p)}  7 Conclusions We have presented an analysis that \nanswers a simple question, but in doing so, subsumes a number of other program analyses for storage optimization \ndescribed in the literature. It works on higher order functional languages in the presence of lists, \nis relatively simple and (hopefully) easy to understand, and makes no assumptions about an underlying \nabstract machine. It remains to be seen if it is useful in practice due to the computational complexity \nof finding fixpoints of higher order functions. However, the fact that the analysis is polymorphically \ninvariant allows one to an\u00adalyze only the simplest instance of each polymorphic function, and we hope \nthat in practice this makes the [11] anal ysis useful at compile-time. References [12] [1] S. Abramsky. \nStrictness analysis and polymor\u00adphic invariance. In Workshop on Programs as Data Objects, LNCS 217, \nSpringer-Verlag, pp. 1-24, 1986.  [13] [2] H. Baker. Unifying and conquer (garbage, up\u00addating, aliasing \n...) in functional languages. In Proceedings of the ACM Conference on Lisp and Functional Programming, \npp. 218-226, 1990. [3] R.A. Brooks, R.P. Gabriel and G.L. Steele. An [14] optimizing compiler for lexically \nscoped LISP. In Proceedings of the SIGPLAN Sympostum on Compiier Consiructzon, pp. 261-275, 1982. [4] \nG.L. Burn, C.L. Hankin, and S. Abramsky. Strictness analysis for higher order functions. [15] Sctence \nof Computer Programming, 7:249-278, 1986. [5] 13.R. Chase. Garbage Collection and Other Opta\u00ad [16] mizations. \nPh.D. Thesis, Rice University, 1987. [6] D.R. Chase. Safety considerations for storage allocation optimizations. \nIn Proceedings of the SIGPLAN Conference on Programming Lan\u00ad [17] guage Design and Impiernentation, \npp. 1-9, 1988. [7] D.R. Chase, M. Wegman, F. K. Zadeck. Analy\u00adsis of pointers and structures. In Proceedings \nof the SIGPLAN Conference on Programming Lan\u00adguage Design and Implementation, pp. 296-310, 1990.  [18] \n[8] A. Deutsch. On determining lifetime and alias\u00ading of dynamically allocated data in higher-order functional \nspecifications. In Proceedings of the ACM Symposium on Principles of Progran~n~al~g [19] Languages, \npp. 157-168, 1990. [9] B. Goldberg. Detecting sharing of partial ap\u00adplications in functional programs. \nIn Proceed\u00adings of the Conference on Funcliond Program\u00adming and Computer Architecture, LNCS 274, Springer-Verlag, \npp. 408-425, 1987. [10] B. Goldberg and Y.G. Park. Higher order escape analysis: Optimizing stack allocation \nin higher order functional program implementations. In Proceedings of the European Symposium on Pro\u00adgramming, \nLNCS 432, Springer-Verlag, pp. 152\u00ad160, 1990. P. Hudak and J. Young. Higher-order strictness analysis \nfor the untyped lambda calculus. In Pro\u00adceedings of the ACM Symposium on Principles of Programming Languages, \npp. 107-118, 1986. K. Inoue, H. Seki, and H. Yagi. Analysis of func\u00adtional programs to detect run-time \ngarbage cells. ACM Transactions on Programming Languages, 10(4):555-578, October 1988. S.B. Jones and \nD. Le Metayer. Compile-time garbage collection by sharing analysis. In Pro\u00adceedings of the Conference \non Functional Pro\u00adgramming Languages and Computer Architec\u00adture, pp. 54-74, 1989. N. Jones and S. Muchnick. \nBinding time op\u00adtimization in programming languages: An ap\u00adproach to the design of an ideal language. \nIn Pro\u00adceedings of the ACM Symposmm on Principles of Programming Languages, pp. 77-94, 1976. D. Kranz. \nORBIT: An Optimizing Compiler for Scheme. Ph.D. Thesis, Yale University, May 1988.  Y.G. Park. Semantic \nAnalyses for Storage Man\u00adagement Opttmizations in Functional Language Implementations. Ph.D. Thesis, \nNew York Uni\u00adversity, 1991. Y.G. Park and B. Goldberg. Reference escape analysis: Optimizing reference \ncounting based on the lifetime of references. In Proceedings of the ACM Symposium on Partial Evaluation \nand Se\u00admantics Based Program Manipulation, pp. 178\u00ad189, 19 31.  C. Ruggieri and T.P. Murtagh. Lifetime \nanalysis of dynamically allocated objects. In Proceedings of the ACM Symposium on Principles of Pro\u00adgramming \nLanguages, pp. 285-293, 1988. P. Wadler. Strictness analysis on non-flat do\u00admains (by abstract interpretation \nover finite do\u00admains), In Abstract Interpretation of Declarative Languages, S. Abstramsky and C.L, Hankin, \ned\u00ad itors, pp. 266-275,Ellis Horwood, 1987.  A Examples Considerthe followingpartitionsort (often mistakenly \ncalled quicksort) program: letrec PS x = if (x=nil) then nil else letrec y = SPLIT [car x) (cdr x) nil \nnil; in APPEND(PS (car y)) (cons (carx) (PS (car (cdr y)))); SPLITpxlh= if (x=nil) then (cons 1 (cons \nh nil)) elseif (car x)<p then SPLIT p (cdr x) (cons (car x) 1) h else SPLIT p (cdr x) 1 (cons (car x) \nh); APPEND X y = if (x=nll) then y else cons (car x] (APPEND(cdr x) y); in PS [5,2,7,1,3,4] We assume \nthat thetype ofeach function is given by PS:ini list -int list, SPLIT :int+int list +int list + int list \n-+ int list list, and APPEND : intlist + int list + int list and that each car in the program is annotated \nas cars which denotes the car takes as its argument a list with s spines. A.1 Escape Analysis The definitionsof \nthe escape semantic values a.ppend, split, andpsof APPEND, SPLIT, and PS (shown unhurried for convenience) \nare: appendzy = yl-l(subl(z )l-lappendzg) split pzih = llJhtJ(split pz(subl(x) Lll)h)U(splitp~ (subl(z)t-lb)) \npsz = append (pssub2(spiit subl(~) ~((0, O)err)((O, O)err))) (sub (z) U (ps sub (spiit sub (z) r ((O, \nO)err) ((O, O)ew})) The meaning of each function in the escape semantic domain is found by fixpoint iteration. \nHere is the fixpoint iteration for APPEND: and for SPLIT: split(o) p z 1 h = J-(intli$t) list sp~it(l) \np x 1 h = 1U hU L(int l~st) I;.i U J-(int Iist) rist luh split(21 p X ! h = / U hU (Sfdit(l) p z (subI(z) \nU /) h)U (spiit(l) p z (subl(x) U h)) = 1U hU ((subl(z) U /) U h) U ((subl(x) U h)) 1U hU subl(z) / U \nhU (spiit(2) p z (subI(z) U /) h)U (split(2) p z (subl(z) U h)) 1U h U ((subl(z) U i) U h) U (/ U (subl(~) \nU h) U subl(z)) 1 U hU subl(z) and for PS (using the values computed for APPENIDand SPLIT): ~$(o) ~ \n= lint ,i$t ps(l) z = append(ps( ) sub2(subl (z))) (subl(x) u (psf ) sub2(subl (z))) = append lint Ii$t \n(subl(subl(x)) U li~t lit) ., = subl(~) PS(2) ~ = append(ps(l) sub2(subl(z))) (subl(r) U (ps(l) subz(subl(x))) \n= append subl (subz(subl (z))) (subl (x) u subl(sub2(subl( x))) = subl(z) Let enve = [APPENDw append, \nSPLIT++ split, PS ++ ps], Then, G(APPEND, 1, enve) = (E, [APPEND x y]env.[x w ((1, 1), err), y * ((0, \n0), err)])(l) = (((0, O), err) Usubl (({l, l), e~r)))(~) = (1,0) G(APPEND, 2, enve) = (E. [APPEND x y]env,[x \nw ((O, O), err), y * ((1, 1), err)].)(~) = (((1, l), err) usubl(((o, o), err)))(~) = (1,1) Thus, we \nconclude that APPENDreturns all of its second argument y, and all but the top spine of the first argument \nx. G(SPLIT, 1, enve) = (E.[SPLIT p x1 h]env. [p M ((1, O), err), X, l,h++ ((0, o),e~~)])[l) = (((o,o),e~r) \nU ((oo)te~r) LJsub (((O, O), y~)))(l) = (0,0) G(SPLIT, 2, env,) = (E, [SPLIT p x1 h]enue[x ~ ((1, l), \nerv-),p,l, hw ((o,o))e7-~)1.)(1) = (((0,0), e~~) u ((ojo), e?>~) u SUb (((1, l),e? ~)))(~) = (1,()) \nG(SPLIT, 3, env,) = (E, [SPLIT p x1 h[env.[1 I--+((1, 1), err), p, x)h~ 0), -)1)(1) ((0,  = (((1, l),er~) \nU ((0,0), e -~) usubl(((o,o),e~~ )))(~) = (1,1) G(SPLIT, 4, enve) = (13e[SPLIT p x1 h!enve[h + ((1, l), \ne.rr), p,x, I+ ((o,o),e~~)l.)(1) = (((o,o),e ~) u ((1, l),er~) u subl(((o, o), e~~)))(l) = (1,1) From \nabove, we conclude that SPLIT returns all of its third and fourth arguments 1 and h, none of the first \nargument p, and all but the top spine of the second argument x. G(Ps, 1, enve) = (E.[Ps xl enve[x E+ \n((1, l),ef r)])(~) -(subl(((l, l),e~~)))(l)  = (1,0) So, we conclude that PS returns all but the top \nspine of its argument x. A.2 Sharing Information fronl Escape Analysk PS takes a list with one spine \nas its argument, and returns a. list with one spine, From the global escape analysis, we know that no \nspine of the argument escapes PS globally. SPLIT takes four arguments p, x, 1 and h where p is an integer, \nand x, 1 and h are lists with one spine, respectively, and returns a list with two spines. From the global \nescape analysis, we know that none of the first parameter p, all but the top spine of the second parameter \nx, and all of the third and fourth parameters 1 and h escape SPLIT globally. Thus, we can determine the \nfollowing sharing properties (among others) of the program: . For the expression (PS e) where e is any \nlist with one spine, the top spine of the result list of (PS e) is not shared. . For the expression \nof (SPLIT el e2 e3 e4 ) where each ei is any possible expression, the top spine of the result list of \n(SPLIT el e2 e3 e4) is not shared.  A.3 Optimization based on Escape Analysk A.3.1 Stack Allocation \nour escape analysis has determined that the spine of the original list [5,2,7,1,3,4] does not escape \nfrom PS. Thus the spine of that list can be allocated in PS activation record, All the cells of the spine \nwill disappear when PS S activation is removed from the stack. A.3.2 In-place Reuse From the global \nescape analysis, we know that APPEND returns all of its second argument y, and all but the top spine \nof the first argument x. We also know that, for any expression (PS e) where e is a list with one spine, \nthe top spine of the result of (PS e) is unshared. Thus, the definition of PS can be transformed into \nPS as follows: PS x= if (x=nil) then nil else letrec y = SPLIT (car x) (cdr x) nil nil; in APPEND (PS \n(car y)) (cons (car x) (PS (car (cdr y)))); where APPEND is a version of APPEND in which cons cells \nin the top spine of its first argument x are directly reused. It is defined by APPEND X J1= if (x=nil) \nthen y else DCONS x (car x) (APPEND (cdr X) y); Furthermore, if we know that the top spine of the argument \nof PS is unshared, then the definition of PS can be transformed into PS in which cons cells in the top \nspine of its argument x are reused as follows: ps, )x= if (x=nil) then nil else letrec y = SPLIT (car \nx) (cdr x) nil nil; in APPEND (PS (car y)) (DCONSX (carx) (PS (car (cdr y)))); Consider, as another \nexample, a naive reverse function REV: REV 1 = if (l=nil) then nil else APPEND (REV (cdr 1)) (cons (car \n1) nil); From our analysis, REVcan retransformed into REV which reuses cons cells in thetopspine ofits \nargument 1, if unshared, as follows: REV 1 = if (l=nil) then nil else APPEND (REVJ (cdr 1)) (DCONS 1 \n(car 1) nil); A.3,3 Block Allocation/Reclan~ ation Suppose wearegiven aprograr nidentica ltothepartition \nsort program,except that the result expression was PS (create-list i) where create_list is some recursive \nfunction creating a list a,ncl i is some variable. The list that create_list returns cannot be allocated \nin PS S activation record because the activation record doesn t exist when the list is created. An optimization \nthat can be performed is as follows: create~ist should allocate the spine of the list in some block of \nmemory. The spine of the list does not escape from PS, SO when PS is finished, the whole block of memory \ncan be put back on the free list, This block of memory is the local heap described by Ruggieri and Murtagh \n[18].   \n\t\t\t", "proc_id": "143095", "abstract": "<p>Higher order functional programs constantly allocate objects dynamically. These objects are typically cons cells, closures, and records and are generally allocated in the heap and reclaimed later by some garbage collection process. This paper describes a compile time analysis, called escape analysis, for determining the lifetime of dynamically created objects in higher order functional programs, and describes optimizations that can be performed, based on the analysis, to improve storage allocation and reclamation of such objects. In particular, our analysis can be applied to programs manipulating lists, in which case optimizations can be performed to allow cons cells in spines of lists to be either reclaimed immediately or reused without incurring any garbage collection overhead. In  a previous paper on escape analysis [10], we had left open the problem of performing escape analysis on lists.</p><p>Escape analysis simply determines when the argument (or some part of the argument) to a function call is returned by that call. This simple piece of information turns out to be sufficiently powerful to allow stack allocation of objects, compile-time garbage collection, reduction of run-time storage reclamation overhead, and other optimizations that are possible when the lifetimes of objects can be computed statically.</p><p>Our approach is to define a high-level non-standard semantics that, in many ways, is similar to the standard semantics and captures the escape behavior caused by the constructs in a functional language. The advantage of our analysis lies in its conceptual simplicity and portability (i.e. no assumption is made about an underlying abstract machine).</p>", "authors": [{"name": "Young Gil Park", "author_profile_id": "81100158224", "affiliation": "", "person_id": "PP14065250", "email_address": "", "orcid_id": ""}, {"name": "Benjamin Goldberg", "author_profile_id": "81100491830", "affiliation": "", "person_id": "PP39072352", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143125", "year": "1992", "article_id": "143125", "conference": "PLDI", "title": "Escape analysis on lists", "url": "http://dl.acm.org/citation.cfm?id=143125"}