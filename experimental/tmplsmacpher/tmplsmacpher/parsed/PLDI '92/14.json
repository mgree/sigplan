{"article_publication_date": "07-01-1992", "fulltext": "\n Beyond Induction Variables Michael Wolfe * Oregon Graduate Institute of Science and Technology Abstract \nInduction variable detection is usually closely tied to the strength reduction optimization. This paper \nstud\u00adies induction variable analysis from a different perspec\u00adtive, that of finding induction variables \nfor data depen\u00addence analysis. While classical induction variable anal\u00adysis techniques have been used \nsuccessfully up to now, we have found a simple algorithm based on the the Static Single Assignment form \nof a program that finds all linear induction variables in a loop. Moreover, this algorithm is easily \nextended to find induction variables in multiple nested loops, to find nonlinear induction variables, \nand to classify other integer scalar assign\u00adments in loops, such as monotonic, periodic and wrap\u00adaround \nvariables. Some of these other variables are now classified using ad hoc pattern recognition, while others \nare not analyzed by current compilers. Giving a unified approach improves the speed of compilers and \nallows a more general classification scheme. We also show how to use these variables in data dependence \ntesting. Introduction In classical compiler analysis, induction variable recog\u00adnition is inextricably \nlinked to the strength reduct~on transformation; in fact, the two terms are sometimes used interchangeably. \nThe most comlmon candidates for strength reduction (and therefore the most impor\u00adtant induction variable \ncandidates) are array address calculations in inner loops. *Supported in part by NSF Grant CCR-8906909 \nand ~y DARPA Grant MDA972-86-.J-1OO4. Permission to copy without fee all or part of this material is \ngranted provided that tha copies are not made or distributed for direct commercial advantage, the ACM \ncopyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0-8979 J.476.7/92/QQ~6 /0 J62... \n$J .50 Nowadays, many compilers now include advanced loop transformations (such as loop distribution \nand loop interchanging). These transformations have been proven to be useful (and are commercially used) \nfor a wide variety of systems, ranging from high perfor\u00admance vector multiprocessor supercomputers down \nto uniprocessor workstations, where the transformations improve the cache utilization. These transformations \nrequire analysis of array subscripts to determine the data dependence relations in loops. Current methods \nto test and characterize data dependence relations for subscripted array references require the subscript \nex\u00adpressions to be a linear combination of induction vari\u00adables in the enclosing loops. The long history \nof research into data dependence testing has found that many variables used in sub\u00adscript expressions \nare not induction variables, but can be characterized as one of several special cases. Current compilers \nclassify these variables using ad hoc pattern recognition algorithms. Many compilers do not include these \nrecognition algorithms at all, ignoring potential optimization opportunities. This paper gives a simple \nalgorithm for finding lin\u00adear induction expressions in nested loops. The strength of the algorithm is \nthat it is fast and that it will also characterize other scalar integer variables in the loops, such \nas wrap-around variables, flip-flop variables, families of periodic variables, non-linear induction variables \n(polynomial and ge\u00adometric), and monotonically increasing or decreasing variables. Finally, we look \nat how to apply dependence tests for these new cases. Our algorithm is based on the Static Single Assignment \nform of the program.  2 Induction Variables and i=o Static Single Assignment An induction variable is \ntypically defined in compiler texts as a variable whose value is systematically incre\u00admented or decremented \nby a constant value in a loop [ASU86, FL88]. This is sometimes broken into basic induction variables, \ni.e. variables which appear only in statements of the form: i=lo LI : loop i =l+k endloop where k is \na constant or loop invariant, and other induc\u00adtion variables, which are linear combinations of known \ninduction variables. Many programming languages in\u00adclude a loop construct with an index variable that \nis defined to be an induction variable. One of the early improvements in induction variable analysis \nwas to ex\u00adtend the definition of basic induction variables to the maximal set of variables which are \nassigned by simple expressions of the form: i=j~k where j and k are invariants or induction variables \n(note j may be zero) [CK77, ACK81]. This allows mutually-defined induction variables, such as: j=n L2 \n: loop i=j+c j=i+k endloop where c and k are invariants in the loop. With each induction variable a \ncompiler associates its initial value and its increment, or step. We represent this by a tuple (i, i,s), \nwhere / is the name of the loop, i is the initial value (represented symbolically if it cannot be determined) \nand s is the step. The examples here will number the loops; thus in the first example, i is a basic induction \nvariable which can be described as (Ll, iO+k, k). In nested loops, an induction variable in the inner \nloop may have an initial value or step that varies in the-outer loop; they may even be induction variables \nin the outer loop: L3 : loop =j. +1 ~=i L4 : loop =j+i j endloop endloop Here, i is an induction variable \n(L3, 1, 1), while j is an induction variable (L4, i + i, i); even though the step of j varies in the \nouter loop, it is still a linear induction variable in the inner loop. We define a multiloop induction \nvariable as an in\u00adduction variable in an inner loop with a step that is invariant in the outer loop, \nand an initial value that is an induction variable in the outer loop: i =0 L5 : loop i=i+2 j=i L6 : \nloop =j+l j endloop endloop Here, i=(L5, 2, 2), while j=(.L6, i + 1, 1). We will rep\u00adresent such multiloop \ninduction variables with nested tuples, so that j=(L6, (.L5, 3, 2), 1). 2.1 Tile Static Single Assignment \nForm Since the algorithm presented here is based on the Static Single Assignment (SSA) form, we briefly \ncover the main ideas behind SSA and show a quick exam\u00adple; readers familiar with Static Single Assignment \ncan skip to the next section. The SSA form of a program can be considered as a sparse representation \nof use-clef chains. The algorithms to convert a sequential program into SSA form are based upon the Control \nFlow Graph (CFG), which is a graph G= (V, E, Entry, llcit), where V is a set of nodes rep\u00adresenting basic \nblocks in the program, E is a set of edges representing sequential control flow in the pro\u00adgram and Entry \nand Exit are distinguished nodes rep\u00adresenting the unique entry point into the program and the unique \nexit from the program respectively. After a program has been converted into its SSA form, it has two \nkey properties: 1. Every use of any variable in the program has ex\u00adactly one reaching definition, and \n2. at confluence points in the CFG, merge functions called @functions are introduced. A +-function for \na variable merges the values of the variable from distinct incoming control flow paths, and it has one \nargument for each control flow predecessor. (a) (b) j=~ jl= nl L7 : loop L7 : 100P i~ = @(ii, is) j2 \n= W1)J3) i= j+c is=jz+cl =l+k j~=is+kl j endloop endloop  h= (-L7 , nl,cl +h) i3= (L7 , nl+cl, cl+kl) \nj~ = (L7 , nl + c1 +kl, cl +kl) Figure 1: Example loop with SSA form. The two key algorithms used to \nconvert a program into SSA form are the placement of ~-functions to satisfy the second property, and \nthe renaming of variables to sat\u00adisfy the first property. Intuitively, a @function for vari\u00adable X is \nplaced at the first CFG vertex w where two clis\u00adtinct definitions of X reach; the ~-function itself counts \nas a new definition of X, and so the algorithm iterates. Once ~-functions are inserted, each variable \ngeneration is renamed to a unique name (here represented by sub\u00adscripted names), and each use of a variable \nis replaced by the unique reaching SSA definition; qifunction ar\u00adguments are renamed to the SSA name \nthat reaches the corresponding control flow edge. The reader is encour\u00adaged to read the SSA paper [CFR+91] \nfor more details on these algorithms, and [AWZ88, RWZ88, WZ91] for details on applications of the SSA \nform. An example of a loop with its SSA form is given in Figure l; the SSA names that correspond to induction \nvariables are shown. 3 The SSA Graph As with previous related work, we assume the pro\u00adgram is represented \nby a CFG, as explained earlier. Moreover, we assume that each basic block is repre\u00adsented by a linked \nlist of tuples of the form (op, left, right, ssalank) where op is the operation code, lefl and right \nare the two operands, and ssalink is used only for loads (and indexed stores, which are not discussed \nhere) to in\u00addicate the single reaching SSA name for this variable. The operators available include those \ngiven in the table in Figure 2. Load and store operations have additional arguments describing the variable \nfor which they apply. 11 J1 12 # J2 @ c1 Lo Lo KI 13 ST lx ID ID AD addition SB subtraction MP multiplication \nDV division EX exponentiation ltG negate PH ~-function LD load ST store LT literal 7 Figure 2: SSA graph \nfor example program, Indexed loads and stores are denoted by the presence of subscripts; since we are \nconcerned only with scalars here, we will not elaborate. The algorithm to analyze the program uses a \ngraph\u00adical abstraction called the SSA graph. When analyzing a loop, the vertices in SSA graph are the \ntuples repre\u00adsenting operations within that loop. The edges go from each tuple to the lefi and right \noperands as well as (for loads) to the ssalink. The SSA graph for the loop in Figure 1 is given in Figure \n2. Note that the edges go from the operators to the source operands. Edges from the LD vertices correspond \nto the ssalink field. 3.1 Basic Linear Induction Variables Our algorithm to find the induction variables \nis based on Tarjan s well-known algorithm to find strongly con\u00adnected regions (SCRS) in directed graphs \n[Tar72]. The primary observation is that each basic linear induction variable will belong to a nontrivial \nSCR in the SSA graph. Each such SCR in the SSA graph must in\u00adclude a loop-header @function, since all \nvalues cycling around the loop must pass through a qifunction. The advantage to using Tarjan s algorithm \nis that when it identifies an SCR in the graph, it will have visited all the successors of the SCR; because \nof the way the edges are directed in our graph, when an SCR is identified, all the source operands reaching \nthe sC!R, will already have been visited and identified. Our modifications to Tarjan s algorithm are \nto classify each SCR as an in\u00adduction variable (or something else) at the time the SCR is identified; \nthis information will then be imme\u00addiately available for other SCRS which use the value of any operator \nin this SCR. A simple method to identify an SCR in the SSA graph as a family of basic linear induction \nvariables is to require that it satisfy the following simple con\u00adstraints: . The SCR contains only one \n@function (at the header of the loop). . The SCR contains only addition and subtraction operators, and \nthe right operand of the subtraction is not part of the SCR (no i=n-i assignments). . The other operand \nof each addition or subtraction is loop invariant. . Any loads and stores are to unsubscripted vari\u00adables \n(i. e. load/store addresses are loop invari\u00adant).  There is one strongly connected region in the graph \nin Figure 2 which satisfies all these constraints, thus we know that it defines a family of basic linear \ninduction variables. The step for all variables in the family is com\u00adputed as the cumulative effect of \nall the increment and decrements in the SCR. The first member of the family is the left hand side of \nthe qLfunction; the initial value of this linear induction variable is the other argument to the @function, \nthe reaching SSA name from outside the loop. Thus we can describe the linear induction variable j2 as \n(L7 , jl, c1 + kl ). The other members of the family differ only by a constant in the initial value. \nOften the initial value coming in from outside the loop can be evaluated and substituted, using an algorithm \nsuch as constant propagation [WZ9 1]. Actually, the rules for basic linear induction variables can be \nrelaxed a bit. As long as the variable is incre\u00admented by the same amount on each path through the loop, \nthe compiler can classify it as an induction vari\u00adable. An example is shown in Figure 3. The only oper\u00adators \nin the strongly connected region in the SSA graph are addition and +-functions and the only values added \nare loop invariants. Starting with the loop-header @ function, the compiler can determine the offset \nof each SSA variable from the value of the first qifunction for that iteration. As long as each @function \nwithin the loop (e.g., at endif statements) has the same offset for all arguments, the SCR defines a \nfamily of basic linear induction variables. (a) (b) i=l il=l L8: loop L8 : loop iz = q$(i~, is) if exp \nthen if exp then i = j.+2 is = iz+2 else else i = i+2 i4 = iz+2 endif endif is = #(is, i~) endloop endloop \niz = (M, 1,2) i3 = (.L8,3,2) i4 = (L8, 3,2) i5 = (L8, 3,2) Figure 3: Example loop with SSA form.  4 \nBeyond Induction Variables 4.1 Wrap-Around Variables After examining many user programs to find how to \nimprove their compilers, vendors and researchers found some particular patterns that appeared often enough \nto warrant special recognition and handling in a compiler. One of these is called a wrap-around variable \n[PW86], such as imi below: iml =n L9: for i =1to nloop A(i) = A(iml) + ... lrnl =1 endloop In all iterations \nbut the first, the value of iml is equal to i-I; in the first iteration, however, iml has some other \nvalue, usually the index of the last element of the array A. Typically, iml is used to wrap the array \nA around a cylinder, hence the name. The standard compiler trick, once a wrap-around variable is found, \nis to peel off the first iteration of the loop and replace the wrap-around variable with the appropriate \ninduction variable. Typically, wrap-around variables are found with a separate pattern matching analysis \nof the loops, following induction variable analysis. In the SSA graph, a wrap-around variable is possible \nwhen a. loop-header @-function appears in a strongly connected region by itself. If the SSA name carried \naround the loop is an induction variable of some type, then the loop-header value is a first-order wrap-around \nvalue; that is, in all but the first iteration, its value i=l il=l LIO: 100P LiO):loop k2 = @(kl,k3) \njz = 4(j11j3) iz = ~(il, is) k=j k3 =j2 j=i j3 =12 i=i+l is=iz+l endloop endloop Figure4: Wra.p-around \nvariable in SSA form. will be an induction variable in the loop. This is the case with jz in Figure 4; \nas we will see in the next section, this will then define k3 as a first-order wrap around variable. Wrap-around \nvariables can be cas\u00adcaded; any loop-header ~-function with one argument corresponding to a n-th order \nwrap-around variable is itself an (n+ 1)-th order wrap-around variable. Thus, k2 isa second-order wrap-aroundv \nariable. In general, any of the other known classes of variables described below could also be wrapped \naround the loop, mean\u00ading that the variable will have that property only af\u00adter some fixed number of \niterations around the loop. A wrap-around variable would then be identified by its order and the class \nof the variable that is wrapped around the loop. If the initial value for the wrap-around variable fits \nthe induction sequence, it may be more precisely iden\u00adtified as an induction variable. For instance, \nif the initial value of jl in loop L1O above had been O, then j2 could have been identified as the induction \nvariable (LIO, O,1). 4.2 Flip-Flop and Periodic Variables A method used in some relaxation codes is \nthe genera\u00adtion of (new values for a. matrix from the old values; one common way to program this is to \nadd a dimen\u00adsion to the matrix with size 2, then to use A( 1, *,*) as the old values and generate the \nnew values into A(2, *, *). For the next iteration of the relaxation, the program flips the definition \nof old and new . Two programming constructs are typically used to imple\u00adment this. The first switches \nthe values of the two variables: jai jold = 2 Lll: for iter = 1to n loop . . . relaxation code .. jtemp \n= jold jold =j j = jtemp endloop L13: 100P L13 :1OOP tz = @(tl, ts) j2 = 4(j1jj3) kz = 4@,h) 12 = 4(11,13) \nt=j ta =j2 j=k j3 =k2 k=l k3 =12 lt = 13 =ts endloop endloop Figure5: Example periodic variable in SSA \nform. Another method takes advantage of the equality j old=3-j to compute the new values of j old and \nj: j=l jold =2 L12: for iter = 1 to n loop ... relaxation code . . . j = CJ-j jold = 3-jold endloop \nMethods have been proposed to recognize the latter form of flip-flop variable, but are not typically \nimple\u00admented in commercial products. In either case, it is extremely important and useful for the compiler \nto re\u00adalize that for any fixed value of it er, j and j old have different values, though it may not know \nwhat those values are. A generalization of flip-flop variables is a tuple of variables whose values are \nrotated around the tuple, as around a ring; see Figure 5. Here, we see the tuple (j, k, 1) which comprises \na set of periodic variables with period 3. Again, it is useful to know that for any given iteration, \nno two of these variables will have the same value (assuming the initial values of these variables are \ndistinct). Flip-flop variables as shown in loop L11 are then immediately recognized as periodic variables \nwith period 2. In the SSA graph, when a strongly connected region contains more than one loop-header \n@-function and has no arithmetic and no other q$-functions, then it corre\u00adsponds to a family of periodic \nvariables. The period is equal to the number of loop-header @-functions in the SCR. In the example in \nFigure 5, this number is three; note that tz does not appear in the strongly connected region with the \nother variables. It is important to note that the compiler will need to determine that the initial values \nfor the periodic variables are distinct in order to take advantage of the periodicity for data dependence \ntesting; these initial values are easily found from the other arguments to the loop-header ~-functions \nin the SCR. 4.3 Non-Linear Induction Variables The classical definition of an induction variable is \nthat it follows a linear sequence of values. With little addi\u00adtional effort, we can also find polynomial \nand geonlet\u00adric series. In the program below the various induction variables are given in the table on \nthe right: J=i k=l 11 = L14: for i = 1to nloop =j+i j k= k+j+l 1=1*2+1 endloop variable sequence closed \nform i 1,2,3,4,... i = (L14, 1, 1) j 2,4,7,11,.,. (h + 3h + 4)/2 k 4,9,17,29,... (h + 6h2 + 23h + 24)/6 \n1 3,7,15,31,... 2h+2 1 where h is a basic loop counter that starts at zero (h= (L14, O, l)). Incrementing \na variable by a polyno\u00admial induction variable produces an induction variable of the next higher order. \nIn general, finding the closed form for polynomial and geometric induction variables is simplified since \n(a) the nonzero coefficients can al\u00adways be identified by the compiler, and (b) the coef\u00adficients will \nalways be rational. We use simple matrix inversion with rational arithmetic to find the coeffi\u00adcients, \nas shown below. Finding polynomial induction variables for strength reduction may not be so interest\u00ading, \nsince the operation is already only addition. From the point of view of data dependence, there are cur\u00adrently \nfew dependence testing algorithms that can take advantage of this additional knowledge. We will revisit \nthis point briefly in a later section. It is interesting to note that the second form of flip-flop variable \ndescribed above can be recognized as a geometric induction vari\u00adable with base 1; in loop L12 at iteration \ni, the value assigned to variable j is Ii + 3 x ~~~~ 1~. In the SSA graph, when Tarjan s algorithm has \nfound a strongly connected region that fails one of the conditions of a linear induction variable, the \nSCR can be analyzed to see if it is an identifiable non\u00adlinear induction variable. In all cases analyzed \nhere, the SCR must include a single ~-function at the loop header. The key is to classify the cumulative \neffect of all the operations in the SCR on the value reaching the loop-header ~-function relative to \nthe value of that @function in the previous iteration. . If the effect is to increment the loop-header \nvalue by a linear induction variable, the SCR is a family of polynomial induction variables of second \norder. . If the effect is to increment the value by a polyno\u00admial induction variable of order n, this \nSCR is a  family of polynomial induction variables of order n+ 1. If the cumulative effect is to multiply \nthe loop\u00adheader value by a known integer, the SCR cor\u00adresponds to a family of geometric induction vari\u00adables. \nIf the cumulative effect is to subtract the loop\u00adheader value from a loop invariant, the SCR cor\u00adresponds \nto a flip-flop variable, equivalent to a pe\u00adriodic variable of period two. This could be taken to extreme, \nsuch as recognizing that multiplying by a linear induction variable gener\u00adates a factorial sequence. \nA polynomial induction vari\u00adable can be represented in the compiler by extending the linear induction \nvariable tuple to include the coeffi\u00adcients of each power of the basic loop counter; thus we write (l, \ni,sl, s , ., ,, s~ ) represents an induction vari\u00adable for loop 1 whose value on the first iteration \nis i, and whose value on iteration h is i + ~~=1 Skhk, or letting so = i, ~~=0 sk hk. A compiler can \nfind the coefficients Sk by matrix inversion. The number of un\u00adknowns (the values of sk) is equal to \nm + 1, where m is the order of the polynomial; since m can be determined by the compiler, the number \nof unknowns is fixed. By inverting the matrix of elements uZj=i~,O~i~m,O~j~ m and multiplying the inverse \nby the computed (perhaps symbolic) first k values of the induction variable, the coefficients can be \nfound. Since the entries of the ma\u00adtrix a~~ are all integer, the inverse will have only ra\u00adtional entries, \nFor instance, in loop L14 above, since k is a third-order polynomial induction variable, the corresponding \nmatrix is: 1000 1111 A= 1248 13927 II The inverse of this matrix is: - [-iii;! which, when multiplied \nby the computed first four val\u00adues of k gives the coefficients of the closed form of the induction variable: \nA-1 [49 1729 ]T=[4 ~ 1 ~]T A geometric indllction variable has both geometric and polynomial terms; \nwe represent this by the poly\u00adnomial coefficients followed by the coefficients of each exponential term, \nWe write to represent the induction variable for loop 1 with initial value so, and whose value on iteration \nh is ~Skhk + ~gkkh k=O k=,?3 A matrix inversion technique to find the coefficients also applies. A basic \ngeometric induction variable will have only a single exponential term (though it may have any number \nof polynomial terms); if the geometric base is g, the matrix to be inverted is: Suppose we had an assignment \nin loop L14 above of the form m=3*m+2*i+l (with initial value m=O); the geomet\u00adric base of this induction \nvariable is 3, while the poly\u00adnomial part would seem to have a quadratic term (since a linear induction \nvariable is being added). Thus, the matrix to be inverted is: 1001 1113 1249 13927 [1  The inverted \nmatrix, multiplied by the first four com\u00adputed values, is: Thus, m can be described by m= 6x 3h h 3 (note \nthere is no quadratic term after all). 4.4 Monotonic Variables Variables that are conditionally incremented \nor decre\u00admented can not generally be recognized as induction variables, However, they often do exhibit \nuseful Prop\u00ad erties. A common pattern is to increment a variable conditionally, for instance, to pack \ncertain values from one vector to another: k=O L15: for i = 1to n loop if A(i) > 0 then k= k+l B(k) = \nA(i) endif endloop L16: loop L16 :loop kz = @(kl, k5) if exp then if exp then k = k+l k3 = kz+l else \nelse k =k+2 k4 = k2+2 endif endif endloop k5 = ~(k3, k4) endloop Figure 6: Example monotonic variable \nin SSA form. Some compilers recognize this whole pattern as a pack operation, as might be converted \ninto the Fortran-90 PACK intrinsic function. Our approach is to realize that within the range of the \nconditional, k will never have the same value for two different iterations of the loop. Moreover, even \noutside the range of the condi\u00adtional, the compiler can determine that the value of k will never decrease \nfor subsequent iterations; we say that k is monotonically increasing. Sometimes our approach can discover \nstronger rela\u00adtions. If the variable is incremented for each iteration, even though it is not always \nincremented by the same value, we can say that the variable is monotonically strictly increasing, as \nin Figure 6. Even though we have no way to determine a closed form for k here, we will see that this \ninformation can be used effectively in dependence analysis. In the SSA graph, a monotonic variable appears \nas a strongly connected region that contains @functions other than at the loop-header, perhaps at an \nendif, as in Figure 6. Since the arguments to the ~-function at the endif assigning k5 have have different \noffsets from the loop-entry value k2, namely kz -I-1 and kz +2, the SCR does not correspond to a family \nof induction vari\u00adables. Nonetheless, it is easy to realize that regardless of the path through the loop, \nthe value of k is always monotonically increasing. Regardless of the operations in the SCR, if the compiler \ncan determine that the values being merged at a qifunction are always larger than the loop-header value, \nit can classify all variables in the SCR as monotonic. Multiply operations can also be allowed, such \nas 2*i+i as long as the initial value of i is known. If the value of a monotonic variable is al\u00adways \nchanged in the loop, then it can be called strictly monotonic. We will see how this distinction is used \nin dependence testing.  5 Other Concerns 5.1 Non-Basic Induction Variables Basic induction variables \n(linear and nonlinear) and monotonic and periodic variables arise from nontrivial strongly connected \nregions (cycles) in the SSA graph. Variables which are functions of these variables must also be classified. \nTo reiterate, one of the advantages of using Tarjan s algorithm is that when it comes time to classify \na non-basic variable, all its arguments will have been already visited and classified. The result of \neach type of operation depends on how its operands have been classified. The rules for classifying some \noperators are simple. For a load instruction, if the address is not invariant (such as indexed arrays, \nor computed pointer derefer\u00adencing) the type is unknown; if the address is invariant, and the ssalink \nof the load comes from outside the loop, the load is classified as invariant; otherwise, the load takes \nthe classification of the ssalink. A store always takes the classification of the value being stored. \nA literal is always invariant. The classification of arithmetic operators can be much more complex. The \nsimple cases, adding or mul\u00adtiplying an invariant to an induction variable, etc., are easily handled. \nA generalized induction variable added to another induction variable will always be an induct\u00adion variable, \nbut multiplication may not be. For in\u00adstance, multiplying (2i + 1) by (3i 5) results in the geometric \ninduction variable (6i + 3i 5 x 2i 5), but multiplying (2; + i) by (3i 2i) results in the expression \n(6i+i~i _ . 12i+l 2i2), which does not match the form of a generalized induction variable; it may, however, \nbe classified as monotonic. In general the compiler needs an algebra of types and operators; space prevents \na complete table of the algebra here, but the reacler can come up with obvious examples (adding wrap \naround variables to get another wrap around variable, adding a monotonic variable to an induction variable \nto get another monotonic variable, etc. ). 5.2 Trip Counts The trip count of a loop can often be determined \nby finding the sequence of values used for the exit condi\u00adtion. If there is a single loop exit and the \ncondition is an integer comparison, the compiler can convert the comparison into the form: if( lefl ~ \nright ) exit the loop using the table below. If the loop conditional expres\u00adsion is as given on the left \nand the true branch exits the loop, it can be converted to the expression in the nlid\u00addle column (where \nwe use rules of integer arithmetic to convert s to <); if the false branch exits the loop, then the condition \nmust first be negated, so we get the expression in the right column below: condition exit stay in a.<b \na~b 1 b~a a~b a~b b<a 1 a>b b~a 1 a~b a~b b~a a~b 1 The compiler can treat the comparison as a subtrac\u00adtion, \nand try to classify it as a linear induction sequence (L, i, s). The trip count of the loop (that is, \nthe num\u00adber of times the loop exit condition chooses to stay in the loop) can be computed as o ifi~O \ntripcount = [i/( s)l if i >0 and s<0 co ifi>Oands~O { If the trip count can be found, the loop is called \na countable loop. This is necessary below for handling multi-level induction variables. When a loop has \nmul\u00adtiple exits, the compiler may not be able to determine the exact number of iterations, but it may \nbe able to find a maximum trip count; this information is useful for dependence testing, to place bounds \non the solution space. Note that some code in the loop above the exit test may have been executed even \nif the trip count is zero; in general, if the trip count is tc, some code above the exit test will have \nbeen executed tc + 1 times while code be\u00adlow the exit test can have only been executed tc times. In single \nexit loops (where this is important), any un\u00adconditional code in the loop that is not dominated by the \nexit test will be executed one more iteration than unconditional code below the exit. 5.3 Nested Induction \nVariables When dealing with nested loops, induction variable recognition proceeds from the inner loops \noutward. While processing any loop, any SSA links to code out\u00adside the loop are treated as loop invariant \n(for the purposes of Tarjan s algorithm and induction variable classification); these links may be chased \ndown to find constant values. MThen an inner loop is classified as a countable loop, the cumulative effect \nof the execution of the loop on all induction variables in the loop can be expressed in closed form. \nGiven a trip count tc, an induction variable (L, i,s) will end up with the value i+tc xs, if all the \nincrements are below the exit con\u00addition. By our definition of trip count, however, some induction variable \nincrements may have been above the loop exit; the correct general formula is i+fc x s+s ~ar~,,, where \ns ~arlv is the cumulative effect of any increme;t ,. statements ;hat can happen before the loop exits. \nWhen the inner loop is processed, the compiler can construct an exit value for the induction variables, \nas kl=O Li7: loop kz = +(kl,ks) il=l L18: loop k3 = q$(k2,k4) iz = ~(il, is) kd=k3+2 if iz > 100 exit \niz=iz+i endloop k5=k4+2 endloop Figure7: Nested induction variables shown in Figures 7 and 8. This value \ncan be assigned to anew variable, and all references outside this inner loop to the exit value are changed \nto refer to the new variable. In this example, the induction variables for the inner loop are: k3 = (L18, \nk2,2) kz = (L18, k2+2,2) iz = (L18,1,1) is = (L18, 2, 1) where il is resolved to its constant value. \nThe exit con\u00additions converted to 100 <i2 1, whichis treated as the induction expression 100 (i2 1) \nor (L18, 100, 1); thus the trip count is 100. To compute the exit value of the induction variables k \nand i, the compiler notices that the k4 statement preceeds the exit condition, so its increment value \nmust be counted one extra time, and the kc and i4 statements are added. Now the induction variables for \nthe outer loop can be found; if there are any remaining SSA links to variables in inner loops, these \nmust correspond to non-induction variables or induction variables for which the exit value is unknown; \nin either case, the value can be treated as an unknown without tracing further. In this example, the \nouter loop has the induction variables: kz = (L17,0, 204) k~ = (L17,202, 204) k~ = (L17, 204, 204) When \nthe outer loops have all been processed, an outer\u00adto-inner loop traversal can replace any outer loop \nin\u00adduction variable expressions in inner loop induction variable initial values, if desired, changing \nk3 and k4 to: kl =0 L17 :100p kz = #(kl, ks) il=l L18 : loop ks = ~(kz, kq) iz = ~(il, iz) k4=k3+2 if \niz > 100 exit is=iz+l endloop kG= k2 + 101*2 ia =il +100*1 ks=kG+2 endloop Figure 8: After processing \ninner loop induction vari\u00adab]es. jl=o L19: for i = 1to nloop j2 = 4(j1)j4) j3=j2+i L20: for k = 1to i \nloop j4 = d(j3jj5) j5=jq+l endloop endloop Figure 9: Generalized induction variable from triangu\u00adlar \ninner loop. k3 = (L18, (L17,0,204),2) k4 = (L18, (L17,2,204),2) It is interesting to note that the case \nof generalized induction variables found to be so difficult in [EHLP92] is simple in this framework. \nTheir given example has a doubly nested loop where the inner loop is triangular, that is, its upper limit \ndepends on the outer loop index variable, as in Figure 9. Our method will process the inner loop, recognizing \nj4 and j~ as linear induction variables in that loop, with the trip count of that loop equal to a variable \ncomputed in the outer loop. The exit value of j will be computed as j6 = j~ + i x 1, ancl j4 will be \nreplaced by j~ in the assignment to j2. M hen processing the outer loop, i will be recognized as a linear \ninduction variable, while j2, j3 and j~ will be recognized as a family of quadratic induction variables: \nkl =0 L15 :for i = i to n loop kz = @(kl,kA) F(kz) = A(i) if A(i) > 0 then C(k2) = D(i) k3 =k2+i B(k3) \n= A(i) E(i) = B(k3) endif k4 = #(k3, k2) G(i) = F(k4) endloop Figure 10: Mixed monotonic and strictly \nmonotonic variable j2 = (L19,0, ;, :) j3 = (L19,1,;,:) j6 = (L19,2, :, ~) When the value forj~ is substituted \ninto theinduc\u00adtionrepresentations forthe inner loop, we get: j4 = (L20, (L19,1,:, ~),l) j5 = (L20, (L19,2,~,~),l) \n 5.4 Loop Exit and Conditional Gates When processing nested induction variables, our algo\u00adrithm inserted \na new assignment to a new SSA name (e.g. kG in Figure 8). Proper engineering can make this a low-cost \ninsertion, but it would have been nice if there had already been an SSA name to distinguish the value \nof the variable upon loop exit. The loop exit )? gating function used in the Program Dependence Web [BM090] \nserves exactly this purpose, While we clon t feel we need all the generality of the PDW, we are in\u00advestigating \nadding some of its elements to the factored use-clef chains used in our compiler internal represen\u00adtation. \nIn one other place the SSA representation is less pre\u00adcise than we would like. The SSA form of the mono\u00adtonic \nvariable in loop L15 (where we add a few more uses of k is shown in Figure 10. In this example, the compiler \ncan easily find the SCR in the SSA graph, in\u00adcluding the assignments to {k2, k3, k4}. The loop entry \nvalue is kz, and the @function for k4 has two different offsets, li2 and k2 + 1; thus this is not a linear \ninduc\u00adtion variable. However, all variables in the SCR are monotonically increasing. Even stronger, we \ncan say that k3 is monotonically strict~y increasing, since if the k3 assignment occurs more than once, \nit must assign a larger value each time. Thus, the dependence due to the assignment and reuse of array \nB will have depen\u00addence direction (=) (see discussion in the next section); on the other hand, since \nkz and k4 are only monotonic, the flow dependence due to array F has dependence direction (~) (and there \nis an anti-dependence with direction (<)). However, we would also like to notice and take advantage of \nthe fact that within the body of the conditional statement (e, g. at the assignment to array C), k2 also \nmust be strictly monotonic. One way to detect this would be to notice that any uses of k2 in this region \nare post-dominated by the strictly mono\u00adtonic assignment. Another way would be to again use elements \nof the Program Dependence Web, in this case toadd a switch at the conditional; this would essen\u00adtially \ngive a unique name for each value entering a new cent rol region.  6 Use in Dependence Testing The driving \nforce for classifying the variables in loops as shown in this paper is to improve the generality of dependence \ntesting in loops, generating more pre\u00adcise dependence graphs and allowing more aggressive optimization. \nDependence testing for linear induc\u00adtion variables is generally well-covered in the liter\u00adature [AI(87, \nBCKT79, GKT91, MHL91]. Banerjee presents some algorithms for handling polynomial in\u00adduction variables \nin his MS thesis [Ban76]. After a brief review, we focus on dependence with wrap-around vari\u00adables, monotonic \nvariables and periodic variables. The algorithm used to classify variables will actually classify each \nsubexpression as one of the generalized variable types. Thus, each subscript expression will be classified \nas an induction expression, monotonic ex\u00ad pression, etc. Dependence testing in such a system invoIves \nconstructing a depen dence equation from the expressions. For instance, in the program: i=O j=3 L21 \n: 100p i=i+i A(i) = A(j-i) j=j+z endloop the left hand side subscript will be classified as the linear \nincluct,ion expression (L21, 1, 1), while the right hand subscript will be classified as (L21, 2, 2). \nFrom this, the compiler can immediately read off the coeffi-the dependence equation is cients to get \nthe dependence equation: l?+l=2i +2 The dependence decision algorithm will determine whether there are \ninteger values of the free variables i , i that solve the dependence equation and lie within the loop \nlimits. Often, dependence distance or direc\u00adtion information can be found, determining that, the solution \nonly occurs when (for instance) i i = 1 or i < i [WB87]. This information is critical to many optimization \nalgorithms. In the case where one of the expressions has the wrap-around attribute, the same dependence \nequation can be constructed and solved, but the dependence re\u00adlation should be flagged as holding only \nafter k iter\u00adations, the order of the wrap-around variable. This allows the compiler to decide whether \nthe loop can be optimized before going through the headache of peeling off the first k iterations of \nthe loop. If the subscript expressions are members of the same periodic variable family, such as: j=l \nk=2 1=3 L22 : 100p A(2*j) = A(2*k) tenip = j =k k=l j 1= temp endloop the dependence equation should \nbe constructed in terms of j and k: 2j = 2k The dependence solvers determine whether there are integer \nvalues of j and k such that the dependence equation has a solution, and may determine distance or direction \ninformation in terms of j and k. What we need is to translate that distance or direction infornla\u00ad tion \ninto the appropriate information in terms of the loop iteration numbers. Suppose, as in this case, the \ndecision algorithm determines that there is a solution when j = k. Let jh refer to the value of j during \niterat\u00ad ion h. Because of the periodic nature of the variables, the compiler also knows that jh~ = khll \nonly when h! # h . Thus, the = direction for the dependence equation translates into a # direction for \nthe depen\u00ad dence relation; this is exactly the information needed to optimize the relaxation codes described \nearlier. For monotonic variables, a similar situation occurs. The dependence equation is constructed \nin terms of the monotonic variables; if xnis monotonic increasing, and 2?nl = %11 the dependence solver \nwill find that there is dependence only when m = m . Now the compiler needs to trans\u00adlate this into direction \ninformation for the loop; since m is monotonic, ml), = mhll can occur when h = h or when h < h (since \nm may not be incremented ev\u00adery iteration). Similar cases hold for strictly monotonic and monotonic decreasing \nvariables. 6.1 Loop Normalization Loop normalization is a linear transformation on the index set of a \nfor loop to change the sequence of values of the loop variable to start at zero (or one) with a step \nof one; thus the loop: for i =2to Ii-by 3loop ... A(i) ... will be changed to: for i = O to (11-2)/3 \n100p ... A(1*3+2) ... This transformation was initially designed to simplify the formulation of data \ndependence testing algorithms [BCKT79]. Since normalization always puts the loop lower limits into subscript \nexpressions, it can com\u00adplicate life for simple dependence analyzers when the lower limit contains other \nvariables, as shown by the work on Parafrase [SLY90]. For this reason, and be\u00adcause it can adversely \naffect the kinds of transforma\u00adtions allowed in programs (such aa loop interchanging), this author has \nin the past argued against implement\u00ading loop normalization [W0186]. It is interesting to note that this \nformulation of induction variables essen\u00adtially normalizes all loops. One example used to argue against \nnormalization is: L23: for i = 1to n loop L24: for j = 1+1 to nloop A(i, j) = A(i-l, j) Modern dependence \nanalysis applied to this example will typically find a dependence distance vector, with one distance \ncomputed for each nested loop; see the tons of literature on this subject for further details [AK87, \nBCKT79, GKT91, MHL91]. In this case, the distance vector is (1, O), meaning distance one in the loop \nand distance zero in the j loop. Normalizing the inner loop (to a lower limit of one) changes it to: \nL23 :for i = 1 to n loop L24 : for j = 1to n-i loop A(i, j+i) = A(i-l, j+i) The computation in the loop \nhasn t changed, but the dependence distance vector has; now the distance vec\u00adtor is (1, l). While this \nmay seem innocuous, some import ant transformations (such as loop interchanging) are prevented by this \ncase. In our framework, however, the shape of the loop iteration space is not part of the induction variable \nrecognition strategy; in fact, boih examples above will have the same results after induction variable \nrecogni\u00adtion: L23: for i = ... loop L24 : for j = .-loop A((L23, 1, 1), (L24, (L23,2, 1), 1)) = A((L23,0, \n1,( L24, (L23,2, 1), 1)) Intuitively, this strategy implicitly normalizes all in\u00ad duction variables \nwith respect to a simple loop counter wil,h initial value zero and step one (recall h = (L14, O, l)). \nOne strategy would be to keep the induc\u00ad tion expression corresponding to the lower loop lim\u00ad its and \nfactor these into the dependence equations. Mc)re likely, use of this induction variable representa\u00ad \ntion will force compilers to abandon the direction. vec\u00ad tor dependence representation, which has been \ncriti\u00ad cized by many researchers as too coarse [IT88]. A di\u00ad rection vector encodes the sign of the elements \nof the distance vector; while less precise in genera], it can be used effectively when the distance vector \nis not con-Shnt. It may also force compilers to implement loop skewing and 100P interchanging as a. single \ntransfornm\u00adtion [W0186]; this has been formulated in the past as a linear transformation on the index \nset [KMW67], and is currently in vogue as unimodular transformations [WL91, Ban91]. 7 Summary and Conclusions \nWe have shown a fast and simple algorithm for classi\u00ad fying all induction variables in a loop; this algorithm \nis linear in the size of the SSA graph, not iterative. The algorithm is a somewhat obvious application \nof Tar\u00ad jan s SCR algorithm on the SSA graph. It has several advantages, such as finding basic and non-basic \ninduc\u00ad ticm variables in one pass. While usually used for a.p\u00ad pl;ying strength reduction, we focus here \non induction variable detection for dependence testing. More importantly, we use the same algorithm to \nclassify many different types of integer variables in loops, including polynomial and geometric induction \nvariables, as well as flip-flop, periodic, wrap-around and monotonic variables. While some of these cases \nhave been classified before, they were done by special case analysis instead of in a unified framework. \nWe have shown that the classification scheme is effi\u00ad cient, and how to use this information in data \ndepen\u00ad dence testing. We have implemented the part of this algorithm that detects linear induction variables \nin our research compiler, and are adding the full algorithm at this time. References [ACK81] F. E. Allen, \nJohn Cocke, and Ken Kennedy. Reduction of operator strength. In Steven S. Muchnick and Neil D. Jones, \ned\u00aditors, Progmm Flow Analysis: Theory and Applications, pages 79-101. Prentice-Hall, 1981. [AK87] John \nR. Allen and Ken Kennedy. Auto\u00admat ic translation of Fortran programs to vector form, ACM Trans. on Programming \nLanguages and Systems, 9(4):491-542, Oc\u00adtober 1987. A V Aho, R Sethi, and J. D. Unm\u00ad[AstJ86] an. Compilers: \nPrinciples, Techniques, and Tools. Addison-Wesley, Reading, MA, 1986. [AWZ88] B. Alpern, M.N. Wegman, \nand F.K. Zadeck. Detecting equality of variables in programs. In C on~ Record 15th An\u00adnual ACM Symp. \nPrinciples of Program\u00adming Languages [POP88], pages 1 11. [Ban76] Utpal Banerjee. Data dependence in \nordi\u00adnary programs. M.S. thesis UIUCDCS-R\u00ad76-837, Univ. Illinois, Dept. Computer Sci\u00adence, November 1976. \n[Ban91] Utpal Banerjee. Unimodular transforma\u00adtions of double loops. In Nicolau et al. [NGGP91], pages \n192-219. [BCKT79] Utpal Banerjee, Shyh-Ching Chen, David J. Kuck, and Ross A. Towle. Time and parallel \nprocessor bounds for Fortran\u00adlike loops. IEEE Trans. on Computers, C\u00ad28(9):660-670, September 1979. [BM090] \nRobert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. The program de\u00adpendence web: A representation \nsupport\u00ading control-, data-, and demand-driven in\u00adterpretation of imperative languages. In Proc. ACM \nSIGPLAN 90 Conf. on Pro\u00adgramming Language Design and Implemen\u00adiaiion, pages 257 271, White Plains, NY, \nJune 1990. [CI?R+91] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth [CK77] \n[EHLP92] [FL88] [GKT91] [IT88] [KMW67] [MHL91] [NGGP91] [POP88] Zadeck. Efficiently computing static \nsin\u00adgle assignment form and the control depen\u00addence graph. ACM Trans. on Pro!granl\u00adrning Languages and \nSystems, 13(4) :451 490, October 1991. John Cocke and Ken Kennedy. An algo\u00adrithm for reduction of operator \nstrength. Communications ACM, 20(1 1):850-856, November 1977. Rudolf Eigenmann, Jay Hoeflinger, Zhi\u00adyuan \nLi, and David Padua. Experi\u00adence in the automatic parallelizatic)n of four Perfect benchmark programs. \n111Ut\u00adpal Banerjee, David Gelernter, Alexa.ndru Nicolau, and David A. Padua, editors, Lan\u00adguages and \nCompilers for Parallel Comput\u00ading. Springer-Verlag, 1992. (to appear). Charles N. Fischer and F~ichard \nJ. LeBla.nc, Jr. Crafting a Compiler. Benjanlin-Cummings, Menlo Park,, CA, 1988. Gina Goff, Ken Kennedly, \nand Chau-Wen Tseng. Practical dependence testing. In Proc. ACM SIGPLAN 91 Conference on Programming Language \nDesign and Inipie\u00admentation [SIG91], pages 15 29. Frangois Irigoin and R&#38;-ni Triolet. Supern\u00adode \npartitioning. In Conf. Record 15tl~An\u00adnual ACM Symp. Principles of Progranl \u00adming Languages [POP88], \npages 319 329. Richard M. Karp, Raymond E. Miller, and Shmuel Winograd. The organization of computations \nfor uniform recurrence equa\u00adtions. J. ACM, 14(3) :56~3-590, July 1$167. Dror E. Maydan, John L. Hennessy, \nand Monica S. Lam. Efllcient and exact data dependence analysis. In Proc. ACM SIGPLAN 91 Conference on \nProgran~\u00adming Language Design and Il)t~~!el??.elli~~li(J?l [SIG91], pages 1-14. Alexandru Nicola.u, David \nGelern\u00ad ter, Thomas Gross, and David Padua, edi\u00adtors. Advances in Languages and Compilers for Parallel \nComputing. Research Mono\u00adgraphs in Parallel and Distributed Comput\u00ading. MIT Press, Boston, 1991. Conf. \nRecord 15th Annual ACM Sy711p. Principles of Programming Languages, San Diego, CA, January 1988. [PW86] \n[RWZ88] [SIG91] [SLY90] [Tar72] [wB87] [WL91] [W0186] [WZ91] David A. Padua and Michael Wolfe. Ad\u00advanced \ncompiler optimizations for su\u00adpercomputers. Communications ACM, 29(12):1184-1201, December 1986. B.K. \nRosen, M.N. Wegman, and F.K. Zadeck. Global value numbers and redun\u00addant computations. In Conf. Record \n15th Annual ACM Symp. Principles of Program\u00adming Languages [POP88], pages 12 27. Proc. ACM SIGPLAN 91 \nConference on Programming Language Design and Imple\u00admentation, Toronto, June 1991. Zhiyu Shen, Zhiyuan \nLi, and Pen-Chung Yew. An empirical study of Fortran pro\u00adgrams for parallelizing compilers. IEEE Trans. \nParallel and Distributed Systems, 1(3):356-364, July 1990. R. Tarjan. Depth-first search and lin\u00adear \ngraph algorithms. SIAM J. Comput., 1(2):146-160, June 1972. Michael Wolfe and Utpal Banerjee. Data dependence \nand its application to parallel processing. International J. Parallel Pro\u00adgramming, 16(2):137-178, April \n1987. Michael E. Wolf and Monica S. Lam. An algorithmic approach to compound loop transformations. In \nNicolau et al. [NGGP91], pages 243-259. Michael Wolfe. Loop skewing: The wave\u00adfront method revisited. \nInternational J. Parallel Programming, 15(4):279-294, Au\u00adgust 1986. Mark N. Wegman and F. Kenneth Zadeck. \nConstant propagation with conditional branches. ACM Trans. on Programming Languaves and Systems, 13(2):181 \n210, April 1991.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Induction variable detection is usually closely tied to the strength reduction optimization. This paper studies induction variable analysis from a different perspective, that of finding induction variables for data dependence analysis. While classical induction variable analysis techniques have been used successfully up to now, we have found a simple algorithm based on the Static Single Assignment form of a program that finds all linear induction variables in a loop. Moreover, this algorithm is easily extended to find induction variables in multiple nested loops, to find nonlinear induction variables, and to classify other integer scalar assignments in loops, such as monotonic, periodic and wrap-around variables. Some of these other variables are now classified using ad hoc pattern recognition, while others are not analyzed by current compilers. Giving a unified approach improves the speed of compilers and allows a more general classification scheme. We also show how to use these variables in data dependence testing.</p>", "authors": [{"name": "Michael Wolfe", "author_profile_id": "81100031703", "affiliation": "", "person_id": "PP39085553", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143131", "year": "1992", "article_id": "143131", "conference": "PLDI", "title": "Beyond induction variables", "url": "http://dl.acm.org/citation.cfm?id=143131"}