{"article_publication_date": "07-01-1992", "fulltext": "\n Debugging Optimized Code with Dynamic Deoptimization Urs H61zle Craig Chambers David Ungar Computer \nSystems Laboratory Dept. of Computer Science and Engineering Sun Microsystems Laboratories CIS 57 Sieg \nHall, FR-35 MTV29-116 Stanford University University of Washington 2500 Garcia St. Stanford, CA 94305 \nSeattle, WA 98195 Mountain View, CA 94043 urs@cs.stanford. edu chambers@cs.Washington.edu ungar@eng.sun.com \n Abstract: SELF s debugging system provides complete source-level debugging (expected behavior) with \nglobally optimized code, It shields the debugger from optimization performed by the compiler by dynamically \ndeoptimizing code on demand. Deoptimization only affects the procedure activations that are actively \nbeing debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging \ninformation at discrete interrupt points; the compiler can still perform extensive optimization between \ninterrupt points without affecting debuggability. At the same time, the inability to interrupt between \ninterrupt points is invisible to the user. Our debugging system also handles programming changes during \ndebugging. Again, the system provides expected behavioc it is possible to change a running program and \nimmediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which \nmay contain inlined copies of the old version of the changed procedure) into new versions reflecting \nthe current source-level state. To the best of our knowledge, SELF is the first practical system providing \nfull expected behavior with globally optimized code. 1. Introduction SELF is a pure object-oriented language \ndesigned for rapid prototyping, increasing programmer productivity by maxi\u00admizing expressiveness and \nmalleability [US87]. SELF s pure message-based model of computation requires exten\u00adsive optimization \nto achieve good performance [CU91, HCU9 1]. Without aggressive procedure integration (inlining), for \nexample, performance would be abysmal [Cha92]. But an interactive programming environment also demands \nrapid turnaround time and complete source-level debugging. To make SELF practical, the system must This \nwork has been supported in part by the Swiss National Science Foundation (Nationslfonds), an IBM graduate \nstudent fellowship, NSF Presidential Young Investigator Award #CCR-8657631, and by Sun, IBM, Apple, Cray, \nTandem, TI, and DEC. Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. ACM SIGPLAN 92 PLDI-61921CA e 1992 ACM 0-89791 -476 -7192 /0006 /0032 . ..$1.50 \nprovide interpreter semantics at compiled-code speed, combining expected behavior [Ze184] with global \noptimiza\u00adtion. Most existing systems do not support the debugging of opti\u00admized code. Programs can either \nbe optimized for full speed, or they can be compiled without optimization for full source-level debugging. \nRecently, techniques have been developed that strive to make it possible to debug optimized code [Hen82, \nZe184, CMR88]. However, none of these systems is able to provide full source-level debugging. For example, \nit generally is not possible to obtain the values of all source-level variables, to single-step through \nthe program, or to change the value of a variable. Optimization is given priority over debugging, and \nconsequently these systems provide only restricted forms of debugging. To the best of our knowledge, \nSELF is the first practical system providing full expected behavior with globally opti\u00admized code. Compared \nto previous techniques, our use of dynamic deoptimization and interrupt points permits us to place fewer \nrestrictions on the kind of optimizations that can be performed while still preserving expected behavior. \nThe remainder of this paper is organized as follows. Section 2 discusses the optimization performed by \nthe SELF compiler and how they affect debugging. Section 3 describes how optimized code can be deoptimized, \nand section 4 explains how running programs can be changed. Section 5 discusses the implementation of \ncommon debug\u00adging operations, Section 6 discusses the benefits and limita\u00ad tions of our approach, and \nsection 7 examines its run-time and space cost. Section 8 relates this paper to previous work, and section \n9 contains our conclusions.  2. Optimization and debugging This section briefly outlines the oplimizations \nthat the SELF compiler performs and discusses some of the problems they cause for the debugger. 2.1 \nOptimizations performed by the SELF compiler SELF uses dynamic compilation [DS84, CUL89]: instead of \ncompiling whole programs prior to execution, code is generated incrementally at run-time and kept in \na caclhe. Whenever a source method is invoked which hasn t already been compiled, a new compiled method \nis created and cached. (In the SELF system, source code is accessible at all times so that methods can \nbe (re-)compiled at any time.) In addition to standard optimization such as global constant propagation, \nconstant folding, and global register allocation, our compiler relies extensively on three optirnizations \nwhich are important for pure object-oriented languages: inlining, customization, and splitting [CUL89, \nCU!)O, CU91]. lnlining reduces the call overhead and enables opti\u00admization which span source method boundaries. \nCustomi\u00adzation creates multiple compiled copies of source methods, each copy specialized for a particular \nreceiver type. Customization allows many dynamically-dispatched calls to be statically bound and subsequently \ninlined. Spli((ing creates multiple compiled copies of a source-level expres\u00adsion and optimizes each \ncopy for a particular set of types,, Additionally, the compiler performs dead code elimination, strength \nreduction, and global common subexpression elim\u00adination of arithmetic expressions, loads, and stores. \nRedun\u00addant computations are eliminated only if they cannot cause observable side effects such as arithmetic \noverflow. The compiler sometimes unrolls loops to avoid repeating type tests in every iteration of the \nloop, which frequently has the effect of hoisting invariant code out of loops. Low-level optimization \nsuch as delay slot filling are also performed; more extensive instruction scheduling could easily be \nsupported but has not been implemented. Induction variable elimination could be supported by extending \nthe structure of our debugging information. Since it must always provide full source-level debugging, \nthe SELF compiler does not perform certain optimization. In general, dead stores cannot be eliminated, \nand the regis\u00adters of dead variables cannot be reused without spilling the variable to memory first. \nBoth optimization can be performed, however, if there is no interrupt point within the variable s scope \n(see section 3.4). Finally, the SELF compiler does not perform tail recursion elimination or tail call \nelimination because they cannot be supported transpar\u00adently: in general, it is not possible to reconstruct \nthe stack frames eliminated by these optimizations. Instead, iteration is supported through a primitive \nwhich restarts execution of the current scope. (The SELF language does not pre-detine common control \nstructures such as if and whi 1 e; such control structures are user-defined). 2.2 Problems caused by \noptimization The code transformations performed by global optimization make it hard to debug optimized \ncode at the source level. Because optimization delete, change, or rearrange parts of the original program, \nthey become visible to the user who tries to debug the optimized program. This sections presents some \nof the problems that must be solved to provide source\u00adlevel debugging of optimized code. 2.2.1 Displaying \nthe stack Optimization such as inlining, register allocation, constant propagation, and copy propagation \ncreate methods whose activation records have no direct correspondence to the source-level activations. \nFor example, a single physical stack frame may contain several source-level activations because message \nsends have been inlined. Variables maybe in different locations at different times, and some variables \nmay not have run-time locations at all. physical stack source-level stack A A B cc F Figure 1. Displaying \nthe stack The example in Figure 1 shows the effects of inlining. The physical stack contains three activations \nA , C , and F . In contrast, the source-level-stack contains additional activa\u00adtions which were inlined \nby the compiler. For example, the activation B was inlined into A , and so B does not appear in the physical \nstack trace. 2.2.2 Single-stepping To single-step, the debugger has to find and execute the machine instructions \nbelonging to the next source operation. Optimization such as code motion or instruction sched\u00aduling make \nthis a hard problem: the instructions for one statement may be interspersed with those of neighboring \nstatements, and statements may have been reordered to execute out of source-level order, In contrast, \nsingle-step\u00ad ping is simple with unoptimized code since the code for a statement is contiguous. 2.2.3 \nChanging the value of a variable Consider the following code fragment i := 3; i :=3; optimizationj :=4: \nj :=4: k:=i+j; ~ k.l; .= Since the expression i + j is a compile-time constant, the compiler has eliminated \nits computation from the generated code. But what if the program is suspended just before the assignment \nto k and the programmer changes j to be 10? Execution of the optimized code cannot be resumed since it \nwould produce an unexpected value for k. With unopti\u00admized code, of course, there would be no problem \nsince the addition would still be performed by the compiled code. 2.2.4 Changing a procedure A similar \nproblem arises when an inlined procedure is changed during debugging. Suppose that the program is suspended \njust before executing the inlined copy of function f when the programmer ch,anges f because she has found \na bug. Obviously, execution cannot simply continue since f s old definition is hard-wired into its caller. \nOn the (other hand, it would be easy to provide expected behavior with unoptimized code: f s definition \ncould simply be replaced, and the subsequent call to f would execute the correct code.  3. Deoptimization \nNone of the above problems would exist with rmoptimized code. If optimized code could be converted to \nunoptimizcd code on demand, programs could be debugged easily while still running at full speed most \nof the time. SELF S debug\u00adging system is based on such a transformation. Compiled code exists in one \nof two states: . Oplimized code, which can be suspended only at relatively widely-spaced interrupt points; \nat every interrupt point, the source-level state can be reconstructed, and . Unoptimized code, which \ncan be suspended at any arbitrary source-level operation and thus supports all debugging operations (such \nas single-stepping). Section 3.1 explains the data structures used to recover the source-level state \nfrom the optimized program state. struct ScopeDesc { oop method; ScopeDesc* caller; int posWithinCaller; \nScopeDesc* enclosingScope; NameDesc args [ ] ; NameDesc locals [ ] ; NameDesc expressions tack [ 1; }; \nstruct NameDesc { enum { const, loc } tag; union { oop value; Location location; }; ); Figure 2. Pseudo-code \ndeclarations Sections 3.2 and 3.3 describe how optimized code can be transformed into unoptimized code \non demand, and section 3.4 discusses how interrupt points lessen the impact of debugging on optimization. \n3.1 Recovering the unoptimized state To display a source-level stack trace and to perform deopti\u00admization, \nthe system needs tc~reconstruct the source-level state from the optimized machine-level state. To support \nthis reconstruction, the SELF compiler generates scope descriptors [CUL89] for each scope contained in \na compiled method, i.e., for the initiaJ source method and all methods inlined within it. A scope descriptor \nspecifies the scope s place in the virtual call tree of the physicall stack frame and records the locations \nor values of its arguments and locals (see Figure 2). The compiler also describes the location or value \nof each subexpression within the compiled method. This information is needed to reconstruct the stack \nof evaluated expressions that are waiting to be consumed by later message sends. To find the correct \nscope for a given physical program counter, the debugger needs to know the virtual program counter (source \nposition), i.e., the pair of a scope descrip\u00adtion and a source position within that scope. Therefore, \nthe debugging information generated with each compiled method also includes a mapping between physicad \nand virtual program counters. With the help of this information, the debugger can hide the effects of \ninlining, splitting, register allocation, constant propagation, and constant folding from the user. For \nexample, if the compiler eliminates a variable because its value is a compile-time constant, the variable \ns descriptor would contain that constant. A straightforward extension of the descriptor structure could \nbe used to handle variables whose values can be computed from other values (such as eliminated induction \nvariables). // pointer to the method object // scope into which this scope was inlined (if any) //source \nposition within caller // Iexically enclosing scope (if any) // descriptors for receiver and arguments \n// descriptors for locals // descriptors for all subexpressions // compile-time constant or run-time \nvalue // constant value // run-time location for scope data structures Program suspended at time t, \nPhysicai stack Physicai-to-virtuai PC mapping Scope descriptors Source-levei stack p +=ly~- ,~j ; H wD \n Progrim suspended at time ~ Physical stack Physical-to-virtual PC mapping Scope descriptors Source-fevel \nstack I PC: 28 I -II stack frame IxENV_l of optimized PC=42 A . method . . c --L DPC: 42 El tI I \nscope D, line 1 1\u00ad 1 Figure 3. IRecovering the source-level state Figure 3 shows a method suspended \nat two different times. When the method is suspended at time tl, the physical PC is 28 and the corresponding \nsource position is line 5 of method B. A stack trace would therefore display B being called by A, hiding \nthe fact that B has been inlined into A by the compiler. Similarly, at time t2 the source-level view \nwould show D being called by C being called by A, displaying three virtual stack frames instead of the \nsingle physical stack frame. To display a complete stack trace, this process is simply repeated for each \nphysical stack frame 3.2 The transformation function Our approach transforms an optimized method into \none or more equivalent unoptimized methods. For the moment, we assume that only the topmost stack activation \nneeds to be transformed so that stack fmmes can easily be removecl or added section 3.3 explains how \nto remove this restriction. The deoptimizing transformation carI then be performed as follow~ 1. Save \nthe contents of the physical activation (stack frame) which is to be transformed, and remove it from \nthe run-time stack. 2. Using the mechanisms described in the previous section, determine the source-level \n(vhw.d) activations con\u00adtained in the physical activation, the values of their lo\u00adcals, and their virtual \nPC. 3. For each virtual activation, create a new compiled meth\u00adod and a corresponding physical activation. \nTo simplify the transformation function and subsequent debugging activities, the new methods (the target \nmethods) are completely unoptimized every message send corre\u00adsponds to a call, and no optimization such \nas constant folding or common subexpression elimination are per\u00adformed. 4. For each virtual activation, \nfind the new physical PC in the corresponding compiled method. Since the @et method is unoptimized, there \nwill be exactly one physi\u00adcal PC for the given virtual PC. (This would not neces\u00adsarily be the case if \nthe target methods were optimized.)  Initialize the stack frames created in the previous step by filling \nin the return PC and other fields needed by the run-time system, such as the frame pointer. ~ un:~:v2fd \n$? 5 ::+::::,:,:.. . .,,,,.,:,,,:..: deoptimize -. ...1..:.., ,,,,,,,,,:,.,. 4 !4 Q I~ m * optimized \n8 stack frame G Figure 4. Transforming an optimized 5. For each virtual activation, copy the values \nof all param\u00adeters, locals, and expression stack entries from the opti\u00admized to the unoptimized activation. \nSince the unoptimized method is a straightforward one-to-one translation of the source method, all variables \nwill be mapped to locations in the target activation, and thus all copied values will have an unambiguous \ndestination. (This would not necessarily be the case if the target methods were optimized.) Furthermore, \nsince the target method is unoptimized, it does not contain any hidden state which would need to be initialized \n(such as the value of a common subexpression). Thus, together with step 4, we have completely initialized \nthe new stack frames for all virtual activations, and the transformation is complete. Figure 4 illustrates \nthe process. The transformation expands an optimized stack fmme containing three virtual activa\u00adtions \ninto a sequence of three unoptimized stack frames, thus creating a one-to-one correspondence between \nvirtual and physical frames. Only those parts of the program which are actively being debugged (e.g., \nby stepping through them) need to be trans\u00adformed. These parts will be the only parts of the program \nrunning unoptimized code; all other parts can run at full speed. No transformations are necessary just \nto inspect the program state, as described in section 3.1. 3.3 Lazy deoptimization But how can a stack \nframe be deoptimized when it is in the middle of the stack, where new stack frames cannot be g returns \nP\u00ad real stack frame f(includes vir\u00adtual activations vfl, vf2, and vf3) { real stack frame g ,-;,, ;.j;.:Jy::y::y;::,.., \n .:::,,:,:,:,::: ~,: :~::,.,: ~.: ,.,~,:,:,:,::;!;: ;~::,.;,:,,.:.:.:::.:;, y,.: .~::;$::;:::,:,:::, \n . k: uno timized stat i frames stack frame into unoptimized form inserted easily? To solve this problem, \nour current imple\u00admentation always transforms stack frames lazily: deoptimi\u00adzation is deferred until \ncontrol is about to return into the frame (see Figure 5). For example, if the virtual activation vfz \nto be deoptimized is inlined in frame~ in the middle of the stack, f is not immediately deoptimized. \nInstead, the return address of g (the stack frame called by H is changed to point to a routine which \nwill transform f when g returns. At that point, f is the topmost frame and is deoptimized. Transforming \nonly the most recent activation simplifies the transformation process because no other stack frames need \nto be adjusted even if deoptimization causes the stack frames to grow in size. Lazy deoptimization can \nsimplify a system considerably, but it may also restrict the debugging functionality. Our system currently \ndoes not allow the contents of local vari\u00adables to be changed during debugging because a variable might \nnot have a run-time location. In order to create a run\u00adtime location for the variable, it might be necessary \nto trans\u00adform an activation in the middle of the stack, which our system currently cannot do. However, \nthis is not a funda\u00admental problem; for example, the transformed stack frames could be heap-allocated \nas described in [DS84]. An even simpler solution would be to always allocate stack locations for eliminated \nvariables. These locations would be unused during normal program execution but would spring into life \nwhen the programmer manually changes the value of an eliminated variable. Since the compiled code depended \non the old (supposedly constant) value, it would be invalidated as if the programmer had changed the \nmethod s source code (see section 4). deoptimization E=\u00ad every virtual activation that was contained \nin f now has its own real stack frame Figure 5. Lazy deoptimization of stack frames 3.4 Interrupt points \nIf optimized programs could be interrupted at any instruc\u00adtion boundary, debugging optimized code would \nbe hard, since the source-level state would have to be recoverable at every single point in the program. \nTo ease the restrictions that this would impose on optimization, an optimized SIZLF program can be interrupted \nonly at certain interrupt poirwrt where its state is guaranteed to be consistent. Notification of any \nasynchronous event occurring between two interrupt points is delayed until the next interrupt point is \nreached. Currently, the SELF system defines two kinds of interrupt points method prologues (including \nsome process control primitives) and the end of loop bodies ( backward branches ). This definition implies \nthat the maximum inter\u00adrupt latency is bounded by the length of the longest code sequence containing \nneither a call nor a loop end, typically only a few dozen instructions. Because the latency is, so short, \nthe use of interrupt points is not noticed by the programmer. (If only sends were interrupt points, loops \nwithout calls could not be interrupted.) Interrupt points need to cover all possible points where a program \ncould be suspended; that is, they also need to handle synchronous events such as arithmetic overflow. \nIn our system, all possible run-time errors are interrupt points because all SELF primitives are safe: \nif the requested opera\u00adtion cannot be performed, the primitive calls a user-defined error handler which \nusually invokes the debugger. Once an optimized program is suspended, the current a~cti\u00advation can be \ndeoptimized if necessary to carry out debug\u00adging requests. In an unoptimized method, every source point \nis an interrupt point, and the program can therefore stop at any point. Since the debugger can be invoked \nonly at interrupt points, debugging information need be generated only for those points. This reduces \nthe space used by the debugging infor\u00admation, but more importantly it allows extensive optimiza\u00adtion \nbetween interrupt points. Essentially, the compiler may perform any optimization whose effects either \ndo not reach an interrupt point or can be undone at that point. For example, the compiler can reuse a \ndead variable s register as long as there are no subsequent interrupt points within the variable s scope. \nThe more widely-spaced interrupt points are, the fewer restrictions source-level debugging imposes on \noptimization. Interrupt points also lessen the impact of garbage collection on compiler optimization. \nGarbage collections can only occur at interrupt points, and so the compiler can generate t ~t~pt pints \nhave berm used in other systems befow, see section 8 for a discussion of the Deutsch-Schiffman Smalltatk-SO \nsystem. code between interrupt points that temporarily violates the invariants needed by the garbage \ncollector [Cha87].  4. Updating active methods During debugging, a programmer might not only change \nthe value of a variable but also the definition of a method. To invalidate the compiled code affected \nby such a change, the SELF system maintains dependency links between compiled code and the objects representing \nsource code methods [CUL89]. For example, if a compiled method contains inlined copies of a method that \nwas changed, the compiled method is discarded. However, if a compiled method containing art inlined copy \nof a changed method is active (i.e., has at least one activa\u00adtion), it cannot simply be discarded. Instead, \nthe compiled method must be replaced by a new compiled method before execution can continue. Fortunately, \ndeoptimization can be used for this ptupose. After the active compiled method has been deoptimized, it \nwill no longer contain any inlined methods. When its execution continues, all subsequent calls to the \nchanged method will correctly invoke the new defini\u00adtion. If the changed method itself is currently active, \nupdating its activation is hard. Fortunately, in SELF we don t have to solve this problem because in \nSELF S language model, acti\u00advations are created by cloning the method object. Once created, the clone \nis independent from its original, so changes to the original do not affect the clone. Lazy transformation \nelegantly solves the problem of invali\u00addated compiled methods in the middle of the stack we simply wait \nuntil the invalid method is on top of the stack, then transform it. Lazy transformation is very desirable \nin an interactive system since it spreads out the repair effort over time, avoiding distracting pauses. \nFurthermore, it handles sequences of changes well, for example when reading in a file containing new \ndefinitions for a group of related objects. With eager transformation, every new defi\u00adnition would cause \nall affected compiled methods to be recompiled, and many methods would be recompiled several times since \nthey are likely to be affected by several of the changes. With lazy transformation, these compiled methods \nwill be invalidated repeatedly (which is no problem since invalidation is very cheap) but only trans\u00adformed \nonce. In conclusion, with our debugging mechanisms it is almost trivial to support changing running programs. \nOur current implementation consists of less than a hundred lines of C++ code on top of the previously \ndescribed debugging function\u00ad ality and the code maintaining the dependencies. 5. Common debugging operations \n This section describes the debugging operations imple\u00admented in the SELF system and outlines possible \nimplemen\u00adtations of additional operations. With deoptimization, it is relatively easy to implement common \ndebugging operations such as single-sfep and jinish because these operations are simple to perform in \nunoptimized code, and deoptimization can supply unoptimized code for every program piece on demand. In \ncontrast, neither single-step nor finish could generally be provided by previous systems for debugging \noptimized code [2%184, CMR88]. 5.1 Single-step Because every source point has an interrupt point associated \nwith it in a deoptimized method, the implementation of single-stepping becomes trivial. The system deoptimizes \nthe current activation and restarts the process with the interrupt flag already set. The process will \nrelinquish control upon reaching the next interrupt point, i.e. after executing a single step. 5.2 Finish \nThe finish operation continues program execution until the selected activation returns. It is implemented \nby changing the return address of the selected activation s stack frame to a special routine that will \nsuspend execution when the acti\u00ad vation returns. Thus, the program is not slowed down during thejnish \noperation because it can run optimized code. If the selected activation does not have its own physical \nstack frame (because it was inlined into another method), its stack frame is deoptimized using lazy deoptimization. \nIn this case, the program can still run optimized code most of the timq only at the very end (when lazy \ndeoptimization is performed) does it run unoptimized code. 5.3 Next The next operation (also called step \nover ) executes the next source operation without stepping into calls, That is, the program will stop \nafter the next source operation has completed. Next can be synthesized by performing a single\u00adstep, possibly \nfollowed by a jinish (if the operation was a call). Consequently, next is implemented by a few lines \nof SELF code in our system. 5.4 Breakpoints and watchpoints SELF currently supports breakpoints through \nsource trans\u00adformation: the programmer inserts a breakpoint by simply inserting a send of halt into the \nsource method (halt explicitly invokes the debugger). To implement breakpoints without explicit changes \nby the programmer, the debugger could perform this source transformation transparently. Watchpoints ( \nstop when the value of this variable changes ) are also easy to provide because SELF is a pure object-oriented \nlanguage, and all accesses are performed through message sends (at least conceptually; the compiler will \nusually optimize away such sends). To monitor all accesses to an object s instance variable x, we can \nrename the variable to privat e_x and install two new methods x and x: which monitor accesses and assignments, \nrespec\u00adtively, and return or change privat e_x. The dependency system will invalidate all code that inlined \nthe old definition of x or x: (i.e., that directly accessed or changed x).  6. Discussion In this section, \nwe discuss some of the strengths and weak\u00adnesses of our approach and assess its generality. 6.1 Benefits \nOur debugging technique has several important advantages. First, it is simple: the current implementation \nof the trans\u00adformation process consists of less than 400 lines of C++ code on top of the code implementing \nthe debugging infor\u00admation described in section 3.1. Second, it allows a loose coupling between debugger \nand compiler neither has to know very much about the other, Third, it places no addi\u00adtional restrictions \nbeyond those described in section 2 on the kind of optimization which can be performed by the compiler. \nThus, many common optimization such as inlining, loop unrolling, common subexpression elimina\u00adtion, and \ninstruction scheduling can be used without affecting debuggability. Finally, our method is well suited \nfor an interactive system since it is incremental: usually, at most one stack frame needs to be converted \nas a result of a user command, 6.2 Current limitations Using unoptimized code during debugging introduces \na potential performance problem when the user decides to continue execution. Execution should proceed \nat full speed, but some of the stack frames may be unoptimized. However, this problem usually is not \nsevere: only a few frames are running unoptimized code, and the unoptimized code will be discarded as \nsoon as these frames return. All other parts of the system can run at full speed. Methods containing \nloops could still pose a problem since they could remain on the stack in unoptimized form indefi\u00adnitely. \nHowever, we currently are working to solve the more general problem of adaptive compilation [HCU91]. \nWith adaptive compilation, methods are created in unoptimized form first to minimize compile pauses. \nLater, the frequently\u00adused parts are automatically recompiled with optimization. Therefore, a system \nwith adaptive compilation would auto\u00admatically reoptimize any unoptimizcd loops created by debugging. \nThe current SELF system already contains a primitive form of adaptive compilation. 6.3 Generality The \ndebugging approach presented here is not specific to SELF and could be exploited in other languages as \nwell. Our system appears to require run-time compilation for deopti\u00admization, but systems without run-time \ncompilation could include an unoptimized copy of every procedure in an executable or dynamically link \nthese in as needed. For pointer-safe languages like Lisp, Smalltalk, or a pointer\u00adsafe subset of C++, \nour approach seems directly applicable. In pointer-unsafe languages like C which allow pointer errors, \ninterrupt points might be more closely spaced. The debugger could potentially be invoked at every load \nor store where the compiler could not prove that no address fault would occur. But even if interrupt \npoints caused by unsafe loads or stores were indeed very frequent, our approach would still allow at \nleast as many optimizations as o{.her approaches for source-level debugging. Pointers into the stack \nrequire special care during dcoptimi\u00adzation if the locations of such pointers are unknown. In this case, \nthe address of a stack variable potentially referenced by a pointer may not be changed. However, this \nproblem could probably be solved at the expense of some stack space by requiring the layout of optimized \nand unoptimized stack frames to be identical. 6.4 Implementation status The first implementation of the \ndebugging system was completed in the spring of 1991 (recovering the source-level state of optimized \nprograms was implemented in 19!39). Today s system implements all functions described in section 5 and \nis in daily use at several research institutions. A source-level debugger (written in SELF by Lars Bak) \nis also part of the system. The SELF implementation is avail\u00adable free of charge via ftp from self. stanford.edu. \n 7. cost Providing full source-level debugging in the presence of an optimizing compiler does not come \nfor free. In this section, we examine the impact of our techniques on responsiveness, run-time performance, \nand memory usage. f ~ a way, ttue source-level debugging of unsafe languages is Sometimg of an oxymoron: \nsince programs can overwrite arbitrary memory regions, they can always produce behavior which cannot \nbe explained at the language (source) level. For example, if an integer is erroneously stored into the \nlocation of a floating-point variable, the resulting behavior cannot be explained without referring to \nthe particular integer rmd floating-point representations used by the system. 7.1 Impact on responsiveness \nNeither the deoptimization process nor the use of interrupt points are perceptible to users. The compiler \ntypically creates the unoptimized methods in about one millisecond on a SPARCStation 1, and thus the \npauses introduced by dynamic deoptimization are negligible. Interrupt points increase the latency for \nuser and system interrupts by only a few microseconds because art interrupt point is usually reached \nwithin a few dozen instructions after the run-time system has set the interrupt flag. In summary, providing \nfull source-level debugging in the SELF system has not reduced its responsiveness. 7.2 Impact on run-time \nperformance Ideally, the performance impact of full source-level debug\u00adging could be measured by completely \ndisabling it and re\u00admeasuring the system. However, this is not possible because source-level debugging \nwas a fundamental design goal of the SELF system. Disabling debugging support would require a major redesign \nof the compiler and run-time system if any better performance is to be achieved. Further\u00admore, the garbage \ncollector already imposes some constraints on the optimizer, such as the requirement that live registers \nmay not contain derived pointers (pointers into the middle of objects). In many cases, the optimizations \ninhibited by garbage collection are very similar to those inhibited by debugging requirements, such as \ndead store elimination and some forms of common subexpression elimination [Cha87]. Thus, it would be \ndifficult to separate the impact of garbage collection on optimization from the impact of full source-level \ndebugging. However, we have measured some effects of source-level debugging in the SELF system. To determine \nthe impact of debugger-visible names, the compiler was changed to release registers allocated to dead \nvariables even if they were visible at an interrupt point. The performance improvement with the changed \ncompiler was insignificant (less than 2%) for a wide range of programs [Cha92]. That is, the extension \nof variable lifetimes needed to support debugging seems to incur virtually no cost in our system. (One \nreason for this might be that SELF methods are typi\u00adcally very short, so that few variables are unused \nin signifi\u00adcant portions of their scope.) The system currently detects interrupts by testing a special \nregiste~ each test takes two cycles on a SPARC. This polling slows down typical programs by about 470; \nsome numerical programs with very tight loops are slowed down by up to 13% [Cha92]. With a more complicated \nrun-time system using conditional traps, the overhead could be reduced to one cycle per check, and loop \nunrolling could further reduce the problem for tight loops. Alternatively, we could switch to a non-polling \nsystem where the interrupt handler would patch the code of the currently executing procedure to cause \na process switch at the next interrupt point. While we could not measure the full performance impact \nof our debugging scheme, inspection of the generated code indicated no obvious debugging-related inefficiencies. \nIn fact, the current SELF system has attained excellent perfor\u00admance, executing a set of benchmarks four \nto six times faster than ParcPlace Smalltalk-80* and about half the speed of optimized C [CU91]. 7.3 \nMemory usage The debugging and dependency information for compiled methods is kept in virtual memory \nin the current SELF system, Table 1 shows the memory usage of the various parts of compiled methods relative \nto the space used by the machine instructions. For example, the physical-to-virtual PC mapping is about \n17% the size of the actual machine code. The column labelled adaptive represents the default configuration \nwhere only often-used methods are optimized, while the optimized only column represents a system which \nalways optimizes. The data were obtained from two interactive sessions using the prototype SELF user \ninterface (written in SELF). Both runs represent more than 7 Mbytes of compiler-generated data. The space \nconsumption can be split into three main groups. The first group contains the method headers and the \nmachine instructions; together, these represent all the infor\u00admation needed to actually execute programs. \nThe second group contains the dependency links needed to invalidate compiled code after programming changes \n(see section 4). The third group contains all debugging-related information: the scope descriptors and \nthe PC mapping (see section 3.1), relocation information for the garbage collector (to update ~smaltt~k-go \nis a trademark of Parcplace Systems. object pointers contained in the debugging information), and the \nvarious objects representing the methods, message name strings, and object literals corresponding to \nthe compiled code, This includes all information needed to recompile methods but not the source code \nitself.t The space consumed by debugging information varies with the degree of optimization. Optimized \nmethods show a higher relative space overhead than unoptimized methods because the debugging information \nfor an inlined method is typically larger than the inline-expanded code. Therefore, the debugging information \ngrows faster with more aggres\u00adsive inlining than the compiled code. The total space overhead for debugging \nis reasonable. In the standard system, debugging information uses slightly more space (122%) than the \ninstructions themselves; in the system that optimizes everything, the overhead is 233%. In other words, \nadding the debugging information increases space usage (excluding the dependencies) by a factor of between \n2.2 and 3.3. In order to be conservative, we have left out the space used by the method headers even \nthough they would also be needed in a system without debugging. (The headers contain the lookup key and \nvarious control information.) If we include the headers, debugging increases space usage by a factor \nof between 1.8 and 2.6. The cost of supporting changes to running programs is smaller. The dependency \ninformation occupies between 0.9 and 1.6 times the size of the instructions. Furthermore, the * ~i~ groupkg \nis a stight simplification. For example, the Compiter occasionally generates instructions just to support \ndebugging. Also, a small portion of the relocation information can be attributed to the code rather than \nthe debugging information. However, these simpliftcattons do not significrmdy distort the numbers presented \nhere. Category I, adaptive (default) I optimized 1 onty I Machine instructions actual machine instruc\u00ad \n1.00 1.00 tions and control infor- Method headers mation 0.56 0.43 to irtvatidate code after Dependency \nlinks 0.92 1.69 programming changes 0.42 1.09 to recreate the source\u00adlevel state of optimized 0.17 0.17 \ncode and to recompile methods 0.24 0.39 0.39 0.69 I II Table 1: Space cost of debugging information \n(relative to instructions) current representation of the dependencies contains signifi\u00adcant redundancies. \nAn alternate implementation could prc}b\u00adably reduce the space usage significantly [Cha92]. As a rough \ncomparison, when compiling the SELF virtual machine, a standard commercial C++ 2,1 compiler gener\u00adated \n55 Mbytes of debugging information on top of an executable of 2.4 Mbytes, incurring an overhead of a \nfactor of 24. Apparently, the compiler produced multiple copies of identical debugging information, one \ncopy per object file, and the linker included them all in the executable file. Using the GNU C++ compiler \nand GNU-specific pragmas, we were able to reduce the space overhead of debugging inform\u00adation to 11.2 \nMbytes, for a factor of 5.7.t While tlhis comparison should be taken with a grain of salt, it indicates \nthat despite the increased functionality, the space overhead of debugging in our system is probably comparable \nto other systems. During the design and implementation of our current data structures, simplicity was \nconsidered more important than space efficiency, and we did not optimize our representation much. For \nexample, our method headers and scope repre\u00adsentations use 32-bit words in many place!s where 16 or even \n8 bits would suffice. A reorganization of these data structures could therefore result in significant \nsavings. Otlher techniques such as generating the debugging information on demand by re-executing the \noptimizing compilation could save even more space at the expense of longer pauses during debugging interactions. \nFurthermore, little of the debugging information needs to remain in main memory at all times, and much \nof it can be paged out, Ultimately, only the machine instructions need be in main memory for working, \ndebugged programs, thus keeping real memory costs down to a fraction of the tc~tal virtual memory costs. \n 8. Related work The Smalltalk-80 system described by Deutsch and Schiffman [DS84] pioneered the use \nof dynhmic compila\u00adtion and interrupt points. To hide the effects of compilation to native code, compiled \nmethods included a mapping from compiled code to source position. Activations normally were created on \nthe stack for run-time efficiency but w(ere converted on demand to the full-fledged activation objects \nrequired by the language definition, and cimverted back when needed for execution. As in our system, \ninterrupts were delayed until the next call or backward branch. Since t GNU C++ attows the source-level \ndebugging of optimized code but offers only restricted functionality. Many optimization are not transparent \nto the programmer. The compiler version used for the measurements was GCC 1.94.7 version 2.0 generates \na significantly larger executable. the compiler performed no global optimization, the system could provide \nexpected behavior without deoptimization. Zurawski and Johnson [ZJ91] describe a model for a debugger \n(developed concurrently with this work) which closely resembles ours, using inspection points and dynamic \ndeoptimization to provide expected behavior for optimized code. However, the system does not use lazy \nconversion. Furthermore, their definition of inspection points allows asynchronous events such as user \ninterrupts to be delayed arbitrarily. Some of their ideas were imple\u00admented for the Tjqxxl Smalltalk \ncompiler [JGZ88], but the system apparently could only run very small programs and was not used in practice, \nunlike our system which is in daily use. Most of the other work on debugging optimized code places the \npriority on optimization; the goal was to get as much debugging as possible while preserving code efficiency \n[CMR88, SW78]. Hennessy [Hen82] addresses the problem of recovering the values of variables in the presence \nof selected local and global code reordering optimization. His algorithms can usually detect when a variable \nhas an incor\u00adrect value (in terms of the source program) and cart some\u00adtimes reconstruct the source-level \nvalue. In contrast, we are not willing to accept any debugging failures and therefore do not perform \noptimization which would create such situ\u00adations at an interrupt point. Zellweger [Ze183, Ze184] describes \nan interactive source\u00adlevel debugger for Cedar which handles two optimization, procedure inlining and \ncross-jumping, to provide expected behavior in most cases. While her techniques can always recover the \nsource-level values of variables, they cannot hide certain code location problems; for example, single\u00adstepping \nthrough optimized code would be difficult, Since our system uses unoptimized code in these situations, \nwe are able to circumvent these problems, LOIPE ~ei83] uses transparent incremental recompilation for \ndebugging purposes. For example, when the user sets a breakpoint in some procedure, this procedure is \nconverted to unoptimized form to make debugging easier. However, LOIPE cannot perform such transformations \non active procedures. Thus, if the program is suspended in an opti\u00admized procedure, it is generally not \npossible to set a break\u00adpoint in this procedure or to continue execution by single\u00adstepping. To mitigate \nthis problem, users were able to specify the amount of optimization to be performed (possibly impacting \ndebuggability) and the amount of debugging transparency needed (possibly affecting code quality). As \nfar as we know, most of the support for opti\u00admized code in LOIPE was not actually implemented. Tolmach \nand Appel [TA90] describe a debugger for ML where the compiler always performs optimizations, but where \nthe program is automatically annotated with debug\u00adging statements before compilation. To debug an optimized \nprogram, the programmer has to manually recompile and re\u00adexecute the program. Like tqtoptimized programs, \nannotated programs run significantly slower than fully optimized programs.  9. Conclusions Global optimization \nneed not impair source-level debug\u00adging, The SELF system increases programmer productivity by providing \nfull source-level debugging of globally opti\u00admized code. To the best of our knowledge, SELF is the first \nsystem to do so; other systems either compromise on debug\u00adging functionality or severely restrict the \nkinds of optimiza\u00adtion that can be performed. In SELF, the compiler can perform optimization such as \nconstant folding, common subexpression elimination, dead code elimination, proce\u00addure integration, code \nmotion, and instruction scheduling without affecting debuggability. Two techniques make this possible: \nlazy deoptimization and interrupt points. The optimizations performed by the compiler are hidden from \nthe debugger by deoptimizing code whenever necessary, Deoptimization supports single\u00adstepping, running \na method to completion, replacing an inlined method, and other operations, but only affects the procedure \nactivations which are actively being debuggd, all other code runs at full speed. Debugging information \nis only needed at relatively widely-spaced interrupt points, so that the compiler can perform extensive \noptimizations between interrupt points without affecting debuggability. Our debugging techniques is not \nspecific to SELF and could be applied to other programming languages as welll. Acknowledgments: We wish \nto thank Ole Agesen, Lars Bak, Bay-Wei Chang, Peter Deutsch, David Keppel, and John Maloney for their \nvaluable comments on earlier drafts of this paper.  References [Cha87] David Chase. Garbage Collection \nand Other Optimiza\u00ad tion. Ph.D. dissertation, Computer Science Depart\u00ad men~ Rice University, 1987. [Cha92] \nCraig Chambers. The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object- \nOriented Programming Languages. Ph.D. dissertation, [CMR88] [CUL89] [CU90] [CU91] [DS84] [Fei83] [HCU91] \n[Hen82] [JGZ88] [SW78] Computer Science Departmen~ Stanford University. March 1992. Deborah S. Coutant, \nSue Meloy, and Michelle Ruscetta. DOC: A Practicat Approach to Source-Level Debugging of Globally Optimized \nCode. In Proceedings of the SIGPLAN 88 Cot@erence on Programming Language Design and Implementation, \npp. 125-134. Craig Chambers, David Ungar, and Elgin Lee. An Effi\u00adcient Implementation of SELF, a Dynamically-Typed \nObject-Oriented Language Based on Prototypes. Jn 00PSLA 89 Conference Proceedings, pp. 49-70, New Orleans, \nLA, October, 1989. Published as SIGPLAN Notices 24(10), October, 1989. Also published in Lisp and Symbolic \nComputation 4(3), Kluwer Academic Publishers, June, 1991. Craig Chambers and David Ungar. Iterative Type \nAnal\u00adysis and Extended Message Splitting: Optimizing Dynamically-Typed Object-Oriented Programs. In Proceedings \nof the SIGPLAN 90 Conference on Programming Language Design and Implementation pp. 150-164, White Plains, \nNY, June, 1990. Published as SIGPLAN Notices 25(6), June, 1990. Also published in Lisp and Symbolic Computation \n4(3), Kluwer Academic Publishers, June, 1991. Craig Chambers and David Ungar. Making Pure Object- Oriented \nLanguages Practical. In 00PSLA 91 Corr$er\u00ad ence Proceedings, pp. 1-15, Phoenix, AZ, October, 1991, Published \nas SIGPLAN Notices 26(11), November, 1991. L. Peter Deutsch and All an M. Schiffman. Efficient Implementation \nof the Smalltalk-80 System. In Confer\u00adence Record of the Eleventh Annual ACM Symposium on Principles \nof Programming Languages, pp. 297-302, Salt Lake City, UT, .January, 1984. Peter H. Feiler. A Language-Oriented \nInteractive Programming Environment Based on Compilation Technology. Ph.D. dissertation, Carnegie-Mellon \nUniversity, 1983. Urs Holzle, Craig Chambers, and David Ungar. Opti\u00admizing Dynamically-Typed Object-Oriented \nProgramm\u00ading Languages with Polymorphic Inline Caches. In ECOOP 91 Conference Proceedings, pp. 21-38, \nGeneva, Switzerland, July, 1991. Published as Springer Vet-lag LNCS 512, 1991. John L. Hennessy. Symbolic \nDebugging of Optimized Code. ACM Transactions of Programming Languages and Systems 4(3), July 1982. Ralph \nE. Johnson, Justin O. Graver, and Lawrence W. Zurawski. TS: An Optimizing Compiler for Smalltalk. In \n00PSLA 88 Conference Proceedings, pp. 18-26, San Diego, CA, October, 1988. Published as SIGPLAN Notices \n23(11), November, 1988. H. Schlaeppi and H. Warren. Design of the FDS Interac\u00adtive Debugging System. \nIBM Research Report RC7214, IBM Yorktown Heights, July 1978. ~A90] Andrew P. Tolmach and Andrew W. Appel. \nDebugging Standard ML Without Reverse Engineering. In Proceedings of the 1990 ACM Conference on Lisp \nand Functional Programming, Nice, France, June 1990, pp. 1-12. [US87] David Ungar and Randall B. Smith. \nSELF The Power of Simplicity. In 00PSU 87 Conference Proceedings, pp. 227-241, Orlando, FL, October, \n1987, Published as SIGPLAN Notices 22(12), December, 1987. Also published in Lisp and Symbolic Computation \n4(3), Kluwer Academic Publishers, June, 1991. [Ze183] Polle T. Zellweger. An Interactive High-Level Debugger \nfor Control-Flow Optimized Programs. Xerox PARC Technical Report CSL83-1, January 1983. [Ze184] Polle \nT. Zellweger. Interactive Source-Level Debugging of Optimized Programs. Ph.D. dissertation, Computer \nScience Department University of California, Berkeley, 1984. Also published asXerox PARC Technical Report \nCSL-84-5, May 1984. [ZJ91] Lawrence W. Zurawski and Ralph E. Johnson. Debug\u00adging Optimized Code With \nExpected Behavior. Unpub\u00adlished manuscript, 1991.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>SELF's debugging system provides complete source-level debugging (<italic>expected behavior</italic>) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically <italic>deoptimizing</italic> code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete <italic>interrupt points</italic>; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles <italic>programming changes</italic> during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.</p>", "authors": [{"name": "Urs H&#246;lzle", "author_profile_id": "81100400656", "affiliation": "", "person_id": "P286890", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}, {"name": "David Ungar", "author_profile_id": "81100365263", "affiliation": "", "person_id": "P64183", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143114", "year": "1992", "article_id": "143114", "conference": "PLDI", "title": "Debugging optimized code with dynamic deoptimization", "url": "http://dl.acm.org/citation.cfm?id=143114"}