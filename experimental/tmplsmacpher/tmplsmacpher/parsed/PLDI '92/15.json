{"article_publication_date": "07-01-1992", "fulltext": "\n A General Framework for Iteration-Reordering Loop Transformations (Technical Summary) Vivek Sarkar \nIBM Palo Alto Scientific Center 1530 Page Mill Road Palo Alto, CA 94304 (vivek@paloalto .vnet. ibm. \ncorn) Abstract This paper describes a general framework for representing iteration-reordering transformations. \nThese transformations can be both matrix-based and non-matrix-based. Trans\u00adformations are defined by \nrules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating \nnew initialization statements. The framework is extensible, and can be used to represent any iteration\u00adreordering \ntransformation. Mapping rules for several com\u00admon transformations are included in the paper. 1 Introduction \nIteration-remdering transformations are a special class of program transformations that only change the \nexecution or\u00adder of loop iterations in a perfect loop nest without changing the contents of the loop \nbody. Iteration-reordering trans\u00adformations are used extensively by restructuring compilers for optimizing \nvector execution, parallel execution, and data locatity. Prior work on iteration-reordering loop transfor\u00admations \ncan mainly be classified as either a) proposing new iteration-reordering transformations, or b) developing \na unified system for matrix-based iteration-reordering trans\u00adformations (also known as unimodular transformations). \nSeveral iteration-reordering transformations have been pro\u00adposed over the years (e.g. interchange, reversal, \nper\u00admutation, skewing, parallelization, strip-mining, blocking, coalescing, interleaving); however, these \ntransformations were defined in isolation, without a framework to help combine them together. Likewise, \na lot of work has been done on developing a unified framework for matrix-based iteration-reordering loop \ntransformations. However, the set *R.Tfrekkatftparttyby NSFPYI Award#MIP-9058-439. supported Permission \nto copy without fee all or pert of this materiel is granted provided that the oopiee ara not made or \ndietributadfor direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appeer, and notice ie given that copying ie by permission of tha Association for Computing \nMachinery. To copy otherwisa, or to rapublish, requiree a fae and/or specific permission. ACM SIGPLAN \n92 PLD1-6/92/CA a 1992 ACM 0-89791-476-7/92/0006/01 75..,$1.50 Radhika Thekkath Dept. of Comp. Science \nand Eng. FR-35 University of Washington Seattle, WA 98195 (radhika@cs. washington. edu) of matrix-based \ntransformations is too restrictive to suffice for a practical compiler system e.g. none of parallelization, \nblocking, coalescing, interleaving can be represented by a transformation matrix. The success of past \nwork on unification of unimodular transformations arose from the special properties of unimod\u00adular matrices \n1 that represent these transformations. From these properties it follows that multiplication of two uni\u00admodular \nmatrices results in a unimodular matrix. This abil\u00adity to compose unimodular transformations by multiplying \nthe transformation matrices gives rise to a natural unifying framework, Thus, a sequence of unimodular \ntransforma\u00adtions can be combined into a single transformation matrix, and then applied only once to the \nloop nest, instead of re\u00adpeatedly applying each transformation in the sequence to the loop nest to obtain \nthe final form. However, as mentioned earlier, the unimodular framework is restrictive because it cannot \nrepresent iteration-reordering transformations such as parallelization, blocking, coalescing, or interleaving. \nIn this paper, we present a general framework for iteration-reordering transformations. We define an \niteration\u00adreordering transformation to be a sequence of template instantiation from a small but extensible \nkernel set of transformation templates. A transformation template is defined by rules for mapping depc~dence \nvectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. \nThe mapping rules for an iteration\u00adreordering transformation are derived automatically from the mapping \nrules for its constituent kernel transformations, It is easy to see that our transformation system is \nclosed under composition, since the composition of two transfor\u00admations can simply be represented by \nthe concatenation of their sequences of template instantiation, The effect of a transformation is to \nmap an input perfect loop nest into an output perfect loop nest (possibly with a different number of \nloops). Our framework provides a systematic way for testing legality and generating code for a given \nsequence of iteration-reordering loop transformations. It frees the Amatrixisummodularif itsatisfiesthefollowingthreeproperties,1) \nItisa squarematrix,2) AU itseIementsareintegers,and3) Itsdeterminant is&#38;1. Thethreemostcommonlyusedunimodulartransfonnations \nareloop reversaI,Ioopinterchangeandloop skewing. compiler writer from the burden of dealing with various \niteration-reordering transformations in isolation, and allows her/him to concentrate her/his efforts \non the problem of selecting transformations to satisfy a given optimization goal. The rest of the paper \nis organized as follows. Section 2 provides a description of our framework and describes the set of \nkernel transformation templates considered in this paper. Section 3 defines the mapping rules for sets \nof data dependence vectors, and shows how the rules are used to test if a given transformation is legal \nfor a given loop nest. Section 4 defines the mapping rules for loop bounds expres\u00ad sions, and shows how \nthe rules are used to test if a given transformation is legal for a given loop nest. Section 4 also provides \ncode generation rules for the transformations; these rules can be composed together to generate code \nfor any arbitrary sequence of transformations. Section 5 discusses related work, and briefly outlines \nsome of the advantages of using this general framework. Section 6 contains our conclusions. Appendix \nA uses a matrix multiply loop nest as an example to show the application of a non-trivial iteration\u00ad \nreordering loop transformation.  The Sequence Representation in the Framework We define an iteration-reordering \ntransformation to be a sequence of template instantiations from a small but ex\u00adtensible kernel set of \ntransformation templates. A frans\u00adformalion template has parameters and providing specific values for \nthese parameters creates a particular instantiation of this transformation template. The set of transformation \ntemplates used in the framework is called the kernei set of loop transformation templates. The optimizer \nsel~ts templates from this kernel set and instantiates them with specific values so as to build a desired \niteration-reordering transformation as a sequence of template instantiations. The transformation template \nspecifies rules for mapping dependence vectors, mapping loop bound expressions, and creating initialization \nstatements for a transformed loop nest. The dependence vectors of a loop nest are used to test the legality \nof applying a transformation, with respect to the data dependence constraints of the original loop nest. \nWhen the loop nest is transformed, its dependence vectors will also change, as specified by the dependence \nvector mapping rules. This avoids recomputing the dependence vectors for the transformed loop nest, which \nis in general an expensive operation. The mapping rules for dependence vectors are discussed in greater \ndetail in Section 3. Transformation of loop bounds and creation of initialization statements are actions \nrequired to generate code for the transformed loop nest. The preconditions in the loop bounds mapping \nrules are used to test the legality of applying a transformation, with respect to the loop bounds constraints. \nThe mapping rules for loop bounds are discussed in greater detail in Section 4. The kernel set of transformation \ntemplates assumed in this paper is shown in Table 1 along with a description of their parameters. The \npower of our framework lies in the following unifying principles: 1, Template instantiation An instantiation \nof a loop transformation template is obtained by supplying values for the parameters re\u00adquired by the \ntemplate. For example, the parameter re\u00adquired by the P a ral lel i ze transformation template are n \n= loop nest size, and par-ag = set of loops to be parallelized. 2. Sequence representation for loop transformations \nAn iteration-reordering loop transformation is defined tobe a sequence, T =< tl, .,tk >, where each t, \nis an instantiation of a kernel loop transformation template. Informally, the effect of transformation \nT is the cumulative effect of first applying tl, then applying tZ, and so on up tO tk. Therefore, a mapping \nrule for transformation T is derived by composing the corre\u00adsponding mapping rules for kernel transformations, \nt1, t~,etc. Using the sequence representation, the composition of transformations T =< fl, .. ,th > and \nU =< Ul, ....u( > is defined by sequence concatenation to bel OT =< tl, ....fk. ul, ul.,ul >. For the \nsake of efficiency, the concatenated sequence can be reduced in length by composing t k and u1 into a \nsingle template instantiation, whenever it is possible to do so. For example, if fk and u 1 are both \niIkWWkWiOIIS of the Unimodul a r template, then their composition can be replaced by a single Unimodul \na r template instantiation with a transformation matrix that equals the product Ofu1 and t k S transformation \nmatrices. 3. Legality test There is a uniform method to test if a given loop transformation sequence, \nT =< t~,....t ~ >, is legal for a given loop nest JV. This legality test consists of two parts: (a) Dependence \nvector test Using the dependence vector mappings for the template instantiation (ti s) contained within \ntransformation T, check if the transformed set of JV sdependence vectors contains a lexicograph\u00ad ically \nnegative2 tuple. If so, transformation T is illegal for loop nest N. This legality test does not require \neach individual template instantiation in T to be legal when applied, only that the entire sequence be \nlegal (see Section 3 for more details). (b) Loop bounds test Using the loop bound expression mappings \nfor the template instantiation (ti s) contained within transformation T, check if any of t, s loop bounds \n2Lexlcographicaltynegative]sdefinedinSection3, withanexample. Kernel Set of Templates Description n \nis the input loop nest size (in all templates). M is then x n unimodular Unimodular(n, M) transformation \nmatrix specifying the desired transformation [4]. rev is a reverse mask such that rev[i] = true implies \nloop i should be ReversePermute(n, rev, per?n) reversed, and perrn is apermutation map indicating that \nloop i should be moved to position perrn [i] after all reversals have been done. Parallelize(n, par~iag) \nparflag[i] = true means that loop i is to be parallelized, The range i. . .j specifies a set of contiguous \nloops to be blocked (tiled), and bsi.ze is a vector such that b.siz e [k] contains an expression for \nthe Block(n, i,j, hsize) block size of loop k. Blocking can be viewed as a combination of strip mining \nand interchanging [15]. The range i. . j specifies a set of contiguous loops to be coalesced (orCoalesce(n,i, \nj) collapsed) into a single loop [11]. The contiguous range i. . .j speeifies all loops to be interleaved. \nThe value isi.ze [k] defines the interleave factor for loop k. This is similar to B1 oc k. In both, the \nouter loop iterates between blocks and the inner loop Interleave(n, i,j, isi:e) iterates between the \nelements of this block. In the B10 ck transformation, every block is a set of contiguous iterations in \nthe original loop, while in the I nte rle a.ve transformation, a block consists of non-contiguous iterations \nfrom the original 100p,based on the interleave factor, Table 1: Kernel Set of Transformation Templates \npreconditions is violated. If so, transformation erwise must be modified, or initialization state-T is \nillegal for loop nest N. This legality test ments must be inserted at the the top of the requires the \nloop bounds preconditions to be sat-loop body to initialize the old index variables isfied for each individual \ntemplate instantiation in as functions of the new index variables. Our T (see Section 4 for more details). \nframework uses the latter approach. Figure l(a) The basic algorithm for the legality test remains the \ncontains a simple loop nest that is transformed same, regardless of the kernel set of loop transforma-by \nskewirtg the j loop with respeet to the i loop tion templates, K, the loop transformation, T, or the \nand then interchanging the two loops. Figure l(b) loop nest, N. shows the transformed loop, when it is \ngenerated using initialization statements (the approach used 4. Code generation for the transformed loop \nnest by our framework). There is a uniform method to generate code for the transformed loop nest, N \n, obtained by applying trans- The initialization statements are automatically formation T =< tl, .... \nt~ > on loop nest N. This generated for loop nest N by applying the rules method consists of two parts: \nfor each template instantiation t,. The set of initialization statements, 1.VITt, for template in\u00ad (a) \nGenerate new loop bound expressions stantiation t, defines t~ s input index variables as For each transformation \ntemplate ti in the se\u00adfunctions of t, s output index variables. There\u00ad quence T, apply its rules for \nmapping loop fore, the sets of initializations statements for the bounds expressions starting with the \nloop nest JY, sequence tl, . . . . fk, should be emitted in the to finally obtain the transformed loop \nnest N . order I.VITk, . . . . INIT1. Note that N need not have the same number of loops as IV. In general, \nthe index variables used for loop nest N will be different from those used for loop nest N,  (b) Generate \nnew initialization statements 3 Data Dependence Vectors Many transformations change the names of the \nindex variables or create additional index vari-In section 3.1 we define the dependence veetor representa\u00adables. \nThus, statements in the loop body that use tion assumed by our framework. In section 3.2, we discuss \nthese index variables as array subscripts or oth-the legality tests and mapping rules for dependence \nvectors. do i = 2, n-1 do j= 2, n-1 a(i, j) = (a(i, j)+a(i-l, j)+ a(i, j-l)+ a(i+l, j)+a(i, j+l) )/5 \nenddo enddo The transformation is to skew the j loop w.r,t the i loop and then interchange the two loops. \nFigure l(a): Loop nest and transformation 3.1 Understanding Dependence Vectors Following prior work \nin the area of iteration-reordering loop transformations, we represent the data dependence constraints \nfor a loop nest by a sec of dependence vectors D: Definition 3.1-A dependence vector fora loop nest of \nsize n is an n-tuple, d = (dl, .... d.,), where entry d~ corresponds to the kth loop (counting from outermost \nto innermost). In practice, there are two kinds of values for d~ that are of interest [9]: 1. Distance \n dk is an integer value, d~= y G2?, 2. Direction d~ is one of the six values, + (pos\u00aditive),  (negative), \n+ (non-negative), + (non\u00adpositive), * (non-zero), * (any) 3. . Let us use S(dk ) to denote the set of \ninteger values that is represented by d~. In the case of a distance value, S(d~ ) is a singleton set, \ni.e. ,S (dk) = {y}. Wlhendk is one of the six possible direction values, then S(d,L) = {z lx E Z A z \ns sign is contained in d~}. A direction value is used when the exact dependence distance is unknc)wn. \nHence it is an imprecise notation that is used to denote the direction of the dependence flow and represents \nall possible integer values in that direction. The set of integer tuples denoted by dependence vector \n~={dl,..., d,,,} is given by Tuples(c~) = S(dl ) x ...x S(d~ ) i.e. the Cartesian product of the integer \nsets corre\u00adsponding to dl, ..., dk. If D is the set of all the dependence vectors for a loop nest, then \nTuples( D) is simply the union of the tuple sets for each dependence vector in D, that is, Tuples(D) \n= U;,_~ Tuples(~). Definition 3.2 An integer tuple, ii = (al, . . . . an ) is lexico\u00adgraphically negative \n(positive) if and onl:yif its first non-zero element, aa, is negative (positive) i.e. aj = O,V1 ~j <i, \nandai<O(ai>O). D Definition 3.3 An execution instance of a loop nest of size n is defined as a n-tuple, \nd = (al) . . . ,tin), where a,k, specifies the iteration number of the ,Vh loop. . The set of dependence \nvectors for a loop nest, D, en\u00ad forces a partial order on the execution instances of the sArt~l~matenotationforthesixdirectionvahmsistousetherelational \noperators, < , > , <, , ~ , # , and * respectively[15]. We do not representan = directionmourframeworkbecauseitisequivalenttoa \nzerodistance. do jj = 4, n+n-2 doii = max(2, j-n+l), min(n-ltj-z) j=jj -ii i= ii a(i, j) = (a(i, j)+a(i-l, \nj)+ a(i, j-l) +a(i+l, j)+a(i, j+l) )/5 enddo enddo Figure l(b): Transformed loop with init statements \nloop nes~ for any two distinct execution instances II and ;, if their difference ; d belongs to Tuples(D), \nthen instance ~ must be executed after instance d. This partial order summarizes all the iteration-reordering \nconstraints imposed by data dependence. We assume that the original execution order satisfies this partial \norder. This implies that Tuples( D) cannot contain a lexicographically negative integer tuple for the \noriginat loop nest (otherwise the original execution order can be shown to violate some data depen\u00addence \nconstraint). Further, if D is the set of dependence vectors that result after a loop transformation is \napplied, and Tuples(D ) contains a lexicographically negative tuple, then the transformed loop nest can \nbe shown to violate some data dependence constraint and so the transformation must be illegal. This fact \nforms the basis of the data dependence legality test discussed in subsection 3.2. The original set of \ndependence vectors for a perfect loop nest is computed using standard data dependence analysis techniques \n[4, 15, 10, 6, 12]. To obtain the best precision possible, it is recommended that a dependence vector \ncon\u00adtaining summary direction values (i.e. one of # , ~ , + , * ) be expanded into an equivalent set \nof dependence vectors that contain no summary direction values. 3.2 Transformation of Dependence Vectors \nIn this subsection, we describe how a set of dependence vectors is transformed by a given iteration-reordering \nloop transformation. Consider a set of dependence vectors D, and an iteration-reordering loop transformation \nT =< tl, ....fk >. Using the sequence structure defined in Section 2, we compute the transformed set \nof dependence vectors as T(D) = Dk, where each Di = ti(Di 1) is computed by applying the dependence vector \nmapping rules for kernel transformation ~~on Di 1 VI< iS k, and Do= D is the initial set of dependence \nvectors for a loop nest, ,v. If we determine that Tuples(T(D)) contains a Iexicographically negative \ntuple4, then we return I.sLc.qa/(T, N ) = f al se. Note that each individual trans\u00adformation stage need \nnot be legal, only that the final result be legal. This is because the final set of tuples denotes the \nfinal  4Given a transfomreddependencevector d? c D , it Is easYto determineby examining,1 whetheror \nnot Tuples(C~) containsa lexico\u00adgraphicaffynegativetuple. execution order of the loop nest iterations, \nand the execution 4 Transformation of Loop Bounds Ex\u00ad order in an intermediate transformation step is \nimmaterial. Table 2 outlines the dependence vector mapping rules for all the transformation templates \nconsidered in this paper. In general, an instantiation of a transformation template maps an input dependence \nvector set, D, into an output depen\u00addence vector set, D . In fact, all transformation templates in Table \n2, except Block and Interleave, map a given dependence vector ~ c D into a single dependence vector, \n#E D ; the Block and Interleave transformations may map d E D into as many as 2~ ~t 1dependence vectors \nin D (this is one reason why they cannot be represented by a matrix). Figure 2(a) shows a sample loop \nnest and its dependence vectors. Figure 2(b) shows a simple Reve rsePe rmut e transformation and the \nresult of applying the correspond\u00ading rules to the original vectors. This interchange trans\u00adformation \nis illegal because it creates a lexicographically negative dependence vector, ( 1, 1). However, note \nthat first reversing loop j makes interchange legal; this new ReversePermut e operation is shown along \nwith the resulting dependence vectors in Figure 2(c). We require all data dependence mapping rules to \nbe consistent. A dependence vector mapping rule is said to be consistent if and only if it preserves \ndata dependence among execution instances. This idea is formalized in the following definition Definition \n3.4 A dependence vector mapping rule, D K D , for transformation template t is consistent if and only \nif Tuples(D ) ~ {t(~) t(d)l~ d G Tulles}, where t(d) and t(~) are executions instances in the transformed \nloop nest corresponding to execution instances d and ~ in the originat loop nest. . Theorem 3.5 All the \ndependence vector mapping rules in Table 2 are consistent. . The proof for Theorem 3.5 is omitted due \nto space limita\u00adtions. The consistency property for mapping rules ensures that a mapping rule does not \nremove any dependence constraints. Another important property for mapping rules is preci\u00adsion, i.e., \ndoes the mapping rule introduce any extra cross\u00aditeration dependence? Informally, we can say that mapping \nrule D I--+ D is equivalent to or more precise than mapping rule D s D , if Tuples( D ) ~ Tuples( D ) \nis always true. If all dependence vectors in D only contain direction values, then we can prove that \nall mapping rules in Table 2 are as precise as possible . If only distance values are present in D, then \nthe result applies to the Reve rsePermut e and Paral lel mapping rules; in all other cases, there is \na loss of precision when distance values are approximated by direction values. Proofs omifted due to \nspace limitations.  pressions In section 4.1, we discuss the legality constraints imposed by loop bounds, \nas well as our classification of loop bounds that makes it convenient to test the legality constraints. \nIn section 4.2, we define the preconditions and mapping rules for atl the transformation templates considered \nin this paper. In subsection 4.3, we outline the matrix-based representa\u00adtion used to support efficient \ntransformation of loop bounds expressions Figure 3 shows the general structure of how a loop nest of \nsize n is transformed into a loop nest of size n by a kernel template instantiation. To obtained the \ncombine-d effect of a sequence of template instantiations, this kind of transformation is repeated for \neach template instantiation in the sequence such that the output loop bounds and loop body from one template \ninstantiation serve as the input for the next template instantiation. Expressions /k, Uk, sk in Figure \n3 contain the lower, upper, and step bounds expressions for the original loop nest. A bounds expression \nfor loop k may depend on index variables x 1, ..., ~k 1. we assume that no bounds expression for loop \nk> 1 has aside effect (if a bounds expression for loop k> 1 had a side effect, it would make loops k \n1 and k imperfectly nested). 100PL.E {do, pardo} indicates if input loop k is sequential or parallel. \nExpressions 1~,U~,s~. contain the 10WW uPPer, and step bounds expressions for the transformed loop nest. \n100P~ c {do, pa rdo} indicates if transformed looP k is sequential or paratlel. In general, the transformed \nloop nest will have n initialization statements to initialize the original index variables, z 1, . . \n. . X,Z,as functions of the new index variables, z~, . . . . z;,,. We make a special effOrt to reuse \noriginal index variable names by avoiding initialization statements of the form $i = x;. 4.1 Loop Bounds \nLegality Constraints The lower bound, upper bound or step in a loop statement is generally an expression \nthat involves constants, index variables of outer loop statements, and other terms that are invariant \nin the loop nest. A transformation may be applied to a given loop nest only if these expressions satisfy \nthepre\u00ad conditions for applying this transformation. For example, consider the doubly-nested triangular \nloop in Figure4(a) and assume that there are no data dependence constraints. These loop bounds satisfy \nthe preconditions for a Unimodula r transformation, so it is legal to permute the loops (say), and the \nresulting loop nest is shown in Figure 4(b). Figure 4(c) contains a triply-nested loop for computing \nthe matrix product of a dense matrix (b) and a sparse matrix (c), with the result stored in a dense matrix \n(a). The lower and upper bounds of loop k are nonlinear functions of j thus making it illegal to interchange \nloops j and k. However, the preconditions for the Reve rseP e rmut e transformation Transformation temrYate \n. Unimodular(n, M) ReversePermute (n, perrn, rev) Parallel(n, par~lag) Block(n, i, j, bsize) Coal esce(n, \ni, j) Interleave(n, i,j, ~s;ze) I De!)endence vector mapping rule ~-. .. D!= {1$ Ii ED A1 =Mxci where \nill x ~is a matrix-vector product, } appropriately extended for direction values [9, 14]. D = { (dj,..., \ndj) I (dI,..., dn)~DA d ~erm[,l =(rev[k] ?reverse(d~): d,) } where reverse(d~ ) is computed as follows: \n dk I + I l + l # l * ] * ]~E2 reverse(d~) II  I + I # ] f I + I * 1 Z D = { (d~,..., d~) I (all,. \n,,, dn)~D A d~ = (parflag[k] ? wmnap(d~): d~) } where parmap(dk ) = D = { (all, . . ..dl. d;,; ,d; \n,d:,,,d; ,dj+l,, dj+l, . . ..dn)[ (dI,..., dn) CDA (d~, d;) G blockmap(dk) Vi ~ k < j where blockmap(dk) \nz I d~ and dir(dk ) = < + , \\ D = { (all, . . ..dl. (d,,..., dn)  { (0,0)} { ( * , * )} { (O,d~), (dk, \n* ) } } ifd~=O ifdk = * ifd~ = Ior 1 { (O,dk), (dir(dk), * ) } otherwise { if dk is a direction value \nor if dk = O if dk is a positive distance value if dk is a negative distance value d,,dj+l,l, dn), dn) \nI cD Ad = mergedirs(dir(d~ ),..., dir(dj)) } where the function mergedirs is evaluated by a pairwise \nmerging of its arguments, for example, mergedirs(+, ) = +, and so on. Similar to Block, but use imap \ninstead of blockmap dk + , + ,x E z+ ,, +!, ~ ~ z-1 *., *, //, +/), ({_,, +,) ] ~ { (t+/, +,),( +,, \nJ)} I i~p(dk) 11{(0:0)}1{( * , * }  Table 2: Dependence vector mapping rules for kernel transformations \nin K allow loop i to be moved to the innermost position since the bounds of loop k are invariant in i. \n- To maintain prwise information about the nature of loop bounds expressions, we define a function type(ezprj, \nxi) that captures how index variable Zi is used in bounds expression ezpr~ (i.e., one of lj, u3, or Sj \n), fori< j: type(ezyw~, z~) = const, if ezprj is a compile-time constant invar, if ezpr~ is invariant \nin z~ linear, if ezpr~ is linear in Zi (and the coefficient of xi in exprj is a compile-time constant) \n nonlinear, otherwise ( The type values form a totally ordered lattice in which const ~ invar ~ linear \nQ nonlinear; a predi\u00adcate expression of the form type(ezpr~, z~) ~ V will therefore be satisfied by any \ntype value that is equal to or below V in the lattice. The first column in Table; 3 and 4 shows the preconditions \nfor applying a given template in\u00adstantiation. As an example, looking at the Unimodular template, we see \nthat one of the preconditions says that t~pe(lj, xi) ~ linear. If we examine the type of the lower bound \nexpression for loop kin example Figure 4(c) as a function of j, we see that it has type = nonlinear; \nhence the precondition is violated and the Unimodular transformation cannot be applied to this example. \nFor a given loop nest IV and transformation sequence T, if a precondition is violated for any kernel \ntransformation contained within T =< tl, ....tk >, then we return IsLega/(T, N) = f al se. When testing \nfor legality, we do not actually generate the new loop bounds expressions and initialization statements \nfor each tkin the sequence T; 180 ReversePermute( n,pe?vn, rev) doi=2, n-1 n = 2,rev = [()O],perm = \n[1 0] doj=2, n-l D = {( 1, 1),(0,+)} a(i,j) = b(j) if (.. .) b(j) = a(i-l, j+l) Figure 2(b): Illegal \nTransformation enddo enddo ReversePermute( n,perm, rev) n=2, rev= [01], perrn=[l O] ~={(1, -l), (+, o)} \nD = {(1, 1),(0,+)} Figure: Loopnest anddependence vectors Figure: Legal Transformation loopl T1 = /1,Ul, \nS1 loopnzn =/n(zl, . . ..xn-l). wn(#l,. .,, rn_l), sn(zl,. ... zn_l) ... * ioop{ x; = 1;, u;, s; Xn=fn(x; \n,. ..,;,;, ) ... L Figure 3: General structure of Tr~sformed LOOp Bounds Expressions and Initialization \nStatements instead, we use a matrix-based representation (outlined in subsection 4.3) to carry sufficient \ninformation to evaluate the type predicates in the preconditions. Though max and min are nonlinear functions, \nthere are special cases in which loop bounds that are max or mi n functions of linear terms will themselves \nbe classified as type = linear. Specifically, if the step is positive and the lower bound expression \nis a max function of linear terms and/or the upper bound expression is a min function of linear terms, \nthen each term of the max or min function can be treated as a separate linear inequality [14] (if the \nstep is negative, then the special case occurs when the lower bound is a min function and the upper bound \nis a max function). This classification is useful for transforming loop nests with this special form \nof max/min functions, a form that is frequently obtained as the output of the Unimodul a r and Blocking \ntransformations. Due to space limitations, the code generation details for transforming maxlmin loop \nbounds were omitted in Tables 3 and 4. 4.2 1 ransfo~mationRules The second column in Tables 3 and 4 depicts \nthe output form of the input loop nest structure of Figure 3. This output form is obtained by applying \nthe rules for mapping loop bounds expressions and creating initialization statements. To clarify some \nnotation, when the output loop nest refers to L/oop~ , this indicates the loopk value (door pa rdo) from \nthe input loop nest. A use of loop~ indicates a loop in the output loop nest. The rules for the Unimodula \nr transformation have been studied in detail in the literature [7, 14] and are omitted here. The preconditions \nand mapping rules for the ReversePe rmut e transformation template are shown in Table 3; this template \npartially overlaps with the Uni_modular template. The preconditions for ReversePermut e require the lower \nand upper bounds expressions to be invariant (rectangular), but do not require the step expressions to \nbe compile-time constants. The preconditions for Unimodular allow the lower and up\u00adper bounds expressions \nto be linear, but require the step expressions to be compile-time constants. For cases in which ReversePermute \nand Unimodular can achieve the same result, it is preferable to usc Reve KSePermut e because a) step \nexpressions are not normalized to +1, b) index variable names are reused without creating initializa\u00adtion-statements, \nand c) matrix computations are avoided on dependence vectors. The Coalesce transformation [11] includes \nnormaliza\u00adtion of the lower bound and the step expression as part of the transformation mapping. The \nB1ock transformation (Table 4) takes special care to bound the iteration space Transformation template \nLoop nest mapping Uni.modular(n, J}I) Preconditions: V 1< i < j < n, The transformation of loop bounds \nexpressions and the creation of 1. type(ij,x~) ~ linear initialization statements have been studied in \ndetail for this case [7, 14]. 2. type(uj, xi) G linear If the (constant) step value is # 1, then the \nbounds are normalized to step 3. type(s,, z~) Q const = 1 before applying the unimodular transformation. \n Output loop nest size: n = n looppe,.n.[l] J+, m[l] = L3 4 ~4 ReversePermute( n,pcr?n, rc?) (Partial \noverlap with Unimodular) Preconditions: ioopl,,r,,, [n] r wrm[n] = L ~~;l* s; type(eqwj, z%)E i.nvar, \nwhere, for 1< k < n, where exprl is one of lj, u] or SJ, rev[k] = F Zpernt[k] , qwn[k] ~.$f?em[k] l;, \nU;, S;. = V ~ < j St. pemn[i] > pemn[j]. rev[k] = T{ UPerm[/t],~perm[k], ,$penn[k] ~Tperm[k] = Output \nloop nest size: n = n -i~,rm[~l), abs(sP..m[kl)) sgn(s~,,pnz[L.l) * mod(abs( u},p~~[~lParallel(n, par~lag) \n%w-7n[k ] loop; xl = 11,?11>s~ Preconditions: none loop;, Xn = in, Ufl, S,i Output loop nest size: \nn = n loop~ parfiu~[~] = false loop; = Vl<k <n, pardo par.fiag[k] = true loopl xl = 11, Ml, .31 loop \nx = 1,[J, 1 Coalesce(n,i, j) loop), %n=lr, (zl,... f2(trc),)fj(~. )1()t)1...)t.. Xt = f,(z )Preconditions: \ntype(ezprm, x~) ~ invar, where exp~n~is one of 1,,,, u,,,, or s,,, ,Cj = .fj (xc ) Vi~k<m<j [: = ~~=, \n[fk, Ontput loop nest size: here ; =kl+ [-1 $0) n =n (j i) if3k,i ~ k~j,suchthat Ioopk # pardo loop \n= 0 pardo otherwise { fori<k<j fk(rc)= +(ln,:;,,l k lrI;::Hs (Oopl $1 = 11,1/1,S1 Interleave(n, i,j, \nisi~~) 100p, 1: = O,isi:f [i] 1, 1 Preconditions: Vi~k<m~j, 1. @pe(L, xk) ~ linear ;OOf}j Z; = O,i.$i?f[j] \n 1, 1 2. type(un,, %k) ~ linear loop, x, = li + xl * .s~,u~, isi:f[i] *.% 3. type(sm, z,+) ~ const \n loo~j Lsi:c [j] * .9~ Xj = !J + T$ *.SJ, llJ, Output loop nest size: n =n+(j i+l) loopn .rn = [n,tln,Sn \n Table 3: Loop nest mapping for kernel transformations in K 182 doi=l, ndoi=l, n doj=l, n doj=l, ndoj=i, \nn doi. =1, j do k = colstr(j), colstr(j+l)-l ... ... a(i., j) += b(i, rowidx(k)) *c(k) Figure: Triangular \nFigure: Interchanged Figure: Nonlinear bounds Transformation template Loop nest mapping ioopl .Z1= 11, \n/41,.91 Block(n, i,j, bsize) loop~.r~=l:, u~,.s,*bsi;e[i] Preconditions: Vi<k<m~j, iOO~j .E j l; ,U~ \n, S] * b.9i2C[~] 1. type(im, z~) ~ linear /001)% X, = 1.:, ([:, .!l~ 2. type(um, r~) ~ linear 3. type(sm, \nx~) ~ const loop] xl =L~,~7~,sj Output loop nest size: n =n+(j i+l) loopn, Xn = In, Un, s~ .. where \ndi<k~jandi~h<k /j = lk(zl .. .zi_l, x~gn[k, i] . . .z~,,,,[k,lc -1]) i_ Uk zf~(z, .X,-,, z ;,lay[k, \ni] . . ..r~c.r[k. k-1]) .r~ bsi:f [h], if the cocff. of r), in any term of lk is <0 .C;nir,[l., h] = \n,ri , otherwise { if the COeff.Of$h in any term Of zl~iS> 0 z~ar[k, h] = :~ + i;([ )l ,, otherwise{ \nmax( x{., 1~), Sk > 0 1); = min(.ri., 1~), Sk < 0 min(z~+ Sk* (bsi:c[k] l), //k), sk >0  u~ = max(,r~. \n+ Sk * (bsi:~[k] 1), ~(~), .s~ < 0 [ Table 4: Loop nest mapping for kernel transformations in K so that \nonly tiles with some work are created (the only exception occurs when there is a max or mi n function \nin the loop bounds and not all the function s terms are monotonic in the same direction). This is unlike \nthe solution in [14] that creates a rectangular boundary around a trapezoidal iteration space, and hence \nmay create many tiles with no work. The blocking scheme proposed in [15] requires that all block sizes \nbe equat for the algorithm to only create tiles with some work. A more general code generation algorithm \nis given in [3], based on scanning polyhedra (e.g. it can be used to generate code for non-rectangular \ntiling); however, this algorithm can also create more tiles than necessary. 4.3 Representation of Loop \nBound Expres\u00adsions To efficiently transform the loop bound expressions through each template instantiation \nin a transformation sequence, we usc a matrix-based representation for loop bound ex\u00adpressions. Specifically, \nthis representation consists of three matrices, LB, UB, STEP, for the lower bound, upper bound, and step \nexpressions respectively. Let 7) be the number of loops in the loop nest for which the bounds are to \nbe represented. These coefficient matrices have the shape (1... ??)x(o.. .n), Matrixentry(i,j)isonlydefinedwhen \ni > j. Figure 5 shows a sample loop nest and its LB, U13 and STEP matrices. We represent multiple linear \nbounds constraints for max and mi n functions by allowing each matrix entry to contain a list of coefficient \nvalues, one per inequality. For example in the tigure, the LB matrix has an do i = max(n,3), 100, 2 max<n, \n3> 111 do j = 1, min (2, i+512), 1 LBz 1 o1.1\u00ad do k = sqrt(i) /2, 2*j, i sqrt(i)/2 Oo1\u00ad [ ... 1 enddo \n11100 J\u00adenddo IJB = mill<2, 512> min <O, 1> ~ ~ enddo o 21 [ 1 type(uz, i) = linear iype(ls, i) = nonlinear \ntype(us, j) = linear STEP = type(ss, i) = linear type = invar or const, in all other cases.   [HO::] \nFigure 5: A sample loop nest and its LB, UB and STEP matrices. entry max< n, 3 >. Each matrix entry has \na type tag. The (i, O) entry represents the loop-invariant part of the bounds expression for loop i; \nthis entry may contain an arbitrmy expression that is only evaluated at run-time. The (i, j) entry repre\u00adsents \nthe constant (integer) coefficient of index variable j in the bounds expression for loop i > j, assuming \nthat type(i, j) C linear. If type(i, j) = nonlinear, then the (i, j) entry is set to zero, and the terms \ninvolving index variable j are combined into the (i, O) entry of the matrix. This is a convenient way \nto store both linear and nonlinear terms in the matrix representation. For an example of the nonlinear \nrepresentation, look at the LB matrix entry for loop k as a function of i. Further delails on the loop \nbounds representa~ionhave been ornilted due to space limitations. Related Work There are several papers \nrelated to iteration-reordering transformations that have been published in the literature, often with \na fair amount of overlap in the material presented. Space limitations prevent us from discussing all \nthese ref\u00aderences; instead, we outline a representative subset that, to the best of our knowledge, covers \nthe prior work related to our paper. We focus on discussing prior work in the area of frameworks for \niteration-reordering transformations, rather than other (orthogonal) topics like data dependence analysis \ntechniques, or optimization algorithms that use iteration\u00adreordering transformations, or loop transformations \nthat reorder statements as well as iterations. Lamport [9] introduced the hyperplane method and the \ncoordinate method for parallel execution of iterations in a loop nest. Both methods are special forms \nof iteration\u00ad reordering transformations. The framework used in [9] included dependence vectors that \ncontain distance or di\u00ad rection values, and iteration-reordering transformations that can be represented \nby 2371+-+Z linear mappings, Further, the legality test for a linear mapping was based on the existence \nof a lexicographically negative tuple in the set of transformed dependence vectors. However, the focus \non the paper was on the two methods for rewriting a sequential loop nest into a form containing parallel \nloops, and the framework was only developed to the extent needed by these transformations. The framework \nin our paper is much more general we support linear and non-linear transformations, allow input and output \nloop nest sizes to be different, and permit all iteration-reordering transformations to be com\u00adposed \ntogether in a general way. Wolfe [15] introduced a comprehensive data dependence graph, with edges labeled \nby direction vectors, as the basis for a loop transformation framework. Several iteration\u00adreordering \ntransformations were supported by this frame\u00adwork e.g. loop interchanging, iteration space liling (block\u00ading), \nloop skewing, vectorizalion, and concurrentization. However, each transformation had its own special \nlegality test based on the direction vectors and on the nature of loop bounds expressions. Our framework \nis more general in several ways: ~,e providea single leg~ity test for dl iteration-reordering transformations, \nwe support a larger class of iteration-reordering transformations (e.g. we allow transformations of parallel \nloops), we allow dependence vectors to contain both distance and direction values, and we treat transformations \nas independent entities, separate from the data dependence graph. Allen and Kennedy [1, 2] introduced \nthe notion of loop\u00adcarried and loop-independent dependence, and defined legality tests for parallclization \nand interchange based on the idea of the level of a loop-carried dependence. They also coined the term, \nreordering transformation ; we in\u00adstead use the term, iteration-reordering transformation , so as to \nbe more specific. Our work can be viewed as a generalization of the results in [1, 21 that relate to \niteration\u00adreordering (reordering) transformations, since our legality test accommodates many other transformations \napart from parallel ization and interchange, and since we also base our legality test on the nature of \nthe loop bounds expressions. Irigoin and Triolct [8] describe a framework for iteration\u00adreordering transformations \nbased on supernode partition\u00ading, an aggregation technique achieved by hyperplane parti\u00adtioning, followed \nby iteration space tiling across hyperplane boundaries. In this framework, data dependence are repre\u00ad \nsented by dependence cones rather than dependence vectors. They also provide a general code generation \nalgorithm for any linear transformation that corresponds to a unimodular change of basis [7]. Their framework \nincorporates loop interchange, hyperplane partitioning, and loop tiling (block\u00ading) in a unified way, \nfor loop nests with linear bounds expressions. Our framework takes its inspiration from this kind of \nunified approach to loop transformations, but is more general in that it incorporates other non-linear \ntransforma\u00adtions like loop parallelization and loop coalescing, and it allows transformation of loop \nnests with non-linear loop bounds expressions by maintaining precise information on the loop bounds constraints. \nOur framework is also more practical, because it is based on dependence vectors rather than dependence \ncones, and because it incurs the overhead of general linear (unimodular) transformations only when strict.tynecessary. \nRecently, unimodular transformations have begun gain\u00ading popularity as a practical framework for iteration\u00adreordering \ntransformations. Banerjee [5] shows how trans\u00adformations like loop interchange, loop reversal, and loop \nskewing, can be represented by a unimodular transformation matrix; however, the results in the paper \nare proved only for loop nests of size = 2, and containing constant loop bounds. Wolf and Lam [14] also \nshow how unimoduku transformations can be used to unify loop interchange, loop reversal, and loop skewing; \nthe results in their paper are applicable to loop nests of arbitrary size and with linear bounds constraints. \nThey also incorporate the tiling (block\u00ading) transformation into their framework by proposing a two-step \napproach, where a unimodular transformation is followed by a tiling transformation. The unifying aspect \nof the unimodular frameworks in [5, 14] is similar in scope to the unifying approach of Irigoin and Triolet. \nHowever, the unimodular frameworks are more practical since they are based on dependence vectors (as \nin [9]), rather than dependence cones. Our framework is superior to the uni\u00admodular frameworks in several \nways: we support a larger class of iteration-reordering transformations (e.g. loop par\u00adallelization, \nloop coalescing, loop reversal/interchange with unknown strides) while still maintaining a uniform legality \ntest and code generation scheme, we allow transformation of loop nests with non-linear loop bounds expressions \nby storing precise information on the loop bounds constraints, and we incur the overhead of unimodular \ntransformations only when strictly necessary. Whitfield and Soffa [13] dctine a specification language \n(called GOSpeL) for global optimization, in which an optimization is specified by preconditions and actions. \nAn optimization can only be enabled when all its preconditions are satisfied; if enabled, the actions \nspecify how the pro\u00adgram representation should be updated to materialize the optimization. They also \ndescribe an optimizer generator tool (called GENe$is) that can be used to automatically generate an optimizer \nfrom a GOSpeL specification. The precondi\u00adtion and action rules in GOSpeL are not powerful enough to \nspecify general iteration-reordering loop transformations, as described in this paper. However, our framework \nfor iteration-reordering loop transformations would be a natural extension to a GENesis-like system since \nthe legality tests for a loop transformation sequence define the preconditions for its applicability, \nand the code generation rules for a loop transformation sequence define the actions to be performed. \nPerhaps the biggest advantage of our framework is that it treats a transformation as an independent entity, \ndistinct from the loop nests on which it may be applied. Thus, transformations may be created, instantiated, \ncomposed, and destroyed, without being tied to a particular loop nest. The advantage of this approach \nis that it makes it easy to consider several alternative transformations for a loop nest, where each \ntransformation could be an arbitrarily complex sequence of template instantiation from the kernel set. \nNote that a loop nest remains unchanged while the transformation system considers the legality and effectiveness \nof applying various alternative transformations; the loop nest only needs to be updated when code generation \nis finally requested for a particular transformation. This flexibility is useful for supporting arbitrary \nlevels of search and undo in an automatic transformation system, and can atso be used to exploit parallel \nprocessing in the compiler by concurrently evaluating the effectiveness of alternative transformations \nfor a loop nest. 6 Conclusions and Future Work We have described a novel unified framework for all iteration-reordering \ntransformations. The key strengths of this framework are: Single legality test for all iteration-reordering \nloop transformations. Ease of addition of new transformations by specifying new rules. Separation of \ntransformations from loop nests. Treatment of Parallel as just another iteration\u00ad reordcring transformation \nin the framework. Special attention to the constraints imposed by loop bounds expressions. An efficient \nand general implementation of the Reve r sePe rmut e transformation, a case that occurs frcqucntl y in \npractice. Generation of efficient code when blocking trapezoidal loops. We believe that our framework \nsupports iteration\u00adreordering transformations in their full generality. The main direction for future \nwork would be in using this framework in an automatic transformation system, so as to optimize loop nests \nfor data locality, parallel exeeution, and vector execution. Another avenue for future work lies in extending \nthis framework to include other transformations like loop distribution or loop unrolling that reorder \nboth iterations and statements. Finally, we observe that the kernel transformation map\u00adping rules for \ndependence vectors and loop bounds expres\u00adsions were derived by hand from the iteration mapping defined \nby the transformation. An interesting area of future theoretical work would be to explore the possibility \nof deriv\u00ading the dependence vector and loop bounds mapping rules automatically from a given iteration \nmapping function. This may be feasible today for the Unimodul a r transformation, but to do so for something \nlike the B1 ock transformation (where iteration mapping is defined using the mod and div operators) would \nindeed be a great challenge.  Acknowledgements The authors would like to thank Ann Campbell, Ray Eller\u00adsick, \nPing Liao, John Ng, Khoa Nguyen, Edith Schonberg, Jin-Fan Shaw, and Morgan Young for their support and \ntechnical assistance at IBM. We also thank Fran Allen, David Callahan, Michael Wolf, and people from \nthe com\u00adpiler seminar class at the University of Washington for their many comments and suggestions on \nimproving the paper. References [1] John R. Allen. Dependence Analysis for Subscripted Variables and \nits Application to Program 77ansforma\u00adtion. PhD thesis, Rice University, 1983. [2] Randy Allen and Ken \nKennedy. Automatic translation of fortran programs to vector form. ACM Transactions on Programming Languages \nand Systems, 9(4):49 l 592, October 1987. [3 Corinne Ancourt and Frangois Irigoin. Scanning polyhedra \nwith DO loops. Third ACM Symposium on Principles and Practice of Parallel Programming, pages 39 50, April \n1991. [4] Utpal Banerjee. Dependence Analysis for Super\u00adcomputing. Kluwer Academic Publishers, Norwell, \nMassachusetts, 1988. [5] Utpal Banerjee. Unimodular transformations of dou\u00adble loops. Proceedings of \nthe Third Workshop on Languages and Compilers for Parallel Computing, August 1990. [6] Gina Goff, Ken \nKennedy, and Chau-Wen Tseng. Prac\u00adtical dependence testing. Proceedings of the ACM SIGPLAN 91 Conference \non Programming Language Design and Implementation, 26(6): 15 29, June 1991. [7] Frangois Irgoin. Code \ngeneration for the hyperplane method and for loop interchange. Technical report, Ecole Nationale Superieure \ndes Mines de Paris, Oc\u00adtober 1988. Report ENSMP-CAI-88-E102/CAIfl. [8] Fran;ois Irigoin and Remi Triolet. \nSupernode parti\u00adtioning. Conference Record of Fifteenth ACM Sympo\u00adsium on Principles of Programming Languages, \npages 319-329,1988. [9] L. Lamport. The parallel execution of do loops. Commwticalions of lhe ACM, pages \n83 93, February 1974. [10]Dror E. Maydan, John L. Hennessy, and Monica S. Lam. Efficient and exact data \ndependence analysis. Proceedings of the ACMSIGPLAN 91 Conference on Programming Language Design and Implementation, \n26(6): 1 14, June 1991. [11] Constantine D. Polychronopoulos and David J. Kuck. Guided self-scheduling: \nA practical scheduling scheme for parallel supercomputers. IEEE Transac\u00adtions on Computers, C-36(12): \n1425 1439, December 1987. [12] William Pugh. The omega test a fast and practical integer programming \nalgorithm for dependence analy\u00adsis. Proceedings of Supercomputing 91, pages 4 13, November 1991. [13] \nDeborah Whitfield and Mary Lou Soffa. Automatic generation of global optimizers. ACMSIGPLAN 91 Conference \non Programming Language Design and Implementation, pages 120 129, June 1991. [14] Michael E. Wolf and \nMonica S. Lam. A loop trans\u00adformation theory and an algorithm to maximize paral\u00adlelism. IEEE Transactions \non Parallel and Distributed Systems, 2(4):45247 1, October 1991. [15: Michael J. Wolfe. Optimizing Supercompilers \nfor ,Yupercomputers. Pitman, London and The MIT Press, Cambridge, Massachusetts, 1989. In the se\u00adries, \nResearch Monographs in Parallel and Distributed Computing This monograph is a revised version of the \nauthor s Ph.D. dissertation published as Technical Report UIUCDCS-R-82-1105, U. Illinois at Urbana-Champaign, \n1982. A Matrix multiply example In this section, we usc a matrix multiply loop nest as an example to \nillustrate the effect of a non-trivial iteration\u00adreordering transformation defined as a sequence of five \nkernel transformations. Figure 6 shows the input loop nest. Figure 7 shows how the dependence vectors \nand loop bounds expressions are updated by the sequence of template instantiations. doi=l, n doj=l, n \ndok=l, n A(i, j) = A(i, j) + B(i, k) * C(k, j) enddo enddo enddo Figure6: Matrix Multiply examplc input \nloopnest. Transform Dep. Vectors Index LB UB STEP loop START (=,=,+) i 1 n 1 do j1n 1do .$ 1 n 1 do ReversePermute \n(=,+,=) .i 1 n1 do n=3 k1 n 1 do rev=[F F F] i 1 n 1 do r perm=[3 12] Block (=,=,=,=,+,=) 1 n bj do \nn=3 (=,+,=,=,*,=) : 1 n hk do i=l 1 n do ii bi j=3 .i min(n,jj+bj 1) 1 do bsize=[bj bk bi] k : min(n,kk+bk \n-1) 1 do i ii min(n. ii+ fii -1) 1 do Parallelize (=,=,=,=,+,=) 1 n bj pardo n=6 (=,+,=,=,*,=) ;<; 1 \nn I)k bi 0 parflag=[l O 100 O] ii 1 pardo .j j.j min(?~.jjn+bj -1) 1 do lk I kk I ntin(n,kk+bk-1) 1 \ndo I i 1 ii 1 min(n. ii+ bi-l) 1 I do I ReversePermute (=,=,=,=,+,=) jj 1 ,, bj pardo n=6 (=,=,+,=,*,=) \n , n bi pardo rev=[FFFFFF] ;;: 1 n bk do perm=[l32 4 56] j rnin(n,j. j+bj 1) 1 do k : nlin(n,i.~ +bk-1) \n1 do . i min(n, ii+bi-1) 1 do ; Coalesce jic \\ -;y j * In-;; ~ ] 1 pardo (=!=l =!+!=) n=6 (=,+,=,*,=) \nkk 1 n bk do i=l j tmpi min (n, tmp. + bj 1 1 do j=z k kk min(n. kk+bk --l) 1 do i tmp, min(n. tmpi+bi \n-1) 1 do jic 1 tmpj=l+ [jic/l(n l+bi)/biJl [(n l+bj)/bjJ* ( ([(n-l+ bj)/fij] *l(n-l+bi)/biJ )) 1 *bj \njic 1 tmp, = 1+ jic [(n-l+ bi)/biJ * ( ( 1( -l +bibijij )) 1 *bi Figure 7: Matrix Multiply exampl~shows \nthe change in the 100p nest for each transformation in the sequence.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>This paper describes a general framework for representing iteration-reordering transformations. These transformations can be both matrix-based and non-matrix-based. Transformations are defined by rules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. The framework is extensible, and can be used to represent any iteration-reordering transformation. Mapping rules for several common transformations are included in the paper.</p>", "authors": [{"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "", "person_id": "PP14206121", "email_address": "", "orcid_id": ""}, {"name": "Radhika Thekkath", "author_profile_id": "81100269053", "affiliation": "", "person_id": "P237400", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143132", "year": "1992", "article_id": "143132", "conference": "PLDI", "title": "A general framework for iteration-reordering loop transformations", "url": "http://dl.acm.org/citation.cfm?id=143132"}