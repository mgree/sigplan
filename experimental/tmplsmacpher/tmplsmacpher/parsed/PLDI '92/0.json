{"article_publication_date": "07-01-1992", "fulltext": "\n A New Approach to Debugging Optimized Gary Brooks, Gilbert J. Hansen, Steve Simmons Convex Computer \nCorporation P.C).Box 833851 Richardson, TX 75083-3851 gbrooks@convex.tom, hansen@convex.tom, ssimmons@convex.com \nABSTRACT Debugging optimized code is a desirable capability not provided by most current debuggers, Usem \nare forced to debug the unoptimized code when a bug occurs in the optimized version. Current research \noffers partial solutions for a small class of optimization, but not a unified approach that handles a \nwide range of optimization, such as the sophisticated optimization performed by supercompluter compilers. \nThe trend with current research is to make the effects of optimization transparent, i.e., provide the \nsame behavior as that of the unoptimized program. We contend that this approach is neither totally feasible \nnor entirely desirable. Instead, we propose a new approach based on the premise that one should be able \nto debug the optimized code. This implies mapping the current state of execution back to the original \nsource, tracking the location of variables, and mapping compiler-synthesized variables back to user\u00addefined \ninduction variables. To aid the user in understanding program behavior, various visual means are provided, \ne.g., diffenmt forms of highlighting and annotating of the source/assembly code. While this unavoidably \nrequires the user to have a basic understanding of the optimization performed, it permits the user to \nsee what is actually happening, infer the optimization performed, and detect bugs. An example illustrates \nthe effectiveness of visual feedback. To support conventional debugger functionality for optimized code, \nthe compiler must generate additional information. Current compiler-debugger interfaces (CDIS) were neither \ndesigned to handle this new information nor are they extensible in a straight forward manner. Therefore, \na new CDI was designed that supports providing vi{sual feedback and the debugging of optimized code. \nThis paper Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed lfor direct commercial advantage, the ACM copyright notice and the title \nof tha publication and its data appaar, and notice is given that copying ia by permission of tha Association \nfor Computing Machinery. To copy otherwise, or to republish, requires a fee and/or spacific permission. \nACM SIGPLAN 92 PLD1-6/92/CA G 1992 ACM 0.8979~.476.7/92/0006/OOOJ ...$1 .5(3 specifies the details of \na new CDI and relates each feature back to the debugger functionality it supports. 1.INTRODUCTION The \ntraditional approach to developing production code is to compile and debug the program without optimization, \nand then compile it with optimization to get performance enhancements. There are cases when the ability \nto debug optimized code is a necessary and desirable capability. For example, Some logic errors only \nmaterialize in the presence of optimization. This includes programs with timing or instability problems, \nor programs with explicit compiler directives that force code to be wrongly vectorized or parallelized, \n A program may need to be optimized in order to be mnnable or fully testable because of time or size \nconstraints. For example, optimization may considerably reduce the execution time to reach a point of \nfailure. By observing the behavior of the optimized code, a user can identify what logic may be causing \nperformance problems. Reduction code is usually fully optimized. Should a customer submit a bug report \ninvolving a core file, the core dump can be analyzed. b To find a bug in optimized code requires recompiling \nwithout optimization. This extra compilation step can be eliminated. Compiler bugs can involve optimization \nbeing performed incorrectly. By observing program behavior, a compiler developer can identify what optirnizations \ncaused the bug. WMout debugger support, debugging an optimized program can be very tedious and time \nconsuming, if not impractical. The user must ke able to correlate the optimized code with the original \nsource code. This is undesirable because it requires expert optimization knowledge and can be a challenging \nprocess even for a compiler developer. The compiler optirnizations reorder, replicate, factor, fuse, \ndelete, and transform source constructs, thereby destroying the direct mapping between object and source \ncode. Furthermore, the use of variables may be eliminated (e.g., constant propagation), synthesized (e.g., \ninduction variables), or assigned to registers. Values at a stop point maybe different from that in the \nunoptimized program. Providing support for the debugging of optimized code is a difficult problem. Some \nof the complications that arise are The behavior of the optimized code must be conveyed in terms of \nthe original source code. Conveying execution state in terms of source code unparsed from the optimized \ncode is generally not possible for the optimized code is generally not expressible using the constructs \nof the source language.  The mapping of object code to source code is no longer register and memory. \nThe use of a variable may even be eliminated.  The usage of an induction variable maybe replaced by \ncompiler-synthesized variables.  The compiler-debugger interface must incorporate information about \nthe transformations performed by the compiler.  Collecting the necessary debugger information requires \nextensive-modifications to the;-ompiler(s) to track-each optimization performed  l-to-l, Code fragments \nwill be replicated, fused, eliminated, or reordered. The location of a variable may alternate betwcxm \na This paper presents the design of a new CDI that permits a debugger to handle the mapping from optimized \nobject code to source code, to make certain optimization transparen~ to provide visual feedback of the \neffect of control flow optimization on program behavior, and to display the correct value of a variable. \nOur approach is a combination of, and departure from, current research efforts. Section 2 briefly surveys \nother approaches taken to debugging optimized code. Section 3 describes the new approach we have taken. \nSection 4 discusses the design of the new CDI and Section 5 presents a debugging example. Section 6 discusses \ncompilation times and space requirements using the new CDI compared to an existing CDI. Section 7 describes \nfuture enhancements. 2. PREVIOUSWORK 2.1 Current Work on Debugging Optimized Code The current approach \nto debugging optimized code is exemplified by the work of Hennessy [Henn82, WaSr851, Zellweger [Zel183, \nZel184], Coutant et al [CoMe88], and Zurawski and Johnson[ZuJo90]. Each effort is concerned with making \nthe effects of the optimization transparent to the programmer and providing expected behavior. Hennessy \ns research deals with the problem of providing the values of noncurrent variables that are consistent \nwith the order of evaluation and assignment in the original unoptimized program. Closely related is the \nwork of Coutant et al. They too can provide the value of noncurrent variables, but also handle global \noptimization and track a variable s value from memory through registers. Zellweger s work addresses the \nproblems of making the effect of inlining and cross-jumping optimization transparent to the programmer. \nZurawski and Johnson take an approach different from Zellweger s. Their debugger is based on a context-oriented \nmodel. Providing expected behavior transparently may involve switching between the optimized and unoptimized \nprogram and placing restrictions on some optimization. The capability to recover the value of noncurrent \nvariables is also provided. Each of the prototypes [2%1183,CoMe88] utilized multiple windows, in particular, \na command and source window. However, none took advantage of available graphical capabilities to provide \nvisual feedback on the effects of optimization. 2.2 Current Status of Debugger Technology Most current \ndebuggers, such as the UNIX debuggers (@b, dbx, and CONVEX csd), VAX DEBUG [Bean83], and the CONVEX Ada \ndebugger, a.db, usually use a line-oriented compiler-debugger interface. The debugging information usually \nconveys the name, type, and location for variables and the starting address of a statement or basic block, \nand the associated source code line. Statement-oriented debugging information has inherent limitations \nthat prohibit it from being extended to support the debugging of optimized code. For example, The granularity \nis too coarse. Many optimization involve expressions rhat cannot be conveyed at the statement level. \n There is only a 1-to-1 mapping between source lines and object code addresses which is insufficient \nin the presence of optimization. Optimization require both mapping multiple source codes to a singular \nobject code (e.g., common subexpression elimination) and mapping a singular source code to multiple object \ncodes (e.g., inlining and dynamic selection).  The implementation is a non-variant record/structure \nwhich is too restrictive for optimized debugging information. For example, UNIX stabs (symbol table structures) \nhave a fixed number of fields of fixed size  Despite the narrow scope of current research on debugging \noptimized code and the problems incurred, much useful 7 support for debugging optimized code can be provided, \nThis paper discusses an alternate approach based on a new compiler-debugger interface (CDI). Instead \nof trying to provide expected behavior, highlighting and animation (via repetitive stepping) is used \nto convey the effects of optimization on the behavior of a program. 3. AN ALTERNATEAPPROACH TO DEBUGGING \nOPTIMIZEDCODE The premise of our approach to debugging optimized code is that it is better to depict \nwhat is actually happening than to always make the effects of optimization transparent. We contend that \nmaking the effects of optimizations transparent is both undesirable and intractable. It is undesirable \nbecause transparent debugging maintains a facade of what is actually happening, thereby inhibiting the \nuser from debugging the actual optimized code. It is intractable because it is not possible to supply \nthe information necessary to invert the effects of all the optimizatkms applied to each code section. \nOptimization are cascaded and reapplied an arbitrary number of times in various orders. It is unrealistic \nto propagate and maintain information needed to unravel the effects of e,ach optimization, especially \nwhen many of the optimization am very complex in nature. One major goal was to convey the effects of \noptimization on a program s behavior. This was achieved by providing effective visual feedback in the \nform of highlighted program text using a finer source granularity than the statement level. This process \nof debugger interaction with the user is referred to as visual debugging. Text sections may be highlighted \nby boxing, using reverse video, grey-scale shading, underlining, multiple fonts and icons. Highlighting \ncan be used foc Curmtly executing code Whenever a program is stopped, the text sections of the program \nwhich correspond to the current point in the executable are highlighted. Previously executed code Whhin \na basic blc)ck, previously executed expnxsions can be highlighted to distinguish them from other expressions \nyet to be executed. Eventpoints Source lines on which an eventpoint has been set can be annotated \nappropriately. Compiler optimizations For example, loop invariant code executed prior to the loop can \nbe highlighted prior to loop entry, but the loop construct is not highlighted, indicating the invariants \nwere hoisted out of the loqp. Animating program behavior As a user steps through source, the expressions \ncurrently being executed are highlighted. Repetitive stepping will give rise to animation as successive \nsections of the program are highlighted while execution proceeds through the program . Based on this \nvisual feedback, a user can make intelligent queries about program state and deduce expected behavior \n(e.g., deduce the value of a variable), and gain insight into a program s actual behavior. Also, visual \nfeedback of abnormal behavior can indicate to a compiler developer there is an optimizer bug. Another \nmajor goal was d always provide the correct value of a variable. Two situations were addressed, FirsL \nthe location of a variable can be allocated to a register and/or memory over different regions of code \n[CoMe 88]. This requires keeping track of the liveness of each variable over various instruction ranges. \nSecond, usages of induction variables am replaced by compiler-synthesized variables. This requires maintaining \nthe linear function relating the two. Additional design goals were n Support standard debugger functionality \n Provide visual debugging for optimization performed by the CONVEX compilers  Make the compiler collection \nof debugging information non-intrusive, i.e., compiling for debugging would not afkt either the optimizations \nperformed or the code generated  Design the debugging information to be extensible, i.e., accommodate \nnew optimization and revisions to existing optimization  Support codes written in different languages \nor in multiple languages  Load debugging information on demand because of the large amount of debugging \ninformation  4. THECOMPILER-DEBUGGERINTERFACE(CDI) The CDI consists of three component s Compiler extensions \nto generate requisite debugging information Data files containing debugging information  Access methods \nwithin the debugger to load debugging information  The last item will not be addressed in this paper. \nHowever, it should be mentioned that the debugging information and its external representation were designed \nso the data files could be loaded incrementally and the internal data structures easily built on the \nfly. Currently the CDI supports all levels of optimization performed by the CONVEX FORTRAN and C compilers \n[CONV90, CONV91], namely, local and global scrdar optimization, vectorization, and parallelization. Table \n1 lists a representative set of optimization for each level. I Level I Optimization I scalar Redundant-assignment \nelimination, assignment substitution, common subexpression elimination, redundant-use elimination, constant \npropagation and folding, algebraic and trigonometric simplification, dead-code elimination, hoisting \nand sinking scalar and array references, copy propagation, code motion, strength reduction, global register \nallocation, instruction scheduling Vector IStrip mining, loop distribution, loop interchange, conditional \ninduction variables, loop unrolling, if-do interchange Parallel parallel strip mining, independent tasks \n(by directive), dynamic selection I Table 1: Optimization by Level 4.1 Debugging Information declaration \nof top-level symbols in the program, thereby allowing .tsi files to be selectively loaded upon demand. \nTable 2 lists the components of the debugging information. The Type/Scope Environment Table contiins \nthe The suffixed data files (.ns, .VL etc.) are kept in a hidden information needed to resolve the definition \nof a program directory. For data files generated by the front-end, there is identifier at debug time. \nThis information consists of one for each primary source jile, i.e., file submitted to the attributes \ndetermined by a compiler front end, such as compiler for compilation. For data files generated by the \nlexical scope, type definition, and kind. The kind is used in back-end, there is one for each object \nfile generated by the name resolution (e.g., when identical C identifiers are used compiler, Multiple \nobject files can be generated by the both as a variable and typedef within the same scope). This CONVEX \nAda and Application Compiler. table contains many of the same identifier attributes The Name Space Table \nfor a single source file identities the typically found in a debugger symbol table. However, it does \nnot include the allocated location of a variable identifier, since a variable can exist in many different \nlocations or be replaced by another variable due to compiler optimization. To determine the runtime value \nof a variable requires using this table in conjunction with the Variable Table, Location Range Table, \nand the Expression Table. Component Purpose Data File Compiler Generation Source File Map Relates an \nobject file to source file(s) I executable back-end Section Table Delineates the bounds of object module \ncomponents in the executable s address space executable back-end Name Space Lists external identifiers \nin the program I.ns front-end Type/Scope Environment Defines attributes of program data types disambiguates \nidentifiers and other lexical program symbols .tsi front-end Source Unit Table Contains the abstracted \ntree of source code elements that reflects the syntax of a program .Sut front-end Source Range Table \nCorrelates instructions source units with in the executable ranges of back-end Variable Table Lists the \nattributes of program variables I .Vt back-end Location Rauge Table Correlates the location of a variable \nranges of instructions in the executable with .lrt back-end Expression Table Maps compiler-synthesized \nuser-defined variables variables back to .Xpt back-end Table 2: Debugging Information The Source Unit \nTable provides a tree of source units that reflects the syntax of the program, and is used in highlighting \nprogram text. An entry for a source unit in the table consists ofi Index A unique integer that identifies \nthe source unit. Kind The granularity of the source unit (see Table 3 and Figure 1 below which illustrates \neach). Text section Start and end character positions of the source unit. Scope The lexical scope in \nwhich the source unit occurs. Parent Parent s source unit index. Granularity Purpose Expression Any \nvalid combination of constants, operators, and operands valid in the current source language 7 Statement \nIAny valid statement in the current source language I Statements that constitute the body of a An iterative \nconstruct (e.g., FOR, DO) ~~ Routine A main routine, subroutine, or function -l Table 3: Source Unit \nGranularities Multiple source unit granularities have the additicml advantage of permitting a more robust \nstepping mechanism (e.g., commands such as jinish loop, next expression) land allowing the user to think \nin terms of language consmlcts instead of source line numbers in a tile. The Source Range Table is used \nto map between executable source units and corresponding instructions in the executable image. An entry \nin the Source Range Table consists of a source unit index, and a start and end instruction address in \nthe executable image. There may be multiple address ranges for a source unit due to optimization (e.g., \ninstruction scheduling which interleaves code for an expression). Given an instruction address, the table \nis uIsed to determine which source units are active. This mapping is used, for example, to determine \nwhich source units to highlight at debugger eventpoints. Given a source unit, the table is used to determine \nthe source unit s address ranges. This mapping is used for stepping and the setting/unsetting of breakpoints, \neventpoints and tracepoints. The Variable Table enumemtes all user-defined and compiler-synthesized variables \nin a program, along with their attributes, An entry in the Variable Table contains the following componen~ \n Home location The storage class and offset for a variable whose lifetime extends across the entire \nprogram. Possible home locations in global memory are absolute, base-offset (for FORTRAN common block \nvariables), and object module relative (for static variables in C).  Scope entry A pointer to the scope \nentry for the variable in the name space (i.e., .tsi data file) which contains the variable s name. \n Additional flags, e.g., a row-major array, a Cray pointer, etc.  The Location Range Table is used \nto track the ephemeral variable-to-location bindings. An entry in the Location Range Table consists of \na variable index in the Variable Table, a start and end instruction address in the executable image over \nwhich the value of the variable resides in a particular machine location, and the machine location. Possible \nmachine locations are registers or stack frame relative (for local and argument variables). There may \nbe multiple overlapping entries for a variable due to optimization. Given a program address and variable, \nthe table is used to determine which location, if any, is associated with a variable. This mapping is \nused to find the value of a variable. If no ephemeral location is bound to the variable, the home location \napplies. If there is no home location, the variable is said to be unavailable; this can occur, for example, \ndue to constant propagation and redundant assignment elimination. Given a program address and a data \nlocation, the table is used to determine which variables reside in the location. This mapping is used \nwhen examining the registers or stack frames to determine the associated variable(s). The Expression \nTable tracks compiler-synthesized variables back to the user-defined variables in the source code. Within \nthe optimized code, the uses of many variables are replaced with more efficient uses of other variables. \nA common example occurs with loop strength reduction, where the loop induction variable is transformed \nftom an array index to a pointer for the current element in the array. For examplti INTEGER*4 A ( 100) \nDOI=1, 100 A(I) =O ENDDO Here, a loop induction variable, ? f oo, is synthesized that is a pointer into \nthe array A. ?fOo = 1oc(A) + 4*(1-1) All uses of A(I) within the loop are replaced with references to \n? f oo. The Expression Table maintains the linear function that expresses the direct relationship between \na synthesized variable and the respective user\u00addefined variables, and the reason for the synthesization. \n 4.2 Generation of Debugging Information CDI debugging information is gathered and generated in a series \nof stages that spans all phases of the compiler. The compiler phases are a front-end (syntax and semantic \nanalysis), a middle phase (machine independent optimization), and a back-end (code generation, machine\u00addependent \noptimization, assembly). There is a separate front-end for FORTRAN and C; the other phases are common. \nDebugging information is communicated from one phase to another by annotating compiler nodes and entries \nin the compiler s symbol table. Table 2 indicates the phase in which each component of the debugging \ninformation is generated. The .ns, .tsi, and .sut data files are emitted at the end of the front-end \nphase. The lexical analyzer and parser determine the starting and ending source positions of selected \nsyntactic units and annotate the parse tree with this information. During semantic analysis, the source \npositions are used to construct the source units which are then as.scxiated with compiler nodes. Also \nconstructed is the type and scope information. During the middle phase and back-end, source units are \ntracked through each optimization. Expressions are determined at the point where the synthesized variable \nis introduced during the compilation. The symbol table entry for that variable is annotated with the \nexpression. At the end of code generation, but before instruction assembly, all variable attributes have \nbeen resolved and the instruction order completely fixed. The .vt and .xpt files are formed by walking \nthe symbol table. Compiler nodes are annotated with labels representing program points of interest to \nthe debugger and with assembler directives for propagating .srt, .v~ and .Irt debugging information during \nassembly. After the instructions are assembled and address labels resolved, the .srt, .v~ and .lrt data \nfiles are generated along with the object module. 5. EXAMPLE The FORTRAN example in Figure 1 demonstrates \nhow highlighting can convey the execution behavior of optimized code. In particular, it demonstrates \nthat repetitive stepping can convey a precise understanding of how the code was optimized. The pictures \nwere extracted from an X SourceWindow Fi leView SourceUnit PrwessW indows file: f+f process: [#WOl Plive \nd 5 6 7 3 9@ 10 11 I subroutine add(a, integer a(rt, m}, ~ =~ doi= lZ III r do~+ =l, m b, c, n, m) b(n, \nm), c(n, m) Expression Source Units  Next Source Unit to be Executed :: 16 17 @ 18 19 20 t Breakpoint \n-Iarker / Routine Source Unit Statement Source Unit Loop Source Units \\ Block Source Unit Figure 1: Source \nWindow with Source Unit Annotations _add : sub . W #16,a0 Entry Point _add~+(Ox4) : ld.w @12(ap),:36 \n_add_+(Ox8) : ld.w @12(ap),s0 Use of parameter N _add_+(Oxc) : ld.w @16(ap),sl Use of parameter M _add_+ \n(Oxl O) : le.w #o, so Test N, if less than O, add 1 _add_+(Ox14) : brs. t _add_+(Oxla) _add_+(Ox16) : \nadd. w #l,so _add_+(Oxla) : le.w #o,sl Test M, if less than O, add 1 _add_+(Oxle) : brs.t _add_+(Ox24 \n) _add_+(Ox20) : add.w #l,sl _add_+(Ox24) : Cvtw .1 so, so Convert N and M to longwords _add_+(Ox26) \n: Cvtw .1 Slrsl _add_+(Ox28) : shf #-lr so Divide both N and Mby 2 _add_+(Ox2c) : shf #-l, S1 _add_+(Ox30) \n: St.w sOr@12(ap) Store N _add_+(Ox34) : lt.w #orso _add_+(Ox38) : St.w sl,@16(ap) Store M _add_+(Ox3c) \n: brs. f _add_+(O~ce8 ) Branch if N < 0 to routine exit _add_+(Ox3e) : ld.w @12(ap),a5 Use of N in loop \n_add_+(Ox42) : ld.w @16(ap),s4 Use of M in loop _add_+(Ox46) : mov a5ral Set up induction variables for \nloop _add_+(Ox48) : shf #2ral _add_+(Ox4a) : ld.w #l,s5 Load addresses of A and C _add_+(Ox4e) : ld.w \n0(ap)ra2 _add_+(Ox52) : ld.w 8(ap),a3 _add_+(Ox56) : shf . W #2,s6 _add_+(Ox5a) : add. w al,a2 _add_+(Ox5c) \n: mov a5,s2 _add_+(Ox5e) : ld.w 4(ap),a5 Load address of B _add_+(Ox62) : add.w al,a3 _add_+(Ox64) : \nSt.w a2,-12(fp) _add_+(Ox68) : add. w al, a5 _add_+(Ox6a) : St.w a3,-4(fp) Store loop induction values \n_add_+(Ox6e) : St.w s6,-16(fp) _add_+(Ox72) : sub . W S4, S5 _add_+(Ox74) : St.w a5,-8(fp) _add_+(Ox78) \n: necj. w S4, S4 _add_+(Ox7a) : le.w #o,s4 Top test of outer loop _add_+(Ox7e) : brs.t _add_+(Oxha) \nFigure 2: Assemb1y Code forFORTRAN Example window that displays the source code [BuCh91]. Sliglhtly restricted \nfunctionality is also available on character\u00adorientedterminals (CRTs). l% ecurrentpointof execution iscommunicated \ntotheuser by highlighting, in reverse video, the innermost source unit(s) to be executed next. The FORTRAN \ncode was compiled at optimization level -02 (which contains all local and global scalar optimization \nand vectorization, but excludes parallelization, loop unrolling, and dynamic selection). Examples of \neach kind of source unit are illustrated. To aid in following the example at the source level, Figure \n2 gives the corresponding machine instructions, annotated with appropriate comments, Ithat lead up to \nthe physical loop. Examining the assembly code is unnecessary in practice. However, when there is a source \nand assembly window, corresponding code is highlighted in both. For the assembly window, only the first \ninstruction of the source unit will be highlighted. The example illustrates the following optimization \ninstruction scheduling, strength reduction, vectorization, code motion, redundant use elimination, and \nassignment substitution. Also illustrated are synthesized variables and mapping of their values back \nto the respective user-defined induction variables. Step granularity is an expression source unit. Upon \nentry to the routine, prolog code is executed that sets up the runtime environment and then execution \nstops on the tirst Soumeunit (_add_+ (OX8 ) ): n=~2 Stepping once stops on the second source unit to \nbe eXeCURd (_add_+ ( Oxc ) ). This highlighting sequence is caused by instruction scheduling. To utilize \nthe instruction pipeline most efficiently, uses of variables in memory are usually executed first within \na basic block. The next step by expression stops at _add_+ ( Oxl O) and resdts in n=m m = mL2 Both n/2 \nand n are highlighted because of assignment substitution which eliminated the assignment to n. Stepping \nby expression once more stops at _add_+ ( Oxla) and RXdts in m=~2 The use of m within the inner loop \nis not highlighted because assignment substitution was not performed on the assignment to m. Several \nmore steps results in the assignment to the parameters n and m, respectively. m = m\\2 t-l = rl\\2 The \nnext step stops at _add_+ ( 0x3e ) and results in All uses of n are highlighted because of code motion \nand redundant use elimination. Changing the step granularity from expression to block and stepping once \nresults in entering the body of the outermost loop, i.e, the i loop. Requesting the values for both i \nand j at this point results in the response: I = (INTEGER*4) 1 J = Value i.s not available. The reason \nfor this is that the uses of i and j have been replaced, using loop strength reduction, with new compiler \nsynthesized induction variables whose uses are more efficient. The values of i and j me only determinable \nwhen the synthesized variables are alive. In this case, j doesn t have a value because the j loop hasn \nt been entered yet, i.e., the synthesized variables for j are alive only within the Imdy of the loop, \n(Liveness is tracked by the global register allocator.) By stepping one more block, only line 13 is highlighted \nand j now has its initial value of 1: I = ( INTEGER*4 ) 1 J = ( INTEGER*4 ) 1 Stepping by loop followed \nby a step by block retxdts ix I = (INTEGER*4) 129 J = ( INTEGER* 4 ) 1 The compiler optimization summary \nwould show that the i loop was vectorized and the i and j loop interchanged. The i loop is divided into \ntwo nested loops, i.e., an inner loop vectorized with a size of 128 and an outer stripmine loop that \niterates n/1211 times. The vectorized loop of i has been interchanged with the j loop. Thus, i is 129 \nsince the stride of the strip-mined outer loop is 128. In order to determine the values of both i and \nj, a set of linear equations is solved, For example, i has the following set of linear equations associated \nwith it (obtained by invoking the command info express ion i): usecl to create 11 synthesized variable(s); \n1+ {OSTR} ?iO E (i+(N/2)}+(1-1) 2. {TRIP) ?il = (-l*(N/2))+(1-U 3, {TRIP) ?i2 = (N/2)+{(1-lM-1) 4, {ISTR) \n?i8 : ?iO+((I-1)%128) 5, {INIIV} ?ia = ((N*2)+l~(C)}+{(?i0-(l+{N/211)$41 6+{INN) ?ib = (loc(B)+(N*2)}+((?i0-(l+(N/2)} \n)$4) 7+{INDV} ?ic : ((N$2)+lw(fi)l+((?i0-{l+(N/2))}$4} 8, {INIIV) ?id = ?i2 9, {INMO ?i12 = ?ia+((4#0$?i6) \n 10, {INIIV) ?i13 = ?ib+((4*N)#?iEJ 11, (INIIV) ?i14 = ?ic+((4*NM?i6) The abbreviations in angular brackets \nto the left of the equations identify the reason for the synthesization. (OSTR stands for outer strip, \nTRIP for trip coun~ ISTR for inner strip, and INDV for induction variable.) The variables prefixed with \na question mark are the compiler synthesized variables. For example, ?i14, ?i13, and ?i12 are pointers \ninto the arrays a, b, and c, respectively, Using these variables as pointers caused the address calculations \nwithin the body of the two loops to be eliminated. This example makes the following points: Traceability \nto the current logical source unit, and thus, point of execution is possible. This can be done even when \nthe machine code for the source units have been merged together or interweaved. It does not imply that \nany source code before or after a source unit has been executed. 0 Successive stepping by expression \nprovides a sequence of highlighting patterns from which the user can infer Optimization Highlighting \nBehavior Example = Redundant The eliminated code is never highlighted. The optimization cm be verified \nusing N= K+ I... Assignment the info line wmman&#38; there will be no address ranges for the eliminated \ncode. N==K*J Elimination DO IIE1, N Assignment Both the expression being assigned and the subsequent \nuses of the assigned variable N-X+1... Substitution are highlighted. DO 1=1, N Common Subex-l he common \nsubexpressions are highlighted. A-X+1... pression Elimination B= C*(X+l) Redundant Use Multiple references \nto a variable between assignments within a basic block are A-X+X/3 Elimination highlighted. Algebraic/Trig \nThe code for the expression simplified is not highlighted. The optimization can be l. A=X*l Simplification \nverified using the info 1 ine command, there will be no address ranges for the 2.~s6x*~ simplified source \nunit. Dead Code The eliminated code is never highlighted. The optimization can be verified using the \nElimination info line cmnrnand, there will be no address ranges for the dead code. Code Motiox The reference \nto a variable or a syntactic element within the loop is higMghted DO I-1,1O Hoisting and before the body \nof the loop is entered and highlighted. AII] s2*~ Sinking References ENDDO Copy Propagation References \nto equivalent variables are highlighted. A = 7.23 * 1.14 B= A:C=B Instruction Expressions in a basic \nblock are highlighted in execution order which is d~erent 1. A= X*3-Y ~*3. y Scheduling than indicated \nby the source code. 2.A = x*3. y3.A = 4. A= X*3-Y Smmgth Reduction Noticeable when viewing both a source \nwindow and assembly window; both the J=I*2 current expression and current instruction are highlighted. \nshf #1,1 Global Register Not noticeable unless examinin g Iiveness ranges via the info variable Allocation \ncommand. Strip Mining The loop induction variable is incremented by the strip mine length, whose default \nis 128. Loop Distribution The body of the loop will be highlighted depending on how its parts were distributed \nand how the d~tributed loops were optimized. Loop Interchange The inner loop is highlighted before the \nouter loop. Conditional The value of a conditional induction variable is incremented by a conditional \nInduction Variables amount, and when displayed its value is derived from the compiler synthesized variable. \n Loop Unrolliig For complete unrolling, the loop induction variable is completely removed. For partial \nunrolling, it is incremented by multiples of 4. If-Do Interchange The expression within the IF statement \nis highlighted before entering the loop body. DO 1=1, N The behavior of the loop dtifers based upon the \nresult of the IF expression. IF X. LE. Y... ENDDO Parallel Strip A new source window is instantiated \nin the second iteration of the loop. Dynamic Selection Different higtilghting behavior occurs, depending \non whether the scalar, vectotied, or pamllelized version of the loop is selected. Selection is based \non the value of runtirne variables. .-.--. ... . ..m Table 4: current HIgnllgnmItg Patterns 9 possible \noptimization. However, the deductions are dependent upon the user s expertise in understanding the effects \nof each optimization. Possible effects may be the nonsequential execution of code or the unexpected change \nin a variable s value. * Full source level debugging capabilities can be achieved when examining user \nvariables in optimized loops. That is, the value of a user defined loop induction variable can still \nbe calculated even if all references to that variable have been optimized away. Table 4 describes the \ncurrent highlighting pattern for each optimization mentioned in Table 1. When appropriate, art example \nis given, For illustration purposes, the portion of the example that would be highlighted is expressed \nin boldface font. If it takes several steps to see the pattern, the steps are numbered. 6. PERFORMANCEMEASUREMENTS \nA number of representative FORTRAN and C programs were compiled to compare differences between the use \nof UNIX stabs (symbol table structures) and CDI data files. The two performance measurements of interest \nare the increase in compilation time to genemte symbolic debugger information and the size of the information. \nThe codes used were two typical C system programs and two typical FORTRAN application programs. The programs \nranged in size from 10K to 150K lines. The measurements were made on an unloaded C240 system. To compare \ncompilation times and debugging information sizes, all programs were compiled multiple times at various \noptimization levels. First, each program was compiled three times at optimization level -no (minimal \noptimization), generating stabs, CDI &#38;ta files, and neither, respectively. Then they were compiled \ntwice at optimization level -01 (scalar optimization) and twice at optimization level -03 (all optimization \nat the lower levels plus vectorization and parallelization of loops), generating CDI data files or no \ninformation, respectively, (NOTE stabs are not generated at high optimization levels because they would \nbe distorted due to the optimization and cause problems while using a conventional debugger,) The average \nincrease in compilation time to generate debugger information over not generating the information was: \nat -no using stabs 4% at -no using CDI 33% at -01 using CDI 36% at -03 using CDI 29% 1. optimization \nalways performed are instruction scheduling, span-dependent instructions, register allocation, and tree-height \nreduction. See [CONV90] or [CONV91 ] for a detailed descrip\u00adtion. Thus, on the average, compilation time \nincreased 33% when generating the CDI information instead of stabs. The average increase in the size \nof the debugger information over not generating the information was: at-no using stabs 2.19 at -no using \nCDI 3.66 at -01 using CDI 3.91 at -03 using CDI 3.96 Thus, on the average, the size of the debugger information \nincreased approximately by a factor of 1.65 when generating the CDI instead of stabs. Even though compilation \ntakes longer and debugger information is larger when using the CDI, it is worthwhile because the CDI \npermits additional debugging functionrdity unattainable by a conventional debugger using stabs. In particular, \nthe CDI permits optimized code to be debugged, and one has finer control of the debugging process, for \nexample, one can step by one of five syntactic units instead of just by statement. 7. FUTUREENHANCEMENTS \nThe new approach to debugging optimized code is implemented in the new CONVEX visual debugger, CXdb VI. \n1, and supported by the FORTRAN and C compilers. Plans are to support the CONVEX Application Compiler, \nAda compiler, and C++ compiler. All of these Compiler products require CDI support for inlined routines. \n(C++ Mines routines as tex~ Ada as parse trees, and the Application Compiler as intermediate language \ndags.) The use of highlighting as a means of visually conveying the effects of optimization has just \nbegun to be explored. Much work is needed to complete and enhance this approach, for example, the use \nof hierarchical highlighting to convey the effects of loop optimization, and stippling to explicitly \nidentify dead code. Other possibilities to be explored are the use of multiple fonts (e.g., italic to \nidentify loop invariant code, common subexpressions, or variables being watched), the use of different \nhighlighting styles (e.g., different shades of gray or color to convey code fragments already executed \nversus those yet to be, and boxing with dotted, dashed, or various thickness lines). Currently, the user \nis required to infer the optimization performed. We plan to investigate better means of conveying this \ninformation, for example, by annotating source/assembly code with icons to indicate the optimization \nperformed (e.g., a tilted e to indicate vectorization or II for parallelization). Clicking on the icon \nwould display details of the optimization, such as that provided by the compiler s optimization summary \nreport. CDI debugging information represents an increased investment in compilation time and space requirements. \nMeans to reduce compilation time and space requirements using additional compression techniques will \nlx investigated. 8. CONCLUSIONS Although the CDI debugging information represents an increased investment \nin compilation time and space requirements over the use of stabs, we think the increases are reasonable \nand acceptable considering the beneficial gains in debugger functionality. We have also demonstrated \nthe approach is effective and viable by illustrating four problems it solves Current execution state \nis mapped back to the original source even though there are l-to-many and many-to-1 mappings that arise \nfrom optimization, in particular, optimization involving movement of code.  The liveness of variables \nare tracked so correct vahes are displayed regardless of whether the value is in memory and/or a register, \n The values of compiler-synthesized variables are mapped back to the value of the user-defined induction \nvariable.  The behavior of an optimized program is conveyed via visual feedback Even though limited \nhighlighting capabilities are currently provided, the techniqpe proved effective.  ACKNOWLEDGEMENTS \nWe would like to acknowledge our colleagues who participated in the definition and development of CXdb, \nin particular, Larry Streepy, Russ Buyse, Mark Chiarelli, David Lingle, Jeff Woods, Mike Garzione, Ken \nHarward, Ray Cetrone, Lloyd Tharel, Tim Powell, Rich Bleikamp, and Bill Torkelson. REFERENCES [Bean 83] \nBeander, B., VAX DEBUG An interactive, Symbolic, Multilingual Debugger , ACM Proceedings of the Software \nEngineering Symposium on High-Level Debugging, SIGPLAN Notices 8,4 (August 1983) 173-1 79. [BuCh 91] \nBuyse, R. and Chiarelli, M., A User Interface Strategy for CXdb , Xhibition 91 Conference Proceedings \n(June 1991) 9-15. [CoMe 88] Coutant, D.S., Meloy, S. and Ruscet@ M., DOC: A Practical Approach to Source-Level \nDebugging of Globally Optimized Code , ACM Proceedings on Program-ng Language Design and Implementation, \nSIGPLAN Notices 23,7 (July 1988) 125-134. [Conv 90] CONVEX FORTRAN Optimization Guide, 2nd edition, CONVEX \nComputer Corporation (1990). [Conv 91] CONVEX C Optimization Guide, 2nd edition, CONVEX Computer Corporation \n(1991). ~enn 82] Hennessy, J., Symbolic Debugging of Optimized Code , ACM Transactions on Programming \nLanguages and Systems, 3,3 (July 1982) 323-344. waSr 85] Wall, D., Srivastav&#38; A. and Templin, F., \nA Note on Hennessy s Symbolic Debugging of Optimized Cod&#38; , ACM Transactions on Programming Languages \nand Systems, 7,1 (January 1985) 176-181. ~11 83] Zellweger, P.T., An Interactive High-Level Debugger \nfor Control-Flow Optimized Programs , ACM Proceedings of the Sofware Engineering Symposium on High-Lwe/ \nDebugging, SIGPLAN Notices, 18,8 (August 1983) 159-171. ~11 84] Zellweger, P.T., Intemctive Source-Level \n Debugging of Optimized Programs , PhD Thesis, University of California, Berkeley, CA (1983); also Report \nCSL-84-5, Xerox PARC, Palo AltQ, CA (May 1984). lZuJo 90] Zurawski, L.W. and Johnson, R.E., Debugging \nOptimized Code with Expected Behavior , University of Illinois at Urbana-Champaign (unpublished paper) \n(August 1990).  \n\t\t\t", "proc_id": "143095", "abstract": "<p>Debugging optimized code is a desirable capability not provided by most current debuggers. Users are forced to debug the unoptimized code when a bug occurs in the optimized version. Current research offers partial solutions for a small class of optimizations, but not a unified approach that handles a wide range of optimizations, such as the sophisticated optimizations performed by supercomputer compilers.</p><p>The trend with current research is to make the effects of optimization transparent, i.e., provide the same behavior as that of the unoptimized program. We contend that this approach is neither totally feasible nor entirely desirable. Instead, we propose a new approach based on the premise that one should be able to debug the optimized code. This implies mapping the current  state of execution back to the original source, tracking the location of variables, and mapping compiler-synthesized variables back to user-defined induction variables. To aid the user in understanding program behavior, various visual means are provided, e.g., different forms of highlighting and annotating of the source/assembly code. While this unavoidably requires the user to have a basic understanding of the optimizations performed, it permits the user to see what is actually happening, infer the optimizations performed, and detect bugs. An example illustrates the effectiveness of visual feedback.</p><p>To support conventional debugger functionality for optimized code, the compiler must generate additional information. Current compiler-debugger interfaces (CDIs) were neither  designed to handle this new information nor are they extensible in a straight forward manner. Therefore, a new CDI was designed that supports providing visual feedback and the debugging of optimized code. This paper specifies the details of a new CDI and relates each feature back to the debugger functionality it supports.</p>", "authors": [{"name": "Gary Brooks", "author_profile_id": "81100076758", "affiliation": "", "person_id": "PP31071067", "email_address": "", "orcid_id": ""}, {"name": "Gilbert J. Hansen", "author_profile_id": "81100463738", "affiliation": "", "person_id": "PP31078808", "email_address": "", "orcid_id": ""}, {"name": "Steve Simmons", "author_profile_id": "81100616991", "affiliation": "", "person_id": "PP31096894", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143108", "year": "1992", "article_id": "143108", "conference": "PLDI", "title": "A new approach to debugging optimized code", "url": "http://dl.acm.org/citation.cfm?id=143108"}