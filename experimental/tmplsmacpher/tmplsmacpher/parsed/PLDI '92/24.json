{"article_publication_date": "07-01-1992", "fulltext": "\n Register Allocation for Software Pipelined Loops B. R. Rau, M. Lee, P. P. Tirumalai, M. S. Schlansker \nHewlett Packard Laboratories 1501 Page Mill Road Palo Alto, CA 94303 Abstract Software pipelining is \nan important instruction scheduling technique for efficiently overlapping successive iterations of looPs \nand executing them in parallel. This parser studies the task of register allocation for-software pipeji;ed \nloops, both with and without hardware features that are specifically aimed at supporting software pipelines. \nRegister allocation for software pipelines presents certain novel problems leading to unconventional \nsolutions, especially in the presence of hardware support. II-&#38; paper formulates these novel problems \nand presents a number of alternative solution strategies. These alternatives are comprehensively tested \nagainst over one thousand loops to determine the best register allocation strategy, both with and without \nthe hardware support for software pipelining. Keywords: register allocation, modulo scheduling, software \npipelining, instruction scheduling, code generation, instruction-level parallelism, multiple operation \nissue, VLIW processors, Very Long Instruction Word processors, superscalar processors, pipelining 1 Introduction \n1.1 Software pipelining Software pipelining [5] is a loop scheduling technique which yields highly optimized \nloop schedules. Algorithms for achievingsoftwarepipeliningfall into twobroad classes: modulo scheduling \nas formulated by Rau and Glaeser [15] and, . algorithms in which the loop is continuously unrolled and \nscheduled until a situation is reached which allows the schedule to wrap back on itself without draining \nthe pipelines [13]. Although, to the best of our knowledge, there are no published measurementson this \nissue, it is our belief that the second class of software pipelining algorithms can cause unacceptably \nlarge code size, Consequently, our interest is in modulo scheduling. In general, this is an NP-complete \nproblem and subsequent work has focused on various heuristic strategies for performing modulo scheduling \n([9, 11, 12, 10] and the as yet unpublished heuristics in the Cydra 5 compiler [6]). Modulo scheduling \nof loops with early exits is described by Tirumalai, et al. [19]. Modulo scheduling is applicable to \nRISC, CISC, superscalar, superpipelined, and VLIW processors, and is useful whenever a processor implementation \nhas parallelism either by having Permission to copy without fee all or part of this material is granted \nprovided that the copies are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying ie by permission \nof the Aeeociation for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. ACM SIGPLAN 92 PLD1-6/92/CA 01992 ACM 0.89791 .476.7 /92/0006 /0283 . ..$1 .50 \npipelined operations or by allowing multiple operations to be issued per cycle. This paper describes \nmethods for register allocation of modulo scheduled loops that were developed at Cydrome over the period \n1984-1988, but which have not as yet been published. These techniques are applicable to VLIW processors \nsuch as the Cydra 5 [18]. The processor model supports the initiation of multiple operations in a single \ncycle where each operation may have latency greater than one cycle. The use of hardware features, that \nspecifically support the efficient execution of modulo scheduled loops, is assumed. These features include \nrotating register files (register files supporting compiler\u00admanaged hardware renaming, which were termed \nthe MultiConncct in the Cydra 5), predicated execution, and the Iteration Control Register (ICR -a boolean \nregister file that holds predicates) [6, 18], This paper also considers register allocation of modulo \nscheduled loops on processors having no special support for modulo scheduling. Our discussion is limited \nto register allocation following modulo scheduling. Register allocation prior to modulo scheduling places \nunacceptable constraints on the schedule and, therefore, results in poor performance. Concurrent scheduling \nand register allocation is preferable, but achieving this, in the context of modulo scheduling, is not \nunderstood. This paper does not attempt to describe a complete scheduling\u00adallocation strategy. For instance, \nlittle will be said on the important issue of what to do if the number of registers required by the register \nallocation exceeds the number available. Instead, the focus is on studying the relative performance of \nvarious allocation algorithms with respect to the number of registers that they end up using and their \ncomputational complexity. In the rest of this section, we provide a brief overview of certain terms associated \nwith modulo scheduling, descriptions of predicated execution and rotating register files, and of modulo \nscheduled code structure in the presence of this hardware support. We also examine the nature of the \nlifetimes that the register allocator must deal with for modulo scheduled loops. Section 2 discusses \nthe various register allocation strategies and code schemas that can bc employed, anti the interdependencies \nbetween them. In the context of these alternatives, the register allocation problem is formulated in \nSection 3 and the candidate register allocation algorithms are laid out in Section 4. Section 5 describes \nthe experiments performed and examines the data gathered from those experiments, finally, Section states \nour conclusions. 1.2 A quick overview of modulo scheduling A rotating register file is addressed by \nadding the register specifier in the instruction to tie contents of the Iteration Control Pointer (ICP) \nmodulo the number of registers in the rotating register file. A special loop control operation, brtop, \ndecrements the ICP amongst other actions. As a result of the brtop operation, a register that was previously \nspecified as ri is specified as ri+l, and a different register now corresponds to the specifier ri. Lifetimes \nof a value generated by an operation in less than SC-1, For DO-loops, with hardware support in the one \niteration co-exist with the corresponding values generated in previous and subsequent iterations. Newly \ngenerated values are written to successive locations in the rotating register file and do not overwrite \npreviously generated values even though exactly the same code is being executed repeatedly. This also \nintroduces the need for the compiler to perform value tracking; the same value, each time it is used, \nmay have to be referred to by a different register specifier depending on the number of brtop operations \nthat lie between that use and the definition [6]. The Iteration Control Register file, ICR, is a rotating \nregister file that stores boolean values called predicates. Predicated execution allows an operation \nto be conditionally executed based on the value of the predicate associated with it. For example, the \noperation a = Op(b,C)if pi executes if the predicate pi is one, and k nullified if pi k zero. The primary \nmotivation for predicated execution k to achieve effective modulo scheduling of loops containing conditional \nbranches [6, 18]. If-conversion [1] may use predicates to eliminate all branches from the loop body. \nThe resulting branch-free loop body can now be modulo scheduled. In the absence of predicated execution, \nother techniques must be used which require either multiple versions of code corresponding to the various \ncombinations of branch conditions [7, 13] or restrictions on the extent of overlap between successive \niterations [11]. A secondary benefit of predicated execution, the one relevant to this study, is in controlling \nthe filling and draining of the software pipeline in a highly compact form of modulo scheduled code known \nas kernel-only code. The number of cycles between the initiation of successive iterations in a modulo \nschedule is termed the initiation interval (II). The schedule for an iteration can be divided into stages \nconsisting of H cycles each. The number of stages in one iteration is termed the stage count (SC). If \nthe schedule length is SL cycles, the number of stages is given by Figure 1 shows the record of execution \nof five iterations of the modtdo scheduled loop with a stage count of 4. The execution of the loop can \nbe divided into three phases: ramp up, steady state, and ramp down. In Figure 1, the first 3*II cycles, \nwhen not all stages of the software pipeline execute, constitute the ramp up phase. The steady state \nportion begins when the fourth and last stage of the first iteration coincides with the first stage of \nthe fourth iteration. During the steady state phase, one iteration completes for every one that starts, \nThe steady state phase ends when the first stage of the last iteration has completed at time 5*11. Thereafter, \nin the ramp down phase, one iteration completes every II cycles until the execution of the loop completes \nat time 8*11. In generating code for a modulo schedule, one can take advantage of the fact that exactly \nthe same pattern of operations is executed in each stage of the steady state portion of the modulo schedule \ns execution. This behavior is achieved by looping on a piece of code corresponding to one stage of the \nsteady state portion of the record of execution. This code is termed the kernel. The record of execution \nleading up to the steady state is implemented with a piece of code called the prologue. The epilogue, \nimplements the record of execution following the steady state. In Figure 1, the number of stages in \nthe prologue and the epilogue are each equal to SC-1. In general, depending on whether or not hardware \nsupport is provided, the register allocation strategy employed and the nature of the loop, the number \nof stages in the prologue and epilogue can be more or form of the rotating register file and predicated \nexecution, it is not necessary to have explicit code for the prologue and the epilogue. Instead a single \ncopy of the kernel is sufficient to execute the entire modulo scheduled loop. This is called kernel-on \nly (KO) code. Consider the KO code depicted in Figure 2a. All the operations from the same stage of the \nsame iteration are logically grouped by attaching them to the same predicate. The KO code is repeatedly \nexecuted every II cycles. The predicates take on values as shown in Figure 2b. Operations in a stage \nSi are executed when the corresponding predicate Pi is one. Five iterations each consisting of four stages \nare swept out diagonally in Figure 2b. P. is the predicate pointed to by the ICP. This predicate is set \nto 1 by the brtop operation during the ramp up and steady state phases, and is set to O during the ramp \ndown phase. Because brtop decrements the ICP, a different physical register P. is written into every \nII cycles. A detailed description of the operation of kernel-only code is provided in [6, 17]. b 1 o \n1 so ier2 probgueC&#38; S1 so ia 3 n S2 .$1 se itr4 2*U 3*U km.] S3 S2 S1 so ia5 4*I1 W&#38; S3 S2 S1 \nso _ Wtt 33 S2 S1 6,1[ S3 S2 epiogue code 7*U S3 Figure 1. Software pipelined loop execution  L.3 \nLifetimes of values within loops Each value produced in the loop has one producer and one or more consumers. \nThe lifetime of a value starts when the producer is issued and ends when all of the consumers have finished. \nThis definition of lifetime is required if the VLIW code is to be interruptible and re-startable with \na hardware model in which operations, that have been issued, always go to completion before the interrupt \nis handled. Otherwise, the lifetime could start when the producer finishes and end when the last consumer \nbegins. Lifetimes correspond either to loop-invariant variables or to loop-variant variables. Loop-invariants \nare repeatedly used but never modified during loop execution. Loop-invariants, which are referenced in \nthe loop, are assumed to have already been allocated in the (non-rotating) GPR file using conventional \nregister allocation techniques. This is not the topic of this paper, A new value is generated in each \niteration for a loop-variant and, consequently, there is a different lifetime corresponding to each iteration. \nLoop-variants can be further categorized based on whether or not the value defined in one iteration is \nused in a subsequent one, and on whether or not a value defined in one of the last few iterations is \nused after the loop, That a loop\u00advariant is used by a subsequent iteration is equivalent to saying that \neach iteration uses a value defined by a previous iteration. In the case of the first few iterations, \nthese previous iterations do not exist, and the expected vahres must be generated before the loop is \nentered and, therefore, are live-in to the loop. Likewise, if a vahre that is defined in one of the last \nfew iterations is used after the loop, it will be live-out from the loop. Kernel Only Code P3 P2 PI Po \no 0 0 1 so 0 0 1 1 S1 so 0 1 1 1 1 1 1 1 S3 S2 S3 S1 S1 so so 4 1 1 1 1 S3 S2 S1 so 1 1 1 0 S3 S2 S1 \n1 1 0 0 S3 S2 1 0 0 0 S3 Predicae Wuea as controlledbybrtop Stages as enabkd by @kates Figure (b) 2. \nKernel-only code FORTRAN example Modulo scheduled code subroutine foo(a,a) time ndex operation real \na(lO), s o OPo vr34:-m2read(vr33[l] lit: O) do i = 1,35 13 OP1 vr35:-fadd(vr34vr35 [11) s=s+ a(i) 15 \nOP2 vr36:-fmpy(vr35vr35) a(i) =s+ s* a(i) 18 0P3 vr37:-fmpy(vr36vr34) enddo 20 OP4 $vr38:-mlwrite(vr37vr33[1] \nlit: O) St Op 0 OP5 vr33:-aladd(vr33[l]vr32) end I I II 0 91.. OP6 --\u00ad $vr39:-brtop(slit: L.4-foo ) ** \n~~hedule for iteration completes ** Figure 3. Mor.lulo scheduled loop example Figure 3 shows a FORTRAN \nDO-loop example, and the Recurrences always result in live-in loop-variants, and it is correspondhtg \npseudo-code after scheduling. The latencies of common for the last definition of a loop-variant to be \nlive-out. the operations are given in Table 3. The schedule has an Furthermore, compiler optimizations \nthat eliminate loads initiation interval, II, of 2 cycles. The issue time for each snd/or stores can \nalso result in live-in end live-out values. When operation is indicated in the left hand column. A single \na load from (store to) a particular memory location in one iteration completes in 21 cycles. In the loop \nshown in Figure 3, iteration is followed by a load from that same memory location virtual register vr32, \nwhich is used in the address add in a subsequent iteration, the latter load is redunden~ the value (operation \n0P5), is a loop-invariant. It is allocated to the GPR may be used directly from the register to (from) \nwhich it was file end has the value 4 written into it prior to entering the loaded (stored). The first \nfew iterations require that the loop. The lifetime corresponding to a loop-invariant such asappropriate \nregisters be pre-loaded with the proper live-in vr32 extenda through all iterations of the loop. Virtual \nregister values prior to loop en~y. Likewise, when multiple stores from vr37 is a loop-variant. It represents \na new value of a(i) separate iterations can be proven to access the same memory computed in each iteration \nof the loop. This value is produced location, all the stores, except for the lest one, are redundant \nby operation 0P3 and has only one consumer, operation 0P4, and can be eliminated. This assumes that that \nlast store actually which belongs to the same iteration. Virtual register vr35 is also occurs. In the \ncase of the last few iterations, the supposedly a loop-variant. It is produced by operation OP1 and represents \nredundant store, that was supposed to be covered by a store in a a new vahte ofs computed in each iteration. \nThis value has two subsequent iteration, is, in fact, not redundant since the consumers, operations 0P2 \nand OP1. Operation 0P2 uses the iteration containing the covering store is not executed. The last vahte \nproduced in the same iteration whereas operation OP1 value that should have been stored must be stored \nafter exiting uses the value produced one iteration earlier, indicated by the the loop and, so, must \nbe live-out. Such load-store [l] after the vr35 operand. For loop variants like vr35 andoptimization, \nof scalar as well as subscripted references, were vr37, there is a different lifetime corresponding to \neach performed by the Cydra 5 compiler [6, 14] and have also been iteration. studied by Callahan, et \nal. [3]. In the loop example in Figure 3, load/store optimization has been done with respect to s (vr35). \nIn the absence of this optimization, each loop iteration would have contained a load and a store ofs. \nThe optimizer recognizes that only the load in the first iteration and the store in the last iteration \nare required, and that intermediate values can be held in registers. The initial load is moved out of \nthe loop and is executed prior to loop entry establishing the value of s (vr35) for the first iteration. \nThe initial value is, therefore, live-in to the loop. Subsequent iterations use the values generated \none iteration earlier (vr35[l]). Correspondingly, the store in the last iteration is moved out of the \nloop and is executed after loop exit. The value of s calculated in the last iteration is required by \nthis store and, therefore, must be live-out. After optimization and scheduling, the set of lifetimes \ncorresponding to a single loop-variant is presented to the register allocator as a 4-tuple. Each 4-tuple \nis of the form (start, end, omega, alpha). The parameters start and end specify, respectively, the issue \ntime of the producer of a v ahse and the latest completion time amongst all the operations that consume \nthat value. Both start and end are expressed relative to the initiation time of the iteration containing \nthe producer. Omega is one less than the highest numbered iteration that uses the value detlned by the \nfkst iteration, i.e., it is the number of iterations spanned by that value. It is also the number of \nlive-in values for the loop-variant. Alpha specifies the number (counting back) of iterations from the \nend of the loop in which the earliest live-out value was defined. It is also the number of live-out values \nfor the loop variant. For instance, if alpha = 1, the only live-out value was defined in the last iteration; \nif alpha = 2, the values of the loop variant defined in the last two iterations are both live-out. Even \nif we want only the vahre from the second last iteration (and not that from the last iteration), we define \nalpha to be 2. The example in Figure 3 has five loop-variants which are shown in Figure 4, The first \nand the fifth have an omega = 1 indicating that there is one live-in v ahte for each. The live-in value \nfor loop-variant 1 is the initial value of s, and that for loop-variant 5 is the initial address of a(i), \ni.e., the address of a(1). These two values must be pre-loaded into the appropriate registers prior to \nloop entry. Loop-variant 1 also has an alpha = 1 indicating that the value computed in the last iteration \nis live-out . This is the final value of s which must be stored after loop exit. Number of loop invariant \n= 1 11=2 five loop variants: 4. 1 13 16 1 1 <---vr35 (s) 2 18 20 0 0 <---vr37 3 15 19 0 0 <---vr36 \n40 19 0 0 <---vr34 (a(i)) 50 22 1 0 <---vr33 (addrof a(i)) Figure 4. Loop-variant lifetimes for the \nprogram in Figure 3. Figure 5 shows a space-time diagram generated by the register allocator for the \n35 iterations of the example in Figure 3. The horizontal axis represents time in clock cycles advancing \nfrom the far left (cycle O) to the far right (cycle 89). The vertical axis denotes physical register \nspace. The number of rows in the chart indicates the number of registers used in the achieved allocation. \nOnce a value is generated into a physical register ri, it occupies that register until the lifetime ends. \nThe same loop\u00advariant produced in the next iteration is generated into physicaf register ri-l due to \nthe decrementing of the ICP (even though the register specifier used is the same). If ri is O, ri-t will \nbe the highest numbered register. As iterations are initiated every If cycles, the start times of these \ntwo lifetimes are II cycles apart. This gives rise to the wand portion (the dlagonaf band)of the set \nof lifetimes for a loop-variant as each iteration yields a lifetime which is lower by one register and \nto the right by II cycles from the lifetime in the previous iteration. The lifetime generated by the \nfirst iteration is shown in boldface. Live-in values, which are pre-loaded prior to loop entry, must \nremain in the registers until their lifetimes end. This adds a leading blade to the wand of recurring \nvalues which extends, on the left, all the way through cycle O. For example, the leading blade forloop-variant \n1 occupies register 18 from cycle O to cycle 14. Note that the blade lifetime need not always be longer \nthan tAe wand lifetime. For instance, the leadlng blade for loop-variant 5, which occupies register 13, \nis shorter than the lifetime for the first iteration, which is in register 12. This is because for this \nloop-variant start is less than II. As discussed, the lifetimes of live-out values, which are used after \nloop exit, must be extended until the loop completes. This adds a trailing blade to the wand of recurring \nvalues. For example, the trailing blade for loop-variant 1 occupies register 11 from cycles 85 through \n89. The thicknesses of the leading and trailing blades depend on the values of omega and alpha, respectively. \nFor example, if loop-variant 1 had omega = 2, the leading blade would include register 19 from cycles \nO through 12 as indicated by the asterisks in Figure 5. When the possibility for confusion is present, \nwe shall refer to a single lifetime (single live range) as a scalar lifetime and to the set of lifetimes \ncorresponding to a loop-vtwiant, across the entire execution of the loop, as a vector lifetime, The register \nallocation problem involves packing these vector lifetimes (wands with or without leading or trailing \nblades) as tightly as possible in the space-time plane. 2 Code Generation Strategies For our purposes \nhere, three activities are involved in generating code for modulo scheduled loops: modulo scheduling, \nregister allocation and determining the correct code schema to use. Register allocation is the topic \nof this paper. However, the manner in which this is done depends both on the presence or absence of rotating \nregister files as well as the code schema used. Any code schema can be assembled from the following components \n[17], for each of which the corresponding notation is also listed: . prologue code (P), . kernel code \n(K), . unrolled kernel code (Kn), . epilogue code (E), _ multiple-epilogue code (En), _ a sequential, \npreconditioning loop (S). For instance, a specific code schema consisting of a prologue, an unrolled \nkernel, and multiple epilogues would be denoted by (PKnEn) whereas one with a preconditioning loop, a \nprologue, unrolled kernel, and epilogue would be denoted by (S PKnE). In the special case when the code \nschema consists of only the kernel or unrolled kernel code, we add the suffix O (KO, K O). We have two \nschemes for handling the overlapping lifetimes for a loop-variant that are encountered in modulo scheduled \ncode: . static renaming via modulo variable expansion, (MVE), and _ dynamic remapping via the use of \nrotating register files (DR), Lastly, we shall consider two styles of register allocation: _ blades allocation, \n(BA), and . wands-only allocation (WO). 0000000000111111llll2222222222333333333344444444445555555555666666666677777777778888888888 \n01234567890123456789012345678901234567890123456789012345678901234567890123456789Ol23456789 I -----------------------------------------------------.-----------------------------------\u00ad11 \n222 55555555555555555555555llll3333344444444444444444444222 0144444444444444444444222 55555555555555555555555llll3333344444444444444444444222 \n271 44444444444444444444222 55555555555555555555555llll3333344444444444444444444222 26I 44444444444444444444222 \n55555555555555555555555llll3333344444444444444444444222 25I 44444444444444444444222 55555555555555555555555llll3333344444444444444444444222 \n24I 44444444444444444444222 55555555555555555555555llll3333344444444444444444444222 23I 44444444444444444444222 \n55555555555555555555555llll3333344444444444444444444222 22I 44444444444444444444222 55555555555555555555555llll3333344444444444444444444 \n21I 44444444444444444444222 55555555555555555555555111133333 20 I 44444444444444444444222 55555555555555555555555111133333 \n 191************* 44444444444444444444222 55555555555555555555555111133333 181111111111111111333334444444444444444444422255555555555555555555555111133333 \n17 I 11113333344444444444444444444222 55555555555555555555555111133333 161 11113333344444444444444444444222 \n55555555555555555555555111133333 151 11113333344444444444444444444222 55555555555555555555555111133333 \n14 I 11113333344444444444444444444222 55555555555555555555555111133333 131555555555555555555555llll3333344444444444444444444222 \n55555555555555555555555111133333 l2l55555555555555555555555llll3333344444444444444444444222 55555555555555555555555111133333 \n111 55555555555555555555555llll3333344444444444444444444222 55555555555555555555555111111111 10 I 55555555555555555555555llll3333344444444444444444444222 \n55555555555555555555555 91 55555555555555555555555llll3333344444444444444444444222 55555555555555555555555 \n81 55555555555555555555555llll3333344444444444444444444222 55555555555555555555555 71 55555555555555555555555llll3333344444444444444444444222 \n55555555555555555555555 61 55555555555555555555555llll3333344444444444444444444222 5555555555555555555555 \n51 55555555555555555555555llll3333344444444444444444444222 41 55555555555555555555555llll3333344444444444444444444222 \n31 55555555555555555555555llll3333344444444444444444444222 21 55555555555555555555555llll3333344444444444444444444222 \nI-----------------------------------------------------------------------------------------\u00ad00000000001111111lll2222222222333333333344444444445555555555666666666677777777778888888888 \n01234567890123456789012345678901234567890123456789012345678901234567890123456789Ol23456789 time ---> \n Figure 5. Space-time register allocation chart All of these are elaborated upon below. The code schema, \nthe renaming scheme end the style of register allocation cannot be selected independently. For instance, \nwands-only allocation precludes kernel-only code and vice versa, whereas blades allocation is compatible \nwith both the kernel-on]y schema aswell as the PKE schema. Modulo variable expansion always requires \nkernel unrolling. Furthermore, the choice of codeschema is influenced and limited by the nature of the \nloop (DO-loop or WHILE-loop) and whether predicated execution or rotating register files or both are \npresent [17]. Table 1. Selected combinations of code schema / renaming scheme / register allocation style. \nHardwareFeatures I Registerallocationstratep,yForDO-loops None I PKnEn / MVE / WO I SPKnEI MVE+WO Predicated \nexecutinn PKEnI DR I WO and rotating register fdes PKEIDRIBA KO/DR/BA A register allocation strategy \nis a specificcombinationof the register allocation style, renaming scheme and code schema. Inthh paper, \nwe shall only discuss in detail DO-loops with either no hardware support or with both predicated execution \nand rotating register files. Table 1 lists the legal register allocation strategies under these circumstances. \nWe shall examine three of these irt detail: _ KO/DR/BA, . PKEn/DR/WO, and . PKnEn/MVE/WO or SPKnE/MVE/WO. \nWhen there is no risk of confusion, we shall abbreviate and refer to the first two strategies as the \nBA and WO strategies, respectively, and to both variations of the third one, collectively, tts the MVE \nstrategy, 2.1 Code generation with hardware support With hardware support in the form of rotating register \nfiles and predicated execution, it is not necessary to have explicit code for the prologue and the epilogue. \nInstead a single copy of the kernel is sufficient to execute the entire modulo scheduled loop. This is \ncalled kernel-only (KO) code (Figure 2). Since the ramp up, steady state and ramp down phases of the \nloop execution are all effected with the same code, the kernel code, the register allocator for the kernel \nmust take into account all the scalar lifetimes corresponding to a loop-variant, over the complete execution \nof the loop, l,e., the vector lifetimes, including the blades. It does so by spacing the vector lifetimes \nadequately far apart in the register file to ensure that no part of one vector lifelime overlaps arty \npart of artother (Figure 5). We shall refer to this style of register allocation as blades allocation \n(BA). KO code is attractive in that it is very compact, but there is the possibility that the requisite \nBA style of allocation requires more registers due to the constraints imposed by the leading and trailing \nblades. Since, in general, each vector lifetime has a different shape to its blade, it might be expected \nthat the quality of the packing of the vector lifetimes in the space-time plane will be relatively poor. \nA second code generation strategy avoids this problem by peeling off an appropriate number of iterations \nof the kernel at the beginning and end of the loop to form a prologue and epilogue, respectively. The \nprologue is designed to be long enough that all the non-wand portions of the leading blades are in the \nprologue, and the epilogue is made long enough that all the non-wand portions of trailing blades are \nin the epilogue. The amount of prologtre peeling to ensure this is   ~ps= n+ fw+ 1} over all loop-variants \nwith omega > 0. Similarly, the amount of epilogu: peeling ?eeded .to ensure that trailing blades need \nnot be considered during register allocation is over all loop-variants with alpha > 0. (Since the leng[h \nof Lhe prologues and epilogues are determined solely by the live-in and live-out values, and not by the \nrequirement of achieving a steady-state schedule, this may not correspond to the SC-1 stages shown in \nFigure 1). The corresponding style of register allocation is termed the wands-only (WO) style since blades \ncan be ignored during register allocation for the kernel, The register allocator need only ensure that \nthe wands do not overlap. Register allocation for the prologue and epilogue is done separately, honoring \nthe constraints imposed by the allocation for the kernel. By construction, the prologue and epilogue \nare long enough to ensure that any conflict with a blade can only occur in the prologue or epilogue where \nthe lifetimes can be re-allocated independently of the way in which the lifetimes for the same loop-variant \nwere allocated in the kernel. However, now that the register assignments in the prologue and epilogue \nare different, multiple epilogues are needed to handle loop exits from the prologue. One epilogue can \nno longer serve in all cases since the register assignments are different. Note, also, that the amount \nof prologue and epilogue peeling given above is sufficient but not necessary to ensure that a register \nallocator which disregards leading and trailing blades will produce a correct allocation; an inspection \nof the register assignments after register allocation may reveal that less unpeeling is adequate based \non the specific way in which the vector lifetimes have been packed. In this study, however, we treat \nthe above formulae as if they are both necessary and sufficient. 2.2 Code generation without hardware \nsupport When no hardware support is available, modulo scheduling can be accomplished by modtrlo variable \nexpansion (MVE), i.e., unrolling the kernel and renaming the multiple copies that now exist of each virtual \nregister definition [11]. The unrolling and renaming is required to prevent successive lifetimes, corresponding \nto the same loop-variant, from overwriting one another in the same register. Register allocation consists \nof packing the scalar lifetimes together compactly --a more traditional register allocation task, except \nthat we would expect to see a much larger fraction of lifetimes live across the back\u00adedge than we would \nwith a sequential loop. The minimum degree of unroll, K.~l~. is determined by the longest lifetime among \nthe Ioop-varmnts. Because iterations are initiated every H cycles, Kmti can be calculated as: Kmin = \nMAX endil~tati) . i  ([ 1) By unrolling the kernel more than Kmin times, there is the possibility that \nfewer registers might be required in the modulo scheduled code, but at the cost of increasing the generated \ncode size and the compile time of the allocation process. With MVE code, prologue and epilogue peeling \nof SC-1 stages are required to ramp up and ramp down the software pipeline. In addition to this requirement, \nlive-in and live-out values also impose prologue and epilogue peeling constraints as explained for the \nWO strategy above. For the MVE renaming scheme, the number of stages of prologue and epilogue are given \nby PS = max{SC-1, LPS], and ES = max {SC-1, LES ], respectively. As a result the blades may be ignored \nwhile performing register allocation for the kernel. We shall refer to this rezister allocation strategy \nas the MVE strategy. (*1OOP precondition code*/ #bile ((N<(PS+K+ES-SC+l)) or ((N-PS-ES+SC-1) mod K # \nO)) N = N-1 sequentialloopbody (stagesA,B,C,D) ?nd while /*MVE software pipeline code*/ lC= N-1 if \nlC2 0 then do WE modulo scheduled code and do MVE sot twzep~dine N acrziens execured A SC =4sageitenuion \nII Sc Ps  B c A B A Ps =5Stageprologw K = 2shgekemel ES =3sIageepiOgw D c B A J D c B A Dc BA K D c \nB AIC#o I Dc B 1 Es Dc 1D Figure 6. Preconditioned MVE code With MVE, either additional epilogues or \nloop preconditioning are required in addition to the prologue, epilogue and unrolled kernel. The software \npipelined code can execute only certain integer numbers of iterations. Assume that PS stages are peeled \naway from the beginning of the loop, ES stages are peeled away from the end of the loop, the kernel is \nunrolled K times, and the schedule for an iteration has SC stages (Figure 6). The code, as described \nabove, can only execute PS+ES-SC+l +i*K iterations for i > 1 if the only way of exiting the loop is at \nthe end of the last stage of the unrolled kernel. One code generation schema adds a sequential, non-modulo \nscheduled version of the loop known as the preconditioning code. The preconditioning code ensures that \nthe pipelined part is entered with an appropriate trip count so that the loop will need to exit only \nafter the last stage of the unrolled kernel. The loop counter, LC, is initialized with this trip count \nprior to entering the modulo scheduled loop and must be decremented by K each time through the unrolled \nkernel (every K*II cycles). The iteration of the kernel terminates when LC is O. B DISTIA,B] AB A5 BO \n[1 II I I ()*11 1*1I 2*II Figure 7. Distance Preconditioning does not work for WHILE-loops and loops \nwith early exits where the trip count is not known prior to beginning the execution of the loop. Also, \nit decreases the performance of DO-loops because of the need to process residual iterations serially. \nThis effect is particularly noticeable with short trip counts. Preconditioning can be avoided by placing \na branch, which decrements and tests LC, in each stage of the prologue and kernel. The loop is, therefore, \nexited after the correct number of iterations. However, each of these exit points now requires distinct \nepilogue code to complete the iterations in progress at the time of exiting the software pipeline. This \nwill result in some amount of increase in code size and code generation complexity. At the same time, \nit yields the highest performance code with the MVE strategy. In this paper, we shall study the code \nsize of both code schemas for the MVE strategy. A more detailed discussion of code schema for modulo \nscheduled loops is provided in [17]. For the sake of brevity, we have limited our discussion to only \na quarter of all the possibilities. It should be noted that WHILE-loops can be handled in a very similar \nmanner as long as certain additional details are addressed. 3 Formulation of the register allocation \nproblem Informally, the register allocation task for modulo scheduled loops with hardware support consists \nof packing together the space-time shapes, that correspond to the vector lifetimes, so as to take up \na minimum number of registers. An allocation is legal if, out of all the scalar lifetimes that make up \nall of the vector lifetimes, no two scalar lifetimes that over] ap in time are allocated to the same \nregister. This bin packing trikes place on the surface of a cylinder with time corresponding to the axis \nand the registers corresponding to the circumference. So, tbe task is to pack the vector lifetimes together \nso thal they fit on the surface of a cylinder having the smallest possible circumference. In describing \nthe allocation algorithms, wc shall need to refer to the linear ordering of registers, and it is important \nto bear in mind that if, for instance, we have 64 registers, it is equally correct to refer to the register \nbelow O as either -1 or as 63. When discussing a vector lifetime, we shall use the lifetime produced \nby the first iteration of the loop as the point of reference. Thus, a vector lifetime has been allocated \nto physical register 7 if the defining operation in the first iteration writes its result to physical \nregister 7. Iterations 2, 3, etc., would write their results to physical registers 6, 5. and so on. Note \nthat, in this discussion, when we refer to registers, we refer to the the absolute addresses of registers \nwhich are different for each iteration. However, assuming the presence of the kind of register renaming \nthat is provided by the rotating register file, 1I I1 I I1  3*11 4*II 5*II 6*11 7*II 8*II 9*I1 matrix \ncalculation the register svecifier in the instruction would be invariant. Also, ~e start ~nd finish times \nof a vector lifetime refer to those of the scalar lifetime corresponding to the first iteration. As another \npoint of terminology, when we say that one vector lifetime is allocated above another one, we mean that \nthe wand of the first lifetime lies above (and to the right) of the wand for the second lifetime in the \nspace-time diagram (see Figure 5). Note that this does not mean that the first lifetime is allocated \nto a higher location than is the second. If the start time for the frst lifetime is sufficiently later \nthan the start time for the second, then it can be allocated to a lower location even though it is above \nthe second lifetime in the space-time diagram. For scalar lifetimes, above and higher location are synonymous. \n 3.1 The register allocation constraint Allocation constraints for modulo scheduled loops can be abstracted \nin order to simplify the formulation of the problem and the implementation of the allocation algorithms. \nGiven four parameters for each vector lifetime (start, end, omega, alpha), we construct the matrix, DIST, \nwhich has one row and one column for each vector lifetime. Each off-diagonal element specifies the minimum \ndistance (in registers) allowable between a pair of lifetimes when they are allocated in the rotating \nregister file. In Figure 7, vector lifetimes A and B for a schedule with II = 3 are shown in a space-time \ndiagram. (Two possible allocations for B are shown in Figure 7, labelled B( and B .) A loop Wilh five \niterations is shown. Lifetime A has: (start = 8, end = 13, omega = 1, alpha = 1). Lifetime B has: (start \n= O, end = 3, omega= O, alpha= O). The shapes of the lifetimes A and B are compl ctel y specified by \nthese four parameters. Whereas lifetime R is a wand, i.e., a simple diagonal shape, lifetime A is not, \nand must carry a value live-in and live-out due to the value of the parameters: omega = 1 and alpha = \n1. Assume that lifetime A has been allocated to some register in the rotating register file. Lifetime \nB can be allocated adjacent to A either below A (B() or above A (B ) in Figure 7. When lifetime Et[ is \nmoved as high as is possible without interfering with the scalar lifetime that constitutes the leading \nblade of A, it ends up being allocated to the same register as A. (Recall that a vector lifetime is said \nto be allocated to the register to which the scalar lifetime corresponding to the first iteration is \nallocated). Therefore, DISTIB,A] has the value O. Similarly, when lifetime B is moved to the lowest position \npossible, it ends up being allocated 5 registers higher than A. Therefore, DISTIA,B] has the value 5. \nAll off-diagonal elements of the DIST matrix are defined likewise. In this example, the distance bctwemr \nA and B( was limited by the interference between the leading blade of A and the wand of B(, and the distance \nbetween A and B was Iimitcd by the interference between the wand of A and the wand of BU , In general, \nthere are three with vector or scalar lifetimes and depending on the register different possible interferences, \nany one of which can limit the minimum distance between two blades: 1. the interference between the two \nwands, 2. the interference between the leading blade of the upper vector lifetime and either the leading \nblade or the wand of the lower lifetime and, 3. the interference between the trailing blade of the lower \nlifetime and either the wand or the trailing blade of the upper lifetime.  Accordingly, for vector lifetimes, \nDISTIA,B] is computed using the following formulae: end(A)-strrrt(B ) dl = 11 II dl, if omega(B ) = O \nd2 = {max(dt, omega(A)), otherwise d2, if alpha(A) = O d3 = [ max(d2, alpha(A)), otherwise For WO, \nDISTIA,B] = dl. For BA, DISTIA,B] = d3. The DIST matrix for scalar lifetimes. with the MVE stratezv. \nis . : much simpler. If two scalar lifetimes X and Y are hve simultaneously, then DISTIX,Y] = DISTIY,X] \n= 1. Otherwise, DISTIX,Y] = DISTIY,X] = O. The register allocation problem is to pack the lifetimes, \nwhether vector or scalar, on the surface of a cylinder with the minimum circumference while honoring \nthe minimum distance requirements, as specified by the DIST matrix, between every pair of lifetimes. \nThe appropriate definition of the DIST matrix is used depending on whether the register allocator is \nworking allocation algorithm being used.  3.2 A lower bound on the number of registers A tight lower \nbound on the number of registers needed for a modulo scheduled loop is useful in assessing the success \nof the register allocation. A simple, yet surprisingly tight, lower bound is obtained by calculating \nthe total number of scalar lifetimes that are live during each cycle and taking the maximum of these \ntotals. Clearly, at least this many registers are needed under all circumstances. This maximum is commtted \nover the entire length of the (possibly unrolled) kernel after scheduling. In view of the repetitive \npattern of lifetimes in an unrolled kernel. the number of scalar lifetimes that are simultaneously live \nneed only be computed for any II consecutive cycles of the kernel. 4 Register Allocation Algorithms The \nnature of the overall register allocation process is described in Figure 8. The set of lifetimes to be \nallocated are presented to the register allocator in some arbitrary order by the compiler. Onc can choose \neither to retain that order or to put it into a better order which improves the quality of the allocation \nachieved by a one-pass allocation algorithm. The choice of ordering heuristic is controlled by the variable \nOrdering Algorithm. Zero, one or multiple heuristics may be selected simultaneously. The allocation process \nconsists of repeatedly selecting an as yet unallocated lifetime, in the order imposed by the ordering \nheuristics, and allocating it. The choice of register location into which this lifetime is allocated \nis determined by the allocation algorithm. The variable that controls this is AllocationAlgorithm. In \nthis study, we only consider one-pass register allocation algorithms, i.e., algorithms in which each \nlifetime is allocated exactly once with no back\u00adtracking or iteration. procedure Allocate; begin case \nOrderingAlgorithm of O: {do nothing); 1: order by start time; 2: order by conflict;  3: order by \nconflict and start time with appropriate priorities; 4: order by adjacency;  5: order by adjacency \nand start time with appropriate priorities; 6: order by adjacency and conflict with appropriate priorities; \n7: order by adjacency, conflict and start time with appropriate priorities; end ( case ); Lifetimes[1].location \n:= O; LastLT := 1; update the set of disallowed allocations for every unallocated lifetime; for SelectedLT \n:= 2 to NumberOfLifetimes do begin case AllocationAlgorithm o.f  1: BestFit(SelectedLT,SelectedLocation); \n 2: FirSCFIC (SelectedLT, SelectedLocation); 3: EndFit(SelectedLTrLastLT, Selecteducation) ; end { case \n}; Lifetimes[SelectedLT]location := SelectedLocation; LastLT := SelectedLT; update the set of disallowed \nallocations for every unallocated lifetime; end ( for SelectedLT := 2 to NumberOfLifetimes do ); end; \n Figure 8. Overview of the register allocation algorithm procedure BestFit (SelectedLT,BestLoCation); \n( Find the best location, BestLocation, in which to allocate } { the selected lifetime, SelectedLT. } \n begin FirstLocation := minimum over all allocated lifetimes, i, of Lifetime[i]location -Dist[SelectedLT,i]; \nLastLoCation := maximum over all allocated lifetimes, i, of Lifetimes[i].location+ Dist[i,SelectedLT]; \nBestFitSoFar := infinity; fOr TrialLocation := FirstLocation to LastLocation do begin if SelectedLT \ncan be allocated to TrialLocation then begin ThisFit := number of registers needed if SelectedLT is allocated \nto TrialLocation; ThisFOP!:=FOM(SelectedLT,TrialLocation); {figure-of-meritfor { allocating SelectedLT \n{ to TrialLocation if (ThisFit <BestFitSoFar)or ((ThisFit =BestFitSoFar)and (ThisFOM>BestFOM))then \nbegin BestFitSoFar := ThisFit; BestFOM := ThisFOM; BestLocation := TrialLocation; end; Iif 1 end; {if \n} end; {for) end;  Figure 9. The best fit algorithm procedure FirstFit (SelectedLT, FirstLocation); \n{ Find the first (lowest) location, FirstLocation, in which } { the selected lifetime, SelectedLT, can \nbe allocated. } begin FirstLocation := O; while SelectedLT cannot be allocated at FirstLocation do \nFirstLocation := FirstLocation + 1; end;  Figure 10. The first fit algorithm procedure EndFit (Select \nedLT, Last LT, SelectedLocation); { Find the location, SelectedLocation, in which to allocate ) { the \nselected lifetime, SelectedLT. ) begin SelectedLocation := Lifetime[LastLT]location + Dist[LastLT,SelectedLT]; \nwhile SelectedLT cannot be allocated at SelectedLocation do SelectedLocation := SelectedLocation + 1; \nend;  Figure 11. The end fit algorithm allocated lifetimes. It is possible to allocate the current lifetime \n4.1 Register allocation algorithms in between a pair of previously allocated lifetimes. For each candidate \nlocation to which it is possible to allocate the The best fit algorithm (Figure 9) is an attempt to come \nup with lifetime, the minimum number of registers needed to permitthe best allocation possible without \nan exhaustive search, this allocation is computed. This is done by wrapping thebacktracking or iteration, \ni.e., once a lifetime is allocated to a infinite plane into a cylinder of minimum circumference, i.e., \nparticular register, it stays allocated to that same register. As minimum number of registers, while \nrespecting the minimumwith all the allocation algorithms considered here, the lifetimes distance requirements \nbetween every pair of lifetimes. The are allocated one by one in the order prescribed by the location \nthst is finally selected is the one that minimizes the conjunction of the selected sorting heuristics \n(see below). number of registers needed. Ties are broken using an additionalInitially, allocation is \nperformed on an infinite plane heuristic, FOM. Each time a lifetime is allocated, some number(unbounded \nregisters). With best fit, every location is examined of locations are made unavailable to the as yet \nunallocatedstarting with the highest, permissible location that is below lifetimes because of conflicts \nwith the lifetime just allocated. all the previously allocated lifetimes up through the 10west, permissible \nlocation that is above all of the previously procedure AdjacencyOrder; begin put all lifetimes in the \nset NotSetAside; initialize OutputList to empty;  remove lifetime 1 from NotSetAside and append it \nto OutputList; LastLT := 1; for i := 2 to NumberOfLifetimes do begin MinDistance := infinity; SelectedLT \n:= null; CurrentSet := empty; for j := 1 to NumberOfLifetimes do if j in NotSetAside then begin \nif WO or BA strategies then begin  ThisDistance := (start[j] -end[LastLT]) + DISTILastLT,j] * II; \nif ThisDistance < MinDistance then begin SelectedLT := j;  MinDistance := ThisDistance; end; end \n else { if MVE strategy then } begin conflict[j] := false;  ThisDistance := start[j] -end[LastLT]; \n if lifetime[j] conflicts with any lifetime in CurrentSet then begin conflict[j] := true;  ThisDistance \n:= ThisDistance + penalty; end;  if ThisDistance < MinDistance then begin SelectedLT := j;  MinDistance \n:= ThisDistance; end; end; end; { if j in NotSetAside then J  remove SelectedLT from NotSetAside \nand append it to OutputList; LastLT := SelectedLT; if MVE strategy then if conflict[SelectedLT] then \nCurrentSet := {SelectedLT} else CurrentSet := CurrentSet u {SelectedLT); end; ( for i := 2 to NumberOfLifetimes \ndo J end; Figure 12. The adjacency However, in general, some subset of these were already unavailable \ndue to the previously allocated lifetimes, The figure-of-merit, FOM, is inversely proportional to the \nnumber of additional locations that are made unavailable if SelectedLT is allocated to TrialLocation. \nThe first fit algoridun (Figure 10) proceeds in a similar fashion except that it always begins its search \nat location O and terminates as soon as a location is found to which the lifetime can be allocated. \nThe end fit algorithm (Figure 11) is intended to be used primarily with the adjacency ordering heuristic \n(see below), with each lifetime being allocated, above the last lifetime allocated, to the lowest possible \nlocation. Consequently, the search for a location starts with the lowest location such that the current \nlifetime is both above the previously allocated lifetime and does not cause a conflict with it. This \nlocation is specified by the DIST matrix. The search ends at the fust legal location that is encountered, \ni.e., one which has no conflict with any of the previously allocated lifetimes. ordering algorithm 4.2 \nLifetime ordering heuristics Three ordering heuristics are considered here: start-time ordering, adjacency \nordering and conflict ordering. Start-time ordering is motivated primarily by the scalar lifetime case. \nFor a single, non-loop basic block, every lifetime consists of a single in terv al. In this case, it \ncan be shown that allocating seal ar lifetimes in increasing order of start time, to the lowest available \nregister, yields an optimal allocation. (A sketch of the proof of this theorem is provided in [16].) \nThis procedure is not necessarily optimal for a loop, especially a modulo scheduled one, in which lifetimes \nextend across the back-edge of the loop yielding discontinuous lifetimes, i.e., live at the beginning \nand end of the loop body but not in between. It can, however, be shown that an upper bound on the number \nof registers needed is L+M, where L and M are the maximum and minimum number of lifetimes, respectively, \nthat are live simultaneously [8]. In any event, ordering by start time was viewed as a potcntiall y successful \nheuristic to use even in thk situation and, in fac~ was used as the secondary ordering heuristic for \nvector lifetime allocation in the Cydra 5 compiler. The primary ordering heuristic in the Cydra 5 compiler \nwas what we shall term adjacency ordering, This greedy heuristic was used in conjunction with the end \nfit allocation algorihn. At each step, that lifetime is selected which minimizes the horizontal distance, \nin the space-time diagram (Figure 5), between the lifetime that was allocated last and the one just \nselected. Bearing in mind that the horizontal dimension is time, we see that this heuristic attempts \nto minimize the amount of time that any given register is idle. The adjacency ordering algorithm (Figure \n12) is quite straightforward for vector lifetimes. Lifetime 1 is selected first. Thereafter, the lifetime, \nB, that is seIected at each step is the one for which (start[B] -end[A]) + DISTIA,B]*II is minimum, where \nA is the lifetime that was previously selected. Note that (start[B] -end[A]) is the horizontal distance \nin the space-time diagram if A and B were allocated to the same register. DISTIA,B]*II is the additional \ngap given that B must be allocated to a register that is at least DISTIA,B] higher than the one to which \nA has been allocated. This heuristic is quite similar to the one used with the Traveling Salesman Problem \nand, in fact, in the WO case, register allocation can be formulated as a Traveling Salesman Problem. \nFor scalar lifetimes, too, an attempt is made to minimize thetime that a register is idle. In fact, the \nadjacency ordering algorithm essentially performs a quick-and-dirty allocation. AL any point in time, \nCurrentSet is the set of lifetimes that are tentatively allocated to the same register--the same row \nin the space-timediagram.This heuristickeepsappendinglifetimesto the right-hand side of the row, always \nselecting at each step thelifetime that minimizes the gap between the end of the previous lifetime and \nthe selected lifetime. When it is impossible to add an additional lifetime without conflicting with one \nof the lifetimes in that row (taking into account the wrap-around of the lifetime at K*II), a new row \nis started. Conflict ordering ismotivatedbyaregisterallocationslrategywhich is to vector lifetimes what \ngraph coloring [4] is to sealar lifetimes. This strategy reduces to the graph coloring approach in the \ncase of scalar lifetimes. The basic idea is to establish, for each lifetime, a reasonably tight upper \nbound on the number of registers to which this lifetime cannot be allocated due to interference with \nthe rest of the lifetimes, across all possible allocations for the rest of the lifetimes. This upper \nbound is referred to as the total conflict for that lifetime. The total conflict plus one is guaranteed \nto be an adequate number of registers to ensure the allocatability of that lifetime. The lifetime with \nthe smallest total conflict is set aside and the total conflict is re-computed ignoring all lifetimes \nthat have been set aside. This process is repeated until all lifetimes have been set aside. The Ii fetimcs \nare ordered in the reverse order in which they were set aside. The conflict ordering algorithm is shown \nin Figure 13. The total conflict is computed using the CONFLICT matrix. For vector lifetimes, CONFLICT[i,j] \n= DIST[i,j] + DIST~,i] -1 for all i%j. For scalar lifetimes, CONFLICT[i,j] = DIST[i,j] = DIST~,i] for \nall i#j. The total remaining conflict for lifetime i at any point in time is the sum of all CONFLICT[i,j] \nsuch that i#j and j has not yet been set aside, Each ordering heuristic takes the incoming list of lifetimes \nand reorders them in accordance with the heuristic. Multiple heuristics may be employed in conduction \nwith one another. When this is done, it is necessary to decide which heuristic is the primary one, which \none is secondary (i.e., is used to break ties when the primary one cannot discriminate between two lifetimes), \nand so on. When two heuristics are used together, there are two possible prioritizations and, when all \nthree are used together, there are six possible prioritizations. The success of the register allocator \nis sensitive to the specific prioritization selected. For each combination of allocation strategy and \nallocation algorithm, all possible prioritizations were ev ahrated using the input data set described \nin Section 5. The best prioritization in each case is listed in Table 2. procedure ConfIictOrder; { \nDefinition: RemainingTotalConflict[i] is defined to be the sum of J [ Conflict[i,j] over all j, i<>j, \nwhich have not yet been } { set aside. } bagin  initialize RemainingTotalConflict for all lifetimes; \n put all lifetimes in the set NotSetAside; while NotSetAside # ampty do begin MinConflict := infinity; \n for i:= 1 to NumberOfLifetimes do if i in NotSetAside then  if RemainingTotalConflict[i] < MinConflict \nthen begin  Minconflict := RemainingTotalConflict[i]; SelectedLT := i; end; {if } remove SelectedLT \nfrom NotSetAside;  for all lifetimes j which are in NotSetAside do update RemainingTotalConflict[j]; \nend; { while ) order the lifetimes in the reverse order of removal from NotSetAside; end; Figure 13. \nThe conflict ordering algorithm Table 2. Prioritization of ordering heuristics when used in conjunction \nwith one another. The order in which the heuristics are listed is primary heuristic firs~ then secondary \nheuristic etc. Allocation Allocation Priorily of Ordering Strategy Algorithm IIusrisIics MvE Best F1t \ncontlict, start FirstFb start, adjacency conflict, adjacency conflict,adjacency start, End Fh conftict, \n start adjacency, start adjacency, cottftict adjacency,contlict start, Wo BestFb start, conftict \nBA First Fb stat-t adjacency, End Fit adjacency,conflict adjacency,sum, conflict In the context of the \ntaxonomy that we have defined, the Cydra 5 compiler generates kernel-only code and performs blades allocation \nusing the end fit allocation algorithm along with adjacency (primary) and start-time (secondary ) ordering. \n 4.3 Computational Complexity The worst-case computational complexity for register allocation depends \nupon the specific combination of ordering heuristics and allocation algorithm that is employed. From \nan inspection of the algorithms (Figures 8-13) one can see that the worst-case complexity of the three \nordering heuristics and the three register allocation algorithms is: start time O(n2) conflict O(n2) \nadjacency O(n2) best fit max(O(n2), 0(n2r)) first fit max(O(n2), O(nr)) end flt max(0(n2), O(n)) where \nn is the number of lifetimes (either scalar or vector) to be allocated and r is the number of registers \nrequired to do so. (Note that for the MVE strategy, the number of lifclimcs will be Kmin times greater \nthan for the other strategies.) Empirically, we found that r was a linear function of n (approximately \n0.61* n+14.5) over the range of values of n for which there was statistical significance, So, effectively, \nbest fit, first tit and end fit are 0(n3), 0(n2) and 0(n2), respectively. Since the ordering heuristics \nare 0(n2), it is the selected allocation algorithm that dominates the complexity of the register allocation \nphase. Note that in the conflict ordering algorithm, updating RemainingTotalConflictfi] for all j that \nare still in NotSetAside is an O(n) operation. When multiple ordering heuristics are used together, the \nlowest priority ordering is performed first, and the highest priority one last. It is necessary, therefore, \nthat each sorting algorid~rn have the property that it maintains the relative ordering of all lifetimes \nthat are equal under the sorting criterion that is currently being applied. Consequently, a simplistic \nsort of 0(n2) complexity was used for start-time ordering instead of using heapsort which is O(n log \nn). The complexity expressions for the allocation algorithms contain two terms, the greater one of which \ndetermines the complexity. The 0 (n2) term is caused by having to update the disallowed locations for \neach as yet unallocated lifetime each time a lifetime is allocated (Figure 8). The updating process is \nO(n) in complexity, and it is invoked n times. The other term is dependent on the allocation algorithm \nselected. In understanding these, one should note that each of the procedures in Figures 9-11 are invoked \nO(n) times and that the loops in Figures 9 and 10 are executed O(r) times on each invocation. Lastly, \nthe function FOM that is called from within the loop in the best fit algorithm is itself O(n) in complexity. \nThe worst-case complexity is generally quite pessimistic compared to what is actually experienced because \nin many cases, the iterations of the loops are guarded by IF-statements that are often not executed. \nSo, we also have measured the empirical computational complexity for each combination. This is presented \nin Section 5. Counters were placed at the entry point as well as in every innermost loop of every relevant \nprocedure. These counters count the total number of times that that point in the program is visited. \nIn the spirit of worst-case complexity analysis, we used the largest of the counts as the indicator of \nempirical computational complexity. 5 Experimental Results Measurements were taken to examine the effectiveness \nof the allocation algorithms described in the previous section. To this end, over one thousand FORTRAN \nDO-loops from the SPEC! [20] and PERFECT Club [2] benchmark suites were modulo scheduled on a hypothetical \nmachine similar to the Cydra 5 [18]. Only measurements on DO-loops were performed, even though all the \nregister allocation algorithms discussed are fully applicable to WHILE-loops and loops with early exits. \nThis is due to the limitation of the Cydra 5 compiler which was used to generate the input to the register \nallocator and which is unable to recognize loops which are not DO-loops. Also, only those DO-loops with \nno conditional branching were selected so as to permit a meaningful comparison between MVE (which assumes \nno predicated execution) and the other two allocation strategies. Apart from the relative code sizes \nfor the three allocation strategies, we do not expect any of the conclusions regarding the relative merits \nof the various allocation algorithms and ordering heuristics to be different across the broader set of \nloops. Table 3. Relevant details of the machine model used by the compiler for this study. I l>ipcline \nNumber Operations Latency II I I Memorypm I 2 I Load 1131 Store 1 AddressALU 2 Addressadd/subrract 1 \n Adder 1 pointadd/subtract Floating1 II I Integer 1 add/subtract Multiplier 1 pointmultiply Floating2 \nI I IntegerI2 multiply Instruction 1 Branch 2 I The relevant details of this hypothetical machine are \nshown in Table 3. The machine model permits the issue of up to seven operations pcr cycle. All the functional \nunit pipelines read their inputs and write their results to a common rotating register file. The 13 cycle \nload latency reflects an access that bypasses the first-level cache. (Our experience has been that locality \nof reference is often missing within imermost loops. Until compiler algorithms are devised which reliably \nascertain whether or not a sequence of references in a loop has locality, it is preferably to have the \ndeterministic albeit longer latency that comes from consistently bypassing the first-level cache.) % \n100\u00ad 0 80 ., f 60 ., I 40 , o o 20 , P 1 s 0: 50 100 150 200 250 Number of loop variants % 100 0 80 f \n60 I 40 o ~  0 20 !~ P s 0 0 1020 3040 Number of live-in variables % 100 T 0 80 ,. f 60 .. I 0 40 \n, 0 P s 0+ o 50 100 150 200 250 4 300 Scheduled loop length in cycles Figure 14. Cumulative distribution \nfunctions The Cydra 5 compiler was used to generate a modulo schedule for each loop and then output a \nset of lifetime descriptions for all the loop-variants. The lifetime descriptions were used as input \nto a program that applied all combinations of ordering heuristics and allocation algorithms to each input \nset in turn. The register allocation program outputs various data for each input data set and for each \ncombination of algorithms and heuristics. The data include the number of registers needed by the loop-variants, \na lower bound on the number required, the code size, and the empirical computational complexity. The \ncharacteristics of the loops that were analyzed in our experiment are shown in Figure 14. Each graph \nis a cumulative distribution. From Figure 14 it can be seen that most loops in our sample set had fewer \nthan 80 loop-v ari ants, although there were a few that had more than 200. In contrast, most loops had \nfewer than 10 loop-invariants with a few having more than 30. Every loop had at least one live-in variable, \nmost having ICSS than 15, and a few having over 30. Live-out variables were less frequent; not shown \nin Figure 14 is that 807. had no live-out variables and, of the remaining, about half had one live-oul \nand the rest had two live-outs. The graph for the initiation interval indicates that most loops had an \nII less than 40 cycles. Nearly 60% of the loops had an II less than 8 cycles. Schedule lengths we generally \nless than 90 cycles, but a few loops had schedules several hundred cycles long. The number of slagcs \nin the software pipeline was 10 or less 90% of the time. % 1oo\u00ad o 80 -, f 60 .. I 40. 0 o 20 -, P \nso o 10 20 3040 Number of loop invariants 0/0 100 \u00ad o 80 <, f 60 . I 40 0 0 20 ,, P 4 s 0 0 50 100 \n150 200 Initiation Interval in cycles % 100 0 80 f 60 40 0 0 20 ;~ , P so o 5 10 1520 Number of stages \nfor key statistics of the input loops Tables 4-6 contains data on the various heuristics and strategies. \nAll combinations were studied. For each combination . Table 4 shows the optimality, i.e., the average \nof the difference between the achieved number of registers and the lower bound, . Table 5 shows the average \nmeasured empirical computational complexity relative to the lowest average empirical computational complexity \nacross all combinations (that for WO with end fit), and . Table 6 shows the average code size in operations. \nFor MVE, two code sizes are shown: with multiple epilogues and with preconditioning. (In the context \nof VLIW machmes, a distinction needs to be made between an operation and an instruction. An instruction \nconsists of multiple operations, whereas an opcratirm is a unit of computation equivalent to a RISC instruction. \nThe code size is proportional to the number of operations for both VLIW and RISC processors.) Table \n4. Comparison of the optimality of strategies and heuristics MVE Strategy WOStratcQv II BA Stratew as \nis Strtt Conf st/cnf best as is 3.21 1.71 1.32 1.30 fit adj 2.40 1.75 1.30 1.30 t-iit as is 3.36 1.77 \n1 36 1.36 2.46 fit adj 2.5S 1.82 1.36 1,36 0,18 end as is 16.42 35.79 27.75 28.80 5,65 fit adj 3.30 2.51 \n3.00 237 018 = Table 5. Comparison of the empirical computational complexity of strategies and heuristics \nMVE Strategy WO Strategy BA Strategy as is Strtt conf stlcnf as is Strrt conf stlcnf best as is 1.21 \n1.14 1,10 1.12 1.24 1.18 1,13 1.17 fit adj 1.01 1.01 1,01 1.01 1.05 1.04 1.04 1.04 fiit as is 1.00 1.00 \n1,00 1.00 1.03 1.03 1.03 fit adj 1.00 1.00 1,00 1.00 1.03 1,03 1.03 m end as is 3.58 1.00 1.00 1.00 1.00 \n1.03 1.03 1.03 1.03 fit adj 3.58 1.00 1.00 1 00 I.oo 1.03 1.03 1.03 1.03 0 1007 9. 100 0 0 80 f r 60 \n1 60\u00ad - E -best &#38; first fit 1 40 0 0 40- BA -first&#38; end tit 0 0 20 P a 20 ~ o 1 4 I 8 t 12 1 \n16 [ 20 P s 0 o 1 16 i 32 1 48 1 64 &#38; 80 1 96 I 112 Number of registers Difference between achieved \nand lower bounf (b) (a) 90 100-~ 100\u00ad 0 80-0 80- MVE with multiple epilogues f f 60-MVE with precOn(fiLioning \n60--WO &#38; BA 1 1 40. -40\u00ad 00 00 20. . P 11I1  0.r 8II I : o~ s o 400 800 1200 1600 2000 o 15000 \n30000 45000 60000 Code size in number of machine operations Empmical computational complexity (c) (d) \nFigure 15. Cumulative distribution functions for (a) the degree of optimality (b) the number of registers \nused (c) the code size for tie loop (d) the empirical computational complexity. For the MVE strategy, \nfewest excess registers were used on the average by best fit allocation, using conflict ordering in conjunction \nwith one or both of start-time and adjacency ordering. If first fit allocation were to be employed instead \nof best fit, but using the same ordering heuristics, marginally poorer allocation is obtained (< 5% worse) \nbut with almost a halving of the empirical computational complexity. The WO strategy achieves near-optimal \nresults when it uses adjacency and start-time heuristics (with or without the use of the conflict heuristic) \nand regardless of which allocation algorithm is used. For the BA strategy, best results were obtained \nwilh besl fit allocation using adjacency, start-time and conflict ordering. This only had slightly higher \nempirical computational complexity (by lYo) than f~st fit or end fit. Figure 15a shows the cumulative \ndistribution of the difference between the actual number of registers used and the lower bound, i.e., \nthe extent of the deviation from the lower bound. It should be noted that the deviation from optimality \nmay be less since even the optimal allocation may require more registers than the lower bound. A separate \nplot is shown for each of the nine combinations of allocation strategy and allocation algorithm. In each \ncase, the results corresponding to the best combination of ordering heuristics were selected. The three \nplots for WO are indistinguishable. Over 90% of the loops yield optimal allocation, and almost all of \nthe rest require only one register over the lower bound. BA with best fit is almost as good, 80 %0of \nthe loops are optimal, another 15% require an additional register, and very few need more than two registers \nover the lower bound. First fit and end fit for BA are very similar to each other, and significantly \ndifferent from best fit. Especially interesting is the plateau between 1 and 10 registers; over 80% of \nthe loops need at most one additional register, and almost 1090 require 11 additional registers, with \nthe rest of the loops requiring some intermediate number of registers. This suggests an interesting, \nhybrid allocation algorithm in which one first attempts first fit allocation and then, if too many additional \nregisters end up being required, performs best fit allocation. For MVE, best fit and first fit are almost \nindistinguishable. L$O %0of the loops are optimal, and rarely are more than 6 additional registers required. \nEnd fit for MVE is significantly worse. h the average, all three strategies, when they employ the best \ncombination of allocation algorithm and ordering heuristics, are able to achieve extremely good register \nallocation; On the average, WO is near-optimal, BA requires 0.24 reg istcrs more than the lower bound, \nand MVE requires 1.3 registers over the lower bound. The differences lie in their empirical complexity \nand the resulting code size. Table 6. Comparison of the code size of strategies Code size MVE -multiple \nepitogues 25S.51 MVE -preconditioning 14243 42.30 18.21 If we limit our discussion to only the best \ncombinations for each strategy, WO has the lowest empirical complexity, B A is about d~o worse, and MVE \nis worse by a factor of 6.6. This factor drops to 3.6 if first fiL instead of best fit, is used for MVE. \nWith respect to code size, BA is clearly the best. WO and MVE require code that is 2.3 and 14.2 times \nlarger, respectively. If preconditioned code is used with MVE, the factor drops to 7.8 (but at the cost \nof reduced performance, especially for small trip counts). It must be noted, however, that these results \ncorrespond only to the modulo scheduled innermost loops. The relative code size and compilation time \nincreases for the complete program would be less depending on what fraction of the code consists of innermost \nloops. Furthermore, the increase in the static code size is relatively unimportan~ what matters is the \neffect of code size on instruction cache performance. The extent to which this is a problem requires \nfurther study. For each combination of allocation strategy and allocation algorithm, Table 7 lists the \nset of preferred ordering heuristics. The combination of ordering heuristics selected is the one that \nyields the best register allocation, on the average. When different combinations of heuristics yield \nresults that are very close in terms of optimality, the least expensive combination, computationally, \nis selected as the preferred one. In comparing the best fit and first fit algorithms for MVE, one sees \nthat the two are very close in terms of optimality (a difference of 0.06 registers on the average) but \nthat first fit has about half the empirical complexity of best fit. Consequently, in Table 7, first fit \nis underlined to indicate that it is preferred over best fit. With WO, all three allocation algorithms \nare equally optimal. So, we prefer end fit since it is the least expensive. With BA, best fit is significantly \nbetter than the other two for verv little added complexity. Accordingly, best fit is the pr~ferred allocation \nalgorithm for BA. Table 7. The preferred allocation alg )rithm for each allocation strategy and the preferred \nordering heuristics for each allocation algorithm. Allocation Slrategy Allocation PreferredOrdering Algorithm \nHeuristics @referred) MVE Best Fit contlkt Eir&#38;Eit conftkt End Fit adjacency, stsrt, ccmtlct Wo Best \nFh adjacency, start Fmt Fh adjacency, start m adjacency, start BA Ik&#38;LEit adjacency, stafi First \nFn adjacency, conflict End Fit adjacency, conflict Comparing the WO and BA strategies (Tables 4-6), \nit cart be seen that the number of registers used and the measured computational complexity are not very \ndifferent, but the average code size for strategy WO is 2.3 times larger. If one is concerned about instruction \ncache performance, it may be preferable to use BA register allocation, and generate KO code. This also \nallows one to avoid the complexities involved with WO in generating the correct code schema for PKEn \ncode [17]. The performance degradation of BA relative to WO is limited to the lost opportunity in percolating \nouter loop computation into the prologue and epilogue. (In both cases, it is assumed that hardware support \nis available in the form of the rotating register file and predicated execution.) With the MVE strategy, \ndegrees of unroll greater than G,. were attempted. Larger degrees of unroll resulted in higher empirical \ncomputational complexity and code size, and provided little to no reduction in the number of registers \nused. For over 80~0 of the loops, unrolling by more than Kmin saved at most one register. Very few cases \nwere found where the savings were more than three registers. Furthermore, the number of registers needed \ndoes not decrease monotonically as the degree of unroll is increased. Additional unrolling beyond Kmin \nis not worthwhile. Figure 15b-d show the cumulative distributions for the number of registers used, the \ncode size and the empirical computational complexity for the preferred allocation algorithm and orderin \nheuristics for each strategy. For the machine model used, 980J of the loops required less than 64 registers \nfor loop-variants, and less than 12 registers for loop-invariants. The difference between the strategies \nis hardly noticeable in Figure 15b, but the MVE strategy performs slightly worse than the other two. \nFigure 15c and Figure 15d show the cumulative distributions of the code size and empirical computational \ncomplexity, respectively. The relative ordering of MVE, WO and BA is the same for the variance and maximum \nof the code size distribution as it is for the mean: MVE is larger than WO which is larger than BA. The \nsame is true for the empirical computational complexity; the mean, variance and maximum for MVE are all \ngreater than the corresponding values for BA which, in turn, are marginally greater than those for WO. \n6 Conclusion When hardware support is present, in the form of predicated execution and the rotating register \nfile, the kernel-only, b] ades allocation strategy is to be preferred over the wands-only strategy requiring \nprologue-kernel-epilogue code. It results in about 55q0 less code size, and with insignificant increases \nin the average number of registers used or in the empirical computational complexity. The best combination \nof heuristics to use with best fit allocation is adjacency and start-time ordering heuristics. The absence \nof hardware support for modulo scheduling necessitates prologue and epilogue code, kernel unrolling, \nmodulo variable expansion, and either a sequential preconditioning loop or multiple epilogues. Our experimental \ndata indicate that first fit allocation with conflict ordering is the best choice. Unrolling the kernel \nmore than the minimum number of times necess sry does not significantly reduce the number of registers \nrequired; instead it increases both the empirical computational complexity and the average code size. \nOur recommendation would be to employ the minimum degree of unroll. The measurements of computational \ncomplexity and average code size lend some weight to arguments in favor of hardware support for modulo \nscheduling, in the form of the rotating register file and predicated execution. Without it, the increased \nempirical computational complexity (by a factor of 3 to 6) results in longer compile times. The increased \naverage code size (by a factor of 7 to 14) can have a negative effect on the instruction hit rate in \ncache and, hence, on performance. The sequential execution of the residual iterations in preconditioned \ncode can further degrade performance for realistic trip counts. In this paper, we have studied the optimality \nof various allocation algorithms and heuristics, This serves as a useful guide in selecting the best \nalgorithm for each situation with an objective to minimizing the number of registers needed. A different \nobjective, and one that is equally important, is the issue of how one performs register allocation so \nas to make do with the number of registers that are available. The best algorithm from the first viewpoint \nis best from the second viewpoint, too. However, once modulo scheduling has been performed, if the register \nallocator fails to find a solution requiring no more registers than are available, some additional action \nmust be taken. The options include one of the following actions: . increasing the 11, _ introducing spill \ncode, or _ splitting the loop into smaller loops followed by another modulo scheduling and register allocation \npass. The issues and the difficulties in so doing are elaborated upon in [16]. Acknowledgements The end \nfit allocation algorithm and the adjacency ordering heuristic, described in this paper, were conceived \nof by Ross Towle who, along with Warren Ristow and Jim Dehnert, implemented register allocation for vector \nlifetimes in the Cydra 5 compiler. References 1. 2. 3. 4. 5. 6. 7. 8. 9.  10. 11. 12. 13. \n Allen, J. R., et al. Conversion of control dependence to data dependence. In Proceedings of the Tenth \nAnnual ACM Symposium on Principles of Programming Languages, (January, 1983). Berry, M., et al. The Perfect \nClub Benchmarks: Effective Performance Evaluation of Supercomputers. The International Journal of Supercomputer \nApplications ,3 (1989), 5-40. Callahan, D., Carr, S., and Kennedy, K. Improving Register Allocation for \nSubscripted Variables, In Proceedings of the ACM SIGPLAN 90 Conference on Programming I&#38;tguage Design \nand Implementation, (June, 1990), 53- Chaitin, G.J. Register allocation and spilling via graph coloring. \nIn Proceedings of the SIGPLAN82 Symposium on Compiler Construction, (June, 1982), 201-207. Charlesworth, \nA.E. An Approach to Scientific Array Processing: The Architectural Design of the AP-120B/FPS\u00ad164 Family. \nIEEE Computer 14, 9 (September, 1981), 18\u00ad 27. Dehnert, J. C., Hsu, P.Y.-T., and Bratt, J.P. Overlapped \nloop support in the Cydra 5. In Proceedings of the Third International Conference on Architectural Support \nfor Programming Languages and Operating Systems, (Boston, Mass., April, 1989), 26-38. Ebcioglu, K., and \nNakatani, T. A new compilation technique for parallelizing loops with unpredictable branches on a VLIW \narchitecture. In Proceedirws of the Second Workshop on Programming Langua~es and Compilers for Parallel \nComputing, (Urbsma-Chsmpaign, 1989), 213-229. Hendren, L. J., et al. Register Allocation using Cyclic \nInterval Graphs: A New Approach to art Old Problem. ACAPS Technical Memo 33. Advanced Computer Architecture \nand Program Structures Group, McGill University, Montreal, Canada, 1992. Hsu, P.Y.T. Highly Concurrent \nScalar Processing. CSG-49. Coordinated Science Lab., University of Illinois, Urbana, Illinois, 1986. \nJain, S. Circular scheduling: A new technique to perform software pipclining. In Proceedings of the ACM \nSIGPLAN 91 Conference on Programming Language Design and Implementation, (June, 1991 ), 219-228, Lam, \nM. Software pipelining: an effective scheduling technique for VLIW machines. In Proceedings of the ACM \nSIGPLAN 88 Conference on Programming Language Design and Implementation, (June, 1988), 318-327. Lee, \nR. L., Kwok, A. Y., and Briggs, F.A. The floating point performance of a superscalar SPARC processor. \nIn Proceedings of the Fourth International Conference on Architectural Support for Programming Languages \nand Operating Systems, (Santa Clara, California, April, 1991), 28-37. Nicolau, A., and Potasman, R. Realistic \nscheduling: compaction for pipelined architectures. In Proceedings of the 23th Annual Workshop on Microprogramming \nand ~~:+-;architecture, (Orlando, Florida, November, 1990), 14. Rau, B.R. Data flow and dependence analysis \nfor instruction level parallelism. In Proceedings of the Fourth Workshop on Languages and Compilers for \nParallel Computing, (Santa Clara, August, 1991). 15. Rau, B. R., and Glaeser, C.D. Some scheduling techniques \nand art easily schedulable horizontal architecture for high performance scientific computing. In Proceedings \nof the Fourteenth Annual Workshop on Microprogramming, (October, 1981), 183-198.  16. Rau, B. R., et \nal. Register Allocation for Moduk] Scheduled Loops: Strategies, Algorithms and Heuristics. HP Labs Technical \nReport HPL-92-48. Hewlett-Packard Laboratories, Palo Alto, California, 1992. 17. Rau, B.R., et al. Code \nGeneration Schema for Modulo Scheduled DO-Loops and WHILE-Loops. HP Labs Technical Report HPL-92-47. \nHewlett-Packard Laboratories, Palo Alto, California, 1992. 18. Rau, B. R., et al. The Cydra 5 departmental \nsupercomputcr: design philosophies, decisions and trade-offs. IEEE Compu(er 22, 1 (January, 1989). 19. \nTirttmalai, P., Lee, M., and Schlansker, M .S. Pamdlelization of loops with exits on pipelined architectures. \nIn Proceedings of the Supercomputing 90, (November, 1990), 200-212. 20. Uniejewski, J. SPEC Benchmark \nSuite: Designed for Today s Advanced Systems. SPEC Newsletter 1, 1 (Fall, 1989).   \n\t\t\t", "proc_id": "143095", "abstract": "<p>Software pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.</p>", "authors": [{"name": "B. R. Rau", "author_profile_id": "81451599881", "affiliation": "", "person_id": "P26298", "email_address": "", "orcid_id": ""}, {"name": "M. Lee", "author_profile_id": "81452596759", "affiliation": "", "person_id": "PP31084173", "email_address": "", "orcid_id": ""}, {"name": "P. P. Tirumalai", "author_profile_id": "81100473471", "affiliation": "", "person_id": "PP31097395", "email_address": "", "orcid_id": ""}, {"name": "M. S. Schlansker", "author_profile_id": "81100380823", "affiliation": "", "person_id": "PP39082934", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143141", "year": "1992", "article_id": "143141", "conference": "PLDI", "title": "Register allocation for software pipelined loops", "url": "http://dl.acm.org/citation.cfm?id=143141"}