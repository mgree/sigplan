{"article_publication_date": "07-01-1992", "fulltext": "\n Sharlit A tool for building optimizers Steven W.K. Tjiang John L. Hennessy Computer Systems Laboratory \n Stanfo-rd A complex and time-consuming function of a modern compiler is global optimization. Unlike \nother functions of a compiler such as parsing and code generation which exam\u00adine only one statement or \none basic block at a time, optimize\u00adrs are much larger in scope, examining and changing large portions \nof a program all at once. The larger scope means optimizers must derive and use complex relationships \namong widely separated parts of a program. To get the best code, optimizers must perform many program \ntransforma\u00adtions. Each of these transformations makes its own particu\u00adlar demands on the internal representation \nof programs; each can interact with and depend on the others in different ways, This makes optimizers \nlarge and co~mplex. Despite their complexity few tools exist to help in build\u00ading optimizers. This is \nin stark contrast with other parts of the compiler where years of experience have culminated in tools \nwith which these other parts can be constructed easily. For example, parser generators are used to build \nfront-ends, and peephole optimizers and tree matchers are used to build code generators. This paper presents \nSharlit, a tool to, support the con\u00adstruction of modular and extensible global optimizers. We will show \nhow Sharlit helps in constructing data-flow ana\u00adlyzers and the transformations that use data-flow analysis \ninformation, both are major components of any optimizer. Sharlit is implemented in C++ and uses C++ in \nthe same way that YACCuses C. Thus we assume the reader has some familiarity with C++ [9]. Sharlit and \nOptimizers Traditionally optimizers are organized as a fixed sequence of phases. Before optimization, \na front-end translates the soume program into an intermediate language. The optimizer reads the IL program \none procedure at a time, converting it into an internal representation (IR),a ccmtrol-flow graph. typicallyEach \noptimizer phase operates on the flow graph, analyzing and transforming it. When all the phase are finished, \nthe IRis writtenout as a better IL program. To write an optimizer with Sharlit, the compiler writer would \norganize each optimization phase around a data-flow analysis (DFA)problem of the IR graph. In such an \norganiza\u00adtion, information is collected by a DFA ste]p, followed by an flow graph traversal that performs \noptimizations at each node based on the collected data-flow i:nformation. Ahor Sethi and Ullman[l], and \nChow[4] use such an organization to describe and implement their optimization. Sharlit auto\u00admates the \nconstruction of optimization phases, using this organization. Permission to copy without fee all or part \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee and/or specific permission. ACM SIGPLAN 92 PLD1-6/92/CA @ 1992 ACM 0-89791 \n-476 -7/92 /0006 /0082 . ..$1 .50 University Because data-flow analysis consumes a lot of time and space, \ncompiler writers are concerned with implementing the analyzers efficiently. This concern has led to two \ndeficiencies in existing optimizers. First, compiler writers lower time and space requirements by limiting \nthe kinds of DFA solved. Unfortunately such limits also restrict the kinds of code opti\u00admization that \ncan be done, a restriction contrary to our goal of building an extensible optimizer. Second, optimizers \nare designed using a control-flow graph with two levels. Each node of the graph is a linear sequence \nof instructions, that is, a basic block. A two-level graph rtduces the number of nodes and the size of \nthe data-flow information required during DFA.Unfortunately the two-level graph obscures data-flow analysis \nand code transformation in several ways. The major drawback of the two-level approach is that it complicates \nthe programming of DFAs. Each analyzer must have both a local analysis phase and a propagation phase. \nLocal analysis computes the flow function of a basic block by composing the flow function of its instructions. \nThe propaga\u00adtor uses the global DFA solution at each basic block to com\u00adpute the local solution at each \nof its instructions. These two phases are custom built by the compiler writer. A second drawback is that \ncompiler writers must under\u00adstand the DFAand all the data structures at two levels before r they can \nbuild prototype DFAs.The two levels appear in most published descriptions of DFAs[2, 16, 30]. Given that \nany DFA can be understood and solved knowing only the data-flow effect of each instruction, the two-level \nstructure is a compli\u00adcating implementation detail. A third drawback is that the two-level structure \nencour\u00adages non-modularity. To minimize the number of CFG tra\u00adversals, compiler writers often combine \nthe local analysis of several DFAproblems in one traversal, and the correspond\u00ading propagators in another \ntraversal. Hence, the compiler writer must shoulder the burden of orchestrating these phases and their \nside-effects so they will perform as desired. Instead of limiting the analyses or using basic blocks, \nSharlit lets compiler writers program global analyses and optimizations in a modular fashion with the \nfollowing abstractions: 1. Flow graphs: specified by the compiler writer, The nodes of the graph can \nbe basic blocks or individual IR instructions. We advocate the latter. 2. Plow values: values that \nflow through the graph. The solution of the DFA is one flow value for each flow\u00adgraph node. For traditional \nDFAproblems, the flow val\u00adues are bit vectors that represent, for example, sets of expressions. 3. Flow \nfunctions: functions that represent the effect of flow graph nodes and paths on the flow values; these \nare specified as input to Sharlit. 4. Action routines: routines that use the DFA solution to perform \nprogram optimization.  5. Path simplification rules: rules that show how to com- Source bine flow functions \ninto other flow functions. Them are o four kinds of rules: create, absorb, join, and star. With these \nabstractions, the compiler writers can pose their DFA problems as if each node of the CFG is an instruction. \nUsing Sharlit, the compiler writer can implement a wide range of program analysis, from the traditional \nbit-vector based DFAs to ones using sophisticated symbolic analysis. Sharlit generates efficient data-flow \nanalyzers that use path simplification rules to simplify the graph. Path simplification allows a compiler \nwriter to use the simple one-instruction-one-node graph in Sharlit and still obtain efficient solvers \nfor the DFAproblems.  1.1Path simplification Path simplification is based on a version of Tarjan s fast \nPiith algorithm [111.Tarjan s algorithm finds a path expression rep\u00ad resenting all paths from the source \nto every node in a graph. Path expressions are regular expressions built from nc,de labels and the operators \n., +, and *. For example, in Figure 1, the path expression from the exit of the source node to the exitofnode \nDis A((B+C) DA) ( B+ C) D, Sharlitusespi~th expressions in data-flow analysis by interpreting the nckde \nlabels as flow functions, path concatenation( ) as function composition, path addition(+) as taking the \nmeet of functions, and the Kleene-star (*) as finding the fixed point of a function. The compiler writer \ntells Sharlit how to perform these opera\u00ad tions with path simplification rules. Path simplification is \nversatile: the rules can describe a wide range of DFA solvers, from simple iterative ones, to solvers \nusing local analysis, to sophisticated solvers that use interval analysis. When confronted with irreducible \ngraphs or incomplete sets of rules, path simplification degrades gracefully it is still able to simplify \nsignificant portions of the graph.  2 Writing DFASwith Sharlit This section provides an overview of \nSharlit and path simpli\u00adfication, and shows how one can use Sharlit to write DFAs.We demonstrate Sharlit \nwith three examples, all solving the armil\u00adable expression problem (AVP)[l]. An expression is available \nat a node n if every path from the source to n has evaluated the expression and the value of the expression \nremains the same if reevaluated at n. Determining available expressions is important in code motion [41. \nThese three examples show Sharlit s ability to apply a variety of solution techniques and solve versions \nof AVP using progressively more sophisticated and potentially faster solution techniques. To underscore \nthe extensibility of Sharlit-generated ana\u00adlyzers, we build each example from the previous one by add\u00ading \nnew flow functions and new path simplification rules. The first example is a prototype solver that uses \niteration, In this first example, eachIR instruction is a node in the flow graph, making the prototype \ninefficient. By adding some simplification rules, we extend the prototype to do local anal\u00adysis, reducing \nthe work needed during iteration. By adding yet more rules meet and the star rules we enhance the solver \nto use interval analysis. 2.1 Preliminaries Figure 2 shows how an internal phase of an optimizer is me\u00adated \nusing Sharlit. Each automatically generated data-flow Figure 1. A smali graph. Shariit Specification \nI Flow Val I Provided OnJ byU r miizl %1. . controi-flow ana- Flow lyzer Function $   J ~1 . path \nsimplifier Classes ii ?::f . iterator ,j Provided by Sharlit ) ptimization Figure 2. Structure of a \nShariit-based optimizer phase. analyzer consists of four major components: control-flow analyzer, path \nsimplifier, iterator, and propagator. The con\u00adtrol-flow analyzer discovers the loop structure of the \ncontrol\u00adflow graph and summarizes the structure (control-flow anal\u00adysis is done once for several DFAs)[8, \n10]. The path simplifier, generated from the rules in the description, uses the contml\u00adflow information \nto eliminate nodes from the flow graph, cre\u00adating a reduced graph that is easier to solve. The iterator \nmakes one or mow phases over the reduced graph, calling flow functions until a solution (consisting of \nflow values) to the data-flow problem is found. The propagator computes the solution at nodes eliminated \nby the path simplifier. In the process, the propagator calls user-supplied action routines for each node, \nroutines that use the solution to perform pr\u00adogramoptimizations. All the components axe compiled and \nlinked (black arrows) into an internal phase of the optimizer. The flow value is implemented as a C-t-+class \nby the compiler writer, The compiler writer can implement the flow graph in any way, but must provide \ngraph routines that let the generated data-flow analyzer view the flow graph as a doubly-linked directed \ngraph with edges stored as adjancy vectors. In our examples, the lR is quad based and each IR instruction \nis implemented using the following structure: class IR_i,nstruction { ... public: int kind; IR_vari.able \ndst,srcl,src2; Expression expr_no; }: -. Only those fields relevant to our analysis are shown. TheIR \nhasseveral types ofinstructions, distinguishable by the value of the field kind. Since expressions are \nbuilt recursively from arithmetic operations and variables, we am interested in the following values \nof kind IR_oP: performs an arithmetic operation on srcl and s rc 2, storing the result into the variable \ndst. I R_ldc: loads a constant into the variable dst. IR kill: modifies thevariabledst. IR~initial: is \nthe instruction type used to mark the source node of the graph. IR others: doesn t affect the contents \nof any of the vafiables. The IL reader has performed value numbering [1] on the instructions, has set \nthe field expr_no to identify what expression is being computed by the instruction, and has computed \nfor each variable the set of expressions Wed by a write to it. 2.2 Example 1: iterative analysis Figure \n3 shows a Sharlit specification that describes a solver called Avai. 1 for available expressions. Within \nthe C++ code of Sharlit specifications, special variables begin with an underscore and are capitalized. \n1. Flow graph nodes and Flow values: Part A describes flow graph nodes and flow values. Sharlit incorporates \nthis information as fields and func\u00adtions of the data-flow analyzer, The brace-enclosed C++ declaration \nfollowing the keyword solver declares vari\u00adables to be use by other parts of the specification. In this \ncase, there is only one field, an array of kill sets that can be accessed as _P->kill_sets in the flow \nfunctions. The node_base and value_base tell Sharlit the class name of the flow graph node and flow value. \nPart A also describes three functions that are called as the DFAproblem is being solved. The new_value \nfunction returns a new flow value initialized to the uni\u00adversal set containing all expressions. The copy-value \nfunction copies a source flow value pointed to by SRC to a destination pointed to by _DST. At the same \nt~me, copy_value returns a boolean value (1 or O)indicating whether the copy will change the value of \nthe destina\u00adtion *_DST.The meet function combines several flow values, stored in the array SRCS,into \none value which will be stored into the dest~nation * DST.For available expressions, meet takes the intersection \nof the values stored in SRCS.  2. Flow functions: Part B describes the flow functions. The incoming \nflow value is pointed to by the variable _v. The C++ code following the keyword flow computes an output \nflow value, leaving a pointer to it in _v. In available expressions, and many other DFAs,the code can \nmodify the incoming value directly into the outgoing value. For large, complex values, modification-in-place \navoids unnecessary copies. There is a flow function corresponding to each instruction type discussed \nabove. . AV_express i.on corresponds to IR_l dc and IR_op instructions. This function inserts expr_no, \nthe expres\u00adsion number being generated by the current node _N, into the incoming flow value, then removes \nfrom it the killed expressions, those whose valuemay change when the variable N->dst is modified. . AV_ki \n11 corresponds to the instruction IR_k i 11. This function removes all the killed expressions from the \nflow value. Avail: = solver: { ~m=-..-= s=.--=. ! Expr_set *kill_sets [ ] ; #(\\}; $ node base = IR_instruction; \nt I valu; base = Expr set; \\ .&#38; new v~lue: { t 5xpr_set *v=new Expr_set (etable) ; \\ v->mk_universal \n(); return v; }: copy value: { i f( *_SRC == *_D ST ) return O; * DST = * SRC; r;turn l; }: meet: \n{ int i; _DST->mk universal (); for(i=O;~<_NSRCS;i++) *_DST &#38;= *_SRCS[i]; }; AV_expression:flow \n{ *_V += _N->expr_no: \\ *_V-=_P >kill_sets [_N->dstJ; \\ \\ }; \\ AV_i.nitial: flow { ~, B _V->mk_empty \n( ) ; : , }; , AV_kill: flow { ,4 ,, *_V-=_P->ki ll_sets [_N >dst ] ; ,. . }; . ... ........... AV_identity: \nflow { }; .S--s==-s-.=s-? flow map: { \\ s=itch (*_N->kind) { \\ \\ case IR_op: / I case IR ldc: \\ \\return \nnew AV_expres sion; \\ case IR kill : \\ k, c retu~n new AV kill; case IR initial : ;1  , return new \nAV initial; ; case IR others : : : retu~n AV_identi.ty; . , } , }: , .... .. ........ .. end;  Figure \n3. Iterative solver for available expression. AV_init ial corresponds to the initial node IR_i.n i\u00adt \nial. This function initializes its output value to the empty set. AV identity corresponds to the IR others \ninstruc\u00adtion;. This fu~ction u~es its incoming flow value as its outgoing value. We must provide the \nabove mapping between instruc\u00adtions and flow functions, Part C is a flow map function called from control-flow \nanalysis to establish the correspondence between flow functions and instructions. Figure 4 shows the \niteration procedure that Sharlit uses to solve DFAproblems. The procedure takes as its input a list of \nnodes ordered in the depth-first order [6], computed dur\u00ading control-flow analysis. The ordering is used \nbecause it is easy to compute and it has fast convergence for bit-vector problems [61. The iterator uses \nthe following information about the DFAproblem: flow(u) is the flow function of the node u. The colm\u00ad \npiler writer provides this node to flow function map\u00ad ping. . ancestor(u) and new_flow (u) are computed \nduring path simplification. Since this example has no rules, the condition ancestor (ui) # Ui is always \nfalse. Thus, for now, we can ignore the first if statement in the loop. O(u) is the flow value (data-flow \ninformation) on exit of node u. Not every node will have one of these. has_meet (u) indicates whether \na meet operation is n~ec\u00ad essary to compute the input flow value of u. In the one-instruction-one-node \nflow graph, if Sharlit kept a copy of data-flow information a flow variable-at every node, the data-flow \nanalyzers would use a prohibitive amount of storage, The last two fields are important for elim\u00adinating \nflow variables. 2.3 Eliminating fiow variabies i When there are no simplification rules, the iterate \nprocedure in Figure 4 will call the flow function of every node in the graph as it iterates. However, \nnot every node needs a flow variable, In a Sharlit specification, a compiler writer cannot write code \nthat references flow variables directly. In particular, flow functions can only modify the variable _V. \nAt the beginning of a flow function, _V contains the input flow value of the node u with which the flow \nfunction is associated. The flow function modifies _V directly to arrive at the output flow value of \nu. Denying direct access to flow variables frees Sharlit to chose how flow variables are stored. Sharlit \nallocates flow variables at three kinds of nodes. We show two of the thwse below, The third kind will \nbe discussed later in Section 2.7. 66 A%? iterate (L) { Let L = [UI, U2,.... UJ be a df-order of the \nCFG. Let O(u) be the output flow variable associated with u _V &#38; new_valueo c &#38; true (fkMQ)(_v) \ncopy_value (O(ul), _V ) while c is txue { c +--false for i=2 to s{ if ancestor (ui) # ui { copy_value \n(_V, O(ancestor (ui))) (new_flow(ui)) (_V) 1 else if has_meet(ui) { Let {WI, Wz,.... w~} be predecessors \nof Ui meet (_V, [O(WJ, 0(7-@, .... O(WJI )  (flow (u;))(-v) } else (flOw(uJ) (-v) if O(ui) exists { \nc + c1copy_value(O(ui), _V) } } } solved(ui) +-true for i = 1, 2, ....s } Figure 4. The iterator solution \nby iteration. an output flow variable because one of its successors, the node M, does not follow it in \nthe depth-first order. Although V as computed by V can be used directly as input to node A, ~s flow \nvalue needs to be stored until node M is referenced during iteration, In the graph on the right, the \nnodes labelled V need variables because node M may not follow them directly in the depth-first oder. \nIn general, a node u requires a flow variable O(u) when u has more than one successor, or when u is \nthe predecessor of a node that requires a meet operation to compute its input flow value. Similarly has_meet \n(w) is true for a node w indicating that w requires a meet operation to compute its input flow value-when \nw is the successor of a node which has more than one successor, or when w has more than one predeces\u00adsor. \nIn the left graph above, Mneeds a meet operation to copy the flow variable of its predecessor. In the \nright graph, M needs a meet operation to combine the flow variables of its predecessors. Sharlit s approach \ndoes not rely on basic blocks as tradi\u00ad tional DFA does to reduce the amount of data-flow informa\u00ad tion \nstored in the flow graph. Traditional DFAs would have a flow vanable at the input and output of each \nnode or basic block. Instead, Sharlit needs only to keep an output flow vari\u00ad able at the end of a linear \nsequence of instructions. Consider two nodes u and w within such a linear sequence. Then u s unique successor \nis w and w s unique predecessor is u. After calling the flow function for u, _V contains the output flow \nvalue, which is equal to the input flow value of w. Hence, _V can be passed directly to the flow function \nof w as its input. Since no other nodes need the value of O(u), it can be elimi\u00ad nated. Nodes labelled \nV need flow variables and nodes labelled M 2.4 Exampie 2: iocai anaiysis need a meet operation or a copy \noperation to compute their input flow values, In the graph on the left, the node V needs Even though \nthe Sharlit example in Figure 3 saves space by eliminating many flow variables, it must still visit every \nIR instruction on every iteration, using a lot of time. Our second version of AVP shows how we can reduce \nthe number of nodes visited by adding path simplification to Figure 3. Many optimizers reduce the number \nof variables and nodes by using a flow graph where the nodes are basic blocks. These optimizers have \ntwo additional data-flow anal\u00adysis steps. Local analysis composes the effects of the instruc\u00adtion sequence \ninto one flow function for the basic block. Propagation computes the data-flow solution at each instruc\u00adtion \nfrom the solution for the basic block. These steps am usu\u00adally written in an ad hoc fashion. By adding \npath simplification rules to a Sharlit specifica\u00adtion, a compiler writer can get the same effect as local \nanaly\u00adsis from an efficiency viewpoint. In the following example, we show how rules can shrink the flow \ngraph so that it con\u00adtains only those nodes that form the boundaries of extended basic block. To add \nlocal analysis to Figun? 3, we add the code in Fig\u00adure 5. Part A declares a path function AV_bb thatrepresents \nthedata-flow effect of a sequence of instructions. In the flow function AV_bb,two localvariables, defs \nand kil1s,are  used in computing the effect of a path: Out = kills) (In udefs. The simplification \nrules compute defs andki11sas a path is being built. Two kinds of rules-create and absorb rules follow \nthe definition of AV_bb;ourthirdexamplewillshowmorekinds. Createrulesgenerateflowfunctions identityfortheempty \nbasicblockorempty path.Absorbrulesgrow thisempty pathby absorbing the flow function that immediate follows \n,,,,.,...................0.!,  ,!, . i AV_bb: local{ t 1,Expr_set defs; t Expr_set kills; 1$, A } \n,/ path { ,1 v -= kills; ,1 V += clefs; + }; ,1 .............................  .... simplifier (: \nAV_bb) create = { }; (b :AV_bb) absorb (1: AV_i.denti.ty) = { }; (b:AV_bb) absorb (l: AV_expression, \nN) = { Expr set*kill_set=&#38;_P ->ki.lls_set [N >dst] ; b->k~lls += *kill_set; b->defs . add (N->expr_no) \n; b->defs -= *ki-ll_set; ); (b: AV_bb) absorb (l: AV_kill, N) = { Expr set*kill_set=&#38;_P ->ki.lls_set \n[N->dst]; b->k~lls += *kill_set; b->defs -= *kill_set; }: (b:AV_bb) absorb (1: AV_bb) = { b->defs -= \nl->kills; b->defs += L >defs; b->kills += l->kills; }: Figure 5. The sharlit specification for available \nexpressions augmented with local analysis. thepath. We summarize what the path simplifier (Section 2.7) \ndoes for extended basic blocks in Figure 6. By applying a series of create and absorb rules, the simplifier \ncomputes instances of the flow function AV_bb that relates the flow value leaving node A to the value \nleaving the leaves. Thus the nodes B and C are no longer necessary and are eliminated for consideration \nby the iterator. For example, say the flow functions at node C and F are both of type AV_expres sion. \nTo find the path function from the exit of Ato the exit of F,the path simplifier uses the create rule \nto create a new path function. That path function absorbs the AV_expressionfunctionofC,usingthe rule \n(b:AV_bb) absorb(l:AV_expressionr N) When the code part of this rule executes, the variable bpoints \nto the path function AV_bb; the variable 1 to AV_expres \u00ads ion; and the variable Nto the node C.The code \nupdates the path function so that it represents the data-flow effect of the original path function composed \nwith the flow function of C. Another application of the above rule will compose the above path with the \nflow function of node F. Figure 6. Simplifying extended basic blocks. propagate(L) { Let L = [U1,U2,.... \nr.+] be a df-order of the CFG. _V &#38; new_value( ) fori=2tosdo{ if has_meet (ui) { Let {vl, Vz,,.., \nVZ} be predecessors of Ui meet (_V, [O(vl), O(vz), .... O(V@)]) 1 (in(ui)) (_V) if solved (ui) copy_value(LV,O(u,)) \nelse (flow(uJ)(-v) (out(u,)) (-v) ifO(ui)exists(ui) andnotsolved copy_value( (ui), V) ) ) Figure \n7. Propagation-computing solutions on the whole flow graph using the solutions from the reduced graph. \n 2,5 Propagation Path simplification builds a smaller list that consists only the nodes at the boundaries \nof extended basic blocks, such( as nodesA,D,EandFin Figure6.Itsetstheancestor of the leaves to the node \nthat heads their corresponding block. For nodes D, E and F, that node is A. It computes for each leaf \nu a new flow function new_flow (u) representing the data-flow eflfect from ancestor (u) to u. The reduced \nlist is given to iterate. The test for ancestor in iterate now has a purpose to call new_flow(u) instead \nof flow (u). Once iteration has found a solution to the smaller list, the routine propagate in Figure \n7 is called to compute the flow values of nodes in the larger flow graph. As propagate computes the flow \nvalues, it calls in(u) and out (u), action routines that can use the flow values at the input and output \nof u to perform optimization. The com\u00adpiler writer specify these action routines together with a flow \nfunction, as shown in Figure 8. 2.6 Exampie 3: intervai anaiysis The advantage of iteration, with or \nwithout local analysis, is that it works on all graphs, even irreducible graphs. But for reducible graphs, \nwe can avoid iteration, potentially speed\u00ad ing up DFAand allowing us to solve those DFAswhich may not \nconverge with iteration. The example of Section 2.4 uses path simplification to name: local { local variables \n 1 flow { .... } in { code that uses$ow value at etiry of node 1 out { code that usesfhw value at exit \nof node  ~(gure 8. Fiow function declaration with in and out functions. (1) (2) perform local analysis, \nbuilding a reduced graph with fewer nodes. We can use path simplification to perform interval analysis, \nbuilding a ~duced graph in which there are no loop back-edges. For an reducible flow graph, Sharlit can \nremove all back-edges, and the DFAproblem can be solved by mak\u00ading one pass of the reduced graph, followed \nby a propagation step. To remove a back-edge, path simplification summarizes the data-flow effect of \na loop and replaces the loop with a tree consisting of some of the nodes of the loop. The flow func\u00adtions \nof the nodes are replaced. This replacement occurs with the innermost loop, proceeds outwards, and ends \nwith the entire graph. For example, in graph of Figure 9(l), a tree replaces the innermost loop, whose \nheader is the node D. The result in Figure 9(2) has the single tree node highlighted. The node E is eliminatedl, \njust as the local analysis of Section 2.4 would. The flow function of F is replaced by a new flow function \nf which is equal to EF. The graph Figure 9(3) shows the result\u00ading graph after processing the loop headed \nby A. Here, the new flow functions are d. D(fDT and g.(BC+df)G. Replacing A sloop eliminated the back-edge \nentering into D -it sdata\u00adflow effect has been summarized into the flow function f. Finally in Figure \n9(4), the outermost interval headed by Source is replaced and that removes the back-edge entering A. \nIn the example, all the back-edges were removed and the DFAproblem can be solved by making one iteration \nover the much smaller, acyclic graph of Figure 9(4), followed by a propagation step that computes the \nsolution at all the other nodes from the solution at Aand D. To add interval analysis to Figure .5,we \nadd two types of rules-join rules and a star rule-shown in Figure 10. These rules tell Sharlit how to \ntake the meet of two alternative paths that share the same end-points; and how to take the Kleene-star \nof a path. The last rule is necessary for removing back-edges,  2.7 Eliminating fiow variabies Ii After \npath simplification in the examples of Section 2.4 and 1. The nodesareonlyina actuallymarkedandeliminated \nlaterbutweshoweliminatehereto step,themeliminated simplifyexplanation. the (3) (4) ourc a d % d= \nD(fD)* a.A(gAT Y SI g=(BC+df)G 1 Sink Sink 6 Figure 9. Replacing loops. (bl: AV_bb) join (b2: AV_bb) \n= { Express i.on_set suml, sum2; suml.uni.versal(_P->etable) ; sum2.universal (_P->etable); suml -= b2->defs; \n stlnll *= b2->lcills; sum2 -= bl->defs; sum2 *-b2->ki.lls; sum2 += bl->kills; bl->kills = wml; bl->kills \n*= sum2; bl->defs *= b2->defs;  }; (b:AV_bb) star ={ b >kills = b->defs; b->defs.clearo ; }: Figure \n10. Enhancing soiver to use interval analysis. cfa(G)/*ControLflow analysis*/ ( Let Gbethecontrol-flow \ngraph. Perform a depth-first traversal to discover the loop structure of G. The output are two lists: \nL which is a df-ordering of the G,andalistoflists [L.l, ....LEachlistisis adf-orderingof thenodeswithin \naloopbody, The lists are mutually exclusive-all pairwise intersections are empty. The lists satisfies \nthe condition depth (Z,l)> depth(L.J 2.,.2 depth (LJ. Alsocomputesfor eachnode utheloop_head (u), thenodewhich \nheadstheloopin whichu belongs. ) simplify(L, [Ll, ....LxI) ( Let Land [Ll, ..., LJ betheresultofcontrol-flow \nanalysis. fori=l toK simplify_loop(Li ) return reduce(L) } reduce(L) { LstL= [U*,u~, .... u,] beareverse \npost-ordering of the flow graph Let M bean em~ty list ofnodes. fori=sto2( if uiisa Ioopheader (Prepend \nuito M.) else if keep(ui) {Prepend uito M) else continue if ancestor (ui)#ui { keep (ancestor (ui)) + \ntrue ) else (keep(v) +true forallpredecessors vof brepend UI to M return M  ) simplify_loop( L) { let.L= \n[Ul,Ul]adf-orderof Z+,...,aloop. fori=lto1{ Letf = flow(u) /*Classify predecessors / Let V = {V1,02, \n..., v.} be predecessors such that vi = loop_head (vJ and Vj+ Uiis a back-edge for ] = 1,2, ....a. Let \n{Wl, Wz,.... w~} be the other predecessors of Ui / C;fm~t~t~ path expression through back-edges*/ qj+ \nevai(vp Ui) forj = 1,2,...)a q+--((q1+q2+.,, + q.) f) / form path expression / ) else q * fld / the identity \nflow function / / Compute thepath expression from a dominator to u*/ Call evai(wj) for j = 1,2, ,.., \nb / Fiid common ancestor / if ancestor (u+) = ancestor (r@ = ... = ancestor (wJ { Let a = ancestor (W.) \nLet pi = new_flow(w.) for j = 1,2,. ...b p$S-(p1+p2 +... + pb j /* form path expression */ ~ ) else if \nwe can t find a then P-+-fJ. / Computq new_tlow / pepq ifp%fl { ancestor (Ui)+-a new_flow (ui) + p \n ) else{ keep (ui) = true keep(vj) = true for j = 1, 2, .... a 1  1} eval( u) { if ancestor(u) # u \n{ path_compress(u ) return new_flow (u) ]else return fI~ ) eval(u, h) { if ancestor(u) # u path_compress( \nu) ifu = h( returnfr~) if ancestor(u) = h ( return new_flow (u) ) return /l 1 path_compress( u) { if \nancestor (ancestor(u)) # u( path_compress() ancestor(u)Letf= new_flow(ancestor (u)) . flOw(U) iff+fl \n{ nzw.flow (u) +--f ancestor (u) +-ancestor (ancestor(u)) ) ) )  Figure 11. Pseudo-code for Shariit \ns simplifier. Figure 12. Pseudo-code for Shariit s simplifier (continued). Section 2.6, Sharlit needs \nto store variables at the nodes that 3.1 Path simplification rules are ancestors to others. This is necessary \nbecause iterate the flow values at the exit of an ancestor may be required as input to many other nodes. \n 3 The Path Simplifier Sharlit s simplifier is shown in Figure 11 and Figure 12, and is based on Tarjan \ns algorithm for the single-source path problem. The algorithm computes for each node a path expression \nrepresenting all the paths from the source to {hat node. Sharlit s path simplifier uses this path expression \ncom\u00ad putation to guide the composition, meet and taking the Kleene-star of flow functions, In this section, \nwe discuss the four key ideas behind the operation of the simplifier. We explain the simplifier in the \ncontext of computing path expressions; Section 3.1 will show how that computation cor\u00ad responds to calls \nof the rules provided in a Sharlit specifica\u00ad tion, Fkst, changing the graph directly to replace loops \nis expensive. Instead a tree edge (a, u) is represented by set\u00ad ting ancestor(u) to a. In Figure 9(2) \nfor example, ancestor(E) and ancestor (F) are both D. Also, the new flow functions computed when replacing \nthe loop are stored in new_flow (u). Notice that ancestor only allows traversals, up the tree from the \nleaves upward, not from the root down\u00ad wards. Second, the simplifier builds the ancestor tree by visiting \neach loop in depth-first order, and visits loops from the inner\u00admost to the outermost, This ensures that \nwhen it comes time to compute the path expression for a node u, the simplifier has already computed the \npath expressions for the predeces\u00adsors of u within the loop. For example, in Figure 9, the visit order \nis [E, F] for the loop headed by D, [B, C,D,G] for the loop headed by A, and [A, Sink] for the outermost \ninterval headed by Source. Third, tree nodes those nodes u whose ancestor is not u have their path expressions \ncomputed on demand. In Figure 9(2) for example, the path expression between A and f is computed during \ncalls to eval when the simplifier is visit\u00ading G. A chronology of the order in which path expressions \nare computed follows: path expression E for the path from Dto E. . EF for path from D to F. . B for \npath from A to B.  BC for path from A to C. When visiting D, the simplifier calls eval(F, D) which returns \nf=EF, Hence, the path expression computed fc}rD is d= D(fD~. When visiting G, the simplifier calls eval(F) \nwhich yields df, the path expression from A to F. Hence the path expression for G is g=(BC+df)G. . A(gA)* \nfor the path from Source toA, witha callof eval(G, A). . Finally A(gA~gSink for the node Sink, with \na call of eval(G).  Fourth, when forming path expression the simplifier can fail because it is unable \nto find a sequence of simplification rules that can simplify the path expression into a single flow function. \nWhen that occurs, the result of the path expression formation is fl, When that occurs for a node u, its \nancestor remains u, and u becomes eligible to become ancestors of other nodes (See Section 3,2). Path \nsimplification builds path expressions and in the pro\u00adcess computes flow functions using composition, \njoin, and taking the Kleene-star of other flow functions. These opera\u00adtions are directed by the compiler \nwriter with path simplifi\u00ad cation rules. An obvious way to tell the analyzer how to combine flow functions \nis to use the times table approach. For example, when the analyzer needs to compose two flow functions, \nit would index a special table with the two flow functions to find what actions are necessary to do the \ncomposition. Although tables are simple, specifying them directly is tedious: A complete specification \nof the composition table for M flow functions would require M2 entries. Instead Sharlit can generate \na complete table using only M+1 rules and a new kind of flow function: a path function. During iteration \n(the solution of the DFA problem), a path function behaves just like any other flow function, except \nthat it represents the data-flow effect of a path com\u00ad puted by the path simplifier. During path simplification, \na path function is treated differently for efficiency reasons they are modified directly or reused. Below \nwe show the five kinds of rules and describe how they work conceptually. Each rule has a code part that \nwhen executed computes a new path function by modifying the path function specified in the left-hand \nside of the rule. 1. p create: Create a new path p whose data-flow effect is the identity. This rule \nis used in the situations depicted in Figure 13. The analyzer inserts identify flow func\u00adtions with the \nintention of extending that path (see below). This insertion does not change the data-flow problem the \nsolution is the same as the solution with\u00adout the insertion of p. 2. p create f: Create a new path p \nwhose data-floweffect is thesame astheflowfunctionf (see Figure 14), Again this replacement will not \nchange the solution of the data-flow problem. This replacement is used by the ana\u00adlyzer to create paths \nin the graph that begins with nodes with flow function f, 3. p absorbs f: Extend a path p so that it \nhas the same data-flow effect as the old p composed with f, that is modify p so that it represents the \ndata-flow effect p. f  (Figure 15). 4. p sta~ Replace the path p with a path that represents the data-flow \neffect of p . This rule, together with other rules, is used to remove back-edges (See Figure 16). 5. \npl join p2: Replace the path pl with a path that repre\u00ad  sents the data-flow effect of the path pl \n+p2 (Figure 17). Relating this back to Figure 11, we can see that the path com\u00adputation steps are application \nof these rules, So computing a path expression, such as q + ( (ql + q2+ ... + qJ H means applying a sequence \nof path simplification rules. 3.2 Incomplete path simplification Sharlit s path simplification differs \nfrom Tarjan s algorithm in how it handles irreducibilitics and incomplete rule sets. For both situations, \npath simplification can still reduce signifi\u00adcant parts of the graph, although it may not be able to \nremove all back-edges. Tarjan s algorithm works only on reducible Figure 13. Appiying the p create ruie. \n4p=f Figure 14. Appiying the p createf ruie  Tfp.g P=13f + 4 f 4 Figure 15. Applying the p absorb \n~ ruie =/? + + Figure 16. Appiy the p star rule The node iabeiied Id is an identity node. They are not \nactually present in the graph but are present for expos\u00aditory reasons oniy. + + Figure 17. Appiying \nthe pl join p2\u00b0 rule This rule oniy applies when the two paths share common endpoints. This condition \nis enforce in the path simplifier because the ancestors of the nodes must be the same flow graphs. We \ncan characterize simplify by saying that it tries to find a flow function from a dominator of u whose \npath to u can be simplified into one flow function. Simplification can fail to find such a flow function \nwhen the simplification rules are incomplete-they cannot simplify some combination of flow functions-or \nwhen the graph is irreducible. When there are no meet rules but the rules are otherwise complete, this \nfarthest dominator will be the entry node of the extended basic block. For irreducible graphs, simplify \nfails because a common ancestor cannot be found (see Figure 11). Even then, the simplifier can still \nsucceed on the reducible parts of the graph. The simplifier degrades gracefully reverting to local analvsis \nwhere it can, or even to iust ~artial local analvsis (onl~ parts of extended basic block: are simplified) \nwhen the rules are extremely incomplete. This gradual degradation makes it easy to test and develop DFA \nincrementally. 3.3 Using ruies in path computation The procedures simplify eval, and path_compress build \npath expressions abstractly using operators ., +, and *. We show how Sharlit turns this process of computing \npath expressions into invocations of rules. We define three func\u00adtions corresponding to the path operators: \n1. compose(f, g): a) If f = f~ or g = f~ then return f~ to indicate failure. b) If f = fI~ then create \na new path function representing g, that is, with the same data-flow effect as g. This cre\u00adation can \noccur by either applying the rules p create g; or the sequence-p create f followed by p absorb g. Ifexactlyoneofthesearepossible \npossibilitiesby therulesgiveninthespecification,p; oth\u00ad thenreturn erwise return c)Iff = p for some \npath function p. Then apply p absorb g if possible; otherwise go to the next case. d)Otherwise, return \ncompose(compose(fI~ ,f),g) but only if the rule sequence to do it exists. 2. join(pl, p2, .... pa): a) \nIfa = 1 returnpl. b) Make a copy of the first path by doing p-compose (fI~, pi), then apply p join p2, \n.... p join P.. etUrn P f Uccessful, therwlse etUrn fId c 3. star(p): a) Invoke the rule p join, if the \nrule is present in the spec\u00adification, otherwise return fl With these functions, we can interpret the \npath computation (pJ+p2+ ... + pa) f as compose (join (pljP2J.f),and ...pa). pq as compose (p, star(q)). \nThe above functions are generated as table lookups. For example, compose would index into a composition \ntable to determine what action to take. Because rule sets are small, Sharlit generates these tables by \nexhaustive search of the rule sequences.  3.4 Absorb ruies save space Path functions computed by the \nanalyzers can potentially take a lot of storage. For example, the path function AV bbin Figure5hastwosetsjust \nofexpressions.Thus,aswe m=stbe carefultoretain,  aboutwhichflowvariableswe mustbecare\u00ad (A) (B) (c) (D) \np=A p=AB B c 9 c8 Figure 18. Reusing flow functions in sequences of node ful about which path functions \nto retain. The traditional way of dealing with this problem in traditional optimizers is to group instructions \ninto basic block. Therefore only one path function is required for each basic block. In Sharlit, the \nprimary mechanism to save space is absorb rules coupled with reference counting of flow func\u00adtions. To \nsee how absorption saves on space, see Figure 18. Assume that we have the rules: p create, p absorb p, \np absorb A, p absorb B, and p absorb C, In Figure 18(A) we apply the rules p crest e, followed by p absorb \nA to compute the path function corresponding to the path expression A. In (B), we apply p absorb B giving \nthe path expression AB.We can reuse p because we know that the node A won t need a path func\u00adtion because \nit will be eliminated by reduce. However, in Fig\u00adure 18(B) this cannot be done because the node B (now \ncorresponding to path expression AB) cannot be reu!$ed because its other successor will need it. Hence, \nthe path expression AB must be copied, as shown in Figure 18(C), Absorption continues in Figure 18(D) \nand reuses the cop~r of AB. To decide when to reuse a path function, we keep a ref\u00aderence count a count \nof the successors for each path func\u00adtion, When composing a path function p with its successor, and p \ns count is one, we can use it directly in an absorb rule; if its count is not one, then it must be copied, \nits count decre\u00admented, and its copy is used directly in absorption. When we add a successor to a path \nfunction (as when path simplifica\u00adtion connect nodes to their ancestors), its count must be incr\u00ademented. \nThere are two exceptions to the reference counting. Path functions associated with loop headers are treated \nas though they have infinite reference counts, This is necessary to ensure that they are never eliminated \nbecause they aw never eliminated by reduce. Identity flow functions are treats simi\u00adlarly, But because \nidentity flow functions have no local vari\u00adables they can be allocated statically and copies do not take \nup more space.  4 Sparse Data-flow Evaluation Graphs Besides being able to do local analysis and interval \nanal\u00adysis, path simplification can also build sparse data-flow eval\u00aduation graph (SDFEG) [4] for reducible \nflow graphs to solve sparse data-flow problems efficiently. In the context of reac\u00adhingdefinitions problem, \na SDFEG corresponds to a 55A graph for one variable. But unlike SSA, SDFEGScan be applied to other data-flow \nproblems. To compute an SDFEG,it is necessary to find paths which consists solely of identity flow functions, \nand to prune away all those flow-graph nodes which are connected to nodes in the SDFEGnodes through identity \npaths. Sharlit can identify such paths with the path simplification rules in Figure 19. In that specification \nthe keywords identity tells Sharlit the flow function or path function is the identity flow function, \nFigure 20 compares an SDFEG computed using Sharlit and that of Choi s. Sharlit s SDFEGdiffers in that \nwe use nodes to represent some of the edges in the Choi s SDFEG We indicate those nodes with dashed circles. \nThese nodes are required because the only way Sharlit can modify a node u s edges in the graph is to \nreplace all of u s edges with one edge from u s ancestor to u. Therefore, at the node F, the only way \nto con\u00adnectFtoitsSDFEGpredecessor isindirectly,asancestorsofC and E. The key point that makes SDFEGefficient \n isitcanelimi\u00adnatesomany more flow variables than our current scheme, The current version of Sharlit \ns simplifier is capable of find\u00ading the paths composed of identity flow function. However, the current \nversion of propagate and reduce needs to be modified in order for us to eliminate the flow variables \nat the ends of these identity paths. We continue to work on integrat\u00ading SDFEGinto Sharlit. 5 Conclusion \nWe have found Sharlit to be a useful tool. Its facilities are ideal for describing DFAs,from simple iterative \nones, to ones that use symbolic analysis (recurrence detection) to ones that use interval analysis. Because \nof its extensibility it is ideal for prototyping optimizations and to experiment with different internal \nrepresentations. We have used Sharlit to build an optimizer as part of the Stanford SUIF compiler project \n[12]. Our results (Figure 21) show that this optimizer can optimize large, real programs such as the \nPERFECTclub [3] as effectively as commercial compilers. Our optimizer uses Sharlit to implement all its \ninternal phases: 1. Value numbering. 2. Reaching definitions. 3. Elimination of partial redundancy[7]: \nGlobal common subexpression elimination, strength reduction, back\u00adward code motion out of loops, 4. \nGlobal register allocation.  .5.Constant propagation and copy propagation. ... Defs: flOW {... } Idflath: \npath Identity; ... simplifier Id_path create = {}; Id~ath absorb Identity = {}; Id_path joinId_path = \n{}; Id~ath star= {) end Figure 19. Rules for doing SDFEG. dExit Figure 20. Example of Sharlit s version \n$DFEG. ip represents an Identity path. SUIF-Cl MIPS-02 SUIF/MIPS LG 97 106 0.82 LW 1437 1405 1.02 NA \n421 377 1.12 SD 88 105 0.84 SR 1270 1253 1.01 m? 243 232 1.05 TI 259 244 1.06 Figure 21. PERFECT club \nexecution results in seconds from runs on a Decstation 5000 with version 1.31 of the Mips compiler. 6. \nLiveness analysis and Dead code elimination. 7. Recurrence detection[2].  Our experiences suggest \nthatSharlitis a good and prac\u00adtical. Constructing the optimizer using Sharlit has required muchlesseffortandhasresultedinacompiler \nthatissimpler, easier to extend, and more versatile. 6 Acknowledgments This research has been supported \nby DARPAunder contract NOO039-91-C-0138.We would like to thank Rob French,Mon\u00adica Lam, Bob Wilson, and \nMichael Wolf for their helpful com\u00adments. 7 References 1. A.V.Aho, R.Sethi,J.D.Unman, Compilers,Principles, \nTechniques,andTools.Addison-Wesle~ 1986. 2, Z. Ammarguellat and W. L. Harrison 111,Automatic recognition \nof induction variables and ~currence relationsby abstractinterpretation.InProceedingsofi!he ACMSXGPLAN \n90 ConferenceonProgrammingLanguage DesignandImplementation, 3. M.Berryetal,ThePERFECTClubBenchmarks:effective \nperformance evaluation of supercomputers. Technici~l Report UIUCSRD Rep. No. 827, University of Illinois \nUrbana-Champaign, 1989. 4. J,Choi, R. Cytron, and J.Ferrante,Automatic construction of sparse data flow \nevaluation graphs, 1n  POPL1991. 5. F,Chow, A PortabkMachine-IndependentGlobal Optimizer,PhD thesis, \nStanford University 1983 6. M.S. Hecht and J.D. Unman. A simple algorithm for global data-flow analysis \nprograms, In SMMJ. Computing,(4):519-532. 7. E. Morel and C. Renvoise, Global optimization by suppression \nof partial redundancies, In Communications of theACM.(22):96-103 8. M.Sharir. Structuralanalysis:anewapproachtoflc~w \nanalysis in optimizing compilers . ComputerLanguages5 (1980), 141-153. 9. B. Stroustrup, TheC++ ProgrammingLunguage,Addison- \n Wesley Reading, Mass. 10, R, E.Tarjan. Testing flow graph wducibility .]. o/ Comput.andSyst.Sci. (91974),355-365. \n 11. R,E,Tarjan,Fastalgorithms for solving Pathproblems. @WJfd ojthe ACM,3(28):591-642,July 1981. 12 \nS.Tjiang,M.E.Wolf,M.S.Lam,K.L.Pieper,andJ.I.. Hennessy Integrating scalar optimization and parallelization, \nIn 4th Workshopon Languagesand Compilersfor ParallelComputing,Aug 1991. 13 J.D.Unman. Fastalgorithmsfortheeliminationof \ncommon subexpressions . Acts hf. 2, 3(July1973),1191\u00ad 213.  \n\t\t\t", "proc_id": "143095", "abstract": "<p>A complex and time-consuming function of a modern compiler is global optimization. Unlike other functions of a compiler such as parsing and code generation which examine only one statement or one basic block at a time, optimizers are much larger in scope, examining and changing large portions of a program all at once. The larger scope means optimizers must perform many program transformations. Each of these transformations makes its own particular demands on the internal representation of programs; each can interact with and depend on the others in different ways. This makes optimizers large and complex.</p><p>Despite their complexity, few tools exist to help in building optimizers. This is in stark contrast with other parts of the compiler where years of experience have culminated in tools with which these other parts can be constructed easily. For example, parser generators are used to build front-ends, and peephole optimizers and tree matchers are used to build code generators.</p><p>This paper presents Sharlit, a tool to support the construction of modular and extensible global optimizers. We will show how Sharlit helps in constructing data-flow analyzers and the transformations that use data-flow analysis information, both are major components of any optimizer.</p><p>Sharlit is implemented in C++ and uses C++ in the same way that YACC uses C. Thus we assume the reader has some familiarity with C++[9].</p>", "authors": [{"name": "Steven W. K. Tjiang", "author_profile_id": "81100257188", "affiliation": "", "person_id": "PP31099364", "email_address": "", "orcid_id": ""}, {"name": "John L. Hennessy", "author_profile_id": "81100207767", "affiliation": "", "person_id": "PP77026887", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143120", "year": "1992", "article_id": "143120", "conference": "PLDI", "title": "Sharlit&#8212;a tool for building optimizers", "url": "http://dl.acm.org/citation.cfm?id=143120"}