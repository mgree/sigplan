{"article_publication_date": "07-01-1992", "fulltext": "\n How to Analyze Large Programs Efficiently and Informatively Dhananjay M. Dhamdhere* Barry K. Resent \nF. Kenneth Zadeck$ dmd @ kailash.ernet .in bkr @ watson.ibm.com fkz @ cs.brown.edu Elimination of partial \nredundancies is a powerful optimiza\u00adtion that has been implemented in at least three important production \ncompilers and has inspired several simdar opti\u00admization. The global data ffow analysis that supports \nthis family of optimization includes some bidirectional prob-Jems. (A bidirectional problem is one in \nwhich the global inform ation at each basic block depends on both control flow predecessors and control \nflow successors.) This paper contributes two ways to simplify and expedite the analysis, especially for \nlarge programs. e For each global data flow question, we examine only the places in the program where \nthe question might have an answer different from a trivial default answer. In a large program, we may \nexamine only a small fraction of the places conventional algorithms would examine. e We reduce the relevant \nbidirectional problems to sim\u00adpler unidirectional problems. These bidirectional prob\u00adlems can be solved \nby applying a quick correction to a unidirectional approm m ation. Introduction Elimination of partial \nredundancies (EPR) [MR79] is a powerful optimization that has been implemented in at least three import \nant production compilers (COMPASS Compiler Engine, MIPS, and IBM PL.8) and has inspired several similar \noptimization [Cho88, JD82 a, JD82b]. While these optimizat ions have been useful in practice, they account \nfor a significant portion of the time and space devoted to optimization. This paper contributes two ways \nto simplify and expedite the data flow analysis that supports this family of optimizations, which we \ncall EPR-lzke. Dept. of Computer Science and Engineering, Indian Institute of Technology, Powai, Bombay \n400076 INDIA t Mathematical Sciences Dept., IBM Research Division, P,O. Box 218, Yorktown Heights, NY \n10598 USA i Computer Science Dept., P.O. Box 1910, Brown University, Providence, RI 02912 USA This author \nwas supported in part by the Office of Naval Research and the Defense Advanced Research Projects Agency \n(under Contracts NOO014-83-K-0146 and ARPA order 6320, and NOOO14-91-J-4o5 2 and ARPA Order 8225) and \nin part by the National Science Foundation under grant CCR-9015988. Permission to copy without fee all \nor part of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, raquires a fee and/or specific parmisaion. ACM SIGPLAN 92 PLDi-6/92/CA  @l1992 ACM \n0.8979 ~-476 .7/92 /0006/02J 2...$1.50 For each global data flow question, we examine only the places \nin the program where the question might have an answer different from a trivial default answer. For reasons \nexplained later in this section, the technique is called slotwise analysis and is also useful for some \nless powerful but more common optimization. For EPR-like optimization, where most expressions are not \nredundant and move only a short dist ante, the potential savings from using a sparse method such as slotwise \nanalysis can be substantial. We show how to reduce the bidirectional problems posed by the various EPR-like \noptimization to simpler unidirectional problems. (A bidirectional problem is one in which the global \ninformation at each basic block depends on both control flow predecessors and control flow successors. \n) These bidirectional problems can be solved by first solving a unidirectional approximation and then \napplying a quick correction. These contributions are described below and more fully ex\u00adplained in the \nfollowing sections, which also discuss relevant previous work in detail. Standard algorithms for data \nflow analysis [ASU86] pro\u00adcess a program of size n in 0(TZ2) bit-vector stepa (worst case) or O(n) bit-vector \nsteps (typical cases). In the most natural formulations of many problems, the lengths of the bit vectors \nare proportional to the size of the program and each bit\u00advector step takes O(n) time. For example, a \nprogram of size n has about n candidates for redundancy elimination. Each candidate deserves its own \nslot in a bit vector. (Slots are also known as components or elements or positions .) With long bit vectors, \nstandard algorithms are O(ns ) at worst and 0(n2) at best. The algorithm presented here is 0(n2 ) at \nworst. For some problems, our algorithm is effectively O(n) despite the fact that n vectors of length \nn would have a total of n2 slots. More precisely, the time devoted to the i-th slot is O(p, ), where \np; < n is the size of the part of the program visited while computing the i-th slot of each of the bit \nvectors at some of the basic blocks in the program. Each slot is computed only in its own relevant part \nof the program, Elsewhere, it is known to take on a given default value. Our method is thus a member \nof the broad family of sparse algorithms. Let p be the average of pi as i varies over n slots. Then the \ntotal time is O(pn), with p = n at worst. For some problems, p is typically a small fraction of n. In \nparticular, for EPR-like problems, experience with the COMPASSCompiler Engine suggests that bit vectors \ntypically have the default value for about 99% of the bits.1 On a machine with 32-bit words, treating \neach slot separately will be faster than standard bit-vector methods -We thank Bob Morgan (personal communication, \nNovember 7, 1990) for this information. 212 whenever 32p < n, and in some other cases as well. Details \nare given in ~2, which also discusses the few other sparse algorithms in data flow analysis. one important \napplication is treated in detail here: placement analysis to support elimination of partial redun\u00addancies. \n(Moving invariant code out of loops is an important special case of this optimization. ) The standard \nequations for this problem are bidirectional: along a control flow edge (x, ~), some of the information \nflows forward (from z toy) while some of it flows backward (from g to o), Most efficient analysis algorithms \ndeal only with unidirectional flow, as in the forward-only flow of ordinary redundancy elimination. We \nshow how to perform placement analysis by a unidirec\u00adtional approximation followed by a quick correction. \nThis unidirectional approximation uses our slotwise analysis. Solving an arbitrary bidirectional problem \nwould re\u00adquire multiple corrections, but placement analysis and the other EPR-like problems are not arbitrary. \nThe complicated equations have some simple properties that imply quick con\u00advergence, if one is careful \nabout where to start. Details are in $3, which is formulated to be read either from a bit-vector viewpoint \nor from a general lattice-theoretic viewpoint. An example is worked in $4. The techniques applied in \n$4 to one particular formulation of placement analysis are equally applicable to other formulations and \nto the other EPR-like optimizations [Cho88, JD82a, JD82b]. Finally, $5 explains how our method of placement \nanalysis can be exploited by a compiler that detects second-order effects: elimination of partial redundancies \ninvolving one expression can create new opportunities involving other expressions. Classical Bit-Vector \nProblems Being a sparse algorithm, slotwise analysis for each bit\u00advector slot visits only the nodes \nwhere the bit in that one slot might be different from a trivial default value. Depending on the problem \nin question, the default is either Oor 1, To handle both cases simultaneously, we review three ways to \nclassify problems. 1. Information may flow either forward or backward along edges. In each classical \nproblem, the flow is in only one of these possible directions. Section 2.1 treats forward flow in detail. \nThe backward case is an easy transformation of the forward case, as $2.2 explains. 2. At a join node \n(in a forward problem) or at a branch node (in a backward problem), previous algorithms need to merge \ntogether information propagated to the node along several edges. The merge operation (de\u00adnoted 11) is \neither bitwise AND (denoted A) or bitwise OR (denoted V). Let J_ be the bit value that controls the merge: \nfor any x, cfl~ = -L, Thus, 1 = Oif n is A but 1 = 1if n is V. In both cases, let T be the other bit \nvalue. The merge rule defines a partial ordering of the bit vectors, with a G y whenever each slot with \nT inz hasT iny. 3. At the Entry node (in a forward problem) or at the Exit node (in a backward problem), \neach slot has a given dejazdt value. For example, the problem of availableex\u00adpressions has the default \nEntryInfo = O = 1 because nothing is available at Entry.2 On the other hand,  2This standard assumption \nis adopted here for ease of pre\u00adsentation but is not important to our method. More extensive optimization \nis sometimes possible with other assumptions. For the problem of reaching definitions has the default \nEntryInfo = O = T because nothing reaches Entry. The same bit O has different significance in these two \nproblems because the merge operators are different, 2.1 Forward Problems Associated with each edge (z, \ny) is a flow finction f = Flow(a, y) that tells how to transform a bit vector ( at z to a bit vector \nq at y when control flows from z to y. The equations to be solved are Info(Entry) = EntryInfo and, for \neach node y # Entry, The desired solution to these equations is the one that could be obtained by iterating \nfrom an initial guess of all T bits. By solving for each slot in Info separately, we simplify the flow \nfunctions. There are only four ways to map from {O, 1} to {O, i}, and only three of them make sense in \ndata flow analysis. Along any edge (z, g), the flow function is one of those displayed in Figure 1. For \nexample, consider function behavior comment START OH 1++1 I constant at 1 STOP 0%0 constant at O 1!+0 \nPROPAGATE OH O lH1 paases erg. along Figure 1. Possible flow functions on a single bit. whether an \nexpression E is available on entry to a basic block. The code within the block x may STARTavailability \nof E by computing E and not assigning to any operand of E thereafter. Otherwise, x may STOP availability \nof E by assigning to an operand of E or may just PROPAGATE to y whatever was true on entry to x. With \nonly three possibilities to consider, we can examine the flow functions to help decide the order in which \nto visit nodes. Standard algorithms, on the other hand, consider n slots at once and cannot enumerate \n3Wpossibilities, In order to treat problems with 1 = O and problems with 1 = i simultaneously, it is \nconvenient to stipulate that LOWERis whichever of START,STOPhas value 1 and RAISEis whichever of START,STOPhas \nvalue T. Edges are considered to be PROPAGATEedges by default. Local analysis for the slot of current \ninterest flags the exceptional (RAISE and LOWER) edges and puts them in two lists. At the end of global \nanalysis for the current slot, these lists are traversed and the flags are reset, so as to be ready for \nlocal analysis of the next slot. Similar resetting is done for other information. In particular, the \nglobal bit of information for each basic example, an expression whose only variables are local temporaries \nmay be considered available at Entry. Without int erprocedural information, on the other hand, the standard \nassumption is defi\u00adnit ely needed for any expression whose only variables are globals or parameters. \nblock defaults to the given value lh!ry~n~o associated with the Entry node. This value may be either \nT or 1, but we assume it is the same for each slot. The general case is easily handled by applying our \nmethod twice, once for slots with EntryInfo = T (high information) and once for slots with EntryInfo \n= 1 (low information). 2.1.1 High Information at Entry Initialize the set NL of visited nodes to hold \nall targets of LOWERedges. Initialize a worklist to hold these same nodes. Whenever a node is taken off \nthe worklist, every successor z reached by a PROPAGATEedge is examined. If z is not already in IVl, then \nz is put in N1 and put on the wo,rklist. Let NObe the full set of nodes. Then .Info(z) = T for z ~ NO \n N1 and Injo(z) = 1 for x E N1. Thus all the Info bits have been determined at cost 0( IINI1]), where \nthe eztended size II...II of a set of nodes counts edges that touch the nodes as well as the nodes themselves. \n2.1.2 Low Information at Entry Initializethefirstsetfil ofvisitednodestoholdalltargetsof RAISEedges. \nInitialize a worklist to hold these same nodes. Whenever a node is taken off the worklist, every successor \nz reached by a PROPAGATEedge is examined. If z iw not already in N1, then z is put in N1 and put on the \nworklist, Let NObe the full set of nodes. We already know that Info(z) = 1 for z E No N1, so there is \nno need to examine these nodes. Nodes in N1 might have either 1 or T. These nodes are classified by a \nsecond worklist search. Initialize the second set Nz of visited nodes to hold every node y E N1 such \nthat, for some inedge e = (z, ~), either e LOWERS information or ~ @ N1 and e does not, RAISE information. \nInitialize a worklist to hold these same nodes y. Whenever a node is taken off the worklist, every successor \nz reached by a PROPAGATE edge is examined. If z is in i~l but is not already in N2, then z is put in \nN2 and put on the worklist. Once the second search is complete, we know that Info(z) = T forzENI N2andInfo(z) \n=1foraEN2.Thusallthe Inj o bits have been determined at cost 0( I]NLII), with Slbout twice as much work \nas in the easier case of high information at entry,  2.2 Backward Problems With both forward and backward \nproblems, it is customary to say that the global information at a basic block is what is known on entry \nto the block, so the flow function associated with an edge from z to y is determined by the code inside \nz and has nothing to do with the code inside y. After the flow functions have been found, backward problems \nare solved by the mirror image of the lmethod for forward problems. Whenever a node is taken off the \nworklist, its predecessors (rather than successors) are examined. The Exit node plays the role of Entry. \nThe given value 17zit,~n~o may be either T or 1. Just as with forward problems, T needs one search while \n1 needs two. 2.3 Discussion After dkcussing proper accounting for lengths of bit vectors when analyzing \nvarious rdgorithms for live (also known as upwards-exposed ) variables, Kou [Kou77] proposed that livevariablesbefound \nby the backwardsversionofthesim\u00adple worklist algorithm in $2.1.1. Apart from more elaborate (but still \nquadratic) algorithms oriented toward incremental analysis [MR90, Zad84], Kou s algorithm has had little \ninfluence. Standard algorithms are effectively quadratic in practice, and the 32-way parallelism of standard \nbit vector operations is still attractive, even when the vectors are long. The way compilers use information \nabout available expressions differs in three ways from the way they use infor\u00admation about live variables \nfor tasks like register allocation: 1. The information in any one slot can be computed, used, and discarded \nbefore computing the inform ation in any other slot. To eliminate redundant computations of an expression \nE, we only need availabllit y information for E. To build an interference graph for register alloca\u00adtion, \non the other hand, we need liveness information for all variables. 2. The information in many of the \nslots is the default bit value at many of the basic blocks in a large program. Many expressions E are \nunavailable throughout much of the program. 3 The default value is -L. The information at Entry (since \nthis is a forward problem) is O in every slot, and O is the cent rolling value for the merge operation. \nDifferences 1 and 2 make a slotwise approach much more attractive, while difference 3 makes the previous \nslotwise approach [Kou77] inapplicable. Our new algorithm in $2.1.2 handles difference 3. Difference \n1 has the interesting con\u00adsequence that using the information for one expression may improve the information \nfor another expression. Details are in $5. The following subsections compare slotwise analysis with two \nother nontraditional approaches. 2.3.1 Incremental Analysis Incremental analysis has a focus quite unlike \nthat of slotwise (or any other sparse) analysis. An incremental algorithm assumes that the data flow \ninformation (and perhaps some auxiliary information ) has already been computed every\u00adwhere. It tries \nto respond to a change in the input with work related more to the corresponding change in the output \nthan to the size of the whole problem. A sparse algorithm, on the other hand, has no previous solution \nof the problem to update: it starts from scratch. 2.3.2 Building Sparse Graphs Choi, Cytron, and Ferrante \n[CCF91] generalized static single assignment form [CFR+91] to derive a sparse representation of any given \nproblem. The original graph is replaced by a collection of smaller graphs such that many of the flow \nfunctions in each graph are either constants or the identity function. The combined cost of constructing \nall of the sparse graphs and solving the problem separately on each of them may be less than the cost \nof solving the problem directly. Building sparse graphs is much more complicated than slotwise analysis \nand is asymptotically slower. When an analytic mistake inherited from [CFR+ 89] is cor\u00adrected [CFR+ 91], \nthe worst-case complexity for constructing each sparse graph is O(ns). Slotwise analysis is o(nz ) for \nsolving the entire problem. 3 Most bit-vector problems are more amenable to either slotwise or traditional \nanalysis than to building sparse graphs. One important potential advantage of sparse analysis, however, \nis its algebraic generality. The information at each 3The pessimism in worst-case bounds is more severe \nfor build\u00ading sparse graphs than for slotwise analysis, so the real difference may be less than the worst-case \nbounds suggest. 214 Info(v) = f (y) n B(y) (1) Ent7yInfo if y = Entry F(y) = n [~o~w~~ow(z,v)](~~fo(z)) \nif y#Entry (2) { z G F%ed(y) EzitInfo if y = Exit B(y) =f n [13add h(y,~)]( InfO(~)) if Y # Exit. (3) \n{ .s e Succ(y) Figure 2. The information at y is the merge of auxiliary information propagated forward \nand backward TO y. slot in a vector is not required to be a bit. For data flow analysis problems like \ndegrees of availability y [Ros8 1], where bit vectors are inappropriate but slots can be partitioned \nto yield many identity or constant flow functions, it is likely that building sparse graphs would be \nuseful. 3 Bidirectional Problems As defined here, bidirectional problems are a natural gener\u00adaliz ation \nof the classical unidirectional problems. Section 4 shows that this generalization covers some important \nprac\u00adtical problems. As with any formal definition, however, the possibility y remains that an intuitively \nbidirectional problem may not fit the definition. Indeed, one such problem is considered briefly at the \nend of this section. Our formal definition is an appropriate one, and it would be futile to agonize over \nthe appropriate one. Notations like n or L in this section may be read in two ways: 1. The information \nassigned to each node is a pair of bits, and not ations from $2 are overloaded in the natural way. The \nBoolean operations applied by a flow function may A or V the argument with local information, but may \nnot negate the argument. This reading covers placement analysis ($4.1) and the other analyses that support \nother EPR-like optimizations [Cho88, JD82a, JD82b]. 2. The information assigned to each node is an element \nof a lattice4 where every descending chain is of finite length and there are designated top (T) and bottom \n (1) elements. The flow functions are monotonic: if ( L q then f(~) L f(q). Informally, the word bidirectional \nis appropriate for any problem where the information at a node depends on both predecessors and successors. \nJust as classical unidi\u00adrectional analysis restricts attention to the simple way that Info(y) depends \non Info(z) for zc Pred(y) in $2.1, our bidirectional analysis restricts attention to the simple way that \nInfo(y) depends on Injo(z) for z 6 Pred(y) and on Info(z) for z 6 SzLcc(y) in Figure 2. Formally, a bkM-ec\u00adtional \nprwbiern associates two flow functions FOTWF1OW(X,y) and Ba&#38;Ftow(s, g) with each edge (*, y). The \nequations to gIf the word {lattice is unfamiliar, then reading the notations in terms of pairs of bits \nis recommended. Though lattices have been put to good use in data-flow analysis, it is difficult to write \nan introduction to lattice theory for these purposes [Ros81]. be solved are as shown in Figure 2, and \nthe desired solution to these equations is the one that could be obtained by iterating from an initial \nguess of Info(y) = T for all y. For arbitrary bidirectional problems, iteration is much less attractive \nthan for unidirectional problems. When the nodes are visited in a well-chosen order [HU75], unidirec\u00adtional \nbit-vector problems can be solved in just three or four passes on the graphs that occur in practice. \nThe cor\u00adresponding strategy for bidirectional problems would be to visit nodes alternately in postorder \nand in reverse postorder. Even an acyclic graph with n nodes needs O(n) passes in the worst case because \na change from T to 1 can propagate alternately forward and backward along edges. The known bidirectional \nproblems of practical importance, however, are all easily shown to satisfy the simple algebraic conditions \nexplained in !j3. 1, despite the superficial complexity of the specific equations for each problem. Moreover, \nthe problems need only be solved for control flow graphs where certain edges have been split, as explained \nin $3.2. Edge splitting and algebra combine to make the important bidirectional problems easy to solve, \nas explained in $3.3. An intermediate step approximates the actual problems as unidirectional bit-vector \nproblems. These problems (and some auxiliary unidirectional problems that are solved to determine their \nflow functions) have the properties (discussed in 82.3) that make the slot wise method preferable over \nstandard met hods. 3.1 Algebraic Assumptions We list three possible assumptions about the flow functions \nin Figure 2. Consider any two outedges (z, y) and (z, z) from a node s. Then l?ackl lozo(z, y) = BackFlow(a, \nz). (4) Consider any backward function b. Then all ~, q satisfy b(( nq) = b(() n b(q). (5) Consider \nany backward function b, any forward function f, and any result ~ of fl-ing together some values of forward \nfunctions. Then all ~ satisfy v L -f(b(v)) = .f(b(v) n f) ~ f(b(~) n 1772twITz.fo). {6) The clustering \n(4) and distributivitg (5) assumptions are familiar from unidirectional data flow analysis, where they \nare oft en made, seldom really used, and sometimes false. In this paper, they are both used and true. \nThe assumption that forward functions are largzsh (6) has no obvious intuitive significance. It just \nhappens to be true for the important bidirectional problems. 3.2 Edge Splitting We consider a simple \ntransformation of the control flow graph underlying the bidirectional problem in Figure 2. To split a \ncontrol flow edge (c, y) is to replace (z, y) by two new edges (z, u) and (u, y), where u is a new node \nrepresenting an empty basic block. (In the course of optimization, some code may be moved into u.) Without \nany splitting, placement analysis would yield weaker information that misses chances to improve the compiled \ncode [MR79, Fig. 6] [RWZ88, p, 18]. Once the principle of splitting has been accepted, there are still \nthe questions of which edges to split and when to split them. Our splitting criteria are simple and independent \nof the optimizations to be performed: o Any edge that runs directly from a branch node to a join node \nis to be split. Q Splitting is to be done before analysis begins. Many papers have considered some form \nof edge split\u00adting, usually with other splitting criteria. Dhamdhere and Isaac [D180], for instance, \nconsidered edge splitting when placing code in light of execution-frequency informa\u00adtion. For elimination \nof partial redundancies, the splitting criteria given above were used by Rosen, Wegman, and Zadeck [RWZ88, \np. 17] and may have been used earlier. Sorkin [Sor89] proposed (incorrectly) that splitting to create \nloop preheaders would suffice. The fact that splitting is more widely needed was implicitly recognized \nby Morel and Renvoise [MR79, Fig. 6]. Alternatively, one can decide in the course of analysis which edges \nto split. This strategy was proposed inde\u00adpendently, for different variations on the original placement \nanalysis [MR79], by Dhamdhere [Dha88] and by Drechsler and Stadel [DS88]. Analysis predicts what will \nhold at each old node, then determines which edges need to be srdit in order to validate the rmedictions. \nFor an ambitious c~mpiler that tries to seize m-any of the opportunities that one optimization crest \nes for another, splitting as needed complicates the compiler, especially if the compiler writer is unwilling \nto bear the expense of a separate call on the memory allocator for each split edge. Once code has been \nmoved into a new block created by splitting for the sake of one optimization, the new block effectively \nbecomes an old block for later optimizations, and a new edge to (from) the new block may need to be split \nlater. Because old basic blocks may become empty in the course of optimization, a late optimization that \nremoves empty blocks is needed anyway. Our splitting criteria make the graph somewhat larger,5 but much \nof the splitting we do will be done eventually, if not by one optimization then by another. Occasionally, \nwe split an edge but never put anything into the new basic block. In this case, the edge will be restored \nas a byproduct of removing empty basic blocks. 5For example, we effectively add a dummy else to each \nif-t hen without one.  3.3 Some Problems Are Easy The bidirectional problem in Figure 2 is mostly backward \nif the information at each node g satisfies (7) where Bd and Fc are defined as follows. Replacing ~n~o(z) \nwith B(z) in (3), we get an ordinary backward problem that can be solved to yield the backward approximation \n~A to Info. Then we visit each node y in any convenient order and use B-4(X) in place of fnfo(z) in (2) \nto compute the forward correction 7cJ(y). Solving a mostly backward problem is only slightly more work \nthan solving the backward approximation, which can share storage with the final answer. The following \nresult shows that placement analysis and other important bidirectional problems are indeed mostly backward. \nTheorem 1 Consider any bidirectional problem (Figure 2) such that (thanks to edge splitting) no edge \nruns from a branch node to a join node. Suppose the forward functions are largish and the backward functions \nare clustered and distributive. Then the problem is mostly backward.~ Prooj. For each node z, let Because \n~n~o(z) ~ B-4(Z) (and thus In.fo(.z) ~ fc(.z)), we already have Info(z) ~ M(z). To show that M(z) ~ Info(z), \nwe show that every edge (z, y) has M(y) L f(M(x)) where f = Fo~wFlow(z, y); (8) M(z) L b(M(y)) where \nb = BackFlow(z, y). (9) Because Info is the largest way to assign information to nodes such that similar \n~ relations hold, these imply M(z) ~ Info(z). Proving (8) and (9) thus implies that the problem is mostly \nbackward. Consider any edge (z, y). Let ~ and b be asin (8) and (9). Thanks to clustering and distributivity, \nthe defini\u00adtion of l?A implies  A(x) 10) =b(zEL!c(x  In particular, BA(X) has the form b(q). Moreover, \n.TC(z) is either EntryInfoor the result of m-ing together some values of forward functions. Thus $C(y) \n~ f(~A(Z)) by definition of FC = f(~d(~) n xc(.)) because j is largish = f(M(z)) by commut ativit y \nBut M(Y) = (7c(Y) n ...) E 7c(Y), so J%) L ~(JWZ)) and (8) holds. For (9), the argument depends on whether \nor not y is a join node. Suppose first that y is a join node. Thanks to edge splitting, Succ(z) = {y}. \nThus (10) implies ~A(z) = b(~A(y)). Similarly, any other predecessor z of y has Bd(z) = b(Bd(~)). Therefore \nf?d(ll) = n A(Y) z ~ P~ed(y) because n is idempotente ~ Fom.uFZOW(Z,V)[ b(~A(y))] n z E P~ed(y) because \nFomDFlow(z, y) is largish n FO?WFZOW(Z,Y)(~A(z)) z ~ Pred(y) because ~A(.z) = b(~d(?l)) = FC(Y) by definition \nof .FG Therefore M(z) ~ t?A(Z) = b(~d(v)) = b(~C(?/) fl BA(?/)) = b(M(Y)) and (9) holds if y is a join \nnode. Now suppose that y is not by definition of M because Succ(z) = {y} by ~A(y) ~ ~c(?J) by definition \nof M a join node. Then because is largish : ~J)) by (10) by Pred(y) = {z} Therefore (10) implies Bd(~) \n~ b(~c(v)). But BA(Z) z b(~A (y) also, yielding ~A(Z) Z b(Yo(Y)) m b(~A(Y)) = b(fic(v) n ~A(?l)) = b(M(w)). \nTherefore M(z) ~ ~A(0) ~ b(M(y)) and (9) also holds if y is not a join node. . Corollary 1 Under the \nhypothesis of Theorem 1, suppose also that y is either a join node or a node with ~A(v) = L, Then Info(y) \n= 8A(Y) and the forward functions along inedges of y have no effect on the SOIUtion. Proof. By the theorem, \n~Tzjo(y) = ~c(y) n BA(Y). Suppose first that y is a join node. The argument for (9) in the proof of the \ntheorem shows that ~A(v) z $c(Y) and problems whose solutions are treated like local information by the \nactual placement analysis. For most of this section, z ESucc(z) we consider a single expression E and \nthe problem of where n A(z) g b(zEQc(z7z)) best to place the computations of E in the given program. \n~ hence that Info(y) = ~A(y). Now f?d(y) = ~. Then ~n~o(y) = . n both cases, in.fo(y) is found without \nfunctions along inedges of y. These known to have no effect on Info(z) have no effect at all on the solution. \nDrechsler and Stadel [DS88, p. suppose instead that 1 = 1 = Bd(y). In applying any forward functions \nwere already foranyz# y,sothey 0 638] implicitly derived another corollary of Theorem 1 for one particular \nversion of placement analysis. If enough of the basic blocks are empty, then the forward correction can \nbe shown to be unnecessary With Theorem 1, on the other hand, we sometimes need the forward correction \nbut never need to split an edge created by earlier splitting. at the nonempty blocks. A compiler using \nthis corollary needs a separate round of edge splitting for each EPR\u00ad like optimization, to replenish \nthe supply of empty blocks. Dhamdhere and Patil [DP90, Dha91] considered a vari\u00adant of placement analysis \nwhere forward-flowing information is merged with V while backward-flowing information is merged with \nA. They showed that this variant could be solved by a backward approximation followed by a variant of \ninterval-based forward data flow analysis. In our result, on the other hand, the merge is the same in \nboth directions, as is more common. The forward correction is a simple traversal in any convenient order. \nOur result also applies to irreducible cent rol flow and to any bidirectional problem (as defined here) \nthat satisfies the conditions explained in $3.1. Similar conditions, with the roles of forward and backward \nfunctions reversed, suffice to make a problem be mostly forward. 4 Worked Example In this section we \nfirst present the details of placement analysis [MR79] as later refined by [Cho83, JD82a] and then work \nthrough a small example in detail. 4.1 Placement Analysis We begin by reviewing of some auxiliary unidirectional \n4.1.1 Availability For each basic block z, let AVGEN(Z) be 1 if the code in z generates the availability \nof E by computing E and not assigning to any operand of E thereafter. Let NONE(z) be I if x assigns to \nnone of the operands of E. It is customary in [MR79]-inspired analysis to associate two global availabil\u00adity \nbits with each node y: AVIN(y) = { AVOUT(I/) Substituting for yields equation only the AVIN low information \n4.1.2 Partial o if y = Entry; AVOUT(X) if y # Entry, A z6 Pred(y) = AVGEN(y) V [AVIN(y) A NONE(y)]. \nAVOUT(Z) in the (11) in Figure 3. bits and exemplifies at Entry. Availabilityy equation for AVIN(Y) This \nequation involves $2.1 with J-= O and The same local information used in 34.1.1 can determine partial \navailability, which says that along some (perhaps not all) paths to a node o PAVIN(~) = PAVOUT(Z) v \nme Pred(y) { also be used to E is available y: if y = 13ntT~ if y # Entry; PAVOUT(Y) = AVGEN(Y) V [PAVIN(Y) \nA NONE(y)]. Substituting for PAVOUT(Z) in the equation for PAVIN(Y) yields equation (12) in Figure 3. \nThis equation involves only 217 o if y= Entry AVIN(y) = AVGEN(Z) V [AVIN(Z) A NONE(z)] if y # Entry (11) \nA z C Pred(y) { o if y = Entry PAVIN(~) = AVGEN(Z) V [PAVIN(Z) A NONE(Z)] if y # Entry (12) v x C F%ed(y) \n { o if y = Entry PAVIN(y) A (CB4(y) V [PPOUT(y) A NONE(y)]) A PPIN(y) = (13)A [PPOUT(z) V AVOUT(Z)] \nif y # Entry z E .%ed(y) {( ) if y= Exit PPOUT(I/) = PPIN(z) if y # Exit (14) AO z E Succ(y) { INSERT(y) \n&#38;f PPOUT(Y) A w AVOUT(Y) A -(PPIN(Y) A NONE(Y)) (15) DELETE(v) ~f PPIN(y) A CB4(y) (16) Figure 3, \nPlacement analysis in a fairly conventional formulation. ~nfo(y) = ~(y) A ~(y) (17) (o,1) if y= Entry \n~(y) Sf A [Fo~wFlow(z,y)](ln~.(.)) if y # Entxy (18) z C F%ed(y) { ( [Node13cd(y)](0), O) if y= Exit \nB(y) ~f A [Ba.kFIow(y,z)](Info(z)) if y # Exj-t (19) z E Succ(y) { &#38;out v AVOUT(Z) [~oTw&#38; ~ow(z, \ny)](.$) : where (20) r) (9=( 1) [Nodel?ack(y)]((,n) [13ack.Flow(y, z)](~) % q where (21) (~::t)=( [n \n) the PAVIN bits and exemplifies $2.1 with 1 = i and low Figure 4. Equations (13) and (14) from placement \nanalysis reformulated in terms of pairs of bits, using the functions NodeBack(y) from bits to bits defined \nby [NodeBack(y)](~) ~f PAVIN(y) A (CB4(y) V ~ A NONE(y)]). 218 information at Entry. Indeed, finding \nthe PAVIN bits by the method of $2.1.1 is precisely the first step in finding the AVIN bits by the method \nof $2.1.2. Traditional algorithms, on the other hand, would not exploit the similarity between partial \navailability y and availability y. 4.1.3 Placement Possible Bits One more bit of local information is \nused here. Let CB4(Z) be 1if E is computed in z before any assignments to operands of E. The global placement \npossitde bits have the more complicated equations (13) and (14) in Figure 3.7 Sub\u00adstituting for PPOUT(... \n) in the equation for PPIN(y) would not be helpful, but the somewhat more elaborate formal manipulations \nin $4.1.5 help reveal the direct applicability y of $3 and the indirect applicability y of $2. 4.1.4 \nInsertion and Deletion Points Once the PPIN(... ) and PPOUT(. ..) bits have been com\u00adputed, it is easy \nto determine where to insert new compu\u00adtations of the expression and where to delete old ones that become \nfully redundant after the insertions. The last two equations in Figure 3 do this. 4.1.5 Pairs of Bits \nWhen several expressions are considered, PPIN(y) and PPOUT(y) become two bit vectors, each of which has \na slot for each expression. This representation is conve\u00adnient for performing many operations, but not \nfor rea\u00adsoning about the results. Mathematically, it is more convenient to consider a single vector whose \nslots hold pairs of bits (PPIN(y), PPOUT(y)). The bitwise notations (A, V, m,1, T, ~) from $2 are overloaded \nin the natural way, so as to apply to pairs of bits as well as to single bits. Unlike a flow function \non a bit vector of length 2, however, a flow function applied to a pair of bits ~ = (~;~, (~~t ) may \nuse both argument bits for each result bit: Equations (13) and (14) in Figure 3 can now be written in \nterms of pairs of bits, where n is A and 1 is (O, O). A pair Info(y) is associated with each node y. \nThis information depends on a pair Y(g) flowing forward from predecessors of ~ and a pair l?(y) flowing \nbackward from successors of y, as shown in Figure 4 and discussed below. The derivation of Figure 4 from \nFigure 3 is typical of what is needed for the analyses that support EPR-like optimizations. The equations \nin Figure 4 are of interest only because they have been chosen to be equivalent to equations (13) and \n(14) in Figure 3: ln~o(y) = (PPIN(y), PPOUT(~)). Consider any node y # Entry. The local information \nassociated with y determines the function NodeBack(y) de\u00adfined in the figure caption, and this function \ntells how to use PPOUT(y) in computing PPIN(y). Thus, (13) for y # Entry can be rewritten as PPIN(y) \n= [NodeBack(y)](PPOUT( ~)) A ~ [*Q@] z C Red(y) 7The original equations [MR79] are even more complicated; \nwe have incorporated later refinements [Cho83, JD82a]. To model this with (17), we want T(y)in B(y)in \n= = x [N ~ [PPOUT(z) V AVOUT(Z)]; e P7-cd(y) odeBack(y)](PPOUT(~)). To get the F(y),n desired, we choose \neach [FOrWrFkMO(Z,y);n] as displayed in (20). To get the B(y);n desired if y # Exit, we observe that \nNodeBack(y) distributes over the A in (14). Then we choose each [BackFlow(y, .z)i~] as displayed in (21). \nTo get the B(y);n desired if y = Exit, we apply NodeBack(y) to the value PPOUT(y) = Oand obtain PAVIN(y) \nA CB4(y) as the appropriate EzitInfo;n in (19). Consider any node y # Exit. We proceed as above, but \nnow the formulas happen to be simpler. In particular, (14) for y # Exit can be rewritten as PPOUT(y) \n= i A PPIN(z). A .2 e Succ(y) To model this with (17), we want 7(V)W = 1; B(y)out = PPIN(z). A  z6 \nsum(y) To get the ~(y)oti, desired if y # Entry, we choose each [Forwd low(z, y)~tit] as displayed in \n(20). TO get the .F(y)~w~ desired if y = Entry, we choose EntTyInfoou* = I in (18). To get the wanted \nL?(y)~tit, we choose each [13ackl iow(y, z)~~t] as displayed in (21). Two choices remain in Figure 4. \nTo model (13) for y = Entry, we choose EntryInfo~~ = O in (18). To model (14) for ~ = Exit, we choose \nEzitInfoow, = O in (19). 4.1.6 Algebraic Properties Now that the PP equations have been cast into the \ngeneral form of Figure 2, we can verify the algebraic assumptions in ~3.1. In the definition (21) of \nthe backward flow functions, the Boolean operations applied to (;~ are independent of the outedge chosen. \nThese operations also distribute over A, so (4) and (5) hold. For (6), consider also the definition (20) \nof the forward flow functions. When a forward function is applied after a backward function, the result \nu = ~(b(q)) has ~ (:::,)= (( 3) ( V: ) Thus q c j(ZI(q)). Moreover, because f ignores the in\u00adcomponent \nof its argument, any < with ~mt = I has ~(b(q)) = j(b(q) A ~). In particular, EntryInfo and all values \nof forward functions have &#38;t = 1, so (6) follows. 4.2 The Worked Example Figure 5 contains a small \nprogram fragment and its control flow graph. Code motion guided by placement analysis elim\u00adinates partial \nredundancies, as shown in Figure 6. Local data flow information for this analysis is in Figure 7. Figure \nshows the placement information found by the steps detailed in the rest of this section. In each column \nin Figure 8, the - entries represent nodes where bits are known to have the default value because the \nnodes were never visit ed in computing that column. W bile the number of such nodes is small here, it \nshould be quite large in practice. Had there been more of the program between Entry and node 1 or between \n6 and Exit in the example, our algorithm would not have visited any of those nodes, We compute PAVIN \nby the algorithm in $2.1.1, using the substituted form in (12). For the comput ation of PAVIN, the controlling \nbit value is 1 = 1 and we have high infor\u00admation (T = O) at Entry. Thus, LOWER = START and any edge with \nAVGEN = 1 at its source node is a LOWER edge. Similarly, RAISE = STOP and any edge with AVGEN = O and \nNONE = O at its source node is a RAISE edge. The remaining edges are PROPAGATE edges. The set of LOWER \nedges is {(4,5), (4,6)}; the set of RAISE edges is {(2,4)}. The set N1 is initialized to {5, 6}. After \npropagation, 1, 2, 3, 4, 7, and Exit are also in iV1. Thus, PAVIN is 1 at nodes 1, 2, 3, 4, 5, 6, 7, \nExit and O elsewhere. We compute AVIN by the algorithm in 32.1.2, using the substituted form in (11). \nFor the computation of AVIN, the controlling bit value is 1 = O and we have low information at Entry. \nThus, RAISE = START and any edge with AVGEN = 1 at its source node is a RAISE edge. Similarly, LOWER \n= STOP and any edge with AVGEN = O and NONE = O at its source node is a LOWER edge. The remaining edges \nare PROPAGATE edges. The set of RAISE edges is {(4,5), (4,6)}; the set of LOWER edges is {(2,4)}. From \nthe calculation of PAVIN, we already know that Ifl is {1, 2, 3, 4, 5, 6, 7, Exit}. The set N2 is initialized \nto {1, 4}. After propagation, 2 and 3 are also in Nz. Thus, AVIN is 1 at nodes {5, 6, 7, Exit} and O \nelsewhere. The next step is to attack the PPIN, PPOUT system. BY applying the transformations in $3.3 \nto equations (18) and (19) in Figure 4, we obtain the forward correction (22) to the backwards approximation \n(23) in Figure 9. Substitution for BackFlow in (23) yields a system where all of the bits .8A oin can \nbe computed before comput\u00ading any of the bits BA oou~ (see (24) and (25) in Figure 9). Problem (24) can \nbe solved by the backwards version of the algorithm in $2.1.2. Since 1 = O, RAISE = START and LOWER = \nsTOP. The RAISE edges are { (4,5), (4,6) }; the LOWER edges are {(Entry, 1), (2,4)}. The remaining edges \nare PROPAGATE edges. We have low information at Exit because CB4(Exit) = O. We initialize N1 as {4}, \nthen add 1, 3, 5, 6, and 7 to it. We initialize Nz as {l}, then add 6 and 7 to it. Thus, the set of nodes \ny with ~A(~)~n = 1 is { 3, 4, 5 }. The set of nodes z with ~A(Z)out = 1is { 2,3, 5}. Substitution for \nFOTWF1OW in (22) yields a system where the bits .FCoout are already known (see (26) and (27) in Figure \n9). Thus, the set of nodes ~ with $_o(w)~~ = 1 is { 4, 5, 6, 7, Exit}, The equations for INSERT and DELETE \nin Figure 3 tell us to insert ncw computations in nodes 2 and 3 while deleting the old computation in \nnode 4. The resulting program is indeed as shown in Figure 6. 5 Second Order Effects Traditional EPR \n[MR79] performs placement analysis for all expressions simultaneously with a bit-vector algorithm but \nthen rearranges the computations for each separate ex\u00adpression E. Several implementors have noticed that \nmoving one expression may provide an opportunity to move other expressions. Slotwise analysis is well \nsuited to detecting and exploit ing such second-order eflects wit bout excessive work. 1 repeat 1 ifP \n2 then A+ O 4 repeat 4 ... +A+B 4 until Q 6 until R Entry 71 ~_--_, L__-. J 23 -----. r ----  A+O ~_,____ \n------ F5 5~ ~-- -, I I 6 Exit I Figure 5. A sample program and its control flow graph. Edge splitting \ninserted the dashed line nodes. Entry T__, 1--. _J 23 ___________ - JI +-O #L+13 ~A+B --,____ -....--J \n~ s I 5~ ~-.__, 1 1,  --* Exit Figure 6. Effect of code motion on Figure 5, Node NONE AVGEN CB4 Entry1 \n0 o 1i o0 20 00 3i o0 41 1i 51 0o 61 00 71 00 Exit1 0 0 T1 Figure 7. Local information for Figure 5. \nNode PAVIN PAVOUT AVIN AVOUT t?Aoin t?Aoout INSERT DELETE  Entry o o o o0i o 0 000 11 i 0 00 10 10 \n21 o 0 i 1 31 1 0 Exit1 1 11 E - o0000i 41 1 11 1 01 51 11i1 i 00 o00 00 61 i11 i 00 71 1ii i 00 \n10 0 o for PAVIN, AVIN, BdoinFigure 8. Global information for Figure 5. Default values (shown as - ) \nare FC(Y) = { A z C %ed(~) [ForwFlow(z,y)](B~(z)) (o,1) if if y = Entry y # Entry (22) .8A(v) = { A \nz E Sticc(u) ( PAVIN(I/) A CB4(I/) [BackFhu(yjz)](BA(z)) ,0 ) if if II = y # Exit Exit (23) ~A(Y)in = \n{ z c A Succ(y) PAVIN(y) A (CB4(Y) PAVIN(y) A CB4(y) V [BA(z)ita A NONE(V)]) if if y = ~ # Exit Exit \n(24) B/l(~) out = { A z G Succ(y) o if ~d(z);n if y = y # Exit Exit (25) Fc(g)i. = { A z E %ed(y) [Bd(.)out \nVAVOUT(zj if if g = y # Entry Entry (26) Fc(y)owt = i (27) Figure 9. Application of Theorem 1 to the \nequations in Figure 4. We deal with the expressions separately throughout EPR, tion of a binary operator \nto a pair of variable or constant using a worklist. Before explaining the details, we review operands. \nThe actual syntax of expressions is irrelevant here, the (sometimes tacit) assumptions about intermediate \ncode so long as the operands are visible and so long as any possible that underlie data flow analysis \nin general and placement changes to variables of interest are displayed as assignments analysis in particular. \nto those variables. To a first approximation, the intermediate code consists of assignments of one of \ntwo kinds. The simpler kind is just Various practical complications can be handled by a copy allowing \nthe left-hand side of a computation to be a tuple (vaviable~e,,) +-(wz~iable,~~,.~). of variables, each \nof which is assigned from an expression in a tuple on the right-hand side. For example, the target The \nmore complex kind is a compw+atiom machine might not separate computing X + Y from com\u00adputing a condition \ncode to support branching on the sign(variable) + (ezp~ession), of X + Y. Doing one entails doing the \nother, and both are where (expression) is often} but not always, the applica-moved or inserted or deleted \nat once. For a target machine Tli--A+B T2< TI*C TA+BtA+B TI i\u00adTA+B T2+TI*C TA+B+A+B T2 +\u00adTA+B * C . . \n. T3+A+B T4+T3*C . . . TA+B+A+B T3 + TA+B T4+T3*C TA+B&#38;A+B T4 &#38; TA+B * C Figure 10. Code that \nexploits the modifications to take advantage of second-order effects. that does addition this way, the \nintermediate code actually manipulated by EPR might include assignments like (A, CC) + (X+ Y, Conditi.on( \n + , X,Y)), where A, X, Y are numerical variables and CC is a condition code variable. Many more examples \nare in [CFR+ 91, \\3. 1]. Each expression considered by placement analysis is actu\u00adally a tuple of expressions \nthat appears on the right-hand side of a computation. Once one sees that the major oversimplification \nin the first approximation is the presumption that all tuples are of length 1, it becomes fairly safe \n(as well as quite convenient) to use the approximation. Optimization that seem to be restricted to simple \ncomputations like A -X+Y are usually more general, but involve some small loops over the compo\u00adnents \nof tuples when written out in general form. Omitting these loops simplifies the present ation in the \nfollowing sub\u00adsections. We also assume that local (within one basic block) redundancies have already \nbeen removed in the obvious way. Thus, a block may compute an expression E at most once before all assignments \nto operands of E and at most once after all assignments to operands of E. (There may also be computations \nof E interspersed with assignments to operands of E.) Section 5.1 deals with ordinary intermediate code. \nSection 5.2 deals with intermediate code in SSA form [CFR+ 91], which permits more extensive optimization \nin exchange for introducing another kind of copy operation and many more variables. 5.1 Ordinary Intermediate \nCode Consider any expression E computed in the intermediate code, perhaps in several different basic \nblocks. This ex\u00adpression forms one slot for placement analysis. Initially, the worklist contains all \nof the slots, On each iteration of the algorithm, a slot is taken from the worklist and the equations \nin Figure 3 are solved for that one slot. If some INSERT or DELETE bits are nonzero, then the intermediate \ncode is changed, as described below (this may entail adding another slot to the worklist). The algorithm \nterminates when the worklist empties. If some of the INSERT or DELETE bits for the current slot E are \nnonzero, then the intermediate code is changed as follows: 1. A new unique temporary name TH is created. \n 2. Consider any basic block where AVGEN = 1 and DELETE = O. The last old computation V+ E in the block \nis replaced by a new computation TE + E followed by a copy V+ TE.  3. Consider any basic block where \nDELETE = 1. The first old computation V +-E in the block is replaced by a copy V tTE. 4, Consider any \nbasic block where INSERT = 1. A new computation TM + E is inserted into the block, after any assignments \nto operands of E that may occur in the block, The example in Figure 10 illustrates several points, On \nthe left of this figure is an intermediate code fragment. Initially there are three slots, one for each \nof the expressions A + B, Ti * C, and T3 * C. If the slots for the last two expressions happen to be \ntaken off the worklist first, then no changes are made to the intermediate code until the slot for A \n+ B is considered. The middle part of the figure shows the results of processing the A + B slot. Two \ncopy statements are added. The code on the right shows the results of copy propagation [ASU86]. Now the \nexpression TA+B forms a new slot that is put on the worklist and leads to further optimization when its \nturn comes, In this example, the copy propagation changes the expressions TI * C and T3 * C to TA+B *C. \nNot only must a new slot be created for the new expression, but also the slots for T1 * C and T3 * C \nmust be added to the worklist, since the deletion (caused by copy propagation) of one instance of the \nexpression may change the placement of the other instances of the expression. 5.2 SSA Form Intermediate \nCode If the intermediate code is in static single assignment (SSA) form [CFR+ 91], then EPR can be more \nextensive though more complex. We plan to submit a longer version of this paper, with details for SSA-form \nintermediate code, to a journal. The rest of this section outlines some of the issues. When a program \nis in SSA form, each variable is assigned a value at exactly one point in the program text. If the original \nintermediate code has several assignments to the same variable V, then each of them becomes an assignment \nto a new unique variable V,. To preserve the original flow of values, SSA form introduces a new kind \nof copy operation at some of the join nodes. A ~-finction at a join node y has the form UUrdost + d(var~our. \nel, vaT30urCe2, . ), where there is one operand for each control flow inedge of g. The operands are \nlisted in the same (arbitrary) order used to list the inedges. If control passes to y along the K-th \ninedge (z, y), then the value of the corresponding operand vaT,OU,ceK is copied to VWd..t and the other \noperands are ignored. 222 With SSA form, copy propagation can always eliminate every ordinary copy ?Ja? \n&#38;@ +-WZr,OU,c, from one variable to another. All uses of ~a~de,~, including uses as operands of@ \nfunctions, can safely be replaced by uses of VW-,OU,==. Details are in [RWZ88], where it is shown that \nthe tesulting expres\u00adsions act like global value numbers -if two occurrences of expressions El and E2 \nin the original program become occurrences of the very same expression E; ~E.j with SSA variables, then \nredundancy elimination can put those two occurrences in the same slot, even though the original El and \nEz may look different. Eliminating redundancies between two expressions that look different but compute \nthe same value is useful. Un\u00adfortunately, translation to SSA form sometimes hides other opportunities \nfor EPR when it replaces one expression E, computed in many different places with operands that have \nmany different values, by many different computations of expressions E , E , ... with different SSA variables \nas the operands. Rosen, Wegman, and Zadeck [RWZ88] try to seize the new opportunities without losing \nthe old ones, but their algorithm is complicated and restricted to reducible control flow. Moreover, \nthere are some cases where the original EPR [MR79] moves an invariant out of a loop with multiple exits \nbut [RWZ88] does not.8 These cases suggest that SSA\u00adbased EPR should be done more along the lines of \n$5.1, but with some provision for putting several different expressions E/, E,, , ... in the same slot \nwhenever SSA form would hide old opportunities. References [ASU86] A. V. Aho, R. Sethi, and J. D. Ullman. \nCompil\u00aders: Principles, Techniques, and Tools. Addison-Wesley, 1986. [CCF91] J. Choi, R. Cytron, and \nJ. Ferrante. Auto\u00admatic construction of sparse data flow evaluation graphs. Conf. Rec. Eighteenth ACM \nSyvnp. on Principles of Programming Langs., pages 55-66, January 1991. [CFR+89] R. Cytron, J. Ferrante, \nB.K. Rosen, M,N. Weg\u00adman, and F. K. Zadeck. An efficient method of computing static single assignment \nform. Conf. Rec. Sixteenth ACM Symp. on Principles of Pro\u00adgramming Langs., pages 25 35, January 1989. \n[CFR+91] R. Cytron, J. Ferrante, B.K. Rosen, M.N. Weg\u00adman, and F. K. Zadeck. Efficiently computing static \nsingle assignment form and the control dependence graph. ACM Trans. on Program\u00adming Lungs. and Systems, \n13(4):451-490, Octo\u00adber 1991. [Cho83] F. C. Chow. A portable machine-independent global optimizer -design \nand measurements. Technical Report 83-254 (PhD Thesis), Com\u00adputer Systems Laboratory, Stanford U. Stanford, \nCA, December 1983. [Cho88] F. C. Chow. Minimizing register usage penalty at procedure calls. Proc. SIG \nPLA N 88 Symp. on Compiler Construction, pages 85-94, June 1988. Published as SIGPLAN Notices Vol. 23, \nNo. 7. s An invariant expression E may be computed along some, but not all, paths through the loop body. \nIt may also be computed along all paths starting from some, but not all, of the loop exits. If the end \nresult is that E is computed along all paths after entering the loop, then [MR79] replaces all these \ncomputations of E by a new computation in the loop preheader. [Dha88] [Dha91] [D180] [DP90] [DS88] [HU75] \n[JD82a] [JD82b] [Kou77] [MR79] [MR90] [ROS81] [RWZ88] [Sor89] [Zad84] D. M. Dhamdhere. A fast algorithm \nfor code movement optimization. SIGPLAN Notices, 9(3):243-273, August 1988. D. M. Dhamdhere. Practical \nadaptation of the global optimization algorithm of morel and ren\u00advoise. ACM Trans. on Programming Langs. \nand Systems, 13(2):291-294, April 1991. D. M. Dhamdhere and J. R. Isaac. A compos\u00adite algorithm for strength \nreduction and code movement optimization. Int. J. of Computer and Information Sci., 23(10):172-180, 1980. \nD. M. Dhamdhere and H. Patil. An efficient algorithm for bidirectional data flow analysis. Technical \nReport TR-016-90, Dept. of Computer Sci. and Eng., Indian Inst. of Technology, 1990. Revision to appear \nin ACM T~ans. on Program\u00adming Langs. and Systems. K.-H. Drechsler and M. P. Stadel. A solution to a problem \nwith Morel and Renvoise s Global Optimization by Suppression of Partial Redun\u00addancies . ACM Trans. on \nProgramming Lungs. and systems, 10(4):635 640, October 1988. M. S. Hecht and J. D. Unman. A simple al\u00adgorithm \nfor global data flow analysis problems. SIAM J. Computing, 4(4):519-532, Dec. 1975. S. M. Joshi and D. \nM. Dhamdhere. A composite hoisting-strength reduction transformation for global program optimization \n(part I). Int. J. of Computer Math., pages 22-41, 1982. S. M. Joshi and D. M. Dhamdhere. A composite \nhoisting-strength reduction transformation for global program optimization (part 11). Int. J. of Computer \nMath., pages 111-126, 1982. L. T. Kou. On live-dead analysis for global data flow problems, J. ACM, 24(3):473-483, \nJuly 1977. E. Morel and C. Renvoise. Global optimization by suppression of partial redundancies. Comm. \nACM, 22(2):96-103, February 1979. T. J. Marlowe and B. G. Ryder. An efficient hybrid algorithm for incremental \ndata flow anal\u00adysis. Conf. Rec. Seventeenth ACM Symp. on Principles of Programming Langs., pages 184\u00ad196, \nJanuary 1990. B. K. Rosen. Degrees of availability as an in\u00adtroduction to the general theory of data \nflow analysis. In S. S. Muchnick and N. D. Jones, edit ors, Program Flow Analysis, chapter 2, pages 55-76. \nPrentice Hall, 1981. B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant \ncompu\u00adt ations. Conf. Rec. Fifteenth ACM Symp. on Principles of Programming Lungs., pages 12-27, January \n1988. A. Sorkin. Some comments on A Solution to a Problem with Morel and Renvoise s Global Optimization \nby Suppression of Partial Redun\u00addancies . ACM Trans. on Programming Langs. and S@ems, 11(4):666 668, \nOctober 1989. F. K. Zadeck. Incremental data flow analysis in a structure program editor. Proc. SIGPLAN \n84 Symp. on Comptler Construction, pages 132-143, June 1984. Published as SIGPLA N Notices Vol. 19, No. \n6. 223   \n\t\t\t", "proc_id": "143095", "abstract": "", "authors": [{"name": "Dhananjay M. Dhamdhere", "author_profile_id": "81100471827", "affiliation": "", "person_id": "P66375", "email_address": "", "orcid_id": ""}, {"name": "Barry K. Rosen", "author_profile_id": "81100316668", "affiliation": "", "person_id": "P28116", "email_address": "", "orcid_id": ""}, {"name": "F. Kenneth Zadeck", "author_profile_id": "81100345054", "affiliation": "", "person_id": "PP39085955", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143135", "year": "1992", "article_id": "143135", "conference": "PLDI", "title": "How to analyze large programs efficiently and informatively", "url": "http://dl.acm.org/citation.cfm?id=143135"}