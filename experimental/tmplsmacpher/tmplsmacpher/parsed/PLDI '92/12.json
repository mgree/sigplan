{"article_publication_date": "07-01-1992", "fulltext": "\n Eliminating False Data Dependence using the Omega Test * William Pugh David Wonnacott Univ. Dept. of \nComputer Science of Maryland, College Park, MD pugh@cs .umd. edu, (301)-405-2705 20742 Dept. of Computer \nScience Univ. of Maryland, College Park, MD davew@cs.umd.edu, (301)-405-2726 20742 Abstract Array data \ndependence analysis methods cur\u00adrentlyin use generate false dependence that can prevent useful program \ntransformations. These false dependence arise because the questions asked are conservative approximations \nto the questions we really should be asking. Unfortu\u00adnately, the questions we really should be asking \ngo beyond integer programming and require deci\u00adsion procedures for a subclass of Presburger for\u00admulas. \nIn this paper, we describe how to extend the Omega test so that it can answer these queries and allow \nus to eliminate these false data depen\u00addences. Wehaveimplemented thetechniques de\u00adscribed here and believe \nthey are suitable for use in production compilers. 1 Introduction Recent studies [HKK+91, CP91] suggest \nthat array data dependence analysis methods currently in use gen\u00aderate false dependence that can prevent \nuseful pro\u00adgram transformations. For the most part, these false dependence are not generated by the conservative \nnature of algorithms such as Banerjee s inequalities [SLY89, KPK90]. These false dependences arise be\u00adcause \nthe questions we ask of dependence analysis al\u00adgorithms are ccmservative approximations to the ques\u00adtions \nwe really should be asking (methods currently in use are unable to address the more complicated ques\u00adtions \nwe should be asking). *This work is supported by NSF grant cC!R-9157384 and a Packard Fellowship. Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMschinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM SIGPLAN \n92 PLD1-6/92/CA 01992 ACM 0-89791-476-7/92/0006/01 40...$1.50 For example, there is a flow dependence \nfrom an ar\u00adray access A(i) to an array access l?(j) iff A is executed with iteration vector i, B is executed \nwith iteration vector j, A(i) writes to the same location as is read by B(j), A(i) is executed before \nB(j), and there is no write to the location read by l?(j) be\u00ad tween the execution of A(i) and B(j). \nHowever, most array data dependence algorithms ig\u00adnore the last criterion (either explicitly or implicitly). \nWhile ignoring this criterion does not change the to\u00adtal order imposed by the dependence, it does cause \nflow dependence to become contaminated with out\u00adput dependence (storage dependence). There are techniques \n(such as privatization, renaming, and array expansion) that can eliminate storage-related depen\u00addence. \nHowever, these methods cannot be applied if they appear to affect the flow dependence of a prcr\u00adgram. \nAlso, flow dependence represent more than or\u00addering constraints: they also represent the flow of in\u00adformation. \nIn order to make effective use of caches or distributed memories, a compiler must have accurate information \nabout the flow of information in a pro\u00adgram. Similarly, many dependence testing algorithms do not handle \nassertions about relationships among non\u00adinduction variables or array references that appear in subscripts \nor loop bounds. To be useful, a system must not only be able to incorporate assertions about these relationships, \nbut also be able to generate a useful di\u00adalog with the user about which relationships hold. Unfortunatelyj \nthe questions we really should be ask\u00ading go beyond integer programming and require deci\u00adsion procedures \nfor a subclass of Presburger formulas. Presburger formulss are those that can be built up from integer \nconstants, integer variables, addition, >, =,1, A,V,+, V and 3. We can use these primitives to allow \nus to also handle subtraction, multiplication by integer constants, and the other arithmetic relations \n(~, <, ~). Presburger formulas are decidable, but the only known decision procedures that handle the \nfull class take at least doubly-exponential worst-cast time. Our original work on the Omega test [Pug91] \nde\u00adscribed efficient ways to answer the usual questions asked for dependence analysis. In this paper, \nwe show how the Omega test can be extended so that it can be used to answer questions in a subclass of \nPresburger arithmetic. We then show how to phrase within that subclass the questions we need to ask. \nWe also describe experiences with an implementation of the methods de\u00adscribed here that convince us that \nthese techniques are suit able for use in production compilers, 2 Notation The notation in Table 1 is \nadapted from [ZC91]. Below, we briefly discuss dependence distance, direction and restraint vectors. \n2.1 Dependence Distance A data dependence has a dependence distance in each loop, which is is the difference \nbetween the values of the loop variables for some pair of iterations involved in the dependence. 1 The \ndependence distance vector for a dependence is a vector of the dependence distances for the loops common \nto both statements involved in the dependence, Often, a dependence distance is not constant, and the \ndependence distance in one loop can be coupled to the dependence distance in another loop. For ex\u00adample \na dependence might have dependence distances {(Ai, Aj) IOs Ai ~ 5A 1< Ai+ Aj ~ 10}. Since dependence \nrepresent a constraint from one statement execution to a later statement execution, dependence 1Thereis \nsomedisagreementwithintheresearchcomrn~tY as to whether dependence distance reflects the difference in \nthe values of the loop variables ([ZC91]), or the difference in the loop trip counts ([W0189]). This \ndisagreement is usually submerged by the fact that many researchers only discuss normalized loops with \nan increment of 1. One problem with using the difference in loop trip counts is that it is not always \nwell defined. For example, in the following code segment, the dependence dist ante is (1, -2) if the \ndifference in the loop variables is used; however, the dependence distance doesn t seem to be defined \nif loop trip counts are used and the loop is not normalized: for i := O to 10 for j :=ito10by3 A[i, j3 \n:= A[i-1, j+2]  The problem with using the difference in the values of loop variables as dependence \ndistance is that negative steps compli\u00adcate our discussion of forward dependence distances (usually de\u00adscribed \nas lexicographically positive distances) and the descrip\u00adtion of the conditions under which loop interchange \nand circula\u00adtion are allowed. In this paper, we use the difference in the loop variables as the dependence \ndistance, and ignore the side issue of complicating the definition of forward dependence dktances. distances \nmust be lexicographically positive2, with a zero dependence distance in all loops possible only if the \ndependence is syntactically forward. 2.1.1 Direction vectors A direction vector summarizes, for each \nloop, the pos\u00adsible signs of the dependence distance in that loop. For example a dependence with dependence \ndistances (Ai, Aj) such that O ~ Ai < 5A0 < Aj <10 would be represented by the direction vectors {(O+, \nO+)}. We also can give a specific distance or range of distances in a direction vector (e.g., {(O+, 1)} \nor {(O :1, l)}). It is not always possible to summarize accurately the possible signs of the dependence \ndistance with a single direction vector. The signs of the dependence distances {(Ai, Aj) I Ai = Aj} can \nbe accurately represented by the direction vectors {(+, +), (O, O), ( , )}. After fil\u00adtering for lexicographically \nforward directions, this de\u00adpendence is represented by {(+, +), (O, O)}. If these two are compressed \ninto a single direction vector (O+, O+), it falsely suggests the signs (O, +) and (+, O) are possi\u00adble. \nGiven a set of constraints that describe the possi\u00adble forward and backward dependence distances, we \nanalyze the constraints to produce a set of partially compressed direction vectors (as described in [Pug91]). \nThese direction vectors are filtered to produce a set of forward direction vectors. 2.1.2 Restraint vectors \nWhile checking for dependence killing, covering and re\u00adfinement (Section 4), we deal with dependence \nas in\u00adteger programming problems. We need to force the dependence directions to be Iexicographically \npositive (e.g., force Ai >0 VA? =O A Aj ~ O). Unfortu\u00adnately, this is not a conjunction of linear constraints. \nHowever, we can often find a conjunction of linear con\u00adstraints that will force the dependence distance \nto be lexicographically forward. For example, we can force the dependence distances Ai = Aj to be lexicographi\u00adcally \npositive by adding the constraint Ai ~ O. We represent constraints that force the dependence directions \nto be lexicographically positive in the same form as direction vectors (specifying the possible signs \nfor each dependence distance). However, these re\u00adstraint vectors only have to filter out lexicographically \nnegative directions which are legal solutions to the in\u00adteger programming problem that describes all \nforward and backward dependence distances. We can use the set of direction vectors as a set of restraint \nvectors, but using restraint vectors usually requires that fewer constraints be added to the problem. \nOur algorithms in Sections 4 and 5 are unable to deal directly with dependence that cannot be represented \n2 &#38;5~ng loop in~e~ents ae pOSitiVe, see footnote 1 or details. A, B,... Refers to a specific array \nreference in a program . .1,~, k,... An iteration vector that represents a specific set of values of \nthe loop variables for a loop nest. [A] The set of iteration vectors for which A is executed A(i) Theinstance \nofarray reference Awhenthe loop variables have the values specified byi A(i) &#38;bA (i ) The references \nA and A refer to the same array and the subscripts of A(i) and A (i ) are equal. A(i) < A (i ) Execution \nof A(i) occurs previous to A (i ) A(i) <<~ A (i ) The dependence dist ante from A(i) to A (i ) is described \nby the direction/dist ante vector D s yzr The set of symbolic constants (e.g., loop-invariant scalar \nvariables) Figure 1: Notation used in this paper by a single restraint vector. Therefore, such depen\u00ad \ndence are split into several dependence, one for each restraint vector. 3 Extending the Omega test The \nOmega test [Pug91] is an integer programming al\u00adgorithm based on Fourier-Motzkin variable elimination. \nThe basic operation supported by the Omega test is projection. Intuitively, the projection of a set of \ncon\u00adstraints is the shadow of a set of constraints. More formally, given a set of linear equalities and \ninequali\u00adties on a set of var~bles V, p~ojecting the constraints onto the variables V (where V CV) produces \na set of constraints on variables v that has the same integer solutions for P as the original problem. \nFor exam\u00adple, projecting {O ~ a ~ 5; b<a< 5b} onto a gives {2 < a s 5}, We use the notation rzl,...,Cn(S) \nto rep\u00adresent the projection of the problem S onto the set of variables xl, ... , Zm and the notation \nTYZ(S) to repre\u00adsent the projection of the problem S onto all variables other than x. The Omega test \ndetermines if a set of constraints has integer solutions by using projection to eliminate variables until \nthe constraints involve a single variable, at which point is it easy to check for integer solutions. \n There are many other applications of projection. For example, if we define a set of constraints for \nan array pair that includes variables for the possible dependence distances, we can project that set \nof constraints onto the variables for the dependence distance. The pro\u00adjected system can be efficiently \nused to determine the dependence directions and distances. Because the Omega test checks for integer \nsolutions, not real solutions, it is sometimes unable to produce a single set of constraints when computing \nmc(S). In\u00adstead, the Omega test is forced to produce a set of problems So, S1, . . . . SP and a problem \nT such that mc(s) = ~=o Si s T. This is called splintering, and we call So the Dark Shadow of mz(S) and \ncall T the Real Shadow of ~C(S) (the Real Shadow may include solutions for z that only have real but \nnot integer so\u00adlutions for the variables that have been eliminated). In practice, projection rarely splinters \nand when it does, So contains almost all of the points of ~Z(S), T doesn t contain many more points than \nr.(S), and p is small. If we are checking to see if S has solutions, we first check if So # @or T = (?I.only \nif both tests fail are we required to examine S1, S2, , . . . SP. Also, when checking for integer solutions, \nwe choose which variable to eliminate to avoid splintering when possible. Although integer programming \nis an NP-Complete problem, the Omega test is efficient in practice [Pug91]. 3.1 How the Omega test works \nFourier-Motzkin variable elimination [DE73] eliminates a variable from a linear programming problem. \nIntu\u00aditively, Fourier-Motzkin variable elimination finds the n 1 dimensional shadow cast by an n dimensional \nobject. Consider two constraints on z: a lower bound ~ ~ bz and an upper bound az ~ a (where a and b \nare posi\u00adtive integers). We can combine these constraints to get a~ ~ ab.z~ ba. The shadow of this pair \nof constraints is a,tl ~ ba. Fourier-Motzkin variable elimination cal\u00adculates the shadow of a set of \nconstraints by combining all constraints that do not involve the variable being eliminated with the result \nfrom each combination of a lower and upper bound on the variable being elimi\u00adnated. The real shadow is \na conservative approxima\u00adtion to the integer shadow of the set of constraints. In [Pug91], we extended \nFourier-Motzkin variable elimination to be an integer programming method. Even if a~~ ba, there may be \nno integer solu\u00adtion to z such that a~ ~ abz ~ ba. However, if a~ + (a l)(b 1)< ba, we know that an \ninteger solution to z must exist. This is the dark shadow of this pair of constraints (described in [Pug91]). \nThe dark shadow is a pessimistic approximation to the in\u00adteger shadow of the set of constraints. Note \nthat if a = 1or b= 1, the dark shadow and the real shadow are identical, and therefore also identical \nto the integer shadow. There are cases when the real shadow contains in\u00adteger points but the dark shadow \ndoes not. In this case, determining the existence of integer solutions to the original set of constraints \nrequires the use of special case techniques, described in [Pug91], that are almost never needed in practice. \n3.2 Determining the validity of Presburger formulas Assume that p and q are propositions that can each \nbe represented as a conjunction of linear equalities and inequalities. We can determine the truthfulness \nof the following predicates: Is p a tautology? Trivial to check when p is a corl\u00adjunction. Is p satisfiable? \nWe can check this using techniques described in Section 3.1 and in [Pug91]. Is pa q a tautology? This \ncould not be efficiently answered using the techniques described in [Pug91], but can be efficiently answered \nin practice using techniques described in Section 3.3. The projection transformation offered by the \nOmega test allows us to handle embeded existential qualifiers: r?$ (p) = (3z s.t. p). We can combine \nthese abilities, as well as any standard transformation of predicate cal\u00adculus, to determine the validity \nof certain Presburger formulas. We have not attempted to formally capture the subclass of Presburger \nformulas we can answer eilh\u00adciently. The following are examples of some Presburger formulas we can answer \nefficiently: Vz, ~y s.t. p: True iff r-v(p) is a tautology. Vz, (3v s.t. p) ~ @z s.t. q): True iff T=v(p) \n~ m~z(g)is a tautology. We can easily determine this if T_Z(g) does not splinter. Vx, ~p V q V 7T: True \niff p A T+-q is a tautology. where p, q and ~ are conjunctions of linear equalities and inequalities \n3.3 Computing Gists and Checking when p+ q is a tautology Intuitively, we define (gist p given q) as \nthe new infor\u00admation contained in p, given that we already know !l. More formally, (gist p given q) is \na conjunction containing a minimal subset of the constraints of p such that ((gist p given g) A q)= (p \nA q)). Note that (gist p given q) = True s q= (pA q)E (q~p). When computing (gist p given q), we first \nconvert any equalities in p into a matched pair of inequalities (e.g., convert z = 1 into 1< z < 1). \nThe following is a naive method for computing gists. The algorithm treats a conjunction as a list of \nindividual constraints: gist []q=[] gist (e :p) q= e :(gistp (e :q)), if (=e) A p Agk satisfiable gist \n(e :p) q= gist p q, otherwise  This algorithm requires many satisfiability tests, each of which takes \na non-trivial amount of time. We handle this problem by checking for a number of special cases (listed \nin order of increasing difficulty to check): . For each equation e in p, we check to see if e is implied \nby any single constraint in p or q. Ifso, e is redundant and not in the gist. . We check to see if there \nis any variable that has an upper bound in p but not in ~, If so, we know that at least one of the upper \nbounds from p must be in the gist. A similar check is made for lower bounds, . If there does not exist \nsome constraint e in p or  q such that the inner product of the normals of e and e! is positive, then \ne must be in the gist. . For each equation e in p, we check to see if e is implied by any two constraints \nin p and/or q (other than e). These fast checks often completely determine a gist. When they do not, \nthey usually greatly simplify the problem before we utilize the naive algorithm. 3.3.1 Checking implications \nAs noted earlier, we determine if q~ p is a tautology by checking if (gist p given q) = True. When perform\u00ading \nsubset tests using the above algorithms, we short\u00adcircuit the computation of the gist as soon as we are \nsure that the gist is not True) . 3.3.2 Combining Projection and Gist computation If is often the case \nthat we need to compute problems of the form gist r-y(p) given maz (q). We could perform this computation \nby performing the projections inde\u00adpendently, and then computing the gists. However, if z is free in \np and y is free in q, there is a more efficient solution. We can combine p and q into a single set of \nconstraints, tagging the equations from p red and the equations from q black. We then project away the \nvari\u00adables v and z and eliminate any obviously redundant red equations as we go. Once we have projected \naway y and z, we then compute the gist of the red equations with respect to the black equations, 3.4 \nRelated Work Several authors have explored methods for using inte\u00adger programming methods to decide subclasses \nof Pres\u00adburger formulas [Ble75, Sho77, JM87]. Previous ap\u00adproaches have not had any way to compute projections \n(and thereby handle embeded existential quantifiers) nor have they had special efficient methods for \ncheck\u00ading when an implication is a tautology. We have not yet looked at wider applications of our work \nor done any direct comparison of our implementation against implementations of other approaches. We believe \nour approach and implementation will compare favorably with previous ones. a(m) := ... for L1 := 1to \n100 a(n) := ... a(Ll) := ... for L1 :=1 tondo for L1 := nto n+iO do for L2 := 1to ndo for L2 :=2 tomdo \na(Ll) := ... a(L2) := ... a(L2) := a(L2-1) a(L2-1) := . . . for LI:=nton+20do for L2:=2 to n-1 do Unrefined \nflow dependence: (0+,1) . . . := a(Ll) ... := a(L2) Refined flow dependence: (0,1) Examplel: Killed \nflowdep Example2: Covering and Killeddep Example 3: Refinement for L1 := 1to ndo for Li :=ltondo for \nL1 := 1 to n do for L2 := n+2-Ll to m do for L2 := L1 to m do for L2 :=2 tomdo a(L2) := a(L2-1) a(L2) \n:= a(L2-1) a(L1 L2) := a(L1-L2) Unrefined flow dependence: (0+,1) Unrefined flow dependence: (0+,1) \nUnrefined flow dependence: (a, a), a >1 Refined flow dependence: (0,1) Refined flow dependence: (0:1,1) \nRefined flow dependence: (1,1) Example4: Trapezoidal Refinement Example 5: Partial Refinement Example \n6: Coupled Refinement 4 Handling array kills A, we attempt to refine the dependence distance, and then \ncheck if it is covering. If it is covering, we can rule In this section, we discuss the elimination of \nfalse out a flow dependence from any write that occurs com\u00addependence in which an intervening write eliminates \npletely before A. If there appear to be multiple flow an apparent dependency. These techniques are most \ndependence to B, we check them pairwise for killing. useful when applied to flow dependence. However, \nwe can also apply thereto output and anti-dependences. 4.1 Killing dependence There are four kinds of \nanalysis we perform: A dependence from a read or write A to a read or write C is killed by the dependence \nfrom a write B to Ciff Killing A dependence from a read or write A to a all elements accessed by A are \noverwritten by B before read or write Ciskilled by the dependence froma C can access them. This is the \ncase if write B to C iff all array elements accessed by A are overwritten by B before C can access them. \n Vi, k, Sym, i~[A]Ake [C] AA(i) < C(k) A A(i) ~b C(k) + Covering A write A covers a read or write B iff \nit ~j s.t. j E [B] A A(i) ~ B(j) < C(k) writes to all the elements of the array that will be accessed \nby B. In this case, any dependence to AB(j) ~b C(k) B from an access that precedes A is killed by the \n In Example 1, the write to a(Ll ) kills the flow fromdependence from A. the write of a(n) to the read \nof a(Ll): Terminating A dependence from a read or write A i~ [A]Ak c [C] A A(i) < C(k) A A(i) ~bC(k) \nto a write B terminates A if B overwrites allele\u00ad ~kl=n ments that were accessed in A. In this case, \nany dependence from A to a read or write after B is 3j s.t. j c [1?]A A(i) R B(j) ~ C(k) A B(i) ~bC(k) \nkilled by the dependence from A to B. =n<kl<n+10 Refinement It may be possible to refine the depen\u00adkl=n~n<kl<n+10 \ndence distances for a dependence from a write A to a read or write B to a subset D of these de-  If \nthe first write were to a(m), we would not be able pendence distances. This is possible iff any de-to \nverify the kill: pendence with a distance not in D is killed by a dependence in D. i G[A] Ak G[C] A A(i) \n< C(k) A A(i) ~bC(k) -n<kl<n+20Akl=m We first compute all output dependence (output de\u00adpendence give \nus fast checks for when to test for killing 3j s.t. j G[B] A A(i) < B(j)< C (k) A B(i) ~b c(k) and refinement \n). Then, for each array read reference ~n<kl<n+10 B, we start computing flow/anti dependence to/from \n B. For each apparent flow dependence from a write n<kl<n+20Akl=m #n< kl<n+10 If n ~ m ~ n + 10 had been \nasserted by the user, we would be able to verify the kill. 4.2 Covering dependence A dependence from \na write A to a read or write B is a covering dependence iff every location accessed by B is previously \nwritten to by A. If the dependence from A to B is a covering dependence, we need not examine any dependence \nto B from any accesses that would precede the writes of A (since the dependence from A would kill such \na dependence). A dependence from a write A covers B iff Vj, Sym,j G [B] * 3i s.t. i~ [A] A A(i) < B(j) \nA A(i) &#38;bB(j) In Example 2, the read of a(L2 ) is covered by the write to a(L2-1): jc [B] sl<j1<100A2<jQ<n \n1 3i s.t. ic [A]A A(i) < B(j) AA(i) ~b B(j) El~jl<100AO~j2~n-1 l~jl~100A2~jz<n 1 *l<jl<100A0~jz<n 1 \n Since we have determined that this dependence is a covering dependence, there is no need to check for \ndependencies with writes that must precede a(L2-1 ) (such as the write to a(m)). However, for writes \nthat may be executed after some executions of the cover (such as the write toa(L2)), we must check separately \nfor a kill. In general, we may have to determine which loop carries the cover to know which write accesses \nmust precede it. The cover in this example is loop in\u00addependent, so we know that all executions of the \nwrite a(Ll ) must precede the cover. Note that we only find out that the cover is Ioop independent when \nwe refine its dependence vector from (O+) to (0). For this reason, we perform refinement before coverage \nanalysis. 4.3 Terminating dependence A dependencefrom a read or write A to a write B is a terminating \ndependence iff every location accessed by A is subsequently overwritten by 13. If the dependence from \nA to B is a terminating dependence, we need not examine any dependence from A to any accesses that would \nfollow the writes of l?. A dependence from A to a write B terminates A ifi V i, sym, iG [A]+ 3j s.t.j \nE[n] AA(i) < B(j) A A(i) ~b B(j) 4.4 Refining dependence distances/directions If all iterations of a \nread or write B that receive a dependence from a write A also receive a dependence from a more recent \niteration of A within distance D, the dependence distances can be refined to D: Vi, k, Sym, iC[A]Ake \n[B] AA(i) < B(k) A A(i) ~b B(k) * A(i) <D B(k) Wj s.t.j G [A] AA(i) < A(j) <ZJ B(k) AA(j) ~b B(k) When \nwe attempt to refine dependence vectors, we do so in a way that ensures that the refined dependence cent \nains the most recent executions of A, or in other words: A(i) < l?(k) AA(i) <D B(k) AA(.j) <D B(k) + \nA(i) < A(j) In this case, we can simplify the condition under which we can perform refinement to: Vi, \nk, Sym, iE[A]Ak E[B] AA(i) < B(k) A A(i) &#38;bB(k) + 3.j s.t. j c [A] A A(j) <~ B(k) AA(j) ~b B(k) \n which can be further transformed to V k, sym, (~i s.t.i c [A] Ak e [B] AA(i) < B(k) AA(i) ~b~(k)) ~ \n(~j s.t. j c [A] A A(j) <D B(k) AA~) ~bB(k)) Example 3 shows a loop with a flow dependence that can \nbe refined from (0+,1) to (0,1): ~is.t. kG[B]Ai6 [A]A A(i) < B(k) A A(i) ~b B(k) =l~kl<nA3<kz<rn 3j \ns.t.j G [A]A A(j) <DW) AW ~b B(k) cl~kl~nA3~kz~m l~kl~nA3~kz~m til~kl~nA35kQSm Example 4 is similar \nto Example 3, but includes a triangular loop. Example 5 is similar to Example 4, but here the distance \ncan only be refined to (0:1,1) because iterations such that 1< LI = L2 receive a flow from iteration \n(LI 1, L2 1). Example 6 shows a case where the dependence distances are coupled. Neither of the two \napproaches similar to ours ([Bra88, Ros90]) would handle Examples 4, 5 or 6. The above methods allow \nus to check to see if any specific D isa correct refinement of a direc\u00adtionldistance vector. We generate \nthe D s by attempt\u00ading to fix the dependenc~ distance, starting with the outermost loop. For each loop, \nwe attempt to set the dependence distance to be the minimum possible dis\u00adtance in that loop (which is \neasily extracted from the set of constraints describing the possible (unrefined) dependence distances). \nIf we succeed, we move on to the next loop. If we fail, we stop refining that de\u00adpendence (attempts to \nfurther refine the dependence distance for inner loops would not satisfy A(i) < B(k) AA(i) <D B(k) AA(j) \n<D B(k) =$-A(i)< A(j)). This method for generating D s to test will not au\u00adtomatically find the partial \nrefinement in Example 5. 4.5 Quick tests for when to check for the above We can often avoid performing \nthe general tests de\u00adscribed above by doing some quick tests for special cases. For example, for the \ndependence from B to C to kill the dependence between A and C , there must be an output dependence between \nA and B, and it must be possible for the dependence distance from A to C to equal the total distance \nfrom A to B and BtoC. Similarly, for there to be any possibility of refining the dependence distance \nin that loop from A to C, A must have a self-output dependence with a non-zero distance in a loop in \norder. If a dependence from A to B does not include the distance O in some loop 1, it can not cover the \nexecution of B the first time through 1, so we do not test it for coverage. Note that A may actually \ncover B if B is not executed the first time through 1-we would fail to detect this cover, and be forced \nto kill the covered dependencies with the A to B dependence later. Finally, if we are trying to kill \na dependence from A to C with a covering dependence from B to C j and the dependence from B is always \ncloser than the de\u00adpendence from A, then we know the dependence from A to C is killed without having \nto perform the general test. 4.6 Related Work In analyzing false array flow data dependence (caused by \noutput dependence), there are two basic ap\u00ad proaches: e Extend scalar dataflow methods by recording \nwhich array sections are killed and/or defined [GS90, Ros90]. e Extend the pair-wise methods typically \nused for array data dependence to recognize array kills [Bra88, Rib90]. Both approaches have merits. \nOur work is an exam\u00adple of the second approach, and we believe it corrects several limitations and flaws \nin previous work on that approach. Brandes [Bra88] describes methods factoring out transitive dependence \nto determine direct depen\u00addence, and his work is similar to our computations for refinement, killing \nand covering. However, his methods do not apply if the dependence distances are coupled or the loop is \nnon-rectangular. Ribas describes [Rib90] techniques to refine depen\u00addence distances. However, Ribas \nonly discusses per\u00adfectly nested loops, and there are some problems with his Theorem 1: Given two references \nMv x + m and U.,?y + u, the refined dependence distance from x to y is constant iff ibfV = Uv,r. In \nour Example 5, we have M. = U.,, (using Ribas s terminology), but the dependence distance is not con\u00adstant. \nThe error is that (6) in [Rib90] should include (Y ~~,,(Y)) E ~nt(A, b) and (7) in [Rib90] should in\u00adclude \n(z+ d; ,T(z)) e In.t(A, b). Ribas s Theorem holds only for iterations not near the beginning or end of \nany loop. Ribas uses a slightly different definition of constant dependence distance than we do. His \ndefinition states that a dependence from A to B has constant distance d iff for all iteration vectors \ni c [A] and j c [B], there is a flow dependence from A(i) to B(j) iffj i= d. The definition we use is \nthat a dependence from A to 1? has constant distance d iff for all iteration vectors ic [A] andj c [B], \na flow dependence from A(i) to B(j) implies j i = d. While Ribas s definition is useful in the context \nof deriving VLSI designs, our definition is more appropriate for standard compiler optimizations. Rosene \n[Ros90] extended standard scalar data flow analysis techniques by using Data Access Descriptors [BK89] \nto keep track of an approximation of the set of array elements that are defined, modified and/or killed \nby each statement. Rosene only determines which lev\u00adels carry a dependence, and doesn t calculate the \nde\u00adpendence distance or direction. Thus, his approach would be unable to handle our Example 6. His use \nof Data Access Descriptors means that his techniques are approximate in situations in which our methods \nare exact. It should be possible to modify his tests to use integer programming constraints to define \nsets of array elements, but that would involve significant work beyond that described in [Ros90] (the \nOmega test could be used to represent array regions, but the Omega test cannot directly form the union \nof two sets of constraints). Rosene s techniques have not been fully implemented. Thomas Gross and Peter \nSteenkiste describe [GS90] methods similar to that of Rosene. Gross and Steenkiste s work is not as thorough \nas that of Rosene s. SUBROUTINE cHOLSKY (IDA, NMAT, M, N, A, NRHS, lDB, B) c c CHOLESKY DECOMPOSITION/SUBSTITUTION \nSUBROUTINE. c c ~1/mJad D H BAILEY MODIFIED FOR NAs KERNEL TEST c 1/28/92 W W PUGH PERFORMED FORWARD \nSUB. AND c NORMALIZED LOOP THAT HAD STEP OF .1 c REAL A(o:lDA, -M:o, o:N), B(o:NRHS, o:IDB, o:N), EPSS(O:2S6) \n DATA EPS/l E-13/ c c CHOLESKY DECOMPOSITION c DO IJ=O, N c c OFF DIAGONAL ELEMENTS c DO 21 = MAX(-M,.J), \n-1 DO 3 JJ = MAX(-M, -J)-1, -1 DO 3L= 0,NMAT 3 A(L,l,J)=A(L,I,J)-A(L,JJ,I+J)*A(L,I+JJ,J) DO 2L= 0,NMAT \n2 A(L,l,J) = A(L,l,J) * A(L,O,I+J) c c STORE INVERSE OF DIAGONAL ELEMENTS G DO 4L= O,NMAT 4 EPss(L) \n= EPS . A(L,O,J) DO 5 JJ = MAX(-M,-J), -1 DO 5L= O,NMAT 5 A(L,o,J)= A(L,o,J)-A(L,JJ,J) . *2 DO 1L= O,NMAT \nA(L,0,7)= I./5QRT (ABs (EPss(L) + A(L,o,J))) ; c SOLUTION c DO 61 = 0,NRHS D07K=0, N DO 6L= 0,NMAT 8 \nB(l,L,K) = B(I,L,K) . A(L,O,K) DO 7 J] = 1,MIN (M, N-K) DO 7L= 0,NMAT 7 B(I,L,K+JJ) = B(I,L,K+JJ) . A(L,.JJ,K+JJ) \n* B(l,L,K) c D06K=0, N DO 9L= O,NMAT 9 B(I,L,N-K) = B(I,L,N-K) . A(L,o,N.K) DO 6 JJ = 1,MIN (M, N-K) \nDO 6L= O,NMAT 6 B(I,L,N-K-JJ) = B(I,L,N-K-JJ) . A(L,-JJ,N-K) B(I,L,N-K) c RETURN END Figure 2: Source \ncode for CHOLSKY, one of the orig\u00adinal NASA NAS benchmark kernals However, they have implemented their \napproach, and obtained some experience with it. 4.7 Experimental Results We have implemented the extensions \nto the Omega test described in Section 3, and have added tests from sec\u00adtion 4 to an augmented version \nof Wolfe s tiny tool [W0191]. Our efforts to date have focused on testing flow dependence, so our changes \nhave no effect on the output or anti dependence computed, and we do not test for terminating or covering \noutput dependence. We have performed an analysis of the time taken by the Omega test to analyze dependencies. \nThese timing figures were measured on on Sun Spare IPX, and are in\u00adclusive: they include the time required \nto scan the loop bounds and subscriptions, the time required to build and analyze the array pair, and \nthe cost of overhead routines such as malloc and free. We ran our tests on the program CHOLSKY from the \noriginal NASA NAS benchmark kernals, the source files that were originally distributed all the tiny source \nfiles distributed with tiny, (which include Cholesky decomposition, LU de\u00adcomposition, several versions \nof wavefront algorithms, and several more contrived examples), as well as several of our own test programs. \nUnfortunately, FORTRAN FROH 3: A(L,I,J)  3: A(L,I,J)  2: A(L,I,J)  2: A(L>I,J)  2: A(L,I,J)  2: \nA(L,I,J)  2: A(L,I,J)  4: EPSS(L) 5: A(L,O,J)  5: A(L,O,J)  1: A(L,O,J)  1: A(L,O,J)  1: A(L,O,J) \n 8: B(I,L,K)  8: B(I,L,K)  S: B(I,L,K) 7: B(I,L,K+JJ)  7: B(I,L,K+JJ)  9: B(I,L,M-K) 6: B(I,L,E-K-JJ) \n 6: B(I,L,M-K-JJ)  TO 3: A(L,I,J) 2: A(L,I,J) 3: A(L,I+JJ,J)  3: A(L,JJ,I+J)  5: A(L,JJ,J) \n7: A(L,-JJ,K+JJ) 6: A(L,-JJ,?J-K) 1: EPSS(L) 5: A(L,O,J) 1: A(L,O,J) 2: A(L,O,I+J) 8: A(L,O,K) \n9: A(L,O,B-K) 7: B(I, L,K) 9: B(I, L,B-K) 6: B(l,L, E-K-JJ) 8: B(I, L,K) 7: B(I, L, K+JJ) 6: B(I, L,~-K) \n9: B(l,L, ~-K) 6: B(I,L, Ii-K-JJ) dirlclist status (0,0,1,0) [ r] (0,0) (o,+) (+,*) (o) [c] [cl [cl \n(o) [Cr] (0,1,0) [ r] (o) (+) [cl [cl (0,0) [cl (o) [cl (o) [cl  (0,1) [ r] (0,1,-1,0) [ r] (0,0) \n[cl (0,1) [ r] (0,1,-1,0) [ r] Figure 3: Live flow dependencies for CHOLSKY FROH 3: A(L,I,J)  3: A(L,I,J) \n 3: A(L,I,J)  3: A(L,I,J)  3: A(L,I,J)  5: A(L,O,J)  5: A(L,O,J)  5: A(L,O,J)  8: B(I,L,K) \n7: B(I,L,K+JJ)  7: B(I,L,K+JJ)  7: B(I,L,K+3J)  7: B(I,L,K+JJ)  6: B(I,L,E-K-JJ) TO 3: A(L,I+JJ,J) \n 3: A(L,JJ,I+J)  5: A(L,JJ,J) 7: A(L,-JJ,K+JJ) 6: A(L,-JJ,lJ-K) 2: A(L,O,I+J) 8: A(L,O,K) 9: \nA(L,O,E-K) 6: B(I,L,Ii-K) 7: B(I,L,IC) 9: B(I,L,E-K) 6: B(I,L,B-K)  6: B(I,L,M-K-JJ)  6: B(I,L,kK) \n  dirldist (0,+,*,0) (+,*,*,0) (o) (+) (0) (0,1,*,0) (o)  (o)  (o) (0,1,*,0)  status [ id [ \nk] [ k] [ k] [ k] [ k] [ k] [ k] [ cl [ kr] [ k] [ c1 [ Kl [ kr] Figure 4: Dead flow dependencies for \nCHOLSKY programs must be translated by hand to restricted lan\u00adguage before tiny can analyze them, and \nso we have been unable to to try our analysis methods on large benchmarks. However, we feel the data \npresented here gives a good feel for the range of analysis times, even if it cannot be used to predicate \nan average analysis time. 4.7.1 Effects on flow dependence We have found that, in many cases, the techniques \nde\u00adscribed here significantly simplify the set of flow de\u00adpendence, Figure 2 shows our version of the \nNAS kernel test that performs the Cholesky decomposition of a set of banded matrices. We have modified \nthe original test by forward-substituting the expression ikfAX(-A4, J), which was originally computed \nat the top of the J loop, and by normalizing the second K loop (which had a negative step). Figure 3 \nlists all of the flow dependencies that are the Omega test has determined are live (i.e., not dead). \nThe dependence in Figure 3 that have been refined are marked with the suffix [r]. Those that cover their \nread have been marked with [C] (note that this tag does not affect the dependence itself, but shows that \nit can be used to eliminate other dependence). The flow dependence in Figure 4 are dead: because of an \nintermediate write, no actual data flows from the source to the destination. These dependencies have \nbeen eliminated by being either covered (marked with [c]) or killed ([k]). Almost all other dependence \nanalysis algorithms would report these as true flow dependen\u00adcies. 4.7.2 Efficiency Figure 6 shows timing \nresults for 417 write/read ac\u00adcess pairs in CHOLSKY and a variety of other test programs. The graph on \nthe left compares the time needed to perform an extended analysis of an array pair that includes checking \nfor refinement and cover\u00adage against the cost of standard analysis (no checks for refinement or covering). \nBoth times were obtained us\u00ading the Omega test to perform the analysis. The solid lines show y = x, y \n= 2x, and y = 4x. In 264 cases, the extended capabilities of the Omega test were not needed (i ,e,, we \ncould determine that refinement and coverage were not possible without needing to consult the Omega test). \nThe 81 #s show the cases in which we performed the general test for either covering or refinement on \none flow dependence vector. These gen\u00aderally took 2 or 3 times the amount of time needed to generate \nthe dependence. The 72 O s show the cases in which we performed the general test for covering or refinement \nand in which we were forced to split the dependence into several dependence vectors. The graph on the \nright of Figure 6 contains one point for each pair of dependence to the same read access (that is, one \npoint for each possible kill). It compares the time needed to test for a kill (horizontal axis) to the \ntime needed to generate and perform coverage and refinement testing of the dependence being killed (ver\u00adtical \naxis). The solid line shows y = z. Note that one point on the left graph may correspond to zero, one, \nor many points on the right. The 284 points with kill time below 0.3 msecs correspond to cases in which \nwe did not have to consult the Omega test to perform a kill test; there were 54 cases in which the Omega \ntest was consulted, leading to kill test times of longer than 0.3 msecs. In Figure 7, we show the time \nrequired to perform standard and extended analysis of each array pair (ex\u00adtended analysis includes checks \nfor refinement, covering and killing). The timing results were sorted according to the times required \nfor extended analysis. s Symbolic dependence analysis A data dependence may only exist if certain variables \ntake on particular values. In Example 7, there is a flow dependence iff2x~n Al~y~m A (x> OVX= O A y < \nm). We can determme the set of constraints under which a dependence exists by setting up the in\u00adteger \nprogramming problem describing when there is a dependence, and projecting the problem onto the set of \nloop-invariant scalar variables. If we are interested in the set of constraints under which a flow dependence \nexists, we first determine a set of restraint vectors (Sec\u00adtion 2.1 .2) for the flow dependence, assuming \nnoth\u00ading about the symbolic variables. We then add the restraint vectors to the integer programming problem \nbefore projecting onto the symbolic variables. Alternatively, we can add any user assertions about the \nrelations between variables to any integer program\u00adming problem involving those variables. What if we \nhave some information about relations between variables, but not enough to rule out a de\u00adpendence? We \nuse our ability to compute gists (Sec\u00adtion 3.3) to determine the appropriate concise queries to make \nof the user, given what we already know. We consider the analysis of Example 7, in the circumst ante \nthat that the user has asserted that all array references are in bounds, 50 ~ n ~ 100 and no additional \ninfor\u00admation is available about n. There are two apparent restraint vectors for this dependence: (+, \nx) and (O, +). For the first restraint vector (which corresponds to a dependence carried by the outer \nloop), we define p as the things that are known plus the fact that there the outer loop has multiple \niterations (which must be true in order for a dependence carried by that loop to be interesting). We \ndefine q to be the additional things we known when there actually is a dependence. The constraints that \ndefine p and q for this example are shown in Figure 5. The conditions on variables (other than n) under \nwhich this dependence actually exists, real A[l:n,l:m], C[l:n,l:m] . . . int q[l:n] for LI :=xto ndo \n... for L2 := 1 to mdo for L1 :=1 tondo AIL1 ,L2] := AIL1-x,y] + CIL1, L21; AIQILI]] := AIQIL1+l]-1] \n+ CIL1]; Example 7 Example8 for i :=ltondo for b := 1 to maxB k := i*(i-1)/2+i for i := B[b] to B[b+l]-1 \nfor i :=ltondo for j := i to n do for j := B[b] to B[b+l]-1 for j :=ltondo a [k] := a[k] +bb[i,j] A[i,j] \n:= ... A[i*j] := ... k := k+j Examp1e9 Example10 Examplell x~il<jl~n loop bounds and restraintvector \nl~iz,jz~m ) P = l~il,jl x~n array references in bounds l~iz,y<m {1 ) 50~n<100 user assertion 1 il \n= jl x q= dependence exists iz=y {() } Figure 5: Set-up of equations for symbolic analysis of outer \nloop carried dependence Example 7 given that we know p, are given by (gist mx,y,m(p A We then determine \nthat: q) given mx,y,m(~)) = {1 < x < 50}. For the restraint (gist n,,,,, ~,,Q,,,n(ZJ A q) given fi,,,~,~,,~,,,n(p)) \n vector (O, +), we similarly compute that it exists only ~ Q, c Q,, if {x = OA y < m}. We can then ask \nthe user whether or not this condition must always hold. This would prompt us to ask the user the following: \nWhat about expressions other than scalar loop- Is it the case that for all a &#38; b such that invariant \nvariables (such as ixj or P [i]) that appear in 1 <= a < b c= n, the following never happens? a subscript \nor loop bound? In this case, we add a dif\u00adferent symbolic variable for each appearance of the ex\u00ad Q[al \n= Q[b] pression. If the expression is parameterized by a set of other symbolic variables, we also introduce \nadditional If the user answers yes, we rule out an output depen\u00adsymbolic variables for those parameters. \nWe can now dence and add Va&#38;b s.t. 1s a< b~ n, Q[a] # Q[b] as use the methods described above to \nask the user queries an assertion. about the relations between these symbolic variables. Checking for \na flow dependence would produce the query: In Example 8, we first check for an output depen\u00addence, assuming \nnothing about Q. This leads to an Is it the case that for all a k b such that output dependence with \ndirection/restraint vector (-l-). 1 <= a < b-1 <= n-1, the following never happens? We next take the \nset of constraints for determining if there is a dependence and, constraints that enforce the Q[a] = \nQ[b]-1 restraint vector (+), and add variables for the index Instead of answering such a question directly, \nthe array subscripts (sl and S2) and the index array values user may choose to tell us more specifically \nwhat prop\u00ad (Q,, and Q,,). Given that all array references are in erties the array has. For example, the \nuser might tell us bounds and the dependence haa a restraint vector (+), that the array is strictly increasing, \nor is a permutation we set-up p and q as: array. This has the advantage of being more natural to the \nuser, and possibly supplying more information l~il<jl<n than a yeslno answer would. p= l~Q$, Q,l, s,s \n~n By applying these techniques, we can handle a wide s =il As =jl range of situations. These techniques \napply directly { 1 to situations where array values appear in loop bounds q={ Qs=Qs~} (such as Example \n9). We handle non-linear terms (such as i*j in Example 10) as an array indexed by all the non-constant \nvariables. In other words, a term i*j would be treated as an array Q[i, j], with the actual term substituted \nwhenever conducting a dialogue with the user. By adding additional algorithms that perform non-linear \ninduction variable recognition and recognize summations and by knowning appropriate linear con\u00adstraints \non summations, these techniques allow us to handle Example 11 (from program S141 of [LCD91]), which could \nnot be handled by any compiler tested by [LCD91]. 5.1 Related Work Methods for incorporating assertions \nabout invariant scalar variables into dependence analysis algorithms and producing queries to ask the \nuser have been part of the compiler folklore for some time (see [HP9 I] for a recent discussion). However, \nprevious work has not addressed how to ask concise questions given that some information is already known. \nKathryn McKinley [McK90] describes how to handle index arrays in dependence analysis. Her work enumer\u00adates \nmany typical cases and discusses how each can be handled. It is not a general purpose method and can\u00adnot \nhandle cases such as array values in loop bounds or complicated subscripts of index arrays. Special purpose \nmethods may prove useful from an efficiency viewpoint for dealing with typical, common cases. Our goal \nhere is to describe as general a method as possible to fall back on. 6 Availability An implementation \nof the Omega test is freely avail\u00adable for anonymous ftp from f tp. cs. umd. edu in the directory pub/omega. \nThe directory contains a stand\u00adalone implement ation of the Omega test, papers de\u00adscribing the Omega \ntest, and an implementation of Michael Wolfe s tiny tool [WO191] augmented to use the Omega test as described \nin this paper. 7 Conclusions We have shown how the Omega test can be extended and utilized to answer \na wide range of questions that previous analysis methods could not address. The pri\u00admary questions we \nconsidered are array kills, handling assertions and generating a dialog about the values of scalar variables, \nand handling assertions and generating a dialog about array values and non-linear expressions. While \nprevious methods could handle special cases of the problems considered here, our work describes much \nmore general methods. Previous approaches to these problems have not been widely implemented. By taking \nadvantage of the power of the Omega test, we have been able to add these ad\u00advanced data dependence analysis \ncapabilities with rel\u00adatively modest implementation investment. We hope that our approach will lead to \na more widespread in\u00adcorporation of these capabilities in compilers and in\u00adteractive analysis tools. \n8 Acknowledgements Thanks to Udayan Borkar and Wayne Kelly for their help in obtaining the experimental \nresults and their comments on the paper. Also, special thanks to Michael Wolfe for making his tiny program \nfreely available, References [BK89] Vasanth Balasundaram and Ken Kennedy. A technique for summarizing \ndata access and its use in parallelism enhancing transformations. In A Ckf SIGPLA N 89 Con~er-enceon \nProgram\u00adming Language Design and Implementation, 1989. [Ble75] W,W. Bledsoe. A new method for proving \ncer\u00adtain presburger formulas. In Advance Papers, lth Int. Joint Conference on Arti\\. Intell,, Tibil\u00adisi, \nGeorgia, U.S.S.R, 1975. [Bra88] Thomas Brandes. The importance of direct de\u00adpendence for automatic parallelism. \nIn Proc of 1988 International Conference on Supercom\u00adprdirzg, July 1988. [CP91] D. Y. Cheng and D. M. \nPase. An evaluation of automatic and interactive parallel programming tools. In Supercomputing 9I, November \n1991. [DE73] G.B. Dantzig and B.C. Eaves. Fourier-Motzkin elimination and its dual. Journal of Combinat\u00ad \norial Theory (A), 14:288 297, 1973. [GS90] Thomas Gross and Peter Steenkiste. Structured dataflow analysis \nfor arrays and its use in an optimizing compiler. Soflware -Practice and lllzperience, 20:133-155, February \n1990. [HKK+91] M. W. Hall, T. Karvey, K. Kennedy, N. McIntosh, K.S. McKinley, J. D. Oldham, M. Paleczny, \nand G. Roth. Experiences from the parascope editor workshop. Technical Re\u00adport RICE COMP TR91-173, Dept. \nof Com\u00adputer Science, Rice University, September 1991. [HP91] Mohammand Reza Haghighat and Consta\u00adntine \nD. Polychronopoulos. Symbolic depen\u00addence analysis for high-performance paralleliz\u00ading compilers. In \nAdvances in Languages and Compilers for Parallel Processing. The MIT Press, 1991. [JM871 Farnam Jahanian \nand Aloysius Ka-Lau Mok. A graph-theoretic approach for timing analysis and its implement ation. IEEE \nTransactions on Computers, 1987. 10 10 7 5 5 analysis ~ 3 with refinement and covering 1 1 (rnsecs) \n0.5 no r/c checks needed . 0.5 0.3 0.3 direction vector split ~ and r/c tests performed 0.1v I I I I \nI I I ! 0.1 0.1 0.2 0.30.40.5 1 2 34 0.02 0.05 0.1 0.5 1 3 5 10 standard analysis (mzec) killtime (msecs) \n Figure 6: Relative times of standard analysis, analysis time including computing refinement and covering \ninform\u00ad ation,and time to compute killing information [KPK90] [LCD91] [MCK90] [Pug91] [Rib90] [Ros90] \n[Sho77] [SLY89] [W0189] David Klappholz, Kleanthis Psarris, and Xi\u00adangyun Kong. On the perfect accuracy \nof an approximate subscript analysis test. In Super\u00adcomputing 90, November 1990. David Levine, David \nCallahan, and Jack Don\u00adgarra. A comparative study of automatic vec\u00adtorizing compilers. Technical Report \nMCS\u00adP218-0391, Argonne National Laboratory, April 1991. Kathryn S. McKinley. Dependence analysis of arrays \nsubscripted by index arrays. Technical Report RICE COMP TR91-162, Dept. of Com\u00adputer Science, Rice University, \nDecember 1990. William Pugh. The omega test: a fast and practical integer programming algorithm for de\u00adpendence \nanalysis. In Supercomputing 91, Nov 1991, To appear in Communications of the ACM. Hudson Ribas. Obtaining \ndependence vectors for nested-loop computations. In l%oc of 1990 International Conference on Parallel \nProcess\u00ading, August 1990. Carl Rosene. Incremental Dependence Analysis. PhD thesis, Dept. of Computer \nScience, Rice University, March 1990. Robert E. Shostak. On the sup-inf method for proving presburger \nformulas. Journal of the ACM, 24(4):529-543, October 1977. Z. Shen, Z. Li, and P. Yew. An emperical stu\u00addent \nof array subscripts and data dependence. In Proc of 1989 International Conference on Parallel Processing, \nAugust 1989. Michael Wolfe. Optimizing Supercompilers for Supercomputers. Pitman Publishing, London, \n1989. [W0191] Michael Wolfe. The search tool. In Proc tiny loop restructuring re\u00adof 1991 International \nCon\u00ad ference on Parallel Processing, 1991. [ZC91] Hans Zima and Barbara Chapman. Supercotn\u00adpilers for \nParallel and Vector Computers. ACM Press, 1991. 20 I I I I I I I II 10 extended analysis time standard \nanalysis time 5 3 (=*CCS) 1 0.5 0.3 ....\u00ad . .. . . ------ IIIIIII  0.1 1t 1 50 100 150 200 250 300 \n350 400 Figure 7: Graph of analysistime per array pair,with and without extended analysis, sorted in \norder of ex\u00adtended analysis time   \n\t\t\t", "proc_id": "143095", "abstract": "<p>Array data dependence analysis methods currently in use generate false dependences that can prevent useful program transformations. These false dependences arise because the questions asked are conservative approximations to the questions we really should be asking. Unfortunately, the questions we really should be asking go beyond integer programming and require decision procedures for a sublcass of Presburger formulas. In this paper, we describe how to extend the Omega test so that it can answer these queries and allow us to eliminate these false data dependences. We have implemented the techniques described here and believe they are suitable for use in production compilers.</p>", "authors": [{"name": "William Pugh", "author_profile_id": "81100057068", "affiliation": "", "person_id": "PP15020758", "email_address": "", "orcid_id": ""}, {"name": "David Wonnacott", "author_profile_id": "81100556244", "affiliation": "", "person_id": "P64419", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143129", "year": "1992", "article_id": "143129", "conference": "PLDI", "title": "Eliminating false data dependences using the Omega test", "url": "http://dl.acm.org/citation.cfm?id=143129"}