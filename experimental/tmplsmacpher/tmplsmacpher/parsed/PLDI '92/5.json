{"article_publication_date": "07-01-1992", "fulltext": "\n A Customizable Substrate for Concurrent Languages Suresh Jagannathan Jim Philbin NEC R(esearch Institute \n41ndependence Way Princeton, NJ 08540 {suresh I philbi,n}>@research .nj .nec. com Abstract We describe \nan approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) program\u00adming \nlanguages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads \nof control and virtual processors as first-class objects, Given the significant degree to which the behavior \nof these ob\u00adjects may be customized, we can easily express a variety of concurrency paradigms and linguistic \nstructures within a common framework without loss of efficiency. Unlike parallel systems that rely on \noperating system ser\u00advices for managing concurrency, STING implements con\u00adcurrency management entirely \nin terms of Scheme obje,cts and procedures. It, therefore, permits users to optimize the runtime behavior \nof their applications without requ~ir\u00ading knowledge of the underlying runtime system. This paper concentrates \non (a) the implications of the design for building asynchronous concurrency structures, (b) organizing \nlarge-scale concurrent computations, and (c) implementing robust programming environments for symbolic \ncomputing.  Introduction Thegrowing interest inparallel computing has led to the creation of a number \nof parallel programming languages that define explicit high-level program and data structures for expressing \nconcurrency, Parallel languages targeted for non-numerical application domains typically support (unvarying \ndegrees of efficiency) concurrency structures that realize dynamic lightweight process creation[13, 15] \nhigh-level synchronization primitives[28, 29], distributed datastructures[6], andspeculative concurrency \n[8,25]. In effect, all these parallel languages may be viewed as con\u00adsisting of two sublanguages a coordination \nlanguage re\u00adsponsible for managing and synchronizing the activities of a collection of processes, and \na computation language responsible for manipulating data objects local to a given process. Permission \nto copv without fee all or part of this material is granted provided thet tha copias are not made or \ndistributed for diract commercial advantage, the ACM copyright notice and the titla of the publication \nand its data appear, and notica is given that ccpying is by permission of the Association for Ccmputing \nMachinery. To copy otherwise, ortc republish, requiras afea andlor specific permission. ACM SIGPLAN 92 \nPLD1-6/92/CA @1992 ACM 0-89791 -476 -7/92 /0006 /0055 ...$1 .50 55 In this paper, we describe the implementation \nof a coor\u00ad dination substrate that permits the expression of a wide range of concurrency structures within \nthe context of a symbolic computation language. Our intention is to de\u00adfine a general-purpoae coordination \nmodel on top of which a number of speciaHzed coordination languages can be ef\u00adficiently implemented. \nWe use Scheme[27] as our compu\u00adtation base, We emphasize, however, that the design of the substrate could \nbe incorporated into any high-level symbolic programming language. One obvious way of implementing a \nhigh-level parallel lan\u00adguage is to build a dedicated (user-level) virtual machine. The virtual machine \nserves primarily as a substrate that implements the high-level concurrency primitives found in the coordination \nsublanguage. Given a coordination language L supporting concurrency primitive P, the role of L s virtual \nmachine (Lp ) is to handle all implement a\u00adtion aspects related to P; this often requires that the ma\u00adchine \nmanage process scheduling, storage management, synchronization, etc. Becauae Lp only is tailoredtowards \nefficient implementation of P, however, it is often unsuit\u00adable for implementing significantly different \nconcurrency primitives. Thus, to build a dialect of L with concur\u00adrency primitive P usually requires \neither building a new virtual machine or expressing the semantics of P using P, Both approaches have \ntheir obvious drawbacks: the firat is costly to implement given the complexity of imple\u00admenting a new \nvirtual machhte; the second is inefficient given the high-level semantics of P and Lp s restricted functionrWy. \nRather than building a dedicated virtual machine for im\u00adplementing concurrency, a language implementation \nmay use low-level operating system services [5, 30], Process cre\u00adation and scheduling ia implemented \nby creating a heavy\u00ador lightweight OS-managed thread of control; synchro\u00adnization is handled using low-level \nOS-managed struc\u00adtures. These implementations tend to be more portable and extensible than systems built \naround a dedicated run\u00adtime system, but they necessarily sacrifice efficiency [2] since every (low-level) \nkernel call requires a context switch between the application and the operating system. More\u00adover, generic \nOS faciHties perform little or no optimization at either compile time or runtime since they are usually \ninsensitive to the semantica of the concurrency operators of int crest, The dialect of Scheme described \nhere (called STING) includes a coordination language (implemented via a dedicated virtual machine) for \nexpressing asynchronous lightweight concurrency that combines the best of both approaches, In contrast \nto other parallel Scheme systems[12, 13, 19] and parallel dialects of similar lligh\u00adlevel languages[lO, \n28], the basic concurrency objects in STING (threads and virtual processors) are streamlined data structures \nwith no complex synchronization or value transmission semantics, Unlike parallel systems that rely on \nOS services for managing concurrency, STING imple\u00adments all concurrency management issues in terms of \nScheme objects and procedures, permitting users to, op\u00adtimize the runtime behavior of their applications \nwithout requiring knowledge of underlying 0;S services. We ar\u00adgue that STING supports the features essential \ntocrea,ting and managing various forms of asynchronous parallelism within a conceptually unified, very \ngeneral framework. Our results show that it is possible to build an efficient substrate upon which various \nparallel dialects of high\u00adlevel symbolic languages can be built. STING is not in\u00adtended merely to be \na vehicle that implements stand\u00adalone short-lived programs, however. We envision this system as providing \na framework for building a rich ,pro\u00adgramming environment for parallel symbolic computing. In this regard, \nthe system provides support for thread preemption, per-thread asynchronous garbage collection, exception \nhandling across thread boundaries, and appli\u00adcation dependent scheduling policies. In addition, it con\u00adtains \nthe necessary functionality to han,dle persistent lcmg\u00adlived objects, multiple address spaces and other \nfeatures commonly associated with advanced programming envir\u00adonments. This paper concentrates on the \nimplications of the STING design for building asynchronous concurrency structures, organizing large-scale \nconcurrent computations! and im\u00adplementing robust programming environments. A detailed description of \nits implementation is given in [18]. The pa\u00adper is structured as follows. In the ne>ct section, we give \nan overview of STING focusing primarily on the structure of the coordination model. Section 3 describes \nthe thread and virtual processor abstractions. Section 4 describes the dynamics of thread execution and \nsynchronization in the context of implementating of several well-known concurrency paradigms (e.g., result-parallel \n(fine-grained) parallelism[3], master-slave computations[l 1], speculative concurrency, and barrier synchronization). \nWe argue that despite obvious syntactic and methodological differences, these paradigms all impose common \nrequirements on the underlying runtime system: they require throttling of dy\u00adnamically generated processes, \ncheap synchronization, ef\u00adficient storage management, and the ability to treat pro\u00adcesses as bona fide \ndata objects. Section 5 presents so]me performance figures, and comparison to related work is given in \nSection 6. 2 The Context Four features of the STING design, when taken as a whole, distinguish the system \nfrom many other symbolic parallel languages: 1. The Concurrency Abstraction: concurrency is ex\u00adpressed \nin STING is via a lightweight thread of cc,n\u00ad trol. A thread is a non-strict first-class data struc\u00adture \nthat superficially resembles the object created by a MultiLisp future[13], for example, We elabo\u00adrate \nupon the differences in the following section. 2. The Processor and Policy Abstractions: Threads ex\u00adecute \non a virtual processor (VP) that represents an abstraction of a physical computing device. There may \nbe many more virtual processors than the ac\u00adtual physical processors available. Like threads, vir\u00adtual \nprocessors are also first-class objects. A VP is closed over a policy manager that determines the scheduling \nand migration regime for the threads that form a virtual machine. A virtual machine is closed over an \naddress space managed by its virtual pro\u00adcessors, Multiple virtual machines can execute on a single physical \nmachine consisting of a set of physi\u00adcal processors, Virtual machines are also denotable Scheme objects \nand may be manipulated as such. 3. Storage Model: A thread allocates data on a stack and heap that it \nmanages exclusively. Thus, threads garbage collect their state independently of one an\u00ad  it executes, \nDifferent VPS can be closed over differ\u00ad ent policy managers without incurring any perfor\u00ad mance penalty. \nA collection of virtual processors can be combined to other; no global synchronization is necessary \nin or\u00adder for a thread to initiate a garbage collection. Data may be referenced across threads. Inter-area \nreference information maintained by areas is used to garbage collect objects across thread boundaries[4, \n26]. Storage is managed via a generational scaveng\u00ading collector[21, 32]; long-lived or persistent data \nallocated by a thread thread is accessible to other threads in the same virtual machine. The design is \nsensitive to storage locality concerns; for example, storage for running threads are cached on VPS and \nare recycled for immediate reuse when a thread terminates. Moreover, multiple threads may share the same \ndynamic context whenever data de\u00adpendencies warrant, 4. The Pvowarn Model: STING permits exceptions to \nbe handled-across threads, supports non-bl~cking 1/0, permits the scheduling of virtual processors on \nphys\u00adical processors to be customizable in the same way that the scheduling of threads on a virtual processor \nis customizable, and provides an infra-structure for implementing multiple address spaces and long-lived \nshared persistent objects. 3 Threads and Virtual Processors The computation sublanguage of STING is \nScheme[27], a higher-order lexically scoped dialect of Lisp. The compiler used is a modified version \nof 0rbit[20]. The main components in the coordination sublanguage of STING are lightweight threads of \ncontrol and virtual pro\u00adcessors. Threads are simple data structures that encapsu\u00adlate local storage (e.g., \nregisters, stack and heap organized into areas), code, and relevant state information (e.g., sta\u00ad tus, \npriorities, preemption bits, locks, etc.). They define a separate locus of control. The code associated \nwith a thread is executed for effect, not value; thus, threads do not adhere to any particular synchronization \nprotocol. The system imposes no constraints on the code encapsu\u00adlated by a thread: any valid Scheme expression \ncan be treated as a distinct process. Each virtual processor (VP) isclosed over a(l) a thread controller \nthat implements a state transition function on threads, and (2) a policy manager that implements both \na thread scheduling and thread migration policy. A VP is also closed over a set of registers and interrupt \nhan\u00addlers. Most of the dedicated registers found in the VP are managed by Orbit; these include a register \ncontaining the return address of the current procedure, registers to hold thearguments to a procedure, \nand registers that re\u00adfer to the top-of-stack and top-of-heap, Another register points to the currently \nexecuting thread. Besides vari\u00adous handlers and a policy manager, a VP also retains the identity of its \nvirtual machine, and the physical processor on which it is executing. Virtual processors are multiplexed \non physical processors in the same way that threads are multiplexed on virtual processors; associated \nwith each physical processor is a policy manager that dictates the scheduling of the virtual processors \nwhich execute on it. 3.1 Threads Threads are first-class objects in STING. Thus, they may be passed as \narguments to procedures, returned as results, and stored in data structures. Threads can outlive the \nob\u00adjects that create them. A thread is closed over a thunk, anullary procedure, that is applied when \nthe thread exe\u00adcutes. The value of the application is stored in the thread on completion. The static \ncomponent of a thread also contains state in\u00adformation on the thread s status. A thread can be either \ndelayed, scheduled, evaluating, stolen or determined, A delayed thread will never be run unless the value \nof the thread is explicitly demanded. A scheduled thread is a thread scheduled to evaluate on some virtual \nprocessor but which has not yet started executing. An evaluating thread is a thread that has started \nrunning. A thread remains in this state until the application of its thunk yields a result, At this point, \nthe thread s state is set to determined. Stolen threads are discussed in Section 4.1. In addition to \nstate information and the code to be eval\u00ad uated, a thread also holds references to 1. other threads \nwaiting for it to complete, 2. references to the thunk s dynamic and exception en\u00advironment, and 3. \ngenealogy information indicating the thread s parent, siblings, and children.  Dynamic and exception \nenvironments are used to imple\u00adment fluid bindings and inter-process exceptions. Geneal\u00adogy information \nserves as a useful debugging and profiling tool that allows applications to monitor the dynamic un\u00adfolding \nof a process tree. Evaluating threads are associated with a dynamic context called a thread control block \n(TCB). Besides encapsulat\u00ad ing thread storage (stacks and heaps), the TCB contains information about \nthe current state of the active thread (e.g., is the thread currently running on some VP, is it blocked, \nsuspended, terminated, etc?), requested state transitions on this thread made by other threads, the cur\u00adrent \nquantum for the thread, and the virtual processor on which the thread is running. TCBS hold some other \nin\u00adformation for implementing speculative and barrier syn\u00adchronization that we discuss in Section 4.3. \n The implementation of threads requires no alteration to the implementation of other primitive operations \nin the language. The synchronization semantics of a thread is a more general (albeit lower-level) form \nof the synchro\u00adnization facility available via e.g., MultiLisp)s touch , Linda s tuple-space[7], or CML \nS sync [28]. The ap\u00adplication completely control the condition under which blocked threads may be resumed. \nHowever, there is ex\u00adplicit system support for dataflow (i. e., future-touch), non\u00addeterministic choice, \nand constraint-based or barrier syn\u00adchronization, Users manipulate threads via a set of procedures (listed \nbelow) defined by a thread controller (TC) that imple\u00adments synchronous state transitions on thread state. \nThe TC is written entirely in Scheme with the exception of a few primitive operations to save and restore \nregisters. The thread controller allocates no storage; thus, a TC call never triggers garbage collection. \nBesides these oper\u00adations, a thread can enter the controller because of pre\u00ademption. (fork-thread expr \nVP) creates a thread to -hate expr, and schedules it to run on VP. (create-thread exyw) creates a dela~edthread \nthat when demanded evaluates espr. (thread-run thread vp) inserts a delayed, blocked or suspended thread \ninto the ready queue of the policy manager for up. (thread-wait thread) causes the thread executing \nthis operation to block until thread s state be\u00adcomes determined. (thread-value thread) returns thevrdue \nof theap\u00adplication associated with thread. (thread-block thread . blocker) requests thread to block; \nb!ockeris the condition on which the thread is blocking. (thread-suspend thread . quantum) requests \nthread to suspend execution. If the quantum argument is provided, the thread is resumed when the period \nspecified has elapsed; otherwise, the thread is suspended indefinitely until it inexplicitly resumed \nusing thread-run. (thread-terminate thread . values) requests thread to terminate with vaJues as its \nresult.1 lA~ in some other scheme dialects, expressions can yield l\u00adtiple values. Thread state Genealogy \n?ther ThUnk Anoiher A 1111 ~ b cad Thread + Stale P krreti Thread LHeap GHeap wad  VP TCB 1.!!2-IL32H!31 \n..... ..... ..............,,..,,,.., ................ ..... ................,..,,...,,,,,,,,.,,.,... \n,.,, .,...,,.,.,,,....-.,.. ., I + cn Shared older I ,, 111,8 VP State Hesp generation J lInterrupt \nHandlers r Registers 1-Stack I AREAS d--l Thread Storage( Root Environment ) Vh-tua6 Machine Figure \n1: Threads and Virtual Processors (yield-processor) finquish control into a suitable causes thecurrent \nthread tore\u00adof its VP. The thread is inserted ready queue. (current-thread) returns thethread executing \nthis operation. 3.1,1 A Simple Example To illustrate how users might program with threads, con\u00adsider \nthe program shown in Fig. 2 that defines a Sieve of Erasthosenes prime finder implementation. Note that \nthe definition makes noreference toany partic\u00adular concurrency paradigm; such issues are abstracted by \nits op argument. This implementation relies on a user-defined synchroniz\u00ading stream abstraction that \nprovides a blocking operation on stream access (hd) and an, atomic operation for ap pending to the end \nof a stream ( attach). (define (filteropn input) (let loop ((x (hd input)) (output (make-stream)) (last? \ntrue)) (cond ((zero? (mod xn)) (loop (rest input) output last?)) (last? (op (lambda () (filter OP n output))) \n(loop (rest input) (attachx output) false)) (else (loop (rest input) (attachx output) last?))))) We can \ndefine various implementations ber finder that exhibit different degrees behavior. For example, (let \n((filter-list (list))) (sieve (lambda (thunk) (set filter-list (cons (create-thread filter-list))) n)) \nof a prime num\u00adof asynchronous (t.hunk)) (define (sieve opn) (let ((input (make-integer-stream (op (lambda \n() (filterop 2 input))))) Figure2: An abstraction ofa concurrent n))) prime finder. defines an implementation \nin which filters are generated lazily; once a demanded, a filter repeatedly removes ele\u00admentsoff its \ninput stream, and generates potential primes onto its output stream. To initiate anew filter scheduled \non a VP using a round-robin thread placement discipline, we might write: (thread-run (car filter-list) \n(mod (1+ (vm. vp-vector (current-vp). vm)) n)) (Current-vp) returns the VP on which the expression is \nevaluated; ((current-vp).vm defines the virtual mac\u00adhine of which the current VP is a part, A virtual \nma\u00adchine s public state includes avector containing its virtual processors. By slightly rewriting the \nabove call to sieve, wecanex\u00adpress amore lazy implementation: (let ((filter-list (list))) (sieve (lambda \n(thunk) (set filter-list (cons (create-thread (block (map thread-unblock filter-list) (thunk))) filter-list)) \n(map thread-block filter-list)) n)) In this definition, a filter that encounters a potential prime p, \ncreates a lazy thread object L and requests all other fil\u00adtersin the chain to block. When L svalue is \ndemanded, it unblocks all the elements in the chain, and proceeds to filter all multiples ofp on its \ninput stream. This im\u00adplementation throttles the extension of the sieve and the consumption ofinput based \non demand. We can also define an eager version of the sieve se follows: (sieve (lambda (thunk) (fork-thread \n(thunk))) n) Evaluating this application schedules a new thread re\u00adsponsible for filtering all multiples \nof a prime. This simple exercise highlights some interesting points about the system. First, STING treats \nthread operations as ordinary procedures, and manipulates the objects ref\u00aderenced by them just as any \nother Scheme object; if two filters attached viaacommon stream are terminated, the storage occupied by \nthe stream maybe reclaimed. STING imposes no a priori synchronization protocol on thread access -application \nprograms are expected to build ab\u00ad stractions that regulate the coordination of threads. The threads \ncreatedby filter maybe terminated inone of two ways. The top-level call to sieve may be struc\u00ad tured \nso that it has an explicit handle on these threads; the filter-list data structure used to create a lazy \nsieveis such an example. One can then evaluate: (map thread-terminate filter-list) to terminate all threads \nfound in the sieve. STING also provides thread groups as a means of gaining control over a related collection \nof threads[19]. A thread group is closed over debugging and thread operations that may be ap\u00adplied en \nmasse to all of its members. Every thread has a thread group identifier that associates it with a given \ngroup. Thread groups provide operations analogous to ordinary thread operations (e.g., termination, suspension, \netc.) aswell as operations for debugging and monitoring (e.g., resetting, listing all threads in a given \ngroup, listing all groups, profiling genealogy information, etc..) Thus, when the thread T under which \nthe call to sieve is ter\u00adminated, users can request all of T s children (which are defined to bepartof \nT sgroup to be terminated) thus: (kill-group (thread.group T)) Second, lazy threads are distinguished \nfrom scheduled ones. A lazy thread defines a thread object closed over a thunk and dynamic state (but \nwhich is unknown to any virtual processor). A scheduled thread is also a lightweight data structure, \nbut is known to some VP and will eventually be assigned aTCB. Applications can choose the degree of laziness \n(or eagerness) desired. Only the thread controller can initiate a thread transition to evaluatirag -the \ninterface does not permit applications to insist that any specific thread immediately run on some virtual \nprocessor, All default policy managers implement afair scheduling policy, but STING imposes no constraints \non user-defined policy managersin this regard. Third, threads can request state changes to other threads; \nthechange itself takes place only when the target thread next makes a TC call (either synchronously or \nbecause of preemption). Requested state changes to a thread T made by another T are recorded as part \nof T s next state in its TCB, State changes are recorded only if they do not violate the state transition \nsemantics (e.g., eva(uat\u00ading threads cannot be subsequently sclwdule~ terminated threads cannot become \nsubsequently Mocked, etc.), and therequesting thread has appropriate authority. Only threads can actually \neffect a change to their own state. This invariant implies that a TCB can perform a state transition \nwithout acquiring locks. 3.2 Virtual Processors Virtual processors (and by extension, virtual machines) \nare first-class objects in STING. According first-class sta\u00adtus to VPS has several important implications \nthat distin\u00adguish STING from other high-level thread systems[9, 10] or other asynchronous parallel languages. \nFirst, one can or\u00adganize parallel computations by explicitly mapping pro\u00adcesses onto specific virtual \nprocessors. For example, a process P known to communicate closely with Q should execute on a VP topologicrdly \nnear V. Such considera\u00adtions can be expressed in STING since VPS can be directly enumerated. Systolic \nstyle programs for example can be expressed by using self-relative addressing off the current VP (e.g., \nleft-W, right-VP, up-Vp, etc.). Thesystem provides a number of default addressing modes for many common \ntopologies (e.g., hypercubes, meshes, systolic ar\u00adrays, etc.). Furthermore, since VPS can be mapped onto \nspecific physical processors, the ability to manipulate vir\u00adtual processors as first-class data values \ngives STING pro\u00adgrammers a great deal of flexibility in expressing different parallel algorithms that \nare defined in terms of specific processor topologies[16]. More significantly, since VPscan be closed \nover different virtual policy managers, different groups of threads cre\u00adatedby an application maybe subject \ntodifferent schedul\u00ading regimes. Virtual machines or VPs can be tailored to handle different scheduling \nprotocols or policies. Wedis\u00adcuss the implications of customizable schedulers in the following section. \n 3.3 The Policy Manager The STING thread controller defines a thread state transi\u00ad tion procedure, but \ndoes not define a priori scheduling or migration policies. These policies can be application de\u00adpendent. \nAlthough several default policies are provided as part of the overall STING runtime environment, users \nare free to write their own. In fact, each virtual proces\u00adsor is closed over its own policy manager; \nthus, different VPS in a given virtual machine may implement differ\u00adent policies, The PM handles t bread \nscheduling, proces\u00adsor/thread mapping, and thread migration. The ability to partition an application \ninto distinct scheduling groups is important for long-lived parallel (or interactive) programs. Threads \nexecuting 1/0 bound pro\u00adcedures have different scheduling requirements than those executing compute bound \nroutines; applications with real\u00adtime constraints should be implemented using different scheduling protocols \nthan those that require only a sim\u00adple FIFO scheduling policy. Tree-structured parallel programs may \nrealize best run\u00adtime performance using a LIFO-based scheduler; appli\u00adcations running master/slave or \nworker farm algorithms may do better using a round-robin preemptive scheduler for fairness. Since all \nof these applications may be compo\u00adnents of a larger program structure or environment, the flexibility \nafforded by having them evaluate with different policy managers is significant. Distinct applications \ncan exist as independent executing threads evaluating on the same virtual machine. Moreover, each distinct \nsch~eduler is realized by a policy manager with different performance characteristics and implementation \nconcerns. Our design seeks to provide a flexible framework able to incorporate and experiment with different \nscheduling regimes transparently without requiring modification to the thread controller itself. To this \nend, all PMs provide the same interface although no constraints are imposed on the implementations themselves. \nThe interface shown be\u00adlow provides operations for choosing a new thread to run, enqueuing an evaluating \nt bread, setting t bread priorities, and migrating threads. These procedures are expected to be used \nexclusively by the TC; in general, user appli\u00adcations need not be aware of the policy/thread manager \ninterface. (pm-get-next-thread VP) returns the next ready TCB or thread to run on VP. If aTCBis re\u00adturned, \nits associated thread is eoaluatinsif a thread is returned, its state is not evaluating, and anew TCB \nmust be allocated for it. (pm-enqueue-thread obj VP state) enqueues obj which may be either a thread \nor aTCB into the ready queue of the policy man\u00adager associated with up. The state argument indicates \nthe state in which the the call to the procedure is made: delayed, kernel-block, user\u00adblock, or suspended. \n (pm-priority priority) and (pm-quantum quantum) use their priority and quantum arguments as hints to \nestablish anew priority and quantum for the currently executing thread. (pm-allocate-vp) returns a new \nvirtual processor on the current virtual machine, (pnr-vp-idle VP) is called by the thread manager if \nthere are no evaluating threads on up. This procedure can migrate a thread from another virtual processor, \ndo bookkeeping information, or call the physical processor to have the pro\u00adcessor switch itself to another \nVP. Besides determining a scheduling order for evaluating threads, the PM implements two basic load-balancing \nde\u00adcisions: (1) it may choose a VP on which a newly cre\u00adated thread should be run, and (2) it determines \nwhich threads onits VPcan be migrated, and which threadsit will choose for migration from other VPS. \nThe first decision point is important to handle initial load\u00adbalancing; the second is important to support \ndynamic load-balancing protocols. Determining the initial place\u00adment of a newly evaluating thread is \noften based on pri\u00adorities different from those used to determine the migra\u00ad tion of currently evaluating \nthreads, The PM interface preserves this distinction. Scheduling policies can declassified along several \nimpor\u00adtant dimensions: Locality: Isthere a single global queue ofthreadsin this system, or does each \nPM maintain its own local queues? Granularity Are threads distinguished based on their current state \norareall threads viewed as equals by the PM? For example, an application might choose an implementation \nin which all threads occupy a single queue regardless of their current state, Alternatively, it might \nchoose to classify threads into different queues based on whether they are evaluating, scheduled, previously \nsus\u00adpended etc. Structure: Are the queues implemented as FIFO s, LIFO s, round-robin, priority, or realtime \nstruc\u00adtures (among others)? Serialization: What kind of Iockkg structure does an application impose \non various policy manager queues?  Choosing different alternatives in this classification scheme leads \nto different performance characteristics. For examn\u00adple, if we adopt a granularity structure that distinguishes \nevaluating threads (i. e., threads with TCBS) from sched\u00aduled ones, and we impose the constraint that \nonly sched\u00aduled threads can be migrated, then no locks are required to access the evaluating thread queue; \nthis queue is local to the VP on which it was created. Queues holding sche~d\u00aduled and suspended threads \nhowever must be locked be\u00adcause they are targets for migration by PMs on other VP S. This kind of scheduling \nregimen is useful if dynamic load\u00adbalancing is not an issue, Thus, when there exist many long-lived non-blocking \nthreads (of roughly equal dura\u00adtion), most VPS will be busy most of the time executing threads on their \nown local ready queue, Eliminating locks on this queue in such applications is therefore beneficial. \nOn the other hand, applications that generate threads of varying duration may exhibit better performance \nwhen used with a policy manager that permits migration of both scheduled and evaluating threads even \nif there is an added cost associated with locklng the runnable ready queue. Global queues imply contention \namong policy managers whenever they need to execute a new thread, but such an implementation is useful \nin implementing many kinds of parallel algorithms. For example, in maater/slave (or worker-farm) programs, \nthe master initially creates a pool of threads; these threads are long-lived structures that do not spawn \nany new threads themselves. Once running on a VP, they rarely block. Thus, a PM executing such a thread \nhas no need to support the overhead of maintaining a local thread queue. Local queues are useful, however, \nin implementing result-parallel programs in which the pro\u00adcess structure takes the form of a tree or \ngraph; these queues can be used in such applications to load balance threads fairly among a set of virtual \nprocessors. 4 The Dynamics of Thread Execution and Synchroniza\u00adtion Obvious differences exist in program \nmethodology, syn\u00adtax, etc. among the numerous proposals for incorporat\u00ading concurrency structures into \nhigh-level symbolic pr\u00ad ogramming languages. STING supports the functionality required by the semantics \nof many of these proposals: (a) threads may be dynamically instantiated and require rum\u00adtime scheduling, \n(a) communication among threads takes place via concurrent data structures that may be shared by many \nreaders and writers, (c) communicating threads execute within a single address space, and (d) threads \nsyn\u00adchronize either by waiting for values generated by other processes, or by waiting at explicit barrier \npoints. 4.1 Support for Result (Fine-Grained) Parallelism In a result parallel program, each concurrently \nexecuting process contributes to the value of a complex data struc\u00adture (e.g., an array or list). Process \ncommunication is via this result structure. Expressions that attempt to access a component of the result \nwhose contributing process is still evaluating block until the process completes. (define (primes limit) \n(let loop ((i 3) (primes (future (list 2)) )) (cond (O i limit) (touch primes)) (else (loop (+ i 2) (future \n(filter i primes))))))) (define (filter nprimes) (let loop ((j 3)) (cond (O (*jj) n) (consn (touch primes))) \n((zero? (modn j)) primes) (else (loop (+ j 2)))))) Figure 3: An implementation ofprimes using futures. \nA future must be explicitly touched to access its value in this implementation. Fbtw-es[13] area good \nexample of an operation well-suited for implementing result parallel algorithms. The object created by \nthe MultiLisp or Mu1-T expression, (future E), creates a thread responsible for computing E; the object \nreturned is known as a future. When E finishes, yielding v as its result, the future is said to be determined. \nAn expression that touches a future either blocks if Eis still being computed or yields uifthefuture \nis determined. Threads are a natural representation for futures. To motivate the implementation of result \nparallelism in STING, Figure 3 is an implementation of a parallel prime number finder using futures. \nIn this program, a future is created for each odd element between 2 and limit. Anumber isadded onto a \ncurrent prime list if filter determinesit to bea prime number. Ina naive implementation, each instantiation \nofafuture will entail the creation of anew thread; thus, the number of threads allocated in executing \nthis program (under this implementation) is proportional to limit. This behavior is undesirable because \na future computing the primality of ihas an implicit dependence with the future created to compute theprimality \nof i-2 and soon. Poor processor and storage utilization will results given the data depen\u00addencies found \nin this program. This is because many of the lightweight processes that arecreated will either: 1. need \nto block when they request the value of other yet-unevaluated futures or, 2. in the case of processes \ncomputing small primes, do a small amount of computation relative to the cost incurred in creating them. \n  Because the dynamic state of a thread consists of large objects (e.g., stacks and heaps), cache and \npage locality is compromised if process blocking occurs frequently or if process granularity is too small. \nThe semantics of touch and future dictate that a future F which touches another future G must block on \nG if G is not yet determined. Assume TF and TQ are the thread representation of F and G, respectively. \nThe run\u00adtime dynamics of the touch operation on G can entail accessing TG either when TG is (a) delayed \nor scheduled, (b) evaluating, or (c) determined. In the latter caae, no synchronization between these \nthreads is necessary. Case (b) requires TF to block until TG completes. STING per\u00adforms an important \noptimization for case (a), however, which we discuss below.  4.1.1 Thread Stealing TF can evaluate \nthe closure encapsulated within TQ (call it E) using its own stack and heap, rather than blocking a forcing \na context switch if TG is delayed or scheduled. In effect, this implementation treats E as an ordinary \nprocedure, and the touch of G as a simple procedure call; we say that TF steals TG in this case. The \ncorrectness of this optimization lies in the observation that that TF would necessarily block otherwise; \nby applying E using TF S dynamic context, the VP on which TF executes does not incur the overhead of \nexecuting a context switch. In addition, no TCB need be allocated for TQ since TF s TCB is used instead. \nThe optimization may only lead to observably different results if used in instances where the calling \nthread need not necessarily block. For example, suppose TG was an element of a speculative call by TF. \nFurthermore, assume TG diverges, but another speculative thread (call it TH) does not. In the absence \nof stealing, both To and T~ would spawn separate thread contexts. T~returns a value to TF, In the presence \nof stealing, however, TF will also loop because TG does. Users can parametrize thread state to inform \nthe TCif a thread can steal or not; STING provides interface procedures for this purpose. Like load-based \ninlining[33] or lazy task creation[24], steal\u00ading throttles process creation. Unlike these other tech\u00adniques, \nhowever, stealing also improves locality. Locality is increased because a stolen thread is run using \nthe TCB of a currently evaluating thread; consequently, the stack and heap of this TCB remains in the \nvirtual machine s working set. Because of stealing, STING reduces the overhead of context switching, \nand increases process granularity for programs in which processes (a) exhibit strong data dependencies \namong one another, and (b) block only when they require data from other processes. Of course, for the \noperation to be most effective, appropriate scheduling policies must be adopted. For example, a preemptible \nFIFO scheduler in the prime number code would not take full advantage of stealing since processes computing \nsmall primes would be run before processes that compute large ones. Stealing operations will be minimal \nin this case: processes exhibit few data dependencies with processes instantiated earlier, and threads \ncomputing small primes must necessarily be determined before threads computing large primes can proceed. \nOn the other hand, a LIFO scheduling policy will cause processes computing large primes (i, e., primes \nclose to limit) to be run first. Stealing will occur much more frequently here since processes will demand \nthe results of T&#38;A TI mead ?2 , Ewtuaring . .., . .....* $chdnlcd Ewlwdng SIolu &#38; Bqq I* . ..+=k \nFigure 4: Dynamics of thread stealing, Dashed lines indi\u00adcate dataflow constraints, solid lines specify \nthread tran\u00adsitions. other processes computing smaller primes which have not yet run; the process call \ngraph will, therefore, unfold more effectively, 4.2 Master-Slave Programs: Blocking and Synchroniza\u00adtion \nThe master-slave paradigm is a popular parallel program structuring technique. In this approach, the \ncollection of processes generated is bounded a priori a master pro\u00adcess generates a number of worker \nprocesses and collates their results. Process communication typically occurs via shared concurrent data \nstructures or variables. Ma.ster\u00adslave programs often are more efficient than result paral\u00adlel ones on \nstock multiprocessor platforms because work\u00aders rarely need to communicate with one another except to \npublish their results, and process granularity can be better tailored for performance. We have used STING \nto build an optimizing implementa\u00adtion of first-class tuple-spaces in Scheme. A tuple-space is an object \nthat serves aa an abstraction of a synchronizing content-addressable memory[6]; tuple-spaces are a natu\u00adral \nimplement ation choice for many maater/slave-bssed algorithms. The semantics of tuple-spaces in our system \ndiffer sig\u00adnificantly from their definition in C. Linda, for example. Besides the added modularity brought \nabout by deno\u00adtable tuple-space objects, our system also treats tuples as objects, and tuple operations \nas binding expressions, not statements. We have built a customized type inference procedure to specialize \nthe representation of tuple-spaces whenever possible[l 7]. In our current implementation, tuple-spaces \ncan be specialized as synchronized vectors, queues, sets, shared variables, semaphores, or bags; the \noperations permitted on tuple-spaces remains invariant over their representation. In addition, applications \ncan specify an inheritance hierarchy among tuple-spaces if so desired. Processes can read, remove or \ndeposit new tuples into a tuple-space. The tuple argument in a read or remove operation is called a tempiate \nand may contain variables prefixed with a ? . Such variables are referred to as joir\u00admals and acquire \na binding-value as a consequence of th~e match operation. The bindings acquired by these formals are \nused in the evaluation of a subordinate expression: thus, we can write: (get TS [?xI (put TS [(+X 1)1)) \n toremoveatomicrdly asingIeton tuple from TS, increment it by one, and deposit it back into TS. Our \nimplementation, in the general case, uses two hash\u00adtables (call them .H~and HP) astherepresentation struc\u00adtures \nfor a fully associative tuple-space. Processes that attempt to read or remove atuple first hash on their \nnon\u00adformal tupleelementsin~p, Ifatleast one match exists, theproper bindings fortheformals preestablished, \nthere\u00adtrieved tupleis marked as deletedin the case of a remove operation, and the executing process proceeds. \nWhen a match does not exist, theprocess hashes onitsnon-formd tuple elements in HR, deposits a structure \nthat indicates its identity. and blocks. A depositing process is defined symmetrically -any pro\u00ad cesseswaiting \nforitstuple in HB are unblocked andresched\u00ad uled. Otherwise, the tuple is deposited into HP using its \nfields as hash keys. The implementation minimizes syn\u00ad chronization overhead by associating a mutex with \nevery hash bin rather than having a global mutex on the entire hash table. This permits multiple producers \nand cons\u00ad umers of a tuple-space to concurrently access its hash tables. The implementation also takes \nadvantage of stealing to permit the construction of fine-grained parallel progranns that synchronize \nontuple-spaces. We use threads as bona jide elements in atuple. Consider a process Pthat exe\u00adcutes the \nfollowing expression: (rd TS CXl X2 1 J?3) where x I and x2 are non-formals. Assume furthermore that \na tuple in TS is deposited as a consequence of t!he operation: (spawn TS[&#38; Ez1) This operation schedules \ntwo threads (call them TE, and TE, ) responsible for computing El and E2. If both !f zr, and TE2 complete, \nthe resulting (passive) tuPle cont~ns two determined threads; the matching procedure applies thread-value \nwhen it encounters athreadinatuple; this operation retrieve the thread s value. If TE1 is still scheduled \nat the time P executes, however, P is free tested it, and then determine ifits result matchles xl. If \na match does not exist, P may proceed to search for another tuple, leaving TE2 still in a scheduled state. \nAnother process may subsequently examine this same tu\u00adple and steal TE2 if warranted. Similarly, if TE1 \ns result matches xl, Pis then free tosterd T&#38;. If either TEL or TE2 are already evaluating, Pmaychoose \nto either block on one (or both) thread(s), or examine other potentially matching tuples in TS. The semantics \nof tuple-spaces im\u00adpose no constraints on the implementation in this regard. STING S combination of first-class \nthreads and stealing rd\u00ad10WS us to write quasi-demand driven fine-grained (result) parallel programs \nusing shared data structures. In this sense, the thread system attempts to minimizes any sig\u00adnificant \ndistinction between structure-based (e.g., tuple\u00adspace) and dat aflow style (e. g., future/touch) synchro\u00adnization. \n 4.2.1 Mutexes Operations on tuple-spaces or similar high-level syn\u00adchronization structures make use \nof mutex operations, mutex-acquire and mutex-release. Mutexes arecreated bythemutex operation, (make-mutex \nactive passive). Mutex-acquire attempts to acquire a mutex. If the mutex is locked, the executing thread \nac\u00adtively spins forthe period specified by active; active spin\u00adning causes the thread to retain control \nof its virtual pro\u00adcessor during the period that it is blocked waiting for the mutex to be released. \nWhen the active spin count be\u00adcomes zero, the thread relinquishes control of its VP, and inserts itself \ninto an appropriate ready queue. When next run, it attempts to re-acquire the mutex, yielding its pro\u00adcessor \nif unsuccessful. This operation is repeated passive number of times. If the passive spin count is exhausted, \nand the mutex has not yet been acquired, the executing thread blocks on the mutex. When the mutexis ulti\u00admately \nreleased, (via mutex-release) all threads blocked on this mutex are restored onto some ready queue. Using \nmutex primitives, macros, and Scheme s support for exception handling, onecan easily build a safe version \nof a mutex acquire operation, (rrith-mutex mutex bodv). This operation ensures that mutez is released \nif bodg raises an exception during its evaluation that causes control to exit the current dynamic environment. \n 4.2.2 Preemption and Interrupts A preemptive round-robin or FIFO scheduling policy is best suited for \nmaster-slave applications in which the mas\u00adter performs relatively little processing after the initial \nset of spawned workers complete. A round-robin pol\u00adicy allocates a specified quantum for each worker \nin the worker pool. Support for preemption is important be\u00adcause workers rarely block; in its absence, \nlong-running workers might occupy all available VPS at the expense of other enqueued ready threads. Preemption \nis sometimes best disabled in master/slave programs that make significant use of barrier synchro\u00ad nization. \nIn these applications, the master generates a new set of worker processes after all previously created \nworkers complete. If the time to execute a particular set of workers is small relative to the total time \nneeded to complete the application, enabling preemption may degrade performance[31]. Threads can disable \nan ini\u00adtial preemption by setting a flag in their TCB; if pre\u00ademption takes place when this quantum flag \nis false, an\u00adother bit in the TCB state is set indicating that a sub\u00adsequent preemption should not be \nignored, Users can encapsulated time critical code using the the syntactic form, (without-preemption \nbody) that evaluates body with preemption disabled. The without-preemption form is in fact a specialized \nversion ofamore general construct, without-interrupts, that disables all interrupts during the evaluation \nofits body, 4.3 Speculative Parallelismand Barrier Synchronization Speculative parallelism is an important \nprogramming tech\u00adnique that often cannot be effectively utilized because of runtime overheads incurred \nin its implementation. The two features most often associated with systems that sup\u00adport a speculative \nprogramming model are the ability to favor certain (more promising) tasks over others, and the means \nto abort, reclaim (and possibly undo) unnecessary computation, STING permits programmers to write speculative \napplica\u00adtions by: 1. allowing users to explicitly program thread priorities, 2. permitting a thread \nto wait on the completion of other threads, and 3. allowing threads to terminate other threads,  Promising \ntasks can execute before unlikely one because priorities are programmable, A task a that completes first \nin a set of tasks can awaken any thread blocked on its complet etion; this functionalist y permits STING \nto support a simple form of OR-parallelism[8]. a can terminate all other tasks in its task set once it \nhas been determined that their results are unnecessary. Speculative computation using STING, however, \nwill not be able to undo non-local side-effects induced by useless tasks; the system does not provide \na primitive backtracking mechanism2. Consider the implementation of a wait-f or-one construct, This operator \nevaluates its list of arguments concurrently, returning the value yielded by the first of its arguments \nto complete, Thus, if a; yields v in the expression: (wait-for-one al az . . . a; . . . an) the expression \nreturns v, and, if desired by the program\u00admer, terminates the evaluation of all the remaining U3, j#i. \nThe specification of a wait-for-all construct that im\u00adplements an AND-parallel operation is similar; \nit also evaluates its arguments concurrently, but returns true only when all its arguments complete, \nThus, the expres\u00adsion: (wait-for-all al az . . . a; . . . an) acts as a barrier synchronization point \nsince the thread executing this expression is blocked until all the a, com\u00adplete. The implementation \nof this operation is very simi\u00adlar to the implementation of the speculative wait-for-one operation. z \nsting does not support first-class continuations across thread boundaries. (define (block-on-group count \ngroup) (let loop ((i count) (threads group)) (cond ((zero? i)) ((null? group) (set-TCB.wait-count (current-TCB) \ni) (thread-block (current-thread))) (else (let ((thread (car threads))) (mutex-acquire thread.mutex) \n(cond ((determined? thread) (mutex-release thread.mutex) (1OOP (1-i) (cdr threads))) (else (let ((tb \n(make-tb))) (set-tb.tcbtb (current-tcb)) (set-tb.threadtb thread) (set-tb.next tb (thread,waiters thread) \n) (set-thread,waiters thread tb)) (mutex-release mutex) ) (loop (1-i) (cdr threads)))))))) Figure5: Definition \nof block-on-group, The TC implements these operations using a common procedure, block-on-group. Threads \nand TCBsare de\u00adfined to support this functionality, For example, associ\u00adated with a TCB structure is \ninformation on the number of threads in the group that must complete before the TCB s associated thread \ncan resume. Block-on-group takes alistof threads and acount. These threads correspond tothe arguments \nof the wait-for-one and wait-for-all operations shown above; the count ar\u00adgument represents the number \nof threads that must com\u00adpletebefore the current thread (Le., thethreadexecuting this procedure) is allowed \nto resume. If thecount is one, we get an implementation of wait-for-one; if the count is equal ton, weget \nan implementation of wait-for-all, The relationship between a thread Tg in the group and the current \nthread (Tw) thatis to wait onTis maintained in a data structure (called a thread barrier (TB)) that contains \nreferences to: 1. TW STCB. 2. the TB of another waiter blocked on Tg (if one ex\u00adists). 3. Tg -this \nis used only for debugging purposes.  We give a definition for block-on-group in Fig,5, The call: (block-on-groupm \nT1 T2 . . . T.) causes the current thread (call it T) to block on the com\u00adpletion of m of the Ti, m < \nn, Each of these Ti have a reference to T in their chain of waiters, The proce\u00addure checks if a thread \nin the thread group has already been determined; in the case, the wait-count is decre\u00admented, but no \nthread barrier is constructed. Other\u00adwise, a TB is constructed as well. When all threads in group have \nbeen examined, the procedure sets the cur\u00adrent thread s wait-count field to the extant count, and issues \na thread-block operation, Applications use Block-on-group in conjunction with a wakeup-waiters procedure \nthat is invoked by the (z; when they complete. Wakeup-waiters examines the list of waiters chained from \nthe waiters slot in its threaLd argument. A waiter whose wait-count becomes zero is enqueued on the ready \nqueue of some VP, The TC iln\u00advokes wakeup-waiters whenever a thread T completes (i.e., whenever it terminates \nor abnormally exits). All threads waiting on T s completion are thus rescheduled. Given these two procedures \nwait-f or-one can redefined simply: (define (wait-for-one block-group) (block-on-group 1 block-group) \n(map thread-terrhate block-group) If Texecutes wait-f or-one, it blocks onall the threads in its block-group \nargument. When Tis resumed, it is placed on a queue of ready threads in the policy manager of some available \nvirtual processor. The map procedure executed upon T s resumption terminates all threads in its group. \nSTING S wait-for-all procedure can omit this operation since all threads in its block-group are guaranteed \nto have completed before the thread executing this operation is resumed. Performance STING is currently \nimplemented onan 8 processor Silicon Graphics MIPS R3000 shared-memory (cache-coherent) multiprocessor. \nThe physical machine configuration maps physical processors tolightweight Unix threads; each node in \nthe machine runs one such thread. We ran the bench\u00admarks shown below using a virtual machine in which \neach physical processor implements a single virtual processor. Fig. 6 gives baseline figures for various \nthread operations; these timings were derived using asingle LIFO queue. The Thread Creation timing is \nthe cost to create a thread not placed in the genealogy tree, and which has no dynamic state. Thread \nFork and Value measures the cost to create a thread that evaluates the null pr\u00adocedure and returns. Scheduling \na Thread is the cc)st of inserting a thread into the ready queue of the cur\u00adrent VP. A [Synchronous Context \nSwitch is the cc)st to make a yield-processor call in which the calling thread is resumed immediately. \nThe cost for Stealin,g does not include the time to schedule the thread being stolen. Thread Block and \nResume is the cost to block and resume a null thread. Tuple Space is the cost to create a tuple-space, \ninsert and then remove a singleton tuple. The speculative synchronization timings reflects the cost to \ncompute two null threads speculatively; the barrier synchronization is the cost to build a barrier syn\u00adchronization \npoint on two threads both computing the null procedure. We present detailed benchmarks of sev\u00aderal application \nprograms in a companion paper[18]. Case Timirags(irt pseconds) Thread Creation 8,9 Thread Fork and Value \n44.9 Scheduling a Thread 18.9 Synchronous Context Switch 3.77 Stealing 7.7 Thread Block and Resume 27.9 \nTriple-Space 170 Speculative Fork (2 threads) 68.9 Barrier Synchronization (2 threads) 144.8 Figure 6: \nBaseline timings, 6 Related Work and Conclusions Insofar as STING is a programming system that permits \nthe creation and management of lightweight threads of control, it shares several common traits with thread \npack\u00adage systems developed for other high-level languages[9, 10, 23]. These systems also view threads \nas a manifest datatype, support preemption in varying degrees, andin certain restricted cases, permit \nprogrammers to specify a specialized schedtding regimen. The thread abstraction defines the coordination \nsublanguage in these systems. There are some important differences however that clearly distinguish STING \nfrom these other systems. First, the scheduling andmigration protocol STING uses iscomp~etery customizable; \ndifferent applications can run different sched\u00adulers without modifying the thread manager or the virtual \nprocessor abstraction; such customization can reapplied to the organization of thevirtual machine itself, \nSecond, STING S support for data loczdity, storage optimization, and process throttling via stealing \nis absent in other sys\u00adtems. Moreover, all thread operations are implemented directly within thesT~G \nvirtual machine: there isnocon\u00adtext switch to a lower level kernel that must be performed in order to \nexecute a thread operation. STING is built on abstract machine intended to support long-lived appli\u00adcations, \npersistent objects, and multiple address spaces. Thread packages provide none of this functionality since \n(by definition) they do not define a complete program en\u00advironment. STING also differs from programming \nlanguages that pro\u00advide high-level abstractions (e.g,, continuations[34, 14] to model concurrency. Because \nwe designed STING as asys\u00adtems programming language, it provides low-level con\u00adcurrency abstractions \n-application libraries can directly create thread objects, andcandefine their own scheduling and thread \nmigration strategies. High-level concurrency constructs are realizable using threads, but the system \ndoes not prohibit users from directly using thread oper\u00ad ations in the ways described above if efficiency \nconsider\u00ad ations warrant. In particular, the same application may define concurrency abstractions with \ndifferent semantics and efficiency concerns within the same runtime environ\u00adment, In certain respects, \nSTING resembles other advanced multi\u00adthreaded operating system environments[l, 22]: for exam\u00adpie, itsupports \nnon-blocklng I/Ocalls with call-back, user control over interrupts, and local address space manage\u00adment \nas user-level operations. It cleanly separates user\u00adlevel and kernel-level concerns: physical processors \nhan\u00addle (privileged) system operations and operations across virtual machines; virtual processors implement \nall user\u00adlevel thread and local address-space functionality. How\u00adever, because STING is an extended dialect \nof Scheme, it provides the functionality and expressivity ofa high-level programming language (e.g., \nfirst-class procedures, gen\u00aderal exception handling, and rich data abstractions) that typical operating \nsystem environments do not offer. STING is a platform for building asynchronous program\u00adming primitives \nand experimenting with new parallel pro\u00adgramming paradigms, In addition, the design also allows different \nconcurrency models to be evaluated competi\u00adtively. Scheme offers an especially rich environment in which \nto undertake such experiments because of its well\u00addefined semantics, its overall simplicity, and its \nefficiency. However, the STING design itself is language independent; we believe it could be incorporated \nfairly easily into any high-level programming language. STING does not merely provide hooks for each \nconcur\u00adrency paradigm and primitive we considered interesting, We focussed instead on basic structures \nand functional\u00adity common to a broad range of parallel programming structures; thus, the implementation \nof blocking is easily used to support speculative computation, the stealing optimization used to throttle \nthe execution of threads is well-suited for implementing futures and tuple-space syn\u00adchronizat ion, and, \nfinally, cust omiz able policy managers make it possible to build fair and efficient schedulers for a \nvariety of other paradigms. References [1] Thomas Anderson, Edward Lazowska, and Henry Levy. The Performance \nImplications of Thread Management Alternatives for Shared Memory Mul\u00adtiprocessors. IEEE Transactions \non Computers, 38(12):1631-1644, December 1989. [2] Thomas E. Anderson, Brian N, Bershad, Edward D. Lazowska, \nand Henry M. Levy. Scheduler acti\u00ad vations: effective kernel support for the user-leyel management of \nparallelism. In Proceedings of Itlth ACM Symposium on Operating systems principles, pages 95-109. Association \nfor Computing Machinery SIGOPS, October 1991. [3] Arvind, Rishiyur Nikhil, and Keshav Pingali. I-Structures: \nData Structures for Parallel Computing. Transactions on Programming Languages and Sys\u00adtems, 11(4):598-632, \nOctober 1989. [4] Peter Bishop. Computer Systems with a Very Large Address Space and Garbage Collection. \nPhD thesis, MIT Laboratory for Computer Science, 1977. [5] David Black. Scheduling Support for Concurrency \nand Parallelism in the Mach Operating System. IEEE Computer, 23(5):35-43, May 1990. [6] Nick Carriero \nand David Gelernter, How to Write Parallel Programs: A Guide to the Perplexed, ACM Computing Surveys, \n21(3), September 1989. [7] Nick Carriero and David Gelernter, Linda in Con\u00adtext, Communications of the \nACM, 32(4):444 -458, April 1989, [8] K.L Clark and S. Gregory. PARLOG: Parallel Pro\u00adgramming in Logic. \nACM Transactions on Program\u00adming .Languages and Systems, 8(1):1-49, 1986. [9] Eric Cooper and Richard \nDraves, C Threads. Tech\u00adnical Report CMU-CS-88-154, Carnegie-Mellon Uni\u00adversity, June. [10] Eric Cooper \nand J. Gregory Morrisett. Adding Threads to Standard ML, TechnicaJ Report CMU\u00adCS-90-186, Carnegie-Mellon \nUniversity, 1990. [11] J. Dongarra, D. Sorenson, and P. Brewer, Tools and Methodology for Programming \nParallel Processors, In Aspects of Computation on Asynchronous Proces\u00adsors, pages 125 138. North-Holland, \n1988. [12] R. Gabriel and J. McCarthy, Queue-Based Multi-Processing Lisp. In Proceedings oj the 1984 \nConf. on Lisp and Functional Programming, pages 25-44, August 1984. [13] Robert Halstead. Multilisp: \nA Language for Concur\u00adrent Symbolic Computation, Transactions on Pro\u00adgramming Languages and Systems, \n7(4):501-538, Oc\u00adtober 1985. [14] Robert Hieb and R. Kent Dybvig, Continuations and Concurrency. In Second \nA CM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 128-137, March 1990. \n[15] Waldemar Horwat, Andrew Chien, and William Dally. Experience with CST: Programming and Im\u00adplement \nation. In ACM SIGPLA N 89 Conference on Programming Language Design and Implementation, pages 101-109, \nJune 1989. [16] Paul Hudak, Para-Functional Programming, IEEE Computer, 19(8):60-70, August 1986. [17] \nSuresh Jagannathan. Optimizing Analysis for First- Class Tuple-Spaces. In Third Workshop on Parallel \nLanguages and Compilers. MIT Press, August 1990. [18] Suresh Jagannathan and James Philbin. A Founda\u00adtion \nfor an Efficient Multi-Threaded Scheme System, In Proceedings of the 1992 Conf. on Lisp and Func\u00adtional \nProgramming, June 1992. [19] David Kranz, Robert Halstead, and Eric Mohr. Mul- T: A High Performance \nParallel Lisp. In Proceedings of the ACM Symposium on Programming Language Design and Implementation, \npages 81-91, June 1989. [20] David Kranz, Richard Kelsey, Jonathan Rees, Paul Hudak, James Philbin, \nand Norman Adams, Oli.-BIT: An Optimizing Compiler for Scheme. ACM SIGPLAN Notices, 21(7):219-233, July \n1986, [21] Henry Lieberman and Carl Hewitt. A Real-Time Garbage Collector Based on the Lifetime of Objects. \nCommunications of the ACM, 26(6):419 429, June 1973, [22] Brian D. Marsh, Michael L, Scott, Thomas J, \nLeBlanc, and Evangelos P. Markatos, First-class user-level threads. In Proceedings of 19th ACM Sym\u00adposium \non Operating Sgmtems Principles, pages 110\u00ad 21. Association for Computing Machinery SIGOPS, October 1991. \n [23] Sun Microsystems, Lightweight Processes, 1990, In SunOS Programming Utilities and Libraries, [24] \nRick Mohr, David Kranz, and Robert Halstead. Lazy Task Creation: A Technique for Increasing the Gran\u00adularity \nof Parallel Programs. In Proceedings of the 1990 ACM Conference on Lisp and Functional Pro\u00adgramming, \nJune 1990. [25] Randy Osborne. Speculative Computation in Mul\u00adtiLisp. In Proceedings of the 1990 ACM \nConference on Lisp and Functional Programming, pages 198-2018, 1990. [26] James Philbin. STING: An Operating \nS~stem Kernel for Highl~ Parallel Computing. PhD thesis, Dept. of Computer Science, Yale University, \n1992, Forthcom\u00ading. [27] Jonathan Rees and William Clinger, editors. The Revised3 Report on the Algorithmic \nLanguage Scheme. ACM Sigp/ara Notices, 21(12), 1986, [28] John Reppy. CML: A Higher-Order Concurrent \nLan\u00adguage. In Proceedings of the SIGPLA N 91 Confer\u00adence on Programming Language Design and Imple\u00admentation, \npages 293-306, June 1991, [29] Vijay Saraswat and Martin Rinard. Concurrent Con\u00adstraint Programming. \nIn Proceedings o~ the 1 Wh ACM Symposium on Principles of Programming Lam\u00adguages, pages 232-246, 1990. \n[30] A, Tevanian, R. Rashid, D. Golub, D, Black, E. Cooper, and M. Young. Mach Treads and the UNIX Kernel: \nThe Battle for Control. In 1987 USENIX Summer Conference, pages 185-197, 1987.  [31] A. Tucker and \nA. Gupta. Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Machines. In Proceedings \nof the 12th Annual ACM Symposium on Operating Systems Principles, pages 114-122, 1989. [32] David Ungar. \nGeneration Scavenging: A Ncm-Disruptive High Performance Storage Reclamation Algorithm. In Proceedings \nof the ACM SIG-SOFT/SIGPLA N Software Engineering Symposium on Practical Software Development Environments, \npages 157-167, 1984. 67 [33] M. Vandevoorde and E. Roberts. WorkCrews: An Abstraction for Controlling \nParallelism, lnterna\u00adtionrd Journal of Paraliel Progmmming, 17(4) :347\u00ad366, August 1988, [34] Mitch Wand. \nContinuation-Based MultiProcessing, In Proceedings of the 1980 ACM Lisp and Functional Programming Conference, \npages 19-28, 1980,  \n\t\t\t", "proc_id": "143095", "abstract": "<p>We describe an approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) programming languages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads of control and virtual processors as first-class objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency.</p><p>Unlike parallel systems that rely on operating system services for managing concurrency, STING implements concurrency management entirely in terms of Scheme objects and procedures. It, therefore, permits users to optimize the runtime behavior of their applications without requiring knowledge of the underlying runtime system.</p><p>This paper concentrates on (a) the implications of the design for building asynchronous concurrency structures, (b) organizing large-scale concurrent computations, and (c) implementing robust programming environments for symbolic computing.</p>", "authors": [{"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "", "person_id": "PP39073732", "email_address": "", "orcid_id": ""}, {"name": "Jim Philbin", "author_profile_id": "81100302761", "affiliation": "", "person_id": "P140529", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143095.143119", "year": "1992", "article_id": "143119", "conference": "PLDI", "title": "A customizable substrate for concurrent languages", "url": "http://dl.acm.org/citation.cfm?id=143119"}