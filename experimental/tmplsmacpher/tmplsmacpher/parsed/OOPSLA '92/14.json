{"article_publication_date": "12-01-1992", "fulltext": "\n Vancouver, British Columbia, Canada 5 -10 October 1992 Report by: Dennis de Champeaux HP Labs With help \nfrom Ralph Jackson and John Bumham Aim of the WS This workshop concentrated on the process dimension \nof 00 analysis: how does one do OOA. The call for contributions identified the following topics: exploration \nof top-down-ness as a core strategy; delineation of the set of artifacts that are the stepping stones \nfor the 00 analysis process; description of dependencies between these artifacts that can be identified \nas generic constituents of an 00 analysis process; formulation of entry and exit criteria for these generic \nconstituents of an 00 analysis process; description of a broad-brush 00 analysis process, either on the \nbasis of reasoned deliberations or on the basis of experience; merging strategies for reusing concepts \net al obtained in an 00 domain analysis into an analysis for a specific system; formulation of metrics \nto monitor progress in the OOA process; experience reports that highlight the process dimension.  Addendum \n A 92 to tt7e Proceedings Workshop Report- The OOA Process Format of the WS The Workshop format was \nprimarily presentation in the morning and early afternoon, with varying amount of discussing during the \npresentations. Informal discussions and a summary finished the day.  Attendance of the WS Organizers: \nDennis de Champeaux, Doug Lea, John Bumham Presenters: Donald Bagert Stephan Clyde Ghica van Emde Boas \nHerman Kaindl Ken Rubin  and the organizers Others in attendance included: Mohamed Fayad John Majarwitz \nScott Woodfield Peter van Emde Boas   Highlights of the Presentations Donald Bagert: An Object Identification \nMethodology for Object-Oriented System Development He concentrated on the task of identifying objects \nand classes. He critiqued the approaches from Abbott, Shlaer/Mellor, Coad-Yourdon and Booth. He proposed \nas an alternative the following set of activities: 1. Study the problem domain. 2. Identify the main \ncommunication messages. 3. Encapsulation of the subjects. 4. Verify the interface between the subjects. \n 5. Repeat the process for each of the subjects.  Stephen Clyde et al: Analysis Methods, Reuse Techniques, \nand Formalisms for Object- Oriented Systems Analysis (OSA) They seem to believe that their OSA method \ncan NOT have a generic process as witnessed by their: We believe that the analysis process should vary \nfor each system depending on its dominating characteristics. This has given rise to an unorthodox analysis \nmethod that has no sequence of steps. Instead, it consists of a collection of activities guided by heuristics. \n They identified the following heuristics: 9 Information should be captured as it becomes available. \n . Information should be properly localized in the model instance. An analysis model instance should \nbe free of implementation-or platform-dependent information. Ghlca van Emde Boas: Food: Framework for \nObject-Oriented Development She presented an 11 step development process that covered also design and \nimplementation activities: 1: Enterprise Modeling 2: Application Scoping 3: Transition to Object Modeling \n4: Model Transformations 5: User Interface Structuring 6: Application Model Design The audience felt \nthat user feedback was necessary much sooner than suggested by these steps. 7: User Interface Representation \n8: Persistent Information Design 9: Implementation lo: User Feedback 11: Iterate This proposal is more \ndetailed than the standard macro processes that do not go beyond requirement capture, analysis, design, \nimplementation, maintenance, but does not go into the details of the micro processes. Herman Kaindl: \nComparing Object-Oriented Analysis with Knowledge Acquisition The thrust of HRs presentation was to point \nout that there are many things in Knowledge Acquisition and OOA that are similar. Since Knowledge Acquisition \nhas been going on for some years it seems reasonable to reuse some of what has been learned. Modeling \nis where these too are most similar. In terms of the process of OOA, we should try to not just transfer \nknowledge from the KA domain, but try to reuse the same process used in KA. HK argued that we needed \nmore than just diagrams, some natural language was useful, and hypertext had been found to be a good \nsemi-formal representation. HK argued that the taxonomy is the most important relationship in classes. \nWe should look for relationships like aggregation, realization and connected. Ken Rubln: no paper KR \nbasically gave us the 30,000 foot view of Object Behavioral Analysis (OBA). OBA is not a Process Model, \nit is a method that tells you how to conduct the process of analysis. The goal of OBA is to represent \nthe problem in an 00 way. OBA has five basic steps: Step 0: Goals and Objectives Step 1: Understanding \nProblem Step 2: Defining Objects Step 3: Relationships Step 4: Dynamic Modeling KR also described the \nrole of scripting in OBA. A script describes a single thread of control, and is modeled as a table with \nfour columns: Initiator, Action, Participation and Service. Each row is a sequence step in the script. \nWith scripts we are interested in the largest view, and are not focused on individual objects.  Dennis \nde Champeaux: no paper He sketched a micro process for OOA. This process is formulated in an 00 fashion \nin the sense that stepping stones in the process space are demarcated. Connections between these stepping \nstones correspond with actions performed by analysts and can be seen as elaboration dependencies. A similar \nmicro process was sketched for the 00 design phase. Both processes are quite non-deterministic and are \ncompatible with any macro process (= waterfall, spiral, fountain, etc.). Subsequently Doug Lea and John \nBumham commented on these micro processes. Doug Lea observed: Process Models are to help figure out what \nneeds to be done and also how to do it. However the following problems were noted: Leakages in both concept \nand practice Fallibility of both methods and practitioner 9 Evolution of requirements, understanding \nand executing systems must be addressed. Incompleteness of understanding or due to sloth. Parts of Solutions \ninclude: Continuity: Analysis use same constructs as designers as programmers. Refinement: We know models \nare incomplete. Iteration: People screw up and have to do it again. Prototyping: Let people visualize \ntheir modes and see errors. Reflection: Seen as increasingly necessary for systems and PM s. John Burnham \nobserved: Software starts with single threading, but as development continues more parallel activities \nare initiated. It takes more mental capacity to envision the entire development process (e.g. class libraries) \n Beware of Human/Sociological Processes: once a process gets in place it becomes the focus. We forget \nwhat we are trying to do. Testability: Black box like Use-cases; white box where we know internals. It \nis normal to re-design, so we must iterate, but how do we know when to quit? If you do not know then \nyou do not have a process. How do we know we are correct? Traceability: people redesign in coding, but \nwhat happens to traceability? How do we track these changes?  Desiderata for the 00 development Process \nThe remaining part of the day was spend brainstorming on requirements for the 00 development process. \nThe following items, unordered, where generated: 1. Metalevel model (description of doing OOA). The meta \nmodel regards the structure and results of the method. 2 Auditibility. This means that a domain expert \nof the system can use the method along with a methodologist. What is being modeled can be audited by \na domain expert. It is hard to find a metric to say how auditible the method is. This suggest a usability \nquestionaire since inside goodness would be hard to justify. 3. Measurable As the method is used, can \nprogress toward the goal of the method be measured? We need metrics to measure the Process, Product, \nand Resources of the thing the method is modeling. This feature will allow cost estimation. 4. Tailorable \nCustomizability to a Domain. Aspects of the method and be altered to tune the method to the problem domain. \n4a. A special case of tailorabilty- Incrementality. This has two aspects: the ability to produce models \nthat allow the product in increments. The method also should have the ability to produce increments of \nits own deliverables so that pieces of the OOA can be delivered.  5. Repeatable What is actually repeatable? \nWe would like the process to be repeatable. If the content of the output of the method is not repeatable \nthat is not so serious, but would be desirable. If two groups come to the same set of models with the \nsame content that would be better than not, but the workshop was divided as to how critical this aspect \nis. 6. Testability 1. Specification has to be testable. 2. This is a difficult research issue. 3. \nProcess must allow (method must incorporate) testing.  7. Multiple Views, multiple models.  The domain \nexpert user of the method must be given various levels of access to the models. The purpose of these \nviews is to promote user understanding. This is like providing (Coad) subjects or OSA high level objects. \n8. Scalability Two types of scalability can be considered. Team size and problem size. The workshop considered \nProblem size more fundamental than team size. 8a. A special case of scalability is Decomposition and \nMerge rules so that groups have a way to work on the models. 9. Applicability The method show apply \nto a wide variety of problem domains. 10. Allow for reuse. This means artifact reuse. It also means the \nability to iterate over the requirements. It also means the ability to support various life cycles. \n11. Termination: The method should support the ending of the process. It should support ways to determine \nwhen the practitioner is done. 12. The Process should be a guideline, not too strict for using it in \nnovel situations. 13. The terms used in the method should be defined, the method should be teachable. \n 14. The method should lend itself to maintainability. 15. The method/process should be manageable. \nThe users of the method process should be able to know when and how to report progress. 16. The method \nshould promote domain experts usage. 17. The method/process should exhibit a continuity between OOA \nand OOD. 18. The core concept of the process/method should be 00. 19. The method/process should concern \nitself with concurrency.   \n\t\t\t", "proc_id": "157709", "abstract": "", "authors": [{"name": "Dennis de Champeaux", "author_profile_id": "81100265123", "affiliation": "", "person_id": "PP31073870", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/157709.157723", "year": "1992", "article_id": "157723", "conference": "OOPSLA", "title": "The OOA process", "url": "http://dl.acm.org/citation.cfm?id=157723"}