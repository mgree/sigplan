{"article_publication_date": "12-01-1992", "fulltext": "\n Addendum A 92 fo fhe Vancouver, British Columbia, Canada 5-10 October 1992 Proceedings Workshop Report- \n Metrics for Object-Oriented Software Development Reported by: Teri Roberts Organizers: Sam Adams Knowledge \nSystems Corporation Teri Roberts Object International, Inc. Rajiv Tewari Temple University  Student \nVolunteer: Ralph Jackson 1. Introduction Use of the object-oriented paradigm for software solutions \nhas gained momentum and popularity. The metrics selection activity has been incubating and is ready to \nhatch. We are interested in nurturing the growth and use of a variety of metrics for the object- oriented \nparadigm. Described herein are the activities and results of the first OOPSLA workshop on metrics. Interest \nand participation will grow as the object-oriented paradigm matures through it s practical application. \n 2. Workshop Objectives, Goals, and Format The objective, as stated in the advance program, was to propose \nmetrics for estimating cost and schedule, for evaluating productivity of object-oriented techniques and \nto gauge improvement in software quality and programmer productivity. In spite of limited response we \nforged ahead with a goal of reaching some level of consensus about useful metrics based on a format of \npresenting and discussing participant experiences. After the welcoming administrivia Teri Roberts suggested \na product metrics versus process metrics point of view. By the end of the workshop we agreed that we \nhad a better handle on product metrics than on process metrics. Contact the individual participants (see \nSection 8) for a copy of their presentation materials. 3. Recent Directions Sam Adams started the day \nby sharing his angle on metrics derived from recent work. He proposed 4 measurement levels: method, class, \ncomponent/application and project management. The first 3 belong to the product camp, while the last \nis in the process camp. He discussed 33 method metrics, combined the class and component levels for a \ntotal of 13 metrics, and presented 8 project management metrics. He also cited 4 issues to consider for \nmetric usage.  4. Participant Presentation Summaries David Tegarden s work is focused on structural \ncomplexity and perceptual complexity. He is trying to map perception to structure. Both of these aspects \nare based on the system complexity, which he sees as part problem complexity and part technique complexity. \n He introduced 4 levels of Droduct metrics: svstem. object, variable and method. At each level he- identified \na set of graph-theoretic and lexically- based measures. He showed a 3 dimensional measurement framework \nwhere these metric levels are mapped against analysis, design, programming, and maintenance activities \nand against generalization, aggregation, association, inheritance, and message passing relationships. \nThis was followed by a cognitive map of perceptions of object-oriented system complexity derived from \na pilot study involving 7 graduate students who were asked, What contributes to the complexity of an \nobject-oriented system? . The students proposed 10 categories which were rated for importance in controlling \ncomplexity. Four of them (class design, structure, method design, and message passing) were mapped back \nto the structural complexity aspect using the 4 levels (system, object, variable and method). Most of \nthe mappings were certain, however, some were questionable or weak. The students then built linkages \nbetween their 10 categories in the cognitive map via pair-wise comparisons to describe the effect of \nincreasing the complexity of one category on the complexity of the other categories. Sallie Henry has \nworked for years on metrics and concludes that traditional complexity metrics like lexical token count, \ncontrol flow complexity of procedures or functions, and inter-connectivity of statements or system components \nare just not suitable for the object oriented paradigm. The mind set and goals are different. Her research \nobjectives are to define and validate object-oriented software complexity metrics, to understand maintenance \nactivities in object-oriented systems and to propose metric instrumentation in an object-oriented software \nlife cycle. She presented 6 questions and hypotheses, gave the results, then backed them up with data \ngathered from 2 large software systems, one for user interface management and on for quality evaluation. \nSallie s first validation model started with the set of metrics proposed by Chidamber and Kemerer last \nyear at OOPSLA. She had automation problems with the coupling between objects (CBO) metric as defined \nby Chidamber and substituted 3 different metrics for coupling complexity and added 1 public interface \nmetric. A second validation model used two size metrics from Dick Nance and Dennis Kafum OOPSLA 92 Her \nobjectives were focused around maintenance activities and the data was hard to get. After a valiant effort \nto find maintenance data, multiple linear regression and cross validation here are the conclusions: Size \nmetrics are important but are not sole predictors and they were incorporated into the other metrics. \n. Prediction of maintenance is possible with the full set of Chidamber based metrics. Dirk Meyerhoff \npresented an overview of the Metrics Education ToolKIT (METKIT) Computer Aided Instruction (CAI) System. \nIts main goal is to support the use of measurement in software enginering by providing required conceptual \nknowledge about software measurement on the computer. Hypertext and graphical browsers are combined to \npresent textual information on concepts and a graphic visualization of the relationships between them. \nThe system can be used by authors or experts to define and update knowledge about concepts and their \nrelationships, and by readers or learners to browse the defined knowledge. Dirk showed how concepts that \nare specific to measuring of Object Oriented Systems can be included in the system such that a learner \ncan find, for instance, the definition of some measure, the entities and attributes it is supposed to \nmeasure, and tools that could be used for the actual measurement. Ross Huitt shared his concern for metrics \nthat focused on maintenance and understandability (readability). A series of bar charts were used to \ngraphically illustrate that 00 has shifted the complexity in systems. If we use traditional metrics we \nwon t be measuring where the complexity is in 00 systems, but rather where it used to be in structured \nsystems. Function size and cyclomatic complexity aren t measuring the more significant aspect of complexity \nin 00 systems. He thought that the Chidamber and Kemerer metrics were a fairly good start, but also had \nsome argument with how depth of inheritance tree (DIT), coupling between objects (CBO), and response \nfor a class (RFC) were defied. He uses automated support from a Metrics for Object-Oriented Software \nEngineering (MOOSE) tool based on modified Chidamber metrics. Ross has plans to fully instrument and \nmeasure a large scale project next year and the results will be nublished. 1 Pedro Inacio shared his \nproject experience with the OBLOG (OBject LOGical) Workbench and made a Vancouver, British Columbia \nplea for metrics at the design and analysis levels, rather than at the implementation level. He works \nwith high level specification languages and plans to generate code. He is looking for patterns that can \nbe measured with validated metrics so he can keep his managers interested in the object-oriented paradigm. \nThere is a real need for some process oriented metrics. Steven Bilow also came in search of metrics that \ncan be applied prior to design and implementation. His goal is to quantify design complexity early in \nthe product lifecycle thereby reducing implementation and maintenance complexity and effort. He presented \n2 unlabelled graphs of roughly the same complexity and indicated that they were remarkable similar. He \nthen identified these graphs as being a control flow diagram from McCabe s paper on cyclomatic complexity, \nand a state model from Shlaer and Mellor s book Object Lifecycles. He noted that there is a strong structural \nrelationship between models of control flow and those of object state and inferred that graph theoretical \nmeasures (such as variations on cyclomatic complexity) may be able to quantify the complexity of the \nstate models of object-oriented systems. Each of the 2 graphs shown were fully connected. Since graph \ntheory provides a way to describe the complexity of any fully connected graph, similar techniques could \nbe used as complexity metrics for both control flow and object state. Since McCabe s metrics for control \nflow is well accepted, similar state model metrics should be further explored. Steven concludes: . Graph \ntheory is useful in evaluating the complexity of object-oriented state models. . Some object-oriented \ninformation models can be characterized, but work is incomplete. . Chidamber &#38; Kemerer had better \nmetrics for single inheritance hierarchy and encapsulation. But even in these cases we can learn a lot \nabout a system by looking at which states are visible from the outside versus those that are hidden internally. \nSo even in the case of encapsulation, state model metrics may prove a useful method of analysis.  5. \nGroup Discussion and Consensus The group agreed that process metrics are important, but without good \nproduct metrics first, resource consumption measurements (the basis of process metrics) are not possible. \nWe have to start with what is available right now. And even though it is scant, there is more data for \nvalidating product metrics than process metrics. Managers are not ready for object-oriented process metrics \nyet. The right mind set for using these metrics is very important and isn t very evident. So we narrowed \nour discussion to product metrics for analysis, design and implementation. If source code (implementation) \nmetrics can predict maintainability, perhaps the lower level metrics can be abstracted up into the design \nlevel and into the analysis level. But representation plays a big role. There is no common agreement \nabout what constitutes the output of object-oriented analysis and design activities and this complicates \nthe selection of appropriate metrics for these levels. If you want to work from the top down, rather \nthan bottom up, you have to choose your methodology first and let it guide your selection of metrics. \nThe point is, what you choose should map from one level to the next. Choose what you think is important \nand what you can measure. 6-12 metrics should be sufficient. Be careful if you take a large number of \nmetrics then condense them down into a few categories . . . you can lie with statistics. It is harder \nto say what is important and easier to say what can be measured. Complexity is in communication links \nand dynamic dispatch. Simple access (read/write/initialize) of variables (1 or 2 lines of code-no side \neffect external to the object) can t really be considered to add complexity. This introduces the concept \nof intra and inter communication levels. Complexity is NOT productivity, NOR quality. Concurrent and \ndistributed systems have another kind of complexity-concurrency of objects and distribution of objects \nseparately and concurrent distribution of objects collectively. We have no idea how to measure these \nhighly dynamic systems. Our static metrics are barely off the ground. Automation is a necessity, but \nwhat do you automate? Here are our suggestions: CLASSES but- We also need a larger grained measure (for \ncategories, packages, building blocks, mechanisms, patterns, clusters-whatever you care to call them) \nand tools that operate on these meaningful sets. Inheritance really throws a curve into things and we \ndon t agree on the best way to measure that yet. Problem domain will make a difference for calibrating \nthe chosen set. We must be careful that semantics don t get lost in the packaging. COUPLING but-at units \nlarger than a single class. This was seen as one of the weaknesses of the Chidamber metrics-the level \nis too low. COHESION but-at units larger than a single class. For the same reason.  STRUCTURE which \nincludes inheritance, aggregation and association.  6. Future Directions After we get product metrics \nnailed down and validated, then we can move on to process metrics. We need more experience and data from \nprojects. We want to have a workshop next year and invite interested participants to focus on the product \nmetrics we have recommended and help us validate them. 7. Participant Papers Object-Oriented System \nComplexity: An Integrated Model of Structure and Perceptions David P. Tegarden Information and Decision \nSciences School of Business and Public Administration Califormia State University, San Bernardino San \nBernardino, CA 92407-2397 tegarden@gallium.csusb.edu Steven D. Sheetz Graduate School of Business University \nof Colorado Boulder, CO 80309-0419  sheetz-s@cubidr.colorado.edu Metrics for Object Oriented Systems \nSallie Henry Wei Li Computer Science Department Virginia Tech Blacksburg,VA 24062  henry@vtopus.cs.vt.edu \n Metrics for Object-Oriented Software Development-Workshop Position Paper Ross Huitt Bellcore RRC-lH206 \n444 Hoes Lane Piscataway, NJ 08854  bytor@ctt.bellcore.com Metrics for Object-Oriented Software Development: \nthe OBLOG Workbench Project Experience Pedro Barros Inacio (pbi@solo.inesc.pt) Peter Hartel @et@solo.inesc.pt) \nEspirito Santo Data Informatica SA. Avenida Avlares Cabral4 l-5 1200 Lisbon, Portugal  Making Knowledge \nabout Software Measurement Available on a Computer: The MBTKIT CAI System D. Meyerhoff M. Milllerburg \nGMD Schloss Birlinghoven D-2505 Sankt Augustin 1 Germany  ma irk@gmdzi.gmd.de Borrowing from McCabe: \nWhat Object-Oriented Methodologists Can Learn from Cyclomatic Complexity Steven C. Bilow Network Displays \nDivision Tektronix, Inc. P.O. Box 1000 60-646 Wilsonville, OR 97070-1000 steveb@orca.wv.tek.com  Contact \ninformation: Sam Adams Knowledge Systems Corporation 114 MacKenan Drive, Suite 100 Cary, NC 275 11 Email: \nksc@cup.portal.com Teri Roberts Object International, Inc. 8 140 N. MoPac Expwy, Bldg 4 - Suite 200 Austin, \nTX 78759 Email: 71210.3642@compuserve.com -or- coad@applelink.apple.com Rajiv Tewari Temple University \nCIS Department Philadelphia, PA 19122 Email: tewari@astro.ocis.temple.edu   \n\t\t\t", "proc_id": "157709", "abstract": "", "authors": [{"name": "Teri Roberts", "author_profile_id": "81100585576", "affiliation": "", "person_id": "PP14202221", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/157709.157728", "year": "1992", "article_id": "157728", "conference": "OOPSLA", "title": "Metrics for object-oriented software development", "url": "http://dl.acm.org/citation.cfm?id=157728"}