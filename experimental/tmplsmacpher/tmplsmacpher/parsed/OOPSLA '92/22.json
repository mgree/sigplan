{"article_publication_date": "12-01-1992", "fulltext": "\n Addendum A 92 to the Vancouver, British Columbia, Canada 5-10 October 1992 Proceedings Workshop Report- \n Objects for Changeable Systems Moderator: Magnus Christerson Objective Systems SF AB Introduction and \nsummary This workshop gathered people interested in finding ways to develop object-oriented systems in \na way so that changes of the system easily can be done. Experiences and investigations on existing systems \nand various approaches were presented and discussed. A large consensus was established that only pure \nobject-orientation in its traditional sense is not enough. Special effort must be put in to a design \nto give an extensible and robust system which minimizes maintenance costs. The largest consensus was \naround the fact that object-oriented analysis, design and implementation tend to focus on information-structures \nin the class structures. If additional objects that focus more on functions of the system are added a \nsignificant decrease of complexity can he obtained. Additionally, changes of such systems tend to be \nmore local in the classes. Thus, a design must consider both functions and data to find a changeable \nstructure. The workshop also noted that an ordered collection of data on what kinds of changes come to \na system has not been published publicly although we know that such investigations have been done. The \n workshop suggested a very simple classification taxonomy to start classify the changes with and we encourage \npeople to classify their changes in this taxonomy and publish the results. Until we have more scientific \ndata we suggest that a list of anticipated changes is written down together with the requirements and \nis used as an important input to design.  Participants Ken Auer Doug Benett Larry Constantine Ward Cunningham \nWayne Haythom Lauren Lemarchanol Karl Lieberherr William Opdyke Alain Plantec Wolfgang Pree Ramana Rao \nChris Rigatuso Yoshinobu Yamamura and Magnus Christerson Moderator Lodewiik Bergmans Student volunteer \n Karl Lieberherr Knowledge Systems Corp. Designways The OPTions group Cunningham &#38; Cunningham Oregon \nGraduate Institute Northeastern University AT&#38;T Bell labs Washington University Xerox PARC Objectivity \nIBM Japan Objective Systems University of Twente Karl presented the ideas behind Demeter. It avoids the \nhardwiring of methods to classes so that the class structure may change without changing the description \nof the methods (the dynamics). The dynamic description is described separated from the class structure \nusing propagation patterns. Demeter thus uses mainly two kinds of descriptions: one for the dynamics \nwhich describes how the dynamic computation are performed and one for the data structures which describes \nhow the classes are structured. Demeter describes an application in mainly two views. The first such \nview is the class dictionary. It describes how classes are related and how these collaborate using a \nstatical description, i.e. only describing statical relationships as inheritance, consistof, attributes, \nacquaintance, etc. This gives one design structure for the solution of the problem. These class dictionaries \nmay be derived from existing examples. The second view is the propagation pattern. These describes how \na specific function should be evaluated. However, to describe this computation typically only a part \nof the class structure is interesting to view. Much of the explicit path in the structure is typically \nonly concerned with propagating a call to its destination. Thus, to describe the computation, normally, \nwe only need to hook the computation up to certain classes, and the actual path between these classes \ncan be automatically derived from the class structure. The propagation patten are thus described using \nonly imaginary classes which should be present in the class dictionary. In this sense Demeter is very \nmuch concerned with graph oriented programming. The same propagation pattern can now be used unchanged \nfor many changes in the class structure. In the same sense, the propagation pattern can be changed in \nmany ways without changing the class structures. However, for every change in either of the descriptions, \na manual check needs to be done to see that the descriptions are still valid. Demeter has grown out of \nthe use of the Law of Demeter which is a set of rules to design good 00 programs. The law mainly states \nthat when implementing one operation, other operations may only be invoked if: 1) they belong to the \ncurrent class 2) they belong to a class that the current class has a direct relationship to 3) classes \nthat are arguments to the operation or 4) classes of which the operation creates instances. Mainly this \nrule constraints to only have coupling to directly related classes. But when this rule is applied typically \na lot of propagation operations must he implemented. These propagation operations are automatically generated \nby Demeter when applying the propagation pattern to the class dictionary, and thus do not need to be \nmaintained by the programmer. The binding of propagation patterns to class- dictionary are now done prior \nto compile time. Ward Cunningham pointed out that it maybe would be possible to do this even at run time, \ngiving an even more flexible system that could be changed during run-time. However, this would have to \nbe investigated in more detail before any conclusions can be drawn.  Larry Constantine Larry initially \nstated that changes may affect a system in various ways. For instance, changing some data structure, \nchanging some specific function etc. Various methods address different kinds of changes. A method may \nbe beneficial for some kind of change, but likely will cause sacrifices for other types of change. An \nexample were presented extending an example originally stated by Dave Thomas. In a (simple) banking application \nwe need to manage different kinds of accounts, thus an Account class. The various accounts differ and \ncan thus he subclasses of Account, e.g. Checking account, Savings account etc. Now, if we want to add \na new account we typically add a new subclass. Simple. But if we want to add a new function, say printing \na new kind of tax report for the government, we probably would have to implement this new function spread \nout over all types of accounts. Not so simple. On the other hand, looking at the same example using a \nfunctional decomposition, we would have structured the application around the functions, Open, Deposit, \nWithdrawal, etc. Now adding a new type of account to this structure would force us to go through and \nchange (add a new condition) in all these functions. Not so simple. But if we want to add a new function \nfor the report we simple add this function Report. Simple. Larry s conclusion is that there is no general \nbest approach. One approach he suggested is that for event driven problems, typically functional decomposition \nis more suitable, whereas for data-oriented problems are better solved by an object-oriented approach. \nThe problem is that most systems are a combination of both. Most of the workshop participators therefore \nagreed that typically a good design needed to focus on both. Larry also pointed out that, to his knowledge, \nthere hasn t been published any real investigation on what changes typically comes to a system. If we \nknew that for different classes of systems, we could also optimize the architectures for each specific \nsystems. One of the outcome of the workshop was to suggest a simple such classification taxonomy, see \nbelow under Conclusion.  Wayne Haythorn Wayne had had the same feeling as Larry presented and felt that \nin his area a pure object-oriented design is better, but only slightly better, than a pure functional \ndecomposition. To investigate this further Wayne had developed three designs to a simple banking simulation \nsystem. The first design was a pure functional decomposition with a main loop controlling the events \nhappening. This design was implemented in C. The second design was a pure naive object-oriented design \nwith only domain objects representing Customer, Teller etc. This design was implemented in C++. The third \ndesign was encountered when Wayne looked at his second design and saw several ways to improve it. Since \nthe simulation much centered around events he started to rebuild the application around these events \nrepresented with an event class. He then turned out with a much cleaner design, but definitely not a \ntypical 00-design encountered in 00-books. It was rather a mixture where polymorphism was used extensively. \nThis design was implemented in C++. Then Wayne started to introduce changes in these designs and measured \nhow many percent of the functions in each design he needed to change in. The result as below: Change \nFunctional decomposition I Naive 00 I I Poly- I I morphic Add type of teller 64% 56% 28% Multiple queues \n45% 54% 10% Cust can leave queue 39% 43% 19% Transaction types 75% 56% 28% Rush hours 6% 17% 11% Varying \n#tellers 23% 26% 10% Cash machines &#38; phone 89% 100% 52% Wayne also measured the module complexity \nin the three different designs. The third design had a significantly lower level of complexity in its \nmodules. Wavne concluded that the third design was the generally best design of them all. But what was \nthe key to this design? One key was for sure the Event class. This application was largely centered around \nthis event. When looking at the other designs, Wayne noticed that what in general drove up the complexities \nof the other two designs was the testing on the events, i.e. many CASE-statements (switch in C or C++) \nwas centered around this data. Making this data as a class of its own-the event class-and using polymorphism \nto test implicitly the type of event reduced the complexity of the designs. Additionally, to foresee \nthe anticipated changes, what potential type-testing could be needed also made the third design more \nrobust against changes. What then is the general key to find this third type of designs. Wayne had come \nup with some general rules: The desien needs to make extensive use of polymor phism to reduce (potential) \ncomplexity. To find these controlling classes any design could be investigated, and then start to add \nchanges, and anywhere we need to add a test on the type, the data we test on should be made a class of \nits own with subclasses which are the true options. In this way we need to add a list of anticipated \nchanges as one of the key inputs to design. During the design use this list and run these changes back \nagainst the proposed design. Such a list is often easy to make since often the designers knows what the \nchanges will be, but this knowledge is seldom used. - Wayne thus summarized his experience to emphasize \nextensibility and maintainability by looking forward in the life cycle of the system. Ask yourself what \nproblems in the future you-will wish to be already solved, instead of solving the problems in front of \nyour nose.  Ward Cunningham Ward presented his experience from building a bond accounting product for \nportfolio management using Smalltalk. This system have now been developed for 3 years with 4-6 engineers \nworking on it. They have used an incremental growth and repair strategy, i.e. they have renewed part \nof the code on an annual basis. They have used a nouns strategy to find the classes from the domain experts. \nThese classes helped them in the design in 9 cases of 10 and often formed a thorough basis of a good \ndesign. But as the system evolved the complexity of the design grew. They started to focus not so much \non reuse, but more on flexibility to capture problem diversity and continously improving the design. \nHowever, the comulexitv obstructed the efforts to improve the design. - When a new service was to be \nintroduced in the system, they saw that this change should really affect many of the classes in the system. \nThe new function was a new report to be published which needed information that was not present in one \nplace, but needed to collect information (dates of buying and selling) from different bonds. So instead \nof spreading this function over several classes, they introduced a new class Advancer that managed this \nfunction. This object was not in the problem domain, and the domain experts did not recognize it. However, \nwhen presented to the domain experts they realized that this was a new way of looking at their problem \ndomain which gave them a new insight. But with this new class in place they not only could easily introduce \nthe new function easily, but also decrease the complexity of the rest of the application -they started \nto fiid new ways of using this new class. However, with this new class in place they started to have \ntwo competing software architectures in their application. They are now thinking of slowly migrating \ntheir design to this new kind of design, but since they feel that they are leaving the pure, naive object-orientation \nin favour of a more behaviour oriented design. (Ward stated later that after this workshop he had gotten \nenough proof that this definitely was the right way to go.) To find these new classes requires a deeper \nknowledge and insight in not only the problem domain, but also in the current chosen design. Therefore \nhe concluded that in addition to noun- objects, design experience is a crucial way to find a stable and \nrobust object structure. Additionally, an incremental design repair is also needed to slowly enhance \nthe design to keep it in shape for future extensions and maintenance. For long-lived programs, re-engineering \nand iterative design are therefore indispensable. Doug Bennet Doug presented his view of building stable \nsoftware structures. He proposed a view which is more mechanical; construction software by building from \nlower level components. Doug thus presented a layered architecture of how this composition may work. \nThe layers must address both business side and user side needs. The main idea behind these layers is \nextensibility: 1) Sites of stability (domain objects) must be maintained.  2) Sites of change must be \nencapsulated and hidden from each other. Also reusability is an issue. It requires design and interface \nconsistency across all objects. The layers are structured in a way so that every layer represent one \naspect of the system and that all layers may use everything in the underlying layers. The layers are \nas shown: Presentation Control Services Data The lowest layers represent the Data (things). These are \npassive and are only operated on by the Service layer. The data layer keeps all the data of the application. \nThe Service layer represents the services that the Data layer offer. Here is generic and reusable services \nfor different functions of the application. The Control layer keeps the logic of the application, it \nmanages events. This layer typically encapsulates what is most likely to be changed. The highest layer \nrepresents the presentation and the dialogue issues of the application. Changes to the interface should \nonly affect this layer. This forms a generic architecture with channels as buses between the functionality \nin these layers. This architecture assumes that changes typically come (locally) in these four layers. \nHowever, no data confiiing this claim could be presented. One hard issue is that of finding the right \nsemantical level on the interfaces between these layers, i.e. to find what level a specific functionality \nshould be placed at.  Magnus Christerson Magnus presented the robustness analysis activity in Objectory \nwhich aims at finding a robust and changeable object structure. Some of the experiences using it was \nalso presented. Robustness analysis is based on the experience using a method which mainly based the \ndesign on problem domain objects. Many times changes was easily incorporated, but in some cases the changes \naffected several objects. The goal of robustness analysis is to keep the good properties with domain \nobjects, but to reduce the problems with these changes. The technique mainly base the object structure \non three different kinds of objects, each of focusing on a specific type of change. Base the structure \non domain objects that are used in many different functions. Use these as resources that are reused when \nadding new functionality. These are called entity objects. The functionality needed to manage the interfaces \nare placed in separate interface objects. These interface objects encapsulate interface specific functionality. \n For any new function (use case of the application) added, reuse as much as possible of the existing \ninterface and entity objects. If there is any functionality left that is specific to this function, place \nthat in a control object. Having it in one place will make it easier to do changes in the use case, i.e. \nhaving a strong degree of traceability from requirements to implementation. Objectory have now been used \nin more than 20 projects and the experiences are very good. Although metrics have not been defined or \ncollected to support this observation, the observations are that new changes are easily introduced. This \nis very positive since almost all projects have had problems with changing requirements also developing \nthe first product version. These changes have been managed in a very clear and seamless way. Magnus described \nhow this technique is developed further based on existing experiences. One crucial point is the separation \nof the functionality to control objects. Much of the true extensibility of the product lies in how successful \nthis partition has been. The more you place in entity objects the easier it will be to add new functionality \nsince then the control objects will be quite small. But on the other hand, if you go to far, the functionality \nof the entity objects will be to use case specific and thus not very reusable for new use cases making \nadditions more expensive than necessary.  Yoshi Yamamura Yoshi described YARN, Yet Another RequiremeNt, \nwhich is a technique that grew out of the need to manage new requirements coming late in the development \nprojects. It is focused to manage very large systems (~100 Million lines of code). YARN assumed three \nbasic technologies: 1) Object-orientation to manage complexity of the large sized systems. 2) Formal \nspecification to manage the large user community and the many invalid, inconsistent and vague requirements. \n 3) Evolution-enabled to prepare for a long lived and maintainable system in the future. The basic assumption \nin YARN is that changing user requirements is an essential issue in software development. Therefore also \nthe requirements specification must be able to evolve. The benefits with a formal specification language \nis that it gives you a lot of early type-checking and gives you also a more defined way to introduce \nmetrics early in the development process. Yoshi divides the maintenance problem into 1) identification \nof where to change 2) doing the modification and 3) verifying and validating the changed software. One \nof the major ideas in YARN is to be able to all modifications locally, i.e. separate the concerns of \nthe maintenance. This also includes to dynamically adjust the data structures. To do the identification \nof where to modify, Yoshi drew parallels with Belady s paging algorithm for paging for virtual memory \nin operating systems. The aim in YARN is to, as soon as possible, finding the part (page) where the modification \nshould be made. This constraints hard traceability between specification and implementation. One problem \nis to capture the evolution of the semantics of the system. A dynamic as well as a static representation \nis then needed at the specification level. In this way it is also necessary to be able to map the dynamic \nrepresentation (in terms of objects) into static structures (in terms of classes).  Bill Opdyke Bill \npresented his research in refactoring object- oriented application frameworks. Frameworks are larger \ngrained modules than classes, typically a group of classes working together to offer some functionality. \nFrameworks make explicit the factoring of functions and also the design of interfaces. Refactoring concerns \nevolving and refining these frameworks to get a truly reusable frameworks. Typically good framework designs \ntakes a lot of iteration so that it evolves from several concrete examples. However, the task is not \nwell understood, takes time and may introduce defects into the program. The goal of the research was \ntherefore to look at this problem and to go towards automated support for this process. Bill showed an \nexample where the generalization of an existing class was done when a new case should be covered. This \nnew case could be covered by use of polymorphism and extracting out a special superclass. The superclass \nwas first created and then common variables and functions was migrated up to this superclass. Typically \nthe definition of this new superclass involved issues like, making the functions compatible by defining \nnew functions to capture the differences and replacing the differences with (virtual) function calls, \nmaking the variables compatible, and moving the common parts to the superclass. To migrate a function \nup to a superclass the following steps needed to be done with a behavior preserving technique; convert \nthe code segment to a member function and replace it with a call, create the member function in the superclass \nand delete the member functions in the subclasses. A safe refactoring must be behavior preserving. For \nexample, it is important not to violate naming/scoping rules or type rules, references and operations \nneed to be semantically equivalent. To ensure safety, Bill felt that preconditions and invariants would \nbe of great help to automate the process. Converting CASE-statements to polymorphic calls is another \ntypical refactoring. When doing this a class invariant would be helpful since then new classes created \nthen assumes that the CASE test is then generally true. This refactoring will simplify conditional statements. \nThe third refactoring was concerned with converting inheritance to aggregation and reusable components. \nHere aggregation have been identified which were modeled/implemented as inheritance and by moving members \nbetween aggregates and component classes a better design could be obtained. Bill also introduced a prototype \nof a system that helps in the refactoring process. Still much human interaction is needed, but the systems \nhelps in managing cross-references and handles refactoring for a constrained set of programs. (Bill mentioned \nthat he would like to go home and redesign part of his system after the discussions in this workshop.) \n Conclusion The workshop came to a large consensus that a good design needs to focus not only on data, \nas often is the case with domain objects, but also on the behavior. To capture these behavioral objects, \nthe design needs to focus much more on the functions of the system. Hence a good design does not only \nconsist of classes representing domain objects (nouns), but also on the controlling of these of the functionality. \nAnother key issue is to design with a list of anticipated changes in mind. Therefore this list should \npreferably be written down at the same time as the requirement specification. The design should then \nuse this list as a key input and e.g. all reviews could be made partially against this list. The workshop \nalso noted that there has not been published any scientific investigations on what changes are the most \ncommon to a system (although we know that such investigations exists inhouse). We therefore propose that \nsuch data starts to be collected. As a first simple classification taxonomy, we propose that all changes \nare categorized into the following groups: 1) Changes specific to the system interface (e.g. changing \nhow a form looks like) 2) Changes regarding the data or information held by the system (e.g. adding \nan attribute to an object) 3) Changes regarding the functions of the system (e.g. adding a service for \nprinting a new kind of report) 4) Changes of the implementation technology (e.g. changing version of \nthe DBMS used) 5) Others Although a change may affect more than one of these, all changes should preferably \nbe broken down so that they can be placed in one of these groups. It is also of high interest to regard \nthe effort combined with the modification. Therefore associated with every change, a number telling the \neffort (e.g. in man- hours) should be attached. By summing up all these figures we can learn on what \nwe should optimize our designs against. These data should also be collected to determine its correlation \nto the type of systems, i.e. to classify the outcome to what class of systems we are investigating (e.g. \nprocess control, MIS application, CASE-tool). Another outcome of the workshop was that, until we have \ngained further insight, to get good designs we need to do incremental repair of our existing designs. \nHowever, when doing this, we can learn much to be used as basis for our future designs.  Acknowledgements \nI hereby wish to thank all the participants of the workshop for a really fresh and enlightening discussion. \nI believe the workshop gave us all many new insights of what constitutes good designs. For sure object-orientation \nis a strong tool for doing this, but it must be treated with care and used together with a focus on the \ndynamics of the system.  The papers submitted were: Doug Bennet, Kinds of objects for changeable systems \n Larry Constantine, Object-oriented and Function-oriented software structure Magnus Christerson, Robustness \nanalysis-a technique to obtain extensible software Hakan Dyrhage &#38; Magnus Christerson, Seamlessness \nequals the inverse of extensibility Wayne Haythom, Defensive analysis to capture potential change Ralph \nJohnson, Constraints (submitted also to Architecture workshop) Bill Opdyke, Refactoring of class hierarchies \n Wolfgang Pree, Reusable classes and abstractions D Todoroy, The problems of adaptable programming \nYoshinubu Yamamura, Evolving Object-Oriented specifications If you want copies of any of the papers \nplease send a request by E-mail to magnus@os.se.  Contact information: Magnus Christerson Objective \nSystems SF AB PO Box 1128 S-164 22 Kista Sweden Tel: + 46 8 703 45 80 Fax:+468751 3096 E-mail: magnus@os.se \n  \n\t\t\t", "proc_id": "157709", "abstract": "", "authors": [{"name": "Magnus Christerson", "author_profile_id": "81319489838", "affiliation": "", "person_id": "PP31074617", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/157709.157731", "year": "1992", "article_id": "157731", "conference": "OOPSLA", "title": "Objects for changeable systems", "url": "http://dl.acm.org/citation.cfm?id=157731"}