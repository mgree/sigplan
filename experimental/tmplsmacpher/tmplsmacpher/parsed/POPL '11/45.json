{"article_publication_date": "01-26-2011", "fulltext": "\n Loop Transformations: Convexity, Pruning and Optimization Louis-No \u00a8el Pouchet Uday Bondhugula C\u00b4edric \nBastoul The Ohio State University IBM T.J. Watson Research Center University of Paris-Sud 11 pouchet@cse.ohio-state.edu \nubondhug@us.ibm.com cedric.bastoul@inria.fr Albert Cohen J. Ramanujam P. Sadayappan Nicolas Vasilache \nINRIA Louisiana State University The Ohio State University Reservoir Labs, Inc. albert.cohen@inria.fr \njxr@ece.lsu.edu saday@cse.ohio-state.edu vasilache@reservoir.com Abstract High-level loop transformations \nare a key instrument in mapping computational kernels to effectively exploit resources in modern processor \narchitectures. However, determining appropriate compo\u00adsitions of loop transformations to achieve this \nremains a signif\u00adicantly challenging task; current compilers may achieve signi.\u00adcantly lower performance \nthan hand-optimized programs. To ad\u00address this fundamental challenge, we .rst present a convex char\u00adacterization \nof all distinct, semantics-preserving, multidimensional af.ne transformations. We then bring together \nalgebraic, algorith\u00admic, and performance analysis results to design a tractable opti\u00admization algorithm \nover this highly expressive space. The frame\u00adwork has been implemented and validated experimentally on \na representative set of benchmarks run on state-of-the-art multi-core platforms. Categories and Subject \nDescriptors D 3.4 [Programming lan\u00adguages]: Processor Compilers; Optimization General Terms Algorithms; \nPerformance Keywords Compilation; Compiler Optimization; Parallelism; LoopTransformations; Af.ne Scheduling \n 1. Introduction Loop nest optimization continues to drive much of the ongoing research in the .elds \nof optimizing compilation [8, 27, 33], high\u00adlevel hardware synthesis [22], and adaptive library generation \n[19, 47]. Loop nest optimization attempts to map the proper granularity of independent computation to \na complex hierarchy of memory, computing, and interconnection resources. Despite four decades of research \nand development, it remains a challenging task for compiler designers and a frustrating experience for \nprogrammers the performance gap, between expert-written code for a given machine and that optimized and \nparallelized automatically by a compiler, is widening with newer generations of hardware. Permission \nto make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithout \nfeeprovidedthat copies arenot madeordistributed forpro.torcommercialadvantage andthatcopiesbearthisnoticeandthefullcitation \nonthe .rstpage.To copy otherwise,to republish,topostonserversorto redistribute tolists, requirespriorspeci.cpermission \nand/ora fee. POPL 11, January26 28,2011,Austin,Texas,USA. Copyright c &#38;#169; 2011ACM978-1-4503-0490-0/11/01. \n. .$10.00 One reason for this widening gap is due to the way loop nest optimizers attempt to decompose \nthe global optimization problem into simpler sub-problems.Recent results in feedback-directed op\u00adtimization \n[33] indicate that oversimpli.cation of this decomposi\u00adtion is largely responsible for the failure. The \npolyhedral compiler framework provides a convenient and powerful abstraction to apply composite transformations \nin one step [20, 24]. However, the space of af.ne transformations is extremely large. Linear optimization \nin such a high-dimensional space raises complexity issues and its ef\u00adfectiveness is limited by the lack \nof accurate pro.tability models. This paper makes fundamental progresses in the understanding of polyhedral \nloop nest optimization. It is organized as follows. Section 2 formalizes the optimization problem and \ncontributes a complete, convex characterization of all distinct, semantics\u00adpreserving, multidimensional \naf.ne transformations. Focusing on one critical slice of the transformation space, Section 3 pro\u00adposes \na general framework to reason about all distinct, semantics\u00adpreserving, multidimensional statement interleavings. \nSection 4 presents a complete and tractable optimization algorithm driven by the iterative evaluation \nof these interleavings. Section 5 presents experimental data.Relatedworkis discussedin Section6. 2. \nProblem Statement and Formalization The formal semantics of a language allows the de.nition of pro\u00adgram \ntransformations and equivalence criteria to validate these transformations. Denotational semantics can \nbe used as an abstrac\u00adtion to decide on the functional equivalence of two programs. But operational semantics \nis preferred to express program transforma\u00adtions themselves, especially in the case of transformations \nimpact\u00ading the detailed interaction with particular hardware. Program op\u00adtimization amounts to searching \nfor a particular point in a space of semantics-preserving transformations. When considering optimiz\u00ading \ncompilation as a whole, the space has no particular property that makes it amenable to anyformal characterization \nor optimiza\u00adtion algorithm. As a consequence, compilers decompose the prob\u00adlem into sub-problems that \ncan be formally de.ned, and for which complexity results and effective optimization algorithms can be \nde\u00adrived. The ability to encompass a large set of program transformations into a single, well understood \noptimization problem is the strongest asset of the polyhedral compilation model. In the polyhedral model, \ntransformations are validated against dependence relations among dynamic instances of individual statements \nof a loop nest. The de\u00adpendence relation is the denotational abstraction that de.nes func\u00adtional equivalence. \nThis dependence relation may not be statically computable, but (conservative) abstractions may be (.nitely) \nrep\u00adresented through systems of af.ne inequalities, or unions of para\u00admetric polyhedra [2, 16, 22]. Two \ndecades of work in polyhedral compilation has demonstrated that the main loop nest transforma\u00adtions can \nbe modeled and effectively constructed as multidimen\u00adsional af.ne schedules, mapping dynamic instances \nof individual statements to lexicographically sorted vectors of integers (or ratio\u00adnal numbers) [6, 8, \n17, 20, 24, 38]. Optimization in the polyhedral model amounts to selecting a good multidimensional af.ne \nsched\u00adule for the program. In this paper, we make the following contributions: We take this well de.ned \noptimization problem, and provide a complete, convex formalization for it. This convex set of semantics-preserving, \ndistinct, af.ne multidimensional sched\u00adules opens the door to more powerful linear optimization algo\u00adrithms. \n We propose a decomposition of the general optimization prob\u00adlem over this convex polyhedron into sub-problems \nof much lower complexity, introducing the powerful fusibility concept. The fusibility relation generalizes \nthe legality conditions for loop fusion, abstracting a large set of multidimensional, af.ne, fusion-enabling \ntransformations.  We demonstrate that exploring fusibility opportunities in a loop nest reduces to the \nenumeration of total preorders, and to the existence of pairwise compatible loop permutations. We also \nstudy the transitivity of the fusibility relation.  Based on these results, we design a multidimensional \naf.ne scheduling algorithm targeted at achieving portable high per\u00adformance across modern processor architectures. \n Our approach has been fully implemented in a source-to-source compiler and validated on representative \nbenchmarks.  2.1 Loop optimization challenge Let us illustrate the challenge of loop optimization from \nthe standpoint of performance portability through a simple example, ThreeMatMat, shown in Figure 1. We \nseek the best combination of loop transformations expressed as a multidimensional af.ne schedule to optimize \na sequence of three matrix-products for a given target platform. Figure 1. ThreeMatMat:C = AB, F = DE, \nG= CF We setN = 512and computed5differentversionsof ThreeMat-Mat using combinations of loop transformations \nincluding tiling. We experimented on two platforms (quad-Xeon E7450 and quad-Opteron 8380), using the \nIntel ICC 11.0 compiler with aggressive optimization .ags. The framework developed in this paper enables \nus to outperform ICC by a factor 2.28\u00d7 and 1.84\u00d7, respectively, for the Intel and AMD machines.We observe \nthat the bestversion found depends on the target machine: for the Intel system, the best found loop fusion \nstructure is shown in Figure 2(b), on which fur\u00adther polyhedral tiling and parallelization were applied \n(not shown in Figure 2).But on the AMD machine, distributing all statements and individually tiling them \nperforms best,1.23\u00d7 better than 2(b). The ef.cient eecution of a computation kernel on a modern multi-core \narchitecture requires the synergistic operation of many hardware resources, via the exploitation of thread-level \nparallelism; the memory hierarchy, including prefetch units, different cache lev\u00adels, memory buses and \ninterconnect; and all available computa\u00adtional units, including SIMD units. Because of the very complex \ninterplay between all these components, it is currently infeasible to guarantee at compile-time which \nset of transformatios leads to maximal performance. To maximize performance, one must carefully tune \nfor the trade-off between the different levels of parallelism and the us\u00adage of the local memory components. \nThis can be performed via loop fusion and distribution choices, that drive the success of subse\u00adquent \noptimizations such as vectorization, tiling or parallelization. It is essential to adapt the fusion structure \nto the program and target machine. However, the state-of-the-art provides only rough models of the impact \nof loop transformations on the actual execution. To achieve performance portability across a wide variety \nof architec\u00adtures, empirical evaluation of several fusion choices is an alterna\u00adtive, but requires the \nconstruction and traversal of a search space of all possible fusion/distribution structures. To make \nthe problem sound and tractable, we develop a complete framework to reason and optimize in the space \nof all semantics-preserving combinations of loop structures. 2.2 Background and notation The polyhedral \nmodel is a .exible and expressive representation for loop nests. It captures the linear algebraic structure \nof statically predictable control .ow, where loop bounds and conditionals are af.ne functions of the \nsurrounding loop iterators and invariants (a.k.a. parameters, unknown at compilation time) [15, 20]; \nbut it is not restricted to static control .ow [2, 4]. The set of all executed instances of each statement \nis called an iteration domain. These sets are represented by af.ne inequalities involving the loop iterators \nand the global variables. Considering the ThreeMatMat example in Figure 1, the iteration domain of R \nis: D R = {(i, j, k) . Z3 | 0= i< N. 0= j< N. 0= k < N} D R is a (parametric) integer polyhedron, that \nis a subset of Z3. The iteration vector xxR is the vector of the surrounding loop iterators, for R it \nis (i, j, k) and takes value in D R. In this work, a given loop nest optimization is de.ned by a multidimensional \naf.ne schedule, which is an af.ne form of the loop iterators and global parameters [18, 20]. DEFINITION \n1 (Af.ne multi-dimensional schedule). Given a state\u00adment S, an af.ne schedule TS of dimension m is an \naf.ne form on the d outer loop iterators xxS and the p global parameters xn. TS . Zm\u00d7(d+p+1) can be written \nas: . . .. .1,1 ... .1,d+ p+1 xxS . .. . TS(xxS)= . .. . . . xn . .. .m,1 ... .m,d+p+11 The scheduling \nfunction TS maps each point in D S with a mul\u00adtidimensional timestamp of dimension m, such that in the \ntrans\u00adformed code the instances of S de.ned in D S will be executed fol\u00adlowing the lexicographic ordering \nof their associated timestamp. Multidimensional timestamps can be seen as logical clocks: the .rst dimension \ncorresponds to days (most signi.cant), next one is hours (less signi.cant), the third to minutes, and \nso on. Note that every static control program has a multidimensional af.ne sched\u00adule [18], and that any \ncomposition of loop transformation can be represented in the polyhedral representation [20, 50]. The \nfull pro\u00adgram optimization is a collection of af.ne schedules, one for each syntactical program statement. \nEven seemingly non-linear trans\u00adformations like loop tiling (a.k.a. blocking) and unrolling can be modeled \n[20, 38]. Finally, syntactic code is generated back from the polyhe\u00addral representation on which the \noptimization has been applied; CLOOG is the state-of-the art algorithm and code generator [3] to perform \nthis task.  2.3 Semantics-preserving transformations A program transformation must preserve the semantics \nof the pro\u00adgram. The legality of af.ne loop nest transformations is de.ned at the level of dynamic statement \ninstances. As a starting point for transformation space pruning, webuilda convex, polyhedral char\u00adacterization \nof all semantics-preserving multidimensional af.ne schedules for a loop nest. Two statements instances \nare in dependence relation if they access the same memory cell and at least one of these accesses is \na write. Given two statements R and S,a dependence polyhedron D R,S is a subset of the Cartesian product \nof D R and D S: D R,S contains all pairs of instances (xxR,xxS) such that xxS depends on xxR, for a given \narray reference. Hence, for a transformation to preserve the program semantics, it must ensure that TR(xxR) \n. TS(xxS), where . denotes the lexicographic ordering.1 To capture all program dependences we build a \nset of depen\u00addence polyhedra, one for each pair of array references accessing the same array cell (scalars \nbeing a particular case of array), thus possibly building several dependence polyhedra per pair of state\u00adments. \nThe polyhedral dependence graph is a multi-graph with one node per statement, and an edge eR.S is labeled \nwith a dependence polyhedron D R,S, for all dependence polyhedra. The schedule constraints imposed by \nthe precedence constraint are expressible as .nding all non-negative functions over the de\u00adpendence polyhedra \n[17]. It is possible to express the set of af.ne, non-negative functions over D R,S thanks to the af.ne \nform of the Farkas lemma [39]. LEMMA 1 (Af.ne formofFarkas Lemma). Let D be a nonempty polyhedron de.ned \nby the inequalities Axx + xb = x0. Then any af.ne function f(xx) is non-negative everywhere in D iff \nit isa positive af.ne combination: f(xx)= .0 +x+x.T =x0. x.T(Axb), with .0 = 0andx .0 andx.T are calledFarkas \nmultipliers. Sinceitisa necessaryandsuf.cient characterization,theFarkas Lemma offers a loss-less linearization \nof the constraints from the dependence polyhedron into direct constraints on the schedule co\u00adef.cients. \nFor instance considering the .rst time dimension (the .rst row of the scheduling matrices, that is, we \nset p = 1 in the following equation), to preserve the precedence relation we have: D R,S .D R,S, .(xxR,xxS). \nD R,S, TS(xxS) - TR(xxR) = dp (1) pp D R,S dp .{0, 1} D R,S We resort to variable d1 to model the dependence \nsatisfaction. D R,S A dependence D R,S can be either weakly satis.ed when d= 0, 1 permitting TS(xxS)= \nTR(xxR) for some instances in dependence, or 11 1(a1,..., an) . (b1,..., bm) iff thereexists an integer1 \n= i= min(n, m) s.t. (a1,..., ai-1)=(b1,..., bi-1) and ai < bi. D R,S strongly satis.ed when d1 = 1, \nenforcing strict precedence (at the .rst time dimension) for all instances in dependence. This model \nextends to multidimensional schedules, observing that once a dependence has been strongly satis.ed, it \ndoes not con\u00adtribute to the semantics preservation constraints for the subsequent time dimensions. Furthermore, \nfor a schedule to preserve seman\u00adtics, it is suf.cient for every dependence to be strongly satis.ed at \nleast once.Following these observations, one may statea suf.cient condition for semantics preservation \n(adapted from Feautrier s for\u00admalization [18]). LEMMA 2 (Semantics-preserving af.ne schedules). Given \na set of af.ne schedules TR , TS ... of dimension m, the program semantics is preserved if: () D R,S \nD R,S .D R,S, .p.{1,..., m}, dp = 1 .. j< p, dj = 0. () D R,S . j= p, .(xxR,xxS). D R,S, TSj(xxS) - TRj(xxR) \n= dj The proof directly derives from the lexicopositivity of depen\u00addence satisfaction [18]. Regarding \nthe schedule dimensionalitym, it is suf.cient to pick m = d to guarantee the existence of a legal schedule \n(the maximum program loop depth is d). Obviously, any schedule implementing the original execution order \nis a valid solution [18]. This formalization involves an oracle to select the dimension p at which each \ndependence shouldbe strongly satis.ed.Toavoid the combinatorial selection of this dimension, we conditionally \nnullify constraint (1) on the schedules when the dependence was strongly satis.edataprevious dimension.To \nnullify the constraint, we may always pick a lower bound lb for TS(xxS) - TR(xxR).With\u00ad kk out anyloss \nof generality, we assume (parametric) loop bounds are non-negative. Vasilache [44] proposed a method \nto evaluate this lower bound when the coef.cients of T are bounded: lb is greater or equal to the sum \nof the maximal coef.cient values times the (possibly parametric) loop bounds. We can always select a \nlarge enough K such that [32] TS(xxS) - TR(xxR) =-K.xn- K (2) kk Note that this lower bound can also \nbe linearized into constraints on TR , TS thanks to the Farkas Lemma. To obtain the schedule constraints \nwe reinsert this lower bound in the previous formula\u00adtion [44], such that either the dependence has not \nbeen previously strongly satis.ed and then the lower bound is (1), or it has been and the lower bound \nis (2). We thus derive the convex form of semantics-preserving af.ne schedules of dimension m for a pro\u00adgram \nas a corollary of Lemma 2. LEMMA 3 (Convex form of semantics-preserving af.ne schedules). Given a set \nof af.ne schedules TR , TS ... of dimension m, the pro\u00adgram semantics is preserved if the three following \nconditions hold: D R,SD R,S D R,S (i) .D R,S, .p, dp . {0, 1} m (ii) .D R,S, . dD R,S p = 1 (3) p=1 \n(iii) .D R,S, .p. {1, . . . , m}, .(xxR,xxS) . D R,S, (4) p-1 TSp(xxS) - TRp(xxR) =- .dk .(K.xn+ K)+ \ndp k=1 One can thenbuild the set Lm of all (bounded) legal af.ne mul\u00adtidimensional schedules of dimension \nm by encoding the af.ne constraints given by Lemma 3 into a problem with m binary vari\u00adables per dependence \nand m \u00d7 (dim(xxS)+ dim(xn)+ 1) variables per statement S.For an ef.cient construction of the constraints \nfor semantics-preservation, one should proceed dependence by depen\u00addence. Building the constraints of \nthe form of (4) is done for each dependence, then theFarkas multipliers are eliminated for instance witha \nscalableFourier-Motzkin projection technique [33]. The set of constraints obtained for each schedules \nare then intersected, and the process is replicated for all dimensions.  2.4 Finding a schedule for \nthe program We achieved completeness: an Integer Linear Program operating on Lm can exhibit the best \naf.ne schedule according to a given objective function. Yet solving an arbitrary ILP on L m is NP\u00adcomplete \n[39]. Given the high dimensionality of this space on the larger loop nests, our convex formalization \ninduces a serious tractability challenge. To address this challenge and reduce the complexity effect \nof the problem, we propose to decouple the optimization into two sub-problems: 1. the problem of selecting \nhow statements are interleaved this is traditionally viewed as selecting a proper combination of loop \nfusion, loop distribution and code motion; 2. the problem of selecting an af.ne schedule compatible \nwith the statement interleaving selected at the previous step.  Selecting a statement interleaving that \nmaximizes a fusion cri\u00adterion is still an NP-complete problem in general [12],but its com\u00adplexity is \ndramatically reduced from the high-dimensionality linear optimization problem.We focus our efforts on \nthis simpli.ed prob\u00adlem.  2.5 Encoding statement interleaving Fusion and fusibility of statements In \nthe polyhedral model, loop fusion is characterized by the .ne-grain interleaving of statement instances \n[6, 24]. Two statements are fully distributed if the range of the timestamps associated to their instances \nnever overlap. Syn\u00adtactically, this results in distinct loops to traverse the domains. One may de.ne \nfusion as the negation of the distribution criterion. For such a case we say that two statements R, S \nare fused if there exists at least one pair of iterations for which Ris scheduled before S, and another \npair of iterations for which S is scheduled before R. This is stated in De.nition 2. DEFINITION 2 (Fusion \nof two statements). Consider two state\u00adments R, S and their schedules TR and TS.R andS are said to be \nfused at level p if, .k .{1... p}, there exists at least three dis\u00ad ' tinct executed instances x R, x \nR and x S such that TR(x R) = TS(x S) = TR(x R ' ). (5) kkk We now propose an encoding of De.nition 2 \nsuch that we can determine, from the dependence graph, if it is possible to .nd a schedule leading to \nfuse the statements whatever additional transformation(s) may be required to enable fusion. This is called \nfusibility. The purpose is to exhibit suf.cient af.ne constraints that can be added to L tokeep in the \nsolution space only the schedules which correspond to fusing the considered statements. We rely on the \nfact that the .rst and last scheduled instances of R and S will conform to De.nition 2 if the two statements \nare fused. These speci.c instances belong to the set of vertices of D R and D S. Thus, to determine whether \nR and S are fusable or not, it is suf.cient to enumerate all vertices of D R and D S, and for each possible \ncombinations of vertices plug their actual value into Eq (5) leading to an af.ne constraint on the schedule \ncoef.cients and test for the existence of a solution in the space of all semantics\u00adpreserving schedules \nL augmented with the new constraint. As soon as there is a solution to one of the problems formed, then \nthe statements are fusable. This is stated in De.nition 3. DEFINITION 3 (Generalized fusibility check). \nGiven vR (resp. vS) the set of vertices of D R (resp. D S). R and S are fusable at level p if, .k .{1... \np}, there exist two semantics-preserving schedules TRk and TkS such that .(x 1,x 2,x 3) . vR \u00d7 vS \u00d7 vR, \nTR(x 1) = TS(x 2) = TR(x 3) kkk Note that in practice, the number of vertices of an iteration do\u00admain \nis often small (from2to 10), hence the number of cases to be checked is limited. We also present in Section \n4.2 a specialization of this test to the case of non-negative schedule coef.cients, which removes the \nneed to enumerate the vertices. Multidimensional statement interleaving Thanks to Lemma 3 and De.nition \n3, we can determine if a set of statements can be fusedatagivenlevel while preserving semantics.For our \noptimiza\u00adtion problem, we are interestedinbuildinga space representing all ways to interleave the program \nstatements. To reason about statement interleaving, one may associate a vector .S of dimension d (d being \nthe maximal loop depth in the program) to each statement S, and order those vectors lexicograph\u00adically. \nIf some statement S is surrounded by less than d loops, .S is post-padded with zeroes. Figure 2 gives \nan example of three pos\u00adsible optimizations for the ThreeMatMat example, as de.ned by different con.gurations \nof the . vectors. Note that to implement the interleaving speci.ed by . several loop permutations may \nbe required. There exist an analogy between .S and a standard encoding of multidimensional af.ne schedules \nwhere statement interleaving vectors are made explicit: for each statement S, one may constrain the rows \nof TS to alternate between constant forms of a vector of dimension d+ 1 (usually denoted \u00df [20]) and \naf.ne forms of the iteration and global parameter vectors. This 2d + 1-dimensional encoding does not \nincur anyloss of expressiveness [11, 18, 20, 24]. However in this paper we explicitly give . important \nstructural properties of the transformed loop nest: 1. if .R = .S , .k .{1,..., p} then the statements \nshare (at least) kk pcommon loops;  2. if .R ..S , then the statements do not share any common loop \n = pp starting at depth p.We thus have .R =..S . .R =..Sk, .k .{p+ 1,..., m} pp k Our objective to \nmodel all possible interleavings is to build a space containing all . vectors for which there exists \na semantics\u00adpreserving af.ne transformation that implements the speci.ed in\u00adterleaving. To guarantee \nthat the solution space contains only use\u00adful points, we need to address the problem that several choices \nof . vectors represent the same multidimensional statement interleav\u00ading. Intuitively, the problem arises \nbecause there exists an in.nite number of vectors which have the same lexicographic ordering. In particular, \nthe order is invariant to translation of all coef.cients of . at a given dimension, or by multiplication \nof all its coef.cients by a non-negative constant. To abstract away these equivalences, we now formally \nde.ne the concept of multidimensional statement interleaving. DEFINITION 4 (Multidimensional statement \ninterleaving). Consider a set of statements S enclosed within at most d loops and their as\u00adsociated vectors \nR = {.S}S.S . For a given k .{1,..., d}, the one-dimensional statement interleaving of S at dimension \nk de\u00ad.ned by R is the partition of S according to the coef.cients .Sk. The multidimensional statement \ninterleaving of S at dimension k de.ned by R is the list of partitions at dimensionk.  (a) (b) (c) Figure \n2. Three possible legal transformations forC = AB, F = DE, G= CF The structural properties of statement \ninterleaving indicate that equivalence classes at dimension k correspond to statements that share common \nloops at depth k in the transformed loop nest. DEFINITION 5 (Total preorder). A total preorder on a set \nS is a relation . which is re.exive, transitive, and such that for any pair of elements (S1, S2) . S \n, eitherS1. S2 orS2 . S1 or both. An important result is that anypreorder of a set S is isomorphic to \na partial order of some equivalence classes of S . Applying this result to the structural properties \nof statement interleavings yields the following lemma. LEMMA 4 (Structure of statement interleavings). \nEachone-dimen\u00adsional statement interleaving corresponds to a unique total pre\u00adorder of the statements \nand reciprocally.  3. Semantics-Preserving Statement Interleavings We now propose a convex, complete \ncharacterization of multidi\u00admensional statement interleavings which serves as a basis to encode the space \nof valid . vectors. 3.1 Convex encoding of total preorders We .rst present a solution for the problem \nof total preorders of scalar elements, that corresponds to one-dimensional interleavings. One-dimensional \ncase For a given set of n elements, we de.ne O as the set of all and distinct total preorders of its \nn elements. The keyproblem is to modelO as a convex polyhedron.2 To the best of our knowledge, uniqueness \nof orderings cannot be modeled in a convex fashion on a set with a linear number of variables. We propose \nto model the ordering of two elements i, j with three binary decision variables, de.ned as follows. pi, \nj = 1iff i precedes j, ei, j = 1 iff i equals jand si, j = 1 iff i succeeds j. To model the entire set, \nwe introduce three binary variables for each ordered pair of elements, i.e., all pairs (i, j) such that1 \n= i< j= n. This modelsa set with3 \u00d7 n(n- 1)/2 variables. .. . 0= pi, j = 1 . 0= ei, j = 1 .. 0= si, j \n= 1 For instance, the outer-most interleaving .R = 0, .S = 0, .1 T = 11 1 of Figure 2(b) is represented \nby: eR,S = 1, eR,T = 0, eS,T = 0 pR,S = 0, pR,T = 1, pS,T = 1 sR,S = 0, sR,T = 0, sS,T = 0 From there, \none may easily recompute the corresponding total pre\u00adorder {.R 0, .S = 0, .T = 1}, e.g., by computing \nthe lexico\u00ad 1 = 11 graphic minimum of a system W of 3 non-negative variables that replicate the ordering \nde.ned by all pi, j, ei, j and si, j. The .rst issue is the consistency of the model: an inconsis\u00adtent \npreorder would make W infeasible, e.g., setting e1,2 = 1 and p1,2 = 1. The second issue is the totality \nof the relation. These two conditions can be merged into the the following equality, capturing both mutual \nexclusion and totality: pi, j+ ei, j+ si, j = 1 (6) To simplify the system, we immediately get rid of \nthe si, j vari\u00adables, thanks to (6).We also relax (6) to get: pi, j+ ei, j = 1 Mutually exclusive decision \nvariables capture the consistency of the model for a single pair of elements. However, one needs to insert \nadditional constraints to capture transitivity. To enforce transitivity, the following rule must hold \ntrue for all triples of statements (i, j, k): ei, j = 1. ei,k = 1. ej,k = 1. Similarly, we have: ei, \nj = 1. ej,k = 1. ei,k = 1. These two rules set the basic transitivity of e variables. Since we are dealing \nwith binary variables, the implications can be easily modeled as af.ne constraints: .k .] j, n], ei, \nj+ ei,k = 1+ ej,k ei, j+ ej,k = 1+ ei,k Slightly more complex transitivity patterns are also required. \n2This problem is not a straight adaptation of standard ordertheory: we look For instance, we also have \ntransitivity conditions imposed by a for the set of all distinct total preorders of n elements, in contrastto \nclassical connection between the value for some e coef.cients and some p work de.ning counting functionsof \nthisset [41]. ones.For instance, i< jand j= k implies i< k. The general rules for those cases are: ei, \nj. pi,k . pj,k ei, j. pj,k . pi,k ek, j. pi,k . pi, j These three rules set the complex transitivity \nbetween the p and e variables. They can be modeled equivalently by following af.ne constraints: . . . \n.k .] j, n] ei, j+ pi,k = 1+ pj,k . ei, j+ pj,k = 1+ pi,k (7) . . .k .]i, j[ ek, j+ pi,k = 1+ pi, j Generalizing \nthis reasoning, we collect all constraints to enforce the transitivity of the total preorder relation. \nThose constraints are gathered in the following system, for 1 = i < j< n, de.ning the convex set of all, \ndistinct total preorders of n elements O : . 0= pi, j = 1 Variables are . . . 0= ei, j = 1 . binary . \n. . . Relaxed mutual . . . pi, j+ ei, j = 1 . exclusion . . . . ..k .] j, n] ei, j+ ei,k = 1+ ej,k Basic \ntransitivity . . . . ei, j+ ej,k = 1+ ei,k on e . . . Basic transitivity .k.]i, j[ . pi,k + pk, j = 1+ \npi, j on p . . . . . ..k .] j, n] ei, j+ pi,k = 1+ pj,k. Complex . . . . ei, j+ pj,k = 1+ pi,k transitivity \n. . . . . .k.]i, j[ ek, j+ pi,k = 1+ pi, j on p and e . .. . . . . Complex . . ..k .] j, n] transitivity \n. ei, j+ pi, j+ pj,k = 1+ pi,k + ei,k . . on s and p The following lemma is proved in [32, appendix A]. \nLEMMA 5 (Completeness and correctness ofO ). The set O con\u00adtains one and only one element per distinct \ntotal preorder of n ele\u00adments. Multidimensional case O encodes all possible total preorders (or, statement \ninterleaving)foragivenlooplevel.Toextendtothe mul\u00adtidimensional interleaving case, we .rst replicate \nO for each di\u00admension of the . vectors. However, further constraints are needed to also achieve consistency \nand uniqueness of the characterization across dimensions. Intuitively, if a statement is distributed \nat di\u00admension k, then for all remaining dimensions it will also be dis\u00adtributed. This is modeled with \nthe following equations: ()() kl kl .l > k, p = 1. p = 1. s = 1. s = 1 i, ji, ji, ji, j The .nal expression \nof the set I of all, distinct d-dimensional statement interleavings is: . Total preorders . ..k .{1,..., \nd}, constraints on O k . . at level k . . kk+1 . p = p Statement interleaving . i, ji, j . . k+1 k+1 \nkk uniqueness e + p = p + e i, ji, ji, ji, j It is worth noting that each O k contains numerous variables \nand constraints; but when it is possible to determine thanks to the dependence graph for instance that \na given ordering of two el\u00adements i and j is impossible, some variables and constraints are eliminated. \nOurexperiments indicate that these simpli.cations are quite effective, improving the scalability of the \napproach signi.\u00adcantly.  3.2 Pruning for semantics preservation The set I contains all and only distinct \nmulti-level statement inter\u00adleavings. A pruning step is needed to remove all interleavings that does \nnot preserve the semantics. The algorithm proceeds level-by\u00adlevel, from the outermost to the innermost. \nSuch a decoupling is possible because we have encoded multi-dimensional interleaving constraints in I \n, and that fusion at level k implies fusion at all pre\u00adceding levels. In addition, leveraging Lemma3and \nDe.nition3 we can determine the fusibility of a set of statements at a given level by exposing suf.cient \nconditions for fusion on the schedules. The general principle is to detect all the smallest possible \nsets of punfusable statements at level k, and for each of them, to update I k by adding an af.ne constraint \nof the form: kk k e + e + ... + e < p- 1 (8) S1,S2 S2,S3 Sp-1,Sp thus preventing them (and anysuper-set \nof them) to be fused all to\u00adgether.We note F the .nal set with all pruning constraints for legal\u00adity, \nF . I .A naive approach could be to enumerate all unordered subsets of the n statements of the program \nat level k, and check for fusibility, while avoiding to enumerate a super-set of an unfus\u00adable set. Instead, \nwe leverage the polyhedral dependence graph to reduce the test to a much smaller set of structures. The \n.rst step of our algorithm is to build a graph G to facili\u00adtate the enumeration of sets of statements \nto test for, with one node per statement. Sets of statements to test for fusibility will be repre\u00adsented \nas nodes connected by a path in that graph. Intuitively, if G is a complete graph then we can enumerate \nall unordered subsets of the n statements: enumeratingallpathsoflength1givesallpairs of statements by \nretrieving the nodes connected by a given path, all paths of length 2 gives all triplets, etc. We aim \nat building a graph with less edges, so that we lower the number of sets of statements to test for fusibility.We \n.rst check the fusibilityof all possible pairs of statements, and add an edge between two nodes only \nif (1) there is a dependence between them, and (2) either theymust be fused to\u00adgether or they can be \nfused and distributed at that level. When two statements must be fused, they are merged to ensure all \nschedule constraints are considered when checking for fusibility. The second step is to enumerate all \npaths of length = 2 in the graph. Given a path p, the nodes in p represent a set of statements that has \nto be tested for fusibility. Each time they are detected to be not fusable, all paths with p as a sub-path \nare discarded from enumeration, and F k is updated with an equation in the form of (8). The complete \nalgorithm is shown in Figure 3. Procedure buildLegalSchedules computes the space of legal schedules L \naccording to Lemma 3, for the set of statements given in argument. Procedure mustDistribute tests for \nthe emptiness of L when augmented with fusion conditions from De.nition 3 up to level d. If there is \nno solution in the augmented set of constraints, then the statements cannot be fused at that level and \nhence must be distributed. Procedure mustFuse checks if it is legal to distribute the statements R and \nS. The check is performed by inserting a splitter at level d. This splitter is a constant one\u00addimensional \nschedule at level d, to force the full distribution of statement instances at that level. If there is \nno solution in this set of constraints, then the statements cannot be distributed at that level and hence \nmust be fused. Procedure canDistributeAndSwap tests if it is legal to distribute R and S at level d and \nto execute S before R. The latter is required to compute the legal values of the sR,S variablesat thatlevel. \nThe checkis performedina similarfashion as with mustFuse, except the splitter is made to make R execute \nafter S. Finally procedure mergeNodes modi.es the graph edges after the merging of two nodes R and S \nsuch that: (1) if for T there was an edge eT.R and not eT.S or vice-versa, eT.R is deleted, and eS,T \n= 0, this is to remove triplets of trivially unfusable sets;  Figure 3. Pruning algorithm (2) if there \nare 2 edges between T and RS, one of them is deleted and its label is added to the remaining one existing \nlabel; (3) the label of the edge eR.S is added to all remaining edges to/from RS, and eR.S is deleted. \nApplications The .rst motivation of building a separate search space of multidimensional statement interleavings \nis to decouple the selection of the interleaving from the selection of the trans\u00adformation that enables \nthis interleaving. One can then focus on building search heuristics for the fusion/distribution of statements \nonly, and through the framework presented in this paper compute a schedule that respects this interleaving. \nAdditional schedule prop\u00aderties such as parallelism and tilability can then be exploited with\u00adout disturbing \nthe outer level fusion scheme.We present in the fol\u00adlowing a complete technique to optimize programs \nbased on iter\u00adative interleaving selection, leading to parallelized and tiled trans\u00adformed programs. \nThis technique is able to automatically adapt to the target framework, and successfully discovers the \nbest perform\u00ading fusion structure, whatever the speci.cs of the program, com\u00adpiler and architecture. \nAnother motivation ofbuilding I is to enable the design of ob\u00adjective functions on fusion with the widest \ndegree of applicability. For instance one can maximize fusion at outer level, by maximiz\u00ad 1 ing .i, je \nor similarly distribution by minimizing the same sum. i, j One can also assign a weight to the coef.cients \nei, j tofavor fusion for statements carrying more reuse, for instance. This formulation allows to devise \nfurther pruning algorithms, offering to the opti\u00admization the most relevant choice of legal interleavings \nfor a given target.  4. Optimizing for Locality and Parallelism The previous sections de.ne a general \nframework for multi-level statement interleaving.We address now the problem of specializing this framework \nto provide a complete optimization algorithm that integrates tiling and parallelization, along with the \npossibility to iteratively select different interleavings. The optimization algorithm proceeds recursively, \nfrom the out\u00adermost level to the innermost. At each level of the recursion, we select the associated \nschedule dimension by instantiating its values. Then, webuild the set of semantics-preserving interleavings \nat that level, pick one and proceed to the next level until a full schedule is instantiated. We present \nin Section 4.1 additional conditions on the schedules to improve the performance of the generated trans\u00adformation, \nby integrating parallelism and tilability as criteria. Then we de.ne in Section 4.2 a new fusibility \ncriterion for a better per\u00adformance impact, while restricting it to non-negative schedule co\u00adef.cients. \nIn Section 4.3, we show how to construct the set of legal interleaving without resorting on testing the \nset of legal schedules for sets larger than a pair of statements. Finally we present the com\u00adplete optimization \nalgorithm in Section 4.4. 4.1 Additional constraints on the schedules Tiling (or blocking) is a crucial \nloop transformation for parallelism and locality. Bondhugula et al. developed a technique to compute \nan af.ne multidimensional schedule such that parallel loops are brought to the outer levels, and loops \nwith dependences are pushed inside [6, 8]; at the same time, the number of dimensions that can be tiled \nare maximized. We extend and recast their technique into our framework. Legality of tiling Tiling along \na set of dimensions is legal if it is legal to proceed in .xed block sizes along those dimensions: this \nrequires dependences not to be backward along those dimensions, thus avoiding a dependence path going \nout of and coming back into a tile; this makes it legal to execute the tile atomically. Irigoin and Triolet \nshowed that a suf.cient condition for a schedule T to be tilable [23], given R the dependence cone for \nthe program, is that T.R=x0 In other words, this is equivalent to saying that all dependences must be \nweakly satis.ed for each dimension Tk of the schedule. Sucha property for the scheduleis also known asForwardCommu\u00adnicationOnly \nproperty[21].Returningto Lemma3,itis possibleto add an extra condition such that the p.rst dimensions \nof the sched\u00adules are permutable, a suf.cient condition for the p.rst dimensions to be tilable. This \ntranslates into the following additional constraint on schedules, to enforce permutability of schedule \ndimensions. DEFINITION 6 (Permutability condition). Given two statements R, S. Given the conditions for \nsemantics-preservation as stated by Lemma 3. Their schedule dimensions are permutable up to dimen\u00adsionk \nif in addition: .D R,S, .p.{1,..., k}, .(x R,x S). D R,S, D R,S TSp(x S) - TRp(x R) = dp To translate \nk into actual number of permutable loops, the k associated schedule dimensions must express non-constant \nsched\u00adules. Rectangular tiling Selecting schedules such that each dimension is independent with respect \nto all others enables a more ef.cient tiling.Rectangular or close to rectangular blocks are achieved \nwhen possible, avoiding complex loop bounds which may arise in the case of arbitrarily shaped tiles. \nWe resort to augmenting the con\u00adstraints, level-by-level, with independence constraints. Hence, to compute \nschedule dimension k, we need instantiate .rst a sched\u00adule for all previous dimensions1to k- 1. This \ncomes from thefact that orthogonality constraints are not linear or convex and cannot be modeled as af.ne \nconstraints directly. In its complete form, adding orthogonality constraints leads to a non-convex space, \nand ideally, all cases have to be tried and the best among thosekept. When the number of statements is \nlarge, this leads to a combinatorial explo\u00adsion. In such cases, we restrict ourselves to the sub-space \nof the orthogonal space where all the constraints are non-negative (that is, we restrict to have .i, \nj. N).Byjust consideringa particular convex portion of the orthogonal sub-space, we discard solutions \nthat usu\u00adally involve loop reversals or combination of reversals with other transformations; however, \nwe believe this does not make a strong difference in practice. For the rest of this paper, we now assume \n.i, j . N. Inner permutable loops Ifitis not possibletoexpress permutable loops for the .rst level, \nBondhugula proposed to split the state\u00adments into distinct blocks to increase the possibility to .nd \nouter permutable loops [8]. Since our technique already supports explic\u00aditly the selection of any semantics-preserving \npossibility to split statements into blocks via the statement interleaving, we propose instead to enable \nthe construction of inner permutable loops, by choosing to maximize the number of dependences solved \nat the .rst levels until we (possibly) .nd permutable dimensions at the current level. Doing so increases \nthe freedom for the schedule at inner di\u00admensions when it is not possible to express permutable loops \nat the outer levels. Maximizing the number of dependences solved at a given level was introduced by Feautrier \n[18] and we use a similar form: D R,S Si = max .d (9) k D R,S This cost function replaces the permutability \ncondition, when it is not possible to .nd a permutable schedule for a given dimension k. Dependence distance \nminimization There are in.nitely many schedules that may satisfy the permutability criterion from De.\u00adnition \n6 as well as (9). An approach that has proved to be simple, practical, and powerful has been to .nd those \nschedules that have the shortest dependence components along them [6]. For polyhe\u00addral code, the distance \nbetween dependent iterations can always be bounded by an af.ne function of the global parameters, represented \nas a p-dimensional vectorxn. u.xn+ w = TS(x S) - TR(x R) (x R,x S). D R,S (10) u . Np, w . N In order \nto maximize data locality, one may include read-after\u00adread dependences in the set of dependences D R,S \nconsidered in the bounding function (10). We showed in (2) that there always exists a parametric form \n-K.xn- K for the lower bound of the schedule latency.Reciprocally, u and w can always be selected large \nenough for u.xn + w to be an upper-bound on the schedule latency. So considering read-after-read dependences \nin the bounding function does not prevent from .nding a legal schedule. The permutability and bounding \nfunction constraints are recast through the af.ne form of the Farkas Lemma such that the only unknowns \nleft are the coef.cients of T and those of the bounding function, namely u, w.Coordinates of the bounding \nfunction are then used as the minimization objective to obtain the unknown coef.cients of T. () minimize. \nu, w,..., .i,1,... (11) The resulting transformation is a complex composition of multi\u00addimensional loop \nfusion, distribution, interchange, skewing, shift\u00ading and peeling. Eventually multidimensional tiling \nis applied on all permutable bands, and resulting tiles can be executed in paral\u00adlel or at worse with \na pipeline-parallel schedule [8]. Tile sizes are computed such that data accessed by each tile roughly \n.ts in the L1 cache.  4.2 Fusability check We presented in Section 2.5 a generalized check for fusibility \nwhich guarantees at least one instance of the iteration domains are fused.Butfor fusiontohavea stronger \nperformance impact,abet\u00adter criterion is preferred to guarantee that at most x instances are not .nely \ninterleaved. In general, computing the exact set of inter\u00adleaved instances requires complex techniques. \nA typical example is schedules generating a non-unit stride in the supporting lattice of the transformed \niteration domain.For such cases, computing the exact number of non-fused instances could be achieved \nusing the Ehrhart quasi-polynomial of the intersection of the image of itera\u00adtion domains by the schedules \n[10]. However, this re.ned precision is not required to determine if a schedule represents a potentially \ninteresting fusion. We allow for a lack of precision to present an easily computable test for fusibility \nbased on an estimate of the number of instances that are not .nely interleaved. For the sake of simplicity, \nwe assume that loop bounds have been normalized such that x0 is the .rst iteration of the domain. When \nconsidering non-negative coef.cients, the lowest timestamp assigned by a schedule TR is simply TR(x0). \nOne can recompute kk the corresponding timestamp by looking at the values of the co\u00adef.cients attached \nto the parameters and the constant. Hence, the timestamp interval c between the two .rst scheduled instances \nby TR and TS is simply the difference of the (parametric) constant kk parts of the schedules. In addition, \nto avoid the case where TkR and/or TS are (parametric) constant schedules, we force the linear k part \nof the schedule to be non-null. This is formalized in De.ni\u00adtion 7. DEFINITION 7 (Fusability restricted \nto non-negative coef.cients). Given two statements R, S such that R is surrounded by dR loops, and S \nby dS loops. They are fusable at level p if, .k .{1... p}, there exist two semantics-preserving schedules \nTR and TS such kk that: (i) -c < TR(x0) - TS(x0) < c kkdR dS (ii) ..kR ,i > 0, ..Sk,i > 0 i=1 i=1 The \nconstant c is an indicator on the timestamp difference between the .rst scheduled instance of R and the \n.rst scheduled instance of S. By tuning the allowed size for this interval, one can range from full, \naligned fusion(c = 0) to full distribution (with c greater than the schedule latencyof R or S). Note \nthat De.nition7is inde\u00adpendent from any composition of af.ne transformations that may be needed to enable \nthe fusion of the statements. The schedules that implement the fusion (hence modeling also the fusion-enabling \nsequence of transformations) are simply solutions in the space of semantics-preserving non-negative schedules \naugmented with the fusibility constraints. 4.3 Computation of the set of interleavings We have now restricted \n.i, j . N. A consequence is the design of a specialized version of our generic algorithm to compute the \nset of semantics-preserving interleavings. This specialized version leverages new results on the transitivity \nof fusibility for the case of non-negative schedule coef.cients, as shown below. Furthermore, it does \nnot require computing with L , which signi.cantly improves the overall complexity of the procedure. \nFusability is the capability to exhibit a semantics-preserving schedule such that some of the instances \nare fused, according to De.nition 7. First let us remark that fusibility is not a transitive relation. \nAs an illustration, consider the sequence of matrix-by\u00advector products x = Ab, y = Bx, z = Cy. While \nit is possible to fuse them 2-by-2, it is not possible to fuse them all together. When considering fusing \nloops for x = Ab, y = Bx, one has to permute loops in y= Bx. When considering fusing loops for y= Bx, \nz = Cy, one has to keep loops in y= Bx as is. We now present a suf.cient condition to determine the transitivity \nof fusibility, based on the existence of compatible pairwise permutations. First we introduce a decomposition \nof one-dimensional sched\u00adules in two sub-parts, with the objective of isolating loop permuta\u00adtion from \nthe other transformations embedded in the schedule. One can decompose a one-dimensional schedule TR with \ncoef.cients in k N into two sub-schedules \u00b5R and .R such that: RR TRk = \u00b5 + .R , \u00b5i . N, .Ri . N without \nanyloss of expressiveness. Such a decomposition is always possible because of the distributivity of the \nmatrix multiplication over the matrix addition.For our purpose, we are interestedin mod\u00adeling one-dimensional \nschedules which are not constant schedules. This is relevant as we do not want to consider building a \nschedule for fusion that would translate only into statement interleaving. On the contrary we aim atbuildinga \nschedule that performs the inter\u00adleaving of statements instances, hence the linear part of the sched\u00adule \nmust be non-null.For R surrounded by d loops, we enforce \u00b5to be a linear form of the d loop iterators: \n()()t RR \u00b5R(x R)= \u00b5 ... \u00b5 x00. i1 ... id xn 1 1 d To model non-constant schedules, we add the additional \nconstraint R .d = 1. Note that by constraining \u00b5 to have only one coef.\u00ad i=1\u00b5i cient set to 1, this does \nnot prevent to model any compositions of slowing or skewing: these would be embedded in the . part of \nthe schedule, as shown in the example below. The \u00b5 part of the schedule models different cases of loop \nper\u00admutations. For instance for statement R surrounded by 3 loops in the ThreeMatMat example, \u00b5R can \ntake only three values: ()()t \u00b5R(x R)= 10000. ij k N 1=(i) ()()t \u00b5R(x R)= 01000. ij k N 1=( j) ()()t \n\u00b5R(x R)= 00100. ij k N 1=(k) while .R can take arbitrary values. For better illustration let us now build \nthe decomposition of the schedule TR =(2. j+ k+ 2). k TRk is the composition of a permutation, a non-unit \nskewing and a shifting, and can be decomposed as follows: () \u00b5R = 01000 () .R = 01102R TR(x R)=(\u00b5 + .R)(x \nR)=(2. j+ k+ 2) k One may note that another possible decomposition is \u00b5R(x R)=(k), .R(x R)=(2j+ 2). In \ngeneral, when the schedule contains skewing it is possible to embed either of the skewing dimensions \nin the \u00b5 partofthe schedule.Forthesakeof coherencyweaddanextra con\u00advention for the decomposition: \u00b5 matches \nthe .rst non-null iterator coef.cientofthe schedule.Returningtotheexample, \u00b5R(x R)=( j), .R(x R)=( j+ \nk+ 2) is thus the only valid decomposition of TR . k Note that this decomposition prevents modeling of \ncomposi\u00adtions of loop permutations in the . part. For . to represent a loop permutation, . must have \nvalues in Z, as shown in the following example: ()()t () \u00b5R(x R)= 10000. ij k N 1= i ()() R (\u00b5 + .)(x \nR)= j. . = -11000 which is not possible as we have constrained .i . N. Hence, when considering arbitrary \ncompositions of permutation, (parametric) shifting, skewing and peeling, the \u00b5+ . decomposition separates \npermutation (embedded in the \u00b5 part of the schedule) from the other transformations (embedded in the \n. part of the schedule). We now show it is possible to determine if a set of statements are fusable only \nby looking at the possible values for the \u00b5part of their schedules. Considering three statements R, S, \nT that are fusable while pre-R serving the semantics at level k. Then there exist TR = \u00b5 + k ST .R \n, TS = \u00b5 + .S , TT = \u00b5 + .T leading to fusing those state\u00ad kk ments. Considering now the sub-problem \nof fusing only R and S, R webuild the set M R,S of all possible values of \u00b5 , \u00b5S for which there RS exist \na .R , .S leading to fuse Rand S. Obviously, the value of \u00b5 , \u00b5 S leading to fusing R, S, T are in M \nR,S, and \u00b5 , \u00b5T are also in M S,T. R Similarly \u00b5 , \u00b5T are in M R,T. We derive a suf.cient condition for \nfusibility based on pairwise loop permutations for fusion. LEMMA 6 (Pairwise suf.cient condition for \nfusibility). Given three statements R, S, T such that they can be 2-by-2 fused and dis\u00adtributed. Given \nM R,S (resp. M R,T, resp. M S,T) the set of possible TS tuples \u00b5R , \u00b5S (resp. \u00b5R , \u00b5 , resp. \u00b5 , \u00b5T)leading \nto fusing R and S (resp.RandT,resp.S andT)such that the fullprogram semantics S is respected.R, S, T \nare fusableif thereexists\u00b5R , \u00b5 , \u00b5T such that: R \u00b5 , \u00b5S . M R,S R \u00b5 , \u00b5T . M R,T S \u00b5 , \u00b5T . M S,T RS \n Proof. Given the schedule TR = \u00b5 + .R , TS = \u00b5 + .S leading to kk ' RR ' RT fusing R and S, T = \u00b5 + \n. , TT = \u00b5 + .T leading to fusing R kk ' SS ' S ' TT  and T, and T = \u00b5 + . , T = \u00b5 + . ' T leading to \nfusing S and kk T, such that theyall preserve the full program semantics. RS The schedule T*R = \u00b5 + .R \n+ . ' R , T*S = \u00b5 + .S + . ' R is le\u00ad kk gal, as adding . ' R consists in performing additional compositions \nof skewing and shifting, which cannot make the dependence vec\u00adtors lexicographically negative. It cannot \nconsist in performing a parametric shift (resulting in a loop distribution), TR is a schedule k fusing \nR and S and T ' R is a schedule fusing R and T. As T*R is kk a non-constant schedule, it leads to fusing \nR and S. Generalizing this reasoning we can exhibit the following semantics-preserving schedule leading \nto the fusion of R, S, T: T *RR + .R ' R + .SS + .T ' T = \u00b5 + . + . ' + . k ' T *SS + .R ' R + .SS + \n.T ' T k = \u00b5 + . + . + . T *TT + .R ' R + .S ' S + .TT = \u00b5 + . + . + . ' k As all statements are fused \n2-by-2, they are fused all together. As the three statements can be distributed 2-by-2, there is no depen\u00addence \ncycle. To stress the importance of Lemma 6, let us return to theThree-MatMat example. We can compute \nthe pairwise permutations for fusion sets at the outermost level: M R,S = {(i, i);(i, j);(i, k);( j, \ni);( j, j);( j, k);(k, i);(k, j);(k, k)} M R,T = {(i, i);( j, k)} M S,T = {(i, k);( j, j)}  These sets \nare computed by iteratively testing, against the set of constraints for semantics-preservation augmented \nwith fusion and orthogonality constraints, for the existence of solutions with a non\u00adnull value for each \nof the coef.cients associated with the statement iterators for only the two considered statements. This \nis in contrast to the general algorithm which requires to consider the whole set of candidate statements \nfor fusion. Here we can decide that R, S, T R ST are fusable, as the solution \u00b5 = j, \u00b5 = i, \u00b5 = k respects \nthe conditions from Lemma 6. This solution is presented in Figure 2(a). To improve further the tractability, \nwe rely on two more stan\u00addard properties on fusion. Given two statements R and S: 1. if R and S are not \nfusable, then any statement on which R transitively depends on is not fusable with S and any statement \ntransitively depending on S; 2. reciprocally, if R and S must be fused, then any statement depending \non R and on which S depends must also be fused with R and S.  These properties cut the number of tested \nsequences dramatically, in particular, in highly constrained programs such as loop-intensive kernels. \nThey are used at each step of the optimization algorithm.  4.4 Optimization algorithm We now present \nour optimization algorithm. The algorithm ex\u00adplores possible interleavings of dimension maxExploreDepth, \nand generates a collection of program schedules, each of them being a candidate for the optimization. \nWe use iterative compilation to se\u00adlect the best performing one.For each candidate program schedule we \ngenerate back a syntactic C code, compile it and run it on the target machine. The structure and principle \nof the optimization algorithm, shown in Figure 4, matches that of the pruning algorithm of Fig\u00adure 3, \nas it also aims at computing a set of feasible interleavings at a givenlevel.Itisin essencea specialization \nof the pruning algorithm for our optimization problem instance. To decide the fusibility of a set of \nstatements, we put the problem in a form matching the applicability conditions of Lemma 6.We merge nodes \nthat must be 2-by-2 fused to guarantee that we are checking for the strictest set of program-wise valid \n\u00b5 values when considering fusibility. Procedure buildLegalOptimizedSchedules computes, for a given pair \nof statements R, S, the set TR,S of semantics-preserving non-negative schedules augmented with permutability \nand orthog\u00adonality constraints as de.ned in the previous Section, given the previously computed rows \nof T. Procedures mustDistribute, mustFuse, mergeNodes operate in a similar fashion as in the gen\u00aderal \ncase. Procedure computeLegalPermutationsAtLevel com- R putes M R,S the set of all valid permutations \n\u00b5 , \u00b5S leading to fusion. R To check if a given permutation \u00b5 , \u00b5S is valid and leading for fu\u00adsion at \nlevel d, the set of constraints is tested for the existence of a solution where the schedule coef.cients \nof row d corresponding to \u00b5R and \u00b5S is not 0. This is suf.cient to determine the existence of the associated \n. part. Procedure existCompatiblePermutation collects the sets M of the pairs of statements connected \nby the path p, and tests for the existence of \u00b5 values according to Lemma 6. If there is no compatible \npermutation, then an additional constraint is added to F d such that it is not possible to fuse the statements \nin p all together. The constraint sets that the ei, j variables, for all pairs i, j in the path p, cannot \nbe set to 1 all together. Pro\u00adcedure embedInterleaving instantiate a schedule row T2d that implements \nthe interleaving i, using only the scalar coef.cients of the schedule. Procedure computeOptimizedSchedule \ninstanti\u00adates a schedule row T2d+1 at the current interleaving dimension d, for all statements. The interleaving \nis given by i, and indi\u00advidually for each group of statements to be fused under a com\u00admon loop, a schedule \nis computed to maximize fusion and to enforce permutability if possible. To select the coef.cient val\u00adues \nwe resort to the objective function (11). Finally, procedure finalizeOptimizedSchedule computes the possibly \nremaining schedule dimensions, when maxEploreDepth is lower than the maximum program loop depth. Note \nthat in this case, maximal fu- Figure 4. Optimization Algorithm sion is used to select the interleaving, \nhence we do not need tobuild a set of interleavings for the remaining depths. In practicethis algorithmprovedtobeveryfast,andfor \ninstance computing the set F 1 of all semantics-preserving interleavings at the .rst dimension takes \nless than 0.5 second for the benchmark ludcmp, pruning I 1 from about 1012 structures to 8, on an initial \nspace with 182 binary variables to model all total preorders. Sub\u00adsequently traversing F 1 and computing \na valid transformation for all interleavings takes an additional2.1 second.  5. Experimental Results \nStudies performed on the performance impact of selecting sched\u00adules at various levels highlighted the \nhigher impact of carefully selecting outer loops [32, 33]. The selection of the statement in\u00adterleaving \nat the outermost level captures the most signi.cant dif\u00adference in terms of locality and communication. \nWe can limit the recursive traversal of interleavings to the outer level only, while ob\u00adtaining signi.cant \nperformance improvement and a wide range of transformed codes. Nevertheless, when the number of candidates \nin F 1 is very small, typically because of several loop-dependent de\u00adpendences at the outer level, it \nis relevant to build F 2 and further. Benchmark advect3d atax bicg gemver ludcmp doitgen varcovar correl \n#loops 12 4 3 7 9 5 7 5 #stmts 4 4 4 4 14 3 7 6 #refs 32 10 10 19 35 7 26 12 #dim 12 12 12 12 182 6 42 \n30 O #cst 58 58 58 58 3003 22 350 215 #points 75 75 75 75 1012 13 47293 4683 #dim 9 6 10 6 40 3 22 21 \nF 1 #cst 43 25 52 28 443 10 193 162 #points 26 16 26 8 8 4 96 176 Time 0.82s 0.06s 0.05s 0.06s 0.54s \n0.08s 0.09s 0.09s Pb. Size 300x300x300 8000x8000 8000x8000 8000x8000 1000x1000 50x50x50 1000x1000 1000x1000 \nperf-Intel 1.47\u00d7 3.66\u00d7 1.75\u00d7 1.34\u00d7 1.98\u00d7 15.35\u00d7 7.24\u00d7 3.00\u00d7 perf-AMD 5.19\u00d7 1.88\u00d7 1.40\u00d7 1.33\u00d7 1.45\u00d7 14.27\u00d7 \n14.83\u00d7 3.44\u00d7 Table 1. Search space statistics and performance improvement Note that in the experiments \npresented in this paper we traverse exhaustively only F 1. Search space statistics Several ei, j and \npi, j variables are set dur\u00ading the pruning of O , so several consistency constraints are made useless \nand are notbuilt, signi.cantly helping to reduce the size of the space to build. Table 1 illustrates \nthis by highlighting, for our benchmarks considered, the properties of the polytope O in terms of the \nnumber of dimensions(#dim), constraints(#cst)and points(#points) when compared to F 1, the polytope of \npossible interleavings for the .rst dimension only.For each benchmark, we list #loops the number of loops, \n#stmts the number of statements, #refs the number of array references, as well as the time to build all \ncandidate interleavings from the input source code (that is, in\u00adcluding all analysis) on an Intel Xeon \n2.4GHz. We also report the dataset size we used for the benchmarks(Pb. Size). Performance evaluation \nThe automatic optimization and paral\u00adlelization approach has been implementedinP OCC, thePolyhedral CompilerCollection, \na complete source-to-source polyhedral com\u00adpiler based on available free software for polyhedral compilation.3 \nThe time to compute the space, select a candidate and compute a full transformation is negligible with \nrespect to the compilation and execution time of the tested versions. In our experiments, the full process \ntakes a few seconds for the smaller benchmarks, and up to about2 minutes for correl on an Intel Xeon \nprocessor. Extensive performance characterization is beyond the scope of this paper. The reader is referred \nto other publications [32, 34] for a more comprehensive analysis of the performance issues that are successfully \naddressed by our iterative framework on various high\u00adend multi-core architectures. Nevertheless, to illustrate \nthe bene.t of our approach, we report in Table 1 the performance improve\u00adment obtained by our iterative \nprocess. We used Intel ICC 11.0 with options -fast -parallel -openmp as the baseline on the original \ncode, and also to compile all our optimized programs. We report the performance improvement achieved \nover ICC with automatic parallelization enabled on the original code, under the column perf-Intel for \na 4-socket Intel hex-core Xeon E7450 (Dun\u00adnington), running at 2.4GHz with 64 GB of memory (24 cores, \n24 hardware threads), and perf-AMD for a 4-socket AMD quad-core Opteron 8380 (Shanghai) running at 2.50GHz \n(16 cores, 16 hard\u00adware threads) with 64GB of memory. Beyond absolute performance improvement, another \nmotivat\u00adingfactor for iterative selection of fusion structures is performance portability.Because of \nsigni.cant differences in design, in particu\u00adlar in SIMD units performance and cache behavior, a transforma\u00adtion \nhas to be tuned for a speci.c machine. This leads to a signif\u00adicant variation in performance across tested \nframeworks. To illus\u00adtrate this, we show in Figure5the relative performance normalized with respect to \nicc-par for gemver, for the Xeon and Opteron. The version index is plotted on the x axis;1 represents \nmaximal fusion 3PoCC is available at http://pocc.sourceforge.net while8corresponds to maximal distribution.For \nthe Xeon, the best found version performs 10% better than the best fusion con.gura\u00adtion for the Opteron. \nOptimizing for the Opteron, the best version performs 20% better than the best fusion structure for the \nXeon. Figure 5. Performance variability for gemver Tuning the trade-off between fusion and distribution \nis a rele\u00advant approach to drive a complete high-level optimization frame\u00adwork. The main motivation is \nthe dif.culty to design a portable heuristic to select the best interleaving, as the best fusion struc\u00adture \nis machine-dependent and even depends on the back-end com\u00adpiler used. Also, as shown in Figure 2, the \ninterleaving selection can dramatically impact the transformations required to implement the interleaving \n(e.g., loop interchange) and subsequently drives the optimization process. Our technique is able to automatically \nadapt to the target frame\u00adwork, and thanks to empirical search successfully discovers the best fusion \nstructure, whatever the speci.cs of the program, compiler and architecture. 6. Related Work Traditional \napproaches to loop fusion [25, 29, 40] are limited in their ability to handle compositions of loop transformations. \nThis is mainly due to the lack of a powerful representation for dependences and transformations. Hence, \nnon-polyhedral approaches typically study fusion in isolation from other transformations. Megiddo and \nSarkar [30] presented a solution to the problem of loop fusion in parallel programs that performs fusion \nwhile preserving the par\u00adallelism in the input program. We believe that several interesting solutions \nare not obtained when fusion is decoupled from paral\u00adlelization in those frameworks where legal fusion \nchoices are left out of the framework. Darte et al. [13, 14] studied the combination of loop shifting \nand fusion for parallelization. In contrast to all of these works, the search space explored in this \npaper includes fusion in the presence of all polyhedral transformations. Heuristics for loop fusion and \ntiling have been proposed in loop\u00adnest optimizers [26, 35, 45, 48]. Those heuristics have also been re\u00advisited \nin the context of new architectures with non-uniform mem\u00adory hierarchies and heterogeneous computing \nresources [37]. The polyhedral model is complementary to these efforts, opening many more opportunities \nfor optimization in loop nest optimizers and parallelizing compilers. Note that the polyhedral model \nis currently being integrated into production compilers, including GCC4, IBM XL and LLVM/Polly. A heuristic \nthat integrates loop fusion and tiling in the poly\u00adhedral model that subsumes a number of transformations \nsuch as interchange, skewing and loop shifting was presented in Bond\u00adhugula et al. [6, 8]. Their work \nbuilds complex sequences of transformations that enable communication-minimal tiling of im\u00adperfectly \nnested loops that generalizes the prior work on tiling perfectly-nested loops [21, 23, 36, 49] and is \nhelpful in identifying parallelism-locality trade-offs. Bondhugula et al. [7] re.ned their static cost \nmodel for fusion by incorporating utilization of hard\u00adware prefetch stream buffers, loss of parallelism, \nand constraints imposed by privatization and code expansion. However, these tech\u00adniques do not model \na closed space of all and only distinct fusion structures, instead they operate on the entire space of \nvalid fusion structures and select a speci.c one that minimizes a static cost function. Across a broad \nrange of machine architectures and compiler transformations, iterative compilation has proven to be effective \nin deriving signi.cant performance bene.ts [1, 5, 19, 31, 33, 35, 37, 42, 46]. Nevertheless, none of \niterative compilation approaches achieved the expressiveness and the ability to apply complex se\u00adquences \nof transformations presented in this paper, while focusing the search only on semantics-preserving transformation \ncandidates. Several powerful semi-automatic frameworks based on the polyhedral model [9, 11, 20, 24, \n43] have been proposed; these frameworks are able to capture fusion structures, but do not con\u00adstruct \npro.table parallelization and tiling strategies using a model\u00adbased heuristic. R-Stream is a source-to-source, \nauto-parallelizing compiler de\u00adveloped by Reservoir Labs, Inc. R-Stream is based on the poly\u00adhedral model \nand targets modern heterogeneous multi-core archi\u00adtectures, including multiprocessors with caches, systems \nwith ac\u00adcelerators, and distributed memory architectures that require ex\u00adplicit memory management and \ndata movement [28]. The af.ne scheduler inR-Streambuilds on contributions from Feautrier [18], Megiddo \nand Sarkar [30] and Bondhugula et al. [8]. It introduced the concept of af.ne fusion to enable a single \nformulation for the joint optimization of cost functions representing tradeoffs between amount of parallelism \nand amount of locality at various depths in the loop nest hierarchy. Scheduling algorithms inR-Stream \nsearch an optimization space that is either constructed on a depth-by\u00addepth basis as solutions are found \nor based on the convex space of all legal multi-dimensional schedules as had been illustrated by Vasilache \n[44].R-Stream allows direct optimization of the tradeoff function using Integer Linear Programming (ILP) \nsolvers as well as iterativeexplorationof the search space.Redundant solutionsin the search space are \nimplicitly pruned out by the combined tradeoff function as they exhibit the same overall cost. In practice, \na solution to the optimization problem represents a whole class of equivalent scheduling functions with \nthe same cost.  7. Conclusions The selection of a pro.table composition of loop transformations is a \nhard combinatorial problem. We proposed a complete, non\u00adredundant characterization of multidimensional \naf.ne transforma\u00adtions as a convex space. We then pruned this polyhedron, focus\u00ading on multidimensional \nstatement interleavings corresponding to a generalized combination of loop fusion, loop distribution \nand 4Graphite framework:http://gcc.gnu.org/wiki/Graphite. code motion. We proposed a practical optimization \nalgorithm to explore the pruned polyhedron, while heuristicallybuildinga prof\u00aditable, semantics-preserving, \naf.ne enabling transformation. This algorithm was applied to relevant benchmarks, demonstrating good \nscalability and strong performance improvements over state-of-the\u00adart multi-core architectures and parallelizing \ncompilers. Acknowledgments We are grateful toPaul Feautrier for providing the foundations for multidimensional \naf.ne scheduling, and the simpli.cation of Vasilache s original convex formulation of all semantics-preserving \nschedules that we presented in this paper. This work was supported in part by the U.S. Defense Advanced \nResearch Projects Agency through AFRL Contract FA8650-09\u00adC-7915, the U.S. National Science Foundation \nthrough awards 0541409, 0811457, 0811781, 0926687 and 0926688, by the U.S. Army through contract W911NF-10-1-0004, \nand by the European Commission through the FP7 project TERAFLUX id. 249013.  References [1] F. Agakov, \nE. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P. O Boyle, J. Thomson, M. Toussaint, and C. K. I. \nWilliams. Using machine learning to focus iterative optimization. In Proc. of the Intl. Symposium on \nCode Generation and Optimization (CGO 06), pages 295 305,Washington, 2006. [2] D. Barthou, J.-F. Collard, \nand P. Feautrier. Fuzzy array data.ow analysis. Journal ofParallel and Distributed Computing, 40:210 \n226, 1997. [3] C. Bastoul. Code generation in the polyhedral model is easier than you think. In IEEE \nIntl. Conf. on Parallel Architectures and Compilation Techniques (PACT 04), pages 7 16,Juan-les-Pins, \nFrance, Sept. 2004. [4] M.-W. Benabderrahmane, L.-N. Pouchet, A. Cohen, and C. Bastoul. The polyhedral \nmodel is more widely applicable than you think. In Intl. Conf. on Compiler Construction (ETAPS CC 10), \nLNCS 6011, pages 283 303,Paphos, Cyprus, Mar. 2010. [5] F. Bodin, T. Kisuki, P. M. W. Knijnenburg, M. \nF. P. O Boyle, and E. Rohou. Iterative compilation in a non-linear optimisation space. In W. on Pro.le \nandFeedbackDirected Compilation,Paris, Oct. 1998. [6] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, \nJ. Ramanujam, A. Rountev, and P. Sadayappan. Automatic transformations for communication-minimized parallelization \nand locality optimization in the polyhedral model. In International conference on Compiler Con\u00adstruction \n(ETAPS CC), Apr. 2008. [7] U. Bondhugula,O. Gunluk,S. Dash, andL. Renganarayanan.Amodel for fusion and \ncode motion in an automatic parallelizing compiler. In Proc. of the 19th Intl. conf. onParallel Architectures \nand Compilation Techniques (PACT 10), pages 343 352, 2010. [8] U. Bondhugula, A. Hartono, J. Ramanujam, \nand P. Sadayappan. A practical automatic polyhedral program optimization system. In ACM SIGPLAN Conference \non Programming Language Design and Imple\u00admentation,June 2008. [9] C. Chen, J. Chame, and M. Hall. CHiLL: \nA framework for compos\u00ading high-level loop transformations. Technical Report 08-897, U. of Southern California, \n2008. [10] P. Clauss. Counting solutions to linear and nonlinear constraints through ehrhart polynomials: \napplications to analyze and transform scienti.c programs. In Proc. of the Intl. Conf. on Supercomputing, \npages 278 285.ACM, 1996. [11] A. Cohen, S. Girbal, D. Parello, M. Sigler, O. Temam, and N. Vasi\u00adlache. \nFacilitating the search for compositions of program transfor\u00admations. In ACM International conference \non Supercomputing, pages 151 160,June 2005. [12] A. Darte. On the complexity of loop fusion. Parallel \nComputing, pages 149 157, 1999. [13] A. Darte and G. Huard. Loop shifting for loop parallelization. Tech\u00adnical \nReport RR2000-22, ENSLyon, May 2000. [14] A. Darte, G.-A. Silber, and F. Vivien. Combining retiming \nand scheduling techniques for loop parallelization and loop tiling. Parallel Proc. Letters, 7(4):379 \n392, 1997. [15] P. Feautrier. Parametric integer programming. RAIRO Recherche Op\u00b4erationnelle, 22(3):243 \n268, 1988. [16] P. Feautrier. Data.ow analysis of scalar and array references. Intl. J. ofParallel Programming, \n20(1):23 53, Feb. 1991. [17] P. Feautrier. Some ef.cient solutions to the af.ne scheduling prob\u00adlem, \npart I: one dimensional time. Intl. J. of Parallel Programming, 21(5):313 348, Oct. 1992. [18] P. Feautrier. \nSome ef.cient solutions to the af.ne scheduling problem, part II: multidimensional time. Intl. J. of \nParallel Programming, 21(6):389 420, Dec. 1992. [19]F. Franchetti, Y.Voronenko, andM.P\u00a8uschel. Formal \nloop merging for signal transforms. In ACM SIGPLAN Conf. on Programming Language Design and Implementation, \npages 315 326, 2005. [20] S. Girbal, N. Vasilache, C. Bastoul, A. Cohen, D. Parello, M. Sigler, and O.Temam. \nSemi-automatic composition of loop transformations. Intl.J. ofParallel Programming, 34(3):261 317,June \n2006. [21] M. Griebl. Automatic parallelization of loop programs for distributed memory architectures. \nHabilitation thesis. at f\u00a8Mathematik Facult\u00a8ur und Informatik, Universit\u00a8at Passau, 2004. [22] A.-C. \nGuillou,F. Quiller\u00b4e,P. Quinton, S. Rajopadhye, andT. Risset. Hardware design methodology with the Alpha \nlanguage. In FDL 01, Lyon, France, Sept. 2001. [23] F. Irigoin and R. Triolet. Supernode partitioning. \nIn ACM SIGPLAN Principles of Programming Languages, pages 319 329, 1988. [24] W. Kelly. Optimization \nwithin a Uni.ed Transformation Framework. PhD thesis, Univ. of Maryland, 1996. [25] K. Kennedy and K. \nMcKinley. Maximizing loop parallelism and improving data locality via loop fusion and distribution. In \nLanguages and Compilers forParallel Computing, pages 301 320, 1993. [26] I. Kodukula, N. Ahmed, and K. \nPingali. Data-centric multi-level blocking. In ACM SIGPLAN 97 Conf. on Programming Language Design and \nImplementation, pages 346 357, LasVegas,June 1997. [27] M.Kudlur andS. Mahlke. Orchestrating theexecutionofstream \npro\u00adgrams on multicore platforms. In ACM SIGPLAN Conf. on Program\u00adming Language Design and Implementation \n(PLDI 08), pages 114 124.ACM Press, 2008. [28] R. Lethin, A. Leung, B. Meister, N. Vasilache, D. Wohlford, \nM. Baskaran, A. Hartono, and K. Datta. R-stream compiler. In D. Padua, editor, Encyclopedia of Parallel \nComputing. Springer, 1st edition., 2011,50p.in4 volumes, notavailableseparately., hardcover edition,June \n2011. [29] K. S. McKinley, S. Carr, and C.-W. Tseng. Improving data local\u00adity with loop transformations. \nACM Trans. Program. Lang. Syst., 18(4):424 453, 1996. [30] N. Megiddo andV. Sarkar. Optimal weighted \nloop fusion for parallel programs. In symposium on Parallel Algorithms and Architectures, pages 282 291, \n1997. [31] A. Nisbet. GAPS:A compiler framework for genetic algorithm (GA) optimised parallelisation. \nIn HPCN Europe 1998: Proc. of the Intl. Conf. and Exhibition on High-Performance Computing and Network\u00ading, \npages 987 989, London, UK, 1998. Springer-Verlag. [32] L.-N. Pouchet. Interative Optimization in thePolyhedral \nModel. PhD thesis, University ofParis-Sud 11, Orsay, France,Jan. 2010. [33] L.-N. Pouchet, C. Bastoul, \nA. Cohen, and J. Cavazos. Iterative opti\u00admization in the polyhedral model: Part II, multidimensional \ntime. In ACM SIGPLAN Conf. on Programming Language Design and Imple\u00admentation (PLDI 08), pages 90 100.ACM \nPress, 2008. [34] L.-N. Pouchet, U. Bondhugula, C. Bastoul, A. Cohen,J. Ramanujam, andP. Sadayappan. \nCombined iterative and model-driven optimization in an automatic parallelization framework. In Conf. \non SuperComput\u00ading (SC 10), New Orleans, LA, Nov. 2010. To appear. [35] A. Qasem and K.Kennedy. Pro.table \nloop fusion and tiling using model-driven empirical search. In Proc. of the 20th Intl. Conf. on Supercomputing \n(ICS 06), pages 249 258.ACM press, 2006. [36] J. Ramanujam and P. Sadayappan. Tiling multidimensional \niteration spaces for multicomputers. Journal of Parallel and Distributed Com\u00adputing, 16(2):108 230, 1992. \n[37] M. Ren,J.Y.Park,M. Houston,A. Aiken, andW.J. Dally. A tuning framework for software-managed memory \nhierarchies. In Intl. Conf. on Parallel Architectures and Compilation Techniques (PACT 08), pages 280 \n291.ACM Press, 2008. [38] L. Renganarayanan, D. Kim, S. Rajopadhye, and M. M. Strout. Pa\u00adrameterized \ntiled loops for free. SIGPLAN Notices, Proc. of the 2007 PLDI Conf., 42(6):405 414, 2007. [39] A. Schrijver. \nTheory of linear and integer programming. John Wiley &#38; Sons, 1986. [40] S. Singhai andK. McKinley.AParameterized \nLoop FusionAlgorithm for ImprovingParallelism and Cache Locality. The ComputerJournal, 40(6):340 355, \n1997. [41] N. J. A. Sloane. Sequence a000670. The On-Line Encyclopedia of Integer Sequences. [42] M. \nStephenson, S. Amarasinghe, M. Martin, and U.-M. O Reilly. Meta optimization: improving compiler heuristics \nwith machine learning. SIGPLAN Not., 38(5):77 90, 2003. [43] A. Tiwari, C. Chen, J. Chame, M. Hall, and \nJ. K. Hollingsworth. A scalable autotuning framework for computer optimization. In IPDPS 09, Rome, May \n2009. [44] N.Vasilache. Scalable Program OptimizationTechniques in thePoly\u00adhedra Model. PhD thesis, University \nofParis-Sud 11, 2007. [45] S. Verdoolaege, F. Catthoor, M. Bruynooghe, and G. Janssens. Fea\u00adsibility \nof incremental translation. Technical Report CW 348, Katholieke Universiteit Leuven Department of Computer \nScience, Oct. 2002. [46]Y.Voronenko,F.de Mesmay, andM.P\u00a8Computer generation uschel. of general size linear \ntransform libraries. In Intl. Symp. on Code Generation and Optimization (CGO 09), Mar. 2009. [47] R. \nC. Whaley, A. Petitet, and J. J. Dongarra. Automated empirical optimization of software and the atlas \nproject. Parallel Computing, 27(1 2):3 35, 2001. [48] M. Wolf, D. Maydan, and D.-K. Chen. Combining loop \ntransforma\u00adtions considering caches and scheduling. In MICRO 29: Proceedings of the 29th annual ACM/IEEE \ninternational symposium on Microar\u00adchitecture, pages 274 286, 1996. [49] M. Wolfe. More iteration space \ntiling. In Proceedings of Supercom\u00adputing 89, pages 655 664, 1989. [50] M. Wolfe. High performance compilers \nfor parallel computing. Addison-WesleyPublishing Company, 1995.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>High-level loop transformations are a key instrument in mapping computational kernels to effectively exploit the resources in modern processor architectures. Nevertheless, selecting required compositions of loop transformations to achieve this remains a significantly challenging task; current compilers may be off by orders of magnitude in performance compared to hand-optimized programs. To address this fundamental challenge, we first present a convex characterization of all distinct, semantics-preserving, multidimensional affine transformations. We then bring together algebraic, algorithmic, and performance analysis results to design a tractable optimization algorithm over this highly expressive space. Our framework has been implemented and validated experimentally on a representative set of benchmarks running on state-of-the-art multi-core platforms.</p>", "authors": [{"name": "Louis-No&#235;l Pouchet", "author_profile_id": "81330496337", "affiliation": "The Ohio State University, Columbus, OH, USA", "person_id": "P2509688", "email_address": "pouchet@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Uday Bondhugula", "author_profile_id": "81326487775", "affiliation": "IBM T.J. Watson Research Center, White Plains, OH, USA", "person_id": "P2509689", "email_address": "ubondhug@us.ibm.com", "orcid_id": ""}, {"name": "C&#233;dric Bastoul", "author_profile_id": "81320487841", "affiliation": "University of Paris-sud 11, Saclay, OH, USA", "person_id": "P2509690", "email_address": "cedric.bastoul@inria.fr", "orcid_id": ""}, {"name": "Albert Cohen", "author_profile_id": "81100146104", "affiliation": "INRIA, Saclay, OH, USA", "person_id": "P2509691", "email_address": "albert.cohen@inria.fr", "orcid_id": ""}, {"name": "J. Ramanujam", "author_profile_id": "81460642764", "affiliation": "Louisiana State University, Baton Rouge, OH, USA", "person_id": "P2509692", "email_address": "jxr@ece.lsu.edu", "orcid_id": ""}, {"name": "P. Sadayappan", "author_profile_id": "81453642049", "affiliation": "The Ohio State University, Columbus, OH, USA", "person_id": "P2509693", "email_address": "saday@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Nicolas Vasilache", "author_profile_id": "81100270395", "affiliation": "Reservoir Labs, Inc., New York, NY, USA", "person_id": "P2509694", "email_address": "vasilache@reservoir.com", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926449", "year": "2011", "article_id": "1926449", "conference": "POPL", "title": "Loop transformations: convexity, pruning and optimization", "url": "http://dl.acm.org/citation.cfm?id=1926449"}