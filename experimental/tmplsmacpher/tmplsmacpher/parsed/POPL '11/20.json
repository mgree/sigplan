{"article_publication_date": "01-26-2011", "fulltext": "\n A Separation Logic for Re.ning Concurrent Objects Aaron Turon * Mitchell Wand Northeastern University \n {turon, wand}@ccs.neu.edu Abstract Fine-grained concurrent data structures are crucial for gaining per\u00adformance \nfrom multiprocessing, but their design is a subtle art. Re\u00adcent literature has made large strides in \nverifying these data struc\u00adtures, using either atomicity re.nement or separation logic with rely-guarantee \nreasoning. In this paper we show how the owner\u00adship discipline of separation logic can be used to enable \natomicity re.nement, and we develop a new rely-guarantee method that is lo\u00adcalized to the de.nition of \na data structure. We present the .rst se\u00admantics of separation logic that is sensitive to atomicity, \nand show how to control this sensitivity through ownership. The result is a logic that enables compositional \nreasoning about atomicity and in\u00adterference, even for programs that use .ne-grained synchronization and \ndynamic memory allocation. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program \nVeri.cation; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams \nGeneral Terms Algorithms, Veri.cation 1. Introduction 1.1 The goal This paper is about the semantic \nrelationship between ownership, atomicity, and interference. We begin with a very simple data struc\u00adture: \nthe counter. Counters permit a single operation, inc: int inc(int *C) { int tmp = *C; *C = tmp+1; return \ntmp; } Of course, this implementation only works in a sequential setting. If multiple threads use it \nconcurrently, an unlucky interleaving can lead to several threads fetching the same value from the counter. \nThe usual reaction to this problem is to use mutual exclusion, wrap\u00adping the operation with lock instructions. \nBut as Moir and Shavit put it, with this arrangement, we prevent the bad interleavings by preventing \nall interleavings [20]. Fine-grained concurrent objects permit as many good interleavings as possible, \nwithout allowing any bad ones. The code * Supported by a grant from Microsoft Research. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 11, January \n26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 \ndo { tmp = *C; } until CAS(C, tmp, tmp+1); implements inc using an optimistic approach: it takes a snapshot \nof the counter without acquiring a lock, computes the new value of the counter, and uses compare-and-set \n(CAS) to safely install the new value. The key is that CAS compares *C with the value of tmp, atomically \nupdating *C with tmp +1 and returning true if they are the same, and just returning false otherwise. \nEven for this simple data structure, the .ne-grained imple\u00admentation signi.cantly outperforms the lock-based \nimplementa\u00adtion [17]. Likewise, even for this simple example, we would prefer to think of the counter \nin a more abstract way when reasoning about its clients, giving it the following speci.cation: inc(c, \nret) . (.x : c . x, c . x +1 . ret = x) This speci.cation says that, for any value x, inc atomically \ntrans\u00adforms a heap in which c points to x into one where c points to x+1, moreover ensuring that the \nvalue of ret (an out-parameter) is x. We express both implementations and speci.cations in a single speci.cation \nlanguage, and say that . re.nes (implements) . if for every context C[-], each behavior of C[.] is a \npossible behavior of C[.].1 That is, no client can detect that it is interacting with . rather than ., \neven if the client invokes . s operations concurrently. This paper presents a logic for proving such \nre.nements, based on separation logic. The key idea is to use a notion of ownership, expressed through \nseparation logic, to reason about both atomicity and interference. The key contributions are the semantics \n( fenced re.nement ) and proof rules enabling this reasoning. We will be able to easily verify the counter \ngiven above (\u00a74.1), as well as more complex nonblocking data structures (e.g. a nonblocking stack \u00a76). \n 1.2 The approach Since this paper is focused on veri.cation rather than development, we will take the \nperspective of abstraction, which is converse to re.nement: . re.nes . iff . abstracts .. The appeal \nof abstraction is that, to prove something about the behaviors of C[.] for a particular client C, it \nsuf.ces to consider the behaviors of C[.] instead and . is usually much simpler than .. For verifying \ndata structures in a sequential setting, one is primarily interested in data abstraction,where . abstracts \naway representation details from .. In a concurrent setting, we want simplicity in another respect as \nwell: atomicity abstraction,where . appears to perform an operation in one atomic step even though . \ntakes many steps. Re.nement hinges on the power of the context C[-]: what can it observe, and with what \ncan it interfere? The more powerful the context, the less freedom we have in implementing a speci.cation. \nConcurrency changes not what,but when. In a .rst-order language, a sequential context can only interact \nwith the state before or after 1 We take the perspective, advanced by Filipovic\u00b4et al. [8], that linearizabil\u00adity \n[11] is a proof technique for re.nement and we choose not to use it. See \u00a78 for more discussion. running \nthe program fragment in its hole. A concurrent context might do so at any time. In his classic 1973 paper, \nProtection in Programming Lan\u00adguages , Morris argued that a programmer should be able to prove that his \nprograms . . . do not malfunction solely on the basis of what he can see from his private bailiwick [22]. \nThat paper and its con\u00adtemporaries showed how to enable such reasoning by weakening the context, e.g. \nby hiding data through lexical scoping [22], pro\u00adtecting data by dynamic sealing [22] or by giving it \nan opaque (ex\u00adistential) type [15, 19]. A basic message of this paper is that these and other hiding \ntechniques serve just as well to localize reasoning in a concurrent setting. We present an approach for \nproving re.nement/abstraction of concurrent objects using an ownership discipline to enforce hiding. \nAs in any separation logic, the ownership discipline is dynamic: what is owned can vary over time and \nis often determined by runtime values in the heap. We derive two major bene.ts from our ownership discipline. \nFirst, it enables us to perform atomicity abstraction, under the basic principle that if you don t own \nit, you can t see it . Thus an atomic step modifying only private data can be absorbed into an adjacent \natomic step, to form a single observably atomic step. Claim 1: By making both atomicity and ownership \nexplicit, we can clarify and exploit their relationship: observed atom\u00ad icity is relative to ownership. \nAtomicity abstraction stems from the notion of private data. The other bene.t of ownership is to do with \nobject instances, which are shared among concurrently-running methods. Claim 2: Because ownership limits \npossible interference, we can exploit it to give a modular and dynamic account of rely/guarantee reasoning. \nRely/guarantee reasoning [12] is a well-known technique for com\u00adpositionally reasoning about interference: \nyou prove that each thread guarantees certain behavior, as long as it can rely on other threads to interfere \nin a limited way. Traditionally, the rely con\u00adstraint must be carried through the whole veri.cation, \neven if it is only relevant to a small portion. Using ownership, we localize rely/guarantee reasoning \nto the de.nition of a concurrent object, since we know all interference must stem from the methods making \nup its de.nition. In our approach, rely/guarantee reasoning is done with respect to a hypothetical object \ninstance; at runtime, there may be zero, one, or many such instances, each residing in its own region \nof the heap. A broadly similar form of rely/guarantee rea\u00adsoning appeared in very recent work by Dinsdale-Young \net al. [4]; we compare the approaches in \u00a78. To support our claims, we make the following contributions: \n We present a new speci.cation language in the tradition of re\u00ad.nement calculi [1, 21], but tailored \nto separation logic-style reasoning (\u00a72). We give it an operational semantics, which de\u00adtermines the \nmeaning of re.nement. Our notion of re.nement captures safety properties only, but we expect most of \nour re\u00adsults to easily transfer to a model incorporating liveness.  We adapt Brookes s transition trace \nmodel [2] to handle dy\u00adnamic memory allocation, and use it to give our speci.cation language a simple \ndenotational semantics (\u00a73). The semantics is adequate for the operational model, so it can be used to \njus\u00adtify re.nements. To our knowledge, this is the .rst model of separation logic that captures atomicity \nexplicitly.  On the strength of the denotational model, we are able to give many basic re.nement laws \n(\u00a74), and we show how these laws can be used to verify a simple nonblocking counter (\u00a74.1). They  Domains \n Syntax x, y, z . VAR variables LOC locations v . VAL values . . VAR -VAL environments s . S LOC -VAL \nheaps o . Si S .{i} outcomes Speci.cations ., ., . ::= .; . | . 1 . | . . . |.x.. | \u00b5X.. | X | let f \n= F in . | Fe | (.x : p, q)|{p}Procedures F ::= f | .x.. Con.gurations . ::= ., s | o Predicates p, q, \nr ::= true | false | emp | e . e' | e = e' | p * q | p . q | p . q | p . q |.x.p |.x.p Expressions e \n::= x | n | e + e' | (e, e') Figure 1. are not strong enough, however, to handle more complex data structures \nfor that, we need ownership-based reasoning. In \u00a75.2 we introduce our ownership discipline, formally \ncap\u00adturedbythenotionof fenced re.nement. The semantics of fenced re.nement is a hybrid between trace \nsemantics and in\u00adput/output relations: traces for shared state, I/O for private state. We give laws of \nfenced re.nement that justify our two claims above. Finally, we give a law DATA2 relating fenced and \nstan\u00addard re.nement, which allows us to employ fenced re.nement compositionally in correctness proofs. \nWe sketch the soundness proof for this law in \u00a77.  We present a case study the veri.cation of a nonblocking \nstack to demonstrate fenced re.nement (\u00a76).  This paper draws on several ideas from recent work, especially \nthat of Vafeiadis et al. [27, 28] and Elmas et al. [5, 6]. We refer the reader to \u00a78 for a detailed discussion \nof prior work.  2. Speci.cations The standard view of re.nement calculus is as follows [21]: we are \nbroadly interested in speci.cations , the most concrete of which are programs . Thus, we have an expressive \nlanguage of speci\u00ad.cations . (so it is easy to say what behavior is wanted), and a sublanguage of programs \n(that say how to produce such behavior). It will be possible, for instance, to write a speci.cation that \nsolves the halting problem, but this will not correspond to any program. The distinction between programs \nand general speci.cations is not important for our purposes here; suf.ce it to say that the implemen\u00adtations \nwe will verify are clearly executable on a machine. The language of speci.cations (Figure 1) includes \ntraditional programming constructs like sequential composition .; . and par\u00adallel composition . 1 ., \nlet-binding of .rst-order procedures, and recursion \u00b5X... In addition, there are logical constructs like \ndis\u00adjunction . . . and quanti.cation .x... These latter can be read operationally as nondeterminism: \n. . . either behaves like . or behaves like .,and .x.. behaves like . for some choice of x. The remaining \nspeci.cation constructs (shaded in gray) will be ex\u00adplained in \u00a72.2. In the largely-standard semantics \ngiven below, a speci.cation interacts step-by-step with a heap s.A con.guration . of the ab\u00adstract machine \nis typically a pair ., s of the remaining speci.cation to execute, and the current heap. In addition, \nthere are two terminal con.gurations, called outcomes: either successful termination with heap s,orelsea \nfault i. Faulting will be explained in \u00a72.2; the important point at the moment is that faulty termination \npropagates up through sequential and parallel composition. Operational semantics of speci.cations . \n. . ' .1,s . . ' 1,s ' .1,s . s ' .1,s . i .1; .2,s . . ' 1; .2,s ' . 1; .2,s . .2,s ' .1; .2,s . i .1 \n1 .2,s . . .1,s . . ' 1,s ' .2 1 .1,s . . .1 1 .2,s . . ' 1 1 .2,s ' ' .1,s . s.1,s . i .1 1 .2,s . \n.2,s ' .1 1 .2,s . i .1 . .2,s . .1,s .1 . .2,s . .2,s v . VAL \u00b5X.., s . .[\u00b5X../X],s .x.., s . .[v/x],s \nlet f = F in ., s . .[F/f],s (.x..)e, s . .[[e] /x],s (s, o) . act(p, q) s |= p * true s |= p * true \n(.x : p, q) ,s . o {p},s . s {p},s . i A few other points of note: Variables x, y, z are immutable; \nonly the heap is mutable.  The domain of values VAL is unspeci.ed, but at least includes locations, \nintegers, and pairs.  We assume a denotational semantics [e]. : VAL for expressions e, which is straightforward. \nWe write [e] for [e]\u00d8 .  A composition .; . steps to . exactly when . terminates.  The .rst rule for \n1 makes it symmetric.  We abbreviate let f = .x.. in . by let f(x)= . in ..  2.1 Heap predicates To \ndescribe the remaining forms of speci.cations (shaded in gray), we .rst need to de.ne heap predicates \np. These are the usual predicates of separation logic: they make assertions about both program variables \n(e.g. x =3) and the heap (e.g. x . 3). As such, the entailment relation s, . |= p holds when p is true \nboth of the heap s and the environment ., which gives the values of program variables: Predicate semantics \ns, . |= p s, . |= emp iff s = \u00d8 ' s, . |= e . e iff s =[[e]. . [e '].] s, . |= e = e ' iff [e]. = [e \n']. s, . |= p * q iff .s1,s2.s = s1 l s2,s1,. |= p, s2,. |= q s, . |= p . q iff s, . |= p or s, . |= \nq s, . |= .x.p iff .v. s,.[x . v] |= p The predicates emp, e . e ' and p * q come from separation logic. \nThe predicate emp asserts that the heap is empty, while e . e ' asserts that the heap contains a single \nlocation e which contains the value e ' . Thus x . 3 is satis.ed when . takes x to the only location \nin s,and s maps that location to 3.The separating conjunction p*q is satis.ed by any heap separable into \none subheap satisfying p and one satisfying q. Thus x . 3*y . 4 asserts that the heap contains exactly \ntwo locations, containing 3 and 4. In particular, it implies that the locations x = y. We write s |= \np for s, \u00d8|= p.  2.2 Actions, assertions, and assumptions In separation logic, a Hoare triple {p}C{q} \nfor a command C asserts that, if s |= p,then C running on s will not fault (by, say, following a dangling \npointer) and,  if it terminates, will do so with a heap s ' such that s ' |= q.  Note that p and q \nmay be quite speci.c about the shape of the heap, for example insisting that it consists of a single \ncell. A key idea of separation logic is that commands are local: what they do on a large heap is determined \nby what they do on smaller heaps. This idea leads to the frame rule, allowing {p*r}C{q *r} to be inferred \nfrom {p}C{q}. In our speci.cation language, we forgo basic commands in favor of a single construct: the \naction (.x : p, q). An action describes an atomic step of computation in terms of the strongest partial \ncorrectness assertion it satis.es (cf. Morgan s speci.cation state\u00adments [21]). In other words, it permits \nevery behavior that satis.es the triple {p}-{q}. The variables x are in scope for both p and q, and are \nused to link them together (as in inc above). To understand the behavior of a simple action (p, q),itis \nhelpful to think in terms of a speci.c starting heap s. First, suppose that s |= p * true, meaning that \nno subheap of s satis.es p.Inthis case, the behavior is unconstrained, and in particular the action is \npermitted to fault. Hence, faults arise when preconditions are not satis.ed. On the other hand, if s \ncan be decomposed into subheaps s = s1 l s2 such that s1 |= p, then the action must take a step to some \nheap s1 ' l s2 such that s1 ' |= q;itmustestablishthe postcondition without modifying the frame s2, and \ncannot fault. The semantics of actions is in essence that of Calcagno et al. [3]: Action semantics act(p, \nq) . S \u00d7 Si (s, o) . act(p, q) iff .s1,s2,.. if s = s1 l s2 and s1,. |= p then .s1' .o = s1 ' l s2 and \ns1' ,. |= q Many familiar commands can be expressed as actions. The following are used in our examples, \nand provide some intuition for the semantics of actions: abort (false, true)miracle (true, false)skip \n(emp, emp) new(x, ret) (emp, ret . x)put(a, x) (a .-,a . x)get(a, ret) (.x : a . x, a . x . x = ret)cas(a, \nold, new, ret) \\) (x = old . ret =0 . a . x) .x : a . x, . (x = old . ret =1 . a . new) where the predicate \na .- is shorthand for .z.a . z.The abort action can always fault, as its precondition is never satis.ed. \nOn the other hand, miracle never faults, but it also never takes any steps, as its postcondition is never \nsatis.ed. From the standpoint of the operational semantics, miracle neither faults nor successfully terminates; \nfrom a partial correctness standpoint, it satis.es every speci.cation [21]. The skip action cannot fault, \nbecause any heap has a subheap that satis.es emp. Likewise, new cannot fault, and it furthermore ensures \nthat ret is distinct from any address allocated in the frame. Finally, put, get and cas fault if a is \nunallocated. Notice that get and cas both quantify over a variable x in order to connect an observation \nmade in the precondition to a constraint in the postcondition. One common theme in these examples is \nthe use of postcondi\u00adtions to .lter possible behaviors, miracle being the most extreme case. For procedures \nlike get, the postcondition is used to constrain the out-parameter ret. Where one might write let x = \nget(a) in . as a program, we write .x.get(a, x); . as a speci.cation. Execu\u00adtions where x took on the \nwrong value simply disappear, neither terminating nor faulting. One use of postcondition .ltering is \nso common that we in\u00adtroduce shorthand for it: an assumption [p] stands for the action (emp, emp . p). \nBecause its precondition is emp, an assumption cannot fault, and because its postcondition implies emp \nas well, it cannot alter the heap; it is a re.nement of skip. Since it is con\u00adjoined with emp, the predicate \np cannot further constrain the heap; it is used only to constrain the value of variables. Assumptions \nare used for control .ow: the speci.cation if p then . else . is sugar for ([p]; .) . ([\u00acp]; .). Finally, \nassertions {p} simply test a predicate: if some subheap satis.es p then the heap is left unchanged, and \notherwise the asser\u00adtion faults. Assertions provide a way to introduce a claim during veri.cation while \npostponing its justi.cation (cf. [5]). We can write the .ne-grained concurrent version of inc in our \nspeci.cation language as follows: inc ' (c, ret) \u00b5X. .t.get(c, t); .b.cas(c, t, t +1,b); if b =1 then \n[ret = t] else X   3. Re.nement: model theory We have seen what speci.cations are, how they behave, \nand how they express common programming constructs. But the point of working with speci.cations is to \ncompare them: we want to say when a relatively concrete speci.cation implements a more ab\u00adstract one. \nInformally, a speci.cation . implements . if no client can observe that it is interacting with . instead \nof ., i.e.,everybe\u00adhavior of . is a possible behavior of .. Formally, this situation is expressed as \nre.nement: Re.nement . Lop . { if C[.],s i then C[.],s i . Lop . iff .C, s. if C[.],s . then C[.],s \n. or C[.],s i where . i iff . . * i and . . iff .s.. . * s where C is a speci.cation context (a speci.cation \nwith a hole) clos\u00ading . and .. The op stands for operational semantics. In this de.nition, we collapse \nthe possible outcomes of a C[.],s to three cases: nontermination, successful termination, and faulting \ntermi\u00adnation. If the speci.cation . can fault in a given context, then . is relieved of any obligations \nfor that context. In this way, fault\u00ading behavior is treated as unspeci.ed behavior, and as we will soon \nsee, it follows that abort is the most permissive speci.cation (. Lop abort for all .). Ultimately, our \ngoal is to give useful axioms and inference rules for proving such re.nements. However, as usual, the \nquanti.cation over all contexts in the de.nition of re.nement makes it dif.cult to work with directly. \nAs a stepping stone, in this section we give a denotational semantics for speci.cations, which will give \nus a sound (but not complete) denotational version Lden of re.nement. Readers primarily interested in \nthe proof rules can read \u00a74.rst, taking the soundness of the rules on faith. The denotational model is \nbased on Brookes s transition trace model [2], which gave a fully abstract semantics for a parallel WHILE \nlanguage. We adjust this model to deal with pointers and the heap, as well as faulting. The challenge \nin giving a denotational model for concurrency is the semantics of parallel composition: to de.ne [. \n1 .] in terms of [.] and [.] we must leave room for interference from . in the meaning of .,and vice-versa. \nWith transition traces, we give meaning to . and . in terms of discrete timeslices of execution, between \nwhich we allow for arbitrary interference. To calculate the meaning of . 1 ., we need only interleave \nthese timeslices. In detail: we model the behavior of each speci.cation as a set of transition traces. \nA transition trace is a .nite sequence of moves (s, s ' ). Each move represents one timeslice of execution, \nwhich may correspond to zero, one, or some .nite number of steps . in the operational semantics. A trace \nof . like (s1,s1' )(s2,s2' ) arises from an execution where .rst ., s1 . * . ' ,s1' , then some as-yet \nunknown speci.cation in the environment changed the heap from . '' s1 ' to s2,then . ' ,s2 . * ,s2' \n. A trace can be terminated by a fault, either on the part of the speci.cation as in (s, i),oronthe part \nof its environment as in (i, i): Domains MOVE S \u00d7 S s, t, u . TRACE FAULT Si \u00d7{i} S, T, U . TRACE TRACE \nMOVE * ; FAULT? The transition traces of a speci.cation can be read directly from the operational semantics: \n Observed traces t .O[.] '' ' ., s . * o .,s . * .,s t .O. (i, i) .O[.] (s, o) .O[.] (s, s ' )t .O[.] \n The inference rules say, respectively: the environment might fault at any time; a terminating timeslice \n(whether successful or faulting) results in a singleton trace; a nonterminating timeslice allows the \nspeci.cation to resume later (under a possibly-changed heap). An important insight in Brookes s model \nis that the transition traces of a speci.cation are closed under stuttering (addition of a step (s, s))and \nmumbling (merging of two steps with a common midpoint). Closure under stuttering means that the context \nof a speci.cation cannot observe timeslices in which it does not change the heap; this justi.es that \nskip is a unit for sequential composition. Closure under mumbling implies that, for example, (p, r)Lop \n(p, q) ; (q, r): the scheduler might give the latter speci.cation a long enough timeslice to execute \nboth actions, so that the behavior of the former is a possible behavior of the latter. Our denotational \nsemantics gives an alternative de.nition of O[.] that is compositional in the structure of ..Ingivingthe \nde\u00adnotational semantics, we must explicitly apply stuttering and mum\u00adbling closure, which we do via the \nclosure operator (a quotient that blurs together observationally indistinguishable trace sets): Closure \nt . T t . T st . T s(s, s ' )(s ' ,o)t . T t . Ts(s, s)t . Ts(s, o)t . T t(s, i) . T ' (i, i) . Tt(s, \ns )u . T The unshaded rules for appeared in Brookes s original paper. To them, we add two rules concerning \nfaults. The .rst captures the fact  Denotational semantics of speci.cations [.]. . TRACE [.; .]. ([.]. \n; [.].) [. 1 .]. ([.]. 1 [.].) . .. [. . .][.]. [.] ..[x .v] [.x..][.] v [let f = F in .]. [.].[f .[F \nJ.] [F (e)]. [F ]. ([e].) . .[X .T ] [\u00b5X..]n{T : T = T, [.]. T } [X]. .(X) [(.x : p, q)]. act(.(p),.(q)) \n[{p}]. {(s, s): s . [p * true].} .{(s, i): s/. [p * true].} Procedures: [f]. .(f) [.x..]. .v. [.].[x \n.v] Figure 2. that the environment of a speci.cation might cause a fault at any time. The second re.ects \nthat faulting on the part of a speci.cation is permissive, so a speci.cation that faults after some interactions \nt can be implemented by one that continues without faulting after t. The reason (i, i) steps are important \nis to handle cases like (\u00b5X.X) 1 abort. Without the (i, i) steps, O[(\u00b5X.X)] would be empty, but the semantics \nof the composition is nonempty. The effect of including (i, i) in the closure is that every .nite pre.x \nof the behavior of a speci.cation is included in its set of traces, but with the marker (i, i) at the \nend signifying that the speci.cation was terminated early by a fault on the part of the environment. \nWith those preliminaries out of the way, the denotational se\u00admantics is straightforward. Sequential composition \nis concatena\u00adtion and parallel composition is nondeterministic interleaving. One caveat: when concatenating \ntraces, a fault on the left wipes out everything on the right, since faulting causes early termination: \nt(o, i); u = t(o, i). This rule applies to both sequential and par\u00adallel composition. Recursion is de.ned \nusing the Tarskian least\u00ad.xed point over closed sets of traces; the order on trace sets is set inclusion. \nDisjunction and existential quanti.cation are given by least upper bounds according to that ordering. \nHere, environ\u00adments . map variables x to values v, speci.cation variables X to closed sets of traces \nT , and procedure variables f to functions VAL . 2TRACE . Environments are ordered pointwise (over the \nsub\u00adset order on closed sets of traces), leading to the following lemma. Lemma 1. For each ., the function \n[.] from environments to trace sets is monotonic. We connect the denotational semantics to re.nement \nin two steps. First, we show that the denotational semantics gives the sames trace sets as the operational \nsemantics, modulo -closure: Lemma 2. If . is a closed speci.cation then O[.] = [.]\u00d8 . This lemma is proved \nseparately in each direction; the . direction goes by induction on the rules de.ning O[-], while . goes \nby induction on the structure of .. We de.ne a denotational version of re.nement, and prove that it soundly \napproximates the operational version ( adequacy ): De.nition 1. . Lden . iff for all closing ., [.]. \n. [.]. . Theorem 1 (Adequacy). If . Lden . then . Lop ..  Some laws of re.nement . L . miracle L . L \nabort .[e/x] L.x.. L . . . . L . L . . . skip; . = . = .; skip DSTL (.1 . .2); . = .1; . . .2; . DSTR \n.;(.1 . .2) = .; .1 . .; .2 STR1 .x..; . = .;(.x..) STR2 .x. (.y : p, q)L(.y : p, .x.q) FRM (.x : p, \nq)L(.x : p * r, q * r) EXT (.x : p, p)={.x.p} (p exact) IDM1 {p}; {p}={p} IDM2 {.x.p}; (.x : p, q)=(.x \n: p, q) ASM (.x : p, q . r)=(.x : p, q) ;[r] (r pure) IND CSQ1 CSQ2 .[./X] L . .x. p . p ' .x. q ' . \nqq . p (') \u00b5X.. L . .x : p ' ,q L(.x : p, q){p}L{q} NB: syntax appearing both in and outside a binder \nfor x in a re.nement (as in .x.. L .) cannot mention x. Figure 3. Proof. Suppose . Lden ..Let C be \na speci.cation context that closes . and .. By the monotonicity of the semantics, one can show that [C[.]]\u00d8 \n. [C[.]]\u00d8 . By Lemma 2, O[C[.]] . O[C[.]] . It follows that if C[.],s i then C[.],s i,andthat if C[.],s \n. then either C[.],s i or C[.],s ..  4. Re.nement: proof theory With the denotational semantics in hand, \nit is easy to prove a number of basic re.nement laws. These laws will be powerful enough to verify a \nbasic nonblocking counter (\u00a74.1), but to tackle more advanced examples we will need to employ ownership-based \nreasoning, the subject of \u00a75. We write L for axiomatic re.nement, which is justi.ed in terms of Lden, \nand we write = when the re.nement goes in both direc\u00adtions. The bene.t of the setup in the previous two \nsections is that axiomatic re.nement is a congruence which is what enables us to verify implementations \nin a stepwise, compositional manner. Many of the rules in Figure 3 are familiar. The top group comes \nfrom .rst-order and Hoare logic; we leave those rules unlabeled and use them freely. The two DST rules, \ngiving the interaction between nondeterminism and sequencing, are standard for a linear\u00adtime process \ncalculus [29]. IND is standard .xpoint induction. FRM is the frame rule from separation logic, capturing \nthe locality of actions. CSQ1 is the consequence rule of Hoare logic. Thelessfamiliarrulesarestilllargelystraightforward. \nEXTpro\u00advides an important case where actions and assertions are equiva\u00adlent: on exact predicates, which \nare satis.ed by exactly one heap, and hence are deterministic as postconditions. The STR rules allow \nus to manipulate quanti.er structure in a way reminiscent of scope extrusion in the p-calculus [25]; \nlike the DST rules, they express that the semantics is insensitive to when a nondeterministic choice \nis made. The IDM rules express the idempotence of assertions recall that the precondition of an action \nacts as a kind of assertion. ASM allows us to move a guard into or out of a postcondition when that guard \nis pure (does not use emp or .). CSQ2 tells us that as\u00adsertions are antitonic, which follows from the \npermissive nature of faulting. Theorem 2. The laws are sound: if . L . then . Lop .. Proof. We prove \nsoundness using the denotational semantics: we show that . L . implies . Lden ., which by Theorem 1 (ade\u00adquacy) \nimplies . Lop .; most laws are very easy to show this way. The only nontrivial case, IND, uses the Knaster-Tarski \n.xpoint the\u00adorem in the standard way. As a simple illustration of the proof rules, we have: Lemma 3. \nget(a, ret) de.nition =(.x : a . x, a . x . x = ret)CSQ1 L(.x : a . x, a . x)EXT L{a .-} This lemma shows \nthat we can forget (abstract away) the constraint that get places on its out-parameter, allowing it to \nvary freely. But we cannot forget that get(a, ret) faults when a is unallocated. 4.1 Example: a nonblocking \ncounter We now return to the example of an atomic counter: inc(c, ret) (.x : c . x, c . x +1 . ret = \nx)inc ' (c, ret) \u00b5X. .t.get(c, t); .b.cas(c, t, t +1,b); if b =1 then [ret = t] else X To show inc ' \nL inc, we .rst prove some useful lemmas about optimistic concurrency: Lemma 4. cas(a, t, e, b); [b =1] \n\\ ) (x = t . b =0 . a . x) def =.x : a . x, ;[b =1] . (x = t . b =1 . a . e) \\( .) (x = t . b =0 . a \n. x) ASM = .x : a . x, . b =1 . (x = t . b =1 . a . e) CSQ1 L(.x : a . x, a . e . x = t) Lemma 5. cas(a, \nt, e, b); [b =1] L{a .-}. Proof. Similar.  Corollary 1. \u00b5X..t.get(a, t); .; \u00b5X..t.get(a, t); .; .\\ \n). .b.cas(a, t, e, b); a . x, L.x : ; . if b =1 then . . a . e . x = t . else X .{a .-}; X Proof. Expand \nif, apply DSTR, and use Lemmas 4, 5. Lemma 6 (Optimism). \u00b5X..t.get(a, t); .; (.t.get(a, t); .) * ;.b.cas(a, \nt, e, b); L.t.get(a, t); .; if b =1 then . (.x : a . x, a . e . x = t) ; . else X where . * is shorthand \nfor \u00b5X.skip . .; X. Notice, too, that the scope of quanti.ers continues over line breaks. The speci.cation \non the left captures a typical optimistic, nonblocking algorithm: take a snapshot of a cell a, do some \nwork ., and update a if it has not changed; otherwise, loop. The speci.cation on the right characterizes \nthe (partial correctness) effect of the algorithm: it performs some number of unsuccessful loops, and \nthen updates a. Proof. Apply Corollary 1. At a high level, the result follows by induction, using IDM2 \nto remove assertions {a .-}. Applying Lemma 6 to inc ' (letting . be skip, which we drop), we have: \ninc ' (c, ret) 1 L (.t.get(c, t)) * ; .t.get(c, t); (.x : c . x, c . x +1 . x = t) ;[ret = t] 2 L (.t.{c \n.-}) * ; .t.{c .-}; (.x : c . x, c . x +1 . x = t) ;[ret = t] 3 L.t.{c .-}; (.x : c . x, c . x +1 . x \n= t) ;[ret = t] 4 L.t. (.x : c . x, c . x +1 . x = t) ;[ret = t] 5 L.t. (.x : c . x, c . x +1 . ret = \nx)6 L(.x : c . x, c . x +1 . ret = x) Step (1) is the application of Lemma 6. In step (2), we abstract \naway the gets (the snapshots) using Lemma 3. In (3), we remove the .rst existential, extrude the remaining \nexistential (STR1)and apply IDM2 inductively to coalesce the assertions; (4) applies IDM1 to remove the \nremaining assertion; (5) applies ASM and CSQ1;(6) removes the remaining existential. The abstraction \nof get in step (2) is reminiscent of havoc ab\u00adstraction [5], and it illustrates that the snapshots are \nnot necessary for the safety of the algorithm (though essential for liveness). Notice that we do not \ngive and did not use any rules for reason\u00ading about parallel composition. We certainly could give such \nrules (e.g., an expansion law [18]), but that would be beside the point. Our aim is to reason about concurrent \nobjects, which are de.ned by sequential methods that clients may choose to execute in paral\u00adlel. Having \nproved the re.nement, we can conclude that even for a concurrent client C[-] we have C[inc ' ] L C[inc]. \n 5. Ownership In the previous section, we were able to reason about the non\u00adblocking counter because \nit possesses a very helpful property: it works correctly regardless of interference. No matter what concur\u00adrent \nreads and writes occur to the counter, the inc ' method will only modify the counter by atomically incrementing \nit. Such an imple\u00admentation is possible because a counter can be represented in a single cell of memory, \nand so cas can be used to operate on the en\u00adtire data structure at once. For more complex data structures, \nsuch as the stack we will study in \u00a76, this will not be the case; such data structures cannot cope with \narbitrary interference. In this section, we will develop an ownership discipline that will enable reasoning \nabout (lack of) interference. We will leverage the denotational semantics to give a meaning to, and .nd \nthe conse\u00adquences of, the ownership discipline. The resulting laws give voice to the key claims of the \npaper: atomicity is relative to ownership, and interference is modularized by ownership. 5.1 The discipline \nThe ownership discipline we have in mind is tied to the notion of a concurrent object, which is given \nby a collection of methods, one of which is a constructor. For example, imagine a counter that supported \nboth incrementing and decrementing: let newcnt(ret)= new(0, ret) in let inc(e, ret)= \u00b7\u00b7\u00b7 in let dec(e, \nret)= \u00b7\u00b7\u00b7 in . Intuitively, the representation of a counter can be described by a simple predicate: \ne . x. Notice that this predicate is speci.c to a counter located at address e and with value x.Aclient \n. = .a.newcnt(a); .b.newcnt(b) that called newcnt twice would expect to be left with a subheap satisfying \na . 0 * b . 0. More complex data structures are described by more complex predicates. Consider the speci.cation \nof a thread-safe stack: let newStk(ret)= new(0, ret) in \\) e . h, let push(e, x)= .h : in .h ' .e . h \n' * h ' . (x, h)  e . h ' e . h let pop(e, ret)=.h, h ' ,x : , * h . (x, h ' ) * h . (x, h ' ) . ret \n= x in . In this representation, an instance of a stack consists of at least a memory cell s which points \nto the head of the stack (0 if the stack is empty). The contents of the stack is given by a linked list, \nwhich can be described by the following recursive predicate: list(e, E) e =0 ' '' list(e, x \u00b7 xs) .e \n.e . (x, e ) * list(e ,xs) The second parameter to list is a sequence of list items; this se\u00adquence is \nabstract in the sense that it exists only at the level of the logic, not as a value in a program. Altogether, \na stack lo\u00adcated at e with abstract contents x is described by the predicate .a. e . a * list(a, x). \nIn general, the memory belonging to an instance of a concurrent object is described by a predicate p[e, \nx] with free variables e and x.Thevariable e gives the location of the object, while x gives its abstract \nvalue. The predicate .x.p[e, x] then describes a heap that contains an object instance at e, with unknown \ncontents. We call this latter predicate the representation invariant for a concurrent object, and use \nthe metavariable I to designate such predicates. We introduce a new metavariable not just as a mnemonic, \nbut also because we restrict representation invariants to be precise.A predicate is precise if, for every \nheap s, there is at most one way to split s = s1 l s2 such that s1 satis.es the predicate. Thus, precise \npredicates serve to pick out ( fence [7]) precisely the region of memory relevant to an object instance. \nFor example, the invariant I[e]= .a..x.e . a * list(a, x) in some sense discovers the linked list associated \nwith a stack, whatever its contents may be. Our ownership discipline works as follows: a concurrent object \nis given (in the logic) a representation invariant I[e] parameterized by location e; an object instance \nat e consists of the subheap de\u00adscribed by I[e]. Clients may not access these subheaps directly, but \nmust invoke methods of the object instead. Each method is param\u00adeterized by a location e of the object \nto operate on, and is given ac\u00adcess only to the subheap described by I[e]. Since multiple methods may \nbe invoked on the same instance concurrently, this memory is shared between invocations. But because \nthe object s methods are the only code with access to this memory, we may assume that any concurrent \ninterference is due to one of these methods. When a method begins executing, its view of the heap consists \nsolely of some object instance I[e]. As it executes, the method may in addition allocate private memory, \nwhich is initially free from interference. If, however, the private memory is made reachable from the \nobject instance, so that it falls into the region described by I[e], it at that point becomes shared, \nand subject to interference. An implementation of push, for example, will .rst allocate a pri\u00advate linked \nlist node, and only later actually link it into the list. Conversely, memory unlinked from the object \ninstance becomes private to the method.  5.2 Fenced re.nement Now for the punchline: our ownership discipline \nenables partly\u00adsequential reasoning about method bodies, through fenced re.ne\u00adment I,. f . L .. As above, \nI describes a speci.c object in\u00adstance; we leave the location e implicit. Fenced re.nement says that \n. re.nes . under the assumption that all memory is either part of the shared instance described by I, \nor private. Moreover, inter\u00adference on the shared memory is bounded by the speci.cation . (the rely [12]). \nHere are the key rules:  Some laws of fenced re.nement I,. f . L . LIFT INV . L . I,. f{I}= skip I,. \nf . L . SEQL (' )(' ) I,. f(.x : p, q) ; .x : q * p,rL{I *.x.p}; .x : p * p,r STAB . L(.x : q, q) I,. \nf(.x : p, q) ; {.x.q}L(.x : p, q)  INV expresses that the representation invariant I always holds, so \nasserting it will always succeed. We check that the method itself maintains the invariant elsewhere, \nin the DATA2 rule. LIFT re.ects the fact that fenced re.nement is more permissive than standard re.nement. \nIn fenced re.nement, any data that is not part of the data struc\u00adture instance fenced by I is private, \nand hence can be reasoned about sequentially. Suppose that, at some point, we know that I *p; that is, \nour view of the heap includes both a shared object instance described by I, and some other memory described \nby p. By our ownership discipline, we know that p is private. In this case, the ex\u00adecution of an action \nlike (p, q) will not be visible to other threads, because it is operating on private memory. The SEQ \nrules permit atomicity abstraction: two sequenced atomic actions can be com\u00adbined if one of them only \noperates on private data. We give one rule, SEQL, which combines two actions when the .rst (the Left) \nis unobservable. Notice that SEQL introduces the assertion I *.x.p as a veri.cation condition for showing \nthat the data operated on by the .rst action really is private. We omit the similar SEQRrule. Finally, \nSTAB allows as assertion {q} to be removed if it is stably satis.ed after an action. That is, the predicate \nq must be established by an action, and once established, must be maintained under any interference .. \nIn addition to those rules, fenced re.nement is also nearly a congruence: if I,. f . L . then I,. f C[.] \nL C[.] for any context C that does not contain parallel composition. It is not a congruence for parallel \ncomposition because of the sequential reasoning it permits on private data. Since we use fenced re.nement \nonly to reason about the method bodies implementing a concurrent object which we assume to be sequential \nthis is all we need. To give a semantics for fenced re.nement, we want to ignore certain aspects of the \ndenotational semantics. In some cases we throw away traces: for example, traces where the environment \ndoes not obey the rely . should be discarded. A deeper issue is the dis\u00adtinction between private and \nshared memory. Because private mem\u00adory cannot be observed or interfered with by concurrent threads, we \ndo not model it with a trace. Instead, we view private memory in terms of input-output behavior, recording \nits value only at the beginning and end of a computation .. As a consequence, sequen\u00adtial reasoning (e.g. \nSEQL) is valid for steps involving only private memory. In short, the fenced semantics of method bodies \nis a hybrid between trace semantics and input-output relations. We thus de.ne: FENCEDTRACE Si \u00d7 TRACE \n\u00d7 Si A fenced trace contains three components: the initial state of private memory, a trace involving \nonly the shared memory, and the .nal state of private memory. Suppose . is a fragment of a method body. \nWe calculate its fenced traces as follows:  Fenced projection [I,. f .]. . FENCEDTRACE {} t l u . [.]. \n,u seq[I,. f .]. (fst(u),t ' , lst(u)) : t ' . rely(., I, ., t) where '' '' E l EE (o1,o1)t l (o2,o2)u \n(o1 l o2,o1 l o2)(t l u) u seq E . rely(., I, ., E) (o, o ' ) seq (o, fst(u))u seq s, . |= I (s ' , fst(t)) \n. [.]. t ' . rely(., I, ., t) s, . |= I o,. |= I '' ' (s, s )t . rely(., I, ., (s, s )t)(s, i) . rely(., \nI, ., (s, o)t) The functions fst(u) and lst(u) return the .rst and last heaps (or outcomes) that appear \nin a trace u. Fenced projection separates each trace of . into a part t dealing with the shared state \nand a part u dealing with the rest of the heap, which is presumed to be method-private. We do this by \nlifting l to traces. Notice that t and u have the same length and represent the same moves of .; we are \njust splitting the heap involved in each move into a shared part and a private part. The rely operator \nperforms several tasks at once. First, it .lters the traces t to include only those where the environment \n(1) main\u00adtains the invariant I and (2) obeys the rely .. At the same time, it checks that the speci.cation \n. maintains the invariant as well, in\u00adserting faults when it does not. One effect is that the resulting \ntrace, t ' , continually satis.es I, which means that is captures precisely the shared state; this shared \nstate is unambiguous, due to the pre\u00adcision of I. It follows that the trace u, which carries the remaining \nstate, contains exactly the private state. We therefore require u to be sequential, meaning that we .lter \nany traces where the private data appears to have been interfered with at all. Fenced projection gives \nus the fenced traces of a speci.cation. Fenced re.nement just compares these traces: Semantics of fenced \nre.nement I,. |= . L . I,. |= . L . iff . closing .. [I,. f .]. . [I,. f .]. Theorem 3. The laws of fenced \nre.nement are sound: if I,. f . L . then I,. |= . L .. Proof. The semantics of fenced re.nement was designed \nto make this theorem straightforward. Each law corresponds to one of the basic assumptions of fenced \nprojection: INV, that the invariant is satis.ed; STAB, that the rely is obeyed; SEQL, that the private \ntraces are sequential. The LIFT rule expresses that fenced projec\u00adtion is monotonic. 5.3 Enforcing ownership: \nhiding So far, fenced re.nement is just a fantasy: we have no way to deduce standard re.nements from \nfenced re.nements. To do so, we have to justify the assumptions made in fenced projection. The ownership \ndiscipline described in \u00a75.1 is the desired policy; now we turn to the mechanism for carrying it out. \nConsider the following speci.cation: let f(ret)= (emp, ret . 0) in .x.f(x); put(x, 1) Here we have a \nsimple concurrent object with a constructor, f,and no methods. Its client, however, violates our ownership \ndiscipline by directly modifying an object instance, using put. If the assump\u00adtions of fenced re.nement \nare to be met, such clients must some\u00adhow be ruled out. Our solution is to introduce an abstraction \nbarrier. We extend the language of speci.cations to include abs a.., which binds the abstraction variable \na in ., and we likewise extend heaps to a include abstract cells e . e. Each location in the heap is \neither aa concrete (.) or abstract (.), so the predicate e .-. e .- is unsatis.able. The purpose of \nabstract cells is to deny clients access to concurrent objects. Revisiting the misbehaved client above, \nwe could instead write: \\) a abs a.let f(ret)= emp, ret . 0in .x.f(x); put(x, 1) In this case, the client \nwill fault when attempting the put, because put operates on concrete cells, but f allocates an abstract \ncell. In general, the concrete representation of an object may involve multiple memory cells. When we \nintroduce an abstraction variable, we also have an opportunity to switch to a more abstract represen\u00adtation, \nas in the following rule (applicable when r[e, -] is precise): DATA1 let fi(e, x)= (.y : pi * r[e, ei],qi \n* r[e, e ' ]) in . i \\) aa L abs a. let fi(e, x)= .y : pi * e . ei,qi * e . e ' iin . As before, the \npredicate r[e, z] is meant to describe an object instance located at e with abstract contents z. Notice \nthat the client . is unconstrained, except for the implicit constraint that a cannot appear free in it. \nThe idea is that, in applying this abstraction rule, we swap a concrete version of a concurrent object \nfor an abstract one. Cases where the client might have accessed a concrete object instance directly will, \nin the context of the abstract object, result in a fault. Since a faulting speci.cation is permissive \n(\u00a73), this means that behaviors where the client performed such an access are irrelevant for this re.nement; \nthey are masked out by the fault. On the other hand, to ultimately verify that, say, abs a.. L(p, q),it \nwill be necessary to show that the client is well-behaved as long as the precondition p is satis.ed. \nIn short: when we introduce hiding, we can assume the client is well-behaved; when we verify a full program, \nwe must check this assumption. We de.ne [abs a..]. as follows: [abs a..]. {s : s la t . [.]. ,t seq, \nfst(t)= \u00d8} The notation s la t denotes s l t, but is only de.ned when no heap aa in s contains . cells, \nand every heap in t consists only of . cells (so it is not commutative!). Thus, as in fenced re.nement, \nwe are isolating in trace t the contents of the heap speci.c to an abstract variable. The operator works \nby projecting away the abstract re\u00adsources, making them unavailable any external environment. In ad\u00addition, \nit only includes those traces where no interference on those resources took place, since such interference \nis impossible due to the projection. While abs hides abstract resources, we also need a way to realize \nan abstract data structure by a concrete one. We do this by extending the closure operator , as follows: \ns . T t seq s; s ' . T t seq fst(t)= \u00d8 fst(t)= \u00d8 lst(t) . s s l t . T (s l t); (s, i) . T The .rst \nrule says that an implementation may always allocate more memory than its speci.cation requires. The \nconstraint that fst(u)= \u00d8 requires the memory is freshly allocated. Notice that the rule also requires \nthe allocated memory to be free from external interference (t seq). Why is the freshly-allocated memory \nfree from external inter\u00adference? The key insight here is that the memory is not present in DATA2 r[e, \n-] precise pi pure r[e, -], ( .i) f .i L .i \\ .i L(.y : r[e, y],r[e, ei] . pi)a ) let f (e)= (emp,r[e, \ne]) in L abs a.let f(e)= emp,e . ein \\ ) let gi(e, x)= .i in . aa let gi(e, x)= .y : e . y, e . ei . \npiin . Figure 4. Rely/guarantee reasoning one possible behavior, s, of the speci.cation T . Therefore, \nthe en\u00advironment cannot assume the memory will be present, and, indeed, if it tries to read or modify \nthe memory and is interleaved with s rather than s l t, it will fault. Although we know that the environ\u00adment \nwill fault if it attempts to interfere, the structure of transition trace semantics is such that arbitrary \ninterference is always con\u00adsidered possible. Thus, we include a second new closure rule that says, in \nthe presence of interference by the environment (which will ultimately result in a fault), the implementation \nis allowed to do anything (and in particular, may itself immediately fault). Of course, we cannot extend \n cavalierly: we must ensure that adequacy still holds. We sketch the proof that it does in \u00a77. The net \neffect of these de.nitions is to protect abstracted data from its client and environment, while allowing \nit to be imple\u00admented in a concrete way. As we mentioned above, when we in\u00adtroduce an abstraction through \na rule like DATA1, we cause an ill\u00adbehaved client to fault where it would have blithely accessed con\u00adcrete \ndata. Ultimately, when we verify a whole program including the client we show it free from faults by \nshowing that it re.nes some atomic action like (\u00d8,p). To do this we must eventually ap\u00adply abstraction \nelimination: abs a.. L ., which by convention requires that a not appear in ..  5.4 Re.nements from \nfenced re.nements: DATA2 Finally, we have the DATA2 rule in Figure 4, which allows us to derive standard \nre.nements from fenced re.nements, ensuring that the assumed ownership discipline is actually followed; \nownership is in the eye of the asserter [23]. In DATA2, we are working with a concurrent object with \ncon\u00adstructor f and methods gi. The predicate r[e, y] describes a con\u00adcrete object instance at location \ne with abstract contents y.Asin DATA1, the abstracted version of the object works on abstract cells a \ne . y instead. We introduce .i as a midpoint between .i and the speci.cation (.y : r[e, y],r[e, ei] . \npi) because we want to keep the rely ..i as simple as possible. In particular, methods usually operate \non small portions of the object, while r[e, y] refers to the entire object instance. The de.nition of \nabs ensures that the client or environment of an object cannot interfere with it. But fenced re.nement \nalso assumes that (1) methods do not violate the invariant r[e, -] and (2) methods do not violate the \nrely ..i. The invariant is maintained because each method body .i is a fenced re.nement (.y : r[e, y],r[e, \nei] . pi), which clearly maintains the invariant.  The rely is never violated because each method body \n.i is a fenced re.nement of .i, and the rely permits the behavior of any of the .i s.  Akey featureof \nDATA2 is its provision for modular and dy\u00adnamic rely/guarantee reasoning. It is modular because we have \nisolated the memory involved (to r[e, -]) and the code involved (each .i) we do not constrain the clients \n., nor the contexts in which the data re.nement holds. It is dynamic because it encom\u00adpasses arbitrary \nallocation of new data structure instances we get rely/guarantee reasoning for each individual instance, \neven though we do not know how many instances the client . will allocate. As given, DATA2 only applies \nto methods whose action on the shared state is given in a single atomic step. The rule can easily be \nextended to allow methods which also take an arbitrary number of steps re.ning (.y : r[e, y],r[e, y]), \nwhich make internal adjust\u00adments to the object instance (often known as helping [10]) but look like skip \nto the client. We will not need this for our case study below, but it is necessary for related data structures \nsuch as queues. The soundness proof for DATA2 is sketched in \u00a77.   6. Case study: Treiber s nonblocking \nstack Using fenced re.nement, we will be able to verify a version of Treiber s nonblocking stack [26]: \nnewStk(ret)= new(0, ret) push(e, x)= .n.new((x, 0),n); \u00b5X. .t.get(e, t); put2(n, t); .b.cas(e, t, n, \nb); if b =0 then X pop(e, ret)= \u00b5X. .t.get(e, t); {t =0}; .n.get2(t, n); .b.cas(e, t, n, b); if b =0 \nthen X else get1(t, ret) The speci.cations geti and puti operate on the ith component of pairs, e.g., \nput2(a, x) (.y : a . (y, -),a . (y, x)).Wehave simpli.ed pop so that it asserts that the stack is nonempty. \nStacks provide two new challenges compared to counters. First, the loop in push modi.es the heap every \ntime it executes, by calling put2, rather than just at a successful cas; this makes atomicity non\u00adtrivial \nto show. Second, pop has a potential ABA problem [10]: it assumes that if the head pointer e is unchanged \n(i.e. equal to t at the cas), then the tail of that cell is unchanged (i.e. equal to n at the cas). Intuitively \nthis assumption is justi.ed because stack cells are never changed or deallocated once they are introduced;2 \nwe must make such an argument within the logic. First, we introduce the following procedures: getHd(e, \nret)= (.x : e . x, e . x . ret = x)pushHd(e, t, n)= (.x : e . x, e . n . x = t)popHd(e, t, n)= (.x : \ne . x, e . n . x = t) We apply Lemma 6 (Optimism) to push and pop:3 push(e, x) L.n.new((x, 0),n); (.t.getHd(e, \nt); put2(n, t)) * ; .t.getHd(e, t); put2(n, t); pushHd(e, t, n) 2 We assume garbage collection here, \nbut we can also verify a stack that manages its own memory using hazard pointers [16]. 3 Technically, \nfor pop, we need a slight variant of the lemma allowing the existential .n to scope over the cas.  pop(e, \nret) L (.t.getHd(e, t); {t =0}; .n.get2(t, n)) * ; .t.getHd(e, t); {t =0}; .n.get2(t, n); popHd(e, t, \nn); get1(t, ret) Notice that pop, after recording a pointer t to the current head node and checking that \nit is non-null, attempts to read the tail of that node (using get2). In a sequential setting this would \nbe unproblematic; in our concurrent setting, we must worry about interference. What if, between taking \nthe snapshot t and getting its tail, the node t was popped from the stack, or worse, deallocated? Clearly \nthe push and pop methods will give no such interference, but we must .nd an appropriate representation \ninvariant I and interference description . to explain that to the logic. push(e, x) 1 L.n.new((x, 0),n); \n(.t.getHd(e, t); put2(n, t)) * ; .t.getHd(e, t); put2(n, t); pushHd(e, t, n) 2 L.n.new((x, 0),n); (.t.put2(n, \nt)) * ; .t.put2(n, t); pushHd(e, t, n) 3 L.n.new((x, -),n); .t.put2(n, t); pushHd(e, t, n) 4 L.n..t.new((x, \n-),n); put2(n, t); pushHd(e, t, n) 5 L.n..t.new((x, t),n); pushHd(e, t, n) aa . (z, A), . (n, A) * n \n. (x, t) . z = t  \\ e 6 L.n..t. .z, A : e )) \\ e ..x, x1,x2,A :  7. Soundness of data abstraction \\ \nIt turns out to be slightly tricky to formulate an appropriate a a . (z, A), .n..t.e . (n, A) * n . (x, \nt) . z = t e representation invariant, because part of what we want to assert 7 L.z, A : that cells, \neven when popped, are neither altered nor deallocated \\ a a e . (z, A), can involve memory that is no \nlonger reachable from the head of 8 L.z, A : the stack. In order to state the invariant, we need some \nway of .n.e . (n, A) * n . (x, z) ) remembering the addresses of cells which used to be part of the stack, \nbut no longer are. To do this, we will introduce an internal Figure 5. High-level proof of pushabstract \nvalue a which caries a ghost parameter A,using DATA1: . (x, A), . (x, A) . ret = x . (x, A), . (n, \nA) . x = t . (x, A), . (n, x \u00b7 A) . x = t  aaaaaa \\ )) e 6.2 Proving pop getHd(e, ret)= .x, A : \ne We do not give the full proof of pop, but instead we show one \\ e important step in very careful \ndetail (Figure 6). Recall that in pop, pushHd(e, t, n)= .x, A : we take two snapshots every time we \nloop: one of the address of the e \\ ) current head node, and one of its tail. We will show that, although \ne popHd(e, t, n)= .x, A : these snapshots take place at distinct points in time, they may as e well \nhave happened in a single atomic step. The justi.cation is that In order to apply DATA1, we have used \nthe predicate p[e, (x, A)] = e . x. Notice that the parameter A does not appear in the concrete predicate. \nThe idea is that the abstract A is a sequence of addresses of popped cells, while the concrete representation \nof A is simply nothing! We can now give a representation invariant I for stacks, along with a bound on \ninterference, .: no interference could change the tail between the two steps. We have labeled each step \nwith the rule it applies, except for the step (*), which applies the derived rule (.x : p, q)L(.x : p \n. r, q . r) for r pure. This rule follows from the frame ruleand CSQ1, because for pure r we have p*(r.emp) \n.. p.r. The proof mostly consists of structural steps which we give for illustrative purposes; in practice, \none would reason with more pow\u00aderful derived rules. These steps serve to introduce enough names alist.**.(n,A) \ncells(A)()x.n,x I[e] .n..A. e and frame to actually say what we want to say about the snapshots. The \n(*) step strengthens the precondition of the second snapshot cells(E) emp by asserting that it has a \n.xed value t2.The STAB step actually cells(x \u00b7 A) x .- * cells(A) demonstrates that the tail of t has \na stable value; strictly speak\u00ad ing, it requires a use of CSQ2 to strengthen the assertion so that it \n matches the postcondition. a a a e . (x, A), . .x, A : .n. e . (n, A) * n . (-,x) . (x, A) * x . (x1,x2), \n\\ ) ) a a . (x2,x \u00b7 A) * x . (x1,x2) Notice that . describes interference only in terms of the effect \non the head of the stack. Implicitly (by framing) this means that the rest of the stack described by \nI including the heap cells given by A remain invariant under interference.  6.1 Proving push We verify \npush using the re.nements in Figure 5, which are fenced by I and .. Step (1) just restates what we have \ndone already. In . (-, -)} (as e step (2), we abstract getHd into the assertion {e  7.1 The closure \noperator The operator acts as a homomorphism on the space of trace sets: if T . U then T . U , but \nnot conversely. To ensure that adequacy is preserved, we must show that this homomorphism acts properly \nas a quotient. It suf.ces to show that has two properties (in addition to being a closure operator): \nLemma 7 (Strength [13]). For each semantic operator .,wehave T . U . (T . U) . Lemma 8 (Preservation \nof observation). 1. (s, i) . T iff (s, i) . T in \u00a74.1), and then apply INV to abstract these assertions \nto skip (because they are implied by the invariant). In steps (3) and (5) we use SEQL to merge the calls \nto new and put2 into a single atomic action, new; this introduces an assertion {I * emp}, again implied \nby the invariant. Step (4) is just STR1. Step (6) again applies SEQL, again producing a trivial assertion \nwhich we remove with INV. Step (7) applies STR2,and step(8)isby CSQ1. 2. .s ' .(s, s ' ) . T iff .s ' \n.(s, s ' ) . T Applying induction, strength says that if we apply to each clause of our denotational \nsemantics, the result will be no larger than had we calculated the semantics without , and only applied \nit at the top level. We then use preservation of observation to tell us that .t.getHd(e, t); .n.get2(t, \nn); . \\) aa defn =.t. .x, A : e . (x, A),e . (x, A) . t = x; .n. (.y1,y2 : t . (y1,y2),t . (y1,y2) . \ny2 = n) ; . FRM L.t. (.x, x1,x2,A : p, p . t = x) ; .n. (.y1,y2 : t . (y1,y2),t . (y1,y2) . y2 = n) ; \n. CSQ1 L.t. (.x, x1,x2,A : p, p . t = x . x2 = x2) ; .n. (.y1,y2 : t . (y1,y2),t . (y1,y2) . y2 = n) \n; . . intr L.t..t2. (.x, x1,x2,A : p, p . t = x . t2 = x2) ; .n. (.y1,y2 : t . (y1,y2),t . (y1,y2) . \ny2 = n) ; . (*) L.t..t2. (.x, x1,x2,A : p, p . t = x . t2 = x2) ; .n. (.y1,y2 : t . (y1,y2) . y2 = t2,t \n. (y1,y2) . y2 = n . y2 = t2) ; . CSQ1 L.t..t2. (.x, x1,x2,A : p, p . t = x . t2 = x2) ; .n. (.y1,y2 \n: t . (y1,y2) . y2 = t2,t . (y1,y2) . y2 = t2 . t2 = n) ; . ASM =.t..t2. (.x, x1,x2,A : p, p . t = x \n. t2 = x2) ; .n. (.y1,y2 : t . (y1,y2) . y2 = t2,t . (y1,y2) . y2 = t2) ;[t2 = n]; . EXT =.t..t2. (.x, \nx1,x2,A : p, p . t = x . t2 = x2) ; .n.{t . (-,t2)};[t2 = n]; . STR1 =.t..t2..n. (.x, x1,x2,A : p, p \n. t = x . t2 = x2) ; {t . (-,t2)};[t2 = n]; . STAB =.t..t2..n. (.x, x1,x2,A : p, p . t = x . t2 = x2) \n;[t2 = n]; . Asm =.t..t2..n. (.x, x1,x2,A : p, p . t = x . t2 = x2 . t2 = n) ; . CSQ1 L.t..t2..n. (.x, \nx1,x2,A : p, p . t = x . n = x2) ; . a . L.t..n. (.x, x1,x2,A : p, p . t = x . n = x2) ; . where pe \n. (x, A) * x . (x1,x2) Figure 6. Detailed atomicity abstraction for pop applying the top-level to a \nclosed speci.cation in context does not change the basic observables we are concerned with (\u00a73): normal \ntermination and faulting termination.  7.2 DATA2 As with the other re.nement laws, we prove DATA2 by \nmeans of the denotational model. However, unlike the other laws, this proof is nontrivial. In order to \nprove the law, we .rst show that if the hypotheses of the law hold, then let f(e)= (emp,r[e, e]) in let \ngi(e, x)= .i in . simulates \\) a let f(e)= emp,e . (e)in \\) aa let gi(e, x)= .y : e . (y),e . (ei) . \npiin . Simulation relates the concrete traces of the .rst speci.cation to the abstract traces of the \nsecond. It is a generalization of fenced re.nement: instead of dealing with a single object instance, \nwe must track the arbitrary instances that may arise as a speci.cation executes. For each object instance, \nwe assume that the environment obeys the rely. We prove the simulation result by induction on the structure \nof the client .. The noninductive cases include method calls, which interact with the concrete objects \nin known ways, and client actions, which cannot interact with the concrete objects. For the latter, we \nuse the fact that in the second speci.cation, the concrete objects do not exist at all they are held \nabstract. Thus, if the client (which does not mention a) attempts to interact with them, it will fault, \nwhich as usual is permissive. For inductive cases, we are assured inductively that the client itself \nnever breaks the rely. This simulation tells us that neither the methods nor the client can break the \nrely condition. To prove DATA2 we introduce abs, which further guarantees that the environment will not \ninterfere at all. An important lemma for showing simulation is locality: De.nition 2. T is local if, \nwhenever t(s1 l s2,s ' )u . T either s ' = s1 ' l s2 and t(s1,s1' )u . T or  t(s1, i) . T .  Lemma \n9 (Locality). For every ., .,theset [.]. is local (assum\u00ading we take the .xpoint \u00b5 over only local trace \nsets). Locality captures the idea that if, at some point, a speci.cation is given fewer resources to \nexecute with it will either fault, or it did not need those resources (so they are part of the frame \nfor that step). We use this lemma when reasoning about steps that the client takes: we know that if we \nremove the concrete object instances, either the client would fault (and hence was ill-behaved) or else \ndid not modify those instances. The details of this and other proofs can be found online at http://www.ccs.neu.edu/home/turon/sepref/ \n  8. Evaluation and related work In the last several years, there has been stunning progress in the \nformal veri.cation of .ne-grained concurrent data structures [4, 6, 9, 27], giving logics appropriate \nfor hand veri.cation as well as automated veri.cation. We have sought in this work to clarify and consolidate \nthis literature the linchpin for us being the connection between ownership, atomicity, and interference. \nOur main contri\u00adbution lies in making this connection, formalizing it with a new se\u00admantics for separation \nlogic, and formulating proof rules based on the idea. While the logic resulting from our work offers \nsome sig\u00adni.cant new features, more experience using it is required before its practicality or applicability \ncan be compared to other approaches. The most closely related work is Concurrent Abstract Pred\u00adicates, \n(CAP, [4]), a recent paper that also seeks to modularize reasoning about interference by using ownership \nand separation logic. CAP takes a radically different approach toward atomicity: rather than proving \nlinearizability or re.nement, in CAP one al\u00adways uses self-stable predicates which are invariant under \ninternal interference thus sidestepping atomicity issues entirely. By con\u00adtrast, we have focused on the \nsemantic issues related to both atom\u00adicity and interference, and have shown both in terms of semantics \nand proof rules how these issues interact. It should be possible to apply our semantic insights to explain \nCAP in terms of contextual re.nement as we have done. Separation logic has also been used to localize \nrely/guarantee reasoning in Feng s work [7], from which we borrow the fence terminology. However, it \ndoes not appear possible to use that tech\u00adnique to both localize reasoning about interference to a group \nof methods and also let clients make use of the methods. The abstract predicates in CAP are related \nto Bierman and Parkinson s work [24] investigating data abstraction in a sequen\u00adtial separation logic \nsetting. In particular, Bierman and Parkinson use abstract predicates whose de.nition is known to the \ndata struc\u00adture implementation, but opaque to the client a form of second\u00adorder existential quanti.cation. \nThere is clearly a close relation\u00adship to our abstract resources, but we do not de.ne abs in terms of \nsecond-order quanti.cation, because we need the ability to in\u00adtroduce faults and require sequentiality. \nThe connections between these approaches certainly merits further investigation. The basic form of our \ncalculus clearly bears resemblance to Elmas et al. s calculus of atomic actions [5, 6]. The key idea \nin that work is to combine Lipton s technique of reduction [14] for enlarging the grain of atomicity, \nwith abstraction (e.g. weakening a Hoare triple) on atomic actions. The authors demonstrate that the \ncombination of techniques is extremely powerful; they were able to automate their logic and use it to \nverify a signi.cant number of data structures and other programs. A signi.cant difference between their \ncalculus and ours is what re.nement entails, semantically. For them, re.nement is ultimately about the \ninput-output relation of a program, where for us, it is about the reactive, trace-based semantics. The \ndistinction comes down to this: is re.nement a congruence for parallel composition? For us it is, and \nthis means that our system as a whole is compositional. We also demonstrate that reduction is not necessary \nfor atomicity re.nement, at least for the class of examples we have considered; we instead use ownership-based \nreasoning. Finally, aside from these points, we have shown how to incorporate separation logic into a \ncalculus of atomic actions, enabling cleaner reasoning about the heap. A signi.cant departure in our \nwork is that we have dispensed with linearizability, which is usually taken as the basic correctness \ncondition for the data structures we are studying [11]. Here we are in.uenced by Filipovi\u00b4c et al. [8], \nwho point out that program\u00admers expect that the behavior of their program does not change whether they \nuse experts data structures or less-optimized but obviously-correct data structures. The authors go on \nto show that, if we take re.nement as our basic goal, we can view linearizabil\u00adity as a sound (and sometimes \ncomplete) proof technique. But im\u00adplicit in their proof of soundness and indeed in the de.nition of linearizability \nis the assumption that the heap data associated with data structures is never interfered with by clients. \nIn a set\u00adting where clients are given pointers into those data structures, this is an assumption that \nshould be checked. In contrast, we are able to give a comprehensive model including both data structures \nand their clients, and make explicit assumptions about ownership. Our use of separation logic and rely/guarantee \nclearly derives from Vafeiadis et al. s work [28], especially Vafeiadis s ground\u00adbreaking thesis [27]. \nIn that line of work, it was shown how to combine separation logic and rely/guarantee reasoning, which \npro\u00advided a basis for verifying .ne-grained concurrent data structures. While their logic for proving \nHoare triples was proved sound, no formal connection to linearizability or re.nement was made; there \nis only an implicit methodology for proving certain Hoare triples about data structures and concluding \nthat those data struc\u00adtures are linearizable. We show how to make that methodology ex\u00adplicit, and moreover \ncompositional, by tying it to data abstraction. As a byproduct, we get a modularized account of rely/guarantee. \nWe also eliminate any need for explicit linearization points (which sometimes require prophecy variables) \nor annotations separating shared from private resources. Finally, it should be noted that our semantics \nowes a debt both to Brookes [2] and to Calcagno et al. [3]. Acknowledgements Thanks to Dan Brown, Claudio \nRusso and Sam Tobin-Hochstadt for feedback and discussions. The .rst au\u00adthor was supported by a grant \nfrom Microsoft Research.  References [1] R. J. Back and J. von Wright. Re.nement calculus: a systematic \nintroduction. Springer, 1998. [2] S. Brookes. Full abstraction for a shared variable parallel language. \nInformation and Computation, 127(2):145 163, 1996. [3] C. Calcagno, P. W. O Hearn, and H. Yang. Local \naction and abstract separation logic. In LICS, pages 366 378. IEEE Computer Society, 2007. [4] T. Dinsdale-Young, \nM. Dodds, P. Gardner, M. Parkinson, and V. Vafeiadis. Concurrent abstract predicates. In ECOOP, June \n2010. [5] T. Elmas, S. Qadeer, and S. Tasiran. A calculus of atomic actions. In POPL, pages 2 15. ACM, \n2009. [6] T. Elmas, S. Qadeer, A. Sezgin, O. Subasi, and S. Tasiran. Simplifying linearizability proofs \nwith reduction and abstraction. In TACAS, pages 296 311. Springer, 2010. [7] X. Feng. Local rely-guarantee \nreasoning. In POPL, pages 315 327. ACM, 2009. [8] I. Filipovi\u00b4c, P. O Hearn, N. Rinetzky, and H. Yang. \nAbstraction for concurrent objects. In ESOP, pages 252 266. Springer, 2009. [9] L. Groves. Reasoning \nabout nonblocking concurrency. JUCS, 15(1): 72 111, 2009. [10] M. Herlihy and N. Shavit. The Art of \nMultiprocessor Programming. Morgan Kaufmann, 2008. [11] M. P. Herlihy and J. M. Wing. Linearizability: \na correctness condition for concurrent objects. TOPLAS, 12(3):463 492, 1990. [12] C. B. Jones. Tentative \nsteps toward a development method for inter\u00adfering programs. TOPLAS, 5(4):596 619, 1983. [13] A. Kock. \nStrong functors and monoidal monads. Archiv der Mathe\u00admatik, 23:113 120, 1971. ISSN 1. [14] R. J. Lipton. \nReduction: a method of proving properties of parallel programs. Commun. ACM, 18(12):717 721, 1975. [15] \nB. Liskov and S. Zilles. Programming with abstract data types. In Symposium on Very high level languages, \npages 50 59. ACM, 1974. [16] M. M. Michael. Hazard pointers: Safe memory reclamation for lock\u00adfree objects. \nIEEE Transactions on Parallel and Distributed Systems, 15:491 504, 2004. ISSN 1045-9219. [17] M. M. Michael \nand M. L. Scott. Nonblocking algorithms and preemption-safe locking on multiprogrammed shared memory \nmul\u00adtiprocessors. J. Parallel Distrib. Comput., 51(1):1 26, 1998. [18] R. Milner. A Calculus of Communicating \nSystems. Springer-Verlag New York, Inc., 1982. [19] J. C. Mitchell and G. D. Plotkin. Abstract types \nhave existential type. TOPLAS, 10(3):470 502, 1988. [20] M. Moir and N. Shavit. Concurrent data structures. \nIn Handbook of Data Structures and Applications, D. Metha and S. Sahni Editors, pages 47 14 47 30, 2007. \nChapman and Hall/CRC Press. [21] C. Morgan and T. Vickers. On the re.nement calculus. Springer, 1993. \n[22] J. H. Morris, Jr. Protection in programming languages. CACM, 16(1): 15 21, 1973. [23] P. W. O Hearn. \nResources, concurrency, and local reasoning. Theor. Comput. Sci., 375(1-3):271 307, 2007. [24] M. Parkinson \nand G. Bierman. Separation logic and abstraction. POPL, 40(1):247 258, 2005. [25] D. Sangiorgi and D. \nWalker. Pi-Calculus: A Theory of Mobile Pro\u00adcesses. Cambridge University Press, 2001. [26] R. K. Treiber. \nSystems programming: coping with parallelism. Tech\u00adnical report, Almaden Research Center, 1986. [27] \nV. Vafeiadis. Modular .ne-grained concurrency veri.cation.PhD thesis, University of Cambridge, 2008. \n[28] V. Vafeiadis and M. Parkinson. A marriage of rely/guarantee and separation logic. In CONCUR, pages \n256 271. Springer, 2007. [29] R. J. van Glabbeek. The linear time -branching time spectrum. In CONCUR, \npages 278 297. Springer, 1990.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Fine-grained concurrent data structures are crucial for gaining performance from multiprocessing, but their design is a subtle art. Recent literature has made large strides in verifying these data structures, using either atomicity refinement or separation logic with rely-guarantee reasoning. In this paper we show how the ownership discipline of separation logic can be used to enable atomicity refinement, and we develop a new rely-guarantee method that is localized to the definition of a data structure. We present the first semantics of separation logic that is sensitive to atomicity, and show how to control this sensitivity through ownership. The result is a logic that enables compositional reasoning about atomicity and interference, even for programs that use fine-grained synchronization and dynamic memory allocation.</p>", "authors": [{"name": "Aaron Joseph Turon", "author_profile_id": "81418594363", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2509610", "email_address": "turon@ccs.neu.edu", "orcid_id": ""}, {"name": "Mitchell Wand", "author_profile_id": "81100072594", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2509611", "email_address": "wand@ccs.neu.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926415", "year": "2011", "article_id": "1926415", "conference": "POPL", "title": "A separation logic for refining concurrent objects", "url": "http://dl.acm.org/citation.cfm?id=1926415"}