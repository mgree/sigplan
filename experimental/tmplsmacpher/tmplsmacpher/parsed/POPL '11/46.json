{"article_publication_date": "01-26-2011", "fulltext": "\n The Essence of Compiling with Traces Shu-yu Guo Jens Palsberg UCLA Computer Science Department University \nof California, Los Angeles, USA {shu,palsberg}@cs.ucla.edu Abstract The technique of trace-based just-in-time \ncompilation was intro\u00adduced by Bala et al. and was further developed by Gal et al. It currently enjoys \nsuccess in Mozilla Firefox s JavaScript engine. A trace-based JIT compiler leverages run-time pro.ling \nto optimize frequently-executed paths while enabling the optimized code to bail out to the original code \nwhen the path has been invalidated. This optimization strategy differs from those of other JIT compilers \nand opens the question of which trace optimizations are sound. In this paper we present a framework for \nreasoning about the sound\u00adness of trace optimizations, and we show that some traditional opti\u00admization \ntechniques are sound when used in a trace compiler while others are unsound. The converse is also true: \nsome trace optimiza\u00adtions are sound when used in a traditional compiler while others are unsound. So, \ntraditional and trace optimizations form incompa\u00adrable sets. Our setting is an imperative calculus for \nwhich tracing is explicitly spelled out in the semantics. We de.ne optimization soundness via a notion \nof bisimulation, and we show that sound optimizations lead to con.uence and determinacy of stores. Categories \nand Subject Descriptors D.2.4 [Program Veri.ca\u00adtion]: Correctness proofs, formal methods; D.3.4 [Processors]: \nCompilers; F.3.2 [Semantics of Programming Languages]: Oper\u00adational semantics General Terms Languages, \nTheory Keywords just-in-time compilation, compiler correctness, bisim\u00adulation 1. Introduction With the \nadvent of Web 2.0 , the web browser has become a plat\u00adform that delivers rich interactive applications. \nThe technology cen\u00adtral to this transformation of the web browser is JavaScript. Java\u00adScipt s dynamic \nnature has since then become a performance bot\u00adtleneck. The performance of dynamic languages is much \nworse than statically typed languages, and JavaScript is no exception. Moreover, traditional just-in-time \n(JIT) compilation techniques de\u00adsigned for static, typed languages are ill-.tted for JavaScript. The \nwork of Bala et al. [1] was adapted as a novel JIT compi\u00adlation technique called trace compilation [2 \n4, 7, 8]. A trace-based JIT compiler uses run-time pro.ling to approximate the hot exe- Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 11, January \n26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 \n cution paths (loops) in the program and compiles only those paths [4, 7, 8]. The rarely executed bits \nof code are interpreted. The idea is quite intuitive: if there is a repeatedly executed section of the \ncode, that section should be top priority for compiling to native code. For example, a micro-blogging \nweb application might take many rows of data and transform them into a news feed format. This loop would \nbe where the program spends the majority of its time; a trace-enabled JIT detects that this loop is a \nhot execution path and compiles it to native code. Tracing JIT compilers are amenable to JavaScript and \nenjoy their greatest success in Mozilla Firefox s JavaScript engine (Trace-Monkey). It is available in \nversions 3.5 and greater, and Mozilla metrics report that approximately 94 million people in the world \nare using the tracing JIT [13].1 Tracing JIT compilers differ greatly in technique from many other JIT \ntechniques. It opens the following question: Which trace optimizations are sound? We distill the essence \nof trace compilation to a simple impera\u00adtive calculus with an operational semantics. This allows us to \nfor\u00admally investigate notions of correctness of trace-based JIT com\u00adpilers and the properties that trace \noptimizations must satisfy to be sound. We present a bisimulation-based soundness criterion for trace \noptimizations, and we prove a determinism theorem: whether one traces or not, the .nal store will be \nthe same. Our framework is modular in two ways. First, an optimization designer needs only prove that \na given optimization satis.es our correctness criterion; the determinism theorem then follows. Sec\u00adond, \nthe composition of two sound optimizations is itself sound. We leverage the .rst kind of modularity to \neasily prove soundness of the folding of free loop variables and dead branch elimination. Proving optimizations \nunsound is equally simple. We show that dead store elimination is unsound with an easy-to-check counterex\u00adample. \nReaders can easily proceed like we did to prove additional trace optimizations sound. Our proof of the \ndeterminism theorem has the following coarse steps. First we prove that an unoptimized, recorded trace \nof the loop is behaviorally correct , or bisimilar, to the original loop. We then prove that the original \nprogram with the new trace stitched in place of the old loop is bisimilar to the original program. This \nthen sets the stage for sound optimizations: sound optimizations are those that do not invalidate this \nbehavioral correctness guarantee had from bisimilarity. Finally, we put the pieces together and prove \ncon.uence and determinacy of stores via a diamond lemma and a strip lemma. Our framework shows, surprisingly, \nthat traditional whole\u00adfunction optimizations and tracing JIT optimizations do not stand in a subset \nrelation in either direction. In one direction, it is clear 1 The exact average of daily usage from January \n1, 2010 until July 12, 2010 of versions 3.5, 3.6, 3.7, and 4.0 is 93,977,941. that traditional optimizations \nare not subsumed by trace optimiza\u00adtions. Informally, trace optimizations are not obliged to be correct \nfor all possible executions and contexts. They are obliged to be correct only for a particular execution \nand a particular context. For instance, in the paper we prove folding of variables that are not assigned \nto be sound for tracing but unsound in general in a tra\u00additional setting. In the other direction, trace \noptimizations are also not subsumed by traditional optimizations. The reason for this is more subtle: \nthe domain of trace optimizations is restricted to only the trace. The code surrounding the trace is \nunavailable to the op\u00adtimizer. Traditional optimizations, on the other hand, are privy to both the pre.x \nand the suf.x of the trace in that their domain is the entire procedure. In other words, those optimizations \ncan prove properties on entire procedures while trace optimizations cannot. For one, whole-function optimizations \nknow that their local vari\u00adables are dead after the function exits. Trace optimizations cannot make the \nsame assumption about their local variables after the trace exits. In the paper we show dead store elimination \nto be unsound as a trace optimization. To expand upon the incomparability of the two sets of opti\u00admizations, \nit is illuminating to spell out the differences between our work and recent prominent works in compiler \ncorrectness [11, 12]. The program equivalence condition (Lemma 1) found in Lacey et al. [11] basically \nstates classical bisimilarity as the condition un\u00adder which to judge the correctness of optimizations. \nAn optimized program must be bisimilar to the original and their respective .nal stores must contain \nthe same values for all variables. In their frame\u00adwork, optimizations are formulated as rewrite rules \nwith side con\u00additions expressed in temporal logic. Despite this difference, we can make the following \nfruitful comparisons. We write m n for s to mean that the program m is bisimilar to n when their initial \nstores are s. Suppose there is an optimization function O. Their correctness criterion describes traditional \noptimizations and is ex\u00adtensional: for all p, p O(p) for all stores. Our correctness crite\u00adrion describes \ntracing optimizations and is intensional. Suppose the program p decomposes into two components, w (the \ntraced loop) and k (the rest of the program). The criterion is then: for all w, k, s, wk O(w, s) k for \ns. Note how our optimization function takes a state s in addition to a program to produce an optimized \nprogram guaranteed to be correct when the initial store is .xed to be that store and the rest of the \nprogram is .xed to be k. This succinctly captures that trace optimizations may not be straightforwardly \nused in a classical setting. The correctness criterion in Chambers et al. [12] raises yet more differences \nbetween classical optimization soundness and trace op\u00adtimization soundness. At the heart of their formulation \nof sound\u00adness is contextual equivalence. However, note that we have said that trace optimization correctness \nis intensionalized to a particular computation suf.x. This necessarily precludes trace optimization correctness \nfrom being contextual equivalence. Classical optimiza\u00adtions speculate about the behavior of a program \nto substitute op\u00adtimized portions for the original portions before execution. Trace optimizations, on \nthe other hand, know exactly the behavior for which they need to optimize and substitute optimized portions \ndur\u00ading execution. This departure also highlights that the input to trace optimizations is mercurial: \nwe do not know a priori what loops are hot. The input to classical optimization on the other hand is \n.xed. In Chambers et al. and Lacey et al. [11, 12], this input is the en\u00adtire program. As mentioned before, \nthe domains of the two kinds of optimizations are simply different. The remainder of this paper is organized \nas follows. In section 2 we introduce our language, its operational semantics, and discuss certain properties \nof compiling with traces. In section 3 we show that the operational semantics of our language is correct \nup to weak bisimulation and relate the results to con.uence. In section 4 we explore various optimizations, \nboth provably correct and provably incorrect, pluggable into the framework. In section 5 we discuss related \nwork. Section 6 concludes. 2. Compiling with Traces What is essential to the trace compilation technique? \nWhat features must our calculus contain? At its core, it is a method of compiling often-executed loops. \nWe must therefore have loops. More specif\u00adically, it is a technique of recording loop bodies at run-time \nand optimizing them. The second essential part must thus be the ability to record execution. What is \noptimized, then, is not the text of the loop but a run-time execution path through the loop. In other \nwords, we are optimizing some .xed execution. So, when the execution diverges from the recorded path, \nthere must exist a mechanism to return us to the original program. The third and .nal component is this \nbail-out mechanism. In the literature of trace compilation, these are called side-exits [8]. We forego \nmodeling the many other features of the technique that exist in implementations. For instance, we shall \nnot model the heuristics in how one actually detects a hot path of execution, we will simply build in \nthe ability to record a path of execution nonde\u00adterministically. This nondeterminism will be realized \nvia overlap\u00adping reduction rules. Nor shall we model implementation details, such as trace trees and \ntheir interactions [7]. We aim to keep the calculus minimal yet high-level, safe and even desirable for \nhu\u00adman consumption. 2.1 The Language and Its Baseline Execution We .rst present the syntax and small-step \noperational semantics for a simple imperative language with two of the three essential ingre\u00addients: \nloops and the ability to bail out modeled by continuations. The syntax of our language, inspired by the \ncalculus presented in Moll [9], is shown in .gure 1. Concretely, traces are a subset of normal statements. \nThey are meant to be straightline sections of code with side-exits, so there are only no-ops, assignments, \nand side-exits. We use x to range over variables, i to range over non-negative integers, s, . to range \nover stores, and l, m, n, k, s, p to range over statements throughout the rest of the paper. The baseline \ntransition rules are shown in .gure 2. Assume . is the real addition operator on integers. We use a labeled \ntransition system where labels correspond to store updates. We assume the reader is familiar with such \nsystems as they are used in the literature of concurrency [14]. The only observable transitions are store \nupdates, which are labeled by the store delta . All other transitions are silent, labeled t , and are \nunobservable. The subscript B denotes baseline transition rules. The subscript A denotes a strict subset \nof the baseline transition rules that will be used in the upcoming proofs. The subscript T denotes tracing \ntransition rules. The baseline rules in .gure 2 are common to both. The baseline rules do the usual things. \nBailTrue is the rule that applies continuations in the bails. It says to clobber the current reduct with \nthe packaged continuation s. 2.2 Recording Traces We extend the baseline execution with the ability \nto record traces. The set of baseline rules is a proper subset of the tracing rules, i.e. -.B .-.T . \nThe abstract syntax is the same between the two languages. The additional transition rules are shown \nin .gure 3. Starting Traces We start a trace at the beginning of a while loop. For technical reasons \nfor the proof of correctness, we record when we have already unfolded at least one iteration of the loop. \nAlso note that Trace puts the reduction rules in recording mode, which is represented syntactically as \n4-tuples. The components are, e ::= n | x +1 expressions b ::= x =0 | x boolean expressions =0 w ::= \nwhile b do s loops s ::= E | cs statements c ::= skip; | x := e; | w | if b then s | bail b to s commands \nt ::= E | ct t traces ct ::= skip; | x := e; | bail b to s recorded commands Figure 1. Syntax of the \nSimple Imperative Language and Traces . .. true if b is x =0 . s(x)=0 false if b is x =0 . s(x)=0 n \nif e = n s (e)=s (b)= s(x) . 1 if e = x +1 .. true if b is x =0 . s(x) =0 false if b is x=0 . s(x)=0 \n d ::= x/i | x/true | x/false store updates a ::= t | d actions d \\s, x := e; k) -.T,B,A \\s[x/s (e)],k) \nwhere d = x/s (e) (Assign) t \\s, skip; k) -.T,B,A \\s, k) (Seq) t \\s, (if b then s) k) -.T,B,A \\s, k) \nif s (b)= false (IfFalse) t \\s, (if b then s) k) -.T,B,A \\s,s k) if s (b)= true (IfTrue) t \\s, (while \nb do s) k) -.T,B,A \\s, (if b then (s while b do s)) k) (While) \\s, (bail b to s) k) tif s (b)= false \n(BailFalse) -.T,B,A \\s, k) t \\s, (bail b to s) k) -.T,B \\s, s) if s (b)= true (BailTrue) Figure 2. Shared \nTransition Rules  x =0 if b is x=0 \u00acb = x if b is x =0 =0 t \\s, (if b then (s (while b do s))) k) \n-.T \\s, (while b do s) k, E,s (while b do s) k) if s (b)= true (Trace) d \\s, kw, t, x := e; k) -.T \\s[x/s \n(e)],kw,t (x := e;),k) where d = x/s (e) (RecordAssign) t \\s, kw, t, skip; k) -.T \\s, kw,t (skip;),k) \n(RecordSeq) t \\s, kw, t, (if b then s) k) -.T \\s, kw,t (bail b to (sk)),k) if s (b)= false (RecordIfFalse) \nt \\s, kw, t, (if b then s) k) -.T \\s, kw,t (bail \u00acb to k),sk) if s (b)= true (RecordIfTrue) t \\s, kw, \nt, (while b do s) k) -.T \\s, kw,t (skip;), (if b then (s while b do s)) k) if kw =(while b do s) k (RecordWhile) \nt \\s, kw, t, (while b do s) k) -.T \\s, O(while b do t, s) k) if kw =(while b do s) k (Stitch) a'a' \\s, \nkw, t, k) -.T \\s,k') if \\s, k) -.T \\s,k'). kw (Abort) = k Figure 3. Tracing Transition Rules in order, \nthe store, the stopping point of the trace, the trace thus far, and the current program being reduced. \nRecording Traces The recording rules record one command at a time and concatenate it to the end of the \ntrace. Concatenation is simple juxtaposition. The trace itself is a straightline section of code, so \nwe install side-exits (pieces of code that jump back to untraced code when the condition we traced no \nlonger holds) when we record conditionals. To ease the task of proving correctness, RecordWhile appends \na skip to the trace while unrolling the loop. Its side condition is to ensure that we are recording an \ninner loop inside the current loop we are tracing, and that we have not come full circle and .nished \ntracing. The work for .nishing up a trace is done in Stitch, whose side condition is mutually exclusive \nwith that of RecordWhile. Ending Well-Behaved Traces We end the trace and stitch it back into the program \nusing Stitch when we .nish tracing the body of the loop. We know we have .nished when we come back to \nreducing the same loop that started the trace. We compile the loop that was traced into the same language.2 \nThe actual optimization is immaterial to the semantics; we assume that there is a sound optimization \nfunction O :(Statement \u00d7 Store) . Statement. What soundness entails here will be made precise when we \ninvestigate correctness. Informally, soundness means that the output of the O function does the same \nthing as the original code, as far as observable behavior (store updates) goes. Ending Badly-Behaved \nTraces We are not guaranteed to .nish tracing the body of the loop. That loop body might never terminate! \nConsider the following example; assume s2 never changes b to 0. 1 a := 1; 2 b := 1; 3 while a = 0 do \n4 s1 5 while b = 0 do 6 s2 If we start tracing the outer loop, once we start executing the inner loop \nwe will never .nish the outer loop body, and thus never .nish tracing. Implementations of trace compilation, \nthen, must use heuristics to end the trace if it is continuing on for too long. In our semantics, we \nmodel this by introducing another nonde\u00adterministic rule that prematurely stops the trace, Abort. This \nrule shares the same premises with all Record rules, where  is a wildcard. Note that there are no axioms \nfor recording bails what this means is that instead of the semantics getting stuck when trying to trace \na trace, we abort the trace (that is, we do not model higher\u00adorder tracing). Also note that Abort s3 \nside condition is mutually exclusive with Stitch, which is intuitively the good situation of a successful \ntrace. In this way the rule models the semantics of bail\u00ading out of tracing mode for all bad situations. \n 2.3 Example Trace Recording To help illustrate the tracing rules and to build some concrete intuition, \nconsider the following contrived example. Example Input 1 x := 0; 2 while x =0 do 3 y := 0; 2 Note that \nthis is a simpli.cation in our model. In actual tracing JITs, the compiled code is in machine language. \n3 The rule is modeled as presented instead of the viable alternative of t (s, kw, t, k) -.T (s, k) if \nkw= k for a cleaner proof of correctness. 4 while y =0 do 5 y := 1; 6 z := 1; 7 b := a +1; There are \ntwo loops; the inner loop only iterates once. The variable a is computed at some earlier point in the \nprogram. We give a rule\u00adby-rule walkthrough of tracing the outer loop. We build up the trace in tandem \nwith our walking through of the reduction rules; each snippet that the Record rules append to the trace \nis displayed one by one. To start, line 1 of the input is matched by Assign, so we reduce by Assign. \nLine 2 is a while loop, which we reduce by While. While converts the loop into an if statement testing \nthe condition x =0. This is indeed true by how we mutated the store in line 1, so we can reduce by IfTrue \nor Trace. In the interest of demonstrating tracing, we reduce by Trace. The trace built thus far is empty, \nor E. We ve only entered recording mode, but we haven t actually recorded any commands yet. Line 3 in \nthe input is an assignment, which is matched by RecordAssign. RecordAssign appends the assignment itself \nonto the trace: Example Trace 1 y := 0;  Line 4 in the input is the inner loop, and we will now see \nhow the tracing rules deal with recording loops. The loop itself will .rst reduce to an if via RecordWhile, \nwhich appends a no-op skip; to the trace. In reducing the resulting if, we are testing the condition \ny =0. It is true, so we reduce using RecordIfTrue. The result is that we append a side-exit as a bail \nto the trace. The computation that would have been executed had the condition been false gets packaged \nup as a continuation and gets put into the body of the bail (shown indented in the listing): 2 skip; \n3 bail y =0 to 4 z := 1; 5 b := a +1; 6 while x =0 do 7 y := 0; 8 while y =0 do 9 y := 1; 10 z := 1; \n11 b := a +1; Now that we have installed the side-exit for entering into the inner loop, we trace the \nbody of the inner loop as straightline code. Line 5 in the input is another assignment, which we record \nusing RecordAssign. 12 y := 1; After the body of the inner loop we attempt to reduce the next iteration \nof that loop. Again, the loop will .rst reduce to an if by RecordWhile. This appends a skip;. Unlike \nthe last time, however, the condition y =0 is now false, so we instead reduce using RecordIfFalse. We \nappend another side-exit as before, but the packaged continuation is different. Since the condition was \nfalse in the actual execution, we need to include the statement that would have been executed if the \ncondition were true. After that statement we package the rest of the iteration of the outer loop and \nappend it: 13 skip; 14 bail y =0 to 15 y := 1; 16 while y =0 do  FV : Statement . Variables FV (s)= \n{x | x is free in s} F : ((Expression + Statement + Command) \u00d7 Store \u00d7V) . Statement . . n if e = n \nF (e, s, v)= s(x) . 1 if e = x +1 . x . v . e if e = x +1 . x . v s if s = E F (s, s, v)= F (c, s, v) \nF (s1, s, v) if s = cs1 x := F (e, s, v) if c = x := e F (c, s, v)= c otherwise O :(Statement \u00d7 Store \n\u00d7V) . Statement while b do F (s1, s, FV (s1)) if s = while b do s1 O(s, s)= s otherwise Figure 4. Variable \nFolding O 17 y := 1; 18 z := 1; 19 b := a +1; 20 while x =0 do 21 y := 0; 22 while y =0 do 23 y := 1; \n24 z := 1; 25 b := a +1; Finally, we apply RecordAssign twice to lines 6 7 and append the assignments \nto the trace. 26 z := 1; 27 b := a +1; Having successfully traced an iteration of the loop, we now reduce \nby Stitch to stitch the trace back into the program using the identity as the O function. Abbreviating \nthe continuations for the side-exits as ki, the .nal stitched traced loop is as follows. Abbreviated \nStitched and Traced Example Loop 1 while x =0 do 2 y := 0; 3 skip; 4 bail y =0 to k1 5 y := 1; 6 skip; \n7 bail y =0 to k2 8 z := 1; 9 b := a +1;  2.4 Example O We have seen the output of tracing, but we obviously \nwant to do more than that. We want to optimize. Consider optimization shown in .gure 4 that folds away \nvariables that we never assign to inside a traced loop. First we de.ne a function that calculates the \nfree (in the sense of never-assigned-to) variables of a statement. It is assumed to be de.ned in the \nusual way. Next we de.ne a helper function F that does the actual optimization. V is the set of free \nvariables. Finally the O function is just a wrapper around F that calculates and passes in the free variables. \nIf we apply it to our running example where a . 41, we fold the assignment to b on line 9 of the abbreviated \nstitch example: 9 b := 42; The main bene.t of run-time optimization is that we can be more aggressive \nthan with ahead-of-time optimization. Here we presented a simple conservative folding of free loop variables. \nThe idea is that free variables in the loop bodycan be treated as constants and folded until we break \nout of the loop. We cannot be so bold with a static version of this kind of folding, as we can only do \nso if we know that the variables we want to fold are constants for the entirety of program execution. \nHere, however, we only need to know that the variable s value does not change until the loop is .nished.4 \n3. Correctness What does it mean for a trace to be correct? First, correctness of the traced code is \nbehavioral correctness the trace has to do the same thing as the original code. Attempting to prove con.uence \nof the program text such as in Pfenning [16] is unfruitful, as there are no guarantees in trace compilation \nof the traced code converging back to the same text as the original program. In a reactive user-interface, \nfor instance, we might trace-compile many inner loops, and those compiled inner loops might execute forever, \nwaiting for user input. But even in those cases of in.nite execution we still want to reason about correctness. \nThe need for in.nite executions suggests the tool of bisimilarity. Second, correctness of the traced \ncode is intensional correct\u00adness. Unlike ahead-of-time compiler correctness, we cannot say that an optimized \ntrace is observationally equivalent, or has the same sequence of observable reductions, to the original \nloop in the traditional, extensional sense. Speci.cally, an optimized trace need not be observationally \nequivalent to the original loop under all stores. Consider the following version of our little example: \n1 x := 0; 2 while x =0 do 3 b := a +1; A reasonable trace-based optimization if a . 41, as we have seen, \nwould be to replace the loop body with b := 42. But this code is 4 In our simple model, the trace is \neffectively discarded after the loop exits. There is no way to re-enter a traced loop once it exits. \nThis is not the case in practice, where constructs such as methods allow compiled traces to be called \nmultiple times. In those cases the tracing JIT has to add in more guards and side-exits to guard the \nfolded values. We omit this complexity. most de.nitely not observationally equivalent to the original: \nthe original has a free variable, a, and the optimized code b := 42 does not. Side-exits are also problematic. \nHow do we ensure that we jump back to the right place in the original code? We retain the familiar notion \nof observational equivalence, but parameterize it over stores and computation suf.xes. Namely, a trace \nis correct if it is observationally equivalent to the original loop for the store that the original loop \nis currently reducing under and for the rest of the program that the original loop would have reduced \nunder. To formalize these intuitions, we model correctness using inten\u00adsionalized bisimulations over \nstores. Intensionalizing to a particular suf.x will be made formal in the de.nition of O soundness in \nsec\u00adtion 3.3. Bisimulation techniques see popular use in process calculi [14]. There is also existing \nwork in the analysis and correctness proofs of program transformation [6, 21, 22]. The de.nitions here \nare built upon, but slightly different from, the standard notions found in the concurrency literature \n[14], as they are de.ned over a store. Observational equivalence also be\u00adcomes formally de.ned as the \nnotion of bisimilarity. Let the set of labels be de.ned as follows. Act = {d | d is a store update}.{t \n} De.nition. If r . Act*, then r is the sequence whereby all occurrences of t are removed. De.nition. \nIf r = a1 \u00b7\u00b7\u00b7 an . Act*, we write m =.r m ' to mean t ttt a1an' m -. * \u00b7 -. \u00b7 -. * \u00b7\u00b7\u00b7 -. * \u00b7 -. \u00b7 -. \n* m That is, there may be any number of intervening silent transi\u00adtions between the observable sequences. \nIn this particular system, the primary observable entity is the store itself, so the intuitive meaning \nof a program becomes the sequence of store updates it performs. We only concern ourselves with closed \nprogram-store pairs in this paper, where the de.nition of closed is as follows. De.nition. For a store \ns and a program m, we say m is s-closed if for all variables that appear in m, s (x) is de.ned. For the \nrest of the paper, when we say for any store or for all stores , we mean for all stores that form closed \nprogram-store pairs with the programs under consideration. De.nition (Bisimulation). A bisimulation for \ntwo reduction rela\u00adtions X, Y is a relation R such that R(s, m, n) implies a 1. Whenever \\s, m) -.X \\s \n' ,m ' ) then, for some n ' , a \\s, n) =.Y \\s ' ,n ' ) and R(s ' ,m ' ,n ' ) a 2. Whenever \\s, n) -.Y \n\\s ' ,n ' ) then, for some m ' , a \\s, m) =.X \\s ' ,m ' ) and R(s ' ,m ' ,n ' ) In the above de.nition \nwe abuse notation and let m, m ' , n, and n ' range over both statements and triples of statements. That \nis, since it does not add to the discussion to distinguish between 2\u00adtuples and 4-tuples in the de.nition, \nwe use a single metavariable to range over both. The traditional notion of bisimilarity is a special \ncase of this one: two programs are bisimilar in the traditional sense if they are bisimilar for all stores. \nDe.nition. m is said to be bisimilar to n under reduction relations X, Y for a store s, written m X Y \nn for s, if R(s, m, n) for some bisimulation R on X, Y . In other words,  X Y = {R | R is a bisimulation \nfor X, Y } Lemma 3.1. Bisimilarity is an equivalence relation. Before stating the main lemma, we note \nthat all nondeterminis\u00adtic rules in our system step to the same store. We prove this later in lemma 3.7. \nFor simplicity in stating the main lemma, we simply say that the two branching stores are always the \nsame. We are now ready to state the main lemma. In the literature, diamond lemmas are usually single \ndiamonds. The trace calculus, however, has a modal .avor with 2-tuples as one mode and 4-tuples as the \nother mode. As such, our calculus has six diamonds up to symmetry. Lemma 3.2 (Diamond Lemma). All of \nthe following hold. For di\u00adamonds 4 6, \\., kw, t, p) is well-formed, a notion we will expound upon in \nsection 3.1. aa 1. If R : \\., p) -.T \\s, m) and R ' : \\., p) -.T \\s, n), then m T T n for s. \\., p)R \nR ' \\s, m) T T \\s, n) aa 2. If R : \\., p) -.T \\s, kw, t, m) and R ' : \\., p) -.T ' \\k '' \\s, k ' ,t ,n), \nthen \\kw, t, m) T Tw,t ,n) for s. w \\., p)R R ' \\s, kw, t, m) T T \\s, k ' ,t ' ,n) w aa 3. If R : \\., \np) -.T \\s, m) and R ' : \\., p) -.T \\s, kw, t, n), then m T T \\kw, t, n) for s. \\., p) R ' R \\s, m) T \nT \\s, kw, t, n) aa 4. If R : \\., kw, t, p) -.T \\s, m) and R ' : \\., kw, t, p) -.T \\s, n), then m T T \nn for s. \\., kw, t, p) R R ' \\s, m) T T \\s, n) aa 5. If R : \\., kw, t, p) -.T \\s, k ' ,t ' ,m) and R \n' : \\., kw, t, p) -.T '' ' \\k '' '' \\s, k '' ,t ,n), then \\k ' ,t ,m) wT T ,t ,n) for s. www \\., kw, \nt, p) R ' R \\s, k ' ,t ' ,m) T T \\s, k '' ,t '' ,n) ww aa 6. If R : \\., kw, t, p) -.T \\s, m) and R ' \n: \\., kw, t, p) -.T \\s, k ' ,t ' ,n), then m T T \\k ' ,t ' ,n) for s. \\., kw, t, p) R ww R ' \\s, m) \nT T \\s, k ' ,t ' ,n) w  This lemma says that should execution branch into two branches, both branches \nwill do the same thing, at least observationally. We aim to use the main lemma to arrive at a more familiar \nplace: con\u00ad.uence of stores, namely corollary 3.10. The rest of this section is organized as follows. \nSection 3.1 introduces the idea of well-formedness for the 4-tuples, or the tracing rules. Section 3.2 \nintroduces the correctness criterion of the unoptimized trace. Section 3.3 proves the main lemma. Section \n3.4 explores the relationship between con.uence and our bisimulation result. Many proofs in this section \nare omitted for brevity. The reader may .nd them in the full version of the paper at http://www.cs. ucla.edu/~palsberg/paper/popl11.pdf. \n 3.1 Well-Formedness of 4-Tuples When the calculus decides to initiate a trace, it steps to a con.gu\u00adration \nin the shape of a 4-tuple. The four components are, in order, the store, the point in the original code \nwhen we started tracing, the trace so far, and the statement currently being reduced. Not all 4-tuples \nare created equal, however, as not all 4-tuples are well\u00adformed. Intuitively, well-formedness is something \nlike an incre\u00admental version of correctness. Only well-formed 4-tuples eventu\u00adally become fully correct \nunoptimized traces. Thus, we want it to be an invariant of the computation. Well-formedness is a tight \nand intricate relationship between the original loop, the trace thus far, and the current reduct. Informally, \nwe need the trace thus far to be a recording of all the steps that the original loop took just before \nit reached the current reduct. Before we formally de.ne well-formedness, we formalize what it means to \nbe a trace thus far . De.nition (Partial Trace Relation). A partial trace relation is a a relation T \nsuch that T (s, t, l) implies that whenever \\s, t) -.B a \\s ' ,t ' ) then, for some l ' , \\s, l) -.B \n\\s ' ,l ' ) and 1. If t stepped by BailTrue, t ' = l ' 2. Otherwise, T (s ' ,t ' ,l ' )  The constituents \nare as follows: t is the trace and l is the original code. Recall that both t and l are just statements. \nThe formalization is a variation on the standard simulation relation and captures the two properties \nthat a partially constructed trace intuitively satis.es. First, BailTrue models jumping back to the original \ncode, so we expect the descendants to be exactly equal. Second, the partial trace is partial, so it can \nterminate before the original code does, signifying that the rest has not yet been traced. De.nition. \nWe call t a partial trace to l for a store s, written t ; l for s, if T (s, t, l) for some partial trace \nrelation T . In other words,  ; = {T |T is a partial trace relation} For the de.nition of well-formedness \nand subsequent lemmas we will be working with the reduction relation A, which we have not used yet, as \nwell as a notion of being stuck . Recall that the reduction relation A = B \\{BailTrue}. De.nition. For \na reduction relation X, we say a con.guration a \\s, m) is not X-stuck iff m = E or \\s, m) -.X \\s ' ,m \n' ) for some s ' ,m ' . De.nition (Well-Formedness). A 4-tuple \\s0,kw, t, m) is well\u00adformed iff all the \nfollowing hold. 1. kw =(while b0 do l0) k0 2. For all s, either  rr \\s, t) -. *' ,E) and \\s, l0 kw) \n-. *' ,m) A \\s A \\s or for some t ' , r ''' ' \\s, t) -. * ,t ) and t is A-stuck but not B-stuck A \\s \n3. For all s, t ; l0 kw for s. This property formalizes the invariant we wish computation to preserve. \nFirst, kw must be the loop where we started tracing. Second, the trace t must do one of two things. It \nmust either go far enough by terminating in the same reduction sequence that body of the original loop \nundergoes to reduce to the current statement, m, or it must eventually step to some descendant that can \nonly reduce by BailTrue. Third, t must be a partial trace to the original loop for all stores. We now \nprove that the reduction relation T preserves this in\u00advariant. We will do this in two steps. First, we \nprove that the trac\u00ading rules themselves the rules that step from a 4-tuple to another 4-tuple preserve \nwell-formedness. Lemma 3.3. Let p = \\s0,kw, t, m). If p is well-formed and a '' ' p -.T p such that \np is a 4-tuple, then p is also well-formed. Proof. The .rst conjunct holds trivially because no rules \nchange kw, and we proceed to prove the next two conjuncts together by case analysis on the structure \nof the reduction relation. Second, we prove that whenever we initiate a trace whenever we step from a \n2-tuple to a 4-tuple the resulting 4-tuple is well\u00adformed. a '' ' Lemma 3.4. If \\s, m) -.T p where p \nis a 4-tuple, then p is well-formed. Proof. By inversion the only rule that results in a 4-tuple is Trace. \nLet w = while b do s. We have t \\s, (if b then (sw)) k) -.T \\s,w k,E,sw k) The .rst two conjuncts are \nclearly satis.ed. It remains to prove conjunct 3, that E ; swk for all s. It suf.ces to exhibit a partial \ntrace relation T such that for all s, T (s,E,sw k). Since t = E, we exhibit the empty relation \u00d8 as one \nsuch T . Lemmas 3.3 and 3.4 lead us to a more general lemma about the transitive, re.exive closure of \nthe T reduction relation. This lemma is not used in the rest of the section, but does clearly convey \nthat well-formedness is a property preserved by computation in our calculus. r '' ' Lemma 3.5. If \\s, \nm) -. * p where p is a 4-tuple, then p is T well-formed. 3.2 Correctness of the Unoptimized Trace Recall \nthat the intuition for well-formedness is that it is an incre\u00admental correctness. With it we can now \nbuild up a bisimulation relation.5 Lemma 3.6 (Stitch Lemma). For some t, let w = while b0 do l0 and w \n' = while b0 do t. If for some k, t ; l0 wk for all s and (*) holds of t, l0 w k,s then, w ' k B B wk \nfor all s. (*) For all s, either rr \\s, t) -. * \\s ' ,E) and \\s, l0 wk) -. * \\s ' ,w k) AA 5 The unoptimized \ntrace is in fact strongly bisimilar to the original code. Since we are simply recording some execution \npath command-for\u00adcommand, it shouldn t be surprising that the resulting trace is exactly equiv\u00adalent \nto the original path. In the interest of less mechanism and since weak bisimilarity subsumes strong bisimilarity, \nwe will directly prove weak bisimilarity. or for some t ' , ''' ' \\s, t) -. * \\s ,t ) and t rA is A-stuck \nbut not B-stuck This lemma is the correctness property we want to express of unoptimized traces. In prose, \nw is the original loop, and w ' is the new loop with the trace stitched in. The t ; l0 wk for all s and \n(*) conditions are what hold of t, l0 w k,s per well-formedness at the point of stitching. The lemma \nsays once we come full circle and stitch the recorded trace into the original program, that trace is \nactually equivalent to the original loop. The reader should bear in mind that this is an almost extensional \nequivalence, a stronger notion than intensional equivalence. The insight here is since we have proven \na more re\u00adstrictive property than we need, we can relax it. For fruitful opti\u00admization we need to make \nthe relation larger, relaxing extensional bisimilarity to the intensional version. Proof. By exhibition \nof a bisimulation relation R under the rela\u00adtions B, B such that R(s, w ' k,w k) for all s.  3.3 Proof \nof the Diamond Lemma With the tool of bisimilarity under our belt, we can now make precise the notion \nof the soundness of O. This de.nition will be crucial for the proof of the diamond lemma. De.nition (O-Soundness). \nAn O function is sound iff for any '' ' w,w ,k,s such that wk B B wk for all stores, O(w ,s) k B B wk \nfor s. Proof of the diamond lemma. If R and R ' are the same, then we are done as m = n or \\kw, t, n) \n= \\k ' ,t ' ,n ' ). It is straightforward w to verify that R = R ' for diamonds 1, 2, 4, and 5, which \nare deterministic, so we only need to prove diamonds 3 and 6. We can rewrite these diamonds more precisely \nbelow. 3. The nondeterministic rules are IfTrue and Trace. We have m = n by inversion. Further, by lemma \n3.4 \\s, kw, t, n) is well\u00adformed. aa If R : \\., p) -.T \\s, m) and R ' : \\., p) -.T \\s, kw, t, n), then \nm T T \\kw, t, n) for s. \\., p) IfTrue Trace \\s, m) T T \\s, kw, t, n) 6. The nondeterministic rules are \nRecord and Abort. We have kw = w and m = n by inversion. Similarly, by lemma 3.3 k ' \\s, kw,t ' ,n) is \nwell-formed. aa If R : \\., w, t, p) -.T \\s, m) and R ' : \\., kw, t, p) -.T \\s, k ' ,t ' ,n), then m \nT T \\k ' ,t ' ,n) for s. \\., kw, t, p) ww Record Abort \\s, m) T T \\s, kw,t ' ,n) To show that T T holds \nfor both diamonds, it suf.ces to exhibit a bisimulation relation R under the reductions T,T such that \nR(s, m, \\kw, t, n)) and R(s, m, \\kw,t ' ,n)) hold. We claim the following relation is a bisimulation \nfor any m, n, u, kv,kw, t, s. R = {(s, m, n) | m B B n for s} .{(s, m, \\kv, u, n)) | m B B n for s and \n\\s, kv, u, n) is well-formed } .{(s, \\kw, t, m),n) | m B B n for s and \\s, kw, t, m) is well-formed } \nThe rest is omitted for brevity.  3.4 From Bisimulation to Con.uence This section aims to be the interface \nbetween bisimulation and con\u00ad.uence. As correctness is often studied in terms of determinacy and con.uence, \nwe seek here to prove something akin to con.uence of stores to show the adequacy of our operational semantics. \nTradi\u00adtionally, con.uence theorems are proven from the bottom up using a diamond lemma, iterating that \ndiamond lemma to build a strip lemma, and .nally using the strip lemma to construct con.uence [16]. Bisimilarity, \nhowever, allows us to skip the iteration of the single-step diamond lemma. Indeed, there is no analog \nto an iter\u00adable diamond lemma here. We instead use bisimilarity to directly obtain a strip lemma. Nevertheless, \nthe techniques and diagrams in this section are strongly in.uenced by the clear and readable ap\u00adproach \nof Pfenning [16]. First we prove the assumption needed for all cases of the dia\u00admond lemma, that nondeterministic \nbranching always branches to con.gurations with the same store. aa Lemma 3.7. If \\s, m) -.T \\s ' ,m \n' ) and \\s, m) -.T \\s '' ,m '' ), = s '' then s ' and a = a ' . Proof. Straightforward case analysis. \nLemma 3.8 (Strip Lemma). If R : \\s, m) -.T \\s ' ,m ' ) \\s '' '' and R *' : \\s, m) -. * ,m '' ), then \nfor some ., n ' ,n , \\s ' ,m ' ) -. * \\., n ' ) and \\s '' ,m '' ) -. * \\., n '' ) such that T TT n ' \nT T n '' for .. \\s, m) R *' R \\s ' ,m ' )\\s '' ,m '' ) S *' S * \\., n ' ) T T \\., n '' ) Proof. By \ncase analysis on the structure of R * and the de.nition of T T . Now we can prove the diamond property \nfor stores on the multistep reduction, which we call store con.uence. Theorem 3.9 (Store Con.uence). \nIf R * : \\s, m) -. * \\s ' ,m ' ) T \\s '' '' and R *' : \\s, m) -. * ,m '' ), then for some ., n ' ,n \n, T \\s ' ,m ' ) -. * \\., n ' ) and \\s '' ,m '' ) -. * \\., n '' ) such that TT n ' T T n '' for .. \n \\s, m)R * R *' \\s ' ' )\\s '' '' ) ,m ,m S *' S * \\., n ' ) T T \\., n '' ) Proof. By induction on the \nstructure of R * . Finally, theorem 3.9 implies the familiar notion of store deter\u00adminacy for terminating \nprograms. \\s '' Corollary 3.10. If \\s, m) -. * \\s ' ,E) and \\s, m) -. * ,E), TT = s '' then s ' . 4. \nSound and Unsound Optimizations We have achieved our project of proving the essence of trace com\u00adpilation \ncorrect, yet at the same time that result is largely interesting due to its modularity with respect to \nthe O function. In this section we show the example O from section 2.4 to be sound and explore which \nkinds of O functions are sound and which are not sound. 4.1 Soundness of Variable Folding Let F, FV \n, and O be the ones presented in .gure 4. For brevity we assume that FV (s) is de.ned in the usual way \nand correctly generates the set of free variables for s. That is, for some store s, FV (s) the set of \nvariables which s never writes to in s during reduction. Lemma 4.1. -. * B is deterministic. Proof. -.B \nhas no points of nondeterminism. Lemma 4.2. O is sound. Proof. Assuming we have for some w, w ' ,k such \nthat w ' k B B wk for all stores, we want to show that O(w ' ,s) k B B wk for s. Our technique will be \nshowing that O(w ' ,s) k B B w ' k, and then obtaining the desired result via transitivity of B B . We \nproceed by case analysis on the O function. Case: s = while b do s1. We want to show that (while b do \nF (s1, s, FV (s1))) k B B (while b do s1) k It suf.ces to exhibit a bisimulation relation R such that \nR(s, (while b do F (s1, s, FV (s1))) k, (while b do s1) k) Let s1 ' = F (s1, s, FV (s1)). We claim the \nfollowing relation is a bisimulation relation for any m, ., s '. Note that s and the loops are .xed from \nthe assumption. R = {(., m, m)} .{(s, (while b do s1' ) k, (while b do s1) k)} .{(s, (if b then (s1 ' \nwhile b do s1' )) k, (if b then (s1 while b do s1)) k)} .{(s ' , (F (n, s, FV (s1)) while b do s1' ) \nk, (n while b do s1) k) | s ' (x)= s(x) for all free variables in s1} We proceed by case analysis on \nthe left-side reduction step. Subcase: The left side and right side are the same. By lemma 4.1 B is \ndeterministic, both sides step using the same rule, producing the same descendants. But then they are \nin R by construction. \\., m ) R\\., m ) aa \\. ' ,m ' ) R\\. ' ,m ' ) Subcase: The left side is (while \nb do s1' ) k and the right side is (while b do s1) k. Both sides reduce by way of While. Their descendants \nare in R by construction. \\s, (while b do s1' ) k)R\\s, (while b do s1) k) tt While While \\s '' \\s ' , \n(if b then (s1 \u00b7\u00b7\u00b7 ) k)R , (if b then (s1 \u00b7\u00b7\u00b7 ) k) Subcase: The left side is \\s, (if b then (s1 ' while \nb do s1' )) k) and the right side is \\s, (if b then (s1 while b do s1)) k) Suppose the left side reduce \nby IfFalse, then both sides step to k, which is already in R by way of the .rst subrelation. \\s, (if \nb then (s ' 1 \u00b7\u00b7\u00b7 ) k)R\\s, (if b then (s1 \u00b7\u00b7\u00b7 ) k) tt IfFalse IfFalse \\s, k)R\\s, k) Subcase: The left \nside is \\s, (if b then (s1 ' while b do s1' )) k) and the right side is \\s, (if b then (s1 while b do \ns1)) k) Suppose the left side reduce by IfTrue, we can then .ll out the diagram as follows. The descendants \nare in R by way of the fourth subrelation, as s1 ' = F (s1, s, FV (s1)) \\s, (if b then (s ' 1 \u00b7\u00b7\u00b7 ) k)R\\s, \n(if b then (s1 \u00b7\u00b7\u00b7 ) k) tt IfTrue IfTrue \\s, (s ' 1 \u00b7\u00b7\u00b7 ) k)R\\s, (s1 \u00b7\u00b7\u00b7 ) k)  Subcase: The left side \nis (F (n, s, FV (s1)) while b do s1' ) k and the right side is (n while b do s1) k The left side steps \nby way of Assign. By inversion, n = cn ' and F (n, s, FV (s1)) = F (c, s, FV (s1)) F (n ' , s, FV (s1)) \nFurther, c = x := e. For brevity let n '' = F (n ' , s, FV (s1)). By case analysis on e we have two subcases. \nIn the case where e = n, we have an identity. In the case where e = x ' +1 . x ' . v, e = s(x ' ) . 1. \nWe know that s(x ' ) is de.ned from assumption that the left side steps at all. This means the left side \nstep looks like the following. The call to F is abbreviated due to space. ' ''' d'' '' \\s, (cn \u00b7\u00b7\u00b7 ) \nk) -.B \\s [x/s(x ) . 1], (n \u00b7\u00b7\u00b7 ) k) where d = x/s(x ' ) . 1 By inversion then we see that the right \nside, starting with c, also steps by Assign. By the de.nition of s ' we have the following reduction \nfor the right side '' d''' ' \\s, (cn \u00b7\u00b7\u00b7 ) k) -.B \\s [x/s (x ) . 1], (n \u00b7\u00b7\u00b7 ) k) ' '' where d = x/s (x \n) . 1 For these two descendants to be in R, we need s ' (x ' )= s(x ' ). We know this to hold for all \nfree variables in s1, as their freeness guarantees them to be never written to during s1 s reduction. \nWe assumed that FV correctly generates the set of free variables for a statement. It is easy to see that \nc is a descendant of s1, thus x . FV (s1) and s ' (x ' )= s(x ' ) holds. Since FV is correct, x is not \nfree in s1. Therefore, ' '' s [x/s (x ) . 1](y)= s(y) for all y free in s1. This .nally gives us ' '' \n''' R(s [x/s (x ) . 1],n ,n ) which holds by way of the fourth subrelation. For the diagram below, let \nv = s(x ' ) . 1. \\s ' , (F (n, s, FV (s1)) \u00b7\u00b7\u00b7 ) k)R\\s ' , (n \u00b7\u00b7\u00b7 ) k) Assign Assign dd \\s ' [x/v], (F \n(n ' , s, FV (s1)) \u00b7\u00b7\u00b7 ) k)R\\s ' [x/v], (n ' \u00b7\u00b7\u00b7 ) k) All other cases (where s is something other than \na while loop) are identities. The proofs for the converses are symmetric. We have shown O(w ' ,s) k B \nB w ' k for s. By transitivity, we have the desired result of O(w ' ,s) k B B wk. The most interesting \npart of the proof is that in every subcase we relied on the right side to be able to mirror the left \nside s move exactly in a single step. This is a stronger property than required by the bisimulation de.nition, \nwhich says only visible moves need to be mirrored.  4.2 Soundness of Dead Branch Elimination What kinds \nof optimizations only mirror visible moves? One can imagine that during tracing we may generate many \nspurious side\u00adexits. Suppose we extend our variable folding example to also eliminate dead , or always-false \nbails. The modi.cations needed for F are shown in .gure 5. For example, considered the following example \ntrace with a dead side-exit. Clearly x is free in the body of the traced loop, and the boolean expression \nx =0 is always going to be false. Example Trace with Dead Bail 1 while x =0 do 2 bail x =0 to k1 3 z \n:= 1;  Plugging the above example into the extended O function will output the following. Example Trace \nwith Dead Bail Optimized Away 1 while x =0 do 2 z := 1; Such an optimization does not generate code \nthat exactly mir\u00adrors the original. This fails to hold if we wholly excise dead condi\u00adtionals, as the \noriginal code would still need to take a step to eval\u00aduate the conditional to false before skipping it. \nTo show that this new optimization is still bisimilar, let us extend lemma 4.2 with the proof sketch \nof a new subcase and its converse. New subcase and its converse for lemma 4.2. Subcase: The left side \nis (F (n, s, FV (s1)) while b do s1' ) k and the right side is (n while b do s1) k The left side takes \nsome step. Let n = cn ' and F (n, s, FV (s1)) = F (n ' , s, FV (s1)) We are concerned with the case \nwhen c = bail b ' to k ' . F (b ' , s, FV (s1)) = false, all other cases for c are identities. For brevity \nlet n '' = F (n ' , s, FV (s1)). The left side step looks like the following for some n '''. The call \nto F is abbreviated due to space. \\s ' '' -.B \\s '' ''' , (n \u00b7\u00b7\u00b7 ) k) a, (n \u00b7\u00b7\u00b7 ) k) By inversion we \nknow that s (b ' )= false. Since we assumed that FV correctly generates the set of free variables for \ns1 and c is a s1-descendant, so s ' (b ' )= false. By inversion then we see that the right side, starting \nwith c, steps by BailFalse. '' t'' \\s, (cn \u00b7\u00b7\u00b7 ) k) -.B \\s, (n \u00b7\u00b7\u00b7 ) k) It remains to show that n ' \ncan take a step to match the left side step that F (n ' , s, FV (s1)) took. We again decompose n ' into \nits .rst command and continuation. We iteratively apply the same reasoning we just underwent until the \n.rst command is not bail b '' to k '' . F (b '' , s, FV (s1)) = false. For these other cases F acts as \nan identity for the .rst command and as congruence for the continuation, so clearly it will take the \nsame a step. In the diagram below, let + mean 1 or more times . \\s '' \\s ' , (F (n ,s, FV (s1)) \u00b7\u00b7\u00b7 ) \nk)R , (n \u00b7\u00b7\u00b7 ) k) t BailFalse+ a \\s ' , (n ' \u00b7\u00b7\u00b7 ) k) a \\s '' '' \\s '' '' , (F (n ,s, FV (s1)) \u00b7\u00b7\u00b7 ) \nk)R , (n \u00b7\u00b7\u00b7 ) k)  F (b, s, v)= true if b = x =0 . x . v . s(x)=0 false if b = x =0 . x . v . s(x)=0 \ntrue if b = x =0 . x . v . s(x)=0 F (c, s, v)= . . .. .. . .. . false if b = x =0 . x . v . s(x)=0 undef \notherwise ... E if c = bail b to s1 . F (b, s, v)= false ... Figure 5. Variable Folding extended with \nDead Branch Elimination F (c, s, v)= .. . ... E if c = x := e . x has no use sites in the trace ... \n Figure 6. Variable Folding extended with Dead Branch and Dead Store Elimination The converse is considerably \nsimpler. We have the case where the right side steps by BailFalse. By the same reasoning above concerning \nfree variables, we see that the left side would have had its bail optimized away into E, thus we can \ncomplete the diagram by using Id. \\s '' \\s ' , (F (n ,s, FV (s1)) \u00b7\u00b7\u00b7 ) k)R , (n \u00b7\u00b7\u00b7 ) k) t Id BailFalse \n\\s ' , (F (n ' , s, FV (s1)) \u00b7\u00b7\u00b7 ) k)R\\s ' , (n ' \u00b7\u00b7\u00b7 ) k) All other cases are still identities.  4.3 \nUnsoundness of Dead Store Elimination Finally, we want to explore what kinds of optimizations are simply \nunsafe in the tracing framework. Put formally, we want to ask what kind of optimizations do not produce \nbisimilar code. Continuing with our existing O function, suppose we were to extend it with dead store \nelimination. That is, suppose variables that we assign to but have no use sites inside the trace body \nare simply excised. This is shown informally in .gure 6. For example, consider the following example \ntrace with a dead assignment. The variable z is assigned but never used. Example Trace with Dead Assignment \n1 while x =0 do 2 z := 1; Plugging the above example into the extended O function will output the following. \nExample Trace with Dead Assignment Optimized Away 1 while x =0 do 2 E Intuitively this is unsafe because \neven though z is dead inside the trace, there very well may be use sites of z after the trace! This intuition \nis re.ected formally. Taking our example above, we need to show that z := 1; takes a step that can be \nmirrored by E. For some s, by inversion z := 1 can step only by Assign: t \\s, z := 1) -.B \\s[z/1],E). \nE needs to be able to match this t move, but \\s, E) =.B \\s[z/1],s) for any s. In fact, it does not step \nat all. In this fashion this optimization does not output bisimilar code, and is not safe for use inside \nthe tracing framework. 4.4 Soundness of Composition One property that correct optimizations enjoy in \nour framework is that the composition of two correct optimizations also yield a cor\u00adrect optimization. \nWe give the following two lemmas to demon\u00adstrate this property. Lemma 4.3. Let I :(Statement \u00d7 Store) \n. Statement be the identity function on its .rst argument. I is sound. Proof. Trivial by the de.nition \nof O-soundness. Lemma 4.4. Let F, G :(Statement \u00d7 Store) . Statement be two sound optimizations. Let \ntheir composition be de.ned as F . G = .(s, s).F (G(s, s),s) F . G is sound. Proof. We want to show that \nfor any w, w ' ,k such that w ' for all stores, (F . G)(w ' ,s) k B B wk for s. By soundness of G on \nw, w ' ,k we know that G(w ' ,s) k B B wk By soundness of F on w, G(w ' ,s),k we then know that F (G(w \n' ,s),s) k B B wk '' k B B wk But F (G(w ,s),s) k =(F . G)(w ,s), so we are done. 5. Related Work The \nwork carried out in this paper depends on both compiler\u00adcorrectness and concurrency techniques. Though \nthe corpora of both communities are large, there is a dearth of truly relevant papers that explore purely \noperational compiler correctness of JIT compilers from as a high-level as ours. Nevertheless, we have \ntaken inspiration as well as fruitful comparisons with several works. Relevant is Wand s work on parallel \ncompiler correctness [20]. We believe Wand s enterprise to be, though also employing bisimu\u00adlations to \nprove compiler-correctness, of a different .avor than our own. His approach is the combination of (syntax-directed) \ndenota\u00adtional semantics and essentially \u00df-convertibility. His picture is also closer to the traditional \npicture of compiler correctness [5] that is, the compilation process preserves denotation up to bisimulation \nthan ours, as his compiler is an ahead-of-time compiler. The ele\u00adgance of Wand s work is that he recognized \nthat \u00df-convertibility induces bisimilarity; in the conclusion he admits that almost all the required \nreasoning is done in the .-caculus and as such, he can re-use work already done in sequential compiler \ncorrectness. Unlike Wand s work, our vision of correctness is purely syntax\u00addirected: the translation \nitself (if the JIT tracing can be seen as such) becomes a non-instantaneous process since we have to \nspell it out in the operational semantics. This is what makes the enterprise non\u00adtrivial. Our notion \nof convertibility informally becomes something akin to store-convertiblity , but this is much less powerful \nthan \u00df\u00adconvertibility as it does not directly imply bisimilarity. We also do not have the luxury of bringing \nto bear the entirety of the .-calculus machinery, so our technique here, while still using bisimulations, \nis at once more basic and less elegant. There is also a breadth of literature exploring using bisimulation \nto show program equivalence by way of contextual equivalence in Pierce et al., Lassen et al., and Wand \net al. [10, 17 19]. Though the topics of their speci.c investigations differ, they all concern themselves \nwith using bisimulation as a more tractable proof tech\u00adnique to prove contextual equivalence without \nhaving to universally quantify over all contexts. Their setting is higher-ordered, modeled within the \n.-calculus. Pierce et al. [19], for instance, aims to prove bisimiliarity sound and complete with respect \nto contextual equiv\u00adalence for a modi.ed .-calculus with recursive types. Wand [10] aims to improve the \nproof technique and reasons about a .-calculus extended with explicit stores. These works are basic investigations \ninto the nature of the proof technique. Ours is an application of the technique to prove equivalence \nof a dynamically transformed program. We also arrived at bisimiliarity by an entirely different motivation, \nthat of proving determinism of a JIT compiler that per\u00adforms the dynamic transform. We are not met with \nthe dif.culty of universally quantifying contexts; in fact, we .x our correctness to hold for one context \nonly. Myreen s method of creating formally correct JIT compilers for x86 [15] is at the much lower level \nof abstraction: machine language. They use Hoare logic, and so still retain a .avor of the denotational. \nWe are much farther from the bare metal than they are. 6. Conclusions and Future Work We have demonstrated \na paradigm for high-level, purely opera\u00adtional correctness of the tracing JIT compilation technique via \nbisimulation and con.uence. Unlike traditional ahead-of-time com\u00adpiler correctness where the translation \nprocess from the source lan\u00adguage to the target language is an opaque function, trace compiler\u00adcorrectness \nrequires the translation the tracing to be spelled out explicitly. We overcome this dif.culty by using \nbisimulations, though we strive to maintain continuity with existing purely opera\u00adtional correctness \napproaches by returning to con.uence. We hope that the theoretical framework we have provided will prove \nuseful in reasoning about trace compilers at a high level. We hope that we have opened up a wealth of \npossible future research in the foundational differences betweens traditional and trace opti\u00admizations. \nThough a different problem, we also feel applying the trace compilation technique to an applicative setting, \nnamely the .-calculus, will be a worthy venture. It is also interesting to further explore O and the \nquestion of just what exactly is observable in computation. We also hope to look at deriving tools from \nthe tech\u00adniques described here in the future. Acknowledgements. We thank Michael Bebenita and the Mozilla \nJavaScript team for enlightening discussions about the implementa\u00adtion of trace compilers. We also thank \nJonathan Lee, Oren Freiberg, Kannan Goudan, Dave Herman, Dimitris Vardoulakis, and the anonymous reviewers \nfor draft reading and helpful discussions. References [1] Vasanth Bala, Evelyn Duesterwald, and Sanjeev \nBanerjia. Dynamo: A transparent dynamic optimization system. In PLDI 00, pages 1 12. ACM, 2000. [2] Michael \nBebenita, Florian Brandner, Manuel Fahndrich, Francesco Logozzo, Wolfram Schulte, Nikolai Tillmann, and \nHerman Venter. SPUR: A trace-based JIT compiler for CIL. In OOPSLA 10, 2010. [3] Michael Bebenita, Mason \nChang, Gregor Wagner, Christian Wimmer, Andreas Gal, and Michael Franz. Trace-based compilation in execu\u00adtion \nenvironments without interpreters. In PPPJ 10, 2010. [4] Mason Chang, Edwin W. Smith, Rick Reitmaier, \nMichael Bebenita, Andreas Gal, Christian Wimmer, Brendan Eich, and Michael Franz. Tracing for web 3.0: \ntrace compilation for the next generation web applications. In VEE, pages 71 80, 2009. [5] Jo\u00a8elle Despeyroux. \nProof of translation in natural semantics. In LICS, pages 193 205, 1986. [6] Cormac Flanagan and Matthias \nFelleisen. The semantics of future and its use in program optimization. In POPL 95, pages 209 220. ACM, \n1995. [7] Andreas Gal. Ef.cient bytecode veri.cation and compilation in a virtual machine. PhD thesis, \n2006. Adviser: Michael Franz. [8] Andreas Gal, Brendan Eich, Mike Shaver, David Anderson, David Mandelin, \nMohammad R. Haghighat, Blake Kaplan, Graydon Hoare, Boris Zbarsky, Jason Orendorff, Jesse Ruderman, Edwin \nW. Smith, Rick Reitmaier, Michael Bebenita, Mason Chang, and Michael Franz. Trace-based just-in-time \ntype specialization for dynamic languages. In PLDI 09, pages 465 478. ACM, 2009. [9] A. J. Kfoury, Michael \nA. Arbib, and Robert N. Moll. A Programming Approach to Computability. Springer-Verlag, 1982. [10] Vasileios \nKoutavas and Mitchell Wand. Small bisimulations for rea\u00adsoning about higher-order imperative programs. \nIn POPL 06, pages 141 152. ACM, 2006. [11] David Lacey, Neil D. Jones, Eric Van Wyk, and Carl Christian \nFred\u00aderiksen. Proving correctness of compiler optimizations by temporal logic. In POPL 02, pages 283 \n294. ACM, 2002. [12] Sorin Lerner, Todd Millstein, and Craig Chambers. Automatically proving the correctness \nof compiler optimizations. In PLDI 03, pages 220 231. ACM, 2003. [13] Mozilla Metrics. Firefox usage: \nhttps://metrics.mozilla.com/. [14] Robin Milner. Communication and Concurrency. Prentice Hall, 1995. \n[15] Magnus O. Myreen. Veri.ed just-in-time compiler on x86. In POPL 10, pages 107 118. ACM, 2010. [16] \nFrank Pfenning. A proof of the Church-Rosser theorem and its rep\u00adresentation in a logical framework. \nJournal of Automated Reasoning, 1993. [17] Kristian St\u00f8vring and Soren B. Lassen. A complete, co-inductive \nsyntactic theory of sequential control and state. In POPL 07, pages 161 172. ACM, 2007. [18] Eijiro Sumii \nand Benjamin C. Pierce. A bisimulation for dynamic sealing. In POPL 04, pages 161 172. ACM, 2004. [19] \nEijiro Sumii and Benjamin C. Pierce. A bisimulation for type abstrac\u00adtion and recursion. In POPL 05, \npages 63 74. ACM, 2005. [20] Mitchell Wand. Compiler correctness for parallel languages. In FPCA, pages \n120 134, 1995. [21] Mitchell Wand and William D. Clinger. Set constraints for destruc\u00adtive array update \noptimization. Journal of Functional Programmng, 11(3):319 346, 2001. [22] Mitchell Wand and Igor Siveroni. \nConstraint systems for useless variable elimination. In POPL 99, pages 291 302. ACM, 1999.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>The technique of trace-based just-in-time compilation was introduced by Bala et al. and was further developed by Gal et al. It currently enjoys success in Mozilla Firefox's JavaScript engine. A trace-based JIT compiler leverages run-time profiling to optimize frequently-executed paths while enabling the optimized code to ``bail out'' to the original code when the path has been invalidated. This optimization strategy differs from those of other JIT compilers and opens the question of <i>which trace optimizations are sound</i>. In this paper we present a framework for reasoning about the soundness of trace optimizations, and we show that some traditional optimization techniques are sound when used in a trace compiler while others are unsound. The converse is also true: some trace optimizations are sound when used in a traditional compiler while others are unsound. So, traditional and trace optimizations form incomparable sets. Our setting is an imperative calculus for which tracing is explicitly spelled out in the semantics. We define optimization soundness via a notion of bisimulation, and we show that sound optimizations lead to confluence and determinacy of stores.</p>", "authors": [{"name": "Shu-yu Guo", "author_profile_id": "81479643835", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P2509699", "email_address": "shu@cs.ucla.edu", "orcid_id": ""}, {"name": "Jens Palsberg", "author_profile_id": "81100375570", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P2509700", "email_address": "palsberg@cs.ucla.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926450", "year": "2011", "article_id": "1926450", "conference": "POPL", "title": "The essence of compiling with traces", "url": "http://dl.acm.org/citation.cfm?id=1926450"}