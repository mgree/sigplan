{"article_publication_date": "01-26-2011", "fulltext": "\n Expressive Modular Fine-Grained Concurrency Speci.cation Bart Jacobs * Frank Piessens DistriNet Research \nGroup, Department of Computer Science, Katholieke Universiteit Leuven, Belgium {bart.jacobs,frank.piessens}@cs.kuleuven.be \nAbstract Compared to coarse-grained external synchronization of operations on data structures shared \nbetween concurrent threads, .ne-grained, internal synchronization can offer stronger progress guarantees \nand better performance. However, fully specifying operations that per\u00adform internal synchronization modularly \nis a hard, open problem. The state of the art approaches, based on linearizability or on concurrent abstract \npredicates, have important limitations on the expressiveness of speci.cations. Linearizability does not \nsupport ownership transfer, and the concurrent abstract predicates-based speci.cation approach requires \nhardcoding a particular usage pro\u00adtocol. In this paper, we propose a novel approach that lifts these \nlimitations and enables fully general speci.cation of .ne-grained concurrent data structures. The basic \nidea is that clients pass the ghost code required to instantiate an operation s speci.cation for a speci.c \nclient scenario into the operation in a simple form of higher-order programming. We machine-checked the \ntheory of the paper using the Coq proof assistant. Furthermore, we implemented the approach in our program \nveri.er VeriFast and used it to verify two challenging .ne\u00adgrained concurrent data structures from the \nliterature: a multiple\u00adcompare-and-swap algorithm and a lock-coupling list. Categories and Subject Descriptors \nF.3.1 [Logics and Mean\u00adings of Programs]: Specifying and Verifying and Reasoning about Programs Speci.cation \ntechniques General Terms Veri.cation Keywords .ne-grained concurrency, separation logic 1. Introduction \n34 years after Owicki and Gries (O&#38;G) proposed their resource\u00adinvariants-based (RI) method [12] and \ntheir interference-freedom\u00adchecks-based (IF) method [13] for the veri.cation of parallel pro\u00adgrams, doing \nso fully modularly is still an area of active research. For parallel programs, two kinds of modularity \ncan be distin\u00adguished: thread-modularity and procedure-modularity. Thread-modularity means that each \nthread can be veri.ed sepa\u00adrately, under a well-de.ned, concise set of assumptions on its envi\u00adronment. \nThe RI method satis.es this criterion, since the resource invariants are the only shared assumptions \namong the threads. The * This work was performed while Bart Jacobs was a Postdoctoral Fellow of the Research \nFoundation -Flanders (FWO) Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 \nACM 978-1-4503-0490-0/11/01. . . $10.00 IF method does not, since it requires each command of each thread \nto be checked for interference with each command of each other thread. The latter problem was solved \nby the rely-guarantee (RG) method [9], which summarizes each thread s interference assump\u00adtions and guarantees \nin a single rely, resp. guarantee condition. Procedure-modularity means that each procedure, or each \ngroup of procedures that cooperate to implement an abstract data type, can be veri.ed separately, again \nunder a well-de.ned, concise set of assumptions on its environment that performs proper abstraction over \nimplementation aspects. Neither the RI method nor RG satisfy this criterion. RI, because it requires \nauxiliary variable annotations that break modularity, as we will show; and RG, because it does not allow \nas-if-atomic operations on data structures to be treated just like atomic machine instructions, although \nthis has been addressed recently with work on linearizability-based veri.cation [6, 15]. In this paper, \nwe propose an extension of the RI method that achieves procedure-modularity. The basic idea is simple: \nat each procedure call, the auxiliary variable updates required to enable veri.cation of the client program \nare passed into the procedure as an extra argument. Correspondingly, the procedure s speci.cation is \nparameterized by a precondition and a postcondition for the updates, and imposes a correctness condition \non the updates in the form of a Hoare triple. We .rst describe the approach informally in the context \nof an informal extension of the RI method with procedures. However, since the RI method imposes syntactic \nrestrictions on which threads may mention speci.c variables, an extension with procedures re\u00adquires more \ninvolved bookkeeping of variable occurrences. These details are not very interesting, and we do not develop \nthis setting formally; rather, our formal system, like our implementation, uses separation logic [11, \n14], where these issues do not occur. We implemented the approach in our program veri.er VeriFast and \nveri.ed two challenging .ne-grained concurrent data structures from the literature: a multiple-compare-and-swap \n(MCAS) algo\u00adrithm [5], and a lock-coupling list. The remainder of the paper is structured as follows. \nIn \u00a72 we re\u00adcall the RI method. In \u00a73 we show that this method is not procedure\u00admodular. In \u00a74, we informally \npresent our approach for extending RI to achieve procedure-modularity. In \u00a75, we describe how our approach \nmay be lifted to a dynamic setting using separation logic and permission accounting. In \u00a76, we introduce \nghost objects, data structures constructed from auxiliary heap cells that enable par\u00adtial information \nsharing. In \u00a77, we describe how programs that use atomic machine instructions can be encoded straightforwardly \ninto the system of this paper. In \u00a78, we describe our proof of a concur\u00adrent set algorithm. In \u00a79 we \ndescribe the veri.cation tool. Finally, we discuss related work in \u00a710. 2. The Owicki-Gries Method Consider \nthe simple parallel program used by O&#38;G [12] to in\u00adtroduce their resource-invariant-based method, \nreproduced in Fig\u00ad  resource r(x): cobegin with r when true do x := x +1 // with r when true do x := \nx +1 coend Figure 1. A simple parallel program; can be veri.ed with resource invariants and auxiliary \nvariables ure 1. It uses the parallel execution command cobegin S1// . . . //Sn coend to run two threads \nthat each increment variable x. Variable x is protected by resource r: the critical section command with \nr when B do S blocks until B is true and no other thread is using r. In this simple language, data races \ncan be avoided by imposing the following simple syntactic restrictions: An assignment to a variable \nx that belongs to a resource r is allowed only inside a critical section for r.  An occurrence of a \nvariable x outside of a critical section for a resource to which it belongs, if any, is allowed only \nif no other thread modi.es the variable.  O&#38;G propose the following axioms for parallel executions \nand critical sections, similar to the ones proposed by Hoare but with re\u00adlaxed conditions on variable \noccurrences. The axioms assume that an assertion I(r) has been de.ned for each resource r, called the \nresource s resource invariant. Parallel Execution Axiom. If {P1} S1 {Q1} and {P2} S2 {Q2}and ...and {Pn} \nSn {Qn} and no variable free in Pi or Qi is changed in Sj (i j) and all variables in = I(r) belong to \nresource r, then {P1 . \u00b7\u00b7\u00b7 . Pn . I(r)} resource r : cobegin S1// . . . //Sn coend {Q1 .\u00b7 \u00b7\u00b7. Qn . I(r)}. \nCritical Section Axiom. If {I(r) . P . B} S {I(r) . Q}, and I(r) is the invariant from the cobegin command, \nand no variable free in P or Q is changed in another process, then {P } with r when B do S {Q}. We would \nlike to prove that if x =0 before the program exe\u00adcutes, then x =2 after the program terminates. As O&#38;G \npoint out, this property cannot be veri.ed using the above axioms directly. They propose to augment the \nprogram with auxiliary variables y and z, that track each thread s contribution to the value of x. Fig\u00adure \n2 shows the proof outline given by O&#38;G for the augmented program. Notice that the auxiliary variables \nare mentioned in assertions outside of the with do commands, even though they are protected by resource \nr; this is allowed provided that the thread that mentions a given variable is the only one that modi.es \nthat variable, per the Parallel Execution Axiom. To regulate reasoning using auxiliary variables, O&#38;G \npropose auxiliary variable sets and the Auxiliary Variable Axiom. De.nition. A set AV of program variables \nis an auxiliary variable set for a given program if variables in AV appear in the program only in assignments \nto variables in AV. Auxiliary Variable Axiom. If AV is an auxiliary variable set for S, let S' be obtained \nfrom S by deleting all assignments to variables in AV. Then if {P } S {Q} is true and P and Q do not \nrefer to any variables from AV, {P } S' {Q} is also true. 3. The Procedure-Modularity Problem Now consider \nagain the un-augmented program of Figure 1. Sup\u00adpose we wish to encapsulate x and the operation on it, \ni.e. the in\u00ad {x =0} begin y := 0; z := 0; {y =0 . z =0 . I(r)} resource r(x, y, z): cobegin {y =0} with \nr when true do {y =0 . I(r)} begin x := x + 1; y := 1 end {y =1 . I(r)} {y =1} // {z =0} with r when \ntrue do {z =0 . I(r)} begin x := x + 1; z := 1 end {z =1 . I(r)} {z =1} coend {y =1 . z =1 . I(r)} end \n{x =2} I(r)= {x = y + z} Figure 2. Owicki and Gries proof of the program of Figure 1. y and z are auxiliary \nvariables. procedure incr() do x := x +1 resource r(x): cobegin with r when true do incr() // with r \nwhen true do incr() coend (a) External synchronization procedure incr(r) do with r when true do x := \nx +1 resource r(x): cobegin incr(r) // incr(r) coend (b) Internal synchronization Figure 3. Two modularized \nversions of the program of Figure 1 crement operation, into a separate module. (Note: neither the pro\u00adgramming \nlanguage of O&#38;G, nor their proof system, support proce\u00addures, since both impose syntactic conditions \non the threads where variables occur, and this is not well-de.ned in the presence of pro\u00adcedures. In \nthis section and the next, we informally introduce our approach, while glossing over this issue. In Section \n5 we present our approach formally, using heap cells instead of global variables, thus eliminating this \nissue.) There are two ways to do this: using external synchronization (Figure 3a) and using internal \nsynchro\u00adnization (Figure 3b). In the version that uses external synchronization, the module is easy to \nspecify: procedure incr satis.es the following speci.ca\u00adtion: {x = X} incr() {x = X +1} where X is a \nlogical variable; the speci.cation holds for all values of X. Using this speci.cation, both module and \nclient program are easy to verify; the proof outline of Figure 2 is mostly unchanged. The reason is that \nthe auxiliary variables can be added in the client program; no augmentation of the module is required. \n procedure incr(r, .) do with r when true do begin x := x + 1; . end begin y := 0; z := 0; resource \nr(x, y, z): cobegin incr(r, y := 1) // incr(r, z := 1) coend end Figure 4. The program of Figure 3b, \naugmented per our approach For the version that uses internal synchronization, this is not the case. \nThe updates of y and z need to be added inside the with do command, but this command is in the module \nand furthermore different updates need to be added for different call sites. One might then wonder whether \ninsisting on internal synchronization is worthwhile; it is, because delegating synchronization to the \nmodule allows the module to perform .ne-grained synchronization, for example by acquiring locks multiple \ntimes for smaller amounts of time, or by using atomic machine instructions such as compare\u00adand-swap. \nOne can easily see that verifying this program with the Owicki-Gries method is impossible. Indeed, consider \nany augmentation of the program with auxiliary variable assignments, and any proof outline for the augmented \nprogram. Since the assignments inside the critical section occur in both threads, no variable modi.ed \ninside the critical section may be mentioned by any thread s proof outline outside the critical section. \nTherefore, removing the critical section from the program does not invalidate the proof outline. Consequently, \nthe proof outline cannot verify the triple {x = 0}\u00b7{x =2}. 4. Achieving Procedure-Modularity In order \nto enable a modular speci.cation of the module of Fig\u00adure 3b, we propose to augment the program not just \nwith auxiliary variables, but with a simple form of higher-order programming to allow the client program \nto pass auxiliary variable updates into the module. Speci.cally, we augment procedure incr with a parame\u00adter \n. that ranges over commands, and its body so that it executes . after the update of x inside the critical \nsection. In the client pro\u00adgram, at each call of incr, the appropriate auxiliary variable update is speci.ed \nas the value for parameter .. The augmented program is shown in Figure 4. The speci.cation of incr is \nnow more involved: x/. FV (P, U, Q) P . I(r) . U(x + 1) {U(x)} . {Q . I(r)} {P } incr(r, .) {Q} The \nspeci.cation is universally quanti.ed over the predicates P , Q, and U; it can be instantiated with appropriate \npredicates at each call site. Notice also that the speci.cation is generic in the resource invariant. \nThe resource invariant for the resource that protects a .ne-grained concurrent data structure is chosen \nby the client of the data structure. This enables the client to specify the relationship between the \nstate of the data structure and the auxiliary variables introduced by the client. It is important to \npoint out that although the speci.cation of incr looks like a proof rule, it is not part of the proof \nsystem and it does not affect the soundness of the proof system. Rather, it is a derived proof rule that \nmust be veri.ed starting from the proof rules of the proof system. {P } with r when true do begin {P \n. I(r)} {U(x + 1)} x := x + 1; {U(x)} . {Q . I(r)} end {Q} Figure 5. Proof outline for procedure incr \n{x =0} begin y := 0; z := 0; {y =0 . z =0 . I(r)} resource r(x, y, z): cobegin {y =0} incr(r, y := 1) \nwith P = y = 0; Q = y = 1; U(X) = X =1+ z {y =1} // {z =0} incr(r, z := 1) with P = z = 0; Q = z = 1; \nU(X) = X = y +1 {z =1} coend {y =1 . z =1 . I(r)} end {x =2} I(r)= {x = y + z} Figure 6. Proof outline \nfor the client program It is easy to see that the implementation of incr satis.es the speci.cation; a \nproof outline is shown in Figure 5. The proof of the client program is equally easy; see the proof outline \nin Figure 6. 5. Formal System We presented our approach informally in the preceding sections. In order \nto achieve a well-de.ned approach, we need to resolve the problem of O&#38;G s syntactic restrictions \non which threads may mention which variables; these are not compatible with procedures. To do so, we \nmove to a programming language without global variables, where threads share data only through the heap; \nand we use separation logic to reason about such programs. Speci.cally, we adopt the programming language \nand program logic of Gotsman et al. [4] for storable locks and threads, with a few modi.cations: We \nadd support for auxiliary heap cells and passing closed commands into procedures as argument values. \n We do not treat local variables as resources.  A translation of the modularized Owicki-Gries example \nwith internal synchronization of Figure 3 (b) to the more dynamic pro\u00adgramming language is shown in Figure \n7. The program consists of a procedure incr and a main program. The main program allo\u00adcates two consecutive \nmemory cells and it initializes the .rst one (at address e) for use as a lock, and releases it. (After \na thread ini\u00adtializes a lock, it initially holds it.) The second cell (at address e+1) corresponds to \nthe global variable x in the original program. The program then starts two threads, both of which increment \nthe sec\u00ad  procedure incr(e)= acquire(e); r := [e + 1]; [e + 1] := r + 1; release(e) e := cons(1, 0); \ninit(e); release(e); t1 := fork incr(e); t2 := fork incr(e); join(t1); join(t2); acquire(e); .nalize(e) \nFigure 7. The Owicki-Gries example, translated into the dynamic programming language ond cell, under \nprotection of the lock. Finally, it joins both threads, acquires the lock, and decommissions it. (A thread \nmay only de\u00adcommission locks that it holds.) We wish to prove that when the program terminates, we have \ne +1 . 2 * true. The original proof by O&#38;G used auxiliary global variables. In this section, we use \nauxiliary heap cells instead. Speci.cally, with each allocated real heap cell, say at address e, we associate \nan in.nite number of auxiliary heap cells, whose address is given by a pair of integers e.e ', where \ne is the real address and e ' is the ghost offset.1 In the example, we use the auxiliary heap cells at \naddresses e.0 and e.1 to track the contributions of thread 1 and 2 to the value of the cell at e +1, \ncorresponding to auxiliary variables y and z in the original proof. The proof system of Gotsman et al. \nrequires that a tag A be associated with each lock, and a lock invariant IA with each tag. We will associate \nthe tag mylock with the lock of the example, and the following lock invariant with the tag: Imylock(e)= \n1/21/2 .C0,C1 e.0 . C0 * e.1 . C1 * e +1 . C0 + C1 The lock invariant corresponds closely to the resource \ninvariant of the original example. It states that the value of e +1 is the sum of the value of e.0 and \ne.1. In the original proof, syntactic restrictions ensured that auxiliary variable y could be modi.ed \nonly inside a critical section and only by the .rst thread; in the current proof, fractional permissions \n[1] achieve the same goal. Speci.cally, one half of the permission for each auxiliary heap cell becomes \nowned by the lock; the other half is retained by the corresponding thread. As in the previous section, \nto verify procedure incr, we start by augmenting it with an auxiliary parameter . that ranges over aux\u00adiliary \ncommands, and by augmenting its body with an occurrence of . after the update of e +1 but before the \nlock is released. As before, this parameter will serve to perform the auxiliary state up\u00addates required \nto preserve the lock invariant. The speci.cation of procedure incr enforces that it does so: IA(e) * \nP ..X e +1 . X * U(X) .X {e +1 . X +1 * U(X)} . {IA(e) * Q} {pA(e) * P } incr(e, .) {pA(e) * Q} The \nspeci.cation is universally quanti.ed over the address e of the lock, the tag A of the lock, the fraction \np of the lock permission available to the procedure (any fraction will do), an additional pre\u00adcondition \nP and postcondition Q, and a predicate U(X), parame\u00adterized over an integer X, that describes the resources \n(speci.cally, the auxiliary heap cells) owned by the lock besides the heap cell at address e +1, and \nstates that those resources are in a state corre\u00adsponding to value X of the heap cell at address e +1. \nThe speci.cation has two premises. The .rst one states that the lock invariant IA(e) combined with the \nadditional precondition P implies full permission to access the heap cell at address e +1, plus 1 We \nuse the terms auxiliary and ghost interchangeably. some extra state U(X), where X is the value of the \nheap cell. The second premise states the correctness of the parameter .: it states that executing command \n. must re-establish the lock invariant after the update of e +1, and the remaining state must satisfy \nQ. We again point out that this speci.cation has the shape of a proof rule, but is not part of the proof \nsystem; as always when ver\u00adifying programs with procedures, the speci.cations of the proce\u00addures must \nbe derived using the proof rules of the system as part of the veri.cation of the program. A proof outline \nof the program is shown in Figure 8. Notice the following: The release of the lock consumes the lock \ninvariant.  A thread speci.cation and thread speci.cation arguments are associated with each fork operation \nfor veri.cation purposes. A fork with thread speci.cation t and arguments n con\u00adsumes the precondition \nof t and produces a thread permission tidt (t, n), where t is the thread identi.er.  Joining a thread \nconsumes the thread permission and produces the thread speci.cation s postcondition.  The acquisition \nof the lock produces the lock invariant. Merging the fractions of the auxiliary heap cells yields full \ninformation about e +1, per the following law:  1/21/2 (a . v *.C a . C * P (C)) . a . v * P (v) 5.1 \nProgramming Language The syntax of arithmetic expressions e, boolean expressions b, and commands c is \ngiven below. All commands return a value. Local variables are scoped, using let commands. The syntax \nx := c; y := ''' ''' c ; c is syntactic sugar for let x := c in let y := c in c . We assume a global \ntable pdef of procedure de.nitions. The re\u00adcursion operator (\u00b5f(x) c)(e) applies the recursive function \nf with parameters x and body c to arguments e. The scope of f is c, minus any command expressions in \nc. The syntax letrec f(x)= c in c ' is syntactic sugar for c ' [(\u00b5f(x) c)/f]. All substitutions are capture-avoiding. \nA command is closed if it has no free variables x . Vars and no free functions f . FuncNames. We assume \na bi\u00adjective encoding L\u00b7J of closed commands into integers. A command expression c denotes the encoding \nof c as an integer. A closure ex\u00adecution command exec(e) executes the closed command obtained by decoding \nthe value of e. In the examples, we simply write . instead of exec(.). That is, a variable name used \nas a command denotes a closure execution. n . Z,x . Vars,p . ProcNames,f . FuncNames e ::= n | x | e \n+ e | e - e | c b ::= e = e | e<e c ::= cons(e) | gcons(e) | [e] | [e.e] | [e] := e | [e.e] := e | dispose(e) \n| if b then c else c | return e | p(e) | exec(e) | let x := c in c | (\u00b5f(x) c)(e) | f (e) | fork c | \njoin(e) | initA(e) | acquire(e) | release(e) | .nalize(e) pdef ::= procedure p(x)= c The evaluation [e] \nof a closed expression e is de.ned as follows: ' '' ' [n] = n [e+e ] = [e]+[e ][e-e ] = [e]-[e ][c] = \nLcJ We de.ne a small-step interleaving semantics. A con.guration consists of a real heap h, a ghost heap \ng, and a thread map T . A real heap is a .nite partial function from positive integers to integers. A \nghost heap is a partial function from pairs of integers to integers. A thread map is a .nite partial \nfunction from thread identi.ers to closed continuations. The continuations . and contexts . are  procedure \nincr(e, .)= {pA(e) * P } acquire(e); {pA(e) * lockedA(e) * IA(e) * P } {pA(e) * lockedA(e) * e +1 . X \n* U(X)} r := [e + 1]; [e + 1] := r + 1; {pA(e) * lockedA(e) * e +1 . X +1 * U(X)} .; {pA(e) * lockedA(e) \n* IA(e) * Q} release(e), {pA(e) * Q} threadspec thread1 (e) 1/2 req 21 mylock(e) * e.0 . 0 1/2 ens 12 \nmylock(e) * e.0 . 1 threadspec thread2 (e) 1/2 req 12 mylock(e) * e.1 . 0 1/2 ens 12 mylock(e) * e.1 \n. 1 {emp} e := cons(1, 0); {e . 1 * (\u00ae\u00a3,.Ne.e ' . 0) * e +1 . 0 * (\u00ae\u00a3,.N(e + 1).e ' . 0)}{e . 1 * e.0 \n. 0 * e.1 . 0 * e +1 . 0 * true} initmylock(e); release(e); 1/21/2 {mylock(e) * e.0 . 0 * e.1 . 0 * true} \nt1 := fork { 11/2 mylock(e) * e.0 . 0} 2 incr(e, [e.0] := 1); 1/2 with U(X)= e.0 . 0 * e.1 . X 1/2 { \n1 mylock(e) * e.0 . 1} 2 1/2 { 1 mylock(e) * e.1 . 0 * tidthread1 (t1,e) * true} 2 t2 := fork 1/2 { \n1 mylock(e) * e.1 . 0} 2 incr(e, [e.1] := 1); 1/2 with U(X)= e.0 . X * e.1 . 0 1/2 { 1 mylock(e) * e.1 \n. 1} 2 {tidthread1 (t1,e) * tidthread2 (t2,e) * true} join(t1); 1/2 { 1 mylock(e) * e.0 . 1 * tidthread2 \n(t2,e) * true} 2 join(t2); 1/21/2 {mylock(e) * e.0 . 1 * e.1 . 1 * true} acquire(e); .nalize(e) 1/21/2 \n{(.C0,C1 e.0 . C0 * e.1 . C1 * e +1 . C0 + C1) 1/21/2 * e.0 . 1 * e.1 . 1 * e .* true} {e.0 . 1 * e.1 \n. 1 * e +1 . 2 * e .* true} Figure 8. Proof outline for the example program de.ned as follows: . ::= \nc; . | n; . . ::= let x := [ ] in c; . | done The step relation . is de.ned in Figure 9. In the step \nrules, symbols n match not just integer literals but other closed expressions as well, and denote their \nvalue. Notice that locks are implemented as a single heap cell that holds either the value 0, if the \nlock is not held, or the value 1, if the lock is held. We omit rules for init and .nalize; we de.ne them \nas equivalent to return 0 (i.e., a no-op) for purposes of the step relation. Throughout, f[x := y] denotes \nfunction update; i.e. f[x := y](x)= y and f[x := y](z)= f(z) for z = x.  5.2 Simple Closures We say \na program has simple closures if there exists a partitioning of procedure parameters into closure parameters \nand non-closure parameters such that all exec commands are of the form exec(x) where x is a closure parameter, \nand all procedure call argument expressions for closure parameters are either command expressions or \nclosure parameters. Applying the speci.cation approach of this paper requires only simple closures. As \nwe will see, simple closures admit a very simple proof system. 5.3 Proof System The correctness of a \ncommand c is expressed in the form of a correctness judgment G f{P } c {Q}, where G is a function environment \nand P and Q are assertions. An assertion describes a set of permissions. The set of permissions is de.ned \nas follows: p ::= e . v | e.e ' . v | A(e) | lockedA(e) | tidt (t, v) A permission bundle is a total \nfunction from permissions to real numbers between 0, inclusive and 1, inclusive. We identify asser\u00adtions \nwith sets of permission bundles. That is, we treat assertions semantically. We denote the empty permission \nbundle (that maps all permissions to 0) as 0. We de.ne some syntax for assertions: emp = {0} p e . v \n= {0[e . v := p]} p e.e ' . v = {0[e.e ' . v := p]} pA(e)= {0[A(e) := p]} ptidt (t, v)= {0[tidt (t, v) \n:= p]} P * Q = {b |.b1,b2 b = b1 + b2 . b1 . P . b2 . Q} (.X P (X)) = {b |.X b . P (X)} (\u00aei.N P (i)) \n= {b |.B b =SiB(i) ..i B(i) . P (i)} where b =SiB(i) . (.p, e .n .i>n |b( p) - S0=j=iB(j)( p)| <e) \nWe say a permission bundle is consistent if there are no two points-to permissions with the same address \nand different values that both map to non-zero coef.cients. We say one assertion A implies another one \nA ', written A . A ', if for every consistent bundle b . A, we have b . A ' . The correctness judgment \nis de.ned inductively by the rules shown in Figure 10. Notice that the proof rules for procedure calls \nand for closure executions simply require the correctness of the procedure or clo\u00adsure s body. It follows \nthat a procedure that calls another procedure, or that executes a closure, does not, in isolation, have \na closed proof tree. Rather, its proof tree is parameterized by the proof trees for the procedures called \nand closures executed. This simple approach is suf.cient if the program has simple closures and an acyclic \npro\u00adcedure call graph. Indeed, in that case, given a main command, one can inline all procedure calls \nto obtain an equivalent command that contains no procedure call or closure execution commands; the shape \nof the proof tree for the original main command will re.ect the shape of the main command after inlining. \n 5.4 Soundness We sketch how one can prove soundness of the program logic used. More details are in \nthe technical report [7]; also, a machine-checked proof is at http://www.cs.kuleuven.be/ bartj/.negrained/. \n Continuation step rules for relation (h, g, .)(h ' ,g ' ,. ')| abort (h, g, cons(n1,...,nm); .)(h .{e \n. n1,...,e + m - 1 . nm},g .{(e1,e2) . 0 | e = e1 <e + m},e; .) if 0 <e and {e, . . . , e + m - 1}n dom(h)= \n\u00d8 '' '' (h, g, gcons(n); .)(h, g .{(0,e ) . n},e ; .) if 0 <e . (0,e ) ./dom(g) (h, g, [n]; .) if n . \ndom(h) then (h, g, h(n); .) else abort (h, g, [n.n ' ]; .) if (n, n ' ) . dom(g) then (h, g, g((n, n \n' )); .) else abort (h, g, [n] := v; .) if n . dom(h) then (h[n := v], g, 0; .) else abort ' '' (h, g, \n[n.n ] := v; .) if (n, n ) . dom(g) then (h, g[(n, n ) := v], 0; .) else abort (h, g, dispose(n); .) \nif n . dom(h) then (h \\dom {n},g \\dom {(n, e ' )|true}, 0; .) else abort (h, g, if b then c else c ' \n; .) if b = true then (h, g, c; .) else (h, g, c ' ; .) (h, g, p(v); .)(h, g, c[v/x]; .) if procedure \np(x)= c (h, g, exec(LcJ); .)(h, g, c; .) (h, g, return n; .)(h, g, n; .) (h, g, let x := c in c ' ; .)(h, \ng, c; let x := [] in c ' ; .) (h, g, (\u00b5f(x) c)(v); .)(h, g, c[(\u00b5f (x) c)/f, v/x]; .) (h, g, acquire(n); \n.)(h[n := 1], g, 0; .) if (n, 0) . h (h, g, acquire(n); .) abort if n/. dom(h) (h, g, release(n); .) \nif n . dom(h) then (h[n := 0], g, 0; .) else abort (h, g, v; let x := [] in c; .)(h, g, c[v/x]; .) Thread \nmap step rules for relation (h, g, T )(h ' ,g ' ,T ')| abort (h ' (h '' (h, g, T ) ,g ' ,T [t := . ' \n]) if (t, .) . T and (h, g, .) ,g ,. ') (h, g, T ) abort if (t, .) . T and (h, g, .) abort (h, g, T )(h, \ng, T [t := t ' ; ., t ' := c; done]) if (t, fork c; .) . T and t ' ./dom(T ) (h, g, T )(h, g, T [t := \nv; .] \\dom {t '}) if (t, join(t ' ); .) . T and (t ' ,v; done) . T (h, g, T ) abort if (t, join(n); .) \n. T and n/. dom(T ) Figure 9. Step rules for continuations . and thread maps T . Note: init and .nalize \nare no-ops. {emp} r := cons(v0,...,vn) {\u00aei r + i . vi * (\u00ae\u00a3,.N (r + i).e ' . 0)}{emp} r := gcons(v) {0.r \n. v} pp' p'' p {e . v} r := [e] {e . v . r = v}{e.e . v} r := [e.e ] {e.e . v . r = v}{e .} [e] := v \n{e . v} '''' {e.e .} [e.e ] := v {e.e . v}{e .* \u00ae\u00a3,.Ne.e .} dispose(e) {emp}{P } r := return v {P . \nr = v} G f{P . b} c {Q} G f{P .\u00acb} c ' {Q} procedure p(x)= c {P } c[v/x] {Q}{P } c {Q} G f{P } if b then \nc else c ' {Q}{P } p(v) {Q}{P } exec(c) {Q} G f{P } r := c {Q(r)}.X G f{Q(X)} c ' [X/x] {Q ' } threadspec \nt (x) req P ens Q G f{P [v/x]} c {Q[v/x]} G f{P } let x := c in c ' {Q ' } G f{P [v/x]} r := fork c {tidt \n(r, v)} G, {P } f(x) {Q}f{P } c {Q} threadspec t(x) req P ens Q .X G f{P (X)} c {Q(X)} G f{P [v/x]} \n(\u00b5f(x) c)(v) {Q[v/x]}{tidt (t, v)} join(t) {Q[v/x]} G f {.X P (X)} c {.X Q(X)} G, {P } f(x) {Q}, G \n' f{P [v/x]} f (v) {Q[v/x]}{s . 1} initA(s) {A(s) * lockedA(s)} {P } c {Q} {pA(s)} acquire(s) {pA(s) \n* lockedA(s) * IA(s)}{lockedA(s) * IA(s)} release(s) {emp} G f{P } c {Q} '' ' P . P G f{P } c {Q} Q \n. Q G f{P } c {Q} {A(s) * lockedA(s)} .nalize(s) {s .} G f{P } c {Q ' } G f{P * R} c {Q * R} Figure \n10. Proof rules The soundness theorem states that if for a command c we have about whether it is the \nweakest precondition or not). We prove {emp} c {true}, then (\u00d8, \u00d8, {(t, c; done)}) * abort for any thread \nidenti.er t. (G f{P } c {Q}) . (P . validG(c, Q)) We do not de.ne the semantics of the correctness judgment \ndirectly. Rather, we de.ne an assertion transformer validG(c, Q) (similar to a weakest precondition \noperator, but we don t worry We can then de.ne validity of a con.guration. A con.guration (h, g, T ) \nis valid iff there exists a set of permissions P such that  for every (e, e ' ) with 0 <e, if (e, e \n' ) . dom(g) then e . dom(h)  h equals the set of non-ghost points-to permissions in P plus one element \ne . 0 or e . 1 for each A(e) permission in P  g equals the set of ghost points-to permissions in P \n there is exactly one tidt (t, v) permission for each thread t . dom(T ), and  there is exactly one \nlockedA(e) permission for each A(e) per\u00admission whose corresponding heap element equals 1, and  there \nexists a permission bundle bt for each thread t, and a permission bundle b\u00a3 for each A(e) permission \nfor which there is no lockedA(e) permission, and a permission bundle bC for the program s environment, \nwhich in particular contains the tid permission for the main thread, such that  the sum of all bt and \nall b\u00a3 and bC equals {( p, 1) | p . P }, and for each thread (t, .) . T , bt . valid(., Q[v/x]) where \nthreadspec t (x) req \u00b7\u00b7\u00b7 ens Q and tidt (t, v) . P . for each lock A(e) . P for which there is no lockedA(e) \n. P , b\u00a3 . IA(e). By correctness of the main program, the initial con.guration is valid. We then prove \nthat each execution step preserves con.gu\u00adration validity. The theorem then follows from the fact that \nabort is not a valid con.guration.  5.5 Ghost erasure After a program is veri.ed, ghost code can be \nremoved without in\u00advalidating the proof. Speci.cally, if all code that is removed is side\u00adeffect-free \nand terminates, then if the program after erasure aborts, the original program aborts. If the program \nhas simple closures and the procedure call graph is acyclic, then non-termination can result only from \nnon-terminating recursive functions. Removed code is side-effect-free if it affects only the ghost heap, \nprovided that all ghost heap accesses are removed. 6. Ghost Objects In the previous section we used fractional \npoints-to assertions to enable a thread to maintain information about a shared object. The location is \nread-only while no thread has full permission, and the thread has full information: it knows the exact \nvalue. Often, proofs require a more .ne-grained type of tracking. A thread needs to maintain partial \ninformation about a value, while allowing other threads to modify the value in ways that preserve all \nthreads assumptions. A general approach to this problem is rely-guarantee reasoning. However, in this \npaper we propose a different strategy. We propose the use of ghost objects. A ghost object is a data \nstructure built from auxiliary heap cells, that represents some mathematical value, and that allows clients \nto obtain handles on the object that represent a condition on the value of the object. Handles represent \npartial information about the object. Correspondingly, they represent the permission to violate the condition, \nin the sense that the object does not allow violating the condition without handing in the handle. A \nbasic ghost object is a ghost bag. The abstract predicate gbag(b, B) represents a ghost bag with identi.er \nb, currently hold\u00ading the bag of integers B. The object provides the following opera\u00adtions: {emp} r := \ncreate gbag() {gbag(r, \u00d8)} {gbag(b, B)} gbag add(b, v) {gbag(b, B l{v}) * gbagh(b, v)} {gbag(b, B) * \ngbagh(b, v)} gbag remove(b, v) {v . B . gbag(b, B -{v})} The predicate gbagh(b, v) represents the knowledge \nthat the ghost bag b currently contains element v. It furthermore represents the permission to remove \nthis element. This ghost object can be implemented in terms of simple auxil\u00adiary heap cells. It does \nnot need to be built into the proof system. A veri.ed ghost bag implementation comes with our veri.cation \ntool (see Section 9). Furthermore, based on ghost bags, a wide variety of ghost objects can be implemented \neasily. 7. Atomic Instructions The programming language of the previous section does not in\u00adclude atomic \nmachine instructions such as atomic compare-and\u00adswap (CAS) instructions, which are available on most \nplatforms. However, one can easily translate a program that uses atomics to a behaviorally equivalent \n(but less ef.cient) program of the formal language that uses locks by introducing an extra lock for each \ndata structure of the program that is accessed using atomics, and then translating the atomic operations \ninto code sequences that acquire the corresponding lock, perform the operation, and then release the \nlock. We will call such a lock an atomic space and its address an atomic space identi.er, ranged over \nby s. (Note: our veri.cation tool supports atomics and atomic spaces directly, and does not re\u00adquire \na translation.) A procedure corresponding to a CAS operation could look as follows, augmented with two \nghost parameters . and . ' for veri.\u00adcation purposes: procedure cas(s, e, o, n, ., . ' )= acquire(s); \nv := [e]; if v = o then ([e] := n; .) else . ' ; release(s); return v We can prove the following speci.cation \nfor it: IA(s) * P ..X e . X * S(X) {S(o) * e . n} . {IA(s) * Q(o)} .X X = o .{S(X) * e . X} . ' {IA(s) \n* Q(X)} {pA(s) * P } r := cas(s, e, o, n, ., . ' ) {pA(s) * Q(r)} We will use this procedure in the \nexamples below, as well as procedures load and store corresponding to atomic loads and stores, respectively, \nspeci.ed as follows: IA(s) * P ..X e . X * S(X) .X {S(X) * e . X} . {IA(s) * Q(X)} {pA(s) * P } r := \nload(s, e, .) {pA(s) * Q(r)} IA(s) * P ..X e . X * S(X) .X {S(X) * e . v} . {IA(s) * Q} {pA(s) * P \n} store(s, e, v, .) {pA(s) * Q} 8. Abstraction: A Concurrent Set In the example of the preceding sections, \nthe data structure be\u00ading manipulated was a simple cell, and its memory representation, e +1 . X, was \ndisclosed in the speci.cation of the operation, incr. In this section, we show that our approach supports \nspeci.\u00adcations that abstract over the representation of the concurrent data structure. Furthermore, we \nshow that the approach allows abstract speci.cation and veri.cation of concurrent data structures built \non top of other concurrent data structures. We do so by .rst showing a speci.cation and implementation \nof a binary semaphore mod\u00adule implemented in terms of atomic machine instructions. We then show a speci.cation, \nimplementation, and proof of a concurrent set module implemented by hand-over-hand locking of a sorted \nlinked list, that uses the semaphore module.  8.1 Semaphore Speci.cation The semaphore module de.nes \nan abstract predicate sema(e, v) which represents a semaphore at address e whose value is v (either 0 \nor 1). The speci.cation of the functions exported by the module can be given in terms of this predicate \nas follows: {e . 0} init sema(e) {sema(e, 0)} IA(s) * P ..v sema(e, v) * S(v) {S(0) * sema(e, 1)} . \n{IA(s) * Q} {pA(s) * P } sema acquire(s, e, .) {pA(s) * Q} IA(s) * P ..v sema(e, v) * S(v) .v {S(v) \n* sema(e, 0)} . {IA(s) * Q} {pA(s) * P } sema release(s, e, .) {pA(s) * Q} {sema(e, )} .nalize sema(e) \n{e .} Actually, to enable sharing of information about the state of a semaphore, the semaphore module \nde.nes a slightly different pred\u00adicate [p]sema(e, v). Just sema(e, v) is shorthand for [1]sema(e, v). \nThe module further exports the following lemmas: [p1 + p2]sema(e, v) . [p1]sema(e, v) * [p2]sema(e, v) \n[p1]sema(e, v) * [p2]sema(e, v ' ) . [p1 + p2]sema(e, v) . v ' = v where p1 and p2 range over positive \nreal numbers. 8.2 Semaphore Implementation The implementation of the semaphore module is straightforward: \np predicate [p]sema(e, v)= e . v procedure init sema(e)= skip procedure sema acquire(s, e, .)= letrec \niter() = r := cas(s, e, 0, 1, ., skip); if r =0 then iter() in iter() procedure sema release(s, e, .)= \nstore(s, e, 0,.) procedure .nalize sema(e)= skip We omit the proof; it, too, is straightforward. 8.3 \nSet Speci.cation The set module exports three procedures: {emp} r := create set() {set(r, \u00d8)} IA(s) * \nS ..V set(o, V ) * U(V ) .V v/. V .{U(V ) * P } . {U(V .{v}) * Q(1)} .V v . V .{U(V ) * P } . ' {U(V \n) * Q(0)} {pA(s) * S * P } r := add(s, o, v, ., . ' ) {pA(s) * S * Q(r)} IA(s) * S ..V set(o, V ) * \nU(V ) .V v . V .{U (V ) * P } . {U(V \\{v}) * Q(1)} .V v/. V .{U(V ) * P } . ' {U(V ) * Q(0)} {pA(s) \n* S * P } r := remove(s, o, v, ., . ' ) {pA(s) * S * Q(r)} Procedure add returns 1 if the element was \nnot yet present, and 0 otherwise. Analogously, procedure remove returns 1 if the element was present, \nand 0 otherwise. Notice that in the speci.cation of add and remove, the .rst premise, which enables the \nprocedure to separate the set out of the lock invariant, uses S instead of P . In earlier speci.cations, \nthere was no separate S predicate and P was used for simplicity; however, using P means the procedure \ncannot perform further atomic operations on the set after the closure . or . ' has been executed, since \nit consumes P and produces Q. Using a separate predicate S means the procedure can access the set both \nbefore and after executing the closure. To simplify the presentation, the example module does not offer \na procedure for disposing a set object. An implementation that supports disposal, veri.ed using our veri.cation \ntool, is available online. 8.4 Sugared Speci.cations In the above speci.cations of procedures add and \nremove, the functional behavior is obscured somewhat by the .ne-grained con\u00adcurrency scaffolding. Fortunately, \nhowever, we can easily de.ne an abbreviated notation for typical .ne-grained speci.cations, which makes \nthem look just like sequential speci.cations. We introduce the notation {object(o, v)} r := fooFG(o) \n{object(o, post(v)) . r = res(v)} as an abbrevation for IA(s) * S ..v object(o, v) * U(v) .v {U(v) * \nP } . {U(post(v)) * Q(res(v))} {pA(s) * S * P } r := foo(s, o, .) {pA(s) * S * Q(r)} Often, it is convenient \nto split the postcondition into multiple cases, with corresponding ghost command parameters and corresponding \npremises. We introduce {object(o, v)} r := fooFG(o) r G1(v) . object(o, post1(v)) . r = res1(v) . G2(v) \n. object(o, post2(v)) . r = res2(v) where Gi are pure assertions, as an abbreviation for IA(s) * S ..v \n object(o, v) * U(v) .v G1(v) .{U(v) * P } .1 {U (post1(v)) * Q(res1(v))} .v G2(v) .{U(v) * P } .2 \n{U (post2(v)) * Q(res2(v))} {pA(s) * S * P } r := foo(s, o, .1,.2) {pA(s) * S * Q(r)} This notation \nallows the above speci.cation for procedure add to be written as {set(o, V )} r := addFG(o, v) {v/. V \n. set(o, V .{v}) . r =1 . v . V . set(o, V ) . r =0}  8.5 Set Implementation The implementation of the \nset module is shown in Figure 11. For node values, we implicitly perform an encoding of Z . {-8, +8} \ninto Z. We use the following syntactic sugar: n.next = n +1, and n.value = n +2. The set is implemented \nas a sorted linked list. For synchroniza\u00adtion, one .eld of each node is converted into a semaphore that \nis used to perform hand-over-hand locking of consecutive nodes. This affords some degree of parallelism \nfor concurrent operations on the set.  8.6 Set proof The core of the proof of the set module is the \nde.nition of the set predicate; it serves as the invariant of the data structure, which holds before \nand after each atomic operation. This invariant must  procedure create set() = lastNode := cons(0, 0, \n+8); .rstNode := cons(0, lastNode, -8); init sema(.rstNode); return .rstNode letrec locate(n)= ' '' n \n:= [n.next]; v := [n.value]; if v ' <v then ( sema acquire(s, n ' ); sema release(s, n); return locate(n \n' ) ) else return n in procedure add(s, o, v)= sema acquire(s, o); n := locate(o); ' '' n := [n.next]; \nv := [n.value]; if v ' = v then ( sema release(s, n); return 0 ) else ( ''' '' n := cons(0,n ,v); init \nsema(n ); [n.next] := n '' ; sema release(s, n); return 1 ) procedure remove(s, o, v)= sema acquire(s, \no); n := locate(o); ' '' n := [n.next]; v := [n.value]; if v ' = v then ( sema acquire(s, n ' ); ''' \n'' n := [n.next]; [n.next] := n ; sema release(s, n); return 1 ) else (sema release(s, n); return 0) \n Figure 11. Implementation of the set module. Note: desugaring inlines locate into add and remove enable \neach thread to retain, between the atomic operations that constitute a set operation, the information \nit needs about the state of the data structure. For example, after locating a node, a thread must know \nthis node will remain in the data structure. For this purpose, we track the set of nodes in the linked \nlist using a ghost bag (see Section 6). We keep the identi.er of the ghost bag in a ghost .eld of the \n.rst node. To refer to this ghost .eld, we use the syntactic sugar o.bag = o.1. Another consideration \nwhen de.ning the invariant is that we wish to retain the shape of the linked list even when a thread \nhas taken ownership of a node s next .eld in preparation for inserting or removing the next node. Therefore, \nwe use the ghost .eld at ghost offset 0 of each node as the oldNext .eld: n.oldNext = n.0. As usual, \nwe use a recursive predicate lseg to describe the linked list: lseg(b, f, vf , e, v\u00a3, a, \u00df)= The predicate \nlseg(b, f, vf , e, v\u00a3, a, \u00df) denotes the section of the sorted linked list from node f to node e, excluding \nnode e. The other parameters are the ghost bag identi.er b, the .rst value vf , the last value v\u00a3, the \nlist of nodes a, and the list of values \u00df. The body of the predicate is a disjunction. The .rst disjunct \ndescribes the case where the .rst node equals the last node and therefore the section is empty. The second \ndisjunct describes the non-empty case. Speci.cally, it describes the .rst node using the predicate node \nand recursively calls the predicate to describe the subsection from the second node to the last node. \nThis disjunct quanti.es existentially over the tail a ' of a, the tail \u00df ' of \u00df, the value of the semaphore \nvs of the .rst node, the next node n, and the value of the next node vn. Predicate node s body, too, \nis a disjunction; the .rst disjunct describes the case where the node is not locked; the second disjunct \ndescribes the case where the node is locked. In the latter case, full ownership of the n.next .eld and \nfractional ownership of the semaphore and the n.oldNext, n.value, and n ' .value .elds has been transferred \nto the thread that acquired the lock. Notice that each node owns half of the value .eld of the next node. \nThis means that when a thread locks a node, it knows not only that node s value but also the next node \ns value. This allows it to safely insert a new node in between, while maintaining the sortedness of the \nlist. The de.nition of the set predicate itself is now straightforward: set(o, V )= .b, e, a, \u00df lseg(b, \no, -8, e, +8, a, -8 \u00b7 \u00df) * o.bag . b * gbag(b, elems(a)) * true . V = elems(\u00df); The de.nition uses the \nmathematical function elems(a) which denotes the bag of the elements of the list a. It states that there \nis a sorted linked list starting at o, that starts with value -8 and ends with value +8 (which means \nthat an insertion point can be found within the list for any .nite value). It further states that the \nnodes of the list are exactly the elements in the ghost bag at o.bag, and that abstract value V of the \nset is exactly the bag of the values of the list. p The syntax e . v is shorthand for .p e . v. That \nis, it denotes an unspeci.ed fraction of the points-to permission. As applied in the set predicate, this \nallows threads to remember the connection between o and b. The true conjunct allows us to leak memory \nlocations (or fractions thereof) that we do not use; speci.cally, of the last node e we use only one-half \nof .eld e.value. We would need to be more precise if we wanted to support disposal of the set object. \nThe speci.cation of local recursive function locate is as fol\u00adlows: . .. pA(s) * S * o.bag . b * gbagh(b, \nn) * . .. .. 1/2 [ 12 ]sema(n, 1) * n.oldNext . n ' * n.next . n ' * 1/41/4 n.value . vn * n ' .value \n. vn, . vn <v .. r := locate(n) pA(s) * S * o.bag . b * gbagh(b, r) * [ 12 ]sema(r, 1) * . .. . .. \n(a = E . \u00df = E . f = e . vf = v\u00a3) . (.a ' ,\u00df ' ,vs, n, vn a = f \u00b7 a ' . \u00df = vf \u00b7 \u00df ' . 1/2 . n * r.next \n. n * node(b, f, vs,vf , n, vn) * lseg(b, n, vn, e, v\u00a3,a ' ,\u00df ' .n, vr,vn r.oldNext )) .... 1/41/4 \nr.value . vr * n.value . vn . vr <v . v = vn where node(b, n, vs, v, n ' ,v ' ) Note that even though \nlocate is shown outside of add and remove, = ) * n.next . n ' * n.oldNext . n ' (vs =0 . sema(n, vs \n* after desugaring it is within the scope of the parameters of these 1/21/2 '' procedures, and furthermore \nits proof can use the premises of these n.value . v * n.value . v ' * gbagh(b, n) . v<v ) . 1/2procedures \nspeci.cations, and in particular the .rst premise. (vs =1 . [ 12 ]sema(n, vs) * n.oldNext . n ' * We \nshow in Figure 12 the set implementation annotated with 1/41/4 '' n.value . v * n.value . v ' . v<v \n) ghost commands. A full proof outline is in the technical report [7]. procedure create set() = lastNode \n:= cons(0, 0, +8); .rstNode := cons(0, lastNode, -8); [.rstNode.oldNext] := lastNode; init sema(.rstNode); \nb := create gbag(); gbag add(b, f); [.rstNode.bag] := b; return .rstNode letrec locate(n)= ' '' n := \n[n.next]; v := [n.value]; if v ' <v then ( sema acquire(s, n ' , skip); sema release(s, n, skip); return \nlocate(n ' ) ) else return n in procedure add(s, o, v, ., . ' )= sema acquire(s, o, skip); b := [o.bag]; \nn := locate(o); ' '' n := [n.next]; v := [n.value]; if v ' = v then ( sema release(s, n, . ' ); return \n0 ) else ( ''''' ' n := cons(0,n ,v); [n.oldNext] := n ; '' '' init sema(n ); [n.next] := n ; sema release(s, \nn, (gbag add(b, n '' ); [n.oldNext] := n '' ; .)); return 1 ) procedure remove(s, o, v, ., . ' )= sema \nacquire(s, o, skip); b := [o.bag]; n := locate(o); ' '' n := [n.next]; v := [n.value]; if v ' = v then \n( sema acquire(s, n ' , skip); n '' := [n ' .next]; [n.next] := n '' ; sema release(s, n, (gbag remove(b, \nn ' ); [n.oldNext] := n '' ; .)); return 1 ) else ( sema release(s, n, . ' ); return 0 ) Figure 12. \nThe set module, with ghost commands (highlighted)  8.7 Client program In this subsection, to illustrate \nhow the speci.cation of the set module can be used to verify rich properties of client programs, we verify \nthe example client program shown in Figure 13. The program starts by creating a set object o and a lock \ns for use by the set module to emulate its atomic operations. (Remember that this lock can be erased \nafter veri.cation if real atomic operations are used; see Section 7.) Then, the lock is initialized. \nFrom this time, the lock protects the set data structure. Finally, a producer thread is forked and the \nmain thread turns into a consumer thread. The producer thread simply adds 1,2,3,. . . to the set. The \nconsumer thread repeatedly performs the following experiment: it picks an arbitrary number (by allocating \na heap cell and disposing it, just letrec producerThread(s, o, x)= add(s, o, x); producerThread(s, o, \nx + 1) consumer(s, o, x)= r := remove(s, o, x); if r =1 then (r := remove(s, o, x); assert(r = 0)) consumerThread(s, \no)= // pick random number x x := cons(0); dispose(x); consumer(s, o, x); consumerThread(s, o) in o := \ncreate set(); s := cons(1); initspace(s); release(s); fork producerThread(s, o, 1); consumerThread(s, \no) Figure 13. Example client program for the concurrent set module for the address) and tries to remove \nit. If the remove operation succeeds, it tries to remove it again and asserts that the latter remove \noperation fails. It always does, since the producer thread never adds the same number twice. Here is \nhow our approach succeeds in verifying the assert command of this program. A proof outline for this program, \ninclud\u00ading ghost commands, is shown in Figure 14. As always, the crucial step is coming up with an invariant; \nspeci.cally, a lock invariant for the lock s. It is shown at the bottom of Figure 14. The proof uses \nthree ghost .elds of s: s.set (sugar for s.0) connects the lock to the set o; s.prod (sugar for s.1) \nrecords the last number added by the producer; and s.cons (sugar for s.2) records the last number removed \nby the consumer. The invariant states that the last value added by the producer is an upper bound for \nthe set s elements; that the last value removed by the consumer is not greater than the last value added \nby the producer; and that the last value removed by the consumer is not in the set. Once the invariant \nis established, the proof outline follows eas\u00adily. As usual, each thread retains half of its associated \nghost .eld: the producer thread retains half of s.prod and the consumer thread retains half of s.cons. \nThe producer passes the required update of s.prod into add as a ghost argument; analogously, the consumer \npasses the required update of s.cons into remove as a ghost argu\u00adment. The speci.cations of add and remove \nare instantiated as fol\u00adlows. For all calls, predicate S is instantiated with s.set . o and predicate \nU (V ) is instantiated with the invariant minus the set data structure itself: 1/21/2 U(V )= .p, c s.set \n. o * s.prod . p * s.cons . c . (.v . V v = p) . c = p . c/. V where variables s and o are bound at \nthe call site. The instantiations of P and Q are shown at the call sites in Figure 14. Given these instantiations, \nthe premises of add and remove s speci.cations can be veri.ed easily. 9. Veri.cation Tool We implemented \nour approach in our program veri.cation tool, VeriFast [8], and we used the tool to verify two challenging \n.ne\u00adgrained concurrent data structures from the literature: a multiple\u00adcompare-and-swap algorithm [5] \nand a lock-coupling list [15]. VeriFast is a general-purpose veri.er prototype for C programs, based \non separation logic. It takes source code annotated with function speci.cations, loop invariants, predicate \nde.nitions, and other annotations, and reports either that the program is memory\u00adsafe, data-race-free, \nand complies with function speci.cations, or  threadspec producerThread(s, o, x)= 1/2 req 21 space(s) \n* s.set . o *.p s.prod . p . p<x ens false threadspec consumerThread(s, o)= 1/2 req 12 space(s) * s.set \n. o *.c s.cons . c ens false letrec producerThread(s, o, x)= { 11/2 space(s) * s.set . o *.p s.prod \n. p . p<x} 2 add(s, o, x, [s.prod] := x, skip); 1/2 P = .p s.prod . p . p<x 1/2 Q(r)= s.prod . x 1/2 \n{ 1 space(s) * s.set . o * s.prod . x} 2 producerThread(s, o, x + 1) consumer(s, o, x)= { 11/2 space(s) \n* s.set . o *.c s.cons . c} 2 r := remove(s, o, x, [s.cons] := x, skip); 1/2 P = .c s.cons . c 1/2 \nQ(r)= .c s.cons . c . (r =1 . c = x) 1/2 { 1 space(s) * s.set . o *.c s.cons . c . (r =1 . c = x)} \n2 if r =1 then ( r := remove(s, o, x, skip, skip); 1/2 P = s.cons . x 1/2 Q(r)= s.cons . x . r =0 1/2 \n{ 1 space(s) * s.set . o * s.cons . x . r =0} 2 assert(r = 0)) consumerThread(s, o)= 1/2 { 1 space(s) \n* s.set . o *.c s.cons . c} 2 // pick random number x x := cons(0); dispose(x); consumer(s, o, x); consumerThread(s, \no) in {emp} o := create set(); {set(o, \u00d8)} s := cons(1); {set(o, \u00d8) * s . 1 * \u00ae\u00a3,.Ns.e ' . 0} [s.set] \n:= o; [s.prod] := 0; [s.cons] := 0; {set(o, \u00d8) * s . 1 * s.set . o * s.prod . 0 * s.cons . 0 * true} \ninitspace(s); release(s); 1/21/2 {space(s) * s.set . o * s.prod . 0 * s.cons . 0 * true} fork producerThread(s, \no, 1); { 11/2 space(s) * s.set . o * s.cons . 0 * true} 2 consumerThread(s, o) Ispace(s)= .o, p, c, V \n1/21/2 s.set . o * s.prod . p * s.cons . c * set(o, V ) . (.v . V v = p) . c = p . c/. V Figure 14. \nProof outline for the client program it shows a symbolic execution trace that leads to a potential error. \nIt symbolically executes each function in turn, using a separation logic formula as the symbolic representation \nof memory. VeriFast supports ghost commands for creating and updating ghost cells. It also supports lemma \nfunctions, which are like ordi\u00adnary C functions except they may contain only ghost commands and VeriFast \nchecks that they terminate. It follows that calls of lemma functions are ghost commands. Thirdly, it \nsupports lemma function pointers and lemma function pointer calls. These features are all that was needed \nto make it possible to apply our approach in VeriFast. More generally, any veri.cation tool that supports \nghost variables, ghost functions, and dynamic binding of ghost functions supports our veri.cation approach. \nThis means it should be easy to extend other veri.cation tools, such as VCC [2] and Chalice [10], to \nsupport our approach. We have used VeriFast to verify the concurrent set module used as the example for \nthis paper. We also veri.ed a multiple-compare\u00adand-swap (MCAS) algorithm proposed by Harris et al. [5]. \nMCAS is built on top of a restricted-double-compare-single-swap (RD-CSS) algorithm by the same authors. \nOur MCAS proof consists of a proof of RDCSS with respect to an abstract speci.cation of RD-CSS, and a \nproof of MCAS based on the abstract speci.cation of RDCSS. We also veri.ed a simple example client program \nfor each algorithm. The annotation overhead, consisting of speci.cations as well as proof steps, is shown \nin the following table: Program LOC LOAnn Overhead Time lcset.c lcset client.c rdcss.c mcas.c mcas client.c \n72 27 51 63 34 610 266 528 1111 230 847% 985% 1035% 1763% 676% 0.37s 0.13s 0.5s 1.33s 0.22s In each \ncase, the annotation overhead is in the order of 10 to 20 lines of annotation per line of code. Three \nthings should be kept in mind when considering the overhead. Firstly, these are probably some of the \nmost complex algorithms in existence. Secondly, we did not optimize the annotation requirements for lemma \nfunction pointers; it currently involves signi.cant boilerplate. Thirdly, we show these results only \nas evidence that the speci.cation approach is applicable to challenging algorithms; this paper is not \nabout VeriFast. Notice that the run-time of the veri.cation tool is very accept\u00adable: on the order of \none second. This enables an interactive anno\u00adtation insertion process. The tool and the annotated example \nprograms are available online at http://www.cs.kuleuven.be/ bartj/verifast/. 10. Related Work To the \nbest of our knowledge, our approach is the .rst that enables fully general modular speci.cation and veri.cation \nof .ne-grained concurrent modules and their clients. We are aware of two existing approaches for speci.cation \nof .ne-grained concurrent data structures, both based on a marriage of rely-guarantee and separation \nlogic [17]: a linearizability-based approach, initially proposed in Vafeiadis PhD thesis [15], and concurrent \nabstract predicates [3]. In the linearizability-based approach, the speci.cation for a data structure \noperation is in the form of a piece of sequential code that operates on a ghost variable that holds the \nabstract state of the data structure. An implementation complies with the speci.cation if for each execution \ntrace, there is a total ordering of the operation invocations in the trace such that their return values \nequal the return values that would result if the operations speci.cations were executed sequentially \nin this total order. In other words, there exists a linearization point between the start and end of \neach operation invocation such that the result values are as if each operation s speci.cation was executed \natomically at the linearization point.  Linearizability-based veri.cation veri.es that the data structure \nis linearizable, by verifying that there exists a linearization point for each operation. The approach \ncan then verify client code as if the operations executed atomically. A limitation of the linearizability-based \napproach is that it does not support the transfer of ownership of memory locations or other resources \nbetween the data structure and its client. For example, a queue implemented as a linked list where nodes \nare allocated by the client, passed into the module on enqueue, and passed back to the client on dequeue, \ncannot be speci.ed by the linearizability-based approach. This is because the de.nition of linearizability \nassumes no memory is shared across the module boundary, and all interac\u00adtion is in the form of invocation \narguments and results. In contrast, in our approach ownership transfer is supported. For example, here \nis a speci.cation for the enqueue and dequeue operations of the queue module suggested above: IA(s) * \nS ..a queue(q, a) * U(a) .a {U(a) * P } . {U(a \u00b7 n) * Q * node(n)} {pA(s) * S * P } enqueue(s, q, n, \n.) {pA(s) * S * Q} IA(s) * S ..a queue(q, a) * U(a) .n, a {U(n \u00b7 a) * P * node(n)} . {U(a) * Q(n)} \n{U(E) * P } . ' {U(E) * Q(0)} {pA(s) * S * P } r := dequeue(s, q, ., . ' ) {pA(s) * S * Q(r)} An important \nadvantage of linearizability, however, is that pow\u00aderful automation techniques have been built for it, \ne.g. [16]. Concurrent abstract predicates (CAP) extend separation logic with shared regions. Each shared \nregion is associated with an interference speci.cation, which is a set of action names with associated \npre-and postconditions. A piece of local state can be converted into a shared region. This gives the \nthread full permission to perform the actions associated with the region. It may then pass fractions \nof these action permissions to other threads. Each assertion about a shared region must be stable with \nrespect to the actions that other threads may perform. The CAP authors [3] propose the following approach \nfor mod\u00adular speci.cation of a .ne-grained data structure. The module ex\u00adposes the data structure to \nclients in the form of a number of concur\u00adrent abstract predicates, each of which give permission to \nperform a particular type of operation. For example, their example lock mod\u00adule exposes predicates isLock(x) \nand Locked(x), which give per\u00admission to acquire, resp. release lock x. Their example set module exposes \npredicates in(h, v) and out(h, v), which give permission to remove, resp. add element v. This speci.cation \napproach does not subsume ours. Whereas our approach enables fully general speci.cation of data structure \noperations, this approach enforces restrictions on how the data structure may be used. Speci.cally, while \nthe set module speci.\u00adcation allows threads to concurrently add or remove distinct ele\u00adments, it does \nnot allow them to race to concurrently add or remove the same element. More generally, the choice of \nwhich predicates to expose is a trade-off between the restrictions on usage and the type of information \na client can track. Indeed: the information con\u00adtent of a predicate imposes a restriction on what other \nthreads can do. For example, if one thread holds an in(h, v) permission, other threads cannot remove \nthis element. To achieve a fully general speci.cation, the choice of stable per\u00admissions must be done \nby the client, not by the module designer. In order to enable this, the client must be able to do atomic \nor unstable observations, not just non-atomic or stable ones. This is what lin\u00adearizability enables by \nallowing operations to be treated like atomic instructions, and what our approach enables by allowing \nthe in\u00adsertion of ghost code into the critical section, and by allowing the client to choose the auxiliary \nvariables and the lock invariant. Note that our comparison is with the way the CAP logic is used in [3], \nnot with other speci.cation approaches based on the CAP logic that may be proposed in the future. However, \nthe CAP logic is a convenient alternative to the use of ghost objects to track partial information, and \nas such is comple\u00admentary to our speci.cation approach. Speci.cally, one could have a single auxiliary \nvariable that holds the precise abstract state of the data structure, and then insert this variable into \na shared region. For example, the ghost bags of Section 6 could be implemented more straightforwardly \nusing shared regions than using a data structure built from auxiliary heap cells. Acknowledgments The \nauthors would like to thank Cristiano Calcagno, Mike Dodds, Peter O Hearn, Matthew Parkinson, Viktor \nVafeiadis, and Hongseok Yang for helpful comments on drafts of this paper. This research is partially \nfunded by the Interuniversity Attraction Poles Programme Belgian State, Belgian Science Policy, and by \nthe Research Fund K.U.Leuven. References [1] Richard Bornat, Cristiano Calcagno, Peter O Hearn, and Matthew \nParkinson. Permission accounting in separation logic. In POPL, 2005. [2] Markus Dahlweid, Michal Moskal, \nThomas Santen, Stephan Tobies, and Wolfram Schulte. VCC: Contract-based modular veri.cation of concurrent \nC. In ICSE, 2009. [3] Thomas Dinsdale-Young, Mike Dodds, Philippa Gardner, Matthew Parkinson, and Viktor \nVafeiadis. Concurrent abstract predicates. In ECOOP, 2010. [4] Alexey Gotsman, Josh Berdine, Byron Cook, \nNoam Rinetzky, and Mooly Sagiv. Local reasoning for storable locks and threads. In APLAS, 2007. [5] Tim \nHarris, Keir Fraser, and Ian A. Pratt. A practical multi-word compare-and-swap operation. In 16th International \nSymposium on Distributed Computing, 2002. [6] Maurice Herlihy and Jeanette Wing. Linearizability: A correctness \ncondition for concurrent objects. ACM TOPLAS, 12(3), 1990. [7] Bart Jacobs and Frank Piessens. Expressive \nmodular .ne-grained con\u00adcurrency speci.cation (extended version). Technical Report CW590, Dept. CS, K.U.Leuven, \n2010. [8] Bart Jacobs, Jan Smans, and Frank Piessens. A quick tour of the VeriFast program veri.er. In \nAPLAS, 2010. [9] C. B. Jones. Speci.cation and design of (parallel) programs. In IFIP Congress, 1983. \n[10] K. Rustan M. Leino, Peter M\u00a8 Foundations of uller, and Jan Smans. Security Analysis and Design V, \nFOSAD 2007/2008/2009 Tutorial Lectures, volume 5705 of LNCS, chapter Veri.cation of concurrent programs \nwith Chalice. Springer, 2009. [11] Peter W. O Hearn, John Reynolds, and Hongseok Yang. Local reason\u00ading \nabout programs that alter data structures. In CSL, 2001. [12] Susan Owicki and David Gries. Verifying \nproperties of parallel pro\u00adgrams: An axiomatic approach. CACM, 19(5):279 285, May 1976. [13] Susan Owicki \nand David Gries. An axiomatic proof technique for parallel programs i. Acta Inf., 6, 1976. [14] J. C. \nReynolds. Separation logic: a logic for shared mutable data structures. In LICS, 2002. [15] Viktor Vafeiadis. \nModular .ne-grained concurrency veri.cation. PhD thesis, Computer Laboratory, University of Cambridge, \nJuly 2007. [16] Viktor Vafeiadis. Automatically proving linearizability. In CAV, 2010. [17] Viktor Vafeiadis \nand Matthew Parkinson. A marriage of rely/guarantee and separation logic. In CONCUR, 2007.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Compared to coarse-grained external synchronization of operations on data structures shared between concurrent threads, fine-grained, internal synchronization can offer stronger progress guarantees and better performance. However, fully specifying operations that perform internal synchronization modularly is a hard, open problem. The state of the art approaches, based on linearizability or on concurrent abstract predicates, have important limitations on the expressiveness of specifications. Linearizability does not support ownership transfer, and the concurrent abstract predicates-based specification approach requires hardcoding a particular usage protocol. In this paper, we propose a novel approach that lifts these limitations and enables fully general specification of fine-grained concurrent data structures. The basic idea is that clients pass the ghost code required to instantiate an operation's specification for a specific client scenario into the operation in a simple form of higher-order programming.</p> <p>We machine-checked the theory of the paper using the Coq proof assistant. Furthermore, we implemented the approach in our program verifier VeriFast and used it to verify two challenging fine-grained concurrent data structures from the literature: a multiple-compare-and-swap algorithm and a lock-coupling list.</p>", "authors": [{"name": "Bart Jacobs", "author_profile_id": "81100068878", "affiliation": "Katholieke Universiteit Leuven, Leuven, Belgium", "person_id": "P2509615", "email_address": "bart.jacobs@cs.kuleuven.be", "orcid_id": ""}, {"name": "Frank Piessens", "author_profile_id": "81100110352", "affiliation": "Katholieke Universiteit Leuven, Leuven, Belgium", "person_id": "P2509616", "email_address": "frank.piessens@cs.kuleuven.be", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926417", "year": "2011", "article_id": "1926417", "conference": "POPL", "title": "Expressive modular fine-grained concurrency specification", "url": "http://dl.acm.org/citation.cfm?id=1926417"}