{"article_publication_date": "01-26-2011", "fulltext": "\n Points-To Analysis with Ef.cient Strong Updates Ond.rej Lhot\u00b4Kwok-Chiang Andrew Chung ak D. R. Cheriton \nSchool of Computer Science University of Waterloo Waterloo, Ontario, Canada {olhotak,kachung}@uwaterloo.ca \nAbstract This paper explores a sweet spot between .ow-insensitive and .ow\u00adsensitive subset-based points-to \nanalysis. Flow-insensitive analysis is ef.cient: it has been applied to million-line programs and even \nits worst-case requirements are quadratic space and cubic time. Flow\u00adsensitive analysis is precise because \nit allows strong updates, so that points-to relationships holding in one program location can be removed \nfrom the analysis when they no longer hold in other lo\u00adcations. We propose a Strong Update analysis combining \nboth features: it is ef.cient like .ow-insensitive analysis, with the same worst-case bounds, yet its \nprecision bene.ts from strong updates like .ow-sensitive analysis. The key enabling insight is that strong \nupdates are applicable when the dereferenced points-to set is a sin\u00adgleton, and a singleton set is cheap \nto analyze. The analysis there\u00adfore focuses .ow sensitivity on singleton sets. Larger sets, which will \nnot lead to strong updates, are modelled .ow insensitively to maintain ef.ciency. We have implemented \nand evaluated the anal\u00adysis as an extension of the standard .ow-insensitive points-to anal\u00adysis in the \nLLVM compiler infrastructure. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors; \nD.2.4 [Software Engineering]: Software/Program Veri.cation General Terms Algorithms, Design, Experimentation, \nLanguages, Performance, Veri.cation Keywords points-to analysis, .ow sensitivity, strong updates, An\u00addersen \ns analysis, LLVM 1. Introduction One of the design decisions facing a developer selecting a sub\u00adset based \npoints-to analysis is .ow sensitivity. On one hand, .ow\u00adinsensitive analyses are well understood, and \ntechniques have been developed that make them quite ef.cient and scalable (e.g. [2, 12, 15, 19, 25, 27], \namong many others). On the other hand, .ow\u00adsensitive analyses promise potentially more precise results. \nRe\u00adcently, there has been a resurgence of interest in techniques that reduce the previously prohibitive \ncost of .ow sensitivity [14, 22, 31, 32]. This paper proposes a hybrid subset-based analysis algorithm \nthat has desirable properties of both .ow-insensitive and .ow- Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 sensitive analyses. This Strong \nUpdate analysis provides the key precision bene.t that .ow sensitivity brings, strong updates. How\u00adever, \nits performance is comparable to that of .ow-insensitive anal\u00adysis: in the worst case, it requires quadratic \nspace and cubic time, and in practice, it is almost as fast as .ow-insensitive analysis. More precisely, \nthe strong update analysis requires O(VA) space and O(EV 2) time, where V is the number of pointer variables \nin the program, A is the number of variables whose address is taken (i.e. possible pointer targets), \nand E is the number of edges in the interprocedural control .ow graph. The idea that enables this good \ncompromise is the realization that the precise points-to sets that matter most are also cheap to propagate, \neven .ow sensitively. A strong update can only be per\u00adformed if the may-point-to set of the dereferenced \npointer cor\u00adresponds to exactly one (runtime) target; otherwise, the analysis could not guarantee that \nany one of the possible targets is de.\u00adnitely overwritten. A necessary condition for this case is that \nthe points-to set must be a singleton (i.e., contain one abstract target). When one strong update improves \nthe precision of a given points-to set, the more precise set may enable a chain of further strong up\u00addates. \nThus in order to be precise overall, an analysis must model these small sets precisely. Yet singleton \nsets are also very cheap to represent and propagate. In particular, it is possible to propagate singleton \nsets .ow-sensitively without signi.cantly increasing the asymptotic complexity of an otherwise .ow-insensitive \nanalysis or its practical running time. Thus our strong update points-to analysis can be summarized as \nfollows. It is a .ow-insensitive subset-based analysis extended with .ow-sensitive modeling of singleton \nsets, which are used to en\u00adable strong updates. The analysis maintains sound .ow-insensitive points-to \nsets for all pointers. In addition, it provides .ow-sensitive points-to sets for those pointers and at \nthose program points where the sets are singletons. When a .ow-sensitive set is available, the analysis \nuses it, possibly to perform a strong update. When no .ow-sensitive set is available (because it is not \na singleton), the analysis falls back to the .ow-insensitive information. Although we have described \nthe analysis here as a combination of two sepa\u00adrate analyses, both analyses are intertwined in the actual \nalgorithm and performed at the same time so that they can query each other. Thus the .ow-sensitive analysis \nimproves the precision of the .ow\u00adinsensitive analysis, and the .ow-insensitive analysis provides a fall-back \nto the .ow-sensitive analysis when necessary. This paper makes the following contributions: It identi.es \nand discusses the characteristics of .ow-sensitive analyses that give rise to improved precision over \n.ow-insensitive analyses. It argues that strong updates are the most important such characteristic. \n It presents the hybrid strong update analysis algorithm, .rst as a system of constraints, and then as \nan algorithm extending the .ow-insensitive algorithm.  It shows that the worst-case complexity of the \nstrong update analysis is the same as that of the .ow-insensitive analysis, quadratic in space and cubic \nin time.  It describes an implementation of the strong update analysis in the LLVM compiler infrastructure \n[24]. The implementa\u00adtion is available for download at http://plg.uwaterloo. ca/~olhotak/su.  It experimentally \nevaluates the implementation on the SPECINT 2000 and SPECCPU 2006 benchmark suites [33], shows that its \npractical performance is comparable to that of the .ow\u00adinsensitive analysis, and that it performs 98% \nof the strong up\u00addates and propagates more precise sets than a .ow-insensitive analysis at 98% of the \nloads at which a fully .ow-sensitive analysis would.  The paper is organized as follows. Section 2 presents \nback\u00adground material. It .rst de.nes the form of the intermediate rep\u00adresentation on which the analyses \nwork. It then presents a high level speci.cation, in the form of subset constraints, of three exist\u00ading \nanalyses: a .ow-insensitive analysis and a .ow-sensitive anal\u00adysis without and with strong updates. Section \n3 presents the high\u00adlevel decisions guiding the design of the strong update analysis. It discusses the \nkey bene.ts of .ow sensitivity and assumptions about the intermediate representation that make analyses \neasier to express. It then presents, at the same high level of subset con\u00adstraints, the strong update \nanalysis for comparison with the existing .ow-insensitive and .ow-sensitive analyses. Section 4 presents \nthe strong update analysis algorithm in detail. It also proves the worst\u00adcase complexity results. Section \n5 presents details of the implemen\u00adtation of the strong update analysis in LLVM as an extension of the \n.ow-insensitive analysis already existing in that framework. Sec\u00adtion 6 presents results of an experimental \nevaluation of the strong update analysis measuring both its practical ef.ciency and the ben\u00ade.ts to precision. \nThe results show that the performance of the strong update analysis is comparable to that of the .ow-insensitive \nanalysis, and that the analysis provides the same bene.ts as a fully .ow-sensitive analysis at 98% of \nstores and loads. Section 7 sur\u00adveys other work related to ef.cient .ow-sensitive points-to analy\u00adsis. \nFinally, Section 8 concludes. 2. Background This section de.nes the program model and the notation that \nwill be used throughout the rest of the paper, brie.y reviews .ow\u00adinsensitive subset-based points-to \nanalysis (often called Andersen s analysis [1]), and speci.es a .ow-sensitive extension of that analy\u00adsis. \nThe program model commonly used in the points-to analysis literature and in the points-to analysis implementation \nin LLVM represents the program using a control .ow graph containing the four kinds of pointer-manipulating \ninstructions shown in the left column of Figure 1. More complicated statements that manipulate pointers \n(such as statements containing multiple levels of indirec\u00adtion) are decomposed into these basic instructions. \nThe ADDROF instruction is used to model all statements that cause a pointer p to point to some new target \na. This includes not only statements that take the address of a variable, but also statements that allocate \nnew objects dynamically, in which case the pointer target is the state\u00adment at which the allocation takes \nplace, the allocation site. The COPY instruction is used to model all copying of one pointer to an\u00adother, \nincluding the interprocedural copying of arguments to pro\u00adcedure parameters due to procedure calls. The \nSTORE and LOAD Figure 1. Constraints for .ow-insensitive subset-based points-to analysis p = &#38;a p \n= q {a} . pt(p) pt(q) . pt(p) [ADDROF] [COPY] *p = q .a . pt(p) . pt(q) . pt(a) [STORE] p = *q .a . pt(q) \n. pt(a) . pt(p) [LOAD] instructions model dereferencing of and writes and reads through pointers. For \nsimplicity of presentation, we follow the LLVM conven\u00adtion of separating variables into two disjoint \nsets of top-level and address-taken variables. The set A is de.ned to contain all possible targets of \na pointer, including address-taken variables and dynamic allocation sites. The set P contains all top-level \npointer variables. The instructions in Figure 1 are restricted to operate only on top\u00adlevel pointers \np, q .P, except for the ADDROF instruction that takes the address of an address-taken variable a .A. \nIf a program contains a variable v violating this restriction (i.e. it has its address taken, and is \nalso used in a copy, store, or load instruction), the program is transformed into an equivalent program \nthat replaces v with a separate top-level pointer pv and target variable av by adding the instruction \npv =&#38;av and replacing all occurrences of v in the original program with *pv. The set of all variables \nis de\u00adnoted V = P.A. We use a, b, and c to range over A, p, q, and r to range over P, and v and w to \nrange over V. The .ow-insensitive points-to relation pt : V. 2A, is de.ned as the least solution to the \nsubset constraints shown in Figure 1. For each pointer in the program, it provides a set of targets to \nwhich the pointer may point. The solution can be computed by initializing all points-to sets to the empty \nset, then iteratively choosing a sub\u00adset constraint that is violated and propagating the contents of \nthe points-to set on the left-hand-side of the constraint into the right\u00adhand-side, thereby satisfying \nthe constraint. In formal terms, this process is equivalent to applying a monotone function on the carte\u00adsian \nproduct lattice of the powerset lattices 2A associated with each of the individual points-to sets. The \nheight of this lattice is .nite. The constraints therefore have a unique least solution, and the iter\u00adative \nprocess converges to it [10]. The concretization of a given points-to analysis result is de.ned as all \nexecution states in which the C expression v == &#38;a evalu\u00adates to false for all v and a for which \na . pt(v). The feature that distinguishes a .ow-sensitive analysis from a .ow-insensitive one is that \nthe .ow-sensitive analysis takes control .ow between instructions into account and computes a possibly \ndifferent result for each program point. The subset-based points-to analysis can be extended to be .ow-sensitive \nas shown in Figure 2. Each instruction is annotated with a label e .L to indicate its position in the \ncontrol .ow graph. The points-to relation is extended with an extra parameter that dictates the program \npoint at which the points-to information applies. The notation e and e indicates the program points immediately \nbefore and after the instruction labelled e, respectively. For example, pt[e](v) gives the points\u00adto \nset of pointer v after the instruction labelled e. The subset constraints modelling the four kinds of \ninstructions are similar to those in the .ow-insensitive analysis, except they now relate a points-to \nset before each instruction with a points-to set after that instruction. The new CFLOW constraints model \nthe effect of control .ow: whenever e2 follows e1 in the control .ow graph, the points-to sets before \ne2 contain everything contained in the points-to sets after e1. The new PRESERVE constraint accounts \nfor the fact that any pointers not affected by an instruction maintain the values that they had before \nthe instruction executed. For each pointer v not in the kill set of the instruction, the points-to set \nafter the instruction contains all the targets that were in the points-to  e : p =&#38;a {a}. pt[e](p) \n[ADDROF] pt[e](q) . pt[e](p) kill(e : p = ...) {p} {}{} . . .. if pt[e](p) > 1 [COPY] e : p = q [STORE] \ne : *p = q .a . pt[e](p) . pt[e](q) . pt[e](a) e : p = *q .a . pt[e](q) . pt[e](a) . pt[e](p) [LOAD] \n if pt[e](p)= {a}. a . singletons if pt[e](p)= {a}. a . singletonskill(e : *p = q) e1 . pred(e2) .v \n.V . pt[e1](v) . pt[e2](v) [CFLOW] .. . {a}V e .L .v .V\\ kill(e) . pt[e](v) . pt[e](v) [PRESERVE] if \npt[e](p)= {} Figure 2. Constraints for .ow-sensitive subset-based points-to analysis set before the instruction. \nFor a simple implementation of a .ow- Figure 3. De.nition of kill sets 1: pa =&#38;a sensitive analysis, \nit is suf.cient (and sound) to de.ne all the kill sets to be empty, so that the PRESERVE subset constraints \napply to every pointer at every instruction. Following past work on .ow-sensitive points-to analysis, \nwe fo\u00adcus on path-insensitive analysis (i.e. the analysis ignores condition expressions in conditional \nbranches). Path-sensitive analyses are a different compromise in the tradeoff between analysis precision \nand ef.ciency, and they are beyond the scope of our study. Additional precision can be obtained using \nstrong updates, which are implemented in the analysis by de.ning kill sets that are not empty. A strong \nupdate occurs when it is known that an in\u00adstruction completely overwrites a previous value of a given \npointer. In this case, the pointer is listed in the kill set of the instruction to prevent the PRESERVE \nconstraints from propagating the previous value of the pointer through the instruction. To soundly include \nan abstract pointer v in the kill set, we must be sure that the instruction de.nitely writes to v, and \nthat the abstract pointer v represents no more than a single concrete pointer in the execution of the \nprogram. For example, if v is a dynamic allocation site, an instruction may overwrite one but not all \nof the objects allocated there, so it would be unsound to include v in the kill set. In Section 5, we \nwill de.ne a set singletons .V of abstract pointers corresponding to a single concrete pointer at run \ntime. Precise kill sets to implement strong updates are de.ned in Fig\u00adure 3. Each of the ADDROF, COPY, \nand LOAD instructions over\u00adwrites a target top-level pointer p, so that pointer is in the kill set. For \na STORE instruction *p = q, the kill set depends on the points\u00adto set of p before the instruction. If \nits size is greater than 1, the analysis cannot determine which of the targets will be overwritten, so \nthe kill set is empty (because no speci.c target is certain to be overwritten). If its size is exactly \n1, and the unique target a is in singletons, then the instruction will de.nitely overwrite a, so a is \nin the kill set. For correctness, we must also consider the case when the points\u00adto set of p is empty. \nIt is tempting but incorrect to suggest that in this case, the instruction cannot have any effect (except \nto dereference a null pointer, halting the program), so the kill set should be empty. Such a de.nition \nwould violate the monotonicity of the subset constraints, which would invalidate the guarantee of a unique \nleast solution and cause the analysis to loop forever on some programs without converging to a .xed point. \nConcretely, suppose the points\u00adto set of p before e : *p = q were empty, so that PRESERVE constraints \nwould be created at e for all variables. Later, some target a might be added to the points-to set of \np and therefore to the kill set of e. This would entail the removal of the PRESERVE constraint for a. \nBut this constraint might have been responsible for causing a to be in the points-to set of p in the \n.rst place, so fully removing the constraint would require removing a from the points-to set of p, thus \nforcing the constraint to be added back again. Thus the analysis would loop forever. When the points-to \nset of p is empty, the correct de.nition of the kill set is V, the set of all variables. As a result, \nno PRESERVE constraints are generated until the points-to set of p becomes non\u00adempty. No subset constraints \never need to be removed after they 2: pb =&#38;b 3: pc =&#38;c 4: *pa = pb 5: *pa = pc Figure 4. Example \nof straight-line code on which .ow sensitivity improves precision are generated, so the non-monotonicity \nof the constraints and non\u00adtermination of the analysis are avoided. Suggesting that a derefer\u00adence of \nan empty points-to set kills the values of all pointers may be surprising, but it is sound. If p can \nonly point to null, dereferenc\u00ading p causes the program to abort, and therefore the values of any pointers \nbefore the null dereference cannot be observed anywhere in the program after the dereference. Since the \nstatements after the null dereference are unreachable, from the point of view of abstract interpretation, \ntheir points-to sets should be ., and indeed, in the points-to domain, this value corresponds to all \npoints-to sets being empty. 3. Design Overview In this section, we present the design objectives for \nthe strong up\u00addate points-to analysis. We begin with a discussion of the bene.cial effects of .ow sensitivity \nin a points-to analysis that are desirable in our strong update analysis. We then discuss the performance \ntrade\u00adoffs that are made to achieve those precision improvements. 3.1 Bene.ts of .ow sensitivity The \nadvantage of .ow sensitivity can be classi.ed into two bene\u00ad.ts: handling of straight-line code and strong \nupdates. Of the two, strong updates generally provide the greater improvement in pre\u00adcision. The strong \nupdate algorithm that we will present aims to provide the bene.t of strong updates at a cost comparable \nto that of a .ow-insensitive analysis. The .ow-sensitive points-to analysis that was presented in Fig\u00adure \n2 provides some improvement in precision even without strong updates (i.e. when all of the kill sets \nare de.ned to be empty). If the program being analyzed contains code that is not inside any loop and \ncan never be executed more than once, a .ow-sensitive anal\u00adysis can determine that facts established \nat the end of such code do not yet hold at the beginning of such code. For example, con\u00adsider the short \nprogram in Figure 4. The program sets pointer a to point to b in line 4 and then to c in line 5. A .ow-insensitive \nanal\u00adysis would report that pt(a)= {b, c}. A .ow-sensitive analysis, even one without strong updates, \nwould determine that after line 4, a does not yet point to c: pt[4](a)= {b}. Thus .ow sensitivity improves \nprecision for this program even without strong updates. However, this bene.t is brittle: if the same \ncode appeared inside a loop, the analysis would determine that pt[e](a)= {b, c} at all points e. More \ngenerally, we can show that the points-to sets at every point inside a given loop are always identical: \n Proposition 1. Suppose that there is a cycle in the interprocedural control .ow graph leading from \ne1 to e2 and back to e1. Then if all the kill sets are empty, pt[e1](v)= pt[e2](v) for every variable \nv. Proof. The cycle in the control .ow graph induces a similar cy\u00adcle of CFLOW and PRESERVE constraints \npt[e1](v) . \u00b7\u00b7\u00b7 . pt[e2](v) . \u00b7 \u00b7\u00b7 . pt[e1](v). Thus pt[e1](v)= pt[e2](v). Most of the code of most programs \nis found inside loops. Many compiler optimizations target loops because loop bodies are where the most \nfrequently executed code appears. Even many long straight-line sequences of code occur inside a large \nouter loop. For example, long-running programs such as web servers or database servers run most of their \ncode inside an outer loop that handles individual requests. Relying on code to be outside of any loop \nis also brittle. For example, when a program is incorporated into a benchmark suite, it is generally \ninvoked from a test harness that executes it multiple times, in a loop. In all of these cases, due to \nProposition 1, a .ow-sensitive analysis without strong updates would compute the same points-to sets \nat all program points inside the loop. That is, its result would be no more precise than that of a .ow-insensitive \nanalysis. We therefore focus the design of the strong update analysis algorithm on providing the bene.ts \nof strong updates at low cost. The bene.t of precisely handling straight-line code is minimal, and it \nis dif.cult to achieve without an expensive analysis that maintains distinct, large points-to sets at \ndifferent program points. On the other hand, we will show how to achieve the more signi.cant bene.t of \nstrong updates within the quadratic space and cubic time bounds of a .ow-insensitive analysis.  3.2 \nUsing SSA form for strong updates of top-level variables The kill sets from Figure 3 de.ne strong updates \nof both top-level variables (the .rst de.nition in the .gure) and of address-taken pointer targets (the \nsecond de.nition). It is well known that the ef\u00adfect of strong updates of top-level variables can be \neasily achieved by .rst converting the program to Static Single Assignment (SSA) form [9]. In SSA form, \nevery variable is written to only once. Conversion to SSA form requires identifying all of the writes \nto a variable. Therefore, for a program with pointers, SSA conver\u00adsion requires points-to information \nto enumerate the indirect writes to variables through pointers, so full SSA conversion cannot be done \nbefore the points-to analysis. However, since top-level vari\u00adables cannot be accessed through pointers, \nit is possible to convert the top-level variables into SSA form prior to points-to analysis. Speci.cally, \nwe require the program to be converted to strict SSA form, which enforces that every use of a variable \nis dominated by its (unique) de.nition. We can show that for a program whose top\u00adlevel variables are \nin strict SSA form, a .ow-insensitive analysis provides the precision of .ow-sensitive analysis with \nstrong up\u00addates for top-level variables: Proposition 2. Given a program whose top-level variables are \nin strict SSA form, a top-level variable p whose unique de.nition is at ed, and an arbitrary label eu \nat which p is used, a .ow\u00adsensitive points-to analysis with strong updates will determine that pt[eu](p)= \npt[ed](p). Proof. Since the program is in strict SSA form, there is a path in the ICFG from ed to eu, \nso pt[ed](p) . \u00b7\u00b7\u00b7 . pt[eu](p) (using CFLOW and PRESERVE constraints and the fact that no instruction \nother than ed kills p). Notice that in the constraints in Figure 2, whenever the points-to set pt[e](p) \nof a top-level variable p appears on the right-hand side of a constraint, either there is a de.nition \nof p at e, or the left-hand side of the constraint also contains the ' same top-level variable (i.e., \nthe constraint is pt[e](p) . pt[e](p), e : p =&#38;a {a}. pt(p) [ADDROF] e : p = q pt(q) . pt(p) [COPY] \n e : *p = q .a . pt(p) . pt(q) . pt[e](a) [STORE] e : p = *q .a . pt(q) . pt[e](a) . pt(p) [LOAD] e1 \n. pred(e2) .a .A . pt[e1](a) . pt[e2](a) [CFLOW] e .L .a .A\\ kill(e) . pt[e](a) . pt[e](a) [PRESERVE] \n Figure 5. Constraints for .ow-sensitive subset-based points-to analysis on SSA form where e' is some \nother label). The only label e for which there can be a constraint whose left-hand side is not a points-to \nset of p is ed, the unique de.nition of p. Therefore, every path {a} . \u00b7\u00b7\u00b7 . pt[eu](p) of constraints \nfrom a pointer target &#38;a to pt[eu](p) must pass through pt[ed](p). Since the analysis .nds the least \nsolution, the only pointer targets in pt[eu](p) will be ones for which there is such a path, and are \ntherefore also in pt[ed](p). That is, pt[eu](p) . pt[ed](p). Since we also showed that pt[ed](p) . pt[eu](p), \nwe conclude that pt[eu](p)= pt[ed](p). As a result of Proposition 2, we can merge all of the .ow\u00adsensitive \npoints-to sets pt[*](p) of p into a single .ow-insensitive points-to set pt(p) without reducing the precision \nof the analy\u00adsis. The subset constraints after this simpli.cation are shown in Figure 5. Note that the \nCFLOW and PRESERVE constraints for a top-level variable p reduce to the trivial pt(p) . pt(p) and are \ntherefore not needed. While this analysis is as precise as the .ow\u00adsensitive analysis, it has regained \nsome of the simplicity of the .ow\u00adinsensitive analysis. The space required to store the points-to sets \nhas been reduced from O(|V||L||A|) to O(|P||A| + |A|2|L|). SSA transformation has also been used as a \npreparatory step for sparse analysis of top-level variables [17], which follows def-use chains between \ntop-level variables instead of paths in the control .ow graph. SSA form simpli.es these def-use chains. \n 3.3 Quadratic-space representation of points-to sets of pointer targets To achieve space requirements \nthat are quadratic in the size of the program, we must further reduce the |A|2|L| term in the above bound, \nwhich is due to the size of the points-to sets pt[e](a) of address-taken variables. The strong update \nalgorithm does this by taking advantage of the following insights: Most of the precision bene.t of .ow-sensitivity \ncomes from strong updates.  A strong update requires the points-to set of the dereferenced pointer to \ncontain at most one pointer target.  A singleton points-to set is cheap to store and manipulate.  Any \nlarger points-to sets will not directly enable strong updates, so there is little bene.t in spending \nmuch space or time storing them.  Therefore, the strong update analysis stores points-to sets of pointer \ntargets .ow sensitively when they are singletons, and only .ow insensitively when they are larger. To \nimplement this, we de.ne the singleton-set lattice S as shown in Figure 6. An element of this lattice \nis either the empty set, a singleton set, or T indicating some larger set. For each program point e and \nfor each pointer target a, the analysis stores an element of this lattice su[e](a). However, unlike in \na constant propagation analysis, the value of T in the strong update analysis does not im\u00admediately imply \nthat a pointer can point to anything. Instead, the analysis also stores a .ow-insensitive points-to set \npt(a) for each pointer target a to be used when the .ow-sensitive analysis indi\u00adcates T. The STORE constraint \nupdates both pt(a) and su[e](a) if  T {a}{ b} {c} \u00b7\u00b7\u00b7 {} Figure 6. The singleton-set lattice S e : \np =&#38;a {a}. pt(p) [ADDROF] e : p = q pt(q) . pt(p) [COPY] e : *p = q .a . pt(p) . pt(q) . su[e](a) \n[STORE] .a . pt(p) . pt(q) . pt(a) e : p = *q .a . pt(q) . ptsu[e](a) . pt(p) [LOAD] e1 . pred(e2) .a \n.A . su[e1](a) . su[e2](a) [CFLOW] e .L .a .A\\ kill(e) . su[e](a) . su[e](a) [PRESERVE]  su[e](a) if \nsu[e](a)= T Where ptsu[e](a) pt(a) if su[e](a)= T Figure 7. Constraints for Strong Update Analysis the \npoints-to set of the variable q being stored is a singleton (and sets su[e](a) to T if it is not). When \nthe LOAD constraint needs the points-to set of a at e, it .rst looks for a possible singleton by consulting \nsu[e](a); if this returns T, it falls back on the points-to set pt(a). This is implemented by the ptsu \nfunction in Figure 7. Only the small su sets need to be propagated .ow-sensitively along the control \n.ow edges of the program, so the CFLOW and PRE-SERVE constraints act only on these sets. The possibly \nlarge pt sets are stored only once for the whole program. As a result, the space bound of this representation \nis O(|P||A| + |A|2 + |L||A|), re.ect\u00ading the space requirements of the points-to sets of P, the points-to \nsets of A, and the sets su, respectively. Of course, it is possible to construct examples on which this \nsimpli.cation loses precision compared to a fully .ow-sensitive analysis: p = &#38;a; *p =&#38;b; if(*) \n*p = &#38;c; else *p = &#38;d; q = *p; r = *q; Both the strong update analysis and the .ow-sensitive \nanalysis will perform strong updates at all three stores. At the .nal load, su[e](a)= T due to the control \n.ow merge, so the .ow-insensitive points-to set pt(a)= {b, c, d} will be propagated to r in the strong \nupdate analysis, whereas the .ow-sensitive analysis would propa\u00adgate {c, d}. However, as we will see \nin Section 6, such examples are very rare in practice. Although the asymptotic complexity of the strong \nupdate repre\u00adsentation is low, we should also consider actual behaviour on real\u00adistic programs. In most \nprograms, only a small number of pointer targets a will have singleton points-to sets at a given label. \nThus the representation of the sets su[e] at each program point e should be worst-case linear not only \nin |A|, but also in the (much smaller) subset of pointer targets a for which su[e](a) is a singleton. \nSince su[e](a)= T for most values of a, we use a map storing only those pointer targets whose associated \nvalue is not T, and default to T when we do not .nd a particular pointer target in the table. There is \none other special case, however. Following a store through a pointer p whose points-to set is empty, \nsu[e](a)= {} for all values of a. Therefore, in our implementation, we use a boolean .ag to indicate \nthis special value. When the .ag is true, su[e](a)= {} for all val\u00adues of a. When the .ag is false, a \nmap stores the values of su[e](a) other than T, and T is returned when a pointer target is not found \nin the map. This hybrid representation is compact in all of the com\u00admon use cases. As we will see in \nSection 6, the mean number of entries in each map is less than 2.2 in all of the SPEC benchmarks.  3.4 \nSparse Allocation of Labels Almost all practical .ow-sensitive analyses share the representation of .ow-sensitive \nfacts for successive program points at which the facts cannot change, and the same can be done for the \nstrong update analysis. We do this by removing redundant labels from the program representation. In our \ndiscussion thus far, every instruction was assigned its own unique label e. However, most instructions \ndo not change the points-to sets of address taken variables a .A. The only instructions whose outgoing \nsu value is different from its incoming su value are STORE instructions, instructions with multiple control \n.ow predecessors (i.e. control .ow merges), and the very .rst instruction in the program. When it is \ncertain that the su sets at one instruction are identical to those at its predecessor, we assign both \ninstructions the same label. Speci.cally, we relabel the instructions in the program in the following \nway. First, every STORE instruction is assigned a unique label. Second, at every control .ow merge point, \nwe add a new no-op instruction and give it a unique label. The su value computed at this label will be \nthe join of the su values at the control .ow predecessors. Third, we add a no-op instruction at the very \nbeginning of the program and also give it a unique label. The su value computed at this label will be \n.a.T, meaning that no .ow-sensitive information is known. Finally, we label every other instruction with \nthe label of its (unique) control .ow predecessor. As a result, every label in the program can be classi.ed \nas either a store, a merge, or a clear (the beginning of the program). In particular, every LOAD instruction \nin the program is now labelled with the same label as the most recent instruction at which the su value \nmay have changed. After this relabelling process, it is necessary to update the ref\u00aderences to program \npoints in the constraints from Figure 7. The notation e and e for the program points immediately before \nand after the instruction at e may no longer designate the appropriate program points when there are \nmultiple instructions with the same label e. Instead of distinguishing su[e] and su[e], the strong update \nanalysis algorithm de.nes one strong update value su[e] for each la\u00adbel e. The value su[e] is de.ned \nto describe the program state at the point immediately after the .rst instruction at label e. By construc\u00adtion, \nthis .rst instruction is always a STORE or a no-op. Because a STORE instruction is always the .rst instruction \nat its label, the su[e] in the STORE constraint from Figure 7 is equivalent to su[e ' ], where e ' is \nthe new label of the instruction. Similarly, because a LOAD instruction always comes after the .rst instruction \nat a label, the ptsu[e] in the LOAD constraint is equivalent to ptsu[e ' ], where e ' is the new label \nof the instruction. Thus, in the remainder of this paper, we no longer use the notation e and e, but \nuse simply e to re\u00adfer to the program point immediately after the .rst instruction with label e. 4. Strong \nUpdate Analysis Algorithm This section presents the strong update analysis algorithm used to solve the \nconstraints of Figure 7. The algorithm is an extension of the .ow-insensitive subset-based points-to \nanalysis algorithm already implemented in LLVM and other compilers. We therefore begin with a brief review \nof that algorithm, and follow it with an explanation of the extensions that enable strong updates. The \noriginal .ow-insensitive algorithm that solves the con\u00adstraints of Figure 1 is shown in Figure 8. The \ncore data structure, 1 foreach ADDROF constraint p =&#38;a do pt(p) .= {a}; worklist .= {p} od 2 foreach \nCOPY constraint p = q do graph .= {q . p} od 3 while worklist = {} do 4 remove a variable v from worklist \n5 . . pt(v) \\ oldpt(v) 6 oldpt(v) . pt(v) 7 foreach STORE constraint *v = q do foreach a . . do AddEdge(q, \na) od od 8 foreach LOAD constraint p = *v do ProcessLoad(p, .) od 9 foreach v . w . graph do 10 pt(w) \n.=. 11 if pt(w) changed then worklist .= {w} . 12 od 13 od 14 proc ProcessLoad(p, .) 15 foreach a . . \ndo AddEdge(a, p) od 16 endproc 17 proc AddEdge(v, w) 18 if v . w . graph then graph .= {v . w}; pt(w) \n.= pt(v); if pt(w) changed then worklist .= {w} . . 19 endproc  Figure 8. Original Flow-insensitive \nPoints-to Analysis Algorithm in LLVM graph, maintains a set of edges corresponding to the subset con\u00adstraints \nbeing solved. The presence of the edge v . w corresponds to the subset constraint pt(v) . pt(w). The \ngraph is initialized with the constraints corresponding to COPY instructions in Line 2, and the constraints \ninduced by STORE and LOAD instructions are added to it as they are discovered during the analysis. The \nworklist keeps track of the variables v .V whose points-to set has grown since the variable was last \nprocessed. The body of the main loop in Lines 3 to 13 is executed for each such variable. In Lines 9 \nto 12, the new elements are propagated along the edges in the constraint graph; as a result, all of the \nsubset constraints with v on their left\u00adhand side become satis.ed. Any other variables whose points-to \nsets grow in the process are added to the worklist. Lines 7 and 8 and the ProcessLoad and AddEdge helper \nprocedures add new subset constraints induced by STORE and LOAD instructions to the graph. Whenever a \nnew constraint v . w is added, the AddEdge procedure immediately propagates the existing contents of \npt(v) into pt(w) in Line 18. This is necessary because the normal propa\u00adgation in Lines 9 to 12 propagates \nonly the part of the points-to set that was added since the last propagation. The algorithm maintains \nthe invariant that if for any variable v, there may be a constraint pt(v) . pt(w) that is not satis.ed, \nthen v is on the worklist. There\u00adfore, once the worklist empties, all of the constraints are satis.ed. \nEvery iteration increases the size of oldpt(v), and since every oldpt is a subset of A, the iteration \nmust eventually terminate. The extended algorithm that enables strong updates and solves the constraints \nof Figure 7 is shown in Figure 9. The lines marked with asterisks are additions to the original .ow-insensitive \nalgo\u00adrithm. Lines not marked with asterisks are identical to or only triv\u00adially changed from corresponding \nlines in the .ow-insensitive al\u00adgorithm of Figure 8. An important change is in the worklist: in the strong \nupdate algorithm, the worklist holds not only variables v whose subset constraints need to be reprocessed, \nbut additionally labels e whose su constraints need to be reprocessed. More precisely, the algorithm \nmaintains the following invariants: 1. If there is a constraint pt(v) . pt(w) that is not satis.ed, then \nv is on the worklist. 2. If there is a LOAD or STORE instruction dereferencing p that induces subset \nconstraints not already in graph, then p is on the worklist.  3. If there is a constraint ptsu[e](a) \n. pt(p) induced by a LOAD that is not satis.ed, then a is on the worklist. 4. If there is a constraint \nof the form su[e](a) su[e ' ](a ' ) that is not satis.ed, then e is on the worklist. 5. If there is \na STORE instruction (e : *p = q) that induces the constraint pt(q) su[e](a) and this constraint is not \nsatis.ed, then e is on the worklist.  The .rst two invariants were already present in the original .ow\u00adinsensitive \npoints-to analysis algorithm. Invariant 3 is a variation of Invariant 1 adapted to the modi.ed constraint \ninvolving ptsu that is induced by a LOAD instruction. Invariants 4 and 5 ensure that all violated constraints \ninvolving su are tracked by the worklist and eventually satis.ed. We will explain how the invariants \nare maintained shortly. First, however, we explain how the algorithm processes a label e appearing on \nthe worklist. As was explained in Section 3.4, each label is associated with either a clear, a control \n.ow merge, or a unique store instruction e : *p = q. The .rst two possibilities are handled in the obvious \nmanner in Lines 17 and 18. The interesting case is that of a STORE instruction. If pt(p) is empty, then \nsu[e] remains at . (i.e. .a.{}), as was explained in Section 2, so nothing needs to be done (Line 20). \nNote that it is not possible for pt(p) to be empty when su[e] is not ., because all of the code that \nmodi.es su[e] is conditional on pt(p) being non-empty, and pt(p) never shrinks, so once it is non-empty, \nit can never become empty again. When pt(p) is non-empty, the algorithm needs to establish the constraints \n.a . pt(p) . pt(q) su[e](a) due to the STORE instruction. The algorithm .rst converts pt(q) into an element \nof the singleton set lattice of Figure 6, substituting T if pt(q) is not a singleton set; this is done \nby the PtToSu procedure. Then a strong or weak update is done to su[e]. If the points-to set of p is \na singleton {a}, the target a of p is certain to be overwritten by the store, so the algorithm simply \nassigns PtToSu(q) to su[e](a), overwriting the existing value (which came from the control .ow predecessor \nof e in Line 21). This is a strong update (Line 24). If pt(p) is not a singleton, weak updates to all \nthe locations a to which p may be pointing are performed, by joining PtToSu(q) with the existing value \nof su[e](a) which came from the control .ow predecessor of e (Line 25). The processing of LOAD instructions \nis updated to take advan\u00adtage of the .ow-sensitive information available in su in Lines 34 1 foreach \nADDROF constraint p =&#38;a do pt(p) .= {a}; worklist .= {p} od 2 foreach COPY constraint p = q do graph \n.= {q . p} od 3 while worklist = {} do 4 remove a variable v or a label e from worklist 5 if a variable \nv was removed then 6 . . pt(v) \\ oldpt(v) 7 oldpt(v) . pt(v) 8* foreach STORE constraint e : *v = q do \nworklist .= {e} od 9* worklist .= affected[v] 10 foreach STORE constraint e : *v = q do foreach a . . \ndo AddEdge(q, a) od od 11 foreach LOAD constraint e : p = *v do ProcessLoad(e, p, .) od 12 foreach v \n. w . graph do 13 pt(w) .=. 14 if pt(w) changed then worklist .= {w} . 15 od 16* else // a label e was \nremoved 17* if e is a clear then su[e] . .a.T' ]18* else if e is a merge then su[e] . su[e  \u00a3I.pred(\u00a3) \n 19* else // e is a store *p = q 20* if pt(p)= {} then continue . 21* su[e] . su[pred(e)] 22* if |pt(q)|= \n1 then affected[q] .= {e} else affected[q] \\= {e} . 23* if pt(p)= {a} and a . singletons 24* then su[e](a) \n. PtToSu(q) // strong update 25* else foreach a . pt(p) do su[e](a) u= PtToSu(q) od . // weak update \n26* . 27* if su[e] changed then 28* worklist .= succ(e) 29* foreach LOAD constraint e : p = *q do ProcessLoad(e, \np, pt(q)) od 30* . 31 . od 32 proc ProcessLoad(e, p, .) 33 foreach a . . do 34* if su[e](a)= T 35 then \nAddEdge(a, p) 36* else pt(p) .= su[e](a); if pt(p) changed then worklist .= {p} . . 37 od endproc 38 \nproc AddEdge(v, w) 39 if v . w . graph then graph .= {v . w}; pt(w) .= pt(v); if pt(w) changed then worklist \n.= {w} . . 40 endproc 41* proc PtToSu(q) 42* if |pt(q)|= 1 and pt(q) . singletons then return pt(q) else \nreturn T . 43* endproc Figure 9. Strong Update Points-to Analysis Algorithm and 36. These lines simply \nimplement the ptsu function and the modi.ed LOAD constraint that uses it from Figure 7. Whereas in the \noriginal .ow-insensitive algorithm, a subset constraint a . p was added to graph unconditionally, it \nis now done only when su[e](a) is T; otherwise, only su[e](a) is propagated to pt(p). The algorithm must \nensure that it maintains the invariants enu\u00admerated earlier. Invariants 1 and 2 are guaranteed by the \nexisting code from the original .ow-insensitive algorithm and by the simi\u00adlar addition of p to the worklist \nin Line 36. Invariant 3 for a LOAD e : *p = q can be violated when, for some a . pt(p), either su[e](a) \nchanges, or su[e](a)= T and pt(a) changes. The .rst case is handled by Line 29, which calls ProcessLoad, \nwhich up\u00addates pt(p) to restore the invariant in Line 36. The second case is handled the same way as \nin the original .ow-insensitive algorithm: when su[e](a) becomes T, an edge a . p is added to graph in \nLine 35, which establishes the invariant and ensures that it remains established in response to changes \nin pt(a) using the normal prop\u00adagation code of Lines 12 to 15. Invariant 4 applies to constraints modelling \ncontrol .ow in the program. Line 28 restores the invari\u00adant by ensuring that whenever su[e] changes, \nevery control-.ow successor of e is added to the worklist. Invariant 5 is the most com\u00adplicated. For \na given STORE e : *p = q, the invariant can be invali\u00addated when either pt(p) or pt(q) grows. Growth \nof pt(p) is detected by the loop on Line 8. Growth of pt(q) is handled by Line 9, by adding all affected \nstores to the worklist. The affected array is used to keep track of all the STORES e : *p = q whose invariant \nmay be invalidated by a change in pt(q). These are all stores whose right\u00adhand side is q, but excluding \nthose for which pt(q) was already seen to be a non-singleton in Line 22 and whose su values are therefore \nalready T. Line 22 ensures that the affected array is correctly main\u00adtained. The invariants ensure that \nwhen the worklist is empty, all of the constraints of Figure 7 are satis.ed. A variable v is added to \nthe worklist only when pt(v) grows. A label e is added to the worklist only when some su[e ' ] grows \nor when e labels a STORE *p = q and either pt(p) or pt(q) has grown. Since each pt(v) and su[e] can grow \nonly a .nite number of times, the algorithm eventually terminates at a .xed point that satis.es all of \nthe constraints. Since each update of pt(v) or su[e] is the application of a monotone function, and since \nthe algorithm begins with all of these values at ., the .xed point at which it converges is the least \n.xed point of all the constraints.  4.1 Worst-case complexity As we have already discussed in Section \n3.3, the strong update al\u00adgorithm maintains the quadratic space bound of the .ow-insensitive points-to \nanalysis algorithm. We will now show that the strong up\u00addate algorithm also maintains the cubic time \nbound of the .ow\u00adinsensitive algorithm. For the worst-case analysis, we assume that the set propagation \noperation s1 .= s2 takes time proportional to the size of the set being propagated (i.e. O(|s2|) time). \nLemma 1. The total number of times that a variable is removed from the worklist is O(|V||A|), and the \nsum of the sizes of all the sets . computed in Line 6 is also O(|V||A|). Proof. A given variable v is \nadded to the worklist only when pt(v) changes. Since the maximum size of pt(v) is |A|, and elements are \nnever removed from pt(v), pt(v) can only change |A| times. Moreover, the sum of all the increases in \nthe size of pt(v) is at most |A|. Thus a variable is added to the worklist O(|V||A|) times and the sum \nof the sizes of . is also O(|V||A|). Lemma 2. The total number of times that a label is removed from \nthe worklist is O(E|A|), where E is the number of edges in the interprocedural control .ow graph. Proof. \nA given label e is added to the worklist only when su[e ' ] changes for some e ' . pred(e), or, if e \nis a store *p = q, when pt(p) or pt(q) changes. The total number of times that the former can happen \nis at most 2E|A|, since for any given a .A, su[e ' ](a) can change at most twice (from an empty set to \na singleton, then to T). The total number of times that the latter can happen is 2|A|for any given store, \nor a total of 2|L||A| times. Since every label in the control .ow graph has a predecessor (else it would \nnot be reachable), |L| <E, and so the total number of times that a label can be added to the worklist \nis O(E|A|). Theorem 1. The worst-case running time of the strong update algorithm is O(E|V|2), where \nE is the number of edges in the interprocedural control .ow graph. Thus it is cubic in the size of the \nprogram being analyzed. Proof. By Lemma 1, the block from Line 6 to 15 is executed at most O(|V||A|) \ntimes. Most of the lines in this block take at most O(max{|V|, |L|}) time. The only exceptions are Lines \n10 and 13, which take O(|L||.|) and O(|V||.|) time. Since the sum of the sizes of all the . sets is O(|V||A|), \nthe total time spent in these lines is O(|V||A|(|L| + |V|)). Since |V| is in O(|L|), the total amount \nof time spent in the block from Line 6 to 15 is O(|V||A||L|). By Lemma 2, the block from Line 17 to 26 \nis executed at most O(E|A|) times. All of the operations in it complete in O(|A|) time, so the total \ntime spent in this block is O(|A|2|L|). For each load e : p = *q, ProcessLoad is called only when su[e] \nor pt(q) changes, each of which can happen O(|A|) times. Each call to ProcessLoad completes in O(A) time. \nTherefore the total time spent in ProcessLoad is O(|A|2|L|). AddEdge does a propagation taking O(|A|) \ntime, but only when a new edge is added to graph, which can happen at most O(|V|2) times, so the total \ntime spent in AddEdge is O(|V|2|A|). T pqr \u00b7\u00b7\u00b7 . Figure 10. The top-level variable equivalence lattice \nAll of the bounds on the total time spent in each section of the algorithm are in O(E|V2|), so the overall \nalgorithm completes in O(E|V2|) time. In general graphs, the number of edges can be up to quadratic in \nthe number of vertices. However, it is well known that control .ow graphs are very sparse, since most \ninstructions have one suc\u00adcessor, some have two (conditional branches), and few rare ones have more (switch \nstatements and indirect function calls). Other complexity analyses of .ow-sensitive points-to analysis \nalso rely on this empirical fact [13]. 4.2 An improvement: equivalence to top-level variables The precision \nof the strong update algorithm can be further im\u00adproved for free by a small extension to the lattice \nfrom which su[e](a) values are chosen. Given a store *p = q where pt(p)= {a}, the lattice presented so \nfar (and shown in Figure 6) can rep\u00adresent the fact that pt(a) is a singleton set after the store, if \npt(q) happens to be a singleton set. However, the analysis can be easily extended to track that pt(a)= \npt(q) even when pt(q) is not a sin\u00adgleton, and this extension has no effect on the asymptotic complex\u00adity. \nIn the extended analysis, su[e] maps each address taken variable a to a pair (a, \u00df). The a component \nis a value from the singleton set lattice, as in the original analysis. The \u00df component is an element \nof the lattice shown in Figure 10: it is either a top-level variable p, or T or .. For example, the pair \nsu[e](a)= ({b},p) indicates that at e, pt(a)= {b} = pt(p), while su[e](a)= (T,p) indicates that although \npt(p) may not be a singleton set, pt(a)= pt(p). To adapt the algorithm to this extended lattice, only \nminor changes are needed. The if statement in Line 42 is updated to return (pt(q),q) in the then clause \nand (T,q) in the else clause. To take advantage of the additional information, an extra else-if clause \nis added to the if statement in Line 34. When su[e](a) is (T,q) but not (T, T), this new clause calls \nAddEdge(q, p) so that the contents of pt(q) (which the su information says are equal to pt(a)) are propagated \nto pt(p). This simple extension increases the height of the lattice that su[e](a) ranges over from 3 \nto only 4, so it does not affect the asymptotic complexity of the algorithm and has a negligible effect \non actual running times. Knowing that pt(a)= pt(p) when pt(p) is not a singleton may not directly enable \nadditional strong updates, but it can yield some improvement in analysis precision.  4.3 Constraint \noptimization For .ow-insensitive points-to analysis, several techniques have been developed to speed \nup the analysis by simplifying the sub\u00adset constraints [12, 15, 16, 26, 28]. With suitable modi.cations, \nthese techniques can also be used for strong update analysis. We have adapted and implemented strong \nupdate versions of the three techniques used in LLVM: Hash-based Value Numbering (HVN), HVN with Union \n(HU), and Hybrid Cycle Detection (HCD).  4.3.1 HVN and HU Both HVN and HU, as well as the earlier Off-line \nVariable Substi\u00adtution (OVS) [28], are intended to simplify the original constraints before the constraint \nsolving begins. All three of the techniques perform the following two steps: 1. Identify top-level pointers \nwhose points-to sets are known to be equal. 2. Merge the nodes representing these top-level pointers \nand up\u00addate all the subset constraints to refer to the newly merged node.  The techniques differ in \nhow aggressive they are in Step 1: each technique .nds possibly different sets of pointers that are provably \nequal. All three techniques are based on the idea of a subset graph [28] (also called the pointer assignment \ngraph [25] and the of.ine con\u00adstraint graph [16]). Nodes in the graph are top-level pointers p, addresses \n&#38;a, and dereferences *q, and edges in the graph corre\u00adspondtothe ADDROF, COPY,and LOAD statementsfromFigure1. \nFor each node of the subset graph, we can de.ne a subset-graph points-to set ptSG as follows: ptSG(p)= \npt(p), ptSG(&#38;a)= {a}, ptSG(*q)= pt(a). All three techniques depend only on a.pt(q) the following \nproperty of the subset graph. Proposition 3. For every top-level pointer p, pt(p) is the union of ptSG(a), \nwhere a ranges over all predecessors of p in the subset graph. The techniques merge pointers appearing \nin a common cycle in the subset graph and pointers having similar ancestors in the subset graph. The \nthree techniques differ in their precise de.nition of similar ancestors and the method used to compute \nthem. HU .nds more pointer equivalences than HVN, but HU takes cubic time while HVN takes only quadratic \ntime. Therefore, LLVM uses the more aggressive HU, but precedes it with a pass of HVN to reduce the size \nof the input to HU and therefore its cost. Thanks to the use of SSA form as described in Section 3.2, \nthe strong update algorithm models top-level pointers .ow-insensitively. Therefore, the strong update \nanalysis can merge top-level pointers (Step 2 above) in the same way as the .ow-insensitive algorithm. \nIt is only Step 1 that must be adapted to the strong update algorithm, since the increased precision \nmay cause different pointers to have equal points-to sets than in a .ow-insensitive analysis. The only \nchange needed is to separate dereference nodes by program point label (i.e. change nodes of the form \n*q to the form e : *q), since in the strong update algorithm, the result of a load from *q depends on \nthe location e of the load. The de.nition of ptSG is changed to include the label e as follows: ptSG(e \n: *q)= ptsu[e](a). This updated de.nition satis.es Proposition 3, a.pt(q) so OVS, HVN, and HU can be \nsafely applied without further modi.cations.  4.3.2 HCD Unlike OVS, HVN, and HU, the HCD technique is \nintended to .nd subset constraint cycles that do not arise until the analysis begins to solve the constraints. \nSuppose there is a path *p . q . ... . r in the subset graph and a store *p = r. Before constraint solving, \nHCD creates a data structure compactly recording all such paths. During constraint solving, when some \naddress a is added to pt(p), the following constraint cycle is created: pt(a) . pt(q) . ... . pt(r) . \npt(a). When this happens, the top-level pointers on the path from q to r can be merged. HCD also merges \nthe address node a with the top-level pointers q to r, but we must be careful in interpreting the meaning \nof merging an address a and a top-level pointer p. HCD merges only the representation of the points-to \nsets pt(p) and pt(a). It does not merge the notion of the address a; ADDROF constraints involving &#38;a \nare not changed. If two addresses a and b are both merged with p, although pt(a) is known to equal pt(b), \nthe addresses a and b are still distinct, and membership of a in a given points-to set is still independent \nof the membership of b in that points-to set. When merging a top\u00adlevel pointer p with an address a, by \nconvention, we can always choose the top-level pointer p as the representative of the merged node. This \nhas the advantage that none of the instructions from Figure 1 involving p need to be rewritten with a, \nand preserves the property that all instructions involve only top-level pointers (except the right-hand \nside of ADDROF, of course). The strong update analysis analogue of an HCD path is a path e : *p . q . \n... . r and a store e : *p = r at the same label e. When a is added to pt(p) during constraint solving, \nthe following constraint cycle is created: ptsu[e](a) . pt(q) . ... . pt(r) . ptsu[e](a). Thus the top-level \npointers q to r can be merged. The .ow-insensitive HCD also merges pt(a) with pt(q); the analogous merge \nof su[e](a) and pt(q) cannot be done in the strong update analysis because su[e](a) is not a points-to \nset. However, such a merge is also unnecessary because unlike pt(a) in the .ow-insensitive analysis, \nwhich is of size T(|A|), su[e](a) has a constant size of only a few bytes. 5. Implementation We have \nimplemented the strong update algorithm by extending the existing .ow-insensitive subset-based points-to \nanalysis that is in\u00adcluded in the LLVM compiler infrastructure [24], version 2.6. In the rest of this \npaper, we call this base analysis .ow-insensitive , even though some .ow sensitivity is achieved by analyzing \nan in\u00adtermediate representation in SSA form as discussed in Section 3.2. The base analysis is an implementation \nof the .ow-insensitive al\u00adgorithm of Figure 8 using sparse bit vectors to represent points-to sets, and \nwith extensions for the constraint optimizations discussed in the previous section. Thus it was straightforward \nto extend the implementation to implement the strong update analysis algorithm, with only a few issues \nthat we will explain in this section. Our implementation is publicly available at http://plg.uwaterloo. \nca/~olhotak/su. An implementation detail that is important for soundness is identifying which address-taken \nvariables are completely overwrit\u00adten by strong updates (i.e. what the singletons set should be). First, \nwe strongly update only variables that are the same size as a pointer, because, for example, a store \nto an array of multiple pointers would only update one element of the array, so the analysis should not \nstrongly update the whole array. Second, we strongly update only global variables and local variables \nof procedures that are not re\u00adcursive (either directly or mutually through other procedures). A local \nvariable of a recursive procedure can have many instances on the stack at the same time, and a store \nonly updates one of those instances, so a strong update would be unsound. Recursive proce\u00addures are found \nby detecting cycles in the call graph built ahead of time by LLVM, which conservatively assumes that \na procedure pointer could point to any procedure whose address has been taken. Finally, we never apply \nstrong updates to dynamically allocated variables, since multiple instances of them can be created by \nre\u00adpeating the allocation. To test the correctness of the implementation, we enabled the LLVM transformations \nthat take advantage of points-to informa\u00adtion and used the analysis to compile the SPEC CINT 2000 and \nSPEC CPU 2006 benchmarks [33] that are written in C. The SPEC harness validated that all of the compiled \nbenchmarks generated the correct output. On these benchmarks, with these test inputs, and for these LLVM \ntransformations using the analysis results, our imple\u00admentation of the strong update analysis is sound. \n To compare the results of the strong update analysis with fully .ow-sensitive analysis results, we \nalso expressed the .ow-sensitive analysis constraints of Figure 2 in Datalog. We used the LogicBlox Datalog \nimplementation and applied the manual Datalog optimiza\u00adtion techniques suggested by Bravenboer and Smaragdakis \n[3]. We extracted the input to the Datalog program from the LLVM points\u00adto analysis to ensure that both \nanalysis implementations were using the same input constraints. This setup enabled us to compare the \nstrong update analysis results with fully .ow-sensitive results. 6. Empirical Evaluation We compared \nthe strong update analysis with the original .ow\u00adinsensitive points-to analysis by running both of them \non the C benchmarks from the SPECINT 2000 and the SPECCPU 2006 suites [33]. The .rst column of Table \n1 gives the name of the bench\u00admarks, and the following four columns give various measurements of the \nsize of each benchmark. Column 2 shows the number of lines of source code. The next three columns show \nthe number of top-level pointers, the number of address-taken pointer targets, and the number of labels \nin the sparse labelling that was de.ned in Sec\u00adtion 3.4. These counts are taken after applying the HVN \nand HU constraint simpli.cations discussed in Section 4.3. The next three columns show the running times \nof the .ow\u00adinsensitive analysis and the strong update analysis, and the relative time difference between \nthem. The times shown are means of ten runs. The time differences have a geometric mean of a 5% slowdown \nand range from a speedup of 9% to a slowdown of 22%. The 9% speedup on 400.perlbench is due to the smaller \npoints-to sets that need to be propagated as a result of the increased precision of the strong update \nanalysis. These results con.rm that the speed of the strong update analysis is comparable to that of \nthe .ow\u00adinsensitive analysis not only in theory, but also in practice. For information only, the next \ntwo columns show the running times of two .ow-sensitive analysis implementations that are not directly \ncomparable with LLVM s built-in .ow-insensitive analy\u00adsis implementation and our strong update adaptation. \nThe FS col\u00adumn shows the running time of the Datalog/LogicBlox implemen\u00adtation of the fully .ow-sensitive \nanalysis. This analysis runs on the same input constraints as the analyses in the FI and SU columns, \nand therefore produces comparable output (except for the added precision from .ow sensitivity). However, \nthis implementation is written in Datalog, whereas the LLVM implementation is written in C. On four of \nthe benchmarks, the fully .ow-sensitive analy\u00adsis did not complete even after running for 48 hours. The \nHL [17] column shows the running time of Hardekopf and Lin s SSO semi\u00adsparse analysis [17]. Ben Hardekopf \nprovided an implementation and helped us make it run on the SPEC benchmarks. In theory, the HL analysis \nshould compute the same output as the FS analysis, satisfying the constraints from Figure 2. Like the \nLLVM imple\u00admentations of FI and SU, the HL analysis is written in C, but there are important differences \nthat affect performance and make it im\u00adpossible to directly compare the analysis output: 1. The LLVM \nimplementation contains code that de.nes the sets of top-level variables (P) and possible pointer targets \n(A) and extracts ADDROF, COPY, STORE, and LOAD constraints out of the intermediate representation. The \nsame extracted con\u00adstraints are used as input to the FI, SU, and FS implementations. The HL implementation \ndoes not use this constraint generation code, but implements its own de.nition of P and A and its own \nextraction of an analogous but different set of constraints. Thus the inputs to the analysis are different, \nand the intermediate and .nal points-to sets are different in cases where top-level vari\u00adables and pointer \ntargets are modelled differently in the input constraints. 2. The LLVM implementation of FI and our adaptation \nSU use sparse bit vectors to represent points-to sets. The HL imple\u00admentation uses binary decision diagrams \n(BDDs). 3. The SU implementation is built on top of the LLVM 2.6 version of the FI analysis. The HL \nimplementation is built on top of LLVM 2.5.  The .nal column shows the space usage of the strong update \nanalysis in terms of the number of tuples (e, a, b, p) such that su[e](a)= ({b},p). This metric was chosen \nbecause it is propor\u00adtional to the size of the su sets, the only data structure of signi.cant size added \nin the transformation of the .ow-insensitive analysis to the strong update analysis. This data structure \nis an array of maps, one for each label e. The last column in the table shows the total number of entries \nin these maps. Each such entry records three 32\u00adbit numbers representing a, b, and p. These maps could \nbe imple\u00admented using different data structures. Even assuming a 2x space overhead for the map data structure, \nfor the largest benchmark (403.gcc), the strong update analysis uses at most 6.1 MB more memory (268375 \nentries times 24 bytes) than the .ow-insensitive analysis. The overall memory usage is lowered by the \nreduction in points-to set sizes due to the increased precision of the strong update analysis. Table \n2 compares the strong update analysis with the fully .ow-sensitive analysis. The three columns under \nStores Strong Updates measure the number of store instructions in the program at which a strong update \ncan be performed (i.e. the points-to set of the dereferenced pointer contains only one target, and this \ntarget is in the singletons set). As discussed in Section 3.1, strong updates are the main bene.t of \na .ow-sensitive analysis compared to a .ow\u00adinsensitive analysis. The FS column counts the number of stores \nat which the .ow-sensitive analysis performs a strong update. In theory, the strong update analysis can \nperform a strong update at some subset of these stores; the size of this subset is shown in the SU column. \nIn total (excluding the four benchmarks on which the .ow-sensitive analysis does not complete), the strong \nupdate analysis performs 98% of the strong updates that the .ow-sensitive analysis performs. The right \nsection of the table presents counts of loads in each benchmark. Load instructions are where any difference \nbetween analyses is observed because they are the only instructions in the LLVM IR in which address-taken \nvariables are read. Every other in\u00adstruction works directly only with top-level variables; if an address\u00adtaken \nvariable is to be used, it must .rst be loaded into a top-level variable using a load instruction. The \nFS column in the table counts the number of loads e : p = *q at which the .ow-sensitive points\u00adto set \nof *q is strictly smaller than the .ow-insensitive points-to set of *q (i.e. .a.pt(q)pt[e](a) S .a.pt(q),\u00a3I.Lpt[e \n' ](a)). At these loads, a smaller (i.e. more precise) set is propagated to p than would be if the analysis \nwere .ow-insensitive. The SU column presents the analogous counts for the strong update analysis (i.e. \n.a.pt(q)ptsu[e](a) S .a.pt(q)pt(a)). In total, of all the loads at which the fully .ow-sensitive propagates \nmore precise sets than a .ow-insensitive analysis would, on 98% of them the strong update analysis does \ntoo. The bene.t that a given client analysis derives from .ow sensi\u00adtivity in the points-to analysis \nneeds to be evaluated separately for each proposed client. Our study has shown that the strong update \nanalysis improves points-to precision at 98% of program points at which a .ow sensitive analysis does. \nThus we conclude that if a client analysis bene.ts from .ow sensitivity in the points-to analy\u00adsis, then \nit will similarly bene.t from the strong update analysis.  Analysis Time (s) Space Benchmark 164.gzip \nkSLOC 8.6 |P| 1740 |A|971 |L|2818 FI 0.13 SU 0.14 slowdown 3% FS 7 HL [17] 0.50 SU 1347 175.vpr 17.8 \n7241 2896 7025 0.51 0.54 6% 46 1.02 7552 176.gcc 230.5 104663 24518 117121 41.86 45.79 9%  97.72 142152 \n181.mcf 2.5 1092 262 821 0.08 0.08 6% 3 0.42 1745 186.crafty 21.2 4145 2345 10671 0.40 0.41 2% 45 0.76 \n1037 197.parser 11.4 6741 2442 8509 0.76 0.92 21% 1206 2.39 9262 253.perlbmk 87.1 45803 11544 49584 13.75 \n16.74 22%  52.88 37284 254.gap 71.5 53285 11560 45431 8.81 9.47 8% 7029 11.47 55112 255.vortex 67.3 \n34531 14305 30759 4.39 4.52 3% 1665 7.68 17802 256.bzip2 4.7 951 577 1590 0.08 0.09 1% 3 0.46 325 300.twolf \n20.5 13423 3255 11650 1.20 1.25 4% 100 1.88 12004 400.perlbench 169.9 89661 21441 93792 66.99 60.71 -9% \n 306.70 70229 401.bzip2 8.3 3265 915 3243 0.28 0.30 9% 10 0.67 4161 403.gcc 521.1 240239 55012 272420 \n190.17 213.01 12%  3526.02 268375 429.mcf 2.7 1096 260 823 0.08 0.09 12% 2 0.44 1744 433.milc 15.0 5269 \n2343 5954 0.43 0.45 5% 30 1.04 4260 445.gobmk 197.2 54022 43881 41769 23.42 23.33 0% 7223 41.80 12391 \n456.hmmer 36.0 20240 4982 17186 2.10 2.22 6% 229 3.11 19713 458.sjeng 13.9 2591 1551 6544 0.26 0.27 2% \n20 0.62 1687 462.libquantum 4.4 1742 912 1652 0.14 0.14 4% 5 0.49 534 464.h264ref 51.6 26884 6817 22951 \n3.35 3.41 2% 547 3.28 18685 470.lbm 1.2 337 150 322 0.05 0.05 -4% 2 0.42 228 482.sphinx3 25.1 12410 4013 \n10332 1.01 1.06 5% 115 2.51 11778 Table 1. Benchmark characteristics, analysis running times and space \nrequirements of strong update and .ow-insensitive analysis Stores Loads Total Strong Updates Total More \nPrecise Benchmark SU FS % SU FS % 164.gzip 246 235 235 100% 564 12 12 100% 175.vpr 916 802 802 100% 3757 \n16 16 100% 176.gcc 26776 23061  88050 690 181.mcf 304 204 207 99% 780 3 3 100% 186.crafty 509 405 423 \n96% 1493 13 13 100% 197.parser 2024 1355 1584 86% 5147 6 6 100% 253.perlbmk 14926 9175  41091 54 254.gap \n12182 10060 10067 100% 39089 358 361 99% 255.vortex 4511 3786 3942 96% 17815 48 48 100% 256.bzip2 30 \n29 29 100% 270 0 0 100% 300.twolf 1829 1446 1446 100% 9718 0 0 100% 400.perlbench 25196 16899  79543 \n105 401.bzip2 316 220 237 93% 2139 22 22 100% 403.gcc 62134 40215  204112 149 429.mcf 300 199 202 99% \n784 3 3 100% 433.milc 944 893 893 100% 2360 0 0 100% 445.gobmk 2206 1931 1955 99% 7043 3 4 75% 456.hmmer \n2880 2216 2267 98% 13925 142 152 93% 458.sjeng 120 114 115 99% 677 0 0 100% 462.libquantum 171 140 147 \n95% 736 0 0 100% 464.h264ref 2143 1778 1844 96% 18944 354 363 98% 470.lbm 53 45 45 100% 134 0 0 100% \n482.sphinx3 1906 1542 1549 100% 7471 17 17 100% Table 2. Comparison of strong update and .ow-sensitive \nanalysis 7. Related Work The study of .ow-sensitive pointer analyses has a long history. Choi et al. \n[6] presented an early .ow-sensitive alias pair analysis as an instantiation of the standard data.ow \nanalysis framework [23]. The analysis was applied on a Sparse Evaluation Graph [5]; that is, a control \n.ow graph with irrelevant nodes removed. In order to improve ef.ciency further, Choi et al. [7] devised \none of the .rst extensions of SSA form [9] to represent indirect writes through pointers. Their Factored \nSSA (FSSA) form allowed preserving de.nitions, analogous to weak updates that may or may not over\u00adwrite \nthe value of a variable. Chow et al. [8] proposed a different extension of SSA form for handling pointers, \nHashed SSA (HSSA) form. This intermediate representation added two new kinds of nodes. A . node was placed \nafter every store to indicate that address-taken variables may or may not have been updated (similar \nto a preserving de.nition). A \u00b5 node was used to indicate a possible use of an address-taken variable \nby a load. Emami et al. [11] de.ned a points-to analysis that was not only .ow-sensitive but also context-sensitive. \nEach points-to relation\u00adship was annotated as either possible or de.nite to enable strong updates. Like \nearlier analyses, the analysis was implemented as a data.ow analysis on the control .ow graph. Wilson \nand Lam [37] presented a context-sensitive pointer analysis based on partial transfer functions (PTF) \nsummarizing the effects of procedures. Each PTF was constructed using a .ow-sensitive analysis, which \nwas ef.cient because it was intra\u00adprocedural. The PTFs were then combined to obtain context\u00adsensitive \ninterprocedural results. They presented experimental re\u00adsults on programs of up to 5 kLOC. This work \nsparked a line of sim\u00adilar points-to analyses that were .ow-sensitive intra-procedurally and generated \nprocedure summaries that could be instantiated at call sites [29, 35, 36]. However, these analyses performed \nstrong updates only on top-level variables. Hasti and Horwitz [18] proposed a technique that iteratively \nbuilds SSA form for variables with known aliasing, then performs alias analysis to increase the set of \nvariables for which aliasing is known. It remains an open question whether the .xed point of this technique \nmatches the results of a fully .ow-sensitive alias analysis. Hind and Pioli [20, 21] performed an empirical \nstudy of the ben\u00ade.ts of .ow sensitivity in alias analysis as well as of techniques to improve its performance. \nThey found that .ow sensitivity improves precision for a small subset of pointers. Goyal [13] derived \na .ow-sensitive points-to analysis algorithm that uses a .ne-grained worklist to run in asymptotically \ncubic time and cubic space in the worst case. The worst-case cubic bounds are likely to be achieved on \ntypical programs because the algorithm explicitly generates a full points-to graph for each program point, \nand points-to graphs are typically quadratic in size. In contrast, the strong update algorithm is similar \nin both structure and empirical behaviour to .ow-insensitive analysis, which has been shown to run in \nquadratic time in practice [30]. We are not aware of any implementation or empirical evaluation of Goyal \ns algorithm, but Staiger-St\u00a8 ohr [31, 32] designed and implemented a similar algo\u00adrithm with the same \nasymptotic complexity. Zhu and Calman [38] took initial steps towards using Bi\u00adnary Decision Diagrams \n(BDDs) [4] to ef.ciently represent .ow\u00adsensitive points-to sets. Tok et al. [34] presented a technique \nto speed up .ow-sensitive data.ow analysis on a control .ow graph using computed def-use chains for address-taken \nvariables. As the analysis discovers new def-use chains, the chains are used to reorder the instructions \nin the worklist to reduce the analysis time. Hardekopf and Lin [17] presented a semi-sparse algorithm \nto improve the running time of a fully .ow-sensitive subset-based points-to analysis. The analysis was \nsparse in that it did not pro\u00adcess control .ow graph nodes as a whole, but instead followed def-use chains \nto directly .nd the stores that produce the values for each load. Because def-use chains for address-taken \nvariables are not known until the analysis completes, the analysis was semi\u00adsparse in that it was sparse \nonly on top-level variables. The analysis used BDDs to keep the memory requirements of full .ow sensi\u00adtivity \nmanageable. It was the .rst fully .ow-sensitive subset-based points-to analysis that scaled to benchmarks \nof hundreds of kLOC. 8. Conclusion We have presented a subset-based points-to analysis algorithm that \ncombines the key advantages of .ow-insensitive and .ow\u00adsensitive analyses. Like a .ow-sensitive analysis, \nthe algorithm enables strong updates, which are the main precision bene.t of .ow sensitivity. Like a \n.ow-insensitive analysis, the strong update algorithm requires, in the worst case, quadratic space and \ncubic time. We have shown that its running time in practice is compara\u00adble to that of the .ow-insensitive \nanalysis. We have also shown that the strong update analysis performs 98% of the strong updates of a \nfully .ow-sensitive analysis, and propagates more precise points-to sets at 98% of the loads at which \na fully .ow-sensitive analysis does. These bene.ts of the algorithm stem from the notion that it is the \nprecise points-to sets that enable strong updates (and therefore further precision), yet it is also these \nsets that can be manipulated ef.ciently. Thus the strong update algorithm focuses attention on these \nsets to gain the precision bene.ts of .ow sensitivity and the ef.ciency bene.ts of .ow insensitivity. \nAcknowledgements We thank Ben Hardekopf for his assistance with the semi-sparse analysis of [17]. We \nare grateful to the anonymous POPL reviewers for their particularly constructive suggestions that helped \nimprove this paper. This research was supported by the Natural Sciences and Engineering Research Council \nof Canada. References [1] L. O. Andersen. Program Analysis and Specialization for the C Pro\u00adgramming \nLanguage. PhD thesis, DIKU, University of Copenhagen, May 1994. (DIKU report 94/19). [2] M. Berndl, O. \nLhot\u00b4 ak, F. Qian, L. Hendren, and N. Umanee. Points\u00adto analysis using BDDs. In Proceedings of the ACM \nSIGPLAN 2003 Conference on Programming Language Design and Implementation, pages 103 114, 2003. [3] M. \nBravenboer and Y. Smaragdakis. Strictly declarative speci.cation of sophisticated points-to analyses. \nIn OOPSLA 09: Proceeding of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages \nand Applications, pages 243 262, 2009. [4] R. E. Bryant. Symbolic boolean manipulation with ordered binary\u00addecision \ndiagrams. ACM Comput. Surv., 24(3):293 318, 1992. [5] J.-D. Choi, R. Cytron, and J. Ferrante. Automatic \nconstruction of sparse data .ow evaluation graphs. In Proceedings of the 18th ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Lan\u00adguages, pages 55 66, 1991. [6] J.-D. Choi, M. Burke, and P. Carini. \nEf.cient .ow-sensitive inter\u00adprocedural computation of pointer-induced aliases and side effects. In Proceedings \nof the 20th ACM SIGPLAN-SIGACT Symposium on Prin\u00adciples of Programming Languages, pages 232 245, 1993. \n[7] J.-D. Choi, R. Cytron, and J. Ferrante. On the ef.cient engineering of ambitious program analysis. \nIEEE Trans. Software Eng., 20(2):105 114, 1994.  [8] F. Chow, S. Chan, S.-M. Liu, and R. Lo. Effective \nrepresentation of aliases and indirect memory operations in SSA form. In Compiler Construction: 6th International \nConference, CC 96, volume 1060 of Lecture Notes in Computer Science, pages 253 267, 1996. [9] R. Cytron, \nJ. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An ef.cient method of computing static single \nassignment form. In Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Prin\u00adciples of Programming \nLanguages, pages 25 35, 1989. [10] B. A. Davey and H. A. Priestly. Introduction to Lattices and Order. \nCambridge Mathematical Textbooks. Cambridge University Press, .rst edition, 1990. [11] M. Emami, R. Ghiya, \nand L. J. Hendren. Context-sensitive interpro\u00adcedural points-to analysis in the presence of function \npointers. In Pro\u00adceedings of the ACM SIGPLAN 94 Conference on Programming Lan\u00adguage Design and Implementation, \npages 242 256, 1994. [12] M. F\u00a8ahndrich, J. S. Foster, Z. Su, and A. Aiken. Partial online cycle elimination \nin inclusion constraint graphs. In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language \nDesign and Implementation, pages 85 96, 1998. [13] D. Goyal. Transformational derivation of an improved \nalias analysis algorithm. Higher Order Symbol. Comput., 18(1-2):15 49, 2005. [14] B. Hardekopf. Pointer \nAnalysis: Building a Foundation for Effective Program Analysis. PhD thesis, University of Texas at Austin, \nMay 2009. [15] B. Hardekopf and C. Lin. The ant and the grasshopper: fast and accurate pointer analysis \nfor millions of lines of code. In PLDI 07: Proceedings of the 2007 ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 290 299, 2007. [16] B. Hardekopf and C. Lin. Exploiting pointer \nand location equivalence to optimize pointer analysis. In H. R. Nielson and G. Fil\u00b4 e, editors, Static \nAnalysis, 14th International Symposium, SAS 2007, volume 4634 of Lecture Notes in Computer Science, pages \n265 280, 2007. [17] B. Hardekopf, and C. Lin,. Semi-sparse .ow-sensitive pointer analy\u00adsis. In POPL 09: \nProceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages \n226 238, 2009. [18] R. Hasti and S. Horwitz. Using static single assignment form to improve .ow-insensitive \npointer analysis. In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design and \nImplementation, pages 97 105, 1998. [19] N. Heintze and O. Tardieu. Ultra-fast aliasing analysis using \nCLA: a million lines of C code in a second. In Proceedings of the ACM SIGPLAN 01 Conference on Programming \nLanguage Design and Im\u00adplementation, pages 254 263, 2001. [20] M. Hind and A. Pioli. Assessing the effects \nof .ow-sensitivity on pointer alias analyses. In Static analysis: 5th International Sympo\u00adsium, SAS 98, \nvolume 1503 of Lecture Notes in Computer Science, pages 57 81, 1998. [21] M. Hind and A. Pioli. Which \npointer analysis should I use? In Proceedings of the 2000 ACM SIGSOFT International Symposium on Software \nTesting and Analysis, pages 113 123, 2000. [22] V. Kahlon. Bootstrapping: a technique for scalable .ow \nand context\u00adsensitive pointer alias analysis. In PLDI 08: Proceedings of the 2008 ACM SIGPLAN Conference \non Programming Language Design and Implementation, pages 249 259, 2008. [23] J. B. Kam and J. D. Ullman. \nMonotone data .ow analysis frameworks. Acta Inf., 7:305 317, 1977. [24] C. Lattner and V. Adve. LLVM: \nA compilation framework for lifelong program analysis &#38; transformation. In CGO 04: Proceedings of \nthe International Symposium on Code Generation and Optimization, page 75, 2004. [25] O. Lhot\u00b4Scaling \nJava points-to analysis using ak and L. Hendren. Spark. In G. Hedin, editor, Compiler Construction, 12th \nInternational Conference, volume 2622 of LNCS, pages 153 169, Apr. 2003. [26] D. J. Pearce, P. H. J. \nKelly, and C. Hankin. Online cycle detection and difference propagation: Applications to pointer analysis. \nSoftware Quality Journal, 12(4):311 337, 2004. [27] F. M. Q. Pereira and D. Berlin. Wave propagation \nand deep propa\u00adgation for pointer analysis. In CGO 09: Proceedings of the 2009 In\u00adternational Symposium \non Code Generation and Optimization, pages 126 135, 2009. [28] A. Rountev and S. Chandra. Off-line variable \nsubstitution for scaling points-to analysis. In Proceedings of the ACM SIGPLAN 00 Confer\u00adence on Programming \nLanguage Design and Implementation, pages 47 56, 2000. [29] A. Salcianu. Pointer Analysis for Java Programs: \nNovel Techniques and Applications. PhD thesis, Massachusetts Institute of Technology, Sept. 2006. [30] \nM. Sridharan and S. J. Fink. The complexity of andersen s analysis in practice. In J. Palsberg and Z. \nSu, editors, Static Analysis, 16th International Symposium, SAS 2009, volume 5673 of Lecture Notes in \nComputer Science, pages 205 221, 2009. [31] S. Staiger-St\u00a8ohr. Implementing sparse .ow-sensitive andersen \nanaly\u00adsis. Technical Report 2009/03, Universit\u00a8 at Stuttgart, 2009. [32] S. Staiger-St\u00a8ohr. Kombinierte \nstatische Ermittlung von Zeigerzielen, Kontroll-und Daten.uss. PhD thesis, Universit\u00a8at Stuttgart, 2009. \n[33] Standard Performance Evaluation Corporation. URL http://www. spec.org/. [34] T. B. Tok, S. Z. Guyer, \nand C. Lin. Ef.cient .ow-sensitive interproce\u00addural data-.ow analysis in the presence of pointers. In \nA. Mycroft and A. Zeller, editors, Compiler Construction, 15th International Confer\u00adence, CC 2006, volume \n3923 of Lecture Notes in Computer Science, pages 17 31, 2006. [35] F. Vivien and M. Rinard. Incrementalized \npointer and escape analysis. In Proceedings of the ACM SIGPLAN 01 Conference on Programming Language \nDesign and Implementation, pages 35 46, 2001. [36] J. Whaley and M. Rinard. Compositional pointer and \nescape analysis for Java programs. In Proceedings of the 1999 ACM SIGPLAN Con\u00adference on Object-Oriented \nProgramming Systems, Languages, and Applications, pages 187 206, 1999. [37] R. P. Wilson and M. S. Lam. \nEf.cient context-sensitive pointer analy\u00adsis for C programs. In Proceedings of the Conference on Programming \nLanguage Design and Implementation, pages 1 12, 1995. [38] J. Zhu. Towards scalable .ow and context sensitive \npointer analysis. In DAC 05: Proceedings of the 42nd Annual Conference on Design automation, pages 831 \n836, 2005.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>This paper explores a sweet spot between flow-insensitive and flow-sensitive subset-based points-to analysis. Flow-insensitive analysis is efficient: it has been applied to million-line programs and even its worst-case requirements are quadratic space and cubic time. Flow-sensitive analysis is precise because it allows strong updates, so that points-to relationships holding in one program location can be removed from the analysis when they no longer hold in other locations. We propose a \"Strong Update\" analysis combining both features: it is efficient like flow-insensitive analysis, with the same worst-case bounds, yet its precision benefits from strong updates like flow-sensitive analysis. The key enabling insight is that strong updates are applicable when the dereferenced points-to set is a singleton, and a singleton set is cheap to analyze. The analysis therefore focuses flow sensitivity on singleton sets. Larger sets, which will not lead to strong updates, are modelled flow insensitively to maintain efficiency. We have implemented and evaluated the analysis as an extension of the standard flow-insensitive points-to analysis in the LLVM compiler infrastructure.</p>", "authors": [{"name": "Ondrej Lhot&#225;k", "author_profile_id": "81100503314", "affiliation": "University of Waterloo, Waterloo, ON, Canada", "person_id": "P2509543", "email_address": "olhotak@uwaterloo.ca", "orcid_id": ""}, {"name": "Kwok-Chiang Andrew Chung", "author_profile_id": "81479656719", "affiliation": "University of Waterloo, Waterloo, ON, Canada", "person_id": "P2509544", "email_address": "kachung@uwaterloo.ca", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926389", "year": "2011", "article_id": "1926389", "conference": "POPL", "title": "Points-to analysis with efficient strong updates", "url": "http://dl.acm.org/citation.cfm?id=1926389"}