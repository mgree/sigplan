{"article_publication_date": "01-26-2011", "fulltext": "\n Delay-Bounded Scheduling Michael Emmi Shaz Qadeer Zvonimir Rakamari\u00b4c LIAFA, Universit\u00b4e Paris Diderot, \nFrance Microsoft Research, Redmond, WA, USA University of British Columbia, Canada mje@liafa.jussieu.fr \nqadeer@microsoft.com zrakamar@cs.ubc.ca Abstract We provide a new characterization of scheduling nondeterminism \nby allowing deterministic schedulers to delay their next-scheduled task. In limiting the delays an otherwise-deterministic \nscheduler is al\u00adlowed, we discover concurrency bugs ef.ciently by exploring few schedules and robustly \ni.e., independent of the number of tasks, context switches, or buffered events. Our characterization \nelegantly applies to any systematic exploration (e.g., testing, model checking) of concurrent programs \nwith dynamic task-creation. Additionally, we show that certain delaying schedulers admit ef.cient reductions \nfrom concurrent to sequential program analysis. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation; D.2.5 [Software Engineering]: Testing and Debugging General Terms Algorithms, \nReliability, Testing, Veri.cation Keywords Concurrency, Asynchronous programs, Delay, Sequen\u00adtialization \n1. Introduction A concurrent program is a transition system combined with a (nondeterministic) scheduler. \nThe program s semantics is easy to describe: the scheduler repeatedly chooses an enabled state-altering \ntransition to execute, executes it, then chooses another transition, and so on. This inherently nondeterministic \nsemantics is the root cause of Heisenbugs i.e., programming errors that manifest rarely, and are hard \nto reproduce and repair. A class of techniques known as model checking [8] systematically explore this \nnondeterminism in order to discover, or occasionally prove the absence of, such bugs. Although systematically \nexploring (or searching) a concurrent program s behavior is a simple and intuitively appealing idea, \nthe exploration is computationally expensive. For programs in which the only source of nondeterminism \nis the scheduler, the combinatorial cost is determined by two factors: the maximum number I of scheduler \ninvocations, and the maximum number C of choices available to the scheduler at each invocation. Given \nthese two factors, the cost of exploration is O(CI ). While I naturally corresponds to the length of \nprogram executions, C corresponds to the number of concurrently executing tasks. Both C and I grow rapidly \nin realistic programs, causing a combinatorial explosion in the exploration cost. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PoPL 11, January 26 28, \n2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 Search \nprioritization is a basic strategy to combat the explosion. In general, one characterizes a subset of \nthe search space by a bounding parameter p. More behaviors are explored as p is increased, and in the \nlimit all behaviors are explored. A prioritization is effective when useful information (e.g., the presence \nof bugs) is obtained by examining few behaviors (i.e., low values of p). In this work we introduce delay-bounding: \nan effective search prioritization strategy for concurrent programs that handles both statically-known \nand dynamically-created tasks. We begin by quan\u00adtifying scheduler nondeterminism with the concept of \ndelay. Our main insight is captured by the premise: A deterministic scheduler is made suf.ciently nondetermin\u00ad \nistic with the ability to delay its next-scheduled task. In other words, we can quantify and thus limit \nscheduling nondeterminism by allotting a .nite delay-budget. More concretely, delay-bounding parameterizes \nthe search space by a deterministic scheduler M, and a delay-bound K. When K =0, exploration is limited \nto the unique execution produced by M. When K> 0, the scheduler may deviate from its usual schedule a \ntotal of K times over an entire execution. For instance, a round-based delaying scheduler executes all \nof its scheduled tasks to completion in a given round i, before advancing to the next round, where the \ntasks delayed in round i are again scheduled; as the total number of exercised delays is bounded by K, \nthe number of rounds is bounded by K +1. The search space for any deterministic scheduler and delay-bound \nK is bounded by IK . Delay-bounding is endowed with many appealing properties: Delay-bounding is a canonical \ncharacterization, and a means of limiting, scheduling nondeterminism. Since the bound is chosen independently \nof the number of tasks, the approach naturally handles both statically-known and dynamically-created \ntasks.  The cost of delay-bounded exploration is polynomial in I. Our preliminary experiments discover \n(previously unknown) bugs in real programs with small delay-bounds, suggesting that delay\u00adbounded search \ndoes provide adequate coverage in practice.  The choice of a deterministic scheduler is independent \nof the delay-bound, and every reasonable scheduler discovers any given bug at some cost. To minimize \nexploration cost it is even possible to perform parallel exploration using various deterministic schedulers \nperhaps even chosen at random.  It is possible, with certain deterministic schedulers, to capture a \nconcurrent program s delay-bounded semantics as a sequential program. Theoretically, the scheduling complexity \nof these schedulers is reduced from undecidable (with preemptible tasks), or EXPSPACE-complete (with \nnon-preemptible tasks), to NP\u00adcomplete (with or without preemption). In practice, the reduction allows \nus to leverage the numerous existing tools and techniques for sequential-program veri.cation, including \nsymbolic model checking, and symbolic exploration with SMT solvers.   1.1 Contributions Our contributions \nare summarized as follows: We introduce a canonical characterization of scheduling non\u00addeterminism, \nallowing a simple, elegant, and uni.ed approach for prioritized exploration of concurrent programs with \ndynamic task-creation.  We show that delaying deterministic schedulers can ef.ciently discover both \nknown and previously-undiscovered bugs by systematic exploration (e.g., testing).  We give concurrent-to-sequential \nprogram translations using clever encodings of certain delaying schedulers. The encodings rely on insightful \nextensions of Lal and Reps [27] s guess and constrain methodology, and ultimately lead to practical veri.cation \nalgorithms.  We identify an NP-complete analysis problem: state-reachability under a particular delaying \nscheduler. The complexity is lower than related bounded reachability problems.   1.2 Comparison to \nRelated Search Prioritizations Delay-bounding represents a convergence of a progression over the last \ndecade of search prioritization techniques for concurrent programs. Given a number I of scheduler invocations \nand C of choices available to the scheduler at each invocation recall the cost of complete search is \nO(CI ): Depth-bounding limits the number of scheduler invocations by a depth-bound d, reducing the search \ncomplexity to O(Cd). Though this approach is taken by VERISOFT [17], it is clearly not effective as d \napproaches I. In practical terms, errors mani\u00adfested deeply in a program execution remain dif.cult to \ndiscover. Context-bounding [34] labels the choices available to the sched\u00aduler by distinct (task) identi.ers, \nand bounds the number of label changes in executions by a context-bound c. Here the complexity is reduced \nto O((IC)c); the polynomial dependence on execu\u00adtion length alleviates, to a large extent, the aforementioned \nprob\u00adlem with depth-bounding. Empirically, low context-bound values are suf.cient for .nding many bugs \nin real programs [27, 35]. However, in many cases, bugs are exhibited only after each of the, say C, \ntasks have a chance to execute. (In practice this dependence arises in, e.g., initialization patterns, \nround-robin protocols, etc.) Consequently, the exponential dependence on c in O((IC)c) is practically \nan exponential dependence on C as well. Worse yet, when tasks are created dynamically, context\u00adbounding \nsuffers the same ill fate as depth-bounding: errors manifested deeply in a task-creation chain are discovered \nonly for high values of c. Preemption-bounding [29] also labels scheduler choices with dis\u00adtinct identi.ers, \nbut only bounds the number of preemptive label changes (by p); label changes due to blocking or completion \nare not budgeted. Preemption-bounded search addresses the main problem with context-bounding, as each \ntask gets the oppor\u00adtunity to complete its execution. Unfortunately, the number of label changes is expected \nto depend on C, rendering the search complexity, O((IC)p+C ), prohibitive for large values of C. As we \ncan see, when tasks are numerous or dynamically created, the existing prioritized exploration techniques \nare ineffective. Delay\u00adbounding .xes these problems by allowing an unbounded number of tasks to execute, \nand bounding instead the degree of variation from a deterministic scheduling order. The exploration cost \nusing K delays is IK, which is independent of the number of (dynamically-created) tasks. 2. Asynchronous \nPrograms So that our approach handles dynamic task-creation by design, we begin with a model of simple \nasynchronous programs corresponding to the style of single-threaded event-driven programming. The style \nis typically used as a lightweight technique for adding reactivity to single-threaded applications by \nbreaking up long-running computa\u00adtions into a collection of tasks. In this model, control begins with \na non-empty task buffer of pending tasks from which a dispatcher picks a single task to execute. The \ndispatcher transfers control to the task, which is essentially a sequential program that can read from \nand write to global storage, and post additional tasks to the task buffer. When a task completes its \nexecution, control returns to the dispatcher, which picks another task from the task buffer, and so on. \nWhen the dispatcher has control and the task buffer is empty, the program terminates. This model forms \nthe basis of (client-side) web applications [15], and has been shown useful for building fast servers \n[32], routers [22], and embedded sensor networks [18]. In what follows we generalize the usual notion \n[37] of task-buffer dispatch (i.e., choosing any pending task) by exposing a scheduler, by which we parameterize \na simple program semantics. To capture programs with interruptible tasks (or, alternatively, concurrently \nrunning tasks on sequentially consistent proces\u00adsors/threads), we extend the simple asynchronous model \nto permit arbitrary task interruption with the yield-statement. Although the extension may seem modest, \nthe resulting language is powerful enough to model concurrent programs with arbitrary preemption and \nsynchronization, e.g., by inserting a yield-statement before ev\u00adery shared variable access. The model \nparticularly resembles typical operating-systems kernel code and device drivers, which are often implemented \nas a collection of preemptible task-handlers [9, 31]. 2.1 The Scheduler For the moment, let Tasks and \nVals be uninterpreted sets, and blocked : Tasks \u00d7 Vals . B be an uninterpreted predicate. Intuitively, \neach element w . Tasks is a task to be scheduled, each element g . Vals is a global state, and blocked(w, \ng) holds when w is not enabled in the global state g. A scheduler M = (D, empty, give, take) consists \nof a data\u00adtype D of scheduler objects m . D, a scheduler constructor empty . D, and scheduler update \nfunctions give : D\u00d7Tasks . D to receive posted tasks, and take : D \u00d7 Vals . P(D \u00d7 Tasks) to determine \nwhich tasks can be scheduled next. The scheduler M is deterministic when for all m . D and g . Vals, \ntake(m, g) contains at most one element, and is non-blocking when for all ' w . Tasks, g . Vals, and \nm, m. D, blocked(w, g) implies ' (m,w) . take(m, g). In general, non-blocking schedulers must be aware \nof the program state; hence the argument g . Vals to take. Fix m . D and g . Vals. A task w is pending \n(in m) if there is a sequence m1w2m2w3m3 ...wj mj such that m1 = m, wj = w, and for 1 <i = j, (mi,wi). \ntake(mi-1,g); in other words, w is pending when w can eventually be taken from m. A task w is schedulable \n(by m) when there exists m' such that ' (m,w). take(m, g). Scheduler 1 (The bag scheduler). The multiset-based \nscheduler bag is de.ned on the multiset domain Dbag of tasks as1 def emptybag = \u00d8 def givebag(m, w)= \nm .{w} def takebag(m, g)= {(m \\{w},w) : w . m}. The bag scheduler is non-deterministic, since all pending \ntasks are returned by each application of takebag. 1 Here . and \\ are the multiset union and difference \noperators.  * P ::= var g : T H H ::= proc p (var l : T ) s s ::= s; s | x := e | skip | assume e | \nif e then s else s | while e do s | call x := p e | return e | post p e x ::= g | l Figure 1. The grammar \nof simple asynchronous programs. We consider only schedulers M that re.ne the bag scheduler, i.e., schedulers \nsatisfying the property: Property 1. Any sequence of give and take operations allowed by M are also allowed \nby bag. Note that a scheduler can be lossy i.e., can drop pending tasks though is not allowed to pull \ntasks out of thin air, i.e., schedule tasks that have not been posted. The following is an example of \na lossy scheduler that re.nes the bag scheduler. Scheduler 2 (The bounded bag scheduler). The K-bounded \nbag scheduler bb is de.ned on the multiset domain Dbb of tasks as def emptybb = \u00d8 def m .{w} if m(w) \n<K givebb(m, w)= m otherwise def takebb(m, g)= {(m \\{w},w) : w . m}. The bounded bag scheduler is non-deterministic, \nsince all pending tasks are returned by each application of takebb. It is lossy since it will drop posted \ntasks if its bound K has been reached.  2.2 Program Syntax Let Procs be a set of procedure names, Vals \na set of values con\u00adtaining true and false, and T the type of values. The grammar of Figure 1 describes \nour language of simple asynchronous programs, where p ranges over procedure names. We intentionally leave \nthe syntax of expressions e unspeci.ed, though we do insist the set of expressions Exprs contains Vals \nand the (nullary) choice operator *. A simple (synchronous) program, or sequential program, is a simple \nasynchronous program which does not contain post-statements. Each program declares a single type-T global \nvariable g, and a sequence of procedures p1 ...pn . Procs *. Each procedure p has single type-T parameter \nl, and a top-level statement denoted sp. The set of program statements s is denoted Stmts. We assign \nthe statements of simple asynchronous programs their usual meaning. In particular, post pe is an asynchronous \ncall to procedure p with argument e, which returns immediately with the expectation that p is invoked \nat a later time; the assume e statement proceeds only when e evaluates to true. (Later on we use the \nassume-statement to block undesired executions in a reduction to sequential programs.)  2.3 Program \nSemantics For the remainder of this section we .x a predicate blocked : Tasks \u00d7 Vals . Band a scheduler \nM = (D, empty, give, take). A frame (e, s) is a valuation e . Vals to the procedure-local variable l \nwith a statement s.A task w is a sequence of frames, and the set (Vals \u00d7 Stmts) * of tasks is denoted \nTasks.A con.guration c = (g, w, m) is a valuation g . Vals of the global variable g with a task w . Tasks, \nand a scheduler object m . D. We say a task w is blocked in a con.guration (g, w ' ,m) (alternatively, \nin a global state g) when blocked(w, g)= true. For expressions without program variables, we assume the \nexistence of an evaluation function [\u00b7] : Exprs . P(Vals) such that [*] = Vals. For convenience, we de.ne \ndef def e((g, (e, s) w, m))= e(g, e)= [e[g/g, e/l]] since g and l are the only variables, the expression \ne[g/g, e/l] has no free variables. A statement context S is a term derived from the grammar S ::= 0| \nS; s. We write S[s] for the statement obtained by substituting a statement s for the unique occurrence \nof 0 in S, and write (g, (e, S) w, m) [s] to denote the con.guration (g, (e, S[s]) w, m). Figure 2 gives \nthe scheduler-parameterized semantics of simple asynchronous programs as a set of operational steps on \ncon.gu\u00adrations. The choice operator * is used in the CALL and RETURN-SYNC rules only as a placeholder \nfor an undetermined value. The ASSUME rule restricts the set of valid executions: a step is only allowed \nwhen the predicated expression e evaluates to true. (This statement usually con.ned to intermediate languages \nis crucial for our reduction to sequential programs in Section 5.) A task w ' in a POST-step is said \nto be posted, and a task w in a DISPATCH-step is said to be dispatched. We refer to the semantics instantiated \nby a scheduler M as the M-semantics. The natural bag-semantics (see Scheduler 1) corresponds to the usual \nasynchronous program semantics [37]. We call the semantics of synchronous programs i.e., those without \npost-statements the synchronous semantics, which is in fact independent of the scheduler. A con.guration \n(g, (e, s; return *) , empty), where s does not contain return-statements, is called M-initial. An M-execution \n(to cj ) is a con.guration sequence h = c1c2 ...cj where c1 is M-initial, and  ci . ci+1 for 1 = i<j. \n (The initial statement s begins the execution by posting tasks; if no tasks are posted by s, the execution \nends when s completes.) We say a con.guration c = (g, w, m) (alternatively, the global value g) is M-reachable \nwhen there exists an M-execution to c, and is M-.nal when in addition w = e and take(m)= \u00d8. For a scheduler \nM and program P , the M-value set on P is the set of values g(c) of the global variable g such that c \nis M-.nal.2 The M -semantics on P is g-equivalent to the M '-semantics on P ' when the M-value set on \nP is equal to the M '-value set on P ' . Although we have not made task identi.ers explicit, we may assume \nthat a set U . Nof task identi.ers is de.ned by an execution h = c1c2 ...cj such that u . U is the task \nidenti.er of a task dispatched3 by cu-1 . cu. With an execution in mind, there is no ambiguity when referring \nto the frame or execution of a task by its identi.er. We say u is posted (resp., dispatched) in h when \nthere exists 1 = i<j such that the task identi.ed by u is posted (resp., dispatched) in ci . ci+1. 2 \nIn the presence of the assume-statement, only the values of completed executions are guaranteed to be \nvalid. 3 We could also de.ne task identi.ers by their tasks posting-points in h.  SKIP c[skip; s] . \nc[s] ASSUME true . e(c) c[assume e] . c[skip] IF-THEN true . e(c) c[if e then s1 else s2] . c[s1] IF-ELSE \nfalse . e(c) c[if e then s1 else s2] . c[s2] LOOP-DO true . e(c) c[while e do s] . c[s; while e do s] \nLOOP-END false . e(c) c[while e do s] . c[skip] ASSIGN-GLOBAL g ' . e(g, e)  (g, (e, S[g := e]) w, \nm).g ' , (e, S[skip]) w, m ASSIGN-LOCAL e ' . e(g, e) ' (g, (e, S[l := e]) w, m).g,e,S[skip]w, m CALL \ne ' . e(g, e)  (g, (e, S[call x := p(e)]) w, m).g,e ' ,sp(e, S[x := *]) w, m RETURN-SYNC v . e(g, e) \n g, (e, return e)e ' ,S[x := *]w, m.g,e ' ,S[x := v]w, m POST ' '' e . e(g, e) w =e ,sp  (g, (e, S[post \np(e)]) w, m).g, (e, S[skip]) w, give(m, w ' ) D ISPATCH  ' RETURN-ASYNC m ,w. take(m, g) \u00acblocked(w, \ng)  (g, e, m).g, w, m '(g, (e, return e) ,m).(g, e, m) Figure 2. The operational semantics of simple \nasynchronous programs, parameterized by the scheduler M = (D, empty, give, take). 2.4 Programs with \nPreemption The language of preemptive asynchronous programs extends the grammar of Figure 1 with the \nproduction s ::= yield, and the semantics of Figure 2 with the rule YIELD w ' = (e, S[skip]) w , (g, \n(e, S[yield]) w, m).g, e, give(m, w ' ) allowing arbitrary task resumptions not simply single frames \nto be added to, and later dispatched from, the task buffer.  2.5 Synchronization and Blocking Our model \nof preemptive asynchronous programs (i.e., with the yield-statement) can express arbitrary synchronization \ndisciplines. Example 1 (Locking). Lock-based mutual exclusion can be mod\u00adeled by adding an additional \nglobal variable lock. Critical sections are surrounded with the lock acquire operation, encoded by (while \nlock = true do yield); lock := true, and the lock release operation, encoded by lock := false. The blocked(w, \ng) predicate is de.ned to hold if and only if w = (e, S[while lock = true do yield]) w ' and lock(g, \ne)= true; i.e., w is waiting for a lock that is in use. Although the mere presence of these primitives \ndoes not imply that critical sections are mutually exclusive, ensuring mutual exclusion in their presence \nis a well-studied, and orthogonal problem [28]. Since exploration deadlocks when blocked tasks are scheduled, \nwe are generally interested in non-blocking schedulers. Example 2 (Non-blocking lock scheduling). A deterministic \nnon\u00adblocking scheduler for the lock-based mutual exclusion encoding of Example 1 can be de.ned by differentiating \nbetween ready tasks, and tasks waiting for the lock; the scheduler must not pick a waiting task when \nlock(g, e)= true. 3. Delay-Bounded Scheduling To limit the nondeterminism present in a scheduler, and \nthus the number of executions explored while at the same time retaining the ability to consider interesting \nexecutions! we allow deterministic schedulers to exercise a limited (and parameterized) number of deviations \nfrom their deterministic schedules. We de.ne a delaying scheduler as a tuple M = (D, empty, give, take, \ndelay), where D, empty, give, and take are de.ned as before for schedulers, and the delay : D \u00d7 Tasks \n. D function is intended to inform the scheduler of task-postponement. The de.nitions of deterministic, \nnon-blocking, pending, and schedulable remain unchanged. We extend the semantics of Figure 2 with a postponing \nrule DELAY m ' ,w. take(m, g) (g, e, m).g, e, delay(m ' ,w) which we henceforth refer to as a delay \n(operation), and we say w is delayed. Note that the delay operation occurs at the point when tasks are \nusually dispatched, not the at point when they are posted. We say an execution h is K-delay-bounded when \nthe number of delay operations in h is at most K. A delaying scheduler M is limit sound when for any \nbag-reachable global value g, there exists K . Nsuch that g is M-reachable in a K-delay-bounded execution. \nLimit soundness has an operational characterization. When h is an execution to c, we say w is blocked \n(resp., pending) in h when w is blocked (resp., pending) in c. A delaying scheduler M is lossless when \nfor any M-execution h and task w posted4 in h, w is either dispatched, pending, or blocked in h; otherwise \nwe say M is lossy. A delaying scheduler M is delay-accessible when for every reachable con.guration c1 \nwith non-blocked, pending task w, there exists a sequence c1 . ... . cj of DELAY-steps such that w is \nschedulable in cj . Lemma 1. A delaying scheduler M is limit sound if M is lossless and delay-accessible. \n4 Here we assume w is uniquely identi.ed by its task identi.er.  Intuitively, a lossless and delay-accessible \nscheduler can always u1 ; u2 u1 u1 spend delays to access any task pending in the bag scheduler, since \n ;   u5 ; \\ \\\\ ;  ;  ! !! ; ! !!\\ \\\\ bag-pending tasks are also pending in lossless schedulers. \nA detailed u5u2 proof appears in our extended technical report [13]. \\ \\\\ u2 ; / // ; /// u3\\\\\\ ; / \n// As a consequence of Lemma 1, our approach will discover any bug, with some delay-bound, using any \nwell-behaved (i.e., lossless &#38; delay-accessible) deterministic scheduler though the delay-bound u3 \nu4; u7 ; /// u10 \\ \\\\ u3 u4; u6u8 ; /// \\ \\\\ u4 u5; u8 ; /// \\\\\\ required to uncover a given bug depends \non the scheduler. Since the u6 u8 u9 u7 u9 u10 u7 u9 u10 number of explored schedules is exponential \nin the delay-bound, in 4. De practice one may want to hand-craft the scheduler to .t a particular Figure \nfore pth-.rst traversa ls of round-partitioned asynchro program-or bug-domain. Alternatively, it is also \npossible to run call sts w ith task identi.e rs u1, . . . , u10 . Arrow s indicate task\u00adnous several \nexplorations with different schedulers in parallel with the posting, dotte d arrows indicate delays, \nand th e dotte d line separates same (small) delay-bound, or even to choose schedulers at random. rounds. \nThe t raversal\u00ad order is u1, u2, . . . , u10. Here we highlight a couple of simple deterministic delaying \nschedulers to compare with existing exploration techniques, and as Schedul er 4 Let dfs be the stack-based \nelay) a basis for de.ning practical exploration algorithms. scheduler (Tasks * \u00d7 Tasks * (The dep \u00d7 Tasks \n* th-.rst scheduler). , (e, e , e) , giv e, take, d 3.1 Round-Robin Scheduling for a handler\u00ad sche duler \nobj ect (qh, elay-sta qr, qd), we ccks where th all qh, and qdqr One simple scheduler cycles through \ntasks in task-creation order. operatio , roun is d nd-, and d e.ned by e give(( qh, qr, qd) , w) The \nscheduler advances to the next task when the current task either (i) completes execution, (ii) yields, \nand is blocked, or (iii) is delayed. 1. push w to qh, Scheduler 3 (The round-robin scheduler). Let rr \nbe the list-based the dela y((qh , qr, qd) , w) ope ration is de.ned by delaying scheduler (Tasks * \u00d7 \nN, (e, 0) , give, take, delay), where the give((m, i) , w) operation is de.ned by5 1. push and the w \nto take( qd, (qh, qr, q d) , g) o peration is de .ned by 1. if w = (e, sp) for some procedure p, then \nappend w to m; 2. else (w is a yielding task): insert w into m at position i, 1. emp2. if qr ty qh = \ne into qr; then emp ty qd in to qr; the delay((m, i) , w) operation is de.ned by 3. if qr = e then return \n\u00d8; 1. insert w into m at position i; 4. pop w fro m qr; 2. increment i modulo |m|, 5. retur n w. and \nthe take((m, i) , g) operation is de.ned by Figure 3 It is easy to show s the dfssee tha -executit dfs \nis on of a simple deterministic asynchronous , lossless, and delay\u00ad program. 1. if m(j) is blocked in \ng for all j . 0 . . . |m| - 1, return \u00d8; 2. while m(i) is blocked in g, increment i modulo |m|; 3. remove \nw from m at position i; accessibTo uwe view le, thexecnders ough is nutions as tand the t ot generally \nnon-blocking. trees, wask-exe here nodes arcution order in e tasks, depth-.rst schedand the parent uling, \n4. return w. relation corr esponds t o the po sting relation on task s. Let h be an It is easy to see \nthat rr is deterministic, lossless, delay-accessible, executio n an d U a se t of tas k identi.ers f \nrom h. We de.ne the and non-blocking. asynchroby the i nous call foredenti.ers u . st6 as thU of ta e \nordered fores t F wit h nodes labeled The round-robin scheduler demonstrates that delay-bounded by a \ntas k u ' in h, u is a child ' in F ; csks posted in of u hildren h. Whe n u was posted scheduling captures \ncontext-bounded scheduling [34]. the orde r the y are po sted. W e capture yiel ding by are ordered by \nExample 3 (Context-bounding). In the rr-scheduler, each delay resumpt ions as newly\u00ad posted t asks. Note \ntha t synchr onous calls are viewing task\u00ad operation simulates a context-switch to the next task; context\u00ad \nnot expl icit in F : the t asks pos ted along the synchronous execution switching to any of n tasks generally \nrequires n successive delay of u appThe ear d irectly as xecution children of u. eds in operations. Given \na context-bound K and a program with n tasks, dfs-e of a p rogram proce counte a sequence of the set \nof nK-delay-bounded rr-executions contains the set of K\u00ad rounds, en ding whe n a tak e operation en \nle each rs both qh context-bounded executions. qr empt y. Wi th this no menclat ure, we schedu delayed \ntask w From the perspective of limiting scheduling choice, context\u00ad in the ro und i +1 follo wing the \nround i wher e w is d elayed, after all bounding does not give a direct bound: the number of choices \nnK pending A ro non\u00adund delayed t asks of round i have been scheduled. us call forest F with is also \ndependent on the number of tasks. (In contrast, K-delay\u00ad nodes U is a partition ing of a n asynchrono \nbounded exploration requires only K choices.) R(u1) = R labeling (u2) wh enever u1 is an anceR : U . \nN on the nodes of F such that stor of u2. 3.2 Depth-First Scheduling executioF into a n h, sync we can \nth ink of t he round partit ioning a s a partition of Another simple scheduler schedules all tasks posted \nby a given in the fo rest hronous c all fores ts {F0, F1, . . ing to the rou .} whernd in wh e each task \nu is task u before scheduling tasks that were pending when u was The dep th-.r FR(u) corst schedu respondler \ntrav erses the asyn chronou s call forest in ich it executes. dispatched, in a stack-based discipline. \nThis scheduler is particularly a round\u00ad by-r ound dep th-.rst preorder (see F igure 4 ). In this order, \nappealing since, as we show in Section 5, for any delay-bound K yielding tasks u are res chedule d only \nafter the tasks th at u has posted and asynchronous program P , we can encode the K-delay-bounded before \ny ieldi ng, have been scheduled. depth-.rst semantics of P compactly as a sequential program! 5 We de.ne \nthe give, delay, and take operations by destructively mutating 6 We conposted ini sider tially forests \nra or as we ther than trees since mo re than one task may be g of each round and returning the scheduler \nobject. in certain delaying schedulers. encount er shortly, at the beginnin u6 the and Given an  var \ng : T proc p1(var l : T ) s1; post p2 e1; s2; post p3 e2; s3 proc p2(var l : T ) s4 proc p3(var l : T \n) s5 s1 post p2 e1 s2 post>p3 e2s3 s4 s5 g0 > g1 > g1 > g2 g2 > g3 g3 > g4 g4 > g5 g g g  return \nreturnreturn \\ \\ r r push p2(e1) push p3(e2) r > > g3 g4 g4 g5 g0 on qh on qh g3 II ??  (e, p1)(e, \ne)(p2,e)(p2,e)(p3p2,e)(e, p2p3)(e, p3)(e, p3)(e, e)(e, e) Figure 3. The no-delay dfs-execution of a simple \nasynchronous program. The scheduler objects (qh,qr) since there are no delays, qd is omitted are given \nbelow the corresponding points in the execution which are labeled with the global values. Solid lines \nindicate procedure control, and dotted lines indicate dispatcher control. Note that the bag scheduler \nallows one additional execution, where p3 executes before p2. 4. Delay-Bounded Testing In concurrency \nanalyses which explicitly enumerate execution sched\u00adules (e.g., systematic testing), using delay-bounded \ndeterministic schedulers has a clear scalability advantage over existing bounded exploration approaches. \nFor instance, the number of p-preemption\u00adbounded executions [29] of a program using n tasks is exponential \nin n, since a complete execution must apply at least max(n, p) la\u00adbel changes; the number of k-delay-bounded \nexecutions does not depend on n. Here we demonstrate that although delay-bounding explores much fewer \nschedules than preemption-bounding (with the same bound), existing bugs caught with p preemptions are \nalso caught with p delays. Furthermore, delay-bounded exploration is able to discover bugs which have \nnot been caught by bounding preemptions due to the exponential increase in schedules with respect to \nthe number of tasks. To demonstrate this advantage, we have implemented the de\u00adlaying round-robin scheduler \n(Scheduler 3) in the CHESS con\u00ad currency testing tool [30] to directly compare preemption-and delay-bounding \non three programs: CCR, Futures (both written by Microsoft product groups), and Region Ownership (written \nby Peter M\u00a8 uller of ETH Zurich). We feel the round-robin scheduler is appropriate for comparison with \npreemption-bounding since it minimizes preemptions. We observe: 1. Every bug found with p preemptions \nis also found with p delays. 2. Delay-bounding explores signi.cantly fewer schedules before discovering \na buggy execution. 3. Delay-bounding can discover at least one bug that cannot be found with preemption-bounding, \nunder similar time-constraints.  As a systematic testing tool, CHESS repeatedly executes each test case \nto completion until all possible schedules for each test case have been explored. During the exploration, \nCHESS has no control over the number of tasks created, nor the number of steps taken by the input program. \nThus, each test case is expected to drive the program to termination under any schedule. CCR Microsoft \ns Concurrency and Coordination Runtime pro\u00advides a concurrent programming model with high-level primitives \nfor data-and work-coordination without the use of explicit threading and synchronization. We evaluate \na suite of 42 test cases exercising various parts of CCR. Each test case takes between 82 255 steps to \ncomplete, and creates at most 3 tasks, before exhibiting a known bug (found with CHESS); 41 of these \nbugs were discovered with 1 preemption, and the remaining one with 2 preemptions. The delay\u00ading rr-scheduler \ndiscovers each of these bugs with the same budget of delays, but fewer schedules. Figure 5 compares the \nnumber of schedules explored to discover each 1-preemption bug. Discover\u00ading the remaining bug required \nexploring 8, 895 schedules using 2 preemptions, and only 2, 728 schedules using 2 delays. Figure 5. \nComparing the number of schedules explored between preemption-and delay-bounding before discovering known \nbugs in 41 tests of Microsoft s Concurrency and Coordination Runtime (CCR). CHESS found each of these \nbugs with 1 preemption/delay. Futures Microsoft s Futures library provides a synchronization primitive \nbased on a proxy value for a computation whose result is not yet known. We .nd two previously-discovered \n(with CHESS) 1\u00adpreemption bugs, also with 1 delay. Both the livelock and uncaught exception are exposed \nafter the program has spawned 4 tasks: Bug # steps # schedules (PB) # schedules (DB) Livelock 1075 259 \n45 Exception 1152 1899 320 Region Ownership Peter M\u00a8 uller s region ownership library man\u00adages concurrency \nand coordination based on objects grouped into re\u00adgions that communicate with each other via asynchronous \nprocedure calls. The library is accompanied by a single test case comprising a one-producer one-consumer \nsystem. During testing, at most 5 tasks are created, and at most 284 execution steps taken. Preemption\u00adbounding \nwith 0 and 1 preemptions terminates without .nding a bug; with 2 preemptions, CHESS generates 340, 000 \nschedules over several hours without terminating, after which we manually killed it. Bounding delays \nto 0, 1, and 2 did not expose any bugs, though a delay-bound of 3 discovered a previously unknown bug \nafter exploring only 132, 507 schedules. To determine whether this bug can be discovered with 2 pre\u00ademptions, \nwe re-ran CHESS, focusing preemptions [3] to the meth\u00ad ods interrupted in our (delay-bounded) error trace. \nAfter exploring 128, 998 schedules, CHESS .nishes without discovering the bug. Thus, this is the .rst \nbug CHESS has ever found in a real-world program that requires at least 3 preemptions!  5. Sequentialization \nof Depth-First Scheduling The depth-.rst scheduling order (i.e., of Scheduler 4) has a rather nice property: \nthe stack of pending tasks used for a depth-.rst traversal of the asynchronous call forest (see Section \n3.2) can be combined with a synchronous program s call stack (i.e., of activation records). In this section \nwe exploit that fact to compactly encode a program s delay-bounded dfs semantics as a sequential program. \n(A similar encoding is possible for reverse depth-.rst scheduling i.e., where each task s children are \ntraversed in reverse order.) In Section 5.1 we reveal the most basic encoding of no-delay depth-.rst \nscheduling for simple asynchronous programs. In Sec\u00adtions 5.2 and 5.3, we orthogonally extend the basic \nencoding to handle delaying and preemption. To improve clarity we leverage the syntactic extensions of \nAppendix A, which each reduce to the original syntax of asynchronous programs. 5.1 No-Delay Depth-First \nScheduling In the translation, we replace post-statements with call\u00adstatements, and introduce the following \nvariables to ensure the cor\u00adrect global values are observed along the mimicked dfs-execution: gtmp caches \nthe global value of a posting task, so that the posted task can observe its future global value in g, \nand the posting task can resume from its current global value without interference.  \u00afg? stores the \n(guessed) global value reached when each posted task completes.  \u00afg stores the (guessed) global value \nreached after all tasks that have been posted thus far (i.e., that have appeared on the call\u00adstack) have \ncompleted.  Example 4. The dfs-encoding of the program in Figure 3 is var g : T, \u00afg : T proc p1(var \nl : T ) s1; We begin by de.ning the function [\u00b7]dfs 0 which translates a simple asynchronous program \nP into a synchronous program [P ]encodes the no-delay depth-.rst scheduler: ... .. ... .. let gtmp : \nT = g in let \u00afg? : T in 0 0 which = [post p2 e1]gtmp = g1, \u00afg? = g4 dfs dfs g := \u00afg; g\u00af:= \u00afg?; call := \np2 e1; ---. = \u00afg?; g := gtmp ; - H assume g 0 def 0 [var g : T [proc p (var l : T ) s [s1; s2 = var g \n: T, \u00afg : T [H]proc p (var l : T ) [s] dfs dfs s2; 0 def 0 = dfs dfs let gtmp : T = g in let \u00afg? : T \nin 00 def 00 = [post p3 e2]gtmp = g2, \u00afg? = g5 [s1]dfs; [s2] x := e dfs = dfs 0 def g := \u00afg; g\u00af:= \u00afg?; \ndfs call := p3 e2; [x := e = dfs assume g = \u00afg?; g := gtmp ; 0 def = skip dfs s3 0 def [skip] [assume \ne = assume e dfs 0 def 00 [if e then s1 else s2 = if e then [s1]dfs else [s2] dfs dfs def [while e do \ns 0 = while e do [s] 0 dfs dfs 0 def [call x := pe dfs = call x := pe 0 def [return e] dfs = return e \n0 def [post pe]dfs = let gtmp : T = g in let \u00afg? : T in g := \u00afg; \u00afg?; g := \u00afcall := pe; assume g = g\u00af?; \ng := gtmp . The initial con.guration is translated as 0 def '' [(g, (e, s; return e) ,m)]dfs = g , e,s \n; return e ,m , def with the initial global value g ' = { g = g, g\u00af= *} , and the initial statement s \n' given by def s ' = let g\u00af? in \u00afg?; g := \u00af 0 [s]dfs; assume g = \u00afg?; g := \u00afg. The key mechanism enabling \nthe encoding is the introduction of guessed global values in the form of unconstrained symbolic constants \n\u00afg?. These are essentially prophecy variables [1] whose values will be available only later in the sequential \nexecution. Once the appropriate global values are available, the corresponding guesses are suitably constrained. \nproc p2(var l : T ) s4 proc p3(var l : T ) s5, where we assume s1 s5 do not contain post-statements. \nTo the right of the braces we indicate the values stored in the let-bound variables gtmp and \u00afg?, which \nare the same values used below. This program has the following unique, sequential execution: s4 s5 g3 \n> g4 g4 > g5 call p2 e1return call p3 e2return r r g0 s1 > g1 g1 s2 > g2 g2 s3 > g3, where the double \nlines indicate part of a [post ...]dfs0 -translation (i.e., up to the call). Here each global value gi \nmatches that of the (asynchronous) dfs-execution (of Figure 3). Execution begins with an initial guess \nfor \u00afg of g3, i.e., the global value after p1 executes. The latter guessed values for \u00afg of g4 and g5 \nare the global values after the execution of p2 and p3. The .nal global value is g5. Lemma 2. Let P be \na simple asynchronous program. The syn\u00adchronous semantics of [P ]0 is g-equivalent to the no-delay dfs\u00ad \ndfs semantics of P . Proof sketch. Let h be a dfs-execution of P , and consider the asynchronous call \nforest F of h with tasks U = {u1,u2,...uj }. Furthermore, without loss of generality, suppose the dispatch \norder C of tasks in h is u1 C u2 C ... C uj . Since the dfs-schedule corresponds to a preorder depth-.rst \ntraversal of the forest F , each task u s children u ' are executed before any other pending task or \ntask to be posted later in u s execution. In this way, the order C corresponds exactly to the execution \nof synchronously executed tasks i.e., the order in which tasks would execute had they been called instead \nof posted except that the tasks would observe not the semantically-consistent global state at the end \nof the current task s execution, but an intermediate global state. To ensure that tasks observe the semantically-consistent \nglobal state, we rely on the following invariant of the synchronous semantics of [P ]dfs0 :  The value \nof \u00afg at the beginning of execution for each task ui+1 is equal to the .nal value of g at the end of \nexecution for the task ui immediately preceding ui+1 in the order C. Combining the invariant with the \ncoincidence of task-execution order in the dfs-schedule of P and the synchronous program [P ]0 dfs, we \nhave the sequence g0g1g2 ...gj of global values during dispatch points of h (i.e., when control is given \nto the dispatcher) is equal to the sequence g\u00af0g\u00af1g\u00af2 ... g\u00afj of global values before the synchronous \nexecution of each task, and after all tasks have executed. Finally, the last statement to be executed \nin [P ]dfs 0 sets the global value to g\u00afj , ensuring the .nal global states of P and [P ]0 are equal. \ndfs Note that in general, for an arbitrary deterministic scheduler, algorithmic delay-bounded exploration \nis not possible. For example, even no-delay scheduling with the round-robin scheduler (Sched\u00aduler 3) \nis undecidable, by reduction from the state-reachability problem for Turing-complete Queue machines. \nFrom an automata\u00adtheoretic point of view, delay-bounded depth-.rst scheduling re\u00admains decidable since \nthe asynchronous task-buffer is a stack which can be combined with the synchronous activation stack. \n 5.2 Delaying Depth-First Scheduling The function [\u00b7]K translates a simple asynchronous program P into \ndfs K a synchronous program [P ]dfs which encodes the K-delay bounded depth-.rst scheduler: -K def K+1 \n\u00af [var g : TH]dfs = var g : T, G : T, kdelay : N ---. [H] K dfs K def K [proc p (var l : T ) s dfs = \nproc p (var l : T, kround : N) [s]dfs K def [call x := pe dfs = call x := p (e, kround ) K def [post \npe] dfs = let gtmp : T = g in let \u00afg? : T in var k : N := kround ; while * and kdelay > 0 do kdelay := \nkdelay - 1; k := k +1 g := \u00af G[k]; \u00af G[k] := \u00afg?; call := p (e, k); assume g = g\u00af?; g := gtmp , where \nthe omitted statements are translated exactly as in the no-delay translation [\u00b7]0 . The initial con.guration \nis translated as dfs K def ' '' [(g, (e, s; return e) ,m)]dfs = g, e,s ; return e ,m , where the initial \nglobal and local values g ' and e ' are ' def g = { g = g, G\u00af= [*, *, . . . ], kdelay = K} , and ' def \ne = { l = e, kround = *} , and the initial statement s ' is given by ' def s = let kround : N =0 in let \n\u00afG? : T K+1 in \u00af G := \u00afG?; K [s]dfs; assume g = \u00afG?[0]; assume \u00af= G\u00af?[1]; G[0] ... assume \u00afG[K - 1] = \n\u00afG?[K]; g := \u00af G[K]. The K-delay-bounded encoding extends the no-delay encoding in two important ways. \nFirst, the schedule proceeds in K+1 rounds; we must note which round each task executes in, and separately \naccumulate the perceived global values in each round. Second, each time a task is posted, there is the \npossibility of delay; we must keep track of how many delays have been spent. To accomplish this, we introduce \nthe following auxiliary variables: gtmp and \u00afg? are used exactly as before, to cache the posting task \ns observed global value, and store the global value reached at the end of the posted task.  \u00afG is the \nmulti-round extension to \u00afg: each \u00af G[i] stores the global value reached after all tasks that have been \nposted thus far, and are executed in round i, have completed.  kdelay stores the remaining budget of \ndelays.  kround indicates which round a given task executes in.  k is incremented once per delay of \nthe posted task, then passed as the round-indicator (kround ).  Note that a task may be delayed more \nthan once, and we simulate all the delays of a given task instantaneously. Example 5. The 1-delay-bounded \ndfs-encoding of the program of Figure 3, allows the following (sequential) execution: g4 s4 > g5 call \np2 e1 return g3 s5 > g4 call p3 e2 return rr s1 s2 s3 >>> g0 g1 g1 g2 g2 g3, where p2 executes before \np3. This execution mimics two rounds, separated by a dashed line: p3, though posted second, executes \nwith p1 in the .rst round, while p2 executes alone in the second. Thus \u00af G[0] takes the values g3, g4, \nand \u00af G[1] takes the values g4, g5. The .nal global value is g5. Lemma 3. Let P be a simple asynchronous \nprogram. The syn\u00adchronous semantics of [P ]K is g-equivalent to the K-delay dfs\u00ad dfs semantics of P . \nProof sketch. As in the proof sketch of Lemma 2, there is a corre\u00adspondence between the K-delay-bounded \ndfs-execution order of tasks in P and [P ]K , except in this case the order is a round-by\u00ad dfs round \ndepth-.rst preorder. Here, we adapt the previous invariant to the K-round case: The value of \u00af G[k] at \nthe beginning of execution for each task ui+1 of round k is equal to the value of g at the end of execution \nfor the task ui immediately preceding ui+1 in the order C. For adjacently executed tasks ui C ui+1 in \ndiffering rounds ka and kb, the resulting global value of ui is guaranteed to be in \u00af G[ka], which is \nin turn guaranteed to be equal the initial global value \u00af G?[kb] of task ui+1.  dfs dfs 0 def 0 [proc \np (var l : T ) s]dfs = proc p (var l, \u00afg? : T ) [s]dfs def [call x := pe]dfs 0 = call (x, \u00afg?) := p (e, \ng\u00af?) 0 def [return e]dfs = return (e, \u00afg?) 0 def [post pe]dfs = let gtmp : T = g in let \u00afg? ' : T in \ng := \u00afg; \u00afg := g\u00af?' ; call ( , g\u00af' ?) := p (e, g\u00af' ?); assume g = \u00afg?' ; g := gtmp 0 def [yield]dfs = \nassume g = g\u00af?; \u00af g? := *; g := \u00afg; \u00afg? g := \u00af Figure 6. The symbolic encoding of no-delay depth-.rst \nschedul\u00ading with yields extending the symbolic encoding of Section 5.1. The translation of the missing \ncontrol-.ow statements and initial con.guration is identical to the yield-free encoding.  5.3 Depth-First \nScheduling with Preemption Perhaps surprising is the fact that our symbolic encodings of delaying depth-.rst \nschedulers can be extended to preemptive asynchronous programs. The encoding of Figure 6 extends the \nno-delay depth-.rst scheduler encoding of Section 5.1 to handle yield-statements. The key difference \nis that the guess (\u00afg?) of the post-state of each handler should not be veri.ed only at the end of a \nhandler s execution; instead, the guess is validated if a handler yields, at which point a new guess \nis made, and the new guess will be validated either at the next yield point if one exists or at the end \nof the handler s execution. To allow multiple guesses throughout a handler s execution, we simply ensure \nthe guess-variable \u00afg? is in scope throughout by making it a parameter to every procedure. The extension \nto delaying depth-.rst scheduling is straightfor\u00adward; extending to multiple rounds is orthogonal to \nhandling yields, and is done exactly according to the extension of Section 5.2. In particular, occurrences \nof \u00afg are replaced by their multi-round coun\u00adterparts \u00af G[k], and incrementing the round counters also \nhappens K at yield-points. We omit the full de.nition of [P ]dfs for preemptive asynchronous programs \nP , since it is redundant. Lemma 4. Let P be a preemptive asynchronous program. The synchronous semantics \nof [P ]K is g-equivalent to the K-delay dfs dfs-semantics of P .  5.4 Complexity of Depth-First Scheduling \nThus far we have given sequentializations that reduce delay-bounded depth-.rst semantics to sequential \nsemantics. Since the number of program variables in the resulting sequential program is O(K), the worst-case \ncomplexity of program-state exploration using this reduction (for programs with .nite-data domains) is \nexponential in K. What remains is the question of whether the exploration via this reduction is (asymptotically) \noptimal. Here we .nd that a sub-exponential algorithm is unlikely. The proofs of these results are quite \ntechnical, and can be found in our extended technical report [13]. For the remainder of this section \nwe assume the data-domain of asynchronous programs (i.e., the set Vals) is .nite-state. We show that \ndelay-bounded depth-.rst state-reachability is an NP\u00adcomplete problem.7 Though this exploration corresponds \nto an underapproximation of the program semantics, the complexity is lower than the precise EXPSPACE-complete \nexplorations of Sen and Viswanathan [37] and Jhala and Majumdar [19] for non-preemptive asynchronous \nprograms. Additionally, our underapproximation has a lower complexity than the PSPACE-hard underapproximation \n[5] used by Jhala and Majumdar [19] s algorithm, which corresponds to our (non-preemptive) bounded bag \nsemantics (see Scheduler 2). Note in the case of preemptive programs, the analysis problem is generally \nundecidable [36]. Interestingly, delay-bounded depth-.rst exploration has the same NP-complete complexity \nas context-bounding for a .nite number of tasks [27], even though delay-bounded scheduling explores an \nunbounded number of tasks. Problem 1 (Delay-bounded depth-.rst scheduling). For a given K . N, 8 initial \ncon.guration c0, and global value g of an asyn\u00adchronous program P , does there exist a K-delay-bounded \ndfs\u00adexecution to g? By reduction from the Circuit Satis.ability problem [33], we show our exponential \nalgorithm for delay-bounded depth-.rst scheduling is likely to be asymptotically optimal. Theorem 1. \nDelay-bounded depth-.rst scheduling is NP-hard. To show membership in NP for the non-preemptive case, \nwe give an algorithm that validates a nondeterministically-guessed execution witness to the target global \nvalue, given by the sequence of tasks delayed in, and a global value reached at the end of, each round. \nFor a delay-bound K, we validate the guess by applying K +1 polynomial-time sequential program analyses \nto sequential encodings of each round of execution the initial conditions of each round are determined \nby the delayed tasks and .nal global state of the previous round. Validation ensures that each round \ncan indeed reach the guessed global value while delaying exactly the guessed delayed tasks. Theorem 2. \nDelay-bounded depth-.rst scheduling for simple asyn\u00adchronous programs is in NP. In fact we can extend \nthe proof of Theorem 2 to the preemptive case. The presence of yields poses an additional technical challenge: \na na\u00a8ive extension of the execution witnesses to record delayed task\u00adresumptions does not work, because \nthe resumptions activation stacks have no bound. We solve this problem essentially by realizing the activation \nstacks of preempted tasks need not be stored across rounds; instead we may re-execute tasks to recreate \ntheir stacks from scratch the same so-called lazy sequentialization technique pioneered by La Torre et \nal. [23]. Theorem 3. Delay-bounded depth-.rst scheduling for preemptive asynchronous programs is in NP. \nThus we achieve tight complexity-bounds on depth-.rst scheduling. Corollary 1. Delay-bounded depth-.rst \nscheduling is NP-complete. 7 Since we are interested in measuring the scheduling complexity (rather than \ncomplexity arising from program data), we have restricted the program syntax so that a .xed number of \nvariables is in scope at any moment. Indeed, the reachability problem is EXPTIME-complete in the number \nof variables (even with only a single recursive task), due to the logarithmic encoding of states in the \ncorresponding pushdown system. 8 We assume the delay-bound K is written in unary.  6. Delay-Bounded \nVeri.cation The sequentialization of Section 5 allows any sequential analysis algorithm to be lifted, \nimmediately, to a concurrent analysis algo- K rithm: a sequential analysis of [P ]dfs is exposed to all \nconcurrent behaviors of P with a K-delay-bounded depth-.rst scheduler (an underapproximation of P s concurrent \nsemantics). The additional implementation effort is minimal since only the source-to-source translation \nis required. As a proof-of-concept, we have implemented a source-to-source symbolic encoding of the delaying \ndepth-.rst scheduler in the STORM [26] concurrent C-program checker. STORM analyzes closed concurrent \nsoftware modules (i.e., each module is closed from below using stubs for external procedures, and closed \nfrom above using a test driver with symbolic inputs). While CHESS concretely executes a closed concurrent \nprogram with a single input vector (see Section 4), STORM symbolically veri.es a closed concurrent program \non a (potentially unbounded) set of input vectors. To analyze the input program precisely, STORM unfolds \nloops and recursive procedure calls up to a user-provided bound. Prior to this work, STORM implemented \ncontext-bounded veri.\u00adcation for programs with a .nite number of statically declared tasks. Our experience \napplying STORM to realistic programs indicated that the theoretical exponential complexity in the number \nof exe\u00adcution contexts does manifest in practice [26]. Furthermore, many concurrency errors in realistic \nprograms require a large (i.e., > 3) number of tasks to be executed, and the number of tasks required \nto manifest a concurrency error is a lower bound on the number of re\u00adquired contexts. In these cases, \ncontext-bounded discovery becomes prohibitively expensive. In contrast, delay-bounded scheduling discovers \nthese errors with very few delays typically 1 or 2. The reason depth-.rst scheduling works well (despite \nthe fact that it is an arti.cial ordering) is that the majority of these tasks need not participate in \nany intricate interaction; they simply need to be executed in causal order to reach a program point which \nmanifests the bug. Thus, our implementation of delay-bounded scheduling improves STORM in two important \nways: we enable STORM to handle programs with dynamic task-creation, and to .nd a thus-far elusive class \nof bugs at low computational cost. The (end-to-end) implementation works in three phases. In the .rst \nphase, we translate a concurrent C program into a concurrent BOOGIE [11] program though BOOGIE was originally \nintended as an intermediate language for representing the semantics of simple imperative sequential programs, \nwe have extended the syntax and semantics to express concurrent behavior. In the second phase, we transform \nthe concurrent BOOGIE program into a sequential BOOGIE program encoding the delay-bounded depth-.rst \nsemantics. It is noteworthy that our algorithm was very easy to implement with simple, minimal extensions \nto STORM. Finally, we verify the resulting sequential BOOGIE program using the triumvirate of .eld abstraction \n[26], veri.cation-condition generation [4], and satis.ability-modulo-theory (SMT) solving [10]. We have \napplied our implementation to symbolic exploration of over 20 preemptible event-driven device drivers \nwith 1K 30K lines of code. In the process we have found 4 previously unknown bugs with a maximum delay-budget \nof 2; the developers of these drivers have con.rmed the accuracy of these bugs. More importantly, with \nthe ability to handle dynamic task-creation, we can precisely model the asynchrony in the device driver \ns execution environment (e.g., interrupts, deferred procedure calls, timers, driver request cancellation \nand completion, etc.), thereby qualitatively extending the applicability and precision of STORM. 7. Related \nWork The programming models considered in this paper have received plenty of attention from researchers \ninterested in stack-based, .nite\u00addata abstractions of concurrent programs. The preemptive model has not \nbeen so heavily studied, primarily because the reachability prob\u00adlem is known to be undecidable [36] \neven with a .nite number of tasks due to interference between multiple stacks. More attention has been \npaid to the non-preemptive model of asynchronous pro\u00adgrams. Sen and Viswanathan [37] introduced the model \nexplicitly to reason about event-driven programs, and showed that control-state reachability is EXPSPACE-hard; \nGanty and Majumdar [14] tight\u00adened this result to show that the problem is EXPSPACE-complete. To combat \nthis high worst-case theoretical complexity, Jhala and Ma\u00adjumdar [19] suggest a scheme that combines \nan underapproximate and an overapproximate computation of the reachable states. Context-bounded veri.cation \n[34, 35] explores only those ex\u00ad ecutions of a concurrent program in which the number of context switches \nis bounded globally by a user-supplied value. However, the idea of context-bounding does not make intuitive \nsense for pro\u00adgrams with large or unbounded number of tasks. This problem is well-known, and other researchers \nhave proposed various .xes. Atig et al. [2] suggested strati.ed context-bounding that allows an un\u00adbounded \nnumber of context switches without sacri.cing decidability. La Torre et al. [24] exploit the nondeterministic \nround-robin schedul\u00ading scheme of Lal and Reps [27] to achieve unbounded number of context switches for \nparameterized concurrent programs. Exploiting sequential veri.ers for concurrent program veri.ca\u00adtion \nis also an active area. The KISS veri.er [35] pioneered this approach by providing a source-to-source \ntransformation from multi\u00adthreaded programs into sequential programs that underapproximates the set of \nbehaviors of the original program. Lal and Reps [27] achieved a breakthrough by providing the .rst source-to-source \ntranslation that computes a context-bounded underapproximation for any context-bound. This approach also \npioneered the idea of guessing future-values and constraining them later at an appropriate control point \nin the execution; we exploit this idea in our transfor\u00admation as well. A key weakness of Lal and Reps \n[27] s so-called eager approach is that control states unreachable in the original concurrent program \nmay be explored in the transformed sequen\u00adtial program; La Torre et al. [23] s lazy technique addresses \nthis weakness by repeatedly re-executing to the control points where guessed values would have been used; \nGhafari et al. [16] empirically compared the two approaches in the veri.cation-condition-checking paradigm \nwhere, as opposed to model-checking, bene.ts of lazi\u00adness are unclear since the eager approach in fact \noutperforms the lazy one. Kidd et al. [21] have introduced a reduction from con\u00adcurrent programs with \npriority-preemptive schedulers to sequential programs, though the construction requires a bound on the \nnumber of tasks. Although none of these sequentializations handle dynamic task-creation, La Torre et \nal. [25] have recently introduced a sequen\u00adtialization of their parameterized model-checking algorithm \n[24] which does handle an unbounded number of tasks. Finally, runtime-schedule fuzzers such as CONTEST \n[12] and CALFUZZER [20] introduce delays by adding sleep statements; PCT [7] introduces delays by randomly \nperturbing task priorities. These techniques share with delay-bounding the ability to scale to many tasks. \nIn fact, a derandomized version of the PCT algorithm provides a scheduling complexity that is independent \nof the number of program tasks [6]; the mechanism for achieving this scalability is based on their characterization \nof a bug s depth as the minimum number of events that must occur in a certain order to reveal the bug. \nBug-depth attempts, like delay-bounding, to canonize the effort required to discover a given bug, but \nis de.ned with respect to ordering constraints rather than deviations from a deterministic scheduler. \n 8. Conclusion We have introduced a canonical characterization of scheduling nondeterminism by considering \ndeterministic schedulers with the ability to delay their next-scheduled task. We demonstrate that delay\u00adbounding \nis an effective search prioritization strategy for concurrent programs by extending the applicability \nof existing prioritization techniques. Furthermore, we identify a lower-complexity concurrent analysis \nproblem via delay-bounded depth-.rst scheduling, and we show that our depth-.rst delaying schedulers \nadmit practical sequential reductions, allowing us to lift existing sequential analyses to concurrent \nanalyses. Our approach is generally applicable to concurrent programs with dynamic task-creation, and \narbitrary preemption and synchronization. Acknowledgments We thank Ahmed Bouajjani, Pierre Ganty, Rupak \nMajumdar, and Gennaro Parlato for providing helpful insight, and the anonymous reviewers for their numerous \ncomments and suggestions. References [1] M. Abadi and L. Lamport. The existence of re.nement mappings. \nTheor. Comput. Sci., 82(2):253 284, 1991. [2] M. F. Atig, A. Bouajjani, and S. Qadeer. Context-bounded \nanalysis for concurrent programs with dynamic creation of threads. In TACAS 09: Proc. 15th International \nConference on Tools and Algorithms for the Construction and Analysis of Systems, volume 5505 of LNCS, \npages 107 123. Springer, 2009. [3] T. Ball, S. Burckhardt, K. E. Coons, M. Musuvathi, and S. Qadeer. \nPreemption sealing for ef.cient concurrency testing. In TACAS 10: Proc. 16th International Conference \non Tools and Algorithms for the Construction and Analysis of Systems, volume 6015 of LNCS, pages 420 \n434. Springer, 2010. [4] M. Barnett and K. R. M. Leino. Weakest-precondition of unstructured programs. \nIn PASTE 05: Proc. ACM SIGPLAN-SIGSOFT Workshop on Program Analysis For Software Tools and Engineering, \npages 82 87. ACM, 2005. [5] A. Bouajjani and R. Majumdar. Personal communication, July 2010. [6] S. Burckhardt \nand M. Musuvathi. Personal communication, November 2010. [7] S. Burckhardt, P. Kothari, M. Musuvathi, \nand S. Nagarakatte. A randomized scheduler with probabilistic guarantees of .nding bugs. In ASPLOS 10: \nProc. 15th International Conference on Architectural Support for Programming Languages and Operating \nSystems, pages 167 178. ACM, 2010. [8] E. M. Clarke and E. A. Emerson. Design and synthesis of synchro\u00adnization \nskeletons using branching-time temporal logic. In Logic of Programs, volume 131 of LNCS, pages 52 71. \nSpringer, 1981. [9] J. Corbet, A. Rubini, and G. Kroah-Hartman. Linux Device Drivers. O Reilly Media, \nInc., 3rd edition, 2005. [10] L. M. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS 08: \nProc. 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, \nvolume 4963 of LNCS, pages 337 340. Springer, 2008. [11] R. DeLine and K. R. M. Leino. BoogiePL: A typed \nprocedural language for checking object-oriented programs. Technical Report MSR-TR\u00ad2005-70, Microsoft \nResearch, 2005. [12] O. Edelstein, E. Farchi, E. Goldin, Y. Nir, G. Ratsaby, and S. Ur. Framework for \ntesting multi-threaded Java programs. Concurrency and Computation: Practice and Experience, 15(3-5):485 \n499, 2003. [13] M. Emmi, S. Qadeer, and Z. Rakamari\u00b4 c. Delay-bounded scheduling: A canonical characterization \nof scheduler nondeterminism. Technical Report MSR-TR-2010-123, Microsoft Research, 2010. http:// research.microsoft.com/apps/pubs/?id=138569. \n[14] P. Ganty and R. Majumdar. Algorithmic veri.cation of asynchronous programs. CoRR, abs/1011.0551, \n2010. http://arxiv.org/abs/ 1011.0551. [15] J. J. Garrett. Ajax: A new approach to web applications, \nFebruary 2005. http://www.adaptivepath.com/ideas/essays/ archives/000385.php. [16] N. Ghafari, A. J. \nHu, and Z. Rakamari\u00b4 c. Context-bounded translations for concurrent software: An empirical evaluation. \nIn SPIN 10: Proc. 17th International Workshop on Model Checking Software, volume 6349 of LNCS, pages \n227 244. Springer, 2010. [17] P. Godefroid. Model checking for programming languages using VeriSoft. \nIn POPL 97: Proc. 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 174 \n186. ACM, 1997. [18] J. L. Hill, R. Szewczyk, A. Woo, S. Hollar, D. E. Culler, and K. S. J. Pister. System \narchitecture directions for networked sensors. In ASPLOS 00: Proc. 9th International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 93 104. ACM, 2000. [19] R. Jhala and R. \nMajumdar. Interprocedural analysis of asynchronous programs. In POPL 07: Proc. 34th ACM SIGPLAN-SIGACT \nSympo\u00adsium on Principles of Programming Languages, pages 339 350. ACM, 2007. [20] P. Joshi, M. Naik, \nC.-S. Park, and K. Sen. CalFuzzer: An extensible active testing framework for concurrent programs. In \nCAV 09: Proc. 21st International Conference on Computer Aided Veri.cation, volume 5643 of LNCS, pages \n675 681. Springer, 2009. [21] N. Kidd, S. Jagannathan, and J. Vitek. One stack to run them all: Reducing \nconcurrent analysis to sequential analysis under priority scheduling. In SPIN 10: Proc. 17th International \nWorkshop on Model Checking Software, volume 6349 of LNCS, pages 245 261. Springer, 2010. [22] E. Kohler, \nR. Morris, B. Chen, J. Jannotti, and M. F. Kaashoek. The Click modular router. ACM Trans. Comput. Syst., \n18(3):263 297, 2000. [23] S. La Torre, P. Madhusudan, and G. Parlato. Reducing context-bounded concurrent \nreachability to sequential reachability. In CAV 09: Proc. 21st International Conference on Computer Aided \nVeri.cation, volume 5643 of LNCS, pages 477 492. Springer, 2009. [24] S. La Torre, P. Madhusudan, and \nG. Parlato. Model-checking parame\u00adterized concurrent programs using linear interfaces. In CAV 10: Proc. \n22nd International Conference on Computer Aided Veri.cation, volume 6174 of LNCS, pages 629 644. Springer, \n2010. [25] S. La Torre, P. Madhusudan, and G. Parlato. Sequentializing parame\u00adterized programs, 2010. \nUnder submission. [26] S. K. Lahiri, S. Qadeer, and Z. Rakamari\u00b4 c. Static and precise detection of concurrency \nerrors in systems code using SMT solvers. In CAV 09: Proc. 21st International Conference on Computer \nAided Veri.cation, volume 5643 of LNCS, pages 509 524. Springer, 2009. [27] A. Lal and T. W. Reps. Reducing \nconcurrent analysis under a context bound to sequential analysis. Formal Methods in System Design, 35(1): \n73 97, 2009. [28] L. Lamport. Proving the correctness of multiprocess programs. IEEE Trans. Software \nEng., 3(2):125 143, 1977. [29] M. Musuvathi and S. Qadeer. Iterative context bounding for systematic \ntesting of multithreaded programs. In PLDI 07: Proc. ACM SIGPLAN Conference on Programming Language Design \nand Implementation, pages 446 455. ACM, 2007. [30] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. \nNainar, and I. Neamtiu. Finding and reproducing Heisenbugs in concurrent pro\u00adgrams. In OSDI 08: Proc. \n8th USENIX Symposium on Operating Systems Design and Implementation, pages 267 280. USENIX Associ\u00adation, \n2008. [31] W. Oney. Programming the Microsoft Windows Driver Model. Mi\u00adcrosoft Press, 2nd edition, 2002. \n[32] V. S. Pai, P. Druschel, and W. Zwaenepoel. Flash: An ef.cient and portable web server. In USENIX \n99: Proc. General Track of the USENIX Annual Technical Conference, pages 199 212. USENIX, 1999.  [33] \nC. H. Papadimitriou. Computational Complexity. Addison Wesley, 1993. [34] S. Qadeer and J. Rehof. Context-bounded \nmodel checking of concurrent software. In TACAS 05: Proc. 11th International Conference on Tools and \nAlgorithms for the Construction and Analysis of Systems, volume 3440 of LNCS, pages 93 107. Springer, \n2005. [35] S. Qadeer and D. Wu. KISS: Keep it simple and sequential. In PLDI 04: Proc. ACM SIGPLAN Conference \non Programming Language Design and Implementation, pages 14 24. ACM, 2004. [36] G. Ramalingam. Context-sensitive \nsynchronization-sensitive analysis is undecidable. ACM Trans. Program. Lang. Syst., 22(2):416 430, 2000. \n[37] K. Sen and M. Viswanathan. Model checking multithreaded programs with asynchronous atomic methods. \nIn CAV 06: Proc. 18th Interna\u00adtional Conference on Computer Aided Veri.cation, volume 4144 of LNCS, pages \n300 314. Springer, 2006. A. Syntactic Sugar The following syntactic extensions are reducible to the original \nsyntax of asynchronous programs of Section 2.2. Here we freely assume the existence of various type-and \nexpression-constructors. This does not present a problem since our program semantics does not restrict \nthe language of types nor expressions. Multiple types. Multiple type labels T1,...,Tj can be encoded \nby cj systematically replacing each Ti with the sum-type T = i=1 Ti. This allows local and global variables \nwith distinct types. Multiple variables. Additional variables x1 : T1,..., xj : Tj can be encoded with \na single record-typed variable x : T , where T is the record type { f1 : T1,..., fj : Tj} , and all occurrences \nof xi are replaced by x.fi . When combined with the extension allowing multiple types, this allows each \nprocedure to declare any number and type of local variable parameters, distinct from the number and type \nof global variables. Local variable declarations. Additional (non-parameter) local variable declarations \nvar l ' : T to a procedure p can be encoded by adding l ' to the list of parameters, and systematically \nadding an initialization expression (e.g., the choice expression *, or false) to the corresponding position \nin the list of arguments at each call site of p to ensure that l ' begins correctly (un)initialized. \nUnused values. Call assignments call x := pe, where x is not subsequently used, can be written as call \n:= pe, where : T is an additional unread local variable, or simpler yet as call pe. Let bindings. Let \nbindings of the form let x : T = e in can be encoded by declaring x as a local variable var x : T immediately \nfollowed by an assignment x := e. This construct is used to explicate that the value of x remains constant \nonce initialized. The binding let x : T in is encoded by the binding let x : T = * in, where * is the \nchoice expression. Tuples. Assignments (x1,...,xj ) := e to a tuple of variables x1,...,xj are encoded \nby the sequence let r : { f1 : T1,..., fj : Tj} = e in x1 := r.f1 ; ... ; xj := r.fj , where r is a fresh \nvariable. A tuple expression (x1,...,xj ) occurring in a statement s is encoded as let r : { f1 : T1,..., \nfj : Tj} = { f1 = x1,..., fj = xj} in s[r/(x1,...,xj )], where r is a fresh variable, and s[e1/e2] replaces \nall occurrences of e2 in s with e1. When a tuple-element xi on the left-hand side of an assignment is \nunneeded (e.g., from the return value of a call), we may replace the occurrence of xi with the variable \nsee the unused values desugaring. Arrays. Finite T j-arrays with j elements of type T can be encoded \nas records of type T ' = { f1 : T,..., fj : T } , where f1 ,..., fj are fresh names. Occurrences of terms \na[i] are replaced by a.fi , and array-expressions [e1,...,ej ] are replaced by record-expressions { f1 \n= e1,..., fj = ej} .   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>We provide a new characterization of scheduling nondeterminism by allowing deterministic schedulers to delay their next-scheduled task. In limiting the delays an otherwise-deterministic scheduler is allowed, we discover concurrency bugs efficiently---by exploring few schedules---and robustly---i.e., independent of the number of tasks, context switches, or buffered events. Our characterization elegantly applies to any systematic exploration (e.g., testing, model checking) of concurrent programs with dynamic task-creation. Additionally, we show that certain delaying schedulers admit efficient reductions from concurrent to sequential program analysis.</p>", "authors": [{"name": "Michael Emmi", "author_profile_id": "81333488438", "affiliation": "Universit&#233; Paris Diderot, Paris, France", "person_id": "P2509641", "email_address": "mje@liafa.jussieu.fr", "orcid_id": ""}, {"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2509642", "email_address": "qadeer@microsoft.com", "orcid_id": ""}, {"name": "Zvonimir Rakamari&#263;", "author_profile_id": "81413597614", "affiliation": "University of British Columbia, Vancouver, BC, Canada", "person_id": "P2509643", "email_address": "zrakamar@cs.ubc.ca", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926432", "year": "2011", "article_id": "1926432", "conference": "POPL", "title": "Delay-bounded scheduling", "url": "http://dl.acm.org/citation.cfm?id=1926432"}