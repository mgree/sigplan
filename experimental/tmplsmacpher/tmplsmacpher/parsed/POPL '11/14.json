{"article_publication_date": "01-26-2011", "fulltext": "\n Calling Context Abstraction with Shapes Xavier Rival INRIA Paris-Rocquencourt * rival@di.ens.fr Abstract \nInterprocedural program analysis is often performed by computing procedure summaries. While possible, \ncomputing adequate sum\u00admaries is dif.cult, particularly in the presence of recursive proce\u00addures. In \nthis paper, we propose a complementary framework for interprocedural analysis based on a direct abstraction \nof the calling context. Speci.cally, our approach exploits the inductive structure of a calling context \nby treating it directly as a stack of activation records. We then build an abstraction based on separation \nlogic with inductive de.nitions. A key element of this abstract domain is the use of parameters to re.ne \nthe meaning of such call stack summaries and thus express relations across activation records and with \nthe heap. In essence, we de.ne an abstract interpretation-based analysis framework for recursive programs \nthat permits a .uid per call site abstraction of the call stack much like how shape analyz\u00aders enable \na .uid per program point abstraction of the heap. Categories and Subject Descriptors D.2.4 [Software \nEngineer\u00ading]: Software/Program Veri.cation; F.3.1 [Logics and Meanings of Programs]: Specifying and \nVerifying and Reasoning about Pro\u00adgrams; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages Program analysis General Terms Languages, Veri.cation Keywords interprocedural analysis, context-sensitivity, \ncalling context, shape analysis, inductive de.nitions, separation logic, symbolic abstract domain 1. \nIntroduction It seems there are few things, if any, more fundamental in program\u00adming than procedural \nabstraction. Hence, without quali.cation, in\u00adterprocedural analysis is simply something that static program \nan\u00adalyzers need to do well. Yet, precise interprocedural static analysis in presence of recursion is \ndif.cult analyzers need to simultane\u00adously abstract unbounded executions, unbounded calling context, \nand unbounded heap structures. Broadly speaking, there are two main approaches to interpro\u00adcedural analysis \nwith different strengths and weaknesses. The .rst approach is to compute procedure summaries (e.g., [4, \n16, 20]). \u00b4Sup\u00b4erieure, Paris * Abstraction Project-team, shared with CNRS and Ecole Normale Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 11, January \n26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 \nBor-Yuh Evan Chang University of Colorado, Boulder bec@cs.colorado.edu These summaries are then used \nto modularly interpret function calls (i.e., derivatives of the functional approach [28]). This approach \nis exceedingly common, as modularity is important, if not a prerequi\u00adsite, for scalability. Unfortunately, \ncomputing effective summaries is not easy for all families of properties. Intuitively, it is more com\u00adplex \nto abstract relations between pairs of states than to abstract sets of states. The former is the essence \nof what needs to be done to compute procedure summaries, while the latter is what is more typical in \nprogram analysis. The second approach is to perform whole program analysis that, at least conceptually, \ncompletely ignores procedural abstraction by inlining function calls. Therefore, the analysis only needs \nto ab\u00adstract sets of states (instead of relations on pairs of states). While this kind of analysis yields \ncontext-sensitivity without the complex\u00adities of deriving procedure summaries, it is clear that the exponen\u00adtial \nblow-up makes scaling signi.cantly more dif.cult. This blow\u00adup then exerts negative pressure on the choice \nin precision of the state abstraction. Certainly, these approaches are not entirely disjoint and over\u00adlap \nto some extent. However, in both situations, the typical result is a rather coarse abstraction of calling \ncontexts. In the modular analysis situation, procedure summaries capture precisely what is touched by \nthe function in question but assume the context to be arbitrary. For whole program analysis, the values \nof locals in the call stack above the current call are typically completely abstract. Coarse calling \ncontext abstraction makes it dif.cult to tackle, for example, recursive procedures where there are critical \nrelations be\u00adtween successive activation records. In this paper, we propose abstracting calling contexts \nprecisely by directly modeling the call stack of activation records, that is, we explicitly push the \ncall stack into the state on which we abstract. To do so, we exploit the fact that the call stack has \na regular, inductive structure so that shape analysis techniques apply. Under the hood, we use separation \nlogic formulas [21] with inductive de.nitions in order to elaborate precise and concise stack descriptions. \nWe formalize our call stack abstraction inside the XISA shape analysis framework [5, 6], as we leverage \nthis abstract domain, which is parametrized by inductive de.nitions. While uncommon in practice, there \nhas been prior work on di\u00adrectly abstracting the call stack [22] in the TVLA framework [27]. This approach \nrequires careful choice of a set of predicates for the modeling of stack summaries. Instead, leveraging \nthe natural in\u00adductive structure of the call stack and a framework built around inductive de.nitions, \nwe seek to lower the con.guration effort. In particular, we show that the inductive de.nitions for the \nsumma\u00adrization of the call stack can be derived automatically. Overall, our motivation is to de.ne global \nprogram analyses for programs with recursive functions that make use of a precise char\u00adacterization of \nthe calling context (including the status of the pend\u00ading call returns). While we apply a shape domain \nto the abstraction of the call stack, our focus is not shape analysis per-se. Certainly, heap shapes \ncontinue to .t naturally into the framework, but there is  void main() { dll* l; . . . // l is a list \n(maybe not doubly-linked) l = fix(l, NULL); } dll* fix(dll* c, dll* p) { dll* ret; if (c != NULL) { c->prev \n= p; . c->next = fix(c->next, c); if (check(c->data)) { ret = c->next; remove(c); I } else { ret = c; \n} } else { ret = NULL; } return ret; } void remove(dll* n) { if (n->prev != NULL) n->prev->next = n->next; \nif (n->next != NULL) n->next->prev = n->prev; free(n); } (a) The recursive function fix. (b) After two \nrecursive calls to fix and just about to make another recursive call at .. (c) Before return from the \nsecond recursive call to fix at I. Figure 1. An explanatory recursive program shown with diagrams depicting \nboth the call stack and the heap at two points in its execution. independent interest for numerical domains. \nIn particular, the pro\u00adcedure summary approach has not been adapted to large classes of numerical domains, \nso the technique proposed in this paper takes steps towards a plug-in or product domain-style extension \nof base domains to precise interprocedural analysis. If heap shapes are of interest, we assume our abstract \ndomain has been instantiated with the appropriate inductive de.nitions describing them (e.g., they come \nfrom the user in the form of an inductive checker [6] corresponding to structural consistency checking \ncode). That is, while we automatically derive a program\u00adspeci.c inductive de.nition to summarize call \nstacks, we do not try to infer inductive de.nitions for recursive heap structures. From a technical point \nof view, we exploit the fact that the call stack has a .xed recursive backbone in contrast to user-de.ned \nrecursive structures in the heap, which do not have a set shape. From a usability perspective, the stack \nof activations records is a low-level implementation mechanism for procedural abstraction and thus such \ninductive de.nitions would be problematic to expect from the user in contrast to heap shapes, which are \nprogrammer-designed. Finally, we clarify that we do not necessarily advocate a precise call stack abstraction \nin all situations. Rather our technique .lls a gap in the analysis of programs with recursive procedures. \nIn this paper, we make the following contributions: We de.ne an abstract domain that models the call \nstack directly in an exact manner (Section 3) and with summarization (Sec\u00ad tion 4). The novel aspect \nof our approach is to leverage the in\u00ad ductive structure of the call stack by using a parametric shape \ndomain based on separation logic and inductive de.nitions.  We give an algorithm for automatically deriving \ninductive cases for call stack summarization (Section 5). That is, the inductive de.nition stack used \nto summarize the call stack is de.ned on the .y in a program-speci.c manner.  We describe an analysis \nfor programs with recursive procedures using this call stack abstraction (Section 6).  We provide evidence \nthrough a case study that our call stack ab\u00adstraction can be used to overcome precision issues in the \nmod\u00adular approach (Section 7). That is, a less precise base domain with the call stack abstraction is \nsuf.cient for certain examples where a more precise one is needed with the modular approach. 2. Overview \nIn this section, we illustrate the main challenges in designing a precise abstraction of the calling \ncontext by following an example execution of the recursive function fix shown in Figure 1(a). Then, through \nthis discussion, we preview our abstraction technique. Consider the recursive function fix shown in Figure \n1(a). This function takes as input a pointer c to a dll structure. A dll structure has three .elds, next, \nprev, and data used to represent a doubly\u00adlinked list of integers. The function fix does two things: \n(1) it takes as input a singly-linked list (i.e., the prev links are unused or potentially invalid) and \nsets the prev .eld of each node to create a valid doubly-linked list; and (2) it implements a .ltering \noperation where all nodes whose data .eld satis.es the check function are removed. It implements this \nfunctionality by a recursive walk using the c pointer. On each call, the p pointer points to the previous \nnode where c->prev should be set. In particular, during the downward sequence of recursive calls, it \nupdates c->prev to set up the doubly\u00adlinked list invariant. Then, on the upward sequence of returns, \nit removes all nodes whose data .eld satis.es check. To simplify our presentation, function fix also \nuses a local variable ret so that there is only one return site. It is certainly possible to perform \nthe .ltering along the down\u00adward path of calls instead of upward path of returns, which in fact would \nlikely make analysis easier. However, in this case, the de\u00adveloper has chosen to do the removal along \nthe upward sequence, perhaps because she wants to call the library function remove. The remove function \nexpects a doubly-linked list node, but the doubly\u00adlinked list invariant is not established until the \ndownward sequence is complete. While this example is synthetic, it exempli.es in a small fragment many \nof the key challenges in analyzing recursive imperative programs: (1) state changes on both the downward \ncall path and upward return path, (2) incomplete or temporary breakage of data structure invariants along \nthe recursive call-return paths, and  (3) interactions with heap state conceptually belonging to callers. \nTo illustrate the analysis challenges, let us consider the concrete program state at two points in an \nexample execution. Figure 1(b) shows the concrete state right after two recursive calls to fix (i.e., \nmain has called fix, which has called itself twice and is at the program point marked with .). To .x \na convention, we say that the .rst call to fix from main is not recursive. Figure 1(c) depicts the concrete \nstate just before the return of this same call and where the current node (pointed to by c) has just \nbeen removed (shown grayed out). In other words, check(11) evaluated to true and the state is at the \nprogram point marked with C. We can also see that between these two states, there have been two call-returns \nfor the last two nodes in the list (where their prev .elds have been set appropriately but neither node \nwas removed). In addition to the list allocated in the heap shown in the right part of Figures 1(b) and \n1(c), we show the call stack in the left part. In our picture, the call stack consists of sequence of \nactivation records where each .eld is a local variable (e.g., p, c, and ret for activation records of \nfix). Our approach to interprocedural analysis is essentially to ab\u00adstract the notion of state depicted \nin Figures 1(b) and 1(c). Tradi\u00ad tional shape analysis focuses on precise summarization of the heap, \nthat is, the right part in the concrete state diagrams, while the call stack is elided or coarsely abstracted. \nIn this paper, we de.ne a precise abstraction of the call stack the left part in the diagrams. To gradually \nbuild up to our technique, let us consider an intu\u00aditive abstraction of the state shown in Figure 1(b) \nwith only heap summarization: Here, the nodes represent heap addresses, the thin edges denote .elds \nof a dll node (i.e., the edges labeled with next and prev), and the thick edge labeled sll represents \na singly-linked list of dll nodes of undetermined length. The next and prev edges correspond to those \n.elds of the .rst three nodes of the input list l; the data .elds have been elided, as well as the prev \n.eld of the .rst node. The call stack is, in a sense, elided by fully qualifying variables with a call \nstring (essentially, converting local variables into globals). For instance, the l variable of main s \nactivation record, c of the activation record for the .rst call to fix, and p of the activation record \nfor the second call to fix all point to the .rst node. Our .rst observation is that this kind of exact \nmodeling of the call stack .ts nicely in the separation logic-based abstraction shown above (depicted \nas a separating shape graph [18]) if we make the following simple extensions: (1) introduce a node for \nthe base address of each activation record, (2) view local variables as .elds of an activation record, \nand (3) link the activation records in a call stack together with a (conceptual) frame pointer .eld. \nNote that this representation does abstract low-level details, much like the diagram in Figure 1(b) (e.g., \nwe do not capture contiguousness of activation records or low-level .elds like the return address of \neach activation). This abstraction with heap summaries but an exact stack is formalized in Section 3. \nAt this point, we have an abstraction suitable for interprocedu\u00adral analysis on non-recursive programs \n(but capable of precise rea\u00adsoning with recursive data structures). However, for precise static analysis \nin the presence of recursive procedures as we propose, it is clear that we need to summarize the call \nstack to prevent our representation from growing unbounded. In particular, we need to abstract the concrete \nstates both along the downward sequence of recursive calls and the upward sequence of returns. In our \nexample, during the downward recursive call sequence, the structure of the activation records is actually \nquite regular: (1) the local variable p contains the same pointer value as c->prev in each activation \nrecord (even in the initial call where it is NULL), and (2) the next and prev .elds of the already visited \nnodes (i.e., directly pointed to by the c variables in the call stack) de.ne a valid doubly-linked list \nsegment. It is not entirely a doubly-linked list, as c->next in the most recent activation record is \nnot NULL. While the upward return sequence mostly preserves this pattern, there are wrinkles. In particular, \nin this case where a node is removed from the list, the next .eld of the previous element has been updated. \nIn other words, the next .eld of the c of the second most recent activation record has been updated (i.e., \nfix::fix::main \u00b7 c is updated when fix::fix::fix::main is still active). Therefore, a suitable call stack \nabstraction must be able to precisely capture the following properties: 1. We must be able to track the \nfragments of the structure where the prev .elds have been .xed. This property is needed so that we know \nthat we obtain the doubly-linked list in the end. We also need this property to validate the call to \nremove (e.g., the prev .eld of the node to be removed is not dangling). 2. We must be able to track \nrelations between the .elds of each activation record and heap structures. In particular, we need to \npropagate invariants during the upward return sequence.  These properties can be expressed using an \ninductive statement since the structure of the call stack is itself inductive. Successive ac\u00adtivation \nrecords must be disjoint regions of memory separate from each other and the heap. Thus, our second key \nobservation is that a shape domain built on separation logic formulas with inductive def\u00adinitions, such \nas the one described in our prior work [5, 6], seems well-suited not only for abstracting recursive heap \nstructures pre\u00adcisely but also for abstracting call stacks crisply. In other words, a novel aspect of \nour approach is that we propose to use separation logic formulas to describe not only heap data structures \nbut also the concrete call stack of activation records. Observe that the concrete states shown in Figures \n1(b) and 1(c) are lower level than descrip\u00ad tions in many language semantics and most analyses. While \na shape domain based on inductive de.nitions seems ade\u00adquate for abstracting the call stack, it is, informally \nspeaking, neces\u00adsary as well. Recall that in our example, the upward sequence of re\u00adturns mostly preserves \nthe pattern along the downward sequence of calls but not entirely. This observation indicates that the \ncall stack abstraction must be .uid in the sense that there is a need for vari\u00adation, for example, along \nthe downward call sequence versus the upward return sequence. A similar kind of .uidity is obtained in \nshape analysis for heap abstraction with materialization [26, 27]. We observe that the classical summarization \nand materialization operations in shape analysis is exactly what we need at function call and function \nreturn to obtain this .uidity: At a function call site, the call stack grows by one activation record. \nWe summarize (i.e., fold) the rest of call stack (exclud\u00ading the new active activation record). In shape \nanalysis, folding is done through either widening [6] or canonicalization [10, 27] operations. Folding \nallows us to continue the analysis with a bounded (and precise) description of the call stack. Moreover, \nthe ability to create partial summaries is critical for addressing challenge 1 above.  At a function \nreturn site, the compact description of the inac\u00adtive activation records (i.e., the call stack excluding \nthe most recent activation record) should be materialized to expose the   activation record of the \ncaller. In shape analysis, materializa\u00adtion corresponds to unfolding [6, 10] or focus [27] operations. \nMaterialization is essential for addressing challenge 2 above. Therefore, the fundamental concepts in \nshape analysis provide the essential ingredients for the precise call stack abstraction that we desire. \nIn Section 4, we describe our combined stack-heap state abstraction that makes use of an inductive predicate \nstack to pre\u00adcisely summarize recursive call stacks; the stack predicate is more complex than inductive \npredicates describing typical recursive data structures. In general, shape domains require some level \nof parametrization to describe the structures or summaries of interest. For example, TVLA [27] uses instrumentation \npredicates, while separation logic\u00ad based analyzes rely on inductive de.nitions (either supplied by the \nanalysis designer [2, 10] or the analysis user [6]). While asking the user to provide descriptions of \nuser-de.ned structures seems quite natural, asking the user to describe the call stack does not. The \ncall stack is not even a structure to which the user has direct access in any high-level language. However, \nat the same time, the inductive backbone (i.e., a stack structure) is .xed here. In Section 5, we describe \na subtraction algorithm to automatically derive program\u00adspeci.c de.nitions of the stack predicate. Because \nthe backbone is .xed, the challenge is in inferring the node type of activation records. This task is \nsimilar in spirit to Berdine et al. [2] in inferring the node type for a polymorphic doubly-linked list \npredicate. 3. Exact Call Stack Abstraction Before we describe our approach for summarizing call contexts \n(see Section 4), we .rst formalize an exact abstraction of call stacks based on separation logic formulas. \nConcrete Machine States. To begin, we describe the concrete machine states on which we abstract (see \nFigure 2(a)). A concrete state describes the status of a program at some point of its execu\u00adtion. An \nenvironment . describes the set of program variables along with their machine addresses de.ned at a point \nin the execution of a program. It includes the global variables and the variables de.ned in each activation \nrecord in the call stack. The environment is given by the grammar in Figure 2(a): an environment encloses \na stack of pairs made of a function name and a local variable to address bind\u00ading before ending with \na global variable binding. We write x for a generic variable drawn from > and use l and g to refer to \nlocal and global variable names, respectively. For any environment ., we de\u00ad.ne a few functions to simplify \nour presentation. Let callString(.) denote the call string de.ned by . (i.e., pn:: \u00b7\u00b7\u00b7 ::p1::p0). Similarly, \nlet vars(.) be the set of variables of environment .. Finally, let addrOf(.): vars(.) . V be the function \nthat gives the address of any variable in the environment. These functions can be de.ned by induction \nover the environment. A program state s is a tuple made of an environment . and a memory state s. A memory \nstate is a .nite mapping from addresses to values (where the set of addresses is included in the set \nof values). Addresses that do not correspond to the address of any variable are heap locations, whereas \naddresses that correspond to the address of a variable de.ned in an activation record are stack locations. \n 3.1 Abstraction The core of the abstraction is a spatial formula in separation logic describing the \nshape of memory. Spatial formulas can be seen equivalently as graphs [18]: (1) nodes, which are given \nsymbolic names (e.g., a), abstract sets of values and (2) edges describe mem\u00adory cells subject to certain \nconstraints, such as cell of address a contains value \u00df (i.e., a . \u00df). A graph G is the separating con\u00adjunction \n* [21] of the memory regions represented by each of its s ::= (., s) program states (. S) . . [ environments \n. ::= X global variables | (p, X) :: . new activation record s . 1 = V -.n V memories X ::= \u00b7| X, x variable \nto address bindings f a x, l, g . > program variables p . I procedure names a . V values including addresses \n(a) The concrete state. G ::= a . \u00df points-to edge | a \u00b7 f . \u00df points-to edge of a .eld | a \u00b7 c(...) \ninductive edge | a \u00b7 c(...) *= a' \u00b7 c'(...) segment edge | emp | G1 * G2 graphs [L N . num numerical \nconstraints in a base domain A ::= (G, N) analysis state (i.e., abstract program state) a, \u00df, . . . , \na, g, . . . \u00af. VL symbolic names (i.e., nodes) f, g,..., fp, l, . . . . I .eld names (i.e., offsets) \n(b) The abstract state. Figure 2. De.ning the concrete machine and abstract analysis states. edges as \nshown in Figure 2(b); the empty graph is written emp.A points-to edge a . \u00df describes a memory cell with \nabstract ad\u00address a and contains the value abstracted by \u00df. If we qualify the left-hand side of points-to \nwith a .eld as in a \u00b7 f . \u00df, we rep\u00adresent a memory cell whose address is a plus the offset of .eld f \nand whose content is \u00df. We assume offsets are symbolic .elds and thus use a relatively high-level Java-like \nmodel; a lower-level mem\u00adory model could be mixed in without much dif.culty by following our prior work \n[18]. An inductive edge a \u00b7 c(...) is an instance of inductive predicate c (with a distinguished traversal \nparameter a), ' while a segment edge a \u00b7 c(...) *= a' \u00b7 c(...) is a partial deriva\u00adtion of an inductive \npredicate c. These edges summarize some set of memory cells as described by an inductive predicate allowing \nus to represent a potentially unbounded memory; Section 3.2 dis\u00ad cusses these notions in greater depth. \nConcrete program states are then abstracted by an analysis state A consisting of a graph G and a numerical \nconstraint N. A numerical constraint describes relations amongst symbolic names a and is drawn from some \nbase domain [L, that is, the abstract domain described is parametrized by a num standard sort of numerical \ndomain. If we instantiate this abstraction with inductive de.nitions describing recursive heap data structures \nsupplied by the user, we essentially obtain the shape domains in our prior work [5, 6]. To abstract the \ncall stack in a concrete state s = (., s), we observe that the environment . plays a signi.cant role \nhere. At the abstract level, we build it directly into the graph as follows: The address of each global \nis represented by a node. Thus, the set of global variable names are included in the set of symbolic \nL names V. We introduce a node to represent the base address of each activation record. For the sake \nof clarity, we distinguish nodes representing activation record addresses by using a bar over the symbolic \nnames as in a\u00afand by drawing them with a bold, dotted border in diagrams. We also annotate them with \nthe function to which they correspond (i.e., indicating the type of the activation record).  Each local \nvariable can be viewed as a .eld of its activation record. The set of .eld names I therefore includes \nlocal vari\u00adable names. This representation is key in allowing locals to be summarized as part of the \ncall stack (see Section 4).  Lastly, we make explicit a frame pointer fp, which is simply a .eld of \nall activation records. Like in the physical memory at run time, the frame pointer fp points to the previous \nactivation record in the call stack. To distinguish them clearly in the diagrams, frame pointer edges \nare drawn as dotted lines (and usually the fp label is omitted).  Thus, with the above observations, \nwe encode the structure of the call stack in our shape domain in a faithful manner essentially as-is \nand without any signi.cant modi.cations. As an example, an abstraction of the concrete state from Fig\u00adure \n1(b) is shown inset. The a\u00afnodes represent the base address of the activation records, and their outgoing \npoints-to edges correspond to the call stack. Ob\u00adserve that the horizontal edges between the a\u00afnodes \nand the \u00df nodes are the stack cells for local variables. Meanwhile, the fp links connect the a\u00afnodes \nto capture the actual stack struc\u00adture. The heap is represented by the edges in the rightmost col\u00adumn: \nthe vertical edges are the next and prev .elds for the .rst three nodes and \u00df3 \u00b7 sll() summa\u00adrizes an \narbitrary singly-linked list. Note that this portion of the graph is the only part of the mem\u00adory state \nthat is captured by traditional shape analyses. We have elided a few edges from this .gure, speci.cally \nthe ret variable edges in the stack and the data .elds in the heap. Concretization. Like for concrete \nstates, we de.ne functions that compute the call string callString(G) and the set of variables it de.nes \nvars(G) given a graph G. These functions follow the chain of frame pointers to compute the desired result. \nWe also de.ne a function addrOfL(G): vars(G) . VL \u00d7 I that maps each variable to the points-to edge representing \nits cell (i.e., an abstract address-of mapping). To de.ne the concretization of [L graph domain graph \na graph, we need [L num numerical domain a mapping be\u00adtween symbolic [L = [L \u00d7 [L shape domain num graph \nnames and con\u00adcrete values. Such mappings . : VL . V are called valuations [5] and allow us to abstract \nirrelevant details like concrete physical addresses. We .rst summarize the types of the concretizations \n(the names of the various domains are given in the inset): L .graph : [graph .P(1 \u00d7 (VL . V)) .: [L .P(VL \n. V) num num [L . : .P([ \u00d7 1) . The concretization of a graph G yields a set of pairs consisting of a \nconcrete memory s and a valuation ., while concretizing a numerical domain element N should give a set \nof valuations. Together, the concretization of the combined domain produces a set of pairs of a concrete \nenvironment . and a concrete memory s. To concretize a graph, we take the concretization of each edge: \ndef (emp)= { ([\u00b7],.) | . . (VL . V) } .graph def .graph(G1 * G2)= { (s1 . s2,.) | (s1,.) . .graph(G1) \nand (s2,.) . .graph(G2) } . We write . for the separating conjunction of concrete memories (i.e., the \nunion of two memory maps with disjoint domains) and [\u00b7] for an empty concrete memory. The frame pointer \nfp .elds are model .elds, so they have no concrete correspondence, but otherwise, the concretization \nof a points-to edge is a single memory cell, written [a1 . a2]: def (\u00afa1 \u00b7 fp . a\u00af2)= { ([\u00b7],.) | . . \n(VL . V) } .graph def (a \u00b7 f . \u00df)= { ([.(a, f) . .(\u00df)],.) |. . (VL . V) } .graph def (a . \u00df)= { ([.(a) \n. .(\u00df)],.) | . . (VL . V) } .graph where .(a, f) gives the base address a plus the offset of .eld f. \nWe postpone de.ning the concretization of summary edges (i.e., inductive and segment edges) to Section \n3.2. Overall, the concretization of an analysis state A are the envi\u00adronment-memory pairs given by the \ngraph and consistent with the numerical constraint: (., s). .(G, N ) iff for some ., callString(.)= callString(G) \nand vars(.)= vars(G) and (s, .) . .graph(G) and . . .num(N) and addrOf(.)(x)= .(addrOfL(G)(x)) for all \nx . vars(.) . Valuations . connect the various components, capturing relations across disjoint memory \nregions and with the numerical constraint.  3.2 Inductive Summarization and Materialization As alluded \nto in Section 3.1, we summarize a potentially un\u00ad bounded memory using edges built on inductive de.nitions. \nAt a high-level, an inductive de.nition consists of a set of unfolding rules or cases that specify how \na memory region can be recognized through a recursive traversal. As stated earlier, our inductive edges \ncome in two forms: (1) an inductive edge a \u00b7 c(...) describes a memory region that satis.es inductive \nde.nition c from a, and (2) a segment edge a \u00b7 c(...) *= a ' \u00b7 c ' (...) describes an incomplete structure, \nin particular, a memory region that can be derived by unfolding a \u00b7 c(...) a certain number of times \nup to a (missing) a '' \u00b7 c (...) sub-region [5]. Relations among pointers or numeri\u00ad cal values between \nsuccessive unfoldings of an inductive edge are captured by de.nitions with additional parameters. For \ninstance, the relation between prev and next pointers in a doubly-linked list can be captured by the \nfollowing inductive de.nition (written as a separation logic formula); ` \u00b4` def l \u00b7 dll(p)= emp . l = \nnull ..n, d. (l \u00b7 prev . p * l \u00b7 next . n * l \u00b7 data . d \u00b4 * n \u00b7 dll(l)) . l= null Unfolding. An inductive \nde.nition gives rise to a natural syn\u00adtactic unfolding operation. Unfolding substitutes an inductive \nedge a \u00b7 c(...) or a segment edge a \u00b7 c(...) *= a ' \u00b7 c ' (...) with one in\u00adductive case of c s de.nition \n(and where all existentially-quanti.ed variables are replaced with fresh nodes). For inductive edges, \nbase cases correspond to a rule with no new inductive edge upon un\u00adfolding; for segment edges, the base \ncase is unfolding to the empty segment (i.e., when a = a ' and c = c '). We write G .unfold G ' for an \nunfolding step from graph G to G ', as well as .. unfold for the re.exive-transitive closure of .unfold. \nBecause the unfolding operation is so closely tied to the inductive de.nition, we often present an inductive \nde.nition by the unfoldings that it induces.  Figure 3. Unfolding operation induced by the dll de.nition. \nFor instance, in Figure 3, we present the doubly-linked list de.ni\u00ad tion dll in this style. The concretization \nof graphs containing inductive or segment edges is based on the concretization without them. In particular, \nthe concretization of a graph G containing inductives is simply the join of the concretizations of all \nthe graphs that can be derived from it by successive unfolding: [ ' L ' .graph(G)= {G . [graph | G unfold \nG } . Unfolding and Analyzing Updates. Unfolding is the key opera\u00adtion for abstractly interpreting writes. \nTo re.ect an update e1 := e2, we traverse the graph to .nd the points-to edges (i.e., the memory cells) \nthat correspond to the addressing expressions e1 and e2. If the points-to edges of interest already exist \nin the graph, then re\u00ad.ecting the update is simply a matter of swinging an edge. Because a graph is a \nseparating conjunction of edges, this modi.cation is a strong, destructive update. However, if the desired \nedges are not present, then we try to materialize them with the unfolding oper\u00adation unfold. Unfolding \nto expose cells in a user-de.ned heap structure is necessarily heuristic, but the distinguished traversal \npa\u00adrameter in our inductive predicates (i.e., a in a \u00b7 c(...)) provide guidance (also see our prior work \n[5] for ways to make unfolding more robust). More crisply, we describe the abstract interpretation of \nan update with the following inference rule: (A, statement).A ' (A,e1) . (A ' ,a1 \u00b7 f1 . \u00df1) (A ' ,e2) \n. (A '' ,a2 \u00b7 f2 . \u00df2) [G -G(A '' (A,e1 := e2).A '' )[a1 \u00b7 f1 . \u00df2]] The abstract interpretation judgment \n(A, statement).A ' says that in abstract state A, evaluating statement statement produces a resulting \nstate A ' (as in a standard structured concrete operational semantics). This judgment is de.ned in terms \nof an auxiliary judg\u00adment (A,e) . (A ' ,a \u00b7 f . \u00df) that evaluates an addressing ex\u00adpression e to a points-to \nedge in A, which may yield a modi.ed state A ' as the result of unfolding. To re.ect the update, we write \nG(A) for looking up the graph component of A, A[G -G ' ] for replacing the graph component of A with \nG ', and G[a \u00b7 f . \u00df] for updating the edge with source a \u00b7 f in G to point to \u00df.  3.3 Towards Analyzing \nCalls and Returns Our whole-program analysis is based on an abstract interpreta\u00adtion [7] of the program \ns interprocedural control-.ow graph. We desire a sound analysis, which means at each step, the analysis \nap\u00adplies locally-sound transfer functions. They cannot omit any pos\u00adsible concrete behavior. With just \nthe abstraction described in this section, we can de.ne an analysis for non-recursive programs (al\u00adbeit \na potentially computationally expensive one). At a function call site in the concrete execution, a new \nactiva\u00adtion record is pushed onto the call stack. Correspondingly at the ab\u00adstract level, we push a new \nab\u00adstract activation record: (1) we create a new node representing the base address of the new activation \nrecord; and (2) we set the content of its .elds (i.e., the formal parameters and local variables) by \nas\u00adsigning formal parameters to the actual arguments and by point\u00ading local variables to fresh nodes. \nFor example, consider again the code from Figure 1(a). At the call to fix from main (i.e., fix(l, NULL)) \nduring the analysis, we push a new abstract ac\u00adtivation record with base address a\u00af0 with .elds for parameters \nc and p and local variable ret as shown in the inset. The a\u00af0 \u00b7 fp .eld is set to point to a\u00af, the base \naddress of the activation record for main, while a\u00af0 \u00b7 c is made to point to \u00df0 (as a\u00af\u00b7 l . \u00df0) and a\u00af0 \n\u00b7 p gets null. For the a\u00af0 \u00b7 ret .eld, it is set to fresh node .0, which indicates it contains an arbitrary \nvalue. The \u00df0 \u00b7 sll() induc\u00adtive edge states that \u00df0 is the head of a singly-linked list (of dll nodes), \nwhich existed before the call. Now, if we continue analyzing the body of fix, we see that \u00df0 \u00b7 sll() \nwould be unfolded along the path where \u00df0 = null (i.e., c != NULL) before arriving at a recursive call \nto fix. It is clear that if we continue analyzing in the manner described above, we will keep on creating \nnew activation records and never terminate. Thus, in order to ensure the termination of the analysis \nin the presence of recursive functions, we must apply a widening that is capable of summarizing the call \nstack. As alluded to earlier, the key observation is that the call stack is itself an inductive structure. \nSpeci.cally, it is a list of activation records where the frame pointer fp .elds form the backbone. Based \non this observation, we use a special inductive predicate stack to summarize recursive segments of the \ncall stack (e.g., the sequence of fix calls fix ::main). This inductive predicate stack is nec\u00adessarily \nmore complex than usual predicates for summarizing re\u00adcursive heap structures and is described in detail \nin Section 4. Fur\u00ad thermore, the stack predicate must be program-speci.c because it depends on the program \ns interprocedural control-.ow. Thus, its de.nition cannot be known before beginning the analysis. In \nSec\u00adtion 5, we detail an algorithm for deriving a de.nition of stack on the .y during the analysis. Finally, \nat a function return site, the analysis proceeds like in a concrete execution by popping off the most \nrecent activation. For instance, in the example above, on return from fix to main, we drop the node a\u00af0 \nand its outgoing edges for fp, c, p, and ret, just like the disposal of heap cells. As an invariant of \nthe analysis, we make sure the topmost activation record is always exposed (i.e., never summarized in \na stack predicate). This invariant ensures that all program variables in scope (globals and locals) are \ndirectly accessible. As part of the transfer function for return, we need to make sure that the activation \nrecord of the caller function becomes exposed after the return, as now it is the topmost one. In the \npresence of call stack summarization that includes the caller, we need to unfold the stack predicate \nto expose the caller s activation record using the unfold operation with stack. As such, stack segments \nalways go from callees to callers, that is, in the direction of the fp links. 4. Summarizing Call Stacks \nInductively As alluded to earlier, we summarize call stacks from recursive pro\u00adgrams using an inductive \npredicate stack, exploiting their inherent inductive structure. The de.nition of stack is particularly \ninterest\u00ading because it depends on the interprocedural control .ow of the program being analyzed. To \nbuild intuition for a de.nition of stack, we .rst explore a number of examples that illustrate requirements \nfor it. We consider analyzing simple recursive functions requiring no relations between successive activations, \nrecursive procedures requiring simultaneous summarization of the stack and heap, nested recursion, and \nmutual recursion. Our algorithm for automatically deriving a stack de.nition on the .y during analysis \nis then de\u00adscribed in Section 5.  Recursion without Heap Relations between Activations. Con\u00adsider the \nsimple program shown in inset (a) that constructs a list of random length using a recursive function \nf. For each call to func\u00adtion f, a new, uninitialized list node is allocated on the heap be\u00adfore the \nnext recursive call. For simplicity in presentation, a list node has just one .eld: next for linking. \nNote that we elide the size-of argument to malloc, treating it as a high-level alloca\u00adtor with types. \nThe next point\u00aders are assigned during the se\u00adquence of returns. Inset (b) shows a graph (with no summarization) \nthat describes the state of the pro\u00adgram at the entry of the .rst re\u00adcursive call to f (i.e., with the \ncall string f::f::main), while inset (c) shows the state at the second re\u00adcursive call (i.e., f::f::f::main). \nThe repeated pattern is clear from these diagrams: all pending acti\u00advations of f have a local variable \ny that point to a list node whose next .eld is arbitrary. This appar\u00adent pattern suggests the unfolding \nrule or case for the de.nition of stack shown in inset (d). Inductive edges of stack are labeled with \na regular expression to convey the set of call strings to which the rule can be ap\u00adplied. Here, the regular \nexpres\u00adsion f::ctx on this rule indicates that it can be applied only to a call stack where the topmost \nac\u00adtivation record corresponds to an f record, while the calling con\u00adtext ctx may be arbitrary. These \nconstraints on the calling context can be expressed as an additional parameter to stack, so the labels \nare simply a shorthand. In other words, the .rst additional param\u00adeter to stack is a set of possible \ncall strings expressed as a regu\u00adlar expression. In the case of rule de.nition, the label is a check \nfor that regular expression pattern (e.g., f::ctx on the left-hand side of the example unfold). For \nan instance of the stack predicate, it gives an abstraction of the call string. As an example, using \nthis rule, we can over-approximate all the possible states at the entry to function f after any number \nof recursive calls with the graph shown in inset (e). The f label indicates that there are zero or more \nf acti\u00advations between a\u00af0 and a\u00af. Intuitively, these labels approximate the sequence of frame pointer \nlinks and the types of activation records along that sequence. Recursion with Mixed Call Stack and Heap \nSummaries. In Fig\u00adure 4, we present a slightly more involved example where an ex\u00ad isting heap data structure \nis traversed recursively. Function f a re\u00adcursive, non-destructive walk of a list (i.e., a singly-linked \nlist con\u00adsisting of list nodes). Before the .rst call to f, the memory state is abstracted by the graph \nshown in Figure 4(a). After the .rst call void main() { list* l; ... /* make l \u00b7 list() */ ...; f(l); \n} void f(list* x) { if (x == NULL) return; else f(x->next); } (a) Before the .rst call to f. (b) After \nthe .rst call to f. (c) After one recursive call. (d) After two recursive calls.   unfold (e) An \nunfolding rule. (f) Summary for the entry point to f. (g) Non-empty unfolding of the stack segment \nin (f). Figure 4. Summarizing the memory states for recursive list traver\u00adsal. to f but before any recursive \ncall, we have the graph shown in (b). In (c), we show the graph after the .rst recursive call at the \nentry point to f, while (d) shows the graph at the same point after two recursive calls. Just like in \nthe previous example, there is a clear repeating pattern consisting of both stack and heap edges for \neach activation. However, unlike the previous example, there is an im\u00adportant relation between successive \nactivation records through the heap: x->next of an activation record aliases x of the subsequent activation \n(e.g., a\u00af0 \u00b7 x . \u00df0 * \u00df0 \u00b7 next . \u00df1 * a\u00af1 \u00b7 x . \u00df1). Such a relation can be captured with an additional \nparameter to the stack predicate, and thus we get the unfolding rule shown in Figure 4(e). The key difference \nbetween the unfolding rule here and the rule from the previous example is the parameter \u00df2 (shown highlighted \nin the .gure) that says the next .eld from the value of a\u00af1 \u00b7 x (i.e., \u00df1) points to an existing node \ngiven by parameter \u00df2. Using this inductive de.nition, we can summarize the possible states at the entry \nto function f using a stack segment edge (shown in Figure 4(f).  In the graph shown in Figure 4(f), \nthe stack segment not only summarizes a portion of the call stack but also a fragment of the heap, speci.cally \nthe list segment between \u00df0 and \u00df2. It also maintains the relation between the x pointers to elements \nin this list the inductive de.nition of stack is quite powerful here. To get a better sense of this aspect, \nconsider unfolding the stack segment in the summary shown in Figure 4(f). There are two cases: The segment \nis empty. This base case says there are no cells summarized by the segment and the ends are equal, that \nis, a\u00af1 = a\u00afand \u00df2 = \u00df0. This state is exactly the one at the entry point after the .rst call to f (i.e., \nwith call string f::main) shown in Figure 4(b).  The segment is non-empty. One step of unfolding yields \nthe graph shown in Figure 4(g). Notice that one step of unfolding exposes the previous activation record \nwith the desired relation between the previous activation s x and current activation s x. Overall, this \ninductive case summarizes the state after succes\u00adsive recursive calls (e.g., states shown in Figures \n4(c) and (d)). For instance, replacing the stack segment in (g) with the empty segment (i.e., unfold \nit to empty), we get the state after one re\u00adcursive call (c) where a\u00af0 = a\u00afand \u00df1 = \u00df0.  In both examples \nthus far in this section, we never de.ned a base case for the stack inductive de.nition. At the same \ntime, it is not particularly meaningful to provide one, as the .rst function called in the program is \nmain, which must be the .rst/oldest activation record. For our analysis, this absence of a base case \nfor stack is actually never a problem, as the stack predicate is always used as a segment edge. For any \nsegment, the base case is the empty segment. Nested Recursion. The program below illustrates a nested \nrecur\u00adsion: main calls function f, which calls itself recursively a certain number of times, until it \ncalls g, which then also calls itself recur\u00adsively a certain number of times. There is no call to f from \ng. void main() { t* x; f(x); } void f(t* x) { if (...) f(x); else g(x); } void g(t* x) { if (...) g(x); \n} After a number of recursive calls, the layout of the call stack at the entry point to function g can \nbe summarized by the above graph where the stack segments correspond to the sequence of recursive calls \nin g and to the sequence of recursive calls in f, respectively. Mutual Recursion. In the program below, \nfunctions f and g are mutually recursive, so the call strings at the entry point to g are of the form \ng::f::(g::f) ::main. void main() { t* x; f(x); } void f(t* x) { if (...) g(x); } void g(t* x) { if (...) \nf(x); } As the cycles are of the form g::f:: \u00b7\u00b7\u00b7, the call stack of the above program at the entry point \nto g is summarized using the following rule: unfold The cycles can be more complex. For example, for \ncall stacks where the call strings are of the form g::(f ::g) ::main, the in\u00adductive stack rule for the \n(f ::g) cycle would unfold into a call stack fragment, which would contain a summary for the inner f \ncycle (i.e., another stack segment over the f call string). De.ning stack. We can now state precisely \nthe notion of an inductive de.nition suitable for abstracting the call stack: A stack segment is a segment \nedge. This edge is labeled with with a regular expression denoting a superset of the call strings that \nit describes.  A stack inductive de.nition is an inductive de.nition stack such that each case unfolds \na sequence of one or more activation records according to a call string.  As stack segments are simply \nsegment edges of the stack de.\u00adnition, the concretization of graphs with stack segments follows from \nthe de.nitions in Section 3. Notably, given the ability to parametrize inductive de.nitions by simple \nregular expressions, the meaning of stack summaries falls directly from the notion of induc\u00adtive segments. \n5. Inferring Call Stack Summarization Rules In Section 4, we illustrated how the call stack corresponding \nto various forms of recursion is summarized by an inductive stack predicate. However, we also need to \nbe able to derive a suitable de.nition for stack. To obtain a terminating analysis, we require a widening \noperator capable of summarizing the call stack. To do so, it must fold fragments into stack segments. \nYet, before beginning the analysis, the de.nition of stack cannot be known, as the interprocedural control \n.ow of the program is still to be explored. This circularity means that the de.nition of stack must be \nderived on the .y during the analysis when recursive calls are found. In this section, we describe such \nan algorithm for de.ning a program-speci.c stack predicate on the .y. Widening in Recursive Cycles. In \nthis section, we consider a re\u00adcursive function f, which directly calls itself. The technique we propose \nalso applies to more complex cycles (e.g., mutual recur\u00adsion). In general, when a function call is recursive, \nthe interproce\u00addural control-.ow graph contains two cycles: one at the function entry (from the recursive \ncall site) and one at the function exit (to the recursive return site). Thus, to ensure termination of \nthe anal\u00adysis in the presence of recursion, widening is applied at the entry and exit points of a recursive \nfunction. We .rst describe, at a high-level, the steps that the analysis takes to compute an invariant \nat such a recursive widening point. The key operations are the widening on program states given some \nstack de.nition (see Section 6.1) and a shape subtraction to generate inductive rules of the stack predicate \ndescribed later in this section. Intuitively, deriving rules for stack comes from .nding the dif\u00adference \nbetween successive abstract program states at, for example, f s entry point after some number of recursive \ncalls. Speci.cally, the analysis takes the following steps at f s entry point: 1. Compute a few abstract \nstates by iterating over the recursive call cycle. In practice, unrolling a few iterations of a cycle \nis  Figure 5. Inferring a stack rule from abstract states computed at the entry point to fix while \nanalyzing the example in Figure 1(a). often bene.cial to get to stable behavior. Suppose we obtain three \nstates A0, A1, and A2 from the .rst iterations, and we wish to extrapolate from A1 and A2. 2. We derive \nan inductive rule for stack from A1 and A2 using the shape subtraction algorithm. 3. We weaken A2 into \na weaker A ' 2 using the rule derived in step 2. To perform this weakening step, we apply the widening \noperator over program states to A1 and A2 to produce A ' 2. The net result is that A ' 2 summarizes both \nA2 and A1 with a stack segment for the difference between them. 4. We perform widening iteration over \nthe cycle until convergence to an invariant A8. The de.nition of the widening operator over program states \nguarantees convergence.  Note that the algorithm for inferring new inductive rules for stack in step \n2 assists widening in obtaining quality invariants. It does not need to be sound or complete. In fact, \nit is possible to craft complicated examples where discovering a repeating pattern would be arbitrarily \ndif.cult. Instead, the goal should be that it is effective at discovering adequate rules in realistic \nsituations. Creating stack Rules by Shape Subtraction. At a high-level, we .nd the difference in the \ngraphs between two successive state iter\u00adances A1 =(G1,N1) and A2 =(G2,N2) using shape subtrac\u00adtion. \nThis difference gives the exposed or unfolded fragment in a new stack rule. In the following, suppose \nA1 corresponds to call string f::f::ctx and A2 to f::f::f::ctx. The .rst step is to derive the graph \npart of the new rule. To do so, we want to isolate the edges that appear in G2 but not in G1. In other \nwords, we want to partition G2 into two disjoints sets of edges Gcommon and G. such that informally speaking, \nG2 = Gcommon * G. and G1 = Gcommon . The above is informal because we must take care of matching sym\u00adbolic \nnode names (which correspond logically to existential vari\u00adables). We illustrate the description of the \nalgorithm by following the example from Section 2 (i.e., Figure 1). Figure 5(a) shows the abstract state \nat the entry point to fix obtained from one itera\u00adtion over the recursive call cycle (i.e., after executing \none recursive call); Figure 5(b) shows the abstract state after one more recursive call (i.e., after \ntwo recursive calls). The graph shown in Figure 5(c) shows the G. computed from the states in (a) and \n(b). In essence, shape subtraction works by performing a simultane\u00adous traversal over G1 and G2 to identify \nmatching structure (i.e., Gcommon). A node naming relation . . VL \u00d7 VL serves to track the correspondence \nbetween the nodes in G1 and those in G2, as well as to de.ne the frontier of the traversal. To start \nthe subtraction process, the node naming relation . is initialized with root nodes of memory regions \nthat we want to be in Gcommon. In particular, we pair the following for the initial .: (1) nodes representing \nad\u00addresses of global variables, (2) base addresses of activation records in the context ctx (e.g., (\u00afa, \na\u00af) . . and (\u00afa0,a\u00af0) . . for initializ\u00ading the example subtraction in Figure 5), (3) the base address \nof the topmost activation record (e.g., (\u00afa1,a\u00af2) . .). This initialization states that Gcommon is any \nportion of memory reachable from the globals, activation records of the context, and the topmost activa\u00adtion. \nWhat remains, G., is the state difference between G1 and G2 that we wish to summarize with a stack segment. \nIn the example, the only activation record node that does not appear in . is the one for the second activation \nin G2 (i.e., a\u00af1 in Figure 5(b)). Observe that this node is exactly the base address of the activation \nthat we wish to summarize. At this point, the algorithm is rather straightforward. We collect together \nedges of the same kind whose the source nodes are in the node naming relation .. Whenever two edges are \nmatched, the target nodes (and any additional checker parameters) are added to .. The matched edges are \ndiscarded from G1 and G2 and added to Gcommon (up to node renaming). For example, in Figure 5, the edges \ncorresponding to .eld l of a\u00afcan be matched and consumed right after initialization. Then, the pair (\u00df0,\u00df0) \nis added to ., and the prev edges from \u00df0 in both graphs can be consumed next. We iterate this match \nand consume traversal until G1 is empty in which case G2 has become G..  It is possible that G1 fails \nto become empty, that is, we are unable to .nd a common fragment. This subtraction algorithm is much \nlike the graph join algorithm [6] in that the result depends on the traversal order. Matching and consuming \ncertain edge pairs too early may cause the algorithm to fail to produce an empty G1 when another traversal \norder would have succeeded. Fortunately, in our experience, a simple breadth-.rst-style strategy suf.ces. \nFirst, we match .elds from the topmost activation record, which are not pointed to by other activation \nrecords directly (e.g., from (\u00afa1,a\u00af2)). Then, we do the same for the .elds from the context (e.g., from \n(\u00afa, a\u00af) and (\u00afa0,a\u00af0)). Nodes directly pointed to by the activation being summarized are considered \nlast. The graph result of shape subtraction G. gives the unfolded edges for a stack unfolding rule, that \nis, the portion of memory that could be summarized by a stack segment (of length 1). As\u00adis, such a rule \ndoes not express all the properties that are needed to describe the call stack precisely. In the Figure \n5 example, we need to express aliasing relations between successive activations (cf., the difference \nbetween the list allocation and the list traversal examples in Section 4). These relations are captured \nby parameters on the stack de.nition. The parameters are given by the nodes at the boundary between the \ntopmost activation and the activations being summarized. In this case, nodes \u00df1 and \u00df2 are this boundary \nand become parameters in the de.nition of the stack rule as shown in Figure 5(d). Finally, any relevant \nnumerical constraints in the base domain is also captured in de.ning a stack rule. Once G. has been computed, \nwe simply take the projection of the numerical invariant N2 onto the set of symbolic node names in G.. \nShape subtraction on the graph portion can be seen as a restric\u00adtion on frame inference [1]. Here, we \nare looking for an exact match as opposed to an entailment between two con.gurations. In spirit, the \nabove algorithm potentially could be applied to derive other kinds of inductive de.nitions besides stack \n(cf., [14]). However, we make critical use of understanding the inductive structure of a call stack to \nget good results. For example, this background knowledge is used in initializing the node naming relation \n.. We hypothesize that having some knowledge on the kind of inductive backbone of interest is key to \ngetting high-quality de.nitions. 6. Applying Call Stack Summaries in Analysis With the mechanism for \nderiving stack rules during analysis, we have all the pieces for analyzing recursive procedures with \ncall stack summarization by following the outline in Section 3.3. In particular, sound transfer functions \nfrom intraprocedural inductive shape analysis [5, 6, 18] for basic program statements, like as\u00ad signment \n(cf., Section 3.2), guard conditions for branching, loops, and memory allocation-deallocation, carry \nover in a straightfor\u00adward manner. A slight difference is that instead of a .xed set of variables as \nin the intraprocedural case, we have both global and local variables. Local variables are .elds of activation \nrecords in our graph, but all program variables in scope are easily accessible, as we ensure that the \ntopmost activation is never summarized. There are two remaining pieces to our interprocedural analysis. \nFirst, we want to see how widening with derived stack rules ap\u00adplies at the function entry and exit points \nin recursive call cycles (Section 6.1). Second, the soundness and termination of extensi\u00ad ble inductive \nshape analysis [5, 6] relies on the assumption that all inductive de.nitions are .xed before the analysis \nstarts. In this pa\u00adper where the de.nition of stack is extended on the .y, we need to justify soundness \nand termination in the presence of such on\u00adthe-.y inductive rule generation (Section 6.2). We conclude \nthis section with a summary of the reasons for termination and sound\u00adness for the overall analysis, as \nwell as some empirical experience (Section 6.3).  (b) Next iteration, which con.rms the inferred invariant \n(A8+1). Figure 6. Widening at the entry point to fix in Figure 1(a). 6.1 Widening with Call Stack Summaries \nAfter an appropriate inductive rule for stack has been derived (see Section 5), widening at function \nentry and exit points in a recursive call cycle is not particularly different than at loop heads. That \nis, the join U and widen V on graphs and program states [6] essentially carries over. We do not rede.ne \nthese algorithms, but we discuss their main features here by following our example introduced in Section \n2. The join on separating shape graphs is stabilizing, so the only difference between join U and widen \nV is the operator applied to elements of the numerical base domain. At a high-level, the join on graphs \nis actually quite similar to the subtraction algorithm de\u00adscribed in Section 5. They both work by a simultaneous \nmatch and consume traversal over two graphs from root nodes using a node naming relation (i.e., .). Roughly \nspeaking, the main difference is that join applies weakening to memory regions delineated by the traversal. \nFor example, it folds fragments consisting of points-to edges (e.g., a \u00b7 next . \u00df) into an instance of \nan inductive de.\u00adnition (e.g., a \u00b7 list()). Folding is in essence applying an unfolding rule in reverse \n[6]. Following the outline in Section 5, at the entry point to fix, we .rst obtain abstract states A1 \nand A2 from Figures 5(a) and (b), respectively. Subtraction is applied to them to get the stack rule \nin Figure 5(d). With this rule, widening on abstract states is applied to A1 and A2 to produce the state \nA8 shown in Figure 6(a), which summarizes both A1 and A2. Beginning at the entry point to fix with A8, \nwe analyze until the next recursive call and returning to the entry point, we get the abstract state \nA8+1 shown Figure 7. An invariant just before returning from a recursive call to fix.  in Figure 6(b). \nObserve that a\u00af3 is the activation record that can be folded into the stack segment (using the Figure \n5(d) rule). Thus, computing A8VA8+1 yields A8, which con.rms that A8 is an invariant that summarizes \nthe set of all concrete states that can be observed at the entry to function fix (after one or more recursive \ncalls). The control point after the function exit before the return site of a recursive call is also \non a cycle in the interprocedural control .ow graph, so we perform widening iterations there. We perform \nwiden\u00ading at the point just before the topmost abstract activation record is discarded. In Figure 7, \nwe show an abstract element summarizing concrete states that can be observed just before returning from \na re\u00adcursive call to fix (and where this call went along the path where c was not null and node c was \nnot removed). Note that during the sequence of returns from recursive calls, the new dll edge appears, \nas upon function return, the tail of the structure has prev pointers set correctly so as to de.ne a dll. \n 6.2 Introducing Inductive Rules on the Fly As noted above, a source of complexity in summarizing the \ncall stack with the stack predicate is that new inductive rules are gen\u00aderated on the .y. To reason about \nthis aspect, we extend our frame\u00adwork with rule set extension. When the analysis starts, the set R is \nempty. Whenever a new recursive call site is discovered, a new rule is added to R. Therefore, we consider \nR an element of a separate lattice, speci.cally the powerset of the set of rules. Each invariant is with \nrespect to a set of rules R, which determines the instance of the graph domain in use. Thus, our analysis \nstate is actually an (R, G, N) tuple in the abstract domain [L de.ned below. For clar\u00adity, we annotate \n[L with the set of rules R allowed for the graph inductive de.nition of stack as follows: [L (and similarly \ngraph(R)with .graph(R)). We also de.ne .(R)(G, N) as we did .(G, N), except we now use .graph(R) in place \nof .graph (and similarly for the order .(R) on states (G, N)). def [L = { (R, G, N) | G . [L num } graph(R) \nand N . [L def .(R, G, N)= {s | s . .(R)(G, N)} (R0,G0,N0) . (R1,G1,N1) iff R0 .R1 and (G0,N0) .(R1) \n(G1,N1) The above ordering . is sound, as R0 .R1 implies that .graph(R0)(G0) . .graph(R1)(G1). All transfer \nfunctions are as before, except for widening at the head of recursive functions, which also adds a new \nrule. Most importantly, as shown in Venet [29] that has similar a construction, this widening stabilizes \nif the Table 1. Micro-benchmarks comparing analysis times for recur\u00adsive and iterative versions of the \nsame operation. Recursive Iterative Benchmark (ms) (ms) list traversal 11 4 list get nth element 22 4 \nlist insertion nth element 48 16 list remove nth element 27 11 list deletion (memory free) 13 4 list \nappend 20 13 list reverse 29 5 process of adding rules is itself bounded. One possible bound is to allow \nat most one rule per call site. The above construction also suggests applying more complex forms of widening \nto the set of rules R, while preserving sound\u00adness. In the ordering (R0,G0,N0) . (R1,G1,N1) de.ned above, \nwe stated that it must the case that R0 .R1. However, we could use a more sophisticated ordering on sets \nof rules. In par\u00adticular, if we have two rules r0 and r1 where r1 is weaker than r0, then we could replace \nr0 with r1 in our set of rules while main\u00adtaining soundness. In terms of the analysis, this observation \nmeans that inductive rules for stack may be weakened during the course of the analysis. Surprisingly, \nwe can use this process to improve what can be summarized. Suppose the analysis discovers a stack rule \nr to summarize the call stack at the entry of some function f. However, widening at the next iteration \nfails (i.e., is imprecise) due to rule r being too speci.c, we are allowed to weaken r to a coarser rule \nr ' that may allow this widening step to succeed. This technique is potentially useful when the shape \npart of the rules is stable, but when the numeric contents of cells need to be computed by a non-trivial \nwidening sequence, as can be seen in Section 7. To guarantee termination of the analysis with this process, \nthe rule weakening step must be shown to stop in some way.  6.3 Termination, Soundness, and Empirical \nExperience To ensure termination, the analysis algorithm applies widening to at least one point in each \ncycle in the set of abstract .ow equations. In the case of whole-program interprocedural analyses, the \nfollowing is one set of such widening points: (1) loop heads for intraprocedural loops and (2) at the \nentry and at the exit of functions when analyzing a recursive call. At the end of the analysis, each \nprogram point is mapped to a .nite set of abstract elements. As all transfer functions are sound, and \nthere is at least one widening point on each cycle in the interprocedural control-.ow graph, the analysis \nterminates and is sound: Soundness. If concrete state s can be reached at program point l and if the \nset of abstract elements computed for point l is { (Rl 0,Gl 0,N0l ),..., (Rl ,Gl ,Nl ) }, then s . nnn \n.(Rl i,Gl i,Ni l ) for some i . 0..n. Preliminary Empirical Experience. We have implemented shape subtraction \nand stack rule inference described in Section 5 in XISA [5, 6]. It discovers the appropriate stack rules \nfor all of the examples given in Sections 2 and 4. In each case, the stack rule inference time is negligible. \nWe also have implemented a prototype analyzer and ran it on a series of micro-benchmarks that compares \nthe analysis time of some recursive functions against their iterative counterparts (Table 1). The tests \nwere performed on a 2.4 GHz MacBook Pro with 8 GB of RAM and under a Linux 2.6.27 virtual machine. In \nall these cases, the memory usage is not signi.cant (at most 6 MB).  While the analysis times on these \nmicro-benchmarks are neg\u00adligible, we do see that analyzing the recursive versions take about two to three \ntimes more time than their iterative counterparts. This slowdown is expected since the analysis of a \nrecursive function in\u00advolves not only the inference of a suitable stack de.nition but also two .xed-point \ncomputations (one over the call sites and one over the return sites). In contrast, the analysis of a \nsingle imperative loop requires only one .xed point computation. Note that in the case of list reverse, \nthe imperative version is quite trivial (e.g., does not even require a segment summary in the loop invariant), \nwhile the stack inductive de.nition for the recursive version is just as com\u00adplex as the other examples. \n7. Case Study: Precision and Modularity In this section, we look more closely at numerical properties, \nwhich are combined with shape properties in the presence of recursion. Figure 8(a) shows an example program \nthat implements a .lter equation over a doubly-linked list. Here, we replaced the integer data .eld with \ntwo .oating point .elds x and y. It walks through the structure and deletes the nodes where the x .eld \nis not in range [-M, M]. In the same pass, it integrates a .lter equation by reading from the x .elds \nand writing the result to the y .elds; this computation is performed only for the nodes that are not \ndeleted. Let us .rst consider the purely numeric part of the code (i.e., the digital .lter) and imagine \nthat it is implemented using a while-loop instead of recursion. On this version of the program, the Astr\u00b4 \nee analyzer [3] can infer rather precise invariants using either the domain of Feret [11] or simply the \ninterval domain with threshold widening. For instance, if we let M =5, a =0.8, and p is initially set \nto 0, then Astr\u00b4 ee computes the range [-85.00008, 85.00008] as an approximation for the terms of the \nsequence after 7 iterations using only the interval domain with threshold widening (on the iterative, \npurely numeric version). Even though Astr\u00b4ee does not support recursion, the basic techniques are applicable \nto the purely numeric part of the program shown in Figure 8(a). On the other hand, using a modular approach \nto interprocedural analysis to reason precisely about the numeric part is quite chal\u00adlenging, as we must \ncapture relations between the inputs and out\u00adputs in the abstract domain. For instance, to obtain the \nsame prop\u00aderties as using the interval domain in the iterative version, we must use a complex domain \nlike polyhedra [8]. For non-linear .lters like those handled by Feret [11], even polyhedra would not \nbe suf.cient to achieve the same level of precision with the modular approach. In essence, relational \nabstract domains are sometimes required to achieve the same level of precision using the modular approach \nas non-relational domains on an iterative counterpart. We now consider analyzing the program shown in \nFigure 8(a) using our call stack abstraction and by instantiating the base do\u00admain with intervals, that \nis, with a simple non-relational domain. When applied to this program, our analysis must infer inductive \nrules for stack at two call sites (as there are two recursive call sites). We consider only the second \ncall site, as the other one is very similar, though slightly more complicated with respect to shape. \nFigure 8(b) depicts the abstract state after two recursive calls before summarization of the call stack \nis performed. Nodes that denote .oating point values are annotated with an interval. In particular, we \nhave that I.0 = I.1 =[-M, +M], Id0 = [-a*M - E, +a*M + E ' ], and so on. The E, E ' values account for \nrounding errors. To obtain an invariant, we .rst derive a stack rule shown in Figure 8(c). This rule \nallows us to fold the call stack at the .rst recursive call site. Interestingly, this rule does not work \nat the next iteration because the interval constraints over the .oating point nodes have not yet stabilized. \nInstead, we weaken the stack as discussed in Section 6.2. In essence, this process amounts to the same \nseries of widening steps on the numerical domain elements as void filter(dll* l, .oat p) { if (l != NULL){ \nif (l->x< -M||l->x >M){ dll* n = l->next; remove(l); filter(n, p); } else { l->y = a * p + l->x; filter(l->next, \nl->y); } } } (a) Recursive implementation of a digital .lter. (b) An abstract state after two recursive \ncalls.   unfold (c) An inferred rule for stack. Figure 8. Obtaining numerical properties on a recursive \nprogram with the call stack abstraction. in the analysis of the iterative program by Astr\u00b4 ee, except \nthey are applied when generating the stack summarization rule. Thus, it will lead to the computation \nof the same numerical invariants. Recall that we are not advocating an abandonment of modular interprocedural \nanalysis. As noted in Section 1, modularity is of\u00ad ten the basis for scalability. When functions should \nbe analyzed out of context (e.g., library API functions), then the modular ap\u00adproach seems ideal. Instead, \nwe present interprocedural analysis by call stack summarization as an alternative that can be more effec\u00adtive \nin certain situations. A potentially hybrid approach could be to apply modular analysis except were call \nstack summarization is absolutely required. For example, call stack summarization is ap\u00adplied for a few \nintricate internal functions as part of a modular, tabulation-based analysis of the program. 8. Discussion: \nInterprocedural Shape Analysis In this section, we consider known challenges in the context of interprocedural \nanalysis of heap manipulating programs and com\u00adment on trade-offs.  8.1 Cutpoints Modular shape analyses \n[4, 12, 13, 16, 23] typically infer an ab\u00ad straction of the effect of a procedure on a portion of the \nheap. To make this effect abstraction precise, the analysis needs to carefully extract the fragment of \nthe heap that may be modi.ed during a call so that the rest of the heap is a frame [21]. Such an analysis \nmust also abstract cutpoints [23] carefully to obtain precise results. Cutpoints are the locations at \nthe border of the callee s reachable heap, that is, any node that is reachable from the callee s parame\u00adters \n(though not directly pointed-to by them) and reachable from a pending activation or a global (without \nfollowing links inside the callee s reachable heap). It is important to track cutpoints precisely in \na modular analysis, as they are used to re.ect the effect of the callee in the caller s state on function \nreturn. Doing so can be chal\u00adlenging, as an unbounded number of cutpoints may arise either due to unbounded \nrecursion or due to the traversal of unbounded heap structures. While there is no general solution for \ncutpoints today, several partial solutions have been proposed. For example, they fo\u00adcus on isolating \ntheir effect by proving cutpoint-freeness [24] or by proving that cutpoints are not live [17], or they \nreason up to a bounded number of them [23]. Our analysis based on call stack summarization also needs \nto cope with cutpoints so that the widening iteration reaches a precise .xed point. In the following, \nwe consider how cutpoints impact our analysis in particular situations. Summarizing Cutpoints. Cutpoints \noften arise when a .eld of a caller s activation record point into the heap space that may be modi.ed \nby the callee (or a subsequent sub-call). This situation arises in the example in Section 2 (Figure 1). \nSuch cutpoints may be unbounded, as each recursive call de.nes a set of local cutpoints and the number \nof recursive calls may be unbounded; however, they can be summarized as part of the call stack and re-exposed \nwhen recursive calls return. Observe that such cutpoints appear among the parameters in inductive rules \nfor stack. Therefore, our approach can deal with an unbounded number of cutpoints by folding them as \npart of the call stack with .nitely many parameters. Case Splitting from Cutpoints. Another case is when \nthe cut\u00adpoints stem from the global variables or from sections of the call stack that are not summarized \n(e.g., as the activation record of main). Such cutpoints cause singularities in the call stack as in \nthe example from Rinetzky et al. [23]. In that example, a singly\u00ad linked list is built by appending two \nsingly linked lists while keep\u00ading a pointer to the element in the middle; then a destructive reverse \nfunction is applied to the whole structure. The state before the list reverse can be summarized as follows: \n Now, our analysis needs to walk through the segment between \u00df0 and \u00df1 (using the segment unfolding [5]) \nand discovers a stack rule for summarizing the call stack. The generated rule is actually very similar \nto the one derived from the list traversal example of Section 4 (Figure 4(e)); it is essentially the \nsame except for the direction of next pointers, which are switched by the reverse function. As we might \npredict, special care must be taken for the case where the reverse reaches node \u00df1. Applying widening \nhere would cause the property that l1 points inside the list to be lost. A solution is to case split \nand not perform widening on that iteration. The trick here is a variation on a classical one: when a \nnew behavior is observed (e.g., reaching \u00df1), postpone widening to the next iteration (cf., [3]). To \nsummarize, cutpoints induce a somewhat different set of is\u00adsues compared with modular shape analyses, \nas our analysis ab\u00adstracts only states and not effects. These issues can be alleviated using standard \ntechniques to some extent.  8.2 Heap Partitioning and Cutpoints Sharing can introduce more se\u00advere problems, \nas the example void f(dll* l) { if (...) { f(l->prev); }shown inset demonstrates. In this else { f(l->next); \n} } code, function f takes as input a doubly-linked list and performs a non-deterministic walk over it: \nat each step, depending on a random test, it either walks one step for\u00adward and calls itself recursively, \nor it walks one step backwards and calls itself recursively. The dif.culty here is that this function \nmay traverse several times the same elements of the doubly-linked list. Thus, at any time in a concrete \nexecution, an element that has been visited in the past may be visited again in the future. This prevents \nthe heap structure from being partitioned into sub-regions touched by successive recursive calls. This \nproblem would also occur in the case of analyses that infer the footprint of procedures so as to com\u00adpute \ntheir effect. This sort of problem can arise even without procedures, so we consider this issue orthogonal \nto the cutpoint problem: cutpoints aim at describing the borders in the heap partition used to abstract \nsuccessive calls, whereas the issue of this example is that no simple and good partitioning exists. 9. \nRelated Work The most closely related work to ours is Rinetzky and Sagiv [22]. They designed an analysis \nbased on three-valued logic [27] where the call stack is abstracted together with the heap. Some earlier \nwork had similar views (e.g., [9]). Rinetzky and Sagiv [22] s con\u00ad crete model is quite similar to ours. \nHowever, the abstraction is radically different, as we use separation logic and inductive de.\u00adnitions. \nIn particular, we found that these tools are natural for ab\u00adstracting the call stack. First, the call \nstack can be de.ned induc\u00adtively, and second, there are built-in separation constraints between the call \nstack and the heap, as well as between successive activation records. Moreover, we found that the operations \nlike function call, function return, and widening encode well in this abstraction. Most interprocedural \nshape analyses take advantage of func\u00adtions in order to achieve modularity. For instance, Jeannet et \nal. [16] abstracts input-output relations in the TVLA framework using three-valued structures with two \noccurrences of each element of the universe: one for the input state and one for the output state. The \nanalysis of Rinetzky et al. [23, 24] is based on the tabulation of pairs of input-output three-valued \nstructures. In Marron et al. [19], cutpoints are used in order to segment the abstract states and re\u00adstrict \nthe analysis to a precise approximation of the footprint of pro\u00adcedures. Similarly, separation logic-based \nmodular analyses, such as Gotsman et al. [12], Calcagno et al. [4], or Gulavani et al. [13], isolate \nthe footprint of procedures and compute the effect on the local heap (though in different manners). As \nwe noted in Section 1, modularity in shape analysis has many advantages, especially when analyzing libraries \nor for scalability reasons. Our approach .lls a gap to tackle a family of analysis problems, such as \nthe combined analysis example of Section 7, where whole program analysis ap\u00ad pears more suitable. Another \narea of static analysis related to our work is that of context-sensitive analyses. The term context sensitive \nis used in many ways and may cover very different levels of abstraction, rang\u00ading from approaches that \nuse call-strings to describe contexts [28] or an abstraction of the control .ow history [25] to techniques \nwhere the call stack content is abstracted [15, 22]. Our technique uses a rather coarse abstraction of \nthe call string and of the control .ow history, so its precision can be further improved by applying \nthose techniques [25, 28]. The analysis of Jeannet and Serwe [15] features a non-relational abstraction \nof the call stack in the sense that it discards the order of the activation records.  10. Conclusion \nIn this paper, we have explored a whole-program approach to inter\u00adprocedural analysis over recursive \nprograms. In particular, we have shown how to apply a shape analysis abstraction based on sep\u00adaration \nlogic and inductive de.nitions to directly summarize call stacks along with heap structures. To automatically \nderive rules for call stack summarization, we exploited its built-in inductive struc\u00adture. The XISA framework \n[5] turned out to be both expressive and robust in abstracting a very concrete model of calling contexts. \nAcknowledgments We would like to thank Josh Berdine, Noam Rinetzky, and Hong\u00adseok Yang for interesting \ndiscussions on interprocedural shape analysis and on the issues related to cutpoints. We also thank the \nanonymous reviewers for their helpful suggestions, including insightful comments on the whole-program \nversus modular ap\u00adproach. References [1] Josh Berdine, Cristiano Calcagno, and Peter W. O Hearn. Symbolic \nexecution with separation logic. In Asian Symposium on Programming Languages and Systems (APLAS), pages \n52 68, 2005. [2] Josh Berdine, Cristiano Calcagno, Byron Cook, Dino Distefano, Pe\u00adter W. O Hearn, Thomas \nWies, and Hongseok Yang. Shape analysis for composite data structures. In Computer-Aided Veri.cation \n(CAV), pages 178 192, 2007. [3] Bruno Blanchet, Patrick Cousot, Radhia Cousot, J\u00b4ome Feret, Lau\u00ad er rent \nMauborgne, Antoine Min\u00b4 e, David Monniaux, and Xavier Rival. A static analyzer for large safety-critical \nsoftware. In Programming Language Design and Implementation (PLDI), pages 196 207, 2003. [4] Cristiano \nCalcagno, Dino Distefano, Peter W. O Hearn, and Hongseok Yang. Compositional shape analysis by means \nof bi-abduction. In Principles of Programming Languages (POPL), pages 289 300, 2009. [5] Bor-Yuh Evan \nChang and Xavier Rival. Relational inductive shape analysis. In Principles of Programming Languages (POPL), \npages 247 260, 2008. [6] Bor-Yuh Evan Chang, Xavier Rival, and George C. Necula. Shape analysis with \nstructural invariant checkers. In Static Analysis (SAS), pages 384 401, 2007. [7] Patrick Cousot and \nRadhia Cousot. Abstract interpretation: A uni.ed lattice model for static analysis of programs by construction \nor ap\u00adproximation of .xpoints. In Principles of Programming Languages (POPL), pages 238 252, 1977. [8] \nPatrick Cousot and Nicolas Halbwachs. Automatic discovery of linear restraints among variables of a program. \nIn Principles of Program\u00adming Languages (POPL), pages 84 97, 1978. [9] Alain Deutsch. On determining \nlifetime and aliasing of dynamically allocated data in higher-order functional speci.cations. In Principles \nof Programming Languages (POPL), pages 157 168, 1990. [10] Dino Distefano, Peter W. O Hearn, and Hongseok \nYang. A local shape analysis based on separation logic. In Tools and Algorithms for the Construction \nand Analysis of Systems (TACAS), pages 287 302, 2006. [11] J\u00b4ome Feret. Static analysis of digital .lters. \nIn European Symposium er on Programming (ESOP), pages 33 48, 2004. [12] Alexey Gotsman, Josh Berdine, \nand Byron Cook. Interprocedural shape analysis with separated heap abstractions. In Static Analysis (SAS), \npages 240 260, 2006. [13] Bhargav S. Gulavani, Supratik Chakraborty, Ganesan Ramalingam, and Aditya V. \nNori. Bottom-up shape analysis. In Static Analysis (SAS), pages 188 204, 2009. [14] Bolei Guo, Neil Vachharajani, \nand David I. August. Shape analysis with inductive recursion synthesis. In Programming Language Design \nand Implementation (PLDI), pages 256 265, 2007. [15] Bertrand Jeannet and Wendelin Serwe. Abstracting \ncall-stacks for interprocedural veri.cation of imperative programs. In Algebraic Methodology and Software \nTechnology (AMAST), pages 258 273, 2004. [16] Bertrand Jeannet, Alexey Loginov, Thomas Reps, and Mooly \nSagiv. A relational approach to interprocedural shape analysis. ACM Trans. Program. Lang. Syst., 32(2), \n2010. [17] J\u00a8org Kreiker, Thomas Reps, Noam Rinetzky, Mooly Sagiv, Reinhard Wilhelm, and Eran Yahav. \nInterprocedural shape analysis for effec\u00adtively cutpoint-free programs. Technical report, Queen Mary \nUniver\u00adsity of London, 2010. [18] Vincent Laviron, Bor-Yuh Evan Chang, and Xavier Rival. Separating shape \ngraphs. In European Symposium on Programming (ESOP), pages 387 406, 2010. [19] Mark Marron, Manuel V. \nHermenegildo, Deepak Kapur, and Darko Stefanovic. Ef.cient context-sensitive shape analysis with graph \nbased heap models. In Compiler Construction (CC), pages 245 259, 2008. [20] Thomas Reps, Susan Horwitz, \nand Mooly Sagiv. Precise interproce\u00addural data.ow analysis via graph reachability. In Principles of Pro\u00adgramming \nLanguages (POPL), pages 49 61, 1995. [21] John C. Reynolds. Separation logic: A logic for shared mutable \ndata structures. In Logic in Computer Science (LICS), pages 55 74, 2002. [22] Noam Rinetzky and Mooly \nSagiv. Interprocedural shape analysis for recursive programs. In Compiler Construction (CC), pages 133 \n149, 2001. [23] Noam Rinetzky, J\u00a8org Bauer, Thomas Reps, Mooly Sagiv, and Rein\u00adhard Wilhelm. A semantics \nfor procedure local heaps and its abstrac\u00adtions. In Principles of Programming Languages (POPL), pages \n296 309, 2005. [24] Noam Rinetzky, Mooly Sagiv, and Eran Yahav. Interprocedural shape analysis for cutpoint-free \nprograms. In Static Analysis (SAS), pages 284 302, 2005. [25] Xavier Rival and Laurent Mauborgne. The \ntrace partitioning abstract domain. ACM Trans. Program. Lang. Syst., 29(5), 2007. [26] Mooly Sagiv, Thomas \nReps, and Reinhard Wilhelm. Solving shape\u00adanalysis problems in languages with destructive updating. ACM \nTrans. Program. Lang. Syst., 20(1):1 50, 1998. [27] Mooly Sagiv, Thomas Reps, and Reinhard Wilhelm. Parametric \nshape analysis via 3-valued logic. ACM Trans. Program. Lang. Syst., 24(3): 217 298, 2002. [28] Micha \nSharir and Amir Pnueli. Two approaches to interprocedural data .ow analysis. In Steven S. Muchnick and \nNeil D. Jones, editors, Program Flow Analysis: Theory and Applications, chapter 7, pages 189 233. Prentice-Hall, \n1981. [29] Arnaud Venet. Abstract co.bered domains: Application to the alias analysis of untyped programs. \nIn Static Analysis (SAS), pages 366 382, 1996.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Interprocedural program analysis is often performed by computing procedure summaries. While possible, computing adequate summaries is difficult, particularly in the presence of recursive procedures. In this paper, we propose a complementary framework for interprocedural analysis based on a direct abstraction of the calling context. Specifically, our approach exploits the inductive structure of a calling context by treating it directly as a stack of activation records. We then build an abstraction based on separation logic with inductive definitions. A key element of this abstract domain is the use of parameters to refine the meaning of such call stack summaries and thus express relations across activation records and with the heap. In essence, we define an abstract interpretation-based analysis framework for recursive programs that permits a fluid per call site abstraction of the call stack--much like how shape analyzers enable a fluid per program point abstraction of the heap.</p>", "authors": [{"name": "Xavier Rival", "author_profile_id": "81100659525", "affiliation": "INRIA Paris-Rocquencourt, Paris, France", "person_id": "P2509593", "email_address": "rival@di.ens.fr", "orcid_id": ""}, {"name": "Bor-Yuh Evan Chang", "author_profile_id": "81464662824", "affiliation": "University of Colorado at Boulder, Boulder, CO, USA", "person_id": "P2509594", "email_address": "bec@cs.colorado.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926406", "year": "2011", "article_id": "1926406", "conference": "POPL", "title": "Calling context abstraction with shapes", "url": "http://dl.acm.org/citation.cfm?id=1926406"}