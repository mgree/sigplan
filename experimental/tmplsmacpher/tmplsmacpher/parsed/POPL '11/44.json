{"article_publication_date": "01-26-2011", "fulltext": "\n Safe Nondeterminism in a Deterministic-by-Default Parallel Language Robert L. Bocchino Jr.1 Stephen \nHeumann2 Nima Honarmand2 Sarita V. Adve2 Vikram S. Adve2 Adam Welc3 Tatiana Shpeisman4 1Carnegie Mellon \nUniversity 2University of Illinois at Urbana-Champaign 3Adobe Systems 4Intel Labs rbocchin@cs.cmu.edu \ndpj@cs.uiuc.edu awelc@adobe.com tatiana.shpeisman@intel.com Abstract A number of deterministic parallel \nprogramming models with strong safety guarantees are emerging, but similar support for non\u00addeterministic \nalgorithms, such as branch and bound search, remains an open question. We present a language together \nwith a type and effect system that supports nondeterministic computations with a deterministic-by-default \nguarantee: nondeterminism must be ex\u00adplicitly requested via special parallel constructs (marked nd), \nand any deterministic construct that does not execute any nd construct has deterministic input-output \nbehavior. Moreover, deterministic parallel constructs are always equivalent to a sequential composi\u00adtion \nof their constituent tasks, even if they enclose, or are enclosed by, nd constructs. Finally, in the \nexecution of nd constructs, in\u00adterference may occur only between pairs of accesses guarded by atomic \nstatements, so there are no data races, either between atomic statements and unguarded accesses (strong \nisolation) or between pairs of unguarded accesses (stronger than strong isolation alone). We enforce \nthe guarantees at compile time with modular check\u00ading using novel extensions to a previously described \neffect system. Our effect system extensions also enable the compiler to remove unnecessary transactional \nsynchronization. We provide a static se\u00admantics, dynamic semantics, and a complete proof of soundness \nfor the language, both with and without the barrier removal feature. An experimental evaluation shows \nthat our language can achieve good scalability for realistic parallel algorithms, and that the barrier \nremoval techniques provide signi.cant performance gains. Categories and Subject Descriptors D.1.3 [Software]: \nCon\u00adcurrent Programming Parallel Programming; D.3.1 [Software]: Formal De.nitions and Theory; D.3.2 [Software]: \nLanguage Class-i.cations Concurrent, distributed, and parallel languages; D.3.2 [Software]: Language \nClassi.cations Object-oriented languages; D.3.3 [Software]: Language Constructs and Features Concurrent \nProgramming Structures General Terms Languages, Veri.cation, Performance 1. Introduction Widely used \nparallel programming models today (Java, C#, Posix, Win32) are based on a low-level and error-prone concept \nof threads. These models provide few or no guards against parallel program- Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 ming errors such as data races, \ndeadlocks, or atomicity viola\u00adtions. Some higher-level programming models are available, or are emerging, \nthat prevent these kinds of errors. However, these models achieve their safety guarantees by greatly \nrestricting side effects, either through functional programming (e.g., STM Haskell [31]) or through data.ow \nor data-parallel styles of programming (e.g., Concurrent Collections [23], Ct [30]). There is much recent \ninterest in supporting deterministic algo\u00adrithms within general imperative languages, via static type \nsys\u00adtems [17], runtime-supported language mechanisms [10, 11, 24, 45, 49, 50], or largely transparent \nruntime techniques [14, 15, 27, 43]. For algorithms that have deterministic input-output behavior, such \nmodels can provide major bene.ts compared with traditional thread-based programming [18, 38]. There are \nalso important algorithms, however, that do not have deterministic input-output behavior, and are not \nsupported by these techniques. Some examples include clustering algorithms, opti\u00admization algorithms \nlike branch-and-bound solvers, and graph al\u00adgorithms like Delaunay mesh re.nement. A common feature of \nsuch algorithms is that they permit any of multiple possible out\u00adputs to be produced for a given input. \nSuch outputs must usually be derived from a controlled set of choices, typically from differ\u00adent, schedule-dependent, \norders of evaluating parallel tasks, e.g., evaluating different groups of neighboring points in clustering \nal\u00adgorithms. Importantly, the nondeterminism should not simply de\u00adrive from unpredictable behavior due \nto data races and atomicity violations. Such behavior is not only potentially erroneous but can also \ncan make proram executions dif.cult to reason about, e.g., by producing non-sequentially-consistent results. \nFurthermore, real\u00adworld applications are composed of a (potentially large) number of different algorithms, \nlikely to be a mixture of deterministic and nondeterministic ones. Therefore, it is essential to be able \nto com\u00adpose deterministic and nondeterministic algorithms in a way that is easy to reason about. These \nobservations pose two challenges for a safe and realistic parallel programming model: (1) How do we express \nnondetermin\u00adism itself in a disciplined manner that simpli.es reasoning about program behavior? (2) How \ndo we allow nondeterministic and de\u00adterministic computations to be composed without weakening the deterministic \nguarantees for the latter? In this work, we present a Java-based parallel language that supports both \ndeterministic and nondeterministic parallel code in a disciplined manner. Our language has the following \nmajor features: 1. Deterministic parallel operations. We provide operations that describe deterministic \nparallel composition of tasks. A deter\u00administic parallel operation enforces noninterference between its \ncomponent tasks (i.e., there are no con.icting reads or writes in any pair of tasks), ensuring that the \nwhole operation behaves like a sequential (and therefore deterministic) composition of its component \ntasks. The noninterference property is enforced at compile time. This part is derived from previous work \n[17]. 2. Nondeterministic parallel operations. We provide operations that describe potentially nondeterministic \nparallel composition of tasks. These operations allow interference between tasks, but any such interference \nis subject to the following guarantees, again enforced at compile time. This part is entirely new. (a) \nRace freedom and sequential consistency. No execution of a valid program in our language can ever produce \na data race. This property is very important, even for nondeterministic codes, because in the Java memory \nmodel, race freedom im\u00adplies sequential consistency, which makes parallel programs much easier to reason \nabout. (b) Strong isolation. Our language provides an atomic state\u00adment atomic S that executes the enclosed \nstatement S in isolation, i.e., as if there were no interleaving with concur\u00adrently executing tasks. \nThe isolation is strong, i.e., isolation is provided with respect to all concurrent operations, not just \nother ones occurring in atomic statements. Novel effect sys\u00adtem features enable our language to be built \non top of an off the shelf runtime, such as software transactional memory, that provides only weak isolation. \nPrevious work [6, 31, 42] has also used effects to enforce strong isolation but, as dis\u00adcussed in Section \n7, our language is less restrictive, and our guarantees are stronger. (c) Composition of deterministic \nand nondeterministic opera\u00adtions. A deterministic parallel operation always behaves as an isolated and \nsequential composition of its component tasks, even if the operation encloses, or is enclosed in, a nondeterministic \nparallel operation. This property allows local, compositional reasoning about deterministic opera\u00adtions, \nwhich we view as essential for a language that sup\u00adports both deterministic and nondeterministic operations. \n (d) Determinism by default. Nondeterminism occurs only where requested by an explicit nondeterministic \noperation,and cannot occur by accident. Speci.cally, if a deterministic construct does not encounter \nany nondeterministic construct for a given input heap state in some execution, then it has de\u00adterministic \ninput-output behavior (i.e., it produces the same output heap state and other results in all executions, \nfor that input heap state).  For the deterministic parallel operations (1), we build on Determin\u00adistic \nParallel Java (DPJ) [17]. In DPJ, the programmer partitions the heap into regions, and writes effect \nsummaries on methods that de\u00adscribe the method s read and write operations on the regions. The compiler \nuses the regions and effect summaries to enforce noninter\u00adference of parallel tasks at compile time. \nHowever, by design, DPJ completely disallows nondeterministic parallel algorithms.To pro\u00advide the nondeterministic \noperations (2) and their associated guar\u00adantees, we must extend DPJ. For the isolation guarantee 2(b), \nwe build on software transac\u00adtional memory (STM) [37]. STM is not the only choice here, but it is a good \none, as it runs on all platforms (as opposed to hard\u00adware transactions, which require special hardware) \nand provides relatively strong guarantees (isolation and deadlock freedom) with very low programming \noverhead. However, STM alone is insuf.\u00adcient for our purposes. First, STM implementations usually provide \nonly weak isolation, and we want strong isolation. Second, even strong isolation is not enough: it allows \ndata races between accesses outside of transactions, and we want to disallow such data races. Finally, \nSTM introduces signi.cant runtime overheads, including scalar overhead and false con.icts (due to over-synchronization), \nwhich can cause poor scalability. To solve these technical challenges, we extend the DPJ effect system \nin several ways. First, we add a new kind of effect called an atomic effect for tracking when memory \naccesses occur inside an atomic statement. The atomic effects allow the compiler to guaran\u00adtee both race \nfreedom (property 2(a)) and strong isolation (2(b)), by prohibiting con.icting memory operations unless \neach operation is in an atomic statement. Second, we introduce new effect checking rules to enforce composition \nof operations (2(c)) and determinism by default (2(d)). For composition of operations, the extended ef\u00adfect \nsystem disallows interference between a deterministic opera\u00adtion and any other concurrent operation unless \nthe whole determin\u00adistic operation is enclosed in an atomic statement. For determinism by default, the \ninterference is disallowed for deterministic paral\u00adlel operations, but allowed for nondeterministic parallel \noperations. Third, to reduce STM overhead, we introduce atomic regions,so that the programmer can identify \nwhich regions may be accessed in an interfering manner. For operations to other regions, the compiler \ncan remove or simplify the STM synchronization, because such op\u00aderations never cause con.icts. Overall, \nthis work makes the following contributions: 1. We present a language that provides the compile-time \nguaran\u00adtees 1 through 2(d) stated above. To our knowledge, no previous language or system has provided \nall these properties for shared\u00admemory parallel programs through any mechanisms, static or dynamic. Our \nlanguage includes novel extensions to the DPJ effect system as discussed above for enforcing race freedom, \nstrong isolation, and determinism by default; and for reducing the runtime overhead of the underlying \nSTM implementation. 2. We formalize our ideas using three formal languages: the .rst has only deterministic \nparallel operations, the second adds non\u00addeterministic parallel operations, and the third adds atomic \nre\u00adgions. We have developed a full syntax, static semantics, and dynamic semantics for all three languages. \nFurther, we have for\u00admally stated the soundness properties given informally above, and proved that the \nproperties follow from the semantic de.ni\u00adtions. Here we summarize the key features of the formal lan\u00adguage \nand the essential soundness results; the full details, in\u00adcluding proofs, may be found in the lead author \ns Ph.D. the\u00adsis [19]. 3. We describe our experience using our language to implement three nondeterministic \nalgorithms: Delaunay Mesh Re.nement from the Lonestar benchmarks [1], the traveling salesman prob\u00adlem \n(TSP), and OO7 [25], a synthetic database benchmark. Our experience shows that porting these algorithms \nform pure Java into our language was relatively straightforward and re\u00adquired neither redesign of existing \ndata structures nor restruc\u00adturing of the algorithms themselves. The language naturally ex\u00adpresses all \nthese algorithms, although the speedups achieved vary depending on the inherent parallelism in the algorithms \nand performance limitations of the underlying STM. Addition\u00adally, judicious use of atomic regions eliminated \na large fraction of the STM-related overhead in two out of three benchmarks.  2. Background In this \nwork, we build on a language called Deterministic Parallel Java (DPJ) [17]. DPJ uses an effect system \nto enforce deterministic semantics for explicitly parallel programs via compile-time type checking. This \nsection brie.y explains the key constructs of DPJ; the details may be found in [17, 19]. In the rest \nof this paper, we refer to the preexisting language as basic DPJ. DPJ provides a fork-join parallel model: \nthe programmer cre\u00adates parallel tasks using either a foreach statement (for a parallel loop) or cobegin \nblock (for a group of mutually parallel state\u00adments). DPJ s effect system guarantees that in a well-typed \nparallel program, any two parallel tasks have noninterfering effects.Anef\u00adfect is a set of operations \non memory. Two effects interfere if they both access a common memory location and at least one of them \nwrites to that location. The noninterference guarantee for parallel tasks implies deterministic input-output \nsemantics for the compu\u00adtation. The DPJ effect system works as follows. The programmer as\u00adsigns every \nobject .eld and array cell to a region and annotates every method with a method effect summary stating \n(a superset of) the reads or write operations performed by the method, in terms of regions. The compiler \nchecks two things: (1) that the effect sum\u00admaries are a superset of the actual effects in the method \nbody; and (2) that no two parallel statements are interfering. The effect sum\u00admaries on method de.nitions \nenable modular checking of effects. 1 class Node<region P> { 2 region L, R; 3 double mass in P; 4 Node<P:L> \nleft in P:L; 5 Node<P:R> right in P:R; 6 void setMass(double mass) writes P { 7 this.mass = mass; // \nwrites P 8 } 9 void setMassOfChildren(double mass) writes P:* { 10 cobegin { 11 if (left != null) left.setMass(mass); \n// writes P:L 12 if (right != null) right.setMass(mass); // writes P:R 13 } 14 } 15 } Figure 1. Some \nfeatures of basic DPJ for deterministic parallelism. Figure 1 illustrates the use of regions and effects \nin basic DPJ. In line 1, we declare class Node to have one region parameter P. Line 3 declares .eld mass \nin region P; the actual region of the .eld is determined when the class is instantiated into a type, \nas shown in lines 4 and 5. Line 2 declares names L and R that have static scope (i.e., they are shared \nby all instances of class Node). Lines 4 and 5 declare .elds left and right and place them in regions \nP:L and P:R, respectively. The form P:L is called a region path list, or RPL, and it expresses the hierarchical \nstructure of regions: intuitively, P:L and P:R are both nested under P. The use of L and R puts the two \n.elds in different regions, while the use of P allows different Node objects instantiated with different \nbindings to P to have their .elds in different regions. Because L and R are distinct names, P:L and P:R \nare guaranteed to refer to different regions, for any common binding to P. Lines 6 and 9 illustrate the \nuse of method effect summaries. Method setMass (line 6) has declared effects writes P, while setMassOfChildren \n(line 9) has declared effects writes P:*, where the * is a wildcard representing any sequence of names. \nIf an effect declaration is omitted, it defaults to most general effect (writes the whole heap). The \ncompiler performs checks (1) and (2) stated above by accu\u00admulating the effects of a method, foreach statement, \nor cobegin statement. The analysis is simple and local because, at each call site, the declared effects \nof the invoked method provide the effects of the invocation, after substituting actual for formal region \nparam\u00adeters. For example, the effect of left.setMass(mass) in line 11 is writes P:L, obtained by substituting \nP:L (from the type of left) for the class parameter P in the declared method effect writes P (the read \nof .eld left is subsumed because in DPJ, write effects imply read effects). Similarly, the compiler infers \nthe effect of a .eld access or assignment to e.f by substituting the region named in the type of e for \nthe parameter in the declared region of the .eld f. As an example of check (1) (correct method effects), \nthe effect of setMass is legal because the method body writes to .eld mass in region P and has no other \nheap effects. As an example of check (2) (parallel noninterference), the compiler infers that the effect \nof lines 11 and 12 are writes P:L and writes P:R, respectively. Because P:L and P:R must be disjoint \nregions, for any common binding to P, the effects are noninterfering. Although this example is somewhat \nsimplistic, these and other features of DPJ can express a range of realistic parallel idioms [17], including \nparallel updates on arrays of objects, parallel traversals and updates of a tree, in-place divide-and-conquer \non arrays, and commutative operations within parallel tasks. 3. Language Support for Nondeterminism \nWe now informally describe the language mechanisms for non\u00addeterministic parallel control, parallel safety \nguarantees, and op\u00adtimization support. We illustrate the new language features with a running example \nof the traveling salesman problem (TSP). Sec\u00adtion 4 describes the language more formally. 3.1 The TSP \nComputation The traveling salesman problem, or TSP, is the well-known prob\u00adlem of .nding a shortest cycle \nin a weighted graph that vis\u00adits all the nodes once (i.e., a Hamiltonian cycle). TSP can be solved by \nbranch and bound search, a common algorithm for solving optimization problems and a classical example \nof a non\u00addeterministic computation. Figures 2 4 show simpli.ed Java\u00adlike pseudocode for TSP. The global \ndata (lines 1 13) include a weighted graph that is the input to the program; a priority queue for storing \nthe paths being explored; and a best (i.e., short\u00adest) tour, which is re.ned as the computation progresses, \neven\u00adtually storing the answer. Two regions are used to hold the data: ReadOnly for .elds that will not \nbe modi.ed during the computa\u00adtion, and Mutable for those that will be. The priority queue s type PriorityQueue<Path<ReadOnly>, \nMutable> indicates that it contains objects of type Path<ReadOnly>, and that the internal data used to \nrepresent the queue itself is in region Mutable.The main computation loop (lines 15 23) iterates in parallel \nover sev\u00aderal worker tasks. Each task generates a pre.x to search (using the pseudocode in Figure 3) \nand adds it to a priority queue. When all pre.xes have been generated, the tasks remove pre.xes from \nthe priority queue and search them (using the pseudocode in Figure 4), until there are no more pre.xes \nto search. 3.2 Nondeterministic Parallel Control To express nondeterministic parallel computations, \nwe introduce a parallel loop denoted foreach nd,where nd stands for nondeter\u00administic. Line 15 in Figure \n2 shows an example. This construct is identical to foreach in basic DPJ [17], except it says explicitly \nthat con.icting accesses, and therefore potential nondeterminism, are allowed between the loop iterations \nof foreach nd.We also intro\u00adduce a cobegin nd construct corresponding to cobegin in basic DPJ. Collectively, \nwe refer to these four (foreach, foreach nd, cobegin,and cobegin nd)as parallel constructs. The resulting \nparallel control structure is just fork-join paral\u00adlelism and can be represented as a static task graph, \nwhere each node or task is a single iteration of a parallel loop (foreach or foreach nd) or a single \nstatement in a cobegin or cobegin nd. All four parallel constructs have an implicit join synchronization \nat the end of the construct for the tasks of the construct. The (di\u00adrected) edges in the task graph represent \neither program order, or forking at the start of a parallel construct, or the join synchro\u00adnization at \nthe end of a parallel construct. Two tasks are concurrent if they are not ordered in the task graph. \nTwo memory accesses are concurrent if they occur in concurrent tasks.     1 /* Regions for partitioning \ndata */ 2 region ReadOnly, atomic Mutable; 3 4 /* Graph we are working on; immutable */ 5 Graph<ReadOnly> \ngraph in ReadOnly = the TSP graph ; 6 7 /* Priority queue for tour prefix paths */ 8 final PriorityQueue<Path<ReadOnly>, \nMutable> priorityQueue = 9 new PriorityQueue<Path<ReadOnly>, Mutable>(); 10 priorityQueue.add(new Path<ReadOnly>(startNode)); \n11 12 /* The answer */ 13 Path<ReadOnly> bestTour in Mutable = infinite path ; 14 15 foreach_nd(int i \nin 0, NWORKERS) { 16 Path<ReadOnly> prefix = null; 17 do { 18 atomic { 19 prefix = generateNextPrefix(); \n20 } 21 if (prefix != null) searchAllToursWithPrefix(prefix); 22 } while (prefix != null); 23 } Figure \n2. Global data and main computation for the Traveling Salesman Problem. 1 Path generateNextPrefix() \nreads ReadOnly writes Mutable { 2 while (!priorityQueue.isEmpty() &#38;&#38; 3 priorityQueue.best().length() \n< bestTour.length()) { 4 Path<ReadOnly> prefix = priorityQueue.removeBest(); 5 if (prefix.nodeCount() \n> PREFIX_CUTOFF) { 6 return prefix; 7 } else { 8 for (each edge edge that can be added to prefix 9 while \nstaying under bestTour.length()) { 10 Path<ReadOnly> newPrefix = 11 new Path<ReadOnly>(prefix, edge); \n12 priorityQueue.add(newPrefix); 13 } 14 } 15 } 16 return null; 17 } Figure 3. Generating the next tour \npre.x. 1 void searchAllToursWithPrefix(Path<ReadOnly> prefix) 2 reads ReadOnly writes atomic Mutable \n{ 3 for (each Hamilton cycle tour in graph with prefix prefix) { 4 atomic { 5 if (tour.length() < bestTour.length()) \n{ 6 bestTour = tour; 7 } 8 } 9 } 10 } Figure 4. Searching all tours with a given pre.x. The speci.c \nparallel constructs used to fork and join tasks are not fundamental to our work. The language mechanisms \nused to enforce safety properties (described next) can be applied directly to other fork-join parallel \nprogramming languages, e.g., Cilk [16], a large subset of OpenMP [4], and potentially other parallel \nlan\u00adguages in which the compiler can identify all groups of concurrent tasks. Distinguishing the constructs \nthat permit interference (i.e., may be nondeterministic) from those that do not (and so are deter\u00administic) \nis a useful property, but again, not necessary for any of our other guarantees.  3.3 Safety Properties \nfor Nondeterministic Code As stated in the introduction, the goal of our language design and type system \nis to achieve four safety guarantees for nondeterminis\u00adtic and deterministic code: (i) data race freedom; \n(ii) strong isola\u00adtion for nondeterministic parallel constructs; (iii) sequential equiv\u00adalence for deterministic \nparallel constructs; and (iv) a property we call determinism by default, de.ned below. These four properties \ngive programmers a simple, elegant execution model for reasoning about partly nondeterministic programs. \nBelow, we discuss the lan\u00adguage mechanisms for expressing synchronization, the effect sys\u00adtem features \nfor enforcing the properties, and the resulting execu\u00adtion model seen by programmers. Synchronization: \nTo ensure correctly synchronized accesses in the presence of interference (de.ned in Section 2), we add \nan atomic statement to the language. This construct is similar to previous work [8, 28, 32, 33], except \nthat in conjunction with our effect system, discussed below, our atomic statements provide stronger guarantees. \nA statement atomic S indicates that S is tobe run as if all other concurrent execution were suspended \nwhile S is executing. This is called strong isolation [40, 47]. With reference to the TSP example, in \nFigure 2 each call to generateNextPrefix is enclosed by an atomic statement that protects the accesses \nto the shared priority queue. Note that while the calls to generateNextPrefix are effectively serialized, \neach worker can start its call to searchAllToursWithPrefix as soon as its call to generateNextPrefix \nis done, in a pipelined manner. This pattern can achieve good speedups because most of the work in this \ncode is done in searchAllToursWithPrefix. In Figure 4, an atomic statement protects the concurrent updates \nto bestTour. An atomic statement may appear inside any of the four parallel constructs, as well as inside \nother atomic statements. Two nested atomic statements in the same task are .attened : that is, the inner \natomic becomes a no-op, and atomicity is enforced entirely at the outer atomic. If a parallel task created \nin an atomic statement contains a nested atomic statement, then the nesting behaves in the standard way: \nthe inner atomic enforces isolation with regard to other tasks created by the immediately enclosing parallel \nconstruct, while the outer atomic enforces isolation as to tasks created by any outer enclosing parallel \nconstruct. Effect System: We now discuss how our effect system enforces the four safety properties stated \nat the outset of this section. Data Race Freedom and Strong Isolation: We use the following strategy \nto ensure both data race freedom and strong isolation. First, a transactional runtime guarantees at least \nweak isolation of atomic statements (i.e., isolation between different atomic statements, but not between \natomic statements and unguarded code). Second, the effect system ensures that for any pair of con.icting \nmemory ac\u00adcesses, each access occurs inside an atomic statement. For exam\u00adple, in Figure 4, any two concurrent \naccesses to bestTour are both enclosed in instances of the atomic block at line 4 (the concur\u00adrency is \ncreated by the foreach nd at line 15 of Figure 2). This requirement ensures strong isolation, because \nno con.icts between unguarded memory accesses and atomic statements are allowed. It also ensures race \nfreedom, because no con.icts between pairs of unguarded accesses are allowed. Notice two things about \nour strategy. First, our language may be built on top of a standard software transactional memory (STM) \nimplementation, which typically guarantees only weak isolation for performance reasons. Second, our strategy \nprohibits all data races. Even TM systems with strong isolation generally allow data races between pairs \nof accesses occurring outside any transaction. To accomplish the effect checking, we extend the DPJ effect \nsystem to distinguish effects that are atomic (meaning the effect occurred inside an atomic statement) \nfrom effects that are non\u00adatomic (meaning the effect occurred outside any atomic statement). The compiler \nensures that interference occurs only between atomic effects. To enable sound modular reasoning about \nmethod invocations, we make atomic effects explicit in method effect summaries. For example, the effect \nwrites atomic Mutable in the summary for searchAllToursWithPrefix (Figure 4) says that any possible writes \nto region Mutable occur inside atomic blocks in the body of the method or its callees. In checking method \neffect summaries, our system is sound but conservative: it is correct to summarize a write to region \nR occurring inside an atomic block as either writes atomic R or simply writes R; the latter is more con\u00adservative \nthan necessary but is correct. However, it is not correct to summarize an access occurring outside any \natomic section as an atomic effect, because such an effect would report a transactional guard when in \nfact there is none.  For example, the effect system can verify that all interfering accesses within \nthe foreach nd in Figure2areatomiceffects. First, the variable prefix is local to each task and so generates \nno con.icts across tasks. Second, according to the effect summary for generateNextPrefix (Figure 3), \nthe method invocation in line 19 produces con.icting effects on region Mutable.These effects are enclosed \nwithin the atomic statement starting at line 18 and so are recorded as atomic effects that may interfere. \nThird, the call to searchAllToursWithPrefix is not within an atomic statement; but according to its effect \nsummary it generates only read effects (which do not interfere with themselves) and atomic write effects \n(which are allowed to interfere with themselves, and are to a different region from the read effects). \nSequential Equivalence for Deterministic Constructs: An impor\u00adtant property we wish to preserve from \nbasic DPJ is sequential equivalence for deterministic constructs: that is, foreach and cobegin are equivalent \nto the sequential execution of their con\u00adstituent tasks in program order. To enforce this property, we \nobvi\u00adously need to disallow interference between cobegin or foreach branches, even if the interfering \neffects are atomic. For example, this program is not allowed: cobegin { atomic x = 0; atomic x = 1; } \nFor this we just have a simple typing rule that interference between atomic effects is allowed only inside \nforeach nd or cobegin nd. However, that is not enough, because interference can also occur between a \ndeterministic task and a concurrent nondeterministic task. For example, consider the following program: \nz= 0; cobegin_nd { cobegin { atomic x = z; atomic y = z; } // S1, S2 atomicz=1; //S3 } This program could \nproduce the result x =1, y =0 by ex\u00adecuting S2; S3; S1. This result violates sequential equivalence of \ncobegin, because it does not correspond to any sequentially consistent execution of the program where \nthe cobegin block is executed in program order. Instead, we wish to ensure that a foreach or cobegin \nexecutes in isolation, even if it appears in\u00adside foreach nd or cobegin nd. Our solution to this problem \nis to convert atomic effects oc\u00adcurring inside a deterministic construct to non-atomic effects when propagating \nthem to the outer context. In the example above, when checking interference, the compiler sees nonatomic \nreads to z in the .rst cobegin nd branch: those reads occurred in atomic state\u00adments, but became nonatomic \nwhen passing outwards across the cobegin. On the other hand, the second branch has atomic writes to z. \nTherefore the cobegin nd branches have illegal read-write in\u00adterference (i.e., not both guarded by atomic) \non z. To write this pro\u00adgram legally in our language, the programmer could put the whole cobegin in an \natomic statement. Determinism by Default: Finally, by virtue of the isolation of deter\u00administic constructs, \nand the noninterference between their internal tasks, both discussed above, we have the following property: \nif a deterministic construct does not dynamically execute any nonde\u00adterministic construct, then the execution \nof the deterministic con\u00adstruct is, in fact, deterministic. That is, a given input heap state to the \ndeterministic construct always produces a .xed result value and .xed output heap state. We refer to this \nproperty as determinism by default: nondeterministic input-output behavior may be introduced only by \nthe execution of an explicit nondeterministic construct. Implications for Programmers: The properties \ndiscussed above, and treated more formally in the next section, provide two key ben\u00ade.ts for programmers. \nFirst, concurrency errors such as data races or unintentional nondeterminism will be detected via compile-time \ntype checking; this bene.t existed in base DPJ for deterministic programs and has now been extended to \nnondeterministic ones. Second, once a program has been type checked, the above prop\u00aderties greatly simplify \nhow programmers can reason about the pos\u00adsible (nondeterministic) execution behaviors. With regard to \nthe second point, many programmers and testing tools analyze program behavior by reasoning about the \npossible in\u00adterleavings (or schedules) of parallel operations. The above proper\u00adties simplify this reasoning \nin several important ways (we focus on cobegin here without loss of generality; foreach is analogous): \n1. We only need to consider interleavings of isolated atomics, cobegins, and unguarded accesses, because \nof strong isolation and sequential equivalence of cobegin.The nd constructs do not constrain interleavings. \n 2. We can reason about the tasks of a cobegin sequentially: the .rst task can be fully evaluated without \nany intervening ac\u00adcesses from elsewhere, immediately followed by a complete evaluation of the second \ntask. 3. cobegin nd provides the only source of nondeterminism. Even within such a construct, the effect \nsystem guarantees that any block of code that is outside an atomic statement and does not execute any \natomic statement (call this an atomic-free section) cannot interfere with any concurrent task. Therefore, \nprogram\u00admers need not consider interactions between any atomic-free sections when reasoning about program \nbehavior.  Put together, these observations mean that the only source of multiple interleavings is from \ndifferent orderings of atomic sec\u00adtions, thereby signi.cantly reducing the number of interleavings that \nprogrammers must consider. Furthermore, programmers can control the granularity of the atomic sections \nto control the number of possible interleavings. The following example illustrates these observations \n(assume the S terms are all atomic-free statements). cobegin_nd { { S11; S12; atomic S13 } { S21; atomic \nS22; S23 } } Even if all the statements are primitive operations (reads or writes), if sequential consistency \nis not guaranteed, then up to 6! = 720 different interleavings are possible. If sequential consistency \nholds, then there are still up to 20 different interleavings. In our language, however, we may consider \nonly two sequentially consistent inter\u00adleavings: one with atomic S13 appearing before atomic S22, and \nvice versa. For example, any execution generated by our lan\u00adguage is equivalent to executing the entire \n.rst cobegin nd branch before the entire second branch, or vice versa.  3.4 Performance: Removing Unneeded \nBarriers We use a Software Transactional Memory (STM) runtime system to implement the atomic construct \nbecause STM provides weak atomicity, better composability than locks, and potentially better scalability \nbecause of optimistic rather than pessimistic synchro\u00adnization. One key drawback of STMs is the overhead \ndue to trans\u00adactional read and write barriers for every load or store to shared data (e.g., see [52]). \nThese barriers are snippets of code, often auto\u00admatically inserted by a transactions-aware compiler, \nthat invoke the STM runtime to implement some transactional concurrency control protocol. The barriers \ncan either read and write shared memory di\u00adrectly (so-called in-place update STM) and undo all transactional \noperations when a transaction aborts, or they can buffer updates into a private data structure (so-called \nwrite buffering STM) and ap\u00adply all the buffered changes into shared memory when a transaction successfully \ncommits. In both cases, barriers can incur signi.cant overhead and minimizing them is essential for performance. \nWe observe that we can use the region and effect system to remove unnecessary STM barriers, where there \nis no interference. However, the effect system as described so far does not carry enough information \nto perform this analysis locally. For example, suppose a method m reads a variable x inside an atomic \nsection. Then the read needs a barrier if and only if m is invoked in some context where there is interference \non x. There is no information in the method body that enables the compiler to make that judgment; interprocedural \nanalysis would be required. However, with a slight extension to the effect system, we can enable local \nreasoning about this kind of noninterference. Specif\u00adically, we have the effect system distinguish two \nkinds of regions: those that may interfere (and so need barriers everywhere) and those that cannot (and \nso do not need read barriers anywhere). We call the .rst kind atomic regions. The programmer can declare \na region to be atomic,suchas region Mutable on line 2 of Figure 2. The key bene.t is that, for a non-atomic \nregion, the compiler can remove read barriers entirely and, assuming STM using in\u00adplace updates, it can \nturn write barriers into logging-only barriers (synchronization is not needed, because there is no interference, \nbut transactions must still log the old value on writes, in case the trans\u00adaction aborts). This completely \neliminates the barrier overheads for read-only shared data. It also substantially reduces the barrier \nover\u00adheads for task-local data and noninterfering shared data. To enable sound reasoning about atomic \nregions and barrier elimination, we require some constraints on the use of these re\u00adgions. Any region \ndeclaration (.eld region, local region, or region parameter) may be declared to be atomic. We impose \nthe following requirements: When instantiating a type, an atomic (respectively, non-atomic) region parameter \nmay only be passed an atomic (respectively, non-atomic) region name as the argument. This is straightfor\u00adward \nto enforce using the region declarations.  A region that is involved in interfering effects must be \ndeclared atomic. This is enforced by the compiler as described below.  The barrier elimination also \nrequires a re.nement in the seman\u00adtics of atomic effects described in the previous section. An effect \nin an atomic statement is marked atomic only if it operates on an atomic region. For example, the read \nof region ReadOnly in Figure 4 (due to the operation tour.length()) does not gener\u00adate an atomic effect, \neven though it is inside the atomic block at line 4. The write to region Mutable does generate an atomic \nef\u00adfect. If region Mutable had not been declared atomic, the write to bestTour would generate a non-atomic \neffect. The compiler would then .ag the effect declaration at line 2 of Figure 4 as an Figure 5. Core \nlanguage syntax. C, ., f, m,and x are identi.ers. Programs P ::= R * C * e Classes Region Names C R ::= \n::= class C<.> { F * M * }region r Fields F ::= Tf in R Methods Regions Types M R T ::= ::= ::= Tm(Tx) \nE { e }r | . C<R> Effects Expressions Variables E e v ::= ::= ::= \u00d8| reads R | writes R | E . E this.f \n| this.f=e | e.m(e) | v | new T |seq(e,e) | cobegin(e,e) this | x  error because an atomic effect does \nnot cover a non-atomic effect, as noted earlier. We can now explain how the last rule above is enforced. \nIf a region is not marked atomic but has an effect that causes interfer\u00adence in some parallel construct, \nthe compiler will detect an error either at the parallel construct or at the method effect summary. For \nexample, if region Mutable were not marked atomic, the write to bestTour would generate a normal effect \nwrites Mutable. This would cause the effect summary at line 2 of Figure 4 to be .agged as an error, as \nnoted above. If the effect summary were changed to not mark the write effect atomic, then the call to \nsearchAllToursWithPrefix at line 21 of Figure 2 would gener\u00adate a nonatomic effect, and the compiler \nwould report the interfer\u00adence there.  4. Formal Semantics and Soundness To make precise the ideas discussed \nin the previous section, we have studied three variants of the same formal language, each one building \non the last: 1. The .rst variant, which we call the deterministic language,is a simple expression language \nwith regions, effects, and deter\u00administic parallel composition. It is a version of Core DPJ [17] simpli.ed \nto focus on the key elements for this work.  2. The second variant, which we call the deterministic-by-default \nlanguage, adds nondeterministic parallel composition, atomic expressions, and atomic effects to the deterministic \nlanguage.  3. The third variant, which we call the atomic regions language, adds atomic regions for \nremoving or simplifying transactional barriers.  Without loss of generality, we only include cobegin \nand cobegin nd in these simple languages; the treatment for foreach and foreach nd is similar. 4.1 Overview \nof Language Variants We .rst explain the syntactic structure of all three languages, and we summarize \nthe soundness guarantees that each one provides. In the following subsections, we explain the formal \nsemantics of each language variant, state the soundness guarantees more for\u00admally, and sketch how the \nguarantees follow from the semantic de.nitions. The full details, including all the semantics rules and \nproofs of all the claims, may be found in the lead author s Ph.D. thesis [19]. Deterministic Language: \nFigure 5 gives the syntax of the deter\u00administic language. A program P consists of zero or more region \ndeclarations, zero or more class de.nitions, and an expression to evaluate. A class C consists of a class \nname C, a region parameter ., zero or more .eld declarations, and zero or more method dec\u00adlarations. \nA .eld F speci.es a type, a .eld name, and a region. A method M consists of a return type, a method name, \na formal parameter type, a formal parameter, an effect, and an expression to   Effects E ::= ... | \natomic reads R | atomic writes R Expressions e ::= ... | cobegin nd(e,e) | atomic e Figure 6. Syntax \nof the deterministic-by-default language (ex\u00adtends Figure 5). evaluate. A region R is either a region \nname r or a region parameter .. A type T is a class instantiated with a region parameter, C<R>. An effect \nE is a possibly empty union of read effects and write effects on regions. For expressions e, we model \n.eld access, .eld assignment, method invocation, variables, new objects, sequential composition (seq), \nand deterministic parallel composition (cobegin). A vari\u00adable v is this or a method formal parameter \nx. The operational semantics of the .rst .ve expressions in Figure 5 is exactly as in Java. The last \ntwo expressions evaluate both component expres\u00adsions (either sequentially or in parallel) and return \nthe value of the second component as the value of the entire expression. The deterministic language provides \nthe following semantic guarantees, stated more formally as Theorems 1 2 in Section 4.2. They follow from \nthe fact that the executions of the two branches of any cobegin expression are required to be noninterfering: \n1. Equivalence of cobegin and seq: In terms of the .nal result (.nal value produced and .nal heap state), \nthere is no differ\u00ad '' ence between executing cobegin(e,e) and seq(e,e).As a consequence, the entire \nprogram is guaranteed to behave like a sequential program (the one that results by replacing cobegin \neverywhere with seq). 2. Determinism: If an expression e evaluates to completion, then the value it produces \nis deterministic. Moreover, if e is evalu\u00adated in a sequential context (i.e., not inside a cobegin), \nthen the .nal heap state is deterministic. In particular, the .nal heap produced by a terminating execution \nof the whole program is deterministic. Deterministic-by-Default Language: Figure 6 shows the addi\u00adtional \nsyntax for the deterministic-by-default language. We extend the syntax of effects to record atomic effects. \nWe also add (1) cobegin nd, which is the same as cobegin, except that it allows interference guarded \nby atomic expressions; and (2) expressions atomic e, which signal that expression e should be executed \nin isolation: that is, as if it were executed all at once, with no inter\u00adleavings from the rest of the \nexecution. The deterministic-by-default language provides the following semantic guarantees, stated more \nformally as Theorems 3 6 in Section 4.3: 1. Race freedom and sequential consistency: Program execution \ncontains no data race. This result follows because the effect sys\u00adtem requires that all parallel interference \noccur between pairs of accesses guarded by atomic expressions. Further, in the Java memory model, race \nfreedom implies sequential consistency, i.e., one can reason about execution as a program-ordered in\u00adterleaving \nof memory operations. 2. Strong isolation: For the same reason that the program is race free, expressions \natomic e execute e in isolation, even if the un\u00adderlying implementation guarantees only weak isolation.More\u00adover, \nthe effect system disallows any interference between the cobegin and concurrent operations that would \nviolate isolation of the cobegin. Therefore, every cobegin expression executes in isolation. Together, \nrace freedom and strong isolation imply that execution is a sequentially consistent interleaving of iso\u00adlated \nexpressions.  Regions R ::= ... | atomic region r * Classes C ::= ... | class C<atomic .> { FM * } \n Figure 7. Syntax of the atomic regions language (extends Fig\u00adure 6). ' 3. Equivalence of cobegin and \nseq: Because cobegin(e,e) executes in isolation, it is equivalent to an isolated execution ' of seq, \ni.e., atomic seq(e,e). As discussed in Section 3.3, for the deterministic-by-default language, we make \ncobegin behave like atomic seq, and not just seq, to guarantee that cobegin executes deterministically, \neven inside a cobegin nd. 4. Determinism by default: Both atomic and cobegin expressions execute deterministically \nin the same sense as discussed for the deterministic language, even inside a cobegin nd, unless they \ncontain a dynamic instance of cobegin nd. Atomic Regions Language: The third variant of the formal lan\u00adguage \nallows some regions to be marked atomic,and only opera\u00adtions on those regions generate atomic effects. \nOperations on non\u00adatomic regions never generate atomic effects, even in an atomic expression. Figure \n7 shows the new syntax. The execution semantics of this language variant is identical to that of the \ndeterministic-by-default language, except that the com\u00adpiler can distinguish, and potentially optimize, \noperations within an atomic expression that never interfere with concurrent tasks. In Section 5, we discuss \na prototype compiler that uses these rules to optimize our STM by omitting or simplifying barriers (inside \nan atomic expression) for such noninterfering operations. 4.2 Deterministic Language Static Semantics: \nThe typing is done with respect to an environ\u00adment G, which consists of elements (v, T ) stating that \nvariable v has type T . The key rule for subeffects (i.e., when one effect con\u00adservatively summarizes \nanother, written G f E . E'), is that a write effect on region r covers a read effect on the same region \nr: SE-READS-WRITES G f reads R . writes R The rules for typing programs, classes, .elds, etc., are straightfor\u00adward. \nThe rule for typing methods enforces effect subsumption:that is, that a method s actual effect must be \na subeffect of its declared effect: METHOD G f Tr G f Tx G f E G . (x, Tx) f e : Tr,E' G f E' . E G f \nTr m(Tx x) E { e } The key rules for noninterfering effects (i.e., effects that may safely go in parallel, \nwith deterministic composition, written G f E # E') are that reads never interfere with reads, and writes \nnever interfere with reads or writes to different regions: NI-READS NI-WRITES ' r = r '' G f reads r \n# reads rG f writes r # writes r NI-READS-WRITES ' r = r ' G f reads r # writes r As in Core DPJ [17, \n19], every expression has a type and an effect. The rules for typing expressions e with type T and effect \nE (G f e : T,E) are straightforward. The most important rule says that in parallel composition, the effects \nof the expressions being evaluated in parallel must be noninterfering: ' COBEGIN G f e : T,E G f e: T \n',E' G f E # E' ' G f cobegin(e,e) : T ',E . E'   Dynamic Semantics: We give a small-step operational \nsemantics describing the recursive reduction of expressions. Execution State: The execution state is \n(e, H), consisting of an expression to evaluate and a heap. A heap H is a partial function from object \nreferences to pairs (O, T ),where O is an object, and T is the type of O. An object O is a mapping from \n.eld names f to object references o. null is a special reference that is in Dom(H) but does not map to \nan object. Attempting to invoke a method of null causes the execution to fail. We extend the static syntax \nof expressions to represent computations: e ::= ... | o | (e, S,E) | ei The additional expressions have \nthe following meanings: object references o are the values produced by reducing expressions; a local \nexecution state (e, S,E) records an expression e to evaluate, an environment S containing the bindings \nfor the free variables in e, and an effect E of reducing e; and the indices i enable us to say unambiguously \nwhich expression is reduced in a given execution step, as explained below. A program execution is a sequence \nof steps * ((eP , \u00d8, \u00d8)i, null) . (ei,H), for some i, e,and H,where eP is the main program expression, \ni is an arbitrary index denoting the top-level expression in the reduction, ei is the evolution of expression \n(eP, \u00d8, \u00d8)i,and H is the evolved heap (represented as a domain containing null plus all object references \no added during the execution). A terminating execution has ei =(o, \u00d8,E)i,where o is the answer computed \nby the program, and E is the union of all effects on H done in the execution. Expression Semantics: Field \naccess and assignment work in the standard way, except that we track dynamic effects, to state and prove \nthe soundness results. As an example of the effect tracking, we give the rule for .eld access: DYN-FIELD-ACCESS \n(this,o) . S H(o)=(O, C<r>) F (C, f)= Tf in R ((this.f, S, \u00d8),H) . ((O(f), S, reads sC<r> (R)),H) The \nfunction sC<r> substitutes the region argument r for the pa\u00adrameter . of class C. The rule for .eld assignment \nis similar, except that the right-hand-side subexpression is evaluated .rst, the heap is updated, and \nthe effect is a write instead of a read. For evaluation of subexpressions, we use the following standard \nrule: DYN-SUBEXP ' (e, H) . (e ,H ' ) '' ''' (e ,H) . (e [ei . e ],H ' ) i It says that if we can reduce \nexpression e to e ' starting with heap H,and e appears with index i as a subexpression of e '' , then \nwe can reduce e '' by rewriting the subexpression ei in place and updating the heap. The rules for cobegin \nillustrate subexpression evaluation: DYN-COBEGIN-EVAL fresh(i) fresh(j) '' ((cobegin(e,e ), S, \u00d8),H) \n. (cobegin((e, S, \u00d8)i,(e, S, \u00d8)j ),H) DYN-COBEGIN-ACCUMULATE '' (cobegin((o, S,E)i,(o, S,E ' )j ),H) \n. ((o, S,E . E ' ),H) DYN-COBEGIN-EVAL creates two new subexpressions with fresh indices i and j for \nevaluation in environment S. The evaluation steps of ei and ej can be arbitrarily interleaved, via rule \nDYN-SUBEXP. This interleaving models the parallelism. When both are done, DYN-COBEGIN-ACCUMULATE accumulates \nthe results into the top-level expression. Field assignment, method invocation, and sequential composition \nare similar, except that the subexpressions are evaluated in a .xed sequential order (so there is no \nparallelism, except inside a cobegin). The rules are entirely standard, and are stated in full in the \nthesis. Soundness Results: As summarized in Section 4.1, there are two main soundness results for the \ncore deterministic language. To state the results, we call out the reduction of a particular expression \ninside the evolution of the whole program. We write '' ((e, S, \u00d8)i,H) .P (e ,H ) i to denote a reduction \nof expression i occurring in program execu\u00adtion. That means P is well-typed with main expression eP ,and \nthere is a program execution *'' *''' ' ((eP , \u00d8, \u00d8)j , null) . (e ,H) . (e ,H ), jj such that e '' \nj contains (e, S, \u00d8)i, which is the .rst appearance of ''' ''' expression i in the execution; and ej \ncontains ei. (ej ,H) is called the initial state of the reduction, and (ej' ,H ' ) is called the .nal \nstate. Our .rst result states that there is no semantic difference in this language between seq and cobegin: \nat any point in the execution that is an initial state for a cobegin reduction, we can replace cobegin \nwith seq and get exactly the same results. The proof follows directly from the noninterference property \nguaranteed by the static and dynamic semantics. Theorem 1 (Equivalence of cobegin and seq). '' ((cobegin(e, \ne ), S, \u00d8)i,H) .P ((o, S,E)i,H ) if and only if '' ((seq(e, e ), S, \u00d8)i,H) .P ((o, S,E)i,H ) with the \nsame initial state, except that expression i is as shown. The second result states that expression evaluation \nis input\u00adoutput deterministic, up to the choice of object reference names. The proof follows from the \nfact that once cobegin is replaced by seq everywhere according to Theorem 1, the only nondeterminism \nleft in the rules is the choice of reference names and expression indices. Theorem 2 (Input-Output Determinism). \nIf ((e, S, \u00d8)j ,H) rP ' ' ''' ((o, S,E)j ,H ) and ((e, S, \u00d8)j ,H) rP ((o, S,E )j ,H ) with the same \ninitial state, then o ~' ,where ~ = o = denotes equivalence up to renaming object references, and E = \nE ' . Moreover, if (e, S, \u00d8)j is not a subexpression of any cobegin expression, then H ~' . = H  4.3 \nDeterministic-by-Default Language Static Semantics: Figure 8 gives the static semantics of atomic effects. \nRule SE-ATOMIC-1 formally expresses the idea that non\u00adatomic effects cover atomic effects: that is, if \nE occurred in an atomic expression, then we can summarize the effect as either atomic E or E. Note that \nthe converse is not true, because we cannot soundly report an atomic effect where there is no atomic \nexpression. Rule SE-ATOMIC-2 provides the subeffect relation for two atomic effects. Rule NI-ATOMIC says \nthat an atomic effect is noninterfering if the underlying effect is. The judgment G f nondet(E, E ' ) \nstates that it is safe to run expressions with effects E and E ' nondeterministically in parallel, inside \na cobegin nd. Figure 8 gives the key rules for making the judgment. Figure 9 gives the rules for typing \nnondeterministic parallel composition and atomic expressions. Rule COBEGIN ND is simi\u00adlar to COBEGIN, \nexcept that interfering effects are allowed if they SE-ATOMIC-1 SE-ATOMIC-2 G f E . E ' G f E . E ' \nG f E . E ' G f atomic E . E ' G f atomic E . atomic E ' G f E # E ' NI-ATOMIC G f E # E ' G f atomic \nE # E ' ' G f nondet(E, E ) NONDET-NI G f E # E ' G f nondet(E, E ' ) NONDET-ATOMIC G f nondet(atomic \nE, atomic E ' ) Figure 8. Static semantics of atomic effects (selected rules). COBEGIN ND '' G f e : \nT, E G f e : T,E G f e : T ,E ' G f nondet(E, E ' ) '' G f cobegin nd(e,e ) : T ,E . E ' ATOMIC G f e \n: T,E G f atomic(E)= E ' G f atomic e : T,E ' '' COBEGIN G f e : T,E G f e : T ,E ' G f E # E ' '' G \nf cobegin(e,e ) : T, nonatomic(E . E ' , G) ATOMIC-READS ' G f atomic(E)= E G f atomic(reads R)= atomic \nreads R NONATOMIC-ATOMIC nonatomic(E, G) G f nonatomic(atomic E)= E Figure 9. Static semantics of cobegin \nnd and atomic expressions (selected rules). The judgment G f atomic(E)= E ' says that E ' is the effect \nobtained after adding atomic from all reads and writes in E;and G f nonatomic(E) is the reverse. are \nboth guarded by atomic expressions. Rule ATOMIC collects the effect E of the expression e, then marks \nall the constituent read and write effects atomic, to re.ect the fact that E is occurring in an atomic \nexpression. Finally, rule COBEGIN has changed. In addition to checking noninterference, as in the basic \nlanguage, the new rule converts all atomic effects occurring inside the cobegin to ordi\u00adnary effects \nusing the judgment G f nonatomic(E)= E ' .This ensures that no atomic effects are ever propagated outward \nfrom inside a cobegin. The last rule is key to ensuring that cobegin executes in isolation, as discussed \nin Section 3.3. Dynamic Semantics: We describe the dynamic semantics of the nondeterministic language \nin two parts, the .rst operational and the second non-operational. The .rst, operational, part is just \nthe same semantics as for the basic language (Section 4.2), with a few minor adjustments to accommodate \nthe new features. The second, non\u00adoperational part, describes a weak isolation constraint on execution \nhistories generated by the operational part. The overall dynamic semantics comprises all execution histories \ndescribed by the oper\u00adational semantics that also satisfy weak isolation. In practice, weak isolation \nwould be enforced by a runtime implementation (such as software transactional memory [37]). Operational \nSemantics of Expressions: The operational semantics is identical to the one described in Section 4.2, \nwith three changes. First, we add a rule to execute cobegin nd; it is identical to the rule for cobegin \nshown in Section 4.2 (i.e., in the operational semantics, there is no difference between executing cobegin \nnd and cobegin the difference is all in the static semantics).   Second, we add a rule for executing \nan expression atomic e. We execute e, then mark all its effects atomic, for purposes of effect tracking: \nDYN-ATOMIC-EVAL fresh(i) ((atomic e, S, \u00d8),H) . (atomic (e, S, \u00d8)i,H) DYN-ATOMIC-MARK-EFFECTS \u00d8f atomic(E)= \nE ' (atomic (o, S,E)i,H) . ((o, S,E ' ),H) Finally, we modify the rule DYN-COBEGIN-ACCUMULATE (shown \nin Section 4.2) to mark the effects of a cobegin expression non-atomic: DYN-COBEGIN-ACCUMULATE )= E '' \n'' \u00d8f nonatomic(E . E ' , S,E '' (cobegin((o, S,E)i,(o, S,E ' )j ),H) . ((o ),H) Weak Isolation Constraint: \nTo state the weak isolation constraint, we need the concept of a reduction history H, which is a se\u00adquence \nof program execution steps witnessing ((e, S, \u00d8)i,H) rP (ei' ,H).If ((e, S, \u00d8)i),H) is the initial program \nstate, then we call H a program execution history and write HP . Two histories occur in parallel under \ncobegin (or cobegin nd) if they each occur in reducing different branches of the same cobegin (or cobegin \nnd) expression, in the same program execution. De.nition 1 (Con.ict relation on atomic expressions). \nFix a pro\u00adgram execution history HP , and let I be the set of expression in\u00addices appearing in HP that \nlabel atomic expressions (i.e., expres\u00adsions introduced by rule DYN-ATOMIC-EVAL). The con.ict rela\u00adtion \non atomic expressions in HP is the transitive closure of the following relation on I \u00d7 I: (i, j) is in \nthe relation if i .j, and = there are con.icting memory accesses ai and aj such that (a) ai occurs in \nthe reduction of an atomic expression ei;(b) aj occurs in the reduction of an atomic expression ej; (c) \nthe reductions of ei and ej occur in parallel under cobegin nd, and (d) ai precedes aj in HP . Notice \nthat we put the relation only on operations under cobegin nd, not under cobegin;and we do not include \nany con\u00ad.icts occurring outside of atomic expressions. That is because the type system will ensure there \nare no con.icts between cobegin tasks or outside of atomic expressions; this is the soundness result \nthat we state below. Now we can de.ne the weak isolation constraint on executions in the language. For \nthe remainder of this section, we assume an implementation that guarantees weakly isolated program execution \nhistories HP . De.nition 2 (Weakly isolated histories). Let H be a history. If the con.ict relation on \natomic expressions in H is a partial order, then we say that H is weakly isolated. Soundness Results: \nAs summarized in Section 4.1, there are three main soundness results for the nondeterministic language: \nrace freedom, strong isolation, and determinism by default. Race Freedom: The .rst result says that the \nlanguage is race free, assuming that pairs of memory accesses in different atomic expres\u00adsions are well-synchronized \n(which is true of any transactional im\u00adplementation). The proof follows from the fact that the type system \ndisallows parallel interference, except for pairs of accesses both oc\u00adcurring in atomic expressions. \nTheorem 3 (Race freedom). If fP, then a history HP that has synchronization orderings consistent with \nthe con.ict relation stated in De.nition 1, contains no data race. Strong Isolation: To state the strong \nisolation result formally, we use the well-known concept of serializable histories [44]. We say that \na history H witnessing ((e, S, \u00d8)i,H) rP (ei' ,H ' ) is serial with respect to expression i if every \nstep in the history transforms expression i or a subexpression of expression i. We say that H is serializable \nwith respect to expression i if it is possible to generate a history H ' with the same initial and .nal \nstates as H that contains '' '''' a serial history witnessing ((e, S, \u00d8)i,H ) rP (ei,H ),for some heaps \nH '' and H ''' . In other words, an expression reduction is serializable if we could have done it serially, \nwith the same results. The following theorem says that a history is serializable if it does not occur \nin a cobegin nd; or it does not reduce any atomic expression; or it reduces a cobegin or atomic expression. \nThe proof follows from the type system s guarantees of noninterference for cobegin tasks, and noninterference \nfor cobegin nd tasks ex\u00adcept where guarded by atomic expressions; together with the weak isolation assumption \nfor atomic expressions. Theorem 4 (Strong isolation). Suppose fP,let HP be a weakly isolated history \nexecuting P, and let H be a history witnessing ((e, S, \u00d8)i,H) rP (ei' ,H ' ) contained in HP .Then H \nis serializ\u00adable with respect to expression i if (1) (e, S, \u00d8)i is not a subexpres\u00adsion of any cobegin \nnd expression; or (2) no atomic expression appears in H;or(3) e is a cobegin or atomic expression. Determinism \nby Default: Finally, we show that the nondeterminis\u00adtic language is deterministic by default. First we \nshow the equiv\u00adalent of Theorem 1 for the nondeterministic language: there is no difference between cobegin \ne and atomic seq e. Note that we need the atomic here because in general a seq occurring under a cobegin \nnd can interfere with the other branch of the cobegin nd. The result follows directly from Theorem 1 \nand The\u00adorem 4. ( Initial state is de.ned before Theorem 1.) Theorem 5 (Semantic equivalence of cobegin \nand atomic seq). '' ((cobegin(e, e ), S, \u00d8)i,H) P ((o, S,E)i,H ) if and only if '' ((atomic seq(e, e \n), S, \u00d8)i,H) P ((o, S,E)i,H ) with the same initial state, except that expression i is as shown. Second, \nwe show the equivalent of Theorem 2 for the nonde\u00adterministic language: for the expressions called out \nby Theorem 4 as having serializable reductions, the execution of such expressions is also input-output \ndeterministic, unless there is explicit nonde\u00adterminism via cobegin nd. The proof follows from Theorem \n2, together with the de.nition of a serializable history. Theorem 6 (Determinism by default). Suppose \nfP, and let H be a history witnessing ((e, S, \u00d8)i,H) rP ((o, S,E)i,H ' ) that is serializable with respect \nto expression i, where no cobegin nd expression ever appears in expression i.If ((e, S, \u00d8)i,H) rP ' ''' \n~ ((o, S,E )i,H ) with the same initial state as in H,then o = o, where ~denotes equivalence up to renaming \nobject references, = and E = E ' . Moreover, if (e, S, \u00d8)i is not a subexpression of any ' ~'' cobegin \nor cobegin nd expression, then H = H .  4.4 Atomic Regions Language Static Semantics: First, the rules \nfor constructing types require that atomic regions bind only to region parameters declared atomic, and \nnon-atomic regions bind only to region parameters not declared atomic. This requirement ensures that \nmemory regions are treated consistently across method invocation. Second, we re.ne the judg\u00adment G f \natomic(E)= E ' so that an atomic expression makes an effect atomic only if the effect is on an atomic \nregion. This rule ensures that, in applying rule COBEGIN ND to check a cobegin nd expression, the compiler \nwill never allow effects on non-atomic re\u00adgions to interfere. The rules are stated in full in the thesis. \nDynamic Semantics and Soundness: The dynamic semantics of this language is exactly as given in the previous \nsection, with two changes. First, marking effects in rule DYN-ATOMIC-MARK-EFFECTS happens according to \nthe re.ned de.nition of \u00d8f atomic(E)= E ' : that is, effects are marked atomic only if they operate on \natomic regions. Second, we rede.ne the con.ict relation on atomic expressions (De.nition 1) so that only \ncon.icts involving accesses to statically identi.able atomic regions (i.e., region names r or region \nparameters . marked atomic) are syn\u00adchronized by the implementation. The soundness result says that Theorems \n3 6 hold for this language variant; the proofs, given in the thesis, are entirely straightforward.  \n 5. Prototype Implementation To implement our new mechanisms, we extended the compiler used for basic \nDPJ. It is a modi.ed version of Sun s javac, which trans\u00adlates DPJ code to standard Java. The compiler \nimplements DPJ s parallelism constructs by generating calls to the ForkJoinTask library [2], which schedules \ntasks onto a pool of worker threads; further details are available in [17]. In this work, we extended \nthe compiler to implement atomic blocks using the Deuce STM library [3]. We used the well\u00adrespected Transactional \nLocking II (TL2) algorithm [28]. TL2 is a write-buffering (i.e., lazy versioning) algorithm with optimistic \nreads. Deuce supports concurrency control at the object .eld level, and uses a light-weight, custom re.ection \nmechanism to access object .elds inside transactions. We selected this STM system for pragmatic reasons \nof ease of implementation, and because it implements a well-known high\u00adperformance STM algorithm. We \nhave not attempted to maximize absolute performance in our implementation; it could be improved signi.cantly \nby using a different STM system, such as one inte\u00adgrated with the JVM [46]. Our method is applicable \nto other types of STM systems and algorithms (including those utilizing in-place updates). For each atomic \nblock, the compiler generates code to execute the body of the atomic block as a transaction, retrying \nuntil the transaction commits successfully. Nested atomic blocks are .at\u00adtened. Methods that are transitively \ncallable within atomic blocks are cloned; versions containing barriers are used when they are called \nwithin atomic blocks. Within atomic blocks, the compiler in\u00adserts normal read and write barriers for \naccesses to .elds in atomic regions. As discussed in section 3.4, the compiler omits barriers for read \naccesses to non-atomic regions, and it generates logging-only barriers for write accesses. We modi.ed \nthe TL2 implementation in Deuce to support these optimized logging-only barriers. However, because TL2 \nis a write\u00adbuffering algorithm, we would have to use read-barriers to obtain correct values in the read-after-write \ncases. To avoid read barriers entirely, we modi.ed the algorithm to perform in-place updates for these \nlocations and we maintain a separate undo log to revert effects of such updates in case the transaction \naborts. Reads to such locations do not need barriers because they can now obtain their values directly \nfrom the original memory location. 6. Evaluation The ideas presented in this paper raise four key questions \nfor exper\u00adimental investigation: (1) Can the language express nondeterminis\u00adtic algorithms in a natural \nway? (2) Can the algorithms expressed in the language give good performance? (3) How effective is the \noptimization of STM barriers? (4) What is the annotation overhead of the language? We used four nondeterministic \nalgorithms to evaluate these questions: two different versions of TSP, Delaunay mesh triangu\u00adlation from \nthe Lonestar Benchmarks [1], and OO7, a synthetic database benchmark that has been used in previous studies \nof paral\u00adlel performance [47, 51]. These codes are discussed further below. 6.1 Benchmarks and Expressing \nParallelism Traveling Salesman Problem: We studied two versions of the TSP algorithm, which we call TSP-PQ \nand TSP-R. TSP-PQ is the algorithm described in Section 3. As discussed there, the algorithm proceeds \nin two phases: the .rst phase breaks the problem up into subproblems and adds them to a priority queue, \nand the second phase concurrently removes items from the queue and processes each one using sequential \nrecursive search. The priority queue orders the work, so that more promising subtrees are explored .rst. \nTSP-R is a variant that eliminates the priority queue and uses recursion to express the entire algorithm. \nAt each level of the tree, the algorithm computes a bound for each subtree and compares the bound against \nthe global current best tour. Bounds that are de.nitely no better than the current best are excluded, \nwhile bounds that may be better are explored recursively. The recursion occurs in parallel until a speci.ed \ndepth of the tree; in our studies we used a depth varying with the log of the number of threads. TSP-R \nis a simpler algorithm than TSP-PQ, but it potentially suffers from more contention, as the global best \ntour must be read before every recursive descent into a subtree to avoid exploring too many bad paths. \nBy contrast, because TSP-PQ uses a priority queue to order the paths, it can read the global best tour \nless often (once per tree level). We adapted both versions of TSP from code that was used in previous \nstudies of STM performance [46, 47]. Our TSP-PQ code uses the identical algorithm to the original code, \nand expresses the parallelism in the same way. The original code had a data race, and we added one extra \natomic block to eliminate that race. Our TSP-R code is a transformation of TSP-PQ that eliminates the \npriority queue, checks the bound at each level of the tree, and parallelizes the recursion. Delaunay \nMesh Re.nement: This code uses Chew s algorithm [26, 36] to .nd and eliminate bad triangles, i.e., those \nthat do not sat\u00adisfy some quality constraint from a Delaunay triangulation of a mesh of points. The program \nis nondeterministic since different orders of processing of bad elements lead to different meshes, al\u00adthough \nall such meshes satisfy the quality constraints [26]. The program uses a foreach nd loop, and each iteration \nof the loop spawns a new worker thread (at most one per core). Each worker thread has a private worklist \nof bad triangles. In each iteration of the worklist loop, the worker selects one bad triangle from the \nalso written TSP-R with a similar race. The four codes do not use any deterministic algorithms but such \nalgorithms do not incur any runtime performance overheads in our language; such overheads are dominated \nby that of atomic sections in nondeterministic com\u00adponents. The performance and expressivity of the language \nfor de\u00adterministic algorithms were studied previously [17].  6.2 Performance To evaluate performance, \nwe measured the self-relative speedup (i.e., the speedup compared to running the transactional code on \none thread) achieved by the three codes. We focused on self-relative speedup rather than absolute speedup \nbecause (a) optimizing the code generation for atomic statements has not been a focus of this paper, \nand (b) the Deuce STM, although using a good algorithm, lacks many many essential performance features \nof a high perfor\u00admance Java STM [46]. Self-relative speedups have the effect of factoring out some of \nthe performance impact of the STM im\u00adplementation while capturing the scalability of the benchmarks. \nWe ran and measured the codes on a 24-core system using four Intel Xeon E7450 processors (each with six \ncores), running Win\u00addows Server 2008. Figure 10 shows the self-relative speedups with barrier optimizations \nenabled, using running times for Delaunay and TSP, and throughput scaling for OO7. Because the runtimes \nare nondeterministic, we averaged 5 10 runs for each data point, using an interquartile method to exclude \na few extreme outliers. For both TSP variants, we used the one-thread version of TSP-PQ, which was the \nfaster of the two, as the baseline. Both versions of TSP show good scaling, and OO7 shows moderately \ngood scaling, throughout the range of numbers of threads t we examined. TSP-R shows better (superlinear) \nspeedup for smaller t; this is because the parallel algorithm is very ef.cient in that range: it rules \nout subtrees quickly, and so visits only about 1/4 of the tree nodes at t =2 com\u00adpared to t =1. However, \nthe scaling curve for TSP-R .attens out as t increases, most likely due to higher contention than TSP-PQ. \nThe speedup curve for Delaunay is poor: it .attens out and reaches only 3x on 22 threads. We pro.led \nthe code to under\u00adstand the source of this behavior and traced it to the method System.identityHashcode() \nin the JVM. This standard Java function is extensively used in Deuce to index into lock tables. We observed \nthat the time spent in this function grows with the number of threads. In Delaunay, which has large transactions, \nthis overhead negatively affected the speedup curve. This problem can by solved by modifying the JVM, \nbut we leave that (and other optimizations for atomic) to future work. 20 work list, forms a cavity around \nit, re-triangulates the cavity, and adds any new bad triangles back to the worklist. Cavity .nding and \n15 re-triangulating code sections access the shared mesh data structure and are enclosed in atomic blocks. \nOO7: OO7 simulates a number of clients, each performing a .xed number of queries on an in-memory database. \nEach query is en\u00adclosed in an atomic block. The performance metric is the through- Speedup 10 5 put \n(queries per unit time), and we measure how this scales by varying the number of clients while keeping \nthe number of queries performed by each one constant. The program uses a foreach nd loop, with one iteration \ncorresponding to each client. We con.g\u00adured it to use a number of clients equal to the number of worker \nthreads, so there is always one thread per client. Thus, the total amount of work performed is proportional \nto the number of threads. Expressing Parallelism: We successfully expressed all the paral\u00adlelism that \ndid not use data races, in these four nondeterministic algorithms. As discussed above, we eliminated \na race in TSP-PQ that was presumably there to avoid synchronization; we could have 0 0 4 8 121620 Number \nof worker threads Figure 10. Self-relative speedups. For OO7, we scaled the amount of work with the \nnumber of worker threads, and measured speedup based on throughput scaling (number of queries done per \nunit time). The barrier optimization was enabled for all of these benchmarks.  6.3 Impact of Barrier \nElimination We compared the performance of two versions of the parallel code for each benchmark: with \nand without the barrier simpli.cation op\u00adtimization for non-atomic regions. Figure 11 shows the improve\u00adment \nin running time for the optimized code compared to the un\u00adoptimized code. Figure 12 shows the reduction \nin the number of dynamically-executed barriers due to our optimizations. 1.2 11   Delaunay OO7  threads \nopt unopt opt unopt    2 0.999 0.944 0.944 0.932 3 0.975 0.848 0.877 0.872 4 0.998 0.810 0.822 0.560 \n7 0.993 0.647 0.700 0.210 12 0.996 0.405 0.539 0.100 17 0.995 0.291 0.442 0.071 22 0.994 0.244 0.369 \n0.071 Table 1. Ratio of committed transactions to started transactions for Delaunay and OO7. Lower numbers \nindicate more aborted transactions. For both versions of TSP, all numbers are 1.000. Opt/unopt time 0.8 \n0.6 0.4 0.2 0 TSP-R Delaunay OO7 1994 302 (15.1%) 3(1) 374(3) 21(7) 165/216 Delaunay Total 4197 TSP-PQ \n2 3 4 7 12 Total Annotated Region Effect 17  SLOC SLOC Decls RPLs Params Summ. Program 22 TSP-PQ 433 \n77 (17.8%) 2(1) 101(4) 6(2) 14/20 TSP-R 200 34 (17%) 2(1) 42(4) 2(0) 6/12 1570 105 (6.7%) 4(1) 76(7) \n6(0) 52/104OO7 518 (12.3%) 11(4) 593(18) 35(9) 237/352 Table 2. Annotation counts for the four benchmarks. \nIn the middle Figure 11. Ratio of optimized runtimes (with barrier elimination) to unoptimized runtimes \n(without barrier elimination). A value lower than 1 means the optimization increased performance. columns, \nthe numbers in parentheses represent the number of an\u00adnotations marked atomic. In the last column, x/y \nmeans of y total method de.nitions in the program, x were annotated with effect summaries. Percent of \ntotal barriers 100% 75% 25% 50%  Remaining Simpli.ed Eliminated 0% 1 7 12 22 1 7 12 22 1 7 12 22 1 \n7 12 22 TSP-PQ TSP-R Delaunay OO7 Figure 12. Reduction in barriers due to optimizations, showing the \nproportion of barriers from the unoptimized version that are  6.4 Annotation Overhead Table 2 provides \na quantitative measure of the annotation overhead of writing the four benchmarks in our language. Column \n1 after the vertical bar shows the total number of non-blank, non-comment lines of source code, counted \nby sloccount. Column 2 gives the count of annotated lines, as an absolute number and as a percentage \nof the total lines. The following three columns show the number of region declarations, RPLs (including \narguments to in, arguments to types and methods, and arguments to effect summaries), and region parameters. \nThe number of annotations marked atomic is shown in parentheses after the main number. The last column \nshows the eliminated entirely, simpli.ed to log-only write barriers, or that number of effect summaries \nbefore the slash, and the number of remain as full barriers in the optimized version, for each of the \nmethod de.nitions after the slash. three benchmarks with 1, 7, 12, and 22 worker threads. The optimization \nhas a substantial impact on performance for three of the four benchmarks (TSP-PQ, Delaunay, and OO7). \nThe performance improvements correlate well with the barrier reduc\u00adtions. The optimizations give essentially \nno improvement for TSP-R, because the transactions are very short (reads and read-modify\u00adwrite operations \non the best tour). As a result (1) there are few if any barriers to remove; and (2) transactional overhead \nis not a sig\u00adni.cant component of the overall runtime. On the other hand, TSP-PQ, OO7, and Delaunay use \nlonger transactions, providing more opportunities for reducing overhead. Our optimizations can eliminate \nbarriers both by actually re\u00admoving barrier operations on certain statements and also by reduc\u00ading the \nnumber of times that transactions must be retried. The latter effect occurs because removing unnecessary \nbarriers reduces the number of false con.icts incurred by the STM system. As shown in Table 1, this effect \nis more pronounced with larger numbers of worker threads, so our optimizations not only reduce scalar \nover\u00adheads but also improve scalability. For example, in Delaunay, the optimization changed this ratio \nfrom 0.944 to 0.999 on 2 threads but from 0.244 to 0.944 on 22 threads. While the average number of \nannotated lines (12.3%) is nontriv\u00adial, we believe it is not unduly high, given the strong safety proper\u00adties \nof the programming model. As in our prior work [17], most of the RPL annotations were arguments to types. \nThe overhead could be reduced by inferring some of the annotations [48], but we leave that for future \nwork. Our approach does impose the limitation that if a programmer wishes to use a class region parameter \nas an atomic region in some context and a non-atomic region in some other context, then the class must \nbe cloned: the programmer must create two copies of the class, one with the atomic parameter and one \nwith the non-atomic parameter. The cloning is required because different barriers must be generated for \nmethods of the class that operate transactionally on the parameter, depending on whether the region bound \nto the parameter is atomic. The cloning could be done automatically by the compiler, similarly to what \nC++ does for templates. While we have not implemented this approach, we believe it does not raise any \nsigni.cant technical issues. In the benchmarks we studied, only Delaunay required class cloning. In Delaunay, \nwe needed both atomic and non-atomic ver\u00adsions of the list and map structures used in the benchmark. \n  7. Related Work Type and Effect Systems: Several researchers have described ef\u00adfect systems for enforcing \na locking discipline in nondeterministic programs that prevents data races and deadlocks [5, 20, 34] \nor guar\u00adantees isolation for critical sections [29]. Matsakis et al. [41] have recently proposed a type \nsystem that guarantees race-freedom for locks and other synchronization constructs using a construct \ncalled an interval for expressing parallelism. While there is some over\u00adlap with our work in the guarantees \nprovided (race freedom, dead\u00adlock freedom, and isolation), the mechanisms are very different (ex\u00adplicit \nsynchronization vs. atomic statements supported by STM). Further, these systems do not provide determinism \nby default. Fi\u00adnally, there is no other effect system we know of that provides both race freedom and \nstrong isolation together. STM Correctness (Language): STM Haskell [31] provides an isolation guarantee, \nbut for a pure functional language that uses monads to limit effects to the transactional store, unlike \nour imperative shared-memory language. Moore and Grossman [42] and Abadi et al. [6] use types and effects \nto guarantee strong isolation for an imperative language, but their languages permit races where neither \naccess occurs in a transaction. Finally, none of these languages allows both transactional and non-transactional \neffects to the same memory, as our language does. Beckman et al. [12] show how to use a form of alias \ncon\u00adtrol called access permissions [21] to verify that the placement of atomic blocks in a threaded program \nrespects the invariants of a speci.cation written by the programmer for example, that a con\u00addition is \nchecked and acted upon atomically. This approach is com\u00adplementary to ours: we provide guarantees of \nrace freedom, strong isolation, and determinism by default for all programs in our lan\u00adguage; on top \nof that one could check that additional programmer\u00adspeci.ed invariants are satis.ed. STM Correctness \n(Compiler and Runtime): Several STMs guarantee strong isolation by preventing interference between transactions \nand non-transactional accesses at runtime. Most of these systems use a combination of sophisticated static \nwhole\u00adprogram analysis, runtime optimizations, and other runtime tech\u00adniques like page protection to \noptimize strong isolation [7, 22, 46, 47]. While these techniques can signi.cantly reduce the cost of \nstrong isolation, they cannot completely eliminate it. In contrast, our language-based approach provides \nstrong isolation without im\u00adposing extra runtime overhead. Reducing STM Overheads: Much research has \nbeen devoted to reducing the cost of compiler-generated STM barriers on trans\u00adactional memory accesses. \nEarly work [8, 32] showed how to elim\u00adinate several classes of transactional overhead including redundant \nbarriers, barriers for accesses to provably immutable memory lo\u00adcations, and certain barriers for accesses \nto objects allocated in a transaction. Recent work by Afek et al. [9] uses the logic of pro\u00adgram reads \nand writes within a transaction to reduce STM over\u00adhead: for example, a shared variable that is read \nseveral times can be be read once and cached locally. These optimizations comple\u00adment ours, as they target \ndifferent kinds of STM overhead from our work. Beckman et al. [13] show how to use access permissions \nto re\u00admove STM synchronization overhead. While the goals are the same as ours, the mechanisms are different \n(alias control vs. type and effect annotations). The two mechanisms have different tradeoffs in expressivity \nand power: for example, Beckman et al. s method can eliminate write barriers only if an object is accessed \nthrough a unique reference, whereas our system can eliminate barriers for access through shared references, \nso long as the access does not cause interfering effects. However, alias restrictions can express some \npatterns (such as permuting unique references in a data struc\u00adture) that our system cannot. As future \nwork, it would be interesting to explore these tradeoffs further. Finally, several researchers have \neliminated STM overhead for accesses to thread-local data using whole-program static escape analysis \n[47] and programmer annotations to specify code blocks that do not require instrumentation [52]. Unlike \nour work, this work either requires whole-program analysis, or it relies on unveri.ed programmer annotations. \nNondeterministic Parallel Programming: Several research efforts are developing parallel models for nondeterministic \ncodes with irregular data access patterns, such as Delaunay mesh re.ne\u00adment. Galois [36] provides a form \nof isolation, but with iterations of parallel loops (instead of atomic statements) as the isolated compu\u00adtations. \nConcurrency is increased by detecting con.icts at the level of method calls, instead of reads and writes, \nand using semantic commutativity properties. Lublinerman et al. [39] have proposed object assemblies \nas an alternative model for expressing irregular, graph-based computations. These models are largely \northogonal to our work. In Galois, strong isolation holds if all shared data is accessed through well\u00adde.ned \nAPIs, but this property is not enforced, either statically or at runtime. We believe that our type and \neffect mechanisms could be applied to Galois to ensure this property. The object assemblies model may \nhave stronger isolation guarantees than Galois, but it is very specialized to irregular graph computations, \nin contrast to the more general fork-join model we present here. Kulkarni et al. [35] have recently proposed \ntask types as a way of enforcing a property they call pervasive atomicity.This work shares with ours \nthe broad goal of reducing the number of concurrent interleavings the programmer must consider. However, \nKulkarni et al. adopt an actor-inspired approach, in which data is non-shared by default, and sharing \nmusk occur through special task objects. This is in contrast to our approach of allowing familiar shared-memory \npatterns of programming, but using effect annotations to enforce safety properties. Finally, none of \nthe work discussed above provides any deterministic-by-default guarantee. 8. Conclusion We have shown \nhow to design a type and effect system that, to\u00adgether with a weakly atomic runtime system, achieves \nour stated goals of providing disciplined and safe nondeterminism, including race freedom, strong isolation \nof atomic operations and determinis\u00adtic parallel operations, compositional reasoning about deterministic \nand nondeterministic operations, and determinism by default. We have also shown how to leverage the system \nto remove unnecessary barriers from the transactional implementation, thereby enhancing performance. \nAcknowledgements This work was supported by the National Science Foundation under grants CCF 07-02724 \nand CNS 07-20772, and by Intel, Microsoft, and the University of Illinois through UPCRC Illinois. An \nanony\u00admous reviewer encouraged us to study TSP-R. Dan Grossman and Brad Chamberlain provided helpful \nsuggestions on a draft of this paper and the supporting proofs.  References [1] http://iss.ices.utexas.edu/lonestar/. \n[2] http://gee.cs.oswego.edu/dl/concurrency-interest. [3] http://http://sites.google.com/site/deucestm. \n[4] OpenMP Application Program Interface, Version 3.0. http://www.openmp.org/mp-documents/spec30.pdf, \n2008. [5] M. Abadi et al. Types for safe locking: Static race detection for Java. TOPLAS, 2006.  [6] \nM. Abadi et al. Semantics of transactional memory and automatic mutual exclusion. In POPL, 2008. [7] \nM. Abadi et al. Transactional memory with strong atomicity using off-the-shelf memory protection hardware. \nIn PPoPP, 2009. [8] A.-R. Adl-Tabatabai et al. Compiler and runtime support for ef.cient software transactional \nmemory. In PLDI, 2006. [9] Y. Afek et al. Lowering STM overhead with static analysis. In LCPC, 2010. \n[10] M. D. Allen et al. Serialization sets: A dynamic dependence-based parallel execution model. In PPOPP, \n2009. [11] A. Aviram et al. Ef.cient system-enforced deterministic parallelism. 2010. [12] N. E. Beckman \net al. Verifying correct usage of atomic blocks and typestate. In OOPSLA, 2008. [13] N. E. Beckman et \nal. Reducing STM overhead with access permis\u00adsions. In IWACO, 2009. [14] T. Bergan et al. CoreDet: A \ncompiler and runtime system for deter\u00administic multithreaded execution. In Int l. Conf. on Arch. Support \nfor Programming Langs. and Operating Systs. (ASPLOS), 2010. [15] E. D. Berger et al. Grace: Safe Multithreaded \nProgramming for C/C++. In OOPSLA, 2009. [16] R. D. Blumofe et al. Cilk: An ef.cient multithreaded runtime \nsystem. PPOPP, 1995. [17] R. L. Bocchino et al. A type and effect system for Deterministic Parallel Java. \nIn OOPSLA, 2009. [18] R. L. Bocchino et al. Parallel programming must be deterministic by default. In \nHotPar, 2009. [19] R. L. Bocchino Jr. An Effect System and Language for Deterministic\u00adby-Default Parallel \nProgramming. PhD thesis, University of Illinois, Urbana-Champaign, IL, 2010. [20] C. Boyapati et al. \nOwnership types for safe programming: Preventing data races and deadlocks. In OOPSLA, 2002. [21] J. Boyland. \nChecking interference with fractional permissions. SAS, 2003. [22] N. G. Bronson et al. Feedback-directed \nbarrier optimization in a strongly isolated STM. In POPL, 2009. [23] Z. Budimlic et al. Multicore implementations \nof the concurrent col\u00adlections programming model. In CPC, 2009. [24] S. Burckhardt et al. Concurrent \nprogramming with revisions and isolation types. In OOPSLA, 2010. [25] M. J. Carey et al. A status report \non the OO7 OODBMS benchmarking effort. In OOPSLA, 1994. [26] L. P. Chew. Guaranteed-quality mesh generation \nfor curved surfaces. In SCG, 1993. [27] J. Devietti et al. DMP: Deterministic Shared Memory Multiprocess\u00ading. \nIn ASPLOS, 2009. [28] D. Dice et al. Transactional locking II. In DISC, 2006. [29] C. Flanagan et al. \nTypes for atomicity: Static checking and inference for Java. TOPLAS, 2008. [30] A. Ghuloum et al. Ct: \nA .exible parallel programming model for tera\u00adscale architectures. Intel White Paper, 2007. [31] T. Harris \net al. Composable memory transactions. In PPoPP, 2005. [32] T. Harris et al. Optimizing memory transactions. \nIn PLDI, 2006. [33] T. Harris and K. Fraser. Language support for lightweight transactions. In OOPSLA, \n2003. [34] B. Jacobs et al. A programming model for concurrent object-oriented programs. TOPLAS, 2008. \n[35] A. Kulkarni et al. Task types for pervasive atomicity. In OOPSLA, 2010. [36] M. Kulkarni et al. \nOptimistic parallelism requires abstractions. In PLDI, 2007. [37] J. Larus and R. Rajwar. Transactional \nMemory (Synthesis Lectures on Computer Architecture). Morgan &#38; Claypool Publishers, 2007. [38] E. \nA. Lee. The problem with threads. Computer, 2006. [39] R. Lublinerman et al. Parallel programming with \nobject assemblies. In OOPSLA, 2009. [40] M. Martin, C. Blundell, and E. Lewis. Subtleties of transactional \nmemory atomicity semantics. IEEE Comp. Arch. Letters, 5(2):17, 2006. [41] N. D. Matsakis and T. R. Gross. \nA time-aware type system for data\u00adrace protection and guaranteed initialization. In OOPSLA, 2010. [42] \nK. F. Moore and D. Grossman. High-level small-step operational semantics for transactions. In POPL, 2008. \n[43] M. Olszewski et al. Kendo: Ef.cient deterministic multithreading in software. In ASPLOS, 2009. [44] \nC. Papadimitriou. The theory of database concurrency control.Com\u00adputer Science Press, Inc., 1986. [45] \nM. C. Rinard and M. S. Lam. The design, implementation, and evaluation of Jade. TOPLAS, 1998. [46] F. \nT. Schneider, V. Menon, T. Shpeisman, and A.-R. Adl-Tabatabai. Dynamic optimization for ef.cient strong \natomicity. In OOPSLA, 2008. [47] T. Shpeisman et al. Enforcing isolation and ordering in STM. In PLDI, \n2007. [48] M. Vakilian et al. Inferring Method Effect Summaries for Determin\u00adistic Parallel Java. Technical \nReport UIUCDCS-R-2009-3038, U. Illi\u00adnois, 2009. [49] C. von Praun et al. Implicit parallelism with ordered \ntransactions. In PPOPP, 2007. [50] A. Welc et al. Safe futures for Java. In OOPSLA, 2005. [51] A. Welc \net al. Revocation techniques for Java concurrency. Concur\u00adrency and Computation: Practice and Experience, \n2006. [52] R. M. Yoo et al. Kicking the tires of software transactional memory: Why the going gets tough. \nIn SPAA, 2008.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>A number of deterministic parallel programming models with strong safety guarantees are emerging, but similar support for nondeterministic algorithms, such as branch and bound search, remains an open question. We present a language together with a type and effect system that supports nondeterministic computations with a deterministic-by-default guarantee: nondeterminism must be explicitly requested via special parallel constructs (marked nd), and any deterministic construct that does not execute any nd construct has deterministic input-output behavior. Moreover, deterministic parallel constructs are always equivalent to a sequential composition of their constituent tasks, even if they enclose, or are enclosed by, nd constructs. Finally, in the execution of nd constructs, interference may occur only between pairs of accesses guarded by atomic statements, so there are no data races, either between atomic statements and unguarded accesses (strong isolation) or between pairs of unguarded accesses (stronger than strong isolation alone). We enforce the guarantees at compile time with modular checking using novel extensions to a previously described effect system. Our effect system extensions also enable the compiler to remove unnecessary transactional synchronization. We provide a static semantics, dynamic semantics, and a complete proof of soundness for the language, both with and without the barrier removal feature. An experimental evaluation shows that our language can achieve good scalability for realistic parallel algorithms, and that the barrier removal techniques provide significant performance gains.</p>", "authors": [{"name": "Robert L. Bocchino", "author_profile_id": "81323487933", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P2509680", "email_address": "rbocchin@cs.cmu.edu", "orcid_id": ""}, {"name": "Stephen Heumann", "author_profile_id": "81444608840", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P2509681", "email_address": "dpj@cs.uiuc.edu", "orcid_id": ""}, {"name": "Nima Honarmand", "author_profile_id": "81479640699", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P2509682", "email_address": "dpj@cs.uiuc.edu", "orcid_id": ""}, {"name": "Sarita V. Adve", "author_profile_id": "81100524186", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P2509683", "email_address": "dpj@cs.uiuc.edu", "orcid_id": ""}, {"name": "Vikram S. Adve", "author_profile_id": "81100524180", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P2509684", "email_address": "dpj@cs.uiuc.edu", "orcid_id": ""}, {"name": "Adam Welc", "author_profile_id": "81100163106", "affiliation": "Adobe Systems, San Francisco, CA, USA", "person_id": "P2509685", "email_address": "awelc@adobe.com", "orcid_id": ""}, {"name": "Tatiana Shpeisman", "author_profile_id": "81100439172", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P2509686", "email_address": "tatiana.shpeisman@intel.com", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926447", "year": "2011", "article_id": "1926447", "conference": "POPL", "title": "Safe nondeterminism in a deterministic-by-default parallel language", "url": "http://dl.acm.org/citation.cfm?id=1926447"}