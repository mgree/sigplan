{"article_publication_date": "01-26-2011", "fulltext": "\n Dynamic Inference of Static Types for Ruby Jong-hoon (David) An Avik Chaudhuri Jeffrey S. Foster* Michael \nHicks* rockalizer@gmail.com achaudhu@adobe.com jfoster@cs.umd.edu mwh@cs.umd.edu Epic Systems Corporation \nAdvanced Technology Labs, Adobe Systems *University of Maryland, College Park Abstract There have been \nseveral efforts to bring static type inference to object-oriented dynamic languages such as Ruby, Python, \nand Perl. In our experience, however, such type inference systems are ex\u00adtremely dif.cult to develop, \nbecause dynamic languages are typi\u00adcally complex, poorly speci.ed, and include features, such as eval \nand re.ection, that are hard to analyze. In this paper, we introduce constraint-based dynamic type infer\u00adence, \na technique that infers static types based on dynamic program executions. In our approach, we wrap each \nrun-time value to asso\u00adciate it with a type variable, and the wrapper generates constraints on this type \nvariable when the wrapped value is used. This tech\u00adnique avoids many of the often overly conservative \napproximations of static tools, as constraints are generated based on how values are used during actual \nprogram runs. Using wrappers is also easy to implement, since we need only write a constraint resolution \nalgo\u00adrithm and a transformation to introduce the wrappers. The best part is that we can eat our cake, \ntoo: our algorithm will infer sound types as long as it observes every path through each method body \nnote that the number of such paths may be dramatically smaller than the number of paths through the program \nas a whole. We have developed Rubydust, an implementation of our al\u00adgorithm for Ruby. Rubydust takes \nadvantage of Ruby s dynamic features to implement wrappers as a language library. We applied Rubydust \nto a number of small programs and found it to be both easy to use and useful: Rubydust discovered 1 real \ntype error, and all other inferred types were correct and readable. Categories and Subject Descriptors \nF.3.2 [Logics and Meaning of Programs]: Semantics of Programming Languages Program analysis; F.3.3 [Logics \nand Meaning of Programs]: Studies of Program Constructs Type structure General Terms Languages, Theory, \nVeri.cation Keywords Dynamic type inference, static types, Ruby, dynamic languages 1. Introduction Over \nthe years, there have been several efforts to bring static type inference to object-oriented dynamic \nlanguages such as Ruby, Python, and Perl [2 5, 7, 10, 12, 16, 18, 24, 27]. Static type in\u00adference has \nthe potential to provide the bene.ts of static typing Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Copyright c \n&#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 well-typed programs don t go wrong [17] without \nthe annotation burden of pure type checking. However, based on our own experience, developing a static \ntype inference system for a dynamic language is extremely dif.cult. Most dynamic languages have poorly \ndocumented, complex syn\u00adtax and semantics that must be carefully reverse-engineered be\u00adfore any static \nanalysis is possible. Dynamic languages are usu\u00adally speci.ed only with a canonical implementation, and \ntend to have many obscure corner cases that make the reverse engineer\u00ading process tedious and error-prone. \nMoreover, dynamic language programmers often employ programming idioms that impede pre\u00adcise yet sound \nstatic analysis. For example, programmers often give variables .ow-sensitive types that differ along \ndifferent paths, or add or remove methods from classes at run-time using dynamic features such as re.ection \nand eval. Combined, these challenges make developing and maintaining a static type inference tool for \na dynamic language a daunting prospect. To address these problems, this paper introduces constraint\u00adbased \ndynamic type inference, a technique to infer static types using information gathered from dynamic runs. \nMore precisely, at run time we introduce type variables for each position whose type we want to infer \nspeci.cally .elds, method arguments, and return values. As values are passed to those positions, we wrap \nthem in a proxy object that records the associated type variable. The user may also supply trusted type \nannotations for methods. When wrapped values are used as receivers or passed to type-annotated methods, \nwe generate subtyping constraints on those variables. At the end of the run, we solve the constraints \nto .nd a valid typing, if one exists. We have implemented this technique for Ruby, as a tool called Rubydust \n(where dust stands for dynamic unraveling of static types). Unlike standard static type systems, Rubydust \nonly con\u00ad.ates type information at method boundaries (where type vari\u00adables accumulate constraints from \ndifferent calls), and not within a method. As such, Rubydust supports .ow-sensitive treatment of local \nvariables, allowing them to be assigned values having differ\u00adent types. Since Rubydust only sees actual \nruns of the program, it is naturally path-sensitive and supports use of many dynamic fea\u00adtures. Finally, \nRubydust can be implemented as a library by em\u00adploying common introspection features to intercept method \ncalls; there is no need to reverse-engineer any subtle parsing or elabora\u00adtion rules and to build a separate \nanalysis infrastructure. In sum, Rubydust is more precise than standard static type inference, and sidesteps \nmany of the engineering dif.culties of building a static analysis tool suite. Although Rubydust is based \npurely on dynamic runs, we can still prove a soundness theorem. We formalized our algorithm on a core \nsubset of Ruby, and we prove that if the training runs on which types are inferred cover every path in \nthe control-.ow graph (CFG) of every method of a class, then the inferred types for that class s .elds \nand methods are sound for all possible runs. In our formalism, all looping occurs through recursion, \nand so the number of required paths is at most exponential in the size of the largest method body in \na program. Notice that this can be dramatically smaller than the number of paths through the program \nas whole. Clearly, in practice it is potentially an issue that we need test cases that cover all method \npaths for fully sound types. However, there are several factors that mitigate this potential drawback. \n Almost all software projects include test cases, and those test cases can be used for training. In fact, \nthe Ruby community encour\u00adages test-driven development, which prescribes that tests be written before \nwriting program code thus tests will likely be available for Rubydust training right from the start. \n While loops within methods could theoretically yield an un\u00adbounded number of paths, in our experimental \nbenchmarks we ob\u00adserved that most loop bodies use objects in a type-consistent man\u00adner within each path \nwithin the loop body. Hence, typically, observ\u00ading all paths within a loop body (rather that observing \nall possible iterations of a loop) suf.ces to .nd correct types. We discuss this more in Section 4. \n Even incomplete tests may produce useful types. In particular, the inferred types will be sound for \nany execution that takes (within a method) paths that were covered in training. We could potentially \nadd instrumentation to identify when the program executes a path not covered by training, and then blame \nthe lack of coverage if an error arises as a result [10]. Types are also useful as documen\u00adtation. Currently, \nthe Ruby documentation includes informal type signatures for standard library methods, but those types \ncould be\u00adcome out of sync with the code (we have found cases of this pre\u00adviously [12]). Using Rubydust, \nwe could generate type annotations automatically from code using its test suite, and thus keep the type \ndocumentation in-sync with the tested program behaviors.  Our implementation of Rubydust is a Ruby library \nthat takes advantage of Ruby s rich introspection features; no special tools or compilers are needed. \nRubydust wraps each object o at run\u00adtime with a proxy object that associates o with a type variable a \nthat corresponds to o s position in the program (a .eld, argument, or return-value). Method calls on \no precipitate the generation of constraints; e.g., if the program invokes o.m(x) then we generate a constraint \nindicating that a must have a method m whose argument type is a supertype of the type of x. Rubydust \nalso consumes trusted type annotations on methods; this is important for giving types to Ruby s built-in \nstandard library, which is written in C rather than Ruby and hence is not subject to our type inference \nalgorithm. We evaluated Rubydust by applying it to several small pro\u00adgrams, the largest of which was \nroughly 750 LOC, and used their accompanying test suites to infer types. We found one real type error, \nwhich is particularly interesting because the error was un\u00adcovered by solving constraints from a passing \ntest run. All other programs were found to be type correct, with readable and correct types. The overhead \nof running Rubydust is currently quite high, but we believe it can be reduced with various optimizations \nthat we intend to implement in the future. In general we found the perfor\u00admance acceptable and the tool \nitself quite easy to use. In summary, our contributions are as follows: We introduce a novel algorithm \nto infer types at run-time by dynamically associating .elds and method arguments and re\u00adsults with type \nvariables, and generating subtyping constraints as those entities are used. (Section 2)  We formalize \nour algorithm and prove that if training runs cover all syntactic paths through each method of a class, \nthen the inferred type for that class is sound. (Section 3)  We describe Rubydust, a practical implementation \nof our algo\u00adrithm that uses Ruby s rich introspection features. Since Ruby\u00addust piggybacks on the standard \nRuby interpreter, we can natu\u00ad  Figure 1. Dynamic instrumentation for a call o.m(v) rally handle all \nof Ruby s rather complex syntax and semantics without undue effort. (Section 4) We evaluate Rubydust \non a small set of benchmarks and .nd it to be useful. (Section 5) We believe that Rubydust is a practical, \neffective method for inferring useful static types in Ruby, and that the ideas of Rubydust can be applied \nto other dynamic languages. 2. Overview Before presenting our constraint-based dynamic type inference \nal\u00adgorithm formally, we describe the algorithm by example and illus\u00adtrate some of its key features. Our \nexamples below are written in Ruby, which is a dynamically typed, object-oriented language in\u00adspired \nby Smalltalk and Perl. In our discussion, we will try to point out any unusual syntax or language features \nwe use; more complete information about Ruby can be found elsewhere [28]. 2.1 Method call and return \nIn our algorithm, there are two kinds of classes: annotated classes, which have trusted type signatures, \nand unannotated classes, whose types we wish to infer. We assign a type variable to each .eld, method \nargument, and method return value in every unannotated class. At run time, values that occupy these positions \nare associ\u00adated with the corresponding type variable. We call this association wrapping since we literally \nimplement it by wrapping the value with some metadata. When a wrapped value is used, e.g., as a re\u00adceiver \nof a method call, or as an argument to a method with a trusted type signature, we generate a subtyping \nconstraint on the associated variable. At the end of the training runs, we solve the generated constraints \nto .nd solutions for those type variables (which yield .eld and method types for unannotated classes), \nor we report an er\u00adror if no solution exists. Note that since all instances of a class share the same \ntype variables, use of any instance contributes to inferring a single type for its class. Before working \nthrough a full example, we consider the oper\u00adation of our algorithm on a single call to an unannotated \nmethod. Figure 1 summarizes the .ve steps in analyzing a call o.m(v) to a method de.ned as def m(x) e \nend,where x is the formal argument and e is the method body. In this case, we create two type variables: \nax,torepresent x s type, and am ret ,torepresent m s return type. In step (1), the caller looks up the \n(dynamic) class of the receiver to .nd the type of the called method. In this case, method m has type \nax . am ret . The caller then generates two constraints. The constraint labeled (a) ensures the type \nof the actual argument is a subtype of the formal argument type. Here, type(x) is the type of an object, \neither its actual type, for an unwrapped object, or the type variable stored in the wrapper. In the constraint \n(b), the type [m : ...] is the type of an object with a method m with the 1 2 3 4 5 6 7 8 9 10 11 12 \n13 14 15 16 17 18 19 20 # Numeric :[...+: Numeric . Numeric...] class A # foo : aw \u00d7 au . afoo ret def \nfoo(w,u) # w =(b : aw), u =(1: au) w.baz() # aw = [baz :() . ()] y=3+u # y =(4: Numeric) au = Numeric \nreturn bar(w) # ret =(7: abar ret ) aw = ax end # abar ret = afoo ret # bar : ax . abar ret def bar(x) \n# x =(b : ax) x.qux() # ax = [qux :() . ()] return 7 # Numeric = abar ret end end A.new.foo(B.new,1) \n# B.new returns a new object b # B = aw # Numeric = au # ret =(7: afoo ret ) Figure 2. Basic method call \nand return example given type. Hence by width-subtyping, constraint (b) speci.es that o hasatleastan \nm method with the appropriate type. We generate this constraint to ensure o s static type type(o) is \nconsistent with the type for m we found via dynamic lookup. For now, ignore the constraint (c) and the \nother constraints (e), (g), and (i) involving .elds @f and @g; we will discuss these in Section 2.3. \nIn step (2) of analyzing the call, the callee wraps its arguments with the appropriate type variables \nimmediately upon entry. In this case, we set x to be v : ax, which is our notation for the value v wrapped \nwith type ax. Then in step (3), we execute the body of the method. Doing so will result in calls to other \nmethods, which will undergo the same process. Moreover, as v : ax maybeusedinsomeofthesecalls, we will \ngenerate constraints on type(v : ax), i.e., ax,thatwe saw in step (1). In particular, if v : ax is used \nas a receiver, we will constrain ax to have the called method; if v : ax is used as an argument, we will \nconstrain it to be a subtype of the target method s formal argument type. At a high-level, steps (1) \nand (2) maintain two critical invariants: Prior to leaving method n to enter another method m,we generate \nconstraints to capture the .ow of values from n to m (Constraints (a) and (b)).  Prior to entering a \nmethod m, all values that could affect the type of m are wrapped (Indicated by (d)).  Roughly speaking, \nconstraining something with a type records how it was used in the past, and wrapping something with a \ntype ob\u00adserves how it is used in the future. Returning from methods should maintain the same invariants \nas above but in the reverse direction, from callee to caller. Thus, in step (4), we generate constraint \n(f) in the callee that the type of the returned value is a subtype of the return type, and in step (5), \nwhen we return to the caller we immediately wrap the returned value with the called method s return type \nvariable.  2.2 Complete example Now that we have seen the core algorithm, we can work through a complete \nexample. Consider the code in Figure 2, which de.nes a class A with two methods foo and bar, and then \ncalls foo on a fresh instance of A on line 17. This code uses Ruby s Numeric class, which is one of \nthe built\u00adin classes for integers. Because Numeric is built-in, we make it an annotated class, and supply \ntrusted type signatures for all of its methods. A portion of the signature is shown on line 1, which \nindicates Numeric has a method + of type Numeric.Numeric.1 As in the previous subsection, we introduce \ntype variables for method arguments and returns, in this case aw, au,and afoo ret for foo,and ax and \nabar ret for bar. Then we begin stepping through the calls. At the call on line 17, we pass in actual \narguments b (the object created by the call to B.new on the same line) and 1. Thus we constrain the formal \nargument types in the caller (lines 18 and 19) and wrap the actuals in the callee (line 5). Next, on \nline 6, we use a wrapped object, so we generate a constraint; here we require the associated type variable \naw contains a no-argument method baz. (For simplicity we show the return type as (), though as it is \nunused it could be arbitrary.) On line 7, we call 3+u. The receiver object is 3, which has actual class \nis Numeric, an annotated class. Thus, we do the normal handling in the caller (the shaded box in Figure \n1), but omit the steps in the callee, since the annotation is trusted. Here, we gen\u00aderate a constraint \nau = Numeric between the actual and formal argument types. We also generate a constraint (not shown in \nthe .gure) type(3) = [+ : ...],but as type(3) = Numeric, this con\u00adstraint is immediately satis.able; \nin general, we need to generate such constraints to correctly handle cases where the receiver is not \na constant. Finally, we wrap the return value from the caller with its annotated type Numeric. Next, \nwe call method bar. As expected, we constrain the actuals and formals (line 8), wrap the argument inside \nthe callee (line 11), and generate constraints during execution of the body (line 12). At the return \nfrom bar, we constrain the return type (line 13) and wrap in the caller (yielding the wrapped value 7: \nabar ret on line 8). As that value is immediately returned by foo, we constrain the return type of foo \nwith it (line 9) and wrap in the caller (line 20). After this run, we can solve the generated constraints \nto infer types. Drawing the constraints from the example as a directed graph, where an edge from x to \ny corresponds to the constraint x = y,wehave: a Numeric B [baz : () . ()] Numeric u  a w [qux : \n() . ()] Numeric abar_ret afoo_ret a x (Here we duplicated Numeric for clarity; in practice, it is \ntypically represented by a single node in the graph.) As is standard, we wish to .nd the least solution \n(equivalent to a most general type) for each method. Since arguments are contravariant and returns are \ncovariant, this corresponds to .nding upper bounds (transitive successors in the constraint graph) for \narguments and lower bounds (transitive predecessors) for returns. Intuitively, this is equivalent to \ninferring argument types based on how arguments are used within a method, and computing return types \nbased on what types .ow to return positions. For our example, the .nal solution is aw =[baz :() . (), \nqux :() . ()] ax =[qux :() . ()] au = Numeric abar ret = afoo ret = Numeric Notice that w must have \nbar and qux methods, but x only needs a qux method. For return types, both bar and foo always return \nNumeric. 2.3 Local variables and .elds In the previous example, our algorithm generated roughly the \nsame constraints that a static type inference system might generate. How\u00ad 1 In Ruby, the syntax e1 + \ne2 is shorthand for e1. +(e2), i.e., calling the + method on e1 with argument e2. 21 class C 22 def \nfoo(x) 23 z = x; z.baz(); 24 z=3; return z+5; 25 end 26 end 27 class D 28 def bar(x) @f = x end 29 def \nbaz() y =3+@f end 30 def qux() @f = foo end 31 def f() bar( foo ) end 32 end Figure 3. Example with local \nvariables and .elds ever, because our algorithm observes only dynamic runs, in many cases it can be more \nprecise than static type inference. Consider class Cin Figure 3. On entry to foo,wewraptheactual argument \nv as v : ax,where ax is foo s formal argument type. At the assignment on line 23, we do nothing special \nwe allow the language interpreter to copy a reference to v : ax into z. At the call to baz, we generate \nthe expected constraint ax = [baz :() . ()]. More interestingly, on line 24, we reassign z to contain \n3, and so at the call to z s + method, we do not generate any constraints on ax. Thus, our analysis is \n.ow-sensitive with respect to local variables, meaning it respects the order of operations within a method. \nTo see why this treatment of local variable assignment is safe, it helps to think in terms of compiler \noptimization. We are essentially performing inference over a series of execution traces. We can view \neach trace as a straight-line program. Consider the execution trace of foo (which is the same as the \nbody of the function, in this case). If we apply copy propagation (of x and 3) to the trace we get z=x; \nx.baz(); z=3; return 3+5; Since z is a local variable inaccessible outside of the scope of foo, it is \ndead at the end of the method, too, so we can apply dead code elimination to reduce the trace to x.baz(); \nreturn 3+5; . The constraints we would generate from this trace are equivalent to those we would generate \nwith our approach. Instance .elds in Ruby are not visible outside an object, but they are shared across \nall methods of an object. Thus, we need to treat them differently than locals. To see why, consider the \nclass D in Figure 3, which uses the instance variable @f (all instance variables begin with @ in Ruby). \nSuppose that we treated .elds the same way as local variables, i.e., we did nothing special at assignments \nto them. Now consider inferring types for D with the run bar(1); baz(); qux(). During the call bar(1), \nwe would generate the constraint Numeric = ax (the type variable for x) and store 1: ax in @f. Then during \nbaz(), we would generate the constraint ax = Numeric, and the call to qux() would generate no constraints. \nThus, we could solve the constraints to get ax = Numeric, and we would think this class has type [bar \n: Numeric . (), baz :() . (), qux :() . ()]. But this result is clearly wrong, as the well-typed sequence \nqux(); baz() produces a type error. To solve this problem, we need to introduce a type variable a@f for \nthe .eld, and then generate constraints and wrap values accordingly. It would be natural to do this at \nwrites to .elds, but that turns out to be impossible with a Ruby-only, dynamic solution, as there is \nno dynamic mechanism for intercepting .eld writes.2 Fortunately, we can still handle .elds safely by \napplying the same 2 Recall we wish to avoid using static techniques, including program rewrit\u00ading, because \nthey require complex front-ends that understand program se\u00admantics. For example, even ordering of assignment \noperations in Ruby can be non-obvious in some cases [11]. 33 class E 34 def foo(x, p) 35 if p then x.qux() \nelse x.baz() end 36 end 37 def bar(p) 38 if p then y=3 else y = hello end 39 if p then y+6 else y. length \nend 40 end 41 end (a) Paths and path-sensitivity 42 class F 43 def foo(x) 44 return (if x then 0 else \nhello end) 45 end 46 def bar(y,z) 47 return (if y then foo(z) else foo(!z) end) 48 end 49 end 50 f= \nF.new (b) Per-method path coverage Figure 4. Additional Examples principles we saw in Figure 1 for method \narguments and returns. There, we needed two invariants: (1) when we switch from method m to method n, \nwe need to capture the .ow of values from m to n, and (2) when we enter a method n, we need to wrap all \nvalues that could affect the type of n. Translating this idea to .elds, we need to ensure: When we switch \nfrom m to n, we record all .eld writes per\u00adformed by m, since they might be read by n. This is captured \nby constraints (c) and (g) in Figure 1.  When we enter n, we need to wrap all .elds n may use, so that \nsubsequent .eld reads will see the wrapped values. This is captured by constraints (e) and (i) in Figure \n1.  Adding these extra constraints and wrapping operations solves the problem we saw above. At the \ncall bar(1), we generate the constraint Numeric = ax, as before. However, at the end of bar, we now generate \na constraint ax = a@f to capture the write. At the beginning of baz,wewrap @f so that the body of baz \nwill now generate the constraints a@f = Numeric.Then qux generates the constraint String = a@f . We can \nimmediately see the constraints String = a@f = Numeric are unsatis.able, and hence we would correctly \nreport a type error. Notice that this design also allows a small amount of .ow\u00adsensitivity for .elds: \nA .eld may temporarily contain a value of a different type, as long as it is overwritten before calling \nanother method or returning. We do not expect that this yields much addi\u00adtional precision in practice, \nhowever. Our implementation handles class variables similarly to in\u00adstance variables. Note that we assume \nsingle-threaded execution if accesses of such variables by other threads are interleaved, the inferred \nconstraints may be unsound. 2.4 Path, path-sensitivity, and path coverage As our algorithm observes \ndynamic runs, to infer sound types we need to observe all possible paths through branching code se\u00adquences. \nFor example, we can infer types for the foo method in Fig\u00adure 4(a) if we see an execution such as foo(a, \ntrue ); foo(b, false );. 51 52 53 54 55 56 57 58 59 In this case, we will generate ax = [qux : ...] \nfrom the .rst call, and ax = [baz : ...] from the second. One bene.t of observing actual dynamic runs \nis that we never model unrealizable program executions. For example, consider the bar method in Figure \n4(a). In a call bar(true), line 38 assigns a Numeric to y, and in a call bar( false ), it assigns a String \nto y. Typical path-insensitive static type inference would con.ate these possibilities and determine \nthat y could be either a Numeric or String on line 39, and hence would signal a potential error for both \nthe calls to + and to length. In contrast, in our approach we do not assign any type to local y, and \nwe observe each path separately. Thus, we do not report a type error for this code. Our soundness theorem \n(Section 3) holds if we observe all pos\u00adsible paths within each method body. To see why this is suf.cient, \nrather than needing to observe all possible program paths, consider the code in Figure 4(b). Assuming \nbar is the entry point, there are four paths through this class, given by all possible truth value com\u00adbinations \nfor y and z. However, to observe all possible types,we only need to explore two paths. If we call f.bar(true,true) \nand f.bar( false ,true), we will generate the following constraints:3 f.bar(true, true) Boolean = ay \nBoolean = az az = ax Numeric = afoo ret afoo ret = abar ret f.bar(false, true) Boolean = ay Boolean \n= az Boolean = ax String = afoo ret afoo ret = abar ret Thus, we can deduce that bar mayreturna Numeric \nor a String. (Note that our system supports union types, so we can express this return type as String \n. Numeric.) The reason we only needed two paths is that type variables on method arguments and returns \nact as join points, summariz\u00ading the possible types of all paths within a method. In our exam\u00adple, both \nbranches of the conditional in bar have the same type, afoo ret . Thus, the other possible calls, f.bar(true, \nfalse ) and f.bar( false , false ), do not affect what types bar could return.  2.5 Dynamic features \nAnother bene.t of our dynamic type inference algorithm is that we can easily handle dynamic features \nthat are very challenging for static type inference. For example, consider the following code, which \noccurred in one of our experimental benchmarks: def initialize (args) args .keys.each do | attrib |self \n.send( #{attrib}= , args[attrib ]) end end This constructor takes a hash args as an argument, and then \nfor each key-value pair (k, v) uses re.ective method invocation via send to call a method named after \nk with the argument v. Or, consider the following code: ATTRIBUTES = [ bold , underscore , ...] ATTRIBUTES.each \ndo |attr| code = def #{attr}(&#38;blk) ... end eval code end For each element of the string array ATTRIBUTES, \nthis code uses eval to de.ne a new method named after the element. 3 Note that an if generates no constraints \non its guard as any object may be supplied ( false and nil are false, and any other object is true). \nAlso, here and in our implementation, we treat true and false as having type Boolean, though in Ruby \nthey are actually instances of TrueClass and FalseClass, respectively. expressions e ::= nil | self | \nx | @f | x = e | @f = e | A.new | e.m(e') | e; e' | iff e then e' else e'' methods d ::= def m(x)= e \nclasses c ::= class A = d * programs P ::= c * c e types t ::= A.@f | A.m | A.m |.|T| A | [m : A.m . \nA.m] | t . t ' | t n t ' x . local variables A . class names @f . .eld names f . labels m . method names \nFigure 5. Syntax of source language We encountered both of these code snippets in earlier work, in which \nwe proposed using run-time pro.ling to gather concrete uses of send, eval , and other highly dynamic \nconstructs, and then ana\u00adlyzing the pro.le data statically [10]. In the dynamic analysis we propose in \nthe current paper, no separate pro.ling pass is needed: we simply let the language interpreter execute \nthis code and ob\u00adserve the results during type inference. Method invocation via send is no harder than \nnormal dynamic dispatch; we just do the usual constraint generation and wrapping, which, as mentioned \nearlier, is actually performed inside the callee in our implementation. Method creation via eval is also \neasy, since we add wrapping instrumenta\u00adtion by dynamically iterating through the de.ned methods of unan\u00adnotated \nclasses; it makes no difference how those methods were created, as long as it happens before instrumentation. \n 3. Formal development In this section, we describe our dynamic type inference technique on a core Ruby-like \nsource language. The syntax is shown in Figure 5. Expressions include nil, self, variables x, .elds @f, \nvariable assignments x = e, .eld assign\u00adments @f = e, object creations A.new, method calls e.m(e'),se\u00adquences \ne; e', and conditionals iff e then e' else e''. Conditionals carry labels f to allow us to reason about \npath coverage. Concep\u00adtually, labels may be thought of as logical propositions literals of the form f \nand \u00acf represent a branch taken or not taken, respec\u00adtively, at run time. We assume that all labels are \ndistinct. The form def m(x)= e de.nes a method m with formal ar\u00adgument x and body e. Classes c are named \ncollections of methods. A program consists of a set of classes and a single expression that serves as \na test. Typically, we run a test on a collection of classes to train the system i.e., infer types and, \nin our proofs, run other tests to monitor the system i.e., show that the inferred types are sound. In \nthe formalism, we use only a single test e to train the sys\u00adtem, but we can always represent a set of \ntests by sequencing them together into a single expression. The syntax of types requires some explanation. \nType variables are tagged to avoid generating and accounting for fresh variables. Thus, A.@f is a type \nvariable that denotes the type of the .eld @f of objects of class A; similarly, A.m and A.m are type \nvariables that denote the argument and result types of the method m of class A. In addition, we have \nnominal types A for objects of class A,and structural types [m : A.m . A.m] for objects with method m \nwhose argument and result types can be viewed as A.m and A.m. Finally, we have the bottom type ., the \ntop type T, union types t . t ', and intersection types t n t '. values v ::= l | nil wrapped values \n. ::= v : t .eld maps F ::= (@f . .) * method maps M ::= (m . .(x)e) * class maps C ::= (A.M) * heaps \nH ::= (l . A(F)) * environments E ::= (x . .) * , (self . l : A)? literals p ::= (f |\u00acf) paths f ::= \np * ' ) * constraints . ::= (t = t ,f * Figure 6. Auxiliary syntax 3.1 Training semantics We now de.ne \na semantics for training. The semantics extends a standard semantics with some instrumentation. The instrumenta\u00adtion \ndoes not affect the run-time behavior of programs; it merely records run-time information that later \nallows us to infer types and argue about their soundness. To de.ne the semantics, we need some auxiliary \nsyntax to de\u00adscribe internal data structures, shown in Figure 6. Let l denote heap locations. Values \ninclude heap locations and nil. Such values are wrapped with types for training. A .eld map associates \n.eld names with wrapped values. A method map associates method names with abstractions. A class map associates \nclass names with method maps. A heap maps locations to objects A(F), which denote ob\u00adjects of class A \nwith .eld map F. An environment maps vari\u00adables to wrapped values and, optionally, self to a location \nwrapped with its run-time type. Formally, the run-time type of a value un\u00adder a heap is de.ned by runtypeH(l)= \nA if H(l)= A(F) and runtypeH(nil)= .. Apath f is a list of label literals f or \u00acf, denoted by p,thatmark \nconditionals encountered in a run. Constraints . include standard subtyping constraints t = t ' and coverage \nconstraints f, meaning that the path f is traversed during some call in the run. The rules shown in Figure \n7 derive big-step reduction judgments of the form H; E; e -.C H ' ; E ' ; . | .; f, meaning that given \nclass map C, expression e under heap H and environment E reduces to wrapped value ., covering path f, \ngenerating constraints .,and returning heap H ' and environment E ' .Wede.ne the following operations \non wrapped values: if . = v : t then val(.)= v, type(.)= t ,and . t ' = v : t ' . In the rules, we use \nan underscore in any position where an arbitrary quantity is allowed, and we write empty set as {} to \navoid confusion with f. By (TNIL), the type assigned to nil is ., which means that nil may have any type. \n(TSELF) is straightforward. In (TNEW),the notation A( . nil : .) indicates an instance of A with all \npossible .elds mapped to nil. (As in Ruby, .elds need not be explicitly initialized before use, and are \nnil by default.) (TVAR) and (TVAR=) are standard, and generate no constraint nor perform any wrapping, \nas discussed in Section 2.3. As explained in Section 2, we permit some .ow-sensitivity for .eld types. \nThus, (FIELD) and (FIELD=) are like (VAR) and (VAR=) in that they generate no constraint and perform \nno wrapping. (TSEQ) is straightforward. By (TCOND), either f or \u00acf is recorded in the current path, depending \non the branch traversed for the conditional with label f. Note that f and \u00acf can never ap\u00adpear in the \nsame path: looping in the formal language occurs only via recursive calls, and callee paths are not included \nin caller paths. That said, the conditional itself may be visited on many occasions during the training \nrun, so that both f and \u00acf may eventually appear in coverage constraints. (TNIL) H; E; nil -.C H; E; \nnil : .|{}; {} (TSELF) E(self)= l : A H; E; self -.C H; E; l : A |{}; {} (TNEW) l fresh H ' = H{l . \nA(. nil : .)} H; E; A.new -.C H ' ; E; l : A |{}; {} (TVAR) E(x)= . H; E; x -.C H; E; . |{}; {} (TVAR \n=) ' ''' H; E; e -.C H ' ; E ; . | .; f E = E{x . .} H; E; x = e -.C H ' ; E '' ; . | .; f (TFIELD) \nE(self)= .l = val(.) H(l)= (F) F(@f)= . H; E;@f -.C H; E; . |{}; {} (TFIELD =) H; E; e -.C H ' ; E ' \n; . | .; f E ' (self)= . ' l = val(. ' ) H ' (l)= A(F) H '' = H ' {l . A(F{@f . .})} = e -.C H '' ' \nH; E;@f ; E ; . | .; f (TSEQ) H; E; e -.C H ' ; E ' ; | .; f '' '''' H ' ; E ; e -.C H '' ; E ; . | \n.; f ' ) -.C H '''' '' H; E;(e; e ; E ; . | ., .; f, f (TCOND) H; E; e -.C H ' ; E ' ; . | .; f p = \nf, ep = e ' if val(.)= nil p = \u00acf, ep = e '' if val(.)= nil ' ''' '' H ' ; E ; ep -.C H '' ; E ; . | \n.; f ' '' -.C H ''''' ' ' H; E; iff e then e else e ; E ; . | ., .; f, p, f (TCALL) H; E; e -.C H ' \n; E ' ; . | .; ft = type(.) '' '''''' ' H ' ; E ; e -.C H '' ; E ; . | .; ft = type(. ) l = E '' (self) \nl = val(. ' ) H '' '' (l)= A() C(A)= MM(m)= .(x)e . '' = t ' = [m : A.m . A.m],t = A.m, constrainl(H \n'' ) H ''' (H ''''' ' = wrapl) E = {self. . A, x . . A.m} H ''' ''' '' ; E ; e -.C H;; . | .; ft = \ntype(.) . ' = t = A.m, constrainl(H),f ' ' H = wrapl(H) . = . A.m ' ' '' ' ' '' ' ' H; E; e .m(e) \n-.C H ; E ; . | ., . , . , ., .; f, f Figure 7. Training semantics (TCALL) performs the actions introduced \nin Figure 1. First, the type of the receiver . ' is constrained to be a subtype of [m : A.m . A.m], and \nthe type of the argument . is constrained to be a subtype of A.m, the argument type of the callee. We \nevalu\u00adate the body e '' with argument x mapped to . A.m,whichis the argument wrapped with method argument \ns type variable. The type of the result . is constrained to be a subtype of the result type A.m, and \nreturned wrapped with that type. Furthermore (TCALL) relates the current path to the set of paths stored \nas coverage con\u00adstraints. In particular, while f, f ' records the current path traversed in the caller, \nthe path f traversed by the callee is recorded as a cov\u00aderage constraint in . ' . In addition, (TCALL) \ninvolves wrapping and generation of subtyping constraints for .elds of the caller and the callee objects. \nLet H(l)= A(F).Wede.ne  wrapl(H)= H{l . A({@f . . A.@f | @f . . .F})}  constrainl(H)= {type(.) = A.@f \n| @f . . .F}  As discussed in Section 2, we constrain the .elds of the caller object and wrap the .elds \nof the callee object before the method call, and symmetrically, constrain the .elds of the callee object \nand wrap the .elds of the caller object after the method call. Finally, the following rule describes \ntraining with programs. (TRAIN) C = classmap(c * ) {}, {},e -.C ;; | .; c * c e . solve(subtyping(.)); \ncoverage(.) wherewede.ne: classmap(c * )= {A . methodmap(d * ) | class A = d * . c * }methodmap(d * )= \n{m . .(x)e | def m(x)= e . d * }coverage(.) = {f | f . .} We assume that solve(subtyping(.)) externally \nsolves the sub\u00adtyping constraints in . to obtain a mapping T from type variables to concrete types (possibly \ninvolving T and ., and unions and in\u00adtersections of nominal types and structural types). We discuss the \nsolving algorithm we use in our implementation in Section 4; how\u00adever, our technique is agnostic to the \nchoice of algorithm or even to the language of solved types. The crucial soundness measure for the inferred \ntypes is F= coverage(.), which collects all the coverage constraints in the run. The key soundness theorem \nstates that any run whose paths are a subset of F must use types that are consistent with F.As a corollary, \nwe show that if all possible paths are covered during inference, then all runs are type safe.  3.2 Monitoring \nsemantics To argue about the soundness of inferred types, next we de.ne a monitoring semantics. This \nsemantics slightly extends a standard semantics with monitoring of calls at the top level, and monitoring \nof conditionals. This affects the run-time behavior of programs we enforce a contract between the run-time \ntype of the argument and the inferred argument type for any call at the top level, and re\u00adquire that \nthe monitoring run only traverses paths that have already been traversed by the training run. A top-level \ncall is simply one that calls into any of the classes for which we have inferred types from outside; \na call from within a typed class (either to its own method or to one of other typed class) is considered \ninternal. To de.ne the monitoring semantics, we modify the syntax of expressions, .eld maps, class maps, \nand environments as follows. expressions e ::= ... | e ' .m(e) class maps C ::= (A .M) * , T , F .eld \nmaps F ::= (@f . v) * environments E ::= (x . v) * , (self . l)? Expressions of the form e ' .m(e) denote \nmethod calls at the top level. Class maps additionally carry the solution T of the subtyping constraints \ncollected during training, and the coverage F of the training run. Field maps and environments no longer \nmap to types. The rules shown in Figure 8 derive big-step reduction judgments of the form H; E; e | f \n-.C ; E ' ; v | f ' ,where f is the H ' prescribed path to be traversed and f ' is the suf.xof f that \nremains. (NIL) H; E; nil | f -.C H; E; nil | f (SELF) E(self)= l H; E; self | f -.C H; E; l | f  (NEW) \nl fresh H ' = H{l . A(. nil)} H; E; A.new | f -.C H ' ; E; l | f (VAR) E(x)= v H; E; x | f -.C H; E; \nv | f  (VAR =) ' ' ''' H; E; e | f -.C H ' ; E ; v | f E = E{x . v} H; E; x = e | f -.C H ' ; E '' ; \nv | f ' (FIELD) E(self)= l H(l)= (F) F(@f )= v H; E;@f | f -.C H; E; v | f  (FIELD =) '' ' H; E; e \n| f -.C H ' ; E ; v | f E (self)= l H ' (l)= A(F) H '' = H ' {l . A(F{@f . v})} = e | f -.C H ''' ' H; \nE;@f ; E ; v | f (SEQ) H; E; e | f -.C H ' ; E ' ; | f ' ''' '' '' H ' ; E ; e | f -.C H '' ; E ; v \n| f ' ) | f -.C H '''' '' H; E;(e; e ; E ; v | f (CALL) H; E; e | f -.C H ' ; E ' ; v | f ' H ''' ' \n-.C H '''' '' ; E ; e | f ; E ; l | f H '' '' (l)= A() C(A)= MM(m)= .(x)e H '' '' | f -.C H ''' ' f .C \n; {self . l, x . v}; e ;; v |' .m(e) | f -.C H ''' ''' '' H; E; e ; E ; v | f (MCOND) H; E; e | f -.C \nH ' ; E ' ; v | p, f ' p = f, ep = e ' if v = nil p = \u00acf, ep = e '' if v = nil ' ' ''' '' H ' ; E ; \nep | f -.C H '' ; E ; v | f ' '' ''' '' H; E; iff e then e else e | f -.C H '' ; E ; v | f (MCALL) H; \nE; e | f -.C H ' ; E ' ; v | f ' ''' '' '' H ' ; E ; e | f -.C H '' ; E ; l | f H '' (l)= A() C(A)= M \nM(m)= .(x)e '' runtypeH.. (v) =C(A.m) H '' '' | f -.C H ''' '' f .C ; {self . l, x . v}; e ;; v |' .m(e) \n| f -.C H ''' '''' '' H; E; e ; E ; v | f Figure 8. Monitoring semantics (NIL), (SELF), (NEW), (VAR), \n(FIELD), (VAR=), (FIELD=),and (SEQ) are derived from the corresponding rules for training, simply by \nerasing types, paths, and constraints. (CALL) is similar, but in addition, it (non-deterministically) \npicks a path from the coverage of the training run, which is carried by C, and prescribes that the call \nshould traverse that path by including it in the environment under which that call is run. By (MCOND), \na particular branch p of a conditional may be traversed only if the path prescribes that branch, i.e., \nit begins with p. The branch is then run under a residual path f ' . By (MCALL), a top level call requires \nthat the run-time type of the argument v is a subtype of the argument type of the method, as prescribed \nby the solution carried by C, which we write as C(A.m). The following rule describes monitoring of programs. \n(RUN) C = classmap(c * ), T , F {}, {}, monitor(e) |{} -.C ;; |{} c * c e . where monitor(e ' ) is e \n' but with any method call underlined. Of course, reduction may be stuck in several ways, so we do not \nexpect to derive such a judgment for every program. In particular, we do not care when reduction is stuck \ndue to calls on nil, viola\u00adtions of argument type contracts for top-level calls, and violation of coverage \ncontracts for conditionals: these correspond to a failed null check, a type-incorrect program, and an \ninadequately moni\u00adtored program, respectively. However, we do care about method not found errors, since \nthey would reveal that our inferred types, even for paths we have monitored, are unsound. The following \nrule (along with other versions of the monitoring rules augmented with error propagation, not shown) \nderives error judgments of the form H; E; e | f i C. (ERROR) H; E; e | f -.C H ' ; E ' ; v | f ' H ''' \n' -.C H '''' '' ; E ; e | f ; E ; l | f H '' (l)= A() C(A)= M m/. dom(M) H; E; e ' .m(e) | f iC Finally, \nthe following rule de.nes program error judgments of the form T ;F f c * c e i. (MONITOR) C = classmap(c \n* ), T , F {}, {}, monitor(e) |{} iC T ;F f c * c e i  3.3 Soundness We prove the following key soundness \ntheorem. Theorem 3.1 (Soundness). Suppose that c * c e1 .T ;F for some e1. Then there cannot be e2 such \nthat T ;F f c * c e2 i. Informally, if we infer types T with a test that covers paths F, then as long \nas we run other tests that only traverse paths in F and satisfy the type contracts in T , we cannot have \nmethod-not-found errors. In particular, this implies the following corollary. Corollary 3.2. If we infer \ntypes with a test that covers all possible paths, then our types are always sound. We sketch our proof \nbelow. Our monitoring semantics con\u00adstrains execution to follow only paths traversed in the training \nrun this guarantees that if some expression in the methods of c * is visited in the monitoring run, then \nit must be visited by the training run. Our proof is by simulation of the executions of such expres\u00adsions \nbetween these runs. We de.ne the following simulation relation between heaps and environments of the \ntraining run (marked by subscript \u00b71) and those of the monitoring run (marked by subscript \u00b72). De.nition \n3.3 (Simulation). H1; E1 simulates H2; E2 under types T , denoted by H1; E1 ~T H2; E2, iff the following \nhold: E1 ~T E2, i.e., E1(x)= .1 if and only if E2(x)= v2 such that runtypeH2 (v2) =T (type(.1)), and \nE1(self)= l1 : A if and only if E2(self)= l2 : A. H1(E1(self)) = A(F1) such that F1(@f)= .1 if and only \nif H2(E2(self)) = A(F2) such that F2(@f)= v2 and runtypeH2 (v2) =T (type(.1)).  whenever H1(l1)= A(F1) \nsuch that F1(@f )= .1 and l1 = E1(self), we have type(.1)= A.@f; and whenever H2(l2)= A(F2) such that \nF2(@f)= v2 and l2 = E2(self), we have runtypeH2 (v2) =T (type(A.@f)).  Informally, let us say that \na value v2 in the monitoring run agrees with a type t1 associated in the training run if the run-time \ntype of v2 is a subtype of the solution of t1. Then, we say that a training state (heap and environment) \nsimulates a monitoring state (heap and environment) if the values of variables in the monitoring environment \nagree with their types in the training environment; the self objects in both environments have the same \nclass; the .elds of objects in the monitoring heap agree with their types in the training heap; and for \nall objects other than self,the .elds of those objects in the training heap are of the form A.@f.Thislast \nrequirement essentially says that while self .elds may have .ow\u00adsensitive types, all other .elds must \nbe stabilized with their .ow\u00adinsensitive types. We de.ne this notion of stability for training and monitoring \nheaps below. De.nition 3.4 (Training heap stability). H1 is training-stable iff, whenever H1(l1)= A1(F1) \nsuch that F1(@f)= .1, we have type(.1)= type(A1.@f). De.nition 3.5 (Monitoring heap stability). H2 is \nmonitoring\u00adstable iff, whenever H2(l2)= A2(F2) such that F2(@f)= v2, we have runtypeH2 (v2) =T (type(A2.@f \n)). Monitoring and training heaps can be stabilized by the opera\u00adtions of constraining and wrapping, \nrespectively, as shown by the lemmas below. Recall that these operations happen upon entry and exit of \nmethod calls; thus, we can ensure that the .ow-sensitivity of .eld types does not leak across method \ncalls. This is crucial for our proof, as shown later. Lemma 3.6 (Constraining). Suppose that H1; E1 ~T \nH2; E2. Suppose that whenever t = t ' . constrainE1(self )(H1)),we have T (t ) =T (t ' ). Then H2 is \nmonitoring-stable. Lemma 3.7 (Wrapping). Suppose that H1; E1 ~T H2; E2. Then wrapE1(self )(H1) is training-stable. \nWe now have a neat way of establishing that a training state simulates a monitoring state. If both heaps \nare stabilized (say by constraining and wrapping), and furthermore, if the environments themselves satisfy \nthe requirements for simulation, then the states are related by simulation. Lemma 3.8 (Proof technique \nfor simulation). Suppose that H1 is training-stable; H2 is monitoring-stable; and E1 ~T E2. Then H1; \nE1 ~T H2; E2. This proof technique is used to show the following main lemma: if we run the same expression \nin both runs and the initial training heap simulates the initial monitoring heap, then we cannot have \nmethod-not-found errors in the monitoring run, the result of the monitoring run agrees with its type \nin the training run, and the .nal training heap simulates the .nal monitoring heap. Lemma 3.9 (Preservation). \nSuppose that c .T , F, in particular deriving H1; E1; e -.C1 ; H1' ; E1' ;: t1 | f.Let C2 = C1, T , F. \nLet H2 and E2 be such that H1; E1 ~T H2; E2. Then: We cannot have H2; E2; e iC2 .  If H2; E2; e -.C2 \nH ' 2; E2' ; v2 with f, f ' .E2 and f ' .E2 '  for some f ' ,then runtypeH2 (v2) =C2(t1) and H ' 1; \nE1 ' ~T H '' 2; E2.  Proof. The proof is by induction on the monitoring run. The only dif.cult case \nis for method calls, which we sketch below. (Note that these method calls are internal, not top-level, \nsince only envi\u00adronments for which self is de.ned can appear in the simulation.) By the induction hypothesis \nwe cannot have errors in the evalu\u00adation of the argument and the receiver; let us say they evaluate to \nv2 and l2. Furthermore, the simulation relation must hold for resulting states, say H1; E1 and H2; E2,and \nv2 and l2 must agree with the types t1 and t1 ' associated with the argument and receiver in the training \nrun, respectively. Since t1 ' is a subtype of a structural type with method m, we cannot have a method-not-found \nerror here. The main complication now is that the method dispatched in the training run may not be the \nsame as that in the monitoring run, although the latter method should be dispatched at some point in \nthe training run. Thus, to apply our induction hypothesis for the method call, we need to show that the \ninitial state for that method call in the monitoring run is simulated by the possibly unrelated initial \nstate with which that method was called in the training run. However, at this point we can apply Lemmas \n3.6, 3.7, and 3.8 to relate those states, since we constrain the initial state of the caller and wrap \nthe initial state of the callee. Thus, now by induction hypothesis, the method expression evaluates to \n(say) v2 '' , such that the simulation '' '' relation must hold for resulting states, say H1 '' ; E1 \nand H '' 2 ; E2 and '' '' v2 must agree with the type t1 associated with the result in the training run. \nFinally, as above, we apply Lemmas 3.6, 3.7, and 3.8 to show that the .nal state for the method call \nin the training run simulates the .nal state for the method call in the monitoring run, since we constrain \nthe .nal state of the callee and wrap the .nal state of the caller. Now using Lemma 3.9, we can show \nthat the following weaker state invariant is preserved at the top level of a monitoring run. De.nition \n3.10. At the top level, invariant(H2; E2) holds iff: whenever H2(l)= A(F) such that F(@f)= v2, we have \nruntypeH2 (v2) =C2(A.@f);  E2(self) is unde.ned.  Lemma 3.11. Suppose that c * c .T ;F with C1 = classmap(c \n* ). Let C2 = C1, T , F.Let H2 and E2 be such that invariant(H2; E2). Then: We cannot have H2; E2; monitor(e0) \niC2 .  If H2; E2; monitor(e0) -.C2 H2' ; E2' ; v2 then we must have invariant(H ' 2; E2' ).  Theorem \n3.1 follows by Lemma 3.11. Perhaps the most interesting thing about this theorem is that it proves we \ndo not need to train on all iterations of a loop (here formalized as recursive calls to a function), \nbut rather just all paths within the loop body. Here is an example to provide some intuition as to why. \nSuppose a recursive method walks over an array and calls baz on each element, e.g., 60 def foo(a, i) \n61 return if i == a.length 62 a[i].baz 63 foo(a, i+1) 64 end Recall that a[i] is actually a method call \nto the [] method, i.e., a[i].baz is really a .[](i, baz).If a is the array s contents type variable (i.e., \nthe array has type Array<a>), this call generates a constraint a = [baz : ...]. This constraint, generated \nfrom a single call to foo, affects all elements of a because when the array was created, we added constraints \nthat each of its element types are compatible with a, e.g., t1 = a, t2 = a, etc. where the ti are the \nelement types. (Thus the solution to a will contain t1 . t2 . ...) 65 class A 66 infer types () # A \nis an unannotated class 67 def foo ... end 68 def bar ... end 69 end 70 class String # String is an annotated \nclass 71 typesig ( insert : (Numeric, String ) . String ) 72 typesig( + : ([to s: ().String]) . String \n) 73 ... 74 end 75 class ATest 76 include Rubydust::RuntimeSystem::TestCase 77 def test 1 ... end # test \ncases called by Rubydust 78 def test 2 ... end 79 end Figure 9. Using Rubydust We can also prove the \nfollowing completeness theorem. Theorem 3.12 (Completeness). If there are N methods and a maximum of \nK labels in each method, and each label is reachable, then a program of size O(N \u00b7 2K ) is suf.cient \nto ensure that the inferred types are sound over all runs. Proof. For any method m, there can be at most \n2K paths with the labels in m; and there can be at most N such methods. In practice, the completeness \nbound N \u00b72K is likely quite loose. Of course, a tighter bound could be obtained by considering the actual \nnumber of labels K in each method. Furthermore, a method with K labels may induce far less paths than \n2K indeed, we are estimating the number of nodes in a branching tree of height K, which may even be K \nif the tree is skewed as a list, as is the case for switch statements.  4. Implementation In this section \nwe describe Rubydust, an implementation of our dynamic type inference algorithm for Ruby. Rubydust comprises \nroughly 4,500 lines of code, and is written purely in standard Ruby. Using Rubydust is straightforward, \nas illustrated with the exam\u00adple in Figure 9. To use a program with Rubydust, the programmer simply runs \nthe program as ruby rubydust.rb (program).The Ruby script rubydust.rb in turn sets up the Ruby environment \nbefore running the target program (program). For each class whose types should be inferred, the programmer \nadds a call to Rubydust s infer types method during the class de.nition (line 66). Note that in Ruby, \nclass de.nitions are executed to create the class, and hence methods can be invoked as classes are de.ned. \nFor classes with annotated types, the programmer calls Ruby\u00addust s typesig method with a string for each \ntype to be declared. For example, on line 71, we declare that String :: insert takes a Numeric and String \nand returns a String. As another example, on line 72, we declare that String ::+ takes a argument that \nhas a to s method and returns a String . We support the full type annotation language from DRuby [12], \nincluding intersection types and gener\u00adics (discussed below). Finally, the programmer also needs to de.ne \nseveral test cases, which are run when the program is executed by Rubydust. After Rubydust runs the test \ncases, it solves the generated constraints and outputs the inferred types (examples in Section 5). Next, \nwe brie.y discuss details of the instrumentation process, constraint resolution, and some limitations \nof our implementation. Instrumenting Unannotated Classes Wrapped objects v : t are implemented as instances \nof a class Proxy that has three .elds: the object that is being wrapped, its type, and the owner of the \nProxy, which is the instance that was active when the Proxy was created. When a method is invoked on \na Proxy, the object s method missing method will be called; in Ruby, if such a method is de.ned, it receives \ncalls to any unde.ned methods. Here method missing does a little work and redirects the call to the wrapped \nobject. To implement the wrapping (with Proxy) and constraint gener\u00adation operations, we use Ruby introspection \nto patch the unanno\u00adtated class. In particular, we rename the current methods of each unannotated class \nand then add method missing to perform work before and after delegating to the now-renamed method. We \nneed to patch classes to bootstrap our algorithm, as the program code we re tracking creates ordinary \nRuby objects whose method invo\u00adcations we need to intercept. On entry to and exit from a patched method, \nwe perform all of the constraint generation and wrapping, according to Figure 1. Note that we perform \nboth the caller and the callee s actions in the callee s method missing. This is convenient, because \nit allows us rely on Ruby s built-in dynamic dispatch to .nd the right callee method, whereas if we did \nwork in the caller, we would need to reimplement Ruby s dynamic dispatch algorithm. Moreover, it means \nwe can naturally handle dispatches via send, which performs re.ective method invocation. Since we are \nworking in the callee, we need to do a little ex\u00adtra work to access the caller object. Inside of each \npatched class, we add an extra .eld dispatcher that points to the Proxy object that most recently dispatched \na call to this object; we set the .eld when\u00adever a Proxy is used to invoke a wrapped-object method. Also \nrecall that each Proxy has an owner .eld, which was set to self at the time the proxy was created. Since \nwe wrap arguments and .elds when\u00adever we enter a method, this means all Proxy objects accessible from \nthe current method are always owned by self . Thus, on entry to a callee, we can .nd the caller object \nby immediately getting its dispatching Proxy,and then .nding the owner of that Proxy. Finally, notice \nthat the above discussion suggests we sometimes need to access the .elds of an object from a different \nobject. This is disallowed when trying to read and write .elds normally, but there is an escape hatch: \nwe can access .eld @f of o from anywhere by calling o. instance eval( @f ). In our formalism, we assumed \n.elds were only accessible from within the enclosing object; thus, we may be unsound for Ruby code that \nuses instance eval to break the normal access rules for .elds (as we do!). Instrumenting Annotated Classes \nAs with unannotated classes, we patch annotated classes to intercept calls to them, and we per\u00adform constraint \ngeneration and wrapping for the caller side only, as in Figure 1. We also fully unwrap any arguments \nto the patched method before delegating to the original method. We do this to support annotations on \nRuby s core library methods, which are ac\u00adtually implemented as native code and expect regular, rather \nthan Proxy-wrapped, objects. The actual method annotations for classes are stored in the class object, \nand can thus be retrieved from the patched method by inspecting self . class . For some methods, the \nproxy forwards intercepted calls to the original method, unwrap\u00adping all the arguments. For example, \nwe forward calls to eql? and hash so wrapped objects will be treated correctly within collections. Rubydust \nincludes support for polymorphic class and method types. If a class has a polymorphic type signature, \ne.g., A<t>,we instantiate its type parameters with fresh type variables whenever an instance is created. \nWe store the instantiated parameters in the instance, so that we can substitute them in when we look \nup a method signature. For methods that are polymorphic, we instantiate their type parameters with fresh \ntype variables at the call. Lastly, we also support intersection types for methods, which are common \nin the Ruby core library [12]. If we invoke o.m(x), and o.m has signature (A . B) n (C . D), we use the \nrun-time type of x to determine which branch of the intersection applies. (Recall we trust type annotations, \nso if the branches overlap, then either branch is valid if both apply.) Constraint solving and type reconstruction \nWe train a program by running it under a test suite and generating subtyping con\u00adstraints, which are \nstored in globals at run time. At the end, we check the consistency of the subtyping constraints and \nsolve them to reconstruct types for unannotated methods. The type language for reconstruction is simple, \nas outlined in Section 3; we do not try to reconstruct polymorphic or intersection types for methods. \nConsequently, the algorithms we use are fairly standard. We begin by computing the transitive closure \nof the subtyping constraints to put them in a solved form. Then, we can essentially read off the solution \nfor each type variable. First, we set method return type variables to be the union of their (immediate) \nlower bounds. Then, we set method argument type variables to be the intersection of their (immediate) \nupper bounds. These steps corre\u00adspond to .nding the least type for each method. Then we set the re\u00admaining \ntype variables to be either the union of their lower-bounds or intersection of their upper-bounds, depending \non which is avail\u00adable. Finally, we check that our solved constraints, in which type variables are replaced \nby their solutions, are consistent. Limitations There are several limitations of our current imple\u00admentation, \nbeyond what has been mentioned so far. First, for prac\u00adticality, we allow calls to methods whose classes \nare neither marked with infer types() nor provided with annotations; we do nothing special to support \nthis case, and it is up to the programmer to en\u00adsure the resulting program behavior will be reasonable. \nSecond, we do not wrap false and nil , because those two values are treated as false by conditionals, \nwhereas wrapped versions of them would be true. Thus we may miss some typing constraints. However, this \nis unlikely to be a problem, because the methods of false and nil are rarely invoked. For consistency, \nwe also do not wrap true. Third, Rubydust has no way to intercept the creation of Array and Hash literals, \nand thus initially Rubydust treats such a literal as having elements of type T.The .rst time a method \nis invoked on an Array or Hash literal, Rubydust iterates through the container elements to infer a more \nprecise type. Thus if an Array or Hash literal is re\u00adturned before one of its methods is invoked, Rubydust \nwill use the less precise T for the element type for the constraint on the return variable. (This limitation \nis an oversight we will address soon.) Fourth, for soundness, we would need to treat global variables \nsimilarly to instance and class .elds, we but do not current im\u00adplement this. Fifth, Ruby includes looping \nconstructs, and hence there are potentially an in.nite number of paths through a method body with a loop. \nHowever, as the foo example at the end of Sec\u00adtion 3 illustrates, if types of the loop-carried state \nare invariant, the number of loop iterations is immaterial as long as all internal paths are covered. \nWe manually inspected the code in our benchmarks (Section 5) and found that types are in fact invariant \nacross loops. Note that looping constructs in Ruby actually take a code block essentially a .rst-class \nmethod as the loop body. If we could as\u00adsign type variables to all the inputs and outputs of such blocks, \nwe could eliminate the potential unsoundness at loops. However, Ruby does not currently provide any mechanism \nto intercept code block creation or to patch the behavior of a code block. Sixth, we currently do not \nsupport annotations on some low\u00adlevel classes, such as IO, Thread, Exception, Proc, and Class. Also, \nif methods are de.ned during the execution of a test case, Rubydust will not currently instrument them. \nWe expect to add handling of these cases in the future. LOC TC E(#) LCov MCov P(#) PCov OT(s) RT(s) \nSolving(s) ministat-1.0.0 96 10 7 75% 11 / 15 19 84% 0.00 11.19 57.11 .nite.eld-0.1.0 103 9 6 98% 12 \n/ 12 14 93% 0.00 1.74 1.28 Ascii85-1.0.0 105 7 3 95% 2 / 2 67 28% 0.01 6.81 0.17 a-star 134 1 5 100% \n20 / 24 41 62% 0.04 114.81 37.46 hebruby-2.0.2 178 19 8 81% 20 / 26 36 91% 0.01 19.97 19.08 style-0.0.2 \n237 12 1 75% 17 / 32 88 28% 0.01 8.46 0.28 Rubyk 258 1 4 69% 15 / 20 37 68% 0.00 7.33 0.56 StreetAddress-1.0.1 \n772 1 10 79% 33 / 44 23 88% 0.02 4.45 24.58 TC -test cases E -manual edits LCov -line coverage MCov \n-method coverage / total # of methods P -paths PCov -path coverage OT -original running time RT -Rubydust \nrunning time Figure 10. Results Finally, in Rubydust, infer types () is a class-level annotation either \nall methods in a class have inferred types, or none do. How\u00ad ever, it is fairly common for Ruby programs \nto patch existing classes or inherit from annotated classes. In these cases, we would like to infer method \nsignatures for just the newly added methods; we plan to add support for doing so in the future. 5. Experiments \nWe ran Rubydust on eight small programs obtained from Ruby- Forge and Ruby Quiz [23]. We ran on a Mac \nPro with two 2.4Ghz quad core processors with 8GB of memory running Mac OS X ver\u00ad sion 10.6.5. Figure \n10 tabulates our results. The column headers are de.ned at the bottom of the .gure. The .rst group of \ncolumns shows the program size in terms of lines of code (via SLOCcount [30]), the number of test cases \ndistributed with the benchmark, and the number of manual edits made, which includes rewriting Array and \nHash literals, and inserting calls to infer types .Forthe mini\u00ad stat benchmark, two of the edits ensure \nthe test setup code is called only once across all tests; without this change, the constraint solver \n1 does not complete in a reasonable amount of time. For all bench\u00ad 2 marks, when calculating the lines \nof code, number of methods, and 3 manual edits made we excluded testing code. For style,wealso 4 did \nnot count about 2,700 lines of the source .le that occur after Ruby s END tag, which tells the interpreter \nto ignore anything below that line. In this case, those lines contain static data that the program loads \nat run-time by reading its own source .le. The next group of columns gives the line coverage from the \ntest cases (computed by rcov [21]), the method coverage, the sum of the number paths in covered methods, \nand the percentage of these paths covered by the test cases. As rcov does not compute path coverage, \nwe determined the last two metrics by hand for each method, we inserted print statements on every branch \nof the program to record which branches were taken at run time. We then manually analyzed this information \nto determine path coverage. When considering paths, we tried to disregard infeasible paths (e.g., conditionals \nwith 1 overlapping guards) and paths that only .agged errors. The path 2 coverage is generally high, \nwith the exception of Ascii85 and style, 3 which have long methods with sequences of conditionals that \ncreate 4 an exponential number of paths. 5 We manually inspected the source code of the benchmark pro\u00ad \ngrams, and we determined that for the methods that were covered, the type annotations inferred by Rubydust \nare correct. It is interest\u00ad ing that the annotations are correct even for the two programs with low \npath coverage; this suggests that for these programs, reason\u00ad able line coverage is suf.cient to infer \nsound types. Rubydust also found one type error, which we discuss below. Performance The last group of \ncolumns in Figure 10 reports Rubydust s running time, which we split into the time to instru\u00ad ment and \nrun the instrumented program, and the time to solve the 1 generated constraints. As we can see, the overhead \nof running un\u00ad 2 der Rubydust, even excluding solving time, is quite high compared 3 to running the \noriginal, uninstrumented program. Part of the reason is that we have not optimized our implementation, \nand our heavy use of wrappers likely impedes fast paths in the Ruby interpreter. For example, values \nof primitive types like numbers are not really implemented as objects, but we wrap them with objects. \nNeverthe\u00adless, we believe that some overhead is acceptable because inference need not be performed every \ntime the program is run. The solving time is high, but that is likely because our solver is written in \nRuby, which is known to be slow (this is being addressed in newer versions of Ruby). We expect this solving \ntime would decrease dramatically if we exported the constraints to a solver written in a different language. \nInferred Types We now describe the benchmark programs and show some example inferred types, as output \nby Rubydust. Ministat generates simple statistical information on numerical data sets. As an example \ninferred type, consider the median method, which computes a median from a list of numbers: median: ([ \nsort !: () . Array<Numeric>; size: () . Numeric; [] : (Numeric) . Numeric]) . Numeric The method takes \nan object that has sort!, size ,and [] methods, and returns a Numeric. Thus, one possible argument would \nbe an Array of Numeric. However, this method could be used with other arguments that have those three \nmethods indeed, because Ruby is dynamically typed, programmers are rarely required to pass in objects \nof exactly a particular type, as long as the passed-in objects have the right methods (this is referred \nto as duck typing in the Ruby community). Finite.eld, another mathematical library, provides basic opera\u00adtions \non elements in a .nite .eld. As an example type, consider the inverse method, which inverts a matrix: \ninverse : ([ > : (Numeric) . Boolean; \u00ab : (Numeric) . Numeric; \u00bb : (Numeric) . Numeric; &#38; : (Numeric) \n. Numeric; : (Numeric) . Numeric]) . Numeric ) As above, we can see exactly which methods are required \nof the argument; one concrete class that has all these methods is Numeric. Ascii85 encodes and decodes \ndata following Adobe s binary-to\u00adtext Ascii85 format. There are only two methods in this program, both \nof which are covered by the seven test cases. Rubydust issues an error during the constraint solving, \ncomplaining that Boolean is not a subtype of [to i:() . Numeric]. The offending parts of the code are \nshown below. module Ascii85 def self .encode(str, wrap lines =80) ... if (! wrap lines) then ... return \nend 4 ... wrap lines . to i 5 end ... 6 end The author of the library uses wrap lines as an optional \nargument, with a default value of 80. In one test case, the author passes in false , hence wrap lines \nmaybe a Boolean.Butas Boolean does not have a to i method, invoking wrap lines . to i is a type error. \nFor example, passing true as the second argument will cause the program to crash. It is unclear whether \nthe author intends to allow true as a second argument, but clearly wrap lines can potentially be an arbitrary \nnon-integer, since its to i method is invoked (which would not be necessary for an integer). A-star is \na solution for the A* search problem. It includes a class Tile to represent a position on a two-dimensional \nMap. Rubydust infers that two of the methods to Tile have types: 1 y: () . Numeric 2 x: () . Numeric \nThe Map class uses Tile , which we can see from the inferred types for two of Map s methods: 1 adjacent: \n([y: () . Numeric; x: () . Numeric]) . Array<Tile> 2 .nd route : () . 3 [push: ([y: () . Numeric; x: \n() . Numeric]) . Array<Tile>; 4 include ?: ([y: () . Numeric; x: () . Numeric]) . Boolean; 5 pop: () \n. [y: () . Numeric; x : () . Numeric]; 6 to ary: () . Array<Tile>] The adjacent method computes a set \nof adjacent locations (rep\u00adresented as an Array<Tile>) given an input Tile .The .nd route method returns \nan array of Tile , which is an instance of the slightly more precise structural type Rubydust infers. \n(Here the structural type is actually the type of a .eld returned by .nd route .) Hebruby is a program \nthat converts Hebrew dates to Julian dates and vice versa. One method type we inferred is 1 leap?: ([ \n * : (Numeric) . Numeric]) . Boolean This method determines whether the year is a leap year. This signature \nis interesting because the argument only needs a single method, *, which returns a Numeric. This is the \nonly requirement because subsequent operations are on the return value of *,rather than on the original \nmethod argument. Rubyk is a Rubik s Cube solver; as this program did not come with a test suite, we constructed \na test case ourselves (one of the four edits we made to the program). Some of the inferred types are \nshown below. 1 visualize :() . Array<Array<String>> 2 initialize :() . Array<Array<String>> We can see \nthat the visualize and initialize methods return a representation of the cube faces as an Array<Array<String>>. \nStyle is a surface-level analysis tool for English. The program has several methods that determine characteristics \nof English text. Some sample inferred types are: 1 pronoun? :([downcase : () . String]) . Boolean 2 \npassive? :([ each : () . Array<String>]) . Boolean 3 preposition? :([downcase : () . String]) . Boolean \n4 abbreviation? :([downcase : () . String]) . Boolean 5 nominalization? :([ downcase : () . String]) \n. Boolean 6 interrogativepronoun? :([downcase : () . String]) . Boolean 7 normalize :([ downcase : () \n. String]) . String Here, most of the methods .rst downcase their argument, and then use the resulting \nString .The passive method takes an array of Strings , representing a sentence, and iterates through \nthe array to determine whether the sentence is in the passive voice. Finally, StreetAddress is a tool \nthat normalizes U.S. street ad\u00address into different subcomponents. As an example, the parse class method \ntakes a String and returns an instance of the class, which is depicted in the following output: 1 class \n<< StreetAddress::US; # de.ne a class method 2 parse:(String) . StreetAddress::US::Address 3 end Notes \nWe encountered all of Rubydust s limitations in these benchmark programs there were tests that called \nout to unan\u00adnotated and uninferred classes; that use false and nil ;thatuse global variables; and that \nuse unannotated low-level classes like IO. However, as already mentioned, the types Rubydust inferred \nwere correct, and so these limitations did not affect the results. We also encountered the use of re.ective \nmethod invocation via send described in Section 2.5. We did not encounter uses of other dynamic features \nfor classes whose types are inferred (though they do occur internally in the standard library). 6. Related \nwork There has been signi.cant interest in the research community in bringing static type systems to \ndynamic languages. Much recent research has developed ways to mix typed and untyped code, e.g., via quasi-static \ntyping [26], occurrence typing [29], gradual typing [25], and hybrid typing [13]. In these systems, types \nare supplied by the user. In contrast, our work focuses on type inference, which is complementary: we \ncould potentially use our dynamic type in\u00adference algorithm to infer type annotations for future checking. \nSeveral researchers have explored type inference for object\u00adoriented dynamic languages, including Ruby \n[2, 3, 10, 12, 16, 18], Python [4, 7, 24], and JavaScript [5, 27], among others. As dis\u00adcussed in the \nintroduction, these languages are complex and have subtle semantics typically only de.ned by the language \nimplemen\u00adtation. This makes it a major challenge to implement and maintain a static type inference system \nfor these languages. We experienced this .rsthand in our development of DRuby, a static type inference \nsystem for Ruby [3, 10, 12]. There has also been work on type inference for Scheme [31], a dynamic language \nwith a very compact syntax and semantics; however, these inference systems do not support objects. Dynamic \ntype inference has been explored previously in several contexts. Rapicault et al. describe a dynamic \ntype inference algo\u00adrithm for Smalltalk that takes advantage of introspection features of that language \n[20]. However, their algorithm is not very clearly ex\u00adplained, and seems to infer types for variables \nbased on what types are stored in that variable. In contrast, we infer more general types based on usage \nconstraints. For example, back in Figure 2, we dis\u00adcovered argument x must have a qux method, whereas \nwe believe the approach of Rapicault et al would instead infer x has type B, which is correct, but less \ngeneral. Guo et al. dynamically infer abstract types in x86 binaries and Java bytecode [14]. Artzi et \nal. propose a combined static and dy\u00adnamic mutability inference algorithm [6]. In both of these systems, \nthe inferred types have no structure in the former system, abstract types are essentially tags that group \ntogether values that are related by the program, and in the latter system, parameters and .elds are either \nmutable or not. In contrast, our goal is to infer more standard structural or nominal types. In addition \nto inferring types, dynamic analysis has been pro\u00adposed to discover many other program properties. To \ncite three ex\u00adamples, Daikon discovers likely program invariants from dynamic runs [9]; DySy uses symbolic \nexecution to infer Daikon-like in\u00advariants [8]; and Perracotta discovers temporal properties of pro\u00adgrams \n[32]. In these systems, there is no notion of suf.cient cov\u00aderage to guarantee sound results. In contrast, \nwe showed we can soundly infer types by covering all paths through each method. There are several dynamic \ninference systems that, while they have no theorems about suf.cient coverage, do use a subsequent checking \nphase to test whether the inferred information is sound. Rose et al. [22] and Agarwal and Stoller [1] \ndynamically infer types that protect against races. After inference the program is annotated and passed \nto a type checker to verify that the types are sound. Similarly, Nimmer and Ernst use Daikon to infer \ninvariants that are then checked by ESC/Java [19]. We could follow a similar approach to these systems \nand apply DRuby to our inferred types (when coverage is known to be incomplete); we leave this as future \nwork. Finally, our soundness theorem resembles soundness for Mix,a static analysis system that mixes \ntype checking and symbolic exe\u00adcution [15]. In Mix, blocks are introduced to designate which code should \nbe analyzed with symbolic execution, and which should be analyzed with type checking. At a high-level, \nwe could model our dynamic inference algorithm in Mix by analyzing method bod\u00adies with symbolic execution, \nand method calls and .eld reads and writes with type checking. However, there are several important differences: \nWe use concrete test runs, where Mix uses symbolic execution; we operate on an object-oriented language, \nwhere Mix applies to a conventional imperative language; and we can model the heap more precisely than \nMix, because in our formal language, .elds are only accessible from within their containing objects. \n 7. Conclusion In this paper we presented a new technique, constraint-based dy\u00adnamic type inference, \nthat infers types based on dynamic execu\u00adtions of the program. We have proved that this technique infers \nsound types as long as all possible paths through each method are traversed during inference. We have \ndeveloped Rubydust, an im\u00adplementation of our technique for Ruby, and have applied it to a number of \nsmall Ruby programs to .nd a real error and to accu\u00adrately infer types in other cases. We expect that \nfurther engineering of our tool will improve its performance. We also leave the infer\u00adence of more advanced \ntypes, including polymorphic and intersec\u00adtion types, to future work.  Acknowledgments We would like \nto thank Mark T. Daly for his work on the type anno\u00adtation parser. Most of this work was performed while \nthe .rst and second authors were at the University of Maryland, College Park. This work was supported \nin part by DARPA ODOD.HR00110810\u00ad073, NSF CCF-0346982, CCF-0541036, and CCF-0915978.  References [1] \nRahul Agarwal and Scott D. Stoller. Type Inference for Parameterized Race-Free Java. In VMCAI, 2004. \n[2] O. Agesen, J. Palsberg, and M.I. Schwartzbach. Type Inference of SELF. ECOOP, 1993. [3] Jong-hoon \n(David) An, Avik Chaudhuri, and Jeffrey S. Foster. Static Typing for Ruby on Rails. In ASE, 2009. [4] \nDavide Ancona, Massimo Ancona, Antonio Cuni, and Nicholas Mat\u00adsakis. RPython: Reconciling Dynamically \nand Statically Typed OO Languages. In DLS, 2007. [5] Christopher Anderson, Paola Giannini, and Sophia \nDrossopoulou. To\u00adwards Type Inference for JavaScript. In ECOOP, 2005. [6] Shay Artzi, Adam Kiezun, David \nGlasser, and Michael D. Ernst. Combined static and dynamic mutability analysis. In ASE, 2007. [7] John \nAycock. Aggressive Type Inference. In International Python Conference, 2000. [8] Christoph Csallner, \nNikolai Tillmann, and Yannis Smaragdakis. Dysy: dynamic symbolic execution for invariant inference. In \nICSE, 2008. [9] Michael D. Ernst, Jeff H. Perkins, Philip J. Guo, Stephen McCamant, Carlos Pacheco, Matthew \nS. Tschantz, and Chen Xiao. The Daikon system for dynamic detection of likely invariants. Sci. Comput. \nPro\u00adgram., 69(1-3), 2007. [10] Michael Furr, Jong-hoon (David) An, and Jeffrey S. Foster. Pro.le\u00adguided \nstatic typing for dynamic scripting languages. In OOPSLA, 2009. [11] Michael Furr, Jong-hoon (David) \nAn, Jeffrey S. Foster, and Michael Hicks. The Ruby Intermediate Language. In Dynamic Language Symposium, \n2009. [12] Michael Furr, Jong-hoon (David) An, Jeffrey S. Foster, and Michael Hicks. Static Type Inference \nfor Ruby. In OOPS Track, SAC, 2009. [13] J. Gronski, K. Knowles, A. Tomb, S.N. Freund, and C. Flanagan. \nSage: Hybrid Checking for Flexible Speci.cations. Scheme and Functional Programming, 2006. [14] Philip \nJ. Guo, Jeff H. Perkins, Stephen McCamant, and Michael D. Ernst. Dynamic inference of abstract types. \nIn ISSTA, 2006. [15] Yit Phang Khoo, Bor-Yuh Evan Chang, and Jeffrey S. Foster. Mixing type checking \nand symbolic execution. In PLDI, 2010. [16] Kristian Kristensen. Ecstatic Type Inference for Ruby Using \nthe Cartesian Product Algorithm. Master s thesis, Aalborg University, 2007. [17] Robin Milner. A Theory \nof Type Polymorphism in Programming. Journal of Computer and System Sciences, 17, 1978. [18] Jason Morrison. \nType Inference in Ruby. Google Summer of Code Project, 2006. [19] Jeremy W. Nimmer and Michael D. Ernst. \nInvariant Inference for Static Checking: An Empirical Evaluation. In FSE, 2002. [20] Pascal Rapicault, \nMireille Blay-Fornarino, St\u00b4ephane Ducasse, and Anne-Marie Dery. Dynamic type inference to support object-oriented \nreenginerring in Smalltalk. In ECOOP Workshops, 1998. [21] rcov. rcov: code coverage for ruby, 2010. \nhttp://eigenclass. org/hiki/rcov. [22] James Rose, Nikhil Swamy, and Michael Hicks. Dynamic inference \nof polymorphic lock types. Sci. Comput. Program., 58(3), 2005. [23] Ruby Quiz, 2010. http://www.rubyquiz.com. \n[24] Michael Salib. Starkiller: A Static Type Inferencer and Compiler for Python. Master s thesis, MIT, \n2004. [25] Jeremy G. Siek and Walid Taha. Gradual typing for functional lan\u00adguages. In Scheme and Functional \nProgramming Workshop, 2006. [26] Satish Thatte. Quasi-static typing. In POPL, 1990. [27] Peter Thiemann. \nTowards a type system for analyzing javascript programs. In ESOP, 2005. [28] Dave Thomas, Chad Fowler, \nand Andy Hunt. Programming Ruby: The Pragmatic Programmers Guide. Pragmatic Bookshelf, 2004. [29] Sam \nTobin-Hochstadt and Matthias Felleisen. The Design and Imple\u00admentation of Typed Scheme. In POPL, 2008. \n[30] David A. Wheeler. Sloccount, August 2004. http://www. dwheeler.com/sloccount/. [31] A.K. Wright \nand R. Cartwright. A practical soft type system for Scheme. ACM TOPLAS, 19(1), 1997. [32] Jinlin Yang, \nDavid Evans, Deepali Bhardwaj, Thirumalesh Bhat, and Manuvir Das. Perracotta: mining temporal API rules \nfrom imperfect traces. In ICSE, 2006.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>There have been several efforts to bring static type inference to object-oriented dynamic languages such as Ruby, Python, and Perl. In our experience, however, such type inference systems are extremely difficult to develop, because dynamic languages are typically complex, poorly specified, and include features, such as eval and reflection, that are hard to analyze.</p> <p>In this paper, we introduce <i>constraint-based dynamic type inference</i>, a technique that infers static types based on dynamic program executions. In our approach, we wrap each run-time value to associate it with a type variable, and the wrapper generates constraints on this type variable when the wrapped value is used. This technique avoids many of the often overly conservative approximations of static tools, as constraints are generated based on how values are used during actual program runs. Using wrappers is also easy to implement, since we need only write a constraint resolution algorithm and a transformation to introduce the wrappers. The best part is that we can eat our cake, too: our algorithm will infer sound types as long as it observes every path through each method body---note that the number of such paths may be dramatically smaller than the number of paths through the program as a whole.</p> <p>We have developed Rubydust, an implementation of our algorithm for Ruby. Rubydust takes advantage of Ruby's dynamic features to implement wrappers as a language library. We applied Rubydust to a number of small programs and found it to be both easy to use and useful: Rubydust discovered 1 real type error, and all other inferred types were correct and readable.</p>", "authors": [{"name": "Jong-hoon (David) An", "author_profile_id": "81438594841", "affiliation": "Epic Systems Corporation, Verona, WI, USA", "person_id": "P2509651", "email_address": "rockalizer@gmail.com", "orcid_id": ""}, {"name": "Avik Chaudhuri", "author_profile_id": "81309507612", "affiliation": "Advanced Technology Labs, Adobe Systems, San Jose, CA, USA", "person_id": "P2509652", "email_address": "achaudhu@adobe.com", "orcid_id": ""}, {"name": "Jeffrey S. Foster", "author_profile_id": "81338488852", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P2509653", "email_address": "jfoster@cs.umd.edu", "orcid_id": ""}, {"name": "Michael Hicks", "author_profile_id": "81100060959", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P2509654", "email_address": "mwh@cs.umd.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926437", "year": "2011", "article_id": "1926437", "conference": "POPL", "title": "Dynamic inference of static types for ruby", "url": "http://dl.acm.org/citation.cfm?id=1926437"}