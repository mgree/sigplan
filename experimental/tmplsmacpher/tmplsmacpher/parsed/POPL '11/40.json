{"article_publication_date": "01-26-2011", "fulltext": "\n Laws of Order: Expensive Synchronization in Concurrent Algorithms Cannot be Eliminated Hagit Attiya \nRachid Guerraoui Danny Hendler Technion EPFL Ben-Gurion University hagit@cs.technion.il rachid.guerraoui@ep..ch \nhendlerd@cs.bgu.ac.il Petr Kuznetsov Maged M. Michael Martin Vechev TU Berlin/Deutsche Telekom Labs IBM \nT. J. Watson Research Center IBM T. J. Watson Research Center pkuznets@acm.org magedm@us.ibm.com mtvechev@us.ibm.com \n Abstract Building correct and ef.cient concurrent algorithms is known to be a dif.cult problem of fundamental \nimportance. To achieve ef\u00ad.ciency, designers try to remove unnecessary and costly synchro\u00adnization. However, \nnot only is this manual trial-and-error process ad-hoc, time consuming and error-prone, but it often \nleaves design\u00aders pondering the question of: is it inherently impossible to elimi\u00adnate certain synchronization, \nor is it that I was unable to eliminate it on this attempt and I should keep trying? In this paper we \nrespond to this question. We prove that it is im\u00adpossible to build concurrent implementations of classic \nand ubiqui\u00adtous speci.cations such as sets, queues, stacks, mutual exclusion and read-modify-write operations, \nthat completely eliminate the use of expensive synchronization. We prove that one cannot avoid the use \nof either: i) read-after\u00adwrite (RAW), where a write to shared variable A is followed by a read to a different \nshared variable B without a write to B in between, or ii) atomic write-after-read (AWAR), where an atomic \noperation reads and then writes to shared locations. Unfortunately, enforcing RAW or AWAR is expensive \non all current mainstream processors. To enforce RAW, memory ordering also called fence or barrier instructions \nmust be used. To enforce AWAR, atomic instructions such as compare-and-swap are required. However, these \ninstruc\u00adtions are typically substantially slower than regular instructions. Although algorithm designers \nfrequently struggle to avoid RAW and AWAR, their attempts are often futile. Our result characterizes \nthe cases where avoiding RAW and AWAR is impossible. On the .ip side, our result can be used to guide \ndesigners towards new algorithms where RAW and AWAR can be eliminated. Categories and Subject Descriptors \nD.1.3 [Concurrent Pro\u00adgramming]; E.1[Data]: Data Structures General Terms Algorithms, Theory Keywords \nConcurrency, Algorithms, Lower Bounds, Memory Fences, Memory Barriers Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 1. Introduction The design \nof concurrent applications that avoid costly synchro\u00adnization patterns is a cardinal programming challenge, \nrequiring consideration of algorithmic concerns and architectural issues with implications to formal \ntesting and veri.cation. Two common synchronization patterns that frequently arise in the design of concurrent \nalgorithms are read after write (RAW) and atomic write after read (AWAR). The RAW pattern consists of \na process writing to some shared variable A, followed by the same process reading a different shared \nvariable B, without that process writing to B in between. The AWAR pattern consists of a process reading \nsome shared variable followed by the process writing to a shared variable (the write could be to the \nsame shared variable as the read), where the entire read\u00adwrite sequence is atomic. Examples of the AWAR \npattern include read-modify-write operations such as a Compare-and-Swap [26] (CAS). Unfortunately, on \nall mainstream processor architectures, the RAW and AWAR patterns are associated with expensive instruc\u00adtions. \nModern processor architectures use relaxed memory mod\u00adels, where guaranteeing RAW order among accesses \nto indepen\u00addent memory locations requires the execution of memory order\u00ading instructions often called \nmemory fences or memory barriers that enforce RAW order.1 Guaranteeing the atomicity of AWAR requires \nthe use of atomic instructions. Typically, fence and atomic instructions are substantially slower than \nregular instructions, even under the most favorable caching conditions. Due to these high overheads, \ndesigners of concurrent algorithms aim to avoid both RAW and AWAR patterns. However, such at\u00adtempts are \noften unsuccessful: in many cases, even after multiple attempts, it turns out impossible to avoid these \npatterns while en\u00adsuring correctness of the algorithm. This raises an interesting and important practical \nquestion: Can we discover and formalize the conditions under which avoiding RAW and AWAR, while ensuring \ncorrectness, is futile? In this paper, we answer this question formally. We show that implementations \nof a wide class of concurrent algorithms must involve RAW or AWAR. In particular, we focus on two widely \nused 1 RAW order requires the use of explicit fences or atomic instructions even on strongly ordered \narchitectures (e.g., X86 and SPARC TSO) that automatically guarantee other types of ordering (read after \nread, write after read, and write after write). speci.cations: linearizable objects [23] and mutual \nexclusion [11]. Our results are applicable to any algorithm claiming to satisfy these speci.cations. \nMain Contributions. The main contributions of this paper are the following: We prove that it is impossible \nto build a linearizable implemen\u00adtation of a strongly non-commutative method that satis.es a de\u00adterministic \nsequential speci.cation, in a way that sequential ex\u00adecutions of the method are free of RAW and AWAR \n(Section 5).  We prove that common methods on ubiquitous and funda\u00admental abstract data types such as \nsets, queues, work-stealing queues, stacks, and read-modify-write objects are strongly non-commutative \nand are subject to our results (Section 6).  We prove that it is impossible to build an algorithm that \nsatis.es mutual exclusion, is deadlock-free and avoids both RAW and AWAR (Section 4).  Practical Implications. \nOur results have several implications: Designers of concurrent algorithms can use our results to de\u00adtermine \nwhen looking for a design without RAW and AWAR is futile. Conversely, our results indicate when avoidance \nof these patterns may be possible.  For processor architects, our result indicates the importance of \noptimizing the performance of atomic operations such as compare-and-swap and RAW fence instructions, \nwhich have historically received little attention for optimization.  For synthesis and veri.cation of \nconcurrent algorithms, our result is potentially useful in the sense that a synthesizer or a veri.er \nneed not generate or attempt to verify algorithms that do not use RAW and AWAR for they are certainly \nincorrect.  The remainder of the paper is organized as follows. We present an overview of our results \nwith illustrative examples in Section 2. In Section 3, we present the necessary formal machinery. We \npresent our result for mutual exclusion in Section 4 and for lin\u00adearizable objects in Section 5. In Section \n6, we show that many widely used speci.cations satisfy the conditions outlined in Sec\u00adtion 5 and hence \nare subject to our result. We discuss related work in Section 7 and conclude the paper with Section 8. \n 2. Overview In this section we explain our results informally, give an intuition of the formal proof \npresented in later sections and show concurrent algorithms that exemplify our result. As mentioned already, \nour result focuses on two practical spec\u00adi.cations for concurrent algorithms: mutual exclusion [11, 31] \nand linearizability [23]. Informally, our result states that if we are to build a mutual exclusion algorithm \nor a linearizable algorithm, then in certain sequential executions of that algorithm, we must use either \nRAW or AWAR. That is, if all executions of the algorithm do not use RAW or AWAR, then the algorithm is \nincorrect. 2.1 Mutual Exclusion Consider the classic mutual exclusion template shown in Fig. 1. Here \nwe have N processes (N> 1), with each process acquiring a lock, entering the critical section, and .nally \nreleasing the lock. The speci.cation for mutual exclusion states that we cannot have multiple processes \nin their critical section at the same time. The template does not show the actual code that each process \nmust execute in its lock, critical, and unlock sections. Further, the code executed by different processes \nneed not be identical. Process 0: Process 1: Process N-1: lock0: ... lock1: ... ..... lockN-1: ... CS0: \n... CS1: ... ..... CSN-1: ... unlock0: ... unlock1: ... ..... unlockN-1: ... Figure 1. N-process mutual \nexclusion template, for N> 1. locki: while ( \u00acCAS(Lock,FREE,BUSY));  Figure 2. Illustrating AWAR: a \nsimpli.ed snippet of a test\u00adand-set lock acquire. locki: .ag[i] = true; while (.ag[\u00aci])... Figure 3. \nIllustrating RAW: simpli.ed snippet from the lock section of Dekker s 2-way mutual exclusion algorithm \n(here 0 = i< 2). Our result states that whenever a process has sequentially exe\u00adcuted its lock section, \nthen this execution must use RAW or AWAR. Otherwise, the algorithm does not satisfy the mutual exclusion \nspeci.cation and is incorrect. 2.1.1 Informal Proof Explanation Let us now give an intuition for the \nproof on a simpli.ed case where the system is in its initial state, i.e., all processes are just about \nto enter their respective lock sections, but have not yet done so. Let us pick an arbitrary process i, \n0 = i<N, and let process i sequentially execute its locki section, enter the critical section CSi, and \nthen stop. Let us assume that process i did not perform a shared write when it executed its locki section. \nNow, let us select another process j .i, 0 = j<N.As = process i did not write to the shared state, there \nis no way for process j to know where process i is. Therefore, process j can fully execute its own lockj \nsection and enter the critical section CSj . Now, both processes are inside the critical section, violating \nmutual exclusion. Therefore we have shown that each process must perform a shared write in its lock section. \nLet us now repeat the same exercise and assume that all pro\u00adcesses are in the initial state, where they \nare all just about to enter their respective lock sections, but have not yet done so. We know that each \nprocess must write to shared memory in the sequential ex\u00adecution of its lock section. Let us again pick \nprocess i to execute its locki section sequentially. Assume that process i writes to shared location \nnamed X. Now, let us assume that the locki section is ex\u00adecuted by process i sequentially without using \nRAW and AWAR. Since there is no AWAR, it means that the write to X cannot be executed atomically with \na previous shared read (be it a read from X or another shared location). There could still be a shared \nread in locki that precedes the write to X, but that read cannot execute atomically with the write to \nX. Let us now have process i exe\u00adcute until it is about to perform its .rst shared write operation, and \nthen stop. Now, let process j perform a full sequential execution of its lockj section (this is possible \nas process i has not yet written to shared memory so process j is not perturbed). Process j now enters \nits critical section CSj and stops. Process i now resumes its locki section and immediately performs \nthe shared write to X.Oncepro\u00adcess i writes to X, it over-writes any changes to X that process j made. \nThis means that if process i is to know where process j is, it must read a shared memory location other \nthan X.However,we assumed that there is no RAW which means that process i can\u00adnot read a shared location \nother than X, without previously having written to that location. In turn, this implies that process \ni cannot observe where process j is, that is, process j cannot in.uence the execution of process i. Hence, \nprocess i continues and completes its locki section and enters its critical section, leading to a violation \nof mutual exclusion. Therefore, any sequential execution of a lock section requires the use of either \nAWAR or RAW.  2.1.2 Examples Here, we show several examples of mutual exclusion algorithms that indeed \nuse either RAW or AWAR in their lock sections. These examples are speci.c implementations that highlight \nthe applica\u00adbility of our result, namely that implementation of algorithms that satisfy the mutual exclusion \nspeci.cation cannot avoid both RAW and AWAR. One of the most common lock implementations is based on \nthe test-and-set atomic sequence. Its lock acquire operation boils down to an AWAR pattern, by using \nan atomic operation, e.g., CAS, to atomically read a lock variable, check that it represents a free lock, \nand if so replace it with an indicator of a busy lock. Fig. 2 shows a simpli.ed version of a test-and-set-lock. \nSimilar pattern is used in all other locks that require the use of read-modify-write atomic operations \nin every lock acquire [2, 18, 36]. On the other hand, a mutual exclusion lock algorithm that avoids AWAR \n[6, 11, 41], must use RAW. For example, Fig. 3 shows a simpli.ed snippet from the lock section of Dekker \ns algorithm [11] for 2-process mutual exclusion. A process that succeeds in entering its critical section \nmust .rst raise its own .ag and then read the other .ag to check that the other process s .ag is not \nraised. Thus, the lock section involves a RAW pattern.  2.2 Linearizability The second part of our result \ndiscusses linearizable algorithms [23]. Intuitively, an algorithm is linearizable with respect to a se\u00adquential \nspeci.cation if each execution of the algorithm is equiva\u00adlent to some sequential execution of the speci.cation, \nwhere the or\u00adder between the non-overlapping methods is preserved. The equiv\u00adalence is de.ned by comparing \nthe arguments and results of method invocations. Unlike mutual exclusion where all sequential executions \nof a certain method (i.e., the lock section) must use either RAW or AWAR, in the case of linearizability, \nonly some sequential execu\u00adtions of speci.c methods must use either RAW or AWAR. We quan\u00adtify these methods \nand their executions in terms of properties on se\u00adquential speci.cations. Any algorithm implementation \nthat claims to satisfy these properties on the sequential speci.cations is subject to our results. The \ntwo properties are: Deterministic sequential speci.cations: Informally, we say that a sequential speci.cation \nis deterministic if a method executes from the same state will always produce the same result. Many classic \nabstract data types have deterministic speci.cations: sets, queues, etc.  Strongly non-commutative methods: \nInformally, a method m1 is said to be strongly non-commutative if there exists some state in the speci.cation \nfrom which m1 executed sequentially by process p can in.uence the result of a method m2 executed sequentially \nby process q, q .p, and vice versa, can  = m2 in.uence the result of m1 from the same state. Note that \nm1 and m2 are performed by different processes. {S = A} contains(k) {ret = k .A.S = A} {S = A} add(k) \n{ret = k .= A.{k}} .A.S {S = A} remove(k) {ret = k .A.S = A\\{k}} Figure 4. Sequential speci.cation of \na set. S. N denotes the contents of the set. ret denotes the return value.    Figure 5. Illustration \nof the reasoning for why RAW is required in linearizable algorithms. Our result states that if we have \nan implementation of a strongly non-commutative method m, then there are some sequential execu\u00adtions \nof m that must use RAW or AWAR. That is, if all sequential executions of m do not use RAW or AWAR, then \nthe algorithm im\u00adplementation is not linearizable with respect to the given sequential speci.cation. \nLet us illustrate these concepts with an example: a Hoare-style sequential speci.cation of a classic \nSet, shown in Fig. 4 where each method can be executed by more than one process. First, this simple sequential \nspeci.cation is deterministic: if an add, remove or contains execute from a given state, they will always \nreturn the same result. Second, both methods, add and remove are strongly non\u00adcommutative. For add, there \nexists an execution of the speci.ca\u00adtion by a process such that add can in.uence the result of add which \nis executed by another process. For example, let us begin with S = \u00d8. Then, if process p performs an \nadd(5),itwillre\u00adturn true and a subsequent add(5) will return false.However, if we change the order, \nand the second add(5) executes .rst, then it will return true while the .rst add(5) will return false. \nThat is, add is a strongly non-commutative method as there ex\u00adists another method where both method invocations \nin.uence each other s result starting from some state (i.e.,S = \u00d8). In this case it happens to be another \nadd method, but in general the two meth\u00adods could be different. Similar reasoning shows why remove is \nstrongly non-commutative. However, contains is not a strongly non-commutative method, as even though \nits result can be in.u\u00adenced by a preceding add or remove, its execution cannot in\u00ad.uence the result \nof any of the three methods add, remove or contains, regardless of the state from which contains starts \nexecuting. For the Set speci.cation, our result states that any linearizable implementation of the strongly \nnon-commutativie methods add and remove must use RAW or AWAR in some sequential exe\u00adcution of the implementation. \nFor example, let us consider a se\u00adquential execution of add(k) starting from a state where k . .S. Then \nthis sequential execution must use RAW or AWAR. However, our result does not apply to the sequential \nexecution of add(k) where k .S. In that case, regardless of whether add(k) is per\u00adformed, the result \nof any other subsequent method performed right after add(k) is unaffected. 2.2.1 Informal Proof Explanation \nThe proof steps in the case of linearizable implementations are very similar to the ones already outlined \nin the case of mutual exclusion implementations. Intuitively, if a method is strongly non\u00adcommutative, \nthen any of its sequential executions must perform a shared write. Otherwise, there is no way for the \nmethod to in.uence the result of any other method that is executed after it, and hence the method cannot \nbe strongly non-commutative. Let us illustrate how we reason about why RAW or AWAR should be present \non our set example. By contradiction, let us assume that RAW and AWAR are not present. Consider the concurrent \nexecution in Fig. 5. Here, some pre.x of the execution marked as H has completed and at the end of H, \nk . .S. Then, process p invokes method add(k) and executes it up to the .rst shared write (to a location \ncalled X), and then p is preempted. Then, another process q performs a full sequential execution of add(k) \n(for the same k) which returns true.After that, p resumes its execution and immediately performs the \nshared write, and completes its execution of add(k) andalsoreturns true. The reason why both returned \ntrue is similar to the case for mutual exclusion: the write to X by p overwrites any writes to X that \nq has made and as we assumed that RAW is not allowed, it follows that process p cannot read any locations \nother than X in its subsequent steps without having previously written to them. Hence, both add(k) s \nreturn the same value true. Now, if the algorithm is linearizable, there could only be two valid linearizations \nas shown in Fig. 5. However, it is easy to see that both linearizations are incorrect as they do not \nconform to the speci.cation: if k . .S at the end of H, then according to the set speci.cation, executing \ntwo add(k) s sequentially in a row can\u00adnot lead to both add(k) s returning the same result. Therefore, \neither RAW or AWAR must be present in some sequential execu\u00adtions of add(k). More generally, as we will \nsee in Section 5, we show this for any deterministic speci.cation, not only for sets. We will see that \nthe central reason why both linearizations are not allowed is because the result of add(k) executed by \nprocess q is not in.uenced by the preceding add(k) executed by process p, violating the assumption that \nadd() is a strongly non-commutative method.  2.2.2 Practical Implications While our result shows when \nit is impossible to eliminate both RAW and AWAR, the result can also be used to guide the search for \nlinearizable algorithms where it may be possible to eliminate RAW and AWAR, by changing one or more of \nthe following dimensions: Deterministic Speci.cation: change the sequential speci.ca\u00adtion, perhaps by \nconsidering non-deterministic speci.cations.  Strong Non-Commutativity: focus on methods that are not \nstrongly non-commutative, i.e.,contains instead of add.  Single-Owner: restrict the speci.cation such \nthat a method can only be performed by a single process, instead of multiple processes (as we will see \nlater, technically, this is also part of the strong non-commutativity de.nition).  bool WFCAS(Val ev, \nVal nv) { 14: if (ev = nv) return WFRead()==ev; 15: Blkb=L; 16: b.X=p; 17: if (b.Y) goto 27; ... Figure \n6. Adapted snippet from Luchagco et al. s [34] wait\u00adfree CAS algorithm. Execution Detectors: design ef.cient \ndetectors that can identify executions which are known to be commutative. The .rst three of these pertain \nto the speci.cation and we illus\u00adtrate two of them (deterministic speci.cation and single-owner) in the \nexamples that follow. The last one is focused on the implemen\u00adtation. As mentioned already, for linearizability \nour result holds for some sequential executions. However, when implementing an algo\u00adrithm, it may be \ndif.cult to differentiate the sequential executions of a given method for which the result holds and \nthose for which it does not. However, if a designer is able to come up with an ef.\u00adcient mechanism to \nidentify these cases, it may be possible to avoid RAW and AWAR in the executions where it may not be \nrequired. For instance, if the method can check that k .S before add(k) is performed, then for those \nsequential executions of add(k) it may not need to use neither RAW nor AWAR. Even though our result only \ntalks about some sequential execu\u00adtions, in practice, it is often dif.cult to design ef.cient tests that \ndifferentiate sequential executions, and hence, it often ends up the case that RAW or AWAR is used on \nall sequential executions of a strongly non-commutative linearizable method.  2.3 Examples: Linearizable \nAlgorithms Next, we illustrate the applicability of our result in practice via several well-known linearizable \nalgorithms. 2.3.1 Compare and Swap We begin with the universal compare-and-swap (CAS) construct, whose \nsequential speci.cation is deterministic, and the method is strongly non-commutative (for a formal proof, \nsee Section 6). The sequential speci.cation of CAS(m, o, n) says that it .rst compares [m] to o and if \n[m]= o,then n is assigned to [m] and CAS returns true. Otherwise, [m] is unchanged and CAS returns false. \nHere we use the operator [] to denote address dereference. The CAS speci.cation can be implemented trivially \nwith a linearizable algorithm that uses an atomic hardware instruction (also called CAS) and in that \ncase, the implementation inherently includes the AWAR pattern. Alternatively, the CAS speci.cation can \nbe implemented by a linearizable algorithm using reads, writes, and hardware CAS, with the goal of avoiding \nthe use of the hardware CAS in the common case of no contention. Such a linearizable algorithm is presented \nby Luchangco et al. [34]. Fig. 6 shows an adapted code snippet of the common path of that algorithm. \nWhile the algorithm succeeds in avoiding the AWAR pattern in the common case, the algorithm does indeed \ninclude the RAW pattern in its common path. To ensure correctness, the write to b.X in line 16 must precede \nthe read of b.Y in line 17. Both examples con.rm our result: AWAR or RAW was nec\u00adessary. Knowing that \nRAW or AWAR cannot be avoided in im\u00adplementing CAS correctly is important as CAS is a fundamental building \nblock for many classic concurrent algorithms. WorkItem take() { 1: b = bottom; 2: CircularArray a = \nactiveArray; 3: b=b-1; 4: bottom = b; 5: t=top; ... Figure 7. Snippet adapted from the take method of \nChase\u00adLev s work stealing algorithm [10]. WorkItem take() { 1: h=head; 2: t=tail; 3: if(h = t) return \nEMPTY; 4: task = tasks.array[h%tasks.size]; 5: head=h+1; 6: return task; } Figure 8. The take method \nfrom Michael et al. s idempo\u00adtent work stealing FIFO queue [38].  2.3.2 Work Stealing Structures Concurrent \nwork stealing algorithms are popular algorithms for implementing load balancing frameworks. A work stealing \nstructure holds a collection of work items and it has a single process as its owner. It supports three \nmain methods: put, take,and steal. Only the owner can insert and extract work items via methods put and \ntake. Other processes (thieves) may extract work items using steal. In designing algorithms for work \nstealing, the highest priority is to optimize the owner s methods, especially the common paths of such \nmethods, as they are expected to be the most frequently executed parts of the methods. Examining known \nwork stealing algorithms that avoid the AWAR pattern (i.e., avoid the use of complex atomic operations) \nin the common path of the owner s methods [3, 16, 19, 20], reveals that they all contain the RAW pattern \nin the common path of the take method that succeeds in extracting work items. The work stealing algorithm \nby Chase and Lev [10] is repre\u00adsentative of such algorithms. Fig. 7 shows a code snippet adapted from \nthe common path of the take method of that algorithm, with minor changes for the sake of consistency \nin presentation. The vari\u00adables bottom and top are shared variables, and bottom is writ\u00adten only by the \nowner but may be read by other processes. The key pattern in this code snippet is the RAW pattern in \nlines 4 and 5. The order of the write to bottom in line 4 followed by the read of top in line 5 is necessary \nfor the correctness of the algorithm. Reversing the order of these two instructions results in an incorrect \nalgorithm. In subsequent sections, we will see why correct implementations of the take and steal methods \nmust use either RAW or AWAR. From deterministic to non-deterministic speci.cations Our re\u00adsult dictates \nthat in the standard case where we have the expected deterministic sequential speci.cation of a work-stealing \nstructure, it is impossible to avoid both RAW and AWAR. However, as men\u00adtioned earlier, our result can \nguide us towards .nding practical cases where we can indeed eliminate RAW and AWAR. Indeed, re\u00adlaxing \nthe deterministic speci.cation may allow us to come up with algorithms that avoid both RAW and AWAR. \nSuch a relaxation is exempli.ed by the idempotent work stealing introduced by Michael et al. [38]. This \nconcept relaxes the semantics of work stealing to require that each inserted item is eventually extracted \nat least once Data dequeue() { 1: h=head; 2: t=tail; 3: next = h.next; 4: if head= h goto 1; 5: if next \n= null return EMPTY; 6: ifh = t {CAS(tail,t,next) ; goto 1; } 7: d = next.data; 8: if \u00acCAS(head,h,next) \ngoto 1; 9: return d; } Figure 9. Simpli.ed snippet of dequeue on lock-free FIFO queue [37]. Data dequeue() \n{ 1: if (tail = head) return EMPTY; 2: Data data = Q[head mod m]; 3: head = head +1 mod m; 4: return \ndata; } Figure 10. Single-consumer dequeue method adapted from Lamport s FIFO queue which does not use \nRAW and AWAR [30]. instead of exactly once. Under this notion the authors managed to design algorithms \nfor idempotent work stealing that avoid both the AWAR and RAW patterns in the owner s methods. Our result \nexplains the underlying reason of why the elimination of RAW and AWAR was possible: because the sequential \nspeci\u00ad.cation of idempotent structures is necessarily non-deterministic, our result now indicates that \nit may be possible to avoid RAW and AWAR. Indeed, this is con.rmed by the algorithms in [38]. Fig. 8 \nshows the take method of one of the idempotent algorithms. Note the absence of both AWAR and RAW in this \ncode. The shared vari\u00adables head, tail,and tasks.array[] are read before writing to head, and no reads \nneed to be atomic with the subsequent write. 2.3.3 FIFO Queue Example In examining concurrent algorithms \nfor multi-consumer FIFO queues, one notes that either locking or CAS is used in the com\u00admon path of nontrivial \ndequeue methods that return a dequeued item. However, as we mentioned already, our result proves that \nmutual exclusion locking requires each sequential execution of a successful lock acquire to include AWAR \nor RAW. All algorithms that avoid the use of locking in dequeue include a CAS operation in the common \npath of each nontrivial dequeue execution. Fig. 9 shows the simpli.ed code snippet from the dequeue method \nof the classic Michael and Scott s lock-free FIFO queue [37]. Note that every execution that returns \nan item must execute CAS. We observe that algorithms for multi-consumer dequeue in\u00adclude directly or \nindirectly at least one instance of the AWAR or RAW patterns (i.e., use either locking or CAS). From \nMulti-Owner to Single-Owner Our results suggest that if we want to eliminate RAW and AWAR, we can focus \non restricting the processes that can execute a method. For instance, we can specify that dequeue can \nbe executed only be a single process. Indeed, when we consider single-consumer FIFO queues, where no \nmore than one process can execute the dequeue method, we can obtain a correct implementation of dequeue \nwhich does not require RAW and AWAR. x, y . Var m . MID l . Lab B . BExp ::= ... E . NExp ::= ... C \n. Com ::= l : x = E | l : x = G[E] |l : G[E]= E | l : if B goto l |l : beg-atomic | l : end-atomic l \n: entry mxx | l : exit mx C; C P ::= C I ... I C Figure 11. Language Syntax  Fig. 10 shows a single-consumer \ndequeue method, adapted from Lamport s single-producer single-consumer FIFO queue [30].2 Note that the \ncode avoids both RAW and AWAR. The variable head is private to the single consumer and its update is \ndone by a regular write. Once again, this example demonstrates a case where we used our result to guide \nthe implementation. In particular, by changing the speci.cation of a method of the abstract data type \nnamely from multi-consumer to single-consumer it enabled us to create an implementation of the method \n(i.e., dequeue)where we did not need RAW and AWAR.   3. Preliminaries In this section, we present the \nformal machinery necessary to spec\u00adify and prove our results later. 3.1 Language The language shown in \nFig. 11 is a basic assembly language with labeled statements: assignments, sequencing and conditional \ngoto s. We do not elaborate on the construction of numerical and boolean expressions, which are standard. \nThe language is also equipped with the following features: Statements for beginning and ending of atomic \nsections. Using these, one can implement various universal constructs such as compare-and-swap.  Parallel \ncomposition of sequential commands.  We use G to model global memory via a one dimensional array.  \nTwo statements are used to denote the start (i.e., entry state\u00adment) and end of a method (i.e., exit \nstatement).  We use Var to denote the set of local variables for each process, MID to denote a .nite \nset of method identi.ers, Lab the set of pro\u00adgram labels and PID a .nite set of process identi.ers. We \nassume the set of values obtained from expression evaluation includes at least the integers and the booleans. \n 3.2 Semantics A program state s is a tuple (pc, locals, G, inatomic). S: S= PC \u00d7 Locals \u00d7 Globals \n\u00d7 InAtomic  PC = PID -Lab  Locals = PID \u00d7 Var -Val  Globals = Val -Val  InAtomic = PID ..  2 The \nrestriction or the lack of restriction on the number of concurrent producers does not affect the algorithm \nfor the dequeue method. A state s tracks the program counter for each process (pc), a mapping from process \nlocal variables to values (locals), the con\u00adtents of global memory (G) and whether a process executes \natomi\u00adcally (inatomic). If no process executes atomically then inatomic is set to .. We denote the set \nof initial states as Init . S (Initially inatomic is set to . for all states in Init). Transition Function \nWe assume standard small-step operational semantics given in terms of transitions between states [45]. \nThe behavior of a program is determined by a partial transition function TF :S \u00d7 PID -S. Given a state \ns and a process p, TF (s, p) returns the unique state, if it exists, that the program will evolve into \nonce p executes its enabled statement. When convenient, we sometimes use the function TF as a relation. \nFor a transition t . TF , we denote its source state by src(t), its executing process by proc(t), its \ndestination state by dst(t), its executing statement by stmt(t). A program transition represents the \nintuitive fact that starting from a state src(t), process proc(t) can execute the statement stmt(t) and \nend up in a state dst(t), that is, TF (src(t),proc(t)) = dst(t). We say that a transition t performs \na global read (resp. write) if stmt(t) reads from G (resp. writes to) and use mloc(t) to denote the global \nmemory location that the transition accesses. If the transition does not read or write a global location, \nthen mloc(t) returns .. That is, only in the case where a transition t accesses a global memory location \ndoes mloc(t) return a non-. value, otherwise, mloc(t) always returns .. We enforce strong atomicity semantics: \nfor any state s, process p can make a transition from s only if inatomics = . or inatomics = p. For a \ntransition t,if stmt(t)= beg-atomic,then inatomicdst(t) = proc(t). Similarly, if stmt(t)= end-atomic, \ninatomicdst(t) = ..Weuse enableds to denote the set of processes that can make a transition from s.If \ninatomics .., = then enableds = {inatomics}, otherwise enableds = PID. The statement entry mxx is used \nto denote the start of a method invoked with a sequence of variables which contain the arguments to the \nmethod (denoted by xx). The statement exit mx is used to denote the end of a method m. These statements \ndo not affect the program state (except the program counter). The meaning of the other statements in \nthe language is standard. Executions An execution p is a (possibly in.nite) sequence of transitions p0,p1,...,where \n.i = 0, pi . TF and .j = 1.dst(pj-1)= src(pj ).Weuse .rst(p) as a shortcut for src(p0), i.e., the .rst \nstate in the execution p, and, last(p) to denote the last state in the execution p, i.e., last(p)= dst(p|p|-1). \nIf a transition t is performed in an execution p then t . p is true, otherwise it is false. For a program \nProg, we use [[Prog]] to denote the set of exe\u00adcutions for that program starting from initial states \n(e.g. states in Init). Next, we de.ne what it means for an execution p . [[Prog]] to be atomic: De.nition \n3.1 (Atomic Execution). We say that an execution p is executed atomically by process p when: All transitions \nare performed by p: .i. 0 = i< |p|.proc(pi)= p.  All transitions are atomic: |p| =1 or .i. 1 = i< |p|, \ninatomicsrc(pi) = p.   We use p(i,j) to denote the substring of p occurring between positions i and \nj (including the transitions at i and j). De.nition 3.2 (Maximal Atomic Cover). Given an execution p \nand a transition pk . p,the maximal atomic cover of pk in p is the unique substring p(i,j) of p,where: \np(i,j) is executed atomically by proc(pk),where i = k = j.  inatomicsrc(pi) = ..  inatomicdst(pj) \n= ..  Intuitively, we can understand the maximal atomic cover as taking a transition and extending it \nin both directions until we reach a leftmost state and a rightmost state where no process is inside an \natomic section in either of these two states. Next, we de.ne read-after-write executions: De.nition 3.3 \n(Read After Write Execution). We say that a process p performs a read-after-write in execution p,if .i, \nj. 0 = i<j< |p| such that: pi performs a global write by process p.  pj performs a global read by process \np.  mloc(pi) .  = mloc(pj ) (the memory locations are different). .k.i < k < j,if proc(pk)= p,then \nmloc(pk) .= mloc(pj ). Intuitively, these are executions where somewhere in the exe\u00adcution the process \nwrites to global memory location A and then, sometimes later, reads a global memory location B that is \ndifferent from A, and in-between the process does not access B. Note that there could be transitions \nin p performed by processes other than p. Note that in this de.nition there is no restriction on whether \nthe global accesses are performed atomically or not, the de.nition only concerns itself with the ordering \nof accesses and not their atomicity. We introduce the predicate RAW(p, p) which evaluates to true if \np performs a read-after-write in execution p and to false otherwise. Next, we de.ne atomic write-after-read \nexecutions. These are executions where a process .rst reads from a global memory loca\u00adtion and then, \nsometimes later, writes to a global memory location and these two accesses occur atomically, that is, \nin-between these two accesses, no other process can perform any transitions. Note that unlike read-after-write \nexecutions, here, the global read and write need not access different memory locations. De.nition 3.4 \n(Atomic Write After Read Execution). We say that a process p performs an atomic write-after-read in execution \np,if .i, j. 0 = i<j< |p| such that: process p performs a global read in pi.  process p performs a global \nwrite in pj .  p(i,j) is executed atomically by process p.  We introduce the predicate AWAR(p, p) which \nevaluates to true if process p performs an atomic write-after-read in execution p and to false otherwise. \n 3.3 Speci.cation 3.3.1 Histories A history H is de.ned as a .nite sequence of actions, i.e., H = .; \n....; ., where an action denotes the start and end of a method: . =(p, entry mxa) | (p, exit mr) where \np . PID is a process identi.er, m . MID is a method identi.er, xa is a sequence of arguments to the method \nand r is the return value. For an action .,weuse proc(.) to denote the process, kind(.) to denote the \nkind of the action (entry or exit), and m(.) to denote the name of the method. We use Hi to denote the \naction at position i in the history, where 0 = i< |H|. For a process p, H lp is used to denote the subsequence \nof H consisting only of the actions of process p. For a method m, H lm is used to denote the subsequence \nof H consisting only of the actions of method m. A method entry (p, entry m1 ax1) is said to be pending \nin a history H if it has no matching exit, that is, .i. 0 = i< |H|such that proc(Hi)= p, kind(Hi)= entry, \nm(Hi)= m1 and .j.i<j < |H|, proc(Hj )= p or kind(Hj ) .exit or m(Hj ) .m1. A history H is said to be \ncomplete if it has no = pending calls. We use complete(H) to denote the set of histories resulting after \nextending H with matching exits to a subset of entries that are pending in H andthenremovingtheremaining \npending entries. A history H is sequential if H is empty (H = E)or H starts with an entry action, i.e., \nkind(H0)= entry and if |H| > 1,entries and exits alternate. That is, .i. 0 = i< |H|- 1,kind(Hi) . = kind(Hi+1) \nand each exit is matched by an entry that occurs immediately before it in H, i.e., .i. 1 = i< |H|,if \nkind(Hi)= exit then kind(Hi-1)= entry and proc(Hi)= proc(Hi-1).A complete sequential history H is said \nto be a complete invocation of a method m1 iff |H| =2, m(H0)= m1 and m(H1)= m1. In the case where H is \na complete sequential invocation, we use entry(H) to denote the entry action in H and exit(H) to denote \nthe exit action in H. A history H is well-formed if for each process p . PID, H lp is sequential. In \nthis work, we consider only well\u00adformed histories.  3.4 Histories and Executions Given an execution \np, we use the function hs(p) to denote the history of p. hs(p) takes as input an execution p and produces \na sequence of actions by iterating over each transition t . p in order, and extracting proc(t) and stmt(t).If \nstmt(t) is an entry statement of a method m, then the transition t contributes the action (proc(t), entry \nmxa),where xa is the sequence of values obtained from evaluating the variables used in the sequence stmt(t), \nin state src(t). Similarly, for exit statements. If the transition t does not perform an entry or an \nexit statement, then it contributes E. For a program Prog, we de.ne its corresponding set of histories \nas [[Prog]]H = {hs(p) | p . [[Prog]]}.Weuse [[Prog]]HS to denote the sequential histories in [[Prog]]H \n. A transition t . p is said to be a method transition if it is performed in-between method entry and \nexit. That is, there exists a preceding transition tprev . p that performs an entry statement with proc(tprev)= \nproc(t), such that proc(t) does not perform an exit statement in-between tprev and t in p. We say that \ntprev is a matching entry transition for t. Note that tprev may be the same as t. A transition that is \nnot a method transition is said to be a client transition. De.nition 3.5 (Well-formed Execution). We \nsay that an execution p is well-formed if: hs(p) is well-formed.  Any client transition t . p: mloc(t)= \n.. stmt(t) ..   = beg-atomic and stmt(t)= end-atomic. For any transition t . p,if stmt(t) is an exit \nstatement, then inatomicsrc(t) = ..  For any transition tr . p,if tr is a method transition that reads \na local variable other than the variables speci.ed in the statement of its matching entry transition \ntm, then there exists a transition tw performed by process proc(t), in-between tm and tr, such that tw \nwrites to that local variable.  That is, a well-formed execution is one where its history is well\u00adformed, \nonly method transitions are allowed to access global mem\u00adory or perform atomic statements, when a method \nexit statement is performed, the inatomic should be ., and methods must initial\u00adize local variables which \nare not used for argument passing before using them. We say that p is a complete sequential execution \nof a method m by process p,if p is a well-formed execution and hs(p) is a complete invocation of m by \nprocess p. Note that p may contain client transitions (both by process p and other processes). A program \nProg is well-formed if [[Prog]] contains only well\u00adformed executions. In this paper, we only consider \nwell-formed programs.  4. Synchronization in Mutual Exclusion In this section, we consider implementations \nthat provide mutually exclusive access to a critical section among a set of processes. We show that every \ndeadlock-free mutual exclusion implementation incurs either the RAW or the AWAR pattern in certain executions. \nA mutual exclusion implementation exports the following meth\u00adods: MID = {lock0,unlock0,...,lockn-1,unlockn-1},where \nn = |PID|. In this setting, we strengthen the de.nition of well\u00adformed executions by requiring that each \nprocess p . PID only invokes methods lockp and unlockp in an alternating fashion. That is, for any execution \np . [[Prog]] and for any process p . PID, hs(p) lp is such that lock and unlock operations alternate, \ni.e., lockp,unlockp,lockp,... . Given an execution p . [[Prog]] and H = hs(p) lp, we say that p is in \nits trying section if it has started, but not yet completed a lockp operation, i.e., m(H|H|-1)= lockp \nand kind(H|H|-1)= entry. We say that p is in its critical section if it has completed lockp but has not \nyet started unlockp,thatis, m(H|H|-1)= lockp and kind(H|H|-1)= exit. We say that p is in its exit section \nif it has started unlockp but has not yet .nished it, that is, m(H|H|-1)= unlockp and kind(H|H|-1)= entry. \nOtherwise we say that p is in the remainder section (initially all processes are in their remainder sections). \nA process is called active if it is in its trying or exit section. For the purpose of our lower bound, \nwe assume the following weak formulation of the mutual exclusion problem [11, 31]. In addition to the \nclassical mutual exclusion requirement, we only require that the implementation is deadlock-free, i.e., \nif a number of active processes concurrently compete for the critical section, at least one of them succeeds. \nDe.nition 4.1 (Mutual Exclusion). A deadlock-free mutual exclu\u00adsion implementation Prog guarantees: \nSafety: For all executions p . [[Prog]], it is always the case that at most one process is in its critical \nsection at a time, that is, for all p, q . PID,if p is in its critical section in hs(p) lp and q is in \nits critical section in hs(p)lq,then p = q.  Liveness: In every execution in which every active process \ntakes suf.ciently many steps: i) if at least one process is in its trying section and no process is in \nits critical section, then at some point later some process enters its critical section, and ii) if at \nleast one process is in its exit section, then at some point later some process enters its remainder \nsection.  Theorem 4.2 (RAW or AWAR in Mutual Exclusion). Let Prog be a deadlock-free mutual exclusion \nimplementation for two or more processes (|PID| > 1). Then, for every complete sequential execution p \nof lockp by process p: RAW(p, p)= true,or AWAR(p, p)= true Proof. Let pbase \u00b7 p . [[Prog]] such that \np is a complete sequential execution of lockp by process p. It follows that no process q . PID, q .p, \nis in its critical section in hs(pbase) lq = (otherwise mutual exclusion would be violated). It also \nfollows that p is not active in hs(pbase)lp, By contradiction, assume that p does not contain a global \nwrite. Consider an execution pbase \u00b7. such that process p does not perform transitions in . and every \nactive process takes suf.ciently many steps in . until some process q .p completes its lockq = section, \ni.e., q is in its critical section in hs(pbase \u00b7 .) lq. The execution pbase \u00b7 . . [[Prog]] since Prog \nis deadlock-free. Since p does not write to a shared location in p, Glast(pbase) = Glast(pbase \u00b7p). \nFurther, the local state of all processes other than p in last(pbase) is the same as their local state \nin last(pbase \u00b7p), i.e., .q . PID, .var . Var,if q . = p,then localslast(pbase)(q, var)= localslast(pbase \n\u00b7p)(q, var). Also, we know enabledlast(pbase) = enabledlast(pbase \u00b7p) as transitions by process p do \nnot access local variables of other processes. Hence, we can build the execution pnc = pbase \u00b7 p \u00b7 .. \nwhere .. is the execution with the same sequence of statements as . (i.e., process p does not perform \ntran\u00adsitions in .D). Hence, hs(pnc) lq = hs(pbase \u00b7 .) lq ,thatis, q is in its critical section in hs(pnc) \nlq.But p is also in its critical section in hs(pnc)lp a contradiction. Thus, p contains a global write, \nand let tw be the .rst global write transition in p.Let p = pf \u00b7 pw \u00b7 pl,where pw is the maximal atomic \ncover of tw in p. We proceed by contradiction and assume that RAW(p, p)= false and AWAR(p, p)= false.Since \nAWAR(p, p)= false and tw is the .rst write transition in p,it follows that tw is the .rst global transition \nin pw. Since pf contains no global writes, Gfirst(pf ) = Glast(pf ). Applying the same arguments as before, \nthere exists an execution pbase \u00b7 pf \u00b7 t . [[Prog]] such that some process q, q .p, is in its = critical \nsection in hs(pbase \u00b7 pf \u00b7 t )lq. The assumption RAW(p, p)= false implies that no global read transition \nby process p in pw \u00b7 pl accesses a variable other than mloc(tw) without having previously written to \nit. Note that tw overwrites the only location that can be read by p in pw \u00b7 pl. Thus, applying the same \narguments as before, there exists an execution pc = pbase \u00b7 pf \u00b7 t wl in [[Prog]] such that q is in its \ncritical \u00b7 p. \u00b7 p. section in hs(pc) lq and p is in its critical section in hs(pc) lp a contradiction. \nThus, either RAW(p, p)= true or AWAR(p, p)= true. 5. Synchronization in Linearizable Algorithms In this \nsection we state and prove that certain sequential execu\u00adtions of strongly non-commutative methods of \nalgorithms that are linearizable with respect to a deterministic sequential speci.cation must use RAW \nor AWAR. 5.1 Linearizability Following [21, 23] we de.ne linearizable histories. A history H induces \nan irre.exive partial order <H on actions in the history: a<H b if kind(a)= exit and kind(b)= entry and \n.i, j. 0 = i<j< |H| such that Hi = a and Hj = b.Thatis, exit action a precedes entry action b in H. A \nhistory H is said to be linearizable with respect to a sequential history S if there exists a history \nH. . complete(H) such that: 1. .p . PID,H. lp = S lp 2. <H .<S .  We can naturally extend this de.nition \nto a set of histories. Let Spec be a sequential speci.cation, a pre.x-closed set of sequential histories \n(that is, if s is a sequential history in Spec,thenany pre.x of s is also in Spec). Then, given a set \nof histories Impl, we say that Impl is linearizable with respect to Spec if for any history H . Impl \nthere exists a history S . Spec such that H is linearizable with respect to S. We say that a program \nProg is linearizable with respect to a sequential speci.cation Spec when [[Prog]]H is linearizable with \nrespect to Spec.  5.2 Deterministic Sequential Speci.cations In this paper, similarly to [8], we de.ne \ndeterministic sequen\u00adtial speci.cations. Given two sequential histories s1 and s2,let maxprefix(s1,s2) \ndenote the longest common pre.x of the two histories s1 and s2. De.nition 5.1 (Deterministic Sequential \nSpeci.cations). Ase\u00adquential speci.cation Spec is deterministic, if for all s1,s2 . Spec, s1 .s2 and \n= sE or = s maxprefix(s1,s2), we have = kind( s| . s|-1)= entry. That is, a speci.cation is deterministic, \nif we cannot .nd two different histories whose longest common pre.x ends with an entry. If we can .nd \nsuch a pre.x, then that would mean that there was a point in the execution of the two histories s1 and \ns2 up to which they behaved identically, but after they both performed the same entry, they produced \ndifferent results (or one had no continuation). 5.3 Strong Non-Commutativity We de.ne a strongly non-commutative \nmethod as follows: De.nition 5.2 (Strongly Non-Commutative Method). We say that a method m1 is strongly \nnon-commutative in a sequential speci.\u00adcation Spec if there exists a method m2 (possibly the same as \nm1), and there exist histories base, s1, s2, s3, s4 such that: 1. s1 and s4 are complete invocations \nof m1 with entry(s1)= entry(s4) and exit(s1) . = exit(s4). 2. s2 and s3 are complete invocations of \nm2 with entry(s2)= entry(s3) and exit(s2) . = exit(s3). 3. proc(entry(s1)) . = proc(entry(s2)). 4. base \nis a complete sequential history in Spec. 5. base \u00b7 s2 \u00b7 s4 . Spec. 6. base \u00b7 s1 \u00b7 s3 . Spec.  In \nother words, the method m1 is strongly non-commutative if there is another method m2 and a history base \nin Spec such that we can distinguish whether m1 is applied right after base or right after m2 (which \nis applied after base). Similarly we can distinguish whether m2 is applied right after base or right \nafter m1 (which is applied after base). Note that m2 may be the same method as m1. In this work we focus \non programs where the speci.cation Spec can be determined by the sequential executions of the program. \nAssumption 1. Spec =[[Prog]]HS .  5.4 RAW and AWAR for Linearizability Next, we state and prove the \nmain result of this section: Theorem 5.3 (RAW or AWAR in Linearizable Algorithms). Let m1 be a strongly \nnon-commutative method in a deterministic se\u00adquential speci.cation Spec and let Prog be a linearizable \nimple\u00admentation of Spec. Then there exists a complete sequential execu\u00adtion pa of m1 by process p such \nthat: RAW(pa,p)= true,or AWAR(pa,p)= true Proof. From the premise that m1 is a strongly non-commutative \nmethod and Assumption 1, we know that there exist executions pbase1 \u00b7 pa \u00b7 pc . [[Prog]] and pbase2 \u00b7 \npb \u00b7 pd . [[Prog]] such that: 1. hs(pbase1 ) and hs(pbase2 ) are complete sequential histories. 2. hs(pbase1 \n)= hs(pbase2 ). 3. pa and pd are complete sequential executions of m1. 4. pb and pc are complete sequential \nexecutions of m2. 5. entry(hs(pa)) = entry(hs(pd)).  6. entry(hs(pb)) = entry(hs(pc)). 7. exit(hs(pa)) \n=.exit(hs(pd)). 8. exit(hs(pb)) .  = exit(hs(pc)). 9. proc(entry(hs(pa))) .= proc(entry(hs(pb))). \nFrom the fact that executions in the program are well-formed, we know that if p, . . [[Prog]] and hs(p), \nhs(.) are complete sequential invocations such that entry(hs(p)) = entry(hs(.)), and Gfirst(p) = Gfirst(.), \nit follows that hs(p) = hs(.) and Glast(p) = Glast(.). That is, if a process completes the same method \ninvocation from two program states with identical global memory, the method will always produce the same \nresult and global memory (Fact 1). Fact 1 follows directly from the fact that transi\u00adtions are deterministic, \nprocesses cannot access the local state of another process, arguments to both methods are the same, and \nthe starting global states are the same. From Fact 1 and hs(pbase1 ), hs(pbase2 ) being complete sequen\u00adtial \nhistories, we can show that Glast(pbase1 ) = Glast(pbase2 ).That is, from first(pbase1 )= first(pbase2 \n) (both are initial states), we can inductively show that any complete sequential invocation preserves \nthe fact that the global state in the last states of the two executions are the same. From Glast(pbase1 \n) = Glast(pbase2 ),itfol\u00ad lows that Gfirst(pa) = Gfirst(pb). Let p = proc(entry(hs(pa))) and q = proc(entry(hs(pb))). \nWe .rst prove that a method transition performed by process p must perform a global write in pa. Let \nus assume the execution pa does not contain a method transition where process p performs a global write. \nAs pa is well-formed, we know that any client transitions performed in pa do not access global memory. \nIt then follows that Gfirst(pa) = Glast(pa). However, from the premise we know that Glast(pa) = Gfirst(pc) \nand hence Gfirst(pa) = Gfirst(pc).Transi\u00adtively, we know that Gfirst(pb) = Gfirst(pc).Fromitem4above \nwe know that pb and pc are complete sequential executions of m2 with entry(hs(pb)) = entry(hs(pc)) (item \n6). Then, it follows from Fact 1 that exit(hs(pb)) = exit(hs(pc)) which contradicts with item 8. Therefore, \nthere must exist a method transition in pa by process p that performs a global write. Let us proceed \nby contradiction and assume that both RAW(pa,p) = false and AWAR(pa,p)= false.Let pa = pf \u00b7 pw \u00b7 pe,where \ntw is the .rst method transition in pa that writes to global memory and pw is the maximal atomic cover \nof tw in pa.As pa is well-formed, we know that all transitions in pw are method transitions. Since AWAR(pa,p)= \nfalse and tw is the .rst global write transition in pa, it follows that there can be no global read transitions \nin pw that occur before tw (otherwise we would contradict AWAR). This means that tw is the .rst global \nread or write transition in pw. As pf does not contain global writes, it follows that Gfirst(pa) = Glast(pf \n). From the premise we know that Gfirst(pa) = Gfirst(pb) and hence Gfirst(pb) = Glast(pf ).As pw is a \nmaximal atomic cover, we know that inatomiclast(pf ) = .. From the fact that client transitions cannot \nsynchronize (they cannot execute atomic statements or access global memory), and that a process cannot \naccess the local variables of another process, it follows that process q can execute m2 concurrently \nwith m1 from state last(pf ). That is, there exists an execution pbase1 \u00b7 pf \u00b7 pbD. [[Prog]] where pbDis \na complete sequential execution of m2 by process q such that entry(hs(pb)) = entry(hs(pbD)).As Gfirst(pD) \n= Glast(pf ), it follows that Gfirst(pD) = Gfirst(pb). bb Then, by Fact 1, it follows that hs(pbD) = \nhs(pb). As p . enabledfirst(pD) and pDis a complete sequential exe\u00ad b b cution by process q, it follows \nthat p . enabledlast(pD). Process p b can now continue execution of method m1 and build the execution \n\u00b7 pD\u00b7 pD\u00b7 pD pbase1 \u00b7 pconc . [[Prog]] where pconc = pf bwe. By assumption, we know that RAW(pa,p) and \nAWAR(pa,p) are false and it follows that pw \u00b7pe does not contain a method transition which reads a global \nmemory location other than mloc(tw) without previously having written to it. In both, pw and pD , process \np w overwrites the only global memory location mloc(tw) that it can read without previously having written \nto it. Then, pw \u00b7 pe and pD \u00b7 pD will contain the same sequence of statements, with all we global transitions \naccessing and reading/writing identical values. Thus, hs(pconc)lm1 = hs(pa). Given that the implementation \nis linearizable the two possible linearizations of pbase1 \u00b7 pconc are: 1. hs(pbase1 ) \u00b7 hs(pconc) lm1 \n\u00b7hs(pbD ). We already established that hs(pconc) lm1 = hs(pa) and hs(pbD ) = hs(pb), and hence by substitution \nwe get hs(pbase1 ) \u00b7 hs(pa) \u00b7 hs(pb).Fromthe premise, we know that hs(pbase1 ) \u00b7 hs(pa) \u00b7 hs(pc) . Spec. \nAs the speci.cation is deterministic, it follows that hs(pb)= hs(pc), a contradiction with item 8. 2. \nhs(pbase1 ) \u00b7 hs(pbD ) \u00b7 hs(pconc) lm1 . We already established that hs(pconc) lm1 = hs(pa) and hs(pbD \n) = hs(pb) and hence by substitution we get hs(pbase1 ) \u00b7 hs(pb) \u00b7 hs(pa).Fromthe premise, we know that \nhs(pbase2 ) \u00b7 hs(pb) \u00b7 hs(pd) . Spec and hs(pbase1 )= hs(pbase2 ). As the speci.cation is deterministic, \nit follows that hs(pa)= hs(pd), a contradiction with item 7.  Therefore, RAW(pa,p)= true or AWAR(pa,p)= \ntrue.  5.5 A Note on Commutativity and Idempotency The notion of strongly non-commutative method is \nrelated to traditional notions of non-commutative methods [44] and non\u00adidempotent methods. Let us again \nconsider De.nition 5.2. Non-Idempotent Method vs. Strongly Non-Commutative Method If it is the case \nthat method m2 is the same as method m1, then the de.nition instantiates to non-idempotent methods. That \nis, given base, if we apply m1 twice in a row, the second invocation will re\u00adturn a different result \nthan the .rst. Consider again the Set speci.\u00adcation in Fig. 4. The method add is non-idempotent. As discussed \nin the example in Section 2, we can start with S = \u00d8 and base = E. Then, if we perform two methods add(5) \nin a row, each one of the add(5) s will return a different result. Classic Non-Commutativity vs. Strong \nNon-Commutativity In the classic notion of non-commutativity [44], it is enough for one of the methods \nto not commute with the other, while here, it is required that both methods do not commute from the same \npre\u00ad.x history. In the classic case, if two methods do not commute, it does not mean that either of them \nis a strongly non-commutative method. However, if a method is strongly non-commutative, then it is always \nthe case that there exists another method with which it does not commute (by de.nition). Consider again \nthe Set speci.\u00adcation in Fig. 4. Although add and contains do not commute, contains is not a strongly \nnon-commutative method. That is, add in.uences the result of contains,but contains does not in.uence \nthe result of add.  6. Strongly Non-Commutative Speci.cations In this section we provide a few examples \nof well-known sequential speci.cations that contain strongly non-commutative methods as de.ned in De.nition \n5.2. 6.1 Stacks De.nition 6.1 (Stack Sequential Speci.cation). A stack object S supports two methods: \npush and pop. The state of a stack is a sequence of items S = (v0, ..., vk). The stack is initially empty. \n The push and pop methods induce the following state transitions of the sequence S = (v0,...,vk), with \nappropriate return values: push(vnew): changes S to be (v0, ..., vk,vnew) and returns ack.  pop(): \nif S is non-empty, changes S to be (v0, ..., vk-1) and re\u00adturns vk. If S is empty, returns empty and \nS remains unchanged.  We let Specs denote the sequential speci.cation of a stack object as de.ned above. \nLemma 6.2 (Pop is Strongly Non-Commutative). The pop stack method is strongly non-commutative. Proof. \nLet base . Specs be a complete sequential history after which S = (v) for some v.Let p and q be two processes, \nlet s1 and s4 be complete invocations of pop by p, and let s2 and s3 be complete invocations of pop by \nq. From De.nition 6.1, {base \u00b7 s1 \u00b7 s3, base \u00b7 s2 \u00b7 s4}. Specs, ret(s1)= ret(s2)= v, and ret(s3)= ret(s4)= \nempty. The claim now follows from De.nition 5.2. It also follows from De.nition 5.2 that push methods \nare not strongly non-commutative. 6.2 Work Stealing As we now prove, the (non-idempotent) work stealing \nobject, dis\u00adcussed in section 2.2, is an example of an object for which two different methods are strongly \nnon-commutative. De.nition 6.3 (Work Stealing Sequential Speci.cation). Awork stealing object supports \nthree methods: put, take, and steal. The state of each process i is a sequence of items Qi = (v0i , ..., \nv i ).All ki queues are initially empty. The put and take methods are performed by each process i on \nits local queue Qi and induce on it the following state transitions, with appropriate return values: \nput(vnew): changes Qi to be (vnew,v0i , ..., v i ) and returns ki ack. take(): if Qi is non-empty, it \nchanges Qi to be (v1i , ..., v i ) and ki returns v0i .If Qi is empty, it returns empty and Qi remains \nunchanged. The steal method is performed by each process i on some queue Qj = (v0j , ..., vj ) for j \n.i.if Qj is non-empty, it changes Qj to = kj ji j  be (v0, ..., v kj-1) and returns v.If Qj is empty, \nit returns empty kj and Qj remains unchanged. We let Specws denote the sequential speci.cation of a work \nstealing object as de.ned above. Lemma 6.4 (Take &#38; Steal are Strongly Non-Commutative). The take \nand steal methods are strongly non-commutative. Proof. Let base . Specws be a complete sequential history \nafter which Qj = (v) for some value v and process j.Let i =.j be some process other than j,let s1 and \ns4 be complete invocations of steal by process i on Qj , and let s2 and s3 be complete invocations of \ntake by process i. From De.nition 6.3, {base\u00b7s1\u00b7s3, base\u00b7s2\u00b7s4}.Specs, ret(s1)= ret(s2)= v,and ret(s3)= \nret(s4)= empty. The claim now follows from De.nition 5.2. It is easily shown that speci.cations for queues, \nhash-tables and sets have strongly non-commutative methods. The proofs are essentially identical to the \nproofs of Lemmas 6.2 and 6.4 and are therefore omitted. 6.3 Compare-and-Swap (CAS) We now prove that \nCAS is strongly non-commutative. De.nition 6.5 (Compare-and-swap Sequential Speci.cation). A compare-and-swap \nobject C supports a single method called CAS and stores a scalar value over some domain V. The method \nCAS(exp,new), for exp, new .V, induces the following state transition of the compare-and-swap object. \nIf C s value is exp, C s value is changed to new and the method returns true; otherwise, C s value remains \nunchanged and the method returns false. We let SpecC denote the sequential speci.cation of a compare\u00adand-swap \nobject as de.ned above. Lemma 6.6 (CAS is Strongly Non-Commutative). The CAS method is strongly non-commutative. \nProof. Let base . SpecC be a complete sequential history after which C s value is v,let i and j be two \nprocesses, let s1 and s4 be complete invocations of CAS(v,v ) by process i, for some v . = v D .V, and \nlet s2 and s3 be complete invocations of CAS(v,v )by process j. From De.nition 6.5, {base \u00b7 s1 \u00b7 s3, \nbase \u00b7 s2 \u00b7 s4}.SpecC , ret(s1)= ret(s2)= true,and ret(s3)= ret(s4)= false. The claim now follows from \nDe.nition 5.2. It follows from lemma 6.6 that any software implementation of CAS is required to use either \nAWAR or RAW. Proving a similar result for all non-trivial read-modify-write speci.cations (such as fetch-and-add, \nswap, test-and-set and load-link/store-conditional) is equally straightforward.  7. Related Work Numerous \npapers present implementations of concurrent data structures, several of these are cited in Section 2. \nWe refer the reader to Herlihy and Shavit s book [22] for many other examples. Modern architectures often \nexecute instructions issued by a sin\u00adgle process out-of-order, and provide fence or barrier instructions \nto order the execution (cf. [1, 33]). There is a plethora of fence and barrier instructions (see [35]). \nFor example, DEC Alpha pro\u00advides two different fence instructions, a memory barrier (MB)and a write memory \nbarrier (WMB). PowerPC provides a lightweight (lwsync) and a heavyweight (sync) memory ordering fence \nin\u00adstructions, where sync is a full fence, while lwsync guarantees all other orders except RAW. SPARC \nV9 RMO provides several .avors of fence instructions, through a MEMBAR instruction that can be customized \n(via four-bit encoding) to order a combination of previous read and write operations with respect to \nfuture read and write operations. Pentium 4 supports load fence (lfence), store fence (sfence) and memory \nfence (mfence) instructions. The mfence instruction can be used for enforcing the RAW order. Herlihy \n[22] proved that linearizable wait-free implementations of many widely-used concurrent data-structures, \nsuch as counters, stacks and queues, must use AWAR. These results do not mention RAW and do not apply \nto obstruction-free [15] implementations of such objects or to implementations of mutual exclusion, however, \nwhereas our results do. Recently, there has been a renewed interest in formalizing mem\u00adory models (cf. \n[40, 42, 43]), and model checking and synthesizing programs that run on these models [29]. Our result \nis complemen\u00adtary to this direction: it states that we may need to enforce certain order, i.e., RAW, \nregardless of what weak memory model is used. Further, our result can be used in tandem with program \ntesting and veri.cation: if both RAW and AWAR are missing from a program that claims to satisfy certain \nspeci.cations, then that program is certainly incorrect and there is no need test it or verify it. Kawash \ns PhD thesis [28] (also in papers [24, 25]) investi\u00adgates the ability of weak consistency models to solve \nmutual exclu\u00adsion, with only reads and writes. This work shows that many weak models (Coherence, Causal \nconsistency, P-RAM, Weak Ordering, SPARC consistency and Java Consistency) cannot solve mutual ex\u00adclusion. \nProcessor consistency [17] can solve mutual exclusion, but it requires multi-write registers; for two \nprocesses, solving mutual exclusion requires at least three variables, one of which is multi\u00adwriter. \nIn contrast, we show that particular orders of operations or certain atomicity constraints must be enforced, \nregardless of the memory model; moreover, our results apply beyond mutual exclu\u00adsion and hold for a large \nclass of important linearizable objects. Boehm [7] studies when memory operations can be reordered with \nrespect to PThread-style locks, and shows that it is not safe to move memory operations into a locked \nregion by delaying them past a lock call. On the other hand, memory operations can be moved into such \na region by advancing them to be before an unlock call. However, Boehm s paper does not address the central \nsubject of our paper, namely, the necessity that certain ordering patterns (RAW or AWAR) must be present \ninside the lock operations. Our proof technique employs the covering technique, originally used by Burns \nand Lynch [9] to prove a lower bound on the number of registers needed for solving mutual exclusion. \nThis technique had many applications, both with read / write operations [4, 5, 12, 14, 27, 39], and with \nnon-trivial atomic operations, such as compare&#38;swap [13]. Some steps of our proofs can be seen as \na formalization of the arguments Lamport uses to derive a fast mutual exclusion algorithm [32]. In terms \nof our result for mutual exclusion, while one might guess that some form of RAW should be used in the \nentry code of read/write mutual exclusion, we are not aware of any prior work that states and proves \nthis claim. Burns and Lynch [9] show that you need to have n registers, and as part of their proof show \nthat a process needs to write, but they do not show that after it writes, the process must read from \na different memory location. Lamport [32] also only hints to it. These works neither state nor prove \nthe claim we are making (and they also do not discuss AWAR). 8. Conclusion and Future Work In this work, \nwe focused on two common synchronization idioms: read-after-write (RAW) and atomic write after read (AWAR). \nUn\u00adfortunately, enforcing any of these two patterns is costly on all cur\u00adrent processor architectures. \nWe showed that it is impossible to eliminate both RAW and AWAR in the sequential execution of a lock \nsection of any mutual exclusion algorithm. We also proved that RAW or AWAR must be present in some of \nthe sequential executions of strongly non\u00adcommutative methods that are linearizable with respect to a \ndeter\u00administic sequential speci.cation. Further, we proved that many classic speci.cations such as stacks, \nsets, hash tables, queues, work-stealing structures and compare-and-swap operations have strongly non-commutative \noperations, making implementations of these speci.cations subject to our result. Finally, as RAW or AWAR \ncannot be avoided in most practical algorithms, our result suggests that it is important to improve the \nhardware costs of store-load fences and compare-and-swap operations, the instructions that en\u00adforce RAW \nand AWAR. An interesting direction for future work is taking advantage of our result by weakening its \nbasic assumptions in order to build useful algorithms that do not use RAW and AWAR. 9. Acknowledgements \nWe thank Bard Bloom and the anonymous reviewers for valuable suggestions which improved the quality of \nthe paper. Hagit Attiya s research is supported in part by the Israel Science Foundation (grants number \n953/06 and 1227/10). Danny Hendler s research is supported in part by the Israel Science Foundation (grants \nnumber 1344/06 and 1227/10).  References [1] Sarita V. Advee and Kourosh Gharachorloo. Shared memory \nconsis\u00adtency models: A tutorial. IEEE Computer, 29(12):66 76, 1996. [2] Thomas E. Anderson. The performance \nof spin lock alternatives for shared-money multiprocessors. IEEE Trans. Parallel Distrib. Syst., 1(1):6 \n16, 1990. [3] Nimar S. Arora, Robert D. Blumofe, and C. Greg Plaxton. Thread scheduling for multiprogrammed \nmultiprocessors. In Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures, \nSPAA, pages 119 129, June 1998. [4] Hagit Attiya, Faith Fich, and Yaniv Kaplan. Lower bounds for adaptive \ncollect and related objects. In Proceedings of the Twenty-Third Annual ACM Symposium on Principles of \nDistributed Computing, pages 60 69, 2004. [5] Hagit Attiya, Alla Gorbach, and Shlomo Moran. Computing \nin totally anonymous asynchronous shared memory systems. Information and Computation, 173(2):162 183, \nMarch 2002. [6] Yoah Bar-David and Gadi Taubenfeld. Automatic discovery of mu\u00adtual exclusion algorithms. \nIn Proceedings of the 17th International Conference on Distributed Computing, DISC, pages 136 150, 2003. \n[7] Hans-J. Boehm. Reordering constraints for pthread-style locks. In Proceedings of the Twevelth ACM \nSIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP, pages 173 182, 2007. [8] \nSebastian Burckhardt, Chris Dern, Madanlal Musuvathi, and Roy Tan. Line-up: a complete and automatic \nlinearizability checker. In PLDI 10: Proceedings of the 2010 ACM SIGPLAN conference on Program\u00adming language \ndesign and implementation, pages 330 340, New York, NY, USA, 2010. ACM. [9] James Burns and Nancy Lynch. \nBounds on shared memory for mutual exclusion. Information and Computation, 107(2):171 184, December 1993. \n[10] David Chase and Yossi Lev. Dynamic circular work-stealing deque. In Proceedings of the Seventeenth \nAnnual ACM Symposium on Paral\u00adlelism in Algorithms and Architectures, SPAA, pages 21 28, July 2005. [11] \nEdsger W. Dijkstra. Solution of a problem in concurrent programming control. Commun. ACM, 8(9):569, 1965. \n[12] Faith Ellen, Panagiota Fatourou, and Eric Ruppert. Time lower bounds for implementations of multi-writer \nsnapshots. Journal of the ACM, 54(6):30, 2007. [13] Faith Fich, Danny Hendler, and Nir Shavit. On the \ninherent weakness of conditional primitives. Distributed Computing, 18(4):267 277, 2006. [14] Faith Fich, \nMaurice Herlihy, and Nir Shavit. On the space complexity of randomized synchronization. Journal of the \nACM, 45(5):843 862, September 1998. [15] Faith Fich, Victor Luchangco, Mark Moir, and Nir Shavit. Obstruction-free \nstep complexity: Lock-free dcas as an example. In DISC, pages 493 494, 2005. [16] Matteo Frigo, Charles \nE. Leiserson, and Keith H. Randall. The imple\u00admentation of the cilk-5 multithreaded language. In Proceedings \nof the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation, PLDI, pages 212 \n223, June 1998. [17] James R. Goodman. Cache consistency and sequential consistency. Technical report, \n1989. Technical report 61. [18] Gary Graunke and Shreekant S. Thakkar. Synchronization algorithms for \nshared-memory multiprocessors. IEEE Computer, 23(6):60 69, 1990. [19] Danny Hendler, Yossi Lev, Mark \nMoir, and Nir Shavit. A dynamic\u00adsized nonblocking work stealing deque. Distributed Computing, 18(3):189 \n207, 2006. [20] Danny Hendler and Nir Shavit. Non-blocking steal-half work queues. In Proceedings of \nthe Twenty-First Annual ACM Symposium on Prin\u00adciples of Distributed Computing, pages 280 289, July 2002. \n[21] Maurice Herlihy. Wait-free synchronization. ACM Trans. Program. Lang. Syst., 13(1):124 149, 1991. \n [22] Maurice Herlihy and Nir Shavit. The art of multiprocessor program\u00adming. Morgan Kaufmann, 2008. \n[23] Maurice Herlihy and Jeannette Wing. Linearizability: a correctness condition for concurrent objects. \nACM Trans. Program. Lang. Syst., 12(3):463 492, 1990. [24] Lisa Higham and Jalal Kawash. Java: Memory \nconsistency and pro\u00adcess coordination. In DISC, pages 201 215, 1998. [25] Lisa Higham and Jalal Kawash. \nBounds for mutual exclusion with only processor consistency. In DISC, pages 44 58, 2000. [26] IBM System/370 \nExtended Architecture, Principles of Operation, 1983. Publication No. SA22-7085. [27] Prasad Jayanti, \nKing Tan, and Sam Toueg. Time and space lower bounds for nonblocking implementations. SIAM Journal on \nComput\u00ading, 30(2):438 456, 2000. [28] Jalal Kawash. Limitations and Capabilities of Weak Memory Consis\u00adtency \nSystems. PhD thesis, University of Calgary, January 2000. [29] Michael Kuperstein, Martin Vechev, and \nEran Yahav. Automatic inference of memory fences. In Formal Methods in Computer Aided Design, 2010. [30] \nLeslie Lamport. Specifying concurrent program modules. ACM Trans. Program. Lang. Syst., 5(2):190 222, \nApril 1983. [31] Leslie Lamport. The mutual exclusion problem: part II -statement and solutions. J. ACM, \n33(2):327 348, 1986. [32] Leslie Lamport. A fast mutual exclusion algorithm. ACM Trans. Comput. Syst., \n5(1):1 11, 1987. [33] Jaejin Lee. Compilation Techniques for Explicitly Parallel Programs. PhD thesis, \nDepartment of Computer Science, University of Illinois at Urbana-Champaign, 1999. [34] Victor Luchangco, \nMark Moir, and Nir Shavit. On the uncontended complexity of consensus. In Proceedings of the 17th International \nConference on Distributed Computing, pages 45 59, October 2003. [35] Paul E. McKenney. Memory barriers: \na hardware view for software hackers. Linux Technology Center, IBM Beaverton, June 2010. [36] John M. \nMellor-Crummey and Michael L. Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. \nACM Trans. Comput. Syst., 9(1):21 65, 1991. [37] Maged M. Michael and Michael L. Scott. Simple, fast, \nand practical non-blocking and blocking concurrent queue algorithms. In Proceed\u00adings of the Fifteenth \nAnnual ACM Symposium on Principles of Dis\u00adtributed Computing, pages 267 275, May 1996. [38] Maged M. \nMichael, Martin T. Vechev, and Vijay Saraswat. Idempotent work stealing. In Proceedings of the Fourteenth \nACM SIGPLAN Sym\u00adposium on Principles and Practice of Parallel Programming, PPoPP, pages 45 54, February \n2009. [39] Shlomo Moran, Gadi Taubenfeld, and Irit Yadin. Concurrent counting. Journal of Computer and \nSystem Sciences, 53(1):61 78, August 1996. [40] Scott Owens, Susmit Sarkar, and Peter Sewell. A better \nx86 memory model: x86-tso. In TPHOLs, pages 391 407, 2009. [41] Gary L. Peterson. Myths about the mutual \nexclusion problem. Inf. Process. Lett., 12(3):115 116, 1981. [42] Vijay A. Saraswat, Radha Jagadeesan, \nMaged M. Michael, and Christoph von Praun. A theory of memory models. In Proceedings of the 12th ACM \nSIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP, pages 161 172, March 2007. \n[43] Susmit Sarkar, Peter Sewell, Francesco Zappa Nardelli, Scott Owens, Tom Ridge, Thomas Braibant, \nMagnus O. Myreen, and Jade Alglave. The semantics of x86-cc multiprocessor machine code. In POPL, pages \n379 391, 2009. [44] William E. Weihl. Commutativity-based concurrency control for ab\u00adstract data types. \nIEEE Trans. Computers, 37(12):1488 1505, 1988. [45] Glynn Winskel. The Formal Semantics of Programming \nLanguages. MIT Press, 1993.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Building correct and efficient concurrent algorithms is known to be a difficult problem of fundamental importance. To achieve efficiency, designers try to remove unnecessary and costly synchronization. However, not only is this manual trial-and-error process ad-hoc, time consuming and error-prone, but it often leaves designers pondering the question of: is it inherently impossible to eliminate certain synchronization, or is it that I was unable to eliminate it on this attempt and I should keep trying?</p> <p>In this paper we respond to this question. We prove that it is impossible to build concurrent implementations of classic and ubiquitous specifications such as sets, queues, stacks, mutual exclusion and read-modify-write operations, that completely eliminate the use of expensive synchronization.</p> <p>We prove that one cannot avoid the use of either: i) read-after-write (RAW), where a write to shared variable A is followed by a read to a different shared variable B without a write to B in between, or ii) atomic write-after-read (AWAR), where an atomic operation reads and then writes to shared locations. Unfortunately, enforcing RAW or AWAR is expensive on all current mainstream processors. To enforce RAW, memory ordering--also called fence or barrier--instructions must be used. To enforce AWAR, atomic instructions such as compare-and-swap are required. However, these instructions are typically substantially slower than regular instructions.</p> <p>Although algorithm designers frequently struggle to avoid RAW and AWAR, their attempts are often futile. Our result characterizes the cases where avoiding RAW and AWAR is impossible. On the flip side, our result can be used to guide designers towards new algorithms where RAW and AWAR can be eliminated.</p>", "authors": [{"name": "Hagit Attiya", "author_profile_id": "81100565190", "affiliation": "Technion, Haifa, Israel", "person_id": "P2509664", "email_address": "hagit@cs.technion.il", "orcid_id": ""}, {"name": "Rachid Guerraoui", "author_profile_id": "81100348136", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2509665", "email_address": "rachid.guerraoui@epfl.ch", "orcid_id": ""}, {"name": "Danny Hendler", "author_profile_id": "81100637193", "affiliation": "Ben-Gurion University, Beersheba, Israel", "person_id": "P2509666", "email_address": "hendlerd@cs.bgu.ac.il", "orcid_id": ""}, {"name": "Petr Kuznetsov", "author_profile_id": "81418594603", "affiliation": "TU Berlin/Deutsche Telekom Labs, Berlin, Germany", "person_id": "P2509667", "email_address": "pkuznets@acm.org", "orcid_id": ""}, {"name": "Maged M. Michael", "author_profile_id": "81332515587", "affiliation": "IBM T. J. Watson Research Center, Yorktown Heights, USA", "person_id": "P2509668", "email_address": "magedm@us.ibm.com", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "IBM T. J. Watson Research Center, Yorktown Heights, USA", "person_id": "P2509669", "email_address": "mtvechev@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926442", "year": "2011", "article_id": "1926442", "conference": "POPL", "title": "Laws of order: expensive synchronization in concurrent algorithms cannot be eliminated", "url": "http://dl.acm.org/citation.cfm?id=1926442"}