{"article_publication_date": "01-26-2011", "fulltext": "\n A Shape Analysis for Optimizing Parallel Graph Programs * Dimitrios Prountzos1 Roman Manevich2 Keshav \nPingali1,2 Kathryn S. McKinley1 1Dept. of Computer Science, The University of Texas at Austin, Texas, \nUSA. 2Institute for Computational Engineering and Sciences, The University of Texas at Austin, Texas, \nUSA. {dprountz@cs.utexas.edu,roman@ices.utexas.edu,pingali@cs.utexas.edu,mckinley@cs.utexas.edu } Abstract \nComputations on unstructured graphs are challenging to parallelize because dependences in the underlying \nalgorithms are usually com\u00adplex functions of runtime data values, thwarting static paralleliza\u00adtion. \nOne promising general-purpose parallelization strategy for these algorithms is optimistic parallelization. \nThis paper identi.es the optimization of optimistically paral\u00adlelized graph programs as a new application \narea, and develops the .rst shape analysis for addressing this problem. Our shape analysis identi.es \nfailsafe points in the program after which the execution is guaranteed not to abort and backup copies \nof modi.ed data are not needed; additionally, the analysis can be used to eliminate re\u00addundant con.ict \nchecking. It uses two key ideas: a novel top-down heap abstraction that controls state space explosion, \nand a strategy for predicate discovery that exploits common patterns of data struc\u00adture usage. We implemented \nthe shape analysis in TVLA, and used it to optimize benchmarks from the Lonestar suite. The optimized \npro\u00adgrams were executed on the Galois system. The analysis was suc\u00adcessful in eliminating all costs related \nto rollback logging for our benchmarks. Additionally, it reduced the number of lock acquisi\u00adtions by \na factor ranging from 10\u00d7 to 50\u00d7, depending on the ap\u00adplication and the number of threads. These optimizations \nwere ef\u00adfective in reducing the running times of the benchmarks by factors of 2\u00d7 to 12\u00d7. Categories and \nSubject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming Parallel Programming; D.1.3 \n[Programming Techniques]: Object-oriented Programming; F.3.2 [Logics and Meanings of Programs]: Program \nanalysis General Terms Algorithms, Languages, Performance, Veri.ca\u00adtion Keywords Abstract Interpretation, \nCompiler Optimization, Con\u00adcurrency, Parallelism, Shape Analysis, Static Analysis, Amorphous Data-parallelism, \nIrregular Programs, Optimistic Parallelization, Synchronization Overheads, Cautious Operators. * This \nwork was supported by NSF grants 0833162, 0719966, 0702353, 0724966, and 0739601, as well as grants from \nthe IBM and Intel Corpora\u00adtions. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 1. Introduction Computations on large, unstructured graphs \nare ubiquitous in many problem domains such as computational biology, machine learning, and data mining. \nThey are dif.cult to parallelize because most de\u00adpendences between computations in these algorithms are \nfunctions of values known only at runtime such as the structure of the (possi\u00adbly mutating) graph; therefore, \nit is impossible to parallelize these algorithms statically using techniques such as shape analysis and \npoints-to analysis [27]. One general-purpose solution to parallelizing graph compu\u00adtations is optimistic \nparallelization: computations are performed speculatively in parallel, but the runtime system monitors \ncon.icts between concurrent computations, and rolls back offending com\u00adputations as needed. There are \nmany implementations of this high\u00adlevel idea such as thread-level speculation [31], transactional mem\u00ad \nory [9, 12], and the Galois system [17]. For concreteness, our re\u00adsults are presented in the context \nof the Galois system but they are applicable to other systems as well. In the Galois system, applications \nprogrammers write algo\u00adrithms in sequential Java augmented with a construct called the Galois unordered-set \niterator1. This iterator iterates in some un\u00ad speci.ed order over a set of active nodes, which are nodes \nin the graph where computations need to be performed. The body of the iterator is considered to be an \noperator that is applied to the active node to perform the relevant computation, known as an activity. \nAn activity may touch other nodes and edges in the graph, and these are collectively known as the neighborhood \nfor that activity. These nodes and edges must be accessed by invoking methods from graph classes provided \nin the Galois library. Figure 1. Neighborhoods in Boruvka s MST algorithm We illustrate these concepts \nusing Boruvka s minimal spanning tree (MST) algorithm [7], the running example in this paper. The MST \nstarts as a forest with each node in its own component. The algorithm iteratively contracts the graph \nby non-deterministically choosing a graph node, examining all edges incident on that node to .nd the \nlightest weight edge, and contracting that edge, which is 1 The Galois system also supports ordered-set \niterators, but we do not consider these in this paper.  added to the MST. The algorithm terminates when \nthe graph has been contracted to a single node. Figure 1 shows an undirected graph. For active node e, \nthe neighborhood of the corresponding activity consists of nodes d, e and f, and the edges between these \nnodes, since these are the edges that must be examined to .nd and contract the lightest weight edge connected \nto e. In most algorithms, each neighborhood is a small portion of the overall graph, so it is possible \nto work on many active nodes concurrently provided the corresponding neighborhoods do not overlap. For \nexample, in Figure 1, the neighborhood for the activity at node c is disjoint from the neighborhood for \nthe activity at node e, so these activities can be performed in parallel. However, the activity at node \nb cannot be performed concurrently with the activity at e since the neighborhoods overlap. In the Galois \nsystem, this concurrency is exploited by adding all graph nodes to the work-set and executing iterations \nof the Galois set-iterator speculatively in parallel. All concurrency control is performed within the \nlibrary graph classes. Conceptually, an exclusive lock called an abstract lock is associated with each \ngraph element, and this lock is acquired by an activity when it touches that element by invoking a graph \nAPI method. If the lock has already been acquired by another activity, a con.ict is reported to the runtime \nsystem, which rolls back offending activities. To permit rollback, methods that modify the state of the \ngraph also record undo actions that are executed on rollback. The idea of handling con.icts at the abstract \ndata type level rather than at the memory level is also used in boosted TM systems [11]. The Galois system \nhas been used to parallelize complex graph algorithms in the Lonestar benchmark suite [15]. Compared \nto static parallelization, optimistic parallelization has several overheads. 1. Wasted work from aborted \nactivities: Because con.icts between activities are detected online, an activity may be rolled back after \nit has performed a lot of computation. 2. Con.ict checking: Abstract locks must be acquired and released \nby activities, and this is an overhead even if no activities are ever aborted. 3. Undo actions: These \nmust be registered for every graph API call that might mutate the graph.  In this paper, we present \na novel shape analysis that can be utilized to reduce the overheads of con.ict checking and registering \nundo actions (reducing the number of aborted activities is mainly a scheduling problem, and is dealt \nwith elsewhere [16]). Our main contributions are the following. Shape Analysis: We develop a novel shape \nanalysis for pro\u00adgrams with set and graph data structures, which infers prop\u00aderties for optimizing speculative \nparallel graph programs. We utilize the structure of stores arising in our programs to design a hierarchy \nsummarization abstraction, which uses a .nite set of reachability relations relative to a given property \n(the object\u00adis-locked property), to abstract stores into shape graphs . Our abstraction assigns unary \npredicates only to root objects, cap\u00adturing reachability facts from root objects to objects deeper in \nthe heap. Thus, the size of an abstracted store is linear in the number of variables, and the number \nof abstracted stores at a program point depends on the number of explored variable-alias sets, which \ntends to be constant in our programs ( 6). There\u00adfore the number of abstract states explored by our analysis \nin practice is linear in the size of the program, circumventing the state-space explosion that is the \nbane of existing shape analyses.  Predicate Discovery: We develop a simple yet effective tech\u00adnique \nfor discovering predicates relevant for inferring the set  objects that are always locked, at each program \nlocation, from data structure speci.cations and footprints of data structure method speci.cations. Evaluation \nof effectiveness: We implement our shape analysis in the TVLA framework and use a Java-to-TVLA front-end \nto analyze several benchmarks from the Lonestar Benchmark Suite [15], a collection of real-world graph-based \napplications that exhibit irregular behavior. Our analysis takes at most 16 seconds on each benchmark \nand infers all available optimiza\u00adtions. These optimizations result in substantial improvements in running \ntime, ranging from 2\u00d7 to 12\u00d7. Several existing heap abstractions, including Canonical Ab\u00adstraction [32], \nBoolean heaps [28], indexed predicate abstrac\u00adtion [20], and generalized typestates [21], abstract the \nheap by recording a set of unary predicates for every object and summariz\u00ading the heap by collapsing \nequivalence classes of objects with the same set of predicate values. Such abstractions achieve high \npreci\u00adsion, as they express every Boolean combination of intersection and union of objects satisfying \nthose predicates. However, the size of a summarized heap can be exponential in the number of predicates, \nand the summarization of a set of stores can be doubly-exponential. We call these bottom-up abstractions, \nsince they typically express reachability facts for objects in the depth of the heap relative to heap \nroots. Our experience with bottom-up abstraction shows that heaps are partitioned very .nely, leading \nto state space explosion. As we discuss in Section 4, our top-down abstraction runs several orders of \nmagnitude faster than an implementation of the bottom\u00adup abstraction approach when analyzing our benchmarks. \nThe rest of the paper is organized as follows. Section 2 provides an overview of our optimizations and \nshape analysis on Boruvka s MST example. Section 3 presents our shape analysis via hierarchy summarization \nand predicate discovery. Section 4 describes the static analysis implementation and gives experimental \nresults that demonstrate the effectiveness of the approach presented in this paper. Section 5 discusses \nrelated work. 2. Overview This section introduces the programming model, the performance optimizations, \nand our shape analysis informally, using Boruvka s MST algorithm as the running example. 2.1 Boruvka \ns MST algorithm Pseudocode for the algorithm is shown in Figure 2. The Galois iterator on line 25 iterates \nover the graph nodes in some non\u00addeterministic order, performing edge contractions. In lines, 31-38 we \nexamine the neighbors of the active node a, and identify the neighbor lt, which is connected to a by \na lightest weight edge. In lines 44-59 we contract the two components by removing lt from the graph, \nand updating all of lt s neighbors to become neighbors of a. This is done by the loop in lines 45-58. \nIf a neighbor n of lt is already connected to a, we update the data value of the edge connecting them \n(lines 49-54). Otherwise, we add an edge connecting the two nodes (lines 55-57). In Boruvka s algorithm, \nthe neighborhood of an active node a consists of the immediate neighbors of a and lt and their related \nedges and data. In more complex examples like Delaunay mesh re.nement, the neighborhood of an activity \ncan be an unbounded subgraph.  2.2 Speculative Execution in Galois The graph data structure Graph<ND,ED> \nis parameterized by data objects referenced by nodes and edges, respectively. The work list of active \nnodes is stored in a set GSet<Node>. The Weight ob\u00ad  1 class GaloisRuntime { 2 @rep static set<Object> \nlocks; // abstract locks 3 // Flag options 4 static int LOCK UNDO=0; // acquire locks + log undo 5 static \nint UNDO =1; // log undo 6 static int LOCK =2; // acquire locks 7 static int NONE =3; // no locks and \nno undo 8 } 9 10 class Weight { 11 static Weight MAX WEIGHT; 12 int v; 13 // We record the end-points \nof the edge that 14 // holds the weight in the input graph. 15 .nal Node<Void> initSrc, initDst ; 16 \nint compareTo(Weight other); 17 } 18 class Boruvka { 19 void main() { 20 Graph<Void,Weight> g = ...// \nread from .le 21 GSet<Node> wl = new GSet<Node>(); 22 wl.addAll(g.getNodes(NONE), NONE); 23 GBag<Weight> \nmst = new GBag<Weight>(); 24 25 // Galois iterator 26 foreach (Node a : wl) { // in any order 27 L1: \nSet<Node> aNghbrs = g.getNeighbors(a, LOCK); 28 // Find neighbor incident to lightest edge 29 Weight \nminW = Weight.MAX WEIGHT; 30 Nodelt = null ; 31 L2: for (Node n : aNghbrs) { // Iterator nIter 32 Edge \ne = g.getEdge(a, n, NONE); 33 Weight w = g.getEdgeData(e, NONE); 34 if (w.compareTo(minW) < 0) { 35 minW=w; \n36 lt =n; 37 } 38 } 39 if ( lt == null) // no neighbors 40 continue; 41 // Contract edge (a, lt ) 42 \nL3: g.getNeighbors(lt, LOCK); // avoids undo in L4 43 L4: g.removeEdge(a, lt , NONE); 44 L5: Set<Node> \nltNghbrs = g.getNeighbors(lt, NONE); 45 L6: for (Node n : ltNghbrs) { // Iterator nIter 46 Edge e = g.getEdge(lt \n, n, NONE); 47 Weight w = g.getEdgeData(e, NONE); 48 Edge an = g.getEdge(a, n, NONE); 49 if (an != null \n) { // merge edges 50 Weight wan = g.getEdgeData(an, NONE); 51 if (wan.compareTo(w) < 0) 52 w=wan; // \nuse minimal weight 53 L7: g.setEdgeData(an, w, NONE); 54 } 55 else { // new neighbor for a 56 L8: g.addEdge(a, \nn, w, NONE); 57 } 58 } 59 L9: g.removeNode(lt, NONE); 60 L10: mst.add(minW, NONE); 61 L11: wl.add(a, \nNONE); // put node back on worklist 62 }}} Figure 2. Simpli.ed implementation of Boruvka s algorithm. \njects, which record the weights of the MST edges and their end\u00adpoints (nodes) in the original graph, \nare stored in another collection GBag<Weight>, which only allows addition operations in a concur\u00adrent \ncontext. The last argument to a data structure method is a .ag that tells the runtime system whether \nthe method should attempt to acquire abstract locks and whether it should log an inverse method call. \nThe default value LOCK UNDO is always a safe choice, ensuring correctness of speculative execution. The \nGalois system protects user-de.ned data types, such as Weight, using a read-write lock (allowing concurrent \nread oper- Syntactic Categories TName Types OFld Pointer .elds SFld Set .elds Field All .elds PVar Pointer \nvariables BVar Boolean variables SVar Set-valued variables Var All variables Data Types (EBNF) TypeDecl \n::= class TName{FieldDecl* MethodDef* } FieldDecl ::= [@rep][static] TName OFld; | @rep [static] set(TName) \nSFld; MethodDef ::= @locks(Path* ) @op(Stmt* ) Java-code Stmt ::= Var = Expr | Var.Field = Expr Expr \n::= Path | Path + Path | Path - Path | choose(Path) | Path in Path | Path notIn Path |isEmpty(Path) | \nnew TName((Field = Var) * ) Path ::= Var.(Var + Field[:SVar]+ rev(Field)[:SVar])+ Figure 3. EBNF grammar \nfor speci.ed data structures. The nota\u00adtion [x] means that x is optional. ations but at most one write \noperation) and maintaining backup copies of such objects. Iterations of the Galois set iterator are executed \nspeculatively in parallel, and this execution has transactional semantics: an iteration either completes \nand commits, or is rolled back and retried.  2.3 Data Structure Speci.cations Figure 3 shows the syntax \nfor a lightweight speci.cation of ab\u00adstract data types (for all clients), de.ning their abstract state, \nab\u00adstract locks acquired by each method, and operational semantics of each method in terms of abstract \n.elds. The set of variables, includes method parameters, the special ret parameter for return\u00ading values, \nstatic variables, and temporary variables used to de.ne the semantics of methods. The formal semantics \nof this language can be found in the accompanying report [30]. Our analysis op\u00aderates in terms of these \nspeci.cations, ignoring the internal details of library ADT s. We assume the correctness of the speci.cations; \napproaches such as [34] can be used for their veri.cation. Figure 4 shows a graph type built from the \nNode and Edge types and the parametric types ND and ED, used to store user-de.ned data on the graph nodes \nand edges. In our example, nodes do not store any data objects and thus their nd .elds are null. Figure \n5 shows a bag, a set, and an iterator type. Specifying Abstract Data Types. The @rep annotations in Fig\u00adure \n4 de.ne the abstract state of a data structure in terms of set\u00ad.elds [18], i.e., .elds whose values are \nsets of objects. For example, the abstract state of Graph is given by a pair of sets ns and es representing \nthe set of graph nodes and edges, respectively. 2 We represent an undirected edge by a single edge, directed \narbitrarily. GaloisRuntime contains a static (i.e., global) locks set, rep\u00adresenting the set of abstract \nlocks acquired by an iteration. 3 Example 1. Figure 6 shows an abstract store representing the input \ngraph of Figure 1 where a references the active node a and lt references c the node connected to a by \nthe lightest edge, discovered on the second iteration after iterating over b. 2 The full speci.cation \nincludes additional .rst-order constraints. 3 Galois implements a lock coarsening scheme by maintaining \na single set of abstract locks, shared among all data structure instances.  class Graph<ND, ED> { // \nUndirected boosted graph @rep set<Node> ns ; / / graph nodes @rep set<Edge> es ; / / graph edges @locks(n.rev(src).dst, \nn.rev(dst).src) @op(nghbrs = n.rev(src).dst + n.rev(dst).src, ret = new Set<Node<ND>>(cont =nghbrs ) \n) Set<Node<ND>> getNeighbors (Node<ND> n, int opt ) ; @locks(f.rev(src).dst.t, t.rev(src).dst.f) @op(f.rev(src):eft.dst.t, \nt.rev(src):etf.dst.f, ret = choose(eft + etf)) Edge<ED> getEdge (Node<ND> f , Node<ND> t, int opt ) ; \n@locks(f, t, f.rev(src).dst.t, t.rev(src).dst.f) @op(f.rev(src):eft.dst.t, t.rev(src):etf.dst.f, ret \n= (eft+etf) in es, ne = new Edge<ED>(src =f , dst =t , ed=d) , es += ne) boolean addEdge (Node<ND> f \n, Node<ND> t, EDd, int opt ) ; @locks(n.rev(src).dst, n.rev(dst).src) @op(ret = n in ns, ns -= n, es \n-= n.rev(src) + n.rev(dst)) boolean removeNode (Node<ND> n, int opt ) ; @locks(f.rev(src).dst.t, t.rev(src).dst.f) \n@op(f.rev(src):eft.dst.t, t.rev(src):etf.dst.f, ret = (etf + eft) in es, es -= (etf + eft)) boolean removeEdge \n(Node<ND> f , Node<ND> t, int opt ) ; @locks (n) @op(ret = n.nd) ND getNodeData(Node<ND> n, int opt ) \n; @locks(e, e.src , e.dst) @op(ret = e.ed) ED getEdgeData(Edge<ED> e, int opt ) ; @locks(e, e.src , e.dst) \n@op(e.ed = d) void setEdgeData (Edge<ED> e, EDd, int opt ) ; } class Node<ND> {ND nd; // data object \n} class Edge<ED> {Node src ; // edge origin Node dst ; // edge destination ED ed; // data object } Figure \n4. Graph speci.cation samples. The .gure does not show objects used by the internal (concrete) representation \nof speci.ed data types. Instead, it uses the (@rep) set .elds to indicate that an object is contained \nin a set .eld of a data structure. Filled locks denote objects in GaloisRuntime.locks; hollow locks denote \nobjects for which our analysis infers that lock protec\u00adtion is not required. Path Language. We use a \nlanguage of access path expressions (access paths for short) to denote the set of objects that can be \nob\u00adtained by following variables and .elds in a store: a variable (x) de\u00adnotes the object it references; \na pointer .eld e.f denotes an object obtained by traversing the .eld f forward from an object denoted \nby the pre.x expression e; a set .eld denotes any object stored in the set stored in a given object; \na reverse .eld, written rev(f) or . - f , denotes objects obtained by traversing .eld f backwards. We \nformalize path expressions in Section 3. class GSet<E> { // boosted set @rep set <E> gcont ; // set contents \n@locks ( e ) @op(ret = e in gcont) boolean contains(E e); @locks ( e ) @op(ret = e notIn gcont, gcont \n+= e) boolean add(E e); } class GBag<E> { // boosted bag for reduction operations @rep set <E> bcont \n; // bag contents @locks() // No locks required! @op( bcont += e ) void add(E e); @op( ret = new Set<E>(bcont \n) ) Set<E> toSet () ; // used only in sequential code } interface Set<E> { // sequential set from java \n. util @rep set <E> cont ; // set contents } interface Iterator <E> { // iterator from java. util @rep \nSet<E> all ; // underlying set @rep set <E> past ; // past iteration elements @rep E at; // element at \ncurrent iteration @rep set <E> future ; // future iteration elements } Figure 5. Set, bag and iterator \nspeci.cation samples. We use the notation :x inside a path expression to denote a set of intermediate \nobjects during an access path traversal. We write + and -for set union and difference respectively. Example \n2 (Legend). The following paths are derived from the abstract store in Figure 6: a.rev(src) represents \nthe outgoing edges of a: {2, 3}.  a.rev(dst) represents the incoming edges of a: \u00d8.  a.rev(src).dst \n+ a.rev(dst).src represents all of the graph nodes adjacent to a: {b, c}.  a.rev(src):x.dst.lt sets \nthe temporary variable x to the edge from a to lt: x = {3} (2 is not on a path from a to lt).  Specifying \nAbstract Locks. A @locks annotation de.nes the set of abstract locks a method should acquire by a set \nof path ex\u00adpressions. To keep speci.cations succinct, access paths expressions in @locks stand for all \nof their pre.xes (e.g., n.rev(src).dst stands for n, n.rev(src), and n.rev(src).dst). We call a node \nreferenced by n along with its incident edges and adjacent nodes the immediate neighborhood of n. We \nspecify the set of locks for such a neighborhood by @locks(n.rev(src).dst, n.rev(dst).src). Example 3 \n(Commutativity via abstract locks). The removeNode method speci.es locks for the immediate neighborhood \nof the node being removed. A call to removeNode( c,LOCK) attempts to lock the Node object c, the Edge \nobjects referencing c via the src .eld or the dst .eld (the edges incident to c), and the Node ob\u00adjects \nthat are neighbors of c. This ensures the concurrent method calls removeNode( c,LOCK) and removeNode( \ne,LOCK) will not cause their respective iterations to abort, since the immediate neighborhoods of c and \ne do not overlap. Specifying Method Semantics. An @op annotation de.nes the se\u00admantics of a method by \na simple imperative language. The language allows a sequence of statements, using set expressions over \nmethod parameters, static .elds, and temporary variables. Expressions of the form a in b, a notIn b, \nand isEmpty(a), test whether a is contained in b, a is not contained in b, and whether a is an empty \nFigure 6. An abstract store arising at L2, using as input the graph from Figure 1. Object are shown by \nrectangles sub-divided by their .elds; circles are used to name objects; locks show objects contained \nin the global GaloisRuntime.locks set. We label Node objects by the same labels used in Figure 1 and \nother objects by a running index.   Figure 7. A shape graph obtained by applying hierarchy summa\u00adrization \nabstraction to the store in Figure 6. Grey boxes represent sets of locked objects. v=T indicates that \nthe numeric value of v has been abstracted away. set, respectively. (We overload these expressions to \ntreat reference variables as singleton sets.) Statements of the form a += exp and a -= exp are shorthand \nfor a = a + exp and a = a -exp, re\u00adspectively. choose(exp) non-deterministically chooses an object from \na set denoted by exp.  2.4 Optimization Opportunities Our static analysis enables the following optimizations. \nEliminating Usage of Concurrent Data Structures. The follow\u00ading conditions allow replacing a concurrent \nimplementation of a data structure by a sequential implementation: the data structure is iteration private, \nor the data structure is never modi.ed. We use a purity analysis [33] to discover objects that are never \nmodi.ed in\u00ad side an iteration (such as Weight in the running example). Reducing Rollback Logging. Logging \ninverse method calls for iterations that commit represents wasted work, as the log is cleared when the \niteration commits and the logged method calls are never used. Our static analysis .nds a minimal set \nof failsafe points pro\u00adgram locations in the client program such that an iteration reaching them cannot \nabort. The analysis computes an under-approximation of the set of objects that are always locked at a \nprogram location. If the set of locks computed for a location L subsumes the set of locks computed for \nall locations reachable from L, then L is a fail\u00adsafe point. An iteration reaching a failsafe point will \nnever fail to acquire a lock and therefore cannot abort. We eliminate logging inverse actions for method \ncalls appearing after a failsafe point. If no method call before a failsafe point modi.es shared data \nstructures, rollback logging is not needed anywhere in the iteration. Algorithms with this property are \ncalled cautious algorithms [26]. Eliminating Redundant Locking. Our analysis can also be used to .nd \nmethod calls for which all locks have already been acquired by preceding method calls. Lock acquisitions \ncan be eliminated for these calls. Furthermore, our analysis .nds user-de.ned objects dominated by other \nlocked objects, i.e., objects that can only be accessed after a unique abstract lock is acquired. We \neliminate lock operations for such objects as well.  2.5 Optimizing the Running Example by Static Analysis \nWe develop a sound static analysis to automatically infer available optimizations of the kind discussed \nabove. The input to our analysis is a Java program with a single parallel loop, given by the foreach \nconstruct, operating over a library of speci.ed boosted data struc\u00adtures. The output of our analysis \nis an assignment of option .ags to each ADT method call and a list of (user-de.ned) types that do not \nneed transactional protection. The core component of our analysis is a shape analysis that under-approximates \nthe set of objects that are always locked at a program point. Intuitively, our analysis abstracts stores \ninto bounded-size shape graphs by collapsing all objects not referenced by variables together and recording \nfor each root object a set of path expressions denoting the set of locked objects. Figure 7 shows a shape \ngraph obtained by applying our abstrac\u00adtion to the store in Figure 6. The object labeled by a double-circle \nshows the set of collapsed objects. The grey box pointing to a ex\u00adpresses the fact that the immediate \nneighborhood of a is locked, along with the Weight objects referenced by its incident edges. This shape \ngraph represents an intermediate invariant inferred by our analysis at L2. The full invariant is given \nby a set of the shape graphs at that point, at the .xpoint. Below, we provide sample invariants that \nour analysis infers for Figure 2 and the corresponding path expressions denoting sets of objects all \nof which are locked: Inv1: At L2, the immediate neighborhood of a is locked: a + a.rev(dst).src + a.rev(src).dst. \nInv2: At L4-L9, the immediate neighborhoods of a and lt are locked: a + a.rev(dst).src + a.rev(src).dst \n+ lt + lt.rev(dst).src + lt.rev(src).dst. Inv3: At L2 and L6, all graph nodes accessible by the itera\u00ad \ntor nIter (past, present, and future iterations) are locked: nIter.past + nIter.at + nIter.future. Inv4: \nAt 33, 47, 50, and 53, the edges referenced by e and an and the nodes they reference are locked: e + \nan + e.src + e.dst + an.src + an.dst. Inv1 is part of the invariant needed to prove that L4 is a failsafe \npoint (before executing the statement). Inv1 needs to be maintained from L2 and on. It is also used to \neliminate locking in lines 32-33. Inv2 helps establish L4 as a failsafe point, since all accesses to \nnodes and edges in the second loop are to objects known to be locked. Also, it helps eliminate redundant \nlocking at L9. Inv3 helps establish the failsafe point at L4 by the fact that the node referenced by \nn is locked at 46 and 48, and eliminate locking at 32. Finally, Inv4 establishes that the calls to getEdgeData \nand setEdgeData in lines 33, 47, 50, and 53 do not lock new objects.  Additionally, our analysis infers \nthat Weight objects are read\u00adonly, which enables eliminating all lock operations and backup copy maintenance \nfor them. Points L7, L8, L10, L11 are after the failsafe point and do not require storing inverse actions. \nAt L10, the calls to the add method of Bag do not require acquiring locks, and trivially commute. We \napply these optimizations to the code of Figure 2 by setting the LOCK option at L1 and L3, which eliminates \nrollback logging, and setting the NONE option in all other calls, eliminating both abstract locking and \nrollback logging. This implementation of Boruvka s algorithm is cautious: our analysis infers that the \nfailsafe point is L4 and that no modi.cations are made to the graph between L1 and L4. If we remove the \nstatement at L3, the failsafe point is at L5, which requires logging an inverse method call for g.removeEdge(a, \nlt).4 3. A Shape Analysis for Graph Programs This section presents our static analysis for enabling the \noptimiza\u00adtions described in previous sections. Our analysis considers only sequential executions, but \nthe inferred properties apply to concur\u00adrent executions as well. We use a result by Filipovic et al. \n[8] and the fact that our concurrent executions are strictly serializable to formally justify this [30]. \nThe core component of the analysis is a shape analysis that under-approximates the set of objects that \nare always locked, at each program location. This section is organized as follows: (1) we discuss the \nclass of programs and stores that our shape analysis addresses; (2) we de.ne Canonical Abstraction [32] \nand partial join [23] in our setting; (3) we de.ne Hierarchy Summarization Abstraction (HSA); (4) we \npresent a technique for discovering predicates relevant to our analysis; (5) we explain how the results \nof the shape analysis are used; (6) we contrast our abstraction with Backward Reachability Abstraction \n(BRA), a commonly used form of shape abstraction; (7) we discuss how our analysis can aid the programmer \nby providing non-cautiousness counterexamples; and .nally (8) we discuss limitations of our analysis. \n3.1 A Class of Programs and Stores We analyze Java programs (excluding recursive procedures) where the \nimplementation of speci.ed data structures is replaced by the abstract .elds in the @rep annotations \nand the semantics of meth\u00adods is given by the @op annotations. Figure 8 de.nes stores in terms of pointer \n.elds and set-valued .elds de.ned by the @rep annotations. We de.ne the meaning of path expressions (recursively), \nwhich denote sets of objects reach\u00adable from a variable by following .elds in speci.ed directions and \ngoing through speci.ed variables. The last de.nition in Figure 8 provides the meaning of variables assigned \nto intermediate objects along path expressions, such as f.rev(src):eft.dst.t. Bounded-depth Hierarchical \nStores. We de.ne the set of types reachable from an object o (by forward paths) to be the set of types \nof all objects in [ o.p] for all path expressions p. This paper focuses on the class of bounded-depth \nhierarchical stores stores where the set of types reachable from [ o.f] is a proper subset of the set \nof types reachable from o, for every object o and .eld f . Such stores are acyclic the length of any \n4 Swapping L4 and L5 makes the code cautious once again, but breaks sequential correctness, since in \nthe Galois library it is illegal to remove an edge while iterating over the neighbors of a node incident \nto it. unidirectional path, i.e., a path where all .elds are either forward or reverse, is linearly bounded \nby the number of program types.  3.2 Canonical Abstraction and Partial Join We implement our shape analysis \nusing the TVLA system [22], which allows de.ning stores by .rst-order predicates, program statements \nby .rst-order transition formulae (formulae relating the values of predicates after a statement to those \nbefore), and abstract states by .rst-order abstraction predicates. The system automati\u00adcally generates \nsound abstract operations and transformers, yield\u00ading a sound abstract interpretation for a given program. \nTVLA uses Canonical Abstraction [32], which abstracts stores into 3-valued logical structures. To focus \nour presentation on the important details of our analysis, we simplify our description of TVLA s abstraction \nand use shape graphs for abstract states instead of 3-valued structures. De.nition 3.1 (Shape Graph). \nLet P = AP . NAP be a set of predicates consisting of two disjoint sets of unary predicates called abstraction \npredicates (AP) and non-abstraction predicates (NAP). A shape graph G is a tuple (NG,P G,EG) where NG \nis a set of abstract objects, P G : NG . 2P assigns predicates to objects, and EG : OFld . SFld . NG \n\u00d7 NG is a set of may-edges for each .eld. We denote the set of shape graphs over P by ShapeGraph[P]. \nWe call the set of abstraction predicates assigned to an abstract def P G node v . NG its canonical name: \nCName(v)= (v) n AP.A shape graph G is bounded if no two abstract nodes have the same canonical name. \nThis means that the number of abstract nodes in a bounded shape graph is exponentially bounded by the \nnumber of abstraction predicates. We de.ne the abstraction function \u00df[P ]: Store . BGraph[P ], which \nmaps a store s =(Ss,Hs) into a bounded shape graph G as follows. We use the helper function P s : TO \n. 2P, which eval\u00aduates the predicates in P for each object, and \u00b5s,G : TO . NG , which maps store objects \nhaving an equal canonical name to an abstract node representing their equivalence class in G. The pred\u00adicate \nassignment function assigns to abstract nodes the predicates common to all objects they represent, and \na .eld edge exists be\u00adtween two abstract nodes if there exist two objects represented by the abstract \nnodes that are related by that .eld. s,Gs,Gss \u00b5(o1)= \u00b5(o2) .. P (o1) n AP = P (o2) n AP P GP s (n)=(o) \ns,G(o) o s.t. n=\u00b5 EG(f)= {(n1,n2) |.o1,o2 : n1 = \u00b5s,G(o1),n2 = \u00b5s,G(o2). n o2 = Hs(f)(o1),f . OFld; o2 \n. Hs(f)(o1),f . SFld. } . We say that a shape graph G' subsumes a shape graph G, written G,G! : NG . \nNG! G . G', if there exists an onto function \u00b5, such G,G! that P G(n) . P G! (\u00b5(n)) for all n . NG, and \n(n1,n2) . G,G! G,G! EG! EG(f) implies that (\u00b5(n1),\u00b5(n2)) . (f) for all n1,n2 . NG,f . OFld . SFld. The \nmeaning of a shape graph G is given by the function .[P ]: ShapeGraph[P ] . 2Store de.ned as .[P ](G)= \n{s | \u00df[P ](s) . G}. We say that two shape graphs G and G' are congruent if there G,G! exists a bijection \nbetween their sets of abstract nodes \u00b5: NG . NG! , which preserves the abstraction predicates: P G(n) \nn G,G! AP = P G! (\u00b5(n)) n AP for all n . NG. Two congruent shape graphs G and G' can be subsumed by a \ncongruent shape graph G'' = G U G', by intersecting corresponding predicate values and taking the union \nof corresponding edges using the bijections  Stores Semantics of Path Expressions [ Path] : Store . \n2TO Base case. Variables: {Ss(x)},x . PVar; [ x]](s)= Ss(x),x . SVar. n Inductive case. p . Path, [ \np]](s) is known, and e is a .eld or variable: [ p]](s) n [ e]](s), e . PVar . SVar; {Hs . .. . . TO Objects \nStack : PVar . TO . Stacks SVar . 2TO . e . OFld; (o, e) | o . [ p]](s)}, Hs(o, e),e . SFld;  [ p.e]](s)= \nBVar .{T, F}Heap :(TO \u00d7 OFld) . TO . Heaps .. . . o.[ p]](s) {o . TO | Hs(o, f) . [ p]](s)},e = rev(f),f \n. OFld; (TO \u00d7 SFld) . 2TO {o . TO | Hs(o, f) n [ p]](s) = \u00d8}, Store : Stack \u00d7 Heap Stores For an object \no . TO and path p . Path, we de.ne s =(Ss,Hs) Store notation [ o.p]](s)= let y be fresh,s ' e = rev(f),f \n. SFld. =(Ss|y . o, Hs) in [ y.p]](s ' ) Meaning of intermediate variables: The expression x.p:v.q assigns \nto v the set of objects that are both on a path from x to q and in x.p. For x . Var, v . SVar, p, q . \nPath [ v]](s)= [ x.p]](s) n{o . TO | [ o.q]](s) = \u00d8}. Figure 8. Stores and semantics of path expressions. \nG!!,G : NG!! G!!,G! : NG!! \u00b5 . NG and \u00b5 . NG! : Predicates Meaning NG!! Abstraction Predicates NG = \n{x(v) | x . Var} x references v P G!! (n)= P G(\u00b5 G!!,G(n)) n P G! (\u00b5 G!!,G! (n)) Non-abstraction Predicates \nG!!,GG!!,G (n1,n2) . EG!! (f) . (\u00b5 (n1),\u00b5 (n2)) . EG(f) or {ForwardReach[x, p](v) | Hierarchy summarization \n(\u00b5 (n1),\u00b5 (n2)) . EG! (f) .x . Var,p . AbsPaths} predicates G!!,G! G!!,G! We use TVLA s partial join \noperator [23], which merges con-Table 1. P HSA predicates for hierarchy summarization abstraction. gruent \nshape graphs into a single shape graph, and keeps non\u00adcongruent shape graphs in a set: n def {G U G ' \n},G and G ' are congruent; {G}U{G ' } = {G, G ' }, else. . 2BGraph The abstraction of a set of stores \na[P ]:2Store [P ] is de.ned as a[P ](S) = s.S \u00df[P ](s).  3.3 Hierarchy Summarization Abstraction Our \nabstraction is de.ned relative to a set of abstraction paths, denoted by AbsPaths, which represent possible \npaths from variables to locked objects. The next subsection discusses a technique to discover a set of \nuseful abstraction paths for a set of data structures. Let s =(Ss,Hs) be a store. For a pointer variable \nx and an abstraction path p, we de.ne a unary predicate expressing the fact that v is a root object referenced \nby x and all objects reachable from it by the path p are locked: def s ss ForwardReach[x, p](v)= [ x] \n= {v}. [ x.p] . [ locks] . We encode hierarchy summarization abstraction via shape graphs and the set \nof predicates P HSA, shown in Table 1, and the ab\u00adstraction paths in Table 2. Since a pointer variable \npoints to at most one node, the number of abstract nodes in a bounded shape graph G . BGraph[P HSA] is \nequal to at most the number of heap roots + 1 (in the case where there exist non-root objects). The canonical \nnames in such a shape graph are the sets of aliased pointer vari\u00adables. We call such sets aliasing con.gurations. \nIn practice, the average number of different aliasing con.gurations discovered by our analysis is a small \nconstant ( 6), which means that the set of bounded shape graphs our analysis explores is linear in the \nnumber of program locations. Figure 7 shows the result of applying \u00df[P HSA] to the store in Figure 6 \nand the predicates in Table 1. Heap roots are labeled with path expressions that denote the sets of objects \nthat are reachable Type Abstraction Paths .-.- es, es.src, es.dst, ns, ns.src, ns.dst, .- .- .- .- ns.dst.ed, \nns.dst.ed, ns.src.dst, ns.dst.src, Graph .-.- ns.nd, ns.src.dst.nd, ns.dst.src.nd, es.ed, es.src.nd, \nes.dst.nd .- .- .- .- a, lt, n, src, dst, src.dst, dst.src, nd Node .- .- .- .- src.ed, dst.ed, src.dst.nd, \ndst.src.nd Edge e, an, src, dst, ed, src.nd, dst.nd .-.... --- - Weight ed, ed.src, ed.dst, ed.src.nd, \ned.dst.nd .- .- .- cont, cont.src, cont.dst, cont.src.dst, .- .- .- Set cont.dst.src, cont.src.ed, cont.dst.ed, \n.- .- cont.src.dst.nd, cont.dst.src.nd, cont.nd GSet gcont, gcont.nd GBag bcont all, all.cont, all.cont.nd, \nIterator past, at, future, past.nd, at.nd, future.nd Table 2. Abstraction paths for the running example. \nWe omit Java Generics parameters when no confusion is likely. from them and are de.nitely locked. At \nL2, we would expect that node a, its neighbors, and the edges connecting a are locked. This is speci.ed \nby the path expressions labeling node a. For example, a.rev(src).dst, a.rev(dst).src refer to all the \nneighbors of a. Additionally, the current element that we are iterating over is node c, which is the \nlightest neighbor of a; this node has its single incoming edge and edge data locked. All other (non-root) \nnodes, edges, and Weight objects are collapsed by our abstraction.  3.4 Predicate Discovery We now \ndescribe heuristics for generating the set of abstraction paths from the data structures in a program, \nand show how it .nds paths expressing the invariants described in Section 2. Our tech\u00ad nique constructs \npaths in three phases: (a) building the type de\u00adpendence graph, (b) discovering variable-to-lock paths \nin method speci.cations, and (c) combining variable-to-lock paths and all for\u00adward paths in the type \ndependence graph. De.nition 3.2 (Type Dependence Graph). A type dependence graph for a program, contains \na type node NT for each program type T , labeled by the set of program variables of that type; and a \n.eld edge from type node NT to type node NT ! , labeled by a .eld of type T ' or set(T ' ) declared in \ntype T . Figure 9 shows the type dependence graph for Figure 2. For the rest of this section, we .x the \nset of variables and .elds, and de.ne the set of well-formed path expressions, WFPath. De.nition 3.3 \n(Well-formed Path Expressions). De.ne the type\u00adnode pair of a path expression element as follows: TNPair(x)= \n(NT ,NT ) for a variable x of type T ; TNPair(f)=(NT ,NT ! ) for a .eld f of type T ' or set(T ' ) declared \nin a type T ; and .- . - TNPair( f )=(NT ! ,NT ) for a reversed .eld expression f , if TNPair(f )=(NT \n,NT ! ). Let p be a path expression e1.e2. . . . .ek and let the correspond\u00ading sequence of type-node \npairs be (N1,N 1' ),..., (Nk,N k' ). We say that p is well-formed if the sequence of type-nodes N1,N \n1' ,...,Nk,N k ' is an undirected path in the type dependence graph. We de.ne the type-node pair of p \nto be TNPair(p)= (N1,N k' ). .--- Example 4. For example, nIter.at.gcont.wl is well-formed, . - whereas \ng.nd and ed.es are not. In the sequel, we consider only well-formed path expressions. We say that a path \nexpression p contains a cycle if the correspond\u00ading path in the type dependence graph contains a cycle. \nA forward path is a (well formed) path expression that contains no reversed .eld sub-expressions. De.nition \n3.4 (Forward Closure). The forward closure of a path expression p, written Forward(p), is the set of \nall path expressions of the form p.p ' where p ' is a forward path not containing program variables (p.p \n' is well-formed) and p ' does not introduce cycles other than ones already contained in p. The forward \nclosure of a type T is the set of all forward paths starting from type T , not containing program variables. \nPath closures of sets of path expressions and types are obtained by taking the union of the closures \nof all set members. Example 5. Forward(Edge)= {ed, src, dst, src.nd, dst.nd} .- .- .- and Forward(an.src.dst)= \n{an.src.dst, an.src.dst.ed}. The forward closure of the types in the type dependence graph represent \ndata access patterns where a sequence of method calls is used to obtain an object of type T from an object \nof a type T ' higher in the hierarchy. For example, in lines 32-33 of Figure 2, a sequence of method \ncalls is used to obtain an edge from the graph and a Weight from an edge. In particular, the forward \nclosure gives us the paths needed to express Inv3 and Inv4. However, these paths ignore the effect of \nmethods, which create more intricate paths, such as the ones needed for Inv1 and Inv2. Those are discovered \nby summarizing method speci.cations, as explained next. Figure 9. Type dependence graph for Figure 2. \n 3.4.1 Discovering Paths in Method Footprints We now explain how to .nd variable-to-lock paths, which \nrepre\u00adsent possible paths between objects referenced by the method pa\u00adrameters (and returned value) and \nobjects accessed by the @locks speci.cation, after the @op speci.cation executes . To .nd these paths, \nwe construct a footprint graph for each method. Intuitively, this graph represents the set of objects \naccessed by the method, sometimes referred to as the footprint of the method. The idea of footprint analysis \nwas de.ned by Calcagno et al. [3] to infer method preconditions and postconditions. We put this idea \nto use for a different purpose. We create a footprint graph by the following steps: Handling statements \nWe interpret the statements in @op in the or\u00adder they appear. For each statement, we create a graph repre\u00adsenting \nevery path expression on the right-hand side of an as\u00adsignment. This is done by creating a new node for \neach position in the expression, connecting them by the respective .elds, and labeling nodes by the variables \nalong the expression. If the left\u00adhand side of the assignment is a pointer or set variable (locks), we \nuse it to label the last node of each path graph. If it is a .eld of the type containing the method, \nwe create a node of that type labeled by this and connect an edge .eld from that node to the last node \nof every path graph created for the right-hand side expression. Creating @locks paths We create path \ngraphs for all path expres\u00adsions in @locks that do not already appear in @op. Merging We merge nodes \nlabeled by a common (pointer or set) variable. Setting locks We label every node matching a path expression \nin @locks by locks. Example 3.5. Figure 10 shows the footprint graph for the getNeighbors method of Graph. \nThe top node represents the outgoing edges of n, the lower node represents the incoming edges  Type \nVariable-to-Lock Paths .- es, es.src, es.dst, ns, ns.src, Graph .- .- .- ns.dst, ns.src.dst, ns.dst.src \n.- .- .- .- Node Var(Node), src, dst, src.dst, dst.src Edge Var(Edge), src, dst .-.. -- Weight ed, ed.src, \ned.dst .- .- cont, cont.src, cont.dst, Set<Node> .- .- cont.src.dst, cont.dst.src GSet gcont GBag<E> \n\u00d8 Table 3. Variable-to-Lock paths for the running example. Var(T) denotes an arbitrary variable to an \nobject of type T. of n. Both are connected to some neighbor of n. The node on the right represents the \nreturned set containing the neighbors of n. We use this graph to obtain paths expressing that getNeighbors \nhas the effect of locking the immediate neighborhood of n. We de.ne the function VarToLock : TName . \n2WFPath associat\u00ading a set of variable-to-lock paths with each program type. We create a set of variable-to-lock \npaths for every type node from all footprint graphs as follows. For each footprint graph, we take all \nthe acyclic non-empty paths from a node labeled by a method parameter (including this and the return \nparameter ret) to any node labeled by locks. We associate these paths with the type node corresponding \nto the type of the parameter. We denote the set of variable-to-lock paths of type T by VarToLock(T ). \nTable 3 shows the variable-to-lock paths that we get for the running example. These paths enable us to \nexpress Inv1 and Inv2. We combine the sets of paths de.ned earlier to obtain the set of abstraction paths: \n AbsPaths = def Forward(t) . Forward(VarToLock(t)) . t.TName Here, expressions of the form Var(T) appearing \nin VarToLock(t) are substituted by the set of paths {x . Var | x is of type T }.  3.5 Putting it All \nTogether Our overall static analysis consists of the following stages: Preprocessing We use a lightweight \npurity analysis [33] to detect objects that do not require concurrency control and .elds that are never \nused inside the parallel loop, e.g., the initSrc and initDst .elds of Weight. The remainder of the analysis \ndoes not consider path expressions in @locks containing unused .elds and sets the opt .ags of read-only \nobjects to NONE. Shape Analysis We execute a forward shape analysis using hier\u00adarchy summarization abstraction \nand TVLA-generated abstract transformers. The .xpoint is a set of bounded shape graphs at every program \nlocation. Finding Redundant Locks We use abstract operations in TVLA to conservatively check whether \nevery shape graph at a program location represents stores that lock all objects de.ned by a @locks speci.cation \nof a method executing at that location. If so, we set the opt argument of that method call to UNDO (if \nit was not already set to NONE). Finding Failsafe Points We perform a backward BFS traversal over the \nCFG (control .ow graph) to .nd earliest program lo\u00adcations where all following method calls are labeled \nby NONE or UNDO (meaning they do not acquire locks). These program locations are the program failsafe \npoints, We set the optimiza- Predicates Meaning Abstraction Predicates {x(v) | x . Var} x references \nv {BackwardReach[x, p](v) | Backward-reachability x . Var,p . AbsPaths} predicates Table 4. Predicates \nfor backward-reachability abstraction. tion argument of all method calls dominated by failsafe points \nto NONE. 3.6 Backward Reachability Abstraction A common abstraction idiom for shape abstraction uses \ncoloring, which records a set of unary (object-)predicates with every object in the store. These predicates \nare used to partition the set of objects into equivalence classes. Examples are Canonical Abstraction \n[32], Boolean heaps [28], Indexed predicate abstraction [20], and gener\u00adalized typestates [21]. These \nabstractions typically employ backward reachability predicates that use paths in the heap to relate objects \nto vari\u00adables. For example, most TVLA-based analyses and analyses using Boolean heaps distinguish between \ndisjoint data structure regions (e.g., list segments and sub-trees) by using transitive reachabil\u00adity \nfrom pointer variables. Indexed predicate abstraction [20] uses predicates that assert that cache clients \nare contained in one of two lists (sharer list and invalidate list). Lam et al. [21] use set containment \npredicates as the generalized typestate of an object. We call such abstractions bottom-up, since they \nrecord proper\u00adties of objects deep in the heap with respect to (shallow) root ob\u00adjects. These abstractions \nachieve high precision as they express ev\u00adery Boolean combination of intersection and union of objects \nsatis\u00adfying the unary predicates. However, the size of an abstracted store can be exponential in the \nnumber of predicates, which might lead to state space explosion in cases where objects satisfy many different \nsubsets of predicates. We de.ne backward reachability abstraction by using the set of abstraction paths \npresented earlier to de.ne backward reachability predicates. For a pointer variable x and an abstraction \npath p, we de.ne a unary predicate expressing the fact that v is a locked object reachable from x by \nthe path p: def ss BackwardReach[x, p](v)= v . [ locks] n [ x.p] . We obtain a backward reachability \nabstraction \u00df[P BRA] from the predicates shown in Table 4. BRA is strictly more precise than HSA. However, \nit can be very expensive the number of abstract nodes in a shape graph obtained by \u00df[P BRA] can be exponential \nin the number of backward-reachability predicates. State space explosion manifests when stores create \noverlaps between different interacting sets (set .elds), which is often the case in our programs. Applying \n\u00df[P BRA] to the store in Figure 6, will con.ate all objects not locked and not referenced by a program \nvariable. Compared to Figure 7, Edge objects 2 and 3, for example, will remain needlessly distinguished. \nSituations such as iterating over the neighbors of a node, exploring multiple neighborhoods simultaneously \nor sharing objects between multiple collections, cause the number of useless distinctions to increase. \n 3.7 Producing Non-Cautiousness Counterexamples When the code of a parallel loop body is not cautious, \nour analysis can sometimes provide a counterexample to demonstrate the viola\u00adtion of the cautious property \nat appropriate program points. To .nd such counterexamples, we assume the small scope conjecture [14], \nwhich says that counterexamples usually manifest in small graphs.  A graph with three nodes and two \nedges is suf.cient to pro\u00advide us with a counterexample for the case of BVK, as shown in Figure 11. The \nregion of the graph where the violation happens is highlighted. This is the smallest counterexample found \nby our anal\u00adysis, taking about 300 seconds to produce.  3.8 Limitations We recognize the following limitations \nof our analysis. Bounded-depth hierarchy. As discussed at the beginning of this section, we assume a \nclass of stores where a .nite-depth hierarchy property exists. This allows us to ensure a bound on the \nnumber of hierarchy summarization paths used to de.ne our abstraction. This precludes us from handling \nbenchmarks where data structures such as lists and trees are explicitly manipulated (and cannot be abstracted \naway by a @rep speci.cation). Generalizing our analysis to handle recursive data structures may be done \nby considering abstraction paths with regular expressions over the pointer .elds of the data structure. \nTemporary violation of invariants. Our abstraction is geared to infer invariants of the form .o.R(o)=. \np(o) where R(o) ex\u00adpresses a heap region (by abstraction paths) and p(o) is a property we wish to summarize \nfor the objects in the region R(o) (the is\u00adlocked property in our analysis). When the property p is temporar\u00adily \nviolated for the objects in R(o) and then restored, our analysis is not able to restore the invariant. \nFor example, assume an invari\u00adant .o.R(o)=. p(o) holds at program point 1. Then a point 2 a single object \nin R(o), referenced by a pointer variable x, is made to have \u00acp(o) and at point 3 it is removed from \nR(o). In order to regain the invariant .o.R(o)=. p(o) at point 3, we may need to re.ne our abstraction \nin order to express an invariant such as .o.(R(o) .\u00acx(o)) =. p(o). 4. Experimental Evaluation The shape \nanalysis described in Section 3 was implemented in TVLA, and used to optimize four benchmarks from the \nLonestar suite [15]. These benchmarks were chosen because they exhibit very diverse behavior. We describe \nthem below. BVK: Boruvka s MST algorithm. This benchmark adds and removes nodes and edges from a graph. \n DMR: Delaunay mesh re.nement. This benchmark uses itera\u00adtive re.nement to produce a quality mesh. In \neach iteration, a neighborhood of a bad triangle, called the cavity of that trian\u00adgle, is removed from \nthe mesh and replaced with new triangles. DMR uses a large number of collections with intricate patterns \nof data sharing, so it is a stress test for the analysis.  SP: Survey propagation, a heuristic SAT solver. \nMost iterations only update node labels, but once in a while, an iteration re\u00admoves a node (corresponding \nto a frozen variable [2]) and its incident edges.  IR Graph Set Field Prog. Optimal Size Calls Calls \nAcc. BVK 340 17/20 4/4 23/23 . DMR 1,168 26/30 30/30 164/164 . SP 925 32/34 16/16 123/123 . PFP 479 6/8 \n3/3 28/28 . Table 5. Program characteristics and static analysis results. x/y measures Optimized/Total. \nAnalysis Total SGs Avg. # Abs. Nodes Avg. # SGs CFG Location Time (sec) BVK HSA 13,594 9 6.25 6 BRA 412,862 \n15 250 3,406 DMR HSA 35,763 13 6.46 16 BRA 1,043,116 20 268 14,909 SP HSA 25,421 13 6.26 12 BRA 394,765 \n21 158 12,446 PFP HSA 17,692 10 6.96 7 BRA 71,800 17 45 972 Table 6. HSA, BRA performance statistics.(SG: \nShape Graph) PFP: Pre.ow-push max.ow algorithm [5]. This algorithm only updates labels of nodes and edges, \nand does not modify the graph structure. 4.1 Static Analysis Evaluation Table 5 reports the results \nof static analysis of our benchmarks. We measure the size of benchmarks by the number of intermedi\u00adate \nlanguage (Jimple) instructions in the client program, excluding the code implementing the data structures \naccompanied by a spec\u00adi.cation. Columns 3 to 5 show the number of static optimization opportunities that \nour analysis enables. Galois protects application\u00adspeci.c objects (e.g., the cavity in DMR) using a variant \nof object\u00adbased STM, which can also bene.t from our optimizations. Col\u00adumn 5 refers to those objects. \nIn all cases, our analysis was precise enough to identify the maximum number of sites that were eligible \nfor optimization, and it discovered the minimal set of latest failsafe points. The optimal result that \nwe compare against was determined manually. Since our analysis is sound, we need to consider only the \nrelatively few calls where the analysis does not suggest con.ict detection or rollback logging optimizations. \n 4.1.1 Comparing Analyses: HSA vs. BRA In Table 6, we compare our analysis using hierarchy summarization \nabstraction (HSA), with an analysis using backward reachability abstraction (BRA). The .rst column reports \nthe total number of shape graphs (SG) explored by the analysis, which is a measure for the amount of \nwork performed. We also report the average size of a shape graph, the average number of SG s per CFG \nlocation (our analysis uses roughly 1.43 CFG locations for a Jimple instruction) at the .xed point, and \nthe running time of the analysis. As expected, HSA generates a constant number of SG s at each program \nlocation, whereas in BRA the number of SG s increases as the benchmarks become more complex (from 45 \nSG s for PFP to 268 for DMR). The bene.ts of HSA are more striking as the complexity of the benchmark \nincreases. For PFP, BRA generates roughly 6 times more SG s than HSA, per CFG location. For DMR, in which \nthe number of collections increases, BRA produces 41 times more structures. Additionally, we observe \nthat in BRA we have more re.ned and, consequently, larger SG s. For all bench\u00admarks the average SG size \nin BRA is roughly 1.6 times larger than in HSA. These facts lead to a signi.cant state space explosion, \nwhich translates to increased work performed by BRA (for DMR we see a 29-fold increase in the number \nof generated SG s), and to increased running times. Thus, HSA is as precise as BRA but more ef.cient. \n  4.2 Experimental Evaluation of Optimizations This section provides detailed performance results for \neach bench\u00admark. To evaluate the performance gains obtained by different lev\u00adels of sophistication of \nthe analysis, we considered the following variants for each benchmark. O1: Baseline version: accesses \nwithin parallel loops to all ob\u00adjects are protected.  O2: Iteration-private objects are not protected. \n O3: O2+ dominated shared objects are not protected.  O4: O3+ duplicate lock acquisitions and unnecessary \nundo op\u00aderations are eliminated.  Even in the baseline version, we do not protect object accesses made \noutside of parallel loops since the analysis required to enable this is trivial. At level O2, iteration-private \nobjects are identi.ed and accesses to them are not protected; this optimization by itself can be accomplished \nby a combination of .ow-insensitive points-to and escape analysis. Optimization levels O3 and O4 target \nshared data; for these levels, a shape analysis similar to ours is necessary. We performed our experiments \nusing the Galois runtime system and a Sun Fire X2270 Nehalem server running Ubuntu Linux version 8.04. \nThe system contains two quad-core 2.93 GHz Intel Xeon processors, which share 24 GB of main memory. We \nused the Sun HotSpot 64-bit server JVM, version 1.6.0. Each variant was executed nine times in the same \ninstance of the JVM. We drop the .rst two iterations to account for the overheads of JIT compilation, \nand report results for the run with the median running time. Because of the don t-care non-determinism \nof unordered-set it\u00aderators, different executions of the same benchmark/input combi\u00adnation may perform \ndifferent numbers of iterations. Since our opti\u00admizations focus on reducing the overhead of each iteration \nand not on controlling the total number of iterations, we focus on a perfor\u00admance metric called throughput, \nwhich is the number of committed iterations per millisecond. For completeness, we also present other \nmeasurements such as the total running time, the number of com\u00admitted iterations, the abort ratio, etc. \nTable 7 shows detailed results for all benchmarks. 4.2.1 Boruvka s Algorithm We do not provide results \nfor level O2, since the number of iteration private objects is insigni.cant. The number of committed \niterations is exactly the same across all thread counts (this is a natural prop\u00aderty of the algorithm \nsince each committed iteration adds one edge to the MST). The analysis is successful in reducing the \nnumber of locks per iteration, and it correctly infers that the operator imple\u00admentation is cautious. \nThe Boruvka algorithm takes roughly 141 seconds to run if we use 1 thread and optimization level O1, \nand 75 seconds if we use 8 threads and optimization level O4. At optimization level O4, no undo s are \nlogged and the number of acquired locks in each iteration is substantially reduced. However, overall \nspeedup is limited by the high abort ratio (for example, for 8 threads, the abort ratio is between 68% \nand 75% for all levels of optimization). The abort ratio decreases as the optimization level increases \nbecause if the time to execute an iteration is reduced, the iteration holds its locks for a smaller amount \nof time, reducing the likelihood of con.icts. This high abort ratio is intrinsic to the algorithm. The \nMST is built bottom-up, so towards the end of the execution, only the top few levels of the tree remain \nto be built and there is not much parallel work. A Non-Cautious Boruvka Implementation. As we discussed \nin Section 2, removing the call to getNeighbors at L3 results in non\u00adcautious iterations. Our analysis \nsuccessfully infers that the failsafe point along this program path moves from L3 to L5. The only difference \nin the inferred method .ags is in L4, where the call to removeEdge requires the UNDO .ag instead of NONE. \nThis example shows the utility of our analysis for optimizing programs in which the operator implementation \nis not cautious. 4.2.2 Delaunay Mesh Re.nement The number of committed iterations for this application \nis fairly stable across thread counts and optimization levels. Lock acquisi\u00adtions drop dramatically in \ngoing from O2 to O3. The analysis de\u00adduces correctly that the operator implementation is cautious, which \nis why the number of undo s per iteration drops to zero at optimiza\u00adtion level O4 (the number of undo \ns per iteration is stable in going from O2 to O3 because the re-triangulated cavity is constructed in \nprivate storage and then stored into the shared graph). The abort ratio is very small even for 8 threads. \nThe reductions in the average number of acquired locks and logged undo s per iteration are re.ected directly \nin the running time. DMR takes 171 sec. to run if we use 1 thread and optimiza\u00adtion level O1, and only \n5 sec. if we use 8 threads and optimization level O4. This is roughly a factor of 34 improvement in the \nrunning time, of which a factor of roughly 8 comes from optimizations and a scaling factor of roughly \n4 comes from increasing the number of threads. Since the number of committed iterations is fairly stable \nacross all optimization levels and thread counts, the same improve\u00adment factors can also be seen in throughput. \n 4.2.3 Survey Propagation The number of committed iterations is fairly stable for this bench\u00admark. The \nanalysis is successful in reducing the number of locks per iteration. The number of undo s per iteration \nis fairly small even at optimization level O1 because the graph is mutated only when a variable is frozen, \nwhich happens in very few iterations. The anal\u00adysis correctly infers that the operator implementation \nis cautious. The SP algorithm takes roughly 180 seconds to run if we use 1 thread and optimization level \nO1, and 9 seconds if we use 8 threads and optimization level O4. Most of this bene.t comes from the optimizations; \nat optimization level O4, we observe a speedup of roughly 1.6 on 8 threads. We see a 5.5\u00d7 improvement \nin throughput for 8 threads when the optimization level goes from O1 to O4, and by 19% from O3 to O4. \n 4.2.4 Pre.ow-push Maximal Flow A distinctive characteristic of PFP is its schedule sensitivity -be\u00adcause \nof don t-care non-determinism, different schedules can per\u00adform very different amounts of work. This \ncan be seen in the 8\u00adthread numbers: at optimization level O4, the program executes twice as many iterations \non 8 threads as it does on a single thread. The number of undos per iteration is 0 for O3, since the \ngraph struc\u00adture is not mutated by the algorithm. The pre.ow-push algorithm takes roughly 104 seconds \nto run if we use 1 thread and optimization level O1, and the best paral\u00adlel time is 6.6 seconds if we \nuse 4 threads and optimization level O4. This is a 16-fold improvement, of which roughly 6-fold im\u00ad \n Th. BVK DMR SP PFP O1 O3 O4 O1 O2 O3 O4 O1 O2 O3 O4 O1 O2 O3 O4 Lock Acquisitions/Iteration 1 2 4 8 \n885 1,114 1,270 1,512 637 799 896 1,020 64 93 118 153 1,429 1,429 1,429 1,430 1,269 1,269 1,269 1,268 \n97 97 97 97 28 28 28 28 99 99 99 100 98 99 100 100 49 50 50 50 6 6 6 6 109 109 109 110 111 111 111 111 \n44 44 44 45 8 8 8 9 Undos/Iteration 1 2 4 8 40.69 47.42 47.46 46.61 40.77 47.30 47.72 47.25 0.00 0.00 \n0.00 0.00 27.77 27.77 27.77 27.79 8.45 8.45 8.45 8.44 8.45 8.44 8.44 8.45 0.00 0.00 0.00 0.00 4.82 4.85 \n4.86 4.90 4.82 4.85 4.94 4.89 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5.85 5.85 5.85 5.92 2.93 2.93 \n2.94 2.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Committed Iterations (Millions) 1 2 4 8 1.60 1.60 1.60 \n1.60 1.60 1.60 1.60 1.60 1.60 1.60 1.60 1.60 1.62 1.61 1.62 1.61 1.62 1.62 1.62 1.62 1.62 1.62 1.62 1.62 \n1.62 1.62 1.62 1.62 21.61 21.52 21.75 21.85 21.52 22.15 21.68 23.79 21.49 22.25 23.84 24.24 21.71 21.70 \n23.15 25.12 8.62 8.48 9.33 9.80 8.34 8.41 8.52 11.23 8.41 8.39 8.50 13.54 8.41 8.38 9.25 16.39 Abort \nRatio % 1 2 4 8 0.00 42.50 61.52 75.70 0.00 40.28 58.57 71.25 0.00 36.99 55.18 68.57 0.00 0.00 0.03 0.05 \n0.00 0.01 0.02 0.04 0.00 0.00 0.01 0.02 0.00 0.01 0.02 0.03 0.00 0.97 3.51 8.39 0.00 0.95 3.60 8.62 0.00 \n1.04 3.50 11.54 0.00 1.23 3.45 7.19 0.00 0.13 0.61 1.02 0.00 0.14 0.35 3.08 0.00 0.11 0.38 5.04 0.00 \n0.11 0.87 3.83 Running Time (sec) 1 2 4 8 141 155 190 219 107 109 102 98 81 81 77 75 171 93 49 28 132 \n77 39 22 24 14 8 5 22 12 7 5 180 109 65 42 176 110 58 42 24 24 14 10 14 17 11 9 104 75 114 108 94 69 \n103 108 22 13 7.3 7.7 18 10 6.6 9.4 Throughput (Iterations/ms) 1 2 4 8 11.38 10.34 8.41 7.31 15.01 14.71 \n15.62 16.38 19.65 19.72 20.87 21.40 9 17 33 57 12 21 41 73 68 118 212 323 75 131 234 347 120 198 333 \n515 122 201 371 573 893 934 1,729 2,404 1,533 1,274 2,074 2,868 83 113 82 91 89 122 83 104 389 659 1,167 \n1,762 463 798 1,402 1,739 Table 7. Performance metrics. BVK input is a random graph of 800,000 nodes \nand 5-10 neighbors per node. DMR input is a random mesh with 549,998 total triangles, 261,100 bad. SP \ninput is a 3-SAT formula with 1, 000 variables and 4, 200 clauses. PFP input is a random graph of 262, \n144 nodes and capacities in the range [0, 10000]. provement comes from the optimizations, and an improvement \nof roughly 3-fold comes from exploiting parallelism.  4.2.5 Summary of Results Our analysis eliminates \nall costs related to rollback logging for our benchmarks, and reduces the number of lock acquisitions \nby a fac\u00adtor ranging from 10\u00d7 to 50\u00d7, depending on the application and the number of threads. These improvements \ntranslate to a notice\u00adable improvement (ranging from 2\u00d7 up to 12\u00d7) in the running time, which is consistent \nacross different thread counts, and robust against pathologies of speculation (e.g. high abort ratio). \n5. Related Work Prior work on shape analysis has focused mostly on analyzing data structure implementations \nto infer heap structure. In contrast, we use data structure speci.cations to abstract away data structure \nrepresentations, and we focus on unstructured graphs. The Jahob system [19] veri.es that a data structure \nimplemen\u00ad tation meets its speci.cation, and it uses the abstract state to sim\u00adplify the veri.cation \nof data structure clients. Our analysis assumes that a given speci.cation is correct. Checking that the \nimplementa\u00adtion and speci.cation of the method semantics match and that the @locks speci.cation ensures \nthat only commutating methods can execute concurrently is an interesting challenge. Maron et al. [24] \nuse specialized predicates to model sharing patterns between objects stored in data structures, and use \nthis information to statically parallelize benchmarks from the JOlden suite and SPECjvm98 benchmarks. \nOur benchmarks operate on unstructured graphs and are not amenable to static parallelization. We exploit \nthe fact that our execution model is speculative to avoid tracking correlations between different data \nstructures, which increases the cost of the analysis considerably. In the current Galois system, the \noptimizations described here are performed manually [26]. Our shape analysis automates these optimizations, \nreducing the burden on the programmer and ensur\u00ading correctness of optimized code. Failsafe points extend \nthe no\u00adtion of cautious operators. Our running example shows that non\u00adcautious code too can be optimized \nby turning con.ict detection and rollback logging off for a subset of the calls, obtaining perfor\u00admance \nimprovement similar to the cautious version. Additionally, in [26] the system optimizes locking only \nafter the failsafe point in contrast to our analysis, which optimizes locking regardless of whether an \noperator is cautious. Prabhu et al. [29] use value speculation to probabilistically reduce the critical \npath length in ordered algorithms. Their static analysis focuses mainly on array programs. Value speculation \nis orthogonal to our approach, and the benchmarks discussed in this paper do not bene.t from value speculation. \nFurthermore, our heap abstractions are very different because we need to handle complex ADTs such as \nunstructured graphs. Harris et al. [10], Adl-Tabatabai et al. [1], and Dragojevic et al. [6] use compiler \noptimizations to reduce the overheads of transactional memory. They also handle immutable, and transac\u00adtion \nlocal objects. Additionally, they describe extending traditional compiler optimizations such as common \nsubexpression elimination (CSE) to reduce the overheads of logging. Although CSE helps to reduce repeated \nlogging for a single object, its effectiveness for our benchmarks is limited by the extensive use of \ncollections. Their approaches cannot capture global properties such as failsafe points. Other optimizations \nthey propose are complementary to ours.  McCloskey et al. [25], Hicks et al. [13], and Cherem et al. \n[4] describe analyses that infer locks for atomic sections. These tech\u00adniques are overly conservative \nfor our benchmarks since they would always infer that an iteration might touch the whole graph. Acknowledgments \nWe would like to thank the anonymous referees, Noam Rinetzky, and Josh Berdine for their helpful comments. \nReferences [1] A. Adl-Tabatabai, B. T. Lewis, V. Menon, B. R. Murphy, B. Saha, and T. Shpeisman. Compiler \nand runtime support for ef.cient software transactional memory. In PLDI. ACM, 2006. [2] A. Braunstein, \nM. M` Survey propagation: ezard, and R. Zecchina. An algorithm for satis.ability. Random Structures and \nAlgorithms, 27(2):201 226, 2005. [3] C. Calcagno, D. Distefano, P. W. O Hearn, and H. Yang. Footprint \nanalysis: A shape analysis that discovers preconditions. In SAS, 2007. [4] S. Cherem, T. Chilimbi, and \nS. Gulwani. Inferring locks for atomic sections. In PLDI. ACM, 2008. [5] T. Cormen, C. Leiserson, R. \nRivest, and C. Stein, editors. Introduction to Algorithms. MIT Press, 2001. [6] A. Dragojevic, Y. Ni, \nand A. Adl-Tabatabai. Optimizing transactions for captured memory. In SPAA, 2009. [7] D. Eppstein. Spanning \ntrees and spanners, pages 425 461. Elsevier, 1999. [8] I. Filipovic, P. W. O Hearn, N. Rinetzky, and \nH. Yang. Abstraction for concurrent objects. In ESOP, 2009. [9] T. Harris and K. Fraser. Language support \nfor lightweight transactions. In OOPSLA 03, 2003. [10] T. L. Harris, M. Plesko, A. Shinnar, and D. Tarditi. \nOptimizing memory transactions. In PLDI. ACM, 2006. [11] M. Herlihy and E. Koskinen. Transactional boosting: \na methodology for highly-concurrent transactional objects. In PPOPP. ACM, 2008. [12] M. Herlihy and J. \nEliot B. Moss. Transactional memory: architectural support for lock-free data structures. In ISCA, 1993. \n[13] M. Hicks, J. S. Foster, and P. Pratikakis. Lock inference for atomic sections. In TRANSACT, June \n2006. [14] D. Jackson. Software Abstractions: Logic, Language, and Analysis. The MIT Press, 2006. [15] \nM. Kulkarni, M. Burtscher, C. Cascaval, and K. Pingali. Lonestar: A suite of parallel irregular programs. \nIn ISPASS, 2009. [16] M. Kulkarni, P. Carribault, K. Pingali, G. Ramanarayanan, B. Walter, K. Bala, and \nL. P. Chew. Scheduling strategies for optimistic parallel execution of irregular programs. In SPAA 08, \n2008. [17] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic \nparallelism requires abstractions. In PLDI. ACM, 2007. [18] V. Kuncak and M. C. Rinard. Decision procedures \nfor set-valued .elds. Electr. Notes Theor. Comput. Sci., 131, 2005. [19] V. Kuncak and M. C. Rinard. \nAn overview of the jahob analysis system: project goals and current status. In IPDPS, 2006. [20] S. K. \nLahiri and R. E. Bryant. Predicate abstraction with indexed predicates. ACM Trans. Comput. Log., 9(1), \n2007. [21] P. Lam, V. Kuncak, and M. C. Rinard. Generalized typestate checking using set interfaces and \npluggable analyses. SIGPLAN Notices, 39(3), 2004. [22] T. Lev-Ami and M. Sagiv. TVLA: A framework for \nimplementing static analyses. In SAS, 2000. [23] R. Manevich, S. Sagiv, G. Ramalingam, and J. Field. \nPartially disjunc\u00adtive heap abstraction. In SAS, 2004. [24] M. Marron, D. Stefanovic, D. Kapur, and M. \nV. Hermenegildo. Identi\u00ad.cation of heap-carried data dependence via explicit store heap mod\u00adels. In LCPC, \npages 94 108, 2008. [25] B. McCloskey, F. Zhou, D. Gay, and E. Brewer. Autolocker: synchro\u00adnization inference \nfor atomic sections. In POPL. ACM, 2006. [26] M. M\u00b4endez-Lojo, D. Nguyen, D. Prountzos, X. Sui, M. A. \nHassaan, M. Kulkarni, M. Burtscher, and K. Pingali. Structure-driven optimiza\u00adtions for amorphous data-parallel \nprograms. In PPOPP. ACM, 2010. [27] K. Pingali, M. Kulkarni, D. Nguyen, M. Burtscher, M. Mendez-Lojo, \nD. Prountzos, X. Sui, and Z. Zhong. Amorphous data-parallelism in irregular algorithms. regular tech \nreport TR-09-05, The University of Texas at Austin, 2009. [28] A. Podelski and T. Wies. Boolean heaps. \nIn SAS, 2005. [29] P. Prabhu, G. Ramalingam, and K. Vaswani. Safe programmable speculative parallelism. \nIn PLDI, 2010. [30] D. Prountzos, R. Manevich, K. Pingali, and K. S. McKinley. A shape analysis for optimizing \nparallel graph programs. Technical Report TR-10-27, UT Austin, http://userweb.cs.utexas.edu/users/ dprountz/UTCS-TR-10-27.pdf, \nJul 2010. [31] L. Rauchwerger and D. A. Padua. The LRPD test: Speculative run\u00adtime parallelization of \nloops with privatization and reduction paral\u00adlelization. IEEE Trans. Parallel Distrib. Syst., 10(2):160 \n180, 1999. [32] M. Sagiv, T. Reps, and R. Wilhelm. Parametric shape analysis via 3-valued logic. ACM \nTrans. Program. Lang. Syst., 24(3), 2002. [33] A. Salcianu and M. C. Rinard. Purity and side effect analysis \nfor java programs. In VMCAI, 2005. [34] K. Zee, V. Kuncak, and M. Rinard. Full functional veri.cation \nof linked data structures. In PPOPP. ACM, 2008.    \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Computations on unstructured graphs are challenging to parallelize because dependences in the underlying algorithms are usually complex functions of runtime data values, thwarting static parallelization. One promising general-purpose parallelization strategy for these algorithms is optimistic parallelization.</p> <p>This paper identifies the optimization of optimistically parallelized graph programs as a new application area, and develops the first shape analysis for addressing this problem. Our shape analysis identifies <i>failsafe</i> points in the program after which the execution is guaranteed not to abort and backup copies of modified data are not needed; additionally, the analysis can be used to eliminate redundant conflict checking. It uses two key ideas: a novel <i>top-down</i> heap abstraction that controls state space explosion, and a strategy for predicate discovery that exploits common patterns of data structure usage.</p> <p>We implemented the shape analysis in TVLA, and used it to optimize benchmarks from the Lonestar suite. The optimized programs were executed on the Galois system. The analysis was successful in eliminating all costs related to rollback logging for our benchmarks. Additionally, it reduced the number of lock acquisitions by a factor ranging from 10x to 50x, depending on the application and the number of threads. These optimizations were effective in reducing the running times of the benchmarks by factors of 2x to 12x.</p>", "authors": [{"name": "Dimitrios Prountzos", "author_profile_id": "81388601660", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2509589", "email_address": "dprountz@cs.utexas.edu", "orcid_id": ""}, {"name": "Roman Manevich", "author_profile_id": "81100232411", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2509590", "email_address": "roman@ices.utexas.edu", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2509591", "email_address": "pingali@cs.utexas.edu", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2509592", "email_address": "mckinley@cs.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926405", "year": "2011", "article_id": "1926405", "conference": "POPL", "title": "A shape analysis for optimizing parallel graph programs", "url": "http://dl.acm.org/citation.cfm?id=1926405"}