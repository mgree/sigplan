{"article_publication_date": "01-26-2011", "fulltext": "\n Space Overhead Bounds for Dynamic Memory * Management with Partial Compaction Anna Bendersky Computer \nScience Department Technion Haifa 32000 Israel annaben@cs.technion.ac.il Abstract Dynamic memory allocation \nis ubiquitous in today s runtime envi\u00adronments. Allocation and de-allocation of objects during program \nexecution may cause fragmentation and foil the program s ability to allocate objects. Robson has shown \nthat a worst case scenario can create a space overhead within a factor of logn of the space that is actually \nrequired by the program, where n is the size of the largest possible object. Compaction can eliminate \nfragmentation, but is too costly to be run frequently. Many runtime systems employ partial compaction, \nin which only a small fraction of the allocated objects are moved. Partial compaction reduces some of \nthe existing frag\u00admentation at an acceptable cost. In this paper we study the effec\u00adtiveness of partial \ncompaction and provide the .rst rigorous lower and upper bounds on its effectiveness in reducing fragmentation \nat a low cost. Categories and Subject Descriptors D.1.5 [Object-oriented Pro\u00adgramming]: Memory Management; \nD.3.3 [Language Constructs and Features]: Dynamic storage management; D.3.4 [Proces\u00adsors]: Memory management \n(garbage collection); D.4.2 [Storage Management]: Garbage Collection General Terms Languages, Performance, \nAlgorithms, Theory. Keywords Runtime systems, Memory management, Storage al\u00adlocation, Dynamic storage \nallocation, Compaction, Partial com\u00adpaction. 1. Introduction The study of the theoretical foundations \nof memory management has not been very extensive. In particular, not much is known about the theoretical \npotential and limitations of memory management functionalities. Previous work that we are aware of includes \na study of fragmentation [18], a study of cache-conscious memory placement [13], and a study of the space \nlimitations of conservative garbage collection and lazy reference counting [5, 6]. In this work * Supported \nby THE ISRAEL SCIENCE FOUNDATION grant No. 283/10. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Erez Petrank \nComputer Science Department Technion Haifa 32000 Israel erez@cs.technion.ac.il we attempt to extend \nthese foundations and study the potential and limitations of partial compaction. Modern software employs \ndynamic memory allocation to sup\u00adport its memory needs. Allocation and de-allocation of memory create \nfragmentation: holes between the allocated objects in the memory may be too small to further satisfy \nfuture allocation. Frag\u00admentation creates a space overhead, since the memory consumed may become larger \nthan the memory required to satisfy the alloca\u00adtion requests when no fragmentation exists. Robson [17, \n18] studied the amount of space overhead that may be caused by fragmentation when no objects are moved. \nHe showed that in the worst case, fragmentation causes quite a large space overhead. In particular, he \npresented a program (or an allocation and de-allocation sequence) that never keeps more than M words \nallocated simultaneously, but any allocator that attempts to satisfy this sequence would require a space \nof (almost) 12 M logn words. The parameter n stands for the largest possible allocated size in the system. \nRobson also provided a simple allocation strategy that can handle any allocation sequence in (approximately) \nM logn words. Frequent compaction eliminates fragmentation completely. If one compacts the heap after \neach de-allocation, then no fragmenta\u00adtion appears at all and M words of space always suf.ce. However, \nfull compaction is costly since a substantial fraction of the objects may be moved and all references \nneed to be updated [1, 11, 12]. Therefore, modern systems tend to either use compaction infre\u00adquently, \nor employ partial compaction, moving some objects in an attempt to reduce fragmentation and keep its \ncost acceptable [2, 3, 8, 9, 14, 15]. Of course, the larger the space that is being com\u00adpacted, the less \nfragmented the heap becomes; on the other hand, the overhead that is posed on the executing program increases. \nThe question that arises is what is the trade-off between the amount of space the can be moved and the \nspace overhead that may occur in the heap? In this work we provide the .rst lower and upper bounds for \nsuch a scenario. Since the amount of moved space makes a difference, we need to bound the amount of relocated \nspace. We choose to study a scenario in which a predetermined percentage of all allocated space can be \nmoved. One could generalize the question to allow a quota of B(S) movement for an arbitrary function \nB of the allocated space S. Other budgeting decisions are also possible; for example, one could limit \nthe relocation according to the amount of live space, or the deleted space. All these make sense as well, \nbut we felt that budgeting by a fraction of the allocated space is interesting, as the amount of allocation \ntypically represents allocation time in the memory management literature, most notably for generational \ngarbage collection. c Copyright &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00  In this work \nwe present general results that bound the heap size required for allocation in the presence of partial \ncompaction. Let the compaction budget at any point in the execution be 1/c of the space allocated so \nfar by the program. To provide an upper bound, we present an allocation and compaction strategy. We show \nthat for programs that never keep more than M words allocated simultaneously, it suf.ces to use a heap \nwhose size is the minimum between M \u00b7 (c + 1) and M \u00b7 logn, where n is the largest possible object that \ncan be allocated. When the compaction budget is high, i.e., c is small, then the heap size can be signi.cantly \nsmaller than the heap size obtained when no compaction is used. The above is formally asserted in Theorem \n1 in Section 3. To show a lower bound on the space overhead, we present a program that incurs a large \nspace overhead for any allocator (whose compaction budget is limited by 1 of the allocated objects). \nThis c program never keeps more than M words allocated simultaneously, and it makes any allocator use \na heap whose size is at least the minimum of 1 \u00b7c and 1 \u00b7 loglogc+n 1 words. When the compaction 10 M \n10 M budget is large (c is small), the minimum is obtained with 1 c. 10 M \u00b7 When the compaction budget \nis tighter, the heap size is at least 1 log n Thus, these asymptotic bounds show that partial 10 M \u00b7 \nlogc+1. compaction can reduce the heap size, but only to a limited extent, which depends on the compaction \nquota. This result is stated as Theorem 2 in Section 4. The above results hold for any possible allocator. \nWe continued this investigation with a study of a speci.c widely used memory allocator: the segregated \nfree list allocator (used for example in [2, 7, 10]). Fixing a speci.c allocator allows a more accurate \nanalysis and yields better bounds. We .rst examine this allocator when no compaction is allowed and improve \nRobson s bounds. It turns out that the bounds depend on the number of free-lists used by the allocation \nscheme. One extreme case is that the allocator maintains a free-list for any possible object size between \n1 and n. In this case v the required space becomes (almost) 45 Mn words. This is much higher than the \n12 M logn words presented in Robson s papers. The other extreme case is that a small number of free lists \nis allowed, speci.cally, one for each power of 2. In this case, we show a lower bound of M logn words, \nwhich is two times stronger than the lower bound for the general allocator. In practice, the number of \nfree lists kept is somewhere in between, and our analysis can be applied to any speci.c choice of sizes \nto yield the corresponding lower bound. These results are formalized in Theorems 3 and 4 in Section 5.2. \nFinally, we examine the effects of adding partial compaction to the segregated free list allocator. We \nshow that a heap size of M \u00b7 c +k \u00b7n suf.ces in this case for any program, where k is the number of different \nfree lists. So when the compaction budget is large, i.e., c is small, partial compaction helps reducing \nfragmentation. See Theorem 5 in Section 5.3. For the lower bound, the number of free lists employed is \nimportant. We .rst show that when using free lists for any object size, the space required is at least \nthe minimum v between 61 Mn and 1 \u00b7 M \u00b7 c. So the true overhead is large if 12 we don t allow a substantial \ncompaction budget. If we only keep free lists for objects sizes that are powers of 2, then the heap size \nrequired is at least the minimum between 1 \u00b7 M \u00b7 logn and 18 \u00b7 M \u00b7 c. 4 See Theorems 6 and 7 in Section \n5.3. This work initiates a study of the cost and effectiveness of par\u00adtial compaction. New techniques \nare developed and novel bounds are shown. The results are asymptotic. They hold for any possible object \nsize limit n, any possible live space bound M, and any pos\u00adsible compaction budget ratio c. However, \nalthough they represent the tightest known bounds, their applicability for speci.c realistic parameters \nof modern systems is limited. We hope that these tech\u00adniques can be extended in future work to provide \ntighter bounds and help us better understand the behavior of memory managers in practice. Organization. \nIn Section 2 we survey previous results on frag\u00admentation when no compaction is allowed, and formally \nde.ne the compaction budget model used in this paper. In Section 3 we present and prove a simple upper \nbound on the effectiveness of par\u00adtial compaction. In Section 4 we state and prove the lower bound on \nthe effectiveness of partial compaction. We divide the proof into two parts. First, we prove the lower \nbound for the restricted case of aligned objects. This proof contains most of the ideas and is eas\u00adier \nto follow. The full proof of the lower bound (with all the hairy details) follows. In Section 5 we study \nthe speci.c segregated free list allocator and prove all the results stated above. We conclude in Section \n6. 2. Problem Description 2.1 Framework and previous work Dynamic storage allocation that does not move \nobjects can suffer from fragmentation. J. M. Robson [17, 18] provided lower and upper bounds on the space \noverhead for a memory management system that allocates and deallocates, but cannot move objects. The \nlower bound demonstrates the existence of a bad program that makes any allocator use a lot of space overhead. \nThe upper bound provides an allocator, whose space overhead is limited, for any program (including the \nbad program that demonstrates the lower bound). Clearly, if the program allocates a lot and does not \nde-allocate space, then a lot of space is required to satisfy the request. It is thus interesting to \nask how much space is required to satisfy the allocation requests of a program that never keeps more \nthan M words alive simultaneously. If compaction is frequently used, a space of M words suf.ces. The \nmemory manager can simply compact the heap after each de-allocation. However, when no compaction is allowed, \nsome allocation sequences demand a large space overhead in order to satisfy the allocation requests. \nAn allocator should be able to handle any allocation (and de\u00adallocation) sequence, and for each allocator, \nwe can ask what is the smallest heap that can handle all allocation sequences that never allow more than \nM words of memory simultaneously allocated. The bounds that are going to be derived depend on the size \nof the largest possible object in the system. We denote by n the size of such an object and assume that \nno allocation in the sequence requests more than n words for the allocated object. Some allocators can \nmove allocated objects in the memory. This is not always the case, as moving objects requires updating \nall pointers referencing them, and some runtime systems cannot always distinguish pointers from integers \nthat hold the same values. Such systems do not move objects whose references cannot be identi.ed. Other \nruntime systems avoid compaction because of its costs. The interaction between the program and the memory \nallocator is divided into stages. Each stage consists of three parts: 1. Deletion: The program removes \nobjects from the heap 2. Compaction: The memory allocator moves objects in the heap. 3. Allocation: \nThe program makes allocation requests and the memory allocator returns the corresponding object addresses \nin the heap.  For the Upper and Lower bounds the program and the memory allocator can be viewed as opponents. \nThe program objective is to make the memory allocator use as much space as possible. The memory allocator \ns objective is to allocate space in a way that will require as little space as possible. The upper and \nlower bounds for the case when is no compaction occurs were studied by Robson [18]. He showed a tight \nconnection between the upper and lower bounds. We de.ne P(M,n) as the set of all programs that never \nkeep more than M words of space allocated simultaneously, and never allocate an object larger than n \nwords. The function HS(A,P) is de.ned as the heap size necessary for allocator A to answer the allocation \nrequests of a program P, so that P . P(M,n). The upper and lower bounds Robson showed demand an overhead \nof \u00d7 21 logn of the actual allocated space M, i.e.:  Upper bound: For all M,n > 0 such that n|M, there \nexists an allocator Ao, such that for all programs P . P(M,n), it holds that: HS(Ao,P) = M \u00b7 logn + M. \nLower bound: For all M,n > 0 such that n|M, there exists a program Po . P(M,n), such that for all allocators \nA, it holds that: 1 HS(A,Po) = M \u00b7 logn + M - n + 1. 2 In this work we investigate the effect of moving \nobjects during the run. We allow the allocator to move a fraction of the objects, i.e. to perform partial \ncompaction. We explore how partial compaction affects these bounds.  2.2 The compaction budget Our objective \nin this work is to explore upper and lower bounds on the space required by allocators that apply partial \ncompaction. We now de.ne partial compaction. If there were no limit on how much space can be moved, the \ntotal heap size required for the allocation would be M, the largest amount of live space throughout the \napplication run. One simple method of compaction that uses only M words could be: allocate sequentially \nusing a bump pointer and compact all of the memory once the M boundary is reached. The problem with this \napproach is that compacting the memory incurs a high computation overhead and is unacceptable in real \nsystems. Practical systems consider trade-offs between the amount of compaction executed and the space \noverhead. Therefore, we introduce a bound on the amount of space that can be moved. This space is denoted \nby B(S), which is the partial compaction function. Speci.cally, after the program allocates a space S, \nthe compactor may move B(S)= 1 \u00b7 S space. c is a constant larger c than 1. The B(S) function is calculated \nincrementally: the quota is added to on every allocation, and reduced with every compaction. For example: \nB(S)= 1 10 S means that after every allocation of S, the 1 compactor can move 10 of the space allocated. \nIn this example, if there were allocations of 200 words (and maybe some deletions) without any compactions, \nspace of size of 1 \u00b7 200 = 20 could be 10 moved. If only 8 words are moved -a quota of 12 words remains. \nWe de.ne an algorithm that works within the B(S) limit as the B(S)-bounded partial compaction. In this \ndocument we always use the compaction threshold function B(S)= 1 \u00b7 S, for some constant c c > 1.  2.3 \nThe setting To present our results we .rst specify what a program is, what an allocator is, in what way \nthey are limited, and how we measure their interaction. We consider a program P that executes a sequence \nof allocations and de-allocations. The program s execution is adaptive in the sense that it may choose \nits upcoming allocation according to the locations in which the allocator chose to place its previous \nallocation. However, to save further quanti.cation over the inputs, we assume that P already has some \n.xed embedded input, so that the allocation sequence depends only on P and the allocation decisions of \nthe allocator (but not on any additional inputs of P). We also consider an allocator A that receives \nthe sequence of allocations and de-allocations one by one (sequentially, or on-line) from the program \nand must satisfy each allocation request as it arrives. Given a speci.c pair of a program P and an allocator \nA, their joint execution is well de.ned and we measure the size of the heap that the allocator A uses \nto satisfy the requests of P. We denote this heap size by HS(A, P). Of course, for the same program P, \nthere could be a non-space-ef.cient allocator A1 that requires a large heap size HS(A1,P) and there could \nbe a better allocator A2 for which HS(A2, P) is much smaller. For a lower bound, we look for a program \nP for which HS(A,P) is high for all allocators. For an upper bound, we look for an allocator A that can \nserve all programs with a low HS(A,P). Finally, we de.ne P(M,n) as the set of all programs that never \nkeep more than M words of space allocated simultaneously, and never allocate an object larger than n \nwords. 3. The Upper Bound We start with the upper bound, which is simpler to show. Formally, we state \nand prove the following theorem. Theorem 1. (Upper bound.) For any real number c = 1, there exists a \nmemory manager Ac that satis.es the compaction bound 1 S, and for all M,n > 0, such that n|M, and all \nprograms P . c P(M, n): HS(Ac,P) = min(M(c + 1),M \u00b7 logn + M). Proof. For any real number c = 1, we present \na speci.c memory manager Ac, that never exceeds the compaction quota 1 \u00b7 S and that c uses a heap size \nof at most M \u00b7 (c + 1) words, i.e., HS(Ac,P) = M \u00b7 (c + 1) for any program P . P(M,n). The memory manager \nAc allocates objects sequentially by bumping a pointer. When the bump pointer reaches the M(c + 1) boundary, \nthe live space is fully compacted to the beginning of the heap. A pseudo-code for the memory allocator \nAc is presented in Algorithm 1. Algorithm 1 Memory Manager Ac Initially: The heap is empty, f ree = 0; \nWhile (TRUE) 1: Receive an allocation request for an object of size e; 2: if f ree + e = M(c + 1) then \n3: Compact all allocated objects to the beginning; 4: f ree . .rst word after the allocated space; 5: \nend if 6: Allocate the object at address f ree; 7: f ree . f ree + e; Clearly, Algorithm 1 never uses \nmore than M \u00b7 (c + 1) words. However, we need to show that there is enough compaction budget to execute \nStep 3, which compacts all live objects to the begin\u00adning of the heap. The required compaction quota \nfor this step is at most M - e. Consider the time interval between any two com\u00adpactions, denoted C1 and \nC2. Since P . P(M,n), there are at most M allocated words at any point in time, and therefore, after \nthe compaction C1 there are at most M words allocated in the begin\u00adning of the heap, and f ree is smaller \nthan M. After executing C1 and before starting to execute C2, the allocator receives requests for allocations \nand deletions, until it fails to allocate e words when the f ree pointer arrives at a point beyond M(c \n+ 1) - e in the heap. The total size of the allocations executed between C1 and C2 must be at least M(c \n+ 1) - M - e, since this is (at least) the number of words that the free pointer has advanced between \nthe two compactions. Therefore, the compaction quota at Step 3 is at least 1 \u00b7 (M(c + 1) - M - e)= M \n- e . According to our as\u00ad  cc sumptions, c = 1 (and e = 0), and therefore, M - e = M - e. Thus, c enough \nbudget is available in the quota so that all of the M - e al\u00adlocated words can be moved to the beginning \nof the heap. This memory manager is effective when c + 1 = logn. Other\u00adwise, c is large, i.e. the compaction \nbudget is small, and then it is possible to obtain a good heap size without moving objects at all. We \ncan use the allocator presented in Robson s paper, not use com\u00adpaction at all, and consume at most M \nlogn + M words for any pos\u00adsible program P . P(M, n). This concludes the proof of Theorem 1. 4. The Lower \nBound In this section we provide a speci.c program that can force any memory allocator to incur a space \noverhead. To simplify the pre\u00adsentation, we start with a program that assumes that the allocators respect \nsome alignment restriction. We later extend this proof to eliminate the alignment restriction. Using \na similar program, we provide a proof for Theorem 2. Due to paper size limitation, the proof details \nappear only in the full version of the paper. 4.1 Bounding the space consumption with aligned objects \nWe start by de.ning what aligned object allocation means. Aligned object allocation places a limitation \non the memory allocator, which is not allowed to allocate objects in an arbitrary location, but only \nin an aligned manner. This constraint simpli.es the proofs, and has fewer corner cases. For the lower \nbound we only consider objects whose size is a power of two (because the program we build only allocates \nsuch sizes). Thus, it is .ne to de.ne alignment only with respect to objects of size 2i for some integer \ni. Without loss of generality, we assume that the smallest object is of size 1. Otherwise, one could \nthink of the object sizes as multiples of the smallest object size. De.nition 4.1. We say that an object \nof size 2i is aligned if it is located at address k \u00b7 2i for some integer k. De.nition 4.2. We say that \na memory allocator uses aligned allo\u00adcation if all the objects it allocates are aligned. We present a \nspeci.c program, called Aligned-Waster and de\u00adnoted PAW , and prove that any memory allocator that satis.es \nthe compaction bound and uses aligned allocation, must incur a space overhead when supporting the allocation \nrequests of PAW . The pro\u00adgram PAW receives the compaction bound c, the largest object size n and the \nbound on the live space M as its input, and for any c,M,n it holds that PAW (c, M,n) . P(M,n). We will \nshow that the program PAW satis.es the following lemma. Lemma 4.3. For all c = 1, and all M = n > 4, \nthere exists a program PAW . P(M, n) such that for all allocators A that use aligned allocation, and \nsatisfy the compaction bound 1 (S), the c following holds: 1 log n - 6n 6 M \u00b7 minc, if c = 4log n logcM \nHS(A,PAW ) =1 \u00b7 logn - n ifc > 4log n 3 M loglogn Note that this lemma is very much like Theorem 2, except \nthat it is stated for aligned allocation and (therefore) the leading constants are better: a 1/6 instead \nof a 1/10. We start with some intuition for constructing PAW and then pro\u00advide its algorithm. PAW works \nin phases. In each phase, it generates a series of allocation requests and lets the allocator allocate \nthe re\u00adquests. Once it sees the locations of the allocated objects, it then decides which objects to \ndelete. The Aligned-Waster Algorithm follows. The idea is that at each phase i, for i = 0,1,2,...,log \nn Aligned-Waster requests allocation of objects of size 2i. It allocates as many of them as possible \nwhile keeping the allocated space below M. It then examines where the allocator places the objects and \ndecides on object deletions. The deletion is intended to prevent space reuse in the next phase, Phase \ni + 1. In the aligned setting, an allocated object of size 2i+1 must be placed on a full aligned interval \nof size 2i+1. Aligned-Waster examines each such interval, and if it is not empty, Aligned-Waster makes \nsure that some object is kept there to avoid re-allocation of a new object on this interval in the next \nphase. However, not leaving an aligned interval of length 2i+1 empty is not enough to prevent its reuse \nin the presence of partial com\u00adpaction. The allocator may choose to move the objects on such an interval \nand clear it for re-allocation. Therefore, Aligned-Waster ap\u00adplies a stricter policy when deciding which \nobjects to delete. In par\u00adticular, it attempts to keep enough allocated space in each interval so that \nit will not be worthwhile for the allocator to waste its com\u00adpaction budget and move all these objects \nout of the interval. Given the compaction bound ratio 1 , the Algorithm PAW sets a density c d 1 > 1 \n, and when deleting objects, PAW attempts to keep a density c of d 1 in each of the aligned 2i+1-sized \ninterval. The algorithm fol\u00adlows. Algorithm 2 Aligned-Waster Program Input: M, n, and c. 1: Compute d \nas a function of c (to be determined). 2: for i = 0 to logn do 3: 4: { Deletion step: }Divide the memory \ninto consecutive areas of size 2i 5: 6: Remove as many objects as possible from each area, subject to \nleaving at least 2i/d space occupied. 7: 8: 9: { Allocation step: }Request allocation of as many objects \nas possible of size 2i (not exceeding the overall allocated size M). 10: end for By de.nition PAW . \nP(M,n) since it never allocates an object larger than n and its overall allocation space never exceeds \nM words. We note that (intuitively) it does not make sense to set d = c. If the allocated space that \nremains in an interval is of size 2i/c (or less), then it makes sense for the allocator to move these \nallocated objects away and make space for reuse. This costs the compaction budget 2i/c, but this budget \nis completely re.lled when the 2i\u00adsized object is later allocated on the cleared interval. So if we want \nto make the allocator pay for a reuse, we must set d < c or in other words, let PAW s deletion step leave \nmore allocated space on each interval. We leave the setting of the density 1/d as a parameter, but one \npossible good setting sets d to be c/2. In this case, the deletion step of PAW leaves enough allocated \nspace on each interval so that not much reuse is possible within the given allocation budget. If reuse \ncannot be applied, then the allocator has to get more fresh space and increase the heap in order to satisfy \nthe allocation requests of PAW . The rest of this section provides a rigorous analysis of PAW showing \nthat it outputs allocation and de-allocation requests that make any allocation strategy incur a space \noverhead as asserted in Lemma 4.3. Section 4.2 extends this algorithm and analysis for the non-aligned \ncase, extending the analysis to obtain the proof of Theorem 2. To bound the space required to satisfy \nthe allocation requests, we bound, on one hand, the amount of space allocated by PAW in the execution, \nand on the other hand, the amount of space that the allocator manages to reuse during the execution. \nLet S(A,PAW ) denote the total space allocated during an execution of the program Figure 1. An example \nof deleting objects in Aligned-Waster, with parameter 1/d = 1/4, and phase i = 3.  PAW with an allocator \nA. Also, let R(A,PAW ) denote the total reused space. A space is considered reused if some object is \nallocated on it, the object is then deleted or relocated by the compaction, and later the same space \nis used for allocation of another object. The same space can be counted as reused more than once if more \nthan two objects are placed on it during the execution. Clearly, during the execution of PAW and A, the \nsize of the heap required to allocate all objects is at least the total size of the allocated objects, \nminus the size of reuse that the allocator manages to make. Formally, we can state this claim as follows. \nClaim 4.4. For any program P and any allocator A, the space HS(A,P) required by A to satisfy the allocation \nrequests of P during their joint execution satis.es: HS(A,P) = S(A,P) - R(A,P). We stress that Claim \n4.4 holds not only for the speci.c program PAW , or in the aligned setting. It holds for any program \nand any allocator. To prove Lemma 4.3 we use Claim 4.4 and show that a lot of space is allocated and \nnot much space is reused. In other words, we bound S(A,PAW ) from below, and R(A, PAW ) from above, for \nany possible allocator A. We start with an upper bound on the space reuse R(A,PAW ). The intuition is \nthat reuse can typically be done only after relocating the objects out of this area. For Aligned-Waster, \nthere are not too many objects that can be moved out, as discussed earlier. Nonetheless, the actual proof \nis more complicated than this simple intuition, because as phases advance, different areas converge into \na single one. An area of Phase i consists of eight areas from Phase i - 3. Some of these eight areas \nmay be empty, while others may be dense, still creating a sparse area of size 2i . Let Q(A,PAW ) denote \nthe total size of compacted space (i.e., relocated objects) throughout the run of the program PAW against \na memory manager A that allocates in an aligned manner and satis.es the compaction threshold 1 S. Let \nd be the deletion threshold of c Aligned-Waster. We show that the reuse R(A,PAW ) is bounded according \nto the following claim. Claim 4.5. For any allocator A that allocates objects in an aligned manner and \nsatis.es the compaction bound 1 S, and a deletion c factor d = 1 employed by PAW , the following bound \nholds. R(A, PAW ) = Q(A, PAW ) \u00b7 d. In the proof of this claim we use several de.nitions of ar\u00adeas, reuses \nand compaction quota that hold in the run of Program Aligned-Waster: De.nition 4.6. If o is an object \nthat is placed on the heap during the run of Program Aligned-Waster, then |o| is its size. ro is the \nspace size reused by the allocation of the object o. ro is de.ned as the total size of objects (and parts \nof objects) that existed in the space that is later occupied by object o. These objects were removed \nor compacted, but there was no object placed on top of them between their removal/compaction and until \nthe allocation of Object o. Namely, the relocation was used for the reuse of o, and not for any previous \nreuse. qo is the total size of objects that were located in the space occupied later by object o. These \nobjects were compacted, but there was no object placed on top of them until the allocation of o. Figure \n2. An example the de.nitions o, ro and qo in the execution of Aligned-Waster, with parameter 1/d = 1/4, \nand phase i = 5. An example of these de.nitions is in Figure 2. In this .gure, a fraction of the heap \nof size 32 words is shown. This fraction of the heap is an empty aligned area, upon which an object o \nis placed in Phase 5 in the run of program Aligned-Waster. The dark gray squares represent words in the \nheap, that the last operation on this word was a compaction of an object. The light gray squares represent \nwords in the heap, that the last operation on this word was a deletion of an object. The white squares \nare words on the heap that were empty since the beginning of the run of the program. In this example, \nqo equals to the total size of the objects that were last compacted -i.e. to the total size of the dark \nwords, 13 words. ro equals to the total size of the deleted + compacted objects in this area, i.e. 27 \nwords. Proof of Claim 4.5. Fix an allocator A and consider a speci.c ex\u00adecution of PAW with A. We break \nthe amount of compaction on Q(A,PAW ) and the amount of reuse R(A, PAW ) into small incre\u00adments. In particular, \nconsider the space reuse in the execution. A reuse occurs when a new object is placed upon an empty aligned \narea that previously contained allocated objects. Consider each such allocation of an object oj on an \naligned area a of size 2i. Just before the placement of oj, we check the size of deletions and com\u00adpactions \nthat occurred in order to make space for the allocation. We show in Claim 4.7 below that roj = qoj \u00b7 \nd for any allocated object oj. Now, summing over all allocations during in the execution of Aligned-Waster, \nwe get: R(A,PAW )= .roj = .qoj \u00b7 d = Q(A,PAW ) \u00b7 d (4.1) jj The second inequality follows from Claim \n4.7. The last inequality follows since any relocation that is later used for space reuse is part of the \nentire set of relocations executed by A during the execution. It remains to show that Claim 4.7 holds. \nClaim 4.7. Consider any phase i during the execution of Aligned-Waster. Observe any object o just before \nit is being allocated in the heap during phase i. It holds: ro = qo \u00b7 d Proof. We look at the area a \nwhere the object o is placed, just before the allocation of o in phase i. The space of a is empty at \nthat time. This space can be sub-divided into aligned sub-areas of size 2k for any ks.t. 0 < k < i, using \nthe Algorithm 3. The result of Algorithm 3 is a list L that contains sub-areas of the area a. The sub-areas \nare distinct, their size is 2k for k . N,0 = k = i. The union of the sub-areas is exactly the area a. \nFor every sub area that is the output of Algorithm 3, one of the following holds: Algorithm 3 Division \nof an area a into sub-areas  Input: An area a of size 2i . 1: List L . a new empty list 2: Stack S . \na new empty stack 3: S.Push(a,i) 4: while S is not empty do 5: (x,k) . S.Pop() 6: if k = 1 then 7: L.Push(x,k) \n8: else 9: Look at the removal step in phase k, during the run of Aligned-Waster. Check if there were \nany removals in area x during that step. 10: if there was at least one removal then 11: L.Push(x,k) 12: \nelse 13: Divide x into its two equal sub areas x1, x2. 14: S.Push(x1, k - 1) 15: S.Push(x2, k - 1) 16: \nend if 17: end if 18: end while 19: return L. 1. The sub-area was either was always empty, or was occupied \nby an object that was compacted away. The sub-area size is 1. 2. The sub-area size is 2ks.t. k = 1, \nand last time objects were deleted from this sub-area happened during the deletion step of phase k in \nAlgorithm Aligned-Waster.  The statement above follows directly from the behavior of Algo\u00adrithm 3. An \narea of size 2k is sub-divided only if it had no deletions in phase k. Therefore, when an area reaches \nsize 1, it never had any deletions, and statement (1.) is true. Since Algorithm 3 divides each aligned \narea of size 2k to its two equal halves, the resulting sub-areas are the exact same areas that Algorithm \nAligned-Waster considered in phase k - 1. The algorithm actually goes back from phase i back to phase \n1 looking for the last deletion for each sub\u00adarea. For all sub-areas sa(k) resulting from Algorithm 3, \nwe exam\u00adine cases (1.) and (2.). In case (1.), it holds that there were no removals at all from the sub-area \nsa(k). The reuse, if larger than 0, occurs since an object was allocated and later compacted (possibly \nmore than once). In this case, the reuse in phase i of the sub-area sa(k) equals to the size of the object \nthat was last compacted away from this sub-area: rsa(k)= qsa(k). Since d = 1, it holds that: rsa(k) = \nqsa(k) \u00b7 d. In case (2.), for each sub-area sa(k), phase k captures the time of the last deletion from \nthe sub area sa(k). If the sub-area sa(k) was not reused between phases k and i (the .rst allocation \non sub-area sa(k) after phase k, occurred in phase i), then we compute the reuse based on the following. \nAccording to the de.nition of the adversarial program Aligned-Waster behavior, for each such area, after \neach removal, at least 1/d of the area remains occupied, we denote these objects by q. These objects \nmust be later compacted away from this sub-area, so that the space could be reused. Therefore, if we \nmultiply the size of this compaction by d, we get the size of the sub-area sa(k). The reuse size in this \nsub-area is smaller or equal to the sub-area size, therefore, in this case rsa(k) = qsa(k) \u00b7 d. Another \noption, is that the area was already reused (fully or par\u00adtially) -an object was already placed on the \nsub-area sa(k), and was later compacted away. In this case, it holds that the size that was compacted \nbut never reused is at least 1/d of the sub-area sa(k) size. This statement is true since after the last \ndeletion, at least 1/d of the area remained occupied. If we notice only these locations in the heap, \nwe can see that objects were compacted, and possibly partially reused by other objects between phases \nk and i, but these reusing objects were later compacted, as the sub-area is eventu\u00adally empty. Therefore, \nin this case also there is always 1/d of the area that was compacted away, and not yet reused. So we \nget that: rsa(k) = qsa(k) \u00b7 d. This statement is true for all sub-areas. The sum of rsa(k), where k denotes \nthe sub-area k of a, holds ra = Skrsa(k), and the sum of qsa(k), where k denotes the sub-area k of a, \nholds qa = Skqsa(k). Since the area a is exactly the location of the newly placed object o, it holds \nthat ro = qo \u00b7 d. This concludes the proof of Claim 4.7 Claim 4.8. Let A be any allocator that satis.es \nthe compaction bound 1 c S, such that c = 1. Let the deletion threshold of PAW be d = 1 and let the total \nspace allocated during the execution of PAW withA be S(A,PAW ). It holds that: d HS(A, PAW ) = S(A, \nPAW ) 1 - c . Proof. By Claim 4.4, it holds that HS(A,PAW ) = S(A,PAW ) - R(A, PAW ). (4.2) By Claim \n4.5, it holds that R(A,PAW ) = Q(A,PAW ) \u00b7 d, (4.3) where Q is the total compacted space. Furthermore, \nby the bound on the partial compaction we know that only 1/c of the allocated space can be compacted, \ni.e., 1 Q(A, PAW ) =\u00b7 S(A,PAW ). (4.4) c Using Equations 4.2, 4.3, and 4.4, we get: dHS(A,PAW ) = S(A,PAW \n) - S(A,PAW ) \u00b7 c as required, and we are done with the proof of Claim 4.8. We now return to Lemma 4.3, \nwhich asserts the lower bound (for the aligned case). We break the argument into its two different cases, \naccording to the compaction bound range. We .rst state the two key claims, then derive the lower bound \nfrom them as a conclusion, and .nally, provide the proofs of these two claims. The .rst claim considers \nthe case of a small compaction allowance. In particular, the compaction ratio c and the density parameter \nd are larger than the logarithm of the largest object size n. Claim 4.9. For all allocators A such that \nA satis.es the compaction bound 1 S, s.t. c > d = 2logn, andM = n > 4, it holds that: c 1 log nd HS(A,PAW \n) = M - 2n1 - . 3 loglognc The second key claim considers the general case in which more compaction can \nbe executed and only requires that c > d > 1. Claim 4.10. For all allocators A such that A satis.es the \ncom\u00adpaction bound 1 S, for all M = n > 2, and for all c > d > 1, it holds c that: 1 1 lognd HS(A,PAW \n) = minMd,M - 2n\u00b71 - . 3 3 log dc  Having stated these two key claims, we now show that they imply Lemma \n4.3, which is the main focus of this section. To this c end we choose the density parameter d for PAW \nto be d = 2 . Claim  4.9 holds for d = 2logn, and therefore, for c = 4logn. In this case we get 1 logn \nHS(A,PAW ) =\u00b7 M \u00b7- n. 3 loglog n For the case that c = 4log n we can use Claim 4.10 and deduce that 1 \nlogn 6n HS(A,PAW ) =\u00b7 M \u00b7 min c, - . 6 logcM These two equations yield the lower bound of Lemma 4.3 exactly, \nas required. To .nish the proof of the lower bound, we need to prove Claims 4.9 and 4.10. We start with \nsome properties of areas as induced by the activity of the program PAW . Claim 4.11. Consider any execution \nof PAW with any allocator A, and any Phase i, 0 = i = log n. Let d be the deletion threshold. Just after \nthe deletion step in Phase i, any non-empty aligned area of size 2i has one of the following two possible \ncon.gurations: 1. The allocated space in the area is smaller than 2i \u00b7 2 (and each d object size is \nsmaller than 2di ).  2. The area contains exactly one object that is larger or equal to  2i d. Proof. \nRecall that the deletion in Phase i attempts to delete as much space as possible, while still leaving \na space of at least 2i words d allocated. Suppose that after the deletion we have an object whose size \nis at least 2di . Then before the deletion this object existed in the area and if the deletion left this \nobject allocated, then it must delete all other objects in the area and this object must be the only \nobject alone in this area, satisfying the second case. Otherwise, after the deletion all objects are \nof size smaller than 2di . Let the smallest of the remaining objects be of size 2j for some j = i. Removing \nthis object was not possible in the deletion step, therefore, the occupied space in this area can be \nat most 2di +2 j -1, which is smaller than 2di \u00b7 2. The above claim shows that small objects must be \nsparsely allocated after a deletion step. In particular, objects smaller than 2i must be allocated on \nan area with density smaller than d 2. We d generalize this sparseness of small objects in the next claim. \nLet i be the phase number, and let k = d be a size threshold, i.e., we consider objects of size 2i as \nsmall objects. Denote by xi(k) the total space k consumed by small objects after Phase i. The following \nclaim asserts that the heap size must be large enough to accommodate these small objects. Claim 4.12. \nConsider an execution of PAW against any allocator A. Let k = d be a size threshold, and xi(k) be the \ntotal space consumed by objects that are smaller than 2i/k after the deletion step in Phase i. After \nthe deletion step of any Phase i in the execution, it holds that: to Claim 4.11, either there is only \none object in the area and its size lies between 2i and 2ki , or the size of all remaining objects in \nd the area is at most 2di \u00b7 2. If k = 21 d, then the size of the area is larger by a factor of at least \nk than the occupied space on this area with objects from xi(k). If 12 d = k = d, then the size of the \narea is larger by a factor of at least 12 k than the occupied space on this area with objects from xi(k). \nThe size of the heap must be at least the sum of all areas containing objects in xi(k). Therefore, we \nget HS(A, PAW ) = xi(k) \u00b7k or HS(A,PAW ) = xi(k) \u00b7 12 k as required. Note that the heap size is monotone, \nbecause the heap size required for Phases 1,...,i,i+1 is at least the space required for Phases 1,...,i. \nTherefore, the obtained bound holds for the heap size of the entire execution. We now prove the key claims. \nIntuitively, it can be argued that either there is a point in the run of the program with many small \nobjects that are so sparsely allocated that the lower bound follows easily, or there is no such point. \nIn the latter case, there must be many memory allocations during the execution of PAW with A, i.e., S(A,PAW \n) is large, and therefore, Claim 4.8 implies the correctness of the lower bound. Proof of Claim 4.9. \nWe need show that if 12 d > log n , then 1 lognd HS(A, PAW ) =\u00b7 M \u00b7- 2n 1 - . 3 loglognc We use Claim \n4.12, and set the size bound k to be log n, which is smaller than 12 d. This means that in Phase i we \nconsider an object 2i to be small if it is smaller than . We divide the analysis into logn two cases. \nIn one case there is a phase in which a lot of small objects are allocated, and in the other case all \nphases do not have a large number of small objects allocated. So let a be some fraction, 0 < a < 1 (to \nbe set later) and consider the case in which the 2i execution has a Phase i for which small objects (smaller \nthan ) logn occupy more than (1 - a)M words. Then by Claim 4.12, the heap size at this point (and on) \nis larger than (1 -a)M \u00b7logn. Otherwise, in each of the Phases i = 0,...,log n, after every allocation \nphase i, the total space occupied in the heap is at least M - 2i. It holds that more than a \u00b7 M - 2i \nof the heap consists of objects that are larger 2i than . The size of the object tells us when it was \nallocated. logn 2iObjects smaller than words were allocated in phases that are at logn least loglogn \nearlier than the current phase, whereas large objects were created in the last loglog n phases. This \nmeans that at least a \u00b7M -2i of the live space must consist of objects that were created in the last \nloglogn phases. An execution consists of logn phases. We divide the execution into discrete sections \nof loglogn phases. In the last phase of each such section, at least a \u00b7 M - 2i of the allocated space \nmust have been allocated during this section. This is true for all sections. Therefore, if we compute \nthe amount of logn space allocated in all sections, we get that that the amount loglog n of space allocated \nin the entire execution, S(A, PAW ), satis.es HS(A,PAW ) = . .. .. logn logn log n 1 S(A, PAW ) = aM \n- . 2i = aM - 2n. xi(k) \u00b7 k ifk = d loglogn loglogn 2 i=loglogn xi(k) \u00b7 21 \u00b7 k if 1 2 d = k = d According \nto the relation between the amount of allocation and the heap size, shown in Claim 4.8, we get that \nProof. Consider the state of the heap after the deletion step in any Phase i. In this phase the areas \nconsidered for allocation and HS(A,PAW ) = S(A,PAW ) \u00b7 1 - d c deletion are of size 2i. All objects \nthat are smaller than 2i belong logn k to xi(k). Consider any area that holds an object in xi(k). According \n= aM \u00b7 loglog n - 2n \u00b7 1 - dc . 1 Setting a =) , and using the fact that d = 2 c , we get that 3(1- \nd c 1 HS(A,PAW ) = M \u00b7 log n 3 in the .rst case and that 1 lognd HS(A,PAW ) = M \u00b7- 2n 1 - 3 loglognc \nin the second case and we are done with the proof of Claim 4.9. Proof of Claim 4.10. We need to show \nthat 1 1 log nd HS(A,PAW ) = min Md, M - 2n \u00b7 1 - . 3 3 log dc Similarly to the proof of Claim 4.9, we \nuse Claim 4.12. This time we set k = d, implying that in Phase i objects are considered small if they \nare smaller than 2di . If there is a Phase i where more than 2 3 M of the live space consists of objects \nsmaller than 2di , then the 11 total heap size is larger than 23 M \u00b7 2 d = 3 Md. Otherwise, in every \nphase i in the execution, it holds that more than 13 M - 2i of the live space consists of objects that \nare larger than 2di . Partitioning the execution into disjoint consecutive sections of log d phases, \nwe get that in each such section, at least 13 M - 2i space was allocated. Therefore, log d 1 logn 1 log \nn S(A,PAW ) = M \u00b7- . 2i = M \u00b7- 2n. 3 logd 3 log d i=loglog d And again, according to Claim 4.8, we get \nthat 1 lognd HS(A,PAW ) = M \u00b7- 2n \u00b7 1 - . 3 logdc One of these cases must hold, and therefore, the heap \nsize satis.es: 1 1 log nd HS(A,PAW ) = min Md, M - 2n \u00b7 1 - , 3 3 log dc as required and we are done \nwith the proof of Claim 4.10. 4.2 Bounding the space consumption when the objects are not necessarily \naligned In the general form of the allocation problem, there are no align\u00adment constraints on the memory \nallocator, and the objects can be allocated anywhere. This requires a modi.cation of the Aligned-Waster \nprogram and an extension of the proofs. The details become more hairy and are omitted due to space limitation. \nThe proofs are available in the full version of this paper [4]. The main problem that comes up in the \nproof is that a new allocation does not neces\u00adsarily cover an aligned area, and so we cannot analyze \nreuse in a clean manner as we did above. The solution is to consider smaller areas in the delete step. \nIf we consider aligned areas whose size is a quarter of the allocated object size, then we can be sure \nthat at least three of them are completely covered by the newly allocated object. The proof details appear \nin the full version of the paper. Here we only state the main theorem. Theorem 2. (Lower bound.) For \nall c = 1, and all M = n > 4, there exists a program PW . P(M,n) such that for all allocators A that \nsatisfy the compaction bound 1 (S) the following holds: c 1 log n 10 M \u00b7 min c, logc+1 - 5Mn ifc = 4logn \nHS(A,PW ) = 1 log n 6 M \u00b7 if c > 4logn loglog n+2 - n 2 5. Segregated Free List Allocator In the previous \nsection we presented a speci.c program that man\u00adages to create a large space overhead for any possible \nmemory al\u00adlocation method. In practice, systems implement speci.c allocators that can be speci.cally \nstudied. It is possible that a program can cause more overhead to a given speci.c allocator. One very \ncom\u00admon memory allocator is the segregated free list allocator, and in particular, the one that is block-oriented \n(e.g., [2, 7, 10]). In this section we study limits of this speci.c allocator and prove tighter lower \nbounds based on its speci.c behavior. We start by de.ning this segregated free list allocation method. \nWe then show bounds on the space requirement when no compaction is allowed, and .\u00adnally, in Section 5.3, \nwe show bounds on the space requirements when partial compaction is used to aid in reducing fragmentation \nfor a segregated free list allocation. 5.1 De.nition of a Segregated Free List Allocator A segregated \nfree list allocator divides the free list into several subsets, according to the size of the free chunks. \nEach subset forms a list of free chunks of the same size (or a small range of sizes) and an array of \npointers is used to index the various free lists. A freed object is placed on the appropriate list according \nto its size. An allocation request is serviced from the appropriate list. Segregated free lists are typically \nimplemented in a block ori\u00adented manner [2, 7, 10]. The heap is partitioned into blocks (typ\u00adically, \nof a page size, i.e., 4KB) and each block may only contain objects of one size. Whenever a full block \nis freed, it is returned to the pool of free blocks. Whenever an allocation is requested for an object \nwhose free list is empty, a free block is pulled from the block pool, it is partitioned into as many \nfree chunks as possible in an aligned manner, and then the new chunks are added to the appropriate free \nlist to allow allocation. The free lists are known as buckets. Each bucket is characterized by the chunk \nsize (or range of chunk sizes) that can be allocated in it. The allocation within each bucket is executed \nin a .rst-.t manner, in the .rst free chunk in the bucket s list that can satisfy the allocation. The \nsimplicity of this method comes with a cost. This method is susceptible to high external fragmentation, \nas shown below. In the following sections we simplify the discussion by assum\u00ading that the maximal object \nsize, n, equals the block size. This is close to what happens in practice, and small adjustments, e.g., \nif the block size is 2n, have small impact on the results. As before, we denote by M the total space \nthat can be allocated simultaneously.  5.2 Memory usage without compaction Let us start with the case \nthat no compaction is used. The general lower bound of Robson [17, 18] holds in this case, implying that \nan overhead factor of at least \u00d7 21 logn can occur in practice for the worst fragmenting program. We \nextend this bound for the speci.c segregated free list allocator. Denote by s1,...,sk the different maximal \nobject sizes inside each bucket, and assume for simplicity that the program only allocates objects whose \nsizes are one of s1,...,sk. (When proving a lower bound, it is enough to show that there exists one bad \nprogram.) A block contains objects of the same size, but this can be any size as long as sk = n. As stated \nearlier, for simplicity we also assume that n = sk. Different segregated free list allocators have different \nsegrega\u00adtion policies, which are determined by the size vector s1,..., sk. We study two rather extreme \ncases by looking at an allocator that keeps a bucket for each possible object size (i.e., a lot of buckets) \nand an allocator that has buckets for exponential object sizes, i.e., sizes that increase by a factor \nof two. Typical implementations use something in between these extreme cases and the tools developed \nhere can be used to explore each speci.c set of sizes.  We .rst look at the allocator SFLall that implements \nthe segre\u00adgated free list method, with a full assortment of bucket sizes: s1 = 1,s2 = 2, s3 = 3,s4 = \n4,...,sn = n. Let the class P(M, [s1,..., sn]) be the class of all programs that always keep the allocated \nspace smaller or equal to M, and allocates objects only of the sizes s1,...,sn. The theorems below show \nthat with no compaction, the space overhead is quite large. It is of the order of the square root of \nn. This is much higher than the logarithmic factor that was shown for the general allocator. Theorem \n3. There exists a program P . P(M, [s1,...,sn]), and the heap size required for allocator SFLall to execute \nthe allocation requests of P satis.es: 4 v HS(SFLall ,P) = Mn - n. 5 The result for the smaller set of \nbuckets is very different. Par\u00adticularly, let SFLlog be an allocator that implements the segregated free \nlist method, with a logarithmic assortment of bucket sizes: s1 = 1,s2 = 2,s3 = 4,s4 = 8,...,slog n+1 \n= n. Let P(M, [s1,..., slogn+1]) be a set of programs that always keep the live space smaller or equal \nto M, and allocates objects only of the sizes s1,...,slogn+1. Theorem 4. There exists a program P . P(M,[s1,...,slog \nn+1]), and the heap size required for allocator SFLlog to execute the allocation requests of P satis.es: \n4 HS(SFLlog,P) = M logn - M - n. 3 Note that Theorems 3 and 4 improve on the general results obtained \nby Robson [17, 18]. Robson could only show an overhead of 12 log n since he needed to work with a general \nallocator, unlike the segregated free list allocator that we assume in this section. The proofs of the \ntheorems are omitted for lack of space and are available in the full version of this paper [4]. Below \nwe only present the bad program that causes large space overheads for any allocator that employs the \nsegregated free list allocation method. Algorithm 4 Program PBW with bucket sizes s1,...,sk. 1: for i \n= 1 to k do do 2: Allocate as many object as possible of size si 3: Deallocate as many objects as possible \nsubject to leaving exactly one object in each allocated block. 4: end for J According to the de.nition \nof Program PBW , after each deallo\u00adcation step, there will be a single object in each allocated block. \nMoreover, this single object will not be deleted until the end of the execution. This property makes \nthe algorithm use a large heap. In the full version of this paper [4] we prove that the heap size in \nphase k, HSk depends on the total space size available for alloca\u00adtion in each of the phases, Mi, and \non the choice for bucket sizes si, in the following way: Mi J. ll . k HSk = . Equation 5.2 represents \nthe connection between object sizes in the different buckets and the memory remaining for allocation \nin every phase of the Program 4 execution. The larger the range of object sizes is, the more memory remains \nfor future phases. Consequently, according to Equation 5.1, the total heap size becomes larger. Simpli.cation \nof the above equations, and assignment of the si parameter yields the proofs of Theorems 3, 4.  5.3 \nSpace overheads with compaction In Section 5.2, we built PBW that wasted many blocks by leaving a single \nobject in each. With no compaction that program created a lot of fragmentation. Modern collectors employ \npartial compaction especially for such scenarios, and they clear sparse blocks by mov\u00ading their elements \nto a different block. The evacuated blocks can then be used for further allocations. In this section \nwe .rst discuss a simple upper bound on the heap usage by looking at a simple com\u00adpaction strategy (which \nis similar to what is used in actual systems in practice). We then provide a program that makes the allocator \nwaste blocks by leaving enough allocated space on them so that the compaction budget will not allow effective \ndefragmentation. 5.3.1 The Upper Bound Our memory manager allocates in a (block-oriented) segregated \nfree list manner as before, but it also has a compacting strategy. We call this memory manager a Compacting \nSegregated Free List allo\u00adcator and denote it by CSFL. We specify the compaction strategy, and later \nshow that using this compaction strategy the following upper bound on the heap size holds. Theorem 5. \nLet c be the compaction threshold, k be the number of buckets, and n be the block size. The maximal heap \nsize required for CSFL to execute any program P that never uses more than M words of allocated space \nsimultaneously and never allocates an object larger than n satis.es: HS(CSFL, P) = M \u00b7 c + k \u00b7 n. In \norder to prove this upper bound, we .rst present the com\u00adpaction strategy of CSFL, show that it is consistent \n(always has enough budget for compaction), and .nally prove the bound in The\u00adorem 5. The de.nition below \nassumes a last block in any bucket. Be\u00adtween all blocks that are allocated for the bucket, the last block \nis the one that was most recently allocated for this bucket. By the be\u00adhavior of the segregated free \nlist allocator, all slots in other blocks were exhausted before the last one was allocated. De.nition \n5.1 (CSFL Compaction Strategy). We denote by b, a block that contains objects of size i. CSFL will compact \nall objects within Block b to other block(s), if Block b holds the following constraints: 1. Block b \nis not the last block for objects of size i, 2. At most 1/c of the space in Block b is allocated, 3. \nThere is enough free space in blocks containing objects of size  i to contain all of the objects that \nare currently placed in Block b. i=1 ... ... si n si has enough compaction budget to execute all required \ncompaction. \u00b7 n. (5.1) Claim 5.2. CSFL with the compaction strategy of De.nition 5.1 Equation 5.1 implies \nthat the more phases there are during the program execution, and the more memory remaining for allocation \nProof. We show that enough space was allocated on this block in every one of them, the larger the heap \nis. The total space size alone to provide the budget for compacting it at the appropriate available for \nallocation in each of the phases, Mi, satis.es: time in the execution. By the allocation strategy of \nthe segregated free list, any block that is not the last block, was .lled with objects Jl .. Mi at some \npoint in the execution. Otherwise, the next block would si n si Mi+1 = Mi - \u00b7 si. (5.2) not have been \nallocated. There were no compactions made on this block since it was taken from the pool, according to \nthe algorithm  (by which every compaction frees an entire block and returns it to the blocks pool). \nSince this block was taken from the blocks pool, the accumulated compaction quota due to allocations \non this block is at least 1/c of the block size. (It can be larger if space on this block was reused.) \nAt the execution point in which compaction is triggered on this block, at most 1/c of the block is allocated, \ntherefore all this space can be moved using only budget in the quota that originated from allocations \non this block. After all of the objects in this block are compacted away, the block is returned to the \nblock pool and can be reused in the future for other bucket sizes. Proof of Theorem 5. According to Claim \n5.2, all blocks (except maybe the last block) are at least 1/c full (otherwise, the block would have \nbeen cleared using compaction). Therefore, the total heap size required for these blocks is at most: \nc \u00b7 M1, where M1 is the total space in all blocks except the last ones in each bucket size. Therefore, \nthe total heap size required is at most c \u00b7 M + k \u00b7 n, where k is the number of different possible sizes, \nand n is the block size.  5.3.2 The Lower Bound We now construct a program that creates a lot of fragmentation \nfor the compacting segregated free list allocator. As before, we look at a CSFL that uses a large number \nof buckets and at a CSFL that uses a small number of buckets. We provide lower bounds for these two extreme \ncases, and the same techniques can be used to work with any speci.c bucket sizes employed in any practical \nsystem. Denote by P(M, [s1,..., sk]) the set of programs that never al\u00adlocate more than M words simultaneously \nand allocate only objects whose size is in the set {s1,...,sn}. The following theorems assert 1 the \ntwo lower bounds. HSk(CSFL,PSW ) = Algorithm 5 is similar to Algorithm 4 presented in Section 5.2 in \nits allocation steps, but differs in its deallocation steps. The difference is that Algorithm 5 keeps \nthe allocated space remaining in each block larger than 1/d of the space allocated within this block. \nLeaving more allocated space in a block makes it dif.cult to compact away all of the objects in it. A \nblock can be used for allocating objects of a different size only when all of the objects in it are moved \naway. Below we investigate the joint run of CSFL and PSW and provide claims that lead to the proof of \nthe Theorems 6 and 7. Note the difference between the execution of CSFL against a program P (with compaction \nenabled), and the execution of SFL against it (with compaction disabled). The difference is the fact \nthat in the compacting scenario space can be reused. After objects on a block are compacted away, this \nsame block can be reused for objects of larger sizes. Therefore, it will be necessary to calculate the \nheap size depending on compaction as well as allocation. The compaction is the reason why in Algorithm \n5, deletions leave 1/d of the space in a block, where d is chosen according to the value of c. This is \ndone in order to make the compaction more costly, but it has another effect: less deletion leaves less \nspace for future allocations. Below, we present some claims that lead to the proof of Theorems 6 and \n7 presented above. Claim 5.3. Consider the execution of a CSFL allocator with the program PSW . For any \nphase i in the execution, let Mi denote the space available for allocation in Phase i, and let si denote \nthe object size allocated in Phase i. Let k denote the number of phases in the execution. The heap size \nrequired for the execution of PSW with CSFL satis.es: kk 1 . . Mi - si. 2 i=1 2 i=1 Theorem 6. Let \nCSFLall be an allocator that implements the segregated free list method and keeps the compaction bound \nof 1 \u00b7S c for c = 4. Let its bucket object sizes be the complete assortment of: s1 = 1,s2 = 2,s3 = 3,s4 \n= 4,...,sn = n for n = 2. Then there exists a program P . P(M,[s1,...,sn]), such that the heap size required \nfor allocator CSFLall to execute the allocations and deallocations of P, HS(CSFLall ,P), satisfy: vv \n1 n + 1 6 \u00b7 M \u00b7 if c = 2 n HS(CSFLall , P) =16 M v 1 M \u00b7 c + 1 if c = 2 n . 12 \u00b7 16 M Sk = Proof. According \nto the de.nition of PSW , the program run consists of phases. In each phase the space available for allocation \nis Mi. The program requests allocations of as many objects as it can of size si. lJ Mi The number of \nallocation requests is, therefore, , and the total si lJ Mi space allocated in Phase i is \u00b7si. The total \nspace allocated from si the beginning of the run until (including) Phase k is k kk Mi . . . Mi - \n(5.3) si = si. \u00b7 Theorem 7. Let CSFLlog be an allocator that implements the i=1sii=1 i=1 segregated \nfree list method and keeps the compaction bound of 1 c \u00b7S. The total quota for compaction is Sk \u00b7 . 1 \nc Let its bucket object sizes be the logarithmic assortment of bucket sizes: s1 = 1,s2 = 2,s3 = 4, s4 \n= 8,...,slog n+1 = n. Then there exists In each phase, the program removes as many objects as it can, \na program P . P(M, [s1,..., slogn+1]), such that the heap size required for the allocator CSFLlog to \nexecute all allocations and subject to leaving at least 1/d of the space already allocated in each block. \nIn order to free a bucket for reuse, all the objects in this block deallocations of P, HS(CSFLlog,P), \nsatisfy: 1 4 \u00b7 M \u00b7 log n - 2M ifc = 2logn HS(CSFLlog,P) = 1 8 \u00b7 M \u00b7 c - 2M ifc = 2logn We now present \nthe program Segregated-Waster, denoted PSW , that creates a large fragmentation for the compacting segregated \nfree list allocator. Algorithm 5 Program Segregated-Waster for the compacting seg\u00adregated free list allocator \n1: Compute d as a function of the input compaction threshold c. 2: for i = 1 to k do do 3: Allocate as \nmany objects as possible of size si 4: In each block, deallocate as many objects as possible subject \nto leaving at least 1/d of the space allocated in this block. 5: end for must be moved to another block. \nTherefore, the total space reuse during the execution is at most d times the space compacted, which 1 \nis at most d \u00b7Sk \u00b7 . Setting d = 2 c in PSW , we get that the space reuse c is at most 12 Sk. The total \nheap size necessary is at least the space allocated, Sk, minus the space reused, which is at most 12 \nSk. Now using the Inequality 5.3 for Sk gets the desired bound. Claim 5.4. Consider the execution of \na CSFL allocator with the Program PSW . Let k be the number of phases in the execution, let Mi be the \ntotal space size available for allocation in Phase i, let si be the size of objects allocated in Phase \ni, let n be the block size, and let d be the density threshold. Then in all phases 1 = i < k it holds \nthat: Mi+1 = Mi 1 - 1 d - si n - si . We omit the proof of this claim due to the constraints of the \npaper length. The proof appears in the full version of the paper [4].  Claim 5.5. Consider the execution \nof a CSFL allocator with the and some algebra we get: program PSW . Let Mi be the size of the space available \nfor alloca\u00ad k(k - 1)tion in Phase i, let si be the objects size in Phase i, let n be the block HSk(CSFLall \n,PSW ) = 12 Mk - M 4d size, and let d be the density threshold. In all phases 2 = i = k it (k - 1)(k \n- 2)(k - 3) k(k + 1) holds that: -M - . F 6(n - k + 1) 4 i-1 v i - 1 sj . The above bound is correct \nfor any phase k,1 = k = . n, but the Mi = M - M \u00b7 + d n - sj quality of the bound is best at where the \ncomputation actually stops. v j=1 We divide into two possible cases. The .rst case is when d = n. v n \nProof. By removing the recursion from the result of Claim 5.4, we In this case, we choose k = 2 and get: \n get: vv i-1 v n(n - 2) si 4 M v (CSFLall ,PSW ) = 1HS v n - M 1 \u00b7 . n 16dv Mi = M 1 - - v (n - 2)(n \n- 4)( v v v . 2 d n - 6) n( n + 2) n - si j=1 -M - . 16 from above by 1. n We use the fact that for \nany positive a1,...,ai it holds that 48(n - 2 + 1) ii vv n Since we assume that d = n, we can bound . \n (1 - aj) = 1 - . aj. (5.4) d Note also that for n = 2 j=1 j=1 vvv (n - 2)(n - 4)(n - 6) v v to simplify \nthe equation above. The result is: = n - 6. F (n - n + 1) i-12 1 si . Mi = M 1 - + . Therefore, dn \n- si v j=1 1 v n - 2 HS v n (CSFLall ,PSW ) = Mn - M Which is what we wanted to prove. 4 16 v 2 1 v n \n+ 2 nM( n - 6) - Claim 5.6. Consider the execution of a CSFL allocator with the - . 48 16 program PSW \n. Let HSk be the total heap size required for the According to our de.nition M = n. Therefore, we can \nreplace the v allocations of CSFL until the end of phase k. The heap size can be bounded by: last n + \n2n with 3M, and get that: 1 v 1 1 k(k - 1) HS v n (CSFLall ,PSW ) = Mn + M. HSk(CSFL,PSW ) = Mk - M \n\u00b7 2 616 24d v k-1 kd (k - i - 1)si 1 1 The other case is when d = n. In this case, we choose k = 2. \n. . M - - si. Therefore: 2 2 n - sk-1 i=1 i=1 HS d (CSFLall ,PSW ) = 2 Proof. According to Claim 5.3, \nthe total heap size until Phase k is: (d - 2)(d - 2)(d - 4)(d - 6) d(d + 2) 1 4 Md - M \u00b7- M - .16 48(n \n- d 2 + 1) 16 kk 1 1 . . HSk(CSFL, PSW ) = Mi - si. v 2 2 n = d, we can replace n with d2. We also \nuse the fact that Since i=1 i=1 for d = 2,Substituting the value of Mi with the result of Claim 5.5, \nwe get: (d - 2)(d - 4)(d - 6)HSk(CSFL,PSW ) == d - 6. FF (d2 - d 2 + 1) ki-1 k 1 i - 1 sj 1 . i 1= . \n . si Therefore, M - M + = - 2 d 2 n - sj j=1 i=11 d - 2 d - 6 d2 + 2d k-1 k HS d (CSFLall , PSW ) \n= Md - M \u00b7 - M - k(k - 1) (k - i - 1)si 1 1 1 . . . 4 16 48 16 Mk - M M 2 = - - si. \u00b7 2 4d 2 2 n - \nsi Since M = n = d2, we can replace the last d2 + 2d with 3M, and i=1 i=1 By substituting -si in the \ndenominator, with the maximal value of si, which is sk-1, we get the claim. We now use the above claims \nto prove Theorems 6 and 7. Proof of Theorem 6. The object sizes in this case are: s1 = 1,s2 = 2,s3 = \n3,s4 = 4,...,sn = n. We set these sizes in the expression of Claim 5.6: 1 k(k - 1) HSk(CSFLall ,PSW ) \n= Mk - M 24d k-1 k 1 (k - i - 1)i 1 . . M i. - - 2 n - k + 1 2 i=1 get that: 11 HS d (CSFLall ,PSW \n) = Md + M 2 616 setting d = c/2 the claim holds. Proof of Theorem 7. The object sizes in this case are: \ns1 = 1,s2 = 2,s3 = 4,s4 = 8,...,sn = 2n. Setting the object sizes in the expres\u00adsion of Claim 5.6 we \nget: k(k - 1)HSk(CSFLlog,PSW ) = 21 Mk - M \u00b7 4d k-1 k (k - i - 1)2i 1 - 12i 2 M . . - . n - 2k-1 2 \n i=1 i=1 i=1 Using the closed sum for squares: Using the following equation: kk-1 k(k + 1)(2k + 1) i2 \n= i2i = 2k(k - 2)+ 2 . . (5.5) (5.6) 6 i=1 i=1  and some algebra, we get: k(k - 1)HSk(CSFLlog, PSW \n) = 21 Mk - M \u00b7 4d 2k-1 - k -M \u00b7- 21 (2k+1 - 1). n - 2k-1 We split the analysis into two possible cases. \nFirst, assume that d = log(n). In this case we look at the phase k = logn. In this case, we get: logn(logn \n- 1) HSlogn(CSFLlog, PSW ) = 21 M logn - M \u00b7 4d n 2 - logn -M \u00b7- 21 (2n - 1) n - n 2 logn(logn - 1) \n= 12 M logn - M \u00b7 4d2logn -M + M \u00b7- n + 12 . n Since d = log n, we can bound log n from above by 1. Also, \nwe use d the fact that M = n = 1 to get: (logn - 1) HSlog n(CSFLlog,PSW ) = 12 M logn - M \u00b7 42logn -M \n+ M \u00b7- n + 12 n = 14 M logn - 2M. The other case is that d = logn. In this case, we choose k = d and \nobtain: d(d - 1)HSd (CSFLlog,PSW ) = 21 Md - M \u00b7 4d 2d-1 - d -M \u00b7- 21 (2d+1 - 1). n - 2d-1 We replace \nn with 2d , and use the fact that M = n = 2d to get: (d - 1) HSd (CSFLlog,PSW ) = 21 Md - M 4 2d-1 - \nd -M \u00b7- 2d + 1 2d - 2d-12 = 14 Md - 2M and again, setting d = c/2, the claim holds. 6. Conclusion In \nthis work we studied the effectiveness of partial compaction for reducing the space overhead of dynamic \nmemory allocation. We developed techniques for showing lower bounds on how much space must be used when \nthe amount of compaction is limited by a given budget. It was shown that partial compaction can reduce \nfragmentation, but up to a limit, determined by the compaction budget. We also studied the effectiveness \nof partial compaction for a speci.c common allocator: the segregated free list allocator. Tighter bounds \nhave been shown based on the speci.c behavior of this allocator. This work extends our understanding \nof the underlying theory behind memory management with compaction. We hope that the techniques developed \nherein can be further extended in future work to achieve even tighter bounds and improve our understanding \nof the effectiveness of partial compaction for modern systems. References [1] Diab Abuaiadh, Yoav Ossia, \nErez Petrank, and Uri Silbershtein. An ef.cient parallel heap compaction algorithm. In Proceedings of \nthe Nineteenth ACM SIGPLAN Conference on Object-Oriented Program\u00adming, Systems, Languages, and Applications, \nACM SIGPLAN No\u00adtices 39(10), pages 224 236, Vancouver, Canada, October 2004. [2] David F. Bacon, Perry \nCheng, and V.T. Rajan. A real-time garbage collector with low overhead and consistent utilization. In \nConference Record of the Thirtieth Annual ACM Symposium on Principles of Programming Languages, ACM SIGPLAN \nNotices 38(1), pages 285 298, New Orleans, LA, USA, January 2003. [3] Ori Ben-Yitzhak, Irit Goft, Elliot \nKolodner, Kean Kuiper, and Vic\u00adtor Leikehman. An algorithm for parallel incremental compaction. In Hans-J. \nBoehm and David Detlefs, editors, Proceedings of the Third International Symposium on Memory Management \n(June, 2002), ACM SIGPLAN Notices 38(2 supplement), pages 100 105, Berlin, Germany, February 2003. [4] \nAnna Bendersky and Erez Petrank. Space overhead bounds for dynamic memory management with partial compaction. \nA full version of this paper is available at http://www.cs.technion.ac.il/ erez/papers.html, 2010. [5] \nHans-Juergen Boehm. Bounding space usage of conservative garbage collectors. In POPL 2002 [16], pages \n93 100. [6] Hans-Juergen Boehm. The space cost of lazy reference counting. In Proceedings of the Thirty-First \nAnnual ACM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices 39(1), pages 210 219, \nVenice, Italy, January 2004. [7] Hans-Juergen Boehm and Mark Weiser. Garbage collection in an uncooperative \nenvironment. Software Practice and Experience, 18(9):807 820, 1988. [8] Cliff Click, Gil Tene, and Michael \nWolf. The Pauseless GC algorithm. In Michael Hind and Jan Vitek, editors, Proceedings of the First ACM \nSIGPLAN/SIGOPS International Conference on Virtual Execution En\u00advironments, pages 46 56, Chicago, IL, \nUSA, June 2005. [9] David Detlefs, Christine Flood, Steven Heller, and Tony Printezis. Garbage-.rst garbage \ncollection. In David F. Bacon and Amer Diwan, editors, Proceedings of the Fourth International Symposium \non Mem\u00adory Management, pages 37 48, Vancouver, Canada, October 2004. ACM Press. [10] Tamar Domani, Elliot \nK. Kolodner, Ethan Lewis, Elliot E. Salant, Katherine Barabash, Itai Lahan, Erez Petrank, Igor Yanover, \nand Yossi Levanoni. Implementing an on-the-.y garbage collector for Java. In Craig Chambers and Antony \nL. Hosking, editors, Proceedings of the Second International Symposium on Memory Management, ACM SIGPLAN \nNotices 36(1), pages 155 166, Minneapolis, MN, October 2000. [11] Richard E. Jones. Garbage Collection: \nAlgorithms for Automatic Dynamic Memory Management. Wiley, Chichester, July 1996. With a chapter on Distributed \nGarbage Collection by R. Lins. [12] Haim Kermany and Erez Petrank. The Compressor: Concurrent, in\u00adcremental \nand parallel compaction. In Michael I. Schwartzbach and Thomas Ball, editors, Proceedings of the ACM \nSIGPLAN Conference on Programming Language Design and Implementation, ACM SIG-PLAN Notices 41(6), pages \n354 363, Ottawa, Canada, June 2006. [13] Erez Petrank and Dror Rawitz. The hardness of cache conscious \ndata placement. In POPL 2002 [16], pages 101 112. [14] Filip Pizlo, Daniel Frampton, Erez Petrank, and \nBjarne Steensgard. STOPLESS: A real-time garbage collector for multiprocessors. In Greg Morrisett and \nMooly Sagiv, editors, Proceedings of the Sixth International Symposium on Memory Management, pages 159 \n172, Montr\u00b4eal, Canada, October 2007. ACM Press. [15] Filip Pizlo, Erez Petrank, and Bjarne Steensgaard. \nA study of con\u00adcurrent real-time garbage collectors. In Rajiv Gupta and Saman P. Amarasinghe, editors, \nProceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, ACM SIG-PLAN \nNotices 43(6), pages 33 44, Tucson, AZ, USA, June 2008. [16] Conference Record of the Twenty-ninth Annual \nACM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices 37(1), Portland, OR, USA, January \n2002. [17] J. M. Robson. An estimate of the store size necessary for dynamic storage allocation. Journal \nof the ACM, 18(3):416 423, July 1971. [18] J. M. Robson. Bounds for some functions concerning dynamic \nstorage allocation. Journal of the ACM, 21(3):419 499, July 1974.    \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Dynamic memory allocation is ubiquitous in today's runtime environments. Allocation and de-allocation of objects during program execution may cause fragmentation and foil the program's ability to allocate objects. Robson has shown that a worst case scenario can create a space overhead within a factor of log(n) of the space that is actually required by the program, where n is the size of the largest possible object. Compaction can eliminate fragmentation, but is too costly to be run frequently. Many runtime systems employ partial compaction, in which only a small fraction of the allocated objects are moved. Partial compaction reduces some of the existing fragmentation at an acceptable cost. In this paper we study the effectiveness of partial compaction and provide the first rigorous lower and upper bounds on its effectiveness in reducing fragmentation at a low cost.</p>", "authors": [{"name": "Anna Bendersky", "author_profile_id": "81479661588", "affiliation": "Technion, Haifa, Israel", "person_id": "P2509662", "email_address": "annaben@cs.technion.ac.il", "orcid_id": ""}, {"name": "Erez Petrank", "author_profile_id": "81100377919", "affiliation": "Technion, Haifa, Israel", "person_id": "P2509663", "email_address": "erez@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926441", "year": "2011", "article_id": "1926441", "conference": "POPL", "title": "Space overhead bounds for dynamic memory management with partial compaction", "url": "http://dl.acm.org/citation.cfm?id=1926441"}