{"article_publication_date": "01-26-2011", "fulltext": "\n A Kripke Logical Relation Between ML and Assembly Chung-Kil Hur * Derek Dreyer Max Planck Institute \nfor Software Systems (MPI-SWS) {gil,dreyer}@mpi-sws.org Abstract There has recently been great progress \nin proving the correctness of compilers for increasingly realistic languages with increasingly realistic \nruntime systems. Most work on this problem has focused on proving the correctness of a particular compiler, \nleaving open the question of how to verify the correctness of assembly code that is hand-optimized or \nlinked together from the output of multiple compilers. This has led Benton and other researchers to propose \nmore abstract, compositional notions of when a low-level program correctly realizes a high-level one. \nHowever, the state of the art in so-called compositional compiler correctness has only consid\u00adered relatively \nsimple high-level and low-level languages. In this paper, we propose a novel, extensional, compiler\u00adindependent \nnotion of equivalence between high-level programs in an expressive, impure ML-like .-calculus and low-level \npro\u00adgrams in an (only slightly) idealized assembly language. We de.ne this equivalence by means of a \nbiorthogonal, step-indexed, Kripke logical relation, which enables us to reason quite .exibly about assembly \ncode that uses local state in a different manner than the high-level code it implements (e.g., self-modifying \ncode). In con\u00adtrast to prior work, we factor our relation in a symmetric, language\u00adgeneric fashion, which \nhelps to simplify and clarify the formal pre\u00adsentation, and we also show how to account for the presence \nof a garbage collector. Our approach relies on recent developments in Kripke logical relations for ML-like \nlanguages, in particular the idea of possible worlds as state transition systems. Categories and Subject \nDescriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; D.3.3 [Programming Languages]: \nLanguage Constructs and Features; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and \nRea\u00adsoning about Programs General Terms Languages, Theory, Veri.cation Keywords Step-indexed Kripke logical \nrelations, biorthogonal\u00adity, compositional compiler correctness, garbage collection, self\u00admodifying code \n* This work was undertaken while the .rst author was at PPS, Univer\u00adsit\u00b4e Paris Diderot, supported by \nDigiteo/Ile-de-France project COLLODI (2009-28HD) and Engineering Research Center of Excellence Program \nof Korea Ministry of Education, Science and Technology (MEST) / National Research Foundation of Korea \n(NRF) Grant R11-2008-007-01002-0. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 1. Introduction While compiler veri.cation is an age-old \nproblem, there has been remarkable progress in the last several years in proving the correct\u00adness of \ncompilers for increasingly realistic languages with increas\u00adingly realistic runtime systems. Of particular \nnote is Leroy s Com\u00adpcert project [18], in which he used the Coq proof assistant to both program and \nverify a multi-pass optimizing compiler from Cmi\u00adnor (a C-like intermediate language) to PowerPC assembly. \nDar\u00adgaye [13] has adapted the Compcert framework to a compiler for a pure mini-ML language, and McCreight \net al. [19] have extended it to support interfacing with a garbage collector. Independently, Chli\u00adpala \n[10, 12] has developed veri.ed compilers for both pure and im\u00adpure functional core languages, the former \ngarbage-collected, with a focus on using custom Coq tactics to provide signi.cant automa\u00adtion of veri.cation. \nThat said, all of the aforementioned work has focused on prov\u00ading the correctness of a particular compiler, \nleaving open the ques\u00adtion of how to verify the correctness of assembly code that is hand\u00adoptimized or \nlinked together from the output of multiple compil\u00aders. The issue is that compiler correctness results \nare typically es\u00adtablished by exhibiting a fairly close simulation relation between source and target \ncode, but code produced by another compiler may obey an entirely different simulation relation with the \nsource program, and hand-optimized code might not closely simulate the source program at all. Thus, existing \ncorrectness proofs depend fundamentally on the closed-world assumption that one has con\u00adtrol over how \nthe whole source program is compiled. In order to lift the closed-world assumption, Benton and Hur [5] \nsuggest that what is needed is a more abstract, extensional notion of what it means for a low-level program \nto correctly implement a high-level one a notion that is not tied to a particular com\u00adpiler and that, \nmoreover, offers as much .exibility in the low-level representation of high-level features as possible. \nWhen reasoning strictly about high-level programs, the canonical extensional notion of when one program \nimplements the same functionality as another is observational (or contextual) equivalence, which says \nthat the two programs exhibit the same (termination) behavior when placed into the context of an arbitrary \nenclosing well-typed high-level pro\u00adgram. However, it is not clear how to de.ne such a contextual no\u00adtion \nof equivalence between high-and low-level programs, because there is no way to run both programs under \nthe same context one would need to quantify over equivalent high-and low-level pro\u00adgram contexts, but \nwhen are two contexts equivalent? We are back to the original question. Benton and Hur s solution is \nto de.ne a logical relation be\u00adtween the high-and low-level languages (actually two relations, one for \neach direction of semantic approximation, employing a de\u00adnotational semantics to represent the high-level \nside). Logical rela\u00adtions are inherently extensional e.g., two functions are logically related iff they \nmap related arguments to related results, regardless of their private implementation details and guarantee \nequivalent termination behavior under arbitrary contexts that are themselves logically related. While \nnot as canonical as contextual equivalence, logical equivalence is nevertheless useful as long as one \ncan estab\u00adlish that the logical relations are suf.ciently populated i.e., that they relate enough programs/contexts \nof interest.  In the traditional setting where one is de.ning equivalence of programs in the same language, \nthis suf.cient population prop\u00aderty is ensured by the fundamental theorem of logical relations, which \nstates that all well-typed programs (and thus all well-typed contexts) are logically self-related. For \nmixed high-low relations, Benton and Hur demonstrate suf.cient population by providing a simple, one-pass \ncompilation translation from their high-to their low-level language and proving that all well-typed high-level \npro\u00adgrams are logically equivalent to their compilations. They also use the logical relations to show \nthe relatedness of some simple hand\u00adoptimized low-level code with corresponding high-level programs. \nBenton and Hur present their work as the .rst step towards compositional compiler correctness . However, \nthe source lan\u00adguage they consider the simply-typed .-calculus with recursion is purely functional, and \nthe target language they consider an SECD machine is relatively high-level. Chlipala [11] has subse\u00adquently \nproposed a more syntactic approach to proving composi\u00adtional compiler correctness, applicable to a richer, \nimpure (albeit untyped) source language, but the target language he considers is also pretty high-level, \nnamely a CPS variant of the source. 1.1 Contributions In this paper, we study compositional equivalence \nof high-and low-level programs in a more realistic setting. Our high-level lan\u00adguage is an expressive \nML-like CBV .-calculus, supporting ab\u00adstract types, general recursive types, and general mutable refer\u00adences. \nOur low-level language is an (only slightly) idealized as\u00adsembly language. Furthermore, our logical relation \nis designed to be sound in the presence of garbage collection, under some fairly abstract assumptions \nabout the behavior of the garbage collector that are satis.ed by both mark-and-sweep and copying collectors. \nFollowing Benton and Hur, we de.ne our equivalence using a biorthogonal, step-indexed logical relation. \nBiorthogonality is useful when reasoning about programs (such as low-level ones) whose behavior is context-sensitive, \nand step-indexing is useful in reasoning about semantically cyclic features like recursive types and \nhigher-order state. We depart from prior work, though, in that our relation is also a Kripke logical \nrelation i.e., it is indexed by possible worlds that specify assumptions about the machine state. Possible \nworlds are useful in enforcing invariants about low-level data structures (e.g., that a heap-allocated \nrepresentation of a closure is immutable). They are also helpful in encoding a variety of runtime system \nin\u00advariants, such as the convention concerning callee-save registers and the notion of data liveness. \nLast but not least, possible worlds enable us to reason quite .exibly about assembly code that uses local \nstate in a different manner than the high-level code it imple\u00adments. An interesting example of this is \nself-modifying assembly code, whose correctness proof involves reasoning about low-level internal state \nchanges speci.cally, changes to the code itself that clearly have no high-level counterpart. This is \nthe essence of what we mean when we say that our relation is extensional. Technically, our approach relies \nclosely on recent develop\u00adments in Kripke logical relations for ML-like languages, in par\u00adticular the \nidea of possible worlds as state transition systems.This idea, which we review in Section 3.3, was proposed \noriginally by Ahmed et al. [2] (in a somewhat different form) as a way to rea\u00adson about representation \nindependence for so-called generative abstract data types, whose private state undergoes a controlled \nseries of state transitions during the execution of the program. Dreyer et al. [14] have subsequently \ngeneralized the idea in or\u00adder to reason about well-bracketed state changes. By basing our high-low logical \nrelations on this most recent work, we are able to cleanly model a variety of state transition systems \nthat arise naturally in low-level code (e.g., in self-modifying code). Lastly, a novel feature of our \nlogical relation is that, while it is de.ned by induction on an ML-like type structure, it is also de\u00ad.ned \nin a language-generic fashion. That is, it may be instantiated to form an equivalence relation between \nany two languages (high\u00ador low-level) that are capable of implementing various relevant lin\u00adguistic constructs \ne.g., function application, plugging a continu\u00adation with a value, etc. Factoring the relation in this \nway helps to simplify and clarify the formal presentation. Moreover, it has the advantage that the relation \nbecomes inherently symmetric and thus easier to use in proving high-low equivalences than Benton and \nHur s asymmetric approximation relations.  2. HIGH and LOW The high-level language HIGH is a System \nF-like polymorphic .\u00adcalculus extended with existential, product and iso-recursive types, as well as \nML-style general references (higher-order state). Fig\u00adure 1 shows the syntax and the typing and evaluation \njudgments. The inference rules for typing and evaluation are standard, so we omit them (see the companion \ntechnical appendix [15] for details). The low-level language LOW is an assembly language ideal\u00adized in \ntwo ways: its word and memory sizes are in.nite and its in\u00adstructions are represented by abstract objects \nrather than by physi\u00adcal words. Its memory consists of four entities: code memory, regis\u00adter .le, stack \nand heap. A code memory is a map from physical ad\u00addresses (represented by natural numbers) to instructions. \nA register .le is a map from registers to words. Words in turn are represented by natural numbers with \nan extra bit indicating whether the word is a pointer to a heap cell or not (useful for garbage collection \npur\u00adposes). There are 12 registers, half of which (sp,sv0,..., sv4)are speci.ed as callee-save registers \nby our calling convention. Both a stack and a heap are random-access memories (i.e., maps from addresses \nto words). The instruction set includes standard instructions (jmp, jnz, jneq, move, plus, minus) supporting \ndifferent addressing modes via lvalues and rvalues. The non-standard operations include halt for normal \ntermination, fail for raising a runtime error, jptr for testing whether a value is a pointer or not, \nsetptr for marking a pointer bit, and isr, isw for inspecting and updating instructions in code memory. \nNote that as instructions are not represented by words, we employ a bijection E and its inverse D (for \nEncode and Decode) to convert back and forth between instructions and words. The dynamic semantics of \nLOW is standard and is given in Figure 1. A machine con.guration (F, pc) is a pair consisting of a memory \nand a program counter; it halts, fails or evolves to another con.guration (F. , pc ') by executing the \ninstruction stored at pc in the code memory F.code.  3. The Key Ideas In this section, we present the \nkey ideas behind our work through the lens of an illustrative, challenging example. We will walk through \nthe code of this example, suggest intuitively how to reason about it, and then explain how our Kripke \nlogical relation formal\u00adizes this intuition. While we will initially ignore the question of how garbage \ncollection affects matters, we will return to this is\u00adsue at the end of the section and discuss how our \nlogical relation enables reasoning in the presence of a garbage collector. This section is intended to \nbe accessible to a broad programming\u00adlanguages audience, and hopefully to serve as a useful guide to \nthe  HIGH Syntax &#38; Semantics t ::= a | b | t1 \u00d7 t2 | t1 . t2 |.a. t |.a. t | \u00b5a. t | ref t e ::= \nx | \u00a3 |(e1,e2)| e.1 | e.2 | .x:t. e | e1 e2 | .a.e | et |pack (t1,e) as t2 | unpack e1 as (a, x) in e2 \n| rollt e | unroll e | ref e | e1 := e2 | !e | e1 == e2 | ... v ::= x | \u00a3 |(v1,v2)| .x:t. e | .a.e | \npack (t1,v) as t2 | rollt v | ... K ::= |(K, e2)|(v1,K)| K.1 | K.2 | Ke2 | v1 K | Kt | rollt K | unroll \nK | pack (t1,K) as t2 | unpack K as (a, x) in e2 | ref K | K := e2 | v1 := K | !K | K == e2 | v1 == K \n| ... S ::= \u00b7| S,\u00a3:t with ftv(t)= \u00d8 . ::= \u00b7| .,a G ::= \u00b7| G,x:t Static semantics : S; .; G f e : t def \nHCVal = { v | ftv(v)= \u00d8. fv(v)= \u00d8} def HHeap = { h . HLoc ..n HCVal }Dynamic semantics : (h, e) '. (h',e') \nLOW Syntax def PConf = { (F, pc) . PMem \u00d7 PAddr } def PMem = { F= (code, reg, stk, hp) . PCode \u00d7 PRegFile \n\u00d7 PStack \u00d7 PHeap } def def PCode = PAddr . Instruction PRegFile =Register . PWord def def PStack = PAddr \n. PWord PHeap = PAddr . PWord def def PAddr = { a . N} PWord = { w .{ 0, 1 }\u00d7 N}r . Register ::= sp | \nsv0 | ... | sv4 | wk0 | ... | wk5 lv . PLvalue ::= lrJ|(a)|(r - o)|(a)h |(r + o)h ss rv . PRvalue ::= \nlv | w . . Instruction ::= fail | halt | jmp rv | jnz rv rv | jneq rv rv rv |jptr rv rv | setptr lv \n| move lv rv | plus lv rv rv |minus lv rv rv | isr lv rv | isw rv rv LOW Semantics def def def def |w| \n= p2(w)isptr(w)=(p1(w)= 1) n =(0,n) aa =(1,a) def def F(w)= w F(lrJ) =F.reg(r) def def F((a)) =F.stk(a) \nF((r - o)) =F.stk(|F(lrJ)|- o) s def sdef F((a))=F.hp(a) F((r + o))=F.hp(|F(lrJ)| + o) h h def F[lrJ \n. w] =(F.code, F.reg[r . w], F.stk, F.hp) def F[(a) =(F.code, F.reg, F.stk[a . w], F.hp) . w] sdef F[(r \n- o) =(F.code, F.reg, F.stk[|F(lrJ)|- o . w], F.hp) . w] sdef F[(a) =(F.code, F.reg, F.stk, F.hp[a . \nw]) h. w] def F[(r + o) =(F.code, F.reg, F.stk, F.hp[|F(lrJ)| + o . w]) h. w] def [fail] (F, pc) = fail \ndef [halt] (F, pc) = halt def [jmp rv] (F, pc) =(F, |F(rv)|) [jnz rv1 rv2] (F, pc) = def if F(rv2)=0 \nthen (F, |F(rv1)|) else (F, pc + 1) [jneq rv1 rv2 rv3] (F, pc) = def if F(rv2)=F(rv3) then (F, |F(rv1)|) \nelse (F, pc + 1) def [jptr rv1 rv2] (F, pc) = if isptr(F(rv2)) then (F, |F(rv1)|) else (F, pc + 1) def \n[move lv rv] (F, pc) = (F[lv . F(rv)], pc + 1) def [setptr lv] (F, pc) = (F[lv .|F(lv)|], pc + 1) def \n[plus lv rv1 rv2] (F, pc) = (F[lv.|F(rv1)| + |F(rv2)|], pc + 1) def [minus lv rv1 rv2] (F, pc) = (F[lv.|F(rv1)|-|F(rv2)|], \npc + 1) def [isr lv rv] (F, pc) = (F[lv . E(F.code(|F(rv)|))], pc + 1) def [isw rv1 rv2] (F, pc) = ((F.code[ \n|F(rv1)| . D(|F(rv2)|)], F.reg, F.stk, F.hp), pc + 1) where E : Instruction . Nis a bijection, and D \n= E-1 . Dynamic semantics : (F, pc) '. [F.code(pc)] (F, pc) Figure 1. Syntax and Semantics for the HIGH \nand LOW Languages (excerpt) vast majority of readers who are not intimately familiar with recent developments \nin Kripke logical relations. 3.1 A Motivating Example Our motivating example is based on Pitts and Stark \ns awkward example [22]. Their original example is almost blindingly simple prove that the following HIGH \nterms are contextually equivalent: def e = let x = ref 0 in .f:unit . unit. (x := 1; f . ;!x) ' def e= \n.f:unit . unit. (f . ;1) The .rst term, e, allocates a fresh memory location x, initially set to 0, and \nthen returns a higher-order function. When called, the latter will set x to 1, invoke its callback argument \nf, and then return the contents of x. The second term, e', is similar, except that it does not bother \nallocating or updating any memory, and the function it de.nes always returns 1. Proving that e and e' \nare contextually equivalent is tantamount to showing that, in the former, whenever the callback invocation \nf . returns, x points to 1. After a moment s thought, it should hopefully be intuitively clear why the \nequivalence holds. The pointer x in e is initially set to 0, but once the function that e returns is \napplied for the .rst time, x will be set to 1 and will never be set back to 0. This is because e owns \nx as a piece of local state and the only thing e ever does to x after .rst allocating it is to set it \nto 1. Thus, the awkward example serves as an elegant distillation of (1) the ability of code to control \nsome local state and impose arbitrary constraints on it, and (2) the ability of that local state to evolve \nover time in an irreversible (or monotone) way. Such irreversible changes to local state arise in a variety \nof real-world situations e.g., in generative ADTs (whose sets of inhabitants grow over time), and in \ndata structure initialization [2]. It is thus rather remarkable that only recently have methods been \ndeveloped for proving an equivalence as simple as the awkward example [8, 2, 23]. (More on that in Section \n3.3.) We are now ready to present our motivating example. We want to prove a variant of the awkward example, \nnamely that the HIGH term e is implemented correctly by a LOW program p,where p s implementation follows \nthe second HIGH term e' fairly closely. By itself, that would already be an interesting result it would \ndemonstrate the extensional equivalence of a high-level program with an optimized low-level program, \nwhere the optimization is based on the inability of high-level program contexts to observe e s manipulation \nof local state. But we will make it more interesting still with an added twist: the code of the function \nthat p evaluates to will be obfuscated using a primitive form of encryption, and when .rst applied, the \nfunction will .rst decrypt and overwrite itself via self-modifying code. The LOW program p is shown in \nFigure 2. Before we walk through the code, let us .rst note that p is parameterized by alloc,a code pointer \nto the memory allocation routine, and bg, the location in the code segment where p s code will be loaded \nand where its execution will begin these parameters will be instantiated as part of linking and loading. \n(We will de.ne the formal semantics of linking and loading for LOW programs in Section 7.) bg: Create \nand return a closure. The evaluation of p is very simple: it does no interesting computation except to \nimmediately create and return a closure value, just as the HIGH term e' does. The .rst 3 instructions \nstarting at bg allocate a fresh one-word closure on the heap by invoking the alloc routine (passing it \nthe size parameter 1 in register wk5 and the return address bg + 3 in register wk4). We only need one \nword for the closure because the function we re implementing (e') is closed, so all we need to store \nin the closure is the naked code pointer. The alloc routine is assumed to return the pointer to a fresh \none-word cell in wk5, without modifying the contents of any registers except wk4 and  def e = let x \n= ref 0 in .f:unit . unit.x := 1; f ;!x def p = . alloc, bg. [ bg move lwk4J bg + 3 move lwk5J 1 jmp \nalloc bg + 3 move (wk5 +0)h bg + 5 jmp lwk0J bg + 5 move lwk3J bg + 10 bg + 6 isr lwk4Jlwk3J minus lwk4Jlwk4J \n666 isw lwk3Jlwk4J plus lwk3Jlwk3J 1 bg + 10 D(E(jneq bg + 6 lwk3J bg+21 )+666) bg + 11 D(E(isw bg + \n5 E(jmp bg+12) )+666) bg + 12 D(E(move (wk1 +0)h bg+13 )+666) bg + 13 D(E(plus lspJlspJ 1 ) + 666) D(E(move \n(sp - 1)lwk0J ) + 666) s D(E(move lwk1Jlwk2J ) + 666) D(E(move lwk0J bg+18 )+666) D(E(jmp (wk1 +0)h ) \n+ 666) bg + 18 D(E(move lwk5J 1 ) + 666) D(E(minus lspJlspJ 1 ) + 666) bg + 20 D(E(jmp (sp - 0)) + 666) \ns ] An important point: Why did we bother both redirecting the bg + 5 instruction (to jump to bg +12) \nand updating the code pointer in the function closure? Would the latter alone not have been suf.cient? \nThe answer is that it depends. If p were only evaluated once, in which case only one closure for this \nfunction were ever generated, then yes, just updating the code pointer would be suf.cient because bg \n+ 5 would become effectively dead code. But we would like our notion of high-low program equivalence \nto be preserved under a rich set of program contexts including those that evaluate the programs here, \np and e more than once. Repeated evaluation of p will result in the creation of multiple closures for \nthe function p de.nes, and merely updating the code pointer for one closure will not change the fact \nthat other closures may still point to bg + 5, so it is necessary to redirect the bg + 5 instruction \nas well. bg + 13: Implement .f. (f ;1). This is the implementation of the function proper. We .rst push \nour return address wk0 onto the stack. We then invoke the callback argument (f in the HIGH code) by moving \na pointer to f s closure into wk1,moving the return address bg +18 into wk0, and jumping to f s code \npointer wk1 +0 h.(Note: f s argument type is unit, so there is no need to pass anything in the argument \nregister wk2.) When control is returned to bg + 18, we store the result 1 in the result register wk5, \npop the return address off the stack, and jump to it. Figure 2. Motivating Example wk5. We then store \nin that cell the code pointer bg + 5,before jumping to the return address, which we assume the linker/loader \nhad passed to p originally in register wk0. (The linker/loader has essentially the same calling convention \nas for ordinary functions, which is different from the one for alloc and is described below see bg + \n12.) Although our calling convention is that return values are passed back to the caller in wk5, our \nreturn value was already in wk5 after the call to alloc, so we need not explicitly move anything into \nwk5 before returning. We now describe the implementation of the closure returned (in wk5) by the evaluation \nof p. Initially, this closure contains just a code pointer to bg + 5, but eventually that code pointer \nwill be updated (see below). bg + 5: Decrypt and overwrite the code. The code from bg + 10 on is obfuscated \nby the addition of 666 to the machine representation of each instruction. Eventually, once the code is \ndecrypted, the function will be executable starting at the address bg + 13. Before that time, however, \nthe function must begin ex\u00adecuting at bg + 5, because the .rst step will be to decrypt and overwrite \nthe obfuscated instructions. The reader can easily verify manually that the code starting at bg + 5 will \nuse register wk3 to loop through the instructions bg +10 through bg +20. For each, it will use isr to \nread the instruction stored at wk3 into wk4, sub\u00adtract 666 from it, and then write the decrypted instruction \nback to the code segment at address wk3. When this loop is .nished, the program counter will be at bg \n+ 11. bg + 11: Redirect the .rst instruction of the function. Hav\u00ading decrypted the code, we do not want \nfuture calls to this function to perform the decryption again. We therefore overwrite the .rst instruction \n(at bg + 5) with a jump to bg + 12. bg + 12: Update the code pointer. The LOW calling conven\u00adtion is \nthat a function is passed its return address in wk0, its argu\u00adment in wk2, and its own closure in wk1. \nThe decryption code start\u00ading at bg + 5 did not touch any of these registers, so at bg + 12 we know that \nwk1 still stores a closure with a code pointer to bg + 5. Having decrypted the code, we can now safely \nupdate this code pointer wk1 +0 h to point to bg + 13, the address where the function begins its computation \nin earnest.  3.2 Discussion of the Motivating Example Why does p implement e? Intuitively, the reason \nis that the self\u00admodifying aspects of p are not visible to p s clients because they do not affect its \nextensional behavior; and ultimately, once p has decrypted itself, it behaves essentially the same as \nthe HIGH term e ' , which we have already argued is equivalent to e. Of course, this begs the question: \nhow exactly do we know that p s clients cannot observe its self-modi.cations? Interestingly, the answer \nis remarkably similar to the argument for why e and e ' are equivalent, namely that what p does to its \nown code takes the form of irreversible changes to local state. Speci.cally, we take it as a given that \np owns its own code, and since the evaluation of p will allocate a fresh memory cell for the closure \nit returns, p owns that closure as well. Thus, in reasoning about p, we can place restrictions on how \nits code and closure may evolve over time. Much as the local variable x in e starts out pointing to 0 \nand eventually points to 1, the code of p starts out in encrypted form, and if/when any closure it returns \nis .rst applied, it changes to decrypted form. In both cases, it is critical that we never revert to \nthe earlier state. Similarly, the closure returned by the evaluation of p starts out with its code pointer \nset to bg + 5, but if/when the closure is ever applied, it will be set to bg + 13.In this case, it is \nnot so essential for correctness that the code pointer never revert back to bg+5,but it is essential \nthat the closure s code pointer only be set to bg + 13 when the code is in the decrypted state. Given \nthese restrictions on how the local state of p and e may evolve, it is but a short distance to a bona \n.de proof. Before sketching that proof, let us .rst review the recent work on Kripke logical relations \nthat will put our reasoning on a solid footing.  3.3 Kripke Logical Relations and State Transition Systems \nLogical relations are a well-established technique for reasoning about equivalence of higher-order programs. \nA logical relation is de.ned inductively on the type structure of the language: at base type the logical \nrelation coincides with observable equality e.g., two programs of type int are logically related if they \nproduce the same integer and at higher type the relation is de.ned by inter\u00adpreting each type operator \nby the appropriate logical connective e.g., two functions are related at type t1 . t2 if relatedness \nof their arguments at type t1 implies relatedness of their results at type t2. The important feature \nabout logical relations for our purposes is that they give considerable leeway to how related functions \nare im\u00adplemented, so long as they produce related results. (As Benton and Hur [5] channeling Machiavelli \nput it, the ends justify the means. )  In the presence of state, we cannot talk about the relatedness \nof two programs without making some assumptions and impos\u00ading some restrictions on how they manipulate \nstate. This is where Kripke logical relations come in. Kripke logical relations are in\u00addexed by possible \nworlds, which represent a set of restrictions on the memories of the two programs under which the programs \nare guaranteed to behave equivalently. When we want to prove the re\u00adlatedness of two programs under a \nworld W , we suppose we are given arbitrary initial memories that are related by (i.e., satisfy the restrictions \nof) W , and we proceed typically by showing that when evaluated under those memories the programs either \n(1) both di\u00adverge (don t terminate), or else (2) produce values and .nal mem\u00adories that are related under \nsome future world W ' of W . What does it mean for W ' to be a future world of W ?If in the course of \nevaluation the programs allocate fresh pieces of memory, W ' may extend the initial world W with new \nrestrictions governing the use of the freshly allocated memory. This approach allows us to establish \nwhatever constraints we want on freshly allocated state that is kept local. (If the state is made globally \naccessible e.g., by being passed to the context at a ref type then the state will have to obey the usual \ninvariants dictated by the ref type.) In traditional Kripke logical relations, such as those of Pitts \nand Stark [22], possible worlds essentially take the form of simple memory relations, i.e., memory invariants. \nAswehaveseen in the awkward example, however, memory invariants are not necessar\u00adily enough; we need \nadditionally the ability to describe assumptions about state that may change in a controlled and monotone \nway. (It is thus not a surprise that Pitts and Stark put forth the awkward example as an example for \nwhich their method was inadequate.) To address this limitation, Ahmed et al. [2] proposed gener\u00adalizing \npossible worlds to include the ability for a memory rela\u00adtion to evolve.Dreyer et al. [14] later streamlined \nand extended Ahmed et al. s approach in various ways, and cast Ahmed et al. s possible worlds as collections \nof state transition systems (STS s). In the case of the awkward example, one can understand the re\u00adstrictions \nplaced on e s local variable x according to the following STS:  o (xy oD) ( D) . 0 xy. 1 When this \nSTS is .rst added to the initial world W , it starts out in the xy . 0 state, because after x is .rst \nallocated, it points to 0. However, under Dreyer et al. s model, future worlds of W may not only place \nadditional restrictions on fresh pieces of memory but also update the state of existing STS s in W . \nThus, in some future world W ' , the above STS may be switched to the xy . 1 state, and memories satisfying \nW ' would have to map x to 1.Furthermore, any future world of W ' would have to remain in the xy . 1 \nstate as there is no transition out of it. This corresponds to the intuitive reasoning about the example \nthat we described in Section 3.1.   3.4 State Transition Systems for the Motivating Example Using Kripke \nlogical relations based on state transition systems, we can now roughly sketch the proof of equivalence \nof p and e. We will prove that p and e are logically related in some initial world W0 that includes some \nbasic assumptions (in the form of STS s) about registers, the stack, etc. (See Section 7 for details.) \nFirst, since we can assume p has just been loaded into memory, we can think of its code as a freshly \nallocated piece of memory, and we are therefore given the opportunity to extend W0 with an STS governing \np s code. For most programs, we would extend W0 at this point with a one-state STS, representing the \nsimple invariant that the code of the program never changes. For our motivating example, we instead extend \nW0 with an STS of the form: d) (d d) eencrypted edecrypted When the STS is in the left state, the \nassociated memory relation will require that p s code be in its initial, encrypted form, and when the \nSTS is in the right state, the memory relation will require that p s code be in decrypted form. Once \ndecrypted, always decrypted. When p and e are executed, the former allocates a fresh closure on the heap, \nsetting its constituent code pointer to bg + 5,and the latter allocates the local ref cell x, setting \nits contents to 0.Since both the closure and x are freshly allocated, we may at that point also extend \nthe world with a new STS governing both of them:  xy. 0 .  xy . 1 .  closure y. bg + 13 . bg + 5closure \ny We have joined the assumptions about them into one STS because they change in lockstep: when the functions \nreturned by p and e are applied for the .rst time, x gets updated to 1 and the closure s code pointer \ngets updated to bg + 13 simultaneously. As noted in Section 3.2, it should never be the case that the \ncode is still encrypted while the closure returned by p points to bg + 13. (The closure would behave \nin an unspeci.ed manner if it were called in such a state.) When adding the second STS, it is therefore \nimportant that we outlaw this possibility up front so that we will not have to consider it later. Fortunately, \nDreyer et al. s model, on which our logical relation is based, allows us to de.ne the second STS in such \na way that we can only be in its right state if the .rst STS is also in its right state. Let W be the \nworld resulting from extending W0 with the above two STS s. What remains to be shown is that the functions \nreturned by p and e are related in world W . So, suppose that W ' is a future world of W , and that we \nbegin executing the high-and low-level functions in some corresponding high-and low-level memories that \nare related by W ' . There are three cases to consider, depending on the states of the two STS s of interest \nin W ' (the fourth case was outlawed, as described above): Case 1: Code is encrypted, xy. bg +5.In this \n. 0,closure ycase, we .rst decrypt the code, and then set x to 1 and the closure s code pointer to bg \n+ 13. Case 2: Code is decrypted, xy. bg +5.In this . 0,closure y case, we set x to 1 and the closure \ns code pointer to bg + 13. Case 3: Code is decrypted, xy. bg + 13.In this . 1,closure ycase, we set x \nto 1 and don t touch the closure s code pointer. In all three cases, we end up transitioning to a future \nworld W '' in which both STS s of interest are in the right state, if they weren t already there in W \n' . After making the state transition, both the high-and low-level functions invoke their callback arguments. \nAssuming they return, they will do so with memories that satisfy some future world of W '' , but in the \nSTS s of interest, there is nowhere to transition to. So we know that x must still point to 1. The high-and \nlow-level functions must therefore both return the same result, namely 1, along with memories that satisfy \na future world of the starting world W ' .  3.5 Well-Bracketed State Changes and Private Transitions \nFor simplicity, we have glossed over many details in the above proof sketch. One important detail is \nhow we reason about the stack. When the functions invoke their unknown callback argu\u00adments, we need to \nknow that the callback will return the stack i.e., the contents of the stack segment up to the stack \npointer sp as it found it. One approach would be to bake this condition into our logical relation for \nfunctions. However, as we explained in the introduction, we have set out to de.ne our relation in a largely \nlanguage-generic fashion, and it would not make sense to bake a low-level property about the stack into \na language-generic relation.  Instead, we wish to build this condition into the initial world under \nwhich we relate high-and low-level programs, but the prop\u00aderty we desire of the stack is not expressible \nin terms of STS s as we have described them so far. To account for stack-like behav\u00adior, we employ another \naspect of Dreyer et al. s possible worlds, namely the idea of private vs. public transitions [14]. Private \ntran\u00adsitions were introduced in order to reason about so-called well\u00adbracketed state changes, of which \nthe behavior of the stack is a perfect example. The basic idea is to label local state transitions as \neither public or private. Functions may make either private or public transitions internally, but viewed \nextensionally (i.e., end-to-end), they must appear to make a public transition. In the STS that we use \nto reason about the stack, the states of the STS correspond to the possible states of the stack; every \nstate is accessible from every other state by a private transition, but the only public transitions are \nself-transitions. This grants logically-related functions plenty of .exibility in how they manipulate \nthe stack, but requires that, when they return, they leave the stack exactly how they found it. We also \nuse private transitions to reason about callee-save reg\u00adisters, which logically-related functions are \nexpected to return as they found them, even if they modify them internally.  3.6 Reasoning in the Presence \nof Garbage Collection Another important detail we have glossed over is how the presence of a garbage \ncollector affects our proof of equivalence of p and e. Let us assume that we are using a standard mark-and-sweep \nor copying collector. The main effect such a collector has on our reasoning is that, whenever we pass \ncontrol to the allocator (or to an unknown function that may call the allocator), we need to make sure \nthat (1) all data that we care about being able to access in the future is in the reachable portion of \nthe heap, and (2) there are no dangling pointers in the reachable portion of the heap. Typically, guaranteeing \nthese two conditions is straightforward. In our example, there is one call to the allocator (at bg+2) \nand one call to an unknown function (at bg + 17). In the case of the former, there is nothing interesting \nto show. But in the case of the latter, it is important that when we increment the stack pointer before \ninvoking the callback, we set the contents at the top of the stack to a value from which no dangling \npointer may be reached. Here, we store in that stack slot the return address Lwk0J, which trivially satis.es \nthis requirement. So at an abstract level, reasoning in the presence of garbage collection is no big \ndeal. What is more interesting is the techni\u00adcal question of how we actually implement this reasoning \nin the context of our logical relation. The central dif.culty is that Kripke logical relations traditionally \nenjoy a monotonicity property, mean\u00ading that when two values are related in a world W , they are related \nin any future world of W . Monotonicity is essential when reason\u00ading about unknown functions, such as \nthe callback argument in our example. There, we were given the assumption that the callback argument \nwas logically related in the world W ' , but we did not ac\u00adtually invoke it until we had transitioned \nto the future world W '' . This step requires a use of monotonicity to show that the callback argument \nis still related in W '' . Unfortunately, garbage collection seems super.cially to throw a wrench into \nmonotonicity. For instance, in our logical relation we want to be able to relate a HIGH pair value v1,v2 \nwith a memory location pointing to a representation of that pair value on the heap. How can we expect \nthose two value representations to be related in all future worlds if, at some point in the future, the \nmemory location may get deallocated (or its contents moved by a moving collector) and later reused for \nstoring something else? Our solution is to employ logical memories, which form a layer of abstraction \nover physical memories. Pointers in a logical memory are never moved or deallocated. Their connection \nto reality is established by a component of the logical memory called the lookup table, which speci.es \nfor any given logical pointer whether it is live and, if so, what physical pointer it corresponds to. \nWe say that a logical memory M represents a physical memory F only if M s lookup table describes a bijection \nbetween the reachable portions of M and F. We maintain a global invariant on the logical memory, which \nmust hold before and after calls to the allocator, requiring that all reachable data be live (and thus \nnot dangling) according to the lookup table. Together with the de.nition of what it means for a logical \nmemory to represent a physical one, this invariant guar\u00adantees the allocator s precondition (2) above \nthat there are no reachable dangling pointers. After the allocator returns, the lookup table of the logical \nmemory may have completely changed e.g., due to a semi-space collection but the only other change to \nthe logical memory will be its extension with a freshly allocated log\u00adical pointer. Thus, if any data \nwe care about was reachable prior to allocation condition (1) above it will still be reachable post\u00adallocation, \nand by the global invariant it will also still be live. With logical memories in hand, we can adapt our \nlogical rela\u00adtions accordingly so that they relate values in HIGH with logical values (i.e., logical \npointers or non-pointer data) in LOW.We also de.ne our possible worlds to impose invariants on logical \nmem\u00adories, not physical ones. In this way, we regain monotonicity, as well as a clean, abstract account \nof memory locations that gives the garbage collector signi.cant .exibility in how it implements them \nand allows us to essentially ignore how it does so.  4. A Language-Generic Kripke Logical Relation Figure \n3 de.nes a Kripke logical relation between two languages that is parameterized by abstract speci.cations \nfor those languages (. LangSpec), as well as by a speci.cation for the possible worlds relating the memories \nof those languages (. WorldSpec). 4.1 Language Speci.cations A language speci.cation (upper left of \nFigure 3) must provide sets of values (Val), computations (Com), continuations (Cont), mem\u00adories (Mem), \nand con.gurations (Conf), together with a number of operations on these sets. (Note that we are assuming \nhere a stan\u00addard strati.cation on sets, and that Val, Com, etc. are smaller than LangSpec.) Most of these \noperations take elements of some of these sets and return a predicate on another of the sets. In most \ncases, this is because there may be a number of different represen\u00adtations of the same thing e.g., in \nLOW, a pair value v1,v2 is represented by a pointer that satis.es some conditions, but many pointers \nmay satisfy those conditions. plugv forms a con.guration by plugging a value into a con\u00adtinuation under \na given memory. plugc does the same, but plug\u00adging a computation instead of a value. step executes a \ncon.gu\u00adration for one step of computation, resulting in either a new con.g\u00aduration, termination (halt), \nor failure (fail). mdom returns the domain of a memory, and mdisj takes two memories and returns a memory \nthat contains their disjoint union, if it exists. oftype(t ) determines whether a value is considered \nsyntacti\u00adcally to have type t under a certain memory. In our speci.cation for HIGH, we include heap typings \nS in memories in order to de.ne this predicate at ref type. For LOW, there is no notion of syntactic \ntyping, but we .nd oftype convenient for expressing assumptions about the syntactic structure of closures \n(see Section 5).  def CType = { t | ftv(t)= \u00d8} def LangSpec = { (Val, Com, Cont, Mem, Conf, plugv, plugc, \nstep, mdom, mdisj, oftype, baseb, pair, app, appty, pack, roll, ref, asgn) | Val, Com, Cont, Mem, Conf \n. Set . plugv . Val \u00d7 Cont \u00d7 Mem . P(Conf) . plugc . Com \u00d7 Cont \u00d7 Mem . P(Conf) . step . Conf . Conf \nI{ fail, halt }.mdom . Mem . P(Val) . mdisj . Mem \u00d7 Mem . P(Mem) . oftype . CType . P(Val \u00d7 Mem) . baseb \n. [b]. P(Val \u00d7 Mem) . pair . Val \u00d7 Val . P(Val \u00d7 Mem) . app . Val \u00d7 Val . P(Com) . appty . Val \u00d7 CType \n. P(Com) . pack . CType \u00d7 Val . P(Val \u00d7 Mem) . roll . Val . P(Val \u00d7 Mem) . ref . Val . P(Val \u00d7 Mem) . \nasgn . Mem \u00d7 Val \u00d7 Val Mem . .M1,M2. .M . mdisj(M1,M2). mdom(M) . mdom(M1) I mdom(M2) } TyValRel . ..1(t) \noftype(t, .) V[a]. V[b]. V[t \u00d7 t ' ]. ' V[t . t]. V[.a. t]. V[.a. t]. V[ref t]. def ' :e = { (W ' ,W \n) | lev(W ) > 0 . W ; tW } def WVRel = { R . P(World \u00d7L1.Val \u00d7L2.Val) } def R(W )= { (v1,v2) | (W, \nv1,v2) . R } def tR = { (W, v1,v2) | lev(W ) > 0=. (tW, v1,v2) . R } def '' DR = { (W, v1,v2) |.W ; \nW. (W, v1,v2) . R } def '' ' RboW = { (W, v1,v2) | W :e W . (W, v1,v2) . R } def (R1,R2)= { (W, v1,v2) \n|.(M1,M2) .M(W ). (v1, M1) . R1 . (v2,M2) . R2 }for R1 . P(L1.Val \u00d7L1.Mem),R2 . P(L2.Val \u00d7L2.Mem) def \n = { (t1,t2,R) | t1,t2 . CType . R . WVRel }. TypeVar TyValRel def def = t[.(a).t1/a] ..2(t)= t[.(a).t2/a] \ndef = D(L1.oftype(..1(t)), L2.oftype(..2(t))) def = { (W, v1,v2) . oftype(a, .) | (W, v1,v2) . D.(a).R \n} def = { (W, v1,v2) . oftype(b, .) |.x . [b]. (W, v1,v2) . D(L1.baseb(x), L2.baseb(x)) } def ' = { \n(W, v1,v2) . oftype(t \u00d7 t ,.) | '' ' .(u1,u2) . tV[t].(W ). .(u1,u ) . tV[t ].(W ). 2 '' (W, v1,v2) \n. D(L1.pair(u1, u ), L2.pair(u2,u )) } 12 def '' = { (W, v1,v2) . oftype(t . t, .) |.W :e W. .(u1,u2) \n.V[t ' ].(W ' ). ' .e1 .L1.app(v1, u1). .e2 .L2.app(v2,u2). (W, e1,e2) .E[t]. } def ' = { (W, v1,v2) \n. oftype(.a. t,.) |.W :e W. .(t1,t2,R) . TyValRel. For L1, L2 . LangSpec, def WorldSpec = { (World, lev, \nM, B, O,t, ;, ;pub) |World . Set . lev . World . N . M. World . P(L1.Mem \u00d7L2.Mem) . B. World . P(L1.Val \n\u00d7L2.Val) . O. World . P(L1.Conf \u00d7L2.Conf) . t . World . World . ;. P(World \u00d7 World) . ;pub. P(World \u00d7 \nWorld) . ;, ;pub are preorders .;pub .; . '' .W ; W.tW ; tW . '' .W ;pub W. tW ;pub tW . .W. tW ;pub \nW . ' .W ; W. lev(W ' ) = lev(W ) . .W. lev(W ) > 0=. lev(tW )= lev(W ) - 1 }  .e1 .L1.appty(v1,t1). \n.e2 .L2.appty(v2,t2). ' (W, e1,e2) .E[t].[a . (t1,t2,R)] } def = { (W, v1,v2) . oftype(.a. t,.) |.(t1,t2,R) \n. TyValRel. .(u1,u2) .V[t].[a . (t1,t2,R)](W ). (W, v1,v2) . D(L1.pack(t1, u1), L2.pack(t2,u2)) } def \n' = { (W, v1,v2) . oftype(ref t, .) |.W ; W. .(M1,M2) .M(W ' ). v1,v2) .B(W ' ) . ( .(u1,u2) . tV[t].(W \n' ). (v1, M1) .L1.ref(u1) . (v2,M2) .L2.ref(u2). ( .(u1,u2) . tV[t].(W ' ). (L1.asgn(M1, v1, u1), L2.asgn(M2,v2,u2)) \n.M(W V[\u00b5a. t]. Fa,t,. \u00b5(F )(W ) K[t]. E[t]. def = \u00b5(Fa,t,.) def = .R. { (W, v1,v2) . oftype(\u00b5a.t,.) \n|.(u1,u2) .V[t].[a . (..1(\u00b5a. t),..2(\u00b5a. t),R)](W ). (W, v1,v2) . D(L1.roll(u1), L2.roll(u2)) } def \n= F (\u00b5(F )boW )(W ) def ' = { (W, K1,K2) . World \u00d7L1.Cont \u00d7L2.Cont |.W ;pub W. .(v1,v2) .V[t].(W ' ). \n.(M1,M2) .M(W ' ). .C1 .L1.plugv(v1, K1, M1). .C2 .L2.plugv(v2,K2,M2). (C1,C2) .O(W ' ) } def = { (W, \ne1,e2) . World \u00d7L1.Com \u00d7L2.Com |.(K1,K2) .K[t].(W ). .(M1,M2) .M(W ). .C1 .L1.plugc(e1, K1, M1). .C2 \n.L2.plugc(e2,K2,M2). (C1,C2) .O(W ) } ' )} Figure 3. Language Speci.cations, World Speci.cations, and \na Language-Generic Kripke Logical Relation The remaining operations de.ne the various syntactic language \nforms that are referenced in the de.nition of the logical relation. One point of note: determining whether \na value represents a par\u00adticular canonical form may require one to consider an accompany\u00ading memory. \nIt is easy to see why e.g., one cannot determine if a pointer to a pair of heap cells represents v1,v2 \nwithout inspecting the heap. Determining whether a computation represents a function application also \nmay require inspecting the memory, but we have found it technically more convenient to assume that any \nmemory inspection is somehow built into the notion of computation (see Section 5 to see how this works \nin the LOW speci.cation).  4.2 World Speci.cations A world speci.cation (lower left of Figure 3) de.nes \na set of possible worlds relating the memories of two languages speci.ed by L1 and L2, together with \na variety of operations on and relations indexed by those worlds. In order to understand some of these, \nit is necessary to .rst say a word about our use of step-indexing. Appel and McAllester [3] introduced \nstep-indexing in order to model recursive types in foundational proof-carrying code. The basic idea is \nto use a natural number index ( step level ) to stratify what would otherwise be a circular construction. \nStep-indexing has also proven useful in modeling other semantically circular notions like higher-order \nstate, which is how we use them here. Due to space considerations, since our step-indexed construction \nfollows closely that of [2] and [14], we refer the interested reader to those previous works for the \nrelevant background. In the world speci.cation, the important step-indexing-related bits are the lev \nfunction, which returns the step level of a given world, and the c (pronounced later ) operator, which \nreturns an approximated version of the given world at one lower step level. We use c to ensure well-foundedness \nof the logical relation. M(W ) is the memory relation associated with W ,which speci.es when two memories \n(from L1 and L2) satisfy the con\u00adstraints of the STS s in W . B(W ) is a bijection on values rep\u00adresenting \nmemory locations in L1 and L2. This is used in de.ning the logical relation for ref type. O(W ) is an \nobservation relation on con.gurations. For the possible worlds we employ in this paper, O(W ) actually \nonly depends on lev(W ), and it is de.ned to relate con.gurations that either both terminate (without \nfailure) or that both run for at least lev(W ) steps of computation. When we prove that two programs \nare logically related, we will prove it in starting worlds of an arbitrary step level, thus ensuring \nthat the programs are observably equivalent for arbitrarily many computation steps.  Finally, ; de.nes \nthe general future world relation between worlds, and ;pub de.nes a restricted public version of that \nrelation: if W ' ;pub W , then for any STS in W , the new state of that STS in W ' must be accessible \nfrom its old state in W only by public transitions (see Section 3.5). Both ; and ;pub are preorders. \n 4.3 Kripke Logical Relation The right side of Figure 3 displays our Kripke logical relation, whose \nde.nition is parametric w.r.t. L1, L2, and an instance of WorldSpec thereon. In the de.nition, we adopt \nthe convention that the entities (values, continuations, etc.) from L1 appear in boldface (v, K, etc.) \nand the entities from L2 appear in italics (v, K,etc.). The coincidence of the notation for L2 entities \nwith the notation for the corresponding entities from HIGH is deliberate, for in the next section we \nwill instantiate L1 and L2 with our speci.cations for LOW and HIGH, respectively. We abuse notation in \nthis way in order to avoid the proliferation of more than two fonts. Our logical relation is based very \nclosely on Dreyer et al. s [14], with the principal difference being that the relevant linguistic forms \nhave been abstracted away in the language speci.cations L1 and L2. For instance, in the logical relation \nfor arrow types, we do not construct the applications v1u1 and v2u2 directly, since L1 and L2 may not \ninclude an explicit application construct. Rather, we quantify over arbitrary computations e1 and e2 \ndrawn from L1.app(v1, u1) and L2.app(v2,u2), respectively. The logical relation consists of a relation \nfor values (V[t ].), one for continuations (K[t ].), and one for computations (E[t ].). Here, we assume \nthat . is a relational interpretation of the free variables of t , mapping them to arbitrary world-indexed \nvalue relations. For t = a, V[t ]. is de.ned as the restriction of .(a) to triples (W, v1,v2) where v1 \nand v2 are well-typed (according to Li.oftype) and continue to be related in all future worlds of W . \nThis last part, which is speci.ed using the DR operator de.ned at the top right of the .gure, is key \nto ensuring monotonicity of the value relation. The pair and existential cases of the value relation \nalso use the DR operator in order to ensure monotonicity of data representations e.g., that if v1 represents \na pair of u1 and u1' ,it will continue to do so in all future worlds. As in Appel et al. [4], the interpretation \nof recursive types is de.ned by induction on the strictly future world relation : . This relation is \nwell-founded because W ' : W implies that W ' has a lower step level than W . By de.ning the recursive \ntype case this way, we can relate HIGH programs, where roll and unroll are explicit coercions, to LOW \nprograms where they have been erased. The ref type case relates two memory locations if dereferencing, \nassigning and testing them for pointer equality will always produce related results. The condition on \npointer equality testing is guaran\u00adteed by the requirement that the locations be in the bijection of \nthe world in which they are related. The value relation is lifted to a relation on computations by the \ntechnique of biorthogonality (aka TT-closure) [22, 17]. The idea is to de.ne two computations to be related \nif they behave in an observably equivalent manner when plugged into related continu\u00adations. Two continuations \nare in turn related if they behave in an observably equivalent manner when plugged with related values. \nBy quantifying only over public future worlds in the de.nition of K[t ]., we ensure that computations \nmay only make public transi\u00adtions when viewed end-to-end, as per the discussion in Section 3.5. This \nmind-bending technique is well-suited to languages where the evaluation of a computation is context-sensitive \nin the sense that it cannot be performed in ignorance of its continuation. Such is the case with LOW, \nwhere a computation always ends with a jump to its return address. As we shall see, the fact that the \nreturn address really is a valid return address, which is part of the contract between computation and \ncontinuation, will be encoded in the LOW implementation of plugc that we de.ne in Section 5.  5. Implementing \nthe Speci.cations In order to instantiate our logical relations to relate LOW and HIGH entities, we must \n.rst show how to implement the abstract LangSpec interface for both languages. For HIGH, the implementation \nof the interface is almost en\u00adtirely straightforward, as all the required entities (values, computa\u00adtions) \nhave direct correspondents in the HIGH language. The only slightly unusual bit is that we de.ne HIGH \nmemories to be pairs of heaps and heap typings S. The inclusion of the heap typing is necessary for de.ning \noftype. For the remaining details, please see the companion technical appendix [15]. Low-Level Entities \nThe implementation of LangSpec for LOW (Figure 4) is much more interesting. As described in Section 3.6, \nwe employ a notion of logical values v, which are either non\u00ad pointer words w or logical pointersal. \nLogical Lvalues are similar to physical PLvalues except that logical Lvalues include logical heap locations \nl : o h in place of physical ones a h. We include the offset o because the logical heap is (for convenience) \nmodeled two\u00addimensionally as a list of blocks. LOW computations e are 4-tuples (cpc, kpc, vloc, data), \nwhere: cpc is the code address where the computation begins, kpc is the return address, vloc is the Lvalue \nwhere the return value will be stored, and data is a memory predicate that must be satis.ed in order \nfor the computation to be correctly executed. LOW con\u00adtinuations K are pairs (kpc, vloc),where: kpc is \nthe code address where the continuation begins, and vloc is the Lvalue where the input to the continuation \nshould be placed. It is worth noting that, unlike e.cpc and e.kpc,the kpc in K must be a code address, \nnot an Rvalue, because when we plug an e into a K, we need to know that the value of K.kpc will be the \nsame before and after the execution of e.Were K.kpc an Rvalue, we would not know that. Logical memories \nM are 6-tuples (code, reg, stk, hp, tab, shp), where: code is the code segment, reg is the register .le \n(minus sp since it is determined by the size of the stack), stk is the stack, hp is the heap, tab is \nthe lookup table (described in Section 3.6), and shp is the system heap, which is a separate portion \nof the heap controlled by the runtime system. The lookup table maps each log\u00adical pointer to a physical \npointer and the size of the memory block starting at that address. The pointer is live iff the size is \n> 0.Note that reg, stk,and hp are all maps from various Lvalues to logical values, whereas the tab and \nshp are maps to physical values. In the proofs, we end up treating tab and shp as essentially black boxes, \nsince they can be changed at whim by the allocator, whereas the allocator should not mess around with \nthe logical portion of the memory represented by reg, stk,and hp. Lastly, note that reg, stk, tab,and \nshp may all be unde.ned (undef). This is a useful technical device for de.ning the disjoint union of \nseveral partial memories: it enables us to specify that only one of those partial memories contains information \nabout, say, the stack. (See the de.nition of mdisj below.) Low-Level Representations of High-Level Constructs \nThe bot\u00adtom left of Figure 4 de.nes LOW representations of various high\u00adlevel constructs, as required \nby the LangSpec interface. A pair of v1 and v2 is represented as a pointer to a pair of cells containing \nv1   def w if v = w Loc = { l . N}|v| =1 def l if v = l def Word = { w . N} def def M(v)= vM(lrJ)= \nM.reg(r) v . Val ::= w | 1l def def M((a))= M.stk(a) M((r - o))= M.stk(|M.reg(r)|- o) ss lv . Lvalue \n::= lrJ|(a)|(r - o)|(l : o)h |(r + o)h def def ss M((l : o)h)= M.hp(l)(o) M((r + o)h)= M.hp(|M.reg(r)|)(o) \nrv . Rvalue ::= lv | v def def M[[T,S]] =(M.code, M.reg, M.stk, M.hp,T,S) Com = { e =(cpc, kpc, vloc, \ndata) def . Rvalue \u00d7 Rvalue \u00d7 Lvalue \u00d7 P(Mem) } M[l : o . v]hp =(M.code, M.reg, M.stk, def Cont = { \nK =(kpc, vloc) . PAddr \u00d7 Lvalue } M.hp[l . (v0,..., vo-1, v, vo+1,..., vn-1)], def CodeFrag = PAddr \n.n Instruction M.tab, M.shp) def RegFile =(Register \\{sp}. Val) I{ undef } if M.hp(l)=(v0,..., vn-1) \n. o<n def . List X = { (x0,...,xn-1) | n . N. x0,...,xn-1 . X } . w if v = w defdef Stack =List Val \nI{ undef } 1if v = 1 phyv(M)(v)= a l . M.tab(l)=(n, a) def . Heap =Loc .n List Val undef otherwise def \n Table =(Loc .n N\u00d7 PAddr) I{ undef } def def phyh(M)= [a . phyv(M)(v0),..., phyv(M)(vn-1)] SysHeap = \n(PAddr Word) I{ undef } M.tab(l)=(n,a) . n>0 . M.hp(l)=(v0,...,vn-1) def Mem = { M =(code, reg, stk, \nhp, tab, shp) def M reprF =F.code . M.code .. CodeFrag \u00d7 RegFile \u00d7 Stack \u00d7 Heap \u00d7 Table \u00d7 SysHeap } F.reg \n. phyv(M) . M.reg . F.reg(sp) = |M.stk|. def Conf =PConf .j< |M.stk|. F.stk(j)= phyv(M)(M.stk(j)) . def \noftype(t)= { (v, M) . Val \u00d7 Mem | F.hp . phyh(M) I M.shp . 1 .t1,t2.t = t1 . t2 =..l,w. v = l . M.hp(l)(0) \n= w . . l,n, a. M.tab(l)=(n, a) . n> 0=.|M.hp(l)| = n ' 1 .a, t ' .t = .a. t =..l,w. v = l . M.hp(l)(0) \n= w } def plugv(v, K, M)= { (F, pc) . Conf | M repr F . def baseb(x)= { (v, M) . Val \u00d7 Mem | v is a \nrepresentation of x } pc = K.kpc . M(K.vloc) = v }def 1def pair(v1, v2)= { (v, M) . Val \u00d7 Mem |. l. v \n= l . plugc(e, K, M)= { (F, pc) . Conf | M repr F . M . e.data . M.hp(l)(0) = v1 . M.hp(l)(1) = v2 } \npc = M(e.cpc) . M(e.kpc) = K.kpc . e.vloc = K.vloc } def 1 app(v1, v2)= { e . Com |. l. v1 = l . defstep(F, \npc) = R with (F, pc) '. R e.cpc = (l :0)h . e.kpc = lwk0J. e.vloc = lwk5J. def e.data = { M . Mem | M.reg(wk1)= \nv1 . M.reg(wk2)= v2 }} mdom(M)= {1l . Val | l . dom(M.hp) } def def appty(v,t)= { e . Com |. l. v = 1l \n. mdisj(M1, M2)= { M . Mem | e.cpc = (l :0)h . e.kpc = lwk0J. e.vloc = lwk5J. M.code . M1.code I M2.code \n. e.data = { M . Mem | M.reg(wk1)= v }} M.hp . M1.hp I M2.hp . def '' nosh(M.reg, M1.reg, M2.reg) . pack(t, \nv)= { (v , M) . Val \u00d7 Mem | v = v } def roll(v)= { (v ' , M) . Val \u00d7 Mem | v ' = v } nosh(M.stk, M1.stk, \nM2.stk) . def '' 1nosh(M.tab, M1.tab, M2.tab) . ref(v)= { (v , M) . Val \u00d7 Mem |. l. v = l . M.hp(l)(0) \n= v } nosh(M.shp, M1.shp, M2.shp) } 1 def M[l :0 . v2]hp if v1 = l .|M.hp(l)| > 0 def asgn(M, v1, v2) \n=nosh(X, X1,X2)=(X1 = undef =. X2=undef . X=X1) . undef otherwise (X2 = undef =. X1=undef . X=X2) Figure \n4. The Implementation of LangSpec for LOW and v2. pack(t, v) and roll(v) are represented the same as \nv.Ref\u00aderences are represented directly as pointers. We also use oftype(t ) to enforce that values of \narrow and universal type are represented by pointers to closures whose .rst cell is not a logical pointer. \n(This ensures that we can jump to the code address directly.) The most interesting bit is the representation \nof app(v1, v2) (and appty(v,t ), which is similar). In order for a computation e to represent this application, \nv1 is assumed to be a pointer al to a closure. The starting address of the function application (e.cpc) \nis thus taken to be the code address stored in the .rst cell of the closure, i.e., l :0 h. Our calling \nconvention is that, when the function is called, the return address should be stored in wk0,and when \nthe function returns, the return value should be stored in wk5, so e.kpc and e.vloc re.ect this convention. \nFinally, the memory predicate e.data requires that when control is passed to e.cpc,the function v1 and \nargument v2 are stored in wk1 and wk2. Connecting Logical and Physical Memories The right side of Figure \n4 de.nes the remaining elements of LangSpec, along with a number of auxiliary operations. The operations \nat the top right give shorthand for various lookup and update operations on logical memories. phyv(M)(v) \nreturns the physical interpretation of v according to M s lookup table, if one exists, and phyh(M) returns \nthe live portion of the physical heap according to M s lookup table. Note that the de.nition of phyh \ndemands that the physical representations of logical memory blocks with distinct head pointers be disjoint, \nthus ensuring a proper bijection between the reachable parts of the physical and logical heaps. M repr \nF saysthat M is a valid logical abstraction of F.The de.nition is fairly straightforward, making use \nof phyv and phyh as one would expect. The fourth line of the de.nition guarantees that the reachable \nheap is disjoint from the system heap, and the .fth condition just checks that the block sizes speci.ed \nfor live data in M.tab are correct. Note that F may contain arbitrary other junk (in the code, stack, \nand heap segments) that is not described by M. Plugging Continuations Using M repr F, it is easy to specify \nhow to plug continuations with values and computations. A con\u00ad.guration (F, pc) belongs to plugv(v, K, \nM) if (1) M is a valid abstraction of F, (2) the program counter pc is set to the starting address of \nthe continuation (K.kpc), and (3) the value v is stored in the location where the continuation is expecting \nit (K.vloc). A con.guration (F, pc) belongs to plugc(e, K, M) if (1) M is a valid abstraction of F,(2) \nM satis.es the memory constraints demanded by e, (3) the program counter pc is set to the starting address \nof the computation (e.cpc), (4) the starting address of the continuation (K.kpc) is stored in the place \nwhere the computation is expecting to .nd its return address (e.kpc), and (5) the place where e will \nstore its return value (e.vloc) is the same place where K is expecting to .nd its input value (K.vloc). \nThe remaining de.nitions (of step, mdom,and mdisj)are fairly self-explanatory. As mentioned earlier, \nthe de.nition of  def [bg=[bg . instrs(0),..., instrs(|instrs|- 1)]. instrs] def {C}code =(C, undef, \nundef, \u00d8, undef, undef) . Mem def {H}heap =(\u00d8, undef, undef,H, undef, undef) . Mem def T if v = w v live \nin M = 1 .n, a. M.tab(l)=(n, a) . n> 0 if v = l def 1 reach0(M)= { l |.r . Register. l = M.reg(r) }. \n1 { l |.j< |M.stk|. l = M.stk(j) } def reachi+1(M) =reachi(M) . { l |.l ' . reachi(M). .j. 1l = M.hp(l \n' )(j) } def reach(M)= i.N reachi(M) def AllocSpec = {A . PAddr .{ (init, alloc, instrs,I) . PAddr \u00d7 \nPAddr \u00d7 List Instruction \u00d7 P(Table \u00d7 SysHeap) }| .gcbg, F, pc. F.code . [gcbg= .A(gcbg).instrs] . F.reg(wk4)= \npc . .M ' , F ' . * (F, A(gcbg).init) '. (F ' , pc) . F ' .code = F.code . M ' .code = [gcbg.A(gcbg).instrs] \n. M ' repr F ' . M ' .A.GR(gcbg) . M ' .A.MR(gcbg) . .gcbg, M, F, pc,n. M repr F . M .A.GR(gcbg) . M \n.A.MR(gcbg) . M.reg(wk4)= pc . M.reg(wk5)= n =. .F ' , M ' ,T,S,w, l,w0,...,wn-1. * (F, A(gcbg).alloc) \n'. (F ' , pc) . M ' repr F ' . M ' .A.GR(gcbg) . M ' .A.MR(gcbg) . M ' = M[[T,S]][wk4 . w]reg[wk5 . 1l \n]reg I {[l . (w0,...,wn-1)]}heap } def 1 A.GR(gcbg) = { M . Mem |. l . reach(M). l live in M } def A.MR(gcbg) \n= { M . Mem | (M.tab, M.shp) .A(gcbg).I . M.code . [gcbg .A(gcbg).instrs] } Figure 5. Abstract Speci.cation \nof the Memory Allocator mdisj uses the undef option for reg, stk, tab,and shp to en\u00adsure that if a memory \nis split into disjoint pieces, each of these indivisible components can only appear in one of the pieces. \nPossible Worlds The model of possible worlds that we use to im\u00adplement WorldSpec is based very closely \non Dreyer et al. [14]. For space reasons, we will therefore not present the details of the model in this \npaper and instead refer the reader to the appendix [15]. (See Section 8 for a more detailed discussion \nof how our model relates to Dreyer et al. s.) That said, there are a few salient aspects of the model \nthat are needed for understanding the de.nition of compositional program equivalence that we will give \nin Section 7. In the model, worlds W are 3-tuples (k, ., GR),where: k is W s step level, . is a .\u00adnite \nset of state transition systems (STS s) of the sort described in Section 3, and GR is a global invariant \ngoverning the entire mem\u00adory. We call the STS s islands because they govern disjoint pieces of memory. \nTwo memories are related by W if (1) they satisfy its global invariant GR, and (2) they can be split \ninto disjoint memo\u00adries (one for each island) such that the n-th pair of memories satis\u00ad.es the local \nmemory relation determined by the current state of the n-thislandin .. In our de.nition of program equivalence, \nwe will use GR to enforce the property that all reachable data is live. We need to use a global invariant \nsince reachability cannot be de\u00adtermined by looking at a local subheap. We use islands to express all \nother assumptions about memory.  6. Assumptions About the Memory Allocator Figure 5 shows the assumptions \nwe make about memory allocation and garbage collection, in the form of the speci.cation AllocSpec. Given \nas input a starting physical address gcbg, the runtime system represented by A will return a 4-tuple \n(init, alloc, code,I),where: init is the starting address of the initialization routine that sets up \nthe runtime system, alloc is the starting address of the allocator, code is the list of instructions \nde.ning the runtime system, which are assumed to be loaded at address gcbg,and I is a private invari\u00adant \nof the runtime system, which describes when a logical mem\u00adory s lookup table is in sync with its system \nheap. The assumption about code is joined together with the private invariant I to form the memory predicate \nMR de.ned at the bottom of the .gure. Assuming init is invoked with the runtime system code in the right \nplace, and with a return address placed in wk4, its spec\u00adi.cation says it will return control in a memory \nthat satis.es MR(gcbg), along with the global invariant GR(gcbg) that all reachable data is live. Assuming \nthat alloc is invoked in a physical memory F repre\u00adsented abstractly by the logical memory M,that M satis.es \nthe MR and GR properties, that the number of cells to be allocated (n)isstored in wk5, and that the return \naddress is stored in wk4, the speci.cation of alloc says that it will return a pointer to a fresh n-word \nblock in wk5, and that the memory it returns (M ' ) will con\u00adtinue to satisfy all the aforementioned \ninvariants. Moreover, while the lookup table and system heap of M ' may be completely dif\u00adferent from \nthose of M, the contents of M must remain otherwise unchanged. This does not of course prevent the allocator \nfrom hav\u00ading performed a GC: any logical pointer that was not reachable in M before the call to alloc \nmay very well be marked as dead in the lookup table of the post-allocation M ' , but any pointer that \nwas reachable in M will still be reachable in M ' and thus, by the de.\u00adnition of GR, still be live. Our \nspeci.cation of the runtime system provides considerable .exibility for example, it should be satis.ed \nby either a mark\u00adand-sweep or a copying collector because the speci.cation says nothing about the private \ninvariant of the runtime system. However, it does assume that the collector places no restrictions (such \nas read or write barriers) on what the mutator does to live data. We believe it should be possible to \nadapt our approach to a wider range of collectors, but we leave that to future work.  7. Compositional \nProgram Equivalence The logical relation E[t ]. characterizes what it means for two computations to be \nlogically equivalent, but ultimately what we really care about is whether a pair of HIGH and LOW programs \nare logically equivalent. What, one may wonder, is the difference between computations and programs? \nIn short, a program is what you write, and a computation is what you run. That is, a program is a piece \nof relocatable code that must be linked with other programs and loaded into memory before it can be executed, \nwhereas a computation describes the next thing to be executed in a running machine con.guration. For \nthe HIGH language, the distinction between computations and programs can be easily glossed over because \nthe operational semantics of HIGH is de.ned directly on HIGH programs. For the LOW language, however, \nespecially given the ability to write self-modifying code, it is important to distinguish the two notions. \nIn this section, we explain what a LOW program is and how to de.ne logical equivalence between HIGH and \nLOW programs, and we then present our key technical results. 7.1 Equivalence of HIGH and LOW Programs \nAs can already be seen from our motivating example in Section 3.1, we de.ne a LOW program p to be a function \nfrom two code pointers to a list of instructions. The .rst of the code pointer inputs is assumed to be \nthe address of the memory allocation routine, and the second is assumed to be the address where the list \nof instructions returned by the program will be loaded into memory.  def H.Prog = { e | .oc(e)= \u00d8} def \nL.Prog = { p . PAddr \u00d7 PAddr . List Instruction } def D[\u00b7]= \u00d8 def D[.,a]= { (., a . R) | . .D[.]. R . \nTyValRel } def G[\u00b7]. = { (W, v, \u00d8) | W . World . v .L.Val } def G[G,x : t]. = { (W, v, (., x . v)) |.v1, \nv2. (W, v, ()) . D(L.pair(v1, v2), H.Val \u00d7H.Mem) . (W, v1,v) .V[t]. . (W, v2,.) .G[G]. } W . =(k, [.regstk,.htyping,.gc(A, \ngcbg)], GR.(A, gcbg)) def k (A, gcbg) def .; G f bg W e : t = '' .W ; W. .. .D[.]. .(v,.) .G[G].(W ). \n((bg, lwk0J, lwk5J, { M | M.reg(sv0)= v }),..e) .E[t].(W ' ) where ..e ::= e[.(a).t2/a][.(x)/x] def .; \nG f p e : t = \u00d8;.; G f e : t . .A, gcbg, bg. .k, W ; W .(A, gcbg). .(M,M) .M(W ). k .M ' . M ' = M I{[bg \n. p(A(gcbg).alloc, bg)]}code =. '' .W ; W. lev(W ' )= lev(W ) . (M ' ,M) .M(W ) . .; G f bg W ! e : t \n Figure 5) and that the HIGH memory satis.es its heap typing. All these components of Wk . are formally \nde.ned in the appendix [15]. We are now ready to de.ne the program equivalence relation . The judgment \n.; G f p e : t says that a LOW program p and a HIGH program e are equivalent if the following is true. \nFirst, let k be any starting step level, and let W be any future world of Wk . that does not already \nimpose any invariants on the code segment of p. (This latter condition is guaranteed by the assumption \nthat we are given initial memories M and M related by W ,but where M does not contain the code segment \nof p. We use the notation M1 l M2 here to denote the smallest memory in mdisj(M1, M2),in a sense de.ned \nformally in the appendix.) Under these assumptions, we must be able to: 1. Extend W to a future world \nW ' (of the same step level) such that W ' relates M ' and M,where M ' is M extended with the code of \np. Intuitively, this step affords p the opportunity to own its own code segment by extending W with an \nisland governing it. Typically, this island will take the form of an in\u00advariant stating that p s code \nmust never be modi.ed, although in the case of self-modifying code we would instead de.ne the is\u00adland \nto be a state transition system (as described in Section 3.4). 2. Show that the HIGH computation e is \nrelated (under the world  Figure 6. Program Equivalence When listing the code of a program (e.g., in \nFigure 2), we write line numbers on selected lines of code (e.g., bg + 3) to indicate the physical addresses \nwhere we expect the code to be loaded, but note that (1) these addresses are always relative to the second \nparameter of the program (typically named bg), so that the code is always relocatable, and (2) the notation \nis merely suggestive the line numbers are not part of the actual program. Now, concerning equivalence \nof HIGH and LOW programs: it is possible to de.ne a notion of logical equivalence strictly between closed \nprograms, but we will .nd it useful when reasoning about compiler correctness to generalize this relation \nto one on open pro\u00adgrams. On the HIGH side, an open program is simply an expres\u00adsion e with free variables \n(and no free locations), but what is an open program on the LOW side? In order to answer this, we need \nto pick an environment-passing convention, specifying what is an of.cial low-level representation of \na high-level environment and where the low-level program expects to get its environment data. In Figure \n6, we give a logical relation G[G]. between low-level values and high-level environments. This relation \nspeci.es that a high-level environment . should be represented as a linked list of low-level values that \nare componentwise related to the high-level values in the range of .. We assume that the environment \ndata is passed in the register sv0 this assumption will be codi.ed in the de.nition of the program equivalence \nrelation (see below). Before we de.ne program equivalence, we must also specify the invariants on memories \nthat are required for executing programs. These conditions are represented by the initial worlds Wk . \nin Fig\u00adure 6. (The k in Wk . simply determines the step level but does not otherwise affect the de.nition.) \nThe initial worlds consist of three islands and one global invariant: .regstk owns the register .le and \nthe stack in the LOW memory and requires that callee-save reg\u00adisters and stack be preserved before and \nafter function calls (this is accomplished using a combination of private and public transi\u00adtions as \ndiscussed in Section 3.5); .htyping requires that the heap typing in the HIGH memory should only grow \nin future worlds; .gc (A, gcbg) owns the lookup table and the system heap and en\u00adforces the private A.MR(gcbg) \ninvariant of the runtime system (Figure 5); and the global invariant GR.(A, gcbg) speci.es that W ' constructed \nin the previous step) to the LOW computation e that starts at the beginning (bg)of p s code segment. \nThis subgoal is encapsulated in the open computation equivalence judgment .; G f bg W ! e : t (Figure \n6). This judgment says that, for any future world W '' of W ' , for any relational interpre\u00adtation . \nof the type variables in ., and for any environments v and . related under W '' by G[G].,the HIGH computation \n..e is logically related (by E[t ]., under W '' )to the LOW compu\u00adtation starting at address bg. The \nother three components of the LOW computation stipulate, respectively, that (1) the compu\u00adtation expects \nto .nd its return address stored in wk0,(2) the computation will store its resulting value in wk5, and \n(3) the computation expects to .nd its environment stored in sv0.One can think of this last assumption \nabout sv0 as having the effect of closing p, in much the same way that . and . close e.  7.2 Compiler \nCorrectness and Other Technical Results Our .rst result is an adequacy theorem for our program equivalence \nrelation . The statement of the theorem refers to the following simple loader for the LOW language: load(A,p) \n.rst runs the initialization routine of the memory allocator A, then executes p, and .nally halts as \nsoon as it gets control back from p: load(A,p) ::= let (init, alloc, gcinstrs, ):= A(105), instrsp := \np(alloc, 105 + |gcinstrs|), loadinstrsp := [ (* 100 *) move Lwk4J 102 jmp init (* 102 *) move Lwk0J 104 \njmp 105 + |gcinstrs| (* 104 *) halt ]++gcinstrs ++instrsp in { (F, 100) . PConf | F.code . [100 . loadinstrsp] \n} The adequacy theorem states that closed HIGH and LOW pro\u00adgrams that are equivalent according to must \nequi-terminate when loaded by the above loader. Theorem 1 (Adequacy). For all \u00d8; \u00d8f p e : t , .A . AllocSpec. \n.(F, pc) . load(A,p). .h. both (F, pc) and (h, e) diverge or both halt without fail. One might think \nthat this adequacy result is weak because the all reachable blocks in the LOW memory are live (A.GR(gcbg) \nin loader does not inspect the result returned by the program p.How\u00ad Papp(p1,p2) ::= . alloc, bg. let \ninstrs1 := p1(alloc, bg +4),c1 := |instrs1|, instrs2 := p2(alloc, bg + c1 +6),c2 := |instrs2| in [ bg \nplus lspJlspJ 2 move (sp - 2)lwk0J s move (sp - 1)0 s move lwk0J bg + c1 +4 instrs1 bg + c1 +4 move (sp \n- 1)lwk5J s move lwk0J bg + c1 + c2 +6 instrs2 bg + c1 + c2 +6 move lwk0J(sp - 2) s move lwk1J(sp - \n1) s move lwk2Jlwk5J minus lspJlspJ 2 jmp (wk1 +0)h ] Figure 7. Compilation of Function Application ever, \ntogether with the compositionality result below, one can link the program p with arbitrary well-behaved \ntest programs and the linked programs are guaranteed to behave the same. In order to show that our logical \nrelations are suf.ciently populated an important desideratum, as we explained in the introduction we \nhave written a very na\u00a8ive compiler from HIGH to LOW and proved that every source program is related \nto its compiled low-level program by our program equivalence. Speci.\u00adcally, we have implemented a low-level \nconstruct corresponding to each high-level construct and then de.ned the compiler purely in\u00adductively \non the structure of source programs using these low-level constructs. For each construct, we have shown \na corresponding compatibility lemma (following Pitts terminology [21]), mean\u00ading that program equivalence \nis preserved under said construct. This implies that equivalent programs behave the same under arbi\u00adtrary \nwell-behaved contexts. One example of a high-level construct is function application. Figure 7 shows \nits low-level realization as a simple linking program Papp(p1,p2). We assume here that p1 is some program \ncomputing a value of function type t ' . t ,and p2 is some program comput\u00ading a value of the argument \ntype t ' . The program begins by bumping up the stack pointer twice, pushing the return address (stored \nin wk0) onto the .rst new stack slot, and clearing the second one with 0. The clearing instruction (at \nbg + 2) is needed because the stack slot at sp - 1 s might otherwise contain a dangling pointer; thus, \nin order to maintain the global invariant that all reachable data is live, we must clear sp - 1 s before \npassing control to p1.When p1 returns, we as\u00adsume it returns a pointer to a function closure in wk5, \nwe store that pointer in the cleared stack slot, and we proceed to execute p2.Af\u00adter p2 returns, we move \np1 s closure pointer into wk1, the argument value (returned by p2)into wk2, and the original return address \nof Papp(p1,p2) into wk0. We then pop off two stack slots and make a tail call to the code pointer stored \nin wk1 s closure. The compatibility result for Papp can be seen as a composition\u00adality result for our \nprogram equivalence relation. Lemma 1 (Compatibility: App). .; G f p1 e1 : t ' . t . .; G f p2 e2 : \nt ' =. .; G f Papp(p1,p2) e1 e2 : t Another related construct of course is .-abstraction. Figure 8 shows \nits low-level realization as the program Pabs(p). We assume here that p is a program implementing the \nbody of a .-abstraction, under the assumption that the argument of that abstraction is the .rst element \nin the linked list environment stored at sv0. Pabs(p) ::= . alloc, bg. let code := p(alloc, bg +16),c \n:= |code| in [ bg move lwk4J bg + 3 move lwk5J 2 jmp alloc bg + 3 move (wk5 +0)h bg + 6 move (wk5 +1)h \nlsv0J jmp lwk0J bg + 6 plus lspJlspJ 2 move (sp - 2)lwk0J s move (sp - 1)lsv0J s move lwk4J bg + 12 \nmove lwk5J 2 jmp alloc bg + 12 move (wk5 +0)h lwk2J move (wk5 +1)h (wk1 +1)h move lsv0Jlwk5J move lwk0J \nbg + c +16 code bg + c +16 move lsv0J(sp - 1) s minus lspJlspJ 2 jmp (sp - 0) s ] Figure 8. Compilation \nof .-Abstraction The actual computation of the program is quite simple, because it merely allocates a \nclosure representing the .-abstraction and returns it (just like the example in Section 3.1). The closure \nconsists of a code pointer (to bg + 6) and an environment pointer, which is of course just whatever environment \nwas passed to Pabs(p) in sv0. Whenever the closure is invoked (by jumping to bg + 6), it .rst saves the \nreturn address (stored in wk0), as well as the callee\u00adsave register sv0, by pushing them on the stack. \nIt then pushes the argument value Lwk2J onto the front of the environment linked-list headed by wk1 +1 \nh (which requires allocating two new memory cells), and stores a pointer to this extended environment \nin sv0 before executing p. Finally, when p returns, it pops the stack twice, and restores the original \ncontents of the callee-save sv0,before jumping to the return address that was stored on the stack. The \ncompatibility result for Pabs is as follows: Lemma 2 (Compatibility: Abs). .; G,x : t ' f p e : t =. \n.; G f Pabs(p) .x:t ' .e : t ' . t Using Papp, Pabs, and all the other LOW program construc\u00adtors, we \ncan de.ne a compiler .G f e. in a simple, syntax-directed fashion, e.g., .G f e1 e2. ::= Papp(.G f e1., \n.G f e2.) .G f .x:t. e. ::= Pabs(.G,x:t f e.) and then establish the following compiler correctness result: \nTheorem 2 (Compiler Correctness). For \u00d8;.;G f e : t , .; G f .G f e. e : t The theorem is easily provable \nby induction on e, using the appro\u00adpriate compatibility lemma in each case. Finally, we prove the self-modifying \nawkward program pawk is equivalent to the high-level awkward program eawk from Figure 2. Theorem 3. \u00d8; \n\u00d8f pawk eawk :(unit . unit) . int As a corollary, we can see that for any \u00d8; \u00d8; \u00d8f e : unit . unit, \nboth eawk e and load(A, Papp(pawk, .\u00d8f e.)) equi-terminate. Similarly, for any \u00d8; \u00d8; \u00d8f e :((unit . unit) \n. int) . t , both eeawk and load(A, Papp(.\u00d8f e.,pawk)) equi-terminate.  Detailed proofs of all these \nresults appear in the companion technical appendix [15].  8. Related and Future Work There is a huge \nbody of work on compiler correctness and seman\u00adtics for low-level code. We focus on the most closely \nrelated work. Compositional Compiler Correctness As explained in the intro\u00adduction, the overall motivation \nof our work is very similar to that of Benton and Hur [5], and our use of logical relations to build \nan ex\u00adtensional, compositional notion of equivalence between high-and low-level languages is inspired \ndirectly by their work. However, there are signi.cant differences between our work and theirs. First, \nthey de.ne a relation between a purely functional PCF\u00adlike language and an SECD machine, whereas we relate \na more expressive, impure, ML-like high-level language to an assembly language that is signi.cantly more \nlow-level and realistic than SECD. Reasoning about compositional equivalence in our setting is signi.cantly \nmore complex, not least because we must deal with reasoning about the heap and the presence of a garbage \ncollector. We make essential use of Kripke logical relations for this purpose. That said, there is a \nsense in which our setting makes the problem easier. One of Benton and Hur s goals was to develop a model \nof low-level programs that would admit program equiva\u00adlences (such as commutativity of addition) whose \nvalidity depend on the purely functional nature of the source language. Toward this end, they related \nlow-level programs to denotations of high-level programs, so that one could use domain-theoretic reasoning \nto establish the purely functional equivalences of interest. These half\u00adoperational, half-denotational \nrelations were of necessity asym\u00admetric. In particular, they employ biorthogonality but only on the low-level \nside of the relation as well as step-indexing but only in de.ning one direction of approximation (in \nthe other direc\u00adtion, they use an admissible closure operation). As a result, proving in their setting \nthat a high-and low-level program are equivalent really involves doing two proofs (one for each direction \nof approx\u00adimation) using very different technical machinery. In our work, we sidestep this problem because \nthe ML-like lack of effect encapsulation in our high-level language causes it to have a relatively weak \nequational theory that simply does not admit the kinds of purely functional equivalences that Benton \nand Hur were interested in. Nevertheless, as our motivating example illustrates, there are still plenty \nof interesting equivalences in our setting, par\u00adticularly involving uses of local state. Moreover, our \nlogical rela\u00adtions, being entirely operational and de.ned in language-generic fashion, are inherently \nsymmetric, making them easier to use. More recently, Benton and Hur [6] have generalized their tech\u00adnique \nto a compiler for a polymorphic (yet still purely functional) language, but their logical relations are \nstill asymmetric. Chlipala [11] proposes a syntactic approach to proving compo\u00adsitional compiler correctness. \nHis idea is to establish a set of criteria for high-low compilation relations, such that the results \nof different compilers can be correctly linked so long as their compilation rela\u00adtions satisfy these \ncriteria. However, the criteria are still syntactic, and thus he cannot reason for instance about our \nmotivating ex\u00adample, wherein the high-low equivalence depends on semantic rea\u00adsoning about local state. \nMoreover, Chlipala only considers a fairly high-level target language that is a CPS version of the source. \nJaber and Tabareau [16] propose an alternative approach to compositional compiler correctness based on \ntype preservation. In\u00adstead of proving compiler correctness directly, they prove that the compiler is \ntype-preserving, but their source language has such a rich type system (with dependent re.nement types) \nthat this effec\u00adtively implies correctness. Like Benton and Hur, they compile a purely functional language \nto an SECD machine, but their correct\u00adness result only applies to terminating programs. None of the aforementioned \nwork considers a target language that supports garbage collection. Kripke Logical Relations Our logical \nrelation is based closely on Dreyer, Neis and Birkedal s (hereafter, DNB) [14], which was in turn a re.nement \nand generalization of Ahmed, Dreyer and Rossberg s [2]. (Of course, these are but the latest in a long \nline of work on Kripke logical relations spanning decades see loc. cit., as well as Pitts and Stark [22], \nfor further pointers to the literature.) DNB s main goal was to show how a Kripke model based on state \ntransition systems could be extended in orthogonal ways to exploit the absence of certain features, namely \nhigher-order state and/or control effects. For the HIGH language considered here, in which there are \nno control operators, DNB showed how to extend their baseline model with private transitions (Section \n3.5) and demonstrated the utility of private transitions in reasoning about a variety of challenging \ncontextual equivalences involving local state. For our purposes, private transitions have proved useful \nin formalizing the well-bracketing assumptions about the stack and callee-save registers, assumptions \nwhich indeed rely on the absence of control operators. Extending the HIGH language with control operators \nwould thus necessitate a signi.cant change to our Kripke model, precisely because the compilation strategy \nfor HIGH would need to change as well. We leave this problem to future work. Despite the close connection, \nour logical relation diverges from DNB s in several ways. First and foremost, whereas DNB s relation \nwas only designed to reason about high-level programs, the whole point of our model is to allow us to \nrelate high-and low-level pro\u00adgrams. As a result, we have no fundamental theorem of logical re\u00adlations \nbecause one cannot even state such a theorem for a relation between two languages. Instead, we prove \na compiler correctness result, whose proof mirrors that of the usual fundamental theorem. Our consideration \nof low-level programs has also led us to make a clear distinction between programs and computations, \nand between the notions of equivalence thereon. This seems to us an interesting and important distinction \nthat is worth investigating further. Dealing with low-level programs introduces signi.cant techni\u00adcal \ncomplexity. In order to isolate this complexity, we factor the presentation of our relation generically \nw.r.t. a language speci.ca\u00adtion interface LangSpec. This helps to clarify the structure of our Kripke \nlogical relation, bringing into relief its essentially sym\u00admetric high-level structure, as well as the \ncomponents of the model that are language-dependent (namely the two implementations of LangSpec, where \nmost of the complexity lies). Although we have in this paper only instantiated our model so as to relate \nHIGH and LOW programs, one may also instantiate the model to relate HIGH and HIGH programs, or LOW and \nLOW programs. The former model would be largely similar to Dreyer et al. s; the latter would enable one \nto reason about equivalence of low-level programs di\u00adrectly, which may prove useful in reasoning about \ncorrectness of low-level optimizations, although this remains to be explored. As far as possible worlds \nare concerned, although ours are largely similar to DNB s, we have extended their worlds in one relatively \nstraightforward way: in addition to local invariants (ex\u00adpressed in our possible worlds by islands), \nour worlds also permit one to express a global invariant. We exploit this added functional\u00adity to encode \nthe allocator s invariant that all reachable data are live (Sections 3.6 and 6). Lastly, as far as the \nhigh-level structure of the logical relation is concerned, ours is also quite similar to DNB s, except \nthat our interpretation of reference types is somewhat different. Various approaches to interpreting \nreference types have been proposed in the literature; our present interpretation is in a more extensional \nstyle than either DNB s or Ahmed et al. s [2], in the sense that it avoids dependence on too many details \nof how possible worlds are structured. This enables us to present it, as we have, in a more world-generic \nfashion. Among existing accounts, our present for\u00admulation is fairly close to the denotational one given \nby Birkedal, St\u00f8vring and Thamsborg [7], but the jury is still out on which of these formulations is \nmost felicitous.  Although the primary bene.t of presenting our logical relation language-and world-generically \nis to clarify its (admittedly com\u00adplex) structure, it also enables us to prove a few structural lem\u00admas \ngenerically as well, most notably monotonicity. (That we can prove monotonicity generically should not \nbe surprising, given that it is essentially baked into the logical relation via the quanti.cation over \nfuture worlds in certain cases and the use of the D operator in others.) Most of the interesting theorems, \nhowever, cannot be stated, let alone proven, without talking about the details of the LangSpec s for \nHIGH and LOW. Garbage Collection Torp-Smith et al. [24] prove the correctness of a Cheney copying collector \nusing separation logic. They specify the behavior of a correct garbage collector in terms of an isomor\u00adphism \nbetween the reachable portions of the initial and .nal heaps. Our speci.cation is broadly similar in \nthat we express the reach\u00adable portion of the heap in our logical memories. We construct our entire LangSpec \nfor LOW around these logical memories. Building on the work of Torp-Smith et al., McCreight et al. [20] \ndevelop a garbage collector interface that is general enough to characterize a variety of different collectors, \nincluding incremen\u00adtal copying collectors with read and write barriers. They prove in Coq that various \ncollectors implement this interface, and that various mutator programs respect it. In more recent work, \nMc-Creight et al. [19] extend Leroy s Compcert compiler [18] with support for garbage collection, by \nbuilding the mutator-collector interface into the design of a new intermediate language, GCminor. They \nprove semantics preservation (mostly) for a compiler from a purely functional, typed language Dminor \nto GCminor, but (as in Compcert) not compositional correctness. Self-Modifying Code Cai et al. [9] provide \none of the only formal accounts of how to verify self-modifying code. They use Hoare\u00adstyle separation-based \nreasoning to enable local reasoning about modi.cations to code, in much the same way that standard sep\u00adaration \nlogic enables local reasoning about the heap. We adopt a similar approach, using possible worlds to impose \nlocal invariants on and, more generally, to establish local state transition systems governing both the \nheap and the code segment. Ours is the .rst relational model for reasoning about (compositional) equivalence \nof self-modifying programs. Future Work Given our ability to reason about self-modifying code, one direction \nfor future work is to adapt this functionality to reason about more practical applications, such as a \njust-in-time compiler or a dynamic linker/loader. We have shown compositionality of our high-low relation \nby showing that it is closed under the linking constructs expressible at the level of our HIGH language, \nsuch as function application. In future work, we would like to prove that our relation is also compositional \nw.r.t. a more realistic linking language. The concrete logical relation we have presented here assumes \na uniform data representation. It is possible in principle to de.ne the meaning of language forms like \npair(v1, v2) in a non-uniform way e.g., to enable .attening but it is not currently possible in our language-generic \nframework to de.ne such language forms in a type-specialized manner. We leave a serious examination of \nthis issue to future work. Lastly, it is unclear how to scale our techniques to reason about compositional \ncorrectness of a multi-phase compiler because the step-indexed logical relations we de.ne are not obviously \ntransi\u00adtive. This is a well-known problem with step-indexed logical rela\u00adtions [1], and it seems a fresh \nidea is needed to circumvent it.  Acknowledgments We would like to thank Jacob Thamsborg for pointing \nout a very subtle technical oversight in an earlier draft of this paper.  References [1] A. Ahmed. Step-indexed \nsyntactic logical relations for recursive and quanti.ed types. In ESOP, 2006. [2] A.Ahmed,D.Dreyer, and \nA. Rossberg. State-dependent representa\u00adtion independence. In POPL, 2009. [3] A. Appel and D. McAllester. \nAn indexed model of recursive types for foundational proof-carrying code. TOPLAS, 23(5):657 683, 2001. \n[4] A. Appel, P.-A. Melli`es, C. Richards, and J. Vouillon. A very modal model of a modern, major, general \ntype system. In POPL, 2007. [5] N. Benton and C.-K. Hur. Biorthogonality, step-indexing and com\u00adpiler \ncorrectness. In ICFP, 2009. [6] N. Benton and C.-K. Hur. Realizability and compositional compiler correctness \nfor a polymorphic language. Technical Report MSR-TR\u00ad2010-62, Microsoft Research, Apr. 2010. [7] L. Birkedal, \nK. St\u00f8vring, and J. Thamsborg. A relational realizability model for higher-order stateful ADTs. Submitted \nfor publication, 2010. [8] N. Bohr. Advances in Reasoning Principles for Contextual Equiv\u00adalence and \nTermination. PhD thesis, IT University of Copenhagen, 2007. [9] H. Cai, Z. Shao, and A. Vaynberg. Certi.ed \nself-modifying code. In PLDI, 2007. [10] A. Chlipala. A certi.ed type-preserving compiler from lambda \ncalcu\u00adlus to assembly language. In PLDI, 2007. [11] A. Chlipala. Syntactic proofs of compositional compiler \ncorrectness, 2009. Submitted for publication. [12] A. Chlipala. A veri.ed compiler for an impure functional \nlanguage. In POPL, 2010. [13] Z. Dargaye. V\u00b4eri.cation formelle d un compilateur pour langages fonctionnels. \nPhD thesis, Universit\u00b4e Paris 7 Denis Diderot, July 2009. [14] D. Dreyer, G. Neis, and L. Birkedal. The \nimpact of higher-order state and control effects on local relational reasoning. In ICFP, 2010. [15] C.-K. \nHur and D. Dreyer. Technical appendix for this paper, 2010. URL: http://www.mpi-sws.org/~dreyer/papers/lrmlasm/. \n[16] G. Jaber and N. Tabareau. Krivine realizability for compiler correct\u00adness. In LOLA, 2010. [17] J.-L. \nKrivine. Classical logic, storage operators and second-order lambda-calculus. Annals of Pure and Applied \nLogic, 68:53 78, 1994. [18] X. Leroy. A formally veri.ed compiler back-end. Journal of Auto\u00admated Reasoning, \n43(4):363 446, 2009. [19] A. McCreight, T. Chevalier, and A. Tolmach. A certi.ed framework for compiling \nand executing garbage-collected languages. In ICFP, 2010. [20] A. McCreight, Z. Shao, C. Lin, and L. \nLi. A general framework for certifying garbage collectors and their mutators. In PLDI, 2007. [21] A. \nPitts. Typed operational reasoning. In B. C. Pierce, editor, Advanced Topics in Types and Programming \nLanguages, chapter 7. MIT Press, 2005. [22] A. Pitts and I. Stark. Operational reasoning for functions \nwith local state. In HOOTS, 1998. [23] E. Sumii. A complete characterization of observational equivalence \nin polymorphic lambda-calculus with general references. In CSL, 2009. [24] N. Torp-Smith, L. Birkedal, \nand J. C. Reynolds. Local reasoning about a copying garbage collector. TOPLAS, 30(4), 2008.  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>There has recently been great progress in proving the correctness of compilers for increasingly realistic languages with increasingly realistic runtime systems. Most work on this problem has focused on proving the correctness of a particular compiler, leaving open the question of how to verify the correctness of assembly code that is hand-optimized or linked together from the output of multiple compilers. This has led Benton and other researchers to propose more abstract, compositional notions of when a low-level program correctly realizes a high-level one. However, the state of the art in so-called \"compositional compiler correctness\" has only considered relatively simple high-level and low-level languages.</p> <p>In this paper, we propose a novel, extensional, compiler-independent notion of equivalence between high-level programs in an expressive, impure ML-like &#955;-calculus and low-level programs in an (only slightly) idealized assembly language. We define this equivalence by means of a biorthogonal, step-indexed, Kripke logical relation, which enables us to reason quite flexibly about assembly code that uses local state in a different manner than the high-level code it implements (<i>e.g.</i> self-modifying code). In contrast to prior work, we factor our relation in a symmetric, language-generic fashion, which helps to simplify and clarify the formal presentation, and we also show how to account for the presence of a garbage collector. Our approach relies on recent developments in Kripke logical relations for ML-like languages, in particular the idea of possible worlds as state transition systems.</p>", "authors": [{"name": "Chung-Kil Hur", "author_profile_id": "81436594089", "affiliation": "Max Planck Institute for Software Systems, Saarbruecken, Germany", "person_id": "P2509585", "email_address": "gil@mpi-sws.org", "orcid_id": ""}, {"name": "Derek Dreyer", "author_profile_id": "81100381796", "affiliation": "Max Planck Institute for Software Systems, Saarbruecken, Germany", "person_id": "P2509586", "email_address": "dreyer@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926402", "year": "2011", "article_id": "1926402", "conference": "POPL", "title": "A kripke logical relation between ML and assembly", "url": "http://dl.acm.org/citation.cfm?id=1926402"}