{"article_publication_date": "01-26-2011", "fulltext": "\n A Technique for the Effective and Automatic Reuse of Classical Compiler Optimizations on Multithreaded \nCode Pramod G. Joisha, Robert S. Schreiber, Prithviraj Banerjee, Hans-J. Boehm, Dhruva R. Chakrabarti \nHewlett-Packard Laboratories, Palo Alto, California, USA {pramod.joisha, rob.schreiber, prith.banerjee, \nhans.boehm, dhruva.chakrabarti}@hp.com Abstract A large body of data-.ow analyses exists for analyzing \nand optimi\u00adzing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, \nfor reasons of correctness. This paper presents a technique to automatically, aggressively, yet safely \nap\u00adply sequentially-sound data-.ow transformations, without change, on shared-memory programs. The technique \nis founded on the no\u00adtion of program references being siloed on certain control-.ow paths. Intuitively, \nsiloed references are free of interference from other threads within the con.nes of such paths. Data-.ow \ntransfor\u00admations can, in general, be unblocked on siloed references. The solution has been implemented \nin a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of \nup to 41% are possible, with an average improve\u00adment of 6% across all the tested programs over all thread \ncounts. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Compilers, Optimization \nGeneral Terms Algorithms, Languages, Theory Keywords Data-Flow Analysis, Parallel-Program Optimization \n1. Introduction It has long been known that classical compiler optimizations, i.e., sequentially-sound \ntransformations based on data-.ow analy\u00adsis frameworks [16], cannot be directly applied on parallel code, \neven under conditions that would be correct for the sequential case [22, 17, 19, 26, 30, 3]. The problem \nstems from asynchronous updates. Classical methods were not designed to reason about them under a multiplicity \nof interleavings [17, 30]. Parallel-code opti\u00ad mization has hence been specially addressed, in mainly \ntwo ways: Devise analyses and optimizations from the ground up, or adapt existing sequential analyses \nand optimizations, often using spe\u00adcialized program representations [35, 17, 31, 19, 26, 30].  Assume \nthe program to be well-synchronized, i.e., free of data races, and restrict the scope of classical transformations \nto the synchronization-free regions (SFRs) of the code.  The .rst has been used to analyze and optimize \nthe so-called ex\u00adplicitly parallel program (EPP). An EPP is shared-memory code Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 in which parallelism is expressed \nusing the cobegin/coend con\u00adstruct, or some equivalent. The second is how all production C/C++ compilers \nthat we are aware of, such as the GNU C/C++ Compiler (gcc) and Open64, currently optimize multithreaded \ncode. The SFR approach promises sequential consistency (SC) [18] to the programmer, if the code is data-race \nfree. The EPP approach usually gives full SC , i.e., SC even in the presence of data races. 1.1 Limitations \nof Past Approaches Since the EPP approach ensures interleaving semantics, it can pro\u00adduce overly conservative \nresults on well-synchronized programs. The reason is that to assure full SC, all con.icting accesses \nthat could be performed by two threads, without them performing in\u00adtervening synchronizations, have to \nbe modeled [36].1 Such ac\u00adcesses do not exist in well-synchronized code, since they would be data races. \nThus, it would be easier to deduce in well-synchronized code, for example, whether the expression x+x \nis even. Second, neither Pthreads [13] nor OpenMP [27] currently de\u00ad .ne semantics under data races. \nNeither do the current drafts of the C and C++ standards.2 Implementations already take advantage of \nthis fact, for example, by reordering memory operations on possi\u00adbly shared locations. Therefore, it \nappears wasteful to arti.cially restrict an analysis not to do the same. Third, since the EPP approach \ntypically relies on IRs (interme\u00addiate representations) that go beyond traditional sequential IRs, it \ncan incur high infrastructure costs. For instance, an EPP IR may in\u00adclude special nodes like .-and p-functions \n[35, 19]. It may include edges to re.ect properties peculiar to a parallel setting, such as con\u00ad.icts \n[19] and synchronizations [31, 26]. To exploit the information borne by these new nodes and edges, existing \ntransformations will have to be reworked. Hence, incorporating the approach into a com\u00adpiler either means \na from-scratch enterprise, or extending a serial compiler s phase or recasting it to a parallel IR. Whichever \nway, the undertaking is expensive. In contrast, this paper s approach allows for the direct reuse of \nexisting data-.ow transformations. In the SFR approach, data statements (i.e., synchronization\u00adfree statements \n[1]) are modeled without concurrency consider\u00ad ations. For example, may-de.nition maydef and may-use \nmayuse sets, which form the basis of data-.ow analyses, do not account for concurrent accesses at data \nstatements. These accesses are consid\u00adered only at synchronizations. In compilers like gcc, this presently \nhappens automatically, albeit exceedingly conservatively, because synchronizations are viewed as fully \nopaque. It should be empha\u00adsized that data-race freedom is essential for this approach. 1 Accesses of \nthe same location, all of which are not reads, con.ict [34]. 2 Java and .NET presently do de.ne limited \nsemantics for data races, though it remains unclear whether this can be done usefully and fully correctly \n[33]. The approach presented here would require adjustment for those languages.  X== Y==0 Thread h2Thread \nh1 0t1 := X 0' do { pthread mutex lock(l) 1' 1 pthread mutex lock(l) t4 := Y 2 Y:=1 2' pthread mutex \nunlock(l) 3 pthread mutex unlock(l) 3' } while (t4 == 0) 4' 4 do { pthread mutex lock(l) X:=11 5t2 := \nY 5' pthread mutex lock(l) 6' 6 pthread mutex unlock(l) Y:=2 7 } while (t2 == 1) 7' pthread mutex unlock(l) \n8t3 := t1 9 print(t3) Figure 1. The two snippets comprise the program, which always prints 0. If X were \ncopy propagated from Line 0 to Line 8, the result would be data-race free, but 11 would always be printed \ninstead. 1.1.1 The Pitfall of Simply Extending Optimization Scopes Consider the multithreaded program \nin Figure 1, which uses the Pthreads library [13]. The snippets in the columns, along with initialization \nand termination code, fully constitute the program. The left and right snippets are only executed by \nthreads h1 and h2 respectively. The only shared variables X and Y are initialized to 0. Despite accesses \nof X not being protected by a lock, the program is data-race free. (The two threads coordinate on Y, \nwhich is always accessed with a lock held.) This program always outputs 0. In the left snippet, the only \naccess of X is the read on Line 0. A compiler, on examining it in isolation, might conclude that an op\u00adportunity \nto propagate through critical sections exists, from Line 0 to Line 8. Interestingly, this would preserve \ndata-race freedom, un\u00adlike many past examples of invalid transformations, such as bit.eld manipulations \nand register promotion [3]. Nevertheless, it is incor\u00adrect since the transformed program would always \noutput 11.3 The root of the problem is that the right snippet updates X when running concurrently with \nthe left snippet. Because detecting this requires more analysis, and potentially of a whole-program nature, \ncurrent compilers follow the SFR approach and safely avoid the problem because a synchronization like \npthread mutex lock,as well as any procedure that may transitively call it, is regarded as having a possible \nside-effect on all globally visible data. 1.1.2 Bene.ts of Judiciously Extending Optimization Scopes \nThe opaque treatment of synchronizations has the adverse conse\u00adquence that legally exploitable opportunities \ncould, at the same time, get blocked. For instance, consider another hypothetical pro\u00adgram consisting \nof the left snippet in Figure 1 but a different right snippet, one that does not update X. As before, \nassume that only h1 executes the left snippet and h2 the right snippet. In this situation, it would be \ncorrect to copy propagate X from Line 0 to Line 8. We now discuss an example, derived from real code, \nwhere syn\u00adchronization opacity hinders the recovery of useful knowledge that is obfuscated by IR lowering. \nFigure 2 displays a critical section originally from SPLASH-2 s FMM benchmark [41]. The only dif\u00adference \nfrom the original are the calls to the LockedPrint proce\u00addure, which FMM itself provides. LockedPrint \nwrites to stdout, under the protection of a lock. The calls have been randomly in\u00adserted to illustrate \nthe additional complexities posed by an arbitrary number of nested critical sections to the recovery \nprocess. Modern compilers often work with a variant of the Static Single Assignment (SSA) Form that takes \ninto account aliasing effects and 3 The example assumes a fair scheduler, so the program is guaranteed \nto make progress. It would have to be modi.ed for the case of a nonpreemptive uniprocessor scheduler, \nby suitably inserting thread-yielding operations. LOCK(G Memory->count lock); my id = G Memory->id; LockedPrint(\"%d\\n\", \nmy id); G Memory->id++; LockedPrint(...); UNLOCK(G Memory->count lock); 4t3 := t'->id 5 LockedPrint(\"%d\\n\", \nt3) 6t'' := .(G Memory) . . . 7t4 := t''->id 8t5 := t4+1 . 9t''->id := t5 0t1 := G Memory 10 LockedPrint(...) \nt''' 1t2 := &#38;(t1->count lock) 11 := .(G Memory) . 2 pthread mutex lock(t2). . 3t' := .(G Memory) \n&#38;(t''' . 12 t6 := ->count lock) . . 13 pthread mutex unlock(t6) Figure 2. A critical section originally \nfrom SPLASH-2 s FMM program, and its lowered HSSA Form. Lowering hides the equiva\u00adlence of the pointers \nt2 and t6. A challenge is to recover such knowl\u00adedge under multithreading, by safely applying classical \nmethods. indirect memory operations, such as the HSSA Form by Chow et al [7]. gcc s Memory SSA Form is \nakin to HSSA [25]. Therefore, for pedagogical reasons, we base our discussions on HSSA. Figure 2 also \nshows the critical section s lowered HSSA Form, in which special constructs, called . assignments and \n\u00b5 oper\u00adations , denote ambiguous de.nitions and uses [7]. The . assign\u00adments on Lines 3, 6 and 11 are \nexamples they signify that a possi\u00adble side-effect of the preceding calls, including concurrent effects, \nis the changed global variable G Memory.4 Since these calls can have other side-effects, such as changed \nheap objects, there can be more . assignments between Lines 3 and 4, 6 and 7, and 11 and 12. All lines \nexcept 2, 5, 10 and 13 are purely data statements. Un\u00adder data-race freedom, concurrent effects need \nnot be accounted for against them, because the . assignments against synchronizations account for these \neffects. This, however, stymies a classical opti\u00admizer from establishing the equivalence of t2 and t6. \nA standard application of a pointer analysis will not uncover this equivalence. If t2 and t6 had nonsingleton \nmay-points-to sets, nothing can be said about their equivalence.5 Likewise, no equivalence conclusions \ncan be drawn if t2 and t6 have empty must-points-to sets. We shall later see, in Section 4.5.1, how a \nclassical optimizer can be transparently empowered to safely regain such knowledge.  1.2 A New Approach: \nThe Siloed-References (SR) Technique This paper presents a two-step technique for extending the scope \nof arbitrary classical optimizations across synchronizations. First, program references to objects that \ncan be statically proved to be free of cross-thread interference are determined. The speci.c attribute \nour analysis identi.es is the siloed property. Informally, an object reference is siloed on a procedure \nf if no other thread writes (reads or writes) the object whenever a thread is executing a path in f in \nwhich it reads (writes) the object. Second, the IR s maydef and mayuse sets are narrowed using these \nsiloed references this unveils previously blocked classical transformation opportunities. Since these \nabstractions are only narrowed, the outcome of the technique is not the same as synchronization removal \n[6, 29]. Our methods use two existing compiler algorithms, designed for sequential programs, as building \nblocks. They are a .ow-insensitive interprocedural pointer analysis, and a transformation to an HSSA\u00adlike \nform. The former is used to derive aliasing information. Its usage is valid because it is known that \n.ow-insensitive schemes re\u00ad 4 For brevity, \u00b5 operations have not been shown. 5A sound may-points-to set \nis never \u00d8 because a pointer will always point to something. ( Special pointer values, such as NULL, \nare modeled by separate targets.) So nonsingleton here means two or more elements.  tain correctness \nin a multithreaded context [30, Page 71]. The usage of the latter is also valid because synchronizations \nare, by default, treated as opaque procedure calls. They can hence affect all glob\u00adally visible memory. \nBy generating new symbols at these calls, the HSSA conversion phase conservatively models the asynchronous \nupdates that could occur in a data-race-free program. 1.2.1 Advantages Because the SR Technique sharpens \nkey data-.ow abstractions, it permits arbitrary bidirectional classical data-.ow analyses across synchronizations, \nin well-synchronized programs. As far as we know, this is a .rst. Previous work on program transformations \nfor the data-race-free model only considered what was effectively for\u00adward data-.ow through a lock synchronization, \nand backward data\u00ad.ow through an unlock synchronization [33]. This is insuf.cient, for instance, to expose \nthe equivalence of t2 and t6 in Figure 2. And in previous work for the full SC model, synchronization \nknowledge was only used to eliminate con.ict edges in the EPP IR [26, 36]. The second important advantage \nis that the technique needs no new IR constructs, no modi.cations to the semantics of exist\u00ading constructs, \nand no changes to existing phases. In fact, exist\u00ading phases are treated as black boxes. Because the \ntechnique only prunes maydef and mayuse sets, dropping a phase that implements it into a serial compiler \ns phase-pipeline will automatically ener\u00adgize downstream data-.ow phases to better optimize parallel \ncode.  1.3 Paper Overview The rest of this article is organized as follows. Section 2 lays out the \nrequisites for our work. Section 3 shows how to build an ab\u00ad straction called the Procedural Concurrency \nGraph (PCG) that en\u00adables the calculation of concurrency and interference information. Siloed references \nare computed from the PCG, using the algorithm presented in Section 4. This section also explains how \na data-.ow analysis can take advantage of the computed information. An im\u00adportant tradeoff involving \ninterference is described in Section 5. Experiments on an implementation in gcc are reported in Section \n6. Finally, Section 7 discusses related work, and Section 8 concludes. 2. Preliminaries The term object \nin this paper has the sense used in the C Standard it means any named piece of storage in the execution \nenvironment [14]. To simplify the presentation, our usage of this term will encompass procedures. Two \nobjects are distinct if either their lifetimes or address ranges are not identical. Thus, if x is a struct \nvariable with .eld f,then x and x.f are distinct objects (although the .rst contains the second). As \nin the C Standard, we use the term lvalues for syntactic expressions that refer to objects, such as x \nand x.f. Speci.c and arbitrary lvalues will be displayed, respectively, with typewriter and italicized \nfonts. 2.1 The Program s Call Universe A program comprises parts for which an IR is available, and parts \nfor which it is not we call the former user code. A procedure in\u00advoked in user code is either de.ned, \nabstracted or inscrutable.It is de.ned if its IR is known, abstracted if unde.ned but with a sum\u00admary \ncapable of describing its effects at call sites, and inscrutable if unde.ned and not abstracted. These \nclasses form the sets Fd , Fa and Finscr together, they constitute the program s call universe. If STMTS( \nf ) is the set of statements in a de.ned procedure f , then U = .f .Fd STMTS( f ) is the set of all user-code \nstatements. An s . U is either non-call code or a call.6 The function proc maps a call to the callee \nin Fd . Fa . Finscr, and non-call code to T. 6 Indirect calls are modeled as sets of direct calls to \npossible targets. This is to simplify the presentation; the implementation retains indirect calls as \nis. 2.2 Synchronizations It is posited that a subset of Fa . Finscr are synchronizing proce\u00addures, and \nthat the program utilizes them to attain data-race free\u00addom [1]. A synchronization is a synchronizing-procedure \ncall. Let Usync be the set of all synchronizations in U . Statements in a .xed subset U\u00b0 sync of Usync \nare postulated to have changeable maydef and mayuse sets. All other user-code statements, i.e., U - U\u00b0 \nsync,have .xed maydef and mayuse sets these will be referred to as the program s unaffected statements \n. There is full latitude in choosing U\u00b0 sync.Aswill be showninSec\u00adtion 5, the choice in.uences the precision \nof the PCG and depends on the PCG s intended use. For instance, if unblocking optimiza\u00adtion opportunities \nis the goal, U\u00b0 sync consists of synchronizations whose maydef and mayuse sets are to be sharpened. U\u00b0 \nsync are then the interesting synchronizations , as far as this goal is concerned.  2.3 On Lvalues An \nlvalue can refer to several objects. For example, if r is a pointer variable that is only assigned the \nreturned value of a malloc call, then the lvalue *r denotes all objects created at that allocation site. \n2.3.1 Aliasing Lvalues alias if they name overlapping objects. For instance, if p and q are pointer variables, \n*p and *q alias if p and q target overlapping objects. By this de.nition, p and q themselves do not alias \nbecause they name nonoverlapping objects in C. Hence, aliasing in this paper is not a points-to relation. \nThe predicate x ~ y is true if the lvalues x and y may alias. We use the set-aliasing operator to .nd \nthe may-aliases in the lvalue sets X and Y : () X Y = {z |.x . X, .y . Y.x ~ y . (z = x . z = y)}, (1) \nwhere u = v is true iff u and v are identical lvalues. As an example, if *p ~ y, *p ~*q, *q ~ x,then \n{*p, x} {*q, y} = {*p, y}. 2.3.2 Renaming Assumptions Without loss of generality, local variables in \nuser code are assumed to be appropriately renamed so that no two de.ned procedures declare local variables \nwith the same name. Also assumed is an SSA-based IR for de.ned procedures that takes into consideration \naliasing effects and indirect memory operations, such as HSSA [7]. For instance, HSSA renames two occurrences \nof *r to different versions if r is rede.ned between them, even if *r is not rede.ned. These two assumptions \nensure that identical lvalues, irrespective of where they occur in user code, always name the same set \nof objects.  2.3.3 Important Lvalue Sets Suppose L is the set of all lvalues that may be accessed in \nuser code. An object is either user-visible or user-invisible, depending on whether there is an lvalue \nin L that refers to it. Not all objects existent in a run are user-visible. For example, external library \nstate that cannot be referred to by any lvalue in L is user-invisible. Our analyses are underpinned by \na bunch of lvalue sets: Lh, Linscr, Ri, Wi, Ri, Wi, SYNC and SYNC. Most of them, as well as sets derived \nfrom them, contain only lvalues for user-visible ob\u00adjects. This is because the SR Technique prunes maydef \nand mayuse sets, which normally are subsets of L .7 Exceptions are the SYNC and SYNC sets, which may \ninclude virtual lvalues for covering sync objects strictly internal to unde.ned procedures. Lh: User-visible \nobjects allocated on the heap. We represent by Lh a subset of L that covers all user-visible objects \nthat may be 7 It may sometimes be more convenient to model all user-invisible objects with a single virtual \nlvalue ui, and to de.ne L as including ui.  allocated on the heap. To ascertain Lh, only unde.ned-procedure \ncalls need to be considered. It is assumed we are given a set H . L covering all user-visible heap objects \nthat may be allocated by abstracted-procedure calls. For instance, if malloc is an abstracted procedure, \nand if r is assigned its result, then *r is in H .When inscrutable-procedure calls are absent, H is a \nsafe choice for Lh. But not when they are present, since they could allocate heap objects and later expose \nthem to user code in myriad ways. Since exposure implies being accessible through some pointer-type lvalue \nin L , the following is a conservative formulation for Lh: Lh = . . H if proc(s) . Finscr.s . U , {*x \n| x . L . (type of x is T* (2) . otherwise. where T is not void)} It should be stressed that the second \ncase in Equation (2) is very conservative. For example, if none of the invoked inscrutable procedures \nhave pointer parameters or return pointers, and if none of the pointer variables in L have external linkage \n[14], then choosing just H for the second case is also safe. Linscr: User-visible objects immediately \naccessible in inscrutable procedures. Let Laddr . L be the set of all variables and pro\u00adcedures whose \naddresses are taken in user code. Let Le . L be the set of all variables and procedures that have external \nlinkage. An lvalue is immediately accessed in a procedure f if a memory operation m in f reads or writes \nit. m is then an immediate access. The following lvalue set conservatively covers user-visible objects \nthat may be immediately accessed in any inscrutable procedure: Linscr = Laddr . Le . Lh. (3) There is \npotential for greater accuracy by calculating Linscr differently for different categories of inscrutable \nprocedures. For instance, inscrutable procedures that belong to external libraries predating the user \ncode can never explicitly access lvalues in Le. Therefore, Linscr for that category can be set to just \nLaddr . Lh. The static call graph models both de.ned and unde.ned pro\u00adcedures. For an inscrutable-procedure \ncall-graph node, procedure lvalues in Linscr will cover all of its immediate successors. Ri, Wi, Ri, \nWi: User-visible objects that may be immediately ac\u00adcessed. We denote the sets of lvalues that may be \nimmediately read and immediately written at a statement s as Ri(s) and Wi(s). Determining these sets \nis simple when proc(s) .{T} . Fd. But when an unde.ned procedure g is called at s, there is the issue \nof accounting for the accesses that occur in it and procedures that it reaches in the call graph. Our \napproach is to include in Ri(s) and Wi(s) the reads and writes immediate to procedures that lie in a \ncall-graph path of unde.ned procedures starting at g.Inother words, the treatment is as if g, along with \nany unde.ned proce\u00addure that it may reach in the call graph without going through a de.ned procedure, \nwere inlined at the call site s. Thus, the effects of unde.ned-procedure calls are accounted for on the \ncaller s side. This is basically a context-sensitization it avoids spuriously con\u00ad.ating the unde.ned \nprocedure s effects over several call sites. An example is the call pthread mutex lock(l).Its Ri(s)= \n{l, *l} and Wi(s)= {*l}. (A synchronization is a read, and con\u00adservatively, also a write since the state \nof the operated-on sync object can change.) Another example is r := scanf(p, q).Its 8 Ri(s)= {p, q, *p, \nstdin, *stdin} and Wi(s)= {r, *q, *stdin}. We postulate the existence of the partial functions R.i and \nW.i that give the immediate read and immediate write sets when s is either an abstracted-procedure call \n(such as the previous scanf and 8 *q may not be written, for instance, if assignment suppression is used \n[14]. But including *q in Wi(s) is safe since these sets encode may information . pthread mutex lock \ninvocations) or not a call. These functions can be used to compute Ri(s) and Wi(s) for any s, as shown \nbelow: . . .largs(s) if proc(s) . Fd, Ri(s)= largs(s) . Linscr else if proc(s) . Finscr, (4) . . .Ri(s) \notherwise, . . .{ret(s)} if proc(s) . Fd, Wi(s)= {ret(s)}. Linscr else if proc(s) . Finscr, (5) . . .Wi(s) \notherwise. The .rst two cases in Equations (4) and (5) are when s invokes a de.ned or inscrutable procedure. \nThey use the partial functions largs and ret, which give the set of argument lvalues and the lvalue assigned \nthe returned result for an invocation s.Thatis, if s is z := f (x1, x2, ..., xk),then largs(s)= {x1, \nx2,...,xk} and ret(s)= z. Because the second case in both equations coincides with an inscrutable-procedure \ninvocation, Linscr is included in Ri(s) and Wi(s). The third case is when proc(s) . Fd . Finscr. Since \nthis corresponds to either non-call code or an abstracted-procedure call, Ri(s) and Wi(s) can be simply \nexpressed as R.i(s) and W.i(s). Lvalues that are immediately accessed at the granularity of a de.ned \nprocedure f can be computed as follows:  Ri( f )= Ri(s), Wi( f )= Wi(s). (6) s.STMTS( f ) s.STMTS( \nf ) SYNC, SYNC: Sync objects. Synchronizations communicate us\u00ading sync objects. Locks, barriers and condition \nvariables are exam\u00adples of sync objects. A partial function SYNC is posited that gives an lvalue set \ncovering the user-visible and user-invisible sync ob\u00adjects that may be immediately or transitively accessed \nat a synchro\u00adnization.9 SYNC can be used to .nd SYNC( f ),the setoflvalues for all sync objects that \nmay be accessed by a thread executing f :  SYNC( f )= SYNC(s). (7) s.UsyncnSTMTS( f ) Although user-invisible \nsync objects are not exposed to user code, they may be needed for analysis, such as when building the \nPCG. They will then be named by the virtual lvalue uisync.  2.4 Notational Conventions Names of program-wide \nsets use a calligraphic font for the main lettering e.g., Fd, FFOLLOW and Lh. Functions with a procedure \ndomain may sometimes have names that share common letters with other names e.g., Ri and Ri, Wi and Wi,and \nIi and Ii. A bar on top is then used to distinguish them. A subscript i is used when a set only consists \nof immediately accessed lvalues. An index that maps a notation to its pertinent equation and/or section \nis in the appendix. 3. Building the Procedural Concurrency Graph The PCG indicates whether a pair of \nprocedures may concurrently execute, and if so, objects accessed in one that may interfere with objects \naccessed in the other. Our approach to building the PCG is to begin with a solution that is possibly \nimprecise but assuredly correct, and to then subject it to a series of transformations, called re.nements, \nthat progressively improve its precision. 3.1 PCG De.nition A PCG is the labeled undirected graph Gp \n=(Fd , E , Ii),where Ii : E . 2L is the immediate interference function. Nodes in Fd correspond to the \nde.ned procedures in user code. An edge (a, b) . 9 An object is transitively accessed at a call site \ns if it is immediately accessed in a procedure that lies in a call-graph path from the callee at s. \n E means that the execution of a by one thread may overlap with an execution of b by a different thread \nthis is the standard MHP () (may-happen-in-parallel) relation [23]. Then, Ii(a, b)is the set of lvalues \non which a and b may immediately interfere. Since a could run in parallel with itself, Gp can have self-loops. \n3.1.1 Immediate Interference The overlapping regions of two control-.ow graph (CFG) paths P and P', executed \nby different threads, are subpaths p1 p2 in P and p3 p4 in P' where p1 through p4 are points such that \np1 abuts p3 and p2 abuts p4 in some interleaving. Given this, two procedures f ' and f are said to immediately \ninterfere on an lvalue x if there exist two immediate accesses, m of x in f and m ' of an lvalue x ' \nin f ',for which the following conditions simultaneously hold: '' C1. m and m con.ict i.e., at least \none is a write, and x aliases x . ' C2. m and m either lie in the overlapping regions of paths in f and \n' f that are executed by different threads, or there are no unaf\u00adfected statements whose maydef and mayuse \nsets prevent them from ending up in such regions due to a sequentially-sound transformation based on \ndata-.ow analysis frameworks [16]. ' From the above de.nition, f and f also immediately interfere on \nx '. Therefore, immediate interference is a special kind of con\u00ad ' .ict [34]. We say that f and f may \nimmediately interfere on x (and x ') if each of the above conditions only may hold. Condition C2 is a \ndisjunction of two clauses. The .rst covers ' the situation of an interleaving in which m and m abut, \nand are executed by different threads. The second conservatively antici\u00adpates the emergence of such situations \nafter a class of sequentially\u00adsound transformations. Recall from Section 2.2 that for an unaf\u00ad fected \nstatement, the maydef and mayuse sets are .xed. So an un\u00ad '' affected statement always prevents the movement \nof an access m '' across it if the lvalue involved in m belongs to its maydef set, or if '' m is a write \nand the lvalue involved belongs to its mayuse set. For the may-immediately-interfere case, Condition \nC2 is as\u00adsumed true unless there is evidence to the contrary. Re.nement 3 exempli.es how evidence to \nthe contrary falsi.es the condition.  3.2 The Initial PCG The initial PCG G0 p =(Fd , E0 , Ii 0) is \na complete graph with self\u00adloops, and has an Ii 0 that is set so that ()() Ii 0 (a, b)=(Ri(a) Wi(b)) \n. Wi(a) (Ri(b) .Wi(b))(8) for all (a, b) . E0,where is as de.ned by Equation (1). Equation (8) .nds \nall lvalues that may satisfy Condition C1; these approximate the may-immediate interference between a \nand b because Condition C2 can always be regarded to hold for them. Observe that accesses in de.ned procedures \ncalled from a or b () do not affect I0 (a, b).Asanexample,if b invokes c . Fd ,then i ()the immediate \naccesses in c do not affect Ii 0 (a, b). These accesses interfere with those in a only if c and a can \nexecute in parallel.10 () But then, I0 (a, c)would capture this interference. i  3.3 Iteratively Improving \na PCG s Precision A re.nement maps a PCG Gpj =(Fd, Ej, Iij ) to Gpj+1. Thus, given an initial G0 , a \nsequence G1 p, G2 p,... can be generated by successively p applying re.nements. All re.nements, by de.nition, \npossess the following two properties: Ej+1 . Ej ,and Ij+1 (e) . Iij (e) for all i e . Ej+1. Hence, the \nPCG sequence converges. A re.nement is a concurrency type if Ij+1 = Iij /Ej+1,where i F/A denotes the \nrestriction of a function F to a subset A of its 10 It is also possible for c and a to not execute in \nparallel e.g., a and b begin running together, a .nishes before b,and b invokes c after a .nishes. domain. \nIt is a purely interference type if Ej+1 = Ej . While the two types are not mutually exclusive, the identity \nre.nement is the only one that is both a concurrency type and a purely interference type. This paper \nonly explores re.nements that are one of these two types. But clearly, there could be other types of \nre.nements depending on how Ej+1 and Ij+1 are related to Ej and Iij . i 3.3.1 A Thread-Based Classi.cation \nof Procedures Several of the re.nements in this paper are formulated using a spe\u00adcial classi.cation of \na program s invoked procedures. This classi.\u00adcation, which is speci.c to a POSIX-like threading model, \ncatego\u00adrizes every invoked procedure, whether de.ned or unde.ned, into one or more of the following groups: \nstart routines , spawners, spawnees and follow routines . A procedure is a start routine if it may be \nthe entry point of a spawned thread. It is a spawnee if it may be executed by a spawned thread. It is \na spawner if it may create a thread and return with the created thread still running. It is a follow \nroutine if a thread may invoke it after that thread returns from a spawner. These categories are respectively \nrepresented by the sets FSTART , FSPAWNEE, FSPAWNER and FFOLLOW . Start routines are often easily recognizable. \nFor instance, they are targeted by the third argument of a pthread create call. Start routines, and every \nprocedure they call-reach (i.e., reach in the static call graph), are conservatively marked spawnees. \nProcedures that call-reach a spawner are conservatively also spawners. This is a recursive de.nition \nits base case is every unde.ned procedure that is a spawner, such as pthread create.A procedure does \nnot become a spawner by the mere act of spawning a thread. It is not a spawner if the created threads \nare not existent on its return. Thus, procedures that create threads and wait for them to exit before \nreturning are not spawners. A conservative set of follow routines can be obtained thus: (1) a procedure \nwhose call site lies in a control-.ow path that starts just after a spawner s call site is a follow routine;11 \nand (2) procedures call-reachable from follow routines are also follow routines.  3.3.2 Concurrency-Type \nRe.nements Re.nements 1 and 2 below are of the concurrency type. Because their only effect on the immediate \ninterference function is to restrict it to the new edge set Ej+1, they can be formally stated by just \ndescribing their effects on Ej . All of the re.nements in this paper are provably sound i.e., the new \nPCG never omits an immediate interference or happens-in-parallel event that occurs at run time. Soundness \nproofs for all the re.nements are available in a technical report [15]; the proof for Re.nement 4 is \nreproduced in this paper. Re.nement 1. Ej+1 = Ej -{(a, b) | a . FSPAWNEE . b . FSPAWNEE}. Re.nement \n2. Ej+1 = Ej -{(a, b) | a . (FSPAWNEE . FSPAWNER . FFOLLOW )}. The above re.nements go after different \nconcurrency-paring op\u00adportunities. Re.nement 1 removes an edge if the involved proce\u00ad dures can only \nbe executed by the main thread. Re.nement 2 ad\u00ad dresses opportunities in which a spawned thread may execute \nat most one of the procedures in an edge. The re.nements do not de\u00adpend on knowledge pertaining to synchronization \norders or thread termination. More concurrency-type re.nements can be devised if calls to other thread-related \nprocedures, such as pthread join and pthread cond wait, are not handled opaquely.12 11 Because the path \nstarts just after the spawner s call site, spawners them\u00ad selves are not follow routines unless they \nare called in the path. 12 If some thread-related procedure is not modeled, it should be treated opaquely, \nand not ignored, in order to ensure correctness.  0 GetArguments(); 4 CREATE(ParallelExecute,...); \n1 InitGlobalMemory(); 5 WAIT FOR END(...); 2 InitExpTables(); 6 printf(...); 3 CreateDistribution(...); \n7 PrintTimes(); Figure 3. An excerpt from the main procedure of SPLASH-2 s FMM benchmark. Calls preceding \nCREATE perform initialization tasks. Calls after WAIT FOR END are to output procedures.  3.3.3 Demonstrations \nof Re.nements 1 and 2 on Real Code Figure 3 shows a code fragment from the main procedure of SPLASH-2 \ns FMM benchmark [41]. The macros CREATE and WAIT FOR END expand to calls to pthread create and pthread \njoin. Only the main thread executes the procedures invoked on Lines 0 to 3, and on Line 7. Therefore, \nthese procedures cannot ex\u00adecute concurrently with each other. Re.nement 1 detects this since none of \nthem belong to FSPAWNEE. Indeed, Re.nement 1 also de\u00ad termines that none of them can happen in parallel \nwith themselves. The .rst argument to CREATE is a start routine. Since the pro\u00adcedures invoked on Lines \n0 to 3 are neither spawnees, spawners nor belong to FFOLLOW , Re.nement 2 discovers that none of them \ncan execute in parallel with ParallelExecute or any procedure call-reachable from ParallelExecute. This \nexample also shows the complementarity of Re.nements 1 and 2. That is, Re.nement 1 will not detect that \nParallelExecute and its call-descendants cannot run in parallel with the procedures invoked on Lines \n0 to 3. And because PrintTimes is in FFOLLOW , Re.nement 2 will not uncover that it is never concurrent \nwith itself.  3.3.4 Purely Interference-Type Re.nements As remarked in Section 3.3, purely interference-type \nre.nements only affect the immediate interference function Ii. Our .rst purely interference-type re.nement \nis based on the observation that lval\u00adues that are only accessed before spawner call sites, in procedures \nthat are only executed by the main thread, can never interfere with accesses that occur in concurrent \nprocedures if the spawner call sites are unaffected statements. Our second purely interference\u00adtype re.nement \nis based on the observation that under a certain condition, two procedures that do not synchronize on \ncommon sync objects cannot interfere in a data-race-free program. A .ow-sensitive re.nement. A statement \ns2 follows a state\u00adment s1 if s2 occurs in a control-.ow path that starts just after s1. Given a set \nof statements S' ,we use follow(S') to designate the set of all statements each of which follows some \nstatement in S' . Let spawner( f ) be the set of all statements that invoke spawn\u00aders in a de.ned procedure \nf . If the spawner call sites all belong to U - U\u00b0 sync, their MOD-REF sets (which correspond to maydef \nand mayuse sets) will ensure that any unsafe movement of shared\u00adobject accesses across them by a data-.ow \nanalysis is blocked. This presents a pruning opportunity, formalized in Re.nement 3, be\u00ad cause lvalues \nthat are not accessed after spawner call sites can never satisfy Condition C2. Notice that no edge-set \neffects are shown be\u00ad cause for purely interference-type re.nements, Ej+1 = Ej . Re.nement 3. If spawner \ncall sites are unaffected statements, then Ij+1(e)= Iij (e) -{x | x . Ri(s) . Wi(s).s . follow(spawner(a))}, \ni where e =(a,b) . Ej ,and a . FSPAWNEE . FFOLLOW . The expression follow(spawner(a)) above can be ascertained \nby .nding basic blocks reachable in the CFG from spawner invocation sites. It can also be used to calculate \nthe program s FFOLLOW .That is, suppose RTC( f ) is the set of all immediate successors of a pro\u00adcedure \nf in the re.exive transitive closure of the static call graph. ' Then, f . FFOLLOW if there exists a \nprocedure f such that f . ' RTC( f ') and a call site of f belongs to some follow(spawner(a)). A re.nement \nbased on data-race freedom. Consider two proce\u00addures a and b that may execute simultaneously. Equation \n(7) gives their SYNC sets. If none of the lvalues in SYNC(a) and SYNC(b) can alias each other, then a \nand b cannot immediately interfere with each other, provided the no-chain condition speci.ed in the statement \nof Re.nement 4 holds. a and b can make con.icting ac\u00adcesses of a data object (i.e., non-sync object) \nwhen they do not overlap during execution.13 If the con.icting accesses were to oc\u00adcur when their executions \noverlap, an interleaving exists in which the accesses are adjacent. But then, the program has a data \nrace. Re.nement 4. If the program is data-race free and (a, b) . Ej ,then () Ij+1 i (a, b)= \u00d8 if SYNC(a) \n SYNC(b)= \u00d8, and the no-chain condition holds, i.e., there are no procedures f1 f2 ... fn (n > 2) that \nsatisfy three clauses: (1) f1 = a (or b), fn = b (or a) can happen in parallel, (2) for at least one \nk, SYNCi( fk) SYNCi( fk+1)= \u00d8 and fk, fk+1 can happen in parallel, and (3) for all other k, a call of \nfk precedes a call of fk+1 in program order. (SYNCi( f ) is the set of immediately accessed sync objects \nin f .) () Soundness Proof. We show by contradiction that Ij+1 (a, b)can i be set to \u00d8. Assume a and \nb immediately interfere on x thus, by Conditions C1 and C2, there can be an execution instance in which \naccesses of x in a and b con.ict and lie in overlapping regions. Then, there must be a happens-before \nrelation hb . between an sa . STMTS(a) and an sb . STMTS(b) otherwise, the program has a data race on \nx [2]. Happens-before is the irre.exive transitive poso closure of the program order . and synchronization \norder . hb relations [2]. Without loss of generality, let sa . sb. Then, there is a chain of statements \ns1s2 ...sm such that s1 = sa, sm = sb, and either sopososk . sk+1 or sk . sk+1.If sk . sk+1, then there \nexist procedures fk and fk+1 such that SYNCi( fk) SYNCi( fk+1)= \u00d8 and fk and pofk+1 can happen in parallel. \nIf sk . sk+1,then sk and sk+1 belong to procedures fk and fk+1 that either are the same, or fk is called \nsobefore fk+1 in program order. Now, sk . sk+1 for at least one k otherwise, an interleaving can be constructed \nthat has a data race on x. Hence, there is a set of procedures that violates the no-chain condition. \nThus, a and b cannot immediately interfere on x. The no-chain condition holds if any of the three clauses \nis false. It is not checked by our current implementation, though for all our tested SPLASH-2 benchmarks, \nit holds whenever Re.nement 4 is applied. That is, some clause is false whenever SYNC(a) SYNC(b)= \u00d8 \nby the time Re.nement 4 is applied. The no-chain condition, as presented, is conservative. It can be \ntightened if other procedure-level information is used e.g., the absence of spinning synchronizations \ninside procedures.  3.3.5 Demonstrations of Re.nements 3 and 4 on Real Code The InitExpTables call \non Line 2 in Figure 3 initializes Zero and One, which are two global struct variables. Various proce\u00addures \ncall-reachable from ParallelExecute use these variables. InitExpTables is suf.ciently small that gcc \ninlines it when the -O3 switch is turned on. Because ParallelExecute and all of its call-descendants \nmay run concurrently with main, Equation (8) in\u00ad cludes Zero and One in the initial immediate interference \nsets be\u00adtween main and procedures in RTC(ParallelExecute) that ac\u00adcess these variables. Nevertheless, \nmain is neither a spawnee nor a follow routine, and statements in follow(spawner(main)) do not 13 For \ninstance, by acquiring a common lock before calling a and b.  access these variables. Hence, Re.nement \n3 removes Zero and One from all of the above immediate interference sets. We illustrate Re.nement 4 by \nconsidering InitBox and Create Boxes, two procedures call-reachable from ParallelExecute. Both write \ninto a global array called Local, so the initial immedi\u00adate interference set for the pair is nonempty. \nNow, pthread mutex lock and pthread mutex unlock are the only synchronizations performed when InitBox \nand CreateB oxes are active. InitBox performs them through the callee Locke dPrint. The locks held are \nall different, however, and InitBox and CreateBoxes satisfy the no-chain condition. (FMM invokes a barrier \nbetween CreateBoxes and InitBox, ensuring that the two cannot happen in parallel.) So Re.nement 4 reduces \nthe immediate interference between them to the empty set.  3.4 Dealing with Inscrutable Procedures \nThe description thus far of the PCG construction algorithm is ade\u00adquate for handling programs in which \nonly de.ned and abstracted procedures are called. We now show that the algorithm works even when there \nare inscrutable-procedure calls, e.g., into arbitrary third\u00adparty libraries distributed as pure binaries. \nThe issue boils down to understanding the effects of inscrutable-procedure calls on the con\u00adstruction \nof the initial PCG and on Re.nements 1 to 4. 3.4.1 Effect on Building the Initial PCG Inscrutable procedures \ndo not affect the PCG s node set Fd .From Equations (4) and (5), Ri and Wi are well de.ned in the presence \nof unde.ned-procedure calls. Therefore, from Equation (6), Ri( f ) and Wi( f ) are well de.ned for all \nf . Fd. Since Equation (8) remains operable, inscrutable procedures pose no problems to building G0 p. \n 3.4.2 Effect on Re.nements 1 to 3 But inscrutable-procedure calls may affect sets such as FSPAWNER, \nFSPAWNEE and FFOLLOW ; these, in turn, affect Re.nements 1 to 3:  FSPAWNER: In the absence of information \nto the contrary, in\u00adscrutable procedures must be regarded as spawners. Then, every inscrutable procedure \ng, and every procedure that call-reaches g, must be added to FSPAWNER.  FSTART : Suppose FSTART is initially \nthe set of start routines, obtained by ignoring all inscrutable-procedure calls. If there is even one \nsuch call, every procedure in Linscr whose function type is indicative of a start routine must be added \nto FSTART .  FSPAWNEE: FSPAWNEE is just .f .FSTART RTC( f ). ' FFOLLOW :Let FFOLLOW be initially the \nset of follow routines, ignoring all inscrutable-procedure calls. If there is a call to an inscrutable \nprocedure g, every procedure in Linscr would have ' to be included in FFOLLOW because g could invoke \nall of them after spawning a thread. FFOLLOW can then be obtained by ' applying the algorithm in Section \n3.3.1 using F FOLLOW .  3.4.3 Effect on Re.nement 4 The weak-ordering model of memory consistency prescribes \nan algorithm for safely distinguishing synchronizations in a data-race\u00adfree program [1, Page 75]. The \nidea is to mark a statement as a synchronization if treating it as a data operation could lead to a data \nrace. We assume that inscrutable-procedure calls have been appositely marked as synchronizations using \nthis algorithm. For instance, if Linscr is the empty set, then inscrutable\u00adprocedure invocations need \nnot be regarded as synchronizations. But in the situation that a call to an inscrutable procedure g should \nbe treated as a synchronization, the SYNC set of every de.ned procedure that call-reaches g will include \nthe virtual lvalue uisync. Re.nement 4 does not require a special handling of the above two situations. \nIt automatically will not apply in the latter situation since SYNC(a) SYNC(b) will then be nonempty. \n4. Enabling Optimizations on Siloed References PCGs have several applications. One is determining a class \nof ref\u00aderences (i.e., accesses) in a multithreaded program on which clas\u00adsical optimization opportunities \ncan be safely unblocked. Members of this class have the read-and write-siloed properties on certain intraprocedural \npaths. More precise maydef and mayuse sets can be obtained by leaving out these references. This section \nproves that the resulting sets always remain sound for a data-.ow analysis. 4.1 The Read-Siloed and Write-Siloed \nProperties Let P be a control-.ow path between two program points. We saythat anlvalue x is read-siloed \nin a thread h on P if once h enters P, no other thread writes an object named by x until h exits P. This \nde.nition is best understood by considering an execution interleaving, such as the one below: '''''''' \n'' ... s1 s2 s1 s3 s4 s5 s2 s6 s3 s7 s ... sn sm s ... 8 m+1 The boldface symbols s1 to sn are instructions \nexecuted by h,and form the path P. s1 and sn are also the .rst and last instructions in P. The lightface \nsymbols are instructions executed by other threads. If x is read-siloed in h on P, then any write of \nx by another thread '' would have to precede s2 or succeed sm.14 Similarly, x is said to be write-siloed \nin h on P if no other thread reads or writes x once h enters P and until it exits P. If these de.nitions \nwere true for all h, we would just say that x is read-or write-siloed on P . P would then be a read-or \nwrite-siloed path with respect to x. There are numerous points about these de.nitions. First, they do \nnot mention an occurrence of x in P.Anlvalue x can be read-siloed in h even on a stretch of code free \nof x, so long as no other thread updates x when h is at any point in this code stretch. Second, write\u00adsiloed \nis a stronger property than read-siloed. If x is write-siloed in h on P, then it is also read-siloed \nin h on P.Third, anlvalue y that is read-siloed (write-siloed) on P in all threads has the salient quality \nthat a write (read or write) of y by any thread outside P can only occur when no other thread is within \nthe con.nes of P.This trait is stronger than read-or write-siloed references within P being just data-race \nfree. It means accesses of y in P, including those involving proper synchronization, are free of cross-thread \neffects.  4.2 The Siloed-on-a-Procedure Property The siloed concept can be extended to whole procedures. \nLet stmts(P) be the set of statements in a path P. Then, an lvalue z is said to be siloed on a procedure \nf if two conditions are met: S1. z is write-siloed on every path P in f in which it may be immediately \nwritten at a statement s and is not in maydef (s ') . ' mayuse(s ') of any unaffected statement s in \nstmts(P) -{s}. S2. z is read-siloed on every path P in f in which it may be immediately read at a statement \ns and is not in maydef (s ') of ' any unaffected statement s in stmts(P) -{s}. The set SOPi( f ) consists \nof lvalues siloed on f . As our work is the .rst treatment of the siloed concept, the focus will be on \nthis simpler procedure-level variant, although working with the concept at a .ner granularity will likely \nyield more powerful results. In this paper, siloed without quali.cation means siloed on a procedure. \n14 To preclude data races, s ' 2 and s ' should not be writes of x. m  4.3 Computing Siloed Lvalues \nLet MHP( f ) be the set of neighbors of a procedure f in the pro\u00adgram s PCG. Then f s overall immediate \ninterference is () Ii( f )= Ii( f , f '). (9) f '.MHP( f ) From Equation (9), it is clear that for every \nx . Ii( f ),thereis '' some f . MHP( f ) such that f and f may immediately interfere on x. So lvalues \nin Ii( f ) may not be siloed on f . But those in Si( f )=(Ri( f ) .Wi( f )) - Ii( f ) (10) will be, as \nTheorem 1 shows. The subscript i, as usual, signi.es that only immediately accessed lvalues comprise \nthe siloed-lvalue set. Theorem 1. Si( f ) . SOPi( f ). Proof. If x . Si( f ),then x . Ii( f ) by Equation \n(10). We prove that if P isa pathin f with a possible immediate write-access m of x at a statement s,and \nx . maydef (s ') . mayuse(s ') for all unaffected ' statements s in stmts(P) -{s},then x must be write-siloed \non P. If not, there exist threads h and h' such that when h is in P in some '' execution instance, h' \nperforms an access m of an alias x of x. '' ' Now, m is immediate to some f . Fd and is in a path P' \nin f . Since none of the unaffected statements in stmts(P) -{s} de.ne or use x, a data-.ow transformation \ncould move m to any point in P. ' Then, because m and m also con.ict, Conditions C1 and C2 can () both \nhold for x. Thus, x . Ii( f , f ').SobyEquation (9), x . Ii( f ), a contradiction. Condition S2 can be \nsimilarly proved for paths that may immediately read x. Therefore, x . SOPi( f ). There is an important \ncase for Equations (9) and (10) that we highlight. If lvalues in SYNC( f ) do not alias with lvalues \nin SYNC( f ') for all f '. MHP( f ),then Ii( f ) will be \u00d8 if the program ' is given to be data-race \nfree and if f and f satisfy the no-chain condition in Re.nement 4. This is because Re.nement 4 will force \n () all Ii( f , f ')to \u00d8; Si( f ) will then equal Ri( f ) . Wi( f ).Since SOPi( f ) . Ri( f ) .Wi( f \n), we would then have Si( f )= SOPi( f ). 4.4 A More Accurate May-De.nition Set Let DU(s) be the set \nof all user-code lvalues z for which there is an intraprocedural path from a potential access of z to \nthe statement s, or from s to a potential access of z.15 From a data-.ow analysis standpoint, it is enough \nif maydef (s) includes two groups of lvalues when s is a synchronization: (1) those in L that could be \nwritten at s by a thread h executing s; and (2) those in DU(s) that could be concurrently written when \nh is executing s. The .rst group is drawn from L , and not the possibly smaller DU(s), because iso\u00adlated \nde.nitions may be used, or may kill de.nitions, in concurrent threads. But only those concurrent de.nitions \nthat may kill de.ni\u00adtions in the current thread, or that may be later killed or used in the current thread, \nneed to be factored into maydef (s) hence, DU(s) is the superset for the second group. This suggests \nthat t maydef (s)= W(s) . (DU(s) n CW(s)) . maydef (s) (11) is a more precise may-de.nition set for synchronizations, \nwhere ' W(s ')= {x | x . L . (the thread executing s may immediately or transitively write x at s ')}, \n(12) CW(s ')= {x | x . L . (an execution exists wherein when (13) a thread is at s ' , another thread \nwrites x)} ' for any statement s . 15 Interprocedural de.nition-use .ow can also be modeled, by expediently \nadding arti.cial assignments and uses to the CFG s entry and exit nodes. It is easy to calculate W(s \n') using the immediate-write sets Wi and Wi, which were de.ned in Equations (5) and (6):  \u00d8 if proc(s \n')= T, W(s ')= Wi(s ') . ') f '.RTC(proc(s ')) Wi( f otherwise. (14) The .rst case in the above coincides \nwith non-call code. The second case coincides with a call. Besides Wi(s '), it includes the Wi set of \n' every de.ned or unde.ned procedure that is call-reachable from s . CW(s ') models the parallel writes \nat s '. Lemma 1 states that for any synchronization s, DUi(s) n CW(s) and SOPi( f ) are disjoint. (Lemma \n1 is proved in [15].) DUi(s), which is a subset of DU(s), comprises lvalues z . L for which a potential \nimmediate access ' of z at a statement s reaches just before s, or is reached from just after s,by apath \nP in f in which the unaffected statements among stmts(P) -{s} do not access z. Theorem 2 uses Lemma 1 \nto prove t an upper bound on maydef (s) that is better than maydef (s). Lemma 1. For all synchronizations \ns in f , DUi(s) n CW(s) n SOPi( f )= \u00d8. Theorem 2. For all synchronizations s in f , () t maydef (s) \n- (DUi(s) n SOPi( f )). W(s) . may def (s). t Proof. Let x . maydef (s).Then x . maydef (s). By Equation \n(11), x . W(s).(DU(s)nCW(s)).If x . DU(s)nCW(s),then x . W(s). If x . DU(s) nCW(s),then x is either in \nor not in DUi(s) nCW(s). If in, then x . SOPi( f ) by Lemma 1. If not in, then x . DUi(s), since DUi(s) \n. DU(s). Either way, x . DUi(s) n SOPi( f ).So () x . maydef (s) - (DUi(s) n SOPi( f )). W(s) always \nholds. Theorem 2 can only be used to tighten the maydef sets of in\u00adteresting synchronizations, since \nthe siloed-on-a-procedure notion is stipulated on the other synchronizations (i.e., unaffected state\u00adments) \nhaving .xed maydef and mayuse sets. Thus, by using Theo\u00adrems 1 and 2, maydef (s) of an interesting synchronization \ns can be () replaced by maydef (s) - (DUi(s) n Si( f )). W(s). There is a similar result for a more accurate \nmayuse set [15].  4.5 Examples We now show how identifying siloed lvalues in some of the previ\u00adous \ncode fragments can expose optimization opportunities in them. 4.5.1 A Value-Numbering Opportunity The \ncritical section in Figure 2 is from FMM s ParallelExecute procedure. Now, InitGlobalMemory is the only \nFMM procedure that writes G Memory. As discussed in Section 3.3.3, the PCG reveals that InitGlobalMemory \ncannot happen in parallel with ParallelExecute.So ParallelExecute s overall immediate interference will \nnot contain G Memory. Equation (10) then as\u00ad certains that G Memory is siloed on ParallelExecute. Hence, \nby Theorem 2, the . assignments on Lines 3, 6 and 11 can be removed, , t '' and t ' and t ''' can be \nreplaced by G Memory. A value-numbering pass will now be able to catch the equivalence of t2 and t6. \n 4.5.2 A Copy Propagation Opportunity Assume that the left and right snippets in Figure 1 are from proce\u00ad \ndures a and b respectively (a and b could be the same). Then, there will be an edge between a and b in \nthe PCG. By Equation (9), X and Y will be in the overall immediate interference of both a and b. Hence, \nby Equation (10), both X and Y will not be in either Si(a) or Si(b). So no optimization opportunities \non X or Y get unblocked. For the second hypothetical program, from Section 1.1.2 (same left snippet, \nbut a different right snippet, one in which X is not  f a : a a: f b : b b: 1.0 LOCK(Lx) 2.0ta :=x 3.0 \nLOCK(Ly) 4.0tb :=y 1.1 LOCK(Ly) 2.1 y:=... 3.1 LOCK(Lz) 4.1 z := ... 1.2 ...y... 2.2 UNLOCK(Ly) 3.2 ...z... \n4.2 UNLOCK(Lz) 1.3 a(...) 2.3 ...ta... 3.3 b(...) 4.3 ...tb... 1.4 UNLOCK(Lx) 2.4 return 3.4 UNLOCK(Ly) \n4.4 return Figure 4. A program in which different optimization opportunities exist, depending on whether \nthe UNLOCK on Line 2.2 in procedure a is an interesting synchronization or an unaffected statement. updated), \nX will be in both Si(a) and Si(b). So a copy propagation opportunity involving X will get unblocked in \nthe left snippet. 5. A Tradeoff Involving Interference The reason for Condition C2 s second clause is \nthat data-.ow in\u00adformation conservatively killed at an interesting synchronization s, due to maydef (s) \nand mayuse(s), could later .ow through s,since maydef (s) and mayuse(s) are alterable. This could allow \nthe move\u00adment of one or both of the accesses mentioned in Section 3.1.1, into an overlapping region of \nexecution. Thus, the fewer the interesting synchronizations among Usync, the more the unaffected statements \nin user code, and so the less the chance of two procedures immedi\u00adately interfering as a result of a \ndata-.ow transformation. On the other hand, not designating a synchronization s as inter\u00adesting could \nresult in s unnecessarily killing useful data-.ow infor\u00admation. Therefore, there is a tradeoff between \nimmediate interfer\u00adence and the marking of synchronizations as interesting. As an example, spawner call \nsites can be marked as interest\u00ading synchronizations, since spawning imposes a synchronization order \n[1]. But then, an unconditional application of Re.nement 3 would not be guaranteed sound, because subsequent \nchanges to a spawner call site s maydef and mayuse sets could lift a killing ef\u00adfect, which may allow \nthe movement of an access across the site, which in turn could change a noninterfering access into an \ninter\u00adfering one. Therefore, spawner call sites must belong to U -U\u00b0 sync for Re.nement 3 to be applicable. \nAnother example is in Figure 4. The only shared data objects are x, y and z, which are always accessed \nholding the locks Lx , Ly and Lz respectively. Procedures a and b are always invoked with the respective \nlock pairs Lx , Ly and Ly , Lz held. Specimen invocations are shown on Lines 1.3 and 3.3. Now, irrespective \nof whether the UNLOCK on Line 2.2 is an unaffected statement, the read of x on Line 2.0 cannot satisfy \nCondition C2. Hence, x . Ii(a) for the perfect Ii(a). So if Line 2.2 were an interesting synchronization, \nthis would permit the SR Technique to drop x from maydef (s2.2), thus enabling the copy propagation of \nx from Line 2.0 to Line 2.3. On the other hand, marking Line 2.2 as an interesting synchro\u00adnization means \na and b may immediately interfere on y.Thisis be\u00adcause there would then be no unaffected statements that \nblock the movement of y across Line 2.2. If Line 2.2 were instead an unaf\u00adfected statement, the accesses \nof y on Lines 2.1 and 4.0 cannot sat\u00adisfy Condition C2. Then, y . Ii(b) for the perfect Ii(b). This would \nallow the SR Technique to drop y from maydef (s4.2),ifLine4.2 were an interesting synchronization. This, \nin turn, would permit the copy propagation of y from Line 4.0 to Line 4.3. 6. Experimental Results We \nhave implemented the SR Technique in revision 148810 of gcc, a pre-release of version series 4.5 of the \ncompiler. This section re\u00adports measurements demonstrating how our implementation fared on benchmarks \nfrom the SPLASH-2 suite [41]. Our test bed was a four-socket 64-bit server, in which each socket was \na 2.40GHz quad-core Intel Xeon E7330 processor having a 1066MHz front\u00adside bus. A socket has two dies, \nwith two cores per die. The per\u00adcore L1 instruction and data cache sizes were 32KB each. The per\u00addie \nL2 cache was 3MB. The system ran Redhat Enterprise Linux 5 (kernel release 2.6.18), and had 32GB of available \nmemory.     6.1 Compilation Details In all our experiments, the -O3 switch was turned on. Our prototype \noperates in gcc s whole-program compilation mode. This is turned on by the -combine and -fwhole-program \n.ags, which require all source .les to be on a single command-line. Until recently, it was the only way \nto do a whole-program interprocedural analysis (IPA) in gcc. New LTO (Link-Time Optimization) support \nin the current release series of gcc (4.5) should remove this de.ciency. 6.1.1 Design of the SR Phase \nAn IPA-based SR phase was created to implement the SR Tech\u00adnique. To evaluate, we focused on opportunities \nenabled by the technique in some of gcc s existent phases that implement funda\u00admental and commonly used \noptimizations. In particular, we quanti\u00ad.ed exposed opportunities in the following .ve arbitrarily selected \nintraprocedural phases: pass ccp (conditional constant propa\u00adgator), pass fre (full-redundancy eliminator), \npass copy prop (copy propagator), pass merge phi (phase that merges directly linked f -nodes), and pass \ndce (dead-code eliminator). When in\u00advoked from within the SR phase, these were executed in the given \norder, as part of an opts on srefs pass list. The SR phase is structured as a loop. In each iteration, \nthe SR Technique is applied once, followed by an application of opts on srefs. This SR loop is repeated \nuntil the IR no longer changes. Applying the technique once means building the PCG using the algorithm \nof Section 3, and sharpening data-.ow abstrac\u00adtions using the algorithm of Section 4. Thus, the SR phase \naims to unfetter a maximal set of opportunities in opts on srefs.16 6.1.2 Enabled and Baseline Executables \nJust prior to the SR phase, opts on srefs is repeatedly applied until the IR reaches quiescence. Hence, \nexecutables produced with and without the SR phase differ in the optimizations enabled by the SR phase. \nWe will refer to these executables as enabled and baseline respectively. It should be noted that optimization \neffects unblocked in phases downstream from the SR phase are included in the enabled executables. These \nare due to a one-time use of siloed information, unlike those unblocked in the opts on srefs phases. \n  6.2 Benchmark Details Table 1 shows the eight SPLASH-2 benchmarks used in our ex\u00adperiments: m-fmm \n(Fast Multipole Method), ocean-c (Contiguous Ocean), barnes (Barnes-Hut), wr-spl (Water-Spatial), wr-nsq \n(Water-Nsquared), lu-c (Contiguous LU), radix (Radix), and fft (FFT). The SPLASH-2 suite has a total \nof 12 application and kernel benchmarks. The remaining four did not successfully compile with the -combine/-fwhole-program \ncombination.17 m-fmm is a slightly modi.ed version of SPLASH-2 s implemen\u00adtation of the Fast Multipole \nMethod. The modi.cation was to out\u00adline two adjacent loops into their own procedure. This was done to \novercome a limitation in gcc s IRA (Integrated Register Allocator) phase, and is explained further in \nSection 6.4.1. The Problem Size column displays the inputs to our bench\u00admarks. These were always above \nthe original defaults [41], and 16 Maximal because for a different opts on srefs pass order, a different \nset of opportunities may be unfettered by the time the IR stops changing. 17 The failures seem to be \nrelated to cross-.le IPA not being a routinely used feature in gcc. This will likely change with LTO \ncoming online.  Program Problem Size Program Size Static Synchronization Statistics |Ri|max |W i|max \nLOC Files BBS |Fd | Transitive Callers Direct Callers Sync Sites Critical Sections m-fmm 1048576 particles \n4381 17 969 86 32 19 52 17 43 36 ocean-c 4098 \u00d7 4098 grid 4774 10 1670 35 4 4 37 4 129 185 barnes 1048576 \nparticles 2887 15 465 50 7 7 21 5 50 50 wr-spl 1331 molecules 2670 23 447 35 7 7 37 8 64 51 wr-nsq 4096 \nmolecules 2063 23 324 34 6 6 41 8 59 51 lu-c 6000 \u00d7 6000 matrix, 16 \u00d7 16 blocks 911 1 301 26 4 4 11 1 \n33 28 radix 335544320 integers 833 1 212 18 2 2 31 6 34 29 fft 67108864 data points 899 1 250 24 3 3 \n13 1 40 38 Table 1. The SPLASH-2 benchmarks used in our experiments. LOC means total lines of code. \nBBS is the total number of basic blocks on entry to the SR phase. Transitive Callers are de.ned procedures \nthat transitively reach a Pthreads procedure. Direct Callers are the direct invokers among them. Sync \nSites are the counts of the Pthreads call sites. |Ri|max and |Wi|max are the largest Ri and Wi set sizes. \nProgram Average |Si| after Re.nements 1 to k Reduc tions k = 1 k = 2 k = 3 k = 4 Edges Intf. m-fmm 0.53 \n1.28 1.37 4.96 813 362 ocean-c 8.05 11.95 28.41 42.67 186 113 barnes 2.51 4.81 5.05 8.89 664 71 wr-spl \n1.03 2.05 3.53 4.29 112 27 wr-nsq 0.84 1.81 3.11 3.70 109 19 lu-c 1.10 1.10 1.10 5.39 15 12 radix 0.54 \n0.54 0.54 2.33 3 2 fft 0.90 0.90 0.90 2.79 10 6 Program Propagations f-Node Merges Eliminations Lock \nPointer Equivalences Constant Copy m-fmm 48 190 7 449 17 ocean-c 146 1334 2 2040 4 barnes 4 145 2 246 \n3 wr-spl 228 265 0 621 6 wr-nsq 317 311 93 677 8 lu-c 0 200 0 273 1 radix 0 187 0 277 6 fft 1 152 2 299 \n1 Table 2. Metrics showing how the Si set precision accrues with ad\u00additional re.nements. Edges are edges \nremoved by Re.nements 1 and 2. Intf. are the Ii(e) sets pared by Re.nements 3 and 4. were chosen, to \nthe extent possible, to make the 16-thread execu\u00adtion times measurably signi.cant; 16 was the maximum \nnumber of threads used. The Program Size section states benchmark sizes in terms of total lines of source \ncode, number of source .les, and the following at the start of the SR phase: total number of basic blocks, \nand total number of de.ned procedures. Therefore, the Files col\u00adumn is indicative of the length of a \ncompilation command-line with the -combine/-fwhole-program combination. The Static Synchronization Statistics \nsection re.ects the stat\u00adic usage intensity of unde.ned synchronizing procedures. For SPLASH-2, these \nare Pthreads library calls. For instance, 37% of m-fmm s 86 de.ned procedures are Transitive Callers \nthese pro\u00adcedures eventually reach the Pthreads library in the static call graph. (By that measure, m-fmm \nis among SPLASH-2 s most Pthreads\u00adintensive programs.) Therefore, their data-.ow abstractions, such as \nMOD-REF sets, may be more conservative than necessary. Among the synchronizations, pthread mutex lock \nand pth read mutex unlock were the only ones treated as interesting. There is built-in support in gcc \nfor certain procedures, such as puts and strtol. These are de.ned procedures, and are counted in |Fd|. \nAll other external library procedures used by SPLASH-2 can be abstracted, since they have speci.cations \ne.g., Pthreads.  6.3 Static Metrics of Improvement Table 2 is a quanti.cation of the effectiveness of \nour re.nement\u00adbased approach to constructing the PCG. We measured the size of the Si set, averaged over \n|Fd|. From Equations (9) and (10), we observe that as the PCG is re.ned, the overall immediate inter\u00adference \ntends to decrease, which tends to increase the size of the siloed-lvalue set. Thus, the average |Si| \nand the precision of Si are Table 3. Static metrics on a maximal set of optimization opportu\u00adnities that \nwere enabled by the SR phase in opts on srefs. correlated. To check how the re.nements affect this precision, \nwe measured the average |Si| over multiple compilations, successively turning on the four re.nements \nin this paper. The column k = 1 shows these measurements with just Re.nement 1 turned on, the column \nk = 2 shows them with both Re.nements 1and2turned on, and so on. From these numbers, we observe that \nthe average increase in |Si| across all tested programs is 62%, 37% and 170% on moving from k = 1to k \n= 4. Re.nement 4 is the most powerful by this measure, at least for the benchmarks studied. The last \ntwo columns show absolute reductions. The reductions can also be expressed as percentages. For example, \nsince the num\u00adber of edges in G0 is |Fd|(|Fd|+1)/2, the percentage edge reduc\u00ad p tions follow from Table \n1: 22%, 30%, 52%, 18%, 4%, 2%, 3%. The measurements in Table 2 were made on the last iteration of the \nSR loop. They are therefore a conservative representation of the improvements, which tend to be higher \nin the initial iterations. Table 3 shows the total number of opportunities that were en\u00ad abled in opts \non srefs, at the end of the SR phase. An op\u00adportunity is an instance of an optimizing transformation. \nFor ex\u00adample, Constant and Copy are unblocked propagations of con\u00adstants and copies. f -Node Merges are \nunblocked opportunities in pass merge phi. Eliminations is the sum of unblocked op\u00adportunities in pass \nfre and pass dce. Since the numbers do not include opportunities unblocked in downstream phases, Table \n3 is a conservative re.ection of all the enabled opportunities. The Lock Pointer Equivalences column \ndisplays the number of uncovered lock-pointer equivalences, of the kind discussed in Section 4.5.1. Every \nsuch equivalence is indicative of a critical sec\u00ad tion. We were therefore able to detect all of the critical \nsections in the source code, except for two each in barnes and wr-spl.Al\u00adthough the missed ones in wr-spl \nwere because of an imprecision  0 2 4 6 810121416 0 2 4 6 810121416 12  0 2 4 6 810121416 0 2 4 6 810121416 \n1.45 99 1.45  88 1.45 10 110 1.45 0.495 1.4045 100 1.3875 87 1.3786 78 1.3591 90 1.325 74 1.325 0.3963 \n 69 1.3136 80 1.3071 1.2682 70 1.2625 62 1.2357 59 1.2227 60 1.2 0.2975 1.2 50 1.1773 50 1.1643 49 \n1.1375 37 1.1318 40 1.0929 39 1.0864 30 1.075 0.1988 1.075 25 1.0409 20 1.0214 30 1.0125 0.9955 \n0.95 0.1 20 0.95 0.95 0 0.95 0 0 2 4 6 810121416 0 2 4 6 8 10121416 0 2 4 6 810121416 0 2 4 6 810121416 \nThread Count Thread Count Thread Count Thread Count wr-spl m-fmm ocean-c barnes 0 2 4 6 8 1012141677 \n0 2 4 6 8101214160 2 4 6 8 101214161.45 0 2 4 6 8 10121416  1.45 391.45 17 154 1.45 6.05 1.4 69 1.4045 \n36 137 1.3944 5.5444 1.3944 62 1.3591 33 1.3389 1.35 120 5.0389 1.3389 1.3136 31 1.2833 1.3 54 4.5333 \n1.2833 103 1.2682 28 1.25 46 1.2278 4.0278 1.2278 86 1.2227 26 1.2 39 1.1722 3.5222 1.1773 231.1722 \n 68 1.15 31 1.1167 1.1318 203.0167 1.1167 51 1.1 23 1.0611 1.0864 18 1.0611 2.5111 34 1.05 15 1.0409 \n15 1.0056 2.0056 1.0056 0.9955 13 1 0.95 1.5 0 0 2 4 6 8 10121416 0.95 0.95 10 0 Thread Count 0 2 \n4 6 8 101214160.95 Thread Count 0 2 4 6 8 10121416 0 2 4 6 8 10121416 Thread Count Thread Count wr-nsq \n lu-c fft radix  Figure 5. Relative performances of the enabled and baseline executables. Each graph \ns right axis measures TB and TE , the execution times in seconds of the baseline and enabled executables \nof a benchmark at thread counts 1, 2, 4, 8 and 16. The left axis measures TB/TE . in our may-alias information, \nthe undetected ones in barnes were due to lock-pointer accesses done through volatile variables.  6.4 \nExecution-Time Improvements Figure 5 shows the execution times TB and TE of the baseline and enabled \nexecutables, and the relative performance TB/TE . Each re\u00adported time is the average of the last .ve \nof an eight-run experiment. The graphs show that improvements can sometimes be substantial, as in the \nm-fmm case, where it ranged from 41% to 19%. In all of the tested benchmarks except two, relative improve\u00adments \nof 5% or more were seen at one or more thread counts. As an example, ocean-c exhibited improvements of \n22% and 15% at two and four threads. It uses a red-black Gauss-Seidel multigrid solver; we suspect this \nbene.ts more from the enabled optimizations at those thread counts. wr-spl showed 14% and 9% improvements \nat two and 16 threads. Another example is barnes, which registered 15% and 11% improvements at four and \neight threads. The gains on fft and radix were at most 4% and 3%. Although radix has six critical sections, \nthey are all in one procedure. While m-fmm s relative performance decreases with increasing thread count, \nlu-c s increases, from 1% to 7%. For the others, there is no speci.c trend. Using simple arguments based \non serial fractions, it can be shown that even when the enabled performance is always better than the \nbaseline performance, the relative perfor\u00admance trend can be either increasing, decreasing, or .at [15]. \n The average improvements across all the tested programs at thread counts 1, 2, 4, 8, 16 were 5%, 9%, \n8%, 6%, 5% respectively. Thus, the average improvement over all threads was 6%. Since 37% of m-fmm s \nde.ned procedures ultimately touch a synchronization, it is perhaps unsurprising that the SR Technique \nbene.ts it the most. A serial compiler would normally be unduly conservative on opportunities associated \nwith those procedures. 6.4.1 Impact of Enabled Optimizations on Register Pressure Program Baseline Time \n(B) SR Technique Statistics A B Total Time No. of Applications Average Time (A) m-fmm 9.36 0.88 3 0.29 \n3% ocean-c 26.88 9.95 4 2.49 9% barnes 6.68 0.58 3 0.19 3% wr-spl 5.93 0.87 3 0.29 5% wr-nsq 5.22 2.97 \n11 0.27 5% lu-c 2.47 0.36 3 0.12 5% radix 1.93 0.56 4 0.14 7% fft 1.75 0.33 3 0.11 6% Table 4. Measurements \nof compilation times, in seconds. B is the time to produce the baseline executables. A is the average \ntime per application of the SR Technique within the SR phase. tions can negatively in.uence the quality \nof the generated code. Register allocation in gcc happens in the IRA phase, which is a fairly recent \naddition it replaced the old allocator in release series 4.4 [9], the series preceding our revision. \nIn the original FMM pro\u00ad gram, the SR phase exposed a number of common-subexpression opportunities in \nVListInteraction, its hottest procedure. Sev\u00aderal of these were between expressions involving .eld-based \narray\u00adelement accesses, in which one was deep inside a loop and the other was outside of it, with another \nloop in between. Unblocking them resulted in the extension of live ranges across entire loops; this suf.ciently \nstrained the IRA phase that improvements from the enabled optimizations were masked. We suppressed this \nregister\u00adallocation artifact by outlining the two loops into their own proce\u00addure. In the absence of \nthe SR phase, the run time of this modi.ed FMM program, i.e., m-fmm, does not perceptibly change. Its \nstatis\u00adtical execution pro.le also remains essentially the same. A secondary effect of enabling optimizations \nis that live ranges gen\u00ad  6.5 Compile-Time Measurements erally become longer. Hence, register pressure \ncan increase. Reg\u00adister allocators typically cope with register pressure by spilling to Table 4 shows \nthe times for producing the baseline versions. The memory. Because spilled code has costs, the enabling \nof optimiza- SR Technique Statistics section displays data on compiling into the enabled versions. Shown \nis the number of times the SR loop iterated before the IR reached quiescence. This is exactly the num\u00adber \nof times the SR Technique was applied for a benchmark. To\u00adtal Time is the aggregate time over all these \napplications. An av\u00aderage per-application time for the SR Technique can therefore be obtained this is \nshown in column A . We thus see that the SR Technique increases baseline compilation times by 5% on average. \n 7. Related Work There have been numerous works in the broad area of multithread\u00aded-program optimization. \nSome were discussed in Section 1. This section covers other works in the area that are relevant to our \neffort. 7.1 Concurrency Analyses Past analyses have utilized a variety of techniques for discovering \nwhether code fragments may execute in parallel. An early one, by Bristow et al., built the Interprocess \nPrecedence Graph, an abstrac\u00adtion for denoting the synchronization-imposed execution ordering among processes \n[4]. They modeled synchronization using event variables. Taylor proposed a state-based technique for \ngenerating, via simulation, an Ada program s concurrency history [37]. The resulting state space, however, \ncan be exponential in the number of tasks , i.e., groups of computations that may concurrently execute. \nA few projects calculated the MHP relation for Java [24, 20]. These were based on an abstraction called \nthe Parallel Execution Graph (PEG). A drawback with PEGs is that they combine the CFGs of individual \nthreads. Therefore, not only do they require a bound on the number of coincident threads modeled, but \nthey also potentially grow in size with this bound. Several projects tackled the concurrency-determination \nproblem by deducing complementary knowledge, such as partial execution orders and the Cannot-Happen-Together \n(CHT) relation [5, 8, 21]. Of these, the nonconcurrency analysis by Masticola and Ryder is perhaps closest \nto our re.nement-based method of constructing PCGs [21]. Their work computes the CHT relation by progressively \nimproving an approximation through a series of re.nements. But they diverge in several crucial ways from \nour work: No notion of interference. Ascertaining only the CHT relation means that interference is at \nbest either none or anything .  The assumption of single-instance tasks. Their initial CHT solution \nis Task(s) -{s} for each statement s,where Task(s) is the set of statements in the task containing s.Thisispremised \non there being at most one instance of a task at any moment. In a POSIX-like threading model, however, \nstatements reachable from a start routine may belong to multiple coincident tasks.  Re.nements speci.c \nto alternative parallel-programming mo\u00addels. Their re.nements were primarily designed for Ada s ren\u00addezvous \nsynchronization mechanism and binary semaphores.  7.2 Interference on Shared Data A past abstraction \nprobably closest to the PCG is the Concurrency Graph (CG), by Zhang et al [42]. Like the PCG, it is a \nlabeled undi\u00ad rected graph in which edges represent the MHP relation. Nodes, however, stand for critical \nsections. But more importantly, the CG has a coarse notion of interference. An edge is labeled I if con\u00ad.icting \naccesses exist between the corresponding critical sections, and labeled N otherwise. Zhang et al. used \nCGs for a purpose dif\u00adferent from in our work to assign locks to critical sections. They assumed CGs \nto be given, and manually constructed them [42]. Rodr\u00b4iguez et al. presented extensions to the Java \nModeling Lan\u00adguage for specifying the noninterference of methods [28]. They in\u00ad formally described two \nkinds of interference: internal and ex\u00adternal . Internal interference is when a concurrent thread alters \nprogram invariants between a method s entry and exit. External interference is when pre-and post-conditions \nare violated due to changes by a concurrent thread between a call and method entry, and between a method \nexit and caller resumption. Their work did not address the issue of automatically inferring interference. \nIn Hendren and Nicolau s analysis of recursive data structures, the concept of interference is synonymous \nwith con.ict [11]. Other authors have considered interference as con.ict combined with the MHP relation \n[17, 30]. In our work, this would be analogous to an edge in the PCG plus Condition C1. There is a family \nof static analyses for determining whether an object may thread-escape, e.g., [6]. An object o thread-escapes \nif it could be accessed by more than one thread. This de.nition lacks temporality. That is, even if o \nis accessed by threads in disjoint time intervals, it still escapes. Although thread-escaping is not \nthe same as interference, the idea is nonetheless complementary because threads can never interfere on \nobjects that do not thread-escape. Praun and Gross devised a static analysis for Java to determine con.icting \nobject accesses [39]. This was based on an abstraction called the Object Use Graph (OUG), which is built \nper abstract object using symbolic execution. Con.ict in their terminology has the sense of interference \nin our work. There are, however, impor\u00adtant differences. Some of them are: (1) there is no counterpart \nto Condition C2 s second clause; (2) to establish pairs of con.icting accesses, OUGs need to be processed \nfor pairs of con.icting events. 7.3 Preserving SC on Weakly Consistent Hardware There have been efforts \non mapping code written for a memory model that is presumably easy to reason about, e.g., SC, to more \nrelaxed memory models offered by the hardware [34, 22, 36]. Their solution has been to insert special \ninstructions, called memory barriers (e.g., IBM POWER3 s sync and Intel x86 s fence), so as to suppress \nthe compiler and hardware from reordering code. These efforts differ from the SR approach in a number \nof ways: Problem solved is different. Their goal is to provide the appear\u00adance of SC, even in the presence \nof data races. The SR approach is about effectively reusing classical optimizations on data-race\u00adfree \nSC code, preserving both SC and data-race freedom.  No reordering across synchronizations. Synchronization \nkno\u00adwledge is only used to remove IR con.ict edges, since doing so improves the results of the barrier-insertion \nanalysis [34].  A solution may not always be possible. There are programs, such as the IRIW example \n[3], for which inserting barriers is insuf.cient to realize SC on certain platforms.  7.4 Rule-Based \nTransformations for Data-Race-Free Models There is work on syntactic elimination and reordering rules \nthat are safe on sequences (i.e., traces) of memory-related operations in data-race-free programs [33]. \nBecause of its phrasing as trans\u00ad formation rules on sequences, it is unclear how to incorporate the \nwork into the existing phases of a serial compiler without nontrivial engineering. A more signi.cant \npoint is that the rules only consider opportunities that do not require information on cross-thread inter\u00adactions. \nIn particular, the lock/unlock-related rules only allow for the movement of code into critical sections, \nnot out of them [33].  7.5 OpenMP Program Optimization Optimizations for OpenMP programs, in general, \neither have used the EPP approach [32, 12] or have been restricted to within paral\u00ad lel constructs [38]. \nSatoh et al. used an IR called the Parallel Flow Graph to model the intra-and cross-thread .ow of information \n[32]. They developed data-.ow analyses for reaching de.nitions, mem\u00adory synchronizations and cross-loop \ndata dependences. Huang et al. observed that it is easier to optimize a high-level version of an OpenMP \nprogram than a lowered threaded version; they used an IR called the Parallel Control Flow Graph for this \npurpose [12].   7.6 Extending Sequential Optimizations to Parallel Code Praun et al. showed how classical \nSSAPRE can be changed (man\u00adually) to consume con.ict information derived from OUGs [40]. Heffner et al. \ndescribed modi.cations to three sequential object\u00adoriented optimizations in order to improve their effectiveness \nin the presence of concurrency [10]. The modi.ed versions were based on a .eld-access analysis that maps \nevery .eld .d to the duple (locks,threads),in which locks is the set of locks held on every access of \n.d, and in which threads is essentially a Boolean that in\u00addicates whether a single thread or multiple \nthreads access .d.No distinction was made between read and write accesses. Moreover, the results of the \n.eld-access analysis were consumed by the mod\u00adi.cations in ways that were speci.c to each extended optimization. \n8. Conclusions We presented an interprocedural static analysis that allows clas\u00adsical optimizations to \nbe applied on data-race-free multithreaded programs in more cases than when synchronizations are viewed \nas opaque operations. We have shown that the additional precision is useful for optimization, and produces \nsuperior performance. It may also be possible to use a version of this analysis in a static concurrency-bug \ndetection tool, where viewing synchronizations as opaque could obscure essential information. Much of \nthe precision of our analysis comes from Re.nement 4, which exploits the fact that C and C++ do not de.ne \nsemantics for data races. The coming standards for these languages will likely forbid them. As stated, \nRe.nement 4 does not apply in all cases in which it could. In particular, it does not yet account for \ndiffer\u00adences in behavior between synchronization primitives. For exam\u00adple, two procedures that both acquire \nbut do not release the same lock, and that perform no other synchronization on common sync objects, must \nhave empty interference sets, since any real interfer\u00adence would re.ect a data race. This probably has \nlittle bearing on most existing code, but a C++ analog is likely to be important for two procedures both \nreading the same C++0x atomic<T> variable. Acknowledgments We thank the anonymous referees for their \nkeen and valuable feed\u00adback on earlier drafts of this paper, which helped improve the work. References \n[1] ADVE,S. V., AND GHARACHORLOO, K. Shared Memory Consis\u00adtency Models: A Tutorial. IEEE Computer 29, \n12 (Dec. 1996), 66 76. [2] ADVE,S. V., AND HILL, M. D. Weak Ordering A New De.nition. In Proc. International \nSymposium on Computer Architecture (May 1990), pp. 2 14. [3] BOEHM,H.-J., AND ADVE, S. V. Foundations \nof the C++ Concur\u00adrency Memory Model. In Proc. Conference on Programming Lan\u00adguage Design and Implementation \n(June 2008), pp. 68 78. [4] BRISTOW,G., DREY,C.,EDWARDS,B., AND RIDDLE, W. Anoma\u00adly Detection in Concurrent \nPrograms. In Proc. International Confer\u00adence on Software Engineering (Sept. 1979), pp. 265 273. [5] CALLAHAN,D., \nAND SUBHLOK, J. Static Analysis of Low-level Synchronization. In Proc. ACM Workshop on Parallel and Distributed \nDebugging (May 1988), pp. 100 111. [6] CHOI,J.-D., GUPTA,M., SREEDHAR,V. C., AND MIDKIFF,S. P. Escape \nAnalysis for Java. In Proc. Conference on Object-Oriented Programming, Systems, Languages and Applications \n(Nov. 1999), pp. 1 19. [7] CHOW,F.,CHAN,S., LIU,S.-M., LO,R., AND STREICH,M. Ef\u00adfective Representation \nof Aliases and Indirect Memory Operations in SSA Form. In Proc. International Conference on Compiler \nConstruc\u00adtion (Apr. 1996), vol. 1060 of Lecture Notes in Computer Science, Springer, pp. 253 267. [8] \nDUESTERWALD,E., AND SOFFA, M. L. Concurrency Analysis in the Presence of Procedures Using a Data-Flow \nFramework. In Proc. Symposium on Testing, Analysis and Veri.cation (Oct. 1991), pp. 36 48. [9] GCC 4.4 \nRelease Series Changes, New Features, and Fixes. At http://gcc.gnu.org/gcc-4.4/changes.html. [10] HEFFNER,K., \nTARDITI,D., AND SMITH, M. D. Extending Object-Oriented Optimizations for Concurrent Programs. In Proc. \nInterna\u00adtional Conference on Parallel Architectures and Compilation Tech\u00adniques (Sept. 2007), pp. 119 \n129. [11] HENDREN,L. J., AND NICOLAU, A. Parallelizing Programs with Recursive Data Structures. IEEE \nTransactions on Parallel and Dis\u00adtributed Systems 1, 1 (Jan. 1990), 35 47. [12] HUANG,L., SETHURAMAN,G., \nAND CHAPMAN, B. Parallel Data Flow Analysis for OpenMP Programs. In Proc. International Work\u00adshop on \nOpenMP (June 2007), vol. 4935 of Lecture Notes in Com\u00adputer Science, Springer, pp. 138 142. [13] THE \nIEEE AND THE OPEN GROUP. IEEE Standard 1003.1, 2004. [14] C Standard ISO/IEC 9899. At http://www.open-std.org/JTC1/. \n[15] JOISHA,P. G.,SCHREIBER,R. S., BANERJEE,P., BOEHM,H.-J., AND CHAKRABARTI, D. R. A Technique for the \nEffective and Au\u00adtomatic Reuse of Classical Compiler Optimizations on Multithreaded Code. Technical Report \nHPL-2010-81R1, Hewlett-Packard Labora\u00adtories, July 2010. [16] KAM,J.B., AND ULLMAN, J. D. Monotone Data \nFlow Analysis Frameworks. Acta Informatica 7, 3 (Sept. 1977), 305 317. [17] KNOOP,J., AND STEFFEN, B. \nParallelism for Free: Ef.cient and Optimal Bitvector Analyses for Parallel Programs. ACM Transactions \non Programming Languages and Systems 18, 3 (May 1996), 268 299. [18] LAMPORT, L. How to Make a Multiprocessor \nComputer That Cor\u00adrectly Executes Multiprocess Programs. IEEE Transactions on Com\u00adputers C-28, 9 (Sept. \n1979), 690 691. [19] LEE,J., MIDKIFF,S. P., AND PADUA, D. A. Concurrent Static Single Assignment Form \nand Constant Propagation for Explicitly Parallel Programs. In Proc. International Workshop on Languages \nand Compilers for Parallel Computing (Aug. 1997), vol. 1366 of Lecture Notes in Computer Science, Springer, \npp. 114 130. [20] LI,L., AND VERBRUGGE, C. A Practical MHP Information Analysis for Concurrent Java Programs. \nIn Proc. International Workshop on Languages and Compilers for Parallel Computing (Sept. 2004), vol. \n3602 of Lecture Notes in Computer Science, Springer, pp. 194 208. [21] MASTICOLA,S. P., AND RYDER, B. \nG. Non-concurrency Analysis. In Proc. Symposium on Principles and Practices of Parallel Program\u00adming \n(May 1993), pp. 129 138. [22] MIDKIFF,S. P., AND PADUA, D. A. Issues in the Optimization of Parallel \nPrograms. In Proc. International Conference on Parallel Processing (Aug. 1990), vol. II, The Pennsylvania \nState University Press, pp. 105 113. [23] NAUMOVICH,G., AND AVRUNIN, G. S. A Conservative Data Flow Algorithm \nfor Detecting All Pairs of Statements that May Happen in Parallel. In Proc. Symposium on Foundations \nof Software Engineering (Nov. 1998), pp. 24 34. [24] NAUMOVICH,G., AVRUNIN,G. S., AND CLARKE,L. A. An \nEf.cient Algorithm for Computing MHP Information for Concurrent Java Programs. In Proc. Symposium on \nFoundations of Software Engineering (Sept. 1999), pp. 338 354. [25] NOVILLO, D. Memory SSA A Uni.ed Approach \nfor Sparsely Representing Memory Operations. In Proc. GCC Developers Summit (July 2007), pp. 97 110. \n [26] NOVILLO,D., UNRAU,R., AND SCHAEFFER, J. Concurrent SSA Form in the Presence of Mutual Exclusion. \nIn Proc. International Conference on Parallel Processing (Aug. 1998), IEEE Computer So\u00adciety Press, pp. \n356 364. [27] OPENMP ARCHITECTURE REVIEW BOARD. OpenMP Application Program Interface, version 3.0 ed., \nMay 2008. [28] RODR\u00b4IGUEZ,E., DWYER,M., FLANAGAN,C., HATCLIFF,J., LEAVENS,G. T., AND ROBBY. Extending \nJML for Modular Spec\u00adi.cation and Veri.cation of Multi-threaded Programs. In Proc. Eu\u00adropean Conference \non Object-Oriented Programming (July 2005), vol. 3586 of Lecture Notes in Computer Science, Springer, \npp. 551 576. [29] RUF, E. Effective Synchronization Removal for Java. In Proc. Conference on Programming \nLanguage Design and Implementation (June 2000), pp. 208 218. [30] RUGINA,R., AND RINARD, M. C. Pointer \nAnalysis for Structured Parallel Programs. ACM Transactions on Programming Languages and Systems 25, \n1 (Jan. 2003), 70 116. [31] SARKAR, V. Analysis and Optimization of Explicitly Parallel Pro\u00adgrams Using \nthe Parallel Program Graph Representation. In Proc. In\u00adternational Workshop on Languages and Compilers \nfor Parallel Com\u00adputing (Aug. 1997), vol. 1366 of Lecture Notes in Computer Science, Springer, pp. 94 \n113. [32] SATOH,S.,KUSANO,K., AND SATO, M. Compiler Optimization Techniques for OpenMP Programs. Scienti.c \nProgramming 9,2/3 (Aug. 2001), 131 142. [33] S.EVC.\u00b4IK,J. Program Transformations in Weak Memory Models.PhD \nthesis, University of Edinburgh, 2008. [34] SHASHA,D., AND SNIR, M. Ef.cient and Correct Execution of \nPar\u00adallel Programs that Share Memory. ACM Transactions on Program\u00adming Languages and Systems 10, 2 (Apr. \n1988), 282 312. [35] SRINIVASAN,H., HOOK,J., AND WOLFE, M. Static Single As\u00adsignment for Explicitly Parallel \nPrograms. In Proc. Symposium on Principles of Programming Languages (Jan. 1993), pp. 260 272. [36] SURA,Z., \nFANG,X., WONG,C.-L., MIDKIFF,S. P., LEE,J., AND PADUA, D. A. Compiler Techniques for High Performance \nSequen\u00adtially Consistent Java Programs. In Proc. Symposium on Principles and Practices of Parallel Programming \n(June 2005), pp. 2 13. [37] TAYLOR, R. N. A General-Purpose Algorithm for Analyzing Con\u00adcurrent Programs. \nCommunications of the ACM 26, 5 (May 1983), 362 376. [38] TIAN,X., BIK,A., GIRKAR,M., GREY,P., SAITO,H., \nAND SU, E. Intel OpenMP C++/Fortran Compiler for Hyper-Threading Tech\u00adnology: Implementation and Performance. \nIntel Technology Journal 6, 1 (Feb. 2002), 36 46. [39] VON PRAUN,C., AND GROSS, T. R. Static Con.ict \nAnalysis for Multi-Threaded Object-Oriented Programs. In Proc. Conference on Programming Language Design \nand Implementation (June 2003), pp. 338 349. [40] VON PRAUN,C., SCHNEIDER,F., AND GROSS, T. R. Load Elimina\u00adtion \nin the Presence of Side Effects, Concurrency and Precise Excep\u00adtions. In Proc. International Workshop \non Languages and Compilers for Parallel Computing (Oct. 2003), vol. 2958 of Lecture Notes in Computer \nScience, Springer, pp. 390 405. [41] WOO,S.C.,OHARA,M., TORRIE,E., SINGH,J. P., AND GUPTA, A. The SPLASH-2 \nPrograms: Characterization and Methodological Considerations. In Proc. International Symposium on Computer \nAr\u00adchitecture (June 1995), pp. 24 36. [42] ZHANG,Y., SREEDHAR,V. C., ZHU,W.,SARKAR,V., AND GAO, G. R. \nOptimized Lock Assignment and Allocation: A Method for Exploiting Concurrency among Critical Sections. \nCAPSL Technical Memo Revised 65, University of Delaware, Mar. 2007. Notation Index PROGRAM-WIDE SETS \nFa, Fd, Finscr ...................................... 2.1 FFOLLOW , FSPAWNEE, FSPAWNER, FSTART ........... \n3.3.1 H ... .... .... .... .... .... .... .... ..... .... .... ....2.3.3 L , Laddr, Le ..................................... \n2.3.3 Lh ................................... 2.3.3, Equation (2) Linscr ................................ \n2.3.3,Equation(3) U .. .... .... .... .... .... .... .... .... ..... .... .... ... 2.1 Usync, U\u00b0 sync \n.... .... .... .... .... .... .... ..... .... .... 2.2 PCG SYMBOLS E .... .... .... .... .... .... .... \n.... .... .... .... .... ...3.1 Ej .................................................. 3.3 Gp ................................................. \n3.1 Gpj ................................................. 3.3 Ii .... .... .... .... .... .... .... \n.... .... .... ..... .... .. 3.1 Ii 0 ......................................3.2,Equation(8) FUNCTIONS \nON STATEMENTS follow ... .... .... .... .... .... .... .... .... ..... .... . 3.3.4 largs ... .... ..... \n.... .... .... .... .... .... .... .... ..2.3.3 maydef , mayuse ...................................... \n1.1 t maydef ................................ 4.4, Equation (11) proc ................................................ \n2.1 ret ................................................ 2.3.3 CW ...................................4.4, \nEquation (13) DU, DUi ... .... .... .... .... .... .... .... .... .... .... . 4.4 Ri .................................... \n2.3.3, Equation (4) R.i . .... .... .... ..... .... .... .... .... .... .... .... ...2.3.3 SYNC ............................................. \n2.3.3 W .................................... 4.4,Equation(12) Wi ................................... \n2.3.3,Equation(5) W.i ................................................ 2.3.3  FUNCTIONS ON PROCEDURES \nIi ...................................... 4.3, Equation (9) MHP ............................................... \n4.3 Ri .................................... 2.3.3, Equation (6) RTC .............................................. \n3.3.4 spawner .. .... .... .... .... ..... .... .... .... .... .... 3.3.4 Si .....................................4.3, \nEquation (10) SOPi ............................................... 4.2 STMTS... .... .... .... .... .... \n.... .... .... .... .... ...2.1 SYNC ................................ 2.3.3,Equation(7) SYNCi ............................................ \n3.3.4 Wi ................................... 2.3.3,Equation(6) OTHER SYMBOLS T .................................................. \n2.1 ~ .. .... .... .... .... .... .... .... .... .... .... .... ... 2.3.1 = .. .... .... .... .... .... \n.... .... .... .... .... .... ... 2.3.1 .................................... 2.3.1, Equation (1) \u00b5 operation \n. .... .... .... .... .... .... .... .... ..... .. 1.1.2 . assignment ...................................... \n1.1.2 stmts.... .... .... .... .... .... .... .... .... .... .... ....4.2 TB, TE .............................................. \n6.4 uisync ............................................. 2.3.3   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>A large body of data-flow analyses exists for analyzing and optimizing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, for reasons of correctness. This paper presents a technique to automatically, aggressively, yet safely apply sequentially-sound data-flow transformations, <i>without change</i>, on shared-memory programs. The technique is founded on the notion of program references being \"siloed\" on certain control-flow paths. Intuitively, siloed references are free of interference from other threads within the confines of such paths. Data-flow transformations can, in general, be unblocked on siloed references.</p> <p>The solution has been implemented in a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of up to 41% are possible, with an average improvement of 6% across all the tested programs over all thread counts.</p>", "authors": [{"name": "Pramod G. Joisha", "author_profile_id": "81100489083", "affiliation": "Hewlett-Packard, Palo Alto, CA, USA", "person_id": "P2509712", "email_address": "pramod.joisha@hp.com", "orcid_id": ""}, {"name": "Robert S. Schreiber", "author_profile_id": "81406595126", "affiliation": "Hewlett-Packard, Palo Alto, CA, USA", "person_id": "P2509713", "email_address": "rob.schreiber@hp.com", "orcid_id": ""}, {"name": "Prithviraj Banerjee", "author_profile_id": "81100147290", "affiliation": "Hewlett-Packard, Palo Alto, CA, USA", "person_id": "P2509714", "email_address": "prith.banerjee@hp.com", "orcid_id": ""}, {"name": "Hans J. Boehm", "author_profile_id": "81423595101", "affiliation": "Hewlett-Packard, Palo Alto, CA, USA", "person_id": "P2509715", "email_address": "hans.boehm@hp.com", "orcid_id": ""}, {"name": "Dhruva R. Chakrabarti", "author_profile_id": "81100424533", "affiliation": "Hewlett-Packard, Palo Alto, CA, USA", "person_id": "P2509716", "email_address": "dhruva.chakrabarti@hp.com", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926457", "year": "2011", "article_id": "1926457", "conference": "POPL", "title": "A technique for the effective and automatic reuse of classical compiler optimizations on multithreaded code", "url": "http://dl.acm.org/citation.cfm?id=1926457"}