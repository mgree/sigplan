{"article_publication_date": "01-26-2011", "fulltext": "\n Resourceable, Retargetable, Modular Instruction Selection Using a Machine-Independent, Type-Based Tiling \nof Low-Level Intermediate Code Norman Ramsey Jo ao Dias Department of Computer Science, Tufts University \nDepartment of Computer Science, Tufts University nr@cs.tufts.edu dias@cs.tufts.edu Abstract We present \na novel variation on the standard technique of selecting instructions by tiling an intermediate-code \ntree. Typical compilers use a different set of tiles for every target machine. By analyzing a formal \nmodel of machine-level computation, we have developed a single set of tiles that is machine-independent \nwhile retaining the expressive power of machine code. Using this tileset, we reduce the number of tilers \nrequired from one per machine to one per archi\u00adtectural family (e.g., register architecture or stack \narchitecture). Be\u00adcause the tiler is the part of the instruction selector that is most dif\u00ad.cult to reason \nabout, our technique makes it possible to retarget an instruction selector with signi.cantly less effort \nthan standard tech\u00adniques. Retargeting effort is further reduced by applying an earlier result which \ngenerates the machine-dependent implementation of our tileset automatically from a declarative description \nof instruc\u00adtions semantics. Our design has the additional bene.t of enabling modular reasoning about \nthree aspects of code generation that are not typically separated: the semantics of the compiler s intermedi\u00adate \nrepresentation, the semantics of the target instruction set, and the techniques needed to generate good \ntarget code. Categories and Subject Descriptors D.3.4 [Processors]: Code generation; D.3.4 [Processors]: \nRetargetable compilers General Terms Algorithms, Theory 1. Introduction Since compilers were .rst written, \nresearchers have worked toward the goal of implementing N programming languages on M target machines \nwith only O(N + M) work instead of the O(N \u00d7M) work required to write a compiler for each language on \neach machine (Conway 1958; Strong et al. 1958). This goal has led to years of fruitful work on retargetable \ncompilers, and at the current state of the art, the major O(N \u00d7 M) component of a retargetable compiler \nis the instruction selector, which maps compiler-speci.c interme\u00addiate code to target-speci.c machine \ncode. We have developed a new way of selecting instructions which reduces N to the num\u00adber of compiler \ninfrastructures and reduces M to the number of architectural families supported. Our implementation supports \none compiler infrastructure and two architectural families (register ma\u00adchines and stack machines). Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 11, January \n26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. . . $10.00 \n Our compiler infrastructure is built on C--, an abstraction that encapsulates an optimizing code generator \nso it can be reused with multiple source languages and multiple target machines (Pey\u00adton Jones, Ramsey, \nand Reig 1999; Ramsey and Peyton Jones 2000). C--accommodates multiple source languages by providing \ntwo main interfaces: the C--language is a machine-independent, language-independent target language for \nfront ends; the C--run\u00adtime interface is an API which gives the run-time system access to the states \nof suspended computations. C--is not a universal intermediate language (Conway 1958) or a write-once, \nrun-anywhere intermediate language encapsulating a rigidly de.ned compiler and run-time system (Lindholm \nand Yellin 1999). Rather, C--encapsulates compilation techniques that are well understood, but expensive \nto implement. Such techniques include instruction selection, register allocation, implementation of procedure \ncalling conventions, instruction scheduling, scalar optimizations, and loop optimizations. To accomodate \na variety of source languages, the C--language is very expressive. The implementation of a code generator \nfor C-\u00adtherefore presents an unusual challenge: ensuring that the code generator accepts all valid input \nprograms is harder than usual, and the compiler writer s job of mapping the language to target\u00admachine \ncode is bigger than usual. In this paper we present a new tileset for machine-level computation, which \nhelps address both problems. Our work makes these contributions: We have developed a new variation on \na standard model of machine-level computation: register transfers. By formalizing and analyzing register \ntransfers, we have found a way of de\u00adcomposing register transfers into tiles, where a tile represents \na simple operation such as a sign-extending load or a three\u00adregister arithmetic operation (Section 5.2). \nThe decomposition is the same for every target machine and is implemented once.  By contrast with current \ntechniques, which require a compiler writer to show that tiling is correct and complete for each new \ntarget machine, our techniques require the compiler writer to show correctness and completeness just \nonce per architectural family. We argue informally that our tiling algorithm is correct and complete \nfor the family of register machines (Section 6).  We show that the syntax and type structure of register \ntransfers, which are almost trivially simple, nevertheless provide enough structure around which to design \nan instruction selector.  Finally, our design enables modular reasoning about a code generator that \nemits quality code; in particular, we decouple knowledge of the compiler s intermediate code from knowledge \nof the target machine, simplifying the task of retargeting the instruction selector to the point where \nmost of the work can be done automatically (Dias and Ramsey 2010).   ASGN ADDRLP n INDIRP ADDP LSHU \nINDIRP ADDU CNSTU 2 ADDRFP a INDIRU CNSTU 1 ADDRFP i lcc tiling for x86 lcc tiling for MIPS Our tiling \nfor register machines      Figure 1. lcc IR for n = a[i+1]; Figure 2. Multiple tilings of the tree \nfrom Figure 1 2. Background The problem we solve is the mapping from a compiler s low\u00adlevel intermediate \ncode to target machine instructions: instruction selection. For a compiler that generates code for more \nthan one target, instruction selection should be easily retargetable. 2.1 Related work: Instruction \nselection by tiling Instruction selection is usually treated as a tiling problem. For each target machine, \nthe compiler writer de.nes a set of tiles, where each tile maps a tree of the compiler s intermediate \ncode (an IR tree) to an effective address or to one or more machine instructions. Using the tiles, the \ninstruction selector covers the tree so that the root of one tile is a leaf of another tile, and so on. \nSuch a covering is called a tiling. Postorder traversal of a tiling produces a sequence of instructions \nthat implements the tree. Figure 1 shows an IR tree created by the lcc compiler (Fraser and Hanson 1995) \nfrom the C assignment n = a[i+1],where i is an unsigned integer, n is a signed integer, and a is a pointer \nto an ar\u00adray of signed integers. If you are not familiar with lcc s IR, suf.x P or U indicates the result \nis a pointer or an unsigned integer. lcc assumes that all variables are in memory, so a use of a variable \nis an INDIR node above an ADDRF (formal parameter) or ADDRL (local variable) node. A later pass may convert \nsome INDIR/ADDR com\u00adbinations to REG. The example tree also includes constants (CNST), operations (ADD, \nLSH), and an assignment (ASGN). In a typical compiler, each target machine requires its own unique set \nof tiles or tileset. Figure 2 shows tilings of the tree in Figure 1 using three different tilesets. The \nleft-hand tiling uses lcc s x86 tileset. The middle tiling uses lcc s MIPS tileset. The right-hand tiling \nuses our register-machine tileset, which works for both x86 and MIPS as well as other typical register \nmachines such as ARM and PowerPC. (Our tiling, instead of lcc s IR, shows the IR used by our Quick C--compiler: \nthe tree is the same, but the nodes are labeled differently. In particular, \u00b7 32 labels a fetch from \na variable.) Given a tileset, there is often more than one way to tile a particular tree. The way is \ncomputed by a tiling algorithm, of which there are two common families: bottom-up rewriting and maximal \nmunch. Bottom-up rewriting, often called BURS or BURG, computes an optimal tiling. A bottom-up rewriter \nuses dynamic programming, and it is usually built using a code-generator generator: the com\u00adpiler writer \nde.nes the tiles using a domain-speci.c language, and the code-generator generator uses the tiles to \nbuild a bottom-up tree matcher (Pelegr\u00b4i-Llopart and Graham 1988; Aho, Ganapathi, and Tjiang 1989; Fraser, \nHenry, and Proebsting 1992; Proebsting 1992; Fraser, Hanson, and Proebsting 1992). Maximal munch is \na greedy tiling algorithm that works by top\u00addown pattern-matching on the IR tree (Cattell 1980; Appel \n1998). The tiling may not be optimal, but the algorithm is easy to imple\u00adment by hand. Our compiler uses \nmaximal munch. 2.2 Retargeting is surprisingly complicated To the uninitiated, writing an instruction \nselector for a new ma\u00adchine seems simple: just de.ne a tile for each machine instruction. But the task \nis more complicated than it seems: A tile does not describe an instruction s semantics. A tile might \nmap a small IR tree to a machine instruction that implements the tree. But it is not safe to assume that \nthe instruction performs only the computation described by the IR tree. For example, it is common to \nhave a tile that maps an addition operation to a machine instruction that performs not only the addition \nbut also an extra assignment to a condition-code register. To use such an instruction, the compiler writer \nmust reason about whether the extra assignments are safe.  Having one tile for each machine instruction \nmay not be suf\u00ad.cient to generate code. The instruction selector must be able to tile any IR tree. Consequently, \nit is not possible to de.ne a set of tiles without reasoning about what IR trees can be cov\u00adered by the \ntiles. For example, on a 32-bit RISC machine, there is no instruction that loads a 32-bit immediate constant \ninto a register. If the tileset contains only one tile for each machine instruction, the compiler will \nnot be able to generate code for an IR tree that loads an immediate constant. The load-immediate problem \nis an instance of a general prob\u00adlem: to make sure that every IR tree can be covered, the com\u00adpiler writer \nmust de.ne tiles that map to sequences of machine instructions, not just single instructions. To discover \nthese tiles, it s not enough to think about the instruction set; the compiler writer has to think about \nthe instruction set and about the struc\u00adture of IR trees and about what IR trees can occur during com\u00adpilation. \n Having one tile for each IR node may not be suf.cient to gen\u00aderate good code. To generate ef.cient \ncode, you might need multiple tiles that can cover the same IR tree using different machine instructions. \nThe x86 tiling on the left of Figure 2 shows two examples in which an addition is tiled using some\u00adthing \nother than the obvious add instruction. The small tile im\u00adplementing ADDU(INDIRU( \u00b7\u00b7\u00b7),CNSTU 1) uses \nthe x86 load ef\u00adfective address instruction, which has a small binary encoding and which doesn t affect \ncondition codes. As another example, the largest tile, which performs a left shift, an add, and a fetch \n  (INDIRP), uses the indexed addressing mode to do the addition in the x86 addressing unit. This instruction \nnot only has a small binary encoding but also frees up an integer unit for some other computation, enhancing \ninstruction-level parallelism. These complications arise because the standard approach to in\u00adstruction \nselection, mapping IR fragments to machine instructions, inhibits modular reasoning: a compiler writer \nmust reason about in\u00adteractions among the compiler s IR, the target instruction set, and the optimizer. \nSuch reasoning requires broad expertise. Worse, for each new machine, crafting new tiles requires that \nsimilar reasoning be repeated using somewhat different but equally broad expertise.  3. A modular approach \nto instruction selection To reduce the effort required to write a code generator for a new target machine, \nwe have developed a new, modular approach to instruction selection. Our approach separates concerns about \nthe compiler from concerns about the machine and concerns about low\u00adlevel optimization. We separate these \nconcerns by refactoring the compiler in two ways: we optimize code after translating IR to machine instructions, \nand we divide tiling into separate machine\u00addependent and machine-independent components. 3.1 Related \nwork: Optimizing machine codes To separate the concerns of choosing machine instructions and gen\u00aderating \nef.cient code, we adapt the approach developed by David\u00adson and Fraser (1984). In this approach, the \ncompiler writer maps IR trees to na\u00a8ive machine code, represented using register-transfer lists, also \ncalled RTLs (Section 4). RTLs are later improved by an optimizer. The optimizer is machine-independent, \nbut it improves the program under the machine-dependent constraint that each RTL represents a single \nmachine instruction: the machine invariant.The machine invariant is enforced by a predicate called the \nrecognizer. Optimizing RTLs does not rule out other optimizations that operate on IR trees or on even \nhigher-level representations, but Davidson has shown that many optimizations which are typically performed \non IR are better performed on RTLs, where they help generate par\u00adticularly ef.cient code (Benitez and \nDavidson 1988, 1994). To ensure that the compiler emits ef.cient machine instructions, the optimizer \nincludes a machine-independent peephole optimizer. Because the peephole optimizer translates the tiler \ns na\u00a8ive machine code into ef.cient machine code, this way of organizing a compiler is sometimes called \ninstruction selection by peephole optimization, and the peephole optimizer is sometimes called the instruction \nselector. This organization is used not only in Davidson s vpo compiler but also in gcc. In Davidson \nand Fraser s original design, the mapping from low\u00adlevel IR to RTLs satisfying the machine invariant \nis done by a component called the code expander. The expander is so named because it maps a typical IR \ntree to a long sequence of simple RTLs, expanding the code. Davidson and Fraser (1984) note that code \nexpanders are simple and easy to to implement; to add a new machine, compiler writers are advised to \nclone and modify the expander for a similar machine. But since 1984, IRs, machines, and expanders have \nbecome more complex, and having an expander for each combination of source language and target machine \ncreates maintenance problems: when bugs are discovered, there is not a single point of truth about how \nto do the translation. In our adaptation of the design, the code expander is split into three parts: \n A language-dependent part maps the compiler s low-level IR to RTLs that respect only the word size and \nbyte order of the target machine not the machine invariant.  The tiler described in this paper expands \nthese RTLs to RTLs that conform to the shapes described in Section 5.2 below.  A machine-dependent implementation, \nwhich can be gener\u00adated automatically (Dias and Ramsey 2010), computes a rep\u00adresentation of each shape \nusing RTLs that represent machine instructions i.e., RTLs that respect the machine invariant. We changed \nDavidson and Fraser s original design in a modest way: we took a single component and split it into three. \nThis change, although modest, has a big impact: the number of compo\u00adnents that have to be maintained \ndrops from N \u00d7 M to N + M,and the M machine-dependent components can be generated automati\u00adcally from \ndeclarative descriptions of the target-machine semantics. A big payoff of Davidson s approach is that \nresponsibility for pro\u00adducing ef.cient machine code is isolated in the optimizer. The tiler is responsible \nnot for performance but only for mapping IR trees to simple machine instructions. The na\u00a8ivet\u00b4e of our \ntiler s output can be seen in Figure 2: compared to lcc s locally optimal, machine\u00addependent tilings, \nQuick C--uses eight tiles instead of .ve. This na\u00a8ive code is improved by the optimizer. 3.2 Modularity \nthrough machine-independent tiling of RTLs Davidson s body of work shows that we can build an excellent \ncompiler by .rst generating na\u00a8ive machine code, then improving it. But even when we don t need to generate \ngood machine code, writ\u00ading a traditional tiler still requires that we reason simultaneously about the \nsemantics of machine instructions and about the com\u00adpiler s intermediate representation. In this paper, \nwe show a novel alternative, which supports modular reasoning: Low-level IR may be translated to almost \nany well-typed RTLs. Unlike earlier compilers, which require that all RTLs satisfy the machine invariant, \nour compiler requires only that RTLs respect the byte order and word size of the target machine. This \ninnovation frees the front end from having to reason about the target instruction set.  Provided there \nis a machine-dependent recognizer, RTLs sup\u00adport machine-independent optimization. The recognizer decou\u00adples \nreasoning about optimization from reasoning about the in\u00adstruction set. Dias and Ramsey (2006) show how \nto generate a recognizer from a description of instructions semantics.  RTLs are language-neutral: no \nmatter what the source language, if the source code can be compiled to a low-level intermedi\u00adate representation, \nthat representation can easily be translated into RTLs. Reasoning about the source language is therefore \ncon.ned to the front end, which emits RTLs.  Part of our strategy of language-neutrality is to avoid \nhigh-level types. Instead, we use a type system that treats values only as bit vectors or Booleans, much \nas the hardware does (Section 4.1). This simple type system leads us to the centerpieces of our design: \na single, machine-independent tileset that can be reused for any register machine (Section 5.2), and \na tiling algorithm that reduces control-.ow graphs to that tileset (Section 6).  A compiler writer can \nreuse our tileset and tiling algorithm un\u00adchanged for any register machine. For each new target machine, \nall the compiler writer has to do is implement our machine\u00adindependent tileset using machine instructions. \nOur tiles are simple enough that this process can be automated by using a tileset generator (Dias and \nRamsey 2010).  Our tileset is almost target-neutral: while not suitable for any machine, it is suitable \nfor any machine that provides roughly interchangeable registers. (We have de.ned another tileset for \nstack machines, but it has been used only to generate code for the x86 legacy .oating-point unit.) Using \nour modular approach, it is signi.cantly easier to add a new target machine: because the tileset doesn \nt change, you don t have to design tiles, and you know our tileset will cover any IR tree. Even if you \ndon t use our tileset generator, our experience shows that it is easy to implement the tileset by hand. \nFinally, our modular approach does not affect the code ultimately generated by the compiler; like any \nother Davidson/Fraser compiler, our compiler generates high-quality code if and only if the optimizer \nused after instruction selection is of high quality. Using machine-independent intermediate forms to \nimprove mod\u00adularity is standard practice; prominent examples include the Java Virtual Machine (Lindholm \nand Yellin 1999) and LLVM (Lattner and Adve 2004). Some of these intermediate forms are very close to \nmachine instructions. Our innovation is not that the intermediate form is machine-independent or low-level; \nrather, it is that As described below, the intermediate form is derived in a prin\u00adcipled way from the \ntype system of a metalanguage used to de\u00adscribe instructions semantics (Ramsey and Davidson 1998).  \nAs described by Dias and Ramsey (2010), the intermediate form eliminates the need for the compiler writer \nto specify how each tile is implemented by machine instructions the imple\u00admentations can be discovered \nautomatically by heuristic search.  Below, we present the ideas that lead to our design: we explain \nRTLs (Section 4); we discuss their static semantics (Section 4.1); we analyze their types (Section 4.2); \nwe show how types suggest tiles (Section 5); and we present a tiling algorithm (Section 6).  4. Register-transfer \nlists and control-.ow graphs Register-transfer lists (RTLs) provide a simple but precise repre\u00adsentation \nof the effects of machine instructions. A register-transfer list denotes a function from machine states \nto machine states. A ma\u00adchine state is represented as a collection of storage spaces, each of which we \ndesignate with a lower-case letter. For example, on the popular x86 architecture, we write m for memory, \nr for general\u00adpurpose registers, f for legacy .oating-point registers, x for SSE registers (%xmmn), and \nc for control registers (including .ags and the instruction pointer). Each storage space is divided into \ncells; a cell is the smallest natural unit with which the hardware reads and writes the storage. On the \nx86, for example, the cell size is 8 bits for memory, 32 bits for general-purpose registers, and 80 bits \nfor legacy .oating-point registers. (Access to a register like AL or AH is treated as access to an internal \nslice of a full cell like EAX.) After program variables have been mapped to machine locations, RTLs are \nrepresented using the syntax shown in Figure 3. A register-transfer list is a list of guarded effects. \nEach effect represents the transfer of a value into a storage location, i.e., an assignment. The transfer \noccurs only if the guard (an expres\u00adsion) evaluates to true (T). Effects in a list take place simultane\u00adously, \nas in Dijkstra s multiple-assignment statement; an RTL represents a single change of state. For example, \nan RTL can represent a swap instruction without introducing temporaries.  A location may be a single \ncell or an aggregate of consecutive cells within a storage space. For example, four 8-bit bytes may be \naggregated to form a location holding a 32-bit word, as in $mBIG,8[addr]32, which speci.es a big-endian \nword starting at address addr. In an aggregate, byte order is explicit. Some storage spaces stand for \ntemporary registers, or tem\u00adporaries. Eventually the register allocator replaces temporaries with hardware \nregisters (Dias and Ramsey 2006, \u00a72.1).  Values are computed by expressions that have no side effects. \nAn expression may be a compile-time constant (kw, T,or F), a link-time constant L, a fetch from a location \nlw,oranapplica\u00adtion of an RTL operator .t to a list of expressions.  a Aggregation order (big-endian, \nlittle-endian, or ID) c Cell width of storage space e,g Expression (aka guard) G Control-.ow graph k \nLiteral integer or bit vector l Location L Label . RTL operator t Type w Width in bits [{}1 RTL . g . \neffect | g . effecteffect . l := we l . $spacea,c[e]w  e . kwTFLlw.t (e1,...,en)  G . eRTLG1; G2L:goto \neLT -g ' LF Figure 3. Syntax of RTLs and control-.ow graphs RTLs are themselves composed into control-.ow \ngraphs G, whose syntax is also shown in Figure 3. Straight-line code includes the empty graph e, a single \nRTL, or a sequence of graphs. Control .ow is represented using labels and branches. (We take one liberty \nwith the syntax; instead of writing L:; G, we write the clearer L: G.) For conditional branches, we use \nthe nonstandard notation LT -g ' LF , which means the same as if g then goto LT else goto LF . This notation \nmakes the inference rules in Section 6 easier to read. As part of instruction selection, each conditional \nand unconditional branch in a control-.ow graph is associated with a machine instruc\u00adtion, which is represented \nby an RTL that assigns to the program counter (PC). For example, a conditional branch is represented \nby an assignment like $r[4] > 0 . PC := L. For computational in\u00adstructions, the PC is updated implicitly, \nas formally speci.ed else\u00adwhere (Ramsey and Cifuentes 2003). A control-.ow graph can represent any code \nfrom a single state\u00adment all the way up to an entire procedure body. For example, on a 32-bit big-endian \nmachine, the C statement n = a[i+1] could be translated to a .ow graph consisting of the RTL n32 := 32 \n$mBIG,8[addt32(a32, shlt32(addt32(i32,132),232))]32 where t32 = 32 bits \u00d7 32 bits . 32 bits. In contrast \nto RTLs used in some earlier work, C-- s RTLs have a precise semantics independent of any particular \nmachine. The details are not relevant to the contribution of this paper, but you can imagine the simplest \npossible denotational semantics: A syntactic location l denotes a function from a machine state to a \nhardware location holding a bit vector. The location may be a single cell or a sequence of cells in a \nstorage space.  A syntactic expression e denotes a function from a machine state to a value, which is \neither a Boolean or a bit vector.  A syntactic effect or a syntactic RTL denotes a function from a machine \nstate to another machine state. If two effects in the same RTL mutate the same cell, the parallel composition \nof those effects denotes the constant wrong function (.). Denota\u00adtions are strict, so if a computation \ngoes wrong, it stays wrong.  An RTL operator denotes a pure function on values.  As usual, the dynamic \nsemantics describes the observable effects of a program. These effects must be preserved by the tiling \nalgorithm, just as by any other instruction selector. The algorithm, however, is inspired not by the \ndynamic semantics but by the static semantics. n A width variable w A literal width wM The width of \nan address on the target machine n bits A value that is n bits wide n loc A location containing an n-bit \nvalue bool A Boolean condition (t is a well-formed type) . ftt n . . n . . w . N w . N . ft n bits . \nft n loc . ft w bits . ft w loc .,n ft t . ft ti . ft t . ft bool . ft .n.t . ft t1 \u00d7 ... \u00d7 tn . t Figure \n4. Types and type-formation rules 4.1 Static semantics of RTLs Our RTLs are typed, but not as expressively \nas typed intermediate languages or assembly languages. Our type system tells us how wide something is \nhow many bits are in a value or a location and whether an expression computes a bit vector or a Boolean. \nOur type system is inspired by System F (Girard 1986), but it is both extended and restricted: Our system \nis extended by allowing a type to be quanti.ed over an integer. The integer is a width and tells how \nmany bits are in a value or in a location.  Our system is restricted in ways that might remind you of \nthe Hindley-Milner system (Milner 1978): The only polymorphic types are prenex-quanti.ed type schemes, \nand only RTL oper\u00adators, which are named and bound in the environment G0,may have polymorphic types. \nAt each use, a type scheme is fully in\u00adstantiated, so the type of any term in the RTL language (from \nFigure 3) is always monomorphic.  Types and type-formation rules are shown in Figure 4. Rules for typing \noperators, locations, and expressions are shown in Figure 5. The side condition ws indexes s says that \nstorage space s is in\u00addexed by an an address or register number that is ws bits wide. If s refers to \nmain memory, then wM indexes s. By giving two distinct rules for literal constants k, we emphasize a \ndesign decision: unlike many other intermediate representations, our RTLs do not distinguish signed integers, \nunsigned integers, .oating-point numbers, and pointers. In this paper, we pretend k is an integer, and \nwe show two side conditions under which an inte\u00adger k can be said to .t in w bits. In our implementation, \na compile\u00adtime constant k has an abstract type, and the side conditions are imposed by the operations \nused to create values of this type. The important part of the type system is the specialization of width-polymorphic \nRTL operators, as shown in the judgment form G0 fop . : t. In this judgment, G0 represents the collection \nof op\u00aderators de.ned in the C--language speci.cation (Ramsey, Pey\u00adton Jones, and Lindig 2005). This collection \nrepresents a union ma\u00adchine: it includes any operator we think might be used to describe a machine instruction \non a current or future target. For example, the collection includes popcnt, which returns the number \nof 1 bits in a word. This operator is native to Intel architectures; on other machines, we use algebraic \nlaws to rewrite it as a composition of other operators, much in the spirit of Warren (2003). Thinking \nabout all the machine-level operators we ever heard of and their types led us to our new tiling for code \ngeneration. f l : t f e : ws bits f e : ws bits ws indexes s ws indexes s c = w a .{BIG, LITTLE} c divides \nw f $sID,c[e]w : w loc f $sa,c[e]w : w loc  f e : t 0 = k < 2w -2w-1 = k < 2w-1 f kw : w bits f kw : \nw bits f L : wM f l : w loc G0 fop . : t1 \u00d7 ... \u00d7 tn . t f ei : ti  f lw : w bits f.t1\u00d7...\u00d7tn.t (e1,...,en) \n: t G0 fop . : t (operator . can be instantiated at type t) .. domG0 G0 fop . : .n.t G0 fop . : G0(.) \nG0 fop . : t[n . w] Figure 5. Typing rules for locations and selected expressions  4.2 Classifying RTL \noperators by type Although the C--language speci.cation lists 81 RTL operators (28 .oating-point operators \nand 53 integer and bitwise operators), these operators have only 14 distinct types (Table 6). Before \nex\u00adplaining how these types help classify RTLs (Section 5 below), we put the 14 types into just 5 groups: \n A standard value operator takes some bit vectors of reasonable width and returns a result of that width. \n(The width is typically the width of one word on the target machine.) These opera\u00adtors are summarized \nin the .rst two rows of Table 6. Binary operators include two s-complement addition and subtraction, \nsigned and unsigned division, quotient and remainder, bitwise Boolean operations, and rotations and shifts. \nUnary operators include bitwise complement, two s-complement negation, pop\u00adulation count, and .oating-point \nabsolute value and negation.  A weird value operator takes some bit vectors of reasonable width, but \nalso takes an argument or returns a result of another ( weird ) width. Such operators require special \ntreatment dur\u00ading tiling. Most .oating-point operators are weird value oper\u00adators because in addition \nto their ordinary operands, they take a 2-bit rounding mode. The other weird value operators are ex\u00adtended \nmultiplies (which double the width of their operands), nullary rounding-mode operators like round_down, \nand mul\u00adtiprecision operators such as carry, borrow, add with carry, and add with borrow.  A size-changing \noperator widens or narrows a bit vector. The integer size-changing operators are sign extension (sx), \nzero ex\u00adtension (zx), and extraction of least-signi.cant bits (lobits). Conversions between integers \nand .oating-point values, and be\u00adtween .oating-point values of different sizes (f2f), are also size-changing \noperators. Size-changing operators help formal\u00adize machine instructions, such as sign-extending loads, \nthat convert values between representations of different sizes.  A comparison operator takes two bit \nvectors and returns a Boolean. Comparison operators include the usual integer and .oating-point comparisons. \n A Boolean operator takes one or more Booleans and returns a Boolean. The Boolean operators are conjoin, \ndisjoin,and not.  These groups provide a starting point from which to classify RTLs.  5. From graphs \nto tiles via types Before explaining how groups of operators in.uence the design of our tileset, we consider \nhow to tile an arbitrary control-.ow graph using a small set of tiles. As explained in Section 6.1 below, \nour tiling algorithm reduces every control-.ow graph to a composition of graphs in which each node is \neither a single assignment, an un\u00adconditional branch, or a conditional branch on the results of a com\u00adparison \noperator. But even a single assignment or branch can con\u00adtain an arbitrarily large expression on the \nright-hand side. To tile these nodes, we use a restricted subset of graphs, where the re\u00adstrictions are \nmotivated by common properties of register machines (Section 5.1). We then build on the groups of operators \nidenti.ed above to classify the restricted graphs and to identify tiles, using a new idea we call shape \n(Section 5.2). 5.1 Restricting graphs in the tileset We would like as few graphs as possible to be tiles, \nbecause the fewer tiles there are and the simpler the tiles are, the easier it is to implement the tileset. \nBut enough graphs need to be tiles to cover any single assignment or branch. In particular, because every \nRTL operator can appear in either an assignment or a branch, we need at least one tile for every RTL \noperator. We have designed our tileset around a fundamental assumption about register machines: if the \ntarget machine can compute operator ., then the machine can apply . to arguments in registers, and it \ncan place the result in a register (or if the result is Boolean, it can use the result to determine control \n.ow). We get our tiles by restricting the graphs and RTLs from Figure 3: A graph must be a single RTL, \na label, goto L, goto t,ora conditional branch.  An RTL must have exactly one assignment,1 and its guard \nmust be the literal T.  If a graph or RTL refers to a memory location, the location s address must be \nstored in a register. (A memory location is one in which the actual location can be computed at run time \nor at link time. A register is a location that must be encoded in the instruction word at compile time.) \n The arguments of an RTL operator must be in locations, and except for size-changing operators, those \nlocations must be registers. (Because putting a weird value into a register can be expensive, we also \npermit that a weird argument may be a compile-time constant.)  Any single assignment or branch can be \ntiled using these restricted graphs. For example (leaving widths and byte order implicit), r1:= r2 +(r3 \n\u00d7 12) can be reduced to this sequence of tiles: t1:= 12 t2:= r3 \u00d7t1 r1:= r2 + t2. The reduction introduces \nfresh temporaries t1 and t2 to hold the values of subexpressions of the original RTL.  5.2 Using types \nto give shapes to tiles The restrictions above still permit the formation of more than 90 tiles. To simplify \nboth the explanation and the implementation of our tileset, we group tiles into equivalence classes we \ncall shapes. 1 We except procedure calls (not otherwise covered in this paper), which both capture and \nupdate the program counter, requiring two assignments. Type of operator Number of operators Sample .n.n \nbits \u00d7 n bits . n bits 17 add .n.n bits . n bits 5 com .n.n bits \u00d7 n bits \u00d7 1 bits . n bits 2 addc .n.n \nbits \u00d7 n bits \u00d7 1 bits . 1 bits 2 carry .n.n bits \u00d7 n bits \u00d7 2 bits . n bits 4 fadd .n.n bits \u00d7 2 bits \n. n bits 1 fsqrt .n.n bits \u00d7 n bits . 2n bits 3 fmulx .n.n bits 4 mzero 2 bits 4 round down .n,m.n bits \n. m bits 5 sx .n,m.n bits \u00d7 2 bits . m bits 3 f2f .n.n bits \u00d7 n bits . bool 24 eq bool \u00d7 bool . bool \n2 conjoin bool . bool 1 not Table 6. Types of selected RTL operators t Any temporary or hardware register \nm Any hardware memory space k Any literal integer, bit vector, or other compile-time constant L Any label \nor other link-time constant . An RTL value operator r A hardware register rm A .oating-point rounding \nmode .,r ,t A weird value, like a carry bit, in all or part of a register ? Any RTL comparison operator \nTable 7. Notational conventions for metavariables A shape is characterized by a control-.ow graph in \nwhich loca\u00adtions, literals, and operators may be represented by the metavari\u00adables in Table 7. Shapes \nfor a register machine, which are inspired both by our restrictions on graphs and RTLs and by our grouping \nof RTL operators by type, are shown in Figure 8. The graphs with these shapes constitute our tileset. \n(We have also de.ned shapes for stack machines, such as the x86 .oating-point unit; these shapes are \nsketched in Appendix A.) To simplify the presentation, we omit nonlocal control constructs such as call \nand return. We also omit some shapes that involve sign-extending or zero-extending a nar\u00adrow result . \ninstead of storing it in a narrow location like .1. A tile is a control-.ow graph that can be obtained \nby substituting for metavariables in the tile s shape: we replace the metavariable . (if any) with an \nactual RTL operator of an appropriate type, and we replace metavariables r and t by hardware registers \nor temporaries appropriate to the context in which they appear. For example, given the binop shape t \n:= .(t1,t2), we get a shift-left tile by using shl in place of . and by using temporaries that stand \nfor general\u00adpurpose registers. As another example, the store tile might require an address or integer \nregister for temporary t1 but should support any register t as the value to be stored. 5.3 Tiling through \na code-generation interface Each shape in Figure 8 is labeled with a name. The names identify functions \nin our code-generation interface, which encapsulates a machine-dependent implementation of every tile. \nOur instruction selector works as follows: The tiler expects an input control-.ow graph in which each \nnode contains a well-typed RTL. Moreover, the RTLs in an input graph may contain only trivially true \nguards. (The C-\u00adlanguage cannot express nontrivial guards.) Nontrivial Boolean  Standard and weird value \noperators t := .(t1,t2) binop (a binary ALU operation) t := .(t1) unop (a unary ALU operation) t := .(t1,t2,rm) \nbinrm (a binary .oating-point operation) t := .(t1,rm) unrm (a unary .oating-point operation or conversion) \nt := .(t1,t2,.) wrdop (a weird value operation, like add with carry) .1:= .(t1,t2,.2) wrdrop (a weird \nvalue/result operation, like carry) thi,tlo := t1 .t2 dblop (an extended multiply operation) Size-changing \noperators t := sx(m[t1]) sxload (load signed byte/halfword) t := zx(m[t1]) zxload (load unsigned byte/halfword) \nm[t1] := lobitsn(t) lostore (store byte/halfword) Data movement (most tiles transfer exactly one word) \nm[t1] := t store t := m[t1] load m[t1] := m[t2] block copy (any number of bytes) t1:= t2 move t := . \nhwget . := t hwset t := k or t := L li (load immediate constant) Control .ow (including comparison operators) \ngoto L b (branch) goto t br (branch register) LT -t1?t2 ' LF ; LF : bc (branch conditional) Figure 8. \nShapes of tiles for a register machine expressions g appear only in conditional branches LT -g ' LF . \n(If a target machine supports guarded assignments, aka predi\u00adcated instructions, the proper RTLs can \nbe found by standard optimizations after instruction selection.) The tiler covers each input graph with \ntiles. For each tile, the tiler identi.es the shape, then calls the function in the code\u00adgeneration interface \nwhich implements that shape. The tiler passes arguments which correspond to the metavariables shown in \nFigure 8; metavariables typically identify locations and an operator. The code-generation function returns \na machine\u00addependent control-.ow graph that implements the tile. This graph may contain RTLs with nontrivial \nguards. For example, to get a control-.ow graph implementing the tile r1:= r2 +t2, the tiler would call \nCG.binop ( r ,1) (\"add\",[32]) ( r ,2) ( t ,2) The binop function would return a small control-.ow graph, \nprobably containing one RTL representing an add instruction. (In the example, the [32] in (\"add\",[32]) \nis the list of widths with which the polymorphic operator add is instantiated.) The tiler produces an \noutput control-.ow graph in which each node contains a well-typed RTL that is representable by a single \ninstruction on the target machine. The code-generation interface also associates each operator with a \nmachine-dependent context, which tells the tiler what registers and temporaries may be operands or results \nof that operator.  5.4 Implementation of tilesets The functions in the code-generation interface must \nbe imple\u00admented for every target machine. Each function corresponds to a shape and must implement all \ntiles of that shape. The implementa\u00adtion of a tile may have side effects not called for in the interface, \nbut these effects must be limited to scratch registers which can t be named by any program and to compiler \ntemporaries. The imple\u00admentor of the tileset decides which registers are scratch registers. It is not \ndif.cult to implement a tileset by hand, but we have de\u00adveloped an algorithm for automatically generating \na tileset from a declarative machine description (Dias and Ramsey 2010). Al\u00adthough the problem is undecidable \nin principle, our tileset generator produces complete tilesets for the x86, PowerPC, and ARM.  6. Speci.cation \nof a machine-independent tiler Our tiler takes as input a control-.ow graph G' and returns a new graph \nG, which has the same observable effect as G', but which is composed entirely of RTLs that represent \nmachine instructions. As shown in the example tiling in Section 5.1, RTLs in G may have side effects \non locations not mentioned in G'. We therefore write the tiling transformation using the judgment form \nL G . G', which states that executing control-.ow graph G has the same effect as executing graph G', \nexcept that G may also overwrite any location in the set L . As shorthand, we say that G implements G'. \nGraph G and set L satisfy important properties: G is made from tiles; L contains all the locations assigned \nby G, which we write defs(G);and defs(G) includes defs(G'). Any locations in L that are not in defs(G') \nmust be scratch registers or compiler tempo\u00adraries, which cannot be observed by a program, so that G \nand G' have the same observable effect. These properties are established by metatheoretic reasoning. \nWe represent calls to the code-generation interface by a very similar judgment: G .L MG' says that graph \nG implements graph G',and moreover, every RTL in G is implementable by a single instruction on the target \nmachine M.The .L M relation is signi.cant for another reason: judgment G .L MG' appears only when G' \nis a tile. The tiler satis.es two properties: Soundness:If G .L G',then G is equivalent to G', modulo \nassignments to unobservable locations in L .  Machine invariant:If G .L G',then G contains only RTLs \nimplementable on the target machine.  These properties follow by inspection of the rules. Ideally the \ntiler would satisfy a third property: Completeness:If G' is composed of well-typed RTLs that use operators \nonly at the word size of the target machine,2 then there exist a G and an L such that G .L G'. Like \nmost compiler writers, we don t prove that our code generator is complete. Instead, we follow typical \nbest practices: we reason informally about the completeness of the tiler, and we supplement that reasoning \nwith regression tests. One advantage of our approach is that we have to do the reasoning just once, and \nthe results apply to any register machine. The current state of the art is that the completeness of an \ninstruction selector must be established once per target machine. 2 The restriction on widths of integer \nand bitwise operators can be relaxed to permit values smaller than a machine word (Redwine and Ramsey \n2004). PAR L1 G1 . l1:= e1 .i > 1: L1 I uses(ei) L2 G2 . l2:= e2 |\u00b7\u00b7\u00b7| ln := en L2 I{l1} L1.L2 G1;G2 \n. l1:= e1 |\u00b7\u00b7\u00b7| ln := en BREAKCYCLE L t fresh G . t := e1 | l2:= e2 |\u00b7\u00b7\u00b7| ln := en ; l1:= t L G . l1:= \ne1 |\u00b7\u00b7\u00b7| ln := en SEQ L1 L2 . G '. G ' G11 G22 L1\\defs(G ' 1) I uses(G2' ) L2 I defs(G1' ) \\ defs(G ' \n2) L1.L2 G1; G2 . G1' ;G2 ' Figure 9. Rules to eliminate parallel assignments and sequences Techniques \nfor proving completeness would represent a signi.cant advance over the state of practice. An even greater \nadvance would be to formalize the idea of architectural family and to prove com\u00adpleteness for an entire \nfamily. Both problems are beyond the scope of this paper. The rest of this section presents rules for \nthe tiling transformation. Although widths and byte order play a role in the tiling, they com\u00adplicate \nthe exposition of the algorithm, so we omit them except when they are crucial. This treatment is consistent \nwith the treat\u00adment of widths and byte order in the C--language itself: a C-\u00adcompiler infers widths when \npossible, so they are rarely needed in surface syntax. Widths are also inferred in our machine descrip\u00adtions \n(Ramsey and Davidson 1998). Similarly, byte order is .xed for each storage space and so is inferred from \nthe space. 6.1 Reducing graphs to RTLs A single assignment can be covered by tiles whose shapes are \ngiven in Figure 8. But as noted in Section 4, an RTL can contain more than one assignment: to be able \nto express the semantics of machine instructions, RTLs must be able to express parallel assignments. \nIn C--, we have chosen to make that expressive power available to front ends. Our compiler must therefore \nhandle parallel assignments. One way to handle a parallel assignment is to present it to the machine-dependent \nrecognizer; if the RTL is accepted, it satis.es the machine invariant and it need not be tiled. If the \nrecognizer does not accept a parallel assignment, we use the rules in Figure 9. Rule PAR takes a parallel \nassignment, extracts a single assignment l1:= e1, and executes it before the remaining assignments. This \ntransformation preserves semantics only if an important side con\u00addition holds: for all i > 1, the value \nof ei does not depend on the contents of l1. The rule uses the slightly stronger condition L1 I uses(ei), \nwhere writing I between two sets means they are disjoint. The stronger condition requires in addition \nthat graph G1, which computes l1:= e1, does not reuse temporaries that appear free in ei. There is an \nadditional side condition that graph G2 does not change the value of l1. Because parallel composition \nof assignments is associative and commutative, rule PAR can be applied to a parallel assignment as long \nas some location li can be changed without affecting the value of any other expression ej, j = i. Otherwise, \nall assignments have cyclic data dependencies, and we apply the BREAKCYCLE rule, which breaks a cycle \nby introducing a fresh temporary. Finally, the SEQ rule shows that graphs in a sequence are tiled in\u00addependently. \nThis rule expresses a standard code-generation tech\u00adnique: additional locations modi.ed by G1 (beyond \nwhat G ' 1 mod\u00adi.es) may not affect values computed in G2, and similarly G2 may not overwrite any observable \nlocations modi.ed by G1. 6.2 Rules for value operators To tile a single assignment of a value operator \n(Figure 10), we use the .rst group of shapes in Figure 8. The BINOP rule is representa\u00adtive; its task \nis to tile the assignment t := .(e1,e2). The .rst step is to allocate fresh temporaries to hold the values \nof e1 and e2 and to tile those RTLs recursively, producing graphs G1 and G2.The side condition L1 I uses(e2) \nensures that graph G1 does not mu\u00adtate any location on which the value of e2 depends. Then, the judg\u00adment \nG .L Mt := .(t1,t2) calls the code-generation interface to get a machine-dependent implementation of \nthe binary operator .. The result of the tiling is the sequence G1;G2;G. All of the rules in Figure 10 \nwork along similar lines, but a few are noteworthy. In rules BINRM through WRDROPZX, we use hat signs \nto indicate weird values, that is, values that aren t the same width as a register. For example, in the \nBINRM rule, expression e3 denotes an IEEE .oating-point rounding mode (RM), which is only two bits wide. \nThe hatted temporary t 3 means something a little different: t 3 stands for a standard register containing \nthe value of e3 in its least signi.cant two bits. In the weird-operator rules, one-bit carries and borrows \nare treated similarly. (Our tiler does some extra work not shown in the rules: it keeps track of whether \nthe high bits of register t 3 are zero, are copies of bit 1, or are unknown. Our tiler uses this information \nto tile sign-extension and zero-extension operations more ef.ciently.) The last noteworthy rule in Figure \n10 is the DBLOP rule, which handles extended-multiply operators. An extended multiply takes two operands \nof equal width and produces a result of twice that width. In hardware, that result is expected to be \nsplit over two reg\u00adisters, as enforced by the code-generation interface in the judgment G3 .L3 M thi,tlo \n:= .(t1,t2).InC--, however, it is not possible to express an assignment of a single result to a register \npair, so in a well-typed RTL, the assignment must be to memory. The addresses of the most and least signi.cant \nwords (hiaddr and loaddr) depend on byte order. 6.3 Rules for tiling size-changing operators To tile \na single assignment of a size-changing operator, we use the second group of shapes in Figure 8. We show \nrules for integer size\u00adchanging operators only; there are similar rules for changing the sizes of .oating-point \nvalues and for converting between integer and .oating point. Our intermediate language uses integer size\u00adchanging \noperators in two contexts: when transferring bytes (or halfwords, etc.) between registers and memory \n(Figure 11), and when transferring small values between special-purpose hardware registers and ordinary \nregisters (Figure 12). Both groups of rules use the same techniques: when transferring a value from a \nnarrow location to a wide location, the value must be sign-extended or zero-extended; when transferring \na value from a wide location to a narrow location, only the least signi.cant n bits are transferred. \nBoth groups of rules allocate fresh temporaries to hold the values of subexpressions, which are tiled \nrecursively. 6.4 Rules for data-movement tiles To tile a single, data-movement assignment (Figure 13), \nwe use the third group of shapes in Figure 8. Rules STORE and LOAD BINOP t1,t2 fresh . : .n.nbits \u00d7 \nn bits . n bits G1 L1 . t1 := e1 G2 L2 . t2 := e2 L .L1.L2 L1 I uses(e2) G L .M t := .(t1,t2) G1; G2; \nG . t := .(e1,e2) UNOP t ' fresh ' LL . : .n.n bits . n bits G . t ' := eG '.Mt := .(t ') ' L .L G; G \n'. t := .(e) BINRM t1,t2,t 3 fresh L1 . : .n.n bits \u00d7 nbits \u00d7 2 bits . n bits G1 . t1:= e1 L2 L3 L G2 \n. t2:= e2 G3 . t 3 := e 3 G .Mt := .(t1,t2,t 3) L1 I uses(e2) . uses(e 3) L2 I uses(e 3) L .L1.L2.L3 \nG1; G2;G3;G . t := .(e1,e2, e 3) UNRM t1,t 2 fresh . : .n.n bits \u00d7 2 bits . n bits L1 I uses(e 2) L1 \nL2 L G1 . t1:= e1 G2 . t 2 := e 2 G .Mt := .(t1,t 2) L .L1.L2 G1;G2;G . t := .(e1,e 2) WRDOP t1,t2,t \n3 fresh L1 . : .n.n bits \u00d7 n bits \u00d7 1 bit . n bits G1 . t1:= e1 L2 L3 L G2 . t2:= e2 G3 . t 3 := e 3 \nG .Mt := .(t1,t2,t 3) L1 I uses(e2) . uses(e 3) L2 I uses(e 3) L .L1.L2.L3 G1; G2;G3;G . t := .(e1,e2, \ne 3) WRDROPSX t1,t2,t 3 fresh . : .n.n bits \u00d7 n bits \u00d7 1 bit . 1 bit L1 L2 G1 . t1 := e1 G2 . t2 := e2 \nL3 L . t 3 := .Mt := sx(.(t1,t2,t 3)) G3 e 3 G L1 I uses(e2) . uses(e 3) L2 I uses(e 3) L .L1.L2.L3 \nG1; G2;G3;G . t := sx(.(e1,e2, e 3)) WRDROPZX t1,t2,t 3 fresh . : .n.n bits \u00d7 n bits \u00d7 1 bit . 1 bit \nL1 L2 G1 . t1 := e1 G2 . t2 := e2 L3 L . t 3 := .Mt := zx(.(t1,t2,t 3)) G3 e 3 G L1 I uses(e2) . uses(e \n3) L2 I uses(e 3) L .L1.L2 .L3 G1;G2; G3;G . t := zx(.(e1,e2,e 3)) DBLOP t1,t2,tlo,thi fresh . : .n.n \nbits \u00d7 n bits . 2n bits L1 L2 L3 G1 . t1:= e1 G2 . t2:= e2 G3 .M thi,tlo := .(t1,t2) G4 . $m[loaddr(m, \ne,n)] := tlo | $m[hiaddr(m, e,n)] := thi L1 I uses(e2) . uses(e) L2 I uses(e) L3 I uses(e) L .L1.L2.L3 \nG1; G2;G3;G . $m[e] := .(e1, e2) Figure 10. Tiling rules for standard and weird value operators SXLOAD \nt ' fresh ' LL G ' G . t ' := e . Mt := sx($m[t ']) L I{$m[e]} ' L .L G; G '. t := sx($m[e]) ZXLOAD t \n' fresh ' LL G ' G . t ' := e .Mt := zx($m[t ']) L I{$m[e]} ' L .L G; G '. t := zx($m[e]) LOSTORE L1 \nt1,t2 fresh L2 I uses(e1) G1 . t1:= e1 L2 L G2 . t2:= e2 G .M $m[t1] := lobitsn (t2) L1.L2.L G2;G1; G \n. $m[e1] := lobitsn (e2) Figure 11. Size-changing rules (registers . memory) HWGETSX HWGETZX LL G .Mt \n:= sx(r) G .Mt := zx(r) LL G . t := sx(r) G . t := zx(r) HWSET ' LL t fresh G . t := eG '. Mr := lobitsn \n(t) ' L .L G; G '. r := lobitsn (e) Figure 12. Size-changing rules (registers . small registers) transfer \nwords between registers and memory. Rule BLOCKCOPY copies data from one memory location to another, and \nit is not limited to a single word: a block copy can move any number of bytes (where a byte is the cell \nsize of the memory space). Rule MOVEREG moves a word between registers; rule LI loads a compile-time \nconstant into a register; and rule LI-LABEL loads a link-time constant into a register. Finally, rule \nMOVEEXP accounts for hardware restrictions on registers and operators. For example, if the result of \nan integer computation e is placed in a .oating\u00adpoint register t, the tiler introduces a fresh integer \ntemporary t ' to hold the result of the integer computation. Use of rule MOVEEXP reduces the burden on \nthe implementor of the tileset: although the implementation must include a tile for every RTL operator \n(see, for example, rules BINOP and UNOP), it is suf.cient that for each RTL operator, there is some set \nof registers in which the tiler can place the operands, and there is some set of registers in which the \ntiler can expect the result. It is not necessary to implement so many tiles that the result of any operator \ncan be placed into any register. Our code\u00adgeneration interface includes a mapping from each RTL operator \ninto a data structure which identi.es what kinds of registers or temporaries may be used as arguments \nand results for that operator. 6.5 Tiling control .ow To tile control .ow (Figure 14), we use the .nal \ngroup of shapes in Figure 8. As with sequential control .ow in Figure 9, the rules in Figure 14 express \nstandard code-generation techniques. For ex\u00adample, if the target of an unconditional branch is known \nstatically STORE t1,t2 fresh L2 I uses(e1) L1 L2 L G1 . t1:= e1 G2 . t2:= e2 G .M $m[t1] := t2 L1.L2.L \nG2; G1; G . $m[e1] := e2 LOAD t ' fresh ' LL G . t ' := eG '. Mt := $m[t '] L I{$m[e]} ' L .L G;G '. \nt := $m[e] BLOCKCOPY t1,t2 fresh L1 I{$m[e2]} L2 I uses(e1) .{$m[e2]} L1 L2 L G1 . t1:= e1 G2 . t2:= \ne2 G .M $m[t1] := $m[t2] L1.L2.L G2;G1; G . $m[e1] := $m[e2] MOVEREG LI LI-LABEL L LL G .Mt1:= t2 G .Mt \n:= kG .Mt := L L LL G . t1:= t2 G . t := kG . t := L MOVEEXP ' LL t ' fresh G . t ' := eG '.Mt := t ' \n' L .L G;G '. t := e Figure 13. Tiling rules for data movement BRANCHR L BRANCHL t fresh G . t := e ' \nLL G .M goto LG '. M goto t ' LL .L G . goto LG;G '. goto e COMPARE t1,t2 fresh L1 ?: .n.n bits \u00d7 n \nbits . bool G1 . t1:= e1 L2 L G2 . t2:= e2 L1 I uses(e2) G .M LT -t1?t2 ' LF L1.L2.L G1; G2; G . LT -e1? \ne2 ' LF Figure 14. Rules for tiling control .ow we use the goto L tile; otherwise we use the tiler to \nput the address into a fresh temporary t,thenuse the goto t tile, which typically corresponds to a branch \nregister instruction. The interesting rule in Figure 14 is the COMPARE rule: it applies only when the \ncondition is a comparison. But in general, a condi\u00adtion can be any Boolean expression. If we look at \nthe last block of Table 6 on page 6, we see that a Boolean expression can be pro\u00adduced not only by comparison \noperators, but also by Boolean con\u00adstants or operators. The rules in Figure 15 show how to reduce any \nconditional branch to a control-.ow graph in which every branch is either unconditional or is conditioned \non the results of a compari\u00adson. These rules formalize standard techniques used when compil- TRUE FALSE \nLL G .M goto LT G .M goto LF LL G . LT -T ' LF G . LT -F ' LF NOT L G . LF -e ' LT L G . LT -not(e) ' \nLF CONJOIN L fresh ' LL G . L -e1 ' LF G '. LT -e2 ' LF L I uses(e2) ' L .L G;L: G '. LT -conjoin(e1,e2) \n' LF DISJOIN L fresh ' LL G . LT -e1 ' LG '. LT -e2 ' LF L I uses(e2) ' L .L G; L: G '. LT -disjoin(e1, \ne2) ' LF Figure 15. Rules to eliminate Boolean operators ing Boolean expressions in a control-.ow context. \nBy considering every RTL operator whose return type is bool, we can be con.dent that these rules are \ncomplete. We have formulated the COMPARE rule in a way which glosses over the fact that a conditional-branch \nnode in a control-.ow graph must be reduced to a sequence of machine instructions. The COMPARE rule shows \nan unserialized control-.ow graph, in which the true and false labels LT and LF are treated symmetrically. \nBut the bc shape in Figure 8 does not treat the labels symmetrically; when the condition is not true, \nthe shape falls through to the successor in\u00adstruction. The judgment G .L M LT -t1?t2 ' LF actually provides \ntwo tiles, one of which is chosen when the .ow graph is serialized. The graph may be serialized either \nas LT -t1? t2 ' L ' ;L ' :goto LF , where L ' is fresh, or as LF -\u00ac(t1? t2) ' L ' ;L ' :goto LT ,where \nL ' is again fresh. Each of these serializations uses a tile of the bc shape and a tile of the b (unconditional \nbranch) shape. The basic blocks of a control-.ow graph are serialized using a reverse postorder depth\u00ad.rst \ntraversal, which in most cases allows the serializer to drop the fresh label L ' and the unconditional \nbranch.  6.6 Informal argument about completeness Our tiler addresses a different problem from the instruction \nselec\u00adtors found in most compilers. Most code-generation problems start with a known source language \nand an intermediate code tailored to that language, and all programs in the source language must be compiled. \nBut C--is not a source language: it is a low-level target language for compilers. It resembles low-level \nintermediate codes used in compilers, but it is not tailored to any particular source lan\u00adguage; rather, \nit is tailored to express today s architectural consen\u00adsus on .oating-point computation, integer computation, \nand bitwise computation on words of any width. Our code-generation prob\u00adlem starts with an unknown source \nlanguage, which compiles to a subset of C--, which we must tile to machine instructions. We do not expect \nto compile all well-typed C--programs. For ex\u00adample, a program that uses 17-bit bytes and 73-bit arithmetic \nmay be well typed, but we cannot compile it. We compile only code that respects the memory-cell size, \nbyte order,and word size of the target machine. For example, code intended to run on the SPARC should \nuse 8-bit cells (bytes) in memory, big-endian loads and stores, and 32-bit RTL operators. Code intended \nto run on a PDP-10 should use 36-bit cells (words) in memory, 36-bit loads and stores, and 36-bit RTL \noperators. With integer and bitwise operations, we are more .exible: a com\u00adpiler pass called the widener \nconverts narrow operations to oper\u00adations at the word size of the target machine. For example, the widener \ncan translate 32-bit code to 64-bit code while minimizing sign extensions and zero extensions (Redwine \nand Ramsey 2004). If a control-.ow graph G contains only well-typed RTLs which use the proper memory \nsize, byte order, and word size, and which have trivial guards, our tiler reduces G to tiles, and thence \nto machine instructions. We argue by structural induction over the graph. Sec\u00adtion 6.1 shows how sequences \nand parallel assignments are reduced to individual graphs and single assignments. Section 6.5 shows how \na control-.ow graph containing a nontrivial guard is reduced to conditional-branch tiles. The most complex \npart of the induction is over assignments. Given only value operators and data movement, our rules cover \nall the type schemes of all the C--operators. A new RTL operator can easily be added without changing \nthe tiler, pro\u00advided the operator has the same type as an existing RTL operator. In all cases, the side \nconditions on L can be satis.ed by choosing a combination of fresh temporaries and scratch registers. \nThe dif.cult part of the argument lies with size-changing operators. We believe that our tiler is complete \nfor C--programs in which values are sign-extended or zero-extended to at most the word size, and low \nbits are extracted from words, not from larger values. Un\u00adfortunately, these restrictions rule out cases \nof practical interest. For example, on the SPARC, it can be necessary to convert a double\u00adprecision .oating-point \nnumber to a 64-bit integer, then transfer the most and least signi.cant words of that integer to two \n32-bit integer registers. This case is supported by the Quick C--compiler, but we do not have a general \nalgorithm for translating size-changing operators when operands or results are larger than one word, \nso the tiling rules for this case are omitted from this paper.  7. Discussion Our main result is a \nmachine-independent tileset and tiling algo\u00adrithm that can be used with any register machine. The tileset \nand algorithm are implemented in our Quick C--compiler, as are a similar tileset and algorithm for stack \nmachines. Experimental re\u00adsults are good, although they re.ect the immaturity of Quick C-- s optimizer: \nour generated code outperforms code generated by lcc or by gcc with optimization turned off, but it is \nnot as good as code generated by gcc with optimization turned on. Details of these re\u00adsults have already \nbeen published (Dias and Ramsey 2010). Our re\u00adsults, together with Benitez and Davidson s (1994) demonstration \nthat standard scalar and loop optimizations can be implemented in a compiler very similar to Quick C--, \nindicate that our tiler could be used in a high-quality optimizing compiler. Our approach to instruction \nselection is built around two ideas: Instead of designing a new tileset for every target machine, design \na single, machine-independent tileset that is reused for every machine in a big architectural family. \n Let the design of the tileset follow from a formal model of machine-level computation (RTLs), which \nexpresses the con\u00adsensus common to the architectural family.  These ideas make it possible to reduce \nthe intellectual work re\u00adquired to add support for new target machines. The consensus common to the architectural \nfamily of register ma\u00adchines includes .at memory addressing, a preferred word size and byte order, and \ngroups of interchangeable registers; these features are expected to be found in all possible targets, \nand in those aspects our design is a classic intersection machine. But our design also supports a rich \nand extensible set of computational operators, and in that aspect our design is a classic union machine. \n We establish the correctness and completeness of the tiling algo\u00adrithm just once per architectural family, \ninstead of once per target machine. While arguing about completeness can be dif.cult, the work we do \nper machine to show that our tileset is implemented correctly and completely is trivially easy. The reduction \nin over\u00adall work represents a signi.cant advance over prior art. To illustrate the work, we conclude \nby discussing how you might create an in\u00adfrastructure, add new languages, add new target machines, and \nadd new optimizations, and how our techniques (together with those of Davidson and Fraser) will help \nyou separate concerns. To create an infrastructure, you will de.ne representations of RTLs and control-.ow \ngraphs, and you will de.ne an interface that al\u00adlows a front end to create control-.ow graphs. You will \nspecify a code-generation interface for our tileset. If you wish to generate code for the x86 legacy \n.oating-point unit or for some other stack machine, you will also specify an interface for a stack-machine \ntile\u00adset, perhaps like the one in Appendix A. You will implement the tiler. And if you choose to generate \nmachine-dependent compo\u00adnents from declarative machine descriptions (Ramsey and David\u00adson 1998), you \nwill implement a recognizer generator and a tileset generator that understand your representation of \nRTLs (Dias and Ramsey 2006, 2010). To add a new language, you will build a front end that translates \nyour language to low-level, imperative intermediate code. For ex\u00adample, if your language includes .rst-class, \nnested functions, you will write a front end that performs closure conversion, defunction\u00adalization, \nor some other translation to .rst-order code. Your pri\u00admary concern will be the semantics of your language, \nbut you will also need a thorough grasp of machine-level computation, and your front end will need to \ngenerate code that matches the word size and byte order of the target machine. You will be insulated \nfrom all other details of the target machine s instruction set. To add a new target machine, you will \nneed to implement a rec\u00adognizer and our tileset for that machine. You can do this by hand, or you can \ngenerate a recognizer and an implementation of the tile\u00adset from a declarative machine description. Your \nprimary concern will be to know what target-machine instructions are useful and what their semantics \nare. You will be insulated from details of op\u00adtimization and from the front end. If your infrastructure \ngenerates the recognizer and the implementation of our tileset, you will also be insulated from the details \nof your compiler s intermediate rep\u00adresentation and from the details of writing the recognizer and im\u00adplementing \nthe tileset and you may be able to reuse a machine description written by someone else. To add an optimization, \nyou will write a code-improving transfor\u00admation on RTLs. Most likely your primary concern will be to \nim\u00adplement a machine-independent code improvement ideally one that can improve almost any RTL and so \nwill be effective on many target machines. But you may instead be concerned with imple\u00admenting a machine-independent \ntransformation which is intended to be effective only on particular machines, for example, a vector\u00adizing \ntransformation. Either way, you will be insulated from irrele\u00advant details of the target-machine architecture: \nthe recognizer will ensure that your optimization preserves the machine invariant and is safe to run \non any machine. This plan of building an infrastructure and then adding to it incre\u00admentally, which we \nhave done with Quick C--, does not require radical departures from established ways of doing things, \nand it is not limited to a particular source language, intermediate represen\u00adtation, or target machine. \nIndeed, our design can be thought of as a modest refactoring of a Davidson/Fraser compiler. And yet, \nas with many good ideas in software design, modest changes yield signi.\u00adcant bene.ts: our design separates \nfront end from back end and IR from target machine to a degree not achieved in previous compilers. When \ncombined with automatic generation of machine-dependent components, our plan offers a uniquely cost-effective \nway of build\u00ading quality compilers.  Acknowledgments Kevin Redwine carefully scrutinized the tiling \nrules. Eddie Af\u00adtandilian helped improve the presentation. The anonymous refer\u00adees provided invaluable \nfeedback about the presentation and the work. In particular, referee A s review showed extraordinary \ndepth, insight, and attention to detail. The work was funded by a grant from Intel Corporation and by \nNSF awards 0838899 and 0311482.  References Alfred V. Aho, Mahadevan Ganapathi, and Steven W. K. Tjiang. \n1989 (October). Code generation using tree matching and dynamic program\u00adming. ACM Transactions on Programming \nLanguages and Systems,11 (4):491 516. Andrew W. Appel. 1998. Modern Compiler Implementation. Cambridge \nUniversity Press, Cambridge, UK. Available in three editions: C, Java, and ML. Manuel E. Benitez and \nJack W. Davidson. 1988 (July). A portable global optimizer and linker. Proceedings of the ACM SIGPLAN \n88 Conference on Programming Language Design and Implementation, in SIGPLAN Notices, 23(7):329 338. Manuel \nE. Benitez and Jack W. Davidson. 1994 (March). The advantages of machine-dependent global optimization. \nIn Programming Languages and System Architectures, LNCS volume 782, pages 105 124. Springer Verlag. Roderic \nG. G. Cattell. 1980 (April). Automatic derivation of code gener\u00adators from machine descriptions. ACM \nTransactions on Programming Languages and Systems, 2(2):173 190. Melvin E. Conway. 1958 (October). Proposal \nfor an UNCOL. Communi\u00adcations of the ACM, 1(10):5 8. Jack W. Davidson and Christopher W. Fraser. 1984 \n(October). Code selec\u00adtion through object code optimization. ACM Transactions on Program\u00adming Languages \nand Systems, 6(4):505 526. Jo ao Dias and Norman Ramsey. 2006 (March). Converting intermediate code to \nassembly code using declarative machine descriptions. In 15th International Conference on Compiler Construction \n(CC 2006), LNCS volume 3923, pages 217 231. Jo ao Dias and Norman Ramsey. 2010 (January). Automatically \ngenerating back ends using declarative machine descriptions. In Proceedings of the 37th ACM Symposium \non the Principles of Programming Languages, pages 403 416. Christopher W. Fraser and David R. Hanson. \n1995. A Retargetable C Compiler: Design and Implementation. Benjamin/Cummings. Christopher W. Fraser, \nDavid R. Hanson, and Todd A. Proebsting. 1992 (September). Engineering a simple, ef.cient code-generator \ngenerator. ACM Letters on Programming Languages and Systems, 1(3):213 226. Christopher W. Fraser, Robert \nR. Henry, and Todd A. Proebsting. 1992 (April). BURG fast optimal instruction selection and tree parsing. \nSIG-PLAN Notices, 27(4):68 76. Jean-Yves Girard. 1986. The System F of variable types, .fteen years later. \nTheoretical Computer Science, 45(2):159 192. Chris Lattner and Vikram Adve. 2004. LLVM: A compilation \nframework for lifelong program analysis &#38; transformation. In CGO 04: Proceed\u00adings of the International \nSymposium on Code Generation and Optimiza\u00adtion, pages 75 86. Tim Lindholm and Frank Yellin. 1999. Java \nVirtual Machine Speci.cation. Addison-Wesley, second edition. Robin Milner. 1978 (December). A theory \nof type polymorphism in programming. Journal of Computer and System Sciences, 17:348 375. Eduardo Pelegr\u00b4i-Llopart \nand Susan L. Graham. 1988 (January). Optimal code generation for expression trees: An application of \nBURS theory. In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, \npages 294 308. Simon L. Peyton Jones, Norman Ramsey, and Fermin Reig. 1999 (Septem\u00adber). C--: A portable \nassembly language that supports garbage collec\u00adtion. In International Conference on Principles and Practice \nof Declar\u00adative Programming, LNCS volume 1702, pages 1 28. Springer Verlag. Todd A. Proebsting. 1992 \n(June). Simple and ef.cient BURS table gener\u00adation. Proceedings of the ACM SIGPLAN 92 Conference on Program\u00adming \nLanguage Design and Implementation, in SIGPLAN Notices,27 (7):331 340. Norman Ramsey and Cristina Cifuentes. \n2003 (March). A transformational approach to binary translation of delayed branches. ACM Transactions \non Programming Languages and Systems, 25(2):210 224. Norman Ramsey and Jack W. Davidson. 1998 (June). \nMachine descriptions to build tools for embedded systems. In ACM SIGPLAN Workshop on Languages, Compilers, \nand Tools for Embedded Systems (LCTES 98), LNCS volume 1474, pages 172 188. Springer Verlag. Norman Ramsey, \nSimon Peyton Jones, and Christian Lindig. 2005 (Febru\u00adary). The C--language speci.cation Version 2.0 \n(CVS revision 1.128). See http://www.cminusminus.org/code.html#spec. Norman Ramsey and Simon L. Peyton \nJones. 2000 (May). A single in\u00adtermediate language that supports multiple implementations of excep\u00adtions. \nProceedings of the ACM SIGPLAN 00 Conference on Program\u00adming Language Design and Implementation, in SIGPLAN \nNotices,35 (5):285 298. Kevin Redwine and Norman Ramsey. 2004 (April). Widening integer arithmetic. In \n13th International Conference on Compiler Construction (CC 2004), LNCS volume 2985, pages 232 249. J. \nStrong, J. H. Wegstein, A. Tritter, J. Olsztyn, Owen R. Mock, and T. Steel. 1958. The problem of programming \ncommunication with changing machines: A proposed solution (Part 2). Communications of the ACM,1 (9):9 \n16. Henry S. Warren. 2003. Hacker s Delight. Addison-Wesley. A. Shapes of tiles for a stack machine These \nshapes are used in our tiler for the x86 legacy .oating-point unit. The stack pointer is st, and the \nstack grows downward. Standard and weird value operators f [st] := .( f [st]) stackop f [st + 1] := .( \nf [st], f [st + 1]) | st := st + 1 stackop f [st] := .( f [st],rm) stackop rm f [st + 1] := .( f [st], \nf [st + 1],rm) | st := st + 1 stackop rm Size-changing operators m[t1] := .( f [st]) | st := st + 1 store \npop cvt f [st - 1] := .(m[t1]) | st := st - 1 push cvt m[t1] := .( f [st],rm) | st := st + 1 store pop \ncvt rm f [st - 1] := .(m[t1],rm) | st := st - 1 push cvt rm Data movement (most tiles transfer exactly \none word) m[t1] := f [st] | st := st + 1 store pop f [st - 1] := m[t1] | st := st - 1 push f [st - 1] \n:= k | st := st - 1 pushk f [st - 1] := .(k) | st := st - 1 pushk cvt Control .ow (including comparison \noperators) LT -f [st]? f [st + 1] ' LF ; LF : bc stack  \n\t\t\t", "proc_id": "1926385", "abstract": "<p>We present a novel variation on the standard technique of selecting instructions by tiling an intermediate-code tree. Typical compilers use a different set of tiles for every target machine. By analyzing a formal model of machine-level computation, we have developed a single set of tiles that is <i>machine-independent</i> while retaining the expressive power of machine code. Using this tileset, we reduce the number of tilers required from one per machine to one per architectural family (e.g., register architecture or stack architecture). Because the tiler is the part of the instruction selector that is most difficult to reason about, our technique makes it possible to retarget an instruction selector with significantly less effort than standard techniques. Retargeting effort is further reduced by applying an earlier result which generates the machine-dependent implementation of our tileset automatically from a declarative description of instructions' semantics. Our design has the additional benefit of enabling modular reasoning about three aspects of code generation that are not typically separated: the semantics of the compiler's intermediate representation, the semantics of the target instruction set, and the techniques needed to generate <i>good</i> target code.</p>", "authors": [{"name": "Norman Ramsey", "author_profile_id": "81100300481", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P2509701", "email_address": "nr@cs.tufts.edu", "orcid_id": ""}, {"name": "Jo&#227;o Dias", "author_profile_id": "81443595159", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P2509702", "email_address": "dias@cs.tufts.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926451", "year": "2011", "article_id": "1926451", "conference": "POPL", "title": "Resourceable, retargetable, modular instruction selection using a machine-independent, type-based tiling of low-level intermediate code", "url": "http://dl.acm.org/citation.cfm?id=1926451"}