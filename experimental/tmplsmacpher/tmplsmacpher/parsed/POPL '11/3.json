{"article_publication_date": "01-26-2011", "fulltext": "\n Learning Minimal Abstractions Percy Liang Omer Tripp Mayur Naik UC Berkeley Tel-Aviv University Intel \nLabs Berkeley pliang@cs.berkeley.edu omertrip@post.tau.ac.il mayur.naik@intel.com Abstract Static analyses \nare generally parametrized by an abstraction which is chosen from a family of abstractions. We are inter\u00adested \nin .exible families of abstractions with many param\u00adeters, as these families can allow one to increase \nprecision in ways tailored to the client without sacri.cing scalability. For example, we consider k-limited \npoints-to analyses where each call site and allocation site in a program can have a dif\u00adferent k value. \nWe then ask a natural question in this paper: What is the minimal (coarsest) abstraction in a given family \nwhich is able to prove a set of client queries? In addressing this question, we make the following two \ncontributions: (i) we introduce two machine learning algorithms for ef.ciently .nding a minimal abstraction; \nand (ii) for a static race detec\u00adtor backed by a k-limited points-to analysis, we show empir\u00adically that \nminimal abstractions are actually quite coarse: it suf.ces to provide context/object sensitivity to a \nvery small fraction (0.4 2.3%) of the sites to yield equally precise re\u00adsults as providing context/object \nsensitivity uniformly to all sites. Categories and Subject Descriptors D.2.4 [Software En\u00adgineering]: \nSoftware/Program Veri.cation General Terms Measurement, Experimentation, Veri.ca\u00adtion Keywords heap abstractions, \nstatic analysis, concurrency, machine learning, randomization 1. Introduction Static analyses typically \nhave parameters that control the tradeoff between precision and scalability. For example, in a k-CFA-based \nor k-object-sensitivity-based points-to analy\u00adsis [10 13, 20, 26], the parameter is the k value, which \ndeter\u00admines the amount of context sensitivity and object sensitiv\u00adity. Increasing k yields more precise \npoints-to information, Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 11, January 26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. \n. . $10.00 but the complexity of the analysis also grows exponentially with k. Shape analysis [19] and \nmodel checkers based on predicate abstraction [3, 5] are parametrized by some num\u00adber of predicates; \nthese analyses also exhibit this tradeoff. In many analyses, these tradeoffs are controlled by a small \nnumber of parameters, for instance, a single k value. Past studies (e.g., client-driven [7] and demand-driven \n[9] approaches) have shown that it is often not necessary to pro\u00advide context sensitivity to each call \nsite or object sensitivity to each allocation site. This motivates working with a larger family of abstractions \nparametrized by a separate k value for each site, akin to the parametric framework of Milanova et al. \n[12, 13]. More generally, we represent an abstraction as a binary vector (e.g., component j of the vector \nspeci\u00ad.es whether site j should be treated context-sensitively). But how much context/object sensitivity \nis absolutely needed, and where is it needed? In this paper, we formulate and tackle the following prob\u00adlem: \nGiven a family of abstractions, .nd a minimal (coars\u00adest) abstraction suf.cient to prove all the queries \nprovable by the .nest abstraction in the family. Studying this problem is important for two reasons: \n(i) a minimal abstraction provides insight into which aspects of a program need to be modeled precisely \nfor a given client; and (ii) reducing the complex\u00adity of the abstraction along some components could \nenable us to increase the complexity of the abstraction along other components more than before. For \nexample, keeping the k values of most sites at zero enables us to use higher k values for a select subset \nof sites. To .nd these minimal abstractions, we introduce two ma\u00adchine learning algorithms. Both treat \nthe static analysis as a black box which takes an abstraction (and a set of client queries) as input \nand produces the set of proven queries as output. The .rst algorithm, STATREFINE, starts with the coarsest \nabstraction, runs the static analysis on randomly chosen abstractions, and from these training examples \nde\u00adtects statistical correlations between components of the ab\u00adstraction and whether a query is proven; \ncomponents highly correlated with proven queries are added (Section 4.1). The second algorithm, ACTIVECOARSEN, \nstarts with the .nest abstraction and samples coarser abstractions at random, in\u00adcrementally reducing \nthe abstraction to a minimal one (Sec\u00adtion 4.2).  We also provide a theoretical analysis of our algorithms. \nLet p be the number of components of the abstraction family and s be the number of components in the \nlargest minimal abstraction. We show that although the number of abstrac\u00adtions considered is exponential \nin p, we only need O(s log p) calls to the static analysis to .nd a minimal abstraction. The signi.cance \nof this result is that when a very small abstrac\u00adtion suf.ces to prove the queries (s \u00ab p), our algorithms \nare much more ef.cient than a na\u00a8ive approach, which would require O(p) calls. Empirically, we found \nthat minimal ab\u00adstractions are indeed very small. This is an instance of spar\u00adsity, an important property \nin machine learning and statistics [4, 24], that very few components of an unknown vector are non-zero. \nOur approach represents a signi.cant departure from tra\u00additional program analysis, where iterative re.nement \ntech\u00adniques are the norm [2, 7, 22]. In particular, our methods exploit randomness to generate information \nin the form of statistical correlations. Also note that iterative re.nement is in general not guaranteed \nto .nd a minimal abstraction, whereas our techniques do have this guarantee. We present one iterative \nre.nement technique (DATALOGREFINE in Section 3.1) and show that it re.nes 18 85% of the com\u00adponents \nwhereas the minimal abstractions found by our ap\u00adproach only re.ne 0.4 2.3% (Section 6). All our empirical \nresults are for a static race detection client [14] backed by a k-limited points-to analysis. Points\u00adto \ninformation is used extensively by the race detector to determine which statements may be reachable, \nwhich state\u00adments may access the same memory locations, which state\u00adments may access thread-shared memory \nlocations, and which statements may happen in parallel. Our points-to analysis is both context-sensitive \nand object-sensitive (see Section 5 for details). 2. Problem Formulation Let (A, j) be a poset corresponding \nto a family of abstrac\u00adtions, and let Q be a set of queries that we would like to prove. We assume we \nhave access to a static analysis, F : A .{0, 1}Q, which maps an abstraction a .A to a binary vector F(a) \nwhere component q is 0 if query q is proven and 1 if it is not. We assume that F is monotone with respect \nto the ab\u00adstraction family (A, j), that is, if a j a', then F(a) c F(a'). In other words, re.ning an \nabstraction (increasing a) can only enable us to prove more queries (decrease F(a)). The focus of this \npaper is on the following problem: De.nition 1 (Minimal Abstraction Problem). Given a fam\u00adily of abstractions \nA with a unique top element 1 (the most precise), output an abstraction a j 1 such that 1. a is correct \nthat is, F(a)= F(1); and 2. a is minimal that is, such that {a' j a : F(a')= F(a)} = {a}.  In general, \nthere could be multiple minimal abstractions, but we are content with choosing any one of them. Further\u00admore, \nwe would like to .nd a minimal abstraction ef.ciently, i.e., minimizing the number of calls to F. 2.1 \nBinary Abstractions We now specialize to abstractions representable by binary vectors, that is, A = {0, \n1}J for some index set of compo\u00adnents J, where the partial order is component-wise inequality (a j a' \niff aj = a'for all j . J). The idea is that for an j abstraction a .A, aj denotes whether component j \n. Jhas been re.ned or not. It will also be convenient to treat a .A directly as a set of components (namely, \n{j . J : aj =1}), so that we can use set notation (e.g., a .{j}). We use \u00d8 and Jto denote the coarsest \nand .nest abstractions in the family, and we denote the size of abstraction a by |a|. Many commonly-used \nabstraction families are binary. In predicate abstraction [3, 5], Jis the set of candidate abstrac\u00adtion \npredicates, and an abstraction a would specify a subset of these predicates to include in the analysis. \nShape analy\u00adsis [19] uses predicates on the heap graph, e.g., reachability from a variable. Similar to \npredicate abstraction, aj =1 if predicate j is to be treated as an abstraction predicate. 2.2 k-limited \nAbstractions In this paper, we focus on k-limited abstractions. Let H be the set of allocation sites \nand I be the set of call sites in a program. We use S = H. Ito denote the set of sites of both types. \nConsider the family of abstractions de.ned by setting a non-negative integer k for each site s . S. In \nthe case of k-CFA, this integer speci.es that the k most recent elements of the call stack should be \nused to distinguish method calls and objects allocated at various sites. We denote this abstraction family \nas AK = {0, 1,...,kmax}S, where kmax is the largest allowed k value. At .rst glance, it may not be evident \nthat the k-limited abstraction family AK is a binary abstraction family. How\u00adever, we can represent AK \nusing binary vectors as follows: Let J = S \u00d7{1,...kmax} be the set of components. We map each element \na .A = {0, 1}J in the binary abstrac\u00adtion family to a unique element aK .AK in the original kmax k-limited \nabstraction family by aK =a(s,k). Essen\u00ad sk=1 tially a .A is a unary encoding of the k values of aK .AK. \nNote that multiple binary vectors a map onto the same aK (i.e., (1, 1, 0) and (1, 0, 1) both represent \nk =2), but this is not important. It is crucial, however, that the mapping from A to AK respects the \npartial ordering within each family. 3. Deterministic Approaches In this section, we discuss two deterministic \napproaches for .nding small abstractions. Conceptually, there are two ways to proceed: start with the \ncoarsest abstraction and re\u00ad.ne, or start with the .nest abstraction and coarsen. We present two algorithms: \nDATALOGREFINE (Section 3.1) and SCANCOARSEN (Section 3.2), which operate in these two directions.  3.1 \nRe.nement via Datalog Analysis We .rst present the DATALOGREFINE algorithm, which as\u00adsumes that static \nanalysis F is expressed as a Datalog pro\u00adgram P . The basic idea behind the algorithm is as follows: \nrun the static analysis with the coarsest abstraction \u00d8 and look at queries which were not proven. DATALOGREFINE \nthen inspects P to .nd all the components of the abstraction which could affect the unproven queries \nand re.nes exactly these components. A Datalog program P consists of (i) a set of relations R (e.g., \nptsV .R, where ptsV(v, o) denotes whether variable v may point to abstract object o); (ii) a set of input \ntuples I (for example, ptsV(v5, o7) .I); and (iii) a set of rules of the following form: R0(w0) . R1(w1),...,Rm(wm), \n(1) where for each i =0,...,m, we have a relation Ri .R and a tuple of variables wi of the appropriate \narity (e.g., when R0 = ptsV, w0 =(v, o)). Given a Datalog program, we derive new tuples from the input \ntuples I by using the rules. Formally, let a derivation be a sequence t1,...,tn of tuples satisfying \nthe following two conditions: (i) for each i =1,...,n, either ti is an input tuple (ti .I) or there exist \nindices j1,...,jm all smaller than i such that ti . tj1 ,...,tjm is an instantiation of a rule; and (ii) \nfor each j =1,...,n - 1, tuple tj appears on the right-hand side of an instantiation of a rule with some \nti (i>j) on the left-hand side. In this formalism, each query q . Q is a tuple. In race detection, the \nset of queries is Q = {race(p1,p2): p1,p2 . P}, (2) where P is the set of program points. For each query \nq, we de.ne F(a)q =1 if and only if there exists a derivation of q. In other words, F(a)q =0 if and only \nif q is proven. The abstraction a determines the input tuples I. More speci.cally, let A(t, j) denote \nwhether the value aj of component j affects the input tuple t .I. For example, A(ptsV(v, o),j)=1 for \nany j =(s, k) and abstract ob\u00adject o where o can represent a concrete object allocated at allocation \nsite s . H. Given A, let JA denote the compo\u00adnents which are involved in a derivation of some (unproven) \nquery: JA \u00a3 {j . J : .t .I,q . Q, A(t, j)=1 . t \" q}, (3) where t \" q denotes that there exists some \nderivation t1,...,tn where ti = t for some i and tn = q. The key point is that a component not in JA \ncannot eliminate existing derivations of q, which would be necessary to prove q. Such a component is \ntherefore irrelevant and not re.ned. Computation via a Datalog Program Transformation We now de.ne the \nDATALOGREFINE algorithm, which com\u00adputes and returns JA as the abstraction. DATALOGREFINE works by taking \na Datalog program P as input and trans\u00ad ' forming it into another Datalog program P whose output contains \nJA. Having this general transformation allows us to leverage existing Datalog solvers [25] to ef.ciently \ncom\u00adpute the set of relevant components JA to re.ne. ' The new program P contains all the rules of P \nplus additional ones. For each Datalog rule of P taking the form ' given in (1), P will contain m rules, \none for each i = 1,...,m: R '(ti) . R ' (t0),R1(t1),...,Rm(tm). (4) i0 R '(ti) is true iff Ri(ti) \" q \nfor any query q . Q. We also i add the following rule for each R .R, which aggregates the relevant components: \nJA(j) . R '(t), A(t, j). (5) It can be veri.ed that JA(j) is true exactly when j . JA , where JA is de.ned \nin (3). Note that DATALOGREFINE is correct in that it outputs an abstraction a which is guaranteed to \nprove all the queries that 1 can, but a will most likely not be minimal. k-limited Abstractions We now \ndescribe how we use DATALOGREFINE for k-limited abstractions. Recall that in our binary representation \nof k-limited abstractions (Sec\u00adtion 2.2), the components are (s, k), where s . S is a site and 0 = k \n= kmax. We start with the abstrac\u00adtion a0 = 0, corresponding to 0-CFA. We then iterate k =1,...,kmax, \nwhere at each iteration, we use ak-1 to construct the input tuples and call DATALOGREFINE to pro\u00adduce \na set JA. We re.ne the sites speci.ed by JA, setting ak = ak-1 .{(s, k '):(s, k) . JA,k = k ' }. In this \nway, in each iteration we increase the k value of each site by at most one. Because we always re.ne all \nrelevant compo\u00adnents, after k iterations, ak is guaranteed to prove the same subset of queries as k-CFA. \nThe approach is the same for k-object-sensitivity.  3.2 Coarsening via Scanning In the previous section, \nwe started with the coarsest abstrac\u00adtion and re.ned it. We now introduce a simple algorithm, SCANCOARSEN, \nwhich does the opposite: it takes the most re.ned abstraction a and coarsens it, preserving correctness \n(De.nition 1) along the way. To simplify presentation, sup\u00adpose we have one query. We will revisit the \nissue of multiple queries in Section 4.4. The idea behind the algorithm is quite simple: for each component, \ntry removing it from the abstraction; if the re\u00adsulting abstraction no longer proves the query, add the \ncom\u00adponent back. The pseudocode of the algorithm is given in Figure 1. The algorithm maintains the invariant \nthat aL isa Coarsening via Scanning  Re.nement via Statistical Learning SCANCOARSEN(aL , aU ): Parameters: \nif aL = aU: return aU a: re.nement probability choose any component j . aU \\aL s: size of largest minimal \nabstraction if F(aU \\{j})=0: [try coarsening j] n: number of training examples per iteration return SCANCOARSEN(aL \n, aU \\{j}) [don t need j] else: SAMPLE(a, aL , aU ): return SCANCOARSEN(aL .{j}, aU ) [need j] a . \naL for each component j . aU \\aL: aj . 1 with probability a return a Figure 1: Algorithm that .nds a \nminimal abstraction. STATREFINE(aL ): if F(aL )=0 or |aL | = s: return aLsubset of some minimal abstraction \nand aU is a superset suf\u00adfor i =1,...,n: [create training examples] .cient to prove the query. The algorithm \nrequires |J| calls to a(i) . SAMPLE(a, aL , J) F, and therefore is only practical when the number of \ncom\u00adfor each j . aL: [compute a score for each component] ponents under consideration is small. (i) nj \n. |{i : a=1, F(a(i))=0}| j Theorem 1 (Properties of SCANCOARSEN). The algorithm j* . argmaxj .aL nj [choose \nbest component] SCANCOARSEN(\u00d8, J) returns a minimal abstraction a with return STATREFINE(aL .{j*}) O(|J|) \ncalls to F. Proof. Let a be the returned abstraction. It suf.ces to show that F(a\\{j})=1 for all j . \na (that is, we fail to prove the query with removal of any j). Take any j . a. Since j was kept, F(aU\\{j})=1, \nwhere aU corresponds to the value when j was considered. However, we also have a . aU, so F(a\\{j})=1 \nby monotonicity of F. 4. Machine Learning Approaches We now present two machine learning algorithms for \n.nding minimal abstractions, which is the main theoretical contri\u00adbution of this paper. The two algorithms \nare STATREFINE, which re.nes an abstraction by iteratively adding compo\u00adnents (Section 4.1) and ACTIVECOARSEN, \nwhich coarsens an abstraction by removing components (Section 4.2). At a high level, these two algorithms \nparallel their deter\u00administiccounterparts, DATALOGREFINE and SCANCOARSEN, presented in the previous section. \nHowever, there are two important distinctions worth noting: (i) the machine learning algorithms .nd a \nminimal abstraction much more effectively by exploiting sparsity, the property that a minimal abstrac\u00adtion \ncontains a small fraction of the full set of components; and (ii) randomization is used to exploit this \nsparsity. For clarity of presentation, we again focus on the case where we have a single query; Section \n4.4 addresses the multiple-query setting. 4.1 Re.nement via Statistical Learning We call a component \nj . J dependent if j appears in any minimal abstraction. Let D . J be the set of dependent components \nand let d = |D|. Note that D is the union of all minimal abstractions. De.ne s to be the size of the \nlargest minimal abstraction, observing that s = d. STATREFINE identi.es dependent components by sampling \nn independent Figure 2: Algorithm for .nding a minimal abstraction by it\u00aderatively adding dependent components \ndetermined via sta\u00adtistical learning. random abstractions and running the static analysis F on them. \nThe component j associated with the most number of proven queries is then added to the abstraction, and \nwe iterate. The pseudocode of the algorithm is given in Figure 2. While DATALOGREFINE inspects the Datalog \nprogram backing F to compute the set of relevant components, STATREFINE relies instead on correlations \nwith the output of F to .nd dependent components.1 As Theorem 2 will show, with high probability, a dependent \ncomponent can be found with n calls to F, where n is only logarithmic in the total number of components \n|J|. We must also ensure that n depends only polynomially on s and d. The main technical challenge is \nto set the re.nement probability a properly to achieve this. To appreciate this problem, suppose that \ns = d, so that F(a) consists of a simple conjunction (F(a)=0 iff aj =1 for each j in the minimal abstraction). \nIf we set a to a constant, then it would take an exponential number of examples (( 1 )s a in expectation) \nto even see an example where F(a)=0. Fortunately, the following theorem shows that if a is set properly, \nthen we obtain the desired polynomial dependence (see Appendix A for the proof): Theorem 2 (Properties \nof STATREFINE). Let d be the num\u00adber of dependent components in J and s be the size of the largest minimal \nabstraction. Suppose we set the re.nement d probability a =( )d and obtain n = T(d2(log |J| + d+1 1 Note \nthat dependent components are a subset of relevant components.  following theorem shows that we can \nbalance the two to yield an ef.cient algorithm (see Appendix A for the proof): Coarsening via Active \nLearning Parameters: a: re.nement probability Theorem 3 (Properties of ACTIVECOARSEN). Let s be the size \nof the largest minimal abstraction. If we set the re\u00ad.nement probability a = e-1/s, the expected number \nof s: size of largest minimal abstraction ACTIVECOARSEN(aU ): calls to the analysis F made by ACTIVECOARSEN(J) \nis if |aU |= s +1: return SCANCOARSEN(\u00d8, aU ) O(s log |J|). a . SAMPLE(a, \u00d8, aU ) if F(a)=0: [run static \nanalysis] 4.3 Adapting the Re.nement Probability return ACTIVECOARSEN(a) [reduced] Until now, we have \nassumed that the size of the largest else: minimal abstraction s is known, and indeed Theorems 2 return \nACTIVECOARSEN(aU ) [try again] and 3 depend crucially on setting a properly in terms of s. In practice, \ns is unknown, so we seek a mechanism for setting a without this knowledge. Figure 3: ACTIVECOARSEN returns \na minimal abstraction The intuition is that setting a properly ensures that a by iteratively removing \na random a-fraction of the compo-queries are proven with a probability p(F(a) = 0) bounded nents from \nan upper bound. SAMPLE is de.ned in Figure 2. away from 0 by a constant. Indeed, in STATREFINE, fol\u00adlowing \nthe prescribed setting of a, we get p(F(a) = 0) = d ()d; in ACTIVECOARSEN, we have p(F(a) = 0) = d+1 \n-1/s)sd  log(s/d))) training examples from F each iteration. Then (e= e-1. (Interestingly, ()d is lower \nbounded by d+1 with probability 1 - d, STATREFINE(\u00d8) outputs a minimal e-1 and tends exactly to e-1 \nas d .8.) abstraction with O(sd2(log |J| + log(s/d))) total calls to F. The preceding discussion motivates \na method that keeps -1 p(F(a) = 0) . e\u00a3 t, which we call the target proba\u00ad 4.2 Coarsening via Active \nLearning bility. We can accomplish this by adapting a as we get new We now present our second machine \nlearning algorithm, examples from F. The adaptive strategy we will derive is ACTIVECOARSEN. Like SCANCOARSEN, \nit starts from the simple: if F(a)=0, we decrease a; otherwise, we increase .nest abstraction J and tries \nto remove components from J. a. But by how much? But instead of doing this one at a time, ACTIVECOARSEN \nTo avoid boundary conditions, we parametrize a = tries to remove a random constant fraction of the compo-s(.) \n= (1+ e-.)-1, which maps -8 <.< 8 to 0 < nents at once. As we shall see, this allows us to hone in on \na a< 1. For convenience, let us de.ne g(.)= p(F(a) = 0). minimal abstraction much more quickly. Now consider \nminimizing the following function: The pseudocode of the algorithm is given in Figure 3. It maintains \nan upper bound aU which is guaranteed to prove O(.)= 1(g(.) - t)2 . (6) 2 aU the query. It repeatedly \ntries random abstraction a j . until a can prove the query (F(a)=0). Then we set aU to a Clearly, the \noptimum value (zero) is obtained by setting . and repeat. so that g(.)= t. We can optimize O(.) by updating \nits Recall that SCANCOARSEN, which removes one compo\u00ad gradient: nent at a time, requires an exorbitant \nO(|J|) calls to the static analysis. The key idea behind ACTIVECOARSEN is to re\u00admove a constant fraction \nof components each iteration. Then we would hope to need only O(log1/a |J|) iterations. However, the \nonly wrinkle is that it might take a lot of trials to sample an a that proves the query (F(a)=0). To \nappreciate the severity of this problem, suppose F(a)= \u00ac(a * j a) for some unknown set a * with |a *| \n= s; that is, we prove the query if all the components in a * are re.ned by a. Then there is only a as \nprobability of sampling a random abstraction a that proves the query. The expected number of trials until \nwe prove the query is thus ( 1 )s, which has an a unfortunate exponential dependence on s. On the other \nhand, when we succeed, we reduce the number of components by a factor of a. There is therefore a tradeoff \nhere: setting a too small results in too many trials per iteration, but setting a too large results in \ntoo many iterations. Fortunately, the dO dO dg(.) . . . - ., =(g(.) - t), (7) d.d. d. where . is the \nstep size. Of course we cannot evaluate g(.), but the key is that we can obtain unbiased samples of g(.) \nby evaluating F(a) (which we needed to do anyway); specif\u00adically, E[1 - F(a)] = g(.). We can therefore \nreplace the gradient with a stochastic gradient, a classic technique with a rich theory [18]. We note \nthat dg(.) > 0, so we absorb it d. into the step size ..2 This leaves us with the following rule for \nupdating . given a random a: . . . - .(1 - F (a) - t). (8) 2 Note that we have not veri.ed the step size \nconditions that guarantee convergence. Instead, we simply set . =0.1 for our experiments, which worked \nwell in practice.   4.4 Multiple Queries and Parallelization So far, we have presented all our algorithms \nfor one query. Given multiple queries, we could just solve each query in\u00addependently, but this is quite \nwasteful, since the informa\u00adtion obtained from answering one query is not used for other queries. We \ntherefore adopt a lazy splitting strategy, where we initially place all the queries in one group and \nparti\u00adtion the groups over time as we run either STATREFINE or ACTIVECOARSEN. More speci.cally, we maintain \na parti\u00adtion of Qinto a collection of groups G, where each g . G is a subset of Q. We run the algorithm \nindependently for each g . G. After each call to F(a), we create two new groups, g0 = {q . g : F(a)q \n=0} and g1 = {q . g : F(a)q =1}, and set G to (G\\{g}) .{g0,g1}, throwing away empty groups. In g0, we \ntake the F(a)=0 branch of the algorithm and in g1, we take the F(a)=1 branch. We thus maintain the invariant \nthat for any two queries q1,q2 . g, we have F(a)q1 = F(a)q2 for any a that we have run F on for g or \nany of g s ancestral groups. Conceptually, from the point of view of a .xed q . Q,it is as ifwe had run \nthe algorithm on q alone, but all of the calls to F are shared by other queries. When the algorithm terminates, \nall the queries in one group share the same minimal abstraction. In Section 6, we will see that the number \nof groups is much smaller than the number of queries. Our algorithms have been presented as sequential \nalgo\u00adrithms, but parallelization is possible. STATREFINE is trivial to parallelize because the n training \nexamples are generated independently. Parallelizing ACTIVECOARSEN is slightly more intricate because \nof the sequential dependence of calls to F. With one processor, we set a so that the target proba\u00adbility \nis e-1. When we have m processors, we set the target probability to e-1/m, so that the expected time \nuntil a re\u00adduction is approximately the same. The upshot of this is that a (monotonically related to \nt) is now smaller and thus we obtain larger reductions.  4.5 Discussion of Algorithms Table 1 summarizes \nthe properties of the four algorithms we have presented in this paper. One of the key advan\u00adtages of \nthe learning-based approaches (STATREFINE and ACTIVECOARSEN) is that they have a logarithmic depen\u00addence \non |J| since they take advantage of sparsity, the prop\u00aderty that a minimal abstraction has at most s \ncomponents. Both algorithms sample random abstractions by includ\u00ading each component with probability \na, and to avoid an ex\u00adponential dependence on s, it is important to set the proba\u00adbility a properly for \nSTATREFINE, so that the pro.le of an irrelevant component is suf.ciently different from that of a relevant \ncomponent; for ACTIVECOARSEN, so that the prob\u00adability of obtaining a successful reduction of the abstraction \nis suf.ciently large. The algorithms are also complementary in several re\u00adspects: STATREFINE is a Monte \nCarlo algorithm (the run\u00adning time is .xed, but there is some probability that it does not .nd a minimal \nabstraction), whereas ACTIVECOARSEN is a Las Vegas algorithm (the running time is random, but we are \nguaranteed to .nd a minimal abstraction). Note that STATREFINE has an extra factor of d2, because it \nimplic\u00aditly tries to reason globally about all possible minimal ab\u00adstractions which involve d dependent \ncomponents, whereas ACTIVECOARSEN tries to hone in on one minimal abstrac\u00adtion. In practice, we found \nACTIVECOARSEN to be more effective, and thus used it to obtain our empirical results. 5. Site-varying \nk-limited Points-to Analysis We now present the static analysis (F(a) in our general nota\u00adtion) for the \nabstraction family AK (de.ned in Section 2.2), which allows each allocation and call site to have a separate \nk value. Figure 4 describes the basic analysis. Each node in the control-.ow graph of each method m . \nMis associated with a simple statement (e.g., v2 = v1). We omit statements that have no effect on our \nanalysis (e.g., operations on data of primitive type). For simplicity, we assume each method has a single \nargument and no return value. Our actual imple\u00admentation is a straightforward extension of this simpli.ed \nanalysis which handles multiple arguments, return values, class initializers, and objects allocated through \nre.ection. Our analysis uses sequences of call sites (in the case of k-CFA) or allocation sites (in the \ncase of k-object-sensitivity) to represent method contexts. In either case, abstract objects are represented \nby an allocation site plus the context of the containing method in which the object was allocated. Our \nabstraction a maps each site s . Sto the maximum length as of the context or abstract object to maintain. \nFor example, k-CFA (with heap specialization) is represented by ah = k +1 for each allocation site h \n. H and ai = k for each call site i . I; k-object-sensitivity is represented by ah = k for each allocation \nsite h . H. The abstraction determines the input tuples ext(s, c, c '), where prepending s to c and truncating \nat length as yields c '. For example, if ah2 =2 then we have ext(h2, [i3, i7], [h2, i3]). Our analysis \ncomputes the reachable methods (reachM), reachable statements (reachP), and points-to sets of lo\u00adcal \nvariables (ptsV), each with the associated context; the context-insensitive points-to sets of static \n.elds (ptsG) and heap graph (heap); and a context-sensitive call graph (cg). We brie.y describe the analysis \nrules in Datalog. Rule (1) states that the main method mmain is reachable in a dis\u00adtinguished context \n[]. Rule (2) states that a target method of a reachable call site is also reachable. Rule (3) states \nthat ev\u00adery statement in a reachable method is also reachable. Rules (4) through (9) implement the transfer \nfunction associated with each kind of statement. Rules (10a) and (10b) popu\u00adlate the call graph while \nrules (11a) and (11b) propagate the points-to set from the argument of a call site to the formal argument \nof each target method. Rules (10a) and (11a) are used in the case of k-CFA whereas rules (10b) and (11b) \n  Algorithm Minimal Correct # calls to F DATALOGREFINE SCANCOARSEN no yes yes yes O(1) O(|J|) STATREFINE \nACTIVECOARSEN prob. 1 - d yes prob. 1 - d yes O(sd2(log |J| + log(s/d)) O(s log |J|) [in expectation] \n Table 1: Summary showing the two properties of De.nition 1 for the four algorithms we have presented \nin this paper. Note that the two machine learning algorithms have only a logarithmic dependence on |J|, \nthe total number of components, and a linear dependence on the size of the largest minimal abstraction \ns. are used in the case of k-object-sensitivity. As dictated by k-object-sensitivity, rule (10b) analyzes \nthe target method m in a separate context o for each abstract object o to which the distinguished this \nargument of method m points, and rule (11b) sets the points-to set of the this argument of method m in \ncontext o to the singleton {o}. Race Detection We use the points-to information com\u00adputed above to answer \ndatarace queries of the form presented in (2), where we include pairs of program points correspond\u00ading \nto heap-accessing statements of the same .eld in which at least one statement is a write. We implemented \nthe static race detector of [14], which declares a (p1,p2) pair as rac\u00ading if both statements may be \nreachable, may access thread\u00adescaping data, may point to the same object, and may happen in parallel. \nAll four components rely heavily on the context\u00adand object-sensitive points-to analysis. 6. Experiments \nIn this section, we apply our algorithms (Sections 3 and 4) to the k-limited analysis for race detection \n(Section 5) to answer the main question we started out with: how small are minimal abstractions empirically? \n6.1 Setup Our experiments were performed using IBM J9VM 1.6.0 on 32-bit Linux machines. All the analyses \n(the basic k-limited analysis, DATALOGREFINE, and the race detector) were im\u00adplemented in Chord, an extensible \nprogram analysis frame\u00adwork for Java bytecode.3 The machine learning algorithms simply use the race detector \nas a black box. The experiments were applied to .ve multi-threaded Java benchmarks: an implementation \nof the Traveling Sales\u00adman Problem (tsp), a discrete event simulation program (elevator), a web crawler \n(hedc), a website download\u00ading and mirroring tool (weblech), and a text search tool (lusearch). Table \n2 provides the number of classes, number of meth\u00adods, number of bytecodes of methods, and number of al\u00adlocation/call \nsites deemed reachable by 0-CFA in these benchmarks. Table 3 shows the number of races (unproven queries) \nreported by the coarsest and .nest abstractions. # classes # methods # bytecodes |H| |I| tsp elevator \nhedc weblech lusearch 167 170 335 559 627 635 637 1,965 3,181 3,798 40K 42K 153K 225K 266K 656 663 1,580 \n2,584 2,873 1,721 1,893 7,195 12,405 13,928 Table 2: Benchmark characteristics. |H| is the number of \nallocation sites, and |I| is the number of call sites. Together, these determine the number of components \nin the abstraction family |J|. For k-CFA, |J| = k(|H| + |I|); for k-object\u00adsensitivity, |J| =(k - 1)|H|. \na tsp elevator hedc weblech lusearch \u00d8 (CFA) J(CFA) diff. (|Q|) 570 494 76 510 441 69 21,335 17,837 3,498 \n27,941 8,208 19,733 37,632 31,866 5,766 \u00d8 (OBJ) J(OBJ) 536 489 47 475 437 38 17,137 16,124 1,013 8,063 \n5,523 2,540 31,428 20,929 10,499 diff. (|Q|) Table 3: Number of races (unproven queries) reported us\u00ading \nthe coarsest abstraction \u00d8 (0-CFA/1-object-sensitivity) and the .nest abstraction J (2-CFA/3-object-sensitivity \nfor tsp, elevator and 1-CFA/2-object-sensitivity for hedc, weblech, lusearch). The difference is the \nset of queries under consideration (those provable by J but not by \u00d8). Their difference is the set of \nqueries Qthat we want to prove with a minimal abstraction.  6.2 Results Table 4 summarizes the basic \nresults for DATALOGREFINE and ACTIVECOARSEN. While both .nd abstractions which prove the same set of \nqueries, ACTIVECOARSEN obtains this precision using an abstraction which is minimal and an order of magnitude \nsmaller than the abstraction found by DATALOGREFINE, which is not guaranteed to be minimal (and is, in \nfact, far from minimal in our experiments). Algorithms aside, it is noteworthy in itself that very small \nabstractions exist. For example, on tsp, we can get the same precision as 2-CFA by essentially using \na 0.01-CFA anal\u00adysis. Indeed, our static analysis using this minimal abstrac\u00adtion was as fast as using \n0-CFA, whereas 2-CFA took signif\u00ad 3 http://code.google.com/p/jchord/ icantly longer. Domains: (method) \nm . M = {mmain, ...} (local variable) v . V (global variable) g . G (object .eld) f . F (method call \nsite) i . I (allocation site) h . H (allocation/call site) s . S = H. I (statement) p . P (method context) \nc . C = Sk k=0 (abstract object) o . O = H\u00d7 C (abstraction) a .AK = {0, 1,... }S Input relations: body \n. M\u00d7 P (method contains statement) trgt . I\u00d7 M (call site resolves to method) argI . I\u00d7 V (call site \ns argument variable) argM . M\u00d7 V (method s formal argument variable) ext . S\u00d7 C\u00d7 C (extend context with \nsite) = {(s, c, (s, c)[1.. min{as, 1+|c|}]) : s . S,c . C} Output relations: reachM . C\u00d7 M reachP . \nC\u00d7 P ptsV ptsG heap cg . C\u00d7 V\u00d7 O . G\u00d7 O . O\u00d7 F\u00d7 O . C\u00d7 I\u00d7 C\u00d7 M (reachable methods) (reachable statements) \n(points-to sets of local variables) (points-to sets of static .elds) (heap graph) (call graph)  p ::= \nv = new h | v2 = v1 | g = v | v = g | v2.f = v1 | v2 = v1.f | i(v) Rules: reachM([],mmain). reachM(c, \nm) reachP(c, p) ptsV(c, v, o) ptsV(c, v2,o) ptsG(g, o) ptsV(c, v, o) heap(o2, f, o1) ptsV(c, v2,o2) cg(c1, \ni, c2,m) cg(c, i, o, m) ptsV(c2,v2,o) ptsV(c, v, c) (1) . cg(*, *, c, m). (2) . reachM(c, m), body(m, \np). (3) . reachP(c, v = new h), ext(h, c, o). (4) . reachP(c, v2 = v1), ptsV(c, v1,o). (5) . reachP(c, \ng = v), ptsV(c, v, o). (6) . reachP(c, v = g), ptsG(g, o). (7) . reachP(c, v2.f = v1), ptsV(c, v1,o1), \nptsV(c, v2,o2). (8) . reachP(c, v2 = v1.f), ptsV(c, v1,o1), heap(o1, f, o2). (9) . reachP(c1,i), trgt(i, \nm), ext(i, c1,c2). (10a) . reachP(c, i), trgt(i, m), argI(i, v), ptsV(c, v, o). (10b) . cg(c1, i, c2,m), \nargI(i, v1), argM(m, v2), ptsV(c1,v1,o). (11a) . reachM(c, m), argM(m, v). (11b)  Figure 4: Datalog \nimplementation of our k-limited points-to analysis with call-graph construction. Our abstraction a affects \nthe analysis solely through ext, which speci.es that when we prepend s to c, we truncate the resulting \nsequence to length as. If we use rules (10a) and (11a), we get k-CFA; if we use (10b) and (11b), we get \nk-object-sensitivity. Query Groups Recall from Section 4.4 that to deal with multiple queries, we partition \nthe queries into groups and .nd one minimal abstraction for each group. The abstraction sizes reported \nso far are the union of the abstractions over all groups. We now take a closer look at the abstractions \nfor individual queries in a group. First, Table 5 shows that the number of groups is much smaller than \nthe number of queries, which means that many queries share the same minimal abstraction. This is intuitive \nsince many queries depend on the same data and control properties of a program. Next, Figure 5 shows \na histogram of the abstraction sizes across queries. Most queries required a tiny abstraction, only requiring \na handful of sites to be re.ned. For example, for object-sensitivity on hedc, over 80% of the queries \nrequire just a single allocation site to be re.ned. Even the most demanding query requires only 9 of \nthe 1,580 sites to be re.ned. Recall that re.ning 37 sites suf.ces to prove all the queries (Table 4). \nFor comparison, DATALOGREFINE re.nes 906 sites. 7. Related Work One of the key algorithmic tools that \nwe used to .nd minimal abstractions is randomization. Randomization, a powerful idea, has been previously \napplied in program analysis, e.g., in random testing [8] and random interpretation [6].  Figure 5: \nA histogram showing for each abstraction size |a|, the number of queries that have a minimal abstraction \nof that size (for three of the .ve benchmarks). Note that most queries need very small abstractions (some \nneed only one site to be re.ned). |J| DATALOGREFINE Minimal tsp (CFA) 4,754 3,170 (67%) 27 (0.6%) tsp \n(OBJ) 1,312 236 (18%) 7 (0.5%) elevator (CFA) 5,112 3,541 (69%) 18 (0.4%) elevator (OBJ) 1,326 291 (22%) \n6 (0.5%) hedc (CFA) 8,775 7,270 (83%) 90 (1.0%) hedc (OBJ) 1,580 906 (57%) 37 (2.3%) weblech (CFA) 14,989 \n12,737 (85%) 157 (1.0%) weblech (OBJ) 2,584 1,768 (68%) 48 (1.9%) lusearch (CFA) 16,801 14,864 (88%) \n250 (1.5%) lusearch (OBJ) 2,873 2,085 (73%) 56 (1.9%) # groups min. mean. max. tsp (CFA) tsp (OBJ) elevator \n(CFA) elevator (OBJ) hedc (CFA) hedc (OBJ) weblech (CFA) weblech (OBJ) lusearch (CFA) lusearch (OBJ) \n10 5 6 4 63 43 79 49 140 72 1 1 3 1 1 1 1 1 1 1 8 9 12 10 56 24 250 52 41 146 26 22 22 18 546 300 17,164 \n899 1,346 5,104 Table 4: This table shows our main results. |J| is the number of components (the size \nof the .nest abstraction). The next two columns show abstraction sizes (absolute and fraction of |J|) \nfor the abstraction found by DATALOGREFINE and a minimal abstraction found by ACTIVECOARSEN. All ab\u00adstractions \nprove the same set of queries. DATALOGREFINE re.nes anywhere between 18% 85% of the components, while \nthe minimal abstraction is an order of magnitude smaller (0.4% 2.3% of the components). Our approach \nis perhaps more closely associated with machine learning, although there is an important difference in \nour goals. Machine learning, as exempli.ed by the PAC learning model [23], is largely concerned with \nprediction that is, an algorithm is evaluated on how accurately it can learn a function that predicts \nwell on future inputs. We are instead concerned with .nding the smallest input on which a function evaluates \nto 0. As a result, many of the results, for example on learning monotone DNF formulae [1], are not directly \napplicable, though many of the bounding techniques used are similar. Table 5: Recall that our learning \nalgorithms group the queries (Section 4.4) so that all the queries in one group share the same minimal \nabstraction. The minimum, mean, and maximum size of a group is reported. In all cases, there is large \nspread of the number of queries in a group. One of the key properties that our approach exploited was \nsparsity that only a small subset of the components of the abstraction actually matters for proving the \ndesired query. This enabled us to use a logarithmic rather than linear num\u00adber of examples. Sparsity \nis one of the main themes in ma\u00adchine learning, signal processing, and statistics. For exam\u00adple, in the \narea of compressed sensing [4], one also needs a logarithmic number of linear measurements to recover \na sparse signal. Past research in program analysis, and pointer analy\u00adsis speci.cally, has proposed various \nways to reduce the cost of the analysis while still providing accurate results. Parametrization frameworks \nprovide a mechanism for the user to control the tradeoff between cost and precision of the analysis. \nClient-driven approaches are capable of computing an exhaustive solution of varying precision while demand\u00addriven \napproaches are capable of computing a partial solu\u00adtion of .xed precision. Below we expand upon each \nof these topics in relation to our work.  Milanova et al. [12, 13] present a parametrized frame\u00adwork \nfor a k-object-sensitive points-to analysis, where each local variable can be separately treated context-sensitively \nor context-insensitively, and different k values can be chosen for different allocation sites. Instantiations \nof the framework using k=1 and k=2 are evaluated on side-effect analysis, call\u00adgraph construction and \nvirtual-call resolution, and are shown to be signi.cantly more precise than 1-CFA while being comparable \nto 0-CFA in performance, if not better. Lhot\u00b4ak and Hendren [10, 11] present Paddle, a parametrized frame\u00adwork \nfor BDD-based, k-limited alias analysis. They empir\u00adically evaluate various instantiations of Paddle, \nincluding conventional k-CFA, k-object-sensitivity and k-CFA with heap cloning, on the monomorphic-call-site \nand cast-safety clients, as well as using traditional metrics, and show that k-object-sensitivity is \nsuperior to other approaches both in performance and in precision. Plevyak and Chien [15] use a re.nement-based \nalgorithm to determine the concrete types of objects in programs writ\u00adten in the Concurrent Aggregates \nobject-oriented language. When imprecision in the analysis causes a type con.ict, the algorithm can improve \ncontext sensitivity by performing function splitting, and object sensitivity through container splitting, \nwhich divides object creation sites and thus en\u00adables the creation of objects of different types at a \nsingle site. In a more recent study, Sridharan and Bodik [21] present a demand-driven, re.nement-based \nalias analysis for Java, and apply it to a cast-safety client. Their algorithm computes an overapproximation \nof the points-to relation, which is succes\u00adsively re.ned in response to client demand. At each stage \nof re.nement, the algorithm simultaneously re.nes handling of heap accesses and method calls along paths, \nestablishing the points-to sets of variables that are relevant for evaluating the client s query. Guyer \nand Lin [7] present a client-driven pointer analysis for C that adjusts its precision in response to \ninaccuracies in the client analysis. Their analysis is a two-pass algorithm: In the .rst pass, a low-precision \npointer analysis is run to detect which statements lead to imprecision in the client analysis; based \non this information, a .ne-grained precision policy is built for the second pass, which treats these \nstatements with greater context and .ow sensitivity. This is similar to our DATALOGREFINE in spirit, \nbut DATALOGREFINE is more general. In contrast to client-driven analyses, which compute an exhaustive \nsolution of varying precision, demand-driven approaches compute a partial solution of .xed precision. \nHeintze and Tardieu s demand-driven alias analysis for C [9] performs a provably optimal computation \nto determine the points-to sets of variables queried by the client. Their anal\u00adysis is applied to call-graph \nconstruction in the presence of function pointers. A more recent alias analysis developed by Zheng and \nRugina [27] uses a demand-driven algorithm that is capable of answering alias queries without constructing \npoints-to sets. Reps [16, 17] shows how to automatically obtain demand\u00addriven versions of program analyses \nfrom their exhaustive counterparts by applying the magic-sets transformation developed in the logic-programming \nand deductive-database communities. The exhaustive analysis is expressed in Dat\u00adalog, akin to that in \nour DATALOGREFINE approach, but unlike us, they are interested in answering speci.c queries of interest \nin the program being analyzed as opposed to all queries. For instance, while we are interested in .nding \nall pairs of points (p1,p2) in a given program that may be in\u00advolved in a race, they may be interested \nin .nding all points that may be involved in a race with the statement at point p42 in the given program. \nThe magic-set transformation takes as input a Datalog program which performs the ex\u00adhaustive analysis \nalong with a set of speci.ed queries. It produces as output the demand-driven version of the analy\u00adsis, \nalso in Datalog, but which eliminates computation from the original analysis that is unnecessary for \nevaluating the speci.ed queries. In contrast, our transformation takes as input a parametrized analysis \nexpressed in Datalog and pro\u00adduces as output another analysis, also in Datalog, whose goal is to compute \nall possible parameters that may affect the out\u00adput of the original analysis, for instance, all possible \nsites in a given program whose small k values may be responsible for a set of races being reported by \nthe original analysis. 8. Conclusion We started this study with a basic question: what is the minimal \nabstraction needed to prove a set of queries of interest? To answer this question, we developed two machine \nlearning algorithms and applied them to .nd minimal k values of call/allocation sites for a static race \ndetector. The key theme in this work is sparsity, the property that very few components of an abstraction \nare needed to prove a query. The rami.cations are two-fold: Theoretically, we show that our algorithms \nare ef.cient under sparsity; empirically, we found that the minimal abstractions are quite small only \n0.4 2.3% of the sites are needed to prove all the queries of interest. A. Proofs Proof of Theorem 2. \nNote that the algorithm will run for at most s iterations, where s is the size of the largest minimal \nabstraction. If we set n so that STATREFINE chooses a de\u00adpendent component each iteration with probability \nat least 1 - d/s, then the algorithm will succeed with probability at least 1 - d (by a union bound). \nLet us now focus on one iteration. The main idea is that a dependent component j is more correlated with \nproving the query (F(a)=0) than one that is independent. This enables picking it out with high probability \ngiven suf.ciently large  n. Recall that D is the set of dependent components with |D| = d. Fix a dependent \ncomponent j+ . D. Let Bj- be the event that nj- >nj+ , and B be the event that Bj- holds for any independent \ncomponent j- . J\\D. Note that if B does not happen, then the algorithm will correctly pick a dependent \ncomponent (possibly j+). Thus, the main focus is on showing that P (B) = d/s. First, by a union bound, \nwe have P (B) =P (Bj- ) =|J| max P (Bj- ). (9) j- j- a(i) For each training example , de.ne Xi = (1 \n- (i)(i) F(a(i)))(aj- - aj+ ). Observe that Bj- happens exactly n when 1 (nj- -nj+ )= 1 Xi > 0. We now \nbound this n ni=1 quantity using Hoeffding s inequality,4 where the mean is E[Xi]= p(F(a)=0, aj- = 1) \n- p(F(a)=0, aj+ = 1), and the bounds are a = -1 and b = +1. Setting E = -E[Xi], we get: -nE2/2 p(Bj- \n) = e ,j+ . D, j- . D. (10) Substituting (10) into (9) and rearranging terms, we can solve for n: -nE2/2 \n2(log |J| + log(s/d)) d/s =|J|e . n = . (11) E2 Now it remains to lower bound E, which intuitively repre\u00adsents \nthe gap (in the amount of correlation with proving the query) between a dependent component and an independent \none. Note that p(aj = 1) = a for any j . J. Also, j- is in\u00addependent of F(a), so p(F(a)=0 | aj- = 1) \n= p(F(a)= 0). Using these two facts, we have: E = a(p(F(a)=0 | aj+ = 1) - p(F(a) = 0)). (12) Let C be \nthe set of minimal abstractions of F. We can think of C as a set of clauses in a DNF formula: F(a; C)= \nn \u00ac aj, where we explicitly mark the dependence c.Cj.c of F on the clauses C. For example, C = {{1, 2}, \n{3}}corresponds to F(a)= \u00ac[(a1 . a2) . a3]. Next, let Cj = {c . C : j . c} be the clauses containing \nj. Rewrite p(F(a) = 0) as the sum of two parts, one that depends on j+ and one that does not: p(F(a) \n= 0) = p(F(a; Cj+ )=0, F(a; C\\Cj+ ) = 1)+ p(F(a; C\\Cj+ ) = 0). (13) Computing p(F(a)=0 | aj+ = 1) is \nsimilar; the only difference due to conditioning on aj+ =1 is that the .rst 4 Hoeffding s inequality: \nif X1,...,Xn are i.i.d. random variables with Pn 2 o n 2n: a = Xi = b, then p( 1 Xi > E[Xi]+ E) = exp \n- . ni=1 (b-a)2 1 term has an additional factor of because conditioning a divides by p(aj+ = 1) = a. \nThe second term is unchanged because no c . Cj+ depends on aj+ . Plugging these two results back into \n(12) yields: E = (1 - a)p(F(a; Cj+ )=0, F(a; C\\Cj+ ) = 1). (14) Now we want to lower bound (14) over \nall possible F (equivalently, C), where j+ is allowed to depend on C. It turns out that the worst possible \nC is obtained by either having d disjoint clauses (C = {{j} : j . D}) or one clause (C = {D} if s = d). \nThe intuition is that if C has d clauses, there are many opportunities (d - 1 of them) for some c . Cj+ \nto activate, making it hard to realize that j+ is a dependent component; in this case, E = (1 - a)a(1 \n- a)d-1. If C has one clause, then it is very hard (probability ad) to even activate this clause; in \nthis case, E = (1 - a)ad . Let us focus on the case where C has d clauses. We can maximize E with respect \nto a by setting the derivative dE1 =0 and solving for a. Doing this yields a = as the da d+1 optimal \nvalue. Plugging this value back into the expression 1 d for E, we get that E = ()d. The second factor \ncan d+1 d+1 -1-2 be lower bounded by e, so E= O(d2). Combining this with (11) completes the proof. Proof \nof Theorem 3. Let T (aU) be the expected number of calls that ACTIVECOARSEN(aU) makes to F. The recursive \ncomputation of this quantity parallels the pseudocode of Figure 3: T (aU ) (15) |aU| if |aU |= s +1 \n= 1+E[(1-F(a))T (a)+F(a)T (aU )] otherwise, where a . SAMPLE(\u00d8, aU) is a random binary vector. By assumption, \nthere exists an abstraction a * j aU of size s that proves the query. De.ne G(a)= \u00ac(a * j a), which is \n0 when all components in a * are active under the random a. We have p(G(a) = 0) = p(a * j a)= as. Note \nthat G(a) = F(a), as activating a * suf.ces to prove the query. We assume T (a) = T (aU) (T is monotonic), \nso we get an upper bound by replacing F with G and performing some algebra: T (aU ) = 1+ E[(1 - G(a))T \n(a)+ G(a)T (aU )] (16) = 1+ asE[T (a) | a * j a] + (1 - as)T (aU) (17) = E[T (a) | a * j a]+ a-s . (18) \nOverloading notation, we write T (n) = max|a|=n T (a) to be the maximum over abstractions of size n. \nNote that |a|given a * j a is s plus a binomial random variable N with expectation a(n - s). Using the \ncrude bound T (n) = (1 - an)T (n - 1) + anT (n)a-s, we see that T (n) = a-s \u00b7 n; in particu\u00ad 1-an lar, \nT (n) is sublinear. Moreover, T (n) is concave for large enough n, so we can use Jensen s inequality \nto swap T and  E: T (n) = T (E[s + N]) + a-s = T (s + a(n - s)) + a-s . (19) Solving the recurrence, \nwe obtain: a-s log n T (n) = + s +1. (20) log a-1 From (20), we can see the tradeoff between reducing \nthe number of iterations (by increasing log a-1) versus reducing the number of trials (by decreasing \na-s). We now set a to minimize the upper bound. Differentiate with respect to x = a-1 and set the derivative \nto zero: s-1 s-1 sx x - =0. Solving this equation yields a = e-1/s. log x log2 x Plugging this value \nback into (20) yields T (n)= es log n + s +1= O(s log n). References [1] D. Angluin. Queries and concept \nlearning. Machine Learning, 2(4):319 342, 1988. [2] T. Ball and S. Rajamani. The SLAM project: debugging \nsys\u00adtem software via static analysis. In Proceedings of ACM Symp. on Principles of Programming Languages \n(POPL), pages 1 3, 2002. [3] T. Ball, R. Majumdar, T. Millstein, and S. Rajamani. Auto\u00admatic predicate \nabstraction of C programs. In Proceedings of ACM Conf. on Programming Language Design and Imple\u00admentation \n(PLDI), pages 203 213, 2001. [4] D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289 \n1306, 2006. [5] S. Graf and H. Saidi. Construction of abstract state graphs with PVS. pages 72 83, 1997. \n[6] S. Gulwani. Program Analysis using Random Interpretation. PhD thesis, UC Berkeley, 2005. [7] S. Guyer \nand C. Lin. Client-driven pointer analysis. In Proceedings of Intl. Static Analysis Symposium, pages \n214 236, 2003. [8] D. Hamlet. Random testing. In Encyclopedia of Software Engineering, pages 970 978, \n1994. [9] N. Heintze and O. Tardieu. Demand-driven pointer analysis. In Proceedings of ACM Conf. on Programming \nLanguage Design and Implementation (PLDI), pages 24 34, 2001. [10] O. Lhot\u00b4ak and L. Hendren. Context-sensitive \npoints-to anal\u00adysis: is it worth it? In Proceedings of Intl. Conf. on Compiler Construction, pages 47 \n64, 2006. [11] O. Lhot\u00b4ak and L. Hendren. Evaluating the bene.ts of context\u00adsensitive points-to analysis \nusing a BDD-based implemen\u00adtation. ACM Transactions on Software Engineering and Methodology, 18(1):1 \n53, 2008. [12] A. Milanova, A. Rountev, and B. Ryder. Parameterized object sensitivity for points-to \nand side-effect analyses for Java. In Proceedings of ACM Intl. Symp. on Software Testing and Analysis, \npages 1 11, 2002. [13] A. Milanova, A. Rountev, and B. Ryder. Parameterized object sensitivity for points-to \nanalysis for Java. ACM Transactions on Software Engineering and Methodology, 14(1):1 41, 2005. [14] M. \nNaik, A. Aiken, and J. Whaley. Effective static race detec\u00adtion for Java. In Proceedings of ACM Conf. \non Programming Language Design and Implementation (PLDI), pages 308 319. [15] J. Plevyak and A. Chien. \nPrecise concrete type inference for object-oriented languages. In Proceedings of ACM Conf. on Object-Oriented \nProgramming, Systems, Languages, and Applications, pages 324 340. [16] T. W. Reps. Demand interprocedural \nprogram analysis using logic databases. In Workshop on Programming with Logic Databases, pages 163 196, \n1993. [17] T. W. Reps. Solving demand versions of interprocedural analysis problems. In Proceedings of \nIntl. Conf. on Compiler Construction, pages 389 403, 1994. [18] H. Robbins and S. Monro. A stochastic \napproximation method. Annals of Mathematical Statistics, 22(3):400 407, 1951. [19] M. Sagiv, T. W. Reps, \nand R. Wilhelm. Parametric shape anal\u00adysis via 3-valued logic. ACM Transactions on Programming Languages \nand Systems, 24(3):217 298, 2002. [20] O. Shivers. Control-.ow analysis in Scheme. In Proceedings of \nACM Conf. on Programming Language Design and Imple\u00admentation (PLDI), pages 164 174, 1988. [21] M. Sridharan \nand R. Bod\u00b4ik. Re.nement-based context\u00adsensitive points-to analysis for Java. In Proceedings of ACM Conf. \non Programming Language Design and Implementa\u00adtion, pages 387 400, 2006. [22] M. Sridharan, D. Gopan, \nL. Shan, and R. Bod\u00b4ik. Demand\u00addriven points-to analysis for Java. In Proceedings of ACM Conf. on Object-Oriented \nProgramming, Systems, Languages, and Applications, pages 59 76, 2005. [23] L. Valiant. A theory of the \nlearnable. Communications of the ACM, 27(11):1134 1142, 1984. [24] M. J. Wainwright. Sharp thresholds \nfor noisy and high-dimensional recovery of sparsity using e1-constrained quadratic programming (lasso). \nIEEE Transactions on Infor\u00admation Theory, 55:2183 2202, 2009. [25] J. Whaley. Context-Sensitive Pointer \nAnalysis using Binary Decision Diagrams. PhD thesis, Stanford University, 2007. [26] J. Whaley and M. \nLam. Cloning-based context-sensitive pointer alias analysis using binary decision diagrams. In Pro\u00adceedings \nof ACM Conf. on Programming Language Design and Implementation (PLDI), pages 131 144, 2004. [27] X. Zheng \nand R. Rugina. Demand-driven alias analysis for C. In Proceedings of ACM Symp. on Principles of Programming \nLanguages (POPL), pages 197 208, 1998.    \n\t\t\t", "proc_id": "1926385", "abstract": "<p>Static analyses are generally parametrized by an abstraction which is chosen from a family of abstractions. We are interested in flexible families of abstractions with many parameters, as these families can allow one to increase precision in ways tailored to the client without sacrificing scalability. For example, we consider k-limited points-to analyses where each call site and allocation site in a program can have a different k value. We then ask a natural question in this paper: What is the minimal (coarsest) abstraction in a given family which is able to prove a set of queries? In addressing this question, we make the following two contributions: (i) We introduce two machine learning algorithms for efficiently finding a minimal abstraction; and (ii) for a static race detector backed by a k-limited points-to analysis, we show empirically that minimal abstractions are actually quite coarse: It suffices to provide context/object sensitivity to a very small fraction (0.4-2.3%) of the sites to yield equally precise results as providing context/object sensitivity uniformly to all sites.</p>", "authors": [{"name": "Percy Liang", "author_profile_id": "81323492904", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2509548", "email_address": "pliang@cs.berkeley.edu", "orcid_id": ""}, {"name": "Omer Tripp", "author_profile_id": "81435610768", "affiliation": "Tel-Aviv University, Tel-Aviv, Israel", "person_id": "P2509549", "email_address": "omertrip@post.tau.ac.il", "orcid_id": ""}, {"name": "Mayur Naik", "author_profile_id": "81100223912", "affiliation": "Intel Labs Berkeley, Berkeley, CA, USA", "person_id": "P2509550", "email_address": "mayur.naik@intel.com", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926391", "year": "2011", "article_id": "1926391", "conference": "POPL", "title": "Learning minimal abstractions", "url": "http://dl.acm.org/citation.cfm?id=1926391"}