{"article_publication_date": "01-26-2011", "fulltext": "\n Regular Expression Containment: Coinductive Axiomatization and Computational Interpretation Fritz \nHenglein Lasse Nielsen DIKU, University of Copenhagen { henglein, lnielsen }@diku.dk Abstract We present \na new sound and complete axiomatization of regular expression containment. It consists of the conventional \naxiomatiza\u00adtion of concatenation, alternation, empty set and (the singleton set containing) the empty \nstring as an idempotent semiring, the .xed\u00adpoint rule E * = 1+ E \u00d7 E * for Kleene-star, and a general \ncoin\u00adduction rule as the only additional rule. Our axiomatization gives rise to a natural computational \ninter\u00adpretation of regular expressions as simple types that represent parse trees, and of containment \nproofs as coercions. This gives the axiom\u00adatization a Curry-Howard-style constructive interpretation: \nCon\u00adtainment proofs do not only certify a language-theoretic contain\u00adment, but, under our computational \ninterpretation, constructively transform a membership proof of a string in one regular expres\u00adsion into \na membership proof of the same string in another regular expression. We show how to encode regular expression \nequivalence proofs in Salomaa s, Kozen s and Grabmayer s axiomatizations into our containment system, \nwhich equips their axiomatizations with a computational interpretation and implies completeness of our \nax\u00adiomatization. To ensure its soundness, we require that the compu\u00adtational interpretation of the coinduction \nrule be a hereditarily total function. Hereditary totality can be considered the mother of syn\u00adtactic \nside conditions: it explains their soundness, yet cannot be used as a conventional side condition in \nits own right since it turns out to be undecidable. We discuss application of regular expressions as \ntypes to bit coding of strings and hint at other applications to the wide-spread use of regular expressions \nfor substring matching, where classical automata-theoretic techniques are a priori inapplicable. Neither \nregular expressions as types nor subtyping interpreted coercively are novel per se. Somewhat surprisingly, \nthis seems to be the .rst investigation of a general proof-theoretic framework for the latter in the \ncontext of the former, however. Categories and Subject Descriptors F.4.3 [Formal languages]: Reg\u00adular \nsets; D.1.1 [Applicative (functional) programming] General Terms Languages, Theory Keywords axiomatization, \ncoercion, coinduction, computational inter\u00adpretation, containment, equivalence, regular expression, type \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n11, January 26 28, 2011, Austin, Texas, USA. 1. Introduction What is regular expression matching? In \nclassical theoretical computer science it is the problem of deciding whether a string belongs to the \nregular language denoted by a regular expres\u00adsion; that is, it is membership testing. In this sense, \nabdabc matches ((ab)(c|d)|(abc))*, but abdabb does not. This in\u00adterpretation is used for most theoretical \ncomputer science results: NFA-generation, DFA-generation by subset construction, DFA\u00adminimization, the \nMyhill-Nerode Theorem, the Pumping Lemma, closure properties, the Star Height Problem, Brzozowski deriva\u00adtives, \nfast regular expression equivalence algorithms and match\u00ading algorithms, coalgebraic characterizations, \nbisimulation, etc. If membership testing is all we are interested in, regular expressions and .nite automata \ndenoting the same language are completely interchangeable. In that case we may as well implement regular \nex\u00adpression matching using a state-minimized DFA and forget about the original regular expression. In \nprogramming, however, membership testing is rarely good enough: We do not only want a yes/no answer, \nwe also want to obtain proper matches of substrings against the constituents of a regular expression \nso as to extract parts of the input for process\u00ading. In a Perl Compatible Regular Expression (PCRE)1 \nmatcher, for example, matching abdabc against E = ((ab)(c|d)|(abc))* yields a substring match for each \nof the 4 parenthesized subexpres\u00adsions: They match abc, ab, c, and E (the empty string), respectively. \nIf we use a POSIX matcher (Institute of Electrical and Electron\u00adics Engineers (IEEE) 1992) instead, we \nget abc, E, E, abc, however. How is this possible? The reason is that ((ab)(c|d)|(abc))* is am\u00adbiguous: \nthe string abc can match the left or the right alternative of (ab)(c|d)|(abc), and returning substring \nmatches makes this differ\u00adence observable. In a membership testing setting ambiguity is not observable \nand thus not much studied. An oddity and limitation of Perl-style matching is that we only get one match \nunder Kleene star, the last one. This is why we get a match of abc above, but not abd. Intuitively, we \nwould like to get the list of matches under the Kleene star, not just a single one. This is possible, \nwith regular expression types (Hosoya et al. 2005b): Each group can be named by a variable, and the output \nmay contain multiple bindings to the same variable. For a variable under two Kleene stars, however, we \ncannot discern the bindings between the different level-1 Kleene-star groups. An even more re.ned notion \nof matching is thus regular expression parsing: Returning a parse tree of the input string under the \nregular expression read as a grammar. A little noticed fact is that the parse trees for a regular ex\u00adpression \nare isomorphic to the elements of the regular expres\u00adsion read as a type; e.g. the type interpretation \nT [ E] of regular expression E = ((ab)(c|d)|(abc))* is ((a \u00d7 b) \u00d7 (c + d)+ Copyright &#38;#169; 2011 \nACM 978-1-4503-0490-0/11/01. . . $10.00 1 See http://www.pcre.org. a \u00d7 (b \u00d7 c)) list with a, b, c, d \nbeing singleton types identi.ed with the respective values a, b, c, d they contain. The values p1 = [inl \n((a, b), inr d), inr (a, (b, c))] and p2 = [inl ((a, b), inr d), inl ((a, b), inl c)] are elements of \n((a \u00d7 b)\u00d7(c+d)+a\u00d7(b\u00d7c)) list, representing two different parse trees of the same type. Since their .attening \n(unparsing) yields the same string abdabc, this shows that ((ab)(c|d)|(abc)) * is grammatically ambiguous. \nWhen we have a parsed representation of a string we sometimes need to transform it into a parsed representation \nof another regu\u00adlar expression. Consider for example Ed = (ab(c|d))*, which is equivalent to E. Ed corresponds \nto a DFA that can be used to match a string ef.ciently. But what if we need a parsed representation of \nthe string with respect to E? We need a coercion, a function that maps parse trees under one regular \nexpression (here Ed) into parse trees under another regular expression (here E) such that the under\u00adlying \nstring is preserved. Since E is ambiguous there are different coercions for doing this. The choice of \ncoercion thus incorporates a particular ambiguity resolution strategy; in particular, we may need to \nmake sure that it always returns the greedy left-most parse, as in PCRE matching, or the longest pre.x \nparse, as in POSIX match\u00ading. Also, we will be interested in favoring ef.cient coercions over less ef.cient \nones amongst extensionally equivalent ones; e.g., for coercing E to E, we prefer the constant-time coercion \nthat copies a reference to its input instead of the linear-time coercion that tra\u00adverses its input and \nreturns a copy of it. Even if coercions are not used to transform parse trees, they are useful for regular \nexpres\u00adsions under their language (membership testing) interpretation: The existence of a well-typed \ncoercion from L[ E] to L[ F ] is a proof object that logically certi.es that E is contained in F . Once \nit is constructed, it can be checked ef.ciently for ascertaining that E is contained in F instead of \nembarking on search of a proof of that each time the containment needs to be checked.2 The purpose of \nthis paper is to develop the basic theory of regu\u00adlar expressions as types with coercions interpreting \ncontainment as a conceptual and technical framework for regular expression based programming where the \nclassic language-theoretic view is insuf.\u00adcient. 1.1 Contributions Before delving into the details we \nsummarize our contributions. 1.1.1 Regular expressions as types The interpretation of regular expressions \nas types built from empty, unit, singleton, sum, product, and list types was introduced by Frisch and \nCardelli (2004) for the purpose of regular expression matching. We allow ourselves to observe and point \nout that the el\u00adements of regular expressions as types correspond exactly to the parse trees of regular \nexpressions understood as grammars. Frisch and Cardelli refer to types as describing a concrete structured \nrep\u00adresentation of values , but do not verbalize that those representa\u00adtions are essentially parse trees. \nConversely, Brabrand and Thom\u00adsen (2010) as well as other works de.ne an inference system for parse trees, \nbut do not make explicit that that is tantamount to a type-theoretic interpretation of regular expressions. \n 1.1.2 Proofs of containment by coercion We observe that containment can be characterized by .nding \na coercion, a function mapping every parse tree under one regular expression to a parse tree with same \nunderlying string in the other regular expression. 2 The size of a coercion will necessarily be exponential \nin the sizes of E and F for complexity-theoretic reasons in the worst case, but it may be small in many \ncases. This means that proving a containment amounts to .nding a coercion for the corresponding regular \ntypes, allowing us to bring functional programming intuitions to bear. For example, E \u00d7E * = E * \u00d7 E \nfor all E can be proved by de.ning the obvious function f fun f : a* alist -> alist* a that retains the \nelements in the input. The idea of a coercion interpretation of an axiomatically given subtyping relation \nis not new. Our observation expresses something more elementary and syntax-free , however: The existence \nof a coercion between regular types, however speci.ed, implies con\u00adtainment of the corresponding regular \nexpressions. Note the direc\u00adtion of reasoning: from existence of coercion to containment. 1.1.3 Coinductive \nregular expression containment axiomatization with computational interpretation We give a general coinductive \naxiomatization of regular expression containment and show how to interpret containment proofs compu\u00adtationally \nas string-preserving transformations on parse trees. Each rule in our axiomatization corresponds to a \nnatural functional pro\u00adgramming construct. Speci.cally, the coinduction rule corresponds to the principle \nof de.nition by recursion, where the side condition guarantees that the resulting function is total. \nWe show that the derivations of the axiomatizations by Salomaa (1966), Kozen (1994) and Grabmayer (2005) \ncan be coded as co\u00adercion judgements in our inference system. This provides a natural computational interpretation \nfor their axiomatizations. As far as we know, no previous regular expression axiomatiza\u00adtion has explicitly \nbeen given a sound and complete computational interpretation, where all derivations are interpreted computation\u00adally. \nSulzmann and Lu (2007) come close, however. They provide what can be considered the .rst coercion synthesis \nalgorithm, im\u00adplemented in an extension of Haskell. They show how to construct an explicit coercion for \neach valid regular expression containment by providing a computational interpretation of Antimirov s \nalgo\u00adrithm (Antimirov 1996; Antimirov and Mosses 1995) for deciding regular expression containment. They \nshow that their treatment is sound (Sulzmann and Lu 2007, Lemma A.3), and state that it is complete. \nWe observe that, being based on the construction of de\u00adterministic linear forms, their work can be thought \nof as implement\u00ading a proof search using Antimirov s algorithm in Grabmayer s ax\u00adiomatization. 1.1.4 \nParametric completeness Let us de.ne E[X1,...,Xm] = F [X1,...,Xm] if the contain\u00adment holds for all substitutions \nof Xi with (closed) regular expres\u00adsions. Our axiomatization is not only complete, but parametrically \ncomplete for in.nite alphabets: If E[X1,...,Xm] = F [X1,...,Xm] for all regular expressions X1,...,Xm \nthen there exists c such that f c : E[X1,...,Xm] = F [X1,...,Xm]. As a consequence, a schematic axiom \nsuch as E \u00d7 E * = E * \u00d7 E is derivable, not just admissible in our ax\u00adiomatization: we can prove it once \nand use the same proof for all instances of E. We observe that Kozen s axiomatization (Kozen 1994) is \nalso parametrically complete, but neither Salomaa s (Salomaa 1966) nor Grabmayer s (Grabmayer 2005) appear \nto be so: In Salomaa s case we need to make a case distinction as to whether the regular expression E \nsubstituted for X has the empty word property; and in Grabmayer s case the proofs use the derivatives \nof E, which are syntax dependent. 1.1.5 Application: Bit coding We believe regular expressions as types \nwith coercions have numer\u00adous applications in programming, both conceptually how to think about regular \nexpressions and technically. We sketch one poten\u00adtial application: how bit coding can be used to compactly \nrepresent parse trees and thus strings. This can be thought of as a regular ex\u00adpression speci.c string \nrepresentation that often can be compressed more than the original string. A thorough investigation of \nthis and other applications requires separate treatment, however.  1.2 Prerequisites We assume basic \nknowledge of regular expressions as in Hopcroft and Ullman (1979), and denotational semantics as in Winskel \n(1993).  1.3 Notation and terminology A denotes an alphabet, a possibly in.nite set of symbols {ai}i.I \n. The strings over A is the set of .nite sequences {s, t, . . .} with elements from A. The length of \na string s is denoted by |s|. The n-ary concatenation of s1,...,sn is denoted by their juxtaposition \ns1 ...sn; for n =0 it denotes the empty string E. We use inl and inr as the tags distinguishing the elements \nof a disjoint sum of two sets such that X + Y = {inl v | v . X}.{inr w | w . Y }. We treat recursive \ntypes iso-recursively, where (fold -1 , fold ) denotes the isomorphism between a recursive type and its \nunrolling. In partic\u00adular, we de.ne the list type X * by \u00b5Y.1+ X \u00d7 Y . The empty list [] is an abbreviation \nfor fold (inl ()); and cons (x, y) stands for fold (inr (x, y)). The list notation [x1,...,xn] is syntactic \nsugar for cons (x1,..., cons (xn, [])). We say a unary predicate P universally implies another unary \npredicate Q if .x.(P (x) . Q(x)). 2. Regular expressions as types and coercions In this section we show \nthat a regular expression E can be inter\u00adpreted as an ordinary type and regular expression containment \nas the existence of a coercion between such types. The elements of the types correspond to proofs of \nmembership of strings in the reg\u00adular language denoted by E, which in turn are the parse trees for E \nviewed as a right-regular grammar. A coercion then is any function that transforms parse trees without \nchanging the underlying string. DEFINITION 1 (Regular expression). Theregular expressions RegA is the \nset of abstract syntax tree de.ned by the following regular tree grammar: E, F, G, H ::= 0 | 1 | a | \nE + F | E \u00d7 F | E * where a .A. In anticipation of our interpretation of regular expressions as types \nwe write \u00d7 instead of the more customary juxtaposition or \u00b7 for concatenation. Our notational convention \nis that * , \u00d7, + bind in decreasing order; e.g. a + a \u00d7 b stands for a +(a \u00d7 b). 2.1 Regular expressions \nas languages The language interpretation of RegA maps regular expressions to regular languages (Kleene \n1956). This is also called the standard interpretation of RegA since it is isomorphic to the free Kleene \nalgebra over A (Kozen 1994). DEFINITION 2 (Language interpretation). The language L[ E] is the set of \nstrings compositionally de.ned by: L[[0]] = \u00d8L[ E + F ] = L[ E] .L[ F ] L[[1]] = {E}L[ E \u00d7 F ] = L[ \nE] \u00b7L[ F ]  L[ a] = {a}L[ E * ] = (L[ E]])i i=0 where S \u00b7 T = {st | s . S . t . T }, E0 = {E},Ei+1 = \nE \u00b7 Ei . We write |= s . E if s .L[ E] ; |= E = F if L[ E] .L[ F ] ; and |= E = F if L[ E] = L[ F ] . \nE . 1 () : 1 a . a s . E s . E + F s . F s . E + F s . E t . F s t . E \u00d7 F s . 1 + E \u00d7 E * s . E * a \n: a v : E inl v : E + F w : F inr w : E + F v : E w : F (v, w) : E \u00d7 F v : 1 + E \u00d7 E * fold v : E * a) \nRegular expression matching b) Type inhabitation Figure 1. Matching relation and type inhabitation \nAs expected, L[ E * ] is the set of all .nite concatenations of strings from L[ E] : L[ E * ] = {s1 ...sn \n| n = 0 . si .L[ E] for all 1 = i = n}. DEFINITION 3 (Constant part). The constant part o(E) of E is \nde.ned as o(E)=1 if E .L[ E] and o(E)=0 otherwise. DEFINITION 4 (Matching). We say s matches E and write \nf s . E if the statement s . E is derivable in the inference system in Figure 1a. Matching is sound and \ncomplete for membership testing: PROPOSITION 5. |= s . E if and only if f s . E. The derivation of a \nmatching statement s . E describes a parse tree for s under E understood as a regular grammar. This paper \nis about studying the parse trees, not just the regular language denoted by E. 2.2 Regular expressions \nas types Parse trees are in one-to-one correspondence with regular expres\u00adsions interpreted as types; \nthat is, all we need to do is interpret the regular expression constructors as type constructors and \nwe obtain exactly the parse trees. DEFINITION 6 (Type interpretation). The type interpretation T [ .] \ncompositionally maps a regular expression E to a set of structured values: T [[0]] = \u00d8T [ E + F ] = T \n[ E] + T [ F ] T [[1]] = {()}T [ E \u00d7 F ] = T [ E] \u00d7T [ F ] T [ a] = {a}T [ E * ] = {[v1,...,vn] | vi \n.T [ E] }  We write |= v : E if v .T [ E] . Note that this is the ordinary interpretation of the regular \nexpression constructors as type constructors: 0 is the empty type, 1 the unit type, a (as a type) the \nsingleton type {a}, + the sum type construc\u00adtor, \u00d7 the product type constructor, and . * the list type \nconstructor. DEFINITION 7 (Inhabitation). We say v inhabits E and write f v : E if the statement v : \nE is derivable in the inference system in Figure 1b. Inhabitation is sound and complete for type membership: \n PROPOSITION 8. |= v : E if and only if f v : E. By inspection of Figures 1a and 1b we can see that \na value v such that f v : E corresponds to a unique derivation of s . E for a string s that is uniquely \ndetermined by .attening v. DEFINITION 9. The .attening function .at(.) from values to strings is de.ned \nas follows: .at(()) = E .at(a)= a .at(inl v) = .at(v) .at(inr w) = .at(w) .at((v, w)) = .at(v) .at(w) \n.at(fold v) = .at(v) In particular we have: THEOREM 10. L[ E] = {.at(v) | v .T [ E] }  2.3 Regular expression \ncontainment as type coercion Since each regular expression can be thought of as an ordinary type whose \nelements are all the parse trees for all its strings under the language interpretation, we can characterize \nregular language con\u00adtainment as the problem of transforming parse trees under one reg\u00adular expression \ninto parse trees under the other regular expression. DEFINITION 11 (Coercion). A function f .T [ E] .T \n[ F ] . is a partial coercion from E to F if .at(v) = .at(f(v)) for all v .T [ E] whenever f(v) = .. \nIt is a total coercion (or just coercion) if f(v) = . for all v .T [ E] . We write T [ E = F.] for the \nset of partial coercions from E to F ; and T [ E = F ] for the set of total coercions from E to F . In \nother words, a coercion from E to F is a function that transforms every parse tree under E to a parse \ntree under F for the same underlying string. Clearly, if there exists a coercion from E to F then |= \nE = F : the coercion takes any membership proof of a string in L[ E] to a membership proof for the same \nstring in L[ F ] . Conversely, if |= E = F , we can de.ne a coercion from E to F by mapping any value \nv : E to a value w : F where .at(v) = .at(w). THEOREM 12 (Containment by coercion). |= E = F if and only \nif there exists a coercion from E to F . An immediate corollary is that two regular expressions are equivalent \nif and only if there is a pair of coercions between them: COROLLARY 13 (Equivalence by coercion pairs). \n|= E = F if and only if there exists a pair of coercions (f, g) such that f . T [ E = F ] and g .T [ \nF = E] . It may be tempting to expect such pairs to be isomorphisms; that is, f . g = idT [ F ] and g \n. f = idT [ E] ). This is generally not the case, however: We have |= a = a + a, but there is no isomorphism \nbetween them, since there are two values for a + a but only one value for a. Theorem 12 provides a simple \nand amazingly useful method for proving regular expression containments by functional program\u00adming: Find \na function from E to F (as types!) and make sure that it terminates, outputs each part of the input exactly \nonce and in the same left-to-right order. The latter is usually easily checked when using pattern matching \nin the de.nition of the function. EXAMPLE 14. We prove the denesting rule (Kozen 1997) |=(a + b) * = \na * \u00d7 (b \u00d7 a * ) * . In one direction, .nd a function f : (a + b) * . a * \u00d7 (b \u00d7 a * ) * and make sure \nthat it terminates, uses each part of the input exactly once and outputs them in the same left-to-right \norder as in the input. f([]) = ([], []) f(inl u :: zz)= let (zx, zy)= f(zz) in (u :: zx, zy) f(inr v \n:: zz)= let (zx, zy)= f(zz) in ([], (v, zx) :: yz) We can see that f terminates since it is called recursively \nwith smaller sized arguments, and the output contains the input com\u00adponents in the same left-to-right \norder. Consequently, f de.nes a coercion, and by Theorem 12 this constitutes a proof that the regu\u00ad ** \n) * lar language L[[(a + b) * ] is contained in L[ a \u00d7 (b \u00d7 a ] . The other direction is similar. In \nthis example we de.ned an element of the function space T [ E] .T [ F ] . and then veri.ed manually that \nit belongs to the subspace T [ E = F ] . The following section is about designing a language of functions \neach of which is guaranteed to be a coercion (soundness) and that furthermore is expressive enough so \nthat it contains a term denoting a coercion from E to F whenever |= E = F (completeness). 3. Declarative \ncoinductive axiomatization At the core of all axiomatizations of regular expression equivalence are the \naxiomatization of product (\u00d7), sum (+), empty (0) and unit (1) as the free idempotent semiring over A. \nSee Figures 2 and 3. We add the familiar fold/unfold axiom for Kleene-star in Figure 4, which models \nthat E * is a .xed point of X =1+ E \u00d7 X. Let us call the resulting inference system weak equivalence. \nIt is a sound, but incomplete axiomatization of regular expression equivalence. In particular, we have \n|=(a + 1) * = a *, but they are not weakly equivalent (Salomaa 1966, Remark 4). Intuitively, this is \nbecause weak equivalence does not allow invoking recursively what we want to prove. The basic idea in \nour axiomatization is to add recursion by way of a general .nitary coinduction rule [E = F ] . . . E \n= F E = F (*)  Here [E = F ] is a hypothetical assumption: It may be used an arbitrary number of times \nin deriving the premise, but is discharged when applying the inference step. Since the premise is the \nsame as the conclusion, without a side condition (*) restricting its applicability this rule is blatantly \nunsound: We could simply satisfy the premise by immediately concluding E = F from the hypothetical assumption. \nBy the coinduction rule E = F for arbitrary E, F would be derivable. The key idea of this paper is to \nmake the side condition not a property of the premise, but of the derivation of the premise. To this \nend we switch from axiomatizing equivalence to axiomatizing containment and equip our inference system \nwith names for the rules, arriving at a type-theoretic formulation with explicit proof terms. These proof \nterms can be computationally interpreted as coercions as de.ned in Section 2. The coinduction rule then \nsuggestively reads [f : E = F ] . . . c : E = F .xf. c : E = F (*) as in Brandt and Henglein (1998, \nSection 4.4), where the side con\u00addition is a syntactic condition speci.c to recursive subtyping. The \ncomputational interpretation of .xf. c is the recursively de.ned partial coercion f such that f = c where \nc may contain free oc\u00adcurrences of f . For soundness all we need is for the coercion to be total, that \nis terminate on all inputs. This leads us to the side condition in its most general form: The computational \ninterpretation of .xf. c must be total. Figure 2. Axioms for idempotent semirings  E + (F + G) = (E \n+ F ) + G (1) E + F = F + E (2) E + 0 = E (3) E + E = E (4) E \u00d7 (F \u00d7 G) = (E \u00d7 F ) \u00d7 G (5) 1 \u00d7 E = E \n(6) E \u00d7 1 = E (7) E \u00d7 (F + G) = (E \u00d7 F ) + (E \u00d7 G) (8) (E + F ) \u00d7 G = (E \u00d7 G) + (F \u00d7 G) (9) 0 \u00d7 E = 0 \n(10) E \u00d7 0 = 0 (11) E = FE = FF = G E = E F = EE = G E = GF = HE = GF = H E + F = G + HE \u00d7 F = G \u00d7 H \n Figure 3. Rules of equality E * = 1+ E \u00d7 E * (12) Figure 4. Fold/unfold rule for Kleene-star Unfortunately, \ntotality turns out to be undecidable, and we present ef.ciently checkable syntactic conditions that entail \ntotality and yet are expressive enough to admit completeness. 3.1 Axiomatization Consider the coercion \ninference system in Figure 5. Each axiom of the form G f p : E = F is a short-hand for two containment \naxioms: G f p : E = F and G f p -1 : F = E. DEFINITION 15 (Coercion judgement). Let G be a sequence of \ncoercion assumptions of the form f : E = F , with no f repeated. A coercion judgement is a statement \nof the form G f c : E = F that is derivable in the inference system of Figure 5. If G is empty we may \nomit it and write f c : E = F . By induction on its derivation, a coercion judgement f1 : E1 = F1,...,fn \n: En = Fn f c : E = F can be interpreted coherently as a continuous function F[[G f c : E = F ] : T [ \nE1 = F1.] \u00d7 ... \u00d7T [ En = Fn.] .T [ E = F.] , which is speci.ed by the equations in Figure 6. For example, \nthe clauses for retag should be understood as F[[G f retag : E + F = F + E]](f1,...,fn)= .x.case x of \ninl v . inr v | inr v . inl v. The interpretation of .xf.c is de.ned to be the least .xed point of a \ncontinuous function on T [ E = F.] , which always exists since T [ E = F.] is empty or a cpo with bottom. \nThe interpretation of abortL, abortR, abortL-1 , abortR-1 is the empty function since the type interpretation \nof their domain is empty. Formally: DEFINITION 16 (Computational interpretation). The computational interpretation \nF[ f1 : E1 = F1,...,fn : En = Fn f c : E = F ] G f shu.e : E +(F + G)=(E + F )+ G G f retag : E + F \n= F + E G f untagL : 0+ F = F G f untag : E + E = E G f tagL : E = E + F G f assoc : E \u00d7 (F \u00d7 G)=(E \u00d7 \nF ) \u00d7 G G f swap : E \u00d7 1=1 \u00d7 E G f proj : 1 \u00d7 E = E G f abortR : E \u00d7 0=0 G f abortL : 0 \u00d7 E =0 G f distL \n: E \u00d7 (F + G)=(E \u00d7 F )+(E \u00d7 G) G f distR : (E + F ) \u00d7 G =(E \u00d7 G)+(F \u00d7 G) G f wrap : 1+ E \u00d7 E * = E * \nG f id : E = E G f d : E' = E'' G f c : E = E' G f c; d : E = E'' G f c : E = E' G f d : F = F ' G f \nc + d : E + F = E' + F ' G f c : E = E' G f d : F = F ' G f c \u00d7 d : E \u00d7 F = E' \u00d7 F ' G,f : E = F, G' \nf f : E = F G,f : E = F f c : E = F (coinduction rule) G f .xf.c : E = F Figure 5. Declarative coercion \ninference system for regular ex\u00adpressions as types. With suitable side conditions for the coinduction \nrule this is sound and complete for regular expression containment. See Sections 3.2 and 3.3 for side \nconditions. of a coercion judgement is the (domain-theoretically least) contin\u00aduous function that maps \npartial coercions from Ei to Fi bound to the fi to a partial coercion from E to F satisfying the equations \nof Figure 6. We can interpret all computation judgements, but without a side condition controlling the \nuse of the coinduction rule, the coercion inference system is unsound for deducing regular expression \ncon\u00adtainments. To wit, we can trivially derive f .xf.f : E = F for any E,F . We might hope that a simple \nguarding rule would en\u00adsure soundness. DEFINITION 17 (Left-guarded). Let G f .xf.c : E = F be a coercion \njudgement. We say an occurrence of f in c is left-guarded by d if c contains a subterm of the form d \n\u00d7 d' and the particular occurrence of f is in d'. We call .xf.c left-guarded if for each occurrence of \nf there is a d that left-guards f. Left-guardedness is not suf.cient for soundness, however. Con\u00adsider \nf .xf.(proj-1 ; (id1 \u00d7 f);proj) : E = F, which is derivable for all E and F . (For emphasis, we have \nan\u00adnotated id with a subscript indicating which regular expression it operates on.) Computationally, \nthis coercion judgement does not terminate on any input. This is an instructive case: It contains both \n shu.e(inl v) = inl (inl v) shu.e(inr (inl v)) = inl (inr v) shu.e(inr (inr v)) =inr v shu.e-1(inl (inl \nv)) =inl v shu.e-1(inl (inr v)) = inr (inl v) shu.e-1(inr v) = inr (inr v) retag(inl v) = inr v retag(inr \nv) = inl v retag-1 = retag untagL (inr v)= v untag (inl v)= v untag (inr v)= v tagL (v) = inl v assoc(v, \n(w, x)) =((v, w),x) assoc -1((v, w),x) =(v, (w, x)) swap(v, ()) = ((),v) swap -1((),v) =(v, ()) proj((),w)= \nw proj-1(w) = ((),w) distL(v, inl w) = inl(v, w) distL(v, inr x) = inr(v, x) distL-1(inl (v, w)) =(v, \ninl w) distL-1(inr (v, x)) =(v, inr x) distR(inl v, w) = inl(v, w) distR(inr v, x) = inr(v, x) distR-1(inl \n(v, w)) = (inl v, w) distR-1(inr (v, x)) = (inr v, x) wrap (v) = fold v wrap -1(v) = fold-1 v id(v)= \nv id-1 = id (c; d)(v)= d(c(v)) (c + d)(inl v) = inl(c(v)) (c + d)(inr w) = inr(d(w)) (c \u00d7 d)(v, w) =(c(v),d(w)) \n(.xf.c)(v)= c[.xf.c/f ](v) Figure 6. Computational interpretation of coercions a proj-1 coercion and \nan f that is left-guarded only by (a co\u00adercion operating on) a regular expression,, in this case 1, whose \nlanguage contains the empty string.  3.2 Soundness We have seen that, without a side condition on the \ncoinduction rule, the coercion inference system is unsound for deducing regular expression containments. \nThe key idea now is this: Impose a side condition that guarantees that the coercion in the conclusion \nof the coinduction rule is total. Since all other rules preserve totality of coercions, this yields a \nsound axiomatization of regular expression containment by Theorem 12. Since our coercions may contain \nfree variables, we need to generalize totality to second-order coercions: DEFINITION 18 (Hereditary totality). \nWe say coercion judgement G f c : E = F for G= f1 : E1 = F1,...,fn : En = Fn is hereditarily total if \nF[[G f c : E = F ]](f1,...,fn) is total whenever fi is a total coercion from Ei to Fi for all i =1,...,n. \nWe are now ready to de.ne sound restrictions of the coercion inference system. Instead of formulating \na speci.c side condition, we parameterize over side conditions for the coinduction rule to express, generally, \nwhat is necessary for such a side condition to guarantee soundness. DEFINITION 19 (Coercion inference \nsystem with side condition). Consider the coercion inference system of Figure 5 where the coin\u00adduction \nrule is equipped with a side condition P , a predicate on the coercion judgement in the conclusion: G,f \n: E = F f c : E = F (P (G f .xf.c : E = F )). G f .xf.c : E = F We write G fP c : E = F , if each application \nof the coinduction rule in the derivation of G f c : E = F satis.es P . We arrive at the Master Soundness \nTheorem, which provides a general criterion for sound side conditions: THEOREM 20 (Soundness). Let P \nbe any predicate on coercion judgements that universally implies hereditary totality. Then fP d : E = \nF implies |= E = F for all d, E, F . This theorem shows that hereditary totality is an upper bound for \nhow liberal the side condition can be without the risk of losing sound computational interpretation of \na regular expression contain\u00adment proof as a coercion. Interestingly, allowing partial coercions does \nnot necessarily make the resulting inference system unsound for proving regular expression containment. \nIf we de.ne the side condition P t(G f .xf.c : E = F ) .. |= E = F, the resulting inference system is \ntrivially sound and complete since f .xf.f : E = F is derivable for those E, F such that |= E = F . Clearly, \nF[ f .xf.f : E = F ] is computationally completely useless, however: it never terminates. Unfortunately, \nhereditary totality itself is undecidable even for the restricted language of coercions denotable by \ncoercion judge\u00adments:3 THEOREM 21. Whether or not G f c : E = F is hereditarily total is undecidable. \nPROOF Even totality of f c :1 = 1 is undecidable. This follows from the undecidability of f c :1 * \u00d7 \n1 * = 1 * \u00d7 1 * , which in turn follows from encoding Minsky machines (2-register machines) as closed \ncoercion judgements, using a unary coding of natural numbers. 0 This makes hereditary totality inapplicable \nas a conventional side condition in an axiomatization, where valid instances of an inference rule are \nexpected to be decidable. Below we provide polynomial-time decidable side conditions that are suf.cient \nto encode existing derivations in previous axiomatizations (see Sec\u00adtion 3.3). In each case their soundness \nfollows from application of Theorem 20. In this sense, hereditary totality can be considered the mother \nof all side conditions , even though it itself is too exten\u00adsional to be used as a conventional side \ncondition. DEFINITION 22 (Syntactic side conditions Si). De.ne predicates S1,S2,S3 and S4 on coercion \njudgements of the form G f .xf.c : E = F as follows: S1(G f .xf.c : E = F ) if and only if each occurrence \nof f in c is left-guarded by a d where G,... f d : E ' = F ' is the coercion judgement for d occurring \nin the derivation of G f .xf.c : E = F and o(E ' )=0 (from De.nition 3).  S2(G f .xf.c : E = F ) if \nand only if each occurrence of f in c is left-guarded and for each subterm of the form c1; c2 in c at \nleast one of the following conditions is satis.ed:  3 Proved by Eijiro Sumii, Yasuhiko Minamide, Naoki \nKobayashi, Atsushi Igarashi and Fritz Henglein at the IFIP TC 2 Working Group 2.8 meeting at Shirahama, \nJapan, April 11-16, 2010 c1 is closed and proj-1-free; c2 is closed. S3(G f .xf.c : E = F ) if c is \nof the form wrap -1; (id + id \u00d7 f); d where d is closed.  S4 = S1 . S3.  It is easy to see that S1,S2,S3 \nand S4 are polynomial-time check\u00adable. They furthermore imply hereditary totality: LEMMA 23 (Hereditary \ntotality for Si). Let G f .xf.c : E = F such that Si(G f .xf.c : E = F ) with i .{1, 2, 3, 4}. Then G \nf .xf.c : E = F is hereditarily total. PROOF (Sketch) Side condition S3 is a special case of S2. The \ncase of S4 follows from S1 and S3. We have formulated S3 sepa\u00adrately since S4 is suf.cient to code all \nderivations in Salomaa s and Grabmayer s axiomatizations. S2 by itself, without S1, is suf.cient for \nKozen s axiomatization. The general idea behind the side conditions S1,S2 is that they ensure that every \nrecursive call f in the body c of a recursively de.ned coercion .xf.c is called with an argument whose \nsize is properly smaller than the size of the original call. The difference between the two conditions \nis the de.nition of size in each case. Consider S1. De.ne the 0-size |v|0 of a value by |v|0 = |.at(v)|; \nthat is, it is the length of the underlying string. Values containing () may be of 0-size 0, e.g. |((), \n())|0 =0, and the size of a component of a pair may be the same as the size of the pair: |((),v)|0 = \n|v|0. Consider a call of .xf.c to a value v of 0-size n. The predicate S1 ensures that all recursive \ncalls to f in c are only applied to a value constructed from the second component of some pair, where \nthe .rst component has size at least 1. Since coercions never increase the size this guarantees that \nthe recursive call is applied to a value of 0-size at most n - 1. Now consider S2. De.ne the 1-size |v|1 \nof v as follows: |()|1 =1 |a|1 =1 |inl v|1 = |v|1 |inr v|1 = |v|1 |fold v|1 = |v|1 |(v, w)|1 = |v|1 + \n|w|1 Note that if we had de.ned |()|1 to be 0 then this would be just the 0-size (hence our terminology). \nThe idea for ensuring termination of a recursively de.ned co\u00adercion is the same as before, but for 1-size \ninstead of 0-size. With 1-size we have the important property that each component of a pair is properly \nsmaller than the pair, in particular |w|1 < |(v, w)|1 for all v. We say a coercion c is nonexpansive \nif |c(v)|1 =|v|1. All primitive coercions except for proj-1 are nonexpansive, and the inference rules \npreserve nonexpansiveness. Side condition S2 guarantees that each recursive call is applied to an argument \nof size properly smaller than the original call. Informally, this is because S2 guarantees that a recursive \ncall of f is never applied to a value (constructed from) the output (return value) of a proj-1-call. \n0 From Theorem 20, Lemma 23 and Theorem 12 we obtain: COROLLARY 24 (Soundness for side conditions Si). \nLet S1, S2, S3, S4 as in De.nition 22, i .{1, 2, 3, 4}. Then fSi d : E = F implies |= E = F .  3.3 Completeness \nWe show now how to code derivations in Salomaa s, Kozen s and Grabmayer s axiomatizations of regular \nexpression equivalence in our coercion inference system (Figure 5) with side condition S4 (Salomaa, Grabmayer) \nor S2 (Kozen). This provides a computa\u00adtional interpretation for each of these systems. Furthermore, \nit im\u00adplies that coercion axiomatization with either S2 or S4 is complete. More precisely, we encode \nevery derivation of f E = F as a pair E = F E ** = F E * = (1+ E) * E = F \u00d7 E + G (if o(F )=0) E = F \n* \u00d7 G Figure 7. Salomaa s rules for axiomatization F1 of coercion judgements f c : E = F and f d : F \n= E, which pro\u00advides a computational interpretation of a regular expression equiv\u00adalence as a pair of \ncoercions that witness f E = F according to Corollary 13. Even though they are for regular expression \nequivalence, these codings also provide completeness of our coercion axiomatization for regular expression \ncontainment. Assume |= E = F . This holds if and only if |= E + F = F . By completeness of the regular \nexpression equivalence axiomatizations, E + F = F is derivable, and we can construct a coercion judgement \nof f c : E + F = F . Composed with tagL this yields f tagL ; c : E = F , and we are done. THEOREM 25 \n(Completeness). Let P be either S2 or S4. If |= E = F then there exists c such that fP c : E = F . It \nfollows that any side condition logically between S2 or S4 on the one hand and hereditary totality on \nthe other hand yields a sound and complete coercion axiomatization of regular expression containment. \nCOROLLARY 26 (Soundness and completeness). Let P be such that either S2 or S4 universally implies P , \nand P universally implies hereditary totality. Then fP c : E = F if and only if |= E = F . Whereas hereditary \ntotality is the natural upper bound we suspect that there are natural weaker lower bounds than S2 and \nS4. 3.3.1 Salomaa Salomaa s System F1 (Salomaa 1966) arises from adding the rules of Figure 7 to the \naxiomatization of weak equivalence (Figures 2, 3 and 4).4 The side condition of the inference rule in \nFigure 7 is called the no empty word property . To be precise, we prove by induction on derivations of \nE = F in System F1 that there exist coercion judgements fS4 c : E = F and fS4 d : F = E. This is straightforward \nfor the weak equivalence rules. We thus concentrate on the rules in Figure 7. E = F Consider E ** . By \ninduction hypothesis there exist = F fS4 c : E = F and fS4 d : F = E. We reason as follows: Assume E \n* = F * and call this assumption f. E * = (1 + E \u00d7 E * ) by wrap -1 = (1 + F \u00d7 F * ) byid+ c \u00d7 f = F \n* by wrap This shows that * * -1** f : E = F fS4 (wrap ;id+ c \u00d7 f; wrap ) : E = F. Note that f .xf.(wrap \n-1;id + c \u00d7 f; wrap ) : E * = F * sat\u00adis.es side condition S3 and thus S4. Its computational interpre\u00adtation \nis the map-function on lists. With S4 satis.ed we can ap\u00ad 4 Technically, this is the left-handed dual \ndue to Grabmayer (2005) to Salomaa s original right-handed formulation, where the fold-unfold rule for \nKleene star is axiomatized as E* =1+ E* \u00d7 E. ply the coinduction rule to conclude fS4 .xf.(wrap -1;id \n+ c \u00d7 f; wrap ) : E * = F *. Similarly, we get fS4 .x g.(wrap -1;id + d \u00d7 g; wrap ) : F . * = E * Consider \nE * = (1+ E) *. The case E * = (1 + E) * follows from the rule above since E = 1+E. For the converse \ncontainment assume f : (1+ E) * = E *. We have: (1 + E) * = 1 + (1 + E) \u00d7 (1 + E) * by wrap -1 = 1 + \n(1 + E) \u00d7 E * by f = 1+1 \u00d7 E * + E \u00d7 E * by distR = 1+ E * + E \u00d7 E * by proj = 1+ E \u00d7 E * + E * by retag \n= by wrap E * + E * = by untag E * We are a bit informal here: We have left associativity, congruence \nand identity steps implicit. Let us consider f .xf.c : (1+ E) * = E * now. Without displaying c in full \ndetail, from the derivation above we can see that f satis.es side condition S3 and thus S4, and we can \nconclude fS4 .xf.c : (1+ E) * = E * by the coinduction rule. Operationally, F[ f .xf.c : (1+ E) * = E \n* ] traverses its in\u00adput list of type T [[(1 + E) * ] , removes all occurrences of inl () and returns \nthe v s for each inr v in the input. E = F \u00d7 E + G Finally, consider * (if o(F )=0). E = F \u00d7 G Our induction \nhypothesis is fS4 c1 : E = F \u00d7 E + G and fS4 d1 : F \u00d7 E + G = E. Let us consider F * \u00d7 G = E .rst. Assume \nf : F * \u00d7 G = E, and we can calculate d2 : F * \u00d7 G = E as follows: F * \u00d7 G = (1 + F \u00d7 F * ) \u00d7 G by wrap \n-1 = 1 \u00d7 G + F \u00d7 F * \u00d7 G by distR = G + F \u00d7 F * \u00d7 G by proj = G + F \u00d7 E by f = F \u00d7 E + G by retag = E \nby d1 We can see that f .xf.d2 : F * \u00d7 G = E satis.es S2 and, since o(F )=0, also S1 and thus S4. We \ncan thus conclude fS4 .xf.d2 : F * \u00d7 G = E by the coinduction rule. Observe that f .xf.d2 : F * \u00d7 G = \nE is hereditarily total, whether or not o(F )=0, since S2 is also satis.ed. For the other direction, \nassume fS4 g : E = F * \u00d7 G, and we can calculate c2 : E = F * \u00d7 G essentially in the reverse direction \nto the above calculation. E = F \u00d7 E + G by c1 = G + F \u00d7 E by retag = G + F \u00d7 F * \u00d7 G by g = 1 \u00d7 G + F \n\u00d7 F * \u00d7 G by proj-1 = (1 + F \u00d7 F * ) \u00d7 G by distR-1 = F * \u00d7 G by wrap Here, the coercion judgement f \n.xg.c2 : E = F * \u00d7 G may com\u00adputationally be nonterminating: Choose, e.g., c1 = proj-1; tagL : E = 1 \n\u00d7 E +0. For o(F )=0, however, f .xg.c2 : E = F * \u00d7 G satis.es side condition S1 and thus S4; in particular, \nit always ter\u00adminates. We can conclude fS4 .xg.c2 : E = F \u00d7 E + G by the coinduction rule.  3.3.2 Kozen \nKozen (1994) has shown that adding the rules in Figure 8 to weak equivalence is sound and complete for \nregular expression equiva\u00adlence. Formally, a containment E = F in his axiomatization is an abbreviation \nfor E + F = F . We show now that all derivations in his system can be coded as coercion judgements with \nside condition S2. 1+(E * \u00d7 E) = E * E \u00d7 F = FE \u00d7 F = E E ** = E \u00d7 F = FE \u00d7 F Figure 8. Kozen s rules \nfor axiomatization of Kleene Algebras 0ai = 0 1ai = 0 (ai)ai =1 (aj )ai =0 (i = j) (E + F )ai = Eai + \nFai (E \u00d7 F )ai = Eai \u00d7 F (o(E) = 0) (E \u00d7 F )ai = Eai \u00d7 F + Fai (o(E) = 1) (E * )ai = Eai \u00d7 E * Figure \n9. De.nition of Brzozowski-derivative Consider 1+(E * \u00d7 E) = E *. It is suf.cient to construct a coercion \njudgement fS2 c : E * \u00d7 E = E \u00d7 E *, since we then have fS2 id + c; wrap : 1 \u00d7 E * \u00d7 E = E *, as desired. \nAssume f : E * \u00d7E = E \u00d7E *. We can calculate c : E * \u00d7E = E \u00d7 E * as follows: E * \u00d7 E = (1 + E \u00d7 E * \n) \u00d7 E by wrap -1 = 1 \u00d7 E + E \u00d7 E * \u00d7 E by distR = 1 \u00d7 E + E \u00d7 E \u00d7 E * by f = E \u00d7 1+ E \u00d7 E \u00d7 E * by swap \n= E \u00d7 (1 + E \u00d7 E * ) by distL-1 = E \u00d7 E * by wrap Writing c explicitly, we have c = (wrap -1 \u00d7 id); \ndistR; swap + (assoc-1; id \u00d7 f); distL-1; id \u00d7 wrap Observe that f .xf.c : E * \u00d7 E = E \u00d7 E * satis.es \nside condition S2, and we can conclude fS2 .xf.c : E * \u00d7 E = E \u00d7 E * by the coinduction rule. E \u00d7 F = \nF Consider the rule . Our induction hypothesis E * \u00d7 F = F is that there exists fS2 d : E \u00d7 F = F . \nAssume f : E * \u00d7 F = F , and we calculate c : E * \u00d7 F = F as follows: E * \u00d7 F = (1 + E \u00d7 E * ) \u00d7 F by \nwrap -1 = 1 \u00d7 F + E \u00d7 E * \u00d7 F by distR = 1 \u00d7 F + E \u00d7 F by f = F + E \u00d7 F by proj = F + F by d = F by untag \n Note that f .xf.c : E * \u00d7 F = F satis.es side condition S2, and we can apply the coinduction rule to \nconclude fS2 .xf.c : E * \u00d7 F = F . E \u00d7 F = E The rule * is similar to the previous rule, with E \u00d7 F \n= E an additional step involving E * \u00d7 E = E \u00d7 E * . 3.3.3 Grabmayer The following results hold for \nall alphabets, but for convenience we assume that A is .nite in this section. The Brzozowski-derivative \nEa (Antimirov 1996; Brzozowski 1964; Conway 1971; Rutten 1998; Salomaa 1966) for regular ex\u00adpression \nE and a .A is de.ned in Figure 9. [E = F ][E = F ] .. . ... . .. Ea1 = Fa1 Ean = Fan E = F (o(E)= o(F \n)) Figure 10. Grabmayer s coinduction rule COMP/FIX Grabmayer (2005) recognized that Brzozowski-derivatives \ncan be combined with the ACI-properties of + and the coinductive .xed point rule for recursive types \nof Brandt and Henglein (1998) to give a coinductive axiomatization of regular expression equiv\u00adalence. \nHis rule COMP/FIX is given in Figure 10. Indeed, it can be seen that in the presence of a transitivity \nrule of equational logic, the compatibility-with-context-rules, and ACI-axioms, only the rule COMP/FIX \nis needed to obtain a complete system for regular expression equivalence, without the other rules of \nGrab\u00admayer s inference system cREG0(S). A sequent style presenta\u00adtion of COMP/FIX is as follows: G,E \n= F fG Ea = Fa for all a .A,o(E)= o(F ) G fG E = F Let us write G= and G= for G where all occurrences \nof = in G are replaced by =, respectively =. We can show by rule induction that for each derivation of \nG fG E = F there exist coercion judgements G= fS4 c : E = F and G= fS4 d : F = E. The only interesting \nrule to consider is COMP/FIX. By induc\u00adtion hypothesis, we have G=,f : E = F f ca : Ea = Fa and G=,g \n: F = E f da : Fa = Ea for all a .A, where o o(E)= o(F ). Note that |= E = o(Eo)+ a.A a \u00d7 Ea. Salo\u00admaa \n(1966) shows that E = o(E)+ a.A a \u00d7 Ea is derivable from the rules for weak equivalence (Figures 2, 3 \nand 4), extended with Salomaa s Axiom A11: F * = (1+ F ) *. (See also Grabmayer (2005, Lemma 5, p. 189).) \nA11 is only required for what Frisch and Cardelli (2004) call problematic regular expressions, regular \nexpressions of the form G * where o(G)=1. By applying the derivation coding of Salomaa s axiomatization \no from Subsection 3.3.1 to the derivation of E = o(E)+ a \u00d7 o a.A Ea, we know that there exist fS4 cE \n: o(E)+ = E o a.A a \u00d7Ea and fS4 dE : E = o(E)+ a.A a \u00d7 Ea. This gives us the following derivable coercion \njudgements: o G=,f : E = F fS4 dE ; (ido(E) + ida \u00d7 ca); cF : E = F oa.A G=,g : F = E fS4 dF ; (ido(F \n) + a.A ida \u00d7 da); cE : F = E We can observe that they satisfy side condition S1 and thus S4. By the \ncoinduction rule we can thus conclude: o G= fS4 .xf.dE ; (ido(E) + ida \u00d7 ca); cF : E = F oa.A G= fS4 \n.xg.dF ; (ido(F ) + a.A ida \u00d7 da); cE : F = E This provides an alternative proof to the one based on \ncoding Salomaa s System F1 for concluding that |= E = F implies fS4 E = F .  3.4 Examples We give examples \nof coercion judgements for regular expression containments. EXAMPLE 27 (Denesting as coercion). We continue \nExample 14 ** ) * by implementing the function proving (a + b) * = a \u00d7 (b \u00d7 a in the coercion language. \n Abbreviate E =(a + b) * and F = a * \u00d7 (b \u00d7 a * ) *. We can calculate (a + b) * = a * \u00d7 (b \u00d7 a * ) * \nas follows. E = 1+(a + b) \u00d7 E by wrap -1 = 1+(a + b) \u00d7 F by f = 1+(b + a) \u00d7 F by retag = 1 + ((b \u00d7 F \n)+(a \u00d7 F )) by distR ** ) * = 1 + (((b \u00d7 a ) \u00d7 (b \u00d7 a )+(a \u00d7 F )) by assoc ** ) * = (1 + (b \u00d7 a ) \u00d7 (b \n\u00d7 a ) + ((a \u00d7 F )) by shu.e * ) * = (b \u00d7 a +(a \u00d7 F ) by wrap * ** = (b \u00d7 a ) * + ((a \u00d7 a ) \u00d7 (b \u00d7 a ) \n* ) by assoc * ** = (b \u00d7 a ) * + ((1 + a \u00d7 a ) \u00d7 (b \u00d7 a ) * ) by tagR * ) * = (b \u00d7 a + F by wrap * ) \n* = 1 \u00d7 (b \u00d7 a + F by proj-1 * ) * = (1 + a \u00d7 a * ) \u00d7 (b \u00d7 a + F by tagL = F + F by wrap = F by untag \n Writing it out in full, the coercion coercion judgement is f .xf. wrap -1; id + retag \u00d7 f; id + distR; \nid + (assoc + assoc); shu.e; wrap + id; id + (tagR;wrap) \u00d7 id; proj-1 + id; (tagL ; wrap ) \u00d7 id + id; \nuntag : (a + b) * = a * \u00d7 (b \u00d7 a * ) * In the above example, the coercion is, operationally, basically \nthe function f de.ned in Example 14: It folds a constant-time computable function over its input list \nand therefore runs in linear time. Kozen (1994, Proposition 2.7) gives a proof of the same inclusion \nin his axiomatization of Kleene algebra. When encoding it as in Section 3.3.2 we obtain a similar, linear-time \ncoercion. This raises the question whether computational interpretation of all proofs of the same containment \nin the axiomatizations of Salomaa, Kozen and Grabmayer yield coercions of the same, linear-time complexity. \nRemarkably, this is not the case, as illustrated by the next example. *** * EXAMPLE 28 (Coercion ef.ciency). \nConsider a \u00d7 a = a . The simplest way to prove this with Kozen s rules is to start from a \u00d7 a * = a * \nproved by tagR ; wrap . By the left inference rule in Figure 8 we then get a * \u00d7 a * = a *. By the right \ninference rule in *** * Figure 8 we .nally obtain a \u00d7 a = a . Let us consider the computational interpretation \nof this proof. We have two (nested) applications of the (left and right, respec\u00adtively) inference rule \nfrom Figure 8. This gives quadratic runtime. It is unclear to us whether there exists a proof using Kozen \ns rules whose computational interpretation as a functional program runs in linear time. It is possible \nto construct a linear-time coercion for *** * a \u00d7 a = a in our coercion inference system, however. This \ncan be systematically obtained by encoding the (minimal) proof in Grabmayer s axiomatization. In fact, \nthe encoding of any proof in Grabmayer s axiomatization will have linear run time. This is be\u00adcause the \nonly admissible application of recursion in Grabmayer s axiomatization is of the form .xf.dE ;(id + Sa.Aida \n\u00d7 ca); cF , where f does not occur in dE and cF , which entails that only con\u00adstant amount of processing \noccurs for each constant part of the input. 3.5 Parametric completeness Let us extend regular expressions \nby adding variables that can be bound to arbitrary regular sets. Formally, E, F, G, H ::= 0 | 1 | a | \nX | E + F | E \u00d7 F | E * where X ranges over a denumerable set of (formal) variables {Xi}i.N. Such a \nregular expression is closed if it contains no formal variables. We de.ne |= .X1,...,Xm.E[X1,...,Xm] \n= F [X1,...,Xm] if the containment holds for all substitutions of Xi with (closed) regular expressions. \n Our axiomatization is immediately applicable to regular expres\u00adsions with free variables. Without change, \nit is not only complete, but parametrically complete for in.nite alphabets: THEOREM 29 (Parametric completeness). \nLet A be in.nite. Let the side condition P for the coinduction rule in Figure 5 be either total hereditariness \nor S2. Then: |= .X1,...,Xm.E = F if and only if fP c : E = F . PROOF Only if: By rule induction, coercion \naxiomatization is closed under substitution, with total hereditariness or S2 as side condition. Note \nthat this is not the case for S4. (Technically, S1 is not even de.ned for regular expressions with variables, \nsince o(X) is unde.ned.) If: Assume |= .X1,...,Xn.E = F . Let b1,...,bn be n dis\u00adtinct alphabet symbols \nnot occurring in E or F . (Since A is in.nite, they exist.) By de.nition of |= .X1,...,Xn.E = F , we \nhave |= E{X1 . b1,...,Xn . bn}= F {X1 . b1,...,Xn . bn}. By Theorem 25 (completeness), there is a derivable \ncoercion judgement fP c : E{X1 . b1,...,Xn . bn}= F {X1 . b1,...,Xn . bn}. Since coercion axiomatization \nwith heredi\u00adtary totality or S2 as side condition is closed under substitution, the b1,...,bn can be \nreplaced by arbitrary regular expressions E1,...,En, respectively, such that f c : E{b1 . E1,...,bn . \nEn}= F {b1 . E1,...,bn . En}. In particular, we can choose X1,...,Xn for E1,...,En and thus obtain f \nc : E = F . 0 As a consequence of Theorem 29, a schematic containment such as E \u00d7 E * = E * \u00d7 E is derivable, \nnot just admissible in our axiomatization: we can synthesize a single coercion judgement for it and use \nit for all instances of E. For .nite alphabets our axiomatization is incomplete, however: |= .X. (X = \n(a + b) * ) holds for A = {a, b}, but X = (a + b) * is not derivable. We observe that Kozen s axiomatization \n(Kozen 1994) is also parametrically complete for in.nite alphabets, but not for .nite alphabets. Neither \nSalomaa s (Salomaa 1966) nor Grabmayer s (Grabmayer 2005) appear to be parametrically complete: In Salo\u00admaa \ns case we need to make a case distinction as to whether the reg\u00adular expression E substituted for X has \nthe empty word property; and in Grabmayer s case E needs to be differentiated, the proof of which depends \non the syntax of E. 4. Application: Compact bit representations of parse trees If the regular expressions \nare statically known in a program we can code their elements, more precisely their parse trees, compactly \nas bit strings. 4.1 Bit coded strings Intuitively, a bit coding of a parse tree p factors p into its \nstatic part, the regular expression E it is a member of, and its dynamic part, a bit sequence that uniquely \nidenti.es p as a particular element of E. Consider for example the string s = abaab as an element of \nH1 =(a + b) *. It has the unique parse tree corresponding to ps = [inl a, inr b, inl a, inl a, inr b] \nwith f ps : H1, which shows that .at(ps)= abaab is an element of L[ H1] . Figures 11 and 12 de.ne regular-expression \ndirected coding and decoding functions from parse trees to their (canonical) bit codings and back. Informally, \nthe bit coding of a parse tree consists of listing the inl -and inr -constructors in preorder traversal, \nwhere inl is mapped to 0 and inr is mapped to 1. No bits are generated code(() : 1) = E code(a : a)= \nE code(inl v : E + F ) = 0 code(v : E) code(inr w : E + F ) = 1 code(w : F ) code((v, w): E \u00d7 F ) = code(v \n: E) code(w : F ) code(fold v : E * ) = code(v :1+ E \u00d7 E * )  Figure 11. Type-directed encoding function \nfrom parse trees (val\u00adues) to bit sequences decode ' (d : 1) = ((),d) decode ' (d : a) =(a, d) decode \n' (0d : E + E ' )= let (v, d ' ) = decode ' (d : E)  in (inl v, d ' ) decode ' (1d : E + E ' )= let \n(w, d ' ) = decode ' (d : E) in (inr w, d ' ) decode ' (d : E \u00d7 E ' )= let (v, d ' ) = decode ' (d : \nE) (w, d '' ) = decode ' (d ' : E ' ) in ((v, w),d '' ) decode ' (d : E * )= let (v, d ' ) = decode ' \n(d :1+ E \u00d7 E * ) in (fold v, d ' ) decode(d : E)= = let (w, d ' ) = decode ' (d : E) in if d ' = E then \nw else error Figure 12. Type-directed decoding function from bit sequences to parse trees (values) for \nthe other constructors. For example, the bit coding bs for ps is 10 11 10 10 11 0. We can think of the \nbit coding of a parse tree p as a bit coding of the underlying string .at(p). Note that the bit coding \nof a string is not unique. It depends on which regular expression it is parsed under; and  if the regular \nexpression is ambiguous, which parse tree is chosen for it.  As an illustration of the .rst effect, \nthe bit representation b ' of s s under H2 = 1+(a + b) * \u00d7 (a + b) is 1 10 11 10 10 0 1. Since both \nH1 and H2 are unambiguous there are no other bit representations of s under either H1 or H2. 4.2 Bit \ncode coercions So what if we have a bit representation of a run-time string under one regular expression \nand we need to transform it into a bit representation under another regular expression? This arises if \nthe branches of a conditional expression each return a bit-coded string, but under different regular \nexpressions E1,E2, and we need to ensure that the result of the conditional is a bit coding in a common \nregular expression F that contains the E1 and E2. Let us consider s again and how to transform bs into \nb ' s. As we have seen in Section 3.3, E * is contained in 1+(E * \u00d7 E) for all E and there is a parametric \npolymorphic coercion c1 : .X.X * = 1+(X * \u00d7 X) mapping any value f p : E * representing a parse tree \nfor string s ' = .at(p) to a parse tree f p ' :1+ E * \u00d7 E for s '. In particular applying c1 to ps yields \nps' . We can compose c1 with code and decode from Figures 11 and 12 to compute a function c 1 operating \non bit codings: c 1 = code \u00b7 c1 \u00b7 decode. Instead of converting to and from values we can de.ne a bit \ncoding coercion by providing a computational interpretation of retag(0d) =1d retag(1d) =0d retag-1 = \nretag tagL (d) =0d untag (bd)= d shu.e(0d) =00d shu.e(10d) =01d shu.e(11d) =1d shu.e-1(00d) =0d shu.e-1(01d) \n=10d shu.e-1(1d) =11d swap(d)= d -1 swap = swap proj(d)= d proj-1(d)= d assoc(d)= d assoc -1(d)= d distL(d \n: E \u00d7 (F + G)) = let (d1, bd2) = split(d : E) in bd1d2 distL-1(bd :(E \u00d7 F )+(E \u00d7 G)) = let (d1,d2) = \nsplit(d : E) in d1bd2 distR(d)= d distR-1(d)= d wrap (d)= d wrap -1(d)= d (c + c ' )(0d) =0 c(d) '' (d \n' (c + c )(1d ' ) =1 c )) (c \u00d7 c ' )(d : E \u00d7 F )= let (d1,d2) = split(d : E) in c(d1) c ' (d2) (c; c \n' )(d)= c ' (c(d)) id(d)= d (.x f.c)(d)= c[.x f.c/f](d) Figure 13. Coercions operating on typed bit sequence \nrepresenta\u00adtions instead of values split(d :1) =(E, d) split(d : a) =(E, d) split(0d : E + E ' )= let \n(d1,d2) = split(d : E) in (0d1,d2) : E + E '' split(1d ' )= let (d1,d2) = split(d ' : E ) in (1d1,d2) \nsplit(d : E * ) = split(d :1+ E \u00d7 E * ) split(d : E \u00d7 E ' )= let (d1,d2) = split(d : E) (d3,d4) = split(d2 \n: E ' ) in (d1d3,d4) Figure 14. Type-directed function for splitting bit sequence into subsequences corresponding \nto components of product type v : a[\u00b5X.a/X] fold v : \u00b5X.a Figure 15. Inhabitation rule for \u00b5 coercion \njudgements that operates directly on bit coded strings. See Figure 13. It uses the type-directed function \nsplit from Figure 14 for splitting a bit sequence into a pair of bit sequences. Consider for example \nthe coercion f c0 : E * \u00d7 E to E \u00d7 E * from Section 3.3. By interpreting c0 according to Figure 13 we \narrive at the following highly ef.cient function gE , which transforms bit codings of values of E * \u00d7 \nE into corresponding bit representations for E \u00d7 E * . gE (0d)=0d gE (1d)=1 fE (d) fE (0d)= d0 fE (1d)= \nlet (d1,d2)= splitE(d)in d1 1 fE (d2) The bit coded version of c1 : E * = 1+ E * \u00d7 E gives us a linear-time \nfunction hE that operates directly on bit codings of (parse trees) of strings in E * and transforms them \nto bit codings in 1+ E * \u00d7 E: hE (0d)=0d hE (1d)=1 gE (d) Note that h(a+b) is the bit coding coercion \nfrom H1 to H2. It trans\u00adforms 10 11 10 10 11 0 into 1 10 11 10 10 0 1 without ever materi\u00adalizing a string \nor value. 4.3 Tail-recursive \u00b5-types The presented bit sequences are compact, but their sizes depend \non the regular expression used. Thus it is necessary to use reason\u00adable regular expressions to obtain \ncompact bit sequences. In fact the most compact representations can be found only by generaliz\u00ading regular \nexpressions to tail-recursive \u00b5-types. We will use the remainder of this section to study this extension \nand the compres\u00adsion it allows. A common representation of strings over an alphabet S= {a1,...,a255} \nof 255 characters from the Latin-1 (ISO/IEC 8859-1:1998) alphabet employs a sequence of 8-bit bytes repre\u00adsenting \neach of the 255 different characters and uses the remain\u00ading byte to indicate the end of the string. \nThis gives a total size of 8 \u00b7 (n + 1) bits to represent a string of length n. Consider now the size \nof the bit sequence from Section 4.1 of a string under regular expression E * S where ES is a sum type \nholding all the 255 characters in S. This can be written in many ways using permutations and associations \nof the characters. For example, if we de.ne ES as a1 +(a2 +(a3 + ... + a255) ...) this means that the \nsize of the bit coding of ak is k bits long. This can be improved by ensuring that the type is balanced \nsuch that each path to a character has the same length. As there are 255 characters this means we will \nuse 8 bits to represent each characters, leaving one path unused (so one character only uses 7 bits). \nNow we can look at the space required for the bit coding of a string under type E * S is S. Since E * \nunfolded to 1+ ES \u00d7 E * S the representation of the empty string requires 1 bit, while the representation \nof other strings is 1 bit plus 8 bits for representing the .rst character, plus the bits to represent \nthe rest of the string for the type E * S. Thus up to 9 \u00b7 n +1 bits are used to represent a string of \nlength n. The reason why bit codings for regular types use one bit more per character is due to the very \nrestrictive recursion in regular expressions. The extra bit is used to say for each character that we \ndo not want to end the string yet. This is because we can only use the . * constructor to de.ne recursive \ntypes, and a regular expression E * always unfolds to 1+ E \u00d7 E *. Therefore we use one bit for each character \nto choose the right hand side after the unfold. This is equivalent to using a unary integer representation \nto state how many times the E inside the . * is used, which leaves room for optimization. We now generalize \nthe recursion to tail-recursive \u00b5-types in order to obtain more compact bit codings. Consider the language \nof expressions UnReg\u00b5 over a .nite alphabet S= {a1,...,an}: S a ::= 0 | 1 | a | a1 + a2 | a1 \u00d7 a2 | \u00b5X.a \n| X We de.ne the free variables of a to be the set of variables X that occur in a without a binder \u00b5X. \nIf there are no free variables in a then we say that a is closed. We call a tail-recursive if a1 is closed \nin all subterms of the form a1 \u00d7 a2. We can now de.ne the language Reg\u00b5 as the closed, tail\u00ad S recursive \nexpressions from UnReg\u00b5 S. We need to de.ne semantics, type-interpretation and inhabita\u00adtion for the \nnew expressions, but we can reuse the de.nitions from regular expressions (L[[]], T [[]],v : E), simply \nby adding new rules for the new \u00b5 and X constructs. We can extend the type-interpretation from De.nition \n6 with an environment s, and replace the de.nition of T [ E * ] with T [ X] s = s(X) and T [ \u00b5X.a] s \n= i=0 T [ a] s(i) where s(0) = s[X .\u00d8] and s(n+1) = s[X .T [ a] s(n) ]. Similarly, L[ .] can be extended \nto Reg\u00b5 S. Finally, we can use the inhabitation rules from Figure 1, except the fold rule is replaced \nwith the rule for \u00b5 in Figure 15. We can now prove that Reg\u00b5 expresses exactly the same lan\u00ad S guages \nas RegS: THEOREM 30 (Conservativity of tail-recursive \u00b5-types). 1. For all E . RegS there is a . Reg\u00b5 \nS such that {.at(v) |f v : E} = {.at(v) |f v : a}. 2. For all a . Reg\u00b5 there is E . RegS S such that \n{.at(v) |f v : a} = {.at(v) |f v : E}. PROOF (Sketch) The .rst statement is proved by encoding E * as \n\u00b5X.1+ a \u00d7 X where a is the encoding of E. The second statement is proved by .rst rewriting \u00b5-types to \nthe form \u00b5X.(a \u00d7 X + \u00df), where X is not free in a or \u00df. Now it can be seen that L[ \u00b5X.(a \u00d7 X + \u00df)]] = \nL[ E * \u00d7 F ] where E is a regular expression encoding of a and F is an encoding of \u00df. 0 The equivalence \nof regular expressions with right-regular gram\u00admars is well known (Chomsky 1959). Tail-recursive \u00b5-types \nare like right-regular grammars, but do not exactly correspond to them: tail-recursive \u00b5-types lack mutual \nrecursion, but offer lo\u00adcally scoped recursion, where grammars only provide top-level recursion. 5 Even \nthough Reg\u00b5 expresses exactly the same languages as S RegS, the new expressions allow us to de.ne (a \n+ b + c) * as \u00b5X.(1 + a \u00d7 X)+(b \u00d7 X + c \u00d7 X) and thus saving us one bit per character we need to express. \nUsing this optimization the representation of any string with respect to the generalized regular expression \ntype aS will use eight bits per Latin-1 character plus eight bits to terminate the string. This is exactly \nthe same size as the standard Latin1-representation. In fact the bit-representations for this type will \nbe exactly the same as the bit-representations for the standard Latin-1 representation if the same permutation \nof characters is chosen in aS. It may not seem very impressive to reinvent the Latin-1 rep\u00adresentation \nthis way, but it can guarantee that bit coding uses at most as much space as the Latin-1 representation. \nThe bene.t of bit coding comes when we no longer consider all Latin-1 strings, 5 Milner (1984) presents \na sound and complete axiomatization of behav\u00adioral equivalence of \u00b5-terms denoting labeled transition \nsystems. Behav\u00adioral equivalence is properly weaker than regular expression equivalence, however. Crucially, \ndistributivity E \u00d7 (F + G)= E \u00d7 F + E \u00d7 G does not hold. but a subset speci.ed by a regular expression. \nIn this case the bit codings will generally be more compact. The ultimate example of this is when the \nregular expression allows exactly one string. For example the bit sequence of abcbcba under regular expression \na \u00d7 b \u00d7 c \u00d7 b \u00d7 c \u00d7 b \u00d7 a uses zero bits since its value contains no inl /inr -choices. Of course the \nprogram needs to know the regular expression in order to interpret the bit sequences, but that can be \nshared across interpretation of multiple bit sequences. We end this section with two examples showing \nthe bit sizes of strings under different regular expressions. EXAMPLE 31. In the table below Z denotes \na designated end-of\u00adstring character; and characters a, b, c their 8-bit Latin-1 codings. Regular expression \nRepresentation Size Latin1 abcbcbaZ 64 S * 1a1b1c1b1c1b1a0 64 ((a + b) + (c + d)) * 1001011101011101011000 \n22 ((a + b) + c) * 10010111101111011000 20 a \u00d7 (b + c) * \u00d7 a 10111011100 11 a \u00d7 b \u00d7 c \u00d7 b \u00d7 c \u00d7 b \u00d7 a \n0 The following is a more realistic example, where we also con\u00adsider the data size before and after \ntext compression. EXAMPLE 32 (Sizes for XML record collection string). Consider the following regular \nexpression, corresponding to a regular XML schema (\u00d7 and associativity have been omitted for simplicity). \n<CATALOG> (<CD><TITLE>S * </TITLE><ARTIST>S * </ARTIST> <COUNTRY>S * </COUNTRY><COMPANY>S * </COMPANY> \n<PRICE>S * </PRICE><YEAR>S * </YEAR> </CD>)* </CATALOG> This regular expression describes an XML-format \nfor representing a list of CDs. We have found the sizes for representing a speci.c list containing 26 \nCDs to be as follows: Uncompressed Compressed Latin-1 32760 7248 bit representation using ES 11187 6654 \nbit representation using aS 9947 6552  As we can see, there is almost a factor 3 reduction in the space \nrequirement when using the regular expression speci.c bit codings. The bene.t is reduced by compression \nof the bit codes with bzip (Seward) but an 8% space reduction is still obtained. 5. Discussion Regular \nexpressions are fundamental to computer science with numerous applications in semi-structured text processing, \nnatural language processing, program analysis, graph querying, shortest path computation, compilers, \nprogram veri.cation, bioinformatics and more. Salomaa (1966) and, independently, Aanderaa provided the \n.rst sound and complete axiomatizations of regular expression equiva\u00adlence , based on a unique .xed point \nrule, with Krob (1990), Pratt (1990) (for extended regular expressions), and Kozen (1994) pro\u00adviding \nalternatives in the 90s. Recently, coinductive axiomatizations based on .nitary cases of Rutten s coinduction \nprinciple (Rutten 1998) for simulation re\u00adlations have become popular: Grabmayer (2005) for regular expres\u00adsions; \nChen and Pucella (2004) and Kozen (2008) for Kleene Alge\u00adbra with Tests. Silva, Bonsangue and Rutten \nshow how to special\u00adize their coalgebraic framework to regular languages and how to translate regular \nlanguages into so-called deterministic expressions (Silva et al. 2010, Example 3.4). Such expressions \nappear to corre\u00adspond to (nondeterministic) linear forms (Antimirov 1996), which in turn represent E-free \nnondeterministic automata. (Their particu\u00adlar translation may generate exponentially bigger expressions \nthan the original regular expressions, however, which may limit practi\u00adcal applicability.) Grabmayer \ns axiomatization (Grabmayer 2005) is based on Br\u00adzozowski derivatives (Antimirov 1996; Brzozowski 1964), \nwhich allow automata constructions and pairwise regular expression rewriting (Antimirov 1995; Ginzburg \n1967; Lu and Sulzmann 2004) to be understood as proof search. In this paper we present a declarative \ncoinductive axioma\u00adtization of regular expression containment, generalizing Grab\u00admayer s algorithmic \ncoinductive axiomatization of regular expres\u00adsion equality.6 Ours is the .rst axiomatization of regular \nexpression containment with a Curry-Howard-style computational interpreta\u00adtion of containment proofs \nas functions (coercions) operating on regular expressions read as regular types. Like Kozen s (noncoin\u00adductive) \naxiomatization, but unlike Salomaa s (noncoinductive) and Grabmayer s (coinductive) it is parametrically \ncomplete for in.nite alphabets: If E[X] = F [X] for all X, then there is a parametric polymorphic coercion \nc : .X. E[X] = F [X]. Frisch and Cardelli (2004) were, as far we know, the .rst to state the precise \nconnection between regular expressions as languages and regular expressions as types (Section 2.2 in \nthis paper). Coinductive axiomatizations with a Curry-Howard style com\u00adputational interpretation have \nbeen introduced by Brandt and Hen\u00adglein (1998) for recursive type equivalence and subtyping, ex\u00adpounded \non by Gapeyev et al. (2002), as an alternative to the ax\u00adiomatization of Amadio and Cardelli (1993), \nwhich uses the unique .xed point principle. In this fashion, classical uni.cation closure can be understood \nas proof search for a type isomorphism, and the product automaton construction of Kozen et al. (1995) \nas search for the coercion embedding a subtype into another type. Di Cosmo et al. (2005) provide a coinductive \ncharacterization of recursive subtyping with associative-commutative products, but it is not a proper \naxiomatization since it appeals directly to bisimilarity.7 Re\u00adcursive type isomorphisms have also been \nstudied by Abadi and Fiore (1996); Fiore (2004, 1996) and have been used for stub gen\u00aderation (Auerbach \net al. 1999). The (.nitary) coinduction rule in its most general form is G,P f P G f P It requires a \nside condition for soundness, which is usually speci.c to the syntax of the formulae P and the particular \nlogical system at hand. Numerous syntactic variations may be possible, and care must be applied to retain \nsoundness; e.g. by not allowing a transi\u00adtivity rule (Gapeyev et al. 2002). This work represents the \nend of a quest for a general semantic side condition, at least for contain\u00adment formulae: Interpret a \nproof of containment computationally as a function, and let the side condition be that the conclusion \n(now with explicit proof object) under this interpretation be (hereditarily) total. For regular expression \ncontainment, hereditary totality turns out to be undecidable, but it justi.es the soundness of our side \ncon\u00additions S2 and S4, which each yield sound and complete axiom\u00adatizations of regular expression containment. \nTheir disjunction is expressive enough to facilitate a compositional encoding of deriva\u00adtions in Salomaa \ns, Kozen s and Grabmayer s axiomatizations. 6 Here, declarative and algorithmic are used in the same \nsense as in Benjamin Pierce s book Types and Programming Languages, MIT Press. 7 Bisimilarity is coinductively \nde.ned (that is, as a greatest .xed point), but not necessarily .nitarily as required in an ordinary \n(recursive) axiomatiza\u00adtion. Brandt and Henglein (1998, Section 4.3) discuss how the .ni\u00adtary coinduction \nrule can be understood as a rule for .nding a .nite set of formulae that are intrinsically consistent. \nIntuitively this cor\u00adresponds to constructing proofs of such formulae that are .nite, but may be circular: \nthey may contain occurrences of formulae to be proved as assumptions. This is also the essence of circular \ncoinduction for behavioral equivalences (Rosu and Goguen 2000). Rosu and Lucanu (2009) provide a general \nproof-theoretic frame\u00adwork for applying circular coinduction soundly. It allows marking certain equations \nas frozen to prevent them from being applied pre\u00admaturely, which would lead to unsound conclusions. Our \napproach is fundamentally different in the following sense: We allow circu\u00adlar equations without any \nrestriction. An equation is interpreted as a pair of potentially partial functions, whose de.nition depends \non the particular (circular) proof of the equation. An equation is valid (sound), however, only if the \ntwo functions making up its compu\u00adtational interpretation are total. Our regular-expression speci.c bit \nrepresentation of (parse trees of) strings corresponds to composing regular expression parsing with the \n.attening function of Jansson and Jeuring (1997), who attribute the technique to work in the 80s on text \ncompression with syntactic source information models (see references in their paper). Bit coding captures \nthe idea that only choices made the tags of sum types need to be encoded, which is also the key idea \nof oracle-based coding in proof-carrying code (Necula and Rahul 2001). Our direct compilation of containment \nproofs to bit manipulation functions appears to be novel, however. It should be noted that compaction \nby bit coding is orthogonal to statistical text data compression. As we have illustrated, combining them \nmay yield signi.cantly shorter bit strings than either technique by itself. Type-and coercion-theoretic \ntechniques appear to be applica\u00adble to regular expression types (Hosoya et al. 2005a,b; Nielsen 2008; \nSulzmann and Lu 2007) and other nonregular extensions, no\u00adtably context-free languages.8 This remains \nto be investigated thor\u00adoughly, however. Note that regular expression types are a proper extension of \nregular expressions as types. There are also numerous practically motivated topics to inves\u00adtigate: Inference \nof regular type containments and search for prac\u00adtically ef.cient coercions implementing them; disambiguation \nof regular expressions by annotating them; instrumenting automata constructions for fast input processing \nthat yields parse trees; and more. We hope that this may lead the way to putting logic and com\u00adputer \nscience into a new generation of expressive, generally appli\u00adcable high-performance regular expression \nprocessing tools. Acknowledgements We would like to thank the anonymous reviewers for their compre\u00adhensive \ncritical and constructive comments, and for detailed recom\u00admendations for improvement. Any remaining \nproblems are solely the authors responsibility. The alphabetically .rst author is grate\u00adful to Eijiro \nSumii, Yasuhiko Minamide, Naoki Kobayashi, and At\u00adsushi Igarashi for jointly solving the question of \ndecidability of hereditary totality (Theorem 21). We would like to thank Dexter Kozen for sharing many \ninsights on Kleene Algebras as part of a mini-course held at DIKU, as well as for ideas and suggestions \nfor the present and for further work. Thanks also to Alexandra Silva for explaining and commenting her \nwork on Kleene coalgebras and its relation to regular expressions. Finally we would like to thank the \nparticipants of our graduate course Topics in Programming Languages: Theory and Practice of Regular Expressions \n, held at DIKU in Spring 2010, for exploring some of the applications of regular expressions as types. \n8 Completeness is out of the question, of course, since context-free grammar containment is not recursively \nenumerable. References M. Abadi and M. P. Fiore. Syntactic considerations on recursive types. In Proc. \n1996 IEEE 11th Annual Symp. on Logic in Computer Science (LICS), New Brunswick, New Jersey. IEEE Computer \nSociety Press, June 1996. R. M. Amadio and L. Cardelli. Subtyping recursive types. ACM Transac\u00adtions \non Programming Languages and Systems (TOPLAS), 15(4):575 631, September 1993. V. Antimirov. Rewriting \nregular inequalities. In Proc. 10th International Conference, FCT 95 Dresden, Germany, volume 965 of \nLecture Notes in Computer Science (LNCS), pages 116 125. Springer-Verlag, August 1995. V. Antimirov. \nPartial derivatives of regular expressions and .nite automaton constructions. Theor. Comput. Sci., 155(2):291 \n319, 1996. ISSN 0304\u00ad3975. doi: http://dx.doi.org/10.1016/0304-3975(95)00182-4. V. M. Antimirov and P. \nD. Mosses. Rewriting extended regu\u00adlar expressions. Theor. Comput. Sci., 143(1):51 72, 1995. doi: http://dx.doi.org/10.1016/0304-3975(95)80024-4. \nJ. S. Auerbach, C. Barton, M. Chu-Carroll, and M. Raghavachari. Mock\u00adingbird: Flexible stub compilation \nfrom pairs of declarations. In ICDCS, pages 393 402, 1999. C. Brabrand and J. Thomsen. Typed and unambiguous \npattern matching on strings using regular expressions. In Proc. 12th International ACM SIGPLAN Symposium \non Principles and Practice of Declarative Pro\u00adgramming (PPDP), 2010. M. Brandt and F. Henglein. Coinductive \naxiomatization of recursive type equality and subtyping. Fundamenta Informaticae, 33:309 338, 1998. J. \nA. Brzozowski. Derivatives of regular expressions. J. ACM, 11(4):481 494, 1964. ISSN 0004-5411. doi: \nhttp://doi.acm.org/10.1145/321239.321249. H. Chen and R. Pucella. A coalgebraic approach to Kleene algebra \nwith tests. Theor. Comput. Sci., 327(1-2):23 44, 2004. N. Chomsky. On certain formal properties of grammars*. \nInformation and control, 2(2):137 167, 1959. J. H. Conway. Regular Algebra and Finite Machines. Printed \nin GB by William Clowes &#38; Sons Ltd, 1971. ISBN 0-412-10620-5. R. Di Cosmo, F. Pottier, and D. Remy. \nSubtyping recursive types modulo associative commutative products. In Proc. Seventh International Con\u00adference \non Typed Lambda Calculi and Applications (TLCA 2005), 2005. M. Fiore. Isomorphisms of generic recursive \npolynomial types. SIGPLAN Not., 39(1):77 88, 2004. ISSN 0362-1340. doi: http://doi.acm.org/10.1145/982962.964008. \nM. P. Fiore. A coinduction principle for recursive data types based on bisim\u00adulation. Information and \nComputation, 127:186 198, 1996. Conference version: Proc. 8th Annual IEEE Symp. on Logic in Computer \nScience (LICS), 1993, pp. 110-119. A. Frisch and L. Cardelli. Greedy regular expression matching. In \nProc. 31st International Colloquium on Automata, Languages and Program\u00adming (ICALP), volume 3142 of Lecture \nnotes in computer science, pages 618 629, Turku, Finland, July 2004. Springer. V. Gapeyev, M. Y. Levin, \nand B. C. Pierce. Recursive subtyping revealed. J. Funct. Program., 12(6):511 548, 2002. A. Ginzburg. \nA procedure for checking equality of regular expres\u00adsions. J. ACM, 14(2):355 362, 1967. ISSN 0004-5411. \ndoi: http://doi.acm.org/10.1145/321386.321399. C. Grabmayer. Using proofs by coinduction to .nd traditional \nproofs. In Proc. 1st Conference on Algebra and Coalgebra in Computer Science (CALCO), number 3629 in \nLecture Notes in Computer Science (LNCS). Springer, September 2005. J. Hopcroft and J. Ullman. Introduction \nto Automata Theory, Languages, and Computation. Addison-Wesley, 1979. H. Hosoya, A. Frisch, and G. Castagna. \nParametric polymorphism for XML. In J. Palsberg and M. Abadi, editors, POPL, pages 50 62. ACM, 2005a. \nISBN 1-58113-830-X. H. Hosoya, J. Vouillon, and B. C. Pierce. Regular expression types for XML. ACM \nTrans. Program. Lang. Syst., 27(1):46 90, 2005b. Institute of Electrical and Electronics Engineers (IEEE). \nStandard for information technology Portable Operating System Interface (POSIX) Part 2 (Shell and utilities), \nSection 2.8 (Regular expression notation). New York, 1992. IEEE Standard 1003.2. P. Jansson and J. Jeuring. \nPolyp a polytypic programming language exten\u00adsion. In Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages (POPL), page 482. ACM, 1997. S. C. Kleene. Representation of events \nin nerve nets and .nite automata. Automata Studies, 1956. D. Kozen. A completeness theorem for Kleene \nalgebras and the algebra of regular events. Information and Computation, 110(2):366 390, May 1994. D. \nKozen. Kleene algebra with tests. Transactions on Programming Languages and Systems, 19(3):427 443, May \n1997. D. Kozen. On the coalgebraic theory of Kleene algebra with tests. Technical report, Computing and \nInformation Science, Cornell University, March 2008. URL http://hdl.handle.net/1813/10173. D. Kozen, \nJ. Palsberg, and M. Schwartzbach. Ef.cient recursive subtyping. Mathematical Structures in Computer Science \n(MSCS), 5(1), 1995. Con\u00adference version presented at the 20th Annual ACM SIGPLAN-SIGACT Symp. on Principles \nof Programming Languages (POPL), 1993. D. Krob. A complete system of b-rational identities. In M. Paterson, \neditor, ICALP, volume 443 of Lecture Notes in Computer Science, pages 60 73. Springer, 1990. ISBN 3-540-52826-1. \nK. Z. M. Lu and M. Sulzmann. Rewriting regular inequalities. In Proc. Second Asian Symposium, APLAS 2004, \nTaipei, Taiwan, November 4-6, 2004, volume 3302 of Lecture Notes in Computer Science (LNCS), pages 57 \n73. Springer, November 2004. R. Milner. A complete inference system for a class of regular behaviours. \nJ. Comput. Syst. Sci., 28(3):439 466, 1984. G. C. Necula and S. P. Rahul. Oracle-based checking of untrusted \nsoftware. In POPL, pages 142 154, 2001. L. Nielsen. A coinductive axiomatization of XML subtyping. Graduate \nterm project report, DIKU, University of Copenhagen, 2008. V. Pratt. Action logic and pure induction. \nIn Proc. Logics in AI: European Workshop JELIA, volume 478 of Lecture Notes in Computer Science (LNCS), \npages 97 120. Springer, 1990. G. Rosu and J. Goguen. Circular coinduction. In Proc. International Joint \nConference on Automated Reasoning, 2000. G. Rosu and D. Lucanu. Circular coinduction: A proof theoretical \nfounda\u00adtion. In Proc. 3rd Conference on Algebra and Coalgebra in Computer Science (CALCO), number 5728 \nin Lecture Notes in Computer Science (LNCS), pages 127 144. Springer, September 2009. ISBN 978-3-642\u00ad03740-5. \nJ. J. M. M. Rutten. Automata and coinduction (an exercise in coalgebra). In D. Sangiorgi and R. de Simone, \neditors, CONCUR, volume 1466 of Lecture Notes in Computer Science, pages 194 218. Springer, 1998. ISBN \n3-540-64896-8. A. Salomaa. Two complete axiom systems for the algebra of regu\u00adlar events. J. ACM, 13(1):158 \n169, 1966. ISSN 0004-5411. doi: http://doi.acm.org/10.1145/321312.321326. J. Seward. Bzip. URL http://www.bzip.org/. \nA. Silva, M. M. Bonsangue, and J. J. M. M. Rutten. Non-deterministic Kleene coalgebras. Logical Methods \nin Computer Science, 6(3), 2010. URL http://arxiv.org/abs/1007.3769. M. Sulzmann and K. Z. M. Lu. XHaskell \n-adding regular expression types to Haskell. In O. Chitil, Z. Horv\u00b4ath, and V. Zs\u00b4ok, editors, IFL, volume \n5083 of Lecture Notes in Computer Science, pages 75 92. Springer, 2007. ISBN 978-3-540-85372-5. G. Winskel. \nThe Formal Semantics of Programming Languages: An Intro\u00adduction. Foundations of Computing series. MIT \nPress, Feb. 1993. ISBN 0-262-23169-7.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>We present a new sound and complete axiomatization of regular expression containment. It consists of the conventional axiomatization of concatenation, alternation, empty set and (the singleton set containing) the empty string as an idempotent semiring, the fixed- point rule <i>E</i>* = 1 + <i>E</i> &#215; <i>E</i>* for Kleene-star, and a general coinduction rule as the only additional rule.</p> <p>Our axiomatization gives rise to a natural computational interpretation of regular expressions as simple types that represent parse trees, and of containment proofs as <i>coercions</i>. This gives the axiom- atization a Curry-Howard-style constructive interpretation: Containment proofs do not only certify a language-theoretic contain- ment, but, under our computational interpretation, constructively transform a membership proof of a string in one regular expression into a membership proof of the same string in another regular expression.</p> <p>We show how to encode regular expression equivalence proofs in Salomaa's, Kozen's and Grabmayer's axiomatizations into our containment system, which equips their axiomatizations with a computational interpretation and implies completeness of our axiomatization. To ensure its soundness, we require that the computational interpretation of the coinduction rule be a hereditarily total function. Hereditary totality can be considered the mother of syn- tactic side conditions: it \"explains\" their soundness, yet cannot be used as a conventional side condition in its own right since it turns out to be undecidable.</p> <p>We discuss application of <i>regular expressions as types</i> to bit coding of strings and hint at other applications to the wide-spread use of regular expressions for substring matching, where classical automata-theoretic techniques are <i>a priori</i> inapplicable.</p> <p>Neither regular expressions as types nor subtyping interpreted coercively are novel <i>per se</i>. Somewhat surprisingly, this seems to be the first investigation of a general proof-theoretic framework for the latter in the context of the former, however.</p>", "authors": [{"name": "Fritz Henglein", "author_profile_id": "81100104232", "affiliation": "University of Copenhagen, Copenhagen, Denmark", "person_id": "P2509636", "email_address": "henglein@diku.dk", "orcid_id": ""}, {"name": "Lasse Nielsen", "author_profile_id": "81100582851", "affiliation": "University of Copenhagen, Copenhagen, Denmark", "person_id": "P2509637", "email_address": "lnielsen@diku.dk", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926429", "year": "2011", "article_id": "1926429", "conference": "POPL", "title": "Regular expression containment: coinductive axiomatization and computational interpretation", "url": "http://dl.acm.org/citation.cfm?id=1926429"}