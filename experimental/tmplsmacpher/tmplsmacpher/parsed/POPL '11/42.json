{"article_publication_date": "01-26-2011", "fulltext": "\n EigenCFA: Accelerating Flow Analysis with GPUs Tarun Prabhu, Shreyas Ramalingam, Matthew Might, Mary \nHall University of Utah, Salt Lake City, Utah, USA {tarunp,sramalin,might,mhall}@cs.utah.edu Abstract \nWe describe, implement and benchmark EigenCFA, an algorithm for accelerating higher-order control-.ow \nanalysis (speci.cally, 0CFA) with a GPU. Ultimately, our program transformations, re\u00adductions and optimizations \nachieve a factor of 72 speedup over an optimized CPU implementation. We began our investigation with \nthe view that GPUs accelerate high-arithmetic, data-parallel computations with a poor tolerance for branching. \nTaking that perspective to its limit, we reduced Shiv\u00aders s abstract-interpretive 0CFA to an algorithm \nsynthesized from linear-algebra operations. Central to this reduction were abstract Church encodings, \nand encodings of the syntax tree and abstract domains as vectors and matrices. A straightforward (dense-matrix) \nimplementation of EigenCFA performed slower than a fast CPU implementation. Ultimately, sparse-matrix \ndata structures and operations turned out to be the critical accelerants. Because control-.ow graphs \nare sparse in prac\u00adtice (up to 96% empty), our control-.ow matrices are also sparse, giving the sparse \nmatrix operations an overwhelming space and speed advantage. We also achieved speedups by carefully permitting \ndata races. The monotonicity of 0CFA makes it sound to perform analysis operations in parallel, possibly \nusing stale or even partially-updated data. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: \nProcessors and Optimization General Terms Languages Keywords abstract interpretation, EigenCFA, program \nanalysis, .ow analysis, lambda calculus, GPU, CPS, matrix  1. Introduction GPUs excel at obtaining speedups \nfor algorithms over continu\u00adous domains with low-control, high-arithmetic kernels. Flow anal\u00adyses [22, \n24, 25], on the other hand, tend to be .xed-point algo\u00adrithms over discrete domains with high-control, \nlow-arithmetic ker\u00adnels, such as abstract interpretations [7, 8]. At .rst glance, GPUs seem ill-suited \nto accelerating .ow analyses. Yet, with a shift in al\u00adgorithmic perspective and the right data structures, \nGPUs make the bedrock .ow analysis for higher-order programs 0CFA nearly two orders of magnitude faster. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n11, January 26 28, 2011, Austin, Texas, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0490-0/11/01. \n. . $10.00 1.1 Motivation After nearly a quarter century, higher-order control-.ow analy\u00adsis [11, 25] \nremains an important analysis for highly-optimizing, whole-program compilers of functional languages. \nYet, the anal\u00adysis also remains stubbornly expensive. Even one of its sim\u00adplest formulations, 0CFA, is \nstill nearly cubic in complexity: O(n 3/ log n) [6, 19]. With CPU clock cycles no longer growing, this \ncomplexity places a de facto upper bound on the size of programs that can be analyzed in a reasonable \namount of time. To analyze large programs, higher-order control-.ow analysis must exploit the ever-increasing \nparallelism available on modern systems. Toward that end, we develop a GPU-accelerated algorithm for \n0CFA EigenCFA that achieves a factor of 72 speedup over existing CPU techniques.  1.2 High-level methodology \nTo develop EigenCFA, we imagined the GPU as a platform for accelerating non-branching, data-parallel \nalgorithms composed entirely of linear-algebra operations. So, we reduced Shivers abstract-interpretive \n0CFA to a data-parallel, non-branching algo\u00adrithm composed entirely of matrix operations: matrix multiplica\u00adtion, \nmatrix addition and matrix transposition. There are .ve key insights to this reduction: 1. Canonicalization \nto binary continuation-passing style. To achieve good data-parallelism on a GPU, we need control\u00ad.ow \nuniformity among the GPU threads, i.e.,toavoid branch\u00ading operations. In .ow analysis, the key step to \nparallelize is the propagation of .ow information at each call site. To eliminate branching from this \npropagation routine, we transform our pro\u00adgrams into a canonical form: binary continuation-passing style \n(binary CPS). In binary CPS, every call site provides two argu\u00adments, and every function accepts two \narguments. By eliminat\u00ading the need to discriminate on the instruction type (there is only one: function \ncall) and on the number of arguments, binary CPS eliminates branching from the propagation sub-routine. \n 2. Abstract Church encodings. One could use Church encodings to reduce every program con\u00adstruct (e.g., \nif, letrec, set!) to binary continuation-passing style. But, these desugarings obscure control-.ow. For \ninstance, desugaring set! requires a global store-passing transform, and the Y combinator fogs up recursion. \nAn abstract Church en\u00adcoding exploits the approximation in 0CFA so that the encoded  program has the \nsame abstract control-.ow as the original pro\u00adgram, if no longer the same concrete behavior. For example: \n(set! x 10) is equivalent (as far as 0CFA is concerned) to: (let ((x 10)) #void)  3. Matrix-vector encoding \nof the syntax tree. We use binary continuation-passing style to provide uniformity to program syntax, \nbut we still need a GPU-friendly way to encode the syntax tree of a program. We encode the syntax tree \nof a program as a collection of selector functions, which are themselves represented as matrices. Individual \nprogram terms are then encoded as vectors. (We ll write ((t)) to mean the vector that encodes term t.) \nFor instance, every call site has three components: the procedure expression, its .rst argument expression \nand its second argument expression. So, there are three selector matrices that operate on call sites: \nFun, Arg1 and Arg2. For example, for a call site (fe1 e2): (((fe1 e2))) \u00d7 Fun = ((f)) (((fe1 e2))) \u00d7 \nArg1 = ((e1)) (((fe1 e2))) \u00d7 Arg2 = ((e2)). 4. Matrix encoding of the abstract store. In 0CFA, the abstract \nstore (also known as the abstract heap) maps variable names to sets of values. It is the primary data \nstructure used during the execution of 0CFA, so it must be en\u00adcoded in a GPU-friendly way. Fortunately, \nit is straightforward to represent this data structure as a matrix. One axis of the store matrix represents \nvariables; the other axis represents lambda terms. If the entry for variable i, lambda j is non-zero, \nthis indi\u00adcates that (a closure over) lambda j may get bound to variable i. Thus, if the matrix s is \nan abstract store in matrix form, and v is a variable, then the vector ((v)) \u00d7 s describes the possible \nvalues of the variable v. 5. Linear-algebraic encoding of the transfer function. Once the syntax tree \nof the program is described in terms of selection matrices, the next step is to describe the action of \nthe small-step transition relation for 0CFA in terms of linear\u00adalgebraic operations. Examination of the \nsmall-step transition relation (Section 2.4) .nds only three operations beyond syn\u00adtactic selection: \nfunction lookup, join over functions and func\u00adtional extension. We reduce function lookup to matrix-vector \nmultiplication, join to matrix addition and functional extension to a combination of matrix addition, \nmatrix multiplication and matrix transposition.  1.3 Key insights for acceleration: Sparseness and races \nFor the implementation, there are two insights that lead to accel\u00aderation: a sparse-matrix representation \nof the abstract store, and a tolerance of benign (monotonic) races that allows the analysis of call sites \nin parallel. 1. Exploiting sparseness. In practice, most control-.ow graphs are sparse. In our matrix \nencoding of the store, the sparseness of this matrix is linked to the sparseness of the control-.ow graph. \nThus, sparse-matrix algorithms have a performance advantage over dense-matrix algorithms for all but \nthe most pathological programs: programs which tend toward completeness in their control-.ow graphs \nprograms in which any point may jump to any other point. 2. Exploiting monotonicity to permit benign \nrace conditions. When analyzing call sites in parallel, they may both attempt to read from and/or write \nto the same location in the abstract store. Fortunately, the monotonic growth of the abstract store during \nan abstract transition guarantees us that these races are benign: if properly engineered, once an entry \nis set to a non-zero value in the abstract store, no other thread will set it back to zero.  Monotonicity \nalso guarantees soundness when a thread works with a stale or partially updated store.  1.4 Contributions \n Our primary contribution is the formulation of GPU-accelerated .ow analysis. To the best of our knowledge, \nEigenCFA is the .rst such formulation.  Our secondary claims are the reductions that made this formu\u00adlation \npossible: the encoding of the syntax tree as selection ma\u00adtrices, abstract Church encodings and the reduction \nto linear algebra of the abstract semantics for 0CFA.  1.5 Outline Theory and implementation cleave \nthis work in halves: Theory. Section 2 is a brief review of the small-step formulation of Shivers 0CFA \nfor continuation-passing style. Section 3 de\u00ad.nes EigenCFA, a linear-algebraic formulation of 0CFA. Sec\u00adtion \n4 describes abstract Church encodings program transfor\u00admations that are meaning-preserving only for the \nabstract se\u00admantics of our analysis, which more precisely and ef.ciently handle language constructs such \nas mutation, recursion, termi\u00adnation, basic values and conditionals. Implementation. Section 5 describes \nhow we map the algorithm for EigenCFA down to a (CUDA-enabled) GPU. In fact, this section describes three \ndifferent approaches for performing the mapping: two of these end up slower than or only as fast as the \nCPU version, but the .nal implementation, which uses sparse matrices, achieves the desired speedup. Section \n6 gives the results of our empirical evaluation. We tested two (GPU) implementations of EigenCFA against \ntwo (CPU) implementations of 0CFA, and found a fac\u00adtor of 72 speedup.  2. Background: Binary CPS and \n0CFA We are going to accelerate Shivers s original 0CFA for continuation\u00adpassing style (CPS). To enable \nhigh performance on the GPU, we are going to specialize it for a canonicalized form: binary continuation-passing \nstyle. In this section, we ll de.ne binary CPS and brie.y review 0CFA. Readers familiar with 0CFA may \nwish to skip this section. Our de.nition of 0CFA is taken from Might and Shivers s recent small-step \nreformulations. (We refer readers to [20] for details such as abstraction maps and proofs of soundness.) \nBinary CPS is a canonicalized variant of the continuation\u00adpassing style .-calculus in which every procedure \naccepts ex\u00adactly two arguments. We use binary, as opposed to unary, CPS because the CPS transform on \nlambda terms introduces an addi\u00adtional argument the continuation argument and CPS cannot use Currying \nto handle multiple arguments, since it violates the no procedure may return principle. We are using binary, \ninstead of variadic, CPS because we want to eliminate branching from the small-step transition relation \nwe re about to de.ne; branches inter\u00adfere with data-parallel single-instruction, multiple-thread (SIMT) \noperations in the GPU. The uniformity in binary CPS also has the bene.t of making our forthcoming matrix \nencodings of syntax trees predictably struc\u00adtured. This predictable structure is amenable to the arithmetic \nopti\u00admizations and compressions presented in section 5. 2.1 Binary CPS The grammar for binary CPS contains \ncalls and expressions, and expressions are either lambda terms or variables:  call . Call ::= (fe1 e2) \nf, e . Exp = Var + Lam v . Var is a set of identi.ers lam . Lam ::= (. (v1 v2) call).  2.2 Concrete \nsemantics The simplest small-step concrete semantics for binary CPS uses just three domains in its state-space, \nS: . . S= Call \u00d7 Env . . Env = Var -Clo clo . Clo = Lam \u00d7 Env, and one transition rule, (.) . S \u00d7 S: \n([[(fe1 e2)]],.) . (call,.''),where ([[(. (v1 v2) call)]],.')= E(f, .) clo1 = E(e1,.) clo2 = E(e2,.) \n.'' = .'[v1 . clo1,v2 . clo2], where the function E : Exp \u00d7 Env -Clo evaluates expressions: E(v, .)= \n.(v) E(lam,.)=(lam,.).  2.3 Abstract state-space for 0CFA Might and Shivers reformulated 0CFA as an \nabstract interpretation of the concrete semantics just given [20]. The core of the abstrac\u00adtion is the \nelimination of environments from the semantics, and the use of an abstract store to represent all environments. \nSo, abstract states ( S ) pair the current call site with the abstract store, and clo\u00adsures lose their \nenvironments: . . Store S= Call \u00d7 Ss . S= Var . S Store Lams L . S= P (Lam). Lams We assume the natural \npartial orders on these sets: lambda sets are ordered by inclusion, abstract stores are ordered point-wise \nby inclusion, and abstract states use a product ordering. For instance, ( s1 U s 2)(v)= s1(v) . s 2(v). \n 2.4 0CFA Abstract Semantics The transition relation for the abstract semantics, (.) . S \u00d7 S , mirrors \nthat of the concrete semantics: ([[(fe1 e2)]],s ) . (call,s '),where [[(. (v1 v2) call)]] .E (f, s ) \nL1 = E(e1,s ) L2 = E(e2,s ) ' . . s = s U [v1 L1,v2 L2], as does the argument evaluator, E : Var \u00d7 \nS Store .P (Lam): E(v, s )= s(v) E(lam,s )= {lam} . A notable change in the abstract semantics is the \nnondeterminism that results from branching to the set of possible lambda terms for the procedure argument \n(note the appearance of .). 2.5 Computing 0CFA To compute classical, .ow-insensitive 0CFA with the small-step \ntransition relation, we need a family of functions that computes the output store with respect to each \ncall site call, f call Store : S. S Store: f call ( s)= s ' :(call,s ) . ( ,s '). Then, we can construct \nthe pass function, F Store . S : SStore, which performs one full pass of the analysis by considering \nthe ef\u00adfect on the store of every call site in the program, call1,..., calln: F ( s)=(f call1 .\u00b7\u00b7\u00b7. f \ncalln )( s). Because the function F is continuous and monotonic, and the height of the abstract store \nlattice is .nite, the result of 0CFA is least .xed point of the function F : F n lfp(F )= (..) for some \n.nite n, Store where the bottom store maps everything to the empty set: ..= .v.\u00d8. Store Complexity If \nthe function F adds one entry to the abstract store per application, there can be at most |Var|\u00d7|Lam| \napplications before it saturates the abstract store and must terminate. Since the cost of each application \nis proportional to |Call|, the complexity of this 0CFA is cubic, as expected.  3. EigenCFA: A linear \nencoding of 0CFA In this section, we discuss our linear-algebra encoding of both binary CPS and 0CFA. \nIn brief, individual program terms will be represented as vectors. The structure of the syntax tree will \nbe compiled into static selection matrices that operate on these term vectors. Finally, the pass function \n( F ) from 0CFA will be encoded as a function operating on stores represented as matrices. Running example. \nThroughout the remainder of this paper, all of the examples will be with respect to this program (the \ncall-sites c1, c2, ... are explicitly labelled for future reference): '' ((.1 (v1 v1)(v1 v1 v1)c1 ) ''' \n(.2 (v2 v2)(v2 v2 v2)c2 ) (.3 (v3 v3')(v3 v3 v3)c3 ))c4 and the same abstract store, s : s (v1)= {.2} \ns (v2)= {.2} s (v3)= {.2} s (v1')= {.3} s (v2')= {.3} s (v3')= {.3} . We do this to improve presentation \nand to emphasize the dif\u00adferences between each data representation strategy in Sec\u00adtion 5. 3.1 Encoding \nterms as vectors We encode each program term (a lambda term, a variable or a call site) as a vector over \nthe set {0, 1}. Every term in the program will  have a unique vector representation.1 For convenience, \nwe write the vector encoding of term t as ((t)). We ll have vectors of two lengths: expression vectors \n(of length |Exp|) and call vectors (of length |Call|). Formally: - . Exp = {0, 1}|Exp| le . - . lCall \n= {0, 1}|Call| call . -. - . lv . Var . Exp --. - . l lam . Lam . Exp. We can assign each expression \na number from 1 to |Exp|,sothat the expression vector with 1 at slot i is the expression vector for expression \ni. We can do likewise for call sites. More speci.cally, variables are represented as vectors of size \n|Exp| (notably not of size |Var|). .-terms are also represented as vectors of size |Exp| (also notably \nnot of size |Lam|). The .rst |Var| entries of an expression vector represent variables, and the last \n|Lam| entries represent lambda terms. Running example. Here is the expression vector representing lambda \nterm .2: vvvv3 v2 v1 .3 .2 .1 321 0000000 1 0 And, here is the expression vector representing variable \nv2: vvvv3 v2 v1 .3 .2 .1 321 0000100 0 0 !! !! !! Running example. The Fun matrix determines the function \nbeing applied at a call site for our running example: v ! 3 v ! 2 v ! 1 v3 v2 v1 .3 .2 .1 c1 . 0 0 0 \n0 0 1 0 0 0 . c2 . 0 1 0 0 0 0 0 0 0 . c3 . 0 0 0 1 0 0 0 0 0 . c4 0 0 0 0 0 0 0 0 1 Running example. \nIn order to look up the function being ! appliedatcallsitelabelled ,wemultiplythevectorrepre-c2! Funsentingwiththe \nmatrix.Theresultisthevectorrepre-c2 ! ! senting the variable v2' : ! cccc1234 !  0100 vvvv3 v2 v1 .3 \n.2 .1 321 .. 00000100 0 . 01000000 0 . \u00d7 .. 00010000 0 00000000 1 v3 v2 v1 v3 v2 v1 .3 .2 .1 =010000000 \n It s worth asking why we don t split expressions into two vector types one for variables, and one for \nlambda terms. We took this approach because it allows us to eliminate branching from argu\u00adment evaluation \nlater on. As it is formulated in the abstract seman\u00adtics, the argument evaluator E must look up its expression \nargument if it is a variable and return its argument if it is a lambda term. We ll be able to construct \na single linear operator that has the effect of the argument evaluator E no branching necessary. 3.2 \nEncoding the syntax tree as matrices To encode the syntax tree, we ll use selectors encoded as static \nma\u00adtrices to destructure syntax terms. That is, given a term vector ((t)), to .gure out the syntactic \nchildren of term t, we ll have a matrix for each type of child (e.g. .rst formal parameter, second argu\u00adment) \nthat maps ((t)) to that child. For instance, the matrix Call maps lambda terms to their call site, so \nthat (((. (v1 v2) call))) \u00d7 Call = ((call)). There are six static syntax matrices: - . - . Fun : Call \n. Exp (function applied in a call site) - . - . Arg1 : Call . Exp (the .rst argument in a call site) \n- . - . Arg2 : Call . Exp (the second argument in a call site) --. - . Call : Lam . Call (the call site \nof a .-term) --. - . Var1 : Lam . Exp (the.rstformalofa .-term) --. - . Var2 : Lam . Exp (the second \nformal of a .-term) These matrices are constructed once per program and remain constant through the \nduration of the analysis. 1 We could formulate the analysis more generally in terms of any orthogonal \nbasis vectors for terms, but there doesn t seem to be any performance advantage to doing so. 3.3 Encoding \n.ow sets as vectors 0CFA manipulates .ow sets; .ow sets are sets of lambda terms (S Lams). We also need \nto represent these as linear-algebraic values. A natural encoding of .ow sets is to use bit vectors of \nlength |Lam|: --. l= {0, 1}|Lam| L . Lam* . In some sense, we appear to be breaking with the uniform \nrepre\u00adsentation of expressions by creating a special encoding of lambda --. terms. However, it is more \naccurate to think of the set Lam* as rep\u00adresenting sets of abstract closures than sets of lambda terms. \nMore\u00adover, we can (and do) ef.ciently interconvert values between the --. --. set Lam and the set Lam* \nsimply by adding or ignoring 0-padding on the front of the vector. For a set of lambda terms, L , we \nwrite their vector encoding as ((L )), and we expect the following property to hold: (({lam1,..., lamn})) \n= ((lam1)) + \u00b7\u00b7\u00b7 + ((lamn)), where the operator + is element-wise Boolean-OR.  3.4 Encoding abstract \nstores as matrices Now that we have a representation for syntactic domains and for .ow sets, we need \na matrix representation for the abstract store. Fortunately, the store is a map from variables to .ow \nsets, and a linear operator (encoded as a matrix) is a natural way of represent\u00ading such a map. We do \nhave a design choice here, and the right answer is not immediately obvious. We could use a |Var|-by-|Lam| \nmatrix to represent stores; but we ll be able to eliminate a branch during argument evaluation if we \nmake all stores into slightly larger |Exp|\u00adby-|Lam| matrices. So, formally: - . --. s . Store = Exp . \nLam* ,  We write the matrix-encoding of an abstract store s as ((s )). And, we de.ne the encoding by \nrelating abstract and matrix stores: ((lam)) \u00d7 s = ((lam))((v)) \u00d7 ((s )) = ((s (v))), where the operator \n\u00d7 is actually Boolean matrix multiplication. According to these rules, the lower portion of all stores \n(the part that corresponds the lambda portion of expression vectors) must be the identity matrix. Theorem \n3.2 (Store Update Theorem). ((s U [v1 . L 1,v2 . L 2])) = ((s )) + ((v1))T \u00d7((L 1)) + ((v2))T \u00d7((L 2)). \nProof. By the prior two lemmas. 3.4.2 Running example: Store update This example shows all the moving \nparts for store update. Running example. The matrix representation of the store is: .3 .2 .1 . . v ! \n3 1 0 0 v ! 2 . . . v1 .3 .2 . . . . . . . . . . . 1 . . . 0 1 0 0 . . . 1 0 1 0 . . . 0 0 0 . . . . \n. . . . . . . .1 0 0 1  Under this encoding of stores, the evaluation of an expression takes a single \nmatrix multiplication: Theorem 3.1. ((E (e, s ))) = ((e)) \u00d7 ((s )). Proof. By case-wise analysis and \nthe rules for the encoding. . ........... . ........ ... . ........... . ........... !! . ........ ... \nRunning example. To update with .2 .owing to variable v2' , ' wemultiplythevectorsrepresenting andandaddthe \n.v22 . ........... result, sfrag, to the current store: . ........... .3 .2 .1 v3 0 v3 000 . ........... \nv2 1 v2 010 . ........... .. .... . ... .............. .........321 !! !! v1 0 \u00d7 010= v1 000 .3 0 .3 \n000 .2 0 .2 000 .1 0 .1 000 sfrag scurr snew v3 000 100 100 v2 010 100 110 . ... ... ... . ... ... \n... . ... ... ... v1 000+ 010 = 010 .3 000 100 100 .2 000 010 010 .1 000 001 001 Running example. Here \ns the look-up of variable v2' : .3 .2 .1 . . v ! 3 1 0 0 v ! 3 0 v ! 2 1 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 v1 0 .3 0 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00d7 \nv ! 2 . . . v1 .3 .2 . . . . . . . . . . . 1 . . . 0 1 0 0 . . . 0 0 1 0 . . . 1 0 0 . . . . . . . . \n. . . .1 0 0 1 .3 .2 .1 = 1 0 0   3.5 Linear encoding of the transfer function Earlier, we constructed \na family of transfer functions, f call ,which produced the effect of the transition relation on the store \nfor a given call site. We can construct an equivalent linearized function, fcall : Store . Store, ' fcall \n(s)= s l L = ((call)) \u00d7 Fun \u00d7 s l L1 = ((call)) \u00d7 Arg1 \u00d7 s 3.4.1 Updates on the store l L2 During the \nabstract transition, we make updates to stores of the form: l lv1 = L \u00d7 Var1 = ((call)) \u00d7 Arg2 \u00d7 s s \n' lv2 = L \u00d7 Var2 = s U [v1 . L 1,v2 . L 2]. l  We need matrix operations that correspond to this update \noperation. First, we have to construct a store representing a single map\u00adping: [v . L ]. Fortunately, \nmatrix transposition and matrix multi\u00adplication make this straightforward: Lemma 3.1. (([v . L ])) = \n((v))T \u00d7((L )) + ((.Store )). It s also the case that Boolean-OR acts as join on stores: Lemma 3.2. \n((s 1 U s 2)) = ((s 1)) + ((s 2)). And, these two lemmas give us store update: 'T T s = s +lv1 \u00d7 Ll1+lv2 \n\u00d7 Ll2.  3.6 Soundness We can prove that the linearized transfer function is equivalent to the traditional \none, under our matrix-vector encoding: Theorem 3.3 (Soundness). ((f call ( s))) = fcall ((s )). Proof. \nThe proof proceeds bidirectionally. First, show that every entry in ((f call ( s))) must be in fcall \n((s )),and then vice versa.In  both directions, it s easiest to split into cases: the one in which the \nentry exists in s , and the one in which it is fresh. 3.7 EigenCFA: The Algorithm Algorithm 1 gives \nthe high-level algorithm for EigenCFA. The algorithm assumes that the store s is empty at the start. \nwhile s changes do foreach call do // Lookup function and arguments in call l L := (((call)) \u00d7 Fun) \u00d7 \ns l L1 := (((call)) \u00d7 Arg1) \u00d7 s l L2 := (((call)) \u00d7 Arg2) \u00d7 s l // Formal arguments of function, L lv1 \n:= (Ll\u00d7 Var1) \u00d7 s lv2 := (Ll\u00d7 Var2) \u00d7 s // Update store s := s + lv1 T \u00d7 Ll1 + lv2 T \u00d7 Ll2 . . Bind \nL1 to v Bind L2 to v! end end Algorithm 1: EigenCFA  4. Abstract Church encodings It s possible to \ntransform any program into binary CPS using mech\u00adanisms like Church encodings and the Y combinator. Some \nen\u00adcodings, like transforming the term (let ((ve)) body) into the term ((. (v) body) e) are benign. But, \nbecause some en\u00adcodings transform data-.ow into control-.ow, they obscure the control-.ow of the original \nprogram. Even Church-encoded inte\u00adgers raise thorny control-.ow issues. Yet, handling forms like set! \nor letrec as special cases in the transfer function is not practi\u00adcal: this would force conditional tests \nand branching, disrupting the data-parallelism that we have carefully engineered and protected. To avoid \nbranching but preserve precision, we turn to abstract Church encodings. An abstract Church encoding is \na program transformation that is meaning-preserving for an abstract seman\u00adtics, but not for the concrete \nsemantics. Caution The abstract Church encodings in this section work for the abstract semantics of 0CFA \nand the simple linear model of EigenCFA. Some of the forthcoming GPU optimizations require alphatization \nof the program, a constraint that these encodings violate. Where violations occur, we will note how to \nadapt. 4.1 Abstracting termination as non-termination We actually need abstract Church encodings to handle \nprogram ter\u00admination: notice that our CPS language has no halt form. So to encode halt, we ll use non-termination; \nthat is, we use the non\u00adterminating program Omega as the abstract encoding for termina\u00adtion. In other \nwords, we apply the following rewrite rule after con\u00adversion to a binary CPS that contains a halt primitive: \nhalt =. (. (ab) ((. (fg) (fff)) (. (fg) (fff)) (. (f g) (f f f)))) This works because once an abstract \ninterpreter hits Omega, that branch won t contribute any more changes to the abstract store, so the abstract \ninterpretation can reach a .xed point. 4.2 Abstracting mutation as binding To Church encode a construct \nlike set!,we dhaveto(1) perform cell boxing on all mutable variables, and then (2) eliminate all cells \nwith a store-passing-style transformation. These kinds of global transforms alter the control-.ow and \ndata-.ow behavior of the program. Yet, 0CFA has a single abstract store that represents all program environments. \nAs a result, let and set! have exactly the same effect on the abstract store; so we can apply a rewriting \nrule: (set! ve) =. (let ((ve)) #void) From the abstract store s perspective, these terms are equivalent. \n 4.3 Encoding recursion as mutation Now that we can handle mutation, we can avoid using the Y com\u00adbinator \n(or a grisly polvariadic, mutually recursive variant thereof) to handle recursion. Letrec normally desugars \ninto letsand set!s. But, since let has the same effect as set!, we can ac\u00adtually turn letrec into let \nwith an abstract rewrite rule: (letrec ((v1 lam1) ... (vn lamn)) e) =. (let ((v1 lam1) ... (vn lamn)) \ne) 0CFA does not distinguish their effects on the abstract store. 4.4 Abstracting basic values as non-termination \nMost CFAs encode all numbers as a single abstract value. In fact, most even convert all basic values \ninto a single abstract value. We can do the same, using the halt function from before as the single abstract \nvalue. (Any attempt to apply a basic value shouldn t allow the program to continue normal execution.) \nAll primitive operations that operate on basic values ignore their arguments and return this basic value. \n 4.5 Abstractly encoding conditionals Most CFAs do not attempt to evaluate conditionals; their behavior \nis to always branch in both directions at an if. We could Church encode Booleans and conditionals, but \nthis introduces a level of in\u00addirection, as conditionals appear to interprocedurally .ow through their \nChurch-encoded condition. Or, we could exploit the fact that .ow sets merge in 0CFA to simulate the non-determinism \nof the conditional with the non-determinism of procedure call. So, after we CPS transform if forms, we \ncan abstractly encode them with a let-based rewrite: (if e call1 call2) =. (let ((next (. () call1 ))) \n(let ((next (. () call2 ))) (next))) It s safe to drop the conditional expression e, because after the \nCPS transform, this expression is atomic, and it makes no changes to the store. By the time the analysis \nreaches the call site (next), both continuations will have been bound to next in the abstract store. \n  5. Our GPU implementation narrative In this section, we discuss the details of mapping our high-level \nalgorithm for EigenCFA down to the nuts and bolts of a GPU implementation. It took effort to discover \nwhich optimizations and data structures were the right ones, so we will discuss both the right turns \nand the wrong ones on the road to EigenCFA. We present three iterations of our implementation: (1) na\u00a8ive, \n(2) dense and (3) sparse. We wrote a pre-processor in Racket which transformed the input program and \ngenerated the static syntax matrices. These matrices were passed to a C program which copied them into \nthe GPU memory and launched the CUDA (GPU) kernels that performed the analysis. (For a brief summary \nof CUDA and the high-level architecture of the GPU we targeted, see the appendix.) All three of our implementations \npicked up at this point.  5.1 Attempt 1: Na\u00a8ive implementation with CUBLAS Our .rst implementation of \nEigenCFA used NVidia s CUBLAS li\u00adbrary for linear algebra to perform the matrix operations. This im\u00adplementation \nwas an almost verbatim transliteration of Algorithm 1, and it turned out to be slower than our CPU implementation. \nWe identi.ed several problems: Dense matrix computations. Although most of the matrices in our analysis \nwere sparse, Running example. Here is bit-packed store update: 0 000 \u00d7 010 = 12801280 100 100 .= 1965811961861 \n(The . operator performs a bitwise-OR.) 5.2.2 Compressing static syntax matrices Each static syntax matrix \nis quadratic in the size of the program. the CUBLAS library is written for accelerating dense-matrix \noperations. Since the matrices are all quadratic in the size of the program, they consumed all of the \nparallel processing facilities of the GPU, even for small programs. Memory requirements. The matrices \nand vectors in EigenCFA only contain boolean values. Use of 4-byte .oats as required by the CUBLAS li\u00adbraries \nfor each element was wasteful. As we describe in Sec\u00adtion 5.2, using one bit per entry yielded large \nconstant-factor speedups and substantial memory savings. Redundant operations. The static syntax matrix \nlookups (each a large multiplication) were repeated in each iteration. Eventually, we were able to compress \nthese matrices and simplify their application.  5.2 Attempt 2: Bit-packed matrix implementation To \novercome some of the limitations of the na\u00a8ive approach, we Transferring them to and manipulating them \non the GPU imposes signi.cant time and memory costs. Fortunately, we can exploit the uniformity of binary \nCPS to compress them and optimize their application at the same time. By choosing the numbering scheme \nfor variables and lambda terms carefully, we compressed the operators Var1 and Var2 into small, explicit \nformulae. Speci.cally, if the lambda term .n is the nth lambda term, then in an expression vector, (n \n+ |Lam|) is the expression number of its .rst formal, and (n +2|Lam|) is the expression number of it \nsecond formal. Now, each expression vector has three distinct segments: the Lam segment for the lambda \nterms; the Var1 segment for formals in the .rst position; and the Var2 segment for formal in the second \nposition. With this scheme, we can determine ((vk)) and ((vk' )) from the representation of .k without \nany matrix multiplication. optimized the matrix and vector representations. 5.2.1 Bit-packing Since \nthe vectors and matrices contain Boolean entries, one obvious way to reduce the sizes of the matrices \nwas to use bit-vectors for terms and values, and bit-matrices for the abstract store and the syntactic \nmatrices. Once encoded as bit-packed data structures, we also gain word-level parallelism by shifting \nfrom Boolean to bitwise-logical operations. Running example. If vk and vk ' are, respectively, the .rst \nand second formal arguments for .k, these are the vectors representing .2, v2 and v2' : .2 .2 .1 ((.2)) \n= 0 v ! 3 1 v ! 2 0 v ! 1 v3 v2 v1 .3 .2 .1 ((v2)) = 0 0 0 0 1 0 0 0 0 ' ((v2)) = 0 1 0 0 0 0 0 0 0 \nRunning example. Here is the same store lookup from ear\u00ad lier, with bit-packed matrices: 0 128 . 1 196 \n0 58 0 1 = 100 Each column of the store has been compressed. To make this example small but interesting, \nwe assume that the size of each word is eight bits. The . operator is bitwise matrix\u00admultiplication that \nworks on our bit-packing scheme: the mul\u00adtiplication and addition in conventional matrix multiplication \nalgorithm become bitwise-AND and bitwise-OR. In our implementation, the word size is 32 bits the word \nsize The Call matrix disappears after making the observation that there is exactly one call site for \nevery .-term. That is, the same vector (suitably truncated) can represent both the .-term and its corresponding \ncall site. (When we shift to a sparse-matrix imple\u00admentation, we will be able to compress the remaining \nstatic syntax matrices.) Caution: Abstract Church encodings The compression strategy we have chosen implicitly \nalphatizes the program, so that each vari\u00adable has a unique lambda term that binds it. Some of the abstract \nChurch encodings carefully exploit the behavior of 0CFA on non\u00adalphatized code. There are two ways to \nresolve this problem: (1) more sophisticated abstract Church encodings or (2) a look-up ta\u00ad on the GPU \non which we ran the analysis. This means that all ble in the GPU implementation: vectors are padded to \nbring their size to the nearest multiple of 32. This bit-packing reduced the size of the matrices by \nalmost a factor of 32. (The word-alignment padding is why the reduction in size isn t always exactly \na factor of 32.) 1. The encoding approach adds a procedure for every variable v: write-v = (. (vq)(q)) \nAll mutation of the variable v must pass through this procedure.  2. The lookup-table approach creates \na 2|Lam|-entry table in We also padded the store to take into account this change in the which entry \nn contains the expression number of the .rst for-size of the vectors. mal and the entry n + |Lam| contains \nthe expression number of the second formal for the nth lambda term. Running example. The corresponding \n0-padded store is below. When creating the store, we need to add blank rows corresponding to the pads: \n.d .3 .2 .1 v ! d . 0 0 0 0 . . . . v ! 1 vd . . . v1 .d .3 .2 . . . . . . . . . . . . . . . . . . . \n. . . . 0 0 . . . 0 1 0 0 . . . 1 0 . . . 0 0 1 0 . . . 0 0 . . . 1 0 0 1 . . . 0 0 . . . 0 0 0 0 . . \n. . . . . . . . . . . . . . . . . . .1 0 0 0 1  5.2.3 Store update optimization Under the encoding \ndescribed in the previous section, the store is also logically divided into the same three equal-sized \nsegments: the Lam region, the Var1 region and the Var2 region. As a result, we determined that store \nupdate, which adds lv T \u00d7 Ll, can only modify a (known) third of the store: the region in which the variables \nin the vector lv live. (Recall that, in the abstract semantics, updates for .rst-argument formals are \ncarried out separately from updates for second-argument formals.) Exploiting this knowledge cuts the \nnumber of operations on update by two-thirds. 5.2.4 GPU-only optimizations A few optimizations in our \nsecond implementation apply only to the GPU: Reducing the number of kernel calls. The CPU initiates parallel \noperations on the GPU through ker\u00adnel invocations. Each kernel call has non-trivial overhead, so Shared \nmemory. In our implementation of matrix multiplication, each thread cal\u00adkernels should be combined whenever \nsynchronization between them is unnecessary. We performed the calculations for lL1 L, l culates the value \nof exactly one cell in the result. We arranged the threads so that all threads within a block write to \nadjacent and Ll2 (see Algorithm 1) within the same kernel. Combined with the static matrix compression \nand the store update opti\u00adcells within the same row of the result. This means that all of the mization, \nthe total number of kernel calls per iteration dropped threads will read from exactly the same part of \nthe vectors be\u00ad from14to3. ing multiplied. This yields signi.cant data reuse. So, we copied Constant \nmemory. Since we were iterating over all call sites in the program, there was strong locality in the \nsyntax matrix lookup, and this data was read-only on the GPU. Therefore, it was a good candidate to use \nthe GPU s constant memory, which unlike the global memory, is cached for low latency memory accesses. \nAlthough the constant memory on the GPU was not large enough to store the entire static matrix, we could \nleverage this locality and store only that part of the matrix which we knew would be used at any point \nof time. This increased speed by 40% compared to the scheme without constant memory. Aligning thirds \nof the vectors to word boundaries. In order to ef.ciently implement static syntax matrix compres\u00adsion \n(Section 5.2.2), we aligned each segment of the expression vectors Lam, Var1 and Var2 along word boundaries. \nthe relevant segments of the vectors into shared memory and observed a 25% reduction in the running-time. \n 5.2.5 Limitations of this approach To our surprise, these optimizations only barely beat the perfor\u00admance \nof our CPU implementation of 0CFA. Through debugging, we narrowed the cause of the poor performance down \nto a few prin\u00adcipal factors: Unnecessary operations. The .rst phase of the store update operation generates \na new store which contains new bindings; the second phase then adds this new store to the old store. \nNot only does this waste memory on a second store, the update only impacts a small number of locations, \nyet we pay a cost proportional to the size of the store twice. ! ! ! ! ! ! Running example. The .gure \nbelow shows 0-padded vectors where |Lam| =3 and the word size is 4. ((.2 .d .3 .2 .1 0 010 ((v2 \u00b7\u00b7\u00b7 v \n\u00b7\u00b7\u00b7 | vd v3 v2 v1 | \u00b7\u00b7\u00b7 . \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 0 \u00b7\u00b7\u00b7 | 0 010 | \u00b7\u00b7\u00b7 0 \u00b7\u00b7\u00b7 ((v 2 vvvv | \u00b7\u00b7\u00b7 v \u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7 . \u00b7\u00b7\u00b7 d 321 \n0 010 | \u00b7\u00b7\u00b7 0 \u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7 0 \u00b7\u00b7\u00b7 Failure to exploit superposition. EigenCFA can do things 0CFA can t: EigenCFA \ncan represent multiple terms, e.g., call sites, in the same vector, simply by setting additional bits. \nThis gives EigenCFA control over a dif\u00adferent kind of parallelism the ability to act on superimposed \nterms. Yet, our implementation ignores this ability. Unutilized parallelism. Since the analysis is .ow-insensitive, \nall of the call sites could be evaluated in parallel and their effects then merged. However, since we \nwere using dense-matrix operations, performing large matrix multiplications for each call site consumed \nthe storage capacity of the GPU. To process each call site required a return to the CPU to retrieve the \npartial solution from the GPU, and copy the input data for the next call site, sequentializing the computation. \n  5.3 Success: Sparse matrix implementation Before constructing our third implementation, we observed \nthat the abstract store tended to be sparse. In fact, 96% of all entries in the .nal store matrix contained \nzeros. And, on average, each variable was bound to about two lambda terms. Using a dense matrix to encode \nthe store is inef.cient. So, we switched to using a sparse matrix representation of the store, and this \noptimization turned out to be the critical accelerant for EigenCFA. Running example. The Fun matrix becomes \nthe vector: c1 c2 c3 c4 5138 5.3.3 Parallelizing call-site evaluation 5.3.1 A sparse representation \nThe sparse matrix representation we use is essentially ELL form, in which the sparse matrix has a .xed \nmaximum number of columns. Each row has a header indicating how many cells in that row are ac\u00adtive, and \nan auxiliary array indicates the column for each nonzero element.[5] ELL is best suited to sparse matrices \nthat have roughly comparable numbers of non-zero elements, and lends itself to com\u00adputationally ef.cient \naccess to the sparse matrix. This scheme is not as memory-ef.cient as the more commonly used CSR (Compressed \nSparse Row) representation [3], but it has a smaller memory foot\u00adprint than the dense representation \nand it retains, for our purposes, the performance advantages. We allocate our initial store matrix by \nallocating a .xed number of columns to each row in the store. Initially, we assume no .ow set will hold \nmore than 4% of all lambda terms in the program. If, during the course of the analysis, this number proves \nto be insuf.cient, we save the store, terminate the analysis and restart with a twice as many columns. \nThe header cell of each row is the index of the .rst free slot available in that row. Each entry in a \n.ow set row is the assigned number of the lambda term in the .ow set, e.g.,ifrow i contains n, then .n \nmay .ow to variable vi. Since the store-update operation was simpli.ed, we freed up enough GPU resources \nto parallelize each iteration across call sites.  5.3.4 Tolerating race conditions Since global synchronization \non a GPU is expensive, it s important to engineer store update to behave correctly in the presence of \nraces once call sites are analyzed in parallel. It will likely be the case that one GPU thread sees a \nstore has been only partially updated by another GPU thread. Fortunately, abstract transfer functions \nin 0CFA are monotonic, which means that .ow sets only get larger. More importantly, it means that 0CFA \nis information-preserving during each iteration: working with stale data does not affect the soundness. \nWe ensure that the store grows monotonically by writing to it only when a GPU thread has new information \nto add. And, for overall ef.ciency, we tolerate some rare inef.ciencies: a row may have more than one \ncopy of the same lambda term if they race on trying to add the same lambda at the same time. This prevents \nthreads with stale data from overwriting any changes to the store, without the cost of global synchronization. \nFor instance, let s say that two call sites c1 and c2 are being evaluated in parallel and c2 reads from \na row before c1 updates it. If the evaluation of c1 does not yield any new store bindings, nothing will \nbe written to the store and the updates of c2 will be preserved. In the next iteration, these updates \nwill be visible during the evaluation of c1 and the .nal result will still be correct. . ........ ... \nRunning example. We sketch the sparse store representation for our running example below: v ! 2 | 3 3 \nv ! 3 | 32 2 . .. ............... . ..... . ..... v1 2 | 2 .3 2 | 3 .2 2 | 2 .1 2 | 1 Robustness against \nraces from monotonicity We can argue more formally that 0CFA is robust in the presence of stale or partially \ncompleted stores and out-of-order analysis for call sites. Suppose that the current abstract store s \nis reset at some point to an arbitrar\u00adily chosen weaker store, s ' ,sothat s ' g s . This represents \nacting on stale or partially completed data. We can show that the analysis will still reach the most \nprecise result: Theorem 5.1. If s ' g lfp(F ) then F 8( s ' )=lfp( F ). Proof. By de.nition, s ' =lfp( \nF ) n s ' . Applying F 8 to both sides yields F 8( s ' )=lfp( F ) n F 8( s ' ). By Kleene s .xed\u00adpoint \ntheorem, F 8( s ' ) must also be a .xed point. Tarski-Knaster guarantees this .xed point will be equal \nto or greater than the least In order to add .2 to the .ow set for v1, we append the number corresponding \nto .2 to the row vector representing v1, now a linear\u00adtime operation. (It s a linear-time operation because \nwe check for membership before appending.) 5.3.2 Optimizing the static matrices In addition to sparsifying \nthe store, we also compressed the remain\u00ading static syntax matrices. Since every variable and .-term \nhas an assigned row in the store, when evaluating the function and argu\u00adment expressions of a given call \nsite, all EigenCFA needs is the corresponding row number in the store. With this insight, we re\u00adduced \nthe Fun, Arg1 and Arg2 matrices to look-up vectors of size |Call|. This eliminates the need to perform \na true matrix multi\u00adplication in the argument-lookup phase of Algorithm 1. .xed point. Thus, lfp(F ) \nn F 8( s ' )=lfp( F ). We can also analyze calls in any order also on partially com\u00adpleted or stale data. \nWe show this by proving that applying the transfer function for any call site to a point below the least \n.xed point remains consistent with the least .xed point: Theorem 5.2. If s ' g lfp(F ) then f call ( \ns ' ) g lfp(F ), for any call site call. Proof. By contradiction, and cases: (1) f call ( s ' ) ; lfp(F \n) and (2) f call ( s ' ) g lfp(F ) and f call ( s ' ) ; lfp(F ). 5.4 GPU-speci.c optimization: Texture \nmemory A .nal GPU-speci.c tweak led to additional speedup. The selection matrices (which had by now been \nreduced to a linear array) were all stored in texture memory, a way of designating read-only data in \nglobal memory so that it is cached for low-latency memory access. Comparison of implementations We did \nthis because texture data doesn t have the size restrictions of  100000 constant memory. The use of \ntexture memory resulted in a roughly 10000 20% speedup over the use of global memory. 1000 6. Empirical \nevaluation Time (seconds, log-scale) 100 10 1 0.1 In total, we created four implementations to evaluate \nand compare. In case it is not clear, EigenCFA and 0CFA have exactly the same precision, so time is the \nonly relevant metric for comparison. Ulti\u00ad mately, our fastest GPU implementation was a factor of 72 \nfaster than our fastest CPU implementation at scale. We compared two CPU implementations against two \nGPU implementations: CPU (S): A fast CPU implementation of 0CFA in Racket. We implemented a traditional \nsmall-step version of 0CFA accord\u00ading to best practices.  CPU (Sp): A sparse-matrix CPU implementation \nof EigenCFA in C. Since many of our optimizations to the sparse-matrix implementation were not GPU-speci.c, \nand many good sparse matrix algorithms for the CPU exist, we had to make sure that merely representing \n0CFA as a sparse matrix algorithm was not the sole cause of the speedup. We implemented this CPU version \nof the sparse-matrix algorithm in order to show that the speedups from the GPU mattered. In fact, this \nmatrix CPU version beat our Racket CPU version in performance, so we report our relative speedup against \nthis sparse-matrix CPU implementation instead.  GPU (D): A dense-matrix implementation of EigenCFA on \nthe GPU. It performed terribly. We implemented this version as a cautionary point of comparison. (This \nimplementation corre\u00adsponds to Attempt 2.)  GPU (Sp): A sparse-matrix implementation of EigenCFA on \nthe GPU. We implemented this version to measure the speedup over the fastest CPU version, which ended \nup being a factor of  72. 6.1 Platform We evaluated our implementations on an NVidia GTX-480 Fermi \nGPU with 1.5 GB of memory. The host machine (on which we also ran the CPU implementations) was equipped \nwith an Intel i7 CPU 0.01 0.001 0.0001 Number of terms Figure 1. Comparison of implementations: Running \ntimes versus number of terms. Note that the time axis is log-scale. The sparse\u00admatrix GPU implementation \nof EigenCFA clearly dominates. Terms GPU (Sp) CPU (Sp) CPU (S) GPU (D) 297 0.4 ms 0.1 ms 6.3 ms 7.7 ms \n545 0.7 ms 0.16 ms 18 ms 13.9 ms 1,041 1.15 ms 0.33 ms 74.3 ms 34.2 ms 2,033 2.27 ms 0.84 ms 433 ms 0.11 \ns 4,017 6.51 ms 4.2 ms 2.46 s 0.47 s 7,985 24.01 ms 34.7 ms 15.53 s 4.13 s 15,921 94.48 ms 0.4 s 1m 30s \n49.16 s 31,793 0.4 s 5.6 s 8m 30s 11m 43s 63,537 2.49 s 1m 24s 52 min 3hr 2m 127,025 21.3 s 20 min 5hr \n46m 8 222,257 2m 53s 3hr30 m 8 8 Table 1. Analysis running times versus number of terms. (8 running \nat 2.79 GHz. 6.2 Benchmarks To benchmark our implementations across a range of program sizes, we exploited \nVan Horn and Mairson s recent work on the complexity of control-.ow analyses [26, 27]. Because Van Horn \nand Mairson offer constructive proofs of complexity, we can extract a benchmark generator from these \nproofs that emits a program of the requested size that will be worst-case to analyze for k-CFA means \ngreater than 6 hours.) GPU vs. CPU for small programs 0.035 0.03 0.025  when k = 1. For 0CFA, the programs \nit generates are dif.cult but not worst-case. Might, Smaragdakis and Van Horn s recent work on the complexity \nof k-CFA also used this generator, and their empirical results provide a sense of its verisimilitude \nto hand\u00adwritten benchmarks [21]. In short, these benchmarks are about an Time (seconds) 0.02 0.015 0.01 \n order of magnitude harder to analyze than code written by humans. 6.3 Measurements Table 1 presents \nthe running time of the analysis for each imple\u00admentation. The .rst column is the number of terms in \nthe program. A time of 8 indicates that the analysis took longer than 6 hours to complete. Figure 1 is \na plot of the same data. Note the logarith\u00admic scale on the time-axis. As programs grow larger, the sparse\u00admatrix \nGPU implementation of EigenCFA has a signi.cant advan\u00adtage over the other implementations. 0.005 0 Number \nof terms Figure 2. Comparison of GPU and CPU at smaller program sizes. Up to 4500 program terms, the \nCPU implementation beats the GPU implementation.  In the interest of showing trade-offs, Figure 2 compares \nthe per\u00adformance of the sparse-matrix implementation of the analysis run\u00adning on the CPU and the GPU \nfor smaller program sizes. For small programs (less than 4500 terms), the CPU (barely) outperforms the \nGPU for the following reasons: Kernel invocation cost. There is a .xed cost associated with each invocation \nof a kernel on a GPU. At smaller program sizes, since the number of iterations and the time taken per \niteration is small, the start-up cost becomes a signi.cant percentage of the total running time. Faster \nCPU clock. The CPU clock runs twice as fast as the GPU (2.8 GHz against 1.4Ghz). In small programs, there \nisn t as much parallelism that can be exploited by the GPU. In this situation, the slower clock of the \nGPU limits speed. Fewer CPU iterations. On larger program sizes, the number of iterations is large enough \nthat in the limit, both the serial and parallel algorithms converge in exactly the same number of iterations. \nFor the smaller programs, this isn t true and the GPU often takes twice as many iterations to converge. \n Figure 3 is a plot of the speedup obtained by the sparse\u00admatrix GPU implementation of EigenCFA over \nthe CPU. Negative speedup values (barely visible at the left side of the chart) indicate a slowdown. \nFigure 2 shows this territory of the chart in more detail. Speedup of GPU over CPU Number of terms \nFigure 3. Speedup (in multiples) of GPU over CPU versus the number of program terms. 7. Related Work \nOur work in .ow analysis descends from a long line of research, beginning with the Cousots foundational \nwork on abstract inter\u00adpretation [7, 8], continuing through Jones work on control-.ow analysis [11] and, \nmost recently, through Shivers work on k-CFA [20, 24, 25]. The literature on parallelizing static analyses \nis sparse. To the best of our knowledge, there haven t been any efforts to parallelize higher-order control-.ow \nanalyses. Notable very recent contribu\u00adtions include Mendez-Lojo et al. s use of multicore architectures \nto accelerate classical inclusion-based points-to analysis [18] and Lopes et al. s framework for distributed \nsoftware model check\u00ading [16]. Given that 0CFA can also be phrased as an inclusion-based analysis, Mendez-Lojo \ns techniques are likely applicable. Most of the prior work in parallelizing static analyses have been \nfocused on data-.ow analysis for .rst-order, imperative pro\u00adgrams. Classically, data-.ow analyses have \nbeen performed by it\u00aderative methods such as those originally proposed by Kildall [12] and Hecht [10], \nelimination methods [2, 23] or hybrid algorithms which combine both approaches [17]. Most efforts at \nparallelizing data .ow analysis have involved parallelizing these algorithms. Zo\u00adbel [28] and Gupta [9] \nparallelized Allen-Cocke s interval analysis. Ryder [15] improved on the graph partitioning scheme from \n[2] which led to a more effective parallel algorithm. Lee implemented the hybrid approach for MIMD architectures \nusing message pass\u00ading between the processors [14]. Gupta et al. moved away from parallelizing existing \nalgorithms to developing speci.c techniques for parallel program analysis. In [13], they convert the \ncontrol-.ow graph of a program into a DAG and solve the data-.ow problem for each node of the graph in \nparallel. The GPU has been used to accelerate static analysis most no\u00adtably by Banterle and Giacobazzi \n[4] who implemented the Oc\u00adtagon Abstract Domain (OAD) on a GPU. Since OAD computa\u00adtions are based on \nmatrices, it was easily mapped to the GPU. 8. Future Work There are at least two promising avenues for \nfuture work: adapting our techniques to pointer analysis and exploiting superposition within EigenCFA \nto achieve a genuinely new kind of .ow analysis. 8.1 Accelerating pointer analyses Given recent results \nunearthing the connection between control\u00ad.ow and pointer analyses [21], we believe that the techniques \npresented here can be adapted to pointer analyses as well. Pointer analyses face additional hurdles, \nsuch as the fact that its small-step transition relation is much more complex, and a reduction to binary \nCPS seems out of the question. But these problem do not seem insurmountable. 8.2 Exploiting superposition \nUsing matrices to represent the store and vectors allows superpo\u00adsition to be used as another potential \nsource of parallelism. This parallelism is implicit in the structure of the matrices themselves, so it \ncould be exploited in addition to the explicit parallelism that was described in section 5.3. 8.3 Phased \nanalysis It took a lot of syntactic normalization and abstract Church en\u00adcodings to impose control-.ow \nuniformity on the abstract transfer function. We hypothesize that a better approach would be to al\u00adlow \ndifferent syntactic forms, and then process each kind of form in parallel to process all function calls \nin parallel, then all set! statements in parallel, then all conditionals, and so on, in each pass of \nthe analysis. A. The GPU and CUDA In this section, we provide a brief, high-level description of the \narchitecture, memory hierarchy and programming model of the GPU. CUDA (Compute Uni.ed Device Architecture) \nis a general purpose parallel computing architecture that leverages the parallel compute engine in NVidia \nGPUs. The parallel programming model provides three levels of abstraction a hierarchy of thread groups, \nshared memories and barrier synchronization [1]. Threads on a GPU are organized into groups called blocks. \nOnly threads within a block can be synchronized cheaply. Synchroniza\u00adtion of threads across blocks would \nhave to be done on the host which can be quite expensive. Threads are scheduled and executed in groups \nof parallel threads called warps. Divergence in execution paths of threads within a warp can have an \nadverse impact on per\u00adformance. The GPU has different types of memories:  Global memory: is large, \nglobal, read-write, uncached DRAM.  Shared Memory: is small, private to each block, read-write memory \nwhose access time is potentially as low as register access time  Constant Memory: is small, global, \nread-only3 and cached.  Texture Memory: is read-only3 and cached. It is optimized for 2D spatial locality \nwhich means that certain memory access patterns can be very ef.cient.  Data placement in memory and \ndivergent execution in threads must be carefully controlled to maximize performance. References [1] NVIDIA \nCUDA Programming Guide 2.3, Aug. 2009. [2] F. E. Allen and J. Cocke. A program data .ow analysis procedure. \nCommun. ACM, 19(3):137+, 1976. ISSN 0001-0782. [3] S. Balay, K. Buschelman, V. Eijkhout, W. Gropp, D. \nKaushik, M. Kne\u00adpley, L. C. McInnes, B. Smith, and H. Zhang. Sparse Matrices. In PETSc Users Manual, \nchapter 3, pages 55 66. 3.0.0 edition, Dec. 2008. [4] F. Banterle and R. Giacobazzi. A Fast Implementation \nof the Oc\u00adtagon Abstract Domain on Graphics Hardware. In H. R. Nielson and G. Fil\u00b4e, editors, Static \nAnalysis, volume 4634 of Lecture Notes in Com\u00adputer Science, chapter 20, pages 315 332. Springer Berlin \nHeidelberg, Berlin, Heidelberg, 2007. ISBN 978-3-540-74060-5. [5] N. Bell and M. Garland. Implementing \nsparse matrix-vector multipli\u00adcation on throughput-oriented processors. In SC 09: Proceedings of the \nConference on High Performance Computing Networking, Storage and Analysis, pages 1 11, New York, NY, \nUSA, 2009. ACM. ISBN 9781-605-5874-4-8. [6] S. Chaudhuri. Subcubic algorithms for recursive state machines. \nIn Proceedings of the 35th annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, \nPOPL 08, pages 159 169, New York, NY, USA, 2008. ACM. ISBN 9781-595-9368-9-9. [7] P. Cousot and R. Cousot. \nAbstract interpretation: A uni.ed lattice model for static analysis of programs by construction or approxima\u00adtion \nof .xpoints. In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, \npages 238 252, New York, NY, USA, 1977. ACM Press. [8] P. Cousot and R. Cousot. Systematic design of \nprogram analysis frameworks. In POPL 79: Proceedings of the 6th ACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Languages, pages 269 282, New York, NY, USA, 1979. ACM. [9] R. Gupta, L. Pollock, and \nM. L. Soffa. Parallelizing data .ow analysis. 1990. [10] M. S. Hecht. Flow Analysis of Computer Programs. \nElsevier Science Inc., New York, NY, USA, 1977. ISBN 0-444-00216-2. [11] N. D. Jones. Flow Analysis of \nLambda Expressions (Preliminary Ver\u00adsion). In Proceedings of the 8th Colloquium on Automata, Languages \nand Programming, pages 114 128, London, UK, 1981. Springer-Verlag. ISBN 3-540-10843-2. [12] G. A. Kildall. \nA uni.ed approach to global program optimization. In POPL 73: Proceedings of the 1st annual ACM SIGACT-SIGPLAN \nsymposium on Principles of programming languages, pages 194 206, New York, NY, USA, 1973. ACM. [13] R. \nKramer, R. Gupta, and M. L. Soffa. The Combining DAG: A Technique for Parallel Data Flow Analysis. IEEE \nTransactions on Parallel and Distributed Systems, 5(8):805 813, Aug. 1994. ISSN 1045-9219. [14] Y. F. \nLee, T. J. Marlowe, and B. G. Ryder. Performing data .ow analysis in parallel. In Supercomputing 90: \nProceedings of the 1990 ACM/IEEE conference on Supercomputing, pages 942 951, Los Alamitos, CA, USA, \n1990. IEEE Computer Society Press. ISBN 0\u00ad89791-412-0. [15] Y. F. Lee, B. G. Ryder, and M. E. Fiuczynski. \nRegion Analysis: A Parallel Elimination Method for Data Flow Analysis. IEEE Trans. Softw. Eng., 21(11):913 \n926, 1995. ISSN 0098-5589. [16] N. P. Lopes and A. Rybalchenko. Distributed and Predictable Software \nModel Checking. In Proc. of the 12th International Conference on Veri.cation, Model Checking, and Abstract \nInterpretation (VMCAI), Jan. 2011. [17] T. J. Marlowe and B. G. Ryder. An ef.cient hybrid algorithm for \nincremental data .ow analysis. In POPL 90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles \nof programming languages, pages 184 196, New York, NY, USA, 1990. ACM. ISBN 0-89791-343-4. [18] M. M\u00b4endez-Lojo, \nA. Mathew, and K. Pingali. Parallel inclusion\u00adbased points-to analysis. In Proceedings of the ACM International \nConference on Object Oriented Programming Systems languages and Applications, OOPSLA 10, pages 428 443, \nNew York, NY, USA, 2010. ACM. ISBN 9781-450-3020-3-6. [19] J. Midtgaard and D. Van Horn. Subcubic control \n.ow analysis algo\u00adrithm. Higher-Order and Symbolic Computation, To appear. [20] M. Might and O. Shivers. \nImproving .ow analyses via GCFA: Ab\u00adstract garbage collection and counting. In ICFP 06: Proceedings of \nthe Eleventh ACM SIGPLAN International Conference on Functional Programming, pages 13 25, New York, NY, \nUSA, 2006. ACM. ISBN 1-59593-309-3. [21] M. Might, Y. Smaragdakis, and D. V. Horn. Resolving and exploiting \nthe k-CFA paradox: illuminating functional vs. object-oriented pro\u00adgram analysis. In PLDI 10: Proceedings \nof the 2010 ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 305 315, \nNew York, NY, USA, 2010. ACM. ISBN 9781-450\u00ad3001-9-3. [22] J. Palsberg. Closure analysis in constraint \nform. ACM Transactions on Programming Languages and Systems, 17(1):47 62, Jan. 1995. ISSN 0164-0925. \n[23] B. G. Ryder and M. C. Paull. Elimination algorithms for data .ow analysis. ACM Comput. Surv., 18(3):277 \n316, 1986. ISSN 0360-0300. [24] O. Shivers. Control .ow analysis in Scheme. In Proceedings of the ACM \nSIGPLAN 1988 Conference on Programming Language Design and Implementation, volume 23, pages 164 174, \nNew York, NY, USA, July 1988. ACM. ISBN 0-89791-269-1. [25] O. G. Shivers. Control-Flow Analysis of Higher-Order \nLanguages. PhD thesis, Carnegie Mellon University, 1991. [26] D. Van Horn and H. G. Mairson. Relating \ncomplexity and precision in control .ow analysis. In ICFP 07: Proceedings of the 12th ACM SIG-PLAN International \nConference on Functional Programming, pages 85 96, New York, NY, USA, 2007. ACM. ISBN 9781-59593-815-2. \n[27] D. Van Horn and H. G. Mairson. Deciding k-CFA is complete for EXPTIME. In ICFP 08: Proceeding of \nthe 13th ACM SIGPLAN International Conference on Functional Programming, pages 275 282, New York, NY, \nUSA, 2008. ACM. ISBN 978-1-59593-919-7. [28] A. Zobel. Parallel interval analysis of data .ow equations. \nvolume II. The Penn State University press, Aug. 1990. 3 For code running on the GPU, this memory is \nread-only. Code running on the host, i.e. the CPU, can write to it.   \n\t\t\t", "proc_id": "1926385", "abstract": "<p>We describe, implement and benchmark EigenCFA, an algorithm for accelerating higher-order control-flow analysis (specifically, 0CFA) with a GPU. Ultimately, our program transformations, reductions and optimizations achieve a factor of 72 speedup over an optimized CPU implementation.</p> <p>We began our investigation with the view that GPUs accelerate high-arithmetic, data-parallel computations with a poor tolerance for branching. Taking that perspective to its limit, we reduced Shivers's abstract-interpretive 0CFA to an algorithm synthesized from linear-algebra operations. Central to this reduction were \"abstract\" Church encodings, and encodings of the syntax tree and abstract domains as vectors and matrices.</p> <p>A straightforward (dense-matrix) implementation of EigenCFA performed slower than a fast CPU implementation. Ultimately, sparse-matrix data structures and operations turned out to be the critical accelerants. Because control-flow graphs are sparse in practice (up to 96% empty), our control-flow matrices are also sparse, giving the sparse matrix operations an overwhelming space and speed advantage.</p> <p>We also achieved speedups by carefully permitting data races. The monotonicity of 0CFA makes it sound to perform analysis operations in parallel, possibly using stale or even partially-updated data.</p>", "authors": [{"name": "Tarun Prabhu", "author_profile_id": "81416597658", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2509673", "email_address": "tarunp@cs.utah.edu", "orcid_id": ""}, {"name": "Shreyas Ramalingam", "author_profile_id": "81479656970", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2509674", "email_address": "sramalin@cs.utah.edu", "orcid_id": ""}, {"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2509675", "email_address": "might@cs.utah.edu", "orcid_id": ""}, {"name": "Mary Hall", "author_profile_id": "81407592676", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2509676", "email_address": "mhall@cs.utah.edu", "orcid_id": ""}], "doi_number": "10.1145/1926385.1926445", "year": "2011", "article_id": "1926445", "conference": "POPL", "title": "EigenCFA: accelerating flow analysis with GPUs", "url": "http://dl.acm.org/citation.cfm?id=1926445"}