{"article_publication_date": "01-08-2014", "fulltext": "\n Optimal Dynamic Partial Order Reduction Parosh Abdulla Stavros Aronis Bengt Jonsson Konstantinos Sagonas \nDepartment of Information Technology, Uppsala University, Sweden Abstract Stateless model checking is \na powerful technique for program veri.cation, which however suffers from an exponential growth in the \nnumber of explored executions. A successful technique for reducing this number, while still maintaining \ncomplete coverage, is Dynamic Partial Order Reduction (DPOR). We present a new DPOR algorithm, which \nis the .rst to be provably optimal in that it always explores the minimal number of executions. It is \nbased on a novel class of sets, called source sets, which replace the role of persistent sets in previous \nalgorithms. First, we show how to modify an existing DPOR algorithm to work with source sets, resulting \nin an ef.cient and simple to implement algorithm. Second, we extend this algorithm with a novel mechanism, \ncalled wakeup trees, that allows to achieve optimality. We have implemented both algorithms in a stateless \nmodel checking tool for Erlang programs. Experiments show that source sets signi.cantly increase the \nperformance and that wakeup trees incur only a small overhead in both time and space. Categories and \nSubject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; D.2.5 [Software Engineering]: \nTesting and Debugging; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs General Terms Algorithms, Veri.cation, Reliability Keywords dynamic partial oder reduction; \nsoftware model check\u00ading; systematic testing; concurrency; source sets; wakeup trees 1. Introduction \nVeri.cation and testing of concurrent programs is dif.cult, since one must consider all the different \nways in which processes/threads can interact. Model checking addresses this problem by systematically \nexploring the state space of a given program and verifying that each reachable state satis.es a given \nproperty. Applying model checking to realistic programs is problematic, however, since it requires to \ncapture and store a large number of global states. Stateless model checking [7] avoids this problem by \nexploring the state space of the program without explicitly storing global states. A special run\u00adtime \nscheduler drives the program execution, making decisions on scheduling whenever such decisions may affect \nthe interaction between processes. Stateless model checking has been successfully implemented in tools, \nsuch as VeriSoft [8] and CHESS [20]. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. POPL 14, \nJanuary 22 24, 2014, San Diego, CA, USA. Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535845 \nWhile stateless model checking is applicable to realistic pro\u00adgrams, it suffers from combinatorial explosion, \nas the number of pos\u00adsible interleavings grows exponentially with the length of program execution. There \nare several approaches that limit the number of explored interleavings, such as depth-bounding and context \nbound\u00ading [19]. Among them, partial order reduction (POR) [3, 6, 21, 26] stands out, as it provides full \ncoverage of all behaviours that can oc\u00adcur in any interleaving, even though it explores only a representative \nsubset. POR is based on the observation that two interleavings can be regarded as equivalent if one can \nbe obtained from the other by swapping adjacent, non-con.icting (independent) execution steps. In each \nsuch equivalence class (called a Mazurkiewicz trace [17]), POR explores at least one interleaving. This \nis suf.cient for checking most interesting safety properties, including race freedom, absence of global \ndeadlocks, and absence of assertion violations [3, 6, 26]. Existing POR approaches are essentially based \non two tech\u00adniques, both of which reduce the set of process steps that are ex\u00adplored at each scheduling \npoint: The persistent set technique, that explores only a provably suf.cient subset of the enabled processes. \nThis set is called a persistent set [6] (variations are stubborn sets [26] and ample sets [3]).  The \nsleep set technique [6], that maintains information about the past exploration in a so-called sleep set, \nwhich contains processes whose exploration would be provably redundant.  These two techniques are independent \nand complementary, and can be combined to obtain increased reduction. The construction of persistent \nsets is based on information about possible future con.icts between threads. Early approaches analyzed \nsuch con.icts statically, leading to over-approximations and therefore limiting the achievable reduction. \nDynamic Partial Order Reduction (DPOR) [4] improves the precision by recording actually occurring con.icts \nduring the exploration and using this information to construct persistent sets on-the-.y, by need . DPOR \nguarantees the exploration of at least one interleaving in each Mazurkiewicz trace when the explored \nstate space is acyclic and .nite. This is the case in stateless model checking in which only executions \nof bounded length are analyzed [4, 8, 20]. Challenge Since DPOR is excellently suited as a reduction \ntech\u00adnique, several variants, improvements, and adaptations for different computation models have appeared \n[4, 15, 22, 24, 25]. The obtained reduction can, however, vary signi.cantly depending on several fac\u00adtors, \ne.g. the order in which processes are explored at each point of scheduling. For a particular implementation \nof DPOR (with sleep sets) [11], up to an order of magnitude of difference in the num\u00ad ber of explored \ninterleavings has been observed, when different strategies are used. For speci.c communication models, \nspecialized algorithms can achieve better reduction [25]. Heuristics for choosing which next process \nto explore have also been investigated without conclusive results [14].  Let us explain one fundamental \nreason for the above variation in obtained reduction. In DPOR, the combination of persistent set and \nsleep set techniques guarantees to explore at least one complete interleaving in each Mazurkiewicz trace. \nMoreover, it has already been proven that the use of sleep sets is suf.cient to prevent the complete \nexploration of two different but equivalent interleavings [10]. At .rst sight, this seems to imply that \nsleep sets can give optimal reduction. What it actually implies, however, is that when the algorithm \ntries an interleaving which is equivalent to an already explored one, the exploration will begin but \nit will be blocked sooner or later by the sleep sets in what we call a sleep-set blocked exploration. \nWhen only sleep sets are used for reduction, the exploration effort will include an arbitrary number \nof sleep-set blocked explorations. It is here where persistent sets enter the picture, and limit the \nnumber of initiated explorations. Computation of smaller persistent sets, leads to fewer sleep-set blocked \nexplorations. However, as we will show in this paper, persistent sets are not powerful enough to completely \nprevent sleep\u00adset blocked exploration. In view of these variations, a fundamental challenge is to develop \nan optimal DPOR algorithm that: (i) always explores the minimum number of interleavings, regardless of \nscheduling decisions, (ii) can be ef.ciently implemented and (iii) is applicable to a variety of computation \nmodels, including communication via shared variables and message passing. Contributions In this paper, \nwe present a new DPOR algorithm, called optimal-DPOR, which is provably optimal in that it always explores \nexactly one interleaving per Mazurkiewicz trace, and never initiates any sleep set-blocked exploration. \nOur optimal algorithm is based on a new theoretical foundation for partial order reduction, in which \npersistent sets are replaced by a novel class of sets, called source sets. Source sets are often smaller \nthan persistent sets and are provably minimal, in the sense that the set of explored processes from some \nscheduling point must be a source set in order to guarantee exploration of all Mazurkiewicz traces. When \na minimal persistent set contains more elements than the corresponding source set, the additional elements \nwill always initiate sleep-set blocked explorations. We will use a two-step approach to describe our \noptimal algo\u00adrithm. In the .rst step, we develop a simpler DPOR algorithm, called source-DPOR, which \nis based on source sets. It is derived by modi\u00adfying the classical DPOR algorithm by Flanagan and Godefroid \n[4] so that persistent sets are replaced by source sets. The power of source sets can already be observed \nin the algorithm source-DPOR: it achieves signi.cantly better reduction in the number of explored interleavings \nthan the classical DPOR algorithm. In fact, source-DPOR explores the minimal number of interleavings \nfor a large number of our benchmarks in Section 9. Although source-DPOR often achieves optimal reduction, \nit may sometimes encounter sleep-set blocked explorations. Therefore, in the second step, we combine \nsource sets with a novel mechanism, called wakeup trees, thus deriving the algorithm optimal-DPOR. Wakeup \ntrees control the initial steps of future explorations, imply\u00ading that optimal-DPOR never encounters \nany sleep-set blocked (i.e. redundant) exploration. An important feature of wakeup trees is that they \nare simple data structures that are constructed from already explored interleavings, hence they do not \nincrease the amount of exploration. On the other hand, they allow to reduce the number of explored executions. \nIn our benchmarks, maintenance of the wakeup trees reduces total exploration time when source-DPOR encounters \nsleep-set blocked explorations and it never requires more than 10% of additional time in the cases where \nthere are none or only a few sleep-set blocked explorations. Memory consumption is practically always \nthe same between the DPOR algorithms and the space cost of maintaining wakeup trees is very small in \nour experience. We show the applicability of our algorithms to a wide range of computation models, including \nshared variables and message passing, by formulating them in a general setting, which only assumes that \nwe can compute a happens-before relation (also called a causal ordering) between the events in an execution. \nFor systems with shared variables, the happens-before relation can be based on the variables that are \naccessed or modi.ed by events. For message passing systems, the happens-before relation can be based \non correlating the transmission of a message with the corresponding reception. Our approach allows to \nmake .ner distinctions, leading to better reduction, than many other approaches that de.ne a happens-before \nrelation which is based on program statements, possibly taking into account the local state in which \nthey are executed [3, 4, 6, 8, 14, 25, 26]. For instance, we allow a send transition to be dependent \nwith another send transition only if the order in which the two messages are received is signi.cant. \nWe have implemented both source-DPOR and optimal-DPOR as extensions for Concuerror [2], a stateless model \nchecking tool for Erlang programs. Erlang s concurrency model focuses primarily on message passing, but \nit is also possible to write programs which manipulate shared data structures. Our evaluation shows that \non a wide selection of benchmarks, including benchmarks from the DPOR literature, but more importantly \non real Erlang applications of signi.cant size, we obtain optimal reduction in the number of interleavings \neven with source-DPOR, therefore signi.cantly outperforming the original DPOR algorithm not only in number \nof interleavings but in total execution time as well. Organization In the next section, we illustrate \nthe basic new ideas of our technique. We introduce our computational model and formulation of the partial-order \nframework in Section 3. In Section 4 we introduce source sets. The source-DPOR algorithm is described \nin Section 5. We formalize the concept of wakeup trees in Section 6, before describing the optimal-DPOR \nalgorithm in Section 7. Implementation of the algorithms is described in Section 8, and experimental \nevaluation in Section 9. The paper ends by surveying related work and offering some concluding remarks. \n2. Basic Ideas In this section, we give an informal introduction to the concepts of source sets and wakeup \ntrees, and their improvement over existing approaches, using some small examples. p : q : r : write x; \n(1) read y; read z; read x; (2) read x; (3) Example 1: Writer-readers code excerpt. Source Sets In Example \n1, the three processes p, q, and r perform dependent accesses to the shared variable x. Two accesses \nare dependent if they access the same variable and at least one is a write. The accesses to y and z are \nnot dependent with anything else. For this program, there are four Mazurkiewicz traces (i.e. equivalence \nclasses of executions), each characterized by its sequence of accesses to x (three accesses can be ordered \nin six ways, but two pairs of orderings are equivalent since they differ only in the ordering of adjacent \nreads, which are not dependent). Any POR method selects some subset of {p, q, r} to perform some .rst \nstep in the set of explored executions. It is not enough to select only p, since then executions where \nsome read access happens before the write access of p will not be explored. In DPOR, assume that the \n.rst execution to be explored is p.q.q.r.r (we denote executions by the sequence of scheduled process \nsteps). A DPOR Initially: x := y := z := 0  p : s : x := 1; (1) r : q : m := y; (3) if m = 0 then z \n:= 1; (4) n := z; (5) l := y; (6) if n = 1 then if l = 0 then y := 1; (2) x := 2; (7) Example 2: Program \nwith control .ow. algorithm will detect the dependency between step (1) by p and step (2) by q, and note \nthat it seems necessary to explore sequences that start with a step of q. The DPOR algorithm will also \ndetect the dependency between (1) and (3) and possibly note that it is necessary to explore sequences \nthat start with a step of r. Existing DPOR methods guarantee that the set of processes explored from \nthe initial state is a persistent set. In short, a set P of processes is persistent in the initial state \nif in any execution from the initial state, the .rst step that is dependent with the .rst step of some \nprocess in P must be taken by some process in P . In this example, the only persistent set which contains \np in the initial state is {p, q, r}. To see this, suppose that, e.g. r is not in the persistent set P \n, i.e. P = {p, q}. Then the execution r.r contains no step from a process in P , but its second step \nis dependent with the .rst step of p, which is in P . In a similar way, one can see that also q must \nbe in P . In contrast, our source set-based algorithms allow S = {p, q}as the set of processes explored \nfrom the initial state. The set S is suf.cient, since any execution that starts with a step of r is equivalent \nto some execution that starts with the .rst (local) step of q. The set S is not a persistent set, but \nit is a source set. Intuitively, a set S of processes is a source set if for each execution E from the \ninitial state there is some process proc in S such that the .rst step in E that is dependent with proc \nis taken by proc itself. To see that {p, q} is a source set, note that when E is r.r, then we can choose \nq as proc, noting that r.r is not dependent with q. Any persistent set is also a source set, but, as \nshown by this example, the converse is not true. Our algorithm source-DPOR combines source sets with \nsleep sets, and will explore exactly four interleavings, whereas any algorithm based on persistent sets \nwill explore at least .ve (if the .rst explored execution starts with p), some of which will be sleep\u00adset \nblocked if sleep sets are used. If we extend the example to include n reading processes instead of just \ntwo, the number of sleep-set blocked explorations increases signi.cantly (see Table 2 in Section 8). \nSleep sets were introduced [6] to prevent redundant exploration. They are manipulated as follows: (i) \nafter exploring interleavings that begin with some process p, the process p is added to the sleep set, \nand (ii) when a process step that is dependent with p is executed, p is removed from the sleep set. The \neffect is that the algorithm need never explore a step of a process in the sleep set. In Example 1, for \ninstance, after having explored executions starting with p, the process p is added to the sleep set. \nWhen exploring executions that start with q, the process p is still in the sleep set after the .rst step \nof q, and should not be explored next, since executions that start with q.p are equivalent to executions \nthat start with p.q. Wakeup Trees As mentioned, by utilizing source sets, source-DPOR will explore a \nminimal number of executions for the program of Example 1. There are cases, however, where source-DPOR \nencounters sleep-set blocked exploration. We illustrate this by Example 2, a program with four processes, \np, q, r, s. Two events are dependent if they access the same shared Initial State  r : (3) q : (2) \nr : (3) q : (2) r : (4)  r : (4) s : (5)   s : (5) s : (6) q : (2)  s : (6) s : (7)  s : (7) p : \n(1)  Other traces q : (2) q : (2) Figure 1. Explored interleavings for Example 2. variable, i.e. x,y \nor z. Variables m,n,l are local. Each statement accessing a global variable has a unique label; e.g., \nprocess s has three such statements labeled (5), (6), and (7). Statements that operate on local variables \nare assumed to be part of the previous labeled statement. For example, label (6) marks the read of the \nvalue of y, together with the assignment on l, and the condition check on n. If the value of n is 1, \nthe condition check on l is also part of (6), which ends just before the assignment on x that has the \nlabel (7). Similar assumptions are made for the other local statements. Consider a DPOR algorithm that \nstarts the exploration with p, explores the interleaving p.r.r.s.s.s (marked in Figure 1 with an arrow \nfrom top to bottom), and then detects the race between events (1) and (7). It must then explore some \ninterleaving in which the race is reversed, i.e., the event (7) occurs before the event (1). Note that \nevent (7) will occur only if it is preceded by the sequence (3) -(4) -(5) -(6) and not preceded by a \nstep of process q. Thus, an interleaving that reverses this race must start with the sequence r.r.s.s. \nSuch an interleaving is shown in Figure 1 between the two chunks labeled SSB traces .  Having detected \nthe race in p.r.r.s.s.s, source-DPOR adds r to the source set at the initial state. However, it does \nnot remember that r must be followed by r.s.s to reverse the race. After exploring r, it may therefore \ncontinue with q. However, after r.q any explo\u00adration is doomed to encounter sleep-set blocking, meaning \nthat the exploration reaches a state in which all enabled processes are in the sleep set. To see this, \nnote that p is in the sleep set when exploring r, and will remain there forever in any sequence that \nstarts with r.q (as explained above, it is removed only after the sequence r.r.s.s.s). This corresponds \nto the left chunk of SSB traces in Figure 1. Optimal-DPOR solves this problem by replacing the backtrack \nset with a structure called a wakeup tree. This tree contains initial fragments of executions that are \nguaranteed not to encounter sleep set blocking. In the example, Optimal-DPOR algorithm will handle the \nrace between (1) and (7) by adding the sequence r.r.s.s.s to the wakeup tree. The point is that after \nr.r.s.s.s, the process p has been removed from the sleep set, and so sleep set blocking is avoided. 3. \nFramework In this section, we introduce the technical background material. First, we present the general \nmodel of concurrent systems for which the algorithms are formulated, thereafter the assumptions on the \nhappens-before relation, and .nally the notions of independence and races.  3.1 Abstract Computation \nModel We consider a concurrent system composed of a .nite set of pro\u00adcesses (or threads). Each process \nexecutes a deterministic program, whereby statements act on the (global) state of the system, which is \nmade up of the local states of each process and the shared state of the system. We assume that the state \nspace does not contain cycles, and that executions have bounded length. We do not restrict to a speci.c \nmode of process interaction, allowing instead the use of shared variables, messages, etc. Let S be the \nset of (global) states of the system. The system has a unique initial state s0 . S. We assume that the \nprogram executed by a process p can be represented as a partial function executep : S . S which moves \nthe system from one state to a subsequent state. Each such application of the function executep represents \nan atomic execution step of process p, which may depend on and affect the global state. We let each execution \nstep (or just step for short) represent the combined effect of some global statement together with the \nfollowing .nite sequence of local statements (that only access and affect the local state of the process), \nending just before the next global statement. This avoids consideration of interleavings of local statements \nof different processes in the analysis. Such an optimization is common in tools such as VeriSoft [7]. \nThe execution of a process is said to block in some state s if the process cannot continue (i.e. executep(s) \nis unde.ned): for example, trying to receive a message in a state where the message queue is empty. To \nsimplify the presentation, we assume in Sections 3 7 that a process does not disable another process, \ni.e. if p is enabled and another process q performs a step, then p is still enabled. This assumption \nis valid for Erlang programs. Note that a process can disable itself, e.g. after a step such that the \nnext statement is a receive statement. An execution sequence E of a system is a .nite sequence of execution \nsteps of its processes that is performed from the initial state s0. Since each execution step is deterministic, \nan execution sequence E is uniquely characterized by the sequence of processes that perform steps in \nE. For instance, p.p.q denotes the execution sequence where .rst p performs two steps, followed by a \nstep of q. The sequence of processes that perform steps in E also uniquely determine the (global) state \nof the system after E, which is denoted s[E]. For a state s, let enabled(s) denote the set of processes \np that are enabled in s (i.e., for which executep(s) is de.ned). We use . to denote concatenation of \nsequences of processes. Thus, if p is not blocked after E, then E.p is an execution sequence. An event \nof E is a particular occurrence of a process in E. We use (p, i) to denote the ith event of process p \nin the execution sequence E. In other words, the event (p, i) is the ith execution step of process p \nin the execution sequence E. We use dom(E) to denote the set of events (p, i) which are in E, i.e. (p, \ni) . dom(E) iff E ' contains at least i steps of p. We will use e, e, . . . to range over events. We \nuse proc(e) to denote the process p of an event e = (p, i). If E.w is an execution sequence, obtained \nby concatenating E and w, then dom[E](w) denotes dom(E.w) \\ dom(E), i.e. the events in E .w which are \nin w. As a special case, we use next[E](p) to denote dom[E](p). We use <E to denote the total order between \nevents in E, i.e. e <E e' denotes that e occurs before e' in E. We use E' = E to denote that the sequence \nE' is a pre.x of the sequence E. 3.2 Event Dependencies A central concept in DPOR algorithms is that \nof a happens-before relation between events in an execution sequence (also called a causal relation [24]). \nWe denote the happens-before relation in the execution sequence E by .E . Intuitively, for an execution \n' sequence E, and two events e and e' in dom(E), e .E emeans that e happens before , or causally precedes \ne'. For instance, e ' can be the transmission of a message that is received by e, or e can ' be a write \noperation to a shared variable that is accessed by e. Our algorithms assume a function (called a happens-before \nassignment), which assigns a happens-before relation to any execution sequence. In order not to restrict \nto a speci.c computation model, we take a general approach, where the happens-before assignment is only \nrequired to satisfy a set of natural properties, which are collected in De.nition 3.1. As long as it \nsatis.es these properties, its precision can vary. For instance, the happens-before assignment can let \nany transmission to a certain message buffer be causally related with a reception from the same buffer. \nHowever, better reduction can be attained if the assignment does not make the transmission of a message \ndependent with a reception of a different one. In practice, the happens-before assignment function is \nimple\u00admented as expected by relating accesses to the same variables, trans\u00admissions and receptions of \nthe same messages, etc., typically using vector clocks [16]. In Section 8 we describe such an assignment \nsuitable for Erlang programs. DE FIN I T I ON 3.1. A happens-before assignment, which assigns a unique \nhappens-before relation .E to any execution sequence E, is valid if it satis.es the following properties \nfor all execution sequences E. 1. .E is a partial order on dom(E), which is included in <E . 2. The \nexecution steps of each process are totally ordered, i.e. (p, i) .E (p, i + 1) whenever (p, i + 1) . \ndom(E), 3. If E' is a pre.x of E, then .E and .El are the same on dom(E'). 4. Any linearization E' \nof .E on dom(E) is an execution se\u00adquence which has exactly the same happens-before relation .El as \n.E . This means that the relation .E induces a set of equivalent execution sequences, all with the same \nhappens\u00adbefore relation. We use E . E' to denote that E and E' are linearizations of the same happens-before \nrelation, and [E]. to denote the equivalence class of E. 5. If E . E', then s[E] = s[El]. 6. For any \nsequences E, E' and w, such that E .w is an execution sequence, we have E . E' if and only if E.w . E'.w. \n 7. If p, q, and r are different processes, then  if next[E](p) .E.p.r next[E.p](r) and next[E](p) .E.p.q \nnext[E.p](q), then next[E](p) .E.p.q.r next[E.p.q](r). D The .rst six properties should be obvious for \nany reasonable happens-before relation. The only non-obvious one would be the last. Intuitively, if the \nnext step of p happens before the next step of r after the sequence E, then the step of p still happens \nbefore the step of r even when some step of another process, which is not dependent with p, is inserted \nbetween p and r. This property holds in any reasonable computation model that we could think of. As examples, \none situation is when p and q read a shared variable that is written by r. Another situation is that \np sends a message that is received by r. If an intervening process q is independent with p, it cannot \naffect this message, and so r still receives the same message. Properties 4 and 5 together imply, as \na special case, that if e and e' are two consecutive events in E with e .E e', then they can be swapped \nand the (global) state after the two events remains the same.  3.3 Independence and Races We now de.ne \nindependence between events of a computation. If E .p and E .w are both execution sequences, then E|=p.w \ndenotes that E.p.w is an execution sequence such that next[E](p) .E.p.w e for any e . dom[E.p](w). In \nother words, E |= p.w states that the next event of p would not happen before any event in w   q: r(y) \np: w(x) r: r(z) p q q r r '  Ew q: r(x) r: r(x) E Figure 2. A sample run of the program in Example \n1 is shown to the left. This run is annotated by a happens-before relation (the dotted arrows). To the \nright, the happens-before relation is shown as a partial order. Notice that E ' |= q.r since q and r \nare not happens-before related in E ' .r.q. We also observe that I[El](w) = {q}, as q is the only process \noccuring in w and its .rst occurrence has no predecessor in the dotted relation in w. Furthermore, WI[El](w) \n= {q, r}, since r is not happens-before related to any event in w. in the execution sequence E.p.w. Intuitively, \nit means that p is independent with w after E. In the special case when w contains only one process q, \nthen E |=p.q denotes that the next steps of p and q are independent after E. We use E|=p.w to denote \nthat E|=p.w does not hold. For a sequence w and p . w, let w \\ p denote the sequence w with its .rst \noccurrence of p removed, and let wIp denote the pre.x of w up to but not including the .rst occurrence \nof p. For an execution sequence E and an event e . dom(E), let pre(E, e) denote the pre.x of E up to, \nbut not including, the event e. For an execution sequence E and an event e . E, let notdep(e, E ) be \nthe sub-sequence of E consisting of the events that occur after e but do not happen after e (i.e. the \nevents e ' that occur after e such that e .E e '). A central concept in most DPOR algorithms is that \nof a race. Intuitively, two events, e and e ' in an execution sequence E, where e occurs before e ' in \nE, are in a race if e happens-before e ' in E, and  e and e ' are concurrent , i.e. there is an equivalent \nexecution sequence E ' E in which e and e ' are adjacent.  Formally, let e <E e ' denote that proc(e) \n= proc(e ' ), that e .E e ' , and that there is no event e '' . dom(E), different from e ' and e, such \nthat e .E e '' .E e ' . Whenever a DPOR algorithm detects a race, then it will check whether the events \nin the race can be executed in the reverse order. Since the events are related by the happens-before \nrelation, this may lead to a different global state: therefore the algorithm must try to explore a corresponding \nexecution sequence. Let e ;E e ' denote that e <E e ', and that the race can be reversed. Formally, if \nE ' E and e occurs immediately before e ' in E ', then proc(e ' ) was not blocked before the occurrence \nof e. In Figure 2, there are two pairs of events e, e ' such that e <E e ' , namely (p, 1), (q, 2) and \n(p, 1), (r, 2). It also holds for both these pairs that e ;E e ' since both q and r are enabled before \n(p, 1). In other words, both the races in the program are reversible. 4. Source Sets In this section, \nwe de.ne the new concept of source sets. Intuitively, source sets contain the processes that can perform \n.rst steps in the possible future execution sequences. Let us .rst de.ne two related notions of possible \n.rst steps in a sequence. For an execution sequence E .w, let I[E](w) denote the set of processes that \nperform events e in dom[E](w) that have no happens-before predecessors in dom[E](w). More formally, p \n. I[E](w) if p . w and there is no other event e . dom[E](w) with e .E.w next[E](p). Algorithm 1: Source-DPOR \nalgorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 Initially E xplore((), \u00d8); E xplore(E, Sleep) ; if .p . (enabled(s[E]) \n\\ Sleep) then backtrack(E) := {p} ; while .p . (backtrack(E) \\ Sleep) do foreach e . dom(E) such that(e \n;E.p next[E](p)) do let E ' = pre(E , e); let v = notdep(e, E ).p ; if I[El](v) n backtrack(E ' ) = \u00d8 \nthen ' add some q . I[El](v) to backtrack(E ' ); let Sleep' := {q . Sleep | E|=p.q} ; E xplore(E .p, \nSleep' ); add p to Sleep ; For an execution sequence E .w, de.ne WI[E](w) as the union of I[E](w) and \nthe set of processes p such that p . enabled(s[E]) and E|=p.w. The point of these concepts is that for \nan execution sequence E.w: p . I[E](w) if and only if there is a sequence w ' such that E.w E.p.w ', \nand  p . WI[E](w) if and only if there are sequences w ' and v such that E.w.v E.p.w ' .  DE FIN I \nT I ON 4.1 (Source Sets). Let E be an execution sequence, and let W be a set of sequences, such that \nE .w is an execution sequence for each w . W . A set P of processes is a source set for W after E if \nfor each w . W we have WI[E](w) n P = \u00d8. D The key property is that if P is a source set for W after \nE, then for each execution sequence of form E .w with w . W , there is a process p . P and a sequence \nw ' such that E.p.w ' E .w.v for some sequence v. Therefore, when an exploration algorithm intends to \ncover all suf.xes in W after E, the set of processes that are chosen for exploration from s[E] must be \na source set for W after E. 5. Source-DPOR In this section, we present the source-DPOR algorithm. It \nis shown in Algorithm 1. As mentioned in the introduction, it is the .rst step towards our optimal algorithm, \nand is derived from the classical DPOR algorithm [4] by replacing persistent sets with source sets. Source-DPOR \nperforms a depth-.rst search, using the recursive procedure Explore(E, Sleep), where E is the stack, \ni.e. the past execution sequence explored so far, and Sleep is a sleep set, i.e. a set of processes that \nneed not be explored from s[E]. The algo\u00adrithm maintains, for each pre.x E ' of E, a set backtrack(E \n' ) of processes that will eventually be explored from E ' . Explore(E , Sleep) initializes backtrack(E) \nto consist of an arbitrary enabled process which is not in Sleep. Thereafter, for each process p in backtrack(E) \nwhich is not in Sleep, the algorithm per\u00adforms two phases: race detection (lines 6 10) and state exploration \n(lines 11 13). In the race detection phase, the algorithm .rst .nds the events e in E that are in a race \nwith the next step of p, and where the race can be reversed (line 6). For each such event e . dom(E), \nthe algorithm must explore an execution sequence in which the race is reversed. Using the notation of \nthe algorithm, such an execution sequence is equivalent to a sequence of form E ' .v.proc(e).z, where \nv is obtained by appending p after the sequence notdep(e, E ) of events that occur after e in E.p, but \ndo not happen after e, and z is any continuation of the execution. Note that we insert all of notdep(e, \nE ) before next[E](p), since notdep(e, E) includes all events that follow e in E that happen before next[E](p), \nwhich must be performed before next[E](p) when the race is reversed. The events in E that happen after \ne, should still occur after e, in the sequence z. A sequence equivalent to E ' .v.proc(e).z can be performed \nby taking a step of a process in I[El](v) immediately after E '. The algorithm therefore (at line 9) \nchecks whether some process in I[El](v) is already in backtrack(E ' ). If not, then a process in I[El](v) \nis added to backtrack(E ' ). This ensures that a sequence equivalent to E ' .v.proc(e).z has been or \nwill be explored by the algorithm.  In the exploration phase, exploration is started recursively from \nE .p, using an appropriately initialized sleep set. Sleep sets are manipulated as follows: (i) after \n.nishing exploration of E.p, the process p is added to sleep set at E, and (ii) when the exploration \nof E .p is started recursively from E, the sleep set Sleep' of E.p is initialized to be the set of processes \ncurrently in the sleep set of E that are independent with p after E (i.e., Sleep' = {q . Sleep |E |= \nq.p}). The effect is that the algorithm need never explore E .p for any process p . Sleep, since that \nwould always lead to a sequence which is equivalent to one that has been explored from a pre.x of E. \nFor processes p that were added according to Case (i) above, this is obvious. To see why a process q \nin the initial sleep set Sleep' of E .p need not be explored, note that any execution sequence of form \nE.p.q.v is equivalent to the execution sequence E.q.p.v (by E|=q.p). Since the algorithm guarantees to \nhave explored some sequence in [E.q.p.v] whenever q is in the sleep set of E, it need not explore E .p.q. \nCorrectness Algorithm 1 is correct in the sense that for all maxi\u00ad mal execution sequences E, the algorithm \nexplores some execution sequence in [E] . For lack of space, the proof is omitted. The main part of the \nproof establishes that when Explore(E, Sleep) returns, the set Sleep will be a source set for W after \nE, where W is the set of suf.xes w such that E.w is an execution sequence. On Source Sets and Persistent \nSets The mechanism by which source-DPOR produces source sets rather than persistent sets is the test \nat line 9. In persistent set-based DPOR algorithms [4, 14, 22, 23, 25], this test must be stronger, and \nat least guarantee that backtrack(E ' ) contains a process q such that q performs some event in v which \nhappens-before next[E](p) in E.p. Such a test guarantees that the .rst event in v which is dependent \nwith some process in backtrack(E ' ) is performed by some process in backtrack(E ' ), thus making backtrack(E \n' ) a persistent set. In contrast, our test at line 9 does not require the added process to perform an \nevent which happens-before next[E](p) in E ' .v. Consider, for instance, that v is just the sequence \nq.p, where q is independent with p after E '. Then, since the event of q does not happen-before the event \nof p, there is an execution sequence E ' .p.q in which p is dependent with the process proc(e) in backtrack(E \n' ) but need not be in backtrack(E ' ). On the other hand, since q . I[El](p.q), the set backtrack(E \n' ) (together with the initial sleep set at E ') is still a source set for the possible continuations \nafter E ' . 6. Wakeup Trees As we described earlier, source-DPOR may still lead to sleep-set blocked \nexplorations. We therefore present an algorithm, called optimal-DPOR, which is provably optimal in that \nit always explores exactly one interleaving per Mazurkiewicz trace, and never encoun\u00adters sleep-set blocking. \nOptimal-DPOR is obtained by combining source sets with a novel mechanism, called wakeup trees, which \ncontrol the initial steps of future explorations. Wakeup trees can be motivated by looking at lines 7 \n11 of Algorithm 1. At these lines, it is found that some execution sequence starting with E ' .v should \nbe performed in order to reverse the detected race. However, at lines 10 11, only a single process from \nthe sequence v is entered into backtrack(E ' ), thus forgetting information about how to reverse the \nrace. Since the new exploration after E ' .q does not remember this sequence v, it may explore a completely \ndifferent sequence, which could potentially lead to sleep set blocking. To prevent such a situation, \nwe replace the backtrack set by a so-called wakeup tree. The wakeup tree contains initial fragments of \nthe sequences that are to be explored after E '. Each fragment guarantees that no sleep set blocking \nwill be encountered during the exploration. To de.ne wakeup trees, we .rst generalize the relations p \n. I[E](w) and p . WI[E](w) to the case when p is a sequence. Let E be an execution sequence and let v \nand w be sequences of processes. Let v g[E] w denote that there is a sequence v ' such that E.v.v ' \nand E.w are execution sequences with E.v.v ' E.w. Intuitively, v g[E] w if, after E, the sequence v is \na possible way to start an execution that is equivalent to w.  Let v ~[E] w denote that there are sequences \nv ' and w ' such that E.v.v ' and E.w.w ' are execution sequences with E.v.v '  E.w.w '. Intuitively, \nv ~[E] w if, after E, the sequence v is a possible way to start an execution that is equivalent to an \nexecution sequence of form E.w.w ' . As special cases, for a process p we have p . I[E](w) iff p g[E] \nw, and p . WI[E](w) iff p ~[E] w. As examples, in Figure 2, we have q.r g[El] q.q.r.r but q.q g[El] r.r. \nWe also have q.q ~[El] r.r since E ' .q.q.r.r E ' .r.r.q.q. Note that ~[E] is not transitive. The relation \nv ~[E] w can be checked using the following recursive de.nition. LEM M A 6.1. The relation v ~[E] w holds \nif either v = (), or v is of form p.v ', and either p . I[E](w) and v ' ~[E.p] (w \\ p), or  E|=p.w \nand v ' ~[E.p] w. D  The following lemma states some useful properties. LEM M A 6.2. Let E be an execution \nsequence, and let v, w, and w ' be sequences. Then 1. E.w ' E .w implies that (i) v g[E] w iff v g[E] \nw ', and (ii) w g[E] v iff w ' g[E] v, and (iii) v ~[E] w iff v ~[E] w ' ; 2. v g[E] w and w ~[E] w ' \nimply v ~[E] w ' ; 3. p . WI[E](w) and w ' g[E] w imply p . WI[E](w ' ); 4. p . WI[E](w) and E |= p.q \nand E |= q.w imply p . WI[E](q.w). D  The above properties follow from the de.nitions. Let us de.ne \nan ordered tree as a pair (B, -), where B (the set of nodes) is a .nite pre.x-closed set of sequences \nof processes, with the empty sequence () being the root. The children of a node w, of form w.p for some \nset of processes p, are ordered by the ordering -. In (B, -), such an ordering between children has been \nextended to the total order -on B by letting -be the induced post-order relation between the nodes in \nB. This means that if the children w.p1 and w.p2 are ordered as w.p1 -w.p2, then w.p1 -w.p2 -w in the \ninduced post-order. DE FIN I T I ON 6.3 (Wakeup Tree). Let E be an execution sequence, and P be a set \nof processes. A wakeup tree after (E, P ) is an ordered tree (B, -), such that the following properties \nhold 1. WI[E](w) n P = \u00d8 whenever w is a leaf of B; 2. whenever u.p and u.w are nodes in B with u.p \n-u.w, and u.w is a leaf, then p . WI[E.u](w). D   Intuitively, a wakeup tree after (E, P ) is intended \nto consist of initial fragments of sequences that should be explored after E to avoid sleep set blocking, \nwhen P is the current sleep set at E. To see this, note that if q . P , then (by the way sleep sets are \nhandled) q . I[E](w) for any sequence w that is explored after E. If, in addition, E|=q.w, then q is \nstill in the sleep set at E.w. To prevent this, we therefore require q . WI[E](w), which is the same \nas Property 1, i.e., WI[E](w) n P = \u00d8. Property 2 implies that if a process p is added to the sleep set \nat E.u, after exploring E.u.p, then by the same reasoning as above, it will have been removed from the \nsleep set when we reach E.u.w. The empty wakeup tree is the tree ({()}, \u00d8), which consists only of the \nroot (). We state a useful property of wakeup trees. LEMM A 6.4. If (B, -) is a wakeup tree after (E \n, P ) and w, w ' . B and w is a leaf which satis.es w ' ~[E] w, then w . w ' . D The lemma states that \nany leaf w ' is the smallest (w.r.t. -) node in the tree which is consistent with w ' after E. Proof: \nWe prove the lemma by contradiction. Assume w ' -w. ' '' Then there are u, p, v, v such that w = u.p.v \nand w = u.v such that u.p -u.v. Since u.v is a leaf, we have p . WI[E.u](v) by Property 2 of De.nition \n6.3. Hence p ~[E.u] v, which implies u.p ~[E] u.v, which implies u.p.v ' ~[E] u.v, i.e. w ' ~[E] w. D \nFor a wakeup tree (B, -) and a process p . B, de.ne subtree((B, -), p) to denote the subtree of (B, -) \nrooted at p, i.e. subtree((B, -), p) = (B ' , -' ) where B ' = {w | p.w . B}and -' is the restriction \nof -to B ' . Let (B, -) be a wakeup tree after (E, P ). For any sequence w such that E.w is an execution \nsequence with WI[E](w) n P = \u00d8, we de.ne the operation insert[E](w, (B, -)), with the properties: 1. \ninsert[E](w, (B, -)) is also a wakeup tree after (E , P ), 2. any leaf of (B, -) remains a leaf of insert[E](w, \n(B, -)), and 3. insert[E](w, (B, -)) contains a leaf u with u ~[E] w.  A simple construction of insert[E](w, \n(B, -)) is the following: Let v be the smallest (w.r.t. to -) sequence v ' in B such that v ' ~[E] w. \nIf v is a leaf, insert[E](w, (B, -)) can be taken as (B, -) and we are done. Otherwise, let w ' be a \nshortest sequence such that w g[E] v.w ', and add v.w ' as a new leaf, which is ordered after all already \nexisting nodes in B of form v.w '' . As an illustration, using Example 1, assume that a wakeup tree (B, \n-) after ((), \u00d8) contains p as the only leaf. Then the operation insert[o ](q.q, (B, -)) adds q.q as \na new leaf with p -q.q. If we thereafter perform insert[o ](r.r, (B, -)), then the wakeup tree remains \nthe same, since q.q ~[o ] r.r, and q.q is already a leaf. 7. Optimal-DPOR In this section, we present \nthe optimal algorithm, shown in Al\u00adgorithm 2. The algorithm performs a depth-.rst search, using the recursive \nprocedure Explore(E, Sleep, WuT), where E and Sleep are as in Algorithm 1, and WuT is a wakeup tree after \n(E, Sleep), containing extensions of E that are guaranteed to be explored (in order) by Explore(E , Sleep, \nWuT). If WuT is empty, then E xplore(E, Sleep, WuT) is free to explore any extension of E. Like Algorithm \n1, the algorithm runs in two modes: race detec\u00ad tion (lines 3 8) and state exploration (lines 9 22), \nbut it is slightly differently organized. Instead of analyzing races at every invocation of Explore, \nraces are analyzed in the entire execution sequence only when a maximal execution sequence has been generated. \nThe reason for this is that the test at line 7 is precise only when the used sequence v, which is de.ned \nat line 6, includes all events in the entire execution that do not happen after e, also those that Algorithm \n2: Optimal-DPOR algorithm. 1 Initially E xplore((), \u00d8, ({()}, \u00d8)); 2 E xplore(E, Sleep, WuT) ; 3 if enabled(s[E]) \n= \u00d8 then ' ' 4 foreach e, e . dom(E) such that (e ;E e ) do 5 let E ' = pre(E , e); 6 let v = (notdep(e, \nE ).proc(e ' )) ; 7 if sleep(E ' ) n WI[El](v) = \u00d8 then 8 insert[El](v, wut(E ' )) 9 else 10 if WuT = \n({()}, \u00d8) then 11 wut(E) := WuT ; 12 else 13 choose p . enabled(s[E]); 14 wut(E) := ({p}, \u00d8) ; 15 sleep(E) \n:= Sleep ; 16 while .p . wut(E) do 17 let p = min-{p . wut(E)}; 18 let Sleep' = {q . sleep(E) | E|=p.q} \n; 19 let WuT' = subtree(wut(E), p) ; 20 E xplore(E .p, Sleep' , WuT' ); 21 add p to sleep(E) ; 22 remove \nall sequences of form p.w from wut(E) ; occur after e '. Therefore v can be de.ned only when E is a maximal \nexecution sequence. In the race detection phase, Algorithm 2 must be able to access the current sleep \nset for each pre.x E ' of the currently explored execution sequence E. For each such pre.x E ', the algorithm \ntherefore maintains a set of processes sleep(E ' ), which is the current sleep set at E '. In a similar \nway, for each pre.x E ' of E, the algorithm maintains wut(E ' ), which is the current wakeup tree at \nE ' . Let us now explain the race detection mode, which is entered whenever the exploration reaches the \nend of a complete sequence (i.e. enabled(s[E]) = \u00d8). In this mode, the algorithm investigates all races \nthat can be reversed in the just explored sequence E. Such a race consists of two events e and e ' in \nE, such that e ;E e '. Let E ' = pre(E , e) and let v = notdep(e, E).proc(e ' ), i.e. the sub\u00adsequence \nof E consisting of the events that occur after e but do not happen after e, followed by proc(e ' ) (this \nnotation is introduced at lines 5 6). The reversible race e ;E e ' indicates that there is another execution \nsequence, which performs v after E ', and in which the race is reversed, i.e. the event e ' happens before \nthe event e. Since E ' .v is incompatible with the currently explored computation, the algorithm must \nnow make sure that it will be explored if it was not explored previously. If some p . sleep(E ' ) is \nin WI[El](v), then some execution equivalent to one starting with E ' .v will have been explored previously. \nIf not, we perform the operation insert[El](v, wut(E ' )) to make sure that some execution equivalent \nto one starting with E ' .v will be explored in the future. In the exploration mode, which is entered \nif exploration has not reached the end of an execution sequence, .rst the wakeup tree wut(E) is initialized \nto WuT. If WuT is empty, then (as we will state in Lemma 7.2) the sleep set is empty, and an arbitrary \nenabled process is entered into wut(E). The sleep set sleep(E) is initialized to the sleep set that is \npassed as argument in this call to Explore. Thereafter, each sequence in wut(E) is subject to exploration. \nWe .nd the .rst (i.e. minimal) single-process branch p in wut(E) and call Explore recursively for the \nsequence E.p. In this call, the associated sleep set Sleep' is obtained from sleep(E) in the same way \nas in Algorithm 1. The associated wakeup tree WuT' is obtained denote Final_sleep(E) \\ Sleep, i.e. the \nset of processes that are as the corresponding subtree of wut(E). Thereafter, Explore is explored from \ns[E]. As inductive hypothesis, we assume that the called recursively for the sequence E.p with the modi.ed \nsleep set theorem holds for all execution sequences E ' with E ' . E. Sleep' and wakeup tree WuT'. After \nExplore(E.p, Sleep' , WuT' ) The inductive step is proven by contradiction. Assume that has returned, \nthe sleep set sleep(E) is extended with p, and all E .w is a maximal execution sequence such that the \nalgorithm sequences beginning with p are removed from wut(E). has not explored any execution sequence \nE ' in [E .w] . We .rst  prove that this implies WI[E](w) n Final_sleep(E) = \u00d8. The proof 7.1 Correctness \n is by contradiction. Assume that there is a p . WI[E](w) with Let us now prove the correctness of the \noptimal-DPOR algorithm. p . Final_sleep(E), i.e. p . sleep(E) at some point during the Throughout, we \nassume a particular completed execution of optimal-' exploration. Let E ' be the longest pre.x of E such \nthat E ' .p . E , l q DPOR. This execution consists of a number of terminated calls and de.ne w ' by \nE ' .w = E. By the handling of sleep sets, we have to E xplore(E, Sleep, WuT) for some values of the \nparameters E, E ' |=p.w '. It follows by the de.nition of WI[E](w) that there is a '' ' ' '' ' E ' Sleep, \nand WuT. Let E denote the set of execution sequences E that w such that E.w E .p.w '' E .w .p.w .p.w \n.w ''. By have been explored in some call of form Explore(E , \u00b7, \u00b7). De.ne the inductive hypothesis applied \nto E ' .p, the algorithm has explored ' ' '' the ordering . on E by letting E . E ' if Explore(E , \u00b7, \n\u00b7) returned some execution sequence in [E .p.w .w ] = [E.w] , which gives before Explore(E ' , \u00b7, \u00b7). \nIntuitively, if one were to draw an ordered a contradiction. tree that shows how the exploration has \nproceeded, then E would be For each process p . Final_sleep(E), let Ep ' be the pre.x of the set of nodes \nin the tree, and . would be the post-order between E such that Ep ' .p is the last (w.r.t. .) execution \nsequence of form '' '' nodes in that tree. Ep .p, with Ep being a pre.x of E, that precedes E (w.r.t. \n.). We begin by establishing some useful invariants. (Note that if p . done(E) then Ep ' = E, but if \np . Sleep then ' '' Ep is a strict pre.x of E.) Let wp be de.ned by E = Ep ' .wp LEMM A 7.1. If E.p \n. E.w then p . I[E](w). ' (note that E ' |=p.w ). Let wp be the longest pre.x of w such that Proof: After \nthe call to Explore(E .p, \u00b7, \u00b7) has returned, we have E|=p.wp, and let ep be the .rst event in dom[E](w) \nwhich is not pp in wp. (Such an event ep must exist, since otherwise wp = w which that p . sleep(E). \nIt follows from the rules for manipulating sleep sets, that if E.w . E and E .p . E .w then p . I[E](w). \nD implies E|=p.w, which implies p . WI[E](w), which contradicts WI[E](w) n Final_sleep(E) = \u00d8.) Let q \n. Final_sleep(E) be such that wq is a longest pre.x among the pre.xes wp for p .LEMM A 7.2. Whenever \nAlgorithm 2 is inside a call of form Final_sleep(E). If there are several processes p . Final_sleep(E) \nE xplore(E, Sleep, WuT), then such that wp is the same longest pre.x, then let q be the process 1. wut(E) \nis a wakeup tree after (E, sleep(E)), among these such that Eq ' .q is minimal (w.r.t. .). Let wR be \nwq.proc(eq). 2. if WuT is empty, then Sleep is empty. D Consider the exploration of Eq ' .q, which happens \nin the call  The following lemma captures the relationship between wakeup E xplore(Eq ' .q, Sleep' , \n\u00b7) with Sleep' as the sleep set argument. ' ' trees and (E, .). Since Eq.q is an execution sequence, \nand Eq ' |=q.(wq.wq), it fol\u00adlows that Eq ' .wq ' .wq.q is an execution sequence. Since Eq ' .wq ' .wq.eq \nLEMM A 7.3. Let (E, .) be the tree of explored execution se\u00adis an execution sequence, and q does not \ndisable eq (since q and quences. Consider some point in the execution, and the wakeup ' proc(eq) are \ndifferent), we conclude that Eq ' .wq.wq.q.eq and hence tree wut(E) at that point, for some E . E . \n' ' ' Eq.q.w q.wq.eq = Eq ' .q.w q.wR is an execution sequence. ' .wR) n Sleep' 1. If w . wut(E) for \nsome w, then E.w . E . We next establish that WI[E\u00d8, us\u00ad .q](w = q 2. If w -w ' for w, w ' . wut(E) then \nE.w . E.w ' ing a proof by contradiction, as follows. Assume that some pro\u00ad q ' .wR) n Sleep' cess p \nis in WI[EBy the construction of .q](w l q . Proof: The properties follow by noting how the exploration \nfrom any Sleep' at line 18, the process p must be in sleep(Eq ' ) just be-E . E is controlled by the \nwakeup tree wut(E) at lines 16-22. D fore the call to Explore(Eq ' .q, Sleep' , \u00b7), and satisfy Eq ' \n|= p.q. .wR) and Property 3 of Lemma 6.2, we We can now give the proof of correctness for the algorithm: \n' From p . WI[E l q .q](wq TH EO R E M 7.4. Whenever a call to Explore(E , Sleep, WuT) re\u00adq ' .wq). From \nthis, Eq 'infer p . WI[E l q p.q and Prop\u00ad |= ](q.w .q](w turns during Algorithm 2, then for all maximal \nexecution sequences ' erty 4 of Lemma 6.2, it follows that p . WI[E.wq), which, l q q of form E.w, the \nalgorithm has explored some execution sequence '' ' using E ' .q.w .wq E ' .w .wq.q (which follows from \nE qq qq |= .wq.q), which by Property 3 of E ' which is in [E .w] . q ' ' .wq)), implies p . WI[E q.(w \n](w q l q q ' Since initially the algorithm is called with Explore((), \u00d8, ({()}, \u00d8)), Lemma 6.2 imply \np . WI[ETheorem 7.4 implies that for all maximal execution sequences of .wq). From this, and the property \n](w q l q that p . sleep(Eq ' ) just before the call to E xplore(Eq ' .q, Sleep' , \u00b7),form E the algorithm \nexplores some execution sequence E ' which ' we have by the handling of sleep sets that p . I[E l q \n](wq.wq), is in [E] . ' '' which together with p . WI[E l q .wq) implies E |=p.w ) just before the call \nto ](w q .wq. q q Proof of Theorem 7.4: By induction on the set of execution se-This, and the fact that \np . sleep(Eq ' quences E that are explored during the considered execution, using the ordering . (i.e. \nthe order in which the corresponding calls to E xplore returned). Base Case: This case is the .rst sequence \nE for which the call E xplore(E, \u00b7, \u00b7) returns. By the algorithm, E is already maximal, so the theorem \ntrivially holds. Inductive Step: Consider an arbitrary execution sequence E . E that is explored by the \nalgorithm. Let Final_sleep(E) denote the value of sleep(E) when Explore(E, Sleep, WuT) returns. Let done(E) \nE xplore(Eq ' .q, Sleep' , \u00b7), implies that p . Final_sleep(Eq ' .wq ' ), i.e. p . Final_sleep(E). Since \nE ' |=p.w ' .wq, we have by con\u00ad q q struction that ep = eq. But since among the processes p with ep \n= eq we chose q to be the .rst one for which a call of form E xplore(Eq ' .p, \u00b7, \u00b7) was performed, we \nhave that p . sleep(Eq ' ) just before the call to Explore(Eq ' .q, Sleep' , \u00b7), whence p . Sleep' . \nThus we have a contradiction. ' '' Let z be any sequence that makes Eq ' .q.w q.wR.z maximal (such a \nz ' can always be found, since Eq ' .q.w q ' .wR is an execution  q ' .wR) n Sleep' = \u00d8 (proven in the \nll](wR), which together with E ' .u|=p.(p ' .v '' ) implies q , which implies ~).wu.plR [Eq .q](w .u.pl.v \n.q](w q ' .wR.z ' ) n Sleep' ' '' ' '' = \u00d8. .u](p .v u.p .v .wR, ] ' ' ' was explored .wR, which contradicts \nthe construction .q.w q.wR.z ] ] w q q before the call to E xplore(Eq ' .q, Sleep' , \u00b7) (otherwise, \nthere would of y. be a call Explore(E '' .p, \u00b7, \u00b7) with E '' a pre.x of Eq ' and p . Sleep' , Thus, w \n' 'Hence, no execution sequence in which implies ~[E yl[Eqpreceding paragraph) follows WIWI.pl l[E[Eq \nsequence). From WIWIl l[E[Eq q q is a strict pre.x of u. Since Eq ' .y, and hence E ' .u, is q q '' by \nE '' .w '' = Eq ' , we would have E '' |=p.w '' ), thus contradicting WI[El .q](w q ' and de.ning w \nand explored by the algorithm, we have Eq.u . Eq.w . Moreover, since q '' ''' q.wR.z ) n u is a pre.x \nof wq.wR, we infer that Eq.u is a pre.x of Eq ' .w ' ' Sleep' = \u00d8). By the inductive hypothesis for Eq \n' WI.p l[Eq .q](w .wR.z .w. q q '' '' This means that there is a sequence w such that Eq ' .u.w E.w. \n .q applied to wq ' .wR.z ', the algorithm then explores some sequence of form It follows by the inductive \nhypothesis applied to Eq ' .u that the ' '' ' '' ] E .q.z in [E ' .q.w .wR.z ] . algorithm has explored \nsome maximal sequence in [Eq.u.w and q qq By the construction of wR, we have next[El ](q) ;El .q.wl .wR.zl \nhence in [E.w] . This contradicts the assumption at the beginning q q q of the inductive step. This concludes \nthe proof of the inductive step, eq. From Eq ' .q.z Eq ' .q.w q ' .wR.z ', it follows that the same race \nbetween next[E](q) and eq will also occur in Eq ' .q.z, i.e. and the theorem is proven. D . Since the \nsequence Eq ' actually explored by the algorithm, it will encounter the race In this section, we prove \nthat optimal-DPOR is optimal in the next[El ](q) ;El . When handling it, we have ;next()ql l[E] Eq q \nq q .q.z is  7.2 Optimality .q.z eq .q.z eq sense that it never explores two different but equivalent \nexecution ' sequences and never encounters sleep set blocking. The following .q.z in this proof, E in \nthe algorithm will correspond to E q theorem, which is essentially the same as Theorem 3.2 of Godefroid \nand in the algorithm will correspond to nexte l[Eq ](q) in this proof, et al. [10] establishes that \nsleep sets alone are suf.cient to prevent exploration of two equivalent maximal execution sequences: \ne ' in the algorithm will correspond to eq in this proof. TH E O R E M 7.5. Optimal-DPOR never explores \ntwo maximal execu\u00adtion sequences which are equivalent. Let v = (notdep(next[El ](q), E q ' .q.z).proc(eq)) \nbe the sequence q ](q), E q ' .q.z) Proof: Assume that E1 and E2 are two equivalent maximal execu\u00ad at \nline in the algorithm. Let be notdepnext6 (v x l[Eq' ' ' be notdepnext(l[Eq ' ). We note that tion sequences, \nwhich are explored by the algorithm. Then they and let x ](q), E q.q.w q.wR.z are both in E. Assume w.l.o.g. \nthat E1 . E2. Let E be their '' '' wq.wq is a pre.x of x . From Eq ' .q.z Eq ' .q.w q.wR.z and longest \ncommon pre.x, and let E1 = E.p.v1 and E2 = E.v2. By Lemma 7.1 we have p . I[E](v2), which contradicts \nE1 E2 the de.nitions of x and x ', it follows that Eq ' .x Eq ' .x ', and hence that Eq ' .x.proc(eq) \nEq ' .x ' '' .proc(eq). Let x be obtained and the maximality of E1 and E2. D We will now prove that Algorithm \n2 is optimal in the sense that it from x ' by adding proc(eq) just after the pre.x wq ' .wq (i.e., in \nthe same place that it has in wq ' .wR.z '). Since eq happens after ](q) in Eq ' .q.w q ' .wR.z ', it \nfollows that no events in x ' hap\u00ad pen after eq nextl[Eq never encounters sleep-set blocking. Let us \n.rst de.ne this precisely. ' '' ' in Eq ' .q.w .wR.z '. Hence Eq ' .x Eq ' .x .proc(eq). q DE FIN I T \nI ON 7.6 (Sleep Set Blocking). During the execution of q ' '' ' '' Since w .wR is a pre.x of x we have \nw .wR g[El x , which ] Algorithm 2, a call to Explore(E, Sleep, WuT) is sleep set blocked if enabled(s[E]) \n= \u00d8 and enabled(s[E]) . Sleep. D q q '' ' ' by Eq ' .x Eq ' .x .proc(eq) implies w ' .proc(eq), q .wR \ng[El ] x q ' '' which by Eq.x.proc(eq) Eq ' .x .proc(eq) implies w g.wlR [Eq q v. By properties of sleep \nsets and the construction of wR, it fol\u00ad) n WI[El \u00d8. This implies, by ] q Now let us state and prove \nthe corresponding optimality theorem. lows that sleep(Eq ' ' ](wq.wR) = TH E O R E M 7.7. During any \nexecution of Algorithm 2, no call to ' ' q WIn)g.wl lR [E[Eq line 7 will succeed, and after performing \nline 8, the wakeup tree ration. Then any sequence in WuT is enabled after E. By Lemma 7.2, qProof: Consider \na call SleepWuTduring the explo-Explore(E, )' will (by the speci.cation of insert) contain a leaf suchwut(E \n) ,yq 'that . Since at this point and WI~. wut .(E ) ()y vq qvl l[E] [E] q q ] v, that sleep(E ](v) = \n\u00d8. Thus, the test at E xplore(E, Sleep, WuT) is ever sleep set blocked. w q q WuT is a wakeup tree after \n(E, Sleep). Thus, if Sleep = \u00d8, then WuT contains a sequence w such that Sleep n WI[E](w) = \u00d8. Letting \np and ~v y l] [Eq we have, by de.nition of insert, that and Eq' |=q.y. By Property 1 of Lemma 7.3 we \nthen have Eq ' .y . E . be the .rst process in w, this implies p . Sleep, implying that p is enabled \nand thus enabled(s[E]) . Sleep. D From w ' ] v, it follows by Property 2 of Lemma 6.2 that ~y l[Eqg.wlR \n[Eq (which follows from Eq' q ' '' .wR. Furthermore, from E |=q.w =q.y, it follows that ] w q qq q ' \n.wq) and Eq' 8. Implementation |=q.w | y is not a pre.x of w ' q. Let u be the longest common pre.x \nof y and ' In this section we describe our implementation in the context of is a strict pre.x of u. \nOtherwise, there are ' .wR. We claim that w w q q Concuerror [2], a stateless model checking tool for \nErlang. q ' '' ' '' ' different processes p, p and a sequence v such that u.p .v = wq and u.p is a pre.x \nof y. From y ~[El ' .wR and Property 2 of Erlang Erlang is an industrially relevant programming language \nw ] q '. If u.p ' . wut(Eq ' q Lemma 6.2, we infer ~y l[Eq is inserted, we infer by Lemma 6.4 and Property \n2 of Lemma 7.3 '''' that Eq ' .y . Eq.u.p . If u.p . wut(Eq) when y is inserted, we also infer Eq ' .y \n. Eq ' .u.p ', since then y will be explored before ' ' '' u.p '. Thus Eq ' .y . Eq ' .u.p ', which implies \nEq.u.p . Eq ' .u.p .v , which by handling of sleep sets implies p . I[El .u](p ' .v '' ). By ) when y \n] u.p q ' , implying u.p ~[El ' ' '' ] w ] u.p .v q based on the actor model of concurrency [1]. In \nErlang, actors are realized by language-level processes implemented by the runtime system instead of \nbeing directly mapped to OS threads. Each Erlang process has its own private memory area (stack, heap \nand mailbox) and communicates with other processes via message passing. A call to the spawn function \ncreates a new process P and returns a process identi.er (PID) that can be used to send messages to P \n. Messages .u] p .v , are sent asynchronously using the ! (or send) function. Messages ' '' '', we have \n~~y pl l[E[Eq which is the same as WI. By I. .)p pl l[E[Eq q q ' '' ' this implies Eq ' .u|=p.(p .v \n). This implies that p . sleep(Eq ' .wq), arrive. A process can then consume messages using selective \npattern i.e., p . sleep(E). Hence by the construction of wR, we have p . matching in receive expressions, \nwhich are blocking operations ' '' .u](p .u](p .v ) get placed in the mailbox of the receiving process \nin the order they .v  1 -m o d u l e ( r e a d e r s ) . 2 -e x p o r t ([ r e a d e r s / 1 ]) . 3 \n4 r e a d e r s ( N ) -> 5 e t s : n e w ( ta b , [p u b l ic , n a m e d _t a b l e ]) , 6 W r i t e \nr = f u n () -> e t s : i n s e r t ( t ab , {x , 4 2 }) en d , 7 R e a d e r = f u n ( I ) -> e ts : \nl o o k u p ( ta b , I ) , e t s : l o o k u p ( t ab , x ) e nd , 8 s p a w n ( W r i t e r ) , 9 [ \ns p a w n ( f u n () -> R e a d e r ( I ) e n d ) || I < -l i s t s : s e q ( 1 , N ) ], 10 r e c e \ni v e a f t e r i n f i n i t y -> d e a d l o c k e n d . Figure 3. Writer-readers program in Erlang. \nwhen a process mailbox does not contain any matching message. Optionally, a receive may contain an after \nclause which speci.es a timeout value (either an integer or the special value infinity) and a value to \nbe returned if the timeout time (in ms) is exhausted. Erlang processes do not share any memory by default. \nStill, the Erlang implementation comes with a key-value store mechanism, called Erlang Term Storage (ETS), \nthat allows processes to create memory areas where terms shared between processes can be in\u00adserted, looked \nup, and updated. Such areas are the ETS tables that are explicitly declared public. The runtime system \nautomatically serializes accesses to these tables when this is necessary. Each ETS table is owned by \nthe process that created it and its memory is reclaimed by the runtime system when this process exits. \nErlang has all the ingredients needed for concurrency via mes\u00adsage passing and most of the ingredients \n(e.g. reads and writes to shared data, etc.) needed for concurrent programming using shared memory. Unsurprisingly, \nErlang programs are prone to the usual errors associated with concurrent execution, although the majority \nof them revolves around message passing and misuse of built-in primitives implemented in C. Figure 3 \nshows Example 1 written in Erlang, generalized to N instead of just two readers. A public ETS table shared \nbetween N+1 processes: N readers and one writer. The writer inserts a key-value pair, using x as a key. \nEach of the N readers tries to read two entries from this table: some entry with a different key in each \nprocess (an integer in the range 1..N) and the entry keyed by x. The receive expression on line 10 forces \nthe process executing the readers code to get stuck at this point, ensuring that the process owning the \ntable stays alive, which in turn preserves the ETS table. Concuerror Concuerror [2] is a systematic testing \ntool for .nding concurrency errors in Erlang programs or verifying their absence. Given a program and \na test to run, Concuerror uses a stateless search algorithm to systematically explore the execution of \nthe test under conceptually all process interleaving. To achieve this, the tool employs a source-to-source \ntransformation that inserts instrumentation at preemption points (i.e. points where a context switch \nis allowed to occur) in the code under execution. This instrumentation allows Concuerror to take control \nof the scheduler when the program is run, without having to modify the Erlang VM in any way. In the current \nVM, a context switch may occur at any function call. Concuerror inserts preemption points only at process \nactions that interact with (i.e. inspect or update) shared state. Concuerror supports the complete Erlang \nlanguage and can instrument programs of any size, including any libraries they use. The tool employs \ntwo techniques to reduce the number of explored traces: (i) an optimization which avoids exploring traces \nthat involve processes blocking on receive expressions [2] and (ii) context\u00adbounding [19], a technique \nthat restricts the number of explored traces with respect to a user-supplied parameter, which Concuerror \ncalls preemption bound. In this respect, Concuerror is similar to the CHESS tool [20]. Our implementation \nWe extended Concuerror with three DPOR algorithms: (i) the algorithm presented by Flanagan and Godefroid \nwith the sleep set extension [5], (ii) source-DPOR and (iii) optimal-DPOR. To implement these we had \nto encode rules for dependencies between operations that constitute preemption points. These rules are \nshared between all DPOR variants. For lookups and inserts to ETS tables (i.e. reads and writes) the rules \nare standard (two operations con.ict if they act on the same key and at least one is an insert). For \nsending and receiving operations the happens before relation (.E ) is the following: Two sends are ordered \nby .E if they send to the same process, even if the messages are the same. (Note that if we would not \norder two sends that send the same message, then when we reorder them, the corresponding receive will \nnot happen after the same send statement.)  A send happens before the receive statement that receives \nthe message it sent. A race exists between these statements only if the receive has an after clause. \n A receive which executes its after clause happens before a subsequent send which sends a message that \nit can consume.  There are also other race-prone primitives in Erlang, but it is beyond the scope of \nthis paper to describe how they interact. Concuerror uses a vector clock [16] for each process at each \nstate, to calculate the happens before relation for any two events. The calculation of the vector clocks \nuses the ideas presented in the original DPOR paper [4]. The only special case is for the association \nof a send with a receive, where we instrument the message itself with the vector clock of the sending \nprocess. 9. Experiments We report experimental results that compare the performance of the three DPOR \nalgorithms, which we will refer to as classic (for the algorithm of [5]), source and optimal . We run \nall benchmarks on a desktop with an i7-3770 CPU (3.40 GHz), 16GB of RAM running Debian Linux 3.2.0-4-amd64. \nThe machine has four physical cores, but presently Concuerror uses only one of them. In all benchmarks, \nConcuerror was started with the option -p inf, which instructs the tool to use an in.nite preemption \nbound, i.e., verify these programs. Performance on two standard benchmarks First, we report performance \non the two benchmarks from the DPOR paper [4]: flesystem and indexer. These are benchmarks that have \nbeen used to evaluate another DPOR variant (DPOR-CR [22]) and a technique based on unfoldings [11]. As \nboth programs use locks, we had to emulate a locking mechanism using Erlang. To make this translation \nwe used particular language features: flesystem: This benchmark uses two lock-handling primitives, called \nacquire and release. The assumptions made for these primitives are that an acquire and a release operation \non the same lock are never co-enabled and should therefore not tried to be interleaved in a different \nway than they occur. Thus, acquires are the only operations that can be swapped, if possible, to get \na different interleaving. We implemented the lock objects in Erlang as separate processes. To acquire \nthe lock, a process sends a message with its identi.er to the lock process and waits for a reply. Upon \nreceiving the message, the lock process uses the identi.er to reply and then waits for a release message. \nOther acquire messages are left in the lock s mailbox. Upon receiving the release message the lock process \nloops back to the start, retrieving the next acquire message and notifying the next process. This behavior \ncan be implemented in Erlang using two selective receives.  Traces Explored Time Benchmark classic source \noptimal classic source optimal flesystem(14) 4 2 2 0.54s 0.36s 0.35s flesystem(16) 64 8 8 8.13s 1.82s \n1.78s flesystem(18) 1024 32 32 2m11s 8.52s 8.86s flesystem(19) 4096 64 64 8m33s 18.62s 19.57s indexer(12) \n78 8 8 0.74s 0.11s 0.10s indexer(15) 341832 4096 4096 56m20s 50.24s 52.35s Table 1. Performance of DPOR \nalgorithms on two benchmarks. indexer: This benchmark uses a CAS primitive instruction to check whether \na speci.c entry in a matrix is 0 and set it to a new value. The Erlang way to do this, is to try to execute \nan insert_new operation on an ETS table: if another entry with the same key exists the operation returns \nfalse; otherwise the operation returns true and the table now contains the new entry. Both benchmarks \nare parametric on the number of threads they use. For flesystem we used 14, 16, 18 and 19 threads. For \nindexer we used 12 and 15 threads. Table 1 shows the number of traces that the algorithms explore as \nwell as the time it takes to explore them. It is clear that our algorithms, which in these benchmarks \nexplore the same (optimal) number of interleavings, beat classic DPOR with sleep sets, by a margin that \nbecomes wider as the number of threads increases. As a sanity check, K\u00e4hl\u00f6nen et al. [11] report that \ntheir unfolding-based method is also able to explore only 8 paths for indexer(12), while their prototype \nimplementation of DPOR extended with sleep sets and support for commutativity of reads and writes explores \nbetween 51 and 138 paths (with 85 as median value). The numbers we report (78 for classic DPOR and 8 \nfor our algorithms) are very similar. Performance on two synthetic benchmarks Next we compare the algorithms \non two synthetic benchmarks that expose differences between them. The .rst is the readers program of \nFigure 3. The results, for 2, 8 and 13 readers are shown in Table 2. For classic DPOR the number of explored \ntraces is O(3N ) here, while source\u00adand optimal-DPOR only explore 2N traces. Both numbers are exponential \nin N but, as can be seen in the table, for e.g. N = 13 source-and optimal-DPOR .nish in about one and \na half minute, while the DPOR algorithm with the sleep set extension [5] explores two orders of magnitude \nmore (mostly sleep-set blocked) traces and needs almost one and a half hours to complete. Traces Explored \nTime Benchmark classic source optimal classic source optimal readers(2) 5 4 4 0.02s 0.02s 0.02s readers(8) \n3281 256 256 13.98s 1.31s 1.29s readers(13) 797162 8192 8192 86m 7s 1m26s 1m26s lastzero(5) 241 79 64 \n1.08s 0.38s 0.32s lastzero(1O) 53198 7204 3328 4m47s 45.21s 27.61s lastzero(15) 9378091 302587 147456 \n1539m11s 55m 4s 30m13s Table 2. Performance of DPOR algorithms on more benchmarks. Variables: int array[0..N] \n:= {0,0,...,0}, i; Thread 0: for (i := N; array[i] != 0; i--); Thread j (j . 1..N): array[j] := array[j-1] \n+ 1; Figure 4. The pseudocode of the lastzero(N) benchmark. Traces Explored Time Benchmark classic source \noptimal classic source optimal dialyzer 12436 3600 3600 14m46s 5m17s 5m46s gproc 14080 8328 8104 3m 3s \n1m45s 1m57s poolboy 6018 3120 2680 3m 2s 1m28s 1m20s rushhour 793375 536118 528984 145m19s 101m55s 105m41s \nTable 3. Performance of DPOR algorithms on four real programs. flesystem(19) indexer(15) gproc rushhour \nclassic 92.98 245.32 557.31 24.01 source 66.07 165.23 480.96 24.01 optimal 76.17 174.60 481.07 31.07 \nTable 4. Memory consumption (in MB) for selected benchmarks. The second benchmark is the lastzero(N) \nprogram whose pseu\u00addocode is shown in Figure 4. Its N+1 threads operate on an array of N+1 elements which \nare all initially zero. In this program, thread 0 searches the array for the zero element with the highest \nindex, while the other N threads read one of the array elements and update the next one. The .nal state \nof the program is uniquely de.ned by the values of i and array[1..N]. What happens here is that thread \n0 has control .ow that depends on data that is exposed to races and repre\u00adsents a case when source-DPOR \nmay encounter sleep-set blocking, that the optimal-DPOR algorithm avoids. As can be seen in Table 2, \nsource-DPOR explores about twice as many traces than optimal-DPOR and, naturally, even if it uses a cheaper \ntest, takes almost twice as much time to complete. Performance on real programs Finally, we evaluate \nthe algo\u00adrithms on four Erlang applications. The programs are: (i) dialyzer: a parallel static code analyzer \nincluded in the Erlang distribution; (ii) gproc: an extended process dictionary(iii) poolboy: a worker \npool factory1; and (iv) rushhour: a program that uses processes and ETS tables to solve the Rush Hour \npuzzle in parallel. The last pro\u00adgram, rushhour, is complex but self-contained (917 lines of code). The \n.rst three programs, besides their code, call many modules from the Erlang libraries, which Concuerror \nalso instruments. The total number of lines of instrumented code for testing the .rst three programs \nis 44596, 9446 and 79732, respectively. Table 3 shows the results. Here, the performance differences \nare not as profound as in synthetic benchmarks. Still, some general conclusions can be drawn: (1) Both \nsource-and optimal-DPOR explore less traces than classic (from 50% up to 3.5 times fewer) and require \nless time to do so (from 42% up to 2.65 times faster). (2) Even in real programs, the number of sleep-set \nblocked explo\u00adrations is signi.cant. (3) Regarding the number of traces explored, source-DPOR is quite \nclose to optimal, but manages to completely avoid sleep-set blocked executions in only one program (in \ndialyzer). (4) Source-DPOR is faster overall, but only slightly so compared to optimal-DPOR even though \nit uses a cheaper test. In fact, its maxi\u00admal performance difference percentage-wise from optimal-DPOR \nis a bit less than 10% (in dialyzer again).  Although, due to space limitations, we do not include a \nfull set of memory consumption measurements, we mention that all algorithms have very similar, and quite \nlow, memory needs. Table 4 shows numbers for gpro c, the real program which requires most memory, and \nfor all benchmarks where the difference between source and optimal is more than one MB. From these numbers, \nit can also be deduced that the size of the wakeup tree is small. In fact, the average size of the wakeup \ntrees for these programs is less than three nodes. 1 https://github.com/uwiger/gproc and https://github.com/devinus/poolboy \n  10. Related Work In early approaches to stateless model checking [8], it was observed that reduction \nwas needed to combat the explosion in number of explored interleavings. Several reduction methods have \nbeen pro\u00adposed including partial order reduction and context bounding [19]. Since early persistent set \ntechniques [3, 6, 26] relied on static analy\u00ad sis, sleep set techniques were used in VeriSoft [7]. It \nwas observed that sleep sets are suf.cient to prevent the complete exploration of different but equivalent \ninterleavings [10], but additional techniques were needed to reduce sleep-set blocked exploration. Dynamic \npartial order reduction [4] showed how to construct persistent sets on-the-.y by need , leading to better \nreduction. Sim\u00adilar techniques have been applied in testing and symbolic execution, e.g., to concolic \ntesting, where new test runs are initiated in response to detected races [24]. Several variants, improvements, \nand adapta\u00ad tions of DPOR for stateless model checking [14, 25] and concolic testing [22, 23] have appeared, \nall based on persistent sets. Our algorithms can be applied to all these contexts to provide increased \nor optimal reduction in the number of explored interleavings. A related area is reachability testing, \nin which test executions of concurrent programs are steered by the test harness. Lei and Carver present \na technique for exploring all Mazurkiewicz traces in a setting with a restricted set of primitives (message \npassing and monitors) for process interaction [15]. The scheduling of new test executions explicitly \npairs message transmissions with receptions, and could potentially require signi.cant memory, compared \nto the more light\u00adweight approach of software model checking. The technique of Lei and Carver guarantees \nto avoid re-exploration of different but equivalent maximal executions (corresponding to Theorem 7.5), \nbut reports blocked executions. Kahlon et al. [12] present a normal form for executions of concur\u00ad rent \nprograms and prove that two different normal-form executions are not in the same Mazurkiewicz trace. \nThis normal form can be exploited by SAT-or SMT-based bounded model checkers, but it can not be used \nby stateless model checkers that enumerate the exe\u00adcution sequences by state-space exploration. K\u00e4hk\u00f6nen \net al. [11] use unfoldings [18], which can also obtain optimal reduction in number of interleavings. \nHowever, this technique has signi.cantly larger overhead than DPOR-like techniques, and also needs an \naddi\u00adtional post-processing step for checking non-local properties such as races and deadlocks. A technique \nfor using transition-based partial order reduction for message-passing programs, without moving to an \nevent-based formulation is to re.ne the concept of dependency between transitions to that of conditional \ndependency [6, 9, 13]. 11. Conclusion We have presented optimal-DPOR, a new DPOR algorithm which is the \n.rst to be provably optimal in that it is guaranteed both to explore the minimal number of executions \nand to avoid sleep set blockings. It is based on a novel class of sets, called source sets. Source sets \nmake existing DPOR algorithms signi.cantly more ef.cient, and can be extended with wakeup trees to achieve \noptimality. In the derivation of the optimal algorithm, we have .rst presented a simpler algorithm, source-DPOR, \nwhich maintains less information than the optimal algorithm. On the other hand, the extra overhead of \nmaintaining wakeup trees is very moderate in practice (never more than 10% in our experiments), which \nis a good trade-off for having an optimality guarantee and the possibility to run arbitrarily faster. \nWe intend to further explore the ideas behind source sets and wakeup trees, not only for veri.cation \nbut also for new ways of testing programs. Acknowledgments This work was carried out within the Linnaeus \ncentre of excellence UPMARC (Uppsala Programming for Multicore Architectures Research Center) and was \nsupported in part by the EU FP7 STREP project RELEASE (287510) and the Swedish Research Council. References \n[1] J. Armstrong. Erlang. Comm. of the ACM, 53(9):68 75, 2010. [2] M. Christakis, A. Gotovos, and K. \nSagonas. Systematic testing for detecting concurrency errors in Erlang programs. In ICST, 2013. [3] E. \nM. Clarke, O. Grumberg, M. Minea, and D. Peled. State space reduction using partial order techniques. \nSTTT, 2:279 287, 1999. [4] C. Flanagan and P. Godefroid. Dynamic partial-order reduction for model checking \nsoftware. In POPL, pages 110 121. ACM, 2005. [5] C. Flanagan and P. Godefroid. Addendum to Dynamic partial-order \nreduction for model checking software, 2005. Available at http: //research.microsoft.com/en-us/um/people/pg/. \n[6] P. Godefroid. Partial-Order Methods for the Veri.cation of Concurrent Systems: An Approach to the \nState-Explosion Problem. PhD thesis, University of Li\u00e8ge, 1996. Also, volume 1032 of LNCS, Springer. \n[7] P. Godefroid. Model checking for programming languages using VeriSoft. In POPL, pages 174 186. ACM \nPress, 1997. [8] P. Godefroid. Software model checking: The VeriSoft approach. Formal Methods in System \nDesign, 26(2):77 101, 2005. [9] P. Godefroid and D. Pirottin. Re.ning dependencies improves partial\u00adorder \nveri.cation methods. In CAV, volume 697 of LNCS, 1993. [10] P. Godefroid, G. J. Holzmann, and D. Pirottin. \nState-space caching revisited. Formal Methods in System Design, 7(3):227 241, 1995. [11] K. K\u00e4hk\u00f6nen, \nO. Saarikivi, and K. Heljanko. Using unfoldings in automated testing of multithreaded programs. In ASE, \npages 150 159. ACM, 2012. [12] V. Kahlon, C. Wang, and A. Gupta. Monotonic partial order reduction: An \noptimal symbolic partial order reduction technique. In CAV, volume 5643 of LNCS, pages 398 413. Springer, \n2009. [13] S. Katz and D. Peled. De.ning conditional independence using collapses. Theoretical Computer \nScience, 101:337 359, 1992. [14] S. Lauterburg, R. Karmani, D. Marinov, and G. Agha. Evaluating ordering \nheuristics for dynamic partial-order reduction techniques. In FASE, volume 6013 of LNCS, pages 308 322. \nSpringer, 2010. [15] Y. Lei and R. Carver. Reachability testing of concurrent programs. IEEE Trans. Softw. \nEng., 32(6):382 403, 2006. [16] F. Mattern. Virtual time and global states of distributed systems. In \nM. Cosnard, editor, Proc. Workshop on Parallel and Distributed Algorithms, pages 215 226, Ch. de Bonas, \nFrance, 1989. Elsevier. [17] A. Mazurkiewicz. Trace theory. In Advances in Petri Nets, 1986. [18] K. \nMcMillan. A technique of a state space search based on unfolding. Formal Methods in System Design, 6(1):45 \n65, 1995. [19] M. Musuvathi and S. Qadeer. Iterative context bounding for systematic testing of multithreaded \nprograms. In PLDI, pages 446 455, 2007. [20] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. Nainar, \nand I. Neamtiu. Finding and reproducing heisenbugs in concurrent programs. In OSDI, pages 267 280. USENIX \nAssociation, 2008. [21] D. Peled. All from one, one for all, on model-checking using represen\u00adtatives. \nIn CAV, volume 697 of LNCS, pages 409 423, 1993. [22] O. Saarikivi, K. K\u00e4hk\u00f6nen, and K. Heljanko. Improving \ndynamic partial order reductions for concolic testing. In ACSD. IEEE, 2012. [23] K. Sen and G. Agha. \nAutomated systematic testing of open distributed programs. In FASE, volume 3922 of LNCS, pages 339 356, \n2006. [24] K. Sen and G. Agha. A race-detection and .ipping algorithm for automated testing of multi-threaded \nprograms. In Haifa Veri.cation Conference, volume 4383 of LNCS, pages 166 182. Springer, 2007. [25] S. \nTasharo. et al. TransDPOR: A novel dynamic partial-order reduction technique for testing actor programs. \nIn FMOODS/FORTE, volume 7273 of LNCS, pages 219 234. Springer, 2012. [26] A. Valmari. Stubborn sets for \nreduced state space generation. In Advances in Petri Nets, volume 483 of LNCS, pages 491 515, 1990. \n  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Stateless model checking is a powerful technique for program verification, which however suffers from an exponential growth in the number of explored executions. A successful technique for reducing this number, while still maintaining complete coverage, is Dynamic Partial Order Reduction (DPOR). We present a new DPOR algorithm, which is the first to be provably optimal in that it always explores the minimal number of executions. It is based on a novel class of sets, called <i>source sets</i>, which replace the role of persistent sets in previous algorithms. First, we show how to modify an existing DPOR algorithm to work with source sets, resulting in an efficient and simple to implement algorithm. Second, we extend this algorithm with a novel mechanism, called <i>wakeup trees</i>, that allows to achieve optimality. We have implemented both algorithms in a stateless model checking tool for Erlang programs. Experiments show that source sets significantly increase the performance and that wakeup trees incur only a small overhead in both time and space.</p>", "authors": [{"name": "Parosh Abdulla", "author_profile_id": "81100490166", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P4383850", "email_address": "parosh@it.uu.se", "orcid_id": ""}, {"name": "Stavros Aronis", "author_profile_id": "81488667954", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P4383851", "email_address": "Stavros.Aronis@it.uu.se", "orcid_id": ""}, {"name": "Bengt Jonsson", "author_profile_id": "81100619282", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P4383852", "email_address": "bengt@it.uu.se", "orcid_id": ""}, {"name": "Konstantinos Sagonas", "author_profile_id": "81100605481", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P4383853", "email_address": "kostis@it.uu.se", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535845", "year": "2014", "article_id": "2535845", "conference": "POPL", "title": "Optimal dynamic partial order reduction", "url": "http://dl.acm.org/citation.cfm?id=2535845"}