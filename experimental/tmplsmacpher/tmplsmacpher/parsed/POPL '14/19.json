{"article_publication_date": "01-08-2014", "fulltext": "\n Sound Compilation of Reals Eva Darulova EPFL eva.darulova@ep..ch Abstract Writing accurate numerical \nsoftware is hard because of many sources of unavoidable uncertainties, including .nite numerical precision \nof implementations. We present a programming model where the user writes a program in a real-valued implementation \nand speci.cation language that explicitly includes different types of uncertainties. We then present \na compilation algorithm that gen\u00aderates a .nite-precision implementation that is guaranteed to meet the \ndesired precision with respect to real numbers. Our compila\u00adtion performs a number of veri.cation steps \nfor different candidate precisions. It generates veri.cation conditions that treat all sources of uncertainties \nin a uni.ed way and encode reasoning about .nite\u00adprecision roundoff errors into reasoning about real \nnumbers. Such veri.cation conditions can be used as a standardized format for verifying the precision \nand the correctness of numerical programs. Due to their non-linear nature, precise reasoning about these \nver\u00adi.cation conditions remains dif.cult and cannot be handled using state-of-the art SMT solvers alone. \nWe therefore propose a new pro\u00adcedure that combines exact SMT solving over reals with approx\u00adimate and \nsound af.ne and interval arithmetic. We show that this approach overcomes scalability limitations of \nSMT solvers while providing improved precision over af.ne and interval arithmetic. Our implementation \ngives promising results on several numerical models, including dynamical systems, transcendental functions, \nand controller implementations. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Program \nVeri.cation Keywords roundoff error; .oating-point arithmetic; .xed-point arithmetic; veri.cation; compilation; \nembedded systems; numeri\u00adcal approximation; scienti.c computing; sensitivity analysis 1. Introduction \nWriting numerical programs is dif.cult, in part because the pro\u00adgrammer needs to deal not only with the \ncorrectness of the algo\u00adrithm but also with different forms of uncertainties. Program inputs may not \nbe exactly known because they come from physical exper\u00adiments or were measured by an embedded sensor. \nThe computation itself suffers from roundoff errors at each step, because of the use of .nite-precision \narithmetic. In addition, resources like energy may Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. POPL 14, \nJanuary 22 24, 2014, San Diego, CA, USA. Copyright c &#38;#169; 2014 ACM 978-1-4503-2544-8/14/01. . . \n$15.00. http://dx.doi.org/10.1145/2535838.2535874 Viktor Kuncak EPFL viktor.kuncak@ep..ch be scarce \nso that only a certain number of bits are available for the numerical data type. At the same time, the \ncomputed results in these domains can have far-reaching consequences if used to control, for example, \na vehicle or a nuclear power plant. It is therefore becoming increas\u00adingly urgent to develop tools that \nimprove con.dence in numer\u00adical code [38]. One of the .rst challenges in doing this is that most of our \nautomated reasoning tools work with real arithmetic, whereas the code is implemented with .nite-precision. \nMany cur\u00adrent approaches to verify numerical programs start with the .nite\u00adprecision implementation and \nthen try to verify the absence of (run\u00adtime) errors. Not only are such veri.cation results speci.c to \na given representation of numbers, but the absence of run-time errors does not guarantee that program \nbehavior matches the desired spec\u00adi.cation expressed using real numbers. Fundamentally, the source code \nsemantics is currently expressed in terms of low-level data types such as .oating-points. This is problematic \nnot only for de\u00advelopers but also for compiler optimizations, because, e.g., the as\u00adsociativity law is \nunsound with respect to such source code seman\u00adtics. In this paper we advocate a natural but ambitious \nalternative: source code programs should be expressed in terms of mathemati\u00adcal real numbers. In our \nsystem, the programmer writes a program using a Real data type, states the desired postconditions, and \nspec\u00adi.es explicitly the uncertainties as well as the desired target preci\u00adsion. It is then up to our \ntrustworthy compiler to check, taking into account all uncertainties and their propagation, that the \ndesired pre\u00adcision can be soundly realized in a .nite-precision implementation. If so, the compiler chooses \nand emits one such implementation, selecting from a range of (software or hardware) .oating-point or \n.xed-point arithmetic representations. A key question that such a compiler needs to answer is whether \na given .nite-precision representation remains close enough to an ideal implementation in terms of real \nnumbers. To answer this ques\u00adtion, we present a method to generate veri.cation conditions that encode \nreasoning about .nite-precision roundoff errors into rea\u00adsoning about real numbers. Our veri.cation conditions \nexplicitly model the ideal program without external uncertainties and round\u00adoffs, the actual program, \nwhich is executed in .nite precision with possibly noisy inputs, and the relationship between the two. \nSolv\u00ading such veri.cation conditions is one of the key tasks of a sound compiler for reals. Our approach \nis parametric in the bitwidth of the representations and can thus be used on different platforms, from \nembedded con\u00adtrollers without .oating-point units (where .xed-point implementa\u00adtions are needed), to \nplatforms that expose high-precision .oating\u00adpoint arithmetic in their instruction set architecture. \nUsing libraries we can also emit code that uses precision twice or four times that of the ubiquitous \ndouble data type. When multiple representations are available, the compiler can select, e.g., the smallest \nrepresen\u00adtation needed to deliver the desired number of trusted signi.cant digits.  To summarize, viewing \nsource code as operating on real num\u00adbers has many advantages: Programmers can reason about correctness \nusing real arithmetic instead of .nite-precision arithmetic. We achieve separation of the design of algorithms \n(which may still be approximate for other reasons) from their realization using .nite-precision computations. \n We can verify the ideal meaning of programs using techniques developed to reason over real numbers, \nwhich are more scalable and better understood than techniques that directly deal with .nite-precision \narithmetic.  The approach allows us to quantify the deviation of implemen\u00adtation outputs from ideal \nones, instead of merely proving e.g. range bounds of .oating-point variables which is used in sim\u00adpler \nstatic analyses.  The compiler for reals is free to do optimizations as long as they preserve the precision \nrequirements. This allows the com\u00adpiler to apply, for example, associativity of arithmetic [17], or even \nselect different approximation schemes for transcendental functions.  In addition to roundoff errors, \nthe approach also allows the developer to quantify program behavior in the face of external uncertainties \nsuch as input measurement errors.  Using our veri.cation conditions, the correctness and the preci\u00adsion \nof compilation for small programs can, in principle, be directly veri.ed using an SMT solver such as \nZ3 [20] (see Section 5.3). The capabilities of such solvers are likely continue to improve as the solvers \nadvance, bene.ting our approach. However, the com\u00adplexity of the generated veri.cation conditions for \nlarger programs is currently out of reach of such solvers and we believe that spe\u00adcialized techniques \nare and will continue to be necessary for this task. This paper presents two specialized techniques that \nimprove the feasibility of the veri.cation task. The .rst technique performs local approximation and \nis effective even in benchmarks contain\u00ading nonlinear arithmetic. The second technique speci.cally handles \nconditional expressions. 1.1 Solving Non-Linear Constraints. Forward Propagation Nonlinear arithmetic \nposes a signi.cant challenge for veri.cation because it cannot directly be handled using Simplex-like \nalgorithms embedded inside SMT solvers. Although interesting relevant frag\u00adments are decidable and are \nsupported by modern solvers, the com\u00adplexity of solving such constraints is much higher, in terms of \nboth worst-case complexity and the experience in practice. Un\u00adfortunately, non-linear arithmetic is ubiquitous \nin numerical soft\u00adware. Furthermore, our veri.cation conditions add roundoff error terms to arithmetic \nexpressions, so the resulting constraints grow further in complexity, often becoming out of reach of \nsolvers. An alternative to encoding into SMT solver input is to use a sound and over-approximating arithmetic \nmodel such as interval or af.ne arithmetic [19]. However, when used by itself on non-linear code, these \napproaches yield too pessimistic results, failing to establish any bounds on precision in a number of \nuseful benchmarks. We show that we can combine range arithmetic computation with SMT solving to overcome \nthe limitations of each of the indi\u00advidual techniques. From the point of view of the logical encoding \nof the problem, range arithmetic becomes a specialized method to per\u00adform approximate quanti.er elimination \nof bounded variables that describe the uncertainties. We obtain a sound, precise, and some\u00adwhat scalable \nprocedure. During range computation, our technique also checks for common problems such as over.ow, division \nby zero or square root of a negative number, emitting the correspond\u00ading warnings. Because the procedure \nis a forward computation, it is suitable for automatically generating function summaries contain\u00ading \noutput ranges and errors of a function. This is a feature that SMT solvers do not solve by themselves, \nbecause their primary functionality is answering formula satis.ability questions. 1.2 Sound Compilation \nof Conditionals In the presence of uncertainties, conditional branches become an\u00adother veri.cation challenge. \nNamely, the ideal execution may fol\u00adlow one branch, but, because of input or roundoff errors, the actual \nexecution follows another. This behavior may be acceptable, how\u00adever, if we can show that the error on \nthe output remains within required bounds. Our approach could bene.t from modular au\u00adtomated analysis \nof continuity, which was advocated previously [53]. Because we are interested in concrete bounds, we \npresent a new method to check that different paths taken by real-valued and .nite-precision versions \nof the program still preserve the de\u00adsired precision speci.cation. Our check does not require continuity \n(which can be dif.cult to prove for non-linear code). Instead, it di\u00adrectly checks that the difference \nbetween the two values on different branches meets the required precision. This technique extends our \nmethod for handling non-linear arithmetic, so it bene.ts from the combination of range arithmetic and \nSMT solving. 1.3 Implementation and Evaluation We have implemented our compilation and veri.cation procedure, \nincluding the veri.cation condition generation, analysis of possibly non-linear expressions, and the \nhandling of conditionals. Our sys\u00adtem is implemented as an extension of the Leon veri.er for func\u00adtional \nScala programs [7]. The implementation relies on a range arithmetic implementation [15] for Scala as \nwell as on the Z3 SMT solver [20]. We have evaluated the system on a number of diverse benchmarks, obtaining \npromising results. Our implementation and the benchmarks are available from http://lara.epfl.ch/w/rosa \nTo support programming of larger code fragments, our system also supports a basic modular veri.cation \ntechnique, which handles functions by replacing calls with function postconditions or by in\u00adlining bodies \nof called functions. We thus expect that our technique is applicable to larger code bases as well, possibly \nthrough refac\u00adtoring code into multiple smaller and annotated functions. Even on the benchmarks that \nwe release, we are aware of no other available system that would provide the same guarantees with our \nlevel of automation.  1.4 Summary of Contributions Our overall contribution is an approach for sound \ncompilation of real numbers into .nite-precision representation. Speci.cally: We present a real-valued \nimplementation and speci.cation lan\u00adguage for numerical programs with uncertainties; we de.ne its semantics \nin terms of veri.cation constraints that they induce. We believe that such veri.cation conditions can \nbe used as a standardized format for verifying the precision and the correct\u00adness of numerical programs. \n We develop an approximation procedure for computing precise range and error bounds for nonlinear expressions \nwhich com\u00adbines SMT solving with interval arithmetic. We show that such an approach signi.cantly improves \ncomputed range and error bounds compared to standard interval arithmetic, and scales better than SMT \nsolving alone. Our procedure can also be used independently as a more precise alternative to interval \narith\u00admetic, and thus can perform forward computation without re\u00adquiring function postconditions to be \nprovided.   We describe an approach for soundly computing error bounds in the presence of branches \nand uncertainties, which ensures soundness of compilation in case the function de.ned by a program with \nconditionals.  We have implemented our framework and report our experience on a set of diverse benchmarks, \nincluding benchmarks from physics, biology, chemistry, and control systems. The results show that our \ntechnique is effective and that it achieves a syn\u00adergy of the techniques on which it relies.  2. Example \nWe demonstrate some aspects of our system on the example writ\u00adten in the Scala programming language [45] \nin Figure 1. The meth\u00adods triangle and triangleSorted compute the area of a triangle with side lengths \na, b and c. We consider a particular application where the user may have two side lengths given, and \nmay vary the third. She has two functions available to do the computation and wants to determine whether \neither or both satisfy the precision require\u00adment of 1e-11 on line 8. require and ensuring give the pre-and \npostconditions of functions, which are written in Scala. Notation res +/- 1e-11 denotes that the return \nvalue res should have an abso\u00adlute error of at most 1e-11 compared to the ideal computation over reals. \nOur tool determines that such requirement needs at least dou\u00adble .oating-point precision and emits the \ncorresponding code. In general, the challenge is to establish that this precision is suf.cient to ensure \nthe required bounds, given that errors in .nite-precision code accumulate and grow without an a priori \nbound. Our tool veri.es fully automatically that the method triangleSorted indeed satis.es the postcondition \nand generates the source code with the Double data type which also includes a more precise and complete \npostcondition on main: 0.01955760940017617 = res . res = 12.51998402556971 . res +/- 8.578997409317759e-12 \n To achieve this result, our tool .rst checks that the precondi\u00adtion of the function call is satis.ed \nusing the Z3 solver. Then, it inlines the body of the function triangleSorted and computes a sound bound \non the result s uncertainty with our approximation proce\u00addure. It uses the computed bounds to show that \nthe postcondition of main is satis.ed. The error computation takes into account in a sound way the input \nuncertainty (here: an initial roundoff error on the inputs), its propagation, and roundoff errors committed \nat each arithmetic operation. Additionally, due to the roundoff error, the comparison on line 21 may \ngive a different truth value in bound\u00adary cases. Certain .oating-point computations will therefore take \na different branch than their corresponding real-valued computa\u00adtion. More precisely, the total error \nwhen computing the condition is 2e - 15, as computed by our tool. That is, .oating-point val\u00adues that \nsatisfy a < b + 2e - 15 may take the else branch, even though the corresponding real values would follow \nthe then branch, and similarly in the opposite direction. Our tool veri.es that, de\u00adspite this phenomenon, \nthe difference in the computed result in two branches remains within the precision requirement. Intuitively, \nthe values of computed branches are close for the interval where the truth value of the condition changes, \nand these values in real-valued and .nite-precision implementation are close to each other. Finally, \nour tool uses our novel range computation procedure to also .nd a more precise output range than we could \nhave obtained in, e.g., interval arithmetic or any static analysis method that does not dif\u00adferentiate \nroundoff error variations from the fact that a program can be ran over an interval of inputs. Our tool \ncomputes the range [0.0195, 12.52] for both methods, but shows a difference in the ab\u00adsolute error of \nthe computation. For the method triangle, the veri\u00ad.cation fails, because the computed error (2.3e - \n11) exceeds the def main(a: Real, b: Real, c: Real): Real = { 2 require(4.500005 = a &#38;&#38; a = \n6.5) val b = 4.0 4 val c = 8.5 //val area = triangle(a, b, c) 6 val area = triangleSorted(a, b, c) \narea 8 } ensuring(res . res +/- 1e-11) 10 def triangle(a: Real, b: Real, c: Real): Real = {require(1 \n< a &#38;&#38; a < 9 &#38;&#38; 1 < b &#38;&#38; b < 9 &#38;&#38; 1 < c &#38;&#38; c < 9 &#38;&#38; 12 \na + b > c + 1e-6 &#38;&#38; a + c > b + 1e-6 &#38;&#38; b + c > a + 1e-6) val s = (a + b + c)/2.0 14 \nsqrt(s * (s - a) * (s - b) * (s - c)) } 16 def triangleSorted(a: Real, b: Real, c: Real): Real = {18 \nrequire(1 < a &#38;&#38; a < 9 &#38;&#38; 1 < b &#38;&#38; b < 9 &#38;&#38; 1 < c &#38;&#38; c < 9 &#38;&#38; \na + b > c + 1e-6 &#38;&#38; a + c > b + 1e-6 &#38;&#38; b + c > a + 1e-6 &#38;&#38; 20 a < c &#38;&#38; \nb < c) if (a < b) {22 sqrt((c+(b+a)) * (a-(c-b)) * (a+(c-b)) * (c+(b-a)))/4.0 } else {24 sqrt((c+(a+b)) \n* (b-(c-a)) * (b+(c-a)) * (c+(a-b))) / 4.0 }26 } Figure 1. Computing the area of a triangle with a given \nprecision. required precision bound. This result is expected the textbook for\u00admula for triangles is known \nto suffer from imprecision for .at trian\u00adgles [35], which is somewhat recti.ed in the method triangleSorted. \nOn the other hand, our tool proves that using triangleSorted delivers the desired precision of 10-11 \non the result. 3. Programs with Reals Each program to be compiled consists of one top-level object with \nmethods written in a functional subset of the Scala programming language [45]. All methods are functions \nover the Real data type and the user annotates them with pre-and postconditions that ex\u00adplicitly talk \nabout uncertainties. Real represents ideal real num\u00adbers without any uncertainty. We allow arithmetic \nexpressions over v Reals with the standard arithmetic operators {+, -, *, /, }, and together with conditionals \nand function calls they form the body of methods. Our tool also supports immutable variable declarations \nas val x = .... This language allows the user to de.ne a computation over real numbers. Note that this \nspeci.cation language is not exe\u00adcutable. The precondition allows the user to provide a speci.cation \nof the environment. A complete environment speci.cation consists of lower and upper bounds for all method \nparameters and an upper bound on the uncertainty or noise. Range bounds are expressed with regular comparison \noperators. Uncertainty is expressed with a predicate such as x +/- 1e-6, which denotes that the variable \nx is only known up to 1e - 6. Alternatively, the programmer can specify the relative error as x +/- 1e-7 \n* x. If no noise except for roundoff is present, roundoff errors are automatically added to input variables. \nThe postcondition can specify constraints on the output, and in particular the range and the maximum \naccepted uncertainty. In ad\u00addition to the language allowed in the precondition, the postcondi\u00adtion may \nreference the errors on inputs directly in the following way: res +/- 3.5 * !x, which says that the maximum \nacceptable error on the output variable res is bounded from above by 3.5 times the initial error on x. \nWhereas the precondition may only talk about the ideal values, the postcondition can also reference the \nactual value directly via ~x. This allows us to assert that runtime values will not exceed a certain \nrange, for instance.  Floating-point arithmetic Our tool and technique support in the generated target \ncode any .oating-point precision and in par\u00adticular, single and double .oating-point precision as de.ned \nby the IEEE 754 .oating-point standard [51]. We assume rounding\u00adto-nearest rounding mode and that basic \narithmetic operations v {+, -, *, /, } are rounded correctly, which means that the re\u00adsult from any such \noperation must be the closest representable .oating-point number. Hence, provided there is no over.ow \nand the numbers are not in denormal range, the result of a binary oper\u00adation .F satis.es x .F y = (x \n.R y)(1 + d), |d| = EM , . . {+, -, *, /} (1) where .R is the ideal operation in real numbers and EM \nis the machine epsilon that determines the upper bound on the relative error. This model provides a basis \nfor our roundoff error estimates. When there is a possibility of an over.ow or denormal values, our analysis \nreports an error. An extension of the analysis to handle denormals is straightforward, by adding an additional \nerror term in Equation 1. Fixed-point arithmetic Our tool and technique also support stan\u00addard .xed-point \narithmetic; for more details see [3]. Our precision analysis supports any bit-width. Our code generator \ngenerates code for 16 and 32 bit .xed-point arithmetic, which are the most com\u00admon choices, using integers \nand bit-shifts. Other .nite-precision representations of rationals The tech\u00adniques described in this \npaper are general in that they are also applicable to other arithmetic representations, as long as roundoff \nerrors can be computed at each computation step from the ranges of variables. Examples include .oating-point \nimplementations with a different number of bits for the exponent and mantissa, or redun\u00addant arithmetic. \n4. Compiling Reals to Finite Precision Given a speci.cation or program over reals and possible target \ndata type(s), our tool generates code over .oating-point or .xed-point numbers that satisfy the given \npre-and postconditions (and thus meet the target precision). Figure 2 presents a high-level view of our \ncompilation algorithm. Our tool .rst analyses the entire speci\u00ad.cation and generates one veri.cation \ncondition for each postcon\u00addition to be proven. To obtain a modular algorithm, the tool also generates \nveri.cation conditions that check that at each function call the precondition of the called function \nis satis.ed. The meth\u00adods are then sorted by occurring function calls. This allows us to re\u00aduse already \ncomputed postconditions of function calls in a modular analysis. If the user speci.es one target data \ntype, the remaining part of the compilation process is performed with respect to this data type s precision. \nIf not or in the case the user speci.ed sev\u00aderal possible types, our tool will perform a binary search \nover the possible types to .nd the least in the list that satis.es all speci.ca\u00adtions. The user of our \ntool can provide the list of possible data types manually and sort them by her individual preference. \nCurrently, the analysis is performed separately for each data type, which is not a big issue due to the \nrelatively small number of alternatives. We did identify certain shared computations between iterations; \nwe can ex\u00adploit them in the future for more ef.cient compilation. In order for the compilation process \nto succeed, the speci.cation has to be met with respect to a given .nite-precision arithmetic, thus the \nprincipal part of our algorithm is spent in veri.cation, which we describe in Section 5. We envision \nthat in the future the compilation task will also include automatic precision-preserving code optimizations, \nbut in Input: spec: speci.cation over Reals, prec: candidate precisions for fnc . spec.fncs fnc.vcs = \ngenerateVCs(fnc) spec.fncs.sortBy((f1, f2) => f1 . f2.fncCalls) while prec = \u00d8 and notProven(spec.fncs) \nprecision = prec.nextPrecise for fnc . spec.fncs for vc . fnc.vcs while vc.hasNextApproximation . notProven(vc) \napprox = getNextApproximation(vc, precision) vc.status = checkWithZ3(approx) generateSpec(fnc) generateCode(spec) \nOutput: .oating-point or .xed-point code Figure 2. Compilation algorithm. this paper we concentrate on \nthe challenging groundwork of veri\u00adfying the precision of code. Our tool can currently generate Scala \ncode over .xed-point arithmetic with a 16 or 32 bit width, or  .oating-point arithmetic in single (32 \nbit), double (64 bit), double-double (128 bit) and quad-double (256 bit) precision.  We currently do \nnot support mixing of data types in one program, but plan to explore this avenue in the future. For double-double \nand quad-double precision, which were implemented in software by [5], we provide a Scala interface to \nthe library with the generated code. In case the veri.cation part of compilation fails, our tool prints \na failure report with the best postconditions our tool was able to compute. The user can then use the \ngenerated speci.cations to gain insight why and where her program does not satisfy the requirements. \nWhile we have implemented our tool to accept speci.cations in a domain speci.c language embedded in Scala \nand generate code in Scala, all our techniques apply equally to all programming languages and hardware \nthat follow the .oating-point abstraction we assume (Equation 1). 5. Verifying Real Programs We will \nnow describe the veri.cation part of our compilation al\u00adgorithm. In the following we will call the ideal \ncomputation the computation in the absence of any uncertainties and implemented in a real arithmetic, \nand the actual computation the computation that will .nally be executed in .nite-precision and with potentially \nuncertain inputs. 5.1 Veri.cation Conditions for Loop-Free Programs For each method with a precondition \nP and a postcondition Q our approach considers the following veri.cation condition: .yx, yx) . body(yres) \n. Q(yres) res, y . P (yx, y , yx, y(*) where yx, y res, y denote the input, output and local variables \nre\u00adspectively. Table 1 summarizes how veri.cation constraints are generated from our speci.cation language \nfor .oating-point arith\u00admetic. Each variable x in the speci.cation corresponds to two real\u00advalued variables \nx, x., the ideal one in the absence of uncertainties and roundoff errors and the actual one, computed \nby the compiled program. Note that the ideal and actual variables are related only through the error \nbounds in the pre-and postconditions, which al\u00ad  a <= x &#38;&#38; x <= b x . [a, b] x +/- k x. = x \n+ errx . errx . [-k, k] x +/- m * x x. = x + errx . errx . [-|mx|, |mx|] ~x x. !x errx x < y (ideal part) \n(x 0 y) x < y (actual part) (x. 0 y.)(1 + d1) sqrt(x) (ideal part) sqrt(x) sqrt(x) (actual part) sqrt(x.)(1 \n+ d2) val z = x z = x . z. = x. if (c(x)) e1(x) ((c(x) . e1(x)) . (\u00acc(x) . e2(x))). else e2(x) ((c.(x.).e1.(x.)).(\u00acc.(x.).e2.(x.))) \ng(x) g(x) . g.(x.) 0 . {+, -, *, /} -Em = di . di = Em, all d are fresh cond. and e. denote functions \nwith roundoff errors at each step Table 1. Semantics of our speci.cation language. lows for the ideal \nand actual executions to take different paths. In the method body we have to take into account roundoff \nerrors from arithmetic operations and the propagation of existing errors. Our v system currently supports \noperations {+, -, *, /, }, but these can be in principle extended to elementary functions, for instance \nby encoding them via Taylor expansions [41]. Note that the resulting veri.cation conditions are parametric \nin the machine epsilon. For .xed-point arithmetic constraints such as above can also be generated, although \nthe translation is more complex due to the fact that .xed-point formats have to be statically determined. \nRoundoff error can thus no longer be encoded by a formula such as Equa\u00adtion 1. One possible translation \nis to speculatively assign .xed\u00adpoint formats (including the position of the .xed point) based on real-valued \nranges, and then verify that the resulting constraint with the assigned formats still holds in the presence \nof roundoff errors. However, because a direct attempt to solve veri.cation con\u00additions using an off-the-shelf \nsolver alone is not satisfactory (see Section 5.3), our tool directly uses the approximation procedure \nfrom Section 5.4 and computes sound ranges for .xed-point imple\u00admentation, allocating .xed-point formats \non the .y.  5.2 Speci.cation Generation In order to give feedback to developers and to facilitate automatic \nmodular analysis, our tool also provides automatic speci.cation generation. By this we mean that the \nprogrammer still needs to provide the environment speci.cation in form of preconditions, but our tool \nautomatically computes a precise postcondition. Formally, we can rewrite the constraint (*) as .yx, res. \n(.y . P (yx) . body(yx, y , res)) . Q(yx, res) where Q is now unknown. We obtain the most precise post\u00adcondition \nQ by applying quanti.er elimination (QE) to P (yx) . body(yx, y , res) and eliminate y . The theory of \narithmetic over re\u00adals admits QE so it is theoretically possible to use this approach. We do not currently \nuse a full QE procedure for speci.cation gen\u00aderation, as it is expensive and it is not clear whether \nthe returned expressions would be of a suitable format. Instead, we use our approximation approach which \ncomputes ranges and maximum er\u00adrors in a forward fashion and computes an (over) approximation of a postcondition \nof the form res . [a, b] . res \u00b1 u. When proving a def getNextApproximation(vc, precision): Figure 3. \nApproximation pipeline. postcondition, our tool automatically generates these speci.cations and provides \nthem as feedback to the user. 5.3 Dif.culty of Simple Encoding into SMT solvers For small functions \nwe can already prove interesting properties by using the exact encoding of the problem just described \nand dis\u00adcharging the veri.cation constraints with Z3. Consider the follow\u00ading code a programmer may write \nto implement the third B-spline basic function which is commonly used in signal processing [33]. def \nbspline3(u: Real): Real = { require(0 = u &#38;&#38; u = 1 &#38;&#38; u +/- 1e-13) -u*u*u / 6.0 } ensuring \n(res . -0.17 = res &#38;&#38; res = 0.05 &#38;&#38; res +/- 1e-11) Functions and the corresponding veri.cation \nconditions of this complexity are already within the possibilities of the nonlinear solver within Z3. \nFor more complex functions however, Z3 does not (yet) provide an answer in a reasonable time, or returns \nunknown. Whether alternative techniques in SMT solvers can help in such cases remains to be seen [10, \n34]. We here provide an approach based on step-wise approximation that addresses the dif.culty of general-purpose \nconstraint solving.  5.4 Veri.cation with Approximations To soundly compile more interesting programs, \nwe have devel\u00adoped an approximation procedure that computes a sound over\u00adapproximation of the range of \nan expression and of the uncertainty on the output. This procedure is a forward computation and we also \nuse it to generate speci.cations automatically. We describe the approximation procedure in detail in \nSection 6, for now we will as\u00adsume that it exists and, given a precondition P and an expression expr, \ncomputes a sound bound on the output range and its associ\u00adated uncertainty: ([a, b], err) = evalWithError(P, \nexpr) . .yx, yx., res, res..P (yx, yx.) . res = expr(yx) . res. = expr.(yx.) . (a = res . res = b) . \n|res - res.| < err We have identi.ed three possibilities for approximation: non\u00adlinear arithmetic, function \ncalls, and paths due to conditionals and each can be approximated at different levels. We have observed \nin our experiments, that one size does not .t all and a combina\u00adtion of different approximations is most \nsuccessful in proving the veri.cation conditions we encountered. For each veri.cation con\u00addition we thus \nconstruct approximations until Z3 is able to prove one, or until we run out of approximations where we \nreport the veri.cation as failed. We can thus view veri.cation as a stream of approximations to be proven. \nWe illustrate the pipeline that computes the different approximations in Figure 3. The routines getPathError, \ngetRange and evalWithError are described in the follow\u00ading sections in more detail.  The .rst approximation \n(indicated by the long arrow in Fig\u00adure 3) is to use Z3 alone on the entire constraint constructed by \nthe rules in Table 1. This is indeed an approximation, as all function calls are treated as uninterpreted \nfunctions in this case. As noted before, this approach only works in very simple cases or when no uncertainties \nand no functions are present. Then, taking all pos\u00adsible combinations of subcomponents in our pipeline \nwe obtain the other approximations, which are .ltered accordingly depend\u00ading on whether the constraint \ncontains function calls or conditional branches. Function calls If the veri.cation constraint contains \nfunction calls and the .rst approximation failed, our tool will attempt to in\u00adline postconditions and \npass on the resulting constraint down the approximation pipeline. We support inlining of both user-provided \npostconditions and postconditions computed by our own speci.\u00adcation generation procedure. If this still \nis not precise enough, we inline the entire function body. Postcondition inlining is implemented by replacing \nthe function call with a fresh variable and constraining it with the postcondi\u00adtion. Thus, if veri.cation \nsucceeds with inlining the postcondition, we avoid having to consider each path of the inlined function \nsep\u00adarately and can perform modular veri.cation avoiding a potential path explosion problem. Such modular \nveri.cation is not feasible when postconditions are too imprecise and we plan to explore the generation \nof more precise postconditions in the future. One step in this direction is to allow postconditions that \nare parametric in the initial errors, for example with the operator !x introduced in Section 3. While \nour tool currently supports postcondition inlining with such postconditions, we do not yet generate these \nautomati\u00adcally. Arithmetic The arithmetic part of the veri.cation constraints gen\u00aderated by Table 1 can \nbe essentially divided into the ideal part and the actual part, which includes roundoff errors at each \ncomputation step. The ideal part determines whether the ideal range constraints in the postcondition \nare satis.ed and the actual part determines whether the uncertainty part of the postcondition is satis.ed. \nWe can use our procedure presented in Section 6 to compute a sound approximation of both the result s \nrange as well as its uncertainty. Based on this, our tool .rst constructs an approximation which leaves \nthe ideal part unchanged, but replaces the actual part of the constraint by the computed uncertainty \nbound. This effectively removes a large number of variables and is many times a suf.cient simpli.cation \nfor Z3 to succeed in verifying the entire constraint. If Z3 is still not able to prove the constraint, \nour tool constructs the next approximation by also replacing the ideal part, this time with a constraint \nof the result s range which has been computed by our approximation procedure previously. Note that this \nsecond approximation may not have enough information to prove a more complex postcondition, as correlation \ninformation is lost. We note that the computation of ranges and errors is the same for both approximations \nand thus trying both does not affect ef.ciency signi.cantly. In our experiments, Z3 is able to prove \nthe ideal, real\u00advalued part in most cases, so this second approximation is rarely used. Paths In the \ncase of several paths through the program, we have the option to consider each path separately or to \nmerge results at each join in the control .ow graph. This introduces a tradeoff be\u00adtween ef.ciency and \nprecision, since on one hand, considering each path separately leads to an exponential number of paths \nto consider. On the other hand, merging at each join looses correlation informa\u00adtion between variables \nwhich may be necessary to prove certain properties. Our approximation pipeline chooses merging .rst, \nbe\u00adfore resorting to a path-by-path veri.cation in case of failure. We believe that other techniques \nfor exploring the path space could also def comparisonValid(x: Real): Real = {require(-2.0 < x &#38;&#38; \nx < 2.0) val z1 = sineTaylor(x) val z2 = sineOrder3(x) z1 - z2 } ensuring(res . res <= 0.1 &#38;&#38; \nres +/- 5e-14) def comparisonInvalid(x: Real): Real = {require(-2.0 < x &#38;&#38; x < 2.0) val z1 = \nsineTaylor(x) val z2 = sineOrder3(x) z1 - z2 } ensuring(res . res <= 0.01 &#38;&#38; res +/- 5e-14) def \nsineTaylor(x: Real): Real = {require(-2.0 < x &#38;&#38; x < 2.0) x - (x*x*x)/6.0 + (x*x*x*x*x)/120.0 \n- (x*x*x*x*x*x*x)/5040.0 } ensuring(res => -1.0 < res &#38;&#38; res < 1.0 &#38;&#38; res +/- 1e-14) \ndef sineOrder3(x: Real): Real = {require(-2.0 < x &#38;&#38; x < 2.0) 0.954929658551372 * x - 0.12900613773279798*(x*x*x) \n} ensuring(res . -1.0 < res &#38;&#38; res < 1.0 &#38;&#38; res +/- 1e-14) Figure 4. Different polynomial \napproximations of sine. be integrated into our tool [12, 36]. Another possible improvement are heuristics \nthat select a different order of approximations de\u00adpending on particular characteristics of the veri.cation \ncondition. Example We illustrate the veri.cation algorithm on the example in Figure 4, using double .oating-point \nprecision as the target. The functions sineTaylor and sineOrder3 are veri.ed .rst since they do not contain \nfunction calls. Veri.cation with the full veri.cation con\u00adstraint fails. Next, our tool computes the \nerrors on the output and Z3 succeeds to prove the resulting constraint with the ideal part un\u00adtouched. \nFrom this approximation our tool directly computes a new, more precise postcondition, in particular it \ncan narrow the resulting errors to 1.63e-15 and 1.11e-15 respectively. Next, our tool con\u00adsiders the \ncomparisonValid function. Inlining only the postcondition is not enough in this case, but computing the \nerror approximation on the inlined functions succeeds in verifying the postcondition. Note that our tool \ndoes not approximate the real-valued portion of the constraint, i.e. Z3 is used directly to verify the \nconstraint z1 - z2 = 0.1. This illustrates our separation of the real reasoning from the .nite-precision \nimplementation: with our separation we can use a real arithmetic solver to deal with algorithmic reasoning \nand verify with our error computation that the results are still valid (within the error bounds) in the \nimplementation. Finally, the tool veri.es that the preconditions of the function calls are satis.ed by \nusing Z3 alone. Veri.cation of the function comparisonInvalid fails with all approximations. Our tool \nis able to determine that the ideal real-valued constraint alone (z1-z2 = 0.01) is not valid, reports \na counterexample (x = 1.875) and returns invalid as the veri.cation result.  5.5 Soundness Our procedure \nis sound because our constraints over-approximate the actual errors. Furthermore, even in the full constraint \nas gen\u00aderated from Table 1, roundoff errors are over-approximated since we assume the worst-case error \nbound at each step. While this en\u00adsures soundness, it also introduces incompleteness, as we may fail \nto validate a speci.cation because our over-approximation is too large. This implies that counterexamples \nreported by Z3 are in gen\u00aderal only valid, if they disprove the ideal real-valued part of the veri.cation \nconstraint. Our tool checks whether this is the case by constructing a constraint with only the real-valued \npart, and reports the counterexamples, if such are returned from Z3.  5.6 Loops and Recursion In principle, \nour techniques can be applied to programs with loops via recursion, however, because of accumulation \nof roundoff er\u00adrors only self-stabilizing systems can be expected to have simple inductive invariants, \nand such systems can to some extent also be addressed using runtime veri.cation techniques [16]. For \nnon-self\u00adstabilizing systems the uncertainties depend on the number of iter\u00adations, which makes speci.cations \nof such functions very complex. Note that, for ensuring the stability of certain embedded control systems, \nit has been shown that it is suf.cient to consider the body of the control loop only [3]. 6. Solving \nNonlinear Constraints Having given an overview of the approximation pipeline, we now describe the computation \nof the approximation for nonlinear arith\u00admetic, which corresponds to the last box in Figure 3. For complete\u00adness \nof presentation, we .rst review interval and af.ne arithmetic which are common choices for performing \nsound arithmetic com\u00adputations and which we also use as part of our technique. We then present our novel \nprocedure for computing the output range of a nonlinear expression given ranges for its inputs that can \nbe a more precise substitute for interval or af.ne arithmetic. Finally, we con\u00adtinue with a procedure \nthat computes a sound over-approximation of the uncertainty on the result of a nonlinear expression. \nOne possibility to perform guaranteed computations is to use standard interval arithmetic [43]. Interval \narithmetic computes a bounding interval for each basic operation as x . y = [min(x . y), max(x . y)] \n. . {+, -, *, /} and analogously for square root. Af.ne arithmetic was originally introduced in [19] \nand ad\u00addresses the dif.culty of interval arithmetic in handling correlations between variables. Af.ne \narithmetic represents possible values of variables as af.ne forms n x = x0 + xiEi i=1 where x0 denotes \nthe central value (of the represented interval) and each noise symbol Ei is a formal variable denoting \na deviation from the central value, intended to range over [-1, 1]. The maximum magnitude of each noise \nterm is given by the corresponding xi. Note that the sign of xi does not matter in isolation, it does, \nhowever, re.ect the relative dependence between values. E.g., take x = x0 + x1E1, then x - x = x0 + x1E1 \n- (x0 + x1E1) = x0 - x0 + x1E1 - x1E1 = 0 ' If we subtracted x= x0 - x1E1 instead, the resulting interval \nwould have width 2 * x1 and not zero. The range represented by an af.ne form is computed as n [ x] = \n[x0 - rad( x), x0 + rad( x)], rad( x) = |xi| i=1 A general af.ne operation ax + \u00dfy + . consists of addition, \nsub\u00adtraction, addition of a constant (.) or multiplication by a constant (a, \u00df). Expanding the af.ne \nforms x and y we get n ax + \u00dfy + . = (ax0 + \u00dfy0 + .) + (axi + \u00dfyi)Ei i=1 def getRange(expr, precondition, \nprecision, maxIterations): z3.assertConstraint(precondition) [aInit, bInit] = evalInterval(expr, precondition.ranges); \n//lower bound if z3.checkSat(expr < a + precision) == UNSAT a = aInit b = bInit numIterations = 0 while \n(b-a) < precision . numIterations < maxIterations mid = a + (b - a) / 2 numIterations++ z3.checkSat(expr \n< mid) match case SAT . b = mid case UNSAT . a = mid case Unknown . break aNew = a else aNew = aInit \n//upper bound symmetrically bNew = ... return: [aNew, bNew] Figure 5. Algorithm for computing the range \nof an expression. An additional motivation for using af.ne arithmetic is that differ\u00adent contributions \nto the range it represents remain, at least partly, separated. This information can be used for instance \nto help iden\u00adtify the major contributor of a result s uncertainty or to separate contributions from external \nuncertainties from roundoff errors. 6.1 Range Computation The goal of this procedure is to perform a \nforward-computation to determine the real-valued range of a nonlinear arithmetic expres\u00adsion given ranges \nfor its inputs. Two common possibilities are in\u00adterval and af.ne arithmetic, but they tend to over-approximate \nthe resulting range, especially if the input intervals are not suf.ciently small (order 1). Af.ne arithmetic \nimproves over interval arithmetic somewhat by tracking linear correlations, but in the case of nonlin\u00adear \nexpressions the results can become actually worse than for in\u00adterval arithmetic (e.g. x * y, where x \n= [-5, 3], y = [-3, 1] gives [-13, 15] in af.ne arithmetic and [-9, 15] in interval arithmetic). Observation: \nA nonlinear theorem prover such as the one that comes with Z3 can decide with fairly good precision whether \na given bound is sound or not. That is we can check with a prover whether for an expression e the range \n[a, b] is a sound interval en\u00adclosure. This observation is the basis of our range computation. The input \nto our algorithm is a nonlinear expression expr and a precondition P on its inputs, which speci.es, among \npossibly other constraints, ranges on all input variables yx. The output is an interval [a, b] which \nsatis.es the following: [a, b] = getRange(P, expr) . .yx, res .P (yx) . res = expr(yx) . (a = res . res \n= b) The algorithm for computing the lower bound of a range is given in Figure 5. The computation for \nthe upper bound is symmetric. For each range to be computed, our tool .rst computes an initial sound \nestimate of the range with interval arithmetic. It then performs an initial quick check to test whether \nthe computed .rst approximation bounds are already tight. If not, it uses the .rst approximation as the \nstarting point and then narrows down the lower and upper bounds using a binary search. At each step of \nthe binary search our tool uses Z3 to con.rm or reject the newly proposed bound.  The search stops when \neither Z3 fails, i.e. returns unknown for a query or cannot answer within a given timeout, the difference \nbetween subsequent bounds is smaller than a precision threshold, or the maximum number of iterations \nis reached. This stopping criterion can be set dynamically. Additional constraints In addition to the \ninput ranges, the pre\u00adcondition may also contain further constraints on the variables. For example consider \nagain the method triangle in Figure 1. The pre\u00adcondition bounds the inputs as a, b, c . [1, 9], but the \nformula is useful only for valid triangles, i.e. when every two sides together are longer than the third. \nIf not, we will get an error at the lat\u00adest when we try to take the square root of a negative number. \nIn interval-based approaches we can only consider input intervals that satisfy this constraint for all \nvalues, and thus have to check several (and possibly many) cases. In our approach, since we are using \nZ3 to check the soundness of bounds, we can assert the additional con\u00adstraints up-front and then all \nsubsequent checks are performed with respect to all additional and initial constraints. This allows us \nto avoid interval subdivisions due to imprecisions or problem speci.c constraints such as those in the \ntriangle example. This becomes es\u00adpecially valuable in the presence of multiple variables, where we may \notherwise need an exponential number of subdivisions.  6.2 Error Approximation We now describe our approximation \nprocedure which, for a given expression expr and a precondition P on the inputs, computes the range and \nerror on the output. More formally, our procedure satis.es the following: ([a, b], err) = evalWithError(P, \nexpr) . .yx, yx., res , res ..P (yx, yx.) . res = expr(yx) . res . = expr.(yx.) . (a = res . res = b) \n. |res - res .| < err where expr. represents the expression evaluated in .nite-precision arithmetic and \nyx, y x. are the ideal and actual variables. The precon\u00addition speci.es the ranges and uncertainties \nof initial variables and other additional constraints on the ideal variables. The uncertainty speci.cation \nis necessary, as it relates the ideal and actual variables. The idea of our procedure is to execute a \ncomputation while keeping track of the output range of the current expression and its associated errors. \nAt each arithmetic operation, we propagate existing errors, compute an upper bound on the roundoff error \nand add it to the overall errors. Since the roundoff error depends proportionally on the range of values, \nwe need to keep track of the ranges as precisely as possible. Our procedure is build on the abstraction \nthat a computation is an ideal computation plus or minus some uncertainty. The abstrac\u00adtion of .oating-point \nroundoff errors that we choose also follows this separation: f l(x 0 y) = (x 0 y)(1 + d) = (x 0 y) + \n(x 0 y)d for d . [-Em, Em] and 0 . {+, -, *, /}. This allows us to treat all uncertainties in a uni.ed \nmanner. For .xed-point arithmetic the situation is similar, but we .rst determine the .xed-point format \nfrom the current range, and only from this compute the roundoff error. Our procedure builds on the idea \nof the SmartFloat data type [15], which uses af.ne arithmetic to track both the range and the errors. \nFor nonlinear operations, however, the so computed ranges become quickly very pessimistic and the error \ncomputation may also suffer from this imprecision. We observed that since the errors tend to be relatively \nsmall, this imprecision does not affect the error propaga\u00adtion itself to such an extent. If the initial \nerrors are small (less than one), multiplied nonlinear terms tend to be even smaller, whereas if the \naf.ne terms are larger than one, the nonlinear terms grow. We thus concentrate on improving the ideal \nrange of values and use our novel range computation procedure for this part and leave the error propagation \nwith af.ne arithmetic as in [15]. In our adaptation, we represent every variable and intermediate computation \nresult as a datatype with the following components: x : (range : Interval, err : A.neForm ) where range \nis the range of this variable, computed as described in Section 6.1 and err is the af.ne form representing \nthe errors. The (over-approximation) of the actual range including all uncertainties is then given by \ntotalRange = range + [ err], where err denotes the interval represented by the af.ne form. Roundoff error \ncomputation Roundoff errors for .oating-point arithmetic are computed at each computation step as . = \nd * maxAbs (totalRange ) where d is the machine epsilon, and added to err as a fresh noise term. Note \nthat this roundoff error computation makes our error computation parametric in the .oating-point precision. \nFor .xed\u00adpoint arithmetic, roundoff errors are computed as . = getFormat (totalRange , bitWidth).quantizationError \nwhere the getF ormat function returns the best .xed-point for\u00admat [3] that can accommodate the range. \nThis computation is also parametric in the bit-width. Error propagation For af.ne operations addition, \nsubtraction, and multiplication by a constant factor the propagated errors are computed term-wise and \nthus as for standard af.ne arithmetic. We refer the reader to [15, 19] for further details and describe \nhere only the propagation for nonlinear arithmetic. For multiplication, division and square root, the \nmagnitude of errors also depends on the ranges of variables. Since our ranges are not af.ne terms themselves, \npropagation has to be adjusted. In the following, we denote the range of a variable x by [x] and its \nassociated error by the af.ne form . When we write [x] * erry we mean that the errx interval [x] is converted \ninto an af.ne form and the multiplication is performed in af.ne arithmetic. Multiplication is computed \nas x * y = ([x]+ errx)([y] + erry) = [x] * [y] + [x] * erry + [y] * errx + errx * erry + . where . is \nthe new roundoff error. Thus the .rst term contributes to the ideal range and the remaining three to \nthe error af.ne form. The larger the factors [x] and [y] are, the larger the .nally computed errors will \nbe. In order to keep the over-approximation as small as possible, we evaluate [x] and [y] with our new \nrange computation. Division is computed as x 1 = x * = ([x] + err x)([1/y] + err 1/y) y y 1 1 = [x] * \n[ ] + [x] * err 1 + [ ] * err x + err x * err 1 + . y y y y For square root, we .rst compute an af.ne \napproximation of square v root as in [15]: x = a * x + . + ., and then perform the af.ne multiplication \nterm wise. Over.ows and NaN Our procedure allows us to detect potential over.ows, division by zero and \nsquare root of a negative value, as our tool computes ranges of all intermediate values. We currently \nreport these issues as warnings to the user.  6.3 Limitations The limitation of this approach is clearly \nthe ability of Z3 to check our constraints. We found its capabilities satisfactory, although we expect \nthe performance to still signi.cantly improve. To emphasize the difference to the constraints that are \nde.ned by Table 1, the constraints we use here do not add errors at each step and thus the number of \nvariables is reduced signi.cantly. We also found several transformations helpful, such as rewriting powers \n(e.g. x * x * x to x 3), multiplying out products and avoiding non-strict comparisons in the precondition, \nalthough the bene.ts were not entirely consistent. Note that at each step of our error computation, our \ntool computes the current range. Thus, even if Z3 fails to tighten the bound for some expressions, we \nstill compute more precise bounds than interval arithmetic overall in most cases, as the ranges of the \nremaining subexpressions have already been computed more precisely. 7. Conditional Statements In this \nSection we consider the difference between the ideal and actual computation due to uncertainties on computing \nbranch con\u00additions and the resulting different paths taken. We note that the full constraint constructed \naccording to Section 3 automatically in\u00adcludes this error. Recall that the ideal and actual computations \nare independent except for the initial conditions, so that it is possible that they follow different \npaths through the program. In the case of using approximation, however, we compute the error on individual \npaths and have to consider the error due to diverging paths separately. We propose the following algorithm \nto explicitly compute the difference between the ideal and the actual computation across paths. Note \nthat we do not assume continuity, i.e. the algorithm allows us to compute error bounds even in the case \nof non-continuous functions. For simplicity, we present here the algorithm for the case of one conditional \nstatement: if (c(x)<0) f1(x) else f2(x). It generalizes readily to more complex expressions. W.l.o.g. \nwe assume that the condition is of the form c(x) < 0. Indeed, any conditional of the form c(x) == 0 would \nyield different results for the ideal and actual computation for nearly any input, so we do not allow \nit in our speci.cation language. The actual computation commits a certain error when comput\u00ading the condition \nof the branch and it is this error that causes some executions to follow a different branch than the \ncorresponding ideal one would take. Consider the case where the ideal computation evaluates f1, but the \nactual one evaluates f2. The algorithm in Fig\u00adure 6 gives the computation of the path error in this case. \nThe idea is to compute the ranges of f1 and f2, but only for the inputs that could be diverging. The \n.nal error is then the maximum difference of these value. The algorithm extends naturally to several \nvariables. In the case of several paths through the program, this error has to be, in principle, computed \nfor each pair of paths. We use Z3 to rule out infeasible paths up front so that the path error computation \nis only performed for those paths that are actually feasible. Our tool implements a re.ned version of \nthis approach, which merges paths to avoid having to consider an exponential number of path combinations. \nIt also uses a higher default precision and number of iterations threshold during the binary search in \nthe range computation as this computation requires in general very tight intervals for each path. We \nidentify two challenges for performing this computation: 1. As soon as the program has multiple variables, \nthe inputs for the different branches are not two-dimensional intervals anymore, which makes an accurate \nevaluation of the individual paths dif.cult. def getPathError: 2 Input: pre: (x . [a, b] . x \u00b1 n) program: \n(if (cond(x) < 0) f1(x) else f2(x)) 4 val pathError1 = computePathError(pre, cond, f1, f2) 6 val pathError2 \n= computePathError(pre, \u00ac cond, f2, f1) return max (pathError1, pathError2) 8 def computePathError(pre, \nc, f1, f2): 10 ([c], errc) = evalWithError(pre, c) 12 ([f2].oat , err.oat ) = evalWithError(pre . c(x) \n. [0, errc], f2) 14 [f1]real = getRange(pre . c(x) . [-errc, 0], f1) 16 return: max |[f1]real - ([f2].oat \n+ err.oat )| Figure 6. Computing error due to diverging paths. Quantities de\u00adnotes by [x] are intervals. \n2. The inputs for the two branches are inter-dependent. Thus, simply evaluating the two branches with \ninputs that are in the correct ranges, but are not correlated, yields pessimistic results when computing \nthe .nal difference (line 16). We overcome the .rst challenge with our range computation which takes \ninto account additional constraints. For the second challenge, we use our range computation as well. \nUnfortunately, Z3 fails to tighten the .nal range to a satisfactory precision due to timeouts. We still \nobtain much better error estimates than with interval arithmetic alone, as the ranges of values for the \nindividual paths are already computed much more precisely. We report in Section 8 on the type of programs \nwhose veri.cation is within our reach today. 8. Experiments The examples in Figure 1 and 4 and Section \n5.3 provide an idea of the type of programs our tool is currently able to verify fully automatically. \nThe B-spline example from Section 5.3 is the largest meaningful example we were able to .nd that Z3 alone \ncould verify in the presence of uncertainties. For all other cases, it was necessary to use our approximation \nmethods. We have chosen several nonlinear expressions commonly used in physics, biology and chemistry \n[44, 48, 54] as benchmark func\u00adtions, as well as benchmarks used in control systems [2] and suit\u00adable \nbenchmarks from [22]. Experiments were performed on a desktop computer running Ubuntu 12.04.1 with a \n3.5GHz i7 pro\u00adcessor and 16GB of RAM. 8.1 Compiling Programs Which data type is suitable for a given \nprogram depends on the parameter ranges, the code itself but also the precision required by the application \nusing the code. For example, take the functions in Figure 7. Depending on which precision on the output \nthe user needs, our tool will select different data types. For the requirement res +/- 1e-12, as speci.ed, \nDouble is be a suitable choice for doppler and turbine, however for the jetEngine example this is not \nsuf.cient, and thus DoubleDouble would be selected by our tool. The user can in.uence which data types \nare preferred by supplying a list to our tool which is ordered by her preference. Figure 8 illustrates \nthe tradeoff between the precision achieved by different data types against the runtime of the compiled \ncode generated by our tool. We used the Caliper [1] framework for benchmarking the running times.  Benchmark \nOur range interval arithmetic Simulated range doppler1 [-137.639, -0.033951] [-158.720, -0.029442] [-136.346, \n-0.035273] doppler2 [-230.991, -0.022729] [-276.077, -0.019017] [-227.841,-0.023235] doppler3 [-83.066, \n-0.50744] [-96.295, -0.43773] [-82.624, -0.51570] rigidBody1 [-705.0, 705.0] [-705.0, 705.0] [-697.132, \n694.508] rigidBody2 [-56010.1, 58740.0] [-58740.0, 58740.0] [-54997.635, 57938.052] jetEngine [-1997.037, \n5109.338] [-8, 8] [-1779.551, 4813.564] turbine1 [-18.526, -1.9916] [-58.330, -1.5505] [-18.284, -1.9946] \nturbine2 [-28.555, 3.8223] [-29.437, 80.993] [-28.528, 3.8107] turbine3 [0.57172, 11.428] [0.46610, 40.376] \n[0.61170, 11.380] verhulst [0.31489, 1.1009] [0.31489, 1.1009] [0.36685,0.94492] predatorPrey [0.039677, \n0.33550] [0.037277, 0.35711] [0.039669,0.33558] carbonGas [4.3032 e6, 1.6740 e7] [2.0974 e6, 3.4344 e7] \n[4.1508 e6, 1.69074 e7] Sine [-0.9999, 0.9999] [-2.3012, 2.3012] [-1.0093, 1.0093] Sqrt [1.0, 1.3985] \n[0.83593, 1.5625] [1.0, 1.3985] Sine (order 3 approx.) [-1.0001, 1.0001] [-2.9420, 2.9420] [-1.0, 1.0] \nTable 2. Comparison of ranges computed with out procedure against interval arithmetic and simulation. \nSimulations were performed with 107 random inputs. Ranges are rounded outwards. Af.ne arithmetic does \nnot provide better results than interval arithmetic. def doppler(u: Real, v: Real, T: Real): Real = { \n2 require(-100 < u &#38;&#38; u < 100 &#38;&#38; 20 < v &#38;&#38; v < 20000 &#38;&#38; -30 < T &#38;&#38; \nT < 50) 4 val t1 = 331.4 + 0.6 * T 6 (- (t1) *v) / ((t1 + u)*(t1 + u)) } ensuring(res . res +/- 1e-12) \n8 def jetEngine(x1: Real, x2: Real): Real = {10 require(-5 < x1 &#38;&#38; x1 < 5 &#38;&#38; -20 < x2 \n&#38;&#38; x2 < 5) 12 val t = (3*x1*x1 + 2*x2 - x1) x1 + ((2*x1*(t/(x1*x1 + 1))* 14 (t/(x1*x1 + 1) - \n3) + x1*x1*(4*(t/(x1*x1 + 1))-6))* (x1*x1 + 1) + 3*x1*x1*(t/(x1*x1 + 1)) + x1*x1*x1 + x1 + 16 3*((3*x1*x1 \n+ 2*x2 -x1)/(x1*x1 + 1))) } ensuring(res . res +/- 1e-12) 18 def turbine(v: Real, w: Real, r: Real): \nReal = {20 require(-4.5 < v &#38;&#38; v < -0.3 &#38;&#38; 0.4 < w &#38;&#38; w < 0.9 &#38;&#38; 3.8 \n< r &#38;&#38; r < 7.8) 22 3 + 2/(r*r) - 0.125*(3-2*v)*(w*w*r*r)/(1-v) - 4.5 24 } ensuring(res . res \n+/- 1e-12) Figure 7. Benchmark functions from physics and control systems. Figure 8. Runtimes of benchmarks \ncompiled for different preci\u00adsions in nanoseconds (left) vs. abs. error bounds computed for that For \nour benchmarks with their limited input ranges, 32 bit .xed\u00adprecision by our tool (right). point implementations \nprovide better precision than single .oating\u00adpoint precision because single precision has to accommodate \na larger dynamic range which reduces the number of bits available for  8.2 Evaluating Effectiveness \non Nonlinear Expressions the mantissa. That said, .xed-point implementations run slower, at least on \nthe JVM, than the more precise double .oating-point arith-Range computation Stepwise estimation of errors \ncrucially de\u00admetic with its dedicated hardware support. However, the choice for pends on the estimate \nof the ranges of variables. The strength of .xed-point rather than .oating-point may be also due to this \nhard-using a constraint solver such as Z3 is that it can perform such esti\u00adware being unavailable. Our \ntool can thus support a wide variety of mation while taking into account the precise dependencies between \napplications with different requirements. We also note that across variables in preconditions and path \nconditions. Table 2 compares the three (not specially selected) benchmarks, the results are very results \nof our range computation procedure described in Section 6 consistent and we expect similar behavior for \nother applications as against ranges obtained with standard interval arithmetic. Interval well. arithmetic \nis one of the methods used for step-wise range estima\u00adtion; an alternative being af.ne arithmetic, which \nwe found to give more pessimistic results in our experiments. We believe that this is due to imprecision \nin computing nonlinear operations. Note, how\u00adever, that we still use af.ne arithmetic to estimate errors \ngiven the computed ranges. For our range computation, we set the default precision thresh\u00adold to 1e-10 \nand maximum number of iterations for the binary search to 50. To obtain an idea about the true ranges \nof our func\u00adtions, we have also computed a lower bound on the range using simulations with 107 random \ninputs and with exact rational arith\u00admetic evaluation of expressions. We observe that our range com\u00adputation \ncan signi.cantly improve over standard interval bounds. The jetEngine benchmark is a notable example, \nwhere interval arith\u00admetic yields the bound [-8, 8], but our procedure can still pro\u00advide bounds that \nare quite close to the true range. Error computation Table 3 compares uncertainties computed by our tool \nagainst maximum uncertainties obtained through exten\u00adsive simulation with 107 random inputs for different \nprecisions. To obtain (under-approximations of) error bounds, we ran the simula\u00adtion in parallel with \nrational and their corresponding .oating-point or .xed-point values and obtained the error by taking \nthe differ\u00adence in the result. Selected benchmarks, marked with (*), also have added uncertainties on \nthe input parameters. To our knowledge this is the .rst quantitative comparison of an error computation \npreci\u00adsion with (an approximation) of the true errors on such benchmarks. Our computed uncertainties \nare mostly within about an order and many times even closer to the under-approximation of the true er\u00adrors \nprovided by simulation. In the case of the jetEngine* bench\u00admark, we believe that the imprecision is \nmainly due to its complex\u00adity and subsequent failures of Z3 in the range computation. The values in parentheses \nin the second column indicate errors com\u00adputed if ranges at each arithmetic operation are computed using \ninterval arithmetic alone. While we have not attempted to improve the af.ne arithmetic-based error computation \nfrom [15], we can see that in some cases a more precise range computation can gain us improvements. The \nfull effect of the imprecision of standard range computation appears when, due to this imprecision, we \nobtain pos\u00adsible errors such as division-by-zero or square root of a negative number errors. The .rst \ncase happens in the case of the non-linear jetEngine benchmark, so with interval arithmetic alone we \nwould therefore not obtain any meaningful result. Similarly, for the tri\u00adangle example from Section 2, \nwithout being able to constrain the inputs to form valid triangles, we cannot compute any error bound, \nbecause the radicand becomes possibly negative. Table 4 presents another relevant experiment, evaluating \nthe ability to use additional constraints during our range computation. For this experiment, we use double \nprecision and the triangle exam\u00adple from Section 2 with additional constraints allowing increasingly \n.at triangles by setting the threshold on line 12 (a + b > c + 1e-6) to the different values given in \nthe .rst column. As the triangles be\u00adcome .atter, we observe an expected increase in uncertainty on the \noutput since the formula becomes more prone to roundoff errors. At threshold 1e-10 our range computation \nfails to provide the nec\u00adessary precision and the radicand becomes possibly negative. Us\u00ading our tool, \nthe developer can therefore go beyond rules of thumb and informal estimates and be con.dent that the \ncomputed area is accurate up to seven decimal digits even for triangles that whose difference a + b - \nc is as small as 10-9 . Compilation Running Time Running times for compilation are below seven seconds \nfor all benchmarks from Table 3 and for the sine example from Figure 4, except for jetEngine which runs \nin about two minutes due to timeouts from Z3 for some intermedi\u00adate ranges. Examples that require computation \nof the path error are much more computationally challenging: verifying the postcondi- Benchmark Our error \n(IA only) Simulated error doppler1* (dbl) 2.36e-6 5.97e-7 doppler1 (dbl) 4.92e-13 (4.95e-13) 7.11e-14 \ndoppler2 (dbl) 1.29e-12 1.14e-13 doppler2 (32bit) 8.67e-6 (8.79e-6) 1.57e-6 doppler3 (dbl) 2.03e-13 (2.05e-13) \n4.27e-14 rigidBody1* (dbl) 9.21e-7 8.24e-7 rigidBody1 (dbl) 5.08e-13 2.28e-13 rigidBody2 (dbl) 6.48e-11 \n2.19e-11 jetEngine* (dbl) 0.15 (-) 3.58e-5 jetEngine (dbl) 1.62e-8 (-) 5.46e-12 jetEngine (32bit) 0.0972 \n(-) 1.77e-4 turbine1* (dbl) 4.86e-6 3.71e-7 turbine1 (dbl) 1.25e-13 (1.38e-13) 1.07e-14 turbine1 (16bit) \n0.0487 (0.0522) 0.00623 turbine2 (dbl) 1.76e-13 (1.96e-13) 1.43e-14 turbine3 (dbl) 8.50e-14 (9.47e-14) \n5.33e-15 verhulst* (dbl) 2.82e-4 2.40e-4 verhulst (dbl) 6.82e-16 2.23e-16 predatorPrey* (dbl) 9.22e-5 \n8.61e-5 predatorPrey (dbl) 2.94e-16 (2.96e-16) 1.12e-16 carbonGas* (dbl) 2114297.84 168874.70 carbonGas \n(dbl) 4.64e-8 (5.04e-8) 3.73e-9 Sine (dbl) 9.57e-16 (1.46e-15) 4.45e-16 Sine (.) 1.03e-6 (1.57e-6) 1.79e-7 \nSine (16bit) 2.87e-4 (6.79e-4) 1.55e-4 Sqrt (dbl) 8.41e-16 (8.87e-16) 4.45e-16 Sqrt (.) 9.03e-7 (9.52e-7) \n2.45e-7 Sqrt (16bit) 5.97e-4 1.58e-4 Sine, order 3 (.) 1.19e-6 (1.55e-6) 2.12e-7 Sine, order 3 (dbl) \n1.11e-15 (1.44e-15) 3.34e-16 Table 3. Comparison of errors computed with our procedure against sim\u00adulated \nerrors. Simulations were performed with 107 random inputs. (*) in\u00addicates that inputs have external uncertainties \nassociated. dbl and . denotes double and single .oating-point precision respectively, 32 or 16 bit is \nthe bit-width of the .xed-point implementation. Benchmark Area Max. abs. error triangle1 (0.1) [0.29432, \n35.0741] 2.72e-11 triangle2 (1e-2) [0.099375, 35.0741] 8.04e-11 triangle3 (1e-3) [3.16031e-2, 35.0741] \n2.53e-10 triangle4 (1e-4) [9.9993e-3, 35.0741] 7.99e-10 triangle5 (1e-5) [3.1622e-3, 35.0741] 2.53e-9 \ntriangle6 (1e-6) [9.9988e-4, 35.0741] 7.99e-9 triangle7 (1e-7) [3.1567e-4, 35.0741] 2.54e-8 triangle8 \n(1e-8) [9.8888e-5, 35.0741] 8.08e-8 triangle9 (1e-9) [3.0517e-5, 35.0741] 2.62e-7 triangle10 (1e-10) \n- - Table 4. Area computed and error on the result for increasingly .at triangles. All values are rounded \noutwards. Interval arithmetic alone fails to provide any result.  def cav10(x: Real): Real = { 2 require(0 \n< x &#38;&#38; x < 10) if (x*x - x >= 0) 4 x/10 else 6 x*x + 2 } ensuring(res . 0 <= res &#38;&#38; \nres <= 3.0 &#38;&#38; res +/- 3.0) 8 def squareRoot3(x: Real): Real = {10 require(0 < x &#38;&#38; x \n< 10 &#38;&#38; x +/- 1e-10 ) if (x < 1e-5) 12 1 + 0.5 * x else 14 sqrt(1 + x) } ensuring(res . res \n+/- 1e-10) 16 def smartRoot(a: Real, b: Real, c: Real): Real = {18 require(3 <= a &#38;&#38; a <= 3 &#38;&#38; \n3.5 <= b &#38;&#38; b <= 3.5 &#38;&#38; -2 < c &#38;&#38; c < 2 &#38;&#38; b*b - a * c * 4.0 > 0.1) 20 \nval discr = b*b - a * c * 4.0 22 if(b*b - a*c > 10.0) {if(b > 0.0) c * 2.0 /(-b - sqrt(discr)) 24 else \nif(b < 0.0) (-b + sqrt(discr))/(a * 2.0) else (-b + sqrt(discr))/(a * 2.0) 26 } else {(-b + sqrt(discr))/(a \n* 2.0) 28 }} ensuring(res . res +/- 6e-15) Figure 9. Path error computation examples. tion on the main \nfunction from the initial example takes about one minute. Running times depend highly on whether Z3 fails \nto tighten intermediate ranges and the timeout used for Z3. Our default setting is one second; we did \nnot .nd much improvement in the success rate above this threshold.  8.3 Evaluating Errors across Program \nPaths Figure 9 presents several examples to evaluate our error compu\u00adtation procedure across different \npaths from Section 7. The .rst method cav10 [26] has been used before as a benchmark function for computing \nthe output range. Our tool can verify the given post\u00adcondition immediately. Note that the error on the \nresult is actually as large as the result itself, since the method is non-continuous, an aspect that \nhas been ignored in previous work, but that our tool detects automatically. The method squareRoot3 is \nalso a non\u00adcontinuous function that computes the square root of 1 + x using an approximation for small \nvalues and the regular library method oth\u00aderwise. Note the additional uncertainty on the input, which \ncould occur for instance if this method is used in an embedded controller. Our tool can verify the given \nspeci.cation. If we change the con\u00addition on line 10 to x < 1e-4 however, veri.cation fails. In this \nfashion, we can use our tool to determine the appropriate branch condition to meet the precision requirement. \nThese two examples verify in under 5 seconds. Finally, the smartRoot method computes one root of a quadratic \nequation using the well-known more precise method from [27]. Our tool compares the values error across \ndiffer\u00adent paths in real and .nite-precision implementation and succeeds in verifying the postcondition \nunder 25s. 9. Related work Current approaches for verifying .nite-precision code include ab\u00adstract interpretation, \ninteractive theorem proving and decision pro\u00adcedures, which we survey in this section. We are not aware \nof work that would automatically integrate reasoning about uncertainties into a programming model. Abstract \ninterpretation (AI) Abstract domains that are sound with respect to .oating-point computations can prove \nbounds on the ranges of variables [8, 14, 24, 32, 42]. The only work in this area that can also quantify \nroundoff errors is the tool Fluctuat [21, 28]. These techniques use interval or af.ne arithmetic and \ntogether with the required join and meet operations may yield too pessimistic results. [47] improves \nthe precision of Fluctuat by re.ning the in\u00adput domains with a constraint solver. Our approach can be \nviewed as approaching the problem from a different end, starting with an exact constraint and then using \napproximation until the solver suc\u00adceeds. Unlike AI tools in general, our system currently does not perform \nwidening to ensure convergence. If the user can provide inductive postconditions, then we can still prove \nthe code correct, but we do not in general discover these postconditions ourselves. Our focus is on proving \nprecise bounds on the ranges in the pres\u00adence of nonlinear computations and the quanti.cation of roundoff \nerrors and other uncertainties. Theorem proving The Gappa tool [18, 39] generates a proof checkable by \nthe interactive theorem prover Coq from source code with speci.cations. It can reason about properties \nthat can be re\u00adduced to reasoning about ranges and errors, but targets very precise properties of specialized \nfunctions, such as software implementa\u00adtions of elementary functions. The speci.cation itself requires \nex\u00adpertize and the proofs human intervention. A similar approach is taken by [4] which generate veri.cation \nconditions that are dis\u00adcharged by various theorem provers. Harisson has also done signif\u00adicant work \non proving .oating-point programs in the HOL Light theorem prover [30]. Our approach makes a different \ncompromise on the precision vs. automation tradeoff, by being less precise, but automatic. The Gappa \nand interactive theorem provers can be used as complements to our tool: if our tool detects that more \nprecision is needed, interactive tools can be employed by an expert user on se\u00adlected methods; the results \ncan then used by our tool in the context of the overall program. Range computation The Gappa tool and \nmost constraint solvers internally use interval arithmetic for sound range computations, whose limitations \nare well-known. [23] describes an arithmetic based on function enclosures and [41] use an arithmetic \nbased on taylor series as an alternative. This approach is useful when checking a constraint, but is \nnot suitable for a forward computation of ranges and errors. Decision procedures An alternative approach \nto veri.cation via range computation are .oating-point decision procedures. Bit\u00adprecise constraints, \nhowever, become very large quickly. [11] addresses this problem by using a combination of over-and under\u00adapproximations. \n[29] present an alternative approach in combining interval constraint solving with a CDCL algorithm and \n[25] is a decision procedure for nonlinear real arithmetic combining inter\u00adval constraint solving with \nan SMT solver for linear arithmetic.[49] formalizes the .oating-points for the SMT-LIB format. While \nthese approach can check ranges on numeric variables, they do not handle roundoff errors or other uncertainties \nand cannot compute speci.\u00adcations automatically. [46] use a .oating-point decision procedure to detect \nstability issues and while their approach provides wit\u00adnesses of instability if such exist, it is not \nable to prove sound error bounds with respect to a real-valued semantics. [3] prove .xed\u00adpoint constraints \nwith a combination of bit vectors and reals, but such an encoding is only possible for .xed-points and \nless ef.cient than reals alone. An alternative to our approach is using linear ap\u00adproximations to solve \npolynomial constraints [10]. We believe that such advances are largely orthogonal to our use of range \narithmetic and complement each other.  Testing Symbolic execution is a well-known technique for gener\u00adating \ntest inputs. [9] use a combination of meta-heuristic search and interval constraint solving to solve \nthe .oating-point constraints that arise, whereas [37] combine random search and evolutionary techniques. \n[52] test numerical code for precision by perturbing low-order bits of values and rewriting expressions. \nThe idea is to exaggerate initial errors and thus make imprecisions more visible. Probabilistic arithmetic \n[50] is a similar approach but it does the perturbation by using different rounding modes. [6] also propose \na testing produce to detect accuracy problems by instrumenting code to perform a higher-precision computation \nside by side with the regular computations. While these approaches are sound with re\u00adspect to .oating-point \narithmetic, they only generate or can check individual inputs and are thus not able to verify or compute \noutput ranges or their roundoff errors. Robustness analysis [31] combines abstract interpretation with \nmodel checking to check programs for stability by tracking the evo\u00adlution of the width of the interval \nrepresenting a single input. [40] use concolic execution to .nd inputs which, given maximum devi\u00adations \non inputs, maximize the deviation on the outputs. These two works however, use a testing approach and \ncannot provide sound guarantees. [12] presents a framework for continuity analysis of programs along \nthe mathematical E - d de.nition of continuity and [13] builds on this work and presents a sound robustness \nanaly\u00adsis. This framework provides a syntactic proof of robustness for programs over reals and thus does \nnot consider .oating-points. Our approach describes a quantitative measure of robustness for nonlin\u00adear \nprograms with .oating-point numbers and other uncertainties, and we believe that it can complement the \ncited framework. 10. Conclusion We have presented a programming model for numerical programs that decouples \nthe mathematical problem description from its real\u00adization in .nite precision. The model uses a Real \ndata type that cor\u00adresponds to mathematical real numbers. The developer speci.es the program using reals \nand indicates the target precision; the compiler chooses a .nite-precision representation while checking \nthat the desired precision targets are met. We have described the soundness criteria by translating programs \nwith precision requirements into veri.cation conditions over mathematical reals. The resulting veri\u00ad.cation \nconditions, while a natural description of the problem being solved, are dif.cult to solve using a state-of-the \nart SMT solver Z3. We therefore developed an algorithm that combines SMT solving with range computation. \nOur notion of soundness incorporates full input/output behavior of functions, taking into account that, \ndue to conditionals, small differences in values can lead to different paths being taken in the program. \nFor such cases our approach estimates a sound upper bound on the total error of the computation. We have \nevaluated our techniques on a number of benchmarks from the literature, including benchmarks from physics, \nbiology, chemistry, and control systems. We have found that invocation of an SMT solver alone is not \nsuf.cient to handle these benchmarks due to scalability issues, whereas the use of range arithmetic by \nitself is not precise enough. By combining these two techniques we were able to show that a .nite-precision \nversion of the code conforms to the real-valued version with reasonable precision requirements. We believe \nthat our results indicate that it is reasonable to in\u00adtroduce Reals as a data type, following a list \nof previously intro\u00adduced mathematical abstractions in programming languages, such as unbounded integers, \nrationals, and algebraic data types. The fea\u00adsibility of veri.ed compilation of our benchmarks suggests \nthat it is realistic to decouple the veri.cation of executable mathematical models over reals from their \nsound compilation. We therefore ex\u00adpect that this methodology will help advance rigorous formal veri.\u00adcation \nof numerical software and enable us to focus more on high\u00adlevel correctness properties as opposed to \nrun-time errors alone. Furthermore, we expect that having real numbers as a data type fa\u00adcilitates automatic \nreordering of .nite-precision computations [17] as well as high-level optimizations such as replacing \none version of a numerical algorithm with another to achieve the desired combi\u00adnation of ef.ciency and \nrigorous worst-case bounds on precision. References [1] Caliper open source framework. http://code.google.com/p/caliper/. \n[2] A. Anta and P. Tabuada. To Sample or not to Sample: Self-Triggered Control for Nonlinear Systems. \nIEEE Transactions on Automatic Control, 55(9), 2010. [3] A. Anta, R. Majumdar, I. Saha, and P. Tabuada. \nAutomatic Veri.cation of Control System Implementations. In EMSOFT, 2010. [4] A. Ayad and C. March e.\u00b4Multi-prover \nveri.cation of .oating-point programs. In IJCAR, 2010. [5] D. H. Bailey, Y. Hida, X. S. Li, and B. Thompson. \nC++/Fortran\u00ad90 double-double and quad-double package. http://crd\u00adlegacy.lbl.gov/ dhbailey/mpdist/, 2013. \n[6] F. Benz, A. Hildebrandt, and S. Hack. A dynamic program analysis to .nd .oating-point accuracy problems. \nIn PLDI, 2012. [7] R. Blanc, E. Kneuss, V. Kuncak, and P. Suter. An Overview of the Leon Veri.cation \nSystem: Veri.cation by Translation to Recursive Functions. In Scala Workshop, 2013. [8] B. Blanchet, \nP. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min \u00b4e, D. Monniaux, and X. Rival. A static analyzer \nfor large safety-critical software. In PLDI, pages 196 207, 2003. [9] M. Borges, M. d Amorim, S. Anand, \nD. Bushnell, and C. S. Pasareanu. Symbolic Execution with Interval Solving and Meta-heuristic Search. \nIn ICST, 2012. [10] C. Borralleras, S. Lucas, A. Oliveras, E. Rodr\u00b4iguez-Carbonell, and A. Rubio. Sat \nmodulo linear arithmetic for solving polynomial constraints. J. Automated Reasoning, 48(1), 2012. [11] \nA. Brillout, D. Kroening, and T. Wahl. Mixed abstractions for .oating\u00adpoint arithmetic. In FMCAD, pages \n69 76, 2009. [12] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity analysis of programs. In POPL, \n2010. [13] S. Chaudhuri, S. Gulwani, R. Lublinerman, and S. Navidpour. Proving Programs Robust. In ESEC/FSE, \n2011. [14] L. Chen, A. Min\u00b4e, J. Wang, and P. Cousot. Interval Polyhedra: An Abstract Domain to Infer \nInterval Linear Relationships. In SAS, 2009. [15] E. Darulova and V. Kuncak. Trustworthy Numerical Computation \nin Scala. In OOPSLA, 2011. [16] E. Darulova and V. Kuncak. Certifying Solutions for Numerical Constraints. \nIn RV, 2012. [17] E. Darulova, V. Kuncak, R. Majumdar, and I. Saha. Synthesis of Fixed-Point Programs. \nIn EMSOFT, 2013. [18] F. de Dinechin, C. Lauter, and G. Melquiond. Certifying the Floating-Point Implementation \nof an Elementary Function Using Gappa. IEEE Trans. Comput., 2011. [19] L. H. de Figueiredo and J. Stol.. \nSelf-Validated Numerical Methods and Applications. IMPA/CNPq, Brazil, 1997. [20] L. De Moura and N. Bj\u00f8rner. \nZ3: an ef.cient SMT solver. In TACAS, 2008. [21] D. Delmas, E. Goubault, S. Putot, J. Souyris, K. Tekkal, \nand F. Vedrine. Towards an industrial use of FLUCTUAT on safety\u00adcritical avionics software. In FMICS, \n2009. [22] V. D Silva, L. Haller, D. Kroening, and M. Tautschnig. Numeric Bounds Analysis with Con.ict-driven \nLearning. In TACAS, 2012. [23] J. A. Duracz and M. Konecny. Polynomial function enclosures and .oating \npoint software veri.cation. In CFV/IJCAR, 2008. [24] J. Feret. Static Analysis of Digital Filters. In \nESOP, 2004.  [25] S. Gao, M. Ganai, F. Ivancic, A. Gupta, S. Sankaranarayanan, and E. Clarke. Integrating \nICP and LRA solvers for deciding nonlinear real arithmetic problems. In FMCAD, 2010. [26] K. Ghorbal, \nE. Goubault, and S. Putot. A Logical Product Approach to Zonotope Intersection. In CAV, 2010. [27] D. \nGoldberg. What every computer scientist should know about .oating-point arithmetic. ACM Comput. Surv., \n23(1), 1991. [28] E. Goubault, S. Putot, and F. V \u00b4edrine. Modular static analysis with zonotopes. In \nSAS, 2012. [29] L. Haller, A. Griggio, M. Brain, and D. Kroening. Deciding .oating\u00adpoint logic with systematic \nabstraction. In FMCAD, 2012. [30] J. Harrison. Floating-Point Veri.cation using Theorem Proving. In Formal \nMethods for Hardware Veri.cation, SFM, 2006. [31] F. Ivancic, M. Ganai, S. Sankaranarayanan, and A. Gupta. \nNumerical stability analysis of .oating-point computations using software model checking. In MEMOCODE, \n2010. [32] B. Jeannet and A. Min e.\u00b4Apron: A Library of Numerical Abstract Domains for Static Analysis. \nIn CAV, 2009. [33] J. Jiang, W. Luk, and D. Rueckert. FPGA-Based Computation of Free-Form Deformations. \nIn FPL, 2003. [34] D. Jovanovi \u00b4c and L. de Moura. Solving Non-linear Arithmetic. In IJCAR 2012, 2012. \n[35] W. Kahan. Miscalculating Area and Angles of a Needle-like Triangle. Technical report, University \nof California Berkeley, 2000. [36] V. Kuznetsov, J. Kinder, S. Bucur, and G. Candea. Ef.cient State Merging \nin Symbolic Execution. In PLDI, 2012. [37] K. Lakhotia, N. Tillmann, M. Harman, and J. de Halleux. FloPSy \n-Search-Based Floating Point Constraint Solving for Symbolic Execution. In Testing Software and Systems. \nSpringer Berlin / Heidelberg, 2010. [38] X. Leroy. Veri.ed squared: does critical software deserve veri.ed \ntools? In POPL, 2011. [39] M. D. Linderman, M. Ho, D. L. Dill, T. H. Meng, and G. P. Nolan. Towards program \noptimization through automated analysis of numerical precision. In CGO, 2010. [40] R. Majumdar, I. Saha, \nand Z. Wang. Systematic Testing for Control Applications. In MEMOCODE, 2010. [41] K. Makino and M. Berz. \nTaylor Models and Other Validated Functional Inclusion Methods. International Journal of Pure and Applied \nMathematics, 4, 2003. [42] A. Min \u00b4 e. Relational Abstract Domains for the Detection of Floating-Point \nRun-Time Errors. In ESOP, 2004. [43] R. Moore. Interval Analysis. Prentice-Hall, 1966. [44] J. D. Murray. \nMathematical Biology. Springer, 2002. [45] M. Odersky, L. Spoon, and B. Venners. Programming in Scala: \nA Comprehensive Step-by-step Guide. Artima Incorporation, 2008. [46] G. Paganelli and W. Ahrendt. Verifying \n(In-)Stability in Floating-point Programs by Increasing Precision, using SMT Solving. In SYNASC, 2013. \n[47] O. Ponsini, C. Michel, and M. Rueher. Re.ning Abstract Interpreta\u00adtion Based Value Analysis with \nConstraint Programming Techniques. In CP, 2012. [48] A. Quarteroni, F. Saleri, and P. Gervasio. Scienti.c \nComputing with MATLAB and Octave. Springer, 3rd edition, 2010. [49] P. R \u00a8ummer and T. Wahl. An SMT-LIB \nTheory of Binary Floating-Point Arithmetic. In Informal proceedings of SMT at FLoC, 2010. [50] N. Scott, \nF. J\u00b4equel, C. Denis, and J.-M. Chesneaux. ez\u00b4Numerical health check for scienti.c codes: the CADNA approach. \nComputer Physics Communications, 2007. [51] I. C. Society. IEEE Standard for Floating-Point Arithmetic. \nIEEE Std 754-2008, 2008. [52] E. Tang, E. Barr, X. Li, and Z. Su. Perturbing numerical calculations for \nstatistical analysis of .oating-point program (in)stability. In ISSTA, 2010. [53] E. M. Westbrook and \nS. Chaudhuri. A Semantics for Approximate Program Transformations. CoRR, 2013. [54] C. Woodford and C. \nPhillips. Numerical Methods with Worked Examples, volume 2nd. Springer, 2012.    \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a finite-precision implementation that is guaranteed to meet the desired precision with respect to real numbers. Our compilation performs a number of verification steps for different candidate precisions. It generates verification conditions that treat all sources of uncertainties in a unified way and encode reasoning about finite-precision roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their non-linear nature, precise reasoning about these verification conditions remains difficult and cannot be handled using state-of-the art SMT solvers alone. We therefore propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Our implementation gives promising results on several numerical models, including dynamical systems, transcendental functions, and controller implementations.</p>", "authors": [{"name": "Eva Darulova", "author_profile_id": "81490689453", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P4383807", "email_address": "eva.darulova@epfl.ch", "orcid_id": ""}, {"name": "Viktor Kuncak", "author_profile_id": "81100277693", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P4383808", "email_address": "viktor.kuncak@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535874", "year": "2014", "article_id": "2535874", "conference": "POPL", "title": "Sound compilation of reals", "url": "http://dl.acm.org/citation.cfm?id=2535874"}