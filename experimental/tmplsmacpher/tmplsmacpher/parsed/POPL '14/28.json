{"article_publication_date": "01-08-2014", "fulltext": "\n Modular, Higher-Order Cardinality Analysis in Theory and Practice Ilya Sergey IMDEA Software Institute \n ilya.sergey@imdea.org Abstract Since the mid 80s, compiler writers for functional languages (es\u00adpecially \nlazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used \nonly once. However it has proved dif.cult to achieve both power and simplicity in prac\u00adtice. We describe \na new, modular analysis for a higher-order lan\u00adguage, which is both simple and effective, and present \nmeasure\u00adments of its use in a full-scale, state of the art optimising compiler. The analysis .nds many \nsingle-entry thunks and one-shot lambdas and enables a number of program optimisations. Categories and \nSubject Descriptors D.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; F.3.2 [Logics \nand Meanings of Programs]: Semantics of Programming Lan\u00adguages Program analysis, Operational semantics \nGeneral Terms Languages, Theory, Analysis Keywords compilers, program optimisation, static analysis, \nfunc\u00adtional programming languages, Haskell, lazy evaluation, thunks, cardinality analysis, types and \neffects, operational semantics 1. Introduction Consider these de.nitions, written in a purely functional \nlanguage like Haskell: wurble1, wurble2 :: (Int -> Int) -> Int wurble1 k = sum (map k [1..10]) wurble2 \nk = 2 * k 0 f1 :: [Int] -> Int f1 xs = let ys = map costly xs in wurble (\\n. sum (map (+ n) ys)) Here \nwe assume that costly is some function that is expensive to compute and wurble is either wurble1 or wurble2. \nIf we replace ys by its de.nition, we could transform f1 into f2: f2 xs = wurble (\\n. sum (map (+ n) \n(map costly xs))) A compiler like GHC can now use short-cut deforestation to fuse the two maps into one, \neliminating the intermediate list altogether, and offering a substantial performance win (Gill et al. \n1993). Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. Copyrights for components of \nthis work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA. \nCopyright c &#38;#169; 2014 ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535861 \nDimitrios Vytiniotis Simon Peyton Jones Microsoft Research {dimitris,simonpj}@microsoft.com Does this \ntransformation make the program run faster or slower? It depends on wurble! For example, wurble1 calls \nits function argument ten times, so if wurble = wurble1, function f2 would compute costly ten times for \neach element of xs; whereas f1 would do so only once. On the other hand if wurble = wurble2, which calls \nits argument exactly once, then f2 is just as ef.cient as f1, and map/map fusion can improve it further. \nThe reverse is also true. If the programmer writes f2 in the .rst place, the full laziness transformation \n(Peyton Jones et al. 1996) will .oat the sub-expression (map costly xs) out of the \\n\u00adexpression, so \nthat it can be shared. That would be good for wurble1 but bad for wurble2. What is needed is an analysis \nthat can provide a sound approxima\u00adtion of how often a function is called we refer to such an analysis \nas a cardinality analysis. An optimising compiler can then use the results of the analysis to guide its \ntransformations. In this paper we provide just such an analysis: We characterise two different, useful \nforms of cardinality, namely (a) how often a function is called, and (b) how often a thunk is forced \nin a lazy language (Section 2). Of these, the former is relevant under both call-by-need and call-by-value, \nwhile the latter is speci.c to call-by-need.  We present a backwards analysis that can soundly and ef.\u00adciently \napproximate both forms of cardinality for a non-strict, higher-order language (Section 3). A signi.cant \ninnovation is our use of call demands to model the usage of a function; this makes the analysis both \npowerful and modular.  We prove that our algorithm is sound; for example if it claims that a function \nis called at most once, then it really is (Sec\u00adtion 4). This proof is not at all straightforward, because \nit must take account of sharing that is the whole point! So we can\u00adnot use standard denotational techniques, \nbut instead must use an operational semantics that models sharing explicitly.  We formalise a number \nof program optimisations enabled by the results of the cardinality analysis, prove them sound and, what \nis more important, improving (Section 5).  We have implemented our algorithm by extending the Glasgow \nHaskell Compiler (GHC), a state of the art optimising compiler for Haskell. Happily, the implementation \nbuilds directly on GHC s current strictness and absence analyser, and is both simple and ef.cient (Section \n6).  We measured how often the analysis .nds called-once func\u00adtions and used-once thunks (Section 7); \nand how much this knowledge improved the performance of real programs (Sec\u00adtions 7.1 7.2). The analysis \nproves quite effective in that many one-shot lambdas and single-entry thunks are detected (in the range \n0-30%, depending on the program). Improvements in per\u00adformance are modest but consistent (a few percent): \nprograms already optimised by GHC are a challenging target!   We discuss related work in Section 8. \nDistinctive features of our work are (a) the notion of call demands, (b) a full implementation measured \nagainst a state of the art optimising compiler, and (c) the combination of simplicity with worthwhile \nperformance improve\u00adments. 2. What is Cardinality Analysis? Cardinality analysis answers three inter-related \nquestions, in the setting of a non-strict, pure functional language like Haskell: How many times is \na particular, syntactic lambda-expression called (Section 2.1)?  Which components of a data structure \nare never evaluated; that is, are absent (Section 2.3)?  How many times is a particular, syntactic thunk \nevaluated (Sec\u00adtion 2.4)?  2.1 Call cardinality We saw in the introduction an example where it is helpful \nto know when a function calls its argument at most once. A lambda that is called at most once is called \na one-shot lambda, and they are extremely common in functional programming: for example a continuation \nis usually one-shot. So cardinality analysis can be a big win when optimising continuation-heavy programs. \nNor is that all. As we saw in the Introduction, inlining under a one\u00adshot lambda (to transform f1 into \nf2) allows short-cut deforesta\u00adtion to fuse two otherwise-separate calls of map. But short-cut de\u00adforestation \nitself introduces many calls of of the function build: build :: (forall b. (a -> b -> b) -> b -> b) -> \n[a] build g = g (:) [] You can see that build calls its argument exactly once, and inlin\u00ading ys in calls \nlike (build (\\cn. ...ys...)) turns out to be crucial to making short-cut deforestation work in practice. \nGill de\u00advotes a section of his thesis to elucidating this point (Gill 1996, Chapter 4.3). Gill lacked \nan analysis for one-shot lambdas, so his implementation (still extant in GHC today) relies on a gross \nhack: he taught GHC s optimiser to behave specially for build itself, and a couple of other functions. \nNo user-de.ned function will have this good behaviour. Our analysis subsumes the hack, by providing an \nanalysis that deduces the correct one-shot information for build, as well as many other functions.  \n2.2 Currying In a higher order language with curried functions, we need to be careful about the details. \nFor example, consider f3 a = wurble a (\\x.let t = costly x in \\y. t+y) wurble1 a g = g 2 a + g 3 a wurble2 \na g = sum (map (g a) [1..1000]) If wurble was wurble1, then in f3 it would be best to inline t at its \nuse site, thus: f4 a = wurble1 a (\\x.\\y. costly x + y) The transformed f4 is much better than f3: it \navoids allocating a thunk for t, and avoids allocating a function closure for the \\y. But if f3 called \nwurble2 instead, such a transformation would be disastrous. Why? Because wurble2 applies its argument \ng to one argument a, and the function thus computed is applied to each of 1000 integers. In f3 we will \ncompute (costly a) once, but f4 will compute it 1000 times, which is arbitrarily bad. So our analysis \nof wurble2 must be able to report wurble2 s argument g is called1 once (applied to one argument), and \nthe 1 We will always use called to mean applied to one argument . result is called many times . We formalise \nthis by giving a usage signature to wurble, like this: wurble1 :: U . C .(C 1(U )) . wurble2 :: U . \nC 1(C .(U )) . The notation C .(C 1(U )) is a usage demand: it describes how a (function) value is used. \nThe demand type U . C .(C 1(U )) . describes how a function uses its arguments, therefore it gives a \nusage demand for each argument. (The  has no signi.cance; we are just used to seeing something after \nthe .nal arrow!) Informally, the C 1(d) means this argument is called once (applied to one argument), \nand the result is used with usage d , whereas C .(d) means this argument may be called many times, with \neach result used with usage d . The U means is used in some unknown way (or even not used at all) . Note \nthat wurble1 s second argument usage is C .(C 1(U )), not C .(C .(U )); that is, in all cases the result \nof applying g to one argument is then called only once. 2.3 Absence Consider this function f x = case \nx of (p,q) -> <cbody> A strictness analyser can see that f is strict in x, and so can use call-by-value. \nMoreover, rather than allocate a pair that is passed to f, which immediately takes it apart, GHC uses \na worker/wrapper transformation to pass the pieces separately, thus: f x = case x of (p,q) -> fw p q \nfw p q = <cbody> Now f (the wrapper ) is small, and can be inlined at f s call sites, often eliminating \nthe allocation of the pair; meanwhile fw (the worker ) does the actual work. Strictness analysis, and \nthe worker/wrapper transform to exploit its results, are hugely impor\u00adtant to generating ef.cient code \nfor lazy programs (Peyton Jones and Partain 1994; Peyton Jones and Santos 1998). In general, f s right-hand \nside often does not have a syntacti\u00adcally visible case expression. For example, what if f simply called \nanother function g that was strict in x? Fortunately the worker/wrapper transform is easy to generalise. \nSuppose the right hand side of f was just <fbody>. Then we would transform to f x = case x of (p,q) -> \nfw p q fw p q = let x = (p,q) in <fbody> Now we hope that the binding for x will cancel with case expres\u00adsions \nin <fbody>, and indeed it usually proves to be so (Peyton Jones and Santos 1998). But what if <fbody> \ndid not use q at all? Then it would be stupid to pass q to fw. We would rather transform to: f x = case \nx of (p,q) -> fw p fw p = let x = (p, error \"urk\") in <fbody> This turns out to be very important in \npractice. Programmers sel\u00addom write functions with wholly-unused arguments, but they fre\u00adquently write \nfunctions that use only part of their argument, and ig\u00adnoring this point leads to large numbers of unused \narguments being passed around in the optimised program after the worker-wrapper transformation. Absence \nanalysis has therefore been part of GHC since its earliest days (Peyton Jones and Partain 1994), but \nit has never been formalised. In the framework of this paper, we give f a usage signature like this: \nf :: U (U , A) . The U (U , A) indicates that the argument is a product type; that is, a data type with \njust one constructor. The A (for absent ) indicates that f discards the second component of the product. \n 2.4 Thunk cardinality Consider these de.nitions f :: Int -> Int -> Int f x c = if x > 0 then c + 1 \nelse if x == 0 then 0 else c -1 g y = f y (costly y) Since f is not strict in c, g must build a thunk \nfor (costly y) to pass to f. In call-by-need evaluation, thunks are memoised. That is, when a thunk is \nevaluated at run-time, it is overwritten with the value so that if it is evaluated a second time the \nalready-computed value can be returned immediately. But in this case we can see that f never evaluates \nits second argument more than once, so the memoisation step is entirely wasted. We call these single-entry \nthunks. Memoisation is not expensive, but it is certainly not free. Opera\u00adtionally, a pointer to the \nthunk must be pushed on the stack when evaluation starts, it must be black-holed to avoid space leaks \n(Jones 1992), and the update involves a memory write. If cardinality anal\u00ad ysis can identify single-entry \nthunks, as well as one-shot lambdas, that would be a Good Thing. And so it can: we give f the usage signature: \nf :: .*U . 1*U . The . * modi.er says that f may evaluate its .rst argument more than once, while the \n1 * says that it evaluates its second argument at most once.  2.5 Call vs evaluation For functions, \nthere is a difference between being evaluated once and called once, because of Haskell s seq function. \nFor example: f1 g = g seq 1 --f1 :: 1*U . f2 g = g seq g 2 --f2 :: .*C 1(U ) . f3 g = g 3 --f3 :: 1*C \n1(U ) . The function seq evaluates its .rst argument (to head-normal form) and returns its second argument. \nIf its .rst argument is a function, the function is evaluated to a lambda, but not called. Notice that \nf2 s usage type says that g is evaluated more than once, but applied only once. For example consider \nthe call f (\\x. x + y) How many times is y evaluated? For f equal to f1 the answer is zero; for f2 and \nf3 it is one. 3. Formalising Cardinality Analysis We now present our analysis in detail. The syntax of \nthe language we analyse is given in Figure 1. It is quite conventional: just lambda calculus with pairs \nand (non-recursive) let-expressions. Constants . include literals and primitive functions over literals, \nas well as Haskell s built-in seq. We use A-normal form (Sabry and Felleisen 1992) so that the issues \nconcerning thunks show up only for let and not also for function arguments. 3.1 Usage demands Our cardinality \nanalysis is a backwards analysis over an abstract domain of usage demands. As with any such analysis, \nthe abstract domain embodies a balance between the cost of the analysis and its precision. Our particular \nchoices are expressed in the syntax of usage demands, given in Figure 1. A usage demand d is one of the \nfollowing: U (d1 , d2 ) applies to pairs. The pair itself is evaluated and its .rst component is used \nas described by d1 and its second by d2 . Expressions and values e ::= x | v | e x | let x = e1 in \ne2 | case e1 of (x1, x2) . e2 v ::= . | .x.e | (x1, x2) Annotated expressions and values m e ::= x | \nv | e x | let x = e1 in e2 | case e1 of (x1, x2) . e2 v ::= . | .m x.e | (x1, x2) Usage demands and multi-demands \nC n d d ::= (d) | U (d1 , d2 ) | U | HU ::= A | n *d n ::= 1 | . m ::= 0 | 1 | . Non-syntactic demand \nequalities C . (U ) = U U (.*U , . *U ) = U U (A, A) = HU Usage types t ::= | d . t Usage type expansion \nd . t -d . t -.*U . Free-variable usage environments (fv-usage) . ::= (x:d ), . | E Auxiliary notation \non environments .(x) = d when (x:d ) . . A otherwise Usage signatures and signature environments . ::= \n(k ; t ; .) k . Z>0 P ::= (x:.), P | E transform ((k ; t ; .), d) = (t ; .) if d . C 1(. . . k-fold . \n. . C 1(U )) = (.*t ; .*.) otherwise Figure 1: Syntax of terms, values, usage types, and environments \nC n (d) applies to functions. The function is called at most n times, and on each call the result is \nused as described by d. Call demands are, to the best of our knowledge, new. U , or used , indicating \nno information; the demand can use the value in an arbitrary way.  HU , or head-used , is a special \ncase; it is the demand that seq places on its .rst argument: seq :: HU . U . .  A usage demand d always \nuses the root of the value exactly once; it cannot express absence or multiple evaluation. That is done \nby d , which is either A (absent), or n *d indicating that the value is used at most n times in a way \ndescribed by d. In both C n(d) and n *d, the multiplicity n is either 1 or . (meaning many ). Notice \nthat a call demand C n(d) has a d inside it, not a d : if a function is called, its body is evaluated \nexactly once. This is different from pairs. For example, if we have let x = (e1, e2) in fst x + fst x \n then e1 is evaluated twice. Both U and HU come with some non-syntactic equalities, denoted by = in Figure \n1 and necessary for the proof of well-typedness (Section 4). For example, U is equivalent to a pair demand \nwhose components are used many times, or a many-call-demand where the result is used in an arbitrary \nway. Similarly, for pairs HU is equivalent to U (A, A), while for functions HU is equivalent to C 0 (A), \nif we had such a thing. In the rest of the paper all de.nitions  P e . d . (t ; .) e \u00b5(d ) = m \u00b5(A) \n= 0 \u00b5(n *d) = n A &#38; d = d d &#38; A = d n1 *d1 &#38; n2 *d2 = .*(d1 &#38; d2) A U d = d d U A = \nd n1 *d1 U n2 *d2 = (n1 U n2)*(d1 U d2) d &#38; U = U U &#38; d = U d &#38; HU = d HU &#38; d = d C n1 \n(d1) &#38; C n2 (d2) = C .(d1 U d2) U (d1 , d) &#38; U (d3 , d) = U (d&#38; d3 , d&#38; d) 24 124 d U \nU = U UU d U= d U HU = d HU U d = d  C n1 (d1) U C n2 (d2) = C n1Un2 (d1 U d2) U (d1 , d) U U (d3 , \nd) = U (dU d3 , dU d) 24 14 2   .1 &#38; .2 = {(x:d&#38; d) | .i (x) = d} 1 2 i .1 U .2 = {(x:dU \nd) | .i (x) = d} 1 2 i t1 U t2 = t3  (d. t1) U (d. t2) = (dU d) . (t1 U t2) 11 12  t U = (t1 ; .1) \nU (t2 ; .2) = (t3 ; .3)  (t1 ; .1) U (t2 ; .2) = (t1 U t2 ; .1 U .2)  1*d = d .*d = d &#38; d n * = \n n *(d . t) = (n *d ) . (n *t) n *. = {x : n *.(x) | x . dom(.)} n1 U n2 = n3 1 U 1 = 1 . U n = . n U \n. = . a b a b . (a U b) = b d&#38; d= ddU d= d 123 123    (x : .) . P (t ; .) = transform (., d) VA \nR DN ' P x . d . (t ; . &#38; (x:1*d)) x * ' * x ./dom(P) VA RUP P x . d . ( ; (x:1*d)) x * P e . de \n. (t ; .) e LA M P .x.e . C n (de) . (.(x) . t ; n *(.\\x )) .n x.e P .x.e . C .(U ) . (t ; .) e * LA \nM U P .x.e . U . (t ; .) e LA M H U P x.e .x.e . HU . (t ; E) .1 * P e1 . C 1(d) . (d.tr ; .1) e1 P y \n. d. .2 2 2 AP PA P e1 y . d . (tr ; .1 &#38; .2) e1 y P e1 . C 1(d) . ( ; .1) e1 P y . .*U . .2 AP \nP B P e1 y . d . ( ; .1 &#38; .2) e1 y P xi . di . .i i . 1, 2 PA I R P (x1, x2) . U (d1 , d) . ( ; \n.1 &#38; .2) (x1, x2) 2 P (x1, x2) . U (.*U , . *U ) . ( ; .) e PA I RU P (x1, x2) . U . ( ; .) e PA \nI R H U P (x1, x2) . HU . ( ; E) (x1, x2) P er . d . (t ; .r ) er P es . U (.r (x), .r (y)) . ( ; .s \n) es CA S E P of (x, y) . er ) case es . d . (t ; .r \\x,y &#38; .s case es of (x, y) . er P x . d . (t \n; .) x AB S MU LT I P x . A . E P x . n *d . n *. d1 &#38; d2 = d3 d1 U d2 = d3 .1 &#38; .2 = .3 .1 U \n.2 = .3 * P x . d . .  n *d= d 1 2 n *t1 = t2 n *.1 = .2 Figure 3: Algorithmic cardinality analysis \nspeci.cation, part 1. and metatheory are modulo-= (checking that all our de.nitions do respect = is \nroutine). 3.2 Usage analysis The analysis itself is shown in Figures 3 and 4. The main judgement form \nis written thus P . e . d . (t ; .) e ' Figure 2: Demands and demand operations which should be read \nthus: in signature environment P, and under usage demand d, the term e places demands (t ; .) on its \ncompo\u00adnents, and elaborates to an annotated term e'. The syntax of each of these components is given \nin Figure 1, and their roles in the judgement are the following:  The signature environment P maps some \nof free variables of e to their usage signatures, . (Section 3.5). Any free variables outside the domain \nof P have an uninformative signature.  The usage demand, d, describes the degree to which e is eval\u00aduated, \nincluding how many times its sub-components are eval\u00aduated or called.  Using P, the judgement transforms \nthe incoming demand d into the demands (t ; .) that e places on its arguments and free variables respectively: \n The usage that e places on its argument is given by t, which more than once, and each call will place \na new demand on the free variables. The n *. operation on the bottom line accounts for this multiplicity, \nand is de.ned in Figure 2. Rule LA M U handles an incoming demand of U by treating it just like C .(U \n), while LA M HU deals with the head-used demand HU , where the lambda is not even called so we do not \nneed to analyse the body, and e is obtained from e by adding arbitrary annotations. Similarly the return \ntype t can be any type, since the .-abstraction is not going to be applied, but is only head-used. Dually, \ngiven an application (e y), rule APPA analyses e with demand C 1(d), re.ecting that  2 . t2 ; .1) gives \na demand d for each argument. e is here called once. This returns the demand (don the context. Then we \ncan analyse the argument under demand The usage that e places on its free variables is given by its \nd 2 , using *, yielding .2; and combine .1 and .2. Rule AP P B applies when analysing e1 yields the \nless-informative usage type . free-variable usage (fv-usage), ., which is simply a .nite mapping from \nvariables to usage demands. ' We will discuss the elaborated expressions e in Section 3.7. 3.5 Usage \nsignatures For example, consider the expression e = .x . case x of (p, q) . (p, f True) Suppose we place \ndemand C 1(U ) on e, so that e is called, just once. What demand does it then place on its arguments \nand free variables? E e . C 1(U ) . (1*U (.*U , A) . ; {f . 1*C 1(U )}) That is, e will use its argument \nonce, its argument s .rst component perhaps many times, but will ignore its arguments second compo\u00adnent \n(the A in the usage type). Moreover e will call f just once. In short, we think of the analysis as describing \na demand trans\u00adformer, transforming a demand on the result of e into demands on its arguments and free \nvariables.  3.3 Pairs and case expressions With these de.nitions in mind, we can look at some of the \nanalysis rules in Figure 3. Rule PA I R explains how to analyse a pair under a demand U (d 2 ). We simply \nanalyse the two components, Suppose we have the term let f = \\x.\\y. x True in f p q We would like to \ndetermine the correct demands on p and q, namely 1*C 1(U ) and A respectively. The gold standard would \nbe to analyse f s right-hand side at every call site; that is, to behave as if f were inlined at each \ncall site. But that is not very modular; with deeply nested function de.nitions, it can be exponentially \nexpensive to analyse each function body afresh at each call site; and it does not work at all for recursive \nfunctions. Instead, we want to analyse f, summarise its behaviour, and then use that summary at each \ncall site. This summary is called f s usage signature. Remember that the main judgement describes how \na term transforms a demand for the value into demands on its context. So a usage signature must be a \n(conservative approximation of this) demand transformer. There are many ways in which one might approximate \nf s demand transformer, but rule LE TDN (Figure 4) uses a particularly simple one: Look at f s right \nhand side .y1 . . . .yk . e1, where e1 is not a lambda-expression. 1 , d respectively, and combine the \nresults with &#38; . The 1 or d auxiliary judgement  under d Analyse e1 in demand U , giving (t1 \n; .1).  Record the triple (k ; .(y) . t1 ; .1\\y  2 * (Figure 3) deals with the multiplicity of .i the \nargument demands d ) as f s usage signature in the environment P when analysing the body of the let. \nThe &#38; operator, pronounced both , is de.ned in Figure 2, and combines the free-variable usages .1 \nand .2. For the most part the de.nition is straightforward, but there is a very important wrinkle for \ncall demands: C n1(d1) &#38; C n2(d2) = C .(d1 U d2) The . part is easy, since n1 and n2 are both at \nleast 1. But note the switch from &#38; to the least upper bound U! To see why, consider what demand \nthis expression places on f: f 1 2 + f 3 4 Each call gives a usage demand for f of 1*C 1(C 1(U )), and \nif we use &#38; to combine that demand with itself we get . *C .(C 1(U )). The inner 1 is a consequence \nof the switch to U, and rightly expresses the fact that no partial application of f is called more than \nonce. The other rules for pairs PA I RU, PA IR HU, and case expressions CASE should now be readily comprehensible \n(.r \\x,y stands for the removal of {x, y} from the domain of .r .).  3.4 Lambda and application Rule \nLA M for lambdas expects the incoming demand to be a call demand C n(de ). Then it analyses the body \ne with demand de to give (t ; .). If n = 1 the lambda is called at most once, so we can return (t ; .); \nbut if n = . the lambda may be called Now, at a call site of f, rule VAR DN calls transform (., d) to \nuse the recorded usage signature . to transform the demand d for this occurrence of f. What does transform \n((k ; t ; .), d) do (Figure 1)? If the demand d on f is stronger than C 1(. . . C 1(U )), where the call \ndemands are nested k deep, we can safely unleash (t ; .) at the call site. If not, we simply treat the \nfunction as if it were called many times, by unleashing (.*t ; .*.), multiplying both the demand type \nt and the usage environment . (Figure 2). Rule LETDNAB S handles the case when the variable is not used \nin the body. 3.6 Thunks The LE T DN rule unleashes (an approximation to) the demands of the right-hand \nside at each usage site. This is good if the right hand side is a lambda, but not good otherwise, for \ntwo reasons. Consider let x = y + 1 in x + x How many times is y demanded? Just once! The thunk x is \nde\u00admanded twice, but x s thunk is memoised, so the y+1 is evaluated only once. So it is wrong to unleash \na demand on y at each of x s occurrence sites. Contrast the situation where x is a function let x = \\v. \ny + v in x 42 + x 239 Here y really is demanded twice, and LE TDN does that. Another reason that LE \nTDN would be sub-optimal for thunks is shown here: P e1 . U . (t1 ; .1) e1 P e1 . U . (t1 ; .1) e1 tf \n= .1(y) . t1 tf = .1(y) . t1 .2(f ) = A P, f :(k ; tf ; .1\\y ) e2 . d . (t ; .2) e2 P, f :(k ; tf ; .1\\y \n) e2 . d . (t ; .2) e2 .2(f ) n * C n1 (. . . (C nk (. . .) . . .)) LE T DNAB S LE T DN P let f = .y1 \n. . . yk .e1 in e2 . d . (t ; (.2\\f )) P let f = .y1 . . . yk .e1 in e2 . d . (t ; (.2\\f )) 0 n let \nf = .1y1 . . . .1yk .e1 in e2 let f = .n1 y1 . . . .nk yk .e1 in e2 P e2 . d . (t ; .2) e2 P e2 . d \n. (t ; .2) e2 A = .2(x) n *dx = .2(x) P e1 . dx . ( ; .1) e1 LE T UPAB S LE T UP P let x = e1 in e2 \n. d . (t ; .1 &#38; (.2\\x )) P let x = e1 in e2 . d . (t ; .2\\x ) 0 let x = e1 in e2 n let x = e1 in \ne2 Figure 4: Algorithmic cardinality analysis speci.cation, part 2 (let-rules). let x = (p,q) in case \nx of (a,b) -> a The body of the let places usage demand 1 * U (U , A) on x, and if we analysed x s right-hand \nside in that demand we would see that q was unused. So we get more information if we wait until we know \nthe aggregated demand on x, and use it to analyse its right\u00adhand side. This idea is embodied in the LE \nT UP rule, used if LE T DN does not apply (i.e., the right hand side is not a lambda). Rule LE T UP .rst \nanalyses the body e2 to get the demand .2(x) on x; then analyses the right-hand side e1 using that demand. \nNotice that the multiplicity n of the demand that e2 places on x is ignored; that is because the thunk \nis memoised. Otherwise the rule is quite straightforward. Rule LET UPABS deals with the case when the \nbound variable is unused in the body.  3.7 Elaboration How are we to take advantage of our analysis? \nWe do so by elabo\u00adrating the term during analysis, with annotations of two kinds (see the syntax of annotated \nexpressions in Figure 1): let-bindings carry an annotation m . 0, 1, ., to indicate how often the let \nbinding is evaluated.  Lambdas .m x.e carry an annotation m . 0, 1, ., to indicate how often the lambda \nis called. 0 serves as an indicator that the lambda is not supposed to be called at all.  Figure 3 shows \nthe elaborated terms after the . The operational semantics (Section 4) gets stuck if we use a thunk \nor lambda more often than its claimed usage; and the optimising transformations (Section 5) are guided \nby the same annotations.  3.8 A more realistic language The language of Figure 1 is stripped to its \nbare essentials. Our implementation handles all of Haskell, or rather the Core language to which Haskell \nis translated by GHC. In particular: Usage signatures for constants . are prede.ned.  All data types \nwith a single constructor (i.e., simple products) are treated analogously to pairs in the analysis. \n Recursive data types with more than one constructor and, cor\u00adrespondingly, case expressions with more \nthan one alterna\u00adtive (and hence also conditional statements) are supported. The analysis is more approximate \nfor such types: only usage de\u00admands that apply to such types are U and HU not U (d1 , d2 ). Furthermore, \ncase expressions with multiple branches give rise to a least upper bound U combination of usage types, \nas usual.  Recursive functions and let-bindings are handled, using the standard kind of .xpoint iteration \nover a .nite-height domain.  4. Soundness of the Analysis We establish the soundness of our analysis \nin a sequence of steps. Soundness means that if the analysis claims that, say, a lambda is one-shot, \nthen that lambda is only called once; and similarly for single-entry thunks. We formalise this property \nas follows: We present an operational semantics, written \"-., for the an\u00adnotated language that counts \nhow many times thunks have been evaluated and .-abstractions have been applied. The semantics simply \ngets stuck when these counters reach zero, which will happen only if the claims of the analysis are false \n(Section 4.1).  Our goal is to prove that if an expression e is elaborated to e by the analysis, then \ne in the instrumented semantics behaves identically to e in a standard un-instrumented call-by-need se\u00admantics \n(Section 4.3). For reasons of space we omit the rules for the un-instrumented call-by-need semantics \nwhich are com\u00adpletely standard (Sestoft 1997), and are identical to the rules of Figure 5 if one simply \nignores all the annotations and the mul\u00ad tiplicity side-conditions. We refer to this semantics as -.. \n We prove soundness by giving a type system for the annotated terms, and showing that for well-typed \nterms, the instrumented semantics \"-. simulates -., in a type-preserving way.  4.1 Counting operational \nsemantics We present a simple counting operational semantics for annotated terms in Figure 5. This is \na standard semantics for call-by-need, except for the fact that multiplicity annotations decorate the \nterms, stacks, and heaps. The syntax for heaps, denoted with H, contains m two forms of bindings, one \nfor expressions [x . Exp(e)] and one m for already evaluated expressions [x . Val(v)]. The multiplicity \nm . {0, 1, .} denotes how many more times are we allowed to de-reference this particular binding. The \nstacks, denoted with S, are just lists of frames. The syntax for frames includes application frames ( \ny), which store a reference y to an argument, case\u00adframes ((x, y) . e), which account for the execution \nof a case\u00adbranch, and update frames of the form #(x, m), which take care of updating the heap when the \nactive expression reduces to a value. The .rst component of an update frame is a name of a variable to \nbe updated, and the second one is its thunk cardinality. Rule ELET allocates a new binding on the heap. \nThe rules EBE TA .res only if the cardinality annotation is non-zero; it de-references an Exp(e) binding \nand emits an update frame. Rules EBE TA, EAP P, EPA I R and EPRE D are standard. Notice that EBE TA also \n.res only if the . s multiplicity m is non-zero. Note that the analy\u00adsis does not assign zero-annotations \nto lambdas, but we need them for the soundness result. Rule ELKPV de-references a binding for an already-evaluated \nex\u00ad m pression [x . Val(v)], and in a standard semantics would return v leaving the heap unaffected. \nIn our counting semantics however, Heaps (H0 ; e0 ; S0) '-. (H1 ; e1 ; S1) H ::= E | [x mm. Exp(e)], \nH ELE T (H ; let x m = e1 in e2 ; S) '-. (H, [x mm. Exp(e1)] ; e2 ; S) | [x mm. Val(v)], H ELK P E (H, \n[x mm. Exp(e)] ; x ; S) '-. (H ; e ; #(x, m) : S) if m = 1 Stacks ELK P V (H, [x m+1m. Val(v)] ; x ; \nS) '-. (H, [x mm. Val(v1)] ; v2 ; S) S ::= E | ( y) : S s.t. split(v)=(v1, v2) || #(x, m) : S ((x, y) \n. e) : S EU P D (H ; v ; #(x, m + 1) : S) '-. (H, [x mm. Val(v1)] ; v2 ; S) s.t. split(v)=(v1, v2) Auxiliary \nde.nitions EBE TA (H ; .m x.e ; ( y) : S) '-. (H ; e[y/x] ; S) if m = 1 split(.m x . e) split(v) = = \n(.m1 x.e, .m2 x.e) where m1+m2=m (v, v) otherwise EA P P EPA I R EP RE D (H ; e y ; S)(H ; case es of \n(x, y) . er ; S)(H ; (x1, x2) ; ((y1, y2) . er ) : S) '-. '-. '-. (H ; e ; ( y) : S)(H ; es ; ((x, y) \n. er ) : S)(H ; er [x1/y1, x2/y2] ; S) Figure 5: A non-deterministic counting operational semantics. \nThe guards for counting restrictions are highlighted by grey boxes. we need to account for two things. \nFirst, we decrease the multiplic\u00adity annotation on the binding (from m + 1 to m in rule ELKP V). Moreover, \nthe value v can in the future be used both directly (since it is now the active expression), and indirectly \nthrough a future de-reference of x. We express this by non-deterministically split\u00adting the value v, \nreturning two values v1 and v2 whose top-level .-annotations sum up to the original (see split in Figure \n5). Our proof needs only ensure that among the non-deterministic choices there exists a choice that simulates \n-.. Rule EUP D is similar ex\u00adcept that the heap gets updated by an update frame.  4.2 Checking well-annotated \nterms We would like to prove that if we analyse a term e, producing an annotated term e, then if e executes \nfor a number of steps in the standard semantics -., then execution of e does not get stuck in the instrumented \nsemantics \"-. of Figure 5. To do this we need to prove preservation and progress lemmas, showing that \neach step takes a well-annotated term to a well-annotated term, and that well\u00adannotated terms do not \nget stuck. Figure 6 says what it means to be well-annotated , using notation from Figures 1 and 2. The \nrules look very similar to the analysis rules of Figures 3-4, except that we check an annotated term, \nrather than producing one. For example, rule TLA M checks that the annotation on a .-abstraction (m) \nis at least as large as the call cardinality we press on this .-abstraction (n). As evaluation progresses \nthe situation clari.es, so the annotations may become more conservative than the checker requires, but \nthat is .ne. A more substantial difference is that instead of holding concrete demand transformers . \nas the analysis does (Figure 1), the environ\u00ad ment P holds generalised demand transformers e. A generalised \ndemand transformer is simply a monotone function from a demand to a pair (t ; .) of a type and a usage \nenvironment (Figure 6). In the TLET DN rule, we clairvoyantly choose any such transformer e, which is \nsound for the RHS expression denoted with P t e1 : e. We still check that that e1 can be type checked \nwith some demand d1 that comes from type-checking the body of the let (.2(x)). In rule TVA R DN we simply \napply the transformer e to get a type and fv-usage environment. Rule WFTRA N S imposes two conditions \nnecessary for the sound\u00adness of the transformer. First, it has to be a monotone function on the demand \nargument. Second, it has to soundly approximate any type and usage environment that we can attribute \nto the expression. One can easily con.rm that the intensional representation used in the analysis satis.es \nboth properties for the .-expressions bound with LE T DN. Because these rules conjure up functions e \nout of thin air, and have universally quanti.ed premises (in WFTR A NS), they do not constitute an algorithm. \nBut for the very same reasons they are convenient to reason about in the metatheory, and that is the \nonly reason we need them. In effect, Figure 6 constitutes an elaborate invariant for the operational \nsemantics. 4.3 Soundness of the analysis The .rst result is almost trivial. Lemma 4.1 (Analysis produces \nwell-typed terms). If P e . d . (t ; .) e then P e . d . (t ; .). We would next like to show that well-typed \nterms do not get stuck. To present the main result we need some notation .rst. De.nition 4.1 (Unannotated \nheaps and stacks and erasure). We use H and S to refer to an un-instrumented heap and stack respectively. \nWe use e= e to mean that the erasure of all annotations from e is e, and we de.ne S. = S and H. = H analogously. \nWe can show that annotated terms run for at least as many steps as their erasures would run in the un-instrumented \nsemantics: Theorem 4.2 (Safety for annotated terms). If E e1 . HU . (t ; E) and e1 = e1.and (E ; e1 ; \nE) -.k (H ; e2 ; S) then there exist H, e2 and S, such that (E ; e1 ; E)\"-.k (H ; e2 ; S), H. = H , S. \n= S and e2.= e2. Unsurprisingly, to prove this theorem we need to generalise the statement to talk about \na single-step reduction of a con.guration with arbitrary (but well-annotated) heap and stack. Hence we \nintro\u00adduce a well-annotated con.guration relation, denoted (H ; e ; S), that extends the well-annotation \ninvariant of Figure 6 to con.gu\u00ad rations. For reasons of space, we only give the statement of the theorem \nbelow, and defer the details of the well-annotation relation to the extended version of the paper (Sergey \net al. 2013). Lemma 4.3 (Single-step safety). Assume that (H1 ; e1 ; S1). If (H1 ; e1 ; S) -. (H2 ; e2 \n; S2) in the un-instrumented semantics, 1 then there exist H2, e2 and S2, such that (H1 ; e1 ; S1) \"-. \n(H2 ; e2 ; S2), H= H2, e= e and S= S2, and moreover 22 2 (H2 ; e2 ; S2). Notice that the counting semantics \nis non-deterministic, so Lemma 4.3 simply ensures that there exists a possible transition in the count\u00ading \nsemantics that always results in a well-typed con.guration. Lemma 4.3 crucially relies on yet another \nproperty, below. P ::= E | P, (x:e) e . d m. (t ; .) P e . d . (t ; .) (x : e) . P (t ; .) = e(d) TVA \nR DN P x . d . (t ; . &#38; (x:1*d)) x ./dom(P) TVA RUP P x . d . ( ; (x:1*d)) d C n (de) m = n P e . \nde . (t ; .) TLA M .m P x.e . d . (.(x) . t ; n *(.\\x )) d HU TLA M H U .m P x.e . d . (t ; E) P e1 \n. C 1(d) . (t1 ; .1) * t1 . d . tr P y . d . .2 2 2 TAP P P e1 y . d . (tr ; .1 &#38; .2) * * d U (d \n) P x1 . d . .1 P x2 . d . .2 1 , d21 2 T PA I R P (x1, x2) . d . ( ; .1 &#38; .2) P er . d . (t ; .r \n)P es . U (.r (x ), .r (y)) . ( ; .s ) TC A S E P case es of (x, y) . er . d . (t ; .r \\x,y &#38; .s \n) m = \u00b5(.2(x)) .2(x) n * d1 t P e1 . d1 . (t1 ; .1) P e1 : e P, (x:e) e2 . d . (t ; .2) T L E T DN m \nP let x = e1 in e2 . d . (t ; (.2\\x )) m = n P e2 . d . (t ; .2) n *dx = .2(x) P e1 . dx . ( ; .1) T \nLE T UP m P let x = e1 in e2 . d . (t ; .1 &#38; (.2\\x )) P e2 . d . (t ; .2) A = .2(x) TL E T UPAB \nS m P let x = e1 in e2 . d . (t ; .1 &#38; (.2\\x )) * P x . d . . P x . d . (t ; .) TA B S T MU LT I \n* * P x . A . E P x . n *d . n *. P t e : e .d1, d2.d1 .d, ., t .(P d2 =. e(d1) e(d2) e . d . (t ; .)) \n=. (t ; .) P t e : e e(d) WFTR A N S Figure 6: Well-annotated terms Lemma 4.4 (Value demand splitting). \nIf P v . (d1 &#38; d2) . (t ; .) then there exists a split split(v) = (v1, v2) such that: P v1 . d1 . \n(t1 ; .1) and P v2 . d2 . (t2 ; .2) and moreover t1 t , t2 t and .1 &#38; .2 .. Why is Lemma 4.4 important? \nConsider the following let x = v in case x 3 of (y,z) -> x 4 H1 . H2 HS I M 1 HSI M 2 E . E 0 H1, [x \nm. Exp(e)] . H2 n = 1 H1 . H2 e1 . e2 H S I M 3 nn H1, [x m. Exp(e1)] . H2, [x m. Exp(e2)] H1 . H2 HS \nI M 4 0 H1, [x m. Val(v)] . H2 H1 . H2 v1 . v2 H SI M 5 .. H1, [x m. Val(v1)] . H2, [x m. Val(v2)] S1 \n. S2 SSI M 1 SSI M 2 E . E (#(x, 1) : S1) . S2 S1 . S2 SS I M 3 (#(x, .) : S1) . (#(x, .) : S2) S1 . \nS2 SS I M 4 ( y) : S1 . ( y) : S2 e1 . e2 S1 . S2 S S I M 5 ((x, y) . e1) : S1 . ((x, y) . e2) : S2 \nFigure 7: Auxiliary simulation relation (heaps and stacks) The demand exercised on x from the body of \nthe let-binding will be C 1(U ) &#38; C 1(U ) = C .(U ) and hence the value v will be checked against \nthis demand (using the LETUP rule), unleashing an environment .. However, after substituting v in the \nbody (which is ultimately what call-by-need will do) we will have checked it against C 1(U ) and C 1(U \n) independently, unleashing .1 and .2 in each call site. Lemma 4.4 ensures that reduction never increases \nthe demand on the free variables of the environment, and hence safety is not compromised. It is precisely \nthe proof of Lemma 4.4 that requires demand transformers to be monotone in the demand arguments, ensured \nby WFTR ANS. Theorem 4.5 (Safety of analysis). If E e1 . HU . (t ; E) e1 and (E ; e1 ; E) -.k (H ; e2 \n; S), then there exist H, e2 and S, such that (E ; e1 ; E)\"-.k (H ; e2 ; S), H = H , S = S and e2 = e2. \nThe proof is just a combination of Lemma 4.1 and Theorem 4.2. 5. Optimisations We discuss next the two \noptimisations enabled by our analysis. 5.1 Optimised allocation for thunks We show here that for 0-annotated \nbindings there is no need to allocate an entry in the heap, and for 1-annotated ones we don t have to \nemit an update frame on the stack. Within the chosen operational model, this optimisation is of dynamic \n.avour so we express this by providing a new, optimising small-step machine for the annotated expressions. \nThe new semantics is de.ned in Figure 8. We will show that programs that can be evaluated via the counting \nsemantics (Figure 5) can be also evaluated via the optimised semantics in a smaller or equal number of \nsteps. The proof is a simulation proof, hence we de.ne relations between heaps / optimised heaps, and \nstacks / optimised stacks that are preserved during evaluation. (H0 ; e0 ; S0) =. (H1 ; e1 ; S1) 0 \nOP T-ELE TA (H ; let x = e1 in e2 ; S) =. (H ; e2 ; S) n n OP T-ELE T U (H ; let x = e1 in e2 ; S) =. \n(H[x m. Exp(e1)] ; e2 ; S) where n = 1 . OP T-ELK P EM (H, [x m. Exp(e)] ; x ; S) =. (H ; e ; #(x , \n.) : S) 1 OP T-ELK P EO (H, [x m. Exp(e)] ; x ; S) =. (H ; e ; S) .. OP T-ELK P V (H, [x m. Val(v)] \n; x ; S) =. (H, [x m. Val(v)] ; v ; S) . OP T-EUP D (H ; v ; #(x, .) : S) =. (H, [x m. Val(v)] ; v ; \nS) OP T-EB E TA (H ; .m x.e ; ( y) : S) =. (H ; e[y/x] ; S) OP T-EAP P (H ; e y ; S) =. (H ; e ; ( y) \n: S) OP T-EPA I R (H ; case es of (x, y) . er ; S) =. (H ; es ; ((x, y) . er ) : S) OP T-EPRE D (H ; \n(x1, x2) ; ((y1, y2) . er ) : S) =. (H ; er [x1/y1, x2/y2] ; S) Figure 8: Optimised counting semantics \nDe.nition 5.1 (Auxiliary .-relations). We write e1 . e2 iff e1 and e2 differ only on the .-annotations. \nH1 . H2 and S1 . S2 are de.ned in Figure 7. For this optimisation the annotations on .-abstractions play \nno role, hence we relate any expressions that differ only on those. Figure 7 tells us when a heap H is \nrelated with an optimised heap Hopt with the relation H . Hopt . As we have described, there are 0 no \n. bindings in the optimised heap. Moreover notice that there 1 are no bindings of the form [x . Val(v)] \nin either the optimised or unoptimised heap. It is easy to see why: every heap binding m starts life \nas [x . Exp(e)]. By the time Exp(e) has become a value Val(v), we have already used x once. Hence, if \noriginally m = . then the value binding will also be . (in the optimised or unoptimised semantics). If \nit was m = 1 then it can only be 0 in the un-optimised heap and non-existent in the optimised heap. If \nit was m = 0 then no such bindings would have existed in the optimised heap anyway. The relation between \nstacks is given with S . Sopt . Rule SSIM2 ensures that there are no frames #(x, 1) in the optimised \nstack. In fact during evaluation it is easy to observe that there are not going to be any update frames \n#(x, 0) in the original or optimised stack. We can now state the optimisation simulation theorem. Theorem \n5.1 (Optimised semantics). If (H1 ;e1;S1) . (H2 ;e2 ;S2)and (H1 ; e1 ; S1) \"-. (H1 ' ; e1 ' ; S1 ' ) \nthen there exists k . {0, 1} s.t. ' '' .k (H2 ; e2 ; S2) =(H2 ' ; e2 ; S2 ' ) and (H1 ' ; e1 ; S1 ' ) \n. (H2 ' ; e2 ; S2 ' ). Notice that the counting semantics may not be able to take a transition at some \npoint due to the wrong non-deterministic choice but in that case the statement of Theorem 5.1 holds trivially. \nFinally, we tie together Theorems 5.1 and 4.5 to get the following result. Theorem 5.2 (Analysis is safe \nfor optimised semantics). If e1 . HU . (t ; E) e1 and (E ; e1 ; E) -.n (H ; e2 ; S) then (E ; e1 ; E) \n=(H ; e2 ; S) s. t. e2 = e2, m = n, and there exist .m H2 and S2 s.t. H2 = H and S2 = S and H2 . H and \nS2 . S. Theorem 5.2 says that if a program e1 evaluates in n steps to e2 in the reference semantics, \nthen it also evaluates to the same e2 (modulo annotation) in the optimised semantics in n steps or fewer; \nand the heaps and stacks are consistent. Moreover, the theorem has informative content on in.nite sequences. \nFor example it says that for any point in the evaluation in the reference semantics, we will have earlier \nreached a corresponding intermediate con.guration in the optimised semantics with consistent heaps and \nstacks.  5.2 let-in .oating into one-shot lambdas As discussed in Section 2, we are interested in the \nparticular case of let-.oating (Peyton Jones et al. 1996): moving the binder into the body of a lambda-expression. \nThis transformation is trivially safe, given obvious syntactic side conditions (Moran and Sands 1999, \n\u00a74.5), however, in general, it is not bene.cial. Here we describe the conditions under which let-in .oating \nmakes things better in terms of the length of the program execution sequence. We start by de.ning let-in \n.oating in a form of syntactic rewriting: De.nition 5.2 (let-in .oating for one-shot lambdas). m1 m2 \n.1 let z = e1 in (let f = x . e in e2) m2 .1 m1 =. let f = x . (let z = e1 in e) in e2, for any m1, \nm2 and z ./FV (e2). Next, we provide a number of de.nitions necessary to formulate the so called improvement \nresult (Moran and Sands 1999). The improvement is formulated for closed, well-formed con.gurations. For \na con.guration (H ; e ; S) to be closed, any free variables in H, e and S must be contained in a union \ndom(H) . dom(S), where dom(H) is a set of variables bound by a heap H, and dom(S) is a set of variables \nmarked for update in a stack S. A con.guration is well-formed if dom(H) and dom(S) are disjoint. De.nition \n5.3 (Convergence). For a closed con.guration (H;e;S), def (H ; e ; S) .N = .H ' , v, N . (H ; e ; S) \n\"-.N (H ' ; v ; E) def (H ; e ; S) .=N = .M . (H ; e ; S) .M and M = N The following theorem shows that \nlocal let-in .oating into the body of a one-shot lambda does not make the execution longer. Theorem 5.3 \n(Let-in .oat improvement). For any H and S, if m m1 1 (H ; let z = e1 in (let f = .x . e in e2) ; S) \n.N and z ./FV (e2), then m1 1 m (H ; let f = .x . (let z = e1 in e) in e2 ; S) .=N . Even though Theorem \n5.3 gives a termination-dependent result, its proof (Sergey et al. 2013) goes via a simulation argument, \nhence it is possible to state the theorem in a more general way without requiring termination. We also \nexpect that the improvement result extends to arbitrary program contexts, but have not carried out the \nexercise. 6. Implementation We have implemented the cardinality analyser by extending the de\u00admand analysis \nmachinery of the Glasgow Haskell Compiler, avail\u00adable from its open-source repository.2 We brie.y summarise \nsome implementation speci.cs in this section. 6.1 Analysis The implementation of the analysis was straightforward, \nbecause GHC s existing strictness analyser is already cast as a backwards analysis, exactly like our \nnew cardinality analysis. So the existing analyser worked unchanged; all that was required was to enrich \nthe domains over which the analyser works.3 In total, the analyser increased from 900 lines of code to \n1,140 lines, an extremely modest change. We run the analysis twice, once in the middle of the optimisation \npipeline, and once near the end. The purpose of the .rst run is to expose one-shot lambdas, which in \nturn enable a cascade of sub\u00adsequent transformations (Section 6.3). The second analysis .nds the single-entry \nthunks, which are exploited by the code genera\u00adtor. This second analysis is performed very late in the \npipeline (a) so that it sees the result of all previous inlining and optimisation and (b) because the \nsingle-entry thunk information is not robust to certain other transformations (Section 6.4). 6.2 Absence \nGHC exploits absence in the worker/wrapper split, as described in Section 2.3: absent arguments are not \npassed from the wrapper to the worker. 6.3 One-shot lambdas As shown in Section 5.2, there is no run-time \npayoff for one-shot lambdas. Rather, the information enables some important compile\u00adtime transformations. \nSpeci.cally, consider let x = costly v in . . . (.y. . . . x . . .) . . . If the .y is a one-shot lambda, \nthe binding for x can be .oated inside the lambda, without risk of duplicating the computation of costly. \nOnce the binding for x is inside the .y, several other improvements may happen: It may be inlined at \nx s use site, perhaps entirely eliminating the allocation of a thunk for x.  It may enable a rewrite \nrule (eg foldr/build fusion) to .re.  It may allow two lambdas to be replaced by one. For example  \nf = .v. let x = costly v in .y. . . . x . . . =. f = .v..y. . . . (costly v) . . . The latter produces \none function with two arguments, rather than a curried function that returns a heap-allocated lambda \n(Marlow and Peyton Jones 2006).  6.4 Single-entry thunks The code that GHC compiles for a thunk begins \nby pushing an up\u00addate frame on the stack, which includes a pointer to the thunk. Then the code for the \nthunk is executed. When evaluation is complete, the value is returned, and the update frame overwrites \nthe thunk with an indirection to the values (Peyton Jones 1992). It is easy to modify this mechanism \nto take advantage of single-entry thunks: we do not generate the push-update-frame code for single-entry \n2 http://github.com/ghc/ghc 3 This claim is true in spirit, but in practice we substantially refactored \nthe existing analyser when adding usage cardinalities. Program Synt. .1 Synt. Thnk1 RT Thnk1 anna 4.0% \n7.2% 2.9% bspt 5.0% 15.4% 1.5% cacheprof 7.6% 11.9% 5.1% calendar 5.7% 0.0% 0.2% constraints 2.0% 3.2% \n4.5% cryptarithm2 0.6% 3.0% 74.0% gcd 12.5% 0.0% 0.0% gen regexps 5.6% 0.0% 0.2% hpg 5.2% 0.0% 4.1% integer \n8.3% 0.0% 0.0% life 3.2% 0.0% 1.8% mkhprog 27.4% 20.8% 5.8% nucleic2 3.5% 3.1% 3.2% partstof 5.8% 10.7% \n0.1% sphere 7.8% 6.2% 20.0% ... and 72 more programs Arithmetic mean 10.3% 12.6% 5.5%  Table 1. Analysis \nresults for nofib: ratios of syntactic one-shot lambdas (Synt. .1), syntactic used-once thunks (Synt. \nThnk1) and runtime entries into single-entry thunks (RT Thnk1). thunks. There is a modest code size saving \n(fewer instructions gen\u00aderated) and a modest runtime saving (a few store instructions saved on thunk \nentry, and a few more when evaluation is complete). Take care though! The single-entry property is not \nrobust to program transformation. For example, common sub-expression elimination (CSE) can combine two \nsingle-entry thunks into one multiple-entry one, as can this sequence of transformations: 1 let y = e \nin let x = y + 0 in x * x 1 Identity of + =. let y = e in let x = y in x * x 1 Inline x =. let y = e \nin y * y Wrong! This does not affect the formal results of the paper, but it is the rea\u00adson that our \nsecond run of the cardinality analysis is immediately before code generation. 7. Evaluation To measure \nthe accuracy of the analysis, we counted the propor\u00adtion of (a) one-shot lambdas and (b) single-entry \nthunks. In both cases, these percentages are of the syntactically occurring lambdas or thunks respectively, \nmeasured over the code of the benchmark program only, not library code. Table 1 shows the results reported \nby our analysis for programs from the nofib benchmark suite (Par\u00ad tain 1993). The numbers are quite encouraging. \nOne-shot lambdas account for 0-30% of all lambdas, while single-entry thunks are 0-23% of all thunks. \nThe static (syntactic) frequency of single-entry thunks may be very different to their dynamic frequency \nin a program execution, so we instrumented GHC to measure the latter. (We did not measure the dynamic \nfrequency of one-shot lambdas, because they confer no di\u00adrect performance bene.t.) The RT Thunk column \nof Table 1 gives the dynamic frequency of single-entry thunks in the same nofib programs. Note that these \nstatistics include single-entry thunks from libraries, as well as the benchmark program code. The re\u00adsults \nvary widely. Most programs do not appear to use single-entry thunks much, while a few use many, up to \n74% for cryptarithm2. 7.1 Optimising nofib programs In the end, of course, we seek improved runtimes, \nalthough the bene.ts are likely to be modest. One-shot lambdas do not confer Program Allocs Runtime \nNo hack Hack No hack Hack anna -2.1% -0.2% +0.1% -0.0% bspt -2.2% -0.0% -0.0% +0.0% cacheprof -7.9% -0.6% \n-6.1% -5.0% calendar -9.2% +0.2% -0.0% -0.0% constraints -0.9% -0.0% -1.2% -0.2% cryptarithm2 -0.3% -0.3% \n-2.3% -2.1% gcd -15.5% -0.0% -0.0% +0.0% gen regexps -1.0% -0.1% -0.0% -0.0% hpg -2.0% -1.0% -0.1% -0.0% \ninteger -0.0% -0.0% -8.8% -6.6% life -0.8% -0.0% -5.9% -1.8% mkhprog -11.9% +0.1% -0.0% -0.0% nucleic2 \n-14.1% -10.9% +0.0% +0.0% partstof -95.5% -0.0% -0.0% -0.0% sphere -1.5% -1.5% +0.0% -0.1% ... and 72 \nmore programs Min Max Geometric mean -95.5% +3.5% -6.0% -10.9% +0.5% -0.3% -28.2% +1.8% -2.2% -12.1% \n+2.8% -1.4% Table 2. Cardinality analysis-enabled optimisations for nofib any performance bene.ts directly; \nrather, they remove potential obstacles from other compile-time transformations. Single-entry thunks, \non the other hand give an immediate performance bene.t, by omitting the push-update-frame code, but it \nis a small one. Table 2 summarises the effect of cardinality analysis when running the nofib suite. Allocs \nis the change in how much heap was allocated when the program is run and Runtime is a change in the actual \nprogram execution time. In Section 2.1 we mentioned a hack, used by Gill in GHC, in which he hard-coded \nthe call-cardinality information for three particular functions: build, foldr and runST. Our analysis \nrenders this hack redundant, as now the same results can be soundly inferred. We therefore report two \nsets of results: relative to an un-hacked baseline, and relative to a hacked baseline. In both cases \nbinary size of the (statically) linked binaries falls slightly but consistently (2.0% average), which \nis welcome. This may be due to less push\u00adupdate-frame code being generated. Considering allocation, the \nnumbers relative to the non-hacked baseline are quite encouraging, but relative to the hacked compiler \nthe improvements are modest: the hack was very effective! Other\u00adwise, only one program, nucleic2 shows \na signi.cant (11%) re\u00adduction in allocation, which turned out to be because a thunk was .oated inside \na one-shot lambda and ended up never being allo\u00adcated, exactly as advertised.4 A shortcoming of nofib \nsuite is that runtimes tend to be short and very noisy: even with the execution key slow only 18 programs \nfrom the suite run for longer than half second (with a maximum of 2.5 seconds for constraints). Among \nthose long-runners the biggest performance improvement is 8.8% (for integer), with an average of 2.3%. \n4 One can notice that the new compiler sometimes performs worse than the cardinality-unaware versions \nin a very few benchmarks in nofib. In a highly optimising compiler with many passes it is very hard to \nensure that every optimisation always makes the program run faster; and, even if a pass does improve \nthe program per se, to ensure that every subsequent pass will carry out all the optimisations that it \ndid before the earlier improvement was implemented. The data show that we do not always succeed. We leave \nfor the future some detailed forensic work to .nd out exactly why. Program RT Thnk1 No-Opt RT RT . binary-trees \n49.4% 66.83 s -9.2% fannkuch-redux 0.0% 158.94 s -3.7% n-body 5.7% 38.41 s -4.4% pidigits 8.8% 41.56 \ns -0.3% spectral-norm 4.6% 17.83 s -1.7% Table 3. Optimisation of the programs from Benchmarks Game \nLibrary .1 Thnk1 Benchmark Alloc . attoparsec binary bytestring cassava 32.8% 16.8% 5.3% 26.4% 19.3% \n0.9% 4.3% 9.8% benchmarks bench builder get boundcheck all benchmarks -7.1% -0.2% -0.3% -4.3% -0.5% -6.6% \n-0.7% Table 4. Analysis and optimisation results for hackage libraries Program LOC GHC Alloc . GHC \nRT . No hack Hack No hack Hack anna cacheprof fluid gamteb parser veritas 5740 1600 1579 1933 2379 4674 \n-1.6% -1.7% -1.9% -0.5% -0.7% -1.4% -1.5% -0.4% -1.9% -0.1% -0.2% -0.3% -0.8% -2.3% -2.8% -0.5% -2.6% \n-4.5% -0.4% -1.8% -1.6% -0.1% -0.6% -4.1% Table 5. Compilation with optimised GHC For more realistic \nnumbers, we measured the improvement in run\u00adtime, relative to the hacked compiler, for several programs \nfrom the Computer Language Benchmarks Game.5 The results are shown in Table 3. All programs were run \nwith the of.cial shootout settings (except spectral-norm, to which we gave a bigger input value of 7500) \non a 2.7 GHz Intel Core i7 OS X machine with 8 Gb RAM. These are uncharacteristic Haskell programs, optimised \nto within an inch of their life by dedicated Haskell hackers. There is no easy meat to be had, and indeed \nthe heap-allocation changes are so tiny (usually zero, and -0.2% at the most in the case of binary-trees) \nthat we omit them from the table. However, we do get one joyful re\u00adsult: a solid speedup of 9.2% in binary-trees \ndue to fewer thunk updates. As you can see, nearly half of its thunks entered at runtime are single-entry. \n 7.2 Real-world programs To test our analysis and the cardinality-powered optimisations on some real-world \nprograms, we chose four continuation-heavy li\u00adbraries from the hackage repository:6 attoparsec, a fast \nparser combinator library, binary, a lazy binary serialisation library, bytestring, a space-ef.cient \nimplementation of byte-vectors, and cassava, a CSV parsing and encoding library. These libraries come \nwith accompanying benchmark suites, which we ran both for the baseline compiler and the cardinality-powered \none. Table 4 contains the ratios of syntactic one-shot lambdas and used-once thunks for the libraries, \nas well relative improvement in memory allocation for particular benchmarks. Since we were interested \nonly in the absolute improvement against the state of 5 http://benchmarksgame.alioth.debian.org/ 6 http://hackage.haskell.org/ \n the art, we made our comparison with respect to the contemporary version of (hacked) baseline GHC. \nThe encouraging results for attoparsec are explained by its relatively high ratio of one-shot lambdas, \nwhich is typical for parser combinator libraries. GHC itself is a very large Haskell program, written \nin a variety of styles, so we compiled it with and without cardinality-powered op\u00adtimisations, and measured \nthe allocation and runtime improvement when using the two variants to compile several programs. The re\u00adsults \nare shown in Table 5. As in the other cases, we get modest but consistent improvements. 8. Related Work \n8.1 Abstract interpretation for usage and absence The goal of the traditional usage/absence analyses \nis to .gure out which parts of the programs are used, and which are not (Peyton Jones and Partain 1994). \nThis question was .rst studied in the late 80 s, when an elegant representation of the usage analysis \nin terms of projections (Hinze 1995) was given by Wadler and Hughes (Wadler and Hughes 1987). Their formulation \nallows one to de.ne a backwards analysis inferring the usage of arguments of a function from the usage \nof its result an idea that we adopted wholesale. Our work has important differences, notably (a) call \ndemands C n(d), which appear to be entirely new; and (b) the ability to treat nested lambdas, which requires \nus to capture the usage of free variables in a usage signature. Moreover our formal underpinning is quite \ndifferent to their (denotational) approach, because we fundamentally must model sharing.  8.2 Type-based \napproaches The notion of single-entry thunks and one-shot lambdas is rem\u00adiniscent of linear types (Girard \n1995; Turner and Wadler 1999), a similarity that was noticed very early (Launchbury et al. 1993). Linear \ntypes per se are far too restrictive (see, for example, Wans\u00ad brough and Peyton Jones (1999, \u00a7 2.2) for \ndetails), but the idea of using a type system to express usage information inspired a series of once \nupon a type papers7 (Gustavsson 1998; Turner et al. 1995; Wansbrough 2002; Wansbrough and Peyton Jones \n1999). Alas, a promising idea turned out to lead, step by step, into a deep swamp. Firstly, subtyping \nproved to be essential, so that a function that used its argument once could have a type like Int1 . \nInt, but still be applied to an argument x that was used many times and had type Int. (Wansbrough and \nPeyton Jones 1999). Then us\u00adage polymorphism proved essential to cope with currying: [Using the monomorphic \nsystem] in the entirety of the standard libraries, just two thunks were annotated as used-once (Wansbrough \n2002, 3.7). Gustavsson advocated bounded polymorphism to gain greater precision (Gustavsson and Sveningsson \n2001), while Wansbrough extended usage polymorphism to data types, sometimes resulting in data types \nwith many tens of usage parameters. The interaction of ordinary type polymorphism with all these usage-type \nfeatures was far from straightforward. The inference algorithm for a poly\u00admorphic type system with bounds \nand subtyping is extremely com\u00adplex. And so on. Burdened with these intellectual and implementa\u00adtion \ncomplexities, Wansbrough s heroic prototype in GHC (around 2,580 LOC of brand-new code; plus pervasive \nchanges to thou\u00adsands of lines of code elsewhere) turned out to be unsustainable, and never made it into \nthe main trunk. Our system sidesteps these dif.culties entirely by treating the prob\u00adlem as a backwards \nanalysis like strictness analysis, rather than as a type system. This is what gives the simplicity to \nour approach, but also prevents it from giving rich demand signatures to third-and 7 The title, as so \noften, is due to Wadler. higher-order functions: our usage types can account uniformly only for the \n.rst-and second-order functions, thanks to call demands. For example what type might we attribute to \nf x g = g x The usage of x depends on the particular g in the call, so usage polymorphism would be called \nfor. This is indeed more expressive but it is also more complicated. We deliberately limit precision \nfor very higher-order programs, to gain simplicity. At some level abstract interpretation and type inference \ncan be seen as different sides of the same coin, but there are some interesting differences. For example, \nour LETDN and LE T UP rules are explicit about information .ow; in the former, information .ows from \nthe de.nition of a function to its uses, while in the latter the .ow is reversed. Type systems use uni.cation \nvariables to allow much richer information .ow but at the cost of generating constraints involving subtyping \nand bounds that are tricky to solve. Another intriguing difference is in the handling of free variables: \nlet f = \\x. y + x in if b then f 1 else y How many times is the free variable y evaluated in this expression? \nObviously just once, and LE T DN discovers this, because we un\u00adleash the demand on y at f s call site, \nand lub the two branches of the if. But type systems behave like LET UP: compute the demand on f (namely, \ncalled once) and from that compute the demand on y. Then combine the demand on y from the body of the \nlet (used at most once), and from f s right hand side (used at most once), yielding the result that y \nis used many times. We have lost the fact that the two uses come from different branches of the conditional. \nThe fact that our usage signatures include the . component makes them more expressive than mere types \n unless we extend the type system yet further with an polymorphic effect system (Hage et al. 2007; Holdermans \nand Hage 2010). Moreover, the analysis approach deals very naturally with absence, and with product types \nsuch as pairs, which are ubiquitous. Type-based approaches do not do so well here. In short, an analysis-based \napproach has proved much simpler in\u00adtellectually than the type-based one, and far easier to implement. \nOne might wonder if a clever type system might give better results in practice, but Wansbrough s results \n(mostly zero change to alloca\u00adtion; one program allocated 15% more, one 14% less (Wansbrough 2002)) were \nno more compelling than those we report. Our proof technique does however share much in common with Wansbrough \nand Gustavsson s work, all three being based on an operational se\u00admantics with an explicit heap. One \nother prominent type-based usage system is Clean s unique\u00adness types (Barendsen and Smetsers 1996). Clean \ns notion of uniqueness is, however, fundamentally different to ours. In Clean a unique-typed argument \nplaces a restriction on the caller (to pass the only copy of the value), whereas for us a single-entry \nargument is a promise by callee (to evaluate the argument at most once). 8.3 Other related work Call \ndemands, introduced in this paper, appear to be related to the notion of applicativeness, employed in \nthe recent work on rele\u00advance typing (Holdermans and Hage 2010). In particular, applica\u00ad tiveness means \nthat an expression either guaranteed to be applied to an argument (S), or may not be applied to an argument \n(L). In this terminology S corresponds to a strong version of our de\u00admands C .(d), which requires d c \nU , and L is similar to our U . The seq-like evaluation of expressions corresponds to our demand HU . \nHowever, neither call-nor thunk-cardinality are captured by the concept of applicativeness. Abstract \ncounting or sharing analysis conservatively determines which parts of the program might be used by several \ncomponents or accessed several times in the course of execution. Early work employed a forward abstract \ninterpretation framework (Goldberg 1987; Hudak 1986). Since the forward abstract interpreter makes assumptions \nabout arguments of a function it examines, the abstract interpretation can account for multiple combinations \nof those and may, therefore, be extremely expensive to compute. Recent development on the systematic \nconstruction of abstract interpretation-based static analyses for higher-order programs, known as abstracted \nabstract machines (AAM), makes it straight\u00adforward to derive an analyser from an existing small-step \nopera\u00adtional semantics, rather than come up with an ad-hoc non-standard one (Van Horn and Might 2010). \nThis approach also greatly sim\u00ad pli.es integration of the counting abstract domain to account for sharing \n(Might and Shivers 2006). However, the abstract inter\u00ad preters obtained this way are whole-program forward \nanalysers, which makes them non-modular. It would be, however, an interest\u00ading future work to build a \nbackwards analysis from AAM. 9. Conclusion Cardinality analysis is simple to implement (it added 250 \nlines of code to a 140,000 line compiler), and it gives real improvements for serious programs, not just \nfor toy benchmarks; for example, GHC itself (a very large Haskell program) runs 4% faster. In the context \nof a 20-year-old optimising compiler, a gain of this magnitude is a solid win. Acknowledgements We are \ngrateful to Johan Tibell for the sug\u00adgestion to use benchmark-accompanied hackage libraries and the cabal \nbench utility for the experiments in Section 7.2. We also thank the POPL 14 reviewers for their useful \nfeedback. References Erik Barendsen and Sjaak Smetsers. Uniqueness typing for functional languages with \ngraph rewriting semantics. Mathematical Structures in Computer Science, 6(6):579 612, 1996. Andy Gill. \nCheap Deforestation for Non-strict Functional Languages. PhD thesis, University of Glasgow, Department \nof Computer Schence, 1996. Andy Gill, John Launchbury, and Simon Peyton Jones. A Short Cut to Deforestation. \nIn Proceedings of the Sixth ACM Conference on Functional Programming Languages and Computer Architecture, \npages 223 232, 1993. Jean-Yves Girard. Linear logic: its syntax and semantics. In Proceedings of the \nworkshop on Advances in linear logic, pages 1 42. Cambridge University Press, 1995. Benjamin Goldberg. \nDetecting sharing of partial applications in functional programs. In Functional Programming Languages \nand Computer Archi\u00adtecture, volume 274 of LNCS, pages 408 425. Springer-Verlag, 1987. J\u00a8A type based \nsharing analysis for update avoidance orgen Gustavsson. and optimisation. In Proceedings of the Third \nACM SIGPLAN Interna\u00adtional Conference on Functional Programming (ICFP 98), pages 39 50. ACM, 1998. J \n\u00a8orgen Gustavsson and Josef Sveningsson. A usage analysis with bounded usage polymorphism and subtyping. \nIn Implementation of Functional Languages (IFL 2000), Selected Papers, volume 2011 of LNCS, pages 140 \n157. Springer, 2001. Jurriaan Hage, Stefan Holdermans, and Arie Middelkoop. A generic us\u00adage analysis \nwith subeffect quali.ers. In Proceedings of the 12th ACM SIGPLAN International Conference on Functional \nProgramming (ICFP 07), pages 235 246. ACM, 2007. Ralf Hinze. Projection-based strictness analysis -theoretical \nand practical aspects. PhD thesis, Bonn University, 1995. Stefan Holdermans and Jurriaan Hage. Making \nstricternes more relevant. In PEPM 10: Proceedings of the 2010 ACM SIGPLAN workshop on Partial evaluation \nand program manipulation, pages 121 130. ACM, 2010. Paul Hudak. A semantic model of reference counting \nand its abstraction. In Proceedings of the 1986 ACM Conference on Lisp and Functional Programming, pages \n351 363. ACM, 1986. Richard Jones. Tail recursion without space leaks. J. Funct. Program., 2 (1):73 79, \n1992. John Launchbury, Andy Gill, John Hughes, Simon Marlow, Simon Peyton Jones, and Philip Wadler. Avoiding \nunnecessary updates. In Proceedings of the 1992 Glasgow Workshop on Functional Programming, Workshops \nin Computing, pages 144 153. Springer, 1993. Simon Marlow and Simon Peyton Jones. Making a fast curry: \npush/enter vs. eval/apply for higher-order languages. J. Funct. Program., 16(4-5): 415 449, 2006. Matthew \nMight and Olin Shivers. Improving .ow analyses via GCFA: abstract garbage collection and counting. In \nProceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming (ICFP 06), \npages 13 25. ACM, 2006. Andrew Moran and David Sands. Improvement in a Lazy Context: An Operational Theory \nfor Call-by-Need. In POPL 99: Proceedings of the 26th annual ACM SIGPLAN-SIGACT symposium on Principles \nof programming languages, pages 43 56. ACM, 1999. Will Partain. The nofib benchmark suite of Haskell \nprograms. In Pro\u00adceedings of the 1992 Glasgow Workshop on Functional Programming, Workshops in Computing, \npages 195 202. Springer, 1993. Simon Peyton Jones. Implementing Lazy Functional Languages on Stock Hardware: \nThe Spineless Tagless G-Machine. J. Funct. Program., 2(2): 127 202, 1992. Simon Peyton Jones and Will \nPartain. Measuring the effectiveness of a sim\u00adple strictness analyser. In Proceedings of the 1993 Glasgow \nWorkshop on Functional Programming, pages 201 220. Springer, 1994. Simon Peyton Jones and Andr\u00b4e Santos. \nA transformation-based optimiser for Haskell. Science of Computer Programming, 32(1-3):3 47, 1998. Simon \nPeyton Jones, Will Partain, and Andr \u00b4e Santos. Let-.oating: Mov\u00ading Bindings to Give Faster Programs. \nIn Proceedings of the First ACM SIGPLAN International Conference on Functional Programming (ICFP 96), \npages 1 12. ACM, 1996. Amr Sabry and Matthias Felleisen. Reasoning about programs in continuation-passing \nstyle. In Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, pages 288 298. ACM, \n1992. Ilya Sergey, Dimitrios Vytiniotis, and Simon Peyton Jones. Modular, Higher-Order Cardinality Analysis \nin Theory and Practice. Extended version. Technical Report MSR-TR-2013-112, Microsoft Research, 2013. \nAvailable at http://research.microsoft.com/apps/pubs/ ?id=204260. Peter Sestoft. Deriving a lazy abstract \nmachine. J. Funct. Program., 7(3): 231 264, 1997. David N. Turner and Philip Wadler. Operational interpretations \nof linear logic. Theor. Comput. Sci., 227(1-2):231 248, 1999. David N. Turner, Philip Wadler, and Christian \nMossin. Once upon a type. In Proceedings of the Seventh ACM Conference on Functional Program\u00adming Languages \nand Computer Architecture, pages 1 11. ACM, 1995. David Van Horn and Matthew Might. Abstracting abstract \nmachines. In Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming (ICFP \n10), pages 51 62. ACM, 2010. Philip Wadler and John Hughes. Projections for strictness analysis. In Func\u00adtional \nProgramming Languages and Computer Architecture, volume 274 of LNCS, pages 385 407. Springer-Verlag, \n1987. Keith Wansbrough. Simple Polymorphic Usage Analysis. PhD thesis, Computer Laboratory, University \nof Cambridge, 2002. Keith Wansbrough and Simon Peyton Jones. Once Upon a Polymorphic Type. In POPL 99: \nProceedings of the 26th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages \n15 28. ACM, 1999.     \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.</p>", "authors": [{"name": "Ilya Sergey", "author_profile_id": "81436603558", "affiliation": "IMDEA Software Institute, Madrid, Spain", "person_id": "P4383840", "email_address": "ilya.sergey@imdea.org", "orcid_id": ""}, {"name": "Dimitrios Vytiniotis", "author_profile_id": "81100156369", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383841", "email_address": "dimitris@microsoft.com", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383842", "email_address": "simonpj@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535861", "year": "2014", "article_id": "2535861", "conference": "POPL", "title": "Modular, higher-order cardinality analysis in theory and practice", "url": "http://dl.acm.org/citation.cfm?id=2535861"}