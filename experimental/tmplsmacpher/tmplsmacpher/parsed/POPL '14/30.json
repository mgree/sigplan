{"article_publication_date": "01-08-2014", "fulltext": "\n Fair Reactive Programming Andrew Cave Francisco Ferreira Prakash Panangaden Brigitte Pientka School \nof Computer Science, McGill University, Montreal, Canada {acave1,.erre8,prakash,bpientka}@cs.mcgill.ca \nAbstract Functional Reactive Programming (FRP) models reactive systems with events and signals, which \nhave previously been observed to correspond to the eventually and always modalities of linear temporal \nlogic (LTL). In this paper, we de.ne a constructive vari\u00adant of LTL with least .xed point and greatest \n.xed point opera\u00adtors in the spirit of the modal mu-calculus, and give it a proofs-as\u00adprograms interpretation \nas a foundational calculus for reactive pro\u00adgrams. Previous work emphasized the propositions-as-types \npart of the correspondence between LTL and FRP; here we emphasize the proofs-as-programs part by employing \nstructural proof theory. We show that the type system is expressive enough to enforce liveness properties \nsuch as the fairness of schedulers and the eventual de\u00adlivery of results. We illustrate programming in \nthis calculus using (co)iteration operators. We prove type preservation of our opera\u00adtional semantics, \nwhich guarantees that our programs are causal. We give also a proof of strong normalization which provides \njusti\u00ad.cation that our programs are productive and that they satisfy live\u00adness properties derived from \ntheir types. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: Applicative (Functional) \nProgramming Keywords temporal logic; propositions-as-types; functional reac\u00adtive programming; liveness \n1. Introduction Reactive programming seeks to model systems which react and respond to input such as \ngames, print and web servers, or user interfaces. Functional reactive programming (FRP) was introduced \nby Elliott and Hudak [10] to raise the level of abstraction for writing reactive programs, particularly \nemphasizing higher-order functions. Today FRP has several implementations [7 9, 29]. Many allow one to \nwrite unimplementable non-causal functions, where the present output depends on future input, and space \nleaks are all too common. Recently there has been a lot of interest in type-theoretic foun\u00addations for \n(functional) reactive programming [14, 17, 22 24] with the intention of overcoming these shortcomings. \nIn particular, Jef- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than the author(s) must be honored. Abstracting with credit \nis permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Request permissions from permissions@acm.org. POPL 14, January \n22 24, 2014, San Diego, CA, USA. Copyright is held by the owner/author(s). Publication rights licensed \nto ACM. ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535881 frey [14] \nand Jeltsch [17] have recently observed that Pnueli s linear temporal logic (LTL) [31] can act as a type \nsystem for FRP. In this paper, we present a novel logical foundation for discrete time FRP with (co)iteration \noperators which exploits the expres\u00adsiveness afforded by the proof theory (i.e. the universal properties) \nfor least and greatest .xed points, in the spirit of the modal \u00b5\u00adcalculus [21]. The always , eventually \nand until modalities of LTL arise simply as special cases. We do this while still remaining relatively \nconservative over LTL. Moreover, we demonstrate that distinguishing between and in\u00adterleaving of least \nand greatest .xed points is key to statically guar\u00adantee liveness properties, i.e. something will eventually \nhappen, by type checking. To illustrate the power and elegance of this idea, we describe the type of \na fair scheduler any program of this type is guaranteed to be fair, in the sense that each participant \nis guaran\u00adteed that his requests will eventually be served. Notably, this exam\u00adple requires the expressive \npower of interleaving least .xed points and greatest .xed points (a construction due to Park [30]) which \nis unique to our system. Our approach of distinguishing between least and greatest .xed points and allowing \nfor iteration and coiteration is in stark con\u00adtrast to prior work in this area: Jeffrey s work [15, 16] \nfor exam\u00ad ple only supports less expressive combinators instead of the primi\u00adtive (co)recursion our system \naffords, and only particular instances of our recursive types. Krishnaswami et al. employ a more ex\u00adpressive \nnotion of recursion, which entails unique (guarded) .xed points [22 24]. This implies that their type \nsystems are incapable of expressing liveness properties, in constrast to our work. Jeltsch [17, 18] describes \ndenotational settings for FRP, some of which are capable of expressing liveness guarantees. However, \nthe theory of .xed points is largely undeveloped in his setting. Our technical contributions are as follows: \n A type system which, in addition to enforcing causality (as in previous systems [14, 23]), also enables \none to enforce live\u00ad ness properties; fairness being a particularly intriguing exam\u00adple. Moreover, our \ntype system acts as a sound proof system for LTL. While previous work [14] emphasized the propositions\u00ad \nas-types component of the correspondence between LTL and FRP, the present work additionally emphasizes \nthe proof-as\u00adprograms part of the correspondence through the lens of struc\u00adtural proof theory.  A novel \noperational semantics which provides a reactive in\u00adtepretation of our programs. One can evaluate the \nresult of a program for the .rst n time steps, and in the next time step, resume evaluation for the (n \n+ 1)st result. It allows one to eval\u00aduate programs one time step at a time. Moreover, we prove type preservation \nof our operational semantics. As a consequence, our foundation is causal: future inputs do not affect \npresent re\u00adsults.   A strong normalization proof using Tait s method of saturated sets which justi.es \nthat our programs are productive total func\u00adtions. It also demonstrates that our programs satisfy liveness \nproperties derived from their types. Notably, our proof tackles the generality of interleaving .xed points, \nand offers a novel treatment of functoriality. The paper is organized as follows: To illustrate the main \nidea, limitations and power of our foundation, we give several examples in Sec. 2. In particular, we \nelaborate the implementation of two fair schedulers where our foundation statically guarantees that each \nre\u00adquest will eventually be answered. We then introduce the syntax (Sec. 3) of our calculus which features \n(co)iteration operators and explicit delay operators together with typing rules (Sec. 4). In Sec. 5 we \ndescribe the operational semantics and prove type preservation. In Sec. 6, we outline the proof of strong \nnormalization. In Sec. 7, we discuss consequences of strong normalization and type preser\u00advation, namely \ncausality, liveness, and productivity. We conclude with related work. 2. Examples To illustrate the expressive \npower of our calculus, we .rst present several motivating examples using an informal ML or Haskell-like \nsyntax. For better readability we use general recursion in our exam\u00adples, although our foundation only \nprovides (co)iteration operators. However, all the examples can be translated into our foundation straightforwardly \nand we subsequently illustrate the elaboration in Sec 3. On the type level, we employ a type constructor \n0 correspond\u00ading to the next modality of LTL to describe data available in the next time step. On the \nterm level, we use the corresponding intro\u00adduction form and elimination form let x = e in e where x \nis bound to the value of the expression e in the next time step. 2.1 The always modality Our .rst example \napp produces a stream of elements of type B, given a stream fs of functions of type A . B and a stream \nxs of elements of type A by applying the nth function in fs to the nth value in xs. Such streams are \nthought of as values which vary in time. Here we use the D type of LTL to describe temporal streams, \nwhere the nth value is available at the nth time step. D A can be de.ned in terms of the 0 modality as \nfollows using a standard de.nition as a greatest .xed point (a coinductive datatype). codata D A = _::_ \nof A \u00d7 0 D A D A has one constructor :: which is declared as an in.x op\u00aderator and takes an A now and \nrecursively a D A in the next time step. The functions hd and tl can then be de.ned, the only caveat \nbeing that the type of tl expresses that the result is only available in the next time step: hd : D A \n. A tl : D A . 0 D A Finally, we can implement the app function as follows: app : D (A . B) . D A . D \nB app fs xs = let fs = tl fs xs = tl xs in ((hd fs) (hd xs)) :: ( (app fs xs )) We use the 0 elimination \nform, let to bind the variables fs and xs to values of the remaining streams in the next time step. \nOur typing rules will guarantee that fs and xs are only usable underneath a , which we will explain further \nin the following examples. Such a program is interpreted reactively as a process which, at each time \nstep, receives a function A . B and a value A and produces a value B. More generally, given n functions \nA . B and n values A, it can compute n values B. 2.2 The eventually modality A key feature of our calculus \nis the distinction between least .xed points and greatest .xed points. In other words, the distinction \nbetween data and codata. This allows us to make a distinction between events that may eventually occur \nand events that must eventually occur. This is a feature not present in the line of work by Krishnaswami \nand his collaborators, Benton and Hoffman [22 24] they have unique (guarded) .xed points which intuitively \ncorrespond to our greatest .xed points. To illustrate the bene.ts of having both, least and greatest \n.xed points, we present here the de.nition of LTL s . operator (read: eventually ) as a data type, corresponding \nto a type of events in reactive programming: data . A = Now of A | Later of 0 . A The function evapp \nbelow receives an event of type A and a time-varying stream of functions A . B. It produces an event \nof type B. Operationally, evapp waits for the A to arrive, applies the function available at that time, \nand .res the resulting B event immediately: evapp : . A . D(A . B) . . B evapp ea fs = case ea of | Now \nx . Now ((hd fs) x) | Later ea . let ea = ea fs = tl fs in Later ( (evapp ea fs )) This is not the \nonly choice of implementation for this type. Such functions could opt to produce a B event before the \nA arrives, or even long after (if B is something concrete such as bool). However, all functions with \nthis type (in our system) have the property that given that an A is eventually provided, it must eventu\u00adally \nproduce a B, although perhaps not at the same time. This is our .rst example of a liveness property guaranteed \nby a type. This is in contrast to the weak eventually modality present in other work, which does not \nguarantee the production of an event. It is interesting to note that this program (and all the other \npro\u00adgrams we write) can rightly be considered proofs of their corre\u00adsponding statements in LTL.  2.3 \nAbstract server Here we illustrate an abstract example of a server whose type guarantees responses to \nrequests. This example is inspired by a corresponding example in Jeffrey s [16] recent work. We wish \nto write a server which responds to two kinds of requests: Get and Put with possible responses OK and \nError. We represent these: data Req = Get | Put data Resp = OK | Error At each time step, a server speci.es \nhow to behave in the future if it did not receive a request, and furthermore, if it receives a request, \nit speci.es how to respond and also how to behave in the future. This is the informal explanation for \nthe following server type, expressed as codata (here, we use coinductive record syntax): codata Server \n= {noreq : 0 Server, some : Req . Resp \u00d7 0 Server}  Now we can write the server program which responds \nto Get with OK and Put with Error: server : Server server = { noreq = server, some = .r. if isGet r \nthen (OK, server) else (Error, server)} Above, we say that if no request is made, we behave the same \nin the next time step. If some request is made, we check if it is a Get request and respond appropriately. \nIn either case, in the next time step we continue to behave the same way. More generally, we could opt \nto behave differently in the next time step by e.g. passing along a counter or some memory of previous \nrequests. It is clear that this type guarantees that every request must im\u00admediately result in a response, \nwhich Jeffrey calls a liveness guar\u00adantee. In our setting, we reserve the term liveness guarantee for \nsomething which has the traditional .avor of eventually some\u00adthing good happens . That is, they are properties \nwhich cannot be falsi.ed after any .nite amount of time, because the event may still happen. The present \nproperty of immediately providing a response does not have this .avor: it can be falsi.ed by a single \nrequest which does not immediately receive a response. In our setting, live\u00adness properties arise strictly \nfrom uses of inductive types (i.e. data, or \u00b5 types) combined with the temporal 0 modality, which re\u00adquires \nsomething to happen arbitrarily (but .nitely!) far into the future.  2.4 Causality-violating (and other \nbad) programs We wish to disallow programs such as the following, which has the effect of pulling data \nfrom the future into the present; it violates a causal interpretation of such programs. Moreover, its \ntype is certainly not a theorem of LTL for arbitrary A! predictor : 0 A . A predictor x = let x = x \nin x -- does not typecheck Our typing rules disallow this program roughly by restricting variables bound \nunder a to only be usable under a , in much the same way as Krishnaswami [22]. Similarly, the type 0(A \n+ B) expresses that either an A or a B is available in the next time step, but it is not known yet which \none it will be. Hence we disallow programs such as the following by disallowing case analysis on something \nonly available in the future: predictor : 0(A + B) . 0 A + 0 B predictor x = let x = x in case x of \n-- does not typecheck | inl a . inl ( a) | inr a . inr ( b) Such a program would tell us now whether \nwe will receive an A or a B in the future. Again this violates causality. Due to this inter\u00adpretation, \nthere is no uniform inhabitant of this type, despite being a theorem of classical LTL. Similarly, 0. \n. . is uninhabited in our system; one cannot get out of doing work today by citing the end of the world \ntomorrow. Although it would be harmless from the perspective of causality, we disallow also the following: \nimport : A . 0 A import x = x -- does not typecheck This is disallowed on the grounds that it is not \nuniformly a theo\u00adrem of LTL. Krishnaswami and Benton allow it in [23], but disallow it later in [24] \nto manage space usage. Syntactically, this is accom\u00ad plished by removing from scope all variables not \nbound under a when moving under a . However, some concrete instances of this type are inhabited, for \nexample the following program which brings natural numbers into the future: import : Nat . 0 Nat import \nZero = Zero import (Succ n) = let n = import n in (Succ n ) Our calculus does not have Nakano s guarded \nrecursion [13] fix : (0 A . A) . A because it creates for us undesirable inhabitants of certain inductive \ntypes (such inductive types collapse into coinductive types). For example, the following would be an \nundesirable inhabitant of .A in which the A is never delivered: never : . A never = fix (.x. Later x) \n-- disallowed Finally, the following program which attempts to build a con\u00adstant stream cannot be elaborated \ninto the foundational calculus. While it is guarded, we must remove all local variables from scope in \nthe bodies of recursive de.nitions, so the occurrence of x is out of scope. repeat : A . D A repeat x \n= xs where xs = x :: xs -- x out of scope ! This is intentional the above type is not uniformly a theorem \nof LTL, so one should not expect it to be inhabited. As Krishnaswami et al. [24] illustrate, this is \nprecisely the kind of program which leads to space leaks.  2.5 Fair scheduling Here we de.ne a type \nexpressing the fair interleavings of two streams, and provide examples of fair schedulers employing this \ntype this is the central example illustrating the unique power of the presented system. This is enabled \nby our system s ability to properly distinguish between and interleave least and greatest .xed points \n(i.e. data and codata). Other systems in the same vein typically collapse least .xed points into greatest \n.xed points or simply lack the expressiveness of recursive types. First we require the standard until \nmodality of LTL, written A U B. This is a sequence of As, terminated with a B. In the setting of reactive \nprogramming, Jeltsch [18] calls programs of this type processes they behave as time-varying signals \nwhich eventually terminate with a value. data A U B = Same of A \u00d7 (0 (A U B)) | Switch of B Notably, \nsince this is an inductive type, the B must eventually occur. The coinductive variant is weak until ; \nthe B might never happen, in which case the As continue forever. We remark that without temporal modalities, \nA U B is isomor\u00adphic to (List A) \u00d7 B, but because of the 0, it matters when the B happens. We de.ne also \na slightly stronger version of until which requires at least one A, which we write A U B. type A U B \n= A \u00d7 0 (A U B) We characterize the type of fair interleavings of a stream of A s and Bs as some number \nof As until some number of Bs (at least one), until an A, and the process restarts. This is fair in the \nsense that it guarantees in.nitely many As and in.nitely many Bs. As a coinductive type: codata Fair \nA B = In of (A U (B U (A \u00d7 0(Fair A B))))  a b b a start Figure 1. Scheduler automaton. The reader \nmay .nd it helpful to compare this type to the cor\u00adresponding B \u00a8uchi automaton in Figure 1. Jeltsch \nbrie.y discusses [18] essentially the same type; here we have the tools necessary to write programs with \nthis type. That is to say, we can write examples of fair schedulers which take a stream of As and a stream \nof Bs and select from them fairly. Here is the simplest fair scheduler which simply alternates selecting \nan A and B: sched : D A . D B . Fair A B sched as bs = let as = tl as bs = tl bs in In (Switch (hd bs, \n (Switch (hd as , let as = tl as  bs = tl bs in (sched as bs )))))  The reader may notice that \nthis scheduler drops the As at odd position and the Bs at even position. This could be overcome if one \nhas import for the corresponding stream types, but at the cost of a space leak. However, this dropping \nbehaviour could be viewed positively: in a reactive setting, the source of the A requests could have \nthe option to re-send the same request or even modify the previous request after observing that the scheduler \ndecided to serve a B request instead. Next we illustrate a more elaborate implementation of a fair scheduler \nwhich serves successively more As each time before serving a B. Again the type requires us to eventually \nserve a B. We will need a special notion of timed natural numbers to implement the countdown. In the \nSucc case, the predecessor is only available in the next time step: data TNat = Zero | Succ (0 TNat) \nWe can write a function which imports TNats: import : TNat . 0 TNat The scheduler is implemented in Figure \n2. It involves two mutu\u00ad ally recursive functions: cnt is structurally decreasing on a TNat, counting \ndown how many As to produce, while the recursive call to sch2 is guarded by the necessary Switches to \nguarantee produc\u00adtivity, and increments the number of As served the next time. sch2 kicks off the process \nstarting with zero As. mutual cnt : TNat . TNat . D A . D B . (A U (B U (A \u00d7 0 (Fair A B)))) cnt n m \nas bs = let m = import m  as = tl as  bs = tl bs  in case n of | Zero . Switch (hd bs, (Switch \n(hd as , let m = import m  as = tl as  bs = tl bs  in (sch2 (Succ (import m )) as bs ))) | Succ \np . let n = p in Same (hd as, ( (cnt n m as bs ))) and sch2 : TNat . D A . D B . Fair A B sch2 n as \nbs = In (cnt n n as bs) -- Main function sch2 : D A . D B . Fair A B sch2 as bs = sch2 Zero as bs Figure \n2. Fair scheduler To understand why type TNat is necessary, we note cnt is struc\u00adturally decreasing on \nits .rst argument. This requires the immediate subterm n to be available in the next step, not the current. \nThe important remark here is that these schedulers can be seen to be fair simply by virtue of typechecking \nand termination/pro\u00adductivity checking. More precisely, this is seen by elaborating the program to use \nthe (co)iteration operators available in the formal language, which we illustrate in the next section. \n3. Syntax The formal syntax for our calculus (Figure 3) includes conventional types such as product types, \nsum types and function types. Addi\u00adtionally the system has the 0A modality for values which will have \nthe type A in the next step and least and greatest .xed points, \u00b5 and . types. Our convention is to write \nthe letters A, B, C for closed types, and F, G for types which may contain free type variables. Types \nA, B, F, G ::= 1 | F \u00d7 G | F + G | A . F |0F | \u00b5X.F | . X.F | X Terms M, N ::= x | () | (M, N ) | fst \nM | snd M | | inl M | inr M | case M of inl x . N | inr y . N' | .x.M | M N | M | let x = M in N | inj \nM | iterX.F (x.M) N| out M | coitX.F (x.M) N | map (..F ) . M Contexts T, G ::= \u00b7 | G, x :A Kind Contexts \n. ::= \u00b7 | ., X : * Type substitutions . ::= \u00b7 | ., F /X Morphisms . ::= \u00b7 | ., (x.M)/X Figure 3. LTL \nSyntax Our term language is mostly standard and we only discuss the terms related to 0 modality and the \n.xed points, \u00b5 and .. M describes a term M which is available in the next time step.  let x = M in \nN allows us to use the value of M which is avail\u00adable in the next time step in the body N. Our typing \nrules will guar\u00adantee that the variable only occurs under a . Our calculus also in\u00adcludes iteration operator \niterX.F (x.M) N and coiteration operator coitX.F (x.M) N. Intuitively, the .rst argument x.M corresponds \nto the inductive invariant while N speci.es how many times to un\u00adroll the .xed point. The introduction \nform for \u00b5 types is inj M, rolling up a term M. The elimination form for . types is out M, unrolling \nM. We note that the X.F annotations on iter and coit play a key role during runtime, since the operational \nsemantics of map are de.ned using the structure of F (see Sec. 5). We do not annotate . abstractions \nwith their type because we are primarily interested in the operational behaviour of our language, and \nnot e.g. unique typing. The term map (..F ) . N witnesses that F is a functor, and is explained in more \ndetail in the next sections. Our type language with least and greatest .xed points is expres\u00adsive enough \nthat we can de.ne the always and eventual modality e.g.: DA = .X.A \u00d7 0X .A = \u00b5X.A + 0X The de.nition \nof DA expresses that when we unfold a DA, we obtain a value of type A (the head of the stream) and another \nstream in the next time step. The de.nition of .A expresses that in each time step, we either have a \nvalue of type A now or a postponed promise for a value of type A. The use of the least .xed point operator \nguarantees that the value is only postponed a .nite number of times. Using a greatest .xed point would \npermit always postponing and never providing a value of type A. We can also express the fact that a value \nof type A occurs in\u00ad.nitely often by using both great and least .xed points. Tradition\u00adally one expresses \nthis by combining the always and eventually modalities, i.e. D.A. However, there is another way to express \nthis, namely: inf A = .X.\u00b5Y.(A\u00d70X + 0Y ) In this de.nition, at each step we have the choice of making \nprogress by providing an A or postponing until later. The least .xed point implies that one can only \npostpone a .nite number of times. The two de.nitions are logically equivalent classically, but have different \nconstructive content they are not isomorphic as types. Intuitively, D.A provides, at each time step, \na handle on an A to be delivered at some point in the future. It can potentially deliver several values \nof type A in the same time step, and the order of the handles may not correspond to the order they are \ndelivered in. On the other hand, infA can provide at most one value of type A at each time step. This \ndemonstrates that the inclusion of general \u00b5 and . operators in the language (in lieu of a handful of \nmodalities) offers more .ne-grained distinctions constructively than it does classically. We show here \nalso the encoding of Server, A U B, A U B and Fair A B which we used in the examples in Sec. 2. Server \n= . X.0X \u00d7 (Req . Resp \u00d7 0X) A U B = \u00b5X.(B + A \u00d7 0X) A U B = A \u00d7 0(A U B) Fair A B = . X.(A U (B U (A \n\u00d7 0X))) Finally, to illustrate the relationship between our formal lan\u00adguage which features (co)iteration \noperators and the example pro\u00adgrams which were written using general recusion, we show here the program \napp in our foundation. Here we need to use the pair of f s and xs as the coinduction invariant: Well-formedness \nof types: . f F : * . f F : * . f G : * . f F : * . f G : * . f 1 : * . f F \u00d7 G : * . f F + G : * \u00b7 f \nA : * . f F : * ., X : * f F : * ., X : * f F : * . f A . F : * . f \u00b5X.F : * . f . X.F : * . f F : * \n(X : *) . . . f OF : * . f X : * Figure 4. Well-formed types Typing Rules for ., \u00d7, +, 1 T; G, x :A f \nM :B T; G f M :A . B T; G f N :A T; G f .x.M :A . B T; G f M N :B x:A . G T; G f M :A T; G f N :B T; \nG f x:A T; G f () : 1 T; G f (M , N ) : A \u00d7 B T; G f M :A \u00d7 B T; G f M :A \u00d7 B T; G f fst M :A T; G f \nsnd M :B T; G f M :A T; G f N :B T; G f inl M :A + B T; G f inr N :A + B T; G f M :A + B T; G, x :A f \nN1 :C T; G, y :B f N2 :C T; G f case M of inl x . N1 | inr y . N2 :C Rules for O modality and least and \ngreatest .xed points \u00b7; T f M :A T; G f M :OA T, x :A; G f N :C T; G f M :OA T; G f let x = M in N :C \nT; G f M : [\u00b5X.F /X ]F \u00b7; x: [C/X ]F f M :C T; G f N :\u00b5X.F T; G finj M :\u00b5X.F T; G fiterX.F (x.M) N:C \n\u00b7; x:C f M : [C/X ]F T; G f N :C T; G f M :. X.F T; G fcoitX.F (x.M) N:. X.F T; G fout M : [. X.F /X \n]F . f F : * T; G f M : [.1]F .1 f . : .2 T; G f map (..F ) . M : [.2]F Typing Rules for morphisms: .1 \nf . : .2 .1 f . :.2 \u00b7; x : A f M : B .1, A/X f ., (x.M)/X : .2, B/X \u00b7 f \u00b7 : \u00b7 Figure 5. Typing Rules \napp : D(A . B) . DA . DB app f s xs = coitX.B\u00d7QX (x. let f s ' = tl (fst x) in ' let xs = tl (snd \nx) in ( (hd (fst x)) (hd (snd x)) , (f s ' , xs ' ) ) ) (f s, xs) 4. Type System We de.ne well-formed \ntypes in Fig. 4. In particular, we note that free type variables cannot occur to the left of a .. That \nis to say, we employ a strict positivity restriction, in contrast to Krishnaswami s guardedness condition \n[22]. We give a type assignment system for our calculus in Fig. 5 where we distinguish between the context \nT which provides types for variables which will become available in the next time step (i.e.  when going \nunder a ) and the context G which provides types for the variables available at the current time. The \nmain typing judgment, T; G f M : A asserts that M has type A given the context T and G. In general, our \ntype system is similar to that of Krishnaswami and collaborators. While in their work, the validity of \nassumptions at a given time step is indicated either by annotating types with a time [23] or by using \ndifferent judgments (i.e. now, later, stable) [22], we separate assumptions which are valid currently \nfrom the assumptions which are valid in the next time step via two different contexts. Much more importantly, \nour foundation differs in the treatment of recursive types and recursion. Most rules are standard. When \ntraversing .x.M, the variable is added to the context G describing the fact that x is available in the \ncurrent time step. Similarly, variables in each of the branches of the case-expression, are added to \nG. The interesting rules are the ones for 0 modality and .xed points \u00b5 and .. In the rule IQ, the introduction \nrule for 0 with corresponding constructor , provides a term to be evaluated in the next time step. It \nis then permitted to use the variables promised in the next time step, so the assumptions in T move to \nthe available position, while the assumptions in G are no longer available. In fact, the variables in \nT remain inaccessible until going under a  this is how causality is enforced. Our motivation for forgetting \nabout current assumptions G and not allowing them to persist is that we wish to obtain a type system \nwhich is sound for LTL. Allowing G to persist would make the unsound A . 0A easily derivable. In the \nwork of Krishnaswami et. al [24], forgetting about current assumptions plays a key role in managing space \nusage. The corresponding elimination form let x = M in N simply adds x:A, the type of values promised \nby M, to the context T and we continue to check the body N allowing it to refer to x under a . The elimination \nform for \u00b5 is an iteration operator iter. Exam\u00adining an instantiation of this rule for Nat = \u00b5X.1 + X \nmay help to clarify this rule: \u00b7 ; x : 1 + C f M : C T; G f N : \u00b5X.1 + X T; G fiterX.1+X (x.M) N: C The \nterm M contains the base case and the step case (M will typically be a case analysis on x), while N speci.es \nhow many times to iterate the step case of M starting at the base case of M. For example, we illustrate \nhere the doubling function on natural numbers written using this iteration operator. We write 0 for the \nterm inj (inl()) and suc m for the term inj (inr m). db = iterX.1+X (x.case x of inl y . 0 | inr w . \nsuc(suc w)) double = .x:Nat.db x We note that dropping T and G in the .rst premise is essential. Intuitively, \nthe bodies of recursive de.nitions need to be used at arbitrary times, while G and T are only available \nfor the current and next time steps, respectively. This is easiest to see for the coit rule, where keeping \nG allows a straightforward derivation of A . DA (repeat), which is unsound for LTL and easily produces \nspace leaks. Similarly, keeping G in the iter rule allows one to derive the unsound A . .B . A U B which \nsays that if A holds now, and B eventually holds, then A in fact holds until B holds. unsound : A . .B \n. A U B unsound = .a. .eb. iterX.B+QX (y. case y of |inl b . inj (inl b) |inr u . inj (inr (a, u)) a \nout of scope! ) eb The typing rules for \u00b5 and . come directly from the universal properties of initial \nalgebras and terminal coalgebras. The reader may .nd it clarifying to compare these rules to the universal \nprop\u00aderties, depicted here: F (iterf) coitf F (\u00b5F )  F (C) C . F outinj f f F (coitf) \u00b5F iter f \n C F (C) F (.F ) Unsurprisingly, the primitive recursion operator below can be derived from our iteration \noperator, so we do not lose expressive\u00adness by providing only iteration. Of course, in practice rec is \nnec\u00adessary to write ef.cient programs. \u00b7; x: [C \u00d7 \u00b5X.F /X ]F f M : C T; G f N : \u00b5X.F T; G f recX.F (x.M) \nN : C The term map (..F ) . M witnesses that F is a functor. It has the effect of applying the transformations \nspeci.ed by . at the positions in M speci.ed by F . It is a generic program de.ned on the structure of \nF . While this term is de.nable at the meta-level using the other terms in the language, we opt to include \nit in our syntax, because doing so signi.cantly simpli.es the operational semantics and proof of normalization. \nIn this term, . binds the free variables of F . It is illustrative to consider the case where F (Y ) \n= list Y = \u00b5X.1 + Y \u00d7 X. If y : A f M : B and N : list A, then map (Y. list Y ) ((y. M )/Y ) N : list \nB. In this case, map implements the standard notion of map on lists! We will sometimes not write the \n. when the intention is clear. We de.ne two notions of substitution: substitution for a cur\u00adrent variable, \nwritten [N/x]M, and substitution for a next vari\u00adable, written [N/x] M. The key case in their de.nition \nis the case for M. For current substitution [N/x]( M), in well-scoped terms, x cannot occur in M, so \nwe de.ne: [N/x]( M) = M For next substitution [N/x] ( M), in the body of M, x be\u00adcomes a current variable, \nso we can defer to current substitution, de.ning: [N/x] ( M) = ([N/x]M) These de.nitions are motivated \nby the desire to obtain tight bounds on how substitution interacts with the operational semantics without \nhaving to keep typing information to know that terms are well-scoped. These substitutions satisfy the \nfollowing typing lemma. Lemma 1. Substitution Typing 1. If T; G, x:A f M : B and T; G f N : A then T; \nG f [N/x]M : B  2. If T, x:A; G f M : B and \u00b7; T f N : A then T; G f [N/x] M : B  5. Operational Semantics \nNext, we de.ne a small-step operational semantics using evaluation contexts. Since we allow full reductions, \na redex can be under a binder and in particular may occur under a 0 modality. We de.ne evaluation contexts \nin Fig. 6. We index an evaluation context Ek by a depth k which indicates how many 0 modalities we traverse. \nIntuitively, the depth k tells us how far we have stepped in time \u00ador to put it differently, at our current \ntime, we know that we have  Ek+1 ::= Ek Ek ::= Ek N | M Ek | fst Ek | snd Ek | (Ek, M ) | (M, Ek) ' \n| .x.Ek | case Ek of inl x . N | inr y . N | inl Ek | case M of inl x . Ek | inr y . N | inr Ek | case \nM of inl x . N | inr y . Ek | let x = Ek in N | let x = M in Ek | inj Ek | iterX.F (x.M) Ek | coitX.F \n(x.M) Ek | out Ek | map (..F ) . Ek E0 ::= [] Figure 6. Evaluation contexts (.x.M) N -. [N/x]M fst(M, \nN ) -. M snd(M, N ) -. N case (inl M) of inl x . N1 | inr y . N2 -. [M/x]N1 case (inr M ) of inl x . \nN1 | inr y . N2 -. [M/y]N2 let x = M in N -. [M/x] N iterX.F (x.M) (inj N) -. [(map (X.F ) ((y. iterX.F \n(x.M) y)/X) N)/x]M out (coitX.F (x.M) N) -. map (X.F ) ((y. coitX.F (x.M) y)/X) ([N/x]M) Figure 7. Operational \nSemantics taken k time steps and therefore terms under k 0 modalities are available to us now. Single \nstep reduction is de.ned on evaluation contexts at a time a < . + 1 (i.e. either a . Nor a = .) and states \nthat we can step Ek[M] to Ek[N ] where M is a redex occurring at depth k where k = a and M reduces to \nN. More precisely, the reduction rule takes the following form: M -. N if k = a Ek[M] a Ek[N] If a = \n0, then we are evaluating all redexes now and do not evaluate terms under a 0 modality. If we advance \nto a = 1, we in addition can contract all redexes at depth 1, i.e. terms occurring under one , and so \non. At a = ., we are contracting all redexes under any number of . We have reached a normal form at time \na if for all k = a all redexes at depth k have been reduced and no further reduction is possible. The \ncontraction rules for redexes (see Fig. 7) are mostly straightforward -the only exceptions are the iteration \nand coit\u00aderation rules. If we make an observation about a corecursive value by (out (coitX.F (x.M) N)), \nthen we need to compute one obser\u00advation of the resulting object using M, and explain how to make more \nobservations at the recursive positions speci.ed by F . Du\u00adally, if we unroll an iteration (iterX.F (x.M) \n(inj N )), we need to continue performing the iteration at the recursive positions of N (the positions \nare speci.ed by F ), and reduce the result using M. Performing an operation at the positions speci.ed \nby F is ac\u00adcomplished with map, which witnesses that F is a functor. The op\u00aderational semantics of map \nare presented in Fig. 8. They are driven by the type F . Most cases are straightforward and more or less \nforced by the typing rules. The key cases, and the reason for putting map in the syntax in the .rst place, \nare those for \u00b5 and .. For \u00b5, we reduce N until it is of the form inj N. At which point, we can continue \napplying . inside N, where now at the recursive positions speci.ed by Y we need to continue recursively \napplying map. The case for . is similar, except it is triggered when we demand an ob\u00adservation with out. \nWe remark that we do not perform reductions inside the bodies of iter, coit, and map (see Fig. 6), as \nthese are in some sense timeless terms (they will be used at multiple points in time), and it is not \nclear how our explictly timed notion of operational semantics could interact with these. We sidestep \nthe issue by disallowing reductions inside these bodies. To illustrate the operational semantics and \nthe use of map, we consider an example of a simple recursive program: doubling a natural number. Example \nWe revisit here the program double which multiplies a given natural number by two given in the previous \nsection. Recall the following abbreviations for natural numbers: 0 =inj (inl()), suc w =inj (inr w), \n1 = suc 0, etc. Let us .rst compute double 0. double 0 -. db (inj (inl ()) -. case M0 of inl y . 0 | \ninr w . suc (suc w) where M0 = map (1 + X) (y.db y/X ) (inl ()) -. * case (inl()) of inl v . inl() | \ninr u . inr(db u) -. * case (inl ()) of inl y . 0 | inr w . suc (suc w) -. 0 We now compute double 1 \ndouble 1 -. db (inj (inr 0)) -. case M1 of inl y . 0 | inr w . suc (suc w) where M1 = map (1 + X) (y.db \ny/X ) (inr 0) -. * case (inr 0) of inl v . inl() | inr u . inr(db u) -. * case (inr(db 0)) of inl y . \n0 | inr w . suc (suc w) -. * case (inr 0) of inl y . 0 | inr w . suc (suc w) -. suc (suc 0) = 2 We have \nthe following type soundness result for our operational semantics: Theorem 2 (Type Preservation). For \nany a, if M a N and T; G f M : A then T; G f N : A Observe that after evaluating M * N, where N is in \nnormal ' form, one can then resume evaluation n N * n+1 N to obtain the cumulative result available at \nthe next time step. One may view this as restarting the computation, aiming to compute the result up \nto n + 1, but with the results up to time n memoized. In practical implementations, one is typically \nonly concerned with 0 (see Sec. 7), however considering the general a gives us the tools to analyze programs \nfrom a more global viewpoint, which is important for liveness guarantees. Our de.nition of substitution \nis arranged so that we can prove the following bounds on how substitution interacts with , which are \nimportant in our proof of strong normalization. Notice that these are independent of any typing assumptions. \nProposition 3. 1. If N a N ' then [N/x]M * ' /x]M a [N 2. If M a M ' then [N/x]M a [N/x]M ' 3. If N \nn N ' then [N/x] M * ' /x] M  n+1 [N 4. If N . N ' then [N/x] M * ' /x] M . [N 5. If M a M ' then [N/x] \nM a [N/x] M ' Our central result is a proof of strong normalization for our calculus which we prove in \nthe next section.  map (.. X ) . N -. [N/x]M if .(X) = x.M map (.. 1) . N -. () map (.. F \u00d7 G) . N -. \n(map (.. F ) . (fst N), map (.. G) . (snd N)) map (.. F + G) . N -. case N of inl y . inl(map (.. F \n) . y) | inr z . inr(map (.. G) . z) map (.. A . F ) . N -. .y.map (.. F ) . (N y) map (.. 0 F ) . N \n-. let y = N in map (.. F ) . y map (.. \u00b5Y.F ) . (inj N) -. inj (map (., Y . F ) (., (x. map (.. \u00b5Y \n.F ) . x)/Y ) N) out (map (.. .Y.F ) . N) -. map (., Y. F ) (., (x. map (.. . Y .F ) . x)/Y ) (out N) \nFigure 8. Operational semantics of map 6. Strong Normalization In this section we give a proof of strong \nnormalization for our calculus using the Girard-Tait reducibility method [11, 12, 34]. In our setting, \nthis means we prove that for any a = ., every reduction sequence M a ... is .nite, for well-typed terms \nM. This means that one can compute the approximate value of M up to time a. In fact, this result actually \ngives us more: our programs are suitably causal using normalization at 0, and the type of a program gives \nrise to a liveness property which it satis.es, using normalization at .. Our logical relation is in fact \na degenerate form of Kripke logical relation, as we index it by an a = . to keep track of how many time \nsteps we are normalizing. It is degenerate in that the partial order we use on . + 1 is discrete, i.e. \na = \u00df precisely when a = \u00df. It is not step-indexed because our strict positivity condition on \u00b5 and . \nallows us to interpret them without the aid of step indexing. This proof is made challenging by the presence \nof interleaving \u00b5 and . types and (co)iteration operators. This has been treated before by Matthes [27] \nas well as Abel and Altenkirch [1]. Oth\u00ad ers have considered non-interleaving cases or with other forms \nof recursion, such as Mendler [28] and Jouannaud and Okada [19]. To our knowledge, ours is the .rst such \nproof which treats map as primitive syntax with reduction rules instead of a derivable opera\u00adtion, which \nwe .nd simpli.es the proof substantially as it allows us to consider map independently of iter and coit \nin the proof. We use a standard inductive characterization of strongly nor\u00admalizing terms: De.nition \n4 (Strongly normalizing). We de.ne sn as the inductive closure of: ' '' .M ,M a M =. M . sna M . sna \nWe say M is strongly normalizing at a if M . sna. It is immediate from this de.nition that if M . sna \nand M a M ' then M ' . sna. Since this is an inductive de.nition, it affords us a corresponding induction \nprinciple: To show that a property P holds of a term M . sna, one is allowed to assume that P holds for \nall M ' such that M a M '. One can easily verify by induction that if M . sna then there are no in.nite \na reduction sequences rooted at M. For our proof, we use Tait s saturated sets instead of Girard s reducibility \ncandidates, as this allows us to perform the syntactic analysis of redexes separate from the more semantic \nparts of the proof. This technique has been used by Luo [26], Altenkirch [2], and Matthes [27]. In the \nfollowing, we will speak of indexed sets, by which we mean a subset of terms for each a = ., i.e. A : \n. + 1 . P (tm), where we write tm for the set of all terms. We overload the notation ., n, and . to mean \npointwise inclusion, intersection, and union. That is, if A and B are indexed sets, we will write A . \nB to mean Aa . Ba for all a. De.nition 5. We de.ne the following next step operator on indexed sets A \n: . + 1 . P (tm): (. A)0 = tm (. A)m+1 Am = (. A). = A. The motivation for this is that it explains what \nhappens when we go under a  we are now interested in reducibility under one fewer . Below, we de.ne \na notion of normalizing weak head reduction. We write M a M ' for a contraction, and M _a N for a con\u00adtraction \noccuring under a weak head context. This is a weak head reduction where every term which may be lost \nalong the way (e.g. by a vacuous substitution) is required to be strongly normalizing. This is a characteristic \ningredient of the saturated set method. It is designed this way so as to (backward) preserve strong normal\u00adization. \nThe intuition is that weak head redexes are unavoidable reducing other redexes can only postpone a weak \nhead reduction, not eliminate it. We de.ne below weak head reduction contexts and normalizing weak head \nreduction. We show only the lambda case. The full de.nition can be found in the long version of the paper. \nH ::= [] | fst H | snd H | HN | iterX.F (x.M) H | map (.. \u00b5X.F ) . H | out H | let x = H in N | case \nH of inl x . N1 | inr y . N2 M a M ' N . sna H[M] _a H[M ' ] (.x.M) N a [N/x]M Lemma 6. sna is backward \nclosed under normalizing weak head reduction: If M ' . sna and M _a M ' then M . sna We note that in \nall of our proofs, the subscript a plays little to no role, except of course in the cases pertaining \nto the next step operator 0. For this reason, we typically highlight the 0 cases of the proofs. We de.ne \nthe indexed set of terms sne ( strongly normalizing neutral ) which are strongly normalizing, but are \nstuck with a variable in place of a weak head redex. De.nition 7. We de.ne snea = {H[x] . sna} We can \nnow de.ne saturated sets as subsets of strongly normal\u00adizing terms. The rest of the proof proceeds by \nshowing that our types can be interpreted as saturated sets, well-typed terms inhabit saturated sets, \nand hence are strongly normalizing.  De.nition 8 (Saturated sets). An indexed set A : . + 1 . P (tm) \nis saturated if 1. A . sn 2. For any a, M, M ', if M a * M ' and M ' . Aa then M . Aa  _ (Backward \nclosure under normalizing weak head reduction) 3. sne . A It is immediate from Lemma 6 that sn is saturated. \nDe.nition 9. For an indexed set A : . + 1 . P (tm), we de.ne A as its closure under conditions 2 and \n3. i.e. '*'' ' A a = {M|.M .M a M . (M . Aa . M . snea)} _ To interpret least and greatest .xed points, \nwe construct a com\u00adplete lattice structure on saturated sets: Lemma 10. Saturated sets form a complete \nlattice under ., with greatest lower bounds and least upper bounds given by:  S = (S) n snS= S For ., \nwe intersect with sn so that the nullary lower bound is w d sn, and hence saturated. For non-empty S, \nwe have S = S. As a consequence, by an instance of the Knaster-Tarski .xed point theorem, we have the \nfollowing: Corollary 11. Given F which takes predicates to predicates, we de.ne: w \u00b5F = 1{C saturated \n| F (C) . C} .F = {C saturated | C . F (C)} If F is monotone and takes saturated sets to saturated sets, \nthen \u00b5F (resp. . F ) is a least (resp. greatest) .xed point of F in the lattice of saturated sets The \nfollowing operator de.nitions are convenient, as they allow us to reason at a high level of abstraction \nwithout having to intro\u00adduce a at several places in the proof. De.nition 12. We de.ne: (A . f)a = {M|f \nM . Aa} (A * f )a = {f M |M . Aa} We will often use these notations for partially applied syntactic forms, \ne.g. A* inj We are now in a position to de.ne the operators on saturated sets which correspond to the \noperators in our type language. De.nition 13. We de.ne the following operations on saturated sets: 1 \n= sn (A \u00d7 B) = (A . fst) n (B . snd) (A + B) = (A * inl ) . (B * inr ) (A . B)a = {M |.N . Aa .(M N ) \n. Ba} (OA) = (. A) * \u00b5F = \u00b5(X . F(X )* inj) .F = .(X .F (X ). out) Notice that \u00b5F is de.ned regardless \nof whether F is monotone, although we only know that \u00b5F is actually a least .xed point when F is monotone. \nWe remark also that our . de.nition does not resemble the Kripke-style de.nition one might expect. That \nis to say, we are using the discrete partial order on . + 1. We do not need the monotonicity that the \nstandard ordering would grant us, since our type system does not in general allow carrying data into \nthe future. Lemma 14. The operators de.ned in De.nition 13 take saturated sets to saturated sets. We \nare now ready to interpret well-formed types as saturated sets. The de.nition is unsurprising, given \nthe operators de.ned previously. De.nition 15. Given ., an environment mapping the free variables of \nF to saturated sets, we de.ne the interpretation [F ](.) of an open type as a saturated set as follows: \n[X](.) = .(X) [1](.) = 1 = [F \u00d7 G](.) [F ](.) \u00d7 [G](.) = [F + G](.) [F ](.) + [G](.) = [A . F ](.) [A](\u00b7) \n. [F ](.) = [\u00b5X.F ](.) = \u00b5(X . [F ](., X /X)) [. X.F ](.) = .(X . [F ](., X /X)) [0F ](.) O[F ](.) Observe \nthat, by lemma 14, every [F ](.) is saturated. If . = N1/y1, ..., Nn/yn and s = M1/x1, ..., Mm/xm are \nsimultaneous substitutions, we write [.; s]M to mean substituting the Ni with next substitution [Ni/yi] \n(-) and the Mi with the current substitution [Mi/xi](-). We may write this explicitly as follows: [N1/y1 \n, ..., Nn/yn; M1/x1, ..., Mm/xm]M If s = M1/x1, ..., Mm/xm and G = x1 : A1, ..., xm : Am, then we write \ns . Ga to mean Mi : Aa i for all i, and similarly for . . Ta . De.nition 16 (Semantic typing). We write \nT; G F M : C, where the free variables of M are bound by T and G, if for any a and any substitutions \n. . (. T)a and s . Ga, we have [.; s]M . Ca Next we show that the term constructors of our language obey \ncorresponding semantic typing lemmas. We prove the easier cases .rst (i.e. constructors other than map, \niter, and coit), as their results are used in the next lemma pertaining to map. We state only the non-standard \ncases. For the full statement, see the long version. Lemma 17 (Interpretation of term constructors). \nThe following hold, where we assume T, G, A, B, and C are saturated. 1. If \u00b7; T F M : A then T; G F M \n: OA 2. If T; G F M : OA and T, x:A; G F N : B then T; G F let x = M in N : B 3. If F is a monotone \nfunction from saturated sets to saturated sets, and T; G F M : F(\u00b5F) then T; G Finj M : \u00b5F 4. If F is \na monotone function from saturated sets to saturated sets, and T; G F M : .F then T; G Fout M : F(.F) \n Proof. We show only the case for . The rest are straightforward or standard. 1. We are given \u00b7; T F \nM : A, i.e. for any a and . . Ta, that [\u00b7; .]M . Aa . Suppose we are given a and . . (. T)a and s . Ga \n. Case a = 0: Then (. A)0 = tm, so [\u00b7; .]M . (. A)0 Case a = m + 1: Then (. A)m+1 = Am, and . . Tm. By \nour assumption, taking a = m, we have [\u00b7; .]M . Am Case a = .: Then (. A). = A., and . . T.. By our assumption, \n[\u00b7; .]M . A. In any case, we have [\u00b7; .]M . (. A)a . Then [.; s]( M) = ([\u00b7; .]M) . (. A * )a Hence [.; \ns]( M) . (0A)a as required. With this lemma, it remains only to handle map, iter, and coit. Below we \nshow the semantic typing lemma for map. We write .1 F . : .2 where . = (x.M1/X1, ..., x.Mm/Xm) and .1 \n= A1/X1, ..., Am/Xm and .2 = B1/X1, ..., Bm/Xm and the Ai and Bi are saturated to mean x : Ai F Mi : \nBi for each i. Notably, because we de.ne map independently of iter and coit, we can prove this directly \nbefore tackling iter and coit, offering a simpli.cation of known proofs for interleaving \u00b5.. For readability, \nwe write F instead of (..F ).  Lemma 18 (Semantic typing for map). If .1 F . : .2 then x : [F ](.1) \nF map F . x : [F ](.2) Proof. Notice by unrolling the de.nitions, this is equivalent to showing [F ](.1) \n. [F ](.2) . (map F .). The proof proceeds by induction on F . We show only the case for \u00b5. The case \nfor . is analogous. The rest are straightforward using Lemma 17 and backward closure. We present the \nproof in more detail in the long version. Case \u00b5X.F : This proceeds primarily using the least .xed point \nproperty and closure. _ Let C = [\u00b5X.F ](.2) = \u00b5(X . [F ](.2, X )* inj) Let D = C . (map(\u00b5X.F ).) Let \nN = (map F (., map (\u00b5X.F ) .)) 1. D is saturated 2. x : D F map (\u00b5X.F ) . x : C (by de.nition of F, \nD) 3. x : [F ](.1, D) F map F (., map (\u00b5X.F ) .) x : [F ](.2, C)  (by I.H.) 4. [F ](.1, D) . [F ](.2, \nC) . (map F (., map (\u00b5X.F ) .)) (by de.nition of F) We call a term M a a-value if it cannot step further \nat time a. We may write this M a. We obtain as a consequence of strong normalization that there is no \nclosed inhabitant of the type . (which we de.ne as \u00b5X.X), since there is no closed a-value of this type. \nPerhaps surprisingly, we can also show there is no closed inhabitant of 0. . .. For if there was some \nf f : 0. . ., we could evaluate x : .; \u00b7 f f( x) : . at 0 to obtain a 0-value x : .; \u00b7 f v : ., which \nby inspection of the possible 0-values, cannot exist! Similarly, we can demonstrate an interesting property \nof the type B = \u00b5X.0X. First, there is no closed term of type B . .. For if there was some f : B . ., \nwe could normalize x : B; \u00b7 f f(inj x) : B at time 0 to obtain a 0-value x : B; \u00b7 f v : ., which cannot \nexist. Moreover, there is no closed term of type B, since there is no closed .-value of type B. That \nis, B is neither inhabited nor provably uninhabited inside the logic. To show how our operational semantics \nand strong normaliza\u00adtion give rise to a causal (reactive) interpretation of programs, as well as an \nexplanation of the liveness properties guaranteed by the types, we illustrate here the reactive execution \nof an example pro\u00adgram x0 : .P f M0 : .Q. Such a program can be thought of as waiting for a P event from \nits environment, and at some point deliv\u00adering a Q event. For simplicity, we assume that P and Q are \npure (non-temporal) types such as Bool or N. We consider sequences of interaction which begin as follows, \nwhere we write now p for inj (inl p) and later t for inj (inr( t)). . F (.2, C) * inj . inj . N [later \n x1/x0]M0 * 0 later M1 . [ F ] (.2, C) * inj . inj . N (by mon. of .) [later x2/x1]M1 = C . inj . \nN (by rolling .xed point) = C . (inj(map F (., map (\u00b5X.F ) .)-)) (by def) . C . (map (\u00b5X.F ) . (inj -)) \n(by closure) * 0 later M2 . . . Each such step of an interaction corresponds to the environment _ = \nC . (map (\u00b5X.F ) .). inj = D. inj (by def) 5. F (.1, D) * inj . D (by adjunction *) 6. [ F ] (.1, D) \n* inj . D (by adjunction (-)) 7. [\u00b5X.F ](.1) . D (by lfp property)  8. y : [\u00b5X.F ](.1) F map (\u00b5X.F \n) . y : [\u00b5X.F ](.2) (by de.nitions of F, D, C) The only term constructors remaining to consider are iter \nand coit. These are the subject of the next two lemmas, which are telling the program that it does not \nyet have the P event in that time step, and the program responding saying that it has not yet produced \na Q event. Essentially, at each stage, we leave a hole xi standing for input not yet known at this stage, \nwhich we will re.ne further in the next time step. The resulting Mi acts as a continuation, specifying \nwhat to compute in the next time step. We note that each xi:.P f Mi : .Q by type preservation. Such a \nsequence may not end, if the environment defers providing a P forever. However, it may end one of two \nways. The .rst is if eventually the environment supplies a closed value p of type P : proven similarly \nto the \u00b5 and . cases of the map lemma. Namely, * [now p/xi+1]Mi+1 . v . they proceed primarily by using \nthe least (greatest) .xed point properties and backward closure under , appealing to the map _ lemma. \nThe proofs can be found in the long version of the paper. Lemma 19. If x : [F ](C/X) F M : C where C \nis saturated, we have y : [\u00b5X.F ] FiterX.F (x.M) y: C Lemma 20. If x : C F M : [F ](C/X) where C is saturated, \nwe have y : C FcoitX.F (x.M) y: [.X.F ] By now we have shown that all of the term constructors can be \ninterpreted, and hence the fundamental theorem is simply an induc- In this case, f [now p/xi+1]Mi+1 : \n.Q. By type preservation and strong normalization, we can evaluate this completely to f v : .Q. By an \ninspection of the closed .-values, v must be of the form later( later( \u00b7 \u00b7 \u00b7 (now q))). That is, a Q \nis eventually delivered in this case. The second way such an interaction sequence may end is if the program \nproduces a result before the environment has supplied a P : . . . * tion on the typing derivation, appealing \nto the previous lemmas. Theorem 21 (Fundamental theorem). If T; G f M : A, then we have [T]; [G] F M \n: [A] Corollary 22. If T; G f M : A then for any a, M is strongly normalizing at a. 7. Causality, Productivity \nand Liveness We discuss here some of the consequences of type soundness and strong normalization and \nexplain how our operational semantics enables one to execute programs reactively. [later xi+1/xi]Mi \nnow q 0 We remark that type preservation of 0 provides an explanation of causality: since xi+1 : .P ; \n\u00b7 f [later xi+1/xi]Mi : .Q, if evaluating the term [later xi+1/xi]Mi with 0 produces a 0\u00advalue v, then \nxi+1 : .P ; \u00b7 f v : .Q and by an inspection of the 0-values of this type, we see that v must be of the \nform later Mi+1 or now p since the variable xi+1 is in the next context, it cannot interfere with the \npart of the value in the present, which means the present component cannot be stuck on xi+1. That is, \nxi+1 could only possibly occur in Mi+1. This illustrates that future inputs do not affect present output \n this is precisely what we mean by causality.  We also remark that strong normalization of 0 guarantees \nre\u00adactive productivity. That is, the evaluation of [later xi+1/xi]Mi is guaranteed to terminate at some \n0-value v by strong normalization. As an aside, we note that if we were to use a time-indexed type system \nsuch as that of Krishnaswami and Benton [23], one could generalize this kind of argument to reduction \nat n, and normalize terms using n to obtain n-value where x can only occur at time step n + 1. This gives \na more global perspective of reactive execu\u00adtion. However, we use our form of the type system because \nwe .nd it corresponds better in practice to how one thinks about reactive programs (one step at a time). \nFinally, strong normalization of . provides an explanation of liveness. When evaluating a closed term \nf M : .Q at ., we arrive at a closed .-value v : .Q. By inspection of the normal forms, such a value \nmust provide a result after only .nitely many delays. This is to say that when running programs reactively, \nthe environment may choose not to satisfy the prequisite liveness requirement (e.g. it may never supply \na P event). In which case, the output of the program cannot reasonably be expected to guarantee its liveness \nproperty, since it may be waiting for an input event which never comes. However, we have the conditional \nresult that if the environment satis.es its liveness requirement (e.g. eventually it delivers a P event) \nthen the result of the interaction satis.es its liveness property (e.g. eventually the program will .re \na Q event). 8. Related Work Most closely related to our work is the line of work by Krish\u00adnaswami and \nhis collaborators [22 24]. Our type systems are simi\u00ad lar; in particular our treatment of the 0 modality \nand causality. The key distinction lies in the treatment of recursion and .xed points. Krishnaswami et \nal. employ a Nakano-style [13] guarded recursion rule which allows some productive programs to be written \nmore conveniently than with our (co)iteration operators. However, their recursion rule has the effect \nof collapsing least .xed points into greatest .xed points. Both type systems can be seen as proofs-as\u00adprograms \ninterpretations of temporal logics; ours has the advantage that it is capable of expressing liveness \nguarantees (and hence it re\u00adtains a tighter relationship to LTL). In their recent work [22, 24], they \nobtain also promising results about space usage, which we have so far ignored in our foundation. In his \nmost recent work, Krishnaswami [22] describes an operational semantics with better sharing behaviour \nthan ours. Another key distinction between the two lines of work is that where we restrict to .xed points \nof strictly positive functors, Krish\u00adnaswami restricts to guarded .xed points type variables always \noccur under 0; even negative occurrences are permitted. In con\u00adtrast, our approach allows a uni.ed treatment \nof typical pure recur\u00adsive datatypes (e.g. list) and temporal types (e.g. DA), as well as more exotic \nmixed types such as . X.X + 0X. Krishnaswami observes that allowing negative, guarded occurrences enables \nthe de.nition of guarded recursion. As a consequence, this triggers a collapse of (guarded) \u00b5 and ., \nso negative occurrences appear in\u00adcompatible with our goals. Also related is the work of Jeffrey [14, \n16] and Jeltsch [17], who .rst observed that LTL propositions correspond to FRP types. Both consider \nalso continuous time, while we focus on discrete time. Here we provide a missing piece of this correspondence: \nthe proofs-as-programs component. Jeffrey writes programs with a set of causal combinators, in contrast \nto our ML-like foundation. His systems lack general (co)recursive datatypes and as a result, one cannot \nwrite practical programs which enforce some liveness properties such as fairness as we do here. In his \nrecent work [16], he illustrates what he calls a liveness guarantee. The key distinction is that our \nliveness guarantees talk about some point in the future, while Jeffrey only illustrates guarantees about \nspeci.c points in time. We illustrated in Sec. 2 that our type system can also provide similar guarantees. \nWe claim that our notion of liveness retains a tighter correspondence to the concept of liveness as it \nis de.ned in the temporal logic world. Jeltsch [17, 18] studies denotational settings for reactive pro\u00ad \ngramming, some of which are capable of expressing liveness guar\u00adantees. He obtains the result that his \nConcrete Process Categories (CPCs) can express liveness when the underlying time domain has a greatest \nelement. He does not describe a language for program\u00adming in CPCs, and hence it is not clear how to write \nprograms which enforce liveness properties in CPCs, unlike our work. He discusses also least and greatest \n.xed points, but does not mention the interleaving case in generality, nor does he treat their existence \nor metatheory as we do here. His CPCs may provide a promising denotational semantics for our calculus. \nThe classical linear \u00b5-calculus (often called . T L) forms the inspiration for our calculus. The study \nof (classical) . T L proof systems goes back at least to Lichtenstein [25]. Our work offers a constructive, \ntype-theoretic view on . T L. Kojima and Igarashi [20] also investigate proof systems for a constructive \nvariant of LTL with many of the same properties as ours (e.g. the non-provability of 0. . .). They do \nnot investigate least/greatest .xed points or modalities other than 0. Synchronous data.ow languages, \nsuch as Esterel [3], Lustre [6], and Lucid Synchrone [32] are also related. Our work contributes to a \nlogical understanding of synchronous data.ow, and in particular our work could possibly be seen as providing \na liveness-aware type system for such languages. Birkedal et al. [5] also use a logical relation indexed \nby an ordi\u00ad nal larger than ., in particular to model countable non-determinism (which arises when modeling \nfairness in concurrent settings). Our . + 1 is a much smaller ordinal than they require, owing to the \nfact that we are not concerned with non-determinism in the present work. Severi and de Vries [33] prove \nnormalization results for pure type systems augmented with a later modality. Their systems have Nakano \nstyle guarded recursion and streams as their only in\u00adstance of (co)inductive types. For this reason, \nthe connection with liveness is absent from their work. 9. Conclusion We have presented a type-theoretic \nfoundation for reactive pro\u00adgramming inspired by a constructive interpretation of linear tempo\u00adral logic, \nextended with least and greatest .xed point type construc\u00adtors, in the spirit of the modal \u00b5-calculus. \nThe distinction of least and greatest .xed points allows us to distinguish between events which may eventually \noccur or must eventually occur. Our type sys\u00adtem acts as a sound proof system for LTL, and hence expands \non the Curry-Howard interpretation of LTL. We prove also strong normal\u00adization, which, together with \ntype soundness, guarantees causality, productivity, and liveness of our programs. 10. Future Work Our \nwork provides a foundational basis for exploring expressive temporal logics as type systems for FRP. \nFrom a practical stand\u00adpoint, however, our calculus is lacking a few key features. For ex\u00adample, the \nimport functions we wrote in Section 2 unnecessarily traverse the entire term. From a foundational perspective, \nit is in\u00adteresting to notice that they can be implemented at all. However, in practice, one would like \nto employ a device such as Krishnaswami s stability [22] to allow constant-time import for such types. \nWe be\u00ad lieve that our system can provide a solid foundation for exploring such features in the presence \nof liveness guarantees. It would also be useful to explore more general forms of re\u00adcursion with syntactic \nor typing restrictions such as sized types to guarantee productivity and termination/liveness, which \nwould al\u00adlow our examples to typecheck as they are instead of by manual elaboration into (co)iteration. \nTackling this in the presence of in\u00adterleaving \u00b5 and . is challenging. This is one motivation for us\u00ading \nNakano-style guarded recursion [13]. A promising direction is to explore the introduction of Nakano-style \nguarded recursion at so-called complete types in our type system (by analogy with the complete ultrametric \nspaces of [23]) roughly, types where occur\u00ad rences of \u00b5 are restricted. This would be a step in the \ndirection of unifying the two approaches.  We are in the process of developing a denotational semantics \nfor this calculus in the category of presheaves on . + 1, inspired by the work of Birkedal et al. [4] \nwho study the presheaves on ., as well as Jeltsch [18] who studies more general (e.g. continuous) time \ndo\u00ad mains. The idea is that the extra point at in.nity expresses the global behaviour it expresses our \nliveness guarantees, which can only be observed from a global perspective on time. The chief chal\u00adlenge \nin our setting lies in constructing interleaving .xed points. It is expected that such a denotational \nsemantics will provide a crisper explanation of our liveness guarantees. References [1] A. Abel and T. \nAltenkirch. A predicative strong normalisation proof for a lambda-calculus with interleaving inductive \ntypes. In TYPES, pages 21 40, 1999. [2] T. Altenkirch. Constructions, Inductive Types and Strong Normaliza\u00adtion. \nPhD thesis, University of Edinburgh, November 1993. [3] G. Berry and L. Cosserat. The esterel synchronous \nprogramming language and its mathematical semantics. In Seminar on Concurrency, Carnegie-Mellon University, \npages 389 448, London, UK, UK, 1985. Springer-Verlag. ISBN 3-540-15670-4. [4] L. Birkedal, R. E. Mogelberg, \nJ. Schwinghammer, and K. Stovring. First steps in synthetic guarded domain theory: Step-indexing in the \ntopos of trees. In Proceedings of the 26th Annual IEEE Symposium on Logic in Computer Science (LICS), \npages 55 64. IEEE Computer Society, 2011. [5] L. Birkedal, A. Bizjak, and J. Schwinghammer. Step-indexed \nrela\u00adtional reasoning for countable nondeterminism. Submitted, journal version of CSL 11 paper, 2013. \n[6] P. Caspi, D. Pilaud, N. Halbwachs, and J. A. Plaice. Lustre: a declar\u00adative language for real-time \nprogramming. In Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, \nPOPL 87, pages 178 188. ACM, 1987. [7] G. H. Cooper and S. Krishnamurthi. Embedding dynamic data.ow in \na call-by-value language. In Proceedings of the 15th European Confer\u00adence on Programming Languages and \nSystems, ESOP 06, pages 294 308. Springer-Verlag, 2006. [8] A. Courtney. Frapp \u00b4e functional reactive \nprogramming in java. In Pro\u00adceedings of the Third International Symposium on Practical Aspects of Declarative \nLanguages, PADL 01, pages 29 44. Springer-Verlag, 2001. [9] J. Donham. Functional reactive programming \nin OCaml. URL https: //github.com/jaked/froc. [10] C. Elliott and P. Hudak. Functional reactive animation. \nIn Proceedings of the second ACM SIGPLAN International Conference on Functional Programming, pages 263 \n273. ACM, 1997. [11] J. Y. Girard. Interpr \u00b4etation fonctionnelle et elimination des coupures de l arithmtique \nd ordre sup \u00b4erieur. etat, Universit \u00b4 These d \u00b4e de Paris 7, 1972. [12] J.-Y. Girard, Y. Lafont, and \nP. Tayor. Proofs and types. Cambridge University Press, 1990. [13] H. Hiroshi Nakano. A modality for \nrecursion. In Proceedings of the 15th Annual IEEE Symposium on Logic in Computer Science, pages 255 266. \nIEEE Computer Society, 2000. [14] A. Jeffrey. LTL types FRP: linear-time temporal logic propositions \nas types, proofs as functional reactive programs. In Proceedings of the sixth Workshop on Programming \nLanguages meets Program Veri.cation, pages 49 60. ACM, 2012. [15] A. Jeffrey. Causality for free! parametricity \nimplies causality for functional reactive programs. In Proceedings of the seventh Workshop on Programming \nLanguages meets Program Veri.cation, pages 57 68. ACM, 2013. [16] A. Jeffrey. Functional reactive programming \nwith liveness guarantees. In 18th International Confernece on Functional Programming. ACM, 2013. [17] \nW. Jeltsch. Towards a common categorical semantics for linear-time temporal logic and functional reactive \nprogramming. Electronic Notes in Theoretical Computer Science, 286(0):229 242, 2012. [18] W. Jeltsch. \nTemporal logic with until: Functional reactive program\u00adming with processes and concrete process categories. \nIn Proceedings of the seventh Workshop on Programming Languages meets Program Veri.cation, pages 69 78. \nACM, 2013. [19] J.-P. Jouannaud and M. Okada. Abstract data type systems. Theoreti\u00adcal Computer Science, \n173(2):349 391, 1997. [20] K. Kojima and A. Igarashi. Constructive linear-time temporal logic: Proof \nsystems and kripke semantics. Information and Computation, 209(12):1491 1503, 2011. ISSN 0890-5401. \n\u00a1ce:title\"Intuitionistic Modal Logic and Applications (IMLA 2008)\u00a1/ce:title\". [21] D. Kozen. Results \non the propositional \u00b5-calculus. Theoretical Computer Science, 27(3):333 354, 1983. [22] N. R. Krishnaswami. \nHigher-order functional reactive programming without spacetime leaks. In 18th International Confernece \non Func\u00adtional Programming. ACM, 2013. [23] N. R. Krishnaswami and N. Benton. Ultrametric semantics of \nreactive programs. In Proceedings of the 2011 IEEE 26th Annual Symposium on Logic in Computer Science, \npages 257 266. IEEE Computer Soci\u00adety, 2011. [24] N. R. Krishnaswami, N. Benton, and J. Hoffmann. Higher-order \nfunctional reactive programming in bounded space. In Proceedings of the 39th annual ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages, pages 45 58. ACM, 2012. [25] O. Lichtenstein. Decidability, \nCompleteness, and Extensions of Linear Time Temporal Logic. PhD thesis, The Weizmann Institute of Science, \n1991. [26] Z. Luo. An Extended Calculus of Constructions. PhD thesis, Univer\u00adsity of Edinburgh, 1990. \n[27] R. Matthes. Extensions of System F by Iteration and Primitive Recur\u00adsion on Monotone Inductive Types. \nPhD thesis, University of Munich, 1998. [28] N. P. Mendler. Inductive De.nition in Type Theory. PhD thesis, \nCornell University, 1988. [29] H. Nilsson, A. Courtney, and J. Peterson. Functional reactive pro\u00adgramming, \ncontinued. In Proceedings of the 2002 ACM SIGPLAN workshop on Haskell, pages 51 64. ACM, 2002. [30] D. \nM. R. Park. Concurrency and automata on in.nite sequences. In P. Deussen, editor, Theoretical Computer \nScience, Proceedings of the 5th GI-Conference, Lecture Notes in Computer Science (LNCS 104, pages 167 \n183. Springer, 1981. [31] A. Pnueli. The temporal logic of programs. In Proceedings of the 18th Annual \nSymposium on Foundations of Computer Science, pages 46 57. IEEE Computer Society, 1977. [32] M. Pouzet. \nLucid Synchrone, version 3. Tutorial and reference man\u00adual. Universit \u00b4e Paris-Sud, LRI, April 2006. \n[33] P. G. Severi and F.-J. J. de Vries. Pure type systems with corecursion on streams: from .nite to \nin.nitary normalisation. SIGPLAN Not., 47 (9):141 152, Sept. 2012. ISSN 0362-1340. [34] W. W. Tait. Intensional \ninterpretations of functionals of .nite type i. The Journal of Symbolic Logic, 32(2):pp. 198 212, 1967. \n  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Functional Reactive Programming (FRP) models reactive systems with events and signals, which have previously been observed to correspond to the \"eventually\" and \"always\" modalities of linear temporal logic (LTL). In this paper, we define a constructive variant of LTL with least fixed point and greatest fixed point operators in the spirit of the modal mu-calculus, and give it a proofs-as-programs interpretation as a foundational calculus for reactive programs. Previous work emphasized the propositions-as-types part of the correspondence between LTL and FRP; here we emphasize the proofs-as-programs part by employing structural proof theory. We show that the type system is expressive enough to enforce liveness properties such as the fairness of schedulers and the eventual delivery of results. We illustrate programming in this calculus using (co)iteration operators. We prove type preservation of our operational semantics, which guarantees that our programs are causal. We give also a proof of strong normalization which provides justification that our programs are productive and that they satisfy liveness properties derived from their types.</p>", "authors": [{"name": "Andrew Cave", "author_profile_id": "81496650900", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P4383845", "email_address": "acave1@cs.mcgill.ca", "orcid_id": ""}, {"name": "Francisco Ferreira", "author_profile_id": "81553100856", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P4383846", "email_address": "fferre8@cs.mcgill.ca", "orcid_id": ""}, {"name": "Prakash Panangaden", "author_profile_id": "81324492772", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P4383847", "email_address": "prakash@cs.mcgill.ca", "orcid_id": ""}, {"name": "Brigitte Pientka", "author_profile_id": "81100506891", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P4383848", "email_address": "bpientka@cs.mcgill.ca", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535881", "year": "2014", "article_id": "2535881", "conference": "POPL", "title": "Fair reactive programming", "url": "http://dl.acm.org/citation.cfm?id=2535881"}