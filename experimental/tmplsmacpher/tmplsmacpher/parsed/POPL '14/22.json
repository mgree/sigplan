{"article_publication_date": "01-08-2014", "fulltext": "\n Freeze After Writing Quasi-Deterministic Parallel Programming with LVars Lindsey Kuper Aaron Turon Indiana \nUniversity MPI-SWS lkuper@cs.indiana.edu turon@mpi-sws.org Abstract Deterministic-by-construction parallel \nprogramming models offer the advantages of parallel speedup while avoiding the nondetermin\u00adistic, hard-to-reproduce \nbugs that plague fully concurrent code. A principled approach to deterministic-by-construction parallel \npro\u00adgramming with shared state is offered by LVars: shared memory locations whose semantics are de.ned \nin terms of an application\u00adspeci.c lattice. Writes to an LVar take the least upper bound of the old and \nnew values with respect to the lattice, while reads from an LVar can observe only that its contents have \ncrossed a speci.ed threshold in the lattice. Although it guarantees determinism, this interface is quite \nlimited. We extend LVars in two ways. First, we add the ability to freeze and then read the contents \nof an LVar directly. Second, we add the ability to attach event handlers to an LVar, triggering a callback \nwhen the LVar s value changes. Together, handlers and freezing enable an expressive and useful style \nof parallel program\u00adming. We prove that in a language where communication takes place through these extended \nLVars, programs are at worst quasi\u00addeterministic: on every run, they either produce the same answer or \nraise an error. We demonstrate the viability of our approach by implementing a library for Haskell supporting \na variety of LVar\u00adbased data structures, together with a case study that illustrates the programming \nmodel and yields promising parallel speedup. Categories and Subject Descriptors D.3.3 [Language Constructs \nand Features]: Concurrent programming structures; D.1.3 [Con\u00adcurrent Programming]: Parallel programming; \nD.3.1 [Formal De.nitions and Theory]: Semantics; D.3.2 [Language Classi.\u00adcations]: Concurrent, distributed, \nand parallel languages Keywords Deterministic parallelism; lattices; quasi-determinism 1. Introduction \nFlexible parallelism requires tasks to be scheduled dynamically, in response to the vagaries of an execution. \nBut if the resulting sched\u00adule nondeterminism is observable within a program, it becomes Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA. \nCopyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2544-8/14/01. \n. . $15.00. http://dx.doi.org/10.1145/2535838.2535842 Neelakantan R. Ryan R. Newton Krishnaswami Indiana \nUniversity University of Birmingham rrnewton@cs.indiana.edu N.Krishnaswami@cs.bham.ac.uk much more dif.cult \nfor programmers to discover and correct bugs by testing, let alone to reason about their code in the \n.rst place. While much work has focused on identifying methods of de\u00adterministic parallel programming \n[5, 7, 18, 21, 22, 32], guaranteed determinism in real parallel programs remains a lofty and rarely achieved \ngoal. It places stringent constraints on the programming model: concurrent tasks must communicate in \nrestricted ways that prevent them from observing the effects of scheduling, a restriction that must be \nenforced at the language or runtime level. The simplest strategy is to allow no communication, forc\u00ading \nconcurrent tasks to produce values independently. Pure data\u00adparallel languages follow this strategy [28], \nas do languages that force references to be either task-unique or immutable [5]. But some algorithms \nare more naturally or ef.ciently written using shared state or message passing. A variety of deterministic-by\u00adconstruction \nmodels allow limited communication along these lines, but they tend to be narrow in scope and permit \ncommu\u00adnication through only a single data structure: for instance, FIFO queues in Kahn process networks \n[18] and StreamIt [16], or shared write-only tables in Intel Concurrent Collections [7]. Big-tent deterministic \nparallelism Our goal is to create a broader, general-purpose deterministic-by-construction programming \nenvi\u00adronment to increase the appeal and applicability of the method. We seek an approach that is not \ntied to a particular data structure and that supports familiar idioms from both functional and imperative \nprogramming styles. Our starting point is the idea of monotonic data structures, in which (1) information \ncan only be added, never removed, and (2) the order in which information is added is not observable. \nA paradigmatic example is a set that supports insertion but not removal, but there are many others. Our \nrecently proposed LVars programming model [19] makes an initial foray into programming with monotonic \ndata structures. In this model (which we review in Section 2), all shared data structures (called LVars) \nare monotonic, and the states that an LVar can take on form a lattice. Writes to an LVar must correspond \nto a join (least upper bound) in the lattice, which means that they monotonically increase the information \nin the LVar, and that they commute with one another. But commuting writes are not enough to guarantee \ndeterminism: if a read can observe whether or not a concurrent write has happened, then it can observe \ndifferences in scheduling. So in the LVars model, the answer to the question has a write occurred? (i.e., \nis the LVar above a certain lattice value?) is always yes; the reading thread will block until the LVar \ns contents reach a desired threshold. In a monotonic data structure, the absence of information is transient \nanother thread could add that information at any time but the presence of information is forever. The \nLVars model guarantees determinism, supports an unlim\u00adited variety of data structures (anything viewable \nas a lattice), and provides a familiar API, so it already achieves several of our goals. Unfortunately, \nit is not as general-purpose as one might hope.  Consider an unordered graph traversal. A typical implementa\u00adtion \ninvolves a monotonically growing set of seen nodes ; neigh\u00adbors of seen nodes are fed back into the set \nuntil it reaches a .xed point. Such .xpoint computations are ubiquitous, and would seem to be a perfect \nmatch for the LVars model due to their use of mono\u00adtonicity. But they are not expressible using the threshold \nread and least-upper-bound write operations described above. The problem is that these computations rely \non negative infor\u00admation about a monotonic data structure, i.e., on the absence of certain writes to \nthe data structure. In a graph traversal, for exam\u00adple, neighboring nodes should only be explored if \nthe current node is not yet in the set; a .xpoint is reached only if no new neighbors are found; and, \nof course, at the end of the computation it must be possible to learn exactly which nodes were reachable \n(which en\u00adtails learning that certain nodes were not). But in the LVars model, asking whether a node \nis in a set means waiting until the node is in the set, and it is not clear how to lift this restriction \nwhile retaining determinism. Monotonic data structures that can say no In this paper, we propose two \nadditions to the LVars model that signi.cantly extend its reach. First, we add event handlers, a mechanism \nfor attaching a call\u00adback function to an LVar that runs, asynchronously, whenever events arrive (in the \nform of monotonic updates to the LVar). Ordi\u00adnary LVar reads encourage a synchronous, pull model of program\u00adming \nin which threads ask speci.c questions of an LVar, potentially blocking until the answer is yes . Handlers, \nby contrast, support an asynchronous, push model of programming. Crucially, it is pos\u00adsible to check \nfor quiescence of a handler, discovering that no call\u00adbacks are currently enabled a transient, negative \nproperty. Since quiescence means that there are no further changes to respond to, it can be used to tell \nthat a .xpoint has been reached. Second, we add a primitive for freezing an LVar, which comes with the \nfollowing tradeoff: once an LVar is frozen, any further writes that would change its value instead throw \nan exception; on the other hand, it becomes possible to discover the exact value of the LVar, learning \nboth positive and negative information about it, without blocking.1 Putting these features together, \nwe can write a parallel graph traversal algorithm in the following simple fashion: traverse :: Graph \n. NodeLabel . Par (Set NodeLabel) traverse g startV = do seen . newEmptySet putInSet seen startV let \nhandle node = parMapM (putInSet seen) (nbrs g node) freezeSetAfter seen handle This code, written using \nour Haskell implementation (described in Section 6),2 discovers (in parallel) the set of nodes in a graph \ng reachable from a given node startV, and is guaranteed to pro\u00adduce a deterministic result. It works \nby creating a fresh Set LVar (corresponding to a lattice whose elements are sets, with set union as least \nupper bound), and seeding it with the starting node. The freezeSetAfter function combines the constructs \nproposed above. First, it installs the callback handle as a handler for the seen set, which will asynchronously \nput the neighbors of each visited node into the set, possibly triggering further callbacks, recursively. \nSec\u00ad 1 Our original work on LVars [19] included a brief sketch of a similar proposal for a consume operation \non LVars, but did not study it in detail. Here, we include freezing in our model, prove quasi-determinism \nfor it, and show how to program with it in conjunction with our other proposal, handlers. 2 The Par type \nconstructor is the monad in which LVar computations live. ond, when no further callbacks are ready to \nrun i.e., when the seen set has reached a .xpoint freezeSetAfter will freeze the set and return its exact \nvalue. Quasi-determinism Unfortunately, freezing does not commute with writes that change an LVar.3 If \na freeze is interleaved before such a write, the write will raise an exception; if it is interleaved \nafterwards, the program will proceed normally. It would appear that the price of negative information \nis the loss of determinism! Fortunately, the loss is not total. Although LVar programs with freezing \nare not guaranteed to be deterministic, they do satisfy a related property that we call quasi-determinism: \nall executions that produce a .nal value produce the same .nal value. To put it another way, a quasi-deterministic \nprogram can be trusted to never change its answer due to nondeterminism; at worst, it might raise an \nexception on some runs. In our proposed model, this exception can in principle pinpoint the exact pair \nof freeze and write operations that are racing, greatly easing debugging. Our general observation is \nthat pushing towards full-featured, general monotonic data structures leads to .irtation with nonde\u00adterminism; \nperhaps the best way of ultimately getting deterministic outcomes is to traipse a small distance into \nnondeterminism, and make our way back. The identi.cation of quasi-deterministic pro\u00adgrams as a useful \nintermediate class is a contribution of this paper. That said, in many cases our freezing construct is \nonly used as the very .nal step of a computation: after a global barrier, freezing is used to extract \nan answer. In this common case, we can guarantee determinism, since no writes can subsequently occur. \nContributions The technical contributions of this paper are: We introduce LVish, a quasi-deterministic \nparallel program\u00adming model that extends LVars to incorporate freezing and event handlers (Section 3). \nIn addition to our high-level design, we present a core calculus for LVish (Section 4), formalizing its \nsemantics, and include a runnable version, implemented in PLT Redex (Section 4.7), for interactive experimentation. \n We give a proof of quasi-determinism for the LVish calculus (Section 5). The key lemma, Independence, \ngives a kind of frame property for LVish computations: very roughly, if a com\u00adputation takes an LVar \nfrom state p to p', then it would take the same LVar from the state p U pF to p' U pF . The Indepen\u00addence \nlemma captures the commutative effects of LVish com\u00adputations.  We describe a Haskell library for practical \nquasi-deterministic parallel programming based on LVish (Section 6). Our library comes with a number \nof monotonic data structures, including sets, maps, counters, and single-assignment variables. Further, \nit can be extended with new data structures, all of which can be used compositionally within the same \nprogram. Adding a new data structure typically involves porting an existing scal\u00adable (e.g., lock-free) \ndata structure to Haskell, then wrapping it to expose a (quasi-)deterministic LVar interface. Our library \nexposes a monad that is indexed by a determinism level: fully deterministic or quasi-deterministic. Thus, \nthe static type of an LVish computation re.ects its guarantee, and in particular the freeze-last idiom \nallows freezing to be used safely with a fully\u00addeterministic index.  In Section 7, we evaluate our library \nwith a case study: par\u00ad allelizing control .ow analysis. The case study begins with an existing implementation \nof k-CFA [26] written in a purely func\u00ad tional style. We show how this code can easily and safely be \npar\u00adallelized by adapting it to the LVish model an adaptation that  3 The same is true for quiescence \ndetection; see Section 3.2.  yields promising parallel speedup, and also turns out to have bene.ts even \nin the sequential case. 2. Background: the LVars Model IVars [1, 7, 24, 27] are a well-known mechanism \nfor deterministic parallel programming. An IVar is a single-assignment variable [32] with a blocking \nread semantics: an attempt to read an empty IVar will block until the IVar has been .lled with a value. \nWe recently proposed LVars [19] as a generalization of IVars: unlike IVars, which can only be written \nto once, LVars allow multiple writes, so long as those writes are monotonically increasing with respect \nto an application-speci.c lattice of states. Consider a program in which two parallel computations write \nto an LVar lv, with one thread writing the value 2 and the other writing 3: let par = put lv 3 = put \nlv 2 (Example 1) in get lv Here, put and get are operations that write and read LVars, respec\u00adtively, \nand the expression let par x1 = e1; x2 = e2; . . . in body has fork-join semantics: it launches concurrent \nsubcomputations e1, e2, . . . whose executions arbitrarily interleave, but must all complete before body \nruns. The put operation is de.ned in terms of the application-speci.c lattice of LVar states: it updates \nthe LVar to the least upper bound of its current state and the new state being written. If lv s lattice \nis the = ordering on positive integers, as shown in Figure 1(a), then lv s state will always be max(3, \n2) = 3 by the time get lv runs, since the least upper bound of two positive integers n1 and n2 is max(n1, \nn2). Therefore Example 1 will deterministically evaluate to 3, regardless of the order in which the two \nput operations occurred. On the other hand, if lv s lattice is that shown in Figure 1(b), in which the \nleast upper bound of any two distinct positive integers is T, then Example 1 will deterministically raise \nan exception, indi\u00ad cating that con.icting writes to lv have occurred. This exception is analogous to \nthe multiple put error raised upon multiple writes to an IVar. Unlike with a traditional IVar, though, \nmultiple writes of the same value (say, put lv 3 and put lv 3) will not raise an ex\u00adception, because \nthe least upper bound of any positive integer and itself is that integer corresponding to the fact that \nmultiple writes of the same value do not allow any nondeterminism to be observed. Threshold reads However, \nmerely ensuring that writes to an LVar are monotonically increasing is not enough to ensure that programs \nbehave deterministically. Consider again the lattice of Figure 1(a) for lv, but suppose we change Example \n1 to allow the get operation to be interleaved with the two puts: let par = put lv 3 = put lv 2 (Example \n2) x = get lv in x Since the two puts and the get can be scheduled in any order, Example 2 is nondeterministic: \nx might be either 2 or 3, depending on the order in which the LVar effects occur. Therefore, to maintain \ndeterminism, LVars put an extra restriction on the get operation. Rather than allowing get to observe \nthe exact value of the LVar, it can only observe that the LVar has reached one of a speci.ed set of lower \nbound states. This set of lower bounds, which we provide as an extra argument to get, is called a threshold \nset because the Figure 1. Example LVar lattices: (a) positive integers ordered by =; (b) IVar containing \na positive integer; (c) pair of natural\u00adnumber-valued IVars, annotated with example threshold sets that \nwould correspond to a blocking read of the .rst or second element of the pair. Any state transition crossing \nthe tripwire for getSnd causes it to unblock and return a result. values in it form a threshold that \nthe state of the LVar must cross before the call to get is allowed to unblock and return. When the threshold \nhas been reached, get unblocks and returns not the exact value of the LVar, but instead, the (unique) \nelement of the threshold set that has been reached or surpassed. We can make Example 2 behave deterministically \nby passing a threshold set argument to get. For instance, suppose we choose the singleton set {3} as \nthe threshold set. Since lv s value can only increase with time, we know that once it is at least 3, \nit will remain at or above 3 forever; therefore the program will deterministically evaluate to 3. Had \nwe chosen {2} as the threshold set, the program would deterministically evaluate to 2; had we chosen \n{4}, it would deterministically block forever. As long as we only access LVars with put and (thresholded) \nget, we can arbitrarily share them between threads without intro\u00adducing nondeterminism. That is, the \nput and get operations in a given program can happen in any order, without changing the value to which \nthe program evaluates. Incompatibility of threshold sets While the LVar interface just described is deterministic, \nit is only useful for synchronization, not for communicating data: we must specify in advance the single \nanswer we expect to be returned from the call to get. In general, though, threshold sets do not have \nto be singleton sets. For example, consider an LVar lv whose states form a lattice of pairs of natural\u00adnumber-valued \nIVars; that is, lv is a pair (m, n), where m and n both start as . and may each be updated once with \na non-. value, which must be some natural number. This lattice is shown in Figure 1(c). We can then de.ne \ngetFst and getSnd operations for reading from the .rst and second entries of lv: 6 getFst p = get p {(m, \n.) | m . N} 6 getSnd p = get p {(., n) | n . N} This allows us to write programs like the following: \nlet par = put lv (., 4) = put lv (3, .) (Example 3) x = getSnd lv in x In the call getSnd lv, the threshold \nset is {(., 0), (., 1), . . . }, an in.nite set. There is no risk of nondeterminism because the elements \nof the threshold set are pairwise incompatible with re\u00adspect to lv s lattice: informally, since the second \nentry of lv can only be written once, no more than one state from the set {(., 0), (., 1), . . . } can \never be reached. (We formalize this in\u00adcompatibility requirement in Section 4.5.)  In the case of Example \n3, getSnd lv may unblock and return (., 4) any time after the second entry of lv has been written, re\u00adgardless \nof whether the .rst entry has been written yet. It is there\u00adfore possible to use LVars to safely read \nparts of an incomplete data structure say, an object that is in the process of being initialized by a \nconstructor. The model versus reality The use of explicit threshold sets in the above LVars model should \nbe understood as a mathematical modeling technique, not an implementation approach or practical API. \nOur library (discussed in Section 6) provides an unsafe getLV operation to the authors of LVar data structure \nlibraries, who can then make operations like getFst and getSnd available as a safe interface for application \nwriters, implicitly baking in the particular threshold sets that make sense for a given data structure \nwithout ever explicitly constructing them. To put it another way, operations on a data structure exposed \nas an LVar must have the semantic effect of a least upper bound for writes or a threshold for reads, \nbut none of this need be visible to clients (or even written explicitly in the code). Any data structure \nAPI that provides such a semantics is guaranteed to provide deter\u00administic concurrent communication. \n3. LVish, Informally As we explained in Section 1, while LVars offer a deterministic programming model \nthat allows communication through a wide va\u00adriety of data structures, they are not powerful enough to \nexpress common algorithmic patterns, like .xpoint computations, that re\u00adquire both positive and negative \nqueries. In this section, we explain our extensions to the LVar model at a high level; Section 4 then \nformalizes them, while Section 6 shows how to implement them. 3.1 Asynchrony through Event Handlers Our \n.rst extension to LVars is the ability to do asynchronous, event\u00addriven programming through event handlers. \nAn event for an LVar can be represented by a lattice element; the event occurs when the LVar s current \nvalue reaches a point at or above that lattice element. An event handler ties together an LVar with a \ncallback function that is asynchronously invoked whenever some events of interest occur. For example, \nif lv is an LVar whose lattice is that of Figure 1(a), the expression addHandler lv {1, 3, 5, . . . } \n(.x. put lv x + 1) (Example 4) registers a handler for lv that executes the callback function .x. put \nlv x + 1 for each odd number that lv is at or above. When Example 4 is .nished evaluating, lv will contain \nthe smallest even number that is at or above what its original value was. For instance, if lv originally \ncontains 4, the callback function will be invoked twice, once with 1 as its argument and once with 3. \nThese calls will respectively write 1 + 1 = 2 and 3 + 1 = 4 into lv; since both writes are = 4, lv will \nremain 4. On the other hand, if lv originally contains 5, then the callback will run three times, with \n1, 3, and 5 as its respective arguments, and with the latter of these calls writing 5 + 1 = 6 into lv, \nleaving lv as 6. In general, the second argument to addHandler is an arbitrary subset Q of the LVar s \nlattice, specifying which events should be handled. Like threshold sets, these event sets are a mathematical \nmodeling tool only; they have no explicit existence in the imple\u00admentation. Event handlers in LVish are \nsomewhat unusual in that they invoke their callback for all events in their event set Q that have taken \nplace (i.e., all values in Q less than or equal to the current LVar value), even if those events occurred \nprior to the handler being registered. To see why this semantics is necessary, consider the following, \nmore subtle example: let par = put lv 0 = put lv 1 = addHandler lv {0, 1} (.x. if x = 0 then put lv 2) \nin get lv {2} (Example 5) Can Example 5 ever block? If a callback only executed for events that arrived \nafter its handler was registered, or only for the largest event in its handler set that had occurred, \nthen the example would be nondeterministic: it would block, or not, depending on how the handler registration \nwas interleaved with the puts. By instead executing a handler s callback once for each and every element \nin its event set below or at the LVar s value, we guarantee quasi\u00addeterminism and, for Example 5, guarantee \nthe result of 2. The power of event handlers is most evident for lattices that model collections, such \nas sets. For example, if we are working with lattices of sets of natural numbers, ordered by subset inclusion, \nthen we can write the following function: forEach = .lv. .f. addHandler lv {{0}, {1}, {2}, . . . } f \nUnlike the usual forEach function found in functional program\u00adming languages, this function sets up a \npermanent, asynchronous .ow of data from lv into the callback f . Functions like forEach can be used \nto set up complex, cyclic data-.ow networks, as we will see in Section 7. In writing forEach, we consider \nonly the singleton sets to be events of interest, which means that if the value of lv is some set like \n{2, 3, 5} then f will be executed once for each singleton subset ({2}, {3}, {5}) that is, once for each \nelement. In Section 6.2, we will see that this kind of handler set can be speci.ed in a lattice\u00adgeneric \nway, and in Section 6 we will see that it corresponds closely to our implementation strategy.  3.2 Quiescence \nthrough Handler Pools Because event handlers are asynchronous, we need a separate mechanism to determine \nwhen they have reached a quiescent state, i.e., when all callbacks for the events that have occurred \nhave .n\u00adished running. As we discussed in Section 1, detecting quiescence is crucial for implementing \n.xpoint computations. To build .exible data-.ow networks, it is also helpful to be able to detect quiescence \nof multiple handlers simultaneously. Thus, our design includes handler pools, which are groups of event \nhandlers whose collective quiescence can be tested. The simplest way to use a handler pool is the following: \nlet h = newPool in addInPool h lv Q f ; quiesce h where lv is an LVar, Q is an event set, and f is a \ncallback. Handler pools are created with the newPool function, and handlers are registered with addInPool, \na variant of addHandler that takes a handler pool as an additional argument. Finally, quiesce blocks \nuntil a pool of handlers has reached a quiescent state. Of course, whether or not a handler is quiescent \nis a non\u00admonotonic property: we can move in and out of quiescence as more puts to an LVar occur, and \neven if all states at or below the current state have been handled, there is no way to know that more \nputs will not arrive to increase the state and trigger more callbacks. There is no risk to quasi-determinism, \nhowever, because quiesce does not yield any information about which events have been handled any such \nquestions must be asked through LVar functions like get. In practice, quiesce is almost always used together \nwith freezing, which we explain next.  3.3 Freezing and the Freeze-After Pattern Our .nal addition \nto the LVar model is the ability to freeze an LVar, which forbids further changes to it, but in return \nallows its exact value to be read. We expose freezing through the function freeze, which takes an LVar \nas its sole argument, and returns the exact value of the LVar as its result. As we explained in Section \n1, puts that would change the value of a frozen LVar instead raise an exception, and it is the potential \nfor races between such puts and freeze that makes LVish quasi-deterministic, rather than fully deterministic. \nPutting all the above pieces together, we arrive at a particularly common pattern of programming in LVish: \nfreezeAfter = .lv. .Q. .f. let h = newPool in addInPool h lv Q f ; quiesce h; freeze lv In this pattern, \nan event handler is registered for an LVar, subse\u00adquently quiesced, and then the LVar is frozen and its \nexact value is returned. A set-speci.c variant of this pattern, freezeSetAfter, was used in the graph \ntraversal example in Section 1. 4. LVish, Formally In this section, we present a core calculus for LVish \nin particular, a quasi-deterministic, parallel, call-by-value .-calculus extended with a store containing \nLVars. It extends the original LVar formal\u00adism to support event handlers and freezing. In comparison \nto the informal description given in the last two sections, we make two simpli.cations to keep the model \nlightweight: We parameterize the de.nition of the LVish calculus by a single application-speci.c lattice, \nrepresenting the set of states that LVars in the calculus can take on. Therefore LVish is really a family \nof calculi, varying by choice of lattice. Multiple lattices can in principle be encoded using a sum construction, \nso this modeling choice is just to keep the presentation simple; in any case, our Haskell implementation \nsupports multiple lattices natively.  Rather than modeling the full ensemble of event handlers, han\u00addler \npools, quiescence, and freezing as separate primitives, we instead formalize the freeze-after pattern \nwhich combined them directly as a primitive. This greatly simpli.es the cal\u00adculus, while still capturing \nthe essence of our programming model.  In this section we cover the most important aspects of the LVish \ncore calculus. Complete details, including the proof of Lemma 1, are given in the companion technical \nreport [20]. 4.1 Lattices The application-speci.c lattice is given as a 4-tuple (D, ., ., T) where D \nis a set, . is a partial order on the elements of D, . is the least element of D according to . and T \nis the greatest. The . element represents the initial empty state of every LVar, while T represents the \nerror state that would result from con.icting updates to an LVar. The partial order . represents the \norder in which an LVar may take on states. It induces a binary least upper bound (lub) operation U on \nthe elements of D. We require that every two elements of D have a least upper bound in D. Intuitively, \nthe existence of a lub for every two elements of D means that it is possible for two subcomputations \nto independently update an LVar, and then deterministically merge the results by taking the lub of the \nresulting two states. Formally, this makes (D, ., ., T) a bounded join-semilattice with a designated \ngreatest element (T). For brevity, we use the term lattice as shorthand for bounded join-semilattice \nwith a designated greatest element in the rest of this paper. We also occasionally use D as a shorthand \nfor the entire 4-tuple (D, ., ., T) when its meaning is clear from the context. 4.2 Freezing To model \nfreezing, we need to generalize the notion of the state of an LVar to include information about whether \nit is frozen or not. Thus, in our model an LVar s state is a pair (d, frz), where d is an element of \nthe application-speci.c set D and frz is a status bit of either true or false. We can de.ne an ordering \n.p on LVar states (d, frz) in terms of the application-speci.c ordering . on elements of D. Every element \nof D is freezable except T. Informally: Two unfrozen states are ordered according to the application\u00ad \nspeci.c .; that is, (d, false) .p (d ' , false) exactly when d . d ' . Two frozen states do not have \nan order, unless they are equal: (d, true) .p (d ' , true) exactly when d = d ' .  An unfrozen state \n(d, false) is less than or equal to a frozen state (d ' , true) exactly when d . d ' .  The only situation \nin which a frozen state is less than an un\u00adfrozen state is if the unfrozen state is T; that is, (d, true) \n.p (d ' , false) exactly when d ' = T.  The addition of status bits to the application-speci.c lattice \nresults in a new lattice (Dp, .p, .p, Tp), and we write Up for the least upper bound operation that .p \ninduces. De.nition 1 and Lemma 1 formalize this notion. De.nition 1 (Lattice freezing). Suppose (D, ., \n., T) is a lattice. 6 We de.ne an operation Freeze(D, ., ., T) = (Dp, .p, .p, Tp) as follows: 1. Dp is \na set de.ned as follows: 6 Dp = {(d, frz) | d . (D - {T}) . frz . {true, false}} . {(T, false)} 2. rp \n. P (Dp \u00d7 Dp) is a binary relation de.ned as follows: 3. .p  (d, false) (d, true) (d, false) (d, true) \nrp rp rp rp (d ' , false) (d ' , true) (d ' , true) (d ' , false) .. .. .. .. d r d ' d = d ' d r d ' \nd ' = T 6 = (., false). 6 4. Tp = (T, false). Lemma 1 (Lattice structure). If (D, ., ., T) is a lattice \nthen Freeze(D, ., ., T) is as well.  4.3 Stores During the evaluation of LVish programs, a store S keeps \ntrack of the states of LVars. Each LVar is represented by a binding from a location l, drawn from a set \nLoc, to its state, which is some pair (d, frz) from the set Dp. .n De.nition 2. A store is either a .nite \npartial mapping S : Loc . (Dp - {Tp}), or the distinguished element TS. We use the notation S[l . (d, \nfrz)] to denote extending S with a binding from l to (d, frz ). If l . dom(S), then S[l . (d, frz)] denotes \nan update to the existing binding for l, rather than an ex\u00adtension. We can also denote a store by explicitly \nwriting out all its bindings, using the notation [l1 . (d1, frz1), l2 . (d2, frz 2), . . .]. It is straightforward \nto lift the .p operations de.ned on ele\u00adments of Dp to the level of stores:  Given a lattice (D, r, \n., T) with elements d . D: con.gurations s ::= (S; e) | error expressions e ::= x | v | e e | get e e \n| put e e | new | freeze e | freeze e after e with e | freeze l after Q with .x. e, {e, . . . } , H stores \nS ::= [l1 . p1, . . . , ln . pn] | TS values v ::= () | d | p | l | P | Q | .x. e eval contexts E ::= \n[ ] | E e | e E | get E e | get e E | put E e | put e E | freeze E | freeze E after e with e | freeze \ne after E with e | freeze e after e with E | freeze v after v with v, {e . . . E e . . . } , H handled \nsets H ::= {d1, . . . , dn} states p ::= (d, frz )threshold sets P ::= {p1, p2, . . .} status bits frz \n::= true | false event sets Q ::= {d1, d2, . . .} Figure 2. Syntax for LVish. De.nition 3. A store S \nis less than or equal to a store S ' (written S S S ') iff: S ' = TS , or  dom(S) . dom(S ' ) and for \nall l . dom(S), S(l) p S ' (l).  Stores ordered by S also form a lattice (with bottom element \u00d8 and \ntop element TS ); we write US for the induced lub operation (concretely de.ned in [20]). If, for example, \n(d1, frz1) Up (d2, frz2) = Tp, then [l . (d1, frz 1)] US [l . (d2, frz2)] = TS. A store containing a \nbinding l . (T, frz) can never arise during the execution of an LVish program, because, as we will see \nin Section 4.5, an attempted put that would take the value of l to T will raise an error.  4.4 The LVish \nCalculus The syntax and operational semantics of the LVish calculus appear in Figures 2 and 3, respectively. \nAs we have noted, both the syntax and semantics are parameterized by the lattice (D, , ., T). The reduction \nrelation \"-. is de.ned on con.gurations (S; e) com\u00adprising a store and an expression. The error con.guration, \nwritten error, is a unique element added to the set of con.gurations, but we consider (TS ; e) to be \nequal to error for all expressions e. The metavariable s ranges over con.gurations. LVish uses a reduction \nsemantics based on evaluation contexts. The E-EVAL -CT X T rule is a standard context rule, allowing \nus to apply reductions within a context. The choice of context determines where evaluation can occur; \nin LVish, the order of evaluation is nondeterministic (that is, a given expression can generally reduce \nin various ways), and so it is generally not the case that an expres\u00adsion has a unique decomposition \ninto redex and context. For exam\u00adple, in an application e1 e2, either e1 or e2 might reduce .rst. The \nnondeterminism in choice of evaluation context re.ects the nonde\u00adterminism of scheduling between concurrent \nthreads, and in LVish, the arguments to get, put, freeze, and application expressions are implicitly \nevaluated concurrently.4 Arguments must be fully evaluated, however, before function application (\u00df-reduction, \nmodeled by the E-BETA rule) can occur. We can exploit this property to de.ne let par as syntactic sugar: \n6 let par x = e1; y = e2 in e3 = ((.x. (.y. e3)) e1) e2 4 This is in contrast to the original LVars formalism \ngiven in [19], which models parallelism with explicitly simultaneous reductions. Because we do not reduce \nunder .-terms, we can sequentially compose e1 before e2 by writing let = e1 in e2, which desugars to \n(. . e2) e1. Sequential composition is useful, for instance, when allocating a new LVar before beginning \na set of side-effecting put/get/freeze operations on it. 4.5 Semantics of new, put, and get In LVish, \nthe new, put, and get operations respectively create, write to, and read from LVars in the store: new \n(implemented by the E-NE W rule) extends the store with a binding for a new LVar whose initial state \nis (., false), and returns the location l of that LVar (i.e., a pointer to the LVar).  put (implemented \nby the E-PU T and E-PUT-ER R rules) takes a pointer to an LVar and a new lattice element d2 and updates \nthe LVar s state to the least upper bound of the current state and (d2, false), potentially pushing the \nstate of the LVar upward in the lattice. Any update that would take the state of an LVar to Tp results \nin the program immediately stepping to error.  get (implemented by the E-GET rule) performs a blocking \nthreshold read. It takes a pointer to an LVar and a threshold set P , which is a non-empty set of LVar \nstates that must be pairwise incompatible, expressed by the premise incomp(P ). A threshold set P is \npairwise incompatible iff the lub of any two distinct elements in P is Tp. If the LVar s state p1 in \nthe lattice is at or above some p2 . P , the get operation unblocks and returns p2. Note that p2 is a \nunique element of P , for if there is another p ' 2 p2 in the threshold set such that p ' 2 p  = p1, \nit would follow that p2 Up p ' 2 = Tp, which contradicts the p1= requirement that P be pairwise incompatible.5 \n Is the get operation deterministic? Consider two lattice elements p1 and p2 that have no ordering and \nhave Tp as their lub, and sup\u00adpose that puts of p1 and p2 and a get with {p1, p2} as its thresh\u00adold set \nall race for access to an LVar lv. Eventually, the program is guaranteed to fault, because p1 Up p2 = \nTp, but in the meantime, get lv {p1, p2} could return either p1 or p2. Therefore, get can behave nondeterministically \nbut this behavior is not observable in the .nal answer of the program, which is guaranteed to subse\u00adquently \nfault.  4.6 The freeze - after - with Primitive The LVish calculus includes a simple form of freeze \nthat im\u00admediately freezes an LVar (see E-FR EE Z E -SIMP LE). More inter\u00adesting is the freeze - after \n- with primitive, which models the freeze-after pattern described in Section 3.3. The expression freeze \nelv after eevents with ecb has the following semantics: It attaches the callback ecb to the LVar elv. \nThe expression eevents must evaluate to a event set Q; the callback will be ex\u00adecuted, once, for each \nlattice element in Q that the LVar s state reaches or surpasses. The callback ecb is a function that \ntakes a lattice element as its argument. Its return value is ignored, so it runs solely for effect. For \ninstance, a callback might itself do a put to the LVar to which it is attached, triggering yet more callbacks. \n If the handler reaches a quiescent state, the LVar elv is frozen, and its exact state is returned (rather \nthan an underapproxima\u00adtion of the state, as with get).  5 We stress that, although incomp(P ) is given \nas a premise of the E -GE T reduction rule (suggesting that it is checked at runtime), in our real implementation \nthreshold sets are not written explicitly, and it is the data structure author s responsibility to ensure \nthat any provided read operations have threshold semantics; see Section 6.  Given a lattice (D, , ., \nT) with elements d . D: 6 incomp (P ) = . p1, p2 . P. (p1 = p2 =. p1 Up p2 = Tp) s \"-. s ' E -EVA L \n-CT X T E -B E TA E-NE W (S; e) '-. (S ' ; e ' ) _ _ (l /. dom(S)) ' (S; E[e]) '-. (S ' ; E e ) (S; \n(.x. e) v) '-. (S; e[x := v]) (S; new) '-. (S[l . (., false)]; l) E -PU T E-PU T-ER R S(l) = p1 p2 = \np1 Up (d2, false) p2= Tp S(l) = p1 p1 Up (d2, false) = Tp (S; put l d2) '-. (S[l . p2]; ()) (S; put l \nd2) '-. error E -GE T E -FR E E Z E -I N I T S(l) = p1 incomp(P ) p2 . P p2 rp p1 (S; get l P ) '-. (S; \np2) (S; freeze l after Q with .x. e) '-. (S; freeze l after Q with .x. e, {} , {}) E-S PAW N -HA N D \nL E R S(l) = (d1, frz1) d2 r d1 d2 ./H d2 . Q (S; freeze l after Q with .x. e0, {e, . . . } , H ) '-. \n(S; freeze l after Q with .x. e0, {e0[x := d2], e, . . . } , {d2} . H) E -FR E E Z E -F I NA L E-FR E \nE Z E -SI M P L E S(l) = (d1, frz1) .d2 . (d2 r d1 . d2 . Q . d2 . H) S(l) = (d1, frz1) (S; freeze l \nafter Q with v, {v . . . } , H ) '-. (S[l . (d1, true)]; d1) (S; freeze l) '-. (S[l . (d1, true)]; d1) \nFigure 3. An operational semantics for LVish. To keep track of the running callbacks, LVish includes \nan auxiliary form, freeze l after Q with .x. e0, {e, . . . } , H where: The value l is the LVar being \nhandled/frozen;  The set Q (a subset of the lattice D) is the event set;  The value .x. e0 is the callback \nfunction;  The set of expressions {e, . . . } are the running callbacks; and  The set H (a subset of \nthe lattice D) represents those values in Q for which callbacks have already been launched.  Due to \nour use of evaluation contexts, any running callback can execute at any time, as if each is running in \nits own thread. The rule E-SPAW N -HA N D L ER launches a new callback thread any time the LVar s current \nvalue is at or above some element in Q that has not already been handled. This step can be taken nondeterministically \nat any time after the relevant put has been performed. The rule E-FR E E ZE-FI NA L detects quiescence \nby checking that two properties hold. First, every event of interest (lattice element in Q) that has \noccurred (is bounded by the current LVar state) must be handled (be in H). Second, all existing callback \nthreads must have terminated with a value. In other words, every enabled callback has completed. When \nsuch a quiescent state is detected, E-FR EEZ E -FI NAL freezes the LVar s state. Like E-SPAWN -HAN D \nL ER, the rule can .re at any time, nondeterministically, that the handler appears quiescent a transient \nproperty! But after being frozen, any further puts that would have enabled additional callbacks will \ninstead fault, raising error by way of the E-PU T-ERR rule. Therefore, freezing is a way of betting that \nonce a collection of callbacks have completed, no further puts that change the LVar s value will occur. \nFor a given run of a program, either all puts to an LVar arrive before it has been frozen, in which case \nthe value returned by freeze - after - with is the lub of those values, or some put arrives after the \nLVar has been frozen, in which case the program will fault. And thus we have arrived at quasi-determinism: \na program will always either evaluate to the same answer or it will fault. To ensure that we will win \nour bet, we need to guarantee that quiescence is a permanent state, rather than a transient one that \nis, we need to perform all puts either prior to freeze - after - with, or by the callback function within \nit (as will be the case for .xpoint computations). In practice, freezing is usually the very last step \nof an algorithm, permitting its result to be extracted. Our implementa\u00adtion provides a special runParThenFreeze \nfunction that does so, and thereby guarantees full determinism.  4.7 Modeling Lattice Parameterization \nin Redex We have developed a runnable version of the LVish calculus6 using the PLT Redex semantics engineering \ntoolkit [14]. In the Redex of today, it is not possible to directly parameterize a language de.nition \nby a lattice.7 Instead, taking advantage of Racket s syntactic abstraction capabilities, we de.ne a Racket \nmacro, define-LVish-language, that wraps a template imple\u00admenting the lattice-agnostic semantics of Figure \n3, and takes the following arguments: a name, which becomes the lang-name passed to Redex s define-language \nform;  a downset operation, a Racket-level procedure that takes a lattice element and returns the (.nite) \nset of all lattice elements that are below that element (this operation is used to implement the semantics \nof freeze - after - with, in particular, to de\u00adtermine when the E-FR E EZE -FI NA L rule can .re);  \na lub operation, a Racket-level procedure that takes two lattice elements and returns a lattice element; \nand  a (possibly in.nite) set of lattice elements represented as Redex patterns.  Given these arguments, \ndefine-LVish-language generates a Redex model specialized to the application-speci.c lattice in ques\u00ad \n6 Available at http://github.com/iu-parfunc/lvars. 7 See discussion at http://lists.racket-lang.org/users/ \narchive/2013-April/057075.html.  tion. For instance, to instantiate a model called nat, where the application-speci.c \nlattice is the natural numbers with max as the least upper bound, one writes: (define-LVish-language \nnat downset-op max natural) where downset-op is separately de.ned. Here, downset-op and max are Racket \nprocedures. natural is a Redex pattern that has no meaning to Racket proper, but because define-LVish-language \nis a macro, natural is not evaluated until it is in the context of Redex. 5. Quasi-Determinism for LVish \nOur proof of quasi-determinism for LVish formalizes the claim we make in Section 1: that, for a given \nprogram, although some executions may raise exceptions, all executions that produce a .nal result will \nproduce the same .nal result. In this section, we give the statements of the main quasi\u00addeterminism theorem \nand the two most important supporting lem\u00admas. The statements of the remaining lemmas, and proofs of \nall our theorems and lemmas, are included in the companion technical report [20]. 5.1 Quasi-Determinism \nand Quasi-Con.uence Our main result, Theorem 1, says that if two executions starting from a con.guration \ns terminate in con.gurations s ' and s '', then s ' and s '' are the same con.guration, or one of them \nis error. s '' Theorem 1 (Quasi-Determinism). If s \"-. * s ' and s \"-. * , and neither s ' nor s '' can \ntake a step, then either: = s '' 1. s ' up to a permutation on locations p, or 2. s ' = error or s '' \n= error.  Theorem 1 follows from a series of quasi-con.uence lemmas. The most important of these, Strong \nLocal Quasi-Con.uence (Lemma 2), says that if a con.guration steps to two different con\u00ad .gurations, \nthen either there exists a single third con.guration to which they both step (in at most one step), or \none of them steps to error. Additional lemmas generalize Lemma 2 s result to multiple steps by induction \non the number of steps, eventually building up to Theorem 1. Lemma 2 (Strong Local Quasi-Con.uence). \nIf s = (S; e) \"-. sa and s \"-. sb, then either: 1. there exist p, i, j and sc such that sa \"-.i sc and \nsb \"-.j p(sc) and i = 1 and j = 1, or  2. sa \"-. error or sb \"-. error.   5.2 Independence In order \nto show Lemma 2, we need a frame property for LVish that captures the idea that independent effects commute \nwith each other. Lemma 3, the Independence lemma, establishes this prop\u00ad erty. Consider an expression \ne that runs starting in store S and steps to e ', updating the store to S '. The Independence lemma allows \nus to make a double-edged guarantee about what will happen if we run e starting from a larger store S \nUS S '': .rst, it will update the store to S ' US S ''; second, it will step to e ' as it did before. \nHere S US S '' is the least upper bound of the original S and some other store S '' that is framed on \nto S; intuitively, S '' is the store resulting from some other independently-running computation. Lemma \n3 (Independence). If (S; e) \"-. (S ' ; e ' ) (where (S ' ; e ' ) = error), then we have that: '' ' US \nS '' ' (S US S ; e) \"-. (S ; e ), where S '' is any store meeting the following conditions: S '' is non-con.icting \nwith (S; e) \"-. (S ' ; e ' ), S ' US S '' =frz S, and S ' US S '' = TS. US S '' Lemma 3 requires as a \nprecondition that the stores S ' and S are equal in status that, for all the locations shared between \nthem, the status bits of those locations agree. This assumption rules out interference from freezing. \nFinally, the store S '' must be non-con.icting with the original transition from (S; e) to (S ' ; e ' \n), meaning that locations in S '' cannot share names with locations newly allocated during the transition; \nthis rules out location name con.icts caused by allocation. De.nition 4. Two stores S and S ' are equal \nin status (written S =frz S ') iff for all l . (dom(S) n dom(S ' )), if S(l) = (d, frz) and S ' (l) = \n(d ' , frz ' ), then frz = frz ' . De.nition 5. A store S '' is non-con.icting with the transition (S; \ne) \"-. (S ' ; e ' ) iff (dom(S ' ) - dom(S)) n dom(S '' ) = \u00d8. 6. Implementation We have constructed \na prototype implementation of LVish as a monadic library in Haskell, which is available at http://hackage.haskell.org/package/lvish \n Our library adopts the basic approach of the Par monad [24], enabling us to employ our own notion of \nlightweight, library\u00adlevel threads with a custom scheduler. It supports the program\u00adming model laid out \nin Section 3 in full, including explicit han\u00ad dler pools. It differs from our formal model in following \nHaskell s by-need evaluation strategy, which also means that concurrency in the library is explicitly \nmarked, either through uses of a fork func\u00adtion or through asynchronous callbacks, which run in their \nown lightweight thread. Implementing LVish as a Haskell library makes it possible to provide compile-time \nguarantees about determinism and quasi\u00addeterminism, because programs written using our library run in \nour Par monad and can therefore only perform LVish-sanctioned side effects. We take advantage of this \nfact by indexing Par computa\u00adtions with a phantom type that indicates their determinism level: data Determinism \n= Det | QuasiDet The Par type constructor has the following kind:8 Par :: Determinism . * . * together \nwith the following suite of run functions: runPar :: Par Det a . a runParIO :: Par lvl a . IO a runParThenFreeze \n:: DeepFrz a . Par Det a . FrzType a The public library API ensures that if code uses freeze, it is marked \nas QuasiDet; thus, code that types as Det is guaranteed to be fully deterministic. While LVish code with \nan arbitrary determinism level lvl can be executed in the IO monad using runParIO, only Det code can \nbe executed as if it were pure, since it is guaranteed to be free of visible side effects of nondeterminism. \nIn the common case that freeze is only needed at the end of an otherwise-deterministic computation, runParThenFreeze \nruns the computation to comple\u00adtion, and then freezes the returned LVar, returning its exact value and \nis guaranteed to be deterministic.9 8 We are here using the DataKinds extension to Haskell to treat Determinism \nas a kind. In the full implementation, we include a second phantom type parameter to ensure that LVars \ncannot be used in multiple runs of the Par monad, in a manner analogous to how the ST monad prevents \nan STRef from being returned from runST. 9 The DeepFrz typeclass is used to perform freezing of nested \nLVars, producing values of frozen type (as given by the FrzType type function).  6.1 The Big Picture \nWe envision two parties interacting with our library. First, there are data structure authors, who use \nthe library directly to implement a speci.c monotonic data structure (e.g., a monotonically growing .nite \nmap). Second, there are application writers, who are clients of these data structures. Only the application \nwriters receive a (quasi-)determinism guarantee; an author of a data structure is responsible for ensuring \nthat the states their data structure can take on correspond to the elements of a lattice, and that the \nexposed interface to it corresponds to some use of put, get, freeze, and event handlers. Thus, our library \nis focused primarily on lattice-generic in\u00adfrastructure: the Par monad itself, a thread scheduler, support \nfor blocking and signaling threads, handler pools, and event han\u00addlers. Since this infrastructure is \nunsafe (does not guarantee quasi\u00addeterminism), only data structure authors should import it, subse\u00adquently \nexporting a limited interface speci.c to their data structure. For .nite maps, for instance, this interface \nmight include key/value insertion, lookup, event handlers and pools, and freezing along with higher-level \nabstractions built on top of these. For this approach to scale well with available parallel resources, \nit is essential that the data structures themselves support ef.cient parallel access; a .nite map that \nwas simply protected by a global lock would force all parallel threads to sequentialize their access. \nThus, we expect data structure authors to draw from the extensive literature on scalable parallel data \nstructures, employing techniques like .ne-grained locking and lock-free data structures [17]. Data structures \nthat .t into the LVish model have a special advantage: be\u00adcause all updates must commute, it may be possible \nto avoid the ex\u00adpensive synchronization which must be used for non-commutative operations [2]. And in \nany case, monotonic data structures are usu\u00ad ally much simpler to represent and implement than general \nones. 6.2 Two Key Ideas Leveraging atoms Monotonic data structures acquire pieces of information over \ntime. In a lattice, the smallest such pieces are called the atoms of the lattice: they are elements not \nequal to ., but for which the only smaller element is .. Lattices for which every element is the lub \nof some set of atoms are called atomistic, and in practice most application-speci.c lattices used by \nLVish programs have this property especially those whose elements represent col\u00adlections. In general, \nthe LVish primitives allow arbitrarily large queries and updates to an LVar. But for an atomistic lattice, \nthe correspond\u00ading data structure usually exposes operations that work at the atom level, semantically \nlimiting puts to atoms, gets to threshold sets of atoms, and event sets to sets of atoms. For example, \nthe lattice of .nite maps is atomistic, with atoms consisting of all singleton maps (i.e., all key/value \npairs). The interface to a .nite map usually works at the atom level, allowing addition of a new key/value \npair, querying of a single key, or traversals (which we model as handlers) that walk over one key/value \npair at a time. Our implementation is designed to facilitate good performance for atomistic lattices \nby associating LVars with a set of deltas (changes), as well as a lattice. For atomistic lattices, the \ndeltas are essentially just the atoms for a set lattice, a delta is an element; for a map, a key/value \npair. Deltas provide a compact way to rep\u00adresent a change to the lattice, allowing us to easily and ef.ciently \ncommunicate such changes between puts and gets/handlers. Leveraging idempotence While we have emphasized \nthe com\u00admutativity of least upper bounds, they also provide another impor\u00adtant property: idempotence, \nmeaning that dUd = d for any element d. In LVish terms, repeated puts or freezes have no effect, and \nsince these are the only way to modify the store, the result is that e; e behaves the same as e for any \nLVish expression e. Idempotence has already been recognized as a useful property for work-stealing scheduling \n[25]: if the scheduler is allowed to occasionally dupli\u00ad cate work, it is possible to substantially save \non synchronization costs. Since LVish computations are guaranteed to be idempotent, we could use such \na scheduler (for now we use the standard Chase-Lev deque [10]). But idempotence also helps us deal with \nraces between put and get/addHandler, as we explain below. 6.3 Representation Choices Our library uses \nthe following generic representation for LVars: data LVar a d = LVar { state :: a, status :: IORef (Status \nd) } where the type parameter a is the (mutable) data structure repre\u00adsenting the lattice, and d is the \ntype of deltas for the lattice.10 The status .eld is a mutable reference that represents the status bit: \ndata Status d = Frozen | Active (B.Bag (Listener d)) The status bit of an LVar is tied together with \na bag of waiting listeners, which include blocked gets and handlers; once the LVar is frozen, there can \nbe no further events to listen for.11 The bag module (imported as B) supports atomic insertion and removal, \nand concurrent traversal: put :: Bag a . a . IO (Token a) remove :: Token a . IO () foreach :: Bag a \n. (a . Token a . IO ()) . IO () Removal of elements is done via abstract tokens, which are ac\u00adquired \nby insertion or traversal. Updates may occur concurrently with a traversal, but are not guaranteed to \nbe visible to it. A listener for an LVar is a pair of callbacks, one called when the LVar s lattice value \nchanges, and the other when the LVar is frozen: data Listener d = Listener { onUpd :: d . Token (Listener \nd) . SchedQ . IO (), onFrz :: Token (Listener d) . SchedQ . IO () } The listener is given access to its \nown token in the listener bag, which it can use to deregister from future events (useful for a get whose \nthreshold has been passed). It is also given access to the CPU-local scheduler queue, which it can use \nto spawn threads.  6.4 The Core Implementation Internally, the Par monad represents computations in \ncontinuation\u00adpassing style, in terms of their interpretation in the IO monad: type ClosedPar = SchedQ \n. IO () type ParCont a = a . ClosedPar mkPar :: (ParCont a . ClosedPar) . Par lvl a The ClosedPar type \nrepresents ready-to-run Par computations, which are given direct access to the CPU-local scheduler queue. \nRather than returning a .nal result, a completed ClosedPar compu\u00adtation must call the scheduler, sched, \non the queue. A Par compu\u00adtation, on the other hand, completes by passing its intended result to its \ncontinuation yielding a ClosedPar computation. Figure 4 gives the implementation for three core lattice-generic \nfunctions: getLV, putLV, and freezeLV, which we explain next. Threshold reading The getLV function assists \ndata structure au\u00adthors in writing operations with get semantics. In addition to an LVar, it takes two \nthreshold functions, one for global state and one for deltas. The global threshold gThresh is used to \ninitially check whether the LVar is above some lattice value(s) by global inspec\u00adtion; the extra boolean \nargument gives the frozen status of the LVar. The delta threshold dThresh checks whether a particular \nupdate 10 For non-atomistic lattices, we take a and d to be the same type. 11 In particular, with one \natomic update of the .ag we both mark the LVar as frozen and allow the bag to be garbage-collected. \n getLV :: (LVar a d) . (a . Bool . IO (Maybe b)) . (d . IO (Maybe b)) . Par lvl b getLV (LVar{state, \nstatus}) gThresh dThresh = mkPar $.k q . let onUpd d = unblockWhen (dThresh d) onFrz = unblockWhen (gThresh \nstate True) unblockWhen thresh tok q = do tripped . thresh whenJust tripped $ .b . do B.remove tok Sched.pushWork \nq (k b) in do curStat . readIORef status case curStat of Frozen . do --no further deltas can arrive! \ntripped . gThresh state True case tripped of Just b . exec (k b) q Nothing . sched q Active ls . do tok \n. B.put ls (Listener onUpd onFrz) frz . isFrozen status --must recheck after --enrolling listener tripped \n. gThresh state frz case tripped of Just b . do B.remove tok --remove the listener k b q --execute our \ncontinuation Nothing . sched q putLV :: LVar a d . (a . IO (Maybe d)) . Par lvl () putLV (LVar{state, \nstatus}) doPut = mkPar $ .k q . do Sched.mark q --publish our intent to modify the LVar delta . doPut \nstate --possibly modify LVar curStat . readIORef status --read while q is marked Sched.clearMark q --retract \nour intent whenJust delta $ .d . do case curStat of Frozen . error \"AttemptutouchangeuaufrozenuLVar\" \nActive listeners . B.foreach listeners $ .(Listener onUpd _) tok . onUpd d tok q k () q freezeLV :: \nLVar a d . Par QuasiDet () freezeLV (LVar {status}) = mkPar $ .k q . do Sched.awaitClear q oldStat . \natomicModifyIORef status $ .s.(Frozen, s) case oldStat of Frozen . return () Active listeners . B.foreach \nlisteners $ .(Listener _ onFrz) tok . onFrz tok q k () q Figure 4. Implementation of key lattice-generic \nfunctions. takes the state of the LVar above some lattice state(s). Both func\u00adtions return Just r if \nthe threshold has been passed, where r is the result of the read. To continue our running example of \n.nite maps with key/value pair deltas, we can use getLV internally to build the following getKey function \nthat is exposed to application writers: --Wait for the map to contain a key; return its value getKey \nkey mapLV = getLV mapLV gThresh dThresh where gThresh m frozen = lookup key m dThresh (k,v) | k == key \n= return (Just v) | otherwise = return Nothing where lookup imperatively looks up a key in the underlying \nmap. The challenge in implementing getLV is the possibility that a concurrent put will push the LVar \nover the threshold. To cope with such races, getLV employs a somewhat pessimistic strategy: before doing \nanything else, it enrolls a listener on the LVar that will be triggered on any subsequent updates. If \nan update passes the delta threshold, the listener is removed, and the continuation of the get is invoked, \nwith the result, in a new lightweight thread. After enrolling the listener, getLV checks the global threshold, \nin case the LVar is already above the threshold. If it is, the listener is removed, and the continuation \nis launched immediately; otherwise, getLV invokes the scheduler, effectively treating its continuation \nas a blocked thread. By doing the global check only after enrolling a listener, getLV is sure not to \nmiss any threshold-passing updates. It does not need to synchronize between the delta and global thresholds: \nif the threshold is passed just as getLV runs, it might launch the contin\u00aduation twice (once via the \nglobal check, once via delta), but by idempotence this does no harm. This is a performance tradeoff: \nwe avoid imposing extra synchronization on all uses of getLV at the cost of some duplicated work in a \nrare case. We can easily provide a second version of getLV that makes the alternative tradeoff, but as \nwe will see below, idempotence plays an essential role in the analogous situation for handlers. Putting \nand freezing On the other hand, we have the putLV func\u00adtion, used to build operations with put semantics. \nIt takes an LVar and an update function doPut that performs the put on the underly\u00ading data structure, \nreturning a delta if the put actually changed the data structure. If there is such a delta, putLV subsequently \ninvokes all currently-enrolled listeners on it. The implementation of putLV is complicated by another \nrace, this time with freezing. If the put is nontrivial (i.e., it changes the value of the LVar), the \nrace can be resolved in two ways. Either the freeze takes effect .rst, in which case the put must fault, \nor else the put takes effect .rst, in which case both succeed. Unfortunately, we have no means to both \ncheck the frozen status and attempt an update in a single atomic step.12 Our basic approach is to ask \nforgiveness, rather than permis\u00adsion: we eagerly perform the put, and only afterwards check whether the \nLVar is frozen. Intuitively, this is allowed because if the LVar is frozen, the Par computation is going \nto terminate with an exception so the effect of the put cannot be observed! Unfortunately, it is not \nenough to just check the status bit for frozenness afterward, for a rather subtle reason: suppose the \nput is executing concurrently with a get which it causes to unblock, and that the getting thread subsequently \nfreezes the LVar. In this case, we must treat the freeze as if it happened after the put, because the \nfreeze could not have occurred had it not been for the put. But, by the time putLV reads the status bit, \nit may already be set, which naively would cause putLV to fault. To guarantee that such confusion cannot \noccur, we add a marked bit to each CPU scheduler state. The bit is set (using Sched.mark) prior to a \nput being performed, and cleared (using Sched.clear) only after putLV has subsequently checked the frozen \nstatus. On the other hand, freezeLV waits until it has observed a (transient!) clear mark bit on every \nCPU (using Sched.awaitClear) before actually freezing the LVar. This guarantees that any puts that caused \nthe freeze to take place check the frozen status before the freeze takes place; additional puts that \narrive concurrently may, of course, set a mark bit again after freezeLV has observed a clear status. \nThe proposed approach requires no barriers or synchronization instructions (assuming that the put on \nthe underlying data struc\u00adture acts as a memory barrier). Since the mark bits are per-CPU .ags, they \ncan generally be held in a core-local cache line in exclu\u00adsive mode meaning that marking and clearing \nthem is extremely 12 While we could require the underlying data structure to support such transactions, \ndoing so would preclude the use of existing lock-free data structures, which tend to use a single-word \ncompare-and-set operation to perform atomic updates. Lock-free data structures routinely outperform transaction-based \ndata structures [15].  cheap. The only time that the busy .ags can create cross-core com\u00admunication \nis during freezeLV, which should only occur once per LVar computation. One .nal point: unlike getLV and \nputLV, which are polymorphic in their determinism level, freezeLV is statically QuasiDet. Handlers, pools \nand quiescence Given the above infrastructure, the implementation of handlers is relatively straightforward. \nWe represent handler pools as follows: data HandlerPool = HandlerPool { numCallbacks :: Counter, blocked \n:: B.Bag ClosedPar } where Counter is a simple counter supporting atomic increment, decrement, and checks \nfor equality with zero.13 We use the counter to track the number of currently-executing callbacks, which \nwe can use to implement quiesce. A handler pool also keeps a bag of threads that are blocked waiting \nfor the pool to reach a quiescent state. We create a pool using newPool (of type Par lvl HandlerPool), \nand implement quiescence testing as follows: quiesce :: HandlerPool . Par lvl () quiesce hp@(HandlerPool \ncnt bag) = mkPar $ .k q . do tok . B.put bag (k ()) quiescent . poll cnt if quiescent then do B.remove \ntok; k () q else sched q where the poll function indicates whether cnt is (transiently) zero. Note that \nwe are following the same listener-enrollment strategy as in getLV, but with blocked acting as the bag \nof listeners. Finally, addHandler has the following interface: addHandler :: Maybe HandlerPool --Pool \nto enroll in . LVar a d --LVar to listen to . (a . IO (Maybe (Par lvl ()))) --Global callback . (d \n. IO (Maybe (Par lvl ()))) --Delta callback . Par lvl ()  As with getLV, handlers are speci.ed using \nboth global and delta threshold functions. Rather than returning results, however, these threshold functions \nreturn computations to run in a fresh lightweight thread if the threshold has been passed. Each time \na callback is launched, the callback count is incremented; when it is .nished, the count is decremented, \nand if zero, all threads blocked on its quiescence are resumed. The implementation of addHandler is very \nsimilar to getLV, but there is one important difference: handler callbacks must be in\u00advoked for all events \nof interest, not just a single threshold. Thus, the Par computation returned by the global threshold \nfunction should execute its callback on, e.g., all available atoms. Likewise, we do not remove a handler \nfrom the bag of listeners when a single delta threshold is passed; handlers listen continuously to an \nLVar until it is frozen. We might, for example, expose the following foreach function for a .nite map: \nforeach mh mapLV cb = addHandler mh lv gThresh dThresh where dThresh (k,v) = return (Just (cb k v)) gThresh \nmp = traverse mp (.(k,v) . cb k v) mp Here, idempotence really pays off: without it, we would have to \nsynchronize to ensure that no callbacks are duplicated between the global threshold (which may or may \nnot see concurrent additions to the map) and the delta threshold (which will catch all concurrent additions). \nWe expect such duplications to be rare, since they can only arise when a handler is added concurrently \nwith updates to an LVar.14 7. Evaluation: k-CFA Case Study We now evaluate the expressiveness and performance \nof our Haskell LVish implementation. We expect LVish to particularly shine for: (1) parallelizing complicated \nalgorithms on structured data that pose challenges for other deterministic paradigms, and (2) composing \npipeline-parallel stages of computation (each of which may be internally parallelized). In this section, \nwe focus on a case study that .ts this mold: parallelized control-.ow analysis. We discuss the process \nof porting a sequential implementation of k-CFA to a parallel implementation using LVish. In the compan\u00adion \ntechnical report [20], we also give benchmarking results for LVish implementations of two graph-algorithm \nmicrobenchmarks: breadth-.rst search and maximal independent set. 7.1 k-CFA The k-CFA analyses provide \na hierarchy of increasingly precise methods to compute the .ow of values to expressions in a higher\u00adorder \nlanguage. For this case study, we began with a simple, se\u00adquential implementation of k-CFA translated \nto Haskell from a ver\u00adsion by Might [26].15 The algorithm processes expressions written in a continuation-passing-style \n.-calculus. It resembles a nondeter\u00administic abstract interpreter in which stores map addresses to sets \nof abstract values, and function application entails a cartesian product between the operator and operand \nsets. Further, an address models not just a static variable, but includes a .xed k-size window of the \ncalling history to get to that point (the k in k-CFA). Taken together, the current redex, environment, \nstore, and call history make up the abstract state of the program, and the goal is to explore a graph \nof these abstract states. This graph-exploration phase is followed by a second, summarization phase that \ncombines all the information discovered into one store. Phase 1: breadth-.rst exploration The following \nfunction from the original, sequential version of the algorithm expresses the heart of the search process: \nexplore :: Set State . [State] . Set State explore seen [] = seen explore seen (todo:todos) | todo . \nseen = explore seen todos | otherwise = explore (insert todo seen) (toList (next todo) ++ todos) This \ncode uses idiomatic Haskell data types like Data.Set and lists. However, it presents a dilemma with respect \nto exposing par\u00adallelism. Consider attempting to parallelize explore using purely functional parallelism \nwith futures for instance, using the Strate\u00adgies library [23]. An attempt to compute the next states \nin parallel would seem to be thwarted by the the main thread rapidly forc\u00ading each new state to perform \nthe seen-before check, todo . seen. There is no way for independent threads to keep going further into \nthe graph; rather, they check in with seen after one step. We con.rmed this prediction by adding a parallelism \nannota\u00adtion: withStrategy (parBuffer 8 rseq) (next todo). The GHC runtime reported that 100% of created \nfutures were duds that is, the main thread forced them before any helper thread could assist. Changing \nrseq to rdeepseq exposed a small amount 14 That said, it is possible to avoid all duplication by adding \nfurther syn\u00adchronization, and in ongoing research, we are exploring various locking and timestamp schemes \nto do just that. 13 One can use a high-performance scalable non-zero indicator [13] to 15 Haskell port \nby Max Bolingbroke: https://github.com/ implement Counter, but we have not yet done so. batterseapower/haskell-kata/blob/master/0CFA.hs. \n of parallelism 238/5000 futures were successfully executed in parallel yielding no actual speedup. Phase \n2: summarization The .rst phase of the algorithm pro\u00adduces a large set of states, with stores that need \nto be joined together in the summarization phase. When one phase of a computation pro\u00adduces a large data \nstructure that is immediately processed by the next phase, lazy languages can often achieve a form of \npipelin\u00ading for free . This outcome is most obvious with lists, where the head element can be consumed \nbefore the tail is computed, offer\u00ading cache-locality bene.ts. Unfortunately, when processing a pure \nSet or Map in Haskell, such pipelining is not possible, since the data structure is internally represented \nby a balanced tree whose struc\u00adture is not known until all elements are present. Thus phase 1 and phase \n2 cannot overlap in the purely functional version but they will in the LVish version, as we will see. \nIn fact, in LVish we will be able to achieve partial deforestation in addition to pipelining. Full deforestation \nin this application is impossible, because the Sets in the implementation serve a memoization purpose: \nthey prevent re\u00adpeated computations as we traverse the graph of states.  7.2 Porting to the LVish Library \nOur .rst step was a verbatim port to LVish. We changed the origi\u00adnal, purely functional program to allocate \na new LVar for each new set or map value in the original code. This was done simply by changing two types, \nSet and Map, to their monotonic LVar counter\u00adparts, ISet and IMap. In particular, a store maps a program \nlocation (with context) onto a set of abstract values: import Data.LVar.Map as IM import Data.LVar.Set \nas IS type Store s = IMap Addr s (ISet s Value) Next, we replaced allocations of containers, and map/fold \nopera\u00adtions over them, with the analogous operations on their LVar coun\u00adterparts. The explore function \nabove was replaced by the simple graph traversal function from Section 1! These changes to the pro\u00ad gram \nwere mechanical, including converting pure to monadic code. Indeed, the key insight in doing the verbatim \nport to LVish was to consume LVars as if they were pure values, ignoring the fact that an LVar s contents \nare spread out over space and time and are modi.ed through effects. In some places the style of the ported \ncode is functional, while in others it is imperative. For example, the summarize function uses nested \nforEach invocations to accumulate data into a store map: summarize :: ISet s (State s) . Par d s (Store \ns) summarize states = do storeFin . newEmptyMap IS.forEach states $ . (State _ _ store _) . IM.forEach \nstore $ . key vals . IS.forEach vals $ . elmt . IM.modify storeFin key (putInSet elmt) return storeFin \nWhile this code can be read in terms of traditional parallel nested loops, it in fact creates a network \nof handlers that convey incre\u00admental updates from one LVar to another, in the style of data\u00ad.ow networks. \nThat means, in particular, that computations in a pipeline can immediately begin reading results from \ncontainers (e.g., storeFin), long before their contents are .nal. The LVish version of k-CFA contains \n11 occurrences of forEach, as well as a few cartesian-product operations. The cartesian prod\u00aducts serve \nto apply functions to combinations of all possible values that arguments may take on, greatly increasing \nthe number of han\u00addler events in circulation. Moreover, chains of handlers registered with forEach result \nin cascades of events through six or more han\u00addlers. The runtime behavior of these would be dif.cult \nto reason about. Fortunately, the programmer can largely ignore the temporal behavior of their program, \nsince all LVish effects commute rather Figure 5. Simpli.ed handler network for k-CFA. Exploration and \nsummarization processes are driven by the same LVar. The triply\u00adnested forEach calls in summarize become \na chain of three han\u00addlers. like the way in which a lazy functional programmer typically need not think \nabout the order in which thunks are forced at runtime. Finally, there is an optimization bene.t to using \nhandlers. Nor\u00admally, to .atten a nested data structure such as [[[Int]]] in a func\u00adtional language, we \nwould need to .atten one layer at a time and allocate a series of temporary structures. The LVish version \navoids this; for example, in the code for summarize above, three forEach invocations are used to traverse \na triply-nested structure, and yet the side effect in the innermost handler directly updates the .nal \naccumulator, storeFin. Flipping the switch The verbatim port uses LVars poorly: copy\u00ading them repeatedly \nand discarding them without modi.cation. This effect overwhelms the bene.ts of partial deforestation \nand pipelining, and the verbatim LVish port has a small performance overhead relative to the original. \nBut not for long! The most clearly unnecessary operation in the verbatim port is in the next function. \nLike the pure code, it creates a fresh store to extend with new bind\u00adings as we take each step through \nthe state space graph: store . IM.copy store Of course, a copy for an LVar is persistent: it is just \na handler that forces the copy to receive everything the original does. But in LVish, it is also trivial \nto entangle the parallel branches of the search, allowing them to share information about bindings, simply \nby not creating a copy: let store = store This one-line change speeds up execution by up to 25\u00d7 on a \nsin\u00adgle thread, and the asynchronous, ISet-driven parallelism enables subsequent parallel speedup as \nwell (up to 202\u00d7 total improvement over the purely functional version). Figure 6 shows performance data \nfor the blur benchmark drawn from a recent paper on k-CFA [12]. (We use k = 2 for the benchmarks in this \nsection.) In general, it proved dif.cult to generate example inputs to k-CFA that took long enough to \nbe candidates for parallel speedup. We were, however, able to scale up the blur benchmark by replicating \nthe code N times, feeding one into the continuation argument for the next. Figure 6 also shows the results \nfor one synthetic benchmark that managed to negate the bene.ts of our sharing approach, which is simply \na long chain of 300 not functions (using a CPS conversion of the Church encoding for booleans). It has \na small state space of large states with many variables (600 states and 1211 variables). The role of \nlock-free data structures As part of our library, we provide lock-free implementations of .nite maps \nand sets based on concurrent skip lists [17].16 We also provide reference implementa\u00adtions that use a \nnondestructive Data.Set inside a mutable container. 16 In fact, this project is the .rst to incorporate \nany lock-free data struc\u00adtures in Haskell, which required solving some unique problems pertaining  \nFigure 6. Parallel speedup for the blur and notChain bench\u00admarks. Speedup is normalized to the sequential \ntimes for the lock-free versions (5.21s and 9.83s, respectively). The normalized speedups are remarkably \nconsistent for the lock-free version be\u00adtween the two benchmarks. But the relationship to the original, \npurely functional version is quite different: at 12 cores, the lock\u00adfree LVish version of blur is 202\u00d7 \nfaster than the original, while notChain is only 1.6\u00d7 faster, not gaining anything from sharing rather \nthan copying stores due to a lack of fan-out in the state graph. Our scalable implementation is not yet \ncarefully optimized, and at one and two cores, our lock-free k-CFA is 38% to 43% slower than the reference \nimplementation on the blur benchmark. But the effect of scalable data structures is quite visible on \na 12-core machine.17 Without them, blur (replicated 8\u00d7) stops scaling and begins slowing down slightly \nafter four cores. Even at four cores, variance is high in the reference implementation (min/max 0.96s \n/ 1.71s over 7 runs). With lock-free structures, by contrast, perfor\u00admance steadily improves to a speedup \nof 8.14\u00d7 on 12 cores (0.64s at 67% GC productivity). Part of the bene.t of LVish is to allow purely functional \nprograms to make use of lock-free structures, in much the same way that the ST monad allows access to \nef.cient in-place array computations. 8. Related Work Monotonic data structures: traditional approaches \nLVish builds on two long traditions of work on parallel programming models based on monotonically-growing \nshared data structures: In Kahn process networks (KPNs) [18], as well as in the more restricted synchronous \ndata .ow systems [21], a network of pro\u00ad cesses communicate with each other through blocking FIFO channels \nwith ever-growing channel histories. Each process computes a sequential, monotonic function from the \nhistory of its inputs to the history of its outputs, enabling pipeline paral\u00adlelism. KPNs are the basis \nfor deterministic stream-processing languages such as StreamIt [16].  In parallel single-assignment \nlanguages [32], full/empty bits are associated with heap locations so that they may be written to at \nmost once. Single-assignment locations with blocking read semantics that is, IVars [1] have appeared \nin Concurrent ML as SyncVars [30]; in the Intel Concurrent Collections system [7]; in languages and libraries \nfor high-performance computing, such as Chapel [9] and the Qthreads library [33]; and have even  to \nHaskell s laziness and the GHC compiler s assumptions regarding refer\u00ad ential transparency. But we lack \nthe space to detail these improvements. 17 Intel Xeon 5660; full machine details available at https://portal. \nfuturegrid.org/hardware/delta. been implemented in hardware in Cray MTA machines [3]. Although most of \nthese uses incorporate IVars into already\u00adnondeterministic programming environments, Haskell s Par monad \n[24] on which our LVish implementation is based uses IVars in a deterministic-by-construction setting, \nallowing user-created threads to communicate through IVars without re\u00adquiring IO, so that such communication \ncan occur anywhere inside pure programs. LVars are general enough to subsume both IVars and KPNs: a lattice \nof channel histories with a pre.x ordering allows LVars to represent FIFO channels that implement a Kahn \nprocess network, whereas an LVar with empty and full states (where empty < full ) behaves like an IVar, \nas we described in Section 2. Hence LVars provide a framework for generalizing and unifying these two \nexisting approaches to deterministic parallelism. Deterministic Parallel Java (DPJ) DPJ [4, 5] is a deterministic \nlanguage consisting of a system of annotations for Java code. A so\u00adphisticated region-based type system \nensures that a mutable region of the heap is, essentially, passed linearly to an exclusive writer, thereby \nensuring that the state accessed by concurrent threads is disjoint. DPJ does, however, provide a way \nto unsafely assert that operations commute with one another (using the commuteswith form) to enable concurrent \nmutation. LVish differs from DPJ in that it allows overlapping shared state between threads as the default. \nMoreover, since LVar effects are already commutative, we avoid the need for commuteswith anno\u00adtations. \nFinally, it is worth noting that while in DPJ, commutativ\u00adity annotations have to appear in application-level \ncode, in LVish only the data-structure author needs to write trusted code. The ap\u00adplication programmer \ncan run untrusted code that still enjoys a (quasi-)determinism guarantee, because only (quasi-)deterministic \nprograms can be expressed as LVish Par computations. More recently, Bocchino et al. [6] proposed a type \nand ef\u00ad fect system that allows for the incorporation of nondeterminis\u00adtic sections of code in DPJ. The \ngoal here is different from ours: while they aim to support intentionally nondeterministic computa\u00adtions \nsuch as those arising from optimization problems like branch\u00adand-bound search, LVish s quasi-determinism \narises as a result of schedule nondeterminism. FlowPools Prokopec et al. [29] recently proposed a data \nstructure with an API closely related to ideas in LVish: a FlowPool is a bag that allows concurrent insertions \nbut forbids removals, a seal op\u00aderation that forbids further updates, and combinators like foreach that \ninvoke callbacks as data arrives in the pool. To retain deter\u00adminism, the seal operation requires explicitly \npassing the expected bag size as an argument, and the program will raise an exception if the bag goes \nover the expected size. While this interface has a .avor similar to LVish, it lacks the ability to detect \nquiescence, which is crucial for supporting exam\u00adples like graph traversal, and the seal operation is \nawkward to use when the structure of data is not known in advance. By con\u00adtrast, our freeze operation \nis more expressive and convenient, but moves the model into the realm of quasi-determinism. Another im\u00adportant \ndifference is the fact that LVish is data structure-generic: both our formalism and our library support \nan unlimited collection of data structures, whereas FlowPools are specialized to bags. Nev\u00adertheless, \nFlowPools represent a sweet spot in the deterministic parallel design space: by allowing handlers but \nnot general freez\u00ading, they retain determinism while improving on the expressivity of the original LVars \nmodel. We claim that, with our addition of handlers, LVish generalizes FlowPools to add support for arbitrary \nlattice-based data structures.  Concurrent Revisions The Concurrent Revisions (CR) [22] pro\u00ad gramming \nmodel uses isolation types to distinguish regions of the heap shared by multiple mutators. Rather than \nenforcing exclusive access, CR clones a copy of the state for each mutator, using a de\u00adterministic merge \nfunction for resolving con.icts in local copies at join points. Unlike LVish s least-upper-bound writes, \nCR merge functions are not necessarily commutative; the default CR merge function is joiner wins . Still, \nsemilattices turn up in the metathe\u00adory of CR: in particular, Burckhardt and Leijen [8] show that, for \nany two vertices in a CR revision diagram, there exists a great\u00adest common ancestor state which can be \nused to determine what changes each side has made an interesting duality with our model (in which any \ntwo LVar states have a lub). While CR could be used to model similar types of data struc\u00adtures to LVish \nif versioned variables used least upper bound as their merge function for con.icts effects would only \nbecome vis\u00adible at the end of parallel regions, rather than LVish s asynchronous communication within \nparallel regions. This precludes the use of traditional lock-free data structures as a representation. \nCon.ict-free replicated data types In the distributed systems lit\u00aderature, eventually consistent systems \nbased on con.ict-free repli\u00adcated data types (CRDTs) [31] leverage lattice properties to guar\u00ad antee \nthat replicas in a distributed database eventually agree. Un\u00adlike LVars, CRDTs allow intermediate states \nto be observed: if two replicas are updated independently, reads of those replicas may disagree until \na (least-upper-bound) merge operation takes place. Various data-structure-speci.c techniques can ensure \nthat non-monotonic updates (such as removal of elements from a set) are not lost. The BloomL language \nfor distributed database programming [11] combines CRDTs with monotonic logic, resulting in a lattice\u00adparameterized, \ncon.uent language that is a close relative of LVish. A monotonicity analysis pass rules out programs \nthat would per\u00adform non-monotonic operations on distributed data collections, whereas in LVish, monotonicity \nis enforced by the LVar API. Future work will further explore the relationship between LVars and CRDTs: \nin one direction, we will investigate LVar-based data structures inspired by CRDTs that support non-monotonic \nopera\u00adtions; in the other direction, we will investigate the feasibility and usefulness of LVar threshold \nreads in a distributed setting. Acknowledgments Lindsey Kuper and Ryan Newton s work on this paper was \nfunded by NSF grant CCF-1218375. References [1] Arvind, R. S. Nikhil, and K. K. Pingali. I-structures: \ndata structures for parallel computing. ACM Trans. Program. Lang. Syst., 11, October 1989. [2] H. Attiya, \nR. Guerraoui, D. Hendler, P. Kuznetsov, M. M. Michael, and M. Vechev. Laws of order: expensive synchronization \nin concurrent algorithms cannot be eliminated. In POPL, 2011. [3] D. A. Bader and K. Madduri. Designing \nmultithreaded algorithms for breadth-.rst search and st-connectivity on the Cray MTA-2. In ICPP, 2006. \n[4] R. L. Bocchino, Jr., V. S. Adve, S. V. Adve, and M. Snir. Parallel programming must be deterministic \nby default. In HotPar, 2009. [5] R. L. Bocchino, Jr., V. S. Adve, D. Dig, S. V. Adve, S. Heumann, R. \nKomuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type and effect system for deterministic \nparallel Java. In OOPSLA, 2009. [6] R. L. Bocchino, Jr. et al. Safe nondeterminism in a deterministic-by\u00addefault \nparallel language. In POPL, 2011. [7] Z. Budimli\u00b4c, M. Burke, V. Cav \u00b4e, K. Knobe, G. Lowney, R. Newton, \nJ. Palsberg, D. Peixotto, V. Sarkar, F. Schlimbach, and S. Tas\u00b8irlar. Concurrent Collections. Sci. Program., \n18, August 2010. [8] S. Burckhardt and D. Leijen. Semantics of concurrent revisions. In ESOP, 2011. [9] \nB. L. Chamberlain, D. Callahan, and H. P. Zima. Parallel programma\u00adbility and the Chapel language. International \nJournal of High Perfor\u00admance Computing Applications, 21(3), 2007. [10] D. Chase and Y. Lev. Dynamic circular \nwork-stealing deque. In SPAA, 2005. [11] N. Conway, W. Marczak, P. Alvaro, J. M. Hellerstein, and D. \nMaier. Logic and lattices for distributed programming. In SOCC, 2012. [12] C. Earl, I. Sergey, M. Might, \nand D. Van Horn. Introspective pushdown analysis of higher-order programs. In ICFP, 2012. [13] F. Ellen, \nY. Lev, V. Luchangco, and M. Moir. SNZI: Scalable NonZero Indicators. In PODC, 2007. [14] M. Felleisen, \nR. B. Findler, and M. Flatt. Semantics Engineering with PLT Redex. The MIT Press, 1st edition, 2009. \n[15] K. Fraser. Practical lock-freedom. PhD thesis, 2004. [16] M. I. Gordon, W. Thies, M. Karczmarek, \nJ. Lin, A. S. Meli, C. Leger, A. A. Lamb, J. Wong, H. Hoffman, D. Z. Maze, and S. Amarasinghe. A stream \ncompiler for communication-exposed architectures. In ASP-LOS, 2002. [17] M. Herlihy and N. Shavit. The \nArt of Multiprocessor Programming. Morgan Kaufmann, 2008. [18] G. Kahn. The semantics of a simple language \nfor parallel program\u00adming. In J. L. Rosenfeld, editor, Information processing. North Hol\u00adland, Amsterdam, \nAug 1974. [19] L. Kuper and R. R. Newton. LVars: lattice-based data structures for deterministic parallelism. \nIn FHPC, 2013. [20] L. Kuper, A. Turon, N. R. Krishnaswami, and R. R. New\u00adton. Freeze after writing: \nQuasi-deterministic parallel program\u00adming with LVars. Technical Report TR710, Indiana University, November \n2013. URL http://www.cs.indiana.edu/cgi-bin/ techreports/TRNNN.cgi?trnum=TR710. [21] E. Lee and D. Messerschmitt. \nSynchronous data .ow. Proceedings of the IEEE, 75(9):1235 1245, 1987. [22] D. Leijen, M. Fahndrich, and \nS. Burckhardt. Prettier concurrency: purely functional concurrent revisions. In Haskell, 2011. [23] S. \nMarlow, P. Maier, H.-W. Loidl, M. K. Aswad, and P. Trinder. Seq no more: better strategies for parallel \nHaskell. In Haskell, 2010. [24] S. Marlow, R. Newton, and S. Peyton Jones. A monad for deterministic \nparallelism. In Haskell, 2011. [25] M. M. Michael, M. T. Vechev, and V. A. Saraswat. Idempotent work \nstealing. In PPoPP, 2009. [26] M. Might. k-CFA: Determining types and/or control\u00ad.ow in languages like \nPython, Java and Scheme. http://matt.might.net/articles/implementation-of-kcfa-and-0cfa/. [27] R. S. \nNikhil. Id language reference manual, 1991. [28] S. L. Peyton Jones, R. Leshchinskiy, G. Keller, and \nM. M. T. Chakravarty. Harnessing the multicores: Nested data parallelism in Haskell. In FSTTCS, 2008. \n[29] A. Prokopec, H. Miller, T. Schlatter, P. Haller, and M. Odersky. Flow-Pools: a lock-free deterministic \nconcurrent data.ow abstraction. In LCPC, 2012. [30] J. H. Reppy. Concurrent Programming in ML. Cambridge \nUniversity Press, Cambridge, England, 1999. [31] M. Shapiro, N. Preguic\u00b8 a, C. Baquero, and M. Zawirski. \nCon.ict-free replicated data types. In SSS, 2011. [32] L. G. Tesler and H. J. Enea. A language design \nfor concurrent processes. In AFIPS, 1968 (Spring). [33] K. B. Wheeler, R. C. Murphy, and D. Thain. Qthreads: \nAn API for programming with millions of lightweight threads, 2008.     \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Deterministic-by-construction parallel programming models offer the advantages of parallel speedup while avoiding the nondeterministic, hard-to-reproduce bugs that plague fully concurrent code. A principled approach to deterministic-by-construction parallel programming with shared state is offered by <i>LVars</i>: shared memory locations whose semantics are defined in terms of an application-specific lattice. Writes to an LVar take the least upper bound of the old and new values with respect to the lattice, while reads from an LVar can observe only that its contents have crossed a specified threshold in the lattice. Although it guarantees determinism, this interface is quite limited.</p> <p>We extend LVars in two ways. First, we add the ability to \"freeze\" and then read the contents of an LVar directly. Second, we add the ability to attach event handlers to an LVar, triggering a callback when the LVar's value changes. Together, handlers and freezing enable an expressive and useful style of parallel programming. We prove that in a language where communication takes place through these extended LVars, programs are at worst <i>quasi-deterministic</i>: on every run, they either produce the same answer or raise an error. We demonstrate the viability of our approach by implementing a library for Haskell supporting a variety of LVar-based data structures, together with a case study that illustrates the programming model and yields promising parallel speedup.</p>", "authors": [{"name": "Lindsey Kuper", "author_profile_id": "83058892657", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P4383815", "email_address": "lkuper@cs.indiana.edu", "orcid_id": ""}, {"name": "Aaron Turon", "author_profile_id": "81418594363", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P4383816", "email_address": "turon@mpi-sws.org", "orcid_id": ""}, {"name": "Neelakantan R. Krishnaswami", "author_profile_id": "81320491252", "affiliation": "University of Birmingham, Birmingham, United Kingdom", "person_id": "P4383817", "email_address": "N.Krishnaswami@cs.bham.ac.uk", "orcid_id": ""}, {"name": "Ryan R. Newton", "author_profile_id": "81341494555", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P4383818", "email_address": "rrnewton@cs.indiana.edu", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535842", "year": "2014", "article_id": "2535842", "conference": "POPL", "title": "Freeze after writing: quasi-deterministic parallel programming with LVars", "url": "http://dl.acm.org/citation.cfm?id=2535842"}