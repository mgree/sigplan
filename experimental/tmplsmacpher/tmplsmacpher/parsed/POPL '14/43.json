{"article_publication_date": "01-08-2014", "fulltext": "\n Abstract Acceleration of General Linear Loops Bertrand Jeannet Peter Schrammel * Sriram Sankaranarayanan \nINRIA University of Oxford University of Colorado, Boulder bertrand.jeannet@inria.fr peter.schrammel@cs.ox.ac.uk \nsrirams@colorado.edu Abstract We present abstract acceleration techniques for computing loop in\u00advariants \nfor numerical programs with linear assignments and condi\u00adtionals. Whereas abstract interpretation techniques \ntypically over\u00adapproximate the set of reachable states iteratively, abstract accel\u00aderation captures the \neffect of the loop with a single, non-iterative transfer function applied to the initial states at the \nloop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without \nrestrictions. Its novelty lies in the use of the Jordan normal form decomposition of the loop body to \nde\u00adrive symbolic expressions for the entries of the matrix modeling the effect of n = 0 iterations of \nthe loop. The entries of such a matrix depend on n through complex polynomial, exponential and trigonometric \nfunctions. Therefore, we introduces an abstract do\u00admain for matrices that captures the linear inequality \nrelations be\u00adtween these complex expressions. This results in an abstract matrix for describing the .xpoint \nsemantics of the loop. Our approach integrates smoothly into standard abstract inter\u00adpreters and can \nhandle programs with nested loops and loops con\u00adtaining conditional branches. We evaluate it over small \nbut complex loops that are commonly found in control software, comparing it with other tools for computing \nlinear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and os\u00adcillatory \nbehaviors that present challenges to existing approaches. Our approach .nds non-trivial invariants to \nprove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches \nin terms of precision while exhibiting good performance. 1. Introduction We present a simple yet effective \nway of inferring accurate loop invariants of linear loops, i.e. loops containing linear assignments and \nguards, as exempli.ed by the programs shown in Figs. 1 and 2. Such loops are particularly common in control \nand digital signal processing software due to the presence of components such as .lters, integrators, \niterative loops for equation solving that com\u00adpute square roots, cube roots and loops that interpolate \ncomplex * Supported by the ARTEMIS VeTeSS project and ERC project 280053. Supported by the US National \nScience Foundation (NSF) under Grant No. 0953941. All opinions involved are those of the authors and \nnot necessarily of the US National Science Foundation. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. POPL 14, \nJanuary 22 24, 2014, San Diego, CA, USA. Copyright c &#38;#169; 2014 ACM 978-1-4503-2544-8/14/01. . . \n$15.00. http://dx.doi.org/10.1145/2535838.2535843 real x,y,z,t; assume(-2<=x<=2 and -2<=y<=2 and -2<=z<=2); \nloop t:=0; head while (x+y <= 30) loop guard loop exit  { x t := := x+y; t+1; y } := y+z; z := z+1; \n Figure 1. Linear loop having a cubic behavior. functions using splines. Static analysis of such programs \nusing standard abstract interpretation theory over polyhedral abstract do\u00admains often incurs a signi.cant \nloss of precision due to the use of extrapolation techniques (widening) to force termination of the analysis. \nHowever, widening is well-known to be too imprecise for such loops. In fact, specialized domains such \nas ellipsoids and arithmetic-geometric progressions were proposed to deal with two frequently occurring \npatterns that are encountered in control loops [12, 13]. These domains enable static analyzers for control \nsystems, e.g.,AST R \u00b4 EE,to .nd the strong loop invariants that can establish bounds on the variables \nor the absence of run-time errors [10]. In this paper, we present a promising alternative approach to \nsuch loops by capturing the effect of a linear loop by means of a so-called meta-transition [5] that \nmaps an initial set of states to an invariant at the loop head. This process is commonly termed accel\u00aderation. \nThe idea of accelerations was .rst studied for communicat\u00ading .nite-state machines [5] and counter automata \n[14]. Such accel\u00aderations can be either exact (see [3] for a survey), or abstract [18]. Abstract acceleration \nseeks to devise a transformer that maps initial sets of states to the best correct over-approximation \nof the invariant at the loop head for a given abstract domain, typically the convex polyhedra domain. \nAbstract acceleration enables static analyzers to avoid widening for the innermost loops of the program \nby replac\u00ading them by meta-transitions. As discussed in [39] and observed experimentally in [38], abstract \nacceleration presents the following bene.ts w.r.t. widening: (i) It is locally more precise because it \ntakes into account the loop body in the extrapolation it performs, whereas widening considers only sequences \nof invariants. (ii) It performs more predictable approximations because it is monotonic (unlike widening). \n  (iii) It makes the analysis more ef.cient byspeedingupconver\u00adgence to a .xed point. For programs without \nnested loops, our acceleration renders the program loop-free. Apart from abstract interpretation, techniques \nsuch as symbolic ex\u00adecution and bounded model-checking, that are especially ef.cient over loop-free systems, \ncan bene.t from loop acceleration. In this paper we present a novel approach to computing abstract accelerations. \nOur approach is non-iterative, avoiding widening. We focus on the linear transformation induced by a \nlinear loop body modeled by a square matrix A. We seek to approximate the set of matrices {I, A, A2 ,...}, \nwhich represent the possible linear trans\u00adformations that can be applied to the initial state of the \nprogram to obtain the current state. This set could be de.ned as .xed point equations on matrices and \nsolved iteratively on a suitable abstract domain for matrices. However, such an approach does not avoid \nwidening and suffers from ef.ciency issues, because a matrix for a program with n variables has n 2 entries, \nthus requiring a matrix abstract domain with n 2 different dimensions. Contributions. The overall contribution \nof this paper is an ab\u00adstract acceleration technique for computing the precise effect of any linear loop \non an input predicate. It relies on the computation of the Jordan normal form of the square matrix A \nfor the loop body. Being based on abstract acceleration, it integrates smoothly into an abstract interpretation-based \nanalyzer and can be exploited for the analysis of general programs with nested loops and conditionals \nby transforming them into multiple loops around a program location. The .rst technical contribution is \nan abstract acceleration method for computing, non-iteratively, an approximation of the set {I, A, A2 \n,...} in an abstract domain for matrices. It enables the analysis of any in.nite, non-guarded linear \nloop. The main idea is to consider the Jordan normal form J of the transformation ma\u00adtrix A. Indeed, \nthe particular structure of the Jordan normal form J has two advantages: (i) It results in closed-form \nexpressions for the coef.cients of Jn , on which asymptotic analysis techniques can be applied that remove \nthe need for widening, (ii) It reduces the number of different coef.cients of Jn to at most the dimension \nof the vector space (ef.ciency issue).  This .rst contribution involves a conceptually simple but tech\u00adnically \ninvolved derivation that we omit in this paper and which can be found in the extended version [23]. The \nsecond technical contribution addresses loops with guards that are conjunctions of linear inequalities. \nWe present an origi\u00adnal technique for bounding the number of loop iterations. Once again, we utilize \nthe Jordan normal form. These two techniques together make our approach more powerful than ellipsoidal \nmeth\u00adods (e.g. [34]) that are restricted to stable loops, because the guard is only weakly taken into \naccount. We evaluate our approach by comparing ef.ciency and the pre\u00adcision of the invariants produced \nwith other invariant synthesis ap\u00adproaches, including abstract interpreters and constraint-based ap\u00adproaches. \nThe evaluation is carried out over a series of simple loops, alone or inside outer loops (such as in \nFig. 2), exhibiting behaviors such as polynomial, stable and unstable exponentials, and inward spirals \n(damped oscillators). We show the ability of our approach to discover polyhedral invariants that are \nsound over\u00adapproximations of the reachable state space. For such systems, any inductive reasoning in \na linear domain as performed by, e.g.,stan\u00addard abstract interpretation with Kleene iteration and widening \nis often unable to .nd a linear invariant other than true . In contrast, our approach is shown to .nd \nuseful bounds for many of the pro\u00adgram variables that appear in such loops. To our knowledge, our method \nis the .rst one able to bound the variables in the convoyCar example of Sankaranarayanan et al. [36]. \nOutline. We introduce some basic notions in \u00a72. \u00a73 gives an overview of the ideas of this paper. \u00a7\u00a74 \nto 6 explain the contri\u00adbutions in detail. \u00a77 summarizes our experimental results and \u00a78 discusses related \nwork before \u00a79 concludes.  2. Preliminaries In this section, we recall the notions of linear assertions \nand convex polyhedra, and we de.ne the model of linear loops for which we will propose acceleration methods. \nreal t,te,time; assume (te=14 and 16<=t and t<=17); while true { time := 0; --timer measuring duration \nin each mode while (t<=22) { --heating mode t := 15/16* t-1/16* te+1; time++; } time := 0; while (t>=18){ \n--cooling mode t := 15/16* t-1/16* te; time++; } } Figure 2. A thermostat system, composed of two simple \nloops inside a outer loop. 2.1 Linear assertions and convex polyhedra Let x1,...,xp be real-valued variables, \ncollectively forming a p \u00d7 1 column vector xx. A linear expression is written as an inner product xc \n\u00b7 xx,wherein xc . Rp. A linear inequality is of the form xc \u00b7 xx = d with d . R.A linear assertion is \na conjunction of linear : q inequalities: .(xx): i=1 xci \u00b7xx = di. The assertion . is succinctly written \nas Cxx = dx,where C is an q \u00d7 p matrix whose ith row is xci. Likewise, dxis an q \u00d7 1 column vector whose \njth coef.cient is dj . The linear assertion consisting of the single inequality 0 = 0 represents the \nassertion true while the assertion 1 = 0 represents the assertion false . Given a linear assertion .,the \nset [[.]] = {xx . Rp | .(xx)}is a convex polyhedron. The set of all convex polyhedra contained in Rp \nis denoted by CP(Rp). We recall that a convex polyhedron P .C P (Rp) can be represented in two ways: \nx (a) The constraint representation Cxx = d with matrix C and vector dx. (b) The generator representation \nwith a set of vertices V = {xv1,. .., xvk} and rays R = {xr1,..., xrl},wherein xx . P iff  kl k k xx \n= .ixvi + \u00b5jxrj with .i,\u00b5j = 0 andi .i =1 i=1 j=1  2.2 Linear loops We consider linear loops consisting \nof a while loop, the body of which is a set of assignments without tests and the condition is a linear \nassertion. DE FINIT ION 1 (Linear loop). A linear loop (G,xh, A,xb) is a pro\u00adgram fragment of the form \nwhile(Gxx = xh) xx := Axx + xb; where . : Gxx = xh is a linear assertion over the state variables xx \nrepresenting the loop condition and (A,xb) is the linear transfor\u00admation associated with the loop body. \nFigure 1 shows an example of a linear loop with a guard that computes y = x(x +1)/2 by the successive \ndifference method. We give another example below. EXAMPL E 1 (Thermostat). Figure 2 models the operation \nof a thermostat that switches between the heating and cooling modes over time. The variables t, te model \nthe room and outside temper\u00adatures, respectively. We wish to show that the value of t remains within \nsome bounds that are close to the switch points 18, 22 units. Any linear loop (G,xh, A,xb) can be homogenized \nby introducing anew variable . that is a place holder for the constant 1 to a loop of the form whileG \nxh  Our approach computes a .nitely representable approximation M of the countably in.nite set of matrices \nn=0 An . Thereafter,    oo A x b x x x = x0 :=; 0 1.abstract acceleration simply applies M to X. \n.. The following example illustrates the .rst step. Henceforth, we will use the notation (G . A) to denote \nthe homogenized linear loop while (Gxx = x0){ x ' := Axx; }. EXAMPLE 2 (Exponential 1/4). We consider \nthe program while(true){ x=1.5*x; y=y+1 } DEFINITION 2 (Semantic function). The semantic function of \na linear loop (G . A) over sets of states is the functional of which Fig. 3 depicts some trajectories. \nAfter homogenization, the .loop s semantic function is (G . A)(X) = A(X n [[Gxx = 0]]) ,X . Rp . . 1.50 \n0 o o where A(Y ) denotes the image of a set Y by the transformation A. .. 00 0 0 11 . A = G = 2.3 Convex \nand template polyhedra abstract domains 0 0 1 The set of convex polyhedra CP(Rp) ordered by inclusion \nis a lat-Here, it is easy to obtain a closed-form symbolic expression of An: . . tice with the greatest \nlower bound n being the set intersection and 1.5n 00 0 1 n 0 0 1 the least upper bound U being the convex \nhull. The de.nition of An .. = the domain includes an abstraction function a that maps sets of states \nto a polyhedral abstraction and a corresponding concretiza\u00ad  tion function .. We refer the reader to \nthe original work of Cousot The idea for approximating n=0 An is to consider a set of matri\u00adand Halbwachs \nfor a complete description [9]. ces of the form . . . . . . . It is well-known that the abstract domain \noperations such as m1 0 0 0 1 m2 0 0 1 join and transfer function across non-invertible assignments \nare . | .M(m1,m2) M = .. computationally expensive. As a result, many weakly-relational domains such \nas octagons and templates have been proposed [29, with .M a linear assertion in a template domain such \nthat .n =0: 37]. Given a matrix T . Rq\u00d7p of q linear expressions, CPT (Rp) s An .M. Using an octagonal \ntemplate, for instance, the following CP(Rp) denotes the set of template polyhedra on T :  assertion \nsatis.es the condition above: .M : . . . ... ....... n pq CPT (R)=P .CP(R) |.xu . R\u00af: P = {x | Txx = \nxu} where R\u00afdenotes R .{8}. A template polyhedron will be denoted by (T, xu).If T is is .xed, it is uniquely \nde.ned by the vector xu. CPT (Rp) ordered by inclusion is a complete lattice. The abstrac\u00ad tion aT and \nconcretization .T are de.ned elsewhere [37]. m1 . [1, +8]= [ inf 1.5n , sup 1.5n] n=0 n=0 m2 . [0, +8]= \n[ inf n, sup n] n=0 n=0 m1 +m2 . [1, +8]= [ inf (1.5n +n), sup(1.5n n=0 +n)] n=0 m1 -m2 . [0.25, +8]= \n[ inf (1.5n -n), sup(1.5n -n)] n=0 n=0  3. Overview This section provides a general overview of the \nideas in this paper, starting with abstract acceleration techniques. Abstract Acceleration Given a set \nof initial states X0 and a loop with the semantic function t , the smallest loop invariant X con\u00adtaining \nX0 can be formally written as  * .t n X = t (X0)= (X0) n=0 Abstract acceleration seeks an optimal approximation \nof t * in a given abstract domain with abstraction function a [18]. Whereas These constraints actually \nde.ne the smallest octagon on entries m1,m2 that makes M an overapproximation of A * = {An | n = 0}. \nIt is depicted in Fig. 3. The technique to evaluate the non-linear inf and sup expressions above is described \nin \u00a75.2. This is the .rst important idea of the paper. \u00a74 formalizes the notion of abstract matrices, \nwhereas \u00a75 will exploit the Jordan normal form of A to effectively compute a(A * ) for any matrix A, \ni.e. to accelerate the loop body. Applying the abstraction to acceleration The next step is to apply \nthe matrix abstraction M = a(A * ) to an abstract element X. For illustration, assume that both M and \nX are de.nedbylinear o o the standard abstract interpretation appr ach seeks to solve the .x point equation \nY ' assertions .M and .X from the polyhedral domain or some sub\u00ad polyhedral domains. Applying the set \nof matrices M to X amounts t (Y ') by iteratively computing = a(X0) U a * . .. . . . to computing (an \napproximation of) m1 0 0 x (1) Y =(a . t ) (a(X0)) . . the abstract acceleration approach uses t * \nto compute .M(m1,m2) . 0 1 m2 .. y . . . (3) * (2) Z = a . t (a(X0)) .X (x, y) . 001 1 Classically, \nEqn. (1) is known as the minimal .xed point (MFP) This is not trivial, as the matrix multiplication generates \nbilinearsolution of the reachability problem whereas Eqn. (2) is called the expressions. \u00a74 proposes \na general approach for performing the abstract multiplication. The result of the procedure is illustrated \nby Merge-Over-All-Paths (MOP) solution. The latter is known to yield more precise results [24]. the example \nthat follows: The technical challenge of abstract acceleration is thus to obtain a closed-form approximation \nof a . t * that avoids both inductive . . EXAMPLE 3 (Exponential 2/4). Assume that in Ex. 2 and Eqn. \n(3), multiplication reasoning in the abstract domain and the use of widening. .X =(x . [1, 3] . y . \n[1, 2]). We compute the abstract matrix Abstract acceleration without guards using matrix abstract do\u00admains \nWe now present an overview for linear loop without . . . . . 1 =x =3 . 0 =y =2 . m1 =1 . m2 =0 . m1 \n\u00b7 x guards, with semantic function t =(true . A). For any set X, MX = . 1+ m2 \u00b7 y .. we have m1 -m2 \n=0.25 1  * t n An t (X)= (X)= X n=0 n=0   . . . . ' x ' =1 . y ' \u00a76 presents the technique for \nover-approximating the number of iterations of a loop where the guard G is a general linear assertion, \nA is any matrix and X any polyhedron. This is the third main =0 . x ' -1.5y ' =-4.125 ' . x . . = Y y \n. ' -y ' =1.5 1 3.5x where Y is the result obtained by the method described in \u00a74 and is depicted in \nFig. 4. Handling Guards We consider loops of the form t = G . A and illlustrate how the loop condition \n(guard) G is handled. A simple approach takes the guard into account after the .xpoint of the loop without \nguard is computed: (G . A) * (X) . X . (G . A) . (G . A * )(X) which is then abstracted with X U (G . \nA) . (G . a(A * ))(X). However, such an approach is often unsatisfactory. EXAMPL E 4 (Exponential 3/4). \nWe add the guard y = 3 to our running Ex. 2. Using the approximation above with X as in Ex. 3, we obtain \nthe invariant Y depicted in Fig. 5. In this result y is bounded but x remains unbounded. contribution \nof the paper. An Illustrative Comparison The capability of our method to compute over-approximations \nof the reachable state space goes be\u00adyond state-of-the-art invariant inference techniques. The following \ntable lists the bounds obtained on the variables of the thermostat of Ex. 1,Fig. 2 for some competing \ntechniques: INTER PRO C [22] AST R \u00b4EE [4] STI NG [7, 36] this paper heating: 16 =t 0 =time 16 =t =22.5 \n0 =time 16 =t =22.5 0 =time =13 16 =t =22.5 0 =time =9.76 cooling: t =22.5 0 =time 17.75 =t =22.50 =time \n17.75 =t =22.5 0 =time =19 17.75 =t =22.5 0 =time =12.79 There are many other invariant generation techniques \nand tools Our idea is based on the observation that the bound on y induced by the guard implies a bound \nN on the maximum number of iterations for any initial state in X. Once this bound is known, we * N t \nn can exploit the knowledge t (X)= n=0 (X) and consider the better approximation (G . A) * (X) . X U \n(G . A) . G . a N -1 An (X) n=0 N -1 The set of matrices An is then approximated in the same for linear \nsystems (see \u00a78). Many approaches sacri.ce precision for speed, and therefore are inaccurate on the type \nof linear loops considered here. Other, more specialized approaches require con\u00additions such as Lyapunov-stability, \ndiagonalizability of the matrix, polynomial behavior (nilpotency or monoidal property), or handle only \ninteger loops. Outline of the rest of the paper The rest of the paper develops the ideas illustrated \nin this section. \u00a74 formalizes the notion of matrix abstract domains and presents a technique for the \nabstract n=0  matrix multiplication operation. \u00a75 shows how to approximate the = n=0 An for any square \nmatrix A in order way as A * in \u00a73. We could perform an iterative computation for set of matrices A * \nsmall N, however, a polyhedral analysis without widening operator to accelerate loops without guards. \n\u00a76 presents a technique for taking the guard of loops into account by approximating N,the maximum number \nof iterations possible from a given set of initial states. \u00a77 presents the experimental evaluation on \nvarious kinds of linear loops, possibly embedded into outer loops. \u00a78 discusses related work and \u00a79 concludes. \n 4. Matrix abstract domains In this section we present abstract domains for matrices. We will is intractably \nexpensive for hundreds or thousands iterations, while our method is both, precise and ef.cient. EXAMPL \nE 5 (Exponential 4/4). In our running example, it is easy to see that the initial condition y0 . [0, \n2] together with the guard y = 3 implies that the maximum number of iteration is N =4. Thus, we can consider \nthe set of matrices M ' = a( N -1 ) de- An n=0 .ned by the following assertion satis.es the condition \n.M. above: . ....... ....... m1 . [1, 3.375] = [ inf 1.5n , 0=n=3 m2 . [0, 3] = [ inf n, 0=n=3 m1 + \nm2 . [1, 6.375] = [ inf (1.5n +n), 0=n=3 m1 - m2 . [0.25, 1] = [ inf (1.5n -n), 0=n=3 sup 1.5n] 0=n=3 \nsup n] 0=n=3 sup (1.5n +n)] 0=n=3 sup (1.5n -n)] 0=n=3 use abstract matrices to represent the accelerated \nabstract trans\u00adformer of a linear loop. Hence, the main operation on abstract ma\u00adtrices we use in this \npaper is abstract matrix multiplication (\u00a74.2). 4.1 Extending abstract domains from vectors to matrices \nWe abstract sets of square matrices in Rp\u00d7p by viewing them as vectors in Rp 2 and by reusing known abstract \ndomains over which is depicted in Fig. 3. Using the formula above, we obtain the vectors. However, since \nthe concrete matrices we will be dealing invariant Z depicted in Fig. 5, which is much more precise than \nthe with belong to subspaces of Rp\u00d7p,we .rst introduce matrix shapes invariant Y discovered with the \nsimple technique. that allow us to reduce the number of entries in abstract matrices. . Rp\u00d7p DEFINITION \n3 (Matrix shape). A matrix shape .: Rm is a bijective, linear map from m-dimensional vectors to p \u00d7 p \nsquare matrices. Intuitively, matrix shapes represent matrices whose entries are linear (or af.ne) combinations \nof m> 0 entries. EXAMPLE 6. In Ex. 2, we implicitly considered the matrix shape .: R2 . Mat. . R3\u00d73 . \n. m1 0 0 1 This corresponds to the well-known non\u00adconvex set of points x . [0, 1] . y . [0, 1] . (x \n-y)2 +1 = 2(x + y), depicted to the right. 0 0 1  We may follow at least two approaches for approximating \nM1M2: Either we consider the constraint representations of M1 and m1 . . 0 1 m2 . M2, and we resort to \noptimization techniques to obtain a tem\u00ad m2 0 0 1 plate polyhedra approximation of the product; The set \nMat. = m) | m . Rm} represents all possi\u00ad {.( xxble matrices that can be formed by any vector x m.Itrepresents \na subspace of the vector space of all matrices. A matrix shape . induces an isomorphism between Rm and \nMat.: .(a1 xm2)= a1.( xm2). m1 + a2 xm1)+ a2.( x Abstract domain for matrices are constructed by (a) \nchoosing an abstract domain for vectors x m and (b) specifying a matrix shape .. Given an abstract domain \nA for vectors x m (eg, the polyhedral domain) and a shape ., the corresponding matrix abstract domain \nde.nes a domain over subsets of Mat.. . R3\u00d73 EXAMPLE 7. Recall the matrix shape .: R2 . Mat. from Ex. \n6. Consider the octagon P =(m1 = 1 . m2 = 0 . m1 + m2 =1 . m1-m2 =0.25) . Oct(R2). Together they represent \nan abstract matrix (P, .) which represents the set of matrices: Or we consider their generator representations \nto obtain a con\u00advex polyhedron approximating the product. We opted in this paper for the second, i.e. \nthe generator approach, which leads to more accurate results: it delivers general convex polyhedra, \nmore expressive than tem\u00adplate polyhedra obtained by optimization;  it computes the best correct approximation \nin the convex poly\u00adhedra domain for bounded matrices (Thm. 1 below), whereas in the constraint approach \nthe exact optimization problem involves bilinear expressions (see Eqn. (3)or Ex. 8) and must be relaxed \nin practice.  Multiplying abstract matrices using generators. Given two .nite sets of matrices X = {X1,... \n,Xm} and Y = {Y1, ...,Yk} we write X . Y to denote the set . . .. . . m1 0 0 m1 =1,m2 =0, X . Y = {XiYj \n| Xi . X, Yj . Y } . 0 1 m2 . m1 +m2 =1, If Ms is expressed as a system of matrix vertices Vs =(Vs,is \n) and . . . 0 0 1 m1 -m2 =0.25 matrix rays Rs =(Rs,js ), s =1, 2,then .is ,\u00b5js = 0 DEFINITION 4 (Abstract \ndomain for matrices induced by .). Ms = .is Vs,is + \u00b5js Rs,js is js .is =1 Let A . P(Rm) be an abstract \ndomain for m-dimensional vectors is ordered by set inclusion and with the abstraction function aA : \nand Eqn. (4) can be rewritten P(Rm) . A. Then, .(A) ordered by set inclusion is an abstract domain for \nP(Mat.) with the abstraction function M = M1M2 = . ... . ... -1 i1,i2 .1,i1 .2,i2 V1,i1 V2,i2 .1,i1 ,\u00b51,j1 \n= 0 + i1,j2 .1,i1 \u00b52,j2 V1,i1 R2,j2 .2,i2 ,\u00b52,j2 = 0 a.(A)(M)=..aA ..(M) . (5)Note that since . is an \nisomorphism: the lattices A and .(A) can be shown to be isomorphic. For generality, the base domain + \ni2,j1 \u00b51,j1 .2,i2 R1,j1 V2,i2 i1 .1,i1 =1 + j1,j2 \u00b51,j1 \u00b52,j2 R1,j1 R2,j2 i2 .2,i2 =1  ... ... A can \nbe an arbitrary abstract domain for the data type of the matrix entries. In our examples, we speci.cally \ndiscuss common numerical domains such as convex polyhedra, intervals, octagons We obtain the following \nresult: THEOREM 1. Let M1 and M2 be two abstract matrices expressed and templates.  4.2 Abstract matrix \nmultiplication We investigate now the problem of convex polyhedra matrix mul\u00ad as a system of vertices \nand rays: V1,R1 for M1 and V2,R2 for M2. The matrix polyhedron M de.ned by the set of vertices V = V1 \n. V2 and the set of rays tiplication, motivated by the need for applying an acceleration a(A * ) to \nan abstract property X as shown in \u00a73. The problem. We consider two convex polyhedra matrices Ms =.s(Ps). \nWe aim at computing an approximation of M = M1M2 = {M1M2 | M1 .M1 . M2 .M2} (4) R =(V1 . R2) . (R1 . \nV2) . (R1 . R2) is an overapproximation of M = M1M2. Moreover, if M1 and M2 are bounded, i.e. if R1 = \nR2 = \u00d8, then M is the smallest polyhedron matrix containing M. PROOF. For the .rst part of the theorem, \nwe observe that in Eqn. (5), .1,i1 .2,i2 =1 and the other similar sums are i1,i2 . ... M positiveandunbounded.Hence \n. ' . ' i1,i2 V1,i1 V2,i2 i1,i2 = 0 M = under the form of a convex polyhedron on the coef.cients of the \nresulting matrix. Observe that M may be non-convex as shown by the following example. . ... i1,i2 \u00b5 \n' \u00b5 ' i1,j2 V1,i1 R2,j2 i1,j2 = 0 + i1,j2 M. \u00b5 '' \u00b5 '' ,\u00b5 ''' + i2,j1 j1,i2 R1,j1 V2,i2 j1,i2 j1,j2 \n= 0 . . . . EXAMPLE 8. Consider the two abstract matrices \u00b5 ''' . ' + R1,j1 R2,j2 =1 j1,j2 j1,j2 i1,i2 \ni1,i2 1-m 0 1-n M1 = m . [0, 1] M2 = n . [0, 1] 0 m n whichprovesthe.rststatement.Nowassumethat R1 = \nR2 = \u00d8, whichmeansthatboth M1 and M2 arebounded andthat R = \u00d8. We have We will show that all the generator \nvertices of M belong to M, (1 - m)(1 - n) hence any of their convex combination (i.e. any element of \nM ) M1M2 = m . [0, 1] . n . [0, 1] mn belongstotheconvexclosureofM:Considerthegeneratorvertex M. By \ntaking in Eqn. (5) .s,is =. .s,i. =0 fors =1, 2,weobtainthatV .M. D = is s M an appropriate basis [28]: \nof =1 and V1,i1 V2,i2 . V i ' s 0 1 ps-1 . . J =Diag[J1 ...Jr] EXAMPLE 9. In Ex. 3, we multiplied the \nunbounded set of matrices .s I . . J1 R, Js = ..... ..... M depicted in Fig. 3 and de.ned by 2 vertices \nand 2 rays by the bounded set of vectors X depicted in Fig. 4 and generated A = R-1 . . . . . . .. . \n. . .. by 4 vertices, which resulted in the convex polyhedra Y depicted in Fig. 4 which is generated \nby 3 vertices and 2 rays (we omit .s I Jr .s redundant generators). with .s = .s and I =1 Regarding complexity, \nthis operation is quadratic w.r.t. the num\u00ad if .s is a real eigenvalue of M, ber of generators, which \nis itself exponential in the worst-case w.r.t. .s cos .s -.s sin .s 10 the number of constraints. In \npractice, we did not face complexity or .s = and I = .s sin .s .s cos .s 01 problems in our experiments, \napart from the high-dimensional con\u00ad i.s -i.s if .se and .se complex conjugate voyCar3 example described \nin \u00a77. Observe that by using generators and applying Thm. 1,we lose are eigenvalues of M, with .s > \n0 and .s . [0,p[. information about matrix shapes. In our case, we will perform only The Jordan form \nis useful because we can write a closed-form abstract matrix-vector multiplication, hence the number \nof entries th expression for its n power Jn =Diag[J1 n ...Jr n]. Each block of the product matrix (actually \na vector) will be the dimension of Js n is given by the space Rp. The multiplication of an abstract \nmatrix M and a oo oo .. .n-1 s .n-ps+1 s n n.n s concrete matrix R (MR or RM) can be computed exactly \nby ... .. .. ... . ps-1 1 considering the generators of M.  5. Abstract acceleration of loops without \nguards ..... ..... oo . .n-1 . s n .n s . Jn s = (6) 1 oo . .n-1 s . n . 1 .n s In this section, we \nconsider loops of the form oo and .n = .n 1 or .n ss s cos n.s - sin n.s sin n.s cos n.s while(true){x \n:= Axx} Given an initial set of states X at the loop head, the least inductive invariant at loop head \nis oo .n-k s n =0 for k> n.with the convention that A * X = {AnX | n = 0,n . Z} . Our goal is to compute \na template polyhedra matrix M such that k Hence, coef.cients of Js n have the general form aT (A * ) \n.M oo .n-k p cos((n - k). - r n k (7) ) .[., ., r, k](n)= given a template T on the coef.cients of the \nmatrices M . A * 2 . The key observation underlying our approach uses a well\u00adknown result from matrix \nalgebra. Any square matrix A can be written in a special form known as the Jordan normal form using a \nchange of basis transformation R: A = R-1JR and J = RAR-1 such that for any n An = R-1Jn R-1 R and Jn \n= RAn . As a result, instead of computing an abstraction of the set with . = 0, . . [0,p], r .{0, 1} \nand k = 0,in which r =1 enables converting the cosine into a sine. The precise expressions for ., ., \nr, k as functions of the position i, j in the matrix J are omitted here to preserve the clarity of presentation. \n Next, we observe that the closed form Js n speci.es the required shape .s( xs for all n = 0. For instance, \nif .s ms) for abstracting Jn is a real eigenvalue, we have .. - 1 A * = {I, A, A2,A3 m0 m1 ... mps ,. \n..} .. m0 .... . . m0 m1 . . . . m1 .... we will abstract the set . . . . . J * = {I, J, J2,J3 ,...} \n. .s : . mps-1 1. The block diagonal structure of J allows us to symbolically m0 compute the coef.cients \nof Jn as a function of n. \u00a75.1 presents details on the Jordan form and the symbolic representation Likewise, \n.( xis obtained by the union m) for the entire matrix Jn of Jn . 2. The form of J immediately dictates \nthe matrix shape .( x m) and the matrix subspace Mat. containing J * . 3. We then consider a .xed set \nT of linear template expressions over x  m. We use asymptotic analysis to compute bounds on each expression \nin the template. \u00a75.2 explains how this is computed. 4. Once we have computed an abstraction M; a(J * \n), we will return into the original basis by computing R-1MR to obtain an abstraction for A * , which \nis the desired loop acceleration. In this section, we assume arbitrary precision numerical computa\u00adtions. \nThe use of .nite precision computations is addressed in \u00a77.  5.1 The real Jordan normal form of a matrix \nRp\u00d7p A classical linear algebra result is that any matrix A . can be transformed in a real Jordan normal \nform by considering of the parameters for each individual Js n . PROPOSITION 1. Given the structure \nof the real Jordan normal form J,we may .x a matrix shape .( x= m) such that J * {I, J, J2 ,...}. Mat. \nand m . Rm xwith m = p,where p is the dimension of the square matrix J. Hence, we will work in a matrix \nsubspace, the dimension of which is less than or equal to the number of variables in the loop. This reduction \nof dimensions using matrix shapes is absolutely essential for our technique to be useful in practice. \n 5.2 Abstracting J * within template polyhedron matrices The principle. Let us .x a template expression \nmatrix T . Rq\u00d7m composed of linear expressions {T1,.. .,Tq} on parameters x m. Knowing the symbolic form \nof each Jn , we obtain a symbolic form mx(n)=.-1(Jn) for parameters x m, hence a symbolic form for linear \nexpressions ej (n)= Tj \u00b7 x m(n) . By deriving an upper bound  3 y1 m2 x m1 4 Figure 6. On the left-hand \nside, the octagon on the two non\u00adconstant coef.cients of the matrices An ,n = 0 of Ex. 10,that de.nes \nthe approximation M.{An | n = 0}. On the right-hand side, the image a(MX) in light gray of the box X \nin dark gray by the set M using the method of \u00a74.2. uj for each ej (n),n = 0, we obtain a sound approximation \nof the set {x. m(n) | n =0}, and hence of J * THEOREM 2 (Abstracting J * in template polyhedron matrices). \nThe template polyhedron matrix aT (J * ) =.([ Txm = ux]]) with xu =supn=0 Txm(n) is the best correct \noverapproximation of J * in the template polyhe\u00addra matrix domain de.ned by T . Moreover, any xu ' = \nxu de.nes a correct approximation of J * . PROOF. J * = Jn = .( x= m(n) | n = m(n)) .({x n=0 n=0 0}).ConsideringthematrixT \nandreferringto\u00a72.3, aT ({xm | Txsup Txm} m(n) | n = 0})= {xm = m .{m (n) | n=0} Txm | Tx = {mx| Txm \n= sup m(n)} = {xm = xu} n=0 D The approximation of the set of matrices J * reduces thus to the computation \nof an upper bound for the expressions ej (n). Computing upper bounds. To simplify the analysis, we restrict \ntemplate expressions Tj to involve at most 2 parameters from x m. As each parameter/coef.cient mk in \nmatrix Jn is of the form of Eqn. (7), we have to compute an upper bound for expressions of the form o \no n .n-k1 p \u00b51 cos((n - k1).1 - r1 ) k1 1 2 o o n .n-k2 p + \u00b52 2 cos((n - k2).2 - r2 2 ) (8) k2 with \n\u00b51,\u00b52 . R . \u00b51 =0. Computing bounds on this expression is at the heart of our technique. However, the \nactual derivations are tedious and do not contribute to the main insights of our approach. Hence we omit \nthe detailed derivations, and refer to the extended version [23] for details. The main properties of \nthe technique we implemented are that it computes exact bounds if the two involved eigenvalues are real \n(.1,.2 . {0,p});  exact bounds in reals if .1 = .2 . k1 = k2 =0 . \u00b52 =0,and reasonable bounds if k1 \n= k2 =0 is replaced by k1 > 0.k2 > 0;  no interesting bounds otherwise (because they are just the linear \n combination of the bounds found for each term). Concerning the choice of template expressions in our \nimplementa\u00adtion, we .xa parameter f and we consider all the expressions of the form (cf. logahedra [20]) \nk \u00a3 \u00b1ami \u00b1 (1-a)mj with a = \u00a3 , 0 =k =2, (9) 2The choice of f =1 corresponds to octagonal expressions. \nExamples. In Ex. 3 and Fig. 3 we showed the approximation of aset A * using octagonal template expression, \nwith the matrix A being a Jordan normal form with real eigenvalues 1.5 and 1. For instance, consider \nthe expression m2-m1 = n-1.5n in Example 2, which falls in the .rst case above. We look at the derivative \nof the function f(x)= x - 1.5x , we infer that .x = 3.6: f ' (x) < 0 by linearizing appropriately f ' \n(t), hence we can compute the least upper bound as max{f(n) | 0 =n =4}. Next, we give another example \nwith complex eigenvalues. EXAMPLE 10. Take a =0.8cos . and b =0.8sin . with . = p/6. and consider the \nloop while(true){x =a*x-b*y; y=a*x+b*y; x=x }. The trajectories (see Fig. 6 (right)) of this loop follow \nan inward spiral. The loop body transformation is f T 0.8cos . -0.8sin . 0 A = 0.8sin . 0.8cos . 0 0 \n01 with A already in real Jordan normal form. The matrix subspace f T m1 -m2 0 containing A * is of \nthe form M = m2 m1 0 . We have 0 01 m1(n)=0.8n cos n. and m2(n)= 0.8n sin n.; applying our bounding technique \non octagonal template constraints on x m,we obtain an approximation M of A * de.ned by the constraints \nm1 . [-0.29, 1.00] m1 +m2 . [-0.29, 1.12] m2 . [-0.15, 0.56] m1 -m2 . [-0.57, 1.00] Consider, for example, \nthe expression m1 + m2 =0.8n(cos n. + sin n.) in Example 10 below, which falls into the second case above \n(.1 = .2 . k1 = k2 =0 . \u00b52 =0). We .rst rewrite it as v f(x)= 0.8x 2sin(x.+ p 4 ).The term 0.8x being \ndecreasing, the least upper bound of f in reals is in the range x . [0,p/4.]. Hence we can consider the \nupper bound max{f(n) | 0 =n =lp/4.l}. The possible values (m1,m2) are plotted in Fig. 6 (right). Assuming \nan initial set X : x . [1, 3] . y . [0, 2], we compute a(MX) to be the polyhedron depicted in Fig. 6 \n(right). In this section, we have described the computation of a cor\u00adrect approximation M of J * in the \ntemplate polyhedron domain, from which we can deduce a correct approximation R-1MR of A * = R-1J * R. \nApplying Thm. 1, we are thus able to approxi\u00admate the set A * X of reachable states at the head of a \nlinear loop while(true){x := Axx} with the expression (R-1MR).X,where X is a convex polyhedron describing \nthe initial states.  6. Abstract acceleration of loops with guards In this section, we consider loops \nof the form while(Gxx = 0){x := Axx} modeled by the semantic function G .A, as explained in \u00a72.Given \nan initial set of states X, we compute an over-approximation of Y =(G . A) * (X) using a convex polyhedral \ndomain, which after unfolding is expressed as  Y = X . GAk. An (X) (10) n=10=k=n-1 The unfolding effectively \ncomputes the pre-condition of the guard G on the initial state X as GAk . 6.1 The simple technique The \nexpression (G . A) * unfolded in Eqn. (10)is too complex to be accelerated precisely. A simple technique \nto approximate it safely is to exploit the following inclusion: PROPOSITION 2. For any set X and linear \ntransformation G .A,  (G .A) * . id .(G .A) . (G . A * ) acceleration .nally, take into without guard \naccount the guard (11)  A G .A G .id G .A X X id (a) Original loop (b) Approximation Figure 7. Original \nloop and approximation in term of invariant in accepting (double-lined) location, illustrating Prop. \n2. Fig. 7 illustrates graphically Prop. 2: the invariant attached to the accepting location of Fig. \n7(a) is included in the invariant attached to the accepting location of Fig. 7(b). It is interesting \nto point out the fact that the abstract acceleration techniques described in [18, 39] make assumptions \non the matrix A and exploit convexity arguments so that the inclusion (11) becomes an equality. The idea \nbehind Prop. 2 is applied to matrix abstract domains to yield Prop. 3: R-1 PROPOSITION 3. Let A = JR \nwith J a real Jordan normal form, T a template expression matrix, M = R-1aT (J * )R and X a convex polyhedron, \nthen (G . A) * (X) can be approximated by the convex polyhedron o o X U (G .A) M. (X n G) (12) This approach \nessentially consists of partially unfolding the loop, as illustrated by Fig. 7(b), and reusing abstract \nacceleration without guard. However, since the guard is only taken into account after the actual acceleration, \nprecision is lost regarding the variables that are not constrained by the guard. Ex. 4 and Figures 4 \nand 5 in \u00a73 illustrate this weakness: y is constrained by the guard, whereas x remains unbounded.  6.2 \nComputing and exploiting bounds on the number of iterations. To overcome the above issue, we propose \na solution based on .nd\u00ading the maximal number of iterations N of the loop for any initial state in X, \nand then to abstract the set of matrices {A0 ,.. .,AN }instead of the set A * . The basic idea is that \nif there exists N = 0 such that AN X n G = \u00d8,then N is an upper bound on the num\u00adber of iterations of \nthe loop for any initial state in X. Bounding the number of iterations is a classical problem in termination \nanalysis, to which our general approach provides a new, original solution. The following theorem formalizes \nthis idea: We assume now a guarded linear transformation G . J where J is already a Jordan normal form. \nTHEOREM 3. Given a set of states X, a template expression matrix T and the set of matrix M = aT (J * \n),we de.ne G = Mn{M |.x . (X n G): GMx = 0} N =min{n | Jn .G} with the convention min \u00d8 = 8. G is the \nset of matrices M .M of which the image of at least one input state x . X satis.es G.If N is bounded, \nthen N (G .J) * (X)= (G .J)n(X) n=0 PROOF. AsJN .M \\ G,wehave.x . (X n G): GJN xx> 0, o or in other \nwords (JN X) n G = \u00d8. Thisimpliesthat (G . o J)N X n G = \u00d8 and (G .J)N+1X = \u00d8. D The de.nition of G and \nN in Thm. 3 can be transposed in the space of vectors using the matrix shape .: THEOREM 4. Under the \nassumption of Thm. 3, and considering x(Jn), we have m(n)= .-1 .-1(M) n .-1(G)= (13) {x m)x = 0} m |.x \n: x . X n G : G.( x N =min{n | mx(n) . .-1(G)} (14) Our approach to take into the guard is thus to compute \na .nite bound N with Eqns. (13)and (14) and to replace in Eqn. (12) M = R-1 \u00b7 aT (J * ) \u00b7 R with M \n' = R-1 \u00b7 aT ({Jn | 0 = n = N -1}) \u00b7 R and R the basis transformation matrix (see Prop. 3). EXAMPLE \n11. In our Examples 2-5, we had x(Jn)= m(n)=.-1 1.5n , .-1(M)=(m1 =1.m2 =0.m1+m2 =1.m1-m2 = n 0.25),see \nFig. 3 (light gray), X =(x . [1, 3] . y . [0, 2]),see Fig. 5 (dark gray), and G =(y = 3). The second \nterm of the intersection in Eqn. (13) evaluates to m2 = 3, thus .-1(G)= (m1 = 1 . 0 = m2 = 3 . m1 +m2 \n= 1 . m1 -m2 = 0.25).This gives us through Eqn. (14) the bound N =4 on the number of loop iterations. \nThe abstraction M ' = aT ({Jn | 0 =n =3}), depicted on Fig. 3 (dark gray), removes those matrices from \nM that do not contribute to the acceleration result due to the guard. Finally, we accelerate using M \n' and obtain the result shown in Fig. 5 (medium gray). More details about these computations are given \nin the next section. 6.3 Technical issues Applying Thms. 3 and 4 requires many steps. First, we approximate \n.-1(G) (see Eqn. (13)), and then as a second step we can approxi\u00admate the maximum number of iterations \nN according to Eqn. (14). Finally, we have to compute aT {Jn | 0 =n =N -1}. Approximating .-1(G). Let \nus denote Q =.-1(G) and P = .-1(M). Q is de.ned in Eqn. (13) by the conjunction of quadratic constraints \non x and xExact m followed by an elimination of x . solutions exist for this problem, but they are costly. \nThe alternative adopted in this paper is to approximate Q by quantifying x on the bounding box of X n \nG instead of quantifying it on X n G.Let us denote the bounding box of a polyhedron Z with the vector \nof intervals [Z, Z].We have Q . P n{x m)x = 0} m |.x : x . [X n G, X n G] . G.( x= P n{xm)[X n G, X n \nG] = 0} Q ' m | G.( x = (15) Q ' is de.nedbyintersecting P with interval-linear constraints on x m and \nit is much easier to compute than Q: one can use algorithms for interval linear constraints [6], in \nparticular inter\u00adval linear programming [33], or  the linearization techniques of [30] that are effective \nif the vectors x  m are well-constrained by P . In this paper, we exploit the last method which is \nimplemented in the APRON library [21]. EXAMPLE 12. Coming back to our running Examples 2-5 and 11, we \nhave . . . . m1 0 0 [1, 3] . . . . G.(mx)[X n G, X n G]= (0 1 -3) 0 1 m2 [0, 2] 001 1 . . [1, 3]m1 =(0 \n1 - 3) .[0, 2] + m2. = m2 +[-3, -1] 1 Hence Q ' = P n [[m2 +[-3, -1] =0]] = P n [[m2 =3]] Approximating \nthe maximum number of iterations N. Comput\u00ading N as de.ned in Eqn. (14) is not easy either, because the \ncom\u00adponents of vector x m(n) are functions of de.ned by Eqn. (7). Our approach is to exploit a matrix \nof template expressions. PROPOSITION 4. Under the assumption of Thm. 4, for any polyhe\u00addron Q ' . Q and \ntemplate expression matrix T ' , N = min n |.j : T ' m(n) > sup Tj ' \u00b7 x(16)\u00b7 xm j m .Q. PROOF. WehaveQ \n. Q ' . aT . (Q ' ).FromQ . aT . (Q ' ), itfollowsmx(n) . aT . (Q ' ) . x m(n) . Q, and min{n | xm(n) \n. Q} = N. m(n) . aT . (Q ' )}= min{n | xmx(n) . aT . (Q ' ) isequivalent to .j : Tj ' \u00b7 m(n) > sup Tj \n' \u00b7 x xm m .aT. (Q.) and sup Tj ' \u00b7 xTj ' \u00b7 x. D m =sup m,hencewegettheresult m .Q. m .aT. (Q.) In our \nimplementation we compute such a Q ' as described in the previous paragraph and we choose for T ' the \ntemplate matrix T considered in \u00a75.2, to which we may add the constraints of Q ' . The computation of \nsuch minima ultimately relies on the Newton-Raphson method for solving equations. Our implementa\u00adtion \ndeals with the same cases as those mentioned in \u00a75.2 and in other cases safely returns Nj = 8. We refer \nto the extended ver\u00adsion [23] for details. EXAMPLE 13. Coming back to Examples 2-5 and 11-12, and con\u00adsidering \nTj =(0 1), we have 1.5n Tj \u00b7 x= n, ej =sup Tj \u00b7m =sup m2 =3m(n)=(0 1)\u00b7 x n m .Q. m .Q. and Nj =min{n \n| Tj \u00b7 x m(n) >ej } =4, which proves that 4 is an upper bound on the maximum number of iterations of \nthe loop, as claimed in Ex. 5. Computing aT {Jn | 0 = n = N -1}. If no .nite upper bound N is obtained \nwith Prop. 4 (given an input polyhedron X), then we apply the method of \u00a76.1. Otherwise, we replace in \nEqn. (12)the set M = aT (A * ) with M ' = aT ({Jn | 0 =n =N-1}).This set M ' can be computed using the \nsame technique as those mentioned in \u00a75.2 anddetailedin[23], or even by enumeration if N is small. Ex. \n5 and Figs. 3-5 illustrate the invariant we obtain this way on our running example. EXAMPLE 14 (Running \nexample with more complex guard). Coming back to Examples 2-5, we consider the same loop but with the \nguard x +2y = 100 represented by the ma\u00adtrix (1 2 - 100). Compared to Ex. 12 we have now G.( x[1, 3]m1 \n- 100, m)[X n G, X n G]= +[0, 4] + 2m2 hence, if P is de.ned by .M as in Ex. 2, Q ' = P n [[[1, 3]m1 \n+2m2 =[96, 100]]] . P n [[m1 +2+2m2 =100]] linearization based on P . m1 =1 . m1 +2=[1, 3]m1 Using Q \n' to bound the number of iterations according to Eqn. (16) leads to N =11 (obtained with the template \nexpression Tj =(1 2) tasken from the guard). At last we obtain the invariant Z depicted in Fig. 8. If \ninstead of octagonal template expressions with f =1 in Eqn. (9), we choose f =3, we do not improve the \nbound N =11 but we still obtain the better invariant Z ' on Fig. 8.  6.4 Summary We summarize now our \nmethod in the general case: Given a guarded linear transformation G . A, with J = RAR-1 the (real) Jordan \nnormal form of A, its associated matrix shape ., a template expression matrix T and a convex polyhedron \nFigure 8. Initial set of states X (dark gray), loop invariants Z (light gray) and Z ' (medium gray) obtained \nin Ex. 14.  X representing a set of states, we compute an overapproximation (G .A) * (X) with o o X \nU (G .A) (R-1M) . Y where 1. Y = R(X n G), 2. P = aT (.-1(J * )), 3. Q ' = P n{GR-1 m)[Y , Y ] = 0}, \n .( x 4. N approximated by some N ' using Prop. 4, aT (J * ) if N ' = 8 5. M = aT ({Jn | 0 =n =N ' \n-1}) otherwise  7. Implementation and Experiments We implemented the presented approach in a prototype \ntool, eval\u00aduated over a series of benchmark examples with various kinds of linear loops, possibly embedded \ninto outer loops, and compared it to state-of-the-art invariant generators. 7.1 Implementation We integrated \nour method in an abstract interpeter based on the APRON library [21]. We detail below some of the issues \ninvolved. Computation of the Jordan normal form To ensure soundness, we have taken a symbolic approach \nusing the computer algebra software SAGE1 for computing the Jordan normal forms and trans\u00adformation matrices \nover the .eld of algebraic numbers. The matri\u00adces are then approximated by safe enclosing interval matrices. \nLoops with conditionals Loops of which the body contains con\u00additionals like while(G){ if (C) x ' = A1x \nelse x ' = A2x } can be transformed into two self-loops t1,t2 around a head location that are executed \nin non-deterministic order: We iteratively accel- G .\u00acC .A2 G . C . A1 erate each self-loop separately \n(t1 . . t2 .) * . Since convergence of the outer loop (\u00b7) * is not guaranteed in general, we might have \nto use widening. Yet, practical experience shows that in many cases a .xed point is reached after a few \niterations. Nested loops We could use a similar trick to transform nested loops while(G1){while(G2){x \n' = A2x }; x ' = A1x } into multiple linear self-loops t1,t2,t3 by adding a variable y (initialized to \n0) to encode the control .ow: t1 :(G1x =0 . y =0) . (x ' =x . y ' =1) t2 :(G2x =0 . y =1) . (x ' = A2x \n. y ' =1) t3 :(G2xx> 0 . y =1) . (x ' = A1x . y ' =0) However, the encoding of the control .ow in an \ninteger variable is ineffective because of the convex approximation of the polyhedral 1 www.sagemath.org \n characteristics inferred bounds analysis time (sec) name type smax #var #bds IProc Sti J J vsIProc \nJvs Sti IProc Sti J JSage JAna Examples with single loops parabola i1 \u00acs,\u00acc,g 3 3 60 46 38 54 +8,+17 \n+16, +12 0.007 237 2.509 2.494 0.015 parabola i2 \u00acs,\u00acc,g 3 3 60 36 32 54 +18, +6 +22, +13 0.008 289 2.509 \n2.494 0.015 cubic i1 \u00acs,\u00acc,g 4 4 80 54 34 72 +18,+26 +38, +12 0.015 704 2.474 2.393 0.081 cubic i2 \u00acs,\u00acc,g \n4 4 80 34 28 72 +38, +9, -2 +44, +11 0.018 699 2.487 2.393 0.094 exp div \u00acs,\u00acc,g 2 2 32 24 21 28 +4, \n+6, -2 +7, +7 0.004 31.6 2.308 2.299 0.009 oscillator i0 s,c,\u00acg 1 2 28 1 0 24 +23, +0, -1 +24, +0 0.004 \n0.99 2.532 2.519 0.013 oscillator i1 s,c,\u00acg 1 2 28 0 0 24 +24, +0 +24, +0 0.004 1.06 2.532 2.519 0.013 \ninv pendulum s,c,\u00acg 1 4 8 0 0 8 +8, +0 +8, +0 0.009 0.920 65.78 65.24 0.542 convoyCar2 i0 s,c,\u00acg 2 5 \n20 3 4 9 +6, +0 +5, +1 0.007 0.160 5.461 4.685 0.776 convoyCar3 i0 s,c,\u00acg 3 8 32 3 3 15 +12, +0 +12, \n+0 0.010 0.235 24.62 11.98 12.64 convoyCar3 i1 s,c,\u00acg 3 8 32 3 3 15 +12, +0 +12, +0 0.024 0.237 23.92 \n11.98 11.94 convoyCar3 i2 s,c,\u00acg 3 8 32 3 3 15 +12, +0 +12, +0 0.663 0.271 1717 11.98 1705 convoyCar3 \ni3 s,c,\u00acg 3 8 32 3 3 15 +12, +0 +12, +0 0.122 0.283 1569 11.98 1557 Examples with nested loops thermostat \ns,\u00acc,g 2 3 24 18 24 24 +6, +1, -1 +0, +6 0.004 0.404 4.391 4.379 0.012 oscillator2 16 \u00acs,c,g 1 2 48 9t.o. \n48 +27, +0 t.o. 0.003 t.o. 4.622 4.478 0.144 oscillator2 32 \u00acs,c,g 1 2 48 9t.o. 48 +39, +0 t.o. 0.003 \nt.o. 4.855 4.490 0.365 s/\u00acs : stable/unstable loop; c/\u00acc : has complex eigenvalues or not; g/\u00acg : loops \nwith/without guard; smax : size of largest Jordan block(s); #var : nb. of variables; #bds : nb. of bounds \nto be inferred at all control points; IProc , Sti , J :nb. of .nite bounds inferred by I NTERP ROC,STI \nNG, and our method ( J ); J vs IProc , J vs Sti : +x,+y,[-z] :nb. of in.nite bounds becoming .nite, nb. \nof improved .nite bounds[, nb. of less precise .nite bounds (omitted if 0)] obtained with our method \nover I NTERP ROC,STI NG; analysis time : running times (seconds), with JSage , JAna corresponding to \ncomputation of the Jordan normal form using S AG E and the analysis itself, and J being the sum of these \ntwo times. t.o. means time out after one hour. Table 1. Experimental results. abstract domain, this transformation \ncauses an inacceptable loss of precision. For this reason, we accelerate only inner loops in nested loops \nsituations. Our experimental comparison shows that comput\u00ading precise over-approximations of inner loops \ngreatly improves the analysis of nested loops, even if widening is applied to outer loops.  7.2 Evaluation \nBenchmarks Our benchmarks listed in Table 1 include various examples of linear loops as commonly found \nin control software. They contain .lters and integrators that correspond to various cases of linear transformations \n(real or complex eigenvalues, size of Jor\u00addan blocks). parabola and cubic are loops with polynomial behav\u00adior \n(similar to Fig. 1), exp div is Ex. 5 and thermostat is Ex. 1. inv pendulum is the classical model of \na pendulum balanced in up\u00adright position by movements of the cart it is mounted on. oscillator is a damped \noscillator, and oscillator2 models a pendulum that is only damped in a range around its lowest position, \nas if it was graz\u00ading the ground, for example. This is modeled using several modes. In ConvoyCar [36], \na leading car is followed by one or more cars, trying to maintain their position at 50m from each other: \n car2 car1 car0 x2 x1 x0 The equations for N - 1 following cars are: for all i . [1,N -1] : x\u00a8i = c(.xi-1 \n- x.i)+ d(xi-1 - xi - 50) We analyzed a discretized version of this example to show that there is no \ncollision, and to compute bounds on the relative po\u00adsitions and speeds of the cars. This example is particularily \ninter\u00adesting because the real Jordan form of the loop body has blocks associated to complex eigenvalues \nof size N - 1. All benchmarks have non-deterministic initial states (typically bounding boxes). Some \nbenchmarks where analyzed for different sets of initial states (indicated by suf.x ix). Comparison Existing \ntools can only handle subsets of these ex\u00adamples with reasonable precision. We compared our method with \n INT E R PROC [22] that implements standard polyhedral analysis with widening [8];  STI NG that implements \nthe method of [7, 36]. A comparison with the AST R \u00b4  EE tool on the thermostat example has been given \nin Section 3. A detailed qualitative comparison between various methods described in \u00a78 is shown in Table \n2. Results Table 1 lists our experimental results.2 We compared the tools based on the number of .nite \nbounds inferred for the program variables in each control point. Where applicable, we report the number \nof bounds more/less precise .nite bounds inferred by our tool in comparison to the other tools. We note \nthat our analysis dramatically improves the accuracy over the two competing techniques. On all the benchmarks \nconsid\u00adered, it generally provides strictly stronger invariants and is practi\u00adcally able to infer .nite \nvariable bounds whenever they exist. For instance, for the thermostat example (Fig. 2), we infer that \nat least 6.28 and at most 9.76 seconds are continuously spent in heating mode and 10.72 to 12.79 seconds \nin cooling mode. I NT E R PROC just reports that time is non-negative and STI NG is able to .nd the much \nweaker bounds [0.88, 13.0] and [0.94, 19.0].On the convoy-Car examples, our method is the only one that \nis able to obtain non-trivial bounds on distances, speeds and accelerations. Yet, this comes at the price \nof increased computation times in general. INT E R PROC is signi.cantly faster on all examples; STI NG \nis faster on half of the benchmarks and signi.cantly slower on the other half: for two examples, S TI \nNG does not terminate within the given timeout of one hour, whereas our tool gives precise bounds after \na few seconds. It must be noted that part of the higher com\u00adputation time of our tool is a one-time investment \nfor comput\u00ading loop accelerations that can pay off for multiple applications in the course of a deployment \nof our approach in a tool such as AST R \u00b4 EE. In all but two of the examples (the exceptions be\u00ading convoyCar3 \ni[2|3]), the one-time cost dominates the overall 2 A detailed account of the benchmarks and the obtained \ninvariants can be found on http://www.cs.ox.ac.uk/people/peter.schrammel /acceleration/jordan/.   type \nof linear transformation illustrating examples linear abstr. accel. this paper [2, 18] relational ellipsoids \nabstraction [13, 31, 34][27, 32, 35, 41] ALIGATOR [26] STING [7, 36] translations/resets .i =1.sk =2..i \n=0 yes yes no yes yes yes polynomials .i .{0, 1} parabola, cubic yes no no no yes no exponentials .i \n. R+ . sk =1 exp div yes no no if |.i|< 1 no if |.i|< 1 rotations .i . C . sk =1 oscillator, oscillator2, \ninv pendulum yes no if |.i|< 1 no no no non-diagonalizable &#38; non-polynomial sk =2 . .i /.{0, 1} thermostat, \nconvoyCar yes no no no no if |.i|< 1 unstable &#38; non-polynomial |.i|> 1 exp div, oscillator2 yes no \nno no no no loop guard handling parabola, cubic, exp div, thermostat, oscillator2 yes yes partially yes \nno yes inputs/noise no yes yes yes no yes abstract domain polyhedra polyhedra ellipsoids template polyhedra \npolynomial equalities polyhedra sk is the size of a Jordan block and .i its associated eigenvalue. Table \n2. Classi.cation of linear loops and the capabilities of various analysis methods to infer precise invariants \ncost of our analysis. All in all, computation times remain reason\u00adable in the view of the tremendous \ngain in precision.  8. Related work Invariants of linear loops. The original abstract acceleration \ntechnique of Gonnord et al [11, 18] precisely abstracts linear loops performing translations, translations/resets \nand some other special cases. The af.ne derivative closure method [2] approximates any loop by a translation; \nhence it can handle any linear transformation, but it is only precise for translations. The tool INVGEN \n[19]uses template-based constraint solving techniques for property-directed invariant generation, but \nit is restricted to integer programs. Methods from linear .lter analysis target stable linear systems \nin the sense of Lyapunov stability. These techniques consider ellip\u00adsoidal domains. Unlike our method, \nthey are able to handle inputs, but they do not deal with guards. Feret [12, 13] designs specialized \ndomains for .rst and second order .lters. These methods are imple\u00admented in the ASTR \u00b4 EE tool [4]. Monniaux \n[31] computes interval bounds for composed .lters. Roux et al [34] present a method based on semide.nite \nprogramming that infers both shape and ratio of an ellipsoid that is an invariant of the system. In contrast, \nour method does not impose any stability requirement. Colon etal[7, 36] describe a method, implemented \nin the tool STING, for computing single inductive, polyhedral invariants for loops based on non-linear \nconstraint solvers. It is a computationally expensive method and in contrast, our approach is able to \ninfer sound polyhedral over-approximations for loops where the only inductive polyhedral invariant is \ntrue. Relational abstraction methods [27, 35, 41]aim at .nding a relation between the initial state x \nand any future state x ' in contin\u00aduous linear systems dxx(t)/dt = Axx(t), which is a problem similar \nto the acceleration of discrete linear loops. The invariants are com\u00adputed based on off-the-shelf quanti.er \nelimination methods over real arithmetic. In contrast to our method, they handle only diago\u00adnalizable \nmatrices A.[32] proposes a similar approach for discrete loops. All these works compute a (template) \npolyhedral relation be\u00adtween input and output states, and do not take into account the ac\u00adtual input \nstates. Hence, in contrast to our method, they are unable to capture accurately unstable and rotating \nbehavior. Strategy iteration methods [15 17] compute best inductive in\u00advariants in the abstract domain \nwith the help of mathematical pro\u00adgramming. They are not restricted to simple loops and they are able \nto compute the invariant of a whole program at once. However, they are restricted to template domains, \ne.g. template polyhedra and quadratic templates, and hence, unlike our method, they are unable to infer \nthe shape of invariants. The ALIGATOR tool [26] infers loop invariants that are poly\u00adnomial equalities \nby solving the recurrence equations representing the loop body in closed form. The class of programs \nthat can be handled by Aligator is incomparable to that considered in this pa\u00adper. Whereas Aligator handles \na subset of non-linear polynomial assignments our work is currently restricted to linear assignments. \nIn contrast, we take into account linear inequality loop conditions, and can compute inequality invariants. \nBounding loop iterations. Many papers have investigated the problem of bounding the number of iterations \nof a loop, either for computing widening thresholds [40] by linear extrapolation, termination analysis \n[1] using ranking functions or for WCET analysis [25] based on solving recurrence equations. Yazarel \nand Pappas [42] propose a method for computing time intervals where a linear continuous system ful.lls \na given safety property. Their method can handle complex eigenvalues by performing the analysis in the \npolar coordinate space. However, they can only deal with diagonalizable systems. All these methods assume \ncertain restrictions on the form of the (linear) loop, whereas our approach applies to any linear loop. \n 9. Conclusion We presented a novel abstract acceleration method for discovering polyhedral invariants \nof loops with general linear transformations. It is based on abstracting the transformation matrices \ninduced by any number of iterations, polyhedral matrix multiplication, and a method for estimating the \nnumber of loop iterations that also works in case of exponential and oscillating behavior. Our experiments \nshow that we are able to infer invariants that are out of the reach of existing methods. The precise \nanalysis of linear loops is an essential feature of static analyzers for control programs. Precise loop \ninvariants are equally important for alternative veri.cation methods based on model checking, for example. \nFurther possible applications include termination proofs and deriving complexity bounds of algorithms. \n Ongoing work. In this paper, we considered only closed systems, i.e. without inputs. However, important \nclasses of programs that we want to analyze, e.g. digital .lters, have inputs. Hence, we are extending \nour methods to loops of the form Gxx + H.x= 0 . x ' = Axx + B.xwith inputs .x(similar to [39]). Moreover, \nour method easily generalizes to linear continuous systems dxx(t)/dt = Axx(t), e.g. representing the \nmodes of a hybrid automaton, by considering their analytical solution x (t)= e At x (0).  References \n[1] C. Alias, A. Darte, P. Feautrier, and L. Gonnord. Multi-dimensional rankings, program termination, \nand complexity bounds of .owchart programs. In SAS, volume 6337 of LNCS, pages 117 133, 2010. [2] C. \nAncourt, F. Coelho, and F. Irigoin. A modular static analysis approach to af.ne loop invariants detection. \nIn NSAD, volume 267 of ENTCS, pages 3 16. Elsevier, 2010. [3] S. Bardin, A. Finkel, J. Leroux, and L. \nPetrucci. Fast: acceleration from theory to practice. STTT, 10(5):401 424, 2008. [4] B. Blanchet, P. \nCousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, D. Monniaux, and X. Rival. A static analyzer for \nlarge safety-critical software. In PLDI, pages 196 207. ACM, 2003.  [5] B. Boigelot and P. Godefroid. \nSymbolic veri.cation of communication protocols with in.nite state spaces using QDDs. In CAV, volume \n1102 of LNCS, July 1996. [6] L. Chen, A. Min\u00b4e, J. Wang, and P. Cousot. An abstract domain to discover \ninterval linear equalities. In VMCAI, volume 5944 of LNCS, 2010. [7] M. Col\u00b4on, S. Sankaranarayanan, \nand H. Sipma. Linear invariant generation using non-linear constraint solving. In Computer Aided Veri.cation, \nCAV 03, volume 2725, 2003. [8] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among \nvariables of a program. In Symposium on Principles of pro\u00adgramming languages, POPL 78, pages 84 96, 1978. \n. [9] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among the variables of a program. \nIn POPL, pages 84 97, 1978. [10] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, and X. Rival. \nWhy does Astr\u00b4ee scale up? FMSD, 35(3), 2009. [11] P. Feautrier and L. Gonnord. Accelerated invariant \ngeneration for C programs with Aspic and C2fsm. ENTCS, 267(2):3 13, 2010. [12] J. Feret. Static analysis \nof digital .lters. In ESOP, volume 2986 of LNCS, pages 33 48, 2004. [13] J. Feret. Numerical abstract \ndomains for digital .lters. In Numerical and Symbolic Abstract Domains, 2005. [14] A. Finkel and J. Leroux. \nHow to compose Presburger-accelerations: Applications to broadcast protocols. In FSTTCS, volume 2556 \nof LNCS, pages 145 156, 2002. [15] S. Gaubert, E. Goubault, A. Taly, and S. Zennou. Static analysis by \npolicy iteration on relational domains. In ESOP, volume 4421 of LNCS, 2007. [16] T. M. Gawlitza and H. \nSeidl. Precise relational invariants through strategy iteration. In Computer Science Logic, volume 4646 \nof LNCS, pages 23 40. Springer, 2007. \u00b4stract interpretation meets convex optimization. Journal of Symbolic \nComputation, 47(12):1512 1532, 2012. [17] T. M. Gawlitza, H. Seidl, A. Adj\u00b4e, S. Gaubert, and E. Goubault. \nAb\u00ad [18] L. Gonnord and N. Halbwachs. Combining widening and acceleration in linear relation analysis. \nIn SAS, volume 4218 of LNCS, 2006. [19] A. Gupta and A. Rybalchenko. InvGen: an ef.cient invariant genera\u00adtor. \nIn CAV, volume 5643 of LNCS, pages 634 640, 2009. [20] J. M. Howe and A. King. Logahedra: A new weakly \nrelational domain. In Automated Technology for Veri.cation and Analysis, ATVA 09, volume 5799 of LNCS, \npages 306 320. Springer, 2009. [21] B. Jeannet and A. Min\u00b4e. APRON: A library of numerical abstract domains \nfor static analysis. In CAV, volume 5643 of LNCS, pages 661 667, 2009. http://apron.cri.ensmp.fr/library/. \n [22] B. Jeannet, M. Argoud, and G. Lalire. The INTERPROC interprocedural analyzer. http://pop-art.inrialpes.fr/interproc/interprocweb.cgi. \n [23] B. Jeannet, P. Schrammel, and S. Sankaranarayanan. Abstract accel\u00aderation of general linear loops. \nCoRR, abs/1311.768, 2013. [24] J. B. Kam and J. D. Ullman. Monotone data .ow analysis frameworks. Acta \nInformatica, 7:305 317, 1977. [25] J. Knoop, L. Kov\u00b4acs, and J. Zwirchmayr. Symbolic loop bound computation \nfor wcet analysis. In Perspectives of Systems Informatics, volume 7162 of LNCS, pages 227 242. Springer, \n2011. [26] L. Kov\u00b4acs. Invariant generation for p-solvable loops with assignments. In CSR, volume 5010 \nof LNCS, pages 349 359, 2008. [27] G. Lafferriere, G. J. Pappas, and S. Yovine. Symbolic reachability \ncomputation for families of linear vector .elds. JSC, 32(3):231 253, 2001. [28] P. Lancaster and M. Tismenetsky. \nThe Theory of Matrices (2nd edition). Academic Press, 1984. [29] A. Min\u00b4e. The octagon abstract domain. \nIn AST 2001 in WCRE 2001, IEEE, pages 310 319. IEEE CS Press, October 2001. [30] A. Min\u00b4e. Symbolic methods \nto enhance the precision of numerical abstract domains. In VMCAI, volume 3855 of LNCS, pages 348 363, \n2006. [31] D. Monniaux. Compositional analysis of .oating-point linear numer\u00adical .lters. In CAV, volume \n3576 of LNCS, pages 199 212, 2005. [32] D. Monniaux. Automatic modular abstractions for linear constraints. \nIn POPL. ACM, 2009. [33] J. Rohn. Solvability of systems of interval linear equations and in\u00adequalities. \nIn Linear Optimization Problems with Inexact Data, pages 35 77, 2006. [34] P. Roux, R. Jobredeaux, P.-L. \nGaroche, and E. Feron. A generic ellipsoid abstract domain for linear time invariant systems. In HSCC, \npages 105 114. ACM, 2012. [35] S. Sankaranarayanan and A. Tiwari. Relational abstractions for con\u00adtinuous \nand hybrid systems. In CAV, volume 6806 of LNCS, pages 686 702. Springer, 2011. [36] S. Sankaranarayanan, \nH. B. Sipma, and Z. Manna. Constraint-based linear-relations analysis. In SAS, volume 3148 of LNCS, pages \n53 68, 2004. [37] S. Sankaranarayanan, H. B. Sipma, and Z. Manna. Scalable analysis of linear systems \nusing mathematical programming. In VMCAI, volume 3385 of LNCS, 2005. [38] P. Schrammel and B. Jeannet. \nLogico-numerical abstract acceleration and application to the veri.cation of data-.ow programs. In SAS, \nvolume 6887 of LNCS, pages 233 248, 2011. [39] P. Schrammel and B. Jeannet. Applying abstract acceleration \nto (co-)reachability analysis of reactive programs. Journal of Symbolic Computation, 47(12):1512 1532, \n2012. [40] A. Simon and A. King. Widening polyhedra with landmarks. In Prog. Languages and Systems, APLAS \n06, volume 4279 of LNCS, 2006. [41] A. Tiwari. Approximate reachability for linear systems. In HSCC, \nvolume 2623 of LNCS, pages 514 525. Springer, 2003. [42] H. Yazarel and G. J. Pappas. Geometric programming \nrelaxations for linear system reachability. In American Control Conference, pages 553 559, 2004.  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>We present abstract acceleration techniques for computing loop invariants for numerical programs with linear assignments and conditionals. Whereas abstract interpretation techniques typically over-approximate the set of reachable states iteratively, abstract acceleration captures the effect of the loop with a single, non-iterative transfer function applied to the initial states at the loop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without restrictions. Its novelty lies in the use of the <i>Jordan normal form</i> decomposition of the loop body to derive symbolic expressions for the entries of the matrix modeling the effect of &#951; &#8805; &#927; iterations of the loop. The entries of such a matrix depend on &#951; through complex polynomial, exponential and trigonometric functions. Therefore, we introduces an <i>abstract domain for matrices</i> that captures the linear inequality relations between these complex expressions. This results in an abstract matrix for describing the fixpoint semantics of the loop.</p> <p>Our approach integrates smoothly into standard abstract interpreters and can handle programs with nested loops and loops containing conditional branches. We evaluate it over small but complex loops that are commonly found in control software, comparing it with other tools for computing linear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and oscillatory behaviors that present challenges to existing approaches. Our approach finds non-trivial invariants to prove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches in terms of precision while exhibiting good performance.</p>", "authors": [{"name": "Bertrand Jeannet", "author_profile_id": "81100333231", "affiliation": "INRIA, Grenoble, France", "person_id": "P4383896", "email_address": "bertrand.jeannet@inria.fr", "orcid_id": ""}, {"name": "Peter Schrammel", "author_profile_id": "81470652628", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P4383897", "email_address": "peter.schrammel@cs.ox.ac.uk", "orcid_id": ""}, {"name": "Sriram Sankaranarayanan", "author_profile_id": "81100300829", "affiliation": "University of Colorado, Boulder, CO, USA", "person_id": "P4383898", "email_address": "srirams@colorado.edu", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535843", "year": "2014", "article_id": "2535843", "conference": "POPL", "title": "Abstract acceleration of general linear loops", "url": "http://dl.acm.org/citation.cfm?id=2535843"}