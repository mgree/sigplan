{"article_publication_date": "01-08-2014", "fulltext": "\n CakeML: A Veri.ed Implementation of ML Ramana Kumar * 1 Magnus O. Myreen 1 Michael Norrish 2 Scott \nOwens 3 1 Computer Laboratory, University of Cambridge, UK 2 Canberra Research Lab, NICTA, Australia \n 3 School of Computing, University of Kent, UK Abstract We have developed and mechanically veri.ed an \nML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as \nan interactive read-eval-print loop (REPL) in x86-64 machine code. Our correctness theorem ensures that \nthis REPL implementation prints only those results permitted by the semantics of CakeML. Our veri.cation \neffort touches on a breadth of topics including lexing, parsing, type checking, in\u00adcremental and dynamic \ncompilation, garbage collection, arbitrary\u00adprecision arithmetic, and compiler bootstrapping. Our contributions \nare twofold. The .rst is simply in build\u00ading a system that is end-to-end veri.ed, demonstrating that \neach piece of such a veri.cation effort can in practice be composed with the others, and ensuring that \nnone of the pieces rely on any over-simplifying assumptions. The second is developing novel ap\u00adproaches \nto some of the more challenging aspects of the veri\u00ad.cation. In particular, our formally veri.ed compiler \ncan boot\u00adstrap itself: we apply the veri.ed compiler to itself to produce a veri.ed machine-code implementation \nof the compiler. Addition\u00adally, our compiler proof handles diverging input programs with a lightweight \napproach based on logical timeout exceptions. The en\u00adtire development was carried out in the HOL4 theorem \nprover. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation \nCorrectness proofs, Formal methods; F.3.1 [Logics and meanings of programs]: Specifying and Verifying \nand Reasoning about Programs Mechanical veri\u00ad.cation, Speci.cation techniques, Invariants Keywords Compiler \nveri.cation; compiler bootstrapping; ML; machine code veri.cation; read-eval-print loop; veri.ed parsing; \nveri.ed type checking; veri.ed garbage collection. * supported by the Gates Cambridge Trust supported \nby the Royal Society, UK NICTA is funded by the Australian Government through the Department of Communications \nand the Australian Research Council through the ICT Centre of Excellence Program. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. Copyrights for components of this work owned by others \nthan the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA.. Copyright is \nheld by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2544-8/14/01. . . $15.00. \nhttp://dx.doi.org/10.1145/2535838.2535841  1. Introduction The last decade has seen a strong interest \nin veri.ed compilation; and there have been signi.cant, high-pro.le results, many based on the CompCert \ncompiler for C [1, 14, 16, 29]. This interest is easy to justify: in the context of program veri.cation, \nan unveri.ed compiler forms a large and complex part of the trusted computing base. However, to our knowledge, \nnone of the existing work on veri.ed compilers for general-purpose languages has addressed all aspects \nof a compiler along two dimensions: one, the compilation algorithm for converting a program from a source \nstring to a list of numbers representing machine code, and two, the execution of that algorithm as implemented \nin machine code. Our purpose in this paper is to explain how we have veri.ed a compiler along the full \nscope of both of these dimensions for a practical, general-purpose programming language. Our language \nis called CakeML, and it is a strongly typed, impure, strict functional language based on Standard ML \nand OCaml. By veri.ed, we mean that the CakeML system is ultimately x86-64 machine code along\u00adside a \nmechanically checked theorem in higher-order logic saying that running that machine code causes an input \nprogram to yield output or diverge as speci.ed by the semantics of CakeML. We did not write the CakeML \ncompiler and platform directly in machine code. Instead we write it in higher-order logic and synthe\u00adsise \nCakeML from that using our previous technique [22], which puts the compiler on equal footing with other \nCakeML programs. We then apply the compiler to itself, i.e., we bootstrap it. This avoids a tedious manual \nre.nement proof relating the compilation algorithm to its implementation, as well as providing a moderately \nlarge example program. More speci.cally, we write, and can run, the compiler as a function in the logic, \nand we synthesise a CakeML implementation of the compiler inside the logic;  we bootstrap the compiler \nto get a machine-code implementa\u00adtion inside the logic; and  the compiler correctness theorem thereby \napplies to the machine-code implementation of the compiler. Another consequence of bootstrapping is that \nwe can include the  compiler implementation as part of the runtime system to form an interactive read-eval-print \nloop (REPL). A veri.ed REPL enables high-assurance applications that provide interactivity, an important \nfeature for interactive theorem provers in the LCF tradition, which were the original motivation for \nML. Contributions Semantics that are carefully designed to be simultaneously suit\u00adable for proving meta-theoretic \nlanguage properties and for sup\u00adporting a veri.ed implementation. (Section 3)  An extension of a proof-producing \nsynthesis pathway [22] orig\u00adinally from logic to ML, now to machine code (via veri.ed compilation). (Sections \n4 6, 10)   A lightweight method for proving divergence-preservation us\u00ading a clock and timeout exceptions \n these only appear in the proof, not in the implementation. It allows us to do all of our compiler proofs \nby induction and in the direction of compila\u00adtion. (Section 7)  To the best of our knowledge, this is \nthe .rst bootstrapping of a formally veri.ed compiler. The bootstrapping is done within the logic so \nthat the compiler correctness theorem can be in\u00adstantiated to apply directly to the compiler s output. \n(Section 9)  Result The result of this work is an end-to-end veri.ed REPL implementation in 64-bit x86, \nincluding lexing, parsing, type in\u00adference, compilation, garbage collection, printing, and arbitrary\u00adprecision \ninteger arithmetic. The entire formal development was carried out in the HOL4 theorem prover [8]. All \nof our de.nitions and proofs are available at https://cakeml.org. 2. Approach The CakeML system and veri.cation \ndivides into three layers. At the top is the semantic speci.cation of the REPL. At the bottom is machine \ncode implementing the REPL. In between is an implementation of the REPL as a function in logic. We describe \nthe components of each layer in more detail below, then give an overview of the proof that each layer \nimplements the one above it. The REPL Speci.cation The semantics for the CakeML REPL is given by a relation \nREPLs : bool list . string . repl result . bool between a string representing the read input and a repl \nresult, which is a list of strings representing the printed output, ending in a nil value that indicates \nwhether the REPL has terminated or di\u00adverged. The bool list argument indicates which input declarations \nhave type errors (needed because we have not yet veri.ed com\u00adpleteness for the type inferencer; see Section \n5). The input string is treated as a series of declarations separated by semicolons. Each declaration \nmight yield an output string de\u00adscribing a result value or reporting a lex, parse, or type error; or \nit might diverge. In the de.nition of REPLs, we apply an executable lexer speci\u00ad.cation to the entire \ninput string and split the resulting list of tokens at top-level semicolons, yielding a list of possibly \ninvalid declara\u00adtions.1 For each declaration, we pick a valid parse tree according to the CakeML grammar, \nif one exists. If none exists, the result for that declaration is a parse error. We then translate away \nsome syntactic sugar, and resolve the scoping of types and datatype con\u00adstructors. CakeML has a declarative \n(non-algorithmic) typing relation in the standard style. If the declaration has a type, we move on to \nthe operational semantics. If it does not have a type, then the semantics gets stuck or signals a type \nerror depending on the corresponding value in the bool list. 1 We use an executable speci.cation instead \nof a regular expression-based one because ML comments are not expressible with regular expressions. \nThe operational semantics is given as a big-step relation, on which our compiler correctness proofs can \nall proceed by induc\u00adtion since both the compiler and big-step semantics are de.ned inductively over \nCakeML abstract syntax. To precisely de.ne di\u00advergence, and to support a syntactic type-soundness proof \n[31], we also give a small-step semantics for expressions as a CEK machine [5], and prove that it is \nequivalent to the big-step relation. If the declaration diverges, REPLs relates it to the repl result \ndivergence-nil value. If it terminates normally or with an un\u00adhandled exception, or if a parse or type \nerror occurred, REPLs up\u00addates its state, converts the result to a string and conses it to the result \nof evaluating the rest of the input. The REPL Implementation The REPLs speci.cation is imple\u00admented by \na HOL function REPLi : string . repl result from an input string to a list of result strings ending \nin either ter\u00admination or divergence (as above). The function is total; however, it is not computable, \nsince it correctly ends the output list with di\u00advergence when necessary: being a function in the logic, \nit is able to ask whether there exist a number of steps in which execution of code for the next declaration \nwould terminate. We de.ne REPLi as a loop that consumes the input string while transforming the state \nof a low-level virtual machine for CakeML Bytecode (Section 6.3) and accumulating results. The loop is \nsplit into three parts: 1. (REPLi step: Read and compile) The .rst part reads and lexes the input until \nthe next semicolon, parses the resulting tokens, performs type inference on the resulting syntax and \nthen com\u00adpiles it to produce bytecode. 2. (Evaluate) The second part returns the result, if it exists, \nof ex\u00adecuting the bytecode simulator on the bytecode, or else signals divergence. 3. (Print and Loop) \nIf the simulator terminates, the last part con\u00adtinues the loop with the new machine state after accumulating \nthe results from the new state. If the simulator diverges, or if all input has been read, the loop ends. \nIf any step fails, the loop continues after accumulating the error message.  The REPL in Machine-Code \nThe bottom layer is a machine\u00adcode implementation of the REPL, created mostly by bootstrap\u00adping. Speci.cally, \nwe translate the HOL function, REPLi step, the main part of REPLi, into CakeML declarations using proof\u00adproducing \nsynthesis [22]. We then evaluate the compiler in the logic on those declarations to produce a veri.ed \nbytecode implementa\u00adtion of REPLi step. This bytecode is mapped via a veri.ed trans\u00adlation into x86-64 \nmachine code. To implement the other parts of REPLi, we separately synthe\u00adsise a small amount of x86-64 \nmachine code to jump to the code produced by the compiler, and to tie the loop together. We also synthesise \nveri.ed machine code for the lexer, printer, garbage col\u00adlector and arbitrary-precision integer package, \nwhich complete the veri.ed implementation. 2.1 Correctness and Veri.cation The correctness of the x86-64 \nmachine-code implementation of the CakeML REPL is stated with respect to our machine model for x86-64. \nThe theorem (which appears in more detail as Theorem 25) can informally be read as follows: Theorem 1. \nIf the machine is loaded with the CakeML REPL code and started with accessible memory for the data and \ncode heaps, and provided with a .nite input character stream i, then one of the following happens. The \nmachine terminates successfully with an output stream output, and there exist l and o such that REPLs \nl i o holds, o ends with termination, and out o = output. (Here out converts a repl result to a string \nby joining the results.)  The machine diverges after producing a .nite output stream output, and there \nexist l and o such that REPLs l i o holds, o ends with divergence, and out o = output.  The machine \nterminates with an out-of-memory error. The out\u00adput stream is terminated with an error message.  We \nprove the correctness theorem using two methods: interac\u00adtive proof, to establish a connection between \nREPLs and REPLi and to put the .nal result together, and a combination of interactive proof and proof-producing \nsynthesis (including bootstrapping) to construct veri.ed machine code that implements REPLi. The main \nlemma proved interactively is that for every input, there is an l such that REPLs l input (REPLi input) \nholds. We prove this (Section 8) by composing proofs that relate each of the components of REPLi to the \ncorresponding component of REPLs. Since REPLi reads its input one character at a time, whereas REPLs \nsplits the whole input string up front, we .rst prove, for the lexer, that the resulting list of declarations \nis the same either way. We then prove our parser sound and complete with respect to CakeML s grammar \n(Section 4), which means the results of parsing in REPLi match the choice of valid parse tree made by \nREPLs. We prove soundness for the type inference algorithm: if a type is inferred, the type system allows \nthat type. We verify the rest of the loop in REPLi under the assumption that type inference suc\u00adceeded, \nand hence the operational semantics does not encounter a type error. Internally, REPLs is de.ned by inductive \nrelations pa\u00adrameterised by an environment and store. We maintain an invariant between these semantic \nobjects, the state of the compiler, and the bytecode machine state. Our .rst compiler correctness theorem \n(Section 6) is for when REPLs says the declaration yields a new environment and output string. We carry \nour invariant around an iteration of the REPLi loop with this theorem, which says running the compiled \ncode for the declaration leads the virtual machine to produce the same output string and to terminate \nin a state that satis.es the invariant updated with the new environment. In the case of divergence, we \nhave a second compiler correct\u00adness theorem (Section 7) that says that if REPLs says a declaration diverges, \nthen a run of the compiled code for that declaration also diverges, causing the entire REPL to diverge. \nThe x86-64 machine-code implementation is constructed using forward proof, most of which is automated. \nThe bulk of the veri.ed machine code, i.e. that implementing REPLi step, is produced via compiler bootstrapping: \n1. We start by translating the HOL function REPLi step into CakeML abstract syntax, a list of CakeML \ndeclarations (ml repl step decls). The translation is automatic and proof\u00adproducing: it proves that the \ngenerated CakeML declarations implement the HOL functions. 2. Once we have the REPLi step in CakeML \nabstract syntax, we evaluate the veri.ed compiler on this abstract syntax. The evaluation happens inside \nthe logic and proves a theorem:  compile decs init ml repl step decls = (bytecode, cstate) By the compiler \ncorrectness theorems, we prove that the gen\u00aderated code, i.e. bytecode, accurately implements the CakeML \ndeclarations produced in the previous step. 3. In order to construct veri.ed and executable machine code \nfrom bytecode, we apply a veri.ed translation from bytecode instructions to x86-64 machine code. The \nsteps outlined above produced veri.ed x86-64 code for part of REPLi, namely REPLi step. We construct \nthe machine code for all id ::= x | Mn.x cid ::= Cn | M.Cn t ::= int | bool | unit | a | id | t id | \n(t(,t)* )id | t * t | t -> t | t ref | (t) l ::= i | true | false | () | [] p ::= x | l | cid | cid p \n| ref p | | (p(,p)* ) | [p (,p)* ] | p :: p e ::= l | id | cid | cid e | (e,e (,e)* ) | [e (,e)* ] | \nraise e | e handle p => e (| p => e)* | fn x => e | e e | ((e ;)* e) | uop e | e op e | if e then e else \ne | case e of p => e (| p => e)* | let (ld|;)* in (e ;)* e end + + ld ::= val x = e | fun x y= e (and \nx y= e)* uop ::= ref | ! | ~ op ::= = | := | + | -| * | div | mod | < | <= | > | >= | <> | :: | before \n| andalso | orelse c ::= Cn | Cn of t tyd ::= tyn = c (| c)* tyn ::= (a(,a)* ) x | a x | x d ::= datatype \ntyd (and tyd)* | val p = e + + | fun x y= e (and x y= e)* | exception c sig ::= :> sig (sl |;)* end sl \n::= val x : t | type tyn | datatype tyd (and tyd)* top ::= structure Mn sig ? = struct (d |;)* end; | \nd; | e; where x and y range over identi.ers (must not start with a capital letter), a over SML-style \ntype variables (e.g., a), Cn over constructor names (must start with a capital letter), Mn over module \nnames, and i over integers. Figure 1. CakeML source grammar other parts, including garbage collector \nand bignum library, using a different form of proof-producing synthesis of machine code [23]. The top-level \ntheorem is proved interactively by plugging together theorems describing the behaviour of the components. \n3. The Speci.cation of CakeML CakeML (Figure 1) is a subset of Standard ML (SML) [18], includ\u00ading datatypes \nand pattern-matching, higher-order functions, ref\u00aderences, exceptions, polymorphism, and modules and \nsignatures. CakeML integers are arbitrary precision. The main features not in\u00adcluded are records, input/output, \nand functors. All CakeML programs should run on an off-the-shelf SML implementation with the same result;2 \nhowever, we have not proved that fact with respect to the De.nition of Standard ML. Thus, any formal \nreasoning about CakeML programs must be carried out with respect to the CakeML semantics, rather than \nSML s. We chose to be faithful to SML so that potential CakeML users do not have to learn a new programming \nlanguage from scratch, but our main focus is not on SML per se, but rather on a call-by-value, impure, \nstrongly typed, functional language. We choose SML over OCaml for technical reasons (Section 12); on \nour subset, the difference is mostly in the syntax. Section 2 summarises the speci.cation: as a lexer, \ncontext-free grammar, de-sugarer, elaborator, type system, and big-step opera\u00adtional semantics wrapped \nin a REPL. Most of these are typical, so here we focus on their properties, and on our design decisions. \nCakeML s concrete syntax is speci.ed by a context-free gram\u00admar similar to Figure 1 s, but that unambiguously \nencodes the vari\u00adous parsing precedences and resolves the dangling case ambigu\u00adity. The transformation \nto abstract syntax removes syntactic sugar. 2 CakeML does not enforce equality types, so an SML implementation \nwill statically reject some programs that CakeML accepts. Our implementation of equality raises an exception \nif one of its arguments contains a closure. For example, a Let expression with multiple val bindings \nis de\u00adsugared into nested Lets, each with one binding. Our abstract syntax uses a .rst-order representation \nof bind\u00ading where variables, constructors, etc. are represented as strings. This straightforward representation \n.ts well with our semantics, which uses closures rather than substitutions, and it also enables bootstrapping, \nbecause the AST closely corresponds to a CakeML datatype (CakeML does not support strings, but they become \nlists of integers during synthesis). The elaborator walks the AST and replaces uses of types and constructors \nin patterns and expressions with fully quali.ed names, i.e., with the full module path to the constructor. \nThus, each con\u00adstructor is canonically named, and the type system and operational semantics do not have \nto generate unique stamps at datatype decla\u00adrations in order for a type preservation lemma to hold. A \nkey design principle for the operational semantics is for it to raise a distinguished exception Rtype \nerror whenever something goes wrong, instead of getting stuck or continuing on anyway. By raising Rtype \nerror instead of getting stuck, the big-step semantics has a precise correspondence between divergence \nand failure of an expression to relate to a result. This is a key part of our technique for verifying \nthe compiler for diverging input programs (Section 7). The Rtype error behaviour of the semantics forms \nthe interface between the type soundness proof and the compiler proof, as the compiler proof assumes \nthat programs do not raise Rtype error, and type soundness theorem guarantees this. Even though these \nchoices only affect programs that do not type check, they are how the compiler proof can use the fact \nthat these situations do not arise. For example, when evaluating a pattern match, the value s constructor \nmight be from a different type than the .rst pattern s, as in case true of None => 0 | true => 1. The \nsemantics could just move on to the next pattern since the value s and .rst\u00adpattern s constructors are \nnot equal. Instead the semantics detects this case and raises Rtype error. The semantics relies on the \nfollowing de.nitions of values v; environments for variables envE, constructors envC , and modules envM \n; stores s; and results r. The constructor environment records the arity of the constructor and what \ntype of values it constructs. ::= loc | l | cid | cid v | (v,v(,v)* ) | Closure(E, x, e) | RecClosure(E \n, (fun x y = e (and x y = e)*), x) loc ::= num envE, E = (x, v) list envC , C = (cid, num*id) list envM \n, M = (id, E) list s = loc list r ::= (s, C , (M , E)) | (s, C , ex) | (s, C , Rtype error) The type \nsystem relies on typing contexts for variables tE, con\u00adstructors tC , modules tM , and stores tS. We \nthen have relations for evaluation, divergence, and typing for top-level declarations. evaluate top : \nM . C . s . E . top . r . bool top diverges : M . C . s . E . top . bool type top : tM . tC . tE . top \n. tM . tC . tE . bool We use closures for function values, rather than performing substi\u00adtutions at function \napplications, to keep a close connection between the semantics of functions and their implementation \nstrategy. The REPL repeatedly calls evaluate top, and must update its state between each call, installing \nnew bindings, using the new store, etc. After completing a module declaration, the REPL up\u00addates the \ntype system s state with the module s bindings according to its signature, since future declarations \nmust not refer to hidden bindings or constructors. However, the operational semantics state must be updated \nbecause references to the hidden constructors will generally be reachable from the environment or from \nthe store. The last remaining subtlety lies in module declarations that raise an un-handled exception \npart-way through. When any declaration raises an exception, its bindings do not take effect, and so the \ntype environments are not updated. However, the operational semantics might need its constructor information \nupdated, since the module might have placed a locally de.ned constructor in the store .rst before raising \nthe exception. We have two options, either updating the constructor environ\u00adment with just the constructors \nseen, or with all of the constructors from the module body. Both are sound, and neither affect the pro\u00adgrammer, \nsince they are not in the type environments. We choose the latter, to match how the compiler implements \nmodules. Metatheory Theorem 2 (Determinism). If evaluate top M C s E top r1 and evaluate top M C s E \ntop r2 then r1 = r2. Proof sketch. By induction on the big-step relation. Theorem 3 (Totality). (.r. \nevaluate top M C s E top r ) iff \u00actop diverges M C s E top . Proof sketch. First, we establish, by induction \nalong a small step trace, that the small-step semantics cannot get stuck. Then we use the big-step/small-step \nequivalence theorem to transfer the result to the big-step semantics. The invariant of the type soundness \ntheorem existentially quan\u00adti.es constructor and module type environments that represent the declarations \nthat are hidden by module signatures. The up\u00addate type sound inv function updates the state depending \non whether the result is an exception or not following the discussion above. type sound inv(tM , tC , \ntE, M, C, E, s) = . tS tM no sig tC no sig. the environments are well-formed and consistent with each \nother, MAP FST tM no sig = MAP FST tM , tM no sig weakens tM , tC no sig weakens tC , and the constructors \nin tC no sig but not tC are all in modules in the domain of tM . Theorem 4 (Type soundness). Let state \n= (tM , tC , tE, M , C , E, s) be given. For all top, if type sound inv state and type top tM tC tE top \ntM ' tC ' tE' then either ' top diverges M C s E top or there exists s, C', and r such that r = Rtype \nerror, ' evaluate top M C s E top (s , C ', r), and type sound inv ' C' (update type sound inv top state \ntM ' tC ' tE' sr) Proof sketch. We prove the usual preservation and progress lemmas along a small-step \ntrace. Then we use the big-step/small-step equiv\u00adalence theorem to transfer the result to the big-step \nsemantics. We then reason about declarations entirely with the big-step seman\u00adtics. This poses no problem \nbecause declarations cannot diverge apart from their sub-expressions diverging. The preservation and \nprogress lemmas were proved directly on our CEK-style small-step semantics, which uses closures and a \ncontext stack. To our knowl\u00adedge no such proof appears in the literature. The main impact on the proof \nis to .atten out its structure (e.g., no substitution lemma), and instead to require an invariant that \ntypes the continuation stack. The usual subtleties about type substitution and weakening were unchanged; \nwe use de Bruijn indices for type variables. Part II: Veri.cation and Implementation We now turn to \nthe implementation and veri.cation of each part of the compiler. Afterward, we include a high-level discussion \nof CakeML s design and signi.cance (Section 12). 4. Parsing We implement the .rst phase of parsing with \na Parsing Expression Grammar (PEG), following Koprowski and Binsztok s mechanisa\u00adtion in Coq [11]. We \nhave a general inductive relation peg eval, which takes a grammar, an input sequence of tokens, a PEG \nex\u00adpression, and returns a verdict of either failure, or success with a returned value and the remaining \ninput. We prove the results from Koprowski and Binsztok that the peg eval relation always has a value \nwhen the PEG is well-formed, and that it is deterministic. Thus, on well-formed PEGs (we prove the CakeML \nPEG is one such), peg eval speci.es a function. PEG expressions are accompanied by semantic actions that \ncan transform the values associated with sub-parses into a com\u00adbined return value. Our semantic actions, \nin HOL, cannot be de\u00adpendently typed, as can be done in Coq where it is pleasant to have parses of different \nnon-terminals return values of different types. Instead, all our PEG-expressions have semantic actions \nthat simply return lists of CFG-parse-trees. In this way, the .rst phase of pars\u00ading concentrates on \nturning a linear sequence of tokens into a tree; later phases map different parts of these trees into \ndifferent types. Even though peg eval speci.es a function, its de.nition is not well-suited to computation. \nRather, we de.ne a general PEG in\u00adterpreter peg exec that is tail-recursive, written in a continuation\u00adpassing \nstyle. Whenever its PEG argument is well-formed, we prove the interpreter has a well-de.ned result, and \none that is equal to the result demanded by the peg eval relation. It is this de.nition that is compiled \ninto the bytecode that is eventually executed. Theorem 5 (Parser Soundness). Whenever the PEG successfully \nparses a non-terminal N, its result is a single CFG parse-tree with N at its head and whose fringe of \ntokens corresponds to the input consumed by the execution of the PEG. Proof sketch. Induction on the \nlexicographic combination of the length of the input given to peg eval, and the rank of the non\u00adterminal. \nEach non-terminal is given a natural number rank such that if executing the PEG on non-terminal N can \nresult in needing to execute non-terminal M without having consumed any input, then N s rank is greater \nthan M s. The fact that this ranking is possible stems from the same argument that gives the well\u00adformedness \nof the PEG. Theorem 6 (Parser Completeness). If a parse tree exists for a given input, then the PEG implementation \nwill .nd it. Proof sketch. Induction on the lexicographic combination of the length of the parse tree \ns fringe, and the rank of the tree s head non-terminal. As PEG execution is deterministic, this also \nimplies that our CFG is unambiguous. 5. Type Inference Type inference is based on Milner s algorithm \nJ (and hence W) [17]. We represent uni.cation variables as numbers, and in\u00adcrement a counter to generate \nfresh variables. Since the inferencer is written in higher-order logic (in a state-and-exception monad), \nwe cannot use the common technique of doing substitutions by up\u00addating pointers in the types. Instead \nwe re-use our previous formal\u00adisation (from the HOL4 examples directory) of triangular substitu\u00adtions, \nwhich are not idempotent, but are instead designed to main\u00adtain sharing for greater ef.ciency in a pure \nfunctional setting [12]. Theorem 7 (Type Inferencer Soundness). If infer top tM tC tE top state = ''' \n' (Success(tM , tC , tE ), state ) and tM , tC , tE contain no uni\u00ad ' ' ' .cation variables, then type \ntop tM tC tE top tM tC tE . Proof sketch. The soundness proof is long and tedious, but not particularly \nsurprising. A large part of the proof is devoted to establishing invariants on the usage of type variables \nin the various environments and state components of the algorithm. There are two ways in which our inferencer \nveri.cation falls short of what could be seen as an ideal for SML, but neither directly affects our end-to-end \ncorrectness theorem. First, the types of let\u00adbound variables are not generalised, restricting polymorphism \nto top-level and module-top-level declarations;3 and second, we have not veri.ed completeness of the \ninferencer. In both cases, the po\u00adtential for the CakeML system to behave badly is limited to sig\u00adnalling \na type error on a declaration that the programmer expected to pass type checking. In particular, there \nis no possibility for the programmer to be deceived by the system appearing to returning an erroneous \nresult of executing the de.nition. We gave proving inferencer completeness low priority because, in practice, \nwe .nd the inferencer does not fail on the programs we expect to have types. If it did, a completeness \ntheorem would tell us that the bug is in CakeML s declarative type system, rather than possibly also \nin the inferencer. 6. Compilation We now turn to translation from abstract syntax to CakeML Byte\u00adcode, \nour assembly language for a virtual stack machine, which is the .nal language before translation to x86-64 \nmachine code. We describe the bytecode in detail in Section 6.3, but .rst take a high\u00adlevel look at the \ncompiler and its veri.cation. The main function is compile top which takes a top-level dec\u00adlaration and \nthe compiler s state, including environments mapping variables to bytecode stack offsets, and returns \ncode and two new compiler states. One is used if the code runs successfully and the other if it raises \nan un-handled exception. For the veri.cation, we de.ne a relation compiler inv envM envC s envE rs z \nrd bs between the semantic context (the module, constructor, and value environments, and the store), \nthe compiler s state rs, the bytecode machine state bs, and additional context indicating the size, z \n, of the stack before the last complete declaration, and information about references and closures in \nrd . We explain the compiler along with its veri.cation, beginning with the statement of the correctness \ntheorem for terminating pro\u00adgrams (for diverging programs see Section 7). Theorem 8 (Compiler Correctness). \nIf top is well-typed, evaluate top m c s e top res , compile top rs top = (rss, rsf , code ), and, compiler \ninv m c s e rs |bs.stack| rd bs , then a run of bs with code terminates with a new state bs ', satisfy\u00ading \nthe following condition, depending on res : If res is a successful result with new environments m ' , \nc ', and e ' , and new store s ', then there exists rd ' such that bs ' has reached the end of code (no \nnext instruction), '''' ' ' compiler inv m c s e rss |bs .stack| rd bs ', and, bs ' has the result of \nprinting the new bindings in its output. 3 Vytiniotis et al. [30, Section 4.3] provide evidence that \ngeneralisation for let expressions is rarely used in practice. Otherwise if res is an exception exc \nwith a new store s ', then there exists rd ' and bv such that the next instruction for bs ' is Stop, \nbs ' .stack = bv :: bs.stack, bv is a re.nement of exc, and, ' ' ' '' compiler inv m c s e rsf |bs.stack| \nrd bs , where bs '' is bs ' with bv popped. (The printing and popping of bv is done by the main REPL \nloop after the Stop instruction is reached.) Proof sketch. The compile top function is implemented using \nfunc\u00adtions for compiling declarations and expressions, mirroring the structure of the big-step semantics. \nThe proof for each layer is by induction on the appropriate big-step relation. Our compilation strategy \nhas three phases: translation to an in\u00adtermediate language, analysis for the purpose of compiling func\u00adtions, \nand .nally translation to bytecode. We proceed with a de\u00adscription of the implementation and veri.cation \nof each phase. 6.1 Translation to Intermediate Language The intermediate language (IL) simpli.es the \nsource language in the following ways: Pattern-matching is replaced by conditional expressions and constructor-tag \nequality tests.  Named variables are replaced by de Bruijn indices. Constructor names are replaced by \nnumeric tags.  All functions are syntactically recursive, and may have zero or more arguments.  The \nset of primitive operations is smaller (e.g. no greater-than, only less-than). The translation to IL \nexpressions is a straightforward recursion  on the abstract syntax of CakeML. We also de.ne a translation \nfrom CakeML values to IL values, which makes use of the expression translation since closures are amongst \nthe values. Veri.cation We give a big-step operational semantics for the intermediate language, similar \nto the one for the source language. The correctness theorem for the translation to IL says whenever the \nCakeML semantics gives a program p a result r, the IL semantics gives the translated program [p] a result \nr ' that is related to [r]. The proof is by induction on the CakeML big-step evaluation relation. We \ncannot prove identity between the translated CakeML result and the result produced by the IL semantics, \nbecause closure en\u00advironments may differ. The translation to IL sometimes introduces fresh variables \nthat will appear in an IL closure s environment but not in the translation of the source closure s. We \nneed a coarser relation on IL closures, allowing their environments to differ. For this purpose, we de.ne \na relation on IL expressions, val\u00adues, and environments. The relation V f (z1, e1) (z2, e2) re\u00adlates \nan IL expression together with the size of its environment to another such pair. It is parameterised \nby a relation V v1v2 indicating variables that are assumed to be bound to equivalent values. To re\u00adlate \nvalues, the relation v1 v2 needs no sizes or parameters, since closures carry their environments so \nthe size can be computed. We have proved re.exive and transitive (symmetry fails as explained in the \nnext section) and the following two theorems. Theorem 9. If the IL semantics says e1 evaluates in environment \nenv1 to a result r1; whenever V v1 v2 holds, env1(v1) env2(v2) does also; and, V f (|env1|, e1) (|env2|, \ne2); then there is a result r2 to which e2 evaluates in env2 and r1 r2. Proof sketch. By induction on \nthe IL evaluation relation. The main purpose of the relation is to enable closure environ\u00adments to be \nmanipulated. The second theorem supports the renum\u00adbering of variables that is required in a closure \ns body when its environment is changed. Theorem 10. Let e ' be the result of renumbering variables in \ne and suppose V relates indices under the same renumbering scheme, then V f e e ' . Proof sketch. By \ninduction on syntax-directed renumbering. We use these theorems for verifying the translation to IL in\u00adcluding \nthe removal of pattern matching. We also use to relate IL function bodies to annotated IL closure bodies, \ndescribed below. 6.2 Intermediate Language Closure Annotation For every function body, we compute how \nto compile each occur\u00adrence of a variable, and how to build the closure environment. We use .at closure \nenvironments that only have bindings for variables that actually occur free in the function s body. As \nan example, consider the following IL expression (written in concrete syntax with named variables, but \nrecall that the IL actually uses de Bruijn indices): let val a = e1 val b = e2 val c = e3 fun f h x = \nif x = 6 then h 7 else (g x) + (f (x -a)) and g x = f (fn y => x + y -b) (a -c) in e4 end  Analysis \nof the bodies records this about the free variables: for f: for g: for .: x -> arg 0 f -> rec 0 x -> \nenv 0 h -> arg 1 b -> env 0 y -> arg 0 g -> rec 1 a -> env 1 b -> env 1 f -> self c -> env 2 cl env = \n[0,2] a -> env 0 cl env = [1,2,0] cl env = [2]  The env annotation gives an index into the closure environment, \nwhich is itself a list of indices into the enclosing environment (so in the anonymous function s closure \nenvironment, 0 refers to g s argument, and 2 refers to the .rst element of g s environment, since 1 would \nrefer to g itself). We update the variables in function bodies so that they refer to the closure environment \ninstead of the enclosing environment. For example, in the body of f, the de Bruijn index for a is decremented \ntwice, because the intervening variables in the enclosing environ\u00adment are omitted in the closure environment. \nCakeML s operational semantics also uses closures, making the proof of correspondence conceptually straightforward, \nbut closures in the semantics contain the whole enclosing environment. We generate, for each function \nbody, a unique label that is used (Section 6.4) as a code pointer to bytecode implementing the body. \nVeri.cation We store the closure environment information as an\u00adnotations on the function bodies within \nthe expression. The IL se\u00admantics uses the closure (rather than enclosing) environment to evaluate a \ncall when such an annotation exists. Similarly, the re\u00adlation V , U f (z1, bs1) (z2, bs2) allows the \nbodies in bs1 and bs2 to be annotated, and uses the closure environment rather than V and z as appropriate. \nThe relation is directed (hence not symmet\u00adric): if a body in bs1 is annotated, then the corresponding \nbody in bs2 must have the same annotation; however an unannotated body in bs1 may be related to an annotated \nbody in bs2. Theorem 11. If e ' is the result of annotating e, and e is unannotated and has free variables \nall less than z, then (=) f (z, e) (z, e ' ), and e ' is fully annotated with environments that cover \nthe free variables of its bodies. Proof sketch. By induction on syntax-directed annotation. bc inst \n::= Stack bc stack op | PushExc | PopExc | Return | CallPtr | Call loc | PushPtr loc | Jump loc | JumpIf \nloc | Ref | Deref | Update | Print | PrintC char | Label n | Tick | Stop bc stack op ::= Pop | Pops n \n| Shift n n | PushInt int | Cons n n | El n | TagEq n | IsBlock n | Load n | Store n | LoadRev n | Equal \n| Less | Add | Sub | Mult | Div | Mod loc ::= Lab n | Addr n n = num bc value ::= Number int | RefPtr \nn | Block n bc value * | CodePtr n | StackPtr n bc state ::= { stack : bc value * ; refs : n . bc value; \ncode : bc inst * ; pc : n; handler : n; output : string; names : n . string; ? clock : n} Figure 2. CakeML \nBytecode syntax, values, and machine state  6.3 CakeML Bytecode The target language of the compiler \nis CakeML Bytecode (Fig\u00adure 2), a low-level assembly language for a virtual machine with a single random-access4 \nstack. CakeML Bytecode was designed with three separate goals: to be (i) conveniently abstract as a target \nfor the compiler and its proofs, and (ii) easy to map into reasonably ef.cient machine code that is (iii) \npossible to reason about and verify w.r.t. an operational semantics for x86-64 machine code. To support \n(i), the bytecode has no notion of pointers to the heap, and provides structured data (Cons packs multiple \nbytecode values into a Block) on the stack instead. Also, the bytecode Number values are mathematical \nintegers; the x86-64 implementation includes a bignum library to implement the arithmetic instructions. \nFor (ii), we ensure that most bytecode instructions map to one or two x86-64 machine instructions; and \nfor (iii), the bytecode essentially only operates over a single stack , the x86-64 stack which we access \nusing the normal stack and base pointers, rsp and rbp registers. (See Section 10 for the implementation \nof the bytecode in x86-64.) The bytecode semantics is a deterministic state transition sys\u00adtem: the relation \nbs1 . bs2 fetches from code the instruction indicated by pc and executes it to produce the next machine \nstate. We give some example clauses in Figure 3. Our data re.nement relation l, r, Cv |= bv says bv is \na byte\u00adcode value representing the IL value Cv. It is parameterised by two functions: l to translate \nlabels to bytecode addresses, and r provid\u00ading extra information about code pointers for closures. The \nre.nement of closures is most interesting. There are two components to a closure: its body expression, \nand its environment which may refer to other closures in mutual recursion. We use a correspondence of \nlabels to link a code pointer to an annotated IL body, and for the environment, we assume the IL annotations \ncorrectly specify the closure environment. In the IL, a closure looks like CRecClos env defs n, where \nenv is the enclosing5 environment, and the body is the nth element of the bundle of recursive de.nitions \ndefs. We say l, r, CRecClos env defs n |= Block c [CodePtr a; Block e bvs] holds (c and e are tags indicating \nclosure and environment Blocks) when: 4 Most operations work on the top of the stack, but Load n and \nStore n read/write the cell n places below the top, and LoadRev takes an index from the bottom. 5 Annotations \non defs[n] build the closure environment from env. fetch(bs) = Stack (Cons t n) bs.stack = vs @ xs |vs| \n= n bs . (bump bs){stack = Block t (rev vs) :: xs} fetch(bs) = Return bs.stack = x :: CodePtr ptr :: \nxs bs . bs{stack = x :: xs; pc = ptr} fetch(bs) = CallPtr bs.stack = x :: CodePtr ptr :: xs bs . bs{stack \n= x :: CodePtr (bump bs).pc :: xs; pc = ptr} ' fetch(bs) = PushExc bs.stack = xs bs = bump bs bs . bs \n' {stack = StackPtr (bs.handler) :: xs; handler = |xs|} fetch(bs) = PopExc bs.handler = |ys| bs.stack \n= x :: xs @ StackPtr h :: ys bs . (bump bs){stack = x :: ys; handler = h} Figure 3. CakeML Bytecode \nsemantics (selection). The helper function fetch calculates the next instruction using the pc and code, \nand bump updates the pc to the next instruction. defs[n] has label lab and annotations ann, l(lab) = \na, and |ann| = |bvs|;  for every variable x with an env annotation in ann, the corre\u00adsponding bytecode \nvalue bv in bvs satis.es l, r, env(x) |= bv; and,  for every variable with a rec i annotation in ann, \nthe corre\u00adsponding bytecode value in bvs is RefPtr p, for some p, and  ' '' there are env , defs ', \nand j such that r(p) = (env , defs , j) and CRecClos env defs i CRecClos env ' defs ' j. Thus, for \na function in mutual recursion we assume it is behind the indirection of a RefPtr; the function r acts \nas an oracle in\u00addicating the closure that should be pointed to. To tie the knot, the inductive hypothesis \nin our compilation proof says whenever r(p) = (env ' , defs ' , j ) the bytecode machine refs binds p \nto a value bv satisfying l, r, CRecClos env defs j |= bv. 6.4 Translation to Bytecode The main compilation \nalgorithm takes an IL expression as input and produces bytecode instructions. Additional context for \nthe compiler includes an environment binding IL variables to stack offsets, and a return context indicating \nthe number of variables that need to be discarded before a jump if the expression is in tail position. \nThe correctness theorem for this phase is similar to Theorem 8 (whose proof uses this one as a lemma), \nassuming evaluation in the IL semantics rather than the source semantics. In particular, we have a relation \ncalled IL inv that captures an invariant between the IL environment and store, the compiler state, the \nbytecode machine state, and proof information like the l and r functions. This relation is used in the \nde.nition of compiler inv, which crosses the three languages (CakeML, IL, Bytecode). The theorem below \ndepends only on the IL and the bytecode. Theorem 12. If the IL semantics says Cexp evaluates in environ\u00adment \nCenv and store Cs to a new store Cs ' and result Cres , and all the values in the context are fully annotated, \nthen for all bytecode machine states bs satisfying IL inv with Cenv and Cs (and proof information including \nl and r), then If Cres is a value, Cv, then Running bs with code from compiling Cexp in non-tail position \nleads the bytecode machine to terminate in a new state bs ' such that IL inv holds of Cenv, Cs ', and \nbs ', and bs ' .stack = bv :: bs.stack with l, r ' , Cv |= bv; and, Assuming bs.stack = lvs @ CodePtr \nret :: args @ st, running bs with code from compiling Cexp in tail position ready to discard |lvs| and \n|args | leads the machine to a state bs ' satisfying the invariant as above, and also bs ' .pc = ret \nand bs ' .stack = bv :: st, with l, r ' , Cv |= bv.  Otherwise, if Cres is an exception value Cx, and \nif bs.stack = vs @ StackPtr h :: CodePtr hdl :: st, and bs.handler = |st| + 1, then running bs with code \nfor compiling Cexp (in either call context) leads the machine to a state bs ' satisfying the invariant, \nand with bs ' .pc = hdl , bs ' .stack = bv :: st, l, r ' , Cx |= bv, and bs ' .handler = h.  Finally, \nif Cres is a timeout exception, then a run of bs on code for Cexp causes the machine to time out. (See \nSection 7 for details.)  Proof sketch. By induction on the big-step semantics for the IL. The invariant \nincludes a condition on bs.code: it must already contain code resulting from compiling all the function \nbodies appearing in Cexp and for closures in Cs and Cenv. This assumption is justi.ed by the compilation \nof function bodies described below. Function Bodies Before compiling the main expression, we compile \nthe functions. For each body, the compilation environment is the closure environment and the return context \nis tail position (ready to discard just the arguments to the function but no local variables). We lay \nthe resulting stretches of bytecode in sequence each preceded by the label annotating the closure s body. \nClosure Creation As we saw in the de.nition of l, r, Cv |= bv, we represent a closure using a Cons block \ncontaining a code pointer and an environment block, which is built following the annotations on the closure \nbody. For mutually recursive closures (the rec an\u00adnotation), we build the closure environment using references, \nand update them with the appropriate closures once all the closures are created, thereby ensuring mutually \nrecursive functions appear as RefPtrs in the closure environment. Function Calls and Proper Tail Calls \nThe generated code for a function call depends on whether it is tail position or not. In both cases, \nwe .rst evaluate the closure and its arguments, and extract the code pointer from the closure. For a \nnon-tail call, we use the CallPtr instruction, which generates a return pointer and jumps. For a tail \ncall, since we are assuming a return pointer is already on the stack, we reorganise the stack, discarding \nthe local variables and arguments, then use the Return instruction to make the call. The key lemma enabling \nour proof by induction for the func\u00adtion call case says that if bs.stack = benv :: CodePtr ret :: bvs \n@ Block c [p; benv ] and l, r, CRecClos env defs n |= Block c [p; benv ], then we can establish IL inv \nfor bs with the clo\u00adsure environment made from env and the annotations on defs[n]. Thus we can use the \ninductive hypothesis on the closure body even though it is not a subexpression of the original Cexp in \nthe theorem statement. Declarations So far we have looked at the compilation of IL ex\u00adpressions. A CakeML \nprogram, however, is a sequence of decla\u00adrations of types, values, or structures. Our IL does not have \ndecla\u00adrations, so to compile a value declaration val p = e, we construct the CakeML expression case e \nof p => vs, where vs is a tuple of the variables appearing in p, translate this expression to the IL \nand compile it, and generate a bit of additional code to extract the new bindings from the tuple. For \ntype declarations, we need not generate any code at all and simply update the compiler s state component \nmapping constructor names to bytecode block tags. Modules Structure declarations are, from the compiler \ns perspec\u00adtive, just a sequence of type and value declarations. But they must be treated as a single \nunit, so there is a subtlety: if any of the decla\u00adrations within a structure raises an un-handled exception, \nthe bind\u00adings for the whole structure need to be discarded. Therefore, we must set up an exception handler \naround the whole sequence of declarations, enabling unwinding of the stack before proceeding to the next \ntop-level declaration. If the sequence of declarations .n\u00adishes successfully, we pop the exception handler \nand hence need to shift the stack offsets for the new bindings in the compiler s envi\u00adronment. We reuse \nthis machinery and its proof for top-level decla\u00adrations (treating them as unnamed structures with one \ndeclaration). 7. Diverging Programs So far we have seen our compilation algorithm and how we prove that \nif a source declaration terminates then the bytecode terminates with the same result. By assuming termination, \nwe might appear to admit a compiler that causes programs that should diverge to do anything at all, including \nreturning a wrong result. Here we show how to establish that our compiler in fact preserves divergence. \nThe proofs of the previous section are all performed in the direction of compilation by induction on \nthe big-step semantics. We would like to handle diverging programs in the same way, to avoid establishing \na simulation from a small-step bytecode trace to a small-step source trace against the direction of compilation, \nand to avoid introducing a co-inductive big-step semantics [15]. Because the bytecode semantics is deterministic, \nall we have to do is show that the compiler maps diverging source expressions to diverging bytecode expressions. \nFirst, we add optional clocks to the source big-step semantics and to the bytecode machine state. In \nthe big-step semantics, the clock is decremented by 1 on each function call, and a timeout exception \nis raised if a function is called when the clock is 0. In the bytecode, the Tick instruction decrements \nthe clock, and the semantics gets stuck if the clock is 0. The compiler emits a Tick instruction for \neach source function call, and we prove that if a program times out in the semantics with a certain clock, \nthen the compiled version times out in the bytecode with the same clock. This is the core of the compiler \nproof for divergence, and it follows the same inductive approach as the rest of the compiler proof. The \nconclusion of Theorem 12 handles the case of a timeout exception, and thereby supports an analogue of \nTheorem 8 for diverging programs. It remains to show how to establish our main divergence result when \nthe source semantics ignores the clock and the Tick instruc\u00adtion is implemented as a no-op (and thus \nproduces no x86-64 in\u00adstructions). We sketch the proofs here with a simpli.ed notation. We will write \nc f e . v for convergence in the source language with clock c to a value (or non-timeout exception), \nand c f e . \u00d8 for a timeout exception. We use a clock of 8 to indicate the version that ignores the clock. \nLemma (Big-step Clocked Totality). For all clocks c and expres\u00adsions e, either c f e . \u00d8 or .v. c f e \n. v. Proof sketch. By well-founded induction on the lexicographic or\u00addering of the clock and size of \nthe expression. In all but one case, the applicable big-step rules have inductive premises that have \nthe same or smaller clocks (because the clock is monotonically non\u00adincreasing) and smaller sub-expressions. \nThus, by induction the re\u00adsults for the sub-expressions combine to give a result for the expres\u00adsion. \n(It is important here that all mis-applied primitives evaluate to an exceptional result.) The only case \nwhere the expression might be bigger is function application, but it decrements the clock .rst. Lemma \n(Big-step Clock/Unclock). c f e . v implies 8 f e . v and, 8 f e . v implies .c. c f e . v. Proof sketch. \nStraightforward induction. The bytecode s operational semantics is small-step, so we de.ne an evaluation \nrelation in the standard way: ' ' '' ' '' c f bs .bc bs = bs{clock = c} . * bs . .bs . \u00ac(bs . bs ) We \nsay the machine has timed out if it evaluates to a state with clock = 0 and next instruction Tick. A \nbytecode machine state diverges if it can always take another step. Lemma (Bytecode Clock/Unclock). c \nf bs .bc bs ' implies bs{clock = 8} . * bs ' {clock = 8}, and 8 f bs .bc bs ' implies .c. c f bs .bc \nbs ' {clock = 0}. Proof sketch. Straightforward induction. Lemma (Clocked Bytecode Determinism). c f \nbs .bc bs ' and '' ' '' c f bs .bc bs implies bs = bs . Proof sketch. The small-step relation is deterministic \nby inspection of the rules; the main result follows by induction on . * . Theorem 13. Evaluation of e \ndiverges in the un-clocked semantics iff the compilation of e (loaded into a bytecode state bs) diverges \nin the un-clocked bytecode semantics. Proof. For the forwards direction, we have c f e . \u00d8, for all clocks \nc, by the source language s determinism, and the totality and clock/unclock lemmas. Therefore by the \ncompiler correctness result, we know for all clocks c there is a bs ' such that c f bs .bc bs ' and bs \n' is timed out. Now we must show that bs{clock = 8}diverges. Suppose, for a contradiction, there is some \nbs '' with 8 f bs .bc bs ''. Let c be one more than the number of Tick instructions on the trace from \nbs to bs '', which is unique by determinism. This contradicts the existence of a bs ' above: if evaluation \nstops before reaching bs '', it will not have passed enough Ticks to deplete the clock, and if it reaches \nbs '' it stops without timing out. The backwards direction follows easily from Theorem 8 and the clock/unclock \nlemmas. 8. Read-Eval-Print Loop To interact with our compiler, we build a REPL. We de.ne this .rst as \na logic function, REPLi, that implements REPLs. (In later sections, we describe how we produce an x86-64 \nmachine code implementation of the REPL.) loop (bs, b) input = case lex until toplevel semicolon input \nof | None . Terminate | Some (tokens, rest of input) . case REPLi step(tokens, s) of | Failure error \nmsg . Result error msg (loop (bs, s) rest of input) | Success (code , s, sexc) . let bs = install code \ns.cstate code bs in case bc eval bs of | None . Diverge | Some bs . let s = if success bs then s else \nsexc in Result bs.output (loop (bs, s) rest of input) REPLi input = loop initial state input On each \niteration of its main loop REPLi lexes part of the in\u00adput string up until the .rst top-level semicolon; \nit then calls the REPLi step function, which performs parsing, elaboration, type inference and then compilation \nto bytecode. Once the input has been turned into code, it installs the code into a bytecode state and \nstarts execution (bc eval) of the new bytecode state. If the gener\u00adated code terminates, then it loops \nback to the top and starts again. Here bc eval is a non-computable function which describes the small-step \nexecution of the bytecode semantics. This function is total and returns None if the bytecode fails to \nterminate, otherwise it returns the resulting state inside Some. 6 Theorem 14 (REPLi Correct). For all \ninputs i and outputs o, if REPLi i = o then REPLs (get type error mask o) i o. Where REPLs is the REPL \nsemantics described in Sec\u00adtion 2 and get type error mask picks out the inputs that output \"<type error>\", \nto let the REPL semantics know where the type system should fail. Proof sketch. By induction on the length \nof i. This theorem pulls to\u00adgether the parser soundness and completeness theorems, the infer\u00adence soundness \ntheorem, the type soundness theorem and the com\u00adpiler correctness theorem. Most of the proof is devoted \nto showing that the various invariants required by the theorems are maintained across the iterations \nof loop. The veri.ed REPLi function is well suited for simulating the implementation in the theorem prover: \nwe can evaluate REPLi using the HOL4 prover s EVAL mechanism. For example, EVAL REPLi \"fun f x = x + \n3; f 2;\" automatically derives a theorem: REPLi \"fun f x = x + 3; f 2;\" = Result \"val f = <fn>\" (Result \n\"val it = 5\" Terminate)  Note that this evaluation of REPLi inside the logic by inference does not terminate \nif the bc eval fails to terminate, i.e., this evalu\u00adation won t return Diverge. 9. Bootstrapping Most \nof the complexity in the de.nition of REPLi is contained within the de.nition of REPLi step, a function \nthat combines pars\u00ading, elaboration, type inference and compilation to bytecode. The next section describes \nhow we construct a veri.ed x86-64 machine\u00adcode implementation of REPLi, and therefore of REPLs. In order \nto make the construction of x86-64 for REPLi an easier task, we use the veri.ed compiler to compile REPLi \nstep to bytecode. The REPLi step function contains the compiler itself, which means that this application \nof compilation amounts to bootstrapping the compiler. This section explains the bootstrapping method; \nthe next section explains how we use the result. Our starting point is REPLi step. We want to have byte\u00adcode \nwhich is proved to implement the REPLi step function. The REPLi step is de.ned in logic (HOL), where \nfunctions are speci\u00ad.ed using equations, e.g. it is easy to de.ne a HOL constant, fac, that is characterised \nby the equations: fac 0 = 1 fac (n + 1) = (n + 1) \u00d7 fac n In order to apply the veri.ed compiler to \nfunctions de.ned in HOL, we need these equations to exists as CakeML abstract syntax for function de.nitions, \ni.e., we need CakeML declarations de.ning REPLi step. A previously developed proof-producing tool [22], \nimplements exactly this kind of translation. Given a constant from HOL with equations that look suf.ciently \nML-like, the tool generates CakeML abstract syntax and proves that the generated CakeML implements the \nconstant described by equations in HOL. When applied to REPLi step, this translation produces a long \nlist of 6 bc eval is the .bc relation from Section 7. CakeML declarations: ml repl step decls = [datatype \n. . . , datatype . . . , fun . . . , fun . . . , . . . , fun repl step x = . . .] The tool also proves \ntwo relevant theorems: Theorem 15 (Evaluating ml repl step decls). Evaluation of the declaration list \nsucceeds and produces a semantic environment, i.e., a mapping from names to values. We will refer to \nthis environment as ml repl step env. Theorem 16 (Evaluating repl step). The value associated with the \nname repl step in ml repl step env, is a closure which im\u00adplements REPLi step w.r.t. re.nement invariants \ninput and output (relations between abstract values and CakeML semantic values). In the notation of Myreen \nand Owens [22]: Eval ml repl step env \"repl step\" ((input . output) REPLi step) Proof sketch. By algorithm \nfrom Myreen and Owens [22] The CakeML-to-bytecode compiler is then applied to the decla\u00adration list. \nThe result is a theorem produced by evaluation. Theorem 17 (Compiling ml repl step decls). Given an initial \ncompiler state init and the declaration list, the compiler produces bytecode and a new compiler state \ncstate. compile decs init ml repl step decls = (bytecode, cstate) Proof sketch. By evaluation in the \nlogic (EVAL from above). Next, we instantiate the compiler correctness theorem (the com\u00adpile decs version \nof Theorem 8) for use with Theorems 15 and 16. Two further theorems are outlined below. We omit the de.nition \nof run inv. Theorem 18 (Running bytecode). Executing the compiler gener\u00adated bytecode terminates in some \nstate bs such that this state con\u00adtains ml repl step env and run inv bs v is true, for some v. Proof \nsketch. By instantiation of a lemma supporting Theorem 8. Theorem 19 (Calling repl step). For any bytecode \nstate bs, se\u00admantics value v and x such that run inv bs v and input x v, running a snippet of bytecode \nwhich calls the code for repl step, termi\u00adnates in a new bytecode state bs ' and v ' such that run inv \nbs ' v ' and output (REPLi step x) v '. In other words, calling the code for repl step computes REPLi \nstep. Proof sketch. By instantiation of Theorem 12 and other lemmas supporting Theorem 8. 10. Implementation \nin x86-64 The veri.ed x86-64 implementation is constructed so that it exactly implements the in-logic-de.ned \nREPLi function from above. Thanks to the bootstrapping, large parts of REPLi, namely REPLi step, exist \nin the form of veri.ed bytecode. In other words, much of the effort involved in constructing the x86-64 \nimplemen\u00adtation of REPLi boils down to producing an implementation of the bytecode that the compiler \ntargets. In order to verify the x86-64 code we need a semantics, a programming logic and proof automation \nfor x86-64 machine code. For these, we build on previous work. Semantics of x86-64 We use a conventional \nsmall-step opera\u00adtional semantics for x86 machine code. This semantics originates in a de.nition for \n32-bit x86 that Sarkar et al. [28] validated against real hardware. The semantics was extended in Myreen \n[19] to han\u00addle self-modifying code and used in our previous work on a veri.ed implementation of a REPL \nfor Lisp [21]. The current semantics has a small coverage of the user-mode x86-64 instruction set. However, \nit tries to be as accurate as possible for the subset it describes. Programming Logic For most proofs \n(manual and automatic), we use the machine-code Hoare logic for our previous work on self-modifying code \n[19]. However, for the current work, we had to de.ne a new more expressive programming logic in order \nto state and prove the top-level theorem. The top-level theorem must be able to specify that the machine \ncode diverges. Our previously developed Hoare logic is only able to specify that a state satisfying the \npostcondition is eventually reached (termination). The new programming logic (also embedded in HOL), \nmakes statements, using temporal logic, about the sequence of all possible executions of the underlying \nx86-64 machine. This new program\u00adming logic makes judgements of the form temporal code f. It is strictly \nmore expressive than the Hoare triple. Theorem 20 (Hoare Triple Instance of temporal). The machine\u00adcode \nHoare triple, {p} code {q}, is an instance of temporal: {p} code {q} .. temporal code ((now p) . .(now \nq)) Proof sketch. Follows immediately from the de.nitions. We de.ne temporal, now, ., ., etc. as follows. \nThe de.ni\u00adtions below take a few concepts from Myreen [19], in particular x86 seq s t is true if t is \na valid x86-64 trace starting from state s, and p : s is true if p holds for a copy of a state s with \na less precise instruction cache [19]; for most purposes, simply read p : s as p s. temporal code f = \n.s t r. x86 seq s t =. f (.p s. (p * code code * r) : s) t now p = .assert t. assert p (t 0) D f = .assert \nt. .k. f assert p (.n. t (n + k)) . f = .assert t. .k. f assert p (.n. t (n + k)) later f = .assert t. \n.k. f assert p (.n. t (n + k + 1)) f . . = .assert t. f assert t . . assert t f . . = .assert t. f assert \nt =. . assert t  Using these, we can specify divergence of a machine-code pro\u00adgram. For example, from \ninitial con.guration p, code will always, at some some later point, reach a state satisfying q. In other \nwords, q will be true in.nitely many times. temporal code ((now p) . D .(now q)) In our theorems, we \ninstantiate q to say that the compiled bytecode is still running. Divergence means that the program runs \nbytecode forever. Bytecode Heap Invariant Central to our proofs is the invariant which speci.es how bytecode \nstates are concretely represented in the x86-64 machine state. This invariant is formalised as a state \nassertion bc heap bs aux s, which relates bytecode state bs and auxiliary state aux to (part of) machine \nstate s. The formal de.nition of bc heap is not shown due to its length. However, informally, this invariant \nstates that: memory is split into two data heaps (of which only one is in use at any point in time, \nenabling stop-and-copy garbage collection), a code heap, the normal x86-64 stack and a separate global \nstate record;  registers rax rdx hold bytecode values in the case of a Block, a large (bignum) Number, \nor a RefPtr, this means they contain a pointer into the data heap;  the top of the bytecode stack is \nstored in register rax,   the rest of the bytecode stack is kept in the x86-64 stack, i.e., all values \nin the x86-64 stack are roots for the garbage collector,  the stack is accessed through the normal stack \nand base point\u00aders, registers rsp and rbp; other registers and state keep track of temporary values, \nthe state of the allocator and system con.guration. output is produced via calls to a special code pointer, \nfor which we have an assumption that each call to this code pointer puts a character onto some external \nstream (in practice we link to C s putc routine). Input is handled similarly (using getc).  memory contains \ncode for supporting routines: the veri.ed  garbage collector, arbitrary-precision arithmetic library \netc. The garbage collector updates the heap and the stack (i.e., the roots for the heap), both of which \ncan contain code pointers and stack pointers. In order for the garbage collector to distinguish between \ndata pointers and code/stack pointers all code/stack pointers must have zero as the least signi.cant \nbit (i.e., appear to be small inte\u00adgers). We ensure that all code pointers end with zero as the least \nsigni.cant bit by making sure that each bytecode instruction is mapped into x86-64 machine code that \nis of even length. Implementation of CakeML Bytecode Having formalised the representation of bytecode \nstates, we de.ne a function that maps CakeML Bytecode instructions into concrete x86-64 machine in\u00adstructions \n(i.e. lists of bytes). Here i is the index of the instruction that is to be translated (i is used for \nthe translation of branch in\u00ad structions, such as Jump). x64 i (Stack Pop) x64 i (Stack Add) = = [0x48, \n0x58] [0x48, . . .] . . . Entire bytecode programs are translated by x64 code: x64 code i [] = [] x64 \ncode i (x :: xs) = let c = x64 i x in c @ x64 code (i + length c) xs We prove a few key lemmas about \nthe execution of the gener\u00adated x86-64 machine code. Theorem 21 (x64 code Implements Bytecode Steps). \nThe code generated by x64 code is faithful to the execution of each of the CakeML Bytecode instructions. \nEach instruction executes at least one x86-64 instruction (hence later). Note that exe\u00adcution must either \nreach the target state or resort to an error (out of memory error). bs . bs ' =. temporal {(base , x64 \ncode 0 bs.code)}(now (bc heap bs (base , aux)) . later (now (bc heap bs ' (base , aux)) . now (out of \nmemory error aux))) Proof sketch. For simple cases of the bytecode step relation (.), the proof was manual \nusing the programming logic from Myreen [19]. More complex instruction snippets (such as the sup\u00adporting \nroutines) were produced using a combination of manual proof and proof-producing synthesis (e.g. [20]). \nTheorem 22 (x64 code Implements Terminating Bytecode Execu\u00adtions). Same as the theorem above, but with \n.bc instead of .. Proof sketch. Induction on the number of steps. Theorem 23 (x86-64 Implementation of \nREPLi step). Executing the x64 code-generated code for the result of the bootstrapping (i.e. bytecode) \nand the bytecode snippet that calls repl step has the desired effect w.r.t. bc heap. Proof sketch. Follows \nfrom theorems 18, 19 and 22. The only source of possible divergence in our x86-64 imple\u00admentation of \nREPLi is the execution performed by bc eval. When the logic function bc eval returns None, we want to \nknow that the underlying machine gets stuck in an in.nite loop and that the output stays the same. (Only \nthe top-level loop is able to print output.) repl diverged out aux = D .(now (.bs. bc heap bs aux * (bs.output \n= out))) Theorem 24 (x86-64 Divergence). For any bs, such that bc eval bs = None, we have: '' ' (.bs \n. bs . * bs =. bs.output = bs .output) =. temporal {(base , x64 code 0 bs.code)} (now (bc heap bs (base \n, aux)) . later (repl diverged bs.output aux) . now (out of memory error aux)))  Proof sketch. Theorem \n21 and temporal logic. Top-level Correctness Theorem The top-level theorem for the entire x86-64 implementation \nis stated as follows. Theorem 25 (x86-64 Implementation of REPLs). If the state starts from a good initial \nstate (init), then execution behaves according to REPLs l for some list l of type inference failures. \ntemporal entire machine code implementation (now (init inp aux ) . later ((.l res . repl returns (out \nres ) aux . (REPLs l inp res . terminates res )) . (.l res . repl diverged (out res ) aux . (REPLs l \ninp res . \u00acterminates res )) . now (out of memory error aux))) Here repl returns states that control \nis returned to the return pointer of aux, and out and terminates are de.ned as follows. out Terminate \n= \"\" out Diverge = \"\" out (Result str rest) = str @ out rest terminates Terminate = true terminates Diverge \n= false terminates (Result str rest) = terminates rest Proof sketch. The execution of bytecode is veri.ed \nas sketched above. The other parts of the x86-64 implementation (the setup code, the lexer and the code \nthat ties together the top-level loop) was veri.ed, again, using a combination of manual Hoare logic \nreasoning and proof-producing synthesis. Theorem 14 was used to replace REPLi by the top-level speci.cation \nREPLs. 11. Small Benchmarks To run the veri.ed x86-64 machine code, we inline the code into a 30-line \nC program, which essentially just allocates memory (with execute permissions enabled) then runs it (passing \nin function pointers for getc and putc). The result of running a few benchmarks is shown below. Exe\u00adcution \ntimes are compared with interpreted OCaml: CakeML runs the Fibonacci example 2.2 times faster than interpreted \nOCaml. compiled OCaml Poly/ML CakeML Fibonacci 7.9 4.6 2.2 Quicksort 3.1 10.0 0.6 Batched queue 2.0 12.9 \n0.4 Binary tree 4.3 5.6 0.6 The Fibonacci benchmark computes the 31st Fibonacci number using the na\u00a8ive \nrecursive de.nition; the second benchmark applies Quicksort to a list of length 2,000; the batched queue \nbenchmark performs enqueue-enqueue-dequeue 1,000,000 times on a purely functional implementation of queues; \nthe last benchmark con\u00adstructs a 2,000-element binary tree and then .attens it. We used OCaml version \n4.01 and Poly/ML 5.4.1. 12. Design and Context Our overall goal for CakeML is to provide the most secure \nsystem possible for running veri.ed software and other programs that re\u00adquire a high-assurance platform. \nThus, our primary focus has been on reducing the trusted computing base, rather than on compiler optimisations \nor exotic language features. 12.1 Trusted Computing Base Our correctness theorem relies on a machine \ncode semantics for x86-64 and on a semantics for CakeML. If the machine code se\u00admantics does not soundly \nre.ect actual processor behaviour, then the program might not behave as veri.ed. Having a machine code \nsemantics in the trusted computing base is intrinsic to the problem. The only alternative is to restrict \nexecution to a veri.ed processor, severely limiting the usefulness of the veri.ed compiler compared to \none that targets off-the-shelf hardware. However, the target ma\u00adchine code semantics only needs to cover, \nand be tested on, the instructions that the compiler actually generates, which in our case is signi.cantly \nsmaller than the entire x86-64 ISA. If a programmer wants to understand what their CakeML pro\u00adgram does, \nthey currently have two strategies: one, is to reason about it relative to the CakeML semantics, and \nthe other is to syn\u00adthesise veri.ed CakeML from higher-order logic (e.g., using the same technique [22] \nthat we use for bootstrapping). In the latter case, the CakeML semantics is not part of the trusted computing \nbase, because the synthesised CakeML is veri.ed with respect to the CakeML semantics. In this sense, \nthe CakeML semantics is just the interface between the compiler, and the user s chosen veri.ca\u00adtion approach. \nIn the future, we hope to implement further (veri.ed) ways to verify CakeML code, in the spirit of the \nPrinceton veri.ed software toolchain [1], which takes the same viewpoint, but for a C-like source language. \nWe also trust a small amount of code to set up the initial exe\u00adcution environment with functions to get \na character from standard input and to write a character to standard output, because our ma\u00adchine model \ndoes not include these features. A theorem prover or checker is also intrinsically part of the trusted \ncomputing base. In practice, our proofs can only be checked by automated means: too much of their content \nis in the minute de\u00adtails. Furthermore, the bootstrapping and machine code synthesis steps use veri.ed \ncomputation in the prover itself to create the ver\u00adi.ed code: the proofs generated here are not human \nreadable. One could apply a separate proof checking tool (e.g., OpenTheory [9]), or simply trust HOL4 \nwhich follows the LCF approach and relies for its own soundness only on a small ( 1000 lines) trusted \nkernel. Lastly, we note that we do not rely on the correctness of another compiler in our trusted computing \nbase (except perhaps as part of the proof checker or theorem prover). 12.2 Other Targets, Other Sources \nCakeML currently translates from an SML-like language to x86-64 machine code; however, neither of those \nchoices are mandated by our approach. In the future, we hope to extend CakeML to generate ARM machine \ncode as well. Because CakeML compiles to a low\u00adlevel, machine independent bytecode, and then to machine \ncode, retargeting it only requires introducing a new bytecode to machine code level. This means that \nthe amount of effort required to get a veri.ed compiler for a second platform is much smaller than the \nef\u00adfort required to build the system in the .rst place. Even though go\u00ading through a machine-independent \nbytecode can potentially limit the compiler s ability to generate optimal code, we consider it well worth \nit in this context. It would take more effort to adapt CakeML to new source languages: any of the lexer, \nparser, type checker and compilation to intermediate language (or even bytecode, if the source language \nis different enough) might have to change. However, this work would not be of a different character \nit would just be in re-doing proofs of different parsing/typechecking/etc. algorithms following the strategy \nlaid out here. In particular, the use of clocks (Section 7) to handle divergence and maintaining equivalent \nsmall-step and big-step semantics will be helpful in building a veri.ed compiler for any similar language. \nWhat about OCaml? For the features that CakeML supports, SML and OCaml are very similar, and CakeML follows \nOCaml in several ways including the capitalisation restrictions on variables and constructors, and our \nlack of equality types and overloading. However, OCaml lacks two things that are important to our de\u00advelopment: \ndeterministic evaluation order and an equality function that always terminates (OCaml s follows reference \ncells into their contents, instead of just comparing them for identity). Thus, in or\u00adder to be a semantic \nsubset of an existing language, SML is our only choice. However, retargeting CakeML to an OCaml-like \nsyn\u00adtax (albeit with these small differences) would just be a matter of providing a new lexer and parser. \n13. Related Work The Jitawa veri.ed Lisp implementation [21] has a similar goal of end-to-end veri.cation \nabout the compiler running as machine code. However, the source language of Jitawa is simpler than ours, \nwith much simpler parsing, no type system, no modules, no han\u00addleable exceptions, no pattern matching, \nand no higher-order func\u00adtions. Thus, CakeML demonstrates a substantial scaling-up of the kind of language \nthat can be supported in a very high assurance set\u00adting. Furthermore, Jitawa does not bootstrap itself, \nand its top-level correctness theorem assumes that every input program terminates. The CompCert compiler \n[14] and projects based on it, in\u00adcluding CompcertTSO [29] and the Princeton veri.ed software toolchain \n[1], focus on a C-like source language in contrast to our ML-like language. The emphasis is variously \non optimisations, concurrency and program logics. Whereas our emphasis is on end\u00adto-end correctness and \na very small trusted computing base. The VLISP project [7], which produced a rigorously not formally \n veri.ed implementation of Scheme, emphasised end\u00adto-end veri.cation and did bootstrap their Scheme compiler \nto produce a veri.ed implementation of their compiler. Chlipala [4] is the most closely related work \non veri.cation of compiling impure functional programs with higher-order functions. Chlipala s compiler \nhas simpler source and target languages, and its proofs do not address divergence. He emphasises the \nuse of parametric higher-order abstract syntax (PHOAS) instead of the more conventional substitution \nsemantics. For CakeML, we made the closure environments explicit in order to keep things simple. Turning \nto the front end, there has been signi.cant focus on mechanised meta-theory for language researchers \n(e.g., POPL-Mark [2]), but it has not typically focussed on .tting into the over\u00adall context of a veri.ed \ncompiler. For type inference, Naraschewski and Nipkow [25] verify algorithm W for a basic typed .-calculus \nplus let expressions. Our overall approach is similar to theirs, but they verify completeness and generalise \nnested lets, whereas we have a much richer language. Our approach to type soundness is similar to OCaml \nlight [26], which also uses a concrete representa\u00adtion for ordinary variables and de Bruijn indices for \ntype variables. The languages supported are also similar, except that they support type abbreviations \nwhereas we support a module system. They also use a substitution-based semantics. Two other notable formalisa\u00adtions \nof ML metatheory by Lee at al. [13] and by Garrigue [6] fo\u00adcus on sophisticated type system features \n(functors and structural polymorphism, respectively). Veri.ed parsing has been a productive area recently. \nOur work distinguishes itself in its combination of soundness, completeness, guaranteed termination, \nand relative ef.ciency. Jourdan et al. [10] validate LR automata generated from a CFG, proving soundness \nand completeness. However, they have to provide a fuel param\u00adeter in order to ensure that the automata \ns execution terminates. Ridge [27] veri.es a sound, complete and terminating parsing al\u00adgorithm for arbitrary \nCFGs. As the input grammars may be ambigu\u00adous, Ridge s algorithm returns a list of parses. It is also \nrather inef\u00ad.cient, potentially O(n 5). Earlier still, work by Barthwal and Nor\u00adrish [3] achieves soundness \nand completeness, but is again missing termination. The big difference between these approaches and our \nown is that our approach is not generic as theirs are. Our PEG was hand-crafted, and its proofs of soundness \nand completeness with respect to the CFG were done manually. 14. Conclusion The primary aim of CakeML \nis to provide an implementation of a practical programming language running on off-the-shelf hard\u00adware \nwith the highest-levels of security and trustworthiness. We hope that it will be used as a platform for \nthe development and deployment of programs where their correctness is the most impor\u00adtant concern. Thus \nthe trade-offs we have made in the design of the project differ from other efforts along the same lines. \nIn particular, our focus has been on minimising the trusted computing base, not on optimisation or on \nbreadth of source language features. In this sense, we believe that CakeML complements the veri.cation \nef\u00adforts based around CompCert, which are focussed on optimisation and mainstream programming languages. \nHowever, the design of CakeML does not rule out source-or IL-level optimisations, such as good support \nfor multiple argument functions, inlining, constant propagation, and lambda lifting; the interesting \nquestion will be how to integrate them into the existing veri.cation without requiring unreasonable effort. \nFurthermore, it does not appear dif.cult to add lower level optimisations with only modest changes to \nCakeML Bytecode, for example, the addition of registers and some form of register allocation. Lastly, \na veri.ed compiler is most important in the context of veri.ed applications to run on it. We are already \nworking toward one example a theorem prover using CakeML as its execution environment [24] and hope \nthat others will join in with applica\u00adtions drawn from a variety of domains. References [1] A. W. Appel. \nVeri.ed software toolchain (invited talk). In ESOP, volume 6602 of LNCS, 2011. [2] B. E. Aydemir, A. \nBohannon, M. Fairbairn, J. N. Foster, B. C. Pierce, P. Sewell, D. Vytiniotis, G. Washburn, S. Weirich, \nand S. Zdancewic. Mechanized metatheory for the masses: The POPLMark challenge. In TPHOLs, volume 3603 \nof LNCS, 2005. [3] A. Barthwal and M. Norrish. Veri.ed, executable parsing. In ESOP, volume 5502 of LNCS, \n2009. [4] A. Chlipala. A veri.ed compiler for an impure functional language. In POPL, 2010. [5] M. Felleisen, \nR. B. Findler, and M. Flatt. Semantics Engineering with PLT Redex. MIT Press, 2009. [6] J. Garrigue. \nA certi.ed implementation of ML with structural poly\u00admorphism. In APLAS, volume 6461 of LNCS, 2010. [7] \nJ. Guttman, J. Ramsdell, and M. Wand. VLISP: A veri.ed implemen\u00adtation of Scheme. Lisp and Symbolic Computation, \n8(1/2):5 32, 1995. [8] HOL4. http://hol.sourceforge.net. [9] J. Hurd. The OpenTheory standard theory \nlibrary. In NASA Formal Methods, volume 6617 of LNCS, 2011. [10] J.-H. Jourdan, F. Pottier, and X. Leroy. \nValidating LR(1) parsers. In ESOP, volume 7211 of LNCS, 2012. [11] A. Koprowski and H. Binsztok. TRX: \nA formally veri.ed parser interpreter. Logical Methods in Computer Science, 7(2), 2011. [12] R. Kumar \nand M. Norrish. (Nominal) Uni.cation by recursive descent with triangular substitutions. In ITP, volume \n6172 of LNCS, 2010. [13] D. K. Lee, K. Crary, and R. Harper. Towards a mechanized metatheory of Standard \nML. In POPL, 2007. [14] X. Leroy. Formal veri.cation of a realistic compiler. Commun. ACM, 52(7), 2009. \n[15] X. Leroy and H. Grall. Coinductive big-step operational semantics. Inf. Comput., 207(2), 2009. [16] \nA. McCreight, T. Chevalier, and A. P. Tolmach. A certi.ed framework for compiling and executing garbage-collected \nlanguages. In ICFP, 2010. [17] R. Milner. A theory of type polymorphism in programming. J. Comput. Syst. \nSci., 17(3), 1978. [18] R. Milner, M. Tofte, R. Harper, and D. MacQueen. The De.nition of Standard ML \n(Revised). MIT Press, 1997. [19] M. O. Myreen. Veri.ed just-in-time compiler on x86. In POPL, 2010. [20] \nM. O. Myreen and G. Curello. Proof pearl: A veri.ed bignum imple\u00admentation in x86-64 machine code. In \nCPP, volume 8307 of LNCS, 2013. [21] M. O. Myreen and J. Davis. A veri.ed runtime for a veri.ed theorem \nprover. In ITP, volume 6898 of LNCS, 2011. [22] M. O. Myreen and S. Owens. Proof-producing synthesis \nof ML from higher-order logic. In ICFP, 2012. [23] M. O. Myreen, K. Slind, and M. J. C. Gordon. Extensible \nproof\u00adproducing compilation. In CC, volume 5501 of LNCS, 2009. [24] M. O. Myreen, S. Owens, and R. Kumar. \nSteps towards veri.ed implementations of HOL Light. In ITP, volume 7998 of LNCS, 2013. [25] W. Naraschewski \nand T. Nipkow. Type inference veri.ed: Algorithm W in Isabelle/HOL. J. Autom. Reasoning, 23(3-4), 1999. \n[26] S. Owens. A sound semantics for OCaml light. In ESOP, volume 4960 of LNCS, 2008. [27] T. Ridge. \nSimple, functional, sound and complete parsing for all context-free grammars. In CPP, volume 7086 of \nLNCS, 2011. [28] S. Sarkar, P. Sewell, F. Zappa Nardelli, S. Owens, T. Ridge, T. Braibant, M. O. Myreen, \nand J. Alglave. The semantics of x86-CC multiprocessor machine code. In POPL, 2009.  . [29] J. Sevc\u00b4ik, \nV. Vafeiadis, F. Zappa Nardelli, S. Jagannathan, and P. Sewell. Relaxed-memory concurrency and veri.ed \ncompilation. In POPL, 2011.  [30] D. Vytiniotis, S. L. Peyton Jones, T. Schrijvers, and M. Sulzmann. \nOutsideIn(X) Modular type inference with local assumptions. J. Funct. Program., 21(4-5), 2011. [31] A. \nK. Wright and M. Felleisen. A syntactic approach to type sound\u00adness. Inf. Comput., 115(1), 1994.   \n\t\t\t", "proc_id": "2535838", "abstract": "<p>We have developed and mechanically verified an ML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as an interactive read-eval-print loop (REPL) in x86-64 machine code. Our correctness theorem ensures that this REPL implementation prints only those results permitted by the semantics of CakeML. Our verification effort touches on a breadth of topics including lexing, parsing, type checking, incremental and dynamic compilation, garbage collection, arbitrary-precision arithmetic, and compiler bootstrapping.</p> <p>Our contributions are twofold. The first is simply in building a system that is end-to-end verified, demonstrating that each piece of such a verification effort can in practice be composed with the others, and ensuring that none of the pieces rely on any over-simplifying assumptions. The second is developing novel approaches to some of the more challenging aspects of the verification. In particular, our formally verified compiler can bootstrap itself: we apply the verified compiler to itself to produce a verified machine-code implementation of the compiler. Additionally, our compiler proof handles diverging input programs with a lightweight approach based on logical timeout exceptions. The entire development was carried out in the HOL4 theorem prover.</p>", "authors": [{"name": "Ramana Kumar", "author_profile_id": "81500648118", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P4383789", "email_address": "Ramana.Kumar@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Magnus O. Myreen", "author_profile_id": "81392605670", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P4383790", "email_address": "Magnus.Myreen@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Michael Norrish", "author_profile_id": "81100614629", "affiliation": "NICTA, Canberra, Australia", "person_id": "P4383791", "email_address": "Michael.Norrish@nicta.com.au", "orcid_id": ""}, {"name": "Scott Owens", "author_profile_id": "81337492133", "affiliation": "University of Kent, Kent, United Kingdom", "person_id": "P4383792", "email_address": "S.A.Owens@kent.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535841", "year": "2014", "article_id": "2535841", "conference": "POPL", "title": "CakeML: a verified implementation of ML", "url": "http://dl.acm.org/citation.cfm?id=2535841"}