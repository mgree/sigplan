{"article_publication_date": "01-08-2014", "fulltext": "\n Minimization of Symbolic Automata Loris D Antoni Margus Veanes University of Pennsylvania Microsoft \nResearch lorisdan@cis.upenn.edu margus@microsoft.com Abstract Symbolic Automata extend classical automata \nby using symbolic alphabets instead of .nite ones. Most of the classical automata al\u00adgorithms rely on \nthe alphabet being .nite, and generalizing them to the symbolic setting is not a trivial task. In this \npaper we study the problem of minimizing symbolic automata. We formally de.ne and prove the basic properties \nof minimality in the symbolic setting, and lift classical minimization algorithms (Huffman-Moore s and \nHopcroft s algorithms) to symbolic automata. While Hopcroft s al\u00adgorithm is the fastest known algorithm \nfor DFA minimization, we show how, in the presence of symbolic alphabets, it can incur an exponential \nblowup. To address this issue, we introduce a new al\u00adgorithm that fully bene.ts from the symbolic representation \nof the alphabet and does not suffer from the exponential blowup. We pro\u00advide comprehensive performance \nevaluation of all the algorithms over large benchmarks and against existing state-of-the-art imple\u00admentations. \nThe experiments show how the new symbolic algo\u00adrithm is faster than previous implementations. Categories \nand Subject Descriptors F.2.2 [Theory of Computa\u00adtion]: Automata over in.nite objects, Regular languages \nKeywords Minimization, Symbolic Automata 1. Introduction Classical automata theory builds on two basic \nassumptions: there is a .nite state space;and there is a .nite alphabet. The topic of this paper is along \nthe line of work challenging the second assumption. Symbolic .nite automata (SFAs) are .nite state automata \nin which the alphabet is given by a Boolean algebra that mayhave anin.\u00adnite domain, and transitions are \nlabeled with predicates over such algebra. Symbolic automata originated from the intent to support regular \nexpressions in the context of static and dynamic program analysis [43]. Lately, they were also used for \nsupporting regular ex\u00adpressions (modulo label theories) in the context of modern logical inference engines \n[9, 42]. Most classical automata algorithms view the size k of the alpha\u00adbet as a constant and use specialized \ndata structures that are opti\u00admized for this view [7]. Therefore, it is not clear if or how such al\u00adgorithms \nwould work when the alphabet is in.nite. Understanding how operations over the .nite alphabet lift to \nthe symbolic setting Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. POPL 14, January 22 24, 2014, \nSan Diego, CA, USA. Copyright c &#38;#169; 2014 ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535849 \nis a challenging task. Some classical automata constructions are extended to SFAs in [26]. For example, \nthe product (intersection) M1\u00d7M2 of two symbolic automata M1 and M2 is computed by .1..2 building product \ntransitions of the form (p1,p2) ----.(q1,q2) .1.2 from transitions p1 - . q1, p2 - . q2,in M1,M2,where \nthe guards .1 and .2 are composed using conjunction and pruned when unsatis.able. The complexity of such \nconstructions clearly depends on the complexity of checking satis.ability in the label theory. In this \nparticular case, constructing the product has com\u00adplexity O(f(i)m 2),where m is the number of transitions, \nf(l) is the cost of checking satis.ability of predicates of size l in the label theory, and i is the \nsize of the biggest predicate in M1 and M2 . This paper focuses on the problem of minimizing automata \nover symbolic alphabets and, to the best of our knowledge, it is the .rst paper that investigates this \nproblem. Minimizing deterministic .\u00adnite automata (DFAs) is one of the fundamental concepts in au\u00adtomata \ntheory. It occurs in numerous areas, such as programming languages, text processing, computational linguistics, \ngraphics, etc. Before looking for an algorithm for minimizing SFAs we .rst need to answer a fundamental \nquestion: what does it mean for an SFA to be minimal? Intuitively, any two distinct states p and q of \na minimal SFA must be distinguishable, where two states p and q are distinguishable if there exists an \ninput sequence s that starting from p leads to a .nal (non-.nal) state and starting from q leads to a \nnon-.nal (.nal) state. This notion is similar to DFA minimality. The original algorithms for minimizing \nDFAs were given by Huffman [29], Moore [34], and Hopcroft [27]. Since, all such algo\u00adrithms use iterations \nover the .nite alphabet, they do not immedi\u00adately extend to the symbolic setting. In the following paragraphs \nwe brie.y describe how we extended the classical algorithms to SFAs, and how we designed a new minimization \nalgorithm that fully takes advantage of the symbolic representation of the alphabet. Our .rst algorithm \nis called MinM SFA and takes inspiration from a reformulation of Moore s (Huffman s) algorithm described \nin [28]. The key idea from the algorithm in [28] is the following: if two states p and q are distinguishable, \nand there exists a 1 1 character a, and transitions d(a, p)= p and d(a, q)= q, then p1 and q1 are distinguishable. \nThis idea nicely translates to the symbolic setting as: if two states p and q are distinguishable, and \nthere exist . . transitions p1 -. p and q1 -. q such that ... is satis.able, then p1 and q1 are distinguishable. \nStarting with the fact that .nal and non-.nal states are distinguish\u00adable, we can use a .xpoint computation \nfor grouping states into groups of indistinguishable states. This procedure uses a number of iterations \nthat is quadratic in the number of states. Moreover, each iteration is linear in the number of transitions. \nUnfortunately, MinM does not scale in the case of a perfor- SFA mance critical application described \nin Section 7.1, where SFAs Table 1. Summary of operations needed over a given label theory in the respective \nminimization algorithms.  MinM SFA MinH SFA MinN SFA Satis.ability checking, ., . , , , Predicate negation \n, , Minterm generation , end up having thousands of states, and the complexity of MinM SFA was not acceptable. \nTo this end, we studied a generalization of Hopcrofts algorithm, called MinH SFA . Hopcroft s algorithm \nis based on a technique called partition re.nement of states, and, with worst case complexity O(kn log \nn) (n = number of states, and k = num\u00adber of alphabet symbols), it is the most ef.cient algorithm for \nmin\u00adimizing DFAs. The main idea behind MinH SFA is to subdivide all the labels in the SFA into non-overlapping \npredicates called minterms. Such minterms can then be seen as atomic characters in the sense of classical \nDFAs, allowing us to use the classical Hopcroft s al\u00adgorithm over the .nite set of minterms. Even though \nMinH per- SFA forms well in the aforementioned application, it suffers from an\u00adother problem: in the \nworst case, the number of minterms can be exponential in the number of edges of the SFA. Moreover, our \nex\u00adperiments showed that this factor can indeed be observed when considering more complex label theories, \nsuch as the theory of pairs over string \u00d7 int (\u00a7 6.4). This leads to our main algorithmic contribution \nin the paper. We designed MinN SFA , a new algorithm for SFA minimization which takes full advantage \nof the symbolic representation of the input alphabet. MinN is inspired by the idea of re.ning the partition \nof SFA the state space used by Hopcroft s algorithm, but does not require the pre-computation of minterms \nas in MinH SFA . The key observation is that the set of relevant predicates can be computed locally, \nrather than using all the transitions of the SFA. While MinN is similar SFA to MinH in terms of state \ncomplexity, it does not suffer from the SFA exponential complexity related to the minterm computation. \nIn fact, in all our experiments, both MinH and MinM are outperformed SFA SFA by MinN SFA . Table 1 presents \na summary of the minimization algorithms, illustrating their dependency on operations over the label \ntheory. We compared the performance of the three algorithms using: 1) the benchmark of randomly generated \nDFAs presented in [3], 2) the SFAs generated by common regular expressions taken from the web, 3) a set \nof SFAs aimed at showing the exponential minterm explosion, 4) a randomly generated set of SFAs over \na complex al\u00adphabet theory, and 5) the SFAs generated during the transformation from Monadic Second Order \nlogic to DFAs [38]. In experiments 2 and 3 we also compared the performance of our implementation of \nMinH against the implementation of Hopcroft s algorithm in [3] SFA andinthe brics.automaton [1] library, \nand we observed similar performance characteristics that validated our implementation. In the .fth experiment \nwe compared our results against Mona [22], the state of the art tool for deciding Monadic Second Order \nlogic. Contributions. In summary, our contributions are: a formal study of the notion of minimality \nof SFAs (\u00a72);  two algorithms for minimizing SFAs based on classical DFA algorithms (\u00a73and \u00a74);  a \ncompletely new algorithm for minimizing SFAs together with a proof of its correctness (\u00a75);  a comprehensive \nevaluation of the algorithms using a variety of different benchmarks (\u00a76); and  a description of several \nconcrete applications of such minimiza\u00adtion algorithms (\u00a77).   2. Effective Boolean algebras and SFAs \nWe .rst formally de.ne the notion of effective Boolean algebra and symbolic .nite automata. Next, we \ndevelop a theory which explains what it means for a symbolic .nite automata to be minimal. An effective \nBoolean algebra A has components (D, ., [[ ]], ., T, ., ., \u00ac). D is an r.e. (recursively enumerable) \nset of domain elements. . is an r.e. set of predicates closed under the Boolean connectives and ., T. \n..The denotation function [[ ]] : . . 2D is r.e. and is such that, [[.]] = \u00d8, [[T]] = D,for all ., . \n. ., [[. . .]] = [[.]] . [[.]], [[. . .]] = [[.]] n [[.]],and [[\u00ac.]] = D \\ [[.]].For . . ., we write \nIsSat (.) when [[.]] \u00d8 = and say that . is satis.able. A is decidable if IsSat is decidable. The intuition \nis that such an algebra is represented program\u00admatically as an API with corresponding methods implementing \nthe Boolean operations and the denotation function. We are primarily going to use the following two effective \nBoolean algebras in the examples, but the techniques in the paper are fully generic. 2BVk is the powerset \nalgebra whose domain is the .nite set BVk, for some k> 0, consisting of all nonnegative integers smaller \nthan 2k, or equivalently, all k-bit bit-vectors. A predicate is rep\u00adresented by a BDD of depth k. 1 The \nBoolean operations corre\u00adspond directly to the BDD operations, . is the BDD represent\u00ading the empty set. \nThe denotation [[\u00df]] of a BDD \u00df is the set of all integers n such that a binary representation of n corresponds \nto a solution of \u00df. SMTs is the decision procedure for a theory over some sort s,say integers, such as \nthe theory of integer linear arithmetic. This algebra can be implemented through an interface to an SMT \nsolver. . contains in this case the set of all formulas .(x) in that theory with one .xed free integer \nvariable x. For example, aformula (x mod k)= 0,say div k, denotes the set of all numbers divisible by \nk.Then div 2 . div 3 denotes the set of numbers divisible by six. We can now de.ne symbolic .nite automata. \nIntuitively, a sym\u00adbolic .nite automaton is a .nite automaton over a symbolic al\u00adphabet, where edge labels \nare replaced by predicates. In order to preserve the classical closure operations (intersection, complement, \netc.), the predicates must form an effective Boolean algebra. DE FINIT IO N 1. A symbolic .nite automaton \n(SFA) M is a tuple (A,Q,q0,F, .) where A is an effective Boolean algebra, called the alphabet, Q is a \n.nite set of states, q 0 . Q is the initial state, F . Q is the set of .nal states,and . . Q \u00d7 .A \u00d7 Q \nis a .nite set of moves or transitions. l Elements of DA are called characters and .nite sequences of \nchar\u00adacters, elements of DA *, are called words; E denotes the empty word. . . A move . =(p, ., q) . \n. is also denoted by p -.M q (or p -. q when M is clear), where p is the source state, denoted Src(.), \nq is the target state, denoted Tgt(.),and . is the guard or predicate of the move, denoted Grd(.). A \nmove is feasible if its guard is satis.\u00ad . able. Given a character a . DA,an a-move of M is a move p \n-. q aa .M - such that a . [[.]], also denoted p -q (or p . q when M is clear). In the following let \nM =(A,Q,q0,F, .) be an SFA. DE FINIT IO N 2. A word w = a1a2 \u00b7\u00b7\u00b7 ak . DA *,is accepted at ai state p \nof M, denoted w . Lp(M),if there exist pi-1 -.M pi for 1 = i = k, such that p0 = p,and pk . F .The language \ndef accepted by M is L(M) = Lq0 (M). l 1 The variable order of the BDD is the reverse bit order of the \nbinary representation of a number, in particular, the most signi.cant bit has the lowest ordinal.  Given \na state q . Q, we use the following de.nitions for the set transitions from and to q: - -. def .def .(q) \n= {. . . | Src(.)= q}, .(q) = {. . . | Tgt(.)= q}. The de.nitions are lifted to sets in the usual manner. \nThe following terminology is used to characterize various key properties of M.A state p of M is called \npartial if there exists a character a for which there exist no a-move with source state p. . . M is \ndeterministic:for all p -. q, p -. q 1 . .,if IsSat(. . .1) then q = q 1 .  M is complete: there are \nno partial states.  . M is clean:for all p -. q . ., p is reachable from q 0 and IsSat(.),  M is normalized:for \nall p, q . Q, there is at most one move from p to q.  M is minimal: M is deterministic, complete, clean, \nnormalized, and for all p, q . Q, p = q if and only if Lp(M)= Lq (M). 2  For the special case in which \nM is deterministic and complete,we denote the transition function using the function dM : DA \u00d7 Q . def \nQ, such that for all a . DA and p . Q, dM (a, p) = q,where q is the state such that p -aq. Observe that, \nbecause of determinism, .M if p .- 1.- 2q2,and a . [[.1 . .2]],then q1 = .M q1, p .M q2. Moreover, due \nto completeness, there exists some q and . such . that p -.M q and a . [[.]]. Determinization of SFAs \nis always possible and is studied in [42]. Completion is straightforward: if M is not complete then add \na new state q\u00d8 and the self-loop q\u00d8 -. q\u00d8, and for each par\u00ad . tial state q add the move (q, -\u00acGrd(.),q\u00d8). \nObserve that ...(q) completion requires negation of predicates. Normalization is obvious: if there exist \nstates p and q and . . two distinct transitions p -. q and p -. q, then replace these ... transitions \nwith the single transition p ---. q. This does not affect Lp(M) for any p. We always assume that M is \nclean. Cleaning amounts to run\u00adning a standard forward reachability algorithm that keeps only reachable \nstates, and eliminates infeasible moves. Observe that in\u00ad . feasible moves p -. q do not add expressiveness \nand might cause unnecessary state space explosion. It is important to show that minimality of SFAs is \nin fact well\u00adde.ned in the sense that minimal SFAs are unique up to renaming of states and equivalence \nof predicates. To do so we use the follow\u00ading construction. Assume M =(A,Q,q0,F, .) to be deterministic \nand com\u00adplete. Let SA denote the .rst-order language that contains the unary relation symbol F and the \nunary function symbol a\u00affor each \u00af a . DA.We de.ne the SA-structure of M, denoted by M,to have the universe \nQ, and the interpretation function: M def M def \u00af F = F, .a\u00af. SA,p . Q (\u00afa (p) = dM (a, p)). Also, let \ndef 0 M(E) = q , def * M(w \u00b7 a) = dM (a, M(w)) for a . DA and w . DA . In other words, M(w) is the state \nreached in M for the word w . DA * . Recall that two S-structures are isomorphic if there 2 It is sometimes \nconvenient to de.ne minimality over incomplete SFAs, in which case the dead-end state q (q0 and Lq (M)= \n\u00d8) is eliminated if = q it is present. exists a bijective mapping between their universes that preserves \nthe meaning of all symbols in S. THEOREM 1. If M and N are minimal SFAs over the same alpha\u00adbet A such \nthat L(M)= L(N),then M and N are isomorphic SA-structures. Proof : Assume M and N to be minimal SFAs \nover A such that L(M)= L(N).We de.ne i : M ~ = N as follows: * def .w . DA (i(M(w)) = N(w)). To show \nthat i is well-de.ned as a function, observe .rst that all states of M correspond to some w because M \nis clean. Second, we prove that for all words v and w,if M(v)= M(w) then N(v)= N(w).Fix v, w . DA * such \nthat M(v)= M(w). Then, for all u . DA * , u . LN(v)(N) . v \u00b7 u . L(N) (by L(M)=L(N)) . v \u00b7 u . L(M) \n. u . LM(v)(M)  (by M(w)=M(v)) . u . LM(w)(M) . w \u00b7 u . L(M)  (by L(M)=L(N)) . w \u00b7 u . L(N) . u . \nLN(w)(N)  So, LN(v)(N)= LN(w)(N), and thus N(v)= N(w) by mini\u00admality of N.So, i is well-de.ned as a \nfunction. By switching the roles of M and N we also get the opposite direction. Thus, (*) .v, w . DA \n* (M(v)= M(w) . N(v)= N(w)) Next, we show that i is an isomorphism. First, we show that i is bijective: \ni is onto because N is clean, i.e., each state of N corresponds to N(w) for some word w; i is into because \nif M(v) = M(w) then, by (*), i(M(v)) = N(v) = N(w)= i(M(w)). Finally, we show that i is an embedding \nof M into N, i.e., that i preserves all the functions and the relations: for all p . QM , p . F\u00afM . i(p) \n. F\u00afN,and forall p . QM and a . DA, i(\u00afa M(p)) = \u00afa N(i(p)). Let p . QM .Let w be any word such that \np = M(w).Then \u00af \u00af p . F M . M(w) . F M . w . L(M) . w . L(N) N NN \u00af\u00af\u00af . N(w) . F . i(M(w)) . F . i(p) \n. F . and, for any a . DA, i(\u00afa M(p)) = i(\u00afa M(M(w))) = i(dM (a, M(w))) = i(M(w \u00b7 a)) = N(w \u00b7 a)= dN \n(a, N(w)) NN N =\u00afa (N(w)) = \u00afa (i(M(w))) = \u00afa (i(p)) Thus M and N are isomorphic. l The theorem implies \nthat minimal SFAs are unique up to renaming of states and up to equivalence of predicates due to normalization. \nDEFINITION 3. Two states p, q . Q are M-equivalent, p =M q, when Lp(M)= Lq (M). l We have that =M is \nan equivalence relation. If = is an equivalence relation over Q, then for q . Q, q/= denotes the equivalence \nclass containing q,for X . Q, X/= denotes {q/= | q . X},and M/= def denotes the SFA M/= =(A,Q/=,q/0 =,F/=, \n./=) where: def . ./= = {(p/=, ., q/=) | p, q . Q, ..((p, ., q) . .)}) (p,.,q)..  Observe that M/= is \nnormalized by construction. We need the following theorem that shows that minimization of SFAs preserves \ntheir intended semantics. THEOREM 2. Let M be a clean, complete and deterministic SFA. Then M/=M is minimal \nand L(M)= L(M/=M ). Proof :Let = be =M . Clearly, M/= is clean and complete because M is clean and complete. \nTo show determinism, let p -aq1, .M/= and p -aq2.Take p1,p2 . p, q1 . q1 and q2 . q2 such that .M/= aa \n p1 -q1 and p2 .M q2.Since Lp1 (M)= Lp2 .M -(M),and M is deterministic, it follows that Lq1 (M)= Lq2 \n(M) i.e., q1 = q2. Thus, (*) .a . DA,p . QM (dM/= (a, p/=)= dM (a, p)/=). Minimality of M/= follows from \nthe de.nition. Next, we show by induction over the length of w that (.) .w . DA * (M(w)/= = M/=(w)) 0 \n0 For w = E we have that M(E)= qM , M/=(E)= q ,and M/= 0 0 q =(qM )/=. M/= For w = v \u00b7 a,where a . DA \nand v . DA * , wehavethat (by (*)) M(v \u00b7 a)/= = dM (a, M(v))/= = dM/= (a, M(v)/=) (by IH) = dM/= (a, \nM/=(v)) = M/=(v \u00b7 a). It follows that, for all words w . DA * , w . L(M) iff M(w) . FM iff M(w)/= . FM/= \niff (by (.)) M/=(w) . FM/= iff w . L(M/=). Thus L(M)= L(M/=). l Theorems 1 and 2 are classical theorems \nlifted to arbitrary (possibly in.nite) alphabets. Theorem 2 implies that SFAs have equivalent minimal \nforms that, by Theorem 1, are unique up to relabeling of states, and modulo equivalence of predicates \nin A. In particular, since for all Q and all equivalence relations E over Q, |Q/E|=|Q|, each equivalent \nform has minimal number of states.  3. Moore s algorithm over symbolic alphabets Moore s minimization \nalgorithm [34] of DFAs (also due to Huff\u00adman [29]) is commonly known as the standard algorithm. Even \nthough the classical version of Moore s algorithm depends on the alphabet being .nite, the general idea \ncan be lifted to SFAs as follows. Given an SFA M, initially, let D be the binary relation (F \u00d7 F c) . \n(F c \u00d7 F ),where F c is Q \\ F . Compute the .xpoint of D as follows: if (p, q). D and there exist moves \n(p 1,.,p) and (q 1,.,q) where . . . is satis.able3 then add (p 1 ,q 1) to D.This process clearly terminates. \nUpon termination, E =(Q \u00d7 Q) \\ D is the equivalence relation =M ,and the SFA M/E is therefore min\u00adimal. \nWe refer to this algorithm as MinM SFA SFA . Observe that MinM checks only satis.ability of conjunctions \nof conditions and does not depend on the full power of the alphabet algebra, in particular it does not \nrequire the ability to complement predicates (assuming the initial completion of M is viewed as a separate \npreprocessing step). This is in contrast with the generalization of Hopcroft s algo\u00adrithm discussed in \nthe next section. Complexity. In the .nite alphabet case Moore s algorithm can be implemented in O(knlog \nn) (using the approach described in [10]), where n is the number of states of the DFA, and k the number \nof characters in the input alphabet. However, such implementation relies on the alphabet being .nite. \n3 In a concrete implementation a dictionary can be used to maintain D similar to the case of DFAs [28, \nSection 3.4]. Pa Q Figure 1. Theideabehind an (a, R)-split of a part P . Our implementation of the version \nof Moore s algorithm pre\u00adsented above has the following complexity. Given an SFA A,let n be the number \nof states of A, m the number of moves of A,and i the size of the largest guard appearing in a transition \nof A (the biggest predicate). If the complexity of the alphabet theory for in\u00adstances of size l is f(l),then \nMinM has complexity O(m 2f(i)), SFA which is O(n 4f(i)) assuming normalized A where m is O(n 2).  4. \nHopcroft s algorithm over symbolic alphabets Hopcroft s algorithm [27] for minimizing DFAs is based on \na tech\u00adnique called partition re.nement of states. The symbolic version of Hopcroft s algorithm is given \nin Figure 3. Initially the set of states is partitioned into two sets: .nal states and non-.nal states, \ni.e., P = {F, Q \\ F }. Hereweassume that theSFA M is complete and nontrivial, so that both F and Q \\ \nF are nonempty. The partition P induces the equivalence relation =P (or = when P is clear from the context) \nover Q, such that Q/= = P. At a high level, P is re\u00ad.ned as follows: suppose there exist parts P, R .P \nand a character a . DA such that some a-move from P goes into R,and some a\u00admove from P goes into Rc (Q \n\\ R). Then the (a, R)-split of P is the set {P1,P2},where P1 (resp. P2)isthe setofall p . P from which \nthere is an a-move into R (resp. Rc). P is then replaced in P by P1 and P2. See Figure 1. The (a, R)-split \nof P is well-de.ned by determinism and completeness of M. The following invariant is maintained by splitting. \nLEMMA 1. After each re.nement step of P, for all p, q . Q,if p = q then Lp(M) = Lq(M). Proof : Initially, \nfor p1 . F : E . Lp1 (M) ,and p2 . Q \\ F : E/. Lp2 (M). So, the statement holds for step i =0. To see \nthat the statement holds after step i +1, .x P, R .Pi and a . DA such that (P1,P2) is the (a, R)-split \nof P . It is enough to show that, for all p1 . P1,p2 (M) = Lp2 (M). Consider p1 -a . P2, Lp1 . q1 and \np2 -a . q2.Since q1 and q2 belong to distinct parts of Pi,byIH, there exists a word w such that w . Lq1 \n(M) iff w/. Lq2 (M). Moreover, by determinism of M,we have a \u00b7 w . Lpj (M) iff w . Lqj (M) for j .{1, \n2}.So, a \u00b7 w . Lp1 (M) iff a \u00b7 w/. Lp2 (M), and thus Lp1 (M) = Lp2 (M). l Assuming a simple iterative \nre.nement loop of P, splitting is repeated until no further splits are possible, i.e., until P is .nest, \nat which point the following property holds. LEMMA 2. Upon termination of re.nement of P, for all p, \nq . Q, if p = q then Lp(M)= Lq(M). Proof : By way of contradiction. Suppose (*) there exists a word \nx and states p1 = p2, such that x . Lp1 (M) . x/. Lp2 (M). Choose w to be a shortest such x.Since w cannot \nbe E (because {p1,p2}. F or {p1,p2}. Q \\ F ), there are a and v such that .1.2 a \u00b7 v = w, and moves p1 \n- . q1 and p2 - . q2 such that a . [[.1]] and a . [[.2]]. The choice of q1 and q2 is unique for given \na by determinism, thus w . Lpi (M) . v . Lqi (M) for i .{1, 2}. So, by (*), v . Lq1 (M) . v/. Lq2 (M). \n def 1 Minterms A(.\u00af) = 2 tree := new Tree (TA, null, null); 3 foreach (. in \u00af.) tree.Re.ne (.); 4 return \nLeaves(tree); //return the set of all the leaf predicates 5 class Tree 6 Predicate .; Tree left; Tree \nright; def 7 Re.ne(.) = 8 if (IsSatA(. .A .) and IsSatA(. .A \u00acA.)) 9 if (left = null) //if the tree \nis a leaf then split . into two parts 10 left := new Tree(. .A .,null,null); //[[.]] n[[.]] 11 right \n:= new Tree(. .A \u00acA.,null,null); //[[.]] \\[[.]] 12 else left.Re.ne(.); right.Re.ne(.); //re.nesubtreesrecursively \n\u00af Figure 2. Minterm generation for . . .A modulo A. We now have the following contradiction. There are \ntwo cases: 1) if q1 = q2,say q1 . R and q2 . Rc for some part R,then P is not .nest because we can split \nthe part P .P such that p1,p2 . P using the (a, R)-split of P .2)if q1 = q2 then, because |v| < |w|and \n(*) holds for x = v and q1,q2 in place of p1,p2, it follows that w is not the shortest x such that (*) \nholds 7. l In addition to the state partition re.nement, in the symbolic case, we also use predicate \nre.nement. Predicate re.nement builds a set of minterms. A minterm is a minimal satis.able Boolean combina\u00adtions \nof all guards that occur in the SFA. The algorithm is shown in Figure 2. It uses a binary tree whose \nleaves de.ne the partition. Initially the tree is the leaf T. Each time a predicate . is used to re\u00ad.ne \nthe tree it may cause splitting of its leaves into .ner predicates. The following example illustrates \nsuch minterm generation. EXAMP L E 1. Consider the alphabet algebra 2BV7 (ASCII charac\u00adters). We use \nstandard regex notation for character classes. Sup\u00adpose that the following two guards occur in the given \nSFA: \\w ([[\\w]] = [[[a-zA-Z0-9_]]]), and \\d ([[\\d]] = [[[0-9]]]). Then, the value of tree in Minterms \n2BV7 ({\\w, \\d}) in line 4 in Figure 2 is either the .rst of the two trees below if \\w is selected .rst \nin the loop of line 3, or else the second tree (note that [[\\d]] S [[\\w]] so [[\\d . \\w]] = [[\\d]]): \n TT   \u00ac\\w \\d \u00ac\\d \\d \\w \\w .\u00ac\\d \\w .\u00ac\\d \u00ac\\w The minterms are the leaves \\d, \\w .\u00ac\\d and \u00ac\\w. l Next, \nwe analyze a property of the set of minterms. Given a predi\u00adcate . . .A and a state p . Q,de.ne: . def \n- d(., p) = {Tgt(.) | . . .(p), IsSat (Grd(.) . .)} . -1 def - d(., p) = {Src(.) | . . .(p), IsSat (Grd(.) \n. .)}  -1 def -1 d(., P ) = d(., p)(for P . Q) p.P def Let Minterms (M) = Minterms A(... Grd(.)). The \nfollow\u00ading proposition implies that all characters that occur in one minterm are indistinguishable. PRO \nP O S I T I O N 1. Let M be deterministic and complete. For all . . Minterms (M) and p . Q, |d(., p)| \n=1. We can therefore treat d as a function from Minterms \u00d7Q to Q and reduce minimization of the SFA to \nminimization of the DFA with alphabet Minterms and transition function d. In particular, we can 1 Min \nH = SFA (M =(A,Q, q0,F, .)) def 2 P:= {F, Q \\F }; //initial partition 3 W := {if (|F |=|Q \\F |) then \nF else Q \\F }; 4 .:= Minterms A({Grd(.) |. ..}); //compute the minterms 5 while (W = \u00d8) //iterate over \nunvisited parts 6 R := choose(W ); W := W \\{R}; 7 foreach (. in .) //iterate over all minterms 8 S := \nd-1(., R); //all states leading into R for given minterm 9 while (exists (P in P) where P nS = \u00d8and P \n\\S = \u00d8) 10 (P,W ):= Split P,W (P, P nS, P \\S); //split P 11 return M/=P ; def 12 Split P,W (P, P1,P2) \n= (P1,W 1)where 13 P1 =(P\\{P }) .{P1,P2}//re.ne P14 W 1 = if (P .W ) then (W \\{P }) .{P1,P2}//both parts \n15 else W .{if (|P1|=|P2|) then P1 else P2}//smaller part Figure 3. Hopcroft s minimization algorithm \nlifted to determin\u00adistic SFAs. M is assumed to be clean, complete, and nontrivial (F = \u00d8 and Q \\ F = \n\u00d8). use Hopcroft s algorithm. We refer to the resulting algorithm by SFA . Such an algorithm is shown \nin Figure 3. Lifting the transition relation to be over minterms is a powerful hammer that can be used \nto adapt most classical automata algo\u00adrithms to the symbolic setting. However, one drawback of minterm \ngeneration is that, in the worst case, the number of minterms of an SFA is exponential in the number \nof guards occurring in the SFA. The following example illustrates a worst case scenario in which, due \nto such a problem, MinH runs in exponential time. MinH SFA EXAMP L E 2. Let the character domain be nonnegative \nintegers < 2k. Suppose \u00dfi(x) is a predicate that is true for x iff the i th bit of the binary representation \nof x is 1, e.g. \u00df3(8) is true and \u00df3(7) is false. Predicate \u00df3 can be de.ned as \u00ac((x&#38;8) = 0), provided \nthat, besides equality, the bitwise-and operator &#38; is a built-in function symbol of .A (e.g., consider \nthe bit-vector theory of an SMT solver). Similarly, we may also use the algebra 2BVk , where the size \nof the concrete BDD representation for \u00dfi is linear in k, it has one node that is labeled by i and whose \nleft child (case bit is 0) is false and whose right child (case bit is 1) is true. The point is that \npredicates are small (essentially constant) in size. Consider the following SFA Mk with such an alphabet \nA. Then Minterms (Mk)= Minterms A({\u00ac\u00dfi,\u00dfi}i<k)= {n }n<2k has 2k elements, where [[ n]] = {n}. For example, \nsuppose k =3, then [[\u00df2 .\u00ac\u00df1 . \u00df0]] = {5}. The minimal automaton is \u00df1 \u00b7\u00b7\u00b7 \u00dfk-1 The dead-end state q\u00d8 \nnecessary for completion is implicit. Complexity. In the .nite alphabet case, Hopcroft s algorithm has \ncomplexity O(kn log n),where n is the number of states of the DFA and k is the number of characters in \nthe input alphabet [27], assuming k is treated as a constant. It is shown in [31] that if k is O(n) then \nthe complexity of the algorithm presented in [27] is  1 Min N = SFA (M =(A,Q, q0,F, .)) def 2 P:= {F, \nQ \\F }; //initial partition 3 W := {if (|F |=|Q \\F |) then F else Q \\F }; 4 while (W = \u00d8) //mainloop \n5 R := choose(W ); W := W \\{R}; . - 6 S := {Src(t) |t ..(R)}; //allstatesleadinginto R //G(p) denotesthesetofallcharacters \nleadingfrom p .S into R 7 G:= {p .Grd(t) |p .S}; . - t..(R),Src(t)=p 8 while (exists (P in P) where P \nnS = \u00d8and P \\S = \u00d8) 9 (P,W ):= SplitP,W (P, P nS, P \\S); //( ,R)-split 10 while (exists (P in P) where \nP nS = \u00d8and 11 exists (p1,p2 in P ) where IsSat(\u00ac(G(p1) .G(p2)))) 12 a := choose([[\u00ac(G(p1) .G(p2))]]); \n13 P1 := {p .P |a .[[G(p)]]}; 14 (P,W ):= SplitP,W (P, P1,P \\P1); //(a, R)-split 15 return M/=P ; Figure \n4. Minimization of deterministic SFAs. M is assumed to be clean, complete and nontrivial. SplitP,W is \nde.ned in Figure 3. O(n 3), but it is also shown that O(kn log n) complexity can be maintained with a \nmore careful implementation. GivenanSFA, let n be the number of states, let m be the num\u00adber of moves, \nand let i be the size of the largest label (guard) of a move. In a normalized SFA there are at most n \n2 moves. Let \u00b5 be the number of minterms of the SFA; \u00b5 is bounded by 2m . Each minterm is of the form \nf1 . ... . fm, where each fi is a guard or a negated guard, and thus minterms have size O(mi).The com\u00adputation \nof minterms is O(2mf(mi)) where f is the complexity to decide satis.ability of formulas of given size. \nThe rest of the algorithm (after line 4) can be seen as a standard implementation of Hopcroft s algorithm, \nwhere the SFA is .rst transformed into a DFA with an alphabet of minterm identi.ers where each SFA la\u00adbel \nhas been replaced by identi.ers of all relevant minterms. Then . is viewed as a concrete alphabet (of \nsuch minterm identi.ers). In the .nal result, the identi.ers are mapped back to minterms and the resulting \nSFA is normalized. The overall complexity is then O(2mf(mi)+ 2m n log n).  5. Minimization without minterm \ngeneration When inspecting the worst case complexity of MinH SFA , the factor that stands out the most \nis the exponential blowup caused by the predicate re.nement (minterm generation). In this section we \nin\u00advestigate a new technique which is based on the symbolic represen\u00adtation of the alphabet, and that \navoids minterm generation. Figure 4 shows MinN SFA , a new minimization algorithm, that does not require \npredicate re.nement. The intuition behind MinN is the following: when splitting a SFA partition s part, \nit is not necessary to provide the exact witness that de.nes the split, instead it is enough to check \nif some witness (or witness set) exists. The main steps are the two inner while loops in Figure 4, they \nboth re.ne the partition P with respect to R.Inthe .rst loop (lines 8 9) a part P is split into P n S \nand P \\ S without using a .xed witness (see Figure 5(a) where {{p1,p1 1 }, {p2,p2 1 }}is a ( ,R)-split \nof P , but it is neither an (a,R)-split nor a (b,R)\u00adsplit of P ). The second loop (lines 10 14) splits \nP if there exists some a that produces an (a, R)-split of P .Suchanelement a  (a) ( ,R)-split Q S (b) \n(a,R)-split  Figure 5. Split cases of P in MinSFA N . Suppose DA = {a, b}. must somehow distinguish \nat least two states p1 and p2 of P , (see Figure 5(b)), so that the split is indeed proper and guarantees \nprogress. The .rst loop (lines 8 9) is an optimization that can be omitted without sacri.cing correctness, \nprovided that for p/. S we let def G(p) = .. The conditions P \\S = \u00d8 and P nS = \u00d8 together imply that \nthere exist p1 . P n S and p2 . P \\ S and thus [[G(p1)]] = \u00d8 but [[G(p2)]] = \u00d8,so \u00ac(G(p1) . G(p2)) is \nsatis.able. The concrete implementation is shown in Figure 6. It differs from the abstract algorithm \nin that it computes local minterms, and does not compute any concrete witnesses (the element a is not \ncomputed) in the second loop. In the concrete implementation it is important to keep the .rst loop for \nthe following reasons. One is ef.ciency, the .rst loop is cheaper. Second is simplicity, it is useful \nto work with G as a dictionary or array whose index set is S,and it is practical to assume that the invariant \nP . S holds during the second loop. Next, we formally prove the correctness of MinN SFA . The proof provides \nmore intuition on how the algorithm works. We then provide more details on how the concrete implementation \nworks. THEOREM 3. MinN SFA (M)) = SFA (M) is minimal, and L(MinN L(M). Proof :We show .rst that the invariant \nof Lemma 1 holds. The invariant clearly holds initially. We show that it is preserved by each split (lines \n8 9, and lines 10 14). First, consider the .rst splitting loop in Figure 4. Fix R .P . - and let S = \n{Src(t ) | t . .(R)}, and choose P .P such that P1 = P n S = \u00d8 and P2 = P \\ S = \u00d8.Fix p1 . P1 and p2 \n. P2. Then, there is a move p1 .1 - . q1 for some q1 . R.Let a . [[.1]]. Since p2 ./S and M is complete, \nfor some q2 . Rc there exist .2 a move p2 - . q2 such that a . [[.2]]. The situation is illustrated in \nFigure 5(a). By using the invariant, Lq1 (M) = Lq2 (M),there is a word w such that w . Lq1 (M) . w/. \nLq2 (M). Thus, by using the fact that M is deterministic, a \u00b7 w . Lp1 (M) . a \u00b7 w/. Lp2 (M). Therefore, \nLp1 (M) = Lp2 (M). Second, consider the second splitting loop. Fix P and p1,p2 . P that satisfy the loop \ncondition. All parts in P that intersect with S must be subsets of S due to the .rst splitting loop, \nso, since M is clean, P . S, IsSat(G(p1)) and IsSat(G(p2)). The condi\u00ad  MinSFA(Automaton<S> fa) { var \nfB = new Block(fa.GetFinalStates()); var nfB = new Block(fa.GetNonFinalStates()); var blocks = new Dictionary<int, \nBlock>(); foreach (var q in fa.GetFinalStates()) blocks[q] = fB; foreach (var q in fa.GetNonFinalStates()) \nblocks[q] = nfB; var W = new BlockStack(); if (nfB.Count < fB.Count) W.Push(nfB); else W.Push(fB); while \n(!W.IsEmpty) { var R = W.Pop(); var G = ... //G in Figure 4 var S = G.Keys; var relevant = ... //blocks \nintersecting with S foreach (var P in relevant){ //lines 8-9 in Figure 4 var P1 = ... //P n S if (P1.Count \n< P.Count) { //( , R)-split of P foreach (var p in P1) { P.Remove(p); blocks[p] = P1;} if (W.Contains(P)) \nW.Push(P1); else if (P.Count <= P1.Count) W.Push(P); else W.Push(P1); }} bool iterate = true; while (iterate) \n{ //lines 10-14 in Figure 4 iterate = false; relevant = ... //blocks intersecting with S foreach (var \nP in relevant) { var P1 = new Block(); var psi = G[P.Current]; //start with some element of P bool splitterFound \n= false; P1.Add(P.Current); while (P.MoveNext()) { var q = P.Current; var phi = G[q]; if (splitterFound) \n{ if (IsSat(psi &#38; phi)) { P1.Add(q); psi = psi &#38; phi;} } else { if (IsSat(psi &#38; !phi)) { \npsi = psi &#38; !phi; //refine the local minterm splitterFound = true; } else { //psi implies phi if \n(IsSat(phi &#38; !psi)) { P1.Clear(); P1.Add(q); //set P1 to {q}psi = phi &#38; !psi; //swap the local \nminterm splitterFound = true; } else P1.Add(q); //psi is equivalent to phi }}} if (P1.Count < P.Count) \n{ //(a, R)-split of P for some a iterate = (iterate || (P.Count > 2)); foreach (var p in P1) { P.Remove(p); \nblocks[p] = P1; } if (W.Contains(P)) W.Push(P1); else if (P.Count <= P1.Count) W.Push(P); else W.Push(P1); \n }}}} ... //construct the result using blocks and normalize it } Figure 6. Concrete implementation of \nMinN SFA . tion IsSat(\u00ac(G(p1) . G(p2))) means that either IsSat(G(p1) . \u00acG(p2)) or IsSat(G(p2) .\u00acG(p1)). \nAssume the former case and choose a . [[G(p1) .\u00acG(p2)]].By de.nition of G, we know that .1 there is a \nmove p1 - . q1 where q1 . R such that a . [[.1]].More\u00adover, since a/. [[G(p2)]],and G(p2) covers all \nthe characters that lead from p2 to R, there must be (by completeness and determin\u00ad .2 ism of M) a move \np2 - . q2 where q2 . Rc and a . [[.2]].See Figure 5(b). It follows as above, by using Lq1 (M) = Lq2 (M), \nthat Lp1 (M) = Lp2 (M). Since each step properly re.nes the partition, Lemma 1 follows. We now show that \nLemma 2 holds. The proof is by way of contradiction. (*) Assume there exists a word x,a part P .P, and \ntwo states p1,p2 . P such that x . Lp1 (M) . x/. Lp2 (M). Let w be shortest such x.Since w cannot be \nE,there exist a and v such that w = a\u00b7v. So, there are, by determinism and completeness of M, unique \nq1 and q2 such that p1 -a-a . q1 and p2 . q2. It follows that v . Lq1 (M) . v/. Lq2 (M).So q1 = q2 or \nelse v satis.es (*) and v is shorter that w. Consider any .xed computation of MinN SFA . It follows from \nthe de.nition of SplitP,W ,that W is always a subset of P and (due to the .rst condition of the update \nto W ), if W ever contains a part containing a state q,then W will keep containing a part that contains \nq until such a part is removed from W in line 5. Next, we show that the following W -invariant must hold \nat all times: for all R . W , q1 . R . q2 . R.Let {., .} = {1, 2}. Suppose, by way of contradiction, \nthat at some point in line 5 a part R is chosen from W such that q. . R,and q. ./R.So, p. . S, with S \nas in line 6. We then have two following cases. 1. If p . ./S,then p. and p. are split apart in the .rst \nsplitting loop. This contradicts the fact that p1 = p2. 2. Assume p . . S and consider the second splitting \nloop. By choice of the character a above, we know that there exist moves  ... . p. -. q. and p. -. q. \nwhere a . [[..]] and a . [[. .]].It fol\u00adlows that a/. [[G(p. )]] (because M is deterministic and q . \n./R) while a . [[G(p.)]].So a . [[G(p.)]] \\ [[G(p. )]], or, in other words, a . [[G(p.).\u00acG(p .)]], and \ntherefore IsSat(\u00ac(G(p.) . G(p .))) holds. Consequently, p. and p . end up in distinct parts upon termination \nof the second splitting loop. This again con\u00adtradicts the fact that p1 = p2. So, initially q1 . F . q2 \n. F , or else the initial part of W violates the invariant. But now consider the point when the part \ncontaining both q1 and q2 is split into two parts containing q1 and q2 respectively. But at this point, \nat least one of those parts will be added to W by de.nition of SplitP,W . Thus, we have reached the desired \ncontradiction, because the W -invariant is violated at that point. We have shown that, upon termination \nof MinN SFA (M), =P co\u00adincides with =M . It follows from Theorem 2 that M/=P is minimal and accepts L(M). \nl Implementation. Asimpli.ed version of our concrete C# imple\u00admentation of MinN is shown in Figure 6. \nParts of a state partition SFA are represented by mutable sets called blocks, i.e., objects of type Block, \nand states are represented by integers. Each block contains a HashSet of states. The search frontier \nW is maintained as a stack of blocks, and the partition P is an array of blocks, called blocks, that \nis indexed by states. The .rst inner while loop (lines 8 9 in Figure 4) is implemented by iterating over \nall blocks P that intersect with S. The content of block P1 becomes P n S, while the content of block \nP is updated to P \\ S. Observe that blocks are objects (pointers), thus if W contains P,after the ( , \nR)-split it will still contain P as well as the new block P1. The second inner while loop (lines 10 14 \nin Figure 4) is im\u00adplemented by en ef.cient encoding of the search for p1 and p2 in line 11 of Figure \n4. Moreover, no concrete witness a is computed. Instead, a local minterm (called psi or . in Figure 6), \nis com\u00adputed using G. This avoids the use of model generation that is more expensive than satis.ability \nchecking (i.e. checking if there exists a model).4 Thus, the implementation does not rely on model gen\u00aderation. \nObserve also that the second inner while loop relies on the fact that all remaining (relevant) blocks \nthat intersect with S must be contained in S due to the .rst inner loop. The split of P 4 Although IsSatA(.) \nis formally de.ned as [[.]]A = \u00d8, satis.ability checking is a more lightweight operation than actual \nmodel generation, in particular, in the context of SMT solvers.  then happens with respect to ., which, \nby construction, is a mem\u00adber of Minterms ({G(p) | p . P }), so some member a of [[.]] would have produced \nthe same split. Relation to classical techniques. Implementation of Hopcroft s algorithm is discussed \nin detail in [7]. In the classical setting, the notions of blocks and (a, R)-splits are standard, the \npair (R, a) is called a splitter in classical algorithms, where it plays a key role. The idea of splitting \na block into two and keeping only the smaller block in the waiting set W is a core classical feature. \nIn the case of partial DFAs, the algorithms are similar except that W must be initialized with both F \nand F c [40]. In classical implementations the waiting set W consists of split\u00adters rather than blocks, \nwhere characters have been preselected. The same is true for partial DFAs except that only those characters \nthat are relevant for a given block are being used, which is bene.\u00adcial for DFAs with sparse transition \ngraphs. In the symbolic setting the character selection is indirect and achieved only through the satis.ability \nchecks using local minterms during the second loop. The alphabet algebra may be in.nite in the case of \nSFAs, this is not possible in the classical setting. As far as we know, the idea behind the .rst inner \nloop of MinN and the notion of ( ,R)-splits SFA (recall Figure 5), say free splitters (R, ), have not \nbeen studied in the classical setting. Complexity. The complexity of MinN depends on several fac- SFA \ntors, most importantly on the representation of predicates and the concrete representation of the partition \nre.nement data structure, that is explained above. First of all, observe that each G(p) has size at most \nO(ni),and the total size of G is O(n 2i). Since the split operator always adds to W only the smallest \npartition, the outer loop is run at most log n times. The two internal loops have different running times: \n the .rst loop is run at most n times, with an internal complexity of O(n), due to the split operation, \nand  the second loop is run at most n times, and if the complexity of the label theory for instances \nof size l is f(l), the complexity of each iteration is at most O(nf(ni)). This is due to the n internal \niterations over the current part P. We also notice that each iteration calls the solver on a predicate \nof size at most  O(ni). We can conclude that MinSFA N has complexity O(n 2log n \u00b7 f(ni)). As we will \nobserve in the next section, the quadratic behavior is not observed in practice.  6. Evaluation We evaluate \nthe performance of MinN SFA ,and MinM SFA , MinH SFA with the following experiments: 1. We minimize the \nrandomly generated DFAs from the bench\u00admark presented in [3]. This experiment analyzes the perfor\u00admance \nin the presence of small .nite alphabets and allows us to validate our implementation of MinH against \nthe results SFA shown in [3]; 2. We minimize the SFAs generated by common regular expres\u00adsions taken \nfrom the web. This experiment measures perfor\u00admance in the case of typical character alphabets (Unicode); \n 3. We minimize the SFAs Mk from Example 2 to show the worst case exponential behavior of MinH  SFA \n. In this experiment we also compare against the brics library, a state-of-the-art automata library that \nuses character ranges as a symbolic representations of .nite alphabets;   Figure 7. Running times on \nbenchmark of randomly generated DFAs. The corresponding SFAs are over the theory of bitvectors and each \nset of input symbols is represented as a predicate using a binary decision diagram (BDD). 4. We minimize \nrandomly generated SFAs over the theory of pairs of integers and strings. Here, we analyze the performance \nin the case in which the alphabet is in.nite; and 5. We implemented the classical procedure for transforming \nMonadic Second Order (MSO) logic formulae into DFAs [38] and measure how the running time of such transformation \nis af\u00adfected by different minimization algorithms. This last test aims at understanding the performance \nin the case of very large al\u00adphabets. We also compare our implementation against the tool Mona [22], \nthe state of the art library for deciding MSO formu\u00adlae.  All the experiments are run on a 64 bits Intel(R) \nXeon(R) 3.60GHz processor with 8 GB of RAM memory. 6.1 Small randomly generated DFAs In [3] the performance \nof several minimization algorithms are com\u00adpared against a set of randomly generated DFAs. Such benchmark \ncontains 5 million DFAs with number of states ranging between 5 and 1000, and alphabet sizes ranging \nbetween 2 and 50. The set of DFAs in [3] is uniformly generated at random, and therefore it offers good \nstatistical coverage of the set of all possible DFAs with such number of states and alphabet sizes. and \nMinH on such set of DFAs. Fig\u00adure 7 shows the results. For simplicity we only plot the results for DFAs \nwith 10, 20, 50, and 100 states. Each plot contains the run\u00adning time for each algorithm, where the x \naxis represents the num\u00adber of symbols in the alphabet. In [3], Moore s algorithm is not considered and \nwe weren t therefore able to validate the accuracy of our implementation of MinM We run MinSFA N , MinSFA \nM SFA SFA . However, we were able to repli\u00adcate the behavior of Hopcroft s algorithm shown [3] using \nMinH SFA . Figure 7 shows how the complexity of MinM highly depends SFA on the number of states, while \nthe complexity of MinH is mainly SFA affected by the number of input alphabets. We can indeed see that, \nalready for 100 states, MinM performs worse than MinH SFA SFA .Inthis experiment, for most of the input \nDFAs the number of minterms was the same as the number of input symbols. It is not a surprise that MinN \nis much faster than both MinM SFA SFA and MinH SFA s performance seems to be resistant SFA . Indeed, \nMinN to both bigger state space and bigger alphabets. Moreover, for no single input MinSFA N is slower \nthan MinM or MinH SFA SFA .  Figure 8. Running times on regexes from regexlib. Both axes are in log-scale. \n 6.2 Unicode regular expressions Next, we compared the performance of different minimization al\u00adgorithms \nover a sample set of 1850 SFAs over the alphabet 2BV16 (of Unicode characters) constructed from typical \nregexes (taken from a public web-site5 of popular regexes). Figure 8 shows the running times. The .gure \nclearly shows how MinN is faster than both SFA MinH and MinM for every input instance. Moreover, it can \nbe SFA SFA appreciated how for bigger state sizes (70-80 states) MinH starts SFA outperforming MinM SFA \n. In all cases, the number of minterms turned out to be smaller (by a factor between 2 and 3) than the \ntotal number of predicates, and the exponential blowup of minterms never occurred. The size of the generated \nSFAs ranged from 2 states to 15800 states with an average of 100 states. After the minimization each \nSFA was 32% smaller (number of states) than the original SFA. The following is a typical regex from the \nsample set: [NS]\\d{1,}(\\:[0-5]\\d){2}.{0,1}\\d{0,},[EW]\\d{1,}(\\:[0-5]\\d){2} The generated SFA uses 16 predicates \n(such as \\d)6 while there are only 7 minterms (such as [:]). For this regex, the determinized SFA has \n47 states and the minimized SFA has 23 states. Since the number of minterms does not blowup, is there \na performance incentive in using MinN SFA in this context? The answer is yes since the total time used \nto minimize all 1700 SFAs was 20 seconds when using MinH SFA , and only 0.8 seconds when using SFA , \nthus showing a 24x speedup. We also measured the performance of the minimization algo\u00adrithm implemented \nin the brics.automaton library (version 1.11\u00ad8) that uses symbolic integer ranges to represent Unicode \nchar\u00adacters, but does not implement them as a Boolean algebra (since ranges are not closed under complement \nand union). We observed that the brics implementation of Hopcroft s algorithms is compa\u00adrable to our \nimplementation of MinH MinN SFA , and for 95% of the regexes the running times of brics Hopcroft s algorithm \nand MinH SFA were at most 5% apart. The trend of MinH in Figure 8 is very simi- SFA lar to that of the \nHopcroft s implementation in the brics library. We decided not to include the latter in the plot for \nthe following reasons: 1) brics is implemented in Java, while our implemen\u00adtations are all in C#, and \n2) in brics, the code corresponding to the minterm computation is not part of the minimization algorithm, \ntherefore the comparison would not be completely fair (especially for big instances). 5 http://regexlib.com/ \n6 We use standard regex character class notation for character predicates. Figure 9. Running times on \nthe Mk SFAs of Example 2. The y\u00adaxis is in log-scale. 6.3 Alphabet corner cases The next experiment \nshows the importance of avoiding the minterm generation used by MinH SFA . We consider the SFAs Mk from \nExam\u00adple 2, for values of k ranging between 1 and 31, over the alphabet 2BV32 of 32-bit bit-vectors. \nFigure 9 shows the running times for MinN SFA ,and MinM SFA , MinH SFA . As expected, the performance \nof MinH degrades exponen- SFA tially. Already for k =11, minimizing the SFA using MinH took SFA 1 second \ndue to minterm generation, while both MinM SFA SFA and MinN took less than 1ms. For MinM the time increased \nfrom 1ms to SFA .1sec with k =31, while for MinN the time remained below 1ms SFA for all the values of \nk. Similarly to the previous experiment, we also compared the run\u00adning time of the sample set against \nHopcroft s and Moore s mini\u00admization algorithm in the brics.autmaton library (version 1.11\u00ad8). For this \nexperiment, we decided to show the brics performance in order to appreciate how this particular example \ncauses the num\u00adber of ranges to grow exponentially, causing Hopcroft s algorithm to be very slow. It \nis interesting to notice that the brics implemen\u00adtation of Moore s algorithm is also slow. This is again \ndue to the alphabet s blowup caused by the ranges algebra.  6.4 Complex theories We compare the performance \nof MinN SFA and MinM over SFA , MinH SFA a sample set of 220 randomly generated SFAs over the theory \nof pairs over string \u00d7 int. The guards of each SFAs are conjunctions of simple predicates over strings \nand integers. In the theory of strings we only allow comparison against a constant string, while for \nintegers we generate unary predicates containing +, -,<, = and integer constants. The set of generated \nSFAs is created as follows. We .rst generated a set S of 10 SFAs with at most 10 states. For each SFA \na . S we also compute the complement, and for every pair a, b . S we compute the union, intersection, \nand difference of a and b. Figure 10 shows the running time of each algorithm for dif\u00adferent numbers \nof states. We .rst observe how the performance of MinH quickly degrades when increasing the number of \nstates. SFA This is mainly due to the large number of minterms. Since the pred\u00adicates are randomly generated, \nmany overlaps are possible and the number of minterms grows quickly. Next, we can see how the per\u00adformance \nof MinM is affected by the increasing number of states. SFA Finally, MinN can quickly minimize SFAs with \nup to 96 states in SFA less than 1.5 seconds. This experiment shows how MinN is not SFA affected by complex \ntheories, while MinH and MinM are both SFA SFA impractical in this setting.   6.5 From Monadic Second-Order \nlogic predicates to DFAs The relation between regular languages and logic has been exten\u00adsively investigated \nin the past [38]. Particularly, every regular lan\u00adguage can be expressed as a formula in Monadic Second-Order \nlogic (MSO) over strings, and vice versa. In the rest of the sec\u00adtion we assume the alphabet to be a \n.nite set S,say {a, b}.The following is an example of an MSO formula: f = .x.a (x) where a is a unary \nrelation symbol. A string s . S * is a model of f, iff there exists a position x in the string with label \na. Transform\u00ading MSO formulas to automata gives us an algorithm for deciding satis.ability of MSO formulas. \nThe procedure for converting an MSO formula into a DFA in\u00adductively transforms each sub-formula into \nthe corresponding DFA and then combines such DFAs using classical regular language operations. The complexity \narises from the presence of free vari\u00adables, that range over positions. For example, in the formula a \n(x), x occurs free. In order to represent such a language, the alpha\u00adbet S is extended with one extra \nbit to S \u00d7{0, 1}. The string (a, 0)(b, 0)(a, 1) over the extended alphabet will then represent that the \nthird position of the string aba is assigned to variable x. Following this intuition, every sub-formula \n. is compiled into a DFA over the alphabet S\u00d7{0, 1}n where n is the number of quan\u00adti.ed variables around \n.. For each existential quanti.cation a pro\u00adjection is performed which leads to a non-deterministic automaton \nthat must be determinized when a negation on it is performed. This means that each quanti.er alternation \nmight therefore lead to an ex\u00adponential blowup, and in general the procedure has non-elementary complexity. \nDespite the non-elementary complexity, practical algorithms that can translate non-trivial MSO formulas \nare presented in [22]. The tool implementing such algorithms is called Mona. The two key-features of \nsuch algorithms are: 1. determinizing and minimizing the intermediate DFA at every step in the transformation, \nand 2. using BDDs for representing the lifted bit-vector alphabets.  We implemented the same transformation \nusing the BDD solver in our library, and compared the performance using different mini\u00admization algorithms. \nFigure 11 shows the performance of the transformation for dif\u00adferent MSO formulas. The running time for \nMona are also shown. The four sub-.gures depict the running time for the MSO to DFA transformation for \nthe following formulas: a) .x1,...,xk.x1 <. . . <xk,for k between 2 and 40, b) .x1, ...,xk.x1 < ... < \nxk . a(x1) . .. . . a(xk),for k between 2 and 40, (a) (b) c) (.x1,...,xk.x1 < ... < xk .a(x1).....a(xk))..y.c(y), \nfor k between 2 and 40, d) .x1,...,xk.fk where f2 =(x1 <x2 . a(x1)) . c(x1),and for n> 2, fn = fn-1 .((xn-1 \n<xn .a(xn-1)) .c(xn-1)), for k between 2 and 17. All the values missing from the graphs are due to the \nalgorithms running out of memory. The .gure shows the following behaviors: for all of the four classes \nof formulas, the transformation based on MinN is able to create the DFAs for higher values of k SFA (number \nof nested variables) than those supported by Mona; for small instances Mona is faster than our implementation. \nHowever for higher values of k our implementation is faster, even when using MinH SFA ; SFA or MinM \nwhile Mona immediately shows an exponential behavior (very steep slope), we couldn t observe such trend \nin our implemen\u00adtation; and  MinN is faster than MinH SFA , and it is also less  and MinM memory intensive \n(it runs out of memory for higher k). SFA SFA Even though such formulas are only a small benchmark, \nthey are quite representative of the kind of inputs that cause Mona s trans\u00adformation to be impractical. \nWe believe that the performance im\u00adprovement of our implementation with respect to Mona is primar\u00adily \ndue to a better use of the BDD solver, rather than due to the minimization algorithm.  7. Applications \nThe development of the theory of symbolic automata, including use of minimization, is motivated by several \nconcrete practical problems. Here we discuss three such applications. In each case we illustrate what \nkind of character theory we are working with, the role of minimization, and focus on the bene.ts of the \nsymbolic representation. 7.1 Regex processing Practical applications of regular expressions or regexes \nis ubiqui\u00adtous. Practical regexes differ from schoolbook regular expressions in the following aspects: \nthey support, besides non-regular features that go beyond capa\u00adbilities of .nite state automata representations, \nconstructs such as bounded quanti.ers and character classes, that make them more appealing (more succinct) \nthan their classical counter\u00adparts, and  the size of the alphabet is 216 due to the widely adopted UTF16 \nstandard of Unicode characters. As a somewhat unusual example, the regex ^[\\uFF10-\\uFF19]$ matches the \nset of digits in the so-called Wide Latin range of Unicode. We let the alphabet algebra be 2BV16 .Let \nthe BDD \u00df7 w represent all ASCII word characters as the set of character codes { 0 ,... , 9 , A ,.. ., \nZ , , a ,.. ., z }. We write 0 for the code 48, a for the code 97, etc. Let also \u00dfd 7 repre\u00adsent the \nset of all decimal digits { 0 ,..., 9 } and let \u00df rep\u00adresent underscore { }. By using the Boolean operations, \ne.g., \u00df7 .\u00ac(\u00df7 . \u00df ) represents the set of all upper-and lower\u00ad w d case ASCII letters. As a regex character \nclass it is expressible as [\\w-[\\d_\\x7F-\\uFFFF]]. Regexes are used in many different contexts. A common \nuse of regexes is as a constraint language over strings for checking pres\u00adence or absence of different \npatterns, e.g., for security validation of packet headers in network protocols. Another application, \nis the use of regexes for generating strings that match certain criteria, e.g., for fuzz testing applications. \nAs another application consider the password generation problem based on constraints given in form of \nregexes. Here is a concrete set of constraints adopted in pass\u00adword generation:7 1. Length is k and characters \nare in visible ASCII range: ^[\\x21-\\x7E]{k}$  2. There are at least two letters: [a-zA-Z].*[a-zA-Z] \n 3. There is at least one digit: \\d 4. There is at least one non-word character: \\W  The smallest feasible \nvalue for k is obviously 4. Consider SFAs A1,A2,A3,and A4 for each case and let A be their product. For \nk =4 the minimized version of A has 12 states, and minimizing A with any of MinN SFA ,or MinM takes a \nfew ms. When SFA , MinH SFA k is increased to 40,8 the number of states increases to 444 and minimizing \nusing MinN or MinH takes respectively 15 and 90 SFA SFA ms, while MinM becomes impractical, taking more \nthan 43 sec SFA (450x slower). The minimal (canonical) form of A plays an important role here. Together \nwith .niteness of the language accepted by A, minimality is used to guarantee a uniform distribution \nof the passwords gener\u00adated from A. Uniformity is achieved by using the canonical form of the state graph \ntogether with the canonical form of the guards. While uniform sampling of a . [[.]] is not possible for \narbitrary alphabet theories, it is a well-known feature of BDDs.  7.2 Sanitizer analysis Sanitizers \nare string transformation routines (special purpose en\u00adcoders) that are extensively used in web applications, \nin particular as the .rst line of defense against cross site scripting (XSS) attacks. There are at least \nthree, semantically very different, string sanitizers involved in a single web page: CssEncoder, UrlEncoder, \nand Htm\u00adlEncoder. A large class of sanitizers (including all the ones men\u00adtioned above) can be described \nand analyzed using symbolic .nite state transducers (SFTs) [25]. SFAs are used in that context for cer\u00adtain \noperations over SFTs, for example for checking domain equiv\u00adalence of SFTs [44]. Since several basic \noperations are quadratic 7 Recall the standard convention: a regex without the start-anchor ^ matches \nany pre.x and a regex without the end-anchor $ matches any suf.x. 8 Relevant values of k in the above \nscenario depend on the concrete context, but range between 4 and 128 in the number of states (product, \nunion), minimization comes in handy as a tool for reducing the state space size, In this setting we choose \na different character theory. We use integer linear modular arithmetic (or bit-vector arithmetic of an \nSMT solver, in our case Z3 [19]) in place of BDDs. The main advantage of this choice is that it makes \nit possible to seamlessly combine the guards over characters with expressions over yields, i.e. the symbolic \noutputs of SFT moves. A concrete example of a yield is the following transformation that takes a character \nand encodes it as a sequence of .ve other characters: f : .x.[ &#38; , # , (((x \u00f7 10) mod 10) + 48), \n((x mod 10) + 48), ; ] In general, a yield denotes a function from DA to DA * (or to DB when the output \nalphabet B is different from the input alpha\u00adbet A). In the yield above, for example, f( a ) is the sequence \n[ &#38; , # , 9 , 7 , ; ] (or the string \"&#38;#97;\"). Given f as above, a typical SFT move . looks like: \n(.x.0<x<32)/.x.f(x) . : q --------------. q This speci.c rule happens to be an HtmlEncoder rule for encoding \ncontrol characters in state q and remaining in that state. What is the connection with SFAs and the theory \nmentioned above? For analyzing the idempotence of an encoder with such rules, the encoder is sequentially \ncomposed with itself. As a result, this leads to an SFT with more complex guards and outputs (SFTs are \nclosed under composition). When composing the move . with itself (i.e., roughly speaking, feeding the \n.ve output characters as its inputs again .ve times in a row), the guard of the composed rule will have \nsub-conditions such as 0 < (((x \u00f7 10) mod 10) + 48) < 32, which may involve nontrivial arithmetic operations \n(in this particular case the guard of the composed move will be infeasible). One of the operations performed \nduring idempotence analysis is checking whether the original SFT and the composed one have the same domain. \nThis reduces to language equivalence of SFAs for which the guards involve arithmetic operations of the \nabove kind, that are not readily expressible using the earlier BDD based approach. Domain equivalence \nof two SFTs checks that both the SFTs accept/reject the same input sequences. Maintaining the SFAs minimal \nspeeds up the equivalence check, and in general provides a better understanding of the structure of the \ndomain language. In general, the alphabet theory may be a Boolean combination of other decidable theories \nthat are available for example in state\u00adof-the-art SMT solvers. In the context of sanitizers, encoders, \nand decoders, the alphabet theory is a combination of lists, tuples, bit\u00advectors and integer linear arithmetic. \nLists are used for example to represent composite characters or characters that represent looka\u00adhead \n[17]. We demonstrated in the evaluation section that, when the alphabet theory is complex the algorithm \nMinN outperforms SFA both MinH and MinM by several orders of magnitude enabling SFA SFA analysis of bigger \nsanitizers/encoders.  7.3 Solving Monadic Second Order logic We already anticipated in Section 6.5 how \nMonadic Second Order (MSO) logic predicates can be transformed into equivalent DFAs using a non-elementary \nalgorithm. Such algorithm also provides a decision procedure for MSO. We already discussed how several \ntechniques have been intro\u00adduced by the tool Mona [22] in order to make such transformation practical. \nKeeping the DFA minimal at any step in the transforma\u00adtion is one of the key techniques. We gave experimental \nevidence of the fact that the new algorithm presented in this paper, and in general the use of symbolic \nautomata, can further move the barrier of solvable MSO formulas, in terms of both formula s size (number \nof nested quanti.ers) and running times.   8. Related work DFA Minimization: The classical algorithms \nfor DFA minimiza\u00adtion have been studied and analyzed extensively in several dif\u00adferent aspects. In particular, \nMoore s algorithm is studied in [10] where it is shown that, by a clever change of data structures, its \ncomplexity can be reduced from O(kn2) to O(kn log n).The bound O(kn log n) has been shown to be tight \nfor Hopcroft s al\u00adgorithm [6, 8]. Brzozowski [13] observed that a DFA can be mini\u00admized by reversing \nits transitions, then determinizing, then revers\u00ading its transitions again, and .nally determinizing \nagain. However, due to the determinization steps, this procedure is exponential and we decided not to \nconsider it in this paper. Linear time algorithms have been studied for minimizing acyclic automata [32, \n36] and automata containing only simple cycles [2]. The book chapter [7] provides an in-depth study of \nthe state-of-the-art techniques for au\u00adtomata minimization, including the approaches mentioned above \nand several other ones. Watson [45] also provides an elegant clas\u00adsi.cation of the classical minimization \nalgorithms based on their structural properties. In the case of DFAs, it matters whether the DFA to be \nmini\u00admized is partial (incomplete), because it may be useful to avoid completion of DFAs with sparse \ntransition graphs. Minimization of partial DFAs is studied in [5, 39, 40]. In this setting the complex\u00adity \nof the algorithm depends on the number of transitions rather than on the size of the alphabet, however, \nin the case of DFAs these two quantities are generally related. In contrast, in a normalized SFA the \nnumber of transitions is independent of the alphabet size, and it is at most n 2 where n is the number \nof states. One concrete difference between the minimization algorithms of complete DFAs versus partial \nDFAs is that the initial value of the waiting set W (see Figure 3) for the partial case must contain \nboth the sets F and F c [40]. We believe that similar modi.cations may be applied to SFA , even though \nwe expect that in the case of SFAs the bene.t of using partiality might not be as visible. In fact, a \ncomplete SFA has at most n transitions more than a partial one. Moreover, in the case of SFAs, there \nare different ways for completing an SFA, e.g., one can effectively restrict the alphabet to only those \ncharacters that are mentioned in the trimmed SFA (SFA without dead-end states) prior to completion. MinN \nDifferent Notions of Minimization: A notion of incremental minimization is investigated in [47]. An incremental \nminimiza\u00adtion algorithm can be halted at any point and produce a partially minimal DFA which is equivalent \nto the starting DFA, and has less or equal number of states. If the algorithm runs till completion, a \nminimal DFA is computed. In this paper we did not address in\u00adcremental computation, however it would \nbe interesting to identify variants of the presented algorithms with such a property. A sim\u00adilar idea, \nbased on intermediate computation results, is used for a modular minimization algorithm in [14]. An algorithm \nfor building a minimal DFA accepting a given .nite set of strings is presented in [15]. The same paper \ninvestigates the problem of maintaining a minimal DFA when applying modi.cation such as node, edge, or \nstring deletion and insertion to an already minimal DFA. This class of problems is called dynamic minimization. \nA parallel version of Moore s algorithm is presented in [37]. We are not aware of a parallel version \nof Hopcroft s algorithm. We leave as an open problem identifying parallel algorithms cor\u00adresponding to \nMinH SFA . SFA and MinN A variant of minimization called hyper-minimization is investi\u00adgated in [24]. \nGiven an input DFA A,a DFA A1 is hyper minimal with respect to A if it is the smallest DFA that differs \nfrom A only on a .nite number of strings. In the case of symbolic automata, this de.nition doesn t extend \nnaturally due to potentially in.nite alphabet. A less restrictive notion called k-minimization is studied \nin [21]. Given an input DFA A,a DFA A1 is k-minimal with re\u00adspect to A if it is the smallest DFA that \ndiffers from A only on strings of length smaller or equal than k. This second restriction naturally extends \nto symbolic automata. A more general notion is that of minimization up to E-equivalence, where given \na regular language E,the DFA A1 is allowed to differ from A on a set of strings L . E [23]. Extending \nthe results of [21, 23] to symbolic automata is an interesting open research direction. Relationships \nbetween minimality and selection of .nal states are studied in [35] where uniform minimality is de.ned \nas minimality for any choice of .nal states. Automata with Predicates: The concept of automata with predi\u00adcates \ninstead of concrete symbols was .rst mentioned in [46] and was .rst discussed in [41] in the context \nof natural language pro\u00adcessing. A symbolic generalization of Moore s algorithm was .rst discussed in \n[43]. To the best of our knowledge, no other minimiza\u00adtion algorithms have been studied for SFAs. The \nMONA implemen\u00adtation [22] provides decision procedures for monadic second-order logic, and it relies \non a highly-optimized BDD-based representa\u00adtion for automata which has seen extensive engineering effort \n[30]. Therefore, the use of BDDs in the context of automata is not new, but is used here as an example \nof a Boolean algebra that seems particularly well suited for working with Unicode alphabets. Minimization \nof Other Automata: The problem of automata minimization has been investigated in several other settings. \nA new approach for minimizing nondeterministic automata and nondeter\u00administic B \u00a8uchi automata has been \nstudied in [33]. The problem of minimizing weighted automata (automata computing functions from strings \nto a semi-ring) is studied in [20]. Classical minimiza\u00adtion algorithms are used in this setting, and \nwe hope that the results shown in this paper can extend to such domain. The minimization problem has \nalso been studied for timed automata [11] and register automata [16]. These models are able to represent \nin.nite domains, but not arbitrary theories. An orthogonal direction is to extend the techniques presented \nin this paper to minimization of symbolic tree automata [18]. Other Applications: The problem of learning \nof symbolic trans\u00adducers has recently been studied in [12]. Classical automata learn\u00ading is based on \nAngluin s algorithm [4], which fundamentally relies on DFA minimality. An almost unexplored topic is \nwhether such learning techniques can be extended to SFAs.  9. Conclusions We presented three algorithms \nfor minimizing Symbolic Finite Au\u00adtomata (SFAs). We .rst extended Moore s algorithm and Hopcroft s algorithm \nto the symbolic setting. We then show that, while in the classical setting Hopcroft s algorithm is the \nfastest known mini\u00admization algorithm, in the presence of symbolic alphabets, it might incur in an exponential \nblowup. To address this issue, we intro\u00adduced a new minimization algorithm that fully bene.ts from the \nsymbolic representation of the alphabet and does not suffer from the exponential blowup. The new algorithm \nis a re.nement of Hopcroft s one in which splits are computed locally without hav\u00ading to consider the \nentire input alphabet. The new algorithm can also be adopted in the classical setting. Finally, we implemented \nall the algorithms and provided experimental evidence that the new minimization algorithm is the most \nef.cient one in practice. Acknowledgments. We thank Marco Almeida for helping us accessing his benchmark \nof randomly generated DFAs. Loris D Antoni was supported by the NSF Expeditions in Computing award CCF \n1138996, and this work was done as part of an intern\u00adship at Microsoft Research. We also thank the anonymous \nreview\u00aders for their insightful comments.   References [1] BRICS .nite state automata utilities. http://www.brics.dk/automaton/. \n[2] J. Almeida and M. Zeitoun. Description and analysis of a bottom\u00adup DFA minimization algorithm. Information \nProcessing Letters, 107(2):52 59, 2008. [3] M. Almeida, N. Moreira, and R. Reis. On the performance of \nautomata minimization algorithms. Technical Report DCC-2007-03, University of Porto, 2007. [4] D. Angluin. \nLearning regular sets from queries and counterexamples. Inf. Comput., 75(2):87 106, 1987. [5] M.-P. B \n\u00b4eal and M. Crochemore. Minimizing incomplete automata. In Finite-State Methods and Natural Language \nProcessing, 7th Interna\u00adtional Workshop, pages 9 16, 2008. [6] J. Berstel, L. Boasson, and O. Carton. \nHopcroft s automaton mini\u00admization algorithm and Sturmian words. In DMTCS 2008, pages 355 366, 2008. \n[7] J. Berstel, L. Boasson, O. Carton, and I. Fagnot. Minimization of automata. To appear in Handbook \nof Automata, 2011. [8] J. Berstel and O. Carton. On the complexity of Hopcroft s state minimization algorithm. \nIn CIAA 2004, volume 3317, pages 35 44, 2004. [9] N. Bj\u00f8rner, V. Ganesh, R. Michel, and M. Veanes. An \nSMT-LIB format for sequences and regular expressions. In P. Fontaine and A. Goel, editors, SMT 12, pages \n76 86, 2012. [10] N. Blum. An 0(n log n) implementation of the standard method for minimizing n-state \n.nite automata. Information Processing Letters, 57:65 69, 1996. [11] M. Bojaczyk and S. Lasota. A machine-independent \ncharacterization of timed languages. In A. Czumaj, K. Mehlhorn, A. Pitts, and R. Wat\u00adtenhofer, editors, \nAutomata, Languages, and Programming, volume 7392 of LNCS, pages 92 103. Springer Berlin Heidelberg, \n2012. [12] M. Botin .c. Sigma*: symbolic learning of input-output can and D. Babi \u00b4speci.cations. In \nPOPL 13, pages 443 456, New York, NY, USA, 2013. ACM. [13] J. A. Brzozowski. Canonical regular expressions \nand minimal state graphs for de.nite events. In Proc. Sympos. Math. Theory of Au\u00adtomata, pages 529 561, \nNew York, 1963. [14] D. Bustan. Modular minimization of deterministic .nite-state ma\u00adchines. In In 6th \nInternational Workshop on Formal Methods for In\u00addustrial Critical Systems, pages 163 178, 2001. [15] \nR. C. Carrasco and M. L. Forcada. Incremental construction and maintenance of minimal .nite-state automata. \nComput. Linguist., 28(2):207 216, June 2002. [16] S. Cassel, B. Jonsson, F. Howar, and B. Steffen. A \nsuccinct canonical register automaton model for data domains with binary relations. In S. Chakraborty \nand M. Mukund, editors, ATVA 2012, volume 7561 of LNCS, pages 57 71. Springer, 2012. [17] L. D Antoni \nand M. Veanes. Equivalence of extended symbolic .nite transducers. In N. Sharygina and H. Veith, editors, \nCAV 2013, volume 8044 of LNCS, pages 624 639. Springer, 2013. [18] L. D Antoni, M. Veanes, B. Livshits, \nand D. Molnar. Fast: A transducer-based language for tree manipulation. Technical Report MSR-TR-2012-123, \nMicrosoft Research, November 2012. [19] L. de Moura and N. Bj\u00f8rner. Z3: An Ef.cient SMT Solver. In TACAS \n08, LNCS. Springer, 2008. [20] M. Droste, W. Kuich, and H. Vogler. Handbook of Weighted Automata. Springer \nPublishing Company, Incorporated, 1st edition, 2009. [21] P. Gawrychowski, A. Je.On minimising automata \nz, and A. Maletti. with errors. In MFCS 11, pages 327 338, Berlin, Heidelberg, 2011. Springer. [22] J. \nHenriksen, J. Jensen, M. J\u00f8rgensen, N. Klarlund, B. Paige, T. Rauhe, and A. Sandholm. Mona: Monadic second-order \nlogic in practice. In TACAS 95, volume 1019 of LNCS. Springer, 1995. [23] M. Holzer and S. Jakobi. From \nequivalence to almost-equivalence, and beyond minimizing automata with errors. In H.-C. Yen and O. Ibarra, \neditors, Developments in Language Theory, volume 7410 of LNCS, pages 190 201. Springer, 2012. [24] M. \nHolzer and A. Maletti. An nlogn algorithm for hyper-minimizing a (minimized) deterministic automaton. \nTheor. Comput. Sci., 411(38\u00ad39):3404 3413, 2010. [25] P. Hooimeijer, B. Livshits, D. Molnar, P. Saxena, \nand M. Veanes. Fast and precise sanitizer analysis with Bek. In USENIX Security, August 2011. [26] P. \nHooimeijer and M. Veanes. An evaluation of automata algorithms for string analysis. In VMCAI 11, volume \n6538 of LNCS, pages 248 262. Springer, 2011. [27] J. Hopcroft. An nlogn algorithm for minimizing states \nin a .nite au\u00adtomaton. In Z. Kohavi, editor, Theory of machines and computations, Proc. Internat. Sympos., \nTechnion, Haifa, 1971, pages 189 196, New York, 1971. Academic Press. [28] J. E. Hopcroft and J. D. Ullman. \nIntroduction to Automata Theory, Languages, and Computation. Addison Wesley, 1979. [29] D. Huffman. The \nsynthesis of sequential switching circuits. Journal of the Franklin Institute, 257(3 4):161 190,275 303, \n1954. [30] N. Klarlund, A. M\u00f8ller, and M. I. Schwartzbach. MONA implemen\u00adtation secrets. International \nJournal of Foundations of Computer Sci\u00adence, 13(4):571 586, 2002. [31] T. Knuutila. Re-describing an \nalgorithm by Hopcroft. Theor. Comput. Sci., 250(1-2):333 363, 2001. [32] S. L. Krivol. Algorithms for \nminimization of .nite acyclic automata and pattern matching in terms. Cybernetics, 27:324 331, 1991. \n[33] R. Mayr and L. Clemente. Advanced automata minimization. In POPL 13, pages 63 74, 2013. [34] E. \nF. Moore. Gedanken-experiments on sequential machines. Au\u00adtomata studies, Annals of mathematics studies, \n(34):129 153, 1956. [35] A. Restivo and R. Vaglica. Some remarks on automata minimality. In G. Mauri \nand A. Leporati, editors, DLT 2011, volume 6795 of LNCS, pages 15 27. Springer, 2011. [36] D. Revuz. \nMinimisation of acyclic deterministic automata in linear time. Theoret. Comput. Sci., 92:181 189, 1992. \n[37] A. Tewari, U. Srivastava, and P. Gupta. A parallel DFA minimization algorithm. In HiPC 2002, volume \n2552 of LNCS, pages 34 40. Springer, 2002. [38] W. Thomas. Languages, automata, and logic. In Handbook \nof Formal Languages, pages 389 455. Springer, 1996. [39] A. Valmari. Fast brief practical DFA minimization. \nInformation Processing Letters, 112:213 217, 2012. [40] A. Valmari and P. Lehtinen. Ef.cient minimization \nof DFAs with partial transition functions. In S. Albers and P. Weil, editors, STACS 2008, pages 645 656, \nDagstuhl, 2008. [41] G. van Noord and D. Gerdemann. Finite state transducers with predi\u00adcates and identities. \nGrammars, 4(3):263 286, 2001. [42] M. Veanes, N. Bj\u00f8rner, and L. de Moura. Symbolic automata con\u00adstraint \nsolving. In C. Ferm \u00a8uller and A. Voronkov, editors, LPAR-17, volume 6397 of LNCS/ARCoSS, pages 640 654. \nSpringer, 2010. [43] M. Veanes, P. de Halleux, and N. Tillmann. Rex: Symbolic regular expression explorer. \nIn ICST 10, pages 498 507. IEEE, 2010. [44] M. Veanes, P. Hooimeijer, B. Livshits, D. Molnar, and N. \nBj\u00f8rner. Symbolic .nite state transducers: Algorithms and applications. In POPL 12, pages 137 150, 2012. \n[45] B. W. Watson. A taxonomy of .nite automata minimization algo\u00adrithms. Computing Science Report 93/44, \nEindhoven University of Technology, January 1995. [46] B. W. Watson. Implementing and using .nite automata \ntoolkits. In Extended .nite state models of language, pages 19 36, New York, NY, USA, 1999. Cambridge \nUniversity Press. [47] B. W. Watson and J. Daciuk. An ef.cient incremental DFA minimiza\u00adtion algorithm. \nNat. Lang. Eng., 9(1):49 64, Mar. 2003.  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Symbolic Automata extend classical automata by using symbolic alphabets instead of finite ones. Most of the classical automata algorithms rely on the alphabet being finite, and generalizing them to the symbolic setting is not a trivial task. In this paper we study the problem of minimizing symbolic automata. We formally define and prove the basic properties of minimality in the symbolic setting, and lift classical minimization algorithms (Huffman-Moore's and Hopcroft's algorithms) to symbolic automata. While Hopcroft's algorithm is the fastest known algorithm for DFA minimization, we show how, in the presence of symbolic alphabets, it can incur an exponential blowup. To address this issue, we introduce a new algorithm that fully benefits from the symbolic representation of the alphabet and does not suffer from the exponential blowup. We provide comprehensive performance evaluation of all the algorithms over large benchmarks and against existing state-of-the-art implementations. The experiments show how the new symbolic algorithm is faster than previous implementations.</p>", "authors": [{"name": "Loris D'Antoni", "author_profile_id": "81384602146", "affiliation": "University of Pennsylvania, Philadelphia, USA", "person_id": "P4383899", "email_address": "lorisdan@cis.upenn.edu", "orcid_id": ""}, {"name": "Margus Veanes", "author_profile_id": "81100266422", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4383900", "email_address": "margus@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535849", "year": "2014", "article_id": "2535849", "conference": "POPL", "title": "Minimization of symbolic automata", "url": "http://dl.acm.org/citation.cfm?id=2535849"}