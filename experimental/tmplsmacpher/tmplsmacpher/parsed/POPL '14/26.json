{"article_publication_date": "01-08-2014", "fulltext": "\n Probabilistic Coherence Spaces are Fully Abstract for Probabilistic PCF * Thomas Ehrhard Christine \nTasson Michele Pagani Laboratoire PPS -CNRS -Universit\u00b4e Paris Diderot Laboratoire LIPN -Universit\u00b4e \nParis 13 thomas.ehrhard@pps.univ-paris-diderot.fr michele.pagani@lipn.univ-paris13.fr christine.tasson@pps.univ-paris-diderot.fr \n Abstract Probabilistic coherence spaces (PCoh) yield a semantics of higher\u00adorder probabilistic computation, \ninterpreting types as convex sets and programs as power series. We prove that the equality of inter\u00adpretations \nin PCoh characterizes the operational indistinguishabil\u00adity of programs in PCF with a random primitive. \nThis is the .rst result of full abstraction for a semantics of probabilistic PCF. The key ingredient \nrelies on the regularity of power series. Along the way to the theorem, we design a weighted intersec\u00adtion \ntype assignment system giving a logical presentation of PCoh. Categories and Subject Descriptors D.3.1 \n[Programming Lan\u00adguages]: Formal De.nitions and Theory -Semantics Keywords Full Abstraction, Probabilistic \nPCF 1. Introduction Probabilistic behaviors appear in many places in the study of pro\u00adgraming languages, \nfor instance if the environment of a program behaves randomly or if the program uses probabilistic constructs. \nTo understand how the introduction of probabilities changes the computational landscape, we use Semantics. \nIndeed, in the last decades [24, 27, 30], semantics has succeeded in giving insights on the way programs \ncompute. More precisely, operational semantics allows one to formalize a program by the sequence of its \nexecu\u00adtion steps, while denotational semantics represents programs by functions in some mathematical \nspace relating the interpretations of inputs and outputs in a compositional way. If this last mathe\u00admatical \nrepresentation is correct and accurate enough, then deno\u00adtational properties lead to computational features. \nA key example is full abstraction [23], stating that operational indistinguishability (i.e. behaving \nin the same way in any context) is characterized by denotational equivalence (i.e. having the same interpretation). \nSo semantics is useful both to give a precise meaning to syntactical constructs and to separate non-equivalent \nprograms. * Partially founded by French ANR project C O QUA S (number 12 JS02 006 01) and CNRS chair \nLogique lin\u00b4eaire et calcul . Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. POPL 14, \nJanuary 22 24, 2014, San Diego, CA, USA. Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535865 \nOf course, probabilistic semantics have already been inves\u00adtigated. First, the domain-theoretic approach \nhas led to a prob\u00adabilistic powerdomain [19, 29] which is a sibling of the non\u00ad deterministic power domain \n[27]. This approach follows the com\u00ad putational monad method [25]: programs are interpreted as func\u00ad \ntions from the input domain to the powerdomain of the output domain. Intuitively, a program takes an \ninput and returns a proba\u00adbility distribution on outputs. This line of work has been continued by the \ncontinuous random variable construction [15], introducing standard tools of probability theory into semantics. \nSecondly, the game-theoretic approach [1, 18] has been extended with probabilis\u00ad tic features [5]. Intuitively, \nprobabilistic programs are interpreted as probabilistic strategies, that are stochastic processes on \nthe plays of the games associated with the types of the programs. Quantitative semantics follows another \ntradition stemming from Linear Logic models [10, 11]. From the very beginning, Lin\u00ad ear Logic has been \nassociated with intuitions coming from calculus and linear algebra [7, 8, 12]. Indeed, programs are interpreted \nas entire series between mathematical spaces or as analytic functors between sets [17], and programs \nthat use their resources only once are interpreted as linear functions. This connection with resource \nconsumption has been fruitful in the last decade with the intro\u00adduction of differential nets and resource \ncalculus [2, 9]. Recently, quantitative semantics has been explicitly related to the study of quantitative \nproperties such as time or space consumed by a com\u00adputation [22]. This illustrates a new paradigm where \na semiring of scalars allows one to encode non-deterministic or probabilistic computations as opposed \nto the domain paradigm where monads are used. Another important tenet of Linear Logic is the perfect \ndu\u00adality between programs and environments since a program can be seen as the environment of other programs. \nTherefore, represent\u00ading probabilistic environments or programs will boil down to the same study. Probabilistic \ncoherence spaces provide a quantitative semantics [4, 14] which model probabilistic computations. The \nmain contribution of the present work is to show that proba\u00adbilistic coherence spaces provide a fully \nabstract model of PPCF, a probabilistic extension of the functional programing language PCF [28]. Although \nthe proof follows the general pattern that consists in .nding a de.nable context that separates two terms, \nthe key in\u00adgredient is based on Calculus (see Lemma 25) since programs are interpreted as power series. \nTo our knowledge, no known model of probabilistic P CF has yet been proved to be fully abstract. Games \nsemantics provide fully ab\u00adstract models of standard PC F via an extensional collapse [1, 18]. This technique \nhas been adapted to probabilistic game seman\u00adtics, providing a fully abstract model of probabilistic \nidealized A L G OL [5] (an extension of probabilistic P CF with references). Our result characterizes \nthe operational indistinguishability without the need of an extensional collapse and deals with a functional \nproba\u00adbilistic language with no references.  Section 2 is devoted to an insight on the way programs \nand data are interpreted in probabilistic coherence spaces. Section 3 is devoted to the syntax and the \noperational semantics. Next, in Section 4, we describe the key notions of probabilistic coherence spaces \nthat will be useful to prove, in Section 5, full abstraction. Along the way to the theorem, we de.ne \nan intersection type as\u00adhappen. The interpretation of a datum is now given by a sequence of non-negative \nintegers indexed by the possible outcomes: [Coin] = ( 1 , 1 , 0, . . . ) and [Rand(n)] = ( 1 , . . . \n, 1 , 0 . . . ). 2 2 n n n In general, we will interpret the integer type by subprobability1 distributions \nover N:  (.n)n.N . RN + signment system (Figure 3) giving a logical presentation to the s.t.n.N .n = \n1 . P (Int) = model. This system has an interest by its own, allowing one to turn a question of computing \nthe semantics of a term (and hence its oper\u00adational behavior) into a proof search problem. Finally, in \nSection 6, we show that inequational full abstraction fails, i.e. the semanti\u00adcal order does not coincide \nwith the operational one. For this, we achieve a context lemma for probabilistic PC F (Proposition 31). \nNotation 1. We write N for the set of non negative integers, N* for the set of positive integers (N* \n. N\\{0}), R+ for the set of non = . negative real numbers and R+ = R+ . {8} for the completed real half \nline. Let S be a set, rS denotes its cardinality. Multisets of elements of S are identi.ed with functions \nS . N. If m is such a multiset, Supp (m) denotes its support set {a . S s.t. m(a) = 0}. A .nite multiset \nis a multiset with a .nite support. We write Mf (S) For probabilistic programs, we follow the same pattern \nas for probabilistic data. First, their non deterministic behavior is de\u00adscribed. Then, coef.cients are \nintroduced in order to render their quantitative behavior. As an example, let us consider the program \nRand : Int . Int that takes an input value n, and returns any non negative integer strictly less than \nn with equal chances and make it interact with probabilistic data (e.g. a probabilistic distribution \nover N). Focusing on the association between input and output values, its non deterministic behavior \nis described as a relation: |Rand| . |Int| \u00d7 |Int|  |Rand| =(n, a) s.t. n . N, a . {0, . . . , n - 1}. \nfor the set of all .nite multisets of elements of S. We enumerate m by using (a, i) . m to denote a . \nSupp (m) and 1 = i = m(a). Whenever (a1, . . . , an) . Sn, we write [a1, . . . , an] for the .nite multiset: \na . S . r{i s.t. ai = a}. The empty multiset is [ ] and l . is the multiset union: (m lp)(a) = m(a) + \np(a). A vector v . RS + Then we associate a coef.cient to each pair input-output. It rep\u00adresents the \nquantitative account of getting the given output know\u00ading the input. The interpretation of the program \nis now turned into a matrix indexed by the values interpreting inputs (column indices) and outputs (raw \nindices)2: [Rand] . (R+)|Int|\u00d7|Int| is given by its values va on any index a . S. Given a multiset m(a) \n 0 1 2 \u00b7\u00b7\u00b7 n \u00b7\u00b7\u00b7 . ... .. m . m . Mf (S), we de.ne the power v . For =v 1 1 a.Supp(m) 0 1 a .0 \u00b7 \u00b7 \u00b7 \n\u00b7 \u00b7 \u00b7 2 n . RS . + be the base vector (ea)b ......... ......... 1 1 any a . S, let ea da,b, with da,b \n0 0 .1 = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 2 n denoting the Kronecker symbol. Let us .x our typographic conventions: a, b, \nc range over web .. . . [Rand] = (1). ... .. 0 . .. . . .n-1 elements and m, p, q over multisets. v, \nw, u range over vectors, f, ., . over matrices, a, \u00df, . over monomials. A, B, C range over . . 1 . 0 \nn simple types, Int being the integer type. x, y, z range over term variables. M, N, P range over terms. \nFinally, ., \u00b5, . range over scalars in R+ and i. denotes a list of scalars and similarly for the other \nmetavariables. 2. Denoting Probabilistic Programing Probabilistic programs use two levels of randomness: \nthe .rst one on data (i.e. terms of ground type), the second one on programs (i.e. terms of higher-order \ntype). A probabilistic datum is a random variable whose outcome is given to the program. Thus, a probabilistic \ndatum will be character\u00adized by its law, that will be its interpretation. Then, a probabilistic program \nuses random instructions and behaves like a function from probabilistic data to probabilistic data. Moreover, \na program can call several times its argument x. Each of its occurrences is repre\u00adsented by the outcome \nof an independent random variable with the same distribution as x. We consider two examples of probabilistic \ndata of type Int. The .rst one, Coin, is the toss of a 0/1 fair coin. The second one, Rand(n), follows \nthe discrete uniform distribution with outcomes between 0 and n - 1. If we are only interested in resulting \nvalues, then Int is inter\u00adpreted by the set of non negative integers |Int| = N. First, we consider Coin \nand Rand(n) as non deterministic data, and inter\u00adpret them by the range of the corresponding random variables: \n|Coin| = {0, 1} and |Rand(n)| = {0, . . . , n - 1}. Then, to take into account the randomized behavior \nof the da\u00adtum, we associate a coef.cient to each outcome: its probability to . . . . ... . . . . . The \ninteraction between the program Rand and a probabilistic datum x is then given by the product of the \nmatrix interpreting the program and of the sequence interpreting the datum. Besides, this probabilistic \nprogram preserves subprobability distributions. Actually, this approach is valid only if the program \nuses exactly once its argument. Indeed, as a side effect of the call-by-name3 ex\u00adecution strategy, each \noccurrence of a probabilistic datum behaves as an independent sample of a random variable. So, if a program \nmakes several calls to a given probabilistic datum, then the out\u00adcomes of the calls may differ due to \nthe randomized setting.Thus, we gather the input values into a .nite multiset: .nite, since if the execution \nof a program terminates, then the number of resources effectively used is .nite; multiset rather than \na sequence since this model is not accurate enough to distinguish the order of the inputs. To illustrate \nthis point, we examine the probabilistic programs: . Once = .xInt .if(x, Coin, 42), . Twice = .xInt .if(x, \nif(x, Coin, 42), if(x, 42, 0)), where if(x, , ) branches depending whether x evaluates to zero or not. \nNotice that the two programs uses, among others, probabilistic 1 Since a call to a datum can fail, the \ntotal probability distribution over the possible values may be less than 1. 2 [Rand] is the transpose \nof a stochastic matrix and the image of a subprob\u00adability distribution is the matrix product [Rand] \u00b7 \nv. 3 This phenomenon will also appear in a call-by-value setting, if every probabilistic datum x is \nreplaced by its CPS translation .aInt .x.  instructions and that they call once or twice their input. \nTheir typing rules (Figure 1(b)) are the usual ones, with rand of type interpretation is given by a matrix \nindexed by .nite multisets (the Int . Int. From now on, we call program a closed term of type . input) \nand integers (the output). Coef.cients are non zero only if Int. Figure 1(c) gives the one step reduction \nrelation - ., imple\u00adthe multiset is of size one or two: menting a weak head-reduction (i.e. a lazy call-by-name \nstrategy). The relation is weighted with a probability . . [0, 1] equal to 1 except for steps .ring a \n(rand) n redex, reducing to any numeral [Once] : 0 ) . 1 2 1 ) . 1 . .. . ([0, 0], ([0, 0], [Twice] : \n0 ) . 1 2 1 ) . 1 2 . .. .. ([0], ([0], in {0, . . . , n - 1} with equal probability 1 . The probability \nof a n reduction sequence is the product of the weights of all its steps. Some notational conventions \nare introduced in Figure 1(d) and will be used henceforth. 2 ([0, a], 42) .2 if a = 0([a], 42) .1 if \na = 0 . .. ([a, b], 0 ) .1 if a = 0, b = 0 (m, a ) .0 otherwise. ( m , a ) .0 otherwise. On the second \nexample, two quantitative phenomena are mixed up. The .rst one is probabilistic: coef.cient 1 2 comes \nfrom the use of coin. The second one stems from branching construction: two different execution traces \nare leading to 42, either the two outcomes of the random variable x are .rst 0, and then a = 0, or its \noutcomes are .rst a, and then 0. In our model, these traces are gathered in Example 2. Notice (by induction \non n = 1) that the term choose(Mi)n i=1, for 1 = i = n reduces (in several steps) to any Mi with equal \nprobability n 1 . In general, any reduction sequence has a probability given by a rational number in \n[0, 1], since rand n introduces just fractions. e Example 3. The .x-point constructor yields in.nite \nreduction the same multiset [0, a] and the coef.cient 2 is the combinatorial 1 sequences, like the typical \nloop OA . 1 . .xA OA OA. .x counterpart. This gives a hint of why programs are not represented However, \nwe can have terms with in.nite reduction sequencesas random variables. Now, the interaction of a probabilistic \nprogram with a proba\u00ad but converging to a normal form with probability 1. Indeed, take . M = .x(.xInt \n.x . 0) (see notations in Figure 1(d)) we have: bilistic data boils down to a matrix product adapted \nto take into 1 account multiplicities. 1 e 1 2 .xInt M . 0 .x . 0M  Let us consider a probabilistic \ndatum x : Int and a program M  M : Int . Int. As a reminder, the a-th coef.cient [x]a is the 1 probability \nthat a is the outcome of x. The probability [(M) x]b, 2 that (M) x returns b, is given by decomposing \nthe computation in pairwise disjoint events. Each event corresponds to an intermediate The probability \nthat M reduces to 0 is equal to the sum of the multiset m gathering the input values that are effectively \nused probabilities of all .nite reductions sequences from M to 0, which 8 1 during computation: is n=0 \n2n+1 = 1, n being the number of loops taken by a = [(M) x]b [M](m,b) [x] m.Mf(N) m) is the probability \nthat independent calls sequence. m , (2) A way for precisely de.ning the probability of convergence of \na term to a normal form is by giving the operational semantics as a Markov process over the set of PPCF \nterms, following [4]. The transition matrix PROBA . [0, 1]PPCF\u00d7PPCF is de.ned by: (see Notation 1 for \n[x] to x returns the values gathered in m. In this way, every coef.cient for a . N. [(M) x]b the [x]a \nApplying Formula (2), we compute the non zero coef.cients: is a power series with in.nitely many variables, \nthat are PROBAM,N . = . .. .. . . if M . N , 1 if M = N is a normal form, (3) 0 otherwise. [(Once) x]0 \n= [(Once) x]1 = 1 2 [x]0  [(Once) x]42 +a stochastic matrix (i.e. for all M, = 1). [x]a [x]b N.PPCF \nPROBAM,N [x]a = Notice that PROBAM,N is well-de.ned since there exists at most a=1 . one reduction step \nM . N, once .xed M and N. PROBA is 1 2  [(Twice) x]0 [(Twice) x]1 = 1 2 [x]0 2  Notice that every coef.cient \nof [(Once) x] is a linear function and every coef.cient of [(Twice) x] is a polynomial of degree 2. Besides, \nOnce and Twice preserve subprobability distributions: = 0 The value of PROBAM,N intuitively describes \nthe probability of a,b=1 evolving from the state M to the state N in one step. 2 [x] [(Twice) x]42 = \n2 [x]0 [x]a A term M is absorbing whenever PROBAM,M = 1: the absorb\u00ad a=1 ing states are those which \nare invariant under the transition matrix. Notice that the normal forms are all absorbing, but the converse \nis false, e.g. O is an absorbing term. The n-th power PROBAn of the matrix PROBA is a stochastic matrix \non PPCF (in case n = 0, we have the identity matrix on PPCF). Intuitively, the value of PROBAn is the \nprobability of M,N evolving from the state M to the state N in exactly n steps.  1 + 1 += 1 2 [x]0 \n2 [x]0 [x]a [(Once) x]b = a=1 b  Proposition 4 ([4, Lemma 32]). Let M . PPCF and N absorb\u00ad 2 0 +()2 \n[(Twice) x]b + 2 [x]0)2 = 1. [x]a [x]a = , ing, the sequence {PROBAn M,N }n.N is monotonic. a=1 a=1 b \nWe can thus de.ne, for every program M and n . N:  [x] (= [x]a a=0 3. Probabilistic PCF We de.ne the \nlanguage PPCF in Figure 1. This system is a mi\u00adnor variant with respect to the probabilistic extension \nof PCF pre\u00adsented in [4] (see Remark 6). The grammar of terms is obtained by adding to standard PCF a \nrandom number generator rand.The 8 . 8 k PROBAM,n = sup(PROBAM,n ) (4) k=0 Intuitively, PROBA8 de.nes \nthe probability that M reaches a M,n numeral n in an arbitrary number of steps. In standard PCF the observational \npre-order is de.ned with respect to the termination of a term in a context of type Int. In a probabilistic \nframework like PPCF, one can re.ne such a pre-order  Types A, B, C ::= Int | A . B Terms M, N, P ::= \nx | .xA.M | (M) N | .x(M) | 0 | s(M) | p(M) | if(M, N, P ) | rand (a) Grammar of types and terms. The \nconstant Int is the base type of integers. Given n . N, n will denote its associated numeral, de.ned \nas sn(0). G, x : A f M : B G f M : A . B G f N : A G f M : A . A G, x : A f x : A G f .xA.M : A . B G \nf (M) N : B G f .x(M) : A G f P : Int G f P : Int G f M : Int G f N : A G f P : A G f 0 : Int G f s(P \n) : Int G f p(P ) : Int G f if(M, N, P ) : A G f rand : Int . Int (b) Simple type assignment system. \n.xA.M N 1-. M [N/x] if(0, N, P ) 1-. N p(n + 1) 1-. n .x(M) 1-. (M) .x(M) if(n + 1, N, P ) 1-. P (rand) \nn 1 n-. k, for k < n ..... . M' . M' . M' . M' . M' M -, M not abstract. M -M -M -M - ..... (M) N -) \nN s(M) -) -) if(M, N, P ) . if(M', N, P ) (rand) M . (rand) M' . (M ' . s(M' p(M) . p(M' -- (c) Reduction \nrules. In case n = 0, (rand) n is a normal form. . . OA = .x(.xA .x) M . N = if((rand) 2, M, N ) . . \nchoose( ) = OA choose(M1, . . . , Mn+1) = if((rand) n + 1, Mn+1, choose(M1, . . . , Mn)) . if(.n+1 . \nif(.0 i=1Mi, N, P ) = N i=1 Mi, N, P ) = i=1Mi, N, P ), P ) if(Mn+1, if(.n . . if(M = 0, N, P ) = if(M, \nN, P ) if(M = n + 1, N, P ) = if(p(M) = n, N, P ) (d) Notational conventions. Also, we can use choose(Mi)n \nas a shortcut for choose(M1, . . . , Mn). i=1 Figure 1: Probabilistic extension of PCF. by comparing \nthe probability of convergence. Let CG,A be the set of contexts Q mapping terms M of type A in the environment \nG, into programs Q[M], i.e. closed terms of type Int. De.nition 5 (Observational pre-order). Given G \nf M : A, and G f N : A, we de.ne4: 8 8 M .G N iff .Q . CG,A , PRO BAQ[M],0 = PRO BAQ[N],0 . Let =G be \nthe equivalence induced by .G. Remark 6. There are slightly different probabilistic primitives that can \nbe added to PCF. We have chosen a quite standard one, rand, implementing C A M L function Random.int. \nIn [5, 21] it is also . considered the more basic Coin = (rand) 2. Another possibility is to allow an \narbitrary superposition of two terms M .. N, for . . [0, 1], evaluating to M with probability . and to \nN with probability 1 - ., see e.g. [15, 19]. Concerning n-ary distributions, there is .n i=1.iMi, evaluating \nto any Mi with probability .i [26]. Finally, [4] allows any probabilistic distribution over the whole \nset of numerals (.n)n.N, evaluating to n with probability .n. Obviously, Coin can be de.ned as . 1 as \nwell as (rand) 2. 2 Also, both .. and rand are de.nable as .n i=1.iMi, and the latter as (.n)n.N. However, \nthe converses are less trivial. In [5, Figure 6] the authors show how to de.ne rand as a recursive program \nus\u00ading just Coin. On the contrary, .. is strictly more expressive than rand and Coin, since it is not \ntrue that all real numbers can be approximated by series of rational numbers generated from PPCF terms \n(hence recursively enumerable series). One can then wonder whether the observational equivalence on PCF \nterms depends on the chosen probabilistic primitive. The full abstraction gives a neg\u00adative answer to \nthe question: all such primitives induce the same ob\u00adservational equivalence on PCF terms, which is in \nfact the equiva\u00adlence induced by Pcoh (Corollary 28). 4. Probabilistic Coherence Spaces We present probabilistic \ncoherence spaces [4, 14] and recall the main results needed to state and prove the full abstraction theorem. \nThe category PCoh of probabilistic coherence spaces is a model of PPCF. As usual, types are interpreted \nas objects of the category and programs as maps between the interpretation of the input and output types. \nActually, PCoh is underlied by a model of linear logic that we do not make explicit for the sake of brevity, \nbut we refer to [4]. The numeral 0 chosen for testing the equality is not signi.cant. Indeed, 4.1 Probabilistic \nCoherence Spaces from a context Q semi-separating two terms M, N on a numeral n, i.e. such that PRO \nBA8 = PRO BA8 , one can get a context semi-A probabilistic coherence space is a pair (|A| , P (A)) of \na set Q[M],nQ[N],n separating M and N on another numeral m by applying a suitable number |A| and a set \nP (A) of vectors in the module R|A|. Intuitively, the + of p( ) or s( ) constructors. elements in |A| \nrepresent atomic data, while the vectors in P (A) express probabilistic data. At the ground type, P (Int) \nis in fact the convex set of the subprobability distributions of the elements in |Int|. However, at higher-order \ntypes, this intuition is lost5 and the de.nition of P (A) depends on a duality condition describing the \nprobability of having an interaction between a datum of type A (which is a program) and an environment. \n Consider a program and an environment interacting on atomic data a included in a set |A| such that their \nrespective interpreta\u00adtion are positive real vectors: v, w . R|A|. Their pairing gives a + quantitative \nestimation of the interaction success (v, w) . va wa (5) = . R+. a.|A| We say that their interaction \nis probabilistic whenever (v, w)= 1. We then de.ne a polar operation on sets of vectors P . R|A|as + \n. . w . R|A| P= + s.t. .v . P (v, w)= 1 . (6) The probabilistic duality environment/program is then enforced \nin our model by the closedness condition P .. = P . De.nition 7 ([4, 14]). A probabilistic coherence \nspace, or PCS for short, is a pair A = (|A| , P (A)) where |A| is a countable set called the web of A \nand P (A) is a subset of R|A|satisfying: + 1. P (A).. = P(A), 2. .a . |A|, .. > 0, .v . P (A), va = \n.,  3. .a . |A|, .. > 0, .ea . P (A). As a side effect, Condition (1) forces P (A) to be a convex set. \nCondition (2) requires the projection of P (A) in any direction to be bounded, while (3) forces P (A) \nto cover every direction.6 Example 8. The set P (Int) of subprobability distributions over N yields a \nPCS. In particular, P (Int).. = P (Int), its polar being P (Int). = [0, 1]N . 4.2 The category PCoh The \nobjects of PCoh are the PCSs and the set PCoh(A, B) of morphisms from A to B is the set of matrices f \n. RMf(|A|)\u00d7|B| such that .v . P (A), f(v) . P (B), where + . .b . |B| , (f(v))b = fm,b \u00b7 v m , (7) m.Mf(|A|) \n(see Notation 1 for the de.nition of v m). Following Section 2, a morphism is presented as a matrix that \ngathered the coef.cients of the power series described by the Equation (7). The identity on A is given \nby the matrix . 1 if m = [a], IdA = m,a 0 otherwise. In fact, we have IdA(v) = v, for every v . P (A). \nLet f . PCoh(A, B) and . . PCoh(B, C), their composition must satisfy: .v . P (A) , (. . f)(v) = .(f(v)). \nIn matricial terms, this comes out as: .c . |C|, e (. . f)m,c \u00b7 v m = .p,c \u00b7 f(v) p m.Mf(|A|) p.Mf(|B|) \ne q p(b) = .p,c \u00b7fq,b \u00b7 v. p.Mf(|B|) b.Supp(p) q.Mf (|A|) 5 P (A) is not anymore a subset of [0, 1]|A|, \nsee discussion in Section 2. 6 These conditions are introduced in [4] for keeping .nite all the scalars \ninvolved, yet they are not explicitly stated in the de.nition of PCS in [14]. Now, to extract the coef.cient \nof the monomial v m , we dis\u00adtribute product over sum. This amounts to choosing a partition of m = (q(b,i))(b,i).p \nmatching the enumeration {(b, i) s.t. b . Supp (p) , i = p(b)} of p. Therefore, the composition . . f \nis de.ned7 as the matrix coef\u00ad.cients, for m . Mf (|A|) and c . |C|: . (. . f)m,c = .p,c (8) fm(b,i),b. \np.Mf(|B|) (m(b,i))(b,i).p (b,i).p s.t. m= (m(b,i)) 4.3 PCoh is Cartesian Closed The cartesian product \nof any countable family (Ai)i.I of PCSs is: M M M M . Ai = ({i} \u00d7 |Ai|), i.I i.I  e . .i . I, P Ai \n=v . (R+)| i.I Ai| s.t. , i.I pi(v) . P (Ai) where pi(v) is the vector in R|Ai| denoting the i-th component \nof + e . v, i.e. pi(v) = v(i,a). a The j-th projection Prj . PCoh( i.I Ai, Aj ) and the prod\u00aduct (fi)i.I \n. PCoh(B, i.I Ai) are given by: j . 1 if m = [(j, a)], Prm,a =((fi)i.I )p,(j,a) = (fj )p,a 0 otherwise. \nThe terminal object 1 is given by the empty product (\u00d8, {0}). Notice that the set of points of a PCS \nA, i.e. the set PCoh(1, A), is isomorphic to the convex set P (A). Notation 9. We write A1\u00d7A2 for the \nbinary product: in the sequel, we present any v . P (A1 \u00d7 A2) as the pair (p1(v), p2(v)) . P (A1) \u00d7 P \n(A2) of its components. eMMNotice that the set Mf M i.I AiM is isomorphic to the set\u00ad e theoretic cartesian \nproduct f (|Ai|) , via the map as\u00ad eMi.I M sociating any m . Mf M i.I AiM with the I-indexed family . \n(mi)i.I de.ned as mi(a) = m(i, a). This means that any mor\u00adphism f . PCoh( i.I Ai, B) can be presented \nas a matrix in\u00ad e dexed by sequences in Mf (|Ai|) \u00d7 |B|. i.I The object of morphisms is de.ned as . . \n|A . B| = Mf (|A|) \u00d7 |B| , P (A . B) = PCoh(A, B). PCoh is then turned into a cartesian closed category \nby the eval\u00aduation Ev . PCoh((A . B) \u00d7 A, B) and the curry.cation Cur(f) . PCoh(C, A . B), for every \nf . PCoh(C \u00d7 A, B), de.ned as: . 1 if m = [(p, v)], . Ev(m,p),a =Cur(f)m,(p,b) = f(m,p),b 0 otherwise, \nNotice that the above equations use Notation 9, e.g. representing a multiset in Mf (|(A . B) \u00d7 A|) as \na pair (m, p) of multisets in Mf (|A . B|) \u00d7 Mf (|A|). Actually, the category PCoh is well-pointed in \nthe sense that the equality of morphisms is extensional, i.e. given two matrices f, . . PCoh(A, B), if \nfor every v . P (A), f(v) = .(v), then f = .. To sum up, we have: 7 The de.nition of Equation (8) is \ndue to [17]. On a side note, remark that in [4], the authors use another formulation in which identical \nsummands produced by different partitions are gathered. This gives rise to multinomial coef.cients that \nare hidden in the sums of the present formulation. However, the two de.nitions give rise to the same \nmatrix.  Proposition 10 ([4, \u00a71.6]). PCoh is a well pointed cartesian closed category. 4.4 Object of \nnumerals and Cpo-Enrichement . The object of numerals of PCoh is the PCS Int =(N, P (Int)) equipped with \nthe morphisms z . PCoh(1, Int) . P (Int), pred, succ . PCoh(Int, Int), and ifz . PCoh(Int \u00d7 I nt \u00d7 Int, \nInt) de.ned as Thanks to Notation 9, [M ] G can be described as a vector in\u00addexed by a tuple ( im Mf \n(|G|) and a web element m, b) of i. b . |B|. This convention will be used hereafter. Example 12. If Coin \nis identi.ed with (rand) 2, then the interpre\u00adtation of Once and Twice are the matrices given in Section \n2. . Consider the term OA = .x(.xA .x). Notice that [.xA .x] is different from zero only on the web elements \nof the form ([a], a), so that .xn [.xA .x] (0) = 0 for any natural number n. We conclude [OA] = 0, as \nexpected. Consider now the term P [.xInt .x . 0]= (m,a) zn = d0,n, predm,n = dm,[n+1], succm,n+1 = dm,[n], \n. .x(.xInt = .x . 0). We have that ifz(m,p,q),n = . .. .. 1 m 1 m + , i.e. it is equal to 1 2 when 2 \n[x]a 2 [0]a = [ ] and a = 0, otherwise it is equal to 0. This means that .xn [.xInt .x . 0] (0) = 1 .xn \n[.xInt 1 if (m, p, q) = ([0], [n], []), m = [a], or when m or (m, p, q) = ([k + 1], [], [n]), + 1 +\u00b7 \n\u00b7 \u00b7+ 1 , for any 4 .x . 02]n (0) = 1. otherwise. 2 0 . n > 0. We conclude that [P ] = sup n Proposition \n13 (Soundness [4]). reduction, i.e. for every G f M : B: The natural order on R+ enriches PCoh with a \ncpo-structure, de.ned componentwise on morphisms: i.e., given f, . . PCoh(A, B) The semantics is invariant \nunder f = . iff .m . Mf (|A|) , .b . Mf (|B|) , fm,b = .m,b. G G = PRO BAM,N [N] Proof (Sketch). The \ninvariance under reduction rules of the stan\u00ad [M] . The matrix 0 is the minimum element and the lub of \na directed net (fd)d.D is then given by N e e (fd) . (fd)m,b , dard PCF redexes follows by cartesian \nclosedness and the cpo\u00ad sup = sup m,bd.D d.D enrichement of PCoh. The soundness of the reduction of (rand) \nn Remark 11. The componentwise order on matrices is not ex\u00ad tensional, i.e. there are f, . . PCoh(A, \nB) such that .v . P (A), f(v) = .(v), but f = .. For example, take f, . . PCoh(Int, Int): is straight \nfrom the de.nition of [Rand]. The soundness of the con\u00adtext rules depends on the fact that the interpretation \nof a context is linear in the argument associated with the .red redex. For exam\u00adple, the soundness of \nthe context rule associated with application . 1 if m = [k], n = 0, . 1 if m = [ ], n = 0,fm,n = .m,n \n= 0 otherwise. 0 otherwise. Although f = ., for every subprobability distribution of natural numbers \nv . P (Int), we get f(v) = ( k vk, 0, 0, . . . ) = (1, 0, 0, . . . ) = .(v). In fact, we will use this \nmismatch for disproving the inequality full abstraction in Section 6. 4.5 PCoh is an Adequate Model of \nPPCF The model of PPCF is obtained by extending the usual categorical interpretation of PCF to rand. \nWith a type A, we associate a PCS A, by induction on the type: Int . I nt and A . B . A . B. Let G = \nx1 : A1, . . . , xn : An. The interpretation of a n judgment G f M : B is a morphism [M] G of PCoh( Ai, \nB), i=1 de.ned in Figure 2 by structural induction on the unique derivation of G f M : B. The .x-point \noperator .x(M) is the lub of its approximants, given by induction by 0 . . .xf = 0, .xn+1f = Ev . (f, \n.xnf). The operator rand is de.ned by using (a multiset variant of) the matrix Rand of Equation (1): \n. 1 if m = [n], k < n, [rand]m,k = n 0 otherwise. Together with the categorical interpretation of a term, \nwe de\u00adscribe its action on the vectors in the convex set associated with its input type. Notice that, \nsince the category is well pointed of the category, this action univocally determines the interpretation \nof the term. Notice also that the matrices interpreting the basic constructs in Figure 2 have 0, 1 coef.cients, \nexcept for [rand] which is inter\u00adpreted as the random function that introduces rational numbers in [0, \n1]. Coef.cients greater than 1 may be produced by composition of morphisms (Equation 8). depends on the \nequality: Ev . (.f + \u00b5., .) = .(Ev . (f, .)) + \u00b5(Ev . (., .)) Theorem 14 (Adequacy [4]). Let M be a closed \nterm of type Int. Then, [M] is the sub-probability distribution on Nsuch that 8 .n . N, [M]n = PROBAM,n \n. Remark 15. Adequacy allows one to prove that speci.c prim\u00aditives are not de.nable in the language. \nA noteworthy example is the parallel or function [27]. In our setting, this should be a closed term por \n: Int . Int . Int such that PROBA8 ((por)0)0,0, PRO BA8 and PRO BA8 are equal to 1. By ade\u00ad ((por)1)O,1 \n((por)O)1,1 quacy, [por] (e0)(e0)1 = [por] (e1)(0)0 = [por] (0)(e1)0 = 1. By de.nition of a morphism \nin PCoh, [por] (e0)(e0) must be a subprobability distribution, hence [por] (e0)(e0)0 = 0. That im\u00adplies \n[por]([ ],[ ]),0 = 0. On the other hand, [por] (e1)(e1)0 = [por] (e1)(0)0 +[por] (0)(e1)0 -[por][ ],[ \n],0 = 2, which contra\u00addicts that [por] (e1)(e1) is a subprobability distribution. We con\u00adclude that por \nis not a term of PPCF. However, let us mention that the Gustave function is a valid morphism of PCoh \n(see [13]). Full abstraction extends to higher-order types the perfect match\u00ading syntax / semantics stated \nby adequacy on Int. One direction of full abstraction is indeed a consequence of Theorem 14. Corollary \n16 (Abstraction). Given G f M : A, and G f N : A, we have that [M] = [N] implies M G N. In particular, \n[M] = [N] implies M =G N. Proof. By induction on C[ ], one proves that [M] = [N] implies [C[M]] = [C[N]]. \nThen the result follows from Theorem 14. 5. Full Abstraction We prove equational full abstraction (Theorem \n27), that is the converse of the part of Corollary 16 dealing with equality. This is a straightforward \nconsequence of Lemma 26 stating that for any   G,x:B G G Cur([M] ) ) Pri ([M] , [N] Ev (A . B) &#38; \nA Ai G A . B G B iv G Cur(u . f(iv, u)) ivi iv G G,x:B G G [xi] .xB.M G , with f = [M][(M) N]G , with \nf = [M], . = [N]  T z [M] G succ G pred  T rand [M] Int G Int Int G Int Int G Int . Int 1 1 G (1, \n0, 0, ...) iv (0, w0, w1, ...) iv (w1, w2, ...) iv [rand] iv G GGG [s(M)]G , with w = [M](vv) [p(M)]G \n, with w = [M](vv) [rand][0] G G ' G G ([M ] , [N] , [N ) ifz supn .xn([M] ) ] Int \u00d7 A \u00d7 A Int G G \n A 8 ' supn .xn(w) wnu iv n=1 G G ' ' G G [if(M, N, P )]G , with w = [M](vv), u = [N](vv), u = [N (vv) \n[.x(M)]G , with w = [M](vv) ] Figure 2: The standard semantics of PPCF terms together with its action \non iv . P (G). closed terms M and N having different interpretations in Pcoh, there is a testing term \nP such that (P ) M and (P ) N reduce to 0 with different probabilities. Let us outline the path to this \nresult. With any web element a, we associate a testing term P(a) that is described in Figure 5 and that \nreminds the contexts used in [3]. P(a) is not an ordinary term of PPCF since its construction uses a \nrandom operator, weighted by a list Xiof formal parameters. How\u00adever, a parameterized term becomes an \nordinary PPCF term when i we substitute X by a list i. of rationals in [0, 1]. Following [6], we introduce \nin Figure 3 an intersection type system that de.nes semantics of parameterized terms. This interpretation \nis a formal power series over Xi(De.nition 21). Moreover, parameterized se\u00admantics is compatible with \nPcoh through substitution of parameters (Lemma 20). Now, assume that M and N are interpreted by different \nmatrices and pick a web element a such that [M]a = . Then, the [N]a semantics of the parameterized terms \n(P(a)) M and (P(a)) N are distinct formal power series. Indeed, Lemma 24 implies that they differ at \nleast on one coef.cient. Finally, Lemma 25 ensures the existence of a list i. of rationals in [0, 1] \non which these power series disagree. So, the substitution of Xiby i. in P(a) produces a testing context \nseparating M and N. We .rst describe parameterized PPCF in Subsection 5.1. Then, testing terms are presented \nin Subsection 5.2. We conclude in Subsection 5.3 with the full abstraction theorem. 5.1 Parameterized \nPPCF Let P be a denumerable set of formal parameters. X, Y, Z range over parameters in P. The grammar \nof parameterized PPCF is an extension of PPCF (Figure 1(a)) by multiplication of terms by parameters: \nPPCFP ::= \u00b7 \u00b7 \u00b7 | X \u00b7 M, where X . P. The simple type of X \u00b7 M under a context G is A whenever G f M \n: A in PPCF (Figure 1(b)). The substitution of parameters by scalars let us recover ordinary terms from \nparameterized ones. More precisely, let M . PCFP P and n . [0, 1] be a rational number. We de.ne Mn \n/X as the m m term obtained by replacing in M any subterm of shape X \u00b7 N with , Om-n . choose(N n ) = \nchoose(N, . . . , N, O, . . . , O) (9) n times m-n times (see Figure 1(d) for choose de.nition). Substitution \nis then gener\u00ad r e alized to lists Xiof parameters and i. of rationals as M i./Xi. Fact 17. If Xiis the \nlist of all parameters in M and i. a list of r e rational numbers in [0, 1], then M i./Xiis a term of \nPPCF. The semantics of PPCFP is a re.nement of PPCF semantics taking into account parameters. Yet, for \nthe sake of the full ab\u00adstraction proof, we give a different presentation and use a weighted intersection \ntype system. Roughly speaking, types are web ele\u00adments and with each type derivation p, we associate \na weight .(p) which is a positive monomial, i.e. a product of rationals in [0, 1] and of .nitely many \nparameters. Then, the interpretation [M] of a PPCFP term M is a matrix indexed by web elements. For each \nweb element, there can be several type derivations and the corre\u00adsponding coef.cient is the sum of their \nweights. More precisely, Figure 3 describes the rules for constructing a derivation p :: fa M : a of \nwhat we call a web judgment G G fa M : a. Notice that G f M : A is a valid simple type judgment, a . \n|A| and a is a monomial. Besides, a web context G is de.ned as a function mapping any typed variable \nx C occurring in G to a .nite multiset m . Mf (|C|) of web elements and mapping variables non-appearing \nin G to the empty multiset. For instance, for any G = x1 : C1 . . . xn : Cn and im denotes m . Mf (|G|), \nGmthe web context x Cii . i mi for 1 = i = n. Disjoint unions G l. of web contexts are de.ned pointwise. \nFact 18. If G fa M : a is derivable and G (x) is a non-empty multiset, then x is free in M. Rules app \nand fix deserve some comments. Application of PPCFP terms is interpreted following Equation (8) that \nde.nes composition. Remark that indices of app mainly coincide with the indices of the sums in (8), with \none more dif.culty since we have to split contexts. Now, fix is derived from the rule app and from  \n G A k<n ,x : m fa M : a var  nat rand abs  x A : [a] f1 x : a f1 n : n f 1 rand : ([n], k) n G \nfa .xA.M : (m, a) . G ' fa M : (m, b) .(a, i) . m, G N : a .m . Mf (|A|) (a,i) f\u00df(a,i) '  app(m,(G \n) s.t. Gl G(a,i) = G G fa (a,i))(a,i).m . (M) N : b (a,i).m \u00df(a,i) (a,i).m . m . Mf (|A|) G ' fa M : \n(m, b) .(a, i) . m, G (a,i) f\u00df(a,i) .x(M) : a . '  fix(m,(G ) s.t. Gl G )(a,i).m (a,i) = G(a,i) G fa \n.x(M) : b . (a,i).m \u00df(a,i) (a,i).m G fa M : n +1 G fa M : n pred succ G fa p(M) : n G fa s(M) : n \n+ 1 G f\u00df M : 0 . fa N : a G f\u00df M : n + 1 . fa P : a G fa M : a if0 ifs G l . f\u00dfa if(M, N, P ) : a G \nl . f\u00dfa if(M, N, P ) : a G faX X \u00b7 M : a par  Figure 3: Semantics of parameterized PPCF. 1 .x(M) . \n(M) .x(M). To illustrate this point and the followings, we detail Examples 22 and 23 at the end of this \nsubsection. Fact 19. If G f M : A then .(p) is a power series m M:a p::GP with .nitely many variables \nin P and non negative coef.cients. e Thus, for any list i. of non negative reals, .(p) (i.) m M:a p::GP \nis well de.ned. The following fundamental lemma ensures the soundness of the weighted type system that \ncomputes the semantics of PPCFP . Lemma 20 (Soundness). Let M be a term of PPCFP such that G f M : B \nand Xilists its parameters. For any ip . Mf (|G|), b . |B|, and any list i. of rational numbers in [0, \n1], r erG e [M i./Xi= .(p) (i.) (10) m p,b p::GPp M:b Proof. We prove Equation (10) by structural induction \non M, using rules of Figure 3 and the de.nition of the categorical interpretation of a term in PPCF. \nWe consider the term .x(M) greater than (Mn) y for any natural number n > 0 and variable y. In fact, \nwe are using the induction on the ordinal .2 . Most cases follow directly from induction hypothesis. \nWe only detail three cases: (i) multiplication with a parameter, (ii) applica\u00adtion, (iii) .x-point. n \n(i) Assume M = Xi \u00b7N and .i = m . Then (using Equation (9)) re re e n we get: M i./Xi= choose N i./Xi, \nOm-n . Besides, r r n [O] = 0 (Example 12), so [M i./XierG = [N i./XierG . m rp,bmmp,b e Now, any derivation \nt :: Gpmf M i./Xi: b consists of a sequence of h = n rules ifs with on top, the rule if0 with a r e p \nderivation p :: Gmf N i./Xi: b as right premise. Indeed, the of h . {1, . . . , n}. So, we compute re \nre n .(t ) i./Xi= .(p) i./Xi, m p N:b t::GpP M:b p::GP r erG n [Nwhich is equal to mi./Xiby induction \nhypothesis. m p,b We conclude by gathering the equalities of the two paragraphs. (ii) Assume M = (N) \nP . Thanks to the categorical semantics, e G GG [(N) P ]m= Ev . ([N] , [P ] ) p,b . Then by de.nition \nof p,b m composition (Equation (8)) and left linearity of Ev, [(N) P ] G is m p,b G G . pm\\ mm m.Mf(|A|), \n(a,i).m (ms.t. [N] p(a,i),(m,b) [P ]p(a,i),a p(a,i))(a,i).m (a,i).m pm(a,i).pm By induction hypothesis \nand distributing product over sum, we get .(t ) .(p(a,i)) m.Mf(|A|), t::qm N:(m,b) (p(a,i))(a,i).m (a,i).m \n(pm(a,i))(a,i).m wheres.t. .(a,i).m, qm=pm\\ m p(a,i) s.t. pm(a,i).pmp(a,i)::pm(a,i) P :a Although the \nindices of the sums might be frightening, they pre\u00adcisely describe all possible derivations of Gpmf (N) \nP : b. Namely, the .rst sum de.nes the label of the terminal application rule, while the other sums give \nthe choices of the derivations of their premises. The total weight is then the product of premise weights. \n(iii) Assume M = .x(N). Any derivation p :: Gpmf .x(N) : b ends with a cluster of fix rules (see Example \n23). For n . N*, let .n be the set of derivations whose cluster height is at most n. Now, remark that \na derivation in .n can be transformed into GpmB a derivation t :: , y : [ ] f (Nn) y : b (where y is \nfresh) by replacing fix rules with app rules (keeping labels) and each e occurrence of .x(N) with Nh \ny for the suitable h = n. Besides, this transformation preserves weights. Therefore: subterm O has no \nweb type (see Example 23). When p is .xed, .(p) = .(t) .(t ) = .(p) and there are as many derivations \nt as many choices p..n m t::GpP,yB:[ ] (Nn)y:b  Since [.x(N)]hypothesis: [(Nn , and by induction = P(n) \n= .xInt .if(x = n, 0, OInt ) G,y n.N [(Nn (mn.N p,[ ]),b .(t ), p t::GP,yB:[ ] (Nn)y:b N (n) = n ) y][.x(N)] \nG =.(p) = .(p) = .(p) m p,b P(a) = .zB.C .(P(c)) ((z) choose(Xi \u00b7 N (bi))n i=1) .n p n.Np..n p. p::GP.x(N):b \nn N (a) = .xB .if(.n i=1 (P(bi)) x, N (c), OC ) According to this result, we generalize the semantics \nnotation Figure 5: testing terms. In the higher-order case, i.e. A = B . C,to parameterized PPCF: we \nsuppose a = ([b1, . . . , bn], c), with bi . |B| and c . |C|. Sub-De.nition 21. For any term M in PPCFP \nwith n parameters, testing terms are supposed to have disjoint parameters and the Xi s 1 2 occurring \nin the de.nition of P(a) are assumed to be fresh. G . (11) [M] It is a power series whose domain of convergence \ncontains [0, 1]n = .(p) a p::G M:a typing M with a web element of shape ([ ], a). As a consequence,. \nThe weighted type assignment system of Figure 3 has an interest by its own. It turns computation of semantics \nof terms (and hence its observational behavior) into a proof search problem. The reader can convince \nhimself that this approach is useful by computing the semantics of the following examples by applying \nthe term OA = .x(.xA .x) has no web type, as .xA .x cannot have a web type of shape ([ ], a). We .nd \nagain [OA] = 0. On the other hand, any derivation of .x(.xInt .x . 0) will end with a branch of n > 0 \nfix rules: n - 1 rules labelled by ([0], ( )), and the rule at the top of the branch labelled by ([ ], \n( )) and having as premise the unique derivation of f .xInt .x . 0 : ([ ], 0). The directly rules of \nFigure 2. .x(.xInt whole derivation has conclusion f.x . 0) : 0 and the 1 Example 22. Let k and k ' be \nnon negative integers. Let us consider n 2 sum of these weights for n > 0 yields [.x(.xInt .x . 0)] \nthe possible derivations of the application (M) N with 0 We .nd again the results of Examples 3 and 12. \n= 1. . . ' Int .if(x, OInt , if(x, OInt , 42)), if(y, k, k M = .x N ).= 5.2 Testing terms Figure 5 associates \nwith every web element a . |A| two closed terms of PPCFP: one term P(a) of type A . Int and one term \nThe derivable judgments on the term N are of the form y Int : [n] f1 N : k, with k . {k, k ' }. In fact, \nonce .xed n, the derivation is unique and is given at the left-hand side of Figure 4. As for the term \nM, the derivable judgments are of the form y Int : [ ] f1 M : ([h, h ' ], 42), with h, h ' > 0. However, \nin this case we have two different derivations whenever h = h ' : one derivation is given at the right-hand \nside of Figure 4, while the other one is obtained by swapping the order between the var rules on h and \nh ' . The weight of a derivation of (M) N is 1. However, the number Int ' of derivation of a .x judgment \ny : [n, n ] f1 (M) N : 42 depends on n, n ' , k, k ' following cases: if k = k ' and n = n ' , then \nthere is exactly one derivation;  if k = k ' and n = n ' , then there are two derivations, one ending \nwith the rule app([k,k],{(k,1) .[n],(k,2) .[n1]}) and the other one  with the rule app([k,k],{(k,1) \n.[n1],(k,2) .[n]}); if k = k ' and n = n ' , then there are two possible derivations, depending on which \nis the derivation used for inferring the judgment on M; notice that the .nal rule typing (M) N must ; \nbe app([k,k1],{(k,1) .[n],(k1,1) .[n]}) if k = k ' and n = n ' , then there are four possible derivations, \ndepending on the last rule (i.e. app([k,k1],{(k,1) .[n],(k1,1) .[n1]}) or app([k,k1],{(k,1) .[n1],(k1,1) \n.[n]})) and on the derivation for inferring M. Actually, by Lemma 20, weights of possible derivations \nsums into y, i.e. [(M) N] . .. . . . . .. [(M) N]y = m,h 4 if h = 42, k = k ' , m = [n = n ' ], N (a) \nof type A, de.ned by mutual induction on A. P(a) and N (a) are named testing terms. The parameters oc\u00adcurring \nin this terms are in a bijective correspondence with the ele\u00adments of the multisets appearing in a (at \nall depths). In the following, we focus on [P(a)](m,0) and [N (a)]a1 , for any m . Mf (|A|) and a ' . \nA, which are formal series in the parameters occurring in the terms (Fact 19). More precisely, we are \ninterested in the coef.cient of the monomial having each parameter of the term occurring with degree \n1. This monomial is called the skeleton of a and is denoted by sk(a). Lemma 24. Let A be a type, a, a \n' . |A| and m . Mf (|A|). In the series [P(a)](m,0) (resp. [N (a)]a1 ), the monomial sk(a) has a non \nzero coef.cient if and only if m = [a] (resp. a ' = a). Proof. By de.nition, the coef.cient of the monomial \nsk(a) in can be recovered from the sum of the weights of the [P(a)](m,0) derivations of shape P :: f.sk(a) \nP(a) : (m, 0), and similarly for [N (a)]a1 . Hence, the statement is equivalent to the following property \nIH(A), which will be proved by induction on type A: * For any a . |A|, there is a derivation of f.sk(a) \nP(a) : (m, 0), with . = 0, if and only if m = [a] * For any a . |A|, there is a derivation of f.sk(a) \nN (a) : a ' , 1 if h = 42, k = k ' , m = [n, n], with . = 0, if and only if a ' = a. 2 if h = 42, k = \nk ' , m = [n = n ' ], For the ground type Int, testing terms have no parameters, so or h = 42, k = k \n' , m = [n, n], that the monomial sk(n) is the constant term of the series. The derivations of P(n) and \nN (n) are unique and of shape: 0 otherwise. m = [n] Example 23. Any derivation of a .x-point term .x(M) \nends with a cluster of fix rules, each rule has one premise typing M and x : m f1 x : n f1 0 : 0 a ' \n= n a number of premises typing .x(M) and along which the cluster ' x : m f1 if(x = n, 0, O.) : 0 f1 \nn : a grows. Since the derivation must be .nite, the cluster eventually ends with fix rules of label \n([ ], ( )). They have exactly one premise f1 .x. .if(x = n, 0, O.) : (m, 0)  Int Int Int Int y : [ ], \nx : [h ' ] f1 x : h ' y : [ ], x : [ ] f1 42 : 42 Int Int Int Int Int Int y : [n] f1 y : n y : [ ] f1 \nk : k y : [ ], x : [h] f1 x : h y : [ ], x : [h ' ] f1 if(x, OInt , 42) : 42 Int Int Int y : [n] f1 N \n: k y : [ ], x : [h, h ' ] f1 if(x, OInt , if(x, OInt , 42)) : 42 y Int : [ ] f1 M : ([h, h ' ], 42) \n Figure 4: Weighted derivations discussed in the Example 22 IH(B): bij = bj ' f\u00dfij N (bij ) : bj ' @ \n m = [(p ' , c)] C ' fXij \u00dfij Xij \u00b7 N (bij ) : b ' j p = [b ' 1, . . . , b ' k1 ] \u00ae \u00ae z : m f1 z : (p \n' , c) .j . {1, . . . , k ' }, f 1 choose(Xi \u00b7 N (bi))k i=1 : b ' j IH(C): q = [c] k Xij \u00dfij \u00ae \u00ae f. P(c) \n: (q, 0) z : m f\u00df (z) choose(Xi \u00b7 N (bi))k i=1 : c z : m f.(P ) (P(c)) (z) choose(Xi \u00b7 N (bi))k i=1 : \n0 C f.(P ) .zB.C .(P(c)) (z) choose(Xi \u00b7 N (bi))k i=1 : (m, 0) Figure 6: Proof derivation contributing \nin the sk(a) monomial in P(a). We conclude that the coef.cient is non zero iff m = [n] and a ' = n. For \nthe inductive case, let us assume that the result holds for type B and C and that a = ([b1, . . . , bk], \nc) . |B . C|. No\u00adtice that sk(a) = sk(c) k i=1 Xisk(bi). A derivation of f.sk(a) P(a) : (m, 0), with \n. = 0, must be as described in Figure 6, as justi.ed below: C The only possible rule is the abstraction \none. .,\u00ae For using the application rule, we have to choose an interme\u00addiate multiset q . Mf (|B . C|) \nand a context G ' such that G ' f. P(c) : (q, 0) is derivable. Since P(c) is a closed term, by Fact 18, \nG ' must be empty (i.e. mapping all variables to the empty multiset). Moreover, by de.nition, the parameters \noccur\u00adring in P(c) must be different from those occurring in each Xi \u00b7 N (bi). Therefore, . = .sk(c), \nwith . = 0. By induction hypothesis in \u00ae, we infer that q = [c]. We then deduce that . is a binary applicative \nrule of label app([c],(z:m)). Hence, the right premise is the sequent z : m f\u00df (z) choose(Xi \u00b7 N (bi))k \ni=1 : c, for a \u00df of the shape \u00b5 k \u00b7 sk(bi). i=1 Xi \u00ae,\u00ae The shape of the term forces us to use the application \nrule again: let p ' = [b ' 1, . . . , b k' 1 ] denote the intermediate multiset. Since the left premise \nis the only one in which z appears, there is only one possible way of separating the context: G ' = z \n: m and G b1 = \u00d8. Since the only possible rule for \u00ae is the var rule, j we get m = [(p ' , c)]. We conclude \nthat \u00ae is an applicative rule with k ' + 1 premises, the k ' right premises being of shape f\u00dfj choose(Xi \n\u00b7 N (bi))k i=1 : bj ' . \u00ae,C,@ For any j . {1 . . . k ' }, there is a cluster of h < k rules ifs and \nat the top of it a rule if0 with, as right premise, a weighted typing of a summand Xij \u00b7 N (bij ). From \nthat we must apply a par rule (numbered C) and get as premise a sequent of shape f\u00dfij N (bij ) : b ' \nj , for ij . 1 . . . k and weight \u00dfij . Then, notice that the weight of the conclusion of C is Xij \u00dfij \nand that of the conclusion of \u00ae is 1 . On the one hand, the reasoning on k Xij \u00dfij . gives us \u00df = \u00b5 ik \n=1 Xi \u00b7 sk(bi), while the de.nition of the 1 weight of the conclusion of \u00ae gives us \u00df = kj=1 1 k Xij \n\u00b7 \u00dfij . Since by de.nition no parameter Xi occurs in N (bij ), hence no Xi appears in \u00dfij , we have that \nthe mapping j . ij is a bijective correspondence between {1, . . . , k ' } and {1, . . . , k}, so k = \nk '. Finally, since the sets of parameters occurring in the N (bij ) are pairwise disjoint, we deduce \nthat \u00dfij is proportional to sk(bij ). We can then apply the induction hypothesis on @ and deduce that \nbij = bi, and so p ' = [b1, . . . , bk]. Combining all these results, we eventually get that m = [(p \n' , c)] = [([b1, . . . , bk], c)] = [a]. We conclude that there is a derivation of f.sk(a) P(a) : (m, \n0) if and only if m = [a]. Now, let us consider a derivation of f.sk(a) N (a) : a ', which must have \nthe following form: . . . x : p ' fa if(.k i=1 (P(bi)) x, N (c), OC ) : c ' fa .xB .if(.ik =1 (P(bi)) \nx, N (c), OC ) : (p ' , c ' ) We prove by recursion on k - j the property RH(j): * if x : p ' fa if(.ik \n=j (P(bi)) x, N (c), OC ) : c ' is derivable, then a is proportional to sk(c) k i=j sk(bi) if and only \nif c = c ' and p ' = [bj , . . . , bk]. The base case (j = k) is the induction hypothesis IH(C). '' ' \nIndeed, if x : p fa N (c) : c is derivable, then, by Fact 18, p is empty, so that the weight a is proportional \nto sk(c) if and only if c = c '. The inductive case is proved accordingly to Figure 7: C Since OC has \nno web type (Example 23), the only possible rule is if0. The choice of how to partition the context m1 \nl m2 will be deduced from the type derivation. Since by de.nition the set of parameters in P(bj ) and \nthe set of parameters in if(.k i=j+1 (P(bi)) x, N (c), OC ) are disjoint, we have that \u00df is proportional \nto sk(bj ) and . is proportional to k sk(bi). i=j+1 . For the right premise we can then apply the hypothesis \nRH(j+1) and deduce that c = c ' and m2 = [bj+1, . . . , bk]. \u00ae,\u00ae For the left premise, the only possible \nrule is app. We have then to choose the intermediate multiset m '. As in the previous cases, one can \nargue that the left premise has empty context  IH(A): m1 = [bj ] f\u00df P(bj ) : (m1, 0) \u00ae RH(j + 1): c \n= c ' , m2 = [bj+1, . . . , bk]  x : m1 f1 x : m1  ' x : m1 f\u00df (P(bj )) x : 0 \u00ae x : m2 f. if(.k i=j+1 \n(P(bi)) x, N (c), OC ) : c C x : m1 l m2 f\u00df. if(.k i=j (P(bi)) x, N (c), OC ) : c ' Figure 7: Proof \nderivation contributing in the sk(a) monomial in N (a). since P(bj ) is closed and then the induction \nhypothesis IH(C) gives m ' = [bj ]. This means that \u00ae has only one premise at the right-hand side, which \nis a conclusion of a var rule, so that m ' = m1. We conclude by combining all these results: we get that \nthe context m1 l m2 = [bj , . . . , bk] and c = c ' . We have proved that RH(1) holds: c = c ' and p \n' = [b1, . . . , bk], hence a ' = a. 5.3 Main Result The theory of multi-variables analytic functions \ncan be subtler than single variable ones (for instance zeros of functions are not isolated in general). \nIn the following Lemma 25, we underline that the coef.cients of a power series vanishing on a neighborhood \nof zero have to be null. Indeed, coef.cients are computed by successive derivations as limits of difference \nquotients. We then apply this result to power series with possibly negative coef.cients. Lemma 25. [16] \nLet f be a power series from Rn to Rabsolutely converging on [0, 1]n. If f vanishes on a dense subset \nof [0, 1]n , then the coef.cients of the power series f are zero. Lemma 26. Let M and N be two closed \nterms of PPCF with the same type A. If [M] = [N], then there is a PPCF term P of type A . Int such that \nPRO BA8 = PRO BA8 (P )M,0 (P )N,0. Proof. Let a . |A| such that [M]a = [N]a. Consider the testing term \nP(a) as de.ned in Figure 5. Let f denote the series [(P(a)) M]0, which, by De.nition 21 is a power series \nfrom [0, 1]n to [0, 1], where n is the number of parameters in P(a). Let cf(sk(a), f ) denote the coef.cient \nof the monomial sk(a) in f. By de.nition (Equation (11)) this coef.cient is given by the sum of the weights \nof the possible derivations of fa (P(a)) M : 0, for a proportional to sk(a). The last rule of this derivation \nmust be an app rule of shape: f. P(a) : (m, 0) .(a ' , i) . m, f\u00dfM : a ' (a1,i) f. \u00df(P(a)) M : 0 (a1,i).m \n(a1,i) Since M has no free parameter (it is a term of PPCF), each \u00df(a1,i) is a positive real number. \nHence, a = . (a1,i).m \u00df(a1,i) is proportional to sk(a) iff . is proportional to sk(a). We can then apply \nLemma 24 and get m = [a]. Moreover, by Lemma 20, the sum of the weights of the derivations of f M : a \nis equal to the scalar [M]a. To sum up, we conclude: cf(sk(a), f ) = cf(sk(a), [P(a)]([a],0)) [M]a with \ncf(sk(a), [P(a)]([a],0)) = 0. By an analogous reasoning: cf(sk(a), g) = cf(sk(a), [P(a)]([a],0)) [N]a \nwhere g denotes the power series [(P(a)) N]0. We can conclude that f and g have a different coef.cient \nfor the monomial sk(a) as soon as [M]a = [N]a . Now, we apply Lemma 25 to the series f - g, which is \nnot the zero power series because of the coef.cient of sk(a). Therefore, as rational numbers are dense \nin [0, 1], there is a list i. of rational numbers in [0, 1] such that f (i.) = g(i.). By Lemma 20, this \nis re re Mr Nr equivalent to [ P(a) i./Xi= [ P(a) i./Xi. r0 e 0 We conclude by setting P = P(a) i./Xi. \nActually, P is a well\u00ad de.ned PPCF term by Fact 17. So, by adequacy (Theorem 14), PRO BA8 = PROBA8 (P \n)M,0 = [(P ) M]0 = [(P ) N]0 (P )N,0 . Theorem 27 (Full Abstraction). Given G f M : A, and G f N : A, \nwe have: M =G N iff [M] G = [N] G . Proof. By Corollary 16 and Lemma 26. As a consequence, the observational \nequivalence does not de\u00adpend on the chosen probabilistic primitive (recall Remark 6): Corollary 28. For \nany p . {Coin, rand , .., .i.i, (.n)n.N}, let =p denote the observational equivalence induced by the \nexten\u00adsion of PCF with the probabilistic primitive p. The equivalence =p coincides with the equality \nof the interpretations in PCoh. In particular, =p gives the same equivalence on PCF terms, for any choice \nof p. Proof (Sketch). We have that =Coin = =rand since the two primi\u00adtives Coin and rand are interde.nable \nby [5]. Similarly, by the discussion of Remark 6, we have: =rand . =.. . =.i.i . =(.n)n.N . In [4], the \nadequacy property of PCoh is es\u00adtablished for the language containing the the (.n)n.N primitive, hence \n=(.n)n.N contains the equality =Pcoh of the interpretations in PCoh. Finally, by Theorem 27, we have \n=Pcoh . =rand and we can conclude. 6. A counter-example to inequational FA We prove that inequational \nfull abstraction fails for PCoh. The proof consists in: (i) showing that the morphisms of Remark 11, \nthat proves the non-extensionality of the PCoh order, are de.nable in PPCF (Equation 12); (ii) achieving \na context lemma (Proposi\u00ad tion 31) allowing us to infer the observational inequality by just observing \nthe behavior of closed terms in applicative contexts. If one would like to enrich the language in order \nto make observable also the componentwise order of PCoh, one should add a kind of differential operator \nto PPCF, in the spirit of [9]. However, such an extension is not trivial, since PCoh is known not to \nbe sound for the whole differential .-calculus. Let us de.ne M1 and M2 as follows: . .xInt . .xInt M1 \n= .if(x, 0, 0), M2 = .0. (12) By using rules of Figure 3 and Lemma 20, one can check that the semantics \nof the two terms is given by the morphisms of Exam\u00adple 11, i.e. [M1] = f and [M2] = .. Hence, the two \nmatrices [M1] and [M2] are incomparable in Pcoh.  In order to prove M1 M2, we use a standard reasoning \nand introduce a logical relation (De.nition 29) allowing us to shrink the set of contexts (Proposition \n31). We conclude with Corollary 32. De.nition 29. By structural induction on a type A, we de.ne the binary \nrelation <A over closed PPCF terms of type A, as follows:N <Int N ' iff .n . N, [N]n = [N ' ]n, and M \n<A.B M ' ' '' iff .N <A N , (M) N <B (M ) N . Lemma 30. Let M be a term of type x1 : A1, . . . , xn : \nAn f M : re re ' i A. If for all i, Ni <Ai Ni , then M N /ix <A M Ni' /ix . Proposition 31. Let M and \nN be closed terms of type A. Then, M <A N iff M A N. Corollary 32. The terms M1 and M2 of Equation (12) \nare logi\u00adcally related: M1 <Int.Int M2. Hence, M1 M2. 7. Conclusion We show that the observational equality \nover probabilistic pro\u00adgrams is faithfully described by PCoh, a relatively abstract model based on convex \nsets and extensional functions. The proof uses in\u00adnovative tools which might be useful to study probabilistic \npro\u00adgraming from a semantical viewpoint. In a language with random functions, two programs should be \nconsidered different not only when they give different results, but also when they give the same result \nbut with different probabilities. Showing this difference can be much harder than in a deterministic \nlanguage. Indeed, it requires a sharp control over coef.cients expressing probabilities. PCoh de\u00adnotes \nprograms with power series, this allows us to use standard tools of Calculus for handling probabilities. \nAlthough we chose one probabilistic primitive (see Remark 6), Corollary 28 shows that our result does \nnot depend on this choice. Moreover, we focused on PPCF, a call-by-name functional lan\u00adguage, but our \nresult might be extended to other frameworks. In fact, PCoh comes from a model of linear logic: it is \nthe cok\u00adleisli category associated with the exponential comonad, that cor\u00adresponds to the translation \nof the functional arrow A . B into the linear logic formula !A -B. Call-by-value can be obtained by using \nthe Eilenberg-Moore construction, i.e. translating A . B into !(A -B). Control operators can be introduced \nconsidering a polarized fragment of linear logic. Yet, extending this model to concurrent systems or \nreferences is certainly more challenging. A crucial tool in the proof of full abstraction is the use \nof the type system of Figure 3, which is a kind of intersection type system. In fact, a web element ([a1, \n. . . , an], b) can be seen as a type (a1 . \u00b7 \u00b7 \u00b7.an) . b, where the intersection is non-idempotent. \nThis system is a quantitative re.nement of De Carvalho s [6] and yields the .rst logical presentation \nof a vectorial based semantics. Clearly, both type inference and type checking are undecidable. However, \none can look for interesting restrictions of PPCF where the system becomes decidable, in the spirit of \n[20]. Last, it should be noticed that, unlike most full abstraction mod\u00adels for PCF, our model has no \nsimple PPCF-de.nability properties. Our full abstraction proof builds applicative contexts using terms \nwhich belong to a very small subset of the domains associated with types. Their discriminating power \nrelies on a strong regularity prop\u00aderty of power series (unlike smooth functions, a power series which \nvanishes on an open set must be equal to 0, see for example to the -1/x2 function de.ned by e for x > \n0 and 0 for x = 0). In contrast, most full abstraction proofs build applicative contexts using terms \nwhich belong to a dense subset of the corresponding domains. References [1] S. Abramsky, R. Jagadeesan, \nand P. Malacaria. Full abstraction for PCF. Information and Computation, 163(2):409 470, Dec. 2000. [2] \nG. Boudol, P.-L. Curien, and C. Lavatelli. A semantics for lambda calculi with resources. Math. Struct. \nComp. Sci., 9(4):437 482, 1999. [3] A. Bucciarelli, A. Carraro, T. Ehrhard, G. Manzonetto, et al. Full \nabstraction for resource calculus with tests. In CSL11 Annual Conference of the EACSL, volume 12, pages \n97 111, 2011. [4] V. Danos and T. Ehrhard. Probabilistic coherence spaces as a model of higher-order \nprobabilistic computation. Inf. Comput., 209(6):966 991, 2011. [5] V. Danos and R. Harmer. Probabilistic \ngame semantics. ACM Trans\u00adactions on Computational Logic, 3(3):359 382, July 2002. [6] D. de Carvalho. \nExecution Time of .-Terms via Denotational Seman\u00adtics and Intersection Types. Preprint, 2009. [7] T. \nEhrhard. On K \u00a8 othe sequence spaces and linear logic. Math. Struct. Comput. Sci., 12:579 623, 2002. \n[8] T. Ehrhard. Finiteness spaces. Math. Struct. Comput. Sci., 15(4):615 646, 2005. [9] T. Ehrhard and \nL. Regnier. The Differential Lambda-Calculus. Theor. Comput. Sci., 309(1):1 41, 2003. [10] J.-Y. Girard. \nThe system F of variable types, .fteen years later. Theor. Comput. Sci., 45:159 192, 1986. [11] J.-Y. \nGirard. Linear logic. Theor. Comput. Sci., 50:1 102, 1987. [12] J.-Y. Girard. Normal functors, power \nseries and lambda-calculus. Ann. Pure Appl. Logic, 37(2):129 177, 1988. [13] J.-Y. Girard. Coherent banach \nspaces: a continuous denotational se\u00admantics. Theor. Comput. Sci., 227:297, 1999. [14] J.-Y. Girard. \nBetween logic and quantic: a tract. In T. Ehrhard, J.- Y. Girard, P. Ruet, and P. Scott, editors, Linear \nLogic in Computer Science, volume 316 of London Math. Soc. Lect. Notes Ser. CUP, 2004. [15] J. Goubault-Larrecq \nand D. Varacca. Continuous random variables. In LICS, pages 97 106. IEEE Computer Society, 2011. \u00b41918. \n[16] E. Goursat. Cours d analyse math\u00b4ematique. Tome I. Gauthier-Villars, [17] R. Hasegawa. Two applications \nof analytic functors. Theor. Comput. Sci., 272(1-2):113 175, 2002. [18] M. Hyland and L. Ong. On full \nabstraction for PCF. Information and Computation, 163(2):285 408, Dec. 2000. [19] C. Jones and G. Plotkin. \nA probabilistic powerdomains of evaluation. In LICS. IEEE Computer Society, 1989. [20] A. J. Kfoury and \nJ. B. Wells. Principality and decidable type inference for .nite-rank intersection types. In POPL, pages \n161 174, 1999. [21] U. D. Lago and M. Zorzi. Probabilistic operational semantics for the lambda calculus. \nRAIRO -Theor. Inf. and Applic., 46(3):413 450, 2012. [22] J. Laird, G. Manzonetto, G. McCusker, and M. \nPagani. Weighted relational models of typed lambda-calculi. In LICS 2013. IEEE Press, June 2013. [23] \nR. Milner. Fully abstract models of typed lambda-calculi. Theor. Comput. Sci., 4:1 22, 1977. [24] R. \nMilner and C. Strachey. A Theory of Programming Language Semantics. Chapman and Hall, London, 1976. [25] \nE. Moggi. Computational lambda-calculus and monads. In LICS, pages 14 23. IEEE Computer Society, 1989. \n[26] A. D. Pierro, C. Hankin, and H. Wiklicky. Probabilistic lambda\u00adcalculus and quantitative program \nanalysis. J. Log. Comput., 15(2): 159 179, 2005. [27] G. D. Plotkin. A powerdomain construction. SIAM \nJ. Comput., 5(3): 452 487, 1976. [28] G. D. Plotkin. LCF considered as a programming language. Theor. \nComput. Sci., 5(3):225 255, 1977. [29] N. Saheb-Djahromi. Cpo s of measures for nondeterminism. Theor. \nComput. Sci., 12:19 37, 1980. [30] D. Scott. Continuous lattices. In Lawvere, editor, Toposes, Algebraic \nGeometry and Logic, volume 274 of Lecture Notes in Math., pages 97 136. Springer, 1972.  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Probabilistic coherence spaces (PCoh) yield a semantics of higher-order probabilistic computation, interpreting types as convex sets and programs as power series. We prove that the equality of interpretations in Pcoh characterizes the operational indistinguishability of programs in PCF with a random primitive.</p> <p>This is the first result of full abstraction for a semantics of probabilistic PCF. The key ingredient relies on the regularity of power series.</p> <p>Along the way to the theorem, we design a weighted intersection type assignment system giving a logical presentation of PCoh.</p>", "authors": [{"name": "Thomas Ehrhard", "author_profile_id": "81100407449", "affiliation": "Laboratoire PPS - CNRS - Universit&#233; Paris Diderot, Paris, France", "person_id": "P4383830", "email_address": "thomas.ehrhard@pps.univ-paris-diderot.fr", "orcid_id": ""}, {"name": "Christine Tasson", "author_profile_id": "81438597543", "affiliation": "Laboratoire PPS - CNRS - Universit&#233; Paris Diderot, Paris, France", "person_id": "P4383831", "email_address": "christine.tasson@pps.univ-paris-diderot.fr", "orcid_id": ""}, {"name": "Michele Pagani", "author_profile_id": "81447594194", "affiliation": "Laboratoire LIPN - CNRS - Universit&#233; Paris 13, Villetaneuse, France", "person_id": "P4383832", "email_address": "michele.pagani@lipn.univ-paris13.fr", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535865", "year": "2014", "article_id": "2535865", "conference": "POPL", "title": "Probabilistic coherence spaces are fully abstract for probabilistic PCF", "url": "http://dl.acm.org/citation.cfm?id=2535865"}