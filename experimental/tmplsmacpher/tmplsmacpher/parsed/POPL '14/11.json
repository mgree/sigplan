{"article_publication_date": "01-08-2014", "fulltext": "\n Bias-Variance Tradeoffs in Program Analysis Rahul Sharma Aditya V. Nori Alex Aiken Stanford University \nMicrosoft Research Stanford University sharmar@cs.stanford.edu adityan@microsoft.com aiken@cs.stanford.edu \n Abstract It is often the case that increasing the precision of a program anal\u00adysis leads to worse results. \nIt is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise \nabstract domains as the basis for inferring strong invariants of programs. We show that bias-variance \ntradeoffs, an idea from learning theory, can be used to explain why more precise abstractions do not \nnecessarily lead to better results and also provides practical techniques for cop\u00ading with such limitations. \nLearning theory captures precision using a combinatorial quantity called the VC dimension. We compute \nthe VC dimension for different abstractions and report on its useful\u00adness as a precision metric for program \nanalyses. We evaluate cross validation, a technique for addressing bias-variance tradeoffs, on an industrial \nstrength program veri.cation tool called YO GI. The tool produced using cross validation has signi.cantly \nbetter run\u00adning time, .nds new defects, and has fewer time-outs than the cur\u00adrent production version. \nFinally, we make some recommendations for tackling bias-variance tradeoffs in program analysis. Categories \nand Subject Descriptors D.2.4 [Program Veri.ca\u00adtion]: Statistical methods; F.3.2 [Semantics of Programming \nLan\u00adguages]: Program analysis; I.2.6 [Learning]: Parameter learning Keywords Program Analysis; Machine \nLearning; Veri.cation 1. Introduction In program analysis, it is well understood that imprecise abstrac\u00adtions \ncan lead to inferior results. However, what is not so well un\u00adderstood is that precise abstractions can \nalso produce inferior re\u00adsults, in some cases even worse than very imprecise abstractions. We show that \nbias-variance tradeoffs, an idea from learning theory, can be used to explain how the quality of analysis \nresults changes with precision. Learning theory quanti.es precision using a combi\u00adnatorial quantity called \nVapnik-Chervonenkis dimension or VC di\u00admension; we use VC dimension to analyze the behavior of a number \nof commonly used program analysis abstractions. In addition, using cross validation, a technique routinely \napplied in machine learning to address bias-variance tradeoffs, we are able to improve the per\u00adformance \nof YO GI, an industrial strength program veri.cation tool. How can better precision adversely affect \nan analysis? Note that we are not talking about cost or ef.ciency the question only con- Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA. Copyright c &#38;#169; \n2014 ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535853 cerns the quality \nof the results. The class of static analysis sys\u00adtems to which our results apply have two distinguishing \ncharacter\u00adistics. First, there is some domain of facts (e.g., an abstract do\u00admain) over which the analysis \ncomputes. Second, there is some step in the analysis that takes such facts and attempts to generalize \nthem (e.g., to an invariant of the program). Many analysis frame\u00adworks such as abstract interpretation \n[17], counter-example guided abstraction re.nement (CEGAR) [13], and various other inference techniques \n[10, 28, 49] have this structure. For example, abstract interpretation can use widening (Section 4.1) \nand CEGAR can use interpolants (Section 4.2) as a generalization step. There are also static analyses \nthat do not have this structure and our results do not apply to them (see Section 6). Using results on \nbias-variance tradeoffs we show that increasing the precision of the underlying domain, at some point, \nmay lead to worse results from the general\u00adization step. Consider the program in Figure 1, which we analyze \nusing different program analyses with increasing precision. Each pro\u00adgram analysis is a basic abstract \ninterpreter over a different ab\u00adstract domain. First, consider an interval analysis [16], which in\u00adfers \nupper and lower bounds for numeric variables. Using intervals to analyze the program in Figure 1, we \nobtain the loop invariant 1 = i = 5 . j = 0 [36]. This result is perhaps the best invari\u00adant that one \ncould expect with intervals. Intervals can only express facts about single variables, and hence the invariant \nhas no details on any relationship between i and j. The problem is lack of preci\u00adsion: the abstract domain \nis not precise enough to express certain behaviors. We increase the precision to octagons [39], which \ncan infer bounds on the sum or difference of pairs of variables. We ob\u00adtain the following loop invariant \n[36]: 1 = i = 5 . i - j = 1 . i + j = 1 . j = 0 This result is quite good, providing a useful relationship \nbetween i and j and good bounds. Next, we use the even more precise domain of polyhedra [19] which can \nexpress bounds on arbitrary linear combinations of program variables. We obtain the weak invariant i \n= 5 [36]. This result is not only worse than octagons, it is even worse than intervals. Increasing the \nprecision of the analysis results in a decrease in the quality of the results. Let us examine this outcome \nin some detail. The abstract in\u00adterpreter generates some abstract states and then tries to generalize \nvia joins and widening [17]. However, because polyhedra are so expressive, there are many incomparable \npolyhedra that perfectly describe any .nite set of abstract states. It is dif.cult for the gen\u00aderalization \nstep to pick the best one for this particular program from such a large candidate pool. For this example, \nwe start with i = 1 . j = 0 as the initial abstract state and we show the .xpoint iterations [17] published \nin [41]. First, we .t on i = 1, j = 0 and i = 2, j = 1 to obtain the polyhedra -i + j = -1 . i = 1. Next, \nwe obtain i + j = -1 . 7i - 4j = 7 that .ts i = 1, j = 0, i = 2, j = 1, and i = 5, j = 7. Note that there \nare an unbounded number of polyhedra, including the one discovered by octagons,  1: int i = 1, j = 0; \n2: while (i<=5) { 3: j = j+i ; 4: i = i+1; 5: } Figure 1. Example program from [41]. that can .t these \nstates. The domain of polyhedra is so precise that the generalization step has many valid choices and \nhence it can pick a hypothesis that .ts these speci.c abstract states but does not hold in general. In \nthe next iteration, we obtain -i + j = -1, and, .\u00adnally, true. Using the loop guard and narrowing [17], \nwe terminate with i = 5 as the invariant. One might be tempted to blame speci.c choices made in this \nanalysis run for the result, but that misses the point that the phenomenon is general: any method that \nattempts to select the best generalization from a large set of equally viable but different candidates \nwill run into the same problem on many pro\u00adgrams. For example, other analyses such as [3] meet a similar \nfate on this example [41]. Very abstractly, a program has some behaviors, and if an anal\u00adysis is not \nexpressive enough to capture these behaviors, then we have under.tting and the results are poor. If the \nanalysis engine is too expressive, then it can over.t the speci.c behaviors and fail to generalize. Currently, \nprogram analysis designers apply folk knowledge to avoid under.tting and over.tting. Our aim is to give \na formal framework to understand these rules of thumb and use the foundations to obtain better tools. \nIn learning theory [34], under.tting is characterized by bias and over.tting is characterized by variance \n[21], and by varying the precision, one gets a bias-variance tradeoff. Low precision leads to high bias \nand low variance, while high precision leads to low bias but high variance. With an appropriate choice \nof precision, one can balance bias and variance and obtain good results. As an example, consider Figure \n2. YO G I is a veri.cation engine in Microsoft Windows SDV (Static Driver Veri.er) toolkit that checks \nsafety properties of Windows device drivers [25]. In Figure 2, increasing values on the x-axis indicate \nincreasing precision (for details see [43] and Section 5). The y-axis shows the time taken by YOGI on \n2490 veri.cation tasks (a superset of the tasks reported in [25]). Higher analysis times are indicative \nof more time-outs and poorer quality of results. One observes the bias-variance tradeoff in Figure 2. \nAt low precision the performance of the tool is poor. With increasing precision, the bias decreases and \nthe performance of the tool improves. However, after a certain point, the variance starts increasing \nand the performance starts to degrade. To develop a theory that can explain these empirical observa\u00adtions, \nwe need a formal de.nition of generalization. Unfortunately, even though the term generalization frequently \noccurs in the pro\u00adgram analysis literature, de.ning generalization precisely is dif.\u00adcult and there is \nno widely accepted de.nition. Just as complex\u00adity theory works with a model of machines and generates \nquali\u00adtative (as opposed to quantitative) results for comparing the ef.\u00adciency of algorithms, we want \na useful theory, perhaps working with a model, that generates useful qualitative feedback about bias\u00advariance \ntradeoffs. In a recent work, [47] applied the de.nition of generalization given by the PAC (probably \napproximately correct) learning frame\u00adwork [48] to prove that a veri.cation algorithm for checking safety \nproperties generalizes. According to this de.nition (which we give formally in Section 2), an algorithm \ngeneralizes if given enough samples of program states as input, it is likely to generate predicates that \nseparate almost all the program s safe reachable states from er\u00adroneous program states. In this paper, \nwe explore an alternative use of this framework, namely modeling bias-variance tradeoffs. We Figure 2. \nResult of running YO G I on 2490 driver-properties pairs. The best performance is achieved at i = 500. \n do not claim that this de.nition is the correct de.nition of general\u00adization. However, the motivation \nfor our work is largely practical, and we show that this existing framework yields some immediately useful \nresults. In the future, different de.nitions of generalization might be available and the framework developed \nhere can be instan\u00adtiated with the alternative de.nitions to derive other useful conclu\u00adsions for the \nbene.t of program analysis tools. We believe that if program analysis tool designers are explicitly made \naware of bias-variance tradeoffs and have mathematical tools to qualitatively reason about generalization, \nthey can make better informed design decisions. In our framework, variance is propor\u00adtional to VC dimension \n(Theorem 2.2). Hence, a high VC dimen\u00adsion is indicative of over.tting. We calculate the VC dimension \nof several abstractions used in program analysis (Section 3), including abstractions for numerical programs, \narray manipulating programs, and heap manipulating programs. We observe that more precise ab\u00adstractions, \nthat is, the abstractions with higher variance, have higher VC dimension. These proofs increase our con.dence \nin the applica\u00adbility of our framework to program analyses and provide evidence that VC dimension is \na useful qualitative metric for characterizing the precision of abstractions. Since our de.nition of \ngeneralization has been borrowed from learning theory, we can build on well-known techniques in the ma\u00adchine \nlearning community for addressing bias-variance tradeoffs. Cross validation [2] is one of the simplest \ntechniques for this pur\u00adpose. Consider a fully automatic analysis tool that has a number of con.guration \nparameters. How to set these parameters optimally is usually unclear, and the typical case is that such \nan optimal con.g\u00aduration might not even exist, although clearly some con.gurations are better than others. \nOne logical candidate con.guration is the one that performs best on a benchmark suite. However, we demon\u00adstrate \nthat this strategy can over.t on the particular benchmark suite and signi.cantly degrade the quality \nof the tool on new inputs (Sec\u00adtion 5). Cross validation avoids over.tting, and we show that by applying \ncross validation to tune the con.guration parameters of YO G I [25], we are able to signi.cantly improve \nYOGI s running time while also reducing the number of timeouts and .nding new defects. Thus, over.tting \ncan adversely impact program analysis in at least two ways. First, overly precise abstract domains can \nover.t and lead to poor generalization and hence inferior invariants. Sec\u00adond, tools can over.t their \nbenchmark suite resulting in poor per\u00adformance on new analysis tasks. We show that these seemingly dif\u00adferent \nproblems are both instances of bias-variance tradeoffs and hence the same principles (Theorem 2.2) apply \nto both. Learning  (a) (b) (c) Figure 3. Regression example: under.t (a), good .t (b), over.t (c). \ntheory addresses bias-variance tradeoffs in general and can provide techniques to tackle both these problems. \nTo summarize, our contributions are as follows: We observe that improving precision does not necessarily \nlead to better results in program analysis. We connect this phe\u00adnomenon to bias-variance tradeoffs in \nlearning theory (Sec\u00adtion 2).  We show that VC dimension captures the precision of abstract domains \nused in program analysis. We calculate the VC dimen\u00adsion of a number of abstractions relevant to program \nanalysis, including formulas over arrays and separation logic (Section 3).  We explain several empirical \nobservations in program analysis using bias-variance tradeoffs. For example, by incrementally increasing \nprecision, one can balance bias and variance (Sec\u00adtion 2), a strategy that is employed in several existing \nprogram analyses [32, 41, 49] (Section 4).  Using bias-variance tradeoffs we show how over.tting to \nbenchmark suites can result in unnecessarily poor performance on new inputs. Using cross validation to \nguide precision tuning, we are able to improve the performance of YO G I, a fully auto\u00admatic, production \nquality program veri.cation tool (Section 5).  We make some speci.c recommendations for tackling bias\u00advariance \ntradeoffs in program analysis (Section 6).  2. Preliminaries We .rst review concepts from learning theory \nused in subsequent sections; readers already familiar with learning theory may safely skip this section. \nFor more details, the reader is referred to the excellent textbook by Kearns and Vazirani [34]. 2.1 Bias-Variance \nTradeoffs in Regression Bias-variance tradeoffs have been well-studied in machine learning and before \nintroducing the formal de.nitions it is instructive to look at a machine learning example. Consider the \nproblem of regression [6]. We are given a set of input-output pairs (observations) (x1, y1), . . . (xn, \nyn) and we want to predict the output y for additional but unknown input values x. In other words, we \nwant to learn a function f, such that yi = f(xi), 1 = i = n. The standard way to solve this problem is \nto consider a template for f (a restricted class of functions from which the solution is chosen) and \nthen .t the template to these observations. Figure 3 .ts three different templates to the same set of \nsix ob\u00adservations. In Figure 3(a), we show the best .t line. Even though we have chosen the best of all \npossible lines, the .t is quite poor (mean\u00ading there are large errors even for the observed data points) \nand we do not expect the predictions obtained to be good. Figure 3(a) il\u00adlustrates under.tting: lines \nare too imprecise to represent our ob\u00adservations. Next, we .t a quadratic curve (Figure 3(b)) and it \nseems to be a good .t: we expect it to produce good predictions. Fig\u00adure 3(c) shows the .t of a polynomial \nof degree .ve: a .fth degree polynomial can interpolate between the six observations. The .t is extremely \ngood for the actual observations, but there seems little reason for con.dence in the predictions for \nvery large or very small values of x given by this particular choice of function. Figure 3(c) illustrates \nover.tting. The nature of the under.tting and the over.tting for the exam\u00adples in Figure 1 and Figure \n3 have similarities and differences. We observe that both excess precision and limited precision lead \nto bad results. But there are some obvious differences. The program anal\u00adysis example in Figure 1 looks \ncleaner: the abstractions are over\u00adapproximating some of the loop behaviors, whereas in Figure 3(b) the \nquadratic model does not even agree with the observations. The difference is due to noise. In machine \nlearning, the data is typically noisy, whereas programs are precise descriptions. Therefore, the de.nition \nof generalization we use and the development in this pa\u00adper are for the noise-free case. Of course, programs \ncan have bugs and these can be thought of as noise and learning theory has mech\u00adanisms for incorporating \nnoise in generalization [34]. De.ning and handling noise in program analysis is interesting future work. \n 2.2 Learning Theory Primer Consider an instance space . which is the set of all instances. Suppose \neach xi . . is associated with a label \u00a3(xi) that belongs to a label set .. Let H be a hypothesis class, \nthat is, the set of all functions h : . . . considered by the learning algorithm. The goal of a learning \nalgorithm is to choose an h . H such that for each xi . ., h(xi) is a good estimate of the label \u00a3(xi). \nFor the example in Section 2.1, . = R, . = R, and H is a set of polynomials. We are given a set S = {(xi, \nyi) : i = 1, . . . , m} . . \u00d7 . called the training set, and we want to .nd a hypothesis h . H which \ngeneralizes over the whole instance space, that is, for all x . ., |h(x) - \u00a3(x)| is small. The notion \nof instance space is general and can capture pro\u00adgram states: it can be the collection of points in Rn \nfor numerical programs, a collection of stack and heap pairs for heap manipu\u00adlating programs, or a valuation \nof some numerical variables and arrays for array manipulating programs. One example of labels can be \nwhether a state is reachable or unreachable and we might want our hypothesis to be a predicate which \npredicts for each state a label true (denoting reachable) or false (denoting unreachable), given some \nknown reachable and unreachable states. For program anal\u00adysis, we are interested in predicates as hypothesis \nclasses. Hence, there are only two labels, true and false, and we limit our discussion to binary labels \n. = {true, false} unless stated otherwise. Given a set of labeled instances called the training set S \n. . \u00d7 ., one natural method to perform learning is empirical risk minimization (ERM): .nd a hypothesis \nh . H such that the number of labeled instances (xi, yi) . S for which h(xi) yi = is minimized. By .nding \na hypothesis that works well on the training set, we hope to .nd a hypothesis which works for the whole \ninstance space .. However, it is not clear whether such an h generalizes. Next, we formally de.ne the \nnotion of generalization that we use. PAC learn\u00ading [48] assumes that the training set S consists of \nm independent and identically distributed (iid) labeled instances drawn from an ar\u00adbitrary but .xed distribution \nD over the instance and label space. If we draw a new labeled iid sample from D, then we are interested \nin the probability that the actual label agrees with the predicted label: if (x, y) ~ D then what is \nPr [h(x) = y]? So .rst we train, that is, generate a hypothesis using a training set, and then we test, \nthat is, evaluate the performance of the hypothesis on the new samples. A PAC learner takes some samples \nfrom D as input and with high probability outputs a hypothesis that is approximately correct if one were \nto draw a new sample from D, then with high probabil\u00adity the predicted and the actual label agree. Since \nthe output of the PAC learner predicts the labels for instances that it has not seen, we say that it \ngeneralizes.  De.nition 2.1. A learner generalizes if given m samples (deter\u00admined by parameters d, \nE and the hypothesis space H) from a dis\u00adtribution D, with probability 1 - d it outputs a hypothesis \nh . H, such that (x, y) ~ D . Pr[h(x) = y] = E. Now, why does this de.nition make sense? Suppose the \nlearner wants to convince an adversary that it can produce hypotheses that generalize. If an adversary \ncontrols the training set, then she can generate a very bad training set with no information about the \nstructure of the problem. For example, the adversary can just duplicate a labeled instance an unbounded \nnumber of times and can claim to have generated a large or even an unbounded training set. Or she can \ngenerate samples in the training set that are related to some particular behaviors, and when testing \nthe generalization properties of the generated hypothesis use completely different behaviors. The poor \nhypothesis generated using certain behaviors is bound to perform poorly on behaviors that it has no idea \nabout. An algorithm that can succeed against such a powerful adversary seems unlikely and so it seems \nreasonable to weaken the adversary. First, to de.ne generalization, the training set should have some \nguarantee of having a good coverage of behaviors; by selecting training inputs randomly, we ensure formally \nthat our training set is not adversarially generated. In testing, the adversary might defeat the generated \nhypothesis by testing on very skewed inputs. By testing on iid samples, we also take this power away \nfrom the adversary. 2.2.1 Bias and Variance First, we formally de.ne ERM. The learner .nds a hypothesis \nthat minimizes the empirical error e over a training set S = {(xi, yi) : 1 = i = n}: r e(h)= 1 m1{h(xi) \n= yi} (1) m i=1 where 1 is the indicator function: 1{b} = 1 if b is true and 0 if b is false. In empirical \nrisk minimization, we try to .nd a hypothesis h that minimizes the empirical error e(h) and hope that \nit generalizes. The generalization error e(h) for a hypothesis h is de.ned as follows: e(h) = Pr(x,y)~D \n[h(x) = y] (2) The objective of a learning algorithm is to compute a hypothesis with low generalization \nerror. By minimizing the empirical error, we hope to achieve this objective. One of the fundamental theorems \nin machine learning is the following [34]: Theorem 2.2. For a hypothesis space H, let d = V C (H) (de.ned \nin Section 2.3). Then, given m samples, empirical risk minimization with high probability produces h \n. H such that: e(h ) = e(h ) + Od m and also e(h ) = e(h * ) + Od m where h * is a hypothesis with \nthe minimum generalization error in H. This theorem gives us a bound on the generalization error. In \nparticular, the .rst part of the theorem bounds the generalization error using the empirical error. To \ngeneralize well or to have a low generalization error we want the bound to be small. This theorem says \nthat we can produce a large generalization error for two reasons: 1. The term e(h * ), the bias, is the \ngeneralization error of the best hypothesis in H, i.e., the one that minimizes the generalization error. \nIf this value is large then the hypothesis class under.ts: even if we select the best available hypothesis, \nwe still have generalization errors. 2. The term Od , the variance, grows with the VC dimen\u00ad m sion \nor precision of the hypothesis class. If this value is large generalization errors occur from over.tting \nthe training data. Therefore, low precision causes generalization error due to bias and high precision \ncauses generalization error due to variance, and this leads to the bias-variance tradeoff. A corollary \nof this theorem has been used by [47] for an alternative purpose: for a speci.c hypothesis class H, using \nV C (H) to bound the number of samples required to ensure that the generalization error is below a user\u00adspeci.ed \ntolerance. By trying multiple hypothesis classes in order of increasing pre\u00adcision, one can address bias-variance \ntradeoffs. One starts with an imprecise hypothesis class and gradually increases precision until the \nbounds start degrading. In the extreme, when we have an empty hypothesis class, then the bias is high \nand the variance is zero. At the other extreme, for very expressive classes, the bias can become zero \nand the variance is high. When the size of the hypothesis class increases, so that successive hypothesis \nclasses include the previous hypothesis classes, then bias decreases monotonically and variance increases \nmonotonically. By gradually increasing precision, we can .nd a sweet spot and achieve low bounds on generalization \ner\u00adror [24].  2.3 VC dimension The Vapnik-Chervonenkis or VC dimension [34] is a purely combi\u00adnatorial \nquantity that measures the capacity of a hypothesis class. De.nition 2.3. A hypothesis h satis.es a set \nof labeled instances (X, \u00a3), iff .x . X, h(x) = \u00a3(x). De.nition 2.4. Given a set X of instances, a hypothesis \nclass H shatters X if for any labeling \u00a3 there exists an h . H s.t. h satis.es (X, \u00a3). De.nition 2.5. \nV C (H) is the cardinality of the largest set that H can shatter. In our setting, when a hypothesis satis.es \na set of labeled in\u00adstances, some of the instances are labeled true and others are la\u00adbeled false, and \nthe hypothesis is a predicate containing all the points labeled true and excluding the points labeled \nfalse. If a class is able to shatter large sets, then it has high precision, and it is precisely able \nto separate instances with different labels. The VC dimension is the largest number of points that one \ncan shatter. If the VC dimension is too high then we can over.t, and ERM does not produce a hypothesis \nthat generalizes. In the extreme case, if the VC dimension of a hypothesis class is in.nite, then we \ncannot bound its generalization error (Theorem 2.2). To prove that the VC dimension of a hypothesis class \nH is at least d, we need to show a set of d points in the instance space . that H can shatter. To prove \nan upper bound u on VC dimension, we need to show that for any possible selection of u points from ., \nH cannot shatter the u points. Since we want an upper bound on generalization error, it is generally \nsuf.cient to .nd upper bounds on VC dimension.  Figure 4. Shattering three points in two dimensions \nusing one inequality. 3. Abstract Domains In this section, we calculate the VC dimension for several \npopu\u00adlar abstract domains and show that the VC dimension gives results which match our expectations. \nOur goal is not to calculate the VC dimension of every possible abstract domain. We consider some simple \nabstract domains and the techniques we develop are use\u00adful (but might not be suf.cient) for computing \nthe VC dimension of other more complicated abstract domains as well. The VC di\u00admensions of the numerical \ndomains we consider (Section 3.1, Sec\u00adtion 3.2, and Section 3.3) follow from standard results in learn\u00ading \ntheory. We also consider predicates over arrays (Section 3.4) and separation logic (Section 3.5). We \nare unaware of any previous study of VC dimensions of these domains. 3.1 Standard numerical domains \nOur main idea is to compute the VC dimension of different abstract domains in order to capture their \nprecision. The formal proofs for the numerical domains discussed in this section are standard textbook \nmaterial in machine learning [34]. The instance space . we .rst consider is Rn, which can represent the \nstate of n-variable numerical programs with no arrays and no data structures. Arrays and data structures \nare discussed in Sections 3.4 and 3.5. 3.1.1 Single Inequality First, let us consider the hypothesis \nclass consisting of single linear inequalities. We select this class as it is one of the simplest hypoth\u00adesis \nclasses. The following result is known: Theorem 3.1. The VC dimension of the set of single inequalities \nin n dimensions is n + 1. Consider the 2-dimensional space R2. This theorem states that the VC dimension \nof the class of inequalities of the form ax + by + c = 0 is three. Figure 4 shows a set of three points \nthat can be shattered using this hypothesis class. Circles are the points labeled false and crosses are \nthe points labeled true. Note that we cannot shatter some con.gurations of three points, but that does \nnot matter for VC dimension (De.nition 2.5). For instance, as shown in Figure 5, if the points are collinear, \nthen there is a labeling that cannot be satis.ed by a single inequality: there is no inequality that \ncan include the two crosses and exclude the circle. We cannot shatter any con.guration of four points \nusing a single inequality, and the canonical con.gurations that cannot be shattered are shown in Figure \n6. In the .rst con.guration of four points, we cannot satisfy the inner point with label false and the \nother three with label true. In the second con.guration, we cannot satisfy the labeling in which diagonally \nopposite points have the same label and adjacent points have different labels. Figure 5. One inequality \ncannot shatter 3 collinear points. Figure 6. One inequality cannot shatter four points. To prove Theorem \n3.1, observe that an inequality in n - 1 dimensions can be looked upon as f(x) = 0, where f(x) is a plane \nin n dimensions passing through the origin. It is easy to see that we can shatter n points with such \ninequalities. Consider a set X of n points where the ith point has the coordinate 1 in the ith dimension \nand 0 otherwise. If Z . X is the set of examples labeled true, then the inequality f(x) = 0, where f(x) \nhas the ith coordinate 1 if xi . Z and -1 otherwise, satis.es X. So the VC dimension of an inequality \nin n dimensions is at least n + 1. To get the upper bound, we instantiate the following generalized lemma: \nLemma 3.2. Let F be a function class containing functions f : . . R, and let A = {{x : f(x) = 0} : f \n. F }, then V C (A) = dimension(F). Since planes in n dimensions passing through the origin are generated \nfrom n basis vectors, we conclude that the dimension of such planes is n, and Theorem 3.1 follows. From \nthis simple example of a VC dimension calculation, one can observe that computing the VC dimension of \na hypothesis class can be a non-trivial task and can require reasoning about the mathematical structures \nunderlying the instance space and the hypothesis class. Now we proceed to some of the more complicated \nhypothesis classes that are relevant for program analyses.  3.1.2 Intervals Intervals or conjunctions \nof inequalities of the form \u00b1x = c is a well-studied abstract domain [16]. It is also known for under.tting, \nas useful invariants often require relationships between multiple variables. Theorem 3.3. The VC dimension \nof intervals in n dimensions is 2n. Proof. Consider the 2n points X = {x-n, . . . , x-1, x1, . . . , \nxn}, where xi has sign(i) as coordinate |i| and the rest of the coordi\u00adnates zero. We can shatter these \n2n points using a construction that is a generalization of Figure 7. Moreover, intervals cannot shatter \n2n + 1 points. Any collection of 2n + 1 points has at most 2n extreme points: an extreme point of a collection \nof points has the highest or the lowest coordinate along some dimension among the points in the collection. \nThere is at least one non-extreme point as there are 2n extremes and 2n + 1 total points. Since intervals \nare convex, if we consider any set of 2n + 1 points, and assign the ex\u00adtreme points the label true and \nthe non-extreme point(s) the label false then no interval can include all the true and exclude the false \npoint(s). For example, if we consider 2n + 1 points consisting of X and the origin, then the points in \nX are extreme points and an interval cannot satisfy the labeling, where points in X are labeled true \nand the origin is labeled false.  Figure 7. Shattering four points using intervals in 2 dimensions. \nSymmetric labelings have been omitted. Figure 8. Using a polyhedron to satisfy a labeling. We now move \non to more expressive abstract domains and show that these have higher VC dimension than intervals. \n 3.1.3 Polyhedra Given an arbitrary number of points on a circle, with arbitrary labelings, using convex \nhulls, polyhedra can separate the points labeled true from points labeled false. An example is shown \nin Figure 8. As a consequence, we have the following theorem: Theorem 3.4. The VC dimension of polyhedra \nis in.nite. Using Theorem 2.2, we can conclude that when using polyhe\u00addra, it is not possible to bound \nthe generalization error in our frame\u00adwork. However, many useful invariants require general inequalities. \nThe fact that the VC dimension of polyhedra is in.nite does not pre\u00advent us from handling general inequalities \nand these are addressed in the next section.  3.2 Templates If we restrict polyhedra to k inequalities, \nthen it turns out that the VC dimension is bounded above by O(kn log(k)). The proof of this fact relies \non general composition theorems [8, 9] that relate how the VC dimension composes when complex hypothesis \nclasses are obtained by composition of more primitive classes. Here we are composing k inequalities to \ngenerate a polyhedra and these theorems are applicable. To state these theorems, we require additional \ntechnical machinery that does not add to the development of the ideas in this paper and therefore we \nomit them. In this case, as expected, the VC dimension is higher than inter\u00advals but seems manageable. \nIf one can perform program analysis while keeping the number of inequalities .xed, then the general\u00adization \nerror is bounded in our framework and one might be able to combat over.tting. Template-based invariant \ninference engines perform analysis assuming the number of inequalities to be a .xed user-provided constant \n[15, 27, 28]. Now consider a given boolean combination instead of just con\u00adjunctions of inequalities. \nApproaches for performing abstract inter\u00adpretation with a given .xed number of disjunctions are known \n[46]. In the case of template-based invariant inference, the recipe for .nding the invariant is the same \none constructs a template and solves constraints to instantiate template parameters. The template can \nhave only conjunctions [15] or a given boolean combination of inequalities [27]. Indeed, the VC dimension \nof a given boolean combination of k inequalities in n dimensions is also bounded by O(kn log(k)), which \nis the same as the VC dimension of conjunc\u00adtions of k inequalities [9]. This observation suggests that \none does not expect results to degrade signi.cantly due to higher variance when using this more expressive \ntemplate. TCM (template constrained matrix) domains [45] provide a knob to vary precision. In TCM, one \ncan provide linear expressions, and abstract interpretation infers lower and upper bounds for them. Intervals \nand octagons are special cases of TCM. As we increase the number of linear expressions, the VC dimension \nincreases. In the limit, when we have an in.nite number of linear expressions, TCM becomes polyhedra \nand the VC dimension is in.nite. Here is an example of a family of abstract domains that can combat bias\u00advariance \ntradeoffs. An intelligent template choice ensures that we neither under.t nor over.t and therefore leads \nto better results. Moreover, by calculating the VC dimension, one can observe how the precision increases \nif additional linear expressions are added and thus can help an abstract interpretation designer make \nsystematic decisions for choosing linear expressions by answering questions such as the expected increase \nin precision from adding a new linear expression. 3.3 Non-linear arithmetic First, we compute an upper \nbound on the VC dimension of the hy\u00adpothesis classes composed of polynomial inequalities. Recall that \nan upper bound on the VC dimension suf.ces for bounding the generalization error (see Theorem 2.2). We \nshow that the bound depends on the degree of the polynomial. Suppose we are given that the invariant \nis composed of quadratic inequalities. Conceptu\u00adally, we can create a new variable for every monomial \nup to degree two [1]. The quadratic invariant in the old set of variables becomes a linear invariant \nover the new set of variables. The calculations done above carry over, just with an increased dimension. \nSince the number of monomials increases with the degree under considera\u00adtion, the bound also increases \nwith the increasing polynomial de\u00adgree. In the extreme, when the degree is unbounded, we cannot bound \nthe VC dimension. If there are other sources of non-linearities, similar arguments apply and we can add \nnew variables so that non\u00adlinear invariants are reduced to linear invariants [26].  3.4 Arrays Consider \nan array manipulating program. The program state is the valuation of integer variables and potentially \nunbounded arrays. The examples of useful invariants over arrays that occur in the pro\u00adgram analysis literature \nare generally universally quanti.ed pred\u00adicates involving a small number (1 to 3) of arrays [7]. Likewise, \nif we consider the predicates of the form .i . \u00b1 a[i] = c as the hypothesis class H, and the instance \nspace . as the valuation of an array denoted by a, then the VC dimension is 2, which is the same as an \ninterval in one variable. Thus, such predicates can suffer from the same problem of under.tting as intervals. \nTheorem 3.5. The VC dimension of predicates .i . \u00b1 a[i] = c is 2. Proof. Consider three states, and let \nvi denote the smallest value stored in the array of the ith state. Among the eight labelings that need \nto be satis.ed, consider the con.guration in which the states with the smallest and largest vi are labeled \ntrue and the third state is labeled false. This labeling cannot be satis.ed by this class of predicates. \nAlso, two distinct states, each consisting of only a single element in the array, are shattered by the \npredicates of the form x = \u00b1c.  We can extend the above result to a more general statement. Con\u00adsider \nthe instance space . of points in n + 1 dimensions, . = Rn+1, that represents the values of n + 1 numerical \nvariables. Also consider the instance space .a that represents the values of an array a of unbounded \nsize and n numerical variables, .a = R. \u00d7 Rn. We de.ne three maps. The map M maps each pred\u00adicate over \n. to a predicate over .a , M(P (x, x1, . . . , xn)) = .j.P (a[j], x1, . . . , xn) and j . {x1, . . . \n, xn /}. We also use M to denote pointwise extension of this map to a set of predicates. Next, f : . \n. .a, is a map such that f(c, c1, . . . , cn) is an in\u00adstance in .a where the array a has all elements \nequal to c and the ith numerical variable of .a is assigned ci. Finally, g : .a . . is a map where g(a, \nc1, . . . , cn) assigns the .rst numerical variable of . an arbitrary element of a and the rest of the \nn variables are assigned values c1, . . . , cn. If H is an arbitrary hypothesis class of predicates over \n. then the following result holds: Theorem 3.6. V C (H) = V C (M(H)). Proof. To prove V C (H) = V C (M(H)), \nobserve that if H can shatter m points p1, . . . , pm using predicates P1, . . . P2m then M(H) can shatter \nf(p1), . . . , f (pm) using M(P1), . . . , M(P2m ). The proof for the reverse inclusion, that is, V C \n(H) = V C (M(H)) is similar and uses g and M-1 (which exists since M is one to one). This result allows \nus to compute the VC dimension of richer hypothesis classes and boolean combinations of the same. For \nex\u00adample, if we have a numerical variable z and an array a as our pro\u00adgram variables, then by the results \nin Section 3.1 and Theorem 3.6 the VC dimension of the class of predicates .j.a[j] + c1z = \u00b1c2 is three \nand for the class of conjunctions of predicates of the form .j.a[j] = \u00b1c and z = \u00b1c is four.  3.5 Separation \nLogic For heap manipulating programs, separation logic has emerged as a successful approach [44]. We \ndo not review all the details of separation logic here and keep the discussion at an abstract level. \nThe predicates or the elements of the hypothesis class are formulas written in separation logic and elements \nof the instance space or the program states are pairs of a store s and a heap h. Since we focus on the \nheap, we keep the store .xed. Our instance space is composed of program states in which the store maps \na single variable z to a heap location l. In [10], a heuristic algorithm for inference with lists has \nbeen described. The related fragment shown below has an in.nite VC dimension. e ::= x | X | c p ::= emp \n| e1 . e2, e3 | p * q | list e An expression is either a variable x, or a logical variable X, or a constant \nc (e.g. nil) and an assertion says that the heap is either empty, or it contains one cons cell and e1 \nis the address of the cons cell with contents e2 and e3, or the assertion has a separating conjunction \n(p*q) that decomposes the heap into two disjoint parts, one where p is true and one where q is true. \nA list denotes a nil terminated singly linked list. The logical variables in the assertions are implicitly \nexistentially quanti.ed. Theorem 3.7. VC dimension of the above class of predicates is in.nite. Bench \nPK/OCT PK/BOX OCT/BOX c : unc. c : unc. c : unc. a2ps 12.74 0.78 0 21.64 0 2.13 18.94 0 0.93 gawk 21.34 \n0 0 26.96 0 0 17.97 0 0 chess 5.99 5.78 2.47 12.67 3.68 2.24 14.87 0 0 gnugo 18.75 2.08 2.08 22.50 1.66 \n1.11 10.86 0 1.12 grep 3.30 0 0 8.26 0 0 8.26 0 0 gzip 21.16 2.18 0 32.84 0.72 1.45 26.27 0 0 lapack \n11.84 5.67 0.85 78.96 2.16 2.99 85.03 0 0 make 6.50 4.00 5.50 6.52 4.34 5.97 11.94 0 0 tar 5.17 4.20 \n0 9.70 3.23 0.97 9.38 0 0 Figure 9. Results of comparing abstract domains published in [29]. Proof. \nWe need to show that given any n, we can construct n heaps and shatter them using the predicates of our \nlogic. We give a proof sketch. The intuition is that we can introduce an unbounded num\u00adber of logical \nvariables in the heap and lists can encode a boolean choice. Therefore, we can make an unbounded number \nof boolean choices and shatter unbounded sets. Suppose n is two. The con\u00adstruction below can be generalized \nto arbitrary n. Recall, the store maps z to l. Consider two heaps, h1 = [l . (l1, nil), l1 . (nil, nil)] \nand h2 = [l . (nil, l1), l1 . (nil, nil)]. The predicates z . (X, Y ), z . (X, Y ) * list(X), z . (X, \nY ) * list(Y ), and z . (X, Y ) * list(X) * list(Y ) shatter the four labelings, (false, false), (true, \nfalse), (false, true), (true, true), of (h1, h2) repectively. In general, by introducing n logical vari\u00adables \nwe can shatter n heaps obtained using this construction. Similar to Section 3.2, we can restrict the \nsize of the predicates to bound the VC dimension and hence the generalization error. We are unaware of \nany template-based analysis that restricts the structure of the predicates in separation logic, but given \nthe success of template-based invariant inference for numerical programs, it seems to be a useful research \ndirection to pursue in the future. 4. Discussion In this section, we consider empirical results from \nvarious papers on program analysis and try to interpret these results using bias\u00advariance tradeoffs. \nWe do not claim to be exhaustive; the goal here is to show that a variety of useful techniques can be \njusti.ed using our framework. 4.1 Abstract Interpretation Consider the subset of results (Figure 9) \nobtained from the PA-GAI static analyzer that were published in [29]. The columns in the table of Figure \n9 compare the quality of invariants found by polyhedra (PK) and octagons (OCT), polyhedra (PK) and intervals \n(BOX), and octagons (OCT) and intervals (BOX) respectively. In each comparison there are three values: \nthe .rst value is the per\u00adcentage of invariants for which the results of the .rst abstract do\u00admain are \nlogically stronger than the second; the second value is the percentage of invariants for which the results \nof the second domain are stronger than the .rst; the third is the percentage of incompara\u00adble invariants; \nthe remaining percentage of invariants are logically identical. It can be seen from Figure 9 that as \nprecision increases the qual\u00adity of the inferred invariants gets better: this result is expected as richer \nabstract domains can express invariants that weaker domains cannot. We share the observation with the \nauthors that for a non\u00adnegligible percentage of invariants, polyhedra perform worse than intervals and \noctagons. Other evaluations in [29] (not studied here) show that the basic forward analysis of [19] can \nproduce better re\u00adsults than the path-focused approach of [30, 40], even though the latter can produce \nmore precise intermediate results. The au\u00adthors explain these observation using the non-monotonicity \nof the Figure 10. Results for interpolation with incremental increase of precision published in [32]. \nSATABS [14] and MAG I C [12] are based on weakest preconditions and BLAST [31] uses interpolants.  Outcome \nSATAB S MAG I C BL A S T BL A S T (new) Veri.ed 0 0 8 12 Re.nement failed 13 13 0 0 Did not .nish 0 0 \n5 1 widening operator. In abstract interpretation, the widening opera\u00adtor is responsible for generalization \n[17]. Widening is usually non\u00admonotonic because widening more precise information can lead to worse generalizations \n[18]. For example, [0, 1]\\[0, 2] = [0, 8] and [0, 2]\\[0, 2] = [0, 2]. In this example, widening more \nprecise intervals leads to less precise results. Non-monotonicity of the widening operator is related \nto bias\u00advariance tradeoffs. Intuitively, if there were a monotonic widening operator, then improving \nthe precision of the underlying abstract domain would lead to better generalizations, consequently, there \nwould be no bias-variance tradeoff. The tradeoff seems to be a fun\u00addamental limit on generalization, \nand therefore monotonic widen\u00ading operators for sophisticated abstract domains seem unlikely.  4.2 Interpolants \nInterpolant-based engines [31, 38] .nd simple proofs of infeasi\u00adbility of a .nite number of spurious \ncounterexample paths and hope that because the proofs are simple the predicates used in the proof will \ngeneralize and refute all possible spurious counterexam\u00adples. Inability to .nd good predicates can result \nin divergence in CEGAR. We motivate the usefulness of simple proofs using bias\u00advariance tradeoffs. An \ninterpolant is required to prove the infeasibility of a spurious counterexample. This requirement ensures \nthat the interpolant is not too weak it must be strong enough to prove a potentially useful fact. This \nrequirement can also be seen as a means to tackle under.tting: we are imposing a lower bound on the precision \nof the language of interpolants. To avoid over.tting, we want the interpolant to be simple. The hope \nis that simple predicates will not over.t to a speci.c path and hence can avoid high variance. The de.nition \nof an interpolant ensures simplicity by restricting the variables that can occur in an interpolant, which \ncorresponds to lowering the dimension of the instance space, and consequently the VC dimension as well. \nHowever, it has been observed that only restricting the variables is not enough to avoid divergence. \nIn [32], the language of inter\u00adpolants or the hypothesis class is restricted to a .nite set and this \nset is expanded gradually. If the hypothesis class is .nite, then in our framework the following result \nis known: Theorem 4.1. If h is obtained from an ERM algorithm, |H| = k, m is the size of the sample set, \nand d is .xed, then with probability at least 1 - d we have e(h ) = (minh.He(h)) + 2 1 log 2k 2m d This \ntheorem states that when the language is inexpressive, we have high bias, and as we increase the expressiveness \nof the lan\u00adguage under consideration, |H| increases, and so does the variance. Similar to Theorem 2.2, \nby gradually increasing the size of the hy\u00adpothesis class, the bounds on the generalization error can \nbe mini\u00admized. Consider the empirical results of Jhala and McMillan published in [32] and shown in Figure \n10. SATAB S, MAGI C, and BLAST are predicate abstraction based tools that use CEGAR [13] and add predicates \nduring their analysis to perform re.nement. A poor choice of re.nement predicates can cause divergence. \nThese pred\u00adicates are obtained by refuting spurious counterexample paths. If the predicates over.t the \npaths, then they are typically not useful for .nding an invariant. The last column shows the results \nobtained from the following strategy for generating predicates: use inter\u00adpolants as re.nement predicates \nand .rst restrict the interpolants to a .nite language L0. For example, L0 can be the language of pred\u00adicates \nthat have their numeric constants restricted to either zero or c \u00b1 0 where c is a numerical constant \nstatically occurring in the program. The language is incrementally expanded to L1, L2, and so on. The \npenultimate column just uses the interpolants in the lan\u00adguage L8. We observe that by incrementally increasing \nthe expres\u00adsiveness of the language of interpolants, one can avoid the over.t\u00adting present in L8 and \nobtain better results, verifying 12 instead of 8 programs from the benchmark suite of 13 programs.  \n4.3 Incrementally Increasing Precision In recent years, there has been a growing interest in exploring \nop\u00adtimal abstractions [37, 49]. These papers argue for the most im\u00adprecise abstraction that is suf.cient \nto prove the desired property of a program. (This line of work is different from the techniques that \naim to compute the least .xed point [22, 23].) Generally, an in\u00adcrease in precision is associated with \na decrease in ef.ciency. Im\u00adprecise abstractions are computationally cheap and hence are de\u00adsirable. \nLiang et al. [37] show that quite imprecise abstractions can be suf.cient to prove most properties of \ninterest. Zhang et al. [49] have an abstraction re.nement algorithm that successively tries ab\u00adstractions \nof increasing cost. Strati.ed analysis [41] is another tech\u00adnique that incrementally increases precision: \nabstract interpretation is performed in a strati.ed fashion, running successive analyses of increasing \nprecision by incrementally increasing the number of program variables under consideration. The later \nanalyses use the results of the previous analyses and heuristics based on data.ow dependencies. By performing \nthe strati.ed analysis, the authors obtain the same or better invariants than classical [19] or alter\u00adnative \nwidening schemes [3] for all the cases of their study [41]. All of these, including [32] (discussed in \nSection 4.2), can be seen as examples of addressing bias-variance tradeoffs by starting with low precision \nand moving to high precision. It is also possible to achieve the same effect by starting from high precision \nand moving towards lower precision (e.g., as done in [5]). In the next section, we explore whether the \ntechniques for combating bias-variance tradeoffs in machine learning can also bene.t program analysis \ntools. 5. Cross Validation In this section, we describe our experience with applying tech\u00adniques for \naddressing bias-variance tradeoffs to program analysis tools. In particular, we discuss the application \nof cross validation to YO G I, a veri.cation engine in Windows SDV [25]. Machine learning tools generally \nhave a number of precision knobs and .nding a con.guration of these knobs that does not over.t the training \nset is recognized as a problem [11, 21]. One widely used solution is cross validation [2]. While providing \nformal guarantees for cross validation algorithms is a topic of current re\u00adsearch [2, 33], cross validation \nhas been found to be extremely use\u00adful empirically and is standard practice in machine learning [21]. \nWe study the simplest cross validation algorithm here. Evaluation of more sophisticated variants such \nas k-fold cross validation [2] is left as future work. Cross validation partitions the training set into \ntraining data and test data. Next, multiple learning algorithms, called learners, are trained on the \ntraining data and tested on the test data. The learner that generates the hypothesis performing best \non the test data is selected. For example, for the regression example of Section 2.1,  we would consider \na subset of the observations as training data and the rest as test data (say a 70-30 split). Now consider \ndifferent learning algorithms where each algorithm .ts a polynomial of a different degree on the training \ndata. We .nd the best .t line, quadratic, cubic, etc., for the training data and pick the degree which \ncorresponds to the hypothesis that performs the best on our test data. We observe that the learner obtained \nfrom cross validation gen\u00aderates a hypothesis using training data that generalizes to test data. If we \ntrain a learner on the full training set without performing cross validation, then we might over.t. In \nFigure 3, the curve that corresponds to the .fth degree polynomial curve best .ts the ob\u00adservations (among \nlinear, quadratic, and .fth degree polynomials) and thus performs the best on the observed data. During \ncross vali\u00addation, the .fth degree polynomial over.ts on the training data and shows large deviations \nfrom the observations in the test data. Thus, cross validation can serve as a guide which rejects .fth \ndegree poly\u00adnomials for this example and prevents over.tting. Here is a full description of cross validation: \n1. Randomly split the training set S into Strain and Stest . 2. For each learner Mi, train it on Strain \nto get a hypothesis hi. Training can be just ERM over Strain . 3. Pick the learner Mk corresponding \nto hk with smallest error on Stest . Train Mk on S to obtain the output hypothesis h.  Note that once \nwe have selected the learner Mk that generalizes best to the test data, the .nal hypothesis is computed \nby training Mk on all of the available data. We remark that cross validation is not a silver bullet and \nif used improperly can itself start to over.t [42]. YO G I is an industrial strength tool for checking \nWindows de\u00advice driver properties. YO GI has a benchmark suite of 2490 driver\u00adproperty pairs and in the \ncurrent production version its preci\u00adsion knobs have been tuned to perform the best on these bench\u00admarks \n[43]. As discussed above, this process can lead to over.t\u00adting; in fact, we show that simply by using \ncross validation we can signi.cantly improve YO G I s performance. In our setting, the instances are \ninput programs that YO GI ana\u00adlyzes and the hypothesis class consists of all the versions of Yogi that \ncan be created by different choices of con.guration parameters. We are given some benchmarks and these \nconstitute our training set. The labels for these benchmarks, correct or buggy, are known. The goal of \nthe learner is to .nd a hypothesis (which is a tool) that generalizes well, that is, even for new programs \nit has not seen, we want it to assign the correct label. For YO G I, the learner is simply ERM, which \nselects the best parameter con.guration for YO GI on the training set. We consider a sequence of tools \nYogi i where i, the number of test steps, is one of the most important parameters in YO G I (see [43] \nfor details). The actual details of how YO G I works are unimportant; what is pertinent here is that \na higher i corresponds to increased precision. In Figure 2, the timing results are shown where each Yogi \ni has been trained on the full benchmark set: the parameters (other than i) have been tuned to obtain \nthe best results. We observe the expected bias-variance tradeoff curve. As i is increasing, the precision \nis increasing, and the results .rst improve with increasing precision and then they degrade. From Figure \n2, the best value of i for all 2490 driver-property pairs is 500. Note that [43] uses the total runtime \nas a performance metric. Hence, we also use total runtime for comparing performance of different con.gurations \nduring cross validation. However, more general performance metrics are certainly possible. We observe \nthat the authors of [43] have discovered a good con.guration of the tool, which we call the old YO GI; \nold YO G I is Yogi 500 trained on the Table 1. Performance of Yogi 350 and Yogi 500 on 2106 new driver\u00adproperty \npairs. The time-out was set to 30 minutes. Tool Time (min) #defects #time-outs Yogi 350 4704 189 21 \nYogi 500 8279 183 66 full set of 2490 driver-property pairs and is the current production version. Now \nwe consider the alternative, performing cross validation on YO GI. We split the 2490 driver-property \npairs into 70% train\u00ading data (consisting of 1743 driver-property pairs), and 30% test data (consisting \nof 747 driver-property pairs). We train each Yogi i (.nding values for the parameters other than i) so \nthat it performs as well as possible on the training data. Next, we pick the Yogi k that performs best \non the test data (shown in Figure 11). From Fig\u00adure 11, this best value occurs for i = 350. Next we train \nYogi 350 on all 2490 driver-property pairs to obtain the new YOG I. When veri.cation is performed on \nnew veri.cation tasks (in this case, a new set of 2106 driver-property pairs), we observe that the new \nYOG I (with i = 350) shows better generalization properties than the old YO G I (with i = 500). Even \nthough the new YO G I performs worse on the 2490 driver-property pairs (the old YO G I was the best con.guration), \nit actually performs better on the new veri.cation tasks, as shown in Table 1. We conclude that old YO \nG I was over.tted to the benchmarks. Here we have been able to reduce the run time by more than 40%, \n.nd new defects, and decrease the number of time-outs simply by using cross validation to set con.guration \nparameters. Here are some additional details about Table 1. The new YO G I and the old YO G I .nd 182 \ndefects in common. The new YO G I timed out on one defect which the old YO G I could .nd. Also, there \nare 19 time-outs that are common between the old and the new YO G I. Such a result is expected as there \nis no best con.guration. Using cross validation we are trying to .nd a con.guration that avoids over.tting \nand is expected to work well in most cases. To summarize, we empirically validate that YO G I has been \nover.tted to its benchmark suite and we remove this over.tting by cross validation to obtain a better \ntool. We believe other existing tools could be similarly improved with the straightforward applica\u00adtion \nof cross validation. 6. Recommendations and Limitations In this section, we consider the implications \nof bias-variance trade\u00adoffs in the design of automatic program analyses. We also discuss some limitations \nof our approach and make a number of speci.c recommendations.  Bias-variance tradeoffs can help in the \nselection of an abstract domain. One should avoid abstract domains with in.nite VC di\u00admension, otherwise \nwe cannot bound the generalization error in our framework (Theorem 2.2) and we expect it to be large \nin practice. Ideally, one should use domains with .nite VC dimension while en\u00adsuring the domain is rich \nenough to express the invariants of interest (to avoid large generalization error due to bias). One natural \nway to achieve this goal is to start with an imprecise domain and increase precision by gradually expanding \nthe hypothesis class (Section 4). We now discuss how bias-variance tradeoffs apply to fully auto\u00admatic \nveri.cation. Most practical tools have benchmarks and some parameters to tune. Take for instance the \nAstr \u00b4 ee tool [20], which has more than a hundred parameters governing the abstract domains to enable, \nwidening strategies, and so on. The best choice for the parameters is not usually clear. If one aims \nfor a fully automatic tool like YO GI [25], then before shipping the tool one generally performs ERM: \nthe parameters are tuned to give best results on a benchmark suite [43]. But if bias-variance tradeoffs \nexists, then this strategy is suboptimal. One can over.t to the benchmarks and unknowingly hamper the \nperformance of the tool on new inputs (Section 5). A common practice in the program analysis community \nis to tune to a benchmark suite and then report results [25, 43]. Given the results in this paper, it \nis clear that this approach provides no pro\u00adtection against over.tting and is therefore methodologically \nweak. As an extreme example, one can build a tool for any benchmark suite by simply creating a lookup \ntable with the correct answer for each benchmark. This tool works perfectly in the reported exper\u00adimental \nresults and fails completely in practice. A more realistic scenario is that a tool builder discovers \nher tool gives poor results on some program P in her benchmark suite. She makes a change and as a result \nthe tool works better on P . But has the tool really improved in general, or has the tool designer over.t \nto P ? It might be that the tool s users were better off without the change. A tool that performs poorly \non some of its benchmarks is not a bad thing if the alternative is over.tting. The goal, of course, should \nbe to build tools that work on previously unseen examples, not just on the benchmark suite. An experimental \nmethodology that divides inputs into a training set that can in.uence the design of the tool and a separate \ntest set that cannot inform the design of the tool is one way to validate that tools generalize beyond \nthe benchmarks used to design them. Some researchers may be doing this already, but we are unaware of \nanyone mentioning this point explicitly in the literature. Note that this recommendation might not be \ndirectly applicable to tools that are not designed to be fully automatic. For example, Astr\u00b4ee [20] relies \non user interaction to select the appropriate parameters for the program under analysis. Despite our \ngeneral critique of over.tting static analysis tools to benchmarks, there are situations in which that \nmay be the right thing to do. When the goal is to analyze a speci.c program (e.g., the veri.cation of \nseL4 [35]) rather than to build a tool that works for any program, it might make sense to over.t to the \none important benchmark. The situation is also different for analysis algorithms that can be proven analytically \nto give an optimal or best result, such as certain classes of type inference, data.ow analysis, abstract \ninterpretation over .nite lattices, and the analysis of loop-free and recursion-free programs. Here the \nsetting is suf.ciently tractable that the generalization strategy is provably the best possible (or generalization \nis not needed at all) and reporting how well the algorithm works on a single full benchmark suite is \na reasonable practice. Finally, if one had some guarantee that a benchmark suite was representative of \nall possible inputs there would be no need for a separate test suite in this case improving performance \non the training set guarantees improvement in general. However, it seems dif.cult to obtain or prove \nthat a benchmark suite is representative, even for a particular domain. The drivers we consider as the \ntraining set in Section 5 are a superset of the benchmarks of [25] and from our results we can conclude \nthat they are not representative. The competition on software veri.cation [4] was introduced as a common \nplatform to compare tools. However, the current structure of the competition does not avoid over.tting. \nThe benchmarks and the expected results of the competition are public and the benchmarks used to compare \ntools are a subset of the publicly released benchmarks. This contest design addresses a real concern, \nwhich is that because the semantics of C is underspeci.ed and the benchmarks are C programs, publishing \nall benchmarks in advance helps ensure that the organizers and participants agree on the intended meaning \nof the programs. Our recommendation to organizers of competitions to evaluate software tools is that \nthey should always evaluate the tools on some new, unseen programs as a check on over.tting. These unseen \nprograms must be chosen carefully to ensure that any possible variations in interpretation are irrelevant \nto the veri.cation task. 7. Conclusion Because of bias-variance tradeoffs, increasing the precision of \na program analysis can lead to a decrease in the quality of results. We have adapted the PAC learning \nframework to explain bias-variance tradeoffs in program analysis and used VC dimension as a measure of \nthe precision of abstract domains. We have computed the VC dimension for some popular abstractions for \nnumerical, array ma\u00adnipulating, and heap manipulating programs and we observe that more precise abstractions \nhave higher VC dimension. We have also shown that standard techniques for addressing bias-variance trade\u00adoffs, \nsuch as incrementally increasing precision and using cross validation in tuning parameters, are applicable \nto program analy\u00adsis tools. Acknowledgments We thank Saurabh Gupta, Bharath Hariharan, Sriram Rajamani, \nand the anonymous reviewers for their constructive comments. This work was supported by NSF grant CCF-1160904. \nThis ma\u00adterial is also based on research sponsored by the Air Force Re\u00adsearch Laboratory, under agreement \nnumber FA8750-12-2-0020. The U.S. Government is authorized to reproduce and distribute reprints for Governmental \npurposes notwithstanding any copyright notation thereon. References [1] G. Amato, M. Parton, and F. Scozzari. \nDiscovering invariants via simple component analysis. J. Symb. Comput., 47(12):1533 1560, 2012. [2] S. \nArlot and A. Celisse. A survey of cross-validation procedures for model selection. Statistics Surveys, \n4:40 79, 2010. [3] R. Bagnara, P. M. Hill, E. Ricci, and E. Zaffanella. Precise widening operators for \nconvex polyhedra. Sci. Comput. Program., 58(1-2):28 56, 2005. [4] D. Beyer. Second competition on software \nveri.cation -(summary of SV-COMP 2013). In TACAS, pages 594 609, 2013. [5] D. Beyer, T. A. Henzinger, \nand G. Th \u00b4 eoduloz. Program analysis with dynamic precision adjustment. In ASE, pages 29 38, 2008. [6] \nC. M. Bishop. Pattern Recognition and Machine Learning (Informa\u00adtion Science and Statistics). Springer-Verlag \nNew York, Inc., 2006. ISBN 0387310738. [7] N. Bj\u00f8rner, K. L. McMillan, and A. Rybalchenko. On solving \nuniver\u00adsally quanti.ed horn clauses. In SAS, pages 105 125, 2013.  [8] A. Blumer, A. Ehrenfeucht, D. \nHaussler, and M. K. Warmuth. Learn\u00adability and the Vapnik-Chervonenkis dimension. J. ACM, 36(4):929 965, \n1989. [9] N. H. Bshouty, S. A. Goldman, H. D. Mathias, S. Suri, and H. Tamaki. Noise-tolerant distribution-free \nlearning of general geometric con\u00adcepts. J. ACM, 45(5):863 890, 1998. [10] C. Calcagno, D. Distefano, \nP. W. O Hearn, and H. Yang. Compo\u00adsitional shape analysis by means of bi-abduction. In POPL, pages 289 \n300, 2009. [11] G. C. Cawley and N. L. C. Talbot. On over-.tting in model selection and subsequent selection \nbias in performance evaluation. Journal of Machine Learning Research, 11:2079 2107, 2010. [12] S. Chaki, \nE. M. Clarke, A. Groce, and O. Strichman. Predicate abstraction with minimum predicates. In CHARME, pages \n19 34, 2003. [13] E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith. Counterexample-guided abstraction \nre.nement. In CAV, pages 154 169, 2000. [14] E. M. Clarke, D. Kroening, N. Sharygina, and K. Yorav. Predicate \nabstraction of ansi-c programs using sat. Formal Methods in System Design, 25(2-3):105 127, 2004. [15] \nM. Col \u00b4on, S. Sankaranarayanan, and H. Sipma. Linear invariant generation using non-linear constraint \nsolving. In CAV, pages 420 432, 2003. [16] P. Cousot and R. Cousot. Static determination of dynamic properties \nof programs. In ISOP, pages 106 130, 1976. [17] P. Cousot and R. Cousot. Abstract interpretation: A uni.ed \nlattice model for static analysis of programs by construction or approxima\u00adtion of .xpoints. In POPL, \npages 238 252, 1977. [18] P. Cousot and R. Cousot. Comparing the Galois connection and widening/narrowing \napproaches to abstract interpretation. In PLILP, pages 269 295, 1992. [19] P. Cousot and N. Halbwachs. \nAutomatic discovery of linear restraints among variables of a program. In POPL, pages 84 96, 1978. [20] \nP. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min \u00b4e, and X. Rival. Why does Astr \u00b4ee scale up? Formal \nMethods in System Design, 35(3): 229 264, 2009. [21] P. Domingos. A few useful things to know about machine \nlearning. Commun. ACM, 55(10):78 87, 2012. [22] T. Gawlitza and H. Seidl. Precise .xpoint computation \nthrough strat\u00adegy iteration. In ESOP, pages 300 315, 2007. [23] T. Gawlitza and H. Seidl. Precise relational \ninvariants through strategy iteration. In CSL, pages 23 40, 2007. [24] S. Geman, E. Bienenstock, and \nR. Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):1 58, 1992. [25] \nP. Godefroid, A. V. Nori, S. K. Rajamani, and S. Tetali. Compositional may-must program analysis: unleashing \nthe power of alternation. In POPL, pages 43 56, 2010. [26] B. S. Gulavani and S. Gulwani. A numerical \nabstract domain based on expression abstraction and max operator with application in timing analysis. \nIn CAV, pages 370 384, 2008. [27] S. Gulwani, S. Srivastava, and R. Venkatesan. Program analysis as constraint \nsolving. In PLDI, pages 281 292, 2008. [28] A. Gupta, R. Majumdar, and A. Rybalchenko. From tests to \nproofs. In TACAS, pages 262 276, 2009. [29] J. Henry, D. Monniaux, and M. Moy. Pagai: A path sensitive \nstatic analyser. Electr. Notes Theor. Comput. Sci., 289:15 25, 2012. [30] J. Henry, D. Monniaux, and \nM. Moy. Succinct representations for ab\u00adstract interpretation -combined analysis algorithms and experimental \nevaluation. In SAS, pages 283 299, 2012. [31] T. A. Henzinger, R. Jhala, R. Majumdar, and K. L. McMillan. \nAb\u00adstractions from proofs. In POPL, pages 232 244, 2004. [32] R. Jhala and K. L. McMillan. A practical \nand complete approach to predicate re.nement. In TACAS, pages 459 473, 2006. [33] M. Kearns and D. Ron. \nAlgorithmic stability and sanity-check bounds for leave-one-out cross-validation. Neural Computation, \n11:152 162, 1997. [34] M. J. Kearns and U. V. Vazirani. An introduction to computational learning theory. \nMIT Press, Cambridge, MA, USA, 1994. ISBN 0\u00ad262-11193-4. [35] G. Klein, J. Andronick, K. Elphinstone, \nG. Heiser, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish, T. Sewell, H. Tuch, \nand S. Winwood. seL4: formal veri.cation of an operating\u00adsystem kernel. Commun. ACM, 53(6):107 115, 2010. \n[36] G. Lalire, M. Argoud, and B. Jeannet. The Interproc An\u00adalyzer. http://pop-art.inrialpes.fr/people/bjeannet/bjeannet\u00adforge/interproc/index.html. \n[37] P. Liang, O. Tripp, and M. Naik. Learning minimal abstractions. In POPL, pages 31 42, 2011. [38] \nK. L. McMillan. An interpolating theorem prover. Theoretical Com\u00adputer Science, 345(1):101 121, 2005. \n[39] A. Min e.\u00b4The octagon abstract domain. Higher-Order and Symbolic Computation, 19(1):31 100, 2006. \n[40] D. Monniaux and L. Gonnord. Using bounded model checking to focus .xpoint iterations. In SAS, pages \n369 385, 2011. [41] D. Monniaux and J. L. Guen. Strati.ed static analysis based on variable dependencies. \nElectr. Notes Theor. Comput. Sci., 288:61 74, 2012. [42] A. Y. Ng. Preventing over.tting of cross-validation \ndata. In ICML, pages 245 253, 1997. [43] A. V. Nori and S. K. Rajamani. An empirical study of optimizations \nin YOGI. In ICSE (1), pages 355 364, 2010. [44] J. C. Reynolds. Separation logic: A logic for shared \nmutable data structures. In LICS, pages 55 74, 2002. [45] S. Sankaranarayanan, H. B. Sipma, and Z. Manna. \nScalable analysis of linear systems using mathematical programming. In VMCAI, pages 25 41, 2005. [46] \nS. Sankaranarayanan, F. Ivancic, I. Shlyakhter, and A. Gupta. Static analysis in disjunctive numerical \ndomains. In SAS, pages 3 17, 2006. [47] R. Sharma, S. Gupta, B. Hariharan, A. Aiken, and A. V. Nori. \nVeri.\u00adcation as learning geometric concepts. In SAS, pages 388 411, 2013. [48] L. G. Valiant. A theory \nof the learnable. Commun. ACM, 27(11): 1134 1142, 1984. [49] X. Zhang, M. Naik, and H. Yang. Finding \noptimum abstractions in parametric data.ow analysis. In PLDI, pages 365 376, 2013.   \n\t\t\t", "proc_id": "2535838", "abstract": "<p>It is often the case that increasing the precision of a program analysis leads to worse results. It is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs. We show that <i>bias-variance tradeoffs</i>, an idea from learning theory, can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for coping with such limitations. Learning theory captures precision using a combinatorial quantity called the <i>VC dimension</i>. We compute the VC dimension for different abstractions and report on its usefulness as a precision metric for program analyses. We evaluate <i>cross validation</i>, a technique for addressing bias-variance tradeoffs, on an industrial strength program verification tool called YOGI. The tool produced using cross validation has significantly better running time, finds new defects, and has fewer time-outs than the current production version. Finally, we make some recommendations for tackling bias-variance tradeoffs in program analysis.</p>", "authors": [{"name": "Rahul Sharma", "author_profile_id": "81488651499", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4383770", "email_address": "sharmar@cs.stanford.edu", "orcid_id": ""}, {"name": "Aditya V. Nori", "author_profile_id": "81320493380", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P4383771", "email_address": "adityan@microsoft.com", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4383772", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535853", "year": "2014", "article_id": "2535853", "conference": "POPL", "title": "Bias-variance tradeoffs in program analysis", "url": "http://dl.acm.org/citation.cfm?id=2535853"}