{"article_publication_date": "01-08-2014", "fulltext": "\n Tabular: A Schema-Driven Probabilistic Programming Language Andrew D. Gordon Thore Graepel Nicolas \nRolland Microsoft Research and University of Microsoft Research Microsoft Research Edinburgh Claudio \nRusso Johannes Borgstr \u00a8John Guiver om Microsoft Research Uppsala University Microsoft Research Abstract \nWe propose a new kind of probabilistic programming language for machine learning. We write programs simply \nby annotating exist\u00ading relational schemas with probabilistic model expressions. We describe a detailed \ndesign of our language, Tabular, complete with formal semantics and type system. A rich series of examples \nillus\u00adtrates the expressiveness of Tabular. We report an implementation, and show evidence of the succinctness \nof our notation relative to current best practice. Finally, we describe and verify a transforma\u00adtion \nof Tabular schemas so as to predict missing values in a concrete database. The ability to query for missing \nvalues provides a uni\u00adform interface to a wide variety of tasks, including classi.cation, clustering, \nrecommendation, and ranking. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language \nClassi.cations Specialized application lan\u00adguages; I.2.6 [Arti.cial Intelligence]: Learning Parameter \nLearn\u00ading Keywords Bayesian reasoning; machine learning; model-learner pattern; probabilistic programming; \nrelational data 1. Introduction The core idea of this paper is to write probabilistic models by annotating \nrelational schemas. We illustrate this idea on a database for recording outcomes of a two-player game \nwithout draws. Matches Player1 link(Players) Player2 link(Players) Win1 bool  In this concrete schema, \nwe have a Players table with column Name, and a Matches table, with columns Player1, Player2, and Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA. Copyright c &#38;#169; \n2014 ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535850 Win1 ( Player \n1 wins ). As well as scalar types such as bool or string, a column may have a type such as link(Players), \nwhich means the column holds integer foreign keys to the Players table. (For simplicity, we assume that \nevery table has a single-column primary key ID, a common case in practice. We also assume that in a table \nwith n rows the keys are integers normalized to lie in the range 0..n - 1; thus, we omit the primary \nkey column from schemas.) To illustrate some of the key ideas of Tabular, we consider the TrueSkill model \n(Herbrich et al. 2006), which is deployed at cloud-scale to make selections of players of roughly equal \nskill as opponents in online gaming. In this model, each player has an underlying numeric skill Skill, \nplayers performances in a match are noisy copies of their skills, and each match is won by the player \nwith the greater performance. Players Name string input Skill real latent Gaussian(25.0,0.01) Matches \nPlayer1 link(Players) input Player2 link(Players) input Perf1 real latent Gaussian(Player1.Skill,1.0) \nPerf2 real latent Gaussian(Player2.Skill,1.0) Win1 bool output Perf1 > Perf2 Although its starting point \nis the underlying concrete schema, a Tabular schema may contain additional latent columns, which con\u00adtain \nrandom variables to help model concrete data. In our example, the Players table has a latent column Skill, \ncontaining a numeric skill for each player, while the Matches table has latent columns Perf1 and Perf2, \ncontaining the performances of the two players in the match. So that a schema de.nes a probability distribution \nover database instances, we annotate columns with probabilistic model expres\u00adsions, which de.ne distributions \nover entries in the column. Model expressions allow predictions to be made for the values of associ\u00adated \ncolumns. Our example shows three sorts of annotated column: (1) A concrete column marked as an output \nhas a model expression that predicts values of the column. For example, the Win1 col\u00adumn is an output; \nits model expression indicates the winner is the player with the greater performance. The model expression \ncan be applied to predict a future match outcome based on skills learnt from training data.  (2) A concrete \ncolumn marked as an input is used to condition the probabilistic model, but has no model expression and \ncannot be predicted by the model. For example, the Player1 column in the Matches table is an input; it \nis used to characterize a match but is not considered to be uncertain. (3) Finally, a column marked \nas latent is an auxiliary column, not present in the concrete database, whose model expression forms \npart of the model, and can be predicted. For example, the Skill column has a model expression indicating \neach entry is drawn from a Gaussian distribution with mean 25 and precision 0.01.  A Tabular program \ndivides the columns of the concrete database into input and output columns, and determines a probabilistic \nmodel that predicts the output columns given the input columns. If all the cells in a concrete column \nhave values we say the column is observed, but otherwise, when there are missing values, we say it is \nobservable. We consider two forms of inference. In both forms, input columns are observed. In query-by-latent-column, \nwe assume that output columns are observed we have data for each cell in the column and the task is to \npredict the latent columns. Towards the end of the paper, in Section 7, we also consider query-by-missing\u00advalue, \nwhere output columns are observable, and the task is to predict the missing values in output columns. \nQuery-by-Latent-Column Given a table of players and a table listing the outcomes of matches between those \nplayers, TrueSkill infers a numeric skill for each player, used for matchmaking. Con\u00adsider the following \ntables of players and matches. Players Matches ID Name ID Player1 Player2 Win1 0 \"Alice\" 00 1 false 1 \n\"Bob\" 11 2 false 2 \"Cynthia\" Initially, TrueSkill assigns the same uncertain skill prior to each player. \nGiven data showing that player 0 has been beaten by player 1, who in turn has been beaten by player 2, \nTrueSkill infers posterior skill distributions re.ecting the likely ranking player 0 < player 1 < player \n2. The query-by-latent-column problem for Tabular is to deter\u00admine the probability distribution over \nlatent databases for a given schema, given a concrete database. In theory, the latent database is a joint \ndistribution over all latent columns of the database. In a practical implementation, we consider only \nthe marginals (projec\u00adtions) of each of the variables in the latent database. In particu\u00adlar, for the \nTrueSkill schema, conditioned on the concrete database above, the marginal representation of the distribution \nover latent databases consists of the following tables. PlayersLatent ID Skill 0 Gaussian(22.51, 1.45) \n1 Gaussian(25.22, 1.53) 2 Gaussian(27.93, 1.45) MatchesLatent ID Perf1 Perf2 0 Gaussian(22.49, 1.11) \nGaussian(25.25, 1.14) 1 Gaussian(25.25, 1.14) Gaussian(27.96, 1.11) The distribution over the latent \ndatabase can be stored in the same relational store as the original concrete database, joined with the \nconcrete tables. While Tabular is speci.c to the domain of specify\u00ading probabilistic models for relational \ndata, users are free to deploy whatever programming or query notation is appropriate to prepro\u00adcess the \ndata into relational form and to postprocess the results of inference. Query-by-Missing-Value In this \nmode, we use tables with miss\u00ading values in observed columns as queries. For example, the follow\u00ading \namounts to a query asking how likely it is that player 2 would beat player 0, to help decide on placing \na bet. Matches ID Player1 Player2 Win1 32 0 ? The result of such a query might be the following, indicating \nthere is an 85% chance player 2 will beat player 0. MatchesQueryLatent ID Win1 3 Bernoulli(0.85) A Schema-Driven \nRecipe for Probabilistic Modelling In design\u00ading Tabular, we have in mind data enthusiasts (Hanrahan \n2012), the large class of end users who wish to model and learn from their data, who have some knowledge \nof probability distributions and database schemas, but who are not necessarily professional pro\u00adgrammers. \nTabular supports the following recipe for modelling data. (1) Start with the schema (such as the Players \nand Matches tables). (2) Add latent columns (Skill, Perf1 and Perf2). (3) Write probabilistic models \nfor latent and observed columns (skills have a prior, performances are noisy copies of skills, the player \nwith the highest performance wins).  (4a) Learn latent columns and table parameters from complete data \n(we learn players skills from a dataset of match outcomes). (4b) Or predict missing values from partially-observed \ndata (we predict a future match outcome based on a row (p1,p2,?)). There is more to the whole cycle of \nlearning from data such as gathering and preprocessing data, and visualizing and interpreting results \nbut the recipe above addresses a crucial component. Models as Factor Graphs Factor graphs are a standard \nclass of probabilistic graphical models of data, with many applications (Koller and Friedman 2009). Having \nmodelled data with a factor graph, one can apply a range of inference algorithms to infer prop\u00aderties \nof the data or make predictions. The TrueSkill model was originally expressed as a factor graph such \nas the one below, in typical plates and gates notation. The circular nodes of the graph represent random \nvariables, and the black squares are factors relating random variables. The large en\u00adclosing boxes labelled \nPlayers i and Matches m are known as plates, and indicate that the enclosed subgraphs are to be replicated. \nThe two dotted boxes are known as gates (Minka and Winn 2008), and indicate choices governed by an incoming \nedge. The nodes for some random variables are shaded to indicate they are observed, while unshaded variables \nare latent. Together with exact factor an\u00adnotations, factor graphs represent joint probability distributions. \n Like many visual notations, factor graphs become awkward as models become complex. Instead, we turn \nto probabilistic program\u00adming languages, where models are code, random variables are pro\u00adgram variables, \nfactors are primitive operations, plates are loops, and gates are conditionals or switches. BUGS (Gilks \net al. 1994) is the most popular example, and there is much current interest, wit\u00adness the wiki probabilistic-programming.org. \nIn this paper, we create models with a direct interpretation as factor graphs by writing schema annotations \nin a high-level probabilistic language. Innovations in the Design of Tabular By using the relational \nmodelling of the data encoded in the concrete schema, we write models succinctly because each table description \nimplicitly de.nes a loop (a plate) over its rows. Moreover, we save our user the trou\u00adble of writing \ncode to transfer data and results between language and database. The main conceptual innovations in Tabular \nare: (1) Annotations on a relational schema so as to construct a graphi\u00adcal model, with input, output, \nand latent columns. (2) A grammar of model expressions to stipulate the models for latent and output \ncolumns, with the semantics of tables and schemas given as models assembled compositionally from the \nmodels for individual columns. (3) Query-by-latent-column: infer latent columns from the con\u00adcrete database, \ngiven input columns and fully-observed output columns. (4) Query-by-missing-value: infer missing values \nin output columns, given input columns and partially-observed output columns.  Technical Contributions \nand Evaluations We present the de\u00adtailed syntax and type system of Tabular, and semantics by trans\u00adlation \nto a core probabilistic calculus, Fun. Theorem 1 (Transla\u00adtion Preserves Typing) asserts that the semantics \nrespects the Tab\u00adular type system. Theorem 2 asserts that a certain factor graph, ex\u00adpressed in Fun, \ncorrectly implements query-by-latent-column. We describe an implementation of Tabular using Infer.NET, \nbased on our semantics. To test Tabular in practice, we reimple\u00adment a series of factor-graph models \nfor psychometric data .rst performed using Infer.NET directly (Bachrach et al. 2012), with es\u00adsentially \nthe same results. Theorem 3 justi.es a transformation on Tabular schemas that implements query-by-missing-value \nin terms of query-by-latent-column. An extended version of this paper, with additional examples and screenshots, \nappears as a technical report (Gordon et al. 2013b). 2. Fun and the Model-Learner Pattern Fun, Probabilistic \nProgramming for Factor Graphs We use a version of the core calculus Fun (Borgstr \u00a8 om et al. 2011) with \narrays of deterministic size, but without a conditioning operation (observe) within expressions. This \nversion of Fun can be seen as a .rst-order subset of the stochastic lambda-calculus (Ramsey and Pfeffer \n2002); it is akin also to HANSEI (Kiselyov and Shan 2009). Fun expressions have a semantics in the probability \nmonad, but also have a direct interpretation using factor graphs. We have scalar types bool, int, and \nreal, record types (that are constructed from .eld typings), and array types. Let string = int[] and \nvector = real[] and matrix = vector[]. Let c range over the .eld names, s range over constants of base \ntype, and let ty(s) = T mean that constant s has type T . Types and Values (Scalars, Records, Arrays): \nT, V S ::= bool | int | real scalar type T,U ::= S | {RT } | T [ ] type RT ::= \u00d8| c : T ; RT .eld typings \nV ::= s | {c1 = V1; . . . ; cn = Vn} | [V1, . . . ,Vn] Expressions of Fun: E E,F ::= expression x | s \nvariable, constant if E then F1 else F2 if-then-else {R} | E.c record literal, projection [E1, . . . \n, En] | E[F] array literal, lookup [for x < E . F] for-loop (scope of index x is F) let x = E in F let \n(scope of x is F) g(E1, . . . , En) primitive g with arity n D(E1, . . . , En) distribution D with arity \nn R ::= \u00d8| c = E;R .eld bindings We write fv(f) for the set of variables occurring free in a phrase of \nsyntax f , such as an expression E, and identify syntax up to consistent renaming of bound variables. \nWe sometimes use tuples (E1, . . . , En) and tuple types T1 * \u00b7\u00b7\u00b7 * Tn below: they stand for the corresponding \nrecords and record types with numeric .eld names 1, 2, . . . , n. We write fst E for E.1 and snd E for \nE.2. The empty record {} represents a void or unit value. We write {c1 : T1; . . . ; cn : Tn} for a concrete \nrecord type, and thus {} for the empty record type; {c1 = E1; . . . ; cn = En}, for a concrete record \nterm; and use the comprehension syntax {ci : Ti}i.1..n and {ci = Ei}i.1..n to index the components of \na record type or term (when ordering matters) or {c : Tc}c.C and {c = Ec}c.C (where C is a set of .eld \nnames) when ordering is irrelevant. Field typings and .eld bindings are just association lists; we sometimes \nuse RT1;RT2 to denote the con\u00adcatenation of .eld typings RT1 and RT2, and R1;R2 for the con\u00adcatenation \nof .eld bindings. We implicitly identify record types up to re-ordering of .eld typings. We assume a \ncollection of total de\u00adterministic functions g, including arithmetic and logical operators. We also assume \nfamilies D of standard probability distributions, including, for example, the following. (A Gaussian \ntakes a pre\u00adcision parameter precision; the standard deviation s follows from the identity s2 = 1/precision.) \nDistributions: D : (x1 : T1; . . . ; xn : Tn) . T Bernoulli : (bias : real) . bool Gaussian : (mean : \nreal,precision : real) . real Beta : (a : real,b : real) . real Gamma : (shape : real,scale : real) . \nreal DirichletSymmetric : (length : int,alpha : real) . vector Discrete : (probs : vector) . int DiscreteUniform \n: (range : int) . int Semi-Observed Models We explain the semantics of Tabular by translating to Bayesian \nmodels encoded using Fun expressions. We consider a Bayesian model to be a probabilistic function, from \nsome input to some output, that is governed by a parameter, it\u00adself generated probabilistically from \na deterministic hyperparame\u00adter. Our semantics is compositional: the model of a whole schema is assembled \nfrom models of tables, which themselves are com\u00adposed from models of rows, assembled from models of individual \ncells. This formulation follows Gordon et al. (2013a), with two re\u00ad.nements. First, when we apply a model \nto data, the model out\u00adput is semi-observed, that is, each output is a pair consisting of an observed \ncomponent (like a game outcome in TrueSkill) plus an unobserved latent component (like a performance \nin TrueSkill). Second, the hyperparameter is passed to the sampling distribution Gen(h,w,x) as well as \nto the parameter distribution Prior(h) for convenient model building.  Notation for Bayesian Models: \nHyper Eh default hyperparameter (Eh deterministic) Prior(h) Ew distribution over parameter (given h) \nGen(h,w, x) Eyz distribution over output (given h, w, and x) (Hyperparameters and parameters both determine \nthe distribution of outputs given an input; the difference is that we specify our uncertain knowledge \nof parameters (but not hyperparameters) using the prior distribution, so that our uncertainty about parameters \n(but not hyperparameters) is reduced by conditioning on data.) For example, here is a model for linear \nregression, that is, the task of .tting a straight line to data points. This example illustrates the \ninformal notation for Fun expressions used in Section 3. For instance, we write a ~ Gaussian(h.\u00b5A,1) \nto mean that random variable a is distributed according to Gaussian(h.\u00b5A,1). We write x := E to indicate \nthat x is the value of deterministic expression E. Linear Regression: (Illustrative of informal notation \nfor Fun) Hyper The record {\u00b5A = 0; \u00b5B = 0}. Prior(h) The record {A = a;B = b} where a ~ Gaussian(h.\u00b5A, \n1) and b ~ Gaussian(h.\u00b5B,1). Gen(h,w, x) The pair (y,z) where z := (w.A) * x + w.B and y ~ Gaussian(z,1). \nIn our formal semantics for Tabular, we use a compact notation P ::= (Eh,(h)Ew,(h,w,x)Eyz) for a model. \nOur regression example is written in compact notation as follows. ({\u00b5A = 0; \u00b5B = 0}, (h)let a = Gaussian(h.\u00b5A,1) \nin let b = Gaussian(h.\u00b5B,1) in {A = a; B = b}, (h,w, x)let z = (w.A) * x + w.B in let y = Gaussian(z, \n1) in (y,z)) We use variable x for the input, y for the observed output, z for the latent output, w for \nthe parameter, and h for the hyperparameter. Databases as Fun Values We view a database as a record {t1 \n= B1;. . . ;tn = Bn} holding (relational) tables B1, . . . , Bn named t1, . . . , tn. A table B is an \narray [r1, . . . , rm] of rows, where each row is a record ri = {c1 = V1;. . . ; cn = Vn}, where c1, \n. . . , cn are the columns of the table, and V1, . . . , Vn are the items in the column for that row. \n(We view a table as an array so that a primary key is simply an index into the array, and omit primary \nkeys from rows.) The column annotations in a Tabular schema partition a whole database into a pair d \n= (dx,dy) where dx is the input database, with the input columns of each table, and dy is the observed \ndatabase, with the observed columns of each table. (For each table, the numbers of rows in the input \nand observed databases must match.) The latent database dz is a database with just the latent columns \nof the schema, and the database parameter Vw is a record holding parameters for each table. The purpose \nof query-by-latent-column is to predict the database parameter and latent database from the input and \nobserved databases. Distributions Induced by a Semi-Observed Model In later sec\u00adtions, we de.ne the semantics \nof a Tabular schema as a model P. In general, a model P de.nes several probability distributions: Prior \np(w | h) is w ~ P.Prior(h).  Full sampling p(y,z | h,w,x) is y,z ~ P.Gen(h,w,x).  Sampling distribution \np(y | h,w, x) isp(y,z | h,w,x)dz. Predictive distribution p(y | x,h) isp(y | h,w, x)p(w | h)dw. Training \ndata for a model consists of a pair d = (dx,dy) where dy is the observed output given input dx. In our \ncase, dx is the input database and dy is the observed database. Conditioned on such data d = (dx,dy) \nwe obtain posterior distributions: p(dy |h,w,dx)p(w|h) Posterior p(w | d,h) = . p(dy|dx,h) p(dy,z|h,w,dx \n) Posterior latent p(z | d,h) =p(w | d,h) dw. p(dy |h,w,dx) (The term p(dy | dx,h) is known as the evidence \nfor the model, used later in our comparison of different models on the same dataset). Given d = (dx,dy), \nthe semantics of query-by-latent-column is to compute the posterior p(w | d, h) on the database parameter, \nand the posterior latent distribution p(z | d,h) on the latent database. 3. Tabular, By Example 3.1 \nTabular and the Generative Process for Tables A schema S is an ordered list of tables, named t1, . . \n. , tn, each of which has a table descriptor T, that is itself an ordered list of typed columns, named \nc1, . . . , cn. The key concept of Tabular is to place an annotation A on each column so as to de.ne \na probabilistic model for the relational schema. We present .rst a core version of Tabular, where the \nmodel expressions M on columns are simply Fun expressions E. Tabular Schemas, Tables and Annotations: \nS, T, A S::= \u00d8| (t . T)S (database) schema T::= \u00d8| (c . A : T )T table descriptor A ::= annotation hyper(E) \nhyperparameter param(M) parameter input input output(M) output latent(M) latent M ::= E (to be completed) \nmodel expression The types T on concrete columns are typically scalars, but our semantics allows these \ntypes to be arbitrary. The Tabular syntax for types and expressions slightly extends Fun syntax with \nfeatures to .nd the sizes of tables and to dereference foreign keys. Additional Types and Expressions \nof Tabular Fun: T, E T ::= \u00b7\u00b7\u00b7 | link(t) type E ::= \u00b7\u00b7\u00b7 | sizeof(t) | (E : link(t)).c expression The \nexpression sizeof(t) returns the number of rows in table t. The expression (E : link(t)).c returns the \nitem in column c of the row in table t keyed by the integer E. In the common case when E is a column \nck annotated with type link(t), we write ck.c as a shorthand for (ck : link(t)).c. Values of type link(t) \nare integers serving as foreign keys to the table t. For simplicity, our type system treats each type \nlink(t) as a synonym for int. Generative Process for Tables A table descriptor Tis a function from the \nconcrete table holding the input and output columns, to the predictive table, which additionally holds \nthe latent columns. The descriptor de.nes a generative process to produce (1) the hyperparameters and \nparameters of the table, and (2) the output and latent columns of the table, by a loop over the rows \nof the table. In step (1), outside the loop over the data, we process the annotations in turn to de.ne \nthe hyperparameters and parameters, ignoring the input, output, and latent annotations. c . hyper(E) \nde.nes c as the deterministic expression E.  c . param(E) samples c from probabilistic expression E. \n In step (2), a loop over each row of the concrete table, we process the annotations in turn to sample \nindependently each row of the predictive table, with items for each of the input, output, and latent \ncolumns.  c . input copies c from the input row.  c . output(E) samples c from probabilistic expression \nE.  c . latent(E) samples c from probabilistic expression E.  In step (2), inside the data loop, we \nignore the hyperparameter and parameter annotations, although expressions may depend on the variables \nde.ned in step (1) outside the loop. A schema S describes a generative process to produce (1) the hyperparameters \nand parameters of each table, and (2) the predic\u00adtive table for each concrete table. Tables and columns \nare lexically scoped in sequence, although the variables bound in step (1) cannot refer to variables \nbound later in step (2). Later on, we formalize the generative processes for tables and schemas using \nour model notation; step (1) corresponds to the Hyper and Prior parts, while step (2) corresponds to \nthe Gen part. Example: Conjugate Bernoulli This standard model is used to generate random bits with a \nprobability distribution that is itself random; it is a key ingredient of mixture models.  In step (1) \nof the generative process, we de.ne both alpha and beta as 1, and sample Bias from the distribution Beta(1,1), \nthe uni\u00adform distribution on the unit interval. In step (2), we generate each row of the table by sampling \nthe Coin variable from the distribution Bernoulli(Bias) on bool, which returns true with probability \nBias. Overall, we sample the shared parameter Bias, whereas we sample each output Coin independently \nfor each row. A concrete database for this schema is simply one table with a single column Coin containing \nBooleans. Inference computes the distribution of the Bias parameter. Distributions with Conjugate Priors \nIn Bayesian theory, the Beta distribution over the parameter of the Bernoulli distribution is a par\u00adticular \ncase of a conjugate prior. It is convenient for ef.cient infer\u00adence to choose a prior that is conjugate \nto a sampling distribution. Hence, we de.ne primitive models for various standard sampling distributions \nand conjugate priors. Library of Primitive Models: P P ::= (Eh,(h)Ew,(h, w,x)Ey) primitive model CBernoulli \n. ({a = 1.0;\u00df = 1.0}, (h)Beta(h.a, h.\u00df ), (h,w, x)Bernoulli(w)) CGaussian . ({\u00b5 = 0.0;t = 1.0;. = 1.0;. \n= 2.0}, (h){\u00b5 = Gaussian(h.\u00b5,h.t); t = Gamma(h..,h.. ))}, (h,w, x)Gaussian(w.\u00b5,w.t)) CDiscrete . ({N \n= 2; a = 1.0}, (h)DirichletSymmetric(h.N,h.a), (h, w,x)Discrete(w)) These models are de.ned as primitives \nbuilt from closed Fun ex\u00adpressions. The model CBernoulli is exactly equivalent to our pre\u00advious example. \nThe concentration a of a CDiscrete determines whether the parameter a probability vector of length N \ndrawn from the symmetric Dirichlet distribution is uniformly distributed (a = 1.0), biased towards sparse \nvectors (a < 1.0) or dense vectors (a > 1.0). Notice that Gaussian is a distribution D that can occur \nwithin an expression E, while CGaussian is a primitive model that may occur as a model expression M in \nthe full syntax of Tabular. Completing Tabular We add primitive and indexed model ex\u00adpressions to enable \nthe succinct expression of complex models. Completing the Syntax of Model Expressions: M M ::= model \nexpression E simple P(c1 = E1, . . . , cn = En) primitive, with hyperparameters M[Eindex < Esize] indexed \nThe semantics of a model expression M for a column c is a model P whose output explains how to generate \nthe entry for c in each row of a table. The model P has a restricted form P = ({},(h)Ew,(h,w, x)Ey), \nwith no hyperparameters, and where h ./fv(Ew,Ey) and x ./fv(Ey). Hence, in our notations below, we omit \nthe bound variables h and x. A simple model E produces its output by running E. Model for Simple Model \nExpression E: Hyper The empty record {}. Prior() The empty record {}. Gen(w) y where y ~ E. c.C1 A primitive \nmodel P(c = Ec ) acts like the library model P, except that when P.Hyper = {c = Fc c.C} and C1 . C, hyper\u00adparameter \nc is set to Ec if c . C1, and otherwise to the default Fc. c.C1 Model for P(c = Ec ): Hyper The empty \nrecord {}. c.C1 c.C\\C1 Prior() P.Prior({c = Ec ; c = Fc }). c.C1 c.C\\C1 Gen(w) P.Gen({c = Ec ; c = Fc \n},w,{}). An indexed model M[Eindex < Esize] creates its parameter to be an array of Esize instances of \nthe parameter of M, and produces its output like M but using the parameter instance indexed by Eindex \n. Model for M[Eindex < Esize] where P is the model for M: Hyper The empty record {}. Prior() [w1, . . \n. , wEsize ] where wi ~ P.Prior() for i = Esize. Gen(w) y ~ P.Gen(wi) where i := Eindex . Generative \nProcess for Tables in Full Tabular In the full lan\u00adguage, the model expression for a column c has both \na parameter and an output; we use the variable c$ for the parameter, and the variable c for the output. \nIn step (1) the generative process, we process the annotations in turn to de.ne the hyperparameters and \nparameters. c . hyper(E) de.nes c as the deterministic expression E.  c . param(M) samples c$ from \nP.Prior() and samples c from P.Gen(c$) where P models M.  c . input is ignored.  c . output(M) samples \nc$ from P.Prior() where P models M.  c . latent(M) samples c$ from P.Prior() where P models M.  In \nstep (2), a loop over each row of the input table, we process the annotations in turn to de.ne each row \nof the predictive table. c . hyper(E) is ignored.  c . param(M) is ignored.  c . input copies c from \nthe input row.  c . output(M) samples c from P.Gen(c$) where P models M.  c . latent(M) samples c from \nP.Gen(c$) where P models M.   The generative process for the core language is a special case, where \nthe $ suf.xed variables are empty records. As before, the variables de.ned in step (1) are static variables \nde.ned once per table, whereas the variables de.ned in step (2) are de.ned for each row of the table. \nThe $ suf.xed variables help de.ne the semantics of Tabular, but are not directly available to Tabular \nprograms.  3.2 Examples of Models and Queries A mixture model is a probabilistic choice between two \nor more other models. We begin with several varieties of mixture model. Mixture of Two Gaussians Our \n.rst mixture model makes use of the library models CBernoulli and CGaussian. In step (1) of the generative \nprocess, we sample parameters z$ (containing the bias) from the prior of CBernoulli(), and parameters \ng1$, g2$ (each containing a mean \u00b5 and precision t) from the prior of CGaussian(). The empty hyperparameter \nlists in CBernoulli() and CGaussian() indicate that we use the default hyperparameters built into the \nmodels, that is, {a = 1.0;\u00df = 1.0} and {\u00b5 = 0.0;t = 1.0;. = 1.0;. = 2.0}. In step (2), we generate each \nrow of the table by sampling z from the distribution Bernoulli(z$), g1 and g2 from the distributions \nGaussian(g1$.\u00b5,g1$.t) and Gaussian(g2$.\u00b5,g2$.t) and .nally de.ning the output y to be g1 or g2, depending \non z. Given a concrete database for this schema (a column y of ran\u00addom numbers that is expected to be \ngrouped into two clusters around the means of the two Gaussians) inference learns the poste\u00adrior distributions \nof the parameters z$, g1$, and g2$, and also .lls in the latent columns. The inferred distribution of \neach z indicates how likely each y is to have been drawn from each of the clusters. Mixture of an Array \nof Gaussians To generalize to a many-way mixture, we .rst decide on a number n of mixture components \n(clusters); in this case we set n=5. To randomly select a cluster we use the CDiscrete library model, \nwhich has an integer hyper\u00adparameter N and outputs natural numbers less than N. The default value of \nN is 2; to de.ne a mixture model with n components we override the default as CDiscrete(N=n). A model \nCDiscrete(N=2) is akin to a CBernoulli that outputs 0 or 1. The indexed model CGaussian()[z < n] denotes \na model whose parameter is an array of n parameter records (containing mean \u00b5 and precision t .elds) \nfor the underlying CGaussian model. The output of the indexed model is obtained by .rst picking the parameter \nrecord at index z, and then getting an output from the CGaussian model with those parameters. The parameter \nof column z is a probability vector of length N, an array of non-negative real numbers that sum to 1, \nindicating the chance of each output value. The parameter for the y column is an array of n parameter \nrecords for the underlying CGaussian model. The observed output of each row is determined by .rst sampling \nthe cluster z from the discrete distribution, and then sampling from CGaussian[z < n]. With n=2 we recover \nour previous mixture of two Gaussians. User/Movie/Rating Schema Our .nal mixture model is a Tabular version \nof the factor graph in Figure 1 of Singh and Graepel (2012), where it was automatically generated from \na relational schema. User z int latent CDiscrete(N=4) Name string input IsMale bool output CBernoulli()[z] \nAge int output CDiscrete(N=100)[z] Movie z int latent CDiscrete(N=4) Title string input Genre int output \nCDiscrete(N=7)[z] Year int output CDiscrete(N=100)[z] Rating u link(User) input m link(Movie) input Score \nint output CDiscrete(N=5)[u.z,m.z] The model for the Score column illustrates a couple of notations \nregarding indexed models. First, a doubly-indexed model M[E1 < F1,E2 < F2] is short for (M[E1 < F1])[E2 \n< F2]. Second, we write M[E] as short for M[E < n] when we know that E is output by CDiscrete(N=n). Each \nrow in the User table belongs to one of four clusters, indexed by the latent variable z which has a CDiscrete \nmodel. For each cluster, there is a corresponding distribution over gender (IsMale) and Age. Similarly, \neach row in the Movie table is mod\u00adelled by a four-way mixture, indexed by z, with Genre and Year attributes. \nFinally, each row in the Rating table has links to a user u and to a movie m, and also a Score attribute \nthat is modelled by a discrete distribution indexed by the clusters of the user and the movie, corresponding \nto a stochastic block model (Nowicki and Snijders 2001). Query-by-Latent-Column and TrueSkill We illustrate \ndirect use of query-by-latent-column with reference to TrueSkill, and also a programming style where \nwe introduce new query tables purely for the purpose of formulating queries. First, as illustrated in \nSection 1, given tables of players and matches, inference computes distributions for the latent Skill \ncol\u00adumn; these skills can be used to do matchmaking or to display in leaderboards. It also infers distributions \nfor the Perf1 and Perf2 columns, which may indicate whether a player was on form or not on the occasion \nof a particular match. Second, suppose we wish to bet on the outcomes of upcoming matches between members \np and q of the Players table. We add a fresh query table Bets, which has the same schema as Matches except \nthat Win1 is latent instead of being an observed output. We place one row in this new table, with p for \nPlayer1 and q for Player2, and inference computes distributions for the three latent columns, including \na Bernoulli for Win1 indicating the odds of a win. By placing multiple rows in the Bets table we can \npredict the outcomes of multiple upcoming matches. Bets Win1 bool latent Perf1 > Perf2 Third, consider \nan online situation where there is a large table of players, and a relatively small number of players \nqi queuing to begin fresh online games. We may wish to select one of the qi to play against a new player \np. To do so, we add the Sim query table below, and .ll it with rows (p,qi) for each i. The latent column \nSimilar holds true if the two players are close in skill (less than 0.1 units apart). Inference .lls \nthis column with Bernoulli distributions which can be used to select a partner close in skill to p. Both \nthe means and variances of the skills of players enter into the marginal probability of being Similar, \nthus making use of the full probabilistic formulation.  4. Formal Semantics of Tabular 4.1 Semantics \nof Fun (Review) We here recall the semantics of Fun without zero-probability obser\u00advations (Bhat et al. \n2013). We write G f E : T to mean that in type environment G = x1 : T1, . . . , xn : Tn (xi distinct) \nexpression E has type T . Let Det(E) mean that E contains no occurrence of D(. . . ). The typing rules \nfor Fun are standard for a .rst-order functional language; some examples follow below. Selected Typing \nRules of Fun Expressions: G f E : T (FU N RA N D O M) (FU N ACO N ST) D : (x1 : T1 * \u00b7\u00b7\u00b7 * xn : Tn) . \nU G f Ei : T for i . 1..n G f Ei : Ti for i . 1..n G f [E1, . . . , En] : T [ ] G f D(E1, . . . , En) \n: U (FUN IND EX)(FU N ITE R) G f E : T [ ] G,x : int f F : T G f E : int Det(E) G f F : int G f [for \nx < E . F] : T [ ] G f E[F] : T The interpretation of a type T is the Borel-measurable set VT of closed \nvalues of type T (real numbers, integers, records, and so on) using the standard topology. A function \nf : T . U is measurable if f -1(A) . VT is measurable for all measurable A . VU ; all continuous functions \nare measurable. A .nite measure \u00b5 over T is a function from (Borel-measurable) subsets of VT to the non-negative \nreal numbers, that is countably additive, that is, \u00b5(.iAi) = Si\u00b5(Ai) if A1,A2, . . . are pair-wise disjoint. \nThe .nite measure \u00b5 is called a probability measure if \u00b5(VT ) = 1.0. If \u00b5 is a probability measure on \nT and f : T . U is measurable, we let f -1\u00b5(A) \u00b5( f -1(A)). In this context f is called a random variable. \nThe semantics of a closed Fun expression E is a probability measure PE over its return type. It is de.ned \nvia a semantics of open Fun expressions (Ramsey and Pfeffer 2002) in the probabil\u00adity monad (Giry 1982). \nWe write PE for the probability measure corresponding to a closed expression E; if \u00d8f E : T then PE is \na probability measure on VT . If f E : T1 *\u00b7\u00b7\u00b7*Tn, and for i = 1..m we have f Vi : Ui and Fi det and \nx1 : T1, . . . , xn : Tn f Fi : Ui, we write PE [x1, . . . , xn | F1 = V1 . \u00b7\u00b7\u00b7 . Fm = Vm] for (a version \nof) the con\u00additional probability distribution of PE given f = (V1, . . . ,Vm) where f (x1, . . . , xn) \n= (F1, . . . , Fm). 4.2 Semantics of Semi-Observed Models A model is associated with four types: a hyperparameter \ntype H, a parameter type W , an input type X, and an output type Y . Model Types and Typing of Models: \nQ, f P : Q Q ::= (H,W,X,Y ) quadruple type of model (MO D E L PR IM) \u00d8f Eh : H Det(Eh) h : H f Ew : W \nh : H,w : W,x : X f Ey : Y f (Eh,(h)Ew,(h,w,x)Ey) : (H,W,X,Y ) In a semi-observed model, Y is a pair \ntype, where the second component holds the latent variables of the model. Given a semi\u00adobserved model, \nthe standard distributions are obtained as follows. Proposition 1. Given a model P = (Eh, (h)Ew, (h,w,x)Eyz) \nsuch that f P : (H,W,X,Y * Z) the following Fun expressions denote the standard distributions: Prior: \nlet h = Eh in Ew.  Full sampling (where h = Vh, w = Vw, x = Vx): let h = Vh in let w = Vw in let x = \nVx in Eyz.  Sampling (where h = Vh, w = Vw, x = Vx): let h = Vh in let w = Vw in let x = Vx in fst \nEyz.   Joint posterior (where x = VX , y = Vy): PEw,yz | fst yz = Vywhere E = let h = Eh in let w \n= Ew in let x = Vx in w, Eyz.  Posterior: fst-1P where P is the joint posterior; and  Posterior latent \n(snd . snd)-1P where P is the joint posterior.   4.3 Typing and Translation of Tabular When typing \nschemas, we use binding times to track the availability of variables. Let B be the set {h,w,xyz} of binding \ntimes ordered such that . = h < w < xyz = T. Here h stands for the (determin\u00adistic) hyperparameter phase, \nw stands for the (non-deterministic) parameter phase, and xyz stands for the generative phase of the \ncomputation. We use metavariables e and pc to range over B. In\u00adformally, variables declared at one time \nmay only be used in expres\u00adsions typed at or above that time (the current time pc is maintained as an \nadditional index of the Tabular typing judgments). Binding times are also used to prevent the mention \nof non-deterministic pa\u00adrameters in expressions used as (necessarily deterministic) hyper\u00adparameters, \nand generative data in the construction of either hyper\u00adparameters or parameters. When translating to \nFun, binding times ensure that the target program is well-scoped, and deterministic where needed. Tabular \nLevels and Typing Environments: e, G e, pc ::= h | w | xyz binding time G ::= environment \u00d8 empty G,x \n:e T variable typing G,t : ({RT }) predictive row type for t Environments declare variables with their \nbinding time and type, and tables with their predictive row types. Judgments of the Tabular Type System: \nG f v environment G is well-formed G f T in G, type T is well-formed G fpc E : T in G at binding time \npc, expr. E has type T G fpc M : W,T in G at pc, model M has params W , returns T G f T: Q in G, table \nThas type Q G f S: Q in G, schema Shas type Q Formation Rules for Environments: G f v (EN V EM P T Y) \n(EN V VA R) (ENV TA B L E) G f T x ./dom(G) G f {RT } t ./dom(G) \u00d8f v G,x :e T f v G,t : ({RT }) f v \nFormation Rules for Types: G f T (TY P E SC AL AR) (TYPE ARR AY) (TY P E REC O RD) G f v G f T G f v \n.c . C. G f Tc G f S G f T [ ] G f {c : Tc}c.C The translation of a Tabular schema to a model is performed \nby four judgments. Though de.ned relationally, the relations are par\u00adtial functions on untyped terms \nand total functions on well-typed Tabular terms.  Judgments of the Translation: E . F Tabular expression \nE translates to Fun expr. F M . (Ew,(w)E) model M translates to (Ew,(w)E) T. P marked up table Ttranslates \nto prim. model P S. P marked up schema Stranslates to P Lemma 2 (Determinacy). If S. P and S. P1 then \nP = P1 . Theorem 1 (Translation Preserves Typing). If \u00d8f S: Q then there exists P such that S. P and \nf P : Q.  4.4 Expressions The main subtlety when translating schemas is to support foreign keys. We \nuse the notation (E : link(t)).c within Fun expressions to stand for the column c of the row in table \nt indexed by key E. In particular, when constructing the model for a table t j, we may dereference a \nforeign key of type link(ti) to a previous ta\u00adble ti with i < j. For instance, in the TrueSkill schema, \nthere is a reference from t2 = Matches to t1 = Players. To translate such foreign keys, we arrange that \nfor each table ti there is a global variable named ti that holds the predictive table for ti, that is, \nthe join of the input sub-table xi, the output sub-table yi, and the la\u00adtent sub-table zi, for each i. \nHence, an expression (E : link(ti)).c means ti[E].c; for example, (Player1 : link(Players)).Skill com\u00adpiles \nto Players[Player1].Skill. Typing Rules for Tabular Expressions: G fpc E : T (TA BU L A R VA R) (TA B \nUL AR SI Z E O F) G f v G = G1,x :e T,G2 e = pc G fpc #t : int t . dom(G) G fpc x : T G fpc sizeof(t) \n: int (TA BU L A R DE RE F) G fpc E : int xyz = pc G = G1 ,t : ({d : Td }d.C),G11 c . C G fpc (E : link(t)).c \n: Tc Rule (TA BU L A R VA R) allows a reference to x only if x is declared with a binding time l = pc, \nwhere pc is the current binding time. Translation Rules for Tabular Expressions: E . F (TR A N S VA R) \n(TRA N S SIZ E O F) (TR A NS DE RE F) E . F x . x sizeof(t) . #t (E : link(t)).c . t[F].c (the remaining \nrules are simple homomorphic translations) 4.5 Model Expressions Typing Rules for Model Expressions: \nG fpc M : W, T (MO D E L PR I M) (MO D E L SI M P L E) G f v P = ({R},(h)Ew,(h, w,x)Ey)G fpc E : T f \nP : ({c : Hc}c.C ,W,{},Y )G fpc E : {},T .c . C1 . C. G fh Ec : Hc . Det(Ec) G fpc P(c = Ec c.C1 ) : \nW,Y (MO D E L IND EXED) G fpc M : W,T G fpc Eindex : int G fh Esize : int Det(Esize) G fpc M[Eindex < \nEsize] : W [], T Primitive models must have void input; we allow to only replace a part C1 of their hyperparameters \nC. The upper bound Esize of an indexed model has binding time h, since it must be deterministic and the \nsame for all rows of the table. Translation Rules for Model Expressions: M . P (TR ANS SI M P L E) (w \n. fv(F)) E . F E . ({}, (w)F) (TR ANS PR IM) (w . fv(Eh)) P = ({c = Fc}c.C ,(h)Ew,(h,w,x)Ey) Ec . E1 \n(c . C1) c E c = if c . C1 then E1 else Fc Eh = {c = E c}c.C c P(c = Ec c.C1 ) . (let h = Eh in Ew, \n(w)let h = Eh in let x = {} in Ey) (TR ANS INDEXED) (w . fv(Findex )) Eindex . Findex Esize . Fsize \nM . (Ew, (w)Ey) M[Eindex < Esize] . ([for < Fsize . Ew],(w)let w = w[Findex ] in Ey) A simple model \nhas no prior. The prior of an indexed model is an array of Fsize independent samples of the prior of \nthe underlying model. In the output, we use the prior value at index Findex .  4.6 Tables The typing \nand translation rules for tables are de.ned inductively and determine the semantics for the shared hyperparameter, \nshared parameter, and a pair of output and latent columns for a single row of the table. Typing Rules \nfor Tables: G f T: Q (TA BLE EM P T Y) G f v G f \u00d8: ({}, {},{},{} * {}) (TA BLE HY P E R) (A = hyper(E)) \n\u00d8fh E : H Det(E) G,c :h H f T: ({RH},W,X,Y * Z) G f (c . A : H)T: ({c : H;RH},W,X,Y * Z) (TA BLE PAR \nA M) (A = param(M)) G fw M : W$,W G,c :w W f T: (H,{RW },X,Y * Z) c$ . dom(G) . dom(T) G f (c . A : \nW )T: (H,{c$ : W$;c : W ;RW },X,Y * Z) (TA BLE IN P U T) (A = input) G,c :xyz X f T: (H,W, {RX},Y * Z) \nG f (c . A : X)T: (H,W,{c : X;RX},Y * Z) (TA BLE OU T P U T) (A = output(M)) G fxyz M : W,Y G,c :xyz \nY f T: (H,{RW },X,Y * Z) G f (c . A : Y )T: (H,{c$ : W ;RW },X, {c : Y ;RY } * Z) (TA BLE LAT EN T) (A \n= latent(M)) G fxyz M : W,Z G, c :xyz Z f T: (H,{RW },X,Y * {RZ}) G f (c . A : Z)T: (H,{c$ : W ;RW },X,Y \n* {c : Z; RZ}) Rule (TA B L E HY P E R) ensures that E is deterministic and closed and declares c at \nbinding time h so it can be referenced at all binding times. Rule (TA B L E PA RA M) ensures that M is \nchecked at level w (not pc) so that its generative expression has no data dependencies and is safe to \nuse at the parameter level. Rule (TAB L E IN P U T) extends the context with c declared at xyz. Rule \n(TAB L E OU TPU T) extends the context with c declared at xyz and records the types of parameter c$ and \noutput c by extending the parameter and output record types of the table. Rule (TAB L E LAT E N T) is \nsymmetric to (TA B LE OU T P U T), but instead extends the latent record type.  The translation rules \nfor tables make use of auxiliary let\u00adcontexts, ranged over by L . These denote a spine of (Fun) let\u00adbindings \nending in a hole [], and are de.ned inductively as follows. (Core Fun) Let contexts: L L ::= let context \n[] hole let x = E in L let binding The operation L [E] plugs the hole of a L with a body E, produc\u00ading \na (Fun) expression. [][E] = E (let x = E1 in L )[E] = let x = E1 in (L [E]) Translation Rules for Tables: \nT. P (TR A N S EMPTY) \u00d8. ({},(h){},(h,w, x)({}, {})) (TR A NS HY PE R) (c . {h, w,x}) E . Eh T. ({Rh}, \n(h)Ew,(h,w,x)E) (c . hyper E : Tc)T. ({c = Eh,Rh},(h)let c = h.c in Ew,(h,w,x)let c = h.c in E) (TR \nA NS PA R AM) (h . fv(Ew,Ec, c$,c) c . {h,w,x}) M . (Ew, (wc)Ec) T. (Eh,(h)Lw[{Rw}],(h,w,x)E) (c . param \nM : Tc)T. (Eh, (h)let c$ = Ew in let c = Ec in Lw[{c$ = c$; c = c;Rw}], (h, w,x)let c = w.c in E) (TR \nA NS INPUT) T. (Eh, (h)Ew,(h,w,x)E) c . {h,w,x} (c . input : Tc)T. (Eh, (h)Ew,(h,w,x)let c = x.c in \nE) (TR A NS OU T P UT) (h . fv(Ew) h,w,x . fv(Ec,c))) M . (Ew, (wc)Ec) T. (Eh,(h)Lw[{Rw}],(h,w, x)Lo[({Ry},Ez)]) \n (c . output M : Tc)T. (Eh, (h)let c = Ew in Lw[{c$ = c,Rw}], (h, w,x)let c = (let wc = w.c$ in Ec) in \nLo[({c = c;Ry},Ez)]) (TR A NS LAT E N T) (h . fv(Ew) h,w,x . fv(Ec,c)) M . (Ew, (wc)Ec) T. (Eh,(h)Lw[{Rw}],(h,w, \nx)Lo[(Ey, {Rz})]) (c . latent M : Tc)T. (Eh, (h)let c = Ew in Lw[{c$ = c,Rw}], (h,w,x)let c = (let wc \n= w.c$ in Ec) in Lo[(Ez,{c = c;Rz})]) Rule (TR A NS HY P E R) merely extends the hyperparameter record \nof the remaining table and rebinds c as the projection h.t in the prior and gen of the model. Rule (TR \nANS PAR A M) extends table T s prior with two .elds for the prior and gen of M, and rebinds parameter \nc as the projection w.c in the gen of the row. Rule (TR A NS IN P U T) just binds c as the projection \nx.c of input row x in the gen of the table (but does not export c since it is neither output nor latent). \nRule (TRA N S OU T P UT) just de.nes c as the gen of its model, whose parameter wc is obtained from w.c$; \nc is exported in the output record of the row. Rule (TRA N S LAT E N T) is symmetric to (TR AN S OUT \nPUT), but instead extends the latent record. For example, here is a single-table schema for linear regression. \n The row semantics of this table is as follows. For readability, we inline some variable de.nitions. \nSince this table only uses simple model expressions, the $ suf.xed .elds for the parameters of model \nexpressions all contain the empty record. Modulo these redundant .elds, we recover the model from Section \n2. Model for a Row of the LinearRegression Table: Hyper {muA = 0; muB = 0} Prior(h) {A$ = {};A = Gaussian(h.muA,1); \nA$ = {};B = Gaussian(h.muB,1); Z$ = {};Y$ = {}} Gen(h,w,x) let Z = w.A* x.X + w.B in let Y = Gaussian(Z,1)} \nin ({Y=Y},{Z=Z})  4.7 Schemas Typing Rules for Schemas: G f S: Q (SC HE MA EM P TY) G f v G f \u00d8: ({}, \n{},{},{} * {}) (SC HE MA TA B L E) G f T: (H,W, {RXt }, {RYt } * {RZt })G,#t :h int,t : ({RXt ; RYt \n;RZt }) f S: ({RH},{RW },{RX},{RY } * {RZ})H1 = {#t : int;RH} W 1 = {t : W ; RW } X1 = {t : {RXt }[];RX}Y \n1 = {t : {RYt }[];RY } Z1 = {t : {RZt }[];RZ} G f (t . T)S: (H1 ,W 1 ,X1 ,Y 1 * Z1) Rule (SCH EM A TA \nBL E) uses the model type of the table to extend the context with a declaration of the table s size, \n#t at level h, ((#t is used in the translation of sizeof(t)) as well as the predictive row type of t: \nthis is the union of its input, output, and latent .elds. The table s default hyperparameters (of type \nH) are applied in the translation of t and do not appear in the type of the schema. The rule extends \nthe components of the schema s model type with additional .elds for the table size; the parameters of \nthe table (as a nested record); the inputs of the table (a nested array of records); and the pair of \noutput and latent table records extended with .elds for the output and latent arrays of records for t. \nTranslation Rules for Schemas: S. P (TR ANS EM P TY) \u00d8. ({}, (h){}, (h,w,x){}) (TR ANS TA BL E) T. (Eh,(ht \n)Ew,(ht ,wt ,xi)Lt [{Ry},{Rz}]) Rx = {c = xi.c | c . inputs(T)} S. ({Rh},(h)Lw[Rw],(h, w,x)Lyz[{Sy},{Sz}] \n Et = let ht = Eh in let wt = w.t in [for i < #t . let xi = x.t[i] in Lt [{Rx;Ry;Rz}]] Ey = [for i < \n#t . {c = t[i].c}c.dom(Ry)] Ez = [for i < #t . {c = t[i].c}c.dom(Rz)] h . fv(let ht = Eh in Ew,t,#t) \nh,w, x . fv(Et ,t, #t) (t . T)S. ({#t = 1,Rh}, (h)let t = let ht = Eh in Ew in let #t = h.#t in Lw[{t \n= t; Rw}], (h,w, x)let #t = h.#t in let t = Et in Lyz[({t = Ey;Ry},{t = Ez;Rz})]) Rule (TRA N S TAB \nLE) takes the model for the parameters and a single row of t and constructs a model that draws once from \nthe prior of t then replicates t s output distribution across an array of size #t. The intermediate array, \nEt , contains the predictive table for t, merging the input, output and latent sub-records of t as single \nrecords. Expressions Ey and Ez are used to reshuf.e the array of merged records into separate arrays \nof output and latent sub-records. The rule extends S s hyperparameter record with a default binding for \n#t (with arbitrary value 1); table sizes must be consistently overriden before inference.  Translation \nexamples To illustrate our schema translation and our treatment of foreign keys, here is the translation \nof TrueSkill, rewritten a little for readability: .rst, the two row models for the two tables, followed \nby the model of the whole schema. Model for a Row of Table Players: P1 Hyper {} Prior(h) {Skill$ = {}} \nGen(h,w, x) let Skill =Gaussian(25,0.01) in ({}, {Skill = Skill}) Model for a Row of Table Matches: \nP2 Hyper {} Prior(h) {Perf1$ = {};Perf2$ = {}; Win1$ = {}} Gen(h,w, x) let Perf1=Gaussian(Players[x.Player1].Skill,1) \nin let Perf2=Gaussian(Players[x.Player2].Skill,1) in let Win1= Perf1 > Perf2 in ({Win1 = Win1},{Perf1 \n= Perf1;Perf2 = Perf2}) Model for the TrueSkill Schema: Hyper {#Players = 1, #Matches = 1} Prior(h) {Players \n= P1.Prior(P1.Hyper), Matches = P2.Prior(P2.Hyper)} Gen(h,w, x) let Players = [for i <h.#Players . \nlet Skill = Gaussian(25,0.01) in {Skill = Skill}] let Matches = [for i <h.#Matches . let Player1 = x.Matches[i].Player1 \nin let Player2 = x.Matches[i].Player2 in let Perf1=Gaussian(Players[Player1].Skill,1) in let Perf2=Gaussian(Players[Player2].Skill,1) \nin let Win1= Perf1 > Perf2 in ({Player1 = Player1;Player2 = Player2; Win1 = Win1;Perf1 = Perf1;Perf2 \n= Perf2}) ] ({ Players = [for i < h.#Players . {}]; Matches = [for i < h.#Matches . {Win1 = Matches[i].Win1}]}, \n { Players = [for i < h.#Players . {Skill = Players[i].Skill}]; Matches = [for i < h.#Matches . {Perf1 \n= Matches[i].Perf1; Perf2 = Matches[i].Perf2}}])  4.8 A Reference Learner for Query-by-Latent-Column \nWe conclude with a learner API, a programming interface for query-by-latent-column: the API allows a \nuser to accumulate a dataset split into input and observed databases. To perform queries, we bundle a \ndatabase and a schema into a learner L = (d | S) where d = (dx,dy) and dx is the input database and dy \nis the observed database. (We assume the types of d and S match, as discussed in the next section.) To \npick out the sizes of tables in a database, we let #({t1 = B1; . . . ;tn = Bn}) {#t1 = |B1|; . . . ; \n#tn = |Bn|}). We support the following functional API. Let L0(S) be the empty learner, that is, Splus \na pair of databases with the right table names but no table rows. Let train(L,(dx 1 ,d1 )) be L1 = ((dx \n+ dx 1 ,dy + d1 ) | S) where + is y y concatenation of arrays in records, and L = ((dx, dy) | S). Let \nparams(L) be the posterior distribution p(w | d,h) induced by P, where L = (d | S), P models S, and h \n= #(dx).  Let latents(L) be the posterior latent distribution p(z | d,h) induced by P, where L = (d \n| S), P models S, and h = #(dx).  Compared to the reference learner of Gordon et al. (2013a), this new \nAPI can learn latent outputs since it works on semi-observed models. Our current implementation uses \nInfer.NET Fun to com\u00adpute approximate marginal forms of the posterior distributions on the database parameter \nand latent database, and persists them to the relational store. The API allows an incremental implementa\u00adtion, \nwhere the abstract state L is represented by a distribution over the parameters and latent variables, \ncomputed after each call to train. Our current implementation does not support this opti\u00admization, maintains \nthe whole dataset d, and does inference from scratch when necessary. The incremental formulation of our \nlearner is consistent with the Algebraic Classi.er formulation of Izbicki (2013), which promises reductions \nin computational complexity for cross-validation and enable ef.cient online and parallel training al\u00adgorithms \nbased on the monoidal or group structure of such learners. Now that we have schema typing and a semantics \nof schemas as models, we can perform inference as follows. A learner L = (dx,dy |S) is queryable if f \nS : (H,W, X,Y * Z) and \u00d8 f dx : X and \u00d8 f dy : Y , and for all tables ti . dom(S) we have |dx.ti| = |dy.ti| \n= 1. In particular, the empty learner is not queryable, since it contains empty tables. We can now implement \na latent column query. Theorem 2. If L = (dx,dy | S) is queryable, there is a closed Fun expression E \n(dx) such that if \u00b5 PE(dx ) w,yz | fst yz = dy then (1) params(L) = fst-1\u00b5; and (2) latents(L) = (snd \n. snd)-1\u00b5.  Proof: Assume that S . (Eh, (h)Ew, (h,w,x)Eyz), and let expres\u00adsion E(dx) let h = #(dx) \nin let w = Ew in let x = dx in w,Eyz. By Proposition 1, \u00b5 as above yields the sought distributions. 5. \nOutline of Practical Implementation Our implementation builds on the model-learner pattern of Gordon \net al. (2013a), in which models are represented as records of type\u00adindexed F# quotations representing \ntyped Fun expressions. Our ini\u00adtial Tabular implementation generates such strongly-typed models. This \ntarget confers two advantages: the quotation fragments are compact yet statically checked for type correctness; \nthe resulting terms are easily JIT-compiled to produce ef.cient sampling code. For clarity, the semantics \nin Section 4 splits compilation into type-checking followed by untyped translation. To create strongly\u00adtyped \nquotations, we need to convince F# s type checker that our dynamically constructed quotations are composed \nin a statically safe manner. The most direct way to do so is to re-structure the sep\u00adarate typing and \ntranslation judgments as single elaboration judg\u00adments that couple type-checking with translation. The \nF# rendi\u00adtion of this idea is a triple of polymorphic functions that repre\u00adsent the typing contexts as \na pair of (nested) tuples. Contexts are extended as required by using polymorphic recursion in recursive \ncalls to elaboration. The output of elaboration is a value of existen\u00adtial type containing both the target \ntype and the target translation of the source term. Since type variables have accurate run-time repre\u00adsentations \nin .NET, we can directly compare the types of generated sub-expressions as needed, avoiding the need \nto maintain separate type representations.  Participants Ability real latent Gaussian(0.0,1.0) Questions \nAnswer int latent DiscreteUniform(8)  QuestionID QuestionID Advantage link(Questions) link(Participants) \nlink(Questions) real input input input latent DB(ParticipantID.Ability - QuestionID.Di.culty,0.2) Know \nbool latent Probit(Advantage,QuestionID.Discrimination) Guess int latent DiscreteUniform(8) Response \nResponsesTrain ResponseID Response int link(Responses) int latent input output if Know then QuestionID.Answer \nelse Guess ResponseID.Response   Figure 1. The DARE model in Tabular and factor-graph notation. The \nmodel is implemented in annotations to the three main tables Participants, Questions, and Responses. \nTables QuestionsTrain and ResponsesTrain provide a mechanism for missing data. 6. Case Study: Intelligence \nTesting Tabular has been designed to make the paradigm of model-based machine learning (Bishop 2013) \nusable for practitioners who are not machine learning experts. We describe a case study of data analysis \nusing Tabular based on a dataset from intelligence testing. Our case study relies on models .rst published \nby Bachrach et al. (2012) and data provided by the Cambridge Psychometrics Centre, based on testing material \nby Pearson Assessment. We use a dataset of responses to a standard multiple-choice intelligence test \ncalled Raven s Standard Progressive Matrices (SPM). The test consists of sixty questions, each comprising \na matrix of shapes with one element missing and eight possible answers, exactly one of which is correct. \nThe sample consists of 121 subjects who .lled SPM for its standardization in the British market in 2006. \nThe factor graph for the full Dif.culty-Ability-Response (DARE) model is shown in Figure 1. Responses \nand true answers may or may not be observed. Figure 1 also depicts the full DARE model in Tabular. Each \npar\u00adticipant is characterized by a latent Ability. Each question is char\u00adacterized by a (true) Answer, \na Di.culty and a Discrimination parameter. Responses depend on ParticipantID and QuestionID. Under the \nmodel, an Advantage variable is calculated as the differ\u00adence between ability of participant and dif.culty \nof question. The Boolean variable Know, which represents whether the participant knows the answer or \nnot, is modelled as a probit over Advantage with Discrimination as the dispersion parameter. DB returns \nits .rst argument, and is a pragma to the underlying inference algo\u00adrithm, to apply a damping factor \nfor better convergence. Guess rep\u00adresents a random guess from a uniform distribution over all possible \nresponses. The participant s Response is taken to be the question Answer if Know is true and Guess otherwise. \nThe model relies on two sources of observed data: correct answers to the questions and responses provided \nby students. A subset of correct answers can be provided through the table QuestionsTrain. A subset of \ngiven responses can be provided through the table ResponsesTrain. Note that there are simpli.ed versions \nof the full DARE model in which a) only the student s ability is modelled (A model) or b) the students \nabilities and the questions dif.culties are modelled (DA model). The model is run once to answer two \ntypes of queries given a subset of the true answers and a subset of given responses: i) Infer the missing \ncorrect answers to questions and ii) Infer the missing responses of students. Figure 2 shows how the \nTabular implementation differs from the Infer.NET implementation on a sample run where 30% of re\u00adsponses \nand 30% of true answers are unobserved. The data con\u00adtained 121 participants, 60 questions, 41 training \nquestions, 7260 responses and 5082 training responses. The inference results of In-fer.NET and Tabular \nbased implementations are very similar. They differ slightly because of differences in the way our compiler \ntrans\u00adlated the Tabular formulation into Infer.NET code from the direct implementation by an expert. \nHowever, the Infer.NET code includ\u00ading the necessary data transformation code is much longer than the \nsuccinct and readable Tabular code that was added to the exist\u00ading data schema to describe the same model. \nTabular s excessively high compilation times are not due to the Tabular to Fun transla\u00adtion, which takes \nless than one second for each model, but to a .aw in the Fun compiler: Fun inlines all data before compiling, \na convenient but unnecessary measure avoided by Infer.NET. To demonstrate that the excessive compile \ntimes can be reduced, we prototyped a second compiler, Tabular II, that translates Tabular programs directly \nto Infer.NET. On the DARE case study Tabular II improves compile times by two orders of magnitude, and \ninfer\u00adence time by up to one order of magnitude, yielding performance that is more competitive with handwritten \nInfer.NET (Figure 2). 7. Query-by-Missing-Value Inference of latent columns requires that all output \ncolumns con\u00adtain a valid value at each row. However, many real datasets contain missing values. Query-by-missing-value \ninfers the posterior proba\u00adbility of missing values in output columns, conditioned on observed values \nactually present in the database. In a missing-values query, each attribute is either known, or missing; \nwe use ? to denote miss\u00ading values. Query-by-Missing-Value Database: d? V ? ::= ? | V missing or known \nvalue V ? V ? r? ::= {c1 = 1 , . . . , cn = } query-by-missing-value row n ? ? R? ::= [r0;. . . ;r] \nquery-by-missing-value table n d? ::= {t1 = R? 1, . . . , tn = R? } query-by-missing-value database n \nLet a missing-values learner (dx,d? | S) be a learner where dx is a y normal value and dy ? is a query-by-missing-value \ndatabase. Such a learner can be queryable (as de.ned in Section 4.8), where we let G f ? : T for any \nT and G. The result of inference on a queryable missing-values learner is the joint posterior distribution \nfor all the ? entries in d?, in addition y to the latent columns and the parameters of each table. For \na formal  Model Language LOC Data LOC Model LOC Inference LOC total Compile seconds Infer seconds Model \nlog evidence Avg. (log) prob. test responses. Avg. (log) prob. test answers. A Tabular 0 17 0 17 126 \n10 -7499.74 (-1.432),0.239 (-3.435),0.032 A Tabular II 0.41 1.47 -7499.74 (-1.432),0.239 (-3.424),0.033 \nA Infer.NET 73 45 20 138 0.32 0.38 -7499.74 (-1.432),0.239 (-3.425),0.033 DA Tabular 0 18 0 18 145 11 \n-5932.80 (-1.118),0.327 (-0.699),0.497 DA Tabular II 0.40 1.54 -5933.52 (-1.118),0.327 (-0.739),0.478 \nDA Infer.NET 73 47 21 141 0.34 0.43 -5933.25 (-1.118),0.327 (-0.724),0.485 DARE Tabular 0 19 0 19 163 \n16 -5823.01 (-1.119),0.327 (-0.551),0.576 DARE Tabular II 0.42 6.46 -5820.40 (-1.119),0.327 (-0.528),0.590 \nDARE Infer.NET 73 49 22 144 0.37 2.8 -5820.40 (-1.119),0.327 (-0.528),0.590 Figure 2. Comparison of \nTabular and Infer.NET implementations of different variants of the DARE model for multiple-choice questionnaires \n(machine con.guration: DELL Precision T3600, Intel(R) Xeon(R) CPU E5-1620 with 16GB RAM, Windows 8 Enterprise \nand .NET 4.0). de.nition, we need to compute the observations of d?, that is, the y entries in d? present \nin the database and their values. y Observations of a missing-values query: OE (\u00b7) OE (?) true OE (V \n) E = V OE ({ci = Vi ?}i.1..n)i.1..n OE.ci (Vi ?) i ?]i.0..n)? OE ([ri.0..n OE[i](ri ) OE ({ti = R? \ni }i.1..n)(R? i.1..n OE.ci i )  far from what is predicted by the model. An Inferno spreadsheet can \nbe considered as a queryable learner, where each spreadsheet column is an output but may have missing \nvalues, and there is an additional latent column for each row. The Tabular schema below corresponds to \nthe Generalized Gaussian model produced by Inferno on a three-column table. We here consider only real-valued \ncolumns; other data types such as Booleans and integers can also be encoded as (vectors of) real num\u00adbers \nwith appropriate (probabilistically invertible) link functions. (The library model CVectorGaussian is \nakin to CGaussian, but outputs vectors from a multivariate Gaussian distribution with Gaussian and Wishart \npriors.) The query is a table GG containing the spreadsheet data, with empty cells replaced by ?, such \nas the following. GG ID X0 X1 X2 0 1.0 2.1 2.9 1 2.1 ? 6.3 2 ? 2.7 3.5  Here Oy(GG) = y[0].X0 = 1.0 \n. y[0].X1 = 2.1 . \u00b7\u00b7\u00b7 . y[2].X1 = 2.7 . y[2].X2 = 3.5. Translating Query-by-Missing-Value to Query-by-Latent-Column \nMissing-values queries can be answered by translating them to a latent column query and performing inference \non the latter. The key idea is to create a new table for each output column of the original table, that \ncontains just the known values in that column. In the translation of the orginal table, each output column \nis simply turned into a latent column. For example, the Inferno GG model translates to the following \ntables. Above, the query tables (X0, X1, and X2) each contain a value col\u00adumn V and a reference column \nR, which denotes the row from which the value came. Since the GG table contains no input columns, all \nthe data is in the query tables. X0 X1 X2 ID R V R V R V 0 0 1.0 0 0 2.1 0 0 2.9 1 1 2.1 1 2 2.7 1 1 \n6.3  2 2 3.5 Formal Translation We .x a queryable missing-values learner (dx,d? | S) where S = (t j \n. Tj)j.1..m and each table Tj = y (c ji . A ji : Tji )i.1..nj . Let Oj = {i . 1.. j | A ji = O( )}. We \nlet [[Observed(M)]] Latent(M), and [[A]] A otherwise. We then translate the schema Sas follows. Tji (R \n. Input : int, V . Observed((R : link(t j)).cji ) : Tji ) if i . Oj T1 j (c ji . [[A ji ]] : Tji )i.1..nj \nS1 (t j . T1 j,(t ji . Tji )i.Oj ) j.1..m To translate the database, we .rst translate the observations \nin dy ? . Rxji [{R = k} | dy ? .t j[k].c ji = ?]k.0..|dy ? .t j |-1 Ryji [{V = dy ? .t j[k].ci} | dy \n? .t j[k].c ji = ?]k.0..|dy ? .t j |-1 .  The translations of the original tables have no observed values. \nRyj [{}]k.0..|dx .t j |-1 Finally, we can combine these tables into a new database dx 1 ,dy 1 . d1 {t \nj . dx.t j;{t ji . Rxji }i.Oj }j.1..m x d1 {t j . Ryj; {t ji . Ryji }i.Oj }j.1..m y Lemma 3. If (dx,d? \n| S) is a queryable missing-values learner, y then (dx 1 ,d1 | S1) as de.ned above is a queryable learner. \ny To answer the missing-values query using the results of infer\u00adence for the translated learner, we need \nto go from an inferred dis\u00adtribution for the translated schema S1 to a distribution for the origi\u00adnal \nschema S. This is done by the function I de.ned below. I(w, ( ,z)) = ({t j = w.tj}j.1..m , ({t j = [{c \nji = z.t j[k].cji }i.Oj ]k.0..|dy ? .tj |-1}j.1..m , {t j = [{c ji = z.tj[k].c ji }i.Lj ]k.0..|dy ? .tj \n|-1}j.1..m)). We can now show that the translation is correct: it reduces query-by-missing-value to query-by-latent-column. \nTheorem 3. Let L = (dx,d? | S) be a queryable missing-values y learner. Let L1 = (dx 1 ,d1 | S1) as de.ned \nabove, and let \u00b5 be the se\u00ad y mantics of the latent column query on L1 as given in Theorem 2. Then I-1\u00b5 \nis a version of the joint posterior conditional distribu\u00ad r tion PE w,yz | Ofst yz(dy ?) of L. Proof: \n(sketch) The compilation merely adds deterministic data and copies of random variables, which are then \nignored by I. As an optimization, an implementation might only translate ob\u00adserved columns where some \ndata is missing in the current database into new tables. In the example above, there are no missing values \nin column X2 in the database, so it can remain observed in GG1 , and no new table needs to be created \nfor its contents. User/Movie/Rating Recommender Recall the User/Movie/Rat\u00ading Schema of Section 3.2. \nGiven existing tables of users, movies, and ratings, suppose we wish to recommend to user i movies that \nthey are likely to rate with .ve stars. To do so, we .rst modify the annotation on the movie column of \nthe Rating table, adding a uni\u00adform per-row prior distribution. Rating u link(User) input m link(Movie) \noutput DiscreteUniform(SizeOf(Movie)) Score int output CDiscrete(N=5)[u.z,m.z] We then add a single \nrow {u = i; m = ?; Score = 5} to the existing data in the Rating table, denoting that user i has rated \nan unknown movie with 5 stars. This missing-values query is then translated to a corresponding latent \ncolumn query in the manner de.ned above. Inference returns a discrete distribution over movie IDs for \nthe missing value. Finally, high probability IDs can be selected for recommendation to the user. In a \nvariation of this query, we can weight the results by how many people have seen (that is, rated) each \nmovie. To this end, we add interdependence between rows (a shared frequency prior) by instead using the \nmodel CDiscrete(N=SizeOf(Movie)) for the movie column, and then proceed as above. 8. Related work Probabilistic \nProgramming Languages There is by now a num\u00adber of probabilistic programming languages, that differ in \ntheir target audience, expressive power, performance, and philosophy. BUGS (Bayesian Inference using \nGibbs sampling) (Gilks et al. 1994) is a simple language for specifying probabilistic models that allows \nfor inference using Gibbs sampling. It is widely used in the Bayesian community, but so far does not \nscale to large datasets. Microsoft Research s Infer.NET (Minka et al. 2012) achieves better scalabililty \nthrough support of deterministic approximate inference algorithms such as expectation propagation and \nvariational mes\u00adsage passing. Church (Goodman et al. 2008) is a relatively new probabilistic programming \nlanguage based on Lisp, which allows for recursion and enables non-parametric Bayesian models through \nmemoization. Furthermore, there are languages like IBAL (Pfef\u00adfer 2007) and Figaro (Pfeffer 2009), which \nincorporate decision\u00adtheoretic concepts as well. FACTORIE (McCallum et al. 2009) is an imperative framework \nfor constructing graphical models in the form of factor graphs, used mostly for information extraction. \nAll these languages follow the traditional paradigm of separating the code from the data schema and hence \nmake it necessary to repli\u00adcate the data schema within the language and to import the data from a database. \nOn the other hand, Tabular is focused on learn\u00ading from relational data, and does not directly address \nsome of the emerging application areas of probabilistic programming such as vision as inverse graphics \n(Mansinghka et al. 2013; Wingate et al. 2011), or decision making for security (Mardziel et al. 2011). \nProbabilistic Databases Probabilistic databases represent a line of research in which the database community \nis concerned with the question of how to handle uncertain knowledge in relational databases (see, for \nexample, Dalvi et al. (2009)). Typically, the as\u00adsumption is made that each tuple is only in the database \nwith a given probability, and that the presence of different tuples are indepen\u00addent events. The resulting \nprobabilistic database can be interpreted in terms of the possible worlds semantics. It is further assumed \nthat the probability values associated with each tuple are provided by the data collector, for example, \nfrom knowledge about measur\u00ading errors or from probabilistic models outside the probabilistic database. \nThe main technical dif.culty is to evaluate queries against probabilistic databases because despite the \nsimplistic independence assumption on the presence of tuples, complex queries involving logical and aggregation \noperators can lead to dif.cult inference problems. This is also the main difference to the Tabular approach: \nwhereas probabilistic databases work with concrete probabilities, Tabular works with non-probabilistic \ndatabase schemas containing simple tuples (possibly with missing values) and allows building probabilistic \nmodels based on that data. In contrast to probabilistic database systems Tabular is thus compatible with \nthe vast majority of existing relational datasets. Statistical Relational Learning Statistical Relational \nLearning operates in domains that exhibit both uncertainty and relational structure (see Getoor and Taskar \n(2007) for an excellent overview). Several contributions focus on combining probability and .rst\u00adorder \nlogic, such as Bayesian Logic (BLOG) (Milch et al. 2005) which allows reasoning about unknown objects \nor Bayesstore (Wang et al. 2008), which bridges the world of probabilistic databases and statistical \nrelational learning. Tabular is more closely related to work that makes direct use of data in a relational \ndatabase schema such as Getoor et al. (2007), Heckerman et al. (2007), and Neville and Jensen (2007). \nTabular is based on directed graphi\u00adcal models, distinguishing it from Markov Logic (Domingos and Richardson \n2004). Tabular was also inspired by a concept called PQL (Van Gael 2011) which augments the SQL query \nlanguage with statements that construct a factor graph aligned with a given database schema. In summary, \nTabular can be viewed as a language that enables the construction of statistical relational models directly \nfrom a schema, but goes beyond prior work in this .eld in that it allows the introduction of latent variables \nand models continuous as well as discrete variables.  Tabular was directly inspired by the question \nof .nding a tex\u00adtual notation for the factor graphs generated by InfernoDB (Singh and Graepel 2012) which \nconstructs a hierarchical mixture-based graphical model in Infer.NET (Minka et al. 2012) from an arbitrary \nrelational schema. CrossCat (Shafto et al. 2006) is a related model, which handles single tables with \nmixed types (real, integer, bool). With Tabular, these types of model can be implemented in a few lines \nof code, and we envisage the automatic synthesis of a Tabular program that best models a given relational \ndataset, similar to the work of Grosse et al. (2012) on matrix decompositions. 9. Conclusions We propose \nschema-driven probabilistic programming as a new principle of programming language design. The idea is \nto design a probabilistic modelling language by starting with a database schema and enriching it with \nnotations for describing random vari\u00adables, their probability distributions and interdependencies, how \nthey relate to data matching the schema, and what is to be inferred. Acknowledgments Conversations about \nthis work with Chris Bishop, Lucas Bordeaux, John Bronskill, Tom Minka, and John Winn were invaluable. \nMisha Aizatulin made many contributions to the Fun system on which this work depends. Marcin Szymczak \nand Danny Tarlow commented on a draft. We would like to thank John Rust and Michal Kosin\u00adski from the \nCambridge Psychometrics Centre as well as Pearson Assessments for providing the IQ dataset for research \npurposes. References Y. Bachrach, T. Graepel, T. Minka, and J. Guiver. How to grade a test without knowing \nthe answers -a Bayesian graphical model for adaptive crowdsourcing and aptitude testing. In Proc. ICML \n12, Omnipress 2012. S. Bhat, J. Borgstr \u00a8 Deriving prob\u00ad om, A. D. Gordon, and C. V. Russo. ability density \nfunctions from probabilistic functional programs. In Proc. TACAS 13, volume 7795 of LNCS, pages 508 522. \nSpringer, 2013. C. M. Bishop. Model-based machine learning. Philosophical Transactions of the Royal Society \nA: Mathematical, Physical and Engineering Sci\u00adences, 371(1984), 2013. J. Borgstr \u00a8om, A. D. Gordon, M. \nGreenberg, J. Margetson, and J. Van Gael. Measure transformer semantics for Bayesian machine learning. \nIn Proc. ESOP 11, volume 6602 of LNCS, pages 77 96. Springer, 2011. Download available at http://research.microsoft.com/fun. \nN. N. Dalvi, C. R\u00b4 e, and D. Suciu. Probabilistic databases: diamonds in the dirt. Commun. ACM, 52(7):86 \n94, 2009. P. Domingos and M. Richardson. Markov logic: A unifying framework for statistical relational \nlearning. In Proc. SRL2004, pages 49 54, 2004. L. Getoor and B. Taskar, editors. Introduction to Statistical \nRelational Learning. The MIT Press, 2007. L. Getoor, N. Friedman, D. Koller, A. Pfeffer, and B. Taskar. \nProbabilistic relational models. In Getoor and Taskar (2007). W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. \nA language and program for complex Bayesian modelling. The Statistician, 43:169 178, 1994. M. Giry. A \ncategorical approach to probability theory. In B. Banaschewski, editor, Categorical Aspects of Topology \nand Analysis, volume 915 of Lecture Notes in Mathematics, pages 68 85. Springer, 1982. N. Goodman, V. \nK. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church: a language for generative models. \nIn Proc. UAI 08, pages 220 229. AUAI Press, 2008. A. D. Gordon, M. Aizatulin, J. Borgstr \u00a8om, G. Claret, \nT. Graepel, A. Nori, S. Rajamani, and C. Russo. A model-learner pattern for Bayesian reasoning. In Proc. \nPOPL 13, pages 403 416, ACM Press, 2013a. A. D. Gordon, T. Graepel, N. Rolland, C. Russo, J. Borgstr \n\u00a8om, and J. Guiver. Tabular: A schema-driven probabilistic programming lan\u00adguage. Technical Report MSR-TR-2013-118, \nMicrosoft Research, 2013b. R. Grosse, R. Salakhutdinov, W. T. Freeman, and J. B. Tenenbaum. Ex\u00adploiting \ncompositionality to explore a large space of model structures. In Proc. UAI 12, pages 306 315. AUAI Press, \n2012. P. Hanrahan. Analytic database technologies for a new kind of user: the data enthusiast. In Proc. \nSIGMOD 12, pages 577 578. ACM, 2012. D. Heckerman, C. Meek, and D. Koller. Probabilistic Entity-Relationship \nModels, PRMs, and Plate Models. In Getoor and Taskar (2007). R. Herbrich, T. Minka, and T. Graepel. Trueskilltm: \nA Bayesian skill rating system. In Proc. NIPS 06, pages 569 576, MIT Press, 2007. M. Izbicki. Algebraic \nclassi.ers: a generic approach to fast cross-validation, online training, and parallel training. In Proc. \nICML 2013, JMLR W&#38;CP 28(3):648-656, 2013. O. Kiselyov and C. Shan. Embedded probabilistic programming. \nIn Proc. DSL 09, volume 5658 of LNCS, pages 360 384. Springer, 2009. D. Koller and N. Friedman. Probabilistic \nGraphical Models. The MIT Press, 2009. V. K. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. B. Tenenbaum. \nAp\u00adproximate Bayesian image interpretation using generative probabilistic graphics programs. To appear \nin Proc. NIPS 13. Available at http: //arxiv.org/abs/1307.0060, 2013. P. Mardziel, S. Magill, M. Hicks, \nand M. Srivatsa. Dynamic enforcement of knowledge-based security policies. In Proc. CSF 11, pages 114 \n128. IEEE Computer, 2011. A. McCallum, K. Schultz, and S. Singh. Factorie: Probabilistic program\u00adming \nvia imperatively de.ned factor graphs. In Proc. NIPS 09, pages 1249 1257. Curran Associates, 2009. B. \nMilch, B. Marthi, S. J. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic models with \nunknown objects. In Proc. Probabilis\u00adtic, Logical and Relational Learning A Further Synthesis, 2005. \nT. Minka and J. M. Winn. Gates. In Proc. NIPS 08, pages 1073 1080. MIT Press, 2008. T. Minka, J. Winn, \nJ. Guiver, and D. Knowles. Infer.NET 2.5, 2012. Microsoft Research Cambridge. http://research.microsoft.com/infernet. \nJ. Neville and D. Jensen. Relational dependency networks. Journal of Machine Learning Research, 8(8):653 \n692, 2007. K. Nowicki and T. A. B. Snijders. Estimation and prediction for stochastic blockstructures. \nJ. Amer. Statist. Assoc., 96:1077 1087, 2001. A. Pfeffer. The design and implementation of IBAL: A general-purpose \nprobabilistic language. In Getoor and Taskar (2007). A. Pfeffer. Figaro: An object-oriented probabilistic \nprogramming language. Technical report, Charles River Analytics, 2009. N. Ramsey and A. Pfeffer. Stochastic \nlambda calculus and monads of probability distributions. In Proc. POPL 02, pages 154 165. ACM, 2002. \nP. Shafto, C. Kemp, V. Mansinghka, M. Gordon, and J. B. Tenenbaum. Learning cross-cutting systems of \ncategories. In Proc. CogSci 06, pages 2146 2151. Cognitive Science Society, 2006. S. Singh and T. Graepel. \nCompiling relational database schemata into probabilistic graphical models. CoRR, abs/1212.0967, 2012. \nJ. Van Gael. PQL probabilistic query language. Blog post available at http://jvangael.github.io/2011/05/12/ \npqla-probabilistic-query-language/, May 2011. D. Z. Wang, E. Michelakis, M. Garofalakis, and J. M. Hellerstein. \nBayesstore: managing large, uncertain data repositories with probabilis\u00adtic graphical models. Proc. VLDB \nEndow., 1(1):340 351, Aug. 2008. D. Wingate, N. D. Goodman, A. Stuhlm \u00a8uller, and J. M. Siskind. Nonstan\u00addard \ninterpretations of probabilistic programs for ef.cient inference. In Proc. NIPS 11, pages 1152 1160, \n2011.   \n\t\t\t", "proc_id": "2535838", "abstract": "<p>We propose a new kind of probabilistic programming language for machine learning. We write programs simply by annotating existing relational schemas with probabilistic model expressions. We describe a detailed design of our language, Tabular, complete with formal semantics and type system. A rich series of examples illustrates the expressiveness of Tabular. We report an implementation, and show evidence of the succinctness of our notation relative to current best practice. Finally, we describe and verify a transformation of Tabular schemas so as to predict missing values in a concrete database. The ability to query for missing values provides a uniform interface to a wide variety of tasks, including classification, clustering, recommendation, and ranking.</p>", "authors": [{"name": "Andrew D. Gordon", "author_profile_id": "81100037731", "affiliation": "Microsoft Research and University of Edinburgh, Cambridge, United Kingdom", "person_id": "P4383833", "email_address": "adg@microsoft.com", "orcid_id": ""}, {"name": "Thore Graepel", "author_profile_id": "81100450764", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383834", "email_address": "thore.graepel@microsoft.com", "orcid_id": ""}, {"name": "Nicolas Rolland", "author_profile_id": "86159198957", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383835", "email_address": "a-niroll@microsoft.com", "orcid_id": ""}, {"name": "Claudio Russo", "author_profile_id": "81100638789", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383836", "email_address": "crusso@microsoft.com", "orcid_id": ""}, {"name": "Johannes Borgstrom", "author_profile_id": "81100636571", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P4383837", "email_address": "johannes.borgstrom@it.uu.se", "orcid_id": ""}, {"name": "John Guiver", "author_profile_id": "81435592336", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P4383838", "email_address": "joguiver@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535850", "year": "2014", "article_id": "2535850", "conference": "POPL", "title": "Tabular: a schema-driven probabilistic programming language", "url": "http://dl.acm.org/citation.cfm?id=2535850"}