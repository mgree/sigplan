{"article_publication_date": "01-08-2014", "fulltext": "\n Symbolic Optimization with SMT Solvers Yi Li Aws Albarghouthi Zachary Kincaid University of Toronto \nUniversity of Toronto University of Toronto liyi@cs.toronto.edu aws@cs.toronto.edu zkincaid@cs.toronto.edu \n Arie Gur.nkel Marsha Chechik Software Engineering Institute, CMU University of Toronto arie@cmu.edu \nchechik@cs.toronto.edu Abstract The rise in ef.ciency of Satis.ability Modulo Theories (SMT) solvers \nhas created numerous uses for them in software veri.cation, program synthesis, functional programming, \nre.nement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments \n(e.g., a witness for a bug) or proving un\u00adsatis.ability/validity (e.g., proving that a subtyping relation \nholds). We are often interested in .nding not just an arbitrary satisfying assignment, but one that optimizes \n(minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions \nthat maximize energy usage (performance bugs), or syn\u00adthesizing short programs that do not make expensive \nAPI calls. Un\u00adfortunately, none of the available SMT solvers offer such optimiza\u00adtion capabilities. In \nthis paper, we present SY MBA, an ef.cient SMT-based op\u00adtimization algorithm for objective functions \nin the theory of linear real arithmetic (LRA). Given a formula . and an objective function t, SY M BA \n.nds a satisfying assignment of . that maximizes the value of t. SY M BA utilizes ef.cient SMT solvers \nas black boxes. As a result, it is easy to implement and it directly bene.ts from future advances in \nSMT solvers. Moreover, SYM BA can optimize a set of objective functions, reusing information between \nthem to speed up the analysis. We have implemented SY M BA and evaluated it on a large number of optimization \nbenchmarks drawn from pro\u00adgram analysis tasks. Our results indicate the power and ef.ciency of SY M BA \nin comparison with competing approaches, and highlight the importance of its multi-objective-function \nfeature. Categories and Subject Descriptors G.1.6 [Optimization]: Con\u00adstrained optimization; F.3.1 [Specifying \nand Verifying and Rea\u00adsoning about Programs]: Invariants Keywords optimization; satis.ability modulo \ntheories; invariant generation; symbolic abstraction; program analysis Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. Copyrights for components of this work owned by others than ACM \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions from \npermissions@acm.org. POPL 14, January 22 24, 2014, San Diego, CA, USA. 1. Introduction Over the past \ndecade or so, we have witnessed an incredible im\u00adprovement in the performance of Satis.ability Modulo \nTheories (SMT) solvers [10] and the range of logical theories they support. These advances made SMT solvers \n(e.g., Z3 [22], MathSAT [18], CVC [9], etc.) household names in the programming languages and veri.cation \ncommunities, creating an explosion in the range of ap\u00adplications in which they are deployed, and paving \nthe way for in\u00adnovations that would not have been possible otherwise. To mention a few, in veri.cation, \nSMT solvers have been used for device driver veri.cation [7, 35], checking complex veri.ca\u00ad tion conditions \n[23, 39, 49], and improving precision of invariant generation [34, 46]; in testing and bug .nding, they \nhave been in\u00adstrumental in making symbolic execution [15, 27], fuzzing [28], and bounded model checking \ntechniques [19, 25] practical; in pro\u00adgram synthesis, they have been used to search for programs satisfy\u00ading \na given speci.cation [31, 59]; in functional programming, they have been used to support strong typing \nguarantees with re.nement types [13, 53]. In all of the aforementioned applications, SMT solvers are \nused for (1) generating satisfying assignments (e.g., a witness for a bug) or (2) proving unsatis.ability/validity \n(e.g., proving that a subtyp\u00ading relation holds). To the best of our knowledge, none of the avail\u00adable \nSMT solvers support .nding optimal satisfying assignments, i.e., satisfying assignments that minimize \n(or maximize) a given objective function. In this paper, we present SY MBA, an ef.cient SMT-based optimization \nalgorithm for objective functions in the theory of linear real arithmetic (LRA). Given a formula . and \nan objective function t, e.g., x + y, SY M BA computes the smallest constant k such that . . t . k 1. \nSpeci.cally, SYMBA .nds a satisfying assignment of . that exhibits the least upper bound k (maximum value) \nof an objective function t. In what follows, we start by arguing that such an algorithm has a wide array \nof appli\u00adcations in veri.cation, bug .nding, synthesis, and others. We then highlight SY M BA s salient \nfeatures and describe its high-level op\u00aderation. Applications of Optimization We start by describing \npotential applications of SY MBA in the following domains: Numerical invariant generation: Numerical \nabstract domains, e.g., intervals [20] and octagons [42], are often used to generate numerical program \ninvariants. The main ingredient in such an analysis is the abstract transformer: an operator that takes \na set of initial states and an instruction (or a basic block), and com\u00ad chttp://dx.doi.org/10.1145/2535838.2535857 \n1 Note that k is 8 if t is unbounded in ., and -8 if . is unsatis.able Copyright &#38;#169; 2014 ACM \n978-1-4503-2544-8/14/01. . . $15.00.  putes a set of reachable states, often using convex optimization \ntechniques. Since SY M BA can optimize over arbitrary LRA for\u00admulas (non-convex optimization), it can \nprecisely compute ab\u00adstract transformers over loop-free program fragments (encoded as formulas), without \nlosing precision due to join or multiple applications of the transformer. This problem is known in the \nliterature as symbolic abstraction and has been studied for a number of domains [45, 46, 52, 60, 61]. \nBy designing optimiza\u00ad tion algorithms that exploit the power of SMT solvers, we en\u00adable ef.cient implementations \nof precise abstract transformers for a variety of numerical domains [55]. We discuss this appli\u00ad cation \nin more detail in Sec. 4, where we generate our bench\u00ad mark suite from symbolic abstraction queries made \nby a pro\u00adgram analysis tool [5]. A similar approach can be used for solving systems of Horn-like clauses \nin LRA [30, 32], which capture a large number of se\u00ad quential and concurrent program veri.cation tasks. \nSince Horn clauses are represented symbolically in SMT-LIB [14], apply\u00ad ing traditional numerical abstract \ndomains for solving them is not a straightforward endeavour, but SY MBA can be easily used to build a \n.xpoint solver for such clauses. Counterexample generation: In symbolic execution and bounded model \nchecking, program executions are encoded succinctly as a formula. An SMT solver is then used to .nd a \nsatisfying as\u00adsignment of this formula that acts as a witness of an erroneous execution. By augmenting \nthe encoding with arithmetic cost functions, we can use SYM BA to .nd counterexamples that maximize or \nminimize certain criteria. For example, we might be interested in .nding performance bugs, e.g., execution \ntraces with the highest energy or memory consumption. By assigning costs to program instructions and \nAPI calls, we can detect such executions using an optimizing SMT solver.  Program synthesis: Program \nsynthesis involves generating a program that satis.es a given speci.cation. For example, in [31, 37], \nnon-trivial bit-manipulating program snippets are synthesized from speci.cations by using SMT solvers \nto search through all possible combinations of bit-level operations (in\u00adstructions). Similar to the counterexample \ngeneration described above, the goal is often to synthesize the shortest programs, or ones with the smallest \ncost. For example, when the synthesized snippet is part of a performance-critical code (e.g., as in su\u00adperoptimization \n[41]), the size of the synthesized snippet and the operations it performs are crucial. By augmenting \nformu\u00adlas given to the SMT solver with costs, we can instruct it to synthesize programs that minimize \na given criterion.  Constraint programming: In recent work [38], K \u00a8oksal et al. proposed incorporating \nan SMT solver into an extension of Scala, allowing constraint manipulation as part of the language. One \nof the important constructs in their language is min/max, which returns the minimum/maximum satisfying \nassignment of a constraint w.r.t to an objective function, and enables ele\u00adgant implementations of algorithms \nfor problems like knapsack. Due to the unavailability of off-the-shelf solvers with an opti\u00admization \nfeature, the authors use a simple binary search opti\u00admization algorithm restricted to bounded discrete \ndomains. The authors of [38] also comment on the state-of-the-art of SMT solvers by saying: ...we found \nthat a number of features, if natively supported by solvers, could directly bring bene.ts to constraint \nprogramming. These include 1) support for enumer\u00adation of theory models and 2) solving constraints while \nmini\u00admizing/maximizing a given term. SYM BA is thus an answer to point 2 for LRA terms (objective functions). \n Interpolant generation: Craig interpolation has proved to be a powerful technique for software veri.cation \nbased on predicate abstraction [36]. Interpolants are generated from unsatis.abil\u00ad ity proofs. For linear \nreal arithmetic, this can be performed by constructing a system of constraints whose solution is a proof \nof unsatis.ability (and an interpolant) [54]. As recently shown [4], the simplicity of the proof can \nbe crucial to discovering the predicates required for a safe inductive invariant. Finding sim\u00adple proofs \nboils down to .nding an optimal solution of the sys\u00adtem of constraints. Using SY MBA, this can be automated, \nwith\u00adout the need for the heuristics employed in [4]. Symbolic Optimization with SY M BA Given a formula \n. and a set of objective functions (objectives for short) T = {t1, . . . , tn}, SY M BA computes the \nstrongest formula optT (.) = t1 k1 . . . . . tn kn, such that . . optT (.), where ki . R . {8, -8}. We \ncall optT (.) the optimal solution of T w.r.t. .. In other words, SY M BA computes the least upper bound \n(maximum value) ki of each ob\u00adjective ti w.r.t. the satisfying assignments of .. 2 Note that we are optimizing \na set of objective functions, as op\u00adposed to a single function as in traditional optimization problems. \nThis allows SY M BA to reuse information computed for one objec\u00adtive in order to speed up the optimization \nof other objectives. This feature allows incremental implementations of SYM BA, where we can continuously \nask for optimizing different objectives without losing previously computed information (in a style similar \nto the push/pop interface implemented by most SMT solvers). The in\u00adcremental interface is useful, for \nexample, when computing an ab\u00adstract transformer for the intervals domain, where we need to .nd the maximum \nand minimum values of each live variable. SYMBA maintains both an under-and an over-approximation of \nthe optimal solution optT (.). It works by sampling points (models) in . in a systematic manner, using \nan SMT solver, and adding the points to the under-approximation in order to extend it. The process continues \nuntil the under-approximation is equal to the optimal solution. The key insight underlying SY M BA is \nhow to carry out the sampling process in an in.nite space of satisfying assignments, while ensuring convergence \nto least upper bounds and discovery of unbounded objectives. SYMBA also maintains an over-approximation \nof optT (.) (when it terminates, the two approximations are equivalent). Main\u00adtaining an over-approximation \nallows us to halt SYM BA at any point and still compute an over-approximation of the optimal solu\u00adtion \n(e.g., an over-approximation of the best abstract transformer). This makes SYM BA resilient to SMT solver \nfailures and resource (e.g., time) depletion. Another important feature of SY MBA is that it treats the \nunder\u00adlying SMT solver as a black-box, using it to generate models and check validity. This makes SY \nM BA easy to implement, allowing us to take advantage of the existing highly-optimized SMT solvers without \nhaving to dive into their intricate implementations, and di\u00adrectly bene.ting from future advances in \nSMT solving. Our implementation of SY M BA uses the Z3 SMT solver. We have performed a thorough evaluation \nof SY MBA on a large set of realistic benchmarks drawn from program analysis tasks. We have also compared \nSY M BA against [57], which is, to the best of our knowledge, the only other proposed SMT-based optimization \ntech\u00adnique. The technique of [57] optimizes a single objective function, and is built into the MATHSAT \nSMT solver. For a fairer compari\u00ad 2 Note that our goal is to .nd an optimal value for each objective \nindepen\u00addently (using a different satisfying assignment of .), as opposed to a pareto\u00adoptimal satisfying \nassignment, i.e., a single assignment that optimizes all objective functions and cannot be improved upon. \n son against SYMBA, we have also implemented a multi-objective\u00adfunction extension of the approach in \n[57] in the Z3 SMT solver (using its available source code [3]) and experimented with various con.gurations. \nOur results demonstrate the ef.ciency and robust\u00adness of SYM BA against competing approaches, and highlight \nthe effectiveness of its multi-objective-function feature and our pro\u00adposed implementation optimizations. \nContributions We summarize our contributions as follows: SY M BA: a novel SMT-based optimization algorithm \nfor objec\u00adtive functions in linear real arithmetic, with wide-ranging appli\u00adcations in program analysis, \nsynthesis, etc. SY M BA utilizes ef.\u00adcient SMT solvers as black boxes. Thus, it is easy to implement \nand it directly bene.ts from future advances in SMT solving. Moreover, SYMBA can optimize a set of objective \nfunctions, reusing information between them to speed up the optimization task.  An extensive evaluation \nof SY M BA against other proposed tech\u00adniques in the literature on a large set of program analysis bench\u00admarks. \nOur results indicate the power and ef.ciency of SY M BA in comparison with competing approaches, and \nhighlight the importance of its multi-objective-function feature.  An implementation of SY MBA, multiple \nimplementations of the approach in [57], and a large set of optimization benchmarks drawn from program \nanalysis tasks. Our source code, binaries, and benchmarks are available at:  http://bitbucket.org/arieg/ufo \n Organization In Sec. 2, we demonstrate the operation of SY M BA on simple examples. In Sec. 3, we formalize \nSY M BA and prove its correctness. In Sec. 4, we describe our implementation and experimental evaluation. \nIn Sec. 5 and 6, we compare SYM BA to related work and conclude the paper, respectively. 2. SY M BA by \nExample In this section, we illustrate the operation of SY M BA on two for\u00admulas, a 2-dimensional and \na 3-dimensional one. 2.1 A 2-dimensional Example Consider the LRA formula . = 1 y 3 . (1 x 3 . x ; 4) \ncontaining the real variables x and y and represented pictorially by the black boxes in Fig. 1. Suppose \nthat our set of objectives is T = {y, x+y}. That is, we would like to .nd the least upper bound for y \nand x + y in .. (Note that if we want to .nd the minimum value for y, we can simply add -y as an objective \nto T .) In this example, the optimal solution is optT (.) = y 3 . x + y 8, since x + y is unbounded in \n.. Formulas of the form t 8 are treated as true, and t -8 as false. Initially, the under-approximation \nof optT (.) is U = y -8 . x + y -8 = false, and the over-approximation is O = y 8 . x + y 8 = true. SY \nM BA maintains the invariant U . optT (.) . O (note that U is an under approximation of optT (.), not \nnecessarily ., and sim\u00adilarly with O). SY M BA alternates between two main operations: GL O BA LPUSH, \nwhich is used to grow the under-approximation by sampling points (models) of . that lie outside the under\u00adapproximation; \nand UN B OUN D E D, which is used to detect un\u00adbounded objectives. In this example, 3 is the upper bound \nfor y, and x + y is unbounded. First GL O BAL PUSH SYM BA starts with a GLOBA L PU S H by querying an \nSMT solver for a model of . that is not a model of U (i.e., lies outside the under-approximation). Suppose \nthe solver returns the point p1 = (2, 2). The under-approximation U is the strongest formula of the form \nt1 k1 .. . . .tn kn that contains the set of points found by the solver (i.e., a convex hull expressed \nin terms of the objectives). So, the under-approximation is updated to U = y 2 . x + y 4 (since the maximum \nvalues of y and x + y seen so far are 2 and 4, respectively). This is shown as the shaded region U1 in \nFig. 1. UN B O U N D E D (p1, y) SY M BA now tries to prove that y is un\u00adbounded. First, we categorize \npoints into boundary classes as fol\u00adlows: Let E(.) = {l = k | l k . .}, i.e., E(.) is the set of all \natomic formulas appearing in . with the inequalities replaced by equalities. In our example, E(.) = {x \n= 1, x = 3, x = 4, y = 1, y = 3}. Informally, E(.) represents the set of edges (bound\u00adaries) appearing \nin Fig. 1. The boundary class [p] of a point p is {e . E (.) | p |= e}, i.e., the set of equalities in \nE(.) satis.ed by p. For p1, [p1] = \u00d8, since p1 does not lie on any of the boundaries. The intuition underlying \nUN BOU N D E D is .nding a ray r from a point p in . such that a given objective t is increasing along \nr, and r never hits any boundaries of . (i.e., completely contained in .). UN BO U N D E D queries the \nSMT solver for a point p\" s.t. [p\"] = [p1] and y(p1) < y(p\"), where y(p\") is the valuation of y at \" \npoint p\". The point p= (2, 2.5) satis.es this condition. Then UN B O UND ED queries for a point p\"\" s.t. \n[p\"] . [p\"\"] and y(p\") \"\"y(p). If no such p\"\" exists, then we know that y is unbounded. Intuitively, \nwe are asking whether we can keep increasing the value of y from p\" without touching a point p\"\" on one \nof the boundaries in E(.). In this case, such a p\"\" exists, so it is added to the under\u00adapproximation \nas p2 = (3, 3) in Fig. 1. Note that p2 exhibits the upper bound of y; SY M BA detects that and strengthens \nthe over\u00adapproximation O to y 3. After witnessing the point p2, U is updated to become y 3 .x + y 6 (see \nregion U2 in the .gure). Second GLO BAL PU S H Suppose that SY M BA calls GL O BAL -PU S H. The result \nis a point in . but not in U. Let p3 = (5, 3) be the point found by GL O BALPU S H. As a result, U is \nupdated to y 3 . x + y 8 (see region U3). UN B O U N D E D (p3, x + y) SY M BA now applies UNB O U NDE \nD to check if x+y is unbounded. Suppose UN BOU N D E D picks the point \" p3. First, it .nds a point p= \n(6, 3) which increases x + y and is \"\" in the same boundary class as p3. Then, it tries to .nd a point \npthat has a boundary class [p\"] . [p\"\"] and has a greater (or equal) valuation of x + y than p\". Since \nno such point p\"\" exists, SY M BA concludes that x + y is unbounded. Intuitively, SYMBA discovers that \nit is possible to keep .nding points along the boundary y = 3 that increase x + y without encountering \nany other boundary, thus concluding that x+y is unbounded. We formally specify and prove the correctness \nof UN BOU N D E D in Sec. 3. The under-approximation U is now updated to become y 3 (region U4), by dropping \nthe upper bound for x + y. At this point, U = O, so SY MBA terminates with the optimal solution y 3. \n 2.2 A 3-dimensional Example We now illustrate the operation of SY M BA on the formula . = 0 x 3 . 0 \nz 2 . (2y -x + 4 . 4y = 3x + 3), containing the variables x, y, and z and depicted in Fig. 2. Suppose, \nfor simplicity, that we would like to .nd the least upper bound only for y, i.e., T = {y}.  y y 1 2 \n3 0 1 2 p1 U1 3 4 p2 U2 5 6 p3 U3 7 8 U4 x Figure 1. Illustration of SYM BA on a 2-D example. First \nGLO BAL PU S H Similar to our previous example, SY M BA starts with U = false and O = true and uses GL \nO BALPU SH to .nd the initial point. Suppose the SMT solver returns the point p1 = (1, 0, 1) denoting \nvalues of (x, y, z). Thus, U = y 0. UNB O U N D ED (p1, y) To check if y is unbounded, SY M BA ap\u00adplies \nUN BOU N D E D starting from p1. Since it cannot prove that y is unbounded, it .nds the point p2 = (0, \n1, 1), where [p1] . [p2] and y(p1) < y(p2), i.e., a point showing that increasing the value of y from \np1 can hit a boundary. After applying UNB O U N DE D to p2, SY M BA can get the point p3, and then point \np4 (after apply\u00ading UNB O U NDE D to p3). As a result, U = y 2. From point p4, SY M BA cannot apply UN \nB OUN D ED, since there does not exist a point p \" where [p \" ] = [p4] that increases the value of y. \nIntuitively, p4 represents a local maximum. Second GL O BAL PUS H To escape the local maximum, SY M BA \nuses GL O BA LPUSH to query the SMT solver for a point outside U. In this case, it might .nd the point \np5 = (1.8, 2.1, 1), and thus U becomes y 2.1. UNB O U N D ED (p5, y) SY M BA continues trying to prove \nthat y is unbounded by performing UN BOU N D E D from p5, leading to p6 and then p7. SY M BA detects \nthat p7 represents the maximum value of y in . and terminates with the optimal solution y 3. We have \nillustrated the workings of SY M BA on two formu\u00adlas representing non-convex shapes, and showed how it \nutilizes an SMT solver to .nd least upper bounds and detect unboundedness of arbitrary linear expressions \n(objective functions). In the following sections, we describe SY M BA formally and discuss our implemen\u00adtation \nand experimental results. 3. SY M BA: The Symbolic Optimization Algorithm In this section, we provide \nde.nitions required for the rest of the paper and formalize SYM BA as a set of inference rules. 3.1 De.nitions \nFormulas Let L be a topologically-closed (i.e., all atoms are non\u00adstrict inequalities) subset of Quanti.er \nFree Linear Real Arithmetic (QF LRA), de.ned as follows: . . L ::= true | false | P . P \" | P . P \" P, \nP \" . Atoms ::= c1x1 + \u00b7 \u00b7 \u00b7 + cnxn k, n . N xi . Vars ::= {x1, . . . , xn}, where ci, k . R. We use \n[.] to denote the set of all satisfying assignments (models) of .. A model p : Vars . R of ., denoted \np |= z Figure 2. Illustration of SYMBA on a 3-D example. ., is a valuation of the variables of . such \nthat .(p) = true, where .(p) is . with every occurrence of a variable x replaced by p(x). Geometrically, \np is a point in Rn, and in what follows, we use the terms model and point to refer to p interchangeably. \nWe use Atoms(.) to denote the set of all Atoms appearing in ., and Vars(.) to denote the set of all Vars \nappearing in .. Optimal Solutions Let . be a formula in L. Let T = {t1, . . . , tn}be a set of linear \nexpressions, objective functions, where each ti is of the form c1x1 + \u00b7 \u00b7 \u00b7 + cmxm, where ci . R and \nVars(.) = {x1, . . . , xm}. The goal of SY M BA is to compute a vector (k1, . . . , kn), where each ki \n. R . {8, -8}, such that for each ti, . . ti ki and there does not exist ki \" < ki where . . ti ki \". \nWe say that (k1, . . . , kn) is the optimal solution of T w.r.t. ., and denote it as optT (.). We call \neach value ki the optimal value (or the least upper bound) of ti in .. Given such a vector V = (k1, . \n. . , kn), where n = |T |, we use . formT (V ) to denote the formula ti ki. Given a model i.[1,n] p of \n., we use p T to denote the vector (t1(p), . . . , t|T |(p)). Given two vectors V1 and V2 of equal length, \nwe use min(V1, V2) and max(V1, V2) to denote the pointwise minimum and maxi\u00admum of the two vectors, respectively. \nWe say V1 V2 if each element of V1 is less than or equal to its corresponding element in V2, or if there \nexists a -8 in V1. Intuitively, V1 V2 iff formT (V1) . formT (V2). Therefore, we say that V2 is weaker \nthan V1 if V1 < V2 (or V1 is stronger than V2). Combinations of Theories For clarity of presentation, \nwe restrict ourselves to applying SY MBA to formulas in L. It is important to note, however, that SY \nMBA is applicable to quanti.er-free formu\u00adlas over any combination of theories T . LRA., where T is an \narbitrary combination of theories, and LRA. is linear real arith\u00admetic restricted to non-strict inequalities. \nThe only restriction we require is that T and LRA. have disjoint signatures. In other words, atomic formulas \nshould be over T or LRA., exclusively. For example, T can be the combination of the theories of bitvec\u00adtors \nand arrays (perhaps for modelling program executions). The rest of our presentation can apply directly \nto SMT formulas over T . LRA. without any modi.cation.  3.2 SY M BA Formalized We now formalize the \nsymbolic optimization algorithm SY M BA as a set of inference rules shown in Fig. 3. Given a set of objectives \nT = {t1, . . . , tn} and a formula . in L, SY M BA computes optT (.). The state of SY M BA is a tuple \n(M, U, O), where M is a set of models of .; U is an under\u00adapproximation of optT (.) (i.e., U optT (.) \nis invariant); and O is an over-approximation of optT (.) (i.e., optT (.) O is  p |= . . \u00acformT (U) \nIN I T GL O BA L PU S H (\u00d8, (-8, . . . , -8), (8, . . . , 8)) (M , U, O) . (M . {p}, max(U, pT ), O) \nU = (k1, . . . , kn) p2 |= . [p2] = [p1] ti(p1) < ti(p2) .p3 |= . \u00b7 ti(p2) ti(p3) . [p2] . [p3] UN B \nO U N D E D(p1 . M , ti . T ) (M, U, O) . (M, max(U, (k1, . . . , ki-1, 8, ki+1, . . . , kn)), O) p2, \np3 |= . ti(p1) < ti(p2) ti(p3) [p1] = [p2] . [p3] UN B O U N D E D -FA I L(p1 . M, ti . T ) (M, U, \nO) . (M . {p3}, max(U, pT 3 ), O) O = (k1, . . . , kn) m = max{ti(p \" ) | p \" . M} . . ti m BO U N D \nE D(ti . T ) (M , U, O) . (M , U, min(O, (k1, . . . , ki-1, m, ki+1, . . . , kn))) Figure 3. Inference \nrules used by SY M BA. invariant). Note that for clarity of presentation, we treated optT (.), U, and \nO as formulas in Sec. 2, whereas here we treat them as vectors and use formT (V ) to convert a vector \nV to the formula it represents. When SYMBA terminates, we know that U = O = optT (.). Initially, as de.ned \nby the rule INI T, M = \u00d8, U = (-8, . . . , -8), and O = (8, . . . , 8). The rules GLO BAL PU S H, UN \nBO U N D E D, and UN BOU N D E D -FA IL are used to weaken U until it is equal to optT (.), whereas BO \nU N DE D strengthens O until it is equal to optT (.). GLOBA L PU S H .nds a model of . that is not captured \nby formT (U) (i.e., lies outside the under-approximation) and adds it to U to weaken it (using max). \nWhen the rule GL O BALPU SH no longer applies, we know that U = optT (.). Note that applying this rule \nalone does not guarantee that U eventually reaches optT (.) for two reasons: 1. Since we are dealing \nwith real variables, GL O BALPU SH might keep .nding models that approach the upper bound of one of the \nobjectives asymptotically. 2. GL O BA L PUS H cannot detect whether an objective is unbounded, so it \nwill keep .nding models that increase the value of the un\u00adbounded objective inde.nitely.  To that end, \nthe rules UNB O U NDED and UN B OU N D E D -FA IL are used to detect unbounded objectives and help GL \nO BA LPUSH avoid asymptotic behavior. UNB O U NDE D takes as parameters a model p1 . M and an objective \nti . T and attempts to prove that ti is unbounded as follows: First, it tries to .nd a point p2 |= . \nsuch that [p1] = [p2] and t(p1) < t(p2). Then, it looks for a point p3 such that p3 |= ., [p2] . [p3] \nand t(p2) t(p3). If no such p3 exists, then t is unbounded in .. Otherwise, UN B O U N D E D -FAI L adds \np3 to M. The intuition here is as follows: If we can .nd a model p2, then we know that t can increase \nalong the hyperplanes in E(.). If no point p3 exists, then we know that we can keep increasing t in\u00adde.nitely \nwithout encountering any of the boundaries in E(.) that are not in [p2], thus showing that t is unbounded. \nThis is analogous to the technique used by the simplex method for showing that a di\u00admension is unbounded \nin a convex polyhedron. We further discuss the intuition underlying UN B O UND ED and prove its correctness \nin Sec. 3.3. In addition to the aforementioned rules, the rule BOU N D E D de\u00adtects whether a model p \n. M exhibits the least upper bound of some objective t, and strengthens the over-approximation accord\u00adingly. \nNote that the over-approximation is not required for the cor\u00adrectness of SY M BA, but its availability \nallows us to guarantee that SY M BA maintains a sound approximation O of optT (.) at every point of its \nexecution. This makes SY M BA resilient to SMT solver failures and allows us to limit resource consumption \nwhen desired. That is, by prematurely terminating SY MBA during its execution, we can recover optimal \nvalues of some of the objective functions, as maintained by the over-approximation. Example We illustrate \nthe applications of the rules on the 2-D example from Sec. 2.1 and shown in Fig. 1. Assume that after \nthe initial call to GL OBA L PU S H, M = {p1 = (2, 2)}, formT (U) = y 2 . x + y 4, and formT (O) = true. \nApplying UN BOU N D E D -FA IL to p1 . M and y . T adds p2 = (3, 3) to M. Next, BOUN D E D is used to \ndetect that p2 exhibits the least upper bound of y, and updates O so that formT (O) = y 3. Assume that \nthe second application of GL OBA L PUS H adds point p3 = (5, 3) to M. Applying UN BOU N D E D(p3, x + \ny) detects that x + y is unbounded. At this point, formT (U) becomes y 3, making GL O BA LPU SH inapplicable. \nTherefore, . . formT (U), and U = O = optT (.). In what follows, we discuss and prove soundness of SYM \nBA, and de.ne terminating sequences of rule applications.  3.3 Soundness We start by showing soundness \nof the UN B O UND ED rule. A necessary and suf.cient condition for proving that a given objective t is \nunbounded within . is the existence of a convex polyhedron .c, e.g., a ray, such that t is unbounded \nin .c and .c . .. Our solution addresses two problems: 1. How to restrict the space from which .c is \ndrawn while main\u00adtaining completeness, i.e., ensuring that .c is found whenever t is unbounded in .. \n 2. How to check that .c . ..  The idea we use here is to restrict .c to formulas of the form E . t \n; k, where E . E (.) and k . R. This space of convex polyhedra is suf.cient for completeness. For instance, \nconsider the example from Fig. 1. To prove that the x+y direction is unbounded, we .nd a point p3 = (5, \n3) that lies on the boundary y = 3 . E (.) and ask whether .c = y = 3 . x + y ; 8 is contained in .. \nFurthermore, we perform the containment check implicitly by checking whether there is a point in .c, \nalong any direction that increases x + y, that intersects a boundary of .. In our running example, such \na point does not exist (see Fig. 1). Thus, x + y is unbounded. For another example, consider the point \np1 = (2, 2). Since p1 does not lie on any boundary, to check if x + y is unbounded we ask whether .c \n= x + y ; 4 is contained in . (i.e., we check whether increasing x + y in .c does not encounter boundaries \nin .). This is not the case, and the counterexample is the point p2, shown in Fig. 1, that lies on the \nboundaries x = 3 and y = 3.  Thm. 1 formalizes this construction using boundary classes and states its \ncorrectness for proving that an objective is unbounded in .. Theorem 1 (Soundness of UNB O U N DE D). \nGiven a formula . in L and a linear expression t over the variables of ., then k . R \u00b7 . . t k (i.e., \nt is unbounded) if and only if there exist p1, p2 |= . such that 1. t(p1) < t(p2) 2. [p1] = [p2] 3. \np3 |= . \u00b7 t(p2) t(p3) . [p2] . [p3]  Proof. Proofs are available in the appendix. In other words, if \nthe UN B OUN D ED rule was applied, then .c = [p2] . t ; t(p2) is contained in .. In the theorem, conditions \n1 and 2 imply that t is unbounded within .c, and condition 3 implies that increasing t in .c does not \nencounter any boundaries of ., i.e., [p2] . t ; t(p2) is subsumed by .. It follows from this theorem \nthat UN B O U N D E D maintains the invariant U optT (.), since the optimal solution cannot have a least \nupper bound for t if it is unbounded (i.e., the least upper bound of t is 8). Theorem 2 (Soundness of \nSY M BA). If GL O BA LPUSH does not apply, i.e., . . \u00acformT (U) . false, then U = optT (.). Proof. Follows \ntrivially from the invariant U optT (.).  3.4 Termination We now discuss suf.cient conditions for ensuring \ntermination of SY M BA. For simplicity of presentation, we assume that T contains a single objective \nt. We start by de.ning a fairness condition on the scheduling of SYMBA s rules that ensures termination. \nA fair scheduling is an in.nite sequence of actions a1, a2, . . ., where ai . {GL O BA LPUSH, UN BO U \nN D E D, UNB O U N DE D-FAI L}, and the following conditions apply: 1. GL O BA L PUS H appears in.nitely \noften, and 2. if a point p is added to M along the execution sequence, then both UN B O U N D E D(p, \nt) and UN B OUN D ED -FA I L(p, t) eventu\u00adally appear.  Condition 1 ensures that SY M BA does not get \nstuck in a lo\u00adcal maximum. Condition 2 ensures that we visit every local max\u00adimum by visiting every boundary \nclass, thus guaranteeing that ei\u00adther the least upper bound of t is found or it is proved unbounded. \nRecall the 3-D example from Sec. 2, where T = {y}. Suppose our execution only applies the GL OBA L PU \nS H rule. Then U might grow asymptotically towards the least upper bound of y, e.g., y 2, y 2.1, y 2.11, \netc., never reaching y 3. Condition 2 forces computing models that lie on one or more of the boundaries \nE(.), thus avoiding this asymptotic behaviour. But applying UN-BOU N D E D and UNB O U NDED-FA I L alone \nwithout applying GLO B -A L PU S H might get us stuck in local maxima. For example, on point p4 in Fig. \n2, UNB O U N DE D(-FA I L) are inapplicable. Condition 1 ensures that we eventually .nd a model outside \nthe current under\u00adapproximation (see p5), thus escaping the local maximum. A k-sequence for an objective \nt is a sequence of points p1, . . . , pk, where .i ; 1 \u00b7 pi |= . . ([pi] . [pi+1]) . t(pi) t(pi+1), and \nUN O U N D E D -FAI L(pk, t) fails to apply. For example, in Fig. 2, p1, p2, p3, p4 is a k-sequence. \nSince [pi] for a k-sequence strictly grows in size and the largest boundary class has size at most |Atoms(.)|, \na k-sequence is of length at most k = |Atoms(.)|. Lemma 1 states that the last model pk of a k-sequence \nalways exhibits the largest value of t in its boundary class [pk]. Lemma 1. Let . be a formula, and t \nbe an objective bounded in .. Then, in every execution of SYM BA, the last element pk in every k-sequence \nfor t satis.es t(pk) = max{t(p) | p |= . . [pk]}. Proof. According to the de.nition of a k-sequence, \nUN B O U N DE D -FA IL(pk, t) does not apply. Since t is bounded, premises of UN-BO U N D E D do not \nhold either. Combining premises of the two rules, there does not exist pk \" |= . . [pk] such that t(pk) \n< t(pk \" ). Thus, t(pk) = max{t(p) | p |= . . [pk]}. We are now ready to prove termination of any fair \nexecution of SY M BA. We assume that SY MBA terminates when GL O BA LPUSH is no longer applicable, i.e., \n. . formT (U). Theorem 3. SYM BA terminates after a .nite number of actions in any fair execution. Proof. \nWe split the proof into two cases as follows: Case 1: t is bounded within .. Suppose SYMBA is non\u00adterminating. \nThen, in any fair scheduling, in.nitely many GL O B -A L PU S H creates in.nitely many k-sequences. Following \nLemma 1, there are in.nitely many models p in the execution sequence such that p |= . and t(p) = max{t(p \n\" ) | p \" |= . . [p]}. We denote the set of such points by P . In any fair execution, GL O BA LPUSH must \nappear after p is added to M. Therefore, there exists a point p \" . P such that t(p) < t(p \" ). As a \nresult, there is a sequence of points p1, p2, . . . in P such t(p1) < t(p2) < t(p3) < \u00b7 \u00b7 \u00b7 . Hence, \n.i, j \u00b7 i j . [pi]= [pj ]. Since the number of boundary classes = is .nite, SY M BA eventually .nds the \nleast upper bound of t and terminates. Case 2: t is unbounded. Using the same argument as above, SY M \nBA eventually .nds a point in an unbounded boundary class (due to the .nite number of boundary classes) \nsuch that the three conditions in Thm. 1 hold. After that, GL O BALPU SH becomes inapplicable. 4. Implementation \nand Evaluation 4.1 Implementation We have implemented SY M BA in C++, using the Z3 SMT solver [22] for \nsatis.ability queries. Our implementation accepts a formula . and a set of objectives T written in the \nstandard SMT-LIB2 [8] format. It then computes the optimal solution optT (.) and returns the result. \nWe have made available the executable and benchmarks online. Detecting Unbounded Objectives Our implementation \nof UN-BO U N D E D and UN B OUN D E D -FA IL exploits the incremental (PU S H/POP) interface that most \nSMT solvers supply. Moreover, in\u00adstead of implementing the BO U N D E D rule explicitly, we show how \nto update the over-approximation for free, as a side effect of apply\u00ading the UNB O U NDED rules. Fig. \n4 shows the procedure UNB O U N DE DIM P L: our implemen\u00adtation of the UN B O U N D E D rules. We assume \nthat there is a global SMT context in which the formula . has been asserted. An active boundary class \nc and a objective ti are passed in as parameters. U[ti] and O[ti] refer to the i-th element of the vectors \nU and O, re\u00adspectively. SAT and UNSAT refer to the current state of the SMT context, and GE T MO D EL() \nreturns a model satisfying the current  1: function UN B O U N D IM P L(c . P (E(.)), ti . T ) 2: PU \nS H() 3: AS S E RT(ti > U [ti]) 4: if U NSAT then 5: O[ti] . U[ti]; PO P(); return 6: AS S E RT(c) 7: \nif U NSAT then 8: PO P(); return : 9: AS S E RT( (E(.) \\ c))) 10: if S AT then t UN B O U N D E D -FA \nI L 11: PO P(); return GE T MO D E L() 12: else t UN B O U N D E D 13: U[ti] . 8 14: PO P(); return Figure \n4. Implementation of UNB O UNDED(-FA IL). state of the context if one exists. PUS H() and POP() are used \nto store and restore the state of the context, respectively. We start by incrementally asserting the \nconditions of UN-BOU N D E D implicitly. Given c and ti, we know that there is a pre\u00adviously sampled \npoint p1 |= c such that ti(p1) U[ti]. First, in lines 3-8, we check if there exists a model p2 |= . such \nthat ti(p2) > ti(p1) and [p2] = [p1]. We do this in two stages. We .rst check if there exists p2 such \nthat ti(p2) > ti(p1). If not, we can update the over-approximation O accordingly (line 5). Otherwise, \nwe check if there exists p2 \" in the same boundary class as p1 (line 6). If no such p2 \" exists, then \nneither UN BOU N D E D nor UN BOU N D E D -FA I L applies. Given that p2 \" exists, we check for the existence \nof p3 in a stronger boundary class (lines 9-13). If p3 exists, we apply UNB O U N DE D-FAI L; otherwise, \nwe apply UNB O U N DE D. Scheduling Policy Our implementation is a scheduling of SYM BA s rules (Fig. \n3) that satis.es the fairness conditions (in Sec. 3.4). We start by applying the GL O BAL PU S H rule \nto obtain an initial point p. We generate a k-sequence starting at p (for each t . T ) by applying UN \nB O U N DE D (-FA I L ) until either UN B OUN D E D is appli\u00adcable (in which case the objective is unbounded) \nor UN BOU N D E D -FA I L is not applicable (in which case we apply GLOBA L PU S H to obtain a new initial \npoint and start the process again). It is easy to check that this is a fair sequence, and therefore this \nprocess always terminates. To evaluate variations of the scheduling policy described above, we instrumented \nour implementation with a parameter balance . (0, 100] which ensures that UN B OUN D ED IM PL does not \ntake more than balance% of the total execution time. Speci.cally, during execution, if UN BO U N D E \nD IM P L has taken more than balance% of the elapsed time, SY M BA switches to applying the GL O BAL \nPU S H rule until the time taken by UN BO U N D E D IM P L so far is less than balance% of the elapsed \ntime. Intuitively, when balance is 100, the deterministic schedule described above is in effect. Optimizations \nAnother effective optimization is to limit E(.) to a relevant subset when applying the UN BOU N D E D \nrule. In our experiments, we noticed that the set E(.) of equality con\u00adstraints can be quite large, which \nburdens the SMT solver. Remov\u00ading irrelevant equality constraints decreases the size of the SMT queries. \nTo .nd the set of relevant constraints, we de.ne a re\u00adlation .: Atoms(.) \u00d7 Atoms(.) as follows: P . P \n\" if and only if 1. Vars(P ) n Vars(P \" ) = \u00d8, or \"\" \"\" . P \"\" \" 2. .P . Atoms(.) \u00b7 P . P . P , where \nVars(P ) is the set of variables appearing in P . We then de.ne the boundary class of p w.r.t. t as [p]t \n= {a . E(.) | p |= a . t . a}. Removing constraints that are not .\u00adrelated to t corresponds to carrying \nout our algorithm on the pro- STAT I S TI C AVG . MA X . MI N . ST D . # of Variables # of Objectives \n# of Nodes in DAG 882 56 7,278 19,170 386 127,987 40 20 1,121 1,647 15 10,619 Table 1. Aggregate statistics \nof our 1,065 benchmarks. jection of . onto a lower-dimensional space, where the projection is guaranteed \nto have the same maximum value for t as .; thus correctness is not affected.  4.2 Experimental Evaluation \nOur experimental evaluation is designed to compare SY M BA against other symbolic optimization techniques, \nand to assess the effectiveness of our different implementation heuristics. We conducted two classes \nof experiments: (1) a comparison with existing SMT-based optimization techniques [57]; and (2) an evaluation \nof the effects of different implementation heuristics, as well as information reuse among multiple objectives, \non the ef.ciency of SY M BA. We describe these in detail below. Benchmarks As discussed in Sec. 1, one \npossible application of optimization is computing abstract transformers for numerical abstract domains. \nWe have incorporated SY M BA into the UFO program analysis and veri.cation framework [5] and used it \nas an abstract transformer (abstract post operator) for the family of Template Constraint Matrix (TCM) \ndomains [55]. A TCM domain is parameterized by a set of templates T = {t1, . . . , tn}, which are linear \nexpressions over program variables. Given an abstract state .pre describing a set of initial (pre) states \nand a loop-free program fragment encoded as a formula .lf, the best (most precise) abstract transformer \nfor a TCM domain computes the strongest formula . ti ki that is implied by .pre . .lf. Thus, we can use \nSY M BA to compute the best abstract transformer by simply computing optT (.pre . .lf). Note that the \nTCM domains subsume a number of popular domains, including intervals, octagons, octahedra, etc. For instance, \nby setting T to all live variables and their negation at the destination program location, then we get \nan intervals domain, since the result of SY M BA can be interpreted as the minimum and maximum value \nof each program variable after executing the program fragment denoted by .lf. We generated our benchmarks \nfrom a set of C programs used in the 2013 Software Veri.cation Competition (SV-COMP) [1]. The programs \ncover a range of software, from Linux and Windows device drivers to models of SSH and sequentialized \nconcurrent SystemC programs.3 We narrowed the set down to 604 C programs that were not trivially discharged \n(proved correct or incorrect) by UFO. We instrumented UFO to record abstract post queries in SMT-LIB2 \nformat, and collected 10K+ queries made by UFO on these C programs. Each abstract post query is represented \nby a formula encoding a set of initial states and a program fragment between two cutpoints (as in large \nblock encoding [12, 33]). For the set of objectives, we used all variables (as well as their negation) \nthat are in scope at the destination cutpoint (i.e., an intervals domain). From the generated queries, \nwe selected the hardest 1,065 benchmarks for evaluation (which took SY M BA more than 0.5s to process). \nTable 1 shows the average, maximum, minimum, and standard deviation, of the number of variables, objective \nfunctions, and nodes in the DAG representation of the formulas in our benchmarks. We conducted all of \nour experiments on a machine running Linux with an Intel i5 3.1GHz processor and 4GB of RAM. 3 We drew \n2,000+ programs from the following SV-COMP cat\u00adegories: ControlFlowIntegers, SystemC, ProductLines, and \nDeviceDrivers64.  Comparing with Existing Tools To the best of our knowledge, the work of Sebastiani \nand Tomasi [57] is the only other SMT tech\u00ad nique that addresses the problem of .nding optimal assignments \nfor LRA objective functions. At a high level, the technique works as follows: A Sample a satis.able disjunct \n.d from a given formula . using an SMT solver. B Since .d is a conjunction of atoms, the linear arithmetic \natoms in .d represent a convex polyhedron. So, use any linear program\u00adming (LP) solver to .nd the optimal \nvalue of a given objective function within the given disjunct. C Check, using an SMT solver, whether \nthe result is optimal for all of .. If not, go back to step A and sample a new disjunct. The process \nis guaranteed to terminate since there are .nitely many disjuncts. We have acquired a binary of the implementation \ndescribed in [57] from the authors. Their tool is called OPT-MATHSAT, as it is built in the MATH SAT \nSMT solver [18]. There are two issues threatening the validity of a direct comparison between SY M BA \nand OPT-MAT HSAT: (1) OPT-MATH SAT accepts a single objective function, meaning that we have to call \nit multiple times per bench\u00admark, each time with a different objective. Since we do not have access to \nits source code, we cannot tell if multiple calls to OPT-MATHSAT incur signi.cant pre-processing overhead. \n(2) Our im\u00adplementation of SYMBA uses Z3 as its underlying SMT solver, whereas OPT-MAT H SAT uses MATHSAT. \nIn order to avoid these issues and establish a fairer comparison, we have implemented two versions of \nthe linear search (LS) algo\u00adrithm proposed in [57] and implemented in OPT-MAT H SAT: 1. LS(OP T-Z3): \nBy accessing the available Z3 source code [3], we modi.ed the linear arithmetic solver of Z3 to allow \nopti\u00admization of LRA objectives in the satisfying assignments. A DPLL(T ) [26] solver like Z3 lazily \n.nds propositionally satis\u00ad .able disjuncts (conjunctions of atoms) from a given formula ., and uses \na (theory) T -solver to decide the satis.ability of the atoms (in our case linear constraints) [24]. \nWe instrumented Z3 s linear arithmetic solver such that it does not terminate im\u00admediately after the \n.rst satisfying assignment is found, but .nds an optimal satisfying assignment for a given objective \nfunction. This was implemented using the standard incremental simplex solving procedure under exact (rational) \nrepresentation. We call the modi.ed tool OPT-Z3. LS(OPT-Z3) is an implementation of LS in Z3 that uses \nOPT-Z3 as its LP solver. This is analo\u00adgous to the implementation described in [57], where a modi.ed \nversion of MAT H SAT s linear arithmetic solver is used as the LP solver. 2. LS(LP) uses an off-the-shelf \nLP solver as its convex optimiza\u00adtion engine. We have chosen two well-known open source LP libraries: \nthe GNU Linear Programming Kit (GL PK) [40] and the Sequential Object-oriented Simplex (SOPL E X) [62] \n4 .  Both versions of LS use Z3 for sampling disjuncts and checking optimality (i.e., steps A and C \nabove), but different LP solvers for .nding optimal values (step B). Unlike OPT-MAT H SAT, both LS(OP \nT-Z3) and LS(LP) accept multiple objective functions, and optimize them simultaneously. Speci.cally, \nin step B, they make multiple calls to the LP solver to .nd an optimal value for each objective function. \n4 We used G L P K v4.51 custom-built. The solver implements the primal two\u00adphase simplex method based \non .oating point arithmetic. We used SOPL E X v1.7.1, which uses .oating point arithmetic. SY M BA Con.gurations \nWe use the following SYM BA con.gu\u00adrations: 1. SY M BA(X): SY MBA with different scheduling policies, \nwhere X speci.es the value of balance. 2. SY M BAON E OBJ: Same as SYM BA(100), but optimizes a single \nobjective at a time. That is, execution is restarted from scratch for each objective function. 3. SY \nM BA(X)OPT-Z3 : Same as SY MBA(X), but uses Z3 with the modi.ed linear arithmetic solver OPT-Z3 (we describe \nthis in more detail below).  Results: SY M BA vs. Other Techniques Fig. 5(a) shows the re\u00ad sults of \nrunning SY MBA(100) vs. OPT-MAT H SAT on the 1,065 SMT-LIB2 benchmarks with a timeout of 100 seconds \nper bench\u00admark. Each point on the graph represents a benchmark. The axes correspond to the CPU time (measured \nin seconds log scale) taken by SYM BA(100) (x-axis) and OPT-MAT H SAT (y-axis). The points above the \ndiagonal represent problems where SY M BA is faster. Points at the top right corner are cases where both \nOPT-MATH SAT and SY MBA(100) cannot complete the benchmark in the allotted 100 seconds5 . OPT-MATH SAT \nhas 13 timeouts vs. 19 timeouts by SY M BA(100). Our results clearly show that SY M BA(100) outperforms \nOPT-MAT H SAT on our set of bench\u00admarks in most cases. The average and maximum speed up of SY M BA(100) \nvs. OPT-MATHSAT are 2.2x and 10.4x, respectively. We now compare SY M BA against our multi-objective-function \nimplementation of the linear search algorithm employed by OPT-MATH SAT. Fig. 5(b) compares the execution \ntimes of SYMBA(100) vs. LS(OP T-Z3). The results clearly demonstrate the superior per\u00adformance of LS(OPT-Z3): \nmost benchmarks are solved in less time by LS(OP T-Z3), often by an order of magnitude. LS(OPT-Z3) has \n7 timeouts. To understand the reasons behind these performance differ\u00adences, we took a closer look at \nthe benchmarks where SY M BA(100) is signi.cantly slower than LS(OPT-Z3). We noticed two recur\u00adring problems \nfor SY M BA on these benchmarks: (1) the major\u00adity of the time is spent in the UNB O U N DE DIM P L function, \nindi\u00adcating the expensive nature of UNB O UND EDIM P L calls and inef\u00adfectiveness of our scheduling strategy \nwhen balance=100; (2) ap\u00adplications of GLO BAL PU S H make very small expansions to the under-approximation \nU , indicating that we prefer guided applica\u00adtions of GLOBA L PU S H. To address point (1), we ensured \nthat UN-BO U N D E D IM P L does not take more than 40% of the execution time by setting balance to 40. \nTo address point (2), we considered ap\u00adplying GL O BA LPUSH using OPT-Z3 (described above) instead of \nZ3. That is, instead of GL OBA L PU S H asking Z3 for a point that lies outside the under-approximation \nU, we made GL O BA L PUSH sup\u00adply OP T-Z3 with one of the objective functions, and whenever the Z3 s \nDPLL(T ) solver .nds a satis.able disjunct with a point out\u00adside the under-approximation, it .nds a satisfying \nassignment that maximizes that objective function within that disjunct. This causes the GL O BA LPUSH \nrule to produce models that are farther away from the current under-approximation, expediting convergence. \nWe call this con.guration SY M BA(40)OPT-Z3 . Fig. 5(c) compares the execution times of SY M BA(40)OPT-Z \n3 vs. LS(OP T-Z3). The results now show that SYMBA(40)OP T-Z3 out\u00adperforms LS(OP T-Z3) on 81% of the \nbenchmarks, with a 5.0x maximum speedup and a 1.4x average speedup per benchmark. Moreover, SYMBA(40)OP \nT-Z3 solves all 1,065 benchmarks without timing out. We have also run the two other con.gurations of \nlin\u00ad 5 Since O PT-M AT H SAT accepts a single objective at a time, we invoked it multiple times per benchmark, \ngiving it a timeout of 100 seconds per objective. If the total time (for all objectives) taken for a \nbenchmark is more than 100 seconds, OP T-MAT H S AT is considered to have timed out.  ear search, LS(G \nLP K) and LS(SOPL E X). They exhibit similar be\u00adhaviour to LS(OP T-Z3), but are slightly slower. We cross-checked \nall results produced by different tools (when they do not timeout) and all of them match. Results: SYM \nBA s Con.gurations Table 2 summarizes the re\u00ad sults of running all the aforementioned algorithms and \ncon.gura\u00adtions on the same set of benchmarks with a timeout of 100 seconds per benchmark. The results \nof running SY M BA(100) are summa\u00adrized in row 1 of Table 2. SYMBA(100) was able to solve 1,046 out of \n1,065 benchmarks in 3,841 seconds. In the process, it made ~395K SMT queries using 24,387 invocations \nof GL O BAL PU S H and 164,156 invocations of UN B O UND ED IM P L. Rows 2-3 capture the results of running \nSY MBA(X), where X is 60 and 20, respectively. When X is 60 (time spent in UN-BOU N D E D IMPL is restricted \nto 60% of the total time) the num\u00adber of GL O BAL PU S H calls goes up by about 400%. Time is spent in \nmaking unguided discovery rather than big leap towards the goal. This even affects UN B O UND ED IM P \nL, the number of calls slightly increases since more points are sampled. When X is 20, it was only able \nto solve 766 benchmarks, for which the number of calls to GL O BA LPUSH goes above 130K while the number \nof calls to UN BO U N D E D IM P L drops to ~42K. Our experiments show that 100 is the best value for \nbalance when running SYMBA(X)6 . Conversely, when running SYM BA(X)OPT-Z 3 , we found that we greatly \nbene.t from a lower balance value (balance=40 gives us best performance), since there the GL O BA LPU \nSH rule can discover unbounded objectives, alleviating the pressure on UN BOU N D E D -IMPL. SY MBAON \nE OBJ (see Row 4 of Table 2) was able to solve 1,045 problems in 6,867 seconds. SY M BAONEOB J uses the \nsame con.g\u00aduration as SY M BA(100) except that it .nds solutions for multiple objectives independently, \nwithout reusing models amongst differ\u00adent objectives (as SY M BA does). Using SY M BAON EOB J causes \nthe number of SMT queries to go up by 15% and the number of GL O BA LPUSH calls to increase by 300%. \nOptimizing multiple ob\u00adjective functions simultaneously ensures that all objectives bene.t from the sampled \nmodels and potentially avoids repeating expen\u00adsive SMT calls. Summary The experiments compare our proposed \nSMT-based symbolic optimization algorithm with existing techniques and highlight the effectiveness of \nvarious implementation heuristics and optimizations. We compared SYMBA with OPT-MATHSAT as well as two \nLP based implementations of its algorithm on a large set of benchmarks generated from program analysis \ntasks. The results demonstrate the power of SY M BA s approach. A con\u00ad.guration that employs both ef.cient \nscheduling policy and convex optimization outperforms them all and solves all the benchmarks. Our experiments \nalso demonstrate the importance of SYM BA s multi-objective-function capability. 5. Related Work Our \nwork intersects with different areas of research. In this section, we compare SYMBA with (1) other optimization \ntechniques in SAT and SMT solvers; (2) optimization techniques employed within the context of abstract \ninterpretation [21]; (3) linear programming tech\u00ad niques; and (4) classi.cation techniques from the machine \nlearning community. Optimization in SAT/SMT Within the SMT and SAT solving arena, numerous forms of optimization \nhave been proposed (e.g., MAX-SAT/SMT in Yices [2]). The closest to SYMBA is the re\u00adcent of work of Sebastiani \nand Tomasi [57]. Similar to SYMBA, 6 We omit SY M BA(40) and S Y M BA(80) from the table as they exhibit \nsimilar performance to S Y M BA(60). they propose an SMT-based solution for optimizing objective func\u00adtions \nin LRA, and to the best of our knowledge, this is the only other work that addresses this problem. As \nexplained in Sec. 4, this approach works by sampling a satis.able disjunct (convex polyhe\u00addron) from \na given formula using the SMT solver as a black box. Then, using any LP solver, it .nds the optimal value \nof an objec\u00adtive function in that disjunct. It then checks if the optimal value is globally optimal (again, \nusing the SMT solver); otherwise, more disjuncts are sampled. Effectively, this approach lazily builds \nthe DNF of a formula until the disjunct with the optimal value of a given objective function is found. \nCompared to [57], SY M BA does not require an off-the-shelf LP solver (or access to a modi.ed the\u00adory \nsolver within the SMT solver [57]) Instead, SY M BA offers a simple and elegant optimization algorithm \nthat can be easily imple\u00admented on top of existing SMT solvers, without using any external tools. Moreover, \nSY M BA can simultaneously optimize multiple ob\u00adjective functions. SY MBA also maintains an over-approximation \nof the optimal solution, allowing us to prematurely terminate it and still retrieve optimal values for \na subset of the objective functions. As we show in Sec. 4, the multi-objective-function feature can also \nbe implemented within the approach of [57]. Both SY MBA and [57] can apply to formulas over mixed theories. \nIn [17], Cimatti et al. proposed a theory of costs for augmenting an SMT solver with pseudo-boolean (PB) \nconstraints [11]. At a high level, their theory allows associating a cost with individual Boolean constraints. \nThe goal then is to .nd a satisfying assignment that minimizes/maximizes the total cost. Thus, the theory \nof costs enables encoding weighted MAX-SAT/SMT problems in fact, the two problems are equivalent. It \nis easy to see that SYMBA subsumes the weighted MAX-SMT problem. For example, we can associate with each \nBoolean constraint Bi a cost ci, and add a constraint ite(Bi, ci = ki, ci = 0), where ki . R. Then, our \nobjective is to minimize or maximize the sum of all costs c1 + . . . + cn. Another form of optimization \nin the SMT framework is that of Nieuwenhuis and Oliveras [47]. Their optimization technique ex\u00ad tends \nthe traditional DPLL(T ) algorithm for SMT solving with ex\u00adtra rules for strengthening the theory T midway \nduring execution, e.g., a theory solver for linear arithmetic might be strengthened to .nd only assignments \nwith values less than 10. This allows guiding the solver towards satisfying assignments that maximize \nor mini\u00admize certain objective functions. The authors show how to imple\u00adment weighted MAX-SMT in their \ngeneral framework. It is unclear to us if their theory strengthening approach can be used to imple\u00adment \nan optimization procedure for LRA objective functions (as in this paper). Moreover, unlike SY M BA, their \napproach is not easily implementable, as it requires deep modi.cations to an SMT solver. In more recent \nwork [16], Chaganty et al. consider the problem of .nding most likely satisfying assignments in the presence \nof probabilistic constraints. There, an SMT solver is used to handle axioms, i.e., constraints with 0 \nor 1 probabilities, and a relational solver (e.g., [48]) is used to handle other probabilistic constraints. \nIt is important to note that the relational solver can be replaced by a weighted MAX-SMT solver (as noted \nin [16]). Thus, SY M BA can be directly applicable to solving such problems. We leave this interesting \ndirection for future work. Optimization in Abstract Interpretation Numerical abstract do\u00admains have been \nan active subject of research due to their impor\u00adtance in different program analyses. As discussed earlier, \nan impor\u00adtant operation in such domains is the abstract transformer (post), which can often be phrased \nas an optimization problem over formu\u00adlas in LRA. As a result, optimization has been a subject of interest \nwithin the program analysis community. For example, Monniaux [44] proposed an algorithm for com\u00ad puting \nbest abstract transformers of Template Constraint Matrix (TCM) domains [55] over loop-free code. As discussed \nin Sec. 4,  (a) (b) (c) Figure 5. Performance comparison between (a) SY M BA(100) vs. OPT-MATH SAT, \n(b) SYM BA(100) vs. LS(OP T-Z3), and (c) SY M BA(40)OPT-Z 3 vs LS(OP T-Z3). CO N FI G U R AT I O N TOTA \nL TI M E(s) # SMT QU E R I E S # SO LV E D # GL O BA L PU S H # U N B O U N D E D IM P L 1 SY M BA(100) \n3,841 394,579 1,046 24,387 164,156 2 SY M BA(60) 5,720 577,068 1,015 120,112 179,278 3 SY M BA(20) 2,716 \n231,906 766 132,051 42,227 4 SY M BAON E OB J 6,867 445,181 1,045 83,421 162,796 5 SY M BA(40)OP T-Z3 \n1,087 84,814 1,065 7,007 51,898 6 OPT-MAT H S AT 5,992 - 1,052 - - 7 LS ( O P T-Z 3 ) 1,521 20,829 a \n1,058 - - 8 LS(G L P K) 3,098 20,854 1,063 - - 9 LS(SOPL E X) 2,791 20,920 1,065 - - a We do not count \ncalls to LP solver (including OPT-Z3) in step B. Table 2. Overall results for different SYM BA and LS \ncon.gurations, as well as OPT-MATHSAT, on the 1065 SMT-LIB2 benchmarks. TCM domains are parameterized \nby a set of templates T , or ob\u00adjective functions. Loop-free program segments, along with a set of initial \nstates, can be encoded as a formula . in LRA. Then, the problem of computing the best abstract transformer \nbecomes that of computing optT (.). This problem is known in the program anal\u00adysis community as symbolic \nabstraction. The approach in [44] uses quanti.er elimination, which is admissible in LRA. In later work, \nMonniaux and Gonnord [46] attack the problem from a different angle, using an SMT solver to sample disjuncts \n.d of ., and, using off-the-shelf solvers, to .nd the optimal solution optT (.d) of each disjunct. Surprisingly, \nthis is done in the same manner as proposed by Sebastiani and Tomassi [57] for LRA optimization in SMT \n(dis\u00ad cussed above). Recently, Thakur and Reps [60] proposed a generalization of St\u00b0 almarck s SAT solving \nmethod [58] to richer logics. The algo\u00ad rithm attempts to prove a formula . unsatis.able by iteratively \nre\u00ad.ning an over-approximation of . starting from true until arriving at false. They showed how the algorithm \ncan be instantiated with abstract domains, such as polyhedra, and used to compute best ab\u00adstractions \nof formulas in LRA within the given abstract domain. Thus, by instantiating their algorithm with a TCM \ndomain, we can compute optT (.) for . in LRA. Their approach is a general frame\u00adwork for symbolic abstraction \nthat is applicable to a wide range of logics and abstract domains. In contrast to these techniques, our \ngoal with SY MBA is to bring LRA optimization to an SMT setting, leveraging the power and generality \nof SMT solvers, and making optimization directly usable by researchers who are already familiar with, \nand actively using, SMT solvers. Additionally, with SYMBA, we do not only .nd the optimal value of a \ngiven objective function, but also the satisfying assignment that results in such a value (found in the \nset of models M). This is a crucial requirement when, e.g., searching for optimal counterexample witnesses, \nwhere we need a trace of the counterexample (i.e., an optimal satisfying assignment) and not the optimal \nvalue of the given objective function. Linear Programming In the .eld of linear programming, the op\u00adtimization \nproblem over conjunctions and disjunctions of convex polyhedra (as in our setting) has been known as \nLinear Disjunctive Programming (LDP) [6]. Later, Linear Generalized Disjunctive Programming (LGDP) [51] \nwas proposed; there, Boolean variables are used to explicitly model discrete decisions. LDP and LGDP \nare equivalent to SY M BA when it is applied to formulas over LRA and propositional variables. Most notable \napproaches for solving LGDP problems carefully convert the problem to a Mixed Integer Linear Programming \n(MILP) problem, e.g., [56], and use existing MILP solvers to solve it. In comparison to such techniques, \nSY M BA can handle formulas over arbitrary theories (in combination with LRA), and can simultaneously \noptimize multiple objective functions. Ad\u00additionally, SY M BA uses in.nite precision rational arithmetic \nem\u00adployed by SMT solvers, whereas LGDP solvers tend to use .oating point arithmetic, potentially losing \nprecision. Classi.cation and Machine Learning A fundamental problem in machine learning is classi.cation: \ngiven a set of positive and negative examples, .nd a classi.er that predicts whether a given example \nis positive or negative. For example, using Support Vector Machines (SVMs) [50], one can compute linear \ninequalities sepa\u00ad rating positive and negative points in some space Rn .  SY MBA can be viewed as a \nsophisticated classi.cation algo\u00adrithm, where positive and negative examples are models of . and \u00ac., \nrespectively. The goal is to .nd the best classi.er, represented by a conjunction of linear inequalities \n(objective functions), that does not misclassify any of the positive examples (i.e., is implied by .). \nSY MBA only samples positive examples (from .) and keeps weakening a classi.er (the under-approximation \nU) until it encom\u00adpasses all positive examples. As Reps et al. point out in [52], weak\u00ad ening an under-approximation \nby sampling more points is analo\u00adgous to the approach of the simple learning algorithm Find-S [43]. Find-S \ngradually weakens a classi.er, starting from false, by itera\u00adtively taking into account more and more \npositive examples. 6. Conclusions and Future Work We proposed SY MBA, an ef.cient SMT-based optimization \nalgo\u00adrithm for objective functions stated in the theory of linear real arith\u00admetic. SYMBA utilizes ef.cient \nSMT solvers as black boxes, mak\u00ading it easy to implement without requiring modi.cations to existing intricate \nSMT solver implementations, and enabling it to directly bene.t from future advances in SMT solving. We \nhave evaluated SY M BA on benchmarks drawn from abstract transformer compu\u00adtations for numerical abstract \ndomains. Our thorough experimental evaluation indicates the advantages of our approach over other pro\u00adposed \ntechniques. We see many avenues for future work. First, the most natural next step is extending SY M \nBA to integer arithmetic objective func\u00adtions. We believe this can be done by augmenting SY M BA s rules \nwith new ones that introduce Gomory cuts (cutting planes) [29] in order to prune infeasible solutions. \nAnother interesting direction is handling non-linear objective functions, in order to model complex cost \nfunctions. From an engineering perspective, we would also like to study ef.cient parallel implementations \nof SY MBA s rules. References [1] Competition On Software Veri.cation -(SV-COMP). http:// sv-comp.sosy-lab.org. \n[2] Yices SMT Solver. http://yices.csl.sri.com. [3] Z3 Source Code Repository. http://z3.codeplex.com/. \n[4] A. Albarghouthi and K. L. McMillan. Beautiful Interpolants. In Proc. of CAV 13, pages 313 329, 2013. \n[5] A. Albarghouthi, A. Gur.nkel, and M. Chechik. UF O: A Framework for Abstraction-and Interpolation-Based \nSoftware Veri.cation. In Proc. of CAV 12, pages 672 678, 2012. [6] E. Balas. Disjunctive programming: \nProperties of the convex hull of feasible points. Discrete Applied Mathematics, 89(1 3):3 44, 1998. \n[7] T. Ball and S. Rajamani. The SLAM Toolkit. In Proc. of CAV 01, volume 2102 of LNCS, pages 260 264, \n2001. [8] C. Barrett, A. Stump, and C. Tinelli. The SMT-LIB Standard: Version 2.0. Technical report, \nDepartment of Computer Science, The Univer\u00adsity of Iowa, 2010. Available at www.SMT-LIB.org. [9] C. Barrett, \nC. Conway, M. Deters, L. Hadarean, D. Jovanovi\u00b4c, T. King, A. Reynolds, and C. Tinelli. CVC4. In Proc. \nof CAV 11, pages 171 177, 2011. [10] C. W. Barrett, R. Sebastiani, S. A. Seshia, and C. Tinelli. Satis.ability \nmodulo theories. In Handbook of Satis.ability, pages 825 885. 2009. [11] P. Barth. A Davis-Putnam based \nenumeration algorithm for lin\u00adear pseudo-Boolean optimization. Technical Report MPI-I-95-2-003, Max-Planck-Institute \nf \u00a8ur Informatik, 1995. [12] D. Beyer, A. Cimatti, A. Griggio, M. E. Keremoglu, and R. Sebastiani. Software \nModel Checking via Large-Block Encoding. In Proc. of FMCAD 09, pages 25 32, 2009. [13] G. M. Bierman, \nA. D. Gordon, C. Hritcu, and D. E. Langworthy. Semantic Subtyping with an SMT Solver. J. Funct. Program., \n22(1): 31 105, 2012. [14] N. Bjorner, K. McMillan, and A. Rybalchenko. Program Veri.cation as Satis.ability \nModulo Theories. In Proc. of SMT 12, 2012. [15] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted \nand Automatic Generation of High-Coverage Tests for Complex Systems Programs. In Proc. of OSDI 08, pages \n209 224, 2008. [16] A. Chaganty, A. Lal, A. Nori, and S. Rajamani. Combining Relational Learning with \nSMT Solvers using CEGAR. In Proc. of CAV 13, pages 447 462, 2013. [17] A. Cimatti, A. Franz \u00b4en, A. Griggio, \nR. Sebastiani, and C. Stenico. Sat\u00adis.ability Modulo the Theory of Costs: Foundations and Applications. \nIn Proc. of TACAS 10, pages 99 113, 2010. [18] A. Cimatti, A. Griggio, B. J. Schaafsma, and R. Sebastiani. \nThe MathSAT5 SMT Solver. In Proc. of TACAS 13, pages 93 107, 2013. [19] E. Clarke, D. Kroening, and F. \nLerda. A Tool for Checking ANSI-C Programs. In Proc. of TACAS 04, volume 2988 of LNCS, pages 168 176, \nMarch 2004. [20] P. Cousot and R. Cousot. Static Determination of Dynamic Properties of Programs. In \nProc. of the Colloque sur la Programmation, 1976. [21] P. Cousot and R. Cousot. Abstract Interpretation: \nA Uni.ed Lattice Model For Static Analysis of Programs by Construction or Approxi\u00admation of Fixpoints. \nIn Proc. of POPL 77, pages 238 252, 1977. [22] L. de Moura and N. Bj\u00f8rner. Z3: An Ef.cient SMT Solver. \nIn Proc. of TACAS 08, LNCS, pages 337 340, 2008. [23] R. DeLine and K. R. M. Leino. BoogiePL: A Typed \nProcedural Language for Checking Object-oriented Programs. Technical report, Microsoft Research, 2005. \n[24] B. Dutertre and L. de Moura. A Fast Linear-Arithmetic Solver for DPLL(T). In Proc. of CAV 06, pages \n81 94, 2006. [25] S. Falke, F. Merz, and C. Sinz. LLBMC: Improved Bounded Model Checking of C Programs \nUsing LLVM. In Proc. of TACAS 13, pages 623 626, 2013. [26] H. Ganzinger, G. Hagen, R. Nieuwenhuis, A. \nOliveras, and C. Tinelli. DPLL(T): Fast Decision Procedures. In Proc. of CAV 04, LNCS, pages 175 188, \n2004. [27] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated Random Testing. In Proc. of \nPLDI 05, pages 213 223, 2005. [28] P. Godefroid, M. Y. Levin, and D. A. Molnar. SAGE: Whitebox Fuzzing \nfor Security Testing. Commun. ACM, 55(3):40 44, 2012. [29] R. E. Gomory. Outline of an Algorithm for \nInteger Solutions to Linear Programs. Bull. Amer. Math. Soc., 64(5):275 278, 1958. [30] S. Grebenshchikov, \nN. P. Lopes, C. Popeea, and A. Rybalchenko. Syn\u00adthesizing Software Veri.ers from Proof Rules. In Proc. \nof PLDI 12, pages 405 416, 2012. [31] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis of \nLoop-Free Programs. In Proc. of PLDI 11, pages 62 73, 2011. [32] A. Gupta, C. Popeea, and A. Rybalchenko. \nSolving Recursion-Free Horn Clauses over LI+UIF. In Proc. of APLAS 11, pages 188 203, 2011. [33] A. Gur.nkel, \nS. Chaki, and S. Sapra. Ef.cient Predicate Abstraction of Program Summaries. In Proc. of NFM 11, volume \n6617 of LNCS, pages 131 145, 2011. [34] W. R. Harris, S. Sankaranarayanan, F. Ivancic, and A. Gupta. \nPro\u00adgram Analysis via Satis.ability Modulo Path Programs. In Proc. of POPL 10, pages 71 82, 2010. [35] \nT. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy Abstraction. In Proc. of POPL 02, pages 58 70, \n2002. [36] T. A. Henzinger, R. Jhala, R. Majumdar, and K. L. McMillan. Ab\u00adstractions from Proofs. In \nProc. of POPL 04, pages 232 244, 2004. [37] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari. Oracle-Guided \nComponent-Based Program Synthesis. In Proc. of ICSE 10, pages 215 224, 2010. [38] A. S. K \u00a8oksal, V. \nKuncak, and P. Suter. Constraints as Control. In Proc. of POPL 12, pages 151 164, 2012. [39] K. R. M. \nLeino. Dafny: An Automatic Program Veri.er for Functional Correctness. In Proc. of LPAR 10, pages 348 \n370, 2010.  [40] A. Makhorin. The GNU Linear Programming Kit. http://www.gnu.org/software/glpk/, 2000. \n[41] H. Massalin. Superoptimizer: a Look at the Smallest Program. SIGARCH Comput. Archit. News, 15(5):122 \n126, Oct. 1987. [42] A. Mine.\u00b4The Octagon Abstract Domain. J. Higher-Order and Symbolic Computation, \n19(1):31 100, 2006. [43] T. M. Mitchell. Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1997. \n[44] D. Monniaux. Automatic Modular Abstractions for Linear Con\u00adstraints. In Proc. of POPL 09, pages \n140 151, 2009. [45] D. Monniaux. Automatic Modular Abstractions for Template Numer\u00adical Constraints. \nLogical Methods in Computer Science, 6(3), 2010. [46] D. Monniaux and L. Gonnord. Using Bounded Model \nChecking to Focus Fixpoint Iterations. In Proc. of SAS 11, pages 369 385, 2011. [47] R. Nieuwenhuis and \nA. Oliveras. On SAT Modulo Theories and Optimization Problems. In Proc. of SAT 06, pages 156 169, 2006. \n[48] F. Niu, C. R\u00b4e, A. Doan, and J. Shavlik. Tuffy: Scaling Up Statistical Inference in Markov Logic \nNetworks Using an RDBMS. Proc. of VLDB 11, pages 373 384, 2011. [49] R. Piskac, T. Wies, and D. Zufferey. \nAutomating Separation Logic using SMT. In Proc. of CAV 13, 2013. [50] J. C. Platt. Fast Training of Support \nVector Machines Using Sequential Minimal Optimization. In Advances in Kernel Methods, pages 185 208. \nMIT Press, 1999. [51] R. Raman and I. Grossmann. Modelling and Computational Tech\u00adniques for Logic Based \nInteger Programming. Computers and Chem\u00adical Engineering, 18(7):563 578, 1994. [52] T. Reps, M. Sagiv, \nand G. Yorsh. Symbolic Implementation of the Best Transformer. In Proc. of VMCAI 04, volume 2937 of LNCS, \n2004. [53] P. M. Rondon, M. Kawaguchi, and R. Jhala. Liquid Types. In Proc. of PLDI 08, pages 159 169, \n2008. [54] A. Rybalchenko and V. Sofronie-Stokkermans. Constraint Solving for Interpolation. J. Symb. \nComput., 45(11):1212 1233, 2010. [55] S. Sankaranarayanan, H. B. Sipma, and Z. Manna. Scalable Analysis \nof Linear Systems using Mathematical Programming. In Proc. of VMCAI 05, pages 25 41, 2005. [56] N. W. \nSawaya and I. E. Grossmann. A Cutting Plane Method for Solv\u00ading Linear Generalized Disjunctive Programming \nProblems. Comput\u00aders And Chemical Engineering, 29(9):1891 1913, 2005. [57] R. Sebastiani and S. Tomasi. \nOptimization in SMT with LA(Q) Cost Functions. In Proc. of IJCAR 12, pages 484 498, 2012. [58] M. Sheeran \nand G. St\u00b0amarck. A Tutorial on St\u00b0almarck s Proof Proce\u00addure for Propositional Logic. FMSD, 16:23 58, \n2000. [59] A. Solar-Lezama, L. Tancau, R. Bod\u00b4ik, S. A. Seshia, and V. A. Saraswat. Combinatorial Sketching \nfor Finite Programs. In Proc. of ASPLOS 06, pages 404 415, 2006. [60] A. V. Thakur and T. W. Reps. A \nMethod for Symbolic Computation of Abstract Operations. In Proc. of CAV 12, pages 174 192, 2012. [61] \nA. V. Thakur, M. Elder, and T. W. Reps. Bilateral Algorithms for Symbolic Abstraction. In Proc. of SAS \n12, pages 111 128, 2012. [62] R. Wunderling. Paralleler und Objekt-orientierter Simplex-Algorithmus. \nPhD thesis, Technische Universit\u00a8at Berlin, 1996. 7. Appendix Lemma 2. Given an L formula .c de.ning \na convex polyhedron (conjunction of linear constraints), if p |= .c, [p] . [p \" ], t(p) \" \"\" \"\" \"\" \" \nt(p ) and p |= .c \u00b7 t(p) t(p ) . [p] . [p ], then p |= .c. Proof. Formula .c . [p] can be written as \na system of linear in\u00adequalities as follows: Apx = pb (1a) Cpx ; dp (1b) By de.nition, p satis.es the \nsystem and Eq. 1a is a subset of [p] according to the de.nition of boundary class. Because [p] . [p \" \n], p \" must satisfy Eq. 1a. Suppose p \" F .c. Then there exists some k such that cpk \u00b7 pp > dk and cpk \n\u00b7 pp \" < dk Note that cpk \u00b7 pp = dk since cpk \u00b7 pp \" = dk and [p] . [p \" ]. We now show that there exists \na point p k = app + (1 - a)pp \" (0 < a < 1) in the convex hull of p and p \" s.t. cpk \u00b7p k = dk. Since \nt(p) t(p \" ), it is p\" easy to show that t(p) t(pk). Let cpk \u00b7p = dk+d1, cpk \u00b7p = dk-d2 (where d1 > 0 \nand d2 > 0), and a = d2 . We have cpk \u00b7p k = dk. d1+d2 We know that [p] . [pk] because pk is in the convex \nhull of p and p \" and {cpk \u00b7 px = dk} ./[p] . {cpk \u00b7 px = dk} . [pk]. Suppose pk F .c, i.e., there exists \nj s.t. cpj \u00b7 p k < dj , we then repeat the above process. Each point pi found satis.es t(p) t(pi) . [p] \n. [pi]. We are guaranteed to .nd a point p \"\" in the convex hull s.t. it is also within .c, because in \neach step we satisfy at least one more inequality. This contradicts the condition \"\" \"\" \"\" \" p |= .c \n\u00b7 t(p) t(p ) . [p] . [p ]. Therefore, p |= .c. Proof of Thm. 1 (.) We prove this direction by contradiction. \nFirst, let p1, p2 |= . be two models satisfying the three conditions of the theorem. Suppose there is \na point p * |= . s.t. t(p * ) is the upper bound for t in .. We show that there is always a point p2 \n\" |= . s.t. t(p2 \" ) > t(p * ). \" \" Pick a point p2 s.t. pp2 = pp2 + .(pp2 - pp1). It follows that \" \n* (pP*-pP2)\u00b7Pt t(p2) > t(p ) when . > . The notation pp denotes the (pP2-pP1)\u00b7Pt vector representation \n(p(x1), . . . , p(xn)) of the model p : Vars . R, where Vars = {x1, . . . , xn}. Let the formula . \" \nde.ne a convex polyhedron s.t. p2 |= . \" and . . \" . .. Let .c = . \" . [p2]. We know the following: 1. \n.c de.nes a convex polyhedron (by its de.nition). \"\" \"\" \"\" 2. p2 |= .c \u00b7 t(p2) t(p2 ) . [p2] . [p2 ] \n(by condition 3 of the theorem). 3. [p2] . [p2 \" ] (since p2 \" is in the af.ne set of p1 and p2). 4. \nt(p2) t(p2 \" ) (since . ; 0).  Following the result of Lemma 2, p2 \" is in .c which is also in .. This \ncontradicts the assumption that t(p * ) is the least upper bound for t. Therefore, term t is unbounded \nin .. (.) Given that t is unbounded in ., we look for two models p1, p2 |= . that satisfy the required \nconditions. Pick p1, p2 |= . s.t. [p1] = [p2], t(p2) > t(p1), and [p1] is the most restrictive boundary \nclass within which t is unbounded (i.e., t is unbounded in . .. [p1], and there does not exist a boundary \nclass c . [p1] s.t. t is . unbounded in . . c). We know that such a class exists because t is unbounded \nin . (otherwise t is bounded in every boundary class and . is bounded). In other words, since (1) there \nare in.nitely many models of . with increasing values of t and (2) .nitely many boundary classes, there \nhas to be a boundary class [p1] s.t. t is . unbounded in . . [p1] and there doesn t exist a class c . \n[p1] . where t is unbounded in . . c. . If there are no classes c . [p1], or for every c . [p1], .. c \n. false, then p1 and p2 satisfy the three conditions of the theorem. s . Otherwise, let m = maxp|=...t(p), \nwhere . = c (i.e., c.[p1] all classes stronger than [p1]). We know that m is de.ned (i.e., not 8) because \nof our assumption on the class [p1]. If m < t(p2), then p1 and p2 satisfy the three conditions of the \ntheorem. Otherwise, . since t is unbounded in .. [p1], we can .nd two models p1 \" , p2 \" |= \"\" \"\"\" . \ns.t. m < t(p1) < t(p2) and [p1] = [p1] = [p2]. As a result, p1 and p2 \" satisfy the three conditions \nin the theorem. D   \n\t\t\t", "proc_id": "2535838", "abstract": "<p>The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities.</p> <p>In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for <i>objective functions</i> in the theory of linear real arithmetic (LRA). Given a formula &#966; and an objective function <i>t</i>, SYMBA finds a satisfying assignment of &#966;that maximizes the value of <i>t</i>. SYMBA utilizes efficient SMT solvers as black boxes. As a result, it is easy to implement and it directly benefits from future advances in SMT solvers. Moreover, SYMBA can optimize a set of objective functions, reusing information between them to speed up the analysis. We have implemented SYMBA and evaluated it on a large number of optimization benchmarks drawn from program analysis tasks. Our results indicate the power and efficiency of SYMBA in comparison with competing approaches, and highlight the importance of its multi-objective-function feature.</p>", "authors": [{"name": "Yi Li", "author_profile_id": "81508695186", "affiliation": "University of Toronto, Toronto, ON, Canada", "person_id": "P4383913", "email_address": "liyi@cs.toronto.edu", "orcid_id": ""}, {"name": "Aws Albarghouthi", "author_profile_id": "81498660436", "affiliation": "University of Toronto, Toronto, ON, Canada", "person_id": "P4383914", "email_address": "aws@cs.toronto.edu", "orcid_id": ""}, {"name": "Zachary Kincaid", "author_profile_id": "81472649374", "affiliation": "University of Toronto, Toronto, ON, Canada", "person_id": "P4383915", "email_address": "zkincaid@cs.toronto.edu", "orcid_id": ""}, {"name": "Arie Gurfinkel", "author_profile_id": "81100225646", "affiliation": "Carnegie Mellon University, Pittsburgh, ON, USA", "person_id": "P4383916", "email_address": "arie@cmu.edu", "orcid_id": ""}, {"name": "Marsha Chechik", "author_profile_id": "81100113224", "affiliation": "University of Toronto, Toronto, ON, Canada", "person_id": "P4383917", "email_address": "chechik@cs.toronto.edu", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535857", "year": "2014", "article_id": "2535857", "conference": "POPL", "title": "Symbolic optimization with SMT solvers", "url": "http://dl.acm.org/citation.cfm?id=2535857"}