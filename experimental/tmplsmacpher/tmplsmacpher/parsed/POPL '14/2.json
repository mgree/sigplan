{"article_publication_date": "01-08-2014", "fulltext": "\n Polymorphic Functions with Set-Theoretic Types Part 1: Syntax, Semantics, and Evaluation Giuseppe Castagna1 \nKim Nguy.n2 Zhiwu Xu1,3 Hyeonseung Im2 Sergue\u00a8i Lenglet4 Luca Padovani5 1CNRS, PPS, Univ Paris Diderot, \nSorbonne Paris Cit\u00e9, Paris, France 2LRI, Universit\u00e9 Paris-Sud, Orsay, France 3State Key Laborator y of \nComputer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China 4 LORIA, Universit\u00e9 \nde Lorraine, Nancy, France 5Dipartimento di Informatica, Universit\u00e0 di Torino, Italy Abstract. This \narticle is the first part of a two articles series about a calculus with higher-order polymorphic functions, \nrecursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, \nand negation). In this first part we define and study the explicitly-typed version of the calculus in \nwhich type instantiation is driven by explicit instantiation annota\u00adtions. In particular, we define an \nexplicitly-typed .-calculus with intersection types and an efficient evaluation model for it. In the \nsecond part, presented in a companion paper, we define a local type inference system that allows the \nprogrammer to omit explicit instantiation annotations, and a type reconstruction system that al\u00adlows \nthe programmer to omit explicit type annotations. The work presented in the two articles provides the \ntheoretical foundations and technical machinery needed to design and implement higher\u00adorder polymorphic \nfunctional languages for semi-structured data. Categories and Subject Descriptors D.3.3 [Programming \nLan\u00adguages]: Language Constructs and Features Polymorphism Keywords Types, polymorphism, XML, intersection \ntypes 1. Introduction The extensible markup language XML is a current standard format for exchanging \nstructured data. Many recent XML processing lan\u00adguages, such as XDuce, CDuce, XQuery, OcamlDuce, XHaskell, \nXAct, are statically-typed functional languages. However, paramet\u00adric polymorphism, an essential feature \nof such languages, is still missing, or when present it is in a limited form (no higher-order functions, \nno polymorphism for XML types, and so on). Polymor\u00adphism for XML has repeatedly been requested to and \ndiscussed in various working groups of standards (eg, RELAX NG [6]) and higher-order functions have been \nrecently proposed in the W3C draft for XQuery 3.0 [9]. Despite all this interest, spurs, and mo\u00adtivations, \na comprehensive polymorphic type system for XML was still missing for the simple reason that, until recently, \nit was deemed unfeasible. A major stumbling block to this research ie, the def\u00adinition of a subtyping \nrelation for regular tree types with type variables has been recently lifted by Castagna and Xu [5], \nwho defined and studied a polymorphic subtyping relation for a type system with recursive, product, and \narrow types and set-theoretic type connectives (union, intersection, and negation). In this work we present \nthe next logical step of that research, that is, the definition of a higher-order functional language \nthat takes full advantage of the new capabilities of Castagna and Xu s system. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advantage and that copies bear this \nnotice and the full citation on the first page. Copyrights for components of this work owned by others \nthan the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on ser vers or to redistribute to lists, requires prior specific permission and/or a fee. Request \npermissions from permissions@acm.org. POPL 14, January 22 24 2014, San Diego, CA, USA. In other words, \nwe define and study a calculus with higher-order polymorphic functions and recursive types with union, \nintersection, and negation connectives. The approach is thus general and, as such, goes well beyond the \nsimple application to XML processing languages. As a matter of facts, our motivating example developed \nall along this paper does not involve XML, but looks like a rather classic display of functional programming \nspecimens: map :: (a -> \u00df) -> [a] -> [\u00df] map f l = case l of | [] -> [] | (x : xs) -> (f x : map f xs) \neven :: (Int -> Bool) . ((a\\Int) -> (a\\Int)) even x = case x of | Int -> (x mod 2) == 0 | _ -> x The \nfirst function is the classic map function defined in Haskell (we just used Greek letters to denote type \nvariables). The second would be an Haskell function were it not for two oddities: its type contains type \nconnectives (type intersection . and type difference \\ ); and the pattern in the case expression is a \ntype, meaning that it matches all values returned by the matched expression that have that type. So what \ndoes the even function do? It checks whether its argument is an integer; if it is so it returns whether \nthe integer is even or not, otherwise it returns its argument as it received it (although even may be \nconsidered as bad programming, it is a perfect minimal example to illustrate all the aspects of our system). \nThe goal of this work is to define a calculus and a type system that can pass three tests. The first \ntest is that it can define the two functions above. The second, harder, test is that the type system \nmust be able to verify that these functions have the types declared in their signatures. That map has \nthe declared type will come as no surprise (in practice, in the second part of this work we show that \nin the absence of a signature given by the programmer the system can reconstruct a type slightly more \nprecise than this [4]). That even was given an intersection type means that it must have all the types \nthat form the intersection. So it must be a function that when applied to an integer it returns a Boolean \nand that when applied to an argument of a type that does not contain any integer, it retur ns a result \nof the same type. In other terms, even is a polymorphic (dynamically bounded) overloaded function. The \nthird test, the hardest one, is that the type system must be able to infer the type of the partial application \nof map to even, and the inferred type must be equivalent to the following one1 map even :: ([Int] -> \n[Bool]) . ([a\\Int] -> [a\\Int]) . (1) ([a.Int] -> [(a\\Int).Bool]) since map even retur ns a function that \nwhen applied to a list of integers it retur ns a list of Booleans, when applied to a list that Copyright \nis held by the owner/author(s). Publication rights licensed to ACM. 1This type is redundant since the \nfirst type of the intersection is an instance ACM 978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535840 \n(eg, for a=Int) of the third. We included it for the sake of the presentation.  does not contain any \ninteger then it retur ns a list of the same type (actually, the same list), and when it is applied to \na list that may contain some integers (eg, a list of reals), then it retur ns a list of the same type, \nwithout the integers but with some Booleans instead (in the case of reals, a list with Booleans and reals \nthat are not integers). Technically speaking, the definition of such a calculus and its type system is \ndifficult for two distinct reasons. First, for the reasons we explain in the next section, it demands \nto define an explicitly typed .-calculus with intersection types, a task that, despite many attempts \nin the last 20 years, still lacked a satisfactor y definition. Second, even if working with an explicitly \ntyped setting may seem simpler, the system needs to solve local type inference 2, namely, the problem \nof checking whether the types of a function and of its argument can be made compatible and, if so, of \ninferring the type of their result as we did for (1). The difficulty, once more, mainly resides in the \npresence of the intersection types: a term can be given different types either by subsumption (the term \nis coerced into a super-type of its type) or by instantiation (the term is used as a particular instance \nof its polymorphic type) and it is typed by the intersection of all these types. Therefore, in this setting, \nthe problem is not just to find a substitution that unifies the domain type of the function with the \ntype of its argument but, rather, a set of substitutions that produce instances whose intersections are \nin the right subtyping relation: our map even example should already have given a rough idea of how difficult \nthis is. The presentation of our work is split in two parts, accordingly: in the first part (this paper) \nwe show how to solve the problem of defining an explicitly-typed .-calculus with intersection types and \nhow to efficiently evaluate it; in the second part (the companion paper [4]) we will show how to solve \nthe problem of local type inference for a calculus with intersection types. In the next section we outline \nthe various problems we met (focusing on those that concern the part of the work presented in this paper) \nand how they were solved.  2. Problems and overview of the solution The driver of this work is the definition \nof an XML processing functional language with high-order polymorphic functions, that is, in particular, \na polymorphic version of the language CDuce [2]. CDuce in a nutshell. The essence of CDuce is a .-calculus \nwith pairs, explicitly-typed recursive functions, and a type-case expres\u00adsion. Types can be recursively \ndefined and include the arrow and product type constructors and the intersection, union, and negation \ntype connectives. In summary, they are the regular trees coinduc\u00adtively generated by the following productions: \nt ::= b |t . t |t \u00d7 t |t . t |t . t |\u00act |0 |1 (2) where b ranges over basic types (eg, Int, Bool) and \n0 and 1 respectively denote the empty (that types no value) and top (that types all values) types. We \nuse possibly indexed meta-variables s and t to range over types. Coinduction accounts for recursive types. \nWe use the standard convention that infix connectives have a priority higher than constructors and lower \nthan prefix connectives. From a strictly practical viewpoint, recursive types, products, and type connectives \nare used to encode regular tree types, which subsume existing XML schema/types while, for what concer \nns ex\u00adpressions, the type-case is an abstraction of CDuce pattern match\u00ading (this uses regular expression \npatterns on types to define pow\u00aderful and highly optimized capture primitives for XML data). We 2There \nare different definitions for local type inference. Here we use it with the meaning of finding the type \nof an expression in which not all type annotations are specified. This is the acceptation used in Scala \nwhere type parameters for polymorphic methods can be omitted. In our specific problem, we will omit and, \nthus, infer the annotations that specify how function and argument types can be made compatible. initially \nfocus on the functional core and disregard products and re\u00adcursive functions since the results presented \nhere can be easily ex\u00adtended to them (we show it in the electronic appendix available at http://dl.acm.org), \nthough we will freely use them for our ex\u00adamples. So we initially consider the following CoreCDuce terms: \n.i.Isi.ti e ::= c |x |ee |.x.e |e.t?e :e (3) where c ranges over constants (eg, true, false, 1, 2, ...) \nwhich are values of basic types (we use bc to denote the basic type of the constant c); x ranges over \nexpression variables; e.t?e1 :e2 denotes the type-case expression that evaluates either e1 or e2 according \nto whether the value retur ned by e (if any) is of type t or not; ..i.Isi.tix.e is a value of type .i.Isi \n. ti that denotes the function of parameter x and body e. In this work we show how to define the polymorphic \nextension of this calculus, which can then be easily extended to a full-f ledged polymorphic functional \nlanguage for processing XML documents. But before let us explain the two specificities of the terms in \n(3), namely, why a type-case expression is included and why we explicitly annotate whole .-abstractions \n(with an intersection of arrow types) rather than just their parameters. The reasons for including a \ntype-case in the terms of the calcu\u00adlus are detailed in [11]: in short, intersection types are used to \ntype overloaded functions, and without a type-case only coherent over\u00adloading \u00e0 la Forsythe [16]can be \ndefined (which, in our setting, precludes for instance the definition of a non diverging function of \ntype (Int.Bool). (Bool.Int)). Also, in our system the rela\u00adtion s1.s2 . t1.t2 . (s1.t1).(s2.t2)is, in \ngeneral, strict, and the functions that are in the difference of these two types are those that distinguish \nnon coherent overloading from coherent one. To inhabit this difference we need real overloaded functions \nthat execute different code according to the type of their input, whence the need of type-case expressions. \nThe need of explicitly typed functions is a direct consequence of the introduction of the type-case, \nbecause without explicit typing we can run into paradoxes such as \u00b5f..x.f .(1.Int)? true : 42, a recursively \ndefined (constant) function that has type 1.Int if and only if it does not have type 1.Int. In order \nto decide whether the function above is well-typed or not, we must explicitly give a type to it. For \ninstance, the function is well-typed if it is explicitly assigned the type 1 . Int.Bool. This shows both \nthat functions must be explicitly typed and that specifying not only the type of pa\u00adrameters but also \nthe type of the result is strictly more expressive, as more terms can be typed. In summary, we need to \ndefine an explicitly typed language with intersection types. This is a difficult problem for which no \nfull-f ledged solution existed, yet: there exist only few intersection type systems with explicitly typed \nterms, and none of them is completely satisfactor y (see Section 7 on related work). To give an idea \nof why this is difficult, imagine we adopt for functions a Church-style notation as .x t .e and consider \nthe following switch function .x t . (x.Int ? true : 42) that when applied to an Int returns true and \nretur ns 42 otherwise. Intuitively, we want to assign to this function the type (Int.Bool). (\u00acInt.Int), \nthe type of a function that when applied to an Int, retur ns a Bool, and when applied to a value which \nis not an Int, retur ns an Int. For the sake of presentation, let us say that we are happy to deduce \nfor the function above the less precise type (Int.Bool). (Bool.Int) (which is a super-type of the former \nsince if a function maps anything that is not an Int into an Int it has type \u00acInt.Int , then in particular \nit maps Booleans to integers ie, it has also type Bool.Int). The problem is to determine which type we \nshould use for t in the switch function above. If we use, say, Int.Bool, then under the hypothesis that \nx : Int.Bool the type deduced for the body of the function is Int.Bool. So the best type we can give \nto the switch function is Int.Bool . Int.Bool which is far  less precise than the sought intersection \ntype, insofar as it does not make any distinction between arguments of type Int and those of type Bool. \nThe solution, which was introduced by CDuce, is to explicitly type by an intersection type whole .-abstractions \ninstead of just their parameters: .(Int.Bool).(Bool.Int)x . (x.Int ? true:42) In doing so we also explicitly \ndefine the result type of functions which, as we have just seen, increases the expressiveness of the \ncalculus. Thus the general form of .-abstractions is, as stated by the grammar in (3), ..i.Isi.tix.e. \nSuch a term is well typed if for all i . I from the hypothesis that x has type si it is possible to deduce \nthat e has type ti. Unfortunately, with polymorphic types, this simple solution introduced by CDuce no \nlonger suffices. Polymorphic extension. The novelty of this work is to allow type variables (ranged over \nby lower-case Greek letters: a,\u00df, ...) to oc\u00adcur in the types in (2) and, thus, in the types labeling \n.-abstractions in (3). It becomes thus possible to define the polymorphic identity function as .a.a x.x, \nwhile classic auto-application term can be written as .((a.\u00df).a).\u00df x.xx. The intended meaning of using \na type variable, such as a, is that a (well-typed) .-abstraction not only has the type specified in its \nlabel (and by subsumption all its super-types) but also all the types obtained by instantiating the type \n.a.a variables occur ring in the label. So x.x has not only type a.a but also, for example, by subsumption \nthe types 0.1 (the type of all functions, which is a super-type of a.a)and \u00acInt (since every well-typed \n.-abstraction is not an integer, then \u00acInt contains ie, is a super-type of all function types), and by \nin\u00adstantiation the types Int.Int, Bool.Bool, etc. The use of instantiation in combination with intersection \ntypes has nasty consequences, for if a term has two distinct types, then it has also their intersection \ntype (eg, .a.a x.x has type (Int.Int). (Bool.Bool). \u00acInt). In the monomor phic case a term can have distinct \ntypes only by subsumption and, thus, inter\u00adsection types are transparently assigned to terms via subsumption. \nBut in the polymorphic case this is no longer possible: a term can be typed by the intersection of two \ndistinct instances of its polymor\u00adphic type which, in general, are not in any subtyping relation with \nthe latter: for instance, a.a is neither a subtype of Int.Int nor vice versa, since the subtyping relation \nmust hold for all possible instantiations of a and there are infinitely many instances of a.a that are \nneither a subtype nor a super-type of Int.Int. Explicit instantiation. Concretely, if we want to apply \nthe poly\u00admorphic identity .a.a x.x to, say, 42, then the particular instance obtained by the type-substitution \n{Int/a}(denoting the replace\u00adment of every occur rence of a by Int) must be used, that is (.Int.Int x.x)42. \nWe have thus to relabel the type decorations of .-abstractions before applying them. In implicitly typed \nlanguages, such as ML, the relabeling is meaningless (no type decoration is used in terms) while in their \nexplicitly-typed counter parts relabel\u00ading can be seen as a logically meaningful but computationally \nuse\u00adless operation, insofar as execution takes place on type erasures (ie, the terms obtained by erasing \nall type decorations). In the presence of type-case expressions, however, relabeling is necessary since \nthe label of a .-abstraction determines its type: testing whether an ex\u00adpression has type, say, Int.Int \nshould succeed for the application of .a.a.a x..a.a y.x to 42 and fail for its application to true. This \nmeans that, in Reynolds terminology, our terms have an in\u00adtrinsic meaning [17], that is to say, the semantics \nof a term depends on its typing. If we need to relabel some function, then it may be necessary to relabel \nalso its body as witnessed by the following daffy though well-typed definition of the identity function: \na.a a.a (.x.(.y.x)x) (4) If we want to apply this function to, say, 3, then we have first to relabel \nit by applying the substitution {Int/a}. However, applying the relabeling only to the outer . does not \nsuffice since the application of (4)to 3 reduces to (.a.a y.3)3 which is not well\u00ad .a.a typed (it is \nnot possible to deduce the type a.a for y.3, which is the constant function that always retur ns 3) although \nit is the reductum of a well-typed application.3 The solution is to apply the relabeling also to the \nbody of the function. Here what to relabel the body means is straightforward: apply the same type-substitution \n{Int/a}to the body. This yields a reductum (.Int.Int y.3)3 which is well typed. In general, however, \nthe way to perform a relabeling of the body of a function is not so straightforward and clearly defined, \nsince two different problems may arise: (i) it may be necessary to apply more than a single type-substitution \nand (ii)the relabeling of the body may depend on the dynamic type of the actual argument of the function \n(both problems are better known as or are instances of the problem of determining expansions for intersection \ntype systems [7]). Next, we discuss each problem in detail. Multiple substitutions. First of all, notice \nthat we may need to re\u00adlabel/instantiate functions not only when they are applied but also when they \nare used as arguments. For instance, consider a func\u00adtion that expects arguments of type Int.Int. It \nis clear that we can apply it to the identity function .a.a x.x, since the iden\u00adtity function has type \nInt.Int (feed it by an integer and it will return an integer). Before, though, we have to relabel the \nlatter by the substitution {Int/a}yielding .Int.Int x.x. As the identity .a.a x.x has type Int.Int, so \nit has type Bool.Bool and, therefore, the intersection of the two: (Int.Int).(Bool.Bool). So we can apply \na function that expects an argument of this inter\u00adsection type to our identity function. The problem \nis now how to relabel .a.a x.x. Intuitively, we have to apply two distinct type\u00adsubstitutions {Int/a}and \n{Bool/a}to the label of the .-abstraction and replace it by the intersection of the two instances. This \ncor\u00adresponds to relabel the polymorphic identity from .a.a x.x into .(Int.Int).(Bool.Bool) x.x. This \nis the solution adopted by this work, where we manipulate sets of type-substitutions delimited by square \nbrackets. The application of such a set (eg, in the previous example [{Int/a},{Bool/a}]) to a type t \nreturns the intersection of all types obtained by applying each substitution in the set to t (eg, in \nthe example t{Int/a}. t{Bool/a}). Thus the first problem has an easy solution. Relabeling of function \nbodies. The second problem is much harder and concerns the relabeling of the body of a function. While \nthe naive solution consisting of propagating the application of type\u00adsubstitutions to the bodies of functions \nworks for single type\u00adsubstitutions, in general, it fails for sets of type-substitutions. This can be \nseen by consider ing the relabeling via the set of type\u00adsubstitutions [{Int/a},{Bool/a}] of the daffy \nfunction in (4). If we apply the naive solution, this yields (Int.Int).(Bool.Bool)(Int.Int).(Bool.Bool) \n(.x.(.y.x)x) (5) which is not well typed. That this term is not well typed is clear if we try applying \nit to, say, 3: the application of a function of type (Int.Int).(Bool.Bool)to an Int should have type \nInt, but here it reduces to (.(Int.Int).(Bool.Bool)y.3)3, and there is no way to deduce the intersection \ntype (Int.Int). (Bool.Bool) for the constant function .y.3. But we can also directly verify that it is \nnot well typed, by trying typing the function in (5). This corresponds to prove that under the hypothesis \nx : Int the 3By convention a type variable is introduced by the outermost . in which it occurs and this \n. implicitly binds all inner occur rences of the variable. For instance, all the a s in the term (4) \nare the same while in a term such as (.a.ax.x)(.a.ax.x)the variables in the function are distinct from \nthose in its argument and, thus, can be a-converted separately, as (.... x.x)(.d.dx.x).  term (.(Int.Int).(Bool.Bool) \ny.x)x has type Int, and that under the hypothesis x : Bool this same term has type Bool. Both checks \nfail because, in both cases, .(Int.Int).(Bool.Bool)y.x is ill\u00adtyped (it neither has type Int.Int when \nx:Bool, nor has it type Bool.Bool when x:Int). This example shows that in order to ensure that relabeling \nyields well-typed terms, the relabeling of the body must change according to the type of the value the \nparameter x is bound to. More precisely, (.a.a y.x)should be relabeled as .Int.Int y.x when x is of type \nInt, and as .Bool.Bool y.x when x is of type Bool. An example of this same problem less artificial than \nour daffy function is given by the classic apply function .f..x.f x which, with our polymorphic type \nannotations, is written as: .(a.\u00df).a.\u00df f..a.\u00df x.f x (6) The apply function in (6) has type (Int.Int).Int.Int, \nobtained by instantiating its type annotation by the substitution {Int/a,Int/\u00df}, as well as type (Bool.Bool).Bool.Bool, \nob\u00adtained by the substitution {Bool/a,Bool/\u00df}. If we want to feed this function to another function that \nexpects arguments whose type is the intersections of these two types, then we have to relabel it by us\u00ading \nthe set of type-substitutions [{Int/a,Int/\u00df},{Bool/a,Bool/\u00df}]. But, once more, it is easy to verify that \nthe naive solution that con\u00adsists in propagating the application of the set of type-substitutions down \nto the body of the function yields an ill-typed expression. This second problem is the showstopper for \nthe definition of an explicitly typed .-calculus with intersection types. Most of the solutions found \nin the literature [3, 13, 18, 21] rely on the dupli\u00adcation of lambda terms and/or typing derivations, \nwhile other cal\u00adculi such as [22] that aim at avoiding such duplication obtain it by adding new expressions \nand new syntax for types (see related work in Section 7); but none of them is able to produce an explicitly\u00adtyped \n.-calculus with intersection types, as we do, by just adding annotations to .-abstractions. Our solution. \nHere we introduce a new technique that consists in performing a lazy relabeling of the bodies. This is \nobtained by decorating .-abstractions by (sets of) type-substitutions. For exam\u00adple, in order to pass \nour daffy identity function (4) to a function that expects arguments of type (Int.Int). (Bool.Bool), \nwe first lazily relabel it as follows: a.a a.a (.[{Int/a},{Bool/a}]x.(.y.x)x). (7) The new annotation \nin the outer . indicates that the function must be relabeled and, therefore, that we are using the particular \ninstance whose type is the one in the interface (ie, a.a) to which we apply the set of type-substitutions. \nThe relabeling will be actually prop\u00adagated to the body of the function at the moment of the reduction, \nonly if and when the function is applied (relabeling is thus lazy). However, the new annotation is statically \nused by the type system to check soundness. Notice that, unlike existing solutions, we preser ve the \nstructure of .-terms (at the expenses of some extra annotation that is propagated during the reduction) \nwhich is of the uttermost importance in a language-oriented study. In this paper we focus on the study \nof the calculus with these lazy type-substitutions annotations. We temporarily avoid the problem of local \ntype inference by defining a calculus with explicit sets of type substitutions: expressions will be explicitly \nannotated with appropriate sets of type-substitutions. Polymorphic CDuce. From a practical point of view, \nhowever, it is important to stress that, at the end, these annotations will be invisible to the programmer \nand, as we show in the second part presented in the companion paper [4], all the necessar y type\u00adsubstitutions \nwill be inferred statically. In practice, the programmer will program in the language defined by grammar \n(3), but where the types that annotate . s may contain type variables, that is, the polymorphic version \nof CDuce. The problem of inferring explicit sets of type-substitutions to annotate the polymorphic version \nof the expressions in (3) is the topic of the second part of this work presented in the companion paper \n[4]. For the time being, simply notice that the language defined by (3)and extended with type variables \npasses our first test inasmuch as the even function can be defined as follows (where s\\t is syntactic \nsugar for s.\u00act): (Int.Bool).(a\\Int.a\\Int) .x . x.Int ? (x mod 2) = 0:x (8) while with the products and \nrecursive functions described in the electronic Appendix map is defined as (see also discussion in Appendix \nE) (a.\u00df).[a].[\u00df] \u00b5mf = .[a].[\u00df](9) . . ..nil ? nil : (f(p1.),mf(p2.)) where the type nil tested in the \ntype-case denotes the singleton type that contains just the constant nil, and [a]denotes the regu\u00adlar \ntype that is the (least) solution of X = (a, X). nil. When fed by any expression of this language, the \ntype infer\u00adence system defined in the companion paper [4] will infer sets of type-substitutions and insert \nthem into the expression to make it well typed (if possible, of course). For example, for the application \n(of the terms defining) map to even, the inference system of the companion paper [4] infers the following \nset of type-substitutions [{(a\\Int)/a,(a\\Int)/\u00df},{a.Int/a,(a\\Int).Bool/\u00df}] and textu\u00adally inserts it \nbetween the two terms (so that the type-substitutions apply to the type variables of map) yielding the \ntyping in (1). Fi\u00adnally, as we explain in Section 5.3 later on, the compiler will com\u00adpile the expression \ninto an expression of an intermediate language that can be evaluated as efficiently as the monomorphic \ncalculus. Outline. The rest of the presentation proceeds as follows. In Section 3 we define and study \nour calculus with explicit type\u00adsubstitutions: we define its syntax, its operational semantics, and its \ntype system; we prove that the type system is sound and sub\u00adsumes classic intersection type systems. \nIn Section 4 we define an algorithm for type inference and prove that it is sound, com\u00adplete, and terminating. \nIn Section 5 we show that the addition of type-substitutions has in practice no impact on the efficiency \nof the evaluation since the calculus can be compiled into an intermedi\u00adate language that executes as \nefficiently as monomor phic CDuce. Section 7presents related work and in Section 8we conclude our presentation. \nIn the rest of the presentation we will focus on the intuition and try to avoid as many technical details \nas possible. We dot the i s and cross the t s in the electronic appendix (available on-line at http://dl.acm.org) \nwhere all formal definitions and complete proofs of proper ties can be found (n.b.: references in the \ntext starting by capital letters eg, Definition A.7 refer to this appendix). All these as well as other \ndetails can also be found in the third author s PhD thesis manuscript [23]. Contributions. The overall \ncontr ibution of our work is the def\u00adinition of a statically-typed core language with (i) polymorphic \nhigher-order functions for a type system with recursive types and union, intersection, and negation type \nconnectives, (ii)an efficient evaluation model, (iii) local type inference for application, and (iv)a \nlimited form of type reconstruction. The main contribution of this first part of the work is the defini\u00adtion \nof an explicitly-typed .-calculus (actually, a family of calculi) with intersection (and union and negation) \ntypes and of its efficient evaluation via the compilation into an intermediate language. From a syntactic \nviewpoint our solution is a minimal extension since it just requires to add annotations to .-abstractions \nof the untyped .-calculus (cf. Section 3.5). Although this problem has been stud\u00adied for over 20 years, \nno existing solution proposes such a minimal extension, which is of paramount importance in a programming \nlanguage-or iented study (see related works in Section 7). The technical contr ibutions are the definition \nof an explicitly typed calculus with intersection types; the proof that it subsumes existing intersection \ntype systems; the soundness of its type system, the definition of a sound, complete and terminating algorithm \nfor type inference (which as byproduct yields an intersection type proof system satisfying the Curry-Howard \nisomorphism); the definition of a compilation technique into an intermediate language that can be evaluated \nas efficiently as the monomor phic one; its extension to the so called let-polymorphism and the proof \nof the adequacy of the compilation. Local type inference for application and type reconstruction are \nstudied in the second part of this work presented in the companion paper [4].  3. A calculus with explicit \ntype-substitutions The types of the calculus are those in the grammar (2)to which we add type variables \n(ranged over by a) and, for the sake of presentation, stripped of product types. In summar y, types are \nthe regular trees coinductively generated by t ::= a |b |t . t |t . t |t . t |\u00act |0 |1 (10) and such \nthat every infinite branch contains infinitely many occur\u00adrences of type constructors. We use T to denote \nthe set of all types. The condition on infinite branches bars out ill-formed types such as t = t . t \n(which does not carry any information about the set denoted by the type) or t = \u00act (which cannot represent \nany set). It also ensures that the binary relation C . T 2 defined by t1 . t2 C ti, t1 . t2 C ti, \u00act \nC t is Noetherian (that is, strongly normalizing). This gives an induction principle on T that we will \nuse without any further explicit reference to the relation. We use var(t)to denote the set of type variables \noccur ring in a type t (see Definition A.2). A type t is said to be ground or closed if and only if var(t)is \nempty. The subtyping relation for the types in T is the one defined by Castagna and Xu [5]. For this \nwork it suffices to consider that ground types are interpreted as sets of values (n.b., just values, \nnot expressions) that have that type and subtyping is set containment (ie, a ground type s is a subtype \nof a ground type t if and only if t contains all the values of type s). In particular, s.t contains all \n.\u00adabstractions that when applied to a value of type s, if they retur n a result, then this result is \nof type t (so 0.1is the set of all functions and 1.0 is the set of functions that diverge on all arguments). \nType connectives (union, intersection, negation) are interpreted as the corresponding set-theoretic operators \nand subtyping is set containment. For what concer ns non-ground types (ie, types with variables occur \nring in them) all the reader needs to know for this work is that the subtyping relation of Castagna and \nXu is preserved by type-substitutions. Namely, if s = t, then ss = ts for every type-substitution s (the \nconverse does not hold in general, while it holds for semantic type-substitutions in convex models: see \n[5]). Two types are equivalent if they denote the same set of values, that is, if they are subtype one \nof each other (type equivalence is denoted by .). An important proper ty of this system we will often \nuse is that every type is equivalent to (and can be effectively transformed into) a type in disjunctive \nnormal form, that is, a union of uniform intersections of literals. A literal is either an arrow, or \na basic type, or a type variable, or their negations. An intersection is uniform if all the literals \nhave the same constructor, that is, either it is an intersection of arrows, type variables, and their \nnegations or it is an intersection of basic types, type variables, and their negations. In summar y, \na disjunctive normal form is a union of summands whose form is either bp . \u00acbn . aq . \u00acar (11) p.P n.N \nq.P r.N or (sp.tp). \u00ac(sn.tn). aq . \u00acar (12) p.P n.N q.P r.N When either P' or N' is not empty, we call \nthe variables aq s and ar s the top-level variables of the normal form. 3.1 Expressions Expressions \nare derived from those of CoreCDuce (with type variables in types) with the addition that sets of explicit \ntype\u00adsubstitutions (ranged over by [sj]j.J)may be applied to terms and decorate .-abstractions .i.Isi.ti \ne ::= c |x |ee |.x.e |e.t?e:e |e[sj]j.J (13) [sj]j.J and with the restriction that the type tested in \ntype-case expressions .i.Isi.ti is closed. Henceforth, given a .-abstraction .x.e we [sj]j.J call the \ntype 1i.I si.ti the interface of the function and the set of type-substitutions [sj]j.J the decoration \nof the function. We write ..i.Iti.six.e for short when the decoration is a singleton containing the empty \nsubstitution. Let e be an expression. We use fv(e)and bv(e)respectively to denote the sets of free expression \nvariables and bound expressions variables of the expression e;we use tv(e) to denote the set of type \nvariables occur ring in e (see Definition A.9). As customar y, we assume bound expression variables to \nbe pair\u00adwise distinct and distinct from any free expression variable occur\u00adring in the expressions under \nconsideration. We equate expressions up to the a-renaming of their bound expression variables. In partic\u00adular, \nwhen substituting an expression e for a variable yin an expres\u00adsion e' (see Definition A.11), we assume \nthat the bound variables of e' are distinct from the bound and free variables of e, to avoid unwanted \ncaptures. For example, (.a.a x.x)y is a-equivalent to (.a.a z.z)y. The situation is a bit more complex \nfor type variables, as we do not have an explicit binder for them. Intuitively, a type variable can be \na-converted if it is a polymorphic one, that is, if it can be instan\u00adtiated. For example, (.a.a x.x)y \nis a-equivalent to (.\u00df.\u00df x.x)y, and (.a.a x.x)y is a-equivalent to (.\u00df.\u00df x.x)y. Poly\u00ad [{Int/a}][{Int/\u00df}] \nmorphic variables can be bound by interfaces, but also by dec\u00adorations: for example, in .\u00df.\u00df x.(.a.a \ny.y)x, the a occur\u00ad [{a/\u00df}]ring in the interface of the inner abstraction is bound by the decoration \n[{a/\u00df}], and the whole expression is a-equivalent to (.\u00df.\u00df x.(.... y.y)x). If a type variable is bound \nby an outer [{./\u00df}]abstraction, it cannot be instantiated; such a variable is called monomorphic. For \nexample, the following expression .(a.a).(a.a) y.((.a.a x.x)[{Int/a}]y) is not sound (ie, it cannot be \ntyped), because a is bound at the level of the outer abstraction, not at level of the inner one. Con\u00adsequently, \nin this expression, a is monomorphic for the inner ab\u00adstraction, but polymorphic for the outer one (strictly \nspeaking, thus, the monomor phic and polymorphic adjectives apply to occur rences of variables rather \nthan variables themselves). Monomor phic type variables cannot be a-converted: .(a.a).(a.a)y.(.a.a x.x)y \nis not a-equivalent to .(a.a).(a.a)y.(.\u00df.\u00df x.x)y (but it is a\u00adequivalent to .(\u00df.\u00df).(\u00df.\u00df)y.(.\u00df.\u00df x.x)y). \nNote that the scope of polymorphic variables may include some type-substitutions [si]i.I: for example, \n((.a.a x.x)y)[Int/a] is a-equivalent to ((.\u00df.\u00df x.x)y)[Int/\u00df]. Finally, we have to be careful when per\u00adforming \nexpression substitutions and type-substitutions to avoid clashes of polymorphic variable namespaces. \nFor example, substi\u00adtuting .a.a z.z for y in .a.a x.x y would lead to an unwanted capture of a (assuming \na is polymorphic, that is, not bound by a .-abstraction placed above these two expressions), so we have \nto a-convert one of them, so that the result of the substitution is, for instance, .a.a x.x (.\u00df.\u00df z.z). \nTo resume, we assume polymorphic variables to be pairwise distinct and distinct from any monomor phic \nvariable in the expres\u00ad  (subsum) (appl) (abstr) . ;G f e : t1 t1=t2 . ;G f e1 : t1.t2 . ;G f e2 : t1 \n. . var(.i.I,j.Jtisj.sisj);G,(x : tisj)f e@[sj]: sisj i . I . ;G f e : t2 . ;G f e1e2 : t2 .i.Iti.si \nj. J . ;G f .x.e : tisj . sisj [sj]j.J i.I,j.J (case) ' (var) ' { t = \u00act . . ;G f e1 : s (inst) (inter) \n. ;G f e : t ' t = t . . ;G f e2 : s . ;G f e : t s r . .j. J. . ;G f e[sj]: tj |J|> 1 . ;G f x : G(x) \n. ;G f (e.t?e1 :e2): s . ;G f e[s]: ts . ;G f e[sj]j.J : tj j.J Figure 1. Static semantics sions under \nconsideration. We equate expressions up to a-renaming of their polymorphic variables. In particular, \nwhen substituting an expression e ' for a variable x in an expression e, we suppose the polymorphic type \nvariables of e to be distinct from the monomor\u00adphic and polymorphic type variables of e ' thus avoiding \nunwanted captures. Detailed definitions are given in Appendix A.2. In order to define both static and \ndynamic semantics for the expressions above, we need to define the relabeling operation @ which takes \nan expression e and a set of type-substitutions [sj]j.J and pushes [sj]j.J to all outermost .-abstractions \noccur ring in e (and collects and composes with the sets of type-substitutions it meets). Precisely, \ne@[sj]j.J is defined for .-abstractions and (inductively) for applications of type-substitutions as: \n.i.Iti.si def .i.Iti.si (.x.e)@[sj]j.J = .x.e [sk]k.K [sj]j.J.[sk]k.K def (e[si]i.I)@[sj]j.J = e@([sj]j.J \n. [si]i.I) where . denotes the pairwise composition of all substitutions of the two sets (see Definition \nA.7). It erases the set of type-substitutions when e is a variable and it is homomor phically applied \non the remaining expressions (see Definition A.12).  3.2 Operational semantics The dynamic semantics \nis given by the following three notions of reduction (where v ranges over values, that is, constants \nand .\u00adabstractions), applied by a leftmost-outermost strategy: e[sj]j.J e@[sj]j.J (14) .i.Iti.si (.x.e)v \n(e@[sj]j.P){v/x} (15) [sj]j.J { e1 if f v : t v.t?e1 :e2 (16) e2 otherwise def where in (15)we have P \n= {j.J |.i.I, f v : tisj}. The first rule (14)performs relabeling, that is, it propagates the sets of \ntype-substitutions down into the decorations of the outer most .-abstractions. The second rule (15) states \nthe semantics of applications: this is standard call-by-value \u00df-reduction, with the difference that the \nsubstitution of the argument for the parameter is performed on the relabeled body of the function. Notice \nthat relabeling depends on the type of the argument and keeps only those substitutions that make the \ntype of the argument v match (at least one of) the input types defined in the interface of the function \n(ie, the set P which contains all substitutions sj such that the argument v has type tisj for some i \nin I: the type system will ensure that P is never empty). For instance, take the daffy identity function, \ninstantiate it as in (7) by both Int and Bool, and apply it to 42 ie, (.a[{.Inta /a},{Bool/a}]x.(.a.a \ny.x)x)42 , then it reduces to (.a.a y.42)42, (which is observationally equivalent [{Int/a}] to (.Int.Int \ny.42)42)since the reduction discards the {Bool/a}substitution. Finally, the third rule (16)checks whether \nthe value returned by the expression in the type-case matches the specified type and selects the branch \naccordingly. The reader may think that defining a functional language in which each \u00df-reduction must \nperform an involved relabeling op\u00aderation, theoretically interesting though it may be, will result in \npractice too costly and therefore unrealistic. This is not so. In Sec\u00adtion 5 we show that this reduction \ncan be implemented as effi\u00adciently as in CDuce. By a smart definition of closures it is pos\u00adsible to \ncompute relabeling in a lazy way and materialize it only in a very specific case for the reduction of \nthe type-case (ie, to perform a type-case reduction (16)where the value v is a func\u00adtion whose interface \ncontains monomorphic type variables and it is the result of the partial application of a polymorphic \nfunction) while all other reductions for applications can be implemented as plain classic \u00df-reduction. \nFor instance, to evaluate the expressions (.a.a x.(.a.a y.x)x)42 above, we can completely dis\u00ad [{Int/a},{Bool/a}]regard \nall type annotations and decorations and perform a couple of standard \u00df reductions that yield the result \n42.  3.3 Type system As expected in a calculus with a type-case expression, the dynamic semantics depends \non the static semantics precisely, on the typ\u00ading of values. The static semantics of our calculus is \ndefined in Figure 1. The judgments are of the form . ;G f e : t, where e is an expression, t a type, \nG a type environment (ie, a finite mapping from expression variables to types), and . a finite set of \ntype vari\u00adables. The latter is the set of all monomorphic type variables, that is, the variables that \noccur in the type of some outer .-abstraction and, as such, cannot be instantiated; it must contain all \nthe type variables occurring in G. The rules for application and subsumption are standard. In the latter, \nthe subtyping relation is the one defined in [5]. We just omitted the rule for constants (which states \nthat c has type bc). The rule for abstractions applies each substitution specified in the decoration \nto each arrow type in the interface, adds all the variables occur ring in these types to the set of monomor \nphic type variables ., and checks whether the function has all the resulting types. Namely, it checks \nthat for every possible input type, the (relabeled) body has the corresponding output type. To that end, \nit applies each substitution sj in the decoration to each input type ti of the interface and checks that, \nunder the hypothesis that x has type tisj, the function body relabeled with the substitution sj at issue \nhas type sisj (notice that all these checks are performed under the same updated set of monomor phic \ntype variables, that is, . . var(.i.I,j.Jtisj.sisj)). If the test succeeds, then the rule infers for \nthe function the type obtained by applying the set of substitutions of the decoration to the type of \nthe interface. For example, in the case of the instance of the daffy identity function given in (7), \nthe . is always empty and the rule checks whether under the hypothesis x : a{Int/a}(ie, x : Int), it \nis possible to deduce that (.a.a y.x)x@[{Int/a}] has type a{Int/a}(ie, that (.Int.Int y.x)x : Int), and \nsimilarly for the substitution {Bool/a}. The type deduced for the function is then (Int . Int).(Bool \n. Bool). The relabeling of the body in the premises of the rule (abstr) is a key mechanism of the type \nsystem: had we used e[sj] instead of e@[sj] in the premises of the (abstr)rule, the expression (7) could \nnot be typed. The reason is that e[sj]is more demanding on typing than e@[sj], since the well typing \nof e is necessary to the well-typing of the former but not to that of the latter. Indeed while under \nthe hypothesis x : Int we just showed that ((.a.a y.x)x)@[{Int/a}] ie, ((.Int.Int y.x)x) is well\u00adtyped, \nthe term ((.a.a y.x)x)[{Int/a}] is not, for (.a.a y.x) does not have type a.a. The rule for abstractions \nalso justifies the need for an explicit set . for monomor phic type variables while, for instance, in \nML it suffices to consider monomorphic type variables that occur in the image of G [15]: when checking \nan arrow of the interface of a function, the variables occur ring in the other arrows must be considered \nmonomor phic, too.  To type the applications of a set of type-substitutions to an ex\u00adpression, two different \nrules are used according to whether the set contains one or more than one substitution. When a single \nsubsti\u00adtution is specified, the rule (inst) instantiates the type according to the specified substitution, \nprovided that s does not substitute vari\u00adables in . (ie, dom(s)n . = \u00d8, noted sr.). If more than one \nsubstitution is specified, then the rule (inter) composes them by an intersection. Finally, the (case)rule \nfirst infers the type t ' of the expression whose type is tested. Then the type of each branch ei is \nchecked only if there is a chance that the branch can be selected. Here the use of = is subtle but crucial \n(it allows us to existentially quantify over type-substitutions). The branch, say, e1 can be selected \n(and therefore its well-typedness must be checked) only if e can retur n a value that is in t. But in \norder to cover all possible cases we must also consider the case in which the type of e is instantiated \nas a consequence of an outer application. A typical usage pattern (followed also by our even function) \nis .a.... x. x.Int ? e1 :e2: the branch e1 is selected only if the function is applied to a value of \ntype Int, that is, if the type a of x is instantiated to Int (notice that when typing the body of the \nfunction . contains only a). More generally, the branch e1 in e.t?e1 :e2 can be selected only if e can \nreturn a value in t, that is to say, if there exists a substitution s for any type variables even those \nin . such as the intersection of t ' s and t is not empty (t is a closed, so ts = t). Therefore, in order \nto achieve maximum precision the rule (case)must check . ;G f e1 : s only if there exists s such that \nt ' s.t = 0. Since t ' = \u00act (strictly) implies that for all substitutions s, t ' s=\u00act (recall that t \nis a closed type), then by the contrapositive the existence of a substitution s such that t ' s = \u00act \nimplies t ' =\u00act. The latter is equivalent to t ' . t = 0: the intersection of t and t ' is not empty. \nSo we slightly over-approximate the test of selection and check the type of e1 under the weaker hypothesis \nt ' .t = 0which ensures that the typing will hold also under the stronger (and sought) hypothesis that \nthere exists s such that t ' s .t = 0(the difference only matters with some specific cases involving \nindivisible types: see [5]). Notice that explicit type-substitutions are only needed to type applications \nof polymorphic functions. Since no such application occurs in the bodies of map and even as defined in \nSection 2 (the m and f inside the body of map are abstracted variables and, thus, have monomor phic types), \nthen they can be typed by this system as they are (as long as they are not applied one to the other there \nis no need to infer any set of type-substitutions). So we can already see that our language passes the \nsecond test, namely, that map and even have the types declared in their signatures. Let us detail just \nthe most interesting case, that is, the typing of the term even defined in equation (8) (even though \nthe typing of the type-case in (9), the term defining map, is interesting, as well). According to the \nrule (abstr)we have to check that under the hypothesis x:Int the expression x.Int ? (x mod 2) = 0:x has \ntype Bool, and that under the hypothesis x :a\\Int the same expression has type a\\Int. So we have two \ndistinct applications of the (case)rule. In one x is of type Int, thus the check Int = Int fails, and \ntherefore only the first branch, (x mod 2) = 0, is type checked (the second is skipped). Since under \nthe hypothesis x :Int the expression (x mod 2) = 0 has type Bool, then so has the whole type-case expression. \nIn the other application of (case), x is of type a\\Int, so the test a\\Int = \u00acInt clearly fails, and only \nthe second branch is checked (the first is skipped). Since this second branch is x, then the whole type-case \nexpression has type a\\Int, as expected. This example shows two important aspects of our typing rules. \nFirst, it shows the importance of . to record monomor phic variables, since it may contain some variables \nthat do not occur in G. For instance, when typing the first branch of even, the type environment contains \nonly x : Int but . is {a}and this forbids to consider a as polymorphic (if we allowed to instantiate \nany variable that does not occur in G, then the term obtained from the even function (8) by replacing \nthe first branch by (.a.a y.y)[{Bool/a}]true would be well-typed, which is wrong since a is monomorphic \nin the body of even). Second, this example shows why if in some application of the (case)rule a branch \nis not checked, then the type checking of the whole type-case expression must not necessar ily fail: \nthe well-typing of this branch may be checked under different hypothesis (typically when occur ring in \nthe body of an overloaded function).4 The reader can refer to Section  3.3 of [11] for a more detailed \ndiscussion on this point. Finally, notice that the rule (subsum) makes the type system de\u00adpendent on \nthe subtyping relation = defined in [5]. It is impor\u00adtant not to confuse the subtyping relation = of \nour system, which denotes semantic subtyping (ie, set-theoretic inclusion of denota\u00adtions), with the \none typically used in the type reconstruction sys\u00adtems for ML, which stands for type variable instantiation. \nFor ex\u00adample, in ML we have a . a = Int . Int (because Int . Int is an instance of a . a). But this is \nnot true in our system, as the relation must hold for every possible instantiation of a, thus in par\u00adticular \nfor a equal to Bool. In the companion paper [4]we define the preorder .. which includes the type variable \ninstantiation of the preorder typically used for ML, so any direct compar ison with constraint systems \nfor ML types should focus on .. rather than = and it can be found in the companion paper [4]. 3.4 Type \nsoundness Subject reduction and progress properties hold for this system. Theorem 3.1 (Subject Reduction). \nFor every term e and type t, if G f e : t and e e ', then G f e ' : t. Theorem 3.2 (Progress). Let e \nbe a well-typed closed term. If e is not a value, then there exists a term e ' such that e e ' . The \nproofs of both theorems, though unsur prising, are rather long and technical and can be found in Appendix \nB.2. They allow us to conclude that the type system is sound. Corollary 3.3 (Type soundness). Let e be \na well-typed closed expression, that is, f e : t for some t. Then either e diverges or it returns a value \nof type t.  3.5 Expressing intersection type systems We can now state the first stand-alone theoretical \ncontr ibution of our work. Consider the sub-calculus of our calculus in which type\u00adsubstitutions occur \nonly in decorations and without constants and 4From a programming language point of view it is important \nto check that during type checking every branch of a given type-case expression is checked ie, it can \nbe selected at least once. This corresponds to checking the absence of redundant cases in pattern matching. \nWe omitted this check since it is not necessar y for formal development.  (ALG-VAR) (ALG-INST) (ALG-APPL) \n. ;G fA e : t . ;G fA e1 : t . ;G fA e2 : s t = 0 . 1 sjr . s = dom(t) . ;G fA x : G(x) . ;G fA e1e2 \n: t \u00b7 s . ;G fA e[sj]j.J : tsj (ALG-ABSTR) j.J (ALG-CASE-FST) '' ' . . . ;G,(x : tisj)fA e@[sj]: sij \n. ' = var(.i.I,j.Jtisj.sisj) . ;G fA e : t . ;G fA e1 : s1 ' t ' =t .i.Iti.si s = sisj, i.I, j.J ij . \n;G fA (e.t?e1 :e2): s1 . ;G fA .x.e : (tisj . sisj) [sj]j.J i.I,j.J (ALG-CASE-SND) (ALG-CASE-BOTH) . \n;G fA e : t ' . ;G fA e2 : s2 . ;G fA e : t ' . ;G fA e1 : s1 . ;G fA e2 : s2 t ' =\u00act ' t =\u00act t ' =t \n . ;G fA (e.t?e1 :e2): s2 . ;G fA (e.t?e1 :e2): s1 . s2 Figure 2. Typing algorithm type-case expressions, \nthat is, .i.Isi.ti e ::= x |ee |.x.e (17) [sj]j.J and whose types are inductively produced by the grammar \nt ::= a |t . t |t . t This calculus is closed with respect to \u00df-reduction as defined by the reduction \nrule (15) (without type-cases, union and negation types are not needed). It constitutes an explicitly-typed \n.-calculus with intersection types whose expressive power subsumes that of classic intersection type \nsystems (without an universal element ., of course), as expressed by the following theorem. Theorem 3.4. \nLet fBCD denote provability in the Barendregt, Coppo, and Dezani system [1], and Iel be the pure .-calculus \nterm obtained from e by erasing all types occurring in it. If fBCD m : t, then .e such that f e : t and \nIel = m. Therefore, this sub-calculus solves a longstanding open problem, that is the definition of explicit \ntype annotations for .-terms in in\u00adtersection type systems, without any further syntactic modification. \nSee Section 7on related work for an extensive compar ison. The proof of Theorem 3.4 is constructive (cf., \nAppendix B.3). Therefore we can transpose decidability results of intersection type systems to our system. \nIn particular, type reconstruction for the sub\u00adcalculus (17) is undecidable and this implies the undecidability \nof type reconstruction for the whole calculus without recursive types (with recursive types type reconstruction \nis trivially decidable since every .-term can be typed by the recursive type \u00b5X.(X.X).*). In Section \n4we prove that type inference for our system is decid\u00adable. The problem of reconstructing type-substitutions \n(ie, given a term of grammar (3), deciding whether it is possible to add sets of type-substitutions in \nit so that it becomes a well-typed term of our calculus) is dealt with in the companion paper [4]. To \ncompare with existing intersection type systems, the calculus in (17) includes neither type-cases nor \nexpressions of the form e[sj]j.J. While it is clear that type-cases increase the expressive power of \nthe calculus, one may wonder whether the same is true for e[sj]j.J. This is not so as expressions e[sj]j.J \nare redundant. Consider the subcalculus whose terms are .i.Isi.ti e ::= x |ee |.x.e |e.t?e :e (18) [sj]j.J \nthat is, the calculus in which sets of type-substitutions appear only in decorations. Consider the embedding \n[.] of our calculus (13) into this subcalculus, defined as [e[sj]j.J] = e@[sj]j.J, as the identity for \nvariables, and as its homomor phic propagation for all the other expressions. Then it is easy to prove \nthe following theorem Theorem 3.5. For every well-typed expression e: 1. e * v . [e] * [v], * *' 2. [e] \nv . e v and v = [v ' ] meaning that the subcalculus defined above is equivalent to the full calculus. \nAlthough expressions of the form e[sj]j.J do not bring any further expressive power, they play a crucial \nrole in local type inference, which is why we included them in our calculus. As we explain in details \nin the companion paper, for local type inference we need to reconstruct sets of type-substitutions that \nare applied to expressions but we must not reconstruct sets of type-substitutions that are decorations \nof .-expressions. The reason is pragmatic and can be shown by consider ing the following two terms: (.a.a \nx.x)3 and (.a.a x.4)3. Every functional programmer will agree that the first expression must be considered \nwell-typed while the sec\u00adond must not, for the simple reason that the constant function (.a.a x.4)does \nnot have type a . a. Indeed in the first case it is possible to apply a set of type-substitutions that \nmakes the term well typed, namely (.a.a x.x)[{Int/a}]3, while no such applica\u00adtion exists for the second \nterm. However, if we allowed reconstruc\u00adtion also for decorations, then the second term could be made \nwell typed by adding the following decoration (.a.a x.4)3. [{Int/a}]  4. Typing algorithm The rules \nin Figure 1do not describe a typing algorithm since they are not syntax directed. As customary the problem \nis the subsump\u00adtion rule, and the way to go is to eliminate this rule by embedding appropriate checks \nof the subtyping relation into the rules that need it. This results in the system formed by the rules \nof Figure 2. This system is algorithmic (as stressed by fA ): in every case at most one rule applies, \neither because of the syntax of the term or because of mutually exclusive side conditions. Subsumption \nis no longer present and, instead, subtype checking has been pushed in all the remaining rules. The rule \nfor type-cases has been split in three rules (plus a fourth uninteresting rule we omitted that states \nthat when e : 0 ie, it is the provably diverging expression then the whole type-case expression has type \n0) according to whether one or both branches can be selected. Here the only modification is in the case \nwhere both branches can be selected: in the rule (case)in Figure1the types of the two branches were subsumed \nto a common type s, while (ALG-CASE-BOTH) returns the least upper bound (ie, the union) of the two types. \nThe rule for abstractions underwent a minor modification with respect to the types returned for the body, \nwhich before were sub\u00adsumed to the type declared in the interface while now the subtyping relation sij \n' = sisj is explicitly checked. The elimination of the subsumption yields a simplification in typing \nthe application of type-substitutions, since in the system of Figure 1 without subsumption every premise \nof an (inter)rule is the consequence of an (inst) rule. The two rules can thus be merged into a single \none, yielding the (ALG-INST)rule (see Appendix C.1 and in particular Theorem C.1). As expected, the core \nof the typing algorithm is the rule for application. In the system of Figure 1, in order to apply the \n(appl) rule, the type of the function had to be subsumed to an arrow type, and the type of the argument \nhad to be subsumed to the domain of that arrow type; then the co-domain of the arrow is taken to type \nthe application. In the algorithmic rule (ALG-APPL), this is done by the type meta-operator \u00b7 which is \nformally defined as  def follows: t \u00b7 s = min{u |t = s.u}. In words, if t is the type of the function \nand s the type of the argument, this operator looks for the smallest arrow type larger than t and with \ndomain s, and it returns its co-domain. More precisely, when typing e1e2, the rule (ALG-APPL)checks that \nthe type t of e1 is a functional one (ie, t = 0.1). It also checks that the type s of e2 is a subtype \nof the domain of t (denoted by dom(t)). Because t is not necessarily an arrow type (in general, it is \nequivalent to a disjunctive normal form like the one of equation (12) in Section 3), the definition of \nthe domain is not immediate. The domain of a function whose type is an intersection of arrows and negation \nof arrows is the union of the domains of all positive literals. For instance the domain of a function \nof type (Int.Int). (Bool.Bool)is Int . Bool, since it can be equally applied to integer or Boolean arguments, \nwhile the domain of even as defined in (8)is Int . (a\\Int), that is Int . a. The domain of a union of \nfunctional types is the intersection of each domain. For instance an expression of type (s1.s2). (t1.t2)will \nretur n either a function of type s1.s2 or a function of type t1.t2, so this expression can be applied \nonly to arguments that fit both cases, that is, to arguments in s1 . t1. Formally, if t = 0.1, then t \nvi.I(1p.Pi(sp.tp). 1\u00ac(sn.tn). 1aq . 1\u00ac\u00dfr)(with all the Pi s n.Ni q.Qi r.Ri def not empty), and therefore \ndom(t) = 1i.I vp.Pi sp (here type variables do not count since they are intersected and universally quantif \nied so the definition of the domain must hold also when their intersection is 1). Finally, the type returned \nin (ALG-APP) is t \u00b7 s, which we recall is the smallest result type that can be obtained by subsuming \nt to an arrow type compatible with s. We can prove that for every type t such that t = 0.1 and type s \nsuch that s = dom(t), the type t \u00b7 s exists and can be effectively computed (see Lemma C.12). The algorithmic \nsystem is sound and complete with respect to the type system of Figure 1 and satisfies the minimum typing \nproperty (see Appendix Cfor the proofs). Theorem 4.1 (Soundness). If . ;G fA e : t, then . ;G f e : t. \nTheorem 4.2 (Completeness). If . ;G f e : t, then there exists a type s such that . ;G fA e : s and s \n= t. Corollary 4.3 (Minimum typing). If . ;G fA e : t, then t = min{s |. ;G f e : s}. Finally, it is \nquite easy to prove that type inference is decidable. v ::= duction semantics of polymorphic expressions \nincludes a run-time relabeling operation. The key observation that allows us to define an efficient execution \nmodel for the polymorphic calculus is that relabeling can be implemented lazily so that the only case \nin which relabeling is computed at run-time will correspond to testing the type of a partial application \nof a polymorphic function. In practice, this case is so rare at least in the XML setting that there is \nno difference between monomor phic and polymorphic evaluation. 5.1 Monomorphic Language Let us start \nby recalling the execution model of monomorphic CDuce, which is a classic closure-based evaluation. Expressions \nand values are defined as e ::= c |x |.t x.e |ee |e . s ?e : e v ::= c |(.t x.e, E) where t denotes an \nintersection of arrow types, s denotes a closed type, and E denotes an environment, that is, a substitution \nmapping expression variables into values. The big step semantics is: (ME-CONST) (ME-VAR) (ME-CLOSURE) \nE fm c . c E fm x . E(x) E fm .t x.e . (.t x.e, E) (ME-APPLY) ' ' E fm e1 . (.t ) x.e, E E fm e2 . v0 \nE , x . v0 fm e . v E fm e1e2 . v (ME-TYPE CASE T) E fm e1 . v0 v0 .m t E fm e2 . v E fm e1 . t ?e2 : \ne3 . v (ME-TYPE CASE F) E fm e1 . v0 v0 .m t E fm e3 . v t.s E fm e1 . t ?e2 : e3 . v To complete the \ndefinition we define the relation v .m t, that is, membership of a (monomor phic) value to a (monomor \nphic) type: def c .m t .. bc = t def (.s x.e, E) .m t .. s = t where = is the subtyping relation of CDuce \n[11].  5.2 Polymorphic Language In the naive extension of this semantics to the explicitly-typed polymorphic \ncalculus of Section 3, we deal with type-substitutions as we do for environments, that is, by storing \nthem in closures. This is reflected by the following definition where, for brevity, we write sI to denote \nthe set of type-substitutions [si]i.I: I ||c x s c |(.t x.e |ee |e . t ?e : e |esI x.e, E,sI) ::= e It \nsuffices to observe that the algorithmic rules strictly reduce the I The big-step semantics is then defined \nas follows, where each ex\u00adsize of the expressions. Theorem 4.4 (Termination). Let e be an expression. \nThen the type inference algorithm for e terminates. pression is evaluated with respect to an environment \nE determining the current value substitutions and a set of type-substitutions sI: (PE-CONST) (PE-VAR) \nsI;E fp c . c sI;E fp x . E(x) This system constitutes a further theoretical contribution of our work \nsince with this type system the language defined by gram-(PE-INSTANCE) (PE-CLOSURE) mar (13), the one \nby grammar (18), and a fortiori the one by gram-sI . sJ;E fp e . v sI;E fp .t x.e . (.t mar (17)are intersection \ntype systems that all satisfy the Curry- Howard isomorphism since there is a one-to-one correspondence \nbetween terms and proofs of the algorithmic system.  5. Evaluation In this section we define an efficient \nexecution model for the polymorphic calculus as a conser vative extension of the execu\u00adtion model of \nthe monomor phic calculus: by efficient we mean that monomor phic expressions will be evaluated as efficiently \nas in the original CDuce runtime. In fact, even polymorphic expres\u00adsions will be evaluated as efficiently \nas well (as if type variables were basic monomor phic types) despite the fact that the formal re\u00ad s \ns J J (PE-APPLY) .l.Lsl.tl ' sI;E fp e1 . (.sK x.e, E ,sH) sI;E fp e2 . v0 sJ = sH . sK P = {j. J |.l \n. L : v0 .p slsj} ' sP;E , x . v0 fp e . v sI;E fp esJ . v sI;E fp e1e2 . v (PE-TYPE CASE T) sI;E fp \ne1 . v0 v0 .p t sI;E fp e2 . v sI;E fp e1 . t ?e2 : e3 . v (PE-TYPE CASE F) sI;E fp e1 . v0 v0 .p t sI;E \nfp e3 . v sI;E fp e1 . t ?e2 : e3 . v x.e, E,sI)  The membership relation v .p t for polymorphic values \nis induc\u00adtively defined as: def c .p t .. bc = t def (.s x.e, E,sI) .p t .. s(sI . sJ)= t sJ where = \nis the subtyping relation of Castagna and Xu [5]. It is not difficult to show that this big-step semantics \nis equivalent to the small-step one of Section 3. Let (.)be the transformation that maps values of the \npolymorphic language into corresponding values of the calculus, that is (c)= c and ((.s x.e, E,sI))= \n.s x.(e(E)) (19) sJ sI.sJ where (E) applies (.) to all the values in the range of E. Let i denote the \nsingleton set containing the empty type-substitution [{}], which is the neutral element of the composition \nof sets of type-substitutions. Then we have: Theorem 5.1. Let e be a well-typed closed explicitly-typed \nexpres\u00adsion (fA e : t). Then: i;\u00d8 fp e . v .. e * (v). This implementation has a significant computational \nburden compared to that of the monomor phic language: first of all, each ap\u00adplication of (PE-APPLY) computes \nthe set P, which requires to im\u00adplement several type-substitutions and membership tests; second, each \napplication of (PE-INSTANCE) computes the composition of two sets of type-substitutions. In the next \nsection we describe a dif\u00adferent solution consisting in the compilation of the explicitly typed calculus \ninto an intermediate language so that these computations are postponed as much as possible and are performed \nonly if and when they are really necessar y. 5.3 Intermediate Language The intermediate language into \nwhich we compile the explicitly\u00adtyped polymorphic language is very similar to the monomor phic version. \nThe only difference is that .-abstractions (both in expres\u00adsions and closures) may contain type variables \nin their interface and have an extra decoration S which is a term denoting a set of type-substitutions. \ne ::= c |x |.t ee |e . t ?e : Sx.e |e v ::= c |(.S t x.e, E)S ::= sI |comp(S,S ' )|sel(x, t, S) Intuitively, \na comp(S,S ' ) term corresponds to an application of the . composition operator to the sets of type substitutions \ndenoted by S and S ', while a sel(x, t, S)term selects the subset of type substitutions s denoted by \nS that are compatible with the fact that (the value instantiating) x belongs to the domain of ts. The \nbig step semantics for this intermediate language is: (OE-CONST) (OE-VAR) (OE-CLOSURE) E fo c . c E fo \nx . E(x) E fo .t Sx.e, E) Sx.e . (.t (OE-APPLY) E fo e1 . (.t ' ) Sx.e, E E fo e2 . v0 E ' , x . v0 fo \ne . v E fo e1e2 . v (OE-TYPE CASE T) E fo e1 . v0 v0 .o t E fo e2 . v E fo e1 . t ?e2 : e3 . v (OE-TYPE \nCASE F) E fo e1 . v0 v0 .o t E fo e3 . v E fo e1 . t ?e2 : e3 . v Notice that this semantics is structurally \nthe same as that of the monomor phic language. There are only two minor differences: (i) .-abstractions \nhave an extra decoration S (which has no impact on efficiency since it corresponds in the implementation \nto manip\u00adulate descriptors with an extra field) and (ii) the corresponding (_E-TYPE CASE)rules use a \nslightly different relation: .o instead of .m. It is thus easy to see that in terms of steps of reduction \nthe two semantics have the same complexity. What changes is the test of the membership relation (.o rather \nthan .m) since, when the value to be tested is a closure, we need to materialize relabelings. In other \nwords, we have to evaluate the S expression decorating the function and apply the resulting set of substitutions \nto the interface of the function. Formally: c .o t defbc .. = t (.s def Sx.e, E) .o t .. s(eval(E,S)) \n= t where the evaluation of the symbolic set of type-substitutions is inductively defined as eval(E,sI)= \nsI eval(E,comp(S,S ' )) = eval(E,S). eval(E,S ' ) eval(E,sel(x, 1i.I ti.si,S)) = [sj . eval(E,S)|.i.I \n: E(x).o tisj ] Notice in the last rule the crucial role played by x and E: by using an expression variable \nx in the symbolic representation of type-substitutions and relying on its interpretation through E, we \nhave transposed to type-substitutions the same benefits that clo\u00adsures bring to value substitutions: \njust as closures allow value\u00adsubstitutions to be mater ialized only when a formal parameter is used rather \nthan at the moment of the reduction, so our technique allows type-substitutions to be materialized only \nwhen a type vari\u00adable is effectively tested, rather than at the moment of the reduction. It is easy to \nsee that the only case in which the computation of .o is more expensive than that of .m is when the value \nwhose type is tested is a closure (.t Sx.e, E) in which t is not closed and S is not i. 5 The S decoration \nis different from ionly if the closure is the result of a partial application of a curried function. \nThe type t is not closed only if such partial application yielded a polymorphic func\u00adtion. In conclusion, \nthe evaluation of an expression in the polymor\u00adphic language is more expensive than the evaluation of \na similar6 expression of the monomor phic language only if it tests the type of a polymorphic function \nresulting from the partial application of a polymorphic curried function. The additional overhead is \nlimited only to this particular test and in all the other cases the evaluation is as efficient as in \nthe monomor phic case. Finally, it is important to stress that this holds true also if we add product \ntypes: the test of a pair of values in the polymorphic case is as expensive as in the monomor phic case \nand so is the rest of the evaluation. Since in the XML setting the vast majority of the computation time \nis spent in testing products (since they encode sequences, trees, and XML el\u00adements), then the overhead \nbrought by adding polymorphism ie, the overhead due to testing the type of a polymorphic partial appli\u00adcation \nof a polymorphic curried function is negligible in practice. All that remains to do is to define the \ncompilation of the explicitly-typed language into the intermediate language: = x [x]S .t [.t sI x.e]S \n= comp(S,sI)x.[e]sel(x,t,comp(S,sI)) = [e1e2]S [e1]S[e2]S [esI]S = [e]comp(S,sI) [e1 . t ?e2 : e3]S = \n[e1]S . t ?[e2]S : [e3]S Given a closed program e we compile it in the intermediate lan\u00adguage as [e]. \nIn practice, the compilation will be even simpler . since we apply it only to expressions generated by \nlocal type in\u00adference algorithm described in the companion paper where all . s are decorated by i (cf. \ndiscussion at the end of Section 3.5). 5To be more precise, when there exists a substitution s . eval(E,S)such \nthat var(t)n dom(s) = \u00d8. Notice that the tests of the subtyping relation for monomor phic and polymorphic \ntypes have the same complexity [5]. 6By similar we intend with the same syntax tree but only closed types. \n So the second case of the definition simplifies to: [.t x.e]S = .t Sx.[e]sel(x,t,S). The compilation \nis adequate: Theorem 5.2. Lete be a well-typed closed explicitly-typed expres\u00adsion (fA e : t). Then i;\u00d8 \nfp e . v .. fo [e] . v ' , with (v)= (v ' ). Sx.e, E)) evaluates all the symbolic expressions and type-substitutions \nin the term (see (19)). By combining Theo\u00adrems 5.1 and 5.2 we obtain the adequacy of the compilation: \nwhere ((.t Corollary 5.3. Lete be awell-typed closed explicitly-typed expres\u00adsion (fA e : t). Then fo \n[e] . v .. e * (v).  5.4 Let-polymorphism Afunction is polymorphic if it can be safelyapplied to arguments \nof different types. The calculus presented supports a varied palette of different forms of polymorphism: \nit uses subtype polymorphism (a function can be applied to arguments whose types are subtypes of its \ndomain type), the combination of intersection types and type\u00adcase expressions yields ad hoc polymorphism \n(aka overloading), and finallythe use of type variables in function interfaces provides parametric polymorphism. \nPolymorphism is interesting when used with bindings: instead of repeating the definition of a function \nev\u00adery time we need to apply it, it is more convenient to define the function once, bind it to an expression \nvariable, and use the vari\u00adable everytime weneed to applythe function. In the current system it is possible \nto combine binding only with the first two kinds of polymorphism: different occurrences of a variable \nbound to a func\u00adtion can be given different types thus, be applied to arguments of different types , \neither bysubsumption (ie, byassigning to the variable a super-type of the type of the function it denotes) \nor byin\u00adtersection elimination (ie,byassigning to avariable one of the types that form the intersection \ntype of the function it denotes). However, as it is well known in the languages of the ML-family, in \nthe cur\u00adrent setting it is not possible to combine binding and parametric polymorphism. Distinct occurrences \nof a variable cannot be given different types by instantiation (ie, by assigning to the variable a type \nwhich is an instance of the type of the function it denotes). In other terms, all .-abstracted variables \nhave monomorphic types (with respect to parametric polymorphism), which is why in ML auto-application \n.x.xx is not typeable. The solution is well known and consists in introducing let bindings. This amounts \nto defining a new class of expression vari\u00adables so that variables introduced byalet havepolymorphic \ntypes, that is, types that have been generalized at the moment of the def\u00adinition and can be instantiated \nin the body of the let. To sum up, .-abstracted variables have monomorphic types, while let-bound variables \n(may) have polymorphic types and, thus, be given differ\u00adent types obtained byinstantiation. For shortwe \ncall the former .\u00adabstracted variables monomorphic (expression) variables and the latter let-bound variables \npolymorphic (expression) variables . In the explicitly-typed calculi of the previous sections we had \njust.-abstracted variables. That these variables havemonomorphic types is clearly witnessed by the fact \nthat, operationally, xsI is equivalent to (ie, reduces to) x. Clearlythis property mustnot hold for polymorphic \ntype variables since let x = (.a.a y.y)in (x[{a.a/a}])x (20) is, intuitively, well typed, while the same \nterm obtained byreplac\u00ading x[{a . a/a}]with x is not (see the extension of the definition of relabeling \nfor polymorphic variables later on). To enable the definition of polymorphic functions to our calcu\u00adlus \nwe add a let expression. To ease the presentation and to stress that the addition of let-bindings is \na conservative extension of the previous system, wesyntacticallydistinguish the current monomor\u00adphic \nvariables (ie, those abstracted bya .)from polymorphic vari\u00ad ables byunderlining the latter ones. e ::= \n\u00b7 \u00b7 \u00b7 |x |let x = e in e Reduction is as usual: let x = v in e e{v/x} Relabeling is extended bythe following \ndefinitions def x@[sj]j.J = x[sj]j.J def (let x= e ' ine)@[sj]j.J = let x= (e ' @[sj]j.J)in(e@[sj]j.J) \nand the (algorithmic) typing rule is as expected: . ;G f(A) e1 : t1 . ;G,(x : t1)f(A) e2 : t2 (let) . \n;G f(A) let x = e1 in e2 : t2 Type environments G now map also polymorphic expression vari\u00adables into \ntypes. Notice that for a polymorphic expression variable x it is no longer true that var(G(x)) . . (not \nadding var(G(x)) to . corresponds to generalizing the type of x before typing e2 as in the GEN rule of \nthe Damas-Milner algorithm: cf. [15]). As before we assume that polymorphic type variables of a let-expression \n(in particular those generalized for let-polymorphism) are distinct from monomorphic and polymorphic \ntype variables of the context that the let-expression occurs in. Likewise, environments now map both \nmonomorphic and poly\u00admorphic expression variables into values so that the rules for eval\u00aduation in Section \n5.2 are extended with the following ones: (PE-PVARf) (PE-PVARc) E(x)= c E(x)= (.t y.e, E ' ,sJ) sI;E \nfp x . c sI;E fp x . (.t y.e, E ' ,sI . sJ) (PE-LET) sI;E fp e1 . v0 sI;E,x . v0 fp e2 . v sI;E fp let \nx = e1 in e2 . v Tocompile let-expressions wehaveto extend the intermediate lan\u00adguage likewise: we will \ndistinguish polymorphic expression vari\u00adables bydecorating them with sets of type-substitution formul\u00e6 \nS that applyto that particular occurrence of the variable. So we add to the productions of Section 5.3 \nthe following ones: e ::= \u00b7 \u00b7 \u00b7 |xS |let x = e in e while the big-step semantics of the new added expressions \nis (OE-PVARf) (OE-PVARc) E(x)= c E(x)= (.t ' ) S y.e, E E fo xS . c E fo xS . (.t ' ) comp(S,S )y.e, \nE (OE-LET) E fo e1 . v0 E,x . v0 fo e2 . v E fo let x = e1 in e2 . v Notice how rule (OE-PVARf)uses the \nS decoration on the variable to construct the closure. The final step is the extension of the compiler \nfor the newlyadded terms: = xS [x S [let x = e1 in e2] S = let x = [e1]S in [e2]S As an example, the \nlet-expression (20)is compiled into let x = (.a.a y.y)in x[{a.a/a}]x where the substitution [{a.a/a}]that \nis applied to the leftmostoc\u00adcurrence of x is recorded in the variable and will be used to instanti\u00adate \nthe closure associated with x bythe environment; the rightmost occurrence of x is decorated byiand therefore \nthe value bound to it will not be instantiated. Theorems 5.1, 5.2, and Corollary5.3 hold also for these \nextensions (see Appendix Dfor the proofs). In an actual programming language there will not be anysyn\u00adtactic \ndistinction between the two kinds of expression variables and compilation can be optimized by transforming \nvariables that are let-bound to monomor phic values into monomor phic variables. So, whenever e1 has \na monomor phic type t1, the let-expression should be compiled as  [let x = e1 in e2]S = [(.t1.t2x.e2{x/x})e1]S \nwhere t2 is the type deduced for e2 under the hypothesis that x has type t1. Notice that this optimization \nis compositional.  6. Design choices and extensions For the sake of concision we omitted two key features \nin the pre\u00adsentation: recursive functions and pairs. Recursive functions can be straightforwardly added \nwith minor modifications. In particular, for .i.Iti.si recursive functions, whose syntax is \u00b5f x.e, it \nsuffices to [sj]j.J add in the type environment G the recursion variable f associated with the type obtained \nby applying the decoration to the interface, that is, f : .i.I,j.Jtisj . sisj: the reader can refer to \nSection 7.5 in [11] for a discussion on how and why recursion is restricted to functions. The extension \nwith product types, instead, is less straightfor\u00adward but can be mostly done by using existing techniques. \nSyntac\u00adtically, we add pairs (e, e)and projections pie (for i=1,2) to terms and the product type constructor \nt\u00d7t to types. Reduction semantics is standard: two notions of reduction pi(v1, v2) vi (for i=1,2) plus \nthe usual context reduction rules. Typing rules are standard, as well: a pair is typed by the product \nof the types of its compo\u00adnents and if e is of type t1\u00d7t2, then its i-th projection pie has type ti. \nThe rule for pairs in the algorithmic system fA is the same as in the static semantics, while the rules \nfor projections pie become more difficult because the type inferred for e may not be of the form t1\u00d7t2 \nbut, in general, is (equivalent to) a union of intersec\u00adtions of types. We already met the latter problem \nfor application (where the function type may be different from an arrow) and there we checked that the \ntype deduced for the function in an application is a functional type (ie, a subtype of 0.1). Similarly, \nfor products we must check that the type of e is a product type (ie, a subtype of 1\u00d71). If the constraint \nis satisfied, then it is possible to define the type of the projection (in a way akin to the definition \nof the domain dom() for function types) using standard techniques of semantic subtyping (see Section \n6.11 in [11]). This is explained in details in Appendix C. For the semantics of the calculus we made \nfew choices that re\u00adstrict its generality. One of these, the use of a call-by-value reduc\u00adtion, is directly \ninherited from CDuce and it is required to ensure subject reduction. If e is an expression of type Int.Bool, \nthen the (.(Int.Int\u00d7Int).(Bool.Bool\u00d7Bool) application x.(x, x))e has type (Int\u00d7Int).(Bool\u00d7Bool). If we \nuse call-by-name, then this re\u00addex reduces to (e, e)whose type (Int.Bool\u00d7Int.Bool)is larger than the \ntype of the redex. Although the use of call-by-name would not hinder the soundness of the type system \n(expressed in terms of progress) we preferred to ensure subject reduction since it greatly simplifies \nthe theoretical development. A second choice, to restrict type-cases to closed types, was made by practical \nconsiderations: using open types in a type-case would have been computationally prohibitive insofar as \nit demands to solve at run-time the problem whether for two given types s and t there exists a type-substitution \ns such that ss = ts (we study this problem, that we call the tallying problems, in the companion paper \n[4]). Our choice, instead, is compatible with the highly op\u00adtimized (and provably optimal) pattern matching \ncompilation tech\u00adnique of CDuce. We leave for future work the study of type-cases on types with monomor \nphic variables (ie, those in .). This does not require dynamic type tallying resolution and would allow \nthe programmer to test capabilities of arguments bound to polymorphic type variables.  7. Related work \nWe focus on work related to this specific part of our work, namely, existing explicitly-typed calculi \nwith intersection types, and func\u00adtional languages to process XML data. Comparison with work on local \ntype inference and type reconstruction is done in the second part of this work presented in the companion \npaper [4]. To compare the differences between the existing explicitly-typed calculi for intersection \ntype systems, we discuss how the term of our daffy identity (.a[{.Inta /a},{Bool/a}]x.(.a.a y.x)x)is \nrendered. In [18, 21], typing derivations are written as terms: different typed representatives of the \nsame untyped term are joined together with an intersection .. In such systems, the function in (4)relabeled \nwith [{Int/a},{Bool/a}]is written (.Int.Int x.(.Int.Int y.x)x). (.Bool.Bool x.(.Bool.Bool y.x)x). Type \nchecking verifies that both .Int.Int x.(.Int.Int y.x)x and .Bool.Bool x.(.Bool.Bool y.x)x are well typed \nseparately, which generates two very similar typing derivations. The proposal of [13] follows the same \nidea, except that a separation is kept between the computational and the logical con\u00adtents of terms. \nA term consists in the association of a marked term and a proof term. The marked term is just an untyped \nterm where term variables are marked with integers. The proof term encodes the structure of the typing \nderivation and relates marks to types. The aforementioned example is written in this system as (.x : \n.(.1Int .0)0) . (.0Bool .(.1Bool 0.(.y : 1.x)x)@((.0Int .0)0)). In general, different occur rences of \na same mark can be paired with different types in the proof term. In [3], terms are duplicated (as in \n[18,21]), but the type checking of terms does not generate copies of almost identical proofs. The type \nchecking derivation for the term ((.Int.Int x.(.Int.Int y.x)x)..Bool.Bool x.(.Bool.Bool y.x)x)ver\u00adifies \nin parallel that the two copies are well typed. The duplication of terms and proofs makes the definition \nof beta reduction (and other transformations on terms) more diffi\u00adcult in the calculi presented so far, \nbecause it has to be performed in parallel on all the typed instances that correspond to the same untyped \nterm. Branching types have been proposed in [22]to cir\u00adcumvent this issue. The idea is to represent different \ntyping deriva\u00adtions for a same term into a compact piece of syntax. To this end, the branching type which \ncorresponds to a given intersection type t records the branching shape (ie, the uses of the intersec\u00adtion \nintroduction typing rule) of the typing derivation correspond\u00ading to t. For example, the type (Int.Int). \n(Bool.Bool)has only two branches, which is represented in [22] by the branching shape join{i=*, j=*}. \nOur running example is then written as {i=Int,j=Bool}{i=Int,j=Bool} .join{i=*, j=*}..x.(.y.x)x. Note \nthat the lambda term itself is not copied, and no duplication of proofs happens during type checking \neither: the branches labeled i and jare verified in parallel. In [8], the authors propose an expressive \nrefinement type sys\u00adtem with intersection, union, but also (a form of) dependent types, making possible \nto define, eg, the type of integer lists of length n, written [Int]n. The variable n can be quantified \nover either uni\u00adversally or existentially (using respectively . and S). Thanks to this it is possible \nto consider different instantiations of a depen\u00addent type and, thus to type our daffy function by different \ninstances of [Int]n (rather than with any type, as for Int and Bool in our example). Type checking requires \ntype annotations to be decid\u00adable: to check that .x.(.y.x)x has type .n.(([Int]2n.[Int]2n). ([Int]2n+1.[Int]2n+1)), \nthe subter m .y.x has to be annotated. This problem is similar to finding appropriate annotations for \nthe daffy function (4) in our language. In [8], terms are annotated with a list of typings: for example, \n.y.x can be annotated with A = (x : [Int]2n f 1.[Int]2n , x : [Int]2n+1 f 1.[Int]2n+1), which says that \nif x : [Int]2n, then .y.x has type 1.[Int]2n (and similarly if x : [Int]2n+1). The above annotation A \nis not sound because when checking that .x.(.y.x : A)x has result type .n.(([Int]2n.[Int]2n). ([Int]2n+1.[Int]2n+1)), \none can see that the occur rences of n in A escape their scope: they should be bound by the quantif ier \n. in the result type. To fix this, typing environments in annotations are extended with universally quanti\u00adfied \nvariables, that can be instantiated at type checking. For exam\u00adple, .y.x : (m : Nat, x : [Int]m f 1 . \n[Int]m)means that .y.x has type 1 . [Int]m, assuming x : [Int]m, where m can be instantiated with any \nnatural number. With this annotation, the daffy function can be checked against .n.(([Int]2n.[Int]2n). \n([Int]2n+1.[Int]2n+1)), by instantiating m with respectively 2n and 2n+1. It is possible to find a similarity \nbetween the annotations of [8] (the lists of typings) and our annotations (ie, the combination of interface \nand decoration) although instantiation in the former is much more harnessed. There is however a fundamental \ndifference between the two systems and it is that [8] does not include a type case. Because of that annotations \nneed not to be propagated at run\u00adtime: in [8] they are just used statically to check soundness and then \nerased at run-time. Without type-cases we could do the same, but it is precisely the presence of type-cases \nthat justifies our formalism.  For what concerns XML programming, let us cite polymorphic XDuce [12] \nand the work by Vouillon [20]. In both, pattern match\u00ading is designed so as not to break polymorphism, \nbut both have to give up something: higher-order functions for [12] and intersection, negation, and local \ntype inference in [20] (the type of function ar\u00adguments must be explicitly given). Furthermore, Vouillon \ns work suffers from the original sin of starting from a subtyping relation that is given axiomatically \nby a deduction system. This makes the intuition underlying subtyping very difficult to grasp (at least, \nfor us). Another route taken is the one of OCamlDuce [10], which jux\u00adtaposes OCaml and CDuce s type systems \nin the same language, keeping them separated. This practical approach yields little theo\u00adretical problems \nbut forces a value to be of one kind of type or an\u00adother, preventing the programmer from writing polymorphic \nXML transformations. Lastly, XHaskell by Sulzmann et al. [19]mixes Haskell type classes with XDuce regular \nexpression types but has two main drawbacks. First, every polymorphic variable must be annotated wherever \nit is instantiated with an XML type. Second, even without inference of explicit annotations (which they \ndo not address), their system requires several restrictions to be decidable (while our system with explicit \ntype-substitutions is decidable).  8. Conclusion The work presented in this and in its companion paper \n[4]pro\u00advides the theoretical basis and all the algorithmic tools needed to design and implement polymorphic \nfunctional languages for semi\u00adstructured data and, more generally, for functional languages with recursive \ntypes and set-theoretic unions, intersections, and nega\u00adtions. In particular, our results pave the way \nto the polymorphic extension of CDuce [2] and to the definition of a real type system for XQuery 3.0 \n[9] (not just one in which all higher-order func\u00adtions have type function() ). Thanks to local type inference \nand type reconstruction defined in the second part of this work, these languages can have a syntax and \nsemantics very close to those of OCaml or Haskell, but will include primitives (in particular com\u00adplex \npatterns) to exploit the great expressive power of full-f ledged set-theoretic types. Some problems are \nstill open, notably the decidability of type\u00adsubstitution inference defined in the second part of this \nwork, but these are of theoretical nature and should not have any impact in practice (as a matter of \nfacts people program in Java and Scala even though the decidability of their type systems is still an \nopen ques\u00adtion). On the practical side, the most interesting directions of re\u00adsearch is to couple the \nefficient compilation of the polymorphic calculus with techniques of static analysis that would perform \npar\u00adtial evaluation of relabeling so as to improve the efficiency of type\u00adcase of functional values even \nin the rare cases in which it is more expensive than in the monomor phic version of CDuce. Acknowledgments. \nThis work was partially suppor ted by the ANR TYPEX project n. ANR-11-BS02-007. Zhiwu Xu was also partially \nsuppor ted by an Eiffel scholarship of French Ministry of Foreign Affairs and by the grant n. 61070038 \nof the National Nat\u00adural Science Foundation of China.  References [1] H. Barendregt, M. Coppo, and M. \nDezani-Ciancaglini. A filter lambda model and the completeness of type assignment. Journal of Symbolic \nLogic, 48(4):931 940, 1983. [2] V. Benzaken, G. Castagna, and A. Frisch. CDuce: an XML-fr iendly general \npurpose language. In ICFP 03. ACM Press, 2003. [3] V. Bono, B. Venner i, and L. Bettini. A typed lambda \ncalculus with intersection types. Theor. Comput. Sci., 398(1-3):95 113, 2008. [4] G. Castagna, K. Nguy.n, \nand Z. Xu. Polymorphic functions with set-theoretic types. Part 2: Local type inference and type recon\u00adstruction. \nUnpublished manuscript, available at http://hal. archives-ouvertes.fr/hal-00880744, November 2013. [5] \nG. Castagna and Z. Xu. Set-theoretic Foundation of Parametric Polymorphism and Subtyping. In ICFP 11, \n2011. [6] J. Clark and M. Murata. Relax-NG, 2001. www.relaxng.org. [7] M. Coppo, M. Dezani, and B. Venner \ni. Principal type schemes and lambda-calculus semantics. In To H.B. Curry. Essays on Combinatory Logic, \nLambda-calculus and Formalism. Academic Press, 1980. [8] J. Dunfield and F. Pfenning. Tridirectional \ntypechecking. In POPL 04. ACM Press, 2004. [9] J. Robie et al. Xquery 3.0: An XML query language (working \ndraft 2010/12/14), 2010. http://www.w3.org/TR/xquery-30/. [10] A. Frisch. OCaml + XDuce. In ICFP 06, \n2006. [11] A. Frisch, G. Castagna, and V. Benzaken. Semantic subtyping: deal\u00ading set-theoretically with \nfunction, union, intersection, and negation types. The Journal of ACM, 55(4):1 64, 2008. [12] H. Hosoya, \nA. Frisch, and G. Castagna. Parametric polymorphism for XML. ACM TOPLAS, 32(1):1 56, 2009. [13] L. Liquori \nand S. Ronchi Della Rocca. Intersection-types \u00e0 la Church. Inf. Comput., 205(9):1371 1386, 2007. [14] \nB.C. Pierce. Types and Programming Languages. MIT Press, 2002. [15] F. Pottier and D. R\u00e9my. The essence \nof ML type inference. In B.C. Pierce, editor, Advanced Topics in Types and Programming Lan\u00adguages, chapter \n10, pages 389 489. MIT Press, 2005. [16] J.C. Reynolds. Design of the programming language Forsythe. \nTech\u00adnical Report CMU-CS-96-146, Carnegie Mellon University, 1996. [17] J.C. Reynolds. What do types \nmean?: from intrinsic to extrinsic semantics. In Programming methodology. Springer, 2003. [18] S. Ronchi \nDella Rocca. Intersection typed lambda-calculus. Electr. Notes Theor. Comput. Sci., 70(1):163 181, 2002. \n[19] M. Sulzmann, K. Zhuo, and M. Lu. XHaskell -Adding Regular Expression Types to Haskell. In IFL, LNCS \nn. 5083. Springer, 2007. [20] J. Vouillon. Polymorphic regular tree types and patterns. In POPL 06, pages \n103 114, 2006. [21] J.B. Wells, A. Dimock, R. Muller, and F.A. Turbak. A calculus with polymorphic and \npolyvariant flow types. J. Funct. Program., 12(3):183 227, 2002. [22] J.B. Wells and C. Haack. Branching \ntypes. In ESOP 02, volume 2305 of LNCS, pages 115 132. Springer, 2002. [23] Z. Xu. Parametric Polymorphism \nfor XML Processing Languages. PhD thesis, Universit\u00e9 Paris Diderot, 2013. Available at http:// tel.archives-ouvertes.fr/tel-00858744. \n  \n\t\t\t", "proc_id": "2535838", "abstract": "<p>This article is the first part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation).</p> <p>In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed lambda-calculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations.</p> <p>The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data.</p>", "authors": [{"name": "Giuseppe Castagna", "author_profile_id": "81100388576", "affiliation": "CNRS. Universit&#233; Paris Diderot, Paris, France", "person_id": "P4383730", "email_address": "Giuseppe.Castagna@cnrs.fr", "orcid_id": ""}, {"name": "Kim Nguyen", "author_profile_id": "81318495209", "affiliation": "Universit&#233; Paris-Sud, Orsay, France", "person_id": "P4383731", "email_address": "Kim.Nguyen@lri.fr", "orcid_id": ""}, {"name": "Zhiwu Xu", "author_profile_id": "86158924857", "affiliation": "Universit&#233; Paris Diderot, Paris, France", "person_id": "P4383732", "email_address": "Zhiwu.Xu@pps.univ-paris-diderot.fr", "orcid_id": ""}, {"name": "Hyeonseung Im", "author_profile_id": "81416603989", "affiliation": "Universit&#233; Paris-Sud, Orsay, France", "person_id": "P4383733", "email_address": "Hyeonseung.Im@lri.fr", "orcid_id": ""}, {"name": "Sergue&#239; Lenglet", "author_profile_id": "81418600277", "affiliation": "Universit&#233; de Lorraine, Nancy, France", "person_id": "P4383734", "email_address": "serguei.lenglet@gmail.com", "orcid_id": ""}, {"name": "Luca Padovani", "author_profile_id": "81100024516", "affiliation": "Universit&#224; di Torino, Torino, Italy", "person_id": "P4383735", "email_address": "padovani@di.unito.it", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535840", "year": "2014", "article_id": "2535840", "conference": "POPL", "title": "Polymorphic functions with set-theoretic types: part 1: syntax, semantics, and evaluation", "url": "http://dl.acm.org/citation.cfm?id=2535840"}