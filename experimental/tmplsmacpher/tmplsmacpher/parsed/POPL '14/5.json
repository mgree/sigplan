{"article_publication_date": "01-08-2014", "fulltext": "\n Tracing Compilation by Abstract Interpretation Stefano Dissegna Francesco Logozzo Francesco Ranzato \nDipartimento di Matematica Microsoft Research Dipartimento di Matematica University of Padova, Italy \nRedmond, WA, USA University of Padova, Italy Abstract Tracing just-in-time compilation is a popular \ncompilation schema for the ef.cient implementation of dynamic languages, which is commonly used for JavaScript, \nPython, and PHP. It relies on two key ideas. First, it monitors the execution of the program to detect \nso-called hot paths, i.e., the most frequently executed paths. Then, it uses some store information available \nat runtime to optimize hot paths. The result is a residual program where the optimized hot paths are \nguarded by suf.cient conditions ensuring the equivalence of the optimized path and the original program. \nThe residual pro\u00adgram is persistently mutated during its execution, e.g., to add new optimized paths \nor to merge existing paths. Tracing compilation is thus fundamentally different than traditional static \ncompilation. Nevertheless, despite the remarkable practical success of tracing compilation, very little \nis known about its theoretical foundations. We formalize tracing compilation of programs using abstract \ninterpretation. The monitoring (viz., hot path detection) phase cor\u00adresponds to an abstraction of the \ntrace semantics that captures the most frequent occurrences of sequences of program points together with \nan abstraction of their corresponding stores, e.g., a type en\u00advironment. The optimization (viz., residual \nprogram generation) phase corresponds to a transform of the original program that pre\u00adserves its trace \nsemantics up to a given observation as modeled by some abstraction. We provide a generic framework to \nexpress dy\u00adnamic optimizations and to prove them correct. We instantiate it to prove the correctness \nof dynamic type specialization. We show that our framework is more general than a recent model of trac\u00ading \ncompilation introduced in POPL 2011 by Guo and Palsberg (based on operational bisimulations). In our \nmodel we can naturally express hot path reentrance and common optimizations like dead\u00adstore elimination, \nwhich are either excluded or unsound in Guo and Palsberg s framework. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation correctness proofs, formal methods; D.3.4 \n[Programming Languages]: Processors compil\u00aders, optimization; F.3.2 [Logics and Meanings of Programs]: \nSe\u00admantics of Programming Languages program analysis Keywords tracing compilation; abstract interpretation; \ntrace se\u00admantics Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. Copyrights for components \nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. POPL 14, January 22 24, 2014, \nSan Diego, CA, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM \n978-1-4503-2544-8/14/01. . . $15.00. http://dx.doi.org/10.1145/2535838.2535866 1. Introduction Ef.cient \ntraditional static compilation of popular dynamic lan\u00adguages like JavaScript, Python and PHP is very \nhard if not impos\u00adsible. In fact those languages present so many dynamic features which make all traditional \nstatic analyses used for program opti\u00admization very imprecise. Therefore, practical implementations of \ndynamic languages should rely on dynamic information in order to produce an optimized version of the \nprogram. In particular, tracing just-in-time compilation (TJITC) [1, 3 6, 15, 16, 24] has emerged as \na valuable implementation and optimization technique for dy\u00adnamic languages. For instance, the Facebook \nHipHop virtual ma\u00adchine for PHP and the V8 JavaScript engine of Google Chrome use some form of tracing \ncompilation [19, 20]. The Mozilla Fire\u00ad fox JavaScript engine used to have a tracing engine, TraceMon\u00adkey, \nwhich has been later substituted by whole-method just-in-time compilation engines (initially J\u00a8 agerMonkey \nand then IonMonkey) [13, 14]. 1.1 The Problem Tracing JIT compilers leverage runtime pro.ling of programs \nto detect and record often executed paths, called hot paths, and then they optimize and compile only \nthese paths at runtime. A path is a linear sequence of instructions through the program. Pro.ling may \nalso collect information about the values that the program variables may assume during the execution \nof that path, which is then used to specialize/optimize the code. Of course, this information is not \nguaranteed to hold for all the subsequent executions of the hot path. Since optimizations rely on that \ninformation, the hot path is augmented with guards that check the pro.led conditions, such as, for example, \nvariable types. When a guard fails, execution jumps back to the old, non-optimized code. The main hypotheses \nof tracing compilers, con.rmed by the practice, are: (i) loop bodies are the only interesting code to \noptimize, so they only consider paths inside program loops; and (ii) optimizing straight-line code is \neasier than a whole-method analysis (involving loops, goto, etc.). Hence, tracing compilers look quite \ndifferent than traditional compilers. These differences raise some natural questions on trace compilation: \n(i) what is a viable formal model, which is generic yet realistic enough to capture the behavior of real \noptimizers? (ii) which optimizations are sound? (iii) how can one prove their soundness? In this paper \nwe answer the above questions. Our formal model is based on program trace semantics [9] and abstract \ninterpretation [10, 12]. Hot path detection is modeled just as an abstraction of the trace semantics \nof the program, which only retains: (i) the sequences of program points which are repeated more than \nsome threshold; (ii) an abstraction of the possible pro\u00adgram stores, e.g., the type of the variables \ninstead of their concrete values. As a consequence, a hot path does not contain loops nor join points. \nFurthermore, in the hot path, all the correctness conditions (i.e., guards) are explicit, for instance \nbefore performing integer ad\u00addition, we should check that the operands are integers. If the guard condition \nis not satis.ed then the execution leaves the hot path, re\u00adverting to the non-optimized code. Guards \nare essentially elements of some abstract domain, which is then left as a parameter in our framework. \nThe hot path is then optimized using standard compi\u00adlation techniques we only require the optimization \nto be sound.  We de.ne the correctness of the residual (or extracted) program in terms of abstraction \nof the trace semantics: the residual program is correct if it is indistinguishable, up to some abstraction \nof the trace semantics, from the original program. Examples of abstrac\u00adtions are the program store at \nthe exit of a method, or the stores at loop entry and loop exit points.  1.2 Main Contributions This \npaper puts forward a formal model of TJITC whose key features are as follows: We provide the .rst model \nof tracing compilation based on abstract interpretation of trace semantics of programs.  We provide \na more general and realistic framework than a re\u00adcent model of TJITC by Guo and Palsberg [17] based on \npro\u00ad gram bisimulations: we employ a less restrictive correctness criterion that enables the correctness \nproof of actually imple\u00admented optimizations; hot paths can be annotated with runtime information on \nthe stores, notably type information; optimized hot loops can be re-entered.  We formalize and prove \nthe correctness of type specialization of hot paths.  Our model focusses on source-to-source program \ntransforma\u00adtions and optimizations of a low level imperative language with untyped global variables, \nwhich may play the role of intermediate language of some virtual machine. Our starting point is that \npro\u00adgram optimizations can be seen as transformations that lose some information on the original program, \nso that optimizations can be viewed as approximations and in turn can be formalized by abstract interpretation. \nMore precisely, we rely on the insight by Cousot and Cousot [12] that a program source can be seen as \nan abstraction of its trace semantics, i.e. the set of all possible execution sequences, so that a source-to-source \noptimization can be viewed as an abstrac\u00adtion of a transform of the program trace semantics. In our model, \nsoundness of program optimizations is de.ned as program equiva\u00adlence w.r.t. an observational abstract \ninterpretation of the program trace semantics. Here, an observational abstraction induces a cor\u00adrectness \ncriterion by describing what is observable about program executions, so that program equivalence means \nthat two programs are indistinguishable by looking only at their observable behaviors. A crucial part \nof tracing compilation is the selection of the hot path(s) to optimize. Of course, this choice is made \nat run\u00adtime based on program executions, so it can be seen once again as an abstraction of trace semantics. \nHere, a simple trace abstraction selects cyclic instruction sequences, i.e. loop paths, that appear at \nleast N times within a single execution trace. These instruction sequences are recorded together with \nsome property of the values assumed by program variables at that point, which is represented as a value \nof a suitable store abstraction, which in general depends on the successive optimization. A program optimization \ncan be seen as an abstraction of a semantic transformation of program execution traces, as described \nby the Cousots in [12]. The advantage of this approach is that optimization properties, such as their \nsoundness, are easier to prove at a semantic level. The optimization itself can be de.ned on the whole \nprogram or, as in the case of real tracing JIT compilers, can be restricted to the hot path. This latter \nrestriction is achieved by transforming the original program so that the hot path is extracted, i.e. \nmade explicit: the hot path is added to the program as a path with no join points that jumps back to \nthe original code when execution leaves it. A guard is placed before each command in this hot path that \nchecks if the necessary conditions, as selected by the store abstraction, are satis.ed. A program optimization \ncan be then con.ned to the hot path only, making it linear, by ignoring the parts of the program outside \nit. The guards added to the hot path allows us to retain precision. We apply our TJITC model to type \nspecialization. Type special\u00adization is de.nitely the key optimization for dynamic languages such as \nJavascript [15], as they provide generic operations whose execution depends on the type of run-time values \nof their operands.  1.3 Related Work A formal model for tracing JIT compilation has been put forward \nin POPL 2011 by Guo and Palsberg [17]. It is based on operational bisimulation [23] to describe equivalence \nbetween source and op\u00ad timized programs. In Section 11 we show how this model can be expressed within \nour framework through the following steps: Guo and Palsberg s language is compiled into ours; we then \nexhibit an observational abstraction which is equivalent to Guo and Palsberg s correctness criterion; \n.nally, after some minor changes that address a few differences in path selection, the transformations \nperformed on the source program turn out to be the same. Our framework over\u00adcomes some signi.cant limitations \nin Guo and Palsberg s model. The bisimulation equivalence model used in [17] implies that the optimized \nprogram has to match every change to the store made by the original program, whereas in practice we only \nneed this match to hold in certain program points and for some variables, such as in output instructions. \nThis limits the number of real optimizations that can be modeled in the theoretical framework. For instance, \ndead store elimination is proven unsound in [17], while it is imple\u00ad mented in actual tracing compilers \n[15, Section 5.1]. Furthermore, their formalization fails to model some important features of actual \nTJITC implementation: (i) traces are simple linear paths of instruc\u00adtions, i.e., they cannot be annotated \nwith store properties; (ii) hot path selection is completely non-deterministic, they do not model a selection \ncriterion; and, (iii) once execution leaves an optimized hot path the program will not be able to re-enter \nit. It is also worth citing that abstract interpretation of program trace semantics roots at the foundational \nwork by Cousot [8, 9] and has been widely used as a successful technique for de.ning a range of static \nprogram analyses [2, 7, 18, 22, 26 28]. Abstract interpretation has been used to describe static compilation \nand opti\u00admizations. In particular, Rival [25] describes various optimizations as trace abstractions they \npreserve. In the Cousot and Cousot termi\u00adnology [12], Rival approach corresponds to of.ine transformations \nwhereas tracing compilation is an online transformation. 2. Language and Concrete Semantics 2.1 Syntax \nFollowing [12], we consider a basic low level language with un\u00ad typed global variables, a kind of elementary \ndynamic language. Program commands range in C and consist of a labeled action which speci.es a next label \n(L is the unde.ned label, where the execution becomes stuck). Labels: L . L L . L Values: v . Value Variables: \nx . Var Expressions: Exp 3 E ::= v | x | E1 + E2 Boolean Expressions: BExp 3 B ::= tt | ff | E1 = E2 \n|\u00acB | B1 . B2 Actions: A 3 A ::= x := E | B | skip Commands: C 3 C ::= L : A . L' (L' . L.{L})  For \nany command C = L : A . L ', we use the following notation: lbl(C) L, act(C) A and suc(C) L '. Commands \nL : B . L ' whose action is a Boolean expression are called conditionals. A program P . P(C) is a (possibly \nin.nite, at least in theory) set of commands, with a distinct initial label Lin from which execution \nstarts, so that Pin denotes the commands in P labeled by Lin (Pin consists of two commands when the initial \ncommand is a conditional). If a program P includes a conditional C = L : B . L ' then P must also include \na unique complement conditional L : \u00acB . L '', which is denoted by cmpl(C) or Cc , where \u00ac\u00acB is taken \nto be equal to B, so that cmpl(cmpl(C)) = C. For simplicity, we consider deterministic programs, i.e., \nwe require that for any C1, C2 . P such that lbl(C1) = lbl(C2): (1) if act(C1) = act(C2) then C1 = cmpl(C2); \n(2) if act(C1) = act(C2) then C1 = C2. The set of well-formed programs is denoted by Program.   2.2 \nTransition Semantics The language semantics relies on the following type values, where Char is a nonempty \nset of characters and undef represents a generic error. Int Z Bool {true, false} String Char * Undef \n{undef } In turn, Value and type names in Types are de.ned as follows: Value Int . String . Undef Types \n{Int, String, Undef, Any, \u00d8} while type : Value . Types provides the type of any value. Here, the type \nname Any plays the role of top type, which is the supertype (i.e., contains) all types, while \u00d8 is the \nbottom type, which is a subtype for all types. Let . . Store Var . Value denote the set of possible program \nstores. The semantics of expressions and program actions is standard and goes as de.ned in Fig. 1. Let \nus remark that: the binary function +Int denotes integer addition; \u00b7 is string concate\u00adnation; logical \nnegation and conjunction are extended in order to handle undef values, i.e., \u00acundef = undef and undef \n. b = undef = b . undef . With a slight abuse of notation we also con\u00adsider the so-called collecting \nversions of the semantic functions in Fig. 1: E : Exp . P(Store) . P(Value) E[E]S {E[E]. | . . S}  B \n: BExp . P(Store) . P(Store) B[B]S {. . S | B[B]. = true}  A : A . P(Store) . P(Store) A[A]S {A[A]. \n| . . S, A[A]. . {., undef }}  Program states are pairs of stores and commands: State Store \u00d7C. If \nP is a program then StateP Store \u00d7P . We extend lbl, act and suc to be de.ned on states, meaning that \nthey are de.ned on the command component of a state. Also, store (s) returns the store of a state s. \nGiven P . Program, the program transition relation S[P ] : StateP . P(StateP ) between states is de.ned \nas follows: S[P ](., C ) {(. ' , C ' ) . StateP | . ' . A[act(C)]{.}, C ' . P, suc(C) = lbl(C ' )}. It \nis worth remarking that, according to the above de.nition, if : B . L '' : \u00acB . L ''' C = L : A . L ' \n, C1 = L ' and C1 c = L ' are all commands that belong to P and . ' . A[A]. then we have that S[P ](., \nC ) = {(. ' , C1), (. ' , C 1c)}. 2.2.1 Trace Semantics A partial (forward) trace is a .nite sequence \nof program states which are related by the transition relation S. If P is a program then we de.ne TraceP \n{s . (StateP ) * | .i . [1, |s|). si . S[P ]si-1} A trace s . TraceP is maximal if for any state s . \nStateP , ss . TraceP . Let us note that according to the above de.nitions, if a trace s . TraceP has \na last state s|s|-1 = (., L : B . L ') with a conditional command such that B[B]. = false then s is maximal. \nAlso, if a trace s . TraceP has a last state s|s|-1 = (., L : A . L) whose next label is the unde.ned \nlabel then s is maximal as well. The trace semantics T[P ] is the set of all the partial (includ\u00ading \nmaximal) traces of the program P . This set is de.ned as the least .xed point of a monotonic operator \nF [P ] : P(TraceP ) . P(TraceP ), called trace transition operator, de.ned as follows: F [P ]X {(., Cin) \n| . . Store, Cin . Pin} . {sss ' . TraceP | ss . X, s ' . S[P ]s} lfp(F [P ]) . P(TraceP ) T[P ] The \nfunction F [P ] is trivially monotone on the complete lattice (P(TraceP ), .), so that its least .xpoint \nT[P ] is well de.ned. Example 2.1. Let us consider the program below written in some while-language: \nx := 0; while (x = 20) do x := x + 1; if (x%3 = 0) then x := x + 3; Its translation in our language is \ngiven below, where, with a little abuse, we assume that the syntax of arithmetic and Boolean expres\u00adsion \nis extended to allow expressions like x%3 = 0. . P = C0 = L0 : x := 0 . L1, C1 = L1 : x = 20 . L2, C \n1 c = L1 : \u00ac(x = 20) . L5, C2 = L2 : x := x + 1 . L3, C3 = L3 : (x%3 = 0) . L4, C 3 c = L3 : \u00ac(x%3 = \n0) . L1 C4 = L4 : x := x + 3 . L1, \u00af C5 = L5 : skip . L The trace semantics T[P ] includes the following \npartial traces, where ? stands for any integer value and stores are denoted within square brackets. ([x/?], \nC0) . F [P ]\u00d8 ([x/?], C0)([x/0], C1) ([x/?], C0)([x/0], C c) (maximal) 1 ([x/?], C0)([x/0], C1)([x/0], \nC2) ([x/?], C0)([x/0], C1)([x/0], C2)([x/1], C3) (maximal) ([x/?], C0)([x/0], C1)([x/0], C2)([x/1], C \nc) 3 ([x/?], C0)([x/0], C1)([x/0], C2)([x/1], C c)([x/1], C1) 3 ([x/?], C0)([x/0], C1)([x/0], C2)([x/1], \nC c)([x/1], C c) (maximal) 3 1 ([x/?], C0)([x/0], C1)([x/0], C2)([x/1], C c)([x/1], C1)([x/2], C2) 3 \n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 ([x/?], C0) \u00b7 \u00b7 \u00b7 ([x/21], C4)([x/24], C1) (maximal) ([x/?], C0) \u00b7 \u00b7 \u00b7 ([x/21], C4)([x/24], \nC c)([x/24], C5) (maximal) 1  E : Exp . Store . Value 8 >< >: E E1 . +Int E[E2]. if type(E Ei .) = \nInt E[ E1] . \u00b7 E[E2]. if type(E[ Ei] .) = String undef otherwise v .(x) E[v]. E[x]. E[E1 + E2]. B : BExp \n. Store . {true, false, undef } B[ tt ]. true B[ ff ]. false B[E1 = E2].B[B1]. . B[B2]. ( E[E1]. = E[E2]. \nif type(E[Ei].) = Int undef otherwise B[\u00acB]. \u00acB[B]. B[B1 . B2]. A : A . Store . Store . {., undef } 8 \n>< >: . if B B . = true . if B B . = false undef if B[ B] . = undef Figure 1. Semantics of program expressions \nand actions. A[x := E]. .[x := E[E].] A[skip]. . A[B]. 3. Abstract Interpretation Background In standard \nabstract interpretation [10, 11], abstract domains (or ab\u00adstractions) are speci.ed by Galois connections/insertions \n(GCs/GIs for short) or, equivalently, adjunctions. Concrete and abstract do\u00admains, (C, =C ) and (A, =A), \nare assumed to be complete lattices which are related by abstraction and concretization maps a : C . \nA and . : A . C that give rise to an adjunction (a, C, A, .), that is, for all a and c, a(c) =A a . c \n=C .(a). A GC is a GI when a . . = .x.x. It is well known that a join-preserving a uniquely determines \n. as follows: .(a) = .{c . C | a(c) =A a}; conversely, a meet-preserving . uniquely determines a as follows: \na(c) = .{a . A | c =C .(a)}. Let f : C . C be some concrete monotone function for simplicity, we consider \n1-ary functions and let f. : A . A be a corresponding monotone abstract function de.ned on some abstraction \nA related to C by a GC. Then, f. is a correct abstract interpretation of f on A when a . f . f. . a holds, \nwhere . denotes the pointwise ordering. Moreover, the abstract function fA a . f . . : A . A is called \nthe best correct approximation of f on A because any abstract function f. is correct iff fA . f.. Hence, \nfor any abstraction A, fA plays the role of the best possible approximation of f on the abstract domain \nA. 4. Store Abstractions As usual in abstract interpretation, a store property is modeled by some abstraction \nStore. that we assume to be encoded through a Galois connection (astore, (P(Store), .), (Store., =), \n.store ). For instance, as we will see later, the static types of program variables give rise to a simple \nstore abstraction. Given a program P , a store abstraction Store. also induces a corresponding state \nabstraction StateP Store. \u00d7P and, in turn, a trace abstraction TraceP (StateP ) * . 4.1 Nonrelational \nAbstractions Nonrelational store abstractions can be easily designed by a stan\u00addard pointwise lifting \nof some value abstraction. Let Value. be a value abstraction as formalized by a Galois connection (avalue \n, (P(Value), .), (Value, =Value. ), .value). The abstract domain Value. induces a nonrelational store \nabstrac\u00adtion Store(Var . Value, .) value where . is the pointwise ordering induced by =Value. : .1 . \n.2 iff for all x . Var, .1(x) =Value. .2(x). Here, the bottom and top abstract stores are, respectively, \n.x..Value. and .x.TValue. . The g. abstraction map a: P(Store) . Storeis thus de.ned as value value follows: \ng a(S) .x.avalue ({.(x) . Value | . . S}) value g. while the corresponding concretization map .: Store. \nvalue value g P(Store) is de.ned by adjunction from aas recalled in Sec\u00ad value tion 3 and it is easy \nto check that it turns out to be de.ned as fol\u00adlows: g. . .(.) = {. . Store | .x . Var . .(x) . .value(.(x))}. \nvalue 5. Hot Path Selection A loop path is a sequence of program commands which is repeated in some execution \nof a program loop, together with a store prop\u00aderty which is valid at the entry of each command in the \npath. A loop path becomes hot when, during the execution, it is repeated at least a .xed number N of \ntimes. In a TJITC, hot path selection is performed by a loop path monitor that also records store prop\u00aderties \n(see, e.g., [15]). Here, hot path selection is not operationally de.ned, it is instead modeled as an \nabstraction map over program traces, i.e., program executions. We .rst de.ne a mapping loop : TraceP \n. P(TraceP ) that returns all the loop paths in some execution trace of a program P . Formally, a loop \npath is a substring (i.e., a segment) t of a trace s such that: (1) the successor command in s of the \nlast state in t exists and coincides with the command or its complement, when this is the last loop \niteration of the .rst state in t ; (2) there is no other such command within t (otherwise the sequence \nt would contain multiple iterations); (3) the last state of t performs a back\u00adward jump in the program \nP . To recognize backward jumps, we consider a topological order on the control .ow graph of commands \nin P , here denoted by <:  loop ((.0, C0) \u00b7 \u00b7 \u00b7 (.n, Cn)) . (.i, Ci)(.i+1, Ci+1) \u00b7 \u00b7 \u00b7 (.j , Cj ) | \n0 = i = j < n, Ci <Cj , \u00af lbl(Cj+1) = lbl(Ci), .k . (i, j ]. Ck . {Ci, cmpl(Ci)} . Let us remark that \na loop path (.i, Ci) \u00b7 \u00b7 \u00b7 (.j , Cj ) . loop ((.0, C0) \u00b7 \u00b7 \u00b7 (.n, Cn)) may contain some sub-loop path, \nnamely it may happen that loop ((.i, Ci) \u00b7 \u00b7 \u00b7 (.j , Cj )) = \u00d8so that some commands Ck, with k . [i, \nj], occur more than once in (.i, Ci) \u00b7 \u00b7 \u00b7 (.j , Cj ). We abuse notation by using astore to denote a \nmap astore : TraceP . TraceP which abstracts program traces in TraceP : astore ((.0, C0) \u00b7 \u00b7 \u00b7 (.n, Cn)) \n(astore ({.0}), C0) \u00b7 \u00b7 \u00b7 (astore ({.n}), Cn). Given a static parameter N > 0, we de.ne a function hotN \n: TraceP . P(TraceP ) which returns the set of Store -abstracted loop paths appearing at least N times \nin some program trace. To count the number of times a loop path appears within a trace we use an auxiliary \nfunction count : TraceP \u00d7 TraceP . N such that count (s, t ) yields the number of times an abstract path \nt occurs in a abstract trace s: ' ' count ((a0, C0) \u00b7 \u00b7 \u00b7 (an, Cn), (b0, C 0) \u00b7 \u00b7 \u00b7 (bm, C m)) ( X ' \n' n-m1 if (ai, Ci) \u00b7 \u00b7 \u00b7 (ai+m, Ci+m) = (b0, C 0) \u00b7 \u00b7 \u00b7 (bm, C m) 0 otherwise i=0 Hence, hotN can be \nde.ned as follows: hotN (s = (.0, C0) \u00b7 \u00b7 \u00b7 (.n, Cn)) . (ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj ) | .(.i, Ci) \u00b7 \u00b7 \u00b7 (.j \n, Cj ) . loop(s) s.t. i = j, astore ((.i, Ci) \u00b7 \u00b7 \u00b7 (.j , Cj )) = (ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj ), \u00af count \n(astore (s), (ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj )) = N . Finally, an abstraction map aN : P(TraceP ) . P(TraceP \n) hot collects the results of applying hotN to a set of traces: aN .s.T hotN (s). hot(T ) A hot path \nhp . aN hot(T[P ]) is also called a N-hot path and is compactly denoted as hp = (a0, C0, ..., an, Cn). \nLet us observe that if the hot path is the body of some while loop then its .rst command C0 is a conditional, \nnamely C0 is the Boolean guard of the while loop. We de.ne the following successor function for indices \nin hot paths: next .i. i = n ? 0 : i + 1. For a N-hot path (a0, C0, ..., an, Cn) . aN hot (T[P ]), for \nany i . [0, n], if Ci is a conditional command Li : Bi . Lnext(i) then throughout the paper its complement \nCi c = cmpl(Ci) will be also denoted by Li : \u00acBi . Lc next(i). Example 5.1. Let us consider the program \nP in Example 2.1. We consider a trivial one-point store abstraction Store = {T}, where all the stores \nare abstracted to the same abstract store T, i.e., astore = .S.T. Here, we have two 2-hot paths in P \n, that is, it turns out that a2 hot (T[P ]) = {hp1, hp2} where: hp1 = (T, C1 = L1 : x = 20 . L2, T, C2 \n= L2 : x := x + 1 . L3, T, C 3 c = L3 : \u00ac(x%3 = 0) . L1); hp2 = (T, C1 = L1 : x = 20 . L2, T, C2 = L2 \n: x := x + 1 . L3, T, C3 = L3 : (x%3 = 0) . L4, T, C4 = x := x + 3 . L1). 6. Trace Extraction For any \nabstract store a . Store , a corresponding Boolean ex\u00adpression guard Ea . BExp is de.ned (where the \nnotation Ea should hint at an expression which is induced by the abstract store a) , whose semantics \nis as follows: for any . . Store, B[guard Ea]. . . .store (a). Thus, in turn, we also have program actions \nguard Ea such that: ( . if . . .store (a) A[guard Ea]. . if . . .store (a) Let P be a program and hp \n= (a0, C0, ..., an, Cn) . aN hot (T[P ]) be a hot path on some store abstraction Store . We de.ne a synctatic \ntransform of P where the hot path hp is explicitly ex\u00adtracted from P . This is implemented by a suitable \nrelabeling of each command Ci in hp which is in turn preceded by the condi\u00adtional guard Eai induced by \nthe store property ai. To this aim, we consider three injective relabeling functions f : [0, n] . L1 \n n : [1, n] . L2 (\u00b7) : L . L  where L1, L2 and L are pairwise disjoint sets of fresh labels, so that \nlabels (P ) n (L1 . L2 . L) = \u00d8. The transformed program extrhp(P ) for the hot path hp is de.ned as \nfollows and a graphical example of this transform is depicted in Fig. 2. De.nition 6.1 (Trace extraction \ntransform). The trace extraction transform of P for the hot path hp is: ` \u00b4 extrhp(P ) P \" {C0} . {cmpl(C0) \n| cmpl(C0) . P } . {L0 : act(C0) . L1} . {L0 : \u00acact(C0) . Lc 1 | cmpl(C0) . P } . stitchP (hp) where \nthe stitch of hp into P is de.ned as follows: stitchP (hp) . {L0 : guard Ea0 . f0, L0 : \u00acguard Ea0 . \nL0} . {fi : act(Ci) . ni+1 | i . [0, n - 1]}.{fn : act(Cn) . L0}. {fi : \u00acact(Ci) . Lc next(i) | i . [0, \nn], cmpl(Ci) . P } . {ni : guard Eai . fi, ni : \u00acguard Eai . Li | i . [1, n]}. The new command L0 : \nguard Ea0 . f0 is therefore the entry conditional of the stitched hot path stitchP (hp), while any command \nC . stitchP (hp) such that suc(C) . labels (P ) . Lis a potential exit (or bail out) command of stitchP \n(hp).  . . .  L0 Lc 1 . . . L0 Lc 1 L1  L1 L2 L2 . . . . . . Ln Ln   Figure 2. An example \nof trace extraction transform: on the left, a hot path hp with commands in pink (in black/white: loosely \ndotted) shapes; on the right, the corresponding trace transform extrhp(P ) with new commands in blue \n(in black/white: densely dotted) shapes. Lemma 6.2. If P is well-formed then, for any hot path hp, extrhp(P \n) is well-formed. Let us remark that the stitch of the hot path hp into P is always a linear sequence \nof different commands, namely, stitchP (hp) does not contain loops nor join points. Furthermore, this \nhappens even if the hot path hp does contain some inner sub-loop. Technically, this comes as a consequence \nof the fact that the above relabeling func\u00adtions are required to be injective. Hence, even if some command \nC occurs more than once inside hp then these multiple occurrences of C in hp are transformed into differently \nlabeled commands in stitchP (hp). Example 6.3. Let us consider the program P in Example 2.1 and the hot \npath hp1 = (T, C1, T, C2, T, C3 c) in Example 5.1, where stores are abstracted to the trivial one-point \nabstraction Store = {T}. Here, we have that for any store . . Store, B[guard ET]. = true. The trace extraction \ntransform of P w.r.t. hp is therefore as follows: extrhp(P ) = P {C1, C1 c} . {L1 : x = 20 . L2, L1 : \n\u00ac(x = 20) . L5} . stitchP (hp) where stitchP (hp) = {H0 = L1 : guard ET . f0, H0 c = L1 : \u00acguard ET . \nL1} . {H1 = f0 : x = 20 . n1, H1 c = f0 : \u00ac(x = 20) . L5} . {H2 = n1 : guard ET . f1, H2 c = n1 : \u00acguard \nET . L2} . {H3 = f1 : x := x + 1 . n2} . {H4 = n2 : guard ET . f2, H4 c = n2 : \u00acguard ET . L3} . {H5 \n= f2 : \u00ac(x%3 = 0) .L1, H5 c = f2 : (x%3 = 0) .L4}. Hence, extrhp(P ) can be rewritten at a high-level \nrepresentation using while loops and gotos as follows: x := 0; L1 : while guard ET do if \u00ac(x = 20) then \ngoto L5; if \u00acguard ET then goto L2; x := x + 1; if \u00acguard ET then goto L3; if (x%3 = 0) then goto L4; \n if \u00ac(x = 20) then goto L5; L2 : x := x + 1; L3 : if \u00ac(x%3 = 0) then goto L1; L4 : x := x + 3; goto L1; \nL5 : skip;  7. Correctness As advocated by Cousot and Cousot [12, par. 3.8], correctness of dynamic \nprogram transformations and optimizations should be de.ned with respect to some observational abstraction \nof program trace semantics: a program transform is correct when, at some level of abstraction, the observation \nof the execution of the subject program is equivalent to the observation of the execution of the transformed \nprogram. The approach by Guo and Palsberg [17] basically relies on a notion of correctness that requires \nthe same store changes in both the transformed/optimized program and the original program. This can be \neasily encoded by an observational abstraction asc : P(TraceP ) . P(Store * ) of trace semantics that \nobserves store changes in execution traces of a program P : * sc : TraceP . Store and Palsberg [17] s \nframework, such as dead store elimination. For example, in a program fragment such as while (x = 0) do \nz := 0; x := x + 1; z := 1; one can extract the hot path hp = (x = 0, z := 0, x := x + 1, z := 1) and \nperform dead store elimination by optimizing hp to hp ' = (x = 0, x := x+1, z := 1). As observed by Guo \nand Palsberg [17, Section 4.3], this is clearly unsound in bisimulation-based correct\u00adness because this \nhot path optimization does not output bisimilar code. By contrast, this optimization can be made sound \nin our sc(s) 8 >>>< >>>: framework by choosing an observational abstraction that records e if s = e \nstore changes at the beginning and at the exit of loops containing . if s = (., C ) extracted hot paths. \nif s = (., C0)(., C1)s ' sc((., C1)s ' .0 sc((.1, C1)s ' ) 7.1 Correctness Proof ) if s = (.0, C0)(.1, \nC1)s ' , .0 = .1 It turns out that observational correctness of the hot path extrac\u00ad asc(T ) {sc(s) | \ns . T } tion transform can be proved w.r.t. the more precise observational abstraction asc. Since asc \nobviously preserves arbitrary set unions, it admits a right adjoint .sc : P(Store * ) . P(TraceP ) de.ned \nas .sc(S) Theorem 7.2 (Correctness of trace extraction). For any P . .{T . P(TraceP ) | asc(T ) . S}, \nthat gives rise to a GC Program, hp . aN (T[extrhp(P )]) = hot (T[P ]), we have that asc In the rest \nof this section we outline a proof sketch of this result. (asc, (P(TraceP ), .), (P(Store * ), .), .sc). \nasc(T[P ]).However, the store changes abstraction asc may be too strong in practice. This condition can \nbe thus relaxed and generalized to an observational abstraction that demands to have the same Let us \n.x a hot path hp = (a0, C0, ..., an, Cn) . aN extrhp(P ). The proof relies on a mapping of traces of \nhot (T[P ]) and the program P into corresponding traces of Php that unfolds the hot path hp (or any its \ninitial fragment) according to the hot path stores (possibly just for some subset of variables) only \nat some let Php speci.c program points. For example, these program points may depend on the language. \nIn a language without output primitives and functions, as that considered in [17], we could be interested \njust in the .nal store of the program (when it terminates), or in the entry and exit stores of any loop \ncontaining an extracted hot path. If a more general language includes a sort of primitive put X that \noutputs the value of program variables ranging in some set X then we may want to have stores with the \nsame values for variables in X at each output point. Moreover, the same sequence of outputs should be \npreserved, i.e. optimizations must not modify the order of output instructions. We therefore consider \nan additional sort of actions: put X . A, where X . Var is a set of program variables. The seman\u00adtics \nof put X obviously does not affect program stores, i.e., A[put X ]. .. Correspondingly, an observational \nabstraction ao : P(TraceP ) . P(Store * ) of trace semantics observes pro\u00adgram stores at output program \npoints only (we use .|X to denote store restriction to variables in X ): out : TraceP . Store * extraction \nstrategy given by De.nition 6.1. hp, trout We de.ne two functions trin hp : TraceP . TracePhp in Fig. \n3. The .rst function, trout hp (ss), on the trace ss begins to unfold in Php the hot path hp when: (i) \ns = (., C0) where C0 is the .rst command of hp; and (ii) the condition guard Ea0 is satis.ed in the store \n.. If this unfolding for the trace ss is actually started by applying trout hp(s), hp (ss) then it is \ncarried on by applying trin i.e., with a in-modality. The second function application, trin hp(ss), carries \non the unfolding of hp in Php when: (i) s = (., Ci) where i . [1, n - 1], namely the command Ci in hp \nis different from C0 and Cn; and (ii) the condition guard Eai holds for the store .. If this is not the \ncase then trin hp((., Ci)s), after a suitable unfolding step for (., Ci), jumps back to the out-modality \nby progressing with trout hp (s). It turns out that these two functions are well de.ned and trout does \nnot alter store change sequences. hp Lemma 7.3. (1) trout and trin are well-de.ned, i.e., for any s . \nTraceP , hp hp out(s) 8 >< >: trout hp(s) . TracePhp . e if s = e hp (s), trin hp (s)) = sc(s). out(s \n' ) if s = ss ' . act(s) = put X (2) For any s . TraceP , sc(trout .|X out(s ' ) if s = (., L : put X \n. L ' )s ' Proof sketch of Theorem 7.2. Let us de.ne trhp : P(TraceP ) . ao(T ) {out(s) | s . T } Similarly \nto asc, here again we have a GC (ao, (P(TraceP ), .), (P(Store * ), .), .o). This approach is clearly \nmore general be\u00adcause the above store changes abstraction asc is more precise than ao, i.e., for any \nset of traces T , .sc(asc(T )) . .o(ao(T )), or, equivalently, asc(T1) = asc(T2) . ao(T1) = ao(T2). Example \n7.1 (Dead store elimination). The above approach based on a generic observational abstraction allows \nus to prove the correctness of program optimizations that are unsound in Guo {trout P(TracePhp ) as trhp(T \n) hp (s) | s . T }. Technically, the proof consists in showing the following two points. (A) trhp(T[P \n]) . T[Php]: this shows that for any execution trace s of P , trout hp (s) is an execution trace of \nPhp; this is not hard to prove.  (B) asc(T[Php]) . asc(T[P ]): this is proved by the following statement: \ns . T[Php] trhp(T[P ]) . F [Php]{s} . trhp(T[P ]). The proof relies on the fact that one such trace s \nis necessarily of the following shape: s = s ' (., C ) where  trout hp (E) E trout hp (ss) 8 >>>>>< \n>>>>>: (., L0 : guard Ea0 . f0)(., f0 : act(C0) . n1) trin if s = (., C0), astore ({.}) = a0 hp(s) (., \nL0 : \u00acguard Ea0 . L0)(., L0 : act(C0) . L1) trout if s = (., C0), astore ({.}) = a0 hp (s) 1) trout \n(., L0 : guard Ea0 . f0)(., f0 : \u00acact(C0) . Lc hp (s) if s = (., cmpl(C0)), astore ({.}) = a0 1) trout \n(., L0 : \u00acguard Ea0 . L0)(., L0 : \u00acact(C0) . Lc hp (s) if s = (., cmpl(C0)), astore ({.}) = a0 s \u00b7 trout \nhp (s) otherwise trin hp(E) E 8 >>>>>>>< >>>>>>>: (., ni : guard Eai . fi)(., fi : act(Ci) . ni+1) trin \nhp(s) if s = (., Ci), i . [1, n - 1], astore({.}) = ai (., nn : guard Ean . fn)(., fn : act(Cn) . L0) \ntrout hp (s) if s = (., Cn), astore ({.}) = an (., ni : \u00acguard Eai . Li)(., Ci) trout hp (s) if s = (., \nCi), i . [1, n], astore ({.}) = ai (., ni : guard Eai trin hp(ss) ) trout hp (s) next(i) . fi)(., fi \n: \u00acact(Ci) . Lc (., ni : \u00acguard Eai . Li)(., cmpl(Ci)) trout hp (s) s \u00b7 trout hp (s) if s = (., cmpl(Ci)), \ni . [1, n], astore({.}) = ai if s = (., cmpl(Ci)), i . [1, n], astore({.}) = ai otherwise Figure 3. \nDe.nition of trout and trin hp hp. act(C) . {guard Eai , \u00acguard Eai }; then, it is not hard to prove \nthat F [Php]{s ' (., C)} . trhp(T[P ]). In words, one such trace s of Php can be extended through an \nexecution step in Php to a trace in trhp(T[P ]). We therefore obtain: asc(T[P ]) = [By Lemma 7.3 (2), \nasc . trhp = asc] asc(trhp(T[P ])) . [By point (A)] asc(T[Php ) . [By point (B)] asc(T[P ] ) and this \ncloses the proof. 8. Type Specialization One key optimization for dynamic languages like JavaScript and \nPHP is type specialization, that is, using type-speci.c primitives in place of generic untyped operations \nwhose runtime execution can be very costly. As a paradigmatic example, a generic addition oper\u00adation \ncould be de.ned on more than one type, so that the execution environment must check the type of its operands \nand execute a dif\u00adferent operation depending on these types: this is the case of the ad\u00addition operation \nin JavaScript (see its semantics in the ECMA-262 standard [21, Section 11.6]) and of the semantics of \n+ in our lan\u00adguage as given in Section 2.2. Of course, type specialization avoids the overhead of dynamic \ntype checking and dispatch of generic un\u00adtyped operations. When a type is associated to each variable \nbefore the execution of a command in some hot path, this type environ\u00adment can be used to replace generic \noperations with type-speci.c primitives. The abstraction map atype : P(Value) . Types takes a set of \nval\u00adues and returns the smallest type containing it. Since Types viewed as a subset of P(Value) is closed \nunder intersections (where Any is interpreted as the top element Value and \u00d8 is the bottom el\u00adement), \natype can be indeed de.ned as a simple closure oper\u00adator (i.e., a monotonic, increasing and idempotent \nfunction) on (P(Value), .): atype(V ) n{T . Types | V . T }. Given a value v . Value, atype ({v}) thus \ncoincides with type(v). Here, the concretization function .type : Types . P(Value) is simply the identity \nmap (with Any = Value). Following the general approach described in Section 4.1, we consider a simple \nnonrelational store abstraction for types . Storet (Var . Types, .) where ..is the usual pointwise lifting \nof the ordering . for Types, so that .x.\u00d8 and .x. Any are, respectively, the bottom and top abstract \nstores in Storet . The abstraction and concretization maps astore : P(Store) . Storet and .store : Storet \n. P(Store) are de.ned as a straight instantiation of the de.nitions in Section 4.1. The abstract type \nsemantics Et : Exp . Storet . Types of expressions is de.ned as best correct approximation of the cor\u00adresponding \nconcrete semantics E on the type abstractions Storet and Types, i.e., Et [E].t atype (E[E].store (.t)). \nThis de.ni\u00adtion leads to the following equalities: Et Et = .t = type(v) (x) [v].t [x].t Et = 8.1 Type \nAbstraction Let us recall that the set of type names is 8 >>>< >>>: [E1 + E2].t \u00d8 if .i. Et = \u00d8 [Ei].t \nelse if Et [E1].t Undef else if .i. Et Et [E1].t Any = Et [E2].t . {Int, String}< Any Types = {Int, String, \nUndef, Any, \u00d8}. [Ei].t Type names can be therefore viewed as the following .nite lattice (Types, .): \nFor instance, we have that: Any  Et x + y [x/ String, y/\u00d8] = \u00d8, Et x + y [x/ String, y/ String] = String, \nInt String Undef  Et x + y [x/ Int, y/ String] = Undef, Et \u00d8 [ x + y] [x/ Int, y/ Any] = Any. otherwise \n According to Section 6, for any abstract type store [xi/Ti | xi . Var] we consider a corresponding \nBoolean action guard guard x0 : T0 \u00b7 \u00b7 \u00b7 xn : Tn . BExp whose corresponding program action has the following \nsemantics, which is automatically induced (as de.ned in Section 6) by the Galois connection (astore , \nP(Store), Storet , .store ): for any . . Example 8.3. Let us consider the following sieve of Eratosthenes \nin a Javascript-like language this is taken from the running exam\u00adple in [15] where primes is initialized \nto an array of 100 true values. With a slight abuse, we assume that our language is ex\u00adtended with Boolean \nvalues and arrays. The semantics of arrays load and stores is as usual: .rst the index expression is \nchecked to be in bounds, then the value is read or stored into the array. If the Store, index is out \nof bounds, we assume the program is aborted. ( . if .i. type(.(xi)) . Ti for (var i = 2; i < 100; i \n= i + 1) do A[guard x0 : T0 \u00b7 \u00b7 \u00b7 xn : Tn]. 8.2 Type Specialization of Hot Paths if (!primes[i]) then \ncontinue; . .i. type(.(xi)) . Ti for (var k = i + i; k < 100; k = k + i) do primes[k] = false; Let us \nconsider some hot path hp = (.0 t , C0, . . . , .t , Cn) . n aN . hot (T[P ]) on the type abstraction \n(Storet , .), where each .t i is therefore a type map. The trace extraction transform extrhp(P ) of P \nfor hp gives rise to the set stitchP (hp) of commands that stitches the hot path hp into P . Hence, for \nany i . [0, n], stitchP (hp) contains a typed guard that we simply denote as guard .t i. Typed guards \nallow us to de.ne type specialization of commands in the stitched hot path: this is de.ned as a program \ntransform that instantiates most type-speci.c addition operations in place of generic untyped additions \nby exploiting the type in\u00adformation dynamically recorded by typed guards in stitchP (hp). This program \nis encoded in our language as follows: P = C0 = L0 : i := 2 . L1, C1 = L1 : i < 100 . L2, C1 c = L1 : \n\u00ac(i < 100) . L8, C2 = L2 : primes[i] = tt . L3, Cc 2 = L2 : \u00ac(primes[i] = tt ) . L7, C3 = L3 : k := i \n+ i . L4, . C4 = L4 : k < 100 . L5, C4 c = L4 : \u00ac(k < 100) . L7, C5 = L5 : primes[k] := ff . L6, C6 \n= L6 : k := k + i . L4,  \u00af C7 = L7 : i := i + 1 . L1, C8 = L8 : skip . L . Note that if C . stitchP \n(hp) and act(C) = x := E1 + E2 then C = fi : x := E1 + E2 . L ' , for some i . [0, n], where L ' . {ni+1, \nL0}. Let Ct denote the set of commands that permits type speci.c additions +Int and +String and, in turn, \nProgramt denote the possible type specialized programs over Ct . The func\u00adtion tshp : stitchP (hp) . \nCt is de.ned as follows: tshp(C) C if act(C) = x := E1 + E2 Let us consider the type environment .t de.ned \nas .t {primes[n]/ Bool, i/ Int, k/ Int} . Storet where primes[n]/ Bool is a shorthand for primes[0]/ \nBool, . . . , primes[99]/ Bool. Then the .rst traced 2-hot path on the type abstraction Storet is: ' \n ) hp1 (.t , C4, .t , C5, .t , C6). tshp(fi : x := E1 + E2 . L .t As a consequence, the typed transform \nextraction of hp1 yields: fi : x := E1 +Int E2 . L ' if Et E1 + E2 i = Int 8 >< fi : x := E1 +String \nE2 . L ' if Et [ E1 + E2] .i t = String fi : x := E1 + E2 . L ' otherwise P1 extrhpt 1 (P ) = P {C4, \nC4 c} . {L4 : k < 100 . L5, L4 : \u00ac(k < 100) . L7} Hence, hot path type specialization TS is de.ned by \n>: . TS(stitchP (hp)) TS(stitchP (hp)) {tshp(C) | C . stitchP (hp)} . Programt . The correctness of \nthis program transform is quite straightforward. where TS(stitchP (hp)) = Let Tracet be the set of traces \nfor type specialized programs in . H0 = L4 : guard (primes[n] : Bool, i : Int, k : Int) . f0, Programt \nand let tt : Tracet . Trace be de.ned as follows: H0 c = L4 : \u00acguard (primes[n] : Bool, i : Int, k : \nInt) . L4, tt(E) E H1 = f0 : k < 100 . n1, H1 c = f0 : \u00ac(k < 100) . L7, >: tt(ss) H2 = n1 : guard (primes[n] \n: Bool, i : Int, k : Int) . f1, (., L : x := E1 + E2 . L ' )tt(s) 2 = n1 : \u00acguard (primes[n] : Bool, \ni : Int, k : Int) . L5, if s = (., L : x := E1 +Type E2 . L ' ) H3 = f1 : primes[k] := ff . n2, s \u00b7 tt(s) \notherwise 8 >cH< H4 = n2 : guard (primes[n] : Bool, i : Int, k : Int) . f2, Theorem 8.1 (Correctness \nof type specialization). For any typed Hc \u00af 4 = n2 : \u00acguard (primes[n] : Bool, i : Int, k : Int) . L6, \nhp . hot (T[P ]), we have that tt(T[TS(stitchP (hp))]) = aN H5 = f2 : k := k +Int i . L4 T[stitchP (hp)]. \nTyped trace extraction extrhp t (P ) consists in extracting and simultaneously type specializing a typed \nhot path hp in a program . 9. A General Correctness Criterion Abstract interpretation allows us to view \ntype specialization inP , i.e., it can be de.ned as follows: Section 8 just as a particular correct hot \npath optimization that can extrhp t (P ) extrhp(P ) stitchP (hp) . TS(stitchP (hp)). Correctness of typed \ntrace extraction extrt is a straight conse\u00ad hp quence of Theorems 7.2 and 8.1. Corollary 8.2 (Correctness \nof typed trace extraction). For any aN t typed hp . hot (T[P ]), we have that asc(T[extrhp(P )]) = asc(T[P \n]). be easily generalized. Guarded hot paths are a key feature of our tracing compilation model, where \nguards are dynamically recorded by the hot path monitor and range over abstract values in some store \nabstraction. An abstract guard for a command C in some hot path hp thus encodes a store property which \nis modeled in some abstract domain Store and is guaranteed to hold at the entry of C. This store information \nencapsulated by abstract guards can then be used to transform and optimize hp, i.e., all the commands \nin the stitched hot path stitchP (hp).  This provides a modular approach to proving the correctness \nof some hot path optimization O. In fact, since correctness has to be proved w.r.t. some observational \nabstraction ao of trace semantics and Theorem 7.2 ensures that this correctness holds for the store changes \nabstraction asc of the unoptimized trace extraction trans\u00adform, we just need to prove the correctness \nof the optimization O on the whole stitched hot path stitchP (hp), which thus includes the abstract guards \nof the hot path hp. Hence, .xing a program P , a hot path optimization O is modeled as a program transform \nN O : {stitchP (hp) | hp . ahot (T[P ])} . Program where Program may permit new expressions and/or actions, \nlike the case of type-speci.c addition operations in type specialization. O is required to be correct \naccording to the following de.nition. De.nition 9.1 (Correctness of hot path optimization). O is cor\u00adrect \nif for any P . Program and for any hp . aN hot (T[P ]), ao(T[O(stitchP (hp))]) = ao(T[stitchP (hp)]). \nAs an example, it would be quite simple to formalize the vari\u00adable folding optimization of hot paths \nconsidered by Guo and Pals\u00adberg [17] and to prove it correct in our framework w.r.t. the store changes \nabstraction asc. 10. Nested Hot Paths Once a .rst hot path hp1 has been extracted by transforming P to \nP1 extrhp1 (P ), it may well happen that a new hot path hp2 in P1 contains hp1 as a nested sub-path. \nFollowing TraceMonkey s trace recording strategy [15], we attempt to nest an inner hot path inside the \ncurrent trace: during trace recording, an inner hot path is called as a subroutine, this executes a loop \nto a successful com\u00adpletion and then returns to the trace recorder that may therefore register the inner \nhot path as part of a new hot path. To this aim, let us reshape the de.nitions in Section 5. Let P be \nthe original program and P ' be some hot path transform of P so that P ' P contains all the commands \n(guards included) in the hot path. We de.ne a function hotcut : TraceP ' . (StateP ' ) * that cuts from \na trace in P ' all the states whose commands appear in (some previous) hot path hp except the entry and \nexit states of hp: We then consider the following trace in T[P1]: s = ([x/?], C0)([x/0], H0)([x/0], H1)([x/0], \nH2)([x/0], H3)([x/1], H4)([x/1], H5) \u00b7 \u00b7 \u00b7 ([x/2], H3)([x/3], H4)([x/3], H 5 c)([x/3], C4)([x/6], H0) \n\u00b7 \u00b7 \u00b7 ([x/9], H 5 c)([x/9], C4)([x/12], H0) \u00b7 \u00b7 \u00b7 Thus, here we have that hotcut(s) = ([x/?], C0)([x/0], \nH0)([x/3], H 5 c)([x/3], C4)([x/6], H0)([x/9], H 5 c)([x/9], C4) \u00b7 \u00b7 \u00b7 so that hp2 = (T, H0, T, H 5 c \n, T, C4) . a2 outerhot (T[P1]) Hence, hp2 contains a nested hot path, which is called at the begin\u00adning \nof hp2 and whose entry and exit commands are, respectively, H0 and H5 c . Let hp = outerhot (T[P ]) be \na N (a0, C0, . . . , an, Cn) . aN ' \u00adhot path in P ', where, for all i . [0, n], Ci = Li : Ai . Lnext(i). \nLet us note that: If for all i . [0, n], Ci . P then hp actually is a hot path in P , i.e., hp . aN hot \n(T[P ]). Otherwise, there exists some Ck . P . If Ci . P and Ci+1 . P then Ci+1 is the entry command \nof some inner hot path; on the other hand, if Ci . P and Ci+1 . P then Ci is the exit command of some \ninner hot path. The transform of P ' for extracting hp is a generalization of De.nition 6.1. De.nition \n10.2 (Nested trace extraction transform). The nested trace extraction transform of P ' for the hot path \nhp is: extrhp(P ' ) P (1) ({C0 | C0 . P } . {cmpl(C0) | cmpl(C0) . P }) (2) . {L0 : act(C0) . L1 | C0 \n. P } . {L0 : \u00acact(C0) . Lc 1 | cmpl(C0) . P } (3) . {L0 : guard Ea0 . f0, L0 : \u00acguard Ea . L0 | C0 \n. P } (4) . {fi : act(Ci) . ni+1 | i . [0, n - 1], Ci, Ci+1 . P }. {fn : act(Cn) . L0 | Cn . P }  (5) \n. {fi : \u00acact(Ci) . Lc hotcut(s) next(i) | i . [0, n], Ci, cmpl(Ci) . P } E if s = E (6) . {ni : guard \nEai . fi, ni : \u00acguard Eai . Li | s0 hotcut(s1 \u00b7 \u00b7 \u00b7 s|s|-1) otherwise 8 >>><' ()()hotcut(., C., Cs ) \n. . }i [1], CP1 1 3 3 , ni 'if ()()() .s ., C., C., Cs &#38; C, C, CP= 1 1 2 2 3 3 1 2 3. {. | . - . \n. }(7) f(C) Li [01], CP, CP: act, n i i i+1 i i+1{| . - . . }(8) Ci [01], CP, CP, n i ii+1 In turn, we \nde.ne outerhotN : TraceP ' . P((StateP ' ) * ) as (9) . {Li : act(Ci) . ni+1 | i . [0, n - 1], Ci . P, \nCi+1 . P } >>>: follows: outerhotN (s) {(ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj ) . (StateP ' ) * | .(.i, Ci) \u00b7 \u00b7 \u00b7 (.j \n, Cj ) . loop (hotcut(s)) s.t. i = j, astore ((.i, Ci) \u00b7 \u00b7 \u00b7 (.j , Cj )) = (ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj ), \n count (astore (hotcut(s)), (ai, Ci) \u00b7 \u00b7 \u00b7 (aj , Cj )) = N}. Clearly, when P ' = P we have that hotcut \n= .s.s so that outerhotN = hotN . Finally, we de.ne the collecting version aN .T . .s.T outerhotN (s). \nouterhot Example 10.1. Let us consider again Example 6.3, where Store is the trivial one-point store \nabstraction {T}. In Example 6.3, we .rst extracted hp1 = (T, C1, T, C2, T, C 3 c) by transforming P to \nP1 extrhp(P ). while stitchP ' (hp) (3) . (4) . (5) . (6) . (7) . (9). Let us observe that: Clauses \n(1) (6) are the same clauses of De.nition 6.1, with the additional constraints that Ci and cmpl(Ci) are \nall commands in P , conditions which are trivially satis.ed in De.nition 6.1.  Clause (7) where Ci . \nP and Ci+1 . P , namely next(Ci) is the call program point of a nested hot path nhp and Ci+1 is the entry \ncommand of nhp, performs a relabeling that allows to correctly nest nhp in hp.  Clauses (8) (9) where \nCi . P and Ci+1 . P , i.e., Ci is the exit command of a nested hot path nhp that returns to the program \npoint lbl(Ci+1), performs the relabeling of suc(Ci) in Ci in order to return from nhp to hp;   L0, \nfi and ni are fresh labels, i.e., they have not been used in P ' . Example 10.3. Let us go on with Example \n10.1. The second traced hot path in a2 outerhot (T[P1]) is: hp2 = (T, H0 = L1 : guard ET . f0, T, H5 \nc = f2 : (x%3 = 0) . L4, T, C4 = L4 : x := x+3 . L1). According to De.nition 10.2, trace extraction of \nhp2 in P1 yields the following transform: extrhp2 (P1) [by clause (8)] P1 {H5 c} [by clause (9)] . {f2 \n: (x%3 = 0) . l2} [by clause (6)] . {l2 : guard ET . n2, l2 : \u00acguard ET . L4} [by clause (4)] . {n2 : \nx := x + 3 . L1} where we used additional fresh labels in l2 and n2. Example 10.4. Let us consider again \nExample 8.3. After the trace extraction of hp1 that transforms P to P1, a second traced 2-hot path is \nthe following: hp2 (.t , C1, .t , C2, .t , C3, .t , H0, .t , H1 c , .t , C7) where .t = {primes[n]/ \nBool, i/ Int, k/ Int} . Storet . hp2 contains a nested hot path which is called at suc(C3) = L4 and whose \nentry and exit commands are, respectively, H0 and H1 c . Here, typed trace extraction according to De.nition \n10.2 provides the following transform of P1: . P2 extr t hp2 (P1) P1 {C1, C1 c} . L1 : i < 100 . L2, \nL1 : \u00ac(i < 100) . L8, H6 = L1 : guard (primes[n] : Bool, i : Int, k : Int) . f3, H6 c = L1 : \u00acguard (primes[n] \n: Bool, i : Int, k : Int) . L1, H7 = f3 : i < 100 . n4, H7 c = f3 : \u00ac(i < 100) . L8, H8 = n4 : guard \n(primes[n] : Bool, i : Int, k : Int) . f4, H8 c = n4 : \u00acguard (primes[n] : Bool, i : Int, k : Int) . \nL2, H9 = f4 : primes[i] = tt . n5, Hc 9 = f4 : \u00ac(primes[i] = tt ) . L7, H10 = n5 : guard (primes[n] \n: Bool, i : Int, k : Int) . f5, Hc 10 = n5 : \u00acguard (primes[n] : Bool, i : Int, k : Int) . L3, \u00af H11 \n= f5 : k := i +Int i . L4 . {H1 c} . (H1 c) ' = f0 : \u00ac(k < 100) . n6, H12 = n6 : guard (primes[n] : Bool, \ni : Int, k : Int) . f6, Hc 12 = n6 : \u00acguard (primes[n] : Bool, i : Int, k : Int) . L7, \u00af H13 = f6 : i \n:= i +Int 1 . L1 . Finally, a third traced 2-hot path in P2 is the following: hp3 (.t , H6, .t , H9 c \n, .t , C7) which contains a nested hot path which is called at the beginning of hp3 and whose entry and \nexit commands are, respectively, H6 and H9 c . Here, typed trace extraction of hp3 yields: . P3 extrhp3 \n(P2) P2 {H9 c} . (H9 c) ' = f4 : \u00ac(primes[i] = tt ) . n7, n7 : guard (primes[n] : Bool, i : Int, k : \nInt) . f7, n7 : \u00acguard (primes[n] : Bool, i : Int, k : Int) . L7, \u00af f7 : i := i +Int 1 . L1 . We have \nthus obtained the same three trace extraction steps as described by Gal et al. [15, Section 2]. In particular, \nin P1 we specialized the typed addition operation k := k +Int i, in P2 we specialized k := i +Int i and \ni := i +Int 1, while in P3 we specialized once again i := i +Int 1 in a different hot path. Thus, in \nP3 all the addition operations have been type specialized. 11. Comparison with Guo and Palsberg s Framework \nA formal model for tracing JIT compilation has been put forward in POPL 2011 by Guo and Palsberg [17]. \nIts main distinctive feature is the use of bisimulation [23] to describe operational equivalence between \nsource and optimized programs. In this section we show how this model can be expressed within our framework. \n11.1 Language and Semantics Guo and Palsberg [17] employ a simple imperative language with while loops \nand a so-called bail construct. E ::= n | x + 1 B ::= x = 0 | x = 0 Cmd 3 c ::= skip | x := E | if B \nthen S | while B do S | bail B to S Stm 3 S ::= E | c; S The baseline small-step operational semantics \n.B. State \u00d7 State, where State Store \u00d7 Stm, is standard. Let us just recall the se\u00admantics of bail commands: \n(., (bail B to S); K) .B (., K) if B . = false (., (bail B to S); K) .B (., S) if [ B] . = true If TraceGP \n{s . State * | .i . [0, |s|). si .B si+1}denotes the set of program traces for Guo and Palsberg s lan\u00adguage \nthen given a program S . Stm, the trace transition operator GP[S] : P(TraceGP ) . P(TraceGP ) is de.ned \nas usual: GP[S](X) {(., S) | . . Store} . {sss ' | ss . X, s .B s ' } so that the trace semantics of \nS is TGP [S] lfp(GP[S]) . P(TraceGP ). 11.2 Language Compilation Programs in Stm can be easily compiled \ninto Program by resort\u00ading to an injective labeling function f : Stm . L that assigns different labels \nto different statements. De.nition 11.1 (Language compilation). The compilation func\u00adtion C : Stm . P(C) \nis recursively de.ned by the following clauses: C(E) {f(E) : skip . L} C(S = skip; K) {f(S) : skip . \nf(K)} . C(K) C(S = x := E; K) {f(S) : x := E . f(K)} . C(K) C(S = (if B then S ' ); K) {f(S) : B . f(S \n' ; K), f(S) : \u00acB . f(K)} . C(S ' ; K) . C(K) ' '' C((while B do S ); K) C((if B then (S ; while B do \nS )); K) C(S = (bail B to S ' ); K) {f(S) : B . f(S ' ), f(S) : \u00acB . f(K)} . C(S ' ) . C(K) Example 11.2. \nConsider the following program S . Stm in Guo and Palsberg s syntax:  x := 0; while B1 do x := 1; x \n:= 2; bail B2 to x := 3; x := 4; E S is then compiled in our language by C in De.nition 11.1 as follows: \n. C(S) = f0 : x := 0 . fwhile, fwhile : B1 . f1, fwhile : \u00acB1 . f2, f1 : x := 1 . fwhile, f2 : x := 2 \n. fbail, fbail : B2 . f3, fbail : \u00acB2 . f4, \u00af f3 : x := 3 . fE, f4 : x := 4 . fE, fE : skip . L . Correctness \nfor the above compilation function C means that for any S . Stm: (i) C(S) . Program and (ii) program \ntraces of S and C(S) have the same store sequences. If st : TraceGP . Trace . Store * returns the store \nsequence of a trace, i.e., st(E) E and st((., S)s) . \u00b7 st(s), and, for a set X of traces ast (X) {st(s) \n| s . X}, then correctness goes as follows: Theorem 11.3 (Correctness of language compilation). For any \nS . Stm, C(S) . Program and ast (TGP [S]) = ast (T[C(S)]). 11.3 Bisimulation Correctness of trace extraction \nin [17] relies on the following notion of bisimulation, which is parameterized by program stores. De.nition \n11.4 ([17]). A relation R . Store \u00d7 Stm \u00d7 Stm is a bisimulation when R(., S1, S2) implies: (1) if (., \nS1) .B (. ' , S 1 ' ) then (., S2) . * (. ' , S 2 ' ), for some (. ' , S 2 ' ) such that R(. ' , S 1 \n' , S 2 ' ); (2) R(., S2, S1). B S1 is bisimilar to S2 for a given store ., denoted by S1 . S2, if R(., \nS1, S2) for some bisimulation R. It is simple to characterize this program equivalence through an abstraction \nmap of traces that observes store changes (this is analogous to the de.nition of sc in Section 7). stc \n: Store * . Store * stc(E) E stc(.) . ( stc(.2s) if .1 = .2 stc(.1.2s) .1 \u00b7 stc(.2s) if .1 = .2 Given \n. . Store, a. : P(TraceGP ) . P(Store * ) is de.ned as stc follows: a. (X) {stc(s) | s . ast(X), s0 = \n.}. stc Theorem 11.5. For any S1, S2 . Stm, . . Store, we have that S1 . S2 iff a. (TGP [S1]) = a. (TGP \n[S2]). stc stc 11.4 Hot Paths and Trace Extraction In Guo and Palsberg s model [17]: (i) hot paths always \nbegin with an entry while-loop conditional, which is however not included in the hot path; (ii) the store \nof a hot path is recorded at the end of the .rst loop iteration and is a concrete store; and (iii) hot \npaths actually are 1-hot paths according to our de.nition. Guo and Palsberg s hot loops can be modeled \nin our framework by relying on a loop selection map loop GP : Trace . P(C* \u00d7 Store) de.ned as follows: \n. loop GP ((.0, C0) \u00b7 \u00b7 \u00b7 (.n, Cn)) (CiCi+1 \u00b7 \u00b7 \u00b7 Cj , .j+1) | 0 = i = j < n, Ci <Cj , lbl(Cj+1) = lbl(Ci), \n\u00af .k . (i, j ]. Ck . {Ci, cmpl(Ci)} . Notice that, for simplicity, the above de.nition includes the entry \nloop conditional in the hot path. The map aGP : P(Trace) . hot P(C* \u00d7 Store) then lifts loop GP to sets \nof traces: aGP hot (T ) .s.T loopGP (s). Let us thus consider a hot path hp = (C0C1 \u00b7 \u00b7 \u00b7 Cn, .) . aGP \nhot (T[P ]), for some P . Program (where P may coincide with a compiled C(S) for some S . Stm) and let \nus follow the same notation used in Section 6. Guo and Palsberg s [17] trace extraction scheme is de.ned \nas follows, where the hot path hp cannot be re\u00adentered once execution leaves hp. De.nition 11.6 (GP trace \nextraction transform). The GP trace extraction transform of P for the hot path hp is: GP extrhp (P ) \nP {C0} . {L0 : act(C0) . f1} . {fi : act(Ci) . fi+1 | i . [1, n)} . {fn : act(Cn) . L0} . {fi : \u00acact(Ci) \n. Lc next(i) | i . [1, n], cmpl(Ci) . P }. GP Clearly, extrhp (P ) remains a well-formed program. The \ncor\u00adrectness of this GP trace extraction transform, which is stated and proved in [17, Lemma 3.6], goes \nas follows. Theorem 11.7 (Correctness of GP trace extraction). For any P . Program, hp = (C0 \u00b7 \u00b7 \u00b7 hot \n(T[P ]), we have Cn, .) . aGP GP that a. (T[extrhp (P )]) = a. stc stc(T[P ]). Example 11.8. Let us consider \nthe program P in Example 2.1 and the GP-hot path hp = (C1C2C3 c, . = [x/1]) . aGP hot (T[P ]). A corresponding \n2-hot path hp1 with the same sequence of com\u00admands has been selected in Example 5.1 and extracted in \nExam\u00ad ple 6.3. Here, the GP trace extraction of hp provides the following program transform: extr GP \nP {C1} hp (P ) . {L1 : x = 20 . f1, f1 : x := x + 1 . f2} . {f2 : \u00ac(x%3 = 0) . L1, f2 : (x%3 = 0) . L4}. \n12. Further Work We have put forward a formal model of tracing compilation and correctness of hot path \noptimization based on program trace se\u00admantics and abstract interpretation. We see a number of interesting \navenues for further work on this topic. We aim at making use of this framework to study and relate the \nfoundational differences be\u00adtween traditional static vs dynamic tracing compilation. We then expect to \nformalize and prove the correctness of most bene.cial optimizations employed by tracing compilers of \npractical dynamic languages like JavaScript, PHP and Python. For example, we plan to cast in our model \nthe allocation removal optimization for Python described in [5] in order to formally prove its correctness. \nFi\u00ad nally, we plan to adapt our framework in order to provide a model of whole-method just-in-time compilation, \nas used, e.g., by Ion-Monkey [14], the current JIT compilation scheme in the Firefox JavaScript engine. \nAcknowledgments We are grateful to the anonymous referees for their helpful com\u00adments. The work of Francesco \nRanzato was partially supported by Microsoft Research Software Engineering Innovation Founda\u00adtion 2013 \nAward (SEIF 2013) and by the University of Padova un\u00adder the project BECOM.  References [1] V. Bala, \nE. Duesterwald, and S. Banerjia. Dynamo: a transparent dynamic optimization system. In Proceedings of \nthe ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2000), pages 1 12, \nNew York, NY, USA, 2000. ACM. [2] R. Barbuti, N. De Francesco, A. Santone, and G. Vaglini. Abstract interpretation \nof trace semantics for concurrent calculi. Information Processing Letters, 70(2):69 78, 1999. [3] M. \nBebenita, F. Brandner, M. Fahndrich, F. Logozzo, W. Schulte, N. Tillmann, and H. Venter. SPUR: a trace-based \nJIT compiler for CIL. In Proceedings of the ACM International Conference on Object Oriented Programming \nSystems Languages and Applications (OOP-SLA 2010), pages 708 725, New York, NY, USA, 2010. ACM. [4] I. \nB \u00a8 ohm, T.J.K. Edler von Koch, S.C. Kyle, B. Franke, and N. Topham. Generalized just-in-time trace compilation \nusing a parallel task farm in a dynamic binary translator. In Proceedings of the 32nd ACM SIG-PLAN Conference \non Programming Language Design and Implemen\u00adtation (PLDI 2011), pages 74 85, New York, NY, USA, 2011. \nACM. [5] C.F. Bolz, A. Cuni, M. Fijalkowski, M. Leuschel, S. Pedroni, and A. Rigo. Allocation removal \nby partial evaluation in a tracing JIT. In Proceedings of the 20th ACM SIGPLAN Workshop on Partial Eval\u00aduation \nand Program Manipulation (PEPM 2011), pages 43 52. ACM, 2011. [6] C.F. Bolz, A. Cuni, M. Fijalkowski, \nand A. Rigo. Tracing the meta\u00adlevel: PyPy s tracing JIT compiler. In Proceedings of the 4th Workshop \non the Implementation, Compilation, Optimization of Object-Oriented Languages and Programming Systems \n(ICOOOLPS 2009), pages 18 25, New York, NY, USA, 2009. ACM. [7] C. Colby and P. Lee. Trace-based program \nanalysis. In Proceedings of the 23rd ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages \n(POPL 1996), pages 195 207, New York, NY, USA, 1996. ACM. [8] P. Cousot. Constructive design of a hierarchy \nof semantics of a transi\u00adtion system by abstract interpretation (extended abstract). Electronic Notes \nin Theoretical Computer Science, 6(0):77 102, 1997. Proceed\u00adings of the 13th Annual Conference on Mathematical \nFoundations of Progamming Semantics (MFPS XIII). [9] P. Cousot. Constructive design of a hierarchy of \nsemantics of a transition system by abstract interpretation. Theoretical Computer Science, 277(1-2):47 \n103, 2002. [10] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice model for static analysis \nof programs by construction or approxima\u00adtion of .xpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN \nSymposium on Principles of Programming Languages (POPL 1977), pages 238 252, New York, NY, USA, 1977. \nACM. [11] P. Cousot and R. Cousot. Systematic design of program analy\u00adsis frameworks. In Proceedings \nof the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (POPL 1979), pages 269 \n282, New York, NY, USA, 1979. ACM. [12] P. Cousot and R. Cousot. Systematic design of program transformation \nframeworks by abstract interpretation. In Proceedings of the 29th ACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Languages (POPL 2002), pages 178 190, New York, NY, USA, 2002. ACM. [13] Mozilla Foundation. \nTraceMonkey. wiki.mozilla.org, October 2010. [14] Mozilla Foundation. IonMonkey. wiki.mozilla.org, May \n2013. [15] A. Gal, B. Eich, M. Shaver, D. Anderson, D. Mandelin, M.R. Haghighat, B. Kaplan, G. Hoare, \nB. Zbarsky, J. Orendorff, J. Ruder\u00adman, E.W. Smith, R. Reitmaier, M. Bebenita, M. Chang, and M. Franz. \nTrace-based just-in-time type specialization for dynamic languages. In Proceedings of the 2009 ACM SIGPLAN \nConference on Programming Language Design and Implementation (PLDI 2009), pages 465 478, New York, NY, \nUSA, 2009. ACM. [16] A. Gal, C.W. Probst, and M. Franz. HotPathVM: an effective JIT compiler for resource-constrained \ndevices. In Proceedings of the 2nd International Conference on Virtual Execution Environments (VEE 2006), \npages 144 153. ACM, 2006. [17] S. Guo and J. Palsberg. The essence of compiling with traces. In Proceedings \nof the 38th ACM SIGACT-SIGPLAN Symposium on Prin\u00adciples of Programming Languages (POPL 2011), pages 563 \n574, New York, NY, USA, 2011. ACM. [18] M. Handjieva and S. Tzolovski. Re.ning static analyses by trace\u00adbased \npartitioning using control .ow. In Proceedings of the 5th International Static Analysis Symposium (SAS \n1998), volume 1503 of LNCS, pages 200 214. Springer, 1998. [19] Facebook Inc. The HipHop Virtual Machine, \nFacebook Engineering, December 2011. [20] Google Inc. A new crankshaft for V8, The Chromium Blog, December \n2010. [21] Ecma International. ECMAScript Language Speci.cation. Standard ECMA-262, Edition 5.1, June \n2011. [22] F. Logozzo. Class invariants as abstract interpretation of trace seman\u00adtics. Computer Languages, \nSystems and Structures, 35(2):100 142, 2009. [23] R. Milner. Communication and Concurrency. Prentice \nHall, 1995. [24] M. Pall. The LuaJIT Project. luajit.org, 2005. [25] X. Rival. Symbolic transfer function-based \napproaches to certi.ed compilation. In Proceedings of the 31st ACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Languages (POPL 2004), New York, NY, USA, 2004. ACM. [26] X. Rival and L. Mauborgne. The \ntrace partitioning abstract domain. ACM Trans. Program. Lang. Syst., 29(5), 2007. [27] D.A. Schmidt. \nTrace-based abstract interpretation of operational se\u00admantics. Lisp Symb. Comput., 10(3):237 271, 1998. \n[28] F. Spoto and T. Jensen. Class analyses as abstract interpretations of trace semantics. ACM Trans. \nProgram. Lang. Syst., 25(5):578 630, 2003.     \n\t\t\t", "proc_id": "2535838", "abstract": "<p>Tracing just-in-time compilation is a popular compilation schema for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python, and PHP. It relies on two key ideas. First, it monitors the execution of the program to detect so-called hot paths, i.e., the most frequently executed paths. Then, it uses some store information available at runtime to optimize hot paths. The result is a residual program where the optimized hot paths are guarded by sufficient conditions ensuring the equivalence of the optimized path and the original program. The residual program is persistently mutated during its execution, e.g., to add new optimized paths or to merge existing paths. Tracing compilation is thus fundamentally different than traditional static compilation. Nevertheless, despite the remarkable practical success of tracing compilation, very little is known about its theoretical foundations.</p> <p>We formalize tracing compilation of programs using abstract interpretation. The monitoring (viz., hot path detection) phase corresponds to an abstraction of the trace semantics that captures the most frequent occurrences of sequences of program points together with an abstraction of their corresponding stores, e.g., a type environment. The optimization (viz., residual program generation) phase corresponds to a transform of the original program that preserves its trace semantics up to a given observation as modeled by some abstraction. We provide a generic framework to express dynamic optimizations and to prove them correct. We instantiate it to prove the correctness of dynamic type specialization. We show that our framework is more general than a recent model of tracing compilation introduced in POPL~2011 by Guo and Palsberg (based on operational bisimulations). In our model we can naturally express hot path reentrance and common optimizations like dead-store elimination, which are either excluded or unsound in Guo and Palsberg's framework.</p>", "authors": [{"name": "Stefano Dissegna", "author_profile_id": "86158881657", "affiliation": "University of Padova, Padova, Italy", "person_id": "P4383744", "email_address": "stefano.dissegna@gmail.com", "orcid_id": ""}, {"name": "Francesco Logozzo", "author_profile_id": "81100572523", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P4383745", "email_address": "logozzo@microsoft.com", "orcid_id": ""}, {"name": "Francesco Ranzato", "author_profile_id": "81100311450", "affiliation": "University of Padova, Padova, Italy", "person_id": "P4383746", "email_address": "ranzato@math.unipd.it", "orcid_id": ""}], "doi_number": "10.1145/2535838.2535866", "year": "2014", "article_id": "2535866", "conference": "POPL", "title": "Tracing compilation by abstract interpretation", "url": "http://dl.acm.org/citation.cfm?id=2535866"}