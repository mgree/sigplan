{"article_publication_date": "06-07-2008", "fulltext": "\n Copy Coalescing by Graph Recoloring Sebastian Hack ENS Lyon / Saarland University hack@cs.uni-sb.de \nAbstract Register allocation is always a trade-off between live-range split\u00adting and coalescing. Live-range \nsplitting generally leads to less spilling at the cost of inserting shuf.e code. Coalescing removes shuf.e \ncode while potentially raising the register demand and caus\u00ading spilling. Recent research showed that \nthe live-range splitting of the SSA form s f-functions leads to chordal interference graphs. This im\u00adproves \nupon two long-standing inconveniences of graph coloring register allocation: First, chordal graphs are \noptimally colorable in quadratic time. Second, the number of colors needed to color the graph is equal \nto the maximal register pressure in the program. However, the inserted shuf.e code incurred by the f-functions \ncan slow down the program severely. Hence, to make such an approach work in practice, a coalescing technique \nis needed that removes most of the shuf.e code without causing further spilling. In this paper, we present \na coalescing technique designed for, but not limited to, SSA-form register allocation. We exploit that \na valid coloring can be easily obtained by an SSA-based register allocator. This initial coloring is \nthen improved by recoloring the interference graph and assigning shuf.e-code related nodes the same color. \nThereby, we always keep the coloring of the graph valid. Hence, the coalescing is safe, i. e. no spill \ncode will be caused by coalescing. Comparing to iterated register coalescing, the state of the art in \nsafe coalescing, our method is able to remove 22.5% of the costs and 44.3% of the copies iterated coalescing \nleft over. The best so\u00adlution possible, found by a colaescer using integer linear program\u00adming (ILP), \nwas 35.9% of the costs and 51.9% of the copies iter\u00adated coalescing left over. The runtime of programs \ncompiled with our heuristic matches that of the programs compiled with the ILP technique. Categories \nand Subject Descriptors D.3.4 [PROGRAMMING LANGUAGES]: Processors Compilers, Code generation General \nTerms Algorithms, Languages, Performance Keywords Register Allocation, SSA Form, Graph Coloring 1. Introduction \nGraph-coloring has probably been the most successful approach for global register allocation for over \n25 years. Its major advantage, the Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 08, June 7 13, 2008, Tucson, Arizona, USA. Copyright c . 2008 ACM \n978-1-59593-860-2/08/06. . . $5.00 Gerhard Goos Universit\u00a8at Karlsruhe ggoos@ipd.info.uni-karlsruhe.de \nelegant abstraction of the problem to graph coloring, is also its ma\u00adjor inconvenience: Chaitin et al. \n[7] showed, that each undirected graph can occur as an interference graph during register allocation. \nThis argument renders register allocation, seen in the graph col\u00adoring setting, NP-complete. Furthermore, \nthe proof of Chaitin et al. has another inconvenient consequence: Theoretically, the gap between the \nregister pressure (the maximal number of simultane\u00adously live variables) and the chromatic number (the \nnumber of col\u00adors needed for an optimal coloring) of the graph can be arbitrarily large. However, Chaitin \ns proof only holds if splitting the variables live ranges is forbidden [2]. For example, if we split \nthe live ranges at the borders of basic blocks, the interference graph degenerates into a set of interval \ngraphs. Each interval graph can be optimally colored in quadratic time. Even more extreme, splitting \nafter each instruction generates an interference graph that solely consists of cliques (completely connected \nsubgraphs) that are trivially col\u00adorable. Fabri [10] noticed already in 1979 that live-range splitting \ncan reduce the gap between the chromatic number and the register pressure. Recent research [6, 3, 14] \nshows that the live-range splitting caused by the f-functions of the SSA form [9] is of great interest \nfor register allocation: The interference graphs of SSA-form pro\u00adgrams are chordal. In short, this means \nthat they can be colored optimally in quadratic time. Furthermore, the chromatic number of an SSA-form \nprogram s interference graph is equal to the regis\u00adter pressure in the program [2, 14]. This property \nis especially at\u00adtractive because it allows for decoupling spilling and coloring: An SSA-based register \nallocator .rst lowers the register pressure to k. The properties of SSA interference graphs guarantee \nthat the graph is then k-colorable and no further spilling will take place during coloring. To split \nthe live range of a variable, a new de.nition of the variable is created, and related to the old variable \nusing some sort of move instruction. This move instruction allows the variable to change its register. \nIn the split-everywhere example mentioned above, splitting after each instruction lets the variables \nchange their registers after each instruction thus attaining maximal .exibility concerning the assignment \nof the registers. However, this freedom is bought by the insertion of additional code that can severely \nslow down the program s execution. Minimizing this shuf.e code is the task of the coalescing part of \nthe register allocator. As live-range splitting may lower the chro\u00admatic number (or at least improve \nthe colorability of the graph concerning some heuristic), coalescing can result in the opposite. Hence, \na good coalescing algorithm should keep the right balance between maintaining the colorability and removing \nas much shuf.e code as possible. Traditionally, one distinguishes between two ap\u00adproaches: Unsafe approaches \n[7, 21] prioritize the minimization of copies and accept that the register demand might be pushed over \nthe limit and spill code is inserted. Safe coalescing techniques [4, 11] guarantee that if the graph \nwas colorable before coalescing a copy, it will be afterwards. In SSA-based register allocation, a pressure-based \nspilling heuristic can already establish the k-colorability of the program. Using an unsafe coalescer \nwould only add unnecessary memory operations to the program. As we follow the never insert a load to \nsave a copy principle, this is not acceptable. Usually, chordal graphs are k-simpli.able, i. e. colorable \nwith the simplify/select scheme due to Chaitin [7] and Briggs [4]. However, the way reg\u00adister constraints \nare treated in these allocators generally renders those graphs uncolorable regarding this scheme (see \nSection 6.3). Since all safe coalescing algorithms rely on this coloring scheme, they cannot be used \nin the context of SSA-based register allocation. Hence, for SSA-based register allocation, there is a \nneed for a safe coalescing heuristic. 1.1 Contributions In this paper we present a novel approach for \nsafe coalescing. Thereby, we base on SSA-based register allocation. As stated above, a valid register \nassignment is easily obtainable under the SSA form. We start with the interference graph of the program \nand an initial coloring that is then improved by our algorithm. To im\u00adprove a coloring, we try to assign \ncopy-related variables (i. e. the source and the target of a copy) the same color. When changing the \ncolor of a variable, con.icts with interfering variables can arise. For these con.icting neighbors, we \ntry to .nd new colors that eliminate the con.ict. This can in turn cause con.icts with other neighbors \nand so on. In short, we propagate these color con.icts recursively through the graph by recoloring variables. \nThe advantage of the recoloring paradigm is twofold: 1. We always maintain a valid coloring: Recoloring \ncan be treated as a transaction. Changing the color of a variable can cause a series of color changes \nto other variables. When changing the color of a variable, we do not overwrite its old color but keep \nthe new color in a separate .eld and mark the variable as temporar\u00adily changed. If we succeed in propagating \nthe con.icts through the graph, the recoloring transaction is committed by setting the variable s old \ncolor to the new one. If not, we rollback by re\u00advoking the temporary color from all affected variables. \nHence, we can aggressively try to bring as many variables to the same color as we want. The validity \nof the coloring will never be vi\u00adolated. 2. The algorithm is highly adjustable: Since the validity of \nthe coloring is always maintained, the algorithm can be interrupted at any time (unless it commits a \ntransaction). Compile time against solution quality can be easily traded by omitting parts of the algorithm \nthat are considered to be too compile-time expensive. Section 5.4 gives some hints at possible compile\u00adtime \noptimizations.   1.2 Structure of the Paper The next section reviews the basic concepts of SSA-based \nregis\u00adter allocation. Preceded by a small section that presents the used notation and formalism, Section \n4 outlines the basic ideas of the coalescing algorithm. A more detailed discussion is then provided in \nSection 5. Section 6 presents the experimental evaluation of the algorithm. Finally, Section 7 discusses \nrelated work and Section 8 concludes. 2. The Problem Register allocation on the SSA form uses the live-range \nsplitting caused by f-functions. The f-functions of a basic block basically act as control-.ow dependent \nparallel copies: x3 . f(x1,x2) y3 . f(y1,y2) Figure 1. f-functions are parallel Copies This splitting \nand the dominance property of the SSA form1 cause the interference graphs for SSA-form programs to be \nchordal (see [6, 3, 14] for proofs). Chordal graphs have two properties that make them appealing for \nregister allocation: 1. They are optimally colorable in quadratic time2. 2. The size of the largest \nclique in the graph is equal to the graph s chromatic number.  Furthermore, for each clique in the interference \ngraph there is a location in the program where all the variables of the clique are live. Thus, unlike \nconventional graph-coloring register allocation, lowering the register pressure to the number of available \nregisters k results in a k-colorable interference graph. Hence, pressure-based spilling heuristics [18, \n20] already lead to k-colorable interference graphs. Register targeting can also be modelled using live-range \nsplit\u00adting [13]. Optimally coloring a chordal graph in the presence of precolored3 nodes is NP-complete. \nHowever, if each color is used at most once in a precoloring, the problem becomes polynomial again [17]. \nBy splitting all live-ranges using a parallel copy in front of a constrained instruction, the interference \ngraph is separated into two non-interfering components. This guarantees that each compo\u00adnent contains \nprecolored nodes from only one instruction. Under the assumption that all colors used in the precoloring \nfor that in\u00adstruction are different, the instruction s component can be colored polynomially. For further \ndetail, we refer to [13]. Live-range splitting to handle register constraints and f-func\u00adtions make use \nof parallel copy instructions. These parallel copy instructions have of course to be implemented using \nreal processor instructions (in the case of f-functions this is called SSA destruc\u00adtion [5]). In contrast \nto traditional approaches, these parallel copies are implemented after register allocation and not before. \nFor example, assume our architecture has four registers. Con\u00adsider the following parallel copy and a \nregister allocation (indicated by the superscripts): 24311324 (a ,b,c ,d) . (e ,f,g ,h) Concerning the \nassigned registers, the parallel copy corresponds to a register permutation that can be implemented with \nmoves, swaps, xors, etc.: \u00bb 1 2 3 4 2 3 4 1 This is where coalescing comes into play: If all pairs (a, \ne), (b, f), . . . are assigned the same color, then no code has to be generated to implement the parallel \ncopy. Hence, the goal of co\u00adalescing is to minimize the number of instructions to implement parallel \ncopies by trying to give corresponding operands and re\u00adsults the same color. 1 The fact that each use \nof a variable must be dominated by its de.nition. 2 In fact, a chordal graph can be colored in (.(G) \n\u00b7|V |) where .(G) is the size of the largest clique in G and V is the set of G s nodes. 3 A node is precolored \nif only one speci.c color can be assigned to it. 3. Notation Let R = {1,...,k} be the set of registers. \nAn interference graph G is a tuple G =(V, E, adm, a. ) containing following elements: A set of nodes \nV .  A set of interference edges E . V \u00d7 V .  A map adm : V .P(R) indicating the allowed registers \nfor a node. For each node v there is adm(v) .  = \u00d8. A cost function a. : V \u00d7 V . N representing the \npenalty if both arguments are assigned different colors. A node is called constrained if |adm(v)| <k. \nA node v is a neighbor of some node w if (v, w) . E. The set of neighbors of a node w is called the neighborhood \nof w and is abbreviated by N(w). It is handy to talk about the set of af.nities A of G which is de.ned \nas the set of all pairs of nodes (v, w) for which a. (v, w) > 0. To better distinguish af.nities from \ninterferences we write [v, w] for af.nities and (v, w) for interferences. In .gures, we indicate an af.nity \nby dashed lines with the value of a. annotated. An af.nity component C is a set of nodes that is closed \nunder the transitivity of the af.nity relation. That is, for each pair of nodes (v, w) for which v .C \nand a. (v, w) > 0 there is w .C as well. De.nition 1 (Coloring): A map . : V . R is a coloring of G if \nthe following conditions are met: For all v . V there must be .(v) . adm(v).  For each (v, w) . E there \nmust be .(v) .  = .(w). Consider an interference graph G =(V, E, adm, a. ). We express coalescing by \nintroducing a cost function on colorings. Let . be a coloring of G. We de.ne map |\u00b7|. to describe the \npenalty an af.nity [v, w] incurs under some coloring .: |\u00b7|. : V \u00d7 V . N ( 0 if .(v)= .(w) |[v, w]|. \n.. a. (v, w) else Concerning a coloring ., we say an af.nity [v, w] is satis.ed, if |[v, w]|. =0 The \ntotal cost .G.. of the graph G under the coloring . is then given by: X .G.. := |(v, w)|. (v,w).A De.nition \n2 (Optimal Coalescing): A coloring . * : V . R is an optimal k-coalescing of a graph G if there is no \ncoloring . : V . R with .G.. < .G..* . 4. Coalescing by Recoloring The aim of coalescing as described \nabove is to .nd a coloring . that minimizes the cost incurred by unsatis.ed af.nities. In our approach \na coloring already exists and our task is to improve this coloring concerning the cost function .\u00b7... \nThereby we follow a simple principle: attempt to recolor af.nity-related nodes to the same color. Let \nus .rst outline what recoloring means and then discuss how it is applied. Recoloring . . . means to assign \na node a new color while main\u00adtaining the validity of the coloring regarding De.nition 1. Suppose we \nwant to recolor a node to a new color c. If c is not used in the node s neighborhood, just changing the \nnode s color does the job. The more interesting case is if c is also assigned to some neighbor of the \nnode. Consider Figure 2 and suppose we want to recolor the node v to color 2 to satisfy the af.nity with \nu. The color 2 is already taken by v s neighbor w. Instead of giving up, we try to recursively resolve \nthis color clash through the graph by assigning w another color. This may cause color clashes with w \ns neighbors, and so on. To avoid endless recursion, we do not allow for changing a node s color if it \nwas already recolored in the current attempt. n5 Node x with color 5 Interference Af.nity Figure 2. \nRecoloring a Node The important fact about recoloring is its transactional charac\u00adter: recoloring a node \nlaunches a transaction. For each node whose color is to be changed, we do not overwrite its old color \nbut keep the new color in a separate .eld and .ag the node as changed. Fur\u00adthermore, the node is added \nto a list that holds all nodes whose color was changed in the current recoloring transaction. Then, if \nwe do not succeed to resolve the color clash, we can rollback the whole operation by simply revoking \nthe changed .ag for each node in the list. If we do succeed, the new coloring can be committed. Therefore, \nwe overwrite the old color with the temporary one and reset the changed .ag for each node in the list. \nBuilding initial Chunks Now that we know how to recolor a node, let us investigate which node is recolored \nwhen to what color. Consider the example graph fragment in Figure 3. The nodes t, . . . , z form an af.nity \ncomponent C. For the sake of conciseness, the costs are not shown; consider them to be uniform.   \nuvwxy zt Figure 3. Interference inside an Af.nity Component First of all, C contains an interference \n(w, y). Hence, not all nodes in C can be colored with the same color: At least, either the af.nity [w, \nx] or the af.nity [x, y] is not satis.able. Assume we can recolor the nodes t, . . . , x to color 1. \nThe node y then cannot have color 1 because it interferes with w. Recoloring z to 1 is thus futile since \nthis also violates the af.nity [y, z]. Hence, trying to bring a whole af.nity component to the same color \nis generally not a good idea. Instead, we segregate the af.nity components into smaller, interference-free \npieces that are called chunks in the following. For example, the af.nity component t, . . . , z could \nbe separated into two chunks t, . . . , x and y, z. In the later phase of the algorithm, recoloring will \nonly take place inside chunks. Hence, by this initial segregation we already agree upon a set of af.nities \nthat will not be optimized later on. Furthermore, the example in Figure 3 also shows why af.nities should \nnot be optimized independently of each other but treated in chunks. Assume all af.nities would be optimized \nseparately and consider following example: First, the af.nity [t, u] is satis.ed by assigning color 1 \nto t and u. Then, w and x are assigned color 2 satisfying [w, x]. Either [u, v] or [v, w] is now lost. \nHence, looking at single af.nities is not purposeful; all nodes in a chunk have to be considered simultaneously. \nWe now have a set of chunks, each of which is best colored with a single color. To administer the chunks, \nwe use a priority queue. (Using a queue is important because, during recoloring, new chunks can be created.) \nEach chunk is assigned a cost that is the sum of all af.nities inside the chunk. The chunks are then \nexposed to the recoloring pass according to the order of the queue. Recoloring Chunks When a chunk is \nselected for recoloring, we .rst determine an order for the colors that seems to be promising for the \nchunk. Promising here means that we believe that many nodes in the chunk may be recolorable to that color. \nFor each color c, we then try to recolor each node in the chunk to c. Due to the color constraints of \nthe nodes and the graph s topol\u00adogy, we might not succeed in recoloring all nodes to the same color. \nTherefore, for each tried color we memorize the best subchunk of the chunk that was recolorable. After \ntrying all the colors, we se\u00adlect the color with the best subchunk, recolor all the nodes in that subchunk \nto that color, and .xate the colors of the nodes to prevent them from being changed by future recoloring \nattempts. The re\u00admaining nodes that did not make it into the best subchunk are used to form new chunks \nwhich are then inserted into the priority queue. 5. Details Let us start our detailed discussion with \na look at the main loop of the algorithm, shown in Figure 4. As mentioned in the last section, the chunks \nare administered in a priority queue. First, the initial chunks are computed and inserted into the priority \nqueue. Then, chunks are popped from the queue and recolored until the queue is empty. Note that recoloring \ncan add new chunks to the priority queue. def main (): queue . PriorityQueue () createInitialChunks (queue) \nwhile \u00acqueue.isEmpty (): chunk . queue.pop () newChunks . recolorChunk (chunk) for c . newChunks: queue.o.er \n(c) Figure 4. The Main Loop The next three subsections discuss the basic components (build\u00ading initial \nchunks and recoloring) of the algorithm in further detail. 5.1 Initial Chunks and the Order of the Chunks \nThe example in Figure 3 shows that there generally is, independent of the number of available colors, \na set of af.nities that cannot be satis.ed. This is due to interferences inside af.nity components. Since \nwe search for a coloring with small costs, we are interested in keeping the costs incurred by these initially \nunsatis.ed af.nities as small as possible. This problem, called optimal aggressive coalescing, is hard: \nFor general graphs, this problem corresponds to a multi-cut problem [3] and is NP-complete regarding \nthe number of af.nities. In the case of SSA-form programs, the interferences and af.nities exhibit a \nspecial structure: The interference edges form a chordal graph and the af.nities are created by live-range \nsplits. Nevertheless, aggressive coalescing stays NP-complete for these more restricted graphs, see Bouchez \net al. [3] and Hack [13]. We follow a straightforward heuristic to create an aggressive coalescing: In \nthe beginning, each node forms a chunk on its own. Then, we sort the af.nities by costs from large to \nsmall. For each af.nity [v, w] in that order, check if v s chunk interferes with w s. If not, unify both \nchunks. Note that the result of this process, in terms of incurred costs, is most sensitive to the order \nin which the af.nities are processed. Even two orders that only differ in the order  Figure 5. Constrained \nNodes inside a Chunk of af.nities with the same costs can lead to different initial costs. We will disregard \nthis issue here and leave the ordering of equal\u00adweighted chunks unde.ned. The result of this uni.cation \nprocess is a set of chunks. Each chunk C is assigned a cost computed from the costs of the af.nities \ncontained in the chunk: X cost(C)= a. (v, w) v,w.C Chunks with higher costs are in front of the queue \nand are thus recolored .rst.  5.2 Recoloring a Chunk As mentioned in Section 4, we .rst determine an \norder of the colors. Making the effort to compute such an order serves two purposes: If all nodes in \na chunk were recolorable to some color c, we will not consider further colors for this chunk. Hence, \nto keep the number of tried colors small, the colors of constrained nodes should be tried .rst. Consider \nthe chunk C1 in Figure 5. Two nodes are constrained in this chunk. The color 1 is assignable to all nodes \nin the graph, the color 2 to all but one. All other colors can only be assigned to |C1|- 2 nodes. Hence, \nthe color sequence for this chunk should start with 1 and 2. The rest is complemented ad lib. To determine \nthis order, we .rst de.ne for each node v and each color c a color preference: ( (1 + k -|adm(v)|) /k \nif c . adm(v) p(v, c)= 0 else The fewer colors are admissible, the higher is the preference for an admissible \ncolor. If the node has only one admissible color, the preference is 1 for this color and 0 for the others. \nFor an unconstrained node v the preference is 1/k for each color. Concerning a chunk C, the color preference \np(C, c) for a color c is just the sum of the color preferences of the chunk s nodes. We can respect the \nconstraints in not yet recolored chunks better: Consider chunk C2 in Figure 5 and assume the chunk C1 \nhas not yet been processed. C2 exhibits no constraints; it does not prefer one color over another. So \nassume we start with color 1 and we succeed in recoloring all nodes in C2 to color 1. Then we will encounter \nsevere penalties when processing the chunk C1 later on: C1 has two nodes that interfere with nodes in \nC2. Hence these two nodes in C1 cannot be recolored to 1. This will potentially violate af.nities to \nthe constrained nodes in C1. Hence, we introduce a secondary measure d(C, c) to express the color dislike \nof a chunk C for some color c. Let X be the set of all chunks that have not yet been processed and interfere \nwith C. The dislike d(C, c) is built by computing the average over the color preference of the interfering \nneighbor chunks: X d(C, c)=1 - 1 p(C. ,c) |X| C..X Based on both measures, the overall priority prio(C, \nc) of a color c when recoloring a chunk C is computed by: prio(C, c) = (1 - a) \u00d7 p(C, c)+ a \u00d7 d(C, c) \nNote that the preference p(C, \u00b7) of a chunk can be computed upon creating the chunk; it is static. The \ncolor dislike d(C, \u00b7) however depends on the state of the algorithm as it only considers chunks interfering \nwith C that have not yet been processed. The parame\u00adter a . [0; 1] determines the in.uence of the interfering \nchunks given to the priority and can be freely chosen by the compiler writer. Figure 7 shows the pseudocode \nof chunk recoloring. The call to computeColorOrder stands for creating the color order based on the measure \nprio. For each color in that order we try to recolor each node in the chunk to that color. Therefore, \nwe call (line 9) the method changeTo that starts a new recoloring transaction on a node. If the recoloring \nattempt was successful, we temporarily .xate the node to prevent its color from being changed by subsequent \nrecoloring attempts concerning this chunk and this color (line 11). This temporary .xa\u00adtion is released \nafter all nodes in the current chunk have been tried for the current color. If no node was recolorable \n(line 17), we di\u00adrectly continue with the next color. Otherwise, we look for the best (in terms of satis.ed \naf.nities) af.nity-connected subset of nodes inside the chunk (line 20): Consider the example in Figure \n6(a). Recoloring succeeded in changing the colors of the nodes v, x, y, z to color 3. The other nodes \nu and w failed. Keeping this coloring is not sensible, since it implies three unsatis.ed af.nities ([x, \nw], [w, v], [u, z]). However, keeping the coloring of x, y, z is more sensible, since they are mutually \nreachable via af.nities. These three node hence constitute the best subchunk as their coloring satis.es \nthree af.nities. In general, this is the connected and equally colored subcomponent of the chunk that \neliminates the most costs. (a) A recolored chunk (b) Sub-chunks of a chunk Figure 6. Chunk Recoloring \nFor each color, we determine the best subchunk. After trying all the colors and having found a best color \nand a corresponding best subchunk, we will make that color de.nitive for the nodes in the best subchunk. \nAgain, we recolor the nodes of the subchunk to the best color and then permanently .xate each node in \nthe best subchunk. Hence, their colors are from now on immutable. Finally, we have to take care of the \nnodes that are not contained in the selected best subchunk (line 20). We construct new chunks of these \nremaining nodes and offer them to priority queue for fur\u00adther optimization. Again, consider the example \non Figure 6(a). Sup\u00adpose, 3 was the best color and thus x, y, z was the best subchunk. Putting all the \nremaining nodes u, v, w in the same chunk is not sensible because trying to assign u the same color as \nv and w brings no gain. This is because there is no path of af.nities (inside the potential chunk u, \nv, w) that connects u to v or w. Therefore, the chunks that are constructed from the remaining nodes \nare formed according to af.nity connectivity. In our example, this results in two chunks: u and v, w, \nas shown in Figure 6(b). 1 def recolorChunk (chunk): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 \n22 23 24 25 27 28 29 30 31 32 33 34 35 36 37 newChunks . [] bestColor .-1 bestChunk . None order . chunk.computeColorOrder \n() for color . order : nSuccessful . 0 for node . chunk.nodes: if changeTo (node, color): nSuccessful \n. nSuccessful +1 node..xated . True un.xate all nodes for node . chunk.nodes: node..xated . False if \nno node was recolorable to that color, try the next color if nSuccessful = 0: continue search the best \nsub-chunk localBest . fragmentChunk (chunk) check, if the sub-chunk is better than the current one if \n\u00acbestChunk . localBest.cost > bestChunk.cost: bestChunk . localBest bestColor . color if all nodes were \nrecolorable, quit trying colors; we found our color if chunk.nodes.size () = nSuccessful : break If we \nfound a best chunk if bestChunk: recolor to the best color and permanently .xate the nodes for node . \nbestChunk.nodes: changeTo (node, bestColor) node..xated . True make new chunks from the remaining nodes \nnewChunks . getNewChunks (chunk, bestChunk) return newChunks Figure 7. Recoloring a Chunk  5.3 Recoloring \na Node Node recoloring is triggered by the method changeTo invoked by chunk recoloring (see line 9 in \nFigure 7). The recoloring itself is performed by recolorNode shown in Figure 8. To recolor a node v to \na color c, we .rst investigate all neighbors of v. All neighbors that are already colored by c have to \nbe recol\u00adored to a different color. In contrast to v, these neighbors have no speci.ed target color and \ncan be recolored to any admissible color but c. Reconsider Figure 2. Assume we want to recolor v to color \n2. This con.icts with the neighbor w which already is on color 2. Nevertheless, we temporarily set v \nto 2 and try to resolve the color clash with w. Furthermore, v is temporarily .xated to prevent it from \nbeing changed again in this recoloring transaction in order to avoid in.nite recursion. If .ve colors \nare available, we just assign color 5 to w and the con.ict is resolved. With only four colors, we have \nto choose among the colors of w s neighbors. The color 2 is not available, since v has already been recolored \nin this transaction and is thus no longer recolorable during this transaction. Hence, the colors 1, 3 \nand 4 remain. We choose the color that is used less frequently, i. e. 3 or 4. Consider the pseudocode \nof recoloring in Figure 8. First of all, each node is equipped with two color .elds: color holds the \nnode s true color  tempColor is used to hold the changed color during a recoloring transaction. If the \nnode is not (yet) affected by an ongoing recoloring transaction, the value of tempColor is -1.  Hence, \nto obtain the color of a node (see method getColor), we have to check if tempColor is = 0. If yes, tempColor \nholds the current color of the node. If not, the color is given by color. To commit a recoloring transaction, \ncolor is overwritten by tempColor which is in turn reset to -1. To rollback, we just reset tempColor \nto -1. Furthermore, there is a .ag .xated, which was already used in the last subsection, to declare \nthe color of a node immutable. We say, a node is loose if its color is still changeable, i. e. it has \nnot (yet) been affected by the current recoloring transaction and has not been .xated. The actual recoloring \nis performed by the method recolorNode. Besides the node to recolor, it accepts a list of possible target \ncolors (colorList) for that node. (We assume that for each call to recolorNode all colors in the list \nare admissible for the respective node.) Furthermore, we pass a list (changed) where all nodes whose \ncolors are changed by the current recoloring transaction are recorded. We attempt to recolor the node \nto the colors in the list one by one until recoloring succeeds for a color. If no color succeeds, the \nrecoloring attempt failed and we return False. To recolor the node, we investigate all its neighbors \nthat have the same color as the one we are recoloring to. The colors of these neighbors have to be changed. \nThis is done by creating a color list that excludes the color in question (the call to makeAvoidList), \nand calling recolorNode on these neighbors. Note that the color list passed to recursive calls must also \nexclude all colors that are not admissible at the neighbor, as mentioned above. Building the List of \nColors to avoid To speed up the recoloring process, we do not just simply pass a list that excludes the \ncolor to avoid. We also investigate the neighborhood of the neighbor w and assign priorities to the colors \naccording to the node s color prefer\u00adence and the color s frequency of occurrence in the neighborhood. \nThe more often a color is used in the neighborhood, the less its priority. Figure 9 shows the pseudocode \nto create the color list to avoid a color. We count the occurrence of each color among the set of non-loose \nneighbors of w (lines 13 19). If a neighbor is not loose, its color cannot be changed and hence that \nneighbor s color is no candidate for w. Hence, that color s priority is set to 0 (line 19). Then, w s \npreference of color c is weighted with the number of loose neighbors that are not assigned to c (line \n24). Finally, the priority for the color to avoid is set to 0, all zero-priority colors are deleted from \nthe list and the list is sorted in ascending order (lines 26 30). Let us return to recoloring and consider \na call of the function recolorNode. If we were not able to recolor a con.icting neighbor of the node, \neither because it was not loose (line 28) or because a recursive call of recolorNode failed (line 30), \nwe roll back all color changes made by the call under consideration. Therefore, upon entering recolorNode \nwe record the current position in the node list (gauge). The rollback resets all nodes from that position \nto the current end of the list. 1 def rollback (nodeList, gauge): 2 while nodeList.size () > gauge : \n3 node . nodeList.pop (gauge) 4 node.tempColor .-1 6 def commit (nodeList): 7 for node . nodeList: 8 \nnode.color . node.tempColor 9 node.tempColor .-1 11 def isLoose (node): 12 return \u00acnode..xated . node.tempColor \n< 0 14 def getColor (node): 15 if node.tempColor = 0: 16 return node.tempColor 17 return node.color \n 19 def recolorNode (node, colorList, changed): 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 gauge \n. changed.size () for color . colorList: success . True node.tempColor . color changed.append (node) \nfor neigh . node.neighbors: if getColor (neigh)= color : success . False if neigh.isLoose (): list . \nmakeAvoidList (neigh, color) success . recolorNode (neigh, list, changed) if \u00acsuccess: rollback (changed[gauge]) \n break if success: return True return False 38 def changeTo (node, targetColor): 39 if node.color = \ntargetColor : 40 return True 41 if isLoose (node) . targetColor . node.adm : 42 if recolorNode (node, \n[targetColor], [ ]) : 43 commit (changed) 44 return True 45 return False Figure 8. Recoloring a Node \nThe driver for recoloring is given by the function changeTo. It calls recolorNode with a color list containing \njust the desired target color. If the recoloring attempt was successful, the new coloring (represented \nby the tempColor .elds of the changed nodes) is committed and the transaction thus completed.  5.4 Tuning \nPossibilities As mentioned in the introduction, the algorithm allows for several possibilities to adjust \nits runtime. In this section, we want to brie.y outline some of these possibilities: 2 def makeAvoidList \n(node, avoidColor): 3 list . [] 4 prio . [] 5 neighCol . [] 6 for col . colors : 7 list.append (col) \n8 prio.append (p (node, col)) 9 neighCol.append (0.0) 10 obtain color usage frequency for all loose \nneighbors; colors of non-loose neighbors get priority 0 12 nLoose . 0 13 for neigh . node.neighbors \n: 14 col . neigh.getColor () 15 if isLoose (n): 16 neighCol[col] . neighCol[col]+1 17 nLoose . nLoose \n+1 18 else : 19 prio[neighCol] . 0.0 20 if there were loose neighbors, use the frequency of occurrence \n of their color to weight the priorities 21 if nLoose > 0: 22 factor . 1.0/nLoose 23 for col . colors \n: 24 prio[col] . prio[col] * (1 - factor * neighCol[col]) 25 the color to-avoid gets priority 0 26 prio[avoidColor] \n. 0.0 27 remove all elements with priority = 0 28 list . .lter (lambda a : prio[a] > 0.0, list) 29 sort \nthe color list in descending priority order 30 list.sort (lambda a, b : cmp (prio[b], prio[a])) 31 return \nlist Figure 9. Building the avoid-color List Time limit In principal, the algorithm is interruptible \nat any time (besides when committing a recoloring transaction, of course). Hence, a simple time limit \nis applicable. The compiler can dispense a certain amount of time, probably dependent on the size of \nthe interference graph, to coalescing. After the time limit is exceeded, the current transaction is .nished \nand coalescing is simply stopped. Optimizing only parts of the graph In a dynamic compilation scenario, \nthe compiler can use pro.le information to determine the important traces through the control-.ow graph. \nThe com\u00adputation of the initial chunks could be modi.ed to only regard af.nities related to shuf.e code \non these traces. Ignoring colors Not all colors need to be tried when recoloring a chuck. One could only \ntry the .rst one and live with the results. Limit recolor depth In large interference components4 the \nrecur\u00adsion depth of recolorNode may get too large for potentially present compile-time constraints. A \nsimple way to overcome this, is to limit the recursion depth to a .xed number. We elab\u00adorate more on \nthis possibility in the next section. 4 Alike the de.nition of an af.nity component, an interference \ncomponent is a set of nodes that is closed under interference. Hence, if node v is contained in the interference \ncomponent I, all neighbors of v are also in I. 6. Experimental Evaluation Our experimental evaluation \nconsists of two parts: First, we ana\u00adlyze the input and the runtime of the algorithm. To this end, we \npresent a brief characterization of the interference graphs of our benchmarks, a breakdown of recoloring \noperations, and an exper\u00adimental investigation of the algorithm s runtime dependent on the input size. \nSecond, we investigate the quality of the results produced by the algorithm in terms of eliminated costs \nand runtime of the compiled programs. 6.1 Setup We implemented the presented coalescing algorithm in \nthe lib-Firm [15] compiler. This compiler produces code for the x86 ar\u00adchitecture and features a completely \nSSA-based register allocator as presented in [13]. All measurements were conducted on the in\u00adteger part \nCINT2000 of the CPU2000 benchmark [8]. The program 252.eon was not compiled because the used compiler \nis not able to process C++. All measurements were taken on a Pentium 4 2.4GHz PC with 1GB RAM running \na Linux with kernel version 2.6.11. All presented data only considers the general-purpose registers of \nthe x86. The number of registers available was seven (k =7). Fol\u00adlowing the techniques outlined in Section \n5.4 we limited the re\u00adcoloring depth to 4 in all our experiments. The parameter a (see Section 5.2) was \nset to 0.1. The cost of an af.nity was set to the execution frequency of the (parallel) copy it corresponds \nto. The execution frequencies were statically calculated using a Markov\u00adchain model [23]. The solver \nused in the ILP experiments was CPLEX 9.1.  6.2 Compile-time Behavior of the Algorithm Let us .rst direct \nour attention to the interference graphs to which the algorithm was exposed. Structure of the Graphs \nFigure 10(b) presents an overview over the characteristics of the graphs. Each line stands for one of \nthe eleven benchmark programs. The column #Graphs gives the total number of interference graphs in the \nrespective benchmark. In total, 6120 interference graphs were investigated. One factor for the runtime \nof the algorithm is the number of nodes in the graph. The average size number of nodes in the graphs \n(column avg |V | ) ranges from 86.23 to 419.13 nodes. The max\u00adimum is 10191 nodes (see column max |V \n| ). The distribution of |V | is shown in Figure 10(a). The recursion depth of the method recolorNode \nis bounded by the size of the interference component the recolored node is in. The column avg CompSize \nin Figure 10(b) gives the average inter\u00adference component sizes. As can be seen, due to the immense live\u00adrange \nsplitting caused by the register constraints of the x86, the components are not large. Thus, with a component \ncontaining only 8 nodes, the recursion depth is structurally limited to 7. Further\u00admore, most nodes are \ninvolved in af.nities as the average af.nity degree of all nodes (column avg AffDeg ) is always greater \nthan 1. Recoloring Operations Considering the number of (recursive) calls to recolorNode caused by a \nsingle recoloring attempt (func\u00adtion changeTo), there were 269718 attempts that caused only one call \nto recolorNode. In total there were 610748 calls to recolorNode (including recursive calls). Hence recoloring \nstopped at the .rst node in 44% of all cases. The maximum number of recolorNode calls caused by a single \ncall to changeTo was 327. The left table in Figure 11 shows the frequency (column Count ) of the number \n(column #succ. Calls ) of recolorNode calls ini\u00adtiated by a single call to changeTo. The .ve lines shown \nalready represent 85% of all calls to recolorNode. The rest is omitted for the sake of brevity. Count \n 6 5 6120 Bench 164 175 #Graphs 86 255 avg |V | max |V | avg AffDeg 118.48 637 1.75 135.86 2061 1.64 \navg CompSize 7.16 7.14 4 176 2202 175.98 7395 1.72 8.56 3 181 186 26 109 86.23 321 1.59 419.13 9810 1.75 \n8.40 9.06 2 197 324 97.32 1604 1.71 6.74 1 253 1067 149.78 10191 1.78 8.06 254 863 166.07 3238 1.68 8.91 \n0 <50 <100 <200 <500 <1000 <2000 <8 #Nodes 255 923 152.27 3379 2.05 8.42 256 74 107.92 633 1.64 6.32 \nCount 300 191 239.66 2627 1.62 8.42 (a) N ode dis tributi on in the Interference Graphs (b) Interference \nGraph Statistics  Remaining Costs Rem. Costs unopt. 0.2 0.1 0 164 175  176 181 186 197 253 254 255 \n256 300 Bench Phi Heur T50 Heur T100 Heur ILP (c) Remaining Costs Remaining Aff. Rem. Aff. unopt. \n0.3 0.2 0.1 0 164 175  176 181 186 197 253 254 255 256 300 Bench Phi Heur T50 Heur T100 Heur ILP \n(d) Remaining Af.nities Runtime Runtime unopt. 1 0.95 0.9 0.85 0.8 0.75 0.7  Bench Phi Heur T50 Heur \nT100 Heur ILP (e) Runtime Experiments on CINT2000 Figure 10. Experimental Evaluation # succ. Calls Count \nRec. Depth Count 1 269718 0 269718 2 94492 1 104904 3 9952 2 6588 4 4160 3 2926 5 2246 4 2686 Figure \n11. Statistics of recolorNode calls  The recursion depth of recolorNode shows a similar behavior (see \nthe right table in Figure 11). The vast majority of recoloring attempts stays within one recursive call, \ni. e. only the nodes in the direct neighborhood are touched. Runtime of the Algorithm Figure 12 shows \nthe measured runtime of the coalescing algorithm against the number of nodes in the graph in a logarithmic \nplot. For better readability we only plotted one mark that represents the average runtime for the given \nnode set size. To indicate a rough scale, the lines above and below the point cloud represent a linear \nand a quadratic function with appropriately chosen constants c1 and c2. Fitting the measured data against \na function fa,p(x)= a \u00b7 xp, we determined a 0.05 and p 1.2 (indicated by the dotted line). Thus, given \nthe structure of interference graphs of our setting, the algorithm exhibited a sub-quadratic runtime \nperformance.  6.3 Quality of the Results We compared the results produced by our coalescing algorithm \nto other approaches. Figures 10(c-e) juxtapose the following methods: NoOpt No coalescing at all; The \ncoloring was taken as provided by the coloring phase. There was no effort to improve that coloring whatsoever. \nPhi The recoloring-based coalescing algorithm presented by Hack et al. [14]. In contrast to the algorithm \npresented in this paper, this algorithm considers each live-range split separately and is specially tuned \nfor the optimization of f-functions. Heur The heuristic presented in this paper. As stated above, the \nrecoloring depth was limited to 4. Furthermore, we also in\u00adcluded two variants using time limits: T50 \nand T100. For these variants, the optimization time was limited to t \u00d7|V | \u00b5s for t .{50, 100}. Regarding \na maximum |V | of 10000, the re\u00adcoloring part of the algorithm ran, for the largest graph in the benchmarks, \nfor at most 500 ms (1s respectively). ILP An ILP formulation of the coalescing problem [12]. We re\u00adstricted \nthe solver s time to .nd an optimal solution to 5 min\u00adutes. 293 graphs were not solved optimally due \nto this time limit. Note, that exceeding the time limit does not imply that the found solution is not \noptimal; The solution might be opti\u00admal but the solver was just not able to prove it in time. Figure \n10(c) shows for each coalescing algorithm and benchmark the ratio of costs remaining after optimization \nto costs remaining when no coalescing is performed. Over all graphs in the respective benchmark, we added \nup the remaining costs and divided them by the sum over the costs remaining when no coalescing is applied. \nThe values for the ILP coalescing constitute lower bounds to the fractions: If the ILP coalescing was \nnot optimal on a graph, we incurred the best bound the solver knew when the time limit ran out. Note \nthat this best bound is not the objective value of the current solution but a number known to the solver \nthat is always smaller or equal to the optimum. Hence, the optimal fraction cannot be smaller than the \nvalue in the ILP column. Figure 10(d) shows the ratio of af.nities that remain unsat\u00adis.ed after coalescing \n(with the respective algorithm) to the un\u00adsatis.ed af.nities remaining when coalescing is not performed. \nThe number of unsatis.ed af.nities in an interference graph cor\u00adresponds roughly5 to the number of copies \nleft in the program. In 186.crafty, the ILP coalescer leaves more copies unsatis.ed than our algorithm. \nThis is because we are minimizing for costs; which does not always correspond to minimizing the number \nof unsatis\u00ad.ed af.nities. Finally, we ran the compiled programs on the above mentioned machine. Figure \n10(e) shows the results of this experiment. Dis\u00adplayed is the runtime relative to the programs compiled \nwithout coalescing. Hence, values smaller than 1 mean that the program ran faster. To explain why the \nILP based solutions are sometimes slower than the one generated by the heuristic we have to consider: \n1. If the ILP coalescer could not optimally coalesce a graph within 5 minutes, we took the best solution \nthe solver had found so far. Hence, sometimes the ILP coalescing might be worse than the one found by \nour heuristic. 2. The costs, which are derived from statically computed execu\u00adtion frequencies may not \nre.ect the runtime behavior of the benchmark program precisely and thus mislead the solver. The heuristic \nmight be unaffected by this by producing a theo\u00adretically worse solution that works better in practice. \n Discussion The runtime experiments show clearly that, when op\u00adtimizing for program speed, coalescing \nis indispensable in a register allocator with extensive live-range splitting. Our coalescing algo\u00adrithm \nimproved the runtime of the programs by 0.60% to 23.14% with an average of 11.33% compared to the uncoalesced \nversions. Comparing to the Phi-Algorithm, the runtime improvements are not that big (up to 7.98% with \nan average of 1.78%). However, our al\u00adgorithm leaves less than the half of the costs (and af.nities) \nbehind. While this has a direct impact on the code size, it does not seem to have a dramatic impact to \nthe performance of the compiled pro\u00adgrams; at least not on the processor used in our experiments. For \nthis processor, there seems to be a cost threshold that has to be attained to produce ef.cient code. \nBeyond that threshold further optimization is not re.ected in faster program execution. It has yet to \nbe determined how sensitive other processor architectures are to coalescing. 5 Depending on how a parallel \ncopy is implemented, the number of instruc\u00adtions needed to implement a parallel copy is generally not \nequal to the num\u00adber of unsatis.ed af.nities of that parallel copy (see Grund and Hack [12] for examples). \nThe time-limited versions mostly behave normally: the more time is put into optimization the more costs \nare optimized. How\u00adever, for benchmarks 253.perlbmk and 300.twolf, 50 ms seem to be too short to achieve \ngood results: the coalescer even produces programs that are slower than their unoptimized counterparts. \nOn the other hand, 100 ms seem to be suf.cient for our benchmarks and processor. In a more general context \nit has to be said that the cost model is much too simple to really re.ect all parameters of modern pro\u00adcessors. \nHence, obtaining very good results in terms of optimized costs does not generally mean to improve the \ncompiled programs performance. Comparison to traditional Allocators Comparing our approach to traditional \nsafe graph-coloring allocators/coalescers is dif.cult. The major problem is the difference in handling \nof register con\u00adstraints. In normal graph-coloring register allocation, a clique of size k is added to \nthe graph. Each node in that clique directly cor\u00adresponds to a register. The constraints of nodes are \nthen expressed by interferences to this register clique. Chordal graphs can be colored6 with the standard \nsimplify/select scheme [2]. By adding the register clique and according inter\u00adferences to our chordal \ninterference graphs, several graphs were no longer simpli.able (but k-colorable!). Of 6120 graphs in \nour benchmark, 4353 remained simpli.able. We ran iterated register coalescing [11] (the state of the \nart in safe coalescing) on these graphs and compared the remaining costs and unsatis.ed af.nities: Iterated \ncoalescing left 1.28 times more costs non-eliminated and 1.79 times more af.nities unsatis.ed than the \nalgorithm presented in this paper. To put it another way, from the cost iterated coalescing left, we \nwere able to optimize 22.5% away (and 44.3% of the af.nities). The optimum, as determined by the ILP, \nis given by 35.9% of the costs (and 51.9% of the af.nites) iterated coalescing left over. 7. Related \nWork The .rst graph-coloring allocator due to Chaitin et al. [7] used ag\u00adgressive coalescing and did \nnot make any effort at all to respect the chromatic number of the graph. Since then, a lot of work was \ndone on safe coalescing. Briggs et al. [4] introduced conservative coalescing. To decide whether an af.nity \ncan be coalesced, they considered the degree of the resulting coalesced node. Only if that node s degree \nwas lower than k, the copy was coalesced. George and Appel s iterated coalescing [11] improves upon conservative \ncoalescing by applying Briggs et al. s criterion and a new one iter\u00adatively to the graph. Park and Moon \nleft the road of safe coalescing and improved upon the aggressive scheme. They .rst perform an aggressive \ncoalescing like Chaitin et al. When, during coloring, a coalesced node is not colorable, the coalescing \nof this node is un\u00addone and the resulting nodes are normally colored or spilled if not possible. However, \nthere is no mechanism used to tell if the co\u00adalescing of a node deteriorates the colorability of the \ngraph. Fur\u00adthermore, once colored, the coalescing of a node cannot be made undone. Both together might \ninitiate spilling. Bouchez et al. [3] investigate the theoretic background of coalescing. They show that \ncoalescing is NP-complete concerning the number of af.nities, also in the SSA-based setting. Live-range \nsplitting has often been proposed to aid coloring. To our knowledge, Fabri [10] was the .rst to observe \nthis. Appel and George [1] presented an ILP approach to reduce the register pressure everywhere to k. \nAfter this transformation, they were in need of a coloring algorithm that colors the program without \nfurther spilling. Their solution was to split around every instruction 6 not optimally, but k-colored \nand use optimistic coalescing that (provably) does not spill in this scenario. Lueh et al. s fusion based \nallocator [16] integrates live\u00adrange splitting into the register allocator. They start by building the \ninterference graphs of certain regions (that can be basic blocks, loops, traces, etc.) that are not imposed \nby the allocator but can be chosen by the compiler writer. In a later step, the interference graphs are \nfused to form the complete interference graph. During this fusion process, live ranges can be split or \nspilled if the fused interference graph was no longer colorable. Recently, Nakaike et al. [19] proposed \nan approach that splits around basic blocks and uses coalescing to unify split live-ranges in hot code \nregions. SSA register allocation is a rather young .eld. In 2005, three groups [2, 6, 14] independently \nfrom each other discovered that the interference graphs of SSA-form programs are chordal. Pereira and \nPalsberg [22] earlier noted that a very high percentage of (non-SSA) interference graphs, from a compiler \nthat used SSA during the compilation process, were chordal. Hack et al. [14] already presented a coalescing \nscheme in the SSA setting. However, it is only targeted at optimizing f-functions and is not suitable \nfor optimizing larger af.nity components that arise when live-range splitting is also used for register \nconstraints as discussed in this paper. Although our algorithm is safe as it does not lead to spilling, \nit is closely related to optimistic coalescing by Park and Moon [21]: We also perform aggressive coalescing \nin the beginning. Hence our chunks initially correspond to the interference graph s nodes. How\u00adever, \nas optimistic coalescing intertwines coloring and coalescing it cannot foresee if the coalescing of a \nnode (giving all the nodes in the chunk the same color) will violate the coloring. The presented algorithm \nhowever does so by adapting the coloring while coalesc\u00ading the nodes of the chunk. Hence, our method \ncan be seen as a safe variant of optimistic coalescing. 8. Conclusions We presented a safe coalescing \ntechnique based on graph recolor\u00ading. In contrast to all graph-coloring coalescing techniques, we do \nnot integrate coalescing into the coloring process but require that a valid coloring of the interference \ngraph already exists. Although our algorithm has been developed in the context of SSA-based reg\u00adister \nallocation, our technique is independent of the underlying al\u00adlocator. Its sole prerequisite is a validly \ncolored interference graph. We evaluated our technique on top of an SSA-based register allocator in a \ncompiler targeting the x86 architecture. The programs compiled with our heuristic ran on average 11.33% \nfaster compared to the program using no coalescing at all. Compared to another coalescing heuristic for \nSSA-based register allocation, we were able to eliminate more than 50% of the costs that heuristic left \nover, resulting in an speedup of 1.78%. Comparing to iterated coalescing, which is considered to be the \nstate of the art in safe coalescing, our algorithm was able to remove 22.5% of the costs and 44.3% of \nthe copies that iterated coalescing left over. Acknowledgments We want to thank Michael Beck, Matthias \nBraun, Benoit Boissinot, Florent Bouchez, Alain Darte, Daniel Grund, and Fabrice Rastello for many fruitful \ndiscussions. Florent Bouchez kindly provided his implementation of iterated coalescing. A big thank you \ngoes to Christian W\u00a8urdig for porting parts of the code. Last but not least, we thank the anonymous reviewers \nfor their thorough work and their productive comments. References [1] Andrew W. Appel and Lal George. \nOptimal Spilling for CISC Machines with Few Registers. In ACM SIGPLAN 2001 Conference on Programming \nLanguage Design and Implementation, pages 243 253, June 2001. [2] Florent Bouchez, Alain Darte, Christophe \nGuillon, and Fabrice Rastello. Register Allocation: What does the NP-Completeness Proof of Chaitin et \nal. really prove? or revisiting Register Allocation: Why and How? In 19th International Workshop on Languages \nand Compilers for Parallel Computing (LCPC 06), New Orleans, USA, Nov 2006. [3] Florent Bouchez, Alain \nDarte, and Fabrice Rastello. On the Complexity of Register Coalescing. In International Symposium on \nCode Generation and Optimization (CGO 07), San Jose, USA, March 2007. IEEE Computer Society Press. [4] \nPreston Briggs, Keith D. Cooper, and Linda Torczon. Improvements to Graph Coloring Register Allocation. \nACM Transactions on Programming Languages and Systems, 16(3):428 455, 1994. [5] Preston Briggs, Keith \nD.Cooper, Timothy J. Harvey, and L. Taylor Simpson. Practical Improvements to the Construction and Destruc\u00adtion \nof Static Single Assignment Form. Software: Practice and Experience, 28(8):859 881, July 1998. [6] Philip \nBrisk, Foad Dabiri, Roozbeh Jafari, and Majid Sarrafzadeh. Optimal Register Sharing for High-Level Synthesis \nof SSA Form Programs. IEEE Trans. on CAD of Integrated Circuits and Systems, 25(5):772 779, 2006. [7] \nG. J. Chaitin, M. A. Auslander, A. K. Chandra, J. Cocke, M. E. Hopkins, and P. W. Markstein. Register \nallocation via graph coloring. Journal of Computer Languages, 6:45 57, 1981. [8] Standard Performance \nEvaluation Corporation. SPEC CPU2000 V1.3. http://www.spec.org/cpu2000/. [9] R. Cytron, J. Ferrante, \nB. K. Rosen, M. N. Wegman, and F. K. Zadek. Ef.ciently computing static single assignment form and the \ncontrol dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, October \n1991. [10] Janet Fabri. Automatic Storage Optimization. In SIGPLAN 79: Proceedings of the 1979 SIGPLAN \nSymposium on Compiler Construction, pages 83 91, New York, NY, USA, 1979. ACM Press. [11] Lal George \nand Andrew W. Appel. Iterated Register Coalescing. ACM Transactions on Programming Languages and Systems, \n18(3):300 324, 1996. [12] Daniel Grund and Sebastian Hack. A Fast Cutting-Plane Algorithm for Optimal \nCoalescing. In Shriram Krishnamurthi and Martin Odersky, editors, Compiler Construction, volume 4420 \nof Lecture Notes In Computer Science, pages 111 115, March 2007. [13] Sebastian Hack. Register Allocation \nfor Programs in SSA Form. PhD thesis, Universit\u00a8at Karlsruhe, October 2007. [14] Sebastian Hack, Daniel \nGrund, and Gerhard Goos. Register Allocation for Programs in SSA Form. In Andreas Zeller and Alan Mycroft, \neditors, Compiler Construction, volume 3923, pages 247 262. Springer, March 2006. [15] The libFirm Compiler. \nhttp://www.libfirm.org. [16] Guei-Yuan Lueh, Thomas Gross, and Ali-Reza Adl-Tabatabai. Fusion-based Register \nAllocation. ACM Transactions on Program\u00adming Languages and Systems, 22(3):431 470, 2000. [17] D\u00b4aniel \nMarx. Graph Coloring with Local and Global Constraints. PhD thesis, Budapest University of Technology \nand Economics, 2004. [18] Robert Morgan. Building an Optimizing Compiler. Digital Press, Newton, MA, \nUSA, 1998. [19] Takuya Nakaike, Tatsushi Inagaki, Hideaki Komatsu, and Toshio Nakatani. Pro.le-based \nGlobal Live-Range Splitting. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 216 227, New York, NY, USA, 2006. ACM. [20] Michael Paleczny, \nChristopher Vick, and Cliff Click. The Java HotSpotTMServer Compiler. In Proceedings of the Java Virtual \nMachine Research and Technology Symposium (JVM 01), April 2001. [21] Jinpyo Park and Soo-Mook Moon. Optimistic \nRegister Coalescing. ACM Transactions on Programming Languages and Systems, 26(4):735 765, 2004. [22] \nFernando Magno Quint ao Pereira and Jens Palsberg. Register allocation via coloring of chordal graphs. \nIn Proceedings of APLAS 05, volume 3780 of Lecture Notes In Computer Science, pages 315 329. Springer, \nNovember 2005. [23] Tim A. Wagner, Vance Maverick, Susan L. Graham, and Michael A. Harrison. Accurate \nStatic Estimators for Program Optimization. In PLDI 94: Proceedings of the ACM SIGPLAN 1994 Conference \non Programming Language Design and Implementation, pages 85 96, New York, NY, USA, 1994. ACM.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Register allocation is always a trade-off between live-range splitting and coalescing. Live-range splitting generally leads to less spilling at the cost of inserting shuffle code. Coalescing removes shuffle code while potentially raising the register demand and causing spilling.</p> <p>Recent research showed that the live-range splitting of the SSA form's &#198;-functions leads to <i>chordal</i> interference graphs. This improves upon two long-standing inconveniences of graph coloring register allocation: First, chordal graphs are optimally colorable in quadratic time. Second, the number of colors needed to color the graph is equal to the maximal register pressure in the program. However, the inserted shuffle code incurred by the &#198;-functions can slow down the program severely. Hence, to make such an approach work in practice, a coalescing technique is needed that removes most of the shuffle code <i>without</i> causing further spilling.</p> <p>In this paper, we present a coalescing technique designed for, but not limited to, SSA-form register allocation. We exploit that a valid coloring can be easily obtained by an SSA-based register allocator. This initial coloring is then improved by recoloring the interference graph and assigning shuffle-code related nodes the same color. Thereby, we always keep the coloring of the graph valid. Hence, the coalescing is safe, i. e. no spill code will be caused by coalescing.</p> <p>Comparing to iterated register coalescing, the state of the art in safe coalescing, our method is able to remove 22.5% of the costs and 44.3% of the copies iterated coalescing left over. The best solution possible, found by a colaescer using integer linear programming (ILP), was 35.9% of the costs and 51.9% of the copies iterated coalescing left over. The runtime of programs compiled with our heuristic matches that of the programs compiled with the ILP technique.</p>", "authors": [{"name": "Sebastian Hack", "author_profile_id": "81315488920", "affiliation": "ENS Lyon/Saarland University, Saarbr&#252;cken, Germany", "person_id": "P1022790", "email_address": "", "orcid_id": ""}, {"name": "Gerhard Goos", "author_profile_id": "81100584316", "affiliation": "Universit&#228;t Karlsruhe, Karlsruhe, Germany", "person_id": "P1022791", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375610", "year": "2008", "article_id": "1375610", "conference": "PLDI", "title": "Copy coalescing by graph recoloring", "url": "http://dl.acm.org/citation.cfm?id=1375610"}