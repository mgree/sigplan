{"article_publication_date": "06-07-2008", "fulltext": "\n Register Allocation by Puzzle Solving Fernando Magno Quint JensPalsberg ao Pereira UCLA Computer Science \nDepartment University of California, Los Angeles {fernando,palsberg}@cs.ucla.edu Abstract We show that \nregister allocation can be viewed as solving a collec\u00adtionof puzzles.We model the register .le asa puzzleboard \nand the program variables as puzzle pieces; pre-coloring and register aliasing .t in naturally. For architectures \nsuch as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented \nthe puzzle solver with a simple heuristic for spilling.For SPECCPU2000, the compilation time of our imple\u00admentationis \nasfast as thatof theextendedversionof linear scan used by LLVM, which is the JIT compiler in the openGL \nstack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced \nby the slower, state-of-the-art iterated register coalescing of George and Appel with the exten\u00adsions \nproposedby Smith, Ramsey, andHollowayin 2004. Categories and Subject Descriptors D.3.4[Processors]: Code \ngeneration General Terms Algorithms, Theory Keywords Register allocation, puzzle solving, register aliasing \n1. Introduction Researchers and compiler writers have used a variety of ab\u00adstractions to model register \nallocation, including graph color\u00ading [12, 17, 36], integer linear programming [2, 19], partitioned Boolean \nquadratic optimization [21, 35], and multi-commodity network .ow [25]. These abstractions represent different \ntrade\u00adoffsbetween compilation speed and quality of the produced code. For example, linear scan [33, 38] \nis a simple algorithm based on the coloring of interval graphs that produces code of reasonable quality \nwithfast compilation time; iterated register coalescing [17] is a more complicated algorithm that, although \nslower, tends to produce code of better quality than linear scan. Finally, the Appel-George algorithm \n[2] achieves optimal spilling, with respect to a cost model, in worst-case exponential time via integer \nlinear pro\u00adgramming. In this paper we introduce a new abstraction: register allocation by puzzle solving.We \nmodel the register .le asa puzzle board and the programvariables aspuzzle pieces. The resultisa collection \nof puzzles with one puzzle per instruction in the intermediate rep\u00adresentation of the source program.We \nwill show that puzzles are Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page.To \ncopyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 08, June 7 13, 2008,Tucson, Arizona, USA. Copyright c . 2008ACM 978-1-59593-860-2/08/06... \n$5.00 easy to use, that we can solve them ef.ciently, and that theypro\u00adduce code that is competitive \nwith the code produced by state-of\u00adthe-art algorithms. Speci.cally, we will show how for architectures \nsuch as PowerPC, x86, and StrongARM we can solve each puz\u00adzle in linear time in the number of registers, \nhow we can extend the puzzle solver with a simple heuristic for spilling, and how pre\u00adcoloring and register \naliasing .t in naturally. Pre-colored variables are variables that have been assigned to particular registers \nbefore register allocation begins; two register names alias [36] when an assignment to one register name \ncan affect the value of the other. We have implemented a puzzle-based register allocator. Our register \nallocator has four steps: 1. transform the program into an elementary program by aug\u00admenting it with \nspecial instructions called .-functions [13], p\u00adfunctions [5], and parallel copies (using the technique \ndescribed in Section 2.2); 2. transform the elementary program into a collection of puzzles (using the \ntechnique describedin Section2.2); 3. do puzzle solving, spilling, and coalescing (using the tech\u00adniques \ndescribedin Sections3and4);and .nally 4. transform the elementary program and the register allocation \nresult into assembly code (by implementing .-functions, p\u00adfunctions, and parallel copies using the permutations \ndescribed by Hack et al. [20]).  For SPEC CPU2000, our implementation is as fast as the ex\u00adtended version \nof linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. We compare \nthe x86 code produced by gcc, our puzzle solver, the version of lin\u00adear scan used by LLVM [14], the iterated \nregister coalescing algo\u00adrithm of George and Appel [17] with the extensions proposed by Smith, Ramsey, \nand Holloway [36], and the partitioned Boolean quadratic optimization algorithm [21]. The puzzle solver \nproduces codethatis,onaverage,fasterthanthecode producedbyextended linear scan, and of similar quality \nto the code produced by iterated register coalescing. Unsurprisingly, the exponential-time Boolean optimization \nalgorithm produces thefastest code. The key insight of the puzzles approach lies in the use of elementary \nprograms, which are described in Section 2.2. In an elementary program, all live ranges are small and \nthat enables us to de.ne and solve one puzzle for each instruction in the program. In the following section \nwede.ne our puzzles andin Section3 weshowhowtosolvethem.In Section4 wepresentourapproach to spilling \nand coalescing, and in Section5 we discuss some opti\u00admizationsin the puzzle solver.Wegive ourexperimental \nresultsin Section 6, and we discuss related work in Section 7. Finally, Sec\u00adtion8concludes the paper. \nTheextendedversionof our paper [32] has appendices with the proofs of the four theorems stated in this \npaper;wehave omittedtheproofsinthisversionduetospace con\u00adstraints.  Figure 1. Three types of puzzles. \n2. Puzzles A puzzle consists of a board anda setof pieces. Pieces cannot overlap on the board, and a \nsubset of the pieces are already placed on the board. The challenge is to .t the remaining pieces on \nthe board. Wewillnowexplainhowtomaparegister.letoapuzzle board andhowtomap programvariablestopuzzle pieces.Every \nresulting puzzle willbeofoneofthe three types illustratedin Figure1 ora hybrid. 2.1 From Register File \nto Puzzle Board The bank of registers in the target architecture determines the shape of the puzzle board.Everypuzzle \nboard hasa numberof separate areas, where each area is divided into two rows of squares. We will explain \nin Section 2.2 why an area has exactly two rows. The register .le may support aliasing, which determines \nthe number of columnsin each area,thevalid shapesofthepieces,andthe rules for placing the pieces on the \nboard.We distinguish three types of puzzles: type-0, type-1 and type-2, where each area of a type-n puzzle \nhas 2n columns. Type-0 puzzles. The bank of registers used in PowerPC and the bank of integer registers \nused in ARM are simple cases because they do not support register aliasing. Figure 2(a) shows the puz\u00adzle \nboard for PowerPC. Every area has just one column that corre\u00adsponds to one of the 32 registers. Both \nPowerPC and ARM give a type-0 puzzle for whichthe pieces are of the three kinds shown in Figure1.We can \nplace an X-piece on anysquarein the upper row, we can place a Z-piece on anysquare in the lower row, \nand we can placeaY-pieceonanycolumn.Itis straightforwardtoseethatwe can solve a type-0 puzzle in linear \ntime in the number of areas by .rst placing all theY-pieces on the board and then placing all the X-pieces \nand Z-pieces on the board. Type-1 puzzles. Figure 2(b) shows the puzzle board for the .oating point registers \nused in the ARM architecture. This register bank has 32 single precision registers that can be combined \ninto 16 pairs of double precision registers. Thus, every area of this puzzle board has two columns, which \ncorrespond to the two registers that canbe paired.Forexample, the 32-bit registersS0 andS1 arein the \nsame area because theycan be combined into the 64-bit register D0. Similarly,becauseS1andS2 cannotbe \ncombinedintoadouble register,theydenote columnsindifferent areas.ARMgivesatype\u00ad1puzzle for which the \npieces are of the six kinds shown in Figure 1. We de.ne the size of a piece as the number of squares \nthat it occupies on the board.Wecan placeasize-1 X-piece on anysquare in the upper row, a size-2 X-piece \non the two upper squares of any area, a size-1 Z-piece on anysquare in the lower row, a size-2 Z\u00adpieceonthetwolower \nsquaresofanyarea,a size-2Y-pieceonany Figure 2. Examples of register banks mapped into puzzle boards. \ncolumn,andasize-4Y-pieceonany area. Section3 explainshow to solve a type-1 puzzle in linear time in the \nnumber of areas. Type-2 puzzles. SPARC V8 [22, pp 33] supports two levels of register aliasing: .rst, \ntwo 32-bit .oating-point registers can be combined to hold a single 64-bit value; then, two of these \n64\u00adbit registers can be combined yet again to hold a 128-bit value. Figure 2(c) shows the puzzle board \nfor the .oating point registers ofSPARCV8.Every areahasfour columns correspondingtofour registers that \ncan be combined. This architecture gives a type-2 puzzle for which the pieces are of the nine kinds shown \nin Figure 1. The rules for placing the pieces on the board are a straightforward extension of the rules \nfor type-1 puzzles. Importantly, we can place a size-2 X-piece on either the .rst two squares in the \nupper row of an area, or on the last two squares in the upper row of an area. A similar rule applies \nto size-2 Z-pieces. Solving type-2 puzzles remains an open problem. Hybrid puzzles. The x86 gives a hybrid \nof type-0 and type\u00ad1puzzles. Figure3shows the integer-register .le of the x86, and Figure 2(d) shows \nthe corresponding puzzle board. The registers AX, BX, CX, DX give a type-1 puzzle, while the registers \nEBP, ESI, EDI, ESP give a type-0 puzzle. We treat the EAX, EBX, ECX, EDX registers as special cases of \nthe AX, BX, CX, DX registers; values in EAX,EBX,ECX,EDX takeup to 32 bits rather than 16 bits. Notice \nthat x86 does not give a type-2 puzzle because even though we can .t four 8-bitvalues intoa32-bit register,x86 \ndoes not provide register namesforthe upper 16-bit portionof thatregister.Forahybridof type-1 and type-0 \npuzzles, we .rst solve the type-0 puzzles and then the type-1 puzzles. The .oating point registers of \nSPARC V9 [39, pp 36-40] give ahybrid of a type-2 and a type-1 puzzle because half the registers can be \ncombined into quad precision registers. Figure 3. General purpose registers of the x86 architecture \n 2.2 From Program Variables to Puzzle Pieces We map program variables to puzzle pieces in a two-step \nprocess: .rst we convert a source program into an elementary program and then we map the elementary program \nintopuzzle pieces. From a source program to an elementary program. We can convert an ordinary program \ninto an elementary program in three steps. First, we transform the source program into static single \nas\u00adsignment (SSA) form [13] by renaming variables and adding .\u00adfunctions at the beginning of each basic \nblock. Second, we trans\u00adform the SSA-form program into static single information (SSI) form [1]. In our \n.avor of SSI form, every basic block ends with a p-function that renames the variables that are live \ngoing out of the basic block. (The name p-assignment was coined by Bodik et al. [5]. It was originally \ncalled s-function in [1], and switch oper\u00adators in [23].) Finally, we transform the SSI-form program \ninto an elementary programby insertinga parallelcopybetween eachpair of consecutive instructions in a \nbasic block, and renaming the vari\u00adables aliveat that point. Appel and George used the idea of inserting \nparallel copies everywhere in their ILP-based approach to register allocation with optimal spilling [2]. \nIn summary, in an elementary program, every basic block begins with a .-function, hasa parallel copybetweeneach \nconsecutivepairof instructions,andendswitha p-function. Figure4(a)showsa program,and Figure4(b)givesthe \ncorresponding elementary program. As an optimization, we have removed useless .-functions from the beginning \nof blocks with a single predecessor. In this paper we adopt the convention that lower case letters denote \nvariables that can be stored into a single regis\u00adter, and upper case letters denote variables that must \nbe stored into a pair of registers. Names in typewriter font, e.g., AL, denote pre\u00adcolored registers.We \nuse x = y to denote an instruction that uses y and de.nes x;it is not a simple copy. Cytron et al. [13]gavea \npolynomial time algorithmto convert a program into SSAform, and Ananian[1]gavea polynomial time algorithmtoconverta \nprogramintoSSIform.Wecanperformthe step of inserting parallel copies in polynomial time as well. From \nan elementary program to puzzle pieces. A program point [2] is a point between any pair of consecutive \ninstructions. For example, the program points in Figure 4(b) are p0,...,p11. The collection of program \npoints where a variable v is alive con\u00adstitutes its live range. The live ranges of programs in elementary \nform contain at most two program points.A variable v is said to be live-in at instruction i if its live \nrange contains a program point that precedes i;v is live-out at i if v sliverange containsaprogram point \nthat succeeds i.For each instruction i in an elementary pro\u00adgram we create a puzzle that has one piece \nfor each variable that is livein or liveout at i (or both). The liveranges that end at i become X-pieces; \nthe live ranges that begin at i become Z-pieces; and the live ranges that cross i becomeY-pieces. Figure5gives \nanexam\u00adpleofaprogram fragmentthat usessixvariables,anditshows their live ranges and the resulting puzzle \npieces. Wecannowexplainwhyeach areaofapuzzleboardhasexactly tworows.Wecanassignaregisterbothtoonelive \nrangethatends in the middle and to one live range that begins in the middle.We model that by placing \nan X-piece in the upper row and a Z-piece Figure 4. (a) Original program. (b) Elementary program. right \nbelow in the lower row. However, if we assign a register to a long liverange, then we cannot assign that \nregister to anyother live range.We model thatby placingaY-piece, which spans both rows. The sizes of \nthe pieces are given by the types of the variables. Forexample,forx86,an 8-bitvariablewithaliverangethatendsin \nthe middle becomes a size-1 X-piece, while a 16 or 32-bit variable with a live range that ends in the \nmiddle becomes a size-2 X\u00adpiece. Similarly, an 8-bit variable with a live range that begins in the middle \nbecomes a size-1 Z-piece, while a 16 or 32-bit variable with a live range that ends in the middle becomes \na size-2 Z-piece. An 8-bitvariable withalonglive range becomesa size-2Y-piece, whilea 16-bitvariable \nwithalonglive rangebecomesa size-4Y\u00adpiece. Figure 9(a) shows the puzzles produced for the program in \nFigure 4(b).  2.3 Register Allocation and Puzzle Solving are Equivalent The core registerallocation \nproblem, also known as spill-free regis\u00adter allocation, is: given a program P and a number K of available \nregisters, can each of the variables of P be mapped to one of the K registers such that variables with \ninterfering live ranges are as\u00adsigned to different registers? In case some of the variables are pre-colored, \nwe call the prob\u00adlem spill-free register allocation with pre-coloring. THEOREM 1. (Equivalence) Spill-free \nregister allocation with pre-coloring for an elementary program is equivalent to solving a collection \nof puzzles. Figure 5. Mapping programvariables into puzzlepieces. Figure 6. Padding: (a) puzzle board, \n(b) pieces before padding, (c) pieces after padding. The new pieces are marked with stripes. 3. Solving \nType-1 Puzzles Figure 8 shows our algorithm for solving type-1 puzzles. Our algorithmic notation is visual \nrather than textual. The goal of this section is to explain how the algorithm works and to point out \nseveral subtleties.Wewilldothatintwosteps.Firstwewill de.ne a visual language of puzzle solving programs \nthat includes the program in Figure 8. After explaining the semantics of the whole language, we then \nfocus on the program in Figure 8 and explain how seemingly innocent changes to the program would make \nit incorrect. Wewill study puzzle-solving programs thatworkbycompleting one area at a time.To enable \nthat approach, we may have to pad a puzzle before the solution process begins. If a puzzle has a set \nof pieces with a total area that is less than the total area of the puzzle board, then a strategy that \ncompletes one area at a time may get stuck unnecessarily becauseofa lackof pieces.So,wepad such puzzles \nby adding size-1 X-pieces and size-1 Z-pieces, until these two properties are met: (i) the total area \nof the X-pieces equals the total area of the Z-pieces; (ii) the total area of all the pieces is 4K, where \nK is the number of areas on the board. Note that total area includes also pre-colored squares. Figure6illustrates \npadding. In the fullversion[32]weshowthata puzzleissolvableifandonlyif its padded version is solvable. \n3.1 A Visual Language of Puzzle Solving Programs We say that an area is complete when all four of its \nsquares are covered by pieces; dually, an area is empty when none of its four squares are covered by \npieces. The grammarin Figure7de.nesavisual languagefor program\u00adming type-1 puzzle solvers: a program \nis a sequence of statements, and a statement is either a rule r or a conditional statement r : s. We \nnow informally explain the meaning of rules, statements, and programs. Figure 7. Avisual language for \nprogrammingpuzzle solvers. Rules. A rule explains how to complete an area. We write a rule as a two-by-two \ndiagram with two facets: a pattern, that is, dark areas which show the squares (if any) that have to \nbe .lled in already for the rule to apply; anda strategy, that is, a description of how to complete the \narea, including which pieces to use and where toput them.Wesay thatthe patternofa rule matches an area \na if the pattern is the same as the already-.lled-in squares of a.For a rule r and an area a where the \npattern of r matches a, the application of r to a succeeds, if the pieces needed by the strategy of \nr are available; the result is that the pieces needed by the strategy of r are placed in a;  the application \nof r to a fails otherwise.  has a pattern consisting of just one square namely, the square in the top-right \ncorner, and a strategy consisting of taking one size-1 X-piece and one size-2 Z-piece and placing the \nX-piece in the top\u00adleft corner and placing the Z-piece in the bottom row. If we apply the rule to the \narea and one size-1 X-piece and one size-2 Z-piece are available, then the result is that the two pieces \nare placed in the area, and the rule succeeds. Otherwise, if one or both of the two needed pieces are \nnotavailable, then the rulefails.We cannot apply the rule to the area because the pattern of the rule \ndoes not match this area. Statements. For a statement that is simply a rule r, we have explained above \nhow to apply r to an area a where the pattern of r matches a.Foraconditional statement r : s,we require \nall the rules in r : s to have the same pattern, which we call the pattern of r : s. For a conditional \nstatementr : s and an area a where the pattern of r : s matches a, the application of r : s to a proceeds \nby .rst applying r to a;if that application succeeds, thenr : s succeeds (and s is ignored); otherwise \nthe result of r : s is the application of the statement s to a. Programs. The execution of a program \ns1 ... sn on a puzzle P proceeds as follows: For eachi from1to n: For each areaa of P such that the pattern \nof si matches a: - apply si to a - if the application of si to a failed, then terminate the entireexecution \nand reportfailure Example. Letus considerin detailtheexecutionofthe program on the puzzle . The .rst \nstatement has a pattern which matches only the .rst area of the puzzle. So, we apply the .rst statement \nto the .rst area, which succeeds and resultsin the followingpuzzle. . The second statement has a pattern \nwhich matches only the second area of the puzzle. So, we apply the second statement to the second area. \nThe second statement is a conditional statement, so we .rst apply the .rst rule of the second statement. \nThat rule fails because the pieces needed by the strategy ofthat rule are not available.Wethenmoveontoapplythe \nsecondruleofthe second statement. That rule succeeds and completes the puzzle. Time Complexity. It is \nstraightforward to implement the appli\u00adcation of a rule to an area in constant time. A program executes \nO(1) rules on each area of a board. So, the execution of a program on a board with K areas takes O(K) \ntime.  3.2 Our Puzzle Solving Program Figure 8 shows our puzzle solving program, which has 15 num\u00adbered \nstatements. Notice that the 15 statements have pairwise dif\u00adferent patterns; each statement completes \nthe areas withaparticular pattern. While our program may appear simple and straightforward, the ordering \nof the statements and the ordering of the rules in con\u00additional statements are in several cases crucial \nfor correctness. In general our program tries to .ll the most constrained patterns .rst. For example, \nstatements 1 8 can only be .lled in one way, while the other statements admit two or more solutions.We \nwill discuss four such subtleties. First, it is imperative that in statement 7 our program prefers a \nsize-2 X-piece over two size-1 X-pieces. Suppose we replace statement7witha statement 7. which swaps \nthe order of the two rules in statement 7. The application of statement 7. can take us from a solvable \npuzzle to an unsolvable puzzle, for example: Figure 8. Our puzzle solving program Because statement \n7 prefers a size-2 X-piece over two size-1 X-pieces, the example is impossible. Notice that our program \nalso prefers the size-2 pieces over the size-1 pieces in statements 8 15 for reasons similar to our analysis \nof statement 7. Second, it is critical that statements 7 10 come before state\u00adments 11 14. Suppose we \nswap the order of the two subsequences of statements. The application of rule 11 can now take us from \na solvable puzzle to an unsolvable puzzle, for example: Notice that the example uses an area in which \ntwo squares are .lled in. Because statements 7 10 come before statements 11 14, the example is impossible. \nThird, it is crucial that statements 11 14 come before statement 15. Suppose we swap the order such that \nstatement 15 comes before statements 11 14. The application of rule 15 can now take us from a solvable \npuzzle to an unsolvable puzzle, for example: Notice that the example uses an area in which one square \nis .lled in. Because statements 11 14 come before statement 15, the example is impossible. Fourth, it \nis essential that in statement 11, the rules come in ex\u00adactlythe ordergivenin our program. Supposewereplace \nstatement 11 witha statement 11. which swaps the order of the .rst two rules of statement 11. The application \nof statement 11. can take us from asolvable puzzletoan unsolvable puzzle.Forexample:  (a) (b) (c) Figure \n9. (a) The puzzles produced for the program given in Figure 4(b). (b) An example solution. (c) The .nal \nprogram. When we use the statement 11 given in our program, this situa\u00adtion cannot occur. Notice that \nour program makes a similar choice in statements 12 14; all for reasons similar to our analysis of state\u00adment \n11. THEOREM 2. (Correctness) A type-1 puzzle is solvable if and only if our program succeeds on the puzzle. \nFor an elementary programP , we generate |P | puzzles, each of which we can solve in linear time in the \nnumber of registers. So, we have Corollary 3. COROLLARY 3. (Complexity) Spill-free register allocation \nwith pre-coloring for an elementary program P and 2K registers is solvable in O(|P |\u00d7 K) time. Asolution \nfor the collection of puzzles in Figure 9(a) is shown in Figure 9(b). For simplicity, the puzzles in \nFigure 9 are not padded. 4. Spilling and Coalescing We now present our approach to spilling and coalescing. \nFigure 10 shows the combined step of puzzle solving, spilling, and coalesc\u00ading. Spilling. If thepolynomial-time \nalgorithm of Theorem3 suc\u00adceeds, then all the variables in the program from which the puzzles were generated \ncan be placed in registers. However, the algorithm mayfail, implying that the need for registersexceeds \nthe number ofavailable registers.In that situation, the register allocatorfaces the task of choosing \nwhich variables will be placed in registers and whichvariables willbe spilled, that is, placed in memory. \nThe goal is to spill as few variables as possible. We use a simple spilling heuristic. The heuristic \nis based on the observation that when we convert a program P into ele\u00ad S = empty  For each puzzlep, \nin a preorder traversal of the dominator tree of the program: while p is not solvable:  - choose and \nremove a piece s from p, and for every subsequent puzzle p . that contains a variable s . in the family \nofs, remove s . from p . . S. = a solution of p, guided by S S = S. Figure 10. Register allocation with \nspilling and local coalescing mentary form, each of P s variables is represented by a family of variables \nin the elementary program. For example, the vari\u00adable c in Figure 4(a) is represented by the family of \nvariables {c23,c3,c4,c67,c7,c8,c9} in Figure 4(b). When we spill a vari\u00adable in an elementary program, \nwe choose to simultaneously spill all the variables in its family and thereby reduce the number of pieces \nin manypuzzles at the same time. The problem of register allocation with pre-coloring and spilling of \nfamilies of variables is to perform register allocation with pre-coloring while spilling as fewfamiliesofvariables \nas possible. THEOREM 4. (Hardness) Register allocation with pre-coloring and spilling of families of \nvariables for an elementary program is NP-complete. Theorem4justi.es our useofa spilling heuristic rather \nthan an algorithm that solves the problem optimally. Figure 10 contains a while-loop that implements \nthe heuristic; a more detailed version of this code is given in [32]. It is straightforward to see that \nthe heuristic visits each puzzle once, that it always terminates, and that when it terminates, all puzzles \nhave been solved. In order to avoid separating registers to reload spilled variables only certain pieces \ncan be removed from an unsolved puzzle. These pieces represent variables that are neither used nor de.ned \nin the instruction thatgave origin to the puzzle.For instance, only theY piece f can be removed from \nthe puzzle in Figure 5. When choos\u00adinga piecetobe removed froma puzzle, we use the furthest-.rst strategyofBelady[3]thatwaslaterusedbyPolettoand \nSarkar[33] in linear-scan register allocation. The furthest-.rst strategy spillsa familyofvariables whoseliverangesextendthe \nfurthest, according to a linearization determined by a depth .rst traversal of the domi\u00adnatortreeofthe \nsource program.Wedonotgive preferencetoany path. Giving preference to a path would be particularly worthwhile \nwhen pro.ling information is available. The total number of puzzles that will be solved during a run \nof our heuristic is bounded by |P | + |F|, where |P | denotes the number of puzzles and |F| denotes the \nnumber of families of variables, that is, the number of variables in the source program. Coalescing. \nTraditionally, the task of register coalescing is to assignthe sameregistertothevariables x and y inacopystatement \nx = y, therebyavoiding the generationof codefor that statement. An elementary program contains many parallel \ncopy statements and therefore manyopportunities for a form of register coalescing. We use an approach \nthat we call local coalescing. The goal of local coalescingisto allocatevariablesinthe samefamilytothe \nsame register, as much as possible. Local coalescing traverses the dominator tree of the elementary program \nin preorder and solves each puzzle guidedbythe solutiontothe previous puzzle, as shown in Figure 10. \nIn Figure 9(b), the numbers nextto each puzzle denote the order in which the puzzles were solved. The \npre-ordering has the good property thatevery timea puzzle corresponding to statement i is solved, all \nthefamiliesofvariables that are de.ned at program points that dominate i have already been given at least \none location. The puzzle solver can then try to assign to the piece that represents variable v the same \nregister that was assigned to other variables in v s family. For instance, in Figure 4(b), when solving \nthe puzzle between p2 and p3, the puzzle solver tries to match the registers assigned to A2 and A3. This \noptimization is possible because A2 is de.ned at a program point that dominates the de.nition site of \nA3, and thus is visited before. During the traversalof the dominator tree, thephysical location of each \nlive variable is kept in a vector. If a spilled variable is reloaded when solving a puzzle, it stays \nin a register until another puzzle, possibly manyinstructions after the reloading point, forces it to \nbe evicted again. Our approach to handling reloaded variables is somewhat similar to the second-chance \nallocation described by Traubet al. [38]. Figure 9(c) shows the assembly code produced by the puzzle \nsolver for our runningexample.Wehave highlighted the instruc\u00adtions used to implement parallel copies. \nThe x86 instruction xchg swaps the contents of two registers. 5. Optimizations We now describe three \noptimizations that we have found useful in our implementationofregister allocationbypuzzlesolvingforx86. \nSize of the intermediate representation. An elementary pro\u00adgram has many more variable names than an \nordinary program; fortunately,wedonothavetokeepanyoftheseextra names.Our solver uses only one puzzle \nboard at anytime: given an instruction i, variables alive before and after i are renamed when the solver \nbuilds the puzzle that represents i. Once the puzzle is solved, we use its solution to rewrite i and \nwe discard the extra names. The parallel copybetween two consecutive instructions i1 and i2 in the same \nbasic block can be implemented right after the puzzle repre\u00adsenting i2 is solved. Critical Edges and \nConventional SSA-form. Before solving puzzles, our algorithm performs two transformations in the target \ncontrol .ow graph that, although not essential to the correctness of our allocator, greatly simplify \nthe elimination of .-functions and p-functions. The .rst transformation, commonly described in com\u00adpiler \ntext books, removes critical edges from the control .owgraph. These are edges betweenabasic block with \nmultiple successors and a basic block with multiple predecessors [8]. The second transfor\u00admation converts \nthe target program into a variation of SSA-form called Conventional SSA-form (CSSA) [37]. Programs in \nthis form have the following property: if two variables v1 and v2 are related by a parallel copy, e.g.: \n(...,v1,...)=(...,v2,...), then the live ranges of v1 and v2 do not overlap. Hence, if these variables \nare spilled, the register allocator can assign them to the same memory slot.Afast algorithm to perform \nthe SSA-to-CSSA conversion is given in [11]. These two transformations are enough to handle the swap \nand lost-copy problems pointed outby Briggs et al. [8]. Implementing .-functions and p-functions. The \nallocator maintains a table with the solution of the .rst and last puzzles solved in each basic block. \nThese solutions are used to guide the elimination of .-functions and p-functions. During the implemen\u00adtation \nof parallel copies, the ability to swap register values is nec\u00adessary to preserve the register pressure \nfound during the register assignment phase [7, 31]. Some architectures, such as x86, provide instructions \nto swap the values in registers. In systems where this is not the case, swaps can be performed using \nxor instructions. 6. Experimental Results Experimental platform. We have implemented our register allo\u00adcator \nin the LLVM compiler framework [26], version 1.9. LLVM is the JIT compiler in the openGL stack of Mac \nOS 10.5. Our tests are executed on a 32-bit x86 Intel(R) Xeon(TM), with a 3.06GHz cpu clock, 3GB of free \nmemory (as shown by the linux command free)and 512KBL1 cache running RedHat Linux 3.3.3-7. Benchmark \ncharacteristics. The LLVM distribution provides a broad variety of benchmarks: our implementation has \ncompiled and runover1.3 million linesofCcode.LLVM1.9and our puzzle solver pass the same suite of benchmarks. \nIn this section we will present measurements based on the SPEC CPU2000benchmarks. Some characteristics \nof these benchmarks are given in Figure 11. All the .gures use short names for the benchmarks; the full \nnames are given in Figure 11. We order these benchmarks by the num\u00adber of non-empty puzzles that theyproduce, \nwhich is given in Fig\u00adure 13. Puzzle characteristics. Figure 12 counts the types of puzzles generated \nfrom SPECCPU2000.Atotal of 3.45% of the puzzles havepiecesofdifferent sizesplus pre-colored areassotheyexercise \nall aspects of the puzzle solver. Most of the puzzles are simpler: 5.18% of them are empty, i.e., have \nno pieces; 58.16% have only pieces of the same size, and 83.66% have an empty board with no pre-colored \nareas. Just 226 puzzles contained only short pieces with precolored areas and we omit them from the chart. \nAs we show in Figure 13, 94.6% of the nonempty puzzles in SPEC CPU2000 can be solved in the .rst try. \nWhen this is not the case, our spilling heuristic allows for solving a puzzle multiple times with a decreasing \nnumber of pieces until a solution is found. Figure13 reportstheaverage numberoftimesthatthepuzzlesolver \nhad to be called per nonempty puzzle. On average, we solve each nonempty puzzle 1.05 times. Number of \nmoves/swaps inserted by the puzzle solver. Fig\u00adure14showsthe numberofcopyandswap instructions insertedby \nthe puzzle solver in each of the compiled benchmarks. Local copies denote instructions used by the puzzle \nsolver to implement paral\u00adlel copies between two consecutive puzzles inside the same basic block. Global \ncopies denote instructions inserted into the .nal pro\u00adgram during the SSA-elimination phase in order \nto implement .\u00adfunctions and p-functions.Target programs contains one copy or swap per each 14.7 puzzles \nin the source program, that is, on av\u00aderage, the puzzle solver has inserted 0.025 local and 0.043 global \ncopies per puzzle. Benchmark LoC asm btcode gcc 176.gcc 224,099 12,868,208 2,195,700 plk 253.perlbmk \n85,814 7,010,809 1,268,148 gap 254.gap 71,461 4,256,317 702,843 msa 177.mesa 59,394 3,820,633 547,825 \nvtx 255.vortex 67,262 2,714,588 451,516 twf 300.twolf 20,499 1,625,861 324,346 crf 186.crafty 21,197 \n1,573,423 288,488 vpr 175.vpr 17,760 1,081,883 173,475 amp 188.ammp 13,515 875,786 149,245 prs 197.parser \n11,421 904,924 163,025 gzp 164.gzip 8,643 202,640 46,188 bz2 256.bzip2 4,675 162,270 35,548 art 179.art \n1,297 91,078 40,762 eqk 183.equake 1,540 91,018 45,241 mcf 181.mcf 2.451 60,225 34,021 Figure 11. Benchmark \ncharacteristics. LoC: number of lines ofC code. asm:sizeofx86 assembly programs producedbyLLVMwith Figure \n13. Number of calls to the puzzle solver per nonempty puz\u00adour algorithm (bytes). btcode: program size \nin LLVM s interme-zle. #puzzles: number of nonempty puzzles. avg and max:average diate representation \n(bytes). and maximum numberoftimesthepuzzlesolverwasusedperpuz\u00ad  Benchmark #puzzles avg max once gcc \n476,649 1.03 4 457,572 perlbmk 265,905 1.03 4 253,563 gap 158,757 1.05 4 153,394 mesa 139,537 1.08 9 \n125,169 vortex 116,496 1.02 4 113,880 twolf 60,969 1.09 9 52,443 crafty 59,504 1.06 4 53,384 vpr 36,561 \n1.10 10 35,167 ammp 33,381 1.07 8 31,853 parser 31,668 1.04 4 30,209 gzip 7,550 1.06 3 6,360 bzip2 5,495 \n1.09 3 4,656 art 3,552 1.08 4 3,174 equake 3,365 1.11 8 2,788 mcf 2,404 1.05 3 2,120  zle. once:numberof \npuzzlesfor which the puzzle solverwas used only once. Three other register allocators. Wecompare our \npuzzle solver with three other register allocators, all implemented in LLVM 1.9 and all compiling and \nrunning the same benchmark suite of 1.3 million lines of C code. The .rst is LLVM s default algorithm, \nwhich is an industrial-strength version of linear scan that uses extensions by Wimmer et al. [40] and \nEvlogimenos [14]. The algorithm does aggressive coalescing before register allocation and handles holes \nin live ranges by .lling them with other variables whenever possible.We use ELS (Extended Linear Scan) \nto denote this register allocator. The second register allocator is the iterated register coalescing \nof George and Appel [17] with extensions by Smith, Ramsey, and Holloway [36] for handling register aliasing. \nWe use EIRC (Ex\u00adtended Iterated Register Coalescing) to denote this register alloca\u00adtor. The third register \nallocator is based on partitioned Boolean quadratic programming(PBQP) [35]. The algorithm runs in worst\u00adcase \nexponential time and does optimal spilling with respect to a set of Boolean constraints generated from \nthe program text. We Figure 14. Number of copy and swap instructions inserted per puzzle. use this algorithmtogaugethe \npotentialforhowgoodaregister allocator can be. Lang Hames and Bernhard Scholz produced the implementations \nof EIRC and PBQP that we are using. Stack size comparison. The top half of Figure 15 compares the maximum \namount of space that each assembly program reserves on itscallstack.Thestacksizegivesan estimateofhowmanydifferent \nvariables are being spilledby each allocator. The puzzle solver and extended linear scan (LLVM s default) \ntend to spill more variables than the other two algorithms. Spill-code comparison. The bottom half of \nFigure 15 compares the number of load/store instructions in the assembly code. The puzzle solver inserts \nmarginally fewer memory-access instructions than PBQP, 1.2% fewer memory-access instructions than EIRC, \nand 9.6% fewer memory-access instructions than extended linear scan (LLVM sdefault). Note that although \nthe puzzle solver spills more variables than the other allocators, it removes only part of the live range \nof a spilled variable. Run-time comparison. Figure 16 compares the run time of the code producedbyeach \nallocator.Eachbarshowstheaverageof.ve runs of each benchmark; smaller is better. The base line is the \nrun time of the code when compiled with gcc -O3 version 3.3.3. Note that the four allocators that we \nuse (the puzzle solver,extended lin\u00adear scan (LLVM s default), EIRC and PBQP) are implemented in LLVM, \nwhile we use gcc, an entirely different compiler, only for reference purposes. Considering all the benchmarks, \nthe four allo\u00adcators producefaster code than gcc; the fractions are: puzzle solver 0.944, extended linear \nscan (LLVM s default) 0.991, EIRC 0.954 and PBQP 0.929. If we remove the .oating point benchmarks, i.e., \nmsa, amp, art, eqk, then gcc -O3 isfaster. The fractions are: puzzle Solver 1.015, extended linear scan \n(LLVM s default) 1.059, EIRC 1.025 and PBQP 1.008.We conclude that the puzzle solver producesfaster code \nthan the other polynomial-time allocators,but slower code than the exponential-time allocator.  We have \nfound that the puzzle solver does particularly well on sparse control-.ow graphs.We can easily .ndexamplesof \nbasic blocks where the puzzle solver outperforms even PBQP, which isa slower algorithm.For instance, \nwithtwo registerpairs(AL, AH, BL, BH)available, the puzzle solver allocates the program in Figure 17. \n(left) Example program. (center) Puzzle pieces. (right) Register assignment. Figure 17 without spilling, \nwhile the other register allocators (ELS, EIRC and PBQP) spill at least one variable. In this example, \nthe puzzle solver inserts one copybetween instructions four and .ve to split the live range of variable \na. Compile-time comparison. Figure 18 compares theregister al\u00adlocation time and the total compilation \ntime of the puzzle solver and extended linear scan (LLVM s default). On average, extended linear scan(LLVM \nsdefault)islessthan1%fasterthanthepuzzle solver. The total compilation time of LLVM with thedefault alloca\u00adtoris \nless than3%faster than the total compilation timeofLLVM with the puzzle solver.We note that LLVM is industrial-strength \nand highly tuned software, in contrast to our puzzle solver. We omit the compilation times of EIRC and \nPBQP because the implementations that we have are research artifacts that have not been optimized to \nrunfast. Instead, wegauge the relative compi\u00adlation speeds from statements in previous papers. The experiments \nshown in [21] suggest that the compilation time of PBQP is be\u00adtweentwoandfour timesthecompilationtimeofextended \niterated register coalescing. The extensions proposed by Smith et al. [36] can be implemented in a way \nthat adds less than 5% to the compi\u00adlation timeofa graph-coloring allocator.Timing comparisons be\u00adtween \ngraph coloring and linear scan (the core of LLVM s algo\u00adrithm) span a wide spectrum. The original linear \nscan paper [33] suggests that graph coloring is about twice as slow as linear scan, whileTraub et al. \n[38] gives an slowdown of up to 3.5x for large programs, and Sarkar and Barik [34] suggests a 20x slowdown. \nFrom these observations we conclude that extended linear scan (LLVM sdefault) and our puzzle solver are \nsigni.cantlyfaster than the other allocators. 7. Related Work We now discuss work on relating programs \nto graphs and on com\u00adplexity results for variations of graph coloring. Figure 21 summa\u00adrizes most of \nthe results. Register allocation and graphs. The intersection graph of the live ranges of a program is \ncalled an interference graph. Figure 19 shows the interference graph of the elementary program in Fig\u00adure \n4(b). Anygraph can be the interference graph of a general pro\u00adgram [12]. SSA-form programs have chordal \ninterference graphs [6, 9, 20, 30], and the interference graphs of SSI-form programs are interval graphs \n[10].We call the interference graph of an ele\u00admentary program an elementary graph [32]. Each connected \ncom\u00adponent of an elementary graph is a clique substitution of P3, the simple path with three nodes.We \nconstructa clique substitutionof P3 by replacing each node of P3 bya clique, and connecting all the nodes \nof adjacent cliques.  Elementary graphs area proper subsetof interval graphs, which are contained in \nthe class of chordal graphs. Figure 20 illus\u00adtrates these inclusions. Elementary graphs are also Trivially \nPerfect Graphs [18], as we show in [32]. In a trivially perfect graph, the size of the maximal independent \nset equals the size of the number of maximal cliques. Spill-free Register Allocation. Spill-free register \nallocation is NP-complete for general programs [12] because coloring general graphs is NP-complete. However, \nthis problem has a polynomial time solution for SSA-form programs [6, 9, 20] because chordal graphs can \nbe colored in polynomial time [4]. This result assumes an architecture in which all the registers have \nthe same size. Aligned 1-2-Coloring. Register allocation for architectures with type-1 aliasing is modeled \nby the aligned 1-2-coloring prob\u00adlem. In this case, we aregiven a graph in which vertices are as\u00adsignedaweightof \neither1or2. Colors are representedbynumbers, e.g.: 0, 1,..., 2K - 1, and we say that the two numbers \n2i, 2i +1 Program general Class of graphs SSA-form SSI-form elementary Problem general chordal interval \nelementary ALIGNED 1-2-COLORING EXTENSION NP-cpt [24] NP-cpt [4] NP-cpt [4] linear [TP] ALIGNED 1-2-COLORING \nNP-cpt [24] NP-cpt [27] NP-cpt [27] linear [TP] COLORING EXTENSION NP-cpt [24] NP-cpt [4] NP-cpt [4] \nlinear [TP] COLORING NP-cpt [24] linear [16] linear [16] linear [16] Figure 21. Algorithms and hardness \nresults for graph coloring. NP-cpt = NP-complete; TP = this paper. are aligned.Wede.ne an aligned 1-2-coloring \ntobea coloring that assigns each weight-two vertex two aligned colors. The problem of .nding an optimal \n1-2-aligned coloring is NP-complete even for interval graphs [27]. Pre-coloring Extension. Register allocation \nwith pre-coloring is equivalent to the pre-coloring extension problem for graphs. In this problem we \nare given a graph G, an integer K and a partial function . that associates some vertices of G to colors. \nThe challenge is to extend . to a total function .. such that (1) .. is a proper coloring of G and (2) \n.. uses less than K colors. Pre\u00adcoloring extension is NP-complete for interval graphs [4] and even for \nunit interval graphs [28]. Aligned 1-2-coloring Extension. The combination of 1-2\u00adaligned coloring and \npre-coloring extension is called aligned 1\u00ad2-coloring extension. We show in [32] that this problem, when \nrestricted to elementary graphs, is equivalent to solving type-1 puzzles; thus, it has a polynomial time \nsolution. Register allocation and spilling. When spills happen, loads and stores are inserted into the \nsource program to transfer values to and from memory. If we assume that each load and store has a cost, \nthen the problem of minimizing the total cost added by spill instructions is NP-complete, even for basic \nblocks in SSA-form, as shown byFarach et al. [15]. If the cost of loads and stores is not taken into \nconsideration, then a simpli.ed version of the spilling problem is to determine the minimum number of \nvariables that must be removed from the source program so that the program can be allocated with K registers. \nThis problem is equivalent to determining if a graph G has a K-colorable induced subgraph, whichis NP-complete \nforchordal graphs,but has polynomial time solution for interval graphs, as demonstrated by Yannakakis \nand Gavril [41]. 8. Conclusion Inthispaperwehave introducedregister allocationbypuzzlesolv\u00ading.We have \nshown that our puzzle-based allocator runs asfast as the algorithm used in an industrial-strength JIT \ncompiler and that it produces code that is competitive with state-of-the-art algorithms. Acompiler writer \ncan model a register .le as a puzzle board, and straightforwardly transformasource program into elementary \nform and then into puzzle pieces.Fora compiler that already uses SSA\u00adformasan intermediate representation,theextrastepto \nelementary form is small. Our puzzle solver works for architectures such as x86, ARM, andPowerPC. Puzzle \nsolving for SPARC V8 and V9 (type-2 puzzles) remains an open problem. Our puzzle solver pro\u00adduces competitive \ncode even though we use simple approaches to spilling and coalescing.We speculate that if compiler writers \nim\u00adplement a puzzle solver with advanced approaches to spilling and coalescing, then the produced code \nwill be even better. Acknowledgments Fernando Pereira was sponsored by the Brazilian Ministry of Ed\u00aducation \nunder grant number 218603-9.We thank Lang Hames and Bernhard Scholz for providing us with their implementations \nof EIRCandPBQP.We thankJo ao Dias, Glenn Holloway,Ayee Kan\u00adnan Goundan, StephenKou, Jonathan Lee,Todd \nMillstein, Norman Ramsey, BenTitzer, and the PLDI reviewers for helpful comments on a draft of the paper. \nReferences [1] Scott Ananian. The static single information form. Master s thesis, MIT, September 1999. \n[2] Andrew W. Appel and Lal George. Optimal spilling for CISC machines with few registers. In PLDI, pages \n243 253.ACM Press, 2001. [3] L. Belady. A study of the replacement of algorithms of a virtual storage \ncomputer. IBM System Journal, 5:78 101, 1966. [4]MBir\u00b4o,MHujter, andZsTuza. Precoloringextension. I:Iinterval \ngraphs. In Discrete Mathematics, pages 267 279.ACM Press, 1992. [5] Rastislav Bodik, Rajiv Gupta, andVivek \nSarkar. ABCD: eliminating array bounds checks on demand. In PLDI, pages 321 333.ACM Press, 2000. [6] \nFlorent Bouchez. Allocation de registres et vidage en m\u00b4emoire. Master s thesis, ENSLyon, 2005. [7] Florent \nBouchez, Alain Darte, Christophe Guillon, and Fabrice Rastello. Register allocation: What does the NP-completeness \nproof of chaitin et al. really prove? Or revisiting register allocation: Why and how. In LCPC, pages \n283 298. Springer, 2006. [8] Preston Briggs,Keith D. Cooper,TimothyJ. Harvey, and L.Taylor Simpson. Practical \nimprovements to the construction and destruction of static single assignment form. SPE, 28(8):859 881, \n1998. [9] Philip Brisk,Foad Dabiri, Jamie Macbeth, and Majid Sarrafzadeh. Polynomial-time graph coloring \nregister allocation. In IWLS, pages 447 454. 2005. [10] Philip Brisk and Majid Sarrafzadeh. Interference \ngraphs for procedures in static single information form are interval graphs. In SCOPES, pages 101 110.ACM \nPress, 2007. [11] Zoran Budimlic,KeithD. Cooper,TimothyJ. Harvey,KenKennedy, TimothyS. Oberg,andStevenW.Reeves. \nFastcopycoalescingand live-range identi.cation. In PLDI, pages 25 32.ACM Press, 2002. [12] Gregory J. \nChaitin, Mark A. Auslander, Ashok K. Chandra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. \nRegister allocation via coloring. Computer Languages, 6:47 57, 1981. [13] Ron Cytron, Jeanne Ferrante, \nBarryK. Rosen, MarkN.Wegman, and F.Kenneth Zadeck. Ef.ciently computing static single assignment form \nand the control dependence graph. TOPLAS, 13(4):451 490, 1991. [14] Alkis Evlogimenos. Improvements to \nlinear scan register allocation. Technical report, University of Illinois, Urbana-Champaign, 2004. [15] \nMartinFarach andVincenzo Liberatore. On local register allocation. In SODA, pages 564 573.ACM Press, \n1998. [16] Fanica Gavril. The intersection graphs of subtrees of a tree are exactly the chordal graphs. \nJournal of Combinatorial Theory, Series B, 16(1):47 56, 1974. [17] Lal George and Andrew W. Appel. Iterated \nregister coalescing. TOPLAS, 18(3):300 324, 1996. [18] Martin Charles Golumbic. Trivially perfect graphs. \nDiscrete Mathematics, 24:105 107, 1978. [19] Daniel Grund and Sebastian Hack.Afast cutting-plane algorithm \nfor optimal coalescing. In CC, volume 4420, pages 111 115. Springer, 2007. [20] Sebastian Hack, Daniel \nGrund, and Gerhard Goos. Register allocation for programs in SSA-form. In CC, pages 247 262. Springer, \n2006. [21] Lang Hames and Bernhard Scholz. Nearly optimal register allocation with PBQP. In JMLC, pages \n346 361. Springer, 2006. [22] Corporate SPARC International Inc. The SPARC Architecture Manual, Version \n8. Prentice Hall, 1st edition, 1992. [23] Richard Johnson andKeshav Pingali. Dependence-based program \nanalysis. In PLDI, pages 78 89.ACM Press, 1993. [24] RichardMKarp. Reducibility among combinatorial problems. \nIn Complexity of Computer Computations,pages 85 103. Plenum, 1972. [25] David RyanKoes and Seth Copen \nGoldstein. Aglobal progressive register allocator. In PLDI, pages 204 215.ACM Press, 2006. [26] Chris \nLattner andVikram Adve. LLVM:Acompilation framework for lifelong program analysis &#38; transformation. \nIn CGO, pages 75 88. IEEE, 2004. [27] JonathanK. Lee, JensPalsberg, and FernandoM.Q. Pereira. Aliased \nregister allocation for straight-line programs is NP-complete. In ICALP, pages 680 691. Springer, 2007. \n[28] Daniel Marx. Precoloring extension on unit interval graphs. Discrete Applied Mathematics, 154(6):995 \n 1002, 2006. [29] ClydeL.MonmaandV.K.Wei. Intersectiongraphsofpathsinatree. Journal of Combinatorial \nTheory, Series B, 41(2):141 181, 1986. [30] Fernando Magno Quintao Pereira and Jens Palsberg. Register \nallocation via coloring of chordal graphs. In APLAS, pages 315 329. Springer, 2005. [31] Fernando Magno \nQuintao Pereira and Jens Palsberg. Register allocation after classic SSA elimination is NP-complete. \nIn FOSSACS, pages 79 93. Springer, 2006. [32] Fernando Magno Quintao Pereira and JensPalsberg. Register \nalloca\u00adtion by puzzle solving, 2008. http://compilers.cs.ucla.edu/ fernando/projects/puzzles/. [33] Massimiliano \nPoletto and Vivek Sarkar. Linear scan register allocation. TOPLAS, 21(5):895 913, 1999. [34] Vivek Sarkar \nand Rajkishore Barik. Extended linear scan: an alternate foundation for global register allocation. In \nCC, pages 141 155. Springer, 2007. [35] Bernhard Scholz and Erik Eckstein. Register allocation for irregular \narchitectures. SIGPLAN Notices, 37(7):139 148, 2002. [36] Michael D. Smith, Norman Ramsey, and Glenn \nHolloway. A generalized algorithm for graph-coloring register allocation. In PLDI, pages 277 288.ACM \nPress, 2004. [37]VugranamC. Sreedhar,RoyDzchingJu,DavidM. Gillies,andVatsa Santhanam. Translating out \nof static single assignment form. In SAS, pages 194 210. Springer, 1999. [38] OmriTraub, GlennH. Holloway, \nand MichaelD. Smith. Quality and speed in linear-scan register allocation. In PLDI, pages 142 151. ACM \nPress, 1998. [39] David L. Weaver and Tom Germond. The SPARC Architecture Manual, Version 9. Prentice \nHall, 1st edition, 1994. [40] ChristianWimmer and Hanspeter Mossenbock. Optimized interval splitting \nin a linear scan register allocator. In VEE, pages 132 141. ACM Press, 2005. [41] MihalisYannakakis andFanica \nGavril. The maximum k-colorable subgraph problem for chordal graphs. Information Processing Letters, \n24(2):133 137, 1987.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.</p>", "authors": [{"name": "Fernando Magno Quint&#227;o Pereira", "author_profile_id": "81351607591", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P1022788", "email_address": "", "orcid_id": ""}, {"name": "Jens Palsberg", "author_profile_id": "81100375570", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P1022789", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375609", "year": "2008", "article_id": "1375609", "conference": "PLDI", "title": "Register allocation by puzzle solving", "url": "http://dl.acm.org/citation.cfm?id=1375609"}