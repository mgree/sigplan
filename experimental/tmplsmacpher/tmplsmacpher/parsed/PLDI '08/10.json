{"article_publication_date": "06-07-2008", "fulltext": "\n Orchestrating the Execution of Stream Programs on Multicore Platforms ManjunathKudlur Scott Mahlke Advanced \nComputer Architecture Laboratory University of Michigan Ann Arbor, MI 48109 {kvman,mahlke}@umich.edu \nAbstract While multicore hardware has become ubiquitous, explicitly par\u00adallel programming models and \ncompiler techniques for exploit\u00ading parallelism on these systems have noticeably lagged behind. Stream \nprogramming is one model that has wide applicability in the multimedia, graphics, and signal processing \ndomains. Streaming models execute as a set of independent actors that explicitly com\u00admunicate data through \nchannels. This paper presents a compiler techniquefor planningand orchestratingtheexecutionof streaming \napplications on multicore platforms. An integrated unfolding and partitioning step based on integer linear \nprogramming is presented that unfolds data parallel actors as needed and maximally packs ac\u00adtors onto \ncores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally \noverlapped with computation on the cores. Tofacilitate experimentation, a gener\u00adalized code generation \ntemplate for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming \napplications, a geometric mean speedup of 14.7x is achieved on a 16-core Cell platform compared to a \nsingle core. Categories and Subject Descriptors D.3.4[Programming Lan\u00adguages]: Processors Compilers General \nTerms Languages, Algorithms, Performance Keywords StreamIt, Cell processor, multicore, stream program\u00adming, \nsoftware pipelining 1. Introduction Multicore systems have become the industry standard from high\u00adend \nservers, down through desktops and gaming platforms, and .nally into handheld devices. Example systems \ninclude the Sun UltraSparc T1 that has 8 cores [14], the Sony/Toshiba/IBM Cell processor that consistsof9 \ncores [10], the NVIDIA GeForce 8800 GTX that contains 16 streaming multiprocessors each with eight processing \nunits [19], and the Cisco CRS-1 Metro router that uti\u00adlizes 192Tensilica processors [5]. Intel and AMD \nare producing quad-core x86 systems today and larger systems are on their near term roadmaps. Putting \nmore cores onachip increases peak perfor- Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 08, June 7 13, 2008,Tucson, Arizona, USA. Copyright &#38;#169;c2008ACM \n978-1-59593-860-2/08/06...$5.00 mance,but has shifted theburden onto both the programmer and compiler \nto identify large amounts of coarse-grain parallelism to e.ectively utilize the cores. Highly threaded \nserver workloads nat\u00adurally take advantage of more cores to increase throughput. How\u00adever, the performance \nof single-thread applications has dramati\u00adcally lagged behind.Traditional programming models, such asC, \nC++, andFortran, are poorly matched to multicore environments because they assume a single instruction \nstream and a centralized memory structure. The stream programming paradigmo.ersapromising approach for \nprogramming multicore systems. Stream languages are moti\u00advated by the application style used in image \nprocessing, graphics, networking, and other media processing domains. Example stream languages are StreamIt \n[26], Brook [3], CUDA [19], SPUR [28], Cg [18], and Baker [4]. Stream languages enable the explicit spec\u00adi.cation \nof producer-consumer parallelism between coarse grain unitsof computation.For thiswork, we focus on StreamIt \nwhere a program is represented as a set of autonomous actors (called .l\u00adters in StreamIt) that communicate \nthrough .rst-in .rst-out (FIFO) data channels [26]. StreamIt implements the synchronous data.ow model \n[15] in which the number of data samples produced and con\u00adsumed by each actor are speci.ed a priori. \nDuring program execu\u00adtion, actors .re repeatedly in a periodic schedule [6]. Each actor hasaseparate \ninstruction stream and an independent address space, thus all dependences between actors are made explicit \nthrough the communication channels. Compilers can leverage these character\u00adistics to plan and orchestrate \nparallel execution. Stream programs contain an abundance of explicit parallelism. The central challenge \nis obtaining an e.cient mapping onto the target architecture. Often thegains obtained through parallelexe\u00adcution \ncan be overshadowed by the costs of communication and synchronization. Resource limitations of the system \nmust also be carefully modeled during the mapping process to avoid stalls. Re\u00adsource limitations include \n.nite processing capability and memory associated with each processing element, interconnect bandwidth, \nand direct memory access (DMA) latency. Lastly, stream programs contain multiple forms of parallelism \nthat have di.erent tradeo.s on when they should be exploited. It is critical that the compiler leverage \na synergistic combination of parallelism, while avoiding both structural and resource hazards. In this \nwork, we propose a modulo scheduling algorithm for mapping streaming applications onto multicore systems, \nreferred to as stream graph modulo scheduling or SGMS. Modulo schedul\u00ading is traditionally a form of \nsoftware pipelining applied at the in\u00adstructionlevel[22].Weapplythe same techniqueona coarse-grain stream \ngraph to pipeline the actors across multiple cores. The ob\u00adjective is to maximize concurrent execution \nof actors while hid\u00ading communication overhead to minimize stalls. SGMS is a phase\u00adordered approach consisting \nof two steps. First, an integrated actor .ssion and partitioning step is performed to assign actors to \neach processor ensuring maximumwork balance.Parallel data actors are selectively replicatedandsplitto \nincreasethe opportunitiesforeven work distribution. This .rst step is formulated as an integer linear \nprogram. The second step is stage assignment wherein each actor is assigned to a pipeline stage for execution. \nStages are assigned to ensure data dependences are satis.ed and inter-processor commu\u00adnication latencyis \nmaximally overlapped with computation. Our target platform is the Cell architecture, which represents \nthe .rst tangible platform that is a decoupled multicore where there is no shared cache so code-data \ncolocation is necessary [10]. SGMS is part of a fully automatic compilation system, known as StreamRoller, \nthat maps StreamIt applications onto a Cell system. The SGMS schedule is output in the form of a C template \nthat executes an arbitrary software pipeline. This template, combined withC versionsofthe actors, are \ncompiled withthe host compiler to execute on the target system. For our experiments, we use an IBM QS20 \nBlade Server running Fedora Core 6.0. It is a Cell system equipped with 16 3.2GHz synergistic processing \nengines (SPEs) on2chips, and1GB RAM. Ourwork has the mostoverlap with the coarse-grained schedul\u00ading \nused in the StreamIt compiler [7, 6]. The StreamIt scheduler consists of two major phases. First, a set \nof transformations are ap\u00adplied on the stream graph to combine and split actors to ensure the computation \ngranularity is balanced. Second, a coarse-grain soft\u00adware pipeline is constructed by iteratively applying \na greedy parti\u00adtioning heuristic that assigns .lters to processors. Each .lter is con\u00adsidered in order \nof decreasing work and assigned to the processor withthe least amountofworksofar.To minimize synchronization, \nthe partitioning algorithm is wrapped with a selective fusion pass that repeatedly fuses the two adjacent \n.lters that have the smallest combined work. This process reduces communication overhead by forcing the \ncombined .lters to reside on the same processor. Our work di.ers along two primary dimensions. First, \nthe StreamIt compiler targets the Raw processor that has a traditional cache on each processor [25]. \nIn [6], intermediatebu.ers needed by the software pipeline of the stream graph are stored o. to the o.-chip \nDRAM banks, and a separate communication stage is in\u00adtroduced between steady states to shu.e data between \nbanks. Our formulation of pipeline stage assignment explicitly models DMA overhead and proactively overlaps \ndata transfers for future itera\u00adtions with computation on the current iteration. Second, we for\u00admulate \nthe partitioning and actor .ssion step as an integer linear program rather than employing iterative partitioning \nand fusing to generate a schedule. Our approach combines packing and .ssion of actors, data transfers, \nand resource constraints to generate more balanced and higher quality schedules for architectures such \nas Cell. This paper o.ers the following contributions: The design, implementation, and evaluation of \nstream graph modulo scheduling for e.ciently mapping streaming applica\u00adtions onto decoupled multicore \nsystems.  An integer linear program formulation for integrated actor .s\u00adsion and partitioning to assign \nactors to processing elements maximizing workload balance.  Apipeline stage assignment algorithm that \nproactivelyoverlaps DMA transfers with computation to minimize stalls.  Afully automated compilation \nsystem for Cell capable of gen\u00aderating performance results on real hardware.   Figure 1: (a) Example \nStreamIt Program and (b) corresponding stream graph. 2. Background and Motivation 2.1 StreamIt Language \nStreamIt [26] is an explicitly parallel programming language that implements the synchronous data .ow \n(SDF) [15] programming model. Actors are speci.edbyparametrized classes, which are sim\u00adilar to Java classes. \nTheycan have local variables corresponding to local actor state, and methods that accesses these variables. \nAn ac\u00adtor can have both read-only and read-write state.A stateful actor that modi.es local state during \nthe work function cannot be paral\u00adlelized as the next invocation depends on the previous invocation. \nHowever, the SDF semantics allow the parallel replication of state\u00adless actors.Aspecial method called \nwork is reserved to specify the work function that is executed when the actor is invoked in steady state. \nThe stream rates (number of items pushed and popped on ev\u00adery invocation) of the work functions are speci.ed \nstatically in the program. The stream graph is constructed by instantiating objects of the actor classes. \nStreamIt provides ways to construct speci.c struc\u00adtures like pipeline, split-join, and feedbackloop. \nUsing these prim\u00aditives, the entire graph can be constructed hierarchically. Note that feedback loops \nprovide a means to create cycles in the stream graph. Feedback loops are na\u00a8ively handledbyfusing the \nentire loop into a single actor. More intelligent ways to handle nested loops is beyond the scope of \nthis paper. Further, the feedback loop pattern does not appear in anyof the benchmarks that we evaluate. \nHence, the rest of the paper assumes an acyclic stream graph. Figure 1 shows an example StreamIt program \nand its corre\u00adsponding stream graph. StreamIt provides the peek primitiveto the programmer, which can \nbe used to non-destructively read values o. the input channel. Note that this is only for convenience, \nand does not make StreamIt deviate from the pure SDF model. This is because a program with peek can always \nbe reimplemented with just pushes and pops, and some local state that holds a subset of values seen sofar. \n 2.2 Cell Broadband Architecture Our compilation target in this paper is the Cell Broadband Engine (CBE) \nshown in Figure 2. The CBE is a heterogeneous multicore system, consisting of one 64bit PowerPC core \ncalled the Power Processing Element (PPE) and eight Synergistic Processing Ele\u00adments (SPEs). Each SPE \nhas a SIMD engine called the synergistic processing unit (SPU), 256 KB of local memory and a memory .ow \ncontrol (MFC) unit which can perform DMA operations to and from the local stores independent of the SPUs. \nThe SPUs can only access the local store, so any sharing of data has to be per\u00ad Figure 2: The Cell broadband \narchitecture. Figure 3: Theoretical speedup for unmodi.ed programmer\u00adconceived stream graph. formed \nthrough explicit DMA operations. The SPEs and PPE are connected viaahigh bandwidth interconnect called \nthe Element In\u00adterconnect Bus (EIB). The main memory and peripheral devices are also connected to the \nEIB. The feature of the CBE most relevant to this paper is the ability of the MFCs to do non-blocking \nDMA oper\u00adations independent of the SPUs. The SPUs can issue DMA requests that are added to hardware queues \nof the MFCs. The SPU can con\u00adtinue doing computation while the DMA operation is in progress. The SPU \ncan query the MFC for DMA completion status and block only when the needed data has not yet arrived. \nThe ability to per\u00adform asynchronous DMA operations allow overlap of computation and communication,andisleveragedfore.cient \nsoftware pipelin\u00ading of stream graphs.  2.3 Motivation Stream programs are replete with pipeline parallelism. \nAn actor can start working on the next data item as soon as it is done with the currentitem,evenwhenother \nactorsinthedownward streamofthe graph are still working on the current item. In a multiprocessor en\u00advironment, \nby running di.erent actors on di.erent processors and overlapping iterations, the outer loop can be greatly \nsped up.Try\u00ading to exploit pipeline parallelism requires (1) a good distribution ofwork amongtheavailable \nprocessorsand(2) managingthe com\u00admunicationoverhead resulting becauseof producersand consumers running \non di.erent processors. The partitioning problem. Figure 3 shows the theoretical speedup possible for \na set of unmodi.ed stream programs for 2 to 64 processors.1 The actors present in the programmer-conceived \nstreamgraphare assignedto processorsinanoptimalfashionsuch that the maximal load on anyprocessor is minimized. \nSpeedup is calculated by dividing the single processor runtime by the load on the maximally loaded processor. \nThe programmer-conceived 1More details of the applications are provided in Section 4. stream graph has \nample parallelism that can be exploited on up to 8processors.Beyond8processors,the speedupbeginstolevelo.. \nMost benchmarks just do not have enough actors to span all pro\u00adcessors.For example, fft has only 17 .lters \nin its stream graph, therefore no speedup is possible beyond 17 processors. The other reasonis thatworkis \nnotevenly distributed across the actors.Even though the computation has been split into multiple actors, \nthe pro\u00adgrammerhasno accurateideaofhowlongan actor swork function will take to execute on a processor \nwhen coding the function. This combined with the fact that work functions are indivisible units leadstolessscalingon16or \nmore processors.Forexample,inthe vocoder benchmark, the longest running actor contributes to 12% of the \nwork, thus limiting the theoretical speedup to 100 = 8.3. 12 Most of the benchmarks are completely stateless, \ni.e., all actors are data parallel [6]. In fact, only mpeg2, vocoder, and radar have actors that are \nstateful. Data parallel actors can be replicated (or .ssed) anynumber of times without changing the meaning \nof the program. The longest running actor in vocoder benchmark is stateless, and can be .ssed to reduce \nthe amount of work done in a single actor. Fissing data parallel actors not only allows work to span \nmore processors, it also allows work to be evenly distributed across processors by making the largest \nindivisible unit of work smaller. Even though data parallel actors provide ample opportunity to divide \nup work evenly across processors, it is not obvious how manytimes an actor has to be .ssed to achieve \nload balance. An actual partitioning has to be performed to decide if actors have been .ssed enough number \nof times. On the other hand, a good partitioning is achieved only when actors have been .ssed into suitably \nsmall units. This circular cause and consequence warrants an integrated solution that considers the problems \nof .ssion and partitioning in a holistic manner. Communication overhead. When an actor that produces \ndata and the actor(s) that consume that data are mapped to di.erent processors, the data must be communicated \nto the consumers. In our implementation on the Cell system, actors are mapped to the SPEs that have disjoint \naddress spaces. Therefore, communicating datato consumersisthroughanexplicitDMA.Whensuch transfers are \nnot avoided, or not carefully overlapped with useful work, the overhead could dominate the execution \ntimes. The next section addresses the problem of partitioning and com\u00admunication overhead. First, an \nintegrated .ssion and partitioning method is presented that .sses the actors just enough to span all \nprocessors, and also obtain an even work distribution. Next, the stage assignment step divides up the \nactors into pipeline stages in which all communication is overlapped with computations. 3. Stream Graph \nModulo Scheduling This section describes our method for scheduling a stream graph onto a multicore system. \nThe objective is to obtain a maximal throughput software pipeline taking both the computation and communication \noverhead into account. The stream graph mod\u00adulo scheduling (SGMS) algorithm is divided into two phases. \nThe .rst phase is an integrated .ssion and processor assignment step based on an integer linear program \nformulation. It .sses data paral\u00adlel actorsas necessarytoget maximalload balance acrossthegiven number \nof processors. The second phase assigns actors to pipeline stages in such a manner that all communication \nis overlapped with computation on the processors. 3.1 Integrated Fission and Processor Assignment Consider \na data.ow graph G = (V, E)corresponding to a stream program. Let |V| = N be the number of actors. Let \nthe basic repetition vector be r, where ri speci.es the number of times vi is executed in a static schedule. \nLet t(vi)be the time taken to execute ri copies of vi. The rest of the section assumes ri executions \nof vi as the basic schedulable unit. Given P processors, a software pipeline needs some assignment of \nthe actors to the processors. The throughput of the software pipeline is determined by the load on the \nmaximally loaded processor. As shown in Section 2, even an optimal assignment on the unmodi.ed programmer \nconceived streamgraphdoesnotprovide linear speedupsbeyond8processors. Some data parallel actors need \nto be .ssed into two or more copies sothat thereis more freedomin distributingworkevenly acrossthe processors.For \neach actor in the stream graph, the following ILP formulation comes up with the number of times the actor \nhas to be .ssed, and an assignment of each copyof the actor to a processor. The objectivefunction is \nthe maximal load on anyprocessor,which is minimized. Aset of 0-1 integer variablesai, j,k,l is introduced \nfor every actor vi. The meaning of the four su.xes is explained below: iidenti.es the actor.  jidenti.es \nthe version ofthe actorthatwould appearinthe.nal graph. For every actor vi, the formulation considers \nmultiple versionsof the actor.Version0of the actoris .ssed0times(no copiesmade),version1ofthe actoris \n.ssed oncesothattwo copies of the actor are considered for scheduling, and so on.  kidenti.es the copyof \nthe jth version of the actor vi.Version 0 has only one copy.Version1 has2 copies of the actor and a splitter \nand joiner. The splitter and joiner have to run on some processor, therefore, theyare considered as independent \nschedulable units. Thus there are(j+ 3) schedulable actors in the jthversion.Wehave either0 = k < j+ \n3when j= 1, or k= 0when j= 0.  lidenti.es the processor to which the kth copyis assigned.  Let Qbe \nthe maximum number of versions considered for an actor. Actors with carried state cannot be .ssed at \nall and Q = 1 for such actors. On the other hand, stateless actors can be .ssed any number of times. \nThe choice of Q a.ects the load balance obtained from the processor assignment. Choosing a low value \nfor Qwould inhibit the freedom of distributing copies of an actor to manyprocessors.Weobservedthatthe \nmaximumnumberofcopies of an actor that appear in the best partitions is always less than Pfor all benchmarks. \nTherefore, in the experiments Qwas set to P, the number of processors under consideration. The following \nequation ensures that a copyof an actor is either assigned to one processor or not assigned to any processor \nat all, implying that a di.erent version was chosen. P j ai, j,k,l = 1 .i, 0= j< Q, 0= k< j+ 3 (1) l=1 \nWhen a copy of an actor is indeed assigned to a processor, all other copies in the same version have \nto be assigned to processors, and all other versions should not be assigned to processors. To ensure \nthis, a set of Q indicator variables, bi,q, 0 = q < Q, are introduced for every actor vi. These indicator \nvariables are 0-1 variables which serve two purposes. First, they indicate which version of the actor \nwas chosen. Second, by virtue of being either 0or1only, ensurethat eitherall copiesofaversion are assignedto \nprocessors, or no copyis assigned to anyprocessor. The following set of equations show the relation between \nthe indicator variables bi,q and the assignment variables ai, j,k,l. P j ai,0,0,l - bi,0 = 0 .i (2) l=1 \n Pj+3 jj ai, j,k,l - (j+ 3)- bi, j = M\u00d7 bi, j .i, 1= j< Q (3) l=1 k=0 Pj+3 jj ai, j,k,l - (j+ 4)- bi, \nj =-M+ M\u00d7 bi, j .i, 1= j< Q (4) l=1 k=0 M in Equations3and4isa constant thatis larger than the upper \nPj+3 jj bound of ai, j,k,l. Note that Equations3and4are standard ILP l=1 k=0 tricks to ensure that a \nlinear sum either equals a constant or is zero. Pj+3 jj In this case, the sum ai, j,k,l either hastobe(j+ \n3), denoting l=1 k=0 that all copies of a version were assigned to some processor, or has to be 0, denoting \nthat none of the copies were assigned to anyprocessor. bi, j conveniently takes on1or0, respectively. \nThe following equation ensures that one and only one version of an actor is chosen in the .nal assignment. \nQ j bi, j = 1 .i (5) j=0 Figure4illustratestheabovesetof equationsforanexample actor. Qis chosentobe3intheexample. \nThreeversionsofthe actor are shown in the .gure. The labels on the nodes indicate the version number \nand copy number. The last equation b1,0 + b1,1 + b1,2 = 1 ensures that only one version is chosen, and \nthe rest of the equations ensure that all copies of the chosen version are assigned to processors. To \ndetermine the quality of an assignment, the amount of work assigned to each processor has to be calculated. \nThe following equation computes the work (in terms of time) done by a copyof an actor. . t(vi) ifj= 0 \nt(vi) . ifj> 1andk< j+1 . j+1 + E Wi, j,k,l = (6) splitter work(vi) ifj> 1andk= j+1 . .joiner work(vi) \nifj> 1andk= j+2 Version0of the actor is same as the original actor. Therefore, the workdonebyversion0isthe \noriginalworkt(vi).Inversion1, there are2copiesof the actor thatdo half thework as the original actor. \nNote that there is a small overhead of E when .ssing actors which peek more elements than theypop. This \nis due to the introduction of a decimation stage on each copy which just pops and ignores part of the \ndata to maintain correct semantics. In addition, there is additional work done by the splitter and joiner \nin version 1. The last three casesin Equation6compute thework doneby copiesof the actor, splitter, and \njoiner. Note that the work done in splitter and joiner depends on the implementation. However, theyboth \nare constants given the number of items popped by the corresponding actor.For some assignment of actors \nto processors, the following equation computes the total work TWp that gets assigned to a processor p. \nNQ jjj TWp = ai, j,k,l \u00d7 Wi, j,k,p (7) i=1 j=0 valid k The processor p with maximum work TWp assigned \nto it consti\u00adtutes the bottleneck processor, and thus TWp denotes the inverse of the throughputoftheoverall \npipeline.We borrowthe terminology from operation-centric modulo scheduling used in compiler back\u00adends, \nand use the term Initiation Interval (II) to denote the inverse of the throughput. The following set \nof equations compute IIfrom the TWp s. TWp = II 1= p= P (8) The ILP program that minimizes IIsubject \nto constraints given by Equations1to8provides the following information. The value of jfor which bi, \nj = 1 identi.es the version of the actor chosen. Note that Equation5 ensures that only oneof the bi, \nj s have the value 1.  Given a copy kof the chosen version j, the set of values ai, j,k,l that are1identifythe \nprocessorsto whichthecopyis assigned. Forexample, ifai, j,k,4 = 1,then thekth copythe actor is assigned \nto processor 4.  The above formulation does not account for any communication overhead.Thedata producedbyan \nactorhastobe communicatedto aconsuming actorif that actorwas assignedtoadi.erent processor. The following \nsection shows how all such communication can be hidden, thus achieving the exact throughput obtained \nfrom the processor assignment step.  3.2 Stage Assignment The processor assignment obtained by the method \ndescribed in the previous section provides only partial information for a pipeline schedule. Namely, \nit speci.es how actor executions are overlapped across processors. It does not specify how they are overlapped \nin time.To realizethe throughput, whichistheloadonthe maximally loaded processor obtained from processor \nassignment, all actors as\u00adsigned to a processor including the necessary DMAs have to be completed within \na window of IItime units. The only goal of pro\u00adcessor assignment step is load balance, therefore it assigns \nactors to di.erent processors without taking anydata precedence constraints into consideration. An actor \nassigned to a processor could have its producer assigned to a di.erent processor, and have its consumer \nassigned to yet another processor.To honor data dependence con\u00adstraints and still realize the throughput \nobtained from processor as\u00adsignment,the actorexecutions correspondingtoasingle iterationof the entire \nstream graph are grouped into stages. Note that the con\u00adcept of stage is adapted from traditional VLIW \nmodulo scheduling. Across all processors, stages of a single iteration execute sequen\u00adtially, thus honoring \ndata dependences.Withina single processor, no stages are active at the beginning of execution. During \nthe ini\u00adtial few iterations, stages are activated sequentially, thus .lling up the pipeline and enabling \nexecutions of data dependent actors be\u00adlonging to earlier iterations concurrently with actors from later \nit\u00aderations. In steady state, all stages are active on a processor, thus realizing the throughput obtained \nfrom processor assignment. The Figure 5: Properties of stages. pipeline is drained by deactivating \nstages during the .nal few iter\u00adations. The overarching goal of the stage assignment step is to overlap \nall data communication (DMAs) between actors.To achieve this, the stage assignment step considers the \nDMAs as schedulable units. To honor data dependences and ensure DMAs can be overlapped with actor executions, \ncertain properties are enforced on the stage numbers of actors. Consider a stream graph G = (V, E). The \nstage to which an actor vi is assignedtois denotedby Si. In addition, the processor to which vi is assignedtois \ndenotedby pi. The following rules enforce data dependences and ensure DMA overlap. (vi, vj). E . Sj \n= Si, i.e., the stage number of a consuming actor should come after the producing actor. This is to preserve \ndata dependence.  If(vi, vj). Eand pi * pj, then a DMA operation must be per\u00adformed to get the data \nfrom pi to pj. The DMA operation is given a separate stage number SDMA. As shown in Figure 5, the inequality \nSi < SDMA < Sj is enforced between the stages of the di.erent actors and the DMA operation. The DMA opera\u00adtion \nis separated from the producer by at least one stage, and similarly, the consumer is separated from the \nDMA operation by one stage. This ensures decoupling, and allows the overlap of the producer and the DMA, \nas well as the DMA and the con\u00adsumer.  Within the set of actors assigned to some processor p, the  \nj inequality t(vj)= II, .sis enforced.In otherwords,the sum Sj=s ofexecution timesof actors(Sj)assignedtoa \nstage(s)should be less than the desired II. This is the basic modulo scheduling constraint, which ensures \nthat the stages are notoverloaded, and that a new iteration can be initiated every II time units. A \nsimple data .ow traversal of the stream graph is used to assign stages to actors as shown in Algorithm \n1. For each actor in data.ow order, the FindStage procedure assigns a stage to the actor. The for loop \nbeginning on the line marked1computes the maximum stage of the producers of the actor under consideration. \nIf any of the producers are assigned to a di.erent processor, the earliest stage considered for actor \nis maxstage + 2, which leaves room for DMAs in maxstage + 1. Otherwise, the actor could be placed on \nmaxstage. The while loop beginning on the line marked 4 .nds a stage number later than stage on which \nthe load is less than the IIobtained from processor assignment. 3.3 Code Generationfor Cell This section \ndescribes a code generation strategy to implement the modulo schedule obtained for a stream program on \na Cell system. The target of our code generation are the multiple SPEs, as opposed to the PPE. This section \ndescribes the general code generation schema, the bu.er allocation strategy, and provides a complete \nexample. Code generation schema. The SPEs are independent proces\u00adsors with disjoint address spaces. The \ngeneral code generation strat\u00ad FindStage (actor): maxstage . 0; flag. false; 1 foreachproducerpof actor \ndo if stage(p)> maxstage then maxstage . stage(p); end if Proc(p) * Proc(actor) then flag. true; end \nend 3 if.ag then stage . maxstage + 2; else stage . maxstage ; end 4 whileLoad(Proc(p), stage) + t(actor) \n> II do stage . stage + 1 end Load(Proc(p), stage)+= t(actor); return stage Algorithm 1: Stage assignment \nprocedure egy is to spawn one thread per SPE. Each thread makes calls to work functions corresponding \nto actors that are assigned to the re\u00adspectiveSPEs, and perform DMAs to get data from other SPEs. The \nmain program, running on the PPE, just spawns the SPE threads and does not intervene thereafter. Figure6shows \npseudoCcode that runs on each SPE thread.It mimicsthekernel-only[23]codeof modulo schedulingforaVLIW \nprocessor. The array stage functions similar to the staging predi\u00adcate, and its size(N)is the maximum \nnumber of stages. The main loop starts o. with only the .rst stage active. The if conditions that test \ndi.erent elements of stage ensure only actors assigned to a particular stage are executed. The last part \nof the loop shifts the elements of the array stage to the left, which has the e.ect of .ll\u00ading up the \nsoftware pipeline. Finally, when all iterations are done, draining the software pipeline is accomplished \nby shifting a 0 into the last element of stage. Figure 6: Main loop implementing the modulo schedule. \nThe code corresponding to each active stage are calls to the work functions of the actors assigned to \nthis SPE and the corre\u00adsponding stage, and the necessary DMAs to fetch data from other SPEs. The Cell \nprocessor provides non-blocking DMA function\u00adality [11], which is leveraged for overlapping DMAs and \ncompu\u00adtation.ADMA operation assignedtoa particular stageis imple- Figure 7: Bu.er allocation for the \nmodulo schedule. mented using the mfc get primitive, which enters the DMA com\u00admand into a queue and \nreturns immediately. The MFC engine in each SPE processes the queue asynchronously and independent of \nthe processor. After enqueuing the DMA request, the code proceeds to execute work functions for actors. \nNote that even though the actual DMA operations are asynchronous, the SPE should queue up the DMA requests \nsynchronously using the mfc get primi\u00adtive. No more DMAs can be queued once work functions begin execution. \nTherefore, all DMA operations belonging to a stage are queued up before any work functions are called \nto ensure maximal overlap of actual DMAs and computation. Finally, the wait for dma completion uses the \nmfc read tag status all primitive to ensure all DMAs issued in the current iteration are completed, and \na barrier synchronization is executed to ensure the current iteration is completed on all SPEs. barrier() \nis imple\u00admented using the signal mechanism available on the SPEs, and with the current implementation,2 \n\u00d7 106 barriers can be performed in1second. Bu.er allocation. In the code generation schema described \nabove, several iterations of the original stream graph are in .ight concurrently. A producer actor could \nbe executed multiple times before one of its consumers is ever executed. To ensure correct operation, \nmultiplebu.ers are used to store the outputs of producer actors. Thebu.ersareusedinafashion similarto \nrotatingregisters in a traditional modulo schedule. The number of bu.ers needed for the output of a producer \nactor assigned to stage Sp feeding a consumer actor on stage Sc can easily be calculated as Sc - Sp + \n1. Figure7shows thebu.er allocation for a producer actor Aand consumer actor B. They are assigned to \ndi.erent processors with an intervening DMA. Since the stage separation between Aand the DMA is 3,4bu.ers \nare allocated on the local memory of PE1, and A uses them in a round-robin fashion. The arrows on the \npicture on the right shows the current bu.ers being used. Note that the DMA operation and actor A are \nexecuting concurrently by using di.erent bu.ers. Similarly, B is usingabu.er di.erent from the DMA. In \nthe current implementation, allbu.ers are allocated on the local memories of the SPEs. Thebu.ers between \na producer actor and a DMA operation are stored on the SPE on which the produceris running. Symmetrically, \nthebu.ers between the DMA operation and the consuming actor are stored on the consumer SPE. 256KB of \nlocal store is su.cient to hold all thebu.ers needed by the benchmarks evaluated. This is corroborated \nby the authors of [6], who report that thebu.ers neededbythe benchmarkswould .t on the 512KB cache of \nthe Cell processor. 3.3.1 Example Figure 8(a) shows an example stream graph. Assume that all actors in \nthe graph are data parallel, i.e., theycan be .ssed anynumber of times. The numbers beside the nodes \nrepresent the amount of work done by the actors. Note that B does the most work of 40 units and the sum \nof work done by all actors is 60 units. When trying to schedule the unmodi.ed graph on to2 processors, \nthe maximum Figure 8: Example illustrating .ssion, processor assignment and stage assignment.  Figure \n9: Example illustrating a modulo schedule running on Cell. achievable speedup is 60 = 1.5. Figure 8(b) \nshows the result of 40 the integrated .ssion on processor assignment step. Node B has been .ssed once, \nresulting in two new nodes B1and B2, and the corresponding splitter S and joiner J, whose work are assumed \nto be2units. The processor assignment obtained has an IIof 32, thus 60 resulting in a speedup of 32 ~ \n2. Finally, Figure 8(c) shows the stage assignment in which DMAs are separated from consumers by one \nstage, thus ensuring complete overlap of computation and communication. Figure9 shows theexecution timelineof \nthe code running on two SPEs. The main feature to note is the steady state execution, which starts from \nthe 5th iteration in Figure 9. In the steady state, Figure10:Mappingofan unfolded streamgraphonto3processors. \n all actors and all DMAs are active. The4iterations shown before the steady state correspond to the \nprologue of the modulo schedule, in which some actor executions and DMAs do not happen as they are predicatedbythe \nstage array. The DMA operations are started before actor executions on the SPEs, thus ensuring overlap \nwith computation. Due to the overlap, the purported speedup of 2 is achieved by the schedule. 4. Evaluation \nThis section presents our evaluation of SGMS. First, a simple alternative scheme which na\u00a8ively unfolds \nthe entire stream graph is presented. Then,various aspectsof SGMS areevaluated, including a comparison \nto na\u00a8ive unfolding. 4.1 Na\u00a8ive Unfolding This technique is based on a simple observation: when all actors \nin a stream program are stateless, the graph can be unfolded Ptimes (where Pis the number of available \nprocessors) and each copyof the graph can be run on one of the processors without incurring anycommunication \noverhead, and thus achieving a speedup of P. Unfolding [20] refers to the process of making multiple \ncopies of the stream program and is analogous to unrolling a loop in tra\u00additional compilation. Unfolding \nis di.erent from .ssion presented earlier in the paper. Fissing an actor introduces additional split \nand join nodes, and stream program semantics does not allow .ssing a stateful actor. Unfolding the entire \nstream graph, including stateful actors, is possible if the additional dependences introduced due to \ncarried state are honored. Also, when the entire graph is unfolded, stateless actors that peek more elements \nthan theypop should also be considered stateful. This is because the extra elements that are peeked have \nto be remembered until the next invocation. Figure10showsa stream graph unfolded3timesand mapped on to3processors. \nThe copies of nodes Aand E shown as darker circles are stateful actors in the original graph. In the \nunfolded ver\u00adsion, new edges A1 . A2, A2 . A3, and A3 . A1 enforce depen\u00addencies due to persistent state \nin actor A. These edges, referred to as state data dependence edges, are di.erent from the edges which \ndenote .ow of stream data. State data dependence edges enforce thefactthatthe secondcopyofthe unfolded \nactorcanexecuteonly after the .rst copyhas .nished its execution and passed on the val\u00adues of state variables. \nUnfolding the stream graph and mapping it to processors as shown in Figure 10 introduces recurrence cycles \nwhich is one of the limitingfactorsof performanceof sucha mapping. Considera stream graph G = (V, E). \nLet the stateful nodes in V be denoted by the set Vs . V. Suppose t(v)be the execution time of actor \nv . V and s(v), the amount of time taken to transfer the state data associated with v . Vs. s(v)depends \non the size of the persistent state of actor v and the communication latency.We assume that the size \nof persistent state is constant and does not grow during run\u00adtime. StreamIt does not allowdynamic memory \nallocation, and thus this property holdsforall benchmarksin ourevaluation.Asevident fromFigure10,forevery \nstatefulnode,arecurrencecycleoflength n \u00d7 s(v)+ n \u00d7 t(v)is introduced in the unfolded version, wheren \nis the unfoldfactor. The longestcyclein the graph constrains the maximum throughput achievable for the \ngraph.We adopt the ter\u00adminology used in traditional instruction centric software pipelin\u00ading, and refer \nto the critical path length as recurrence constrained minimum initiation interval , or RecMII. Thus, \nthe RecMII in the unfolded graph is RecMII = max(n\u00d7 s(v)+ n \u00d7 t(v)) (9) v.Vs The maximum achievable throughput \nis also limited by the resources, in this case the limited number of processors available toexecutethe \ngraph.The constrainton throughputdueto resources is referred to as resource constrained minimum initiation \ninterval , or ResMII. In the mapping shown in Figure 10, each processor executes all the actors in the \noriginal stream graph. In addition, forevery stateful actor,the processor performsaDMAtomovethe state \ndata. Thus, every processor is equally loaded, and the load is jj ResMII = s(v)+ t(v) (10) v.Vs v.V The \nbest throughput for the graph using the above mapping de\u00adscribed, referred to as the minimum initiation \ninterval , or MII, is simple the maximum of RecMII and ResMII. Suppose the number of actors in the stream \nprogram are much larger than the number of available processors, i.e., |V|\u00bb P. Then, RecMII would be \nmuch smaller than ResMII because ResMII is the sum of work on all ac\u00adtors, whereas RecMII dependsontheworkof \none actor.Aslongas the stream program does not have a large stateful .lter that dom\u00adinates the run time, \nwhich is true of our benchmark set, we have ResMII > RecMII. Given that MII = ResMII, in steady state, \nthe above mapping on n processors completes n iterations in MII cy\u00adcles. Thus the speedup achieved by \nthis mapping over one proces\u00adsor is givenby j n\u00d7 t(v) Speedup = jv.Vj(11) s(v)+ t(v) v.Vs v.V The code \nto run the na\u00a8iveunfolding schedule on the Cell proces\u00adsor consistsofonethreadperSPE.SPEsare orderedtokeeptrack \nof which iterations are executed on which SPE. Each SPE executes all actors in the stream graph in data \n.ow order. Before executing a stateful actor, an SPE synchronizes with the previous SPE, and gets the \nvalues of state variables. The SPE then synchronizes with the next SPE and passes on the values of state \nvariables. This is done repetitively, so that an SPE executes iterations i, i+ n, i+ 2n..., where n is \nthe total number of SPEs. The main di.erences between na\u00a8ive unfolding and SGMS can be summarized as \nbelow. All DMA transfers of stream data can be overlapped with com\u00adputation in SGMS where as DMA transfers \nof state data cannot be overlapped with anycomputation as it is present in the criti\u00adcal path.  In the \nna\u00a8ive unfolding method, each SPE runs all actors in the original stream graph, whereas in SGMS, an SPE \nruns only a subset of the actors. Therefore, the memory footprint of code for na\u00a8ive unfolding is much \nlarger than for SGMS.  Benchmark Actors Stateful Peeking State size (bytes) bitonic 28 2 0 4 channel \n54 2 34 252 dct 36 2 0 4 des 33 2 0 4 .t 17 2 0 4 .lterbank 68 2 32 508 fmradio 29 2 14 508 tde 28 2 \n0 4 mpeg2 26 3 0 4 vocoder 96 11 17 112 radar 54 44 0 1032 Table 1: Benchmark characteristics. The latency \nfor one iteration of the original stream graph is equal to the uni-processor execution time of an iteration \nin the na\u00a8ive unfolding method. This is because all actors belonging to one iteration is executed sequentially \nby an SPE. In con\u00adtrast, task level parallelism is exploited within an iteration in SGMS, and therefore, \nthe latencyfor an iteration could be much smaller. Despite the shortcomings compared to SGMS, na\u00a8ive \nunfolding is a simple method which requires no sophisticated compiler analyses, and is straightforward \nto implement for the Cell processor. We compare SGMS with na\u00a8ive unfolding in the following section. \n 4.2 Experiments This section presents the results of the experimental evaluation of SGMS, and comparison \nto the na\u00a8ive unfolding method.Aunipro\u00adcessor schedule was .rst generated for one SPE, with instrumen\u00adtations \nadded for measuring running time of each actor. The SPU decrementer ,alowoverhead timing measurement \nmechanism,is used for pro.ling. The timing pro.le for each actor is used by the SGMS scheduler that generates \nschedules for 2-16 processors. The scheduler uses the CPLEX mixed integer program solver during the integrated \n.ssion and processor assignment phase. The code gener\u00adation phase outputs plainCcode thatisdivided into \ncode that runs on the Power processor and code that runs on individual SPEs. The main thread running \non Power processor spawns one thread per SPE. Each SPE thread executes a code pattern that was described \nin Section 3.3. IBM s Cell SDK 2.1 was used to implement the DMA copies, and the barrier synchronization. \nThe GNUC com\u00adpilergcc4.1.1targetingtheSPEwasusedtocompilethe programs. Note that only vectorization that \nwas automatically discovered by gcc were performed on the actors codes. The hardware used for ourevaluationisanIBMQS20 \nBlade server.Itis equipped with2 CellBE processors and1GB XDRAM. Benchmark suite. Thesetof benchmarksavailablewith \nStreamIt software version 2.1.1 was used to evaluate the scheduling meth\u00adods. Most benchmarks are from \nthe signal processing domain. bitonic implements the parallel bitonic sorting algorithm. des is a pipelined \nversion of DES encryption cipher. [6] provides de\u00adscriptions of the benchmarks. Table 1 shows the details \nrelevant to our evaluation. Number of stateful actors with explicit state and peeking actors with implicit \nstate are important to understand the speedups from na\u00a8ive unfolding. Typical sizes of states in these \nbenchmarks are also shown. SGMS performance. Figure 11 shows the speedups obtained by SGMS over single \nprocessor execution on 2 to 16 processors for the benchmark suite. SGMS obtains near linear speedup for \nall benchmarks, resulting in the geometric mean speedup of 14.7x on 16 processors. The main reasons for \nnear linear speedups are listed below.  The integrated .ssion and partitioning step .sses enough data \nparallel actors and the resulting number of actors is enough to span all available processors.  The \npartitioning assigns actors to processors with maximal load balance.  Stage assignment separates data \ntransfers and actors that use the data into di.erent stages. This ensures that all data transfers are \noverlapped with computation.  Note that with perfect load balance and complete overlap of all communication \nwith computation shouldalways resultinaspeedup of N on N processors. However, the observed geometric \nmean speedupisonly 14.7xon16 processors.Oneofthemainoverheads in our implementation arises from the barrier \nsynchronization. As shown in Figure 6, all SPEs do a barrier synchronization at the end of every iteration \nof the loop implementing the modulo schedule. Our implementation of the barrier on the SPEs adds an overhead \nof 1 second for every 2 \u00d7 106 calls. Depending on the number iterations the stream graph is executed, \nbarrier synchronization adds an overhead of up to 3 seconds in some benchmarks. A notable benchmark is \nvocoder for which the 16 processor speedup is only 13x. vocoder has 96 actors in the stream graph. On \n16 processors, the partitioning results in over 30 DMA operations being in .ight at the same time, which \nadds some overhead to the steady state. SGMS relies on static work estimates during the partitioning \nphase. Any deviation from the static estimate during runtime would change the balance of work across \nprocessors and cause a reduction in speedup. However, this e.ect is di.cult to quantify. Comparing na\u00a8ive \nunfolding to SGMS.Figure 12 compares the speedup obtained by SGMS and na\u00a8ive unfolding on 16 processors. \nThere are3 bars per benchmark. The .rst bar is the speedup ob\u00adtained by na\u00a8ive unfolding for the original \nstream graph. The sec- Figure 13:E.ect of exposed DMA latency. ondbaristhe speedup obtainedbyna\u00a8ive \nunfoldingonthe sameset of benchmarks,but with the sizeof statevariables arti.cially in\u00adcreased by 16x \ncompared to the original implementation. The last bar the speedup obtained by SGMS for the original stream \ngraph. Figure 12 has to be correlated withTable1for better understand-ing.For benchmarks that are almost \ncompletely stateless, such as dct, des and mpeg2, na\u00a8ive unfolding achieves over 15.5x speedup on 16 \nprocessors. This is not surprising as independent iterations run on di.erent processors without anycommunication. \nNote that each benchmark nominally has2stateful actors, which are the in\u00adput and output actors. These \nare used for preserving program order. The small amount of communication needed for these two state\u00adful \n.lters adds very little overhead, and thus completely stateless stream programs achieve close to 16x \nspeedup on 16 processors. The SGMS method for these programs does not unfold the stream graph completely,butonly \n.sses enough actorstogetanevenwork distribution. The selective .ssing adds extra splitters and joiners \nthat add non-zero overhead to the steady state. Also, SGMS uses a barrier synchronization at the end \nof each iteration, whereas in na\u00a8ive unfolding, the stateful actors perform a point to point syn\u00adchronization. \nBecauseof thesetwofacts,na\u00a8ive unfolding performs 5-10% better than SGMS for completely stateless stream \nprograms. For stream programs with many stateful and peeking actors, such as vocoder, radar, and fmradio, \nSGMS outperforms na\u00a8ive unfolding by up to 20%. The DMA transfer of state data in na\u00a8ive unfolding is \ncompletely exposed as it is in the critical path. How\u00adever, all DMA transfers of stream data are overlapped \nwith compu\u00adtation in SGMS. The exposed DMA overhead for na\u00a8ive unfolding is more pronounced when the \nstate size is arti.cially increased to 16x the original state size. In this case, SGMS, whose performance \nis una.ectedbythe state size increase, outperforms na\u00a8iveunfolding by up to 35%. E.ect of exposed DMA \nlatency. Figure13 illustratesthee.ec\u00adtiveness of computation/communicationoverlap.For each bench\u00admark, \na version of the C code for SPEs was generated in which the data transferoverheadwas completelyexposed.For \nthis case, the stage assignment did not separate the DMA operation and the consumer actor into di.erent \nstages. Rather, they were put in the same stage and the consumer SPE stalls until the DMA operation is \ncompleted. The e.ect of exposed DMA latency is detrimental for all benchmarks.For channel, filterbank, \nand radar, which have high computation to communication ratios, the e.ect is not very pronounced and \ntheyretain most of their speedups even with exposed DMA latency. bitonic and des have low computation \nto communication ratios, and they su.er up to 25% perfomance loss when the DMA latencies are exposed. \nComparing ILP partitioning to greedy partitioning. The in\u00adtegrated .ssion and processor assignment phase \nis in part an opti\u00admal formulation for bin packing. In addition to deciding how many times each actor \nhas to be .ssed, this phase also does the assign\u00adment with maximal load balancing. Figure 14 compares \nthe optimal formulation witha greedy heuristic.We only compare the8 pro\u00adcessor speedup. This is because \nthe programmer conceived stream graphalreadyhasenough parallelismtospan8processorsasshown in Figure3and \nthe .ssion partof the formulation does not .ss any actors. Thus, Figure 14 e.ectively compares an optimal \nbin pack\u00ading formulationtoa greedy strategy.We usethe Metis[12]graph partitioner as our greedy strategy. \nThe original stream graph is par\u00adtitioned into N parts using Metis, where N is the number of pro\u00adcessors. \nThe same work estimates are used as weights on the nodes of the graph. Note that this greedy partitioning \nis similar to the one used in [6]. In [6], a separate communication stage is introduced between steady \nstates to shu.e data between banks. However, to make the comparisonfair, the same algorithm for stage \nassignment is used in both cases which overlaps all DMA transfers with com\u00adputation. Figure 14 shows \nthat the quality of graph partition us\u00ading a greedy method depends greatly on the structure of the graph. \nFor example,fft and tde are just linear graphs with no splitters or joiners.For these cases, the greedy \ngraph partitioner is able to achieve the same load balance as the optimal partitioner.For highly parallel \ngraphs like filterbank and vocoder, heuristics perform up to 35% worse than an optimal formulation. Overall, \nthe optimal partitionerachievesageometric meanspeedupof7.6x, whereasthe greedy partitioner achieves 6.7x \non8processors. Scaling of ILPformulation.The vocoder benchmark is used to study how the CPLEX solver \nrun times scales when trying to partition the graph for2to 128 processors. vocoder is the largest benchmark \nin the suite, and the solver run times are smaller for all other benchmarks. The solver run times were \nunder 30 seconds for up to 16 processors. The time taken for partitioning on 32, 64 and 128 processors \nwere 2, 6, and 16 minutes, respectively on a Intel PentiumDrunning at 3.2GHz. 5. RelatedWork There is \na large body of literature on synchronous data.ow graphs, on languages to express stream graphs, and \nmethods to exploit the parallelism expressed in stream graphs. Even though SDF is a powerful explicitly \nparallel programming model, its niche has beeninDSP domainforalongtime.EarlyworksfromthePtolemy group \n[17, 16, 15] has focused on expressing DSP algorithms as stream graphs. Some of their scheduling techniques \n[21, 9] have focused on scheduling stream graphs to multiprocessor systems. However, they focus on acyclic \nscheduling and do not evaluate scheduling to a real architecture. There has been other programming systems \nbased on the stream programming paradigm, and each of those systems have compilers which target multiprocessors.[8] \nmaps StreamCtoa multithreaded processor. This was more of a feasibility study, and the scheduling was \ndone manually. In [27], the authors map the Brook language to a multicore processor. They make use of \na.ne partitioning techniques which are more suitable for parameterized loop based programs.With StreamIt, \nthe stream graph is completely resolved atcompiletime,andadirect scheduling techniquelikeoursis more \ne.ective. Note that any stream programming system in which the computation can be expressed as an stream \ngraph could utilize our scheduling method. There has been a recent spur of research in the domain of \ncom\u00adpiling to the Cell processor. CellSs [1] is a stylized C model for programming the cell. The computation \nis expressed as functions which make all their inputs and outputs explicit in terms of pa\u00adrameters. Functions \ncan be stringed together to form a data .ow graph. A run time scheduler treats this graph in the same \nway a superscalar processor treats operations, and schedules these func\u00adtionsontothecellSPEsas soonastheirinputsareready.Ourwork \nis distinctly di.erent from theirs in that, we use a static compile time schedule which does not have \nrun time scheduling overheads. [13] talks about compiling the Sequoia language to the Cell pro\u00adcessor. \nThis paper s focus is more on representing machines with multiple levels of memories, possibly with disjoint \naddress spaces, in a reusable way, and a compiler to automatically target such rep\u00adresentations. Our \nwork focuses more on the actual scheduler, and assumesa.xed machine.[2] talks about parallelizingaspeci.cap\u00adplication \nat multi levels of granularity on the Cell processor. This is more of an experiences paper, and the parallelization \nwas done manually. The problem scheduling coarse grain actors to processors on a multicore with distributed \nmemory is conceptually similar to scheduling operations to the function units in a multicluster VLIW \nprocessor [22, 24]. However, stream graph exposes more optimiza\u00adtion opportunities such as the ability \nto .ss actors. Also, the con\u00adstraints of limited register space is not an issue on multicores as thereisample \nmemoryavailabletoholdthe intermediatebu.ers. 6. Conclusion The widespread use of multicore processors \nis pushing explicitly parallel high-level programming models to the forefront. Stream programmingisa \npromising approachasit naturallyexpressespar\u00adallelism in applications from a wide variety of domains. \nIn this paper, we develop methods to automatically map a stream pro\u00adgram on to the Cell processor. One \nof the main issues of getting an even distribution of computation across processors is dealt in an integrated \n.ssion and partitioning step that breaks up computa\u00adtion units just enough to span the available processors. \nThe issue of communicationoverheadisovercomeby an intelligent stage as\u00adsignment, whichoverlaps all communication \nwith computation.A detailed evaluation of our method on real hardware shows consis\u00adtentspeedupforawiderangeof \nbenchmarks.Streamgraphmodulo scheduling provides a geometric mean speedup of 14.7x over sin\u00adgle processor \nexecution across the StreamIt benchmark suite.We compare our method to na\u00a8ive unfolding that unfolds \nall actors as many times as the number of processors. Even though na\u00a8ive un\u00adfolding gets speedups similar \nto SGMS for completely stateless programs, SGMS demonstrates wider applicabilitybyo.ering con\u00adsistent \nspeedups on both stateless and stateful programs. Finally, the integrated .ssion and partitioning phase \nis largely independent of the underlying architecture, and can be used when compiling to di.erent multicore \nplatforms. 7. Acknowledgments We thank Dr. Rodric Rabbah at IBM Research for his time, energy, and useful \nfeedback on this research. We also extend our thanks to the anonymous referees who provided excellent \ncomments. This researchwas supportedby the National ScienceFoundation grants CNS-0615261 and CCF-0347411, \nARM Ltd., and equipment do\u00adnated by Hewlett-Packard and Intel Corporation. References [1] Pieter Bellens, \nJosep M. Perez, Rosa M. Badia, and Jesus Labarta. Cellss: a programming model for the cell be archi\u00adtecture. \nProceedings Supercomputing 06, 00(1):5, 2006. [2] Filip Blagojevic, Dimitris S. Nikolopoulos, Alexandros \nSta\u00admatakis, and Christos D. Antonopoulos. Dynamic multigrain parallelization on the cell broadband engine. \nIn Proc. of the 12thACM SIGPLAN Symposium on Principles andPrac\u00adtice ofParallel Programming, pages 90 \n100, NewYork, NY, USA, 2007.ACM Press. [3] I. Buck et al. Brook for GPUs: Stream computing on graphics \nhardware. ACM Transactions on Graphics, 23(3):777 786, August 2004. [4] M. Chen, X. Li, R. Lian, J. Lin, \nL. Liu, T. Liu, and R. Ju. Shangri-la: Achieving high performance from compiled net\u00adwork applications \nwhile enabling ease of programming. In Proc. of the SIGPLAN 05 Conference on Programming Language Design \nand Implementation, pages 224 236, June 2005. [5] W. Eatherton. The push of network processing to the \ntop of thepyramid, 2005. [6] Michael I. Gordon,William Thies, and Saman Amarasinghe. Exploiting coarse-grained \ntask, data, and pipeline parallelism in stream programs. In 14th International Conference on Ar\u00adchitectural \nSupport for Programming Languages and Oper\u00adating Systems, pages 151 162, New York, NY, USA, 2006. ACM \nPress. [7] Michael I. Gordon, William Thies, Michal Karczmarek, Jasper Lin, Ali S. Meli, AndrewA. Lamb, \nChris Leger,Jeremy Wong, Henry Ho.mann, David Maze, and Saman Amaras\u00adinghe. Astream compiler for communication-exposed \narchi\u00adtectures. In Tenth International Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 291 303, October 2002. [8] Jayanth Gummaraju and Mendel Rosenblum. Stream \npro\u00adgramming on general-purpose processors. In Proc. of the 38th Annual International Symposium on Microarchitecture, \npages 343 354,Washington, DC, USA, 2005. IEEE Computer So\u00adciety. [9] Soonhoi Ha and Edward A. Lee. Compile-time \nschedul\u00ading and assignment of data-.ow program graphs with data\u00addependent iteration. IEEE Transactions \non Computers, 40(11):1225 1238, 1991. [10] H.P. Hofstee. Powere.cient processor design and the Cell processor. \nIn Proc. of the 11th International Symposium on High-Performance Computer Architecture,pages 258 262, \nFebruary 2005. [11] IBM. Cell Broadband Engine Architecture, March 2006. [12] G. Karypis andV.Kumar. \nMetis:ASoftwarePackageforPari\u00adtioning Unstructured Graphs,Partitioning Meshes and Com\u00adputingFill-Reducing \nOrderings of Sparce Matrices. Univer\u00adsity of Minnesota, September 1998. [13]TimothyJ. Knight,JiYoungPark, \nManman Ren, Mike Hous\u00adton, Mattan Erez, KayvonFatahalian, Alex Aiken,WilliamJ. Dally, andPat Hanrahan. \nCompilation forexplicitly managed memory hierarchies. In Proc.ofthe12thACM SIGPLANSym\u00ad posium on Principles \nand Practice ofParallel Programming, pages 226 236,NewYork,NY, USA, 2007.ACM Press. [14]P.Kongetira,K.Aingaran,andK. \nOlukotun. Niagara:A32\u00adway multithreaded SPARC processor. IEEE Micro, 25(2):21 29, February 2005. [15] \nE. Lee and D. Messerschmitt. Synchronous data .ow. IEEE Proceedings of, 75(9):1235 1245, 1987. [16] E. \nA. Lee and D. Messerschmitt. Pipeline interleaved programmable dsp s: Synchronous data .ow programming. \n35(9):1334 1345, 1987. [17] Edward Ashford Lee and David G. Messerschmitt. Static scheduling of synchronous \ndata .ow programs for digital sig\u00adnal processing. IEEETransactions on Computers, 36(1):24 35, 1987. [18] \nW. Mark, R. Glanville, K. Akeley, and J. Kilgard. Cg: A system for programming graphics hardware in a \nC-like lan\u00adguage. In Proc. of the 30thInternational Conference on Com\u00adputer Graphics and Interactive \nTechniques, pages 893 907, July 2003. [19] J. Nickolls and I. Buck. NVIDIA CUDAsoftware and GPU parallel \ncomputing architecture. In Microprocessor Forum, May 2007. [20] K.K. Parhi and D.G. Messerschmitt. Static \nrate-optimal scheduling of iterative data-.ow programs via optimum un\u00adfolding. IEEE Transactions on Computers, \n40(2):178 195, 1991. [21] Jose Luis Pino, Shuvra S. Bhattacharyya, and Edward A. Lee. Ahierarchical multiprocessor \nscheduling framework for synchronous data.ow graphs. Technical Report UCB/ERL M95/36, University of California, \nBerkeley, May 1995. [22] B. R. Rau. Iterative modulo scheduling: An algorithm for software pipelining \nloops. In Proc. of the 27th Annual In\u00adternational Symposium on Microarchitecture, pages 63 74, November \n1994. [23] B. R. Rau, M. S. Schlansker, and P. P. Tirumalai. Code generation for modulo scheduled loops. \nIn Proc. of the 25th Annual International Symposium on Microarchitecture, pages 158 169, November 1992. \n[24] J.S\u00b4alez. Modulo scheduling fora fully\u00ad anchez and A. Gonz\u00b4distributed clustered VLIW architecture. \nIn Proc. of the 33rd Annual International Symposium on Microarchitecture, pages 124 133, December 2000. \n [25] Michael Bedford Taylor et al. The Raw microprocessor: A computationalfabric for software circuits \nand general purpose programs. IEEE Micro, 22(2):25 35, 2002. [26]W. Thies,M. Karczmarek,andS.P. Amarasinghe. \nStreamIt: Alanguage for streaming applications. In Proc. of the 2002 International Conference on Compiler \nConstruction, pages 179 196, 2002. [27] ShihweiLiao,ZhaohuiDu,GanshaWu,andGuei-YuanLueh. Data and computation \ntransformations for brook streaming applications on multiprocessors. Proc. of the 2006 Inter\u00adnational \nSymposium on Code Generation and Optimization, 0(1):196 207, 2006. [28]D. Zhang,Z.Li,H.Song,andLLiu.Aprogramming \nmodel for an embedded media processing architecture. In Proc. of the 5thInternational Symposium on Systems, \nArchitectures, Modeling, and Simulation, volume 3553 of Lecture Notes in Computer Science, pages 251 \n261, July 2005.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>While multicore hardware has become ubiquitous, explicitly parallel programming models and compiler techniques for exploiting parallelism on these systems have noticeably lagged behind. Stream programming is one model that has wide applicability in the multimedia, graphics, and signal processing domains. Streaming models execute as a set of independent actors that explicitly communicate data through channels. This paper presents a compiler technique for planning and orchestrating the execution of streaming applications on multicore platforms. An integrated unfolding and partitioning step based on integer linear programming is presented that unfolds data parallel actors as needed and maximally packs actors onto cores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally overlapped with computation on the cores. To facilitate experimentation, a generalized code generation template for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming applications, a geometric mean speedup of 14.7x is achieved on a 16-core Cell platform compared to a single core.</p>", "authors": [{"name": "Manjunath Kudlur", "author_profile_id": "81100105059", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1022758", "email_address": "", "orcid_id": ""}, {"name": "Scott Mahlke", "author_profile_id": "81100622742", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1022759", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375596", "year": "2008", "article_id": "1375596", "conference": "PLDI", "title": "Orchestrating the execution of stream programs on multicore platforms", "url": "http://dl.acm.org/citation.cfm?id=1375596"}