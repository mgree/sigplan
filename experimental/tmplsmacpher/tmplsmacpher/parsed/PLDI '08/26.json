{"article_publication_date": "06-07-2008", "fulltext": "\n Velodrome: A Sound and Complete Dynamic Atomicity Checker for Multithreaded Programs Cormac Flanagan \nStephen N. Freund Jaeheon Yi Computer Science Department Computer Science Department Computer Science \nDepartment University of California at Santa Cruz Williams College University of California at Santa \nCruz Santa Cruz, CA 95064 Williamstown, MA 01267 Santa Cruz, CA 95064 Abstract Atomicity is a fundamental \ncorrectness property in multithreaded programs, both because atomic code blocks are amenable to se\u00adquential \nreasoning (which signi.cantly simpli.es correctness argu\u00adments), and because atomicity violations often \nreveal defects in a program s synchronization structure. Unfortunately, all atomicity analyses developed \nto date are incomplete in that they may yield false alarms on correctly synchronized programs, which \nlimits their usefulness. We present the .rst dynamic analysis for atomicity that is both sound and complete. \nThe analysis reasons about the exact depen\u00addencies between operations in the observed trace of the target \npro\u00adgram, and it reports error messages if and only if the observed trace is not con.ict-serializable. \nDespite this signi.cant increase in pre\u00adcision, the performance and coverage of our analysis is competitive \nwith earlier incomplete dynamic analyses for atomicity. Categories and Subject Descriptors D.2.4 [Software \nEngineer\u00ading]: Software/Program Veri.cation reliability; D.2.5 [Software Engineering]: Testing and Debugging \nmonitors, testing tools; F.3.1 [Logics and Meanings of Programs]: Specifying and Veri\u00adfying and Reasoning \nabout Programs General Terms Languages, Algorithms, Veri.cation Keywords Atomicity, serializability, \ndynamic analysis  1. Introduction Reasoning about the behavior and correctness of multithreaded programs \nis notoriously dif.cult, due to both memory-model is\u00adsues and the non-deterministic interleaving of the \nvarious threads. The widespread adoption of multicore processors and increasingly\u00admultithreaded software \nsigni.cantly exacerbates this reliability problem. Indeed, the advent of multi-core processors may actually \ndegrade the reliability of our software infrastructure. To avoid this undesirable outcome, better tools \nfor building reliable concurrent systems are clearly needed. For most multithreaded programs, an important \n.rst step is to verify the key correctness properties of race-freedom and atomicity. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 13, 2008, \nTucson, Arizona, USA. Copyright c . 2008 ACM 978-1-59593-860-2/08/06. . . $5.00 Race-freedom guarantees \nthat the program s behavior can be understood as if it executes on a sequentially-consistent mem\u00adory \nmodel [17].  Atomicity guarantees that the program s behavior can be un\u00adderstood as if each atomic block \nexecutes serially (without in\u00adterleaved steps of other threads), which enables straightforward sequential \nreasoning.  Race-freedom and atomicity are complementary properties. For example, the following method \nSet.add is free of race conditions because all shared mutable variables are correctly synchronized. However, \nSet.add still violates its atomicity speci.cation because other threads could update the underlying vector \nelems between the calls to elems.contains and elems.add. class Set { final Vector elems = new Vector(); \natomic void add(Object x) { if (!elems.contains(x)) elems.add(x); } } class Vector { synchronized void \nadd(Object o) { ... } synchronized boolean contains(Object o) { ... } } For race conditions, a variety \nof dynamic race detection tools have been developed [36, 33, 47], including complete race detectors that \nnever produce false alarms [34, 37, 7]. For atomicity, a variety of dynamic analyses have also been de\u00adveloped \n(e.g. [11, 44, 1, 43, 46]), but these tools are all incomplete in that they report false alarms on some \ncorrectly-synchronized programs. For example, the Atomizer [11] uses the Lockset al\u00adgorithm [36] to reason \nabout standard mutual-exclusion locks, but may report false alarms if the target program uses additional \nsyn\u00adchronization idioms to protect its data. Indeed, it has proven surprisingly dif.cult and time consuming \nto identify real errors among the spurious warnings produced by these tools. Even if a code block looks \nsuspicious, it may still be atomic due to some subtle synchronization discipline that is not (yet) understood \nby the current programmer or code maintainer. Additional real bugs (e.g., deadlocks) could be easily \nintroduced while attempting to .x a spurious warning produced by these tools. Conversely, real errors \ncould be ignored because they appear to be false alarms. Sound and Complete Atomicity Checking. To address \nthese problems, this paper presents the .rst dynamic analysis for atom\u00adicity that is both sound (it reports \nan error if the observed trace is non-serializable) and complete (it reports an error only for non-serializable \ntraces). An execution trace is considered serial\u00adizable (also referred to as con.ict-serializable) if \nit can be trans\u00adformed into an equivalent serial trace by commuting adjacent, non\u00adcon.icting operations \nof different threads [4]. We illustrate the behavior of our analysis in terms of the fol\u00adlowing trace \ndiagram, where the vertical ordering of instructions re.ects their interleaved execution. The operations \nbegin and end demarcate atomic blocks, which are intended to be serializable. Atomic blocks may be nested \n(due to function calls, etc), and the outermost block starts a new transaction. Transactions are indi\u00adcated \nvia boxes with associated labels A, B, B, etc. Operations outside an atomic block execute in their own \nunary transaction. The primary goal of our analysis is to verify that each transaction (and thus each \natomic block) in the observed trace is serializable. Thread 1 Thread 2 Thread 3 C begin B z = x A begin \n... rel(m) z = 0 B' endbegin acq(m)  y = 1 C' begin s = 1 t = x end end Our analysis infers happens-before \nedges [26] between program operations. These edges re.ect synchronization, communication, and thread \nordering, as shown via arrows in the above diagram. We then lift this relation from operations to a transactional \nhappens\u00adbefore relation .. For the above trace, our analysis infers that A. B. (via the release-acquire \nedge on m), B. . C. (via the write\u00adread edge on y), and .nally that C. . A(via the write-read edge on \nx). At this stage, our analysis detects a cycle A. B. . C. . A in the transactional happens-before graph, \nwhich reveals that the observed trace is not serializable.1 Blame Assignment. Given the dif.culty in \nunderstanding the warnings produced by prior tools, a particular focus of this work is on blame assignment, \ne.g. how to further localize the error in the above cycle A. B. . C. . A. In particular, our analysis \ndetects that this cycle re.ects the following happens-before path on operations: A: rel(m) <B: acq(m) \n<B: y=1 <C: x=y <A: t=x Since the above path interleaves transaction Awith other con.ict\u00ading operations, \nthere is no equivalent trace where Aexecutes se\u00adrially, and so we blame transaction Afor this atomicity \nviolation. Furthermore, only the outermost atomic block in Ais blamed; no error is reported for the inner \nblock, which is serializable. Transactional Happens-Before Representation. A key contribu\u00adtion of this \nwork is an ef.cient and scalable representation of the transactional happens-before relation. The traditional \nrepresenta\u00adtion technique of clock vectors [30] is not applicable because our happens-before relation \nis over compound transactions and not in\u00addividual operations. Moreover, a trace may contain millions \nof 1 Similar ideas have been explored in the database literature [4]; our work adapts these ideas to \ngeneral-purpose programming languages, in a similar spirit to much recent work on transactional memory. \n transactions, and storing the entire happens-before graph would be infeasible. Our analysis uses two \ntechniques to avoid this: The analysis garbage collects transactions as soon as it can guarantee they \nwill never occur on a cycle. In the above graph, transactions Band Care collected as soon as they terminate, \n but transactions B, B, C,and Care kept alive until A terminates.  The analysis dynamically detects \nwhen transaction allocation is not even necessary. For example, since transaction Bwould be collected \nas soon as it terminates, it is not allocated in the .rst place, and transaction C. is merged with its \npredecessor C. .  Empirical Validation. Combining the above techniques with careful data-representation \nchoices yields a sound and complete atomicity analysis whose performance is competitive with earlier \natomicity analyses, and which provides a signi.cant increase in accuracy. On a range of benchmarks, the \nAtomizer [11] detects 154 non-atomic methods but produces 84 false alarms. In contrast, Velodrome (our \nprototype checker for Java) detects 133 non-atomic methods (and misses the remaining 21 by not generalizing \nfrom the observed traces), but produces zero false alarms. Thus, in the termi\u00adnology of information retrieval, \nVelodrome provides slightly less recall but vastly increased precision a trade-off that we believe is \nquite appropriate for all but the most safety-critical applications. In addition, Velodrome and the Atomizer \nare complementary tools. A promising approach is to run both tools simultaneously, correcting real Velodrome-reported \nerrors as a top priority, and investigating the additional Atomizer-reported warnings as a lower priority \ntask. We also enhance coverage by extending our analysis to explore interleavings that are more likely \nto be non-serializable. In particu\u00adlar, we use the Atomizer to recognize potential atomicity violations, \nsuch as an unsynchronized read-modify-write sequence. Our tool then temporarily blocks the thread performing \nthose operations in the hope that an interleaved write by a second thread will provide a concrete witness \nto the violation. Preliminary experience with this technique is quite promising. It enabled us to .nd \nseveral additional atomicity violations in our benchmarks, and it substantially im\u00adproved Velodrome s \nsuccess rate at .nding randomly-inserted syn\u00adchronization defects in several small programs. Summary. \nThis paper makes the following main contributions: We present the .rst dynamic analysis for atomicity \nthat is both sound and complete: the analysis identi.es exactly those traces that are not con.ict-serializable. \n The analysis performs precise blame assignment, and can typ\u00adically assign blame for each violation \nto particular instructions in a particular atomic block.  We show that completeness can be achieved \nwith little addi\u00adtional overhead: the performance of our analysis is competitive with earlier dynamic \nanalyses.  On a range of standard benchmarks, we showed that our anal\u00adysis detects almost all (85%) \nnon-atomic methods detected by the Atomizer, while reducing the false alarm ratio from roughly 40% to \n0%.  We leverage the Atomizer s dynamic analysis to heuristically guide our checker to explore traces \nmore likely to exhibit atom\u00adicity violations. This technique provides increased coverage with no loss \nof completeness.  Outline. The following section formalizes the semantics of mul\u00adtithreaded programs. \nSection 3 describes an initial version of our analysis, and Section 4 re.nes it to achieve better performance \nand more precise blame assignment. Sections 5 and 6 describe and eval\u00aduate our implementation, respectively. \nSections 7 and 8 conclude with a discussion of related work and potential future work.  2. Atomicity \nin Multithreaded Programs To provide a formal basis for our dynamic analysis, we .rst de\u00ad.ne the execution \nsemantics of multithreaded programs in a style similar to [11]. A program consists of a number of concurrently \nex\u00adecuting threads, each of which has an associated thread identi.er t . Tid.2 Each thread has its own \nlocal store p containing thread\u00adlocal data, such as the program counter and call stack. In addition, \nthe threads communicate through a global store s, which is shared by all threads. The global store maps \nprogram variables x to val\u00adues v. The global store also records the state of each lock variable m . Lock \n.If s(m)= t, then the lock m is held by thread t;if s(m)= ., then that lock is not held by any thread. \nA state S=(s, .) of the multithreaded system consists of a global store s and a mapping . from thread \nidenti.ers t to the local store .(t) of each thread. Execution starts in an initial state S0 =(s0, .0). \nTransitions. The behavior of each thread is captured via the tran\u00adsition relation T .Tid \u00d7LocalStore \n\u00d7Operation \u00d7LocalStore . The relation T (t, p, a, p) holds if the thread t can take a step from a local \nstore p to a new local store p. by performing the operation a . Operation on the global store. The set \nof possible operations that a thread t can perform on the global store include: rd (t, x, v) and wr \n(t, x, v), which read a value v from variable x and write a value v to x, respectively.  acq(t, m) and \nrel (t, m), which acquire and release a lock m.  beginl (t) and end(t), which mark the beginning and \nend of an atomic block. (The label l identi.es a particular atomic block, and is used for error reporting.) \n In code examples, we often omit t and l when they are clear from the context or irrelevant, and we \nuse more familiar syntax, such as x=v, for reads and writes. We use the function tid(a) to extract the \nthread identi.er from an operation. a The relation s .s. models the effect of an operation a on the global \nstore s: see Figure 1. In these rules, the global store s[x := v] is identical to s but maps the variable \nx to the value v. a The transition relation S .S. performs a single step of computation. It chooses an \noperation a by thread t that is applicable in the local store .(t), performs that operation on the current \nglobal store s yielding a new global store s, and returns a new state (s. , .[t := p]) containing the \nnew global and local stores s. and p. . Traces. A trace a is a sequence of operations that captures an \nexecution of a multithreaded program by describing the operations performed by each thread and their \nordering. The behavior of a trace a = a1.a2. \u00b7\u00b7\u00b7 .an is de.ned by the relation S0 .a Sn, which holds \nif there exist intermediate states S1,..., Sn-1 such a2 an that S0 .a1 S1 .\u00b7\u00b7\u00b7.Sn. A transaction in a \ntrace a is the sequence of operations executed by a thread t starting with a beginl (t) operation and \ncontaining all t operations up to and including a matching end(t) operation, or up to the end of the \ntrace, if there is no matching end(t) operation. In addition, if an operation a by thread t does not \noccur within an atomic block for t, then the operation a by itself is considered a (unary) transaction. \nThus, each transaction is non-empty. 2 Although dynamic thread creation is not explicitly supported by \nthe se\u00admantics, it can be modeled within the semantics in a straightforward way. Figure 1: Semantics \nof Multithreaded Programs Domains: u, t .Tid x .Var v .Value m .Lock p .LocalStore . .LocalStores = Tid \n.LocalStore s .GlobalStore =(Var .Value) .(Lock .(Tid .{.})) S .State = GlobalStore \u00d7LocalStores Operations: \na .Operation ::= rd (t, x, v) |wr (t, x, v) | acq(t, m) |rel (t, m) | beginl (t) |end(t) l . Label a \n Effect of operations: s .s. [ACT READ] [ACT WRITE] s(x)= v s .rd(t,x,v) wr(t,x,v) ss .s[x := v] [ACT \nACQUIRE] [ACT RELEASE] s(m)= . s(m)= t s .acq(t,m) s .rel(t,m) s[m := t] s[m := .] [ACT OTHER] a .{beginl \n(t), end(t)} s .a s a S  State transition relation: S . [STD STEP] a t = tid(a) T (t, .(t),a, p) s \n.s. a (s, .) .(s. , .[t := p]) Serializable Traces. A trace is serial if each transaction s opera\u00adtions \nexecute contiguously, without interleaved operations of other threads. The notion of serializability \nis based on the idea of con\u00ad.icting operations. Two operations in a trace con.ict if: 1. they access \n(read or write) the same variable, and at least one of the accesses is a write; 2. they operate on (acquire \nor release) the same lock; or 3. they are performed by the same thread.  If two operations do not con.ict, \nthey commute. Two traces are equivalent if one can be obtained from the other by repeatedly swapping \nadjacent commuting operations. Equivalent traces exhibit equivalent behavior. A trace is serializable \nif it is equivalent to some serial trace. Examples. In the following trace, the read-modify-write se\u00adquence \nof Thread 1 is interleaved with a write by Thread 2. This trace is clearly not serial; it is also not \nserializable, because the write by Thread 2 con.icts with both the read and write by Thread 1 and cannot \nbe commuted outside the atomic block. Thread 1 Thread 2 begin tmp= x x=0 x=tmp+1 end Atomicity violations \nsuch as this one can be caught by the At\u00adomizer [11] and other dynamic atomicity tools [44, 43], but \nthese tools are prone to false alarms. For example, the Atomizer uses Eraser s LockSet algorithm [36] \nto reason about lock-based syn\u00adchronization and cannot understand more complex synchronization patterns. \nTo illustrate this limitation, the following program uses a volatile variable b to indicate whether thread \n1 or thread 2 has ex\u00adclusive access to the shared variable x. Thread 1 Thread 2 while (true) { while \n(true) {while (b != 1) { while (b != 2) {skip; skip; } } begin begin inttmp=x; inttmp=x; x=tmp +1; x=tmp+1; \nb=2; b=1; end end } } Even though this program yields only serializable traces, the At\u00adomizer will report \nfalse alarms because it cannot understand the program s synchronization discipline; other atomicity tools \nbehave in a similar fashion.  3. Dynamic Analysis for Serializability We now describe our dynamic analysis \nfor precisely identifying non-serializable traces. Given a trace a,the happens-before relation <a for \na is the smallest transitively-closed relation on operations such that if an operation a occurs before \nb in a,and a con.icts with b,then a happens-before b.3 The transactional structure of traces induces \nan equivalence relation on operations: a ~a b if aand boccur in the same transaction in a. Since all \noperations in a transaction are intended to (conceptually) happen contiguously, we combine these two \nrelations into a (transitively-closed) extended happens-before relation: def * <~a =(<a .~a) We lift \nthis extended happens-before relation from operations to transactions, and so a transaction Ahappens-before \ntransaction B in a(written A.a B)if A . =B and there exists some operations a of A and b of B such that \na<~a b. We then leverage existing results in database theory [4] to show that a is serializable if and \nonly if the transactional happens-before order .a is acyclic. Analysis Details. Our analysis is an online \nalgorithm that main\u00adtains an analysis state f; when the target program performs an op\u00ad a eration a, the \nanalysis updates its state via the relation f.f. . For clarity, we initially present a basic version \nof our analysis. This initial analysis allocates a node in the happens-before graph for each transaction \nin the observed trace. We let Node denote the set of such nodes, and Node. =Node .{.}. The instrumentation \nstore f =(C,L,U,R,W,H)is a tuple of six components: C : Tid . Node. identi.es the current transaction \nnode (if any) for each thread;  L:Tid .Node. identi.es the transaction that executed the last operation \n(if any) of each thread;  U : Lock . Node. identi.es the last transaction (if any) to release or unlock \neach lock;  3 In theory, a particular operation a could occur multiple times in a trace. We avoid this \ncomplication by assuming that each operation includes a unique identi.er, but, to avoid clutter, we do \nnot include this unique identi.er in the concrete syntax of operations. Figure 2: Instrumentation Relation: \nf.a f [INS ENTER] C(t)= . n is fresh H. = H{(L(t),n)}C. = C[t:=n] ,W,H).beginl (t) (C) (C,L,U,R ,L,U,R,W,H \n [INS EXIT] n = C(t) . n = . C. . = C[t :=.] L= L[t:=n] end(t) (C,L,U,R,W,H).(C. ,L,U,R,W,H) [INS ACQUIRE] \nn=C(t) n =.. H. =H{(U(m),n)} acq(t,m) (C,L,U,R,W,H).(C,L,U,R,W,H) [INS RELEASE] U. n=C(t) n =.. =U[m \n:=n] rel(t,m) (C,L,U,R,W,H).(C,L,U. ,R,W,H) [INS READ] n = C(t) n =.. R. = R[(x,t):=n] H. = H{(W(x),n)} \nrd(t,x,v) (C,L,U,R,W,H).(C,L,U,R,W,H) [INS WRITE] n =C(t) n=.. W. = W[x :=n] H. = H({(R(x,t),n)|t. \n.Tid}.{(W(x),n)}) wr(t,x,v) (C,L,U,R,W,H).(C,L,U,R,WH) [INS OUTSIDE] C(t)=. lis a fresh label a.{acq(t,m),rel \n(t,m),rd (t,x,v),wr(t,x,v)}f .beginl (t) f1 end(t) f f1 .a f2 f2 . f f.a R:Var \u00d7Tid .Node. identi.es \nthe last transaction of each thread to read from each variable;  W : Var . Node. identi.es the last \ntransaction (if any) to write to each variable; and  H . Node \u00d7Node is the happens-before relation on \ntrans\u00adactions. (More precisely, the transitive closure H * of His the happens-before relation, since, \nfor ef.ciency, H is not transi\u00adtively closed.)  In the initial analysis state f0, these components \nare all empty: f0 =(.t..,.t..,.m..,.x,t..,.x.., \u00d8) a The relation f .f. shown in Figure 2 updates the \nanalysis state appropriately for each operation aof the target program. The .rst rule [INS ENTER] handles \na beginl (t)operation, which starts a new transaction by thread t. The rule checks that thread t is not \nalready in a transaction (i.e., C(t)=.) and updates Cto record that thread t is inside a fresh transaction \nn. (We defer handling nested atomic blocks to the following section.) This rule uses the operation HE \nto extend the happens-before graph with additional edges E .Node. \u00d7Node., .ltering out self-edges and \nedges that start or end on .: def HE = H.{(n1,n2).E |n1 .=n2,n1 .=.,n2 =..} Thus, in [INS ENTER],if L(t)=., \nthen the happens-before graph is unchanged. Otherwise it is extended with an edge from the last transaction \nof thread t to the current transaction of t.The rule [INS EXIT] handles end(t)simply by updating C(t)and \nL(t) appropriately. The rule [INS ACQUIRE] for a lock acquire acq(t, m) updates the happens-before graph \nwith an edge from the last release U(m)of that lock. Conversely, the rule [INS RELEASE] for a lock release \nrel (t, m)updates U(m)with the current transaction n. The rule [INS READ] for a read operation rd(t, \nx, v) records (1) that the last read of the variable x by thread t occurs in the current transaction \nn, and (2) that the last write to x happens before the current transaction (since reads and writes con.ict). \nThe rule [INS WRITE] for a write operation wr(t, x, v)records that the last write to x is by the current \ntransaction n, and that all previous accesses (reads or writes) to x happen before n. For operations \noutside the dynamic scope of any atomic block (and thus outside any existing transaction), the rule [INS \nOUTSIDE] enters a new transaction, performs that operation, and then exits that transaction. This rule \nis simple but inef.cient; optimized vari\u00adants are described in the following section. a We extend the \nrelation f .f. from operations to traces: the relation f0 .a fn holds for a trace a =a1. \u00b7\u00b7\u00b7.an if there \nexist intermediate analysis states f1,...,fn-1 such that: a1 a2 an f0 .f1 .\u00b7\u00b7\u00b7.an-1 fn-1 .fn Correctness. \nThe set Error denotes analysis states that contain a non-trivial cycle in the happens-before relation: \ndef Error = {(C, L, U, R, W, H)|H * contains a non-trivial cycle} Our central correctness result is that \nthe dynamic analysis is both sound and complete: it identi.es exactly the non-serializable traces. That \nis, if f0 .a f then f .Error if and only if a is not serializable. This result follows from the following \ninductive invariant describ\u00ading how particular properties of the trace a are represented in the analysis \nstate f. This correspondence relies on a mapping f : a . Node from each operation a in a to a corresponding \nnode in the happens-before graph representing the transaction in which a appears. DEFINITION 1. Given \nf =(C, L, U, R, W, H), the invariant Inv(a, f, f) is the conjunction of the following conditions, for \nall t .Tid,x .Var, m .Lock , and a, b .a: 1. If t is in a transaction at the end of a and a is the last \noperation by t in a,then C(t)=f(a). 2. If t is not in a transaction at the end of a,then  C(t)=.. \n If the last operation by t in a is a,then L(t)=f(a); if there is no such operation, then L(t)=..  \n3. If the last write to x in a is a,then W(x)=f(a); if there is no such write, then W(x)=..  4. If the \nlast read of x by t in a is a,then R(x, t)=f(a); if there is no such read, then R(x, t)=..  5. If the \nlast release of m in a is a,then U(m)=f(a); if there is no such release, then U(m)=..  6. If f(a)=f(b)then \na ~a b. 7. If a<a b then (f(a),f(b)).H * . 8. If (n1,n2).Hthen there exists a1,a2 .a such that f(a1)=n1, \nf(a2)=n2 and a1 <a a2.  a THEOREM 1. Given f0 .f, f .Error . a is not serializable PROOF SKETCH:The \ninvariant .f. Inv(a, f, f)holds via a proof by induction on the length of the trace a. Hence: f .Error \n . H * contains a non-trivial cycle . <~a contains a cycle with operations from different transactions \n . .a contains a cycle . a is not serializable where the last step follows from a standard argument \n[4].   4. Extensions and Optimizations The analysis presented so far is correct but requires substantial \nimprovement in order to scale to realistic programs. 4.1 Garbage Collection A trace may include many \nmillions of transactions, making stor\u00adage of the entire happens-before graph on transactions infeasible. \nHence, a key challenge is garbage collecting old, redundant nodes. References to a particular node n \ncan be stored in the vari\u00adous components of the analysis state, with the result that an out\u00adgoing edge \nfrom n can be added at any time (for example, via [INS ACQUIRE], etc). A careful reading of the instrumentation \nrules, however, reveals that incoming edges to a node can be added only by the thread executing that \ntransaction. Thus, if a transaction n has already .nished (i.e., n .. Range(C)), additional incoming \nedges will never be added to n. Hence, a .nished node n with no incom\u00ading edges (i.e., n ..Range(H)) \nwill never occur on a cycle. In this situation, we can safely garbage collect n and remove it from the \nhappens-before graph. There still may be references to n from the analysis state components L, U, W,and \nR,but these are weak references and should be reset to .when n is collected. This garbage collection \nprocess is formalized via the following rule, which can be applied at any time during the analysis: n \n..Range(C) n ..Range(H) L. =L\\{n}U. =U\\{n} . W R=R\\{n} =W\\{n}H=H\\{n} gc . (C, L, U, R, W, H).(C, L, U. \n, R, W. , H) The rule uses the following notation to update maps and relations: def W\\{n} = .x. if W(x)=n \nthen .else W(x) = n1,n2).H|n1 =n, n2 =n} H\\{n} def {(.. In practice, we trigger garbage collection by \nincluding in each node a count of the number of references to that node from within Hor C. We maintain \nthe invariant that the happens-before graph is acyclic, since any attempt to add a cycle-generating edge \nindicates an error that is immediately reported. Thus, the absence of cycles means that reference counting \nimmediately collects all nodes as soon as they become garbage. The experimental results of Section 6 \nshow that garbage collec\u00adtion is extremely effective; we typically have at most a few dozen live nodes \nat any time, even for sizeable benchmarks. 4.2 Non-Transactional Operations The rule [INS OUTSIDE] described \nabove is inef.cient, in that it allocates nodes at an extremely fast rate (one node per non\u00adtransactional \nheap access) and leads to long sequences of unary transactions. In many situations, this allocation is \nunnecessary. In particular, for an operation rd (t, x, v)outside a transaction, the [INS OUTSIDE] rule \ncreates a new node n with predecessors L(t) and W(x). Suppose, however, that L(t) and W(x) are already \n., perhaps because they have already been collected. In this case, n would be Figure 3: Optimized Rules \nfor Non-Transactional Operations [INS OUTSIDE ACQUIRE] C(t)= . .H. ,n. = merge(H,{L(t),U(m)}) L. = L[t:= \nn] (C,L,U,R,W,H) .acq(t,m) (C,L. ,U,R,W,H) [INS OUTSIDE READ] C(t)= . .H. ,n. = merge(H,{L(t),W(x)}) \nR. L. = R[t := n]= L[t:= n] (C,L,U,R,W,H) .rd(t,x,v) (C,L. ,U,R. ,W,H) [INS OUTSIDE RELEASE] C(t)= . \nU. = U[m := L(t)] (C,L,U,R,W,H) .rel(t,m) (C,L,U. ,R,W,H) [INS OUTSIDE WRITE] C(t)= . S = {R(x,t) | t. \n. Tid}. {W(x),L(t)}.H. ,n. = merge(H,S) W. L. = W[t := n]= L[t:= n] (C,L,U,R,W,H) .wr(t,x,v) (C,L. ,U,R. \n,W,H) merge : ((Node \u00d7 Node) \u00d7 2Node. ) . ((Node \u00d7 Node) \u00d7 Node.) merge(H,{n1,...,nk})= 8 < .H, .. .H,nj \n. if ni = ..i. 1..k if .j such that nj .= . and .i . 1..k, ni = . or (ni,nj ) .H * : .H . {(ni,n) | i. \n1..k},n. otherwise, where nis fresh immediately collected once the operation .nishes, and so we avoid \nallocating it in the .rst place, via the rule: C(t)= .L(t)= .W(x)= . R. = R[(x,t):= .] (C,L,U,R,W,H) \n.rd(t,x,v) (C,L,U,R. ,W,H) Alternatively, if W(x) is . but L(t) is not, then L(t) is the unique predecessor \nof n,and nwill never have additional incoming edges. Hence, the nodes L(t) and n can be merged without \nintroducing additional cycles in the happens-before graph, or in later versions of that graph. This reasoning \nis summarized by the following rule: C(t)= .L(t) .= .W(x)= . R. = R[(x,t):= L(t)] (C,L,U,R,W,H) .rd(t,x,v) \n(C,L,U,R. ,W,H) Even if neither L(t) nor W(x) are ., we we can still re-use L(x) if there is a happens-before \npath from W(x) to L(x): C(t)= .L(t) .= .W(x) . = . (L(t),W(x)) .H * R. = R[(x,t):= L(t)] (C,L,U,R,W,H) \n.rd(t,x,v) (C,L,U,R. ,W,H) To avoid a multitude of such rules, we introduce the auxil\u00adiary function merge \nto identify various situations where merging can safely be performed: see Figure 3. This function takes \nas in\u00adput a happens-before relation and a collection of argument nodes n1,...,nk, and it returns a (possibly \nextended) happens-before re\u00adlation, and a (possibly new) node that happens-after each of the argument \nnodes. The .gure also includes analysis rules that lever\u00adage merge to handle non-transactional operations \nef.ciently along the lines outlined above.  4.3 Blame Assignment When the analysis determines that a \nparticular trace a is not seri\u00adalizable, it can produce a cycle of transactions whose combination is \nnot serializable. We now investigate how to assign blame to a particular transaction in that cycle. A \ntransaction Ais self-serializable in trace aif ahas an equiv\u00adalent trace a. in which A executes serially. \n(Other transactions in a. need not execute serially, and so the notions of self-serializable transactions \nand serializable traces are distinct.) Once our algo\u00adrithm identi.es a non-serializable trace a, we would \nlike to assign blame to a particular transaction within that trace that is not self\u00adserializable. For \nthe cycle A. B. . C. . A described in the in\u00adtroduction, we should assign blame to transaction A, since \nall other transactions in the cycle are self-serializable. To support blame assignment, we extend the \nhappens-before graph to identify the particular operations inducing each edge be\u00adtween transactions. \nSpeci.cally, we store with each edge the times\u00adtamp of the operations at its head and tail. We assign \nblame using these timestamps as follows. First, note that when an operation d performed by thread t during \nsome transaction D completes the .rst cycle in the happens-before graph, the trace prior to dis seri\u00adalizable. \nThus transactions other than D are still serializable, and we can only potentially blame D. From the \ntransactional happens\u00adbefore cycle, we know that D.a E.aD,where Eis some transac\u00adtion of another thread. \nHence, there exists some earlier operations d. . D and e . E such that d. <~a e<~a d. The key ques\u00adtion \nis whether d. <a e<a d; if so, then transaction D is not self-serializable and should be blamed. Let \nnbe the node for D. For each node m. = non the happens\u00adbefore cycle, if the timestamp on the incoming \nedge to m is less than or equal to the timestamp on the outgoing edge from m,then the cycle is said to \nbe increasing. In this situation, the happens\u00adbefore relation on transactions re.ects the underlying \nhappens\u00adbefore relation on operations, and so there do exist some earlier operations d. . D and e . E \nsuch that d. <a e<a d. Hence the transaction D, which contains both d. and d, is not self-serializable. \nSomewhat surprisingly, it is not always possible to blame a sin\u00adgle transaction, since all transactions \nin a non-serializable trace may still be self-serializable. To illustrate this point, consider the following \ntwo traces. The .rst trace executes D. serially, but trans\u00adaction E. is also self-serializable (as illustrated \nby the second, equivalent trace). Thus, both transactions are self-serializable, even though together \nthey constitute a non-serializable trace. Thread 1 Thread 2 E' 0: begin D' 1:  Despite this theoretical \ndif.culty, in practice our algorithm is gen\u00aderally successful at assigning blame for each non-serializable \ntrace to a particular transaction that is not self-serializable. 0: begin 1: x = 0 2: u = y 3: end 3: \nend Thread 1 Thread 2 D' 0: begin 1: x = 0 u = y 3: end E' Nested Atomic Blocks. We now extend our system \nto nested atomic blocks. Only the outermost atomic block is considered to start a new transaction; nested \nblocks execute within that transac\u00adtion but can still be refuted by our blame assignment algorithm. To \nsupport nesting, we extend C(t) to denote a stack, where the entries record both the identifying label \nand the timestamp of the .rst operation in each atomic block in the dynamic scope. For example, right \nbefore Thread 1 executes step 4 below, C(1) would contain (p, 0).(q, 1).(r, 3):  Thread 1 Thread 2 A \n0: beginp 1: beginq 2: t = x B 3: beginr 4: x = t+1 5: end 6: end 7: end Once Thread 1 executes step \n4, our algorithm detects an in\u00adcreasing cycle from A and will refute any atomic block in A that contains \nboth the root and target operations ( 2: t= x and 4: x= t+1 , respectively) of that cycle. Thus, the \nalgorithm will refute the atomic blocks p and q; the block labeled r is not refuted, and is serializable \n(indeed, serial in this trace). Blame Assignment Details. To implement blame assignment, we introduce \nthe notion of a Step, which is a pair of a transaction node and a timestamp. We extend the component \nUof the analysis state so that U(m) isnow a Step that records both the transaction and the timestamp \nof the last release operation on m; the other state components are extended in a similar fashion: f :(C, \nL, U, R, W, H) C : Tid . (Label \u00d7Step) * L : Tid . Step. U : Lock . Step. R : Var \u00d7Tid . Step. W \n: Var . Step. H. Step \u00d7Step  Step = Node \u00d7Nat f0 =(.t.., .t..,.m..,.x, t..,.x.., \u00d8) The revised analysis \nis de.ned by the rules in Figure 4. The rule [INS2 ENTER] for beginl (t) handles the case where a new \ntransac\u00adtion is required because the stack C(t) is empty. It allocates a fresh node n, creates a step \nthat pairs n with the initial timestamp 0 of the beginl (t) operation, and records that thread t is now \nexecuting an atomic block labeled l,where s is the .rst step of that block. We disallow multiple edges \nwith different timestamps between the same nodes in the happens-before graph, for space reasons. That \nis, if ((n, i), (m, j)) and ((n, i), (m, j)) are both in Hthen i = i. and j = j. This invariant bounds \nthe size of Hby |Node|2 . To preserve this invariant, rule [INS2 ENTER] uses the following operation \nto extend the happens-before relation Hwith additional edges G .(Step. \u00d7Step.): HG = {((n, i), (m, j)) \n.H|\u00ac.i,j. . ((n, i), (m, j)) .G}.{((n, i), (m, j)) .G |n . = m} If thread t is already inside a transaction \nwhen it executes beginl (t),rule [INS2 RE-ENTER] extends the stack C(t) with an additional entry for \nthe new atomic block. These rules use the notation L(t)+1 to increment the timestamp within a step; if \nL(t)=(n, k),then L(t)+1 =(n, k +1).The rule [INS2 EXIT] exits an atomic block by popping the last entry \nof the stack and, as in the other rules, incrementing the timestamp in L(t). The four [INS2 OUTSIDE \n...] rules are variants of the earlier [INS OUTSIDE ...] rules that use the revised merge function shown \nin Figure 4, which operates on steps. These rules mostly ignore timestamps, since they operate on unary \ntransactions that are by de.nition serializable. In particular, the merge function ignores timestamps \nwhen searching for a representative step sj that (non\u00adstrictly) happens after steps s1, ..., sk. The \ngarbage collection rule [INS2 GC] picks a node n such that no step in S = {n}\u00d7Nat occurs on any transaction \nstack or in the range of the happens-before relation H; all references to steps in S are then removed \nfrom the analysis store and replaced with ..  5. Velodrome Prototype We have developed a prototype \nimplementation, called Velodrome, of our atomicity analysis. This tool takes as input a compiled Java \nprogram and a speci.cation of which methods in that program should be atomic, and it reports an error \nwhenever it observes a non-serializable trace of an atomic method. For example, on the Set example from \nthe introduction, Velodrome reports the fol\u00adlowing error graph (generated with dot [16]) when two threads \nconcurrently add elements to the same set object: Warning: Set.add is not atomic: Thread 1: in #3.Set.add \nacq(#2) #2.Vector.contains  #2.Vector.add acq(#2) In this error message, #3 denotes a particular Set \nobject and #2 de\u00adnotes the set s underlying Vector. The boxes indicate the transac\u00adtions whose combination \nis not serializable. Each happens-before edge is labeled with the operation that generated it, and the \nlast edge in a cycle is dashed. The outlined box indicates where Velo\u00addrome has placed blame. These graphs \nare extremely useful for understanding error messages, and Velodrome records additional diagnostic information \nto construct them. Velodrome is a component of RoadRunner, a general framework for implementing dynamic \nconcurrent program analyses. Road-Runner is written entirely in Java and can run on any standard JVM. \nRoadRunner instruments class .les at load time using the BCEL Bytecode Engineering Library [3]. The instrumented \ncode generates an event stream, with one event for each lock acquire or release, memory read or write, \nand atomic method entry or exit per\u00adformed by the target program. RoadRunner passes this event stream \nto the analysis back-end. Working exclusively at the bytecode level offers several advan\u00adtages. Speci.cally, \nthe tool can check any Java program, regardless of whether the full source code is available, and only \nneeds to rea\u00adson about the relatively simple bytecode language. However, this does make it dif.cult to \nsupport source-level annotations, which are useful for specifying which methods should be atomic, specifying \nlibrary behaviors, and so on. We currently support such con.gura\u00adtion through command-line options. Re-entrant \n(and hence redundant) lock acquires and releases are .ltered out by RoadRunner, and so do not complicate \nthe back\u00adend analysis. RoadRunner is typically con.gured to also .lter out operations on thread-local \ndata, which dramatically improves the performance of the analyses, although this optimization is slightly \nunsound [36]. One limitation of our current prototype is that it performs the analysis only on objects \nand .elds, and not on arrays. Supporting arrays would be possible, but would add additional complexity. \n Figure 4: Instrumentation Relation with Blame Assignment [INS2 ENTER] [INS2 RE-ENTER] C(t)= .s =(n,0) \nwhere nis fresh C(t)= \u00df .s = L(t)+1 C. C. = C[t := (l,s)] L= L[t := s]= C[t := \u00df.(l,s)] L= L[t := s] \nH. = H{(L(t),s)}H. = H{(L(t),s)} ,W,H) .beginl (t) (C. ,W,H) .beginl (t) (C. (C,L,U,R,L,U,R,W,H)(C,L,U,R,L,U,R,W,H) \n[INS2 GC] [INS2 EXIT] S = {n}\u00d7Nat .t.(C(t) ..L(t) .S) C(t)= \u00df.(s ,l) s = L(t)+1 . C. Hn(Step \u00d7S)= \u00d8H= \nH\\S = C[t := \u00df] L= L[t := s] . U W L= L\\S R= R\\S = W\\S = U\\S end(t) (C,L,U,R,W,H) .(C. ,L,U,R,W,H) \ngc . (C,L,U,R,W,H) .(C,L,U. ,R,W. ,H) [INS2 INSIDE ACQUIRE] [INS2 INSIDE RELEASE] C(t) ..s = L(t)+1 \nC(t)= .s = L(t)+1 L= L[t := s] H= H{(U(m),s)}L= L[t := s] U. = U[m := s] acq(t,m) rel(t,m) (C,L,U,R,W,H) \n.(C,L,U,R,W,H)(C,L,U,R,W,H) .(C,L,U. ,R,W,H) [INS2 INSIDE READ] [INS2 INSIDE WRITE] = s L(t)+1 . C(t) \n.. = C(t)= .s = L(t)+1 W. H. = H{(W(x),s)}H. = H({(R(x,t),s) |t. .Tid}.{(W(x),s)}) L= L[t := s] R= R[(x,t):= \ns] L= L[t := s]= W[x := s] rd(t,x,v) . wr(t,x,v) (C,L,U,R,W,H) .(C,L,U,R,W,H)(C,L,U,R,W,H) .(C,L,U,R,W. \n,H) [INS2 OUTSIDE ACQUIRE] [INS2 OUTSIDE RELEASE] C(t)= . .H. ,s. = merge(H,{L(t),U(m)}) C(t)= .s = L(t)+1 \nL. = L[t := s] L. = L[t := s] U. = U[m := s] acq(t,m) rel(t,m) (C,L,U,R,W,H) .(C,L,U,R,W,H)(C,L,U,R,W,H) \n.(C,L,U. ,R,W,H) [INS2 OUTSIDE WRITE][INS2 OUTSIDE READ] C(t)= .C(t)= . S = {R(x,t) |t. .Tid}.{W(x),L(t)}.H. \n,s. = merge(H,{L(t),W(x)}) . .H. ,s. = merge(H,S) R= R[t := s] L= L[t := s] W. = W[t := s] L= L[t := \ns] rd(t,x,v) . (C,L,U,R,W,H) .(C,L,U,R,W,H) wr(t,x,v) . (C,L,U,R,W,H) .(C,L,U,R,W,H) merge : ((Step \n\u00d7Step) \u00d72Step. ) .((Step \u00d7Step) \u00d7Step.) 8 >  .H, .. if si = ..i .1..k < .H,sj. if .j such that sj =..and \nmerge(H,{s1,...,sk})= > .i .1..k, si = .or si happens-before sj in H : .H{(si,s) |i .1..k},s. otherwise, \nwhere s =(n,0) and nis fresh RoadRunner includes several race detection algorithms (includ\u00ading Eraser \n[36] and a complete happens-before detector), which can be run concurrently with Velodrome if race conditions \nare a concern in the target program. As mentioned in the introduction, Velodrome and the Atomizer can \nalso be run concurrently. Analysis Store. Ef.ciently implementing the analysis of Figure 4 requires a \nnumber of careful data representation choices, particu\u00adlarly for handling weak references to nodes that \nare internally col\u00adlected and recycled by our analysis. Each step is represented as a 64-bit integer \nwhose top 16 bits identify a particular Node object, and whose lower 48 bits represent a timestamp within \nthat Node. When a Node n is collected, we also record the last timestamp k used for that Node.If a step \n(n,k) is later dereferenced, we check whether k. =k; if so, then that step is interpreted as being .,since \nthe conceptual node it pointed to has been collected, even though the corresponding Node object has been \nrecycled to represent a new conceptual node. For each node, we maintain a set of ancestors of that node. \nThis ancestor set allows us to immediately detect when a cycle is about to be added to the graph, which \nyields several bene.ts: (1) It supports more precise error messages, which could include, for example, \nthe stack trace of the current thread. (2) It enables us to avoid adding that edge and thus maintain \nan acyclic graph, which facilitates reference-counting garbage collection. (3) It supports an ef.cient \nimplementation of the merge function of Figure 4. Adversarial Scheduling. As mentioned in the introduction, \nour system can guide the scheduler to generate traces likely to exhibit atomicity violations. In particular, \nwe can con.gure Velodrome to concurrently perform the Atomizer analysis and temporarily suspend any thread \nthat is about to perform an operation leading to a potential atomicity violation. This delay, which we \ncurrently set to 100 milliseconds, increases the probability that other threads will perform con.icting \noperations and yield a non-serializable trace that is then caught by Velodrome. We are exploring a number \nof other scheduling policies, such as pausing writes but not reads, Base Instrumented Time Velodrome \nTransactions Program Size Time (slowdown) Without Merge With Merge (lines) (sec.) Empty Eraser Atomizer \nVelodrome Allocated Max. Alive Allocated Max. Alive elevator 520 5.64 1.1 1.1 1.1 1.1 174,000 20 170,000 \n13 hedc 6,400 0.21 6.2 6.0 5.9 6.3 79 37 58 4 tsp 700 0.46 30.9 50.9 60.2 71.7 >1,000,000 8 12,000 1 \nsor 690 0.34 2.3 2.3 2.4 2.9 2,000 2 2 2 jbb 36,000 9.84 2.9 3.2 3.4 3.1 21,000 9 14,000 13 mtrt 11,000 \n0.85 9.3 14.3 22.4 18.3 645,000 5 645,000 5 moldyn 1,400 0.77 3.8 4.0 4.1 4.5 5 4 5 4 montecarlo 3,600 \n1.70 1.6 1.7 1.7 1.7 410,000 4 300,000 4 raytracer 18,000 2.00 4.5 6.7 9.4 9.2 128 8 23 8 colt 29,000 \n16.40 1.2 1.2 1.2 1.2 113 11 58 19 philo 84 2.71 1.0 1.0 1.2 1.2 34 5 34 5 raja 10,000 0.55 4.3 4.4 4.5 \n4.5 60 1 60 1 multiset 300 0.10 4.0 4.4 4.7 10.0 218,000 8 8 8 webl 22,300 0.52 8.6 8.9 9.3 21.0 470,000 \n4 395,000 4 jigsaw 91,100 8.2 1.1 1.1 1.1 1.1 123,000 99 36,600 17 Table 1: Benchmark sizes and running \ntimes, analysis slowdowns, and happens-before graph statistics. allowing some threads to never pause, \nand so on. Similar techniques have proven quite effective in other contexts [20].  6. Evaluation Benchmarks \nThis section summarizes our experience applying Velodrome to several benchmark programs: elevator,a discrete \nevent simulator for elevators [41]; hedc, a tool to access astro\u00adphysics data from Web sources [41]; \ntsp, a Traveling Salesman Problem solver [41]; sor,a scienti.c computing program [41]; mtrt, a multithreaded \nray-tracing program from the SPEC JVM98 benchmark suite [39]; specJBB, the SPEC JBB2000 business ob\u00adject \nsimulator [39]; moldyn, montecarlo,and raytracer from the Java Grande benchmark suite [24]; the colt \nscienti.ccom\u00adputing library [5]; the raja ray tracer [15]; and multiset,a ba\u00adsic multiset implementation, \nphilo, a dining philosophers simu\u00adlation [7]; webl, a scripting language interpreter for processing web \npages, con.gured to execute a simple web crawler [25]; and jigsaw, an open source web server [45] con.gured \nto serve a .xed number of pages to a crawler. We performed all experiments on an Apple Mac Pro with a \nquad-core 3GHz Pentium Xeon processor and 2GB of memory, using OS X 10.4 and Sun s Java HotSpot Client \nVM, version 1.5.0. All classes loaded by the benchmark programs were instrumented, except those from \nthe standard Java libraries. Table 1 presents the size and uninstrumented running time of each program, \nas well as the slowdown of each program when in\u00adstrumented and analyzed by four back-end analyses: Empty \n(which does no work and simply measures the instrumentation overhead), Eraser [36], Atomizer [11], and \nVelodrome (using the optimized semantics of Figure 4). To make the performance experiments realistic, \nwe used Velo\u00addrome to identify non-atomic methods and con.gured the Atomizer and Velodrome to only check \nthe remaining methods for atomicity. This con.guration mimics using these tools in contexts where most \nor all methods satisfy their atomicity speci.cation. This con.gura\u00adtion actually increases the overhead \nfor Velodrome when compared to checking all methods for atomicity, because program traces con\u00adtain many \nsmall transactions rather than a few monolithic ones. Overall, the performance of Velodrome is quite \npromising, and mostly competitive with the less precise Eraser and Atomizer al\u00adgorithms. (It should be \nnoted that elevator, hedc, philo, webl, and jigsaw are not compute-bound, but the remaining benchmarks \nare and experienced an average slowdown of 9.3 for Eraser, 10.4 for Atomizer, and 12.7 for Velodrome.) \nWe believe the substantial slowdown for tsp is due to the instrumented code preventing the virtual machine \ns adaptive compiler from performing certain opti\u00admizations. The last four columns of Table 1 highlight \nthe impact of node merging and garbage collection from Section 4. The columns la\u00adbeled Allocated and \nMax. Alive under the heading Without Merge show the total number of graph nodes allocated and the maximum \nnumber that are active at any time during execution whenusingthena\u00a8ive [INS OUTSIDE]rule.Thecolumnsunder \nWith Merge show the same measurements for the .nal, optimized se\u00admantics of Figure 4. Two important observations \nare worth noting: (1) Garbage collection is essential and extremely effective, reduc\u00ading the number of \nlive nodes by up to four orders of magnitude. (2) Merging reduces the number of node allocations by up \nto several orders of magnitude and has a dramatic impact on running times. Table 2 presents the number \nof methods for which Atomizer generated warnings, under the assumption that all methods are atomic. For \nthese measurements, we counted the number of distinct warnings over a series of .ve runs. We classi.ed \neach Atomizer warning either as actually corresponding to a non-atomic method (that is, a method that \nis not serializable in some trace) or as a false alarm. The non-atomic methods include several errors \nreported in earlier work [11, 32], as well as methods that were not intended to be atomic (such as Thread \nrun() methods and similar routines). The false alarms were due to imprecision in the Atomizer s under\u00adlying \nrace condition and reduction analyses and its inability to rea\u00adson about non-lock-based synchronization. \nThat table also shows the number of non-atomic methods found by Velodrome during the .ve runs, as well \nas how many non-atomic methods reported by the Atomizer were missed by Velodrome. For both tools, the \nlarge majority of errors were reported on the .rst of the .ve runs. Overall, these results indicate that \nVelodrome is quite effec\u00adtive at identifying non-atomic methods. As expected, Velodrome s completeness \n(and hence lack of generalization) did occasionally cause it to require more runs to .nd some errors, \nand it did miss a small number of non-atomic methods in some programs because the execution of these \nmethods happened to be serializable in the observed traces. In jigsaw, 6 of the missed warnings were \ndue to a single non-atomic method that Velodrome mischaracterized. On most benchmarks, however, Velodrome \ndid identify most or all non-atomic methods identi.ed by the Atomizer. Moreover, Velodrome did not report \nany of the large number of false alarms generated by the Atomizer. In particular, it avoided re\u00adporting \nmany spurious warnings on jbb and mtrt causedbyimpre\u00adcise race analysis, fork-join synchronization, and \nother idioms Program Warnings Atomizer Velodrome Non-Serial False Alarms Non-Serial False Alarms Missed \nelevator 5 1 5 0 0 hedc 6 2 6 0 0 tsp 8 0 8 0 0 sor 3 0 3 0 0 jbb 5 42 5 0 0 mtrt 2 27 2 0 0 moldyn 4 \n0 4 0 0 montecarlo 6 0 6 0 0 raytracer 2 3 1 0 1 colt 27 2 20 0 7 philo 2 0 2 0 0 raja 0 0 0 0 0 multiset \n5 0 5 0 0 webl 24 2 22 0 2 jigsaw 55 5 44 0 11 Total 154 84 133 0 21 Table 2: Warnings produced by the \nAtomizer and Velodrome, under the assumption that all methods should be atomic. not understood by the \nAtomizer. The mtrt code also makes heavy use of the standard Java libraries, which are not instrumented. \nAs such, Atomizer cannot reason about synchronization performed in\u00adside those libraries and generates \nmany warnings as a result. Velo\u00addrome does not suffer from this limitation: if Velodrome observes a subsequence \na. of the actual program trace a,then if a. is not serializable it follows that a is also not serializable. \nThus, uninstru\u00admented libraries do not cause Velodrome to report false alarms. Interestingly, the number \nof warnings produced was fairly uni\u00adform when these experiments were repeated using only a single core, \ndespite Velodrome being more sensitive to scheduling than other tools. This may not always be the case \nbut additional experi\u00admentation on large programs is needed to fully quantify the impact of the number \nof cores on Velodrome s analysis. Also, Velodrome s blame assignment algorithm is quite effective, and \nassigned blame to a speci.c method for over 80% of the warnings. In summary, Velodrome dramatically reduces \nthe false alarm rate in comparison to the Atomizer. Roughly half of the Atomizer warnings are false alarms, \nbut Velodrome produces none, while still detecting almost all (85%) of the non-atomic methods. Using \nthe Atomizer to adjust the scheduler, as described in the previous section, improved Velodrome s ability \nto .nd defects dur\u00ading several small experiments. Velodrome found the second non\u00adserial method in raytracer, \nas well as one additional non-serial method in colt and several more in jigsaw. To further study this \ntechnique, we injected atomicity defects into two programs, elevator and colt, by systematically removing \neach synchro\u00adnized statement that induced contention between threads one at a time and then running our \nanalysis on each corrupted program. Without scheduler adjustments, a single run by Velodrome found the \ninserted defect approximately 30% of the time. With scheduler adjustments, the success rate increased \nto approximately 70%.  7. Related Work A variety of tools have been developed to check for atomicity \nviolations, both statically and dynamically. The Atomizer [11] uses Lipton s theory of reduction [27] \nto check whether steps of each transaction conform to a pattern guaranteed to be serializable. Wang and \nStoller developed an alternative block-based ap\u00adproach to verifying atomicity. This approach is more \nprecise than reduction-based approaches, but it is signi.cantly slower for some programs. A detailed \nexperimental comparison of the two ap\u00adproaches is presented in [44]. Wang and Stoller also developed \nmore precise commit-node algorithms [43]. These algorithms fo\u00adcus on both con.ict-atomicity (referred \nto simply as atomicity in this paper) and view-atomicity. By design, these algorithms detect serializability \nviolations that do not occur on the current interleav\u00ading but which could occur on other interleavings. \nOf course, those other interleavings may not be feasible under the program s seman\u00adtics, so these algorithms \nmay yield false alarms. In other work, Xu, Bodik, and Hill [46] developed a precise dynamic analysis \nfor enforcing Strict 2-Phase Locking [9], a suf.\u00adcient but not necessary condition for ensuring serializability. \nHence violations, while possibly worthy of investigation, do not necessar\u00adily imply that the observed \ntrace is not serializable. An alternative approach for verifying atomicity using model-checking has been \nexplored by Hatcliff et al. [19]. Their results suggest that checking atomicity with model-checking is \nfeasible for unit-testing, where the reachable state space is relatively small. Other work explores static \nanalyses, including approaches to statically compute and look for cycles in the happens-before graph \n[10]. Type systems [14, 2, 12, 35, 13, 42] have also been inves\u00adtigated. Compared to dynamic techniques, \nstatic systems provide stronger soundness guarantees and detect errors earlier in the devel\u00adopment cycle, \nbut many of them require more effort from the pro\u00adgrammer or are limited in precision and scalability. \nTo date, none can yet handle all synchronization disciplines precisely. Hoare [22] and Lomet [29] .rst \nproposed the use of atomic blocks for synchronization, and the Argus [28] and Avalon [8] projects developed \nlanguage support for implementing atomic ob\u00adjects. More recent studies have focused on lightweight transac\u00adtions \n[38, 18, 23] and automatic generation of synchronization code from high-level speci.cations [6, 31, 40, \n21]. Much of this work is orthogonal to ours, and while these approaches offer a promising alternative \nconcurrency control, we believe that a combination of the two approaches will be the most effective programming \nmodel for the foreseeable future. 8. Conclusions Programmer support for building reliable multithreaded \nprograms will only continue to grow in importance. Despite the successes of previous race condition and \natomicity checkers, the need to iden\u00adtify false alarms places a large burden on the programmer. We have \npresented a sound and complete atomicity checker that .nds al\u00admost all of the atomicity violations found \nby less precise tools, and which guarantees each warning represents a real violation of con.ict-serializability. \nOur tool occasionally misses a warning that would be produced by other tools because it does not generalize \nthe observed trace to reason about behavior under different schedules. To close this coverage gap, we \nare exploring ways to guide execu\u00adtion toward traces most likely to contain real atomicity errors. Acknowledgments \nThis work was supported in part by the National Science Foun\u00addation under Grants 0341179, 0341387, and \n0644130, and by a Sloan Foundation Fellowship. We thank Kenn Knowles and Caitlin Sadowski for formalizing \nparts of the Velodrome metatheory and clarifying aspects of our development. We thank Tayfun Elmas, Christoph \nvon Praun, and Ben Wood for their assistance with the benchmark programs. References [1] R. Agarwal, \nA. Sasturkar, L. Wang, and S. D. Stoller. Optimized run-time race detection and atomicity checking using \npartial discovered types. In International Conference on Automated Software Engineering, pages 233 242, \n2005. [2] R. Agarwal and S. D. Stoller. Type inference for parameterized race-free Java. In Conference \non Veri.cation, Model Checking, and Abstract Interpretation, pages 149 160, 2004. [3] BCEL. http://jakarta.apache.org/bcel, \n2007. [4] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency Control and Recovery in Database \nSystems. Addison-Wesley, 1987. [5] CERN. Colt 1.2.0. http://dsd.lbl.gov/~hoschek/colt, 2007. [6] X. Deng, \nM. Dwyer, J. Hatcliff, and M. Mizuno. Invariant\u00adbased speci.cation, synthesis, and veri.cation of synchronization \nin concurrent programs. In International Conference on Software Engineering, pages 442 452, 2002. [7] \nT. Elmas, S. Qadeer, and S. Tasiran. Goldilocks: a race and transaction-aware Java runtime. In Conference \non Programming Language Design and Implementation, pages 245 255, 2007. [8] J. L. Eppinger, L. B. Mummert, \nand A. Z. Spector. Camelot and Avalon: A Distributed Transaction Facility. 1991. [9] K. P. Eswaran, J. \nGray, R. A. Lorie, and I. L. Traiger. The notions of consistency and predicate locks in a database system. \nCommunications of the ACM, 19(11):624 633, 1976. [10] A. Farzan and P. Madhusudan. Causal atomicity. \nIn Computer Aided Veri.cation, pages 315 328, 2006. [11] C. Flanagan and S. N. Freund. Atomizer: A dynamic \natomicity checker for multithreaded programs. In Symposium on Principles of Programming Languages, pages \n256 267, 2004. [12] C. Flanagan, S. N. Freund, and M. Lifshin. Type inference for atomicity. In Workshop \non Types in Language Design and Implementation, pages 47 58, 2005. [13] C. Flanagan, S. N. Freund, and \nS. Qadeer. Exploiting purity for atomicity. IEEE Trans. Soft. Eng., 31(4):275 291, 2005. [14] C. Flanagan \nand S. Qadeer. A type and effect system for atomicity. In Conference on Programming Language Design and \nImplementation, pages 338 349, 2003. [15] E. Fleury and G. Sutre. Raja, version 0.4.0-pre4. http://raja.\u00adsourceforge.net/, \n2007. [16] E. R. Gansner and S. C. North. An open graph visualization system and its applications to \nsoftware engineering. Software Practice Experience, 30(11):1203 1233, 2000. [17] K. Gharachorloo. Memory \nConsistency Models for Shared-Memory Multiprocessors. PhD thesis, Stanford University, 1995. [18] T. \nHarris and K. Fraser. Language support for lightweight trans\u00adactions. In Conference on Object-Oriented \nProgramming, Systems, Languages and Applications, pages 388 402, 2003. [19] J. Hatcliff, Robby, and M. \nB. Dwyer. Verifying atomicity speci.ca\u00adtions for concurrent object-oriented software using model-checking. \nIn International Conference on Veri.cation, Model Checking and Abstract Interpretation, pages 175 190, \n2004. [20] K. Havelund. Using runtime analysis to guide model checking of Java programs. In SPIN Model \nChecking and Software Veri.cation, pages 245 264, 2000. [21] M. Hicks, J. S. Foster, and P. Pratikakis. \nInferring locking for atomic sections. In Workshop on Languages, Compilers, and Hardware Support for \nTransactional Computing, 2006. [22] C. A. R. Hoare. Towards a theory of parallel programming. In Operating \nSystems Techniques, volume 9 of A.P.I.C. Studies in Data Processing, pages 61 71, 1972. [23] S. Jagannathan, \nJ. Vitek, A. Welc, and A. L. Hosking. A transactional object calculus. Sci. Comput. Program., 57(2):164 \n186, 2005. [24] Java Grande Forum. Java Grande benchmark suite. http://www.\u00adjavagrande.org/, 2003. [25] \nT. Kistler and J. Marais. WebL a programming language for the web. In World Wide Web Conference, pages \n259 270, 1998. [26] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications \nof the ACM, 21(7):558 565, 1978. [27] R. J. Lipton. Reduction: A method of proving properties of parallel \nprograms. Communications of the ACM, 18(12):717 721, 1975. [28] B. Liskov, D. Curtis, P. Johnson, and \nR. Schei.er. Implementation of Argus. In Symposium on Operating Systems Principles, pages 111 122, 1987. \n[29] D. B. Lomet. Process structuring, synchronization, and recovery using atomic actions. Language Design \nfor Reliable Software, pages 128 137, 1977. [30] F. Mattern. Virtual time and global states of distributed \nsystems. In Parallel and Distributed Algorithms: International Workshop on Parallel and Distributed Algorithms. \n1988. [31] B. McCloskey, F. Zhou, D. Gay, and E. Brewer. Autolocker: synchronization inference for atomic \nsections. In Symposium on Principles of Programming Languages, pages 346 358, 2006. [32] R. O Callahan \nand J.-D. Choi. Hybrid dynamic data race detection. In Symposium on Principles and Practice of Parallel \nProgramming, pages 167 178, 2003. [33] E. Pozniansky and A. Schuster. Ef.cient on-the-.y data race detection \nin multihreaded C++ programs. In Symposium on Principles and Practice of Parallel Programming, pages \n179 190, 2003. [34] M. Ronsse and K. D. Bosschere. RecPlay: A fully integrated practical record/replay \nsystem. ACM Trans. Comput. Syst., 17(2):133 152, 1999. [35] A. Sasturkar, R. Agarwal, L. Wang, and S. \nD. Stoller. Automated type-based analysis of data races and atomicity. In Symposium on Principles and \nPractice of Parallel Programming, pages 83 94, 2005. [36] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, \nand T. E. Anderson. Eraser: A dynamic data race detector for multi-threaded programs. ACM Transactions \non Computer Systems, 15(4):391 411, 1997. [37] E. Schonberg. On-the-.y detection of access anomalies. \nIn Conference on Programming Language Design and Implementation, pages 285 297, 1989. [38] N. Shavit \nand D. Touitou. Software transactional memory. In Symposium on Principles of Distributed Computing, pages \n204 213, 1995. [39] Standard Performance Evaluation Corporation. SPEC benchmarks. http://www.spec.org/, \n2003. [40] M. Vaziri, F. Tip, and J. Dolby. Associating synchronization constraints with data in an object-oriented \nlanguage. In Symposium on Principles of Programming Languages, pages 334 345, 2006. [41] C. von Praun \nand T. Gross. Static con.ict analysis for multi-threaded object-oriented programs. In Conference on Programming \nLanguage Design and Implementation, pages 115 128, 2003. [42] L. Wang and S. D. Stoller. Static analysis \nof atomicity for programs with non-blocking synchronization. In Symposium on Principles and Practice \nof Parallel Programming, pages 61 71, 2005. [43] L. Wang and S. D. Stoller. Accurate and ef.cient runtime \ndetection of atomicity errors in concurrent programs. In Symposium on Principles and Practice of Parallel \nProgramming, pages 137 146, 2006. [44] L. Wang and S. D. Stoller. Runtime analysis of atomicity for multi\u00adthreaded \nprograms. IEEE Trans. Soft. Eng., 32:93 110, Feb. 2006. [45] World Wide Web Consortium. Jigsaw. http://www.w3c.org, \n2001. [46] M. Xu, R. Bod\u00b4ik, and M. D. Hill. A serializability violation detector for shared-memory server \nprograms. In Conference on Programming Language Design and Implementation, pages 1 14, 2005. [47] Y. \nYu, T. Rodeheffer, and W. Chen. Racetrack: ef.cient detection of data race conditions via adaptive tracking. \nIn Symposium on Operating System Principles, pages 221 234, 2005. \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Atomicity is a fundamental correctness property in multithreaded programs, both because atomic code blocks are amenable to sequential reasoning (which significantly simplifies correctness arguments), and because atomicity violations often reveal defects in a program's synchronization structure. Unfortunately, all atomicity analyses developed to date are incomplete in that they may yield false alarms on correctly synchronized programs, which limits their usefulness.</p> <p>We present the first dynamic analysis for atomicity that is both sound and complete. The analysis reasons about the exact dependencies between operations in the observed trace of the target program, and it reports error messages if and only if the observed trace is not conflict-serializable. Despite this significant increase in precision, the performance and coverage of our analysis is competitive with earlier incomplete dynamic analyses for atomicity.</p>", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "University of California at Santa Cruz, Santa Cruz, CA, USA", "person_id": "P1022804", "email_address": "", "orcid_id": ""}, {"name": "Stephen N. Freund", "author_profile_id": "81100165065", "affiliation": "Williams College, Williamstown, MA, USA", "person_id": "P1022805", "email_address": "", "orcid_id": ""}, {"name": "Jaeheon Yi", "author_profile_id": "81351599511", "affiliation": "University of California at Santa Cruz, Santa Cruz, CA, USA", "person_id": "P1022806", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375618", "year": "2008", "article_id": "1375618", "conference": "PLDI", "title": "Velodrome: a sound and complete dynamic atomicity checker for multithreaded programs", "url": "http://dl.acm.org/citation.cfm?id=1375618"}