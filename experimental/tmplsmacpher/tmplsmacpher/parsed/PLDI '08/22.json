{"article_publication_date": "06-07-2008", "fulltext": "\n Bootstrapping: A Technique for Scalable Flow and Context-Sensitive Pointer Alias Analysis Vineet Kahlon \nNEC Labs America, Princeton, NJ 08540, USA. vineetkahlon@gmail.com Abstract We propose a framework for \nimproving both the scalability as wellas the accuracy of pointer alias analysis, irrespective of its \n.ow orcontext-sensitivities, by leveraging a three-pronged strategy that ef\u00adfectively combines divide \nand conquer, parallelization and function summarization. A key step in our approach is to .rst identify \nsmallsubsets of pointers such that the problem of computing aliases ofany pointer can be reduced to computing \nthem in these small sub\u00adsets instead of the entire program. In order to identify these subsets,we .rst \napply a series of increasingly accurate but highly scalable(context and .ow-insensitive) alias analyses \nin a cascaded fashionsuch that each analysis Ai works on the subsets generated by the previous one Ai-1. \nRestricting the application of Ai to subsets gen\u00aderated by Ai-1, instead of the entire program, improves \nit scalabil\u00adity, i.e., Ai is bootstrapped by Ai-1 . Once these small subsets have been computed, in order \nto make our overall analysis accurate, weemploy our new summarization-based .ow and context-sensitivealias \nanalysis. The small size of each subset offsets the higher com\u00adputational complexity of the context-sensitive \nanalysis. An impor\u00adtant feature of our framework is that the analysis for each of thesubsets can be carried \nout independently of others thereby allow\u00ading us to leverage parallelization further improving scalability. \nCategories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors-Optimization General Terms \nAlgorithms, Languages, Performance Keywords Divide and Conquer, Steensgaard partitioning, sum\u00admarization, \ncontext-sensitive analysis, demand-driven analysis 1. Introduction Static analysis has recently emerged \nas a powerful technique fordetecting potential bugs in large-scale real-life programs. To be ef\u00adfective, \nsuch static analyses must satisfy two key con.icting criteria-accuracy and scalability. Since static \nanalyses typically work on heavily abstracted versions of the given program, they may poten\u00adtially generate \nmany false positives. The key challenge, therefore,is to reduce the number of false positives while keeping \nthe analysisscalable. However the accuracy and scalability of most static errordetection methods strongly \nhinge on the precision and ef.ciency Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 08, June 7 13, 2008, Tucson, Arizona, USA. Copyright c . 2008 ACM \n978-1-59593-860-2/08/06. . . $5.00 of the underlying pointer analysis, especially for C programs. Thismakes \nan accurate as well as scalable pointer analysis indispens\u00adable for such applications. We propose a framework \nfor improving both the scalability aswell as the accuracy of pointer alias analysis, irrespective of \nits.ow or context-sensitivities, by leveraging a combination of divide and conquer, parallelization and \nfunction summarization. The keystrategy underlying our analysis is to .rst use an ef.cient and scal\u00adable \nanalysis to identify small subsets of pointers, called clusters, that have the crucial property that \nthe computation of the aliasesof a pointer in a program can be reduced to the computation of itsaliases \nin each of the small clusters in which it appears. This, in ef\u00adfect, decomposes the pointer analysis \nproblem into much smallersub-problems where instead of carrying out the pointer analysisfor all the pointers \nin the program, it suf.ces to carry out sep\u00adarate pointer analyses for each small cluster. Furthermore, \ngivena cluster, only statements that could potentially modify aliases ofpointers in that cluster need \nbe considered. Thus each cluster in\u00adduces a (usually small) subset of statements of the given programto \nwhich pointer analysis can be restricted thereby greatly enhanc\u00ading its scalability. Once this partitioning \nhas been accomplished, ahighly accurate pointer analysis can then be leveraged. The smallsize of each \ncluster then offsets the higher computational complex\u00adity of this more precise analysis. In order to \nidentify the clusters, we apply a series of increas\u00adingly accurate (but less scalable) alias analyses \nin a cascaded fash\u00adion such that each analysis Ai works on the pointer subsets gen\u00aderated by the previous \none Ai-1 and not on the entire program. Restricting the application of Ai to subsets generated by Ai-1, \nin\u00adstead of the entire program, improves its scalability. In other words, Ai is bootstrapped by Ai-1. \nOnce these small clusters have been computed, in order to make our overall analysis accurate, we em\u00adploy \nour new summarization-based .ow and context-sensitive aliasanalysis. We start the bootstrapping by applying \nthe highly scalableSteensgaard s analysis (Steensgaard 1996) to identify clusters aspoints-to sets de.ned \nby the (Steensgaard) points-to graph. SinceSteensgaard s analysis is bidirectional, it turns out that \nthese clus\u00adters are, in fact, equivalence classes of pointers and so the resultingclusters are referred \nto as Steensgaard Partitions. In case there exist Steensgaard partitions whose cardinality is too large \nfor a context\u00adsensitive alias analysis to be viable (as determined by a thresholdsize), Andersen s analysis \n(Andersen 1994) is then performed sepa\u00adrately on these large partitions. Thus whereas Andersen s analysis,which \nis more accurate than Steensgaard s, might have been lessscalable on the original program, leveraging \nSteensgaard s anal\u00adysis to .rst partition the set of pointers in the program improvesit scalability, \nviz., Steensgaard s analysis bootstraps Andersen sanalysis. Indeed, the maximum Steensgaard partition \nsize that weencountered in our benchmark suite was 596 for the sendmail pro\u00adgram that had a total of \n65134 pointers thus clearly demonstrat\u00ading the reduction in the scale of the problem. Note that Ander\u00adsen \ns points-to analysis, being unidirectional, is more precise thanSteensgaard s which is bidirectional. \nHence it produces smaller clusters than Steensgaard s analysis. For instance, the Steensgaardpartition \nof size 596 in the sendmail example was broken up intoseveral Andersen clusters, the maximum size of \nwhich was 193. Usually, Andersen clusters are small enough so that our summary\u00adbased .ow and context-sensitive \npointer analysis becomes viable. Cluster Size Frequencies for autofs: Steensgaard vs. Andersen Cluster \nSize Figure 1. Cluster Size Frequencies : Steensgaard vs. Andersen Figure 1 shows that the plot of frequency \nof each cluster size inthe Linux Driver autofs which is typical across all the examples thatwe considered. \nThe white squares represent Steensgaard partitionswhereas black squares represent Andersen clusters. \nNote (i) thehigh density of both white and black squares for low values of clus\u00adter size, (ii) the stark \ndifference in maximum size of Steensgaardpartitions (isolated white square to the far right) and Andersen \nclus\u00adters. It shows that by simply generating Steensgaard/Andersen clus\u00adters, we can localize the aliasing \nproblem to only the relevant point\u00aders making it possible to ef.ciently compute precise aliases forthem. \nMore generally, bootstrapping allows one to string together aseries of pointer analyses of increasing \naccuracy till the subsets aresmall enough to ensure scalability of a highly precise alias analysis,context-sensitive \nor not. This not only ensures scalability but alsoimproves the accuracy of the overall analysis. Furthermore, \nsincethese clusters can be analyzed independently of each other, it givesus the ability to leverage parallelization \nwhich is important giventhe increasing prevalence of multi-core processors. Another key contribution \nof this paper is a new summarization\u00adbased approach for .ow and context-sensitive alias analysis. Oneof \nthe key bottlenecks in context-sensitive alias analysis is that thenumber of contexts grows exponentially \nwith the number of func\u00adtions in the given program. Typically, large-scale C programs tendto have a large \nnumber of small functions which can easily causethe number of contexts to blow up and overwhelm the analysis. \nFunction summarization offers an effective solution for han\u00addling this blow-up in the number of contexts. \nIndeed, summariza\u00adtion is, by its very nature, compositional thus making it robustlyscalable to large \ncode. A summarization-based technique relyingon the building of partial transfer functions has been explored \nbe\u00adfore for pointer analysis (Wilson and Lam 1995). The key ideathere was to capture all the different \nways in which the points-to relation between the parameters of a function can be modi.ed byexecuting \nthe function body. Our summarization technique is dif\u00adferent and relies on capturing sequences of assignments \nto point\u00aders that are relevant for aliasing. Summarization of assignment se\u00adquences, which are simpler \nobjects than points-to graphs, can beaccomplished more compactly. In order to compute summaries fora \ngiven Andersen cluster A, we .rst use a data.ow analysis to iden\u00adtify the pointers VA and the set StA \nof statements modifying thesepointers that may affect aliases of pointers in A. Next, the sum\u00admary tuples \nare computed by analyzing pointers in VA. Recursion is handled by processing the strongly connected components \nof thefunction call graph in reverse topological order. For each stronglyconnected component, we use \na .xpoint algorithm to compute sum\u00admary tuples for each function in that component. The aliases of apointer \nin a given context can then be computed by splicing to\u00adgether local assignment sequences as captured \nby the summary tu\u00adples for all the functions in the order in which they appear in thegiven context. Tracking \nsequences of assignments locally withineach function and splicing them together across functions in thegiven \ncontext makes our analysis .ow-sensitive not just locally in\u00adside a function, as was the case in (Whaley \nand Lam 2004), but forthe entire program. A crucial point is that bootstrapping allows us to exploit \nlocalityof reference. Indeed, since Andersen s clusters are typically small,by restricting summary computation \nto each individual cluster en\u00adsures that the resulting summaries will also be small. Secondly, thenumber \nof statements modifying values of pointers in a given clus\u00adter also tend to be few and highly localized \nto a few functions. Thisin turn, obviates the need for computing summaries for functionsthat don t modify \nany pointers in the given cluster which typicallyaccounts for majority of the functions. Note that without \nclusteringit would be hard to ensure viability of the summarization approach.Thus it is the synergy between \ndivide and conquer and summariza\u00adtion (and parallelization) that ensures scalability of our approach. \nAnother advantage of bootstrapping is .exibility as it gives usthe ability to pick and choose which clusters \nto explore. Indeed,based on the application, we may not be interested in accuratealiases for all pointers \nin the program but only a small subset. Asan example, for lockset computation used in data race detection,we \nneed to compute must-aliases only for lock pointers. Thus weneed to consider only clusters having at \nleast one lock pointer. Infact, as is to be expected, since a lock pointer can alias only toanother lock \npointer, we need to consider clusters comprised solelyof lock pointers. This makes our analysis extremely \n.exible as itcan be adapted on-the-.y based on the demands on the application.Moreover one may choose \nto engage different pointer analysismethods to analyze different clusters based on their sizes and accessdensities \nresulting in a hybrid approach. Thus bootstrapping makesour framework .exible which can adapted according \nto the needsof the target application. Related Work. Most scalable pointer alias analyses for C pro\u00adgrams \nhave been context or .ow-insensitive. Steensgaard (Steens\u00adgaard 1996) was the .rst to propose a uni.cation \nbased highly scal\u00adable .ow and context-insensitive pointer analysis. The uni.cationbased approach was \nlater extended to give a more accurate one\u00ad.ow analysis that has one-level of inclusion constraints (Das \n2000;Das et al. 2001) and bridges the precision gulf between Steens\u00adgaard s and Andersen s analysis (Andersen \n1994). Inclusion-basedalgorithms have been explored to push the scalability limits ofalias analysis (Berndl \net al. 2003; Hardekof and Lin 2007; Heintzeand Tardieu 2001; Lhot\u00b4ak and Hendren 2003; Whaley and Lam2002). \nFor many applications where .ow-sensitivity is not impor\u00adtant, context-sensitive but .ow-insensitive \nalias analyses have beenexplored (F\u00a8ahndrich et al. 2000; Foster et al. 2000). The idea of partitioning \nthe set of pointers in the given programinto clusters and performing an alias analysis separately on \neachindividual cluster has been explored before (Zhang et al. 1996).However, the clustering in (Zhang \net al. 1996) was based on treat\u00ading pointers, references or dereferences thereof, purely as syntacticobjects \nand by computing a transitive closure over them with re\u00adspect to the equality relation. A clustering \nbased on Steensgaard sanalysis takes into account not just assignments between pointers(at the same level \nin Steensgaard s hierarchy) but also points-to re\u00adlation between objects (at different levels in the \nhierarchy). As a re\u00adsult, Steensgaard partitions are much more re.ned, i.e., smaller insize than the \nones based on purely syntactic criteria. Furthermore,cascading of several analyses for increasing precision \nvia clusterre.nement has, to the best of our knowledge, not been consideredbefore. There is also substantial \nprior work on .ow and context\u00adsensitive alias analysis. An early attempt, shown to handle up to4KLOC, \nwas proposed in (Ryder et al. 2001). The use of partialtransfer functions for summarization (Wilson and \nLam 1995) dis\u00adcussed before has been shown to handle code up to 20KLOC ofC code. A method for computing \naliasing information for stackallocated data structures was presented in (Emami et al. 1994). Acompositional \nand interprocedural pointer analysis for Java pro\u00adgrams was given in (Whaley and Rinard 1999). By using \nsym\u00adbolic data structures like BDDs to represent summaries encoded interms of transfer functions, context \nand .ow-sensitive alias anal\u00adysis has been shown to scale up to 25KLOC (Zhu and Calman2005). The use \nof BDDs for pointer analysis was pioneered by Zhu(Zhu 2002; Zhu and Calman 2005). In related work, it \nhas beenshown that BDDs can be used for ef.ciently computing context\u00adsensitive inclusion-based points-to \nsets for Java programs (Berndlet al. 2003). More recently, the use of BDDs to represents relationsin \nDatalog (Whaley et al. 2004) has been proposed. Represent\u00ading pointer analysis as a logic programming \nproblem allows it beformulated as a sets of datalog rules which can then be used tocompute BDDs for a \ncontext-sensitive alias analysis (Whaley andLam 2004) with limited .ow sensitivity. This approach has \nbeenshown to be successful for Java but less so for C programs. A .owand context-sensitive alias analysis \nwith (limited) path sensitivitybut one which requires user annotations, and is therefore semi\u00adautomatic, \nwas presented in (Hackett and Aiken 2006). In contrast,our analysis is fully automatic. To sum up, our \nkey new contributions are a framework forscalable .ow and context-sensitive pointer alias analysis that \n ensures scalability as well as accuracy by applying a series ofanalysis in a cascaded fashion  is .exible \n is fully automatic  provides a new summarization technique that is more succinctthan existing ones. \n  2. Bootstrapping We start by .xing some notation. For a given program P rog, let P denote the set \nof all pointers of P rog. Then for Q . P , we use StQ to denote the set of statements of Prog executing \nwhich may affect the aliases of some pointer in Q. Furthermore, for q . Q, Alias(q, StQ) denotes the \nset of aliases of q in a program P rogQ resulting from Prog where each assignment statement not in StQ \nis replaced by a skip statement and all conditional statements of P rog are treated as evaluating to \ntrue. In other words, all statements of Prog other than those in StQ are ignored in P rogQ. Our main \ngoal in this section is to show how to compute subsets P1, ..., Pm of P such that S (i) P = i Pi S (ii) \nFor each p . P , Alias(p, StP )= i Alias(p, StPi ) (iii) The maximum cardinality of Pi (and of StPi ) \nover all i is small. This is required in order to ensure scalability in computingthe sets Alias(p, StPi \n). Note that goal (ii) allows us to decompose computation ofaliases for each pointer p . P in the given \nprogram to computing aliases of p with respect to each of the subsets Pi in the program P rogPi . This \nenables us to leverage divide and conquer. However,in order to accomplish this decomposition care must \nbe taken inconstructing the sets P1 , ..., Pn which need to be de.ned in a way so as not to miss any \naliases. We refer to sets P1, ..., Pm satisfying conditions (i) and (ii) above as a Disjunctive Alias \nCover. Furthermore, if the sets P1,...,Pm are all pairwise disjoint then they are referred to as a Disjoint \nAlias Cover. Remark 1. As is usual (see, for example, (Das 2000)), we as\u00adsume, for the sake of simplicity, \nthat each pointer assignment inthe given program is of one of the following four types (i) x = y, (ii) \nx =&#38;y, (iii) ~x = y, and (iv) x = ~y. These cases capturethe main issues in pointer alias analysis. \nThe general case can behandled with minor modi.cations to our analysis. Recursion is al\u00adlowed. Heaps \nare handled by representing a memory allocation atprogram location loc by a statement of the form p =&#38;allocloc. \nA memory deallocation is replaced by a statement of the form p = NULL. We .atten all structures by replacing \nthem with col\u00adlections of separate variables -one for each .eld. This converts all accesses to .elds \nof structures into regular assignments betweensuch variables. While this was required in our framework \nfor modelchecking programs, an important side bene.t is that it makes ourpointer analysis .eld sensitive. \nPointer arithmetic is, for now, han\u00addled in a naive manner, by aliasing all pointer operands with theresulting \npointer. Function pointers are handled as in (Emami et al.1994). Remark 2. In the interest of brevity, \nwe touch on (the now standard) Steensgaard s analysis and associated terminology likepoints-to relations, \nSteensgaard points-to graph, etc., only brie.ywithout providing a formal description which can be found \nin(Steensgaard 1996). 2.1 Steensgaard Partitioning In Steensgaard s analysis (Steensgaard 1996), aliasing \ninformationis maintained as a relation over abstract memory locations. Everylocation l is associated \nwith a label or set of symbols . and holds some content a which is an abstract pointer value. Points-to \nin\u00adformation between abstract pointers is stored as a points-to graphwhich is a directed graph whose \nnodes represent sets of objects andedges encode the points-to relation between them. Intuitively, anedge \ne : v1 . v2 from nodes v1 to v2 represents the fact that a symbol in v1 may point to some symbol in the \nset represented by v2 . The effect of an assignment from pointers y to x is to equate the contents of \nthe location associated with y to x. This is car\u00adried out via uni.cation of the locations pointed-to \nby y and x into one unique location and if necessary propagating the uni.cation totheir successors in \nthe points-to graph. Assignments involving ref\u00aderencing or dereferencing of pointers are handled similarly. \nSinceSteensgaard s analysis does not take the directionality of assign\u00adments into account, it is bidirectional. \nThis makes it less precise buthighly scalable. Figure 2 shows the Steensgaard points-to graph fora small \nexample. The Steensgaard Points-to Hierarchy. The key feature of Steens\u00adgaard s analysis that we are \ninterested in is the well known fact thatthe points-to sets so generated are equivalence classes. Hence \nthese main(){ 1a: p=&#38;a; 2a: q=&#38;b; 3a: r=&#38;c; 4a: q=p; 5a: q=r; } Andersen Points-to Graph \nSteensgaard Points-to Graph Figure 2. Steensgaard vs. Andersen Points-to Graphs sets de.ne a partitioning \nof the set of all pointers in the programinto disjoint subsets that respect the aliasing relation, i.e., \na pointercan only be aliased to pointers within its own partition. We shallhenceforth refer to each equivalence \nclass of pointers generated bySteensgaard s analysis as a Steensgaard Partition. For pointer p, let np \ndenote the node in the Steensgaard points-to graph represent\u00ading the Steensgaard partition containing \np. A Steensgaard points-tograph de.nes an ordering on the pointers in P which we refer to as the Steensgaard \npoints-to hierarchy. For pointers p, q . Q, we say that p is higher than q in the Steensgaard points-to \nhierarchy, de\u00adnoted by p>q, or equivalently by q<p, if np and nq are distinct nodes and there is a path \nfrom np to nq in the Steensgaard points-to graph. Also, we write p = q to mean that p and q both belong \nto the same Steensgaard partition. The Steensgaard depth of a pointer p is the length of the longest \npath in the Steensgaard points-to graphleading to node np. That the notion of Steensgaard depth is well\u00adde.ned \nfollows from the fact that a Steensgaard points-to graph isa forest of directed acyclic graphs (see comment \nbelow). Important Remark. The Steensgaard points-to graph should notbe confused with graph of the points-to \nrelation. The graph ofthe points-to relation can contain cycles. However, a Steensgaardpoints-to graph \nwhich is over sets (equivalence classes) of pointersand not individual pointers is always acyclic. Consider \nthe assign\u00adment ~p = p which creates a loop in the graph of the points-torelation. Since both ~p and \np belong to the same Steensgaard equiv\u00adalence class (p =~p), they will be represented by the same nodein \nthe Steensgaard points-to graph. Since the Steensgaard points\u00adto graph only has edges between different \nnodes, we can deducethat it will be acyclic for the above statement. This ensures thatthe < relation \nintroduced above is well-de.ned. Note that such cy\u00adcles in the points-to graph can arise in common situations \ninvolvingcyclic data structures, void pointers, etc. We therefore distinguishbetween the points-to hierarchy \nand the points-to relation. Hence\u00adforth, whenever we use the term points-to hierarchy we mean theSteensgaard \npoints-to hierarchy. Divide and Conquer. When computing the aliases of pointers in agiven Steensgaard \npartition P , we want to restrict our analysis only to those statements of P rog that may affect aliases \nof pointers in P . A standard, but important, observation is that aliases of a pointer in P can be affected \nonly by assignments to either a pointer in P or a pointer q higher in the Steensgaard points-to hierarchy \nthan some pointer in P . Assume now that our goal is to compute Andersenaliases of a pointer p . P . \nThen it suf.ces to restrict our analysisonly to statements that directly modify values of pointers in \nthe set main(){ int a, b; int *x; 1a: x= &#38;a; 2a: y= &#38;b; 3a: p= x; 4a: *x =*y; } Figure 3. Identifying \nrelevant statements P> comprised of all pointers q such that either q>p or q = p. However, we show that \nwe can do more effective slicing. Consider the program P rog shown in .g. 3. The Steensgaard partitions \nfor program P rog are {a, b}, {y} and {p, x}. Supposethat we are interested in the aliases of pointers \nin the partition P = {a, b}. Note that since x is (one level) higher in the points-to hierarchy than \nboth a and b, any modi.cation to ~x or x may also affect aliases of a or b. Thus any assignment to ~x \nor x needs to be taken into consideration while performing the alias analysis for a or b. Hence we need \nto add 4a and 1a to StP , the set of statements of P rog that need be considered while computing aliases \nof pointers in P . Note that 1a affects aliases of a due to the fact that x points-to a. We also need \nto take into account assignments between pointersat the same depth in the points-to hierarchy. Indeed, \nfrom statement 4a we can deduce that the value of ~x depends on the value of ~y and so any statement \nthat modi.es ~y or y (as in this program y> ~y) should also be taken into account thereby including 2a \ninto StP . Then it is easily seen that no more statements need be added to StP . Observe that if we had \nsimply taken StP to be the set of all statements modifying any pointer in P>, we would also have included \nstatement 3a which does not affect the aliases of a or b. In a large program, the effect of discarding \nsuch statements canbe quite signi.cant.  The above example points to a simple .xpoint computation(algorithm \n1) that, given a Steensgaard partition P , can identify the set of statements StP that may affect aliases \nof pointers in P . The procedure .rst computes the set of variables (or references ordereferences thereof) \nVP which may affect aliases of pointers in P . We start by initializing VP to P (line 2). Next we iterativelycompute \nnew pointers that can affect aliases of pointers in VP . The values of a pointer p . VP can be modi.ed \neither via a direct assignment to p (line 5) or via an assignment to a dereferencing of a pointer q where \neither q>p or (the case of a cyclic points-to relation) q, ~q and p all occur in the same Steensgaard \npartition(line 8). When all such pointers have been discovered, we simplyreturn the set of all statements \nthat modify a pointer in VP and hence the possible aliases of a pointer in P . Soundness. The key step \nis showing that given a Steensgaard partition P , restricting Andersen s analysis to statements in StP \ndoes not result in the loss of any aliases. Towards that end, we .rst give a necessary and suf.cient \ncondition as to when two pointersare aliased to each other. We start with some de.nitions. A pointer \np is said to be semantically equivalent to q at location l if p and q have the same value at l (even \nif they are syntactically different). De.nition 3 (Complete Update Sequence). Let A : l0, ..., lm be \na sequence of successive program locations and let s be the sequence li1 : p1 = a0, li2 : p2 = a1,..., \nlik : pk = ak-1, of pointer assignments occurring along A. Then s is called a complete update sequence \nfrom p to q leading from locations l0 to lm iff a0 and pk are semantically equivalent to p and q at locations \nl0 and lm, respectively. Algorithm 1 Computing Relevant Statements 1: Input: Program P rog, Steensgaard \nPartition: P . 2: Initialize VP to P . 3: repeat Vj 4: P = 0. 5: if there exists a pointer q such that \nthere exists a statement of the form p = q, where p . VP and q .. VP then 6: add q to Vj P . 7: end if \n8: if there exists a pointer q such that either (i) q>p, or (ii) q =~q and ~q = p (cyclic case), where \np . VP , and there exists a statement of the form ~q = r with r .. Vp then 9: add ~q, q, r to VPj. 10: \nend if 11: VP = VP u VPj. 12: until VPj= 0 13: return the set of all statements of P rog of the form \np = q, where p . VP . for each j, aj is semantically equivalent to pj at lij ,  for each j, there does \nnot exist any (semantic) assignment to pointer aj between locations lij and lij+1 ; to a0 between l0 \nand li1 ; and to pk between lik and lm along A.  A related concept is that of maximally complete update \nsequences. De.nition 4 (Maximally Complete Update Sequence). Given a sequence A : l0, ..., lm of successive \ncontrol locations starting at the entry control location l0 of the given program, the maximally complete \nupdate sequence for pointer q leading from locations l0 to lm along A is the complete update sequence \ns of maximum length, over all pointers p, from p to q (leading from locations l0 to lm) occurring along \nA. If s is an update sequence from p to q leading from locations l0 to lm, we also call it a maximally \ncomplete update sequence from p to q leading from locations l0 to lm. Typically, l0 and lm (see above \nde.nitions) are clear from the con\u00adtext. Then we simply refer to s as a complete or maximally com\u00adplete \nupdate sequence from p to q. As an example, consider the pro\u00adgram in .gure 4. The sequence 4a is a complete \nupdate sequence from b to a leading from 1a to 4a, but not a maximally complete one. It can be seen that \n1a, 4a is a maximal completion of 4a. Note that at location 4a, ~x is not syntactically but semantically \nequal to a due to the assignment at location 2a. Maximally com\u00adplete update sequences can be used to \ncharacterize aliasing. Theorem 5. Pointers p and q are aliased at control location l iff there exists \na sequence A of successive control locations starting at the entry location l0 of the given program and \nending at l such that there exists a pointer a with the property that there exist maximally complete \nupdate sequences from a to both p and q (leading from l0 to l) along A. Proof. (*) Obvious. (.) Suppose \nthat p and q are aliased at control location l. Then there exists an execution 0 of the given program \nleading to a statein which the program is at control location l and p and q are both pointing to the \nsame object. Let A : l0, ..., lm be the sequence of control locations occurring along 0, where l0 is \nthe entry location of P and lm = l. Let s1 and s2 be the maximally complete update sequences to p and \nq leading from l0 to l, respectively, along A. Let pi1 = pi0 be the .rst assignment in sequence si and \nsuppose that it occurs at location li1 along A. Then since p and q are aliased, we claim that pointers \np10 and p20 are the same. Indeed, since p and main(){ int *a, *b, *c; int **x, **y; 1a: b =c; 2a: x =&#38;a; \n3a: y =&#38;b; 4a: *x =b; } Figure 4. Complete vs. Maximally Complete Update Sequences q are aliased \nto each other, p10 and p20 point to the same object at locations, l11 and l21 , respectively. If these \npointers are different,then the only way they can be made to point to the same locationis via an assignment \nto at least one of them. Thus there must beeither an assignment to p10 before location l11 or to p20 \nbefore l21 . But this implies that at least one of s1 or s2 is not a maximallycomplete update sequence, \nthus contradicting our assumption. We can now get back to our original goal of showing that givena Steensgaard \npartition P , restricting pointer analysis to program statements in StP does not cause us to miss any \naliases. Theorem 6. Given a set of pointers Q belonging to the same Steensgaard partition and pointer \np . Q, Alias(p, StP )= Alias(p, StQ), where P is the set of pointers of the given pro\u00adgram. Proof Sketch \n(\u00d8) Obvious. (.) The proof is by induction on the Steensgaard depth ofpointers in Q (all pointers in \nthe same Steensgaard partition havethe same depth). For every q . Alias(p, StP ), we show that q . Alias(p, \nStQ). By theorem 5, p and q are aliased at location l iff there exists a sequence A of successive locations \nstarting at the entry location l0 of the given program and ending at l such that there exists a pointer \na with maximally complete update sequences s1 and s2 from a to p and from a to q (leading from l0 to \nl),respectively, along A. Let s1 be the sequence li1 : p1 = a, li2 : p2 = a1,..., lik : pk = ak-1, and \ns2 the sequence lj1 : q1 = a, lj2 : q2 = b1 ,..., ljm : qm = bm-1 . Base Case: Assume .rst that all pointers \nin Q are at depth zero. Weshow that each of the statements occurring along s1 and s2 also occur in StQ. \nWe start with the last statement in s1. In pk = ak-1 , pointer pk is either p or of the form ~r where \nr points to p at lik . In the .rst case, the statement at lik would have been added to StQ via steps \n5-6 of algorithm 1. In the second case, since pointers in Q have Steensgaard depth zero, we have that \nr and ~r both belong to Q (the cyclic case). Then statement pk = ak-1 would be added to StQ via steps \n8-9 of algorithm 1. Similarly, we can, by traversing s1 (s2 ) backwards, show that all statements occurring \nalong s1 (s2) also occur in StQ. This completes the base step. Induction Step: The induction step is \nsimilar except that whenever we encounter a statement of the form pk = ak-1, there are now three cases \nto consider based on whether pk is of the form (i) p, (ii) ~r, where r . Q, or (iii) ~r, where r>p. The \n.rst two cases are handled as above. The third case is handled by using the inductionhypothesis. Computing \nAndersen Covers Next, we show how a large Steens\u00adgaard partition can be broken up into yet smaller sets \nthat form adisjunctive alias cover for the Steensgaard partition. Unlike Steens\u00adgaard s analysis, the \npoints-to sets generated by Andersen s anal\u00adysis are not equivalence classes. An example is shown in \n.gure 2. Here the node representing the set {q} has out-degree three int **x, **u, **u, **w, **z; foo(){ \nint *a, *b, *c; 1b: ~x = d; main(){ 2b: a = b; 1a: x =&#38;c; 3b: x = w; 2a: w = u; } 3a: foo(); 4a: \nz = x; bar(){ 5a: ~z = b; 1c: ~x = d; 6a: bar(); 2c: a = b; }} Figure 5. An Example Program whereas \nin the Steensgaard points-to graph for the same example,each node has out-degree at most one. An Andersen \npoints-to setis de.ned to be a set of pointers pointing to the same object inthe Andersen points-to graph. \nSince a pointer can appear in morethan one Andersen points-to sets, these sets do not form equiva\u00adlence \nclasses. Thus we refer to them as Andersen Clusters instead of partitions. However, the next result shows \nthat they do form aDisjunctive Alias Cover. Theorem 7 Let pointer p belong to the Andersen points-to \nsets S A1, ..., Am. Then Alias(p, StP )= i Alias(p, StAi ), where P is the set of pointers of the given \nprogram. One potential drawback of Andersen clustering is that since theclusters are not disjoint they \ncan, in some cases, have considerableoverlap with each other. Thus a single Steensgaard partition canproduce \na large number of Andersen clusters forming a cover. Thepractical implication is that although the maximum \ntime taken toprocess each cluster decreases, the total time taken to process allclusters may actually \nincrease. A solution to this problem is toidentify an Andersen Threshold such that Andersen clustering \nisperformed only on Steensgaard partitions larger in cardinality thanthis threshold. This threshold can \nbe determined empirically. For our benchmark suite it turned out to be 60.   3. Scalable Context Sensitive \nAlias Analysis Using Steensgaard partitioning and Andersen clustering, once the pointer aliasing problem \nhas been reduced from the set of all point\u00aders in the given program to a small subset, we can effectivelyemploy \nprocedure summarization for scalable .ow and context\u00adsensitive pointer alias analysis. Indeed, since \nmost Andersen clus\u00adters are small, the density of access for pointers belonging to a givencluster is \ntypically low. An important implication is that summariesfor a given cluster are usually small in size \nor even empty for mostfunctions and can therefore be computed ef.ciently. We emphasizethat it is clustering \nthat allows us to leverage locality of reference.Indeed, without the clustering-induced decomposition, \nwe wouldhave to compute summaries for each function with a pointer access,viz., practically every function \nin the given program. Additionally,for each function we would need to compute the summary for allpointers \nmodi.ed in that function, not merely the pointers belong\u00ading to the cluster being currently processed, \nwhich could greatly af\u00adfect the scalability of the analysis. Thus our technique exploits thesynergy that \nresults by combining divide and conquer with summa\u00adrization. Procedure Summaries for Context-Sensitive \nMay-Alias Analy\u00adsis. We propose a new summarization-based technique for context\u00adsensitive pointer alias \nanalysis. Given a context, viz., a sequence offunction calls, con = f1 ...fn , by theorem 5 we have that \npointers p and q are aliased at location l in fn iff there exists a sequence A of successive control \nlocations in con starting at the entry loca\u00adtion l0 of the given program and leading to l such that there \nex\u00adists a pointer a with maximally complete update sequences from a to p and from a to q leading from \nl0 to l along A. Thus in order to compute .ow and context-sensitive pointer aliases it suf.ces tocompute \nfunctions summaries that allow us to construct maximallycomplete update sequences on demand. The key \nidea is for the sum\u00admary of a function f to encode local maximally complete update sequences in f starting \nfrom the entry location of f. Then the max\u00adimally complete update sequences in context con = f1...fn \ncan be constructed by splicing together the local maximally complete up\u00addate sequences for functions \nf1 , ..., fn in the order of occurrence. We motivate our notion of summaries with the help of anexample. \nConsider the program P rog shown in .gure 5. The Steensgaard partitions of P rog are P1 = {x, u, w, z} \nand P2 = {a, b, c, d}. In this case, the Steensgaard points-to graph for P rog has two nodes n1 and n2 \ncorresponding to P1 and P2 , respectively, with n1 pointing to n2. Consider the Steensgaard partition \nP1. Note that none of the statements of function bar can modify aliases of pointers in P1 . This can \nbe determined by checking that no statement of StP1 (computed via algorithm 1) occurs in bar. Thus for \npartition P1 , summaries need to be computed only for functions main and foo. Consider function foo. \nThe effect of executing foo on pointers in P1 is to assign w to x. Thus the local maximally complete \nup\u00addate sequence for x leading from the entry location 1b of foo to 3b is x = w which is represented \nvia the summary tuple (x, 3b, w, true). The last entry in the tuple encodes points-to con\u00adstraints that \nare explained later on. Note that with respect to eachof the locations 1b and 2b, the summaries of foo \nare empty as the aliases of none of the pointers in P1 can be modi.ed by executing foo up to and including \nlocation 2b. Now suppose that we want the maximally complete updatesequences for z leading from the entry \nlocation 1a of main to its exit location 6a. Since bar does not modify aliases of any pointer in P1, \nthe .rst statement encountered in traversing main backwards from its exit location that could affect \naliases of z is 4a. Since z is being assigned the value of x, we now start tracking x backwards instead \nof z. As we keep traversing backwards, we encounter a call to foo which has the already computed summary \ntuple (x, 3b, w, true) for its exit location 3b. Since we are currently tracking the pointer x and since \nwe know from the summary tuple that x takes it value from w, the effect of executing foo can be captured \nby replacing x with w in our backward traversal and jumping directly from the return site 3a of foo in \nmain to its call site 2a. Traversing further backwards from 2a, we encounter w = u at location 2a causing \nus to replace w with u. Since no more transitions modifying pointers of P1 are encountered in the backward \ntraversal, we see that w = u, [x = w],z = x is a maximally complete update sequence and so (z, 6a, u, \ntrue) is logged as a summary tuple for main. Here, x = w is shown in square brackets to indicate a summary \npair. Let us now consider the set of pointers P2. Suppose that we areinterested in tracking the maximally \ncomplete update sequences for a leading from 1c to 2c in bar. Tracking backwards we immedi\u00adately encounter \n2c causing a to be replaced with b. However, when we encounter statement ~x = d at location 1c, in order \nto propa\u00adgate the complete update sequence further backwards, we need toknow whether x points to b or \nnot at 1c. If it does then we prop\u00adagate d backwards else we need to propagate b. Note that what x points \nto cannot, in general, be determined for function bar in iso\u00adlation as it might depend on the context \nin which bar is called. We therefore generate the two tuples t1 =(a, 2c, d, 1c : x . b) and t2 =(a, 2c, \nb, 1c : x .. b) accordingly as x points to b or not at 1c, with the last entries in the tuples encoding \nthe points-to constraints. De.nition 8 (Summary). The summary for function f is the set of tuples (p, \nloc, q, c1 . ... . ck ) such that there is maximal complete update sequence from q to p starting at the \nentry location of f and leading to location loc of f under the points-to constraints imposed by c1, ..., \nck. Each constraint ci is of one of the following forms (i) l : r . s (r points-to s at l) (ii) l : r \n.. s (r does not point to s at l), (iii) l : r = s (r and s point-to the same object at l), or (iv) \nl : r .= s (r and s do not point-to the same object at l) respectively.  Top-Down Processing. Recall \nfrom our discussion in the previoussection that when computing summaries for a given Steensgaardpartition \nP , it suf.ces to restrict our analysis only to pointers in VP and statements in StP as computed by algorithm \n1. As seenabove, in processing a statement of the form ~x = y at program location l, we need to know \nbefore hand what x points to at l. One option is to consider all possible pointers x may point to in \ntheAndersen points-to graph. Since Andersen s analysis in both .owand context-insensitive, this may result \nin too many aliases whichin turn may generate too many summary tuples. Because of the.ow insensitivity \nof Andersen s analysis most of these summarytuples will likely be spurious. Thus we need to determine \nthe set ofpointers x may point to at location l accurately enough to keep the number of summary tuples \nsmall. Towards that end, we observe that for summarization we need to consider all possible aliases of \nx for each path leading to l irrespective of the context. Thus when computing summary tuples,we cannot \ncompute the objects that x may point to at location l in a context-sensitive fashion. To keep the number \nof summarytuples generated small, we, therefore, consider .ow-sensitive andcontext-insensitive (FSCI) \npoints-to sets of x. We show that the computation of these FSCI points-to sets can, in fact, be foldedseamlessly \ninto the summary computation for a given Andersencluster P . The key observation that makes this possible \nis that if ~x = y is a statement of P rog such that x>y, then in computing the summary tuples for P , \nif we encounter ~x = y, the points\u00adto sets for x need already have been computed beforehand. Aconsequence \nis that the summary computation for pointers in VP needs to be carried out in a top-down manner in increasing \norder ofSteensgaard depth. If, on the other hand, due to cycles in the points\u00adto relation ~x, x and y \noccur in the same Steensgaard partition, thenwe might need to track points-to constraints as given in \ndef. 8. Dovetailing. For top-down summary computation, we .rst identifythe Steensgaard partitions Vd11 \n, ..., Vdknk of VP , where Vdj i is the ith partition of VP occurring at depth dj in the Steensgaard \npoints\u00adto hierarchy, with 0= d1 < ... < dk. Next, we start computing summaries for these partitions of \nVP in non-decreasing order ofSteensgaard depth. In the sequel, depth refers to Steensgaard depthas de.ned \nin sec. 2.1. The key idea is to dovetail the computation ofthe summary tuples with the computation of \nthe FSCI aliases as isformalized in algorithm 2. We start by computing summary tuplesfor pointers of \nVP that have the least Steensgaard depth (line 2).Next for these pointers we compute the FSCI points-to \nsets (lines4-5) which, as observed before, can then be used in the computationof summary tuples for partitions \nof VP one depth lower in theSteensgaard points-to hierarchy (lines 7-8). The procedure iteratesover the \ndepth of the points-to hierarchy. Note that if in computing the summaries for a partition Vji of VP at \ndepth j, we encounter a statement of the form ~x = y, where x points to some pointer in Vji, then if \nx>y we would already have computed the FSCI points-to set of x which would then allow us to decide how \nto propagate the complete update sequence backwards, else we needto track points-to constraints as formulated \nin de.nition 8. Thusthe two key steps that we need are (i) the computation of the FSCI Algorithm 2 Dovetailing \nSummary Computation with Compu\u00adtation of FSCI-aliases 1: Input: Andersen Cluster P , pointers VP (as \ncomputed by alg. 1) and a Steensgaard partition Vd11, ..., Vd1n1 , ..., Vdknk of VP as de.ned above. \n2: Compute function summaries for partitions Vd1 1, ..., Vd1 n1 . 3: for i =1 to k - 1 do 4: for each \nj . [1..ni ] do 5: Compute FSCI points-to sets for pointers in Vdij 6: end for 7: for each l . [1..ni+1 \n] do 8: Compute summaries for Vdi+1l 9: end for 10: end for points-to sets for pointers of P at depth \nd given summaries for all pointers of P at depth d or less and FSCI aliases for pointers of P of depth \nless than d, and (ii) compute function summaries for pointers at depth d given FSCI points-to sets for \npointers of depth less than d. Computing Flow-Sensitive and Context-Insensitive Aliases. Let p . P , \nwhere P is a given Steensgaard partition. Assuming thatsummary tuples have been computed for all pointers \nof VP of depth less than or equal to that of p, and FSCI aliases have been computedfor all pointers of \ndepth less than that of p, we now show how to compute FSCI-aliases of p at location l of function f. \nNote that from theorem 5, it follows that two pointers p and q are FSCI\u00adaliased to each other at program \nlocation l if there exist paths inthe control .ow graph of the given program Prog from its entry location \nto l along which there exist maximally complete updatesequences from a pointer a to pointers p and q. \nIt follows that in order to compute the FSCI-aliases of p at location l, it suf.ces to .rst compute the \nset A of pointers such that for each a . A there is a maximally complete update sequence from a to p \nleading from the entry location of P rog to l. Then the set of aliases of p is simply the set Q of all \npointers q such that there is a maximally completeupdate sequence from a pointer in A to q leading from \nthe entry location of P rog to l. From the above discussion, it follows that we need two procedures: \none to compute the set A from p and the other to compute the set Q from A. Computation of A. Given a \nSteensgaard partition P at depth d and a program location l in function f, we show how to compute the \nset A of pointers such that there is a maximally complete updatesequence from a pointer in A to a pointer \nin P leading from the entry location of P rog to location l. We start with P and do a backward data.ow \nanalysis wherein we track the frontiers of the maximal update sequences, i.e., the set of pointers Fm \nsuch that there is a maximally complete update sequence from each pointerin Fm to a pointer in P leading \nfrom location m to l. We assume that summary tuples (which capture the local maximally completeupdate \nsequences from the entry location of each function) havealready been pre-computed for pointers of depth \nd or less. Then A can be computed simply by splicing together these local completeupdate sequences. We \nstart with the set P and .rst compute the set of pointers P ropagate such that there is a local maximally \ncomplete updatesequence from each pointer in P ropagate to a pointer in P startingat the entry location \nof f and leading to l, i.e., whether there is a tuple of the form (p, l, q, cond) . Sumf , where p . \nP and cond is satis.able (line 2). In order to test satis.ability of cond, we note that, by our construction, \ncond is built from conjuncts of the form (i) m : r . s (r points-to s at m) (ii) m : r .. s (r does not \npoint to s at m), (iii) m : r = s (r and s point-to the same object at m), or (iv) m : r .= s (r and \ns do not point-to the same object at m), where r and s have Steensgaard depth d or less. Then since summary \ntuples, and hence points-to sets, have already been computed for all pointers of Steensgaard depth d \nor less, the satis.ability of cond can be checked at the time of computing the frontier. If the aliases \nof a pointer p . P cannot be affected during any execution of f leading to location l, we need to retain \nall such pointers, forming the set Retain (line 3), unchanged in the frontier.The frontier at the entry \nlocation of f is Pf = P ropagate u Retain. We start the .xpoint computation by adding (f, Pf ) to Processing, \nthe set of tuples that are currently being processed (line4). A tuple belonging to Processing is of the \nform (h, Ph), where h is a function and set Ph is comprised of pointers r such that there is a maximally \ncomplete update sequence from r to a pointer p . P leading from the entry location of h to location l \nof f. Next, to propagate the frontier backwards, we consider all functions g that call h. For each call \nsite calli of h in g we consider, as gh in the starting step, the set P ropagatei of pointers that have \na gh maximally complete update sequence from the entry location of g to calli (line 10). As before, any \npointer of Ph for which there gh is no local maximally complete update sequence leading to calli gh is \nsimply propagated without modi.cation (line 11). Note that fora function g if we have propagated the \nfrontier for a pointer p backwards from the entry location of g once then it need not be done again. \nTo ensure that, we maintain a map from each function g to the set of pointers which have been propagated \nbackwards fromthe entry location of g. A pointer q is scheduled to be propagated backwards from the entry \nlocation of g only if it already hasn t been done previously (lines 13-16). Algorithm 3 Computing Flow-Sensitive \nContext-Insensitive Aliases 1: Input: Set P of pointers at Steensgaard depth d, location l of function \nf and the entry function entry function of P rog. 2: P ropagate = {q|p . P and for some satis.able condition \ncond, (p, l, q, cond) . Sumf } 3: Retain = {p|p . P and there does not exist a tuple of the form (p, \nl, q, cond) . Sumf , where cond is satis.able } 4: Initialize P rocessing = {(f, P ropagate u Retain)} \n5: P rocessed[f]= P ropagate u Retain 6: while P rocessing .0 do = 7: remove a tuple tup =(h, Q) from \nProcessing 8: for each function g calling h do 9: for each call site calli of h in g do gh 10: P ropagatei \n= {q|p . Q, for some satis.able gh condition cond, (p, calli } gh, q, cond) . Sumg 11: Retaini = {p|p \n. Q and there does not exist gh a tuple of the form (p, calli j, cond) . Sumg , gh,q where cond is satis.able} \n12: Not-Processed =(P ropagatei u Retaini \\ P rocessed[g] gh gh) 13: if Not-Processed .= 0 then 14: Insert \ntuple (g, Not-Processed) in P rocessing 15: P rocessed[g]= P rocessed[g]u Not-Processed 16: end if 17: \nend for 18: end for 19: end while 20: return P rocessed[entry function] Computation of Q The second step, \ni.e., the computation of Q from A is similar to algorithm 3 -the only difference being thatwe now start \nat the entry location of the entry function of thegiven program and perform the data.ow analysis forward \n(instead of backwards as in algorithm 3) to propagate maximally completeupdate sequences from pointers \nin A till location l is reached. Computing Summary Tuples. The .nal step is to show howto compute summary \ntuples for a set of pointers P in the same Andersen cluster given the FSCI points-to sets of pointers \nhigher inthe Steensgaard hierarchy than those in P . We analyze strongly connected components of the \ncall graphof the given program in reverse topological order. For computingsummaries for functions in \na given strongly connected component Scc, we start with an arbitrary function func belonging to Scc and \nthen analyze each function in Scc till a .xpoint is reached. Given a pointer ptr and location loc in \nfunction func, we performa backward traversal on the control .ow graph (CFG) of the givenprogram starting \nat loc and track maximally complete update se\u00adquences as tuples of the form tup =(p, f, l, m, q, cond). \nTuple (p, f, l, m, q, cond) indicates that during our backward traversalwe are currently at program location \nm and there is a maximally complete update sequence from q to p starting at m and leading to location \nl in function f under the points-to constraints encoded in cond. The algorithm maintains a set W of tuples \nthat are yet to be processed and a set P rocessed of tuples already processed. Ini\u00adtially, W contains \nthe tuple (ptr, func, loc, loc, ptr, true) (line2 in alg. 5). The tuples in W are processed one by one \ntill thereare none left to process. In each processing step, we delete a tu\u00adple (p, f, l, m, q, cond) \nfrom W (line 4 in alg. 5). In process\u00ading (p, f, l, m, q, cond), with respect to the program statement \nst : r = t at location m, we need to generate the new pointer value newP tr that is propagated backwards \nas well the new con\u00addition newCond under which it is propagated (line 6 in alg. 5).This is accomplished \nvia alg. 4. We consider two cases. First assume that the lhs expression r in st : r = t, is simply a \n(pointer) variable. By remark 1, insection 2, there are three sub-cases to consider (i) q is a pointer \nvariable, (ii) q is of the form ~s, and (iii) q is of the form &#38;s. In cases (i) and (iii) we know \nprecisely during our backward traversalwhat q is. However in case (ii) the value of q depends on what \ns points to. Thus in cases (i) and (iii), if q is the same pointer as r then newP tr = t (line 6 in alg. \n4) else newP tr = q (line8 in alg. 4), with newCond = cond in either case. If q is of the form ~s then \nusing the FSCI points-to analysis presented before, wedetermine the set of pointers PT sm that s can \npoint to at location m. Then if r . PT sm there are two possible scenarios: newP tr = t and newCond = \ncond . m : s . r (line 15 in alg. 4) or newP tr = q and newCond = cond . m : s .. r (line 17 in alg. \n4) accordingly as s points to r at m or not, respectively. If, on the other hand, r .. PT sm, then we \nknow that q can never equal r and so it is propagated without change, i.e., newP tr = q and newCond = \ncond (line 13 in alg. 4). Now consider the case that r is of the form ~u. Again weconsider the three \nsub-cases listed above. In cases (i) and (iii), newP tr = t and newCond = cond . m : u . q (line 24 in \nalg. 4) or newP tr = q and newCond = cond . m : u .. q (line26 in alg. 4) accordingly as u points to \nq or not at m, respectively. In case (ii), newP tr = t and newCond = cond.m : s = u (line32 in alg. 4) \nor newP tr = q and newCond = cond . m : s .= u (line 34 in alg. 4) accordingly as s and u point to the \nsame object or not, respectively. The next step is to propagate the data.ow facts backwards tothe predecessor \nlocations of m. There are two cases to consider. First, assume that m is a return site of a function \ng that was called Algorithm 4 Processing a Tuple with respect to a statement 1: Input: Tuple (p, f, l, \nm, q, cond), a statement st : r = t at control location m and an Andersen cluster P . 2: if the statement \nat location m is of the form st : r = t where st . StP and r is at the same Steensgaard depth as q then \n3: if r is a pointer variable then 4: if q is a pointer variable or q is of the form &#38;s then 5: if \nq = r then 6: newP tr = t and newCond = cond 7: else 8: newP tr = q and newCond = cond 9: end if 10: \nelse if q is of the form ~s then 11: Compute the FSCI points-to set PT sm of s at location m 12: if r \n.. PT sm then 13: newP tr = q and newCond = cond 14: else if s points to r at location m then 15: newP \ntr = t and newCond = cond . m : s . r 16: else 17: newP tr = q and newCond = cond . m : s .. r 18: end \nif 19: end if 20: else if r is of the form ~u then 21: Compute the FSCI aliases PT um of u at location \nm 22: if q is a pointer variable or q is of the form &#38;s then 23: if u points to q at m then 24: newP \ntr = t and newCond = cond . m : u . q 25: else 26: newP tr = q and newCond = cond . m : u .. q 27: end \nif 28: else if q is of the form ~s then 29: if s cannot be FSCI aliased to u at m then 30: newP tr = \nq and newCond = cond 31: else if s and u can be aliased to each other at m then 32: newP tr = t and newCond \n= cond . m : s = u 33: else 34: newP tr = q and newCond = cond . m : s .= u 35: end if 36: end if 37: \nend if 38: else 39: newCond = cond and newP tr = q 40: end if from within f. Then we have to propagate \nthe effect of execut\u00ading g backwards for newP tr. Towards that end, for each summary tuple of the form \n(newP tr, g, exitg , w, condj ) we add the new tuple (p, f, l, callm fg , w, newCond . condj) (line 12 \nin alg. 5), where callm is the call site of g corresponding to the call return fg at location m, to W \n. If there exists no such tuple and if newP tr is modi.ed semantically inside g (which can be determined \nbychecking whether the left hand side of an assignment is FSCI\u00adaliased to newP tr) then we need to propagate \nnewP tr through g and so we add the tuple (p, f, l, exitg , newP tr, newCond) to W (line 15 in alg. 5). \nAt the same time, we also add (newP tr, g, exitg, exitg , newP tr, true) to W (line 15 in alg. 5) so \nthat we do not have to re-compute the above tuple the next time aroundwe need to propagate this information \nback through g. Finally, if such a tuple does not exist in Sumg nor can newP tr be se\u00admantically modi.ed \ninside g then executing g has no effect on newP tr and so we can jump straight to the call site callm \nof g fg matching the return site m of g. Accordingly, we add the tuple (p, f, l, callm fg , newP tr, \nnewCond) to W (line 17 in alg. 5). For the second case, we assume that, m is not a function call return \nsite. We consider the set P red of all the predecessor loca\u00adtions of m in f (line 21 in alg. 5). For \neach pred . P red, we form the tuple tup =(p, f, l, pred, newP tr, newCond). If tup has already been \nprocessed, no action is required, else we add tup to W . Note that algorithm 5 is interprocedural and \ncan handle recur\u00adsion. Indeed, when building the summary for a function func be\u00adlonging to a strongly \nconnected component Scc, if we encounter a call to function funcj for which the necessary summary tupleshaven \nt been computed then we .rst analyze that function to com\u00adpute the required summary tuples via step 15. \nIn this way we ex\u00adplore that part of Scc which is relevant to computing the summary tuples for func. \nBy repeating the procedure for each function in Scc, we end up building summaries for each function in \nScc. Algorithm 5 Interprocedural May-Alias Summary Computa\u00adtion for an Andersen Cluster 1: Input: Andersen \nCluster: P , Lock Pointer: ptr, Control Loca\u00adtion loc, Function func. 2: Initialize W to {(ptr,func, \nloc,loc,ptr,true)} and P rocessed to 0. 3: repeat 4: Remove a tuple tup =(p, f, l, m, q, cond) from W \n. 5: Add tup to P rocessed 6: Process tup with respect to the statement at m (alg. 4) to determine newP \ntr and newCond 7: if m is the entry location of f then 8: we add (p, l, newP tr, newCond) to Sumf 9: \nelse if m is the call return site of a function call for g then 10: if the summary tuple has been computed \nfor newP tr for the exit location exitg of g then 11: for each tuple of the form (newP tr, g, exitg , \nw, condj ) . Sumg do 12: Add the tuple (p, f, l, callm w, condj . fg , newCond), where callm is the call-site \nof g in f fg matching the return site m, to W and P roc if it doesn t belong to P roc 13: end for 14: \nelse if newP tr can be (semantically) modi.ed by g then 15: Add the tuples (p, f, l, exitg, newPtr, newCond) \nand (newP tr, g, exitg, exitg , newP tr, true) to W and P roc if they don t already belong to P roc 16: \nelse 17: Add the tuple (p, f, l, callm fg , newPtr, newCond) to W and P roc if it doesn t already belong \nto P roc 18: end if 19: else 20: for each predecessor pred of m do 21: Add tup =(p, f, l, pred, newP \ntr, newCond) to W and P rocessed if tup doesn t already belong to P roc 22: end for 23: end if 24: until \nW is empty Comparison with Existing Summarization Approaches. Due to top-down processing, the points-to \nrelations for pointers which arehigher in the Steensgaard hierarchy than P can be resolved at the time \nof building the summary tuples for P and so we do not need to consider the many possible points-to relations \ninvolving thesepointers. A standard summarization approach would be monolithic in nature as it would \ntrack how a function could modify the points\u00ad Example KLOC # pointers Partitioning Clustering Time (secs) \nSteensgaard Partitioning Andersen Clustering #cluster Max Time #cluster Max Time sock 0.9 1089 0.02 0.04 \n0.11 517 9 0.03 539 6 0.01 hugetlb 1.2 3607 0.3 0.5 8 1091 45 0.7 1290 11 0.78 ctrace 1.4 377 0.01 0.03 \n0.07 47 36 0.03 193 6 0.03 autofs 8.3 3258 0.6 1 6.48 589 125 0.52 907 27 0.92 plip 14 3257 0.7 1.2 6.51 \n568 26 0.57 761 14 0.62 ptrace 15 9075 0.9 1.1 16 924 96 1.46 5941 18 0.67 raid 17 814 0.01 0.06 0.12 \n100 129 0.03 192 26 0.03 jfs dmap 17 14339 2.9 4.7 510 4190 39 3.62 9214 11 1.34 tty io 18 2675 0.9 2.1 \n22 828 8 0.52 882 6 0.45 ipoib multicast 26 2888 0.9 1.2 54.7 1167 15 1 1378 9 0.5 wavelan ko 20 3117 \n0.6 1.4 17.68 591 44 1.2 744 19 1 pico 22 1903 2 10 . 15min 484 171 4.98 871 102 4.46 synclink 24 16355 \n12 18 . 15min 1237 95 26.85 3503 93 26 icecast-2.3.1 49 7490 2 12 459 964 114 15 2553 52 15 freshclam \n54 1991 0.3 0.9 . 15min 157 77 0.6 740 45 0.44 mt-daapd 92 4008 1.4 6.8 . 15min 635 89 4.8 1118 83 12.79 \nsigtool-0.88 95 5881 2 10 . 15min 552 151 8 981 147 7 clamd 101 16639 13 34 61 1274 346 49 3915 187 41 \nsendmail 115 65134 125 675 76min 21088 596 187.8 24580 193 138.9 httpd 128 16180 40 89 . 15min 1779 199 \n35 3893 152 32 Table 1. Comparing Flow and Context-Sensitive Alias analysis without Clustering and with \nSteensgaard and Andersen Clustering to relation of all pointers, i.e., those in P and the ones occurringhigher \nin the hierarchy. Combinatorially, this would result in a lotmore possible points-to con.gurations and \nthus larger summaries.This explain why our summarization approach is more succinct.Moreover, within each \nfunction f, we do a backward propagationand can extract precisely those points-to relations among parame\u00adters \nthat can affect aliases of pointers in P due to execution of f. Existing approaches carry out a forward \npropagation and thereforeconsider all possible relations between parameters many of whichmay be irrelevant. \nComputing Flow and Context-Sensitive Aliases. Finally, to com\u00adpute the .ow and context-sensitive aliases \nof pointer in a givenAndersen cluster P at a given location loc in a given context con = f1 , ..., fk, \nwe follow a procedure very similar to the com\u00adputation of FSCI aliases formulated in algorithm 3. The \nonly dif\u00adference is that the computations of the sets A and Q are now done with respect to a given context \ninstead of taking the union over allcontexts leading to a given program location. Path Sensitivity. In \nour analysis, we have so far ignored condi\u00adtional statements rendering it path-insensitive. However, \nwe caneasily track the conditional statements encountered while buildingsummaries as boolean expressions \nover program variables in thesame way as we tracked points-to constraints. Thus in this case, asummary \ntuple would be of the form (p, loc, q, c1 . ... . ck , conb), where the additional entry conb is a boolean \nexpression captur\u00ading the branching constraints along an update sequence from q to p. One may chose to \ntrack the branching constraints only locallywithin a function as was done in (Hackett and Aiken 2006) \nor glob\u00adally across functions. Furthermore, BDDs can be used to representthe boolean expression conb \nin a canonical fashion so as to weed out infeasible paths and hence bogus summary tuples.  4. Experimental \nResults Our experiments were conducted on a variety of commonly usedprograms on a machine with an Intel \nPentium4 3.20GHz processor and 2GB RAM. Table 1 show data for .ow and context sensitive (FSCS) pointer \nalias analysis. The times taken for the initial Steens\u00adgaard partitioning and the bootstrapped Andersen \nclustering aregiven in columns 4 and 5, respectively. Columns 6 shows the datafor FSCS-analysis without \nuse of any clustering. Columns 8 and9 show data when carrying out (bootstrapping) the FSCS-analysison \nSteensgaard partitions, whereas columns 11 and 12 show datafor FSCS-analysis on Andersen clusters which \nare, in turn, gottenfrom Steensgaard partitions. To simulate parallelization, we dis\u00adtribute the clusters \ninto 5 parts (to simulate 5 machines). This isdone using a greedy heuristic. First we divide the total \nnumber ofpointers in the given program by 5 which gives us a rough esti\u00admate, denoted by size5 , of the \nnumber of pointers in each part.Then we process the clusters one-by-one and as soon as the sum ofthe \nnumber of pointers in each clusters exceeds size5, we combine all clusters processed so far into a single \npart at which point were-start the processing. The time spent on each part is the sum ofthe times taken \nto analyze all clusters individually in that part. Wereport the maximum time taken over all parts. The \npurpose of these experiments was to show the effect of bootstrapping on FSCS-aliasanalysis. This is in \nline with the goals of the paper which is to showhow bootstrapping via clustering can be used to enhance \nexistingpointer analyses. Thus our contribution is orthogonal to improvingany existing pointer analysis \nor proposing new ones. For instance,any new pointer analysis that enhances Andersen s analysis (see forinstance \n(Hardekof and Lin 2007)) can be plugged directly into ourframework to replace the existing Andersen s \nanalysis. Another op\u00adtion is to cascade another analysis like the One-Flow analysis (Das2000; Das et \nal. 2001) between Steensgaard and Andersen. The original motivation for this work was static data race \ndetec\u00adtion for Linux device drivers. We present data for ten drivers: sock, hugetlb, ctrace, autofs, \nplip, ptrace, raid, jfs dmap, tty io, and ipoib multicast. Additionally, we consider the mail transfer \nagents sendmail and pico (part of Pine). Other examples were taken from gnu.org. It can be seen from \nthe results (cols. 6 vs. 9; cols.6 vs. 12) that bootstrapping clearly enhances our FSCS-analysis.The \ntime indicated is in seconds. While there was a clear reduction in the time taken when using clustering, \nthe comparison betweenSteensgaard and Andersen clustering (cols. 9 vs. 12) is more inter\u00adesting. Whereas \nfor the sendmail example, the time taken decreases substantially, for the mt-daap example the time taken \nwhen using Andersen clustering becomes almost threefold. A closer look at the mt-daap example reveals \nthat there is barely any reduction in themaximum cluster size, i.e., from 89 for Steensgaard to 83 for \nAn\u00addersen clusters. This indicates a considerable overlap among theSteensgaard partitions because of \nwhich there is little to be gainedfrom Andersen clustering. For the sendmail example, on the otherhand, \nthe maximum cluster size drops from 596 to 193 and accord\u00adingly so does the running time from 187.8 to \n138.9. Thus Andersenclustering is a good option for large code with high pointer accessdensity but little \noverlap among Andersen clusters which can begauged by the difference in the maximum sizes of Steensgaard \nandAndersen clusters. If that difference is below a threshold then one need not resort to Andersen clustering. \n   References Lars Ole Andersen. Program Analysis and Specialization for the C Pro\u00adgramming Language. \nIn PhD. Thesis, DIKU, 1994. Marc Berndl, Ondrej Lhot\u00b4ak, Feng Qian, Laurie J. Hendren, and Navindra Umanee. \nPoints-to analysis using BDDs. In PLDI, 2003. Manuvir Das. Uni.cation-based pointer analysis with directional \nassign\u00ad ments. In PLDI, 2000. Manuvir Das, Ben Liblit, Manuel F\u00a8ahndrich, and Jakob Rehof. Estimatingthe \nimpact of scalable pointer analysis on optimization. In SAS, 2001. Maryam Emami, Rakesh Ghiya, and Laurie \nJ. Hendren. Context-Sensitive Interprocedural Points-to Analysis in the Presence of Function Pointers. \nIn PLDI, 1994. Manuel F\u00a8ahndrich, Jakob Rehof, and Manuvir Das. Scalable context\u00adsensitive .ow analysis \nusing instantiation constraints. In PLDI, 2000. Jeffrey S. Foster, Manuel F\u00a8ahndrich, and Alexander Aiken. \nPolymorphic versus Monomorphic Flow-Insensitive Points-to Analysis for C. Brian Hackett and Alex Aiken. \nHow is aliasing used in systems software?In FSE, 2006. B. Hardekof and C. Lin. The ant and the grasshopper: \nfast and accurate pointer analysis for million lines of code. In PLDI, 2007. Nevin Heintze and Olivier \nTardieu. Ultra-fast Aliasing Analysis using CLA: A Million Lines of C Code in a Second. In PLDI, 2001. \nOndrej Lhot\u00b4ak and Laurie J. Hendren. Scaling Java Points-to Analysis Using SPARK. In CC, 2003. Barbara \nG. Ryder, Willian Landi, Phil Stocks, Sean Zhang, and Rita Al\u00adtucher. A Schema for Interprocedural Modi.cation \nSide-Effect Analysis with Pointer Aliasing. In ACM Trans. Program. Lang. Sys., volume 23, pages 105 186, \n2001. Bjarne Steensgaard. Points-to Analysis in Almost Linear Time. In POPL, 1996. John Whaley and Monica \nS. Lam. An Ef.cient Inclusion-Based Points-To Analysis for Strictly-Typed Languages. In SAS, 2002. John \nWhaley and Monica S. Lam. Cloning-based context-sensitive pointeralias analysis using binary decision \ndiagrams. In PLDI, 2004. John Whaley and Martin C. Rinard. Compositional Pointer and Escape Analysis \nfor Java Programs. In OOPSLA, 1999. John Whaley, C. Unkel, and M. Lam. A BDD-based deductive database \nfor program analysis. In http://suif.stanford.edu/bddbddb, 2004. Robert P. Wilson and Monica S. Lam. \nEf.cient Context Sensitive Pointer Analysis for C Programs. In PLDI, 1995. S. Zhang, B. G. Ryder, and \nW. Landi. Program Decomposition for Pointer Aliasing: A Step Towards Practical Analyses. In FSE, 1996. \nJianwen Zhu. Symbolic pointer analysis. In ICCAD, pages 150 157, 2002. Jianwen Zhu and Silvian Calman. \nContext sensitive symbolic pointer analysis. In IEEE Trans. on CAD of Integrated Circuits and Systems, \nvolume 24, pages 516 531, 2005. \n\t\t\t", "proc_id": "1375581", "abstract": "<p>We propose a framework for improving both the scalability as well as the accuracy of pointer alias analysis, irrespective of its flow or context-sensitivities, by leveraging a three-pronged strategy that effectively combines <i>divide and conquer, parallelization and function summarization</i>. A key step in our approach is to first identify small subsets of pointers such that the problem of computing aliases of any pointer can be reduced to computing them in these small subsets instead of the entire program. In order to identify these subsets, we first apply a series of increasingly accurate but highly scalable (context and flow-insensitive) alias analyses in a cascaded fashion such that each analysis <i>A<sub>i</sub></i> works on the subsets generated by the previous one <i>A<sub>i-1</sub></i>. Restricting the application of <i>A<sub>i</sub></i> to subsets generated by <i>A<sub>i-1</sub></i>, instead of the entire program, improves it scalability, i.e., <i>A<sub>i</sub></i> is <i>bootstrapped</i> by <i>A<sub>i-1</sub></i>. Once these small subsets have been computed, in order to make our overall analysis accurate, we employ our new summarization-based flow and context-sensitive alias analysis. The small size of each subset offsets the higher computational complexity of the context-sensitive analysis. An important feature of our framework is that the analysis for each of the subsets can be carried out independently of others thereby allowing us to leverage parallelization further improving scalability.</p>", "authors": [{"name": "Vineet Kahlon", "author_profile_id": "81100153948", "affiliation": "NEC Laboratories, Princeton, NJ, USA", "person_id": "P1022795", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375613", "year": "2008", "article_id": "1375613", "conference": "PLDI", "title": "Bootstrapping: a technique for scalable flow and context-sensitive pointer alias analysis", "url": "http://dl.acm.org/citation.cfm?id=1375613"}