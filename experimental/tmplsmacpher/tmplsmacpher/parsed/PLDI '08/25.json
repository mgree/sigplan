{"article_publication_date": "06-07-2008", "fulltext": "\n Program Analysis as Constraint Solving Sumit Gulwani Saurabh Srivastava * Ramarathnam Venkatesan Microsoft \nResearch, Redmond University of Maryland, College Park Microsoft Research, Redmond sumitg@microsoft.com \nsaurabhs@cs.umd.edu venkie@microsoft.com Abstract A constraint-based approach to invariant generation \nin programs translates a program into constraints that are solved using off-the\u00adshelf constraint solvers \nto yield desired program invariants. In this paper we show how the constraint-based approach can be used \nto model a wide spectrum of program analyses in an ex\u00adpressive domain containing disjunctions and conjunctions \nof lin\u00adear inequalities. In particular, we show how to model the problem of context-sensitive interprocedural \nprogram veri.cation. We also present the .rst constraint-based approach to weakest precondition and strongest \npostcondition inference. The constraints we gener\u00adate are boolean combinations of quadratic inequalities \nover integer variables. We reduce these constraints to SAT formulae using bit\u00advector modeling and use \noff-the-shelf SAT solvers to solve them. Furthermore, we present interesting applications of the above \nanalyses, namely bounds analysis and generation of most-general counter-examples for both safety and \ntermination properties. We also present encouraging preliminary experimental results demon\u00adstrating the \nfeasibility of our technique on a variety of challenging examples. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; F.3.1 [Logics and Meanings of Programs]: \nSpecifying and Verifying and Reasoning about Pro\u00adgrams; F.3.2 [Logics and Meanings of Programs]: Semantics \nof Programming Languages Program analysis General Terms Algorithms, Theory, Veri.cation Keywords Program \nVeri.cation, Weakest Precondition, Strongest Postcondition, Most-general Counterexamples, Bounds Analysis, \nNon-termination Analysis, Constraint Solving 1. Introduction Discovering inductive program invariants \nis critical for both prov\u00ading program correctness and .nding bugs. Traditionally, iterative .xed-point \ncomputation based techniques like data-.ow analy\u00adses [25], abstract interpretation [11] or model checking \n[13] have been used for discovering these invariants. An alternative is to use a constraint-based invariant \ngeneration [8, 10, 7, 32] approach that translates (the second-order constraints represented by) a program \n* This author performed the work reported here during a summer internship at Microsoft Research. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 \n13, 2008, Tucson, Arizona, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-860-2/08/06. . . $5.00. into \n(.rst-order quanti.er-free) satis.ability constraints that can be solved using off-the-shelf solvers. \nThe last decade has witnessed a revolution in SAT/SMT based methods enabling solving of indus\u00adtrial sized \nsatis.ability instances. This presents a real opportunity to leverage these advances for solving hard \nprogram analysis prob\u00adlems. Constraint-based techniques offer two other advantages over .xed-point computation \nbased techniques. First, they are goal\u00addirected and hence have the potential to be more ef.cient. Sec\u00adondly, \nthey do not require the use of widening heuristics that are used by .xed-point based techniques leading \nto loss of precision that is often hard to control. In this paper, we present constraint-based techniques \nfor three classical program analysis problems, namely program veri.cation, weakest precondition generation \nand strongest postcondition gen\u00aderation over the abstraction of linear arithmetic. Using this core framework \nof analyses we further show interesting applications to bounds analysis and .nding most-general counterexamples \nto safety and termination properties. A distinguishing feature of our preliminary tool is that it can \nuniformly handle a large variety of challenging examples that otherwise require many different spe\u00adcialized \ntechniques for analysis. The key contributions of this pa\u00adper lie in the uniform constraint-based approach \nto core program analyses (Sections 2 5) and their novel applications (Section 6). The goal of program \nveri.cation is to discover invariants that are strong enough to verify given assertions in a program. \nCurrent constraint-based techniques are limited to discovering conjunctive invariants in an intraprocedural \nsetting. We present a constraint\u00adbased technique that can generate linear arithmetic invariants with \nbounded boolean structure (Section 2), which also allows us to extend our approach to a context-sensitive \ninterprocedural setting (Section 3). A key idea of our approach is a scheme for reduc\u00ading second-order \nconstraints to SAT constraints and this can be re\u00adgarded as an independent contribution to solving a \nspecial class of second-order formulas. Another key idea concerns an appropriate choice of cut-set, which \nhas until now been overlooked in other constraint-based techniques. Our tool can verify assertions (safety \nproperties) in benchmark programs (used by alternative state-of\u00adthe-art techniques) that require disjunctive \ninvariants and sophis\u00adticated procedure summaries. We also show how constraint-based invariant generation \ncan be applied to verifying termination proper\u00adties as well as the harder problem of bounds analysis \n(Section 6.1). The goal of strongest postcondition generation is to infer pre\u00adcise invariants in a given \nprogram so as to precisely characterize the set of reachable states of the program. Current constraint-based \ninvariant generation techniques work well only in a program veri\u00ad.cation setting, when the problem enforces \nthe constraint that the invariant should be strong enough to verify the assertions. How\u00adever, in absence \nof assertions in programs, there is no guarantee about the precision of invariants. We describe a constraint-based \ntechnique that can be used to discover some form of strongest in\u00advariants (Section 5). In the area of \n.xed-point computation based techniques, the problem of generating precise invariants has led to development \nof several widening heuristics that are tailored to spe\u00adci.c classes of programs [40, 18, 16, 17]. Our \ntool can uniformly discover precise invariants for all such programs. The goal of weakest precondition \ngeneration is to infer the weakest precondition that ensures validity of all assertions in a given program. \nWe present a constraint-based technique for dis\u00adcovering some form of weakest preconditions (Section \n4). Our tool can generate weakest preconditions of safety as well as termination properties for a wide \nvariety of programs that cannot be analyzed uniformly by any other technique. We also describe an interesting \napplication of weakest precondi\u00adtion generation, namely generating most-general counterexamples for both \nsafety (Section 6.2) and termination (Section 6.3) prop\u00aderties. The appeal of generating most-general \ncounterexamples (as opposed to generating any counterexample) lies in characterizing all counterexamples \nin a succinct speci.cation that provides better intuition to the programmer. For example, if a program \nhas a bug when (n . 200 + y) . (9 >y > 0), then this information is more useful than simply generating \nany particular counterexample, say n = 356 . y =7 (Figure 10). We have also successfully ap\u00adplied our \ntool to generate weakest counterexamples for termination of some programs (taken from recent work [22]). \n2. Program Veri.cation Given a program with some assertions, the program veri.cation problem is to verify \nwhether or not the assertions are valid. The challenge in program veri.cation is to discover the appropriate \nin\u00advariants at different program points, especially inductive loop in\u00advariants that can be used to prove \nthe validity of the given asser\u00adtions. (The issue of discovering counterexamples, in case the asser\u00adtions \nare not valid, is addressed in Section 6.2.) Program model In this paper, we consider programs that have \nlinear assignments, i.e., assignments of the form x := e, or non\u00addeterministic assignments x :=?. We \nalso allow for assume and assert statements of the form assume(p) and assert(p), where p is some boolean \ncombination of linear inequalities e . 0. Here x denotes some program variable that takes integral values, \nand e denotes some linear arithmetic expression. Since we allow for assume statements, without loss of \ngenerality, we assume that all conditionals in the program are non-deterministic. 2.1 Background: Conversion \nof programs to constraints true PV1 (int y) {x := -50; while (x< 0) { x := x + y; y++; } assert(y> 0) \n} y> 0 (a) (b) 'x,y\u00a2(I): true * I[-50/x] I = x< 0 * I[(y+1)/y, (x+y)/x] I = x . 0 * y> 0 (c) Figure \n1. (a) A program veri.cation example (b) The correspond\u00ading control .ow graph (c) Constraint generated \nfrom the program over the unknown loop invariant I at loop header. Our tool gener\u00adates a disjunctive \nsolution (x< 0 . y> 0) for the invariant I. is to include all targets of back-edges in any depth .rst \ntraversal of the control-.ow graph. (In case of structured programs, where all loops are natural loops, \nthis corresponds to choosing the header node of each loop.) However, as we will discuss in Section 2.3, \nsome other choices of cut-set may be more desirable from an ef.\u00adciency/precision viewpoint. For notational \nconvenience, we assume that the cut-set always includes the program entry location 1entry and exit location \n1exit . We then associate each cut-point 1 with a relation I\" over program variables that are live at \n1. The relations I\"entry and I\"exit at program s entry and exit locations respectively are set to true, \nwhile the relations at all other cut-points are unknown relations that we seek to discover. Two cut-points \nare adjacent if there is a path in the control .ow graph from one to the other that does not pass through \nany other cut-point. We establish constraints between the relations at adjacent cut-points 11 and 12 \nas follows. Let Paths(11,12) denote the set of paths between 11 and 12 that do not pass through any other \ncut-point. We use the notation VC(11,12) to denote the constraint that the relations I\"1 and I\"2 at adjacent \ncut-points 11 and 12 respectively are consistent with respect to each other: 0@ (I\"1 = w(., I\"2 )) 1A \n The problem of program veri.cation can be reduced to the problem of .nding solutions to a second-order \nconstraint. The second-order VC(11,12)= X ^ unknowns in this constraint are the unknown program invariants \nPaths(\"1,\"2) that are inductive and strong enough to prove the desired asser-Above, X denotes the set \nof program and fresh variables that occur tions. In this section we describe the conversion of programs \nto constraints. in I\"1 and w(., I\"2 ). The notation w(., I) denotes the weakest pre- We .rst illustrate \nthe process of constraint generation for an example program. Consider the program in Figure 1(a) with \nits control .ow graph in Figure 1(b). The program s precondition is true and postcondition is y> 0. To \nprove the postcondition, we need to .nd a loop invariant I at the loop header. There are three paths \nin the program that constrain I. The .rst corresponds to the entry case; the path from true to I. The \nsecond corresponds to the inductive case; the path that starts and ends at I and goes around the loop. \nThe third corresponds to the exit case; the path from I to y> 0. Figure 1(c) shows the corresponding \nformal constraints. We now show how to generate such constraints in a more gen\u00aderal setting of any arbitrary \nprocedure. The .rst step is to choose a cut-set. A cut-set is a set of program locations (called cut-points) \nsuch that each cycle in the control .ow graph passes through some condition of path . (which is a sequence \nof program instructions) with respect to I and is as de.ned below: w(skip,I)= Iw(assume p, I)= p = I \nw(x := e, I)= I[e/x] w(assert p, I)= p . I w(x :=?,I)= I[r/x] w(S1; S2,I)= w(S1,w(S2,I)) where r is some \nfresh variable and the notation [e/x] denotes substitution of x by e and may not be eagerly carried out \nacross unknown relations. Let 11,12 range over pairs of adjacent cut-points. Then any solution to the \nunknown relations I\" in the following (veri.cation) constraint (which may also have substitutions), yields \na valid proof of correctness. ^ VC(11,12) (1) program location in the cut-set. One simple way to choose \na cut-set \"1,\"2 Observe that this constraint is universally quanti.ed over the pro\u00adgram variables and \nis a function of If, the vector of relations I\" at all cut-points (including I\" entry ,I\" exit ). We \ntherefore write it as the veri.cation constraint X..(If). For program veri.cation I\" entry are set to \ntrue. Going back to the example, the second\u00ad and I\" exit order constraints corresponding to the program \nin Figure 1(a) are shown in Figure 1(c) and correspond to the entry, inductive and exit constraints for \nthe loop.  2.2 Constraint solving In this section we show how to solve the second-order constraint from \nEq. 1 that represents the veri.cation condition of unknown relations at cut-points. One way to solve \nthese constraints for dis\u00adcovering the unknown invariants I\" is to use .xed-point based tech\u00adniques like \nabstract interpretation. Another (signi.cantly manual) approach is to require the programmer to provide \nthe invariants at the cut-points, which can then be veri.ed using a theorem prover. Instead, we take \nthe approach of reducing the second-order con\u00adstraint into a boolean formula such that a satisfying assignment \nto the formula maps to a satisfying assignment for the second-order constraint. Throughout this section, \nwe will illustrate our reduction over the constraints from Figure 1(c). Our constraint-solving approach \ninvolves three main steps. First, we assume some invariant templates (possibly disjunctive) and reduce \nthe second-order constraints to .rst-order constraints over the unknown parameters of the templates. \nWe then make use of Farkas lemma [38] to translate the .rst-order constraints (with uni\u00adversal quanti.cation) \ninto an existentially quanti.ed multi-linear quadratic constraint. These constraints are then translated \ninto a SAT formula using bit-vector modeling (instead of solving them using specialized mathematical \nsolvers [8, 10]). These three steps are detailed below. Step 1 First, we convert second-order unknowns \nto .rst-order un\u00adknowns. Instead of searching for a solution to unknown relations (which are second-order \nentities) from an arbitrary domain, we re\u00adstrict the search to a template that is some boolean combination \nof linear inequalities among program variables. For example, an un\u00adknown relation can have the template \n(Paixi . 0 . Pbixi . ii 0) . (Pcixi . 0 . Pdixi . 0), where ai,bi,ci,di are all un\u00adii known integer constants \nand xi are the program variables. The tem\u00adplate can either be provided by the user (for example, by specifying \nthe maximum number of conjuncts and disjuncts in DNF represen\u00adtation of any unknown relation), or we \ncan have an iterative scheme in which we progressively increase the size of the template until a solution \nis found. Given such templates, we replace the unknown relations in the constraint in Eq. 1 by the templates \nand then apply any pending substitutions to obtain a .rst-order logic formula with unknowns that range \nover integers. For the example in Figure 1(a), a relevant invariant template is a1x + a2y + a3 . 0 . \na4x + a5y + a6 . 0, where the ai s are (integer) unknowns to be discovered. If the chosen domain for \nthe template is not expressive enough then the constraints will be unsatis.able. On the other hand if \nthere is redundancy then redundant templates can always be instantiated with true or false as required. \nThis step of the reduction translates the veri.cation constraint in Figure 1(c) with second-order unknowns \nI to .rst\u00adorder unknowns ai s. For example, the .rst constraint in Figure 1(c) after Step 1 is true = \n(-50a1 + a2y + a3 . 0) . (-50a4 + a5y + a6 . 0). Step 2 Next, we translate .rst-order universal quanti.cation \nto .rst-order existential quanti.cation using Farkas lemma (at the cost of doing away with some integral \nreasoning). Farkas lemma implies that a conjunction of linear inequalities ei . 0 (with integral coef.cients) \nis unsatis.able over rationals iff some non\u00adnegative (integral) linear combination of ei yields a negative \nquan\u00adtity, i.e., ! X \u00ac(^ ei . 0) .= i =A > 0, Ai . 0 \" X XAiei = -A !# i The reverse direction of the \nabove lemma is easy to see since it is not possible for a non-negative linear combination of non-negative \nexpressions ei to yield a negative quantity. The forward direction also holds since the only way to reason \nabout linear inequalities over rationals is to add them, multiply them by a non-negative quantity or \nadd a non-negative quantity. The universal quanti.cation on the right hand side of the above equivalence \nis over a polynomial equality, and hence can be gotten rid of by equating the coef.cients of the program \nvariables X on both sides of the polynomial equality. We can convert any universally quanti.ed linear \narithmetic for\u00admula X(.) into an existentially quanti.ed formula using Farkas lemma as follows. We convert \n. in conjunctive normal form V.i, j where each conjunct .i is a disjunctions of inequalities Weii . 0. \nj Observe that X(.)= V X(.i) and that .i can be rewritten as i j \u00ac V(-ei - 1 . 0). Hence, Farkas lemma, \nas stated above, can be j applied to each X(.i). We illustrate the application of this step over the \n.rst constraint from Figure 1(c) that we obtained after Step 1. After Step 1 we have true = e1 . 0.e2 \n. 0 (where e1 =-50a1 +a2y +a3 . 0 and e2 =-50a4 + a5y + a6 . 0 as obtained earlier). After expanding \nthe implication we get a constraint that is already in CNF form and therefore the corresponding unsatis.ability \nconstraint is \u00ac((-e1 - 1 . 0) . (-e2 - 1 . 0)). Farkas lemma can now be applied to yield =A1,A2 . 0,A \n> 0( x,yA1(-e1 -1)+A2(-e2 -1) =-A). Now we can collect the coef.cients for x, y to get a .rst-order existential \nconstraint. Notice that A1 (respectively A2) is multiplied with the coef.cients inside e1 (respectively \ne2) and therefore this is a multi-linear quadratic constraint over integer variables. Equating the coef.cients \nof y and the constant term we get the constraints: (50a1A1 - a3A1 - A1) + (50a4A2 - a6A2 - A2)= -A and \na2A1 + a5A2 =0. Application of Farkas lemma leads to a loss of completeness since we do away with some \nintegral reasoning. For example, Farkas lemma cannot help us prove unsatis.ability of 3x . 1 . 2x . 1, \nwhere x ranges over integers. However, we have not found this loss of completeness to be a hindrance \nin any of our benchmark examples. Step 3 Next, we convert the .rst-order existentially quanti.ed (or \nquanti.er-free) formula obtained from Step 2 to a SAT formula. The formula that we obtain from Step 2 \nis a conjunction of (multi\u00adlinear quadratic polynomials) over integer variables. We convert such a formula \ninto a SAT formula by modeling integer variables as bit-vectors and encoding integer operations like \narithmetic, mul\u00adtiplication, and comparison as boolean operations over bit-vectors. Our approach to constraint \nsolving is sound in the sense that any satisfying solution to the SAT formula yields a valid proof of \ncor\u00adrectness. However, it is not complete, i.e., there might exist a valid proof of correctness but the \nSAT formula might not be satis.able. This is not unexpected since program veri.cation in general is an \nundecidable problem, and no algorithm can be expected to be both sound and complete. However, our constraint \nsolving approach is complete under two assumptions (i) the unknown invariants are in\u00adstances of given \ntemplates, (ii) checking consistency of invariants at adjacent cut-points does not require integral reasoning. \nWe have found that both these assumptions are easily met for our benchmark examples. The real challenge \ninstead lies in .nding the satis.ability assignment for the SAT formula, for which the recent engineering \nadvances in SAT solvers seem to stand up to the task.  2.3 Choice of cut-set The choice of a cut-set \naffects the precision and ef.ciency of our algorithm (or, in fact, of any other constraint-based technique). \nThe choice of a cut-set has been overlooked in constraint-based approaches. [4] recently proposed a technique \nfor performing .xed\u00adpoint computation on top of constraint-based technique to regain some precision, \nwhich we claim was inherently lost in the .rst place because of a non-optimal choice of cut-set. In this \nsection, we describe a novel strategy for choosing a cut-set that strikes a good balance between precision \nand ef.ciency. From de.nition of a cut-set, it follows that we need to include some program locations \nfrom each loop into the cut-set. One simple strategy is to include all header nodes (or targets of back-edges) \nas cut-points. Such a choice of cut-set necessitates searching/solving for unknown relations over disjunctive \nrelations when the proof of correctness involves a disjunctive loop invariant. It is interesting to note \nthat for several programs that require disjunctive loop invari\u00adants, there is another choice for cut-set \nthat requires searching for unknown relations over only conjunctive domains. Furthermore, even the number \nof conjuncts required are less compared to those required when the header nodes are chosen to be cut-points. \nThis choice for cut-set corresponds to choosing one cut-point on each path inside the loop. In presence \nof multiple sequential condition\u00adals inside a loop, this requires expanding the control-.ow inside the \nloop into disjoint paths and choosing a cut-point anywhere on each disjoint path. In fact, this choice \nfor cut-set leads to the greatest precision in the following sense. THEOREM 1. Let C be a cut-set that \nincludes a program location on each acyclic path inside a loop (after expansion of control .ow inside \nthe loop into disjoint paths). Suppose that the search space for unknown relations is restricted to templates \nthat have a speci.ed boolean structure. If there exists a solution for unknown relations corresponding \nto any cut-set, then there also exists a solution for unknown relations corresponding to cut-set C. The \nproof of Theorem 1 is given in the full version of this pa\u00adper [21]. Furthermore, there are several examples \nthat show that the reverse direction in Theorem 1 is not true (i.e., there exists a solution to the unknown \nrelations corresponding to cut-set C, but there is no solution to unknown relations corresponding to \nsome other choice of cut-set). This is illustrated by the example in Fig\u00adure 2 (discussed below). Examples \nConsider the example shown in Figure 2. Let 1i denote the program point that immediately precedes the \nstatement at line i in the program. The simplest choice of cut-set corresponds to choosing the loop header \n(program location 12). The inductive invariant that is required at the loop header, and is discovered \nby our tool, is the disjunction (0 . x . 51 . x = y) . (x . 51 . y . 0 . x + y = 102). If we instead \nchoose the cut-set to be {14,16} (based on the strategy described in Theorem 1), then the inductive invariant \nmap is conjunctive. This is signi.cant because conjunctive invariants are easier to discover. Our tool \ndiscovers the inductive invariant map {14 .. (y . 0 . x . 50 . x = y),16 .. (y . 0 . x . 50 . x + y = \n102)} in such a case. However, the choice of cut-set mentioned in Theorem 1 does not always obviate the \nneed for disjunctive invariants. The example in PV2() { 1 x := 0; y := 0; 2 while (true) { 3 if (x . \n50) 4 y++; 5 else 6 y--; 7 if (y< 0) 8 break; 9 x++; 10 } 11 assert(x = 102) } Figure 2. Another program \nveri.cation example (taken from [16]) that requires a disjunctive invariant at the loop header. However, \na non-standard choice of cutset (as suggested in Theorem 1) leads to conjunctive invariants. Figure 1(a) \nhas no conditionals inside the loop, and yet any (linear) inductive invariant required to prove the assertion \nis disjunctive (e.g., (x< 0) . (y> 0), which is what our tool discovers). Heuristic proposals [34, 4] \nfor handling disjunction will fail to discover invariants for such programs. 3. Interprocedural Analysis \nThe w computation described in previous section is applicable only in an intraprocedural setting. In \nthis section, we show how to extend our constraint-based method to perform a precise (i.e., context\u00adsensitive) \ninterprocedural analysis. Precise interprocedural analysis is challenging because the be\u00adhavior of the \nprocedures needs to be analyzed in a potentially un\u00adbounded number of calling contexts. Procedure inlining \nis one way to do precise interprocedural analysis. However, there are two prob\u00adlems with this approach. \nFirst, procedure inlining may not be possi\u00adble at all in presence of recursive procedures. Second, even \nif there are no recursive procedures, procedure inlining may result in an exponential blowup of the program. \nA more standard way to do precise interprocedural analysis is to compute procedure summaries, which are \nrelations between proce\u00addure inputs and outputs. These summaries are usually structured as sets of pre/postcondition \npairs (Ai,Bi), where Ai is some relation over procedure inputs and Bi is some relation over procedure \nin\u00adputs and outputs. The pre/postcondition pair (Ai,Bi) denotes that whenever the procedure is invoked \nin a calling context that satis.es constraint Ai, the procedure ensures that the outputs will satisfy \nthe constraint Bi. However, there is no automatic recipe to ef.ciently construct or even represent these \nprocedure summaries, and ab\u00adstraction speci.c techniques may be required. Data structures and algorithms \nfor representing and computing procedure summaries have been described over the abstraction of linear \nconstants [33], and linear equalities [29]. Recently, some heuristics have been de\u00adscribed for the abstraction \nof linear inequalities [39]. In this section, we show that a constraint-based approach is particularly \nsuited to discovering such useful pre/postcondition (Ai,Bi) pairs. The key idea is to observe that the \ndesired behavior of most procedures can be captured by a small number of such (unknown) pre/postcondition \npairs. We then replace the procedure calls by these unknown behaviors and assert that the procedure, \nin fact, has such behaviors as in assume-guarantee style reason\u00ading [23]. For ease of presentation and \nwithout loss of generality, let us assume that a procedure does not read/modify any global vari\u00adables; \ninstead all global variables that are read by the procedure are passed in as inputs, and all global variables \nthat are modi.ed by the procedure are returned as outputs. IP1() { x := 5; y := 3; result := Add(x, y); \nIP2() { assert(result = 8); result :=M(19)+M(119); }assert(result = 200); Add(int i, j) {}if i : 0 M(int \nn) {ret := j; if(n> 100) else return n - 10; b := i - 1; else c := j +1; return M(M(n + 11)); ret := \nAdd(b, c); }return ret; } (a) (b) Figure 3. Interprocedural analysis examples. (a) is taken from [39, \n30]. (b) is the famous McCarthy 91 function [28, 27, 26], which requires multiple pre/postcondition pairs. \nSuppose we conjecture that there are q interesting pre/post\u00adcondition pairs for procedure P (x){S; return \ny; } with the vector of formal arguments x and vector of return values y. In practice, the value of q \ncan be iteratively increased until invariants are found that make the constraint system satis.able. Then, \nwe can summarize the behavior of procedure P using q tuples (Ai,Bi) for 1 . i . q, where Ai is some relation \nover procedure inputs x, while Bi is some relation over procedure inputs and outputs x and y. We assert \nthat this is indeed the case by generating constraints for each i as below and asserting their conjunction: \nassume(Ai); S; assert(Bi); (2) We compile away procedure calls v := P (u) on any simple path by replacing \nthem with the following code fragment: ! v :=?; assume ^ (Ai[u/x] = Bi[u/x, v/y]) ; (3) i Observe that \nin our approach, there is no need, in theory, to have q different pre/postcondition pairs. In fact, the \nsummary of a procedure can also be represented as some formula .(x, y) (with arbitrary Boolean structure) \nthat represents relation between pro\u00adcedure inputs x and outputs y. In such a case, we assert that . \nindeed is the summary of procedure P by generating constraint for {S; assert(.(x, y)); }, and we compile \naway a procedure call v := P (u) by replacing it by the code fragment v := ?; assume(.[u/x, v/y]). However, \nour approach of maintaining multiple symbolic pre/postcondition pairs (which is also inspired by the \ndata structures used by the traditional .xed-point computa\u00adtion algorithms) is more ef.cient since it \nenforces more structure on the assume-guarantee proof and leads to lesser unknown quantities and simpler \nconstraints. Examples Consider the example shown in Figure 3(a). Our tool veri.es the assertion by generating \nthe pre/post pair (i . 0, ret = i+j) for procedure Add. This example illustrates that only relevant pairs \nare computed for each procedure. In addition to serving as the base case of the recursion the true branch \nof the condition inside Add has the concrete effect formalized by the pre/post pair (i< 0, ret = j). \nHowever, this behavior is not needed to prove any assertion in the program and is therefore suppressed. \nThe procedure M(int n) in Figure 3(b) is the widely known McCarthy91 function whose most accurate description \nis given by the pre/post pairs (n> 100, ret = n - 10) and (n . 100, ret = 91). The function has often \nbeen used as a benchmark test for automated program veri.cation. The goal directed nature of the veri.cation \nproblem allows our tool to derive (101 . n . 119, ret = n - 10) and (n . 100, ret = 91) as the pairs \nthat prove the program assertion. As such, it discovers only as much as is required for the proof. 4. \nWeakest Precondition Given a program with some assertions, the problem of weakest precondition generation \nis to infer the weakest precondition I\" entry that ensures that whenever the program is run in a state \nthat satis.es I\" entry , the assertions in the program hold. In Section 6 we show that a solution to \nthis problem can be a useful tool for a wide range of applications. In this section, we present a constraint-based \napproach to infer\u00adring weakest preconditions under a given template. Since a precise solution to this \nproblem is undecidable, we work with a relaxed notion of weakest precondition. For a given template structure \n(as de.ned in step 2.1 in Section 2), we say that A is a weakest pre\u00adcondition if A is a precondition \nthat .ts the template and involves constants whose absolute value is at most c (where c is some given \nconstant such that the solutions of interest are those that involve constants whose absolute value is \nat most c) and there does not exist a weaker precondition than A with similar properties. The .rst step \nin a constraint-based approach to weakest precon\u00addition generation is to treat the precondition I\" entry \nas an unknown relation in Eq. 1, unlike in program veri.cation where we set I\" entry to be true. However, \nthis small change merely encodes that any consistent assignment to I\" entry is a valid precondition, \nnot neces\u00adsarily the weakest one. In fact, when we run our tool with this small change for any example, \nit returns false as a solution for I\" entry . Note that false is always a valid precondition, but not \nnecessarily the weakest one. One simple approach to .nding the weakest precondition may be to search \nfor a precondition that is weaker then the current solution (which can be easily enforced by adding another \nconstraint to Eq. 1), and to iterate until none exists. However, this approach can have a very slow progress. \nWhen we analyzed Figure 4(a) (discussed below) using this approach, our tool iteratively produced i . \nj+127, i . j+126,..., i . j under a modeling that used 8-bit two s-complement integers. In general this \nna\u00a8ive iterative technique will be infeasible. We need to augment the constraint system to encode the \nnotion of a weakest relation. We can encode that I\" entry is a weakest precondition as follows. The veri.cation \nconstraint in Eq. 1 can be regarded as function of two arguments I\" entry and Ir , where Ir denote the \nrelations at all cut-points except at the program entry location, and can thus be written as X..(I\" entry \n,Ir ). Now, for any other relation I' that is strictly weaker than I\" entry , it should not be the case \nthat I' is a valid precondition. This can be stated as the following constraint. X..(I\" entry ,Ir ) . \n''''' I,Ir `weaker(I,I\" entry ) =\u00ac X..(I,Ir )\u00b4 def I' where weaker(I',I\" entry )=( X.(I\" entry = ) .=X.(I' \n. \u00acI\" entry )). The trick of using Farkas lemma to get rid of universal quanti.\u00adcation (Step 2.2 in Section \n2) cannot be applied here because there is existential quanti.cation nested inside universal quanti.cation. \nIn this section we describe some iterative techniques for generating weakest preconditions. We present \ntwo different novel approaches in Sections 4.1 and 4.2. Examples For the procedure in Figure 4(a), our \ntool generates two different conjunctive preconditions (which individually ensure the validity of the \ngiven assertion): (i) (i . j), which ensures that when the loop terminates then x . y, (ii) (i . 0), \nwhich ensures Merge(int m1, m2, m3) {assert(m1 . 0 = m2 . 0) WP1(int i, j) {x := y := 0; while (x : 100) \nx := x + i; y := y + j; }assert(x . y) } { k := i := 0; while (i < m1) {assert(0 : k < m3) A[k++] = B[i++]; \n}i := 0; while (i < m2) {assert(0 : k < m3) A[k++] = C[i++]; (a) } } (b) Figure 4. Weakest Precondition \nExamples. that the loop never terminates making the assertion unreachable and therefore trivially true. \nFigure 4(b) shows an array merge procedure that is called to merge two arrays B, C of sizes m1,m2 respectively \ninto a third one A of size m3. The procedure is correct if no invalid array access are made (stated as \nthe assertions inside the loops) when it is run in an environment where the input arrays B and C are \nproper (i.e. m1,m2 . 0, which is speci.ed as an assertion at the procedure entry). For the Merge procedure \nin Figure 4(b), our tool generates two different conjunctive preconditions m3 . m1 + m2 . m1 . 0 . m2 \n. 0 and m1 =0 . m2 =0. 4.1 Binary search strategy First, note that without loss of generality we can \nassume that the weakest precondition to be discovered is a conjunctive invariant. This is because we \ncan obtain the disjunctive weakest precondition as disjunctions of disjoint weakest conjunctive solutions. \n1 n THEOREM 2. Let I = V ei . 0 be some non-false precondition. i=1 For any n \u00d7 (n + 1) matrix D of non-negative \nconstants, let I(D) nn! ' denote the formula V Di,n+1 + PDi,j ei . 0 . Let I be i=1 j=1 some weakest \nprecondition (in our template structure) s.t. I = I ' . Then, A1. There exists a non-negative matrix \nD ' such that I ' = I(D ' ). A2. For any matrix D '' that is strictly larger than D ' i,j . (i.e., D \n'' D ' i,j for all i, j and D '' i,j for some i, j), I(D '' ) is not a i,j >D ' precondition (in our \ntemplate structure). A3. For any (non-negative) matrix D ''' that is smaller than D ' (i.e., D ''' i,j \nfor all i, j), I(D ''' ) is a precondition. i,j . D ' PROOF: A1 follows from Farkas lemma. A2 follows \nfrom the fact that I(D '' ) is strictly weaker than I(D ' ) and I(D ' ) is a weakest precondition. A3 \nfollows from the fact that I(D ''' ) is stronger than I(D ' ). Theorem 2 suggests a binary search based \nalgorithm (described in Figure 5) for .nding a weakest precondition. The parameters MaxN and MaxD denote \nan upper bound on the values of the numer\u00adator and denominator of any rational entry of the matrix D \n' referred to in Theorem 2(A1). Since the absolute values of all coef.cients 1 The signi.cance of generating \na weakest conjunctive solution that is disjoint with other weakest conjunctive solutions already generated \nlies in the fact that the number of weakest conjunctive solutions may potentially be unbounded. However, \nthe number of weakest disjoint conjunctive solutions is .nite. WPreFromPre(Input: Precondition I) 1 Di,j \n:= 0; 2 foreach 1 . i, j . n: 3 low := 0; high := MaxN; 1 4 while (high - low > MaxD ) 5 mid := (high \n+ low)/2; 6 Di,j := mid; 7 if = a precondition I ' s.t. I(D) = I ' 8 then low := mid; 9 else high := \nmid; 10 Di,j := low; 11 Output a precondition I ' s.t. I(D) = I ' . Figure 5. A binary-search based \niterative algorithm for computing a weakest precondition starting from any non-false precondition. in \nI and I ' are bounded above by c, MaxD and MaxN are bounded above by NN/2 \u00d7 c N , where N = n 2 . 2 Observe \nthat the preconditions in line 7 and line 11 can be generated by simply adding the additional constraint \nI(D) = I\" entry to the veri.cation condition for the procedure, and then solving for the resulting constraint \nusing the technique discussed in Section 2. Also note that the matrix D computed at the end is not exactly \nthe matrix D ' referred to in Theorem 2(A1) but is close enough in the sense that any precondition weaker \nthan I(D) is a weakest one. The algorithm in Figure 5 involves making a maximum of n 2\u00d7log (MaxN \u00d7 MaxD) \nqueries to the constraint solver. Hence, it is useful to start with a non-false precondition with the \nleast value of n (where n denotes the number of conjunctions of linear inequalities in the input precondition \nI). Such a precondition can be found by iteratively increasing the number of conjuncts in the template \nfor the precondition until one is found. In the next Section, we describe another algorithm for generat\u00ading \na weakest precondition, which we found to be more ef.cient for our benchmark examples.  4.2 Locally \npointwise-weakest strategy For simplicity of presentation, we assume that each non-trivial maximally \nstrongly connected component in the control .ow graph has exactly one cut-point (an assumption that can \nalso be ensured by simple transformations [21]). However, the results in this section can be extended \nto the general setting without this assumption. The algorithm for generating a weakest precondition is \nde\u00adscribed in Figure 6. Line 8 initializes I\" to a pointwise-weakest relation (de.ned below) for each \ncut-point 1 in reverse topologi\u00adcal order of the control dependences between different cut-points. (Note \nthat since we assume that each maximal SCC does not have more than one cut-point, there are no cyclic \ncontrol dependences between different cut-points.) We de.ne a relation I at a cut-point to be pointwise-weakest \nif it is a weakest relation that is consis\u00adtent with respect to the relations at its neighboring (successor) \ncut\u00adpoints. It is easy to see that the pointwise-weakest relation thus 2 This is because the entries \nin matrix D are solutions to a system of linear equations each of whose coef.cients are bounded in absolute \nvalue by c. These linear equations are obtained by equating the coef.cients of ' corresponding variables \nin the n equivalences represented by I = I(D ' ). The solution to each unknown in a system of linear \nequations can be described by ratio of two determinants whose entries are coef.cients of the linear equations. \nSince there can be at most n2 linearly independent equations among n2 unknowns, each entry in matrix \nD can be expressed as ratio of two determinants, each of size at most N \u00d7 N where N = n, and all of whose \nentries are bounded in absolute value by c. WPre(Input: Neighborhood structure N) 1 foreach cutpoint \n1 in reverse topological order: 2 I := false; 3 while = a relation I ' at 1 s.t. ' 4 (a) V VC(1, 1 ' \n)[I\" + I ] \"' Successors (\") 5 (b) I = I ' but I ' .= I ! '' 6 (c) VW \u00acVC(1, 1 ' )[I\" + I ] I'' N(I' \n) \"' Successors (\") 7 do {I := I ' }; 8 I\" := I; 9 Output I\" entry ; Figure 6. Another iterative algorithm \nfor computing a weakest precondition based on an input neighborhood structure N. generated at the program \nentry location will be a weakest precon\u00addition. The while loop in Line 3 generates a pointwise-weakest \nrelation at a cut-point 1 by generating a locally pointwise-weakest relation (as de.ned below) with respect \nto the input neighborhood struc\u00adture N in each iteration and repeating the process to obtain a weaker \nlocally pointwise-weakest relation until one exists. (This process is conceptually similar to iterating \nover local minimas to obtain a global minima.) We say that a relation I is a locally pointwise\u00adweakest \nwith respect to a neighborhood N if it is a weakest rela\u00adtion among its neighbors that is consistent \nwith respect to the rela\u00adtion at its neighboring (successor) cut-points. A locally pointwise\u00adweakest \nrelation can be generated by simply solving the constraints on Lines 5-6 using the technique discussed \nin Section 2. Observe that the constraints I ' .= I and \u00acVC(1, 1 ' ) are already exis\u00adtentially quanti.ed, \nand hence do not require the application of Farkas lemma to remove universal quanti.cation. The only \ndif\u00adference is that we now obtain quadratic inequalities as opposed to quadratic equalities obtained \nat the end of Step 2 (on Page 3) of our constraint-solving methodology. However, the bit-vector modeling \nin Step 3 works equally well for quadratic equalities as well as in\u00adequalities. Also note that the neighborhood \nstructure N should be such that it should be possible to enumerate all elements of N(I ' ) for any invariant \ntemplate I ' . The performance of our algorithm crucially depends on the choice of the input neighborhood \nstructure, which affects the num\u00adber of iterations of the loop in Line 3. A denser neighborhood struc\u00adture \nmay result in lesser number of iterations of the while loop (i.e., a lesser number of queries to the \nconstraint solver), but a larger sized query as a result of the condition in Line 7. We describe be\u00adlow \na neighborhood structure that we found to be quite ef.cient for our purposes; in fact, it required upto \n3 iterations for most of our benchmark examples. However, (unlike the binary search strategy described \nin previous section), we have not been able to prove a formal bound on the worst-case number of queries \nto the constraint solver that our choice of neighborhood structure can yield because of repeated iterations \nof the while loop. 4.2.1 Neighborhood Structure In this section, we describe the neighborhood structure \nN that we used in our experiments. The set of relations that are in the neigh\u00adborhood N of a conjunctive \nrelation (in which, without loss of gen\u00aderality, we assume that all inequalities are independent of each \nSP2() { Swap(int x) { d := t := s := 0; while (*) while(1) if (x = 1) if (*) x := 2; t++; s := 0; else \nif (x = 2) else if (*) x := 1; if (s< 5) assert(x : 8); d++; s++; } } (a) (b) Figure 7. (a) Weakest precondition \nexample that has two locally pointwise-weakest relations at program entry. (b) Strongest Post\u00adcondition \nexample taken from [16, 17]. other) are as described below. n N(^ ei . 0) = {ej +1 . 0 . ^ ei . 0 | 1 \n. j . n}= i=1 i=*j {ej + ee . 0 . ^ ei . 0 | j .= p . 1 . j, p . n} (4) i* =j Geometric Interpretation: \nThe neighborhood structure described above has a nice geometric interpretation. The neighbors of a con\u00advex \nregion Vei . 0 are obtained by slightly moving any of the i hyper-planes ej . 0 parallel to itself, or \nby slightly rotating any of the hyper-planes ej . 0 along its intersection with any other hyper-plane \nee . 0. We extend the neighborhood structure de.ned above to relations in DNF form (in which, without \nloss of generality, we assume that all disjuncts are disjoint from each other) as: m N(_Ii)= {Ij ' . \n_Ii | 1 . j . m; Ij ' . N(Ij )} i=1 i=*j Notice how the above choice of the neighborhood structure helps \navoid the repeated iteration over the preconditions i . j + 127,i . j + 126,...,i . j (as alluded in \nSection 4 on Page 4) to obtain the weakest precondition i . j for the example in Fig\u00adure 4(a). None of \nthese preconditions except i . j is locally pointwise-weakest with respect to the above neighborhood \nstruc\u00adture. Hence, the use of the above neighborhood structure requires only one iteration of the while \nloop in the algorithm in Figure 6 for obtaining weakest precondition for the example in Figure 4(a). \nHowever, a locally pointwise-weakest relation with respect to the neighborhood structure de.ned above \nmay not be a pointwise\u00adweakest relation. For example, consider the program in Figure 7(a). The relations \nx . 0 and x . 8 are both locally pointwise-weakest relations (with respect to the above neighborhood \nstructure) at pro\u00adgram entry. However, only the relation x . 8 is a pointwise\u00adweakest relation at program \nentry (and hence a weakest precon\u00addition). Hence, use of the above neighborhood structure requires two \niterations of the while loop in the algorithm in Figure 6 for obtaining weakest precondition for the \nexample in Figure 7(a). 5. Strongest Postcondition The problem of strongest postcondition is to generate \nthe most pre\u00adcise invariants at a given cut-point. Just as in the weakest precon\u00addition case, we work \nwith a relaxed notion of strongest postcon\u00addition, wherein we are interested in .nding a strongest postcondi\u00adtion, \nwhose proof of correctness is expressible in the given template structure. Our technique for generating \nstrongest postcondition is very similar to the weakest precondition inference technique described in \nthe previous section. We use the algorithm described in Figure 8 SPost(Input: Neighborhood structure \nN) 1 foreach cutpoint 1 in topological order: 2 I := true; 3 while exists a relation I ' at 1 s.t. ' \n4 (a) V VC(1 ' ,1)[I\" + I ] \" ' Predecessors (\") 5 (b) I ' = I but I .= I ' ! '' 6 (c) VW \u00acVC(1 ' ,1)[I\" \n+ I ] I '' N(I ' ) \" ' Predecessors (\") 7 do {I := I ' }; 8 I\" := I; 9 Output ; I\" exit Figure 8. An \niterative algorithm for computing a strongest post\u00adcondition based on an input neighborhood structure \nN. (in place of the algorithm described in Figure 6) with the following neighborhood structure (in place \nof the one mentioned in Eq. 4). n N(^ ei . 0) = {ej - 1 . 0 . ^ ei . 0 | 1 . j . n}= i=1 i=*j {ej - ee \n. 0 . ^ . ei . 0 | j = p . 1 . j, p . n} i* =j Examples For a more general version of the procedure in \nFig\u00adure 2, wherein we replace the constant 50 by a symbolic constant m that is asserted to be non-negative, \nour tool generates the post\u00adcondition x =2m +2. For the procedure in Figure 7(b), our tool generates \nthe strongest postcondition s + d + t . 0 . d . s +5t. 6. Applications In earlier sections, we have described \nconstraint-based techniques for veri.cation of safety properties. In this section, we show how to apply \nthose techniques for .nding counterexamples to safety prop\u00aderties, veri.cation of termination (which \nis a liveness property), and .nding counterexamples to termination. 6.1 Termination and Bounds Analysis \nThe termination problem involves checking whether the given pro\u00adcedure terminates under all inputs. In \nthis section, we show how to use the constraint-based approach to solve a harder problem, namely bounds \nanalysis. The problem of bounds analysis is to .nd a worst-case bound on the running time of a procedure \n(say in terms of the number of instructions executed) expressed in terms of its in\u00adputs. We build on \nthe techniques that reduce the bounds analysis problem to discovering invariants of a speci.c kind [20]. \nThe key idea is to compute bounds on loop iterations and number of re\u00adcursive procedure call invocations. \nEach of these can be bounded by appropriately instrumenting counter variables and estimating bounds on \ncounter variables. In particular, the number of loop it\u00aderations of a while loop while c do S can be \nbounded by com\u00adputing upper bound on the instrumented variable i inside the loop in the following code \nfragment: i := 0; while c do { i++; S; } . The number of recursive procedure call invocations of a procedure \nP (x) { S } can be bounded similarly by computing an upper bound on global variable i inside procedure \nP in the following code ''' ' fragment: P (x) { i := 0; P (x); }; P (x ) { i++; S[x /x]; } . CLAIM 1. \nLet P be a given program. Let P ' be the transformed program obtained after instrumenting counters that \nkeep track of loop iterations and recursive call invocations and introducing par\u00adtial assertions at appropriate \nlocations that assert that the counters Loop(int n,m,x0,y0) {assert(x0 < y0); Loop(int n,m,x0,y0) {assert(x0 \n< y0); x := x0; y := y0; i := 0; x := x0; y := y0; while (x < y) while (x < y) i++; x := x + n; assert(i \n: f(n, m, x0, y0)); y := y + m; x := x + n; } Original Program y := y + m; } Instrumented Program Figure \n9. Discovering weakest preconditions for termination. are bounded above by some function of the inputs. \nThe program P terminates iff the assert statements in P ' are satis.ed. Invariant generation tools such \nas abstract interpretation can be used to compute bounds on the counter variables (as proposed in [20]). \nWe show instead that a constraint-based approach is par\u00adticularly suited for discovering these invariants \nsince they have a speci.ed form and involve linear arithmetic. We introduce assert statements with templates \ni<a0 + Paixi (at the counter incre\u00ad i ment i++ site in case of loops and at the end of the procedure \nin case of recursive procedures) for bounding the counter variable. Besides the counter instrumentation \nstrategy mentioned above, [20] also describes some other counter instrumentation strategies that can \nbe used to compute non-linear bounds as a composition of linear bounds on multiple instrumentation counters. \nSuch tech\u00adniques can also be used in our framework to compute non-linear bounds using linear templates. \nAdditionally, the constraint-based approach solves an even harder problem, namely inferring preconditions \nunder which the procedure terminates and inferring a bound under that precondition. For this purpose, \nwe simply run the tool in weakest precondition inference mode. This is particularly signi.cant when procedures \nare expected to be called under certain preconditions and would not otherwise terminate under all inputs. \nWe are not aware of any technique that can compute such conditional bounds. Example For the example in \nFigure 9, our tool computes the weakest precondition n . m +1 . x0 . y0 - 1 and the bound y0 -x0. The \nlatter requires discovering the inductive loop invariant i . (x - x0) - (y - y0).  6.2 Counterexamples \nfor Safety Properties Since program analysis is an undecidable problem, we cannot have tools that can \nprove correctness of all correct programs or .nd bugs in all incorrect programs. Hence, to maximize the \npractical success rate of veri.cation tools, it is desirable to search for both proofs of correctness \nas well as counterexamples in parallel. Earlier, we showed how to .nd proofs of correctness of safety \nand termina\u00adtion properties. In this section, we show how to .nd most-general counterexamples to safety \nproperties. The problem of generating a most-general counterexample for a given set of (safety) assertions \ninvolves .nding the most general characterization of inputs that leads to the violation of some reach\u00adable \nsafety assertion. We show how to .nd such a characterization using the techniques discussed in Section \n4 and Section 6.1. The basic idea is to reduce the problem to that of .nding the weakest precondition \nfor an assertion. This reduction involves con\u00adstructing another program from the given program P using \nthe fol\u00adlowing transformations: B1 Instrumentation of program with an error variable: We intro\u00adduce \na new error variable that is set to 0 at the beginning of the program. Whenever violation of any assertion \noccurs (i.e., the negation of that assertion holds), we set the error variable to 1 Bug1(int y, n) { \n1 x := err := i1 := i2 := 0; 2 if (y< 9) Bug1(int y, n) { 3 while (x<n) 1 x := 0; 4 i1++; 2 if (y< 9) \n5 assert(i1 : f1(n, y)); 3 while (x<n) 6 if (x . 200) 4 assert(x< 200); 7 err := 1; goto L; 5 x := x \n+ y; 8 x := x + y; 6 else 9 else 7 while (x . 0) 10 while (x . 0) 8 x++; 11 i2++; } 12 assert(i2 : f2(n, \ny)); 13 x++; 14 L: assert(err =1); } (a) Original Program (b) Instrumented Program Figure 10. A most-general \ncounterexample that leads to violation of the safety assertion in the original program is (n . 200 + \ny) . (0 <y< 9). Our tool discovers this by instrumenting the program appropriately and then running our \nweakest precondition algorithm. and jump to the end of the program, where we assert that the error variable \nis equal to 1. We remove the original assertions from the program. B2 Instrumentation to ensure termination \nof all loops: For this we use the strategy described in Section 6.1, wherein we instrument the program \nwith counter variables and introduce assertion tem\u00adplates that assert that the counter variable is upper \nbounded by some function of loop inputs or procedure inputs. CLAIM 2. Let P be a program with some safety \nassertions. Let P ' be the program obtained from program P by using the transfor\u00admation described above. \nThen, P has an assertion violation iff the assertions in program P ' hold. Claim 2 holds and its signi.cance \nlies in the fact that now we can use weakest precondition inference (Section 4) on the trans\u00adformed program \nto discover most-general characterization of inputs under which there is a safety violation in the original \nprogram. Example The program shown in Figure 10(a) is instrumented using transformations B1 and B2 and \nthe resulting program is shown in Figure 10(b). Our tool discovers the precondition (n . 200 + y) . (9 \n>y> 0). The loop invariant (at line 3) that establishes all assertions in the instrumented program is \n(n . 200 + y) . (i . x) . (9 >y> 0) . (x<n). Note that this invariant implies the instantiation n for \nthe loop bound function f1(n, y). On the other hand the precondition y< 9 implies that the loop on line \n10 is unreachable, and hence any arbitrary f2 suf.ces. Observe the importance of transformation B1. An \nalternative to transformation B1 that one might consider is to simply negate the original safety assertion \ninstead of introducing an error variable. This is incorrect for two reasons: (a) It is too stringent \na criterion because it insists that in each iteration of the loop the original as\u00adsertion does not hold, \n(b) It does not ensure reachability and allows for those preconditions under which the assert statement \nis never executed. In fact, when we run our tool with such an alternative transformation on the example \nin Figure 10(a), we obtain n . 0 as the weakest precondition. Also, observe the importance of transformation \nB2. If we do not perform transformation B2 on the example in Figure 10(a), then the tool comes up with \nthe following weakest precondition: y . 0. Note that under this precondition, the assertion at the end \nof the program always holds since that location is unreachable. NT2(int i) {even := 0; NT1(int x, y) \n{ while (i . 0) while (x . 0) if (even =0) x := x + y; i--; y++; else } i++; even := 1 - even; } (a) \n(b) Figure 11. Non-termination examples taken from [22]. Note that the transformation B2 does not ensure \ntermination of all loops in the original program. The transformation B2 ensures termination of only those \nloops that are reachable under the to\u00adbe-discovered weakest precondition and that too in the program \nobtained after transformation B1, which introduces extra control\u00ad.ow that causes loops to terminate as \nsoon as the violation of any safety property occurs. For example, the loop on line 10 in Figure 10(b) \nis unreachable under the discovered preconditions and therefore any arbitrary function f2 suf.ces.  \n6.3 Counterexamples for Termination The problem of generating a most-general counterexample for pro\u00adgram \ntermination involves .nding the most-general characteriza\u00adtion of inputs that leads to non-termination \nof the program. Without loss of generality we assume that the program has at most one exit point. CLAIM \n3. Let P be a given program with a single exit point. Let P ' be the program obtained from P by adding \nthe assert statement assert(false) at the end of the program. Then, P is non\u00adterminating iff the assert \nstatement in P ' is satis.ed. The signi.cance of Claim 3 lies in the fact that now we can use weakest \nprecondition inference (Section 4) on the transformed program to discover most-general characterization \nof inputs under which the original program is non-terminating. Examples Consider the example shown in \nFigure 11(a). If we instrument assert(false) at the end of the program, then our tool generates the precondition \nx . 0 . y . 0, which is one of the weakest af.ne conditions under which the program is non\u00adterminating. \nNow consider the program shown in Figure 11(b). If we instru\u00adment assert(false) at the end of this program, \nthen our tool generates the precondition i . 1. Notice that the loop guard i . 0 is not suf.cient to \nguarantee non-termination. 7. Experiments In previous sections, we have shown how to reduce various pro\u00adgram \nanalysis problems to the problem of solving SAT constraints. We now present encouraging experimental \nresults illustrating that SAT solvers can in fact solve the constraints generated from our chosen set \nof examples in a reasonable amount of time. Our exam\u00adples are drawn from benchmarks used by state-of-the-art \nalternative techniques. Our reduction technique is parameterized by several parameters (such as the cut-set, \nthe number of bits used in bit-vector modeling, and the size of templates in terms of the number of conjuncts \nand disjuncts) whose choice presents a completeness/ef.ciency trade\u00adoff. An increase in size of these \nparameters increases the chance that the required invariant/pre-condition would .t the template, but \nat the cost of generating a bigger SAT formula. Name Time (secs) Num. Clauses cegar1 [19] 0.08 5 K cegar2 \n[19] 0.80 50 K barbr [18] 0.41 76 K berkeley [18] 3.00 441 K bk-nat [18] 5.30 174 K seesaw [18] 3.23 \n70 K hsortprime [18] 0.51 54 K lifnatprime [18] 1.27 51 K swim [18] 1.63 45 K cars [18] 2.93 86 K ex1 \n[18] 0.10 10 K ex2 [18] 0.75 92 K .g1a [18] 0.14 20 K .g2 [18] 0.56 239 K .g3 [18] 16.60 547 K w1 [5], \npg12 0.14 25 K w2 [5], pg12 1.80 165 K  Name Time Num. (secs) Clauses Fig 3(a), [39] 0.57 63 K a1 [31], \npg9 9.90 174 K a2 [29], pg2 0.50 75 K mergesort 0.19 43 K quicksort 0.45 133 K .bonacci 11.00 90 K Fig \n3(b) 72.00 558 K (b) Name Time Num. (secs) Clauses Fig 2 [16, 17] 1.40 107 K Fig 7(b) 16.10 273 K w1 \n[5], pg12 0.60 60 K speed [17], pg10 18.20 41 K merge [16], pg 11 3.90 128 K burner [15], pg14 1.50 91 \nK (c) (a) (d) Table 1. (a) Program veri.cation examples (b) Interprocedural analysis examples (c) Weakest \nprecondition inference (including non\u00adtermination and bug-.nding) examples. (d) Strongest postcondition \ninference examples Name Time Num. (secs) Clauses [22], pg3 0.80 42 K Fig 11(b) [22] 0.40 57 K Fig 11(a) \n[22] 0.60 43 K Fig 4(a) 14.40 119 K Fig 4(b) 80.00 221 K Fig 7(a) 0.50 50 K Fig 9 11.60 118 K Fig 10 \n68.00 135 K In our experiments, we used the cut-set suggested by Theo\u00adrem 1. For discovering the remaining \nparameters, we used an in\u00adcremental strategy. We progressively increased the number of bits required \nfor bit-vector modeling by 2 bits (starting from 3 bits for unknown coef.cients and 6 bits for unknown \nconstants and 1 bit for the multipliers A s used in Farkas lemma). The number of dis\u00adjuncts and conjuncts \nwere progressively increased by 1 (starting with 1 disjunct and 2 conjuncts). The increment was performed \nun\u00adtil the SAT solver stopped saying UNSAT. For most of our bench\u00admark examples, our choice of parameters \nrequired upto 2 iteration steps. The speci.c choice of these parameters was motivated by the observation \nthat for most of our examples, the required invari\u00adants involved only one disjunct and one bit for the \nmultiplier A s. We also observed that working with a smaller number of disjuncts and a smaller number \nof bits for the multiplier A s is important for ef.ciency reasons because the size of the generated SAT \nformula usually blows up with these two particular parameters. Table 1 describes the results of our analysis \non our benchmark examples. It shows the number of clauses in the generated CNF formula (for the choice \nof parameters for which the SAT solver was able to .nd a satisfying assignment) and the time taken by \nthe SAT solver (Z3 [12]) to .nd a satisfying assignment under the discovered parameters. Table 1(a) shows \nthe time taken by our tool on several pro\u00adgram veri.cation examples. Most of these examples are taken \nfrom benchmarks used by some state-of-the-art abstraction re.nement based techniques [19, 18], which \nalso provide exhaustive compar\u00adison against similar techniques. The last two examples w1 and w2 are taken \nfrom [5]. w1 is a simple loop iteration but with x . n replaced with x .n while w2 is a loop with the \nguard moved in\u00ad = side a non-deterministic conditional. Standard narrowing is unable to capture the precision \nlost due to widening in these instances. Our solution times compare favorably against previous techniques. \nTable 1(b) shows the time taken by our tool for generating re\u00adquired invariants for establishing validity \nof assertions in an inter\u00adprocedural setting for different examples. The .rst three examples are taken \nfrom alternate proposals [29, 31, 39] for discovering lin\u00adear invariants in an interprocedural setting. \nWe also analyze some recursive procedures (Mergesort, Quicksort and Fibonacci) for dis\u00adcovering invariants \nthat establish upper bounds on the number of recursive procedure call invocations after the respective \nprocedures have been instrumented with the counter instrumentation strategy described in Section 6.1. \nThe invariants for all of these examples required producing one pre/post pair for each procedure. Proving \ncorrectness of the McCarthy91 function in Figure 3(b), however, required computing two pre/post pairs. \nTable 1(c) shows the time taken by our tool for generating weak\u00adest preconditions for respective examples. \nOur tool implements the methodology described in Section 4.2 for generation weak\u00adest precondition. The \n.rst three examples in Table 1(c) are taken from [22], and we infer the weakest preconditions that ensure \nnon\u00adtermination of these examples. Our most challenging example (Fig\u00adure 10) takes 68 seconds. Table \n1(d) shows the time taken by our tool for generating strongest postconditions for respective examples \ntaken from bench\u00admarks used by some sophisticated widening techniques [5, 15, 16, 17]. For each of these \nexamples, we compute the strongest linear invariants that hold at the end of the respective procedures. \nThe examples speed, merge, and burner model hybrid automaton for some real systems. 8. Related Work Constraint \nsolving based techniques Theoretical expositions of program analysis techniques frequently formulate \nthem as con\u00adstraints (constraint-based CFA, type inference, reachable states in abstract interpretation \n[11], model checking among others) and typically solve them using .xed-point computation. We instead \nconcentrate on techniques that reduce the analysis problem to con\u00adstraints that can be solved using SAT/SMT \nsolvers. Constraint\u00adbased techniques have been successfully applied for discovering conjunctive linear \narithmetic invariants [8, 36, 35, 37] in an in\u00adtraprocedural setting. In contrast, our approach discovers \nlinear arithmetic invariants with arbitrary (but pre-speci.ed) boolean structure in a context-sensitive \ninterprocedural manner. Constraint-based techniques have also been applied for dis\u00adcovering non-linear \npolynomial invariants [24] and invariants in the combined theory of linear arithmetic and uninterpreted \nfunc\u00adtions [3], but again in a conjunctive and intraprocedural setting. It is possible to combine these \ntechniques with our formulation to lift them to disjunctive and context-sensitive interprocedural settings. \nConstraint-based techniques, being goal-directed, work natu\u00adrally in program veri.cation mode where the \ntask is to discover inductive loop invariants for the veri.cation of assertions. Oth\u00aderwise, there is \nno guarantee on the precision of invariants gen\u00aderated. [6] describes a simple iterative strategy of \nrerunning the solver with the additional constraint that the new solution should be stronger than the \nprevious solution. Such a strategy can have a very slow progress. Our approach for strongest postcondition \nprovides a more ef.cient solution. Additionally, we present a methodology for generating weakest preconditions. \nConstraint solvers have been used for .nding bugs in loop-free programs [41] (obtained by unrolling loops \nin programs heuristi\u00adcally). In contrast, our methodology can be used to .nd a most\u00adgeneral counterexample \nand also .nd bugs in programs that require an unbounded or a large number of loop iterations for the \nbug to manifest. Abstract interpretation based techniques for discovering linear arithmetic invariants \nThere is a large body of work on sophisti\u00adcated widening techniques [16, 17], abstraction re.nement [40, \n18] and specialized extensions (using acceleration [15], trace partition\u00ading and loop unrolling [5]) \nfor discovering conjunctive linear in\u00adequality invariants in an intraprocedural setting. Powerset exten\u00adsions \n[14, 19] of linear inequalities domain and derived techniques utilizing control .ow structure [34, 4] \nhave been proposed for dis\u00adjunctive invariants. All these are specialized to work for speci.c classes \nof programs. In contrast, our approach can uniformly dis\u00adcover precise invariants in all such classes \nof programs in strongest postcondition setting, while also offering the added advantage of being goal-directed \nin veri.cation setting. There has been work on interprocedurally discovering linear equality relationships \n[33, 29]; however the problem of discover\u00ading linear inequalities in an interprocedural setting has not \nbeen effectively addressed. Recently, some heuristics have been pro\u00adposed [39] to discover linear inequality \nrelationships in an inter\u00adprocedural setting based on extension of an earlier work on tran\u00adsition matrices \nand postponing conditional evaluation. The preci\u00adsion of these techniques is unclear in the presence \nof conditionals. Since our approach facilitates disjunctive reasoning, our approach discovers linear \ninequalities interprocedurally as naturally (and as precisely) as it does so in an intraprocedural setting. \nProofs and counterexamples to termination There is a large body of work on proving termination properties \nby synthesizing ranking functions [9, 32, 7, 1, 2, 10] using a variety of techniques including those \nbased on constraint solving. We show how to use constraint solving to solve the harder problem of timing \nanalysis. Moreover, we also show how to do conditional termination analy\u00adsis, wherein we infer preconditions \nfor termination. Recently, .nding counterexamples to termination was ad\u00addressed in [22]. Their technique \n.nds counterexamples to termi\u00adnation properties by means of identifying lassos (linear program paths \nthat end in a non-terminating cycle) using a constraint-based approach to .nd recurring sets of states. \nIn contrast, our scheme for proving non-termination is based on inferring weakest precon\u00additions, thereby \ninferring a most-general counterexample to termi\u00adnation. 9. Conclusion and Future Work This paper describes \nhow to model a wide spectrum of program analysis problems as constraints that can be solved using off-the\u00adshelf \nconstraint solvers. We show how to model the problem of dis\u00adcovering invariants that involve (conjunctions \nand disjunctions of) linear inequalities (both intraprocedurally and interprocedurally), and apply it \nto the problem of checking safety properties and timing analysis of programs. We also show how to model \nthe problem of discovering weakest preconditions (and strongest postconditions) and apply it to inferring \nmost-general counterexamples for both safety and termination properties. The constraints that we gener\u00adate \nare boolean combinations of quadratic inequalities over integer variables, which we reduce to SAT formulas \nusing bit-vector mod\u00adeling. We show experimentally that the SAT solver can ef.ciently solve such constraints \ngenerated from hard benchmarks. The work described here can be extended in two directions. The .rst one \nis to extend these techniques to discover a richer class of invariants involving arrays, pointers, and \neven quanti.ers. Sec\u00adondly, one can also consider new constraint solving techniques, in particular QBF \n(Quanti.ed Boolean Formula) solvers. This would alleviate the need for applying Farkas lemma to compile \naway uni\u00adversal quanti.cation, leading to smaller sized SAT formulas, but those that are universally \nquanti.ed. References [1] I. Balaban, A. Cohen, and A. Pnueli. Ranking abstraction of recursive programs. \nIn VMCAI, pages 267 281, 2006. [2] J. Berdine, A. Chawdhary, B. Cook, D. Distefano, and P. W. O Hearn. \nVariance analyses from invariance analyses. In POPL, pages 211 224, 2007. [3] D. Beyer, T. Henzinger, \nR. Majumdar, and A. Rybalchenko. Invariant synthesis for combined theories. In VMCAI 07, pages 378 394, \n2007. [4] D. Beyer, T. A. Henzinger, R. Majumdar, and A. Rybalchenko. Path invariants. In PLDI, pages \n300 309, 2007. [5] B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, D. Monniaux, \nand X. Rival. Design and implementation of a special-purpose static program analyzer for safety-critical \nreal-time embedded software. In The Essence of Computation: Complexity, Analysis, Transformation., LNCS \n2566, pages 85 108. Oct. 2002. [6] A. R. Bradley and Z. Manna. Veri.cation constraint problems with strengthening. \nIn ICTAC, pages 35 49, 2006. [7] A. R. Bradley, Z. Manna, and H. B. Sipma. Linear ranking with reachability. \nIn Proc. 17th Intl. Conference on Computer Aided Veri.cation (CAV), volume 3576 of Lecture Notes in Computer \nScience. Springer Verlag, July 2005. [8] M. Col\u00b4on, S. Sankaranarayanan, and H. Sipma. Linear invariant \ngeneration using non-linear constraint solving. In CAV, pages 420 432, 2003. [9] M. Col\u00b4on and H. Sipma. \nPractical methods for proving program termination. In CAV 02: Proceedings of the 14th International Conference \non Computer Aided Veri.cation, pages 442 454. Springer-Verlag, 2002. [10] P. Cousot. Proving program \ninvariance and termination by parametric abstraction, lagrangian relaxation and semide.nite programming. \nIn VMCAI, pages 1 24, 2005. [11] P. Cousot and R. Cousot. Abstract interpretation: A uni.ed lattice model \nfor static analysis of programs by construction or approximation of .xpoints. In POPL, pages 238 252, \n1977. [12] L. M. de Moura and N. Bj\u00f8rner. Ef.cient e-matching for smt solvers. In CADE, pages 183 198, \n2007. [13] J. Edmund M. Clarke, O. Grumberg, and D. A. Peled. Model checking. MIT Press, Cambridge, MA, \nUSA, 1999. [14] R. Giacobazzi and F. Ranzato. Optimal domains for disjunctive abstract interpretation. \nSci. of Comp. Prg., 32(1-3):177 210, 1998. [15] L. Gonnord and N. Halbwachs. Combining widening and acceleration \nin linear relation analysis. In 13th International Static Analysis Symposium, SAS 06, LNCS 4134, Aug. \n2006. [16] D. Gopan and T. W. Reps. Lookahead widening. In CAV, pages 452 466, 2006. [17] D. Gopan and \nT. W. Reps. Guided static analysis. In SAS, pages 349 365, 2007. [18] B. S. Gulavani, S. Chakraborty, \nA. V. Nori, and S. K. Rajamani. Automatically re.ning abstract interpretations. Technical Report TR-07-23, \nIIT Bombay, 2007. [19] B. S. Gulavani and S. K. Rajamani. Counterexample driven re.nement for abstract \ninterpretation. In TACAS, pages 474 488, 2006. [20] S. Gulwani, K. Mehra, and T. Chilimbi. Statically \ncomputing complexity bounds for programs with recursive data-structures. Technical Report MSR-TR-2008-16, \nMicrosoft Research, Redmond, Jan. 2008. [21] S. Gulwani, S. Srivastava, and R. Venkatesan. Program analysis \nas constraint solving. Full version. Technical Report MSR-TR-2008-44, Microsoft Research, Mar. 2008. \n[22] A. Gupta, T. Henzinger, R. Majumdar, A. Rybalchenko, and R.-G. Xu. Proving non-termination. In POPL, \n2008. [23] C. B. Jones. Speci.cation and design of (parallel) programs. In IFIP Congress, pages 321 332, \n1983. [24] D. Kapur. Automatically generating loop invariants using quanti.er elimination. In Deduction \nand Applications, 2005. [25] G. A. Kildall. A uni.ed approach to global program optimization. In POPL, \npages 194 206, 1973. [26] Z. Manna. Mathematical Theory of Computation. McGraw-Hill, New York, 74. [27] \nZ. Manna and J. McCarthy. Properties of programs and partial function logic. Machine Intelligence, 5, \n1970. [28] Z. Manna and A. Pnueli. Formalization of properties of functional programs. Journal of the \nACM, 17(3):555 569, 1970. [29] M. M\u00a8uller-Olm and H. Seidl. Precise interprocedural analysis through \nlinear algebra. In POPL, pages 330 341, 2004. [30] M. M\u00a8uller-Olm, H. Seidl, and B. Steffen. Interprocedural \nanalysis (almost) for free. In Technical Report 790, Fachbereich Informatik, Universitt Dortmund, 2004. \n[31] M. M\u00a8uller-Olm, H. Seidl, and B. Steffen. Interprocedural herbrand equalities. In ESOP, pages 31 \n45, 2005. [32] A. Podelski and A. Rybalchenko. A complete method for the synthesis of linear ranking \nfunctions. In VMCAI, pages 239 251, 2004. [33] S. Sagiv, T. W. Reps, and S. Horwitz. Precise interprocedural \ndata.ow analysis with applications to constant propagation. Theor. Comput. Sci., 167(1&#38;2):131 170, \n1996. [34] S. Sankaranarayanan, F. Ivancic, I. Shlyakhter, and A. Gupta. Static analysis in disjunctive \nnumerical domains. In SAS, pages 3 17, 2006. [35] S. Sankaranarayanan, H. Sipma, and Z. Manna. Non-linear \nloop invariant generation using gr\u00a8obner bases. In POPL, pages 318 329, 2004. [36] S. Sankaranarayanan, \nH. B. Sipma, and Z. Manna. Constraint-based linear-relations analysis. In SAS, pages 53 68, 2004. [37] \nS. Sankaranarayanan, H. B. Sipma, and Z. Manna. Scalable analysis of linear systems using mathematical \nprogramming. In VMCAI, pages 25 41, 2005. [38] A. Schrijver. Theory of Linear and Integer Programming. \n1986. [39] H. Seidl, A. Flexeder, and M. Petter. Interprocedurally analysing linear inequality relations. \nIn ESOP, pages 284 299, 2007. [40] C. Wang, Z. Yang, A. Gupta, and F. Ivancic. Using counterexamples \nfor improving the precision of reachability computation with polyhedra. In CAV, pages 352 365, 2007. \n[41] Y. Xie and A. Aiken. Saturn: A sat-based tool for bug detection. In CAV, pages 139 143, 2005.  \n  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>A constraint-based approach to invariant generation in programs translates a program into constraints that are solved using off-the-shelf constraint solvers to yield desired program invariants.</p> <p>In this paper we show how the constraint-based approach can be used to model a wide spectrum of program analyses in an expressive domain containing disjunctions and conjunctions of linear inequalities. In particular, we show how to model the problem of context-sensitive interprocedural program verification. We also present the first constraint-based approach to weakest precondition and strongest postcondition inference. The constraints we generate are boolean combinations of quadratic inequalities over integer variables. We reduce these constraints to SAT formulae using bitvector modeling and use off-the-shelf SAT solvers to solve them.</p> <p>Furthermore, we present interesting applications of the above analyses, namely bounds analysis and generation of most-general counter-examples for both safety and termination properties. We also present encouraging preliminary experimental results demonstrating the feasibility of our technique on a variety of challenging examples.</p>", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022801", "email_address": "", "orcid_id": ""}, {"name": "Saurabh Srivastava", "author_profile_id": "81100062128", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1022802", "email_address": "", "orcid_id": ""}, {"name": "Ramarathnam Venkatesan", "author_profile_id": "81100057891", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022803", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375616", "year": "2008", "article_id": "1375616", "conference": "PLDI", "title": "Program analysis as constraint solving", "url": "http://dl.acm.org/citation.cfm?id=1375616"}