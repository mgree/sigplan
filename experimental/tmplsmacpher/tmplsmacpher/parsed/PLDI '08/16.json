{"article_publication_date": "06-07-2008", "fulltext": "\n Type-Preserving Compilation for Large-Scale Optimizing Object-Oriented Compilers * Juan Chen Chris \nHawblitzel Frances Perry Mike Emmi Microsoft Research Princeton University University of California, \nLos Angeles {juanchen, chrishaw}@microsoft.com frances@cs.princeton.edu mje@cs.ucla.edu Jeremy Condit \nDerrick Coetzee Microsoft Research {jcondit, dcoetzee}@microsoft.com Abstract Type-preserving compilers \ntranslate well-typed source code, such as Java or C#, into veri.able target code, such as typed assembly \nlanguage or proof-carrying code. This paper presents the imple\u00admentation of type-preserving compilation \nin a complex, large-scale optimizing compiler. Compared to prior work, this implementation supports extensive \noptimizations, and it veri.es a large portion of the interface between the compiler and the runtime system. \nThis paper demonstrates the practicality of type-preserving compilation in complex optimizing compilers: \nthe generated typed assembly language is only 2.3% slower than the base compiler s generated untyped \nassembly language, and the type-preserving compiler is 82.8% slower than the base compiler. Categories \nand Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language Constructs and Features Classes and \nobjects General Terms Veri.cation Keywords Type-preserving compilation, object-oriented compil\u00aders  \n1. Introduction Because of compiler bugs, compilers may not preserve safety prop\u00aderties of source-level \nprograms through the compilation process. Over the last decade, many researchers have proposed techniques \nfor removing the compiler from the trusted computing base by en\u00adsuring that the output of the compiler \nhas the same safety properties as the input. Necula and Lee proposed Proof-Carrying Code (PCC), in which \nlow-level code is accompanied by a safety proof that can be veri.ed ef.ciently [12]. Morrisett et al. \ndeveloped Typed As\u00adsembly Language (TAL), in which the compiler produces type\u00ad * The work by Frances \nPerry, Mike Emmi, and Polyvios Pratikakis was done during internship at Microsoft Research Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 \n13, 2008, Tucson, Arizona, USA. Copyright c . 2008 ACM 978-1-59593-860-2/08/06. . . $5.00 Polyvios Pratikakis \nUniversity of Maryland, College Park polyvios@cs.umd.edu annotated assembly code; here, the soundness \nof the TAL type sys\u00adtem guarantees that a well-typed TAL program is both type-safe and memory-safe [11]. \nLeroy built a certi.ed compiler with a for\u00admal proof that the compilation preserves the semantics of \nsource programs [8]. However, none of these compilers was a large-scale optimizing compiler that is designed \nfor use in real-world software development. Such compilers may be orders of magnitude larger in size \nthan research prototypes because of their support for advanced language features as well as their aggressive \noptimization. This paper presents the implementation of type-preserving com\u00adpilation in a large-scale \noptimizing compiler of approximately 200,000 lines. By type-preserving compilation, we mean that the \ncompiler preserves types through each intermediate representation, from source code to assembly code, \nallowing a lightweight veri.er to check the safety of the generated assembly code without trusting the \ncompiler itself. Our compiler is about an order of magnitude larger than previously published type-preserving \ncompilers,1 and it is used by about 30 developers on a daily basis. Although the compiler was originally \nimplemented with a typed intermediate representation, these types were previously discarded prior to \nin\u00adstruction selection and register allocation. With our enhancements, these types are preserved down \nto the assembly code, where they can be checked by a standalone veri.er. The source language of the compiler \nis Microsoft Common In\u00adtermediate Language (CIL) [6]. We chose a compiler for an object\u00adoriented language \nbecause object-oriented languages such as Java, C#, and C++ are among the most popular programming languages \nfor industrial software development. For our medium-level and low-level type system, we chose to implement \nLILC (Low-level Intermediate Language with Classes) [2] and SST (Simple Stack Types) [13] because they \nare sound and have decidable type check\u00ading. In addition, these type systems are expressive enough to \nsup\u00adport type-safe implementations of language features such as dy\u00adnamic dispatch and reference parameters. \nOur work makes the following contributions: 1. We implemented the LILC and SST type systems in an exist\u00ading \noptimizing compiler at reasonable cost. Although these type systems were published previously, they had \nnot been imple\u00admented or tested in a practical setting. Our compiler uses ap\u00adproximately 40 low-level \noptimizations, and with our changes 1 SpecialJ has about 33k lines for the compiler and 25k for the VCGen \nand the checker [4]. The TALx86 compiler has about 18k lines [7]. The LTAL compiler has about 50k for \nthe typed backend [3]. to the compiler, it can preserve types through almost all of these optimizations. \nFor the benchmarks we measured (ranging from 4MB to 20MB in executable size), the generated TAL code \nis 0.95-1.09 times slower than the base compiler s generated code, with a geometric mean of 1.02. The \nTAL compiler is 0.99-17.12 times slower than the base compiler, with a geometric mean of 1.83. The TAL \nchecking time is 1.61%-18.78% of the compi\u00adlation time, with a geometric mean of 5.59%. The TAL size \nis 1.69-2.52 times larger than the x86 code in terms of object .le size, with a geometric mean of 2.05. \nWe achieved this re\u00adsult by modifying approximately 10% of the existing compiler code (19,000 out of \n200,000 lines) and by adding 12,000 lines of code for TAL de.nition and veri.cation. 2. The TAL compiler \ngenerates explicit proofs about integer val\u00adues, and the TAL checker veri.es these proofs. In particular, \nthe compiler generates proofs that array indices are within ar\u00adray bounds, even for cases where the compiler \ns optimizations eliminate run-time bounds checks. The compiler generates the necessary loop invariants \nand proofs just before converting to the back-end intermediate representation, and it preserves the invariants \nand proofs through the back-end phases. 3. The TAL checker veri.es a large portion of the interface \nbe\u00adtween the compiler and the runtime system. The current system trusts the implementation of the runtime \nsystem, including the garbage collector, but it veri.es most of the object layout and garbage collection \ninformation that is generated by the compiler for use by the runtime system.  We believe that type-preserving \ncompilation is a practical way to verify the output of large-scale optimizing compilers. Types are a \nconcise way to represent data properties, and they impose few constraints on optimizations. Because many \nexisting compilers already use types and type checking internally, it is natural to extend these compilers \nto preserve types at a low level. These bene.ts give type-preserving compilation a number of advantages \nover using a certi.ed compiler, where the compiler it\u00adself is proven correct once and for all. Whereas \nexisting optimizing compilers can be extended to preserve types in a natural way, it is not obvious whether \nwe can certify a large-scale optimizing com\u00adpiler. For example, Leroy s work [8] showed that well-understood \ntransformations such as layout of stack frame are dif.cult to prove correct, and a production compiler \nwill have many such analyses and transformations. Furthermore, a realistic compiler is under con\u00adstant \nchange, which requires the compiler to be re-proven after each change. The rest of the paper is organized \nas follows. Section 2 pro\u00advides background information regarding the base compiler and our type systems. \nSection 3 presents our implementation of the type\u00adpreserving compiler. Section 4 discusses our techniques \nfor veri\u00adfying array bounds accesses, Section 5 presents our approach to verifying interaction with the \ngarbage collector, and Section 6 dis\u00adcusses our performance results. Section 7 discusses related work, \nand Section 8 concludes. 2. Background 2.1 The Base Compiler The base of our implementation is Bartok, \nwhich allows the use of managed languages like C# for general-purpose programming. The compiler has about \n200,000 lines of code, mostly written in C#, and is fully self-hosting. Bartok can compile each binary \nmodule individually or it can compile them all as a whole program with interprocedural opti\u00admization. \nWe use separate compilation mode to separate user pro\u00adgrams from the libraries that we currently do not \nverify. a geometric mean of 1.66. The architecture of Bartok is shown in Figure 1. Bartok trans\u00adlates \nprograms written in CIL, which is an intermediate represen\u00adtation for C# and other languages, into native \nx86 code, through three intermediate representations: High-level IR (HIR), Medium\u00adlevel IR (MIR), and \nLow-level IR (LIR). Bartok performs about 40 optimizations on the three IRs. HIR is similar to CIL, except \nthat HIR does not use stack-based computation. The type system of HIR is almost the same as that of CIL \n(or C#), consisting of primitive types, classes, interfaces, arrays, and so on. All object-oriented operations \nare primitives in HIR, such as virtual method invocation, type cast, and array access. These primitives \nare lowered during the translation from HIR to MIR. For example, virtual method invocation is translated \nto code for fetching the vtable out of an object, fetching a method out of the vtable, and calling the \nmethod on the object. The original MIR had the same type system as HIR, which is not expressive enough \nto represent the result of such a lowering. For example, no HIR types could represent the vtable of a \nclass that contains virtual methods. MIR is further lowered to LIR. Then several transformations are \nperformed on LIR: class layout, instruction selection, register allocation, and stack frame layout. The \noriginal LIR and the back end were untyped. The lowest level of LIR (essentially assembly) is written \nto object .les in a standard format. A standard linker links the object .les and creates native x86 executables. \n 2.2 The LILC Type System LILC is a typed intermediate language we implement as the typed MIR in Bartok \nto represent implementations of lowering object\u00adoriented primitives. LILC was developed by Chen and Tarditi \nfor compiling core object-oriented language features, such as classes, objects, and ar\u00adrays [2]. We decided \nto use LILC because it faithfully represents standard implementations of object layout, virtual method \ninvoca\u00adtion, and runtime libraries such as type test and array store check. Furthermore, LILC preserves \nnotions of classes, objects, and sub\u00adclassing, whereas other encodings compile those notions to func\u00adtional \nidioms (records and functions). It is easier to implement LILC in Bartok because LILC preserves those \nobject-oriented no\u00ad Figure 2. Object Layout for Point tions in the input language of Bartok. LILC has \nbeen proven sound and its type checking is decidable. This section explains the main ideas of how LILC \nrepresents objects and runtime libraries. Other features can be found in earlier work [2]. Classes and \nObjects LILC has both nominal (name-based) class names and structural record types. Each class C has \na correspond\u00ading record type R(C) that describes the object layout for the class C. For a typical layout \nstrategy, an object contains a vtable and a set of .elds. The vtable contains a tag a unique identi.er \nto identify the class at runtime, and a set of virtual methods. Suppose a class Point is de.ned as follows: \nclass Point {int x; virtual int distance(){...}} The class Point contains an integer .eld x and a virtual \nmethod distance that takes no parameters and returns an integer. The object layout of Point is shown \nin Figure 2. Type R(Point) represents this layout naturally: R(Point)= {vtable : {tag : Tag(Point), distance \n:(.a \u00ab Point.a) . int}, x : int} R(Point) is a record type with two .elds, vtable and x. The type for \nvtable is another record type containing .elds tag and distance. Note that objects and pointers to objects \nare used interchangeably in this paper. It should be clear from the context to which we refer. For example, \nthe vtable .eld is actually a pointer to a record of two .elds, but we omit the representation of the \npointer in the type and use a record type directly. The tag of the class Point identi.es Point at run \ntime. Its type is represented as Tag(Point), where T ag is an abstract type construc\u00adtor. LILC treats \ntags as abstract for simplicity. The vtable contains a method pointer for the virtual method distance \n. The method distance now takes one parameter the this pointer to re.ect the self-application semantics \nwhere we pass an object to a virtual method when calling the method on the object. The type of the this \npointer requires that the this pointer be an instance of Point or Point s subclasses. We explain this \npointer types later in this section. An instance of C can be coerced to and from a record of type R(C) \nwithout any runtime overhead. The coercions are runtime no-ops. Objects are lightweight because interesting \noperations are performed on records. Object and class notions are preserved to simplify the type system. \nThe separation of the nominal and structural types eliminates explicit structural recursion, because \nthe record type R(C) can re\u00adfer to any class name, including C itself. Also, the separation al\u00adlows a \nstraightforward typing of self-application semantics, which is the most challenging problem in typed \nintermediate languages for object-oriented languages. Virtual Method Invocation Virtual method invocation \nrequires a distinction between the static type and the dynamic type (actual runtime type) of an object. \nTo call a method m on an object o with static type C, we need to pass o as the this pointer to m, or \nat least pass an object that has the same dynamic type (or its subclasses) as o. Passing an object with \nthe same static type C may be unsafe. Consider the following example: void Unsafe(Point p, Point q){ \nvt = p.vtable; dist = vt.distance; dist(q); } This function is unsafe, even though the distance method \nfetched from p requires an object of Point and q is indeed an object of Point. The function can be called \nin an unsafe way if p is actually an instance of a subclass of Point and the subclass overrides distance \nto access .elds in the subclass but not in Point. To guarantee the soundness of virtual method invocation, \nLILC introduces exact notions of classes to represent dynamic types. Unlike source languages Java and \nC# where a class name C rep\u00adresents objects of C and C s subclasses, LILC uses C to represent only objects \nof exact C, not C s subclasses. LILC uses an exis\u00adtential type .a \u00ab C. a for objects of C and C s subclasses. \nThe notion \u00ab represents subclassing. The type variable a indicates the dynamic type, which must be a \nsubclass of C. Objects with source-level type (or static type) C are translated to have the ex\u00adistential \ntype in LILC. LILC has subtyping rules to guarantee that subclass objects can be used as superclass objects. \nSuppose an object has dynamic type t . Any virtual method fetched from the object has this pointer type \n.a \u00ab t. a, mean\u00ading only objects of t or t s subclasses. The Unsafe example is ill-typed in LILC. The \nobject p has type .a \u00ab Point.a in LILC. To invoke method distance on p, we .rst open p and introduce \na type variable \u00df for p s dynamic type. The type of the distance method fetched from p requires that \nthe this pointer be an object of \u00df or \u00df s subclasses. The type checker accepts passing p to distance \nbut rejects passing q because we cannot guarantee and q is an object of \u00df or \u00df s subclasses. Type-safe \nRuntime Libraries The type-safe runtime libraries are represented in the LILC type system exactly like \nuser programs. This gives the compiler freedom to inline and optimize them. Type Cast. Downward type \ncasts check at run time whether an arbitrary object is an instance of an arbitrary class or its subclasses. \nIn a typical implementation, each class stores a tag in its vtable. If C extends B, then the tag of C \nhas a pointer pointing to the tag of B. The pointers form a tag chain. Downward type cast fetches tags \nin the chain and compares them with the tag of the target class. This typical implementation is expressed \nas a well-typed poly\u00admorphic function in LILC that can be applied to arbitrary objects and arbitrary \nclasses. The key ideas are to use types to connect an object with the tag it contains, and to re.ne types \naccording to the tag comparison result. Two classes are the same if and only if their tags are equal. \nIf an object o has type t and the tag in o is equal to tag(C), then t = C. If one of the parent tags, \nwhich identi.es a parent class of t, is equal to tag(C), then t \u00ab C and o can be cast to C. Array Store \nCheck. Source languages such as Java and C# have covariant array types, that is, if A is a subtype of \nB, then Array(A) is a subtype of Array(B). Covariant array types re\u00adquire runtime store checks each time \nan object is stored into an array. If array a has static type array(B), to store an object of type B \nin a, we have to check whether the object has the actual ele\u00adment type of a because a might be an array \nof A. LILC uses invariant array types enclosed with existential types to express source-level array types. \nAn LILC array type is a subtype of only itself. The source-level array subtyping is transferred to subtyping \non the enclosing existential types in LILC. To store an object in an array that has type array(C), LILC \nprograms must explicitly check whether the object is an instance of the element type C, which can utilize \nthe previous type cast function. 2.3 The SST Type System SST is a simple, sound, and decidable type \nsystem that supports most common stack operations, aliased stack locations, and by\u00adreference parameters \n[13]. Other type systems for stacks either were undecidable or did not support by-reference parameters. \nA stack is viewed as a sequence of stack slots. The stack grows toward lower addresses. The stack can \nbe accessed by arbitrary pointers to the stack. But only changing the value of the stack pointer sp can \ngrow or shrink the stack. SST uses a stack type to describe the current stack state. SST checks one function \nat a time. Each function can see only its own stack frame. The previous stack frames are abstracted by \na stack variable. Each stack slot in the current stack frame is labeled by a loca\u00adtion. A pointer to \na stack slot labeled by location . has a singleton type Ptr(.). The stack type tracks mapping from stack \nlocations to current types of values stored at those locations. We call such mapping capabilities. Capabilities \nare linear, that is, they cannot be duplicated. Because of the separation between singleton pointer types \nand capabilities, the capabilities can evolve, independently of the pointer types, to track updates and \ndeallocation. SST uses a non-commutative, non-associative operator :: to glue capabilities together to \nform a stack type: .2 : int :: .1 : int :: .0 : . means that two integers are on top of the stack (at \nlocations .1 and .2) and the rest of the stack is abstracted as a stack variable .. Capabilities glued \nby :: form the spine of a stack. To represent aliasing, SST introduces a . operator to attach a capability \nto a stack type. The capability describes an aliased location to some location inside the stack type. \nTherefore, the scope of the capability is the stack type: the capability is safe to use as long as the \nstack type is not modi.ed. To guarantee safety, the scope can be only expanded to a larger stack type \nbut not contracted. By-reference Parameters. Consider a function swap that has two by-reference integer \nparameters x and y: void swap(ref int x, ref int y) {...} Suppose the compiler pushes parameters onto \nthe stack from right to left. In SST, the stack type in the precondition of swap is next(next(.0)) : \nPtr(.x) :: next(.0): Ptr(.y) :: .0 :(. .{.y : int}.{.x : int}). Upon entry to swap, the stack holds the \narguments x and y on its top, each of which is a pointer to some aliased location inside .. Note that \naliased locations .x and .y may appear anywhere in ., in any order. In fact, .x and .y may be the same \nlocation. The swap function can pass x and y to other functions, further expanding the scopes of .x and \n.y. But swap cannot return a reference to a local variable de.ned in itself, because this contracts the \nscope of the reference to .. 2.4 Arrays Loads and stores to array elements are primitive operations \nin CIL. Naively, each array element access requires a run-time bounds check to ensure that the element \nindex is within the array s bounds. Compilers break up each array access into a sequence of more primitive \nmachine instructions. Many compilers, including Bartok, will also optimize away many of the bounds checks. \nThe TAL type system must be able to ensure that every access is within bounds even after the compiler \ns transformations and optimizations. Figure 3. Architecture of the New Compiler To solve this problem, \nour type system incorporates tech\u00adniques from earlier work. First, following Xi and Harper s work on \nDTAL [16], it uses singleton types to represent known infor\u00admation about integer values. For example, \nthe number 5 has type int, but 5 also has the more speci.c type S(5), the singleton type of integers \nequal to 5. More interestingly, a variable x might have singleton type S(a), where a basic block s pre-condition \nspeci.es a constraint a< 5 on the integer type variable a. The singleton type together with the constraint \nensure that x is less than 5. DTAL used an arithmetic constraint solver to verify that the pre-conditions \nfor basic blocks and array operations are satis.ed. Unfortunately, the constraint solver sits in the \nTAL veri.er s trusted computing base, and for decidability s sake the solver is limited to a subset of \narithmetic. Therefore, our type system follows the approach ad\u00advocated by Crary and Vanderwaart [5] and \nShao et al. [14], which keeps DTAL s singleton types and pre-conditions, but also allows explicit proofs \nof pre-conditions in the TAL program. In this ap\u00adproach, the TAL veri.er only needs to check the supplied \nproofs, and does not need to automatically solve arithmetic constraints. Of course, the compiler must \ngenerate the proofs before the TAL veri.er can check the proofs; proof generation for arrays was be\u00adyond \nthe scope of Crary and Vanderwaart [5] and Shao et al. [14]. Section 4 describes how Bartok generates \nthese proofs.  3. Implementation of a Type-Preserving Compiler This section provides implementation \ndetails for the new type\u00adpreserving compiler. In order to preserve types, we must implement typed MIR \nand LIR using the LILC and SST type systems. We must also ensure that types are preserved across low-level \noptimizations. The architecture of the new compiler is shown in Figure 3. The original MIR type system \nwas enhanced to express lowering of object-oriented primitives. The original untyped LIR and untyped \nbackend were modi.ed to preserve types. When generating object .les, the compiler adds a new section \nfor type information. Other sections in the object .le have exactly the same format as the ones generated \nby the base compiler. The veri.er disassembles the code and the type information and then veri.es the \ntyped assembly code. A standard linker generates x86 executables from the object .les and discards the \ntype information section. Section 3.1 describes special handling of type variables to allow optimizations. \nSection 3.2 shows how LILC and SST types are represented in Bartok. Section 3.3 lists the optimizations \nperformed on typed MIR and LIR. 3.1 Type Variables Type variables are important to guarantee soundness, \nas shown in Section 2.2. A type variable that identi.es the dynamic type of an object should be associated \nwith only that object. Like traditional typed calculi, LILC introduces a fresh type variable each time \nan existential type is opened. The type variable identi.es the dynamic type of an object. The type variable \nis in scope until the end of the basic block. The type checker rejects the Unsafe function in Section \n2.2 (translated to LIR): ' (1) p= open(\u00df)(p); // p' : \u00df ' (2) q= open(.)(q); // q' : . ' (3) vtable = \np.vtable; (4) dist = vtable.distance // dist :(.d \u00ab \u00df. d) . int  ' (5) dist(q) Instructions (1) and \n(2) introduce distinct type variables \u00df and . for the dynamic types of p and q respectively. The method \ndist expects an object of \u00df or \u00df s subclasses and q' does not have that type. This strategy hinders common \noptimizations, though. For exam\u00adple, two consecutive virtual method invocations on the same object p \nmay be translated to the following LIR code: p1 = open(a)(p); vtable1 = p1.vtable; m1 = vtable1.m1 m1(p1,...) \np2 = open(\u00df)(p); vtable2 = p2.vtable; m2 = vtable2.m2 m2(p2,...) The object p does not change between \ntwo calls, and thus it is sound to apply common subexpression elimination (CSE) to combine the two opens \nand vtable fetches. But the two distinct type variables a and \u00df prevent CSE from optimizing the code. \nTo work around this problem, we separate type variables used in the programs and during type checking, \nto both allow optimizations and guarantee soundness. In typed MIR, type variables in programs do not \nidentify dy\u00adnamic types of objects. It is not required that each open instruction introduces a fresh \ntype variable. In fact, type variables are grouped by their bounds. Two type variables that have the \nsame upper and lower bounds are considered the same. The bounds of type vari\u00adables have to be accurate \nbecause optimizations may query infor\u00admation about members in the type variables, which relies on the \nbounds. Opening two objects with the same existential type can reuse a type variable. For example, the \ntwo open instructions in the above example can use the same type variable a. The sharing of type variables \nallows CSE to optimize the code sequence to: p1 = open(a)(p); vtable1 = p1.vtable; m1 = vtable1.m1 m1(p1,...) \nm2 = vtable1.m2 m2(p1,...) Note that if p changes between two calls or we call two methods on two distinct \nobjects, CSE cannot unsoundly optimize the code because it cannot tell whether the two opens are the \nsame. The discrepancy of type variables used in programs and in type checking may cause problems at control \nmerge point. Suppose basic block B1 has an open instruction x = open(a)(p1) and block B2 has x = open(a)(p2) \nand B1 and B2 merge to block B3. This is acceptable in the program because x has only one type a. But \nduring type checking, the checker introduces two type variables for x in B1 and B2, which results in \ndisagreement of x s type in B3. Typed MIR solves this problem by specifying for each basic block its \nprecondition, including for the type variables visible in the basic block. In the above merge example, \nblock B3 uses a new type variable merged from the ones in B1 and B2 for x s type. Optimizations may move \nthe code across basic block bound\u00adaries. To free the optimizations from maintaining preconditions of \nbasic blocks, typed MIR uses a two-phase type checking. The .rst phase infers the precondition for each \nbasic block, and the second phase checks the block. The .rst type-inference phase mainly deals with merging \ntype variables. Merging is similar to a union operation. Merging a set of unrelated type variables produces \na fresh type variable that tracks the components from which the type variable was merged. Merging a type \nvariable with one of its components results in the type variable itself. For ef.ciency, the type checker \nperforms a few straightforward optimizations: variables whose types contain no type variables are not \ntracked in type-checking environments because their types do not change; if a method has only such variables, \nthen type inference is not applied. 3.2 Type Representations The implementation of LILC and SST requires \nmany new types such as type variables, existential types, and polymorphic types. Naively adding them \nto the compiler would incur signi.cant changes to the existing code, including type representations, \ntype checking, and optimizations. Our implementation changed or added only 10% of the code in the compiler, \nbecause of the choices we made in type representation and type checking. 3.2.1 Typed MIR Representation \nBartok, which is written in C# itself, uses a set of classes to rep\u00adresent MIR types, which include classes \nin the source programs, function types, and so on. ClassType represents classes. Inter\u00adfaceType represents \ninterfaces. FunctionType represents func\u00adtion types. These classes form a hierarchy naturally. For example, \nClassType and InterfaceType are subclasses of NamedType. Ir-Type is the root class. Each instance of \nthese classes corresponds to a type. An instance of ClassType represents a class in the source pro\u00adgram. \nThe instance contains all information related to the corre\u00adsponding class, and has a table of all the \nmembers (.elds and meth\u00adods) of the class. Typed MIR views an instance of ClassType as a combination \nof both an LILC class name and the corresponding record type that de\u00adscribes the class layout. The instance \nhas both name-based informa\u00adtion (class name, superclasses and interfaces) and structure-based information \n(members). This way, typed MIR can reuse ClassType without change, yet still preserve the two views of \nclasses in LILC. Another bene.t of reusing ClassType is that typed MIR does not need to coerce between \nobjects and records, which saves many co\u00adercion instructions and makes the code more concise. Those coer\u00adcions \nhave no runtime overhead, though. The class name C (an instance of ClassType) in typed MIR also serves \nas an encoding of the LILC existential type .a \u00ab C. a. As a result, optimizations can remain as they \nare: they do not have to deal with the new existential types. Also, the encoding makes it unnecessary \nto coerce between existential types and class names each time a .eld is fetched from an object or an \nobject is created. Typed MIR has a new type ExactClassType to represent exact classes. In a few cases, \ntyped MIR still needs explicit existential types for those types that don t have the format .a \u00ab C. a \nand therefore cannot use the encoding mechanism. An example is the interface ta\u00adble entry. For this purpose, \ntyped MIR has a class ExistentialType. Typed MIR adds a class TypeVarClass for type variables that will \nbe instantiated with classes and interfaces. To represent the explicit implementations of tags (or runtime \ntypes) in the Bartok compiler, typed MIR introduces RuntimeType to model tags. An instance of RuntimeType \nthat represents the tag of class C, denoted as Runtime(C) , contains a reference to the representation \nof class type C. Tags of different classes have different types. Similarly to tags, typed MIR introduces \na new type VTableType for vtables. An instance of VTableType that represents the vtable of class C (denoted \nas Vtable(C) ) contains a reference to the class C. The new types ExactClassType, TypeVarClass, RuntimeType, \nand VTableType all inherit from ClassType. Therefore, each in\u00adstance of these types is regarded as a \nnormal class and contains a table of its members. The members are added by need to the ta\u00adble. For example, \nvirtual methods can be fetched as normal .elds out of vtables. 3.2.2 Typed LIR Representation The implementation \nof Typed LIR extends SST to support stack\u00adallocated large values, such as .oating-point numbers and struc\u00adtures. \nBasic SST assumes that stack-allocated values are word\u00adsized. Typed LIR changes the symbolic representation \nfor stack loca\u00adtions to . + n , meaning n bytes between the location labeled by .. For aliased locations, \nthe offset n is always 0. Symbolic repre\u00adsentations for stack locations in SST are not suitable for typed \nLIR, because the difference between two stack locations varies. The new location representation makes \nit easier to tell whether a location is an aliased location and to compute the new location when moving \nalong the stack. Typed LIR also extends SST to support other features such as objects, classes, and arrays. \nIn fact, typed LIR can reuse most types in typed MIR, for example, class types, interface types, array \ntypes, existential types, etc. To avoid duplicating the types, Bartok allows converting an MIR type to \nan LIR type by simply wrapping the MIR type.  3.3 Optimizations Our new compiler supports more than \n40 optimizations of the base Bartok compiler, with only 2 optimizations unsupported (see Section 3.3.3). \nOur experience shows that most optimizations can be easily modi.ed to preserve types. This is partly \ndue to our effort to design suitable type representations. 3.3.1 MIR Optimizations Major optimizations \nperformed at MIR level are as follows: Copy propagation  Constant propagation  Constant folding  \nCommon subexpression elimination (including redundant load elimination)  Dead-code elimination  Loop-invariant \nremoval  Reverse copy propagation of temporaries, which transforms t = e; v = t to v = e  Optimizing \nconvert instructions, which compacts chains of con\u00advert instructions  Jump chain elimination, which \nremoves jumping to jump in\u00adstructions  Short circuiting, which removes redundant tests of boolean vari\u00adables \n Loop header cloning, which turns while loops into do-while loops  Inlining (with heuristics to limit \ncode size increase)  Elimination of unreachable classes, methods, and .elds (tree\u00adshaking)  Because \nour original MIR has type information already, the im\u00adplementation of typed MIR only needs to change \nthree optimiza\u00adtions. Others are performed on typed MIR as they are on the orig\u00adinal MIR. Two optimizations \nCSE and treeshaking were changed to support the new operators (such as open ) and new types. CSE needs \nto index all subexpressions. Treeshaking analyzes all instruc\u00adtions to determine which types are accessed. \nThe inlining optimiza\u00adtion was changed to support cloning new operators and to sup\u00adport inlining of polymorphic \nfunctions. The changes are local and straightforward. The fact that we can reuse almost all optimization \ncode con.rms our typed MIR design choices. 3.3.2 LIR Optimizations Major optimizations on LIR include: \n Copy propagation, including stack locations  Constant propagation  Dead-code elimination  Jump chain \nelimination  Reverse common subexpression elimination of load effective ad\u00address computations  Peephole \noptimizations  Elimination of redundant condition code setting  Boolean test and branch clean up  \nFloating point stack optimizations  Conversion of add to lea (load effective address), for exam\u00adple, \necx = eax + ebx to lea ecx, [eax + ebx]  Graph-coloring register allocation  Code layout  Conversion \nof switch instructions to jumping to entries in jump tables  Changes on LIR optimizations are mostly \nto propagate types, since the original LIR was untyped. A few optimizations need more signi.cant changes. \nThe optimization that converts add instructions to lea in\u00adstructions may introduce invalid effective \naddresses. It is still safe because the addresses will never be used to access the memory. But the type \nchecker needs to differentiate such cases and prevent those addresses from being used unsafely. During \nregister allocation, the compiler .xes the stack frame and assigns stack slots to callee-save registers, \nfunction arguments, and spills. We need to record the types of those stack-resident val\u00adues to type check \nstack allocation upon entry to a function. The checker needs to know the intended types for the newly-allocated \nstack slots because of stack-allocated structures. Otherwise, the checker has dif.culty .nding out the \nboundaries of slots. The origi\u00adnal SST did not need such annotations because each slot was word\u00adsized. \nWhen jump tables are created for translating switch instruc\u00adtions, each entry in the jump table can be \nviewed as a function entry point. We need to give types as preconditions to the entries in the jump table. \nAll entries have the same preconditions, so the jump table can be typed as an array of function pointers. \n 3.3.3 Unsupported Optimizations The base Bartok compiler optimizes memory allocation by inlining the \nallocator implementation directly into the compiler-generated code. Currently the TAL compiler cannot \ntype-check the internal implementation of the memory allocator, so it cannot support this inlining. Our \ndisabling of this optimization is responsible for most of the difference in execution time between code \ngenerated by the base Bartok compiler and the type-preserving compiler, as reported in section 6. Also, \nas explained in Section 4, we do not support the Array Bounds Check elimination on Demand (ABCD) algorithm. \n  4. Arrays and Proofs Bartok implements several optimizations that can eliminate run\u00adtime bounds checks \nfrom array accesses. First, the common subex\u00adpression elimination attempts to consolidate repeated bounds \nchecks for the same array element. For example, the C# expres\u00adsion a[i] = 1 -a[i] makes two accesses \nto the same array element; Bartok s common subexpression elimination removes the bounds check from the \nsecond access. Second, an induction variable anal\u00adysis looks for loops where an array index variable \nis initialized to a non-negative number, is incremented by one in each loop itera\u00adtion, and is checked \nagainst an array length in the loop condition. Third, Bartok implements the Array Bounds Check elimination \non Demand (ABCD) algorithm [1], which infers unnecessary bounds checks by solving a system of difference \nconstraints (constraints of the form x = y + c, where c is a constant). This section describes our extensions \nto Bartok to generate proofs of array access safety, including proofs for run-time bounds checks and \nproofs for checks eliminated by common subexpression elimination and by induction variable analysis, \nand then discusses why we were unable to generate proofs for Bartok s ABCD algo\u00adrithm. We illustrate \nthe proof generation with a simple C# example: public static void Main(string[] args) {for (inti = 0;i \n< args.Length; i++) args[i] = null; } With our proof-generation extensions, Bartok automatically gen\u00aderates \nthe following TAL code for the Main method s inner loop (omitting the stack types and irrelevant register \ntypes, and renam\u00ading and reformatting for clarity): B1 (a:Int, \u00df:Int, .:Obj, ai:(ArrIndex .a), al:(ArrLen \n.\u00df), gl:(Ge0LtMax a)) {eax:Sint32(a), ebx:Sint32(\u00df), ecx:Sstring[](.)} = mov (dword ptr [ecx + eax *4+8] \nusing ai), 0 add eax, (1 : Sint32(1)) ; coerce eax to Sint32(Succ a) using (addToSucc a) cmp eax, ebx \njl(p:Lt (Succ a) \u00df) B1(Succ a, \u00df, ., incrIndex .a\u00df p gl al, al, incrGe0LtMax .a\u00df p gl al) In this code, \nthe basic block B1 is polymorphic over three type variables: a represents an array index, \u00df represents \nan array length, and . represents an array. B1 is also polymorphic over three proof variables. First, \nai is a pre-condition that requires ArrIndex .\u00df, which is de.ned to be equivalent to 0 = a< ALen(.), \nwhere ALen(.) is the length of the array .. Similarly, al and gl require that \u00df = ALen(.) and 0 = a< \nMAX, where MAX is the max\u00adimum 32-bit signed integer. (We use the optimization-speci.c ab\u00adbreviations \nArrIndex, ArrLen, and GeLtMax to reduce the gener\u00adated annotation size and to reduce veri.cation time.) \nThe registers eax, ebx, and ecx hold the index, array length, and array, respec\u00adtively, where the type \nSint32(X) is the singleton type of 32-bit signed integers equal to X and St[](X) is the singleton type \nof arrays equal to X. The .rst instruction of the loop moves null into array element a, where ecx + 8 \nis the address of element 0, and each element occupies 4 bytes. The type checker demands a proof that \neax hold a valid index for array held in ecx; the annotation using ai supplies this proof. (Bartok can \nalso generate proofs for the more complicated address computations that occur for arrays of large C# \nstructs, but we discuss just the simplest case here.) The add instruction increments eax, coercing the \nresulting singleton type from a +1 to Succ a for conciseness, where Succ stands for successor. The cmp \nand jl instructions branch back to the beginning of the block if a +1 less than the array length \u00df. The \nbranch requires instantiations of B1 s type variables and proof variables with valid types and proofs \nto satisfy B1 s pre-condition. In this case, the axiom incrIndex proves that Succ a is an in-bounds array \nindex, given proofs that Succ a<\u00df, that 0 = a < MAX (so that a +1 doesn t over.ow), and that \u00df = ALen(.). \nHere, the jl instruction supplies the proof variable p, asserting that Succ a<\u00df in the taken branch. \nBroadly speaking, there are two ways a compiler might gener\u00adate proofs of array access safety. First, \na compiler can generate the proper proofs during the array bounds check introduction and dur\u00ading the \narray bounds check elimination optimizations. In this case, the compiler must preserve the proofs through \nsubsequent com\u00adpiler phases, using proof-preserving compilation [14]. Alternately, a compiler can postpone \nproof generation until after all compiler phases have completed, and try to infer proofs by analyzing \nthe compiler-generated assembly language code. We decided to use some of both approaches, to compare \ntheir relative strengths. We postpone proof generation until the conver\u00adsion from MIR to LIR, which happens \nwell after the HIR s in\u00adtroduction of explicit array bounds check instructions, and after MIR s common \nsubexpression elimination and induction variable optimizations. We change Bartok to annotate HIR and \nMIR code with hints that highlight the variables and constants used for ar\u00adrays, array indices, and array \nlengths. Just before MIR-to-LIR con\u00adversion, Bartok runs two data.ow analyses. The .rst analysis prop\u00adagates \nthe highlighted variables and constants forward as they .ow from one variable to another. The analysis \nclassi.es these variables and constants into equivalence classes at each program point, intro\u00adducing \na type variable for each equivalence class. In the example above, a represents the equivalence class \ncontaining just the vari\u00adable eax. The second analysis computes properties about the equiv\u00adalence classes \n(for example, \u00df holds the length of array .); each property becomes a proof variable (e.g. al ). If the \nanalyses fail to .nd a safety proof for an array access, they insert an extra run\u00adtime bounds check; \nthis ensures that compilation to veri.able TAL can proceed even if the HIR and MIR optimizations outsmart \nthe analyses. After conversion to LIR with proof annotations, the compiler preserves the proofs through \nthe LIR phases. For example, the LIR jump chain elimination optimization takes jumps from block Bi to \nblock Bj, where Bj consists of a single jump to block Bk, and mod\u00adi.es Bi to jump directly to Bk. Both \nthe Bi-to-Bj jump and Bj-to-Bk jump may have proofs that prove their target s precondition, so we modi.ed \nthe compiler to compose these Bi-to-Bj and Bj-to-Bk proofs together to produce valid Bi-to-Bk proofs. \nNo optimization or transformation required anything more sophisticated than this to preserve proofs. \nNevertheless, due to the large number of optimiza\u00adtions, we did not implement proof preservation for \nevery instance of every optimization. In particular, constant propagation may want to replace a variable \nof type S(a) with a constant c of type S(c); rather than .xing the types (e.g. by proving a = c), we \nsimply don t propagate c to the S(a) variable in this case. Our experience so far shows that practical \nproof preservation is possible, but requires effort proportional to the number of compiler optimizations \nand transformations. Implementing the proof infer\u00adence for the MIR-to-LIR conversion required less effort \nthan im\u00adplementing the LIR proof preservation. Nevertheless, proof preser\u00advation has one potential advantage \nover inference: if the initial in\u00adput to the compiler already has proofs of array access safety (or other \ninteger-related properties), a proof-preserving compiler can preserve these proofs, even if the proofs \nare too sophisticated for inference to re-discover [14]. For example, we ve used Bartok to compile a \nhand-written LIR program, hand-annotated with proofs showing that the program correctly computes the \nfactorial, to TAL code annotated with proofs showing correct factorial computation. This works even though \nBartok knows nothing about factorials. 4.1 ABCD We also attempted to generate safety proofs for Bartok \ns ABCD array-bounds check elimination optimization, but the proofs seemed to require axioms that are \nunsound for modular arithmetic. This observation led to a counterexample: void F(int[] arr, int i) { \nif (i <= arr.Length) { intj = i -1; if (j >= 0) arr[j]++; }} At the arr[j]++ statement, Bartok s ABCD \nanalysis correctly con\u00adcludes that 0 = j and i = arr.Length and j = i - 1, but then incor\u00adrectly eliminates \nthe bounds check based on the erroneous conclu\u00adsion that 0 = j = arr.Length - 1. This conclusion fails \nwhen i is the minimum signed 32-bit integer, so that i -1 under.ows. The out-of-bounds array store clobbers \nnearby memory, undermining type safety. Since the original paper on ABCD [1] did not claim to work for \nmodular arithmetic, it was no surprise that the algorithm might fail for 32-bit integer indices, and \nin fact Bartok already had heuristics to squelch the ABCD algorithm whenever it saw large integer constants. \nUnfortunately, the method above contains no constants larger than 1, so Bartok still (incorrectly) eliminates \nthe bounds check. We were disappointed at not generating safety proofs for ABCD, but we were grateful \nthat our attempt led us to discover a real vulnerability in Bartok.  5. GC Information Veri.cation Object \n.les generated by the compiler also include information used by the garbage collector. This information \nallows the garbage collector to identify all references on the heap and stack at run time. If this information \nis incorrect, the garbage collector may attempt to trace non-reference data or prematurely reclaim live \ndata, both of which can lead to safety violations. Thus, in order to ensure safety, we must check that \nall of the garbage collection information in the object .les is consistent with our type information. \nThe compiler generates three kinds of information for the garbage collector s use: The static data table \nis a bitmap that indicates which words in the static data section contain traceable references. The object \ntable in each class s vtable contains information that allows the garbage collector to locate reference-containing \n.elds in instances of that class. The stack table maps every valid return address in the program to information \nabout how to .nd references in the caller s stack frame. At collection time, the collector walks the \nstack, using the information for each return address on the stack to .nd references in each frame. For \neach kind of information, we must verify that the garbage collector s information is consistent with \nour type information. Checking the static data table is straightforward: we simply com\u00adpare each bit \nin the bitmap with the type of the corresponding word of memory. Checking the object tables is similar: \neach vtable ob\u00adject in static data is identi.ed by a special type, so we can extract the word containing \npointer information and use it to check the cor\u00adresponding class s type information. We give this word \na singleton type that depends on the class, which ensures that it cannot be al\u00adtered and that invalid \nvtables cannot be constructed by user code. We currently omit checks for .elds of classes de.ned in other \ncom\u00adpilation units when those .elds are not directly accessed by the user s code. Checking stack tables \nis the most involved aspect of our garbage collector veri.cation. After checking each call instruction, \nwe look up the garbage collection information for the associated return ad\u00address. This information indicates \nwhich arguments, spill slots, and callee-saved registers contain references, which can be compared against \nthe stack type at that call instruction. In addition, this infor\u00admation indicates whether callee-saved \nregisters have been saved on this stack. In order to ensure that these values are traced by the col\u00adlector, \nwe must ensure that they were correctly saved to the stack, which we can do by comparing the fresh type \nvariables assigned to each callee-saved register with the type of the corresponding stack slot. Finally, \nwe must ensure that the return address saved on the stack has not been replaced with a return address \nthat happens to have the same type but is not a valid index into the garbage collec\u00adtor s tables. To \ndo so, we re.ne the function pointer type to create an immutable function pointer type that can only \nbe pushed on the stack with a call instruction and can only be popped off the stack with a ret instruction. \nFinally, we must ensure that any write barriers required by a concurrent garbage collector have been \nplaced appropriately by the compiler. In our compiler, write barriers are implemented as a method invocation \nthat does any appropriate checks in addition to performing the write. Thus, we can check write barriers \nby making locations that require barriers immutable; any writes to these locations must be performed \nby invoking the write barrier, which is trusted code outside of the object .le. Currently, we use a collector \nthat does not require write barriers. When implementing these checks, we found it helpful to adapt the \nsource code for the runtime system directly for use in the ver\u00adi.er. The encodings for these garbage \ncollection tables are quite complex, so we reused the runtime system s table decoder, adding further \nchecks where necessary to guard against invalid encodings. To design the checker, we abstracted the garbage \ncollector s ref\u00aderence scanning code, replacing all direct memory references with references to the associated \ntype information. This approach gave us greater con.dence that our veri.er s checks match the assump\u00adtions \nof the runtime system.  6. Measurements The compiler has about 150 benchmarks for day-to-day testing. \nThis section shows the numbers for the seven largest ones (Table 1). The performance numbers were measured \non a PC running Win\u00addows Vista with two 2.66GHz CPUs and 4GB of memory. We use separate compilation mode \nto compile the benchmarks separately from the libraries. The runtime uses a mark-sweep garbage collec\u00adtor. \nThe running times are averaged over .ve runs of each program. Execution Time. Table 2 shows the performance \ncompari\u00adson between TAL, the base-compiler-generated code, and CLR\u00adgenerated code. The fourth column \nTAL/Base is the second col\u00adumn TAL divided by the third column Base . The TAL code is 0.95-1.09 times \nslower than the base compiler s generated code, with a geometric mean of 1.02. The TAL code is slightly \nless ef.\u00adcient than the base compiler-generated code because the new com\u00adpiler does not support inlining \nof heap allocations. The two bench\u00admarks ahcbench and asmlc are allocation-intensive, and therefore are \naffected most. The TAL code is nearly as ef.cient as the base\u00adcompiler-generated code if the base compiler \nturns off inlining of heap allocation. Name Description Executable Size (in bytes) ahcbench An implementation \nof Adaptive Huffman Compression. 4,820,992 asmlc A compiler for Abstract State Machine Language. 20,586,496 \nlcscbench The front end of a C# compiler. 7,766,016 mandelform An implementation of mandelbrot set computation. \n12,775,424 sat solver selfhost1421 An implementation of SAT solver written in C#. A version of the Bartok \nCompiler. 4,943,872 7,847,936 zinger A model checker to explore the state of the zing model. 13,074,432 \n Table 1. Benchmarks. Benchmark TAL Base TAL/Base CLR mandelform 1262.93 1234.92 1.02 1423.14 selfhost1421 \n256.30 235.46 1.09 93.99 ahcbench 4.08 3.75 1.09 4.50 lcscsbench 6.82 6.66 1.02 10.55 asmlc 0.96 0.92 \n1.04 1.58 zinger 2.89 3.01 0.96 3.32 sat solver 4.50 4.76 0.95 4.31 geomean 1.02 Benchmark TAL Base \nTAL/Base mandelform 117088 65445 1.79 selfhost1421 18871862 9012642 2.09 ahcbench 148880 61077 2.44 lcscsbench \n14205932 8004809 1.78 asmlc 31851094 18856426 1.69 zinger 2400600 1076470 2.23 sat solver 883589 351134 \n2.52 geomean 2.05 Table 2. Execution Time (in seconds) Benchmark TAL Base TAL/Base mandelform 16.15 \n15.65 1.03 selfhost1421 1087.99 63.54 17.12 ahcbench 6.37 6.24 1.02 lcscsbench 153.60 78.09 1.97 asmlc \n158.78 160.76 0.99 zinger 25.59 17.68 1.45 sat solver 9.64 7.18 1.34 geomean 1.83 Table 3. Compilation \nTime (in seconds) Compilation Time. Table 3 shows the performance comparison of the TAL compiler and \nthe base compiler. The TAL compiler is 0.99-17.12 times slower than the base compiler, with a geometric \nmean of 1.83. The slowdown is largely due to type inference and writing type information to object .les. \nTyped MIR and LIR have type inference to infer the precondition of each basic block. Typed LIR type inference \nand writing type information to object .les ac\u00adcount for 49% of the compilation slowdown in lcscbench, \n20% in zinger, and 13% in sat solver, and 70% in selfhost1421. The bench\u00admark selfhost1421 has signi.cant \nslowdown because of inef.cient type inference for arrays. Other slowdown is due to a larger number of \nIR instructions and MIR/LIR types. Typed MIR and LIR insert coercion instructions to change types, most \nof which are no-ops at runtime. Object File Size. Table 4 shows the size comparison of object .les generated \nby the TAL compiler and the base compiler. The TAL object .les are signi.cantly larger than those generated \nby the base compiler: 1.69-2.52 times larger, with a geometric mean of 2.05. We decided to make the TAL \nchecker simple because the checker is in the TCB. Therefore, the TAL programs are extensively annotated \nwith types. Excluding the type information, the object .les generated by the TAL compiler are as small \nas those generated by the base compiler. A smarter checker might not require as much type information \nbecause it could infer type information, but would also be more dif.cult to trust or verify than a simple \nchecker. Type Checking Time. Table 5 shows the TAL checking time compared with compilation time. The \nTAL checking time is 1.61%- Table 4. Object File Size (in bytes) Benchmark Checking Time (% of Compilation \nTime) mandelform 1.61 % selfhost1421 5.67 % ahcbench 4.40 % lcscsbench 18.78 % asmlc 2.14 % zinger \n18.13 % sat solver 5.87 % geomean 5.59 % Table 5. Checking Time Compared with Compilation Time 18.78% \nof the compilation time, with a geometric mean of 5.59%. Type checking TAL is fast, as in prior TAL compilers. \n 7. Related Work Many compilers use typed intermediate representations. We com\u00adpare our compiler with \na few ones that preserve types to assembly code. None of them has as many optimizations as Bartok has, \nor supports compiler-runtime interface veri.cation. SpecialJ [4] is a certifying compiler for Java. SpecialJ \nproduces PCC binaries (code with a safety proof), which provides a more general framework than TAL. It \nsupports exception handling via pushing and popping handlers. (Our system uses exception han\u00addling tables, \nand we cannot yet check the tables.) SpecialJ treats all runtime libraries as primitives, and thus it \ncannot verify them or inline them into user programs. The SpecialJ paper used hand\u00adoptimized code as \nexamples and mentioned only two optimizations that SpecialJ supported. The TALx86 compiler is a type-preserving \ncompiler from a C\u00adlike language to x86 code [10], but it does not perform aggressive optimizations such \nas array bound check elimination. The type system lacks support for many modern language features such \nas CIL s by-reference parameters. The LTAL compiler compiles core ML programs to typed SPARC code [3]. \nIt has a minimal runtime (no GC) and does not support proofs about integers. Leroy implemented a certi.ed \ncompiler from a C-like language to PowerPC assembly code [8], verifying most of the compiler code itself, \nnot just the compiler s output. The compiler has about 5,000 lines of code with few optimizations. The \nproof of the compiler is about 8 times bigger in size than the compiler. It is dif.cult to scale to large-scale \ncompilers like Bartok. Menon et al. proposed an SSA presentation that uses proof vari\u00adables to encode \nsafety information (for example array bounds) [9]. It supports optimizations by treating proof variables \nas normal vari\u00adables. SSA is not suitable for TAL. Section 2.4 has discussed other work about verifying \narray-bounds checking. Vanderwaart and Crary developed a type theory for the compiler-GC interface, but \ndid not have an implementation [15]. 8. Conclusion In this paper, we have presented a large-scale, type-preserving, \nop\u00adtimizing compiler. This work shows that preserving type informa\u00adtion at the assembly language level \nis practical even in the presence of aggressive low-level optimizations. We were also able to imple\u00adment \na lightweight veri.er that checks the safety of our compiler s output, including many aspects of its \ninteraction with the runtime system. We believe that type-preserving compilation is a useful ap\u00adproach \nto minimizing the trusted computing base. Acknowledgments We would like to thank David Tarditi, Jim \nLarus, and Galen Hunt for their guidance and support of this work, David Tarditi and the Bartok team \nfor their help with Bartok-related issues, and anony\u00admous reviewers for their comments on an earlier \nversion of the pa\u00adper. References [1] R. Bodik, R. Gupta, and V. Sarkar. ABCD: eliminating array-bounds \nchecks on demand. In ACM SIGPLAN Conference on Programming Language Design and Implementation, pages \n321 333, 2000. [2] J. Chen and D. Tarditi. A simple typed intermediate language for object-oriented languages. \nIn ACM Symposium on Principles of Programming Languages, pages 38 49, 2005. [3] J. Chen, D. Wu, A. W. \nAppel, and H. Fang. A provably sound TAL for back-end optimization. In ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, 2003. [4] C. Colby, P. Lee, G. C. Necula, F. Blau, K. Cline, and \nM. Plesko. A certifying compiler for Java. In ACM SIGPLAN Conference on Programming Language Design and \nImplementation, June 2000. [5] K. Crary and J. C. Vanderwaart. An expressive, scalable type theory for \ncerti.ed code. In ACM SIGPLAN International Conference on Functional Programming, pages 191 205, 2002. \n[6] Microsoft Corp. et al. Common Language Infrastructure. 2002. http://msdn.microsoft.com/net/ecma/. \n[7] D. Grossman and J. G. Morrisett. Scalable certi.cation for typed assembly language. In International \nWorkshop on Types in Compilation, pages 117 146, 2001. [8] X. Leroy. Formal certi.cation of a compiler \nback-end or: program\u00adming a compiler with a proof assistant. In ACM Symposium on Principles of Programming \nLanguages, pages 42 54, 2006. [9] V. S. Menon, N. Glew, B. R. Murphy, A. McCreight, T. Shpeisman, A. \nAdl-Tabatabai, and L. Petersen. A veri.able ssa program representation for aggressive compiler optimization. \nIn ACM Symposium on Principles of Programming Languages, pages 397 408, 2006. [10] G. Morrisett, K. Crary, \nN. Glew, D. Grossman, R. Samuels, F. Smith, D. Walker, S. Weirich, and S. Zdancewic. TALx86: A realistic \ntyped assembly language. In ACM SIGPLAN Workshop on Compiler Support for System Software, pages 25 35, \n1999. [11] G. Morrisett, D. Walker, K. Crary, and N. Glew. From System F to typed assembly language. \nIn ACM Symposium on Principles of Programming Languages, pages 85 97, 1998. [12] G. Necula. Proof-Carrying \nCode. In ACM Symposium on Principles of Programming Languages, pages 106 119, 1997. [13] F. Perry, C. \nHawblitzel, and J. Chen. Simple and .exible stack types. In International Workshop on Aliasing, Con.nement, \nand Ownership (IWACO), July 2007. [14] Z. Shao, B. Saha, V. Trifonov, and N. Papaspyrou. A type system \nfor certi.ed binaries. In ACM Symposium on Principles of Programming Languages, 2002. [15] J. C. Vanderwaart \nand K. Crary. A typed interface for garbage collection. In ACM SIGPLAN workshop on Types in languages \ndesign and implementation, pages 109 122, 2003. [16] H. Xi and R. Harper. A dependently typed assembly \nlanguage. In 2001 ACM SIGPLAN International Conference on Functional Programming, pages 169 180, September \n2001.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Type-preserving compilers translate well-typed source code, such as Java or C#, into verifiable target code, such as typed assembly language or proof-carrying code. This paper presents the implementation of type-preserving compilation in a complex, large-scale optimizing compiler. Compared to prior work, this implementation supports extensive optimizations, and it verifies a large portion of the interface between the compiler and the runtime system. This paper demonstrates the practicality of type-preserving compilation in complex optimizing compilers: the generated typed assembly language is only 2.3% slower than the base compiler's generated untyped assembly language, and the type-preserving compiler is 82.8% slower than the base compiler.</p>", "authors": [{"name": "Juan Chen", "author_profile_id": "81100119052", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022776", "email_address": "", "orcid_id": ""}, {"name": "Chris Hawblitzel", "author_profile_id": "81100064145", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022777", "email_address": "", "orcid_id": ""}, {"name": "Frances Perry", "author_profile_id": "81333490569", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P1022778", "email_address": "", "orcid_id": ""}, {"name": "Mike Emmi", "author_profile_id": "81333488438", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P1022779", "email_address": "", "orcid_id": ""}, {"name": "Jeremy Condit", "author_profile_id": "81100200251", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022780", "email_address": "", "orcid_id": ""}, {"name": "Derrick Coetzee", "author_profile_id": "81351607644", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022781", "email_address": "", "orcid_id": ""}, {"name": "Polyvios Pratikaki", "author_profile_id": "81351608984", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1022782", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375604", "year": "2008", "article_id": "1375604", "conference": "PLDI", "title": "Type-preserving compilation for large-scale optimizing object-oriented compilers", "url": "http://dl.acm.org/citation.cfm?id=1375604"}