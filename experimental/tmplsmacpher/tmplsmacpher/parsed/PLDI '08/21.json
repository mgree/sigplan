{"article_publication_date": "06-07-2008", "fulltext": "\n Ef.cient Program Execution Indexing Bin Xin William N. Sumner Xiangyu Zhang Department of Computer Science, \nPurdue University, West Lafayette, Indiana 47907 {xinb,wsumner,xyzhang}@cs.purdue.edu Abstract Execution \nindexing uniquely identi.es a point in an execution. De\u00adsirable execution indices reveal correlations \nbetween points in an execution and establish correspondence between points across mul\u00adtiple executions. \nTherefore, execution indexing is essential for a wide variety of dynamic program analyses, for example, \nit can be used to organize program pro.les; it can precisely identify the point in a re-execution that \ncorresponds to a given point in an original ex\u00adecution and thus facilitate debugging or dynamic instrumentation. \nIn this paper, we formally de.ne the concept of execution index and propose an indexing scheme based \non execution structure and pro\u00adgram state. We present a highly optimized online implementation of the \ntechnique. We also perform a client study, which targets pro\u00adducing a failure inducing schedule for a \ndata race by verifying the two alternative happens-before orderings of a racing pair. Index\u00ading is used \nto precisely locate corresponding points across multiple executions in the presence of non-determinism \nso that no heavy\u00adweight tracing/replay system is needed. Categories and Subject Descriptors D.2.5 [Software \nEngineer\u00ading]: Testing and Debugging Debugging aids, Diagnostics, Mon\u00aditors; D.3.4 [Programming Languages]: \nProcessors Debuggers General Terms Algorithms, Measurement, Reliability Keywords Execution indexing, \nExecution alignment, Control de\u00adpendence, Structural indexing, Semantic Augmentation, Data race 1. Introduction \nDuring program execution, a static program statement could be ex\u00adecuted multiple times, resulting in \ndifferent execution points. A fundamental challenge in dynamic program analysis is to uniquely identify \nindividual execution points so that the correlation between points in one execution can be inferred and \nthe correspondence be\u00adtween execution points across multiple executions can be estab\u00adlished. Solving \nthis problem is signi.cant for a wide range of ap\u00adplications. Pro.ling. Program pro.ling collects information \nabout program executions such as frequently executed paths, referenced addresses, produced values, and \nexercised dependences. Such information can be used in program optimizations, debugging, testing, paralleliza\u00adtion, \nand so on. Currently, most program pro.ling techniques in\u00addex pro.les through static program points [7] \nand can effectively Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 08 June 7 13, 2008, Tucson, Arizona, USA. Copyright c . 2008 ACM 978-1-59593-860-2/08/06. \n. . $5.00 answer queries such as .nding the set of addresses referenced at program point x. Such an \nindexing scheme merges the information of individual execution instances of x and thus is insuf.cient \nfor some applications. For example, in order to study the available con\u00adcurrency in program execution, \nit is essential to distinguish com\u00adputation performed in different iterations of a loop. Moreover, we \nshould only compare the iterations that have similar dynamic con\u00adtexts because two iterations of a loop, \nalthough processing disjoint datasets, may be nested in completely different calling contexts so that \nparallelizing them requires signi.cant code restructuring. In other words, applications like parallelization \nrequire more expres\u00adsive indexing techniques to organize execution pro.les so that the correlation between \npoints can be unveiled. Debugging. A debugging practice often entails setting breakpoints, re-running \nthe program, and inspecting program state when the ex\u00adecution is trapped. In many situations, the need \nof setting break\u00adpoints at a particular execution instance of a static program point arises. Although \nmany debuggers support skipping a certain num\u00adber of instances of a breakpoint, it is known to be insuf.cient \nas the ith instance of a statement s in the re-execution might not be the same ith instance in the original \nexecution, due to nondeterminism or program state perturbations performed at earlier breakpoints. An \nindexing scheme that tolerates nondeterminism and execution per\u00adturbations is highly desirable. Dynamic \nInstrumentation. The recent advances of program in\u00adstrumentation techniques allow instrumentation to \nbe arbitrarily turned on and off at runtime, which provides .exibility for many dynamic program analysis. \nFor example, execution omission errors (EOE) result in failures through not executing certain statements. \nWhile any statements that are not executed cannot be traced, EOEs are dif.cult for most trace-based techniques. \nIn [23], EOEs are tackled by identifying implicit dependence between a predicate ex\u00adecution instance \nand a memory reference point. This is achieved by forcing the predicate instance to take its opposite \nbranch and then observing the value change at the memory reference point. Switch\u00ading the predicate instance \nrequires the instrumentation to precisely locate the instance. Furthermore, switching the predicate instance \nperturbs the execution and thus the instrumentation needs to .nd the corresponding memory reference point \nin the perturbed execu\u00adtion. Similarly, generating failure inducing scheduling for a data\u00adrace is often \nachieved by forcing the two alternative happens-before relations between the two racing execution points. \nIdentifying the same two points in the two executions in the presence of nondeter\u00adminism demands accurate \nindexing techniques. Execution Comparison. Execution comparison focuses on the similarity between executions. \nIt has been used in debugging and testing. In [22], the minimal differences between the program states \nof two executions, one is passing and the other is failing, are com\u00adposed as the failure inducing chain \nfrom the root cause to the failure symptom. The comparison entails a highly complex procedure of establishing \nmatches between memory states in the two executions. An indexing that can establish correspondence between \ntwo differ\u00adent executions can signi.cantly reduce the complexity. In testing, the similarity between \nexecutions can be used as a criterion to re\u00adduce redundancy and prioritize test cases. Despite the great \nneed for execution indexing, to the best of our knowledge, it has not been studied as a stand-alone problem. \nSome research has been carried out in related .elds. Many existing pro\u00adgram trace related techniques \n[24] use si, which denotes the ith instance of statement s or the instance s at time stamp i, to iden\u00adtify \nan execution point. While it uniquely identi.es an execution point, it does not contain any information \nto represent the relations between points and lacks the power of establishing correspondence between \nexecutions. In [23, 9], of.ine algorithms are proposed to align multiple executions or regions in multiple \nexecutions for their own purposes. An ideal execution indexing technique should be on\u00adline with low overhead \nsuch that indices can be queried any time during execution like querying calling contexts. With indices, \nex\u00adecution alignment becomes trivial as two points in two different executions must align if they have \nthe same index. In the context of aspect-oriented programming [18, 3] and security [13], event pat\u00adterns \nare described by regular languages or automata to locate ex\u00adecution points where certain interesting \nstates have been reached. These techniques are not general in the sense that they are only in\u00adterested \nin some execution points and the user needs to de.ne the set of events and their patterns. In this paper, \nwe propose a general execution indexing scheme that is based on execution structure and program state. \nThe ba\u00adsic idea is to parse program executions according to their nesting structure, which is expressed \nas a set of grammar rules. The set of rules that have been used to parse a given execution point reveals \nits structure and thus constitutes its index. To improve the .exibility and expressiveness of our indexing \nscheme, the grammar rules can be augmented with semantic information such as values at particu\u00adlar execution \npoints. Our technique features a cost-effective online implementation with a set of optimizations. Our \ncontributions are highlighted as follows: We formally de.ne the problem of execution indexing.  We \npropose to describe program execution by a context free language. The set of rules are constructed based \non the nesting structure of the program, which can be inferred from program control dependence. An algorithm \nis given to explicitly derive the set of rules.  We present an ef.cient online algorithm to compute \nindices without requiring explicit construction of the grammar rules. A few optimizations further reduce \nthe overhead to 42% on average.  We propose semantic augmentation to the structure based exe\u00adcution \nindexing. With the augmentation, program state can be incorporated into the grammar rules and thus become \npart of an index. As a result, users insights of program behavior can be leveraged in index construction. \n We perform a client study of the indexing technique by apply\u00ading it to lightweight generation of a \nfailure inducing scheduling for data races. Data races are benign if they can not lead to any failure. \nFor a pair of racing program points, two executions are emitted to verify the two alternative orderings. \nThe indices are used to precisely locate the corresponding points in these exe\u00adcutions. Our experimentation \nshows that with indexing, failure inducing schedules can be easily generated without relying on an expensive \ntracing and replay system.   2. Execution Indexing In this section, we .rst formally de.ne the concept \nof execution index. We then describe the structural indexing scheme. DEFINITION 1. (Execution Index) \nGiven a program P, the index of an execution P(ii), denoted as EIP(ii) with ii being the input vector, \nis a function of execution points in P(ii) that satis.es the following property: . two execution points \nx i)(y). = y, EIP(ii)(x)= EIP(i From the de.nition, any function that uniquely identi.es an execution \npoint can serve as an index. A very important property of an index function is that it establishes a \ncorrespondence relation between points in multiple executions. DEFINITION 2. (Execution Correspondence) \nTwo execution points, x in execution E and y in E', correspond to each other iff EIE (x) = EIE' (y). \nE and E' may correspond to different inputs, or even though they have the same input, E' is a perturbation \nof E, caused by nondeterministic scheduling and so on. Code: Original Execution (p1=TTF) 1.  2. while \n(p1) 2. 3. while (p1) { get_input (buf); 3. get_input (buf); 9. read (buf,512); E1 ... 2. while (p1) \n4. 5. } get_input (buf); 3. get_input (buf); 9. read (buf,512); E2 6. ... 2. while (p1)  7. void get_input \n(char * buf) 5. get_input(buf); E3 8. { 9. read (buf, ); 9.  read (buf, 512); Perturbed Execution (p1=F) \n10. } 2. while (p1) 5. get_input (buf); Figure 1. Log Based Replay with Perturbation 2.1 Motivating \nExample As described in the introduction, constructing execution correspon\u00addence is essential for a wide \nrange of applications. Simple indexing schemes are not suf.cient in providing meaningful correspondence. \nConsider the example in Figure 1. The program reads input from a .le within a loop and then reads another \npiece of input outside the loop. Assume in the original execution, p1 takes the value se\u00adquence of true, \ntrue,and false. As a result, function get input() is called three times, twice from inside the loop. \nThe logged events are highlighted by boxes and labeled with E1, E2,and E3. Assume a failure happens and \nthe programmer tries to identify the correla\u00adtion between p1 and the failure by changing the branch outcome \nof the .rst instance of p1, i.e., from true to false, and observes if the failure disappears. Such a \nswitching process is also used in handling execution omission errors [23] to automatically unveil im\u00adplicit \ndependence. The programmer replays the execution using the event log and perturbs the replayed execution \nby switching p1 at its .rst evaluation. As the perturbed replay relies on the event log collected in \nthe original run, the challenge lies in correctly supply\u00ading events during replay. In this case, it is \nto associate event E3 with statement 9 in the perturbed run. Note that we assume E3 is independent of \nE1 and E2. This often occurs when the program parses E1 and E2 for one structure and then parses E3 for \nanother structure. The simplest indexing scheme that uses the time order fails because the .rst event \nin the original run is E1, whereas the .rst event expected in the perturbed execution is E3.Asmarterin\u00addexing \nthat represents an execution point as si, meaning the ith instance of statement s, although has been \nwidely used in existing dynamic analyses [19, 24, 14], is not suf.cient either. E1 has the index of 91, \nwhich is the same as Ex s. In other words, E1 is the correspondence of Ex and thus E1 is supplied to \nstatement 9 during replay, which is wrong. In this paper, we propose an indexing technique based on \nexe\u00adcution structure. In Figure 1, event E1 is processed at statement 9, which is nested in the method \ncall made at 3, which in turn is nested in the true branch of predicate p1 at 2. Other executed statements \nsuch as the second and third calls at 3 are not related to the nesting structure of E1 and should not \nbe part of the index. Therefore, E1 has the index of [2, 3, 9]. E2 has the index of [2, 2, 3, 9].The \ntwo consecutive 2s in the index indicate that the event is nested in the second iteration of the loop. \nBoth E3 and Ex have the same index [5, 9], meaning the structures of these two events occur within the \ncalls made at 5 and are not related to other statements such as 2 or 3. Based on the structural indices, \nE3 is provided as the expected event. Note that using call stacks does not work because E1 and E2 have \nthe same call stack. In other words, call stacks are not a valid execution index function. This is due \nto call stacks only record\u00ading a partial image of the nesting structure. The proposed indexing scheme \nnot only succeeds in establishing desired correspondence for points across different executions in many \ncases, but also facil\u00aditates highly ef.cient online computation.  2.2 Structural Indexing In this subsection, \nwe present our design in detail. Implementation will be discussed in the next section. The key observation \nof our technique is that all possible executions of a program can be de\u00adscribed by a language called \nexecution description language (EDL) based on structure. An execution is a string of the language. Code \n1 s1; 2 s2; 3 s3; 4 s4; 1 if (...) 2 s1; 3else 4 s2; 1 while (...) {2 s1; 3 }4 s2; 1void A() {2 B(); \n3 }4void B() {5 s1; 6 } EDL S -. e1 e2 e3 e4 S -. e1 R1 R1 -. e2 | e4 S -. e1 R1 e4 R1 -. e2 e1 R1 | \n. S -. e2 RB RB -. e5 Str. 1234 12 14 1214 121214 25 Table 1. EDLs for Simple Constructs. Table 1 presents \nthe EDLs for a list of basic programming lan\u00adguage constructs. The .rst column shows sequential code \nwithout nesting, whose execution is described by a grammar rule that lists all the statements. Note that \na terminal symbol s is denoted as es in this paper. In the second column, the if-else construct in\u00adtroduces \na level of nesting and thus the EDL has two rules, one expressing the top level structure that contains \nstatement 1 and the intermediate symbol R1 representing the substructure led by 1. The two alternative \nrules of R1 denote the substructure of the construct. The self recursion in the grammar rule for the \nwhile loop in the third column expresses the inde.nite iterations of the the loop. From these examples, \nwe can see that EDLs are different from programming languages. The strings of EDLs are executions whereas \nthe strings of programming languages are programs. The alphabet of an EDL contains all the statement \nids in the program, whereas that of a programming language contains program con\u00adstructs, variable identi.ers, \nand so on. The second observation is that program control dependence perfectly re.ects execution struc\u00adture. \nA statement x control depends on another statement y, usually a predicate or a method call statement, \nif y directly decides the ex\u00adecution of x. The formal de.nition can be found in the seminal paper [8]. \nFor example in the second column of Table 1, statement 2 is control dependent on statement 1. The statements \nthat share the same control dependence are present on the right hand side of the same rule, representing \nthe same level of nesting. Consider the rules for the while construct. Statements 1 and 4 have the same \ndependence and they are listed on the right hand side of the .rst rule; the body of rule R1 lists the \nstatements that are dependent on statement 1. Note that statement 1 control depends on itself as the \nexecution of a loop iteration is decided by its previous iteration. We de.ne the EDL of a program as \nfollows. DEFINITION 3. (Execution Description Language) Given a pro\u00adgram P, its execution description \nlanguage, denoted as EDL(P), is the language described by the grammar rules generated by Al\u00adgorithm 1. \nAlgorithm 1 Grammar Construction Input: a program P. Output: a set of grammar rules that describe the \nexecutions of P. 1: ConstructGrammar (P) 2: { rules = \u00d8; 3: for each method M{ 4: /* CD denotes control \ndependence*/ 5: T = statements in Min .ow order satisfying CD = STARTM; 6: rules .= RM.T; 7: for each \nstatement s in M{ 8: if(s is a predicate) { true; 9: T = statements in .ow order s.t. CD = s false; \n 10: F = statements in .ow order s.t. CD = s 11: rules .= Rs .T |F ; 12: } 13: } 14: } 15: /*post processing \nto complete the rules */ 16: for each rule r .X 17: for each symbol s .X { 18: if(s is a predicate) 19: \nreplace s with sRs in X; 20: if(s is a callto M) 21: replace s with sRM in X; 22: } 23: } While there \nexist different grammar rules that describe the same language, we rely on the rules generated by Algorithm \n1 as they lead to a clear and concise de.nition of execution index, which will be discussed later in \nthis subsection. Algorithm 1 is based on pro\u00adgram control dependence: a grammar rule is created for statements \nthat share the same control dependence. It consists of two major steps. Lines 3-14 describe the .rst \nstep, in which statements in in\u00addividual methods are clustered based on their control dependences. Here \nwe consider all statements in a method that have empty con\u00adtrol dependence to be control dependent on \nthe method. In the second step (lines 16-22), the rules generated in the .rst step are scanned and symbols \nthat have control dependents are appended with the grammar rules that describe the substructures of their \ncon\u00adtrol dependents. To demonstrate the algorithm, we use a more complete example in Figure 2. The code \ncontains a recursive call (line 9) and non\u00adstructural control .ow (line 5). The .rst rule in Figure 3 \nrepre\u00adsents the top level structure of method A. Due to the return at line 5, as shown by the CFG, only \nstatements 2 and 3 are control dependent on the start node of A, STARTA. The second step of 1: A () { \n2: s1; 3: if(C1) { 4: B(); 5: return; 6: } 7: while (C2) { 8: if(C3) 9: A(); 10: s2; 11: } 12: B(); 13: \n} 14: 15: B () { 16: s3 17: } Figure 2. A Running Example RA -. e2 e3 R3 ee R3 -. 4 RB e5 | e7 R7 12 \nRB ee R7 -. 8 R8 10 e7 R7 | E RB -. 16 e e R8 -. 9 RA | E Figure 3. The Grammar Generated by Algorithm \n1 for Figure 2.  E: 2 3 7 8 9 2 3 4 16 5 10 7 8 10 7 12 16 E : 2 3 7 8 10 7 8 9 2 3 7 12 16 10 7 12 \n16 Figure 4. Indexing Two Executions of The Program In Figure 2. the algorithm inserts R3 right behind \nsymbol 3 in the rule, denot\u00ading the lower level composition that are control dependent on 3. Note that \nthe top level rule RA does not re.ect the syntactic struc\u00adture of method A as a rule derived from the \nsyntactic structure, i.e., RA . 3 R3 e7 R7 12 , fails to describe executions, e.g., 2 3 4 e2 ee16 5 . \nStatements 4 and 5 are control dependent on the true branch of 3 and statements 7 and 12 are dependent \non the false branch. Adding the intermediate symbols denoting the substructures led by 4, 7, and 12 results \nin the second rule in Figure 3. The remaining rules are similarly derived. Recall that our goal is to \ndesign an execution indexing tech\u00adnique. Based on EDLs, we are ready to introduce our indexing scheme. \nAs illustrated earlier, any execution of a program P is a string of EDL(P ). The index of an execution \npoint can be de.ned based on the derivation tree of the string. DEFINITION 4. (Structural Indexing) \nGiven a program P and its EDL(P ), the structural index of a point x in execution P (ii), i) denoted \nas SEIP(i(x), is the path from the root of the derivation tree to the leaf node representing x. According \nto the de.nition of EDL, each grammar rule cap\u00adtures the statements at the same nesting level. Therefore, \nthe path in the derivation tree, which leads from the root to a terminal sym\u00adbol and contains all the \nintermediate symbols, denotes the top\u00addown nesting structure and serves as a perfect structural index. \nFigure 4 shows the indices for two executions of the code in Fig\u00adure 2. Execution E recursively calls \nA() in the .rst iteration of the while loop, whereas the recursive call happens in the second iteration \nin E ' . We can see SEIE(21)= SEIE ' (21)=[RA], in which 21 denotes the .rst instance of statement 2 \nin the traces. Thus, the .rst executed statement 2s correspond to each other in the two executions, as \nlinked by the dotted line. SEIE(161)= [RA, R3, R7, R8, ...], which clearly expresses the nesting structure \nof the .rst executed 16. In contrast, the index of 161 in the second execution is SEIE ' (161)=[RA, R3, \nR7, R7, ...], different from SEIE(161). The indices imply that the 161 in E ' is nested in the second \niteration of the while loop while the 161 in E is nested in the .rst iteration. Therefore, the two 161s \ndo not structurally cor\u00adrespond. In some situations, the structural correspondence differs from the desired \ncorrespondence, we will discuss how we handle these cases in Section 4. We have de.ned what is structural \nindexing. However, we are yet to show that structural indexing is a valid indexing scheme. THEOREM 1. \nThe structural indexing function de.ned in De.ni\u00adtion 4 is a valid execution indexing function. To prove \nthis theorem, we need to show that no two different execution points in an execution have identical structural \nindices. The proof is omitted for brevity.  3. Implementation and Optimizations A faithful implementation \nof structural indexing according to the de.nition is not practical because it requires collecting the \nwhole execution trace and then parsing the trace. However, our goal can be interpreted as maintaining \nthe current index for each execution point on the .y, just like maintaining the calling context. In this \nway, we avoid any form of logging. The user has the freedom to collect the indices for any interesting \npoints such as breakpoints and perturbation points. This interpretation entails highly ef.cient implementation. \n3.1 Indexing Stack The basic idea is to use an indexing stack (IS) to keep track of the index, which \nis the set of rules used in parsing the current nesting structure. More speci.cally, an entry is pushed \nto IS once a predicate statement or a method call is executed, implying a new rule representing the lower \nlevel nesting structure is taken to parse the following execution. The entry is popped if the immediate \npostdominator of the predicate is executed or the method call is returned, indicating that parsing based \non the current grammar rule is .nished and the following execution should be parsed based on the parent \nrule, which is now found in the top entry of the current IS. The algorithm is presented in Algorithm \n2. According to the algorithm, only predicates and method calls and their immediate postdominators are \ninstrumented. The circles and triangles in the CFG in Figure 2 illustrate the instrumentation of calls \nto enter () and exit (). For instance, statement 3 is instrumented with an enter () call, meaning rule \nR3 is em\u00adployed to parse the following execution. At EXITA, the statement Algorithm 2 Maintaining Indexing \nStack. s is an executed predicate or a executed method call; b is the entry of a basic block; IPD(s) \ndenotes the immediate postdominator of s; Each stack entry is a pair, with the .rst element being the \nrule and the second element being the terminating IPD. 1: Enter (s) { 2: if(s is a predicate) 3: IS.push(<Rs, \nIPD(s) >); 4: if(s is a method call to M ) 5: IS.push(<RM ,IPD(s) >); 6: } 7: Exit (b) { 8: while (IS.top().second \n= b) 9: IS.pop(); 10: } 3 s immediate postdominator, a call to exit () is inserted. Note that a statement \nmay serve as the immediate postdominator of mul\u00adtiple predicates or method calls. For example, statement \n10 is the IPD of both 8 and 9. As a result, multiple entries in IS may have the same terminating IPD. \nThe property of dynamic control depen\u00addence further dictates that multiple entries with the same terminat\u00ading \nIPD must be consecutive in IS [20]. This explains why Algo\u00adrithm 2 needs to push the terminating IPD \nto the stack and in lines 8-9 uses a loop to pop all entries with s being the terminating IPD. trace \ninstrumentation indexing stack 2. s1 - [RAXA] 3. if (C1) . (R3XA) [RAXA R3XA ] 7. while(C2) . (R712) \n[RAXA R3XA R712 ] 8. if (C3) . (R810) [RAXA R3XA R712 R810] ... ... ... 10. s2 . (*10) [RAXA R3XA R712] \n7. while(C2) . (R712) [RAXA R3XA R712 R712] ... ... ... 7. while(C2) . (R712) [RAXA R3XA R712 R712 R712] \n12. B() . (*12) [RAXA R3XA RBXB] . (RBXB) ... ... ... Figure 5. The Maintenance of IS for Execution \nE in Figure 4. A stack entry (rule,ipd) is represented as ruleipd to save space. . and . stand for push \nand pop; XA stands for EXITA. Table 5 shows the partial computation of IS for the execution E in Figure \n4. At the .rst step, the IS inherits the entry of RAXA from the preceding call site to A() beyond the \ntrace, meaning the current parsing rule is RA and the its terminating IPD is EXITA. The statement executions \nof 3, 7 and 8 lead to push operations as they are predicates. The instrumentation at 10 pops the entry \nR810 , indicating the current rule of R8 terminates. The next two steps of 7 push two R712 entries, representing \ntwo loop iterations. The execution of 12 terminates all entries that have 12 as their IPD and pushes \na new entry. We want to point out that the sequence of rules recorded in the IS is exactly the index \nof the current execution point. Algorithm 2 is extended to handle recursive functions and irreg\u00adular \ncontrol .ow caused by setjmp/longjmp. The extension is similar to our prior work [20] and thus not the \nfocus of this paper.  3.2 Optimizations Algorithm 2 entails easy implementation. However, the algorithm \nincurs signi.cant runtime overhead as it requires stack operations upon execution of predicates and their \nimmediate postdominators. In this subsection, we discuss how to optimize the algorithm so that it becomes \nmore affordable. 1 if (...) { 1if(C1) {2 s1;2 s1; 3 if (...) { 3 goto 8; 4 if (...) 4 }5 s2; 5if(C1 ||6 \nelse 6 C2) {7 s3;7 s2; } 8 s3 }}  (a) (b) (c) Figure 6. Rule Inference. Rule Inference. The key to \nreduce runtime overhead is to reduce the number of stack operations. The .rst optimization is rule infer\u00adence, \nwhich removes stack operations for non-loop predicates and their postdominators. The .rst observation \nis that some of the pred\u00adicate rules can be inferred from the current execution point such that it is \nnot necessary to explicitly record them onto IS. Consider an example in Figure 6 (a). At the moment s1 \nis executed, it must be the case that R1 . e2 e3 R3 is the active grammar rule because it is the only \nrule to parse s1. Similarly, when s2 is the current ex\u00adecution point, it can be inferred that [R1 R3 \nR4] must be the top three entries on IS. The case for s3 is similar. In other words, a predicate and \nits postdominator are not instrumented if and only if any statements that ever appear in the body of \nthe predicate s rules do not appear in rules of any other predicates. It is equivalent to not instrumenting \na predicate and its postdominator if and only if the predicate s control dependents have only one static \ncontrolling predicate. The second observation is that even the above mentioned con\u00addition is not satis.ed, \nthe stack operations for non-loop predicates can still be replaced with simple counter operations. Consider \nthe example in Figure 6 (b), whose CFG is presented in Figure 6 (c). Due to the OR operation at line \n5 and the jump at line 3, statement 8 has three control dependence predecessors: predicates at line 1, \n5, and 6. In other words, it appears in the following rules of these three different predicates: e3 e \nR1 -. 2 e8 | e5 R5 e7 e R5 -. 6 R6 | e8 R6 -. 7 e8 | E e In this case, the parsing rule cannot be inferred \nfrom the execution of 8 as it is not unique. To handle this situation, we enumerate the possible rules \nand use the number as the identi.cation of a rule. Consider the CFG in Figure 6 (c). The instrumentation \nis highlighted on the .ow edges. A variable id is used to enumerate the three possible rules. Different \n.ow edges being taken implies different parsing rules, identi.ed by different id values. Note that at \nruntime, only one out of these three edges can be taken. The optimized instrumentation algorithm is presented \nin Algo\u00adrithm 3. Assume CD(s) is the ordered set of predicates on which s is control dependent. It implies \nthat s is parsed by the rule of one of these predicates. Here the code order is used. In lines 4\u00ad7, the \nalgorithm .rst identi.es the set of non-singleton CDsthat are maximum, namely, they are not subsets of \nany other CD sets. These maximum sets are stored in MAX. We use a counter idj for set MAX[j] to identify \nwhich predicate out of the set is exercised at runtime, In lines 8-11, the control .ow edges that correspond \nto the predicates in MAX[j] are instrumented by setting idj to a con\u00adstant that uniquely identi.es the \npredicate. Here the constant is the order of the predicate in the set. It is worth noting that control \nde\u00adpendence is indeed de.ned between one of the two branch outcome (True/False) of a predicate p and \na statement s even though we often say the dependence is between p and s for brevity [8]. That explains \nthe superscript at line 10. Consider the example in Figure 6 (b). The control dependences for 7 and 8 \nare computed as CD(7) = [5T ,6T],and CD(8) = [1T , 5T ,6T]. The vector MAX has only one element, which \nis the maximum set [1T ,5T ,6T]. According to the algorithm, a counter id is assigned to the set. All \nthe predicates in the set are instru\u00admented with assignments to id. The resulting instrumentations are \nexactly those presented in Figure 6 (c). We can see that (a) counters are not assigned to singleton CDs \nat runtime and thus the rules can be inferred as discussed earlier; (b) counters are not assigned to \nCD sets that are subsets of some other sets. Assume x and y are two CD sets, and x .y and y is maximum, \nthe instrumentations caused by y are able to distinguish the exercised predicate in x at runtime. For \ninstance, CD(7) . CD(8) and thus it is not necessary to associate another counter to CD(7) as the instrumentions \nof id =2 and id =3 on edges 5T and 6T are able to identify which rule should be used to parse an executed \ninstance of 7. THEOREM 2. Algorithm 3 is correct, i.e., the parsing rule of a statement execution s can \nbe always decided by the value of a counter. Theorem 2 asserts the correctness of Algorithm 3. We can \neasily derive the decoding algorithm that reconstruct the full execution index from the values of counters. \nThe proof of Theorem 2 and the decoding algorithm are omitted due to the space limit. Algorithm 3 Instrumentation \nFor Non-Loop Predicates. Mis a method. CD(s) denotes the ordered set of non-loop predicates on which \ns is control dependent. MAX is a vector storing the maximum CD sets. order(cd,p) returns the position \nof p in the ordered set cd. 1: EncodePredicate (M) { 2: Compute control dependence for M; 3: i =0; 4: \nfor each statement s in M 5: if( .t.(t .M.t!= s .CD(s) .CD(t))) 6: if(CD(s) .MAX &#38;&#38; |CD(s)|> \n1) 7: MAX[i ++] = CD(s); 8: for each non-loop predicate p in M 9: for j=0 to (i-1) b=True/False .MAX[j]) \n 10: if(p 11: instrument the b = True/False edge of p with idj = order(MAX[j],p) ; 12: } Loop Optimization. \nAs many hot program paths reside in loops, optimizing the instrumentation for loop predicates is also \nessential to bringing down the cost. A loop predicate decides the execution of an iteration and thus \na loop predicate instance at runtime is di\u00adrectly/indirectly control dependent on its previous instance. \nA loop predicate is different from a predicate inside a loop, which often does not decide the execution \nof an iteration. Therefore, the EDL grammar rule for a loop always contains a recursion, for example, \nthe rule in the 3rd column in Table 1. Optimization opportunities arise if a loop has a unique predicate \n1 as consecutive iterations are thus parsed by a sequence of the same rule that corresponds to the loop \npredicate. A counter can be used to compress the sequence on IS. Consider the unique loop predicate 7 \nin E of Figure 4. Sym\u00adbol R7s of consecutive iterations are always consecutive along any paths from the \nroot to a leaf. The optimization is to assign a unique counter to each loop that has one loop predicate. \nThe counter is initialized to zero before entering the loop and then incremented whenever the back edge \nis taken. Therefore, push operations can be avoided for the loop predicate instances. Even in the presence \nof multiple loop predicates, we can still use a counter on IS to en\u00adcode a sequence of identical predicates. \nPushes are conducted upon encountering a different predicate. Packing Multiple Counters. Let us consider \ncounters for non-loop predicates .rst. Our experience shows that the sizes of CD sets tend to be small. \nThe majority of CD sets are singleton and most of the non-singleton CD sets have the cardinality of 2. \nThe reason is that non-singleton sets are mostly caused by Boolean OR opera\u00adtions or nonstructural control \n.ow. In other words, the value range of a counter we need for non-loop predicates tends to be small be\u00adcause \nthey only need to distinguish elements in small sets. There\u00adfore, we further optimize our implementation \nby packing multiple counters into one word. We treat each bit of a word as a conceptual register. These \nbit-registers are allocated to a counter as needed. For instance, a counter with the range of [0-3] is \nallocated 2 bit\u00adregisters. These registers have a live range delimited by the predi\u00adcates that are associated \nwith the counter and their postdominators. Eventually, the counter packing problem is reduced to register \nallo\u00adcation problem and we use standard algorithm to solve the problem. In our current implementation, \nwe do not analyze the ranges of loop iterators. We conservatively assign a word to each loop counter. \nAfter the above optimizations, most remaining stack operations occur on function boundaries as we need \nto maintain the IS state across multiple functions. The information that is pushed to the stack usually \ncontains only a few active loop counters and one or two words that contain multiple packed non-loop counters. \nFinally, handling multi-threading is straightforward, each spawned thread inherits the IS state from \nits parent. 6: ... 7: while (C2) { e8 e 8: c=getc(); R7 -. 9 R9 e7 R7 | . eee 9: switch (c) { R9 -. 11 \nRF | 13 RG | 15 RH 10: case a : is augmented to 11: F (c); break; e R7 -. 8 | 12: case b : S8a -. e9 \nR9 e7 R7 13: G(c); break; e 14: case c : S8b -. 9 R9 e7 R7 e 15: H(c); break; S8c -. 9 R9 e7 R7 R9 -. \n11 RF | 13 RG | 15 RH 16: } eee 17: } 18: ... Figure 7. Indexing with Semantic Anchor Points.  4. Semantic-Augmented \nIndexing One of the most important aims of execution indexing is to estab\u00adlish meaningful correspondence \namong points across multiple exe\u00adcutions. However, as correspondence is essentially a semantic con\u00adcept. \nIt is machine undecidable to conclude if two execution points 1 Note that a loop may have multiple loop \npredicates especially when break statements are used. (a)  (c) (d) Figure 8. Indexing Executions with \nInput acb and ab . in two respective executions correspond to each other. In practice, programmers often \ndecide the correspondence according to their understanding of the executions. For example, if the two \nexecutions have their inputs overlapped, the sub-executions driven by the over\u00adlapping input elements \nshould correspond. The proposed technique so far constructs indices from execution structure. The advantage \nof structural indexing is the capability given by the execution de\u00adscription language, which is to con.ne \nperturbation in its nesting regions while sustaining correspondence in the remaining part of the execution. \nHowever, the technique parses an execution starting from a single point, namely, the beginning of the \nexecution. We call the point an anchor point. In many applications, due to non\u00addeterministic scheduling, \nmis-aligned inputs, execution perturba\u00adtion, etc., the single anchor point is often insuf.cient to harness \nthe whole execution and thus the technique fails to sustain meaningful correspondence. An example is \npresented in Figure 7 and 8. The code is shown on the left hand side of Figure 7 and it is modi.ed from \nour previ\u00adous running example in Figure 2. Inside the while loop, the compu\u00adtation is replaced with a \nstatement that gets input and a following switch-case statement that calls different functions according \nto the incoming input values. The EDL grammars are presented on the upper half of the right hand side. \nGiven two executions: one with input acb and the other with ab , their indices are shown in Figure 8 \n(a) and (c). The calls to function G() at 13 in the two executions do not correspond to each other as \nthey have different indices although the programmer might intend to align them. Such misalignment will \nlead to no correspondence being identi.ed inside the method body of the two respective G() calls. Further \ninspection shows that structure based indexing decides the second iterations in the two executions correspond \nto each other while one calls method H() and the other calls G(). To overcome this problem, the technique \nhas to acquire help from the programmer. We further extend our technique to incorpo\u00adrate the programmer \ns knowledge by introducing semantic-based anchor points to harness the whole execution. The idea is to \nincor\u00adporate the values at semantic anchor points into the EDL grammar rules so that different rules \nare selected to parse execution accord\u00ading to different anchor point values, resulting in semantic-based \nindices. Consider the previous example, the EDL grammar rules are enhanced as shown on the lower half \nof the right hand side of Figure 7. Rule R7 is altered to only parse statement 8. Based on the input \nvalues at 8, three different root level rules are introduced. As a result, the derivation tree is transformed \nto a derivation forest as shown in Figure 8. With the semantic-augmented indexing, the calls to G() are \nsuccessfully aligned. The semantic-augmented indexing requires the programmer to annotate their intended \nanchor points by using our prede.ned C macroes. If aggregate data (e.g. arrays) are used to select a \nparsing rule, the programmer also need to provide a hash function to map the aggregate data to a single \nvalue (see the TSP client study in Section 6.2 for an example). In practice, the places that the programmer \nneeds to annotate tend to be few. They are usually input points such as those reading values from a .le \nor receiving external events. The extension can be implemented by creating a new stack whenever an anchor \npoint is executed and then pushing the value at the executed anchor point to the new stack. The new stack \nis maintained exactly as before. The old IS stack is restored once the execution goes beyond the semantic \nrules. Finally, it is worth mentioning that the same implementation provides the .exibility of selectively \nindexing execution. More precisely, indexing can be started at de.ned anchor points instead of from the \nbeginning. 5. Discussion The challenge of execution indexing has been lurking in a number of prior research \nprojects [9, 23] on dynamic program analysis that require establishing correspondence among points across \nmultiple executions. Although this problem is in general machine undecid\u00adable as the correct answer only \nresides in the programmer s mind, our technique shows the promise of providing a practical solution. \nTo the best of our knowledge, this is the .rst work that formu\u00adlates and tackles the problem and thus \nit has its limitations. Our technique heavily relies on execution structure based on the obser\u00advation \nthat execution control structure is a strong indicator of pro\u00adgram semantics. However, there exist applications \nwhose control structure is decoupled from the semantics. An example we have encountered is LR parsers \nsuch as those generated by yacc.Our technique fails to construct meaningful mappings between two ex\u00adecutions \nwith highly similar inputs, even with semantic-augmented indexing. The main reason is that the execution \nof a LR parser is driven by a DFA whose semantics are revealed more by data .ow than by control .ow. \nThis implies a different direction of execution indexing -data .ow based indexing, which we leave to \nour future work. 6. Experiments 6.1 Overhead The .rst experimentation is on the runtime overhead of \nour index\u00ading technique. The implementation is based on Diablo/FIT [17], a generator that produces customized \nATOM-like binary instru\u00admentation tools. Diablo has its own toolchain based on GCC-3.2.2, which is used \nto compile the benchmarks, generate the desired post\u00addominance information, as well as the .nal instrumented \nprograms. Instrumentation are inserted at the start and end of procedures, at the beginning of a basic \nblock that has multiple static CDs, at and before loop head basic blocks, and at programmer de.ned se\u00admantic \nindexing points. Macros are provided for programmers to de.ne semantic indexing points. Inlining is employed \nfor instru\u00admentation with a small number of instructions. We use a subset of SPEC benchmarks from SPEC95 \nand SPEC2000. Some of the SPEC benchmarks failed to get through Diablo/FIT infrastructure and thus are \nnot used in our evaluation. All data are collected on a Benchmarks Statistics on BBL s CDs Runtime Total \n0 CD 1 CD 2 CDs (> 1 CDs) Base CDS OPT CDS Over\u00adhead OPT Over\u00adhead 008.espresso 29059 3841 19588 4225 \n22% 0.9 3.2 1.5 256% 67% 124.m88ksim 28198 3173 19149 4288 23% 71.4 207.5 122.0 191% 71% 129.compress \n21508 1877 14484 3774 26% 105.8 192.3 149.4 82% 41% 132.ijeg 26596 3370 17691 4112 24% 27.2 52.8 38.2 \n94% 40% 164.gzip 23179 2122 15713 3932 25% 3.3 9.2 4.6 179% 39% 175.vpr 28078 3257 19172 4194 23% 22.8 \n66.0 28.4 189% 25% 181.mcf 21743 1886 14689 3798 26% 54.5 76.4 64.6 40% 19% 197.parser 27636 3076 18849 \n4238 23% 13.3 30.4 21.7 129% 63% 256.bzip2 22740 2174 15283 3866 26% 23.5 45.6 31.3 94% 33% 300.twolf \n31638 3120 22197 4640 22% 31.7 51.4 39.1 62% 23% Average - - - - 24% - - - 132% 42% Table 2. Runtime \nOverhead and Statistics on Control Dependences. Pentium 4 (1.8GHz) platform with 500M of RAM, running \nGentoo Linux (kernel 2.6.22). We have described the rule inference optimization in Sec\u00adtion 3.2. The \npremise for this optimization to work is that the number of basic blocks that have more than one static \ncontrol de\u00adpendence is small. The statistics of how many static control depen\u00addences a basic block has \nare shown in Table 2. The Total column represents the total number of basic blocks in each benchmark. \nThe next three columns show that number of basic blocks that has 0, 1, and 2 static control dependences, \nrespectively. We can clearly see that across all benchmarks, most of the basic blocks have either 0 or \n1 static control dependence; for those basic blocks that have more then 1 static control dependence, \nmajority of them have 2. On average, among the basic blocks that have at least 1 control dependence, \nonly about 24% have more than 1 CD, as shown in the sixth column in the table. Since FIT is an optimizing \ninstrumentor, for comparison pur\u00adposes, the Base version of the benchmarks are generated through a dummy \ninstrumentor (No instrumentation code is added, but it bene.ts from all the optimizations available from \nFIT. Thus, they are actually faster than native runs, which are not shown here.). The CDS version corresponds \nto a faithful implementation of the stack\u00adbased algorithm (Algorithm 2) and the OPT version corresponds \nto the optimized implementation. The numbers are presented in Ta\u00adble 2. Both the CDS Overhead and OPT \nOverhead are relative to the Base runtimes. From the table, we can see that maintaining the execution \nindices for programs incurs, on average, a 42% runtime overhead. This cuts more than two thirds of the \noverhead of the CDS implementation. 1. void run ( ) { 2. Node [ ] prefix; 3. for (;;) { 4. lock (a); \n 5. prefix=next_prefix (minLen); 6. unlock(a); 7. remaining(prefix); 8. } 9. } 10. void set_best \n(int best, ) { 11. lock (a); 12. 13. unlock (a); 14. }  15. void remaining (Node [] prefix) { \n16.  17. if (prefix.size == total) { 18. 19. set_best(len(prefix), ); 20. } else { 21. /*extend \nprefix*/ 22. remaining(prefix);23  24. } } Figure 9. The Abstract Code of TSP. T1 : len(prefix) = 500, \n300 T2 : len(prefix) = 200 18. if (200< mL) 18. if (500< mL)   (a). Original Run T1 : len(prefix) \n= 300 T2 : len(prefix) = 500, 200 18. if (500< mL) 12. mL=500; 18. if (200< mL) wait (c1) notify (c1) \nwait (c2)   notify (c2) 12. mL=300; (b). A Second Run With The Happens-Before Switched Figure 10. \nBenign and Harmful Race Discretion by Happens\u00adbefore Switching. 6.2 Client Study -Lightweight Benign \nand Harmful Data Race Classi.cation To show the effectiveness of our indexing technique, we apply it \nto data race detection. One of the major challenges in data race detec\u00adtion is determining whether a \ndata race is benign or harmful, as data race detection tools often produce many false positives, namely, \nbe\u00adnign races. In [15], Satish et al. propose classifying con.rmed, real data races by switching the \nhappens-before edge between the two dynamic accesses involved. Although the technique is highly effec\u00adtive, \ntheir system also relies on a heavyweight tracing and replay system because concurrent execution is nondeterministic. \nWe propose a lightweight race classi.cation approach for po\u00adtential races based on our indexing technique. \nThe key idea is to .rst use a lockset algorithm [16], which is lightweight, to identify potentially racing \naccesses. To produce a failure inducing schedule for a pair of racing accesses, two more executions are \nperformed nondeterministically with the two happens-before alternatives en\u00adforced and the output is observed. \nAs in [15], a race is classi.ed as harmful if different output is produced, otherwise it is benign, or, \nin the case of deadlock, the race itself could not be con.rmed. We avoid tracing and replay by creating \nindices for the racing accesses, which are points in the original execution, and identifying them in \nthe re-executions. As a lockset algorithm often produces a large number of racing pairs, we heuristically \nselect accesses to achieve a reduced set of con.icting access pairs. In particular, we .rst prune con.icts \nby thread segments [10] and keep track of the indices for those con.icting accesses that occur furthest \napart chronologically. The goal is to perturb the computation in between as much as possi\u00adble when enforcing \nthe two happens-before alternatives to increase the chance of a failure. Let us consider the technique \nby observing its behavior on the TSP (travelling salesman problem) benchmark. As our implemen\u00adtation \nworks on C programs, we port the java TSP benchmark to C and remove the double check in the the setbest() \nfunction to introduce a harmful race. Note that the original TSP does not have any harmful races. The \nabstract code is shown in Figure 9. As shown in method run(), the algorithm enumerates all pre\u00ad.xes with \na .xed length and assigns them to individual threads. Each thread takes a pre.x and .nishes the traversal \nby calling remaining().The remaining() method recursively checks if the current path covers all the nodes. \nIf so, it compares the path length with the current minimum length and updates the minimum length by \ncalling setbest() at lines 18-19. If the current pre.x is not a full path, it continues traversing. Data \nraces happen between execution instances of lines 12 and 18 as boxed in Figure 9. The challenge is that \nthe same thread in different executions may pick up completely different sets of pre.xes, which causes \nsimple indexing schemes to fail. As shown in Figure 10, in the original execution, thread one is assigned \nthe pre.x that leads to path lengths of 500 and 300 and thread two is assigned the pre.x that leads to \nthe length of 200. In a separate execution with the same input, thread one is assigned the 300 pre.x \nand thread two is assigned the 500 and 200 pre.xes. We de.ne 5 as the semantic anchor point so that a \nnew IS is started at line 5 and the current pre.x (hash) is pushed to the stack as part of the index, \nAs a result, we are able to locate the two con.icting accesses in the new run and insert synchronization \nas shown in .g. 10. The synchronization expresses both happens-before orderings of the racing accesses, \nand eventually produces the correct output of 200 in one execution and the faulty output of 300 in the \nother. Therefore, this is a harmful race. Note that due to nondeterminism, the con.icting accesses may \nnot appear in the new run. It may be necessary to run the trial a number of times in order to have both \naccesses occur; however, because of the approach to selecting these con.icting pairs and the small number \nof scheduling decisions that determine whether or not they occur, their chances of omission are small, \nas seen by the frequency of successfully detecting harmful races in TSP2 of Table 3. Here we see that \na large number of dynamic con.icts can be reduced to a small set of representative con.icts that are \nmore likely to be harmful when .ipped. For TSP, exactly one of these selected dynamic con.icts is harmful, \nand execution indexing enables its consistent classi.cation. By comparison, nondeterminism adversely \naffects simpler in\u00addexing schemes. Using the thread, instruction, and dynamic execu\u00adtion count as an \nidentifying tuple is susceptible to both scheduling differences and execution perturbation, as previously \nseen. Thus, such an index may map one execution point to another in a different run or even to a nonexistent \nexecution point. Such indices are frail and failure prone for this classi.cation scheme, as seen by the \nlow frequency of properly classifying races in TSP1 of Table 3. Here the reduced set is computed analogously \nby projecting accesses to static instructions to get static con.ict pairs from the dynamic ones. In fact, \nthe data race alone could not be detected in 56% of trials using these tuples, much less classi.ed. We \nfurther examine how indexing contrasts to a simple tuple identi.cation scheme on real world races. In \nparticular, we examine previously discovered races in the MySQL database server [2] and the Apache HTTP \nserver [1] as also explored in [21, 12]. MySQL. The MySQL server contained a data race where is\u00adsuing \nan insert command to the database could con.ict with log maintenance such that its log .les would omit \ndatabase modi.\u00adcations [2]. Namely, the log maintenance operation brie.y made server logs appear closed, \npreventing the insert command from recording its operation. This lost update bug can be detected in the \ntrivial case where one client accesses the server on one thread to perform an update and another client \naccesses the server on a sec\u00adond thread to instigate log maintenance. Note that, apart from the race \nin question, thread scheduling will not cause nondeterminism within the clients once they have connected \nto the server, but the order in which clients themselves connect to the server cannot strictly be guaranteed. \nBecause of this, it is not knowable which of the server s threads must be instru\u00admented using simple \ntuple identi.cation; however, by semantically anchoring execution indices to the precise requests of \nthe client threads, the threads pertaining to the database update and log main\u00adtenance can be distinguished \nand the appropriate accesses forced to exhibit the race. Under one happens-before direction, a lost update \noccurs, and under the other, the database operates normally. Thus the race is harmful. Testing this by \nperforming both of the client requests in nondeterministic order con.rms the results as re.ected in MySQL2 \nof Table 3. Here the frequency again tells us that the race was consistently observed as being harmful. \nAgain, simple tu\u00adple identi.cation cannot consistently classify this race, yielding the lower frequency \nof successfully classifying the race in MySQL1 . Apache HTTP Server. The Apache webserver contained a \ndata race where threads handling different client requests could simultaneously try to write to a buffered \noperation log, causing it to become corrupted [1]. In the middle of one thread s update, the other can \nsimultaneously write, and both threads can overwrite each other s information. This data corruption bug \ncan be detected as a potential data race in a simple scenario where two clients submit different requests \nto the webserver. In this example, thread nondeterminism in.uences the nature of the resulting corruption, \ncausing either invalid writes or a lost update. Due to the nature of the signalling in our approach to \nforcing race expression, a lost update is signi.cantly more likely. The particular update that is lost \nis determined by which of the two happens-before orders is enforced. In addition, because the update \nis dependent upon the actual request made by a client, the resulting output depends upon the happens-before \nordering being imposed. Thus, as in the MySQL example, under nondeterminism, it becomes critical to identify \nthe threads by the particular client requests to which they respond. Otherwise, the same happens\u00adbefore \nordering may be enforced twice, causing the same output to be produced. When both happens-before orderings \nare successfully imposed, the logs from the two executions differ, con.rming that the race is harmful. \nIn Apache2 and Apache1 of Table 3, we see that the frequencies of successful classi.cation parallel those \nof MySQL, emphasizing the commonality of the dif.culties they face under nondeterminism. Thus, we see \nthat execution indexing offers a tangible, real world bene.t over traditional identi.cation approaches \nusing in\u00adstructions and dynamic execution counts. For the purposes of con\u00adsistently identifying execution \npoints and identifying semantically appropriate execution points, execution indexing succeeds while traditional \nmethods are prone to failure.  7. Related Work Our work is related to [23], which tackles execution \nomission er\u00adrors. They decide if an implicit dependence exists between an exe\u00adcuted statement and a predicate \nby forcing the branch outcome of the predicate to be its opposite. An execution alignment algorithm was \nused to align the switched execution to the original so that in\u00adTable 3. Potential and detected harmful \nraces. 1) Using tuple based identi.cation. 2) Using execution indices.  TSP1 399614390 13 1 1/50 MySQL1 \n4 4 1 23/50 Apache1 7 6 6 27/50 TSP2 478861017 19 1 50/50 MySQL2 4 4 1 50/50 Apache2 7 6 6 50/50 formation \ncollected in the switched run can be migrated back to the original run. The alignment algorithm is based \non matching exe\u00adcution regions. It is of.ine and requires constructing the dynamic program dependence \ngraph. A similar of.ine algorithm was used by Liang et al. in [9] to identify similar execution traces \nfor fault localization. Neither of these works realized that the challenges are essentially an execution \nindexing problem, which indeed is a grand challenge for dynamic program analysis and has the potential \nto im\u00adpact a wide range of applications. This work, for the .rst time, for\u00admalizes the problem, devises \nef.cient online algorithms, and pro\u00adposes the concept of semantic anchor points to deliver .exibility. \nExecution monitoring detects patterns of events at runtime. In the context of aspect oriented programming \n[18, 3], languages are proposed to describe event patterns and automata are constructed to parse these \npatterns at runtime. The goal is to address the program\u00admer s design concerns such as checking if the \nprocess of enumer\u00adation, comprising a sequence of access events to the enumerator object, is intervened \nby a modi.cation to the underlying collec\u00adtion. PQL [13] designs a query language to provide similar \npattern matching at runtime for the purposes of debugging and security. PQL is able to recognize non-regular \nlanguages. In comparison, our work has a similar observation that describing execution by context free \nlanguages provides the capability of ignoring irrelevant part of an execution. However, our work has \na different goal and does not require the programmer to pre-de.ne pattern languages. This work is tightly \nrelated to dynamic control dependence detection. The existing work [14, 20] disclose that dynamic control \ndependence has a stack-like structure at runtime so that a control dependence stack is proposed to detect \ndependence. Our technique is based on dynamic control dependence detection. However, as our goal is to \nindex program execution, which has a different set of applications and higher requirement on runtime \noverhead, we focus more on de.ning the concepts of execution indexing and optimizing the algorithms. \nThis work is related to program trace representation, which records program execution at a .ne-grained \nlevel for later inspec\u00adtion. Depending on applications, various information can be traced such as control \n.ow [11, 24], values [5, 4], addresses [6], and so on. Existing tracing techniques focus on compressing \ntraces. Our technique focuses on providing meaningful identi.cation for exe\u00adcution points. It does not \nrely on traces. In particular, whole pro\u00adgram paths [11] represent traces with grammar rules. However, \nthe rules are derived for the optimal compression performance and can\u00adnot be used for the purpose of \nindexing.  8. Conclusion We propose a novel dynamic analysis called execution indexing which provides \na unique identi.cation for an execution point so that points in one execution can be correlated and points \nacross multiple executions can be aligned. Execution indexing can serve as a cor\u00adnerstone in various \napplications such as pro.ling, debugging espe\u00adcially in the presence of nondeterminism, dynamic instrumentation \nand so on. We formally de.ne the concepts of execution indexing, devise a highly optimized online algorithm, \nand conduct a client study on applying execution indexing to a light weight method to produce a failure \ninducing schedule for a data race warning without relying on a tracing and replay system.   Acknowledgments \nThis work is supported by grants from NSF grants CNS-0720516 and CNS-0708464 to Purdue University. References \n[1] Apache bug. http://issues.apache.org/bugzilla/show bug.cgi?id=25520. [2] MySQL bug. http://bugs.mysql.com/bug.php?id=791. \n[3] C. Allan, P. Avgustinov, A. S. Christensen, L. Hendren, S. Kuzins, O. Lhot\u00b4ak, O. de Moor, D. Sereni, \nG. Sittampalam, and J. Tibble. Adding trace matching with free variables to AspectJ. In Proceedings of \nthe 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, \npages 345 364, San Diego, CA, USA, 2005. ACM Press. [4] S. Bhansali, W.-K. Chen, S. de Jong, A. Edwards, \nR. Murray, M. Drinic, D. Mihocka, and J. Chau. Framework for instruction\u00adlevel tracing and analysis of \nprogram executions. In Proceedings of the 2nd International Conference on Virtual Execution Environments, \npages 154 163, Ottawa, Canada, June 2006. ACM. [5] M. Burtscher and M. Jeeradit. Compressing extended \nprogram traces using value predictors. In Proceedings of the 12th International Conference on Parallel \nArchitectures and Compilation Techniques, pages 159 169, New Orleans, Louisiana, 2003. [6] T. M. Chilimbi. \nEf.cient representations and abstractions for quantifying and exploiting data reference locality. In \nProceedings of the 2001 ACM SIGPLAN Conference on Programming Language Design and Implementation, pages \n191 202, Snowbird, UT, June 2001. ACM. [7] J. Dean, J. E. Hicks, C. A. Waldspurger, W. E. Weihl, and \nG. Chrysos. Pro.leMe: hardware support for instruction-level pro.ling on out-of\u00adorder processors. In \nProceedings of the 30th annual ACM/IEEE international symposium on Microarchitecture, pages 292 302, \nResearch Triangle Park, NC, 1997. [8] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence \ngraph and its use in optimization. ACM Transactions on Programming Languages and Systems, 9(3):319 349, \n1987. [9] L. Guo, A. Roychoudhury, and T. Wang. Accurately choosing execution runs for software fault \nlocalization. In 15th International Conference on Compiler Construction, pages 80 95, Vienna, Austria, \n2006. [10] J. J. Harrow. Runtime checking of multithreaded applications with visual threads. In Proceedings \nof the 7th International SPIN Workshop on SPIN Model Checking and Software Veri.cation, pages 331 342, \nLondon, UK, 2000. Springer-Verlag. [11] J. R. Larus. Whole program paths. In Proceedings of the 1999 \nACM SIGPLAN Conference on Programming language Design and Implementation, pages 259 269, Atlanta, Georgia, \nMay 1999. ACM. [12] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: detecting atomicity violations via access \ninterleaving invariants. In Proceedings of the 12th International Conference on Architectural Support \nfor Programming Languages and Operating Systems, pages 37 48, San Jose, CA, 2006. [13] M. C. Martin, \nV. B. Livshits, and M. S. Lam. Finding application errors and security .aws using PQL: a program query \nlanguage. In Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, \nLanguages, and Applica\u00adtions, pages 365 383, San Diego, CA, 2005. [14] W. Masri, A. Podgurski, and D. \nLeon. Detecting and debugging insecure information .ows. In Proceedings of the 15th International Symposium \non Software Reliability Engineering (ISSRE 04), pages 198 209, Saint-Malo, Bretagne, France, 2004. [15] \nS. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and B. Calder. Automatically classifying benign and \nharmful data races using replay analysis. In Proceedings of the ACM SIGPLAN 2007 Conference on Programming \nLanguage Design and Implementation, pages 22 31, San Diego, CA, June 2007. [16] S. Savage, M. Burrows, \nG. Nelson, P. Sobalvarro, and T. Anderson. Eraser: a dynamic data race detector for multithreaded programs. \nACM Transactions on Computer Systems, 15(4):391 411, 1997. [17] L. Van Put, D. Chanet, B. De Bus, B. \nDe Sutter, and K. De Bosschere. DIABLO: a reliable, retargetable and extensible link-time rewriting framework. \nIn Proceedings of the 2005 IEEE International Symposium On Signal Processing And Information Technology, \npages 7 12, Athens, Greece, December 2005. IEEE. [18] R. J. Walker and K. Viggers. Implementing protocols \nvia declarative event patterns. In Proceedings of the 12th ACM SIGSOFT Interna\u00adtional Symposium on Foundations \nof Software Engineering, pages 159 169, Newport Beach, CA, 2004. [19] T. Wang and A. Roychoudhury. Using \ncompressed bytecode traces for slicing Java programs. In Proceedings of the 26th International Conference \non Software Engineering, pages 512 521, Edinburgh, United Kingdom, May 2004. [20] B. Xin and X. Zhang. \nEf.cient online detection of dynamic control dependence. In Proceedings of the ACM/SIGSOFT International \nSymposium on Software Testing and Analysis, pages 185 195, London,UK, July 2007. ACM. [21] M. Xu, R. \nBod\u00b4ik, and M. D. Hill. A serializability violation detector for shared-memory server programs. In Proceedings \nof the ACM SIGPLAN 2005 Conference on Programming Language Design and Implementation, pages 1 14, Chicago, \nIL, 2005. ACM. [22] A. Zeller. Isolating cause-effect chains from computer programs. In Proceedings of \nthe 10th ACM SIGSOFT Symposium on Foundations of Software Engineering, pages 1 10, Charleston, SC, November \n2002. ACM. [23] X. Zhang, S. Tallam, N. Gupta, and R. Gupta. Towards locating execution omission errors. \nIn Proceedings of the ACM SIGPLAN 2007 Conference on Programming Language Design and Implementation, \npages 415 424, San Diego, California, USA, June 2007. ACM. [24] Y. Zhang and R. Gupta. Timestamped whole \nprogram path representation and its applications. In Proceedings of the 2001 ACM SIGPLAN Conference on \nProgramming Language Design and Implementation, pages 180 190, Snowbird, UT, June 2001. ACM.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Execution indexing uniquely identifies a point in an execution. Desirable execution indices reveal correlations between points in an execution and establish correspondence between points across multiple executions. Therefore, execution indexing is essential for a wide variety of dynamic program analyses, for example, it can be used to organize program profiles; it can precisely identify the point in a re-execution that corresponds to a given point in an original execution and thus facilitate debugging or dynamic instrumentation. In this paper, we formally define the concept of execution index and propose an indexing scheme based on execution structure and program state. We present a highly optimized online implementation of the technique. We also perform a client study, which targets producing a failure inducing schedule for a data race by verifying the two alternative happens-before orderings of a racing pair. Indexing is used to precisely locate corresponding points across multiple executions in the presence of non-determinism so that no heavyweight tracing/replay system is needed.</p>", "authors": [{"name": "Bin Xin", "author_profile_id": "81351594691", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1022792", "email_address": "", "orcid_id": ""}, {"name": "William N. Sumner", "author_profile_id": "81418595402", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1022793", "email_address": "", "orcid_id": ""}, {"name": "Xiangyu Zhang", "author_profile_id": "81384614270", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1022794", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375611", "year": "2008", "article_id": "1375611", "conference": "PLDI", "title": "Efficient program execution indexing", "url": "http://dl.acm.org/citation.cfm?id=1375611"}