{"article_publication_date": "06-07-2008", "fulltext": "\n Grammar-based Whitebox Fuzzing PatriceGodefroid MicrosoftResearch Redmond,WA,USA pg@microsoft.com \n AdamKie.zun MassachusettsInstitute of Technology ComputerScience andArti.cial IntelligenceLaboratory \nCambridge,MA,USA akiezun@mit.edu MichaelY.Levin MicrosoftCenterforSoftware Excellence Redmond,WA,USA \n mlevin@microsoft.com Abstract Whitebox fuzzing is a form of automatic dynamic test gen\u00aderation,based \non symbolic execution and constraint solving, designed for security testing of large applications. Unfortu\u00adnately, \nthe current effectiveness of whitebox fuzzing is lim\u00aditedwhentesting applications withhighly-structuredinputs, \nsuch as compilers and interpreters. These applications pro\u00adcesstheirinputsin stages, such aslexing,parsing \nand evalu\u00adation.Due to the enormous number of controlpathsin early processing stages, whitebox fuzzing \nrarely reaches parts of theapplicationbeyondthose .rst stages. In thispaper,we studyhow to enhance whiteboxfuzzing \nof complex structured-input applications with a grammar\u00adbased speci.cationof their validinputs.Wepresent \na novel dynamic test generation algorithm where symbolic execu\u00adtion directly generates grammar-based \nconstraints whose satis.abilityis checked using a customgrammar-based con\u00adstraint solver.Wehaveimplementedthis \nalgorithm and eval\u00aduatedit on alarge security-critical application,theJavaScript interpreter of Internet \nExplorer 7 (IE7). Results of our ex\u00adperiments show that grammar-based whitebox fuzzing ex\u00adploresdeeperprogrampaths \nand avoidsdead-endsdueto non-parsableinputs.Comparedto regularwhiteboxfuzzing, grammar-basedwhitebox \nfuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from53% to81% \nwhile using three timesfewer tests. Categories and Subject Descriptors D.2.4 [Software Engi\u00adneering]:Software/ProgramVeri.cation; \nD.2.5[SoftwareEn\u00adgineering]:Testing andDebugging; F.3.1[Logics and Mean\u00adings of Programs]: Specifying \nand Verifying and Reasoning aboutPrograms General Terms Veri.cation,Algorithms,Reliability Keywords Software \nTesting, Automatic Test Generation, Grammars,ProgramVeri.cation Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish,topost on servers orto redistributetolists, \nrequiresprior speci.c permission and/or afee. PLDI 08, June7 13,2008,Tucson,Arizona,USA. Copyright c \n. 2008ACM978-1-59593-860-2/08/06. . .$5.00 1. Introduction Blackboxfuzzingisaformof testing,heavily \nusedfor .nding security vulnerabilitiesin software.It simply consistsin ran\u00addomly modifying well-formedinputsand \ntesting the result\u00ading variants[3,12].Blackboxfuzzing sometimes uses gram\u00admars togeneratethe well-formedinputs, \nas well asto encode application-speci.cknowledge andtestheuristicsforguid\u00adingthegeneration ofinput variants[1,37]. \nA recently proposed alternative, whitebox fuzzing [16], combinesfuzz testing withdynamic testgeneration[6,14]. \nWhitebox fuzzing executes the program under test with an initial,well-formedinput,both concretely and \nsymbolically. During the execution of conditional statements, symbolic execution creates constraints \nonprograminputs.Those con\u00adstraints capturehowtheprogramusesitsinputs, and satis\u00adfying assignments for \nthe negation of each constraint de.ne new inputs that exercise different control paths. Whitebox fuzzing \nrepeats this process for the newly created inputs, with the goal of exercising many different control \npaths of the program under test and .nding bugs as fast as possi\u00adble using various searchheuristics.Inpractice,the \nsearchis usually incomplete because the number of feasible control paths maybeastronomical(evenin.nite) \nandbecausethe precision of symbolic execution, constraint generation and solvingisinherentlylimited.Nevertheless,whiteboxfuzzing \nhas been shown to be very effective in .nding new security vulnerabilitiesin several applications. Unfortunately, \nthe current effectiveness of whitebox fuzzing is limited when testing applications with highly\u00adstructured \ninputs. Examples of such applications are com\u00adpilers and interpreters. These applications process their \nin\u00adputsin stages, such aslexing,parsing and evaluation.Due to the enormous number of control paths in \nearly process\u00ading stages, whitebox fuzzing rarely reaches parts of the application beyond these .rst \nstages. For instance, there are manypossible sequences ofblank-spaces/tabs/carriage\u00adreturns/etc. separatingtokensin \nmost structuredlanguages, each correspondingto adifferent controlpathinthelexer.In addition topath explosion, \nsymbolic executionitself maybe defeated already inthe .rstprocessing stages.Forinstance, lexers often \ndetect language keywords by comparing their pre-computed,hard-codedhashvalues withthehash values of strings \nreadfrom the input; this effectivelyprevents sym\u00adbolic executionand constraint solving fromevergenerating \ninputstringsthatmatchthosekeywordssincehashfunctions cannotbeinversed(i.e.,given a constraint x == hash(y) \nand a valuefor x, one cannot compute a valuefor y that sat\u00adis.es this constraint). 1 // Reads and returns \nnext token from file. 2 // Terminates on erroneous inputs. 3 Token nextToken(){ 4 ... 5 readInputByte(); \n6 ... 7 } 8 9 // Parses the input file, returns parse tree. 10 // Terminates on erroneous inputs. 11 \nParseTree parse(){ 12 ... 13 Token t = nextToken(); 14 ... 15 } 16 17 void main(){ 18 ... 19 ParseTree \nt = parse(); 20 ... 21 Bytecode code = codeGen(t); 22 ... 23 } Figure 1. Sketch of aninterpreter.Theinterpreterprocesses \nthe inputs in stages: lexer (function nextToken), parser (function parse), and codegenerator(function \ncodeGen). Next, the interpreter executes thegeneratedbytecode(omit\u00adtedhere). Inthispaper, wepresent grammar-based \nwhitebox fuzzing, which enhances whitebox fuzzing with a grammar-based speci.cation of validinputs.Wepresent \nadynamic testgen\u00aderation algorithm where symbolic execution directly gen\u00aderates grammar-based constraints \nwhose satis.ability is checked using a custom grammar-based constraint solver. The algorithmhas twokeycomponents: \n1. Generationofhigher-levelsymbolicconstraints,expressed in terms of symbolic grammar tokens returned \nby the lexer,instead ofthetraditional[6,14,16] symbolicbytes read asinput. 2. A custom constraint solver \nthat solves constraints on symbolicgrammar tokens.The solver looksfor solutions that satisfy the constraints \nand are accepted by a given (context-free)grammar.  Assuming the grammar accepts inputs only if they \nare parsable, our algorithm never generates non-parsable in\u00adputs,i.e.,it avoidsdead-endsinthelexer andparser.More\u00adover, \nthe grammar-based constraint solver can complete a partial set of token constraints into a fully-de.ned \nvalid in\u00adput, hence avoiding exploring many possible non-parsable completions. By restricting the search \nspace to valid inputs, grammar-basedwhiteboxfuzzing can exercisedeeperpaths, andfocusthe search ontheharder-to-test,deeperprocessing \nstages. We have implementedgrammar-based whitebox fuzzing and evaluated it on a large application, the \nJavaScript inter\u00adpreter ofthe Internet Explorer7 Web-browser.Results of ex\u00adperiments showthatgrammar-basedwhiteboxfuzzing \nout\u00adperforms whiteboxfuzzing,blackboxfuzzingandgrammar\u00adbasedblackboxfuzzingin overallcodecoverage,while \nusing fewer tests. Example. Considertheinterpreter sketchedinFigure1and the JavaScript grammar partially \nde.ned in Figure 2. By tracking the tokens returned by the lexer, i.e., the function nextToken (line \n3) in Figure 1, and considering those as symbolicinputs, ourdynamictestgeneration algorithmgen-FunDecl \n. function id ( Formals) FunBody FunBody . { SrcElems} SrcElems . o SrcElems . SrcElemSrcElems Formals \n. id Formals . id , Formals SrcElem . ... ... Figure 2. Fragmentof a context-freegrammarforJavaScript. \nNonterminalshave names startingwith uppercase.Symbol o denotes the empty string. The starting nonterminal \nis Fun-Decl. erates constraints in terms of such tokens.For instance, run\u00adning theinterpreter on the \nvalidinput function f(){} may correspond to the sequence of symbolic token con\u00adstraints token0 = function \ntoken1 = id token2 = ( token3 = ) token4 = { token5 = } Negating the fourth constraint in this path constraint \nleads to the new sequence of constraints: token0 = function token1 = id token2 = ( token3 . ) There are \nmany ways to satisfy this constraints but most so\u00adlutionsleadtonon-parsableinputs.Incontrast,ourgrammar\u00adbased \nconstraint solver can directly conclude that the only way to satisfy this constraint while generating \na valid input accordingto thegrammaris to set token3 = id and to complete the remainder oftheinput with, \nsay, token4 = ) token5 = { token6 = } Thus,thegeneratedinputthatcorrespondstothis solutionis function \nf(id){} where id canbe anyidenti.er. Similarly,thegrammar-basedconstraintsolver canimme\u00addiatelyprovethat \nnegating thethird constraintintheprevi\u00adouspath constraint, thusleadingto the newpath constraint token0 \n= function token1 = id token2 . ( is unsolvable, i.e., there are no inputs that satisfy this con\u00adstraint \nand are recognizedby thegrammar.Grammar-based whitebox fuzzing prunes in one iteration the entire sub\u00adtree \nof lexer executions corresponding to all possible non\u00adparsableinputs matchingthis case.  2. Grammar-based \nWhitebox Fuzzing In this section, we recall the basic notions behind whitebox fuzzing(Section2.1) andintroducegrammar-based \nwhite\u00adbox fuzzing (Section 2.2). We then discuss how to check grammar-based constraintsfor context-freegrammars(Sec\u00adtion \n2.3). Finally, we discuss additional aspects of our ap\u00adproachand some ofitslimitations(Section2.4). \n2.1 Whitebox Fuzzing Algorithm1 shows the whiteboxfuzzing algorithm[16](the underlinedtext shouldbeignoredfor \nnow).Given a sequen\u00adtial deterministic program P under test and an initial pro\u00adgraminputI, thisdynamic \ntestgeneration algorithmgener\u00adates newtestinputsby negating constraintsgenerateddur\u00adingthe symbolic execution \nofprogramPwithinput I.These newinputs exercisedifferent executionpathsin P.Thispro\u00adcess is repeated and \nthe algorithm executes the program with new inputs multiple times each newly generated in\u00adput may lead \ntothegeneration of additionalinputs.The al\u00adgorithmterminates when a testingtimebudgetexpires or no moreinputs \ncanbegenerated. The algorithm starts by checking whether running pro\u00adgram P with input I triggersaruntime \nerror(line3).The algorithm associates an attribute bound with each input, ini\u00adtially set to 0 (line4),as \nan optimization that avoidsgenerat\u00adingthe samepathconstraint multipletimes.Variable worklist representsapriorityqueue \nofinputsthathaveyettobe ex\u00adplored.Thequeueisinitializedto a singleton containing the initialinput(line6).The \nalgorithmthen enters aniterative phase that continues as long as there are more inputs to ex\u00adplore,and \naslong as the timebudget allows(line7).Aslong asthisisthe case, aninputistaken out ofthe worklist (line8). \nThen, the program is executed symbolically on the input (line9).The resultofsymbolicexecution(explainedbelow)is \na pathconstraint, which is a conjunction of constraints on the program sinputparameters,c1 .... .cn,that \nare all satis.ed on the current execution path. The algorithm then creates newtestinputsby modifying \nthepath constraint(lines10 16),asfollows.Foreachpre.xof thepath constraint(longer than the bound associated \nwith the last input), the algorithm negatesthelast conjunct(line11).A solution,ifit exists,to such an \nalternativepath constraint correspondsto aninput that will execute the program along the pre.x of the \norigi\u00adnal executionpath,but take the opposite branch of the condi\u00adtional statement corresponding to the \nlast constraint in that pre.x(assuming symbolic execution and constraint solving have perfect precision, \notherwise the actual execution may diverge from this path). The algorithm calls the constraint solver \nto .nd a concrete input that satis.es the alternative path constraint(line12).If such a value exists(line13), \nand can be found by the constraint solver, this new test input is run and checked(line 14), its bound \nis set to value i (line15), anditis addedto thequeue(line16). Symbolic execution, implemented in procedure \nexecuteSymbolic in Algorithm 1, is a well established technique [20]. Here we consider a particular form \nof symbolic execution which is carried out dynamically, while the program is running on a particular \ninput. Dynamic execution allows any imprecision in symbolic execution to be alleviated using concrete \nvalues and randomization: whenever symbolic execution does not know how to gen\u00aderate a constraint for \na program statement depending on some inputs, one can always simplify this constraint using the concrete \nvalues of thoseinputs[14].Symbolic execution takes the program and the input and records how the parameters:Program \nP,input I,grammar G result :Bugsin P 1 Procedure grammarBasedWhiteboxFuzzing(P, I, G): 2 bugs . \u00d8 3 \nbugs . bugs .Run&#38;Check(P, I) 4 I.bound . 0 5 worklist . emptyQueue() 6 enqueue(worklist, I) 7 while \nnot empty(worklist) and not timeExpired() do 8 input . dequeue(worklist) 9 c1 .... .cn . executeSymbolic(P, \ninput) 10 for i . input.bound,. . .,ndo 11 pc . c1 .... .ci-1 .\u00acci 12 newInput . solve(pc, G) 13 if \nnewInput . . then 14 bugs . bugs .Run&#38;Check(P, newInput) 15 newInput.bound . i 16 enqueue(worklist, \nnewInput) 17 return bugs 18 Procedure executeSymbolic(P, I): 19 pathconstraint pc . true 20 foreach instructioninst \nexecutedby P with I do 21 update the symbolic store 22 switch instdo case returnfrom tokenizationfunction \n23 marktoken as symbolic variable 24 25 case input-dependentconditional statement 26 c . expressionfor \nthe executedbranch 27 pc . pc .c 28 otherwise if false. instreadsbytefrom I then 30 markinputbyte as \nsymbolic variable; 31 return pc 29 Algorithm 1: Grammar-based whitebox fuzzing. Changes from whiteboxfuzzing \nare underlined.The auxiliarypro\u00adcedure executeSymbolic (lines18 31)changesforgrammar\u00adbased fuzzing. Grammar-based \nwhitebox fuzzing requires the constraint solver(auxiliaryprocedure solve)to handle grammar constraints(Algorithm2). \nprogram s input affects the control .ow in the program. The result of symbolic execution is a path constraint \nthat is a logic formula that is satis.ed by the currently executed concrete input and any other concrete \ninput that will drive theprogram s execution along the same controlpath.Sym\u00adbolic variables in the path \nconstraint refer to bytes in the program s input. The algorithm keeps a symbolic store that mapsprogramvariablesto \nsymbolic expressionscomposed of symbolic variables and constants. The algorithm updates the symbolic \nstore whenever theprogram manipulatesinput data(line21).At every conditional statement thatinvolves symbolic \nexpressions,the algorithm extendsthe currentpath constraint pc with an additional conjunct c that represents \nthe branch of the conditional statement taken in the current execution (line 27). At every instruction \nthat reads a byte from the input, a symbolic variable is associated with the inputbyte(line30). parameters:Path \nconstraint pc,grammar G with start symbol S result : s . L(pc) n L(G) or . 1 Procedure solve(pc, G): \n2 R . buildConstraint(pc) G' 3 . G 4 G' . duplicateproductionsfor starting nonterminal S 5 G' . rename \nS to S ' (butnotin theduplicated productions) 6 n . highestindexi oftokeni variablein R 7 for i . 1 \n... n do 8 let ci denote the constraintin R on variable tokeni 9 worklist W . productionsforS in G' \n 10 while not empty(W) do 11 prod . dequeue(W) 12 Si . ith symbolin prod.rhs 13 if Si is nonterminal \nN then 14 add copies of prod to W and G' , with Si expanded using allproductionsfor N in G' (unroll) \n 15 else 16 remove prod fromG' if Si does not satisfy ci (prune) 17 if L(G' ) = \u00d8 then 18 return . 19 \nelse 20 return generate s fromG' Algorithm 2: Procedure solve(pc, G) implements a context-free constraint \nsolver. The auxiliary function buildConstraint(pc) converts the path constraint pc to a regular expression. \nNotation prod.rhs denotes the right hand side oftheproduction prod.  2.2 Grammar-based Extension to \nWhitebox Fuzzing Grammar-based whitebox fuzzing is an extension of the al\u00adgorithm in Section 2.1. The \nunderlined text in Algorithm 1 contains the necessary changes.  The new algorithm requires a grammar \nG that describes validprograminputs(line1).  Instead of marking thebytesinprograminputs as sym\u00adbolic(line30),grammar-basedwhiteboxfuzzing \nmarks tokens returned from a tokenization function such as nextToken inFigure1assymbolic(line24);thusgrammar\u00adbased \nwhitebox fuzzing associates a symbolic variable with each token1, and symbolic execution tracks the in\u00ad.uence \nof the tokens on the control path taken by the programP.  The algorithmusesthegrammarGto requirethatthe \nnew inputnot only satis.esthe alternativepath constraintbut is alsoin thelanguage acceptedbythegrammar(line12). \nAs the examples in the introduction illustrate, this ad\u00additional requirementgivestwo advantagestogrammar\u00adbased \nwhitebox fuzzing: it allows pruning of the search tree corresponding toinvalidinputs(i.e.,inputsthat \nare not accepted by the grammar), and it allows the direct  1Symbolic variables could also be associated \nwith other values re\u00adturnedbythetokenizationfunctionfor speci.c types oftokens,such as the string value \nassociated with each identi.er, the numerical value associated with each number, etc. completion of satis.able \ntoken constraints into valid in\u00adputs.  2.3 Context-free Constraint Solver The constraint solver invoked \nin line 12 of Algorithm 1 im\u00adplements theproceduresolve and computes language inter\u00adsection:it checks \nwhether thelanguage L(pc) ofinputs satis\u00adfying the path constraint pc contains an input that is in the \nlanguage accepted by the grammar G. By construction, the language L(pc) is always regular, as we discuss \nlater in this section. If G is context-free, then language intersection with L(pc) isdecidable.IfG is \ncontext-sensitive, then a soundand completedecisionprocedurefor computing languageinter\u00adsection may not \nexist(but approximations arepossible).In whatfollows, we assume that G is context-free. We assume that \nthe set T of tokens that can be returned by the tokenization function is .nite. Therefore, all token \nvariables tokeni have a .nite range T, and satis.ability of any constraint on a .nite set of token variables \nis decidable. Given any such constraint pc, one can sort its set of token variables tokeni by their index \ni, representing the total order by which they have been created by the tokenization func\u00adtion, andbuild \na regularexpression(language) R represent\u00ading L(pc) for that constraint pc. Acontext-freeconstraint solver \ntakesasinputsa context-free grammarG and a regular expression R, and returns either a string s . L(G) \nn L(R), or . iftheintersectionis empty. Algorithm2presents adecisionprocedurefor sucha con\u00adstraint solver. \nThe algorithm exploits the fact that, by con\u00adstruction, any regularlanguage R always constrains onlythe \n.rst n tokens returned by the tokenization function, where n is the highest index i of a token variable \ntokeni appear\u00ading in the constraint represented by R. The algorithm starts by converting the path constraint \ninto a regular expression R (line2).Thisis straightforwardand involvesgrouping the constraintsin pc bythetoken \nvariableindex.The next3lines (lines 3 5)are technical steps to eliminate recursion for the start symbol \nS .The algorithm employs a simple unroll-and\u00ad ith pruneapproach:inthe iterationof themainloop(line7), \nthe algorithm unrolls the right-hand sides of productions to expose a 0 ... i pre.x ofterminals(line14), \nandprunes thoseproductionsthat violatethe constraint ci on the ith to\u00adken variabletokeni in the regular \nexpressionR (line16).Dur\u00ading each round of unrolling andpruning, the algorithm uses the worklist W to \nstore productions that have not yet been unrolled and examinedfor conformance with the regular ex\u00adpression. \nAfter the unrolling and pruning, the algorithm checks emptiness [18] of the resulting language L(G' ) \nand gener\u00adates a string s from the intersection grammar G' (line 20). For speed, our implementation uses \na bottom-up strategy that generates a string with the lowest derivation tree for each nonterminalinthegrammar,by \ncombining the strings from the right-hand sides of productions for nonterminals. This strategy is fast \ndue to memoizing strings during gen\u00aderation. Section 2.4 discusses alternatives and limitations of Algorithm2. \nSolving example. We illustrate the algorithm on an exam\u00adple,a simpli.edS-expressiongrammar.Starting withtheini\u00adtialgrammar, \nthe algorithm unrolls andprunesproductions given a regularpathconstraint.Thegrammaris(S isthe start symbol, \nnonterminals are uppercase) S . (let ((id S)) S) |(Op S S) |num |id Op . + |- and the regularpath constraintis \ntoken1 .{(} token2 .{+} token3 .{(} token4 .{(, ), num, id, let} Beforethe mainiteration(line7), thegrammaris: \nS ' . (let ((id S ' )) S ' ) |(Op S ' S ' ) |num |id Op . + |- S . (let ((id S ' )) S ' ) |(Op S ' S \n' ) |num |id Next, the main iteration begins. The .rst conjunct in the grammar constraint is token1 .{(}, \ntherefore the algorithm (line16)removesthelasttwoproductionsfromthegrammar. The resultisthefollowinggrammar(executionisnowback \nat the top of theloopinline7). S ' . (let ((id S ' )) S ' ) |(Op S ' S ' ) |num |id Op . + |- S . (let \n((id S ' )) S ' ) |(Op S ' S ' ) In the next iteration of the for loop, the algorithm ex\u00adamines the second \nconjunct in the regular path constraint, token2 .{+}.Thealgorithmprunesthe .rstproductionrule from S \nsince let does not match + (line 16), and then ex\u00adpands the nonterminal Op in the production S . (Op \nS ' S ' ) (line 14). The production is replaced by two productions, S . (+ S ' S ' ) and S . (- S ' S \n' ), which areaddedtothe work\u00adlistW.Thegrammar G' is then S ' . (let ((id S ' )) S ' ) |(Op S ' S ' ) \n|num |id Op . + |- S . (+ S ' S ' ) |(- S ' S ' ) In the next iteration of the while loop, the second \nof the newproductionsis removedfrom thegrammar(line16)be\u00adcauseit violatesthegrammar constraint.Afterthe \nremoval, the executionis now again at the top oftheloopinline7. S ' . (let ((id S ' )) S ' ) |(Op S ' \nS ' ) |num |id Op . + |- S . (+ S ' S ' ) After 2 more iterations of the for loop, the algorithm ar\u00adrivesatthe \n.nalgrammar S ' . (let ((id S ' )) S ' ) |(Op S ' S ' ) |num |id Op . + |- S . (+ (let ((id S ' )) S \n' ) S ' ) As the last two steps, the algorithm checks that L(G' ) . \u00d8 (line 17) and generates a string \ns from the .nal grammar G' for the intersection of G and R (line 20). Our bottom\u00adup strategygeneratesthe \nstring (+ (let (( id num )) num) num). From this string of tokens, our tool generates a matching string \nof input bytes by applying an application-speci.c de\u00adtokenizationfunction. 2.4 Discussion and Limitations \nComputing language intersection. Computingtheintersec\u00adtion of a context-free grammar with a regular expression \nis a well-known problem. A standard polynomial-time algo\u00adrithm consistsintranslating thegrammarinto apushdown \nautomaton, translating the regular expression into a .nite\u00adstate automaton, computing the product of \nthese two au\u00adtomatatoobtainanotherpushdownautomaton,and .nally translating the resulting pushdown automaton \nback into a context-free grammar. Alternatively, the intersection can be computed withoutthe explicit \nautomata conversion[39],by an adaptation ofthe context-free reachability algorithm[27]. The unroll-and-prune \nalgorithm wepresentinSection2.3 is simplerbecauseitdoes notgothrough an explicitpush\u00addown automaton translation \nand exploits the structure of the regular language that describes the path constraint on only the .rst \nn tokens returned by the tokenization func\u00adtion, where n is thehighestindexi of a token variable tokeni \nappearing in the constraint represented by R. This algo\u00adrithm is not polynomial in general, but performs \nsatisfac\u00adtorilyinpracticefor small values of n (around 50 60). Also, if thegrammarisleft-recursive,Algorithm \n2 may not termi\u00adnate. However, context-free grammars for .le formats and programming languages are rarely \nleft-recursive, and left\u00adrecursion canbe ef.ciently removed[29]. Approximate grammars. Grammar-basedwhiteboxfuzzing \ncanbe used with approximategrammars.Let us call anin\u00adputparsableiftheparser successfullyterminates when \nrun on thatinput.Ifthegrammaraccepts allparsableinputs orover\u00adapproximates the set ofparsableinputs, \nthen Algorithm1 is sound:itdoesnotprune any of thefeasiblepathsforwhich theparser successfullyterminates. \nIn practice, the set of valid inputs speci.ed by a gram\u00admarisboundtobe some approximation ofthe set ofparsable \ninputs.Indeed,parserstypically implement additional vali\u00addation(e.g., simpletype-checking) thatis notpart \nof atyp\u00adical grammar description of the language. Other grammars mayhave some context-sensitivebehaviors \n(asinproto\u00adcoldescriptionlanguageswhere avariable sizeparameter k is followed by k records), that are \nomitted or approximated in a context-free or regular manner. Other grammars, espe\u00adcially for network \nprotocols, are simpli.ed representations of validinputs, anddo not requirethefullpower of context\u00adsensitivity[4,9,31]. \nDomain knowledge. Grammar-based whitebox fuzzing re\u00adquires a limited amount of domain knowledge, namely \nthe formalgrammar,identifying the tokenization function to be instrumented, and providing a de-tokenization \nfunction to generate input byte strings from input token strings gener\u00adatedby a context-free constraint \nsolver.Webelievethisis not a severepractical limitation.Indeed,grammars are typically available for many \ninput formats, and identifying the tok\u00adenization function is, in our experience, rather easy, even in \nunknown code,providedthat the source codeis available or that the tokenizationfunctionshas a standard \nname, such as token, nextToken, scan, etc. For instance, we found the tokenizationfunctionintheJavaScriptinterpreter \nofInternet Explorer7inamatterof minutes,bylookingforcommonly used namesin the symboltable. Lexer and \nparser bugs. Usingagrammarto .lteroutinvalid inputs may reduce code coverage in the lexer and parser \nthemselves, since the grammar explicitly prevents the exe\u00adcution of codepathshandlinginvalidinputsin \nthose stages. For testing those stages, traditional whiteboxfuzzing canbe used. Moreover, our experiments \n(Section 3) indicate that grammar-based whitebox fuzzing does not decrease cover\u00adagein thelexer orparser. \nGrammar-based whitebox fuzzing approach uses the ac\u00adtual lexer and parser code of the program under test. \nIn\u00addeed,removing theselayersand using automaticallygener\u00adated software stubs simulating thoseparts mayfeed \nunreal\u00adisticinputs to the restoftheprogram.  3. Evaluation We evaluate grammar-based whitebox fuzzing \nexperimen\u00adtally anddesign experiments with thefollowinggoals: Comparegrammar-based whiteboxfuzzing to \nother ap\u00adproaches, grammar-less as well as blackbox. We compare how the varioustestgeneration strategiesperform \nwitha limitedtimebudgetand also examinetheirbehavior over longperiods oftimeinSection3.5.1.  Measure \nwhether test inputsgeneratedby our technique are effective in exercising deep execution paths in the \napplication, i.e., reaching beyond the lexer and parser. Section3.5.1gives the relevant experimental \nresults.  Measure how the set of inputs generated by each tech\u00adnique compares. In particular, do inputs \ngenerated by grammar-based whitebox fuzzing exercise the program inwaysthatothertechniquesdonot?Section3.5.2presents \nthe results.  Measure the effectiveness of token-level constraints in preventing path explosion in the \nlexer. See Section 3.5.3 for the results.  Measure the performance of the grammar constraint solver \nofSection2.3 with respectto the size oftestinputs. Section3.5.4discussesthispoint.  Measure the effectiveness \nof the grammar-based ap\u00adproachinpruningthe searchtree.SeeSection3.5.5.  The rest of this section describes \nour experiments and discusses the results. Naturally, because they come from a limited sample, these \nexperimental results need to be taken with caution.However,ourevaluationisextensiveandper\u00adformed with \na large, widely-used JavaScript interpreter, a representative real-world program. 3.1 Subject Program \nWe performed the experiments with the JavaScript inter\u00adpreter embedded in the Internet Explorer 7 Web-browser. \nOur experimental setup runs the interpreter with no source modi.cations. The total size of the JavaScript \ninterpreter is 113562 machine instructions. In our experiments, we also measure coverage in the lexer, \nparser and code gen\u00aderator modules of the interpreter. Their respective sizes are 10410, 18535 and 3693 \nmachine instructions. The code generatoristhe deepest of theexamined modules,i.e., ev\u00adery input that \nreaches the code generator also reaches the other two modules (but the converse does not hold). The lexer \nandparser are equallydeep,because theparser always calls thelexer. Weusethe of.cialJavaScriptgrammar2.Thegrammaris \nquitelarge:189productions,82terminals(tokens), and102 nonterminals. 3.2 Test Generation Strategies We \nevaluate thefollowingtestinputgeneration strategies,to compare them togrammar-based whiteboxfuzzing. \nblackbox generates test inputs by randomly modifying an initial input. We use a Microsoft-internal widely-used \nblackboxfuzzing tool. grammar-based blackbox generates test inputs by creating random strings from a \ngiven grammar. We use a strat\u00ad egy thatgenerates strings of agivenlength uniformly at 2 http://www.ecma-international.org \n strategy seed random tokens inputs blackbox .. grammar-based blackbox .. whitebox . whitebox+tokens \n.. grammar-based whitebox .. Figure 3. Test input generation strategies evaluated and their characteristics. \nThe seed inputs column indicates which strategies require initial seed inputs from which to gener\u00adate \nnew inputs. The random column indicates which strate\u00adgies use randomization. The tokens column indicates \nwhich strategiesusethelexical speci.cation(i.e.,tokens) of thein\u00adput language. Each technique s name \nindicates whether the technique uses a grammar and whether is it whitebox or blackbox. random[26],i.e.,each \nstring of agivenlengthis equally likely. whitebox generates test inputs using the whitebox fuzzing algorithm \nof Section 2.1. We use an existing tool, named SAGE, that implements this algorithm for x86 Windows applications[16]. \nwhitebox+tokens extends whitebox fuzzing with only the lexical part of the grammar, i.e., marks token \nidenti.ers as symbolic, instead of individual input bytes, but does not use agrammar.This strategy wasimplemented \nas an extension ofSAGE. grammar-based whitebox is the grammar-based whitebox fuzzing algorithm of Section \n2, which extends whitebox fuzzing both using symbolic tokens and an input gram\u00admar.This strategy was \nalsoimplemented as an extension oftheSAGE tool. Figure 3 tabulates the strategies used in the evaluation \nand shows their characteristics. Other strategies are conceivable. For example, whitebox fuzzingcouldbe \ncombineddirectly with thegrammar, with\u00adouttokens.Doing so requirestransformingthegrammarinto a scannerless \ngrammar [33]. Another possible strategy is boundedexhaustive enumeration[23,36]. Wehave notin\u00adcluded \nthe latter in our evaluation because, while all other strategies we evaluated can be time-bounded (i.e., \ncan be stopped at anytime), exhaustive enumeration upto somein\u00adput length is biased if terminated before \ncompletion, which makesithardtofairly compareto time-boundedtechniques. 3.3 Methodology To avoid bias \nwhen using test generation strategies that require seed inputs (see Figure 3), we use 50 seed inputs \nwith15 to20 tokensgenerated randomlyfromthegrammar. Section 3.4 provides more information about selecting \nthe size of seed inputs. Also, to avoid bias across all strategies, we run all experimentsinsidethe same \ntestharness. The whitebox+tokens and grammar-based whitebox strate\u00adgiesrequireidentifyingthetokenizationfunctionthatcreates \ngrammar tokens. Our implementation allows doing so in a simple way,by overriding a singlefunction. For \neach ofthe examined modules(lexer,parser and code generator),we measurethe reachabilityrate,i.e.,thepercent\u00adage \nofinputsthat execute atleast oneinstruction ofthe mod\u00adule.Deeper modules alwayshavelower reachability \nrates. We measure instruction coverage, i.e., the ratio of the number of unique executed instructions \nto all instructions size reach average maximum (tokens) code gen. % coverage % coverage % 6 100 8.5 8.5 \n10 76.0 8.2 9.2 20 67.0 8.3 9.7 30 38.0 7.5 9.8 50 9.0 6.5 10.1 100 1.0 6.3 10.4 120 0.0 6.2 6.8 150 \n0.0 6.2 6.7 200 0.0 6.2 6.7 Figure 4. Coverage statistics for nine sets of 100 inputs each,generated \nrandomly fromtheJavaScriptgrammar us\u00ading the same uniform generator as grammar-based blackbox. The reach \ncodegen. columndisplaysthepercentageofthe generatedinputsthat reachthe codegenerator module.The two right-most \ncolumns display the average and the maxi\u00admum coverage of the wholeinterpreterfor thegeneratedin\u00adputs. \nin the module of interest. This coverage metric seems the most suitable for our needs, since we want \nto estimate the bug-.nding potential of the generated inputs, and blocks with more instructions are more \nlikely to contain bugs than shortblocks.In addition to the totalinstruction coveragefor theinterpreter, \nwe also measure coveragein thelexer,parser and codegenerator modules. We run each test input generation \nstrategy for 2 hours. The2-hour timeincludesall experimentaltasks:program ex\u00adecution, symbolic execution \n(where applicable), constraint solving (where applicable), generation of new inputs and coverage measurements. \nTo see whether giving more time changes the results, we also let each strategy run much longer, until \ninstruction coverage does not increase during thelast10hours.SeeSection3.5.1. For reference, we also \ninclude coverage data and reach\u00adability results obtained with a manual test suite, created over several \nyears by the developers and testers of this JavaScript interpreter. The suite consists of more than 2,800 \nhand-craftedinputs that exercisetheinterpreter thoroughly. 3.4 Seed Size Selection Fourof ourgeneration \nstrategiesrequireseedinputs(Fig\u00adure 3). To avoid bias stemming from using arbitrary inputs, we useinputsgeneratedrandomlyfromtheJavaScriptgram\u00admar.Thelength \nofthe seedinputs mayin.uence subsequent testinputgeneration.Toselectthe rightlength,wegenerate inputs \nof different sizes and measure the coverage achieved by each of thoseinputs as well as whatpercentage \nofinputs reaches the codegenerator.For eachlength, wegenerate100 inputs andperformthe measurements onlyfor \nthoseinputs. Figure4presentsthe results. The .ndingsarenotimmediatelyintuitive:longerinputs achieve, \non average,lower total coverage.The reasonis that the of.cialJavaScriptgrammaris only apartial speci.cation \nof what constitutes syntactic validity.Thegrammardescribes an over-approximation of the set ofinputs \nacceptable by the parser. Longer, randomly generated, inputs are more likely to be accepted by the grammar \nand rejected by the parser. For example, the grammar speci.es that break statements may occur anywhere \nin the function body, while the parser enforces that break statements may appear only in loops and switch \nstatements.Enforcing thisispossibleby mod\u00adifying the grammar but it would make the grammar much larger.Another \nexampleofover-approximationconcernsline breaks and semicolons. The standard speci.es that certain semicolons \nmay be omitted, as long as there are appropri\u00adate line breaks in the .le3. However, the grammar does \nnot enforce this requirement and allows omitting all semicolons. By analyzing the results in Figure 4, \nwe select 15 to 20 as the size range, in tokens, of the input seeds we use in other experiments. This \nlength makes the seed inputs vari\u00adable without sacri.cingthepenetration rate(i.e., reachability ofthe \ncodegeneration module).  3.5 Results 3.5.1 Coverage and Penetration Figure 5 tabulates the coverage \nand reachability results for the 2-hour runs with each of the .ve automated test gen\u00aderation strategiespreviouslydiscussed.For \ncomparison, re\u00adsults obtained with the manually-written test suite are also included,even though runningit \nrequires morethan2hours (as those2,820inputJavaScriptprograms are typicallymuch larger and takes more \ntime tobeprocessed). Among all the automated test generation strategies con\u00adsidered, grammar-based whitebox \nachieves the best total cov\u00aderage as well as the best coverage in the deepest examined module,the codegenerator.It \nachieves resultsthat areclos\u00adesttothe manualtest suite, whichpredictablyprovidesthe bestcoverage.The \nmanual suiteisdiverse and extensive,but was developed with the cost of many man-months of work. In contrast, \ngrammar-based whitebox requires minimalhuman effort, andquicklygenerates relativelygoodtestinputs. We \ncan also observethefollowing. Grammar-based whitebox fuzzing achieves much better coverage than regular \nwhitebox fuzzing.  Grammar-based whitebox fuzzing performs also signi.\u00adcantly better than grammar-based \nblackbox. Even though the latter strategy achieved good coverage in the code generator, whitebox strategies \noutperformblackbox ones in total coverage.  Grammar-based whitebox fuzzing achieves thehighest cov\u00aderageusingthefewestinputs,which \nmeansthatthis strat\u00adegy generates inputs of higher quality. Generating few, high-quality test inputs \nis important for regression test\u00ading.  The blackbox and whitebox strategies achieved similar re\u00adsultsinallcategories.Thisshowsthat, \nwhentesting appli\u00adcations withhighly-structuredinputsin alimited amount oftime(2hours),whiteboxfuzzing, \nwiththepower of symbolic execution, does not improve much over sim\u00adpleblackboxfuzzing.In fact,in the \ncodegenerator, those grammar-less strategies do not improve coverage much above theinitial set of seedinputs. \n Reachability results show that almost all tested inputs reach the lexer. A few inputs generated by \nthe blackbox and whitebox strategies contains invalid, e.g., non-ASCII, characters and the interpreter \nrejects them before using the lexer. To exercise the interpreter well, inputs must reachthedeepest module,the \ncodegenerator.The results showthat grammar-basedwhitebox hasthehighestpercent\u00adage of suchdeep-reachinginputs. \n To analyzethelonggeneration-timebehavior ofthe exam\u00adinedstrategies,welet each strategy runfor aslong \nasitkeeps covering newinstructions atleast every10hours.The results 3See Section 7.9 of the speci.cation: \nhttp://interglacial. com/javascript spec/a-7.html#a-7.9  strategy inputs total lexer parser code generator \n coverage % reach % coverage % reach % coverage % reach % coverage %    blackbox 8658 14.2 99.6 \n24.6 99.6 24.8 17.6 52.1  grammar-based blackbox 7837 11.9 100 22.1 100 24.1 72.2 61.2  whitebox 6883 \n14.7 99.2 25.8 99.2 28.8 16.5 53.5  whitebox+tokens 3086 16.4 100 35.4 100 39.2 15.5 53.0  grammar-based \nwhitebox 2378 20.0 100 24.8 100 42.4 80.7 81.5  seed inputs 50 10.6 100 18.4 100 20.6 66.0 50.9  \nmanual test suite 2820 58.8 100 62.1 100 76.4 100 91.6 Figure 5. Coverage results for 2-hour runs. \nThe seed inputs row lists statistics for the 50 seed inputs used by some of the test generation strategies(seeSections3.3 \nand3.4).The manual test suite takes more than 2 hours to run and is included here for reference.The inputs \ncolumngivesthenumberofinputstestedby each strategy .The total coverage columngivesthetotal instruction \ncoveragepercentage.Coverage statisticsforlexer,parser andcodegenerator modules aregiveninthe corresponding \ncolumns.The reach columnsgivethepercentageofinputsthat reach themodule sentry-point. strategy S only \nSS and GBW only GBW blackbox 4.9 62.5 32.6 grammar-based blackbox 2.2 56.2 41.6 whitebox 7.2 61.3 31.5 \nwhitebox+tokens 10.9 62.0 27.1 Figure 6. Relative coveragein%comparedto grammar-based whitebox (GBW).Thecolumn \nonly S givesthetotal number ofinstructions coveredby each strategybut not byGBW.The column S andGBW gives \nthe total number ofinstructions coveredbyboth strategies.Thelast columngives the total of instructionscoveredby \nonlyGBW . are that, after the initial 2 hours, each con.guration reaches around90% of coveragethatitis \neventually capable of reach\u00ading(this validates our selection ofthe2-hourtimelimitfor our experiments.)Thelonggeneration-time \nruns con.rmthe .ndings of the 2-hour runs: grammar-based whitebox fuzzing isthe mosteffective ofthe examinedtechniques, \nasit reaches thehighest coverage andkeepsdiscovering new codeforthe longestthanthe othertechniques(97hoursforgrammar-based \nwhitebox vs.84 hoursfor whitebox and 82 hours for grammar\u00adbasedblackbox). In summary,the results ofthese \nexperiments validate our claim that grammar-based whitebox fuzzing is effective in reachingdeeperintothetestedapplication \nand exercisingthe codemorethoroughly than otherautomated testgeneration strategies.  3.5.2 Relative \nCoverage Figure 6 compares the instructions covered with grammar\u00adbasedwhitebox fuzzingand the other analyzed \nstrategies.The numbers show that the inputs generated by grammar-based whitebox cover most oftheinstructions \ncoveredbytheinputs generatedby the other strategies(seethe small numbersin the column only S ), while \ncovering many other instruc\u00adtions(seethelargenumbersinthecolumn onlyGBW ). Combined with the results \nof Section 3.5.1, this shows that grammar-based whitebox fuzzing achieves the highest to\u00adtal coverage, \nhighest reachability rate and coverage in the deepest module, while using the smallest number of inputs. \nIn other words,grammar-basedwhitebox createstestsinputs of thehighestquality among the analyzed strategies. \n 3.5.3 Statistics on Symbolic Executions Figure 7 presents various statistics related to the symbolic \nexecutionsperformedduringthe 2hours runs of each of the three whitebox strategies evaluated. We make \nthe following observations. All three whitebox strategies perform roughly the same number of symbolic \nexecutions.  However, the whitebox strategy creates a larger average number of symbolic variablesbecauseit \noperates on char\u00adacters, while the other two strategies work on tokens (cf.Figure3).  The whitebox+tokens \nstrategy creates the smallest average number of symbolic variables per execution. This is be\u00adcause whitebox+tokens \ngenerates many unparsable inputs (cf.Figure5), whichtheparser rejects early and therefore no symbolic \nvariables are created for the tokens after the parse error.  Figure7also showshow constraint creationisdistributed \namong thelexer,parser and codegenerator modules ofthe JavaScriptinterpreter.Thetwo token-basedstrategies(white\u00adbox+tokens \nand grammar-based whitebox) generate no con\u00adstraints in the lexer. This helps avoiding path explosion \nin that module.Those strategiesdo explorethelexer(indeed, Figure5 showshigh coverage)but theydo notgetlostin \nits error-handlingpaths. All strategiescreate constraintsinthedeepest,codegen\u00aderator, module. However, \nthere are few such constraints be\u00adcause theparser transforms the stream of tokensinto anAb\u00adstractSyntaxTree(AST) \nand subsequent code,likethe code generator,operates on theAST.WhenprocessingtheASTin later stages, symbolic \nvariables associated with input bytes or tokens are largely absent, so symbolic execution does not create \nconstraints from code branches in these stages. The number of symbolic constraintsin thosedeeper stages \ncould be increased by associating symbolic variables with other values returned by the tokenization function \nsuch as string andinteger values associated with some tokens.  3.5.4 Context-Free Constraint Solver \nPerformance To measure the performance of the grammar constraint solver, we repeated the 2-hour grammar-based \nwhitebox run 9 times withdifferentsizes of seedinputs(between10 and200 tokens). The average number of \nsolver calls per symbolic execution was between 23 and 53 (with no obvious corre\u00adlation between seed \ninput size and the average number of solver calls).The results arethat upto58% oftotal execution time \nwas spentin the constraint solver(with no obvious cor\u00adrelation between seed size and solving time). Such \nsolving times aretypicalindynamictestgeneration(e.g.,a recent report[15] indicates up to62% ofthetestgenerationtime \nspentin the constraint solver). strategy created constraints % symbolic average number of average number \nof  lexer parser code gen. executions symbolic variables constraints    whitebox 66.6 33.1 0.3 131 \n57.1 297.7 whitebox+tokens 0.0 98.0 2.0 170 11.8 66.9  grammar-based whitebox 0.0 98.0 2.0 143 21.1 \n113.0 Figure 7. Symbolic execution statistics for 2-hour runs of whitebox strategies. The created constraints \ncolumns shows the percentages of all symbolic constraints created in the three analyzed modules of the \nJavaScript interpreter. The symbolic executions column gives the total number of symbolic executions \nduring each run. The two right-most columns give the average number of symbolic variablesper symbolic \nexecution andthe average number of symbolic constraintsper symbolic execution.  3.5.5 Grammar-based \nSearch Tree Pruning Grammar-basedwhiteboxfuzzingis effectiveinpruningthe search tree. In our 2-hour experiments, \n29.0% of grammar constraints are unsatis.able. When a grammar constraint is unsatis.able, the corresponding \nsearch tree is pruned be\u00adcause there is no input that satis.es the constraint and is valid accordingto \nthegrammar.  4. Related Work Systematicdynamic testgeneration[6,14] isbecomingin\u00adcreasinglypopular[2,16,34] \nbecauseit .ndsbugswithout generatingfalse alarms andrequires no domainknowledge. Our work enhances dynamic \ntest generation by taking ad\u00advantage of aformalgrammar representing validinputs, thus helpingthegeneration \noftestinputsthatpenetratethe appli\u00adcationdeeper. Miller spioneeringfuzzingtool[28]generatedstreams of \nrandombytes,but mostpopularfuzzerstoday support some form of grammar representation, e.g., SPIKE4, Peach5, \nFile-Fuzz6, Autodaf\u00b4e7. Sutton et al. present a survey of fuzzing techniques andtools[37].Workongrammar-basedtestinput \ngeneration startedinthe1970 s[17,32] and canbebroadly dividedinto random[8,24,25,35] and exhaustivegenera\u00adtion[21,23].Imperativegeneration[7,10,30] \nisa related ap\u00adproachin which a custom-madeprogramgeneratesthein\u00adputs(ineffect,theprogramencodesthegrammar).In \nsys\u00adtematic approaches, test inputs are created from a speci.ca\u00adtion,given either a specialpiece of code(e.g.,Korat[5]) \nor a logic formula(e.g., TestEra[19]). Grammar-based test in\u00adputgenerationis an example ofmodel-basedtesting(seeUt\u00adting \net al. for a survey[38]), whichfocuses on covering the speci.cation(model) whengenerating testinputs \nto check conformance of theprogramwith respecttothemodel.Our work also usesformalgrammars as speci.cations.However, \nin contrast to blackbox approaches, our approach analyses the codeof theprogramundertest andderivesnewtestin\u00adputsfromit. \nPath explosionin systematicdynamic testgeneration can be alleviated by performing test generation composition\u00adally[13],by \ntesting functions systematicallyinisolation,en\u00adcoding and memoizing test results as function summaries \nusing function input preconditions and output postcondi\u00adtions, and re-using such summaries when testing \nhigher\u00adlevelfunctions.Agrammarcanbeviewed as a specialform of user-provided compact summary for the entire \nlexer/\u00adparser, that may include over-approximations. Computing such a .nite-sizesummary automatically \nmaybeimpossible 4 http://www.immunitysec.com/resources-freesoftware.shtml 5 http://peachfuzz.sourceforge.net/ \n 6 http://labs.idefense.com/software/fuzzing.php 7 http://autodafe.sourceforge.net due to in.nitely \nmany paths or limited symbolic reasoning capability when analyzingthelexer/parser.Grammar-based whitebox \nfuzzing and test summaries are complementary techniques which couldbe used simultaneously. Another approachtopath \nexplosion consists of abstract\u00adinglower-levelfunctions usingsoftwarestubs, markingtheir return values \nas symbolic, and then re.ning these abstrac\u00adtions to eliminate unfeasibleprogrampaths[22].In contrast, \ngrammar-based whitebox fuzzing is always grounded in concrete executions, andthusdoes not requirethe \nexpensive step of removing unfeasiblepaths. Emmiet al. [11]extend systematictesting with constraints \nthat describe the state of the data for database applications. Our approach also solvespath and data \nconstraints simulta\u00adneously,but oursisdesignedfor compilersandinterpreters instead ofdatabase applications. \nMajumdar and Xu s recent and independent work [23] is closest to ours. These authors combine grammar-based \nblackbox fuzzing with dynamic test generation by exhaus\u00adtively pre-generating strings from the grammar \n(up to a given length), and then performing dynamic test genera\u00adtion starting fromthosepre-generated \nstrings,treating only variable names, number literals etc. as symbolic.Exhaustive generation inhibits \nscalability of this approach beyond very short inputs. Also, the exhaustive grammar-based genera\u00adtion \nand the whiteboxdynamictestgenerationpartsdo not interact with each other in Majumdar and Xu s framework. \nIn contrast, ourgrammar-based whiteboxfuzzing approach is morepowerful asit exploits thegrammarfor solving \ncon\u00adstraintsgeneratedduring symbolic execution togeneratein\u00adputvariants that areguaranteed tobe valid. \n 5. Conclusion Weintroducedgrammar-basedwhiteboxfuzzingto enhance the effectivenessofdynamictestinputgenerationfor \nappli\u00adcations with complex, highly-structured inputs, such as in\u00adterpreters and compilers.Grammar-based \nwhitebox fuzzing tightly integrates constraint-based whitebox testing with grammar-basedblackboxtesting,andleveragesthe \nstrengths ofboth. As shown by our in-depth study with the IE7 JavaScript interpreter, grammar-based whitebox \nfuzzing generates higher-quality tests that exercise more code in the deeper, harder-to-testlayersofthe \napplication undertest(seeFig\u00adure5).In our experiments,grammar-basedwhiteboxfuzzing strongly outperformedboth \nwhitebox fuzzing and blackbox fuzzing. Code generator coverage improved from 61% to 81%anddeep reachabilityimprovedfrom72%to80%.Deep \nparts ofthe application are the hardest to test automatically and our technique showshow to addressthis. \nSincegrammars areboundtobepartial speci.cations of valid inputs, grammar-based blackbox approaches are \nfun\u00addamentallylimited.Thanks to whiteboxdynamic testgener\u00adation, some of this incompleteness can be recovered, \nwhich explains why grammar-based whitebox fuzzing also out\u00adperformed grammar-based blackbox fuzzing in \nour exper\u00adiments. Acknowledgments We thankMichaelD.Ernst andDannyDigfor comments on drafts ofthispaper.We \nalso thank the anonymous reviewers fortheir suggestionstoimprovethepresentation.The work ofAdamKie. zun \nwasdone mostly while visitingMicrosoft. References [1] D.Aitel.TheAdvantagesofBlock-BasedProtocolAnalysisfor \nSecurityTesting. ImmunityInc.,February,2002. [2] S. Artzi, A. Kie.zun, J. Dolby, F. Tip, D. Dig, A. Paradkar, \nand M. D. Ernst. Finding bugs in dynamic Web applications. Technical Report MIT-CSAIL-TR-2008-006, MIT \nComputer Science andArti.cialIntelligenceLaboratory,Cambridge,MA, Feb.2008. [3] D. Bird and C. Munoz. \nAutomatic Generation of Random Self-CheckingTest Cases. IBM Systems Journal, 22(3):229 245, 1983. [4] \nN. Borisov, D. Brumley, H. Wang, J. Dunagan, P. Joshi, and C. Guo. Generic application-level protocol \nanalyzer and its language. In NDSS,2007. [5] C. Boyapati, S. Khurshid, and D. Marinov. Korat: automated \ntestingbased onJavapredicates. In ISSTA,2002. [6] C.Cadar,V.Ganesh,P.Pawlowski,D.Dill, andD.Engler.EXE: \nautomaticallygeneratinginputs ofdeath. In CCS,2006. [7] K.ClaessenandJ.Hughes.QuickCheck:Alightweighttoolfor \nrandom testing ofHaskellprograms. In ICFP,2000. [8] D. Coppit and J. Lian. yagg: an easy-to-use generator \nfor structuredtestinputs. In ASE,2005. [9] W. Cui, J. Kannan, and H. J. Wang. Discoverer: Automatic protocolreverseengineeringfromnetwork \ntraces. In USENIX SecuritySymposium,2007. [10] B. Daniel, D. Dig, K. Garcia, and D. Marinov. Automated \ntesting of refactoring engines. In FSE,2007. [11] M. Emmi, R. Majumdar, and K. Sen. Dynamic test input \ngenerationfordatabase applications. InISSTA,2007. [12] J. E. Forrester and B. P. Miller. An Empirical \nStudy of the Robustness of Windows NT Applications Using Random Testing. In Proceedings of the 4th USENIX \nWindows System Symposium,Seattle,August2000. [13] P. Godefroid. Compositional Dynamic Test Generation. \nIn POPL,2007. [14] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed automated random testing. In \nPLDI,2005. [15] P. Godefroid, M. Levin, and D. Molnar. Active property checking. TechnicalReportMSR-TR-2007-91,Microsoft,2007. \n[16] P.Godefroid,M.Levin,andD.Molnar. Automated whitebox fuzz testing. In NDSS,2008. [17] K.Hanford. \nAutomaticGenerationofTestCases. IBMSystems Journal,9(4),1970. [18] J. Hopcroft and J. Ullman. Introduction \nto automata theory, languages andcomputation. Addison-WesleySeriesinComputer Science,1979. [19] S. Khurshid \nand D. Marinov. TestEra: Speci.cation-Based TestingofJavaProgramsUsingSAT. In ASE,2004. [20] J.King. \nSymbolic executionandprogramtesting. Communica\u00adtions of theACM,19(7):385 394,1976. [21] R. L\u00a8ammel and \nW. Schulte. Controllable combinatorial coverageingrammar-basedtesting. In TestCom,2006. [22] R. Majumdar \nand K. Sen. LATEST: Lazy dynamic test input generation. Technical Report UCB/EECS-2007-36, EECS Department,UniversityofCalifornia,Berkeley,2007. \n[23] R. Majumdar and R.-G. Xu. Directed test generation using symbolicgrammars. In ASE,2007. [24] B. \nMalloy and J. Power. An interpretation of Purdom s algorithmfor automaticgeneration of test cases. In \nICIS,2001. [25] P. Maurer. Generating test data with enhanced context-free grammars. IEEESoftware,7(4),1990. \n[26] B.McKenzie.Generatingstringsat randomfromacontextfree grammar. Technical Report TR-COSC 10/97, Department \nof ComputerScience,UniversityofCanterbury,1997. [27] D.Melski andT.Reps.Interconvertbility of set constraintsand \ncontext-freelanguage reachability. In PEPM,1997. [28] B.P.Miller,L.Fredriksen, andB.So.Anempirical study \nof the reliability ofUNIX utilities. Communications of theACM,33(12), 1990. [29] R. C. Moore. Removing \nleft recursion from context-free grammars. InProceedings ofthe .rstconference onNorthAmerican chapter \nof theAssociation forComputational Linguistics,2000. [30] C. Pacheco, S. K. Lahiri, M. D. Ernst, and \nT. Ball. Feedback\u00addirectedrandom testgeneration. In ICSE,2007. [31] R.Pang,V.Paxson,R.Sommer, andL.Peterson.binpac: \nayacc for writing applicationprotocolparsers. In IMC,2006. [32] P. Purdom. A sentence generator for testing \nparsers. BIT NumericalMathematics,12(3),1972. [33] D.J.SalomonandG.V.Cormack.ScannerlessNSLR(1)parsing \nofprogramminglanguages. In PLDI,1989. [34] K.Sen,D.Marinov,andG.Agha.CUTE: aconcolicunittesting engineforC. \nIn FSE,2005. [35] E. Sirer and B. Bershad. Using production grammars in software testing. In DSL,1999. \n[36] K. Sullivan, J. Yang, D. Coppit, S. Khurshid, and D. Jackson. Software assurance bybounded exhaustive \ntesting. In ISSTA, 2004. [37] M. Sutton, A. Greene, and P. Amini. Fuzzing: Brute Force VulnerabilityDiscovery. \nAddison-Wesley,2007. [38] M. Utting, A. Pretschner, and B. Legeard. A Taxonomy of Model-Based Testing. \nDepartment of Computer Science, The University ofWaikato,NewZealand,Tech.Rep,4,2006. [39] G.WassermannandZ.Su. \nSound andpreciseanalysisofWeb applicationsforinjection vulnerabilities. In PLDI,2007.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages.</p> <p>In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53% to 81% while using three times fewer tests.</p>", "authors": [{"name": "Patrice Godefroid", "author_profile_id": "81100504535", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1022785", "email_address": "", "orcid_id": ""}, {"name": "Adam Kiezun", "author_profile_id": "81100087482", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P1022786", "email_address": "", "orcid_id": ""}, {"name": "Michael Y. Levin", "author_profile_id": "81100404720", "affiliation": "Microsoft Center for Software Excellence, Redmond, WA, USA", "person_id": "P1022787", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375607", "year": "2008", "article_id": "1375607", "conference": "PLDI", "title": "Grammar-based whitebox fuzzing", "url": "http://dl.acm.org/citation.cfm?id=1375607"}