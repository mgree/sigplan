{"article_publication_date": "06-07-2008", "fulltext": "\n Deriving Linearizable Fine-Grained Concurrent Objects Martin Vechev IBM Research Abstract Practical \nand ef.cient algorithms for concurrent data structures are dif.cult to construct and modify. Algorithms \nin the literature are often optimized for a speci.c setting, making it hard to separate the algorithmic \ninsights from implementation details. The goal of this work is to systematically construct algorithms \nfor a concurrent data structure starting from its sequential implementation. Towards that goal, we follow \na construction process that combines manual steps corresponding to high-level insights with automatic \nexploration of implementation details. To assist us in this process, we built a new tool called PARAGLIDER. \nThe tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability \nof algorithms. Starting from a sequential implementation and assisted by the tool, we present the steps \nthat we used to derive various highly\u00adconcurrent algorithms. Among these algorithms is a new .ne\u00adgrained \nset data structure that provides a wait-free contains operation, and uses only the compare-and-swap (CAS) \nprimitive for synchronization. Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]; D.2.4 \n[Program Veri.cation] General Terms Algorithms, Veri.cation Keywords concurrent algorithms, veri.cation, \nsynthesis, model checking, linearizability 1. Introduction Concurrent data-structures, also known as \nconcurrent objects [15], allow the programmer of a concurrent system to work with the illu\u00adsion of a \nsequential data-structure, while permitting a high-level of concurrency. To achieve this objective, concurrent \nobjects are usu\u00adally implemented using .ne-grained locking, or other .ne-grained synchronization mechanisms \nsuch as compare-and-swap (CAS). Unfortunately, this makes them notoriously hard to design, imple\u00adment, \nand verify. This is especially true when their implementations employ low level pointer manipulations \n(see, e.g., [8]). Universal construction methodologies such as [14], provide a systematic way to construct \nconcurrent objects, but produce inef.\u00adcient results. Other popular concurrency mechanisms such as soft\u00adware \ntransactional memory [11], if used naively, can lead to in\u00adef.ciencies due to false sharing: unaware \nof the underlying data Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 08, June 7 13, 2008, Tucson, Arizona, USA. Copyright c . 2008 ACM 978-1-59593-860-2/08/06. \n. . $5.00. Eran Yahav IBM Research structure invariants, a transactional operation may restart unneces\u00adsary \neven if its effect is benign. To gain ef.ciency, one can try to reduce the scope of the transaction, \nbut then we are faced with sim\u00adilar challenges as designing .ne-grained data structures. Direct construction \nof speci.c algorithms is the realm of ex\u00adperts, and produces algorithms that are very ef.cient, but are \nal\u00adready specialized for a particular environment. This process mixes algorithmic insights with implementation \ncomplexity, and makes it hard to adapt the algorithms to a different setting, modify them, and verify \ntheir correctness. In this paper, we show how to systematically derive .ne-grained concurrent objects, \nstarting from a sequential implementation. Our derivation process combines manual steps that correspond \nto high\u00adlevel insights with automatic exploration of implementation de\u00adtails. The exploration procedure, \nimplemented in a tool called PARAGLIDER, helps the designer focus attention on speci.c al\u00adgorithmic variations \nby quickly rejecting incorrect ones. We show how the systematic derivation, assisted by the tool, yields \ninterest\u00ading and practical concurrent algorithms. To guarantee that a concurrent object appears to the \nprogrammer as a sequential data-structure, concurrent objects are often required to be linearizable [15]. \nIntuitively, linearizability provides the illu\u00adsion that any operation performed on a concurrent object \ntakes ef\u00adfect instantaneously at some point between its invocation and its re\u00adsponse. Existing approaches \nfor automatic veri.cation and check\u00ading of linearizability (e.g., [9, 10, 2, 30]) and of related correctness \nconditions (e.g., [6, 5]) are valuable. However, these generally ap\u00adply when the concurrent object has \nalready been implemented. In this paper, we follow a different direction, and provide a tool that can \nassist a designer in a systematic derivation process of lineariz\u00adable .ne-grained concurrent objects. \n 1.1 Motivating Example Fig. 1 shows a standard sequential implementation of a set data structure. This \nimplementation uses an underlying singly linked\u00adlist. The list is sorted in ascending key order, and \nuses two sen\u00adtinel nodes head (with the smallest possible key) and tail (with the largest possible key). \nThe set supports three operations: add, remove, and contains, with their standard meaning. All oper\u00adations \nuse a macro LOCATE to traverse the list and locate an item based on the value of its key. Fig. 2 shows \na concurrent set algorithm derived by a designer using PARAGLIDER. We explain the exact details of this \nalgorithm in Sec. 4.3. For now, it suf.ces to note that this algorithm is quite distant from the sequential \nimplementation of Fig. 1, and in particular, uses the lower-level CAS synchronization primitive. Note \nthat designing practical algorithms that use a double compare and swap primitive (DCAS) is a challenging \ntask [8]. Algorithms relying only on CAS are even more complex and involved [21]. The design of a concurrent \nobject requires expertise and de\u00adsigner insights, but also requires dealing with a large number of implementation \ndetails. Particular design decisions are being made bool add(int key) { Entry *pred,*curr,*entry; LOCATE(pred, \ncurr, key) k =(curr->key == key) if (k) return f alse entry = new Entry(key) entry->next = curr pred->next \n= entry return true } bool remove(int key) { bool contains(int key) { LOCATE(pred, curr, key) { Entry \n*pred,*curr,*r; Entry *pred,*curr; pred = head LOCATE(pred, curr, key) LOCATE(pred, curr, key) curr = \nhead->next k = (curr->key = key) k = (curr->key == key) while(curr->key < key){ if (k) return f alse \nif (k) return true pred = curr r = curr->next if (\u00ack) return f alse curr = curr->next pred->next = r \n} } return true } } Figure 1. A sequential implementation of a set algorithm based on a sorted singly-linked-list. \nboolean add(int key) { Entry *pred,*curr,*entry; restart : LOCATE(pred, curr, key) k =(curr->key == key) \nif (k) return f alse entry = new Entry(key) entry->next = curr val = CAS(&#38;pred->next, (curr.ptr, \n0*, (entry.ptr, 0*) if (\u00acval) goto restart return true } boolean remove(int key) { boolean contains(int \nkey) { Entry *pred,*curr,*r Entry *pred,*curr; restart : LOCATE(pred, curr, key) LOCATE(pred, curr, key) \nk =(curr->key == key) k =(curr->key key) if (\u00ack) return = f alse if (k) return f alse if (k) return true \nr = curr->next } lval = CAS(&#38;curr->next, r, (r.ptr, 1*) if (\u00aclval) goto restart pval = CAS(&#38;pred->next, \n(curr.ptr, 0*, (r.ptr, 0*) if (\u00acpval) goto restart return true } Figure 2. A concurrent set algorithm \nusing a marked bit to mark deleted nodes. The bit is stored with the next pointer in a single word. Synchronization \nis implemented using CAS. containsdoes not use synchronization and does not restart. The LOCATE is the \none of Fig. 1. based on the environment (e.g., available memory model and syn\u00adchronization primitives) \nand the requirements from the algorithm (e.g., required memory overhead and progress guarantees). Each \ndesign decision entails different low level implementation details. The designer wishing to obtain a \nworking algorithm for her speci.c setting is often forced to follow a single path of design choices due \nto the cost of exploring alternatives. When designing such algorithms, the high-level skeletal struc\u00adture \nof the algorithm is often known. The key to an ef.cient de\u00adsign process is the ability to quickly check \nvarious design choices. The designer may be facing questions relating to alternative design choices such \nas: can synchronization be reduced by adding more coordination meta-data? can I obtain a correct algorithm \nwith lower space overhead? can I obtain a similar algorithm for a slightly dif\u00adferent environment? (e.g., \nassuming no garbage collection.) Our system is geared towards helping such experts in the design process \nof concurrent objects. The system allows the domain expert to specify her insights and assists her by \nexploring a space of algorithms based on the provided insights. For example, the insight in the concurrent \nalgorithm of Fig. 2 is that a marked bit has to be used to coordinate between threads. Introducing a \nmarked bit for a deleted node was .rst used in the lock-free FIFO queue algorithm of Prakash et. al. \n[23] . This in\u00adsight, later used in [12], can lead to many different implementa\u00adtions, depending on the \nenvironment of the concurrent object, and requirements imposed on it. For example, Michael [21] used \nthis insight as a basis for a concurrent set algorithm that operates with explicit memory management \n(making the problem signi.cantly harder). Heller et. al. [13] used this insight as the basis of a lock\u00adbased \nalgorithm. In this paper, we show how PARAGLIDER can as\u00adsist a designer in the systematic derivation \nof such algorithms.  1.2 Domain-speci.c Exploration The usage scenario of PARAGLIDER is one that combines \nman\u00adual transformations with automatic exploration. The designer pro\u00advides the insightful leaps by manual \ntransformations of algorithm schemas, and the system .lls the lower-level details by performing a combinatorial \nsearch around each of the provided schemas. Since different algorithms produced from the same schema \ncan have dif\u00ad Fig.9 Figure 3. Flow of derivation in this paper. ferent tradeoffs in terms of space overhead, \nsynchronization and progress, PARAGLIDER produces a set of results. The designer can choose the appropriate \nalgorithm from the reported set. PARAGLIDER allows the designer to provide domain-speci.c knowledge about \nthe space of algorithms being explored by: (i) de.ning a partial order of correctness-implication between \nal\u00adgorithms; (ii) de.ning an equivalence between algorithms. This enables the exploration procedure to \noften infer the correctness (or incorrectness) of an algorithm by the correctness (or incorrectness) \nof another algorithm in the search space. This reduces the number of algorithms that are checked by the \nmodel checker to less than 3% of the total number of algorithms in the space. This reduction is the key \nto feasibility of exploration. 1.3 Derivation Flow and Correctness Conditions Fig. 3 shows an overview \nof the example derivation process de\u00adscribed in this paper. The starting point is the sequential algorithm \nof Fig. 1. This algorithm, when considered in a concurrent setting, is not linearizable. A naive solution \nwould be to put each operation inside a coarse-grained atomic section. In this case, the resulting al\u00adgorithm \nwould be linearizable, but permit no concurrent operations. The basic question that we ask is: How do \nwe derive an algorithm with .ne-grained synchro\u00ad nization, while preserving correctness? We answer this \nquestion by following a systematic derivation process which combines manual transformations (shown as \nsolid arrows), with automatic exploration (shown as dashed arrows). The manual transformations capture \ndesigner insights and produce al\u00adgorithm schemas (shown as half-full circles) from which auto\u00admatic exploration \ncan proceed. Automatic exploration instantiates a schema into a set of algorithms (each shown as a circle) \nby .ll\u00ading the missing lower-level implementation details (such as order between operations, and partition \nto atomic sections). We use . to denote cases when all instances of a schema fail the correctness check. \nCorrectness Conditions All automatic exploration is done while checking for linearizability. More details \nare provided in Sec. 5. 1.4 Main Results The main contributions of this paper are: PARAGLIDER,a tool \nthat assists algorithm-designers to system\u00adatically derive concurrent algorithms by exploring an algorithm \nspace based on the designers insights.  The tool checks algorithms in the space for linearizability, \nand supports checking of linearizability both with .xed lineariza\u00adtion points and with automatic linearization. \n The tool uses a domain-speci.c exploration that leverages the relationship between algorithms in the \nspace to reduce the num\u00adber of algorithms that have to be checked by the model checker.  We show how \nthe tool is used in a systematic derivation process. We derive a variety of concurrent set algorithms, \nincluding algorithms close to the ones of [12], [13] and [21]. Speci.cally, we derive a new concurrent \nset algorithm that provides a wait\u00adfree contains operation, and uses only the compare-and\u00adswap (CAS) \nprimitive for synchronization.  1.5 Assumptions Our current implementation makes the following assumptions: \n we assume an underlying sequentially consistent memory model. Our tool can operate without this assumption, \nbut the correctness condition will have to be adapted to that setting (e.g., sequential consistency, \nas in [5]), and the state space ex\u00adplored by the checking procedure will increase signi.cantly.  we \nassume that algorithms operate in the presence of a garbage collector, and that a node removed from a \ndata-structure is never reused. This is a standard assumption (e.g., [13]) that allows us to focus on \nthe algorithms and ignore details of explicit memory management. The addition of explicit memory management \nis a separate challenging problem (see, e.g., [21]).  We focus on the exploration of large spaces of \nalgorithms and use bounded model-checking to check for linearizability and safety of algorithms. Automatic \nveri.cation of linearizability for the algo\u00adrithms we explore remains a problem for future work. At this \npoint, the system can be viewed as producing a set of candidate algorithms in a systematic manner. In \nparticular, the system narrows the space of algorithms that the user has to examine by ruling out algorithms \nobserved as incorrect. inv a(4) a(4)/true inv c(4) c(4)/false inv a(7) a(7)/true inv r(4) r(4)/true inv \nc(7) c(7)/false inv c(7) c(7)/true (H1) inv a(4) a(4)/true inv r(4) r(4)/true inv c(4) c(4)/false inv \nc(7) c(7)/false inv a(7) a(7)/true inv c(7) c(7)/true (L1) inv a(4) a(4)/true inv c(4) c(4)/true inv \na(7) a(7)/true inv r(4) r(4)/true c(7) c(7)/false inv c(7) c(7)/true (H2) Figure 4. Concurrent histories \nand possible sequential histories corresponding to them.  2. Background: Linearizability Linearizability \n[15] is a commonly used correctness criterion for implementations of concurrent objects. Intuitively, \nlinearizability provides the illusion that any operation performed on a concurrent object takes effect \ninstantaneously at some point between its invo\u00adcation and its response. The linearizability of a concurrent \nobject is veri.ed with respect to a speci.cation of the desired behavior of the object in a sequen\u00adtial \nsetting. This sequential speci.cation de.nes a set of permitted sequential executions. Informally, a \nconcurrent object is lineariz\u00adable if each concurrent execution of operations on the object is equivalent \nto some permitted sequential execution, in which the real-time order between non-overlapping operations \nis preserved. The equivalence is based on comparing the arguments of operation invocations, and the results \nof operations (responses). Other correctness criteria in the literature, such as sequential consistency \n[17], and serializability [22] also require that a concur\u00adrent history be equivalent to some sequential \nhistory in which op\u00aderations appear to have executed atomically (i.e., without interleav\u00ading). However, \nthese criteria differ on the requirements on ordering of operations. Sequential consistency requires \nthat operations in the sequential history appear in an order that is consistent with the or\u00adder seen \nat individual threads. Serializability is de.ned in terms of transactions, and requires that transactions \nappear sequentially, that is, without interleavings. Note that a transaction may include sev\u00aderal operations, \nand may operate on several objects. Compared to these correctness criteria, linearizability is more intuitive, \nas it preserves the real-time ordering of non-overlapping operations. In addition, linearizability is \ncompositional, meaning that a system consisting of linearizable objects is guaranteed to be linearizable \n[15]. EXAMPLE 2.1. Fig. 4 shows two concurrent histories H1 and H2 , and a sequential history L1 . All \nhistories involve two threads in\u00advoking operations on a shared concurrent set. In the .gure, we abbreviate \nnames of operations, and use a,r, and c, for add, remove, and contains, respectively. We use inv op(x) \nto denote the invocation of an operation op with an argument value x, and op/valto denote the response \nopwith return value val. Consider the history H1 . For now, ignore the star symbols. In this history, \nadd(4) is overlapping with remove(4), and add(7)overlaps contains(7). The history H1 is linearizable. \nWe can .nd an equivalent sequential history that preserves the global order of non-overlapping operations. \nThe history L1 is a possible linearization of H1 (in general, a concurrent history may have multiple \nlinearizations). In contrast, the history H2 is non-linearizable. This is because remove(4)returns true \n(removal succeeded), and contains(4) that appears after remove(4)in H2 also returns true.However, the \nhistory H2 is sequentially consistent. Checking linearizability is challenging because it requires cor\u00adrelating \nany concurrent execution with a corresponding permitted sequential execution. In some cases, it is possible \nto specify for each operation a point in the code (more generally, a set of alternative points) in which \nthe operation appears to take place instantaneously. Such points are commonly referred to as linearization \npoints. When the linearization points of a concurrent object are known (e.g., by user speci.cation), \nthey induce an order between overlapping operations of a concurrent history. This obviates the need to \nenumerate all possible permutation for .nding a linearization. EXAMPLE 2.2. Consider the history H1 of \nFig. 4, the star symbols in the .gure denote the occurrence of a user-speci.ed linearization point in \neach operation. Using the relative ordering between these points determines the order between overlapping \noperations, and therefore determines a unique linearization of H1 , shown as L1 . We can check linearizability \nin two settings: (i) automatic lin\u00adearization the system explores all permitted permutations of a concurrent \nhistory to .nd a valid linearization; (ii) .xed lineariza\u00adtion points the linearization point of each \noperation is de.ned in terms of a user-speci.ed statement in the program in which it appears to take \nplace. The ability to check algorithms using auto\u00admatic linearization is essential for the exploration \nprocess, as the linearization points of algorithms being explored are unknown. In this paper, we focus \non checking linearizability, as it is the appro\u00adpriate condition for the domain of concurrent objects \n[15]. Our tool can also use other conditions (e.g., operation-level serializability). 3. Paraglider \nIn this section, we show how our domain speci.c exploration pro\u00adcedure, implemented in a tool called \nPARAGLIDER, is able to ef.\u00adciently explore a vast number of combinations with a small number of invocations \nof the checking procedure. For now, we treat the checking procedure as a black box (see Sec. 5 for details). \n3.1 Input / Output The exploration framework uses the following inputs: a program schema with placeholders \nfor missing code-fragments.  a (correct) sequential implementation of the concurrent object, used as \nan executable speci.cation of correct behavior. optional: a speci.cation of linearization points.  \nThe output of the exploration procedure is a set of instantiated schemas (referred to as algorithms ), \nthat have been checked by the checking procedure. Informally, a placeholder de.nes a set of code fragments \nthat can be used to .ll the missing part in the program schema. A placeholder is de.ned over a set of \nbuilding blocks, and a set of constraints on how these blocks can be used. The building blocks can be \nviewed as the statements of a domain speci.c language for constructing concurrent data structures. Generally, \nany building block that references a shared location contains one memory access (although it may operate \non any number of local values). For example, a building block r = curr->next reads the next pointer of \ncurr into a local variable r. We present building blocks for the algorithms explored in this paper in \nSection 4. We allow the user to leave the exact details of a building block undetermined, and let a block \nbe parametric on the expressions it is using. The set of possible expressions used by a building block \ncan be de.ned using a regular expression (we assume a predetermined bounded length). For example, the \nparametric block x = y.[f |g|h] can be instantiated to the three instantiated blocks x = y.f , x = y.g \nor x = y.h. The exploration preformed by PARAGLIDER will try each of these instantiated blocks. In this \npaper, a placeholder can be constrained by sequencing constraints that restrict the permitted sequences \nof blocks, and atomicity constraints that specify which blocks have to be executed atomically. DEFINITION \n3.1. A placeholder ph is a pair =B, C., where B is a set of building blocks, and C is a set of atomicity \nconstraints and sequencing constraints expressed as regular expressions over B. For atomicity constraints, \nwe write [b1 ,b2 ] when a block b1 must appear in the same atomic section with b2 in ph (note that this \ndoes not restrict the order in which they appear within the atomic section). For sequencing constraints, \nwe use the following shorthand notations: we write b1 <b2 when b1 must appear before b2 in ph, and (b1 \n,b2 ) when b1 must appear adjacent to b2 in ph. A placeholder instance is a single code fragment (sequence \nof instantiated blocks and atomic sections) that satis.es the con\u00adstraints of the placeholder. DEFINITION \n3.2. Given a placeholder ph = =B, C., a place\u00adholder instance is a code fragment cf such that all blocks \nof cf are instantiated from B, and cf satis.es the constraints speci.ed in C. In particular, the code \nfragment cf includes an equivalence rela\u00adtion atom(cf ) < B \u00d7 B, that satis.es the atomicity constraints. \nWe note that atom(cf ) partitions cf to atomic sections. Given a placeholder ph, we use Dph to denote \nthe set of all placeholder instances for ph, and refer to it as the domain of ph. A program schema consists \nof a skeleton and a set of placehold\u00aders (including expression placeholders). Given a schema S, we use \nPH(S) to denote the set of placeholders in S. Given a program schema S,a placeholder assignment A is \na (partial) function assigning a placeholder to a speci.c instance in its domain. When A assigns an instance \nfor each placeholder of S, we say that A is a complete assignment. A schema instance is a pair of a schema \nS, and a complete as\u00adsignment A, assigning a placeholder instance for each placeholder of S, i.e., a \nschema in which all placeholders are instantiated. 3.2 Exploration Our system exhaustively explores \nthe space of placeholder assign\u00adments. For most problems, this space is huge and contains millions of \npotential assignments. A key ingredient of PARAGLIDER is the ability to reduce this space by exploiting \ndomain speci.c knowl\u00adedge. PARAGLIDER leveragesapartialorderofimpliedcorrectness de.ned between assignments, \nreducing the number of algorithms that are checked to less than 3% of the total number of algorithms \nin the space. This reduction is the key to feasibility of exploration. Given a placeholder ph we allow \nthe user to de.ne a partial order between placeholder instances. This partial order de.nes im\u00adplied correctness \nbetween assignments, thus allowing the search to use the correctness (or incorrectness) of an assignment \nto determine the correctness (or incorrectness) of other assignments. The system is pre-equipped with \na partial order de.ned between instances based on atomic sections. Given a placeholder ph and two placeholder \ninstances i1 ,i2 \u00d8 Dph , we say that i1 : i2 when for every two (instantiated) blocks b1 ,b2 , they appear \nin the same order in i1 and i2 , and if (b1 ,b2 ) \u00d8 atom(i1 ) then (b1 ,b2 ) \u00d8 atom(i2 ). This partial \norder captures the domain-speci.c knowledge that if a given code-fragment yields a correct algorithm, \nthen increasing the scope of atomic sections in this code fragment maintains cor\u00adrectness. In the search, \nour system also leverages this constraint in the opposite direction when a code fragment yields an incorrect \nalgorithm, any code fragment that is less atomic will also yield an incorrect algorithm. We note that \none has to be careful when match\u00ading the correctness criterion used by the checking procedure with the \nnotion of implied correctness. DEFINITION 3.3 (Ordering schema instances). Given a schema S and two complete \nassignments A1 and A2 , we say that A1 implies the correctness of A2 , and write A1 : A2 when for every \nph \u00d8 PH(S), A1 (ph) : A2 (ph). The system also allows a user to de.ne a notion of equivalence between \nplaceholder instances. The system is pre-equipped with an equivalence of instances based on atomic sections. \nIntuitively, when the operations inside an atomic section are independent of each other in terms of data.ow, \nthey can be reordered. Two place\u00adholder assignments that only differ on the ordering of independent operations \ninside an atomic section are considered equivalent. The system uses implied correctness and incorrectness \nbased on schema instance ordering and equivalence to reduce the number of algorithms that have to be \nchecked. In Section 4.5, we report experimental results that show the effectiveness of this reduction. \nCorrectness In this paper we focus on checking linearizability, but our tool can use any other checking/veri.cation \nprocedure.  4. Concurrent Set Algorithms In this section, we demonstrate how a designer uses the sys\u00adtem \nto construct several .ne-grained data structures starting from a sequential implementation. The section \nfollows the derivation roadmap shown in Fig. 3. We explore algorithms by initially considering the general \nno\u00adtion of an atomic section, without specifying how it is implemented. As a second step, we show how \nto realize the atomic sections in the derived algorithms via the CAS and DCAS synchronization primi\u00adtives. \nKeeping the .rst part of the derivation at the level of atomic sections permits alternative realizations \n(e.g., via locks). 4.1 From Sequential to Optimistic Concurrency Our .rst observation is that the traversal \nof the list searching for the appropriate key (in LOCATE) is only reading values from the list, and not \nmodifying it. In the absence of concurrent modi.cations, the search part of an operation could be performed \nconcurrently. This motivates our .rst step: using optimistic concurrency. 4.1.1 Optimistic Concurrency \nWe apply a well-known concurrency transformation due to [16] and obtain a basic schema using optimistic \nconcurrency. In this method, the optimistic list traversal no longer uses synchronization. However, after \nthe traversal, we atomically check a validation con\u00addition and perform the operation if the condition \nholds. The result\u00ading schema is shown in Fig. 5, where the validation conditions are left as unspeci.ed \nexpression placeholders =VALADD., =VALREM., =VALCT.. In order to turn the schema of Fig. 5 into a correct \n(but still a coarse-grained) algorithm, we need to .nd a suitable assign\u00adment of validation conditions. \nWhile we can also .nd the conditions via a blind search over some set of user-provided expressions, it \nmay be valuable for a de\u00adsigner to observe the states that cause failure and consider a number of alternatives. \nThe designer can try and construct a validation con\u00addition by starting with the weakest (true), or by \nstarting with the strongest (false) condition. Starting with the weakest and strength\u00adening it seems \nmore natural, as it may expose violations of a safety property that would help us identify how to strengthen \nthe condi\u00adtion. Starting with the strongest condition and weakening it is more challenging, as the base \ncase is a one of non-termination (the algo\u00adrithm always restarts). boolean add(int key) { Entry *pred,*curr,*entry; \nrestart : LOCATE(pred, curr, key) atomic if ((VALADD*){ . k =(curr->key == key) . if (k) return f alse \n . entry = new Entry(key) . entry->next = curr  pred->next = entry return true } goto restart } boolean \ncontains(int key) { Entry *pred,*curr; restart : LOCATE(pred, curr, key) atomic if ((VALCT*){ . k =(curr->key \n== key) . if (k) return true  if (\u00ack) return f alse } goto restart } boolean remove(int key) { Entry \n*pred,*curr,*r; restart : LOCATE(pred, curr, key) atomic if ((VALREM*){ . k =(curr->key = key) . if \n(k) return f alse . r = curr->next . pred->next = r  return true } goto restart } Figure 5. An algorithm \nschema using optimistic concurrency with =VALADD.,=VALRM.,=VALCT. as unknown validation conditions. head \ntail  T1: pred T1: curr T1:inv add(4) T2:inv add(7) T1:add(4) / true T2:inv add(7) T2:add(7) / true \nT2:add(7) / true Figure 6. Invalid execution with the validation condition true. head tail head tail \n head tail T1: pred T1: curr T1: curr T1: pred T1:inv add(4) T2:inv remove(1) T1:add(4) / true T1: inv \nadd(4) T2:remove(1) / true T1: add(4) / true Figure 7. An invalid execution with the validation condition \npred->next = curr.  4.1.2 Finding Validation Conditions We choose to start with the weakest conditions \nmainly because the underlying checking procedure provides a counterexample when a safety property is \nviolated. We start by setting =VALADD., =VALCT. and =VALREM. assigned to true. We then run the system \nwith only add operations. Fig. 6 shows an invalid execution under this choice. Thread T1 invokes add(4), \n.nds the location in the list, and stops. At this state, the pointer pred of T1 (hereafter T1: pred) \npoints to the node 1, and T1: curr points to the tail of the list. Before T1 continues, a thread T2 invokes \nand completes the operation add(7). This results in the node 7 inserted between T1: pred and T1: curr. \nNext, T1 continues execution of add(4), resulting in the loss of the node 7. Finally, another invocation \nof add(7)by T2 adds 7 to the list, and returns true where all corresponding sequential executions return \nfalse. At this point, the system reports a linearizability violation. boolean add(int key) { Entry *pred,*curr,*entry \nrestart : LOCATE(pred, curr, key) (ADD* return true } (ADD* = { A1: val =(pred->next == curr) A2: k =(curr->key \n== key) A3: if (k) return f alse A4: pred->next = entry A5: entry = new Entry(key) A6: entry->next = \ncurr A7: if (\u00acval) goto restart } with { (A1,A7), (A2,A3), A3 <A4,A7 <A4 } boolean remove(int key) \n{ Entry *pred,*curr,*r restart : LOCATE(pred, curr, key) (REMOVE* return true } (REMOVE* = { R1: val \n=(pred->next == curr) R2: k =(curr->key = key) R3: if (k) return f alse R4: r = curr->next R5: curr->next \n= null R6: pred->next = r R7: if (\u00acval) goto restart } with { (R1,R7), (R2,R3) R3 <R6,R7 <R6,R7 <R5 \n} boolean contains(int key) { Entry *pred, *curr LOCATE(pred, curr, key) k =(curr->key == key) if (k) \nreturn true if (\u00ack) return f alse } LOCATE(pred, curr, key) { pred = head curr = head->next while(curr->key \n< key){ pred = curr curr = curr->next if (curr == null) goto restart } } Figure 8. Paraglider speci.cation \nfor a concurrent set algorithm using curr->next = null for deleted nodes. The cause of the problem is \nthat an update done by addassumes pred->next == curr, a property that can be violated by another thread \nbetween the search and the update. To prevent this scenario, we manually strengthen =VALADD. to check \nwhether pred->next still points to curr. When that is the case, add can proceed, otherwise the operation \nis restarted. We again check the algorithm running the system only with add operations, and no error \nis reported. We therefore move on to derive a validation condition in the presence of removeoperations. \nA similar counter-example between remove operations leads to strengthening =VALREM. to pred->next == \ncurr. Unfortunately, the resulting algorithm is still incorrect. The problem is due to interference between \naddand remove. Fig. 7 shows an example of an invalid execution. First, thread T1 invokes add(4), .nds \nthe location in the list, and stops. Thread T2 then executes a full remove(1)operation and removes the \nnode pointed to by T1: pred from the list. Next, T1 resumes and inserts key 4 between pred and curr. \nBecause the node pointed to by pred was already removed from the list, the update will cause node 4, \ninserted by T1 , to be lost. Finally, another invocation of add(4)by T1 adds 4 to the list and returns \ntrue, thus observing a violation of linearizability (as all sequential executions would have returned \nfalse). This sample execution suggests the need for a mechanism to prevent removed nodes from being used \nin other operations. At this point, it is useful to stop and ask the question: How can we systematically \nstrengthen the validation condi\u00ad tion based on this information? In principle, we follow two general \nguidelines: (i) perform more work without adding space for coordination metadata, (ii) add space for \ncoordination metadata and more work based on that metadata.  4.2 Performing Additional Work The high-level \nidea is to make the fact that a node has been re\u00admoved observable to threads that may be trying to use \nit. Our in\u00adsight is that removal of a node can be made observable to other threads by setting the next.eld \nof the node to null when it is be\u00ading removed. Since this might break the traversal performed by the \noptimistic search, we have to adjust LOCATE, in addition to the op\u00aderations building blocks to correctly \nact upon a pointer that was set to null. Fig. 8 shows the schema for algorithms using the above in\u00adsight. \nIt consists of an implementation of LOCATE and contains, and skeletal implementations for add and remove. \nThe LOCATE macro is manually adapted from the sequential implementation, adding a restart when curr == \nnull. ADD = { REMOVE = { k =(curr->key == key) k =(curr->key = key) if (k) return f alse if (k) return \nf alse entry = new Entry(key) atomic entry->next = curr val =(pred->next == curr) atomic . if (\u00acval) \ngoto restart . r = curr->next val =(pred->next == curr) curr->next = null pred->next = entry l if (\u00acval) \ngoto restart pred->next = r } } Figure 9. A placeholder assignment for the schema of Fig. 8 re\u00adsulting \nin a new concurrent set algorithm. The skeletal implementation for add uses the placeholder =ADD., and \nthat of remove uses =REMOVE.. Building blocks are shown as labeled statements inside the placeholder \nde.nitions. Note that most blocks correspond to the same core actions per\u00adformed by the original sequential \nimplementation. The block R5 re.ects the designer insight that the next pointer of a removed node should \nbe set to null. Blocks A1 and R1 correspond to the valida\u00adtion expressions we obtained in the previous \nsection. The placeholder =ADD. uses two constraints clumping together certain building blocks, and two \nconstraints that force an order be\u00adtween blocks. The constraint (A1,A7) forces the conditional restart \nto be placed adjacent to the computation of the validation con\u00addition (the order between the blocks is \nforced by data.ow con\u00adstraint that is automatically determined by the system). The con\u00adstraint (A2,A3) \nforces the conditional return of false to be adja\u00adcent to the computation of its condition (computation \nof k). The constraints A3 <A4 and A7 <A4 force an order in which the op\u00aderation checks the conditions \non key equality and the validation condition before applying the update. These constraints are added \nin order to reduce the size of the search space, and are based on de\u00adsigner knowledge. In addition to \nuser-speci.ed constraints, the sys\u00adtem automatically generates data.ow constraints, and adds these as \nordering constraints between building blocks. While the insight of using null is known to the designer, \nthe exact way in which this should be implemented is unclear. In par\u00adticular, the designer would like \nto obtain the least atomic algorithm that can be constructed using this insight. This is where the sys\u00adtem \nassists the process, by systematically exploring the space of algorithms that can be constructed from \nthe speci.ed schema. For this schema, PARAGLIDER found 16 instances. Fig. 9 shows one of the algorithms \nproduced by the system, other algorithms found are variations of this algorithm. Note that there are \nstill many pos\u00adsible combinations that may yield different algorithmic variations. In particular, there \nare algorithms where the validation check and restart take place before the check of key. In addition, \nnote that .nding a single correct instance for a schema is not enough. We are interested in investigating \nthe tradeoffs between multiple correct instances of a schema. Next, we show how the atomic sections in \nthis algorithm can be directly expressed using low-level synchronization primitives. 4.2.1 Using Low-level \nSynchronization Primitives The atomic section in the add operation contains a single read, write and \ncomparison operations. This atomic section can be di\u00adrectly implemented as a CAS. The CAS operation compares \nthe content of a given address to an expected value, and if the value matches, atomically writes a new \nvalue to the given location. The precise meaning is: CAS(addr, old, new) { atomic . if ((*addr == old) \n{*addr = new; return true} else return f alse } The atomic section in the addis: atomic l val =(pred->next \n== curr) if (\u00acval) goto restart pred->next = entry Since the update of pred->next is only performed conditionally, \nand val is a local value computed only once, the goto restart blockcan be taken outside the atomic. val \n= CAS(&#38;pred->next, curr, entry) The atomic section in the remove operation contains two reads, one \ncomparison, and two writes. We would like to implement this atomic section using a DCAS. Similarly to \nthe add, we note that we can move the local operation that is the restarting block after the atomic section. \nThe DCAS operation compares and swaps two locations atomically: DCAS(addr1, addr2, old1, old2, new1, \nnew2) { atomic l if ((*addr1 == old1)&#38;&#38;(*addr2 == old2)){ * addr1= new1; * addr2= new2; return \ntrue } else return f alse } However, in order to use a DCAS, we .rst need to transform it to perform \nr = curr->next outside of the atomic section, so it matches the meaning of a DCAS. We use a standard \noptimistic concurrency transformation in which an atomic read of a value is replaced by a non-atomic \nread followed by a validating compari\u00adson. Using this transformation, we rewrite the atomic block as: \nr = curr->next atomic l val =(pred->next == curr) / (curr->next == r) curr->next = null pred->next = \nr The atomic section can now be transformed to: val = DCAS(&#38;pred->next, &#38;curr->next, curr, r, \nr, null)  4.3 Adding Coordination Metadata The Null-based algorithms obtained in Section 4.2 were designed \nunder the assumption that no additional space should be used for coordination metadate. The best algorithm \nwe obtained had two main disadvantages. First, the optimistic search in an operation might be disrupted \nand forced to restart when the next pointer of a node has been set to null. Secondly, our goal is to \nreach boolean remove(int key) { Entry *pred,*curr,*r restart : LOCATE(pred, curr, key) (REMOVE* return \ntrue } (REMOVE* = { R1: val =(pred->next == curr)(/mp)? (/mc)? R2: k =(curr->key = key) R3: if (k) return \nf alse R4: r = curr->next R5: curr->marked = true R6: pred->next = r R7: if (\u00acval) goto restart R8: mp \n= \u00acpred->marked R9: mc = \u00accurr->marked } with { (R2,R3), (R1,R7) R3 <R6,R7 <R6 } Figure 10. Paraglider \nspeci.cation for a removeoperation using a marked bit for deleted nodes.The block R1 is a parametric \nblock. algorithms that only use CAS, rather than DCAS as the best null\u00adbased algorithm does. From our \nprevious study on another class of concurrent algo\u00adrithms [29], we know that it is possible to trade-off \nspace for syn\u00adchronization. That is, introducing more space may allows us to re\u00adduce the required synchronization. \nTo that end, we introduce a sep\u00adarate marked .eld in each node denoting that the node is being removed. \nThis is inline with modern algorithms who interpret the setting of that bit as logical removal of the \nnode. Adding a Marked Bit We modify the schema to include a sepa\u00adrate marked bit to denote deleted nodes. \nFig. 10 shows the schema for the remove operation using a marked bit. We omit the rest of the schema \nsince it is similar to the schema of Fig. 8. In the new schema, we modify the LOCATE code to remove the \n(redundant) restart on the case that curr==null. We modify the building blocks of Fig. 8 by: (i) replacing \nthe block R5 with a new block setting a marked bit; (ii) adding new blocks for reading the marked bit \nof pred and curr (blocks R8 and R9 for remove, and similar blocks for add); (iii) replacing the blocks \ncomputing the validate condition A1,R1 by new validate conditions as described below. After the addition \nof the marked bit, it is no longer clear whatthe validation conditions should be. We know from our experiencewith \nthe previous algorithm that it must contain the check for pred->next == curr. However, we do not know \nthe markedbits of which nodes should be checked in each condition. We therefore de.ne the building blocks \ncomputing the conditions asparametric blocks. A parametric block can draw its value from aset of values, \nspeci.ed as a regular expression. PARAGLIDER then searches through these value assignments exhaustively \nas part ofthe exploration. In the case of the validate condition, we de.nethree parametric blocks, one \nfor each operation. The parametricblocks for all three operations are of the form: val =(pred->next == \ncurr)(/mp)? (/mc)? It is important to note that knowing the meaning of the expression, the fact that \nit is a validate expression, allows us to impose an order on the search. For a given ordering of the \nblocks, and a given assignment of atomic sections, if an expression e yields an incorrect algorithm, \nthen any weaker expression also yields an incorrect algorithm (as it permits a superset of values permitted \nby e). Running the system with the new set of blocks produces 6 algorithms. Note that these algorithms \nonly check the marked bit of the pred node (and not the curr node). In particular, the algorithm of Fig. \n11 is one of the results with the smallest atomic boolean add(int key) { boolean remove(int key) { Entry \n*pred,*curr,*entry; Entry *pred,*curr,*r restart : restart : LOCATE(pred, curr, key) LOCATE(pred, curr, \nkey) k =(curr->key == key) k =(curr->key = key) if (k) return f alse if (k) return f alse entry = new \nEntry(key) curr->marked = true entry->next = curr r = curr->next atomic atomic mp = \u00acpred->marked mp \n= \u00acpred->marked . val =(pred->next == curr) / mp . val =(pred->next == curr) / mp ll if (\u00acval) goto restart \nif (\u00acval) goto restart pred->next = entry pred->next = r  return true return true }} LOCATE(pred, curr, \nkey) { boolean contains(int key) { pred = head Entry *pred,*curr; curr = head->next LOCATE(pred, curr, \nkey) while(curr->key < key){ k =(curr->key == key) pred = curr if (\u00ack) return f alse curr = curr->next \nif (k) return true }} } Figure 11. A set algorithm using a marked bit to mark deleted nodes. A variation \nof [13] that uses a weaker validation condition. sections. As we will see in the next section, this can \nbe leveraged towards an ef.cient implementation of the algorithm. One of the differences between the \nalgorithm in Fig. 11 and the one presented by Heller et. al. [13] is that it uses weaker validation conditions. \nThe algorithm of [13] also checks the marked bit of the node curr. At this point in the derivation, we \nhave obtained all algorithms that can be produced from the schema using a marked bit. 4.3.1 Using Low-level \nSynchronization Primitives After obtaining several .ne-grained algorithms using generic atomic sections, \nour goal was to bring them closer to practical im\u00adplementations. The algorithm of Fig. 11 uses atomic \nsections that are similar to the ones we saw in the algorithm of Fig. 9. However, the algorithm reads \nthe content of an additional memory location (the marked bit) inside the atomic sections of add and remove. \nTo implement this algorithm using CAS/DCAS we follow [12, 21] in which the marked bit is stored together \nwith the pointer in the same atomic storage unit (ASU). Using a single ASU (usually a machine word) to \nrecord both the next pointer and the marked bit of a node makes it possible to read both, atomically, \nwith a single read operation.1 We demonstrate the use of a single ASU on the atomic section of the add \noperation. We use =ptr, bit. to denote a pair of a reference and a bit value stored in the same ASU. \nThe atomic section of the addoperation can be rewritten as: atomic (pnext, pmrk* = pred->next l val =(pnext \n== curr) /\u00acpmrk if (val) pred->next = entry This, in turn, can be now expressed as a CAS, similar to \ntheCAS in Section 4.2.1: val = CAS(&#38;pred->next, (curr.ptr, 0*, (entry.ptr, 0*) where curr.ptr refers \nto the pointer component of the pointer curr, and =curr.ptr, 0. is the composite pointer with 0 as the \nvalue of the marked bit. Similarly for entry. The atomic section of the remove operation can be rewritten \nto use only CAS operations. To do that, we follow the same basic idea as outlined in Section 4.2.1: a \nstandard optimistic concurrency 1 as suggested by [21], the marked bit can be stored in a low order bit \nof the next pointer as pointers are at least word aligned on most systems. transformation in which an \natomic read of a value is replaced by a non-atomic read followed by a validating comparison. We .rst \nread curr->next into a temporary and use it in a CAS in order to set the marked bit of curr. If the CAS \nfails, we restart the operation. Since the marked bit is stored together with the next pointer, writing \nthe marked bit requires an atomic manipulation of the entire next .eld. As a result, the marking operation \nitself, although non-conditional, now requires a CAS. This is purely due to the implementation choice \n(space reduction) of storing the marked bit together with the pointer. The marking is therefore performed \nby: r = curr->next lval = CAS(&#38;curr->next, r, (r.ptr, 1*) where the value read into r is also used \nlater for the removal from the list. Similarly, we use another CAS to check the validate of the actual \nremoval, and to perform it atomically. The process results in the least atomic algorithm we managed to \nderive and was already shown in Fig. 2.  4.4 Adapting Set Algorithms A practical problem is that often \nan existing algorithm needs to be adapted to a slightly different setting. The advantage of our approach \nis that the designer can (sometimes) adapt the building blocks of the algorithm, and not the algorithm \nitself, letting the tool .nd algorithms with the adapted building blocks. We consider two adaptations \nof set algorithms: (i) priority queue, and (ii) a stack. In our variation of a priority queue, addcorresponds \nto an insertin a priority queue, and adds elements with a unique priority key. The removecorresponds \nto a deleteMinoperation, and removes the element with the lowest key (highest priority). This corresponds \nto changing the blocks of remove to only allow access to the .rst item in the list. After adapting the \nblocks and adjusting the sequential speci.cation, we run the system and obtain 4 variations of a priority \nqueue. Similarly, we repeat the process for stacks and obtain 8 variations, including the well-known \nTreiber algorithm [27]. For space reasons, we do not show the results in the paper. 4.5 Experimental \nResults Table 1 is a summary of the major exploration experiments per\u00adformed in the derivation process. \nEvery row in the table corre\u00adsponds to an exploration starting from a single schema. We report how many \nalgorithms passed the correctness checks (Accepted In\u00adstances), the total time of exploration in minutes, \nthe total number of instances that could have been checked under a naive exploration procedure (Total \nInstances), and what percentage of the space was actually checked by the the system (number of algorithms \nis shown in parentheses). Note that in each exploration experiment, we are using the method for automatically \nchecking linearizability (see Section 5.1). This is because the tool explores many instances for each \nschema and the linearization points for each instance can be different. In cases where we found an instance \n(algorithm) that we thought was interesting (such as the one in Fig. 11), we checked that instance by \nspecifying its linearization points (see Section 5.2). The effectiveness of the domain speci.c exploration \nprocedure is apparent from the percentage of algorithms being checked. In all cases, it is less than \n3% of the total number of algorithms. Our ex\u00adploration procedure can be parallelized by exploring different \nas\u00adsignments separately. This might reduce the opportunities for re\u00adduction of the space explored, but \nis expected to yield an overall improvement in the running times. 4.6 Summary In this section we started \nwith a sequential algorithm. Using an optimistic concurrency transformation we arrived at an intermedi\u00adate \nschema. The challenge there was to .nd correct restarting val\u00adidation conditions with respect to linearizability. \nStarting with the Schema Accepted Instances Time (Min.) Total Instances Checked Instances % Null-based \n16 26 64,512 0.088 (57) Marked-based 6 15 138,240 0.048 (66) ) Prio-Q 4 2 4,608 0.67 (31) Stack 8 2 2,560 \n2.27 (58) Table 1. Exploration Results. weakest possible validation conditions (e.g. all set to true), \nwe used the tool to produce a counter-example. By manually observing the counter-examples we found some \nof the simpler conditions. Unfor\u00adtunately, these conditions were not enough to guarantee correctness \nand we had to strengthen them further. Towards that end, we pro\u00adposed two insights (using null or marked \nfor removal) and speci\u00ad.ed the building blocks corresponding to each. We than ran our tool which found \na number of linearizable algorithms with various de\u00adgrees of atomicity for each insight. We then took \nthe best results of each exploration and manually applied transformations in order to obtain CAS and \nDCAS versions. It seemed that this step can be au\u00adtomated. Finally, we adapted some of the set algorithms \nto simpler data structures such as priority queues and stacks. To what extent these steps can be automated \nremains a fascinating item of future work.  5. Checking Linearizability As mentioned in Section 2, \nlinearizability is veri.ed with respect to a sequential speci.cation (pre/post conditions). A concurrent \nobject is linearizable if each execution of its operations is equivalent to a permitted sequential execution \nin which the order between non\u00adoverlapping operations is preserved. Formally, an operation op is a pair \nof invocation and a response events. An invocation event is a triple (tid, op, args) where tid is the \nthread identi.er, op is the operation identi.er, and args are the arguments. Similarly, a response event \nis triple (tid, op, val) where tid and op are as de.ned earlier, and val is the value returned from the \noperation. For an operation op, we denote its invocation by inv(op) and its response by res(op).A history \nis a sequence of invoke and response events. A sequential history is one in which each invocation is \nimmediately followed by a matching response. A thread subhistory, h|tid is the subsequence of all events \nin h that have thread id tid. Two histories h1 ,h2 are equivalent when for every tid, h1 |tid = h2 |tid. \nAn operation op1 precedes op2 in h, and write op1 <h op2 , if res(op1 ) appears before inv(op2 ) in h.A \nhistory h is linearizable, when there exists an equivalent sequential history s, called a linearization, \nsuch that for every two operations op1 , op2 , if op1 <h op2 then op1 <s op2 . That is, s is equivalent \nto h, and respects the global ordering of non-overlapping operations in h. Checking linearizability boils \ndown to .nding a linearization for every concurrent execution. Towards this end, we employ a model checker \nas follows. The model consists of three parts: the concur\u00adrent object (e.g. a set algorithm), the executable \nsequential speci.\u00adcation of the concurrent object and the most general client of that concurrent object \n[2]. The client operates by non-deterministically selecting the operations and the key values, thus exploring \nall possi\u00adble sequences of operations. This client is executed by each thread. We check that every concurrent \nexecution explored by the model checker is linearizable. The checking is supported via two complementary \napproaches. The .rst method is an entirely automated one, requires no user annotations and is discussed \nin Section 5.1. This method is the key for the success of the systematic derivation process, as it requires \nno algorithm-speci.c user annotations. The second method discussed in Section 5.2 requires algorithm-speci.c \nuser annotations, but inv a(5) a(5)/true inv a(5) a(5)/true (H1) inv r(5) r(5)/true inv r(5) r(5)/false \ninv a(5) a(5)/true inv a(5) a(5)/false (L1) inv a(5) a(5)/true inv r(5) r(5)/true inv a(5) a(5)/true \n(L2) inv a(5) a(5)/true inv a(5) a(5)/false inv r(5) r(5)/true (L3) Figure 12. Enumeration of linearizations. \n(H1) is a concurrent his\u00adtory, and (L1),(L2), and (L3) are its three potential linearizations.  allows \nthe model checker to explore a larger state space of the algorithm than the .rst. 5.1 Checking Linearizability \nby Recording Histories In this approach, for every history h, the system explores all pos\u00adsible linearizations, \ntrying to .nd one that satis.es the sequential speci.cation. The worst case time of this approach is \nexponential in the length of the history, as it may have to try all possible permu\u00adtations of h. Technically, \nthe concurrent history h is obtained by instrument\u00ading the schema to record h as part of the state. Upon \ninvocation of an operation, we record the invocation event together with its arguments. Upon operation \ncompletion, we record the response event together with the result. Note that this is done only once per \nschema and is shared between all instances of that schema. How\u00adever, recording the concurrent history \nas part of the state leads to an unbounded number of states that results in non-termination of the model \nchecker. In order to bound the state space and guarantee ter\u00admination, the most general client is parameterized \nby the maximum number of operations each thread can invoke. In our experiments this bound is typically \nset to two or three. Enumerating Linearizations When the model checker .nishes the exploration of a concurrent \nexecution, the recorded history h is part of the .nal state. At that point, a procedure external to the \nmodel checker, is invoked to check linearizability of h. The proce\u00addure takes as input the history h \nand a sequential speci.cation of the concurrent object. The procedure then attempts to .nd a lineariza\u00adtion \nof h. Note that there could be many possible linearizations of h, but it is enough to .nd a single witness. \nIf a witness is found, the model checker continues checking the algorithm. Otherwise, the al\u00adgorithm \nis rejected. The procedure also supports checking of other correctness criteria (e.g., operation-level \nserializability). EXAMPLE 5.1. Fig. 12 demonstrates how the checking procedure works for a simple history \n(H1). This history is an example for the kind of histories recorded inside a single state that is explored \nby the model checker. There are two threads in the history: one thread executes two add(5)operations, \nboth of which return true while the second thread executes a remove(5) operation that also re\u00adturns true. \nTo check linearizability of H1, our procedure enumer\u00adates all possible linearizations of H1. In this \ncase, these are (L1), (L2) and (L3). These histories are obtained by re-ordering only overlapping operations \nof H1. For each of these three potential lin\u00adearizations, we execute the operations over the (executable) \nspeci\u00ad.cation, and compare the return value of each operation to its cor\u00adresponding return value in the \nconcurrent history. In this example, (L1) is not a valid linearization of (H1) as its remove(5)returns \nfalse. Similarly, (L3) is not a valid linearization of (H1) as its sec\u00adond add(5)returns false. The sequential \nhistory (L2) is a valid linearization of (H1) as its invocation and response values match the ones in \nthe concurrent history. 5.2 Checking via Linearization Points Recall that even though there could be \nmany possible linearizations of a concurrent history h, .nding a single linearization is enough to declare \nthe history correct. While requiring no user annotations, the main shortcoming of the previous approach \nis that it records the entire history as part of the state. We know from the de.nition of linearizability \nthat for every operation op, there exists a point between its invocation inv(op) and response res(op) \nin the history h where op appears to take effect. This point is typically referred to as the linearization \npoint lp(op) of the operation op. Given a concurrent history h, the (total) ordering between these points \ninduces a linearization. To perform checking, every time lp(op) is reached in h, the operation op in \nthe sequential speci.cation is executed and the results are compared. If the results are the same, the \nexploration process continues, otherwise an error is raised. EXAMPLE 5.2. Consider the algorithm of Fig. \n9, and an addop\u00aderation. When the add succeeds (and returns true), the lineariza\u00adtion point is at the \nstatement pred->next = entry. This is the point at which the update of shared information takes place. \nWhen addreturns false, the linearization point is at the last execution of the statement curr = curr->next \nin LOCATE. This is the point where the negative outcome of the operation is already determined. This \napproach typically requires an insight on how the algo\u00adrithm operates. Its advantage however is that \nbecause the lineariza\u00adtion is built and checked on-the-.y, we no longer need to record h as part of the \nstate. In turn, this allows to check algorithms with each thread executing the most general client, without \na bound on the number of operations. The most general client is a program that non-deterministically \nselects the operations and key values that are used with the concurrent object. Multiple Points Note \nthat there could be more than one lineariza\u00adtion of a concurrent history h. Therefore, there could be \nmore than one linearization point lp(op) for each operation op. For simpler al\u00adgorithms, lp(op) is usually \na point in the code of op. But for more complex .ne-grained algorithms, there may be several linearization \npoints for lp(op) which may reside in method(s) other than op. The choice of which point is selected \nis conditional on the history h. EXAMPLE 5.3. Consider the algorithm in Fig. 11. The lp(remove) when \nremove returns true resides in the code of remove and is the physical removal of the node, that is, building \nblock R6. Now consider the case where remove returns false for key k. The lp(remove) in this case is \nconditional on the order and types of operations executed by other threads during the execution of remove. \nIt is the earliest of the two points: (i) right before a successful addition of k by another thread and \n(ii) execution of building block R2 (comparing the key). Typically, in order to check algorithms where \nthe linearization points occur in another thread, additional instrumentation of the model is required. \nWe have performed this instrumentation for the algorithm in Fig. 11.  6. Related Work There is a signi.cant \nbody of work describing concurrent objects as well as various formal methods for checking their correctness. \nDue to space reasons, we only survey some of this work. Concurrent Set Algorithms The algorithm of [12] \nwas the .rst lock-free set algorithm using only CAS instructions. This algorithm relies on a garbage \ncollector (GC). Later, [21] introduced another lock-free set algorithm using explicit memory management. \nThe in\u00adtroduction of manual memory management introduces many com\u00adplexities that are beyond the scope \nof this paper. In [13], the au\u00adthors present a concurrent algorithm with a wait-free contains also assuming \na GC. The add and removeoperations are block\u00ading, due to the use of locks. The best algorithm derived \nin this paper uses only CAS operations, assumes GC, and also supports a wait-free contains. Similarly \nto [13], it is blocking in both add and remove. The reason is because once a node is marked, the thread \nwho marked the node could crash before it physically removes the node. The marked node could then stay \nin the list per\u00admanently which may cause other operations to restart inde.nitely. This scenario technically \nprecludes the algorithm from being re\u00adferred to as lock-free. This case can be avoided by requiring each \noperation to physically remove a marked node once it encounters it during locate. This will result in \na different linearization point for a successful remove [18]. Exploration and Derivation In previous \nwork [29], we also used a semi-automated approach for exploring a space of concurrent GC algorithms. \nThat work used a limited search procedure and an ab\u00adstraction speci.cally geared towards the safety property \nrequired for that speci.c domain. In this work, we concentrate on exploring and checking a broader class \nof concurrent algorithms. To that end, we de.ne a more general exploration mechanism and provide an automatic \nprocedure for checking linearizability, a property rele\u00advant to a wide class of concurrent objects. There \nhas been a considerable amount of work on using brute\u00adforce search to .nd a correct completion of a partial \nprogram (e.g., Sketching [25, 26, 24]), a superoptimized code sequence [3, 19], or an algorithm within \na limited family (e.g., mutual exclusion algorithms [4]). Our approach differs in several ways, notably: \n Systematic Derivation We follow a systematic derivation that combines manual steps with automated exploration \nsteps per\u00adformedby PARAGLIDER.Thisclari.eswhichdecisionsarefun\u00addamental to the algorithm and which are \na result of a speci.c implementation (e.g. merging the pointer with a marked bit).  Checking Linearizability \nAutomatic checking of properties rel\u00adevant to concurrent algorithms such as linearizability.  Domain-speci.c \nExploration The ability to explore and check a large space of algorithms is due to the domain-speci.c \nexplo\u00adration that leverages relationships between algorithms.  [1] introduces a formal derivation of \nconcurrent queue algo\u00adrithms close to the ones of [20]. Their derivation is an attempt to reconstruct \nknown algorithms. In contrast, we explore a wide range of algorithms, and use an automated tool for exploring \nalternatives. Checking Linearizability and Related Conditions CheckFence [6] checks the correctness of \nalgorithms using symbolic bounded test-cases. It .rst constructs all sequential executions for operations \ninvoked in the test program and creates a set of correct observable behaviors (sequences of invokes and \nreturns). It then checks that every concurrent executions is observationally equivalent to some correct \n(sequential) behavior. In [6], algorithms were tested using symbolic test-cases. Previous work by the \nsame authors [5] checked for operation-level sequential consistency using bounded test-cases and user \nprovided speci.cation of commit points . In contrast, we assume a sequentially consistent memory model, \nand check for linearizability. Moreover, when the user speci.es .xed lineariza\u00adtion points, our approach \nuses test-case invoking an unbounded sequences of operations (over an underlying data-structure of a \nbounded size). Veri.cation Of Linearizability There have been several works on formally verifying linearizability. \nThe algorithm of [13] has been veri.ed by [7] using an automata based approach and by [28] us\u00ading the \nrely-guarantee proof method. In the work of [2], the authors assumed a .xed number of threads and using \nabstract interpretation veri.ed some small concurrent algorithms. These proof methods typically require \nmanual intervention, such as specifying lineariza\u00adtion points and/or constructing the actual proof. As \nnoted by [7], this is especially dif.cult when the linearization point for a method does not reside in \nits code and is dependent on the program trace. This is the case for several of the algorithms derived \nin this paper as well as the contains of [13]. We see our approach as com\u00adplementary to a formal veri.cation \nof a .nal algorithm. The system produces .ne-grained candidates which could then be further ver\u00adi.ed \nformally if necessary. The work of [10] provides bounded model checking of commit-atomicity for small \nalgorithms, assum\u00ading user-provided .xed commit points, which reside in the code of the method. Our method \nfor checking .xed points does not require the point to reside in the method and hence we can check more \ncomplex algorithms. Wing and Gong [30] provide a simulation pro\u00adcedure for testing linearizability with \nuser provided or randomly generated test-cases. A concrete test case is executed in a simula\u00adtor and \nthe system attempts to .nd a linearization of one particular execution. The key advantage of our system \nis that up to a bound, all possible test cases are automatically generated, and for each test case, exhaustively, \nall possible executions are checked. 7. Conclusion and Future Work We presented a systematic construction \nof several linearizable con\u00adcurrent algorithms. Assisted by a new tool, we were able to arrive at practical \nconcurrent algorithms while clearly distilling the algorith\u00admic insights from implementation details. \nIn particular we derived a practical set algorithm that only uses CAS, and provides a wait\u00adfree containsoperation. \nWhile modest, this step is encouraging in further exploring algorithmic construction mechanisms. In the \nfuture, we plan to formalize the informal derivation steps presented in this paper and extend the construction \nmechanism to arrive at veri.ed rather than checked algorithms.  Acknowledgements We thank Maged Michael, \nDragan Bosnacki, Gerard Holzmann, Mooly Sagiv, Greta Yorsh, David Bacon and Noam Rinetzky. References \n[1] ABRIAL, J.-R., AND CANSELL, D. Formal construction of a non\u00adblocking concurrent queue algorithm (a \ncase study in atomicity). J. UCS 11, 5 (2005), 744 770. [2] AMIT, D., RINETZKY, N., REPS, T. W., SAGIV, \nM., AND YAHAV, E. Comparison under abstraction for verifying linearizability. In CAV (2007), vol. 4590 \nof LNCS, Springer, pp. 477 490. [3] BANSAL, S., AND AIKEN, A. Automatic generation of peephole superoptimizers. \nSIGOPS Oper. Syst. Rev. 40, 5 (2006), 394 403. [4] BAR-DAVID, Y., AND TAUBENFELD, G. Automatic discovery \nof mutual exclusion algorithms. In Proc. of the symp. on Principles of Distributed Computing (2003), \npp. 305 305. [5] BURCKHARDT, S., ALUR, R., AND MARTIN, M. M. K. Bounded model checking of concurrent \ndata types on relaxed memory models: A case study. In CAV (2006). [6] BURCKHARDT, S., ALUR, R., AND MARTIN, \nM. M. K. Check\u00adfence: checking consistency of concurrent data types on relaxed memory models. SIGPLAN \nNot. 42, 6 (2007), 12 21. [7] COLVIN, R., GROVES, L., LUCHANGCO, V., AND MOIR, M. Formal veri.cation \nof a lazy concurrent list-based set algorithm. In CAV (2006). [8] DOHERTY, S., DETLEFS, D. L., GROVES, \nL., FLOOD, C. H., LUCHANGCO, V., MARTIN, P. A., MOIR, M., SHAVIT, N., AND GUY L. STEELE, J. Dcas is not \na silver bullet for nonblocking algorithm design. In SPAA (2004), pp. 216 224. [9] ELMAS, T., TASIRAN, \nS., AND QADEER, S. Vyrd: verifying concurrent programs by runtime re.nement-violation detection. In PLDI \n(2005), pp. 27 37. [10] FLANAGAN, C. Verifying commit-atomicity using model-checking. In SPIN (2004). \n[11] HARRIS, T., AND FRASER, K. Language support for lightweight transactions. SIGPLAN Not. 38, 11 (2003), \n388 402. [12] HARRIS, T. L. A pragmatic implementation of non-blocking linked\u00adlists. In DISC 01: Proc. \nof conf. on Distributed Computing (London, UK, 2001), Springer, pp. 300 314. [13] HELLER, S., HERLIHY, \nM., LUCHANGCO, V., MOIR, M., SCHERER, W., AND SHAVIT, N. A lazy concurrent list-based set algorithm. \nIn Proc. of conf. On Principles Of Distributed Systems (OPODIS 2005) (2005), pp. 3 16. [14] HERLIHY, \nM. A methodology for implementing highly concurrent data objects. ACM Trans. Program. Lang. Syst. 15, \n5 (1993), 745 770. [15] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for \nconcurrent objects. Trans. on Prog. Lang. and Syst. 12, 3 (1990). [16] KUNG, H. T., AND ROBINSON, J. \nT. On optimistic methods for concurrency control. ACM Trans. Database Syst. 6, 2 (1981), 213 226. [17] \nLAMPORT, L. How to make a multiprocessor computer that correctly executes multiprocess progranm. IEEE \nTrans. Comput. 28, 9 (1979), 690 691. [18] MICHAEL, M. Personal communication. [19] MASSALIN, H. Superoptimizer: \na look at the smallest program. In ASPLOS-II: Proc. of conf. on Architectual support for programming \nlanguages and operating systems (1987), IEEE, pp. 122 126. [20] MICHAEL, M., AND SCOTT, M. Simple, fast, \nand practical non\u00adblocking and blocking concurrent queue algorithms. In PODC (1996). [21] MICHAEL, M. \nM. High performance dynamic lock-free hash tables and list-based sets. In SPAA (2002), pp. 73 82. [22] \nPAPADIMITRIOU, C. H. The serializability of concurrent database updates. J. ACM 26, 4 (1979), 631 653. \n[23] PRAKASH, S., LEE, Y. H., AND JOHNSON, T. A nonblocking algorithm for shared queues using compare-and-swap. \nIEEE Trans. Comput. 43, 5 (1994), 548 559. [24] SOLAR-LEZAMA, A., ARNOLD, G., TANCAU, L., BOD\u00b4IK, R., \nSARASWAT, V. A., AND SESHIA, S. A. Sketching stencils. In PLDI (2007), pp. 167 178. [25] SOLAR-LEZAMA, \nA., RABBAH, R. M., BOD\u00b4IK, R., AND EBCIOGLU, K. Programming by sketching for bit-streaming pro\u00adgrams. \nIn PLDI (2005), ACM, pp. 281 294. [26] SOLAR-LEZAMA, A., TANCAU, L., BOD\u00b4IK, R., SESHIA, S. A., AND SARASWAT, \nV. A. Combinatorial sketching for .nite programs. In ASPLOS (2006), pp. 404 415. [27] TREIBER, R. K. \nSystems programming: Coping with parallelism. Tech. Rep. RJ 5118, IBM Almaden Research Center, APR 1986. \n[28] VAFEIADIS, V., HERLIHY, M., HOARE, T., AND SHAPIRO, M. Proving correctness of highly-concurrent \nlinearisable objects. In PPoPP (2006). [29] VECHEV, M. T., YAHAV, E., BACON, D. F., AND RINETZKY, N. \nCGCExplorer: a semi-automated search procedure for provably correct concurrent collectors. In PLDI (2007), \npp. 456 467. [30] WING, J. M., AND GONG, C. Testing and verifying concurrent objects. J. Parallel Distrib. \nComput. 17, 1-2 (1993), 164 182.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Practical and efficient algorithms for concurrent data structures are difficult to construct and modify. Algorithms in the literature are often optimized for a specific setting, making it hard to separate the algorithmic insights from implementation details. The goal of this work is to systematically construct algorithms for a concurrent data structure starting from its sequential implementation. Towards that goal, we follow a construction process that combines manual steps corresponding to high-level insights with automatic exploration of implementation details. To assist us in this process, we built a new tool called Paraglider. The tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability of algorithms.</p> <p>Starting from a sequential implementation and assisted by the tool, we present the steps that we used to derive various highly-concurrent algorithms. Among these algorithms is a new fine-grained set data structure that provides a wait-free contains operation, and uses only the compare-and-swap (CAS) primitive for synchronization.</p>", "authors": [{"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1022760", "email_address": "", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1022761", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375598", "year": "2008", "article_id": "1375598", "conference": "PLDI", "title": "Deriving linearizable fine-grained concurrent objects", "url": "http://dl.acm.org/citation.cfm?id=1375598"}