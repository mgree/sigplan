{"article_publication_date": "06-07-2008", "fulltext": "\n Data.ow Analysis for Concurrent Programs using Datarace Detection * Ravi Chugh Jan W. Voung Ranjit Jhala \nSorin Lerner University of California, San Diego {rchugh,jvoung,jhala,lerner}@cs.ucsd.edu Abstract Data.ow \nanalyses for concurrent programs differ from their single\u00adthreaded counterparts in that they must account \nfor shared mem\u00adory locations being overwritten by concurrent threads. Existing data.ow analysis techniques \nfor concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively \nkills facts about all data that might possibly be shared by multiple threads; at the other end, a precise \nthread-interleaving analysis de\u00adtermines which data may be shared, and thus which data.ow facts must \nbe invalidated. The former approach can suffer from impreci\u00adsion, whereas the latter does not scale. \nWe present RADAR, a framework that automatically converts a data.ow analysis for sequential programs \ninto one that is correct for concurrent programs. RADAR uses a race detection engine to kill the data.ow \nfacts, generated and propagated by the sequen\u00adtial analysis, that become invalid due to concurrent writes. \nOur ap\u00adproach of factoring all reasoning about concurrency into a race de\u00adtection engine yields two bene.ts. \nFirst, to obtain analyses for code using new concurrency constructs, one need only design a suitable \nrace detection engine for the constructs. Second, it gives analysis designers an easy way to tune the \nscalability and precision of the overall analysis by only modifying the race detection engine. We describethe \nRADAR frameworkanditsimplementationusingapre\u00adexisting race detection engine. We show how RADAR was used \nto generate a concurrent version of a null-pointer dereference analy\u00adsis, and we analyze the result of \nrunning the generated concurrent analysis on several benchmarks. Categories and Subject Descriptors D.2.4 \n[Software Engineer\u00ading]: Software/Program Veri.cation Validation; F.3.2 [Seman\u00adtics of Programming Languages]: \nSemantics of Programming Lan\u00adguages Program analysis General Terms Languages, Reliability, Veri.cation \nKeywords Interprocedural Analysis, Locksets, Multithreaded Programs, Summaries * This work was supported \nby NSF CAREER grants CCF-0644306, CCF\u00ad0644361, NSF PDOS grant CNS-0720802, NSF Collaborative grant CCF\u00ad0702603, \nand the UCSD FWGrid Project, NSF Research Infrastructure Grant Number EIA-0303622. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 13, 2008, \nTucson, Arizona, USA. Copyright c . 2008 ACM 978-1-59593-860-2/08/06. . . $5.00 1. Introduction Advances \nin static algorithms for program optimization and error detection have shown that compiler technology \ncan dramatically improve the reliability and performance of computer systems. Most of these algorithmic \nadvances are limited to sequential programs and ignore the challenges introduced by concurrency,where \nthe need for static checking and potential for optimization are even greater. The main dif.culty with \nconcurrent code is that for an analysis to be sound, it must account for the concurrent interleaving \nof multiple executions. Consider the following scenario found in an old version of the Linux kernel. \nA thread acquires the lock on a list to .nd an array within the list. Once found, it releases the lock. \nIt then reads the array length once and iterates through the array. A sequential analysis would unsoundly \nreport the system safe as the thread .rst checks the array size before indexing the array. Unfortunately, \nanother thread can change the array size in the time between the check and the index, thereby causing \na memory error. Currently, the problem of analyzing concurrent programs is ad\u00addressed in one of the following \nways. First, the programmer can provide annotations (e.g. volatile) that alert the compiler to the pieces \nof data that can be modi.ed by concurrent threads. The com\u00adpiler can then safely ignore these pieces \nof data i.e., not perform any analysis that depends on them. Unfortunately, this solution is error-prone \nas the programmer can mistakenly forget annotations. Second, the compiler could sidestep the need for \nannotations using an escape analysis that determines if a piece of data is modi.ed by multiple threads \n[23]. However, the precision of these analy\u00adses is inherently limited even if the escape information \nis per\u00adfect, there are some critical pieces of shared data about which the analysis can infer nothing, \nmaking it impossible, for example, to statically prove the safety of dereferences of shared pointers \nor the access of shared arrays. Third, to overcome this imprecision, one can use custom concurrent analyses \n tailored to speci.c problems and models of concurrency to infer speci.c kinds of information [9, 13, \n22]. These analyses can be precise, but one must painstak\u00adingly retool a new analysis for each concurrent \nsetting. Finally, one could use model checking to infer facts by exhaustively exploring all thread interleavings \n[2, 7, 11]. While this is an extremely precise and generic approach, such analyses are unlikely to scale \ndue to the combinatorial explosion in the number of interleavings. We present a solution to the problem \nof precisely analyzing concurrent programs in a scalable way. Our solution is based on two insights. \nFirst, the most common way for a programmer to ensure a fact about a piece of shared data at any given \npoint in a thread is to ensure that no other thread can modify the data while the .rst thread is still \nat that point. Our second insight consists of a way of using race detection to determine when data.ow \nfacts may be killed by the actions of other threads. A data race occurs when multiple threads are about \nto access the same piece of memory and at least one of those accesses is a write. Since data races are \na common source of tricky bugs, several static analyses have been developed to .nd races, or show their \nabsence. Our insight is that to determine whether the actions of other threads can invalidate, or kill, \na fact inferred about some data at some point, it suf.ces to determine whether an imaginary read of the \ndata at the point can race with a write to that data by another thread. We combine these insights in \na framework called RADAR that takes as input a sequential data.ow analysis and a race detection engine, \nand returns as output a version of the sequential analysis that is sound for multiple threads. RADAR \ncombines our insights as follows. It .rst runs the sequential analysis. At each program point, after \nthe transfer function for the sequential data.ow analysis has propagated facts to the point, RADAR queries \nthe race detector to determine which facts must be killed due to concurrency. More precisely, for each \npropagated fact, RADAR asks the detector if an imaginary read, at that program point, of the memory locations \nthat the fact depends on can race with writes performed by other threads. If the answer is yes (that \nis, if another thread may be concurrently writing to one of the locations), then the data.ow fact is \nkilled. If the answer is no (that is, if no other threads can possibly be writing to these locations), \nthen the data.ow fact remains valid in the concurrent setting. RADAR sapproach of factoring all reasoning \nabout concurrency into a race detection engine is less precise than a custom analysis that may also generate \nfacts from concurrent writes. However, RADAR yields two concrete bene.ts. First, to obtain analyses for \ncode using new concurrency constructs, one need only design a suitable race detection engine for the \nconstructs. Second, it gives analysis designers an easy way to tune the scalability and precision of \nthe overall analysis by only modifying the race detection engine. To sum up, the main contributions of \nour paper are as follows. Wehavedesignedaframeworkcalled RADAR thatautomatically converts a sequential \ndata.ow analysis into a concurrent one using a race detection engine (Section 3).  We have instantiated \nthe RADAR framework with an existing race detection engine called RELAY [29]. We describe this im\u00adplementation, \nwhich we call RADAR(RELAY) (Section 4).  We have used RADAR(RELAY) to transform a sequential null\u00adpointer \nanalysis into a concurrent null-pointer analysis. We de\u00adscribe the null-pointer analysis and evaluate \nthe precision of the resulting concurrent null-pointer analysis. We demonstrate that RADAR(RELAY) easily \nscales to hundreds of thousands of lines of code, and achieves good precision relative to some appropri\u00adate \nupper and lower bounds (Section 5).   2. Overview We begin with an overview of our technique using \nsome simple examples. First, consider the multithreaded program shown in Fig\u00adure 1, which executes a \nsingle copy of the Producer thread and a single copy of the Consumer thread. There is a shared, acyclic \nlist of structures named bufs, and a shared performance counter perf ctr. To enable mutually exclusive \nlist access, there is a lock buf lock which is initially unlocked , i.e., not held by any thread. The \nProducer (respectively Consumer) thread has a local refer\u00adence px (respectively cx) used to iterate over \nthe list bufs. The Producer thread iterates over the cells in the list bufs.In each iteration, it acquires \nthe lock buf lock protecting the cell, and resets the px.data to a new buffer initialized with the value \n0 that will hold the data that will be produced. Next, it obtains the value, via a call to produce and \nonce it is ready, the producer writes the value into *px.data and moves onto the next cell.   The Consumer \nthread iterates over the cells in the list bufs. In each iteration, it acquires the lock buf lock and \nif the pointer cx.data is non-null, it consumes the data, resets the pointer to free the buffer, and \nmoves to the next cell in the list. Finally, the consumer releases the lock. We assume that the shared \nlist contains no cycles and that it starts off with all the data .elds set to NULL. Thus, the net effect \nof having the Producer and Consumer running in parallel is that the producer walks through the list setting \nthe data .eld of each individual cell, and the consumer trails behind the producer, using the data .eld \nin each cell and resetting it to NULL. Finally, notice the Consumer thread initializes perf ctr without \nholding any locks, and so the initialization races with the increment operation at P5 in the Producer \nthread. However, we shall assume that the programmer has deemed that the race is benign as it is on an \nirrelevant performance counter. 2.1 Sequential Non-Null Analysis Suppose we wish to statically determine \nwhether any null-pointer dereference occurs during the execution of the program in Figure 1. To this \nend, we could perform a standard sequential data.ow analy\u00adsis, using .ow facts of the form NonNull(l) \nstating that the lvalue l is non-null, and .ow functions that appropriately generate, prop\u00adagate, and \nkill such facts using guards and assignments. Let us assume that new() returns a non-null pointer to \na cell initialized with 0. The set of facts in the .xpoint solution computed by this analysis is shown \non the left in Figure 1 (for the moment, ignore the line crossing out a fact on the last line). At points \nP0 and P1, every lvalue may be null. At P2 and P3 px is non-null, and everywhere else, the sequential \nanalysis determines that the lvalues px and px.data are non-null. Thus, the analysis determines that \nat each program point where a pointer is dereferenced, the pointer is non-null, and so all the dereferences \nare safe. Sequential analysis is unsound, even without data races. In this case, the above conclusion \nis sound: there are indeed no null\u00adpointer dereferences in the program. In general, however, a sequen\u00adtial \nnon-null data.ow analysis may actually miss some null-pointer dereferences. As an example, consider a \nscenario where the pro\u00adgrammer who wrote Figure 2 mistakenly uses the intuition that the system is safe \nas long as there are no data races on any of the shared cells. Thus, to improve the performance of the \nprogram from Fig\u00adure 1, the programmer writes the modi.ed version of the Producer thread shown in Figure \n2, where buf lock is temporarily released while the data is being produced to allow the Consumer thread \nto concurrently make progress. The resulting system has no races, and so the programmer may think that \nthe system is safe. However, this intuition turns out to be incorrect, and in fact the programmer has \nactually introduced a null-pointer bug, even though there are no races. This is because after the producer \nthread has initialized px.data and released the lock, the consumer can acquire the lock and reset the \npointer. When the producer thread re-acquires the lock after storing the data temporarily in t,the dereference \nat P8 can cause a crash because the pointer is null. Unfortunately, even though the new program has a \nnull-pointer bug, the sequential analysis returns exactly the same solution as for the original program \n(shown on the left in Figure 2). That is, at each point the same lvalues are deemed to be non-null, and \nthus the sequential analysis would not discover the null-pointer bug. 2.2 The Problem: Adjusting for \nMultiple Threads To address the above problem, we want to to automatically convert a sequential data.ow \nanalysis into one that is sound in the presence of multiple threads. Doing this in a way that is also \nprecise is not trivial.   Adjusted  Adjusted  Adjusted Analyysis  Analyysis Analyysis   Figure \n1. Producer-Consumer program Figure 2. Buggy version Figure 3. Flag-based version Consider for example \nthe simple solution of running an es\u00adcape analysis, and keeping only NonNull facts for those lval\u00adues \nthat do not escape the current thread. The px.data .eld escapes the current thread, so the analysis would \nnever infer NonNull(px.data). Although this solution makes the analysis sound in the face of concurrency \n(and in particular, it would .nd the bug in Figure 2), it also makes the analysis imprecise: the re\u00adsulting \nconcurrent analysis would not even be able to show that the original program from Figure 1 is free of \nnull-pointer bugs. Alternatively, one may be tempted to run independent sequen\u00adtial analyses over blocks \nthat are atomic in the sense of [16, 7] and conservatively kill facts over shared variables at atomic \nblock exit points [3]. Intuitively, a block is atomic if for each execution of the operations of the \nblock where the operations are interleaved with those of other threads, there exists a semantically equivalent \nexecu\u00adtion where the operations of the block are run without interleaving. Unfortunately, the body of \nthe Producer loop from Figure 1 is not atomic because of the benign race on the performance counter perf \nctr. This race splits the body of Producer into multiple atomic blocks the statements before, at, and \nafter the racy incre\u00adment. Thus, such an analysis would kill the NonNull(px.data) fact at P5, and would \nbe too imprecise to prove that the program from Figure 1 is free of null-pointer bugs.  2.3 Our Solution: \nPseudo-Race Detection We now describe our solution to this problem, which allows us to leverage existing \nrace detection engines to build a sound concurrent analysis that is strictly more precise than the previously \nmentioned simple approaches. As discussed in Section 6, race detection is a well-studied problem. For \nprograms using lock-based synchroniza\u00adtion, there are scalable race detectors that infer the sets of \nlocks that protect each shared memory location and produce a race warning if two threads access a shared \ncell without holding a common lock. Adjust. Our .rst insight is that the facts that can soundly be inferred \nto hold in the presence of multiple threads are the subset of facts es\u00adtablished via sequential analysis \nwhich are not killed by operations of other threads. Thus, the multithreaded data.ow analysis can be \nreduced to determining which facts inferred at a particular point by the sequential analysis are killed \nby other threads. To determine if a fact can be killed by a concurrently executing operation of another \nthread, it suf.ces to check if another thread can concurrently write any lvalue appearing in the .ow \nfact. Pseudo-Races. Our second insight is that to perform this check we can insert pseudo-reads corresponding \nto the lvalues in the .ow fact at the program point, and query a race detection engine to determine if \nany of the pseudo-reads can race with a write to the same memory location. If such a pseudo-race occurs, \nthen the fact is killed; otherwise, the analysis deduces that the fact continues to hold at the point \neven in the presence of other threads. We have designed a framework called RADAR that combines these \ntwo insights to convert an arbitrary data.ow analysis for se\u00adquential programs into one that is sound \nfor multithreaded pro\u00adgrams. During analysis, RADAR uses the sequential .ow function, but at each program \npoint, it kills the facts over lvalues that have pseudo-races at that point. This mechanism captures \nthe following informal idiomatic manner in which the programmer reasons about multiple threads. Each \nthread performs some operation that estab\u00adlishes a certain fact in the programmer s head, e.g. a null \ncheck or initialization or an array bounds check. The programmer can only expect that the fact continues \nto hold as long as other threads cannot modify the memory locations. As a result, the programmer uses \nsynchronization mechanisms to protect the memory loca\u00adtions from writes by other threads as long as the \ninformation is needed. Our technique of adjusting preserves only those facts that the race detection \nengine deems to be protected from modi.cation. 2.4 Multithreaded Non-Null Analysis Let us consider the \nresult of running the non-null analysis adjusted using RADAR toaccountformultiplethreads.ThelinesinFigures1 \nand 2 show the facts generated by the sequential analysis that get killed during the adjusting because \nof pseudo-races. For both the correct and the buggy programs, in the Consumer thread the adjusting has \nno effect because cx is thread-local, and due to the held lock buf lock, there are no races on the pseudo\u00adreads \nof cx.data at program points C4 and C5. Safety without Atomicity. In the correct Producer thread of Fig\u00adure \n1, the adjusting process has no effect on facts over (only) the thread-local, and hence, race-free lvalue \npx. The initialization at P3 causes the fact NonNull(px.data) to get generated at pro\u00adgram point P5. \nThe adjusting does not kill this fact because the lock buf lock held at P5 ensures there is no pseudo-race \non px.data. Similarly, the fact NonNull(px.data) generated at P3 is not killed by the adjusting at P6-P9, \nas the held lock buf lock ensures there are no races with the pseudo-read on px.data at any of these \npoints. As the lock is released at P9, the fact NonNull(px.data) is killed by the adjusting at PA, as \nthe pseudo-read can race with the write in the Consumer thread. The adjusted analysis shows that the \ndereferences in the program are safe, as px.data is soundly inferred to be non-null at P8, where the \ndereference takes place. Notice the adjusted analysis can soundly show that the program does not cause \nany null-pointer dereferences, even though the pro\u00adducer thread, even the loop-body, is not atomic, due \nto perf ctr which may be accessed without any synchronization. By preserv\u00ading the facts that are over \nprotected lvalues, our adjusting technique can ignore atomicity breaks caused by benign races on irrelevant \nentities like perf ctr. Thus, our RADAR adjusting technique is strictly more precise than running independent \nsequential analyses over semantically atomic blocks. Concurrency Errors without Races. In the buggy Producer \nthread of Figure 2, the facts over the thread-local lvalues are not killed by adjusting. However, notice \nthat although the sequential analysis would propagate the fact NonNull(px.data) to P5,the adjusted version \nkills the fact since once the protecting lock is re\u00adleased at P4, the pseudo-read of px.data at P5 can \nrace with the write at C5 in a Consumer thread. As this fact is killed at P5, it does not propagate in \nthe adjusted analysis to P6 -P9, as happens in the sequential analysis. Thus, as a result of the adjusting, \nthe derefer\u00adence at P8 is no longer inferred to be safe as px.data may be null at this point! Thus, our \ntechnique .nds an error caused by multi\u00adthreading that is absent from the sequential version of the program, \neven though there are no data races in the program except on the irrelevant perf ctr. Beyond lock-based \nsynchronization. Although we have used lock\u00adbased synchronization to show how RADAR works, our adjusting \ntechnique is applicable to any synchronization regime for which race detection techniques exist, not \njust those based on locks. Con\u00adsideraversion ofthe Producer-Consumer example, shown in Fig\u00adure 3, which \nhas .ner-grained synchronization done with a .ag .eld in each of the structures in the cyclic list. Now, \ninstead of acquiring the lock, the Producer thread spins in a loop while px.flag is non-zero, which indicates \nthat the data in the struc\u00adture has not yet been consumed. Once the .ag is zero, the producer, initializes \nthe px.data .eld, writes the new data into it, and sets the px.flag .eld to 1 indicating the data is \nready. Dually, the Consumer thread spins while the cx.flag .eld is zero, at which point it consumes the \ndata and resets the cx.data .eld. The result of the adjusted analysis for this program is identical to \nthe result for the .xed program of Figure 2, as a more general race detection en\u00adgine (e.g. one based \non model checking [11]) would deduce that there were no pseudo-races on px.data in the locations P5 -P9. \nOnce the .ag is set at P9, the pseudo-read of px.data at PA can race with the write at C5 in a Consumer \nthread, and so the adjust kills the fact NonNull(px.data) at PA. In the rest of the paper, we formalize \nthe RADAR framework and show how it converts sequential analyses into concurrent ones.  3. The RADAR \nFramework This section presents the RADAR framework for concurrent data.ow analysis in several steps. \nWe start by presenting (in Sec\u00adtion 3.1) a basic version of RADAR. Although this basic version lacks \ncertain important features and optimizations (such as support for function calls), it illustrates the \nfoundation of our approach. We then gradually re.ne the basic framework (in Sections 3.2 and 3.3) by \nadding various optimizations and features.    Assumptions.We make two standard assumptions about the \npro\u00adgram being analyzed and the system it is to be run on. First, we as\u00adsume that for each procedure, \neither we have its code or we have a summary that soundly approximates its behaviors. As a result, our \nframework can analyze incomplete programs (e.g. programs that use libraries), since we can model the \nmissing procedures using summaries. Second, we assume that the shared memory system is sequentially consistent \n[14] in that memory operations are executed in the order in which they appear in the program. 3.1 Intraprocedural \nFramework We start by presenting a basic framework for generating an in\u00adtraprocedural concurrent data.ow \nanalysis from an intraprocedural sequential data.ow analysis. Sequential Data.ow Analysis. We assume \na Control Flow Graph (CFG) representation of programs, where each node represents a statement, and edges \nbetween nodes are program points where data.ow information is computed. We use Node to represent the \nset of all CFG nodes and PPoint the set of all program points. In the program shown in Figure 1, the \nprogram point P3 is the point just before executing the statement at P3. We assume that the sequential \ndata.ow analysis computes a set of data.ow facts at each program point, where the set of all possi\u00adble \ndata.ow facts is Data.owFact. For the purposes of exposition, we assume that data.ow facts in Data.owFact \nare must facts, which means that may information, if needed, has to be encoded by the absence of must \ninformation. Although we make this as\u00adsumption in our exposition, our implementation explicitly supports \nmay facts. Thus, the domain of the sequential data.ow analysis is D = P(Data.owFact), ordered as a lattice \n(D, ., T, ., n, U), where .is ., Tis \u00d8, .is Data.owFact, nis .,and Uis n. We also assume that the .ow \nfunction is given as: F : Node \u00d7D \u00d7PPoint .D Given a node n, some incoming data.ow information d,and \nan outgoing program point p for node n, F (n, d, p) returns the outgo\u00ading data.ow information. We assume \nthat if a node n has more than one incoming program point, the data.ow information is merged using Ubefore \nbeing passed to the .ow function. Examples of data.ow facts that can be propagated include: HasConstantValue(x, \n5), which states that x has the value of 5, MustPointTo(x, y), which states that x points to y,and NonNull(p), \nwhich states that p is safe to dereference. Requirement on Data.ow Information. As our framework does \nnot depend on the exact details of the Data.owFact set, analysis writers have the freedom to choose the \nway in which they encode data.ow information. However, we do place a requirement on the Data.owFact set: \nwe assume the existence of a function lvals that returns the set of lvalues that a fact depends on. Intuitively, \ngiven a data.ow fact f .Data.owFact, lvals(f ) returns the set of lvalues that, if written to with arbitrary \nvalues, would invalidate the fact f . We denote the set of all lvalues by LVal, and so the function lvals \nhas type Data.owFact .P(LVal). As an example, for the data.ow facts mentioned above, we would have: lvals(HasConstantValue(x, \n5)) = {x}lvals(MustPointTo(x, y)) = {x}lvals(NonNull(p)) = {p} Although we assume that the lvals function \nis given, it can easily be computed from the sequential .ow function F if F handles havoc CFG nodes of \nthe form l :=. . In particular: lvals(f )={l |l .LVal .f ./F ( l :=. , {f }, )} Concurrent Data.ow Analysis. \nWe capture the way in which con\u00adcurrency affects the sequential data.ow analysis through a function ThreadKill \n:PPoint \u00d7LVal .Bool. Given a program point p and an lvalue l, ThreadKill(p, l)returns whether or not \nl may be written to by concurrent threads when the current thread is at pro\u00adgram point p.The ThreadKill \nfunction, which we will de.ne later in terms of a race detection engine, is at the core of our technique: \nit allows RADAR to kill data.ow facts that are made invalid by con\u00adcurrent writes. Given the sequential \n.ow function F and the ThreadKill func\u00adtion, we de.ne FAdj , the .ow function for the concurrent analysis: \nFAdj (n, d, p)={f | f .F (n, d, p). .l .lvals(f ). \u00acThreadKill(p, l)} This adjusted .ow function essentially \npropagates data.ow facts that are produced by the original .ow function F and that are not killed by \nany concurrent writes. Adjusting via Race Detection. The key contribution of our work lies in the way \nin which we use a race detection engine to com\u00adpute ThreadKill. As a result, we need a way to abstract \nthe race detection engine. We achieve this through a function RacyRead : PPoint \u00d7LVal .Bool, which behaves \nas follows: given a pro\u00adgram point p andanlvalue l, RacyRead (p, l)returns true if a read of l at program \npoint p would cause a race. Soundness. For our framework to be sound, the race detection engine must \nbe sound, in the sense that if there really is a race, then RacyRead must return true (but RacyRead can \nalso re\u00adturn true in cases where there is no race). To formalize this soundness property, we assume a \nperfect race detection oracle RealRace :PPoint \u00d7LVal .Bool, such that RealRace(p, l) returns true exactly \nwhen there is an execution in which a read of l at p would cause a race. The following requirement states \nthat the race detection engine RacyRead must approximate the oracle RealRace: .p .PPoint, l .LVal . (1) \nRealRace(p, l).RacyRead (p, l) Having a sound race detection engine, the ThreadKill function can then \nbe de.ned as: ThreadKill(p, l)=RacyRead (p, l) This basic de.nition of ThreadKill expresses the key insight \nbe\u00adhindthe RADAR framework,whichisthatpseudo-racescanbeused as a way of determining when concurrent writes \ncould happen. Instantiation Requirements: To instantiate the basic RADAR framework, one needs to provide \na race detection engine RacyRead : PPoint \u00d7LVal . Bool that satis.es the sound\u00adness property (1). 3.2 \nOptimized Framework: Race Equivalence Regions The basic RADAR framework from Section 3.1 performs a race \ncheck at each program point for each lval that the data.ow facts depend on. This can lead to a large \nnumber of race checks, in the worst case n \u00d7m,where n is the number of program points and m is the number \nof lvalues. To reduce this large number of race checks, we partition program points into race equivalence \nregions. Intuitively, a race equivalence region is a set of program points that have the same raciness \nbehavior: for each lvalue, either the lvalue is racy throughout the entire region, or it is not racy \nthrough\u00adout the entire region. It is not possible for an lvalue to be racy in one part of the region \nand not racy in another part. Race equivalence regions reduce the number of race checks because by checking \nthe raciness of an lvalue at (any) one program point in a region, RADAR can know the raciness of the \nlvalue throughout the entire region. Race Equivalence Regions. Formally, we de.ne a partitioning of \nprogram points into race equivalence regions as a pair (R, Reg ), where R is a set of regions, and Reg \n:PPoint .R is a function mapping each program point to a region. Two program points ' ' p and pare race \nequivalent if Reg(p)=Reg (p).We say a program point p belongs to aregion r if Reg(p)=r. Since all program \npoints belonging to a region are equivalent in terms of race detection, we change the interface to the \nrace detection engine to take a race equivalence region rather than a program point: RacyRead :R \u00d7LVal \n.Bool One possible implementation for this new RacyRead is to choose a unique representative program \npoint for each region, and when queried with a particular region r and lvalue l, toreturnthe result of \nthe old RacyRead on r s representative point and l. Soundness. Instead of imposing a particular way of \nimplement\u00ading RacyRead , we de.ne a soundness requirement for the new RacyRead engine and the Reg function: \n.p .PPoint, l .LVal . (2) RealRace(p, l).RacyRead (Reg (p), l) With this new abstraction for the race \ndetection engine, the ThreadKill function becomes: ThreadKill(p, l)=RacyRead (Reg(p), l) This new de.nition \nof ThreadKill reduces the number of race checks for each lvalue from at most once per program point to \nat most once per region. Instantiation Requirements: To instantiate the region-based RADAR framework, \none needs to provide: 1. A race detection engine RacyRead :R \u00d7LVal .Bool 2. A region map Reg :PPoint \n.R  such that RacyRead and Reg satisfy property (2). We now give two examples to illustrate the idea \nof race equivalence regions. Example: Global Locksets. One possible instantiation of regions uses locksets. \nIf we assume that there is a global set of locks L,we can de.ne R =P(L), which means that a race equivalence \nregion is simply a set of locks, and the program points in the region are those program points at which \nthe region s locks are held. Consider the buggy example from Figure 2. The Reg map is: . .{buf lock} \nif p .{P3, P4, P8, P9} . Reg(p)= {buf lock} if p .{C3, C4, C5, C6, C7} . . \u00d8 otherwise which captures \nthe set of program points where buf lock is held. The RacyRead function is de.ned as: RacyRead (r, l)=(l \n=px.data .buf lock. r). (l =cx.data .buf lock. r) which captures the fact that an access of px.data \nin Producer or cx.data in Consumer is racy at any point where the lock buf lock is not held, and that \nall other accesses are non-racy. It is easy to check that the Reg and RacyRead functions soundly approximate \nthe possible races and pseudo-races. The adjusting at program point P5 kills the fact NonNull (px.data)as \nRacyRead (Reg(P5), px.data)=true In the correct version from Figure 1, the absence of the unsafe lock \noperations changes the Reg map to: . .{buf lock} if p .{P3, P5, P6, P8, P9} . Reg(p)= {buf lock} if p \n.{C3, C4, C5, C6, C7} . . \u00d8 otherwise re.ecting the fact that in this program, buf lock is held throughout \nfrom P3 to P9.The RacyRead function remains the same as before, as the synchronization discipline is \nunchanged: as in Figure 1, buf lock is held at all points where the buffer cells are written, namely \nP3, P8,and C5. In the .xed program, the adjusting at p .{P5, P6, P8}does not kill the fact NonNull(px.data),as \nfor each of these p RacyRead (Reg(p), px.data)=false Example: Predicates. Another possible instantiation \nof regions consists of having R be a set of predicates Pred. A race equivalence region is then a predicate \nfrom Pred, and the set of program points in the region are those program points at which the predicate \nholds. The predicate instantiation is more general than the lockset instantiation because we can encode \na set of locks as a predicate stating that all the locks in the set are held. Recall the version of the \nProducer-Consumer program shown in Figure 3, where the synchronization is performed via a flag .eld and \nnot explicitly declared locks. As shown in [11], one can generalize race regions to an access predicate \ndescribing the thread s state. For the example in Figure 3, the Reg map is: . .px.flag =0 if p .{P3, \nP5, P6, P8, P9} . Reg(p)= cx.flag =0 if p .{C3, C4, C5, C6, C7} . . true otherwise The RacyRead function \nis de.ned as: RacyRead (., l)= (l =px.data .. .px.flag =0). (l =cx.data .. .cx.flag =0) Together, Reg \nand RacyRead capture the intuition that a read of px.data in Producer (respectively cx.data in Consumer) \nis racy at any point where the px.flag is zero (respectively cx.flag is zero). 3.3 Interprocedural Framework \nThe RADAR framework presented so far does not take function calls into account. To understand how function \ncalls affect our basic framework, consider the example from Figure 4, which is a version of the Producer-Consumer \nexample where there is a call to a function foo right before the increment of perf ctr. Let us assume \nthat foo itself does not modify px.data,and that the sequential data.ow analysis uses a simple modi.es \nanal\u00adysis to determine this. As a result, the sequential data.ow analysis is able to propagate NonNull(px.data)from \nP4 to P5 (that is to say, through the call to foo). However, in the face of concurrency, even if foo \nitself does not modify px.data, a call to foo while other threads are running can in fact lead to px.data \nbeing modi\u00ad.ed. In particular, calling foo has the effect of unlocking buf lock and then re-locking it, \nwhich gives concurrent threads an opportu\u00adnity to modify px.data. As a result, the adjusted .ow function \nneeds to kill NonNull(px.data)at P5, aswell asany data.ow information about px.data. Unfortunately, with \nthe de.nition of ThreadKill givensofar, the adjusted .ow function would not do this. In particular, the \nadjusted .ow function would ask ThreadKill if px.data could be written concurrently at the program point \nright after foo returns (which is P5). ThreadKill in turn would ask the race detection Adjusted Analyysis \n Figure 4. Producer-Consumer with function calls engine if a read of px.data would cause a race at \nP5. The race detection engine would answer back saying no race since by the time foo returns, the lock \nprotecting px.data would already have been re-acquired. As a result, the information about px.data being \nnon-null would incorrectly survive the adjusting process. The problem in the example above is that the \nexecution of foo passes through a region that does not hold buf lock,which allows concurrent threads \nto modify px.data, and callers of foo must take this into account. More broadly, the problem is that \nthe execution of a function can pass through a variety of race equivalence regions, and callers need \na way to summarize the effect of having passed through all of the callee s regions. Summary Regions. \nTo address this problem, we add to RADAR a new function called SumReg :CS .R, which returns for each \ncall-site an interprocedural summary region. This call-site-speci.c summary region is meant to approximate \nthe possible regions that the callee can go through when invoked at the given call-site. We denote by \nCS the set of all call-sites, and we de.ne the call-site of a call node to be the CFG edge that immediately \nfollows the call node (so that CS .PPoint). Soundness. Intuitively, for soundness we require that for \nevery lvalue l, if a read of l is racy at some program point transitively reachable during the call made \nat cs, then the summary region for the call-site cs must be a region that is racy for l. Thus, to formalize \nsoundness in a context-sensitive manner, we extend the perfect race detection oracle to RealRace :CS \n\u00d7PPoint \u00d7LVal .Bool, such that RealRace(cs, p, l)returns true exactly when there is an execution in which \na read of l at p while cs is on the callstack would cause a race. Let cs * be the set of program points \nin the function being called at call-site cs and any of its transitive callees. The soundness requirement \nfor SumReg can be formally stated as: .cs .CS, l .LVal . [.p .cs * . RealRace(cs, p, l)]. (3) RacyRead \n(SumReg(cs), l) Having de.ned SumReg and its soundness property, we can now de.ne the ThreadKill function \nas follows: ThreadKill(p,l)= RacyRead (Reg(p),l). (RacyRead (SumReg(p),l).p .CS) Instantiation Requirements: \nTo instantiate the interprocedural RADAR framework, one needs to provide: 1. A race detection engine \nRacyRead :R \u00d7LVal .Bool 2. A region map Reg :PPoint .R 3. A summary map SumReg :CS .R  such that RacyRead \nand Reg satisfy property (2), and SumReg satis.es property (3). We now go through the same two instantia\u00adtion \nfrom Section 3.2, and show how SumReg can be de.ned. Example: Global Locksets. If we instantiate regions \nas locksets over a global set of locks L, the summary of a function is the intersection of all the locksets \nthat the function goes through. As a result, in the example from Figure 4, the SumReg function is de.ned \nas: SumReg(P5)=\u00d8 since the unlock in foo causes the lockset to be empty at G1,and thus the intersection \nof all locksets in foo is empty. The Reg and RacyRead functions are the same as in Section 3.2. With \nthese de.nitions, the adjusting process now correctly kills the fact NonNull(px.data)at P5 , since, even \nthough RacyRead (Reg(P5),px.data)=false we have RacyRead (SumReg(P5),px.data)=true which, combined with \nP5 .CS, means ThreadKill(P5,px.data)=true Example: Predicates. If we useaset Pred of predicates for the \nrace equivalence regions, the summary of a function is the disjunction of all the predicates in the function. \n4. An implementation of RADAR using RELAY We have implemented an instantiation of the RADAR framework \nusinganexistingscalableracedetectionenginecalled RELAY [29]. We call this instantiation RADAR(RELAY). \nWe.rst giveabrief overview of RELAY, and then we describe RADAR(RELAY). 4.1 Overview of RELAY RELAY is \na lockset-based static race detection tool that scales to code bases as large as the Linux kernel (5 \nmillion lines of C code). RELAY works by traversing the call graph in a bottom-up manner, analyzing each \nfunction in isolation. Assumptions.We assume that programmers can create threads, ac\u00adquire locks and \nrelease locks by calling functions from speci.c thread libraries (e.g. pthreads). Our techniques work \neven when matching lock and unlock calls appear in different procedures, as is oftenthecaseinmanyofourbenchmarks. \nRELAY isalsosummary\u00adbased, so the programmer can provide summaries that describe the effect of missing \nprocedures on the lockset. Finally, when analyz\u00ading a library in isolation, we conservatively assume \nthat each API function can be called by separate threads created by clients. Relative Locksets. For each \nfunction, RELAY .rst performs a lock\u00adset analysis on the function using a relative lockset representation. \nA relative lockset at a program point is a disjoint pair of locksets (L+,L-)(respectively called the \npositive and negative locksets), which encodes the difference between the locks held at the given program \npoint and the locks held at the function entry point. In particular, the set L+ represents the locks \nthat have de.nitely been acquired since the beginning of the function, and the set L- repre\u00adsents the \nlocks that may have been released since the beginning of the function. Notice that L+ is a must set and \nthat L- is a may set. Locks can be represented as lvalues, and so L+ . LVal and L- . LVal. We denote \nby L = P(LVal)\u00d7P(LVal)the set of all relative locksets. The set of relative locksets form a lattice (L,,T,.,n,U)de.ned \nas: .=(LVal ,\u00d8), T=(\u00d8,LVal) ''' ' (L+,L-)(L+,L-)iff L+ .L+ .L- .L- '' '' (L+,L-)U(L ,L-)=(L+ nL ,L- \n.L-). ++ '' '' (L+,L-)n(L ,L-)=(L+ .L ,L- nL-) ++ Relative Lockset Summaries. The relative lockset \ncomputed for the exit point of the function is stored in a context-sensitive relative lockset summary. \nThis summary soundly approximates the effect of the function on the thread s lockset just before calling \nthe function and is used to update locksets at call-sites. Guarded Access Summaries. For each function, \nRELAY also com\u00adputes the set of guarded accesses in the function. A guarded access is a triple of an \nlvalue, the relative lockset at the program location where the access takes place, and the kind of access, \neither a read or a write. The set of guarded accesses of a function is the set of triples corresponding \nto accesses that may occur during the execution of the function. RELAY stores this set as a guarded access \nsummary for the function. RELAY computes these two summaries (the relative lockset summary and the guarded \naccess summary) in a bottom-up manner, plugging in the summaries of the callees at call-sites to compute \nthe relative lockset and guarded access summaries of the callers. Once summaries have been computed for \nall functions, RELAY looks at the guarded accesses at thread entry points. For each pair of guarded accesses \nwhose lvalues may be aliased, if the intersection of the positive locksets is empty, RELAY reports a \nrace. 4.2 Putting RELAY into RADAR We now show how to instantiate the three functions Reg, RacyRead ,and \nSumReg using RELAY. The result is an instan\u00adtiation of RADAR using RELAY, namely RADAR(RELAY). Region \nmap. We de.ne R = F \u00d7L,where F is the set of function identi.ers. Given a program point p, Reg(p) returns \n(g,(L+,L-)),where g is the function to which p belongs, and (L+,L-)is the relative lockset computed at \np by the RELAY lock\u00adset analysis. Summary map. To de.ne the SumReg function, we .rst de.ne a helper function \nAllUnlocks. Intuitively, the set AllUnlocks(cs) represents an overapproximation of the set of locks that \ncould possibly be released by performing the call at cs. In particular, given a call-site cs, AllUnlocks(cs) \ncomputes the union of all the L- sets in the function being called at cs, and then converts this set \ninto the caller context. The conversion consists of replacing the callee s formals that occur in the \nlocksets with the parameters passed in at the call-site. Given a call-site cs .CS where function hcalls \ng, and given that (L+,L-)is the relative lockset computed by RELAY for the program point right before \nthe call to gat cs,then SumReg(cs)is de.ned as follows: SumReg(cs)=(h,(L+-AllUnlocks(cs),L-.AllUnlocks(cs))) \nEssentially, SumReg subtracts the AllUnlocks set from the locks that were held before the call is made. \nThe result of this subtraction is a conservative approximation of the set of locks that are guaranteed \nto remain locked at all points during the call. Race detection engine. Given a region r =(g,(L+,L-)) \nand an lvalue l, RacyRead (r,l) conceptually runs a full RELAY analysis bottom-up, except that when it \nanalyzes function g, it inserts an additional guarded access to the guarded access set. The additional \nguarded access is the triple (l,(L+,L-),read ), indicating that we are simulating a read of l with a \nlockset of (L+,L-).The RacyRead (r,l) function returns true if and only if, after being propagated up \nto the thread roots, this pseudo-read leads to a RELAY race warning. This conceptual description of RacyRead \n(r,l) is not how we implement it, since each call to RacyRead would lead to an entire RELAY bottom-up \nanalysis of the program. Instead, we structure the execution of RADAR(RELAY) into the following four \npasses, two of which are the RELAY bottom-up analysis. First pass. RADAR(RELAY) runs the bottom-up RELAY \nanaly\u00adsis to compute the relative locksets at each program point, and hence, the race equivalence regions. \n Second pass. RADAR(RELAY) runs the sequential analysis on the entire program with a RacyRead function \nthat returns false all the time. This has the effect of running the sequential analysis without any adjusting, \nbut it allows RADAR(RELAY) to collect the parameters of all the RacyRead queries into a set S of (r,l) \npairs. Since the sequential analysis computes a superset of the facts computed by the adjusted analysis, \nthe set S is a superset of the queries that the adjusted analysis will make.  Third pass. RADAR(RELAY) \nthen runs the bottom-up RELAY analysis again to insert pseudo-accesses. In particular, when analyzing \na function g, for each pair (r,l) .S where r =(g,(L+,L-)),RADAR(RELAY) adds the guarded ac\u00adcess (l,(L+,L-),read \n) to the guarded access summary of g. RADAR(RELAY) uses the results of the second RELAY run to build \na map RelayResults : S . Bool.Given (r,l) . S, RelayResults(r,l) returns whether or not the pseudo-read \nin\u00adserted for (r,l) caused a RELAY race warning.  Fourth pass. Finally, RADAR(RELAY) runs the sequential \nanal\u00adysis again, but this time performs the adjusting process. In par\u00adticular, RADAR(RELAY) uses the \nRelayResults map computed in the second pass to answer the RacyRead queries.  Example: Relative Locksets. \nWe now illustrate how RADAR(RELAY) would analyze the program in Figure 4. First pass. RELAY computes \nthe Reg map where Reg(p) is: . . (Producer,(\u00d8,\u00d8)) if p .{P0} . . . .(Producer,({buf lock},\u00d8)) if p \n.{P3,P4,P5,P6,P8,P9} . . . .(Producer,(\u00d8,{buf lock})) if p .{P1,P2,PA} . . . (Consumer,(\u00d8,\u00d8)) if p .{C0} \n.(Consumer,({buf lock},\u00d8)) if p .{C3,C4,C5,C6,C7} . . . . .(Consumer,(\u00d8,{buf lock})) if p .{C1,C2} . \n. . .(foo,(\u00d8,\u00d8)) if p .{G0} . . (foo,(\u00d8,{buf lock})) if p .{G1} Notice that both locksets are empty at \nthe .rst point in each function meaning the lockset is trivially the same as at the entry point. Using \nthe above Reg map, RADAR(RELAY) determines: AllUnlocks(P5)={buf lock} as the buf lock is released inside \nfoo. Thus, SumReg(P5)=(Producer,(\u00d8,{buf lock}))  Second pass. In the second pass, RADAR(RELAY) runs \na se\u00adquential non-null analysis which generates the .ow facts shown on the left in Figure 4, including \nthe facts crossed out by a line. Using these facts, RADAR(RELAY) computes the superset S as the set of \ntuples {(Reg(p),l) |NonNull(l) at p}.  Third pass. RADAR(RELAY) then inserts pseudo-reads corre\u00adsponding \nto the queries S generated above, and builds the map RelayResults which yields the following RacyRead \nmap:  RacyRead ((g,(L+,L-)),l)= (l = px.data .buf lock .L+) .(l = cx.data .buf lock .L+) Fourth pass. \nWhen the adjusted sequential analysis is per\u00adformed, the fact NonNull(px.data) gets killed at P5 since \nthe summary region at that call-site does not include buf lock in L+. The result is shown on the left \nin Figure 4 the data.ow solution includes only the facts that are not crossed out. 5. Evaluation To \nevaluate the RADAR framework, we have designed a sequential null-dereference analysis, which we describe \n.rst. We then present three instantiations of the adjusting framework to convert the analy\u00adsis into multithreaded \nversions of the analysis, and we compare the results of each on three benchmarks: the Apache web server \n(ap\u00adproximately 130,000 lines of code), the OpenSSL library (210,000 lines of code) and part of the Linux \nkernel (830,000 lines of code). 5.1 Sequential Null-Dereference Analysis We describe the sequential null-dereference \nanalysis in two steps: we start with an intraprocedural analysis, and then we extend it to an interprocedural \none. Intraprocedural Analysis. To describe the analysis, we de.ne the set of data.ow facts Data.owFact \n(whose powerset is the domain D of the analysis) and the sequential .ow function F . Data.ow Facts: \nThe data.ow facts used by the intraprocedural sequential analysis are Data.owFact ={NonNull(l) |l .LVal}. \nIntuitively, if the analysis computes NonNull(l) at a program point, we conclude that the lvalue l is \nalways non-null at that program point. Thus, the dereference of an lvalue l is safe at a node n if NonNull(l) \nis in the set of facts computed for the program point right before n.  Flow Function: Recall that the \nsequential .ow function F takes as input a CFG node n, a set of input facts d, and an outgoing program \npoint p for which to compute a set of output facts.  NonNull(l ' ) .F ( if e ,d,p) if NonNull(l ' ) \n.d or [p = true out and e=(l ' != NULL)] NonNull(l ' ) .F ( l := e ,d, ) if [NonNull(l ' ) .d and \u00acAliased(l,l \n' )] or [l = l ' and NonNull(e) .d] or [l = l ' and e= malloc(...)] or [l = l ' and e=&#38; ...] Interprocedural \nAnalysis. In the presence of multiple procedures, the set of facts and the .ow function are updated to \nsummarize the effects of procedure calls, but the safety check remains the same. Data.ow Facts: The set \nof data.ow facts includes the facts used for the intraprocedural analysis, as well as facts NotMod(l) \nthat state whether a given lvalue l has not been modi.ed during the course of executing the function \ncall. The NotMod() facts are a must version of the may information captured via the usual may-modify \nanalysis. Flow Function: The .ow function must be updated to handle the NotMod() facts and function calls. \nWe de.ne the summary of a function the .ow facts that hold at its exit point. We use a function Norm \n: CS . D, which takes a call-site cs as input, looks up the current summary for the function being called, \nand normalizes the .ow facts by renaming formals to be in terms of the parameters passed in at the call-site \ncs.Using the summaries, we get a new .ow function, where in addition to the rules described above for \nthe intraprocedural case, we have rules to propagate the non-modi.ed facts, and rules that propagate \nthe non-null facts at procedure call-sites: NotMod(l ' ) . F ( l := e , d, ) if NotMod(l ' ) . d and \n\u00acAliased (l, l ' ) NotMod(l ' ) . F ( if e , d, ) if NotMod(l ' ) . d NotMod(l ' ) . F ( call... , d, \ncs) if NotMod(l ' ) . d and NotMod(l ' ) . Norm(cs) NonNull(l ' ) . F ( call... , d, cs) if [NonNull(l \n' ) . d and NotMod(l ' ) . Norm(cs)] or NonNull(l ' ) . Norm(cs) 5.2 Instantiations In addition to the \nnon-null sequential analysis, RADAR requires a black box to answer RacyRead queries. We present four \nimple\u00admentations of this component. Steensgaard-based instantiation. The simplest and least precise instantiation \nwe consider is based on Steensgaard s pointer anal\u00adysis [26], and we call this instantation RADAR(STEENS). \nFor this instantiation, RacyRead (r, l) ignores the region r it is passed and returns true if l is reachable \nfrom a global in the Steensgaard s points-to graph. This matches our intuition that lvalues that can\u00adnot \nbe reached from globals cannot be shared, and thus cannot be racy. RELAY-based instantiations. We have \nalready described the RELAY-based instantiation of RADAR in Section 4. However, for the purpose of better \nunderstanding where the precision of RADAR(RELAY) is coming from, we separate RADAR(RELAY) into two instantiations \nbased on the observation that RELAY can prove the absence of a race in two different ways: (1) by show\u00ading \nthat the two lvalues do not alias; and (2) if they can alias, by showing that the intersection of the \nlocksets is non-empty. To better understand how these two different ways of show\u00ading the absence of a \nrace contribute to RADAR(RELAY),we sep\u00adarate the instantiation into two parts, RADAR(RELAY\u00acL) and RADAR(RELAY). \nWe have already seen the latter; it is just as de\u00adscribed in Section 4. The former is a version of RADAR(RELAY) \nwhere we change the Reg maptoalways return T, which repre\u00adsents the empty set of locks. This modi.cation \nsimulates a version of RELAY that only answers race queries based on possible aliasing relationships \nbut not on locksets. Optimistic instantiation. The last instantation we consider is the most optimistic \npossible: the one where RacyRead always returns false. We call this version SEQ because it is equivalent \nto the se\u00adquential analysis without any adjusting. Although this instantiation of RADAR is unsound, it \nestablishes an upper bound on how well any adjusted analysis can do. Each one of these four instantiations \n RADAR(STEENS), RADAR(RELAY\u00acL),RADAR(RELAY),and SEQ is strictly more precise than the previous, with \nthe last one being unsound. 5.3 Results Figure 5 shows the number of dereferences proven safe, as a frac\u00adtion \nof all dereferences, by each of the four RADAR-adjusted anal\u00adyses. As expected, the size of each bar \ngrows from left to right, indicating that each analysis is more precise. The .rst thing to notice is \nthat for each benchmark the sequen\u00adtial analysis (the fourth bar in each cluster) can prove only a small \npercentage of dereferences safe, between 42% and 68%. Thus, no matter how precise the adjusting process, \nthe resulting multi\u00adthreaded analysis will not be able to prove a majority of derefer\u00adences. The imprecision \nin the sequential non-null analysis is mostly due to imprecision in analyzing the heap. The alias analysis \nwe use merges many of the lvalues on the heap into blob nodes, thus losing precision for heap-allocated \nvariables. Previous null-pointer analyses [4] have also found that heap structures are hard to an\u00adalyze \nprecisely and lead to many false-positives when performing null-dereference checks. To factor this degree \nof imprecision out of our experiments, we plot in Figure 6 the percentages of safe dereferences to pointers \nnot including those in blob nodes. The percentage of all non-blobby dereferences is approximately 52% \nfor Apache, 76% for SSL, and 71% for Linux. Considering these remaining dereferences, the sequential \nanalysis is able to prove the safety of the majority of dereferences on each benchmark (again the fourth \nbar in each cluster). This demonstrates that, aside from the issue of precise heap analysis, this sequential \nanalysis is a rea\u00adsonable and practical one on which to test RADAR. Recall that the sequential analysis \nis unsound in the concurrent setting because it assumes lvalues cannot be modi.ed by concur\u00adrent threads, \nso it may miss actual null-pointer dereferences. Nev\u00adertheless, because we cannot know what an oracle \nwould provide as the correct answer for adjusting, we use SEQ as an upper bound; the other three analyses, \nas well as the oracle, cannot do any better. At the other end of the spectrum is RADAR(STEENS), the least \nprecise of the analyses. We included this Steensgaard\u00adbased approach in our evaluation because it can \neasily be imple\u00admented in a compiler or program analyzer that needs to be sound but is not concerned \nwith being extremely precise. We therefore use RADAR(STEENS) as a baseline for comparison. We now evaluate \nhow the RADAR(RELAY\u00acL) and RADAR(RELAY) instantiations compare to the RADAR(STEENS) lower bound and the \nSEQ upper bound. We consider what per\u00adcentage of this gap the difference between the results of SEQ \nand RADAR(STEENS) is bridged by the other two analyses. Keep in mind that because SEQ is an unsound \noverapproxima\u00adtion, the real gap the difference between a perfect oracle and RADAR(STEENS) may be smaller \nthan the gap we consider. Thus, the percentages we report are in fact lower bounds on how much of the \nreal gap we bridge. Figure 7 summarizes these results. On average, RADAR(RELAY\u00acL) bridges 55.0% of the \ngap on non-blobby dereferences while RADAR(RELAY) bridges 58.4%. The results on Linux are what we would \nexpect: each analysis is incrementally better than the previous one. From left to right, each analysis \nincorporates, in the following order, a simple alias analy\u00adsis, a more precise thread sharing analysis, \nand a lockset analysis. As a result, each analysis better captures concurrency interactions in the program. \nThis leads to more precise race detection, which is ultimately the factor that determines RADAR s effectiveness. \n100% 100% 90% 90% 80% % dereferences proven safe 80%  % dereferences proven safe 70% 60% 50% 70% 60% \n50% 40% 30% 40% 30%Apache SSL Linux Apache SSL Linux Figure 5. Percentage of all dereferences proven \nsafe by each in-Figure 6. Percentage of non-blobby dereferences proven safe by stantiation. each instantiation. \nw/ blobs no blobs no locks w/ locks no locks w/ locks Apache 53.9 53.9 54.2 54.2 SSL 63.3 63.4 63.9 64.0 \nLinux 46.4 61.2 46.9 57.0 Average 54.5 59.5 55.0 58.4 Figure 7. Percent of the gap bridged. based context-sensitivity \nto improve the accuracy of lockset com\u00adputations. The results of [17] show that even in Java benchmarks, \nthe large majority potential races are eliminated by a precise shar\u00ading analysis. Analyses for C programs \nmust cope with the unstruc\u00adtured use of locks and thread creation, which make .ow-or context\u00adinsensitivity \nvery imprecise. [5] works by computing summaries in a top-down manner, but prunes summaries to scale, \n[20] uses a con\u00adstraint based technique to compute correlations that describe the locking protocol. The \nresults for Apache and SSL, however, are different from the Linux ones. In particular, RADAR(RELAY\u00acL) \nis just as ef\u00adfective as RADAR(RELAY) on Apache and almost as effective as RADAR(RELAY) on SSL. To better \nunderstand the implications of these results, recall how RADAR(RELAY) and RADAR(RELAY\u00acL) differ: RADAR(RELAY) \nuses the full version of RELAY, whereas RADAR(RELAY\u00acL) uses a version of RELAY that answers race queries \nbased on pos\u00adsible aliasing relationships, but not on locksets. The fact that RADAR(RELAY\u00acL) is nearly \nas precise as RADAR(RELAY) indi\u00adcates that in many of the cases arising in our non-null analysis, the \nlvalue being adjusted is simply not shared. This in turn is an indica\u00adtion that a more precise alias \nanalysis in the RELAY race detection engine could drastically improve the precision of RADAR(RELAY). \nOverall, our experiments on RADAR(RELAY) demonstrate the precision and scalability of RADAR. Running \nRADAR(RELAY) on a single machine took 1 hour on SSL, 4 hours on Apache, and 12 hours on Linux. But because \nthe callgraph of Linux is embarass\u00adingly parallel, the implementation of RADAR(RELAY) can easily be parallelized \nin the same way as RELAY to runmuchfaster on a cluster of nodes. In each of the test cases, RADAR(RELAY) \nwas able to bridge a sizable portion of the gap between optimistic and conservative conconcurrent non-null \nanalyses, while still produc\u00ading a sound result. 6. Related Work Race Analyses. Java s native support \nfor threads and its syntacti\u00adcally scoped locks enable many techniques for detecting and prov\u00ading the \nabsence of races. These include type systems encoding lock\u00adset information [6], and extended with ownership \n[1]. Another ap\u00adproach is to statically approximate the happens-before relation [28]. Escape analyses \nhave been proposed as a simpler way of detecting shared objects and removing synchronization [23]. A \nrecent line of work [17] shows how to combine nested locking with cloning-Data.ow Analysis. There are \nframeworks for intraprocedural data.ow analysis of concurrent programs that use par-begin and par-end \nconstructs nested within functions. These frameworks work by building a parallel .ow graph (PFG) (a control \n.ow graph with concurrency nodes). The data.ow analysis is lifted by extending the .ow equations to handle \nfork, join and the com\u00admunication between threads. Examples include a reaching de.\u00adnitions analysis [9] \nand bit-vector analyses [13]. Intraprocedural representation-based approaches include [24] which uses \ncobe\u00adgin/end and wait/notify constructs to build a Parallel Program Graph, analagous to the PDG, and \n[15] which describes a Con\u00adcurrent SSA (CSSA) representation which enables subsequent op\u00adtimizations. \nThe pointer analysis of [22] is interprocedural but matching par-begin and par-end constructs must be \nin the same function. Recently, [3] proposed an analysis framework for op\u00adtimizing embedded programs \nwritten in NESC [8]. This frame\u00adworkistailoredto NESC sinterrupt-basedconcurrencyandexplicit atomic sections. \n All the above frameworks are more precise than our framework in which facts can only be killed by concurrent \ninteractions. In con\u00adtrast, these frameworks exploit speci.c concurrency constructs to also allow new \nfacts to be generated by concurrent interactions. However, RADAR is more general, as it is independent \nof the un\u00adderlying concurrency constructs, requiring only that a race detector exists for the constructs. \nFor example, none of these approaches could be applied to our thread-based benchmarks. Model Checking. \nModel checkers explore all interleavings to ver\u00adify abitrary safety properties, and so they can be used \nto encode data.ow analyses [25]. Flavers [2] is a .nite-state property checker that employs conservative \nstate and interleaving reductions, e.g. a may-happen-in-parallel data.ow analysis ([18]) that conservatively \nprunes interleavings. Even with techniques like these and others like partial-order and symmetry reduction \nthat mitigate the effect of combinatorial explosion in interleavings, model checking has only been shown \nto scale to relatively small code bases. A tech\u00adnique related to RADAR is the thread-modular approach, \nproposed in [19, 12] which requires that users provide annotations describing when other threads can \nmodify shared state. Model checking can be used to infer the annotations [7, 11], but these techniques \ndo not scale. If the programs include recursive procedures, model check\u00ading (and hence, exact data.ow \nanalysis in the sense of computing MOP solutions) is undecidable [21]. In contrast to the above, the \nprincipal bene.t of our framework RADAR is that it is not tied to any particular concurrency constructs \nor structure, as all reasoning about concurrency is folded into the race detection engine. This allows \nRADAR to switch between race detection engines to explore the tradeoff between precision and scalability \nof the data.ow analysis. Moreover, RADAR enables a .ner view of concurrency by preserving facts that \nare not killed by other threads, without exploring interleavings caused by irrelevant atomicity breaks \nas in Figure 1. 7. Conclusions and Future Work We have presented a framework called RADAR for converting \na sequential data.ow analysis into a concurrent one using a race de\u00adtection engine as a black box. The \nmain bene.t of our approach is that it cleanly separates the part of the analysis that deals with concurrency, \nthe race detection engine, from the rest of the anal\u00adysis. With this separation in place, the race detection \nengine can be .ne-tuned to improve its precision without changing any of the client analyses. As a result, \nRADAR provides a framework that al\u00adlows the precision with which concurrency is analyzed to be easily \ntuned. Our experiments show that the framework scales, and for a particular analysis, achieves good precision \nwith respect to some upper and lower bounds. Our experience also shows that the preci\u00adsion of the overall \nconcurrent analysis depends on the precision of the underlying alias, escape, sequential and race analyses. \nWe have identi.ed two lines of future work that will, in combination, lead to the understanding and addressing \nof these issues. The .rst direction is to apply the adjusting approach to lift a variety of previously \ndeveloped sequential analyses to the con\u00adcurrent setting. Examples include array bounds checking analy\u00adses \n[27, 10], other kinds of null-dereference analyses [4], and analy\u00adses that guide compiler optimizations, \nsuch as reaching de.nitions, available expressions, and live variables. The second direction is to explore \nprecise and scalable race de\u00adtection techniques for other concurrency constructs. Improvements will likely \ninvolve a combination of better alias and escape analysis, better inter-thread .ow analysis to handle \nsystem calls like wait and notify, path-sensitive analysis to handle .ag-based synchro\u00adnization, and \npossibly programmer-supplied annotations to help with cases that are too dif.cult to analyze automatically. \nAdjusting a wide variety of sequential analyses will allow us to tune the precision of the race detector \nusing actual facts deduced while analyzing real systems for a variety of properties. We believe that \nthe generated concurrent analyses can lead to an empirical understanding of the concurrency idioms used \nin real programs. These patterns can then be used to iteratively tune the precision of the race detector, \nleading to a variety of scalable and precise concurrent program analyses and optimizations. References \n[1] C. Boyapati, R. Lee, and M. Rinard. Ownership types for safe programming: preventing data races and \ndeadlocks. In OOPSLA, pages 211 230, 2002. [2] J. M. Cobleigh, L. A. Clarke, and L. J. Osterweil. FLAVERS: \nA .nite state veri.cation technique for software systems. IBM Systems Journal, 41(1):140 165, 2002. [3] \nN. Cooprider and J. Regehr. Pluggable abstract domains for analyzing embedded software. In LCTES, pages \n44 53, 2006. [4] I. Dillig, T. Dillig, and A. Aiken. Static error detection using semantic inconsistency \ninference. In PLDI, 2007. [5] D. Engler and K. Ashcraft. Racerx: Effective, static detection of race \nconditions and deadlocks. In SOSP, pages 237 252. ACM Press, 2003. [6] C. Flanagan and S.N. Freund. Type-based \nrace detection for Java. In PLDI, pages 219 232. ACM, 2000. [7] C. Flanagan and S. Qadeer. Thread-modular \nmodel checking. In SPIN, LNCS 2648, pages 213 224. Springer, 2003. [8] D. Gay,P. Levis,R.von Behren,M.Welsh, \nE. Brewer, and D. Culler. The nesC language: A holistic approach to networked embedded systems. In PLDI \n2003, pages 1 11. ACM, 2003. [9] D. Grunwald and H. Srinivasan. Data .ow equations for explicitly parallel \nprograms. In PPoPP, San Diego, CA, 1993. [10] Brian Hackett, Manuvir Das, Daniel Wang, and Zhe Yang. \nModular checking for buffer over.ows in the large. In ICSE, pages 129 144. ACM, 2006. [11] T.A. Henzinger, \nR. Jhala, and R. Majumdar. Race checking by context inference. In PLDI, pages 1 12. ACM, 2004. [12] C.B. \nJones. Tentative steps toward a development method for interfering programs. TOPLAS, 5(4):596 619, 1983. \n[13] J. Knoop, B. Steffen, and J. Vollmer. Parallelism for free: Ef.cient and optimal bitvector analyses \nfor parallel programs. TOPLAS, 18(3):268 299, May 1996. [14] L. Lamport. Proving the correctness of multiprocess \nprograms. IEEE Transactions on Software Engineering, SE-3(2):125 143, 1977. [15] J. Lee, D.A. Padua, \nand S.P. Midkiff. Basic compiler algorithms for parallel programs. In PPOPP, pages 1 12, 1999. [16] R.J. \nLipton. Reduction: A new method of proving properties of systems of processes. In POPL, pages 78 86, \n1975. [17] M. Naik, A. Aiken, and J. Whaley. Effective static race detection for java. In PLDI, pages \n308 319, 2006. [18] G. Naumovich, G.S. Avrunin, and L.A. Clarke. An ef.cient algorithm for computing \nmhp information for concurrent java programs. In ESEC / SIGSOFT FSE, pages 338 354, 1999. [19] S. Owicki \nand D. Gries. An axiomatic proof technique for parallel programs. Acta Informatica, 6(4):319 340, 1976. \n[20] P. Pratikakis, J.S. Foster, and M.W. Hicks. Locksmith: context\u00adsensitive correlation analysis for \nrace detection. In PLDI, pages 320 331, 2006. [21] G. Ramalingam. Context-sensitive synchronization-sensitive \nanalysis is undecidable. TOPLAS, 22(2):416 430, 2000. [22] R. Rugina and M.C. Rinard. Pointer analysis \nfor multithreaded programs. In PLDI, pages 77 90, 1999. [23] A. Salcianu and M.C. Rinard. Pointer and \nescape analysis for multithreaded programs. In PPOPP, pages 12 23, 2001. [24] V. Sarkar. Analysis and \noptimization of explicitly parallel programs using the parallel program graph representation. In LCPC,pages \n94 113, 1997. [25] D.A. Schmidt. Data .ow analysis is model checking of abstract interpretation. In POPL, \npages 38 48. ACM, 1998. [26] B. Steensgaard. Points-to analysis in almost linear time. In POPL, pages \n32 41. ACM, 1996. [27] A. Venet and G.P. Brat. Precise and ef.cient static array bound checking for large \nembedded c programs. In PLDI, pages 231 242, 2004. [28] C. von Praun and T.R. Gross. Static con.ict analysis \nfor multi\u00adthreaded object-oriented programs. In PLDI, pages 115 128, 2003. [29] J. Voung, R. Jhala, and \nS. Lerner. Relay: Static race detection on millions of lines of code. In ESEC/FSE. ACM, 2007.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Dataflow analyses for concurrent programs differ from their single-threaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise thread-interleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale.</p> <p>We present RADAR, a framework that automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. RADAR uses a race detection engine to kill the dataflow facts, generated and propagated by the sequential analysis, that become invalid due to concurrent writes. Our approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine. We describe the RADAR framework and its implementation using a pre-existing race detection engine. We show how RADAR was used to generate a concurrent version of a null-pointer dereference analysis, and we analyze the result of running the generated concurrent analysis on several benchmarks.</p>", "authors": [{"name": "Ravi Chugh", "author_profile_id": "81435596644", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P1022810", "email_address": "", "orcid_id": ""}, {"name": "Jan W. Voung", "author_profile_id": "81336493714", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P1022811", "email_address": "", "orcid_id": ""}, {"name": "Ranjit Jhala", "author_profile_id": "81100198278", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P1022812", "email_address": "", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P1022813", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375620", "year": "2008", "article_id": "1375620", "conference": "PLDI", "title": "Dataflow analysis for concurrent programs using datarace detection", "url": "http://dl.acm.org/citation.cfm?id=1375620"}