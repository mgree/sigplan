{"article_publication_date": "06-07-2008", "fulltext": "\n Iterative Optimization in thePolyhedral Model: Part II, MultidimensionalTime Louis-Noel Pouchet\u00a81 C\u00b4 \n edric Bastoul1 1 ALCHEMY Group INRIA Saclay Ile-de-France andParis-Sud University HiPEACmembers {.rstname.lastname}@inria.fr \nAbstract High-level loop optimizations are necessary to achieve good perfor\u00admance over a wide variety \nof processors. Their performance impact can be signi.cant because they involve in-depth program transfor\u00admations \nthat aim to sustain a balanced workload over the computa\u00adtional, storage, and communication resources \nof the target architec\u00adture. Therefore, it is mandatory that the compiler accurately mod\u00adels the target \narchitecture as well as the effects of complex code restructuring. However, because optimizing compilers \n(1) use simplistic per\u00adformance models that abstract away many of the complexities of modern architectures, \n(2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of \ntrans\u00adformation sequences, they typically uncover only a fraction of the peak performance available on \nmany applications. We propose a complete iterative framework to address these issues. We rely on the \npolyhedral model to construct and traverse a large and expres\u00adsive search space. This space encompasses \nonly legal, distinct ver\u00adsions resulting from the restructuring of anystatic control loop nest. We .rst \npropose a feedback-driven iterative heuristic tailored to the search space properties of the polyhedral \nmodel. Though, it quickly converges to good solutions for smallkernels, larger bench\u00admarks containing \nhigher dimensional spaces are more challenging and our heuristic misses opportunities for signi.cant \nperformance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators \nthat leverage the polyhedral representa\u00adtion of program dependences. We provide experimental evidence \nthat the genetic algorithm effectively traverses huge optimization spaces, achieving good performance \nimprovements on large loop nests with complex memory accesses. ACM Categories &#38; Subject Descriptors \nD 3.4 [Programming languages]: Processor Compilers, Optimization GeneralTerms Algorithms, Experimentation, \nPerformance Keywords Af.ne Scheduling, Genetic Algorithm, Iterative Com\u00adpilation, LoopTransformation, \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n08, June 7 13, 2008,Tucson, Arizona, USA. Copyright c . 2008ACM 978-1-59593-860-2/08/06... $5.00 Albert \nCohen1 John Cavazos2 2 Dept. of Computer&#38;Information Sciences University of Delaware cavazos@cis.udel.edu \n1. Introduction In recent years, feedback-directed iterative optimization has be\u00adcome a promising direction \nto harness the full potential of future and emerging processors with modern compilers. Building on op\u00aderation \nresearch, statistical analysis and arti.cial intelligence, it\u00aderative optimization generalizes pro.le-directed \napproach to inte\u00adgrate precise feedback from the runtime behavior of the program into optimization algorithms. \nThrough the many encouraging re\u00adsults that have been published in this area, it has become apparent that \nachieving better performance with iterative techniques depends on two major challenges. Search space \nexpressiveness To achieve good performance with iterative techniques that are portable over a variety \nof archi\u00adtectures, it is essential that transformation search space be ex\u00adpressive enough to let the \noptimizations target all important ar\u00adchitecture components and address all dominant performance anomalies. \nSearch space traversal It is also important to construct search algorithms (analytical, statistical, \nempirical) and acceleration heuristics (performance models, machine learning) that effec\u00adtively traverse \nthe search space by exploiting its static and dy\u00adnamic characteristics. This paper targets the optimization \nproblem of selecting an af.ne transformation to optimize imperfectly nested loops. Gir\u00adbal et al. show \nthat complex sequences of loop transformations are needed to generate ef.cient code for full-size loop \nnests on mod\u00adern architectures [18]. They also show that such transformation sequences are out of reach \nof classical loop optimization frame\u00adworks, whereas multidimensional af.ne scheduling can success\u00adfully \nmodel them as one single optimization step [17, 14], and scales to large loop nests with hundreds of \narray references [18]. Within this space of complex sequences of loop transformations, our work is the \n.rst to simultaneously address the two aforemen\u00adtioned challenges. We make the following contributions. \n1. By considering multidimensional schedules, we tackle any pos\u00adsible static-control loop nest in a program, \na major leap for\u00adward from the state-of-the-art iterative optimization on one\u00addimensional schedules [34]. \nWe simultaneously extend the ap\u00adplication domain applicability and scalability to larger codes and search \nspace expressiveness construction of more complex sequences of transformations. 2. To harness the combinatorial \nexplosion of the optimization problem, we present a scalable traversal heuristic and origi\u00adnal genetic \noperators, tailored to ef.ciently traverse a space of legal, distinct multidimensional schedules. These \noperators leverage the algebraic structure and statistical properties of the space. 3. We simultaneously \ndemonstrate good performance gains and excellent convergence speed on huge search spaces, even on larger \nloop nests (up to 20 loops) where iterative af.ne schedul\u00ading has never been attempted before. 4. We \ndemonstrate signi.cant performance improvements on 3 different architectures.  The paper is structured \nas follows. Section 2 introduces multi\u00addimensional scheduling in the polyhedral model. Section 3 con\u00adstructs \nthe search space of legal, distinct versions (multidimen\u00adsional schedules) for a program, and the key \nproperties of this space. Section4proposesa .rst heuristictoef.cientlytraversethis spacein the caseof \nsmallkernels. Section5de.nesa genetic algo\u00adrithm with specialized mutation and reproduction operators \nthat can effectively traverse huge search spaces associated with larger loop nests. We show that our \ncustom genetic algorithm achieves good performance improvements despite the poor statistical distribution \nof performance-enhancing schedules. Section 6 discusses related work and we conclude in Section 7. 2. \nThinking inPolyhedra Most compiler internal representations match the inductive seman\u00adtics of imperative \nprograms (syntax tree, call tree, control-.ow graph, SSA). In such reduced representations of the dynamic \nexe\u00adcution trace, a statement of a high-level program occurs only once, even if it is executed many times \n(e.g., when enclosed within a loop). Representing a program this way is not convenient for ag\u00adgressive \noptimizations which often need to consider a representa\u00adtion granularity at the level of dynamic statement \ninstances. For example, complex transformations like loop interchange, fusion or tiling operate on the \nexecution order of statement instances [46]. Due to compilation-time constraints and to the lack of an \nadequate algebraic representation of the semantics of loop nests, traditional (non-iterative) compilers \nare unable to adapt the schedule of state\u00adment instances of a program to best exploit the architecture \nre\u00adsources.Forexample, compilers can typically not apply any trans\u00adformation if data dependences are \nnon-uniform (unimodular trans\u00adformations, tiling), if the loop trip counts differ (fusion) or simply \nbecause pro.tability is too unpredictable. As a simple illustration, consider the Ring-Roberts edge detection \n.lter shown in Figure 1. While it is straightforward to detect a high level of data reuse be\u00adtween the \ntwo loop nests, none of the compilers we considered Open64 4.0, ICC 10.0, PathScale 3.0, GCC 4.2.0 \nwere able to apply loop fusion for a potentially 50% cache miss reduction when arrays do not .t in the \ndata cache (plus additional scalar promotion and instruction-level-parallelism improvements). Indeed, \nthis ap\u00adparently simple transformation actually requires a non-trivial com\u00adposition of (two-dimensional) \nloop shifting, fusion and peeling. /* Ring blur filter */ for (i=1;i<length-1;i++) for (j=1;j<width-1;j++) \n R Ring[i][j]=(Img[i-1][j-1]+Img[i-1][j]+Img[i-1][j+1]+ Img[i][j+1] + Img[i][j-1] + Img[i+1][j-1]+Img[i+1][j]+Img[i+1][j+1])/8; \n/* Roberts edge detection filter */ for (i=1;i<length-2;i++) for (j=2;j<width-1;j++) P Img[i][j]=abs(Ring[i][j]-Ring[i+1][j-1])+ \nabs(Ring[i+1][j]-Ring[i][j-1]); Figure 1. Ring-Roberts edge detection for noisy images To build complex \nloop transformations, a well known alterna\u00adtive is to represent programs in the polyhedral model. It \nis a .exi\u00adble and expressive representation for loop nests with statically pre\u00addictable control .ow. \nThe polyhedral model captures control-.ow and data-.ow with three linear algebraic structures, described \nin the following subsections. Such loop nests amenable to algebraic representation are called static \ncontrol parts (SCoP) [15, 18]. 2.1 Iteration Domains Iteration domains capture the dynamic instances \nof all statements all possible values of surrounding loop iterators through a set of af.ne inequalities. \nFor example, statement R in Figure 1 is executed for every value of the pair of surrounding loop counters, \ncalled the iteration vector: the iteration vector of statement R is x.R =(i, j). Hence, the iteration \ndomain of R is de.ned by its enclosing loop bounds: DR = {i, j| 1= i= length - 1. 1= j= width - 1}, which \nforms a parametric polyhedron (a space bounded by inequal\u00adities, a.k.a. hyperplanes or faces). Each integral \npoint inside this polyhedron corresponds to exactly one execution of statement R. This model allows the \ncompiler to manipulate statement execution and iteration ordering at the most precise level. 2.2 Subscript \nFunctions Subscript functions capture the data locations on which a statement operates. In static control \nparts, memory accesses are performed through array references (a variable being a particular case of \nan array). We restrict ourselves to subscripts of the form of af.ne expressions which may depend on surrounding \nloop counters (e.g., i and j for statement R) and global parameters (e.g., length and width in Figure \n1). Each subscript function is linked to an array that represents a read or a write access. For instance, \nthe subscript function for the read reference Img[i-1][j] of statement R is simply f(i, j)=(i- 1, j). \n 2.3 Multidimensional Schedules Iteration domains de.ne exactly the set of dynamic instances for each \nstatement. However, this algebraic structure does not describe the order in which each instance has to \nbe executed with respect to other instances. Of course, we do not want to rely on the inductive semantics \nof the sequence and loop iteration for this purpose, as it would break the algebraic reasoning about \nloop nests. A convenient way to express the execution order is to give each instance an execution date. \nIt is obviously impractical to de.ne all of them one by one since the number of instances may be either \nvery large or unknown at compile time. An appropriate solution is to de.ne, for each statement, a scheduling \nfunction that speci.es the execution date for each instance of a corresponding statement. For tractability \nreasons, we restrict these functions to be af.ne (relaxationofthis constraintmayexists[4],but challengesthecode \ngeneration step [5]). This work deals with multidimensional schedules: given a state\u00adment S, it is an \naf.ne form on the outer loop iterators .xS and the global parameters .n. It is written .. x.S .S(.xS)= \nTS..n . 1 where TS is a matrix of constants (possibly not integers). Multi\u00addimensional dates can be seen \nas clocks: the .rst dimension cor\u00adresponds to days (most signi.cant), next one is hours (less sig\u00adni.cant), \nthe third to minutes, and so on. Unlike one-dimensional af.ne schedules, every static control program \nhas a multidimen\u00adsional af.ne schedule [17]. Hence the application domain of the present work extends \nto all static control parts in general programs. For a concrete intuition of scheduling functions, consider \nagain the Ring-Roberts example of Figure 1. A possible multidimen\u00adsional schedule is: .R(i, j)=(i, j) \nand .P(i, j)=(i+ length - 2, j). This means the schedule of statement Rorders its instances accord\u00ading \nto i .rst and then j. This matches the structure of the loops of Figure 1. This is similar for statement \nP, except for the offset on the .rst time-dimension which states that the .rst nest runs before the second \none: while the largest value of the .rst time-dimension for R is length - 2, the smallest value of the \n.rst dimension of P is length - 1. Hence the loop surrounding P starts after the loop surrounding R. \nEf.cient algorithms and tools exist to generate tar\u00adget code from a polyhedral representation with multidimensional \naf.ne schedules [37, 5]. Recent work by Vasilache et al. [42] im\u00adproved these algorithms to scale up \nto thousands of statements.  2.4 Bene.tsofaPolyhedral Representation Reasoning about programs in such \na polyhedral representation has manyadvantages, for both program analysis and transformation: 1. exact \ndependence analysis is possible [15, 35]; 2. there exist ef.cient algorithms and tools to regenerate \nimpera\u00adtive code [37, 5]; 3. loop transformation sequences of arbitrary complexity can be constructed \nand transparently applied in one single step.  Amore complete descriptionof static control partswasgivenby \nXue [47] and their applicability to compute intensive, scienti.c or embedded applications have been extensively \ndiscussed by Girbal et al. andPalkovi.c [18, 31]. Frameworks to highlight SCoPs in gen\u00aderal programs \nand to extract both iteration domains and subscript functions already exist or are in active development \nin compiler platforms like WRAP-IT/URUK for Open64 [18], and Graphite for GCC [32]. Multidimensional \naf.ne schedules support arbitrary complex compositions of a wide range of program transformations. Several \nframeworkshavebeen designedtofacilitatetheexpressionofsuch transformations [21], or to enable their composition \nand semi\u00adautomatic construction [18, 43]. As illustration, a trivial loop fusion is not possible to improve \ndata locality on the Ring-Robertskernel in Figure 1. Because of both data dependences and non-matching \nloop bounds, only a partial loop fusion is possible, which translates into a sequence of, e.g., fusion, \nshifting and index-set splitting [46]. Using multidimensional schedules, a correct transformation (found \nusing chunking [6]) is simply: .R(i, j)=(i, j) and .P(i, j)=(i+ 2, j). One may care to check, using anypolyhedral \ncode generator, that the corresponding target code corresponds to a quite complex composition of syntactic \ntransformations. 3. Generating ProgramVersions The space of multidimensional af.ne schedules is very \nexpressive. Each point in the space corresponds to potentially very different program versions, exposing \na wide spectrum of interactions be\u00adtween architectural components and back-end compiler optimiza\u00adtions. \nThis section presents the formal construction of the space of legal, distinct schedules only. We also \ngive a practical heuristic to reduce the combinatorial of any algorithm to traverse this space while \npreserving the legality property. 3.1 Generating All Legal Schedules Nisbet [30], then Long and Fursin \n[28] experimentally observed that choosing a schedule at random is very likely to lead to an illegal \nprogram version. Moreover, the probability of .nding a legal one (which do not alter semantics) decreases \nexponentially with program size [34]. This challenge can only be tackled when integrating data de\u00adpendence \ninformation into the construction of the search space. Two statement instances are in dependence if they \naccess the same memory location and at least one of these accesses is a write. Main\u00adtaining the relative \norder of such instances is a suf.cient condition to preserve the original program semantics [7]. Dependences \nin static control parts can be expressed by depen\u00addence polyhedra whose formal description has been proposed \nby Feautrier [15]. A dependence polyhedron DR,S is a subset of the Cartesian product of the iteration \ndomains of statements R and S. Each integral point of a dependence polyhedron corresponds to a pair of \ninstances of the statements in dependence. Thus, a schedule does not change the semantics of the original \nprogram if it satis.es the precedence constraint: for all pairs of iteration vectors x.R and x.S in all \ndependence polyhedra, .R(x.R) . .S(x.S), where . denotes the lexicographic ordering.1 The af.ne form \nof Farkas lemma allows to translate such con\u00adstraints into an af.ne equivalent [39]. Feautrier used this \nresult to express every constraint that a one-dimensional schedule must re\u00adspect to preserve the semantics \nof the original program[16]. Those constraints bound a space where each integral point corresponds to \na legal schedule. Pouchet et al. showed it is possible to traverse this space ef.ciently for small programs \nthat accept one-dimensional schedules [34]. But dealing with multidimensional schedules leads to a combinatorial \nexplosion. Using one-dimensional schedules, all dependences have to be satis.ed within a single time \ndimension: the precedence constraint is simply .R(x.R) < .S(x.S) and . is a row vector. In multidimen\u00adsional \nschedules, the legality constraints can also be built time di\u00admension per time dimension, with the difference \nthat a dependence needs to be weakly solved .S(x.S) - .R(x.R) . 0 for the .rst time dimensions until \nit is strongly solved .S(x.S) - .R(x.R) > 0 at a given time dimension d. Once a dependence has been \nstrongly solved, no additional constraint is required for legality at dimensions d ' > d. Reciprocally, \na dependence must be weakly solved for all d '' < d. There is freedom to decide at which time dimension \na dependence will be strongly solved. Each possible de\u00adcision leads to a potentially different search \nspace. Furthermore, it is possible to arbitrarily increase the number of time dimensions of the schedule, \nresulting in an in.nite set of scenarios in general. The output is in the form of a list of polyhedra \nof legal schedules, one for each time dimension. Anaive solutiontobuild(a representative subsetof)all \nmultidi\u00admensional schedules would be to restrict the possible scenarios by setting an upper bound on \nthe number of time dimensions (e.g., the loop nest depth + 1), which is already unrealistic for programs \nof more than a few dependences. Note that loop tiling (a.k.a. blocking) is not directly expressible with \nmultidimensional schedules. It requires modi.cations of the iteration domain (insertion of new dimensions) \n[2, 18] or speci.c handling in the code generator [38]. Because of this, our search space does not currently \nencompass loop tiling. Recent results by Renganarayanan et al. and Bondhugula et al. are promising direc\u00adtions \ntowards fully integrating loop tiling with af.ne scheduling algorithms [38, 9, 10]. 1(a1,..., an) . (b1,..., \nbm) iff thereexists an integer1 = i= min(n, m) s.t. (a1,..., ai-1)=(b1,..., bi-1) and ai < bi. 3.2 Building \na Practical Search Space To build the search space, we face two combinatorial problems. First, there \nare too many polytopes to be considered. For in\u00adstance, the Ring-Roberts .lter shown in Figure 1 has \n12 depen\u00addence polyhedra, from which follows a huge number of possible strongly/weakly solved dependence \nscenarios. Second, one needs to limit the search to bounded polytopes. Yet, even the smallest bound leads \nto polytopes that are too large to be explored exhaus\u00adtively for complex loop nests. Feautrier found \na systematic solution to the explosion of the number of polyhedra: he considers a space of legal schedules \nlead\u00ading to maximum .ne-grain parallelism [17, 44]. To achieve this, a greedy algorithm maximizes the \nnumber of dependences solved for a given dimension. While this solution is interesting because it reduces \nthe number of dimensions and may exhibit inner paral\u00adlelism, it is not practical enough for several reasons. \nFirst, it needs to solve a system of linear inequalities involving every schedule co\u00adef.cient plus a \ndecision variable per dependence [17]. This makes the problem intractableforkernels withalarge setof \ndependences. Moreover, minimizing the number of dimensions often translates into big schedule coef.cients; \nthese generally leads to algorithmic complexity and both signi.cant loop bounds and control .ow over\u00adhead \nafter generation of the target imperative code [21]. We suggest a simple variation to overcome those \nissues. The following algorithm sketches our search space construction for a given static control part: \n1. Compute the exact set G of dependences for the SCoP through instancewise analysis [15] 2. d . 1 \n3. while G./  = 0 do (a) Initialize Ld (the space of legal schedules for time dimen\u00adsion d)to the full-space \npolyhedron (b) for each dependence DR,S . G  Compute WDR,S the space of legal schedules weakly satisfying \nonly DR,S by enforcing, for all pairs of points in DR,S: .S(.xs) - .R(x.R) = 0 Ld . Ld n WDR,S (c) for \neach dependence DR,S . G Compute SDR,S the space of legal schedules strongly satisfying only DR,S by \nenforcing, for all pairs of points in DR,S: .S(.xs) - .R(x.R) > 0 if Ld n SDR,S ./= 0 then Ld . Ld n \nSDR,S G. G- DR,S (d) d . d+ 1 This heuristic outputs for each schedule dimension d a space Ld of legal \nsolutions. The algorithm terminates; the proof uses the same argument as Feautrier s multidimensional \nscheduling algo\u00adrithm [44]: at least one dependence can be strongly solved per time dimension d. Nevertheless, \nit differs from Feautrier s algorithm as it does not guarantee a maximal number of dependences solved \nper dimension. Therefore, it may not minimize the number of dimen\u00adsions of the schedule: this is not \nan issue as we only consider se\u00adquential codes.2 However, this algorithm is ef.cient and only needs one \npolyhedron emptiness test per dependence,3 and the elimina\u00adtion ofFarkas multipliers used to enforce \nthe precedence constraint on schedule coef.cients is performed dependence per dependence (i.e., on very \nsmall systems) [34]. Since we consider sequential codes only, we can bound the coef.cient values within \n{-1, 0, 1} to minimize control-.ow overhead. This would have been very re\u00adstrictive if we were constrained \nto one-dimensional schedules. In the multidimensional case, although it eliminates some schedules from \nthe space (e.g., non-unit skewing), these bounds are compat\u00adible with the expression of arbitrary compositions \nof loop fusion, distribution, interchange, code motion; in the worst case, it trans\u00adlates into additional \ntime dimensions. Overall, this solution gives an interesting tradeoff between scalability and expressiveness \n(and performance of the generated code). So far, we have not de.ned the order in which dependences are \nconsidered when checking against strong satisfaction. This or\u00adder can havea signi.cant impact on the \nconstructed space.A long term approach would be to consider this order as part of the search space, but \nthis is not currently practical (combinatorial explosion). Instead, we use two analytical criteria to \norder the dependences. First of all, each dependence is assigned a priority, depending on the memory \ntraf.c generated by the pair of statements in depen\u00addence. We use a simpli.ed version of the model by \nBastoul and Feautrier [6]: for each array Aand dimension d, we approximate the rA traf.c as m , where \nmd is the size of the dth dimension of the array, d and rA is the rank of the concatenation of the subscript \nmatrices of all references to dimension d of array A in the statement. Thus, the generated traf.c evaluation \nfor a given statement is a multivariate polynomial in the parametric sizes of all arrays.We use pro.ling \nto instantiate these size parameters. Intuitively, maximizing the depth where a dependence is strongly \nsolved maximizes reuse in inner loops and minimizes the memory traf.c in outer loops. Therefore, we start \nwith dependences involved in the statements with the less traf.c. Our second criterion is based on dependence \ninterference; it is used in case of non-discriminating priorities resulting from the .rst criterion.Two \ndependences interfere if it is impossible tobuild a one-dimensional schedule strongly satisfying these \ntwo depen\u00addences.We .rst try to solve dependences interfering with the lower number of other dependences, \nmaximizing our chance to strongly solve more dependences within the current time dimension. 3.3 Scanning \nthe Search SpacePolytopes The algorithm presented in Section 3.2 constructs one polytope per dimension \nof the schedule. Picking one point in every poly\u00adtope fully describes one multidimensional schedule, \nhence one pro\u00adgram version: the generated imperative codes will be distinct if the scheduling matrices \nare distinct. Hence we need to scan the le\u00adgal polytopes tobuild multidimensional schedules. This is \nreminis\u00adcent of the classical polyhedron scanning problem [22, 5]; however, none of the existing algorithms \nscale to the hundreds of dimensions we are considering.Fortunately, our problem happens to be simpler \nthan static loop nest generation: we only need to dynamically enumerate every integral point that respects \nthe set of constraints. Each program version is represented by a unique scheduling matrix T. The .rst \ncolumns are schedule coef.cients associated with each loop iterator surrounding a statement in the original \npro\u00adgram (.i), for all statements. The next set of columns are sched\u00adule coef.cients associated with \nglobal parameters(.p), for all state\u00adments. The last column are the schedule coef.cients associated with \nthe constant(c), for all statements. 2Af.ne partitioning may be better suited to characterize parallelism \nin the polyhedral model [27]. 3Over Ld which contains exactly one variable per schedule coef.cient. \nSince we represent legal schedules as multidimensional af.ne functions, each row Td of the scheduling \nfunction corresponds to an integer point in the polytope of legal coef.cients Ld, built ex\u00adplicitly for \nthis dimension. A program version in the optimization space can thus be represented as follows, for a \nSCoP of t state\u00adments, a schedule of dimension s, and the iteration vector.x: .. .x1 . . . . . . . .. \n. . .xt 1 111 .. .i1 \u00b7\u00b7\u00b7 .i1 .p \u00b7\u00b7\u00b7 .pc \u00b7\u00b7\u00b7 c 1 t 1 t 1 t .. . .n1 . . . . . . . . . . . . T..x = . . \n. . . . . . . s sss .. .is \u00b7\u00b7\u00b7 .is .p \u00b7\u00b7\u00b7 .pc \u00b7\u00b7\u00b7 c .. 1 t 1 t 1 t . nt . .. 1 .. . .. . . 1 To build \neach row Td, we scan the legal polytope Ld, by suc\u00adcessively instantiating values for each coef.cient \nin a prede.ned or\u00adder.4 Fourier-Motzkin elimination a.k.a. projection [39] pro\u00advidesa representationof \ntheaf.ne constraintsofa polytope suitable for its dynamic traversal. Computing the projection of all \nvariables of a polytope Ld results in a set of constraints de.ning the same polytope, but where it is \nguaranteed that for a point v . Ld, the value of the kth coordinate vk only depends on v1,..., vk-1, \nthat is the af.ne inequalities involve only v1,..., vk. Thus, the sequential order tobuild coef.cients \nis simply the reverse order of theFourier-Motzkin elimination steps. This scheme guarantees that provided \na value in the projection of v1,..., vk-1, a value exists for vk, for all k.5 In its basic form, theFourier-Motzkin \nalgorithm is known to generate manyredundant constraints; these redundancies reduce its scalability on \nlarge polyhedra.We improved it,by maintaining the following properties for each variable elimination: \n1. anyconstraint de.ningahyperplane paralleltoanexisting con\u00adstraint is removed (this is trivially computed \nthanks to constraint normalization); 2. any variable which is linearly dependent to any other one(s) \nis removed (thanks to implicit equalities detection and Gaussian elimination); 3. constraints are removed \nif, once opposed, no point exists in the solution polytope (we apply the Le Fur descending method [25]). \n In practice, this modi.ed algorithm scales to hundreds of vari\u00adables (schedule coef.cients) in the \noriginal system. It is applied on each polytope Ld generated.  3.4 Schedule Completion Algorithm For \nSCoPs with more than 3 or 4 statements, the presented space construction algorithm leads to very large \nsearch spaces, challeng\u00ading any traversal procedure. It is possible to focus the search on some coef.cients \nof the schedule with maximal impact on perfor\u00admance, postponing the instantiation of a full schedule \nin a second heuristic step. We show that such a two-step procedure can be de\u00adsigned without breaking \nthe fundamental legality property of the search space. This approach will be used extensively to simplify \nthe optimization problem. We rely on the previous projection pass to guarantee it is always possible \nto complete or even correct anypoint, by slightly modify\u00ad 4The order has no impact on the completeness \nof the traversal. 5The case of holes in Z-polyhedra is handled through a schedule comple\u00adtion algorithm \ndescribed in the next section. ing its coordinates, to make it lie within a given polytope Ld. Our completion \nalgorithm is sketched in the following. Given a point v in a n-dimensional space with some unde.ned coordinates: \n1. set all unde.ned coordinates to 0; 2. for each k . [1, n]: (a) compute the lower bound lb and the \nupper bound ub of vk in Ld, provided the coordinate values for v1... vk-1, (b) if vk ./[lb, ub], then \nvk = lbif vk < lbor vk = ub if vk > ub.6   Thereforeitis possibleto partiallybuilda schedule pre.x, \ne.g., values for the.i coef.cients, leaving the other coef.cients unde\u00ad.ned. Then, applying this completion \nalgorithm will result in .nd\u00ading the minimal amount of complementary transformations to make the transformation \nlie in the computed legal space. The comple\u00adtion algorithm motivates the order of coef.cients in the \nT matrix. We showed that the most performance impacting transformations (interchange, skewing, reversal) \nare embedded in the .rst coef.\u00adcients of T the .i coef.cients; followed by coef.cients usually involved \nin fusion and distribution the .p coef.cients; and .\u00adnally the less impacting c coef.cients, representing \nloop shifting and peeling [33]. The completion algorithm .nds complementary transformations in order \nof least to most impacting, as it will not alter any vector pre.x if a legal vector suf.x exists in the \nspace. Three fundamental properties are embedded in this completion algorithm: 1. if v1,..., vk is a \npre.x of a legal point v, a completion is always found; 2. this completion will only update vk+1,..., \nvdmax, if needed; 3. when v1,..., vk are the .i coef.cients, the heuristic looks for the smallest absolute \nvalue for the .p and constant coef.cients, which corresponds to maximal (nested) loop fusion relative \nto the.i coef.cients.  Picking coef.cients as close as possible to0 has several advan\u00adtages in general: \nsmaller coef.cients tend to simplify code genera\u00adtion, improve locality, reduce latency, and increase \nthe size of basic blocks in inner loops. 4. Traversing the search space While it is possible to exhaustively \ntraverse the constructed space of legal versions for small SCoPs, in the case of one-dimensional schedules, \nit becomes unpractical in the multidimensional case. Pouchet et al. give a preliminary answer by means \nof a heuristic to narrow this space and accelerate the traversal [34].Webuild on this result to design \na powerful heuristic suitable for the multidi\u00admensional case. 4.1 A Multidimensional Decoupling Heuristic \nOur approach is called the decoupling heuristic as it leverages the completion algorithm of Section 3.4 \nto stage the exploration of large search spaces. It derives from the observation of the per\u00adformance \ndistribution, where density patterns hinted that not all schedule coef.cients have a signi.cant impact \non performance [34, 33]. The principle of the decoupling heuristic for one-dimensional schedules is (1) \nto enumerate different values for the.i coef.cients, (2) to instantiate full schedules with the completion \nalgorithm, and (3) to select the best completed schedules and further enumerate the different coef.cients \nfor the .p part.  A direct extension to the multidimensional case exhibits two major drawbacks. First, \nthe relative performance impact of the 6Z-holes are detected by checking if lb> ub. Benchmark compress-dct \n#Inst. 6 #Loops 6 #Dep. 56 #Dim. 3 dim 1 20 dim 2 136 dim 3 10857025 dim 4 n/a Total 2.9\u00d7 1010 edge 3 \n4 30 4 27 54 90534 43046721 5.6\u00d7 1015 iir 8 2 66 3 18 6984 > 1015 n/a > 1019 fir 4 2 36 2 18 52953 n/a \nn/a 9.5\u00d7 107 lmsfir 9 3 112 2 27 10534223 n/a n/a 2.8\u00d7 108 matmult 2 3 7 1 912 n/a n/a n/a 912 latnrm \n11 3 75 3 9 1896502 > 1015 n/a > 1022 lpc 12 7 85 2 63594 > 1020 n/a n/a > 1025 ludcmp 14 10 187 3 36 \n> 1020 > 1025 n/a > 1046 radar 17 20 153 3 400 > 1020 > 1025 n/a > 1048 Figure 2. Search space statistics \ndifferent schedule dimensions must be quanti.ed. Second, an ex\u00adhaustive enumeration of .i coef.cients \nfor all dimensions is out of reach, as the number of points exponentially increases with the number of \ndimensions. Figure 2 illustrates this assertion by sum\u00admarizing the size of the legal polytopes for different \nbenchmarks, for all schedule dimensions.We consider10 SCoPsextracted from classical benchmarks. The .rst \neight are UTDSP benchmarks [26] directly amenable to polyhedral representation: compress-dct is an image \ncompression kernel (8x8 discrete cosine transform), edge-convolve2d is an edge detection kernel (different \nfrom Ring-Roberts), fir is a Finite Impulse Response .lter, lmsfir is a Least Mean Square adaptive FIR \n.lter, iir is an In.nite Impulse Response .lter, matmult isa matrix multiplicationkernel, latnrm is a \nnormalized lattice .lter, and lpc (LPC analysis) is the hot function of a linear predictive coding encoder. \nWe considered two additional benchmarks: ludcmp solves simultaneous linear equa\u00adtions by LU decomposition, \nand radar is an industry code for the analysisof radar pulses.For each benchmark, we report the number \nof (complex) instructions carrying array accesses (#Inst), the num\u00adber of loops(#Loops), dependences(#Dep), \nschedule dimension (#Dim), and the total number of points for those dimensions (still only legal schedules) \nwhere > 10n provides a conservative lower bound when it was not possible to compute the exact space size \nin a reasonable amount of time. Relations between schedule dimensions To extend the decou\u00adpling approach \nto multidimensional schedules, we need to inte\u00adgrate interactions between dimensions. For instance, to \ndistribute the outer loop of a nest (which can improve locality and vectoriza\u00adtion [3]), one can operate \non the .p and c parts of the schedule for the .rst dimension (a parametric shift). On the other hand, \naltering the.i parts will lead to the most signi.cant changes in the loop con\u00adtrols. Indeed, the largest \nperformance variation is usually captured through the.iparts [33], and a careful selection of those coef.cients \nis mandatory to attain the best performance; conversely, it is likely that the best performing transformations \nwill share similar.i coef.\u00adcients in their schedules. Furthermore, the .rst dimension is highly constrained \nin gen\u00aderal, since all dependences need to be weakly or strongly considered. Conversely, the last dimension \nis the less constrained and often carries only very few dependences.7 The decoupling heuristic in a nutshell \nWe conducted an exten\u00adsive experiment showing that T1 (the .rst time dimension of the schedule)isa major \ndiscriminantoftheoverall performance distri\u00adbution[33]. Therefore,theheuristic startswithanexplorationofthe \n7This is typically the case when the .nal dimension is required to order the statements within an innermost \nloop. different legal values for the coef.cients of T1, and the completion algorithm is called to compute \nthe remaining rows of T. Further\u00admore, this exploration is limited to the subspace associated with the \n .i coef.cients of T1 (and the remaining coef.cients of T1 are also computed with the completion algorithm), \nexcept if this subspace is smaller than a given constant L1(L1 = 50 in our experiments). L1 drives the \nexhaustiveness of the procedure: the larger the degree of freedom, the slower the convergence. By limiting \nto the.i class we target only the most performance impacting subspaces. To enumerate points in the polytopes, \nwe incrementally pick a dimension then pick an integer in the polyhedron s projection onto this dimension. \nNote that the full projection is computed once and for all by the Fourier-Motzkin algorithm presented \nin Section 3.3, before traversal. Technically, to enumerate integer points of the subspace composed of \nthe .rst m columns of Ld, we de.ne the following recursive procedure tobuilda point v: EXPLORE (v, k, \nLd): 1. compute the lower bound lb and the upper bound ub of vk in Ld, provided the coordinate values \nfor v1... vk-1; 2. for each x . [lb, ub]: (a) set vk = x, (b) if k < m call EXPLORE (v, k+ 1, Ld)else \noutputv.   The enumeration is initialized with a call to EXPLORE (v, 1, Ld). The completion algorithm \nis then called on each point v generated, to compute a legal suf.x for v (corresponding to the columns \n[m+ 1, n] of Ld), .nally instantiating a legal point of full dimensionality. Then, the heuristic selects \nthe x%best values forT1(x = 5% in our experiments), it proceeds with the exploration of values for co\u00ad \nef.cients of T2 with the selected values of T1, and recursively until the lastbut one dimension of the \nschedule. The last dimension (cor\u00ad responding to the innermost nesting depth in the generated code) is \nnot traversed,but completed witha singlevalue:exploringitwould yield a huge number of iterations, with \nlimited impact on the gen\u00ad erated code, and negligible impact on performance. Eventually, the exploration \nis bounded with a static limit (1000 evaluations in our experiments).  4.2 Experiments We consider three \ntarget architectures. The AMD Alchemy Au1500 is an embedded SoC with a MIPS32 core (Au1) running at 500MHz. \nWe used GCC 3.2.1 with the -O3 .ag (version of GCC and option with peak performance numbers, according \nto the manufacturer). The STMicroelectronics ST231 is an embed\u00added SoC with a 4-issue VLIW core running \nat 400MHz and a blocking cache. We used st200cc 1.9.0B (Open64) with the .ags -O3 -mauto-prefetch -OPT:restrict. \nThe AMD Athlon X64 3700+ has a 1MB L2 cache and runs at 2.4GHz. It runs Mandriva Linux and the native \ncompiler is GCC 4.1.1.We used the following optimization .ags for this platform which are known to bring \nex\u00adcellent performance: -O3 -msse2 -ftree-vectorize . For this particular machine, hardware counters \nwere used to collect .ne\u00adgrained cycle counts, and we used a real-time priority scheduler to minimize \nOS interference. We used the average of 10 runs for all performance evaluations. We implemented an instancewise \ndependence analysis, the con\u00adstruction of the space of legal transformations, and the ef.cient scanning \nalgorithms introduced in this paper.8 We used free soft\u00adware such as PipLib [15, 45] (a polyhedral library \nand paramet\u00adric integer linear programming solver) and CLooG [5] (an ef.cient code generator for the \npolyhedral model). For each tested point in the search space, (1) we generated thekernelC code with CLooG,9 \n(2) then we integrated this kernel in the original benchmark along with instrumentation to measure running \ntime (we use performance counters when available), (3) we compiled this code with the native compiler \nand appropriate options, (4) and .nally run the program on the target architecture and gathered performance \nresults. The original code is included in this procedure starting at the second step, for appropriate \nperformance comparison. The full iterative compilation and execution process takes a few seconds using \nour heuristic, and up to a few minutes using the GA described Sec\u00adtion 5 for the largest benchmark (up \nto 1000 tested versions). The time to compute the legal space and to generate points is negligible with \nrespect to the total running time of the tested versions. Results Figure 3 shows the results for the \nthree architectures we considered.We reportthe total numbersof testedversions(Tested), the run index \nof the best performing version(Id Best; the lower, the earlier), and the performance improvement of execution \ntime in percentage(Perf. Imp.).We also imposeda static limitofevaluating 1000 data points in the search \nspace.10 All UTDSP experiments use the reference dataset size. Increas\u00ading data size would emphasize \nlocality effects, yielding better per\u00adformance improvements. E.g., matmult on Athlon with n = 250 yields \n361% performance improvement, n = 64 yields 318% im\u00adprovement, whereas the reference value n = 10 yields \n43% im\u00adprovement. Discussion Our results show signi.cant improvement on allker\u00adnels of the UTDSP suite. \nIn addition, about 50 runs were suf.cient forkernels with less than 10 statements (allbut lpc and radar). \nFor all benchmarks, the best program version is syntactically veryfar from the original one. A good illustration \nof this is given for the Ring-Roberts run\u00adning example, which achieves a 47% performance improvement \non a full HD image on AMD Athlon; hardware counter details show a 54% reduction of the L1 hit/miss ratio \nand a 51% of the data TLB misses. This complex transformation is the result of multidimen\u00adsional shifting \nand peeling of the iterations preventing fusion, and the complete fusion of the remaining iterations. \nThe limited performance improvement for edge-convolve2d is directly correlated to the code structure: \nthis benchmark performs a convolution of a 3x3 kernel, and is an excellent candidate for optimization \nwith loop unrolling a transformation not embedded in our search space. Our technique is fully compatible \nwith other iterative search techniques such as parameters tuning [1], and it is 8LETSEE, the LEgal Transformation \nSpacE Explorator, available at http://letsee.sourceforge.net. 9We use CLooG version 0.14.0 with default \noptions. 10 It matches the maximum number of versions considered by the genetic algorithm in Section \n5. expected that this combination would bring excellent performance in this case. We also noticed that \nperformance improvements are often the result of indirect enabling of back-end compiler optimizations \n(e.g., vectorization or scalar promotion), in addition to the direct impact on hardware components (e.g., \nlocality). Modern compiler opti\u00admization heuristics are still fragile, and the interactions between optimization \nphases are not captured in their design. Predicting this interaction on non-trivial codes is still out \nof reach, and slight syn\u00adtactic differences can trigger different optimization results.Testing different \nsource code having the same semantics is one way to cir\u00adcumvent the compiler s optimization unpredictability. \nIn addition, the best iteratively found transformation for a given benchmark is different when considering \na different target architec\u00adture. This is due to different interactions with the compiler, as well as \ndifferent architectural features to optimize for. Note that it is not a consequence of working with more \nexpressive schedules: we al\u00adready highlighted a similar pattern for the case of one-dimensional schedules \n[34]. It con.rms the complexity of the optimization prob\u00adlem and the relevance of a feedback-directed \napproach. The heuristic heavily relies on the observation that the .rst dimension of the schedule contains \nvery few points it traverses this dimension exhaustively. However, exhaustive enumeration is only possible \nfor smallkernels, such as most UTDSP benchmarks. Unfortunately, for larger programs like lpc, ludcmp, \nradar, and to some extent on latnrm, this approach does not scale. To address this scalability issue, \nwe substitute the exhaustive search with a traversal driven by a genetic algorithm. 5. EvolutionaryTraversal \nof thePolytope This section introduces novel genetic operators tailored to the traversal of polytopes \nof legal af.ne schedules. Genetic algorithms (GA) [19] are known for their genericity: we chose an evolutionary \napproach because of the natural encoding of the geometric properties of the search space into crossover \nand mutation operators. The two main properties are the following: 1. to enforce legality and uniqueness \nof the program versions, the genetic operators must be closed on the search space polytope; we construct \ndedicated mutation and crossover operators satis\u00adfying this property; 2. unlike random search, the traversal \nis characterized by its non\u00aduniformity (from the initial population and the crossovers); this is utterly \nimportant as the largest part of the search space is generally plagued with poor or similar performing \nversions [34, 33].  Genetic algorithms have often be used in program optimization. Our contribution \nis to reconcile .ne-grain control of a transforma\u00adtion heuristics as opposed to optimization .ag or \npass selection [41, 1] with the guaranteed legality of the transformed program as opposed to .ltering \napproaches [30, 29, 28] or always-correct transformations [40, 24]. 5.1 Genetic Algorithm Using classical \nGA operators would not be an ef.cient way to gen\u00aderate data points in our search space. This is because \nlegal sched\u00adules lie in af.ne bounds that are strongly constrained and changing them at random has a \nvery low probability of preserving legality. Moreover, in general, this probability decreases exponentially \nwith the space dimension [30, 34].We thus need to understand the prop\u00aderties of the space of legal schedules, \nand to embed them into ded\u00adicated GA operators. Some properties of af.ne schedules The construction algorithm \noutputs one polytope per schedule dimension. We can deduce nu\u00ad AMD Athlon Tested Id. Best Perf. Imp. \n480 19 37.11% compress-dct 243 11 5.58% edge 1000 34 37.50% iir 77 33 40.24% .r 1000 51 30.98% lms.r \n81 16 42.87% matmult 1000 6 15.11% latnrm 1000 489 31.15% lpc 1000 37 4.50% ludcmp 1000 405 6.42% radar \n25.14% Average ST231 Tested Id. Best Perf. Imp. 480 39 15.11% 243 12 3.10% 1000 6 24.91% 77 2 17.96% \n1000 9 10.17% 81 16 17.91% 1000 13 2.61% 1000 158 1.99% 1000 391 6.33% 1000 709 4.12% 10.42% Au1500 Tested \nId. Best Perf. Imp. 480 30 22.37% 243 17 2.51% 1000 38 3.12% 77 27 14.00% 1000 11 15.80% 81 17 20.18% \n1000 43 15.19% 1000 82 14.08% 1000 175 3.66% 1000 454 3.39% 11.43% Figure 3. Results of the decoupling \nheuristic for AMD Athlon, ST231 and Au1500 merous properties on these polytopes, either deriving from \nthe con\u00adstruction algorithm or from af.ne scheduling itself. In the follow\u00ading, the term af.ne constraint \nrefers to any dependence, iteration domain, or search bound constraint on coef.cients of the schedule. \n1. No af.ne constraint involves coef.cients from different rows of T, since those coef.cients are computed \nfrom distinct poly\u00adtopes. Of course, multiple coef.cients inside a row can be in\u00advolved in a constraint. \n 2. Multiple coef.cients involved in a constraint are called de\u00adpendent. Each row can be partitioned \ninto classes of depen\u00addent coef.cients, where no constraint involves coef.cients from different classes. \nFor example, in the polyhedron de.ned by {x1 + x2 = 0. x3 = 0} we say that the set {x1, x2} is indepen\u00addent \nfrom the set {x3}. Legality preservation is local to each class of dependent coef.cients.  We design \nnovel genetic operators exploiting and preserving these properties. Initialization We .rst introduce \nan individual with a statically computed schedule,builtby applying the completion algorithm on a fully-unde.ned \nschedule. This choice shares its motivation with the decoupling heuristic in Section 4.1. The rest of \nthe population is initialized by performing aggres\u00adsive mutations on this static schedule; we generate \n30 to 100 indi\u00adviduals, depending on the space dimension. The initial population is heavily biased towards \na particular subspace (typically the sub\u00adspace of the.i coef.cients), emphasizing the non-uniformity \nof the traversal. Mutation The mutation operator starts with the computation of the distribution of probabilities \nto alter every coef.cient. This prob\u00adabilityis drivenby threefactors; the .rst one derives directly from \nthe heuristic of the one-dimensional case [34]: coef.cients of the iteration vectors have a dramatic \nimpact on the structure of the generated code; minor modi.cations trigger wild jumps in the search space; \n coef.cients with little linear dependences with others may re\u00adquire more mutations to trigger signi.cant \nchanges, e.g., mod\u00adifying their value will not require updating many other coef.\u00adcients to make the point \nlegal;  lower dimensions and especially the scalar ones usually have a lower impact on performance. \n In addition, we weigh the probabilities with a uniform annealing factor, to tune the aggressiveness \nof the mutation operator along with the maturation of the population. We randomly pick a value within \nthe legal bounds for this coef.cient, and according to the distribution of probabilities. As this mutation \nmay cause other coef.cients to becomeincorrect, we then update the schedule with the completion algorithm \ndepicted in Section 3.4; it is a simple update because the schedule pre.x can be kept in the legal space, \ncomputing mutated coef.cients in the reverse order ofFourier-Motzkin elimination. We also experimented \nwith a simpler mutation operator, where the bounds to pick mutated values where not adjusted to the corre\u00adsponding \npolytope of legal versions, applying our correction mech\u00adanism a posteriori. This approach did not prove \nvery effective as coef.cients are often correlated or severely constrained: randomly picking values for \nmultiple correlated coef.cients often leads to identical schedules after correction. Only an incremental \napplica\u00adtion of the correction mechanism avoids the generation of many duplicates (which strongly degrade \nthe effectiveness of the muta\u00adtion operator). Crossover We propose two operators. The row crossover aims \nto compensate the row-wise scope of the mutation operator. Given two individuals represented by T and \nT ' , the row crossover operator randomly picks rows of either T or T ' to build a new individual T '' \n. This operator obviously preserves legality since there are no dependences between rows. Since the mutation \noperates within a schedule dimension, it may succeed in .nding good candidates for a given row of T or \nT ' , but may mix these with ineffective rows. Combining these rows may lead to a good schedule, with \na much higher probability than with mutation alone. The column crossover is dedicated to crossing independent \nclasses of schedule coef.cients (represented by sets of columns not connected by any af.ne constraint); \nthis operator is quite orig\u00adinal and speci.c to the geometrical properties of the search space. It can \nbe seen as a .ner-grained crossover operator. From two in\u00addividuals T and T ' , it randomly selects an \nindependent class from either parent at every dimension to build T '' . When there is only one independent \nclass in a given schedule dimension this operator behaves like the row crossover. We rely on the geometric \nproperties of the polytope to compute classes of dependent coef.\u00adcients for a given schedule dimension. \nThese classes are computed immediately after the search space polytope is built, and do not change during \ntraversal. This operator preserves legality as it only modi.es independent sets of schedule coef.cients. \nDependences constrain schedule coef.cients in pairs of state\u00adments. Several transitive steps are needed \nto characterize all cor\u00adrelations between coef.cients in a dependent class. This operator carefully re.nes \nthe grain of schedule transformations, while pre\u00adserving legality as it only modi.es independent sets \nof coef.cients. Selection The selection process uses the best half of the current population for the \nnext generation. Mutation and crossover are applied on these individuals to generate a new full population. \nInstead of considering only running time, a better option might be to combine multiple metrics, including \nperformance predictors (to avoid running the code) or multiple hardware counters.  5.2 Experimental \nResults Figure4summarizes the results of the genetic algorithm applied to all benchmarks for all architectures. \nRow Heuristic/GA shows the fraction of the performance improvement achieved by the decou\u00ad  Figure 4. \nResults of the genetic algorithm. The decoupling heuristics succeeds in discovering 78-100% of the performance \nimprovement achieved by GA for all benchmarks of less than 10 statements. For larger benchmarks, the \nGA performs 2.46\u00d7 better in average, and up to 16\u00d7 better. pling heuristic w.r.t. the genetic algorithm, \nand fractions are aver\u00adaged for the benchmarks of less than 10 statements versus more than 10. We initialized \nthe population with 30 to 100 individu\u00adals, and performed at most 10 generations; therefore, the maximum \nnumber of runs for each program was 1000. Comparing these results with the tablein Figure3shows the ef\u00ad.ciencyand \nscalability of our method. The genetic algorithm (GA) achieves good performance improvements for the \nlarger kernels; these improvements are much better than those of the decoupling heuristic for the larger \nbenchmarks. On the other hand, the decou\u00adpling heuristic exposes 78 100% of the improvement obtained \nwith GA within the .rst50 runs, for allkernelsof less than10 statements Results are better on AMD Athlon \nthan on embedded proces\u00adsors, probably because the architecture is more complex: a good interaction between \narchitectural components is harder to achieve and brings higher improvements. Conversely, the ST231 and \nAMD Au1500 have a predictable behaviour, more effectively harnessed by the back-end compiler, and showing \nless room for improvement; yet our results are still signi.cant for such targets. We report a detailed \nstudy of the representative compress-dct benchmark, on AMD Athlon. Figure5summarizes the results, and \ncon.rms the huge advantage of the GA given the statistically sparse and chaotic occurrence of performance-enhancing \nschedules. Fig\u00adure 5(a) shows the convergence of our GA approach versus a Ran\u00addom traversal in the space \nof legal schedules (only legal points are drawn). The GA algorithm ran for 10 generations from an initial \npopulation of 50 individuals. Both plots are an average of 100 com\u00adplete runs. Figure 5(b) reports the \nperformance distribution of the legal space.Weexhaustively enumerate andevaluate all points with a distinct \nvalue for the.i+.pcoef.cients of the .rst schedule dimen\u00adsion, combined with all points with a distinct.i \nvalue for the second one; a total of 1.29 \u00d7 106 schedules are evaluated. For each dis\u00adtinct value of \nthe .rst schedule dimension (plotted in the horizontal axis), we report the performance of the Best schedule, \nthe Worst one, and the Average for all tested values of the second schedule dimension. Figure 5(c) shows \nthe performance distribution for all tested points of the second schedule dimension, provided a single \nvalue for the .rst one, sorted from the best performing one to the worse (the best performing schedule \nbelongs to this chart). Figure 5(a) shows that our GA converges muchfaster than ran\u00addom search. Random \nsearch achieves only 18% performance im\u00adprovement, after 500 runs, while the GA takes only 120 runs to \nmatch this result. The GA converges towards 44.1% performance improvement after 350 runs, at the 7th \ngeneration, before the im\u00adposed limit of 10 generations. This the maximum performance im\u00adprovement available, \nas shown by the exhaustive search experi\u00adments in Figure 5(b). The effectiveness of the genetic operators \nis illustrated by the decorrelation of the performance improvements and the actual performance distribution. \nConversely, random traver\u00adsal follows the shape of the performance distribution, and in aver\u00adage is not \nable to reach the best performing schedules as their density in the space is very low. The dif.culty \nto reach the best points in the search space is emphasized by their extremely low proportion: only 0.14% \nof points achieve at least 80% of the max\u00adimal performance improvement, while only 0.02% achieve 95% \nand more. Conversely, 61.11% degrade performance of the original code, whilein total 10.88%degrade the \nperformancebyafactor2. Finally, we studied the behavior of multiple schedules for the compress-dct benchmark, \nanalyzing hardware counters on Athlon. This study highlights complex interactions between the memory \nhierarchy (both L1 and L2 accesses are minimized to achieve good performance), vectorization, and the \nactivity of func\u00adtional units. The best performing transformation reduces the num\u00adbersof stallcyclesbyafactorof3, \nwhileimprovingtheL2 hit/miss ratioby 10%.Transformation sequences achieving the optimal per\u00adformance \nare opaque at .rst glance: they involve complex com\u00adbinations of skewing, reversal, distribution and \nindex-set splitting. These transformations address speci.c performance anomalies of the loop nest, but \nthey are often associated with the interplay of multiple architecture components. Moreover, we observe \nthat the best optimizations are usually associated with more complex con\u00adtrol .ow than the original code. \nThe number of dynamic branches is increased in most cases [33], although stall cycles are heavily reduced \ndue to locality and ILP improvements. Our results con.rm the potential of iterative optimization to ac\u00adcurately \ncapture the complex behavior of the processor and com\u00adpiler, and extends its applicability to optimization \nproblems far more complex than those typically solved in adaptive compilation. 6. RelatedWork In recent \nyears, the bene.ts of iterative compilation have been widely reported [23, 12, 13, 20]. Iterative compilation \nis often able to .nd optimization sequences that out-perform the highest optimization settings in commercial \ncompilers. Kulkarni et al. [24] introduce the VISTA system, an interactive compilation system which concentrates \non reducing the time to .nd good solutions. Another system that attempted to speedup iterative compilation \nwas introduced by Cooper et al. called ACME [11]. Triantafyllis et al. [41] develop an alternative approach \nto reduce the total number of evaluations of a new program. Here the space of compiler options is examined \noff-line on a per function basis and the best performing ones are classi.ed into a small tree of options. \nIterative optimization has been used effectively on a variety of compilation and parallelization problems \nand its applicability and practicality has been demonstrated beyond the academic world [36]. Although \nmultidimensional af.ne scheduling is an obvious target for iterative optimization, its pro.tability is \none of the most dif.cult to assess, due to (1) the model s intrinsic expressiveness (the downside of \nits effectiveness) and (2) its lack of analytical models for the impact of transformations on the target \narchitec\u00adture.Hence, relatedworkhasbeenvery limiteduptothispoint.To the best of our knowledge, Nisbet \npioneered research in the area with one of the very .rst papers in iterative optimization. He de\u00adveloped \nthe GAPS framework [30] which used a genetic algorithm GA versus random - compress-dct Performance distribution \n- compress-dct Performance distribution (sorted) - compress-dct 1.4 1.6 GA Random 1.4 1.6 Best Average \nWorst 1.4 1.6 1.2 1.2 1.2 Speedup Speedup 1 0.8 Speedup 1 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 50 100 150 \n200 250 300 350 400 450 500 Tested versions (a)  10 20 30 40 50 60 Point index of the first schedule \ndimension (b) 0.6 0.4 0.2 0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 Point of the second schedule \ndimension, first dimension fixed (c) Figure 5. Performance Distribution of compress-dct, AMD Athlon. \nGA discovers the maximum performance improvement available in the search space. to traverse a search \nspace of af.ne schedules for automatic paral\u00adlelization. In addition, Long and O Boyle [29] considered \na search space of transformation sequences represented in the UTF frame\u00adwork [21]. Both of these approaches \nsuffer from under-constraining the search space by considering all possible schedules, including il\u00adlegal \nones. Downstream .ltering approaches do not scale, due to the exponentially diminishing proportion of \nlegal schedules with re\u00adspect to the program size.For instance, Nisbet obtains only3 - 5% of legal schedules \nfor the ADI benchmark (6 statements). More\u00adover, under-constraining the search space limits the possibility \nto narrow the search to the most promising subspaces. Pouchet et al. demonstrated a more ef.cient approach, \nby em\u00adbedding program dependences and af.ne scheduling properties into the search space itself. However, \nthis approach was only applied to small kernels with single dimensional schedules. We remove the expressiveness \nlimitations of this prior result, extending the search space construction, preconditioning and traversal \nalgorithms to ar\u00adbitrary multidimensional af.ne schedules. 7. Conclusion Present day compilersfail to \nmodel the complex interplay between different optimizations and their effect on all the different processor \narchitecture components. Also, the complexity of current hardware has made it impossible for compilers \nto accurately model archi\u00adtectures analytically. Thus, empirical search has become essential to achieve \nportable high performance. Most iterative compilation techniques target compiler optimization .ags, parameters, \ndecision heuristics, or phase ordering [8, 13, 40, 24, 41, 1].We takea more aggressive stand, aiming \nfor the construction and tuning of complex sequences of transformations. Af.ne schedules build a very \nexpressive search space, since a single schedule can represent an arbitrarily complex sequence of loop \ntransformations. The .rst attempts to traverse such a space faced legality problems and showed poor results \nbecause only few legal af.ne schedules were found [30, 29]. Pouchet et al. recently proposed a solution \nfor a restricted class of loop nests and trans\u00adformations [34]. This paper targets all static control \nprograms and, by construction, enables iterative optimization in a closed space of semantics-preserving \ntransformations. To overcome the combina\u00adtorial nature of the optimization search space, we designed \nheuris\u00adtics and a genetic algorithm with specialized operators that leverage the algebraic properties \nof this space, embedding the legality con\u00adstraints into the operators themselves. We simultaneously demon\u00adstrate \ngood performancegains andexcellent convergence speed on huge search spaces, even on larger loop nests \nwhere fully iterative af.ne scheduling has never been attempted before. For future work, we intend to \ndevelop enhanced search space construction strategies with corresponding effective and ef.cient traversal \ntechniques.We willexplore supportingtiling through the embeddingofaf.ne permutability constraints. Also \nwe willexplore targeting the generation of coarse-grain parallel code. Acknowledgments We are extremely \ngrateful to PLDI reviewers for providing extensive and accurate comments on our paper. We thank Marco \nCornero and Erven Rohou from STMicroelectronics for lending the ST231 Traviata board. Finally, we thank \neveryone who provided us with constructive criticism on this piece of work. References [1] F. Agakov, \nE. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P. O Boyle, J. Thomson, M. Toussaint, and C. K. I. \nWilliams. Using machine learning to focus iterative optimization. In IEEE/ACM Intl. Symp. on Code Generation \nand Optimization (CGO 06), pages 295 305,Washington, DC, USA, 2006. IEEE Computer Society. [2] N. Ahmed, \nN. Mateev, and K. Pingali. Tiling imperfectly-nested loop nests. In ACM/IEEE Conf. on Supercomputing \n(SC 00), Dallas, TX, USA, Nov. 2000. [3] J. Allen and K. Kennedy. Optimizing Compilers for Modern Architectures. \nMorgan Kaufmann Publishers, 2002. [4] D. Barthou, J.-F. Collard, and P. Feautrier. Fuzzy array data.ow \nanalysis. J. of Parallel and Distributed Computing, 40:210 226, 1997. [5] C. Bastoul. Code generation \nin the polyhedral model is easier than you think. In IEEE Intl. Conf. on Parallel Architectures and Compilation \nTechniques (PACT 04), pages 7 16, Juan-les-Pins, France, Sept. 2004. [6] C. Bastoul andP. Feautrier. \nImproving data localityby chunking. In Intl. Conf. on Compiler Construction (ETAPS CC 12), volume 2622, \npages 320 335,Warsaw, Poland, Apr. 2003. [7] A. Bernstein. Analysis of programs for parallel processing. \nIEEE Trans. on Electronic Computers, 15(5):757 763, Oct. 1966. [8] F. Bodin, T. Kisuki, P. M. W. Knijnenburg, \nM. F. P. O Boyle, and E. Rohou. Iterative compilation in a non-linear optimisation space. In W. on Pro.le \nandFeedback Directed Compilation,Paris, Oct. 1998. [9] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, \nJ. Ramanujam, A. Rountev, and P. Sadayappan. Automatic transformations for communication-minimized parallelization \nand locality optimization in the polyhedral model. In Intl. Conf. on Compiler Construction (ETAPS CC \n17), Budapest, Hungary, Apr. 2008. [10] U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A \npractical automatic polyhedral parallelization and locality optimiza\u00adtion system. In ACM SIGPLAN Conf. \non Programming Languages Design and Implementation (PLDI 08), Tucson, AZ, USA, June 2008. [11] K. D. \nCooper, A. Grosul, T. J. Harvey, S. Reeves, D. Subramanian, L. Torczon, and T. Waterman. ACME: adaptive \ncompilation made ef.cient. In ACM SIGLPAN/SIGBED Conf. on Languages, Compilers, andTools for Embedded \nSystems (LCTES 05), pages 69 77, Chicago, IL, USA, 2005.ACM Press. [12] K. D. Cooper, P. J. Schielke, \nand D. Subramanian. Optimizing for reduced code space using genetic algorithms. In Workshop on Languages, \nCompilers, and Tools for Embedded Systems, pages 1 9, Atlanta, GA, USA, July 1999.ACM Press. [13] K. \nD. Cooper, D. Subramanian, and L.Torczon. Adaptive optimizing compilers for the 21st century. J. Supercomputing, \n23(1):7 22, 2002. [14] A. Darte, Y. Robert, and F. Vivien. Scheduling and Automatic Parallelization. \nBirkhauser, 2000. [15] P. Feautrier. Parametric integer programming. RAIRO Recherche Op\u00b4erationnelle, \n22(3):243 268, 1988. [16] P. Feautrier. Some ef.cient solutions to the af.ne scheduling problem, part \nI: one dimensional time. Intl. J. of Parallel Programming, 21(5):313 348, Oct. 1992. [17] P. Feautrier. \nSome ef.cient solutions to the af.ne scheduling problem, part II: multidimensional time. Intl. J. of \nParallel Programming, 21(6):389 420, Dec. 1992. [18] S. Girbal, N.Vasilache, C. Bastoul, A. Cohen, D.Parello, \nM. Sigler, and O.Temam. Semi-automatic composition of loop transformations for deep parallelism and memory \nhierarchies. Intl. J. of Parallel Programming, 34(3), 2006. [19] D. E. Goldberg. Genetic Algorithms in \nSearch, Optimization and Machine Learning. Addison-Wesley Longman Publishing Co. Inc., Boston, MA, USA, \n1989. [20] M. Haneda,P. M.W. Knijnenburg, and H. A. G.Wijshoff. Automatic selection of compiler options \nusing non-parametric inferential statistics. In IEEE Intl. Conf. on Parallel Architectures and Compilation \nTechniques (PACT 05), pages 123 132, Saint Louis, MO, USA, 2005. IEEE Computer Society. [21] W.Kelly. \nOptimization withina Uni.edTransformationFramework. PhD thesis, Univ. of Maryland, 1996. [22] W. Kelly, \nW. Pugh, and E. Rosser. Code generation for multiple mappings. In Intl. Symp. on the frontiers of massively \nparallel computation, pages 332 341, McLean,VA, USA, Feb. 1995. [23] T. Kisuki, P. M. W. Knijnenburg, \nand M. F. P. O Boyle. Combined selection of tile sizes and unroll factors using iterative compilation. \nIn IEEE Intl. Conf. on Parallel Architectures and Compilation Techniques (PACT 00), pages 237 246, Philadelphia,PA, \nUSA, 2000. IEEE Computer Society. [24] P. A. Kulkarni, S. R. Hines, D. B. Whalley, J. D. Hiser, J. W. \nDavidson, and D. L. Jones. Fast and ef.cient searches for effective optimization-phase sequences. ACMTrans. \non Architecture and Code Optimization, 2(2):165 198, 2005. [25] M. Le Fur. Scanning parameterized polyhedron \nusing Fourier-Motzkin elimination. Concurrency -Practice and Experience, 8(6):445 460, 1996. [26] C. \nLee. UTDSP benchmark suite, 1998. http://www.eecg.toronto.edu/ corinna/DSP. [27] A.W. Lim and M. S. Lam. \nMaximizing parallelism and minimizing synchronization with af.ne transforms. In ACM Symp. on Principles \nof Programming Languages (PoPL 97), pages 201 214,Paris, France, 1997.ACM Press. [28] S. Long and G. \nFursin. Systematic search within an optimisation space based on uni.ed transformation framework. IJCSE \nIntl. J. of Computational Science and Engineering, 2006. [29] S. Long and M. O Boyle. Adaptive Java optimisation \nusing instance\u00adbased learning. In ACM Intl. Conf. on Supercomputing (ICS 04), pages 237 246, Saint-Malo, \nFrance, June 2004. [30] A. Nisbet. GAPS: A compiler framework for genetic algorithm (GA) optimised parallelisation. \nIn HPCN Europe 1998: Proc. of the Intl. Conf. and Exhibition on High-Performance Computing and Networking, \npages 987 989, London, UK, 1998. Springer-Verlag. [31] M.Palkovic..Enhanced Applicability of LoopTransformations. \nPhD thesis,T. U. Eindhoven, The Netherlands, Sept. 2007. [32] S. Pop, A. Cohen, C. Bastoul, S. Girbal, \nP. Jouvelot, G.-A. Silber, and N. Vasilache. GRAPHITE: Loop optimizations based on the polyhedral model \nfor GCC. In Proc. of the 4th GCC Developper s Summit, Ottawa, Canada, June 2006. [33] L.-N. Pouchet, \nC. Bastoul, J. Cavazos, and A. Cohen. A note on the performance distribution of af.ne schedules. 2nd \nWorkshop on Statistical and Machine learning approaches to ARchitectures and compilaTion (SMART 08),G\u00a8oteborg, \nSweden, Jan. 2008. [34] L.-N. Pouchet, C. Bastoul, A. Cohen, and N. Vasilache. Iterative optimization \nin the polyhedral model: Part I, one-dimensional time. In IEEE/ACM Intl. Symp. on Code Generation and \nOptimization (CGO 07), pages 144 156, San Jose, CA, USA, Mar. 2007. [35] W. Pugh. The Omega test: a fast \nand practical integer programming algorithm for dependence analysis. In ACM Intl. Conf. on Supercom\u00adputing \n(ICS 91), pages 4 13, Albuquerque, NM, USA, Aug. 1991. [36] M.P \u00a8uschel, B. Singer, J. Xiong, J. Moura, \nJ. Johnson, D. Padua, M. Veloso, and R. W. Johnson. SPIRAL: A generator for platform\u00adadapted libraries \nof signal processing algorithms. J. of High Performance Computing and Applications, special issue onAutomatic \nPerformanceTuning, 18(1):21 45, 2004. [37] F. Quiller\u00b4 Generation of ef.cient e, S. Rajopadhye, and D. \nWilde. nested loops from polyhedra. Intl. J. of Parallel Programming, 28(5):469 498, Oct. 2000. [38] \nL. Renganarayanan, D. Kim, S. Rajopadhye, and M. M. Strout. Parameterized tiled loops for free. SIGPLAN \nNotices, Proc. of the 2007 PLDI Conf., 42(6):405 414, 2007. [39] A. Schrijver. Theory of Linear and Integer \nProgramming. JohnWiley &#38; Sons, 1986. [40] M. Stephenson, S. Amarasinghe, M. Martin, and U.-M. O Reilly. \nMeta optimization: improving compiler heuristics with machine learning. SIGPLAN Notices, 38(5):77 90, \n2003. [41] S. Triantafyllis, M. Vachharajani, and D. I. August. Compiler optimization-space exploration. \nIn J. of Instruction-levelParallelism, volume 7, Jan. 2005. [42] N.Vasilache, C. Bastoul, and A. Cohen. \nPolyhedral code generation in the real world. In Proc. of the Intl. Conf. on Compiler Construction (ETAPS \nCC 16),volume 3923, pages 185 201,Vienna, Austria, Mar. 2006. Springer-Verlag. [43] N.Vasilache, A. Cohen, \nand L.-N. Pouchet. Automatic correction of loop transformations. In IEEE Intl. Conf. on Parallel Architectures \nand Compilation Techniques (PACT 07), pages 292 302, Brasov, Romania, Sept. 2007. [44] F.Vivien. On the \noptimality of Feautrier s scheduling algorithm. In Intl. Euro-Par Conf. on Parallel Processing (EURO-PAR \n02), pages 299 308, London, UK, 2002. Springer-Verlag. [45] D. K. Wilde. A library for doing polyhedral \noperations. Technical Report 785, IRISA, Rennes, France, 1993. [46] M. Wolfe. High performance compilers \nfor parallel computing. Addison-WesleyPublishing Company, 1995. [47] J. Xue. Transformations of nested \nloops with non-convex iteration spaces. Parallel Computing, 22(3):339 368, 1996.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>High-level loop optimizations are necessary to achieve good performance over a wide variety of processors. Their performance impact can be significant because they involve in-depth program transformations that aim to sustain a balanced workload over the computational, storage, and communication resources of the target architecture. Therefore, it is mandatory that the compiler accurately models the target architecture as well as the effects of complex code restructuring.</p> <p>However, because optimizing compilers (1) use simplistic performance models that abstract away many of the complexities of modern architectures, (2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of transformation sequences, they typically uncover only a fraction of the peak performance available on many applications. We propose a complete iterative framework to address these issues. We rely on the polyhedral model to construct and traverse a large and expressive search space. This space encompasses only legal, distinct versions resulting from the restructuring of any static control loop nest. We first propose a feedback-driven iterative heuristic tailored to the search space properties of the polyhedral model. Though, it quickly converges to good solutions for small kernels, larger benchmarks containing higher dimensional spaces are more challenging and our heuristic misses opportunities for significant performance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators that leverage the polyhedral representation of program dependences. We provide experimental evidence that the genetic algorithm effectively traverses huge optimization spaces, achieving good performance improvements on large loop nests.</p>", "authors": [{"name": "Louis-No&#235;l Pouchet", "author_profile_id": "81330496337", "affiliation": "ALCHEMY Group, INRIA Saclay -- Ile-de-France and Paris-Sud University, Orsay, France", "person_id": "P1022750", "email_address": "", "orcid_id": ""}, {"name": "C&#233;dric Bastoul", "author_profile_id": "81320487841", "affiliation": "ALCHEMY Group, INRIA Saclay -- Ile-de-France and Paris-Sud University, Orsay, France", "person_id": "P1022751", "email_address": "", "orcid_id": ""}, {"name": "Albert Cohen", "author_profile_id": "81100146104", "affiliation": "ALCHEMY Group, INRIA Saclay -- Ile-de-France and Paris-Sud University, Orsay, France", "person_id": "P1022752", "email_address": "", "orcid_id": ""}, {"name": "John Cavazos", "author_profile_id": "81100096445", "affiliation": "Dept. of Computer & Information Sciences, University of Delaware, Newark, DE, USA", "person_id": "P1022753", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375594", "year": "2008", "article_id": "1375594", "conference": "PLDI", "title": "Iterative optimization in the polyhedral model: part ii, multidimensional time", "url": "http://dl.acm.org/citation.cfm?id=1375594"}