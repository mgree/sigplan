{"article_publication_date": "06-07-2008", "fulltext": "\n APracticalAutomaticPolyhedralParallelizer and Locality Optimizer Uday Bondhugula1 Albert Hartono1 1Dept. \nof Computer Science and Engineering The Ohio State University {bondhugu,hartonoa,saday}@cse.ohio-state.edu \nAbstract We presentthe designand implementationofan automatic polyhe\u00addral source-to-source transformation \nframework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism \nand locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic \ntrans\u00adformationinthe polyhedralmodel farbeyondwhatispossibleby current production compilers. Unlikepreviousworks, \nour approach is an end-to-end fully automatic one drivenby an integer linear op\u00adtimization framework \nthat takes an explicit view of .nding good ways of tiling for parallelism and locality using af.ne transforma\u00adtions.The \nframeworkhasbeen implementedintoatoolto automat\u00adically generate OpenMP parallel code from C program sections. \nExperimental results from the tool show very high speedups for lo\u00adcaland parallelexecutionon multi-coresover \nstate-of-the-art com\u00adpiler frameworks from the research community as well as the best native production \ncompilers. The system also enables the easy use of powerful empirical/iterative optimization for general \narbitrarily nested loop sequences. Categories and Subject Descriptors D.3.4[Programming Lan\u00adguages]: \nProcessors Compilers, Optimization, Code generation General Terms Algorithms, Design, Experimentation, \nPerfor\u00admance Keywords Automatic parallelization, Locality optimization, Poly\u00adhedral model,Loop transformations,Af.ne \ntransformations,Tiling 1. Introduction and Motivation Current trends in microarchitecture are increasingly \ntowards larger number of processing elements on a single chip. This has made parallelism and multi-core \nprocessors mainstream. The dif.culty of programming these architectures to effectively tap the potential \nof multiple on-chip processing units is a signi.cant challenge. Among several approaches to addressing \nthis issue, one that is very promising but simultaneously very challenging is automatic parallelization. \nThis requires no effort on part of the programmer in the process of parallelization and optimization \nand is therefore very attractive. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 08, June 7 13, 2008,Tucson, Arizona, USA. Copyright c &#38;#169; 2008ACM \n978-1-59593-860-2/08/06... $5.00 J. Ramanujam2 P. Sadayappan1 2Dept.of Electrical&#38;Computer Engineering&#38;CCT \nLouisiana State University jxr@ece.lsu.edu Manycompute-intensive applications often spend most of their \nexecution time in nested loops. This is particularly common in sci\u00adenti.c and engineering applications. \nThe polyhedral model pro\u00advides a powerful abstraction to reason about transformations on such loop nests \nby viewing a dynamic instance (iteration) of each statement as an integer point in a well-de.ned space \ncalled the statement s polyhedron.With such a representation for each state\u00adment and a precise characterization \nof inter or intra-statement de\u00adpendences, it is possible to reason about the correctness of complex loop \ntransformations in a completely mathematical setting relying on machinery from linear algebra and linear \nprogramming. The transformations .nally re.ect in the generated code as reordered execution with improved \ncache locality and/or loops that havebeen parallelized. The polyhedral model is applicable to loop nests \nin which the data access functions and loop bounds are af.ne combi\u00adnations (linear combination with a \nconstant) of the enclosing loop variables and parameters. While a precise characterization of data dependences \nis feasible for programs with static control structure and af.ne references/loop-bounds, codes with non-af.ne \narray ac\u00adcess functions or code with dynamic control can also be handled, but only with conservative \nassumptions on some dependences. The task of program optimization (often for parallelism and lo\u00adcality) \nin the polyhedral model may be viewed in terms of three phases: (1) static dependence analysis of the \ninput program, (2) transformations in the polyhedral abstraction, and (3) generation of code for the \ntransformed program. Signi.cant advances were made in the past decade on dependence analysis [19, 18, \n46] and code generation[31,25]inthe polyhedralmodel,butthe approachessuf\u00adfered from scalability challenges. \nRecent advances in dependence analysis and more importantly in code generation [47, 6, 55, 54] have solved \nmany of these problems resulting in the polyhedral techniques being applied to code representative of \nreal applications likethe spec2000fp benchmarks [14, 22]. These advances havealso made the polyhedral \nmodel practical in production compiler con\u00adtexts [43] as a .exible and powerful representation to compose \nand apply transformations. Thekeymissing step has been the demon\u00adstration of a scalable and practical \napproach for automatic trans\u00adformation for parallelization and locality. Our work addresses this by developing \na compiler, based on the theoretical framework we previously proposed [9], to enable end-to-end fully \nautomatic par\u00adallelization and locality optimization. Tiling[28,58,61]isakeytransformationin optimizingforpar\u00adallelism \nand data locality. There has been a considerable amount of research into these two transformations. Tiling \nhas been stud\u00adied from two perspectives data locality optimization and paral\u00adlelization. Tiling for \nlocality requires grouping points in an iter\u00adation space into smaller blocks (tiles) allowing reuse in \nmultiple directionswhentheblock.tsinafaster memory(registers,L1,or L2 cache).Tiling for coarse-grained \nparallelisminvolves partition\u00ad for (i=0; i<N; i++) for (j=0; j<N; j++)  c1 S1: A[i,j] = A[i,j]+u1[i]*v1[j] \n+ u2[i]*v2[j]; c2 c3 for (k=0; k<N; k++) for (l=0; l<N; l++) Data dependence graph S1 S2 i 0 1 0 j 1 \n0 0 const 0 0 0 k 1 0 0 l 0 1 0 const 0 0 1 parallel fwd dep scalar Statement-wise transformation S2: \nx[k] = x[k]+A[l,k]*y[l]; 3 21 0 10 0 000 i = 0 Original code BBBBB@ 0 -10 01 -1 00 1 000 666664 CCCCCA \nj k l N 777775 = 0 for (c1=0; c1<N; c1++) 1 01 0 = 0 for (c2=0; c2<N; c2++) 1 000 i N B@ 00 0 -11 -1 \n = 0 A[c2,c1] = A[c2,c1]+u[c2]*v[c1]; B@ 0 100 CA j CA = 0 10 0 -1 00 =0 x[c1] =x[c1]+A[c2,c1]*y[c2]; \n-101 -1 01 -1000 1=00 -11 -1 1 Transformed code Dependence polyhedron for S1.S2 edge Domain of S1 Figure \n1. Polyhedral representation ing the iteration space into tiles that may be concurrently executed on \ndifferent processors with a reduced frequency and volume of inter-processor communication: a tile is \natomically executed on a processor with communication required only before and after exe\u00adcution. One \nof thekeyaspects of our transformation framework is to .nd good ways of performing tiling. Existing automatic \ntransformation frameworks [37, 36, 35, 24] haveone or more drawbacks or restrictions that limit their \neffective\u00adness.Acommon signi.cant problemis thelackofa realistic cost function to choose among the large \nspace of legal transformations that are suitable for coarse-grained parallel execution, as is used in \npractice with manually developed/optimized parallel applications. Most previously proposed approaches \nalso do not consider locality and parallelism together. Comprehensive performance evaluation on parallel \ntargets using a range of test cases has not been done using a powerful and general model like the polyhedral \nmodel. This paper presents the end-to-end design and implementation of PLuTo [1],a parallelization and \nlocality optimization tool. Find\u00ading good ways to tile for parallelism and locality directly through \nan af.ne transformation framework is the central idea. Our ap\u00adproach is thus a departure from scheduling-based \napproaches in this .eld [20, 21, 17, 24, 15] as well as partitioning-based ap\u00adproaches [37, 36, 35] (due \nto incorporation of more concrete op\u00adtimization criteria), however, is built on the same mathematical \nfoundationsand machinery.Weshowhowtiledcode generationfor statement domains of arbitrary dimensionalities \nunder statement\u00adwise af.ne transformations is done for local and shared mem\u00adory parallelexecution.Weevaluate \nthe performanceof the imple\u00admented system on a multicore processor using a number of appli\u00adcationkernels \nthat are non-trivial for anyexisting auto-parallelizer. Model-driven empirical optimization and automatic \ntuning ap\u00adproaches (e.g., ATLAS) have been shown to be very effective in optimizing single-processorexecution \nfor some regularkernels like matrix-matrix multiplication [56, 63]. There is considerable inter\u00adest in \ndeveloping effective empirical tuning approaches for arbi\u00adtrary inputkernels. Our framework can enable \nsuch model-driven or guided empirical search to be applied to arbitrary af.ne pro\u00adgrams, in the context \nof both sequential and parallel execution. Also, since our transformation system operates entirely in \nthe poly\u00adhedral abstraction, it is not just limited toC orFortran code,but could accept any high-level \nlanguage from which polyhedral do\u00admains can be extracted and analyzed. Therestofthispaperisorganizedasfollows. \nSection3provides an overview of our theoretical framework for automatic transfor\u00admation that we proposed \nin [9]. Section 4 and Section 5 discuss some considerations in the design and techniques for generation \nof ef.cient tiled shared memory parallel code from transformations found. Section6describes the implemented \nsystem. Section7pro\u00advides experimental results. Section 8 discusses related work and conclusions are \npresented in Section 9. 2. Background and Notation This section provides background on the polyhedral \nmodel. All row vectors are typeset in bold. 2.1 ThePolyhedral model DEFINITION 1 (Af.ne Hyperplane). \nThe set X of all vectors x . Zn suchthat h.xx = k, for k . Z, is an af.ne hyperplane. In otherwords,ahyperplaneisa \nhigher dimensional analogof a (2-d) plane in three-dimensional space. The set of parallel hyper\u00adplane \ninstances corresponding to different values of k is character\u00adizedbythevector xh whichis normaltothehyperplane.Twovectors \nx 1 and x 2 liein the samehyperplaneif h.x 1 = h.x 2. DEFINITION 2 (Polyhedron). The set of all vectors \nxx . Zn such that Axx + xb = 0, where A is an integer matrix, de.nes a (convex) integer polyhedron.Apolytopeisa \nbounded polyhedron. Polyhedral representation of programs. Given a program, each dynamic instanceofastatement, \nS,is de.nedbyitsiterationvector x i which contains values for the indices of the loops surrounding S, \nfrom outermost to innermost. Whenever the loop bounds are linear combinations of outer loop indices and \nprogram parameters (typically, symbolic constants representing problem sizes), the set of iteration vectors \nbelonging to a statement de.ne a polytope. Let DS represent the polytope and its dimensionality be mS. \nLet xp be the vector of program parameters. Polyhedral dependences. Our dependence model is of exact \naf.ne dependences and same as the one used in [20, 36, 14, 45]. Dependences are determined precisely \nthrough data.ow analy\u00adsis [19], but we consider all dependences including anti (write\u00adafter-read), output \n(write-after-write) and input (read-after-read) dependences, i.e., input code does not require conversion \nto single\u00adassignmentform.TheData DependenceGraph(DDG)isadirected multi-graph with eachvertexrepresentingastatement, \nand an edge, e . E, from node Si to Sj representing a polyhedral dependence from a dynamic instance of \nSi to one of Sj : it is characterized by a polyhedron, Pe, called the dependence polyhedron that cap\u00adtures \nthe exact dependence information corresponding to e. The dependence polyhedronisin the sumof the dimensionalitiesof \nthe source and target statement s polyhedra (with dimensions for pro\u00adgram parameters as well). Let xs \nrepresent the source iteration and xt bethetarget iteration pertainingtoadependenceedge e. It is possi\u00adble \nto express the source iteration as an af.ne function of the target iteration, i.e., to .nd the last con.icting \naccess. This af.ne func\u00adtion is also known as the h-transformation, and will be represented by he for \na dependence edge e. Hence, xs = he(xt). The equalities corresponding to the h-transformation are a part \nof the dependence polyhedron and can be used to reduce its dimensionality. Figure1 shows the polyhedral \nrepresentation of a simple code. Let S1, S2, ... , Sn be the statements of the program.A one\u00addimensional \naf.ne transform for statement Sk is de.ned by: hi ` \u00b4 fsk (xi)= c1 ... cmSk xi + c0,ci . Z (1) fSk canalsobe \ncalledanaf.nehyperplane,ora scattering function when dealing with the code generator.Amulti-dimensional \naf.ne transformation fora statementis representedbya matrix with each row being anaf.nehyperplane. DEFINITION \n3 (Dependence satisfaction). An af.ne dependence with polyhedron Pe is satis.ed at a level l iffthe following \ncondi\u00adtion is satis.ed: `\u00b4 .k(1 = k = l - 1) : fk xt - fk (xs) = 0, (xs,xt).Pe sj si `\u00b4 x and fl t - \nfl (xs) = 1, (xs,xt).Pe sj si 3. OverviewofAutomaticTransformation Approach In this section, we give \nan overview of our theoretical framework for automatic transformation. Complete details on the theory \nare available in another report [8]. 3.1 Legality of tiling multiple domains with af.ne dependences LEMMA \n1. Let fsi be a one-dimensional af.ne transform for statement Si.For {fs1 , fs2 , ... , fsk }, to be \na legal (statement\u00adwise) tiling hyperplane, the following should hold for each edge e . E: `\u00b4 fsj xt \n- fsi (xs) = 0, (xs,xt).Pe (2) The above is a generalization of the classic condition proposed by Irigoin \nand Triolet [28] (as hT .R = 0)for the legality of tiling a single domain. The tiling of a statement \ns iteration space by a setofhyperplanesis saidtobelegalif eachtile canbeexecuted atomicallyandavalid \ntotal orderingofthe tiles canbe constructed. This impliesthat thereexistnotwotilessuchthattheybothdepend \non each other. The above is a generalization to multiple iteration domains with af.ne dependences and \nwith possibly different di\u00admensionalities coming from possibly imperfectly nested input. Let {f1 s1 ,fs1 \n2 ,... ,f1 },{f2 s1 ,fs2 2 ,... ,f2 } be twostatement\u00ad sk sk wise 1-d af.ne transforms that satisfy (2). \nThen, {f1 s1 , f1 s2 , ... , f1 sk }, {f2 s1 , fs2 2 , ... , f2 sk } represent rectangularly tilable \nloops in the transformed space.Atile canbe formedby aggregatingagroup of hyperplane instances along f1 \nand f2 . Due to (2), if such a si si tile is executed on a processor, communication would be needed only \nbefore and after its execution. From the point of view of data locality, if such a tile is executed with \nthe associated data .tting in afaster memory, reuse is exploited in multiple directions. Hence, j j j \nany f,f,...,fthat is a solution to (2) represents a com- S1 S2 Sn mon dimension (for all statements) \nin the transformed space with both inter and intra-statement af.ne dependences in the forward direction \nalong it. Partial tiling at any depth. The legality condition as written in (2) is imposed on all dependences. \nHowever, if it is imposed only on dependences that have not been satis.ed up to a certain depth, the \nindependent f s that satisfy the condition represent tilinghy\u00adperplanes at that depth, i.e., tiling at \nthat level is legal.  3.2 Cost function, bounding approach and minimization Consider the following af.ne \nform de: de(xs,xt)= fsj (xt) - fsi (xs), (xs,xt).Pe (3) The af.ne form de(xs,xt) is very signi.cant. \nThis function is the number of hyperplanes the dependence e traverses along the hy\u00adperplane normal f. \nIf f is used as a space loop to generate tiles for parallelization, this functionisafactorin the communication \nvolume. On the other hand, if f is used as a sequential loop, it gives us a measure of the reuse distance. \nAn upper bound on this functionwould meanthatthe numberofhyperplanesthatwouldbe communicated as a result \nof the dependence at the tile boundaries would not exceed the bound, the same for cache misses at L1/L2 \ntile edges,orL1 cache loadsforaregistertile.Of particular interest is,ifthis functioncanbe reducedtoaconstant \namountor zero(free of a parametric component) by choosing a suitable direction for f: if this is possible, \nthen that particular dependence leads to constant boundary communication or no communication (respectively) \nfor thishyperplane. An attempt to minimize the above cost function ends up in an objective non-linearinloopvariablesandhyperplane \ncoef.cients. For example,f(xt) - f(xs) could be c1i +(c2 - c3)j, under 1 = i = N, 1 = j = N, i = j. Sucha \nform results whena dependence is not uniform or for an inter-statement dependence. The dif.culty canbeovercomebyusingabounding \nfunction approachthatallows the applicationofFarkas Lemma[20,51]and castingthe objective intoanILP formulation. \nSincetheloopvariables themselves canbe boundedbyaf.ne functionsof the parameters, one canalways .nd an \naf.ne form in the program parameters, px, that bounds de(xs,xt) for every dependence edge e, i.e., there \nexists v(px)= u.xp + w, such that fsj (xt) - fsi (xs) = v(px), (xs,xt).Pe, .e . E i.e., v(px) - de(xs,xt) \n= 0, (xs,xt).Pe, .e . E (4) Suchabounding function approachwas .rst usedbyFeautrier [20], but for a \ndifferent purpose to .nd minimum latency schedules. Now,Farkas Lemma canbe appliedto(4). me X v(px) \n- de(xs,xt) = .e0 + .ekPek ,.ek = 0 k=1 where Pek isaface of Pe. Coef.cients of each of the iterators \nin xi and parameters in pxon the LHS and RHS can be gathered and equated, to obtain linear equalities \nand inequalities entirely in coef.cients of the af.ne mappings for all statements, components of row \nvector u, and w. The ILP system comprising the tiling legality constraints from (2) and the bounding \nconstraints can be at once solved by .nding a lexicographic minimal solution with xu and w in the leading \nposition. Let u =(u1,u2,...uk). minimize-{u1,u2,...,uk, w, . . . , c.is, . . . } (5) Finding thelexicographic \nminimal solution is within the reach of theSimplexalgorithmandcanbe handledbytheParametricInteger Programming \n(PIP) software [18]. Since the program parameters are quite large, their coef.cients are minimized with \nthe highest priority. The solutiongivesahyperplane for each statement. Note thetrivial zero solutionisavoidedbymakingapractical \nchoicethat is described in the next section. Iteratively .nding independent solutions. Solving the ILP \nfor\u00admulation in the previous section gives us a single solution to the coef.cients of the best mappings \nfor each statement. We need at least as manyindependent solutions (fora statement) as the dimen\u00adsionality \nof its domain. Hence, once a solution is found, we aug\u00adment the ILP formulation with new constraints \nthat make sure of linear independence with solutions already found. This is done by constructing the \northogonal sub-space [40, 34] of the transforma\u00adtion rows found sofar(HS)and forcing a non-zero component \nin HS . for the next solution. -1 . TT HS = I - HS HS HS HS (6) Linearly independent (statement-wise) \nhyperplanes are found iteratively till all dependences are satis.ed. Dependences from pre\u00adviously foundhyperplanes \nare not removed as independent tiling hyperplanes are found unless theyhaveto be to allowthe next band \nof tilinghyperplanestobe found. Maximal setsof fully permutable loops are found like in the case of [59, \n16, 36], however, with a optimization criterion (5) that goes beyond maximum degrees of parallelism. \nOuter space and inner time: communication and locality opti\u00admization uni.ed The best possible solution \nto (5) is with(u = 0,w =0), whichisahyperplane thathasno dependence compo\u00adnents along its normal this \nis a fully parallel loop requiring no synchronization if at the outer level(outer parallel), or an inner \nparallel one if some dependences were removed previously and so a synchronization is required after the \nloop is executed in parallel. Thus,in eachof the steps that we .nda new independenthyper\u00adplane, we endup \n.rst .nding all synchronization-freehyperplanes when they exist; these are followedbya setofhyperplanes \nrequir\u00ading constant boundary communication(u = 0; w> 0). In the worst case, we have a hyperplane with \nu> 0,w = 0 resulting in long communication from non-constant dependences; such solu\u00adtions are found last. \nFrom the point of view of data locality, since the samehyperplanesusedtoscanthetilespace scanpointsinatile, \ncache misses at tile boundaries (that are equivalent to communica\u00adtion along processor tile boundaries) \nare minimized.Byminimizing f(xt) - f(xs) as we .ndhyperplanes from outermost to innermost, we push dependence \nsatisfaction to inner loops, at the same time ensuring that the new loops have non-negative dependence \ncompo\u00adnents (to the extent possible) so that they can be tiled for locality and pipelined parallelism \ncan be extracted if (forward) space de\u00adpendences exist. If the outer loops are used as space (how many \never desired, say k), and the rest are used as time, communication in the processor space is minimal \nas the outer space loops are the k best ones. Whenever the loops are tiled, they result in coarse\u00adgrained \nparallelism as well as better reuse within a tile. Fusion. Fusion across multiple iteration spaces that \nare weakly connected, as in sequences of producer-consumer loops is also en\u00adabled. Since the hyperplanes \ndo not include coef.cients for pro\u00adgram parameters(1),asolution found correspondstoa.ne-grained interleaving \nof different statement instances at that level [8]. 4. More design considerations Inthis section,we discussafew \nenhancementstothe frameworkas well as some practical choices for scalability. 4.1 Handling input dependences \nInput dependences need to be considered for optimization in many cases as reuse can be exploited by minimizing \nthem. Clearly, le\u00adgality (ordering between dependent RAR iterations) need not be preserved.We thus do \nnot add legality constraints (2) for such de\u00adpendences,but consider them for the bounding objective function \n(4). Since input dependences can be allowed to havenegativecom\u00adponents in the transformed space, they \nneed to be bounded from both above and below.Forevery, PeR corresponding to a input de\u00adpendence, we have \nthe constraints: . `\u00b4 . .fsj xt - fsi (xs). = v(px), (xs,xt).PeR `\u00b4 x i.e., fsj t - fsi (xs) = v(px), \n(xs,xt).PeR , `\u00b4 and (xs) - fsj xt = v(px), (xs,xt).PR fsi e  4.2 Avoiding combinatorial explosion There \naretwosituations when thereisapossibilityof combinatorial explosion (with the number of statements) if \nreasonable choices are not made. 1.Avoidingthetrivial zerovector solutiontothehyperplanes 2. Construction \nof linearly independent sub-space for each state\u00adment s transformation Removing the trivial zero solution \nto (5) on a per-statement basis leads to a non-convex space, and in this case a union of a large number \nof convex spaces each of which has to be tried. Similarly, while constructing a linearly independent \nsub-space for each state\u00adment, there are several choices for each statement and the number of choicestobeexhaustively \ntriedwillbea productofall these[8]. The above dif.culties can be solved at once by only looking for non-negative \ntransformation coef.cients. Then, the zero solution P can be avoided with the constraint of ci = 1, \n1 = i = mSk , for each statement Sk. Doing so mainly excludes transformations that include loop reversal, \nand in practice, we do not .nd this to bea concernatall.The current implementationofPluto[1]iswith this \nchoice, and scales very well without loss of good transforma\u00adtions. Exploring all possible choices if \none wishes is still possible for arounduptoten statementswhilekeeptherunningtime within a few tens of \nseconds. 5. Tiled code generationfor arbitrarily-nested loops under statement-wise transformations In \nthis section, we describe how tiled code is generated from trans\u00adformations foundin the previous section. \nThisisakeystepin gen\u00aderation of high performance code. We.rstgiveabrief descriptionofthe polyhedralcode \ngenerator CLooG [13, 6]. CLooG can scan a union of polyhedra, and option\u00adally,underanewgloballexicographic \nordering speci.edas through scattering functions. Scattering functions are speci.ed statement\u00adwise, and \nthe legality of scanning the polyhedron with these di\u00admensions in the speci.ed order should be guaranteed \nby the spec\u00adi.er in our case, an automatic transformation system. The code generator does not have any \ninformation on the dependences and hence, in the absence of any scattering functions would scan the union \nof the statement polyhedra in the global lexicographic or\u00adder of the original iterators (statement instances \nare interleaved). CLooG uses PolyLib [57, 42] (which in turn uses the Chernikova algorithm[33])forits \ncore operations,andthecode generatedisfar moreef.cient than thatby older code generators based onFourier-Motzkinvariable \nelimination like Omega Codegen [46] or LooPo s internal code generator [25, 24]). Also, code generation \ntime and memory utilization are much lower [6]. Such a powerful and ef.\u00adcient code generator is essential \nin conjunction with the transfor\u00admation framework we develop, since the statement-wise transfor\u00admations \nfound when coupled with tiling lead to complex execution reordering. This is especially so for imperfectly \nnested loops and generation of parallel code, as will be seen in the rest of this paper. 5.1 Tiling the \ntransformed AST vs.Tiling the scattering functions Before proceeding further, we differentiate between \nusing the term tiling for, (1) modeling and enabling tiling through a transforma\u00adtion framework (as was \ndescribed in the previous section), (2) .nal generationof tiledcode from thehyperplanes found. Both are \ngen\u00aderally referred to as tiling. Our approach models tiling in the trans\u00adformation framework by .nding \naf.ne transformations that make rectangular tiling in the transformed space legal. Thehyperplanes found \nare the new basis for the loops in the transformed space and have special properties that have been detected \nwhen the transfor\u00admation is found e.g. being parallel, sequential or belonging to a band of loops that \ncan now be rectangularly tiled. Hence, the trans\u00adformation framework guarantees legality of rectangular \ntiling in the new space. The .nal generation of tiled loops can be done in two ways broadly, (1) directly \nthrough the polyhedral code generator itself in one pass itself, or (2) as a post-pass on the abstract \nsyntax tree generated after applying the transformation. Each has its merits and both can be combined \ntoo. For transformations that possibly lead to imperfectly nested code, polyhedral tiling is a natural \nway to get tiled code from the code generator in one pass guaranteeing legality. Consider the code inFigure3(a)forexample.Ifcodeis \ngeneratedbyjust applyingthe transformation .rst, we get code shown in Figure 3(b). Even though the transformation \nframework obtained two tilinghyperplanes, the transformedcodeinFigure3(b)hasno2-d perfectly nestedkernel. \nDoing a simple unroll-jam of the imperfect loop nest is illegal in this case; hence, straightforward \n2-d syntactic tiling violates dependences. The legality of syntactic tiling or unroll/jam (for register \ntiling)of suchloops cannotbe reasoned aboutin the target AST easily since once we obtain the transformed \ncode, we are outside of the polyhedral model, unless advanced techniques like re-entrance are used. Even \nwhen re-entrance is used to reason about legality through dependence analysis on the target AST, such \nan approach would miss ways of tiling that are possible by reasoning about the obtained tiling hyperplanes \non original domains itself we describe an approach to accomplish the latter which is the subjectof Section5.Forexample,forthecodeinFigure3,2-dtiled \ncode can be generated in one pass, both applying the transformation as well as accomplishing tiling. \n 5.2 Tiles under a transformation Our approach to tiling is to specify a modi.ed higher dimensional \ndomain and specify transformations for what would be the tile space loops in the transformed space. Consider \na very simple ex\u00adample: a two-dimensional loop nest with original iterators: i and j. Let the transformation \nfound be c1 = i, and c2 = i + j, with c1, c2 constitutinga permutable band; hence, theycanbe blocked \nleading to2-d tiles.Wewouldliketo obtaintargetcodethatistiled rectan\u00adgularly along c1 and c2. The domain \nsupplied to the code generator is a higher dimensional domain with the tile shape constraints like that \nproposed by Ancourt and Irigoin [4]; but the scatterings are duplicated for the tile space too. T subscript \nis used to denote the corresponding tile space iterator. The tile space and intra tile loop scattering \nfunctions are speci.ed as follows. Domain Scattering 0 = i = N - 1 c1T = iT 0 = j = N - 1 c2T = iT + \njT 0 = i - 32iT = 31 c1 = i 0 = (i + j) - 32(iT + jT ) = 31 c2 = i + j (c1T ,c2T ,c1,c2) . scatter(iT \n,jT , i, j) c1T and c2T are the tile space loops in the transformed space. This approach can seamlessly \ntile across statements of arbitrary di\u00admensionalities, irrespective of original nesting structure, as \nlong as the cis have dependences (inter-stmt and intra-stmt) in the forward direction this is guaranteed \nand detected by the transformation framework. Algorithm1 Tiling for multiple stmts under transformations \nINPUT Hyperplanes (statement-wise) belonging to a tilable band of width k: fi ,fi+1,...,fi+k-1, expressed \nas af.ne functions of corre- SS S sponding original iterators, iS ; Original domains: DS; Tile sizes: \nti,ti+1,...,ti+k-1 1: /* Update the domains */ 2: foreach statementS do 3: foreach fj = fj (iS)+ f0 \ndo S 4: Increase the domain(DS )dimensionalitybycreating supernodes for all original iterators that \nappear in fj S 5: Let the supernode iterators be iT 6: Add the following two constraints to DS : tj \n* fj (iT S) = fj (iS )+ fj = tj * fj (iT S )+ tj - 1 0 7: endfor 8: endfor 9: /* Update the transformation \nmatrices*/ 10: foreach statement S do 11: Add k new rows to the transformation of S at level i 12: Add \nas manycolumns as the number of supernodes added to DS in Step4 13: foreach fj = fj (iS )+ f0 j , j = \ni, ... , i + k - 1 do S 14: Adda supernode for thishyperplane: fT j = fj (iT S ) S 15: endfor 16: endfor \nOUTPUT Updated domains(DS )and transformations With this, we formally state the algorithm to modify the \norig\u00adinal domain and updating the statement-wise transformations (Al\u00adgorithm 1). The (higher-dimensional) \ntile space loops are referred to as supernodes in the description. For example, in the exam\u00adple above, \niT, jT were supernodes in the original domain, while c1T,c2T are supernodes in the transformed space. \nNote that the transformation matrix computed for each statement has the same number of rows. THEOREM \n1. The set of scattering supernodes, fTSi , fTSi+1 , ... , fTSi+k-1 obtainedfrom Algorithm1satisfy the \ntilinglegality con\u00addition (2) Since, fSj , i = j = i + k - 1 satisfy (2) and since the supern\u00adodes step \nthrough an aggregation of parallelhyperplane instances, dependences continue to be in the forward direction \nfor the scat\u00adtering supernode dimensions too. This holds true for both intra j j j and inter-statement \ndependences. fT ,fT ,...,fT Sn thus rep- S1 S2 resent a common supernode dimension in the transformed \nspace with all af.ne dependences in its forward direction or null-space.. Figure 5.2 shows tiles for \nimperfectly nested 1-d Jacobi. Note that tiling it requires a relative shift of S2 by one and skewing \nthe space loopsbyafactoroftwow.r.t time(as opposedtoskewing byafactorof one thatis required for the space \nmemory-inef.cient perfectly nested version). Example: 3-d tilesfor LU The transformation obtained for \nthe LU decomposition code is: \"# \" #\u00bb \"#\" #\"# c1 10 c1 100 k k S1 : c2 = 01 S2 : c2 = 001 i j c3 10 c3 \n010 j Hyperplanes c1, c2 and c3 are identi.ed as belonging to one tilable band. Hence, 3-d tiles for \nLU decomposition from the above transformation are speci.ed as shown in Figure 2. The code is shown in \nFigure 9. Tiling multiple times The same tilinghyperplanes canbe usedto tile multiple times (due to Theorem \n1), for registers, L1, L2 caches, andfor parallelism,andthelegalityofthe sameis guaranteedbythe transformation \nframework. The scattering functions are duplicated #de.ne S1(t,i) {b[i]=0.333*(a[1+i]+a[i]+a[i-1]);}\" \n#\u00bb \"# #de.ne S2(t,j) {a[j]=b[j];} 10 0 /* Generatedby CLooG v0.14.164 bits in 0.02s. */ t for (t=0; \nt<T;t++) { 21 + 0 for (c1=0;c1<=.oord(T-1,256);c1++) { i for (i=2; i<N-1; i++) { 00 0 for (c2=max(0,ceild(512*c1-253,256));b[i] \n= 0.333*(a[i-1]+a[i] c2<=min(.oord(N+2*T-3,256),.oord(512*c1+N+509,256));c2++){ + a[i+1]); 23 23 if ((c1 \n<= .oord(256*c2-N+1,512)) &#38;&#38; (c2 >= ceild(N-1,256))) {}10 \u00bb 0 if ((-N+1)%2 == 0) {for (j=2; j \n<N-1; j++){ 4215 t + 415 S2(c1,-2*c1+c2,(256*c2-N+1)/2,N-2); a[j] =b[j]; j 00 1 } } } } } (a) Original \ncode for (c3=max(max(ceild(256*c2-N+2,2),256*c1),0); c3<=min(min(T-1,256*c1+255),.oord(256*c2-N+256,2));c3++){ \n#de.ne S1(t,i) {b[i]=(0.333*(a[1+i]+a[i]+a[i-1]);} for (c4=256*c2;c4<=2*c3+N-2;c4++) { #de.ne S2(t,j) \n{a[j]=b[j];} S1(c1,-2*c1+c2,c3,-2*c3+c4); S2(c1,-2*c1+c2,c3,-2*c3+c4-1); for (c1=0;c1<=T-1;c1++) {}S1(c1 \n,2); S2(c1,-2*c1+c2,c3,N-2); for (c2=2*c1+3;c2<=2*c1+N-2;c2++) {} S1(c1,-2*c1+c2);  for (c3=max(max(0,256*c1),ceild(256*c2-N+257,2)); \nS2(c1,-2*c1+c2-1); c3<=min(min(256*c1+255,T-1),128*c2-2);c3++){ }for (c4=256*c2;c4<=256*c2+255;c4++) \n{ S2(c1,N-2); S1(c1,-2*c1+c2,c3,-2*c3+c4); } S2(c1,-2*c1+c2,c3,-2*c3+c4-1); (b) Transformed(without \ntiling) }}for (c3=max(max(128*c2-1,0),256*c1); c3<=min(min(128*c2+126,256*c1+255),T-1);c3++){S1(c1,-2*c1+c2,c3); \nfor (c4=2*c3+3;c4<=256*c2+255;c4++) { S1(c1,-2*c1+c2,c3,-2*c3+c4); S2(c1,-2*c1+c2,c3,-2*c3+c4-1); }}} \n(d) Optimized with tiling (tile size 256), cloog -f3 -l5 } S1S2 S1 S2 3 23 232 3 232 3 23 232 3 232 \n c1 10000 tT 10000 tT c1 31000 tT 31000 tT 6664 c2 c3 c4 7775 = 6664 2 1 0 0 0 0 0 1 0 0 0 0 2 1 0 6664 \n7775 iT t i 6664 7775 2 1 0 0 0 0 0 1 0 0 0 0 2 1 1 6664 7775 jT t j 6664 7775 c2 c3 c4 7775 = 6664 \n6664 7775 2 1 0 0 0 0 0 1 0 0 0 0 2 1 0 iT t i 6664 7775 21000 00100 00211 6664 7775 jT t j 7775 c5 \n00000 1 00001 1 c5 00000 1 00001 1 (e)Transformationfor generationoflocallytiledcodein(c) (f)Transformationfor \ngenerationof parallelized+locallytiledcode Figure 3. Tiling imperfectly nested Jacobi foreachsuchlevelasitwasdoneforonelevel.Sucha \nguaranteeis available even when syntactic tiling is to be done as a post-pass on a perfectly nest band \nin the target AST.  5.3 Parallel code generation Once the algorithm in Sec. 5.2 is applied, outer parallel \nor inner parallel loops can be readily marked parallel (for example with openmp pragmas). However, unlike \nscheduling-based approaches, since we .nd tiling hyperplanes and the outer ones are used as space, there \nmay not be a single loop in the transformed space that satis.esall dependences(evenifthecodeadmitsaone \ndimensional schedule). Hence, when one or more of the space loops satis.es a (forward) dependence (also \ncalled doacross loops), care has to be taken while generating parallel code. Hence, for pipelined parallel \ncodes, our approach to coarse-grained (tiled) shared memory par\u00adallel code generation is as described \nin Figure 2. Once the technique described in the previous section is applied to generate the tile space \nscatterings and intra-tiled loops depen\u00addence components are all forward and non-negative for any band \nof tile space loops. Hence, the sum fT 1 + fT 2 + \u00b7\u00b7\u00b7 + fT p+1 Algorithm2 Tiled pipelined parallel code \ngeneration INPUT Given that Algorithm 1 has been applied, a set of k (statement\u00adwise) supernodes in the \ntransformed space belonging to a tilable band: fT 1, fT 2, . . . , fT k S S S 1: To extract m (<k)degrees \nof pipelined parallelism: 2: /* Update transformation matrices */ 3: foreach statement S do 4: Perform \nthe following unimodular transformation on only the scat\u00adtering supernodes: fT 1 . fT 1 + fT 2 + \u00b7\u00b7\u00b7 \n+ fT m+1 5: Mark fT 2, fT 3, . . . , fT m+1 as parallel 6: Leave fT 1, fT m+2, . . . , fT k as sequential \n7: endfor OUTPUT Updated transformation matrices/scatterings satis.esallaf.ne dependences satis.edby \nfT 1 , fT 2 , ... ,fT p+1 , and gives a legal wavefront (schedule) of tiles. Since the transfor\u00admation \nis only on the tile space, it preserves the shape of the tiles. Communication still happens along boundaries \nof f1 , f2 , ... , fs , andthe sameshapedtilesareusedto scanatile,thus preservingthe Domains S1 S2 0 \n= k = N - 10 = k = N - 1 k +1 = j = N - 1 k +1 = i = N - 1 k +1 = j = N - 1 0 = k - 32kT = 31 0 = k - \n32kT = 31 0 = j - 32jT = 31 0 = i - 32iT = 31 0 = j - 32jT = 31 Scatterings S1 S2 c1T = kT c1T = kT c2T \n= jT c2T = jT c3T = kT c3T = iT c1 = kc1 = k c2 = jc2 = j c3 = kc3 = i (c1T ,c2T ,c3T ,c1,c2,c3) (c1T \n,c2T ,c3T ,c1,c2,c3) . scatter(kT ,jT , k, j) . scatter(kT ,jT ,iT , k, j, i) Figure 2. Tiled speci.cation \nfor LU bene.ts of the optimization performed by the bounding approach. Moreover, performing such a unimodular \ntransformation to the tile space introduces very less additional code complexity (modulo s do not appear \nin the generated code due to unimodularity). In contrast, obtaining an af.ne (.ne-grained) schedule and \nthen enabling time tiling would lead to shapes different from above our approach. The above technique \nof adding up 1-d transforms resem\u00adbles that of [37] where (permutable) time partitions are summed up \nfor maximal dependence dismissal; however, we do this in the tile space as opposed to for .nding a schedule \nthat dismisses all dependences. for(i=1; i <N; i++) for(j=1; j <N; j++) a[i,j] =a[i-1,j] +a[i,j-1]; \n(a) Original (sequential) code for (c1=-1;c1<=.oord(N-1,16);c1++) #pragma omp parallel for shared(c1,a) \nprivate(c2,c3,c4) for (c2=max(ceild(32*c1-N+1,32),0); c2<=min(.oord(16*c1+15,16),.oord(N-1,32));c2++) \nfor (c3=max(1,32*c2);c3<=min(32*c2+31,N-1); c3++) for (c4=max(1,32*c1-32*c2); c4<=min(N-1,32*c1-32*c2+31); \nc4++) S1(c2,c1-c2,c3,c4) ; /* barrier happens only here(in tile space) */ (b) Coarse-grained tile schedule \nFigure 4. Shared memory parallel code generation example Figure4showsa simpleexample with tilinghyperplanes \n(1,0) and (0,1). Our scheme allows clean generation of parallel code without any syntactic treatment. \nAlternate ways of generating pipelined parallel code exist that insert special post/notify or wait\u00ad/signal \ndirectives to handle dependences in the space loops [36, 24], but, these require syntactic treatment. \nNote that not all degrees of pipelined parallelism need be exploited. In practice, a few degrees  Figure \n5. The PLuTo source-to-source transformation system are suf.cient; using several could introduce code \ncomplexity with diminishing return. 5.4 Intra-tile reordering Duetothenatureofour algorithm,evenwithinalocaltile(L1)that \nisexecuted sequentially,the intra-tile loops that are actuallyparallel do not end up being outer in the \ntile (Sec. 3.2): this goes against vectorization of the transformed source for which we rely on the native \ncompiler. Also, the polyhedral tiled code is often complex for a compiler to further analyze and say, \npermute and vectorize. Hence, as part of a post-process in the transformation framework, we move the \nparallel loop within a tile innermost and make use of ignore dependence pragmas to explicitly force vectorization. \nSimilar reordering is possible to improve spatial locality that is not considered by our cost function \ndue to the latter being fully dependence-driven. Note that the tile shapes or the schedule in the tile \nspaceis not alteredby such post-processing. 6. Implementation Theproposedframeworkhasbeen implementedintoatool,PLuTo[1]. \nFigure5shows the entire tool-chain.We used the scanner, parser and dependence tester from the LooPo infrastructure \n[38], which is a polyhedral source-to-source transformer including implemen\u00adtations of various polyhedral \nanalyses and transformations from the literature.We used PipLib 1.3.3 [41, 18] as the ILP solver and \nCLooG 0.14.1 [13] for code generation. The transformation frame\u00adwork takes as input, polyhedral domains \nand dependence polyhe\u00addra from LooPo sdependence tester,computes transformations and provides it to Cloog. \nCompilable OpenMP parallel code is .nally output after some post-processing on the Cloog code. Syntactic \npost-processing. Wehavealso integrated an annotation\u00addriven system of Norris et al. [39] to perform syntactic \ntransforma\u00adtions on the code generated from Cloog as a post-processing; these include register tiling \nfollowedbyunrolling or unroll/jamming. The choice of loops to perform these transformations on is speci.ed \nby the transformation framework, and hence legality is guaranteed. In this paper, we do not discuss anyfurther \non how exactly these transformations are performed and the corresponding performance improvement. They \nare non-trivial to perform for non-rectangular iteration spaces, for example. The complementary bene.ts \nof syn\u00adtactic post-processing will be reported in future. However, a pre\u00adview of the potential performance \nimprovement is provided for one kernel in the experimental evaluation section. 7. Experimental evaluation \nIn this section, we evaluate the performance of the transformed codes generated by our system. Comparison \nwith previous approaches Several previous papers on automatic parallelization have pre\u00adsentedexperimental \nresults.Adirect comparisonis dif.cult since the implementations of those approaches (with the exception \nof Griebl s[24,38])isnotavailable; further mostpreviously presented studies did not use an end-to-end \nautomatic implementation, but performed some manual code generation based on solutions gen\u00aderated by \na transformation framework, or by selecting a solution from a large space of solutions characterized. \nIn assessing the effectiveness of our system, we compare per\u00adformance of the generated code with that \ngenerated by produc\u00adtion compilers, as well as undertakinga best-effortfair comparison with previously \npresented approaches from the research commu\u00adnity. The comparison with other approaches from the literature \nis in some cases infeasible because there is insuf.cient information forusto reconstructa complete transformation(e.g. \n[2]).Foroth\u00aders [37, 36, 35], a complete description of the algorithm allows us to manually construct \nthe transformation;but since we do not have access to an implementation that can be run to determine \nthe transformation matrices or generate compilable optimized code, we have not attempted anexhaustive \ncomparison for all the cases.For the above reasons, the .rstkernel chosen (imperfectly-nested 1-d Jacobi) \nis a relatively simpler one. The current state-of-the-art with respect to optimizing code has been semi-automatic \napproachesthat requireanexpertto manually guide transformations [22]. As for scheduling-based approaches, \nthe LooPo system[38] includes implementationsofvarious polyhe\u00addral scheduling techniques including Feautrier \ns multi-dimensional time scheduler which can be coupled with Griebl s space and FCO time tiling techniques.We \nthus provide comparison for some num\u00adber of cases with the state of the art (1) Griebl s approach that \nuses Feautrier s schedules along with Forward-Communication-Only allocations to enable time tiling [24] \nthat will be referred to as Scheduling-based (time tiling) and (2) Lim/Lam s af.ne parti\u00adtioning [37, \n36, 35] referred to as Af.ne partitioning (max degree parallelism, no cost function) in the graphs.For \nbothof these pre\u00advious approaches, the input code was run through our system and the transformations \nwere forcedtobe what those approacheswould have generated. Hence, these techniques get all bene.ts of \nCLooG and our tiled code generation scheme. Experimental setup. The results were obtained on a quad-core \nIntel Core 2 Quad Q6600 CPU clocked at 2.4 GHz (1066 MHz FSB) witha32KBL1D cache, 8MBofL2 cache (4MB \nshared percorepair),and2GBof DDR2-667RAM,runningLinuxkernel version 2.6.22 (x86-64). ICC 10.0 was the \nprimary compiler used to compile the base codes as well as the source-to-source trans\u00adformed codes; it \nwas run with -fast -funroll-loops (-openmp for parallelized code); the -fast option turns on -O3, -ipo, \n-static, \u00adno-prec-div on x86-64 processors these options also ensure auto\u00advectorization in icc. The \nOpenMP implementation of icc supports nested parallelism needed to exploit multiple degrees of pipelined \nparallelism when theyexist. Our transformation framework itself runs quitefast withina fractionofasecondforall \nbenchmarks consideredhere.Alongwith code generation time, the entire source-to-source transformation \ndoes not take more than a few seconds in any of the cases. The OpenMP parallel for directive(s) achieves \nthe distribution of the blocks of the tile space loop(s) among processor cores. Hence, execution on each \ncore is a sequence of L2 tiles or L1 tiles if L2 tiling is not done. Tile sizes were set automatically \nusing a very rough model. Equal tile sizes were used along all dimensions, except when loops were marked \nfor vectorization (Sec.5.4), in which case the tile size of the loop to be vectorized was increased. \nIn all cases, the optimized code for our framework was obtained automaticallyina turn-keyfashion from \nthe input source code. Imperfectly nested stencil code. The original code, code opti\u00admized by our system \nwithout tiling, and optimized tiled code are shown in Figure 3. The performance of the optimized codes \nare shown in Figure 6. Speedup s ranging from 4x to 7x are obtained for single core execution due to \nlocality enhancement. The parallel speedups are compared with Lim/Lam s technique (AlgorithmA in [37]) \nwhich obtains (2,-1), (3,-1) as the maximally independent time partitions, and Griebl s time tiling technique \nwhich uses an FCO allocation of 2t + i along with the schedule 2t for S1, 2t +1 for S2. Just space tiling \nin this case does not expose suf.cient par\u00adallelism granularity and an inner space parallelized code \nhas very poor performance. This is also the case with icc s auto parallelizer; hence, we just show the \nsequential run time for icc in this case. Analysis of cache misses with each of the schemes is available \nin the detailed report [10]. for (t=0; t <tmax; t++) { 23 for (j=0; j<ny;j++) 10 0 11 0 10 0 ey[0][j] \n= exp(-coeff0*t1); 45 for (i=1; i <nx;i++) for (j=0; j <ny;j++) ey[i][j] =ey[i][j] - 23 100 0 coeff1 \n*(hz[i][j]-hz[i-1][j ]); 45 101 0 110 0 for (i=0; i <nx;i++) for (j=1; j <ny;j++) ex[i][j] =ex[i][j] \n23 100 0 - coeff1*(hz[i][j]-hz[i][j-1]); 45 101 0 110 0 for (i=0; i <nx;i++) for (j=0; j <ny;j++) hz[i][j] \n= hz[i][j] - 23 100 0 coeff2 *(ex[i][j+1]-ex[i][j] 45 101 1 +ey[i+1][j]-ey[i][j]); 110 1 } Figure 7. \n2-d FDTD (original code) and transformation Finite Difference Time Domain electromagnetic kernel. The \nFDTD (Finite DifferenceTime Domain) code is as shown in Fig\u00adure 7. The arrays ex and ey represent electric \n.elds in x and y directions, while hz is the magnetic .eld. The code has four state\u00adments -three of them \n3-d and one 2-d and are nested imperfectly. Our transformation framework .nds three tilinghyperplanes(allin \none band -fully permutable). The transformation representsa com\u00adbinationof shifting, fusionand timeskewing.Parallel \nperformance results shown are for nx = ny = 2000 and tmax = 500. Results are shown in Figure 8. LU decomposition. \nThree tilinghyperplanes are found all be\u00adlonging to a single band of permutable loops. The .rst statement, \nthough lower-dimensional, is naturally sunk into a a 3-dimensional fully permutable space. Thus, there \nare two degrees of pipelined parallelism. Icc is unable to auto-parallelize this. Performance results \non the quad core machine are shown in Figure 10(b). Scheduling-based parallelization performs poorly \nmainly due to code complexity arising out of a non-unimodular transformation, that also inhibits vectorization. \nMatrix vector transpose. TheMVTkernelisa sequenceoftwo matrix vector transposes as shown in Figure 11. \nIt is found within an outer convergence loop with the Biconjugate gradient algorithm. The only inter-statement \ndependence is a non-uniform read/input on matrix A. The cost function bounding (4) leads to minimization \nof this dependence distance by fusion of the .rst MV with the per\u00admutedversionof the secondMV (note that \nf(xt) - f(xs) for this de\u00adpendence becomes0for both c1 and c2).This however leads to loss of synchronization-free \nparallelism, since, in the fused form, each Figure 6. Imperfectly nested Jacobi stencil (a) Single \ncore: T=500 (b) Parallel: nx = ny = 2000, tmax = 500 Figure 8. 2-d FDTD loop satis.es a dependence. However, \nsince these dependences are in the forward direction, the parallel code is generated correspond\u00ading to \none degree of pipelined parallelism. Existing techniques do not consider input dependences. Hence, other \napproaches only ex\u00adtract synchronization-free parallelism from each of the MVs sepa\u00adrately with a barrier \nbetween the two, giving up reuse on array A. Figure12showsthe resultsforaproblem size N = 8000. Fusion \nof ij with ij does not exploit reuse on matrix A, whereas the code generated by our tool performs the \nbest it fuses ij with ji, tiles it andextractsadegreeof pipelined parallelism.For this case, results \nare also shown with further syntactic transformations performed on the Pluto code. 3-D Gauss-Seidel successive \nover relaxation. The Gauss-Seidel computation allows tiling of all three dimensions after skewing. The \ntransformation obtained by our tool skews the two space di\u00admensionsbyafactorof oneandtwo, respectively,w.r.t \ntime.Two degreesof pipelined parallelism canbeextracted subsequently,and all three dimensions can be \ntiled. Figure 13 shows the performance improvement achieved with 2-d pipelined parallel space as well \nas 1-d: the latter is better in practice mainly due to simpler code. for (c1=0;c1<=N-1;c1++)for (i=0; \ni <N; i++) for (c2=0;c2<=N-1;c2++) for (j=0; j <N; j++) x1[c1] = x1[c1]+a[c1,c2]*y1[c2]; x1[i] =x1[i] \n+a[i,j]*y1[j]; x2[c2] = x2[c2]+a[c1,c2]*y2[c1]; for (i=0; i<N; i++) (b) Transformed for (j=0; j <N; \nj++) x2[i] =x2[i] +a[j,i]*y2[j]; S1 S2 (a) Original \u00bb c1 \u00bb 1 0 \u00bb i \u00bb 0 1 \u00bb i = c2 0 1 j 1 0 j Figure \n11. Matrix vector transpose Again,icc is does not parallelize this. The absolute GFLoPs perfor\u00admance \nhere is on the lower side when compared to other codes due to a unique dependence structure that prevents \nauto-vectorization. 7.1 Analysis. All experiments show very high speedups with our approach, both for \nsingle thread and multicore parallelexecution. The performance for (k=0; k<N; k++) for (j=k+1;j<N; j++) \na[k][j] =a[k][j]/a[k][k]; for(i=k+1; i<N; i++) {for (j=k+1;j<N; j++) {a[i][j] =a[i][j] - a[i][k]*a[k][j]; \n(a) Original code S1 #de.ne S1(zT0,zT1,k,j) {a[k][j]=a[k][j]/a[k][k];}#de.ne S2(zT0,zT1,zT2,k,i,j) {a[i][j]=a[i][j]-a[i][k]*a[k][j];} \n/* Generatedby CLooG v0.14.164 bits in 0.02s. */ for (c1=-1;c1<=.oord(2*N-3,32);c1++) lb = max(max(ceild(16*c1-15,32),ceild(32*c1-N+2,32)),0); \nub = min(.oord(32*c1+31,32), .oord(N-1,32)); #pragma omp parallel for shared(c1,lb,ub,a) private(c2,c3,c4,c5,c6,i,j,k,l,m,n) \nfor (c2=lb;c2<=ub;c2++) for (c3=max(ceild(16*c1-16*c2-465,496),ceild(16*c1-16*c2-15,16));c3<=.oord(N-1,32);c3++) \nif (c1 ==c2+c3) {for (c4=max(0,32*c3);c4<=min(min(32*c3+30,N-2),32*c2+30);c4++) 3 23 2 c1 1100 32 666664 \n777775 = 666664 0100 1000 0010 0001 777775 kT c2 c3 c4 for (c5=max(32*c2,c4+1);c5<=min(N-1,32*c2+31);c5++) \nS1(c1-c2,c2,c4,c5) ; 64 jT k 75 for (c6=c4+1;c6<=min(32*c3+31,N-1);c6++) j c5 S2(c1-c2,c1-c2,c2,c4,c6,c5) \n; c6 0010 } S2 for (c4=max(0,32*c1-32*c2);c4<=min(min(32*c1-32*c2+31,32*c3-1),32*c2+30);c4++) for (c5=max(32*c2,c4+1);c5<=min(N-1,32*c2+31);c5++) \n 3 23 23 2 c1 101000 kT for (c6=32*c3;c6<=min(32*c3+31,N-1);c6++) S2(c1-c2,c3,c2,c4,c6,c5) ; if ((-c1 \n== -c2-c3) &#38;&#38; (c1 <= min(.oord(32*c2+N-33,32),.oord(64*c2-1,32)))) {for (c5=max(32*c1-32*c2+32,32*c2);c5<=min(32*c2+31,N-1);c5++) \n666664 777775 = 666664 001000 010000 000100 000001 666664 777775 iT jT k i 777775 c2 c3 c4 c5 c6 000010 \nj S1(c1-c2,c2,32*c1-32*c2+31,c5); c2 is marked omp parallel } (b) 1-d pipelined parallel (c) LU (1-dpipelined \nparallel +L1 tiled)(tile size 32) cloog -f4 -l7 Figure 9. LU decomposition (a) Single core (L1 and L2 \ntiled) (b) On a quadcore: N=8000 improvement is very signi.cant over production compilers as well as \nstate-of-the-art from the research community. Speedup ranging from 2x to 5x are obtained over previous \nautomatic transformation approaches in most cases, while an order of 10x improvement is obtained over \nthe best native production compilers. Linear to super-linear speedups are seen for almost all compute-intensive \nkernels considered here due to optimization for locality as well as parallelism.To the bestof our knowledge, \nsuch speedup shave not been reported by anyautomatic compiler framework as general as ours. Hand-parallelization \nof many of the examples we considered hereisextremely tediousandnot feasiblein some cases, especially \nwhen time skewed code has to be pipelined parallelized or imper\u00adfectly nested loops areinvolved; this \ncoupledby thefact that the codehastobe tiledforat leastfor onelevelof local cache,anda2-d pipelined parallel \nschedule of 3-d tiles is to be obtained makes man\u00adual optimization very complex. The performance of the \noptimized stencil codes through our system is already comparable to hand op\u00adtimized versions reported \nin [29]. Also, for many of the codes, a simple parallelization strategy of exploiting inner parallelism \nand leaving the outer loop sequential (i.e., no time tiling) hardly yields anyparallel speedup (Figure \n8(b), Figure 6(b)) scheduling-based approaches that do not perform time tiling, or production compil\u00aders \nauto-parallelizers perform such transformations.  As mentioned before, tile sizes were not optimized \nthrough any search or a concrete model. In addition, studying the interplay of the transformed codes \nwith prefetching is important. Using cost models for effective tile size determination with some amount \nof empirical search, in a manner decoupled with the pure model\u00addriven scheme presented here, we expect \nto move performance closer to the machine peak. Integration of these techniques is in progress.For simpler \ncodes like matrix-matrix multiplication, this latter phase of optimization, though very simple and straightfor\u00adward \nwhen compared to the rest of our system, brings most of the bene.ts.We are also integrating complementary \nsyntactic transfor\u00admations on the generated code: note that this latter phase fully relies on the transformation \nframework for correctness and systematic ap\u00adplication. Additional experimental results and comments about \nthe optimized codes and transformations are available in an extended report [10]. 8. Related work Iteration \nspace tiling [28, 58, 48, 61] is a standard approach for aggregating a set of loop iterations into tiles, \nwith each tile be\u00ading executed atomically. It is well known that it can improve reg\u00adister reuse, improve \nlocality and minimize communication. Re\u00adsearchers have considered the problem of selecting tile shape \nand size to minimize communication, improvelocality or minimize .n\u00adish time [5, 11, 26, 27, 48, 50, 60]. \nHowever, these studies were restricted to very simple codes like single perfectly nested loop nests \nwith uniform dependences and/or sometimes loop nests of a particular depth.To the bestof our knowledge, \ntheseworkshave notbeenextendedto moregeneral casesandthecost functionspro\u00adposed therein not been implemented \nto allow a direct comparison for those restricted cases. Our work is in the direction of a prac\u00adtical \ncost function that works for the general case (anypolyhedral program or one that can be approximated \ninto it) as opposed to a more sophisticated function for restrictive input.With sucha func\u00adtion, we are \nable tokeep the problem linear, and since sparse ILP formulations that result here are solved very quickly, \nwe are at a sweetspot between cost-function sophistication and scalability to real-world programs. Re.nements \nto the function that stillkeep it linear are discussed in Section 3.10 of [8]. Note that our function \ndoes not capture tile size optimization, but our results show that decoupling optimization of tile shapes \nand sizes is a practical and very effective approach; all the performance improvement shown were with \ntile sizes that were selected with rough thumb rules au\u00adtomatically. Ahmed et al. [2] proposed a framework \nfor data locality opti\u00admization of imperfectly nested loops for sequential execution. It was among the \n.rst attempts to tile imperfectly nested loops. How\u00adever, the solution proposed for reuse distance minimization \nis not scalable [53]. Some specialized works [52, 62] also exist on tiling a restricted class of imperfectly \nnested loops for locality. Loop parallelization has been studied extensively. The reader is referred \nto the survey of Boulet et al. [12] for a detailed sum\u00admary of older parallelization algorithms which \naccepted restricted input and/or were based on weaker dependence abstractions than exact polyhedral dependences; \nthese prominently include [3, 17, 16, 59]. Automatic parallelization efforts in the polyhedral model \nbroadlyfall into two classes: (1) scheduling/allocation-based, and (2) partitioning-based. The works \nof Feautrier [20, 21], Darte and Vivien [17] and Griebl [24] (to someextent)fall into the former class, \nwhileLim/Lam s approach[37,36,35]fallsintothe second class.We now compare with approaches from both classes. \nPure scheduling-based approaches are geared towards .nd\u00ading minimum latency schedules or maximum .ne-grained \nparal\u00adlelism, as opposed to tilability for coarse-grained parallelization with minimized communication \nand improved locality. Clearly, on most modern parallel architectures, at least one level of coarse\u00adgrained \nparallelism is desired as communication/synchronization costs matter, and so is improving locality. Several \nworks are based on such schedules [7, 24, 14, 45, 44]. Griebl [24] presents an integrated framework for \noptimizing lo\u00adcality and parallelism with space and time tiling, by treating tiling as a post-processing \nstep after a schedule is found. When sched\u00adules are used, the inner parallel (space) loops can be readily \ntiled. In addition, if coarser granularity of parallelism is desired, Griebl .nds an allocation that \nsatis.es the forward communication-only constraint: this enables time tiling. As argued in [8] from a \ntheo\u00adretical standpoint and as demonstrated here through experiments, using schedules as one of the loops \nis not best suited for communi\u00adcation and locality optimization as well as target code complexity. Lim \nand Lam [37, 36] proposed a framework that identi.es outer parallel loops (communication-free space partitions) \nand per\u00admutable loops (time partitions) to maximize the degree of paral\u00adlelism and minimize the order \nof synchronization. They employ the same machinery for blocking [35]. Several (in.nitely many) solutions \nequivalent in terms of the criterion theyoptimize for re\u00adsult from their algorithm, and these signi.cantly \ndiffer in perfor\u00admance. No metric is provided to differentiate between these solu\u00adtions as maximally \nindependent solutions without a cost function are sought. As shown through this work, without a cost \nfunction, solutions obtainedevenforsimpleinputmaybe unsatisfactorywith respect to communication cost, \nlocality, and target code complex\u00adity. Our approach is closer to the latter class of partitioning-based \napproaches. However, to the best of our knowledge, it is the .rst to explicitly model tiling in the transformation \nframework thereby allowing it to .nd good ways of extracting coarse-grained paral\u00adlelism and locality. \nAt the same time, input which cannot be tiled or only partially tiled is all handled, and traditional \ntransformations are captured. In addition to model-based approaches, semi-automatic and search-based \ntransformation frameworks in the polyhedral model alsoexist[30,14,22,45,44]. Cohenetal.[14]and Girbaletal.[22] \nproposedanddevelopedapowerful framework(URUK/WRAP-IT) to compose and apply sequences of transformations \nin a semi\u00adautomaticfashion.Transformations are applied automatically,but speci.ed manually by an expert. \nAs for the recent iterative polyhe\u00addral compilation approaches [45, 44], the search space constructed \ndoes not include tiling and its integration poses a non-trivial chal\u00adlenge. Though our system now is \nfully model-driven, some amount of empirical iterative optimization on complementary aspects, such as \ndetermination of optimal tile sizes and unroll factors, and in other cases when interactions with the \nunderlying hardware and native compiler cannot be well-captured. Code generation under multiple af.ne \nmappings was .rst ad\u00addressedbyKelly et al. [31]. Signi.cant advances relying on new algorithms and mathematical \nmachinery were made by Quiller\u00b4e et al. [47] and recently by Bastoul et al. [6], resulting in a power\u00adful \nopen-source code generator, Cloog [13]. Our tiled code gen\u00aderation scheme uses Ancourt andIrigoin s [4] \nclassic approach to specify domains with .xed tile sizes and shape information, but combines it with \nCloog s support for scattering functions to al\u00adlow generation of tiled code for multiple domains under \nthe com\u00adputed transformations. Goumas et al. [23] reported an alternate tiled code generation scheme \n(to [4]) to address the inef.ciency in\u00advolved in using Fourier-Motzkin however, this is no longer an \nissue as the state-of-the-art uses ef.cient algorithms [47, 6] based on PolyLib [57, 42] and its implementation \nof the Chernikovaalgo\u00adrithm[33].Techniquesfor parametrictiledcode generation[49,32] were recently proposed \nfor single statement domains for which rectangular tiling is valid. These techniques complement our sys\u00adtemverywellandwe \nintendtoexplorethe possibilityofintegrating them. 9. Conclusions We have presented the design and implementation \nof a fully au\u00adtomatic polyhedral source-to-source program optimizer that can simultaneously optimize \nsequences of arbitrarily nested loops for parallelism and locality. Through this work, we have shown \nthe practicality and promise of automatic transformation in the poly\u00adhedral model, beyond what is possible \nby current production com-pilers.Wehave implemented our framework intoa toolto gener\u00adate OpenMP parallel \ncode fromCprogram sections automatically. Experimental results show very signi.cant speedup for single \ncore and parallel execution on multi-cores. Our system also leaves a lot of .exibility for future optimization, \nmainly iterative and empiri\u00adcal and/or through more sophisticated cost models, and promise to achieve \nperformance close to or beat manually developed codes. The transformation system presented here is not \njust applicable toC/Fortrancode,buttoanyinputlanguagefromwhich polyhedra can be extracted and analyzed. \nSince our entire transformation framework works in the polyhedral abstraction, only the polyhedra extractor \nand dependence tester, needs to be adapted to accept a future language. It could be applied for example \nto very high\u00adlevel languages or domain-speci.c languages to generate high\u00adperformance parallel code. \n 10. Availability A beta release of the Pluto system including all codes used for experimental evaluation \nin this paper are available at [1]. Acknowledgments Wewouldliketo thankC\u00b4edric Bastoul(Paris-SudXIUniversity, \nOrsay,France)very much for CLooG.Wewould also acknowledge Martin Griebl and team (FMI, Universit\u00a8at Passau, \nGermany) for the LooPo infrastructure. Thanks also to the reviewers of the sub\u00admission for detailed comments. \nIn addition, we thank Alain Darte for useful feedback that has improved presentation. This work is supportedin \npartby the U.S. National ScienceFoundation through grants 0121676, 0121706, 0403342, 0508245, 0509442, \n0509467, and 0541409. References [1] PLuTo:Apolyhedral automatic parallelizer and locality optimizer \nfor multicores. http://pluto-compiler.sourceforge.net. [2] N. Ahmed, N. Mateev, and K. Pingali. Synthesizing \ntransformations for locality enhancement of imperfectly-nested loops. Intl. J. of Parallel Programming, \n29(5), Oct. 2001. [3] R. Allen andK.Kennedy. Automatic translationofFortran programs tovector form. ACMTrans.onProgrammingLanguagesand \nSystems, 9(4):491 542, 1987. [4]C. AncourtandF. Irigoin. Scanning polyhedrawithdoloops.In ACM SIGPLAN \nPPoPP 91, pages 39 50, 1991. [5] R. Andonov, S. Balev, S. Rajopadhye, and N.Yanev. Optimal semi\u00adoblique \ntiling. IEEETrans.Par.&#38;Dist. Sys., 14(9):944 960, 2003. [6] C. Bastoul. Code generation in the polyhedral \nmodel is easier than you think. In IEEEPACT, pages 7 16, Sept. 2004. [7] C. Bastoul andP. Feautrier. \nImproving data localityby chunking. In Intl. Conf. on Compiler Construction (ETAPS CC), pages 320 335, \nWarsaw, Apr. 2003. [8] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, J. Ramanujam, A. Rountev, and P. \nSadayappan. Af.ne transformations for communication minimal parallelization and locality optimization \nof arbitrarily-nested loop sequences. Technical ReportOSU-CISRC\u00ad5/07-TR43, The Ohio State University, \nMay 2007. [9] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, J. Ramanujam, A. Rountev, and P. Sadayappan. \nAutomatic transformations for communication-minimized parallelization and locality optimization in the \npolyhedral model. In Intl. Conf. on Compiler Construction (ETAPS CC), Apr. 2008. [10] U. Bondhugula, \nJ. Ramanujam, and P. Sadayappan. Pluto: A practical and fully automatic polyhedral parallelizer and locality \noptimizer. Technical Report OSU-CISRC-10/07-TR70, The Ohio State University, Oct. 2007. [11]P. Boulet,A. \nDarte,T. Risset,andY. Robert. (Pen)-ultimate tiling? Integration, the VLSIJournal, 17(1):33 51, 1994. \n[12]P. Boulet,A. Darte, G.-A. Silber, andF.Vivien. Loop parallelization algorithms: From parallelism \nextraction to code generation. Parallel Computing, 24(3 4):421 444, 1998. [13] CLooG: The ChunkyLoop \nGenerator. http://www.cloog.org. [14] A. Cohen, S. Girbal, D. Parello, M. Sigler, O. Temam, and N.Vasilache. \nFacilitating the search for compositions of program transformations. In ACM Intl. Conf. on Supercomputing, \npages 151 160, June 2005. [15] A. Darte, Y. Robert, and F. Vivien. Scheduling and Automatic Parallelization. \nBirkhauser Boston, 2000. [16] A. Darte, G.-A. Silber, and F. Vivien. Combining retiming and scheduling \ntechniques for loop parallelization and loop tiling. Parallel Processing Letters, 7(4):379 392, 1997. \n[17] A. Darte andF.Vivien. Optimal .ne and medium grain parallelism detection in polyhedral reduced dependence \ngraphs. Intl.J.Parallel Programming, 25(6):447 496, Dec. 1997. [18] P. Feautrier. Parametric integer \nprogramming. RAIRO Recherche Op\u00b4erationnelle, 22(3):243 268, 1988. [19] P. Feautrier. Data.ow analysis \nof scalar and array references. Intl.J. ofParallel Programming, 20(1):23 53, Feb. 1991. [20] P. Feautrier. \nSome ef.cient solutions to the af.ne scheduling problem: I. one-dimensional time. Intl.J. ofParallel \nProgramming, 21(5):313 348, 1992. [21] P. Feautrier. Some ef.cient solutions to the af.ne scheduling \nproblem. part II. multidimensional time. Intl.J. ofParallel Programming, 21(6):389 420, 1992. [22] S. \nGirbal, N.Vasilache, C. Bastoul, A. Cohen, D.Parello, M. Sigler, andO.Temam. Semi-automatic compositionofloop \ntransformations. Intl.J. ofParallel Programming, 34(3):261 317, June 2006. [23] G. Goumas, M. Athanasaki, \nand N. Koziris. Code Generation Methods forTilingTransformations. J. of Information Science and Engineering, \n18(5):667 691, Sep. 2002. [24] M. Griebl. Automatic Parallelization of Loop Programs for Distributed \nMemory Architectures. University of Passau, 2004. Habilitation thesis. [25] M. Griebl, C. Lengauer, and \nS. Wetzel. Code generation in the polytope model. In IEEEPACT, pages 106 111, 1998. [26] E. Hodzic andW. \nShang. On time optimal supernode shape. IEEE Trans.Par.&#38;Dist. Sys., 13(12):1220 1233, 2002. [27] \nK. Hogstedt, L. Carter, and J. Ferrante. Selecting tile shape for minimal execution time. In SPAA, pages \n201 211, 1999. [28]F. IrigoinandR.Triolet. Supernode partitioning.In ACM SIGPLAN PoPL, pages 319 329, \n1988. [29]S.Kamil,K. Datta,S.Williams,L. Oliker,J. Shalf, andK.Yellick. Implicit and explicit optimization \nfor stencil computations. In ACM SIGPLAN workshop on Memory Systems Perofmance and Correctness, 2006. \n[30]W.KellyandW.Pugh.Aunifyingframeworkfor iteration reordering transformations. Technical Report CS-TR-3430, \nDept. of Computer Science, Universityof Maryland, CollegePark, 1995. [31]W.Kelly,W. Pugh, and E. Rosser. \nCode generation for multiple mappings. In Intl. Symp. on the frontiers of massively parallel computation, \npages 332 341, Feb. 1995. [32] D. Kim, L. Renganarayanan, M. Strout, and S. Rajopadhye. Multi\u00adlevel tiling: \nm for the price of one. In Supercomputing, 2007. [33] H. LeVerge. A note on chernikova s algorithm. Technical \nReport Research report 635, IRISA, Feb. 1992. [34] W. Li and K. Pingali. A singular loop transformation \nframework based on non-singular matrices. Intl.J. ofParallel Programming, 22(2):183 205, 1994. [35] A. \nLim, S. Liao, and M. Lam. Blocking and array contraction across arbitrarily nested loops using af.ne \npartitioning. In ACM SIGPLAN PPoPP, pages 103 112, 2001. [36]A.W. Lim,G.I. Cheong, andM.S. Lam. Anaf.ne \npartitioning algorithm to maximize parallelism and minimize communication. In ACM Intl. Conf. on Supercomputing, \npages 228 237, 1999. [37] A.W. Lim and M. S. Lam. Maximizing parallelism and minimizing synchronization \nwith af.ne partitions. Parallel Computing, 24(3\u00ad4):445 475, 1998. [38] The LooPo Project -Loop parallelization \nin the polytope model. http://www.fmi.uni-passau.de/loopo. [39] B. Norris, A. Hartono, andW. Gropp. \nAnnotations for performance and productivity. 2007. Preprint ANL/MCS-P1392-0107. [40] R. Penrose. Ageneralized \ninverse for matrices. Proceedings of the Cambridge Philosophical Society, 51:406 413, 1955. [41] PIP: \nTheParametric Integer Programming Library. http://www.piplib.org. [42] PolyLib -Alibraryof polyhedral \nfunctions. http://icps.u-strasbg.fr/polylib/. [43] S. Pop, A. Cohen, C. Bastoul, S. Girbal,P. Jouvelot, \nG.-A. Silber, and N.Vasilache. GRAPHITE: Loop optimizations based on the polyhedral modelfor GCC. In \nProc. of the 4th GCC Developper s summit, Ottawa, Canada, June 2006. [44] L.-N. Pouchet, C. Bastoul, \nJ. Cavazos, and A. Cohen. Iterative optimizationinthe polyhedral model:PartII,multidimensional time. \nIn PLDI 08,Tucson, Arizona, June 2008. [45] L.-N. Pouchet, C. Bastoul, A. Cohen, and N.Vasilache. Iterative \noptimizationin the polyhedral model:PartI, one-dimensional time. In ACM CGO, Mar. 2007. [46]W. Pugh. \nThe omega test:afast and practical integer programming algorithm for dependence analysis. Communications \nof theACM, 8:102 114, Aug. 1992. [47]F. Quiller\u00b4e, S.V. Rajopadhye, and D.Wilde. Generation of ef.cient \nnested loops from polyhedra. Intl. J. of Parallel Programming, 28(5):469 498, 2000. [48] J. Ramanujam \nandP. Sadayappan.Tiling multidimensional iteration spaces for multicomputers. JPDC, 16(2):108 230, 1992. \n[49] L. Renganarayana, D. Kim, S. Rajopadhye, and M. M. Strout. Parameterized tiled loops for free. InPLDI, \npages 405 414, 2007. [50] R. Schreiber and J. Dongarra. Automatic blocking of nested loops. Technical \nreport,UniversityofTennessee, Knoxville,TN,Aug. 1990. [51] A. Schrijver. Theory of Linear and Integer \nProgramming. JohnWiley &#38;Sons, 1986. [52] Y. Song and Z. Li. New tiling techniques to improve cache \ntemporal locality. In PLDI, pages 215 228, 1999. [53] N. Vasilache. Scalable Program Optimization Techniques \nin the Polyhedral Model. PhD thesis, Universit\u00b4e deParis-Sud, INRIA, Futurs, Sept. 2007. [54]N.Vasilache,C.Bastoul,andA. \nCohen. Polyhedralcode generation in the real world. In Intl. Conf. on Compiler Construction (ETAPS CC), \npages 185 201, Mar. 2006. [55] N. Vasilache, C. Bastoul, S. Girbal, and A. Cohen. Violated dependence \nanalysis. In ACM ICS, June 2006. [56] R. Whaley, A. Petitet, and J. Dongarra. Automated Empirical Optimizations \nof Software and the ATLAS Project. Parallel Computing, 2000. [57] D. K.Wilde. Alibrary for doing polyhedral \noperations. Technical Report RR-2157, IRISA, 1993. [58] M.Wolf and M. S. Lam. Adata locality optimizing \nalgorithm. In ACM SIGPLAN PLDI 91, pages 30 44, 1991. [59] M. Wolf and M. S. Lam. A loop transformation \ntheory and an algorithm to maximize parallelism. IEEETrans.Parallel Distrib. Syst., 2(4):452 471, 1991. \n[60] J. Xue. Communication-minimal tiling of uniform dependence loops. JPDC, 42(1):42 59, 1997. [61] \nJ. Xue. Loop tiling for parallelism. Kluwer Academic Publishers, Norwell, MA, USA, 2000. [62]Q.Yi,K.Kennedy,andV.Adve.Transforming \ncomplexloop nests for locality. J. of Supercomputing, 27(3):219 264, 2004. [63]K.Yotov,X.Li,G.Ren,M.Cibulskis,G. \nDeJong,M. Garzaran,D.A. Padua,K.Pingali,P. Stodghill,andP.Wu.Acomparisonof empirical and model-driven \noptimization. In PLDI 03, pages 63 76, 2003.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.</p>", "authors": [{"name": "Uday Bondhugula", "author_profile_id": "81326487775", "affiliation": "The Ohio State University, Columbus, OH, USA", "person_id": "P1022754", "email_address": "", "orcid_id": ""}, {"name": "Albert Hartono", "author_profile_id": "81351607741", "affiliation": "The Ohio State University, Columbus, OH, USA", "person_id": "P1022755", "email_address": "", "orcid_id": ""}, {"name": "J. Ramanujam", "author_profile_id": "81100351630", "affiliation": "Louisiana State University, Baton Rouge, LA, USA", "person_id": "P1022756", "email_address": "", "orcid_id": ""}, {"name": "P. Sadayappan", "author_profile_id": "81100364545", "affiliation": "The Ohio State University, Columbus, OH, USA", "person_id": "P1022757", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375595", "year": "2008", "article_id": "1375595", "conference": "PLDI", "title": "A practical automatic polyhedral parallelizer and locality optimizer", "url": "http://dl.acm.org/citation.cfm?id=1375595"}