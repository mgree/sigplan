{"article_publication_date": "06-07-2008", "fulltext": "\n Foundations of the C++ Concurrency Memory Model Hans-J. Boehm HP Laboratories Hans.Boehm@hp.com Abstract \nCurrently multi-threaded C or C++ programs combine a single\u00adthreaded programming language with a separate \nthreads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, \nto address these issues by explicitly providing semantics for threads in the next revision of the C++ \nstandard. Our approach is similar to that recently followed by Java [25], in that, at least for a well\u00adde.ned \nand interesting subset of the language, we give sequentially consistent semantics to programs that do \nnot contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar \nwith the Java effort: We (mostly) insist on sequential consistency for race-free pro\u00adgrams, in spite \nof implementation issues that came to light after the Java work.  We give no semantics to programs with \ndata races. There are no benign C++ data races.  We use weaker semantics for trylock than existing languages \nor libraries, allowing us to promise sequential consistency with an intuitive race de.nition, even for \nprograms with trylock.  This paper describes the simple model we would like to be able to provide for \nC++ threads programmers, and explain how this, to\u00adgether with some practical, but often under-appreciated \nimplemen\u00adtation constraints, drives us towards the above decisions. Categories and Subject Descriptors \nD.3.3 [Programming Lan\u00adguages]: Language Constructs and Features Concurrent Pro\u00adgramming Structures; \nD.1.3 [Programming Techniques]: Concur\u00adrent Programming Parallel Programming; B.3.2 [Memory Struc\u00adtures]: \nDesign Styles Shared Memory General Terms languages, standardization, reliability Keywords Memory consistency, \nmemory model, sequential con\u00adsistency, C++, trylock, data race 1. Introduction As technological constraints \nincreasingly limit the performance of individual processor cores, the computer industry is relying in\u00adcreasingly \non larger core counts to provide future performance im\u00adprovements. In many domains, it is expected that \nany substantial Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. PLDI 08, June 7 13, 2008, Tucson, Arizona, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-860-2/08/06. \n. . $5.00 Sarita V. Adve University of Illinois at Urbana-Champaign sadve@cs.uiuc.edu Initially X=Y=0 \nT1 T2 r1=X r2=Y if (r1==1) if (r2==1) Y=1 X=1 Is outcome r1=r2=1 allowed? Figure 1. Without a precise \nde.nition, it is unclear if this example is data-race-free. The outcome shown can occur in an execution \nwhere threads T1 and T2 speculate that the values of X and Y respectively are 1, then each thread writes \n1, validating the other s speculation. Such an execution has a data race on X and Y, but most programmers \nwould not envisage such executions when assessing whether the program is data-race-free. future performance \ngains will require explicitly parallel applica\u00adtions. The most established way to write parallel applications \nin stan\u00addard programming languages is as a collection of threads sharing an address space. In fact, a \nlarge fraction of existing desktop applica\u00adtions already use threads, though typically more to handle \nmultiple event streams, than parallel processors. The majority of such appli\u00adcations are written in either \nC or C++, using threads provided by the operating system. We will use Pthreads [21] as the canonical \nexample. The situation on Microsoft platforms is similar. Both the C and C++ languages are currently \nde.ned as single\u00adthreaded languages, without reference to threads. Correspondingly, compilers themselves \nare largely thread-unaware, and generate code as for a single-threaded application. In the absence of \nfur\u00adther restrictions, this allows compilers to perform transformations, such as reordering two assignments \nto independent variables, that do not preserve the meaning of multithreaded programs [29, 2, 25]. The \nOS thread libraries attempt to informally address this problem by prohibiting concurrent accesses to \nnormal variables or data races. To prevent such concurrent accesses, thread li\u00adbraries provide a collection \nof synchronization primitives such as pthread mutex lock(), which can be used to restrict shared vari\u00adable \naccess to one thread at a time. Implementations are then pro\u00adhibited from reordering synchronization \noperations with respect to ordinary memory operations in a given thread. This is enforced in a compiler \nby treating synchronization operations as opaque and po\u00adtentially modifying any shared location. It is \ntypically assumed that ordinary memory operations between synchronization operations may be freely reordered, \nsubject only to single-thread constraints. Unfortunately, informally describing multithreaded semantics \nas part of a threads library in the above way is insuf.cient for several reasons, as described in detail \nby Boehm [7]. Brie.y, the key reasons are as follows. The informal speci.cations provided by current \nlibraries are ambiguous at best; e.g., what is a data race and what precisely are the semantics of a \nprogram without a data race? Figure 1 struct s { char a; char b; } x; Thread 1: Thread 2: x.a=1; x.b=1; \nThread 1 is not equivalent to: struct s tmp = x; tmp.a = 1; x = tmp; Figure 2. Compiler transformations \nmust be aware of threads. Many compilers perform transformations similar to the one above when a is declared \nas a bit-.eld. The transformation may be visible to client code, since the update to b by thread 2 may \nbe overwritten by the store to the complete structure x. shows an example [1] where the answers to these \nquestions are unclear from the current speci.cation. Without precise semantics, it is hard to reason \nwhen a com\u00adpiler transformation will violate these semantics. For example, Boehm [7] shows that reasonable \n(common) interpretations of the current speci.cation do not preclude a class of compiler transformations \nthat effectively introduce new writes to poten\u00adtially shared variables. Examples include Figure 2 and \nspecula\u00adtive register promotion. As a result, conventional compilers can, and do, on rare occasions, \nintroduce data races, producing com\u00adpletely unexpected (and unintended) results without violating the \nletter of the speci.cation.  There are many situations in which the overhead of preventing concurrent \naccess to shared data is simply too high. Either ven\u00addors, or sometimes applications, generally provide \nfacilities for atomic (indivisible, safe for unprotected concurrent use) data access, without the use \nof locks. For example, gcc provides a family of sync intrinsics, Microsoft provides Interlocked operations, \nand the Linux kernel de.nes its own atomic opera\u00adtions (which have been regularly (ab)used in user-level \ncode). These solutions have not been satisfactory, both because they are not portable and because their \ninteractions with other shared memory variables are generally not carefully or clearly speci\u00ad.ed.  In \norder to address the above issues, an effort was begun several years ago to properly de.ne the semantics \nof multi-threaded C++ programs, speci.cally, the memory model, in the next revision of the C++ language \nstandard. Although this standard is not expected to be completely approved until 2009 or 2010, the core \nchanges to support threads and the memory model described here were recently voted into the C++ working \npaper, and are now also under discussion in the C committee. This effort has been proceeding in parallel, \nand cooperatively with, a similar effort for Microsoft s native compilation platforms [31]. The key result \nof the above effort is a memory model for mul\u00adtithreaded C++ programs that supports a simple programming \nmodel. This paper describes that model and several new funda\u00admental issues that had to be resolved along \nthe way. 1.1 State-of-the-art for Memory Models The memory model, or memory consistency model, speci.es \nthe values that a shared variable read in a multithreaded program is al\u00adlowed to return. The memory model \nclearly affects programmabil\u00adity. It also affects performance and portability by constraining the transformations \nthat any part of the system may perform. In prac\u00adtice, any part of the system (hardware or software) \nthat transforms the program must specify a memory model, and the models at the different system levels \nmust be compatible. For example, the C++ model constrains the transformations allowed by a C++ compiler. \nThe memory model for the hardware that will run the produced bi\u00adnary must not allow results that would \nbe illegal for the C++ model applied to the original program. There has been extensive work in memory \nmodels over the last two decades. Sequential consistency, de.ned by Lamport [24], is considered to be \nthe most intuitive model. It ensures that memory operations appear to occur in a single total order (i.e., \natomically); further, within this total order, the memory operations of a given thread appear in the \nprogram order for that thread. Unfortunately, sequential consistency restricts many common compiler and \nhardware optimizations [2]. There has been signi.\u00adcant academic progress in compiler algorithms to determine \nwhen a transformation is safe for sequential consistency [29, 30, 23] and in hardware that speculatively \nperforms traditional optimizations without violating sequential consistency [19, 28, 33, 15]. However, \ncurrent commercial compilers and most current commercial hard\u00adware do not preserve sequential consistency. \nTo overcome the performance limitations of sequential consis\u00adtency, hardware vendors and researchers \nhave proposed several re\u00adlaxed memory models [2]. These models allow various hardware optimizations, \nbut most are speci.ed at a low level and are gener\u00adally dif.cult to reason with for high level language \nprogrammers. They also limit certain compiler optimizations [25]. An alternative approach, called the \ndata-race-free [3, 1] or prop\u00aderly labeled [18, 17] models, has been proposed to achieve both the simple \nprogrammability of sequential consistency and the imple\u00admentation .exibility of the relaxed models. This \napproach is based on the observation that good programming practice dictates pro\u00adgrams be correctly synchronized \nor data-race-free. These models formalize correct programs as those that do not contain data races in \nany sequentially consistent execution. They guarantee sequential consistency to such data-race-free programs, \nand do not provide any guarantees whatsoever to others. Thus, the approach combines a simple programming \nmodel (sequential consistency) and high performance (by guaranteeing sequential consistency to only well\u00adwritten \nprograms). Different data-race-free models successively re\u00ad.ne the notion of a race to provide increasing \n.exibility, but requir\u00ading increasing amounts of information from the programmer. The data-race-free-0 \nmodel uses the simplest de.nition of a data race; i.e., two concurrent con.icting accesses (formalized \nlater). Among high-level programming languages, Ada 83 [32] was perhaps the .rst to adopt the approach \nof requiring synchronized accesses and leaving semantics unde.ned otherwise. As mentioned above, Pthreads \nfollows a similar approach, but neither Ada nor Pthreads formalized it suf.ciently. Recently, the Java \nmemory model underwent a major revision [25]. For programmers, for all practical purposes, the new Java \nmodel is data-race-free-0. How\u00adever, the safety guarantees of Java preclude leaving the semantics with \ndata races as unde.ned. Much of the Java effort therefore focused on de.ning these semantics in a way \nthat preserves max\u00adimum implementation .exibility without violating the safety and security guarantees \nof Java. Nevertheless, the Java model does preclude some compiler optimizations and the full model is \nquite complex [25]. Since C++ is not type safe, neither the restrictions nor the complexity of the full \nJava model appear justi.ed for C++. 1.2 The C++ Model and Contributions of this Paper The model chosen \nfor C++ is an adaptation of data-race-free-0; i.e., it guarantees sequential consistency for programs \nwithout data races and has unde.ned semantics in the presence of a data race. Given the recent work on \nthe Java model described above, the data\u00adrace-free-0 model may seem an obvious choice. However, when \nthis process began, there were several factors that prevented using data-race-free-0, some of which were \nexposed after conclusion of the Java work and are also relevant to that work. This paper pro\u00advides an \nunderstanding of those factors, shows how they were re\u00adsolved to make the data-race-free-0 model acceptable \nto C++ devel\u00adopers and most hardware vendors, and provides the .rst published description of the C++ \nmodel in the full context of prior work.1 Speci.cally, there are three main issues we discuss in this \npaper: (1) Sequentially consistent atomics: The data-race-free mod\u00adels require that all operations that \nare not ordinary data opera\u00adtions (e.g., synchronization operations, atomics in C++, volatiles in Java2) \nappear sequentially consistent. The recent advent of mul\u00adticore systems exposed important hardware optimizations \nthat ap\u00adpear to con.ict with this requirement, initially resulting in most hardware vendors and many \nsoftware developers opposing it. We describe the con.ict and show that unfettered exploitation of such \nhardware optimizations leads to models that are too hard to formal\u00adize and/or use. These arguments were \nlargely responsible in con\u00advincing hardware vendors (e.g., AMD and Intel) to develop speci\u00ad.cations that \nare now consistent with the use of sequentially con\u00adsistent atomics. (2) Trylock and impact on the de.nition \nof a data race: Synchronization primitives such as trylock can be used in non\u00adintuitive ways that previously \nwould have required more complex de.nition of data races and/or overly strict fences. We show a simple \nway to get around this problem. (3) Semantics for data races: We do not provide any semantics for programs \nwith data races. This was not uncontroversial; we explain the arguments for this position in Section \n5.  We believe that sequentially consistent atomics provide a pro\u00adgramming model that is both easier \nto use and describe than the alternatives, and is the model we should strive for. However, it be\u00adcame \nclear during the standardization process that exclusive sup\u00adport for sequentially consistent atomics \nwas not viable, largely for two reasons: It is suf.ciently expensive to implement on some existing processors \nthat an experts-only alternative for performance\u00adcritical code was felt to be necessary. It is unclear \nto us whether this need will persist inde.nitely.  Existing code is often written in a way that assumes \nweak memory ordering semantics and relies on the programmer to explicitly provide the required platform-dependent \nhardware instructions to enforce the necessary ordering. The Linux kernel is a good example of this. \nIt is often easier to migrate such code if we provide primitives closer to the semantics assumed (currently \nsometimes incorrectly) by such code.  As a result, the C++ working paper provides for both sequen\u00adtially \nconsistent atomics, and a mechanism to weaken memory or\u00addering with explicit speci.cations. We refer \nto the latter as low-level atomics. This unfortunately requires a signi.cantly more compli\u00adcated memory \nmodel. Here we .rst present the simpler model, as we would expect most programmers to use it. This is \nsuf.cient to understand the core points of this paper. We then return to a presentation closer to the \nstandards working paper that does support the addition of low\u00adlevel atomics, and outline the proofs that \nthe two speci.cations are equivalent. 1 Much of the work here, aside from the discussion of architectural \nissues, is described less formally in earlier unrefereed standards committee papers, notably [9, 10, \n11, 13, 27] and their predecessors. 2 See [6] for the reasons they are not called volatile in C++. 2. \nThe C++ Model Without Low-Level Atomics We try to specify memory models in this paper suf.ciently pre\u00adcisely \nthat if we had a completely formal sequential semantics, this could be translated into a completely formal \ndescription of the multi-threaded semantics. This section assumes only a single .avor of atomics; i.e., \nthe default, sequentially consistent atomics in the standard. Memory operations are viewed as operating \non abstract memory locations. Each scalar value occupies a separate memory location, except that contiguous \nsequences of bit-.elds inside the same in\u00adnermost struct or class declaration are viewed as a single \nloca\u00adtion.3 Thus the .elds in the struct s of Figure 2 are separately updateable locations unless we \nchange both .eld declarations to bit-.elds. The remainder of the C++ standard was modi.ed to de.ne a \nsequenced-before relation on memory operations performed by a single thread [14]. This is analogous to \nthe program order relation in Java and other work on memory models. Unlike prior work, this is only a \npartial order per thread, re.ecting unde.ned argument evaluation order. De.ne a memory action to consist \nof: 1. The type of action; i.e., lock, unlock, atomic load, atomic store, atomic read-modify-write, load, \nor store. All but the last two are customarily referred to as synchronization operations, since they \nare used to communicate between threads. The last two are referred to as data operations. 2. A label \nidentifying the corresponding program point. 3. The values read and written.  Bit-.eld updates can \nbe modeled as a load of the sequence of contiguous bit-.elds, followed by a store to the entire sequence. \nDe.ne a thread execution to be a set of memory actions, to\u00adgether with a partial order corresponding \nto the sequenced-before ordering. De.ne a sequentially consistent execution of a program to be a set \nof thread executions, together with a total order <T on all the memory actions, which satis.es the constraints: \n1. Each thread execution is internally consistent, in that it corre\u00adsponds to a correct sequential execution \nof that thread, given the values read from memory, and respects the ordering of op\u00aderations implied by \nthe sequenced-before relation. 2. T is consistent with the sequenced-before orders; i.e., if a is sequenced \nbefore b then a<T b. 3. Each load, lock, and read-modify-write operation reads the value from the last \npreceding write to the same location ac\u00adcording to <T . The last operation on a given lock preceding \nan unlock must be a lock operation performed by the same thread.  Effectively this requires that <T \nis just an interleaving of the individual thread actions. Two memory operations con.ict if they access \nthe same mem\u00adory location, and at least one of them is a store, atomic store, or atomic read-modify-write \noperation. In a sequentially consistent execution, two memory operations from different threads form \na type 1 data race if they con.ict, at least one of them is a data op\u00aderation, and they are adjacent \nin <T (i.e., they may be executed concurrently). We can now specify the C++ memory model simply as: If \na program (on a given input) has a sequentially consistent ex\u00adecution with a (type 1) data race, then \nits behavior is unde.ned. 3 Zero-length bit-.elds can be used to break such sequences. This extends \nthe existing convention for the use of zero-length bit-.elds. T1 T2 x = 42; while (trylock(l) == success) \nlock(l); unlock(l); assert(x == 42); Figure 3. Undesirable use of trylock. Otherwise, the program (on \nthe same input) behaves according to one if its sequentially consistent executions. 2.1 Optimizations \nAllowed by the Model Previous work [3, 18, 1, 12] shows that with the above model, hard\u00adware and compilers \nmay freely reorder memory operation M1 se\u00adquenced before memory operation M2 if the reordering is allowed \nby intra-thread semantics and: (1) M1 is a data operation and M2 is a read synchronization operation, \nor (2) M1 is write synchro\u00adnization and M2 is data, or (3) M1 and M2 are both data with no synchronization \nsequence-ordered between them. Additionally, when locks and unlocks are used in well\u00adstructured ways, \nthe following reorderings between M1 sequenced-before M2 are safe (assuming they are allowed by intra\u00adthread \nsemantics) [1]: M1 is data and M2 is the write of a lock operation; or M1 is unlock and M2 is either \na read or write of a lock. Finally, previous work also discusses the hardware optimization of executing \na write non-atomically [2]; i.e., making the value of a data write visible to a thread before it becomes \nvisible to all threads. It has been shown that for the model described above, data writes and writes \nfrom well-structured locks and unlocks can be executed non-atomically [1]. We note an unfortunate overload \nof the term atomic (1) the above usage to describe how a write is executed in hardware and (2) the C++ \nquali.er to denote a special class of memory opera\u00adtions. The default atomic write in C++ (quali.ed as \nsequentially consistent and discussed here) needs to be executed atomically by hardware, but another \nclass of low-level atomic writes (discussed later) need not be executed atomically in the above sense, \nthough it is still atomic in the sense that no other individual thread can see a partially updated value. \nThe model imposes signi.cant restrictions for synchronization operations. For all practical purposes, \nsynchronization operations must appear sequentially consistent with respect to each other. This means \nthat atomic operations must execute in sequenced\u00adbefore order and atomic writes must execute atomically \n[2], re\u00adferred to henceforth as the sequenced-order and write-atomicity re\u00adquirements respectively. For \nlocks and unlocks, their special usage restrictions enable some optimizations as mentioned above, with\u00adout \nviolating the appearance of sequential consistency. We make some further observations on allowable optimizations \nin [10]. 3. Making Trylock Ef.cient The preceding gives undesirably strong semantics to programs using \ncalls that try to acquire a lock without blocking, such as pthread mutex trylock(), or lock acquisitions \nwith timeouts. Consider the example in Figure 3, equivalent to one found in [12]: This program essentially \ninverts the sense of the lock l by hav\u00ading a thread wait for T1 to acquire the lock, instead of waiting \nfor the lock to be released. This is clearly not a desirable programming idiom.4 4 One of the reviewers \npoints out that trylock could be, and sometimes is, used with a lock that is never released to ensure \nthat an action is performed by only a single thread. This fails in our model, re.ecting the fact Based \non a conventional interpretation of trylock(), in a se\u00adquentially consistent execution, the assertion \nin T2 cannot be exe\u00adcuted (i.e., appear in the <T ordering) until after T1 acquires the lock and until \nafter it assigns x the value of 42. The program is therefore data-race-free and, by our current semantics, \nthe asser\u00adtion cannot fail. The dif.culty is that the assertion can fail if either the com\u00adpiler or hardware \nreorders the two statements executed by T1, by moving the assignment after the lock. Prohibiting such \nreordering on many architectures requires a memory fence before the lock. As mentioned in Section 2.1, \nfor well-structured uses of locks and unlocks, such a reordering is safe. Thus, this fence represents \nun\u00adnecessary overhead, possibly doubling the cost of lock acquisition, without bene.t to reasonably written \ncode. As pointed out in [12], many lock implementations as a result fail to enforce this require\u00adment, \neven though it appears to already exist in Posix. There is potentially a similar issue with reordering \nthe .nal failing trylock() and assertion in thread T2. Even Posix tries to allow this reordering. The \nfundamental dif.culty here is that trylock() reads the value written by T1 s lock to infer that the lock \nhas been acquired. This communication from a lock write to a trylock read is used to synchronize the \naccesses on x. We wish to prevent such use to avoid paying the overhead of the additional fence on all \nlocks. Pre\u00advious work has achieved this by explicitly distinguishing between different types of synchronization \noperations and/or rede.ning data operations that are separated only by such synchronization as races. \nFor example, the data-race-free-1 model distinguishes pairable and unpairable synchronization [1], allowing \nonly the former to prevent a data race. The Java memory model requires con.icting ordinary (data) operations \nto be explicitly ordered by volatiles and/or pairs of locks/unlocks to avoid a data race [25]. A happens-before \nrela\u00adtion is used to formalize these notions. Instead of introducing the complexity of different types \nof syn\u00adchronization or a happens-before relationship to de.ne a data race, we propose a simple solution. \nWe change the speci.cation of trylock(), though preferably not its implementation, to not guarantee that \nit will succeed if the lock is available. The C++0x speci.cation is expected to allow trylock to spuriously \nfail in this manner. This effectively prevents a failed trylock from reli\u00adably revealing anything about \nthe state of the lock (and hence the write of the lock operation has no means to convey any synchro\u00adnization \ninformation). In particular, it is now clear that the assertion in the above example may fail. The execution \nin which the assertion fails is now sequentially consistent; it simply involved a spurious trylock() \nfailure. Although simple, to our knowledge, this is the .rst solution that eliminates the need to provide \na fence before a lock, while still maintaining a simple de.nition of a race (i.e., con.icting accesses \nadjacent to each other in the total order). For the remainder of this discussion, the reader may assume \nthat a trylock() allowing spurious failures is included. A successful trylock() is treated as lock(). \nAn unsuccessful trylock() is treated by the memory model as a no-op. 4. The Cost of Sequentially Consistent \nAtomics As mentioned in Section 2.1, the model so far requires atomics to appear sequentially consistent. \nWhen this work began, there was concern from various hardware vendors and software developers that many \ncurrent implementations fail to provide sequential consistency for this use, while we want to insist \non it. In our model, the equivalent can be accomplished with more straightforward semantics with an atomic \ntest and set. Initially X=Y=0 T1T2T3 T4 X=1 Y=1 r1=X r3=Y fence fence r2=Y r4=X r1=1, r2=0, r3=1, r4=0 \nviolates write atomicity Figure 4. Write-atomicity may be too expensive for Independent\u00adReads-Independent-Writes \n(IRIW). that this requirement was excessive and unnecessarily restricted performance. In principle, the \nperformance impact of these requirements should be restricted only to synchronization operations. Unfor\u00adtunately, \nmost current processor instruction sets do not directly distinguish synchronization operations (Itanium \nis a notable ex\u00adception). The most common way for compilers to convey memory ordering requirements to \nhardware is through fence or memory barrier instructions. Fences were designed primarily to convey the \nsequenced-order (program order) requirement. For example, the strongest fences ensure that all memory \noperations sequenced-before the fence will appear to execute before all memory operations sequenced after \nthe fence. Other variants impose ordering between different subsets of memory operations; e.g., a Store|Load \nfence orders previous stores before later loads. Some processors ensure certain orderings by default, \nremoving the need for fences for those cases; e.g., AMD64 and Intel 64 implicitly provide Load|Load and \nLoad|Store fence semantics after each load and Store|Store fence semantics after each store. When this \nwork began, many speci.cations of fences were am\u00adbiguous about their impact on the hardware write-atomicity \nre\u00adquirement as described in Section 2.1 (notable exceptions are Alpha and Sun). Some processor vendors \nargued that full write-atomicity was too expensive and software developers argued that weak ver\u00adsions \nof write-atomicity would suf.ce. Section 4.1 discusses the cost of enforcing write-atomicity in hardware \nand why it may be unnecessary for some programs. Section 4.2 then systematically shows how meaningful \nrelaxations of write-atomicity result in un\u00adintended consequences for other programs. As a result, despite \nmany attempts, it was dif.cult to formalize semantics for synchro\u00adnization operations that were meaningfully \nweaker than sequential consistency for hardware and provided a simple enough interface for most programmers. \nIn this section, all variables in all examples are atomics. (The syntax does not re.ect the current C++ \nworking paper.) 4.1 Cost of Enforcing Write-Atomicity Consider the example in Figure 4, referred to \nas the Independent\u00adReads-Independent-Writes (IRIW) example. The fences ensure that the reads will execute \nin program order, but this does not guarantee sequential consistency if the writes execute non-atomically. \nFor example, if the writes to X and Y propagate in different orders to threads T3 and T4, the outcome \nin the .gure violating sequential consistency can occur (T3 sees the new value of X and the old value \nof Y and vice versa for T4). Systems with ownership-based invalidation protocols and single\u00adcore/single-threaded \nprocessors can avoid the non-sequentially consistent outcome in a straightforward way. Consider a typical \nsuch system employing a directory-based cache coherence pro\u00adtocol. If T1 does not already have ownership \nof X, then it must request it from the directory. The directory then sends invalidations for all cached \ncopies of X and either it or T1 collect acknowledge\u00adments for these invalidations. To see the new value \nof X, T3 must .rst go to the directory which will forward the request to T1. To ensure writes appear \natomic, the system simply needs to ensure that all copies of X are invalidated (i.e., all acknowledgements \nre\u00adceived) before T3 gets the updated copy of X, and analogously, all outstanding copies of Y are invalidated \nbefore T4 gets an updated copy of Y. Now it is no longer possible for both T3 and T4 to read the old \nvalues of Y and X respectively. The key to ensuring sequential consistency for the above ex\u00adample is \nthat a read is not allowed to return a new value for the accessed location until all older copies of \nthat location are invali\u00addated. This requirement can be somewhat relaxed further for pro\u00adcessors that \nrequire explicit fences for ordering reads the read can return a new value as long as a subsequent fence \nwaits for all old copies of that location to be invalidated. Following [2], we refer to this as the read-others \n-write-early restriction. Note that as described above, the read-others -write-early re\u00adstriction does \nnot require any global serialization point for all oper\u00adations to all locations (e.g., a bus). It simply \nrequires a serialization point for reads and writes to the same location, which is usually provided by \nthe cache coherence protocol (the need for which is widely accepted). Further, this per-location serialization \nis techni\u00adcally only required for atomic operations. Unfortunately, the use of fences for memory ordering \nobfuscates which memory accesses are the atomic ones; therefore, without a mechanism to distinguish in\u00addividual \nwrites as atomic, our system must preserve write atomicity for all writes. The advent of multicore and \nsimultaneous multithreading (SMT), where threads may share a data cache or store queues, has pre\u00adviously \nunexplored implications for the read-others -write-early restriction. These architectures offer tempting \nperformance opti\u00admization opportunities that can violate sequential consistency for the IRIW example \nas follows. Suppose T1 and T3 share an L1 writethrough data cache. Suppose T3 s read of X occurs shortly \nafter T1 s write to X. It is tempting to return the new value of X to T3, even if T1 s ownership request \nfor X has not yet made its way through the lower levels of the cache hierarchy. If T2 and T4 share a \ncache, an analogous situation can occur for their write and read of Y T4 reads the new value of Y even \nbefore T2 s ownership request reaches the rest of the memory system. It is now possible that both T3 \nand T4 read the old values of Y and X respectively (from their caches), violating sequential consistency. \nThus, while previously the read-others -write-early restriction appeared to have (acceptable) implications \nonly for the main mem\u00adory system and the cache coherence protocol, new SMT and multi\u00adcore architectures \nmove this restriction all the way to the .rst level cache and even within the processor core (if there \nare shared store queues). At the same time, most programmers agree that the IRIW code does not represent \na useful programming idiom, and imposing restrictions to provide sequential consistency for it appears \nexces\u00adsive and unnecessary. For this reason, some existing machines do not provide ef.cient methods for \nensuring sequential consistency for IRIW and there was initial reluctance to adopting sequentially consistent \nsemantics for atomics. 4.2 Unintended Consequences of Relaxing Write-Atomicity We next show examples \nwhere relaxing the read-others -write\u00adearly requirement in ways described for IRIW can give unaccept\u00adable \nresults. 4.2.1 Write-to-Read Causality (WRC) Must be Upheld Figure 5 illustrates a simple causal effect. \nThread T1 writes a new value of X, T2 reads it, executes a fence, and writes a new value of Y. T3 reads \nthe new Y, executes a fence, and then reads X. Sequential consistency requires that T3 return the new \nvalue for Initially X=Y=0 T1 T2 T3 X=1 r1=X r2=Y fence fence Y=1 r3=X r1=1, r2=1, r3=0 violates write \natomicity Figure 5. Write-to-Read Causality (WRC) must be respected Initially X=Y=0 T1 T2 T3 X=1 r1=X \nY=1 fence fence r2=Y r3=X r1=1, r2=0, r3=0 violates write atomicity Figure 6. Should Read-to-Write Causality \n(RWC) be respected? X. We call this example Write-to-Read Causality (WRC) because a write of a new value \nby a thread followed by a read of the same value by another thread is used to establish a causal connection \nbetween the threads. Most programmers agree that this causality should be respected, and the violation \nof sequential consistency shown in Figure 5 should not be allowed. Now consider applying the IRIW optimization \nto the WRC example. Suppose T1 and T2 share an L1 writethrough cache and T3 is on a separate system node. \nSuppose T2 is allowed to read T1 s update to X early. Then it may be possible for T3 to receive the invalidation \nfor Y before that for X, resulting in T3 reading the new value of Y and the old value of X. One solution \nthat preserves sequential consistency for WRC while retaining much of the impact of the IRIW optimization \nis to ensure that writes separated by a fence from a given system node are seen in the same order by \nall nodes. This solution continues to allow reads within the node to return a new value early. We next \nshow that this solution unfortunately does not suf.ce for other programs.  4.2.2 Read-to-Write Causality \n(RWC) Figure 6 illustrates an example similar to WRC, except that the causal relationship between T2 \nand T3 is established by T2 s read of an old value followed by T3 s write of a new value to Y. The IRIW \noptimization can violate sequential consistency for this ex\u00adample similar to WRC. Unfortunately, the \nWRC solution (ordering writes separated by fences from the same node) does not work in this case since \neach node (T1/T2 or T3) contains at most one write. Unlike the WRC example, some programmers may .nd \nthis violation of read-to-write causality acceptable; however, it is no longer as clear-cut as the IRIW \ncode.  4.2.3 Subtle Interplay Between Coherence and Causality Our .nal example illustrates how IRIW \nstyle optimizations can interfere in a non-intuitive way with a subtle interplay between cache coherence \nand write-to-read causality, referred to as CC. Cache coherence is a widely accepted property that guarantees \nthat writes to the same location are seen in the same order by all threads. There is wide consensus that \nthis property must be respected (for synchronization operations) to write meaningful code. There is also \nconsensus that write-to-read causality must be respected. Figure 7 shows that IRIW style optimizations \nmake it dif.cult to compose cache coherence and simple write-to-read causality interactions. In Figure \n7, assume again that T1 and T2 are on the same node with a shared writethrough L1 cache. Assume T3 and \nT4 Initially X=Y=0 T1T2T3 T4 X=1 r1=X Y=1 r3=X fence fence fence r2=Y X=2 r4=X r1=1, r2=0, r3=2, r4=1 \nviolates write atomicity Figure 7. CC: Subtle interplay between cache coherence and write-to-read causality. \nare on separate nodes. As with RWC, T2 reads T1 s new value (1) of X early, executes a fence, and reads \nan old value for Y. Now assume T3 writes the new value of Y, executes a fence, and then also updates \nX to 2. After all of T3 s operations complete in the memory system, T4 reads X (returns 2) and executes \na fence. At this point, assume T1 s write permission request for X goes through the memory hierarchy \nand all its invalidations are done. Then when T4 reads X for the second time, it gets 1. Thus, the IRIW \noptimization can result in the violation of se\u00adquential consistency shown in Figure 7. As with RWC, the \nWRC solution does not apply because no node has more than one write. No other simple solutions that would \nretain the advantage of the IRIW optimization are apparent. A memory model that would violate sequential \nconsistency for the above example seems dif.cult to formulate and reason with because such a model would \nnot be able to easily compose cache coherence and write-to-read causality. Speci.cally, in the above \nexample, T4 establishes that X=2 was serialized in the memory hierarchy before X=1. Cache coherence therefore \nallows reasoning that T2 s read of 1 for X must have occurred after T3 s write of 2 for X. Using a write-to-read \ncausality argument leads to the inference that T3 s write of Y must have occurred before T2 s read of \nY, which should return 1 and not 0. Thus, the IRIW optimization and the consequent result in Fig\u00adure \n7 precludes inferences from a simple and intuitive composition of cache coherence and write-to-read causality. \n 4.3 Implications for Current Processors The above examples show that a departure from sequential con\u00adsistency \nfor synchronization operations can lead to subtle non\u00adintuitive behaviors that appear dif.cult to formalize \nin an intuitive fashion. We therefore retained sequential consistency semantics for default atomics and \nsynchronization operations. In.uenced by our work, the new AMD64 [4] and Intel 64 [22] memory ordering \nspeci.cations now provide a clear way to guar\u00adantee sequentially consistent semantics, although these \nspeci.ca\u00adtions require atomic writes to be mapped to atomic xchg instruc\u00adtions (which are read-modify-write \ninstructions). Hardware now need only ensure that xchg writes execute atomically. Additionally, with \nthese speci.cations, xchg also implicitly ensures semantics of a Store|Load fence, removing the need \nfor an explicit fence after an atomic store (such a fence would otherwise be required for the sequenced-before \nrequirement). While it may seem cumbersome and inef.cient to convert atomic stores to read-modify-writes, \nthe following observations make this a reasonable compromise: (1) it is better to pay a penalty on stores \nthan on loads since the former are less frequent and (2) the Store|Load fence replaced by the read-modify-write \nis as expensive on many processors today. There are three other approaches that could be used to achieve \nsequential consistency for atomics, without requiring converting all atomic stores to read-modify-writes. \nFirst, the processor ISA could provide a simple mechanism to distinguish atomic stores (e.g., Itanium \ns st.rel) instead of having to use read-modify-writes. Hardware could then ensure that these stores execute \natomically. Second, write-atomicity could simply be provided for all writes, as with Sun s total store \norder (TSO) and the Alpha memory mod\u00adels. The mechanisms to implement this are similar to those for the \nread-modify-write and st.rel type solutions, except that they need to be invoked for all writes. Third, \nmany processors today use speculative approaches to reorder memory operations beyond that allowed by \nthe memory model; e.g., AMD s and Intel s processors speculatively reorder loads even though the memory \nmodel disallows it [19]. A similar approach could be used to allow a read to return a value early from \na shared cache. So far, the literature on such optimizations has focused on bene.ts for speculatively \nrelaxing the sequenced-order requirement; we are not aware of prior work on understanding the impact \nof speculatively relaxing write-atomicity. To the best of our knowledge, most existing machines today \nprovide write-atomicity by default. A notable exception is some PowerPC machines which require a particularly \nexpensive form of a fence instruction after atomic loads to achieve sequentially consistent results for \nthe RWC and CC examples. 5. Semantics of Data Races For languages like Java, it is critical to de.ne \nthe semantics of all programs, including those with data races. Java must support the execution of untrusted \nsandboxed code. Clearly such code can introduce data races, and the language must guarantee that at least \nbasic security properties are not violated, even in the presence of such races. Hence the Java memory \nmodel [25] is careful to give reasonable semantics to programs with data races, even at the cost of signi.cant \ncomplexity in the speci.cation. For C++, there is no such issue. Initially, there was still some concern \nthat we should limit the allowable behavior for programs with races. However, in the end, we decided \nto leave the semantics of such programs completely unde.ned. In the current working paper for the C++ \nstandard, in spite of discussions such as [26], there are no benign data races. The basic arguments for \nunde.ned data race semantics in C++ are: 1. Although generally under-appreciated, it is effectively the \nsta\u00adtus quo. Pthreads states [21] Applications shall ensure that ac\u00adcess to any memory location by more \nthan one thread of control (threads or processes) is restricted such that no thread of con\u00adtrol can read \nor modify a memory location while another thread of control may be modifying it. As we mention in the \nintro\u00adduction, Ada earlier took the same approach. The intent behind win32 threads appears to have been \nsimilar. 2. Since the C++ working paper provides low-level atomics with very weak, and hence cheaply \nimplementable, ordering prop\u00aderties, there is little to be gained by allowing races, other than allowing \ncode to be obfuscated. We effectively require only that such races be annotated by the programmer. Since \nthe result is usually exceedingly subtle, we believe this should be required by any reasonable coding \nstandard in any case. 3. Giving Java-like semantics to data races may greatly increase the cost of some \nC++ constructs. It would presumably require that we not expose uninitialized virtual function tables, \neven in the event of a race, since those could otherwise result in a wild branch. This in turn often \nrequires fences on object con\u00adstruction. In Java, this is arguably less major, since object con\u00adstruction \nis always associated with memory allocation, which typically already carries some cost. This does not \napply to C++.  4. Current compiler optimizations often assume that objects do not change unless there \nis an intervening assignment through a potential alias. Violating such a built-in assumption can cause \nvery complicated effects that will be very hard to explain to a programmer, or to delimit in the standard. \nWe believe this assumption is suf.ciently ingrained in current optimizers that it would be very dif.cult \nto effectively remove it. As an example of the last phenomenon, consider a relatively simple example, \nwhich does not include any synchronization code. Assume x is a shared global and everything else is local: \nunsigned i = x; if (i < 2) { foo: ... switch (i) { case 0: ...; break; case 1: ...; break; default: ...; \n } } Assume the code at label foo is fairly complex, and forces i to be spilled and that the switch implementation \nuses a branch table. (In reality, the second assumption might require a larger switch statement.) The \ncompiler now performs the following reasonable optimiza\u00adtions: It notices that i and x (in its view \nof the world) contain the same values. Hence when i is spilled, there is no need to store it; the value \ncan just be reloaded from x.  It notices, e.g. by value range analysis as in [20] that the switch expression \nis either 0 or 1, and hence eliminates the bounds check on the branch table access and the branch to \nthe default case.  Now consider the case in which, unbeknownst to the compiler, there is actually a \nrace on x, and its value changes to 5 during the execution of the code labeled by foo. The results are: \n1. When i is reloaded for the evaluation of the switch expression, it gets the value 5 instead of its \noriginal value. 2. The branch table is accessed with an out-of-bounds index of 5, resulting in a garbage \nbranch target. 3. We take a wild branch, to arbitrary code.  The result would only be slightly less \nsurprising if the switch tested x instead of i. It seems dif.cult to explain either behavior in a language \nstandard in any way other than to leave it completely unde.ned.5 Finally, even after the large effort \nto de.ne semantics for data races in Java, recent work by David Aspinall and Jaroslav Sevcik has uncovered \nsome bugs in that speci.cation [5]. Thus we decided to leave the semantics of races unde.ned, in spite \nof some potential negative effects on debuggability. 5.1 Compiler-introduced Data Races Since we expect \nthe next C++ standard to completely prohibit data races at the source level, it will not be possible \nto write a 5 It is of course not impossible for compilers to avoid this kind of behavior. Java compilers \nmust avoid it, and C++ compilers rarely exhibit it in practice. But our experience has been that compiler \nwriters are disinclined to promise it will not happen, particularly since current Posix and C or C++ \nstandards clearly allow the fundamental assumption that ordinary variables do not change asynchronously. \nportable C++-to-C++ source translator that introduces data races. In particular, source-to-source optimizers \nmay never introduce a data race. This requirement disallows some important optimizations; speci.cally, \nit prevents such an optimizer from introducing spec\u00adulative loads as well as speculative stores. Optimizations \nsuch as Partial Redundancy Elimination commonly introduce speculative loads. For example, the requirement \nprevents a potentially shared but loop invariant variable x from being loaded into a register once outside \nthe loop, if there is any possibility that it might not actu\u00adally be referenced in the loop. The requirement \nalso prevents a load from being scheduled before it was clear that the value was actually needed, though \na prefetch of such a value would generally still be correct. It is important to note that this restriction \napplies only if the target language of the optimizer prohibits data races. We are not aware of any hardware \narchitectures that do so. The above opti\u00admizations introduce potentially racing loads of values whose \nresults are not used. Machine architectures allow those without altering the meaning of the remaining \ncode. Hence compilers that target a con\u00adventional hardware architecture may continue to insert speculative \nloads. Such optimizers may generally still not insert speculative stores, since those could overwrite \na legitimate store in another thread, thus changing the semantics of the program, even when the original \nprogram contained no race. If a compiler were to translate to an architecture that gives un\u00adde.ned semantics \nfor races at the machine level, then the compiler would be prohibited from introducing speculative loads \nas well. This arises for example, if the target is a virtual machine that itself detects races as in \n[16]. And that is as it should be. Any introduced speculative loads could race with a store in another \nthread, thus re\u00adsulting in the diagnosis of a race where the original program had none.6 By the same \nreasoning, a C++-to-C++ translator should not introduce races, since it is not aware of the .nal target. \n6. C++ Model Supporting Low-Level Atomics As mentioned earlier, enforcing sequential consistency is expensive \non some platforms, and there are some frequently used idioms for which sequential consistency is not \nrequired. For example, it is common to use counters that are frequently incremented by multiple threads, \nbut are only read after all threads complete. Our semantics, since they guarantee sequential consis\u00adtency, \nrequire that all memory updates performed prior to a counter update become visible after any later counter \nupdate in another thread. This property typically requires the addition of fences, which in some cases \nare substantially more expensive than the rest of the operation. Similarly, in the absence of context \ninformation, sequentially consistent atomic stores often require two fences, one before and one after \nthe store. The .rst ensures that memory operations se\u00adquenced before the store are visible to any thread \nthat sees the result of the store. The second, often more expensive, fence ensures that the store is \nnot reordered with respect to a later atomic load. The latter property is required when implementing \nDekker s algorithm, for example, but is often unnecessary. The actual C++ atomics library speci.cation \n(see [13] or Chap\u00adter 29 of [14]) supports low-level atomic operations that allow the programmer to explicitly \nspecify memory ordering constraints, al\u00ad 6 Arguably, the same holds for a physical machine target if \nthe resulting execution is analyzed for races, as in [26]. Here the race semantics are well de.ned, but \ncompiler-introduced races still result in undesirable output. Unfortunately [26] does not make as clear \na distinction between source\u00adlevel and machine-level races. lowing close to optimal implementations \nof these idioms by very careful programmers. As presented so far, our memory model inherently does not \nsup\u00adport non-sequentially-consistent synchronization operations. As\u00adsume that incr increments a variable \nwithout enforcing any sort of visibility ordering. (Alpha, ARM, and PowerPC, for example, would allow \na signi.cantly cheaper implementation of incr than its sequentially consistent counterpart.) Now consider \nthe follow\u00ading, where x and y are atomic, z is not, and all are initially zero: Thread 1: Thread 2: incr(x); \nincr(y); r1=y; r2=x; if(r1==0) if(r2==0) z =1; z=2; In a sequentially consistent execution, r1 and r2 \ncannot both be zero, and hence there is no data race in a sequentially consistent execution. However, \nany implementation that takes advantage of the lack of ordering between the atomic operations will allow \nboth values to be zero, and hence potentially encounter a data race. Hence this should be given unde.ned \nsemantics by all the arguments in the preceding section. But this requires that we de.ne a notion of \ndata race in terms of the actual language semantics, not in terms of a sequentially consistent execution. \nHere we give an alternate memory model speci.cation that allows extension to low-level atomics. This \ndescription is a more mathematical formulation of the C++ working paper (see [27, 8] or primarily section \n1.10 of [14]). For the time being, we continue to omit low-level atomics. Thus the model presented here \nis intended to be equivalent to the earlier one (as demonstrated in the following sections). The last \nsection of this paper then describes brie.y how low-level atomics are actually incorporated into the \nC++ working paper. This version parallels the simpler parts of the Java memory model [25] much more closely \nthan our original version, though there are some differences. De.ne a program execution to be 1. A set \nof thread executions 2. A mapping W from atomic loads and atomic read-modify\u00adwrite operations to atomic \nstores and atomic-read-modify-write operations to the same location, and from lock acquisitions to lock \nreleases of the same lock. W is intended to map each read-like operation to the corresponding write whose \nvalue it observes. 3. An irre.exive total order <S of atomic operations, intended to re.ect the global \norder in which they are executed. 7  We de.ne an atomic load, atomic read-modify-write, or lock acquisition \nto be an acquire operation on the read locations. Con\u00adversely, an atomic store, atomic-read-modify-write, \nor lock release is a release operation on the affected locations. We de.ne a memory action A to synchronize \nwith a memory action B if B is an acquire operation on a location, A is a release operation on the same \nloca\u00adtion, and W (B)= A. We de.ne happens-before (<hb) to be the smallest relation on memory actions \nsuch that If a is sequenced before b, then a happens before b.  If a synchronizes with b, then a happens \nbefore b.  7 This corresponds roughly to Java s synchronization order. In the C++ approach, the requirement \nfor this total order is speci.ed as part of the atomic library, in chapter 29 of [14]. If a happens before \nb and b happens before c, then a happens before c. We de.ne a visible side effect with respect to a load \nor read\u00admodify-write operation b to be an update (store or read-modify\u00adwrite operation) a to the same \nlocation l such that a happens before b, but there is no intervening update c to l such that a happens \nbefore c and c happens before b. (The intent is that in the absence of races, ordinary loads see the \nunique visible side effect.) We de.ne a program execution to be consistent (a notion that is not explicit \nin the C++ working paper) if 1. Each thread execution is internally consistent, given the values read \nfrom memory. 2. The order <S is consistent with happens-before, i.e. if a hap\u00adpens before b, then a<S \nb. 3. For each non-atomic load l, W (a) is a visible side effect with respect to l. (If there are no \ndata races, it is the unique visible side effect corresponding to l.) 4. For each atomic (i.e. synchronization) \nload or atomic-read\u00admodify-write operation a, then W (a) is the last preceding up\u00addate in <S corresponding \nto the same location. 5. Lock and unlock operations on each individual lock are totally ordered by happens-before, \nand alternate in each such individ\u00adual order. Furthermore each lock operation is sequenced before the \nnext unlock operation in this total order, if there is one, i.e. locks are released only by the acquiring \nthread.  A consistent execution contains a type 2 data race if two data accesses to the same memory \nlocation are unordered by happens-before. We can now specify the C++ memory model sim\u00adply as: If a program \n(on a given input) has a consistent execution with a (type 2) data race, then its behavior is unde.ned. \n Otherwise, the program (on the same input) behaves according to one if its consistent executions. \n The following two sections show that this characterization is equivalent to the original one in section \n2: First, data-race-free programs still have sequentially consistent semantics. Second, the two notions \nof data race are equivalent. We then conclude with an outline of the additional changes needed to actually \ninclude low\u00adlevel atomics. 7. Sequential Consistency for Data-Race-Free Programs THEOREM 7.1. The model \nde.ned in Section 6 provides sequen\u00adtial consistency to programs whose consistent executions do not contain \na type 2 data race. Proof Consider a type 2 data-race-free program and its con\u00adsistent execution (on \nthe given input). We need to show that this execution exhibits sequentially consistent behavior. The \ncorresponding happens-before relation (<hb) and synchro\u00adnization order <S are irre.exive and consistent, \nand hence can be extended to a strict total order <T . Clearly the actions of each thread appear in <T \nin sequenced\u00adbefore order. Since lock operations are totally ordered by happens\u00adbefore, they must occur \nin the same order in <T . We can say little about where a failed trylock() operation on a lock l appears \nin <T . But, since we assume that trylock() may fail spuriously, it does not matter. A failure outcome \nis acceptable no matter what state the lock was left in by the last preceding operation (in <T ) on l. \nNo matter where the failed trylock() appears in <T , the operations on l could have been executed in \nthat order, and produced the original results. It remains to be shown that each load sees the last preceding \nstore in <T that stores to the same location. Clearly this is true for operations on atomic objects, \nsince all such operations appear in the same order as in <S , and each load in <S sees the preceding \nstore in <S . From here on, we consider only ordinary, non-atomic memory operations. Consider a store \noperation Ss seen by a load L. Ss must be a visible side effect with respect to L, and hence must happen \nbefore L. Hence Ss precedes L in <T . Assume that another store Sb appears between Ss and L in <T . \nWe know from the fact that <T is an extension of <hb, that we cannot have either of L<hb Sb or Sb <hb \nSs since that would be inconsistent with the reverse ordering in <T . However all three operations con.ict \nand we have no data races. Hence they must all be ordered by <hb, and Sb must also be happens-before \nordered between the other two. But this contradicts the fact that Ss must be a visible side effect with \nrespect to L, concluding the proof. Although this is a useful theorem, it does not (yet) ensure that \nour re.ned memory model is equivalent to the original one. We must also show that a program that is data-race-free \nby our original type 1 de.nition is also data-race-free by our revised (type 2) de.nition. 8. Equivalence \nof Race De.nitions THEOREM 8.1. If a program allows a type 2 data race in a consis\u00adtent execution, then \nthere exists a sequentially consistent execution, with two con.icting actions, neither of which happens \nbefore the other.8 In effect, we only need to look at sequentially consistent execu\u00adtions in order to \ndetermine whether there is a data race in a consis\u00adtent execution. For example, a program such as the \none in .gure 1 cannot possibly contain a race, since in a sequentially consistent execution, each variable \nis accessed by only one thread. Proof We show that any type 2 data race in a consistent execu\u00adtion corresponds \nto a data race (again de.ned in terms of happens\u00adbefore, not simultaneous execution) in a sequentially \nconsistent ex\u00adecution. Consider a consistent execution with a data race. Let <T be the total extension \nof the happens-before and synchronization orders, as constructed above. Consider the longest pre.x P \nof <T that contains no data race. Note that each load in P must see a store that precedes it in either \nthe synchronization or happens-before orders. Hence each load in P must see a store that is also in P \n. Similarly each lock operation must see the state produced by another lock operation also in P , or \nit must be a failed trylock() whose outcome could have occurred if it had seen such a state. By the arguments \nof the preceding section, the original execu\u00adtion restricted to P is equivalent to the pre.x of a sequentially \ncon\u00adsistent execution. The next element N of <T following P must be an ordinary memory access that introduces \na race. If N is a store operation, consider the original execution restricted to P .{N}. Otherwise consider \nthe same execution except that N sees the value written by the last write to the same variable in P . \n8 The latter is essentially the condition used in [25] to de.ne correctly synchronized for Java. In either \ncase, the resulting execution (of P plus the operation introducing the race) is a pre.x of a sequentially \nconsistent execu\u00adtion; if N was a write, it could not have been seen by any of the reads in P , since \nthose reads were ordered before N in <T ; if N was a read, it was modi.ed to ensure that it sees the \nlast applicable write in P . In either case, we have a sequentially consistent exe\u00adcution of the program \nthat exhibits the data race (a straightforward extension of P .{N}). THEOREM 8.2. A program allows a \ntype 2 data race on a given input if and only if there exists a sequentially consistent execution in \nwhich two unordered con.icting actions are adjacent in the sequential interleaving, i.e. it allows a \ntype 1 data race. Proof Assume we have a sequentially consistent execution with total order <T and a \ntype 1 data race. There is a straightforward mapping of this execution, up to the .rst such data race, \nto a consistent execution as de.ned in section 6 in which each load sees the corresponding preceding \nstore in <T , and the synchronization order is just a restriction of <T . It is easy to see that the \ntype 1 data race maps to a type 2 data race. It remains to show the converse: If we have a type 2 data \nrace, there must be a sequentially consistent execution with con.icting adjacent operations. Start with \nthe execution restricted to P .{N}, as above, with any value read by N adjusted as needed, also as above. \nWe know that nothing in this partial execution depends on the value read by N. Let M be the other memory \nreference involved in the race. We can further restrict the execution to those operations that happen \nbefore either M or N. This set still consists of pre.xes of the sequences of operations performed by \neach thread. Since each load sees a store that happens before it, the omitted operations cannot impact \nthe remaining execution. De.ne a partial order (race-order) on {x | x<hb M}.{x |x<hb N}.{M}.{N} which \norders everything in the .rst two sets before the other two, but imposes no other order. Race-order is \nconsistent with happens-before and synchroniza\u00adtion order. It imposes no additional order on the initial \nsubset. Nei\u00adther M nor N is ordered by the synchronization order, and nei\u00adther is race ordered or happens \nbefore any of the elements in the .rst subset. If we had a cycle A0,A1, ..., An = A0, where each element \nof the sequence happens before or is race-ordered or syn\u00adchronization ordered before the next, neither \nM nor N could thus appear in the cycle. This is impossible, since happens-before and the synchronization \norder are required to be consistent. We can thus construct the total order <T as a total extension of \nthe re.exive transitive closure of the union of happens-before, syn\u00adchronization order, and race order. \nBy the preceding observation, this exists. By the same arguments as in the proof of theorem 7.1, every \nmemory read must see the preceding write in this sequence, except possibly N, since it is the only one \nthat may see a value stored by a racing operation. But we can again simply adjust the value seen by N \nto obtain the property we desire, without affecting the rest of the execution. Thus <T gives us the desired \nsequentially consistent execution in which the last two operations, namely M and N, con.ict. 9. Model \nAdjustments for Low-Level Atomics The C++ working paper provides low-level atomic operations that are \nexplicitly parameterized with respect to a memory ordering constraint. The value of an atomic variable \nx can be retrieved, for example, with x.load(memory order relaxed) , allowing it to be reordered with \nother memory operations. In terms of the memory model, this speci.es that the load is never an acquire \noperation, and hence does not contribute to the synchronizes-with ordering. For read-modify-write operations, \nthe programmer can specify whether the operation acts as an acquire operation, a release operation, neither \n( relaxed ), or both. In order to include such low-level atomics in our memory model, as is done in \nthe C++ working paper, we need several re\u00ad.nements to the model in section 6: 1. The total order S of \nsynchronization operations contains only high-level (sequentially consistent) atomic operations. (These \ncan also be speci.ed with an explicit memory order seq cst parameter.) 2. We do however still want updates \nto a single variable to occur in a total order, dubbed the modi.cation order in the standard. 3. There \nwas a feeling that, for example a weakly ordered atomic increment performed by another thread between \nan atomic store and load should not break the synchronizes-with rela\u00adtionship between the store and the \nload. Hence the de.nition of synchronizes-with was strengthened to cover this case (see release sequence \nin the C++ working paper). The precise def\u00adinition in the working paper is a compromise between ease \nof use and implementability on common hardware. 4. Since S no longer includes all synchronization operations, \nthe value seen by an atomic load is no longer uniquely determined by S. We de.ne a visible sequence with \nrespect to an atomic load or read-modify-write operation a of location l, to be the maximal subsequence \nV of l s modi.cation order such that every element of V either is or occurs after a visible side effect \nof a, but no element of V happens after a. V then represents all possible updates that might be seen \nby a.  Although we can now accommodate low-level atomics in the memory model, it is important to remember \nthat these are still very hard to use correctly, and very much an experts-only feature. Most users will \nbene.t only indirectly from libraries written using these facilities. 10. Conclusions We have outlined \nthe foundations of the C++ threads memory model. From the user s perspective, we provide a simple programming \nmodel. In return for avoiding data races or, equivalently, identifying variables and other objects involved \nin data races as atomic, most users can ignore the intricacies of hardware memory models and compiler \noptimizations; they are guaranteed sequentially consistent execution. All of this can be based on the \nmost intuitive de.nition of a data race: simultaneous execution of con.icting operations. The one place \nin which modern machine architectures do unavoidably show through slightly is that updates to adjacent \nbit-.elds con.ict; otherwise, operations con.ict only when they touch the same ob\u00adject. For those few \nusers whose performance requirements cannot be satis.ed with either locks or sequentially consistent \natomic op\u00aderations, we provide low-level, explicitly ordered, atomics, which trade simplicity for cross-platform \nperformance. From the compiler implementors perspective, we preserve the guarantee that ordinary variables \ndo not appear to change asyn\u00adchronously. Hence, standard program analyses remain valid, except for objects \nof atomic type, even in the presence of threads. In re\u00adturn, the implementation must refrain from introducing \nuser visible data races, for example, as a result of rewriting adjacent structure .elds or register promotion. \nMore complete implementation guide\u00adlines are given in [10]. From the hardware implementors perspective, \naside from now standard features, such as compare-and-swap and similar opera\u00adtions, we need a modest \ncost facility that allows us to implement se\u00adquentially consistent atomics, and particularly write atomicity. \nWe do not need write atomicity for all store operations; but we do need it for atomic operations. Ideally, \nsequentially consistent atomics should be implementable with very small overhead for load opera\u00adtions. \nAcknowledgments This effort has bene.tted greatly from contributions by many oth\u00aders, including especially \nHerb Sutter, Doug Lea, Paul McKenney, Bratin Saha, Jeremy Manson, Bill Pugh. We clearly would not have \nbeen successful at generating standards-appropriate text without the help of Clark Nelson and Lawrence \nCrowl, our coauthors for [13] and [27]. Lawrence Crowl is also responsible for most of the detailed design \nof the atomics interface. The anonymous reviewers provided a number of useful suggestions. Mark Hill \ngave comments on a previous version of the paper. This work has made it this far through the standards \nprocess only with the the help of processor vendors including AMD, ARM, IBM, and Intel, who were helpful \nboth in pointing out potential is\u00adsues, and cooperating with the standardization process, even when signi.cant \neffort was required. References [1] S. V. Adve. Designing Memory Consistency Models for Shared-Memory \nMultiprocessors. PhD thesis, University of Wisconsin-Madison, 1993. [2] S. V. Adve and K. Gharachorloo. \nShared memory consistency models: A tutorial. IEEE Computer, 29(12):66 76, 1996. [3] S. V. Adve and M. \nD. Hill. Weak ordering A new de.nition. In Proc. 17th Intl. Symp. Computer Architecture, pages 2 14, \n1990. [4] AMD Corp. AMD64 Architecture Programmer s Manual -Volume 2: System Programming, July 2007. \n[5] D. Aspinall and J. Sevcik. Java memory model examples: Good, bad, and ugly. VAMP07 Proceedings http://www.cs.ru.nl/ \n~chaack/VAMP07/, 2007. [6] H. Boehm and N. Maclaren. Should volatile acquire atomicity and thread visibility \nsemantics? C++ standards committee paper WG21/N2016 = J16/06-0086, http://www.open-std.org/JTC1/ SC22/WG21/docs/papers/2007/n2016.html, \nApril 2006. [7] H.-J. Boehm. Threads cannot be implemented as a library. In Proc. Conf. on Programming \nLanguage Design and Implementation, 2005. [8] H.-J. Boehm. A less formal explanation of the proposed \nc++ concurrency memory model. C++ standards committee paper WG21/N2480 = J16/07-350, http://www.open-std.org/JTC1/ \nSC22/WG21/docs/papers/2007/n2480.html, December 2007. [9] H.-J. Boehm. Memory model rationales. C++ standards \ncommittee paper WG21/N2176 = J16/07-0036, http://www.open-std. org/JTC1/SC22/WG21/docs/papers/2007/n2176.html, \nMarch 2007. [10] H.-J. Boehm. N2338: Concurrency memory model compiler conse\u00adquences. C++ standards committee \npaper WG21/N2338=J16/07-198, http://www.open-std.org/JTC1/SC22/WG21/docs/papers/ 2007/n2338.htm, August \n2007. [11] H.-J. Boehm. N2392: A memory model for c++: Sequential consistency for race-free programs. \nC++ standards committee paper WG21/N2392=J16/07-252, http://www.open-std.org/JTC1/ SC22/WG21/docs/papers/2007/n2392.htm, \nSeptember 2007. [12] H.-J. Boehm. Reordering constraints for pthread-style locks. In Proc. 12th Symp. \nPrinciples and Practice of Parallel Programming, pages 173 182, 2007. [13] H.-J. Boehm and L. Crowl. \nC++ atomic types and operations. C++ standards committee paper WG21/N2427=J16/07-0297, http: //www.open-std.org/JTC1/SC22/WG21/docs/papers/2007/ \nn2427.htm, October 2007. [14] C++ Standards Committee, Pete Becker, ed. Working Draft, Standard for Programming \nLanguage C++. C++ standards committee paper WG21/N2461=J16/07-0331, http://www.open-std.org/JTC1/SC22/ \nWG21/docs/papers/2007/n2461.pdf, October 2007. [15] L. Ceze et al. BulkSC: Bulk Enforcement of Sequential \nConsistency. In Proc. Intl. Symp. on Computer Architecture, 2007. [16] T. Elmas, S. Qadeer, and S. Tasiran. \nA race and transaction-aware java runtime. In Proc. Conf. on Programming Language Design and Implementation, \npages 245 255, 2007. [17] K. Gharachorloo. Memory Consistency Models for Shared Memory Multiprocessors. \nPhD thesis, Stanford University, 1995. [18] K. Gharachorloo et al. Memory Consistency and Event Ordering \nin Scalable Shared-Memory Multiprocessors. In Proc. 17th Intl. Symp. on Computer Architecture, pages \n15 26, 1990. [19] K. Gharachorloo, A. Gupta, and J. Hennessy. Two Techniques to Enhance the Performance \nof Memory Consistency Models. In Proc. Intl. Conf. on Parallel Processing, pages I355 I364, 1991. [20] \nW. H. Harrison. Compiler analysis of the value ranges for variables. IEEE Trans. Software Engineering, \n3(3), May 1977. [21] IEEE and The Open Group. IEEE Standard 1003.1-2001. IEEE, 2001. [22] Intel Corp. \nIntel 64 Architecture Memory Ordering White Paper, August 2007. http://www.intel.com/products/processor/ \nmanuals/318147.pdf. [23] A. Kamil, J. Su, and K. Yelick. Making sequential consistency practical in titanium. \nIn Proceedings of the 2005 ACM/IEEE SC 05 Conference (SC 05), page November, 2005. [24] L. Lamport. How \nto make a multiprocessor computer that correctly executes multiprocess programs. IEEE Transactions on \nComputers, C-28(9):690 691, 1979. [25] J. Manson, W. Pugh, and S. Adve. The Java memory model. In Proc. \nSymp. on Principles of Programming Languages, 2005. [26] S. Narayanasamy et al. Automatically classifying \nbenign and harmful data races using replay analysis. In Proc. Conf. on Programming Language Design and \nImplementation, pages 22 31, 2007. [27] C. Nelson and H.-J. Boehm. Concurrency memory model (.nal revision). \nC++ standards committee paper WG21/N2429=J16/07\u00ad0299, http://www.open-std.org/JTC1/SC22/WG21/docs/ papers/2007/n2429.htm, \nOctober 2007. [28] P. Ranganathan, V. S. Pai, and S. V. Adve. Using Speculative Re\u00adtirement and Larger \nInstruction Windows to Narrow the Performance Gap between Memory Consistency Models. In Proc. Symposium \non Parallel Algorithms and Architectures, 1997. [29] D. Shasha and M. Snir. Ef.cient and correct execution \nof parallel programs that share memory. ACM Transactions on Programming Languages and Systems, 10(2):282 \n312, April 1988. [30] Z. Sura et al. Compiler Techniques for High Performance Sequen\u00adtially Consistent \nJava Programs. In Symp. Principles and Practice of Parallel Programming, 2005. [31] H. Sutter. Prism: \nA principle-based sequential memory model for microsoft native code platforms. C++ standards committee \npaper WG21/N2197 = J16/07-0057, http://www.open-std.org/JTC1/ SC22/WG21/docs/papers/2007/n2197.pdf, March \n2007. [32] United States Department of Defense. Reference Manual for the Ada Programming Language: ANSI/MIL-STD-1815A-1983 \nStandard 1003.1-2001, 1983. Springer. [33] T. Wenisch et al. Mechanisms for Store-wait-free Multiprocessors. \nIn Proc. Intl. Symp. on Computer Architecture, 2007.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7].</p> <p>We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:<ul><li>We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work.</li> <li>We give no semantics to programs with data races. There are no benign C++ data races.</li> <li>We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock.</li></ul></p> <p>This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.</p>", "authors": [{"name": "Hans-J. Boehm", "author_profile_id": "81423595101", "affiliation": "HP Labs, Palo Alto, CA, USA", "person_id": "P1022746", "email_address": "", "orcid_id": ""}, {"name": "Sarita V. Adve", "author_profile_id": "81100524186", "affiliation": "University of Illinois, Urbana-Champaign, IL, USA", "person_id": "P1022747", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375591", "year": "2008", "article_id": "1375591", "conference": "PLDI", "title": "Foundations of the C++ concurrency memory model", "url": "http://dl.acm.org/citation.cfm?id=1375591"}