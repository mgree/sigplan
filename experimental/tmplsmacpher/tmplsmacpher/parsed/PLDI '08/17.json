{"article_publication_date": "06-07-2008", "fulltext": "\n QuantitativeInformationFlow asNetworkFlowCapacity StephenMcCamant MichaelD.Ernst MITComputerScience \nandAILab {smcc,mernst}@csail.mit.edu Abstract Wepresenta newtechniquefordetermininghow muchinformation \nabout aprogram s secretinputsis revealedbyitspublic outputs.In contrast to previous techniques based \non reachability from secret inputs(tainting),it achieves a moreprecisequantitative resultby computing \namaximum .owofinformationbetweentheinputsand outputs. The technique uses static control-.ow regions to \nsoundly accountforimplicit .owsviabranches andpointeroperations,but operates dynamically by observing \none or more program execu\u00adtionsandgiving numeric .owboundsspeci.ctothem(e.g., 17 bits ).Themaximum .owinanetwork \nalsogivesaminimumcut (a setof edgesthat separatethe secretinputfromthe output), which canbe used to ef.ciently \ncheck that the samepolicy is satis.ed on future executions. We performed case studies on 5 real C, C++, \nand Objective C programs, 3 of which had more than 250K lines of code.The tool checked multiple securitypolicies,including \none that was violatedby apreviously unknown bug. Categories and Subject Descriptors D.2.4 [Software/Program \nVeri.cation]; D.2.5[Testing and Debugging]; E.4[Coding and Information Theory]; G.2.2[GraphTheory] General \nTerms Languages, Measurement, Performance, Secu\u00adrity,Theory,Veri.cation Keywords Information-.ow analysis, \ndynamic analysis, implicit .ow 1. Introduction The goal of information-.ow security is to enforce limits \non the dissemination of information. For instance, a con.dentiality prop\u00aderty requires that a program \nthat is entrusted with secrets should not leak thosesecretsintopublicoutputs.Absoluteprohibitions oninformation \n.owarerarely satis.edby realprograms:if asen\u00adsitive input does not affect a program s output at all, \nit is better to simply omit it, and unrelated computations at different security levels should be performed \nby separate processes. Rather, the key challengeforinformation-.ow securityistodistinguish acceptable \nfrom unacceptable .ows. Systems often deal with private or sensitive information by revealing only a \nportion or summary of it. The summary contains fewerbits of secretinformation,providing a mathematicallimit \non theinferences an attacker coulddraw.Forinstance, an e-commerce Permission to make digital or hard \ncopies of all or part of this work for personal or classroomuseisgranted withoutfeeprovided that copiesarenot \nmadeordistributed forpro.tor commercial advantage andthat copiesbearthis notice andthefull citation onthe \n.rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute tolists, requiresprior speci.cpermission \nand/or afee. PLDI 08, June7 13,2008,Tucson,Arizona,USA. Copyright c &#38;#169; 2008ACM978-1-59593-860-2/08/06. \n. .$5.00 web site prints only the last four digits of a credit card number, a photograph is released \nwith a face obscured, an appointment scheduler shows what times I m busy but not who is meeting me, a \ndocument is released with text replaced by black rectangles, or a strategy game reveals my moves but \nnot the contents of my board. However, it is not easy to determine by inspection how much information \na program s output contains. For instance, if a name is replaced by a black rectangle, it might appear \nto contain noinformation,butifthe rectanglehasthe same width as thetextit replaces, anddifferent lettershavedifferent \nwidths, the total width might determine which letters were replaced. Or a strategy game might reveal \nextra information in a network message that is not usuallydisplayed. The approach of quantitative information-.ow \nsecurity ex\u00adpresses a con.dentiality property as a limit on the number of bits that maybe revealed, measures \nthebits aprogram actually reveals, and detects a violation if the measured .ow exceeds the policy. The \nproblem we address here is how to measure, by observing an execution of aprogram(dynamic analysis),how \nmuchinformation about a subset of its inputs (the secret inputs ) can be inferred fromasubsetofitsoutputs(the \npublicoutputs ).Thetextof the program itself is always considered public, and other techniques must be \nused to prevent inferences from observable aspects of the program s behavior other than its output, such \nas its use of time or system resources. The measurement produced is a sound upper bound on the actual \ninformation .ow, so that our technique can overestimatethe amount ofinformation revealed,but can never \nun\u00adderestimateit.Thebound applies only totheexamined execution: other executions might reveal either \nmoreinformation orless. Most previous research on quantitative information .ow has focused on very small \n.ows. For instance, an unsuccessful login attempt reveals only a small fraction of a bit, if the attacker \nhad no previous knowledge of the password. We focus on a broader classofproblemsinwhich a .owof many \nbitsmaybeacceptable (thoughaquantitativepolicyis still only applicableifthe allowable .ows are allless \nthan the undesirable ones). In some violations of information-.ow policies, con.dential data is exposed \ndirectly, for instance if the memory containing a user-provided password is not cleared before being \nreused by the operating system. A number of existing techniques can track such direct data .ows. However, \nin many other cases information is transformed among formats, and may eventually be revealed in a form \nvery different from the original input. Our research aims to soundly account for all of the in.uence \nthat the secret input has on the program s output, even when the in.uence is indirect. Speci.cally, this \nmeans our technique must account for implicit .ows in which the value of a variabledepends on aprevious \nsecret branch condition orpointer value. Most previous approaches to information-.ow program analy\u00adsis \nare based on some kind of tainting: a variable or value in a program is taintedifit might contain secretdata.Thebasic \nrule of taintingis that the result of an operation shouldbe taintedif any of Figure1. Twopossiblegraphs \nrepresenting thepotentialinforma\u00adtion .ow in the expression c=d=a+b, where each variable is a32-bitinteger.Thegraph \non theleftpermits32bits ofinforma\u00adtionto .owfrom a to c,and adifferent32bitsto .owfrom b to d. To avoidthis, \nour tool uses thegraph on the right. the operandsis.Taintingis appropriatefordetermining whether an \nillegal .owispresentornot,butitcannotgiveaprecisemeasure\u00adment of secretinformationbecause ofits conservative \ntreatment of propagation. A single tainted input can cause many later values to be tainted, but making \ncopies of secret data does not multiply the amount of secretinformationpresent. Akey newideainthepresent \nworkisto measureinformation\u00ad.ownot using taintingbut asakind of network .owcapacity.One can model the \npossible information channels in an execution of a program as a networkoflimited-capacitypipes, and secretinforma\u00adtion \nas an incompressible .uid. Then the maximum rate at which .uid can .ow through the network corresponds \nto the amount of secretinformation the execution can reveal.According to the clas\u00adsic max-.ow-min-cuttheorem,this \ncapacity also correspondstothe weight of a minimum cut: a setof edges whose removaldisconnects the secretinput \nfrom thepublic output, representing a set of secret intermediate values that(along withpublicinformation)determine \ntheprogram s output. The rest of this paper is organized as follows. Section 2 de\u00adscribeshowtoconstructa \n.ownetwork representing thepropaga\u00adtion of secretsin aprogram execution, andSection3de.nes sound\u00adness \nanddescribeshowtoensureitbetween resultsfromdifferent runs. Section 4 gives an implementation of the \ntechnique that op\u00aderates at theinstruction level.Then,Section5discusses ef.ciently computing the maximum \n.ow in a large network, and Section 6 describeshowa .owbound,oncefound,canbechecked onfuture program \nruns.Section7discussesthe situationsin whichourtoolis most appropriate, andSection8 evaluatesiton con.dentialityprop\u00aderties \nin a number of real applications. Finally, Section 9 surveys related research, Section 10 discusses future \nwork, and Section 11 concludes. 2. Dynamic maximum-.ow analysis Our basic technique is to construct \na graph that represents the possible .ows of secret information through a program execution. This section \ndescribes that construction, including how to account forimplicit .ows,andhowtoassigncapacitiestoedgesinthe \n.ow graph. 2.1 Basic approach The .ow graphs our technique constructs represent an execution in a form \nsimilar to a circuit. For ef.ciency, the graph represents byte or word-sized operations. Edges represent \nvalues, and have capacities giving how many bits of data they can hold. Nodes represent basic operations \non those values, where the in-degree of a node is the operation s arity. For the case when the result \nof an operation is used in more than one subsequent operation, our tool adds an additional single edge \nand node, which represents the constraintthatthe operationhas only one output(seeFigure1);this is also \nequivalent togiving a capacitylimit on a node. Copying a piece of data without modifying it does not \nlead to the creation of new nodes or edges, but because memory is byte-oriented, loads and stores of \nlarger values are split into bytes for stores and recombined after loads. The graph is directed, with \nedges always pointing from older to newer nodes, and so is also acyclic. Inputs and output are represented \nby two distinguished nodes, a source node representing all secretinputs, and a sink node representing \nallpublic outputs. 2.2 Implicit .ows General programs are more complex than circuits because of op\u00aderations \nsuch as branches, arrays, and pointers that allow data to affect which operations are performed or what \ntheir operands are. These operationsleadtoindirector implicit.ows whichdo not cor\u00adrespondtoanydirectdata \n.ows.Forinstance,laterexecutionmight be affectedbyabranch that caused alocation not tobe assigned to, \northefactthatthe5th entryin an arrayis zero might reveal thatthe index used in a previous store was not \nequal to 5. To account for such situations,asoundgraph representation musthaveedgesthat represent allpossibleimplicit \n.ows. To recover the intuitive perspective of execution as a circuit, our tool treats each operation \n(e.g., branch) that might cause an implicit .owasbeingenclosed aspart of alarger computation with de.ned \noutputs. The tool adds edges to connect each implicit .ow operation to the outputs of the enclosed computation.Forinstance, \nconsider computing a square root. If a single hardware instruction computessquareroots,thenthereisnoimplicit \n.ow,butthesquare root of a secret valueisitself secret.Onthe otherhand,ifthe square rootis computedby \ncode that uses aloop orbranches on the secret value, these implicit .ows can be conservatively accounted \nfor by assuming that they might all affect the computed square root value, soourtool canrepresenttheimplicit \n.owswith edgestotheresult. Forprecision,thecapacityoftheseimplicit .owedgescorresponds to the number \nofpossible different executions:forinstance, a two\u00adwaybranchyields an edge with capacity onebit, whilethe \ncapacity for apointer operation such as anindirectload, store, orjumpis as manybits as are secretinthepointer \nvalue.(Multi-waybranches show up as either nestedtwo-waybranches orjump tables atthe instructionlevel.) \nTo achieve soundness, it is suf.cient to consider the entire program as being enclosed in this way: our \ntool s default behavior connects each implicit .ow operation to the program s output. Better precision \nresults from using additional enclosure regions around smaller sub-computations, such as the square-rootfunction \nmentioned earlier. In our system, enclosure regions are speci.ed using annotations which mark a single-exit \ncontrol-.ow region and declare all ofthelocationsthe enclosed code might writeto(see Figure 2 for examples). \nThese annotations can be inferred using standard static analysis techniques; Section 8.6 describes a \npilot study examining what is required. Our tool can also dynamically check thatthe soundness requirementsfor \nan enclosure regionhold at runtime, but this is less satisfactory because if a check fails, it is not \nalways possible to continue execution in a way that is both sound andbehavior-preserving. For precision \nand ef.ciency, the graph structures our tool con\u00adstructs are somewhat more complex than simple edges \nfrom each implicit .ow operation to each output. Each enclosure region has a distinguished node, and \nour tool adds edges from each implicit .ow operation tothat node, and thenfromthat node to each output. \nForthe enclosure ofthe entireprogram, our tooltakes advantage of thetime sequencein outputsbybuilding \na chain of nodes each cor\u00adresponding toanoutput operation.Eachimplicit .owisconnected to the then-current \nend of the chain, so that information leaked by animplicit .owcanescapeviaany subsequent outputone,but \nnot an output that occurred earlier. 1 /* Print all the \".\"s or \"?\"s, 2 whichever is more common. */ \n3 void count_punct(char *buf) { 4 unsigned char num_dot = 0, num_qm = 0, num; 5 char common, *p; 6 ENTER_ENCLOSE(num_dot, \nnum_qm); 7 while (p = buf; *p != \\0 ; p++) 8 if (*p == . ) 9 num_dot++; 10 else if (*p == ? ) 11 num_qm++; \n12 LEAVE_ENCLOSE(); 13 ENTER_ENCLOSE(common, num); 14 if (num_dot > num_qm) { 15 /* \".\"s were more common. \n*/ 16 common = . ; num = num_dot; 17 }else { 18 /* \"?\"s were more common. */ 19 common = ? ; num = num_qm; \n20 } 21 LEAVE_ENCLOSE(); 22 /* print \"num\" copies of \"common\". */ 23 while (num--) 24 printf(\"%c\", common); \n25 } Figure2. C code toprint all the occurrences of the most common punctuation characteris a string.Forinstance, \nwhen run onits own source code, the program produces the output ........ . As detailed in Section 2.4, \nour tool reports that this execution reveals 9bits ofinformation about theinput. 2.3 Bit-capacity analysis \nSubsections 2.1 and 2.2 described the structure of the graph our tool computes for a program execution, \nbut to compute a maxi\u00admum .ow, each edge must also be labelled with a bound on the amount of information \nit can convey. To compute these bounds, our tool simultaneously performs a dynamic bit-width analysis \nto determine which of thebitsin eachdata value might contain secret information. This analysis is essentially \nimplemented as dynamic tainting, but at the level of bits. The analysis maintains, for every location \nin memory or a register, a shadow bit vector of the same size representing which data bits might be secret. \nFor each basic program operation the analysis computes conservative secrecy bits forits resultsbasedonthe \nsecrecybits ofthe operands.The amount of secretinformationthat might .owthrough avalueisboundedby the \nnumber of its bits that are marked secret. This analysis is very similartothe analysisthattheValgrindMemchecktool[47] \nuses to track unde.ned values, so we were able to reuse much of its implementation(asdid,independently, \ntheFlayer tool[17]). 2.4 Example As a concrete example of the techniques of this section, consider the \ncode shown in Figure 2. This function counts the number of periods and question marks in a string, and \nthen whichever was more common,prints as many as appearedinthe string(modulo 256,becauseituses an8-bit \ncounter).Forinstance,the source code contains8periods and4question marks, sothe when run onits own source \nthe program prints 8 periods. Our tool measures that such an execution reveals 9 bits of information \nabout the secret input: 1 bit from the selection of which character is more common, and 8bitsfromthecount.Thecorresponding \nminimumcutconsistsof two edges, one for the implicit .ow from the comparison between num dot and num \nqm online14(capacity1bit), and oneforthe value of num after the second enclosure region online21(capacity \n8bits).  This example shows theimportance ofseveral of thetechniques introduced above. The only relationships \nbetween the input buffer and num dot,between num dot and common, andbetween num and the output, are implicit \n.ows. A tool that did not account for all of them could give an unsound (too small) result. The enclosure \nregions marked by ENTER ENCLOSE and LEAVE ENCLOSE improve the precision of the results: without them, \nthe default treatment of enclosingthe entireprogram would causethetoolto measure aleak of 1 bit each \ntime a value from the input buffer was compared to aconstant,1855intotal.Themaximum .owcomputationalsoim\u00adproves \nprecision; without it, simple tainting would determine that all of the bits in the output might depend \non the input, giving a bound of64bits.Though wechosethisexampleasasimpleillus\u00adtration, similar situations \noccurred commonly in real applications such as thosedescribedinSection8.  3. Soundness and consistency \nThetechnique ofSection2oftengives agoodboundbased on only a singleprogram execution.Butfrom a theoreticalperspective, \nthe amount ofinformation aprogram reveals shouldbede.nedinterms of multiplepossibleruns.Thissection .rstprovidesamorespeci.c \nde.nition of what our technique computes, then describes how to merge .ow graphs to compute a bound that \nis sound in that sense across multipleprogram executions. 3.1 Soundnessforadynamicanalysis Wedescribe \nwhatitmeansfor an analysisthat examines a subset of possibleexecutionstogiveanacceptable .owboundintwosteps. \nFirst, we present a general attack model in which an adversary uses theprogram to communicate a secret \nmessage ofher choosing to a confederate. Second, we de.ne a sound .ow bound in this model: in summary, \na bound of k bits is sound if an adversary couldhave communicated the sameinformationby sending a k-bit \nmessagedirectly.Throughout, we usetheperspective of expressing information with a code that represents \npossible messages via bit strings of variablelength. Other quantitative information-.ow analyses have \ncommonly treated thesecrettobeprotected asbeingdrawnfroma .xed(e.g., uniform) distribution. Though this \nis appropriate in some circum\u00adstances, such assumptionslimit a technique s usability:inpractice, the \ndistribution of an input is often unknown, or worse, might be controlled by an adversary. Instead, wehave \nfound it more natural to consider a more powerful adversary who can choose the secret inputsto revealas \nmuchinformation aspossible.Forinstance, con\u00adsider adivisionfunctionfor32-bit words thathidesits normal \nout\u00adput, but has an observably different behavior on a divide by zero error. If one assumes that the \ninputs are uniformly distributed, the expectedinformation revealedis a very smallfraction of abit, since \na zerodivisor would almost never occurby chance(though whenit does,itshouldbe counted as revealing32bits).Butif \nan adversary could in.uence the divisor, she might cause it to be 0 with proba\u00adbility one half, in which \ncase each execution would reveal one bit ofinformation. In more detail, consider a pair of spies, Alice \nand Bob. Alice wants to use the program to send a message to Bob, by choosing theprogram s secretinputs \nto cause some change tothepublic out\u00adputs thatBobobserves.Alice andBobhavepriorknowledge of the program, \nandtheyhave agreedin advance on a setofpossible mes\u00adsages they might want to communicate. The public \ninputs might be out of Alice and Bob s control, or Alice and Bob might have chosen them, but we will \ntreat them as being .xed in advance: the analysis s results and soundness will be with respect to a particu\u00adlar \nset of public inputs. We will also assume that Alice and Bob areinterestedin error-free communication(theprogramisdeter\u00administic), \nand have no computational limits, so their strategy is to choose a set ofpossibleprograminputsthatAlice \nmight send, each of which will cause a distinct public output. For instance, in the division example \nabove, theymight choose thefollowing code:Al\u00adicegives the input 5/3, causing normalprogram output, to \nconvey attack at dawn , while she gives 2/0, causing an error report, to convey no attack . In essence, \nwe treat the program s execution as a channelfor transmitting messages, and areinterestedin an up\u00adper \nbound on the amount of information the channel can convey under any coding scheme: its channel capacity. \nThe channel ca\u00adpacityisdeterminedbythe total number ofdifferentpublic outputs theprogram canproduce,but \ncounting themdirectly wouldbeim\u00adpractical.Instead, our tool s measurements correspond to a natural coding \nscheme suggestedbythe structure ofthe analyzedprogram, which will alwaysbe an upperbound on the channel \ncapacity.   Theperspectiveofa .xed uniformdistribution,inwhich onebit ofinputdata always carries afullbit \nofinformation,has anintuitive appeal:forinstance,in thedivision example,itistempting to argue that .nding \nout that a 32-bit input has all bits zero should always count asdiscovering32bits ofinformation.However, \nwebelieveit is ultimatelyless usefulbecause itis tied to aparticulardata repre\u00adsentation,that oftheinputstotheprogrambeing \nanalyzed.By con\u00adtrast, channel capacity abstracts away from a particular represen\u00adtation to more abstractly \ncharacterize the computation a program performs. In particular, channel capacity can be more naturally \nbe approximated compositionally, as ourgraph-based analysisdoes. To Alice and Bob, the originally intended \nbehavior of the pro\u00adgram mightjustbe adistraction:they wish to useitsinput/output behavior as a communications \nchannel. To de.ne how well they can exploit the program, we can compare their results using it to what \nthey could achieve by using a direct communications chan\u00adnel. Instead of an execution of the program \nthat our tool measures to reveal at most k bits, we imagine that Alice sends a string of up to k binary \ndigitsdirectly toBob according to a code theyhave settled on in advance. For instance, 0 might correspond \nto attack at dawn , and 1 to no attack . Suppose that for each input i . I that Alice sends, our tool \nreports an information-.ow bound k(i). We willsay that that resultis soundifthereis also a codeby which \nAliceandBob couldhaveunambiguously communicated thesame messages, in which each message was represented \nby a string of k(i) bits. Thus, in the division example, it would be sound for the tool to report abound \nof1bit. There is also an equivalent characterization of soundness as a numeric condition on the amounts \nk(i).Intuitively,itisimpossible for there to be many distinct outputs, none of which reveal much information. \nThe precise characterization of this relationship is 2-k(i) Kraft sinequality, whichin our notation statesthati \n= 1. (Kraft s inequality holds for any uniquely-decodable code, and conversely, it is straightforward \nto construct a code to match a set of lengths that satisfy the inequality [12].) Several more speci.c \nconsequences follow from this soundness de.nition. First, if a sound tool ever reports a .ow of 0 bits, \nthen it must be the case that the public output for that execution is the only one that can possiblybeproduced \nwith any other secretinputs(forthatpublic input). In other words, the case of 0 bits corresponds to the \nnon\u00adinterference criterion for no information .ow. Second, if there are N messages that all carry the \nsame information, each one must be convey at least log2 N bits: k bits are enough to distinguish between \n2k possibilities. 3.2 Achieving consistency over multiple runs Asexplained above, soundnessisbestde.ned \nas aproperty about sets of inputs, even if the tool examines only a single execution. But if a tool analyzes \na set of executions, soundness requires that the results taken together correspond to a single possible \ncode. As described sofar,themaximum .owvaluesourtechniqueproduces would onlybeguaranteed tobe soundin \nthis senseif the minimum cut alwaysoccurred atthesameplaceinthe .owgraph. Forinstance,considerthe .nalphase(lines22-24) \noftheex\u00adampleprogram ofFigure2,in which a characterisprinted n times (0 = n = 255).Ifthe analysis chooses \na cutbefore theloop,n will be measured initsbinary representation, and so willbe counted as revealing8bits.Alternatively,ifit \nchooses a cut attheimplicit.ow edges corresponding to each loop test, then printing n characters will \nbe counted as revealing n +1 bits. Either of these choices is sound onits own(they correspond tobinary \nand unary encodings of n), but always choosing the smaller one (i.e., min(8,n + 1)) gives measurements \nthat are too small.Kraft sinequality con.rms .255 2-min(8,n+1) 503 this unsoundness: = > 1. n=0 256 We \nhave seen that if our maximum-.ow analysis is run inde\u00adpendently on different executions of aprogram, \nthe results may be inconsistent with each other: some of the variationbetween the ex\u00adecutions may cause \nthe tool to pick different cut locations, rather than contributing to the estimated information .ow. \nTo get sound resultsfrom multipleexecutions, ourtoolcombinesthegraphsfrom multiple executions and analyzes \nthem together. In outline, this graph combining process merges all the edges that correspond tothe same \nprogramlocation.Moreprecisely,it labels eachedge with a value thatincludes a staticlocation(i.e.,in\u00adstruction \naddress), and optionally a 64-bit hash of the calling con\u00adtext (stack backtrace), similarly to Bond and \nMcKinley s proba\u00adbilistic calling context [4]. Then, any number of labelled graphs canbe combined by \nidentifying edges with the samelabel(replac\u00adingthem with a single edge whose capacityis the sum of the \norigi\u00adnal capacities), and unifying all of the nodes the original edges are incidentupon.This canbedonein \nalmost-lineartime with a union\u00ad.nd structure: for each edge (u,v) with location l, merge the sets containing \nu and aplaceholderfor sourceof edgesat l , and sim\u00adilarlyfor v and target of edges at l . When .owgraphs \narecombinedinthisway,any sum ofpossible .owsin the originalgraphsispossiblein the combinedgraph, so a \nboundcomputedforthe combinedgraphis still sound.Onthe other hand, the possible cuts in the combined graph \ncorrespond only to sets of cuts that appear in the same places in each original graph, excluding the \npossibility of lower .ow bounds corresponding to inconsistentlyplaced cuts.  4. Machine-levelimplementation \nWe have implemented the information-.ow analysis described in theprevious sections asadynamicbinary analysisforexecutables \nonLinux/x86 systems, calledFlowcheck(http://people.csail. mit.edu/smcc/projects/secret-flow/flowcheck.html). \n4.1 Dynamicinstructionrewriting Our tool instruments a program by dynamically rewriting its in\u00adstruction \nstream, using the Valgrind framework [40]. Valgrind translates each basic block of instructions into \na simple compiler\u00adlikeintermediaterepresentation; ourtool addsinstrumentation op\u00aderationsinthatformat; \nandthenValgrindtranslatestheIRbackinto x86 instructions for execution. This translation insulates our \nanal\u00adysisfrom most of the complexities of thelarge x86instruction set: Valgrind automatically handles \nfeatures such as complex address\u00ading modes, implicit operands, string instructions, condition codes, \nand conditional moves, which require specialtreatmentintoolsthat operatedirectly oninstructions[10].Valgrind \ns automatic register allocation also makesit easier toinsertinstrumentation operations. One architectural \ncomplexity of the x86 that is not abstracted by Valgrind is the presence of overlapping registers: for \ninstance, the16-bit register %dx consists of thelower-orderbits of the32-bit register %edx. In order \nto be able to treat each register as distinct, wehave changedValgrind stranslation of such sub-registers \nsothat instructions that access them instead read or write from the full register, selecting the relevantportion \nusingbitwise operations. 4.2 Graph constructionby valuetagging To build the .ow graph described in Section \n2, the tool associates a positive integer, which we call a tag, with each execution-time value that might \ncontain secret information; values that are not reachablefromthe secretinputhave atag of0.Thesetags represent \nthe identities of nodes in the .ow graph; a tag is associated with each register, and each byte in memory. \nThe tags are maintained in parallel with the secrecy bit-masks described in Section 2.3; if a value s \ntag is 0, its bit-mask is necessarily all public, and it is omittedfrom thegraph.If atleast one operand \nof abasic operation has a non-zero tag, the instrumentation code for the operation assigns afresh tagfor \nthe result of the operation, and creates edges linking theinputs to the result. The representation of \nedges depends on whether the graph\u00adcombining feature of Section 3.2 is in use. If every edge is to be \nconsidered unique,theydo not need anyin-memory representation: each edgeis output to thegraphimmediately, \nas an orderedpair of node tags. In this mode, the memory usage of the tool is bounded by a multiple of \nthe memory usage of the originalprogram:itdoes notgrow asthegraphbecomeslarger.Onthe otherhand,if edges \nare to be combined based on their program locations, it is more ef.cienttokeep arepresentation of each \nclassof equivalent nodes in the tool s memory. However, it is not necessary to retain the entire original \ngraph: instead, all that is needed is the combined graph(whose sizeisboundedbytheprogram size),andinformation \nabout those nodes that still correspond to values in registers or memory. The tool implements an algorithm \nsimilar to mark-and\u00adsweep garbage collection to identify when tags can be reclaimed. These techniques \ndifferfromprevious implementationsbecause of our constraint that memory usage must not grow with the \nlength of aprogram s execution.Forinstance,Redux[39]builds a similar graph with an in-memory linked data \nstructure, which facilitates computing abackward slicefrom the outputbutisless scalable. 4.3 Optimizinglarge-regionoperations \nBecause the output of an enclosure regions can be an entire array or other large data structure, the \ntool often needs to represent the fact that a piece of information might .ow to any byte in a large memory \nregion. It would be too slow to do this by modifying the tag of each memory location individually: for \ninstance, consider a loopoperating on an arrayin which eachiteration mightpotentially modify any element(say,if \ntheindexis secret).Operating on each elementduring eachiteration wouldlead toquadratic runtime cost. \nInstead, the tool performs operations on large memory regions lazily.Itmaintains alimited-size set(default \nsize:40) of regionde\u00adscriptors, each of whichdescribes a range of more than10 contigu\u00adous memorylocations, \nalong with anotherlist of upto30 addresses excepted.Operationssuch asa .owtoanentireregionarerecorded \njustbymodifyingthedescriptor, and operations on single addresses are marked as exceptions. However, if \na region accumulates more than30 exceptions,itis either shrunk to excludethem(if they are allinthe .rsthalf),oreliminated. \n 4.4 Otherissues Because the analysis operates atthebinarylevel, allof thelibraries thataprogram uses \nareincluded automatically.Itwouldbepossible to treat malloc as part of the instrumented program, though \nwe currently inherit Memcheck s behavior of replacing the program s allocator. Doing so leaves the possibility \nof information .ow via the addresses returnedfrom malloc;this channel couldbeblocked by using a separate \narena for allocations inside enclosure regions, or randomizing the addresses. Inputs and outputs are \nrecognized based on system calls, such as read and write respectively.Memory-mappedI/Ois not recog\u00adnized,thoughdoing \nsowould notbedif.cultbecause every mem\u00adory operationis alreadyinstrumented. We have not studied the best \nextension of our technique to multi-threaded programs, since Valgrind implicitly serializes the programs \nit executes; it would likely suf.ce to execute enclosure regions atomically.(The case studies ofSection8 \nare all single\u00adthreaded.) Many aspects of a program s interactions with its environment might reveal \ninformation about its internals, such as how long it takes to execute or how much power the CPU draws. \nIf such side channels are re.ectedintheprogram s output,they canbeincluded in our approach:forinstance,the \nresultof gettimeofday couldbe treated as secret.However, observations made outside theprogram arebeyond \nthis scope of our technique.  5. Ef.cient maximum-.ow Computing themaximum .owinanetworkisalong-studied \ncom\u00adputational task, but the .ow graphs constructed by our technique are both very large and fairly well-structured, \nso specialized op\u00adtimizations are both necessary and possible. We have investigated both exact algorithms \nwith thepotential tobe ef.cient, and uncon\u00additionally ef.cient algorithms with thepotential tobeprecise.Em\u00adpirically, \nthelater approach seems to workbetter. 5.1 Exact approaches The best general algorithms for computing \na maximum .ow have time complexity atleast O(VE), where V and E arethe number of verticesand edgesintheinputgraph[11],butadynamicprogram \nanalysisis usually onlyfeasibleifits running timeis close tolinear in the running time of the original \nprogram. What is needed is an algorithmthatislikely to runin closetolineartimeonthegraphs that arisein \n.owanalysis. Flow graphs often have many nodes connected in series or parallel,forwhich .owcomputationislinear-time,soweexplored \nthe use ofSPQRtrees, anincrementalgraph representationthat can capture series-parallel structure[3].In \nour case studies, thegraphs have a mixture of series-parallel and non-series-parallel structure, with \nneither onedominant.Forinstance,in bzip2, thelargest non\u00adseries-parallel structure represents 16% of \nthe graph size over a range of input sizes, and this constant fraction of the graph still requires super-linearprocessing \ntime.Therefore, whileSPQRtrees capture some useful regularities, they do not appear suf.cient to allow \nthe technique to scale to very large graphs. More details of these experiments canbefoundin atechnical \nreport[33] andthe .rst author s thesis[31]. 5.2 Graph collapsingby codelocation An alternative to exactly \ncomputing the maximum .ow in large graphsisto simplifythegraphin a waythatmakesit much smaller, while \nstill being sound and not greatly increasing the maximum .ow.The mostimportant regularitiesinlargegraphs \nseem to come from loops in the original program, and are most easily exploited by using information about \nthe program. Our tool does this using the sameimplementation of edgelabelling and node collapsingthat \nwas described in Section 3.2: even the graph of a single run can be simpli.ed by combining edges with \nthe same context-sensitive code location, since the context does notdistinguish differentloop iterations. \nA graph can be collapsed even further by combining edgesbased on their codelocation(context-insensitive). \nThe size of the collapsed graph grows not with the runtime of the original execution, but with its code \ncoverage; since the latter Information flow (bits) 1e+08 1e+07 1e+06 100000 10000 1000 100 10 10 100 \n1000 10000 100000 1e+06 1e+07 1e+08 Input size (bits) Figure3. Theamountofinformationrevealedincompressing.les \nwith bzip2, as measuredby ourtool(notelog-log scale).The solid lineshowsthe .owsmeasuredby ourtool,inbits.Thedottedlines \nrepresent otherfunctionsthat wouldbeexpected tobound the .ow: The straight line through the origin represents \nthe input size. The two curvedlines(which are close tolinearbutdo notpass through the origin) represent \nthe size of theprogram s output, minus upper andlowerapproximationsoftheamountof output(such as .xed \nheaders andprogress messages) thatdoes notdepend on theinput. tends toplateau(andisboundedbytheprogram \nsize), muchlonger executions can be analyzed. Graph collapsing can potentially re\u00adduce precision, for \ninstance if two calculations in a loop had neg\u00adatively correlated .ows on different iterations. However, \nwe have not observedthistobe apracticalproblem(graph collapsing was enabledin all of our case studies). \n 5.3 Maximum-.owperformanceinpractice Totestthe scalabilityof ourgraph construction and maximum-.ow computations \nonlargegraphs, we ran ourtool on bzip2, ageneral\u00adpurpose(lossless) compressiontoolbased onblock sorting, \ncom\u00adpressing inputs marked as entirely secret. bzip2 was not intended as arealistictargetforsecurity \nanalysis(obviouslyitsoutput con\u00adtains the sameinformation asitsinput).We choseitbecauseit rep\u00adresents \na worst-case for our analysis sperformance: itis computa\u00adtionally intensive, almost all of the computation \noperates on data derived from the input, and it makes extensive use of large arrays thatnecessitatethelazinessdescribedinSection4.3.(Section8dis\u00adcusses \nlarger, more security-relevantprograms; for them the tool s overheadisless,because many operations are \nnot connected to the secret data.) Also, it is easy to select inputs of various sizes, and theexpected \namountofinformation .owcanbecomputed apriori togiveabound ontheexpected results.Wechoseaclassofinputs \nthat arehighly compressible: thedigits of p, written outinEnglish words,asin three point one four one \nfive nine . We ran our tool with context-sensitive edge collapsing, and bzip2 in verbose mode -vv with \na100kblock size.The computer was a 1.8GHz AMD Opteron 265 running Linux; bzip2 and our tool ranin32-bit \nmode. Figure3comparesthe .ow measuredby ourtooltothe expected bound, whichistheminimum ofthesize oftheinput,andthe \nsizeof thatportion ofthe outputthatdepends ontheinput.The exact value for the latter is somewhat uncertain, \nbecause part of the output format consists of .xed headers, and the commentary printed to the terminal \nis only partially input-dependent; so we estimate it with lower and upper bounds (curved dotted lines \nin the .gure). The results match our expectations: very small inputs cannot be compressedby bzip2,butforinputs \nthat bzip2 can compress, our tool s .owbound matchesthesizeof thecompressed output. The running time \nof our tool grows linearly over this range of input sizes, thanks to the lazy range operation implementation \nandgraph collapsing techniques.Forthelargestinput,2.5MB,the tool s running time was 1.5 hours. Though \nstill quite slow com\u00adpared to an uninstrumented execution, this time re.ects process\u00adingagraph(before \ncollapsing) with3.6billion nodes, since almost all of bzip2 s timeis spent operating on secretdata.(After \ncol\u00adlapsing, the graph had only about 22000 nodes and 30000 edges.) Whentracingcode thatis not operating \non secrets, nographis con\u00adstructed, sothetool sis overheadless,though still morethanMem\u00adcheck s. The \ntime to compute a maximum .ow on the collapsed graph wasless than a secondin all cases.  6. Checkinga.owbound \nOncethemain .owmeasurementtechniquediscussedinthispaper has been used to determine the amount of information \na program reveals under testing, users of a program would also like to check thatthe samebound alwaysholds \nastheprogramis usedindeploy\u00adment. Checking a bound is a simpler and faster than discovering it. Section \n6.1 describes how to compute a cut of the .ow graph fromamaximum .ow,thenSections6.2and6.3givetwochecking \ntechniques that use such a cut. 6.1 Computing a minimum cut A cut(an s-t cut, to be precise) is a way \nof dividing a .ow graph intotwopieces, one containing the source and the other the sink.A cut canbede.ned \nas the set of nodes thatliein thehalf containing the source,but we areinterestedin the set of edges that \ncrossfrom that set to its complement; their removal disconnects the source from the sink. There is duality \nbetween .ows and cuts, captured by the classic max-.ow-min-cut theorem: the value of any .ow is bounded \nby the capacity of any cut, and the maximum .ows are those with the same value as the minimum-capacity \ncuts, since thereis no way to augment them[11]. Onceamaximum .owhasbeendiscovered,ourtool computes a \ncut by .rst enumerating the nodes on the source side of the cut bydepth-.rstsearch: they are the nodes \nthat are reachablefromthe source along apath in which each edge has excess capacity. Then, the cut edges \nare those that connect nodes reached in the DFS to nodes not reached. On the analyzed run(s), the edges \nof the cut carried an amount ofdata equalto ourtool s estimate ofthe amount ofinformationthe executionrevealed,and \nallinformation .owsfromthesecretinputs to the public outputs passed through them. On future executions, \nthe amount ofdata corresponding edges carry willbe a sound mea\u00adsure of the information revealed, as long \nas no other .ows occur. Therefore, a static representation of the edges can be used to ef.\u00adciently check \nwhen an analogouspolicyholds onfuture executions, reducingdetection to a reachability check;Sections6.2 \nand6.3de\u00adscribe speci.cimplementations.Such checking will soundlydetect any leaks other than, or larger \nthan, those allowed by the cut; the pricepaidfor reducedoverheadisthat novelleaks may notbe mea\u00adsuredprecisely. \n 6.2 Tainting-basedchecking Checking that no secret information reaches the output other than across \na given cut is a simple tainting problem. We have imple\u00admented this as an alternate mode of our tool, \nreusing the bit-level taintinganalysisdescribedinSection2.3.The cut edges correspond to annotations that \nclearthetaintbits ondata, while simultaneously incrementinga counter ofinformation revealed.If any othertainted \nbitsreach theoutput oranimplicit .owoperation,they areconser\u00advatively counted in the same way, and the \nlocation reported: en\u00adclosure regions are still required. The runtime overhead of this ap\u00adproach is comparable \nto that of Memcheck: between 10 and 100 times the uninstrumented execution time. 6.3 Output-comparison \nchecking An even more ef.cient checking technique is based on running two copies of a program. The basic \nidea is to run two copies of a program in lockstep, one which initially has access to the secret input, \nand the other which operates on a non-sensitive input of the samesize.Atthepointwhentheprograms reach \nacutannotation, the program with the real secret input sends a copy of the values on the cut to the second \ncopy. If the programs produce the same output, then the data that the second program received from the \n.rst atthe cutsisthe only secretinformation needed toproduce the output,and the .owpolicyissatis.ed.If \ntheoutputsdiverge,then another .owispresentand executionshouldbeterminated.(The disadvantage compared \nto a tainting approach is that detecting a violation at output timeis oflesshelpin trackingdownits cause.) \nThe key advantage of this technique is that the execution of the twoprograms canbe mostly uninstrumented: \nthey only need to behave unusually at the cut points. Enclosure regions are also not required, aslong \nasthe non-sensitiveinputis such thattheprogram can execute the code that would be enclosed without crashing \nor looping. A factor of two overhead is less than any binary-level dynamic tainting system, and using \ntwo copies can take advantage of multipleprocessors. A simpler version ofthistechnique(without a cut,for \ncheck\u00ading only complete non-interference) has been implemented in an operating-system-level tool called \nTightLip [57]. We previously suggested the extension to quantitative policies by sending in\u00adformation \nat a cut, but in a theoretical context to convert an information-.owpropertyinto a safetypropertythatcouldbe \nmore easilyprovedbyinduction[34].  7. Discussion This section provides some additional discussion of \nthe ways in which a dynamic quantitative analysis would be useful in devel\u00adoping secure software,including \nwhichpoliciescanbequanti.ed, how to use a dynamic tool, and a comparison between our tech\u00adnique and standard \ntainting. A quantitative policy may only be an approximation to a com\u00adplete securitypolicy theprojection \nofa set of acceptable and un\u00adacceptablebehaviors onto a single axis butitis usually suf.cient to catchlarge \ncategories of attack.Forinstance,in a systemprotect\u00adingprivacyin a censusdatabase, a simplequantitativepolicy \ncould not prevent the query Was Stephen McCamant s income more or less than $40,000? , since it carries \nthe same amount of informa\u00adtionasanacceptablequerylike WastheaverageincomeofBoston residents more orless \nthan$40,000? . Butit couldprevent aquery fromrequesting theincomesof everyoneinBoston.Sincethe .ow bounds \nour tool supports are whole numbers, it is also important to control the number of times an attacker \nmight repeat a process, since even a small bound would become large if multiplied by a large number of \nrepeated requests; but if the executions are ana\u00adlyzed together, ourtool canbeused todetermine whetherthey \nare revealing the same ordifferentinformation. Our tool measures the .ows in particular executions, and \nis intended for testing or debugging: its results do not say anything about other possible executions, \nwhich might leak either more information orless.Aswith any otherkinds of testing,developers must choose \ninputs that exercise program behaviors relevant to a policy. It is still important for a dynamic tool \nthat its results never underestimate the amount of .ow that has occurred on a single run, even though \nthis soundness for adynamic analysisisdifferent # of secret Program KLOC libraries data  KBattleship \n6.6 37 shiplocations OpenSSH client 65 13 authentication key ImageMagick 290 20 original imagedetails \nOpenGroupware.org 550 34 schedule details Xserver 440 11 displayed text Figure 4. Summary of theprograms \nexaminedinthecase studies of Section 8. The program sizes, measured in thousands of lines of code (KLOC), \ninclude blank lines and comments, but do not includebinarylibraries(3rdcolumn, measured with ldd)thatwere \nincluded in the analysis but not directly involved with the security policy. from soundness for a static \nanalysis that describes all possible executions.AsdiscussedinSection6, othertechniques canbe used to \ncheck for violations of a policy on future executions, such as after a systemhasbeendeployed. No matter \nhow automated a .ow measurement tool is, it is still the responsibility of a developer to decide which \n.ows are acceptable, andhowto resolve any violations.Using atoollike ours can be seen as a kind of machine-checked \nauditing: the developer conjectures a securitypolicytheprogramis expected to satisfy, and the tool checks \nwhether it really is satis.ed in a particular case. Mismatches might either represent a policy that is \ntoo restrictive, or a bug in the program. The same kind of understanding and policy speci.cation would \nbe required to annotate a program with an information-.ow type system: the difference is that a dynamic \ntool canbe usedto examine oneprogram execution at atime, while a static approach requires that a policy \ncovering every possibility beprovided up-front. Our analysishas a close relationship withdynamic tainting: \nthe graph it constructs contains all the values that a tainting analysis would mark assecret.Ourtool \nreportsa .owof0bitsinexactlythe cases when a(sound) tainting analysis would allow aprogram; any program \nwith non-zero .ow would be rejected by a taint analysis (countingthe number of tainted outputbits corresponds \nto the total capacity of edgestothesinkinourgraph).Using maximum .ows allows our technique to .nd a more \nprecise .ow measurement, but it does not provide any more precise information about which parts of the \noutput contain secret information. For instance, in the example of Section 2.4, 64 bits of the output \nare tainted, and our tool .ndsthattogether,thesebitscarry9bitsofinformationabout the secretinput.Butitis \nnotpossible topick out aparticular9bits out of the64 that contain theinformation. 8. Case studies To \nlearn about the practical applicability of our tool, we used it to test a different security property \nin each of .ve open-source applications.Theprograms andthe secretinformationprotected are summarized \nin Figure 4. In each program the secret information participates in implicit .ows, and is partially disclosed \nin ways that are nonetheless acceptable; thus both a quanti.ed policy and asound treatment ofimplicit \n.owsareneeded. To obtain precise results, all of the programs required enclo\u00adsure region annotations. \nSection 8.6 describes a pilot experiment with a very simple static analysis for C which was able to infer \na majority of the annotations used, and discusses how to improve its results by adding other standard \ntechniques. We supplied the remaining enclosure annotations by hand: we found the locations where they \nwere needed by running the tool in a mode in which every implicit .ow operation caused a warning message. \nBecause of limitations in our current syntax for specifying such regions, this sometimes required local \ncode refactorings, such introducing a temporary variable to hold a return value. Writing annotations \nwas easy: we spent about as much time writing such annotations as compiling and con.guring the programs \nto run on our system and developing test casesfor the relevantpolicies. 8.1 KBattleship In the children \nsgameBattleship, successfulplay requireskeeping secretsfromone s opponent.Eachplayer secretly chooseslocations \nforfour rectangular ships on agridrepresentingthe ocean, andthen theplayerstaketurns .ring shotsatlocationsontheotherplayer \ns board. The player is noti.ed whether each shot is a hit or a miss, andif ahithas sunk a complete ship.Aplayer \nwinsby shooting all ofthe squares of all ofthe opponent s ships.In a networked version of thisgame, one \nwouldlike toknowhow muchinformation about thelayout of one sboardis revealedinthe network messagestothe \nother player. If the program is written securely, each missed shot by the opponent should reveal only \none bit, since hit and miss represent only two possibilities. KBattleship is an implementation of the \ngame that is part of the KDE graphical desktop. We used our tool to measure how much information about \ntheplayer s ship locationsis revealed whenplayingKBattleship. We were inspired to try this example because \nJif, a statically information-.ow secureJavadialect(thelatestdescendant ofthe workdescribedin[35]) includes \nas an example a500-lineBattle\u00adshipgame.Apparently unlikeJifBattleship,however,theversion of KBattleshipwe \nexamined(3.3.2) contains aninformationleakbug. In respondingto an opponent s shot,a routine calls a methodnamed \nshipTypeAt to check whether aboardlocationis occupied, and re\u00adturns theinteger return value in the network \nreply to the opponent. However, as the name suggests, this return valueindicates not only whetherthelocationis \noccupied,butthetype(length) of the ship occupying it. An opponent with a modi.ed game program could use \nthisfact toinfer additionalinformation about the state of adja\u00adcent board locations. The KBattleship \ndevelopers agreed with our judgementthatthispreviouslyunrecognizedleakage constituted a bug, and ourpatch \nforit appears in version 3.5.3.Though thisbug showsup asexcessive .owunderourtool,wediscovereditby in\u00adspection \nwhile considering whether to use the program as a case study(before the tool wasimplemented). Our tool \ncan verify that the bug is eliminated in a patched version: we marktheposition and orientation of each \noftheplayer s ships as secret, and measurehow much of thisinformation reaches the network. In response \nto a miss, the program reports one bit of information;a non-fatalhit reveals twobits, oneindicating the \nshot is a hit and a second indicating it is non-fatal. These .ows can be observedin realtimeby running \nourtoolin a modethat recomputes the .ow on every program output, or each second, whichever is less frequent. \nInformation about the ship locations is also revealed via the program s graphical interface, but we excluded \nthat code from the analysis by explicitly declassifying some data passed to drawing routines; thus this \nanalysis could miss leaks that occurred through theGUIlibraries. 8.2 OpenSSH OpenSSHisthemost commonly \nused remote-loginapplicationon Unix systems. In one of the authentication modes supported by the protocol, \nan SSH client program proves to a remote server the identity of thehost on whichitis running using a \nmachine-speci.c RSA key pair. For this mode to be used, the SSH client program must be trusted to use \nbut not leak the private key, since if it is revealed to the network or even to a user on the host where \nthe client is running, it would allow others to impersonate the host. (We were inspired to consider this \nexample by the discussion of itbySmith andThober[49].) We used our tool to measurehow Figure 5. Image \ntransformations vary in how much information they preserve. Our tool veri.es that pixelating (left) or \nblurring (middle)the originalimage(top,375120bits), reveals only1464or 1720bitsrespectively.By contrast,thebound \nourtool .ndsforthe information revealedbya twistingtransformation(right)is375120 bits, no less than the \ninput size.Applying the same transformation with the opposite direction to the twisted image gives back \nan imagefairly close to the original(lower right). much information about the private key is revealed \nby a client execution using this authentication mode, by marking the private key(a number of arbitrary-precisionintegers) \nas secret asitis read from a .le. Our tool .nds that 128 bits of information about the secret key arerevealed.The \ncutlocation revealsthatthisistheMD5 checksum of a response that includes a value decrypted with the public \nkey, as expected under the protocol. Of course, our tool is not able to verify that MD5 is a secure one-way \nfunction, though that belief is part of why revealing those particular 128 bits is acceptable. Our tool \ndemonstrates that if the 218-line MD5 implementation is secure, the entire execution obeys the con.dentiality \nproperty: no informationleaksfrom the rest of theprogram. 8.3 ImageMagick ImageMagickisasuiteofprogramsforconverting \nand transform\u00adingbitmapimages.We evaluatedsome ofitstransformations to as\u00adsesshow muchinformation aboutthe \noriginaltheypreserve.Forin\u00adstance,ifonetriestoanonymize aphotographby obscuringthe sub\u00adject sface, usingatransformationthatpreserves \nverylittleinforma\u00adtion wouldprevent the originalfacefrombeing reconstructed. Figure 5 shows an original \n125-pixel square image, which is represented by 375120 bits in an uncompressed PPM format, and the output \nof three different transformations. Pixelation to a 5x5 grid uses the options -sample 5x5 -sample 125x125, \nwhile blurring uses -resize 5x5 -resize 125x125, and the twisting transformation uses -swirl 720. Though \nall three transformed images are visuallyunidenti.able,theydiffergreatlyinthe amount of information they \npreserve, as our tool veri.es. Pixelation and blurring both involve shrinking the image to a small intermediate \nform and then enlarging it, so the maximum .ow is dominated by the size of the intermediate form. Since \nImageMagick uses 16-bit pixel component valuesinternally, a5-pixel squareimageis repre\u00adsentedby1200bits.Inadditiontherearesomeimplicit \n.ows,since theheaderof the .le,whichincludesitssizeand othermetadata,is also considered secret. In total \nour tool gives bounds of 1464 bits revealedforpixelation and1720bitsforblurring. On the otherhand, the \ntwist transformation computes each out\u00adput pixel by .nding the corresponding input image location under \na continuous transformation, andinterpolatingbetween thefourin\u00adputpixels nearit.Thereis no apparent bottleneck \nin this computa\u00adtion, so our tool s bound is the same as the input and output size, 375120bits.Thoughthe \nresultis onlyan upperbound, anddoes not prove thatnoinformationislost,it accords with theintuition that \na continuous transformationis reversible, asidefromblurring caused by the interpolation. In fact, a twist \nof the same magnitude in the opposite direction gives back an image fairly close to the original (andmore \nsophisticatedinversion techniques arepossible). 8.4 OpenGroupware.org OpenGroupware.org is a web-based \nsystem for collaboration be\u00adtween usersin an enterprise,providing email and calendarfeatures similar \nto Microsoft Outlook or Lotus Notes. We focused specif\u00adically on its appointment scheduling mechanism. \nEach user may maintain a calendar listing of personal appointments, and the pro\u00adgram allows one usertorequesta \nmeeting witha second userduring a speci.ed time interval. The program then displays a grid that is colored \naccording to what times the second user is busy or free. Thisgrid isintended toprovide enough information \nabout the sec\u00adond user s schedule to allow choosing an appropriate appointment time, but without revealing \nall the details of the schedule: for in\u00adstance,theboundaries of appointments are not shown, andthegran\u00adularity \nof thedisplay is only 30 minutes. We used our tool to mea\u00adsure the amount of information about the user \ns calendar this grid reveals, marking the starting and ending times of appointments as tainted when theprogram \nreads them with aSQLquery. Forinstance,for aproposalfor a onehour appointmentbetween 9:00am and6:00pm, \nwhen the targetuserhas an appointmentfrom 10 to noon, our toolbounds the amount ofinformation revealed \nas 12 bits. In previous experiments using the tainting version of our tool, we had discovered that a \nloop that computes time period in\u00adtersections unnecessarily considered times every minute, and .xed it \nto use the same half-hour interval as the .nal display; the 12-bit measurement corresponds to a cut at \nchecks madein thisloop. Thisexamplealsodemonstratesthepossibilityofdifferent .ow estimates that are equally \ncorrect,butdifferin when they are more precise. Later in the code, the objects created in the intersection\u00adcheckingloop \nare used todecide whether each of the18 squaresin thegrid should be colored beige or red; a cut there \nwould measure every one-day appointment search as revealing18bits.Forthe case of a single morning appointment, \na cut attheintersectionloopgives a moreprecisebound,butif the userhad many appointments,later in theday, \nan18-bitboundfrom thedisplay routine wouldbe more precise. 8.5 XWindowSystemserver In the X Window System \ncommonly used on Unix, a single pro\u00adgram called the X server manages the display hardware, and each program(X \nclient) that wishestodisplay windows communicates with the server over a socket.TheX server s mediating \nrole makes ita signi.cantpotential source of securityproblems:programs can useitto communicate with each \nother(including using the same mechanisms that support cut and paste), and any information dis\u00adplayed \non the screen also passes through the server. The original design ofX addressed security only with respect \nto access control; more recently, the protocol has been extended with mechanisms that can enforce information-.owpolicies,by \ndividing clientsinto trusted and untrusted classes and restricting what untrusted clients cando[55].However,itcanbedif.cultinalargemonolithic \nsys\u00adtemliketheXservertoensurethatenoughpermissions checkshave been added.SincetheXserveris writteninC,thereis \nalsothedan\u00adger thatan attack such as abuffer over.ow could allow any checks to be subverted. As an alternate \napproach, we examined whether it is possible to avoid trusting most of the server implementation, and \ninstead enforce our information .ow goals directly. We used our tool to measure how much information \nfrom client programs is revealed to other clients or otherwise leaked from the server, by marking textdata \nas secret whenit arrivedin requests usedfor cut\u00adand-paste ordrawing text on the screen. Data bytes provided \nfor cut-and-paste are uninterpreted by the server, and cause no implicit .ows. By contrast, drawing text \non the screeninvolves a number of computations: looking up bitmaps from a font, computing the width of \nthe area drawn, and drawing eachpixel accordingtothe current rendering mode.The main effect is to change \npixels in the framebuffer, which we do not count as a public output;butasa side effect,the serveralso \ncomputes abound\u00ading box for the text that was drawn, for use in later redrawing cal\u00adculations.Thedimensions \nof thisbounding box revealinformation about the text that wasdrawn,in the same way that thedimensions \nof ablack redaction rectanglein adeclassi.eddocument would,by constraining the sum of the widths of the \ncharactersdrawninside. Forinstance, our tool estimates(somewhat imprecisely) thatin onefont anddrawing \ncontext,theboundingboxgeneratedfromthe string Hello, world! could reveal up to21bits about the charac\u00adters \nof the string. However, on examining the location of this pos\u00adsible leak, it was clear to us that it \ncould be eliminated by using a more conservativebounding box(notdependent on the contents of string), \nperhaps at the expense of requiring more redrawing later. Once the expected leaks are accounted for, \neither with cut anno\u00adtations or algorithmic changes, a dynamic checking tool can catch any other information \n.ows that violate the policy. For instance, we used our tainting-based checker with a single policy to \ncatch bothleaks causedby user errors,likepasting textfrom a secret ap\u00adplication into an untrusted one, \nand code injection attacks, like a simulated exploitation of a aninteger over.ow vulnerability[24] in \nwhich code supplied via a network request walks through mem\u00adory, looks for strings of digits that resemble \ncredit card numbers, and writesthemtoahidden .lein /tmp. 8.6 Inferring enclosure regions Enclosure regions, \nintroduced in Section 2.2 (and illustrated in Figure 2), are static program annotations that improve \nour tool s precision by directing the implicit .ows from a code region to the locations holding results \nused by the rest of the program. This section discusses how they can be inferred by static analysis. \nWe .rst describe the general approach, then describe a pilot study with a simple analysis tool. Even \nour very simple analysis tool discovered most of the annotations needed in our case studies, and the \naspectsitdidnot cover couldbehandledby other standard static analysis techniques. An enclosure regiondelimitsparticular \nstartingand endingpro\u00adgramlocations, andlistslocations, whichwe call outputs, thathold results used in \nthe rest of the program. If no implicit .ows occur within them, enclosure regions have no effect, so \nan inference can simply choose starting and ending points enclosing every possi\u00adble implicit .ow operation \nin a program. Also, there is no harm in including extra outputs that might not be read. Therefore, the \nkey challenge in inferring enclosure regions is, given a fragment of code in aprogram, to conservatively \ndetermine alist ofdata lo\u00adcations it might write to: essentially a kind of side-effect analysis. As with \nother kinds of side-effect analysis, it is necessary to take aliasinginto account[7,46]:in our case,the \nannotation requires pilot analysis hand need missed found Program annot. length exp n interproc.   \n  bzip2 79 17 17 13 49 OpenSSH client 2 0 0 1 1 ImageMagick 23 1 1 0 22 Xserver 19 2 0 2 17 Figure6. \nSummaryofthe results ofthe static analysisdiscussedin Section8.6 to compute whichlocations a code region(containing \nan implicit .ow) might modify. Overall, the pilot analysis found 72%( found column)ofthehand-veri.ed \noutput annotations used in the case studies( hand annotations column). an expression valid at the enclosure \nentrance that must-aliases the lvalue expressionin alater assignment, similartotheinterstatement must-aliaspairs \nusedbyQian et al.[44]. For an initial assessment of the prospects for automatic infer\u00adence of enclosure \nregions, we built a very simple pilot implemen\u00adtation, and compared its results to the complete hand-checked \nan\u00adnotations used in the case studies above. The inference is a static analysis forC source code,based \non theCILframework[38].Itis intraprocedural, syntax-directed, andcontext-insensitive, operating as a \nsingle pass that disregards control .ow except as implied by block structure. It does not use an alias \nanalysis, so it only .nds locations that can be named by the same expression at the region entrance as \nat the modi.cationlocation. Treatingthe setof output annotations usedinthe case studies as our target, \nwe measuredhow many of the region outputs annotated by hand were found correctly by the pilot analysis. \nThe results of the comparison are showninFigure6.(The remaining case stud\u00adies are written in C++ or Objective \nC, so CIL cannot parse them.) Overall, even this very simple analysis found 72% of the required annotations:inmost \ncases,theimplicit .ow,side-effect,and anno\u00adtation were all close together, and no aliasing wasinvolved. \nWe then further classi.ed the remaining missed outputs, deter\u00admining that more sophisticated analysis \nin two areas would be re\u00adquired toinfer afull set of annotations: arrays, andinterprocedural aliasing. \nThe column need length in Figure 6 counts the outputs where thelocationbeing writtento was adynamically \nallocated ar\u00adray, andthe enclosure annotationhas abound(currently supplied by hand) on the size of the \narray. These bounds would not be re\u00adquiredin alanguagelikeJava whose arrayskeeptrack of their own size. \nAmong the output annotations the tool missed, the column missed/ expansion counts cases where theinferred \nenclosure re\u00adgion referred to only a single element in an array, but it needed insteadto refertothe entire \narray, commonlybecause theindex ex\u00adpressionwasnotconstant.Finally,thecolumn missed/ interpro\u00adcedural \ncounts cases where the annotation we added byhand was in a different function than the side-effecting \noperation. While we found no cases in which an intraprocedural alias was required, in\u00adterprocedural annotations \noften required that the modi.ed location be referred to with adifferent expressionin the annotation, \nsuch as by substituting an argument expression in place of a parameter in anlvalue expression. Comparingthe \nresultsbetweenthe various case studyprograms, bzip2 is an outlierin the complexity ofits annotations,because \nof itssophisticateduse of arrays andpointers:forinstance,to conserve space, many of its main data structures \nare allocated as subranges of twolarge arrays.  9. Related work Our technique combines some of the attributes \nof static analyses (includingtype systems) that checkprogramsforinformation-.ow security ahead of time, \nand ofdynamic tainting analyses that track data .owinprogramsasthey execute. 9.1 Staticinformation-.ow \nStatic checking aims to check the information-.ow security of programsbefore executingthem[13].The mostcommontechnique \nuses a type system, along with a declassi.cation mechanism to allowcertain .ows.Itisalsopossibletoquantifyinformation \n.ows in a static system, though thishasbeendif.cult to makepractical. Despite advances suchas selectivedeclassi.cation[20,36],bar\u00adriers \nremaintothe adoption ofinformation-.owtype checking[53] extensions togeneralpurposelanguages[35,48,27].Statictype \nsystems may also be too restrictive to easily apply to pre-existing programs:forinstance, we are unaware \nofanylargeJava orOCaml applicationsthathavebeen successfullyported totheJif[35](clos\u00adest are thepoker \ngame of[2] and the email client of[25]) or Flow Caml[48] dialects.Techniquesbased on type safety areinappli\u00adcable \ntolanguages thatdo notguarantee type safety(such asC) or ones with no statictype system(such as many \nscriptinglanguages). Information-.ow type systems generally aim to prevent all in\u00adformation .ow. Many \ntype systems guarantee non-interference, the property that for any given public inputs to a program, \nthe public outputs will be the same no matter what the secret inputs were[22,53].Becauseitis often necessaryinpractice \nto allow some information .ows, such systems often include a mechanism for declassi.cation: declaring \npreviously secret data to be public. Such annotations are trusted: if they are poorly placed, a program \ncanpass atype checkbut stillleak arbitraryinformation.The mini\u00admum cutsdescribedinSection6 couldbe used \nto choose theplace\u00adment ofdeclassi.cation annotations, sincethey wouldbe a minimal interfacebetween secret \nanddeclassi.eddata.However, wedo not envisionthemtobeatrusted representationoftheinformation .ow policy: \nrather, the policy is a numeric .ow bound, and a cut is an untrustedhint to assist enforcement. Quantitative \nmeasurements based on information theory have often been used in theoretical de.nitions of information-.ow \nse\u00adcurity [23, 16, 28]. Clark et al. s system for a simple while lan\u00adguage[9]isthemost completestaticquantitativeinformation \n.ow analysis for a conventional programming language. Any purely static analysisisimpreciseforprogramsthatleakdifferent \namounts of information when given different inputs. For instance, given an example program with a loop \nthat leaks one bit per iteration, but without knowing how many iterations of the loop will execute, the \nanalysis must assume that all the available information will be leaked. A formula giving precise per-iteration \nleakage bounds for loops[29] maybedif.cult to automate.Our technique s results re\u00ad.ect the number ofiterations \nthat occur on aparticular execution. 9.2 Dynamic tainting Manyofthe vulnerabilitiesthat allowprograms \ntoinadvertently re\u00adveal information involve a sequence of calculations that transform secret input into \na different-looking output that contains some of the same information. To catch violations of con.dentiality \npoli\u00adcies, it is important to examine the .ow of information through calculations, including comparisons \nand branches that cause im\u00adplicit .ows.Severalrecentprojectsdynamically trackdata .owfor data con.dentiality \nand integrity, but without a precise and sound treatment ofimplicit .ows. Some of the earliest proposed \nsystems for enforcing con.den\u00adtialitypolicies onprograms werebased on run-time checking:Fen\u00adton discovered \nthe dif.culties of implicit .ows in a tainting-based technique[19], andGat andSaalpropose reverting writes \nmadeby secret-using code[21]toprevent .ows.Thegeneral approach clos\u00adest to ours,in which run-time checkingis \nsupplemented with static annotations to account for implicit .ows, was .rst suggested by Denning[14].However,thesetechniques \naredescribedas architec\u00adtures for new systems, rather than for as tools evaluating existing software, \nand they do not support permitting acceptable .ows or measuringinformationleakage. Many recent dynamic \ntools to enforce con.dentiality policies do not account for all implicit .ows. Chow et al. s whole-system \nsimulatorTaintBochs[8] tracesdata .owattheinstructionlevel to detect copies of sensitive data such as \npasswords. Because it is concerned only with accidental copies or failures to erase data, TaintBochsdoesnottrack \nallimplicit .ows.Masri etal.[30] de\u00adscribe adynamicinformation-.ow analysis similartodynamic slic\u00ading, \nwhichrecognizes someimplicit.ows via codetransformations similarin effectto our simple enclosure regioninference.However, \nit appears that other implicit .ows are simply ignored, and their casestudiesdonotinvolveimplicit .ows. \nDYTAN [10],ageneric framework for tainting tools, applies a similar technique at the bi\u00adnary level, where \nthe dif.culties of static analysis are even more acute. In case studies on Firefox and gzip, they found \nthat their partialsupportforimplicit .owsincreasedthenumberofbytesthat were tainted in a memory snapshot, \nbut they did not evaluate how close their tool came to a sound tainting. For instance, they mark theinput \nto gzip as tainted, much as wedo with bzip2,butdo not measure whether the output was tainted. Accounting \nfor all implicit .ows requires static information, as provided by enclosure regions in our system. Several \nprojects have combined completely automatic static analyses withdynamic checking: the key challenge is \nmaking such analysis scalable and suf.cientlyprecise. TheRIFLEproject[52]is an architectural ex\u00adtension \nthat tracks direct and indirect information .ow with com\u00adpiler support. The authors demonstrate promising \nresults on some realisticsmallprograms,buttheirtechnique sdependence onalias analysis leaves questions \nas to how it can scale to programs that store secrets in dynamically allocated memory. Our approach also \nuses a mix of static analysis and dynamic enforcement, but our static analysis only needs to determine \nwhich locations might be written, whileRIFLE attempts to match eachload with allpossible storestothesamelocation, \nwhichismoredif.culttodopreciselyin thepresence of aliasing.Two recent tools[37,6] apply toJavapro\u00adgrams, \nmaking static analysis somewhat easier: their experimental results show low performance overheads, but \ndo not measure pre\u00adcision.None of these tools enforce aquantitative securitypolicy. In an earliertechnicalreport[32], \nwepresented atainting-based quantitative information-.ow analysis that was the predecessor to the implementation \ndescribed here. That system had no maximum .ow or minimum cut analysis; instead it used manual annotations, \ncalled preemptive leakage annotations, that played the role of a(not necessarily minimal) cut. Enclosure \nregions in that system were also manually supplied, and were unsoundbecause theyprop\u00adagated tainting \nonly to locations that were dynamically accessed. More recently, wegave a soundnessproof[34] for a simplefor\u00admalized \nsystem that can be seen as modelling our tainting based implementation, with enclosure regions modi.ed \nto be sound by specifying outputs in the same way those in the present paper do. The simulation proof \ntechnique used there could be extended to thepresent systemby treating the minimum cut corresponding \nto a maximum .owasapreemptiveleakageannotation.Aslong asall possibleinformation .owsarecaptured asedgesinthegraph,any \ncutinthatgraph representsasound .owbound.(Theleakagean\u00adnotationsin[34] are static; weplanto extendthat \nresultto a cutthat isdependent on theprograminputbyformalizing of the soundness de.nition ofSection3.) \nRestrictionsoninformation .owcanalsobeenforcedby anop\u00aderating system.Traditional mandatory access control(MAC) \ntech\u00adniques[15] atthegranularity ofprocessesand .lesaretoocoarse for the examples we consider.Anew operating \nsystem architecture with lightweight memory-isolated processes, such as the event processes of theAsbestos \nsystem[18] or similar mechanismsin HiStar[58],ismoresuitableforcontrolling .ne-grainedinforma\u00adtion .ow,butisnot \ncompatiblewith existing applications.Likeour technique s enclosure regions, Asbestos event processes \nprovide isolation of side effects, but they are implemented using hardware memoryprotection. In attacks \nagainstprogram integrity, thedatabytesprovided by an attacker are often used unchangedby the unsuspectingprogram. \nThus, many such attacks canbeprevented by an analysis that sim\u00adply examineshowdatais copied.Quantitativepolicies \nare rarely used for integrity; one exception is recent work by Newsome and Song[42],which measuresthe \nchannel capacity between aninput and a control-.ow decision to distinguish between legitimate in\u00ad.uence \nand malicious subversion. Their measurement technique, based on querying the space of possible outputs \nwith a decision procedure,is verydifferentfrom ours. The most active area of research is on tools that \nprevent integrity-compromising attacks on network services, such as SQL injection and cross-site scripting \nattacks against web applications and code injection into programs susceptible to buffer overruns. These \ntools generally ignore implicit .ows or treat them incom-pletely.Newsome andSong sTaintCheck[41]isbasedonthe \nsame Valgrind framework as our tool, while other researchers have sug\u00adgested using more optimized dynamic \ntranslation[26,45], source\u00adlevel translation[56], or novelhardware support[50] toperform such checking \nmore quickly. The same sort of technique can also be used in the implementation of a scripting language \nto detect attacks such as the injection of malicious shell commands (as in Perl s taint mode [54])orSQL \nstatements[43]. 10. Futuredirections Directionsforpossiblefurther application oftheseideasincludein\u00adteractionsbetweendifferentkinds \nof secret, replacing thedynamic parts ofthe current technique toproduce a completely static analy\u00adsis, \nand supportinginterpretedlanguages withouttrusting theinter\u00adpreter. 10.1 Differentkindsof secret If a \nprogram operates on different classes of secret information, such as Alice s secrets and Bob s secrets, \nor classi.ed secrets and top secret secrets, our analysis can be used independently for each kind of \nsecret. This is conceptually straightforward, and possible withour currenttooljustby running aprogram \nrepeatedly, but for ef.ciency and ease of use, it would be better for to run the analyses together. A \nquestion is how much of the analysis can be shared between kinds of secret without hurting precision. \nFor instance, woulditbe enough tohave one set ofgraph capacitiesfor anykind of secret, or should thebit-width \nanalysisbe repeated? There may also be a possibility of increasing precision by an\u00adalyzing the interactions \nbetween different types of secret, because of crowding-out effects: for instance, a certain byte might \nbe able to store 8 bits of Alice s data, or 8 bits of Bob s data, but not both at once. However, the \nobvious approach of analyzing the .ows of multiple kinds of information as multi-commodity .ow would \nnot besoundingeneral,becausemultipleinformation .owscanshare capacity via coding[1]. 10.2 An all-static \nmaximum-.ow analysis Since thedynamic analysis consideredin thebody of thispaper al\u00adready takesadvantageof \nstaticinference,and wefound that a .ow graphlabelledwith staticidenti.ers wasfairlyprecise,itisinstruc\u00adtivetoconsiderhowthesamebasicideaof \nnetwork maximum .ow could be applied to an entirely static version of the information\u00ad.owtask.The .owgraphsweconsideraresimilartotheprogram \ndependencegraphs usedin slicing, andthedynamicbit-widthanal\u00adysis ofSection2.3has a close static analogue[5].Thekeydif.\u00adcultyislikelyhowtobound \nthenumberof timesastatic .owedge will execute, in terms of a developer-understandable parameter of the \nprogram input. The result of a static information .ow analysis would needtobe aformulainterms of suchparameters, \nratherthan a single number. 10.3 Supportinginterpreters In the past, information .ow tracking for languages \nsuch as Perl and PHP has been implemented by adding explicit tracking to op\u00aderationsin aninterpreter[54,43].However, \nsince suchinterpreters arethemselves writteninlanguages such asC,analternativetech\u00adnique would be to \nadd a small amount of additional information about theinterpreterto makeits control-.ow state accessible \nto our tool in the same way a compiled program s is, and then use the rest of the tracking mechanism \n(for data) unchanged. This tech\u00adnique is analogous to Sullivan et al. s use of an extended program counter \ncombining the real program counter with a representation of the current interpreter location to automatically \noptimize an in\u00adterpreter viainstructiontrace caching[51].Compared to ahand\u00adinstrumented interpreter, \nthis technique would exclude most of the scripting language s implementation from the trusted computing \nbase, and could also savedevelopment time.  11. Conclusion We have presented a new approach for determining \nhow much in\u00adformation a program reveals, based on the insight that maximum .ow is a more precise graph \nmodel of information propagation than reachability(asimplementedby tainting) is.Using apracti\u00adcal quantitative \nde.nition of leakage, the technique can measure the information revealed by complex calculations involving \nim\u00adplicit .ows. By applying that de.nition with an instruction-level bit tracking analysis and optimized \ngraph operations, it is applica\u00adbleto realprograms writteninlanguages such asC andC++.Ina series of case \nstudies, our implementation checked a wide variety of con.dentiality properties in real programs, including \none that was violated by a previously unknown bug. We believe this tech\u00adniquepointsoutapromising newdirectionforbringing \nthepower of language-based information-.ow security to bear on the prob\u00adlemsfacedby existing applications. \n Acknowledgments This research was supported in part by DARPA under contracts FA8750-06-2-0189 and HR0011-06-1-0017, \nand by an NSF grant CCR-0133580. References [1] R. Ahlswede, N. Cai, S.-Y. R. Li, and R. W. Yeung. Network \ninformation .ow. IEEE Transactions on Information Theory, 46(4):1204 1216, 2000. [2] A. Askarov and A. \nSabelfeld. Security-typed languages for implementation of cryptographic protocols: A case study. In \nProceedings of the 10th European Symposium On Research In Computer Security (LNCS 3679), pages 197 221, \nMilan, Italy, September12 14,2005. [3] G.D.BattistaandR.Tamassia.Incrementalplanarity testing.In 30th \nAnnualSymposium onFoundationsofComputerScience,pages436 441, Research Triangle Park, NC, USA, October \n30 November 1, 1989. [4] M. D. Bond and K. S. McKinley. Probabilistic calling context. In Object-OrientedProgrammingSystems,Languages, \nandApplications (OOPSLA 2007), pages 97 112, Montr\u00b4eal, Canada, October 23 25, 2007. [5] M. Budiu, M. \nSakr, K. Walker, and S. C. Goldstein. BitValue inference: Detecting and exploiting narrow bitwidth computations. \nIn European Conference on Parallel Processing, pages 969 979, Munich,Germany,August29 September 1,2000. \n[6] D. Chandra and M. Franz. Fine-grained information .ow analysis and enforcement in aJava virtual machine. \nIn 23rdAnnualComputer Security Applications Conference,pages463 475,MiamiBeach,FL, USA,December 10 14,2007. \n[7] J.-D. Choi, M. Burke, and P. Carini. Ef.cient .ow-sensitive interprocedural computation of pointer-induced \naliases and side effects. In Proceedings of the Twentieth Annual ACM SIGPLAN-SIGACTSymposium onPrinciples \nofProgrammingLanguages,pages 232 245,Charleston, SC,January 1993. [8] J.Chow,B.Pfaff,T.Gar.nkel,K.Christopher, \nandM.Rosenblum. Understanding data lifetime via whole system simulation. In 13th USENIXSecurity Symposium,pages \n321 336,SanDiego,CA,USA, August11 13,2004. [9] D. Clark, S. Hunt, and P. Malacaria. Quanti.ed interference \nfor a whilelanguage. In Proceedings of the2ndWorkshop onQuantitative Aspects ofProgrammingLanguages(ENTCS112), \npages 149 159, Barcelona, Spain,March27 28,2004. [10] J. Clause, W. Li, and A. Orso. Dytan: A generic \ndynamic taint analysis framework. In ISSTA 2007, Proceedings of the 2007 International Symposium on Software \nTesting and Analysis, pages 196 206,London,UK,July10 12,2007. [11] T. H. Cormen, C. E. Leiserson, and \nR. L. Rivest. Introduction to Algorithms. MIT Electrical Engineering and Computer Science Series.MITPressandMcGraw-Hill,Cambridge,Massachusetts \nand NewYork,NewYork,1990. [12] T.M.CoverandJ.A.Thomas. Elements ofInformationTheory. John Wiley,1991. \n[13] D. E. Denning. A lattice model of secure information .ow. Communications of the ACM,19(5):236 243, \nMay1976. [14] D. E. R. Denning. Secure Information Flow in Computer Systems. PhD thesis,PurdueUniversity, \nMay1975. [15] Department ofDefenseComputerSecurityCenter. TrustedComputer System Evaluation Criteria, \nAugust1983. CSC-STD-001-83. [16] A. Di Pierro, C. Hankin, and H. Wiklicky. Approximate non\u00adinterference. \nIn15thIEEEComputerSecurityFoundationsWorkshop, pages3 17,CapeBreton,NovaScotia,Canada, June24-26,2002. \n[17] W.Drewry andT.Ormandy. Flayer:Exposing applicationinternals. In FirstUSENIXWorkshop onOffensiveTechnologies, \nBoston, MA, USA,August6,2007. [18] P. Efstathopoulos, M. Krohn, S. VanDeBogart, C. Frey, D. Ziegler, \nE.Kohler,D.Mazi`eres,F.Kaashoek, andR.Morris. Labelsand event processes in the Asbestos operating system. \nIn Proceedings of the 20th ACM Symposium on Operating Systems Principles, pages 17 30,Brighton,UK,October \n32 26,2005. [19] J. S. Fenton. Memoryless subsytems. The Computer Journal, 17(2):143 147, May1974. [20] \nE.Ferrari,P.Samarati,E.Bertino,andS.Jajodia.Providing.exibility ininformation .owcontrolforobject-oriented \nsystems.In 1997IEEE Symposium on Security and Privacy, pages 130 140, Oakland, CA, USA,May4 7,1997. [21] \nI. Gat and H. J. Saal. Memoryless execution: A programmer s viewpoint. Software:Practice andExperience, \n6(4):463 471, 1976. [22] J.A.GoguenandJ.Meseguer.Securitypolicies and security models. In 1982 IEEE Symposium \non Security and Privacy, pages 11 20, Oakland, CA,USA,April26 28,1982. [23] J. W. Gray III. Toward a \nmathematical foundation for information .ow security. In 1991IEEESymposium onResearchinSecurity and Privacy,pages21 \n34,Oakland,CA,USA,May20 22,1991. [24] M. Herrb. X.org security advisory: multiple integer over\u00ad.ows in \nDBE and Render extensions, January 2007. http: //lists.freedesktop.org/archives/xorg-announce/ 2007-January/000235.html. \n[25] B. Hicks, K. Ahmadizadeh, and P. McDaniel. From languages to systems: Understanding practical application \ndevelopment in security-typed languages. In Proceedings of the 2006 Annual Computer Security Applications \nConference, pages 153 164, Miami Beach,FL,USA,December 11 15,2006. [26] V.Kiriansky,D.Bruening,andS.Amarasinghe.Secureexecutionvia \nprogram shepherding. In 11th USENIX Security Symposium, pages 191 206,SanFrancisco, CA,USA,August7 9,2002. \n[27] P. Li and S. Zdancewic. Encoding information .ow in Haskell. In 19th IEEEComputer Security Foundations \nWorkshop, pages 16 27, Venice,Italy,July5-6,2006. [28] G. Lowe. Quanti.ying information .ow. In 15th \nIEEE Computer Security Foundations Workshop, pages 18 31, Cape Breton, Nova Scotia,Canada,June24-26,2002. \n[29] P. Malacaria. Assessing security threats of looping constructs. In Proceedings of the34rdAnnualACMSIGPLAN-SIGACTSymposium \non Principles of Programming Languages, pages 225 235, Nice, France,January 17 19,2007. [30] W. Masri, \nA. Podgurski, and D. Leon. Detecting and debugging insecure information .ows. In Fifteenth International \nSymposium on Software Reliability Engineering, pages 198 209, Saint-Malo, France,November3 5,2004. [31] \nS. McCamant. Quantitative Information-Flow Tracking for Real Systems. PhD thesis,MITDepartment ofElectricalEngineering \nand ComputerScience, Cambridge,MA,June2008. [32] S. McCamant and M. D. Ernst. Quantitative information-.ow \ntracking forC and related languages. Technical Report MIT-CSAIL\u00adTR-2006-076, MIT Computer Science and \nArti.cial Intelligence Laboratory,Cambridge,MA,November17,2006. [33] S. McCamant and M. D. Ernst. Quantitative \ninformation .ow as network .ow capacity. Technical Report MIT-CSAIL-TR-2007\u00ad057, MIT Computer Science \nand Arti.cial Intelligence Laboratory, Cambridge,MA,December 10,2007. [34] S.McCamant andM.D.Ernst. Asimulation-basedproof \ntechnique for dynamic information .ow. In PLAS 2007: ACM SIGPLAN Workshop on Programming Languages and \nAnalysis for Security, pages41 46,SanDiego,California, USA,June14,2007. [35] A. C. Myers. JFlow: Practical \nmostly-static information .ow control. In Proceedings of the 26th Annual ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, pages 228 241,SanAntonio,TX,January20 22,1999. [36] A. C. Myers \nand B. Liskov. A decentralized model for information .ow control. In Proceedings of the 16th ACM Symposium \non Operating Systems Principles, pages 129 142, St. Malo, France, October5 8,1997. [37] S. K. Nair, P. \nN. Simpson, B. Crispo, and A. S. Tannenbaum. A virtual machine based information .ow control system for \npolicy enforcement. In The First International Workshop on Run Time Enforcement for Mobile and Distributed \nSystems, pages 3 16, Dresden,Germany,September 27,2007. [38] G. C. Necula, S. McPeak, S. Rahul, and W. \nWeimer. CIL: Intermediatelanguage and toolsforanalysis and transformation ofC programs. InCompiler Construction: \n11th International Conference, CC2002,pages213 228,Grenoble, France,April8 12,2002. [39] N.Nethercote \nandA.Mycroft.Redux:Adynamicdata.owtracer.In Proceedings of theThirdWorkshop onRuntimeVeri.cation, Boulder, \nCO,USA,July13,2003. [40] N.NethercoteandJ.Seward.Valgrind:Aframeworkforheavyweight dynamic binary insrumentation. \nIn Proceedings of the ACM SIGPLAN 2007 Conference on Programming Language Design and Implementation, \npages 89 100, San Diego, CA, USA, June 11 13, 2007. [41] J. Newsome and D. Song. Dynamic taint analysis: \nAutomatic detection, analysis, and signature generation of exploit attacks on commodity software. In \nAnnual Symposium on Network and Distributed System Security, San Diego, CA, USA, February 3 4, 2005. \n[42] J.NewsomeandD.Song.In.uence:Aquantitative approachfordata integrity. Technical Report CMU-CyLab-08-005, \nCarnegie Mellon University CyLab,February2008. [43] A.Nguyen-Tuong,S.Guarnieri,D.Greene,J.Shirley,andD.Evans. \nAutomatically hardening web applications using precise tainting. In 20th IFIP International Information \nSecurity Conference, pages 295 307,Chiba,Japan,May30 June1,2005. [44] J. Qian, B. Xu, and H. Min. Interstatement \nmust aliases for data dependence analysis ofheaplocations. In ACM SIGPLAN/SIGSOFT Workshop on Program \nAnalysis for Software Tools and Engineering (PASTE2007),pages17 23,SanDiego,CA,USA,June13 14,2007. [45] \nF. Qin, C. Wang, Z. Li, H.-S. Kim, Y. Zhou, and Y. Wu. LIFT: A low-overhead practical information .ow \ntracking system for detecting general security attacks. In Proceedings of the 39th Annual IEEE/ACM International \nSymposium on Microarchitecture, pages 135 148,Orlando,FL,USA,December9 13,2006. [46] A. S.alcianu and \nM. C. Rinard. Purity and side-effect analysis for Java programs. In VMCAI 05, Sixth International Conference \non Veri.cation,ModelChecking andAbstractInterpretation,pages199 215,Paris,France,January17 19,2005. [47] \nJ. Seward and N. Nethercote. Using Valgrind to detect unde.ned value errors with bit-precision. In Proceedings \nof the 2005 USENIX Annual Technical Conference, pages 17 30, Anaheim, CA, USA, April10 15,2005. [48] \nV. Simonet. Flow Caml in a nutshell. In First Applied Semantics II (APPSEM-II)Workshop,pages152 165, \nNottingham, UK,May26 28,2003. [49] S.Smith andM.Thober.Refactoringprogramstosecureinformation .ows. In \nPLAS 2006: ACM SIGPLAN Workshop on Programming Languages and Analysis for Security, pages 75 84, Ottawa, \nCanada, June10,2006. [50] G. E. Suh, J. W. Lee, D. Zhang, and S. Devadas. Secure program execution via \ndynamic information .ow tracking. In Proceedings of the 11th International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 85 96, Boston,Massachusetts, USA,October7 \n13,2004. [51] G. T. Sullivan, D. L. Bruening, I. Baron, T. Garnett, and S. Ama\u00adrasinghe. Dynamic native \noptimization of interpreters. In ACM SIGPLAN 2003 Workshop on Interpreters, Virtual Machines and Emulators,pages50 \n57,SanDiego,California, USA,June12,2003. [52] N. Vachharajani, M. J. Bridges, J. Chang, R. Rangan, G. \nOttoni, J. A. Blome, G. A. Reis, M. Vachharajani, and D. I. August. RIFLE:An architectural frameworkfor \nuser-centricinformation-.ow security. In Proceedings of the37thAnnualIEEE/ACMInternational Symposium \non Microarchitecture, pages 243 254, Portland, OR, USA,December 4 8,2004. [53] D. Volpano, G. Smith, \nand C. Irvine. A sound type system for secure .ow analysis. Journal of Computer Security, 4(3):167 187, \nDecember 1996. [54] L. Wall and R. L. Schwartz. Programming Perl. O Reilly &#38; Associates, 1991. [55] \nD.P.Wiggins. Security Extension Speci.cation. XConsortium,Inc., November1996. [56] W.Xu,S.Bhatkar,andR.Sekar.Taint-enhancedpolicy \nenforcement: A practical approach to defeat a wide range of attacks. In 15th USENIX Security Symposium, \npages 121 136, Vancouver, BC, Canada, August2 4,2006. [57] A. R. Yumerfendi, B. Mickle, and L. P. Cox. \nTightLip: Keeping applications from spilling the beans. In 4th USENIX Symposium on Networked Systems \nDesign and Implementation, pages 159 172, Cambridge, MA,USA,April11 13,2007. [58] N.Zeldovich,S.Boyd-Wickizer,E.Kohler,andD.Mazi`eres.Making \ninformation .ow explicit in HiStar. In USENIX 7th Symposium on OS Design and Implementation, pages 263 \n278, Seattle, WA,USA, November6 8,2006.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>We present a new technique for determining how much information about a program's secret inputs is revealed by its public outputs. In contrast to previous techniques based on reachability from secret inputs (tainting), it achieves a more precise quantitative result by computing a maximum flow of information between the inputs and outputs. The technique uses static control-flow regions to soundly account for implicit flows via branches and pointer operations, but operates dynamically by observing one or more program executions and giving numeric flow bounds specific to them (e.g., \"17 bits\"). The maximum flow in a network also gives a minimum cut (a set of edges that separate the secret input from the output), which can be used to efficiently check that the same policy is satisfied on future executions. We performed case studies on 5 real C, C++, and Objective C programs, 3 of which had more than 250K lines of code. The tool checked multiple security policies, including one that was violated by a previously unknown bug.</p>", "authors": [{"name": "Stephen McCamant", "author_profile_id": "81100413108", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P1022783", "email_address": "", "orcid_id": ""}, {"name": "Michael D. Ernst", "author_profile_id": "81100204056", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P1022784", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375606", "year": "2008", "article_id": "1375606", "conference": "PLDI", "title": "Quantitative information flow as network flow capacity", "url": "http://dl.acm.org/citation.cfm?id=1375606"}