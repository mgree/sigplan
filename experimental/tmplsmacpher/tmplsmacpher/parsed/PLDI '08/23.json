{"article_publication_date": "06-07-2008", "fulltext": "\n ExplainingFailuresofProgram Analyses * Daniel von Dincklage Department of Computer Science University \nof Colorado daniel.vondincklage@colorado.edu Abstract With programs getting larger and often more complex \nwith each new release, programmers need all the helpthey can get in under\u00adstandingand transforming programs.Fortunately, \nmoderndevelop\u00adment environments, such as Eclipse, incorporate tools for under\u00adstanding, navigating, and \ntransforming programs. These tools typ\u00adically use program analyses to extract relevant properties of \npro\u00adgrams. These tools are often invaluable to developers; for example, manyprogrammers use refactoring \ntools regularly. However, poor results by the underlying analyses can compromise a tool s useful\u00adness.Forexample,abug \n.nding tool may produce too manyfalse positives if the underlying analysis is overly conservative, and \nthus overwhelm the user with too manypossible errors in the program. In such cases it wouldbe invaluable \nfor the tool to explain to the user why it believes that eachbugexists. Armed with this knowl\u00adedge, the \nuser can decide whichbugs areworth pursing and which arefalse positives. The contributions of this paper \nare as follows: (i)We describe requirements on the structure of an analysis so that we can produce reasons \nwhen the analysisfails; the userof the analysis determines whether or not an analysis s results constitute \nfailure. We also describe a simple language that enforces these requirements; (ii) We describe how to \nproduce necessary and suf.cient reasons for analysis failure; (iii) We evaluate our system with respect \nto a number of analyses and programs and .nd that most reasons are small (and thus usable) and that our \nsystem is fast enough for interactive use. Categories and Subject Descriptors D.3.4[Programming Lan\u00adguages]: \nProcessors; D.3.2[Programming Languages]: Language Classi.cations General Terms Measurement, Reliability, \nDocumentation 1. Introduction Tools based on program analyses are useful for understanding and transforming \nprograms.Inthebestcase,thesetoolsproduceexactly * This work is supported by NSF grant ST-CRTS 0540997. \nAnyopinions, .ndingsand conclusionsor recommendationsexpressedin this material are the authors and do \nnot necessarily re.ect those of the sponsors. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 08, June 7 13, 2008,Tucson, Arizona, USA. Copyright c &#38;#169; \n2008ACM 978-1-59593-860-2/08/06... $5.00. Amer Diwan Department of Computer Science University of Colorado \n diwan@cs.colorado.edu the results that the programmer needs; e.g., the tool identi.es the few lines \nof code that contain the source of a bug. In the worst case, these tools are ineffective; e.g., the tool \ntells the programmer that thebug couldbe anywherein the program. If program analyses produce undesirable \nresults, the tool that depends on them may become ineffective. Prior work suggests two possible strategies \nfor this. First, we cangiveup.Typically,compil\u00aders adopt this strategy: if they are unable to prove, \nusing program analyses, that an optimization applies to a particular program lo\u00adcation, theygive up on \nthe optimization for that location. Second, we can obtain guidance from the programmer; interactiveoptimiza\u00adtion \nsystems [6, 1] take this approach. The .rst strategy is accept\u00adable especially for compiler optimizations \n(e.g., it would be cum\u00adbersome for a compiler to ask a user about hundreds of possible aliases). If the \npotential payoffis large enough, the second strategy is worthwhile; e.g., guidance from the user may \neliminate a major bottleneck in the program. This paper focuses on the second strat\u00adegy. In order to \nobtain useful guidance from the programmer, the program analysis should be able to produce a reason every \ntime it produces an undesirable result. The user of the analysis (e.g., in\u00adteractive optimization system) \nprovides a predicate that determines whether or not a given analysis result is desirable.1 This reason \nshould satisfy two requirements: 1. The reason should be necessary: the reason should only de\u00adscribe \nissues that contribute to undesirable results. 2. The reasons should be suf.cient: the analysis should \ncease to produce undesirable results once we have addressed all the issues identi.ed by the reason. \n While some prior interactive optimization systems produce rea\u00adsons for theirfailure, their reasons typically \ndo not satisfy our re\u00adquirements. Speci.cally, ParaScope [1] tells the user which de\u00adpendences prevented \nit from parallelizing a loop. SUIF Explorer [6] goes further by also providing the programmer with program \nslices [13] of the array references involved in the dependences. While knowing those dependencies that \ninhibit the parallelization is useful to the user, the user does not know why the system be\u00adlieves that \nthere is a dependence; thus dependencies alone are not suf.cient reasons. Since slices may contain many \nstatements that are irrelevant to the dependencies that inhibit parallelization, SUIF Explorer s slices \nare not necessary reasons. This paper describes andevaluatesa novel approach for computing reasons that \nare nec\u00adessary and suf.cient. The contributions of this paper are as follows: We describe the requirements \nan analysis must satisfy to enable us to produce reasons when the analysis produces undesirable 1In this \npaper we will use the phrase analysisfailed synonymously with analysis produced undesirable results . \nresults. We describe an analysis language that ensures that all analyses written in it satisfy these \nrequirements.We show how to express data-.ow analyses using our language and thus demonstrate that our \nrequirements do not prevent us from at least expressing all data-.ow analyses. We give two semantics \nfor the analysis language. The value semantics de.nes how to evaluate analyses written in the lan\u00adguage. \nThe reason semantics de.nes how to compute reasons when the analyses produce undesirable results.  We \nshow that our approach produces necessary and suf.cient reasons.  We evaluate our prototype and demonstrate \nthat it ef.ciently computes reasons for undesirable results. Moreover,we demon\u00adstrate that for the analyses \nthat we tried, over 98% of all unde\u00adsirable results have small reasons (3 or fewer terms). In other words, \nwe do not overwhelm the programmer with complex reasons for eachfailure.  This paper is structured as \nfollows. Section 3 describes what we meanby reasons. Section4 describes our requirements fora lan\u00adguage \nfor writing analyses and also describes a language that satis\u00ad.es our requirements. Sections5and6describe \nhow we compute reasons for boolean and non-boolean analyses respectively. Sec\u00adtion7 discusses theoretical \nproperties of our approach. Section8 presents issues when using our system. Section9 presents anex\u00adampleof \nusing our system. Section10experimentallyevaluates our system. Section 11 reviews related work and Section \n12 concludes. 2. High-Level Example Before we introduce the details of our approach, we illustrate the \napproach using an example.2 In our example, a user asks an inter\u00adactive optimizer to apply dead-store \nelimination to the store a = x.f in the code fragment below: a = x.f; ... q = a; The optimizer uses the \nfollowing analysis to determine whether it can apply the optimization to the store: deadStore(store) \n:= hasNoUse(store) or forall use in usesOf (store): isDead(use) If this analysis returns true for a given \nstore, the optimizer can safely eliminatethe store. If it returns false the dead-store elim\u00adination is \nunsafe. Unfortunately, this analysis evaluates to false for our example; thus, the optimizer uses our \napproach to produce a reasonforthisfailurewhichit communicatestothe user. To compute a reason, our approach \nrecords which properties contribute to the failure of the analysis. Each such property con\u00adtributes to \nthe overall reason for whythe dead-store elimination is inapplicable. Our analysis for dead-store elimination \nuses three properties: hasNoUse, usesOf, and isDead. The invocation of hasNoUse(a = x.f) evaluates tofalse \nand thus contributes to the analysisfailure: Ifithad evaluatedto true,the analysiswouldhave succeeded.We \ntherefore record hasAUse(a = x.f) as a failure reason. The in\u00advocation of isDead(q = a) fails since the \nassignment is not dead. We record not-isDead(q=a) asfailure reason. Since the analysis combines these \ninvocations with an or , we construct the follow\u00ading reason: OrReason(not-hasNoUse(a = x.f), not-isDead(q \n= a)) 2For ease of exposition, this section uses simpli.ed notation for reasons and analyses. This reason \nexplains to the user exactly why her requested opti\u00admization is inapplicable. The user can either address \nthe reason (e.g., by asserting that g(a) is dead) to enable the optimization or decide that the optimization \nis indeed inapplicable. 3. Whatisa Reason Before we can describe how we compute reasons for analysis \nfailures,weneedto precisely describewhatwe meanbyreasons.A reason is a value of type R. R containstwo \nkindsof reasons: root reasons and constructed reasons. A root reason says that the analysisfailed becausea \nproperty computedexternallyto our systemisfalse.3We canaddnew root reasons to our system by providing \nit with new properties and the means to compute the properties. Herearesomeroot reasonsweuseintheexamplesinthispaper \n(we introduce additional root reasons as convenient): 1. AnalysisFailed(CannotModify, s, v). The property \nModi.es with arguments s and v isfalse (i.e., statement s does not mod\u00adify variable v). 2. AnalysisFailed(canThrow, \ns). The statement s may throw an exception. 3. NoReason. No reason needed since the analysis produced \ndesir\u00ad  able results. Constructed reasons combine reasons to indicate that the analysis failedduetoacombinationof \nreasons.We combine reasonsusing conjunction and disjunction, speci.cally, we use AndReason(a,b)/ OrReason(a,b), \na, b .R, to combine reasons a and b. Ifthe programmerwantsafailing analysisto succeed,she must resolve \nthe reasons for the analysis sfailure.A user can resolvea reason by: (i) for an AndReason, resolving \nboth sub-reasons; (ii) for an OrReason, resolving either sub-reason; (iii) for a NoReason, doing nothing; \nand (iv) for a AnalysisFailed(not-property,\u00b7\u00b7\u00b7 ) reason, taking appropriate steps to make property(\u00b7\u00b7\u00b7 \n)hold. As stated earlier, we require reasons to be necessary and suf.\u00adcient. By necessary, we mean that \nresolving the reason guarantees that the analysis will be successful. By suf.cient, we mean that if the \nreasonisnot resolvedfully,the analysiswill continuetofail. 4. Languagefor expressing analyses An analysis \nresultis undesirableifitis not what its clientwants. Forexample,iftheclientisthe constant propagation \noptimizationit willwantthe underlying analysisto concludethatavariable svalue isaconstant. Therefore, \nnon-constantvalues are undesirable for the constant propagation optimization. Undesirable results typically \noriginate from one or more of the following sources: Modeling of properties of the analyzed program.For \nexample, an analysis may assume that a call may modify any variable.  Data and control .ow merges.For \nexample, a constant prop\u00adagation may merge x=7 and x=5 to conclude that x is not a constant.  Underlying \nanalyses.Forexample,aconstant propagation anal\u00adysis may use the results of an imprecise call graph analysis. \n If we wish to determine why an analysis produces undesirable re\u00adsults, we need to identify the abovesources \nin the analysis code; we call these sources failure points since theymay cause an analysis tofail.Oncewehave \nidenti.edthefailurepointsinan analysis,we can trackexactly whichfailure pointsdegrade each analysis result. \n3For ease of explanation, we only consider boolean analyses for the time being with true being desirable \nand false being undesirable; later we show how we also handle non-boolean analyses. Identifying failure \npoints in analysis code is dif.cult (and in general impossible) if the analysis writer does not somehow \nmark these points in the code. Thus, for our work we write our analyses inalanguage thatexplicitly identi.esfailure \npoints. Note that there are manyalternative approaches for identifyingfailure points (e.g., adding annotations \nto the Java source code of the analysis or using naming conventions); our ideas on identifying reasons \nfor analysis failure are applicable to all of these alternatives. We now describe our language for writing \nanalyses (Sec\u00adtion 4.1), illustrate the language with an example (Section 4.2), and demonstrate that \nour language can express anydata-.ow anal\u00adysis (Section 4.3). 4.1 Syntax and value semantics of the condition \nlanguage For ease of exposition, this section focuses only on analyses that produce booleanvalues.We \nconsider true valuestobe desirable and false valuestobe undesirable. Section6 describeshowwe handle analyses \nwith arbitrary lattices. Figure1gives the syntaxof our language. Most constructsin our language (except \nfor the property or simple variable reference) evaluate to a pair of a boolean (the value of the construct) \nand a reason of type R. The value and the reason within the pair are related: If the value is true, the \nreason is NoReason . If the value is false, the reason explains why the value is false. A variable reference \ns type usually depends on the type of some property (see below). Figure 2 describes how we compute the \nboolean values; Section5showshow we compute the reasons. Our semantics (both for values and for reasons) \nuse a pair of environments, (E, F ), where environment E binds variables and F contains results of prior \nanalyses. Both environments have two parts, oneto storevaluesandthe otherto storethe reasonsfor those \nvalues.We subscript the environments with B (boolean values) or R (reasons) to indicate which part of \nthe environment we mean. We now describe the constructs of our language. The query construct queries \nanalyses results from user-de.ned analyses that have already completed or are in progress. Since two \nanalyses may depend on each other, the computation of query may be cyclic. We use ideas from work on \nsolving boolean equation systems[3,7]tocomputethe.xedpointofthesecycles. Intuitively, our solution takes \nthe dependence graph between multiple analyses and injects true orfalse (whichever is the neutral case) \nfor queries involved in a cyclic dependency. The property construct queries the results of analyses that \nare implemented externally to our system. The set of properties is not .xed; a user of our system can \n(and will) add new properties eas\u00adily. Moreover, different properties may return entities of different \ntypes.Forexample an allPaths property returns a set of paths, an allPredecessors property returnsasetof \nstatements,anda Modi.es property returns a boolean. The .rst argument to both query and property identi.es \nthe query or property to look up. The remaining arguments are parame\u00adterstothe queryor property.Forexample, \nquery(isLive, s, x)looks up whether or not x is live at statement s. The arguments to query and property \nmay themselvesbe termsin our language;inthis case we use our language s value semantics to evaluate the \nparameters. forall iterates over the set of elements y (e.g., paths or state\u00adments) and evaluates z for \neach path or statement after binding x to the current element in the iteration. The forall evaluates \nto true only if z evaluates to true for all elements of the set. Thus, forall models a control merge. \nThe . and . model data merges. The unless and whenever model decision points in the analysis.4 The unless \nevaluates to true if the condition, x, evaluates to true; 4Note that we use these constructs instead \nof the more usual if;Section5 explains the reason for this choice. (Term) ::= (Variable) | query((Literal), \n\u00b7\u00b7\u00b7 ) | property((Literal), \u00b7\u00b7\u00b7 ) | forall (Variable) :(Term) in (Term) | unless((Term))then(Term) | \nwhenever((Term))ensure (Term) |(Term).(Term) |(Term).(Term) Figure 1: The syntax of the language for \nwriting analyses F(IsNotAReachingDef, d, u, x) ::= forall p: property(AllAcyclicPaths, d, u) in property(Modifies, \nproperty(ExcludeEndPoints, p), x) Figure 3: De.nition of IsNotAReachingDef otherwise it evaluates to \nwhatever y evaluates to. The whenever evaluates to true if both its condition, x, and its body, y,evaluate \nto true. Otherwiseitevaluatestofalse. 4.2 Example The following example shows how to write an IsConstant0, \nwhich determines whether or not variable x is a constant at statement s. Section6describesthe machineryweneedtowritea \nmoregeneral IsConstant. F(IsConstant0, s, x) ::= forall d: property(DefiningStatements, x) in unless \nquery(IsNotAReachingDef,d, s,x) then property(RHSIs0, d) IsConstant0 uses the following properties: De.ningStatements \n(returnsthesetofall potential de.nitionsofavariable)and RHSIs0 (whether or not the right-hand side of \nan assignment is 0). Intu\u00aditively, IsConstant0 iterates over all assignments to x and if that assignment \nreaches s then IsConstant0 checks that the right-hand side of the assignment is 0. IsConstant0 also queries \nanother anal\u00adysis, IsNotAReachingDef, which returns true if a de.nition, d, of variable x does not reach \nuse u. We show the de.nition of IsNotAReachingDef in Figure 3. IsNotAReachingDef uses three properties: \nAllAcyclicPaths (returns the set of acyclic paths between twopoints), Modi.es (determines if apath modi.esavariable), \nandExcludeEndPoints (which strips the end points out of a path). Intuitively, IsNotAReachingDef checks, \nfor each acyclic path between d and u, whether or not the path modi.es x. Onevaluating the aboveanalyses \non appropriate arguments, our system populates the environments, FB and FR,with the outcomes and reasons \nfor the analyses. Note that it is straightforward to mix parts written in other languages with parts \nwritten in our language by modeling them as properties.  4.3 Expressiveness We now show how to express \nboolean data-.ow analyses in our language.Tosimplify our argument, we present our argument using a forward \ndata .ow analysis. The argument for backward analyses is analogous.We assume that the data-.ow analysisisin \nthe form: Ain(s)= n{Aout(s ')|s ' . s} Aout(s)= f(Ain(s),s) Dependingon whetherthe analysisisa meetorjoin \nanalysis, n VW V is either or .For now we assume that n is ;later we discuss W howwe handle . s' . s \nmeans that s isa direct successorof s' in [query(id, \u00b7\u00b7\u00b7 B(E, F ) ::= FB[(id, \u00b7\u00b7\u00b7)] B(E, F ) ::= EB[x] \n)h[xh[property(id, \u00b7\u00b7\u00b7 )hB(E, F ) := whatever the property computes [forallx : y in zhB(E, F ) ::= . \na . [yhB(E, F ).[zhB(EB[x\\a],F ) [x . yh[x . yh B(E, F ) ::= [xhB(E, F ) B(E, F ) B(E, F ) . [yhB(E, \nF ) ::= [xhB(E, F ) . [yh[unless x then yhB(E, F ) ::= [x . yhB(E, F ) [whenever x ensure yhB(E, F ) \n::= [x . yhB(E, F ) Figure 2: The semantics []B the CFG of the program.We assume that Ain(entry) is initialized \naccording to the analysis. To emulate this analysis in our language, we need to emulate Ain and Aout.To \nemulate Ain we use the property Predecessors which returns the immediate predecessors of its argument \nstate\u00adment. Ain(s)= forall x : property(Predecessors, s)in query(Aout,x) To emulate Aout we need to emulate \nf. To express f in our language, we rewrite f(x, s) to be (x .\u00acf1(s)) . f2(s) (any boolean function can \nbe expressed in this form). Many data .ow formulations (e.g., using Kill and Gen sets) already express \nf in this form.We can use properties to implement \u00acf1 and f2. W To emulate (i.e., join) analyses in our \nsystem, the most ob\u00advious approach is to frame join analyses as meet analyses (we use duals of the properties \nto do this) and then use the above approach. In summary, we can straightforwardly express boolean data\u00ad.ow \nanalyses in our language. 5. Computingreasonsfor boolean analyses There are two main insights that enable \nus to derive reasons for analysisfailure. First, as already discussed in Section 4, we explicitly mark \nall failure points in analyses; this enables us to track exactly the points that contribute to analysisfailure. \nSecond, we express analyses using positive logic only; the only language constructthatcanproducefalseonitsownisthe \nproperty construct. This enables us to easily identify those invocations of property that contribute \nto undesirable results.Forexample, com\u00adparethe de.nitionof IsNotAReachingDeffromFigure3, writtenin positivelogic, \nto the following analysis, written in full logic (which uses the exists operator which our language does \nnot include): F(IsAReachingDef,d, u,x) ::= exists p: property(AllAcyclicPaths, d, u) in property(DoesNotModify, \nproperty(ExcludeEndPoints, p),x) Ifthe .rst analysisevaluatestofalse,it mustbethe casethat one or more \nof the property(Modi.es, p, x) evaluated tofalse. The properties that evaluated to false are exactly \nthe properties that contribute to the reason for the undesirable result of the whole analysis. The user \nmust address all of the false properties before the analysis can succeed. If the second analysis evaluates \nto false, then it must be the case that there is no path for which property(DoesNotModify, p, x) is true. \nThe user can address any one of the failing prop\u00aderty(DoesNotModify, p,x) before the analysis succeeds. \nInourexperienceusingour system,wehavefoundthe.rst case to be more manageable than the second. The reason \nfor this is that the .rst case enumerates exactly the reasons that the user has to address in order to \nmake the analysis succeed (and no more or no less); in the second case the user can address one of many \nreasons but the user does not have any further guidance on which one to actually try to address. Thus \nwe do not allow not or exists in our language.We could modify our system to compute reasons even without \nthese restrictions or use properties to get around these Listing 1. Example x =1; if (\u00b7\u00b7\u00b7 ) x =2; else \na =0; a =7; a =x; restrictions; it is just that we have found that our reasons are more useful to the \nuser with these restrictions. 5.1 Semanticsfor computingreasons The reason semantics (Figure 4) for our \nlanguage specify how to compute reasons for undesirable analysis results.To compute the reasons,the reason \nsemanticsmakeuseofthevalue semantics(Fig\u00adure 2). Speci.cally, the reason semantics produce a reason only \nfor constructs thatevaluatetofalse(and thus contributeto undesir\u00adable analysis results). Note how disallowing \nnot and exists in our language also simpli.es these semantics; without these restric\u00adtions we also have \nto compute reasons for constructs that evaluate to true in case the true construct is surrounded by a \nnot . To obtain the reason for a query or a variable reference, we simply look up the reason in the appropriate \nenvironment. Note that we may have to evaluate the arguments using the value semantics before we can \nperform the lookup. If a boolean property evaluates to true, then we do not need a reason (recall that \nwe track reasons only for undesirable results). If a boolean property evaluates tofalse, we label the \nreason as not of the property. This is the base case for constructing a reason. If . evaluates to true, \nwe do not need a reason. If only one of x or y isfalse,the reasonisthe reasonforthefalse one.Ifboth x \nand y arefalse,thenthe reasonisthe AndReasonofthetwo reasons.If . evaluatestotrue,wedonotneeda reason.Ifitevaluatestofalse, \nthe reason is the OrReason of the reasons for x and y. For unless and whenever we use the rules for . \nand . to generate reasons. If a forall evaluates to true, then we do not need a reason. If itevaluatestofalse,thenall \ninstancesof z thatevaluated tofalse must be addressed before the forall succeeds. Thus, we construct \nan AndReason with the reasonsof all thefalse instancesof z.  5.2 Example Let s suppose we wish to apply \nthe IsNotAReachingDef analysis fromFigure3tothecodeinListing1.Ifwe instantiatethe analysis as IsNotAReachingDef(x \n=1,a = x, x), we .nd that the analy\u00adsisfails. From the forall rule in Figure4 we compute the reason AnalysisFailed(not-Modi.es, \na=0 . a=7, x); in other words, Is-NotAReachingDef failed because along one path (passing through a =0 \nand a =7)the de.nition actually reachesa = x 6. Computingreasonsfor non-boolean analyses Many program \nanalyses compute non-boolean values.Forexam\u00adple, a constant propagation analysis determines whether or \nnot a variable reference is a constant as well as the value of the constant. [query(id, \u00b7\u00b7\u00b7 )hR(E, F \n) ::= FR[(id, \u00b7\u00b7\u00b7)] NoReason | property(id, x, y)hB(E, F )= true Only for boolean properties AnalysisFailed(not-id, \nx, y) | [ property(id, x, y)hB(E, F )= false [xhR(E, F ) ::= ER[x] [property(id, x, y)hR(E, F ) := [unless \nx then yhR(E, F ) ::= [x . yhR(E, F ) [whenever x ensure yhR(E, F ) ::= [x . yhR(E, F ) [forall x : \ny in zhR(E, F ) ::= j8< 8 >< NoReason | [forall x : y in zhB(E, F )= true AndReason( [forall x : y in \nzhB(E, F ) = true : {[zhR(E[x\\a],F ) |a . [yhB(E, F ) . [zhB(E[x\\a],F )= false}) NoReason | xhR(E, \nF ) | | xhB(E, F )= true . [yhB(E, F )= true xhB(E, F )= false . [yhB(E, F )= true[ yhR(E, F ) | xhB(E, \nF )= true . [yhB(E, F )= false AndReason([xhR(E, F ), [yhR(E, F )) | [ xhB(E, F )= false . [yhB(E, F \n)= false NoReason | xhB (E, F )= true . [yhB(E, F )= true OrReason([xhR(E, F ), [yhR(E, F )) | [ xhB \n(E, F )= false . [yhB(E, F )= false [x . yhR(E, F ) ::= [x . yhR(E, F ) ::= >: j Figure 4: The semantics \n[]R To support these analyses, we generalize our language constructs analysis since before they can use \na value, they need to con.rm to compute a value from an analysis s lattice, L.We .rst describe how we \ncompute reasons for non-boolean analyses. Then we give an example that demonstrates our approach. 6.1 \nThe L and RL semantics Figure 5 de.nes how we produce the result from L for a meet analysis (the rules \nfor a join analysis are similar and and thus omitted from paper). Note that the E and the F environments \nnow have a L part instead of a B part. The semantics in Figure5use UL and nL which the analysis writer \nprovides to our system (i.e., they are not written in our language); theyimplement join and meet on the \nanalysis s lattice. For boolean analyses we had de.ned true as a desirable value and false as undesirable.For \nnon-boolean analyses, the user of our system provides a function desirableL which takes values of the \nanalysis s lattice L and returns trueorfalse;in essence,the user provides a way for our system to determine \nwhether or not a given analysis result is desirable. The user provides one implementation of desirableL \nfor each analysis s lattice. When we write just de\u00adsirable (i.e., omit the subscript L)we intend it as \nan overloaded operator; our system picks the appropriate implementation of de\u00adsirable based on the domain \nof the argument.We assume that top of L (TL) is desirable, and .L is undesirable. For each use of a property,desirableL \neffectively distinguishes between situations that allowthe transformation (desirable) and sit\u00aduations \nthat disallow the transformations (undesirable).Aproperty (or query) may produce undesirable results \nin two ways: (i) the property or query is actually provably undesirable; or (ii) the prop\u00aderty or query \nuses an imprecise analysis and that causes it to pro\u00adduce undesirable results. In our system, desirable \nvalues are always maximally precise: As every desirable value is provably desirable, no amount of added \nprecisions in our analyses will ever make a desirable result unde\u00adsirable. In contrast, undesirable values \nare always maximally im-precise:Abetter analysis could easily make an undesirable value desirable. To \nmake our system run fast without sacri.cing precision, it transparently swaps analyses with more precise \nones during the evaluation of a client analysis, if the client analysis produces too imprecise results.To \nallow sucha transparent swap, all clientsof the analysis must when given the choice use the more precise \nvaluesofthenewanalysis insteadofthe imprecisevaluesoftheold analysis. If implemented navely, this approach \ncomplicates client that the value is not superseded by a more precise value. We sidestep this complication \nby using a single set of opera\u00adtors for comparing both precision and desirability of values: Since added \nprecision may only make a value become desirable, and not undesirable, every comparison of desirability \nwill always examine precision, too.This eliminatestheburdenofhavingtodoa second comparison, and allows \nour system to replace an analysis with a more precise analysis without any additional effort on part \nof the client analysis. The semanticsin Figure5are straightforward:theyusethe user\u00adprovided meet or join \noperator instead of using boolean . and . when combining values. The x for a whenever or unless must \nevaluate to a boolean; it is up to the user ensure this (e.g., by using the same criteria as desirable). \nWhen the x of an unless evaluates to true, the whole unless succeedsand producesthetopvalueofthe lattice;the \nreasonforthisisthatwecanmeetanyvaluewiththetop without degrading analysis results. When the x value ofawhenever \nevaluates tofalse, the whole whenever fails; thus it produces the bottomvalueofthe latticeto indicate \nanalysisfailure. Figure6gives the semantics for computing reasons when using the L lattice and assumingameet \nanalysis. The semantics forajoin analysis are analogous and so we omit them. Foraquery oravariable reference,wesimplylookupthe \nreason in the appropriate environment. For aproperty we evaluate the property using the value seman\u00adtics \nand check if the value is desirable; if it is desirable we do not need a reason. If the value is not \ndesirable, we produce a reason that is the not- of the property. For an unless we do not need a reason \nif the unless produces a desirable result. If it produces an undesirable result then we produce an OrReason \nsince addressing either x or y will cause the unless to produce a desirable result. For awhenever we \ndo not need a reason if the whenever pro\u00adduces a desirable result. If either x isfalse or y is undesirable, \nwe return the reason of the undesirable one. If both x and y are unde\u00adsirable, we return the AndReason \nof both reasons. For a forall we do not need a reason if the forall produces a desirable result. If it \nproduces an undesirable result, it could be due to one of two reasons: (i) z produced an undesirable \nresult for one or more of the bindings of y;(ii)z produced a desirable resultforall bindingsbutthe meetofthe \nresultswas undesirable. In the .rst case, we produce an AndReason of all the instances of z that produced \nundesirable results; in the second case we produce a MeetFailed Reason. [query(id, \u00b7\u00b7\u00b7 )hL(E, F ) ::= \nFL[(id, \u00b7\u00b7\u00b7)][xhL(E, F ) ::= EL[x] [property(id, \u00b7\u00b7\u00b7 )hL(E, F ) := whatever the property computes [forall \nx : y in zhL(E, F ) ::= n a . [yhL(E, F ).[zhL(EL[x\\a],F ) [x . yhL(E, F ) ::= [xhL(E, F ) UL [yhL(E, \nF ) [x . yhL(E, F ) ::= [xhL(E, F ) nL [yhL(E, F ) j j TL | xhL(E, F )= true | [ xhL(E, F )= false \nFigure 5: The semantics []L (assuming an all-paths analysis) | xhL(E, F )= true [yhL .L | [ xhL(E, F \n)= false [unless x then yhL(E, F ) ::=[whenever x ensure yhL(E, F ) ::= [yhL [query(id, \u00b7\u00b7\u00b7 (E, F ) \n::= FR[(id, \u00b7\u00b7\u00b7)] )hRL (E, F ) ::= ER[x] [xhRL j NoReason | desirable( property(id, x, y)hL(E, F )) \n= true AnalysisFailed(not-id, x, y) | [property(id, x, y)hRL (E, F ) := desirable([ property(id, x, \ny)hL(E, F )) = false NoReason | xhL(E, F )= true NoReason | xhL(E, F )= false . desirable( yhL(E, F )) \n= true | 8< : [unless x then yhRL (E, F ) ::= OrReason([xhRL [whenever x ensure yhRL (E, F ) ::= 8 >>< \n[ xhL(E, F )= false . desirable([ yhL(E, F )) = false NoReason | xhL(E, F )= true . desirable( yhL(E, \nF )) = true (E, F ) | [ xhL(E, F )= true . desirable([ yhL(E, F )) = false | xhL(E, F )= false . desirable([yhL(E, \nF )) = true (E, F ), [yhRL (E, F )) [yhRL xhRL (E, F ) >>: AndReason([xhR(E, F ), [ yhRL [ xhL(E, F )) \n= false . desirable([yhL(E, F )) = false AndReason({[zhRL (E[x\\a],F )|a . [yhL(E, F ).|.a . [yhL(E, F \n).\u00acdesirable([zhL(E[x\\a],F )) \u00acdesirable([zhL(E[x\\a],F )))}) MeetFailed Reason({a|a . [yhL(E, F )}) |.a \n. [yhL(E, F ).desirable([zhL(E[x\\a],F )). \u00acdesirable(na.[yTL(E,F )[zhL(E[x\\a],F )) NoReason otherwise \n(E, F ) | 8 >>>< >>>: [forall x : y in zhRL (E, F ) ::= | 8><>: 8<: NoReason | desirable([x nL yhL(E, \nF )) xhRL |\u00acdesirable([xhL(E, F )) . desirable( yhL(E, F ))[ yhRL | desirable([xhL(E, F )) .\u00acdesirable([ \nyhL(E, F )) (E, F ) ::= [x . yhRL (E, F ) ::= [x . yhRL MeetFailed Reason(x, y) otherwise (i.e.\u00acdesirable([xhL(E, \nF ) n [yhL(E, F ))) NoReason | desirable([x UL yhL(E, F )) OrReason([xhRL , [yhRL ) |\u00acdesirable([xhL(E, \nF )) and \u00acdesirable([yhL(E, F )) JoinFailed Reason(x, y) | otherwise (i.e.\u00acdesirable([xhL(E, F ) U [yhL(E, \nF ))) | Figure 6: The semantics []RL For an . (.), if both (one) of the arguments are undesirable, we \nuse the reason from the undesirable arguments. If both the argumentsare desirablebutthe outcomeofthe \n./. is undesirable, then it produces a JoinFailed Reason/MeetFailed Reason. Our reason semantics and \ndiscussion of the forall, ., and . are simpli.edforthe purposeof presentation.Toseethis,let s suppose \nwe are writingaconstant propagation analysisandwedo 7.. (this may arise becauseavariable hasvalue7on \none path andavalue . (i.e., not a constant) on another path). Our reason semantics will use the reason \nfor the . as the reason for the undesirable outcome of the ..Whilethisispartofthe reason,itisnotasuf.cient \nreason: thevariablemustnotonlyhaveanon-. value on both pathsbut the two values must be the same. Our \nsystem actually handles this by producing a hierarchical reason: the top-level reason is MeetFailed Reason(7, \n.);below it there is a reason for the.. 6.2 Example Given a reaching de.nitions analysis, writing a simple \nconstant propagation is easy. The following produces the constant value of x at statement s if x is a \nconstant, and . otherwise. The analysis writer provides a meet operator, which returns its argument if \nboth arguments are the same, the non-T argument if one of the argu\u00adments is T, and . otherwise (i.e., \na standard constant propagation lattice). The analysis writer also implements desirable, which re\u00adturnsfalseif \nits argumentis . and true otherwise. (assuming an all-paths analysis) F(ConstantPropagation, s, x) ::= \nforall d: query(ReachingDefinitions, s, x) in property(getRhs, s) If the different reaching de.nitions \nreturn different constant values (say7and5),oursystem producesaMeetFailed Reason(7,5) which tells the \nuser that constant propagation produced an undesirable result because7and5are different. The user can \nthenexplorewhy the analysis considered the 7 value; the reason computation for ReachingDe.nitions produces \nthis reason. 7. Properties of Reasons Recall from Section 3 that we want reasons to be necessary and \nsuf.cient. These properties are desirable since theyguarantee that a user of our approach will (i) not \nwaste time addressing issues that do not need to be addressed (necessary); and (ii) not .nd that an analysis \nproduces undesirable results even after the user has addressed all the reasons (suf.cient). The full \nproofs that our reasons are necessary and suf.cient uses structural inductionoverthe language presentedin \nFigures1. Since our proofs are straightforward, we only sketch them here and only for the boolean analyses. \nThe proof for the non-boolean analyses, over the hierarchical reasons of our full system, is similar. \nTo see whyour reasons are necessary, we .rst consider condi\u00adtions that do not use . or unless. From Figure4we \nsee that only propertiesthatevaluatetofalseendupaspartofthe reason (either directly or embedded inside \nof AndReason). As, by construction, these properties must be satis.ed in order for the analysis to eval\u00aduate \nto true, our system produces necessary reasons. For analyses that use . we construct an OrReason;we trivially \nsee that sat\u00adisfying the OrReason will satisfy the . condition and thus our reasons are necessary in \nthe presence of . .For conditions that use unless we generate an OrReason when both the condition and \nbody of the unless evaluatetofalse.Inthis case,wecansatisfythe unless by either making its condition \nor its body evaluate to true; this is exactly what the OrReason for this case of the unless cap\u00adtures. \nThus, our reasons are necessary for our full language. Thekeyinsight behind our proof that our reasons \nare suf.cient is that our analyses only use positive logic and thus the analysis fails only as a result \nof one or more terms that evaluate to false. From the semanticsin Figure4we see that our reasons incorporate \nall terms that cause an analysis to fail. Speci.cally, our reasons omit all true terms and those false \nterms whose falsehood does not propagateuptothe analysis (e.g.,false termsthat arepartof a disjunct with \nthe other term in the disjunct evaluating to true). Thus, our reasons are suf.cient. This property is \nmuch harder to prove if our analysis can use negative logic. In summary,we haveshown that our approach \nproduces reasons that are necessary and suf.cient for an analysis sfailure. 8. Using our system We envision \ntwo kinds of uses of our approach: in program trans\u00adformation tools and in program understanding tools. \nIn program transformation tools, each transformation has an ap\u00adplicability condition that describes whether \nor not a given applica\u00adtion of the transformation is legal (i.e., preserves semantics). If a user requests \nan illegal transformation, the user may wish to know why the transformation is illegal. Our system naturally \nproduces reason for this illegality for boolean and non-boolean analyses. In a program understanding \ntool, our system may not immedi\u00adately produce the reason that the user needs. In particular, for lat\u00adtices \nwhose elements are sets, our system computes a single reason instead of one reason for each element within \nthe set As an example of this, suppose a programmer requests a pro\u00adgram slice and the slice is much larger \nthan what the programmer expected. How should the user de.ne desirable? If desirable re\u00adturns a true \nif a slice is small (perhaps using some arbitrary thresh\u00adold) and otherwise returnsfalse, our system \nproducesa reason for large slices. This reason describes whythe slice is large. This may not be what \nthe user wants. Often, the user will want to know why a particular statement is or is not in the slice. \nWe can get such reasons by de.ningdesirable so that it checks just for the statement of interest. While \nthis approach works, it is cumbersome: we would effectively have to rerun the reason computation once \nfor each such query by the user; speci.cally, once the user knows why s1 is in the slice, she may wish \nto know why s2 is in the slice, and so on. In other words, for analyses whose lattice elements are sets, \nwe may wish to associate a reason with each element in the set rather than a single reason for the entire \nset. In terms of our slicing example, rather than having a single reason for why the slice is large or \nwhy a particular statement is in the slice, we can have a reason pre-computed for each statement that \nbelongs to the slice. Our system automatically does this when the user-provided meet and join operators \nare set union and intersection (or vice versa). Toseehowthisworks,let s supposethatthe reaching de.nition \nneeds to meet de.nitions reaching along two paths; along one path the de.nitions are d1 and d2 and along \nthe other path the de.nitions are d2 and d3. Furthermore, assume we have reasons ra and rb for d1 and \nd2 along the .rst path and reasons rc and rd for de.nitions Listing 2. Analysis for LICM 1 F(LICM, toRemove) \n::= 2 property(cannotModify, toRemove, property(RHS, toRemove)) 3 . 4 forall path: property(ExcludeEndPoints, \n 5 property(AllAcyclicPaths, toRemove, toRemove)) in 6 forall stmt: property(allStatementsInPath, path) \nin 7 (property(CannotModify, stmt, property(LHS, toRemove)) 8 . 9 property(CannotModify, stmt, property(RHS, \ntoRemove)) 10 . 11 unless property(CannotThrow, toRemove) then 12 (property(cannotModifyAnyState, stmt) \n13 . 14 property(CannotThrow, stmt))) Listing 3. Program with an opportunity for LICM boolean isEqual(Iterator<Integer> \niter) {while (b) {int v =x.f; int v2 = iter .next(); if(v != v2) { return false; }} return true; } d2, \nand d3 on the second path. Since d2 arrives from both paths, the reason for it after the meet is AndReason(rb, \nrc). As d1 and d3 occur only on one path, their reasons remain ra and rd. 9. An example In order to demonstrate \nhow one goes about using our system, we nowpresentafullexample which implements the analysis for loop\u00adinvariant \ncode motion (LICM). The analysis (Listing 2) evaluates to true if toRemove can be hoisted outside its \nenclosing loop (for simplicity our condition assumes that there is exactly one loop that surrounds toRemove). \nLine 2 ensures that executing toRemove does not change its own subsequent behavior. The two forall iterate \nover all paths through the loop and all statementsin those paths. Lines7 and9 make sure that no statement \nmodi.es the left or right-hand side of toRemove. The unless in line11 ensuresthat either toRemove does \nnot throw an exception or if it does, then other statements in the loopdonotthrowexceptionsormodifyanyvariables.This \nensures that LICM respects Java s exception semantics. An implementation of LICM that handles all cases \n(such as jumps out of the loop) is 139 lines in our condition language. The analysis in Listing 2 ignores \nmany corner cases for ease of exposition. If we evaluate our LICM condition on the code in Listing 3 \nwith v=x.f as toRemove we get the reason in Figure 7. The top\u00adlevel AndReason in Line2says that the user \nmust address both of its children (Lines3and6)to enable LICM. The sub-reasoninLine3comes directlyfromLines7 \n9inList\u00ading 2. Speci.cally,the reason says that the call to next may modify either the left or the right-hand \nside of toRemove. The sub-reason in Line6 comes directly from Lines 11 14 in Listing 2. Speci.cally, \nit says that moving toRemove out of the loop may violate Java s exception semantics either by reordering \nexceptions (in case x.f or the call to next throws an exception) or by reordering excep\u00adtions with other \nside effects (since the call to next may modify any state). 1 Optimization \"LICM\" for statement \"int \nv = x.f\" 2 AndReason: 3 AndReason: 4 AnalysisFailed(CanModify, \"v2 = iter.next()\", \"x.f\") 5 AnalysisFailed(CanModify, \n\"v2 = iter.next()\", \"v\") 6 OrReason: 7 AnalysisFailed(CanThrowException, \"int v = x.f\") 8 AndReason: \n9 AnalysisFailed(CanModifyAnyState, \"v2 = iter.next()\") 10 AnalysisFailed(CanThrowException, \"int v2 \n= iter.next()\") Figure7:Areason for thefailureof LICM s condition 10. Evaluation We have already proved \nthat our system produces reasons that are necessary and suf.cient and that our analysis language can \nexpress arbitrary data .ow analyses. This section experimentally evaluates our approach. 10.1 Methodology \nTo evaluate our system for computing reasons, we embedded our system in our interactive optimizer. Our \ninteractive optimizer im\u00adplements interprocedural optimizations for the Java programming language using \nthe approach outlined in this paper. As a result, our system alertsa user wheneveran optimizationhasonlyafewfail\u00adure \nreasons (i.e., almost applies). Our system then interacts with the user by providing a reason for the \ninapplicability of the optimiza\u00adtion. The user responds by possiblyadding assumptions about the program \nwhich enable the optimizer to apply manyof the almost applicable optimizations. We now discuss the implementation \nof a few traditional com\u00adpiler optimizations (speci.cally constant propagation, copypropa\u00adgation, dead \nassignment elimination, and loopinvariant code mo\u00adtion)inoursystem.Welimitourselvestosimple, well-knownopti\u00admizations \nto make the discussion more accessible to the reader. Our optimizer breaks down the task of determining \nthe appli\u00adcability of an optimization into two parts. First, the optimizer uses simple syntactic pattern \nmatching to identify possible optimization opportunities.Forexample,asimple pattern may indicate thatause \nof a variable may be replaced by a constant if there is a preceding assignment of a constant value to \nthe variable. Second, the opti\u00admizer uses program analyses (implemented in the language in Sec\u00adtion4)tocheck \nwhetherthe optimizationis actuallyapplicable.For example, our syntactic pattern matching cannot account \nfor aliases; the second stage incorporates aliasing information to check the le\u00adgalityofthe constant \npropagation.Ifthe secondstage .nds thatthe optimization is not really applicable, it produces reasons \nwhich the user can addressby providing assumptions. We used this system to optimize all applications \ntaken from SPECjvm98 [12] benchmark suite that are available as Java source code and SPECjbb [11] benchmark.Weconducted \nourexperiments on a 3.0GHz CoreDuo workstation with 8GB of memory. Table1 summarizes our results. The \nSize(LOC) row gives the size of the benchmarks. These sizes include the size of anylibraries that are \nshipped with the benchmarks. Since our optimizations are interprocedural, they also analyze these libraries \nalong with the application code. The remainder of the table has one section for each optimiza\u00adtion. In \naddition, it has a section for Reaching Def : for this anal\u00adysis, our system computes the reaching de.nitions \nof all variables usedinall statementsand producesareasonforeach statementthat ends up in a reaching de.nitions \nset. The Applicable rows for the optimizations give the number of times the optimization was applicable \nwithout needing any input from the user. The Inapplicable rows give the number of times the optimization \nwas inapplicable and thus we had to produce reasons Jess SpecJBB DB compress raytrace Size (LOC) 22215 \n30777 11601 11370 14183 Constant Propagation (85 lines) Applicable Inapplicable Time (sec) 14662 28975 \n7.81 19071 38204 11.34 8899 13766 3.64 9070 13949 3.82 10648 17912 4.21 CopyPropagation (98 lines) Applicable \nInapplicable Time (sec) 37339 7690 94.47 49676 10585 51.08 20213 3329 17.49 20482 3222 15.79 25119 4166 \n19.52 Dead Assignment Elimination (11 lines) Applicable Inapplicable Time (sec) 1409 13113 3.88 650 19589 \n4.15 13 7872 2.06 5 8026 1.08 7 9285 1.42 LICM (139 lines) Applicable Inapplicable Time 1140 169 1.01 \n1374 149 1.31 512 96 0.51 491 86 0.41 591 91 0.48 Reaching de.nitions (93 lines) Evaluations Time (sec) \n30409 21.75 38376 23.23 15114 6.17 15353 22.19 19760 22.54 Table 1. Summary statistics for thefailureof \nthe optimization. The Timerows give the time (in seconds) to compute the reasons for all the inapplicable \ncases. The Evaluations row (for reaching de.nitions only) gives the number of different reaching de.nitions \nsets we computed (thereis one for each use of a variable). From these results we see that the time to \ncompute reasons depends on the number of reasons we compute (which depends on the number of times an \noptimization is inapplicable). If we compute only a few reasons (e.g., 169 for LICM on Jess), our system \ntakes only about a second; if we compute many reasons (e.g., 37339 for copypropagation on Jess)our system \ntakes longer (94.47 seconds). Our systemtakesonaverage0.1 secondsto produceasingle reason and thus our \nsystemisfast enough for interactive use. 10.2 Dif.culty of writing analyses If our approach to makes \nanalyses too hard to write, it will not be practical.Toexplore this issue, we wrotea numberof analysesin \nour language (Section 10.1).Table1gives the numberof linesof code we had to write for each analysis in \nour language. We see the analyses are quite small (ranging from 11 lines to 139 lines) especially considering \nthat these analyses are interprocedural and handle the full Java language. The analyses were straightforward \nto write.Infact,we foundthe reasonsextremely helpfulfordebug\u00adging the analyses since theytold us exactly \nwhy our analyses were producing poor results; we used these reasons to selectively tune our analyses \nso that the analyses enabled the most optimization op\u00adportunities. 10.3 Size of reasons The reasons computed \nby PT are small: over 96% of reasons for our compiler optimizations consist of less than 3 root reasons. \nComputing these reasons is fast: Over 95% of the reasons PT computes are computed within 0.1 seconds \nor less. Figure 8 gives an example of a small reason for Jess along with the relevant code fragment from \nJess. Note that we translate our reasons into prose for the user s convenience. This is a rea\u00adson of \nsize1which says that constant propagationfailed because initialStatus could have one of two values. Figure9givesanexampleofalarge \nreasonfor Jess(this time we do not give the relevant code for Jessbecause it is too large to String initialStatus \n= \"Ready\"; if (!testMode) { [...] initialStatus = \"Error\"; } [...] showStatus(initialStatus); ================================================================ \nOptimization \"ConstantProp\" in Line 779: MeetFailed, Two values of \"initialStatus\" were inequal: -> initialStatus \n= \"Ready\", line 747 -> initialStatus = \"Error\", line 760 Figure8:Atypically small reason for constant \npropagationin Jess Optimization \"ConstantProp\" in Line 585: And: Inequal: -> tok = null, line 453 -> \nnon-constant value due to propagation of undesirable. PropagationOfUndesirable: The result of nextToken() \nmay not be constant, line 455 PropagationOfUndesirable: The result of nextToken() may not be constant, \nline 487 PropagationOfUndesirable: The result of nextToken() may not be constant, line 500 PropagationOfUndesirable: \nThe result of nextToken() may not be constant, line 515 PropagationOfUndesirable: The result of nextToken() \nmay not be constant, line 533 PropagationOfUndesirable: The result of nextToken() may not be constant, \nline 540 PropagationOfUndesirable: The result of nextToken() may not be constant, line 560 Figure 9: \nAn atypically large reason for constant propagation in Jess reproduce here). This reason says that constant \npropagationfailed because one of the reaching de.nitions for tok was null and the others were not constant \n(i.e., .). Our system tags reasons with PropagationOfUndesirable in its handling of the forall construct; \nspeci.cally, it marks reasons as PropagationOfUndesirable when thevalues being merged along the different \npaths are already unde\u00adsirable to start with. In summary, we see that most of the reasons that our system \ncomputes are small and thus will not overwhelm users. 10.4 Usefulnessofreasons We have shown that our \nsystem produces reasons that are usually small and it produces them quickly; however, are the reasons \nac\u00adtually useful? To determine this, we examined many of the rea\u00adsons producedforthe optimizationsinTable1and \ntriedto address the reasons.We addressed reasonsbyproviding assumptionsto our system. After providing \nthe assumptions, we ran the programs and compared their output to the unoptimized program; in this way \nwe con.rmed,experimentally,that our optimizations were correct (and thus our reasons were correct). We \nnow report our experience in providing assumptions for the copypropagation optimization. On examining \nthe reasons for all three benchmarks, we found that four reasons recurred in all benchmarks; we describe \nthem here.Table2shows the numberof copypropagation opportunities that our assumptions enabled. We now \ndescribe the assumptions and the reasons that led to them. Assumption 1 tells our optimizer to disregard \ndynamic class loading. Assumption2tells our optimizer to disregard the possibil\u00adity of NullPointerException. \nOur reasons indicated that these twofactors frequently inhibit copypropagation. Since we knewthat these \nbenchmarks do not rely on dynamic class loading or on catch\u00ading NullPointerExceptions we added the two \nassumptions. Assumption3tells our optimizerto assumethatlibrary methods cannot modify instancevariablesofthe \napplication sclasses.Many Assumption Jess SpecJBB DB compress raytrace #1 and #2 #3 #4 25 546 1033 29 \n505 1188 17 377 639 57 368 20 119 394 11 Table 2. Addressing reasons for copypropagation failure reasons \nreferredtothis possibility;weknewthattobefalse and thus added the assumption. Assumption4tells our optimizerto \nassumethatlibrary methods will not call the application s methods. Onceagain,manyfailure reasons referred \nto this possibility. Note that these assumptions are macro assumptions: i.e., rather than saying that \nmethodfin library cannot modify instance variable i in class C , we simply said no method in library \ncan modify an instance variable in an application class . Our system uses reason abstraction: it combines \nsimilar concrete reasons to more abstract reasons and thus enables the user to address many reasons with \na single assumption. In summary, we found our reasons to be useful: we were able to address many reasonsby \naddinga handfulof assumptions. Adding these assumptions enabled many optimization opportunities and thus \nthe reasons were useful. 11. RelatedWork We are not aware of any prior work on a general approach for \nproducing reasons for analysisfailure. Interactive transformations systems, however,do use ad-hoc approaches \nfor producing reasons. The SUIF Explorer [6] and theParaScope programming envi\u00adronment [1] are both interactive \nsystems designed to help program\u00admers to parallelize their programs. Both SUIF Explorer andParaS\u00adcope \nenable programmers to see the dependencies that inhibit the parallelization of a loop. While these dependencies \ncan be helpful, theyjusttellthe userthata dependencyexistsbutnot why it exists. The SUIF Explorer goes \na step further by incorporating program slicing: the SUIF Explorer enables a user to look at the program \nslice of the references involved in dependencies. In a way, these program slices serve as reasons for \ndependencies: they suggest what code may be responsible for the dependences that inhibit par\u00adallelization. \nWhile slices are invaluable they can often be large; if a slice is large, the user may have no idea for \nwhere to look within the slice in order to resolve the dependencies. The SUIF Explorer can bene.t from \nour approach: our approach would, for example, enable the user to ask why a particular statement is in \nthe slice. We can obtain some reasons for analysisfailureby implement\u00ading our analyses in a declarative \nlanguage, such as Prolog or Dat\u00adalog. Prolog producesa trace that captureswhysomethingfailed. However, \nthese systemswould produce one reason forfailure and not the full reason; i.e., the reasons are not suf.cient. \nIn addition, manydeclarative languages have dif.culties with cyclic dependen\u00adcies between rules. Theyalso \ndo not restrict their language to posi\u00adtive logic: This prevents them from computing useful reasons. \nTheFALCON system [2] system enables programmers to man\u00adually select a region of code to which certain \noptimizations should be applied.FALCONthen attemptstoapplythe optimizations.Un\u00adlikeour system,FALCONdoesnotprovideanyreasonsiftheopti\u00admizations \nare inapplicable. Refactoring tools, such as Eclipse [10], allow programmers to transform their programs \nby picking refactorings from a menu. These tools implement reasons in an ad-hoc way: each refactoring \ndoes its own checking for legality and reports problems to the user as it encounters them. Since the \nreason checking is ad-hoc, there is no reason to believe that the reasons are actually good enough. For \nexample, most Eclipse refactorings are optimistic; i.e., their associated analyses do not fully check \nthat a refactoring is legal. Thus, even if a user addresses a reason, there is no guarantee that the \nenabled refactoring will actually be correct. The structure of our analysis language is related to the \nway pro\u00adgramanalysesareexpressedwiththehelpofmodel checking.Stef\u00adfen and Schmidt [9] show that program \nanalyses can be performed using model checking. Their work expresses optimizations using the modal mu-calculus, \nresulting in formulas that are similar to the analysesin our approach. Our approach also performsa kind \nof model checking: The reasons denote a generalization of those models that cause the optimization to \nnot apply. Therefore, Steffen and Schmidt sworkfaces someofthe same challengesas ours.For example, Schmidt \nnotes [8] that manyanalyses are used as their du\u00adals, citing theexampleoflivevariable analysis.We also \noften .nd it convenient to rephrase meet analyses as join analyses in order to get effective reasons. \nAs in our system, the Cobalt system [5] also requires users to express their analyses in a special purpose \nlanguage; Cobalt s goal is to prove the correctness of optimizations. There are some signi.cant similarities \nin how one expresses the safety condition of optimizationsin Cobalt andin our system.Forexample users \nexpress safety conditions in Cobalt as predicates that must hold in the code region between enabling \nstatements and the statement affected by the optimization; our analyses are expressed similarly except \nthat in our system the forall is explicit while in Cobalt it is implicit. Unlike our system, Cobalt does \nnot require analyses to be expressed in positive logic; this restriction enables us to compute useful \nreasons for analysisfailure. To summarize, we are the .rst to come up with a general ap\u00adproach for computing \nreasons. While some prior systems attempt to produce reasons,theyusean ad-hoc approach.Asaconsequence \ntheir reasonsmaynotbe necessaryorsuf.cient.Webelievethatour approach can be bene.cial to interactive \ntransformation systems. 12. Conclusion As programs and programming languages become increasingly complex, \nwe will increasingly .nd that many desirable transfor\u00admations are inapplicable and many analyses produce \nundesirable results. For example, the presence of dynamic class loading and re.ection in Java degrades \nthe results of both analyses and trans\u00adformations [4]. Thus, we anticipate that many analysis and trans\u00adformation \ntools will need input from the user.To obtain this input, the analysis and transformation tools must \nbe able to produce rea\u00adsonsthatguidethe useringivingthe neededinput.Inthispaperwe described and evaluated \nan approach for producing such reasons. To produce such reasons, we have designed a language that captures \nparts of program analyses that degrade results; the anal\u00adysis writer can write the remaining parts of \nthe analysis using any method.Wegivetwo semanticsforthe language;thevalue seman\u00adtics evaluates analyses \nin the language and the reason semantics produces reasons when analyses produce undesirable results. \nWe evaluate our approach by implementing a number of anal\u00adyses in our language and running them on a \nnumber of standard Javabenchmarks.Weshowthat our approach produces reasons that are typically small (and \nthus probably useful to the user) and it takes,onaverage, 0.01 secondsto producea reason(and thusfast \nenough for interactive use).We also demonstrate theoretical prop\u00aderties about our approach; speci.cally, \nwe show that our reasons for an analysis are necessary and suf.cient for ensuring the success of the \nanalysis and that our analysis language can express arbitrary data-.ow analyses. 13. Acknowledgements \nWe would like to thank Christoph Reichenbach, Mike Hind, Clay\u00adton Lewis, DarkoStefanovic and Mark Marron \nfor their manyhelp\u00adful suggestions. References [1] Keith D. Cooper, Mary W. Hall, Robert T. Hood, Ken \nKennedy, KathrynS. McKinley, JohnM. Mellor-Crummey, LindaTorczon,and ScottK.Warren. TheParaScope parallel \nprogramming environment. Proceedings of the IEEE, 81(2):244 263, 1993. [2] Luiz DeRose,Kyle Gallivan, \nEfstratios Gallopoulos, BretA. Marsolf, andDavidA.Padua.FALCON:AMATLAB interactive restructuring compiler. \nIn Languages and CompilersforParallel Computing,pages 269 288, 1995. [3] Jan Friso Groote and MisaKein\u00a8anen. \nSolving disjunctive/conjunctive boolean equation systems with alternating .xed points, 2004. [4] Martin \nHirzel, Daniel von Dincklage, Amer Diwan, and Michael Hind.Fast online pointer analysis. ACMTrans.Program.Lang. \nSyst., 29(2):11, 2007. [5] Sorin Lerner,Todd Millstein, and Craig Chambers. Automatically proving the \ncorrectness of compiler optimizations, 2003. [6] Shih-Wei Liao, Amer Diwan, Robert P. Bosch Jr., Anwar \nM. Ghuloum, and Monica S. Lam. SUIF explorer: An interactive and interprocedural parallelizer. In Principles \nPractice ofParallel Programming, pages 37 48, 1999. [7] Angelika Mader. Veri.cation of Modal Properties \nUsing In.nite Boolean Equation Systems. PhD thesis. [8] David A. Schmidt. Data .ow analysis is model \nchecking of abstract interpretations. In POPL 98, pages 38 48, NewYork, NY, USA, 1998.ACM Press. [9] \nDavid A. Schmidt and Bernhard Steffen. Program analysis as model checking of abstract interpretations. \nIn SAS, pages 351 380, 1998. [10] Sherry Shavor, Jim D Anjou, Scott Fairbrother, Dan Kehn, John Kellerman, \nandPat McCarthy.TheJavaDevelopers Guideto Eclipse. Addison-Wesley, May 2003. [11] Standard performance \nevaluation corporation. SPECjbb2000 (java business benchmark). http://www.spec.org/jbb2000. [12] Standard \nperformance evaluation corporation. SPECjvm98 bench\u00admarks. http://www.spec.org/jvm98. [13] M.Weiser. \nProgram slicing. In Proceedings of ICSE, pages 439 449. IEEE Computer Society Press, 1981.   \n\t\t\t", "proc_id": "1375581", "abstract": "<p>With programs getting larger and often more complex with each new release, programmers need all the help they can get in understanding and transforming programs. Fortunately, modern development environments, such as Eclipse, incorporate tools for understanding, navigating, and transforming programs. These tools typically use program analyses to extract relevant properties of programs.</p> <p>These tools are often invaluable to developers; for example, many programmers use refactoring tools regularly. However, poor results by the underlying analyses can compromise a tool's usefulness. For example, a bug finding tool may produce too many false positives if the underlying analysis is overly conservative, and thus overwhelm the user with too many possible errors in the program. In such cases it would be invaluable for the tool to explain to the user <i>why</i> it believes that each bug exists. Armed with this knowledge, the user can decide which bugs are worth pursing and which are false positives.</p> <p>The contributions of this paper are as follows: (i) We describe requirements on the structure of an analysis so that we can produce reasons when the analysis fails; the user of the analysis determines whether or not an analysis's results constitute failure. We also describe a simple language that enforces these requirements; (ii) We describe how to produce necessary and sufficient reasons for analysis failure; (iii) We evaluate our system with respect to a number of analyses and programs and find that most reasons are small (and thus usable) and that our system is fast enough for interactive use.</p>", "authors": [{"name": "Daniel von Dincklage", "author_profile_id": "81100459374", "affiliation": "University of Colorado at Boulder, Boulder, CO, USA", "person_id": "P1022796", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado at Boulder, Boulder, CO, USA", "person_id": "P1022797", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375614", "year": "2008", "article_id": "1375614", "conference": "PLDI", "title": "Explaining failures of program analyses", "url": "http://dl.acm.org/citation.cfm?id=1375614"}