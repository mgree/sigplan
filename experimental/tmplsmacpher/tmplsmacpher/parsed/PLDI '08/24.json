{"article_publication_date": "06-07-2008", "fulltext": "\n Sound, Complete and Scalable Path-Sensitive Analysis * Isil Dillig Thomas Dillig Alex Aiken Computer \nScience Department Stanford University {isil, tdillig, aiken}@cs.stanford.edu Abstract We present a \nnew, precise technique for fully path-and context\u00adsensitive program analysis. Our technique exploits \ntwo observa\u00adtions: First, using quanti.ed, recursive formulas, path-and context\u00adsensitive conditions \nfor many program properties can be expressed exactly. To compute a closed form solution to such recursive \ncon\u00adstraints, we differentiate between observable and unobservable variables, the latter of which are \nexistentially quanti.ed in our ap\u00adproach. Using the insight that unobservable variables can be elimi\u00adnated \noutside a certain scope, our technique computes satis.ability\u00adand validity-preserving closed-form solutions \nto the original recur\u00adsive constraints. We prove the solution is as precise as the original system for \nanswering may and must queries as well as being small in practice, allowing our technique to scale to \nthe entire Linux kernel, a program with over 6 million lines of code. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation General Terms Languages, Reliability, Veri.cation, \nExperimen\u00adtation Keywords Static analysis, path-and context-sensitive analysis, strongest necessary/weakest \nsuf.cient conditions 1. Introduction Path-sensitivity is an important element of many program analy\u00adsis \napplications, but existing approaches exhibit one or both of two dif.culties. First, so far as we know, \nthere are no prior scalable techniques that are also sound and complete for a language with re\u00adcursion. \nSecond, even in implementations of incomplete methods, interprocedural path-sensitive conditions can \nbecome unwieldy and expensive to compute. Existing approaches deal with these prob\u00adlems by some combination \nof heuristics, accepting limited scala\u00ad * This work was supported by grants from DARPA, NSF (CCF-0430378, \nCNS-050955, CNS-0716695, and SA4899-10808PG-1), and equipment grants from Dell and Intel. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 \n13, 2008, Tucson, Arizona, USA. Copyright c .2008 ACM 978-1-59593-860-2/08/06. . . $5.00. bility, and \npossible non-termination of the analysis (see Section 2 for discussion of related work). In this paper, \nwe give a new approach that addresses both is\u00adsues. Our method is sound and complete; for the class of \nprogram properties we address, we can decide all path-and context-sensitive queries. We also demonstrate \nthe performance and scalability of our approach with experiments on the entire Linux kernel, a program \nwith over 6MLOC. Two ideas underpin our technique: We can write constraints capturing the exact path-and \ncontext\u00adsensitive condition under which a program property holds. Un\u00adfortunately, we do not know how \nto solve these constraints.  However, to answer may (might a property hold?) or must (must a property \nhold?) queries, it is equivalent to decide the question for a particular necessary or suf.cient condition \nextracted from the exact constraints. While non-trivial, the constraints for these necessary/suf.cient \nconditions can be solved, and furthermore, the necessary/suf.cient conditions are typically much smaller \nthan the original constraints, improving scalability. Since these necessary and suf.cient conditions \nare satis.ability and validity preserving respectively, they involve no loss of precision for answering \nmay and must queries about program properties.  In practice, program analysis systems ask may or must \nqueries about program properties. Our approach effectively takes advan\u00adtage of which kind of query is \nto be asked (may or must) to special\u00adize a general representation of the exact path-and context-sensitive \ncondition to a form where the query can be decided. The complete\u00adness of our approach is only guaranteed \nif the base domain of ab\u00adstract values is .nite; for in.nite domains our method is still sound, but not \nnecessarily complete. Thus, for example, our approach can be used for sound and complete context-and \npath-sensitive type state, type quali.er, or data.ow properties, but not, for example, arbitrary shape \nproperties. Our approach is best illustrated using an example. Consider thefollowing function: bool queryUser(bool \nfeatureEnabled) { if(!featureEnabled) return false; char userInput = getUserInput(); if(userInput == \ny ) return true; if(userInput== n ) return false; printf(\"Input must be y or n! Please try again\"); \nreturn queryUser(featureEnabled); } Under what condition does queryUser return true? Informally, the \nargument featureEnabled must be true, and the user input on some recursive call must be ' y ' . Our system \nformalizes this in\u00adtuition by computing a constraint .a,true characterizing the condi\u00adtion under which \nqueryUser, given argument a, returns true: '' ' .a,true = .\u00df.(a = true).(\u00df = y .(\u00ac(\u00df = n ' )..a,true \n[true/a])) (*) This constraint is recursive, which is not surprising given that queryUser is a recursive \nfunction. The formula under the exis\u00adtential quanti.er encodes the conditions under which the function \nbody evaluates to true: 1. featureEnabled must be true (clause: a = true), 2. the user input is ' y \n' (clause: \u00df = ' y '),  '' '' 3. or the user input is not n (clause: \u00ac(\u00df = n )) and the recursive call \nreturns true (clause: .a,true [true/a]). Because the function argument must be true if the recursive \ncall is reached, the argument to the recursive call is also true, which is expressed by the substitution \n[true/a]. The equation above illustrates the main features of our constraint language. Primitive constraints \ninclude comparing variables repre\u00adsenting program values with constants (a = true), and compound constraints \ncan be built with the usual boolean connectives ., ., and \u00ac. The translation between actual and formal \narguments in a function call is represented, as usual, by a substitution ([true/a]), but note the substitution \nis part of the constraint. The most unusual feature is existential quanti.cation. The quanti.ed variable \n\u00df intu\u00aditively captures the unknown user input: we do not know statically the value of \u00df, just that it \nhas some value i.e., it exists. The quan\u00adti.er also captures the scope of the user input, namely that \neach input is used for one recursive call. We refer to the existentially bound variables as unobservable \nvariables. The values of unob\u00adservable variables cannot be expressed in terms of the inputs to a function; \nhence unobservable variables are not visible outside of the procedure invocation in which they are used. \nOne division between program analysis approaches is between those that are whole-program (requiring the \nentire program to per\u00adform analysis) and those that are modular (can analyze parts of a program in isolation). \nA standard approach to modular analysis is to use formulas to represent program states, with free variables \nin the formulas capturing the unknown state of the partial program s environment. Our motivation for \ndistinguishing unobservable vari\u00adables is that that they commonly arise in modular program analysis systems. \nFor example, the results of unknown functions (functions unavailable for analysis) are unobservable; \nsome system state may be hidden (e.g., the garbage collector s free list, operating system s process \nqueue, etc.) that can be modeled by unobservable variables, or a static analysis may itself introduce \nunobservables to represent imprecision in the analysis (e.g., selecting an unknown element of an array). \nUnobservable variables are useful within their natural scope, for example, tests on unobservables can \npossibly be proven mutually exclusive (e.g., testing whether the result of a malloc call is null, and \nthen testing whether it is non-null). A key observation is that outside of that scope unobservables provide \nno additional information, at least for answering may or must queries, and can be eliminated. By distinguishing \nunobservable values, we help sepa\u00adrate what is essential to path-sensitive analysis from inherent, but \northogonal, sources of imprecision for any analysis. Thus, the dis\u00adtinction between observables and unobservables \nis what enables us to prove both soundness and completeness for our constraint reso\u00adlution algorithm. \nThe combination of existential quanti.cation and recursion is more expressive than it may .rst appear. \nIn particular, in the pre\u00advious example, it captures that the user input may be different on different \nrecursive calls. To see this, consider the constraint written with the recursion fully (in.nitely) unfolded: \n'' '' .a,true = .\u00df.(a = true).(\u00df = y .\u00ac(\u00df = n ). '' '' .\u00df ' .(true = true).(\u00df ' = y .\u00ac(\u00df ' = n ). .\u00df \n'' '' \u00ac(\u00df '' '' .(true = true).(\u00df '' = y .= n ). ... For clarity, we have performed the substitution \n[true/a]on the un\u00adfolded constraints. More interestingly, the quanti.ed variables are renamed to emphasize \neach recursive call has an independent user input. Thus, the original recursive constraint captures the \nfull in\u00adterprocedural, path-sensitive condition under which the result of a call to getUserInput is true. \nThe .rst part of our work, which in\u00adcludes the predicate language, inference algorithm for constraints, \nand the class of program properties for which we can compute sound and complete path-and context-sensitive \nconditions, is dis\u00adcussed in Section 3. In a standard constraint-based program analysis algorithm, hav\u00ading \nde.ned the constraints of interest, the next step would be to give an algorithm for solving them. However, \nwe know of no algorithm for solving equations such as (*). The dif.culty is illustrated by the unfolding \nof (*)given above; because the recursive constraint introduces the equivalent of an unbounded number \nof quanti.ed variables, it is not obvious how to come up with an equivalent but .nite and non-recursive \nrepresentation that makes explicit all pos\u00adsible solutions. However, there is a way to side-step this \nproblem entirely. As mentioned above, in practice, clients of a program analysis need to answer either \na may analysis query (e.g., May queryUser return true?) or a must analysis query (e.g., Must queryUser \nreturn true?). While we do not solve the constraints in general, there is a sound and complete algorithm \nfor deciding may/must queries. Consider the may analysis query: Is it possible for queryUser to return \ntrue, in other words, is constraint (*) satis.able? Our algorithm decides this question as follows. First, \nfor a constraint C, we compute a modi.ed constraint .C. that is a necessary condition for C. By de.nition, \n.C. is a necessary condition for C if C ..C.. Thus, if .C. is not identically false, then the property \nmay hold; if .C. is false, then C is never true. Now, any choice of necessary condition is sound (if \nthe necessary condition is unsatis.able then so is the original constraint), but to guarantee completeness \n(if the necessary condition is satis.able then so is the original constraint) and termination of our \nalgorithm the necessary condition must satisfy two additional properties: For completeness, the necessary \ncondition must be the best possible. The strongest necessary condition is implied by every other necessary \ncondition.  For termination, the necessary condition should be only over the observable variables. Eliminating \nthe unobservable variables is what makes solving the constraints possible; it turns out that the strongest \nnecessary condition on observable variables is still suf.cient for completeness.  Consider constraint \n(*)again. The strongest observable neces\u00adsary condition for queryUser to return true is a = true; i.e., \nif the input to queryUser is true, then it may return true, otherwise it cannot return true. The computation \nof necessary conditions for answering may queries, and dually suf.cient conditions for answer\u00ading must \nqueries, is discussed in Sections 5 and 6. We have implemented our algorithm and experimented with several \nlarge open source C applications (see Section 8). We show that for these applications we can answer all \nmay and must queries for two signi.cant applications: computing the path-and context\u00adsensitive condition \nunder which every pointer dereference occurs, and .nding null dereference errors. For the former, perhaps \nour most interesting result is that the observable necessary and suf.\u00adcient conditions we compute do \nnot grow with program size, show\u00ading that our approach should scale to even larger programs. For the \nlatter, we show that the interprocedurally path-sensitive analysis re\u00adduces false positives by almost \nan order of magnitude compared to the intraprocedurally path-sensitive, but interprocedurally path\u00adinsensitive \nanalysis. To summarize, this paper makes the following contributions: We distinguish observable and \nunobservable variables in path\u00adand context-sensitive conditions, and it is this distinction that ultimately \nallows us to give a sound and complete algorithm.  We show how to obtain strongest necessary and weakest \nsuf.\u00adcient observable conditions by eliminating unobservable vari\u00adables and solving the resulting recursive \nsystem of constraints over observable variables.  We give the .rst scalable, sound, and complete algorithm \nfor computing a large class of precise path-and context-sensitive program properties.   2. Related \nWork In this section we survey previous approaches to path-and context\u00adsensitive analysis. The earliest \npath-sensitive techniques were de\u00adveloped for explicit state model-checking, where essentially every \npath through the program is symbolically executed and checked for correctness one at a time. In practice, \nthis approach is used to verify relatively small .nite state systems, such as hardware protocols [9]. \nMore recent software model-checking techniques address sound and complete path-and context-sensitive \nanalysis [5, 3, 13]. Build\u00ading on techniques proposed for context-sensitivity [20, 22], Ball et al. propose \nBebop, a whole-program model checking tool for boolean programs [5, 3]. Bebop is similar to our approach \nin that it exploits the scope of local variables through implicit existen\u00adtial quanti.cation and also \ndeals with recursion through context\u00adfree reachability. However, Bebop combines these two steps, while \nour approach separates them: we .rst explicitly construct formulas with existentially quanti.ed unobservable \nvariables and then sub\u00adsequently perform a reachability analysis as a .xed point compu\u00adtation. This design \nallows us to insert a new step in between that manipulates the existentially quanti.ed formulas, in particular \nto convert them to (normally) much smaller formulas that preserve may or must queries prior to performing \nthe global reachability computation. This extra step is, we believe, the reason that we are able to scale \nour approach to programs much larger than have been previously reported for systems using model checking \nof boolean programs [5, 3, 13]. Another advantage of this approach is that we can use unobservable variables \nto model .xed, but unknown, parts of the environment (see discussion in Section 1). Our method is also \nmodular, in contrast to most software model checking systems that require the entire program. Current \nstate-of-the-art software model-checking tools are based on counter-example driven predicate abstraction \n[4, 16]. Predicate abstraction techniques iteratively re.ne an initial coarse abstraction until a property \nof interest is either veri.ed or refuted. Re.nement-based approaches may not terminate, as the sequence \nof progressively more precise abstractions is not guaranteed to converge. Our results show that for a \nlarge class of properties the exact path-and context-sensitive conditions can be computed di\u00adrectly without \nre.nement and for much larger programs (millions of lines) than the largest programs to which iterative \nre.nement approaches have been applied (about one hundred thousand lines). We believe our techniques \ncould be pro.tably incorporated into software model checking systems. An obstacle to scalability in early \npredicate abstraction tech\u00adniques was the number of irrelevant predicates along a path. Craig interpolation \n[16] allows discovery of locally useful predicates and, furthermore, these predicates only involve predicates \nin scope at a particular program point. Our approach addresses similar issues in a different way: our \ntechnique also explicitly accounts for vari\u00adable scope, and extracting necessary/suf.cient conditions \nelimi\u00adnates many predicates irrelevant to the queries we want to de\u00adcide. Unlike interpolants, our technique \ndoes not require counter\u00adexample traces, and thus does not require the additional machinery of theorem \nprovers and successive re.nement steps. Some of the most scalable techniques for path-and context\u00adsensitive \nanalysis are either unsound or incomplete. For exam\u00adple, ESP is a light-weight and scalable path-sensitive \nanalysis that tracks branch correlations using the idea that conditional tests re\u00adsulting in different \nanalysis states should be tracked separately, while branches leading to the same analysis state should \nbe merged [11]. ESP s technique is a heuristic and sometimes fails to com\u00adpute the best path-sensitive \ncondition. Another example of an in\u00adcomplete system is F-Soft [17]. F-Soft unrolls recursive functions \na .xed number of times, resulting in a loss of precision beyond some predetermined recursion depth of \nk. In contrast, our approach does not impose any limit on the recursion depth and therefore does not \nlose completeness for programs with recursion. A .nal example of an incomplete system is Saturn [1]. \nWhile Saturn analyses are generally fully path-sensitive within a single procedure, Saturn has no general \nmechanism for interprocedural path-sensitivity and pub\u00adlished Saturn analyses are either interprocedurally \npath-insensitive or use heuristics to determine which predicates are important to track across function \nboundaries [23, 12, 8, 14]. We implement the ideas proposed in this paper in Saturn. Our technique of \ncomputing necessary and suf.cient conditions is related to the familiar notion of over-and under-approximations \nused both in abstract interpretation and model checking. For example, Schmidt [21] proposes the idea \nof over and under\u00adapproximating states in abstract interpretation and presents a proof of soundness and \ncompleteness for a class of path-insensitive anal\u00adysis problems. Many model-checking approaches also \nincorporate the idea of over-and under-approximating reachable states to ob\u00adtain a more ef.cient .xed \npoint computation [6, 10]. Our contri\u00adbution is to show how to compute precise necessary and suf.cient \nconditions while combining context-sensitivity, path-sensitivity, and recursion. The idea of computing \nstrongest necessary and weakest suf.\u00adcient conditions for propositional formulae dates back to Boole \ns technique of eliminating the middle term [7]. Lin presents ef.cient algorithms for strongest necessary \nand weakest suf.cient condi\u00adtions for fragments of .rst-order logic, but does not explore com\u00adputing \nstrongest necessary and weakest suf.cient conditions for the solution of recursive constraints [18]. \nIn our system, the analysis of a function f may be different for different call-sites even within f s \nde.nition, which gives it the expressiveness of context-free reachability (in the language of data.ow \nanalysis) or polymorphic recursion (in the language of type theory). Most polymorphic recursive type \ninference systems are based on instantiation constraints [15]. Our formalization is closer to Mycroft \ns original work on polymorphic recursion, which represents instantiations directly as substitutions [19]. \n 3. Constraints We use a small functional language to present our techniques: Program P ::= F + Function \nF ::= de.ne f (x)= e Expression E ::= true |false |ci |x |f (e) |if e1 then e2 else e3 |let x = e1 in \ne2 |e1 = e2 |e1 .e2 |e1 .e2 |\u00ace Expressions are true, false, abstract values ci, function argu\u00adments \nx, functional calls, conditional expressions, let bindings and comparisons between two expressions. Boolean-valued \nexpres\u00adsions can be composed using the standard boolean connectives, ., ., and \u00ac. We model unobservable \nbehavior in the language by references to unbound variables, which are by convention taken to have a \nnon-deterministic value chosen on function invocation. Thus, any free variables occurring in a function \nbody are unob\u00adservable. All other sources of unobservable behavior discussed in Section 1 can be modeled \nusing references to unde.ned variables. For simplicity of presentation, we assume that boolean-valued \nexpressions are used only in conditionals, that equality compar\u00adisons e1 = e2 are always between expressions \nthat evaluate to ab\u00adstract values, and that functions return one of the abstract values ci. This small \nlanguage includes two essential features needed to motivate and illustrate our techniques. First, there \nis an expressive language of predicates used in conditionals, so that path-sensitivity is a non-trivial \nproblem. Second, functions can return any one of a set of values ci, but this set is .nite. Intuitively, \nthe ci s stand for possible abstract values we are interested in assigning to a program. The goal of \nour technique is to assign each function constraints of the following form: DEFINITION 1 (Constraints). \n[F Equation E ::= .i]= .\u00df1,...,\u00dfm.[FFi] Constraint F ::= (t1 = t2 )|.[Ci/a] |F1 .F2 |F1 .F2 |\u00acF Type \nt ::= a |Ci Constraints are equations between types (type variables and ab\u00adstract values), constraint \nvariables with a substitution, or boolean combinations of constraints. Constraints express the condition \nun\u00adder which a function f with input a returns a particular abstract value c; we usually index the corresponding \nconstraint variable .f,a,C for clarity, though we omit the function name if it is clear from context. \nSo, for example, if there are two abstract values c1 and c2, the equation [.f,a,C1 , .f,a,C2 ]=[true, \nfalse] describes the function f that always returns c1 , and [.f,a,C1 , .f,a,C2 ]=[a = C2,a = C1] describes \nthe function f that returns c1 if its input is c2 and vice versa. As a .nal example, the function define \nf(x)= if(y = c2 )then c1 else c2 where y is free is modeled by the equation: [.f,a,C1 , .f,a,C2 ]= .\u00df.[\u00df \n= C2 ,\u00df = C1 ] The existentially quanti.ed variable \u00df models the unknown result of referencing y. Note \nthat \u00df is shared by the two constraints; in particular, in any solution \u00df must be either C1 or C2 , capturing \nthat a function call returns only one value. Figure 1 presents most of the constraint inference rules \nfor the small language given above. The remaining rules are omitted for lack of space but are all straightforward \nanalogs of the rules shown. Rules 1-5 prove judgments A .true,false e : F, describing the constraints \nF under which an expression e evaluates to true or false in environment A. Rules 6-11 prove judgments \nA .ci e : F that give the constraint under which expression e evaluates to abstract value ci. Finally, \nrule 12 constructs systems of equations, giving the (possibly) mutually recursive conditions under which \na function returns each abstract value. We brie.y explain a subset of the rules. In Rule 3, two expres\u00adsions \ne1 and e2 are equal whenever both evaluate to the same value. Rule 8 says that if under environment A, \nvariable x has type a, then x evaluates to ci only if a = Ci. Rule 11 presents the rule for func\u00adtion \ncalls: If the input to function f is the abstract value ck, and the constraint under which f returns \nci is .f,a,Ci, then f(e)has type Ci under the constraint Fk ..f,a,Ci[Ck/a]. EXAMPLE 1. Suppose we analyze \nthe following function: define f(x)= if((x = c1 ).(y = c2 ))then c1 else f(c1 ) (1) A .true true : true \n(2) A .true false : false A .ci e1 : F1,i A .ci e2 : F2,i (3) W A .true (e1 = e2): (F1,i .F2,i) i A .true \ne : F (4) A .false e : \u00acF A .true e1 : F1 A .true e2 : F2 ..{., .} (5) A .true e1 .e2 : F1 .F2 (6) A \n.ci ci : true i j = (7) A .ci cj : false A(x)= a (8) A .ci x :(a = Ci) A .true e1 : F1 A .ci e2 : F2 \nA .ci e3 : F3 (9) A .ci if e1 then e2 else e3 :(F1 .F2).(\u00acF1 .F3) A .cj e1 : F1j A, x : a .ci e2 : F2i \n(a fresh) (10) W A .ci let x = e1 in e2 : j(F1j .F2i .(a = Cj)) A .ck e : Fk (11) W A .ci f (e): k(Fk \n..f,a,ci[Ck/a]) a .{\u00df1 ,...,\u00dfm} x : a, y1 : \u00df1 ,...,yn : \u00dfm .ci e : Fi 1 = i = n (12) . de.ne f(x) = \ne :[F].\u00df1 ,...,\u00dfm. [F i].f,a,ci= Figure 1. Inference Rules where y is unde.ned and the only abstract \nvalues are c1 and c2 . Then \"#\" # .f,a,C1 (a = C1 .\u00df = C2 ). = .\u00df. \u00ac(a = C1 .\u00df = C2 )..f,a,C1 [C1 /a] \n... ... is the equation computed by the inference rules (see Figure 2). Note that the substitution [C1/a]in \nthe formula expresses that the argument of the recursive call to f is c1 . Due to space constraints we \ncan only sketch the semantics of constraints. Constraints are interpreted over the standard four point \nlattice with .= true, false, .and ., true, false =., where . is meet, . is join, and \u00ac.=., \u00ac. = ., \u00actrue \n= false, and \u00acfalse = true. Given an assignment . for the existential variables, the meaning of a system \nof equations E is a standard limit of a series of approximations .(E0),.(E1),... generated by repeatedly \nunfolding E. We are interested in both the least .xed point (where the .rst approximation of all . variables \nis .) and greatest .xed point (where the .rst approximation is .) A(x)= aA(y)= \u00dfA .c1 c1 : true A .true \nx = c1 :(a = C1 ) A .true y = c2 :(\u00df = C2 ) A .c2 c1 : false A .true (x = c1 ). (y = c2 ):(a = C1 . \u00df \n= C2 ) A .c1 c1 : true A .c1 f(c1 ):(true . .f,a,C1 [c1 /a]) . false . ... x : a,y : \u00df = A .c1 if((x \n= c1 ). (y = c2 ))then c1 else f(c1 ):((a = C1 . \u00df = C2 ).true) . (\u00ac(a = C1 . \u00df = C2 ). .f,a,C1 [c1 /a]) \nFigure 2. Type derivation for the body of function f in Example 1 semantics. The value . in the least \n.xed point semantics (resp. . in the greatest .xed point) represents non-termination of the analyzed \nprogram. We do not attempt to reason about termination, and our results are generally quali.ed by an \nassumption that the program terminates. By construction, the inference rules in Figure 1 guarantee that \n.f,a,Ci . .f,a,Cj = false in the least .xed point W semantics if i = j, and i .f,a,Ci = true in the greatest \n.xed point semantics; we rely on this property in Section 6.2.  4. Boolean Constraints Our main technical \nresult is a sound and complete method for answering satis.ability (may) and validity (must) queries for \nthe constraints of De.nition 1. The algorithm has four major steps: eliminate the existentially bound \n(unobservable) variables by extracting necessary/suf.cient conditions from the equations;  rewrite the \nequations to be monotonic in the .variables;  eliminate recursion by a .xed point computation;  .nally, \napply a decision procedure to the closed-form equations.  Our target decision procedure for the last \nstep is SAT, and so at some point we must translate our type constraints into equivalent boolean constraints. \nWe perform this translation .rst, before per\u00adforming any of the steps above. For every type variable \n(observable or unobservable) si, we in\u00adtroduce boolean variables si1 , ..., sin such that sij is true \nif and only if si = Cj. We refer to boolean variables aij as observable variables and \u00dfij as unobservable \nvariables. We map the equa\u00adtion variables .f,a,Ci to boolean variables of the same name. A variable .f,a,Ci \nrepresents the condition under which f returns ci, hence we refer to .f,a,Ci s as return variables. We \nalso translate each t1 = t2 occurring in a type constraint: Ci = Ci . true Ci = Cj . false i = j vi = \nCj . vij Note that subexpressions of the form vi = vj never appear in the constraints generated by the \nsystem of Figure 1. We replace every substitution [Cj/ai]by the boolean substitution [true/aij] and [false/aik]for \nj = k. EXAMPLE 2. The .rst row of Example 1 results in the following boolean constraints (here boolean \nvariable a1 represents the equa\u00adtion a = C1 and \u00df2 represents \u00df = C2): .f,a,C1 = .\u00df2 .(a1 .\u00df2 ).(\u00ac(a1 \n.\u00df2 )..f,a,C1 [true/a1 ]) The existentially quanti.ed variable \u00df1 and substitution [false/a2 ] are omitted \nbecause neither \u00df1 nor a2 occurs in the formula. In the general case, the original type constraints result \nin a recursive system of boolean constraints of the following form: EQUATION 1. 23 FF [F]= .\u00dfF1 .[fF1i(aF1 \n,\u00df1 , .[Fb1 /Fa])] .f1 ,a,Ci 6 .. 7 E = 4 .. 5 .. [F.\u00dfFk.[FFF .fk,a,Ci]= fki(aFk,\u00dfk, .[Fbk/Fa])] where \n.F= (.f1 ,a,C1 , ..., .fk,a,Cn )and bi .{true, false}and the f s are quanti.er-free formulas over \u00dfF, \nF.. The substi\u00ad a, and Ftutions of the form .[FFb/Fa] result from translation of constraints produced \nby Rule 11 of Figure 1. 4.1 Satis.ability, Validity, and Monotonicity In this subsection we give a few \nde.nitions and technical lemmas used in Section 6 to prove our main result. As it stands the boolean \nconstraints do not quite preserve solutions of the type constraints. We add additional constraints guaranteeing \nthat a solution of the boolean translation of a type constraint guarantees every type vari\u00adable is assigned \nsome abstract value (existence) and that no type variable is assigned multiple abstract values (uniqueness): \nV 1. Uniqueness: .unique =( jj=k \u00ac(vij .vik)) W 2. Existence: .exist =( j vij) where vij is any boolean \nvariable. We can now formulate de.ni\u00adtions of satis.ability and validity for our system, SAT* and VALID*: \nDEFINITION 2. SAT* (f)= SAT(f ..exist ..unique) In other words, any satisfying assignment must observe \nthe exis\u00adtence and uniqueness assumptions for all boolean variables vij. DEFINITION 3. VALID* (f)= ({.exist}.{.unique}|= \nf) Using VALID*, we can conclude, for example, that a11 .a12 is a tautology in a language with two abstract \nvalues c1 and c2 . DEFINITION 4. Let f be a quanti.er-free formula over aij, \u00dfij, and .ij. Let M(f)be \nf converted to negation normal form (nega\u00adtions driven in and \u00ac\u00acx replaced by x) and replacing any negative \nliteral \u00acvij by W kjvik. Then M(f)is monotonic in vij. =j LEMMA 1. SAT* (f). SAT(M(f)..unique) PROOF. \nConsider any satisfying assignment v\u00afto M(f)..unique. Suppose that v\u00afassigns all vij to false such that \n.exist is violated. But since M(f)does not contain any negations, setting one vij to true satis.es M(f), \n.exist, and .unique, implying SAT* (f). The other direction is also easy and omitted. LEMMA 2. VALID* \n(f). ({.exist}|= M(f)) PROOF. Dual to the proof of Lemma 1. The original constraints have four possible \nmeanings (c.f., Sec\u00adtion 3) while the boolean constraints have only two. We claim with\u00adout proof that \nthe translation is correct in that whenever the mean\u00ading of the original constraints is either true or \nfalse (i.e., the original program terminates), the translation has the same meaning.  5. Necessary and \nSuf.cient Conditions As discussed in previous sections, a key step in our algorithm is extracting necessary/suf.cient \nconditions from a system of con\u00adstraints C. The necessary (resp. suf.cient) conditions should be sat\u00adis.able \n(resp. valid) if and only if C is satis.able (resp. valid). This section makes precise exactly what necessary/suf.cient \nconditions we need; in particular, there are two technical requirements: The necessary (resp. suf.cient) \nconditions should be as strong (resp. weak) as possible.  The necessary/suf.cient conditions should \nbe only over observ\u00adable variables.  In the following, we use V-(f)(resp. V+(f)) to denote the set of \nunobservable (resp. observable) variables \u00dfij (resp. aij) used in f. DEFINITION 5. Let f be a quanti.er-free \nformula. We say .f. is the strongest observable necessary condition for f if: (1) f ..f. (2) .f ' .((f \n. f ' ). (.f.. f ' )) where V-(f ' )= \u00d8.V+(f ' )= V+(f)  The .rst condition says .f.is necessary for \nf, and the second con\u00addition ensures .f. is stronger than any other necessary condition with respect \nto f s observable variables V+(f). The additional re\u00adstriction V-(.f.)= \u00d8enforces that the strongest \nnecessary condi\u00adtion for a formula f has no unobservable variables. DEFINITION 6. Let f be a quanti.er-free \nformula. We say .f. is the weakest observable suf.cient condition for f if: (1) .f.. f (2) .f ' .((f \n' . f). (f ' ..f.)) where V-(f ' )= \u00d8.V+(f ' )= V+(f)  We include the following variant of well-known \nresults. LEMMA 3. Observable strongest necessary and weakest suf.cient conditions for any formula f exist \nand are unique up to logical equivalence. Strongest necessary and weakest suf.cient conditions are im\u00admediately \nuseful in program analysis for answering queries about program properties. For example, let f be the \ncondition under which a given program property P holds. It follows immediately from the de.nition of \nnecessary and suf.cient conditions that: If SAT(.f.), then P MAY hold.  If VALID(.f.), then P MUST \nhold.  Furthermore, for the strongest and weakest such conditions, we have the following additional \nguarantees: If UNSAT(.f.), then P MUST NOT hold.  If INVALID(.f.), then P MAY NOT hold.  In this sense, \nstrongest necessary and weakest suf.cient condi\u00adtions of f de.ne a tight observable bound on f. If f \nhas only ob\u00adservable variables, then the strongest necessary and weakest suf.\u00adcient conditions of f are \nequivalent to f. If f has only unobserv\u00adable variables, then the best possible bounds are .f. = true \nand .f. = false. Intuitively, the difference between strongest nec\u00adessary and weakest suf.cient conditions \nof a formula de.ne the amount of uncertainty present in the original formula. EXAMPLE 3. Suppose we are \ninterested in determining the con\u00additions under which a pointer is dereferenced in a function call. Consider \nthe implementations of f and g given in Figure 3. Scanning the implementations of f and g, we see that \np is dereferenced under the following constraint: ' p!=NULL .flag!=0 .buf!=NULL .*buf == i ' Since the \nreturn value of malloc (i.e., buf) and the user input (i.e., *buf) are unobservable outside of f, the \nstrongest observable necessary condition for f to dereference p is given by the simpler condition: p!=NULL \n. flag!=0 1. int g(int* p) { 2. if(p==NULL) return -1; 3. return 1; 4. } 5. void f(int* p, int flag) \n{ 6. if(g(p)<0 || !flag) return; 7. char* buf = malloc(sizeof(char)); 8. if(!buf) return; 9. *buf \n= getUserInput();  10. if(*buf== i ) 11. *p =1; 12. }  Figure 3. Example code. On the other hand, \nnothing in a calling context of f guarantees that p is dereferenced when f is called; hence, the weakest \nobservable suf.cient condition for the dereference is false. 6. Solving the Constraints In this section, \nwe give an algorithm for computing observable strongest necessary and weakest suf.cient conditions for \nthe equa\u00adtions given in Section 4. Our algorithm .rst eliminates existentially quanti.ed variables from \nevery formula (Section 6.1). We then transform the equations to both be monotonic in the return variables \nand preserve strongest necessary (weakest suf.cient) conditions under substitution (Section 6.2). Finally, \nwe solve the equations to eliminate recursive constraints (Section 6.3), yielding a system of (non-recursive) \nformulas over observable variables. Each step pre\u00adserves the satis.ability/validity of the original equations, \nand thus the original may/must query can be decided using a standard SAT solver on the .nal formulas. \n6.1 Eliminating Unobservable Variables The .rst step of our algorithm is to eliminate each existentially \nquanti.ed unobservable variable \u00dfij. We use the following result: LEMMA 4. 1. The strongest necessary \ncondition of f not containing v is: SNC(f, v)= f[true/v].f[false/v] 2. The weakest suf.cient condition \nof f not containing v is: WSC(f, v)= f[true/v].f[false/v] Proofs of these results were .rst given by \nBoole [7]. This technique for computing strongest necessary and weakest suf.cient condi\u00adtions for formulas \nnot containing a given variable vi is sometimes referred to as eliminating the middle term or forgetting \na variable. Recall from Section 4.1 that any satis.able formula must also satisfy existence and uniqueness \nconstraints, while any formula en\u00adtailed by .exist and .unique is a tautology. For example, the strongest \nnecessary condition for \u00df11 .\u00df12 not containing \u00df s is false in our system, even though applying the \ntechnique from Lemma 4 yields true. Similar problems arise for weakest suf.cient conditions. To compute \nstrongest necessary and weakest suf.cient conditions that obey the additional existence and uniqueness \nconditions of our sys\u00adtem, we de.ne SNC* and WSC* as follows: DEFINITION 7. 1. The strongest necessary \ncondition SNC* of f without v is: SNC* (f, v) = (f ..exist ..unique)[true/v]. (f ..exist ..unique)[false/v] \n2. The weakest suf.cient condition WSC* of f without v is: WSC* (f, v) = (f .\u00ac.exist .\u00ac.unique)[true/v]. \n(f .\u00ac.exist .\u00ac.unique)[false/v] That these are in fact the strongest necessary/weakest suf.cient observable \nconditions follows from Lemma 4 and De.nitions 2 and 3. We compute necessary (resp. suf.cient conditions) \nby replacing all expressions .v.f by SNC* (f, v)(resp. WSC* (f, v)). Thus, this step yields two distinct \nsets of equations, one for necessary and one for suf.cient conditions. Note that, in the general case \nof Lemma 4, the strongest neces\u00adsary and weakest suf.cient conditions of any formula may double the size \nof the original formula. However, it is easy to see that if a literal v occurs only positively in f, \nthe strongest necessary condi\u00adtion can be computed as f[v/true], and if v occurs only negatively, the \nstrongest necessary condition is given by f[v/false]. Analogous optimizations apply to weakest suf.cient \nconditions. Furthermore, it is not always necessary to add the existence and uniqueness con\u00adstraints \nfor any unobservable variable as suggested by De.nition 7. For example, if a formula contains \u00dfij, but \nno \u00dfik, it is unnecessary to add the explicit uniqueness constraint \u00ac(\u00dfij .\u00dfik). EXAMPLE 4. Consider \nthe function given in Example 1, for which boolean constraints are given in Example 2. We compute the \nstrongest necessary condition for .f,a,C1 : ..f,a,C1 . =(a1 .true). (\u00ac(a1 .true)...f,a,C1 .[true/a1 ]) \n. (a1 .false). (\u00ac(a1 .false)...f,a,C1 .[true/a1 ]) = true The reader can verify that the weakest suf.cient \ncondition for .f,a,C1 is also true. In the above derivation, the existence and uniqueness constraints \nare omitted since they are redundant. After eliminating unobservable variables from formulas fij of Equation \n1, we obtain two systems of constraints ENC and ESC, giving the strongest necessary and weakest suf.cient \nobservable conditions, respectively: EQUATION 2. 2 3 ..f1 ,a,C1 . = f ' 11 (aF1 , ..F .[Fb1 /Fa]) 6 7 \n6 ENC = .. 7 4 . 5 ..fk,a,Cn . = f ' kn(aFk, ..F .[Fbk/Fa]) ESC is analogous to ENC.  6.2 Preservation \nUnder Substitution Our goal is to solve the recursive system given in Equation 2 by an iterative, .xed \npoint computation. However, there is a problem: as it stands, Equation 2 may not preserve strongest necessary \nand weakest suf.cient conditions under substitution, a serious problem if we are to compute .xed points \nby repeated substitution. EXAMPLE 5. define g(x)= if(x = c1 .z = c2 )then c1 else c2 define f(x)= let \ny = g(c1 )in if(\u00ac(y = c1 ))then c1 else c2 Suppose we want the strongest necessary condition for f returning \nc1 . Using the machinery presented so far, we compute: ..f,a,C1 . = \u00ac(..g,a,C1 .[true/a1 ][false/a2 ]) \n..g,a,C1 . = a1 where a1 represents the constraint a = C1, and a2 represents a = C2. When we replace \na1 for the occurrence of ..g,a,C1 . in the .rst equation, we obtain false as the strongest necessary \ncondition for f to return c1 . This result is wrong, since f returns c1 if the variable z in function \ng is c1 . Thus, the strongest necessary condition for f returning c1 is true. This example illustrates \nthat Equation 2 does not preserve strongest necessary conditions under substitution; in fact, the re\u00adsult \nwe obtained is not even a necessary condition. The problem arises because .\u00acf. = \u00ac.f.. To ensure that \nstrongest necessary and weakest suf.cient conditions are preserved under substitution, the return variables \nmay only occur monotonically in a formula. Note that replacing \u00ac.f,a,Ci by W jj.f,a,Cj is not suf.cient \n=i to solve the problem, because the satis.ability of a formula in our system also requires that the \nformula obey the uniqueness V constraint \u00ac( ij \u00ac(.f,a,Ci . .f,a,Cj)) which also contains =j negations \non return variables. Similar problems arise for weakest W suf.cient conditions because the existence \nconstraint j .f,a,Cj appears anti-monotonically in the de.nition of validity.1 Fortunately, we can transform \nENC and ESC into monotonic system of equations T(ENC)and T(ESC)such that: 1. The latter equations contain \nno negations on return variables. 2. SAT* (ENC). SAT(T(ENC)) 3. VALID* (ESC). VALID(T(ESC))  The .rst \nproperty is necessary to guarantee that strongest necessary and weakest suf.cient conditions are preserved \nunder substitution, while conditions 2 and 3 are required to ensure that strongest nec\u00adessary and weakest \nsuf.cient conditions obtained by our algorithm are satis.ability and validity preserving respectively. \nLemma 5 below states that T(ENC)has properties 1 and 2 listed above, and the proof of the lemma presents \nan outline of this trans\u00adformation. Lemma 6 states that T(ENC)preserves strongest neces\u00adsary conditions \nunder syntactic substitution such that the strongest necessary conditions computed by our technique are \nsatis.ability preserving. Lemmas 7 and 8 state dual results for weakest suf.\u00adcient conditions and validity \npreservation. LEMMA 5. For a system of equations ENC, there exists a system T(ENC)in which all return \nvariables occur only monotonically and for f . ENC and f ' .T(ENC), SAT* (f). SAT(f ' ). PROOF. From \nLemma 1, we have SAT* (f) . SAT(M(f) . .unique). Hence it suf.ces to show there exists a f ' such that: \nSAT(f ' ). SAT(M(f)..unique) To obtain f ' , we convert M(f)to disjunctive normal form: ' (p11... ....p1n)....(pi1 \n....pij... .pin) f = DNF(M(f))= ....(pm1 ... ....pmn) We enforce uniqueness by dropping contradictions \nfrom every dis\u00adjunct of the form (pi1 ....pij... .pin), i.e., if any disjunct contains both .f,a,Ci and \n.f,a,Cj for i = j, we replace the clause by false. The .nal formula f ' is equi-satis.able to M(f)..unique. \nNow we can show that T(ENC)preserves strongest necessary con\u00additions under substitution. This result \nis what ultimately guarantees our algorithm computes the strongest necessary condition for the original \nrecursive system given in Equation 1. 1 The uniqueness and existence constraints of the form V W \u00ac( \nij\u00ac(.f,a,Ci . .f,a,Cj)) and j only apply to . =j .f,a,Cj variables arising from the same call site. While \nwe do not explicitly label . variables with their respective instantiation sites, from here on, we assume \nthat the uniqueness and existence constraints only apply to return variables arising from the same call \nsite. We rely on this assumption in the proof of Lemmas 5 and 7. LEMMA 6. Let f .T(ENC)be a formula containing \nliterals aFand F .. Let F be the strongest necessary condition for a return variable .a,Ci only containing \nobservable variables. Then the strongest necessary condition for f not containing .a,Ci is given by: \n.f. = f[F/.a,Ci] PROOF. We .rst show that f[F/.a,Ci] is a necessary condi\u00adtion for f. If f =.a,Ci, then \nf[F/.a,Ci]= F. So, f . f[F/.a,Ci]since .a,Ci .F. If f = v where v =.a,Ci, then f . f[F/.a,Ci]since f[F/.a,Ci]= \nf. Note that we do not need to consider the case f = \u00ac.a,Ci since .a,Ci s occur only monotonically. The \n.rst of two cases for the inductive step is (1) f = f1 .f2. Suppose f1 .f2 . (f1 .f2)[F/.a,Ci]such that \nf1 .f2 . (f1 [F/.a,Ci]).(f2[F/.a,Ci]) (*) Then, there is a truth assignment v\u00afsatisfying f1 . f2, but \nnot (f1[F/.a,Ci])).(f2[F/.a,Ci]). By induction, f1 . f1 [F/.a,Ci] . f2 . f2[F/.a,Ci] Hence if f1 and \nf2 are true then (f1[F/.a,Ci])).(f2[F/.a,Ci]) must also be true, yielding a contradiction with (*). Case \n(2) f = f1 .f2 is similar to case (1). We now show f[F/.a,Ci] is the strongest necessary condi\u00adtion for \nf, i.e., f[F/.a,Ci] . f[F '' /.a,Ci] for any neces\u00adsary condition F '' . Consider the case where f =.a,Ci. \nLet f[F '' /.a,Ci]be another necessary condition for f. Since F is the strongest necessary condition \nfor .a,Ci, we have F.F '' and hence f[F/.a,Ci] . f[F '' /.a,Ci]. The case where f = v and v =.a,Ci is \nalso trivially true. There are two inductive cases. The .rst is (1) f = f1 .f2 . Let f[F '' /.a,Ci]be \nanother necessary condition for f such that f[F/.a,Ci]. f[F '' /.a,Ci] Then, there must exist a truth \nassignment v\u00afsatisfying f[F/.a,Ci], but not f[F '' /.a,Ci]. By induction: f1[F/.a,Ci]. f1 [F '' /.a,Ci] \n. f2[F/.a,Ci]. f2 [F '' /.a,Ci] Hence, '' '' (f1[F/.a,Ci].f2 [F/.a,Ci]). (f1[F /.a,Ci].f2 [F /.a,Ci]) \nthus v\u00afmust also satisfy f[F '' /.a,Ci], a contradiction. Case (2) f = f1 .f2 is again symmetric to case \n(1). The dual of this result holds for weakest suf.cient conditions. LEMMA 7. For every system of equations \nESC, there exists another system of equations T(ESC)in which all return predicates occur only monotonically \nand for every f . ESC and f ' .T(ESC)such that VALID* (f). VALID(f ' ). PROOF. From Lemma 2, we have \nVALID* (f) .{.exist |= M(f)}. Hence it suf.ces to show there exists a f ' such that ({.exist}|= f) . \n({} |= f ' ). We obtain such a f ' by convert\u00ading M(f)to conjunctive normal form: ' (p11 ... ....p1n)....(pi1 \n....pij... .pin) f = CNF(f)= ....(pm1 ... ....pmn) We then eliminate all tautologies by replacing every \nclause that contains .a,Ci for all i, 1 = i = n, with true. It is easy to see that ({.exist}|= f). ({}|=). \nf ' The following lemma states that T(ESC)is validity-preserving under substitution: LEMMA 8. Let f .T(ESC)be \na formula containing literals aFand F .. Let F be the weakest suf.cient condition for .a,Ci. Then the \nweakest suf.cient condition for f not containing .a,Ci is: .f. = f[F/.a,Ci] PROOF. The proof is similar \nto the proof of Lemma 6. 6.3 Eliminating Recursion After eliminating unobservable variables and transforming \nthe con\u00adstraints into monotonic satis.ability and validity-preserving sys\u00adtems respectively, we obtain \nthe following systems of equations: EQUATION 3. 2 T(..f1 ,a,C1 .)= 6 . T(ENC)= 4 . . 3 f11 (aF1 , T(..F.)[Fb1/Fai]) \n7 5 T(..fk,a,Cn .)= fkn(F..)[Fai]) ak, T(.Fbk/F The system T(ESC)is analogous. Consider vectors of boolean \nformulas F. over the aij s appearing in the constraints; these formulas have no unobservable or return \nvariables. We de.ne a lattice L with the following ordering: --.- . .NC = falsen\u00b7m .SC = truen\u00b7m - .--. \n.NC = truen\u00b7m .SC = falsen\u00b7m F.1 .NC F.2 = (..., .1i ..2i, ...) F.1 .SC F.2 = (..., .1i ..2i, ...) The \nlattice L is .nite (up to logical equivalence) since there are only a .nite number of variables aij and \nhence only a .nite number of logically distinct formulas. We de.ne two functions from L to L FNC(F.NC)= \n(...,fij[F..)],...) .NC/T(.FFSC(F.SC)= (...,fij[F..)],...) .SC/T(.F substituting boolean formulas F. \n.a,Ci for return variables T(F). We compute a least .xed point solution for ENC as .x(FNC(.NC)) and for \nESC as .x(FSC(.SC)). The .xed points exist because both systems are monotonic in T(.F..)respectively. \n..)and T(.F EXAMPLE 6. Recall that in Example 4 we computed ..f,a,C1 . for the function f de.ned in \nExample 1 as: ..f,a,C1 . = a1 .(\u00aca1 ...f,a,C1 .[true/a1 ]) Note that this formula is already monotonic \nin ..f,a,C1 . and does not contain any contradictions or tautologies. To .nd the weakest suf.cient condition \nfor .f,a,C1 , we .rst substitute true for ..f,a,C1 .. This yields the formula a1 .\u00aca1 , a tautology. \nAs a result, our algorithm .nds the .xed point solution true for the weakest suf.cient condition of .f,a,C1 \n. Since f is always guaranteed to return c1 , the weakest suf.cient condition computed using our algorithm \nis the most precise solution possible.  7. Implementation We have implemented our method in Saturn, \na summary-based, context, and intraprocedurally path-sensitive analysis framework [1]. Our implementation \nextends the existing Saturn infrastructure to allow client analyses to query fully interprocedural strongest \nnecessary and weakest suf.cient conditions for the intraprocedural constraints computed by Saturn, where \nfunction return values and side effects are represented as unconstrained variables.2 For exam\u00adple, given \nan intraprocedural constraint computed by Saturn, such as x = 1 . queryUser(y)= true for the queryUser \nfunction from Section 1, our analysis yields the interprocedural constraints 2 Saturn treats loops as \ntail-recursive functions; hence, we also compute strongest necessary and weakest suf.cient conditions \nfor side effects of loops. x = 1 .y = true as the strongest necessary condition and false as the weakest \nsuf.cient condition. While it is important in our technique that the set of possible values can be exhaustively \nenumerated (i.e., so that the comple\u00adment of \u00ac.a,Ci is expressible as a .nite disjunction, recall Sec\u00adtion \n6.2), it is not necessary that the set be .nite, but only .nitary, that is, .nite for a given program. \nFurthermore, while it is clear that the technique can be applied to .nite state properties or enumerated \ntypes, it can also be extended to any property where a .nite number of equivalence classes can be derived \nto describe the possible out\u00adcomes. Our implementation goes beyond .nite state properties; it .rst collects \nthe set of all predicates corresponding to comparisons between function return values (and side effects) \nand constants. For instance, if a condition such as if(foo(a)== 3)is used at some call site of foo, then \nwe compute strongest necessary and weakest suf.cient conditions for .foo,a,3 and its negation. This technique \nal\u00adlows us to .nitize the interesting set of return values associated with a function and makes it possible \nto use the algorithms described so far with minor modi.cations. Note that any .nitization strategy en\u00adtails \na loss of precision in some situations. For example, if the return values of two arbitrary functions \nf and g are compared with each other, the strategy we use may not allow us to determine the exact necessary \nand suf.cient condition under which f and g return the same value. The algorithm of Section 6.3 computes \na least .xed point. How\u00adever, the underlying Saturn infrastructure can fail by exceeding re\u00adsource limits \n(e.g., time-outs); if any iteration of the .xed pointcomputation failed to complete we would be left \nwith unsound ap\u00adproximations. Thus, our implementation computes a greatest .xedpoint, as we can halt \nat any iteration and still have sound results.The greatest .xed point is less precise than the least \n.xed pointin some cases, such as for non-terminating computation paths. Forinstance, for the simple everywhere \nnon-terminating function: define f(x)= if(f(x)= c1 )then c1 else c2 the greatest .xed point computation \nyields true for the strongest necessary condition for f returning c1 while the least .xed point computation \nyields false. The toy language used in the technical part of the paper assumesthat each function has \nexactly one output (i.e., functions do nothave side effects). This restriction makes it safe to eliminate \nallunobservable variables while still guaranteeing completeness for.nite domains. When functions have \nmultiple outputs, however,there may also be correlations between different outputs, wherethe correlation \nis established through the use of an unobservablevariable. The example below illustrates such a correlation: \nint foo(int** p) { int* x = malloc(sizeof(int)); if(!x) return -1; *p = x; return 1; } Note that the \npredicate foo(p) == 1 implies that *p is initialized to a non-null value; however, we cannot reason about \nthis correla\u00adtion if we eliminate the unobservable variable corresponding to the return value of malloc. \nIn such cases, our implementation intro\u00adduces additional variables describing output state to capture \nexter\u00adnally visible dependencies between different outputs.  8. Experimental Results We conducted two \nsets of experiments to evaluate our technique on OpenSSH, Samba, and the Linux kernel. In the .rst set \nof experi\u00adments we compute necessary and suf.cient conditions for pointer dereferences. Pointer dereferences \nare ubiquitous in C programs and computing the necessary and suf.cient conditions for each and every \nsyntactic pointer dereference to execute is a good stress test Necessary and Sufficient Condition Size \nFrequency 100000 10000 1000 100 10 1 Size of necessary and sufficient conditions  Figure 4. Frequency \nof necessary and suf.cient condition sizes (in terms of the number of boolean connectives) at sinks for \nFrequency (in log scale) Linux Average original guard size Average NC size (sink) Average SC size (sink) \nAverage NC size (source) Average SC size (source) Average call chain depth Lines of code Linux Samba \nOpenSSH 2.6.17.1 3.0.23b 4.3p2 3.00 4.45 3.02  0.75 1.02 0.75 0.48 0.67 0.50 2.39 2.82 1.39 0.45 0.49 \n0.67 5.98 4.67 2.03 6,275,017 515,689 155,660 Figure 5. Necessary and suf.cient condition sizes (in \nterms of number of boolean connectives in the formula) for pointer dereferences. for our approach. As \na second experiment, we incorporate our tech\u00adnique into a null dereference analysis and demonstrate that \nour technique reduces the number of false positives by close to an order of magnitude without resorting \nto ad-hoc heuristics or compromis\u00ading soundness. In our .rst set of experiments, we measure the size \nof neces\u00adsary and suf.cient conditions for pointer dereferences both at sinks, where pointers are dereferenced, \nand at sources, where pointersare .rst allocated or read from the heap. In Figure 3, consider thepointer \ndereference (sink) at line 11. For the sink experiments, wewould, for example, compute the necessary \nand suf.cient condi\u00adtions for p s dereference as p!= NULL . flag!= 0 and false respectively. To illustrate \nthe source experiment, consider the fol\u00adlowing call sites of function f from Figure 3: void foo() { int* \np = malloc(sizeof(int)); /*source*/ ... bar(p, flag, x); } void bar(int* p, int flag, int x) { if(x \n> MAX) *p = -1; else f(p, flag); } The line marked /*source*/ is the source of pointer p;the neces\u00adsary \ncondition at p s source for p to be ultimately dereferenced is x > MAX .(x <= MAX .p!= NULL .flag!= 0)and \nthe suf.cient condition is x > MAX. The results of the sink experiments for Linux are presented in Figure \n4, and the results of source experiments are given in Figure 6. The table in Figure 5 presents a summary \nof the results of both the source and sink experiments for OpenSSH, Samba, and Linux. Total Reports Bugs \nInterprocedurally OpenSSH 4.3p2 3 1 PathSamba 3.0.23b 48 17 -sensitive Linux 2.6.17.1 171 134 Intraprocedurally \nOpenSSH 4.3p2 21 1 PathSamba 3.0.23b 379 17 -sensitive Linux 2.6.17.1 1495 134 False Positives 2 25 37 \n20 356 1344 Undecided 0 6 17 0 6 17 Report to Bug Ratio 3 2.8 1.3 21 22.3 11.2 Figure 7. Results of \nnull dereference experiments for the interprocedurally path-sensitive (.rst three columns) and intraprocedurally \npath-sensitive, but interprocedurally path-insensitive (last three columns) analyses Necessary Condition \nSize vs. Depth of Propagation in Linux 6 5 4 3 2 1 0 Maximal depth of dereference propagation Sufficient \nCondition Size vs. Depth of Propagation in Linux 1.2 1 0.8 0.6 0.4 0.2 0Maximal depth of dereference \npropagation Sufficient condition size Necessary condition size Figure 6. Necessary and suf.cient condition \nsizes at sources vs. call chain length in Linux The histogram in Figure 4 plots the size of necessary \n(resp. suf.\u00adcient) conditions against the number of guards that have a necessary (resp. suf.cient) condition \nof the given size. In this .gure, red bars indicate necessary conditions, green bars indicate suf.cient \ncon\u00additions, and note that the y-axis is drawn on a log-scale. Observe that 95% of all necessary and \nsuf.cient conditions have fewer than .ve subclauses, and 99% have fewer than ten subclauses, showing \nthat necessary and suf.cient conditions are small in practice. Fig\u00adure 5 presents average necessary and \nsuf.cient condition sizes at sinks (rows 2 and 3) for all three applications we analyzed, con\u00ad.rming \nthat average necessary and suf.cient condition sizes are consistently small across all of our benchmarks. \nFurther, the av\u00aderage size of necessary and suf.cient conditions are considerably smaller than the average \nsize of the original guards (which contain unobservable variables as well as the place-holder return \nvariables representing unsolved constraints, denoted by .in our formalism). Figure 6 plots the maximal \nlength of call chain from a source to any feasible sink against the size of necessary and suf.cient condition \nsizes at sources for Linux. In this .gure, the points mark average sizes, while the error bars indicate \none standard deviation. First, observe that the size of necessary and suf.cient conditions is small and \ndoes not grow with the length of the call chain. Second, note that the necessary condition sizes are \ntypically larger than suf.cient condition sizes; the difference is especially pronounced as the call \nchain length grows. Figure 5 also corroborates this trend for the other benchmark applications; average \nsize of necessary conditions (row 4) is larger than that of suf.cient conditions (row 5) at sources. \n Our second experiment applies these techniques to .nding null dereference errors. We chose null dereferences \nas an application because checking for null dereference errors with suf.cient preci\u00adsion often requires \ntracking complex path conditions. To identify null dereference errors, we query the strongest necessary \ncondi\u00adtion g1 for the constraint under which a pointer p is null and the strongest necessary condition \ng2 of the constraint under which p is dereferenced. A null pointer error is feasible if SAT(g1 . g2). \nOur implementation performs a bottom-up analysis and reports er\u00adrors in the .rst method where a feasible \npath from a null value to a dereference is determined. The .rst three columns of Figure 7 give the results \nof our fully (interprocedurally) path-sensitive null dereference experiments, and the last three columns \nof the same .gure present the results of the intraprocedurally path-sensitive, but interprocedurally \npath\u00adinsensitive null dereference experiments. One important caveat is that the numbers reported here \nexclude error reports arising from array elements and recursive .elds of data structures. Saturn does \nnot have a sophisticated shape analysis; hence, the overwhelming majority (> 95%) of errors reported \nfor elements of unbounded data structures are false positives. However, shape analysis is an orthogonal \nproblem; we leave incorporating shape analysis as fu\u00adture work. (To give the reader a rough idea of number \nof reports involving arrays and unbounded data structures, the number of total reports is 50 and 170 \nwith and without full path-sensitivity respectively for OpenSSH.) A comparison of the results of the \nintraprocedurally and inter\u00adprocedurally path-sensitive analyses shows that our technique re\u00adduces the \nnumber of false positives by close to an order of magni\u00adtude without resorting to heuristics or compromising \nsoundness in order to eliminate errors arising from interprocedurally correlated branches. Note that \nthe existence of false positives for the fully path-sensitive experiments does not contradict our previous \nclaim that our technique is complete. First, even for .nite domains, our technique can only provide relative \ncompleteness; false positives can still arise from orthogonal sources of imprecision in the analy\u00adsis \n(e.g., imprecise function pointer targets, inline assembly, imple\u00admentation bugs, time-outs). Second, \nwhile our results are complete for .nite domains, we cannot guarantee completeness for arbitrary domains. \nFor example, when arbitrary arithmetic is involved in path constraints, our technique may fail to compute \nthe strongest neces\u00adsary and weakest suf.cient conditions. The null dereference experiments were performed \non a shared cluster, making it dif.cult to give precise running times. A typical run with approximately \n10-30 cores took around tens of minutes on SSH, a few hours on Samba, and up to more than ten hours on \nLinux. The running times (as well as time-out rates) of the fully path-sensitive and the intraprocedurally \npath-sensitive analy\u00adsis were comparable for OpenSSH and Samba, but the less precise analysis took substantially \nlonger for Linux because the fully path\u00adsensitive analysis rules out many more interprocedurally infeasible \npaths, substantially reducing summary sizes. The results of Figure 7 show that interprocedurally path\u00adsensitive \nanalysis is important for practical veri.cation of software. For example, according to Figure 7, .nding \na single correct er\u00adror report in Samba requires inspecting approximately 22.3 error reports for the \ninterprocedurally path-insensitive analysis, while it takes 2.8 inspections to .nd a correct bug report \nwith the fully path-sensitive analysis, presumably reducing user effort by a factor of 8. 9. Conclusions \nWe have given a method for computing the precise necessary and suf.cient conditions for program properties \nthat are fully context\u00adand path-sensitive, including in the presence of recursive func\u00adtions. We have \ndemonstrated the practicality of our system, con\u00ad.rming that the approach scales to problems as computationally \nintensive as computing the necessary and suf.cient condition for each pointer dereference in multi-million \nline C programs, as well as checking for null dereference errors in the largest existing open\u00adsource \napplications. Acknowledgments We would like to thank Rajeev Alur, Suhabe Bugrara, Philip Guo, Chris Unkel, \nand the anonymous reviewers for helpful comments on earlier drafts of this paper. References [1] A. Aiken, \nS. Bugrara, I. Dillig, T. Dillig, B. Hackett, and P. Hawkins. An overview od the SATURN project. In Proc. \nWorkshop on Program Analysis for Software Tools and Engineering, pages 43 48, 2007. [2] A. Aiken, E.L. \nWimmers, and J. Palsberg. Optimal Representations of Polymorphic Types with Subtyping. Higher-Order and \nSymbolic Computation, 12(3):237 282, 1999. [3] T. Ball and S. Rajamani. Bebop: A symbolic model checker \nfor boolean programs. In Proceedings of the 7th International SPIN Workshop on SPIN Model Checking and \nSoftware Veri.cation, pages 113 130, London, UK, 2000. Springer-Verlag. [4] T. Ball and S. Rajamani. \nAutomatically validating temporal safety properties of interfaces. LNCS, 2057:103 122, 2001. [5] T. Ball \nand S. Rajamani. Bebop: a path-sensitive interprocedural data.ow engine. In PASTE 01: Proceedings of \nthe 2001 ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering, pages 97 \n103, New York, NY, USA, 2001. ACM. [6] R. Bloem, I. Moon, K. Ravi, and F. Somenzi. Approximations for \n.xpoint computations in symbolic model checking. [7] G. Boole. An Investigation of the Laws of Thought. \nDover Publications, Incorporated, 1858. [8] S. Bugrara and A. Aiken. Verifying the safety of user pointer \ndereferences. In IEEE Symposium on Security and Privacy, 2008. [9] J. Burch, E. Clarke, K. McMillan, \nD.. Dill, and L. Hwang. Symbolic model checking: 1020 states and beyond. In Proc. Symposium on Logic \nin Computer Science, June 1990. [10] D. Dill and H. Wong-Toi. Veri.cation of real-time systems by successive \nover and under approximation. In Proc. International Conference On Computer Aided Veri.cation, volume \n939, pages 409 422, 1995. [11] M. Das, S. Lerner, and M. Seigle. ESP: Path-sensitive program veri.cation \nin polynomial time. In Proc. Conference on Programming Language Design and Implementation, pages 57 68, \n2002. [12] I. Dillig, T. Dillig, and A. Aiken. Static error detection using semantic inconsistency inference. \nIn Proc. Conference on Programming Language Design and Implementation, pages 335 345, 2007. [13] J. Esparaza \nand S. Schwoon. A bdd-based model checker for recursive programs. Lecture Notes in Computer Science, \n2102/2001:324 336, 2001. [14] B. Hackett and A. Aiken. How is aliasing used in systems software? In Proc. \nInternational Symposium on Foundations of Software Engineering, pages 69 80, 2006. [15] F. Henglein. \nType inference and semi-uni.cation. In Proc. Conference on LISP and Functional Programming, pages 184 \n197, 1988. [16] T. Henzinger, R. Jhala, R. Majumdar, and K. McMillan. Abstractions from proofs. In Proc. \n31st Symposium on Principles of Programming Languages, pages 232 244, 2004. [17] F. Ivancic, Z. Yang, \nM.K. Ganai, A. Gupta, I. Shlyakhter, and P. Ashar. F-soft:software veri.cation platform. Lecture Notes \nin Computer Science, 3576/2005:301 306, 2005. [18] F. Lin. On strongest necessary and weakest suf.cient \nconditions. In Proc. International Conference on Principles of Knowledge Representation and Reasoning, \npages 143 159, April 2000. [19] A. Mycroft. Polymorphic type schemes and recursive de.nitions. In Proc. \nColloquium on International Symposium on Programming, pages 217 228, 1984. [20] T. Reps, S. Horwitz, \nand M. Sagiv. Precise interprocedural data.ow analysis via graph reachability. In POPL 95: Proceedings \nof the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 49 61, New York, \nNY, USA, 1995. ACM. [21] D. Schmidt. A calculus of logical relations for over-and underap\u00adproximating \nstatic analyses. Science of Computer Programming, 64(1):29 53, 2007. [22] M. Sharir and A. Pnueli. Two \napproaches to interprocedural data .ow analysis. Program Flow Analysis: Theory and Applications, pages \n189 234, 1981. [23] Y. Xie and A. Aiken. Scalable error detection using boolean satis.ability. SIGPLAN \nNot., 40(1):351 363, 2005.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>We present a new, precise technique for fully path- and context-sensitive program analysis. Our technique exploits two observations: First, using quantified, recursive formulas, path- and context-sensitive conditions for many program properties can be expressed exactly. To compute a closed form solution to such recursive constraints, we differentiate between <i>observable</i> and <i>unobservable</i> variables, the latter of which are existentially quantified in our approach. Using the insight that unobservable variables can be eliminated outside a certain scope, our technique computes satisfiability- and validity-preserving closed-form solutions to the original recursive constraints. We prove the solution is as precise as the original system for answering may and must queries as well as being small in practice, allowing our technique to scale to the entire Linux kernel, a program with over 6 million lines of code.</p>", "authors": [{"name": "Isil Dillig", "author_profile_id": "81331491247", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P1022798", "email_address": "", "orcid_id": ""}, {"name": "Thomas Dillig", "author_profile_id": "81331491149", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P1022799", "email_address": "", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P1022800", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375615", "year": "2008", "article_id": "1375615", "conference": "PLDI", "title": "Sound, complete and scalable path-sensitive analysis", "url": "http://dl.acm.org/citation.cfm?id=1375615"}