{"article_publication_date": "06-07-2008", "fulltext": "\n XMem: Type-Safe, Transparent, Shared Memory for Cross-Runtime Communication and Coordination Michal \nWegiel Chandra Krintz Computer Science Department Univ. of California, Santa Barbara {mwegiel,ckrintz}@cs.ucsb.edu \nAbstract Developers commonly build contemporary enterprise applications using type-safe, component-based \nplatforms, such as J2EE, and ar\u00adchitect them to comprise multiple tiers, such as a web container, application \nserver, and database engine. Administrators increas\u00adingly execute each tier in its own managed runtime \nenvironment (MRE) to improve reliability and to manage system complexity through the fault containment \nand modularity offered by isolated MRE instances. Such isolation, however, necessitates expensive cross-tier \ncommunication based on protocols such as object se\u00adrialization and remote procedure calls. Administrators \ncommonly co-locate communicating MREs on a single host to reduce com\u00admunication overhead and to better \nexploit increasing numbers of available processing cores. However, state-of-the-art MREs offer no support \nfor more ef.cient communication between co-located MREs, while fast inter-process communication mechanisms, \nsuch as shared memory, are widely available as a standard operating sys\u00adtem service on most modern platforms. \nTo address this growing need, we present the design and imple\u00admentation of XMem type-safe, transparent, \nshared memory sup\u00adport for co-located MREs. XMem guarantees type-safety through coordinated, parallel, \nmulti-process class loading and garbage col\u00adlection. To avoid introducing any level of indirection, XMem \nma\u00adnipulates virtual memory mapping. In addition, object sharing in XMem is fully transparent: shared \nobjects are identical to local ob\u00adjects in terms of .eld access, synchronization, garbage collection, \nand method invocation, with the only difference being that shared\u00adto-private pointers are disallowed. \nXMem facilitates easy integra\u00adtion and use by existing communication technologies and software systems, \nsuch as RMI, JNDI, JDBC, serialization/XML, and net\u00adwork sockets. We have implemented XMem in the open-source, \nproduction\u00adquality HotSpot Java Virtual Machine. Our experimental evalua\u00adtion, based on core communication \ntechnologies underlying J2EE, as well as using open-source server applications, indicates that XMem signi.cantly \nimproves throughput and response time by avoiding the overheads imposed by object serialization and network \ncommunication. Categories and Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language Constructs \nand Features Dynamic Storage Management, Classes and Objects; D.3.4 [Programming Lan- Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 08, June 7 \n13, 2008, Tucson, Arizona, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-860-2/08/06. . . $5.00 guages]: \nProcessors Run-Time Environments, Memory Manage\u00ad ment (Garbage Collection), Compilers, Optimization \nGeneral Terms Design, Experimentation, Languages, Manage\u00adment, Measurement, Performance Keywords Interprocess \nCommunication, Managed Runtimes, Shared Memory, Transparent, Type-Safe, Garbage Collection, Syn\u00adchronization, \nClass Loading, Parallel 1. Introduction Developers today predominately construct modern, enterprise, \ncomponent-based, middleware for portable, distributed applica\u00adtions using type-safe, object-oriented \nlanguages, such as Java, which users execute within managed runtime environments (MREs). These MREs typically \nsupport garbage collection (GC), dynamic class loading, incremental compilation, as well as high-level \nthread\u00ading and synchronization primitives, among other runtime services. One popular example from this \napplication domain is JBoss, an application server that provides a complete implementation of the J2EE \n[32] speci.cation, and that is architected on top of the Java platform [34]. A common architectural design \npattern employed by adminis\u00adtrators of enterprise applications is multi-tier deployment that par\u00adtitions \nthe system into independent domains, typically run using separate MRE instances (OS processes). Such \nisolation enables fault containment and modularity as well as more aggressive spe\u00adcializations in the \nMREs (e.g., using the best-performing compila\u00adtion strategy or garbage collection system for a particular \napplica\u00adtion, set of activities, or domain). J2EE-based applications typically comprise at least three \ntiers: a web container (front-end presenta\u00adtion layer), an application server (business logic), and a \ndatabase engine (back-end data source) [34, 8, 60]. Multi-tier decomposition, however, necessitates expensive \ninter\u00adprocess communication (IPC) between MREs (isolated compo\u00adnents). Since most general-purpose servers \n(e.g., web, applica\u00adtion, database) are designed for online transaction processing, in which many clients \nperform many short transactions simultane\u00adously, communication overhead can constitute a signi.cant portion \nof the observed, end-to-end response time (especially when multi\u00adple isolation units are involved). To \nreduce the overhead of MRE IPC, administrators commonly co-locate multiple tiers on a single machine. \nCo-location simpli\u00ad.es administration and con.guration, enables ef.cient use of local network communication \nfor IPC, and makes better use of multi\u00adprocessor architectures through increased thread-level parallelism. \nEmerging multi-and many-core systems are likely to make MRE co-location increasingly commonplace. Cross-MRE \nIPC mechanisms cannot depend on co-location, however, since MREs may alternatively be distributed across \ndiffer\u00adent cluster nodes or be migrated to achieve load balancing and more effective utilization of server \nresources, an increasingly important operation in virtualizing systems today [44, 47, 57]. Thus, MRE \nIPC employs high-overhead implementations of standard commu\u00adnication protocols, such as remote procedure \ncalls and object seri\u00adalization, regardless of the proximity of the communicating MREs. We introduce \nsupport for transparent and type-safe, cross-MRE communication and coordination, called XMem. XMem is \nan IPC mechanism that enables object sharing between MREs co-located on the same machine and communication \nvia extant distributed protocols when physically separated. XMem is transparent in that shared objects \nare the same as unshared objects (in terms of .eld access, synchronization, GC, and method invocation, \namong oth\u00aders), except that XMem disallows pointers from shared objects into MRE-private storage. To \nenable ef.cient object sharing, XMem manipulates virtual memory mapping to avoid indirection, i.e., all \nobject references in the system are direct. Moreover, existing com\u00admunication technologies, e.g., those \nemployed by J2EE or network sockets, can use XMem without application modi.cation. XMem guarantees type-safety \nby ensuring that the MREs em\u00adploy the same types for shared objects when the communication medium is \nshared memory. XMem is also compatible with core MRE services such as GC, dynamic class loading, and \nthread syn\u00adchronization. XMem coordinates MREs through infrequent, syn\u00adchronized global operations that \ninclude GC and class loading. In summary, we make the following contributions with this work: Improved \nintegration of MREs with the underlying system. XMem provides a new, ef.cient communication mechanism \nwhile maintaining standard, portable interaction with the lower\u00adlevel layers of the software/hardware \nstack.  Direct object sharing via isolated channels between co-located MREs isolated as distinct OS \nprocesses that avoids the trade\u00adoffs inherent to previous approaches [17, 4] by enabling com\u00admunication \nwithout serialization and data copying.  Extensions to the MRE services and abstractions, including \nparallel, cross-process class loading and garbage collection. XMem implements changes to MRE subsystems \nand system li\u00adbraries to enable transparent and ef.cient use of shared memory support when available. \n Empirical evaluation of XMem that quanti.es the reduction in response time and the increase in throughput \nin the context of the most commonly-used J2EE communication technologies, such as JDBC, JNDI, and RMI, \nas well as a database server and a web server.  In the sections that follow, we describe the necessary \nsupport of object sharing (Section 2), of multi-threading (Section 3) and man\u00adagement of the shared memory \nsegment (Section 4). In Section 5, we present the implementation details of our prototype as well as \nour experimental methodology and empirical evaluation of XMem. Finally, we contrast related work (Section \n6) and present our con\u00adclusions and our future directions in Section 7.  2. XMem: Transparent Object \nSharing The goal of XMem is to improve communication performance for enterprise-class, object-oriented, \nsoftware systems, a popular appli\u00adcation domain for web services. XMem enables transparent IPC via shared-memory \nbetween isolated MREs that are co-located on the same machine; such co-location of related processes \nis an increas\u00adingly common technique for the exploitation and better utilization of multicore systems. \nUsing XMem, MREs share objects directly to avoid the overhead that is imposed by distributed communication \nprotocols due to object marshalling and serialization. To enable direct object sharing, XMem maps the \nshared mem\u00adory segment at the same location in the virtual address space (VAS) low virtual XMem MRE \nVirtual Address Space (VAS) high virtual addresses addresses MRE and XMem App. GlobalOp Threads Thread \n Co-located MREs  MRE and XMem App. GlobalOp Threads Thread Figure 1. Co-located XMem MREs, and their \nvirtual address spaces (VAS), that are attached to a shared memory segment (gray area). The shared region \ncontains meta-data (SHM-Meta) and shared objects and is mapped at the same virtual address in each MRE. \nThe GlobalOp thread in each MRE performs infrequent global operations that XMem synchronizes across attached \nMREs. of all attached MREs. Figure 1 depicts an example instance of an XMem system. Two MREs attach to \nthe same shared mem\u00adory segment (gray area of the VAS) to share objects. We refer to the VAS of each \nMRE that is not mapped to the shared memory segment (white area of the VAS) as MRE-private. XMem systems \nshare per-instance, non-static data only static (per-class) data is MRE-private since static .elds typically \nrecord program-speci.c or MRE-speci.c state. Sharing of such .elds can violate both type safety and inter-process \nresource isolation. Since we map shared memory to the same virtual address in all MREs, objects within \nthe shared memory have the same addresses in all MREs. To guarantee memory and type safety, we disallow \npointers from shared objects to private objects via a write barrier (described further below), since \nthe address space of the non-shared areas in MREs is independent and unrelated across MREs. Regard\u00adless \nof this constraint however, XMem MREs provide services, such as class loading, GC, allocation, synchronization, \ncompila\u00adtion, uniformly for shared and MRE-private objects, i.e., XMem provides object-level transparency. \nKey to enabling such transparency ef.ciently is that (i) the internal representations of object types \n(classes) are the same across all attached MREs, and that (ii) the underlying operating system provides \nsupport for virtual memory paging and its manipulation by user-level processes (the MRE in our case). \n2.1 VAS Manipulation XMem manipulates the virtual address space to enable direct ac\u00adcess to objects as \nwell as to their class representations. Objects in most object-oriented language systems typically contain \na ref\u00aderence to an internal representation of the class (type) from which they are instantiated. This \nreference enables direct retrieval of ob\u00adject metadata for fast implementation of common operations such \nas dynamic dispatch, .eld access, and dynamic compilation. These internal representations of classes, \nhowever, are MRE-speci.c and cannot be shared, as they commonly record application-or MRE\u00adspeci.c state \nand provide access to static (private) data. Class point\u00aders, therefore, must resolve to the MRE-private \ninternal representa\u00adtion of the class. To avoid moving (reordering) existing class objects (internal \nrepresentations) within each attached MRE (which can be complex and expensive), yet to ensure that the \nsame virtual address refers to the same MRE-private internal representation of the class in all MRE 1 \n(Private VAS) SHM VAS MRE 2 (Private VAS)  Figure 2. Manipulation of VAS mapping so that class pointers \nre\u00adsolve to equivalent MRE-private class representations across at\u00adtached MREs without copying or moving, \nand while enabling di\u00adrect retrieval of object metadata (for dynamic dispatch, .eld access, etc.). Each \nbox is a virtual page (4KB in size), potentially mapped to physical memory. Blank boxes are unmapped. \nWe omit mapping lines (dotted with round ends) for classes other than K, for clarity. MREs, XMem aligns \nclass objects to virtual memory page bound\u00adaries (we assume traditional 4KB pages) and manipulates virtual \naddress mapping as depicted in Figure 2 via double mapping. In the virtual address space (VAS) of each \nattached MRE in XMem, there is a global class table (GCT) and a local class table (LCT), both of which \nare MRE-private. The LCT holds the representations of both MRE-private and global classes. LCTs across \nMREs are independent and unrelated. In contrast, the GCT in each MRE is identical in structure and layout \n(class order, count) and has the same virtual address in MRE-private space. XMem maps the physical page \nof a particular (global) class to a virtual page in both LCT and GCT, to achieve resolution of class \npointers within shared objects to private class representations with\u00adout copying or moving and without \nintroducing pointer indirection. In the example, the class pointer of unshared objects (instances of \nclass K) refers to the internal class representation in the LCT in their MRE (address 0x200 in MRE 1 \nand 0x300 in MRE 2). When the two MREs share an object of type K, XMem adds an entry for class K to the \nGCT at the same location in each MRE. Since the GCTs are identical in each MRE and start at the same \nvirtual ad\u00address, the class pointer in the shared object is the same for both MREs (0x900). We overview \nthe class loading process that makes use of this implementation in Section 4.2. There are two side-effects \nof this double-mapping. First, in the worst case, XMem consumes twice the VAS needed for classes (worst \ncase is when each MRE-private class is a globally shared class). This case is uncommon in our experience \nas the number of MRE-private classes typically far exceeds that of globally shared classes. Moreover, \nsuch VAS use is negligible for machines with large address spaces (64-bit platforms). Second, class alignment \nto virtual page boundaries limits the class size to that of a virtual page and can cause fragmentation \nin the LCT (when classes are smaller than the page size). In practice, we have never found a class object \nto be larger than our virtual page size. However, if this proves to be a limitation, we can reserve a \nmultiple of the page size for each class. In our implementation, the LCT is the permanent generation \nof the MRE which stores other long-lived data (e.g., MRE data structures, static strings) in addition \nto class objects. This data consumes part of each page which helps to reduce fragmentation. We measure \nand report the space overhead of fragmentation in Java benchmarks in Section 5.2. We plan to investigate \nthe impact of large page sizes on XMem systems as part of future work. 2.2 Shared-to-Private Pointers \nTo guarantee that shared objects never refer to private heaps (since such references are particular to \na speci.c MRE process), XMem piggy-backs on the extant write barrier implementation of generational garbage \ncollection (GC). Generational GCs are in widespread use in modern MREs as they provide superior perfor\u00admance \nwhich they achieve by exploiting similarity in object life\u00adtimes and by partitioning the heap into distinct, \ncontiguous spaces called generations [58, 62, 37]. These systems allocate most objects from the young \ngeneration, and collect this region frequently since a majority of objects die young [6, 36]. To enable \nef.cient, indepen\u00addent collection of generations, generational GCs use a write barrier at every reference \nstore in a program to track references from older to younger generations. Modern MREs typically also \nemploy a per\u00admanent generation that is rarely collected and that holds long-lived objects such as internal \nclass representations, constant strings, and MRE data structures. XMem extends write barriers with two \nchecks needed to com\u00adpare the source and destination of a particular pointer against the constant boundary \nbetween MRE-private and shared part of the heap. We need the source check for each pointer store, and \nthe des\u00adtination check only for stores to the shared memory. If a program makes an assignment that violates \nthe XMem constraint, the run\u00adtime throws an exception and the instruction fails. Since we map the shared \nmemory segment to the same location in each MRE and the segment has a .xed size, this check is very ef.cient: \nit consists of a register and constant comparison. Such checks impose negligi\u00adble overhead on modern \narchitectures because there is no memory access and the branch direction is typically highly biased and \nthus, easily predictable. 2.3 Using XMem Developers make use of XMem via a simple application program\u00adming \ninterface (API). The XMem API for Java comprises the fol\u00adlowing public static methods declared in the \nipc.SharedMemory class: void sharedModeOn(); boolean isSharedModeOn(); void sharedModeOff(); boolean \nisShared(Object o); Object accept(int p); void connect(int p, Object o); void bind(int p); Object copyToShared(Object \no); To support transparency and backward-compatibility, programs within XMem allocate objects using \nthe conventional new operator, regardless of whether they are allocating shared or private memory. XMem \ndetermines from which region (shared or private) to allocate using a per-thread allocation mode. Initially, \nthe allocation mode is private. Programs or libraries change the allocation mode explicitly via the sharedModeOn \nand sharedModeOff methods. The system throws an ipc.SharedMemoryException to prevent shared-to\u00adprivate \npointers as well as signal binding/connection failures and out-of-memory errors. XMem makes use of the \nconcept of ports to enable co-existence of multiple, isolated communication channels in a single shared \nmemory segment. To initiate communication, two distinct MREs (to which we refer as a client and a server) \nmust obtain a refer\u00adence to a shared object (to which we refer as a root). A client al\u00adlocates a root \nin shared memory and passes a reference to it to the connect method along with a port to which a speci.c \nserver has been bound via the bind method. The server retrieves the root from the accept method. Once \nthe root is exchanged, further communi\u00adcation proceeds according to an application-speci.c protocol which \ncommonly includes monitor synchronization (wait/notify) on the root. Objects shared through a particular \nchannel are reachable only to threads/MREs that have established the connection. However, an arbitrary \nnumber of threads/MREs can share a speci.c object if a server makes a reference to a shared object available \nto multiple clients (which use distinct channels for communication with the server). To enable interoperability \nwith libraries that do not guarantee immutability of the objects they take as arguments, XMem provides \na mechanism for recursive (deep) copying of objects into the shared memory via the copy method. Object \ncloning, commonly available from the underlying language (e.g., Java or C#) platform, by default creates \nshallow object copies and must be overridden on a per\u00adapplication basis to support deep coping. XMem \nprovides this general service uniformly across applications. XMem uses stack\u00adbased, depth-.rst copying \nand handles cycles in the object graph by maintaining a hash table that maps the already-visited objects \nto their copies. We describe how such copying to shared memory interacts with shared-memory garbage collection \nin Section 4.3. Although, in this work, we focus primarily on shared memory, other IPC mechanisms such \nas signals and message queues can be built on top of XMem in a straightforward way. We have integrated \nXMem, through the use of its API, into existing communication mechanisms, such as RMI, applying only \nminimal library modi.\u00adcations. Such XMem-aware implementations provide two paths of execution that the \nlibrary routine selects based on the proximity of the communication target: one that employs shared memory \nand one that uses traditional distributed communication.  3. XMem Runtime Support To enable cross-MRE \nobject sharing, XMem extends the MRE multi-threading implementation by adding dual mode (shared or private) \nobject allocation and support for cross-process synchro\u00adnization based on shared object monitors. XMem \nautomatically preserves the guarantees provided by the memory consistency model of a speci.c MRE (e.g., \nthe Java Memory Model [43]) since the system consists of homogeneous MREs. 3.1 Dual Mode Object Allocation \nXMem extends the common allocation technique of thread-local allocation buffers (TLABs). TLABs are used \nby modern MREs to reduce contention between threads that concurrently perform lin\u00adear (bump-pointer) \nallocation from a common area. This approach requires no synchronization when allocating within a TLAB. \nThe system allocates TLABs to threads linearly, using more expensive atomic operations. XMem associates \ntwo TLABs with each appli\u00adcation thread, one in private and one in shared memory. We do not initialize \nthe latter until the thread performs its .rst allocation into shared memory, e.g., when it .rst executes \na new bytecode within the XMem shared mode (sharedModeOn()). XMem excludes ob\u00adjects that the system creates \nby side-effect of other operations, such as class loading or lazy data structure initialization, from \nallocation in shared memory to prevent unintended object leaks. XMem also uses private mode for allocation \nof all internal data structures (data commonly stored in a permanent area of the heap).  3.2 Thread \nSynchronization Two locking schemes are commonly used to implement language\u00adlevel (e.g., Java) monitors \nin extant MREs: lightweight locking [52] and biased locking [52]. Biased locking optimistically assumes \nthat a single thread uses a monitor (i.e., there is no contention); when this proves not to be the case, \nbiased locking falls back to lightweight locking. Both lightweight and biased locking require a re-design \nto work with shared memory. XMem adapts and employs lightweight locking since it is the basis for both \nschemes. We .rst overview lightweight locking and then describe its implementation in XMem. Lightweight \nLocking. To avoid using OS primitives (a pair con\u00adsisting of a mutex and a condition variable) in the \ncommon case of uncontended locking, lightweight locking employs atomic compare-and-swap (CAS) operations. \nOnly when two threads at\u00adtempt to lock the same object does the MRE in.ate the lightweight lock into \na heavyweight OS-backed monitor. Lightweight locking improves performance as user-mode locking is signi.cantly \nmore ef.cient than system calls. The MRE stores basic locking information in the object header which \noccupies one machine word. The lowest two bits encode one of the three possible states: unlocked (UL), \nlightweight-locked (LL), and heavyweight-locked (HL). When an object is LL (by a monitorenter bytecode), \nthe system inserts a lock record into the stack of the thread performing the lock acquisition operation. \nDuring stack unwinding (which takes place when an exception is thrown), the system uses lock records \nto unlock the objects that are locked in the discarded stack frames. Normally, objects are unlocked by \na monitorexit bytecode generated as part of the epilogue of block-structured critical sections. Each \nlock record holds a pointer to the locked object and the original value of the overwritten object header. \nDuring locking, a thread attempts an atomic CAS on the object header to replace it with a pointer to \nthe stack-allocated lock record. Lock records are word-aligned, therefore the two lowest bits are always \ncleared and do not con.ict with the locking state bits kept in the header. If the CAS succeeds, the thread \nowns the monitor. Otherwise, a slow path is taken and the lock is in.ated the object header is CAS-updated \nto point to a data structure containing a mutex and a condition variable. This data structure is stored \nin private, MRE-managed, memory. During the unlock operation of an LL object, a thread tries to CAS-restore \nthe header that it has stored on the stack. On success, no fall-back is needed and the fast path is complete. \nThe CAS fail\u00adure indicates that the lock was contended for (and in.ated) while it was held. Under such \ncircumstances, it is necessary to notify the competing (and now waiting) threads that the object is unlocked. \nThese threads are blocked on the condition variable. When awak\u00adened, they unlock the mutex and resume \nexecution by trying to re\u00adacquire the mutex. The mutex and the condition variable are multi\u00adplexed here: \n.rst they are used to wait until the LL object becomes unlocked and then they are used in a standard \nway to provide mu\u00adtual exclusion along with the wait/notify functionality. Recursive locking in the lightweight \ncase is based on implicit lock ownership if the object header points into the stack of the current thread \nthen the current thread already owns the lock and in a new lock record on the stack the header .eld is \nset to NULL.When unlocking, a lock record with the NULL header .eld is ignored. Recursive locking in \nthe heavyweight case uses a counter located in the aforementioned MRE data structure. Lightweight Locking \nin XMem. The challenges to lightweight locking in XMem shared memory are three-fold. First, the header \nof an LL object points into a private thread stack. Such refer\u00adences cannot be interpreted properly across \nMREs directly. Sec\u00adond, heavyweight data structures allocated in MRE-private mem\u00adory must now be accessible \nto other MREs. Finally, POSIX syn\u00adchronization primitives by default work within a single process. To \naddress these issues, XMem allocates a lock data structure (LDS) in shared memory, both in case of lightweight \nand heavy\u00adweight locking and uses POSIX object attributes to enable cross\u00adprocess synchronization. LDS \nreserves space for a mutex and a conditional variable (which are initialized only in case of in.a\u00adtion). \nLDS contains a process identi.er (PID) and a thread identi.er (TID), that together unambiguously identify \nthe owner, as well as a recursion count, the locked object reference, the binary .ag used for mutex/condition \nvariable multiplexing, and the original object header. Lock records that are stored on the stack contain \nonly the address of the locked object. An object header, instead of pointing into a stack, always refers \nto the corresponding LDS, when locked. XMem maintains an LDS pool in SHM-Meta (metadata area in shared \nmemory). Application threads atomically bulk-allocate mul\u00adtiple LDSes at once from the global pool to \nreduce synchroniza\u00adtion overhead. Each thread holds several LDSes in a local queue with a FIFO discipline. \nAn LDS of an LL object is returned to the thread-local queue when unlocking succeeds (i.e., no contention \nis detected). An in.ated LDS can be freed only during shared mem\u00adory GC when the HL shared object becomes \nunreachable. Invoking wait/notify on an LL object results in the lock in.a\u00adtion. This is necessary as \nthese operations require support from the OS. An important aspect of LL is the hash value computation. \nMREs typically store the hash value in the object header and lazily initialize it. The hash code, once \ncomputed, should never change. Since LL displaces an object header, a race condition arises when an LL \nobject is simultaneously unlocked and its hash code is being initialized. Such circumstances force lock \nin.ation and safe ini\u00adtialization of the hash code (in.ated locks are more stable as their unlocking \ndoes not change the object header). XMem uses this modi.ed LL scheme only in the shared mem\u00adory each \nMRE uses the original scheme internally as it is more space ef.cient. Each lock/unlock operation checks \nwhether the cor\u00adresponding object is shared or not and dynamically applies the ap\u00adpropriate locking scheme. \n  4. Shared Memory Segment Management Each XMem MRE executes a Global Operations (GlobalOp) thread that \nperforms .ve coordinated global operations: attachment, de\u00adtachment, connection, class loading, and garbage \ncollection (GC). XMem serializes these, relatively rare, operations using a global lock (a mutex and \na condition variable located in the shared mem\u00adory). The system performs every global operation in parallel \nby all currently attached MREs using the GlobalOp thread in each MRE. Since MRE attachment and detachment \nare global operations, there is a well-de.ned set of attached MREs with respect to the current global \noperation. This is an important property, as global operations terminate only when all attached MREs \nreport operation comple\u00adtion. With the exception of GC, global operations execute concur\u00adrently with \napplication threads (i.e., without stopping them). 4.1 Attachment, Detachment, and Connection Two JVM \nproperties, ipc.shm.file and ipc.shm.destroy, control MRE-OS interaction. The .rst one identi.es a shared \nmemory segment to create or attach to (we employ Linux Sys\u00adtem V IPC [49]). The second one speci.es if \nan MRE should mark the segment for destruction upon termination. The OS re\u00adleases only marked segments \nwhose attachment count reaches zero. Upon startup, each MRE attempts to create a new shared memory segment. \nThe creation process fails if the segment already exists, which causes a fall-back to attachment. The \nMRE that succeeds in segment creation, initializes the shared data structures (located in SHM-Meta). \nMREs that attach/detach to/from an existing segment perform a global attach/detach operation. Attachment \ntakes place after com\u00adpleting the MRE bootstrap procedure and before invoking the pro\u00adgram s main method. \nDetachment is performed upon program ter\u00admination. These two global operations are automatic and not \nacces\u00adsible via the XMem API. An MRE can attach only to one segment at a time. However, XMem supports \nmultiple communication chan\u00adnels over a single shared memory segment. A con.guration with a single segment \nper host is most memory-ef.cient but multiple seg\u00adments can be used if needed. Attach and detach operations \nupdate a global counter that tracks the number of attached MREs. The connection operation establishes \na communication channel. Connection allows two MREs to obtain a reference to a shared object while guaranteeing \nprivacy (other MREs cannot reach that shared object). It implements semantics similar to that of a network \nsocket. The arguments passed to the connect (i.e., a port number and a shared object), are propagated \nto other MREs as parameters of the global operation. Each MRE maintains a list of ports to which it is \nbound. When a connection request to a locally bound port is detected, an MRE adds the corresponding shared \nobject to a local queue and awakens the threads that are blocked on the accept call on the port. The \nshared object is then dequeued and returned by the accept method. XMem ensures that only one MRE is bound \nto any port (an atomically-updated boolean table is kept for this purpose in the shared memory). Since \nconnection is a global operation, it is serialized with respect to GC, and as a result, the shared object \n(root) has a stable location while the operation is in progress. 4.2 Global Class Loading Through global \nclass loading, XMem ensures that a speci.c class is privately loaded by, and is the same in, all attached \nMREs, to guarantee type safety. XMem implements the latter by comparing the 160-bit SHA-1 hash value \ncomputed for the class bytecode, across MREs. If an MRE encounters a bytecode mismatch, global class \nloading fails and an exception is thrown. Since XMem places no restrictions on MRE-private class load\u00ading, \nthe class of a shared object may or may not be loaded in all at\u00adtached MREs when it is instantiated in \nshared memory. Therefore, following each object allocation, XMem executes a class loading barrier which \nchecks if the new object resides in the shared mem\u00adory. If the object is shared, the MRE checks whether \nits class has been loaded globally. To make this check fast (note that it is done for each allocation), \nXMem adds a .eld (a forwarding pointer) to each private class object. The forwarding pointer is initially \nset to NULL to indicate that the class is loaded only privately. After global class loading, the forwarding \npointer is set to the GCT address of the class. Following each allocation in shared memory, the MRE updates \nthe class pointer of the new shared object to the forwarding pointer. If the check fails, i.e., the class \nof the new shared object has not been loaded globally, the MRE initiates global class loading. Global \nclass loading uses the default system class loader (which corresponds to the CLASSPATH variable). XMem \npermits classes de.ned by user-de.ned class loaders to be instantiated in the shared memory as long as \nthe corresponding user-de.ned class loaders are themselves allocated in the shared memory. However, even \nthough class loaders can be shared, the internal class representations are al\u00adways MRE-private. XMem \nrelies on MRE-private class loader con\u00adstraints to guarantee type safety in the presence of lazy class \nload\u00ading, user-de.ned class loaders, and delegation [39]. No extension is needed because we .rst locally \nload all globally loaded classes and thus, local constraints are always a superset of global constraints. \n 4.3 Global Garbage Collection Global GC in XMem identi.es and reclaims dead, shared objects (i.e., those \nthat are not reachable from any attached MRE). The GC is initiated by one of the attached MREs when allocation \nof a new TLAB in shared memory fails. In order to interoperate with different GC algorithms and heap \nlayouts [63, 37], XMem provides a generic mechanism for identifying root objects in the shared memory. \nRoot objects in this context are objects directly reachable from one or more MREs by following pointers \nthat are located on thread stacks, in registers, or in the live part of a private heap. Once a snapshot \nof the root objects is obtained, shared memory can be collected in a conventional way using any tracing \ncollector. The key challenge is in identifying the root objects without re\u00adsorting to scanning all the \nlive objects in each MRE. Note that pointers into shared memory can be scattered across all genera\u00adtions. \nAt the same time, we can expect the number of such pointers to be relatively small. XMem identi.es roots \nby piggy-backing on a fast minor col\u00adlection (the one con.ned to the young generation). To enable this, \nXMem extends a card table mechanism [64] that supports gener\u00adational GC so that it tracks pointers from \nolder generations that point into the young generation or into shared memory. As a result, a young generation \ncollection is able to detect all root objects that originate from a given MRE without an exhaustive scan \nof older generations. For each global GC, XMem triggers a minor collec\u00adtion in the attached MREs. To \nperform a minor GC, state-of-the-art MREs typically employ a parallel copying collector [23] that is \nex\u00adecuted in a stop-the-world (STW) fashion as it imposes very short pause times (i.e., concurrent collection \n[19, 48] is not necessary). An XMem system implements global GC of the shared mem\u00adory segment using STW \nparallel copying collection. All attached MREs perform GC in parallel, each contributing multiple GC \nthreads. MREs synchronize only before and after collection. A full barrier is needed after all MREs reach \na safepoint (i.e., state where application threads are suspended) because one cannot start mov\u00ading the \nshared objects while other MREs are actively using them. For similar reasons, all GC threads from all \nMREs synchronize when leaving a safepoint. Any additional coordination depends on the GC algorithm used. \nAlthough, global GC stops all MREs, it is not unscalable or deadlock-prone since bringing an MRE to a \nsafepoint is a low-delay operation robust with regard to I/O. Since global GC can interrupt an XMem deep \ncopy from private to shared memory, we must be careful to avoid introducing tempo\u00adrary shared-to-private \npointers during the copy process. To this end, when we copy an object to its new location, we clear its \nreference .elds (as they may still point to private objects). We update these .elds with the correct \nvalues (new locations) when we copy the cor\u00adresponding objects to shared memory. Global GC needs to update \nthe entries in the stacks and hash tables used by XMem copy oper\u00adation because it is moving objects. \nWe provide a new object header to each shared memory replica to preclude them from inheriting the synchronization \nstate of original objects. Non-global GCs (both minor and major) do not follow pointers that point into \nthe shared memory. Because of the XMem invariant that no shared-to-private pointers are allowed, it is \ncorrect to stop tracing when a shared object is encountered. GC completeness is preserved because scanning \nof objects in the shared memory cannot lead to the discovery of any additional live objects in the private \nheap. Local GC performance, is thus the same regardless of the number of objects in shared memory. The \nmost suitable GC algorithm for shared memory collec\u00adtion depends on the demographics and total size of \nthe live shared objects [36]. If XMem is used to share a large amount of long\u00adlived data, then compacting \ncollectors are most appropriate. On the other hand, if the primary purpose of XMem is communication be\u00adtween \nstrongly isolated MREs, then copying collection is a better choice [62]. This is because the communicating \nMREs exchange a small number of objects which exhibit relatively short lifetimes. Generational collection \ncan be used to support a wide range of ob\u00adject lifetimes. To accommodate short-lived communication behav\u00adior \ntypical of J2EE applications, we implement a non-generational, parallel copying in our XMem prototype. \nParallel copying collectors employ several GC threads to evac\u00aduate live objects from the currently-used \nsource space(s) to the currently-unused target space [28]. Since most objects are expected to be unreachable, \nthe target space is typically smaller than the source space(s) and the worst-case scenario is handled \nby falling back to the promotion of over.ow objects into older generation(s). In the absence of a generational \nheap layout, half of the space needs to be set aside as a copy reserve. XMem employs two equal-sized \nsemi-spaces in the shared memory and the collection of the source semi-space is performed in parallel \nby all attached MREs. This process is interleaved with local minor GCs so that the object graph is traversed \nonly once. Each MRE uses multiple GC threads, which correspond to schedu\u00adlable kernel threads and whose \ntotal number equals the number of processors/cores available or dedicated to each MRE. XMem employs a \ntwo-level load balancing scheme in the form of work stealing [23]. GC threads that become idle attempt \nto steal object references from non-empty marking stacks of other GC threads. Each GC thread is associated \nwith two marking stacks, which we refer to as the local and shared stack. Intra-MRE load balancing is \nlimited to local stacks while inter-MRE work stealing uses shared stacks only. MREs push references to \nobjects residing in the shared memory onto the shared stacks to make them available to other MREs. Local \nload balancing is preferred and global steal\u00ading is done only when all local stacks become empty. The \nstealing target (i.e., the marking stack/stack entry) is selected randomly. Global GC is an STW operation \nthat comprises three barriers: prologue, epilogue, and GC termination. The GC prologue .ips the semi-spaces. \nThe GC epilogue forwards the pointers in SHM-Meta and de.ates heavyweight monitors associated with dead \nobjects. To ensure that each live object is processed exactly once, GC threads claim objects atomically. \nAtomic CAS instructions are sup\u00adported by most processors and can be used across processes (as they are \nbased on physical rather than virtual addresses). To reduce contention, each GC thread owns a parallel \nlocal allocation buffer (PLAB) where it copies the objects it has claimed. We allocate PLABs linearly, \natomically, and on-demand, from the target semi\u00adspace. The GC .rst copies an object to its destination, \nand then pushes the addresses of its reference-type .elds onto the marking stack (local and/or shared). \nThen, a GC thread tries to CAS-forward the original object header to the new location. If a thread loses \na race to another thread, the GC removes the object from the PLAB and pops the new pointers off the stack. \nThis order of operations is motivated by fault-tolerance (Section 4.5). 4.4 Global Meta-Data Management \nThe SHM-Meta data structures support the runtime and global op\u00aderations of XMem. They include a descriptor \nfor the current global operation, which contains the operation code, its input arguments, barrier counters, \nstate .ags, and a mutex and condition variable with which the system serializes the execution of global \noperations. In addition, SHM-Meta holds the marking stacks for global GC and a table that records the \nmeta-information for all globally loaded classes including the class name, de.ning class loader (set \nto NULL if the default system class loader is used), and a bytecode hash for type-safety veri.cation. \nSHM-Meta also holds a list of the bound ports that are currently in use for communication sessions between \nco-located MREs. Finally, SHM-Meta contains single-word entries for (i) the number of attached MREs, \n(ii) the number of globally loaded classes, (iii) the boundaries of and current position in the shared \nheap (for allocation of new TLABs), and (iv) the start and end of a pool of global locks that enable \ncross-MRE monitor syn\u00adchronization. 4.5 Fault Tolerance XMem tolerates unexpected MRE termination, between \nand during global operations, by implementing a timeout mechanism (based on the pthread timed wait on \na condition). If an MRE fails, the next global operation times out. Upon timeout, XMem subtracts the \nnumber of not-responding MREs from the counter of the attached MREs and releases any shared locks that \nwere held by the termi\u00adnated MRE. Connection, and class loading are global operations that do not require \nany additional handling upon timeout. Bench- Generation Size [MB] Execution Number of GCs Number of XMem \nOverhead mark Young+Old Permanent Time [s] Minor Major Classes Time [%] Space [MB] bloat 40 5 55.3 \u00b1 \n0.5 528 \u00b1 2 1 \u00b1 0 827 3.5 2.24 pmd 34 6 19.5 \u00b1 0.1 495 \u00b1 1 8 \u00b1 1 1186 3.5 2.94 xalan 42 6 49.2 \u00b1 0.3 \n1480 \u00b1 8 107 \u00b1 4 1179 3.4 3.11 antlr 8 4 4.5 \u00b1 0.1 380 \u00b1 1 5 \u00b1 0 679 2.8 1.79 chart 30 9 15.7 \u00b1 0.1 355 \n\u00b1 7 9 \u00b1 0 1440 1.9 3.74 eclipse 68 16 66.5 \u00b1 0.2 551 \u00b1 3 11 \u00b1 0 2627 2.3 7.04 hsqldb 336 5 13.6 \u00b1 0.1 \n9 \u00b1 0 5 \u00b1 0 736 0.3 2.00 fop 20 6 2.0 \u00b1 0.0 19 \u00b1 0 0 \u00b1 0 1423 2.8 3.45 luindex 8 4 7.2 \u00b1 0.1 260 \u00b1 8 \n3 \u00b1 0 689 1.6 1.83 lusearch 18 4 8.6 \u00b1 0.1 706 \u00b1 1 1 \u00b1 0 683 2.6 1.79 jython 8 8 43.1 \u00b1 0.3 3539 \u00b1 2 \n1 \u00b1 0 1325 3.0 3.14 jbb/6wh 476 8 90 \u00b1 0.0 149 \u00b1 0 4 \u00b1 0 1296 0.64 3.59 jbb/8wh 636 8 90 \u00b1 0.0 116 \u00b1 \n1 3 \u00b1 0 1296 1.78 3.59 jbb/10wh 780 8 90 \u00b1 0.0 98 \u00b1 0 3 \u00b1 0 1296 0.82 3.59 Table 1. The overhead introduced \nby XMem in terms of application throughput (Jbb) or execution time (Dacapo) and occupancy of the permanent \ngeneration. For each benchmark we report generation sizes, execution time (note that Jbb runs for a .xed \nperiod of time), the number of minor/major collections, and the number of loaded classes. In case of \ntimed-out detachment and attachment operations, the system needs to determine whether it was the detaching/attaching \nMRE that failed (to correctly keep track of the number of live MREs). This is done based on the PID of \nthe process which initi\u00adated attachment/detachment (XMem sends a signal using the kill system call and \ngets an error if the process is dead). GC requires more complex handling, as the shared stacks of a terminated \nMRE can contain pointers stolen from other MREs. These stacks are located in shared memory so they are \nnot lost and can still be processed. During GC, objects are forwarded to their new locations only when \nthey have been copied and when their content has been scanned (and pushed onto a stack). Therefore, copying \ncollection can be interrupted at any time without losing correctness, provided that whatever is on the \nstack(s) is eventually processed. If a global GC times out, it is suf.cient to empty all the marking \nstacks located in the shared memory.  5. Implementation and Evaluation We have implemented XMem in \nHotSpot [46], an open-source, high-performance JVM written in C/C++. The heap in HotSpot [28] comprises \nthree generations: young (where new object allocations take place), old (where long-lived objects are \npromoted), and per\u00admanent (where classes are stored). HotSpot reserves two words per object. The .rst \nword (the header) contains the locking state, age bits, and the hash code. The second word is a pointer \nto a class object located in the permanent generation. Class objects encapsu\u00adlate static .elds, a virtual \nmethod table, a class loader reference, and pointers to other meta-objects that describe methods and \n.elds (among others). The PTHREAD PROCESS SHARED attribute is set on the POSIX mutexes and condition \nvariables to enable cross-process synchro\u00adnization. To create or look up a shared memory segment, XMem \nemploys shmget. This system call is used with the IPC PRIVATE key to implement double mapping in LCT \nand GCT. Global shared memory segments are identi.ed by a key generated by ftok based on a .le name. \nXMem supports multiple global segments on a sin\u00adgle host, differentiated by a .le name (the JVM ipc.shm.file \nproperty). We implement attachment with shmat, which allows to specify a virtual address that a segment \nis mapped to. MRE\u00adprivate memory is allocated using mmap, which is called with the MAP FIXED .ag when \npinning GCT at a speci.c location. For atomic operations we use the x86 cmpxchg instruction. LCT corre\u00adsponds \nto the permanent generation.   5.1 Methodology Our experimental platform is a dedicated machine with \na dual\u00adcore Intel Core 2 Duo (Conroe B2) processor clocked at 2.66GHz, equipped with 4M 16-way L3 cache, \n32K 8-way L1 cache, 2GB main memory, and running Debian GNU/Linux 3.0 with the 2.6.17 kernel. We use \nthe HotSpot OpenJDK [46] v7-ea-b18 (Aug. 2007) compiled with GCC 3.2.3 in the optimized client-compiler \n(C1) mode. This version of HotSpot implements a highly-optimized, state-of-the-art serialization mechanism \nand uses standard (not process-shared) mutexes/condition variables. For our experiments, we employ standard \ncommunity bench\u00admarks from the Dacapo [18] and SPECjbb2005 [55] suites to eval\u00aduate the impact of XMem \non programs that do not communicate across MREs. We use the large input for Dacapo and 6, 8, and 10 warehouses, \nwith 90s runs, for Jbb. To evaluate the impact of us\u00ading shared memory, we develop a number of benchmarks \nourselves (an approach taken in [41] in a similar context), which exercise shared memory and implement \nthe J2EE communication protocols. We describe these benchmarks with each experiment. We evaluate XMem-aware \nimplementations of RMI and CORBA, serialization and XML, JNDI, and TCP/IP sockets. Finally, we evaluate \nXMem for two server-side applications: Hsqldb [29] and Tomcat [1]. In all experiments, there are 2 MREs \nand the shared memory size is 30MB. Whenever running the original HotSpot JVM, we set the young generation \nto 30MB. 5.2 XMem Overhead To investigate the overhead introduced by adding support for XMem, we compare \nthe performance of shared-memory-oblivious applications run on top of unmodi.ed HotSpot JVM against our \nim\u00adplementation of XMem. Table 1 summarizes the results. For each benchmark, we employ a heap size (i.e., \ntotal size of the young and old generation) of twice the minimum (the same methodology that is used in \n[56]). We employ this methodology to ensure some GC activity without having GC dominate performance \nso that we are able to measure other sources of overhead potentially introduced by XMem. We set the permanent \ngeneration size to the minimum required for XMem to load all the necessary classes. The young generation \nconstitutes one fourth of the heap. We report genera\u00adtion sizes, the number of minor and major GCs, and \nthe number of loaded classes. For timings, we execute 5 warm-up runs then compute the average and standard \ndeviation of the next 5 runs. XMem imposes negligible time overhead which we express as the percentage \nof total execution time (for DaCapo) or throughput ing a speci.c number of nodes. We report average GC \npause times Average GC Time [ms] 60 50 40 30 20 10 0 Figure 3. Global GC pause times with and without \ninter-MRE load for different live data sizes. Global GC latency (computed by ex\u00ad trapolating GC pause \ntime for live data size equal to zero) is 0.9ms. Safepoint latency in a single MRE is 0.7ms on average. \nSafepoints are reached concurrently by two MREs (they do not add up). Thus, there is 0.2ms overhead imposed \nby XMem to coordinate global GC across MREs. Copying throughput is 3.3 million nodes/second (where each \nnode corresponds to 5 small objects). This throughput is identical in case of a single MRE (XMem does \nnot degrade it). 5.4 Communication Ef.ciency for Microbenchmarks We next evaluate the impact of XMem \non the performance of Java communication technologies using our microbenchmarks. RMI and CORBA. RMI [51] \nenables inter-MRE type-safe remote method calls. A server registers a remote object using a direc\u00adtory \nservice which is later consulted by the client to look up the remote object by name. Once a remote reference \n(proxy) is con\u00adstructed, the client can call remote methods. A client and a server balancing for different \ndistributions (percentage) of shared objects use automatically-generated stubs and skeletons to (de)marshall \nar\u00adreachable from individual MREs. guments and return values. CORBA [14] employs a more portable  transport \nprotocol (IIOP) to interoperate with other runtimes. Our microbenchmark times a remote method call that \ntakes a binary tree of objects as an input argument and returns another binary tree as an output value. \nWe employ binary trees as the microbenchmark since they represent a middle-ground in common data structures: \nthey are neither sparsely-connected (like linked lists) nor densely\u00ad connected (like complex graphs). \nFigure 5(a) shows the average invocation time (y-axis) for an increasing number of nodes in the binary \ntree (x-axis). We im\u00ad plement the remote call using XMem by allocating binary trees directly in the shared \nmemory. Client-server interaction is coor\u00addinated by monitor synchronization. Having allocated a tree, \nthe client noti.es the server that the input is ready. Once the server al\u00ad 0 25 50 75 100 125 135 locates \nthe output tree, the client is noti.ed that the call is complete. Live Data [thousand of nodes] Live \nData [node] Figure 4. Impact of the size of live shared objects on global GC XMem reduces latency 15x \nand 37x while increasing throughput (calls/second) 6x and 35x, compared to RMI and CORBA, respec\u00adtively, \nsince XMem avoids argument marshalling and network com\u00ad pause times. We present two views of the same \ngraph to show both munication. copying throughput and global safepoint latency. (for SPECjbb2005). The \nsources of overhead are two additional checks per write barrier and internal checks for whether or not \nan Serialization and XML. Object serialization [54] provides a type\u00adsafe mechanism for transforming \nan arbitrary graph of objects im\u00adplementing the java.io.Serializable interface into a binary byte stream \nwhich then can be used to reconstruct the original data object is shared. We report absolute values for \nthe space overhead structure. A runtime-portable alternative to binary representation introduced in the \npermanent generation (by the page alignment is XML. We compare default and XML-based serialization against \nimplementation) as this overhead does not depend on generation their XMem implementation. Our microbenchmark \ntimes the ex\u00adsizes (only on the number of loaded classes). The space overhead change of an object graph \nbetween a server and a client. A client ranges from 1MB to 7MB and on average is 3.1MB across the allocates \na binary tree of objects, serializes it and sends the result to 14 programs. This overhead is bounded \nby the meta-data size (as the server over a socket. The server deserializes the tree, allocates a opposed \nto the application working set size). response (being a binary tree of the same size) and sends it back \nto 5.3 Global GC Performance Figure 3 shows the impact of inter-MRE GC load balancing (work stealing) \non average pause times of global GC. In this experi\u00ad the client in a serialized form. In XMem, we allocate \nthe tree in the shared memory and notify the other side that the data is ready (we consider the overhead \nof copying below). Figure 5(b) presents the average serialization time (in msecs on y-axis) for a tree \nof 1 1024 ment, each MRE executes a single GC thread and can reach only nodes (x-axis). XMem eliminates \nthe need for serialization and data a speci.c fraction of shared objects. We express the distribution \ntransfer and thus improves throughput (calls/second) 20x and 391x of reachable objects (imbalance) as \na pair of percentage values. while reducing latency by around 7000x compared to default and For perfect \nbalance (50/50), load balancing adds a small overhead. XML serialization. For the most imbalanced con.guration \n(100/0), inter-MRE work stealing reduces GC pause time by 44%. We report average GC pause times (and \nstandard errors) from 15 GCs. This result indi\u00adcates that cross-MRE load balancing is important for ef.cient \nGC in an XMem system. XMem implements STW parallel copying collection and there\u00adfore its GC pause times \nincrease linearly with live data size. Figure 4 presents measurements obtained using two MREs, each with \na single GC thread, where live data consists of a binary tree compris-JNDI. JNDI provides access to directory \nservices, such as LDAP or RMI registry, where clients can look up objects by name as well as evaluate \nsearch queries. Our microbenchmark .rst binds a speci.c number of objects in an RMI registry and then \nperforms a query that lists all available bindings (name/object pairs). We time the latter operation \nonly as it is more important (directories are rarely modi.ed). XMem keeps the bindings in the shared \nmemory and returns an enumeration of their subset in response to each query. 90 80 70 60 50 40 30 20 \n10 0  0 200 400 600 800 1000 1 3 5 0 2000 4000 6000 8000 1 3 5 (a) Remote method invocation time (ms) \nfor (b) Object serialization time (s) for client/server binary tree pass/return (x-axis is node count). \nbinary tree send/receive (x-axis is node count). 1.6 0.6  0.015 0.05 1.4 0.5 0.012 0.04 1.2 0.4 1.0 \n0.009 0.03 0.8 0.3 0.006 0.02 0.6 0.2 0.4 0.01 0.003 0.1 0.2 0.0 0.00 0.0 0.000 0 200 400 600 800 \n10001 3 5 0 2000 4000 6000 8000 1 3 5  (c) Object lookup time when a directory server returns (d) Data \ntransfer time (ms) for client/server array a number (x-axis) of name/object pairs (bindings). send/receive \n(x-axis is array size in bytes).  Figure 5. Microbenchmark communication performance. We blow up the \naxes using a second graph snapshot to make latency visible. We include regression lines on each graph. \nThis enumeration is allocated in the shared memory and returned to the client by means of a noti.cation. \nFigure 5(c) shows the average results gathered for a varying number of bindings (1 1024). XMem reduces \nlatency 32x and increases throughput (lookups/second) 240x which can be attributed to copy and transfer \navoidance. TCP/IP Sockets. Network sockets operate at the byte level (as op\u00adposed to the object level) \nand therefore have no notion of type\u00adsafety. However, we compare their ef.cacy with XMem for com\u00adpleteness. \nOur microbenchmark measures the time needed to trans\u00adfer a byte array of a certain length from a client \nto a sever and vice versa using TCP/IP sockets. We implement XMem-based commu\u00adnication by allocating \na shared byte buffer. Each party writes into the shared buffer and then noti.es its peer that the new \ndata is avail\u00adable. Figure 5(d) compares the transfer time (in ms) using conven\u00adtional sockets for data \nsizes 1 to 8192 bytes (x-axis). XMem in\u00adcreases throughput and decreases latency both by 2x by avoiding \nnetwork stack interposition and redundant data copying. Copying Overhead. Occasionally, an object graph \nneeds to be copied to the shared memory to ensure full transparency of com\u00admunication. In case of remote \nmethod invocation and object seri\u00adalization this translates to allocating locally and then copying an \nobject tree to the shared memory just before noti.cation. In case of sockets, two copies are necessary \nin the worst case: .rst from a local buffer (client side) to a shared buffer and then from the shared \nbuffer to a local buffer (server side). Since bindings in directory services are immutable, it is suf.cient \nto copy only the enumera\u00adtion object encapsulating the query result while leaving the bind\u00adings intact. \nTable 2 shows the impact of copying on latency and throughput. We report relative performance of XMem \nwith copy-Bench- Latency Throughput mark vs. HS vs. XMem vs. HS vs. XMem RMI 2.5x 5.8x 2.4x 2.4x CORBA \n6.4x 5.8x 14.3x 2.4x Serial. 1152x 6.1x 8.3x 2.4x XML 1204x 6.1x 164x 2.4x JNDI 6.9x 4.6x 71x 3.4x Socket \n2.2x 1.1x 1.5x 1.6x    Table 2. Impact of copying shared data (so that both client and server operate \non their own instance) on latency and throughput. Columns 2 and 4 show these metrics for XMem with copying \nvs. HotSpot and columns 3 and 5 show these metrics for XMem without copying vs. XMem with copying. ing \nto existing technologies run on top of HotSpot (HS) and the non-copying version of XMem. Columns 2 and \n4 show latency and throughput for XMem with copying vs. HotSpot and columns 3 and 5 show these metrics \nfor XMem without copying vs. XMem with copying. When copying is used, XMem still signi.cantly outper\u00adforms \nthe extant mechanisms (Columns 2 and 4). 5.5 Application Performance We next evaluate the impact of \nXMem on the performance of two enterprise applications. We quantify the improvement in user\u00adperceived \nthroughput and response time by comparing an unmodi\u00ad.ed database server (Hsqldb) and a web server (Tomcat) \nwith their XMem-based variants. Hsqldb [29] is a relational SQL database management system that supports \nin-memory and disk-based data storage. JBoss uses an 5 0.10 4 0.08 3 0.06 2 0.04 1 0.02 0 0.00 0 200 \n400 600 800 1000 1 3 5 (a) Database query processing time (ms) when a server returns a set of records \n(x-axis is number of records) 66 5 5 4 4 3 3 2 2 1 1 0 0 0 2000 4000 6000 8000 1 3 5 (b) Request processing \ntime (ms) when a web server retrieves a web page (x-axis is page size) Figure 6. Application performance: \n(a) shows Hsqldb data; (b) shows Tomcat data. We blow up the axes using a second graph snapshot to make \nlatency visible. We include regression lines on each graph. Bench-Latency Throughput mark HS XMem HS \nXMem RMI 0.18 ms 15x 75.8 call/s 6x CORBA 0.45 ms 37x 12.5 call/s 35x Serial. 80.4 ms 6977x 21.8 object/s \n20x XML 84.0 ms 7292x 1.11 object/s 391x JNDI 0.24 ms 32x 833 lookup/s 240x Socket 0.01 ms 2.3x 279 kB/s \n2.3x Hsqldb 0.06 ms 1.4x 227 query/s 2.3x Tomcat 4.46 ms 3.9x 104 request/s 4.2x Table 3. Summary of \nXMem impact on latency and throughput for microbenchmarks and applications. We report average baseline \nperformance (Columns 2 and 4) and XMem improvement as a multiple of the baseline (Columns 3 and 5). embedded \nHsqldb database engine by default for persistence and caching. We have modi.ed Hsqldb 1.8.0 to employ \nshared mem\u00adory. A client allocates an SQL query as a shared string. The server is then noti.ed, parses \nthe query, and computes the result in the shared memory. Hsqldb maintains an object cache in the shared \nmemory. Internal representation of leaf data in the object cache is based on immutable objects (strings, \nintegers, dates that model SQL objects). Clients can be given a reference to such objects without a risk \nof modi.cation and therefore most data (and meta\u00addata) does not need copying. We have encapsulated interaction \nover XMem into a JDBC driver for Hsqldb to achieve full transparency. The server listens for connections \nboth on a network socket and in the shared memory. For Hsqldb we measure the impact of XMem on end-to-end \nthroughput (queries/second). Our microbenchmark times the SELECT * FROM T statement executed against \na table T which contains between 1 and 1024 3-.eld records. Figure 6(a) shows the results. XMem increases \nthroughput 2.3x and decreases latency 1.4x. The Hsqldb JDBC driver performs proprietary data serialization, \nwhich is unnecessary in XMem. Apache Tomcat [1] is an industrial-strength web and servlet container. \nWe have modi.ed Tomcat 6.0 to optimize local request handling using XMem. A client and a server share \na byte array and notify each other when sending data. We measure end-to\u00adend performance (requests/second) \nwhen retrieving (HTTP GET method) static web pages of different sizes (multiples of a unit page size). \nWe use the Apache httpclient package to generate conventional HTTP requests. Figure 6 (b) shows the time \nneeded to retrieve a page of a given size. XMem achieves 4x better throughput and 4x shorter latency. \n 5.6 Results Summary Table 3 summarizes our results in terms of average latency and throughput. We use \nleast-squares linear regression to obtain la\u00adtency and throughput as the coef.cients in the equation \ntime = latency + size/throughput, following [10]. We report through\u00adput in the units appropriate for \neach protocol. While microbench\u00admarks focus on communication ef.ciency (the only additional pro\u00adcessing \nis initialization/allocation of the exchanged data), Hsqldb and Tomcat provide insight into the end-to-end \napplication perfor\u00admance. We observe very signi.cant reduction in latency (over three orders of magnitude) \nin case of serialization (default and XML\u00adbased) RMI, CORBA, and JNDI use their own, more ef.cient, \nserialization and thus bene.t less due to XMem. XML-based seri\u00adalization yields the most signi.cant throughput \nincrease (over two orders of magnitude) since it uses a verbose representation of the object graph and \nthus transfers more data.  6. Related Work The key difference between XMem and previously reported sys\u00adtems \nthat coordinate co-located and isolated applications written in type-safe languages is that XMem takes \na top-down approach by assuming full isolation between MREs and providing an ef.cient and straightforward \nmechanism for direct object sharing while pre\u00adserving strong OS-assisted resource protection as much \nas possible. Prior work has focused on bottom-up approaches by introducing weak isolation implemented \nthrough replication of basic OS facili\u00adties within a single OS process. Such systems are much more com\u00adplex \nthan XMem, have weaker protection guarantees, and duplicate existing OS mechanisms. KaffeOS [4] and the \nMulti-tasking Virtual Machine (MVM) [17] employ a single-application MRE and add support for isolation \nand multi-tasking. MVM provides isolation via the Isolate API [31]. Multiple programs (tasks) execute \nin a single MRE instance (OS process) and the MRE manages resources and sharing across them. MVM introduces \na level of indirection when accessing static .elds and does not support direct object sharing. The system \nintroduces links (communication channels between tasks) but cannot elimi\u00adnate the object serialization \nand data copying. KaffeOS supports direct object sharing by means of shared heaps. However, shared heaps \nare not garbage collected and are coarse-grained entities re\u00adclaimed in full when they become unreferenced. \nKaffeOS lacks support for many state-of-the-art MRE mechanisms like parallel GC and modern synchronization. \nOther systems that implement the process/task model within a JVM, include Alta [5], GVM [5], and J-Kernel \n[59], as well as a multi-tasking JVM described in [7]. These systems strive to provide resource management \nand isolation within a single process with\u00adout relying on hardware/OS protection. Class-loader-based \nisola\u00adtion [16] is a standard technique commonly employed by applica\u00adtions servers in order to avoid \nname space pollution/con.icts be\u00adtween multiple web applications hosted within a single JVM. Such isolation, \nhowever, does not prevent interference through static .elds of classes loaded by the system (bootstrap) \nclass loader. This last problem was addressed in [11] by introducing a control access model called object \nspaces where cross-space object accesses are mediated by a security policy. This approach, however, provides \nweak isolation and imposes overhead on inter-space method calls. XMem does not have the aforementioned \nlimitations and is sig\u00adni.cantly simpler than multi-tasking approaches as it leverages the existing infrastructure \nboth at the MRE and OS level. At the same time, XMem offers better fault containment critical errors \ndo not automatically propagate to other MREs unless a fault affects the shared memory. This decreases \nthe probability of a failure escalat\u00ading to multiple components. In XMem, MREs are not completely isolated \nas they share part of their virtual address spaces. However, XMem is signi.cantly more robust than multi-tasking \napproaches, given that resources other than memory are fully isolated and mem\u00adory itself is only partially \nshared. XMem achieves stronger isola\u00adtion, while providing direct object sharing without introducing \nany level of indirection (unlike the MVM). The notion of transparent global and local objects in the \ncontext of distributed shared memory (DSM) multi-processors has been used in Split-C [15] and UPC [21]. \nUnlike XMem, these systems are not type-safe and provide access to global objects at a differ\u00adent cost \nthan to local objects. JavaSpaces [24] provide DSM for applications that implement object .ows. Object \nrepositories in JavaSpaces are type-safe but the system uses serialization and pro\u00advides no shared memory \nsupport for co-located application compo\u00adnents. Other DSM systems for type-safe languages include single\u00adsystem-image \napproaches to implementing a global object space such as cJVM [2], JAVA/DSM [67], JESSICA [40], Hyperion \n[42], JavaParty [50], and MultiJav [12]. While XMem targets sharing between co-located MREs, software \nDSM focuses mostly on dis\u00adtributed protocols necessary to guarantee memory consistency and cache coherence \nmodels de.ning certain semantics for concurrency in a distributed system. Runtime systems for concurrent \nlanguages that offer built-in constructs for inter-process communication include Erlang [3], Oc\u00adcam [45], \nand Limbo [20]. These systems build on the algebra of communicating sequential processes [27] and provide \na point-to\u00adpoint message passing mechanism for lightweight processes with share-nothing semantics. In \ncontrast, XMem adheres to the shared memory programming model. Unlike XMem, Erlang is a func\u00adtional language \nand requires the shared objects to be immutable. XMem targets general-purpose imperative procedural languages. \nIn language-based operating systems [53], such as Singu\u00adlarity [22, 30], JX [25], JNode [35], Inferno \n[20], SPIN [9], Oberon [65], and JavaOS [33], processes share a single address space and use type and \ncontrol safety provided by a trusted com\u00adpiler (via static analysis) to guarantee memory protection and \nre\u00adsource isolation without implementing a hardware-assisted ref\u00aderence monitor. Singularity is a micro-kernel \nOS, implemented mostly in C#, supporting ef.cient communication between mul\u00adtiple isolated processes. \nIts design differs from XMem in several ways. First, XMem leverages hardware memory protection, while \nSingularity provides lightweight software-based isolation via type\u00adsafety (multiple applications execute \nin a single address space). Second, Singularity provides message-passing via typed channels and explicit \ncommunication primitives. In contrast, XMem pro\u00advides a shared-memory-based implicit communication where \nonly the initial handshake employs the channel abstraction. Next, in Sin\u00adgularity communication is limited \nto two endpoints and involves the transfer of ownership of a memory block (there is no data shar\u00ading \nbetween the sender and the receiver). XMem enables direct and transparent object sharing between any \nnumber of threads, po\u00adtentially from distinct MREs. Finally, Singularity employs block\u00adbased reference \ncounting garbage collection while XMem uses more .ne-grained tracing GC. To date, virtual memory manipulation \n(which is used by XMem to implement double mapping of the GCT and LCT) has been used in MREs mostly in \nthe context of GC [61, 38, 66, 26, 13]. For example, the Compressor [38] employs double mapping to enable \nconcurrent compaction, and the Mapping Collector [61] compacts free space by remapping to avoid object \ncopying. 7. Conclusions We present XMem, type-safe and transparent shared memory for isolated, co-located \nMREs. The motivation behind XMem is more ef.cient, cross-component interaction and communication in enter\u00adprise \nmulti-tier applications deployed on a single host. XMem pro\u00advides stronger fault and resource isolation \nthan previously reported systems, while enabling ef.cient direct object sharing over private channels. \nTo guarantee type-safety, XMem extends state-of-the-art MRE services such as synchronization, class loading, \nobject alloca\u00adtion, and garbage collection, as well as introduces global operations to coordinate MREs \nusing a single shared segment. XMem manip\u00adulates virtual memory mapping (using a standard OS interface) \nto avoid indirect memory access. XMem is transparently integrated within the MRE infrastructure and can \nbe used to optimize exist\u00ading communication protocols, such as RMI. We implement XMem in the HotSpot \nJVM and evaluate it empirically. XMem introduces tolerable space/time overhead while improving ef.ciency \n(latency and throughput) of extant J2SE/J2EE communication mechanisms by up to several orders of magnitude. \nAs part of future work, we plan to extend XMem to heteroge\u00adneous MREs. Currently XMem requires that the \nattached MREs implement the same object model and have the same internal rep\u00adresentation of classes (types). \nWe are investigating ways to support shared memory communication and coordination between different Java \nVirtual Machines as well as between different language virtual execution environments to enable better \ncross-language interaction that is simple and easy to use. In addition, we are investigating sim\u00adilar \napproaches for IPC between other virtualization systems, e.g., virtual machine monitors and full system \nvirtual machines.  Acknowledgments We thank the anonymous reviewers for providing insightful com\u00adments \non this paper. This work was funded in part by NSF grants CCF-0444412, CNS-CAREER-0546737, and CNS-0627183. \n References [1] Apache Tomcat. http://tomcat.apache.org. [2] Y. Aridor, M. Factor, and A. Teperman. cJVM: \nA single system image of a JVM on a cluster. In ICPP, 1999. [3] J. Armstrong, R. Virding, C. Wikstrom, \nand M. Williams. Concurrent Programming in Erlang. Prentice-Hall, 1996. [4] G. Back, W. C. Hsieh, and \nJ. Lepreau. Processes in KaffeOS: Isolation, resource management, and sharing in Java. In OSDI, 2000. \n[5] G. Back, P. Tullmann, L. Stoller, W. C. Hsieh, and J. Lepreau. Java operating systems: Design and \nimplementation. Technical report, Univ. of Utah, 1998. [6] H. G. Baker. Infant mortality and generational \ngarbage collection. SIGPLAN Not., 28(4), 1993. [7] D. Balfanz and L. Gong. Experience with secure multi-processing \nin Java. In ICDCS, 1998. [8] BEA WebLogic Application Server. http://www.bea.com. [9] B. N. Bershad, \nS. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski, D. Becker, C. Chambers, and S. J. Eggers. Extensibility, \nsafety and performance in the SPIN operating system. In SOSP, 1995. [10] F. Breg and C. D. Polychronopoulos. \nJava virtual machine support for object serialization. In Java Grande, 2001. [11] C. Bryce and C. Raza.mahefa. \nAn approach to safe object sharing. SIGPLAN Not., 35(10), 2000. [12] X. Chen and V. H. Allan. MultiJav: \nA distributed shared memory system based on multiple Java virtual machines. In PDPTA, 1998. [13] C. Click, \nG. Tene, and M. Wolf. The pauseless GC algorithm. In VEE, 2005. [14] CORBA Speci.cation. http://www.omg.org. \n[15] D. E. Culler, A. C. Arpaci-Dusseau, S. C. Goldstein, A. Krish\u00adnamurthy, S. Lumetta, T. von Eicken, \nand K. A. Yelick. Parallel programming in Split-C. In SC, 1993. [16] G. Czajkowski. Application isolation \nin the Java virtual machine. In OOPSLA, 2000. [17] G. Czajkowski and L. Daynes. Multitasking without \ncompromise: A virtual machine evolution. In OOPSLA, 2001. [18] The DaCapo benchmarks. http://dacapobench.org. \n[19] D. Detlefs, C. Flood, S. Heller, and T. Printezis. Garbage-.rst garbage collection. In ISMM, 2004. \n[20] S. Dorward, R. Pike, D. L. Presotto, D. Ritchie, H. Trickey, and P. Winterbottom. Inferno. In COMPCON, \n1997. [21] T. El-Ghazawi, W. Carlson, and J. Draper. UPC Language Speci.cations V, 2001. [22] M. Fahndrich, \nM. Aiken, C. Hawblitzel, O. Hodson, G. C. Hunt, J. R. Larus, and S. Levi. Language support for fast and \nreliable message-based communication in Singularity OS. In EuroSys, 2006. [23] C. Flood, D. Detlefs, \nN. Shavit, and C. Zhang. Parallel garbage collection for shared memory multiprocessors. In JVM, 2001. \n[24] E. Freeman, S. Hupfer, and K. Arnold. JavaSpaces Principles, Patterns, and Practice (Jini Series). \nPearson Education, 1999. [25] M. Golm, M. Felser, C. Wawersich, and J. Kleinoder. The JX operating system. \nIn USENIX Annual Technical Conference, 2002. [26] M. Hertz, Y. Feng, and E. D. Berger. Garbage collection \nwithout paging. In PLDI, 2005. [27] C. A. R. Hoare. Communicating sequential processes. Commun. ACM, \n26(1), 1983. [28] HotSpot Java Virtual Machine GC. http://java.sun.com/ javase/technologies/hotspot. \n[29] Hsqldb. http://www.hsqldb.org. [30] G. C. Hunt and J. R. Larus. Singularity: Rethinking the software \nstack. Operating Systems Review, 41(2):37 49, 2007. [31] Isolate API. JSR-121. http://jcp.org. [32] Java \n2 Enterprise Edition. http://java.sun.com/javaee/. [33] JavaOS : A Standalone Java Environment. Sun Microsystems, \n1996. [34] JBoss Enterprise Middleware. http://www.jboss.com. [35] JNode. http://www.jnode.org. [36] \nR. Jones and C. Ryder. Garbage collection should be lifetime aware. In ICOOOLPS, 2006. [37] R. E. Jones. \nGarbage Collection: Algorithms for Automatic Dynamic Memory Management. John Wiley and Sons, 1996. [38] \nH. Kermany and E. Petrank. The Compressor: Concurrent, incremental and parallel compaction. In PLDI, \n2006. [39] S. Liang and G. Bracha. Dynamic class loading in the Java virtual machine. In OOPSLA, 1998. \n[40] M. J. M. Ma, C.-L. Wang, and F. C. M. Lau. JESSICA: Java-enabled single-system-image computing architecture. \nJ. Parallel Distrib. Comput., 60(10), 2000. [41] J. Maassen, R. V. Nieuwpoort, R. Veldema, H. E. Bal, \nT. Kielmann, C. J. H. Jacobs, and R. F. H. Hofman. Ef.cient Java RMI for parallel programming. Programming \nLanguages and Systems, 23(6), 2001. [42] M. Macbeth, K. McGuigan, and P. Hatcher. Executing Java threads \nin parallel in a distributed-memory environment. In CASCON, 1998. [43] J. Manson, W. Pugh, and S. V. \nAdve. The Java memory model. SIGPLAN Not., 40(1), 2005. [44] M. Nelson, B.-H. Lim, and G. Hutchins. Fast \ntransparent migration for virtual machines. In USENIX Technical Conference, 2005. [45] Occam Programming \nManual. Inmos Corporation, 1984. [46] Open Source J2SE. http://openjdk.java.net. [47] S. Osman, D. Subhraveti, \nG. Su, and J. Nieh. The design and imple\u00admentation of Zap: A system for migrating computing environments. \nIn OSDI, 2002. [48] Y. Ossia, O. Ben-Yitzhak, and M. Segal. Mostly concurrent compaction for mark-sweep \nGC. In ISMM, 2004. [49] M. Perry. Shared Memory Under Linux, 1999. http://fscked. org/writings/SHM/shm.html. \n[50] M. Philippsen and M. Zenger. JavaParty transparent remote objects in Java. Concurrency: Practice \nand Experience, 9(11), 1997. [51] Java RMI Speci.cation. http://java.sun.com. [52] K. Russell and D. \nDetlefs. Eliminating synchronization-related atomic operations with biased locking and bulk rebiasing. \nSIGPLAN Not., 41(10), 2006. [53] F. B. Schneider, G. Morrisett, and R. Harper. A language-based approach \nto security. Lecture Notes in CS, 2001. [54] Java Object Serialization Speci.cation. http://java.sun.com. \n[55] SPEC. http://www.spec.org. [56] D. Stefanovic, M. Hertz, S. M. Blackburn, K. S. McKinley, and J. \nE. B. Moss. Older-.rst garbage collection in practice: Evaluation in a Java virtual machine. In MSP, \n2002. [57] T. Suezawa. Persistent execution state of a Java virtual machine. In Java Grande, 2000. [58] \nD. M. Ungar. Generation scavenging: A non-disruptive high performance storage reclamation algorithm. \nSIGPLAN Not., 19(5), 1984. [59] T. von Eicken, C.-C. Chang, G. Czajkowski, C. Hawblitzel, D. Hu, and \nD. Spoonhower. J-Kernel: A capability-based operating system for Java. In Secure Internet Programming, \n1999. [60] IBM WebSphere Application Server. http://www.ibm.com. [61] M. Wegiel and C. Krintz. The Mapping \nCollector: Virtual memory support for generational, parallel, and concurrent compaction. In ASPLOS, 2008. \n[62] P. R. Wilson. Uniprocessor garbage collection techniques. Technical report, Univ. of Texas, 1994. \n[63] P. R. Wilson, M. S. Johnstone, M. Neely, and D. Boles. Dynamic storage allocation: A survey and \ncritical review. In IWMM, 1995. [64] P. R. Wilson and T. G. Moher. A card-marking scheme for controlling \nintergenerational references in generation-based garbage collection on stock hardware. SIGPLAN Not., \n24(5), 1989. [65] N. Wirth and J. Gutknecht. Project Oberon: the design of an operating system and compiler. \nACM Press/Addison-Wesley, 1992. [66] T. Yang, E. D. Berger, S. F. Kaplan, and J. E. B. Moss. CRAMM: Virtual \nmemory support for garbage-collected applications. In OSDI, 2006. [67] W. Yu and A. L. Cox. Java/DSM: \nA platform for heterogeneous computing. Concurrency -Practice and Experience, 9(11), 1997.  \n\t\t\t", "proc_id": "1375581", "abstract": "<p>Developers commonly build contemporary enterprise applications using type-safe, component-based platforms, such as J2EE, and architect them to comprise multiple tiers, such as a web container, application server, and database engine. Administrators increasingly execute each tier in its own managed runtime environment (MRE) to improve reliability and to manage system complexity through the fault containment and modularity offered by isolated MRE instances. Such isolation, however, necessitates expensive cross-tier communication based on protocols such as object serialization and remote procedure calls. Administrators commonly co-locate communicating MREs on a single host to reduce communication overhead and to better exploit increasing numbers of available processing cores. However, state-of-the-art MREs offer no support for more efficient communication between co-located MREs, while fast inter-process communication mechanisms, such as shared memory, are widely available as a standard operating system service on most modern platforms.</p> <p>To address this growing need, we present the design and implementation of XMem ? type-safe, transparent, shared memory support for co-located MREs. XMem guarantees type-safety through coordinated, parallel, multi-process class loading and garbage collection. To avoid introducing any level of indirection, XMem manipulates virtual memory mapping. In addition, object sharing in XMem is fully transparent: shared objects are identical to local objects in terms of field access, synchronization, garbage collection, and method invocation, with the only difference being that sharedto-private pointers are disallowed. XMem facilitates easy integration and use by existing communication technologies and software systems, such as RMI, JNDI, JDBC, serialization/XML, and network sockets.</p> <p>We have implemented XMem in the open-source, productionquality HotSpot Java Virtual Machine. Our experimental evaluation, based on core communication technologies underlying J2EE, as well as using open-source server applications, indicates that XMem significantly improves throughput and response time by avoiding the overheads imposed by object serialization and network communication.</p>", "authors": [{"name": "Michal Wegiel", "author_profile_id": "81351596018", "affiliation": "Univ. of California, Santa Barbara, Santa Barbara, CA, USA", "person_id": "P1022814", "email_address": "", "orcid_id": ""}, {"name": "Chandra Krintz", "author_profile_id": "81100025597", "affiliation": "Univ. of California, Santa Barbara, Santa Barbara, CA, USA", "person_id": "P1022815", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1375581.1375621", "year": "2008", "article_id": "1375621", "conference": "PLDI", "title": "XMem: type-safe, transparent, shared memory for cross-runtime communication and coordination", "url": "http://dl.acm.org/citation.cfm?id=1375621"}