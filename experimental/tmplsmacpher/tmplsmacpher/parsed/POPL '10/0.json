{"article_publication_date": "01-17-2010", "fulltext": "\n Recon.gurable Asynchronous Logic Automata (RALA) Neil Gershenfeld David Dalrymple Kailiang Chen Ara \nKnaian Forrest Green Erik D. Demaine Scott Greenwald Peter Schmidt-Nielsen MIT Center for Bits and Atoms \ngersh@cba.mit.edu Abstract Computer science has served to insulate programs and program\u00admers from knowledge \nof the underlying mechanisms used to ma\u00adnipulate information, however this .ction is increasingly hard \nto maintain as computing devices decrease in size and systems in\u00adcrease in complexity. Manifestations \nof these limits appearing in computers include scaling issues in interconnect, dissipation, and coding. \nRecon.gurable Asynchronous Logic Automata (RALA) is an alternative formulation of computation that seeks \nto align log\u00adical and physical descriptions by exposing rather than hiding this underlying reality. Instead \nof physical units being represented in computer programs only as abstract symbols, RALA is based on a \nlattice of cells that asynchronously pass state tokens corresponding to physical resources. We introduce \nthe design of RALA, review its relationships to its many progenitors, and discuss its bene.ts, implementation, \nprogramming, and extensions. Categories and Subject Descriptors C.1.3 [Other Architecture Styles]: Adaptable \nArchitectures General Terms Design, Languages, Performance, Theory Keywords Recon.gurable, Asynchronous, \nLogic, Automata 1. Introduction Computers are increasingly distributed systems, from multi-core processors \nto multi-processor servers to multi-server systems. However, this physical reality is typically not re.ected \nin how they are programmed. Common distinctions among programming languages include whether they are \nimperative or declarative, com\u00adpiled or interpreted, or strongly or weakly typed. However, these distinctions \nall assume that computation happens in a world with different rules from those that govern the underlying \ncomputing machinery. Each of these program types manipulates arbitrary sym\u00adbols, unconstrained by physical \nunits. This is a .ction that is increasingly dif.cult to maintain. Be\u00adcause the computational descriptions \nand physical realities diverge, ever-faster switches, networks, and memory have been required to eliminate \ninterconnect bottlenecks, and the productivity of both Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 10, January 17 23, 2010, Madrid, Spain. Copyright c . 2010 \nACM 978-1-60558-479-9/10/01. . . $10.00 Figure 1. RALA cells and token states. For clarity, AND cells \nwith duplicated inputs used for logical wires transporting and buffering tokens are drawn as circles. \nprogrammers and processors in large systems has not kept pace with the underlying device performance. \nThe one thing that most programming models don t describe is space: geometry is bound late in the scheduling \nof a program s execution. This division dates back to the theoretical foundations of computation; Turing \nmachines [37] and von Neumann architec\u00adtures [40] localize information processing in a logical unit, \nwith\u00adout regard to its spatial extent. There is an equally-long history of geometrical descriptions of \ncomputation, from Wang tiles [42], to cellular automata [41], to billiard balls [25]. However, these \nmod\u00adels have primarily been aimed at modeling rather than implement\u00ading logic. Although special-purpose \ncomputers have been devel\u00adoped for spatial computing models, they ve typically been emulated rather than \nequivalent to the underlying hardware [36]. Physical dynamics, in comparison, are inherently distributed, \nparallel, asynchronous, and scalable [22]. RALA (Recon.gurable Asynchronous Logic Automata) seeks to \ncapture these attributes by building on a foundation that aligns computational and physical de\u00adscriptions. \nComputation, communication, and storage then become derived rather than assumed system properties. RALA \nis based on a lattice of cells (in 2D or 3D) that locally pass tokens representing logical states [13]. \nIn the implementation presented here there are 8 cell types, shown in Figure 1 and with functions described \nbelow. Four of the cells are used for logic, two for creating and destroying tokens, one for transporting \ntokens, and one for recon.guration. When these have valid tokens on their inputs and no tokens on their \noutputs they pull the former and push to the latter. In RALA, the distance that information travels is \nequal to the time it takes to travel (in gate delay units), the amount of informa\u00adtion that can be stored, \nand the number of operations that can be performed. These are all coupled, as physical units are. RALA \ncan be understood as the end-point, and intersection, of many existing computational paradigms, building \nthe essential elements of each into every cell:  Figure 2. Asynchronous operation of a RALA LFSR. \nIt is similar to cellular automata, but eliminates the unphysical need for a global clock, and does not \nrequire many cells for logic [5] or deterministic asynchronous operation [2].  It is a Petri Net [30] \non a lattice with nearest-neighbor connec\u00adtions.  It is a form of RISC [28], where the instructions \nare reduced to a single logical operation.  It is a multicore processor that effectively has a core \nfor every bit.  It is a .eld-programmable gate array with single-gate logic blocks and nearest-neighbor \nswitch matrices; it does not need a clock, because of its homogeneity a .tter is not needed to map a \nlogic diagram onto corresponding device resources, and a larger program can transparently be distributed \nacross multiple smaller chips.  It is a systolic array [21] that allows for arbitrary data .ow and logic. \n It is a form of data.ow programming [3, 14, 20] in which the graph itself is executable, without requiring \nevent scheduling.  It is a form of recon.gurable [24] asynchronous [32] logic [8] in which a logical \nspeci.cation is equal to its asynchronous implementation because each gate implements deterministic asynchronous \noperation.  The following sections discuss RALA operation, bene.ts, im\u00adplementation, programming, and \nextensions. 2. Operation Logic: The RALA implementation used here has four logical cell types, AND, NAND, \nOR, and XOR. NAND alone is universal [19]; the others are included to simplify logic design. Figure 2 \nshows the operation of an XOR cell in a Linear Feedback Shift Register (LFSR), with the feedback loops \ncarried by AND cells with duplicated inputs. Transport: in Figure 2 the AND cells with duplicated inputs \nserve two purposes, moving tokens for interconnect, and buffering them for delays. However, not all of \nthese are needed as buffers; RALA includes a non-blocking transport cell that connects its input and \noutput physically rather than logically, reducing the overhead in time and energy for the latter. The \ntransport cell is also used as a crossover, to allow tokens to pass in perpendicular directions. Copy \nand Delete: RALA cells implicitly create and destroy to\u00adkens when they perform fan-in and fan-out operations, \nhowever asynchronous algorithms can require explicit token creation and destruction when controlled changes \nin token numbers are nec\u00adessary. A token at the designated data input of a copy or delete cell is passed \nunchanged if the control input token is a 0; a 1 fold direction: forward=000 backward=001 right=010 \nleft=011 up=101 down=110 end=111 gate: and=000 nand=001 or=010 xor=011 transport=100 copy=101 delete=110 \nstem=111 input 1: forward=000 backward=001 right=010 left=011 up=101 down=110 not used=111 init 1: 0=00 \n1=01 x=10 input 2: forward=000 backward=001 right=010 left=011 up=101 down=110 not used=111 init 2: 0=00 \n1=01 x=10 stay connected=11 output directions: forward backward right left up down: 0=off 1=on Table \n1. RALA stem cell con.guration code; a right-hand rule coordinate system is assumed. control input causes \nthe copy cell to replicate a data token, and a delete cell to consume it. Recon.guration: because RALA \naligns the description of hard\u00adware and software, it cannot assume the presence of an omnipo\u00adtent operating \nsystem with global access to system resources. Instead, control of program execution must be implemented \nlo\u00adcally, within the system. RALA accomplishes this with stem cells, which, like their biological counterparts, \ncan be differen\u00adtiated into other cell types. All RALA cells have internal state to store their con.guration \n(cell type and inputs); stem cells ex\u00adpose this state so that it can be set by input tokens. Program \ngeometry is speci.ed by the universality of folding linear paths [12]. A linked string of stem cells \npass data tokens to the termi\u00adnal cell, which interprets them according to the code in Table 1. These \nwill cause the terminal cell to either differentiate and detach, with its input cell becoming the new \nterminal cell, or to extend the string in a direction speci.ed relative to its input, linking to a neighboring \ncell that is converted back to the stem cell state and becomes the new terminal cell. Figure 3 shows \nthe growth of the LFSR in Figure 2. Because each differenti\u00adated cell waits for valid inputs and outputs, \nthe circuit turns on consistently without requiring global communication or coordi\u00adnation [27]. 3. Bene.ts \nEach RALA cell provides a primitive unit of time, space, state, and logic. Bene.ts of this uniformity \ninclude: Simplicity: a program is completely speci.ed by the locations and connections of eight cell \ntypes. Portability: any process technology that can perform cell up\u00addates can execute the same RALA program, \nwhether tokens are passed between system servers, server processors, proces\u00adsor cores, microcontroller \narrays, CMOS cells, or atom lattices. Scalability: the only design rules are within each cell; as long \nas cells are updated locally, the system will operate correctly globally. Flexibility: because each cell \ncan buffer, transform, and transfer tokens, a system s interconnect, memory, and processing can be dynamically \nallocated based on applications and workloads. Word sizes for memory addressing or arithmetic operations \ncan be adapted to a program s needs. Performance: RALA execution times are determined by the dis\u00adtance \ninformation travels, not clock cycles, so that, for exam\u00adple, both sorting and matrix multiplication \nbecome linear-time algorithms (Figures 5 and 7). The speed of logical operations is Figure 3. Asynchronous \nsteps in the RALA construction of an LFSR from a source cell streaming tokens through a stem cell.  \n the local gate propagation delay, rather than a worst-case global clock. Security: Once they re con.gured, \nALA programs run on what is effectively special-purpose hardware rather than a general\u00adpurpose processor; \nattacks that are based on shared resources, such as buffer overruns, are prevented by spatially separating \nprograms. Power: there is no static power consumption, other than leakage; power is needed only for token \ntransitions. Although leakage is an increasingly severe limit as gates scale down in feature size, RALA \ncan instead grow up in system size because it is not limited by clock domains or chip boundaries. Yield: \nbecause RALA is constructed from simple cells, a fabrica\u00adtion process need produce only hundreds rather \nthan hundreds of millions of transistors, and then repeat these units. Faulty cells can be accommodated \nby adding spatial error-correction to RALA programs. This signi.cantly relaxes today s severe constraints \non optical .atness, planarization, and defect density, allowing existing processes to be used much further \nalong the semiconductor scaling roadmap [29]. Programmability: conventional computer programming requires \nresource allocation. In a multi-threaded system this entails spawning and synchronizing threads, in a \nmulti-processor sys\u00adtem assigning processes to processors, or in a data.ow system scheduling and executing \nmessages. These actions are implicit in the design of a RALA program, because they are effectively performed \nby each cell. Veri.cation: the cost of large-scale full-custom ASIC design has become prohibitive for \nall but the most important applications, projected to past $100 million by the 32-nm node [15]. Tap\u00ading \nout a chip requires a substantial effort to validate the de\u00adsign, including transistor sizing, transition \ntiming, clock skew, Figure 4. CMOS ALA AND cell. crosstalk, and parasitics [18]. RALA divides this into \nsoftware simulation of a system and the much simpler hardware task of verifying the operation of a single \ncell. An added bene.t is that a standard cell library of RALA cell types can be used to im\u00admediately \nconvert a RALA program into an equivalent special\u00adpurpose ALA ASIC. 4. Implementation In a conventional \nprocessor, the logic of updating a RALA cell requires roughly 10 instructions. Therefore, a $100 microprocessor \nthat executes 109 instructions per second can update 108 cells per second; at 10 W that corresponds to \n10-7 J per cell update and $10-6 per cell/s. For a $1 microcontroller that executes 107 instructions \nper second at 10 mW, the corresponding numbers are 10-8 J per cell update and again $10-6 per cell/s. \nA non-recon.gurable ALA cell can be implemented with roughly 100 transistors (shown in Figure 4), and \na recon.gurable RALA cell with about 1000, in dual-rail CMOS logic; in a 90 nm design this requires on \nthe order of 10-13 J to update a cell, with a propagation time of 1 ns [11]. Assuming a comparable cost \nof $10-6 per transistor [23], this is $10-14 per cell/s. While these estimates have ignored prefactors, \nthe many orders-of-magnitude difference in energy and cost are due to corresponding differences in the \nnumber of transistors needed to natively implement RALA vs simulate it on a general-purpose computer. \nRALA might appear to incur overheads due to the absence of dedicated interconnect or functional units. \nState-of-the-art gate de\u00adlays are on the order of ps over micron features [35], corresponding to ns to \ncross mm-scale chips with logic rather than wires, compa\u00adrable to current clock speeds. The largest HPC \nsystems consume about 10 MW for 1 peta.op/s, or 10-8 J/.op; at 0.1 pJ/cell and as\u00adsuming a 100 bit word \nsize, a RALA adder requires approximately 10 cells for 10-10 J per add, and a multiplier 1000 cells for \n10-8 J for a (linear-time) multiply. And at 100 transistors for a non\u00adrecon.gurable cell and 1000 transistors \nfor a recon.gurable cell, area can be traded off against speed by virtualization with asyn\u00adchronous cell \nprocessors containing memory and event queues, to reduce the number of transistors per cell to order \nunity. 5. Programming The lowest level of RALA programming is explicit placement of cells types and connections. \nThis is like full-custom logic, but is immediately executable; the preceding .gures can be taken as executable \nprograms. A cell-level description misses the functional organization of a RALA program; one way to convey \nthis is with a hardware description language. Figure 5 shows a linear-time RALA sort; here are sections \nof the de.nitions of its modules:  Figure 5. Section of a linear-time RALA sort, interleaving com\u00adparators \nand switchers. # # upper 10 detector # gate(l, transport , wxsx ,1,3) gate(l, wire , sx ,1,4) gate(l, \nand , wxnx ,2,3) gate(l, nand , wx ,2,4) # # upper latch # gate(l, or , wxn0 ,3,3) gate(l, and , sxex \n,3,4) and their connections: c2 = comparator(l,c1.x+c1.dx-1,c1.y+c1.dy, label= comparator 2 ,flip= \nx ,color= #ffffdd ) s2 = switcher(l,c2.x-c2.dx,c2.y, label= switcher 2 ,flip= x ,color= #faeefa ) wire(l, \nx , c2.cell[ out0 ], s2.cell[ in0 ]) wire(l, x , c2.cell[ out1 ], s2.cell[ in1 ]) Because programs \nare spatial structures, this is similar to the internal representation of a hierarchical parametric CAD \nsystem. Figure 6 shows a RALA programming tool built this way, with a CAD-like interface [34]. A RALA \nCAD diagram can be abstracted as a data.ow graph; because there is a one-to-one map between nodes and \nlinks in such a graph and RALA modules, data.ow representations of high-level languages [38] can be directly \nexecuted as RALA programs. There is also a direct mapping of mathematics. Figure 7 shows linear-time \nRALA matrix multiplication, implementing the .ow of information through the matrices. Along with the \nspaces repre\u00adsented by mathematical symbols, there is also a space occupied by mathematical symbols on \na page; RALA can be used to solve prob\u00adlems in this space by converting the equations onto corresponding \nspatial structures [17]. This is computationally universal, providing a constructive implementation of \nconstraint programming [33]. Figure 6. RALA CAD. Figure 7. RALA linear-time matrix multiplication, with \nan array of adders and linear-time multipliers. 6. Extensions RALA assumes a regular lattice in 2D or \n3D, re.ecting the con\u00adstruction of the computational substrate. It could be generalized to probabilistic \noperation with random cell locations and connec\u00adtions [1, 9], however the regularity of the lattice does \nnot repre\u00adsent a signi.cant scaling cost in fabrication; there are a number of high-throughput processes \nthat can produce arrays of simple cells, including soft lithography, self-assembly, parallel direct-write, \nand printing digital materials [31]. Mixed-signal applications such as software radios convention\u00adally \ndigitize analog signals which are then processed digitally. Power can be reduced, and speed and noise \nmargins increased, for these applications by bit-slicing at the end rather than beginning of the signal \nchain. Analog logic uses continuous device degrees of freedom to represent bit probabilities rather than \nbits; it operates in the state-space of the corresponding digital problem, but relaxes the binary constraint \nto be able to pass through the interior of the logical hypercube [39]. Programmability can be introduced \nwith an array of soft cells (shown in Figure 8 [10]), which can be extended to RALA with asynchronous \nevent logic. Real-time constraints can be enforced by the external timing of token creation or consump\u00adtion. \nThere are a number of quantum systems that naturally imple\u00adment the conditional asynchronous dependency \nof RALA cells, in\u00adcluding multi-photon transitions in cavity QED [26], and electron passage through a \nCoulomb blockade [4]. Implementing RALA over these systems offers an alternative paradigm for quantum \ncom\u00adputing that is aligned with device physics. Because of the need for unitary operation, the universal \nlogical cells would be replaced with single-bit rotations and a nonlinear two-bit interaction [6], and \nthe token creation and destruction cells correspond to raising and low\u00adering operators.  Figure 8. \nAnalog logic automata cells. RALA could also be implemented with classically reversible logic cells [16] \nto eliminate the energetic cost of creating and destroying tokens in logical operations. However, this \ndoes require either steady-state bit sources and sinks or running results back from outputs to inputs \n[7]. Non-reversible RALA still operates by transporting tokens, localizing the energetic cost in token \ncreation and destruction. The CMOS implementation described in the previous section stores con.guration \nin static memory, and transfers tokens with logical handshaking. The transistor count can be reduced \nby using device physics for these functions, storing con.guration on .oating gates or in phase-change \nmaterials, and handshaking by sensing charge packets. 7. Conclusion Although RALA is motivated by scaling \nlimits, it can also be implemented over existing systems as an alternative program\u00adming paradigm offering \nsimplicity and portability. Ongoing work is developing versions targeting low-level message passing on \nhigh-performance computing clusters, arrays of microcontrollers, ASICs, and nanostructures. Potential \nbene.ts include reducing power consumption, increasing speed, building larger systems, us\u00ading smaller \ndevices, adapting to workloads, and simplifying pro\u00adgramming. RALA should be understood not as a new \nprogramming model, but rather as the asymptotic limit of many familiar ones. But in this limit their \nintersection aligns physical and computational descrip\u00adtions, providing an opportunity to revisit assumptions \nthat date back to the origins of modern computing. Acknowledgments The development of RALA builds on \nthe work of, and has bene.ted from discussions with, colleagues including Charles Bennett, Seth Lloyd, \nMarvin Minsky, Tom Toffoli, Norm Margolus, Bill Butera, and Mark Pavicic. This work was supported by \nMIT s Center for Bits and Atoms and the U.S. Army Research Of.ce under grant numbers W911NF-08-1-0254 \nand W911NF-09-1-0542. References [1] H. Abelson, D. Allen, D. Coore, C. Hanson, G. Homsy, T. F. Knight, \nJr., R. Nagpal, E. Rauch, G. J. Sussman, and R. Ron Weiss. Amorphous computing. Commun. ACM, 43:74 82, \n2000. [2] S. Adachi, F. Peper, and J. Lee. Computation by asynchronously updating cellular automata. \nJournal of Statistical Physics, 114:261 289, 2004. [3] Arvind and D. E. Culler. Data.ow architectures. \nAnnual Reviews of Computer Science, pages 225 253, 1986. [4] D. Averin and K. Likharev. Coulomb blockade \nof single-electron tunneling, and coherent oscillations in small tunnel junctions. J. Low Temp. Phys., \n62:345 373, 1986. [6] A. Barenco, C. H. Bennett, R. Cleve, D. P. DiVincenzo, N. Margolus, P. Shor, T. \nSleator, J. Smolin, and H. Weinfurter. Elementary gates for quantum computation. Phys. Rev. A, 52:3457 \n67, 1995. [7] C. Bennett. Time/space trade-offs for reversible computation. Siam J. Comput., 18:766 776, \n1989. [8] G. Birtwistle and A. Davis, editors. Asynchronous Digital Circuit Design. Springer-Verlag, \nNew York, 1995. [9] W. Butera. Programming a Paintable Computer. PhD thesis, MIT, 2002. [10] K. Chen, \nL. Leu, , and N. Gershenfeld. Analog logic automata. In Proceedings of the IEEE Biomedical Circuits and \nSystems Conference (BioCAS), pages 189 192, 2008. [11] K. Chen, F. Green, and N. Gershenfeld. Asynchronous \nlogic automata asic design. Preprint, 2009. [12] K. C. Cheung, E. D. Demaine, and S. Grif.th. Programmed \nassembly with universally foldable strings. Preprint, 2009. [13] D. Dalrymple, N. Gershenfeld, and K. \nChen. Asynchronous logic automata. In Proceedings of AUTOMATA 2008, pages 313 322, 2008. [14] J. Dennis. \nData .ow supercomputers. Computer, 13:48 56, 1980. [15] EE Times, 2009. March 17. [16] M. Frank, C. Vieri, \nM. J. Ammer, N. Love, N. Margolus, and T. Knight, Jr. A scalable reversible computer in silicon. In C. \nCalude, J. Casti, and M. Dineen, editors, Unconventional Models of Computation, pages 183 200, Berlin, \n1998. Springer. [17] S. Greenwald, B. Haeupler, and N. Gershenfeld. Mathematical operations in asynchronous \nlogic automata. Preprint, 2009. [18] S. Hassoun and T. Sasao, editors. Logic Synthesis and Veri.cation. \nKluwer Academic Publishers, Norwell, MA, 2002. [19] F. J. Hill and G. R. Peterson. Computer Aided Logical \nDesign with Emphasis on VLSI. Wiley, New York, 4th edition, 1993. [20] W. M. Johnston, J. P. Hanna, and \nR. J. Millar. Advances in data.ow programming languages. ACM Computing Surveys, 36:1 34, 2004. [21] S. \nKung. Vlsi array processors. IEEE ASSP Magazine, pages 4 22, 1985. [22] S. Lloyd. Ultimate physical limits \nto computation. Nature, 406: 1047 1054, 2000. [23] W. Maly. Cost of silicon viewed from vlsi design perspective. \nIn DAC 94: Proceedings of the 31st annual Design Automation Conference, pages 135 142, 1994. [24] R. \nManohar. Recon.gurable asynchronous logic. In Proceedings of the IEEE Custom Integrated Circuits Conference, \npages 13 20, 2006. [25] N. Margolus. Physics-like models of computation. Physica D, 10: 81 95, 1984. \n[26] R. Miller, T. Northup, K. Birnbaum, A. Boca, A. Boozer, and H. Kimble. Trapped atoms in cavity qed: \nCoupling quantized light and matter. J. Phys. B: At. Mol. Opt. Phys., 38:S551 S565, 2005. [27] M. Minsky. \nComputation: Finite and In.nite Machines. Prentice-Hall, Englewood Cliffs, 1967.  [28] D. A. Patterson \nand D. R. Ditzel. The case for the reduced instruction set computer. SIGARCH Comput. Archit. News, 8:25 \n33, 1980. [29] P. S. Peercy. The drive to miniaturization. Nature, 406, 2000. [30] C. Petri. Nets, time \nand space. Theoretical Computer Science, 153: 3 48, 1996. [31] G. Popescu, P. Kunzler, and N. Gershenfeld. \nDigital printing of digital materials. In DF 2006 International Conference on Digital Fabrication Technologies, \n2006. Denver, Colorado. [32] Proceedings of the IEEE. Special issue on asynchronous circuits and systems, \n1999. 87:2. [33] J.-F. Puget and I. Lustig. Program does not equal program: Constraint programming and \nits relationship to mathematical programming. Interface, 31:29 53, 2001. [34] P. Schmidt-Nielsen. RALA \nCAD. To be published, 2009. [35] M. Sokolich, A. Kramer, Y. Boegeman, and R. Martinez. Demonstra\u00adtion \nof sub-5 ps cml ring oscillator gate delay with reduced parasitic alinas/ingaas hbt. IEEE Electron Device \nLetters, 22:309 311, 2001. [36] T. Toffoli and N. Margolus. Cellular Automata Machines: A New Environment \nfor Modeling. MIT Press, Cambridge, MA, 1991. [37] A. Turing. Computing machinery and intelligence. \nMind, 59:433 560, 1950. [38] G. Venkataramani, M. Budiu, T. Chelcea, and S. C. Goldstein. C to asynchronous \ndata.ow circuits: An end-to-end tool.ow. In IEEE 13th International Workshop on Logic Synthesis (IWLS), \nTemecula, CA, June 2004. [39] B. Vigoda, H. Dauwels, M. Frey, N. Gershenfeld, T. Koch, H.-A. Loeliger, \nand P. Merkli. Synchronization of pseudo-random signals by forward-only message passing with application \nto electronics circuits. IEEE Transactions of Information Theory, 52:3843 3852, 2006. [40] J. von Neumann. \nFirst draft of a report on the EDVAC, 1945. [41] J. von Neumann. Theory of Self-Reproducing Automata. \nUniversity of Illinois Press, Urbana, 1966. Edited and completed by Arthur W. Burks. [42] H. Wang. Proving \ntheorems by pattern recognition II. Bell System Tech. Journal, 40:1 41, 1961.  \n\t\t\t", "proc_id": "1706299", "abstract": "<p>Computer science has served to insulate programs and programmers from knowledge of the underlying mechanisms used to manipulate information, however this fiction is increasingly hard to maintain as computing devices decrease in size and systems increase in complexity. Manifestations of these limits appearing in computers include scaling issues in interconnect, dissipation, and coding. Reconfigurable Asynchronous Logic Automata (RALA) is an alternative formulation of computation that seeks to align logical and physical descriptions by exposing rather than hiding this underlying reality. Instead of physical units being represented in computer programs only as abstract symbols, RALA is based on a lattice of cells that asynchronously pass state tokens corresponding to physical resources. We introduce the design of RALA, review its relationships to its many progenitors, and discuss its benefits, implementation, programming, and extensions</p>", "authors": [{"name": "Neil Gershenfeld", "author_profile_id": "81100302788", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911158", "email_address": "", "orcid_id": ""}, {"name": "David Dalrymple", "author_profile_id": "81453651966", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911159", "email_address": "", "orcid_id": ""}, {"name": "Kailiang Chen", "author_profile_id": "81453623547", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911160", "email_address": "", "orcid_id": ""}, {"name": "Ara Knaian", "author_profile_id": "81453658183", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911161", "email_address": "", "orcid_id": ""}, {"name": "Forrest Green", "author_profile_id": "81453628243", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911162", "email_address": "", "orcid_id": ""}, {"name": "Erik D. Demaine", "author_profile_id": "81100553346", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911163", "email_address": "", "orcid_id": ""}, {"name": "Scott Greenwald", "author_profile_id": "81453619481", "affiliation": "MIT Center for Bits and Atoms, Cambridge, USA", "person_id": "P1911164", "email_address": "", "orcid_id": ""}, {"name": "Peter Schmidt-Nielsen", "author_profile_id": "81453661731", "affiliation": "MIT Center for Bits and Atoms, Cambridge, MA, USA", "person_id": "P1911165", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706301", "year": "2010", "article_id": "1706301", "conference": "POPL", "title": "Reconfigurable asynchronous logic automata: (RALA)", "url": "http://dl.acm.org/citation.cfm?id=1706301"}