{"article_publication_date": "01-17-2010", "fulltext": "\n Toward a Veri.ed Relational Database Management System * Gregory Malecha Greg Morrisett Avraham Shinnar \nRyan Wisnesky Harvard University, Cambridge, MA, USA {gmalecha, greg, shinnar, ryan}@cs.harvard.edu \nAbstract We report on our experience implementing a lightweight, fully ver\u00adi.ed relational database management \nsystem (RDBMS). The func\u00adtional speci.cation of RDBMS behavior, RDBMS implementation, and proof that \nthe implementation meets the speci.cation are all written and veri.ed in Coq. Our contributions include: \n(1) a com\u00adplete speci.cation of the relational algebra in Coq; (2) an ef.cient realization of that model \n(B+ trees) implemented with the Ynot ex\u00adtension to Coq; and (3) a set of simple query optimizations proven \nto respect both semantics and run-time cost. In addition to describ\u00ading the design and implementation \nof these artifacts, we highlight the challenges we encountered formalizing them, including the choice \nof representation for .nite relations of typed tuples and the challenges of reasoning about data structures \nwith complex shar\u00ading. Our experience shows that though many challenges remain, building fully-veri.ed \nsystems software in Coq is within reach. Categories and Subject Descriptors F.3.1 [Logics and mean\u00adings \nof programs]: Mechanical veri.cation; D.2.4 [Software En\u00adgineering]: Correctness proofs, formal methods, \nreliability; H.2.4 [Database Management]: Relational databases, query processing General Terms Languages, \nVeri.cation Keywords relational model, dependent types, separation logic, B+ tree 1. Motivation Relational \ndatabase management systems (RDBMSs) have become ubiquitous components of modern application software. \nFor exam\u00adple, SQLite, a lightweight RDBMS, ships as a component of Fire\u00adfox, Skype, SymbianOS, and McAfee \nAntivirus, among others. In many of these applications, the RDBMS is used to store data whose integrity \nand con.dentiality must be strictly maintained (e.g., .\u00adnancial records or security credentials). In \nan ideal world, an ap\u00adplication developer would be provided with a high-level speci.ca\u00adtion for the behavior \nof the data manager, suitable for formal (i.e., mechanical) reasoning about application-level security \nand correct\u00adness properties. Furthermore, the implementation of the data man\u00ad * This research was supported \nin part by National Science Foundation grant 0910660 (Combining Foundational and Lightweight Formal Methods \nto Build Certi.ably De\u00adpendable Software), NSF grant 0702345 (Integrating Types and Veri.cation), and \na NSF Graduate Research Fellowship. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 10, January 17 23, 2009, Madrid, Spain. Copyright c &#38;#169; \n2009 ACM 978-1-60558-479-9/10/01. . . $10.00 ager would be proven correct with respect to this speci.cation \nto ensure that a bug cannot lead to accidental corruption or disclosure. It is for these reasons that \nwe see veri.ed RDBMSs as a compelling challenge to the programming languages and software veri.cation \ncommunities that moves beyond the now successful domains of veri.ed compilers and theorem provers. As \na step towards this goal, we have constructed a veri.ed, lightweight, in-memory RDBMS using the Coq proof \nassistant [2]. Currently, our RDBMS supports queries, written in a stylized subset of SQL, over an in-memory \nrelational store that can be [de]serialized to disk. As such, it provides much of the function\u00adality \nneeded for single-threaded client applications, but lacks the ACID properties (Atomicity, Consistency, \nIsolation, Durability) necessary in a concurrent, persistent storage system. The relational store is \nmodeled using .nite sets of typed tuples, and query seman\u00adtics are expressed in terms of this model. \nBefore execution, a query is transformed by a simple and provably semantics-preserving opti\u00admizer. The \nresulting optimized query is then mapped to a sequence of low-level operations over B+ trees. We implement \nB+ trees using Ynot [15], an axiomatic extension to Coq that provides facilities for writing and reasoning \nabout imperative, pointer-based code. The design and implementation are highly modularized to sup\u00adport \ncode (and proof) re-use, and to enable alternative implemen\u00adtations. For example, the query execution \nengine works in terms of a generic .nite map interface that can be realized with hash tables or other \ndata structures besides B+ trees. As another example, the query optimizer can be extended with semantic \noptimizations that exploit a priori knowledge about relations, as long as appropriate (semantic) certi.cates \nof that knowledge can be presented to it. The goal of this paper is to describe our veri.ed, lightweight \nRDBMS and discuss the challenges we faced using a proof devel\u00adopment environment such as Coq to build \nthe system. Foremost among these were: Choosing an appropriate encoding of the relational model. In\u00adformally, \na relation is simply a .nite set of tuples over some basic value types. In Coq, there are many ways to \nrepresent such relations, each with different tradeoffs. For example, pre\u00advious work encoding relational \nalgebra in Agda suggests that schemas should be represented as functions from a .nite set of column names \nto basic types [9], but in practice, we found that a concrete encoding using a list of type names yields \na more workable representation.  Another choice was how to represent .nite sets. Finite sets are a common \nabstraction and Coq conveniently provides them as a standard library. Unfortunately, the library is currently \ncoded as a compile-time, ML-style functor parameterized by a .xed element type. This is too restrictive \nfor our RDBMS, which must determine the type parameter at run-time.  Finally, we found formalizing correctness \nproofs for compli\u00adcated, pointer-based data structures particularly dif.cult despite previous work in \nthis area [3, 23]. B+ trees, which are gener\u00ad   alized binary search trees with leaves connected by \na linked\u00adlist skirt for rapid traversal, are often used to index data and are crucial for query execution \nperformance. However, they are tricky to get right. BerkeleyDB, a high-performance embedded RDBMS that \nhas hosted Google s accounts information [20], experiences around two dozen B+ tree-related bugs per \nversion, according to its change-log. The Ynot extension to Coq was designed to support writing and proving \ncorrect this kind of pointer-based code, using a variant of separation logic [18, 22]. Separation logic \nmakes it particularly easy to reason about data structures with local pointer structure, such as trees. \nHowever, in the case of B+ trees, the pointer structure does not directly .t this pattern, which leads \nto complicated invariants and proofs of correctness. Industrial strength RDBMSs include features which \nwe have not yet implemented, such as indices and sophisticated query planning, as well as features which \nwe can not yet reason about using our sys\u00adtem. Nevertheless, the above challenges must be addressed before \nmore sophisticated implementations can be veri.ed. Outline This paper is structured as follows. In the \nnext section we give an overview of what our RDBMS does, what we verify, and how the RDBMS is implemented. \nThe sections after that present the speci\u00ad.cation and implementation in detail. We then discuss what \ndevel\u00adopment was actually like, give the lessons we learned, and provide measurements about veri.cation \noverhead. We conclude with com\u00adparisons to related formalizations of the relational algebra in Agda. \nThe source code is available at http://ynot.cs.harvard.edu. Note that for purposes of exposition we will \nsometimes omit infer\u00adable arguments and take other notational liberties. 2. Overview We have constructed \na simple, fully veri.ed, in-memory RDBMS. A command line interface lets users create tables, load tuples \ninto a table, save/restore a table to/from disk, and query the tables using a subset of SQL. The main \nveri.cation task is showing that the RDBMS correctly executes queries with respect to a denotational \nsemantics of SQL and relations. Execution includes parsing SQL concrete syntax into abstract syntax trees \n(ASTs), transforming the ASTs into relational algebra expressions, performing source-to\u00adsource optimizations \non the relational algebra, and then interpreting queries as series of operations over B+ trees, as shown \nin Figure 1. The RDBMS has .ve main components, each of which has been implemented in Coq: The relational \nalgebra model (Section 3) de.nes schemas, rela\u00adtions, and declarative speci.cations of query operations. \n The SQL compiler (Section 4) includes the parser, de.nitions for SQL abstract syntax, a denotational \nspeci.cation for SQL in terms of the model, and semantics-preserving SQL optimiza\u00adtions. We also formulated \na run-time cost model and proved that several transformations are not cost-increasing.  The SQL execution \nengine (Section 5) interprets the optimized SQL expression as a series of operations over imperative \n.nite maps. Correctness is established using Hoare-style reasoning relating imperative .nite maps to \nthe relations they represent.  The B+ tree implementation (Section 6) provides .nite map op\u00aderations \nfor insertion and lookup of key-value pairs, and itera\u00adtion (amongst others). These imperative operations \nare coded with the Ynot extension to Coq and veri.ed using a variant of separation logic.  Figure 1. \nRDBMS Architecture. The storage interface (Section 7) is responsible for [de]serializing relations to \ndisk and establishing integrity constraints. The stor\u00adage manager includes a proof that deserializing \nthe serialized form of a relation R results in R, under the assumption that disk operations do not fail. \nThe Coq extraction facility allows us to run our code by trans\u00adlating it to OCaml. During this process, \nnon-computational content used only in speci.cations, such as the relational model, is erased. The Ynot \nlibrary provides imperative OCaml de.nitions for Coq axioms (e.g., Ynot references become OCaml references \nmanipu\u00adlated with standard effectful OCaml read/write operations). 3. The Model The relational algebra \nhas a standard de.nition in terms of set theory, so a large portion of our RDBMS speci.cation deals with \nrealizing both sets and the relational algebra in Coq. We begin with an informal overview of relational \nconcepts and then discuss how each of these is realized within Coq. As is standard, we model a database \nusing relations. A relation can be represented as a .nite set of tuples over a list of primitive types. \nIt is helpful to think of a relation as a table (with no duplicate rows) where the rows represent entries \nand the columns represent attributes of an entry. The list of primitive types that describes the columns \nis known as the schema for the relation. Tuples in a relation are indexed by a set of attribute names. \nTo simplify our implementation, we use the position (column) of an element as the attribute name, but \nit would be easy to support another index set by maintaining a mapping from names to offsets. New relations \nare constructed using a basis of operations:  Selection. Given a predicate P , selection returns the \nsubset of the relation s tuples that satisfy P .  Projection. Restricts each tuple in a relation to \na subset of the relation s columns.  Permutation. Permutes a relation s columns.  Union. Returns the \nset-theoretic union of two relations.  Difference. The difference of relation A and B returns the result \nof removing every tuple in B from A.  Cartesian Product. The cartesian product of relation A with n \ncolumns and relation B with m columns consists of every (n + m)-tuple that can be formed by concatenating \na tuple in A with a tuple in B.  This basis is relationally complete, and equal in expressive power \nto other relational formalisms, like relational calculus [1]. However, the choice of basis is somewhat \narbitrary; for example, we could have chosen to use join and recovered cartesian product as a degenerate \ncase. The ordered, nameless schema representation is a design deci\u00adsion that we believe simpli.ed reasoning \nand implementation. To accommodate an unordered, named representation would require a renaming operation \ninstead of permutation. 3.1 Schema In our model, the schema for a relation is de.ned as follows:1 Parameter \ntname : Set. Parameter tnameDenote : tname -> Set. Definition Schema : Set := list tname. Thus, a schema \nis represented as a list of type names, and type names can be mapped to Coq types by a denotation function. \nThe de.nition for type names and the denotation function are parame\u00adters to the system so that users \ncan easily add new constructors to the set of schema types. For example, we might de.ne tname as the \ninductive de.nition: Inductive tname : Set := | Nat : tname | Bool : tname | Str : tname | Option : tname \n-> tname. and de.ne the denotation function as: Fixpoint tnameDenote (t:tname) : Set := match t with \n| Nat => nat | Bool => bool | Str => string | Option t => option (tnameDenote t ) end. where nat, bool, \netc. are the corresponding Coq types. The values that make up tuples are inhabitants of the denoted Coq \ntypes. For example, \"Abc\" : tnameDenote Str and 17 : tnameDenote Nat We need to be able to compare schemas \nfor equality (e.g., to check that an optimization is admissible). Equality between arbitrary Coq types \nis undecidable, so we require decidable equality on type names as another parameter to the system: Parameter \ntname_dec_eq : forall (n1 n2: tname), {n1=n2} + {n1<>n2}. 1 Here Set refers to Coq s type universe, not \nsets in the sense of relations. Additionally, we require that for any type name, the Coq type T that \nit denotes satis.es the following properties: 1. T must be a decidable setoid; that is, come equipped \nwith a decidable equivalence relation.  2. T must be a decidable total order; that is, come equipped \nwith a decidable total ordering compatible with the setoid.  3. T must be serializable; that is, come \nequipped with a pair of functions ser : T . string and deser : string . option T such that .x:T, deser(ser \nx)= Some x.  Like decidable equality for type names, these properties on denota\u00adtions are given as \nparameters to the system. Property (1) allows for equivalence relations on column types that are weaker \nthan syntac\u00adtic (Leibniz) equality. For instance, we can treat strings in a case\u00adinsensitive way. Property \n(2) is required because of the way we build sets of tuples, and property (3) is used for persistence. \n 3.2 Tuples As we have seen, each column in a relation has a type, and a tuple associates a value of \nthe appropriate type to each column. That is, a tuple is a heterogeneously-typed list. The type of a \ntuple is given by a recursive, type-level function (where :: is list cons) : Fixpoint Tuple (A: Schema) \n: Set := match A with | nil => unit | n :: t => tnameDenote n * Tuple t end. Tuples are essentially iterated \npairs of values, terminated by a unit (the single inhabitant of unit is written tt). For example, Definition \naSchema : Schema := Str :: Nat :: Bool :: nil. Definition aTuple : Tuple aSchema := (\"Hello world\", (17, \n(true, tt))). To express the relational operations we need to de.ne several tuple manipulation functions. \nFor example, to perform product we need an operation with the following type to fuse tuples: fuseTuples \n(I J: Schema)(x: Tuple I)(y: Tuple J) : Tuple (I ++ J). The type of this function ensures that the schema \nof the resulting fused tuple is the concatenation of the input schemas. We also use the richness of Coq \ns type system to help simplify reasoning about error cases. For instance, to project out the type name \nof a particular column n from a schema I, we need to provide a proof that witnesses that n is less than \nthe length of I: colType (I:Schema) (n:nat) (pf:n<length I) : tname. The operation to project a single \ncolumn from a tuple uses colType in its type: projTupleCol (I: Schema) (n: nat) (pf: n < length I) (t: \nTuple I) : tnameDenote (colType I n pf). We implement multi-column projection by iterating colType and \nprojTupleCol to obtain colTypes and projTupleCols.  3.3 Relations as Finite Sets The next choice we \nconsider is how to represent .nite sets in Coq. Coq provides the FSets library for this, but we could \nnot use it directly. The library is coded as an ML-style functor which requires the static determination \nof the carrier. Our RDBMS must compute this type from a schema at run-time, because it does not know \ntable schemas until the user actually loads data at run-time. Rather than try to encode such behavior \nusing modules, we modi.ed the FSet library to be .rst-class using Coq s type class mechanism.  Type \nclasses are a recent addition to Coq and behave similarly to their Haskell counterparts [27]. They allow \nthe user to overload a set of operations across a class of types. We have a class of types FSetInterface \nthat is parameterized by a type elt of elements and a total ordering E over elt that can be used as speci.cations \nof .nite sets. Here Prop indicates Coq s type of computationally irrelevant propositions: Class FSetInterface \n(elt: Set) (E: OrderedType elt) : Type := { Fset: Set; (* the abstract type of finite sets *) (* operations \n*) empty : Fset; union : Fset -> Fset -> Fset; is_empty : Fset -> bool; ... (* predicates *) In : elt \n-> Fset -> Prop; Equal := fun s s => forall a : elt, In as<->Ina s ; ... (* axioms *) union_1 : forall \ns s x, In x (union s s ) -> In xs\\/Inxs ; union_2 : forall s s x, In x s -> In x (union s s ); ... }. \nIn addition to specifying the operations that de.ne the class, we specify a set of axioms that allow \nus to reason about the operations. We must also show that this speci.cation is realizable, which we do \nby providing a simple implementation using lists. Given that we require the element types to be ordered, \nan al\u00adternative implementation of a .nite set would be as a sorted list with a proof that the list contains \nno duplicates. The advantage of this concrete representation, over our axiomatic interface, is that we \ncould: (1) derive the axioms directly from the representation; and (2) rewrite directly using Leibniz \nequality. The disadvantage, of course, is that the interface would not be as re-usable in other application \ncontexts where we desire a weaker notion of equality. In general, we have found that the richness of \nCoq, including support for ML-style modules, dependent records, and type classes, coupled with abstraction \nand equality issues yields a set of design tradeoffs that are dif.cult to evaluate without exploring \n(by coding) many alternatives. One set of tradeoffs is present in any RDBMS formalization; for instance, \nwhether to name columns or use col\u00adumn numbers. Another set of tradeoffs arises only because certain \nforms of reasoning appear to be more effective in Coq and would not appear in a paper and pencil formalization; \nfor instance, the choice of axiomatic sets instead of sets as concrete lists. Both sets of tradeoffs \nare equally important for veri.ed software because ev\u00adery detail must be checked. Unfortunately, it can \noften take signi.\u00adcant time to fully understand the Coq-engineering consequences of a seemingly inconsequential \ndesign choice; for instance, whether to represent schema as lists of type names or as functions from \ncol\u00adumn numbers to type names. At present it is unclear how best to explore the design space.  3.4 Relational \nAlgebra in Coq We de.ne the relational operations over .nite sets of schema\u00adtyped tuples (relations). \nBuilding relations requires de.ning a total ordering over tuples and interacting with the type class \nmechanism, but, for purposes of exposition, we essentially have that: Definition Relation (I:Schema) \n:= FSetInterface (Tuple I). Union, difference, and selection are implemented in terms of the FSetInterface \nunion, difference, and filter functions. Pro\u00adjection and product are de.ned using the generic fold function \nprovided by the FSetInterface. Selection allows any Coq predicate that respects the setoid equality of \nthe schema column types to be used, but our SQL syntax is less expressive. Projection is implemented \nby iterating through a set, projecting out each tuple individually. Cartesian product is only slightly \nmore complicated, requiring two iterations. To com\u00adpute the product of A and B, for each a . A we compute \nthe set { fuseTuples ab | b . B } and then union the results. In addition to de.ning the relational operations, \nwe need a number of lemmas to support basic reasoning. For instance, when we project out column n from \nschema I, using a proof pf that n is less than the length of I, we must demonstrate that we get the same \nresult as when we use a different proof pf'. (This lemma is proof irrelevance for colType). These foundational \nlemmas are typically easy to prove but nonetheless required. Similarly, our use of dependent types means \nthat we often run into situations where we would like schemas to be de.nitionally equal but they are \nonly propositionally equal, and so we are forced to reason about equality explicitly. This leads to more \nverbose theorem statements and proofs. To gain con.dence in the accuracy of our model of the relational \nalgebra, we have shown that several dozen standard equivalences are derivable from our de.nitions. We \nalso use these identities to justify the correctness of our query optimizations. Some equivalences are \nuniversally valid; for example, the com\u00admutativity of selection: select P1 (select P2 R) = select P2 \n(select P1 R) Other equivalences only apply in the presence of constraints on the input relations. For \ninstance, let A and B be relations over schemas I and J, respectively, and let l indicate the columns \n0...|I|- 1. Then we have the following conditional equivalences: B <> empty -> proj l (prod A B) = A \nB = empty -> proj l (prod A B) = empty Proving this statement requires reasoning about how projection \nof l can be pushed through the nested iteration that de.nes the product. The actual proof proceeds much \nlike on paper: .rst we demonstrate that every element in the product of A and B must be a fused tuple \nof the form fuseTuples xy, and that fusing x with y followed by projection by l yields x. The theorem \nthen follows from showing that every element of A has at least one corresponding fused tuple in the product \nof A and B, provided B is non-empty. We constructed proofs manually, but it may be possible to adapt \nan automated theorem prover for relations (e.g., [25]) to Coq to reduce the proof burden. 4. The SQL \nCompiler In the previous section we described our Coq encoding of a model for relational data and algebraic \noperations on relations. In this section we describe the front-end of our SQL implementation and relate \nit to this model. 4.1 Abstract Syntax We de.ne a subset of SQL abstract syntax with a Coq data type \nthat is indexed by the schema of the relation that the expression denotes.  Variables, which are really \ntable names, are represented as strings and are explicitly typed. The overall effect is that queries \nare well\u00adtyped by construction and coupled with our other design decisions means that they can always \nbe given a meaning in terms of the relational algebra. Queries are de.ned by the following inductive \ndata type: Inductive Query : Schema -> Set := | varExp : forall I, string -> Query I | unionExp : forall \nI, Query I -> Query I -> Query I | diffExp : forall I, Query I -> Query I -> Query I | selectExp : forall \nI, whereExp I -> Query I -> Query I | projExp : forall I (l:list nat)(pf: bounded I l), Query I -> Query \n(colTypes l pf) | prodExp : forall (I J: Schema), Query I -> Query J -> Query (I++J). The projection \nexpression requires a proof that the columns l that de.ne the projection are each less than the length \nof I. Selection is de.ned using an additional syntax for predicates over tuples. Our de.nition of selection \nlets us use arbitrary Coq functions (that re\u00adspect the setoid equality), but we restrict users to only \ncertain kinds of predicates (whereExps). We support boolean combinations of comparisons between columns \nand constants. That is, Inductive atomicExp (I : Schema) : tname -> Set := | const: forall t (c:tnameDenote \nt), atomicExp I t | col : forall n (pf: n < length I), atomicExp I (colType I n pf). Inductive compareExp \nI : Set := | compareEq : forall t, atomicExp I t -> atomicExp I t -> compareExp I | compareLt ... Inductive \nwhereExp I := | compExp: compareExp I -> whereExp I | andExp : whereExp I -> whereExp I -> whereExp I \n| orExp ...  4.2 Parsing The RDBMS uses Ynot s packrat PEG parser [5] to parse user input. The parser \nis implemented as a veri.ed compiler [12]: given a speci.cation consisting of a PEG grammar and semantic \nactions, the parser creates an imperative computation that, when run over an arbitrary imperative character \nstream, returns a result that agrees with the speci.cation. To make the parsing ef.cient, the packrat \nalgorithm used by the resulting computation uses a sophisticated caching strategy that is implemented \nusing imperative hash tables. Queries written in our abstract syntax resemble SQL statements, but to \nsupport some features of SQL our system must express the operations in terms of the basis we have chosen. \nSQL supports, for instance, syntax for both cartesian product and join, but our basis only has product. \nHence an SQL-ish join query, such as: SELECT 0, 1, 2 FROM (JOIN tbl1 , tbl2 ON col 0 = col 0) WHERE col \n0 = \"hello world\" AND col 1 < col 3 is translated by our system into a product, selection, and projection: \nSELECT 0, 1, 2 FROM (SELECT 0, 1, 2, 4, 5 FROM tbl1, tbl2 WHERE col 0 = col 3) WHERE col 0 = \"hello world\" \nAND col 1 < col 3 assuming that tbl1 has 3 columns (indexed 0-2 in the product). During query optimization, \nselection fusion will optimize this query (Section 4.4). 4.3 SQL Semantics To give a query a meaning \nin terms of the relational algebra we need to know the relations that correspond to the query s variables. \nThese relations correspond to the actual data that the user has loaded, and the association between the \nvariables and the tables is captured using the traditional mechanisms of a context and an environment: \nDefinition Ctx := list (string * Schema). Fixpoint Env (G: Ctx) : Set := match G with | nil => unit | \n(_, J) :: b => Relation J * (Env b) end. When an environment has a (properly-typed) relation corre\u00adsponding \nto each variable in a query (which is easy to check), the denotational semantics of a query is de.ned \nby recursively apply\u00ading the relational operations or looking up the value of a table in the environment \nas necessary: Fixpoint denote (I: Schema) (q: Query I) (G: Ctx) (E: Env G) : Relation I := match q with \n| varExp J v => lookup E I v | unionExp J a b => union (denote a) (denote b) | diffExp J a b => diff \n(denote a) (denote b) | selectExp J r f => select (whereDenote f) (denote r) | projExp J l pf e => proj \nl pf (denote e) | prodExp I J a b => prod (denote a) (denote b) end. This de.nition uses an auxiliary \ndenotation function whereDenote: whereExp I -> Tuple I -> bool with the obvious de.nition.  4.4 Source-to-Source \nOptimization A query optimization is a semantics-preserving source-to-source transformation (over the \ntyped SQL syntax) that ideally reduces execution time. For instance, common optimizations include reduc\u00ading \nthe number of joins in a query and pushing selection towards the leaves. These work well in practice \nbecause joins tend to dom\u00adinate query execution time, and selection tends to reduce relation size. In \nour system optimization is a distinct phase before execution. We have implemented a number of optimizations, \nincluding the following three from a standard introductory database textbook [6]: Selection fusion. \nThe query select P2 (select P1 Q) can be transformed to a single selection: select (P1 and P2) Q.  Projection \nfusion. The query proj l2 (proj l1 Q) can be transformed to use a single projection: proj (l2 . l1) Q, \nwhere (l2 . l1) is the composition of permutations l1 and l2.  Selection-Projection re-ordering. The \nquery select P (proj l Q) can be transformed to proj l (select P Q), where P is P extended to operate \nover tuples with additional columns.  In principle any of the several dozen algebraic identities that \nwe have proven can be used as the basis of a transformation, although not all will be bene.cial.  4.4.1 \nCost Estimation We would like a lightweight way to check whether or not our op\u00adtimizations are likely \nto reduce running time. We can estimate the execution time and resulting relation size for a query by \nmaking conservative assumptions about the data and the algorithms being used to implement the relational \noperations. Currently, we assume that selection and projection must iterate through every tuple in a \nrelation, and that computing the union, difference, or product of two relations requires comparing every \ntuple in both relations. Of course, these assumptions are overly pessimistic in practice, union can \nbe an O(n + m) operation assuming the data are sorted, not an O(n\u00b7m) one but we are only interested \nin the difference be\u00adtween the running time of the query before and after optimization. Because this \nsimple cost model is data-independent, it does not in\u00adcorporate useful assumptions such as how selection \ndecreases the cardinality of relations. However, even using this simple model we can still show that \nsome of our optimizations, like selection fusion, do not increase cost.  4.4.2 Semantic Optimization \nSemantic optimizations [8] are rewrites that are semantics-preserving only because of the speci.c constraints \non the particular database instance at hand. Our proj-prod equivalence for relations A and B of schemas \nI and J where l is the columns 0...|I|- 1: B <> empty -> proj l (prod A B) = A B = empty -> proj l (prod \nA B) = empty can be used as such an optimization because which rewrite to per\u00adform depends on whether \nor not B is empty. Our RDBMS tracks the emptiness of user tables through load operations and provides \nthis information to a semantic optimization, which can rewrite and witness the semantics preservation \nwith the available empti\u00adness proof. Semantic information about relations is maintained in RelInfo records: \n(* Relation information *) Record RelInfo := relInfo { isEmpty : bool }. (* Meta data for each table \n*) Definition DbInfo : string -> Schema -> RelInfo. Definition ra_sem_rewrite : Set := DbInfo -> (forall \nI, Query I -> Query I). In the future, this record can be elaborated with additional statis\u00adtics, such \nas the number of tuples in the relation and selectivity properties of certain columns, allowing the optimizer \nto optimize queries more intelligently. For example, we might choose an order for symmetric operations \nso as to iterate over the smaller relation. The optimization associated with the proj-prod equivalence \nidenti.es expressions of the form proj l (prod qv), where q is a query over schema I, and v is a variable \nof schema J. The optimization uses the DbInfo to decide which rewrite to apply. We need one .nal component \nto ensure that semantic optimiza\u00adtions are used correctly: we need to make sure that the RelInfo information \nassociated with the database is accurate. This is nec\u00adessary to prove that the semantic optimizations \nare in fact meaning preserving. Therefore we guarantee that our database is always in a state such the \nfollowing invariant holds: Definition accurate (m: DbInfo) (G: Ctx) (E: Env G) : Prop := forall s I, \ngetRelation s I E = empty <-> isEmpty (m s I) Semantic optimizations are well studied but have traditionally \nbeen dif.cult to deploy. In [8] the authors argue that the limiting factor for industrial deployment \nof semantic optimizations is the lack of support for explicitly manipulating symbolic constraints. One \nbene.t of our formalization is that semantic optimization be\u00adcomes less risky because we are required \nto prove semantic preser\u00advation formally. 5. The SQL Execution Engine Once a query has been optimized, \nour RDBMS executes the query using a sequence of operations over imperative .nite maps. Antic\u00adipating \nthe use of keys for indexed retrieval (see Section 5.3), we use maps from keys to values, instead of \nsets. Our .nite maps are implemented using B+ trees as described in Section 6, but the en\u00adgine is insulated \nfrom this detail by working in terms of a generic interface. Indeed, it is possible to replace the implementation \nwith an alternative, such as a .nite map based on hash tables. In this section, we describe the .nite \nmap interface, how SQL queries can be na\u00a8ively implemented in terms of the interface, and how key con\u00adstraints \ncan be utilized to optimize query execution. 5.1 The Finite Map Interface The .nite map interface is \nspeci.ed with two components: the .rst is a functional speci.cation that describes .nite maps as simple \nas\u00adsociation lists that map keys to values. The second is an imperative speci.cation, in the style of \nHoare Type Theory (HTT) [14] and realized by the Ynot extension to Coq [15]. In an HTT speci.ca\u00adtion, \nwe describe operations over an imperative Abstract Data Type (ADT) using commands indexed by pre-and \npost-conditions. We use the functional model to describe the state of the ADT before and after the command \nis executed and to relate the result of the command to the pre-and post-state of the ADT. The functional \nstate of a .nite map is given by a sorted associ\u00adation list of key-value pairs. The ADT operations, such \nas lookup and insert, have simple de.nitions as pure Coq code: Parameters key value : Set. Definition \nAssocList := list (key * value). Fixpoint specLookup (k : key) (m : AssocList) : option value := match \nm with | nil => None | (k ,v) :: b => if k = k then Some v else specLookup k b end. Fixpoint specInsert \n(k : key) (v : value) (m : AssocList) : AssocList := match m with | nil => (k, v) :: nil | (k ,v ) :: \nb => match compare k k with | LT => (k ,v ) :: specInsert k v b |EQ=> (k,v)::b | GT => (k ,v ) :: (k \n,v ) :: b end end. The imperative interface is shown in Figure 2. It speci.es an abstract type handle \nthat represents a handle on the imperative state of the .nite map. Next, the interface requires a predicate \nrep that connects a handle to an association list in a particular state. The intention is that the predicate \nrep he should hold in a heap when the ADT h represents the association list e.  (* The abstract type \nof imperative .nite maps. *) Parameter handle : Set. (* The predicate rep hm holds in a heap when the \n.nite map h represents the association list m. *) Parameter rep : handle -> AssocList key value -> heap \n-> Prop. (* Newly created .nite maps are empty (emp is the empty heap assertion in separation logic), \nand so represent the nil association list. *) Parameter new : Cmd emp (fun h : handle => rep h nil). \n(* Destroy a .nite map, reclaiming the memory used. The return type is unit. *) Parameter free : forall \n(h : handle) (m : AssocList), Cmd (rep h m) (fun _:unit => emp). (* The function lookup h k can be run \nin a state where h represents some model m, and it returns the value associated with k in m. In the post-state, \nh continues to represent m.*) Parameter lookup : forall (h : handle) (k : key) (m : AssocList), Cmd \n(rep h m) (fun res : option value => rep h m * [res = specLookup k m]). (* The function inserth kv can \nbe run in a state where h represents some model m. In the post-state, h represents m extended to map \nk to v. The operation returns the old value associated with k (if any). *) Parameter insert : forall \n(h : handle) (k : key) (v : value) (m : AssocList), Cmd (rep h m) (fun res : option value => rep h (specInsert \nv m) * [res = specLookup k m]). (* Fold a command over the elements of the map. The I serves as a loop \ninvariant on the command, relative to the list of elements that have been touched at each step in the \ncomputation. *) Parameter iterate : forall (T : Type) (h : handle) (I : T -> AssocList -> heap -> Prop) \n(tm : AssocList) (acc : T) (fn : forall (k: key) (v: value) (acc: T) lm, Cmd (I acc lm) (fun a:T => I \na (lm ++ (k, v) :: nil))), Cmd (rep h tm * I acc nil) (fun res:T => rep h tm * Exists tm , I res tm * \n[Permutation tm tm ]). Figure 2. The Imperative Finite Map Interface. The rest of the interface gives \nthe speci.cations for the .nite map operations. The speci.cations are encoded using a type Cmd of imperative \ncommands (also called computations) that are indexed by pre-and post-conditions over the heap. Cmd is \nsimilar to the IO and ST monads in Haskell except that the indices capture the behavior of the command \nin a Hoare-logic style. The intention is that given a Cmd PQ, we can run the command in any state satisfying \nP , and if that command terminates, then we are assured that Q holds on the .nal state. Furthermore, \nwe are guaranteed that the computation will not terminate prematurely due to type errors or other run-time \nerrors such as null-pointer dereferences. The pre-and post-conditions are speci.ed using a variant of \nseparation logic [18, 22], and commands are constructed to satisfy the frame rule with respect to separating \nconjunction. In other words, if c : Cmd PQ, then c also has type Cmd (P * R)(Q * R) for any heap predicate \nR, where * is separating conjunction. For example, the new command speci.es emp as a pre-condition and \nthus demands that the heap is empty. It returns a handle h and terminates in a state where h represents \nthe empty association list. Because new satis.es the frame rule, it can be run in any heap satisfying \na property R, and we are ensured that R continues to hold after executing the command. The free rule \nis dual to new. It requires that we pass in a handle which, in the initial state, represents some association \nlist m. The post-condition speci.es that the heap will then be empty. But as before, this can be run \nin any larger, disjoint heap satisfying a pred\u00adicate R, and we are ensured R continues to hold after \nexecution. The lookup operation takes a handle representing association list m and a key k and returns \nan optional value. The post-condition captures the facts that: (1) the returned value is equivalent to \ndoing a functional lookup on the model list m; and (2) the handle continues to represent m in the post-state. \nThe brackets around the expression res = specLookup k m are notation for lifting a pure predi\u00adcate (type \nProp) to a predicate over heaps (type heap -> Prop). The insert operation takes a handle h, key k, and \nvalue v such that h represents association list m on input, and it ensures that h represents the result \nof inserting (k, v) into m on output. The command also returns the old value (if any) that was assigned \nto k in the input state; this makes it easy to undo the insertion. The iterate operation is a higher-order \ncommand that iterates over the elements of the .nite map, applying another command fn to each key-value \npair and an accumulator to yield a .nal value. The computation is parameterized by an invariant I which \nis used to accumulate logical results about the iteration of fn. The separating conjunction ensures that \nfn cannot change the map during iteration. In the actual code, the association list parameters are marked \nas computationally irrelevant using the approach described in our previous work on Ynot [5]. This ensures \nthat these values are only used in types and speci.cations and do not affect the behavior of programs. \nConsequently, the extracted ML code does not need to include them, and these model values incur no run-time \noverhead.  5.2 Interpreting SQL Operations Our SQL engine executes queries using the .nite map interface. \nIn an industrial RDBMS, the engine would be able to choose from multiple relation representations and \nalgorithms (e.g. hash tables vs B+ trees, hash-join vs merge-join, etc.), and an input query would be \nexplicitly lowered into another intermediate form that more closely corresponds to these implementation \nchoices. This lowered form is also typically optimized again. In our case, however, we simply execute \nqueries by directly interpreting them as sequences of .nite map operations, as described below:  Union. \nTo union A and B, iterate through A, inserting each (k, v) . A into B.  Difference. To subtract B from \nA, iterate through A, inserting each a . A into a new relation when a/. B.  Projection. To project columns \nfrom A, iterate through A, inserting the projected tuples into a new relation.  Selection. To select \nrows from A, iterate through A, inserting tuples that meet the criteria into the new relation.  Product. \nTo compute the cartesian product of A and B, iterate through each a . A and, for each a . A, iterate \nthrough each b . B, inserting a fused tuple a + b into the new relation.  5.3 Taking Advantage of Key \nConstraints Key constraints are useful both for ensuring data integrity and for increasing performance. \nWhen a user loads a table into our RDBMS, they may indicate which of the columns are intended to form \na key for the table. Given a relation R over schema I, a subset i of I s columns is a key for R iff for \nevery two tuples x, y . R, we have that proj ix = proj iy implies x = y. A good example of a key is a \nperson s social security number; in principle, knowing this value is enough to uniquely identify a person. \nIn general, a relation may have many keys, and the set of all of a relation s columns always forms a \nkey. It is easy to check if a set of columns forms a key, and our RDBMS aborts the loading process if \na key constraint is violated. We are working toward (but have not completed) taking advan\u00adtage of key \nconstraints using the .nite map interface to execute point queries in logarithmic time using lookup. \nA point query is a degenerate range selection [7] where the predicate uniquely iden\u00adti.es a tuple. The \npredicate must consist of a set of equalities be\u00adtween columns and values such that the columns form \na key for the relation being scanned. For example, suppose we have a table with two columns, user name \nand password, where user name is a key. To determine a user s password, we might run the following query: \nSELECT 1 FROM users WHERE col 0 = \"Adam\" Since the name is a key we know that there exists at most one \nentry in the relation with the name Adam . Furthermore, if the name is indexed, then we can directly \ncall the .nite map s lookup operation which answers the query in logarithmic time rather than linear \ntime. 6. The B+ Tree Implementation At the core of our RDBMS is an implementation of the .nite map interface \nof Figure 2 using a B+ tree [1], a ubiquitous data structure when ordered key access is common. (In industrial \nRDBMSs, B+ trees are also used to build additional indices into relations). In this section we describe \nthe B+ tree implementation as well as the choices we made for representing the structure in separation \nlogic and reasoning about its correctness. A B+ tree is a balanced, ordered, n-ary tree which stores \ndata only at the leaves and maintains a pointer list in the fringe to make in-order iteration through \nthe data ef.cient. Figure 3 shows a simple B+ tree with arity 3. As with most tree structures, B+ trees \nare comprised of two types of nodes: Leaf nodes store data as a sequence of at most n key-value pairs \nin increasing order. Leaves use the trailing pointer position to store the pointer to the next leaf node. \n Branch nodes contain a sequence of at most n pairs of keys and subtrees. These pairs are ordered such \nthat the keys in a subtree are less than or equal to the given key (represented in  Figure 3. B+ tree \nof arity 3 with numeric keys and string values. the .gure as treeSorted min max). For example, the second \nsubtree can only contain values greater than 3 and less than or equal to 5. In addition to the n sub-trees, \nbranch nodes include a .nal subtree that covers the upper span. In the .gure, this is the span greater \nthan 5. B+ trees must maintain several additional invariants needed to ensure logarithmic-time insertion \nand lookup. First, non-root nodes must contain at least I n 2 l keys. Second, if the root node is a branch, \nit must contain at least 2 children. Third, all subtrees of a branch must be the same height. 6.1 B+ \nTree Representation and Invariants In order to meet the .nite map interface, we need to characterize \nB+ trees and their associated heaps. As previously mentioned, B+ trees are parameterized by an arity \n(previously n) which we will call SIZE which must be greater than 1, and, for simplicity when reasoning \nabout division by 2, even: Parameter SIZE : nat. Parameter SIZE_min : 1 < SIZE. Parameter SIZE_even : \neven SIZE. In the heap, we store records that represent nodes in the tree. These are represented by the \nfollowing Coq data type: Record bpt_node : Set := mkBpt { height : nat; content : array; next : option \nptr }. The content .eld stores an array of length SIZE; the type of values in the array are captured \nin the heap assertions. Following common C practice, we use the next .eld to encode the .nal child when \nthe node is a branch and the next pointer in the linked-list of leaves when the node is a leaf. Our next \ntask is to de.ne predicates that describe when a heap contains a tree with a valid shape. To simplify \nthis task, we .rst de.ne a functional model of the tree, which we call a ptree: Fixpoint ptree (h : nat) \n: Set := ptr * match h with | 0 => list (key * value) | S h => list (key * ptree h ) * ptree h end. In \nthis de.nition, ptrees are indexed by a height h. When the height is 0, the ptree consists of a pointer \nand list of keys and associated values. When the height is h ' +1 for some h ', the ptree consists of \na pointer, a list of keys and ptrees of height h ', and a .nal ptree of height h '. Our general strategy \nis to associate a ptree model with each B+ tree, to capture the salient aspects of its shape. We then \nimpose well-formedness constraints on the model, such as the requirement that the keys in a node are \nsorted.  We say that a ptree p of height h is a model of a B+ tree with root pointer r when the predicate \nrepTree hr None p holds. This predicate is de.ned to capture the shape constraints of B+ trees as described \nabove and uses two auxiliary predicates, repLeaf and repBranch discussed below: repTree 0 r optr (p ' \n, ls) .. [r = p ' ] *.ary. r . mkNode 0 ary optr * repLeaf ary |ls| ls repTree (h + 1) r optr (p ' , \n(ls, nxt)) .. [r = p ' ] *.ary.r . mkNode (h + 1) ary (ptrFor nxt) * repBranch ary (firstPtr nxt) |ls| \nls * repTree h (ptrFor nxt) optr nxt The repTree predicate has two cases, depending upon the ptree s \nheight. In both cases, we require that the root of the B+ tree r is equal to the pointer recorded in \nthe ptree (determined by ptrFor) and that r points to a node record with appropriate information, including \nan array. The firstPtr function computes the pointer of the .rst leaf in the given tree and is used to \nensure the link between adjacent subtrees. When the ptree height is zero, we require that the array holds \nthe list of key-value pairs in the ptree as de.ned by the repLeaf predicate: repLeaf ary n [v1, ..., \nvn] .. ary[0] . Some v1 * ... * ary[n - 1] . Some vn * ary[n] . None * ... * ary[SIZE - 1] . None When \nthe height of the ptree is non-zero, we require that the array holds a list of key-pointer pairs and \nthat these are valid models for the key-sub-tree pairs speci.ed by the ptree. These properties are captured \nby the repBranch predicate: repBranch ary n optr [(k1,t1), ..., (kn,tn)] .. ary[0] . Some (k1, ptrFor \nt1)* repTree h (ptrFor t1)(firstPtr t2) t1 * ...* ary[n - 2] . Some (kn-1, ptrFor tn-1)* repTree h (ptrFor \ntn-1)(firstPtr tn) tn-1* ary[n - 1] . Some (kn, ptrFor tn)* repTree h (ptrFor tn) optr tn * ary[n] . \nNone * ... * ary[SIZE - 1] . None These de.nitions are encoded as a set of mutually-recursive Coq .xpoints \nwhich describe, computationally, the heap-shape of a given ptree. In order to iterate over the elements \nof the B+ tree, we only need access to the leaves. Consequently, in addition to the repTree predicate, \nwe have de.ned predicates repLeaves and repTrunk which separate the tree into two disjoint components. \nThis facili\u00adtates reasoning about operations such as iteration, which only need access to the leaves. \nA critical lemma connects the two views: Theorem repTree_iff_repTrunk : forall h (r : ptr) (optr : option \nptr) (p : ptree h) (H : heap), repTree r optr p H <-> (repTrunk r optr p * repLeaves (Some (firstPtr \np)) (leaves p) optr) H. Here leaves extracts the leaves of a ptree. Bornat et al. [3] choose to use classical \nconjunction to capture these two views of the tree. We found it more convenient to prove an equivalence \nand forgo the conjunction, since most operations do not use the leaf list and it reduces the burden of \nproving additional facts about the leaf\u00adtrunk division. In addition to the heap shape described by repTree, \nthe model must satisfy the appropriate sortedness constraints. Re\u00adcall that this consists of an ordering \non the elements in a node and between the keys and subtrees as shown in Figure 3. More formally, the \nconstraints are captured by the predicate treeSorted: treeSorted 0 min [(k1, -), ..., (kn, -)] max .. \nmin < (Key k1) . k1 <k2 . ... . (Key kn) = max treeSorted (h + 1) min [(k1,t1), ..., (kn,tn)] max .. \ntreeSorted h min t1 (Key k1) . treeSorted h (Key k1) t2 (Key k2) . .... treeSorted h (Key kn-1) tn max \nwhere min and max are drawn from the inductively de.ned type: Inductive exkey: Set := | Min: exkey | \nKey: key -> exkey | Max: exkey. which provides minimal and maximal elements, and where key comparisons \nare lifted to exkey in the obvious way. Finally, to satisfy the .nite map interface of Figure 2 we must \nde.ne an abstract type handle that abstracts the details of the B+ tree implementation and hides the \ninternal de.nitions of ptrees and their associated properties. The abstract rep predicate relates B+ \ntree handles and key-value association lists in a given state. We begin by de.ning a function as map \nwhich extracts an association list of keys and values from a ptree: Fixpoint as_map (h : nat) : ptree \nh -> AssocList := match h as h return ptree h -> AssocList with |0 =>funm=>sndm |S h =>funm=> List.flat_map \n(fun x => as_map (snd x)) (contents m) ++ as_map (next m) end. Finally, we de.ne a handle to be a pointer \nto a pair, where the .rst component is the root of the B+ tree, and the second is the irrelevant ptree \nmodel for the B+ tree. These facts, along with the shape and sortedness invariants, are captured in the \nde.nition of the rep predicate, which is similar to the de.nition below: Definition handle : Set := ptr. \nDefinition rep (hdl : handle) (m : AssocList) : heap -> Prop := Exists (r : ptr) (h : nat) (p : ptree \nh), hdl --> (r, existT (fun h => [ptree h]) h p) * repTree h r None p * [m = as_map p] * [treeSorted \nh p Min Max]. In this de.nition, we have taken some notational liberties by using Exists to stand for \nheap-dependent existential quanti.cation and --> to represent the points-to relation. By packing the \nptree model with the actual B+ tree, we avoid the need to search for a model during proofs. Rather, as \nthe B+ tree is updated, we perform the corresponding (functional) updates on the associated model. This \nsort of ghost state is often useful for simplifying Hoare-style proofs. An alternative to packaging the \nB+ tree with its model is to simply show that there is at most one ptree that a given pointer and heap \ncan satisfy (i.e., that repTree is precise [4, 17]). However, this is complicated by the fact that the \ntypes of ptrees are indexed by height, and thus comparison (and substitution) demand the use of heterogenous, \nJohn Major equality [10].  6.2 Implementation of B+ Tree Operations In order to meet the .nite map speci.cation, \nwe must provide the .ve operations given in Figure 2. The implementations of new and free are relatively \nstraightforward. Both lookup and insert recurse over the tree to .nd the leaf which should contain a \ngiven key, perform the appropriate action on the leaf, and merge the result into the tree. Sexton and \nThi\u00adelecke [24] formulate this by building a language of tree-operations for a stack-machine. We take \na similar approach by factoring out the steps of .nding the appropriate leaf, performing an operation, \nand then merging the results. This is encapsulated in the traverse command which takes an operation cmd \nto apply at the leaf. The de.nition of traverse is sketched in Figure 4 as ML code.  let traverse tKey \ntree cmd = let (v,ms) = trav tkey !tree cmd in (tree := match ms with | Merge tr -> tr |Splitlk r-> buildTree \n((k,l)::nil) r end) ;v let rec trav tKey tree cmd = if isLeaf tree then cmd tree else let tr = findSubtree \ntKey tree in let (v,ms) = trav tKey tr cmd (v, match ms with | Merge tr -> replaceSubtree tKey tree tr \n| Split l k r -> spliceSubtree tKey tree l k r end) Figure 4. The traverse function for performing leaf-local \ntree computations. A call to traverse invokes a helper operation trav which searches for the appropriate \nleaf using tKey to guide the search and then executes cmd on that leaf. The execution of cmd, or a recursive \ncall to trav, results in a value v and either a new tree (Merge tr ) or a pair of trees l and r and a \nkey k (Split l k r) such that k is greater than or equal to the elements of l and less than the elements \nof r. In the former case, we simply replace the original sub-tree with the newly returned tree. In the \nlatter case, we must splice the two trees into the current node or, at the top\u00adlevel, build a fresh node \n(increasing the height of the tree by one.) The spliceSubtree operation must replace the old sub-tree \ntr with the new sub-trees l and r. In the easy case, we have room in the interior node s array, but must \nshift some elements over to make room. In the hard case, we may not have room and thus must split the \nnode in half and return a pair of trees to the caller. The key point is that traverse is general enough \nto support a range of operations, including both lookup and insert, while maintaining the B+ tree invariants. \nOf course, our Ynot code demands considerably more annota\u00adtion to capture the speci.cation of the cmd \nargument, and to re.ect the appropriate invariants for recursion, as well as the proofs that the code \nrespects pre-and post-conditions. For example, to describe the post-condition for cmd, we abstract over \na return type T for the value returned by cmd and predicate Q of type: Q : AssocList . T . AssocList \n. heap . P rop The predicate is intended to capture the effect of executing cmd by relating the association \nlist model before and after the command is executed, as well as the value returned by cmd. For example, \nwe can implement lookup by using a command with post-condition speci.ed by: Qlookup mpre rt mpost = [mpost \n= mpre] * [rt = specLookup tKey mpre]. Here, the model (i.e., list of key-value pairs in the leaves) \nremains unchanged, and the return value is equivalent to performing a functional lookup on the initial \nassociation list. We can implement insert by using a command with post-condition speci.ed by: Qinsert \nmpre rt mpost = [mpost = specInsert tKey tV alue mpre] * [rt = specLookup tKey mpre]. This captures the \nfact that the new model is obtained from the old by inserting the key-value pair into the association \nlist and that the returned value is equal to the result of looking up the key in the original model. \nIn addition, we need knowledge that the effect of running traverse (and hence trav) on the tree will \nonly affect the path from the root to the leaf of interest. That is, we must show that Q satis.es a frame \nproperty with respect to the rest of the model: .H min min ' max ' max tKey low i i ' hi rt, min ' < \ntKey = max ' . i '' sorted min ' i max ' . sorted min ' max . sorted min low min ' . sorted max ' hi \nmax . Q i rt i ' H . Q (low++i++hi) rt (low++i ' ++hi) H The frame properties for Qlookup and Qinsert \nfollow from the fact that specLookup and specInsert do not alter portions of the tree which do not intersect \nthe target key. The abstraction provided by traverse considerably reduces the burden of writing and verifying \nboth lookup and insert, since we only need to de.ne the operation to perform at the leaf. Further\u00admore, \nwe believe that traverse will be useful for de.ning other operations which traverse the B+ tree. One \nsuch simple operation is insertion without replacement in a single traversal. The .nal interface command \nis iteration. Verifying the imple\u00admentation of iterate relies crucially on our ability to view a B+ tree \nin two ways, as per theorem repTree iff repTrunk. Con\u00adceptually, the implementation can be broken into \n4 steps: 1. Follow left links in the tree until the .rst leaf is reached. 2. Change views, separating \nthe heap into trunk and leaves. 3. Iterate over the skirt, calling a function on each element in each \nleaf. 4. Change views, recombining the leaves and trunk.  Step 1 is a simple recursion of the tree \nwhich could be eliminated if the head of the skirt was stored along with the root. Steps 2 and 4 are \npurely proof-associated steps, incurring no run-time overhead, and follow immediately from the repTree \niff repTrunk theo\u00adrem de.ned previously. Step 3 is a straightforward, nested iteration over a linked-list \nof arrays. 7. The Storage Manager The RDBMS maintains an environment of tables in memory, and persistence \nis implemented by serializing the relations as strings and writing/reading these strings from disk using \ninverse functions. A set of saved tables can be loaded with a user command, and the relation information \nis checked and constructed during loading. Veri.cation amounts to checking an internal consistency property \nthat ensures reading and printing are appropriate inverses: PrintTable: forall I, Relation I -> string \nReadTable : forall I, string -> option (Relation I) Theorem storage_ok : forall I (tbl: Relation I), \nReadTable (PrintTable tbl) = Some tbl.  8. Evaluation Overall, we found this project to be extremely \nchallenging, as it re\u00adquired a wide range of formalization tasks, from encoding seman\u00adtics and compiler \ncorrectness, to reasoning about pointer-based data structures, to issues involving parsing and serialization. \nOn the one hand, it is impressive that Coq supports such a wide range of tasks and, furthermore, provides \nthe abstraction capabilities to allow for such a modular decomposition. On the other hand, we found the \ndifference between informal models and invariants and their Coq representations to be much larger than \none would hope. It is instructive to begin by comparing the amount of different kinds of code that make \nup the development. Figure 5 describes the breakdown of functional code, imperative code, and proofs \nin our RDBMS. The Functional column gives line counts for both pure code (including speci.cations) and \nfunctional code used at run-time. The Imperative column gives line counts for Ynot code, which is written \nin a style that generates a large number of veri.cation conditions. The Proofs column gives line counts \nfor both tactics and Coq terms used to construct proofs. Functional Imperative Proofs Model 360 0 700 \nSQL Compiler 840 0 440 SQL Engine 0 250 1350 B+ tree 360 510 5190 Storage 450 160 340 Total 2110 920 \n8020 Figure 5. Numbers of lines of different kinds of code These totals do not include our FSet modi.cations \nand the base Ynot tactics and data structures that we use (such as the packrat parser toolkit). They \ndo include the [de]serializers for basic types and the grammars used for parsing queries and tuples. \nLine counts were taken as-is , and there is no doubt that we can improve these. Nevertheless, the numbers \nindicate that formalizing a system to this degree requires a substantial investment over and above the \ncode. On the other hand, many of the components are directly re-usable in other contexts (e.g., the .nite \nmap interface and its associated theory, the B+ trees, etc.). 8.1 Some Lessons Learned In what follows \nwe highlight some of the lessons learned from our development. Many of these lessons are well known to \nCoq experts, but we hope this discussion will help those interested in similar developments. There are \na number of design decisions we made that initially seemed right but ultimately led to complicated proofs. \nFor exam\u00adple, it took many tries to .nd the right formulation of the invariants for the B+ trees. Originally, \nwe avoided introducing the intermedi\u00adate ptree models described in Section 6.1. Instead, we tried to \nde\u00ad.ne predicates that would directly connect association lists with the pointer graph in the heap. However, \nthis de.nition required a large number of existential quanti.ers (up to 25 in many goals). This made \nreasoning dif.cult, as we were forced to provide witnesses explicitly. Introducing ptrees and explicitly \nassociating them with the B+ tree as ghost state helped avoid this complication. In general, we found \nthat one should avoid disjunctions of any .avor when something can be readily computed. This is because \nCoq will automatically reduce computations but requires explicit guidance to eliminate disjunctions. \nFor example, we originally en\u00adcoded ptrees as an inductively de.ned type: Inductive ptree (h:nat) : Set \n:= | Leaf : ptr -> list (key * value) -> ptree 0 | Node : forall h, ptr -> list (key * ptree h) -> ptree \nh -> ptree (S h). However, eliminating such de.nitions in proofs requires an explicit use of inversion, \neven when the height is known to be zero or non\u00adzero. In contrast, Coq automatically reduces the recursive \nde.nition based upon match. In many cases, this substantially reduced the number of manual steps needed \nin proofs. Whether and how to use dependent types in our de.nitions was another source of frustration. \nOn the one hand, we found depen\u00addency useful to capture schemas for relational operations and to rule \nout various error cases that would otherwise arise. On the other hand, writing transformations, such \nas the optimization to fuse ad\u00adjacent projections, requires tedious arguments and coercions to en\u00adsure \nthat the output is well-typed. Newer languages such as Epi\u00adgram [11] and Agda [16] provide better support \nfor programming with dependent types, and Matthieu Sozeau is working to adapt many of these ideas to \nthe Coq setting [26], so we are hopeful that this frustration will diminish over time. Another lesson \nwe have learned is the value of automation, both through hints and custom tactics. This insulates proofs \nagainst code changes and greatly speeds up the proof process as tactics mature. In our previous work \non Ynot [5], we advocated a style of proof (derived from our colleague Adam Chlipala) where instead of \nwriting many small, manual proof steps, it is better to write a custom tactic that searches for a proof \nof the goal. Our experience with this project con.rms the usefulness of this methodology. For example, \nthe proof terms for the B+ tree are about 22MB in size. Writing them by hand would be impossible, but \nwriting tactics that search for these terms has proven remarkably robust throughout the evolution of \nour code. Another lesson we have learned is that the Coq modules are use\u00adful for controlling name spaces, \nbut their second-class nature makes it dif.cult to use them effectively for abstraction. For instance, \nthe section mechanism of Coq, which is extremely useful for factor\u00ading parameters, does not work well \nin the context of modules: we cannot de.ne a module within a section. Rather, we found core lan\u00adguage \nmechanisms, such as dependent records and type classes, to be more useful than modules. Consequently, \nwe avoided sophisti\u00adcated use of the module system when possible.  8.2 Related Formalization A .nal \nlesson regards the formalization of relational algebra. The idea of mechanizing (data models more general \nthan) relational algebra goes back to at least NuPrl [21]. The inspiration for our work, however, is \nthe formalization of the relational algebra in Agda found in Carlos Gonzalia s Ph.D. thesis [9]. Like \nthat thesis, we use axiomatic .nite sets; however, we opted for a more concrete tuple representation \nand a different, but equivalent, choice of base relational operations. The thesis (essentially) represents \na schema as a function from a .nite set of columns to an Agda type. Inductive Fin : nat -> Set := | zero \n: forall k, Fin (S k) | succ : forall k, Fin k -> Fin (S k). Definition Schema (k: nat) : Set := Fin \nk -> Set. Definition Tuple k (I: Schema k) : Set := forall (col: Fin k), I col. We originally adopted \nthis representation, but found that the easiest way to establish certain results like the decidability \nof equality over schemas was to convert schemas into lists and use an isomorphism between the two representations. \nOther times this representation forced us to appeal to functional extensionality (.x, fx = gx . f = g), \nwhich is not a theorem in Coq. Using this axiom compli\u00adcates reasoning since axioms do not have computational \nbehavior. Finally, while induction on k is possible, our more concrete de.ni\u00adtion provides a direct induction \nprinciple that simpli.es proofs.  We also differ from the thesis in our choice of relational basis operations. \nWe implemented cartesian product and included a per\u00admutation operation to simplify SQL compilation. Instead \nof prod\u00aduct, the thesis implements join and supports projection through two operations for splitting \na relation in half. Although in principle the two models are equivalent, certain operations possible \nin SQL, like swapping two columns, are easier to encode in our basis. In [19], Oury and Swierstra give \na dependently-typed relational algebra syntax that is similar to ours, but that uses column names instead \nof numbers. Their approach leads to a different proof bur\u00adden; for instance, in their approach the schemas \nA and B must have disjoint attribute names in A\u00d7B. We expect to study this particular tradeoff more closely \nin the future. 9. Future Work Our work lays the foundation for mechanically veri.ed RDBMSs, but to be \npractically useful for real systems, there are a number of tasks that remain. Some of these tasks are \nrelatively modest extensions to the current implementation. For example, .nishing incorporating key information \nto enable ef.cient point and range queries. This, along with additional relation statistics such as size \nand selectivity measures, will enable additional optimizations such as better join planning and more \nsemantic optimizations. Addi\u00adtional B+ tree features needed for range queries will also enable more ef.cient \nrelational algorithms such as merge-join. Finally, reifying the low-level query plan will allow us to \nfuse operations to avoid materializing transient data. With these features, and sup\u00adport for aggregation \nand basic transactions, the system should be usable for simple in-memory, single-threaded applications. \nIn the long term, realizing the ACID guarantees of concurrency, transactional atomicity and isolation, \nand fault-tolerant storage will demand substantial extension. For example, Ynot does not yet sup\u00adport \nwriting or reasoning about concurrent programs, though we have done some preliminary work extending Hoare \nType Theory with support for concurrency and transactions [13] based on the ideas of concurrent separation \nlogic [4]. We expect that implement\u00ading and verifying the correctness of high-performance, concurrent \nB+ trees will be a particularly challenging problem. As another ex\u00adample, we need to .nd the right model \nfor disk I/O which accu\u00adrately re.ects possible failure modes and incorporate this into the program \nlogic. Our ultimate goal is to combine these features with the enhancements above to obtain a fully veri.ed, \nrealistic RDBMS that can be used in safety-and security-critical settings. Acknowledgments We would like \nto thank Adam Chlipala and Matthieu Sozeau for their help with proof engineering in Coq. We would also \nlike to thank David Holland and Margo Seltzer for bringing perspective from the database community. References \n[1] Serge Abiteboul, Richard Hull, and Victor Vianu. Database Founda\u00adtions. Addison-Wesley, 1995. [2] \nYves Bertot and Pierre Cast\u00b4eran. Interactive Theorem Proving and Program Development. Coq Art: The Calculus \nof Inductive Construc\u00adtions. Texts in Theoretical Computer Science. Springer Verlag, 2004. [3] R. Bornat, \nC. Calcagno, and P. OHearn. Local Reasoning, Separation and Aliasing. Proc. SPACE, volume 4, 2004. [4] \nStephen Brookes. A semantics for concurrent separation logic. Theor. Comput. Sci., 375(1-3):227 270, \n2007. [5] Adam Chlipala, Gregory Malecha, Greg Morrisett, Avraham Shinnar, and Ryan Wisnesky. Effective \ninteractive proofs for higher-order imperative programs. In Proc. ICFP, 2009. [6] C. J. Date. Introduction \nto Database Systems. Addison-Wesley Longman Publishing Co., Inc., 2002. [7] Ramez Elmasri and Shamkant \nB. Navathe. Fundamentals of Database Systems (5th Edition). Addison Wesley, 2006. [8] Parke Godfrey, \nJarek Gryz, and Calisto Zuzarte. Exploiting constraint\u00adlike data characterizations in query optimization. \nIn Proc. SIGMOD, 2001. [9] Carlos Gonzalia. Relations in Dependent Type Theory. PhD Thesis, Chalmers \nUniversity of Technology, 2006. [10] Conor Mcbride. Elimination with a motive. In Proc. TYPES, 2000. \n[11] Conor McBride and James McKinna. The view from the left. J. Functional Programming, 14(1):69 111, \n2004. [12] James Mckinna and Joel Wright. A type-correct, stack-safe, provably correct expression compiler \nin epigram. In J. Functional Program\u00adming, 2006. [13] Aleksandar Nanevski, Paul Govereau, and Greg Morrisett. \nTowards type-theoretic semantics for transactional concurrency. In Proc. TLDI, 2009. [14] Aleksandar \nNanevski, Greg Morrisett, and Lars Birkedal. Polymor\u00adphism and separation in hoare type theory. In Proc. \nICFP, 2006. [15] Aleksandar Nanevski, Greg Morrisett, Avraham Shinnar, Paul Gov\u00adereau, and Lars Birkedal. \nYnot: Dependent types for imperative pro\u00adgrams. In Proc. ICFP, 2008. [16] Ulf Norell. Towards a Practical \nProgramming Language Based on Dependent Type Theory. PhD thesis, Chalmers University of Technol\u00adogy, \n2007. [17] Peter W. OHearn. Resources, concurrency, and local reasoning. Theor. Comput. Sci., 375(1-3):271 \n307, 2007. [18] Peter W. O Hearn, John C. Reynolds, and Hongseok Yang. Local reasoning about programs \nthat alter data structures. In Proc. CSL, 2001. [19] Nicolas Oury and Wouter Swierstra. The power of \npi. Proc. ICFP, 2008. [20] Sharon E. Perl and Margo Seltzer. Data management for internet-scale single-sign-on. \nIn Proc. WORLDS, 2006. [21] P. Rajagopalan and C. P. Tsang. A generic algebra for data collections based \non constructive logic. In Algebraic Methodology and Software Technology, volume 936 of LNCS, pages 546 \n560. Springer Berlin / Heidelberg, 1995. [22] John C. Reynolds. Separation logic: A logic for shared \nmutable data structures. In Proc. LICS, 2002. [23] Alan Sexton and Hayo Thielecke. Reasoning about b+ \ntrees with operational semantics and separation logic. Electron. Notes Theor. Comput. Sci., 218:355 369, \n2008. [24] Alan Sexton and Hayo Thielecke. Reasoning about b+ trees with operational semantics and separation \nlogic. Electron. Notes Theor. Comput. Sci., 218:355 369, 2008. [25] Carsten Sinz. System description: \nAra -an automatic theorem prover for relation algebras. In Proc. CADE-17, 2000. [26] Matthieu Sozeau. \nProgram-ing .nger trees in coq. In Proc. ICFP, 2007. [27] Matthieu Sozeau and Nicolas Oury. First-class \ntype classes. In Proc. TPHOLs, 2008.     \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We report on our experience implementing a lightweight, fully verified relational database management system (RDBMS). The functional specification of RDBMS behavior, RDBMS implementation, and proof that the implementation meets the specification are all written and verified in Coq. Our contributions include: (1) a complete specification of the relational algebra in Coq; (2) an efficient realization of that model (B+ trees) implemented with the Ynot extension to Coq; and (3) a set of simple query optimizations proven to respect both semantics and run-time cost. In addition to describing the design and implementation of these artifacts, we highlight the challenges we encountered formalizing them, including the choice of representation for finite relations of typed tuples and the challenges of reasoning about data structures with complex sharing. Our experience shows that though many challenges remain, building fully-verified systems software in Coq is within reach.</p>", "authors": [{"name": "Gregory Malecha", "author_profile_id": "81342503370", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P1911082", "email_address": "", "orcid_id": ""}, {"name": "Greg Morrisett", "author_profile_id": "81339518683", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P1911083", "email_address": "", "orcid_id": ""}, {"name": "Avraham Shinnar", "author_profile_id": "81314491648", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P1911084", "email_address": "", "orcid_id": ""}, {"name": "Ryan Wisnesky", "author_profile_id": "81435603377", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P1911085", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706329", "year": "2010", "article_id": "1706329", "conference": "POPL", "title": "Toward a verified relational database management system", "url": "http://dl.acm.org/citation.cfm?id=1706329"}