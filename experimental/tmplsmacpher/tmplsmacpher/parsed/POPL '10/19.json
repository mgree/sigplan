{"article_publication_date": "01-17-2010", "fulltext": "\n Static Determination of Quantitative Resource Usage for Higher-Order Programs Steffen Jost Kevin Hammond \nHans-Wolfgang Loidl* Martin Hofmann University of St Andrews, St Andrews, UK Ludwig-Maximilians University, \nMunich, Germany {jost,kh}@cs.st-andrews.ac.uk {hwloidl,mhofmann}@tcs.i..lmu.de Abstract We describe \na new automatic static analysis for determining upper-bound functions on the use of quantitative resources \nfor strict, higher-order, polymorphic, recursive programs dealing with possibly-aliased data. Our analysis \nis a variant of Tarjan s manual amortised cost analysis technique. We use a type-based approach, exploiting \nlinearity to allow inference, and place a new emphasis on the number of references to a data object. \nThe bounds we infer depend on the sizes of the various inputs to a program. They thus expose the impact \nof speci.c inputs on the overall cost behaviour. The key novel aspect of our work is that it deals directly \nwith polymorphic higher-order functions without requiring source-level transformations that could alter \nresource usage. We thus obtain safe and accurate compile-time bounds. Our work is generic in that it \ndeals with a variety of quantitative resources. We illustrate our approach with reference to dynamic \nmemory allocations/deal\u00adlocations, stack usage, and worst-case execution time, using met\u00adrics taken from \na real implementation on a simple micro-controller platform that is used in safety-critical automotive \napplications. Categories and Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of \nProgramming Languages Program analysis General Terms Languages, Reliability, Performance, Theory. Keywords \nFunctional Programming, Resource Analysis, Types. 1. Introduction Automatically obtaining good quality \ninformation about resource usage (e.g. space/time behaviour) is important to a number of ar\u00adeas including \nreal-time embedded systems, parallel systems, and safety-critical systems. While there has been signi.cant \nwork on automatic analyses for .rst-order programs, to date there has been correspondingly little work \non analyses for higher-order programs. Developing such analyses is important both to enable the deploy\u00adment \nof functional programming languages, and to assist the in\u00adcreasing number of conventional programming \napproaches that rely on higher-order information (e.g. aspect orientation). * Current af.liation: Heriot-Watt \nUniversity, Edinburgh. Part of this work was done while being employed by the University of St Andrews. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c .2010 ACM 978-1-60558-479-9/10/01. . . $10.00 This \npaper introduces a new automatic static analysis for de\u00adtermining upper-bound functions on the resource \nusage of strict, higher-order, polymorphic, recursive functional programs. Re\u00adsource may here refer to \nany quanti.able resource. In particular, we discuss and analyse worst-case execution time, stack-space \nus\u00adage, and heap-memory consumption. The bounds that we obtain are simple linear expressions that depend \non the input sizes. They thus expose the impact of the size of each input on overall execution cost. \nThese bounds can be inferred both easily and ef.ciently. This is the .rst automatic amortised analysis \nthat can deter\u00admine costs for higher-order functions directly rather than relying on program transformations \nsuch as defunctionalisation [33] to trans\u00adform higher-order programs into .rst-order ones. Such transforma\u00adtions \nare not acceptable for several reasons. Firstly, they usually change time and space properties. This \nis unacceptable in any con\u00adtext where the preservation of costs is important, such as the in\u00adcreasingly \nimportant class of resource-aware applications. More\u00adover, they may also change which programs can be \ncosted (e.g. by making linear programs non-linear, etc.), and they can destroy the programmers intuitions \nabout cost. Unlike transformation methods such as defunctionalisation, our approach is fully compositional. \nThis is important, since compositionality enhances modularity. Our technique can produce usage-dependent \nupper-bound functions on costs for closed-source libraries of (possibly higher-order) func\u00adtions. In \norder to analyse a program that uses such a library, it is only necessary to know the previously inferred \nannotated type for any function that is exported, and not its de.nition. Our automatic analysis is a \nvariant of the amortised cost analy\u00adsis that was .rst described by Tarjan [37]. Amortised cost analysis \nis a manual technique, which works as follows: using ingenuity, one devises a mapping from all possible \nmachine states to a non\u00adnegative rational number, henceforth referred to as the potential of that state. \nThis map must be constructed in such a way that the ac\u00adtual cost of each machine operation is amortised \nby the difference in potentials before and after the execution of the operation. For ex\u00adample, for heap \nspace an operation that allocates n memory units must always lead to states whose potential is then decreased \nby n. It follows that the cost of each operation, including entire loops or complete recursive calls, \nbecomes zero, and the overall execution cost is then equal to the potential of the initial state. There \nare two main problems to be overcome. Firstly, devising a useful mapping from each machine state to the \nnumber representing its potential is a dif.cult task. Secondly, Okasaki notes that [31]: As we have seen, \namortized data structures are often tremendously effective in practice. [. . . ] traditional meth\u00ad ods \nof amortization break in presence of persistence Our type-based variant solves both of these issues: \ni) we can au\u00adtomatically determine the abstraction through ef.cient linear pro\u00adgramming; and ii) we can \ndeal with the persistent data structures that are commonly found in a functional setting by assigning \npo\u00ad  vars ::= (varid1 , ... , varidn ) n = 0 expr ::= const |varid |varid vars |conid vars | . varid \n. expr | if varid then expr1 else expr2 | case varid of conid vars -> expr1| expr2 | case! varid of conid \nvars -> expr1| expr2 | let varid = expr1 in expr2 | LET varid = expr1 IN expr2 () varid1 = expr1; | let \nrec \u00b7\u00b7\u00b7 in expr n = 1 varidn = exprn Figure 1. Schopenhauer Syntax tential on a per-reference basis, \nrather than resorting to a lazy\u00adevaluation strategy as Okasaki does [31]. The price we pay is that our \nmethod is currently limited to linear cost formulas (a restriction which is not inherent to amortised \ncost analysis). However, we be\u00adlieve that an ef.cient automatic analysis that can be run repeatedly at \nthe press of a button is a major advantage over a cumbersome and error-prone manual analysis requiring \nsome human ingenuity. It is important to realise that the implementation of our method is indeed quite \nsimple, being based on a standard type system, aug\u00admented by a small set of linear constraints that are \ncollected as each type rule is applied. We do not need to count references: it is suf.\u00adcient to examine \nthe points in the rules where new aliases are intro\u00adduced. Furthermore, the automatically-inferred potential \nmappings always allow the initial potential to be determined simultaneously for large classes of inputs. \nThese mappings can thus be transformed into simple closed cost formulas. Contributions: We present a \ntype system for a compile-time anal\u00adysis that infers input-dependent upper bounds on program execu\u00adtion \ncosts for various resource metrics on strict, higher-order, poly\u00admorphic programs. We prove that the \ntype system is sound with respect to a given operational semantics. We also present the asso\u00adciated fully-automatic \ntype inference, which has been implemented using a standard external linear programming solver. Our main \nnovel contributions are to extend previous work [19, 25]: a) by analysing the resource usage of higher-order \nfunctions, which may be both polymorphic and mutually recursive, in a cost-preserving way; b) by dealing \nwith polymorphism, also in a cost-preserving way; c) by considering the resource parametricity of (polymorphic) \nhigher-order functions, so allowing a function to have a differ\u00adent cost behaviour for its different \nuses, without re-analysing the function. Other notable advances over our earlier work [19] are: a) the \nhandling of arbitrary (recursive) algebraic datatypes, possi\u00adbly containing functions; b) the use of \na storeless semantics instead of the (awkward) be\u00adnign sharing condition from [19]; c) a uni.ed, generic \napproach that presents a single soundness proof for several resource metrics and for several different \nop\u00aderational models, including dynamic memory, stack allocations and worst-case execution time (speci.cally \nfor the Renesas M32C/85U embedded system microcontroller). These are discussed in more depth in a companion \npaper [25].  2. The Schopenhauer Notation We illustrate our approach using the simple Schopenhauer lan\u00adguage, \nwhich acts as a compiler intermediate language. The syn\u00adtax of Schopenhauer (Figure 1) is mostly conventional, \nexcept that: i) we distinguish between identi.ers for variables and those for data constructors; ii) \nall expressions are in let-normal form, i.e. most sub-expressions are variables; iii) we have two let-constructs \nthat have identical meaning, but differing costs (see the following paragraph); iv) pattern matches are \nnot nested and allow only two branches; v) pattern matching comes in two variants read-only and destructive. \nNone of these peculiarities are actually required, but they have been chosen to simplify the presentation \nof our work. For example, our implementation readily deals with nested pattern matches with an arbitrary \nnumber of branches. Note that the re\u00adcursive let-rec form allows not only the construction of recursive \nfunctions, but also that of aliased circular data. The use of let-normal form means that the threading \nof re\u00adsources is limited to let-expressions. This simpli.cation avoids the need to replicate large parts \nof the soundness proof for let\u00adexpressions in the proofs for the other cases shown in Section 5. However, \na transformation to let-normal form could, obviously, al\u00adter execution costs. We avoid this by adding \na second LET-construct that is used only for transformed expressions. By assigning a differ\u00adent cost \nto this construct (generally zero), we can make the transfor\u00admation to let-normal form entirely cost-neutral. \nThe LET-construct also allows us to construct an accurate cost metric for stack space usage despite the \nfact that we have chosen to use a big-step seman\u00adtics. We explain the rationale for this choice in Section \n6.1.1. Since non-monotone cost metrics are interesting to deal with, Schopenhauer includes a primitive \nfor deallocation, which we com\u00adbine with pattern matching (case!). We do not deal with the safety of \ndeallocations, since this is an orthogonal and complex problem that deserves its own treatment (see, \nfor example, Walker and Mor\u00adrisett s alias types [40], or the bunched implication logic of Ish\u00adtiaq and \nO Hearn [23]). We encapsulate this problem by adopting essentially a storeless semantics [34, 24]. While \nwe do deal with explicit memory addresses, these should be considered as symbolic handles, as used, for \nexample, in early versions of the JVM. A deal\u00adlocated memory address is then simply overwritten with \nthe special tag Bad. This prevents its reuse and so guarantees that evaluation halts when dereferencing \nany stale pointer. As a consequence, we can prove that the required resource bounds are maintained. \n 3. Schopenhauer Operational Semantics We now state how Schopenhauer programs are executed, and de\u00ad.ne \nthe cost for a speci.c execution sequence, thereby .xing a (resource-aware) operational semantics. The \nSchopenhauer type rules in Section 4 govern how potential is associated with the run\u00ad time values of \na particular type. The operational semantics is inde\u00adpendent of the type rules. Evaluation may, however, \nget stuck for untypable programs. An environment V is a partial map from variables x to loca\u00adtions l. \nOur semantics is therefore based on a boxed heap model. By varying the cost parameters explained below, \nwe can, how\u00adever, also capture evaluation costs for an unboxed heap model. A heap H is a partial map \nfrom locations to labelled values w. H[l . w] denotes a heap that maps l to value w and oth\u00aderwise acts \nas H. All values are labelled for simplicity, e.g. (bool, tt), (int, 7), (constrc,l1,...,ln), (.x.e , \nV . ). Here Ind(bl) is a special value modelling an indirection. To follow such in\u00addirections we de.ne \nnext(H,l)= bl if H(l)= Ind(bl) and next(H,l)= l otherwise. These indirections are needed to model recursive \nde.nitions, which we explain with an example at the end of this section. As discussed above, deallocated \nlocations are overwritten with the tag Bad to prevent stale pointers. Our operational semantics is fairly \nstandard, except that it is instrumented by a resource counter, which de.nes the cost of each operation. \nThe cost counter is used to measure execution costs. If  n . Z l/. dom(H) (OP CONST INT) ' m + KmkInt \nV,H ' n l,H l . (int,n) m w =(bool,tt/ff ) l/. dom(H) ' m + KmkBool V,H ' true/false l,H[l . w] m (OP \nCONST BOOL) V,H next(H,V(x)) = l m ' + KpushVar + Knext m ' x l,H (OP VAR) V. = V.FV(e)\\x w = (.x.e, \nV.) l /. dom(H) V,H m ' + KmkFun(|V . |) m ' .x.e l,H[l . w] (OP ABS) next(H,V(x0)) = bl H(bl) = (.x.e, \nV.) V. [x . V(x0)] ,H m -Kapp m ' + Kapp ' m + Knext e l,H ' ' (OP APP) V,H m ' y x0 l,H next(H,V(x)) \n= bl H(bl) = (bool,tt) V,H m -KifT m ' + KifT ' et l ' ,H ' m + Knext ' V,H ' if x then et else ef l \n' ,H m (OP CONDITIONAL TRUE) next(H,V(x)) = bl H(bl)=(bool,ff ) m -KifF ' V,H ' ef l ' ,H m + KifF ' \nm + Knext ' V,H ' if x then et else ef l ' ,H m (OP CONDITIONAL FALSE) k = 0 c . Constrs l/. dom(Hk ) \n`\u00b4 w = constrc,V(x1),..., V(xk ) ' m + Kalloc(c) V,H ' c (x1,...,xk) l,H[l . w] m (OP CONSTRUCTOR) `\u00b4 \nnext(H,V(x)) = bl H(bl)= constrc,l1,...,lk m -KcaseF(c) ' V,H ' (c) e2 l,H m + KcaseF ' V,H m + Knext \nm ' case x of c (y1,. . . ,yk) -> e1|e2 l,H ' (OP CASE FAIL) ` \u00b4 next(H,V(x)) = bl H(bl)= constrc,l1,...,lk \nm -KcaseT(c) ' V[y1 . l1,...,yk . lk],H ' m + KcaseT ' (c) e1 l,H V,H m + Knext m ' case x of c (y1,. \n. . ,yk) -> e1|e2 l,H ' (OP CASE SUCCEED) ` \u00b4 next(H,V(x)) = bl H(bl)= constrc,l1,...,lk V . = V[y1 \n. l1,...,yk . lk] m -KcaseT(c)+ Kcealloc(c) ' V. ,H[k . Bad] ' e1 l,H m + KcaseT ' (c) m + Knext ' V,H \n' case! x of c (y1,...,yk)-> e1|e2 l,H m (OP CASE! SUCCEED) m1 -Klet1 V,H m2 e1 l1,H1 V1 = V[x . l1] \nm2 -Klet2 V1,H1 ' e2 l2,H2 m + Klet3 (OP LET) m1 V,H ' let x = e1 in e2 l2,H2 m m = m1 + Krec1 + nKnext \nV . = V[x1 . l1,...,xn . ln] H0 = H[l1 . Bad,...,ln . Bad] ' H =[l1 . Ind(lb1),...,ln . Ind(c)] Hnln \n.i .{1,...,n}.li ./dom(H) . lbi = next(Hn,l ' ) i mi -Krec2 .i .{1,...,n}. V. ,Hi-1 mi+1 ei l ' i,Hi \nmn+1 -Krec3 ' V . ,H ' e l,H m + Krec4 m ' V,H ' let rec {x1 = e1; ... ;xn = en}in e l,H m (OP REC) \nFigure 2. Schopenhauer Operational Semantics this counter becomes negative, then program execution becomes \nstuck. We are interested in .nding the smallest number for each input that safely allows execution. The \npurpose of the analysis in Section 4 is to provide an upper bound on this number for large classes of \ninputs, without evaluating the program in any way. m The judgement V, H ' e \" l, H ' means that under \nthe ini\u00ad m tial environment V and heap H, the expression e evaluates to loca\u00adtion l, containing the result \nvalue, and post-heap H ' , provided that there are at least m . N units of the selected resource available \nbe\u00adfore the computation. Furthermore, m ' . N units will be available after the computation. We write \nV, H . e \" l, H ' to denote that e evaluates to l using an unknown, but .nite, amount of resources. 3 \nFor example, V, H 1 e \" l, H ' means that three resource units are suf.cient to allow e to be evaluated, \nand that exactly one resource unit is unused after the computation. This unused resource unit might or \nmight not have been used temporarily. Note that this tracks both the overall net resource costs as well \nas the minimum number of free resources that are necessary for the computation to be started. These two \nnumbers may be different if there is some temporary resource usage, as with stack space usage. m Lemma \n3.1. For all k = 0, if V, H ' e \" l, H ' holds, then m m + k both m ' = 0and V, H ' e \" l, H ' hold. \nm + k The operational semantics rules for Schopenhauer are shown in Figure 2. Two rules are omitted because \nthey are almost identical to other rules: OP CASE! FAIL is similar to OP CASE FAIL; and OP LET (which \ncovers LET x = e1 IN e2), is identical to OP LET if the cost metric parameters Klet1, Klet2 and Klet3 \nare replaced by KLET1, KLET2 and KLET3, respectively. The rules exploit a number of constant cost parameters. \nThis al\u00adlows us to deal with several different cost metrics without changing the operational model. Since \nour analysis uses the same constants regardless of the metric, our soundness proof is completely inde\u00adpendent \nof the cost metric and so does not require to be performed anew for each new cost metric. These parameters \nmust be chosen carefully so that the costs of the operational semantics match re\u00adality. For example, \nthe constant KmkInt denotes the cost of con\u00adstructing an integer constant. So, if we are interested in \nheap al\u00adlocation and an integer occupies two heap units, as in our boxed heap model, then we set this \nconstant to two. In an unboxed heap model, however, it is set to zero, since the integer is created directly \nin the stack. Likewise for stack usage, KmkInt is either the size of a pointer (in the boxed model) or \nthe actual size of an integer (in the unboxed model); and for worst-case execution time (WCET) we set \nit to the greatest number of clock cycles needed to create an integer constant. For example, the commercial \naiT WCET analyser (http://www.absint.com) determines this to be 83 cycles on the Renesas M32/85U microcontroller. \nRecursive let-bindings use indirections. An indirection never points to another indirection, since indirections \nare only introduced in rule OP REC to locations which have been followed. This prop\u00aderty is formalised \nin De.nition 5.1, which serves as the invari\u00ad ant for our soundness theorem. This also allows a constant \ncost bound (Knext) when dereferencing an indirection.  Let us see, for example, how let rec {x = cons(x)}in \nx is evaluated in the empty heap and store (where cons . Constrs is assumed). We omit resource annotations. \nWe put H0 =[l1.Bad] and V . =[x.l1]and have . l ' V , H0 cons(x) 1, [l1.Bad,l ' 1.(cons,l1)] Now we have \nnext(H1,l ' 1)= l ' 1. De.ning the new heap H ' = [l1.Ind(l ' 1),l ' 1.(cons,l1)]we get \u00d8, \u00d8 let rec \n{x = cons(x)}in xl1, H ' yielding the expected cyclic data structure with indirection.  4. Schopenhauer \nType Rules We use a, \u00df, . to denote type variables. Let CVbe an in.nite set of resource variables ranging \nover Q+, usually denoted by q, p,r, s, being disjoint from the identi.er sets for variables and construc\u00adtors \nVar, Constrs. Sets of type and resource variables are referred to using the vector notation, e.g. Bq. \nAll other decorations stand a, Bfor different entities. We use ., f, . to range over sets of linear inequalities \nover non-negative rational constants and resource vari\u00adables, plus special terms involving type variables \nthat are mapped to linear inequalities when the type variables are substituted with closed type terms. \nA valuation v is a two-fold mapping, that maps resource variables to Q+ and type variables to closed \ntypes. We write v . f if v satis.es all constraints in f, and . . f to denote that . entails f, i.e. \nthat all valuations that satisfy . also satisfy all constraints in f. The annotated types of Schopenhauer \nare then given by the following grammar: T ::= int |bool |a -. -. | \u00b5a.{c1:(q1,T1 )| ... | ck :(qk,Tk \n)} -p. -.' |.Br... T ' T p |.aB:..T - . where ci . Constrs are constructor labels and T stands for (T1 \n... Tn )where n = 0 *. Algebraic datatypes are de.ned as usual, except that each constructor also carries \na resource variable in addition to the usual type information. The types contain two different universal-quanti.ers: \none for resource variables, and one for type variables. For example, the type of a function counting \nthe length of a list could be: x .a:\u00d8..{x, y, u, v}.f.\u00b5\u00df.{Cons:(u, (a, \u00df))}|Nil:(v)}-.int y with f = \n{x = 156 + y, u = 940}. So the type tells us that this length function can be applied to lists of any \ntype (.a:\u00d8.). Further\u00admore, it admits several resource behaviours, since .{x, y, u, v}.f. tells us that \nwe can rename x, y, u, v to independent resource vari\u00adables. Of course, the constraints f must be substituted \naccordingly. The admissible valuation x = 156,y =0,u = 940,v =0 would then indicate that evaluating the \nfunction requires at most 156 resource units (in this case clock cycles), plus at most 940 re\u00adsource \nunits per Cons constructor in the input. In other words, if n is the length of the input list, the execution \ncost is bounded by 940n +156. However, the connection between the actual cost of running a program and \nits annotated type, such as the one above, is only guaranteed by Theorem 1. Continuing with the annotated \ntype example, we also see that the above function can be called with more resources available, since \nthe valuation x = 256,y = 100,u = 999 is also admis\u00adsible, leading to the bound 999n +256. Of these resources, \nat least * Note that all operators are extended pointwise when used in conjunction with the vector notation \nand are only de.ned if both vectors have the same -. - . length, i.e. A = B stands for .i.Ai = Bi. 100 \ncan be recovered after the call (the value of y). So list types having extra potential may be accepted, \nbut their additional poten\u00adtial would be lost. This is safe, since it increases the upper bound on resource \nusage, but of course we will usually avoid such a loss. The free resource variables of the type and constraint \nsets are denoted by FV.(\u00b7). We also de.ne a mapping |\u00b7|from annotated types to standard unannotated types, \nwhich simply erases all an\u00ad -. p -.-. notations. For .a... T ' T ', we require that a . FV.(T ). p {p, \np ' }. FV.(T ' )holds, but not that FV.(.)is a subset of a. Any intermediate variables which would then \nonly occur in . can be eliminated by projecting the polytope described by . to the rel\u00adevant dimensions. \nThis ensures that subtyping remains decidable. The type rules for Schopenhauer govern how potential is \nassoci\u00adated with each particular runtime value through its type. We denote the part of the potential \nassociated with a runtime value w of type A by Fv (w : A)(see De.nition 5.4). Intuitively, this is de.ned \nas the H sum of the weights of all constructors that are reachable from w, where the weight of each constructor \nin the sum is determined by the type A. A single constructor at a certain location may contribute to \nthis sum several times, if there is more than one reference to it. It is natural to extend this de.nition \nto environments and contexts by summation, i.e. Fv (V : G)is the sum of Fv (V(x): G(x))over all HH variables \nx in G. Since the potential depends on the state, (static) type rules do not have access to this number, \nbut only govern the relative changes. Note that we never actually need to compute the potential (apart \nfrom the initial state), so the potential mainly serves as an invariant in our soundness proof. We now \nformulate the type rules for Schopenhauer. These dif\u00adfer from standard Hindley-Milner typing judgements \nonly in that they also refer to cost and resource variables. Note that we are not concerned with type \ninference itself (a generally solved problem), but only with the inference of our new type annotations. \nLet G de\u00adnote a typing context mapping identi.ers to annotated Schopen\u00adhauer types. The Schopenhauer \ntyping judgement q G ' e : A | f q then reads for all valuations v that satisfy all constraints in f, \nthe expression e has Schopenhauer type v (A)under context v (G); fur\u00adthermore, evaluating e under environment \nV and heap H requires a potential of at most v (q)+Fv (V : G) and leaves a potential of at H least v \n(q ' )+Fv ' (l : A)available afterwards, where l is the result H value and H ' the post-heap . In Section \n5, we will formalise this statement as Theorem 1 (our main theorem), which requires as a precondition \nthat the context, environment and heap all agree. We use a compressed notation that makes the following \ntwo formulations equivalent for . = {q1 = q2 + c1,q2 ' = q1 ' + c2}: q2 q2 G q ' e2 : A2 | f G q ' + \nc2 e2 : A2 | f 1 q1 q2 + c1 2 G q ' e1 : A1 | f .. G q ' e1 : A1 | f 1 1 The constraints . that were \nexplicitly introduced in the left-hand form have thus become implicit in the compressed notation on the \nright. We believe that, with a little practice, the compressed notation is actually easier to read. It \nis also closer to our implemen\u00adtation, which avoids the introduction of unnecessary intermediate variables. \nNote that we do not simplify constraints after they are generated, since the LP-solver is much faster \nif we do not do so. Basic Expressions. KpushVar (VAR) x:A 0 x : A |\u00d8 n . Z e .{true, false} (INT) (BOOL) \nKmkInt KmkBool \u00d8 0 n : int |\u00d8 \u00d8 0 e : A |\u00d8  Since primitive terms such as integers (INT) or variables \n(VAR) al\u00adways have .xed evaluation costs, a .xed initial potential and a returned potential of zero suf.ces. \nThe restriction to empty con\u00adtexts and the use of explicit weakening, rule WEAK below, just serves to \nsimplify our soundness proof by removing redundancies. For our prototype implementation, we have merged \nthe WEAK and RELAX rules into all terminal rules. Structural rules. We use explicit structural rules \nfor weakening and sharing (contraction), while exchange is built-in. It is necessary totrackpointersthatarediscarded \n(WEAK)orduplicated (SHARE), since such operations may affect resource consumption. An addi\u00adtional structural \nrule (RELAX) allows potential to be discarded both before or after a term, as well as allowing a constant \namount of po\u00adtential to bypass a term. In our system, unlike in a strictly linear type system, variables \ncan be used several times. However, the sum of all the potential bestowed by each type of all the existing \nreferences must not ex\u00adceed the potential that was originally attached to the type associated with the \nentity when it was created. It is the job of the SHARE rule to track multiple occurrences of a variable; \nand it is the job of the Y-function to apportion potential appropriately. The application of these rules \nis straightforward. For example, where there are multiple uses of a variable, sharing is used only at \nthe latest point; WEAK is applied before each terminal rule; and RELAX is built-in throughout the rules \nwith an additional slack variable that is punished in the objective function, so discouraging the LP-solver \nfrom using relaxations. p G ' e : A | f p q (RELAX) G ' e : A | f .{q = p, q -p = q ' -p ' } q q G \n' e : C | .f . . q q (WEAK) G,x:A ' e : C | f q q G,x:A1,y:A2 q ' e : C | f (SHARE) q G,z:A ' e[z/x, \nz/y]: C | f .Y(A |A1,A2 ) q The ternary function Y(A |B, C )returns a set of constraints that enforce \nthe property that each resource variable in A is equal to the sum of its counterparts in B and C. This \nfunction is only de.ned for structurally identical types A, B, C, i.e. types that differ at most in the \nnames of their resource variables. For example, we have A = \u00b5X.{Nil:(a, ())|Cons:(d, (int,X))} B = \u00b5X.{Nil:(b, \n())|Cons:(e, (int,X))} C = \u00b5X.{Nil:(c, ())|Cons:(f, (int,X))} Y(A |B, C )= {a = b + c, d = e +f } For \ntype variables we simply record Y(a |\u00df1,\u00df2 )within the con\u00adstraints, and replace it by the according \nconstraints upon speciali\u00adsation. The crucial property of sharing is expressed in Lemma 5.7.  Function \nAbstraction &#38; Application. q . dom(G) =FV(e)\\xB = A -' Cf .. . . q Sq G,x:A ' e : C | .f . Y(D |D, \nD ) q D.ran(G) Br ./FV.(G).FV.(f) (ABS) KmkFun(|G|) G 0 .x.e : .Br...B | f Since the potential stored \nin the function closure becomes avail\u00adable for each function application, in order to allow the unlimited \nrepeated application of functions, we must restrict the potential stored in a function closure to zero. \nThis is achieved by abusing the sharing operator Y. Here, Y(D |D, D )just generates the con\u00adstraint x \n= x + x for each resource variable in D, forcing them all to zero. All the potential required during \nthe execution of the function body must therefore be provided by its arguments, except for a constant \namount. This, relatively minor, restriction only affects functions that re\u00adcurse over a captured free \nvariable, but not over one of their inputs. We have not yet encountered an interesting program example \nwhere this restriction would be an issue. In order to deal with such func\u00adtions, potential could be allowed \nwithin the closure, provided that a static bound on the number of calls to such functions could be determined. \nWe plan to experiment with use-n-times functions in future work, if this restriction turns out to be \na real issue. Alterna\u00adtively, knowing the sizes of potential-bearing entities captured in a closure would \nallow us to recharge their potential at each call. Combining our work with a sized-type analysis (e.g. \n[39]) might thus also avoid this limitation. Each function body is only analysed once, associating a \nset of constraints with the function. At each application, these constraints are copied from the type. \nAll resource variables that only occur in the function s type and constraints, but nowhere else, are \ngiven fresh names for each application. Thus, although each function is only analysed once, the LP-solver \nmay still choose a different solution for each individual application of the function. s : Br . CV a \nsubstitution to fresh resource variables q . s(B)= A -' C q (APP) q + Kapp + Knext x:A, y:.Br...B ' yx \n: C | s(.) q -Kapp ' Note that the LET-construct can be used to specialise the func\u00adtion before application. \nThis is required anyway, if we follow the convention suggested in Section 6.1.1 that normal sub-expressions \nshould always be unique variables, and that these are introduced by a LET-construct immediately before \ntheir single use. Algebraic Datatypes and Conditionals. q -KifT q -KifF q + KifT ' q + KifF ' G ' et \n: A | f G ' ef : A | . q + Knext G,x:bool ' if x then et else ef : A | f .. q (CONDITIONAL) c . Constrs \nC = \u00b5X.{\u00b7 \u00b7\u00b7|c :(p, (B1,...,Bk))| \u00b7 \u00b7 \u00b7} Ai = Bi CX (for i =1,...,k) p + Kalloc(c) x1:A1,...,xk:Ak 0 \nc (x1,...,xk): C |\u00d8 (CONSTR) The CONSTR rule plays a crucial role in our annotated type system, since \nthis is where available potential may be associated with a new data structure. Potential cannot be used \nwhile it is associated with data; it can only be used once it has been released using the CASE rule that \nforms the dual to the CONSTR rule. A successful match re\u00adleases the potential associated with the corresponding \nconstructor. c . Constrs A = \u00b5X.{\u00b7 \u00b7 \u00b7|c :(p, (B1,...,Bk))| \u00b7 \u00b7 \u00b7} .= y1:B1[A/X],...,yk:Bk [A/X] q + \np + Kdealloc(c)-KcaseT(c) G, . ' e1 : C |f q + KcaseT ' (c) q -KcaseF(c) G,x:A q ' + KcaseF ' (c) e2 \n: C |. q + Keof q G,x:A ' case! x of c (y1,...yk)-> e1|e2 : C |f .. (CASE!) The CASE rule for the read-only \ncase pattern-match is identi\u00adcal to CASE!, except that it doesn t include the cost parameter Kdealloc(c), \nthe (possibly negative) cost of deallocating construc\u00adtor constrC .  Let-bindings. q1 q2 G q2 + Klet2 \ne1 : A1 | . .,x:A1 e2 : A2 | f q3 (LET) q1 + Klet1 G, . q3 -Klet3 let x = e1 in e2 : A2 | . .f The type \nrule for the alternative form of let-expression LET ... IN (LET), is almost identical, except it substitutes \nthe cost constants KLET1, KLET2, KLET3 for Klet1, Klet2, Klet3, respectively. Rec-bindings. .= x1:A1,...,xn:An \nqi -Krec2 .i .{1,...,n}. Gi, . qi+1 ei : Ai | .i qn+1 -Krec3 Gn+1, . ' e : C | . q + Krec4 SS f . . . \n.i . Y(B |B, B ) i=1,...,n B.ran(.) f .{q = q1 +Krec1 +n \u00b7 Knext} q G1,..., Gn+1 ' let rec {x1 = e1; \n... ;xn = en}in e : C | f q (REC) Our recursive let rec construct allows circular data to be con\u00adstructed. \nIn contrast to non-circular aliased data, which may be created carrying per-reference potential, as usual, \ncircular data is ill-suited for bounding recursion since its type-based potential must be either in.nite \nor zero. The REC rule therefore enforces zero po\u00adtential by abusing the sharing operator Y in the same \nmanner as the ABS rule. As previously noted, function types are always as\u00adsigned zero potential, and \nso are not affected, since the potential that is required to execute the body of a function must come \nfrom the arguments to the function. Polymorphism. q a /B. dom(G) a/. G ' e : C | . .f B. q q G q ' e \n: .aB:f.C | . (GENERALISE) q q G ' e : .aB:..C | .f . . ..B aB q  (SPECIALISE) G q ' e : CB aB| f We \nuse the standard Hindley-Milner rules for polymorphism: GENERALISE is used to generalise a type; and \nSPECIALISE al\u00adlows a polymorphic type to be specialised to any valid type, as de.ned by the other type \nrules. Subtyping. The type rules for subtyping depend on another in\u00adductively de.ned relation . . A<:B \nbetween two types A and B, de.ned below, which is relative to a constraint set .. q G,x:B q ' e : C | \nf. . A<:B (SUPERTYPE) q G,x:A ' e : C | f .. q q q G ' e : D | f. . D<:C (SUBTYPE) q G ' e : C | f .. \nq For any .xed constraint set ., the following relation is both re.exive and transitive, but not necessarily \nanti-symmetric. . . A<:A -. -. for all i holds . .{pi = qi}and . . Ai <:Bi -. -. . . \u00b5X.{\u00b7 \u00b7 \u00b7|ci:(pi,Ai \n)| \u00b7 \u00b7 \u00b7}<:\u00b5X.{\u00b7 \u00b7 \u00b7|ci:(qi,Bi )| \u00b7 \u00b7 \u00b7} s : Bs . CV a substitution .\u00af . .f . s(.) . .f . s(p)= q, s(p \n' )= q ' . .f . D<:s(C) . .f . s(A)<:B pq. '' . ..Bs...C -.A<:.Br.f.D -B pq H(l)=(int, n) n . Z H(l)=(bool, \ntt/ff ) H Fv l :int H Fv l :bool (WFINT) (WFBOOL) H(l)=(constrc,l1,...,lk) C = \u00b5X.{\u00b7 \u00b7 \u00b7|c :(q, (B1,...,Bk))| \n\u00b7 \u00b7 \u00b7}  .i .{1,...,k}. H[l . Bad]Fv li :Bi CX (WFCON) H Fv l :C H Fv l :A .f. v . f .f . A<:B (WFSUBTYPE) \nH Fv l :B H(l)=(.x.e , V) There exists G, p, p ' ,f such that: p H[l . Bad]Fv V :G v F f G ' .x.e : F \n| f p H Fv l :F (WFFUN) H(l)= Ind(bl) H(l)= Bad H(bl) Ind(k ) = (WFBAD) b H Fv l :A H Fv l :A (WFINDIRECT) \nH Fv l :A Figure 3. Derivation rules for well-formed environments. s : aB. \u00dfBa substitution . .f . s(.) \n. . s(A)<:B . ..aB:..A <: .\u00dfB:f.B The inference itself follows straightforwardly from these type rules. \nFirst, a standard typing derivation is constructed, and each type occurrence is annotated with fresh \nresource variables. We insert the structural rules as outlined above and then traverse the type derivation \nprecisely once to gather all the constraints. Because all types have been annotated with fresh resource \nvariables, subtyping is required throughout, but this will always succeed and it will generate the necessary \ninequalities. We illustrate this process in more detail with a simple example in Section 6.1.2. In the \n.nal step, the constraints that have been gathered are solved by a standard LP-solver [4]. In practice, \nwe have found that the sparse LPs that are generated can be easily solved, partly because they have a \nsimple structure [19]. Furthermore, the number of constraints that are generated is linear in the size \nof the analysed program without resource parametricity; and at most quadratic with resource parametricity. \nSince only a single pass over the program code is needed to construct these constraints, this leads to \na highly ef.cient analysis. An online demo of our analysis is available at http://www.embounded.org/software/cost/cost.cgi \n.  5. Soundness of the Analysis We now sketch the most important steps for formulating our main soundness \ntheorem. We .rst formalise the notion of a well\u00adformed environment, written H Fv V :G, which simply states \nthat for each variable, the type assigned by the typing context agrees with the actual value found in \nthe heap location that is assigned to that variable by the environment. This is an essential invariant \nfor our soundness proof. De.nition 5.1. A memory con.guration consisting of heap H and stack V is well-formed \nwith respect to context G and valuation v , written H Fv V :G, if and only if for all variables x . G \nthe statement H Fv V(x):G(x)can be derived by the rules in Figure 3. Lemma 5.2. If H Fv V :Gthen for \nall l also H[l . Bad]Fv V :G It is an obvious requirement that evaluation must maintain a well\u00adformed \nmemory con.guration.  Lemma 5.3. If H Fv V :G and V, H .e l, H ' then H ' Fv V :G. We remark that one \nmight wish to prove an extended statement that the result l of the valuation is also well-formed if the \nexpression e was typable. Unfortunately such a statement cannot be proven on its own and must be interwoven \ninto Theorem 1. De.nition 5.4 (Potential). If H Fv l :A holds, then the potential of location l for type \nA in heap H under valuation v , written Fv (l:A), is recursively de.ned for recursive datatypes by H \nX Fv Fv H (l:A)= v (q)+ H (li:Bi[A/X]) i when both A = \u00b5X.{\u00b7 \u00b7 \u00b7|c:(q, (B1,...,Bk))| \u00b7 \u00b7 \u00b7} and also \nH(next(H,l))=(constrc,l1,...,lk)holds, and zero in all other cases. We extend this de.nition to contexts \nby X `\u00b4 Fv Fv H V :v (G) = H V(x):v G(x) x.dom(G) Subtyping must respect the well-formed environments \nand the amount of potential associated with any value of that type. Lemma 5.5. If H Fv l :A and f . A<:B \nholds and v is a valuation satisfying f, then H Fv l :B and Fv (l:A)= Fv (l:B) HH If a reference is duplicated, \nthen the type of each duplicate must be a subtype of the original type. Lemma 5.6. If Y(A |B, C )= f \nholds then also f . A<:B. The potential attached to any value of a certain type is always shared linearly \namong types introduced by sharing. In other words, the SHARE rule does not increase the total available \npotential. Lemma 5.7. If H Fv l :A and Y(A |B, C )= f holds and v satis.es f then Fv H (l:A)=Fv H (l:B)+Fv \nH (l:C). Moreover, for A = B and A = C, it follows that Fv H (l:A)=0also holds. p Lemma 5.8 (Inversion). \nIf G ' .x.e : B | f holds, then there p q. exists ., ., .Bs...A -' C such that all of the following hold \n` q \u00b4 q q. f ..Ba...A -' C<:B .,x:A ' e : C | . qq . . G dom(.) =FV(e)\\x Bs/. FV.(.).FV.(f) [ f ....f \n. Y(D |D, D ) f . p = p ' +KmkFun(|.|) D.ran. We can now formulate the main theorem, as described intuitively \nin Section 4. Theorem 1 (Soundness). Fix a well-typed Schopenhauer program. Let r . Q+ be .xed, but arbitrary. \nIf the following statements hold q q G ' e:A | f (1.A) V, H . e l, H ' (1.B) H Fv V :v (G) (1.C) v : \na valuation satisfying v (f) (1.D) then for all m . N such that m = v (q)+Fv V :v (G) +r (1.E ) H there \nexists m ' . N satisfying m m V, H ' e l, H ' (1.I) m ' = v (q ' )+Fv H' l:v (A)+ r (1.II) H ' Fv l \n:v (A) (1.III) The proof is by induction on the lengths of the derivations of (1.B) and (1.A) ordered \nlexicographically, with the derivation of the eval\u00ad uation taking priority over the typing derivation. \nThis is required since an induction on the length of the typing derivation alone would fail for function \napplications, since in this case we extend the length of the typing derivation by the typing judgment \nfor the body, using the invariant for well-formed environments. On the other hand, the length of the \nderivation for the term evaluation never increases, but may remain unchanged where the last step of the \ntyping derivation was obtained by a structural rule. In these cases, the length of the typing derivation \ndecreases, allowing an induction over lexicographically-ordered lengths of both derivations. The proof \nis complex but unsurprising for most rules. The arbi\u00adtrary value r is required to carry over excess potential, \nwhich may be required in the second sub-expression of a let-expression, but left untouched by the .rst \nsub-expression. We sketch some impor\u00adtant cases: (ABS) In the case that the last step of the derivation \nfor (1.A) was derived by rule ABS, we also know that the last step for (1.B) must have been performed \naccording to rule OP ABS. We have H ' (l) =(.x.e , V . ). Fix r . Q+ and choose any m . N such that m \n= v (q)+Fv (V:G) + r. By the H de.nition of ABS and the observation that Ghas no potential by Lemma 5.7, \nwe have m = KmkFun(|G|)+r. Furthermore m = KmkFun(|V .|)+ r since |G| = |V .|by V . = V.FV(e)\\x from \nOP ABS and dom(G) = FV(e)\\ x from ABS. By OP ABS and Lemma 3.1 we thus obtain m ' = m -KmkFun(|V .|)+ \nr which satis.es m ' = r as required, since the potential of l is zero by De.nition 5.4. This leaves \nus to prove (1.III), which followsinthiscasedirectlyfrom WFFUN,sinceallexistentially quanti.ed requirements \nare among our premises, except for H ' [l . Bad] Fv V :G which follows by Lemmas 5.2 and 5.3 from (1.C). \n`\u00b4 (APP) From OP APP we have HV(y) =(.x.e , V . ) and from (1.III) through WFFUN we obtain the existence \nof a typing judgement for the function body. By the inversion Lemma 5.8, we obtain all the required properties \nto derive KmkFun(|G|) G 0 .x.e : .Br...B | f through the application of the ABS type rule. This allows \nus now to apply the induction hypothesis, together with the premise of (1.B) for the body of the function. \nThe application of the induction hypothesis is jus\u00adti.ed despite the increased type derivation, since \nthe evaluation was shortened by one step. Again, note that the potential of G is zero. This follows from \nLemmas 5.5 and 5.7. Lemma 5.5 is also important for deriving the necessary inequalities between m and \nm ' and their counterparts from the induction hypothesis. Conclusion (1.III) follows from the induction \nhypothesis and the .rst part of Lemma 5.7. (RELAX) Let r . Q+ be .xed but arbitrary. We observe that \nm = v (p)+Fv (V:G)+r = v (q)+Fv (V:G)+r 'if we choose HH r ' = r + v (p)- v (q)for applying the induction \nhypothesis. We can do this since v (p)-v (q)= 0 holds by the constraints of the RELAX rule. We thus obtain \nm ' with '' ' m = v (q )+Fv r H (V:G)+ = v (q ' )+FH v (V:G)+ r + v (p)-v (q) ' '' = v (q )+Fv H (V:G)+ \nr + v (p )-v (q ) = v (p ' )+Fv H (V:G)+ r which follows by v (p)-v (q)= v (p ' )-v (q ' )from the other \nconstraint added in RELAX.  HeapStackTime Heap Stack  TimeHeapStack TimeHeapStack TimeHeapStack Time \n N=1 N=2 N=3 N=4 N=5 sum (see Fig 4) Analysis 16 39 3603 24 39 5615 32 39 7627 40 39 9639 48 39 11651 \nMeasured 16 34 3066 24 34 4606 32 34 6146 40 34 7686 48 34 9226 Ratio 1.00 1.15 1.18 1.00 1.15 1.22 1.00 \n1.15 1.24 1.00 1.15 1.25 1.00 1.15 1.26 .atten (see Fig 6) Analysis 21 34 4485 38 60 8732 55 66 12979 \n72 82 17226 89 98 21473 Measured 21 34 4275 38 50 7970 55 50 11665 72 66 15360 89 66 19055 Ratio 1.00 \n1.00 1.05 1.00 1.20 1.10 1.00 1.32 1.11 1.00 1.24 1.12 1.00 1.49 1.13 repmin Analysis 17 42 5020 35 69 \n10991 53 96 16962 71 123 22933 89 150 28904 Measured 17 42 4633 35 52 9395 53 61 14157 71 62 18919 89 \n71 23681 Ratio 1.00 1.00 1.08 1.00 1.33 1.17 1.00 1.57 1.20 1.00 1.98 1.21 1.00 2.11 1.22 Table 1. Measurement \nand analysis results for list-and tree-processing functions  6. Example Cost Analysis Results In this \nsection, we compare the bounds inferred by our analysis against concrete measurements. Our measurement \nresults were ob\u00adtained from an instrumented version of the underlying abstract ma\u00adchine that counts resources \nused during the execution. For readability, the programs in this section use a more compact functional \nnotation than Schopenhauer, expression-level Hume [14], without a restriction to let-normal form. This \nHaskell-style no\u00adtation uses multiple rules with pattern matching instead of top\u00adlevel, asymmetric case \nexpressions. The basic type of integers is parametrised with its bit-size precision. We use the familiar \nnota\u00adtion of [] for Nil and _:_ for Cons in the pre-de.ned list type: data [a] = Nil | Cons a [a]. This \nnotation is automatically translated to the Schopenhauer code that is actually analysed. The examples \nchosen in this section focus on the main language features that are of interest in this paper: higher-order \nfunctions, polymorphism and destructive pattern matching. The examples are deliberately kept small to \ndemonstrate the applicability of our ap\u00adproach to these language features, without being side-tracked \nby previously-solved problems. For example, the variants of the sum\u00adof-squares function demonstrate how \nour analysis faithfully re.ects the increased performance that is achieved when turning a com\u00adposition \nof higher order functions into direct recursion. The .nal evaluator example is interesting because it \nmodi.es the argument function as it is passed through the recursive calls. 6.1 List-sum Our .rst example \ncomputes the sum of a list of integers (Figure 4). In order to demonstrate the use of our analysis on \nhigher-order functions, we de.ne the sum function as an instance of the standard (left-) fold function. \nA bound on the heap usage for the sum function is given by the following enriched type, where # represents \nthe \u00b5-type, i.e. list, with the constructors Cons and Nil. type num = int 16; add :: num -> num -> num; \nadd x y= x+ y; fold :: (num -> num -> num) -> num -> [num] -> num; fold fn[] =n; fold f n (x:xs) =fold \nf(f x n) xs; sum :: [num] -> num; sum xs = fold add 0 xs; Figure 4. Source code of list-sum SCHOPENHAUER \ntyping for HeapBoxed: list[Cons<2>:int,#|Nil] -(6/0)-> int The argument type includes annotations for \neach constructor, sepa\u00adrated by |. This shows that at most two units of heap are needed for every Cons \nconstructor in the input list (shown by the annotation Cons<2>). In addition to this input-dependent \npart, the sum func\u00adtion needs at most 6heap units, shown by the .rst annotation to the function type \n(-(6/0)->). As shown by the second annotation (the zero) and the absence of annotations in the result \ntype, the anal\u00adysis could not .nd a guarantee that any portion of the requested heap memory is unused \nafter execution. In total, given an input list of length n, the heap consumption of this function is \ntherefore bounded by 2n +6. This bound can be seen to be exact by direct inspection of the source code \nin Figure 4. In the sum function, a constant of 2 is needed to allocate the initial integer value of \n0. Another constant of 4 is needed to create a closure for the add function (a closure includes a tag, \na function pointer, plus counts of expected and sup\u00adplied arguments). In the fold function, a new integer \nvalue is cre\u00adated in each iteration through the application of fxn. This re\u00adquires two heap cells per \niteration. This value is therefore attached to the Cons constructor of the input. The bound on the stack \nconsumption for sum, shown below, is a constant for this tail-recursive program. The absence of annotations \nto the Cons constructor indicates that the bound is independent of the size of the input list. SCHOPENHAUER \ntyping for StackBoxed: list[Cons:int,#|Nil] -(27/17)-> int Finally, we can infer an upper bound on the \ntime consumption of this function using our worst-case execution costs in clock cycles for the Renesas \nM32C/85U processor. As expected, time consump\u00adtion is linear in the length of the input list (n), namely \n1714n+909. SCHOPENHAUER typing for TimeM32: list[Cons<1714>:int,#|Nil<225>] -(684/0)-> int The .rst block \nin Table 1 compares the analysis bounds above with our measured results, applying sum to the initial \nsegments of the input list [1,2,3,4,5] of lengths N =1... 5. Since we analyse and measure the entire \ncode, including the costs for generating the test input, the absolute values given in the table are slightly \nhigher than the values calculated from the function types above. The ratio of inferred to measured costs \nis used to assess the quality of our bounds against actual behaviour. We can see that the predicted heap \nconsumption is exact in all cases. For stack usage, the measured costs for this tail-recursive program \nare constant (34). The inferred  fold f n l= LET l1 =l IN case l1 of [] -> LET n1 =n IN n1; | (x:xs) \n-> LET xs1 = xs IN LET n1 =n IN LET x1 =x IN LET z1 =fx1 n1 IN LET f1 =f IN fold f1 z1 xs1; Figure 5. \nIntermediate code form of function fold. bounds are also constant but not exact in this case. Finally, \nour time predictions are a close match to actual execution times, yielding an estimate that is between \n18% and 26% higher than the actual cost. In general, we expect less accurate bounds for time, because \nthe entries in the cost table are already worst-case bounds for the primitive operations of the abstract \nmachine. 6.1.1 Let-normal form Recall from the introduction that in order to remove annoying redundancies \nfrom the proof of Theorem 1, we require programs to be in let-normal form. Programs can automatically \nbe transformed into let-normal form without altering their (cost) behaviour using a second LET-construct \nthat simply has zero costs assigned to it. Another advantage of the LET-construct is that we can keep \nour big\u00adstep semantics for measuring worst-case execution time and stack space usage, for which small-step \nsemantics are usually required. This is achieved by adopting the policy that each sub-expression must \nbe a unique variable and that this variable is introduced by the LET-construct immediately before its \n(single) use. For example, the fold function from Figure 4 would be transformed into the let\u00ad normal \nform of Figure 5. Under this policy, the rule for function calls can expect that all arguments are available \non the stack. The cost for pushing variables on the top of the stack or creating constants was already \nmodelled by the ordinary VAR, INT and BOOL rules. It follows that only the cost of popping the arguments \nfrom the stack, after returning from the call, must be included in rule APP. An additional bene.t is \nthat the order in which the arguments are placed on the stack is also made explicit in the code by the \norder of the LET bindings. Al\u00adthough our prototype implementation always adheres to it, we have refrained \nfrom strictly enforcing this policy in Schopenhauer be\u00adcause it is not intrinsic to our analysis method, \nand it is conceivable that other cost models might not require such a strict convention. maining resources \nafter. In that sense, we start with x resources available, since we are in the body of the fully applied \nfunction. The outermost term constructor is a case distinction, so CASE applies. On top of the turnstile \nin the conclusion we have q1+Keof. Hence we gather the implicit inequality x = q1 + Keof. We follow the \nbranch of the type derivation for the successful match of the Cons-constructor, which according to the \nCASE rule now has q1 + p -KcaseT resources available. Next, the LET rule applies. Matching the available \nresources yields the second inequality q1 + p -KcaseT = q2 + KLET1, and according to the .rst premise \nwe have q2 resources available for the call fxn. According to the APP type rule, a call to function f \nrequires us to pay Kapp + Knext. Hence we have the inequality q2 = v + Kapp + Knext. In addition, we \nmust apply subtyping to match the annotations of the argument types and the functions. However, no constraints \nare generated here, since both are unannotated numeric types. Furthermore, any constraints that are directly \nattached to function f are also added now. The inference renames all bound variables in these constraints, \nprobably including v and w, but again, for simplicity, we ignore this here. The renaming allows different \npossible resource usages for each function application, as described in Section 6.6.1. Since APP is a \nterminal rule, we are left with w - Kapp ' resources. Note that we always apply WEAK before any terminal \nrule, to allow excess resources to be carried over. Again, we ignore this in this example. We have now \nreturned to the second premise of rule LET and can obtain the constraint w - Kapp ' = q3 + KLET2, leaving \nus with q3 resources for the body of the LET-expression, a recursive call. The application of WEAK is \ncrucial in this case, so we obtain q3 = q4 and q3 - q4 = q6 -q5, where q4 and q6 are fresh, and q5 represents \nthe remaining resources after applying APP again. This in turn yields the constraints q4 = x + Kapp + \nKnext and y -Kapp ' = q5. We are therefore left with q6 after the weakening. Matching q6 against the \nremaining resources guaranteed by LET then yields q6 -KLET3 = q7. Finally, using CASE we obtain q7 -KcaseT \n' = y in a similar way. Let . denote the set containing these constraints. If we instan\u00adtiate the resource \nparameters according to the desired cost model and specify that all variables must be non-negative, . \ncould now be solved by an LP-solver, yielding an annotated type for the func\u00adtion. However, if this function \nde.nition is part of a bigger program, we do not solve the constraints at this point, but rather use \nthem to improve the type of the function to  6.1.2 Manual amortised cost analysis demonstration ` We \nnow illustrate how the type rules are applied and perform a manual analysis for a simpli.ed version of \none branch of the fold. case l of (x:xs) -> LET z1 = fx nIN fold fz1 xs; The .rst step is to enrich \nthe type for the fold function with fresh variables, representing the as yet unknown annotations. \u00b4 for \nhuman readers. These constraints in fact reduce to: xy 00 00 vw 00 int-.int-.-.int-.list(int,p)-. where \nBr = {p, q1,q2,q3,q4,q5,q6,q7, v, w, x, y}, so that the function may be used with differing resource \nbehaviours. Simplifying . by eliminating the intermediate variables (qi) and summing the constant cost \nparameters to give some suitable constant Ci can help make the constraint set more comprehensible .Br... \nint int `\u00b4 xy ef c vw ab int-.int-.-.int-.list(int,p)-.where list(int,p)is a convenient shorthand \nfor the simpli.ed list int d int x +p = v + C1 w = x + C2 type \u00b5a.{Cons:(p, (int,a))|Nil:(0)}. For the \nsake of simplic\u00adity, we immediately set the variables a, b, c, d, e and f to the zero value, since they \nare non-essential in this example. We then follow the standard type derivation tree for the code, gathering \nconstraints as we go. We must also reconstruct the im\u00adplicit inequalities hidden by our compressed type \nrule notation. However, this is very simple, if we adopt the view that the value on top of the turnstile \nrepresents the currently available resources before executing the term and the one below the guaranteed \nre-It is possible to spot the recursive nature of fold in these con\u00adstraints, since x occurs both on \nthe left and right hand side, i.e. the cost must be paid in full by p. This is justi.ed, since each recursive \nstep introduces a new Cons-constructor, bearing a potential of p. Both x and y can have arbitrary values, \nwhich is sound since we ignored the terminating case branch in this example. For the full fold function, \nwe need to similarly examine the branch dealing with [], This produces the constraint x = y +C3, restricting \nx and y, as expected.  CallsHeapStack Time data tree = Leaf num | Node tree tree; dfsAcc ::(num -> \n[num] -> [num]) -> tree -> [num] -> [num]; dfsAcc g (Leaf x) acc = g x acc; dfsAcc g (Node t1 t2) acc \n= let acc = dfsAcc g t1 acc in dfsAcc g t2 acc ; cons :: num -> [num] -> [num]; cons x xs = x:xs; revApp \n:: [num] -> [num] -> [num]; revApp [] acc = acc; revApp (y:ys) acc = revApp ys (y:acc); flatten :: tree \n-> [num]; flatten t = revApp (dfsAcc cons t []) []; Figure 6. Source code of tree-.attening (flatten) \n  6.2 Tree operations The next two examples operate over trees. The .rst is a tree .at\u00adtening function, \nusing a higher-order depth-.rst-traversal of a tree structure that is parametrised by the operation that \nis applied at the leaves of the tree. The source code is given in Figure 6. Again, the bounds for heap, \nstack, and time consumption arelinear in the number of leaves (l) and nodes (n) in the input struc\u00adture: \nthe heap consumption is 8l +8, the stack consumption is 10l +16n +14, and the time consumption is 2850l \n+938n +821. SCHOPENHAUER typing for HeapBoxed: (tree[Leaf<8>:int|Node:#,#]) -(8/0)-> list[Cons:int,#|Nil] \nSCHOPENHAUER typing for StackBoxed: (tree[Leaf<10>:int|Node<16>:#,#]) -(14/23)-> list[Cons:int,#|Nil] \nSCHOPENHAUER typing for TimeM32: (tree[Leaf<2850>:int|Node<938>:#,#]) -(821/0)-> list[Cons:int,#|Nil] \nThe second block in Table 1 compares analysis and measurement results for the tree-.attening example. \nAgain the bounds for heap are exact. For stack, the analysis delivers a linear bound, whereas the measured \ncosts are logarithmic in general. Here, we could use\u00adfully apply a further extension of the amortised \ncost based analysis. Campbell [6] has developed methods for associating potential in re\u00ad lation to the \ndepth of data structures. This is more suitable for stack\u00adspace usage. It also allows temporary borrowing \nof potential. The time bounds give very good predictions, with an over-estimate of at most 13%for the \nrange of inputs shown here. The second operation on trees is the repmin function which replaces all leaves \nin a tree with the element with the minimal value. This function is implemented in two phases, both using \nhigher\u00adorder functions: the .rst phase computes the minimal element using a tree-fold operation; the \nsecond phase .lls in this minimal element using a tree-map function. The third block in Figure 1 compares \nanalysis and measurement results for the repmin example. Again the bounds for heap are exact. For stack, \nwe observe a linear bound but with a more pro\u00adnounced difference between the measured and analysed costs. \nThis is due to the two tree traversals. The time bounds, however, show a good match against the measured \ncosts, with an over-estimate of at most 22%.  6.3 Sum-of-squares In this section, we study 3 variants \nof the classic sum-of-squares example (Figure 7). This function takes an integer n as input and calculates \nthe sum of all squares ranging from 1to n. The .rst vari\u00adant is a .rst-order program using direct recursion, \nwhich does not N= 10 sum-of-squares (variant 1: direct recursion) Analysis 22 114 30 18091 Measured 22 \n108 30 16874 Ratio 1.00 1.06 1.00 1.07 sum-of-squares (variant 2: with map and fold) Analysis 56 200 \n114 53612 Measured 56 200 112 42252 Ratio 1.00 1.00 1.02 1.27 sum-of-squares (variant 3: also unfold) \nAnalysis 71 272 181 77437 Measured 71 272 174 59560 Ratio 1.00 1.00 1.04 1.30 Table 2. Results for three \nvariants of the sum-of-squares function --common code sq n=n*n; add mn=m+n; --map, (left-) fold over \nlists are standard --enumFromTo m n generates [m..n] (tail-recursive) --variant 1: direct recursion \nsum_sqs n m s = if (m>n) then s else sum_sqs (n-1) m (s+(sq n)); sum_sqs n = sum_sqs n 1 0; --variant \n2: uses h-o fcts fold and map sum xs = fold add 0 xs; sum_sqs n = sum (map sq (enumFromTo 1 n)); --variant \n3: uses h-o fcts unfold, fold and map data maybenum = Nothing | Just num; unfoldr :: (num -> maybenum) \n-> num -> [num]; unfoldr f z = case f z of Nothing -> [] | (Just z ) -> z :(unfoldr f z ); countdown \n:: num -> maybenum; countdown m = if (m<1) then Nothing else Just (m-1); enum :: num -> [num]; --this \ngenerates [n,n-1..1] enum n = if (n<1) then [] else n:(unfoldr countdown n); sum_sqs :: num -> num; \nsum_sqs n = sum (map sq (enum n)); Figure 7. Source code of sum-of-squares (3 variants) construct any \nintermediate list structures. The second variant uses the higher-order functions map and fold to compute \nthe squares over all list elements and to sum the result, respectively. The third version additionally \nuses the higher-order function unfold to gen\u00aderate the initial list of numbers from 1to n. Table 2 summarises \nanalysis and measurement results for all three variants. As expected, the higher-order versions of the \ncode exhibit signi.cantly higher resource consumption, notably for the second and third variants which \ngenerate two intermediate lists. These additional costs are accurately predicted by our analysis. In \nparticular, the heap costs are exactly predicted and the stack costs are almost exact. The time results \nare within 30% of the measured costs. We consider this to be a very good worst-case estimate for higher-order \ncode. As discussed before, our inference engine is largely independentof the actual resource being inferred. \nWe can therefore easily adaptour analysis to other resources simply by replacing the basic costtable \nthat is used to model the program execution costs. We exploit this capability here to infer bounds on \nthe total number of functioncalls in a program expression. This metric is of particular interestfor higher-order \nprograms (this is discussed in more detail in [30]).The results for this resource are given in the second \ncolumn ofTable 2. The .rst variant exhibits the lowest number of function calls, since all three phases \nof the computation are covered by onerecursive function. Thus, we have one function call to the squarefunction \nand one recursive call for each integer value. Additionally,we have one call to the top level function: \n SCHOPENHAUER typing for CallCount: (int<2>) -(2/1)-> int The second variant separates the phases of \ngenerating a list, com\u00adputing the squares and summing them. The generation phase, im\u00adplemented using \ndirect recursion, needs one call per iteration. Theother two phases each need two calls per iteration: \none for thehigher-order function, and one for the function being applied. Intotal we have 5n +6 calls, \nas encoded by the following type: SCHOPENHAUER typing for CallCount: (int<5>) -(6/0)-> int The third \nvariant again has three phases. Now all three phases usehigher-order functions, with the enumeration \nbeing implementedthrough a call to unfold. The number of calls therefore increases to 7n +1. This is \nencoded by the following type: SCHOPENHAUER typing for CallCount: (int<7>) -(1/0)-> int  6.4 Polymorphic \nfunctions As an example of a simple polymorphic function we examine theresource consumption of the twice \nand quad functions: type afct =a -> a; twice :: afct -> afct; twice fx =f (f x); quad :: afct -> afct; \nquad fx=let f =twice fin twice f x; We obtain the following polymorphic type as heap bound for quad: \nSCHOPENHAUER typing for HeapBoxed: (a -(0/0)-> a) -(0/0)-> a -(5/0)-> a The resource consumption for \nquad is expressed by the annotationon the top level function type: .ve heap cells are required to build \naclosure for twice f, which contains one .xed argument. The zerosfor the function argument are provisional, \nthe LP-solver has simplychosen a possible solution. When applied to a concrete function,the merged constraints \nwill need to be solved once again. Applyingthe successor function succ, which has a .xed cost of four \nheapunits, then yields the correct typing of: SCHOPENHAUER typing for HeapBoxed: int -(21/0)-> int Using \na call count metric for the number of calls to the function succ in the expression quad succ 1, we obtain \nthe following bound. This accurately indicates that succ is called precisely four times. SCHOPENHAUER \ntyping for CallCount: 4, int ,0 6.5 Destructive pattern matching A primary motivation for our analysis \nis to prove bounded resourceconsumption on resource constrained hardware, such as embeddedsystems. It \nis, therefore, important that our analysis can cover tech\u00adniques that are frequently employed to produce \nprograms with asmall resource footprint. We address this issue here, and in our sub\u00adsequent examples, \nby i) testing our analysis on programs with de\u00adstructive pattern matching, and ii) by using a more space-ef.cient,unboxed \nrepresentation of the heap. Due to the .exible design ofour inference engine, both aspects can be modelled \nwithout modi\u00adfying the engine itself: only the cost tables need to be changed. Our.rst example to test \nthese features is in-place list reversal: revApp acc zs = case! zs of [] -> acc | (x:xs) -> revApp (x:acc) \nxs; reverse xs = revApp [] xs; type pred = num -> num -> bool; insert :: pred -> num -> [num] -> [num]; \ninsert cmp x zs = case! zs of [] -> x :[] |(y:ys) -> if cmp x ythen x: y :ys else y:insert cmp x ys; \nsort :: pred -> [num] -> [num]; sort cmp zs = case! zs of [] -> [] | (x:xs) -> insert cmp x (sort cmp \nxs); leq :: pred; leq xy = x<=y; isort :: pred -> [num] -> [num]; isort xs = sort leq xs; Figure 8. Source \ncode of in-place insertion sort The heap bound below shows that only constant heap space isneeded for \nthe reverse function. A constant heap space of 2 is needed for the initial Nil constructor passed to \nrevApp. Due to the destructive nature of the pattern matches in revApp, the list cells of the input list \ncan be re-used. Similarly, the Nil constructor of the input-list can be re-used for the result. Thus, \nthe second 2 in the function type indicates that two resources are given back aftercompletion of the \nrevApp function. In total, the heap usage afterexecution is the same as before. SCHOPENHAUER typing for \nHeapUnboxed: list[Nil|Cons:int,#] -(2/2)-> list[Nil|Cons:int,#] The stack and time consumption, however, \nare both linear: SCHOPENHAUER typing for StackBoxed: list[Nil<1>|Cons<3>:int,#] -(11/11)-> list[Nil|Cons<1>:int,#] \nSCHOPENHAUER typing for TimeM32: list[Nil<225>|Cons<858>:int,#] -(481/0)-> list[Nil|Cons:int,#] A more \ninteresting example is in-place insertion sort, parametrisedover the comparison function (Figure 8). \nThe insert function uses destructive pattern-matching to re-use the current cell of the inputlist when \nconstructing the result list. The destructive pattern matchin sort ensures that the call to insert has \none list cell to start with. Using an unboxed heap model, which does not allocate heapspace for the comparison \noperations, we can show that this functiondoes not require any additional heap space: SCHOPENHAUER typing \nfor HeapUnboxed: (list[Cons:int,#|Nil]) -(0/0)-> list[Cons:int,#|Nil]  6.6 An evaluator for expressions \nOur .nal example is an evaluator function for a small subset of the Schopenhauer language itself, using \nonly integer types and without function calls. Even this loop-free version of the language is interesting, \nsince it uses a function to model the environment, and the evaluation of a let-expression modi.es this \nfunction. The code for the evaluation function is shown in Figure 9. The analysis of the eval function \nproduces the following heapbound for an unboxed heap model: SCHOPENHAUER typing for HeapUnboxed: (int \n-(0/0)-> int) -(0/0)-> exp[Const:int |VarOp:int |IfOp:int,#,# |LetOp<9>:int,#,# |UnOp:(int -(0/ANY)-> \nint<ANY>),# |BinOp:(int -> int -(0/ANY)-> int<ANY>),#,#]-(0/0)->int Most notably the analysis distinguishes \nbetween different con\u00adstructors when examining an expression. For constants or variables,  type num \n= int 16; type val = num; type var = int 16; type env = var -> val; data exp = Const val | VarOp var \n| IfOp var exp exp | LetOp var exp exp | UnOp (val->val) exp | BinOp (val->val->val) exp exp; _true = \n1; _false = 0; eval :: env -> exp -> val; eval rho (Const n) = n; eval rho (VarOp v) = rho v; eval rho \n(IfOp v e1 e2) = if (rho v)==_false then eval rho e2 else eval rho e1; eval rho (LetOp v e1 e2) = let \nx = eval rho e1 ; rho v = if v==v then x else rho v in eval rho e2; eval rho (UnOp f m) = f (eval rho \ne1); eval rho (BinOp f m n) = f (eval rho e1) (eval rho e2); Figure 9. Source code of the evaluator example \nno heap costs are incurred, since the result value is returned on the stack. For an if-expression, the \ntotal costs comprise those for the sub-expressions, represented as # in the type. No further costs are \nadded for the variable lookup. For a let-expression, a modi.ed en\u00advironment is de.ned. This amounts to \nthe construction of a closure with two .xed variables in the heap (9heap cells in total). Finally, the \nprimitive unary and binary operators do not use any heap cells, since the result value will be produced \ndirectly on the stack. 6.6.1 Resource parametric recursion Interestingly, the eval function cannot be \nanalysed under the boxed heap cost model analysing this function would require polymor\u00adphic recursion \n[15, 26], which we do not support. The second re\u00ad cursive call in the case dealing with LetOp requires \na different type, since the annotated type of the .rst argument has changed. Function rho is more expensive \nto execute than rho, because adding the equality operation allocates a boolean value in the boxed heap \ncost model, and this cannot be amortised against the inputs of rho . In future, we intend to investigate \nwhether the considerable increase in complexity brought about by polymorphic recursion might be warranted \nby the possible gain in expressivity.   7. Related Work Our discussion of related work focuses on analyses \nfor strict, higher-order programs. A discussion of analyses for .rst-order pro\u00adgrams is given in another \npaper [25]. 7.1 Amortised Analysis The focus of most previous work on automatic amortised cost analyses \nhas been on determining the costs of .rst-order rather than higher-order programs. For example, Hofmann \ns linearly\u00adtyped functional programming language LFPL [17] uses linear types to determine resource usage \nin terms of the number of con\u00adstructors used by a program. First-order LFPL de.nitions can be computed \nin bounded space, even in the presence of general recur\u00adsion. Adding higher-order functions to LFPL raises \nthe expressive power in terms of complexity theory from linear space (LFPL) to exponential time [18]. \nHofmann and Jost subsequently described an automatic inference mechanism for heap-space consumption in \na functional, .rst-order language [19], using an amortised cost model. This work uses a deallocation \nmechanism similar to that we have used here, but is built on a difference metric similar to that of Crary \nand Weirich [9]. The latter, however, only checks bounds, and does not infer them, as we do. Taha et \nal. s GeHB [36] staged notation automatically generates .rst-order, heap-bounded LFPL programs from higher-order \nspeci\u00ad.cations, but likewise requires the use of non cost-preserving trans\u00adformations. We are not aware \nof any other work targeting automatic amortised analysis for higher-order de.nitions. However, Camp\u00adbell \n[6] has studied how the Hofmann/Jost approach can be applied to stack analysis for .rst-order programs, \nusing give-back anno\u00adtations to return potential. This improves the quality of the analysis results that \ncan be obtained for stack-like metrics. While, in order to keep the presentation clear, we have not done \nso here, there is no technical reason why give-back potential cannot also be applied to the higher-order \nanalysis that we have described. Recent work has aimed to overcome the linearity restriction when analysing \n.rst-order programs. For example, Shkaravska et al. aim to extend the amortised cost approach to non-linear \nbounds using resource functions in the constraints, rather than simple variables [35].  7.2 Sized Types \nSized types [22] express bounds on data structure sizes. They are attached to types in the same way as \nthe weights we have used here. The difference is that sized types express bounds on the size of the underlying \ndata structure, whereas our weights are factors of a linear resource bound. Hughes, Pareto and Sabry \n[22] originally described a type checking algorithm for a simple higher-order, non-strict functional \nlanguage to determine progress in a reactive system. This work was subsequently developed to describe \nspace usage in Embedded ML [21], a strict functional language using regions to control memory usage. \nAbel [1] extended higher-order sized types to allow higher-kinded types with embedded function spaces. \nHe used this system to formalise termination checking but did not tackle resource consumption in general. \nA combination of sized types and regions is also being developed by Pe na and Segura [32], building on \ninformation provided by ancillary analyses on termination and safe destruction. The focus of this work \nis on determining safety properties rather than resource usage, however. Chin and Khoo [7] introduced \na type inference algorithm that is capable of computing size information from high-level program source. \nChin et al. [8] presented a heap and a stack analysis for a low-level (assembler) language with explicit \n(de-)allocation. By inferring path-sensitive information and using symbolic evaluation they are able \nto infer exact stack bounds for all but one example program. Vasconcelos and Hammond have independently \ndeveloped au\u00adtomatic inferences that are capable of deriving cost equations for abstract time-and heap-consumption \nfrom unannotated program source expressions based on the inference of sized types for re\u00adcursive, polymorphic, \nand higher-order programs [39]. Vasconce\u00ad los PhD thesis [38] extended these previous approaches by using \nabstract interpretation techniques to automatically infer linear ap\u00adproximations of the sizes of recursive \ndata types and the stack and heap costs of recursive functions. By including user-de.ned sizes, it is \npossible to infer sizes for algorithms on non-linear data struc\u00adtures, such as binary trees. Finally, \nDanielsson [10] has recently introduced a library of functions that he claims makes the analysis of a \nnumber of purely functional data structures and algorithms almost fully formal. He does this by using \na dependent type system to encode information about execution time, and then by combining individual \ncosts into an overall cost using an annotated monad.  7.3 Abstract Interpretations While having the \nattraction of being very general, one major dis\u00adadvantage of abstract interpretations is that analysis \nresults usually depend on the existence of concrete data values. Where they can be applied, impressive \nresults can, however, be obtained even for large commercial applications. For example, AbsInt s aiT tool \n[11], and Cousot et al. s ASTREE system [5] have both been deployed in the design of the software of \nAirbus Industrie s Airbus A380. Typically, such tools are limited to non-recursive programs , and may \nrequire signi.cant programmer effort to use effectively. We are aware of very little work that considers \nuser-de.ned higher-order programs, though Le M\u00b4etayer s work [28] can handle prede.ned higher-order functions \nwith known costs, and Benzinger s work on worst-case complexity analysis for NuPrl [3] similarly supports \nhigher-order functions if the complexity information is provided explicitly. Huelsbergen, Larus and Aiken \n[20] have de.ned an ab\u00ad stract interpretation of a higher-order, strict language for determin\u00ading computation \ncosts that dependent on the size of data structures. This static analysis is combined with run-time size \ninformation to deliver dynamic granularity estimates. Gulwani, Mehra and Chilimbi s SPEED system [13] \nuses a symbolic evaluation approach to calculate non-linear complexity bounds for C/C++ procedures using \nan abstract interpretation\u00adbased invariant generation tool. Precise loop bounds are calculated for 50% \nof the production loops that have been studied. Unlike our work, they target only .rst-order programs. \nAlso unlike our work, they consider only time bounds. They do, however, consider non\u00adlinear bounds and \ndisjunctive combination of cost information. The COSTA system [2] performs a fully automatic resource \nanalysis for an object-oriented bytecode language. It produces a closed-form upper bound function over \nthe size of the input. Unlike our system, however, data-dependencies cannot be expressed. Finally, G\u00b4omez \nand Liu [12] have constructed an abstract inter\u00ad pretation for determining time bounds on higher-order \nprograms. This executes an abstract version of the program that calculates cost parameters, but which \notherwise mirrors the normal program execution strategy. Unlike our type-based analysis, the cost of \nthis analysis therefore depends directly on the complexity (or actual values) of the input data and the \nnumber of iterations that are per\u00adformed, does not give a general cost metric for all possible inputs, \nand will fail to terminate when applied to non-terminating pro\u00adgrams.  8. Conclusions and Further Work \nBy developing a new type-based, resource-generic analysis, we have been able to automatically infer linear \nbounds on real-time, heap usage, stack usage and number of function calls for strict, higher-order functional \nprograms. The use of amortised costs al\u00adlows us to determine upper bound cost functions on the overall \nresource cost of running a program, which take the sizes of pro\u00adgram arguments as their inputs. We have \nextended previous work on amortised-cost-based inference [19, 25] by considering higher\u00ad order and polymorphic \nprograms, and by constructing a generic treatment of resource usage through resource tables that can \nbe spe\u00adcialised to different cost metrics and execution models. In this way we achieve a clean separation \nof the mechanics of inference from the concrete cost metrics that we use. We have demonstrated the .exibility \nof the resource table approach by building an analysis to determine the number of function calls in a \nhigher-order program. Another key advantage of this separation is that our basic sound\u00adness proof applies \nregardless of the cost metric that we use. There is, however, signi.cant recent work on determining loop \nbounds for iterative programs as part of a worst-case execution time analysis, e.g. [29]. Our results \nfor a range of higher-order programs demonstrate the high quality of the bounds that we can infer. For \nheap space, we can generally achieve an exact prediction. For worst-case execution time, the bounds we \nachieve are within 30% of the measured costs. For stack, we generally achieve good results, but occasionally \nobtain bounds that are linear where the measured costs are constant. This is not inherent to our analysis. \nFor example, Campbell has studied how to improve stack bounds for amortised analysis [6]. Crucial to \nthe usability of our inference is its high degree of ef.\u00adciency, its full automation and the absence \nof mandatory program\u00admer annotations. Being built on a high-performance linear program solver our inference \nis very ef.cient: for the examples that we have used in this paper, the sizes of the constraint sets \nvary between 64 and 350 constraints, with the analysis runtime never exceeding 1 second, including constraint \nsolving. However, the restriction to a linear constraint system does impose limits on the range of pro\u00adgrams \nwhose costs can be analysed. Precisely classifying the pro\u00adgrams that can be analysed is an interesting \ntheoretical question for all forms of cost analysis. While it would be possible to construct a restrictive \nclassi.cation on source-level programs, this would ei\u00adther exclude many programs that are, in fact, analysable, \nor include many programs that were not analysable. This does not, therefore, seem to be a constructive \nactivity. The most precise classi.cation is that our analysis will succeed exactly where the cost equations \nhave a linear bound. While the inclusion of tail-call optimisations and other cost-simplifying optimisations \ncan actually extend the range of programs that can be costed, the restriction to linearity remains both \na theoretical and practical limitation. 8.1 Further Work Incorporating Sized Types. As we have seen, \nsized-type systems provide information about data structure sizes. Although they can be used to provide \ncost information when combined with a suitable constraint inference algorithm [39], they are complementary \nto the amortised cost approach described here, in that our weights for data structures are multiples \nof input data structure sizes. Sized type systems should allow these sizes to be inferred statically \nfor a number of common data structures. Non-Linear Constraints. An extension of the amortised cost based \napproach to polynomial bounds for a .rst-order language is ongoing work [16]. We have also begun to investigate \nwhether combining our approach with a sized-type analysis might also al\u00adlow the inference of super-linear \nbounds, while still using ef.cient LP-solver technology (multiple times). Non-Strictness. Our work is \nrestricted to strict programming lan\u00adguages. An extension of our work to non-strict programming lan\u00adguages, \nsuch as Haskell, requires the solution of two technical prob\u00adlems: .rstly, we must identify when computations \nare needed; and, secondly, we must have a formal operational semantics of non-strict evaluation that \nwill allow us to identify resource usage in the way we have done here. We are in the process of producing \na cost model and analysis based on Launchbury s semantics for graph reduc\u00adtion [27], which incorporates \nnotions of evaluation-and sharing\u00adcontexts to determine where potentials may be used.  Acknowledgements \nWe would like to thank our colleagues Pedro Vasconcelos and Hugo Sim oes (both Universidade do Porto, \nPortugal) for their fruit\u00adful discussion of the rules and proofs, and the anonymous reviewers for their \nhelpful suggestions. This work has been supported by EU Framework VI grants IST-510255 (EmBounded), IST-15905 \n(Mo\u00adbius), and RII3-CT-2005-026133 (SCIEnce); and by EPSRC grant EP/F030657/1 (Islay).   References \n[1] A. Abel. A Polymorphic Lambda-Calculus with Sized Higher-Order Types. PhD thesis, Ludwig-Maximilians-Universit\u00a8at \nM\u00a8unchen, 2006. [2] E. Albert, S. Genaim, and M. G\u00b4omez-Zamalloa. Live Heap Space Analysis for Languages \nwith Garbage Collection. In Proc. ISMM 2009: Intl. Symp. on Memory Management, pages 129 138, Dublin, \nIreland, June 2009. ACM. [3] R. Benzinger. Automated Complexity Analysis of Nuprl Extracted Programs. \nJournal of Functional Programming, 11(1):3 31, 2001. [4] M. Berkelaar, K. Eikland, and P. Notebaert. \nlp solve: Open Source (Mixed-Integer) Linear Programming System. Pub\u00adlished under GNU LGPL (Lesser General \nPublic Licence). http://lpsolve.sourceforge.net/5.5 . [5] B. Blanchet, P. Cousot, R. Cousot, J. Feret, \nL. Mauborgne, A. Min\u00b4e, D. Monniaux, and X. Rival. A Static Analyzer for Large Safety-Critical Software. \nIn Proc. PLDI 03: Conf. on Programming Lan\u00adguage Design and Implementation, pages 196 207, San Diego, \nUSA, June 2003. ACM. [6] B. Campbell. Amortised Memory Analysis Using the Depth of Data Structures. In \nProc. ESOP 2009: European Symposium on Program\u00adming, LNCS 5502, pages 190 204, York, UK, 2009. Springer. \n[7] W.-N. Chin and S.-C. Khoo. Calculating Sized Types. Higher-Order and Symbolic Computing, 14(2,3):261 \n300, 2001. [8] W.-N. Chin, H. Nguyen, C. Popeea, and S. Qin. Analysing Memory Resource Bounds for Low-Level \nPrograms. In Proc. ISMM 08: Intl. Symp. on Memory Management, pages 151 160, Tucson, USA, June 2008. \nACM. [9] K. Crary and S. Weirich. Resource Bound Certi.cation. In Proc. POPL 00: Symp. on Princ. of Prog. \nLangs, pages 184 198, Boston, USA, 2000. ACM. [10] N. Danielsson. Lightweight Semiformal Time Complexity \nAnalysis for Purely Functional Data Structures. In Proc. POPL 08: Symp. on Princ. of Prog. Langs, pages \n133 144, San Francisco, USA, 2008. [11] C. Ferdinand, R. Heckmann, M. Langenbach, F. Martin, M. Schmidt, \nH. Theiling, S. Thesing, and R. Wilhelm. Reliable and Precise WCET Determination for a Real-Life Processor. \nIn Proc EMSOFT 01: Intl Workshop on Embedded Software, LNCS 2211, pages 469 485, Tahoe City, USA, Oct. \n2001. Springer. [12] G. Gomez and Y. Liu. Automatic Time-Bound Analysis for a Higher-Order Language. \nIn Proc. LCTES 98: Conf. on Languages, Compilers and Tools for Embedded Systems, LNCS 1474, pages 31 \n40, Montreal, Canada, June 1998. Springer. [13] S. Gulwani, K. Mehra, and T. Chilimbi. SPEED: Precise \nand Ef.cient Static Estimation of Program Computational Complexity. In Proc. POPL 09: Symp. on Princ. \nof Prog. Langs, pages 127 139, Savannah, USA, Jan. 2009. ACM. [14] K. Hammond and G. Michaelson. Hume: \na Domain-Speci.c Lan\u00adguage for Real-Time Embedded Systems. In Proc. GPCE 2003: Intl. Conf. on Generative \nProg. and Component Eng., LNCS 2830, pages 37 56, Erfurt, Germany, Sept. 2003. Springer. [15] F. Henglein. \nType Inference with Polymorphic Recursion. ACM TOPLAS: Trans. Prog. Langs. Systs, 15(2):253 289, April \n1993. [16] J. Hoffmann and M. Hofmann. Amortized Resource Analysis with Polynomial Potential. In preparation. \n[17] M. Hofmann. A Type System for Bounded Space and Functional In-Place Update. Nordic Journal of Computing, \n7(4):258 289, 2000. [18] M. Hofmann. The Strength of non Size-Increasing Computation. In Proc. POPL 02: \nSymp. on Princ. of Prog. Langs, pages 260 269, Portland, USA, Jan. 2002. ACM. [19] M. Hofmann and S. \nJost. Static Prediction of Heap Space Usage for First-Order Functional Programs. In Proc. POPL 03: Symp. \non Princ. of Prog. Langs, pages 185 197, New Orleans, USA, Jan. 2003. ACM. [20] L. Huelsbergen, J. Larus, \nand A. Aiken. Using the Run-Time Sizes of Data Structures to Guide Parallel Thread Creation. In Proc. \nLFP 94: Symp. on Lisp and Functional Programming, pages 79 90, Orlando, USA, June 1994. ACM. [21] R. \nHughes and L. Pareto. Recursion and Dynamic Data Structures in Bounded Space: Towards Embedded ML Programming. \nIn Proc. ICFP 99: Intl. Conf. on Functional Programming, pages 70 81, Paris, France, Sept. 1999. ACM. \n[22] R. Hughes, L. Pareto, and A. Sabry. Proving the Correctness of Reactive Systems Using Sized Types. \nIn Proc. POPL 96: Symp. on Princ. of Prog. Langs, pages 410 423, St. Petersburg Beach, USA, Jan. 1996. \nACM. [23] S. Ishtiaq and P. O Hearn. BI as an Assertion Language for Mutable Data Structures. In Proc. \nPOPL 01: Symp. on Princ. of Prog. Langs., pages 14 26. ACM, Jan. 2001. [24] H. Jonkers. Abstract Storage \nStructures. In Algorithmic Languages, pages 321 343. IFIP, North Holland, 1981. [25] S. Jost, H.-W. Loidl, \nK. Hammond, N. Scaife, and M. Hofmann. Carbon Credits for Resource-Bounded Computations using Amor\u00adtised \nAnalysis. In Proc. FM 09: Intl. Symp. on Formal Methods, LNCS 5850, Eindhoven, the Netherlands, Nov. \n2009. Springer. [26] A. J. Kfoury, J. Tiuryn, and P. Urzyczyn. Type Reconstruction in the Presence of \nPolymorphic Recursion. ACM TOPLAS: Trans. Prog. Langs. Systs, 15(2):290 311, April 1993. [27] J. Launchbury. \nA Natural Semantics for Lazy Evaluation. In Proc. POPL 93: Symp. on Princ. of Prog. Langs., pages 144 \n154, Charleston, USA, Jan. 1993. ACM. [28] D. Le M\u00b4etayer. ACE: An Automatic Complexity Evaluator. ACM \nTOPLAS: Trans. on Prog. Langs. and Systs, 10(2), April 1988. [29] B. Lisper. Fully Automatic, Parametric \nWorst-Case Execution Time Analysis. In Proc. WCET 03: Intl. Workshop on Worst-Case Execu\u00adtion Time Analysis, \npages 99 102, 2003. [30] H.-W. Loidl and S. Jost. Improvements to a Resource Analysis for Hume. In Proc. \nFOPARA 09: Intl. Workshop on Foundational and Practical Aspects of Resource Analysis, Nov. 2009. Submitted. \n[31] C. Okasaki. Purely Functional Data Structures. Cambridge University Press, 1998. ISBN 0521663504. \n[32] R. Pena, C. Segura, and M. Montenegro. A Sharing Analysis for Safe. In Proc TFP 06: Symp. on Trends \nin Functional Programming, pages 109 128, Nottingham, UK, Apr. 2006. Intellect. [33] J. C. Reynolds. \nDe.nitional Interpreters for Higher-Order Program\u00adming Languages. In Proc of the 25th ACM National Conference, \npages 717 740. ACM, 1972. [34] N. Rinetzky, J. Bauer, T. Reps, M. Sagiv, and R. Wilhelm. A Semantics \nfor Procedure Local Heaps and its Abstractions. In Proc. POPL 05: Symp. on Princ. of Prog. Langs, pages \n296 309. ACM, Jan. 2005. [35] O. Shkaravska, R. van Kesteren, and M. van Eekelen. Polynomial Size Analysis \nof First-Order Functions. In Proc. TLCA 2007: Intl. Conf. on Typed Lambda Calculi and Applications, LNCS \n4583, pages 351 365, Paris, France, June 2007. Springer. [36] W. Taha, S. Ellner, and H. Xi. Generating \nHeap-Bounded Programs in a Functional Setting. In Proc. EMSOFT 03: Intl. Conf. on Embedded Software, \nLNCS 2855, pages 340 355. Springer, 2003. [37] R. E. Tarjan. Amortized Computational Complexity. SIAM \nJournal on Algebraic and Discrete Methods, 6(2):306 318, April 1985. [38] P. Vasconcelos. Cost Inference \nand Analysis for Recursive Functional Programs. PhD thesis, University of St Andrews, Feb. 2008. [39] \nP. Vasconcelos and K. Hammond. Inferring Cost Equations for Re\u00adcursive, Polymorphic and Higher-Order \nFunctional Programs. In Proc. IFL 2003: Intl. Workshop on Impl. of Functional Languages, LNCS 3145, pages \n86 101, Edinburgh, UK, Sept. 2003. Springer. [40] D. Walker and G. Morrisett. Alias Types for Recursive \nData Struc\u00adtures. In Proc. TIL 00: Types in Compilation, LNCS 2071, pages 177 206. Springer, 2000.  \n \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We describe a new <i>automatic </i> static analysis for determining upper-bound functions on the use of quantitative resources for strict, higher-order, polymorphic, recursive programs dealing with possibly-aliased data. Our analysis is a variant of Tarjan's manual <i>amortised cost analysis </i> technique. We use a type-based approach, exploiting linearity to allow inference, and place a new emphasis on the number of references to a data object. The bounds we infer depend on the sizes of the various inputs to a program. They thus expose the impact of specific inputs on the overall cost behaviour.</p> <p>The key novel aspect of our work is that it deals directly with polymorphic higher-order functions <i>without requiring source-level transformations that could alter resource usage </i>. We thus obtain <i>safe </i> and <i>accurate </i> compile-time bounds. Our work is <i>generic </i> in that it deals with a variety of quantitative resources. We illustrate our approach with reference to dynamic memory allocations/deallocations, stack usage, and worst-case execution time, using metrics taken from a real implementation on a simple micro-controller platform that is used in safety-critical automotive applications.</p>", "authors": [{"name": "Steffen Jost", "author_profile_id": "81100111171", "affiliation": "University of St Andrews, St Andrews, United Kingdom", "person_id": "P1911077", "email_address": "", "orcid_id": ""}, {"name": "Kevin Hammond", "author_profile_id": "81100001518", "affiliation": "University of St Andrews, St Andrews, United Kingdom", "person_id": "P1911078", "email_address": "", "orcid_id": ""}, {"name": "Hans-Wolfgang Loidl", "author_profile_id": "81100427673", "affiliation": "Ludwig-Maximilians University, Munich, Germany", "person_id": "P1911079", "email_address": "", "orcid_id": ""}, {"name": "Martin Hofmann", "author_profile_id": "81452607849", "affiliation": "Ludwig-Maximilians University, Munich, Germany", "person_id": "P1911080", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706327", "year": "2010", "article_id": "1706327", "conference": "POPL", "title": "Static determination of quantitative resource usage for higher-order programs", "url": "http://dl.acm.org/citation.cfm?id=1706327"}