{"article_publication_date": "01-17-2010", "fulltext": "\n Automatically Generating Instruction Selectors Using Declarative Machine Descriptions Jo ao Dias Tufts \nUniversity dias@cs.tufts.edu Abstract Despite years of work on retargetable compilers, creating a good, \nreliable back end for an optimizing compiler still entails a lot of hard work. Moreover, a critical component \nof the back end the instruction selector must be written by a person who is expert in both the compiler \ns intermediate code and the target machine s in\u00adstruction set. By generating the instruction selector \nfrom declar\u00adative machine descriptions we have (a) made it unnecessary for one person to be both a compiler \nexpert and a machine expert, and (b) made creating an optimizing back end easier than ever before. Our \nachievement rests on two new results. First, .nding a mapping from intermediate code to machine code \nis an undecidable problem. Second, using heuristic search, we can .nd mappings for machines of practical \ninterest in at most a few minutes of CPU time. Our most signi.cant new idea is that heuristic search \nshould be controlled by algebraic laws. Laws are used not only to show when a sequence of instructions \nimplements part of an intermediate code, but also to limit the search: we drop a sequence of instructions \nnot when it gets too long or when it computes too complicated a result, but when too much reasoning will \nbe required to show that the result computed might be useful. Categories and Subject Descriptors D.3.4 \n[Processors]: Code generation; D.3.4 [Processors]: Retargetable compilers General Terms Algorithms, Experimentation, \nTheory Keywords Instruction selection, retargetable compilers, declara\u00adtive machine descriptions 1. Introduction \nWriting a back end for an optimizing compiler is dif.cult. A com\u00adpiler writer must know not only the \nsyntax and semantics of a target instruction set but also the internal data structures and in\u00advariants \nof a compiler, which can be very complex. For years, the state of the art has been to write instruction \nselectors using domain-speci.c languages which combine knowledge of a com\u00adpiler s data structures with \nknowledge of a target machine (Aho, Ganapathi, and Tjiang 1989; Emmelmann, Schr\u00a8oer, and Landwehr 1989; \nFraser 1989; Fraser, Henry, and Proebsting 1992; Fraser, Hanson, and Proebsting 1992). We present a new \ntechnique which realizes a long-sought goal: decoupling compiler knowledge from machine knowledge (Conway \n1958; Strong et al. 1958). Using our Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 10, January 17 23, 2010, Madrid, Spain. Copyright c . 2010 ACM \n978-1-60558-479-9/10/01. . . $10.00 Norman Ramsey Tufts University nr@cs.tufts.edu technique, an instruction \nselector is generated automatically from declarative machine descriptions. (A declarative machine descrip\u00adtion \ncontains no code and no information about any compiler s data structures; instead, it simply and formally \ndescribes properties of a target machine.) Our contributions are as follows: We show that given a description \nof an arbitrary instruction set, generating an instruction selector is undecidable (Section 8). To .nd \nmachine instructions that implement intermediate code, it is therefore necessary to search heuristically. \n We present a new heuristic-search algorithm, which starts with the expressions computed by the machine \ns instruction set and gradually adds to a pool of computable expressions until every intermediate-code \nexpression is computable. A crucial invariant is that we consider only computations that we know can \nbe implemented entirely by machine instructions. This invariant makes our algorithm signi.cantly simpler \nthan earlier search algorithms, which start with goal computations whose implementations by machine instructions \nare not known.  To increase the pool of computable expressions, we rewrite existing computable expressions \nusing algebraic laws. To match the left-hand side of an algebraic law, we have developed a new algorithm \ncalled establishment, which uses a novel combination of uni.cation and machine code to make two expressions \nequal (Section 7, especially Figure 4).  Finally, we present a new pruning mechanism which does not \nrestrict the size of a computable expression or the length of the sequence of machine instructions used \nto compute an expres\u00adsion. Instead, we restrict the amount of reasoning that is ex\u00adpected to be applied \nto reach a goal (Section 9).  Our heuristic search easily .nds instruction sequences for popular hardware. \nWe have generated working back ends for x86, PowerPC, and ARM targets, which are representative of desktops, \ngame con\u00adsoles, and handhelds. By selecting instructions before optimizing, our back ends generate code \nthat is as good as code generated by hand-written back ends (Section 10.1). As a result of our work, \na good, reliable back end for an optimizing compiler can be built more quickly and easily than ever before. \n 2. Instruction selection and register transfers We use a design developed by Davidson and Fraser (1980; \n1984): a code expander translates intermediate code to low-level register\u00adtransfer lists (RTLs). The \ncode expander establishes the machine invariant: each RTL is implementable by a single instruction on \nthe target machine. An optimizer improves the RTLs while preserving the machine invariant. Finally, the \ncompiler emits assembly code by translating each RTL to a corresponding machine instruction.  Literal \nconstant k RTL operator . Storage space s Location set ls Expression e, p ::= x#w | k | l |.(e1,...,en) \nLocation l ::= x.ls | $s[e] Assignment A ::= l := e Guarded assignment g ::= p . A RTL R ::= g {| g} \nControl-.ow graph G ::= . | R; G Figure 1. Grammar for register transfers. To make this approach practical, \nthe optimizer s code-improving transformations must be independent of the machine invariant. Davidson \nand Fraser s (1980) idea is to have the optimizer call a machine-dependent recognizer, which enforces \nthe machine in\u00advariant. A code improvement is accepted only if the recognizer says that each RTL in the \nimproved code is implementable by a single instruction on the target machine. We have generated recognizers \nfrom declarative machine descriptions (Dias and Ramsey 2006). Even though the optimizer works at a very \nlow level, Davidson and Fraser s approach can easily implement all the scalar and loop optimizations \nexpected of an optimizing compiler (Benitez and Davidson 1994). The approach is ideal for our work because \nan automatically generated code expander need only generate correct code; good code is not required. \nIn fact, Davidson (2008) reports that this approach works best when starting with na\u00a8ive code. Before \nexplaining how to generate a code expander (Section 6), we cover background: how RTLs represent instructions \n(Section 3), what is extracted from a declarative description of the semantics of instructions (Section \n4), and how our code expander is divided into machine-independent and machine-dependent parts (Section \n5).  3. Representing instructions as RTLs Register transfers are composed primarily from expressions, \nloca\u00adtions, assignments,and RTL operators (Figure 1). When analyzing an instruction set at compile-compile \ntime, we also allow operand metavariables x#w and x.ls to stand for expressions and locations, respectively. \nInternally, each expression, location, and operator has a width, but these widths are typically inferred \nby a type checker, and for simplicity, in this paper we usually leave widths implicit. At compile time, \nan expression e is a literal constant k,a fetch from a location l, or the application of an RTL operator \n. to sub\u00adexpressions. An RTL operator represents a pure computation on bit vectors; our library of about \n90 operators includes standard integer, logical, and IEEE .oating-point operations. If these don t suf.ce \nto describe the semantics of an instruction, a machine description can introduce a new operator. Operator \napplications are written using pre.x notation, but for binary operators we use in.x notation too. At \ncompile time, a location l is one or more contiguous cells in a storage space s. Storage spaces include \nmemory, general-purpose registers, and special-purpose registers; collectively they form the machine \nstate. Each storage space gets a one-character name; we conventionally use r for general-purpose registers, \nf for .oating\u00adpoint registers, and m for memory. Locations are referred to using an array-index notation, \nbut for convenience, we also name locations; for example, on the x86, the name ESP refers to location \n$r[4]: general-purpose register 4, the stack pointer. An assignment computes the value of an expression \nand stores the result in a location. A guarded assignment p . l := e evaluates a predicate p called the \nguard,and if p is true, e is evaluated and the result is stored in l. For ordinary instructions, the \nguard is trivially true and is omitted; guarded assignments with nontrivial guards typically represent \nconditional-branches or predicated instructions. A register-transfer list or RTL R is a parallel composition \nof guarded assignments; it is like Dijkstra s (1976) multiple assign\u00adment except that each assignment \nhas its own guard. A single RTL can represent the input/output behavior of any machine instruction. Our \nalgorithm, like our compiler, works with control-.ow graphs G. For simplicity, in this paper we assume \nthat a control-.ow graph is a sequence of RTLs; an empty sequence is written ..  4. Declarative machine \ndescriptions and .-RTL A declarative machine description speci.es properties of a machine using formalism \nthat is independent of any particular tool or pro\u00adgramming language. Properties of interest for code \ngeneration in\u00adclude the semantics and the binary or assembly-language represen\u00adtations of machine instructions. \nBy design, a declarative machine description can be written by a machine expert who knows nothing about \nthe internals of any compiler. A declarative machine description can be used to generate parts of many \ntools, not just a compiler back end. For example, the declar\u00adative semantic-description language .-RTL \n(Ramsey and David\u00adson 1998) and its sister language SLED (Ramsey and Fern\u00b4andez 1997) have been used \nto help generate assemblers and disassem\u00adblers (Ramsey and Fern\u00b4andez 1995), linkers (Fern\u00b4andez 1995), \ndy\u00adnamic code generators (Auslander et al. 1996), debuggers, and bi\u00adnary translators (Cifuentes, Van \nEmmerik, and Ramsey 1999). A declarative machine description is not a program in a domain\u00adspeci.c language: \nextracting a program requires analysis. The con\u00adtribution of this paper is a set of analyses that together \nsolve the most dif.cult and important problem in this .eld: extracting a trans\u00adlation of intermediate \ncode into machine instructions. .-RTL and SLED model an instruction set using an algebraic datatype that \nrepresents the abstract syntax of instructions. Each instruction corresponds to a constructor, which \ncan be applied to operands. For example, the PowerPC s three-register add instruc\u00adtion has the abstract \nsyntax add(rd, rs1, rs2);the add construc\u00adtor is applied to three operands. An operand may be a bit vector \nor the result of applying another constructor. .-RTL s algebraic datatypes are equivalent to a grammar \nwith named productions. This equivalence makes it easy to explain how .-RTL speci.es semantics: a .-RTL \ndescription de.nes an at\u00adtribute grammar in which the meaning of an instruction is a synthe\u00adsized attribute \nin the form of an RTL with metavariables. Each pro\u00adduction in the grammar is associated with an equation \nthat tells how to synthesize a result attribute from the attributes of its operands. For example, the \nPowerPC add instruction is speci.ed as follows: default attribute add(rd, rs1, rs2) is $r[rd] := $r[rs1] \n+ $r[rs2] The rd, rs1,and rs2 on the left-hand side are binding instances. These metavariables have been \ndeclared as 5-bit operands; in the language of attribute grammars, they are terminal symbols. When an \nRTL is created at compile time, metavariables are instantiated with values, so for example the abstract-syntax \ntree add(1, 2, 3) has the semantics $r[1] := $r[2] + $r[3]. RTL metavariables have the forms x#w and \nx.ls . A metavariable of the form x#w ranges over bit vectors of width w. A metavariable of the form \nx.ls ranges over locations in set ls. In our add exam\u00adple, a metavariable x#5 could stand for the register \nnumber rs1; a metavariable x.$r[0..31] could stand for the location $r[rs1], which might be any register \nin space r.  From a .-RTL description, we extract the following information: A grammar from which all \ninstructions can be generated  A list of storage spaces and locations  Equations that show how to compute \nthe observable effect of any instruction, represented as an RTL with metavariables   5. Code expansion \nby tiling A code expander translates a statement in an input language into a control-.ow graph G in which \neach graph node holds an RTL that is implementable by a single instruction on the target machine. Our \ninput language is C--, whose ASCII syntax is a thin veneer over RTLs (Ramsey and Peyton Jones 2000). \nC--is easily targeted by a front end: a front end may emit any assignment so long as no value or intermediate \nresult is wider than a machine word. The front end, therefore, needs to know almost nothing about the \ntarget machine: only the word size and possibly the byte order. Because a front end can generate almost \nany RTL, a code expander for C--has to work harder than a compiler for a language like C or Java: it \nmust translate any well-formed RTL to equivalent RTLs that satisfy the machine invariant. To make it \neasier to write code expanders, we factor each expander into two parts: a set of ex\u00adpansion tiles (or \ntileset) provides machine-dependent implementa\u00adtions for a .xed, machine-independent set of RTLs; and \na machine\u00adindependent tiler translates any well-formed RTL into an equiva\u00adlent control-.ow graph whose \nnodes hold tiles from the tileset. Our tiler is implemented by tree covering (Pelegr\u00b4i-Llopart and Graham \n1988; Aho, Ganapathi, and Tjiang 1989; Emmelmann, Schr\u00a8oer, and Landwehr 1989; Fraser, Hanson, and Proebsting \n1992). What distinguishes our tiler from previous work is that no matter what the target machine, every \nback end provides the same set of tiles and uses the same tiler (Dias 2008, Chapter 5). The tiler uses \nmaximal munch to translate a well-typed RTL to a sequence of tiles. For example, given the memory-to-memory \nmove $m[ESP +8] := $m[ESP +12] the tiler may generate a sequence of six tiles, each of which either moves \na value or applies a single RTL operator: t1 := 12 // load-immediate tile t2 := ESP + t1 // binop tile \nt3 := $m[t2] // load tile t4 := 8 // load-immediate tile t5 := ESP + t4 // binop tile $m[t5]:= t3 // \nstore tile The example shows why this transformation is called code expan\u00adsion: a single RTL is expanded \ninto many tiles. The expansion fac\u00adtor is large because the tiles are small: each tile implements at \nmost one RTL operator. Each tile in turn will be implemented by a se\u00adquence of machine instructions, \nbut because a tile can usually be implemented using one or two instructions, the remaining expan\u00adsion \nfactor is smaller. Even so, the expanded code can be horribly inef.cient but it is ideal for peephole \noptimization and other opti\u00admizations that take place after instruction selection (Davidson and Fraser \n1984; Benitez and Davidson 1994). Our machine-independent tileset is designed to cover all well-typed \nRTLs (Dias 2008, Chapter 5). We have de.ned dozens of tiles: one for each RTL operator, plus tiles for \ndata movement and control transfer. But what matters to our tiler is not the number of tiles but the \nnumber of forms, which we call shapes. The shape of a tile is obtained by abstracting over RTL operators \nand over the widths of arguments and results, and the size of the tiler is proportional to the number \nof shapes, not the number of tiles. Because the tiler is machine-independent, we have reduced the problem \nof generating an instruction selector to the problem of .nding an implementation of each tile using only \ntarget-machine instructions. Before moving on to this problem, we discuss the structure of the tileset: \n An operator tile applies a single RTL operator to constant or register operands and places a result \nin one or more registers. Operator tiles come in 7 shapes.  A data-movement tile moves a constant into \na location or moves a value between two locations. There are shapes for register\u00adregister copies, loads \nfrom memory, and stores to memory. Data-movement tiles also move data between locations of dif\u00adferent \nwidths, so there is a shape for sign-extending and zero\u00adextending loads and a shape for instructions \nthat store the low bits of a register in a memory location. In total, data-movement tiles come in 13 \nshapes.  A control-transfer tile changes the .ow of control. There are shapes for conditional and unconditional \nbranches, indirect branches, direct and indirect calls, and returns. Control-transfer tiles come in 9 \nshapes.  The shapes above are for register machines. Stack machines, such as the x86 legacy .oating-point \nunit, add another dozen shapes.  6. Our algorithm for .nding tiles Finding implementations of tiles \nturns out to be undecidable (Sec\u00adtion 8). We have therefore developed a new heuristic search algo\u00adrithm, \nwhich works well for machines of practical interest. Our algorithm resembles answer-set programming. \nWe maintain a pool of RTLs, each of which is known to be implementable by a se\u00adquence of instructions \non the target machine. The pool is initialized with RTLs from the declarative description of the target \nmachine. These RTLs may or may not include some tiles. To .nd more tiles, we combine RTLs in the pool, \ncreating new RTLs which are also implementable by machine instructions. We continue adding RTLs to the \npool until we have found an implementation of each tile or until our pruning heuristic tells us to stop. \nIn the rest of this section, we re.ne the description of our algorithm, with examples. Formal development \nis deferred to Section 7. 6.1 Intuitions about the algorithm We grow a pool of RTLs that we know how \nto implement on the target machine. We therefore maintain the following invariant: each RTL in the pool \nhas an implementation that uses only instructions which exist on the target machine. We begin by populating \nthe pool with RTLs from the .-RTL description: Although we talk about RTLs, an element of the pool is \nactually a control-.ow graph G, together with a postcondition that says what expression e is computed \nby G.Both G and e are parame\u00adterized by operand metavariables x#w and x.ls . Initially, every graph G \nis a machine instruction, and the associated postcondi\u00adtion gives the contents of one location assigned \nto by G. We enlarge the pool using the following ideas: Sequence RTLs to implement new RTLs: For example, \nif the pool contains RTLs l1 := e1 and l2 := e2 such that e2 uses location l1, we can combine the RTLs \nin sequence so the result stored into l2 will be e2[l1 ..e1],where e1 is substituted for l1.  Compensate \nfor unwanted assignments: A tile assigns to just one location. But a sequence of RTLs may assign to many \nlocations. To use such a sequence, we compensate for unwanted assignments by making sure they cannot \nbe observed.   Apply algebraic laws: An algebraic law asserts that two ex\u00adpressions are equivalent. \nIf a sequence of RTLs computes a complicated expression, we might be able to use an alge\u00adbraic law to \nshow that the same sequence computes a simpler expression like the right-hand side of an expansion tile. \nIf by using these ideas, we get a sequence of instructions that com\u00adputes an expression e which is not \nalready computed by some other RTL in the pool, then provided e satis.es the pruning heuristic de\u00adscribed \nin Section 9, we add the sequence to the pool. Our pruning heuristic predicts whether an assignment is \nlikely to help imple\u00adment a tile. Pruning not only ensures termination but also reduces the time spent \n.nding new combinations of RTLs. We illustrate our algorithm with an example: .nding an implemen\u00adtation \nof the bitwise-complement tile r1 := \u00acr2 on the PowerPC. The initial pool of RTLs does not include bitwise \ncomplement, but it does include a load-zero instruction1 and a bitwise nor: Gldzero . r3 := 0 Gbitnor \n. r1 := \u00ac(r2 . r3) Here r1, r2,and r3 are RTL metavariables standing for general\u00adpurpose registers. We \ncan combine these instructions in a sequence Gcom+ . r3 := 0; r1 := \u00ac(r2 . r3) which computes both the \nbitwise nor and the assignment to r3: Gcom+ = r1 := \u00ac(r2 . 0) | r3 := 0 Now we can apply the algebraic \nlaw x . 0= x, with r2 substituted for x, to simplify the bitwise-nor expression: Gcom+ = r1 := \u00acr2 | \nr3 := 0 The RTL r1 := \u00acr2 | r3 := 0 computes bitwise complement, but it also assigns zero to r3. To compensate \nfor the assignment to r3, we have to make it unobservable by the rest of the program. One way is to replace \nr3 with a location that cannot be observed: a fresh temporary or a scratch register. A fresh temporary \nis guaranteed to be distinct from every other lo\u00adcation in the program; for most sets of machine registers, \nthe com\u00adpiler provides an in.nite supply of fresh temporaries. Temporaries are eventually mapped to hardware \nregisters by a register alloca\u00adtor. If we substitute a fresh temporary t for the metavariable r3,the \nassignment to t cannot be observed: Gcom . Gcom+[r3 ..t] = r1 := \u00acr2 | t := 0 Not every hardware register \nhas associated temporaries. For ex\u00adample, a unique hardware resource like a condition-code or status \nregister would not have temporaries. In this case, the best option is for the back end to designate the \nresource as a scratch regis\u00adter. A scratch register is made unnameable in source code and un\u00adavailable \nto the register allocator. Scratch registers may be mutated freely in the implementation of any tile. \nFinally, if we have an unwanted assignment to a location that has no temporaries and is not a scratch \nregister, our only choice is to insert code to save and restore the value in the location. This option \nis workable only in straight-line code; to save before and restore after a branch instruction would require \ninserting code at the branch target, which would violate the abstraction of the tileset. In our example \ngraph Gcom , the only observable assignment is r1 := \u00acr2, which implements the bitwise-complement tile, \nand we add Gcom to the pool. 1 Actually the PowerPC instruction set does not include a load-zero instruc\u00adtion, \nbut the load-zero simpli.es this example. The graph Gldzero is derived by our algorithm; it is equivalent \nto the graph labelled D in Figure 6.  6.2 Algebraic Laws An algebraic law de.nes an axiom used to show \nthat two RTL expressions are equivalent. The language of algebraic laws uses the same operators as the \nlanguage of RTL expressions, but it refers only to constants and to special algebraic-law metavariables \nxL, never to locations: Law metavariable xL LAW LAW LAW Law expression e ::= xL | k |.(e1 ,...,en ) LAW \nLAW Law Law ::= e1 = e2 When we apply an algebraic law, its metavariables may be replaced by any RTL \nexpressions e, not only expressions of the form e LAW . To improve readability, we usually omit the subscript \nL of an algebraic-law metavariable. Algebraic laws are universally true, independent of any machine. \nThe most familiar kinds of laws are identities and inverses, such as x +0= x and \u00ac\u00acx = x. Another familiar \nkind of law shows how to implement one RTL operator in terms of other RTL operators; for example, \u00acx \n= -x -1. A more interesting kind of law shows how a value can be equal to an expression involving that \nsame value; for example, the following law is used to .nd instruction sequences that load large immediate \nvalues on RISC architectures: ((x .l n) . n) . zx(lobitsn(x)) = x where zx is zero-extension and lobitsn \nis an operator that extracts the low n bits of its argument. There are many such laws, which embody both \nmachine knowledge and some of the kind of domain\u00adspeci.c knowledge presented by Warren (2003). 6.3 Data \nmovement To save and restore mutated locations and to implement data\u00admovement tiles, we must .nd sequences \nof instructions that move data between locations. We put such sequences in a directed data\u00admovement graph. \nEach node in the graph represents a set of lo\u00adcations on the machine, determined using Feigenbaum s (2001) \nlocation-set analysis, as adapted by Dias and Ramsey (2006). This analysis identi.es locations that are \ninterchangeable for use in some instructions. For example, most machines have general-purpose registers \nthat are interchangeable in most instructions. (Particular registers may have special, noninterchangeable \nstatus in a few in\u00adstructions such as multiply or divide.) For each RTL in the pool that moves data from \nlocation l. to location l, we add the edge l. . l to the data-movement graph. We then compute the transitive \nclo\u00adsure of the graph, so if a sequence of RTLs G moves data from l1 to l2, we add another edge to the \ndata-movement graph, and we associate G with that edge. For example, to show that we can move data between \ninteger registers on the x86, we add an edge from the location set $r[0..7] to itself; we represent the \nedge using two metavariables (x.$r[0..7] . x.. $r[0..7] ); and we associate the edge with the RTL for \nthe move instruction, with the metavariables x.$r[0..7] and x.$r[0..7] as operands. An ideal data-movement \ngraph would be complete: able to move a value from any location on the machine to any other location \nof the same size. Real data-movement graphs usually lack some edges that involve special registers, such \nas condition-code registers.  7. Formal development The essence of our algorithm is to .nd a control-.ow \ngraph G, composed of machine instructions, such that when G is executed, it implements an assignment \nof the form l := e, which is the form of almost every tile. When we discover a graph that implements \na new assignment, what we add to the pool is not an RTL but rather  CREATE CANDIDATE LAW LAW e1 = e2 \n. Laws LAW LAW est. \u00b7.{e = e1 } G0 {l0 = in E[e2 ] . modi.es L0} . ..{true} G{l= in e . modi.es L}.Gs,Gr, \nLs,.sr .. observable(L) LAW LAW e1 =e2 {true} G0 {l0 = in E[e] . modi.es L0} --------.{true} Gs; .sr \n(.(G)); Gr {.sr (l)= in .sr (e ) . modi.es Ls} Figure 2. Generating a candidate the control-.ow graph \nG that implements the RTL, plus a postcon\u00addition that gives the source and destination of the assignment. \nA graph and a postcondition suf.ce to express the solutions to our problem the implementations of the \ntiles we are trying to dis\u00adcover. But this form is not general enough to enable us to rea\u00adson about composition \nof machine instructions, which requires us to reason about intermediate states. We therefore use Hoare \ntriples: {P } G {Q} The Hoare triple says that executing graph G in an input state satis\u00adfying precondition \nP results in an output state satisfying postcon\u00addition Q. The postcondition may refer to both states: \nto refer to the contents of a location l in the output state, we write just l; to refer to the contents \nof that same location in the input state, we write in l. We lift in to expressions using the obvious \nhomomorphism. Because we are interested in implementing tiles, and because a tile must establish its \npostcondition regardless of machine state, the precondition of a Hoare triple in the pool is always true. \nBecause each tile implements a single assignment, we also restrict postconditions to the following canonical \nform: l = in e . modi.es L The postcondition describes the contents of an output-state loca\u00adtion l in \nterms of the value of an input-state expression. The post\u00adcondition also says what extra locations L \nare modi.ed by the graph; this set, which does not contain l, accounts for unwanted as\u00adsignments. If \na location is not in the set L.{l}, its value remains unchanged. As an example, the PowerPC implementation \nof the bitwise-complement tile in Section 6.1 is described by this triple: {true} Gcom {r1 = in \u00acr2 . \nmodi.es {t}} 7.1 How new candidate Hoare triples are created Our search algorithm looks for Hoare triples \nthat implement new assignments. Its fundamental operation is to use an algebraic law LAW LAW e1 = e2 \nto rewrite the postcondition of a Hoare triple that is already in the pool, producing a new, candidate \nHoare triple with a new postcondition. We write the operation using this judgment: LAW LAW e1 =e2 {true} \nG {Q} --------.{true} G{Q}. A derivation of this judgment, which always ends in rule CREATE CANDIDATE \nfrom Figure 2, is constructed as follows: 1. Choose a Hoare triple {true} G0 {l0 = in e0 . modi.es L0}. \n 2. Choose a subexpression e of e0 such that e0 = E[e].  LAW LAW 3. Choose an algebraic law e1 = e2 \n. 4. Use the algebraic law to rewrite e. Speci.cally, in the precon\u00addition, we assume that e = e LAW \n1 and use that assumption to rewrite the postcondition:  LAW LAW {e = e1 } G0 {l0 = in E[e2 ] . modi.es \nL0}. Because this new Hoare triple has a nontrivial precondition, we can t use it right away. 5. To use \nthe new triple, we try to discharge the precondition e = e LAW 1 by running an algorithm we call establishment.Es-LAW \ntablishment tries to match e and e1 in a manner akin to uni\u00ad.cation, so one of the results is a substitution \n.. A substitu\u00adtion maps operand metavariables x.ls to locations and x#w to compile-time constant expressions: \n. ::= \u00b7| .[x.ls ..l] | .[x#w ..e] We write \u00b7 for the identity substitution; square brackets stand for \nfunction composition. For example, .[x.ls .. l] .rst applies ., then substitutes l for x.ls in the result. \nUni.cation alone cannot establish the truth of equations that refer to the contents of machine locations. \nGiven the triple LAW {e = e1 } G0 {Q}, our algorithm therefore produces not only a substitution .. but \nalso a new graph G. = Gest ; G0, LAW where Gest alters machine state in order to make e = e1 . We write \nthe result ..{true} G{Q}, and we call this form an extended Hoare triple. An extended Hoare triple is \nvalid whenever applying .. to precondition, graph, and postcondition produces a valid Hoare triple. We \nkeep the substitution separate, to the left of a turnstile, in order to detect aliasing when a machine \ninstruction is used. (See the .nal premise of rule USE RESULT in Figure 4.) Establishment takes an extended \nHoare triple as a goal, and when successful, it produces a new extended Hoare triple whose precondition \nis trivially satis.ed. We write the relation between goal and result using the arrow est .: est. . .{P \n} G {Q} . ..{true} G{Q}. In any machine state, executing graph .(G) puts the machine into a state satisfying \nthe postcondition .(Q). The inference rules for est . are given in Figure 4 and explained in Section \n7.3. 6. In a candidate triple, it is not enough that substitution .. and graph G. establish postcondition \nl. = in e . . modi.es L,be\u00adcause L. may include observable locations that should not be modi.ed, such \nas machine registers. If this is so, we exam\u00adine L. and .nd a substitution .sr and graphs Gs and Gr such \nthat only locations in set Ls, which are unobservable, are mod\u00adi.ed. We write .Gs,Gr, Ls,.sr .. observable(L). \nSubstitu\u00adtion .sr renames observable operand metavariables in L. to un\u00adobservable fresh temporaries. \nIf observable locations remain, which rarely happens, graph Gs saves their contents to tem\u00adporaries or \nscratch locations in Ls,and Gr restores the orig\u00adinal values from Ls. The graph Gs; .sr (.(G)); Gr therefore \nmakes modi.cations to locations in L. unobservable. 7. The candidate Hoare triple is created by using \nthe graph Gs; .sr (.(G)); Gr computed in Step 6 and the postcondi\u00adtion derived by establishment in Step \n5. Only .sr is applied to the postcondition; by construction, no metavariables from .. appear in the \npostcondition derived by establishment.   INSTANTIATE MACHINE CODE WITH MOVE l . locs(e) {true} G {l \n= in e . modi.es L} . Pool INSTANTIATE MACHINE CODE l/. locs(e) {true} G {l = in e . modi.es L} . Pool \n.inst {true} G. {l = in l. . modi.es L}. = .i . .fresh .fresh freshens metavariables l/. locs(l) . = \n.i . .fresh .fresh freshens metavariables .inst {true} .(G) {.(l)= in .(e) . modi.es .(L)}.inst {true} \n.(G. ; G) {.(l)= in .(e[l ..l]) . modi.es .(L.L)} Figure 3. Instantiation of code from pool EQUAL EXPRESSIONS \nALL SATISFIED est. . .{P } G {Q} . ..{true} G{Q} estest. . .{true} G {Q} . . .{true} G {Q} . .{e = e \n. P } G {Q} . ..{true} G{Q} SPECIALIZE LAW SYMMETRY e has no law metavariables xL ./fv(G) . est. est. \n. .{e = e . P } G {Q} . ..{true} G{Q} . .{P [xL ..e]} G {Q[xL ..e]} . ..{true} G{Q} . est. est. . .{e \n= e . P } G {Q} . ..{true} G{Q} . .{xL = e . P } G {Q} . ..{true} G{Q} MATCH COMPILE-TIME CONSTANT EXPRESSION \nMATCH LOCATION e is computable at compile time and .ts in w bits x#w ./fv(e) l . ls x.ls ./dom(.) .ext \n= .[x.ls ..l] shrinks(x#w = e . P ) x#w ./dom(.) .ext = .[x#w ..e] est. est. .ext .{P [x.ls ..l]} G {Q[x.ls \n..l]} . ..{true} G{Q} .ext .{P [x#w ..e]} G {Q[x#w ..e]} . ..{true} G{Q}est. est. . .{x.ls = l . P } \nG {Q} . ..{true} G{Q} . .{x#w = e . P } G {Q} . ..{true} G{Q} REDUCE APPLICATIONS . est . .{e1 = e1 \n.\u00b7\u00b7\u00b7. en = e . P } G {Q} . .. .{true} G. {Q} n . est. . .{.(e1,...,en)= .(e1,...,e ) . P } G {Q} . ..{true} \nG{Q} n PREPARE OPERATOR .inst {true} Gp {l= in .(e1,...,e ) . modi.es L} ^ n . . est . .{l = l. l. = \n.(e1,...,e ) . e = ei . P } G {Q} . .. .{true} G. {Q} ni 1=i=n est. . .{l = .(e1,...,en) . P } G {Q} \n. ..{true} G{Q} PREPARE CONSTANT e and e . are computable at compile time . . . est. .inst {true} Gp \n{l= in e . modi.es L} . .{l = l. l= e . e = e . P } G {Q} . ..{true} G{Q} est. . .{l = e . P } G {Q} \n. ..{true} G{Q} USE RESULT .inst {true} Gp {l = in e . modi.es L} l/. locs(e) shrinks(l = e . P ) L1 \n= L0 .L.{l}\\{l0} est. . .{P [l ..e]} Gp; G {l0 = in e0[l ..e] . modi.es L1} . ..{true} G{Q} .(L.{l}) \nn .(locs(e0) \\{l})= \u00d8 est. . .{l = e . P } G {l0 = in e0 . modi.es L0} . ..{true} G{Q} est Figure 4. \nRules for establishment: . .{P } G {Q} . .. .{true} G. {Q} 7.2 Example of .nding a new Hoare triple \nWe now give a full formal treatment of the bitwise-complement example from Section 6.1. In the process \nwe explain many of the rules for deriving Hoare triples, as well as our algorithmic search for derivations. \nWe begin with this pool:2 {true} Gldzero {r1 . = in 0 . modi.es \u00d8} {true} Gbitnor {r1 = in \u00ac(r2 . r3) \n. modi.es \u00d8} We choose the triple with graph Gbitnor and the algebraic law x . 0= x.From rule CREATE \nCANDIDATE in Figure 2, we choose 2 We write the load-zero triple using metavariable rin order to emphasize \n1 that when we use a triple in the pool, we freshen all its metavariables. subexpression r2.r3 as e,so \nE[] is \u00ac[]. We can apply the algebraic law if we can discharge the precondition r2 . r3 = x . 0,byusing \nthe rules in Figure 4 to derive \u00b7.{r2 . r3 = x . 0} Gbitnor {r1 = in \u00acx . modi.es \u00d8} est . .. .{true} \nG. {Q} . relates a valid extended Hoare triple . .{P } G {Q}to a new extended Hoare triple .. .{true} \nG. {Q}. Algorithmi\u00adcally, we start with the identity substitution and a precondition P , and we search \nfor a substitution .. and graph G. = Gest ; G such that when we substitute for metavariables as speci.ed \nby .,ex\u00adecuting .(G) establishes postcondition Q,where Q. is derived from Q by substitution for both \nmetavariables and machine loca-Logically, est  {true} r1 . := 0 {r . = in 0 . modi.es \u00d8} . Pool 1 r3 \n/. =[r1 . .. r3] .\u00d8 INST. MACH. CODE .inst {true} r3 := 0 {r3 = in 0 . modi.es \u00d8} r3 /shrinks(r3 =0 \n. true) .\u00d8 {r3}n{r2} = \u00d8 USE RESULT = in \u00acr2 . modi.es \u00d8} est= in \u00acr2 . modi.es {r3}} \u00b7.{r3 =0} Gbitnor \n{r1 . \u00b7.{true} r3 := 0; Gbitnor {r1 r2 has no law metavariables x/.{r1,r2,r3} SPECIALIZE LAW est \u00b7.{x \n= r2 . r3 =0} Gbitnor {r1 = in \u00acx . modi.es \u00d8} . \u00b7.{true} r3 := 0; Gbitnor {r1 = in \u00acr2 . modi.es {r3}} \nSYMMETRY est \u00b7.{r2 = x . r3 =0} Gbitnor {r1 = in \u00acx . modi.es \u00d8} . \u00b7.{true} r3 := 0; Gbitnor {r1 = in \n\u00acr2 . modi.es {r3}} REDUCE APPS est \u00b7.{r2 . r3 = x . 0} Gbitnor {r1 = in \u00acx . modi.es \u00d8} . \u00b7.{true} \nr3 := 0; Gbitnor {r1 = in \u00acr2 . modi.es {r3}} Figure 5. Derivation of a graph that implements r1 := \u00acr2 \ntions. An invariant of the algorithm is that precondition P is a con\u00adjunction of equalities. The algorithm \n.nds a sequence of graphs that establishes the equalities; the last graph in the sequence establishes \nthe .rst equality in the conjunction. From the logical perspective, equalities e = e . and e . = e are \nequivalent, but in order to guar\u00adantee termination, our search for a derivation distinguishes them (Section \n7.3.3). Our algorithm proceeds by structural induction on the precondi\u00adtion r2 . r3 = x . 0, eventually \nproducing the derivation shown in Figure 5. The .rst (and only) equality in the precondition applies \noperator . on both sides, and the rule with this syntactic form in its conclusion is REDUCE APPLICATIONS.3 \nThe rule converts equality of application to conjunction of equalities, so our new precondition is r2 \n= x . r3 =0. At this point, we must apply SYMMETRY to make progress, and we get precondition x = r2 . \nr3 =0. Now only one rule applies: SPECIALIZE LAW. We substitute r2 for x in the rest of the precondition \nand in the postcondition, which gives us the new Hoare triple {r3 =0} Gbitnor {r1 = in \u00acr2 . modi.es \n\u00d8}for the inductive case. SPECIALIZE LAW does not extend ., because . tracks substitutions for operand \nmetavariables only. To establish precondition r3 =0,we use graph r3 := 0 from the pool, applying rule \nUSE RESULT. This rule is the only rule that adds code from the pool to the graph in progress, and it \nrequires a new judgment for instantiating a graph from the pool: .inst {true} G {Q} Our example uses \nrule INSTANTIATE MACHINE CODE (Figure 3): we make metavariables fresh using a substitution .fresh , and \nthen to match the postcondition Q, we may rebind the fresh metavariables using an additional substitution \n.i. In our example, the precondition is r3 =0 so (ignoring modi.es clauses) we want a .inst judgment \nwith postcondition r3 = in 0. The code in the pool has postcon\u00addition r1 . = in 0. Metavariable r1 . \nmust be replaced with a fresh metavariable, but we can then choose .i to replace that metavari\u00adable with \nr3, enabling ourselves to conclude .inst {true} r3 := 0 {r3 = in 0 . modi.es \u00d8} The remaining precondition \nis the empty conjunction true,sowe apply ALL SATISFIED. (To avoid clutter, we omit the application from \nFigure 5.) We now complete the derivation, .lling in the right-hand side of each judgment; each rule \nderives the same substitution and Hoare triple as its inductive case. We also check the .nal premise \nof USE RESULT, here verifying (by {r3}n{r2} = \u00d8) that the assignment to r3 does not affect the value \nof \u00acr2. The result of the derivation 3 SYMMETRY matches any syntactic form, but it is used only to enable \nprogress when progress is not otherwise possible. is the conclusion \u00b7.{r2 . r3 = x . 0} Gbitnor {r1 = \nin \u00acx . modi.es \u00d8}est= in \u00acr2 . modi.es {r3}} . \u00b7.{true} r3 := 0; Gbitnor {r1 When r3 is replaced by \na temporary t,graph t := 0; Gbitnor [r3 ..t] will be a candidate for addition to the pool.  7.3 Our \nrules for deriving Hoare triples 7.3.1 Instantiation Our example applies rule INSTANTIATE MACHINE CODE \nto use an instruction from the pool. But an instruction with postcondition l. = in e . can be used to \nestablish l. = e . only if l. ./locs(e ), where locs(e ) are the locations mentioned in e . But on a \nreal ma\u00adchine, l. may appear in locs(e ); that is, an instruction may insist on writing one of the locations \nit reads. (See the x86 or any other two\u00adaddress instruction set.) To exploit such instructions, we provide \na second way of deriving the .inst judgment, INSTANTIATE MACHINE CODE WITH MOVE.Givenatriplefromthepoolthatcomputes \nl := e, where l . locs(e), this rule constructs a sequence of instructions to replace every use of l \nin e with a different location l. .  7.3.2 Soundness Given a set of algebraic laws and a pool of Hoare \ntriples, our algorithm .nds new Hoare triples which are candidates for addition to the pool. Provided \nthe algebraic laws are sound and the pool contains only valid Hoare triples, the rules for establishment \nare est sound. That is, if . .{P } G {Q} . .. .{true} G. {Q},and if . .{P } G {Q} is valid, then .. .{true} \nG. {Q} is also valid. To simplify the proof of soundness, we have carefully engineered the rules so that \nonly USE RESULT actually uses code from the pool. We prove soundness by induction on the height of derivations. \n The base case is ALL SATISFIED; the rule is sound because the right-hand side of the conclusion is valid \nby hypothesis.  The most interesting rule is USE RESULT. By design, it is the only rule that uses machine \ncode (Gp) to satisfy a precon\u00addition. Its soundness is based on Hoare s axiom for assign\u00adment, but it \nmay be easier to see that the new precondition P [l .. e] is the weakest precondition of l := e with \nrespect to P . In the inductive case, the substitution e0[l .. e] is correct provided that the only location \nin e0 which Gp mu\u00adtates is l. This provision is enforced by the side condition .(L.{l}) n .(locs(e0) \n\\{l})= \u00d8, which in our example was {r3}n{r2} = \u00d8. We have to apply the substitution .. because establishment \nmight cause two otherwise distinct loca\u00adtions to alias.  Rules EQUAL EXPRESSIONS, SYMMETRY, REDUCE APPLICATIONS, \nPREPARE OPERATOR,and PREPARE CONSTANT are variations on a theme: replace a goal P1 with a smaller or \nbetter goal P2.   Initial pool: A: r1 := sx(lobits16 (k32)) load signed k16 B: r1 := r2 . zx(lobits16 \n(k32)) bitwise-or immediate C: r1 := r2 . (k32 .l 16) . 16 load upper immediate Round 1 .nds load-immediate \n0 and or-with-0: D: r1 := sx(lobits16 (0)) {r1 := 0} E: r1 := r2 . zx(lobits16 (0)) {r1 := r2 . 0} Round \n2 .nds register-register move and zero-extend-low-bits: F: r1 := r2 . zx(lobits16 (0)) {r1 := r2} G: \nt1 := sx(lobits16 (0)); r1 := t1 . zx(lobits16 (k32)) {r1 := zx(lobits16 (k32))} Round 3 .nds load-immediate \nof a 32-bit k: H: t1 := sx(lobits16 (0)); t2 := t1 . zx(lobits16 (k32)) r1 := t2 . (k32 .l 16) . 16; \n{r1 := k32} Figure 6. Finding instructions by rounds on the PowerPC. Modi\u00ad.es clauses are omitted to \nsave space. These rules are sound because if . .{P1} G {Q} is valid and if P2 implies P1,then . .{P2} \nG {Q} is valid. By induction, the extended triple .. .{true} G. {Q} derived in the inductive case must \nalso be valid. The PREPARE rulesavoidusingcodefromthepool;instead,they set up a precondition that is \ndesigned to be discharged by USE RESULT. This trick simpli.es the proof of soundness, at the cost of \nmaking the proof of termination slightly more complicated. The metavariable rules SPECIALIZE LAW, MATCH \nLOCATION,and MATCH COMPILE-TIME CONSTANT EXPRESSION all introduce sub\u00adstitutions. They are sound because \nif we apply a substitution to a valid triple, the result is also a valid triple.  7.3.3 Termination \nof establishment It is easy to write a terminating algorithm that searches for deriva\u00adtions. During the \nsearch, we say that a precondition gets smaller if the number of law metavariables xL decreases, or if \nthe expressions on the right-hand sides get smaller, or if the number of conjuncts decreases. (The precise \ncondition involves counting the number of conjuncts with right-hand sides of height n, for each n.) In \nmost in\u00adference rules, the inductive case obviously uses a smaller precondi\u00adtion. But USE RESULT and \nMATCH COMPILE-TIME CONSTANT require side conditions to ensure the new precondition is smaller: in USE \nRESULT, condition shrinks(l = e . P ) ensures that the new goal P [l ..e] is no larger than P . A similar \ncondition applies to MATCH COMPILE-TIME CONSTANT.We de.ne shrinks(l = e . P ) to hold unless e is an \napplication and l appears free in the right-hand side of an equality in P . SYMMETRY does not reduce \nthe goal, but we use it only to move xL or x#w from the right-hand side of a subgoal to the left, after \nwhich search makes progress or fails. PREPARE OPERATOR and PREPARE CONSTANT temporarily increase the \ngoal, but in both rules, the second subgoal of the new goal will be discharged by USE RESULT. The idea \nis to see (via the in\u00adstantiation judgment .inst ) if code in the pool applies the operator or computes \nthe compile-time constant expression in the precon\u00addition. If so, the PREPARE rules take the postcondition \nl. = in e . established by the code in the pool, and they try a new goal whose precondition includes \nthe equality l. = e , knowing that this equal\u00adity will be discharged by USE RESULT. 1 Initialize Pool \nwith Hoare triples from machine description 2 repeat until Pool stops growing 8 if {true} G0 {l0 = in \nE[e] . modi.es L0}--------. 3 Build data-movement graph from current contents of Pool 4 Add new data-movement \nedges to Pool as Hoare triples 5 6 foreach {true} G0 {l0 = in e0 . modi.es L} . Pool foreach subexpression \nand context E[e]= e0 7 foreach law e1 = e2 LAW LAW e1 =e2LAW LAW 9 {true} G. {l. = in e . . modi.es \nL} 10 and l. = in e . passes our utility test (Section 9) 11 then Pool += {true} G. {l. = in e . . modi.es \nL} Figure 7. Pseudocode summarizing our algorithm.  7.4 Adding new Hoare triples in rounds Above, we \nshow how by choosing an existing Hoare triple, an algebraic law, and a subexpression, we can search for \na new Hoare triple to add to the pool. Here, we show what we do with the triples we .nd. After initializing \nthe pool with triples derived from the machine description, we add new triples in rounds. Hoare triples \nfound in one round can be used to .nd new Hoare triples in later rounds, as shown in Figure 6. Figure \n6 shows the control-.ow graphs of eight example triples from the PowerPC. The three triples in the initial \npool come di\u00adrectly from machine instructions: load 16-bit immediate, bitwise-or immediate, and load \nupper immediate, labelled A, B, and C respec\u00adtively. In the .rst round, applying the law sx(lobitsn (0)) \n= 0 to triple A produces an implementation of load zero (D); and ap\u00adplying the law zx(lobitsn (0)) = \n0 to triple B produces an im\u00adplementation computing the bitwise-or of a register and zero (E). In the \nsecond round, applying the law xL . 0= xL to triple E produces an implementation of register-register \nmove (F); and applying the law 0 . xL = xL to triple B results in a sequence that combines triples D \nand B to compute the zero-extension of the least signi.cant 16 bits of an immediate value (G). Nei\u00adther \nof these laws can be applied in the .rst round because we need to use triples that are produced by the \n.rst round. The interesting round is the third round, where we apply the law zx(lobitsn(xL)) . ((xL .l \nn) . n)= xL to triple C, which results in a sequence (H) that combines triples G and C to load a full \n32-bit literal constant: it implements the load-immediate tile. Our search for triples in rounds is implemented \nby the algorithm in Figure 7. The .rst step (line 1) is to initialize the pool using RTLs from the machine \ndescription. Because not all RTLs are useful for implementing tiles, we add only those that pass a utility \ntest, which we de.ne in Section 9. For each useful assignment li := ei of an RTL l1 := e1 | \u00b7\u00b7\u00b7 | ln \n:= en, we add the following Hoare triple to the pool: {true} l1 := e1 |\u00b7\u00b7\u00b7| ln := en {li = in ei . modi.es \n.{lj }} j =i We then begin the .rst round of search. A round is implemented by the repeat loop on line \n2 of Figure 7, which keeps searching for new candidates until no more Hoare triples are added to the \npool. Before the search begins, we use the data-movement triples in the pool to build a new data-movement \ngraph, on line 3. (Because each round may discover new data\u00admovement triples, the graph from the previous \nround may be ob\u00adsolete.) Any new edges are added to the pool (line 4). The search itself tries all possible \ncombinations of triples in the pool, algebraic laws, and subexpressions (lines 5 7). For ef.ciency, candidate \nlaws are chosen after subexpressions (line 7): when we  LAW know that the goal is to establish e = e1 \n,then if e is an applica- LAW tion of an operator, we need consider only laws in which e1 is an application \nof the same operator. Having made our choices, we attempt to construct a derivation (lines 8 9) that \nends in an application of the CREATE CANDIDATE rule from Figure 2 in Section 7.1. If we .nd a new triple, \nwe can add it to the pool, provided it passes our utility test (line 10). The new triple is equivalent \nto the assignment l. := e , with additional modi.cations to unobservable locations. If this assignment \nappears useful, we add the new Hoare triple to the pool for use in the next round (line 11); otherwise, \nwe discard it. By controlling which Hoare triples are added to the pool, our utility test guarantees \nthat the number of rounds is bounded. Because the argument about termination is not trivial, we discuss \nit in its own section (Section 9).  8. Finding implementations of tiles is undecidable Ideally there \nwould be a decision procedure that would enable us both to limit the number of Hoare triples added to \nthe pool and to guarantee that every potentially useful Hoare triple is added. Un\u00adfortunately, the underlying \nproblem is undecidable: no terminat\u00ading algorithm can guarantee to .nd implementations of all imple\u00admentable \ntiles. The problem of .nding an implementation of a tile is closely related to the more general, undecidable \nproblem of de\u00adtermining if two programs are equivalent. A sequence of instructions implements a tile \nif, using some set of algebraic laws, we can show that the RTL computed by the sequence of instructions \nis equivalent to the RTL that represents the tile. For convenience, we work not with algebraic laws, \nbut with rewrite rules; an algebraic law e = e . can be expressed as a pair of rewrite rules e . e . \nand e . . e. THEOREM 1. There is no algorithm that, given an arbitrary set of rewrite rules and RTLs \nR1 and R2, can decide if R1 . * R2. Due to space restrictions, we only sketch the proof; a full proof \nis given by Dias (2008, Appendix A). The proof proceeds by reduc\u00adtion from the halting problem; we adapted \nwell-known proofs of undecidability for strong normalization in term-rewriting systems (Bezem, Klop, \nand de Vrijer 2003). The reduction de.nes an embedding from a Turing-machine con\u00ad.guration to an RTL, \nand an embedding from the Turing-machine s transition function to rewrite rules. We de.ne a deterministic \nTur\u00ading machine as a triple .Q, S, d.,where Q is a .nite set of states, S is the .nite set of symbols \nthat may be written on the Turing machine s tape (including the blank symbol .), and d is the tran\u00adsition \nfunction Q \u00d7 S . Q \u00d7 S \u00d7{L, R},where L and R stand for left and right. The model of the Turing machine \nis that there is a tape of in.nite length in both directions, and at each position on the tape is a symbol. \nAt any given time, the Turing machine is in some state q at a position p on the tape. The machine takes \na step by reading the symbol s at position p and calling the transition function d(q, s), which returns \nthe new state of the machine, the new symbol to write at position p, and the direction to move along \nthe tape. The con.guration of the Turing machine is represented by the tuple .q, Tl,s,Tr.,where q is \nthe current state, Tl is the stack of symbols to the left, s is the current symbol, and Tr is the stack \nof symbols to the right. To embed a Turing-machine con.guration as an RTL, we encode each state q as \na ternary operator and each symbol s as a nullary operator. We use the literal 0 to represent an empty \nstack of blank symbols, and we use the in.x binary operator : as a cons operator to construct stacks. \nFor example, a Turing-machine con.guration in state q1 with symbols s1 and s2 to the left, symbol s3 \nat the current position, and symbol s4 to the right, can be encoded using the RTL l0 := q1(s1 : s2 :0,s3,s4 \n:0) where l0 is de.ned as a location on the machine. The embedding of the Turing-machine transition function \nto rewrite rules is complicated by the fact that the embedding of Turing\u00admachine con.guration may produce \neither an empty or a nonempty stack. Consequently, each transition is embedded as a pair of rewrite rules. \nFor example, the embedding of a transition to the left d(q, s)=(q . ,s ,L) produces two rewrite rules: \nq(x1 : x2,s,x3) . q (x2,x1,s . : x3) q(0,s, x) . q (0, .,s . : x) We also de.ne a distinguished halting \nRTL: l0 := halted(),where halted is a unique, nullary RTL operator. The reduction is com\u00adpleted by adding \na rewrite rule to ensure that the embedding of any halting state in the Turing machine can be rewritten \nto the distinguished halting RTL. The Turing machine halts if the tran\u00adsition function is not de.ned \non a pair (q, s). For each such pair (q, s) ./dom(d), we add the rewrite rule q(x, s, x ) . halted() \nGiven this embedding, our proof shows that there is a bisimulation between the transitions of the Turing \nmachine and the rewrite rules produced by the embedding of the transition function. Using the bisimulation \nresult, we show that an algorithm that decides RTL equivalence could be used to solve the halting problem \nby embedding the starting con.guration of the Turing machine in an RTL R and using the algorithm to check \nif R is equivalent to the halting RTL. Because the halting problem is undecidable, we conclude that there \ncan be no algorithm that decides RTL equivalence. This undecidability result holds for the general problem, \nwhere we can choose an arbitrary input instruction set, an arbitrary set of tiles, and an arbitrary set \nof algebraic laws. An important question remains open: is there a limited set of algebraic laws, expressive \nenough to .nd implementations of all the tiles for a well-de.ned set of machines, but for which the problem \nof .nding tiles is decidable?  9. Termination and our utility test Because the general problem is undecidable, \nwe use a heuristic to restrict what Hoare triples can be added to the pool. We have found a heuristic \nwith excellent properties: On real machines, we are likely to .nd implementations of all the tiles. \n The pool stays small enough that the tileset generator runs in minutes, not hours.  The discovery \nof this heuristic is one of the signi.cant contributions of this paper. Before describing it, we explain \nwhy more typical heuristics are not suitable. One obvious heuristic would be to limit the number of machine \ninstructions used in a graph G. Another would be to limit the maximum height of any expression appearing \nin a postcondition. Because these heuristics make decisions based on size alone, not using any information \nabout whether the RTL might help imple\u00adment a tile, they are unsuitable in two ways. First, they are \nlikely to keep useless implementations that are the right size but will never help implement a tile. \nSecond, they are likely to reject use\u00adful implementations: to implement some tiles, long sequences of \n1 estimateLaws({true} G {l = in e . modi.es L}) a larger covering, we discard it. Enough triples pass \nthis test to .nd 2 return expEstimate(e) complete tilesets for the x86, PowerPC, and ARM.  3 We implement \nour utility test using the algorithm in Figure 8. 4 expEstimate(e): e . int Function estimateLaws takes \na candidate triple and returns the 5 if e is computed by an expansion tile then return 0 size of the \nsmallest covering of the expression in the postcondition ) (lines 1 and 2). We compute the size using \nthe mutually recursive functions expEstimate (lines 4 7) and coverWithLawFragment else return min{1+ \ncoverWithLawFragment(e, e | E[e ]= e2 . Laws} (lines 9 14). Function expEstimate considers a potential \ncovering 9 coverWithLawFragment(e, e ): e \u00d7 e . int using every subexpression e . on the left-hand side \nof every alge\u00ad case (e, e ) of: braic law (lines 6 7). 1,...,en )): 11 (.(e1,...,en), . (e Pn 12 return \ni=1 min(coverWithLawFragment(ei,e i), expEstimate(ei)) with another expression e Function coverWithLawFragment \ntries to cover expression e . . If both expressions are applications 13 14 default: return 8 Figure \n8. Our heuristic estimates the minimum number of alge\u00adbraic laws that might be applied before our algorithm \ncan conclude that the input Hoare triple is used in the implementation of a tile. instructions and tall \nexpressions may be necessary. For exam\u00adple, as Figure 6 shows, a load-immediate tile for a RISC ma\u00adchine \ntypically requires multiple instructions and an expression ((k .l n) . n) . zx(lobitsn(k)) of height \n4. For these rea\u00adsons, we abandoned traditional heuristics. 9.1 Our new heuristic: pruning according \nto predicted utility We have developed a new heuristic which limits the pool to hold only those Hoare \ntriples deemed likely to be useful in implementing some expansion tile. Our utility test is inspired \nby the technique we use to .nd new implementations: the application of algebraic laws. We hypothesize \nthat when more algebraic laws have to be applied to a Hoare triple, it is less likely that the triple \nwill be useful. After all, general-purpose architectures are designed to implement almost all of our \nRTL operators, and many of the rest can be implemented by sequences of such simple, ubiquitous instructions \nas shifts, log\u00adical operations, and two s-complement arithmetic (Warren 2003). Implementing a tile may \nrequire a long sequence of instructions, but the amount of reasoning required is usually small. Our utility \ntest estimates the minimum number of algebraic laws that might be applied to a Hoare triple before our \nalgorithm may .nd an implementation of a tile. Given a Hoare triple that computes an expression e, our \ntest covers e with fragments of algebraic laws. The more fragments required to cover e, the less likely \ne is to pass the utility test. Our utility test uses fragments of laws because when the establish\u00adment \nprocedure is given a goal that includes the left-hand side of alaw, rule REDUCE APPLICATIONS or PREPARE \nOPERATOR may split that goal into subgoals, which are formed from fragments of the original law. To be \nuseful, therefore, a Hoare triple need only com\u00adpute a fragment of the left-hand side of an algebraic \nlaw. For exam\u00adple, to cover the overlined and underlined parts of the expression r1 := r2 . (k .l 16) \n. 16, which is computed by the load upper-immediate instruction on the PowerPC, we can use the overlined \nand underlined fragments of the following algebraic laws: of the same operator, then we try to cover \nthe subexpressions of e. We may be able to cover a subexpression of e inductively, using the corresponding \nsubexpression of e . (line 12). But we may also be able to cover a subexpression of e with another algebraic \nlaw by calling expEstimate recursively (line 13). We try both options and use whichever is cheaper. \n 9.2 Termination of the search for tiles By using our utility test to discard Hoare triples that are \nunlikely to lead to tiles, we bound the size of the search space. The post\u00adcondition of every triple \ncan be covered by at most four algebraic laws. There are .nitely many algebraic laws, so an expression \nthat can be covered with at most four laws has bounded height, so there are .nitely many expressions. \nThe machine descriptions mention .nitely many locations, so there are .nitely many postconditions. For \ntermination, it therefore suf.ces to ensure that a new Hoare triple is added to the pool only if no existing \nHoare triple can es\u00adtablish the same postcondition. We actually do slightly better. Sometimes an expression \nsuch as r1 + x#w can be used in any context where another expression such as r1 +1 can be used. In these \ncases, we say that the .rst ex\u00adpression subsumes the second. To exploit subsumption, whenever we have \na useful candidate triple that computes an expression e, we consider all the triples in the pool that \ncompute the same ex\u00adpression e, modulo locations and constants, which may differ. We then check whether \ne subsumes, or is subsumed by, any of the other expressions. Triples whose expressions are subsumed are \ndropped from the pool. 10. Evaluation Our work is intended to make it easier to retarget compilers, and \nwe evaluate it accordingly: 1. We show that the components we generate are suitable for use in a production \ncompiler. 2. We show that retargeting a compiler using our algorithm and declarative machine descriptions \nis easier than retargeting well\u00adknown compilers.  It is a side bene.t that the very same machine descriptions \nwe use have also been used to help retarget other tools. 10.1 Suitability of generated components for \nproduction use We have generated back ends for our research compiler, Quick C--, targeting the x86, PowerPC, \nand ARM architectures. We have eval\u00ad 0 . x = x LL uated these back ends for suitability by examining \nthe run times of ((xL .l n) . n) . zx(lobitsn(xL)) = xL. compiled programs and the run time of our compiler, \nthen compar\u00ading them with analogous times from production compilers. Relative The expression can therefore \nbe covered using two algebraic laws. to a production compiler, Quick C--has received very few man-A \ntriple passes our utility test if the expression in its postcondition hours of development, so Quick \nC--cannot be expected to compile can be covered using at most four algebraic laws; if a triple requires \nthe same benchmark suite that a production compiler can handle, Table 9. Running times for benchmarks \non x86 and PowerPC: On each architecture, we give the time in seconds for a baseline compiler, then normalize \nall other times to the baseline. The Hand and Generated columns represent Quick C--with hand-written \nand automatically generated x86 back ends. Benchmarks were compiled without debugging or pro.ling information, \nthen run on an AMD Athlon MP 2800+ with 2 GB of memory and a PowerPC 7447A clocked at 1.25 GHz with 1 \nGB of memory. x86 experiments PowerPC experiments Benchmark lcc Hand / lcc Generated / lcc gcc -O0 / \nlcc gcc -O1 / lcc gcc -O0 gcc -O1 / gcc -O0 Generated / gcc -O0 compress-95 43.32 s 0.84 0.80 1.01 0.69 \n107.07 s 0.39 0.72 go-95 25.84 s 0.98 0.88 1.01 0.61 62.03 s 0.40 0.73 vortex-95 55.84 s 1.09 1.10 1.05 \n0.77 144.81 s 0.54 0.73 mcf-2006 230.24 s 0.94 0.92 0.97 0.90 238.63 s 0.77 0.81 bzip2-2006 316.06 s \n0.91 0.90 1.06 0.68 670.14 s 0.43 0.60 and it cannot be expected to produce equally optimized code. \nStill, our measurements provide a basis for judging whether our back ends are suitable for production \nuse. Our choice of benchmarks is limited by the availability of retar\u00adgetable front ends. In order to \navoid porting a run-time system to Quick C--, we work only with C benchmarks. Among C com\u00adpilers, the \nonly one we know of that can cleanly and easily be connected to a new back end is lcc (Fraser and Hanson \n1995). Unfortunately, because lcc is limited to ANSI C, and because Quick C--does not support <stdarg.h>,4 \nthe combination of lcc and Quick C--can compile only a fraction of typical benchmarks. From the SPEC \nCPU95 and SPEC CPU2006 benchmark suites, we have chosen those benchmarks that can be compiled by this \ncombi\u00adnation. For each benchmark, we measured the wall-clock running time, averaged over .ve runs. The \nCPU95 benchmarks show results on the reference inputs, and the CPU2006 benchmarks show results on the \ntraining inputs. At present, Quick C--has another signi.cant limitation: it can\u00adnot use software libraries \nto implement RTL operators that are not supported in hardware. Although it would be possible to write \nal\u00adgebraic laws to help our algorithm .nd implementations of unsup\u00adported operators, that approach would \nbe impractical: it would lead to bloated executables. The standard technique is to implement a missing \noperator by making a function call into one of the soft\u00adware libraries that are provided by the manufacturers \nof the ARM. Because Quick C--does not yet support this technique, we have run no benchmarks on the ARM. \nWe have, however, veri.ed that our automatically generated ARM back end correctly compiles the integer-only \nprograms in the Quick C--and lcc test suites. On the x86, we compare benchmarks compiled using lcc, a \nhand\u00adwritten back end for Quick C--, our automatically generated back end, and gcc using both -O0 and \n-O1. On the PowerPC, no lcc back end or hand-written C--back end is available, so we compare only our \nautomatically generated back end and gcc. Table 9 shows the running times of the benchmarks. Running \ntimes of code from baseline compilers are in seconds; other times are given as ratios. Ratios for our \nautomatically generated back ends are highlighted. Programs compiled by our automatically generated back \nend run at least as fast as when compiled by the hand-written back end. The one exception is the vortex-95 \nbenchmark, which when com\u00adpiled by our automatically generated back end is just 0.3% slower. Programs \ncompiled by our automatically generated back end are also comparable to unoptimized code produced by \nlcc or by gcc -O0. Although none of the compilers perform many optimiza\u00adtions, code from Quick C--is \noften a bit faster, probably because of our peephole optimizer. But when we run gcc with a suite of 4 \nDe.nition of C-style variadic functions is incompatible with proper tail scalar optimizations (gcc -O1), \nit produces code that is 2% to 30% faster than code produced by our automatically generated back end. \nUntil we have implemented a substantial set of optimizations in Quick C--, we cannot provide direct, \nincontrovertible evidence that we can generate a back end for an optimizing compiler that is competitive \nwith gcc. But our results, together with Benitez and Davidson s (1994) prior work showing that standard \nscalar and loop optimizations can be implemented in a compiler very similar to Quick C--, indicate that \nour code expanders and recognizers could be used in a high-quality optimizing compiler. Compiled code \nshould run fast, but the compiler itself should run reasonably fast, too. We measured the time required \nto compile the Quick C--test suite using generated and hand-written tilesets (Table 10). When using the \nautomatically generated tileset, compile time increases by about 10%. The extra time is spent not in \nthe code expander but in the optimizer; the compiler takes longer because the generated code expander \nemits worse code.  10.2 Ease of retargeting It is easy to measure speed of compiled programs. It is \nharder to evaluate whether we have developed a good approach to retarget\u00ading compilers. In this paper, \nwe evaluate only components related to instruction selection. Machine-dependent components that solve \nother important problems register allocation, stack-frame layout, and calling conventions cannot be generated \nfrom declarative ma\u00adchine descriptions alone, even in principle; some knowledge of software convention \nis required. For that reason, these components are best retargeted using other techniques, as we have \ndescribed elsewhere (Smith, Ramsey, and Holloway 2004; Lindig and Ram\u00adsey 2004; Olinsky, Lindig, and \nRamsey 2006). We compare retargeting effort with two very different compilers: gcc and lcc; Dias (2008, \n\u00a78.2) presents a more thorough case study which also includes vpo. Like Quick C--, gcc is an opti\u00admizing \ncompiler that uses a code expander and a recognizer, and like Quick C--, gcc tries to use the full instruction \nset of the tar\u00adget machine. Although gcc is widely retargeted, it is not clear how much design effort \nhas been invested in making retargeting easy. By contrast, lcc has been designed for retargeting and \nfor compi\u00adlation speed, but not for optimization. It translates low-level inter\u00admediate code to assembly \ncode using bottom-up rewriting, which is speci.ed using a BURG mapping from lcc s intermediate code to \nassembly-code strings (Fraser, Henry, and Proebsting 1992). All three compilers use domain-speci.c languages \nto help create in\u00adstruction selectors, but the languages are processed very differently. We illustrate \nthe descriptions by using the x86 subtract instruction as an example. Quick C--uses this concise .-RTL \ndescription: SUB^mr^od (Eaddr, reg) is Eaddr := sub (Eaddr, reg)  calls, which every C--compiler is \nrequired to support. | Reg.EFLAGS := x86_subflags(Eaddr, reg) .-RTL can be used for multiple purposes, \nbut as shown in this paper, creating an instruction selector requires substantial analysis. The lcc compiler \nuses an even more concise description, which maps a subtract node in lcc s intermediate form to an assembly\u00adlanguage \ntemplate: reg: SUBI4(reg,mrc) \"?movl %0,%c\\nsubl %1,%c\\n\" 1 In the template, %c represents the target \nregister; %0 and %1 rep\u00adresent the two arguments. The two-instruction sequence simulates lcc s three-address \nnode on the two-address target machine. The lcc BURG pattern is very concise, and processing is simple: \nthe compiler .nds a minimum-cost covering of each intermediate code, then instantiates the templates. \nBut instantiation is less sim\u00adple than it may appear. For example, the initial question mark sig\u00adnals \na special case in which lcc avoids emitting the movl instruc\u00adtion when %0 and %c denote the same register. \nAnd the template language includes an escape clause by which a template can be instantiated by calling \nthe function emit2, which can execute arbi\u00adtrary C code. Gcc s description of an x86 subtract is dramatically \ndifferent from either a .-RTL machine description or a BURG mapping: (define_expand \"subsi3\" [(parallel \n[(set (match_operand:SI 0 \"nonimmediate_operand\" \"\") (minus:SI (match_operand:SI 1 \"nonimmediate_operand\" \n\"\") (match_operand:SI 2 \"general_operand\" \"\"))) (clobber (reg:CC FLAGS_REG))])] \"\" \"ix86_expand_binary_operator(MINUS,SImode,operands); \nDONE;\") (define_insn \"*subsi_3\" [(set (reg FLAGS_REG) (compare (match_operand:SI 1 \"nonimmediate_operand\" \n\"0,0\") (match_operand:SI 2 \"general_operand\" \"ri,rm\"))) (set (match_operand:SI 0 \"nonimmediate_operand\" \n\"=rm,r\") (minus:SI (match_dup 1) (match_dup 2)))] \"ix86_match_ccmode (insn, CCmode) &#38;&#38; ix86_binary_operator_ok \n(MINUS, SImode, operands)\" \"sub{l}\\t{%2, %0|%0, %2}\" [(set_attr \"type\" \"alu\") (set_attr \"mode\" \"SI\")]) \nThe description language exists only to generate gcc s expander, recognizer, and parts of its optimizer; \nwhen the description is processed, different parts are used to generate different com\u00adponents, and arbitrary \nC code can be spliced in. For example, define_expand and define_insn help generate gcc s expander and \nrecognizer. And to verify that an RTL is a proper two-address RTL, whose destination is the same as the \n.rst operand, the recog\u00adnizer calls C function ix86 binary operator ok. Examining a single instruction \nilluminates the kind of effort re\u00adquired to retarget each compiler: Our work requires only compiler\u00adindependent \ndescriptions of instructions semantics, but it must be supplemented by a compiler-compiler that generates \ncompiler\u00adspeci.c expanders and recognizers. Both lcc and gcc use de\u00adscriptions organized around their \nrespective low-level intermediate codes, supplemented by C code that is both compiler-and target\u00adspeci.c. \nRe.ecting years of design effort, lcc s descriptions are an order of magnitude simpler than gcc s. We \nnow turn to the degree of effort required to retarget each com\u00adpiler. How many instructions does each \ncompiler exploit, and at the cost of what effort? We believe that number of instructions rec\u00adognized \npredicts the code quality achievable with a back end, be- Back end Expansion Time (s) Compile Time (s) \nGenerated 0.87 23.72 Hand-written 0.77 21.45 Table 10. Compile times for the Quick C--test suite (10,718 \nlines in 91 .les) using generated and hand-written x86 back ends. Times are averaged over .ve runs, with \neach run compiling the entire test suite to assembly code with one invocation of the compiler. The compiler \nran on a Pentium M, 1.5 GHz with 1 GB of memory. cause the more instructions the recognizer accepts, \nthe more code\u00adimproving transformations an optimizer can apply. As expected, lcc exploits the fewest \ninstructions: a code generator based on BURG covers the compiler s intermediate code, not the target \ninstruction set. In lcc 4.2, the back end uses 249 BURG patterns but only 53 instruction opcodes; the \nescape hatch is used in 8 patterns but uses only one additional opcode. The back end uses 549 lines of \nBURG description plus 463 lines of target\u00addependent C code. The compilers designed for optimization require \nlarger descriptions and more target-dependent code, but they exploit many more in\u00adstructions. In the \nhand-written Quick C--back end for the x86, the tileset and recognizer require 1,286 lines of code but \ncan gen\u00aderate only 233 distinct instructions. In the automatically generated Quick C--back end, the .-RTL \nand SLED descriptions require 1,948 lines, but they describe 639 distinct instructions. (And the SLED \ndescription has been reused to create other tools, includ\u00ading a binary translator, a link-time optimizer, \nand a dynamic code generator, so perhaps not all of its lines should be charged only to Quick C--.) In \ngcc 4.2.1, the compiler-speci.c machine de\u00adscription requires 18,462 lines to exploit 754 instructions \nand to describe 320 expansions and 83 machine-speci.c peephole opti\u00admizations. Among the four back ends, \nlcc may have the most impressive power-to-weight ratio, but our automatically generated instruction selectors \nare competitive, and they support code-improving trans\u00adformations, such as peephole optimization, that \nBURG does not support. And when we compare the two automatically generated optimizing compilers, the \ncontrast between declarative machine de\u00adscriptions and gcc s machine descriptions cannot be overstated. \nGcc s machine description is an order of magnitude longer than the .-RTL machine description, suggesting \nthat retargeting effort will be proportionally greater. There are also signi.cant differences in kind. \nDeclarative descriptions can be tested independently (Fer\u00adn\u00b4andez and Ramsey 1997; Bailey and Davidson \n2003); gcc s de\u00adscriptions cannot. The declarative machine description concisely describes a property \nof a machine, independent of any particular tool, whereas gcc s domain-speci.c code is verbose and requires \nknowledge of gcc s internals. More signi.cantly, gcc s retargeting strategy relies on textual inclusion \nof arbitrary snippets of C code, including calls to hand-written, target-speci.c functions. This strat\u00adegy \nmakes it unlikely that one of gcc s machine descriptions could be analyzed or reused for another purpose. \nFinally, we believe that the compiler writer s task is made more dif.cult by gcc s tactic of writing \ncritical code fragments by hand and using a macro frame\u00adwork to include those fragments in the right \nparts of the generated back end.   11. Related Work People have been working on retargetable compilers \nsince 1958 (e.g., Strong et al.). We discuss only the most closely related work: back ends or optimizers \ngenerated using machine descriptions or search techniques. We focus on the search strategy used to .nd \nin\u00adstructions and the pruning heuristic used to guarantee termination.  The most closely related work \nwas conducted thirty years ago by Cattell (1980). Cattell pioneered some ideas that we use, includ\u00ading \nrewriting with algebraic laws and compensating for unwanted assignments by using temporaries and scratch \nregisters. But unlike our search, Cattell s search starts with the goal of implementing a particular \ncomputation (analogous to one of our expansion tiles) and searches for an implementation composed of \nmachine instruc\u00adtions. Given a goal, Cattell s algorithm rewrites the goal using an algebraic law. A \nheuristic chooses which law to apply; the heuris\u00adtic looks for a law that it estimates will produce a \nnew goal that is somehow closer to machine instructions. Cattell prunes the search by limiting both the \ndepth of rewriting and the number of laws which may be used to rewrite a single expression. These limits \nguarantee termination, but they place a tremendous burden on the heuristic that chooses laws: it must \nprioritize the algebraic laws that are likely to lead to machine instructions. By contrast, our search \ndoes not limit the depth of rewrites apriori, and it attempts to apply every applicable law.5 Hoover \nand Zadeck (1996) present the TOAST machine-description language and an algorithm for generating instruction \nselectors. Their algorithm tries to recursively match a goal expression with the expression computed \nby an assignment in a machine instruc\u00adtion. They prune the search by checking the result of each rewrite: \nsearch continues only if the root of the resulting goal expression matches the expression in the machine \ninstruction. This heuristic ensures that the search is bounded by the size of the expression computed \nby the machine instruction. But this pruning heuristic misses opportunities for one algebraic law to \nrewrite the results produced by applying another algebraic law. Ceng et al. (2005) generate most of a \nBURG-style instruction selector using a LISA machine description. They have two ways to .nd an implementation \nof their equivalent of a tile: there may be a machine instruction with an assignment that implements \nthe tile; or if not, they have a collection of rewrite rules, one of which may implement the goal tile \nusing other tiles. Their search does not .nd implementations of every tile; some are implemented by hand. \nUnlike many others, Ceng et al. present experimental results: code produced by their generated back end \nis about 5% slower than code produced by a hand-written back end. The instruction selector runs after \nthe optimizer, so when the instruction selector produces inef.cient code, the .nal program is also inef.cient. \nRemarkably, it is possible to retarget a back end without a machine description, by extracting knowledge \nfrom an existing C compiler (Collberg 1997). Collberg s algorithm generates hundreds of small, carefully \ncrafted sample programs, compiles them, and analyzes the resulting assembly code. Collberg de.nes a simple, \nRISC\u00adlike intermediate code, and his algorithm uses the results of the analyses to generate a mapping \nfrom that code to target instruc\u00adtions. The mapping is given to BEG (Emmelmann, Schr\u00a8oer, and Landwehr \n1989), which generates a back end. Although Collberg s techniques are largely unrelated to the other \ntechniques discussed here, his techniques are ingenious, the degree of automation is im\u00adpressive, and \nthe work repays careful study. A less closely related problem is superoptimization: a search for more \nef.cient ways of implementing computations whose imple\u00admentations on a target machine are already known \n(Massalin 1987; Granlund and Kenner 1992). Massalin s superoptimizer exhaus\u00adtively combines sequences \nof machine instructions, then executes 5 To be fair to Cattell, technology has changed dramatically, \nand our algo\u00adrithm might be unworkable on 1980 hardware. each sequence against test inputs to see if \nthe sequence implements a goal computation. The superoptimizer doesn t use a machine de\u00adscription; it \nuses the machine itself. The search is pruned by reject\u00ading sequences, either because a subsequence is \nknown to be subop\u00adtimal or because a later instruction in the sequence does not use the results of any \nearlier instructions. This pruning does not guarantee termination, but since the goal of a superoptimizer \nis to .nd ways of making a working back end better, a superoptimizer needn t be guaranteed to terminate \nit can be killed at any time. Joshi, Nelson, and Zhou (2006) present the Denali superoptimizer, which \ndoes use a machine description. The description is declara\u00adtive, and it takes the form of axioms given \nto a refutation-based theorem prover. Denali uses goal-directed search to .nd a se\u00adquence of machine \ninstructions that implements a goal computa\u00adtion. The search is limited by a matcher, which enumerates \nequiv\u00adalence relations for each expression in the input code. (It is not clear how Denali ensures that \neach equivalence class is .nite; the published examples suggest that the technique involves limiting \nthe set of algebraic laws.) After identifying equivalent ways to imple\u00adment the goal computation, Denali \nuses a SAT solver to search for implementations that use the fewest processor cycles. Denali .nds high-quality \nimplementations of register-to-register computations.  12. Conclusion In this paper, we generate an \ninstruction selector using declarative machine descriptions, which give only the syntax and semantics \nof target-machine instructions. We show that the problem is undecid\u00adable, so a heuristic search is indicated. \nWe present a new search technique and a new pruning heuristic, which rely on three key ideas: We search \nonly computations that we know can be implemented entirely by machine instructions.  When a sequence \nof instructions implements an expansion tile, we can discover this sequence by matching the left-hand \nside of an algebraic law with the expression computed by the .nal instruction in the sequence.  When \nwe match two expressions, we try uni.cation, but when the expressions involve machine state, we instead \nsearch for a sequence of machine instructions whose execution makes the expressions equal.  Our technique \n.nds not just implementations but also valid Hoare triples, which could be used to generate a certi.ed \ncode expander (cf Leroy 2006). Our new search algorithm generates high-quality instruction selec\u00adtors \nfor the x86, PowerPC, and ARM. And thanks to Davidson and Fraser s (1980) compiler design, our back ends, \nunlike earlier auto\u00admatically generated back ends, produce code that is as good as the code produced \nby hand-written back ends. Acknowledgments This work was funded by a grant from Intel Corporation and \nby NSF awards 0838899 and 0311482. We are grateful for extensive feedback from anonymous reviewers, especially \nthose who pressed for deeper treatment of our results.  References Alfred V. Aho, Mahadevan Ganapathi, \nand Steven W. K. Tjiang. 1989 (October). Code generation using tree matching and dynamic program\u00adming. \nACM Transactions on Programming Languages and Systems,11 (4):491 516.  Joel Auslander, Matthai Philipose, \nCraig Chambers, Susan Eggers, and Brian Bershad. 1996 (May). Fast, effective dynamic compilation. Proceedings \nof the ACM SIGPLAN 96 Conference on Programming Language Design and Implementation, in SIGPLAN Notices, \n31(5):149 159. Mark W. Bailey and Jack W. Davidson. 2003 (November). Automatic detection and diagnosis \nof faults in generated code for procedure calls. IEEE Transactions on Software Engineering, 29(11):1031 \n1042. Manuel E. Benitez and Jack W. Davidson. 1994 (March). The advantages of machine-dependent global \noptimization. In Programming Languages and System Architectures, LNCS volume 782, pages 105 124. Springer \nVerlag. Mark Bezem, Jan Willem Klop, and Roel de Vrijer, editors. 2003. Term Rewriting Systems. Cambridge \nUniversity Press, Cambridge, UK. Roderic G. G. Cattell. 1980 (April). Automatic derivation of code gener\u00adators \nfrom machine descriptions. ACM Transactions on Programming Languages and Systems, 2(2):173 190. Jianjiang \nCeng, Manuel Hohenauer, Rainer Leupers, Gerd Ascheid, Hein\u00adrich Meyr, and Gunnar Braun. 2005. C compiler \nretargeting based on instruction semantics models. In Design, Automation, and Test in Eu\u00adrope, pages \n1150 1155. IEEE Computer Society. Cristina Cifuentes, Mike Van Emmerik, and Norman Ramsey. 1999 (Oc\u00adtober). \nThe design of a resourceable and retargetable binary translator. In Proceedings of the Sixth Working \nConference on Reverse Engineering (WCRE 99), pages 280 291. Christian S. Collberg. 1997 (May). Reverse \ninterpretation + mutation analysis = automatic retargeting. Proceedings of the ACM SIGPLAN 97 Conference \non Programming Language Design and Implementation, in SIGPLAN Notices, 32(5):57 70. M. E. Conway. 1958 \n(October). Proposal for an UNCOL. Communications of the ACM, 1(10):5 8. Jack W. Davidson. 2008. Personal \ncommunication, 14 November 2008. Jack W. Davidson and Christopher W. Fraser. 1980 (April). The design \nand application of a retargetable peephole optimizer. ACM Transactions on Programming Languages and Systems, \n2(2):191 202. Jack W. Davidson and Christopher W. Fraser. 1984 (October). Code selec\u00adtion through object \ncode optimization. ACM Transactions on Program\u00adming Languages and Systems, 6(4):505 526. Jo ao Dias. \n2008 (December). Automatically Generating the Back End of a Compiler Using Declarative Machine Descriptions. \nPhD thesis, Harvard University, School of Engineering and Applied Sciences. As of July 2009, at http://tinyurl.com/lxhly5. \nJo ao Dias and Norman Ramsey. 2006 (March). Converting intermediate code to assembly code using declarative \nmachine descriptions. In 15th International Conference on Compiler Construction (CC 2006), LNCS volume \n3923, pages 217 231. Edsger W. Dijkstra. 1976. A Discipline of Programming. Prentice-Hall, Englewood \nCliffs, NJ. Helmut Emmelmann, Friedrich-Wilhelm Schr\u00a8oer, and Rudolf Landwehr. 1989 (July). BEG a generator \nfor ef.cient back ends. Proceedings of the ACM SIGPLAN 89 Conference on Programming Language Design and \nImplementation, in SIGPLAN Notices, 24(7):227 237. Lee D. Feigenbaum. 2001 (April). Automated translation: \ngenerating a code generator. Technical Report TR-12-01, Computer Science, Harvard University. Mary F. \nFern\u00b4andez and Norman Ramsey. 1997 (May). Automatic checking of instruction speci.cations. In Proceedings \nof the International Con\u00adference on Software Engineering, pages 326 336. Mary F. Fern\u00b4andez. 1995 (June). \nSimple and effective link-time optimiza\u00adtion of Modula-3 programs. Proceedings of the ACM SIGPLAN 95 \nConference on Programming Language Design and Implementation, in SIGPLAN Notices, 30(6):103 115. Christopher \nW. Fraser. 1989 (July). A language for writing code generators. Proceedings of the ACM SIGPLAN 89 Conference \non Programming Language Design and Implementation, in SIGPLAN Notices, 24(7):238 245. Christopher W. \nFraser and David R. Hanson. 1995. A Retargetable C Compiler: Design and Implementation. Benjamin/Cummings, \nRedwood City, CA. Christopher W. Fraser, David R. Hanson, and Todd A. Proebsting. 1992 (September). Engineering \na simple, ef.cient code-generator generator. ACM Letters on Programming Languages and Systems, 1(3):213 \n226. Christopher W. Fraser, Robert R. Henry, and Todd A. Proebsting. 1992 (April). BURG fast optimal \ninstruction selection and tree parsing. SIG-PLAN Notices, 27(4):68 76. Torbjoern Granlund and Richard \nKenner. 1992. Eliminating branches using a superoptimizer and the GNU C compiler. Proceedings of the \nACM SIGPLAN 92 Conference on Programming Language Design and Implementation, in SIGPLAN Notices, 27(7):341 \n352. Roger Hoover and Kenneth Zadeck. 1996. Generating machine speci.c optimizing compilers. In Conference \nRecord of the 23rd Annual ACM Symposium on Principles of Programming Languages, pages 219 229. Rajeev \nJoshi, Greg Nelson, and Yunhong Zhou. 2006. Denali: A practical algorithm for generating optimal code. \nACM Transactions on Program\u00adming Languages and Systems, 28(6):967 989. Xavier Leroy. 2006. Formal certi.cation \nof a compiler back-end or: programming a compiler with a proof assistant. In J. Gregory Morrisett and \nSimon L. Peyton Jones, editors, Conference Record of the 33rd Annual ACM Symposium on Principles of Programming \nLanguages, pages 42 54. Christian Lindig and Norman Ramsey. 2004 (April). Declarative compo\u00adsition of \nstack frames. In 13th International Conference on Compiler Construction (CC 2004), LNCS volume 2985, \npages 298 312. Henry Massalin. 1987 (October). Superoptimizer: A look at the small\u00adest program. Proceedings \nof the 2nd International Conference on Ar\u00adchitectural Support for Programming Languages and Operating \nSystem (ASPLOS II), in SIGPLAN Notices, 22(10):122 127. Reuben Olinsky, Christian Lindig, and Norman \nRamsey. 2006 (January). Staged allocation: A compositional technique for specifying and imple\u00admenting \nprocedure calling conventions. In Proceedings of the 33rd ACM Symposium on the Principles of Programming \nLanguages, pages 409 421. Eduardo Pelegr\u00b4i-Llopart and Susan L. Graham. 1988 (January). Optimal code \ngeneration for expression trees: An application of BURS theory. In Conference Record of the 15th Annual \nACM Symposium on Principles of Programming Languages, pages 294 308. Norman Ramsey and Jack W. Davidson. \n1998 (June). Machine descriptions to build tools for embedded systems. In ACM SIGPLAN Workshop on Languages, \nCompilers, and Tools for Embedded Systems (LCTES 98), LNCS volume 1474, pages 172 188. Springer Verlag. \nNorman Ramsey and Mary F. Fern\u00b4andez. 1995 (January). The New Jersey Machine-Code Toolkit. In Proceedings \nof the 1995 USENIX Technical Conference, pages 289 302. Norman Ramsey and Mary F. Fern\u00b4andez. 1997 (May). \nSpecifying repre\u00adsentations of machine instructions. ACM Transactions on Programming Languages and Systems, \n19(3):492 524. Norman Ramsey and Simon L. Peyton Jones. 2000 (May). A single in\u00adtermediate language that \nsupports multiple implementations of excep\u00adtions. Proceedings of the ACM SIGPLAN 00 Conference on Program\u00adming \nLanguage Design and Implementation, in SIGPLAN Notices,35 (5):285 298. Michael D. Smith, Norman Ramsey, \nand Glenn Holloway. 2004 (June). A generalized algorithm for graph-coloring register allocation. ACM \nSIGPLAN 04 Conference on Programming Language Design and Im\u00adplementation, in SIGPLAN Notices, 39(6):277 \n288. J. Strong, J. H. Wegstein, A. Tritter, J. Olsztyn, Owen R. Mock, and T. Steel. 1958. The problem \nof programming communication with changing machines: A proposed solution (Part 2). Communications of \nthe ACM,1 (9):9 16. Henry S. Warren. 2003. Hacker s Delight. Addison-Wesley.  \n\t\t\t", "proc_id": "1706299", "abstract": "<p>Despite years of work on retargetable compilers, creating a good, reliable back end for an optimizing compiler still entails a lot of hard work. Moreover, a critical component of the back end---the instruction selector---must be written by a person who is expert in both the compiler's intermediate code and the target machine's instruction set. By <i>generating </i> the instruction selector from declarative machine descriptions we have (a) made it unnecessary for one person to be both a compiler expert and a machine expert, and (b) made creating an optimizing back end easier than ever before.</p> <p>Our achievement rests on two new results. First, finding a mapping from intermediate code to machine code is an undecidable problem. Second, using heuristic search, we can find mappings for machines of practical interest in at most a few minutes of CPU time. </p> <p>Our most significant new idea is that heuristic search should be controlled by algebraic laws. Laws are used not only to show when a sequence of instructions implements part of an intermediate code, but also to limit the search: we drop a sequence of instructions not when it gets too long or when it computes too complicated a result, but when <i>too much reasoning </i> will be required to show that the result computed might be useful.</p>", "authors": [{"name": "Jo&#227;o Dias", "author_profile_id": "81443595159", "affiliation": "Tufts University, Medford, USA", "person_id": "P1911131", "email_address": "", "orcid_id": ""}, {"name": "Norman Ramsey", "author_profile_id": "81100300481", "affiliation": "Tufts University, Medford, USA", "person_id": "P1911132", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706346", "year": "2010", "article_id": "1706346", "conference": "POPL", "title": "Automatically generating instruction selectors using declarative machine descriptions", "url": "http://dl.acm.org/citation.cfm?id=1706346"}