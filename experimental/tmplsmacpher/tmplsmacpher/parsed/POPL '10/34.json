{"article_publication_date": "01-17-2010", "fulltext": "\n Semantics and Algorithmsfor Data-dependentGrammars Trevor Jim Yitzhak Mandelbaum David Walker AT&#38;T \nLabs-Research AT&#38;T Labs-Research Princeton University trevor@research.att.com yitzhak@research.att.com \ndpw@cs.princeton.edu Abstract Wepresentthedesignandtheoryofanewparsingengine,YAKKER, capable of satisfying \nthe manyneeds of modern programmers and modern data processing applications. In particular, our new pars\u00ading \nengine handles(1)full scannerless context-free grammars with (2) regular expressions as right-hand sides \nfor de.ning nontermi-nals.YAKKER also includes (3)facilities for bindingvariables to intermediate parse \nresults and (4) using such bindings within arbi\u00adtrary constraints to control parsing. Thesefacilities \nallow the kind of data-dependent parsing commonly needed in systems applica\u00adtions, particularly those \nthat operate over binary data. In addition, (5) nonterminals may be parameterized by arbitrary values, \nwhich gives the system good modularity and abstraction properties in the presence of data-dependent parsing. \nFinally, (6)legacyparsing li\u00adbraries, such as sophisticated libraries for dates and times, may be directly \nincorporated into parser speci.cations. We illustrate the importance and utility of this rich collection \nof features by pre\u00adsenting its use on examples ranging from dif.cult programming language grammars to \nweb server logs to binary data speci.cation. We also show that our grammars have important compositionality \npropertiesandexplainwhysuch propertiesareimportantinmodern applications such as automatic grammar induction. \n In terms of technical contributions, we provide a traditional high-level semantics for our new grammar \nformalization and show how to compile grammars into nondeterministic automata. These automata are stack-based, \nsomewhat like conventional push-down automata,but are also equipped with environments to track data\u00addependent \nparsing state.Weprovethe correctnessof our translation of data-dependent grammars into these new automata \nand then show how to implement the automata ef.ciently using a variation of Earley s parsing algorithm. \nCategories and Subject Descriptors F.4.2[Mathematical Logic andFormal Languages]:Grammars and Other Rewriting \nSystems GrammarTypes,Parsing; D.3.1[Programming languages]:For\u00admal De.nitions and Theory Syntax; D.3.4[Programming \nlan\u00adguages]: Processors Parsing General Terms Algorithms, Languages, Theory Keywords Data-dependent grammars, \ncontext-sensitive gram\u00admars, scannerless parsing, EBNF, regular right-sides, regular ex- Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 10, January 17 23, \n2010, Madrid, Spain. Copyright c &#38;#169; 2010ACM978-1-60558-479-9/10/01... $10.00. pressions, L-attributed \ngrammars, ambiguous grammars, semantic predicates, Earleyparsing, automata, transducers 1. Introduction \nThe study of parsing is one of theoldest and most intellectually sat\u00adisfying areas of programminglanguages, \nstretching back decades to thedawnof computer science.Thebestworkinthis.eldhasnearly universal practical \napplication and yet is based around remarkably elegant and general automaton theories. More recently, \nhowever, the study of parsing has come to be viewed as a somewhat boring, largelysolved problem.Tobe \nhonest,usingtheword parsing in the introduction of a POPL paper is a bit of a dicey move. Perhaps one \nreason the study of parsing in programming lan\u00adguages circles may have gone out of vogue is that widely-used \nparser generators such as YACC [16] hit a sweet spot in the expressiveness-performance tradeoffspacein \nthe 1970s. YACCand its relatives were based around the LR(1) fragment of context-free grammarsand hence \nwerepowerful enoughtoexpressthe syntaxof manyprogramming languages (with only the occasional egregious \nhack), and yet also gave linear-time performance. Consequently, in an era when computational resources \nwere less plentiful than in modern times, programmer convenience and expressiveness were sacri.ced for \nperformance. Since then, for the mostpart, program\u00adming language implementors have hungon to legacytools \nbecause they are well-known, well-supported, taught in school and univer\u00adsally available, rather than \nbecause they are optimally designed. On the other hand, programmers outside the minuscule world of PL \nimplementors almost never use parser generators. Despite thefact that they are constantly writing parsers \nfor data formats, networking protocols, con.guration .les, web scraping and small domain-speci.c languages \ntheydo mostofitbyhand, often using regular expression libraries that support context-sensitive features \nlike backreferencing and POSIX anchors. This is not because they are unawareof parser generators,but \nrather because these toolsdo not solvetheir problems.Forexample, when Rescorla implemented the SSL message \ndata format, he attempted to write a grammar\u00addriven parser (using YACC)for the language with the aim \nof me\u00adchanically generating a decoder,but abandoned the projectin frus\u00adtration [27, p. 68].Another highly-visible \nproject, HTML 5.0 [13], has abandoned the grammar formulation of previous versions and is essentially \nspecifying the syntax of their language using an im\u00adperative program! Hence,in ordertobetter serve the \nprogramming language com\u00admunity and, more importantly, the legions of other programmers who actually \nwrite most of the parser code in the world, we need substantial improvements in the power and .exibility \nof parser gen\u00aderation technology. 1.1 Towards full context-free grammars Therehavebeenanumberof recenteffortstobuild \nparser genera\u00adtors that support unrestricted context-free grammars[24,33,30,6].  In particular, McPeak \nhas made a number of convincing arguments for abandoning the constraints of LR [23]. From a theoretical \nper\u00adspective,one centralproblemwithLRisthatithaspoor composi\u00adtionality properties. One cannot necessarily \nde.ne languages L(A) and L(B) with separate grammarsandthen combinetheminanew grammar: S =(A | B). If \nL(A) and L(B) overlap, then S is am\u00adbiguous and the new grammar is notLR. In this case, the obvious strategy \nfor creatingagrammar for L(A).L(B) would be to com\u00adbine A with a grammar for the asymmetric difference \nL(B)/L(A) (or vice versa: take B with a grammar for the asymmetric differ\u00adence L(A)/L(B)). Unfortunately,LRis \nnot closed under set dif\u00adference [24] so combining the two languages by .nding the dif\u00adference could \nbe impossible! Even if the set difference remains in LR, .nding the appropriate way to express it requires \ndigging into and adjusting the de.nitions of A and/or B these sublanguages cannotbedeveloped independentlyina \nmodularfashion. In practice, working within the constraints of LR can be very dif.cult, as anyone who \nhas attempted to debug a shift-reduce or reduce-reduce con.ict can attest. Debugging such con.icts is \nnot only dif.cult for the initial programmer, but often results in grammars that are harder to read, \nunderstand and maintain. Conse\u00adquently,the grammarsloseagreatdealoftheirbene.tas documen\u00adtation. Moreover, \nnot all practical grammars are amenable to such techniques.For instance,CandC++ containanumberof inherent \nambiguities in their grammars. One troublesome example is that there is no way to determine whether (a)&#38;(b) \nis a bit-wise con\u00adjunctionorifitisa castof &#38;(b) to the type (a), without knowing whether a is a variable \nname or a type name. Hence, attempting toparseCusing an LR-based parseris only possibleby stepping completely \noutside the grammar speci.cation mechanism and hav\u00ading the parser communicate dynamic information about \nnew type namesbacktothelexer.Inafull context-free grammar speci.cation mechanism, such hacks can elegantly \nbe avoided simply by allow\u00ading ambiguities at parse time and disambiguating during semantic analysis \n[23].  1.2 Beyond context-free grammars Arobust parsergenerator for full context-free grammars may serve \nprogramming language researchers well,butif we lookbeyond our community, there is a huge market for substantially \nmore sophisti\u00adcated parsing technology. For example, in the web space, there is no more common task than \nmatching identical XML tags, but, of course, this is not a context-free task. In binary formats, it is \nextremely common to use data .elds that specify the lengths or sizes of other data .elds, another non-context-free \nfeature. In addition, the speci.cations of many systems data formats are made simpler by the presence \nof constraints, such as bounds on integer ranges, expressed as ar\u00adbitrary predicates over parsed data. \nIn summary, without moving aggressively beyond the bounds of context-free grammars, parser generators \nare nearly useless to systems developers,networking en\u00adgineers and web programmers. One .nal limitingfactorof \nstandard parser generatorsis the in\u00adability to incorporate useful legacy code and libraries into parser \nde.nitions.For instance, there areovera hundred commonly used date and time formats and consequently \nprogrammers have devel\u00adoped sophisticated libraries for dealingwith them. It is extremely useful for \nprogrammers to be able to incorporate such libraries di\u00adrectly into their parser speci.cations after \nall, handling dates and times correctly might well be the most dif.cult task in parsing some server log. \nIf programmers cannot incorporate their favourite li\u00adbraries, they may well reject the parser speci.cation \nmechanism altogether.  1.3 YAKKER:Ageneral solutionto modern parsingproblems In this paper, we present \nthe theory and design of a new, general\u00adpurpose parsing engine called YAKKER.YAKKER is designedtobe generalenoughtosolvetheparsing \nproblemsofthe modernworld, both inside and outside the programminglanguage community. In particular, \nit combines all of the following key elements in one universal platform: Full, scannerless context-free \ngrammars. We support unre\u00adstricted context-free grammars, making it easier to express the syntax of programming \nlanguages like C++. Scannerless parsers reduce the burden on both parser-generator users and implementors \nby avoiding the need for a separate lexing tool and description language [28], and theymake it easier \nfor users to combine sublanguages with different tokens or whitespace conventions [1].  Regular right \nsides. YAKKER uses regular expressions over ter\u00adminals and nonterminals for the right-hand sides of nontermi\u00adnal \nde.nitions. Regular right sides allow for very concise and readable de.nitions in manysituations. YAKKER \nhandles regu\u00adlar right sides directly, rather than desugaring them into a more restricted form (an inef.cient \ntranslation).  Variable binding. Our grammars allow variables to be bound to data read earlier in the \nparse and used to direct parsing that comes later.  Data-dependent constraints. Our grammars can include \narbi\u00adtrary predicates over parsed data. When combined with our variable binding functionality, this feature \nallows for easy ex\u00adpression of length .elds and other data-dependent parsing id\u00adioms.  Parameterized \nnonterminals.Parameterized nonterminals allow users to create libraries of convenient abstractions that \nare easily customized and highly reusable.  Inclusion of parser libraries. Arbitrary parser libraries \ncan be included in grammar speci.cations as blackboxes, so legacy parser libraries can be reused.  The \ncentral technical contribution of this paper is a compre\u00adhensive theory that explains how to combine \nthis powerful set of features into a single grammar formalism and to implement that grammar formalism \ncorrectly. The implementation strategy is car\u00adried outby translating our grammars intoa newkindof automa\u00adton, \nthe data-dependent automaton (DDA).Wegivea direct, non\u00addeterministic operational semantics to the data-dependent \nautoma\u00adton and prove that it implements our grammars correctly. Like its brethren, the .nite automaton \nand the pushdown automaton, our new automaton is an elegant, powerful abstraction in its own right and \na useful implementation-level tool amenable to formal analy\u00adsis, transformation and optimization. The \nlast piece of our theory is to show how to implement the nondeterministic semantics ef.\u00adciently by extending \nthe traditional Earley parsing algorithm [5]. We prove our parsing algorithm correct with respect to \nthe nonde\u00adterministic automaton semantics. By transitivity, we have a proof that it implements our high-level \ngrammar speci.cations correctly as well. In addition to these theoretical contributions, we illustrate \nthe importance, utility and generality of our new design with example grammar fragments that specify \na number of different sorts of lan\u00adguage paradigms. These paradigms are drawn from a wide variety of \ndomains ranging from complex programming language syntax to widely-used systemslogsto networking protocols.Wehave \nalso implemented a prototype of our system which is able to compile and run all of these examples. The \nrest of the paper is structured as follows. In Section 2, we introduce the featuresof YAKKER byexample. \nThen,in Sections3 and 4, we present the syntax and semantics of grammars and data\u00addependent automata, \nrespectively. Section5containsaproof that an automaton soundly and completely implements a grammar, assum\u00adingthatasmallsetof \nconditionsrelatingthegrammarand automa\u00adton are met. The section concludes witha straightforward compila\u00adtion \nof grammars into automata along with a proof that the result\u00ading automata satisfy the speci.ed conditions \n(thus guaranteeing the correctness of the compilation). Then, in Section 6, we present an Earley-style \nalgorithm for parsing with data-dependent automata andproveits correctness.In Section7we discuss relatedwork,and \nthen conclude in Section 8.  2. YAKKER by Example We now illustrate thekeyfeaturesofYAKKER throughexamples. \nRegular right sides In YAKKER, nonterminals are de.ned by regular expressions over the terminals and \nnonterminals of the grammar.Forexample,herewe de.netwo nonterminals: digit = 0 | 1 | 2 | 3 | 4 | 5 | \n6 | 7 | 8 | 9 number = digit digit* The .rst nonterminal, digit, is de.ned as an alternation of terminals \n(in bold), andit denotes the setof ASCII numerical characters.The second, number, is de.ned in terms \nof digit using concatenation and Kleene-closure. This familiar notation is standard for lexer generators,but \nuncommonin parser generators.We allowit for all language constructs. Thus a list of expressions can be \nwritten expr , expr(, expr)* Parser generators that do not support regular right sides force pro\u00adgrammers \nto de.ne such a list as a separate nonterminal, as in this excerpt from the OCAML implementation [20]: \nexpr_comma_list : expr_comma_list COMMA expr | expr COMMA expr This is more verbose and less readable \nthan the equivalent regular right side, a fact emphasized by the use of regular right sides to describeOCAML \nsyntaxin the user manual. When implemented properly, regular right sides are also ef.\u00adcient. Any context-free \ngrammar with regular right sides can be transformed into a grammar without them, however, this will add \nmanynonterminals to the language, and thus a great deal of extra structure to parse trees. Regular expressions \nare highly ambiguous, e.g., the expression (x * )(x * ) can be matched against an input of length n in \nn +1 ways. Compiling away regular right sides there\u00adfore results in highly ambiguous grammars, which \nare more ex\u00adpensive to parse. YAKKER handles regular right sides directly and ef.ciently, ignoring their \nambiguities via determinization, as is tra\u00additionalwithregularexpressions(cf Section4). Full context-free \ngrammars We support context-free grammars without restriction, including ambiguous grammars.Forexample, \nthe problem we mentioned in the introduction involving C s syntax can be avoided simply by using an ambiguous \ngrammar: typename = identi.er expr = &#38; expr ;address-of | expr &#38; expr ;bitwise conjunction | \n( typename ) expr ;cast | ( expr ) | identi.er | ... The grammar is ambiguous because typenames and identi.ers \nare identical. Of course, ambiguities such as these must be resolved at some point. One way would be \nto resolve them during the type\u00adchecking phase of the compiler, when all type names are avail\u00adable [23]. \nAnother possibility is to use one of the disambiguation techniques suggestedbyvandenBrand,etal.[32].Yet \nanotherway is suggested later in this section. Outside of programming languages, ambiguous grammars are \noften used as documentation.Forexample, manynetwork protocol message formats are speci.ed using grammars \n(in IETF Request For Comments, or RFCs), and we have found that these grammars are almost invariably \nambiguous. Our work allows direct expression of such grammars and pro\u00advides an way to implement them. \nWhen grammars are ambiguous, our generated parsers produce a parse forest of results. A com\u00adprehensive \nanalysis of techniques for choosing a single parse tree within the forest, while important, is beyond \nthe scope of this paper. Attribute-directed parsing Nonterminals can capture input sub\u00adstrings which \nthe programmer can use to direct the parser s be\u00adhaviour on the remainder of the input. This is commonly \nneeded when parsing systems formats, such as network protocol messages. For example, protocols that need \nto transfer binary data often do so using messages consistingof the lengthof the data followedby the \ndata. In the IMAP mail protocol [4] these are called literals, and their syntax is speci.ed in the RFC \n[25] as follows: literal8 = \" {\" number [\"+\"] \"}\" CRLF *OCTET ;; A string that might contain NULs. ;; \n<number> represents the number of OCTETs ;; in the response string. Here number is an ASCII representation \nof the length of the data, CRLF is a carriage return and linefeed, and OCTET is any 8-bit value.We canexpress \nthe syntaxof an IMAP literal directly using bindings and constraints: literal8 = { x:=number(+ | .)} \n{n :=string2int(x)} CRLF ([n > 0]OCTET{n:=n - 1})* [n =0] Here we bind the string parsed by number to \na variable x, convert x to a number n and use constraints (expressions within square brackets) and value \nbinding (assignments in braces) to express the RFC s side condition on the length directly in the grammar. \nOne way to implement this speci.cation would be to evaluate these bindings and constraints in a semantic \nanalysis following parsing,aswe suggestedfortheC example,above.However,this wouldbeveryinef.cient our \nparserwouldhavetotreatthelength abstractly, potentially promoting all possible lengths to its output. \nInstead, we evaluate constraints during parsing to prune ambigu\u00adities as soon as possible. We call this \nattribute-directed parsing, followingWatt [34]. Notably, attribute-directed parsing subsumes backreferences, \nwhich are commonly used in hand-written parsers based on regular expression libraries. Note also that \nattribute\u00addirected parsing is not the same as the simple attribute-processing parsing that occurs via \nsemantic actions in standard YACC-style speci.cations.YACC-style semantic actionsdonotproduceboolean \nvaluesthat directly dictate whetherastringisinthe languageofthe speci.ed grammar or not. Parameterized \nNonterminals The length+data idiom of IMAP literals is used in many systems formats.We have parameterized \nnonterminalsto support modular reuseof such idioms.Forexam\u00adple, we could de.ne a .xed-width string nonterminal: \nstringFW(n) = ([n > 0]CHAR8{n:=n - 1})* [n =0] This is an imperative de.nition, relying on the assignment \nn:=n - 1.We could alternatively use a functional style: stringFW(n) =[n =0]| [n > 0]CHAR8 stringFW(n- \n1) (The [n =0]case parses the empty string.) Our formalism, in principle, would allow an optimizing parser \ngenerator to convert tail calls of this form into Kleene-closures of the previous form. Parameters can \nbe used to thread parsing state through a parse to aid attribute-directed parsing.Forexample, ambiguitiesin \ntheC grammar can be pruned at parse time by passing a table of type identi.ers to the expr nonterminal: \n decls(s) = typedef type-expr x:=identi.er decls(insert(s.types,x)) | ... typename(s) = x:=identi.er[member(s.types,x)] \nexpr(s) = &#38; expr(s) ;address-of | expr(s) &#38; expr(s) ;bitwise conjunction | ( typename(s) ) expr(s) \n;cast | ( expr(s) ) | x:=identi.er[not(member(s.types,x))] | ... Scannerless parsing YAKKER does not \ndivide parsing into sepa\u00adrate scanning (lexing) and parsing phases. Manydata formats use hand-written \nparsers and were not designed with a two-phase pars\u00ading strategyin mind.Itwouldbedif.cultto writetwo-phase \nparsers forthese formats.Forexample,wehavefoundthatmanyofthefor\u00admats de.ned in RFCs using grammars require \ncontext-dependent tokenization. Scannerless parsers are often also useful in parsing program\u00adming languages. \nOne example is Javascript, which sometimes al\u00adlows semicolons to be omitted at the end of a line, e.g., \nafter a void return statement. Here is a simpli.cation that illustrates how we can handle thisin YAKKER: \nstatement = return (SP | TAB)* (LF | ; | identi.er ;) Another example is the use of indentation for block \nstructure, as in Python and Haskell (the offside rule [19]). This is context\u00addependent whitespace:it \nmustbe handled speciallybythelexerifit is at the beginning of a line within a block, and the block structure \nis only known by the parser. This forces the lexer and parser to be mutually recursive. For a .nal example, \nconsider a template language like PHP, which mixes the syntax of twolanguages, HTML and the PHP con\u00adtrol \nlanguage.Thelanguageshavedifferentkeywordsand comment syntax, which again leads to context-dependent \nlexing. Blackbox parsers Support forthe full range of context free gram\u00admars (and beyond) allows us to \nseamlessly integrate support for foreign parsers, or blackboxes, into our formalism, which can be essential \nfor parsing real-world languages and data formats. For example, consider these two sample lines from \na web server log, which include a complex date .eld (the lines are broken with a \\ to .t). 207.136.97.49 \n--[15/Oct/1997:18:46:51 -0700] \\ \"GET /tk/p.txt HTTP/1.0\" 200 30 tj62.aol.com --[16/Oct/1997:14:32:22 \n-0700] \\ \"POST /scpt/dd@grp.org/confirm HTTP/1.0\" 200 941 Here is a grammar fragment describing this \ndata, where the date .eld is described with a blackbox nonterminal, bbdate. SP is a nonterminal de.ned \nas the space character; we omit the de.nitions of the other nonterminals. blackbox bbdate entry = client \nSP auth-id SP auth-id SP [ bbdate ] SP request SP response SP length In our formalism, blackbox nonterminals \ncome from a different namespace than other nonterminals.We indicate this here with the blackbox bbdate \ndeclaration. While blackboxes appear to be a simple, easy-to-understand feature, it may not be obvious \nhow to integrate them correctly with agiven parsing algorithm. In our case, we are using an Earley-style \nparsing algorithm, which extends multiple possible parses by a single characterateachstep,ina breadth-.rst \nstrategy.Ablackbox that consumes an arbitrary number of input characters in a single step does not .t \nin neatly. Compositionality Our grammars are closed under union, con\u00adcatenation, and Kleene closure.We \nlearned about the importance of these compositionality properties from our experience with the PADS family \nof languages [7, 8, 22]. PADS is a domain-speci.c language that interprets specialized type declarations \nas grammars. These type declarations can be used to generate parsers and print\u00aders as well as a variety \nof useful end-to-end data processing tools. PADSgrammars are super.cially similar to thegrammars presented \nin this paper they contain regular expressions, variable binding, constraints, parameterized de.nitions \nand blackbox parsers. How\u00adever, PADS grammars have a different semantics that makes them easyandef.cientto \nimplementusingatop-down, recursivedescent parsing algorithm with limited backtracking. In particular, \nif, when parsing the union (A | B), PADS succeeds in parsing an A, it will committo that choice and never \nbacktrack,even whendownstream errors arise that could be avoided if the input was interpreted as a B. \nPEGs, a related grammar speci.cation language analyzed by Ford[10,11],and implementedef.cientlybyGrimm[12],hassim\u00adilar \nexpressive power and semantics. Unfortunately, the PADS semantics has undesirable conse\u00adquences in certain \napplications because closure under union and concatenationfail:1 L(A | B) = L(A) . L(B) L(AB) = L(A) \nL(B) Both of these principles are required for the correct functioning of divide-and-conquer grammar \ninduction algorithms, including algo\u00adrithms designed to infer PADS descriptions [9]. The PADS gram\u00admar \ninduction algorithms attempt to avoid learning incorrect gram\u00admars by using various heuristics, but the \nheuristics are not al\u00adways successful the algorithms dofail occasionally on real data sources. More generally,these \nprinciples are essential for modular gram\u00admar design. If theyhold, programmers can develop sublanguages \nA and B in isolation and then, without further modi.cation, perform natural operations such as union \nor concatenation and receive the expected resulting semantics. 3. Grammars and Languages We now specify \nthe formal syntax and semantics of grammars. We assume a simple, untyped language of expressions(e) that \nincludes variables(x, y, z, etc.), boolean values true and false, the unit value (), and string values \n(sequences of terminals) w. An environment E ::= \u00b7| E[x:=v] is a .nite partial map from variables tovalues.We \nwrite E, E' for the concatenation of two environments. Bindings to the right take precedence over bindings \nto the left (an update semantics). We write [ e] E to denote the valuev that results fromevaluatingexpression \ne in environment E. Evaluation is assumed to be a function of the environment E and expression e (and \nhence cannot depend upon anyimplicit state). De.nition1 Agrammar G is a tuple (S, ., F,A0, R) where \nS is a .nite set of terminals;  . is a .nite set of nonterminals;  F is a .nite set of blackboxes; \n A0 . . is the start nonterminal; and  1Ford [11] provesPEGs, which are related to PADS, are closed \nunder union, but only under a non-standard de.nition of what it means for a string to be ina language(the \nPEG needonly succeedona pre.x of the string).  E . w . r . E ' GL-EPS E . . . . .\u00b7 GL-TERM E . c . c \n.\u00b7 [ e] E = true GL-PRED E . . . [e] .\u00b7 [ e] E = v GL-BIND E . . . x:=e . [x:=v] [ e] E = vw . F(f)(v) \nGL-f E . w . f(e) .\u00b7 [ e] E = v R(A)= r [y0:=v] . w . r . E ' GL-A E . w . x:=A(e) . [x:=w] E . w1 . \nr1 . E1 E, E1 . w2 . r2 . E2 GL-SEQ E . w1w2 . (r1 r2) . E1,E2 E . w1 . r1 . E1 GL-ALTL E . w1 . (r1 \n| r2) . E1 E . w2 . r2 . E2 GL-ALTR E . w2 . (r1 | r2) . E2 E, E1,...,Ei-1 . wi . r . Ei, for i =1 to \nk GL-* E . w1 ...wk . (r * ) . E1,...,Ek Figure 1. The string-inclusion judgment for rules. R maps nonterminals \nto regular right sides (de.ned below). We use A, B, C to range over nonterminals, a, b, c to range over \nterminals, and f to range over blackbox parsers. The empty sequence is written ..We use r to range over \nthe (regular) right sides, de.ned as follows: r ::= . the empty string | empty the empty language | c \nterminal | x:=A(e) nonterminal | x:=e binding | (r r) concatenation | (r | r) alternation | (r * ) Kleene-closure \n| [e] constraint | f(e) blackbox We reserve a distinguished variable, y0, for use as the formal parameter \nof nonterminals, and require thaty0 not appear as the x in x:=A(e) or in x:=e in a regular right side. \nIf R(A) contains y0 we say that A is parameterized, otherwise, A is unparameterized. We require the start \nnonterminalA0 to be unparameterized. For brevity, we may write A(e) in place of x:=A(e) in a right side \nwhen this is the only occurrence of x in the right side. Similarly, we write A in place of A(e) in right \nsides when A is unparameterized. Figure1de.nes the judgment E . w . r . E ', which says that string w \nbelongs to the language of a right side r in environ\u00adment E, and produces new bindings E '. One of the \nsimpler rules is GL-TERM, which states that the string c is in the language of the right side c in anyenvironment \nE and produces no new bind\u00adings. RuleGL-f shows that we model blackboxesabstractly as lan\u00adguage recognizers, \nparameterized by a value from the expression language.Asomewhat more interesting ruleisGL-A, which states \nthat w is in the language of x:=A(e) provided that e evaluates to v and w is in the right side of A evaluated \nwith an environment mapping y0 to v, and no other bindings. This rule also produces the binding [x:=w]. \nRule GL-SEQ shows how to process a concate\u00adnationexpression noticethewayit threadsenvironments through \nthe rule from one part to the next. The language of a nonterminal A is then de.ned as follows. LA = .y.{w \n| [y0:=y] . w . r . E, R(A)= r} The language of a grammar G is just the language of its start nonterminal: \nLG = LA0 (). Then our grammars have thedesired compositionality properties: Theorem1(CompositionalityProperties) \nLet G1 and G2 be grammars with disjoint variables and nontermi\u00adnals and letA0,1 and A0,2 be the start \nnonterminals of G1 and G2. It is possible to construct three new grammars with the following properties. \ni. If G ' is the union of G1 and G2 with a new start nonterminal de.ned as (A0,1 | A0,2) then L(G ' )= \nL(G1) . L(G2). ii. If G ' is the union of G1 and G2 with a new start nonterminal de.ned as (A0,1 A0,2) \nthen L(G ' )=(L(G1) L(G2)). iii. If G ' is G1 with a new start nonterminal de.ned as A * 0,1 then L(G \n' )= L(G1) * . The compositionality properties follow immediately from the rules of Figure 1. The following \ncorollary is a direct consequence. Corollary2(ClosureProperties) If L1 and L2 are languages of grammars, \nthen (L1 . L2), (L1 L2), and L * 1 are languages of grammars.  3.1 Additional context sensitivity We \ncould greatly increase the power of our system with one sim\u00adple extension: allow the start nonterminal \nof a grammar to take a parameter, and pass in as its value the complete input. This would essentially \nmake the complete input available to all nonterminals, constraints, and blackboxes. This would enable \nmany useful fea\u00adtures, e.g., matching the beginning or end of input. Our parsing algorithm and most theoretical \nresults could ac\u00adcommodate this extension, with the exception of our performance bounds and, of course, \nour Compositionality Properties (which re\u00adlied crucially on start nonterminals being unparameterized). \n4. Data-dependentAutomata One of the beauties of parsing theory is that high-level grammati\u00adcal concepts \ncan often be implemented in terms of lower-level au\u00adtomata, which are themselves useful abstractions \nfor programmers and compiler implementers. In this section, we present a new kind of automaton that can \nimplement our data-dependent grammars. Our new automata are technically transducers that is, theydo notonly \nrecognizeaninput,butalsoproduce outputs,althoughonly from .nal states. Theyextend the transducers of \nJim and Mandel\u00adbaum [15], which were created to parse context-free grammars with regular right-hand sides.Jimand \nMandelbaum sideawasto encode regular right sides as subautomata within the transducer, to explic\u00aditly \nlabel calls from one nonterminal s automaton to another s, and to explicitly label .nal states with the \nname of the nonterminal be\u00ading completed. Constructing transducers in this way gives them three essential \ncharacteristics. First, they allow for left-factoring of grammar alternatives, which makes it possible \nto reduce the nondeterminism that will arise during parsing. Second, theyallow for direct implementation \nof regular expressions as traditional au\u00adtomata, rather than through desugaring into a more restricted \nform of context-free grammar. Such a direct implementation has a sig\u00adni.cant impact on parsing ef.ciency \nby reducing stack activity. Third, the transducers support left-recursion programmers are not forced \nto write their grammars with repetition operators, as is the case for PEGs [11] and manyparser combinator \nlibraries [14].  In this section, we extend the core ideas found in Jim and Man\u00addelbaum sworkbyshowinghowtoadd \nsupportfor binding, depen\u00addency,constraints, parameters and blackboxes.Wegiveanondeter\u00administic operational \nsemantics for transducers containing this rich set of features. While this semantics could be used to \nimplement the transducer directly with a backtracking, depth-.rst parser, we donotdo so Section6presentsa \nbreadth-.rst algorithm basedon ideas drawn from Earley s parsersfor context-free grammars [5]. However, \nbefore presenting any of these technical details, we ex\u00adplain the high-level ideas through the use of \nseveral simple exam\u00adples. 4.1 Transducersby example Figure2shows threeexample transducers alongwith their \nsource grammars.Figure2(a) de.nesa.xed-widthinteger,withthewidth speci.ed as a parameter to the nonterminal \nnamed int. In this picture, some edges (such as the edge between states2and3) are labeled with terminal \nsymbols. These edges can be interpreted in the ordinary way: a transition is enabled when the current \ninput symbol matches the label of the edge. There are also predicate\u00adlabeled edges (with square brackets \nenclosing the predicates) the edge between states 1 and 2 is a predicate edge. A transition is enabled \nalong a predicate edge when the predicate evaluates to true in the current environment. The edges with \nlabels enclosed in curly braces are assignment edges traversing the edge from3 to1assigns n - 1 to n. \nFinal states are marked by double-circles and are labeled with a set of nonterminals; in this case, the \nsingle nonterminal int. Whenevaluation reachesa.nal state,it indicates completion of all the nonterminals \nlabeling the state. Figure 2 (b) contains a grammar and transducer for the same languageas Figure2(a),expressedina \nfunctional style. Whereas the previous example made no use of the stack, this example will build a stack \nof depth equal to the value of the argument of int. Stack frames are pushed (saving the current environment \nand call\u00ading state) at each call transition. Stack frames are popped upon arrival at .nal states.For \ninstance, imagine the transducer takes the call transition between states2and5andthenreadsadigittoarrive \nat the state labeled dig. At this point, control will return to state 2, pop the stack (reinstalling \nthe saved environment) and take the transition between states2 and 3. The 2 3 transition is taken be\u00adcause \nit is labeled dig the same nonterminal found in the .nal statethe transduceris returning from.Aslightly \nmore sophisticated call pattern occurs between states3and1.In this casea parameter is passed to the callee. \nNotice that the 3 1 edge is matched by an\u00adother edge leading from3labeled int(n-1). The int part of the \nlabel indicates this edge supports returns from states labeled with nonterminal int. The n-1 part of \nthe label indicates it also re\u00adquires that the parameter passed to the call from which a return is made \nis equal to n-1. Finally, Figure 2 (c) demonstrates the ability to represent left factoring ef.ciently \nand the utility of .nal states labeled with mul\u00adtiple nonterminals. In this transducer, the call transition \nfrom states 1 to 4 is coupled with two return transitions from 1. One return transition is labeled with \nnonterminal B and one with nontermi\u00adnal C. Intuitively, the single call along the 1 4 edge implements \na parser for bothB andC.Now,executionof the transducer can, if the input matches x-x, proceed from state4 \nto states5 then6 and .nally to the state labeled C. In this case, we have found a parse forConly and \nupon return can transition only from state1 to state 3. Alternatively, if the input matches x+x, execution \nwill proceed from4 to5 to7 to the B,C state. In this case bothBand Cnonterminals have been parsed simultaneously \nand transitions to either2 or3may be taken from state1upon return. The ability to optimize automata by \nmerging states and to parse multiple nonter\u00adminals simultaneously results in substantial practical performance \ngains [15].We retain this important feature despite theextensions requiredby data-dependent grammars. \n 4.2 Trees Our transducer semanticsbuilds parse trees. De.nition2(ParseTrees) AtreeT is a sequence of \n terminals c,  bindings {x:=v},  blackbox strings (w), or  four-tuples x:A(v)(T ') representing a \nparse of a nonterminal A applied to value v and with leaves bound to x.  De.nition3(Subtrees) T1 is \na subtree of T at depth n iff 1. T = T0 T1 T2 and n =1, or 2. T = T0 x:A(v)(T ') T2, and T1 is a subtree \nof T ' at depth n - 1.  We useW to range over abstract strings:elements of the set (c |{x:=v}|(w)| x:A(v):=w) \n* . We will discuss abstract strings further in Section 5.We usem to range over abstract strings that \nare nonterminal-free: (c |{x:=v}|(w)) * . We de.ne anerasure function, ||\u00b7||, from abstract strings W \nto strings w as follows: ||.|| = . ||cW || = c||W || ||{x:=v}W || = ||W || ||(w)W || = w||W || ||x:A(v):=wW \n|| = w||W || The following de.nition provides a way to refer to the concrete strings embedded in abstract \nstrings. De.nition4(String membersof an abstract string) Strings(W, A, v)= {w | x:A(v):=w is a substring \nof W }. Two additional concepts, theleaves of a tree and the roots of a tree are central in the following \ntechnical development. Intuitively, the leaves of a tree is the string of terminals that were parsed. \nThe roots ofa treeisthe top-level, abstract stringof symbolsinthe tree. De.nition5(Leaves and Roots) \nThe leaves and roots of a tree that consists of a single nonterminal\u00adfree abstract string are: leaves(m)= \n||m||  roots(m)= m  The leaves and roots of a tree that consists of a single nonterminal element are: \n leaves(x:A(v)(T ))= leaves(T ).  roots(x:A(v)(T ))= x:A(v):=leaves(T ).   int(n) = ([n > 0](0 | \n... | 9) dig = 0 | 1 | ... | 9 A=(B ?)| (C !) B= x+x (a) (b) (c) {n:=n - 1})* [n =0] int(n) =[n =0]| \n[n > 0]dig int(n- 1) C=(x+x)| (x-x) Figure 2. Transducers for (a) an imperative speci.cation of a .xed-width \ninteger, (b) a functional speci.cation of a .xed-width integer, and (c)a(very) simpleexpression languagedemonstrating \nleft-factoring. Final states are labeled with nonterminals, rather than state numbers. ' '' (q, E, T, \nr)::tl . (q ,E ' ,T ,r )::tl ' c r . s S-TERM (q, E, T, r)::tl . (q, E, T c, s)::tl e r -. s [ e] E = \ntrue S-PRED (q, E, T, r)::tl . (q, E, T, s)::tl x:=e r -. s [ e] E = v (x = y0) S-BIND (q, E, T, r)::tl \n. (q, E[x:=v],T {x:=v},s)::tl f(e) r -. s [ e] E = vw . F(f)(v) S-f (q, E, T, r)::tl . (q, E, T (w),s)::tl \ncall(e) r -. s [ e] E = v S-CALL (q, E, T, r)::tl . (s, [y0:=v], ., s)::(q, E, T, r)::tl x:=A(e) r . \nAr ' -. s [ e] E ' = v = E(y0)(x = y0) S-RETURN ' '' (q, E, T, r)::(q ,E ' ,T ,r )::tl . (q ' ,E ' [x:=leaves(T \n)],T ' x:A(v)(T ),s)::tl Figure 3. The stack evaluation relation. current environment. It is followed \nby the parse tree(T )under construction. The last elementof the tuple(r)is the current state. Figure3de.nes \n., a single-step evaluation relation for con.g\u00adurations. The simplist rule is S-TERM, which extends the \ncurrent tree with a terminal based on a transition on that terminal appear\u00ading in the transducer. S-CALL \nand S-RETURN manage the stack. RuleS-CALL transitionsto another statemuchlikea functioncall. Anew stacktupleispushedwiththe \ncalleeasthe current state,an empty tree, and an environment that contains only the call argu\u00adment bound \nto y0.To guarantee access to the call argument at any point during evaluation, we ensure that y0 is not \nupdated with the side condition x = y0 on rulesS-BINDING andS-RETURN. Rule S-RETURN is invoked whenever \na .nal state is reached. The stackispopped and anytransition x:=A(e) from the previous current-state \nr ' can be followed, as long as e evaluates to the call argument of the current tuple. Also, the string \nparsed by A(e), leaves(T ), is bound to x in the environment. The leaves and roots of an arbitrary tree \nare computed by concate\u00adnating the leaves and roots of its sequence ofelements. We now state some simple \nproperties of roots and leaves that follow from these de.nitions. Lemma1(The Leaf Lemma) If T = m0 x1:A1(v1)(T1) \nm1 ...xn:An(vn)(Tn) mn then leaves(T )= w0 w1 ' w1 ...w ' wn, where wi = ||mi|| and n wi ' = leaves(Ti), \nfor 0 = i = n. Proof: from the de.nition of the leaves function. Lemma2(The Root-Leaf Lemma) ||roots(T \n)|| = leaves(T ). Proof: from the de.nition of roots(T ), T must have the form m0 x1:A1(v1)(T1) m1 ...xn:An(vn)(Tn) \nmn,withleaves(Ti)= wi' . The result then follows from the de.nition of the erasure func\u00adtion and the \nLeaf Lemma.  4.3 Semantics gcall(e) Aparsing transducer T isatuple (S, ., Q, F,A0,q0, ., -. , .) where \n S is a .nite set of terminals;  . is a .nite set of nonterminals;  Q is a .nite set of states;  F \nis a a .nite set of blackboxes;  A0 . . is the start nonterminal;  q0 . Q is the initial state;  g \n. is the transition relation, where g has one of the following forms: c (terminal), e (constraint), x:=e \n(binding), f(e) (black\u00adbox), x:=A(e) (nonterminal); call(e) -. . Q \u00d7 O \u00d7 Q is the call relation, where \nO is the set of expressions from the expression language; and  . . Q \u00d7 . is the output relation from \n.nal states to nonter\u00adminals.  We write the re.exive and transitve closure of the transition relation \nas r. * s. When we wish to refer to the labels of the P constituent transitions, we write . *, where \nP . g *. We use q, r, s, t, u to range over states. A con.guration is a 5-tuple (q, E, T, r) :: tl where \ntl acts as a stack that grows to the left. The .rst elementof the tuple(q)is thecallee at which the parse \nof the current nonterminal(s) began. While the callee does not in.uence the parsing process, its inclusion \nsimpli.es the task of proving that the Earley-style parsing algorithm in Section6 preserves the semantics \nof transducers. The next element of the tuple(E)is the  The multi-step evaluation relations, . * and \n.+, are de.ned as usual. Notice that these relations de.ne a nondeterministic algo\u00adrithm for parsing \nwith the transducer. Wecomplete this sectionbynoting that just as we can talk about nonterminal languages \nin the grammar, we can describe nontermi\u00adnal languagesin the transducer.However,given that the transducer \nmighthave multiple callee statesforanygiven nonterminal,wede\u00adscribethe nonterminal languageswith respecttoa \nparticular callee. De.nition6 We characterizeLA(q), the language of A at q, as LA(q)=. y.{w | (q, [y0:=y], \n., q) . * (q, E, T, r), r . A, w = leaves(T )} and the language of the transducer LT = LA0 (q0)(). 5. \nGrammars andTransducers Before we present our Earley-style parsing algorithm, we will prove that data-dependent \nautomata are powerful enough to parse the languagesof data-dependent grammars.Wedosointwo steps. First, \nwe identify suf.cient conditions for a given transducer to parse the languageofagiven grammar. Then, \nwe presenta transla\u00adtion from grammars to transducers that satis.es the conditions. 5.1 Abstract languages \nOur strategy for relating grammars to transducers is to reduce the problem to one of independently comparing \nsubautomata within the transducer with right-hand sides in the grammar, without ref\u00aderence to the remainder \nof the grammar and transducer. In Sec\u00adtion 4.2, we de.ned abstract strings W , which contain symbolic \nel\u00adements, like nonterminals, in addition to terminals. Here, we de.ne abstract languages sets of abstract \nstrings for grammar nonter\u00adminals and transducer callees.We then compare subautomata with right-hand \nsides via their respective abstract languages and show how equivalence of these abstract languages implies \nequivalence between a grammar and transducer. In essence, abstract languages are de.ned as the abstract \nstrings that can be read directly from right-hand sides and transducer subautomata, without recurring \non nonterminals.Forexample,in the following grammar, A = B | CB = bC = c the (concrete) language of A \nis the set {b, c} whereas the ab\u00adstract language is the set {B, C}. The difference is that the latter \nmakes no mention of the languages of B and C. For grammars with bound nonterminals, the abstract language \nis a little more in\u00advolved. Speci.cally, each bound nonterminal in an abstract string W is represented \nas x:A(v):=w that is, it carries an associated concrete string along with it. The strings w must be included \nbe\u00adcause evaluating W s membership requires a value to be bound to x for use in evaluating the remainder \nof W . However, the essential abstraction is that we do not force that value to be a valid member of \nthe language of A(v). Although constraints in the language can constrain the range of possible associated \nstrings, theydo not nec\u00adessarily constrain them to be drawn strictly from the language of the respective \nassociated nonterminal. In Figure 4, we de.ne the abstract-stringinclusion judgment for grammars, which \nwe can then use to de.ne the abstract language of a nonterminal. De.nition7(Grammar Abstract Language) \nThe abstract language of nonterminal A is GA = . y.{W | [y0:=y] . W . r . E, R(A)= r} E . W . r . E ' \nAGL-EPS E . . . . .\u00b7 AGL-TERM E . c . c .\u00b7 [ e] E = true AGL-PRED E . . . [e] .\u00b7 [ e] E = v AGL-BIND \nE .{x:=v}. x:=e . [x:=v] [ e] E = vw . F(f)(v) AGL-f E .(w). f(e) .\u00b7 [ e] E = v AGL-A E . x:A(v):=w . \nx:=A(e) . [x:=w] E . W1 . r1 . E1 E,E1 . W2 . r2 . E2 AGL-SEQ E . W1W2 . (r1 r2) . E1,E2 E . W . r1 . \nE1 AGL-ALTL E . W . (r1 | r2) . E1 E . W . r2 . E2 AGL-ALTR E . W . (r1 | r2) . E2 E,E1,...,Ei-1 . Wi \n. r . Ei, for i =1 to k AGL-* E . W1 ...Wk . (r * ) . E1,...,Ek Figure 4. String-inclusion rules for \nabstract languages of nonter\u00adminals. In Figure 5, we present a judgment relating abstract strings W to \npaths through the transducer. We also de.ne its re.exive and transitive closureintheexpectedway.We cannow \nde.nethe abstract language of a nonterminal at a callee. De.nition8(Transducer Abstract Language) The \nabstract language of nonterminal A at callee q is TA(q)= . y.{W | [y0:=y] . W : q . * s; E and s . A} \n 5.2 Transducer conditions Perhaps the key element of the proof that transducers implement grammars \nproperly is the de.nition of the conditions (T0), (T1) and (T2) that relate the language of a grammar \nG to the language implemented by a transducer T . Their intuition is as follows. (T0) states that the \nlanguage of the transducer starting at q0 (the start state) and .nishing at a .nal state for A0 (the \nstart nonterminal) must be the same as the language of A0 in the grammar. In a nutshell, this condition \nstates that the transducer implements the start nonterminal correctly. x:=A(e) An A(e)-edge is anyedge \nwith the form r -. s.With this in mind, (T1) states that anyA(e)-edge from a state r must be coupled \nwith a call edge from r with parameter e. Moreover, that call edge must transition to a state q that \nimplements the language of nonterminal A. In a nutshell, this condition states that the transducer contains \nsome call that implements each A(e)-edge correctly.  E . W : q . r; E ' c q -. r ATL-TERM E . c : q \n. r; \u00b7 e q -. r [ e] E = true ATL-PRED E . . : q . r; \u00b7 x:=e q -. r [ e] E = v (x = y0) ATL-BIND E .{x:=v} \n: q . r;[x:=v] f(e) q -. r [ e] E = vw . F(f)(v) ATL-f E .(w) : q . r; \u00b7 x:=A(e) q -. r [ e] E = v (x \n= y0) ATL-A E . x:A(v):=w : q . r;[x:=w] E . W : q . * r; E ' E . . : q . * q; \u00b7 E . W1 : q . * r ' ; \nE1 E,E1 . W2 : r ' . r; E2 E . W1W2 : q . * r; E1,E2 Figure5. Abstract string inclusion for transducers \nand its re.exive and transitive closure. A callee is either the start state q0 or anystate that has a \ncall edge leading to it. Hence, (T2) states that for each nonterminal A, the language of A at every callee \nis a subset of the language of A in the grammar. In a nutshell, the transducer contains no calls thatputextragarbage \nintothe languageofanynontermi\u00adnal. Formally, these conditions are speci.ed as follows. De.nition9 T is \na transducer for G iff (T0) TA0 (q0)() = GA0 (). x:=A(e) call(e) (T1) If r -. s then r -. q, for some \nq with TA(q)(v)= GA(v), for all v. (T2) If q is a callee, then TA(q)(v) . GA(v), for all A, v.  5.3 \nGrammar-transducer correspondence In our .rst few lemmas, we investigate some properties of stack executions. \nOur .rst result, the Backup Lemma, shows that if there is an execution path resulting in a particular \nstack, then there is a (shorter) execution path to any of the substacks within it. It is used frequently \nin the S-RETURN case of inductive proofs before applicationofthe inductionhypothesis.We usethe notation \n.+ to indicate that at least one step was taken. Lemma3(The Backup Lemma) If tl1 .+ (r1,E1,T1,s1)::(r2,E2,T2,s2)::tl \nthen a. tl1 . * (r2,E2,T2,s2)::tl in fewer steps, and call(e)  b. s2 -. r1 and [ e] E2 = E1(y0), and \n c. (r1, [y0:=E1(y0)], ., r1)::(r2,E2,T2,s2)::tl . * (r1,E1,T1,s1)::(r2,E2,T2,s2)::tl, in equal or fewer \nsteps. Proof: by induction on the number steps taken. Rule S-RETURN is the interesting case, where we \napply the induction hypothesis twice, each time removing the top stack element. Corollary4 If tl1 .+ \n(r1,E1,T1,s1)::(r2,E2,T2,s2)::tl then (r2,E2,T2,s2)::tl . * (r1,E1,T1,s1)::(r2,E2,T2,s2)::tl, in equal \nor fewer steps. Proof:by parts(b)and(c)ofthe Backup LemmaandruleS-CALL. Our next lemma generalizes the \nthird part of the Backup Lemma to show that for any execution there is a subexecution starting at the \ncallee .eld of the top stack element, with an environment containing only y0 and an empty tree. Corollary5(Start-at-callee) \nIf (q, E0, ., q) . * (q1,E1,T1,s1)::tl and y0 . dom(E0) then (q1, [y0:=E1(y0)], ., q1)::tl . * (q1,E1,T1,s1)::tl. \nProof:Byinductiononthenumberofstepstaken,usingtheBackup Lemma for the caseof ruleS-RETURN. Next, show \nthat the callee .eld is true to its name that is, it is always populated by a transducer callee. Lemma6 \nIf q is a callee and (q, E0, ., q) . * (q1,E1,T1,s1)::tl then q1 is a callee. Proof:by induction on numberof \nsteps taken. We follow with simple result showing thaty0 is immutable, as desired: Lemma7(y0 Immutability) \nIf (q, E, T, r)::tl . * (q, E ' ,T ' ,r ' )::tl then E(y0)= E ' (y0). Proof:by induction on the numberof \nsteps taken. The Pre.x Lemma, below, is helpful for reasoning about a subcomponent of a tree. Lemma8(ThePre.x \nLemma) If (q, E0, ., q) . * (q, E, T1T2,r), then . s, E ' . (q, E0, ., q) . * (q, E ' ,T1,s) and dom(E \n' ) . dom(E). Proof:by induction on height . * derivation. We will now present a theorem showing that \nresults about stack evaluationsin one context canbe appliedin other contexts.We sin\u00adgle it out as a theorem \nbecause it also provides intuition regard\u00ading the extent to which parsing is context sensitive. Speci.cally, \nit showsthatthe context-sensitivityofaparseis strictly limitedtothe environment in the top stack tuple. \nAnything outside of that envi\u00adronment, located in the stack below, cannot affect parsing at higher stack \nlevels. This theorem plays an important role in the proofs of nearly all of the lemmas and theorems that \nfollow. However, its correctness isfar from obvious and relies on the fact that if one were to attempt \nto pop a series of stack frames and then push them back on, one would not arrive back in exactly the \nsame state, because the embedded parse treeswould grow.We prove this property .rst, before stating the \ntheorem. Note that the trickyaspectofthis lemmawas.ndingan appropriate statementof the desired property.We \nuse the notation @ to represent the append function on stacks. Lemma9(No Stack Repeat) If tl1@(q1,E1,T1,r1)::tl \n. * (q2,E2,T2,r2)::tl,wheretl1 is not the empty stack, then length(T2) > length(T1).  Proof: by induction \non height . * derivation. Notice that the case of rule S-CALL is impossible, because application of the \ninduc\u00adtionhypothesis leadstoa contradiction.ForruleS-RETURN,itei\u00adther follows immediately from the premises, \nor from the Backup Lemma and the inductionhypothesis. Theorem 10 (Context Independence) If (q, E, T, \nr)::tl . * (q, E1,T1,r1)::tl then . tl ' . (q, E, T, r)::tl ' . * (q, E1,T1,r1)::tl ' Proof: by induction \non . * derivation.We use Lemma9to show that the caseof ruleS-CALL is inapplicable. The following lemmas \ncomplete the statement of correspon\u00addence between trees and stack evaluations. In essence, they state \nthat every subtree within the tree produced by an evaluation has some corresponding sub-evaluation. Lemma \n11 (The Direct-Subtree Lemma) If (q, E0, ., q) . * (q, E, T0 x:A(v)(T ),r) and y0 . dom(E0) then there \nexist e1,e2,E1,r ' , s, t, u such that call(e1) x:=A(e2)  s -. t, s -. r ' , u . A, and [ e1] E1 =[ \ne2] E1 = v, and  (q, E0, ., q) . * (q, E1,T0,s), and  (q, [y0:=v], ., t)::(q, E1,T0,s) . * (t, E2,T,u)::(q, \nE1,T0,s),  and ' . * rr. Proof: by induction on . * relation, using the Backup Lemma and Lemma 5 for \ncase of rule S-RETURN. The only other applicable case is rule S-PRED, which follows directly from the \ninduction hypothesis. Lemma 12 (The Subtree Lemma) If q is a callee, y0 . dom(E0), (q, E0, ., q) . * \n(q, E1,T,r), and x:A(v1)(T1) is a subtree of T at depth n, then there exists a stack tl, callee s and \nstate t . A such that (s, [y0:=v1], ., s)::tl . * (s, E2,T1,t)::tl. Proof:by induction on thedepthof \nT1.Fora treeof depth1,where T = T0 x:A(v)(T1) T2 we use the Pre.x Lemma to remove T2 and then apply the \nDirect-Subtree Lemma. The inductive case follows a similar approach. T1 must be a subtree of some direct\u00adsubtree \nT '.We use the Pre.x and Direct-Subtree Lemmas on T ' to satisfy the premises of the inductionhypothesis. \nThe only twist is that we use the Context-Independence Lemma to remove the stack before applying the \ninductionhypothesis. Next, we seek to establish the relationships between our con\u00adcrete and abstract \nlanguages in a series of lemmas. These lemmas are essential to our end goal because we are seeking to \nshow that correspondence between a grammar and transducer at the abstract level is enough to ensure correspondence \nat the concrete level.To do so, we need to understand the relationship between concrete and abstract \nlanguages for grammars and transducers, respectively. First, we show that the presence of a concrete \nstring in the concrete language of a nonterminal implies the existence of some corresponding abstract \nstring in the abstract language of that same nonterminal. Next, we show that the opposite holds, under \nthe important constraint that every concrete string within the abstract string actually belongs to language \nof the nonterminal with which it is associated. Lemma 13 (Grammar Concrete-Abstract Correspondence) If \nE . w . r . E ' then there exists W such that w = ||W ||, E . W . r . E ', and, for all w1 . Strings(W, \nA, v), [y0:=v] . w1 . r1 . E1 with a lower derivation height (where R(A)= r1). Proof:By induction on \npremise derivation. Lemma 14 (Grammar Abstract-Concrete Correspondence) If E . W . r . E ' and Strings(W, \nA, v) . LA(v) then E . ||W || . r . E ' . Proof: By induction on the height of the .rst derivation. The \ncase of ruleAGL-Auses the second premise. Lemma15(Transducer Concrete-Abstract Correspondence) If (q, \nE0, ., q) . * (q, E, T, r) then E = E0,E1 and E0 . roots(T ): q . * r; E1. Proof: by induction on the \n. * relation, usingthe Backup Lemma (Lemma3)in theS-RETURN case. Below, we de.nea predicate on transducers \nand abstract strings which corresponds to the constraint Strings(W, A, v) . LA(v) from Lemma 14. This \nde.nition is used in the statement of Lemma 17. De.nition 10 x:=A(e) Ok(T ,W ) iffr -. s implies that \nthere exists a callee t such call(e) that r -. s and Strings(W, A, v) . LA(t)(v). Lemma 16 (Ok W -Subcomponents) \nIf Ok(T ,W1W2) then Ok(T ,W1) and Ok(T ,W2). Proof:By inspectionof structureof W and de.nition of Ok. \nLemma17(Transducer Abstract-Concrete Correspondence) If q isastate, E . W : s . * r; E ',andOk(T ,W ),thenexistsT \n' ' '' such that W = roots(T ), (q, E, T, s) . * (q, [E,E ],TT ,r). Proof: By induction on the height \nof the derivation of the third premise.For the inductive case, use Lemma 16 to help satisfy the inductionhypothesis. \nThen, proceedbycase analysisofATLrules. The case of ruleATL-A uses the fourth premise, ruleS-CALL, Lemma \n10, and, .nally, rule S-RETURN. For the latter, we use Lemma7to help satisfy the third premise. Now, \nwe may begin the task of relating the languages of gram\u00admars and transducers. Corollary 18 (The Roots \nLemma) If q is a callee, (q, [y0:=v], ., q) . * (q, E, T, r) and r . A, then roots(T ) .TA(q)(v). Proof: \nThis Corollary follows directly from Lemma 15 and the de.nition of TA(q)(v). With this result, we can \nnow relate trees constructed in a stack evaluation directly to a grammar. Lemma 19 (The Roots-Grammar \nLemma) If T is a transducer for G, q is a callee, (q, [y0:=v], ., q) . * (q, E, T, r), and r . A, then \nroots(T ) in GA(v). Proof: The proof of the Roots-Grammar Lemma follows directly from Lemma 18 together \nwith condition (T2) that is required for T to be a transducer for G. Atthispoint,wearereadytoproveour \n.nal result,namely,that thelanguageofa transducerwillmatchthelanguageofagrammar, assuming our conditions \nare met. We prove language equality in two steps, by proving mutual inclusion of the two languages. The \nLeaves Lemma will showthat the transducer slanguage is included in the grammar s, and the Callee Correctness \nLemma will show the reverse.  Lemma 20 (The Leaves Lemma) If T is a transducer for G, q is a callee, \n(q, [y0:=v], ., q) . * (q, E, T, r), and r . A, then leaves(T ) . LA(v). Proof:byinductionon heightofthe \ntree T .For all cases, the es\u00adsential lemmas are the Root-Grammar lemma, and Lemma 14.To\u00adgether, theytake \na string from a transducer language to an abstract transducer language to an abstract grammar language \nand then to a concrete grammar language. In the inductive case, for every di\u00adrect subtree of T , we use \nthe Pre.x Lemma and the Direct-Subtree Lemma to satisfy the inductionhypothesis for that subtree. Then, \nLemma14 and Lemma1give us our result. Lemma 21 (Callee Correctness) If T is a transducer for G, q is \na callee, GA(v)= TA(q)(v), R(A)= r, and [y0:=v] . w . r . E, then w . LA(q)(v). Proof: by induction on \nthe height of the derivation of the last premise. However, in place of case analysis on the GL rules, \nwe perform cases analysis on the structure of w. Speci.cally, we des\u00adignate base cases as those for which \nno portion of w corresponds to a nonterminal, and those for which somedo. In the proof of this lemma, \nwe take the opposite path as the previous lemma, taking a string from a grammar language to an abstract \ngrammar language to an abstract transducer language and thentoa concrete transducer language.Forall cases,we \nstartwith Lemma 13, use the third premise to cross over to the abstract grammar language,andthen .nishupwithLemma17.Forthebase \ncases, we satisfy Ok(T ,W ) trivially (the relevant W is just an m), while for the inductive cases we \nmake use of condition (T1) and then apply the inductionhypothesis. In the inductive case, we tie together \nour results using Lemma 2. Theorem22(Transducerfor Grammar Correctness) If T is a transducer for G then \nLT = LG. Proof: with the above lemmas and the de.nitions of LT and LG, we can directly show that LT . \nLG and LG . LT , respectively, with the latter result relying on condition (T0).  5.4 Translation from \ngrammars to transducers The translation from data-dependent grammars to transducers, pre\u00adsented in Figure \n6, is an extension of the Thompson translation of regular expressions into automata. The .rst judgment, \nS . r . (s, F, T ), states that right-hand side r is translated into transducer graph T with start state \ns and .nal state F .2 S isa.nite partial map from nonterminals to the start states for the automata implement\u00ading \nthem. The second judgment, G . T , uses the .rst judgment tobuild transducer graphs for all right-hand \nsides of nonterminals in grammar G andputsthe results togetherto constructa complete transducer T . These \njudgments use the following notation. z [s -. t] is a transducer graph with a single arc of sort z from \nstates s to t.  [s -. t] is a transducer graph with a single epsilon transition from states s to t. \nAn epsilon transition is an abbreviation for  true the predicate transition s -. t. T1; T2 isthe transducergraphbuiltbytakingtheunionofnodes \nand edges from graphs T1 and T2.  [s . t] is the transducer graph with disconnected states s and  t. \n2In a slight abuse of notation, we have overloaded meta-variable T so it represents both transducer graphs \n(the set of nodes and edges making up the transducer relations) and full transducers (the graphs plus \nauxiliary information such as the blackboxes, terminal alphabet, etc.). Given a transducer graph T , \n[S, ., F,A0,sA0 , T ] builds the transducer with graph T and other components speci.ed by S, ., F,A0 \nand sA0 . The following lemma establishes the correctness of the right\u00adhand side translation. Lemma23 \n(RuleTranslation Correctness) i. If S . r . (s, F, T ) then the following are true of transducer graph \nT : a. There are no .nal states in T . x:=A(e) call(e) b. Ifr -. t in T then r -. S(A) in T . call(e) \nc. If r -. s in T then s = S(A) for some A. ii. If E . W . r . E ' and S . r . (s, F, T ) then E . W \n: s . * F ; E ' . iii. If S . r . (s, F, T ) and E . W : s . * F ; E ' then E . W . r . E ' . Proof:Eachpartisproven \nindependently,by inductiononheightof the .rst derivation. Intuitively,parts(ia)and(ic)helpestablish(T2) \n(noextragarbageinthe language).Part(ib)helps establish(T1) (all A(e) edges are implemented). Parts(ii)and(iii)help \nestablish the language equivalence conditions speci.ed in (T0), (T1) and (T2). The Rule Translation Correctness \nLemma, together with the de.nition of the grammar translation and De.nition9is suf.cient to prove that \na transducer produced by the translation implements its grammar correctly: Theorem24 (GrammarTranslation \nCorrectness) If G . T then T is a transducer for G. 6. An Earley-styleParsing Algorithm The stack evaluation \nrelation, while informative as a semantics of the transducer, does not lend itself to ef.cient direct \nimplementa\u00adtion. The nondeterministic nature of the relation could result in an exponential time bounds \nfor parsing even relatively simple gram\u00admars, and nontermination for grammars with left-recursion. There\u00adfore, \nin this section, we provide an alternative, Earley-style parsing algorithm that matches the transducer \nsemantics, while (often) im\u00adproving execution behavior. 6.1 The Algorithm The traditional Earley algorithm \nproceeds by computing a set of Earleyitems for each position in the input. These items are com\u00adputed \nfrom left to right: First the Earleyset for input position1is computed, then the set for input position \n2, etc.. Each item con\u00adtains information about what grammar rule is being parsed (and how much of that \ngrammar rule has been parsed) as well as the position in the input where the parse for that rule started. \nA key aspect of the algorithm is that the Earley sets act like a memo\u00adization table rather than re-parsing \nportions of the input multiple times like an exponential-time back-tracking algorithm would do, Earley \nsavesworkby reusing items from Earley sets.In Earley s case,the algorithmworks because context-freegrammarsare,well, \ncontext-free. Intuitively, in our case, an extension of the algorithm will work because we include the \nlocal context E in our modi.ed Earley sets and, crucially, because, as stated by the Context Inde\u00adpendence \nLemma (Lemma 10), parsing a particular grammar rule only depends upon that local context, not on the \ntail of the stack. With that background in mind, we present our modi.ed Ear\u00adley algorithm. The Earley \nsets involved in our algorithm are in\u00addexed sets of parse trees (forests). A tree T belongs to the set \n T . tree(i, j, q, E, s) S . r (s, F, T ) s, F fresh T-EPS S . . (s, F, [s -. F ]) s, F fresh T-TERM \nc S . c (s, F, [s -. F ]) s, F fresh T-PRED e S . [e](s, F, [s -. F ]) s, F fresh x = y0 T-BINDING x:=e \nS . x:=e (s, F, [s -. F ]) s, F fresh T-BLACKBOX f(e) S . f(e)(s, F, [s -. F ]) s, F fresh x = y0 T-A \nx:=A(e) call(e) S . x:=A(e)(s, F, [s -. F ]; [s -. S(A)]) s, F fresh S . r1 (s1,F1, T1) S . r2 (s2,F2, \nT2) T-SEQ T3 =[s -. s1]; [F1 -. s2]; [F2 -. F ] S . r1.r2 (s, F, T1; T2; T3) s, F fresh S . r1 (s1,F1, \nT1) S . r2 (s2,F2, T2) T-ALT T3 =[s -. s1]; [s -. s2]; [F1 -. F ]; [F2 -. F ] S . r1 + r2 (s, F, T1; \nT2; T3) s, F fresh S . r1 (s1,F1, T1) T-* T2 =[s -. s1]; [F1 -. F ]; [s -. F ]; [F1 -. s1] S . r1 * (s, \nF, T1; T2) s, F fresh T-EMPTY S . empty (s, F, [s . F ]) G T R =[A0 = .y0.rA0 ,...,Ak = .y0.rAk ] sA0 \n,...,sAk fresh S =[A0 = sA0 ,...,Ak = sAk ] S . rAi (si,Fi, Ti) (for i =0,...,k) T-G Tinit =[sA0 -. \ns0]; \u00b7\u00b7\u00b7 ;[sAk -. sk] Tfinal =[F0 . A0]; \u00b7\u00b7\u00b7 ;[Fk . Ak] T = T0; \u00b7\u00b7\u00b7 ; Tk; Tinit; Tfinal (S, ., F,A0, \nR) [S, ., F,A0,sA0 , T ] Figure 6. Translation from grammars to transducers. ET-INIT . . tree(0, 0,q0, \n[y0:=()],q0) cj T . tree(i, j-1, q, E, r) r -. s ET-TERM Tcj . tree(i, j, q, E, s) e T . tree(i, j, q, \nE, r) r -. s [ e] E = true ET-PRED T . tree(i, j, q, E, s) x:=e T . tree(i, j, q, E, r) r -. s ET-BIND \n[ e] E = v (x = y0) T {x:=v}. tree(i, j, q, E[x:=v],s) f(e) T . tree(i, k-1, q, E, r) r -. s ET-fck..cj \n. F(f)(v)[ e] E = v T (ck..cj ). tree(i, j, q, E, s) call(e) T . tree(i, j, q, E, r) r -. s [ e] E = \nv ET-CALL . . tree(j, j, s, [y0:=v],s) [ e1] E2 =[ e2] E2 = E1(y0)= v (x = y0) call(e1) T1 . tree(k, \nj, q, E1,r) t -. qr . A x:=A(e2) ET-RETURN T2 . tree(i, k, s, E2,t) t -. u T2 x:A(v)(T1). tree(i, j, \ns, E2[x:=leaves(T1)],u) Figure 7. The EarleySets tree(i, j, q, E, r) when that tree is constructed by \nparsing the in\u00adput from position i +1 to position j. The parse of this subsequence must have begun with \nthe transducer in callee state q and ended with the transducer in state r. Environment E is the environment \nthatwasbuilt during the courseof the parse. Figure7givesadeclarativepresentationof our Earleyalgorithm \nby specifying the trees that belong to each Earleyset. The .rst .ve rules de.ne tree construction when \nno subtrees are involved and are quite similar to their counterparts (by name) in the de.nition of the \nstack evaluation relation. Rule ET-TERM refers to character cj the jth symbol in the input string. Rules \nET-CALL and ET-RETURN control the construction of subtrees. ET-CALL adds an empty tree to the forest \nwhose start index matches the current index of the caller and whose callee is related to the current \nstate via a call edge.3 ET-RETURN .nds a parse tree for some nonterminal A (that is, the tree is a member \nof a forest whose state is a .nal state for A)and looks for all the potential parents of that tree. Theyare \nfound via the following criteria: their current position is k,their current state calls the callee state, \nthey transition on A, and the value of A s argument in the context of the parent tree s forest must match \nthe value of the calling argument recorded in the context of the subtree s forest. Based on the tree \nsets de.ned in Figure 7, we de.ne Earley parsing as follows: 3The latter criterion is characteristic \nof Earley s algorithm in that it ensures thata subparseis only attemptedifitis predicted by the grammar \n(trans\u00adducer).  De.nition11 (EarleyParsing) If T . tree(0, j, q0,E,r) and r . A0 then Earley(c1 ...cj \n)= tree(0, j, q0,E,r). Therefore, we can say that an entire string w = c1 ...cn is suc\u00adcessfully parsed \nif Earley(w)= S = \u00d8. Moreover, S contains all possible parse trees for w. We note that our declarative \nrules do not specify the order in which to construct the Earley sets, and many different orders are possible. \nThe simplest order to use is tobuild the parse trees in a breadth-.rst fashion, moving left to right \nthrough the input: initialize tree(0, 0,q0, [y0:=()],q0) to . (as speci.edbyruleET\u00adINIT)and then, for \neach index j from 0 to the size of the input, apply all rules which add a tree to some forest whose second \nindex is j, until those forests stop changing. There are a number of potential optimizations one could \napply to this algorithm, but exploring them is beyond the scope of this paper.  6.2 Correctness Wewouldliketobe \nsurethat our algorithm matchesthe transducer semantics de.ned earlier. We therefore show that for every \ntree derivable in one schema, a corresponding tree is derivable in the other schema.We.rstshowthattheEarleyalgorithmissoundwith \nrespect to the stack semantics. Theorem 25 (Earley Soundness) If T . tree(i, j, q, E, r) then there exists \ntl such that (q0, [y0:=()], ., q0) . * (q, E, T, r)::tl. Proof:by induction on the derivation that T \n. tree(i, j, q, E, r). Next, we show that the Earley algorithm is complete. First, though, we extend \nthe de.nition of leaves(\u00b7) to stacks: leaves((q, E, T, r)::tl)= leaves(tl) leaves(T ) Theorem 26 (Earley \nCompleteness) If (q0, [y0:=()], ., q0) . * (q, E, T, r)::tl, leaves(tl)= c1..ci, leaves(T )= ci+1..cj \n, then T . tree(i, j, q, E, r). Proof:by induction on the heightof the . * derivation. Corollary27 (EarleyParsing \nSimulatesTransducer Execution) T . Earley(w) iff (q0, [y0:=()], ., q0) . * (q0, E, T, r) and r . A0. \n 6.3 Running time We now turn to the issue of the running time and termination of an algorithm implementing \nthe rules of Figure 7, assuming that such an algorithm does not needlessly revisit elements for which \nall possible rules have already been applied. Given an input string of length n, Earleyshowed a time \nbound of O(n 3) for his original algorithm4 [5]. Our extension enjoys a pay-as-you-go property, with \nthe following consequences: In the case that a context-free grammar is speci.ed, we re\u00adtain the O(n \n3) bound, given an ef.cient representation of tree sets (for example, the binarised Shared Packed Parse \nForests (SPPFs) usedby Scott [29]).  In the case that the full features of our system are used, the \nalgorithm is guaranteed to terminate on all inputs, if (a) all expressions withinthegrammar terminate,(b)thesizeofvalues \nin environments is bounded, (c) all blackboxes terminate, and  (d) tree setshavea.nite representation.The \nasymptotic running 4Following Earley, we consider the set insertion step in each rule as a primitive \noperation, whose complexity is independent of the input [5]. timeof the algorithmin this case will depend \non manyfactors. Notably, environment values can contribute to the running time afactor that is exponential \nin their size. In the case that one or more of the aboveconditions are violated, no guarantees can be \nmade. Note, though, that those are suf.\u00adcient,but not necessary conditions, because theexact behaviour \nof the algorithm will usually depend upon the particular input. 7. RelatedWork Throughout the paper, \nwe have mentioned a number of important related systems we will not reiterate all of the points of com\u00adparison \nwith those systems here. However, please recall the major differences between our system and systems \nfor Generalized LR (GLR) include our support for direct compilation of regular-right sides, attribute-directed \nparsingand blackboxes.RegardingParsing Expression Grammars (PEGs), we are principally distinguished by \nthe compositionality properties of our formalism.These composi\u00adtionality properties also distinguish \nus from the various data de\u00adscription languages such asPADS[7,22] and the Data Description Calculus formalism \n[8]. Attribute grammars (AGs) are a very powerful extension of context-free grammars originally proposed \nby Knuth for de.n\u00ading the semantics of programming languages [18]. Much work in AGs has been devoted \nto .nding tractable and ef.cient restrictions, such as those based on LR or LL grammars [17, 26]. Within \nat\u00adtribute grammars, our calculus corresponds most closely to the L\u00adattributed grammars [21]; our nonterminal \nparameters correspond to inherited attributes, and our environments and bindings overlap with synthesized \nattributes. Watt introduced the idea of directed parsing [34], and applied it to the LR fragment of context-free \nlan\u00adguages. Correa[3]andTokudaandWatanabe[31]haveextended Earley s algorithm to L-attributed grammars, \nthough omitting fea\u00adtures such as our environments, regular right sides, andblackboxes; Correa implemented \nattribute-directed parsing. Woods augmented-transition networks (ATNs) [35] are an au\u00adtomaton formalism \nclosely related to our data-dependent automata. They support all context-free languages, regular right-hand \nsides and attribute-directed parsing. Moreover, Chou and Fu describe an Earley-style algorithmcapableof \nparsingwithATNs[2].However, theydifferinanumberof subtle,yet important,details. First,ATNs arelower-levelthanour \ntransducers(forexample, requiringexplicit stack manipulation to handle call arguments and return values) \nand are speci.ed directly, rather than with a grammar which can be compiled into anATN.Woods does not \npresent anysuch high-level grammar formalism, nor state or proveanycorrespondence to some existing grammar \nformalism, as wehave.In addition,ATNsdo not support merging the automata of multiple nonterminals, because \n.nal states are not labeled with their corresponding nonterminal. ATNs do not support blackboxes, although \ntheycould be extended todosointhe samewayaswehavedoneinour formalism.Finally, to the best of our knowledge, \nthe literature onATNs does not in\u00adclude proofs of correctness of the Earleyalgorithm with respect to \na transducer semantics. Finally, as we have mentioned before in the paper, this pa\u00adper builds on our \nprevious work on Earley parsing for context\u00adfree grammars with regular right sides [15], extending it \nto handle attribute-directed parsing and blackboxes. Also new in this paper is the presentation of a \ncomprehensive meta-theoretic framework in which we show how to prove the correspondence between gram\u00admars, \ntransducers and Earleyparsing. 8. Conclusion Modern programmers require modern parser generators.Parsingis \nstill very much an essential element of software systems in nearly every area of software development, \nyet the technology underlying the most common tools is outdated and the tools, therefore, largely irrelevant. \nPromising advances are still being made in support of full context-free grammars, most notably surrounding \nthe GLR al\u00adgorithm.Yet, we believe, and have attempted to demonstrate with a number of examples, that \neven support for all context-free gram\u00admars is not enough for manymundane parsing tasks, particularly \nin the area of systems programming. Features like scannerless pars\u00ading, data-dependence, and blackbox \nsupport are crucial to meet the manyandvaried demandsof modernprogrammers.  We have presented a concise \nformalism which incorporates all of these features into one framework. We have demonstrated the utility \nand necessity of its features with a variety of examples and formalized its syntax and semantics.We have \nalso presented and formalized the novel data-dependent automata, which are capa\u00adbleof parsing the languagesof \ndata-dependent grammars.Wehave speci.ed suf.cient conditions under which an automaton can be said to \nimplement a grammar, proven that under those conditions the language of the automaton matches the language \nof the gram\u00admar, and presented an example compilation from grammars to au\u00adtomata that satis.es the suf.cient \nconditions. Finally, we have pre\u00adsented and proven correct an (often) ef.cient algorithm for parsing \nwith data-dependent automata based on Earley s classic algorithm for parsing the full rangeof context-freegrammars. \nAcknowledgments Thanks to Kathleen Fisher for helpful discussions about the mate\u00adrialinthispaper.WealsothankthePOPLreviewersfortheir \nuseful comments. This material is based in part upon work supported by the NSF under grants 0612147 and \n0615062 and by a gift from Google. Anyopinions, .ndings, and conclusions or recommenda\u00adtions expressed \nin this material are those of the authors and do not necessarily re.ect the views of the NSF or Google. \nReferences [1] Martin Bravenboer and EelcoVisser. Concrete syntax for objects: domain-speci.c language \nembedding and assimilation without restrictions. In Proceedings of theACM SIGPLAN Conference on Object-oriented \nProgramming,Systems, Languages, and Applications (OOPSLA), 2004. [2] S.M. ChouandK.S.Fu.Transition networksfor \npattern recognition. Technical Report TR EE 75 39, School for Electrical Engineering, Purdue University,West \nLafayette, IN, 1975. [3] N. Correa. An extension of Earley s algorithm for S-and L-attributed grammars. \nIn Proceedings of the International Conference on Current Issues in Computational Linguistics, Penang, \nMalaysia, 1991. [4] M. Crispin. Internet Message Access Protocol Version 4rev1. http://www.ietf.org/rfc/rfc3501.txt, \nMarch 2003. [5] Jay Earley. An ef.cient context-free parsing algorithm. Communica\u00adtionsof theACM, 13(2):94 \n102, 1970. [6] Giorgios Economopoulos, Paul Klint, and Jurgen Vinju. Faster scannerless GLR parsing. \nIn Proceedings of the 18th International Conference on Compiler Construction (CC). Springer-Verlag, 2009. \n[7] Kathleen Fisher and Robert Gruber. PADS: A domain speci.c language for processing ad hoc data. In \nPLDI, pages 295 304, 2005. [8] Kathleen Fisher,Yitzhak Mandelbaum, and DavidWalker. The next 700 data \ndescription languages. In ACM Symposium on Principles of Programming Languages, 2006. [9] Kathleen Fisher, \nDavid Walker, Kenny Q. Zhu, and Peter White. From dirt to shovels: Fully automatic tool generation from \nad hoc data. In ACM Symposium on Principles of Programming Languages, pages 421 434, January 2008. [10] \nBryanFord.Packrat parsing: simple,powerful,lazy, linear time.In ACM International Conference on Functional \nProgramming, pages 36 47.ACM Press, October 2002. [11] BryanFord.Parsingexpression grammars:a recognition-based \nsyn\u00adtactic foundation. In ACM Symposium on Principles of Programming Languages, pages 111 122.ACM Press, \nJanuary 2004. [12] Robert Grimm. Practical packrat parsing.Technical Report TR2004 854,NewYork University, \nMarch 2004. [13] Ian Hickson andDavid Hyatt. HTML5:Avocabulary and associated APIs for HTML and XHTML. \nhttp://dev.w3.org/html5/ spec/Overview.html#parsing. [14] R. John M. Hughes and S. Doaitse Swierstra. \nPolish parsers, step by step. In ACM International Conference on Functional Programming, pages 239 248,NewYork,NY, \nUSA, 2003. [15]Trevor Jim andYitzhak Mandelbaum. Ef.cient earleyparsing with regular right-hand sides. \nIn Workshop on Language Descriptions Tools and Applications, 2009. [16] S. C. Johnson. Yacc: Yet another \ncompiler compiler. Technical Report 32,AT&#38;T Bell Laboratories, Murray Hill, NJ, 1975. [17] Neil Jones \nand Michael Madsen. Attribute-in.uenced LR parsing. In Semantics-Directed Compiler Generation, volume \n94 of Lecture Notes in Computer Science, pages 393 407. Springer Berlin, 1980. [18] Donald E. Knuth. \nSemantics of context-free languages. Theory of Computing Systems, 2(2):127 145, June 1968. [19] Peter \nJ. Landin. The next 700 programming languages. Communica\u00adtionsof theACM, 9(3):157 166, March 1966. [20] \nXavier Leroy, Damien Doligez, Jacques Garrigue, Didier R\u00b4emy, andJ\u00b4er\u00b4omeVouillon. The Objective Caml \nsystem release 3.10: Documentation and user s manual, 2007. [21] P.M. Lewis, D.J. Rosenkrantz, and R.E. \nStearns. Attributed translations. Journal of Computer and System Sciences, 9(3):279 307, December 1974. \n[22]Yitzhak Mandelbaum, Kathleen Fisher,DavidWalker,Mary Fernan\u00addez, and Artem Gleyzer. PADS/ML:Afunctional \ndata description language. In ACM Symposium on Principles of Programming Lan\u00adguages, 2007. [23] Scott \nMcPeak. Elkhound:Afast, practical GLR parser generator. Technical Report UCS/CSD 2 1214, University of \nCalifornia, Berkeley, 2002. [24] Scott McPeak and George C. Necula. Elkhound:Afast, practical GLR parser \ngenerator. In Proceedings of Conference on Compiler Constructor, April 2004. [25] A. Melnikov. Collected \nextensions to IMAP4 ABNF. http: //www.ietf.org/rfc/rfc4466.txt, April 2006. [26] KarelM\u00a8uller. Attribute-directed \ntop-down parsing. In Compiler Construction, volume 641 of Lecture Notes in Computer Science, pages 37 \n43. Springer Berlin, 1992. [27] Eric Rescorla. SSL and TLS: Designing and Building Secure Systems. Addison-WesleyProfessional, \nOctober 2000. [28] D. J. Salomon and G.V. Cormack. Scannerless NSLR(1) parsing of programming languages. \nIn PLDI, pages 170 178, 1989. [29] Elizabeth Scott. SPPF-style parsing from Earley recognisers. In Proceedings \nof the Seventh Workshop on Language Descriptions, Tools, and Applications (LDTA), March 2007. [30] Elizabeth \nScott and Adrian Johnstone. Right nulled GLR parsers. ACMTrans.Program. Lang. Syst., 28(4):577 618, 2006. \n[31]TakehiroTokuda andYoshimichiWatanabe. An attributeevaluation of context-free languages. Information \nProcessing Letters, 52(2):91 98, October 1994. [32] M.G.D. van den Brand, J. Scheerder, J.J. Vinju, and \nE. Visser. Disambiguation .lters for scannerless generalized LR parsers. In Compiler Construction 2002, \nvolume 2304 of Lecture Notes in Computer Science, pages 143 158, April 2002. [33] EelcoVisser. Syntax \nDe.nition for Language Prototyping. PhD thesis, University of Amsterdam, September 1997. [34] David Watt. \nRule splitting and attribute-directed parsing. In Semantics-Directed Compiler Generation, Lecture Notes \nin Com\u00adputer Science, pages 363 392. Springer Berlin, 1980. [35]W.A.Woods. Transition network grammars \nfor natural language analysis. Communicationsof theACM, 13(10):591 606, 1970.    \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We present the design and theory of a new parsing engine, YAKKER, capable of satisfying the many needs of modern programmers and modern data processing applications. In particular, our new parsing engine handles (1) full scannerless context-free grammars with (2) regular expressions as right-hand sides for defining nonterminals. YAKKER also includes (3) facilities for binding variables to intermediate parse results and (4) using such bindings within arbitrary constraints to control parsing. These facilities allow the kind of data-dependent parsing commonly needed in systems applications, particularly those that operate over binary data. In addition, (5) nonterminals may be parameterized by arbitrary values, which gives the system good modularity and abstraction properties in the presence of data-dependent parsing. Finally, (6) legacy parsing libraries,such as sophisticated libraries for dates and times, may be directly incorporated into parser specifications. We illustrate the importance and utility of this rich collection of features by presenting its use on examples ranging from difficult programming language grammars to web server logs to binary data specification. We also show that our grammars have important compositionality properties and explain why such properties areimportant in modern applications such as automatic grammar induction.</p> <p>In terms of technical contributions, we provide a traditional high-level semantics for our new grammar formalization and show how to compile grammars into non deterministic automata. These automata are stack-based, somewhat like conventional push-down automata,but are also equipped with environments to track data-dependent parsing state. We prove the correctness of our translation of data-dependent grammars into these new automata and then show how to implement the automata efficiently using a variation of Earley's parsing algorithm.</p>", "authors": [{"name": "Trevor Jim", "author_profile_id": "81100065932", "affiliation": "AT&#38;T Labs - Research, Florham Park, NJ, USA", "person_id": "P1911133", "email_address": "", "orcid_id": ""}, {"name": "Yitzhak Mandelbaum", "author_profile_id": "81100175667", "affiliation": "AT&#38;T Labs - Research, Florham Park, NJ, USA", "person_id": "P1911134", "email_address": "", "orcid_id": ""}, {"name": "David Walker", "author_profile_id": "81100426485", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P1911135", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706347", "year": "2010", "article_id": "1706347", "conference": "POPL", "title": "Semantics and algorithms for data-dependent grammars", "url": "http://dl.acm.org/citation.cfm?id=1706347"}