{"article_publication_date": "01-17-2010", "fulltext": "\n Programming with Angelic Nondeterminism Shaon Barman Rastislav Bod\u00edk Satish Chandra* Joel Galenson Doug \nKimelman* Casey Rodarmor Nicholas Tung University of California, Berkeley *IBM T.J. Watson Research Center \nAbstract Angelic nondeterminism can play an important role in program de\u00advelopment. It simpli.es speci.cations, \nfor example in deriving pro\u00adgrams with a re.nement calculus; it is the formal basis of regular expressions; \nand Floyd relied on it to concisely express backtrack\u00ading algorithms such as N-queens. We show that angelic \nnondeterminism is also useful during the development of deterministic programs. The semantics of our \nangelic operator are the same as Floyd s but we use it as a substitute for yet-to-be-written deterministic \ncode; the .nal program is fully deterministic. The angelic operator divines a value that makes the program \nmeet its speci.cation, if possible. Because the operator is executable, it allows the programmer to test \nincomplete programs: if a program has no safe execution, it is already incorrect; if a program does have \na safe execution, the execution may reveal an implementation strategy to the programmer. We introduce \nre.nement-based angelic programming, describe our embedding of angelic operators into Scala, report on \nour imple\u00admentation with bounded model checking, and describe our experi\u00adence with two case studies. \nIn one of the studies, we use angelic op\u00aderators to modularize the Deutsch-Schorr-Waite (DSW) algorithm. \nThe modularization is performed with the notion of a parasitic stack, whose incomplete speci.cation was \ninstantiated for DSW with angelic nondeterminism. Categories and Subject Descriptors D.2.1 [Requirements/Spec\u00adi.cations]: \nMethodologies; D.2.2 [Tools and Techniques]: Top\u00addown programming; D.2.4 [Software/Program Veri.cation]: \nVali\u00addation General Terms Design, Languages, Veri.cation Keywords Angelic non-determinism, constraints, \nbounded model\u00adchecking, traces, re.nement 1. Introduction Model checking and testing leverage compute \npower to validate complete programs. However, much less work has addressed the issue of how to assist \nwith the construction of a program. This paper proposes using the clairvoyance of angelic nondetermin- \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . \n$10.00. ism in the solving of programming problems. The programmer encodes his partial understanding \nof the programming problem in an angelic program, relying on the (executable) nondetermin\u00adistic choose \noperator to produce values that the programmer is as yet unable to compute. The system answers whether \nthe angelic program has a safe execution for a given input; if so, the sys\u00adtem outputs the program s \ntraces. The process of programming with angelic nondeterminism amounts to (1) testing hypotheses about \nplausible solutions by formulating angelic programs and, as the understanding develops, (2) gradually \nre.ning the angelic program until all angelic nondeterminism is removed. An angelic program is a program \nwith a choose operator. This nondeterministic operator was .rst proposed by Floyd [7], who intended it \nas programming abstraction for hiding implementa\u00adtion details of backtracking search, with the goal of \nallowing ab\u00adstract speci.cations of algorithms such as N-queens. The choose operator divines a value \nthat makes the program terminate with\u00adout violating an assertion, if possible. We say that an angelic \npro\u00adgram is correct if such a safe execution exists for all inputs. The operator is nondeterministic \nbecause it can evaluate to any suit\u00adable value; it is angelic because it collaborates with the program \nagainst the environment (i.e., the input). The input sequence can be seen as demonic nondeterminism, \n.nding a value that might make the program fail. The choose operator is clairvoyant in that it looks \nahead into the execution and returns a value that allows further choose operators to return values that \nlead to a successful execution, if possible. Angelic nondeterminism can be a deduc\u00adtive device (e.g., \n[4]) or it can be executable [7], with implemen\u00adtations relying on backtracking or constraint solving \n(Section 7). Our choose operator is executable but, unlike Floyd, we pro\u00adpose to use it only during program \ndevelopment. It is used to ex\u00adpress partially-developed programs; .nal programs are choose\u00adfree. The \nchoose operator stands for code that the programmer has not yet implemented, either because he does not \nyet know how to implement it or because he does not even know what values it should produce. The choose \noperator produces values based on a correctness condition; the .nal program will compute these values \nwith deterministic code fragments. Executable angelic nondeterminism aids programming in several ways. \nFirst, the programmer can test hypotheses about his implementation strategies. He does so by formulating \nan an\u00adgelic program, using the choose operators as substitutes for as yet unimplemented code fragments. \nAn example of a hypothe\u00adsis that the programmer may test is whether there exists a way to satisfy a postcondition \nin at most n iterations of a loop; the loop will typically have angelic statements in the body. If no \nsafe angelic execution exists, the hypothesis is rejected. The op\u00aderator represents the most general \nway of computing a subtask, but the programmer can declaratively (with assertions) constrain it in ways \nthat re.ect his hypothesis. For example, the operator may be prevented from visiting a graph node multiple \ntimes. The envisioned bene.t to the programmer is that the implementation strategy can be safely aborted \nbefore the programmer understood its infeasibility on his own.  Second, if the angelic program is correct, \nthe system outputs its safe traces. The traces serve as demonstrations of how to per\u00adform the desired \ntask, for example: how to rotate nodes to es\u00adtablish the red-black property. Given these demonstrations, \nthe programmer is left with the hopefully simpler task of generaliz\u00ading the traces into an algorithm \nthat works for all inputs. These demonstrations represent vast amounts of combinatorial reason\u00ading that \nthe programmer may otherwise have to carry out on his own. The envisioned bene.t of angelic demonstrations \nis a rev\u00adelation of an insight into how to carry out a computation under given time or space constraints, \npotentially leading to faster dis\u00adcovery of the algorithm. An angelic program may, of course, generate \nmany alterna\u00adtive traces, and most of the traces may be the result of choose operators abusing their \nclairvoyance to complete the computa\u00adtion in an input-dependent fashion that does not correspond to an \neasily encodable deterministic algorithm. The programmer can exclude such irregular traces with assertions \nthat limit nondeter\u00administic choices. For example, the operator may be required to satisfy a data structure \nconsistency invariant or to evaluate only to values that have already been stored in memory. Third, nondeterminism \nallows re.nement-based program\u00adming. The programmer can develop the program gradually in several ways. \nAs mentioned above, by adding assertions, he can prune the set of safe traces, working towards a trace \nthat corre\u00adsponds to a desired algorithm. The programmer can also focus on one subproblem at a time, \nignoring those subproblems expressed with choose. In general, it is not possible to re.ne angelic opera\u00adtors \nindependently of each other, but as we show in Section 5.2, sometimes we can add suf.cient constraints \nin the program for them to be independent. The programmer can also implement the choose operator with \na subprogram that is itself nondeterminis\u00adtic, applying the methodology recursively until a deterministic \nprogram is obtained. Finally, the clairvoyance of the choose operator helps the programmer avoid complex \nglobal reasoning. In Section 8, we rely on this property to modularize the Deutsch-Schorr-Waite (DSW) \nalgorithm [15]. We refactor the algorithm to hide its backtracking logic in a parasitic stack, a data \nstructure that stores its values in memory locations borrowed from the host data structure, i.e., the \ngraph being traversed. The hard decisions of which memory locations the parasitic stack is allowed to \nborrow and how to restore their values are left to choose operators. The programmer has to solve only \nthe more local problem of how to implement the stack with these borrowed locations. In fact, without \nthe choose, we were unable to carry out this new modularization of DSW. This paper makes these contributions: \n We propose a program development methodology based on re.nement of angelic programs. In contrast to \nprevious work in program re.nement, our methodology permits the exe\u00adcution of incomplete programs, which \nhelps a programmer in several ways, from rejecting infeasible implementation strategies to making reasoning \nmore local.  We add the choose construct to the Scala programming lan\u00adguage [13]. We describe our implementation \nof angelic non\u00ad  determinism. We also present experimental results on the ef\u00ad.ciency of two different \nimplementation strategies, one based on backtracking and one on SAT solving. We present case studies \nof two classic problems: the Dutch Flag problem [6, 10] and the Deutsch-Schorr-Waite graph marking algorithm \n[15]. For the second problem, angelic pro\u00ad gramming allowed us to develop a new way of modularizing the \nalgorithm that we believe is novel. We also include a case study on recursive list manipulation. Section \n2 gives an overview of the programming methodol\u00adogy using the case study of the Dutch Flag problem. Sections \n3 5 formalize the methodology and give re.nement transformations. Section 6 compares our (algorithmic) \nmethodology with the (de\u00adductive) re.nement of Morgan. Section 7 describes our imple\u00admentation and Section \n8 reports on the case study of the DSW algorithm. Related work is described in Section 9. 2. Overview \nThis section gives an overview of programming with angelic nondeterminism on the Dutch Flag problem [6]. \nThe section does not give a beauti.ed tutorial example; instead, we present rather faithfully how a programmer \nused the programming methodol\u00adogy. We highlight three aspects of angelic programming: How an angelic \nprogram can express a hypothesis about the programmer s implementation plan, which will be rejected if \nit is guaranteed not to lead to a correct implementation.  How the programmer improves his understanding \nof the problem by observing angelic executions.  How angelic programs are re.ned by constraining the \nangels and by deterministically implementing them.  Dijkstra presents the Dutch Flag problem in [6]: \ngiven an array of n pebbles, each of which is either red, white, or blue, the algorithm must sort them \nin-place, in order of the colors in the Dutch Flag: .rst red, then white, then blue. The algorithm must \nexamine the color of each pebble at most once and can only move the pebbles by swapping. A crucial constraint \nis that only a constant amount of storage can be used to remember the color of pebbles that have been \nexamined. In traditional algorithm development, a programmer must implement the entire algorithm .rst \nbefore he can test it. Perhaps he can proceed bottom up, testing small subroutines .rst. With angelic \nnondeterminism, one can proceed top down, starting with a very nondeterministic program that implements \ncoarse steps of the entire algorithm. In the case of the Dutch Flag problem, the programmer starts with \nthis initial angelic program, which tests the hypothesis that the problem can indeed be solved with a \nsequence of swap operations. Program P0 while ( choose ){ swap(choose(n), choose(n)) } assert isCorrect \n The choose operator nondeterministically chooses a Boolean value, and the expression choose(n) nondeterministically \nchooses a value from the range [0, n). This angelic program is correct (i.e. the angels can .nd a safe \nexecution that validates the hypothesis).1 In thinking about how 1 Our tester validates the hypothesis \non only a small set of inputs, so the best we can actually ascertain is that the hypothesis has not been \nrejected  to re.ne the program, the programmer prints information about the choices being made by the \nangels. ' Program P0 angelicprint(input) while ( choose ){ i= choose(n) j= choose(n) swap(i,j) angelicprint(i,j) \n} assert isCorrect The call angelicprint prints its arguments, but only on suc\u00adcessful executions. Henceforth, \nassume that all of our programs are instrumented to print inputs and the indices of swapped peb\u00adbles \non safe executions into a log. The following is the output of one of the safe executions of this program: \nInput: bwrwrwb (0,2) (1,4) (2,5). The following are swap sequences from two of the thousands of other \nsafe executions, one line per safe execution: (4,0) (4,5) (2,1). (0,1) (1,5) (4,1) (2,0). In examining \nthe executions, the programmer focuses on which pebbles were swapped. Not surprisingly, he discovers \nthat the highly nondeterministic program P0 swaps pebbles seem\u00adingly arbitrarily. Now the programmer \nhypothesizes the .rst plausible imple\u00admentation strategy. Namely, he posits that it is possible to tra\u00adverse \nthe array of pebbles left-to-right while swapping the cur\u00adrent pebble with a suitable counterpart. Program \nP1 re.nes Program P0 i=0 while ( choose ){ swap(i, choose(n)) i += 1 } assert isCorrect In P1, a value \nthat was previously provided by an angelic operator (the .rst argument to swap in P0) is now computed \nby deterministic code. We say that choose(n) was implemented by this deterministic code. Technically, \nP1 is a re.nement of P0 in that it removes some of the nondeterministic choices present in P0. In other \nwords, P1 generates a subset of executions of P0. Program P1 is correct, too. Below, we show the log \nfrom one of the safe executions. Input: bwrwrwb (0,6) (1,0) (2,0) (3,1) (4,1) (5,3) (6,5). P1 still generates \ntoo many traces (the angels have too much freedom) but the programmer notices a pattern in some of the \nsuccessful traces: towards the end of each run, the angels are swapping red pebbles to the left end of \nthe array and blue peb\u00adbles to the right end. The programmer hypothesizes that a more on those inputs. \nEven if correctness is proven on all inputs, there is of course no guarantee that a deterministic algorithm \nexists that can replace the nondeterministic operators. deterministic program would be explicit about \ngathering red peb\u00adbles at the left end of the array and blue pebbles at the right from the very beginning. \nThis observation leads to an implementation strategy, en\u00adcoded in the next program, where the programmer \nimplements two zones of uniform color: red at the left end of the array and blue at the right. The programmer \nfurther hypothesizes that a zone of white pebbles in the middle of the array will simply fall out as \na consequence. (The idea of three zones turns out to be exactly the insight that lead Dijkstra to his \nalgorithm [6].) Program P2 i=0 R = 0 // Pebbles to the left of R are red B=n-1 // Pebbles to the right \nof B are blue while ( choose ){ c = pebbles(i) // examine color if (c == red) { j = R; R += 1 } else \nif (c == blue) { j = B; B -=1} else /* (c == white) */{j =i} swap(i, j) i += 1 } assert isCorrect In \nthe next step, captured in program P2, the angelic opera\u00adtor that provided the second argument to swap \nin P1 has been replaced by deterministic code. Program P2 is, however, not cor\u00adrect, as it has no successful \ntraces. P2 is therefore not a proper re.nement of P1. Below is a log from a pre.x of a failed trace: \nInput: bwrwrwb (0,6) (1,1) (2,0) (3,3) (4,1). Examining the log, the programmer can see a problem: after \nthe swap (2,0), a blue pebble is brought into position 2, and the algorithm then proceeds to examine \nthe pebble in position 3, leaving the blue pebble in position 2. This pebble is never re\u00advisited, and \nhence is never swapped to the right end of the array where it belongs. Generalizing from this observation, \nthe programmer decides that in some cases a pebble brought into position i may need to be handled before \nmoving on to position i +1. He decides to handle this by not advancing i in some cases. Since the programmer \nis unsure of exactly when to advance i, he uses an angelic choose operator to make the decision. The \nprogrammer returns to P1, which encoded an infeasible implementation strategy because it allowed at most \none swap per position of the array. The programmer creates P1a an alterna\u00adtive version of P1 that is \na less restrictive re.nement of P0. Program P1a re.nes Program P0 i=0 while ( choose ){ j= choose(n) \nswap(i, j) if ( choose ) i += 1 } assert isCorrect P1a is also correct. The programmer now again applies \nthe re.nement that intro\u00adduces zones of uniform color.  Program P3 re.nes Program P1a i=0 R = 0 // \nPebbles to the left of this are red B=n-1 // Pebbles to the right of this are blue while ( choose ){ \nc = pebbles(i) // examine color if (c == red) { j = R; R += 1 } else if (c == blue) { j = B; B -=1} else \n/* (c == white) */{j =i} swap(i, j) if ( choose ) i += 1 } assert isCorrect One log for this correct \nprogram, including the values of i as well as swaps, is: Input: bwrwrwb i=0:(0,6) i=0:(0,5) i=1:(1,1) \ni=2:(2,0) i=3:(3,3) i=4:(4,1). The next step is to implement the nondeterministic operator that guards \nthe advancement of i. Logs reveal that i is not ad\u00advanced when the current pebble (i.e., the ith pebble \nbefore the swap) is blue. On re.ection, this is reasonable because when the current pebble is blue, it \nis swapped with a pebble from farther right in the array a pebble whose color has not yet been exam\u00adined \nand is not yet known. In contrast, when the current pebble is red, it is swapped with a pebble of a known \ncolor. The programmer now implements the angelic operator with deterministic code, creating re.nement \nP4. Program P4 re.nes Program P3 i=0 R = 0 // Pebbles to the left of this are red B=n-1 // Pebbles to \nthe right of this are blue while ( choose ){ c = pebbles(i) // examine color if (c == red) { j = R; R \n+= 1 } else if (c == blue) { j = B; B -=1} else /* (c == white) */{j =i} swap(i, j) if (c != blue) i \n+= 1 // changed from choose in P3 } assert isCorrect The .nal re.nement must implement the non-deterministic \nloop bound. The programmer might rashly replace the bound with i<n. In this case, the programmer would \n.nd that the program is incorrect, because i would overrun the blue zone. Logs would reveal that the \ncorrect bound is i <= B, as shown in P5, which is the .nal deterministic program. Program P5 the .nal \nprogram; re.nes Program P4 i=0 R = 0 // Pebbles to the left of this are red B=n-1 // Pebbles to the right \nof this are blue while ( i <= B ) { // changed from choose in P4 c = pebbles(i) // examine color if (c \n== red) { j = R; R += 1 } else if (c == blue) { j = B; B -=1} else /* (c == white) */{j =i} swap(i, j) \nif (c != blue) i += 1 } assert isCorrect The development of this solution to the Dutch Flag problem showed \nhow the programmer can pose hypotheses about his im\u00adplementation plans by writing and testing angelic \nprograms and how he can develop the .nal deterministic program by gradu\u00adally creating re.nements that \ninvolve implementing angelic con\u00adstructs with deterministic code. 3. An Angelic Programming Language \nIn this section we describe the syntax and semantics of a small programming language for writing angelic \nprograms. Note that our actual implementation is written as an extension to Scala. 3.1 Core language \nWe consider the following core language: stmt ::= v = expr (Assign) assert b (Assert) stmt1 ; stmt2 ; \n... ; stmtn (Sequence) if (b) { stmt } else { stmt } (Conditional) while (b) { stmt } (Loop) expr ::= \nchoose (Angelic) \u00b7\u00b7\u00b7 Here v is a variable and b is a boolean-valued expression. (The full language supports \narrays and heap operations as well, but their handling is routine and is ignored in this section.) A \npro\u00adgram fails on assert false. The only source of nondeterminism in the language is An\u00adgelic, which \nguesses a value in the domain of integers, addresses or booleans as appropriate in the context. The values \nof choose expressions are chosen so that, if at all possible, the execution terminates without failing \nany assert statement encountered in the execution. 3.2 Semantics Following previous work [3], we give \nsemantics of this language using the wp predicate transformer [6]. Given a statement S and a postcondition \nR, wp(S, R) is the weakest precondition such that when S executes in a state satisfying that precondition, \nthe execution results in a state satisfying R. wp for statements in the core language are straightforward: \nwp(v = e,R)= R[e/v] wp(v = choose,R)= .v.R wp(assert b, R)= b . R ... The wp for assignment substitutes \nfree occurrences of v in the postcondition with the expression e. The wp for choose binds free occurrences \nof v in R by existentially quantifying it: this expresses the notion that if possible, an angel will \nsupply a value such that R is true. The assert statement conjoins its condition b to the postcondition, \nso that the program fails if b is false. The semantics of the control-.ow statements are routine and \nare omitted.2  3.3 Program correctness A program is assumed to be parameterized by an input vector Il, \nwhich binds the initial values of variables. A program is assumed to check for expected postconditions \nusing assert statements. It is not necessary, though, that assert statements are placed only at the end \nof a program. A correct program is one for which .lI), true) is sat- I.wp(P (l is.able. (We assume that \nall variables are de.ned before being 2 We assume that the programmer guarantees the existence of ranking \nfunc\u00adtions such that loops terminate independently of choose expressions.  read, so that the formula \nabove does not contain any free vari\u00adables.) The formula says that on any input there is a way in which \nthe angel can choose values for choose expressions such that no assertion is violated in an execution \nof the program. 4. Traces and Program Re.nement A trace is a sequence of values produced by choose expressions \nduring a safe i.e. not assert-failing and terminating execution of a program. Given an input and a recorded \ntrace, the execu\u00adtion of an angelic program can be reconstructed faithfully. Be\u00adcause the program s postconditions \nmay only care about the .nal state of variables, a particular input might result in multiple safe traces. \nDe.ne safetraces(P (Il)) to be the set of safe traces for a program P on input Il. P is correct if .l \nI.(safetraces(P (Il))= {}) This correctness condition corresponds to the wp-based condi\u00adtion from the \nprevious subsection: each trace corresponds to the values which when assigned to existentially quanti.ed \nvariables in the wp-based condition would satisfy the condition. The purpose of re.nement is to decrease \ndependence on the angel while carrying out effectively the same computation steps as those computed by \nthe angels. In particular, executions that a programmer ruled out by eliminating choices available to \nan an\u00adgel (e.g., by adding assertions) should not reappear in successor programs. If programs P and Q \ncontain identical set of choose statements, then for Q to re.ne P , it must be the case that: .l I.safetraces(Q(Il)) \n. safetraces(P (Il)) In general Q will not contain an identical set of choose ex\u00adpressions as P , because \na programmer typically eliminates some choose expressions from P , implementing them in Q using de\u00adterministic \ncode or possibly using additional choose expressions. We de.ne a set of program transformations that \na programmer is allowed to use for re.nements, and for which we give a suitably adjusted trace containment \nproperty. 4.1 Program transformations We de.ne three program transformations that, in combination, can \nbe used to determinize an angelic program. Two of the transformations (T1 and T3) are correctness-preserving, \ni.e., they preserve safe traces of an angelic program, while the correctness of T2 must be validated \nwith a checker (see Section 4.2). T1. State in.ation Consider a pair of angelic programs P and Q. We \nsay that Q is obtained from P by state in.ation if Q adds program variables and statements (including \nchoose) in such a way that Q does not alter the traces of P : there is no change in data or control dependence \nof any of the statements of P . Let proj P represent a projection of safe traces of Q to the choose operators \nin P (all of which are also in Q). Then, .I.lproj P (safetraces(Q(Il))) = safetraces(P (Il)) The role \nof this preparatory step is to introduce meta-data (a.k.a. book keeping ) required to implement a choose \nexpres\u00adsion; an initial version of the program may not have included all the meta-data it eventually \nneeds. T2. assert introduction If Q is obtained from P by adding an assert statement, where the (boolean) \nvariable being asserted is one that the program already de.nes, we have the property that: .l I.safetraces(Q(Il)) \n. safetraces(P (Il)) This is true because the additional assert can only convert a trace that was previously \nsafe to unsafe. T3. choose determinization If P contains choosev followed immediately by assert (v = \nw) for some w, then we can replace this pair of statements by assign vw to obtain program Q. If traces \nof P are projected to choose expressions that are common to P and Q (using proj Q) then: .l I.safetraces(Q(Il)) \n= proj Q(safetraces(P (Il))) This is true because the angel for choose v in P could have produced whatever \nvalue Q generates in variable w. Example 1. Suppose we are developing a program to reverse a linked list, \nfor which we hypothesize the following angelic program works. The correctness postconditions (assertions) \nare omitted for clarity. while (choose){ x= choose; y= choose; x.next = y; } Next, we might realize \nthat we need a cursor variable cur. We use T1 to introduce it. cur = in while (choose){ x= choose; y= \nchoose; cur = cur.next; x.next = y; } Now we add an assert using T2 to constrain one of the angelic \nchoices to use this cur variable. cur = in while (choose){ x= choose; assert x == cur; y= choose; cur \n= cur.next; x.next = y; } We can then use T3 to determinize and remove this choose expression. cur = \nin while (choose){ x = cur y= choose; cur = cur.next; x.next = y; }  4.2 Trace-based Re.nement Formally, \nwe say P is re.ned by Q, denoted P . Q, if 1. Q is derived from P using zero or more of the above trans\u00adformations, \nand, 2. Q is correct, i.e. .lI))= {}).  I.(safetraces(Q(l  The re.nement relation is re.exive and \ntransitive. Note that assert-introduction may cause the second condition to be violated. In this work, \nour intention is to use bounded\u00admodel-checking to establish the second condition. The goal of re.nement \nis to reduce the nondeterminism in a program. While state in.ation of P into Q can introduce into Q new \nchoose expressions, the non-determinism in Q with respect to P cannot increase with re.nement because \nwe do not reduce constraints on the choose expressions already present in P . This is because constraints \non nondeterminism are relaxed only when statements are removed (in T3) and we remove statements only \nwhen their effect is already captured by an assertion. Program development can be seen as creating a \nseries of programs, P0,...,Pn, such that Pi Pi+1, 0 = i<n, where P0 is the .rst angelic program a programmer \ncreates, and Pn is free of any choose statements, i.e. a suitable deterministic program that takes the \nintended computation steps. Existence of such a sequence does not mean that a program\u00admer will make only \ncorrect re.nements. In practice, a program\u00admer will occasionally need to revert to a previous version \nand try some other transformation. Adhering to the trace re.nement methodology brings valu\u00adable software \nengineering bene.ts to the practical programmer; we explain these in the next section.  4.3 Program \nRestructuring Sometimes a programmer might wish to re.ne a partial program in a way that is not expressible \nin terms of transformations T1 T3. Suppose s is a statement in P . In program Q, we wish to replace s \nwith a statement s '. We assume that s is choose-free but s ' is permitted to introduce new choose expressions.3 \nProgram Q ' will be a re.nement of P if Q is correct and s does not afford the choose operators in Q \nmore freedom than they had in P . The second condition corresponds to ensuring that Q does not allow \nnew traces: .l I.proj P (safetraces(Q(Il))) . safetraces(P (Il)) To ensure this property, we enforce, \nby construction, two properties: ' 1. The input values of s and s are the same in all safe execu\u00ad ' tions. \nAssume that s has the same free variables as s. This property ensures that any choose expressions executed \nprior to s are constrained in Q at least as much as they were in P (the constraints are the union of \nconstraints imposed by s and s ' , respectively). ' 2. The output values of s and s are the same in all \nsafe exe\u00ad ' cutions. Assume that s and s update identical output vari\u00ad ' ables. This ensures that s and \ns end up in the same state, ensuring that yet to be executed choose expressions in Q will be constrained \nat least as much as they were in P (the con\u00adstraints are the union of constraints of s and s ' , respectively). \n' Note that s is free to contain any implementation; we do not impose any constraints on the values computed \nat intermediate ' points in s . The next two transformations are intended to allow safe re\u00adstructuring. \nT4. Coupling The goal of the construction is to assert equality on input values to s and s ' , as well \nas on their output values. This 3 Note that occurrences of choose in the statement s can be removed by \nlifting all instances of choose into a special argument of s, a list angels, whose elements are read \nby s. is done by recording these values in P and asserting their equal\u00adity in Q. We call this transformation \ncoupling of programs P and Q. Assume we can establish communication channels between P and Q. Each channel \nsupports a snd(v) operation to send a value and a rcvandassert(v) operation that receives a value and \nasserts it to be equal to the value given as its argument. Just be\u00adfore executing s, P communicates all \nits input values (to s) to Q using a series of snd operations. Q receives them using rcvandassert operations \njust before entering s ', supplying its in\u00adput values in the same sequence. Likewise, just after executing \ns, P communicates its output values over to Q, which compares them against its own output values. T5. \nUncoupling When the re.nement process arrives at a choose-free program Q, the channels attached to Q \nare discon\u00adnected and the ancestor programs on the other end of the chan\u00adnels are discarded. Because \nQ is choose-free, the constraints im\u00adposed by the channels are not needed as they are already implicit \nin Q. Henceforth, we consider this construction as part of our re\u00ad.nement toolset. 5. Programming with \nAngels We assume that the programmer starts with a safe angelic pro\u00adgram. Note that he may start with \na relaxed correctness condi\u00adtion, strengthening it during the re.nement process. 5.1 Methodology As \nmentioned before, a programmer applies a succession of trace re.nement steps until a program does not \ncontain any choose expressions. Tool support helps in two ways: 1. Testing hypotheses. When the programmer \ncarries out a trans\u00adformation (T2 or T4) that adds constraints on the nondeter\u00administic choices, he checks \nwhether the resulting program maintains safety. If the program is not safe, the implemen\u00adtation strategy \nencoded in the angelic program is infeasible. In our implementation, this validation is performed with \na bounded model checker. 2. Trace demonstration. The tool outputs the safe traces, which the programmer \ncan examine to learn about the steps taken by the angel. In our case studies, these traces provided important \nclues as to what further re.nement steps to take.  We remind the reader that execution of the angelic \nprogram is carried out on a small suite of test cases, which means that we may fail to ascertain that \nan angelic program (or the .nal pro\u00adgram) is unsafe. As a result, it is possible that the programmer \nmakes incorrect re.nement decisions. In this sense, we are pro\u00adviding no more guarantees than the classical \n(test-driven) pro\u00adgram development on which we wish to improve. However, as corroborated by work on using \nbounded model checking for soft\u00adware validation [8], a small test suite often reveals many software errors. \nExample 2. This example illustrates that the programmer can discover that his implementation strategy \nis infeasible without necessarily having to obtain an incorrect angelic program. Con\u00adsider Example 1, \nwhere we partially re.ned an angelic program for linked list reversal. If our initial re.nement step \nin Example 1 had been slightly different, we would have produced following angelic program:  cur = in \nwhile (choose){ x = cur y= choose; x.next = y; cur = cur.next; } This program differs from the program \nproduced in Example 1 in that the last two statements inside the loop have been switched. It is easy \nto write a program like this one (we did), considering that programmers are used to incrementing the \nloop induction variable at the very end of the loop. This angelic program encodes an infeasible strategy. \nSpecif\u00adically, the program contains a bug in that it overwrites cur.next before the old value of cur.next \nis read and used to obtain the next node. As a result, the angelic program is prevented from walking \nforward down the list. The angelic program is not incor\u00adrect, though, because some safe traces remain. \nFor instance, in the .rst loop iteration, the program can travel to the last node and then walk the list \nbackwards. The programmer can notice that no safe traces correspond to a desired algorithm by observ\u00ading \nthat all traces require at least n +1 steps for a list with n elements. If desired, this observation \ncan be con.rmed by limit\u00ading the angelic program to n loop iterations, at which point no safe traces \nremain. The programmer then reverts to an earlier ver\u00adsion of the program and corrects the implementation \nof how the induction variable cur is advanced through the list.  5.2 Removing angelic dependences Informally, \nwe say a choose expression c1 is independent from a choose expression c2 if c1 can make its choices without \nconsid\u00adering what choice has been (or will be) made by c2 in a given ex\u00adecution. Angelic programs with \nonly independent choose expres\u00adsions may lead to simpler algorithms because there is no need to deterministically \nimplement the communication that happens between dependent choose expressions. We leave the full charac\u00adterization \nof angelic dependence for future work. Here, we give only a suf.cient condition for independence and \ndiscuss one of its bene.ts. A suf.cient condition for independence of all choose expres\u00adsions in a program \nis that the program has only one safe trace per input. If a choose expression has the choice of only \none value at any time it is evaluated, it need not consider the choices made by other angels. When angelic \nchoices are independent we get the bene.t that choose expressions can be re.ned independently of each \nother. That is, the implementation need not consider how the other choices are determinized, as the next \nexample illustrates. Example 3. In Example 2, by examining traces returned by the oracle, the programmer \nrecognized that it might be useful to enforce that the loop that reverses the elements in a linked list \nexecutes n times for a list with n elements. He might enforce this by writing the following program (with \nthe bug from Example 2 .xed). cur = in while (n) (choose){ x = cur y= choose; cur = cur.next; x.next \n= y; } Here, n is the number of elements in the list and while (n) is syntactic sugar for a loop that \nperforms no more than n iterations. This program has exactly one trace (to see this, note that each of \nthe n nodes in the list must have its next .eld overwritten, and by design the loop iterates at most \nn times, so to generate a safe trace the oracle must set each node s next .eld to the cor\u00adrect .nal value). \nWith this, we are guaranteed to have removed the angelic dependence mentioned above. The programmer can \nthus consider implementing the two remaining angelic choices separately. The loop condition is relatively \neasy: we know that we must ensure that cur is not null, and by examining the traces returned by the oracle \nwe can see that this is indeed the correct condition. We can thus use T2 and T3 to generate the following \nprogram. cur = in while (n) (cur != null){ x = cur; y= choose; cur = cur.next; x.next = y; } For the \nremaining angelic choice, by examining the safe trace we notice that y is always chosen as the element \nbefore cur in the original list. With this insight, we can recognize that we must use T1 to introduce \na new variable and have it walk down the list (as well as remove the now-unnecessary bounded loop), T2 \nto assert that y is chosen to be equal to it, and then T3 to generate the following fully-deterministic \ncorrect program. cur = in prev = null while (cur != null){ x = cur; y = prev; cur = cur.next; x.next \n= y; prev = x; }  5.3 Example of Program Restructuring The next example illustrates transformations \nT4 and T5. Recall that, in contrast to T1 T3, the transformation T4 does not im\u00adplement an instance of \nchoose, but instead substitutes some de\u00adterministic code in program P with another (potentially angelic) \ncode. The constraints on nondeterminism present in P are im\u00adposed on Q using communication channels between \nthe two pro\u00adgrams. T5 is a cleanup transformation. Example 4. We partially develop an implementation \nof the ZipReverse problem assigned by Olivier Danvy at a sum\u00admer school. Given two lists x and y, the \nproblem is to com\u00adpute zip(x, reverse(y)). For example, given x = [1, 2, 3, 4] and y = [a, b, c, d], \nthe program outputs [(1,d), (2,c), (3,b), (4,a)]. The requirements are: neither list can be traversed \nmore than once; the lists cannot be copied; the length of the lists is not known a priori; and lists \nmust be manipulated with the low-level operations car, cdr and cons. The .rst angelic program tests a \nhypothesis that the result can be computed with a sequence of cons operations. When x is a list, choose(x) \nselects an element from the list.  r = nil while (choose){ r = cons((choose(x), choose(y)), r) } The \nhypothesis tested by the program is admittedly trivial but the program is nonetheless a useful start \nfor re.nement. This is because it has only one safe trace for each input. Considering that a list can \nonly be created with cons, this trace shows the steps that .nal implementation will have to take. The \nprogrammer then attempts to re.ne this program by im\u00adplementing choose(x) and choose(y) with code that \ntraverses the two lists by maintaining pointers that advance down the lists. The lists are traversed \naccording to the restrictions of the problem statement but nondeterminism allows all legal traversals. \nThis attempt at re.nement, not shown, has no safe trace, so the pro\u00adgrammer concludes that a recursive \nprocedure needs to be used in place of the iterative one. This recursive angelic program, shown below, \ncan be re.ned into a deterministic program; thanks to recursion, the list y can be traversed in the opposite \ndirection, which was not possible in the iterative version. r = nil descent() def descent() { if (choose) \nreturn descent() r = cons((choose(x), choose(y)), r) } How is this program a re.nement of the .rst program? \nThe programmer can make it so by invoking transformation T4. The program that couples the two programs \ncan be written as follows: r = nil r = nil descent() ch1 = choose snd(ch1) def descent() { while (ch1) \n{ ch1 = choose ch2 = choose(x) rcvandassert(ch1) snd(ch2) if (ch1) return ch3 = choose(y) descent() \n|| snd(ch3) ch2 = choose(x) r = cons((ch2, ch3), r) rcvandassert(ch2) ch1 = choose ch3 = choose(y) snd(ch1) \nrcvandassert(ch3) } r = cons((ch2, ch3), r) ch4 = pack(r) } snd(ch4) ch4 = pack(r) rcvandassert(ch4) \nThe program on the left is the .rst program, with inserted channel send operations; on the right is the \nsecond program, with inserted channel receives. The output of the statement that we have replaced with \nT4 is the list r, which is packed with pack(r) before it is sent across a channel. (The output of the \nsecond program is considered to be the output of the entire program.) Super.cially, we violated the conditions \nof T4: the program on the the left contains choose statements while T4 requires that the replaced statement \nbe choose-free. However, conceptually we could have collected angelic values in advance in an array angels \nand read from it. Once the program on the right is developed into deterministic code (not shown for lack \nof space), we can use T5 to disconnect the program on the left and remove the channels. 6. Comparison \nto Deductive Re.nement We brie.y review Morgan s [11] re.nement methodology, and then contrast it with \nours. We note at the outset that Morgan s methodology is geared towards developing a proof of a program \nas it is being developed, whereas our intention is to help the programmer develop a program that is correct \nwithin the con.nes of bounded model checking. In Morgan s methodology, one develops programs using re\u00ad.nement \nof speci.cation statements. A speci.cation statement is of the form lv :[pre, post], and it can be used \nany place an or\u00addinary statement may be used. The meaning of this statement is that, if executed in a \nstate in which pre holds, it modi.es state variables in lv such that post holds. The speci.cation statement \nis a demonic non-deterministic statement: all values that satisfy post must suf.ce. The wp-semantics \nof a speci.cation statement are given as: wp(lv[pre, post],R)= pre . (.lv.post =. R) Re.nement of a speci.cation \nstatement is either a determin\u00adistic statement or another speci.cation statement that could be used in \nplace of the original one. For P to be correctly re.ned by Q, denoted P = Q, .R, wp(P, R)=. wp(Q, R) \nwhere wp is the weakest precondition operator and R is a post\u00adcondition. Semantically, our de.nition \nof trace-based re.nement ( ) is a special case of the re.nement de.ned above (=). In Morgan s methodology, \na programmer makes a sequence of re.nement steps until the program is completely free of non\u00addeterministic \nstatements. Rather than prove correctness of re.ne\u00adment steps from the principle above, the programmer \ncan use a set of proven re.nement laws, some of which we show here: It is always legal to strengthen \nthe postcondition or weaken the precondition.  One of the laws for introduction of a sequential composition \nrequires a suitable intermediate condition (mid below): lv : [pre, post] -. lv :[pre, mid]; lv :[mid, \npost]. The mid should be suitable in that further re.nement of the two new speci.cation statements is \nfeasible.  The law for introduction of a loop requires a speci.cation to be brought in of the form: \n[pre . I, post . I], where I is a suitable loop invariant. (We omit the actual transformation; see [11].) \n An attractive property of Morgan s re.nement is that occur\u00adrences of speci.cation statements in a program \ncan be re.ned independently of each other (Theorem 2, pg 8, in Morgan and Vickers [12]). This makes the \napproach compositional. However, from the perspective of a practical programmer, this is also the main \nproblem in using Morgan s methodology: in\u00addividual speci.cation statements must have suf.ciently power\u00adful \npostconditions and correct invariants for the programmer to make progress. If the programmer does not \n.gure out the correct loop invariant, the methodology might get stuck later during de\u00advelopment, and \neven the correct invariant must often be modi.ed in later steps. This is not surprising: a programmer \nis developing a rigorous proof simultaneously with the program. Angelic speci.cation statements have \nalso been suggested in the literature (e.g. Celiku and Wright [4], Ward and Hayes [20]). Ward and Hayes \nused angelic re.nement to prove correctness of backtracking algorithms. Let us denote an angelic speci.cation \nstatement as lv : {pre, post}, meaning that the lv must be assigned in such a way that post holds, and \nmoreover, the values are cho\u00adsen cooperatively to make the rest of the program run correctly. Formally, \n wp(lv{pre, post},R)= pre . (.lv.post . R) At .rst glance, this seems to free the programmer from the \nnecessity of providing suitable post, because the angel would al\u00adways select a value that suits the rest \nof the computation (in addi\u00adtion to satisfying whatever postcondition is provided manifestly in the speci.cation \nstatement.). However, local re.nement of an\u00adgelic speci.cations, i.e. a re.nement that can be applied \noblivi\u00adous to the rest of the program, is only allowed to increase choices available to the angel. This \ndoes not help the programmer to get to a deterministic program. (As stated above, their purpose was to \nprove correctness of backtracking algorithms.) Celiku and Wright propose strengthening the postconditions \nof angelic speci.cations in such a way that they essentially be\u00adcome demonic speci.cations, which can \nthen be re.ned using Morgan-like methodology. The process of converting angelic to demonic speci.cation \nrequires manual, non-local, whole pro\u00adgram reasoning. More formally, the following conversion can be \ncarried out for a strong enough f: lv{pre, post}; assert f -. lv[pre, post . f] The problem for the programmer \nis to determine the f that would capture the expectation of the rest of the program. Our methodology \nis intended as a practical tool for program development, not as a proof procedure. In our methodology, \nwe do not have compositionality: by default, angelic choose expres\u00adsions cannot be re.ned obliviously \nto each other. (Removing an\u00adgelic correlation, as we discussed in Section 5.2, is similar to converting \nangelic to demonic nondeterminism.) 7. Implementation We have embedded the angelic choice construct into \nthe Scala programming language [13]. The programmer passes to the an\u00ad gelic choice operator a list of \nvalues from which a parallel back\u00adtracking solver selects one that leads to a safe trace. The solver \ncomputes all safe traces, which can then be browsed with a user interface. The angelic choice operator \nranges over arbitrary prim\u00aditives or references to heap-allocated variables and can be used on arbitrary \nprograms. We have used our implementation to de\u00advelop the examples in this paper as well as several others. \nThe rest of this section discusses our backtracking solver, as well as a more scalable SAT solver. We \nconclude by comparing their scal\u00adability. 7.1 Parallel Backtracking Solver Our backtracking solver performs \na depth-.rst traversal over the space of traces, searching for safe traces. The solver executes a program \nand whenever it encounters a choice operator, it pushes a new entry corresponding to the dynamic instance \nof this opera\u00adtor to a stack. It then tries the .rst value for the entry on the top of the stack by executing \nthe program further. On an assertion fail\u00adure, the solver backtracks to the execution point associated \nwith the top of the stack and tries the next value. If there are no values left for this entry, it is \npopped from the stack and the execution backtracks to the execution point corresponding to the previous \nentry on the stack, continuing the process. A safe trace is found when the execution terminates without \nfailing an assertion. Figure 1. Time taken (in seconds) by the SAT solver and the backtracking solver \nwhen inserting a number of nodes into an initially empty binary tree. Our backtracking solver explores \nthe trace space in parallel. Each task receives from the dispatcher a pre.x of a trace, includ\u00ading the \ncorresponding stack that captures the state of the angelic operators executed on that pre.x. The stack \ncaptures which val\u00adues have already been tried for these operators, and it is thus an ef.cient way of \nrepresenting the part of the space that has already been explored. Each parallel task is responsible \nfor exploring all suf.xes of the pre.x. Except for communicating with the dis\u00adpatcher, the tasks are \nindependent, so the parallel computation is very ef.cient. Our solver is ef.cient in its memory use so \nmost of its time is spent executing the program. Backtracking is not a good .t for problems that contain \nas\u00adsertions mostly at the end of the execution because the search takes a long time to encounter a failure. \nSAT solvers, on the other hand, propagate constraints and can generate new con\u00adstraints (con.ict clauses) \nas they explore the search space. The backtracking solver s scalability on these problems can be im\u00adproved \nby adding assertions with intermediate invariants, which can drastically prune the search space. 7.2 \nSAT-based Angelic Solver Our second solver is based on symbolically unrolling the pro\u00adgram for given \ninputs and representing the bounded execution as a SAT formula. The satisfying assignment then gives \nthe val\u00adues of the angelic operators on these inputs. We use the pro\u00adgram translator developed as part \nof the SKETCH synthesizer project [18], which achieves scalability by representing integers in sparse \nunary encoding, thus optimizing the SAT formula for the small values that one is likely to see during \nprogram devel\u00adopment based on unit testing. This solver is not yet connected to our Scala-based frontend \nbut angelic programs for that solver can be written in the SKETCH language by using simple macros.  \n7.3 Solver Ef.ciency Study All experiments presented in this paper were performed on the backtracking \nsolver, whose scalability was suf.cient (its response was usually interactive). It is interesting to \nevaluate whether the SAT-based angelic solver allows us to scale to harder problems. While individual \nproblems vary widely, some exper\u00adimental performance numbers may be helpful to compare the scalability \nof angelic solver techniques. In Figure 1, the time of the backtracking and SAT solvers is shown on an \nangelic program that inserts nodes into a binary search tree. The an\u00adgel chooses any node in the tree \nand then chooses whether to insert the new node below the left child or the right child. As\u00adsuming that \nwe only insert into nodes already in the tree, this is a search space of 2n-1 n!, where n is the number \nof nodes to insert. There is only one correct solution in this space. The backtracking solver is usable \nuntil the search space is about 109 , while the SAT-based angelic solver scales to 8 * 1020, or more \nthan ten orders of magnitude higher. This gives us hope that an\u00adgelic re.nement based on executable agents \nmay be usable on a range of realistic programs.  8. Case Study: Modularizing DSW This section describes \na case study where angelic programming aided in modularizing the Deutsch-Schorr-Waite (DSW) graph marking \nalgorithm [15]. This algorithm has long been consid\u00ad ered one of the most challenging pointer algorithms \nto reason about [2]. We modularize DSW with a new generic abstraction, called the parasitic stack, which \nis parameterized for DSW with angelic operators. Our case study shows that (i) modulariza\u00adtion of DSW \nsimpli.es reasoning about the algorithm because the parasitic stack separates concerns intertwined in \nDSW; and (ii) choose operators allow the algorithm designer to sidestep global reasoning. We conjecture \nthat modularization by angel\u00adically parameterizing an abstraction may be applicable more widely to other \ncomplex algorithms. The DSW algorithm solves the problem of marking nodes reachable from the root of \na directed graph. The crucial re\u00adquirement is to do so with only constant-size additional stor\u00adage. (One \nexception is that a node can store an index into its list of children.) This requirement is motivated \nby garbage col\u00adlection where object tracing is invoked when the system cannot offer more than constant-size \nmemory. If linear-size additional memory were available, one could perform a DFS traversal of the graph, \nusing a stack for backtracking. The DSW trick is to guide the DFS traversal by temporarily rewiring child \npoint\u00aders in nodes being visited. The price for the space ef.ciency is intertwining of the two parts \nof the DFS traversal: Iterative graph traversal. Because recursive DFS traversal is ruled out, DSW relies \non the more involved iterative traversal.  Backtracking structure. Because an explicit stack is ruled \nout, DSW encodes a backtracking structure in the child pointers of graph nodes.  Ideally, these aspects \nshould be separated, for example by hid\u00ading the backtracking structure under an abstraction. However, \nas shown in Figure 2, this separation seems dif.cult. We do not ask the reader to understand the algorithm \nin Figure 2; we merely want to point out the essence of what complicates modularization. In particular, \nnote that the memory location current.children[current.idx] serves two roles: on the right-hand side \nof the .rst parallel assignment, the location stores an edge of the graph; on the left-hand side, it \nserves as storage for the backtracking structure. Because the role of the location changes in the middle \nof an assignment, it is not obvious how to hide one role under a procedural abstraction. The original \ngoal of our case study was not to modularize DSW; we started by asking simply whether angelic program\u00adming \ncould aid in discovering and implementing the classical (.at) DSW algorithm. We explained the trick in \nDSW to a few students and then gave them access to the Scala language ex\u00adtended with choose operators. \nThe insight was explained by telling them to use the child .elds in the graph nodes to encode the backtracking \nstack needed in DFS traversal. As students ex\u00adplored algorithmic ideas, it became clear that it was dif.cult \nto design DSW even with angelic nondeterminism. The reason was that the programmer found it hard to explain \nhow the angels ma\u00ad def DSW(g) { val vroot = new Node(g.root) var up, current = vroot, g.root while (current \n!= vroot) { if (!current.visited) current.visited = true if (current has unvisited children) { current.idx \n= index of .rst unvisited child // the child slot changes roles in this assignment up, current, current.children[current.idx] \n= current, current.children[current.idx], up } else { // the child slot restores its role up, current, \nup.children[up.idx] = up.children[up.idx], up, current } } } Figure 2. The classical (.at) DSW algorithm. \ndef DSW(g) { val vroot = new Node(g.root) var current = g.root ParasiticStack.push(vroot, List(vroot,g.root)) \nwhile (current != vroot) { if (!current.visited) current.visited = true if (current has unvisited children) \n{ current.idx = index of .rst unvisited child val child = current.children[current.idx] ParasiticStack.push(current, \nList(current, child)) current = child } else { current = ParasiticStack.pop(List(current)) } } } Figure \n3. The parasitic (modular) DSW algorithm. nipulated the individual pointer .elds. It became apparent \nthat it was necessary to raise the level of the programming abstraction so that one could observe the \nangelic decisions at a more mean\u00adingful semantic level. Unfortunately, as we pointed out, DSW does not \nseem to permit such modularization. It occurred to us that it might be possible to express DSW as using \na special stack-like data structure. Like a regular stack, this data structure would support the push \nand pop operations with the usual semantics. Unlike a regular stack, this data structure would be implemented \nby borrowing memory locations from its host in our case, the graph being traversed. The hope was that \nthe metaphor of borrowing a location would allow us to express the transition between roles in a systematic \nfashion. We termed this data structure a parasitic stack because it borrows locations from its hosts \nand eventually restores them (parasites do not destroy their host). The challenge was how to express \nDSW with a parasitic stack, if that was at all possible. The parasitic stack has the interface shown \nbelow. As usual, the pop operation returns the value x stored in the corresponding push operation. The \nnodes argument to push passes into the stack the environment of the parasitic stack s client. Through \nthis ar\u00adgument, the traversal code loans the nodes referenced by in\u00adscope variables to the stack. The \nenvironment is also passed into pop, where the values may be useful for restoring the values of  ParasiticStack \n{ e= new Location // constant-size storage (one location su.ces to support DSW) push(x,nodes) { // nodes \nis the set of nodes o.ered \"on loan\" by the host data structure n= choose(nodes) // angelically select \nwhich node to borrow from the host ... c= choose(n.children.length) // ... and which child slot in that \nnode to use as the location n.idx2 = c // remember the index of the borrowed child slot v = n.children[n.idx2] \n// read the value in the borrowed location; it may be needed for restoring the borrowed slot in pop() \n// angelically select values to store in the two locations available to us ( e and the borrowed location) \ne, n.children[n.idx2] = angelicallySemiPermute(x, n, v, e) } pop(nodes) {  n= choose(nodes, e) // the \nborrowed location better be in either e or in nodes v = n.children[n.idx2] // v is the value we stored \nin the borrowed location // select what value to return and update the two locations we work with (the \nborrowed child slot and e ) r, n.children[n.idx2], e = angelicallySemiPermute(n, v, e, *nodes) // *nodes \nunpacks arguments from list nodes return r } } Figure 4. An angelic implementation of the parasitic \nstack, ParasiticStack0. borrowed memory locations when the stack returns them to the host. // parasitic \nstack can borrow a .eld in some node from nodes push(x:Node, nodes:List[Node]) // values in nodes may \nbe useful in returning storage to host pop(nodes:List[Node]) : Node With the parasitic stack in hand, \nDSW can be expressed as shown in Figure 3; it can be derived almost mechanically from a recursive DFS \ntraversal procedure. If one ignores the additional arguments to push and pop, this parasitic DSW appears \nto use a regular stack that has private storage. The parasitic DSW is thus modular in that it abstracts \naway the details of how the parasitic stack is implemented. Note that at this point of our user study, \nwe did not yet know whether DSW was expressible on top of such a stack interface. The challenge behind \nanswering this question was to determine which location the parasitic stack can borrow from the host \nit must be a location the host does not need until the location is returned to the host by the parasitic \nstack;  how to restore the value in this location when returning it to the host; and  how to use this \nlocation to implement the push/pop interface.  These three questions are formulated as nondeterministic \nchoices in the implementation of the parasitic stack. We were not able to answer these questions without \nangelic help. The reader might wonder if we simply substituted one hard problem, namely implementing \nDSW using low-level pointer manipulations, with another equally hard one, namely imple\u00admenting the parasitic \nstack. What we achieved is that an angelic formulation of the three questions (for parasitic stack) is \nrela\u00adtively straightforward. Furthermore, we found that the angelic answers to these questions can be \ninterpreted by the program\u00admer more easily than when the angels implement the low-level DSW pointer manipulations \nbecause the questions raise the level of abstraction. The three questions are encoded in the angelic \nimplementa\u00adtion of the parasitic stack, shown in Figure 4. As we discuss this code, it will be obvious \nthat the three questions, answered by angels, capture the global reasoning necessary to make the parasitic \nstack operate correctly. As a result, the programmer is freed to constrain himself to local reasoning, \nwhose goal is to de\u00adscribe how a generic parasitic stack might operate. Genericity is achieved with angelic \nnondeterminism. The next two paragraphs describe the angelic parasitic stack. We then use re.nement to \nparameterize this angelic parasitic stack for DSW. The parasitic stack in Figure 4 keeps only a single \nmemory lo\u00adcation (e). The push method .rst angelically selects which mem\u00adory location to borrow from \nthe host. This is done by selecting a suitable node n and a child slot c in that node. The borrowed location \nis n.children[c]. The stack (deterministically) stores the selected child slot index in the node itself, \nas that is allowed by the constraints of the DSW problem. Next, push reads the value in the borrowed \nlocation since it will need to be restored later and so may need to be saved. Finally, there is a hard \ndecision. The stack has four values that it may need to remember: the pushed value x, the reference to \nthe borrowed location n, the value in that location v, and the value in the extra location e. However, \nthere are only two locations available to the stack: the borrowed location n and the extra location e. \nClearly, only two of the four values can be stored. Perhaps the value of e is not needed, but the remaining \nthree values are essential. A little reasoning reveals that the parasitic stack is plausible only if \nthe value that push must throw away is available at the time of pop from the variables of the enclosing \ntraversal code. Therefore, we decided to make the environment of the client available to pop. The pop \nmethod .rst guesses which location was borrowed in the corresponding push. This location is either n \nor is in nodes; no other alternatives exist. Next, pop reads the value from the borrowed location. Finally, \npop angelically decides (i) which value to return, (ii) how to update the extra locations, and (iii) \nhow to restore the borrowed location. As in the case of push, it must select from among four values. \nThe bene.ts of clairvoyance should now be clear: while the human found it hard to make the global decisions \nnecessary to instantiate the parasitic stack for DSW, these decisions were rather straightforward to \nformulate as nondeterministic choices. Next, we instantiated the parasitic stack for DSW by re.n\u00ading \nit into a deterministic program. To arrive at the .rst re.ne\u00adment, we observed how the angels permuted \nthe values. We iden\u00adti.ed a trace in which the angels performed the same permuta\u00adtion throughout the \nentire execution, except in the .rst call to push. This .rst call was an outlier because our parasitic \nDSW algorithm (Figure 3) originally invoked the .rst push incorrectly, with the reversed environment, \nas follows:  ParasiticStack.push(current, List(g.root,vroot)). After we modi.ed the parasitic DSW, we \nwere able to implement the permutations with deterministic code, shown below. We no\u00adticed that the value \nv of the borrowed location was not saved by the angel and that it was later obtained from nodes[0] in \npop. This answered the question of how to restore the value in the borrowed location. ParasiticStack1 \n re.nes ParasiticStack0 ParasiticStack { e= new Location push(x,nodes) { n= choose(nodes) c= choose(n.children.length) \nn.idx2 = c // rhs was angelicallySemiPermute(x, v, e, n) e, n.children[n.idx2] = x, e } pop(nodes) { \n n = e // rhs was choose(nodes, e) v = n.children[n.idx2] // rhs was angelicallySemiPermute(n, v, e, \n*nodes) r, n.children[n.idx2], e = e, nodes[0], v return r } } To arrive at the second re.nement, we \nobserved how the angels selected the borrowed node (the value n in push). We tested if the choice was \nconsistent across all instances of push (it was), and then we implemented the angelic choice. ParasiticStack2 \n re.nes ParasiticStack1 ParasiticStack { e= new Location push(x,nodes) { n = nodes[0] // rhs was choose(nodes) \nc= choose(n.children.length) n.idx2 = c e, n.children[n.idx2] = x, e } pop(nodes) { ... unchanged ... \n} } To perform the last re.nement step, we examined how the re\u00admaining angel selected the child slot \nto borrow from the selected node n. We learned that it selected a slot whose value equaled nodes[1], \nthe second variable passed to push from the traversal code. This implied that c equaled the value of \nn.idx maintained in the traversal code. Therefore, maintaining a separate .eld n.idx2 was not necessary. \nWe turned this observation into the determin\u00adistic code shown below. This completed the construction \nof par\u00adasitic DSW. ParasiticStack3 re.nes ParasiticStack2 ParasiticStack { e= new Location push(x,nodes) \n{ n = nodes[0] // invariant: n.children[c] == nodes[1], // hence c == n.idx == n.idx2 // was c = choose(n.children.length); \nn.idx2 = c e, n.children[n.idx] = x, e } pop(nodes) { n=e v = n.children[n.idx] // was n.idx2 r, n.children[n.idx], \ne = e, nodes[0], v return r } } Angelic nondeterminism has simpli.ed implementation of the DSW algorithm. \nThe parasitic DSW still takes some ef\u00adfort to understand but the comprehension task has been broken down \ninto three smaller questions: how the stack is implemented, which location the stack is borrowing, and \nhow its value is re\u00adstored. It would be interesting to consider whether this modular\u00adization of DSW leads \nto a simpler deductive proof of correctness. 9. Related Work Floyd was one of the .rst to propose using \nangelic nondetermin\u00adism as a programming construct [7]. The concept appears also in formal languages, \ne.g., in nondeterministic automata [14]. An\u00ad gelic non-determinism also allowed abstracting speci.cations \nby non-deterministically coordinating concurrent components [16]. In the rest of this section, we focus \non work related to systematic program development. J-R. Abrial has presented a very successful methodology \nfor construction of event-based systems [1] and showed that the same methodology works for pointer manipulating \nprograms [2]. Abrial s methodology starts with a declarative speci.cation of the desired program and \ngives a set of laws by which speci.\u00adcations can be translated to lower-level speci.cations until they \nreach the level of basic program statements orchestrated by the usual control-.ow constructs. At each \nstep, his methodology asks a programmer to discharge a number of proof obligations, some of which can \nbe automated. However, the methodology is intended primarily for programmers who are well-versed in the \nuse of automated theorem provers. His derivation of DSW does not modularize the constituent concepts \nof the backtracking structure, the graph, and the traver\u00adsal order [2]. He de.nes the backtracking structure \nas closely tied with the traversal order: the structure is a list of nodes cur\u00adrently being visited. \nThis characterization is then re.ned by im\u00adplementing the list as a rewiring of the graph. Therefore, \nthe content of the structure remains intimately linked with the graph structure. Angelic nondeterminism \nhas been used in re.nement-based program development methodology proposed by Back, Wright, Morgan, and \nothers. Back and von Wright [3] simpli.ed problem decomposition in program re.nement [11], using angelic \nnonde\u00ad terminism to satisfy intermediate conditions that the programmer would be able to spell out only \nlater in the development process. In contrast to deductive angelic re.nement, our angelic pro\u00adgrams are \nexecutable, as in Floyd [7]. This allows a test of the correctness of angelic programs. If the test fails, \nwe have con\u00adclusively proved that the angelic program is incorrect and cannot be successfully re.ned. \nA passing test is not conclusive (we rely on bounded model checking [5]), but the executable angel shows \nus a demonstration of how to execute the program. Safe angelic traces thus serve as demonstrations of \nwhat steps the angel takes to execute the program, which has the bene.t of increasing pro\u00adgram understanding. \n Celiku and Wright [4] show how to re.ne angelic nondeter\u00ad ministic statements to demonic ones, with \nthe goal of being able to re.ne them independently. In order to achieve this goal, they prove, manually, \nsuf.cient postconditions that an angelic state\u00adment must satisfy. In general, however, angelic correctness \nis es\u00adtablished with a proof that is obtained by effectively completing the re.nement process all the \nway to the deterministic program. While such a deductive process provides a proof of the .nal program, \nit does not seem to enhance programmer productivity. Our approach changes the problem: we side-step the \nintermedi\u00adate stage of obtaining an equivalent demonic nondeterministic program. Instead, we aim to re.ne \nto a deterministic program directly, which we achieve by making the choose operator exe\u00adcutable. There \nhas been a long tradition of speci.cation-based pro\u00adgramming at Oxford University, and Morgan s work \nrepresents that school of thought. The Z programming methodology [19] is closely related to that of Morgan. \nA complete description of re\u00adlated work in this tradition is well outside the scope of this paper, but \n[11] is a good reference. The work presented in this paper can be viewed as an evolu\u00adtion of the SKETCH \nproject [18, 17]. In SKETCH, a program\u00ad mer leaves syntactic holes in a program, which can later be .lled \nautomatically by a family of expressions. SKETCH has been shown to work very well in several application \ndomains, in particular bit-manipulating programs, in which it is easy to give a speci.cation of an unoptimized \nprogram but dif.cult to de\u00advelop a correct optimized program. One of the limitations of the SKETCH work \nis that it requires a substantial amount of work on the part of a programmer to write the incomplete \nprogram, since holes are substitutes for only a very limited family of ex\u00adpressions. This not only is \nwork for the programmer, but it also creates the possibility of making human mistakes that can cause \na SKETCH synthesizer to not be able to .ll in the holes. The present work addresses both criticisms. \nThe work presented in this paper was in part inspired by the work of Lau et al., who synthesize editing \nmacro programs from user demonstrations of executions (i.e., traces) of the desired macros [9]. We realized \nthat such demonstrations are useful not only for synthesis but also for gaining understanding about an \nalgorithm that is under construction. In fact, if an oracle gave the programmers a trace of the program \nthat they are developing, they might .nd it easier to debug the program as well as develop it in the \n.rst place. Program development could then be viewed as generalizing the algorithm from demonstrations. \nOur next observation was that such oracular traces could be created by angelic programs, which led to \nthe work presented in this paper. 10. Conclusion We have demonstrated that an executable implementation \nof an\u00adgelic nondeterminism may help in program development. First, the programmer can evaluate hypotheses \nabout his implementa\u00adtion strategy by testing whether his incomplete program can be executed angelically. \nIf angelic operators cannot complete the program with values that lead to a successful execution, neither \nwill be the programmer, and so the program follows an infeasi\u00adble implementation strategy. Second, if \nan incomplete program can be completed angelically, the successful angelic executions may reveal steps \nthat the incomplete algorithm should follow. The traces computed by the angelic operators may thus lead \nthe programmer to an a-ha moment. Third, angelic operators support re.nement-based programming, where \nthe programmer develops the program gradually, by replacing angelic operators with pro\u00adgressively more \ndeterministic implementations. In contrast to angelic nondeterminism that is purely a proof device, the \nability to execute, test and observe angelic programs allows the programmer to re.ne programs and develop \nabstrac\u00adtions, such as the parasitic stack, that seem dif.cult for humans without the computational power \nof the oracle. References [1] J.-R. Abrial. The B Book: Assigning Programs to Meanings. Cambridge Un. \nPress, Aug. 1996. [2] J.-R. Abrial. Event based sequential program development: Application to constructing \na pointer program. In FME, pages 51 74, 2003. [3] R.-J. Back and J. von Wright. Contracts, games, and \nre.nement. Inf. Comput, 156(1-2):25 45, 2000. [4] O. Celiku and J. von Wright. Implementing angelic nondeterminism. \nIn APSEC, pages 176 185. IEEE Computer Society, 2003. [5] E. M. Clarke, O. Grumberg, and D. A. Peled. \nModel Checking. The MIT Press, Cambridge, Massachusetts, 1999. [6] E. W. Dijkstra. A Discipline of Programming. \nPrentice Hall PTR, Upper Saddle River, NJ, USA, 1997. [7] R. W. Floyd. Nondeterministic algorithms. Journal \nof the ACM, 14(4):636 644, oct 1967. [8] M. F. Frias, C. G. L\u00f3pez Pombo, G. A. Baum, N. M. Aguirre, and \nT. S. E. Maibaum. Reasoning about static and dynamic properties in alloy: A purely relational approach. \nACM Trans. Softw. Eng. Methodol., 14(4):478 526, 2005. [9] T. A. Lau, P. Domingos, and D. S. Weld. Learning \nprograms from traces using version space algebra. In K-CAP, pages 36 43, 2003. [10] C. L. McMaster. An \nanalysis of algorithms for the dutch national .ag problem. Commun. ACM, 21(10):842 846, 1978. [11] C. \nMorgan. Programming from Speci.cations. 1998. [12] C. Morgan and T. Vickers, editors. On the Re.nement \nCalculus. Springer-Verlag, London, 1992. [13] M. Odersky and al. An overview of the scala programming \nlanguage. Technical Report IC/2004/64, EPFL Lausanne, Switzerland, 2004. [14] M. O. Rabin and D. S. Scott. \nFinite automata and their decision problems. IBM J. Res. and Develop, 3:114 125, 1959. [15] H. Schorr \nand W. Waite. An ef.cient machine independent procedure for garbage collection in various list structures. \nCommunications of the ACM, 10(8):501 506, Aug. 1967. [16] G. Smith and J. Derrick. Abstract speci.cation \nin object-Z and CSP. In C. George and H. Miao, editors, Formal Methods and Software Engineering, volume \n2495 of Lecture Notes in Computer Science, pages 108 119. Springer, Nov. 2002. [17] A. Solar-Lezama, \nG. Arnold, L. Tancau, R. Bodik, V. Saraswat, and S. Seshia. Sketching stencils. In PLDI 07: Proceedings \nof the 2007 ACM SIGPLAN conference on Programming language design and implementation, pages 167 178, \nNew York, NY, USA, 2007. ACM. [18] A. Solar-Lezama, L. Tancau, R. Bodik, S. Seshia, and V. Saraswat. \nCombinatorial sketching for .nite programs. SIGPLAN Not., 41(11):404 415, 2006. [19] J. M. Spivey. Understanding \nZ: a speci.cation language and its formal semantics. Cambridge Un. Press, New York, NY, USA, 1988. [20] \nN. Ward and I. J. Hayes. Applications of angelic nondeterminism. In P. A. Bailes, editor, Proc. 6th Australian \nSoftware Engineering Conference (ASWEC91), pages 391 404. Australian Computer Society, JUL 1991.   \n \n\t\t\t", "proc_id": "1706299", "abstract": "<p>Angelic nondeterminism can play an important role in program development. It simplifies specifications, for example in deriving programs with a refinement calculus; it is the formal basis of regular expressions; and Floyd relied on it to concisely express backtracking algorithms such as N-queens.</p> <p>We show that angelic nondeterminism is also useful during the development of deterministic programs. The semantics of our angelic operator are the same as Floyd's but we use it as a substitute for yet-to-be-written deterministic code; the final program is fully deterministic. The angelic operator divines a value that makes the program meet its specification, if possible. Because the operator is executable, it allows the programmer to test incomplete programs: if a program has no safe execution, it is already incorrect; if a program does have a safe execution, the execution may reveal an implementation strategy to the programmer.</p> <p>We introduce refinement-based angelic programming, describe our embedding of angelic operators into Scala, report on our implementation with bounded model checking, and describe our experience with two case studies. In one of the studies, we use angelic operators to modularize the Deutsch-Schorr-Waite (DSW) algorithm. The modularization is performed with the notion of a parasitic stack, whose incomplete specification was instantiated for DSW with angelic nondeterminism.</p>", "authors": [{"name": "Rastislav Bodik", "author_profile_id": "81100033082", "affiliation": "Univerisity of California, Berkeley, Berkeley, CA, USA", "person_id": "P1911109", "email_address": "", "orcid_id": ""}, {"name": "Satish Chandra", "author_profile_id": "81100394237", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P1911110", "email_address": "", "orcid_id": ""}, {"name": "Joel Galenson", "author_profile_id": "81453635176", "affiliation": "Univerisity of California, Berkeley, Berkeley, CA, USA", "person_id": "P1911111", "email_address": "", "orcid_id": ""}, {"name": "Doug Kimelman", "author_profile_id": "81100165368", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P1911112", "email_address": "", "orcid_id": ""}, {"name": "Nicholas Tung", "author_profile_id": "81453615977", "affiliation": "Univerisity of California, Berkeley, Berkeley, CA, USA", "person_id": "P1911113", "email_address": "", "orcid_id": ""}, {"name": "Shaon Barman", "author_profile_id": "81453611005", "affiliation": "Univerisity of California, Berkeley, Berkeley, CA, USA", "person_id": "P1911114", "email_address": "", "orcid_id": ""}, {"name": "Casey Rodarmor", "author_profile_id": "81453661278", "affiliation": "Univerisity of California, Berkeley, Berkeley, CA, USA", "person_id": "P1911115", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706339", "year": "2010", "article_id": "1706339", "conference": "POPL", "title": "Programming with angelic nondeterminism", "url": "http://dl.acm.org/citation.cfm?id=1706339"}