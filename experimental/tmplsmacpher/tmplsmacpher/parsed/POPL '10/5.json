{"article_publication_date": "01-17-2010", "fulltext": "\n Continuity Analysis of Programs Swarat Chaudhuri Sumit Gulwani Roberto Lublinerman Pennsylvania State \nUniversity Microsoft Research Pennsylvania State University swarat@cse.psu.edu sumitg@microsoft.com rluble@psu.edu \n Abstract We present an analysis to automatically determine if a program represents a continuous function, \nor equivalently, if in.nitesimal changes to its inputs can only cause in.nitesimal changes to its out\u00adputs. \nThe analysis can be used to verify the robustness of programs whose inputs can have small amounts of \nerror and uncertainty e.g., embedded controllers processing slightly unreliable sensor data, or handheld \ndevices using slightly stale satellite data. Continuity is a fundamental notion in mathematics. However, \nit is dif.cult to apply continuity proofs from real analysis to func\u00adtions that are coded as imperative \nprograms, especially when they use diverse data types and features such as assignments, branches, and \nloops. We associate data types with metric spaces as opposed to just sets of values, and continuity of \ntyped programs is phrased in terms of these spaces. Our analysis reduces questions about con\u00adtinuity \nto veri.cation conditions that do not refer to in.nitesimal changes and can be discharged using off-the-shelf \nSMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation \nin the value of a variable often leads to divergent control-.ow that can lead to large changes in values \nof variables. Our proof rules identify appropriate synchro\u00adnization points between executions and their \nperturbed counter\u00adparts, and establish that values of certain variables converge back to the original \nresults in spite of temporary divergence. We prove our analysis sound with respect to the traditional \nE-d de.nition of continuity. We demonstrate the precision of our anal\u00adysis by applying it to a range \nof classic algorithms, including algo\u00adrithms for array sorting, shortest paths in graphs, minimum span\u00adning \ntrees, and combinatorial optimization. A prototype implemen\u00adtation based on the Z3 SMT-solver is also \npresented. Categories and Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of programming \nlanguages Program analysis.; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs Mechanical ver\u00adi.cation; G.1.0 [Numerical Analysis]: General Error analysis, Stability \nGeneral Terms Theory, Veri.cation Keywords Continuity, Program Analysis, Uncertainty, Robust\u00adness, Perturbations, \nProof Rules. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . \n$10.00 DIJK(G : graph, src : node) 1 for each node v in G 2 do d[v] :=.; prev[v] := UNDEF ; 3 d[src] \n:= 0; WL := set of all nodes in G; 4 while WL= \u00d8 5 do choose node w . WL such that d[w] is minimal; 6 \nremove w from WL; 7 for each neighbor v of w 8 do z := d[w]+ G[w, v]; 9 if z<d[v] 10 then d[v] := z; \nprev[v] := w Figure 1. Dijkstra s shortest-path algorithm 1. Introduction Uncertainty in computation \nhas long been a question of interest in computing [8]. An important reason for the uncertain behavior \nof programs is erroneous data [21]: the traf.c data that a GPS de\u00advice uses to plan a path may be slightly \nstale at the time of com\u00adputation [15], and the sensor data that an aircraft controller pro\u00adcesses may \nbe slightly wrong [11]. In a world where computation is increasingly intertwined with sensor-derived \nperceptions of the physical world [12], such uncertain inputs are ubiquitous, and the assurance that \nprograms respond robustly to them often vital. Does the output of a GPS device change only slightly in \nresponse to .uc\u00adtuations in its inputs? If so, can we prove this fact automatically? Robustness of programs \nto small amounts of error and uncer\u00adtainty in their inputs can be de.ned via the mathematical notion \nof continuity. Recall that a function f(x): R . R is continuous at the point x = c if, for all in.nitesimal \ndeviations of x from c, the value of f(x) deviates at most in.nitesimally from f(c). This pro\u00advides a \nconcrete de.nition of robustness: if a program implements a function that is continuous at c, then its \noutput is not affected by small .uctuations of its input variable around the value c. To see this de.nition \nof continuity of programs and its appli\u00adcation in specifying robustness, consider an algorithm routinely \nused by path-planning GPS devices: Dijkstra s shortest-path algo\u00adrithm. A program Dijk implementing the \nalgorithm is shown in Figure 1 here, G is a graph with real edge-weights, src is the source node, and \nG[u, v] is the weight of the edge (u, v). It is a property of Dijk that the set of paths that it computes \ncan change completely in response to small perturbations to G (by this, let us mean that the weight of \nsome edge of G changes slightly). How\u00adever, what if our robustness requirement asserts that it is the \nweight of the shortest path that must be robust to small changes to G? In other words, assuming the array \nd of shortest-path distances is the program s output, is the program continuous? We note that it is d \nchanges at most in.nitesimally if G changes in.nitesimally. Questions of continuity and robustness appear \nroutinely in the literature on dynamical and hybrid systems [16, 17]. However, these approaches apply \nto systems de.ned by differential equa\u00adtions, hybrid automata [1], or graph models [18]. In the program \nveri.cation literature, robustness has previously been considered in the restricted settings of functional \nsynchronous programs [2], .nite-state systems [10], and .oating-point roundoff errors [6, 7, 13, 14]. \nAlso, for purely numerical programs, robustness can be analyzed by abstract interpretation using existing \ndomains [4, 5]. In contrast, this paper assumes a framing of robustness in terms of continuity, and \npresents a general proof framework for continuity that applies to programs such as Dijk that use data-structures \nsuch as graphs and arrays, as well as features like imperative as\u00adsignments, branches, and loops. The \nsearch for such a proof frame\u00adwork, however, is fraught with challenges. Even a program whose inputs \nand outputs are over continuous domains may use temporary variables of discrete types, and manipulate \ndata using imperative assignments, branches, and loops. It can have multiple inputs and outputs, and \nan output can be continuous in one input but not in an\u00adother. Indeed, prior work [9] has argued for a \nnotion of continuity for software, but failed to offer new program analysis techniques, concluding that \nit is not possible in practice to mechanically test for continuity in the presence of loops. Recall the \nseemingly simple continuity property of the program Dijk: if d is its output, then it is continuous. \nHowever, it is highly challenging to establish this property from the text of Dijk. One way to prove \nit would be to .rst prove that Dijk computes shortest paths, and then to establish that the costs of \nthese paths are con\u00adtinuous in the weights of the edges of G. Such a proof, however, would be highly \nspecialized and impossible to automate. What we want, therefore, is a proof methodology that reasons \nabout continu\u00adity without aiming to prove full functional correctness, is applica\u00adble to a wide range \nof algorithms, and can be automated. Here we present such a method. We highlight below some of the challenges \nthat arise and our proof rules for addressing them. Presence of Control-.ow. One challenge in proving \ncontinuity of programs is control-.ow: a small perturbation can cause control to .ow along a different \nbranch leading to a syntactically divergent behavior. For example, consider the branch in Lines 9-10 \nin Dijk, which allow semantically different behaviors of either setting d[v] to z or leaving d[v] unchanged \n. We present a rule for proving continuity of such if-then-else code-fragments. The key idea is to show \nthat the two (otherwise semantically different) branches be\u00adcome semantically equivalent in situations \n(known as discontinu\u00adities) where the conditional can .ip its value. Using this rule (ITE\u00ad1), we can \nshow that the l-value d[v] is continuous after the code\u00adfragment if d[v] <z then d[v] := z. This is because \nthe condi\u00adtional d[v] <z can .ip values on small perturbations only when d[v] was already close to z; \nhowever, under such a condition the expressions d[v] and z evaluate to approximately the same value. \nNon-inductiveness of continuity. The next challenge comes in extending the continuity proofs to loops. \nA natural approach is to set up an inductive framework for establishing continuity during each loop iteration \n(rule SIMPLE-LOOP). However, it turns out that continuity is not an inductive property for several loops \n(unlike invariants), meaning that the program variables that are continuous at the end of the loop are \nnot necessarily continuous in each loop iteration. For example, while the array d is a continuous function \nof G on termination of Dijk, it is not continuous across each loop iteration. This is because the array \nd is updated in each loop iteration based on the choice of w from the workset W such that d[w] is minimal. \nNow small .uctuations in the input weights can cause small .uctuations in the elements of d, causing \nit to choose a very different node w and potentially alter d completely. Key to solving this challenge \nis the observation that if we group some loop iterations together, then continuity becomes an induc\u00adtive \nproperty of the groupings. These groupings are referred to as epochs, and they have the property that \nthe constituent iterations can be executed in any order without violating the semantics of the program. \nThe LOOP proof-rule discharges this obligation by establishing commutativity of the loop body. Returning \nto Dijk\u00adstra s algorithm, this grouping is based on the set of elements w that have similar weight d[w]. \nThe property of this grouping is that P (w1); P (w2) is semantically equivalent to P (w2); P (w1) where \nw1 and w2 are two elements such that d[w1]= d[w2], where P (w) represents the code-fragment in Lines \n7-10. Perturbations in Number of Loop Iterations. Another challenge in continuity proofs for loops is \nthat the number of loop iterations may differ as a result of small perturbations to the inputs. We note \nthat whenever such a behavior happens in continuous loops, then the effect of the extra iterations either \nin the original or the per\u00adturbed execution is almost equal to that of a skip-statement. This property \ncalled synchronized termination condition is asserted in our rules for loops. (Dijkstra s algorithm does \nnot exemplify this challenge though, as the loop body is executed once for each graph node regardless \nof small changes to the edge weights.) 1.1 Contributions and Organization of the Paper This paper makes \nthe following contributions. We formalize the notion of continuity of programs by associat\u00ading data-types \nwith metric spaces and operators with continuity speci.cations (Sec. 2).  We present structural rules \nto prove the continuity of programs in presence of control-.ow (Sec. 4) and loops (Sec. 5), after es\u00adtablishing \na formalism to reason about continuity of expressions in Sec. 3. These proof rules require establishing \nstandard prop\u00aderties of code-fragments, in particular, establishing equivalence or commutativity, which \ncan be discharged using off-the-shelf SMT solvers or assertion checkers.  We prove our proof rules sound \nwith respect to the standard E-d de.nition of continuity. This is quite challenging because the proof \nrules do not refer to E or d.  We demonstrate the precision of our proof rules by showing that our framework \ncan be used to prove continuity of several con\u00adtinuous classical algorithms. Sec. 6 discusses our implementa\u00adtion \nof a prototype of our framework that discharges proof rules using the SMT-solver Z3. Our current implementation \nrequires the user to provide some annotations to identify the requisite components of the LOOP proof \nrule, though there are heuristics that can be used to automate this step.  2. Problem formulation In \nthis section, we .x a notion of continuity for imperative pro\u00adgrams and formulate the problem of continuity \nanalysis. First, we de.ne a language whose semantics allows for a notion of distances between states \nin fact, states are now elements of a metric space. Second, we de.ne continuity for programs as standard \nmathemat\u00adical continuity applied to their semantic functions. As programs have multiple inputs and observable \noutputs, we allow for state\u00adments such as Program P is continuous in input x but not in input y, meaning \nthat a small change to x must cause small changes to the observable program outputs, but that a small \nchange to y may change the observable outputs arbitrarily. Programs and expressions. We begin by .xing \na simple imper\u00adative language (henceforth called IMP). The language has a single non-standard feature: \na distance metric for each data type. Types here represent metric spaces rather than sets of values, \nand the se\u00admantics of expressions and programs are given by functions be\u00adtween metric spaces. This lets \nus de.ne continuity of programs us\u00ading standard mathematical machinery. Let a distance be either a non-negative \nreal, or a special value 8 satisfying x< 8 for all x . R=0. We de.ne metric spaces over these distances \nin the usual way. Also, we assume: A set of data types. Each type t in this set is associated with dis\u00adtance \nmetric distt , and represents a metric space Valt whose distance measure is distt . The space Valt is \nknown as the space of values of t . For our convenience, we assume that each Valt contains a special \nvalue . representing unde.ned . In particular, we allow the types bool and real of booleans and reals. \nThe type real is associated with the standard Euclidean metric, de.ned as distreal (x, y)= |x - y| if \nx, y=., and distreal (x, y)= 8 otherwise. The metric on bool is the discrete metric, de.ned as distbool \n(x, y)=0 if x = y, and 8 otherwise. A universe Var of typed variables.  A set O of (primitive) operators. \nEach operator op comes with a unique signature op : t (p1 : t1,...,pn : tn), where for all i, pi ./Var. \nIntuitively, pi is a formal parameter of type ti, and t is the type of the output value. For example, \nthe real type comes with operators for addition, multiplication, division, etc.  The syntax of expressions \ne is now given by e ::= x |op(e1,...,en), where x . Var and op .O. Here op(e1,...,en) is an application \nof the operator op on the operands e1,...,en. The set of variables appearing in the text of e is denoted \nby Var(e). For easier reading, we often write our expressions in in.x. Expressions are typed by a natural \nset of typing rules as our analysis is or\u00adthogonal to this type system, we assume all our expressions \nto be well-typed. As for programs P , they have the syntax: P ::= skip | x := e | if b then P1 else P2 \n| while b do P1 | P1; P2 where e is an expression and b a boolean expression. We denote the set of variables \nappearing in the text of P by Var(P ). For convenience, we sometimes annotate statements within a program \nwith labels l. The interpretation is standard: l represents the control point immediate preceding the \nstatement it labels. As for semantics, let us .rst de.ne a state: De.nition 1 (State). A state is a map \ns assigning a value in Valt to each x . Var of type t . The set of all states is denoted by S. The semantics \nof an expression e of type t is now de.ned as a function [ e] of the form S . Valt . As expressions are \nbuilt using operators, we presuppose a semantic map [ op] for each operator op .O. Let op have the signature \nop : t (p1 : t1,...,pn : tn), and let Sop be the set of maps assigning suitably typed values to the pi \ns. Then [ op] is a map of type Sop . Valt . The semantic function [ e] for an expression e is now de.ned \nas: j [ op]]([[e1]](s),..., [ en]](s)) if e = op(e1,...,en)[ e]](s)= s(x) if e = x . Var. As for programs, \nwe use a standard functional (denotational) se\u00admantics [23] for them. For simplicity, let us only consider \nprograms that terminate on all inputs. The semantic function for a program P is then a map [ P ] of type \nS . S such that for all states sin, [ P ]](sin) is the state at which P terminates after starting execution \nfrom the state sin. The inductive de.nition of [ P ] , being standard, is omitted. Note that both [ e] \nand [ P ] are functions between metric spaces. Continuity. Now that we have de.ned the semantics of programs \nas maps between metric spaces, we can use the standard E-d de.ni\u00adtion [20] to de.ne their continuity. \nAs programs usually have mul\u00adtiple inputs and outputs, we consider a notion of continuity that is parameterized \nby a set of input variables In and a set of observable variables Obs. For a set of variables V , let \nus call two states V -close if they differ at most slightly in the values of the variables in V , and \nV \u00adequivalent if they agree on values of all variables in V . Let a state s' be a small perturbation \nof a state s if the value of each variable in In is approximately equal, and the value of each variable \nnot in In exactly the same, in s and s'. We de.ne: De.nition 2 (Perturbations, V -closeness, V -equivalence). \nFor E . R+, a state s, and a set In . Var of input variables, a state s' is an E-perturbation of a state \ns (written as PertE,In (s, s')) if for all variables x . In of type t , we have distt (s(x),s'(x)) <E, \nand for all variables y/. In of type t , we have s(y)= s'(y). For V . Var and E . R, the states s and \ns' are E-close in V (written as s E,V s') if for all x . V of type t, we have distt (s(x),s'(x)) <E. \nThe states are V -equivalent (written as s =V s') if for all x . V , we have s(x)= s'(x). Continuity \nof programs and expressions can now be de.ned by applying the traditional E-d de.nition: De.nition 3 \n(Continuity of expressions and programs). Let In . Var be aset of input variables. An expression e of \ntype t is continuous at a state s in In if for all E . R+, there exists a d . R+ such that for all s' \nsatisfying Pertd,In (s, s'), we have distt ([[e]](s), [ e]](s')) <E. A program P is continuous at a state \ns in a set In of input variables and a set Obs . Var of observable variables if for all E . R+, there \nexists a d . R+ such that for all s' satisfying Pertd,In (s, s'), we have [ P ]](s) E,Obs [ P ]](s'). \nIntuitively, if e is continuous in In, then small changes to vari\u00adables in In can change its value at \nmost slightly, and if P is contin\u00aduous in In and Obs, then small changes to variables in In can only \ncause small changes to variables in Obs (variables outside Obs can be affected arbitrarily). While the \nE-d de.nition can be directly used in continuity proofs [20], such proofs are highly semantic. More syntactic \nproofs would reason inductively, using axioms, inference rules, and invari\u00adants. The appeal of a framework \nfor such proofs would be twofold. First, instead of closed-form mathematical expressions, it would target \nprograms that may often not correspond to cleanly de.ned or easily identi.able mathematical functions. \nSecond, it would al\u00adlow mechanization, even automation. Therefore, we formulate the problem of continuity \nanalysis: Problem (Continuity analysis). Develop a set of syntactic proof rules that can soundly and \ncompletely determine if a program P is continuous in a set In of input variables and a set Obs of observable \nvariables, at each state s satisfying a property c. In the next few sections, we present our solution \nto this problem. Our rules are sound. While we do not claim completeness, we offer an empirical substitute: \nnearly-automatic continuity proofs for 11 classic algorithms, most of them picked from a standard undergraduate \ntextbook [3]. 3. Continuity judgments and speci.cations In this section, we de.ne the basic building \nblocks of our reasoning framework. These include the judgments it outputs, as well as the user-provided \nspeci.cations that parameterize it. 3.1 Continuity judgments Suppose our goal is to judge the continuity \nof an expression e or a program P in a set In of input variables and, in the latter case, a set Obs of \nobservable variables. Instead of obtaining judgments that hold a speci.c state s, we judge continuity \nat a set of states symbolically represented by a formula b. Therefore, we de.ne: De.nition 4 (Continuity \njudgment). A continuity judgment for an expression e is a term b f Cont(e, In), where b is a formula \nwith free variables in Var, and In . Var. A judgment for a program P is a term b f Cont(P, In, Obs), \nwhere b is a formula over Var, and In, Obs . Var. The judgment b f Cont(e, In) is read as: e is continuous \nin In at each state s satisfying the property b. The judgment b f Cont(P, In, Obs) says that the program \nP is continuous in the set In of input variables and the set Obs of observable variables at all states \nsatisfying b. The judgments are sound if these statements are true according to the de.nition of continuity \nin De.nition 3. Note that for a judgment b f Cont(P, In, Obs) (similarly, b f Cont(e, In)) to be sound, \nit suf.ces for In to be an under\u00adapproximation of the set of input variables, and Obs to be an over\u00adapproximation \nof the set of observable variables, in which P (sim\u00adilarly, e) is continuous. Example 1. The expression \n(x+y), where + denotes real addition, is always continuous in {x, y}. On the other hand, the expression \nxy , which evaluates to . for y =0, is not always continuous. Two sound judgments involving it are true \nf Cont( x , {x}) and y (y = 0) f Cont( x , {x, y}), which say that: (1) the result of y division is always \ncontinuous in the dividend, and (2) continuous in all non-zero divisors. Now consider the type (t . real) \nof real-valued arrays: partial functions from the index type t to the type real. For any such array A, \nwe de.ne Dom(A) to denote the domain of A i.e., the set of all x such that A[x] is de.ned. Let us consider \nthe following supremum metric: 8 < maxi.Valt {distreal (A[i],B[i])}distt.real (A, B)= if Dom(A)= Dom(B) \n: 8 otherwise. Intuitively, the distance between A and B is the maximum distance between elements in \nthe same position in the two arrays. Now consider the array-update operator Upd, commonly used to model \nwrites to arrays. The operator has the parameters A (of type t . real), i (an integer), and p (a real), \nand returns the array A ' such that A ' [i]= p, and A ' [j]= A[j] for all j = i. To exclude erroneous \nwrites, let Upd evaluate to . if p =. or if A contains an unde.ned value. In that case, the following \njudgment is sound: (p =.) . (.i, A[i]=.) f Cont(Upd(A, i, p), {A, i, p}) . Observe that Upd(A, i, p) \nis judged to be continuous in i. The reason is that as i is drawn from a discrete metric space (the int \ntype), the only way to change it in.nitesimally is to not change i at all. Continuity in i follows trivially. \n0 Example 2. Consider the program P = x := x + 1; y := z/x. A sound continuity judgment for P is (x +1 \n= 0) f Cont(P, {x, y, z}, {y}). Now consider the following program P ' : if (x = 0) then r := y else \nr := z. Denote by c the formula (x = 0) . (y = z). Then the continuity judgment c f Cont(P ' , {x, y, \nz}, {r}) is sound. To see why, note that for .xed x and z, an in.nitesimal change in y either causes \nno change to the .nal value of r (this happens if x< 0), or changes it in.nitesimally. A similar argument \nholds for in.nitesimal changes to z. As for x, the guard (x = 0) is continu\u00adous in x (i.e., is not affected \nby small changes to x) at all x =0. As a result, under the precondition x =0, in.nitesimal changes to \nx, y, z changes the .nal value of r at most in.nitesimally. At x =0, of course, the guard is discontinuous \ni.e., can change value on an in.nitesimal change to x. In this case, an in.nitesimal change to x may \ncause the output r to change from the value of y to that of z. However, by our precondition, we have \ny = z whenever x =0. Thus means that even if the guard evaluates differently, the observable output r \nis affected at most in.nitesimally by in.nitesimal changes to x, y, z. In other words, under the assumed \nconditions, the discontinuous behavior of the guard does not affect the continuity of P ' in r. Now \nconsider the judgment true f Cont(P ' , {y, z}, {r}). As we only assert continuity in inputs y and z, \nit is clearly sound. 0 3.2 Continuity speci.cations As the operators in our programming language can \nbe arbitrary, we need to know their continuity properties to do continuity proofs. This information is \nprovided by the programmer through a set of continuity speci.cations. We de.ne: De.nition 5 (Continuity \nspeci.cation). A continuity speci.cation for an operator op, with the signature op : t(p1 : t1,...,pn \n: tn), is a term c f S, where c is a boolean expression over p1,...,pn and S .{p1,...,pn}. An operator \nis allowed to have multiple speci.cations. Suppose the operator op has a speci.cation c f S. The interpretation \nis that the semantic map [ op] is continuous in S at each state over {p1,...,pn} that satis.es c. The \nspeci.cation is sound if this is indeed the case. Intuitively, application of the operator preserves \nthe continuity properties of arguments corresponding to parame\u00adters pi . S, and can potentially introduce \ndiscontinuities in the remaining arguments. Example 3. Let the real addition operator have the signature \n+: real(x : real,y : real). A sound speci.cation for it is true f {x, y}. Now consider the real division \noperator /, with similar signature. Two sound speci.cations for it are true f{x} and (y = 0) f{x, y}. \n0 Continuity speci.cations as above have a natural relationship with modular analysis. While operators \nin programming languages are usually low-level primitives, nothing in our de.nition prevents an operator \nfrom being a procedural abstraction of a program. As the reasoning framework assumes its continuity properties, \nit de.nes a level of abstraction at which continuity is judged. In the current paper, we assume that \nthis procedural abstraction is de.ned by the programmer. In future work, we will consider an interprocedural \ncontinuity analysis, where operator speci.cations are generalized into continuity summaries mined from \na program. 4. Analysis of expressions and loop-free programs In this section, we begin the presentation \nof our analysis. The main contributions presented here are: (1) a continuity analysis of expressions \nthrough structural induction; and (2) an analysis of branching programs based on identi.cation of the \ndiscontinuities of a boolean expression, and queries about program equivalence discharged through an \nSMT-solver. 4.1 Analysis of expressions The main idea behind continuity analysis of expressions is simple: \nan expression e is continuous in In if it is obtained by recursively applying continuous operators on \nvariables in In. If e has a subex\u00adpression e ' that is either discontinuous or an argument to an opera\u00adtor \nthat does not preserve its continuity, then we should judge e to be discontinuous in all variables in \ne ' . The inference rules for the analysis are presented in Fig\u00adure 2. Here, for expressions c, e1,...,en, \nthe notation c[x1 . e1,...,xn . en] denotes the expression obtained by substituting each variable xi \nin c by ei. The rule BASE states that a variable is always continuous in itself. WEAKEN observes that \na continuity x . Var (Base) true f Cont(x, {x}) ' b f Cont(e, In) b ' . b In . In (Weaken) b ' f Cont(e, \nIn ' ) b f Cont(e, In1) b f Cont(e, In2) (Join) b f Cont(e, In1 . In2) b f Cont(e, In) z/. Var(e) (Frame) \nb f Cont(e, In .{z}) op has parameters p1,...,pn and a speci.cation c f S .pi . S. b f Cont(ei, In) .pi \n./S. In n Var(ei)= \u00d8 ' c = c[p1 . e1,...,pn . en] (Op) c ' . b f Cont(op(e1,...,en), In) Figure 2. Continuity \nanalysis of expressions. judgment can be soundly weakened by restricting the set of in\u00adput variables \nin which continuity is asserted, or the set of states at which continuity is judged. FRAME observes that \nan expression is always continuous in variables to which it does not refer. As for JOIN, it uses the \nmathematical fact that if a function is continuous in two sets of input parameters, then it is continuous \nin their union. The rule OP derives continuity judgments for expressions e = op(e1,...,en), where op \nis an operator. Intuitively, if ei is con\u00adtinuous in a set of variables In, and if op is continuous in \nits i-th parameter, then e is also continuous in each x . In. The situation is complicated by the fact \nthat a variable can appear in multiple ei s, and that if x . In appears in any ej such that op is not \ncontinuous in pj or ej is not continuous in x, then e is potentially discontinu\u00adous in x. Thus, we must \nensure that In n Var(ej )= \u00d8 for all such ej ; also, each ei such that pi . S must be continuous in all \nthe variables in In. The rule OP has these conditions as premises. Example 4. Consider the expression \ne = x , where x and y x+y are real-valued variables, and the judgment (x> 0) . (y> 0) f Cont(e, {y}). \nTo prove it in our system using speci\u00ad.cations of real addition and division as before, we .rst use the \nrules BASE and FRAME, OP, and the speci.cation of + to prove that true f Cont((x + y), {x, y}). Now we \nderive that true f Cont(x, {x, y}), then use OP and the speci.cation of / to show that whenever (x+y)=0, \ne is continuous in {x, y}. Finally, we use WEAKEN to get the desired judgment. 0 Using induction and \nE-d reasoning, we can show that: Theorem 1. If all operator speci.cations are sound, the proof system \nin Figure 2 only derives sound continuity judgments.  4.2 Analysis of loop-free programs The analysis \nof loop-free programs brings out some subtleties e.g., to prove the continuity of programs with branches, \nwe must discharge a program equivalence query through an SMT-solver. Example 5. Recall the program in \nExample 2: P = x := x + 1; y := z/x. As argued earlier, the judgment (x +1 = 0) f Cont(P, {x, y, z}, \n{y}) is sound. One proof for it is as follows: 1. Show that (x + 1) is always continuous in x. From this, \nderive the judgment true f Cont(x := x +1, {x, y, z}, {x}). 2. Establish that (x = 0) f Cont(y := z/x, \n{x}, {y}). 3. Propagate backward the condition for continuity of P2, obtain\u00ading the precondition (x \n+1) = 0 for P . 4. Compose the judgments in (1) and (2), using the fact that x is the only observable \noutput in the former and the only input in the latter. This gives us the desired judgment.  (Skip) true \nf Cont(skip, \u00d8, \u00d8) b f Cont(P, In1, Obs) b f Cont(P, In2, Obs) (Join) b f Cont(P, In1 . In2, Obs) b \nf Cont(P, In, Obs) b ' . b '' Obs . Obs In . In (Weaken) '' b ' f Cont(P, In , Obs ) b f Cont(P, In, \nObs) z/. Var(P ) (Frame) b f Cont(P, In .{z}, Obs .{z}) b f Cont(e, In) (Assign-1) b f Cont(x := e, In \n.{x}, Var(e) .{x}) (Assign-2) b f Cont(x := e, Var(e) .{x}, Var(e) \\{x}) b1 f Cont(P1, In1, Obs1) In2 \n. Obs1 b2 f Cont(P2, In2, Obs2) {b1}P1{b2} (Sequence) b1 f Cont(P1; P2, In1, Obs2) c f Cont(P1, In, Obs) \nc f Cont(P2, In, Obs) ' c f Cont(b, Var(b)) (c .\u00acc ' ) f (P1 =Obs P2) (Ite-1) c f Cont(if b then P1 else \nP2, In, Obs) c f Cont(P1, In, Obs) c f Cont(P2, In, Obs) '' c f Cont(b, In ) (Ite-2) '' c . c f Cont(if \nb then P1 else P2, In n In , Obs) Figure 3. Continuity analysis of loop-free programs. Now consider \nthe program P ' from Example 2: if (x = 0) then r := y else r := z. As we argued earlier, the judgment \nc f Cont(P ' , {x, y, z}, {r}), where c equals (x = 0) . (y = z). A proof can have the following components: \n1. Identify an overapproximation S ' of the set of states (in this case captured by the formula (x = \n0)) at which the loop guard is discontinuous i.e., can .ip on a small perturbation.  2. Assuming c holds \ninitially, the two branches of P ', when ex\u00adecuted independently from a state in S ', terminate at states \nagreeing on the value of r.  3. Each branch is continuous at all states in c, in the set {x, y, z}of \ninput variables and the set {r} of observable variables.  Here, condition (2) asserts that even if a \nstate executing along a branch were executed along the other branch, the observable re\u00adsult would be \nthe same. Together with condition (3), it implies that even if an execution and its perturbed counterpart \nfollow differ\u00adent branches, they reach approximately equivalent states at the join\u00adpoint. Thus, it asserts \na form of synchronization following a pe\u00adriod of divergence between the original and perturbed executions. \nNow consider the sound judgment true f Cont(P ' , {y, z}, {r}). This time, we establish that: (1) each \nbranch of P ' is uncondition\u00adally continuous in y and z, and (2) the guard (x = 0) is uncondi\u00adtionally \ncontinuous in these variables as well. Let min be the program if (x = y) then x else y comput\u00ading the \nminimum of two real-valued variables. Using a similar style of proof as for P ', we can establish the \nsound judgment true f Cont(min, {x, y}, {x, y}) i.e., the fact that min is un\u00adconditionally continuous. \nA similar argument holds for max. 0 Let us now try to systematize the ideas in the above examples into \na set of inference rules. We need some more machinery: Discontinuities of a boolean expression. Our rule \nfor branch\u00adstatements requires us to identify the set of states where a boolean expression b is discontinuous. \nFor soundness, it suf.ces to work with an overapproximation of this set. This can be obtained by in\u00adferring \na judgment of the form c f Cont(b, Var) about b by the soundness of our analysis for expressions, \u00acc \nis overapproximates of the set of states where b is discontinuous in any variable. As we have not written \ncontinuity speci.cations for boolean operators so far, let us do so now. To judge the continuity of boolean \nexpressions, we plug these into the system in Figure 2. Example 6. For simplicity, let us only consider \nboolean expressions over the types real and bool. We allow the standard comparison operators =, =, and \n> with signatures such as =: bool(x : real,y : real); we also have the usual real arithmetic operators. \nFor boolean arithmetic, we use the standard operators ., ., and \u00ac. Speci.cations for operators involving \nbooleans are as follows: We specify each comparison operator in {=, >, <, =, =} (let it have formal \nparameters p and q) as (p = q) f{p, q}. It is easy to see that this speci.cation is sound.  The operators \n. and . (with formal parameters p and q) have the speci.cation true f{p, q}; the operator \u00ac (with parameter \nq) has the speci.cation true f{q}. The reason (which also showed up in Example 1) is that these operators \nhave discrete inputs. This implies that the only way to in.nitesimally change an input x to any of them \nis to not change x at all. Unconditional continuity in the parameters follows trivially.  For example, \nconsider the boolean expression \u00ac(x 2 = 4) . (y< 10) (let us call it b). The reader can verify that using \nthe above speci.cations, we can derive the judgment (x 2 = 4) . (y = 10) f Cont(b, {x, y}, {x, y}). It \nfollows that \u00ac((x 2 = 4) . (y = 10)) overapproximates the set of states at which b is discontinuous. \n0 Hoare triples and program equivalence. To propagate conditions for continuity through a program, we \ngenerate Hoare triples as ver\u00adi.cation conditions. These are computed using an off-the-shelf in\u00advariant \ngenerator. Additionally, we assume a solver for equivalence of programs 1. Let V . Var and c be a logical \nformula; now con\u00adsider programs P1 and P2. We say that P1 and P2 are V -equivalent under c, and write \nc f (P1 =V P2), if for each state s that satis.es c, we have [ P1]](s) =V [ P2]](s). Our rule for branches \ngenerates queries about program equivalence as veri.cation conditions. The rules. Our rules for continuity \nanalysis of loop-free programs are presented in Figure 3. Here, the rule JOIN is the analog of the rule \nJOIN forexpressions.Therule WEAKEN letsusweakenajudg\u00adment by restricting either the precondition under \nwhich continuity holds, or restricting the sets of input and observable variables. The rule FRAME says \nthat a program is always continuous in variables not in its text. The rule SKIP (taken together with \nthe rule FRAME) says that skip-statements are always continuous in every variable. Therule ASSIGN-1saysthatiftheright-handsideofanassignment \nstatement is continuous in In, then the statement is continuous in In even if the lvalue x is observable. \nASSIGN-2 says that if x is not observable, then the statement is unconditionally continuous. The rule \nSEQUENCE addresses sequential composition, system\u00adatizing the insight in Example 2. Suppose P1 is executed \nfrom a state s satisfying b1 and ends at s '. We have {b1}P1{b2}; there\u00adfore, P2 is continuous in In2 \nat s '. Now suppose we modify s by in.nitesimally changing some variables in In1; the altered output \nstate s '' of P1 approximately agrees with s ' on the values of vari\u00adables in In2 (as In2 . Obs1). Due \nto the premise about P2, this can only change the output of P2 in.nitesimally (assuming Obs2 is the set \nof observable variables). The rule ITE-1 generalizes the .rst continuity judgment for the program P ' \nmade in Example 2. Here, \u00acc ' is an overapproximation 1 We note that program equivalence is a well-studied \nproblem, an important application being translation validation [19] of optimizing compilers. FLOYD-WARSHALL(G \n: graph) 1 for k := 1 to n 2 do for i, j := 1 to n 3 if G[i, j] >G[i, k]+ G[k, j] 4 then G[i, j] := G[i, \nk]+ G[k, j]; prev[i, j] := prev[k, j] Figure 4. Floyd-Warshall algorithm for all-pairs shortest paths \n of the set of states at which the guard b is discontinuous. As P1 and P2 are equivalent whenever (c.\u00acc \n' ), it does not matter if the guard .ips as a result of perturbations the if-statement is continuous \nin each variable in which both branches are continuous. As for the rule ITE-2, it generalizes the second \njudgment about P ' in Example 2. The (conditional) equivalence of P1 and P2 is not a premise here; therefore, \nthe if-statement is guaranteed to be continuous only in the variables in which b is continuous. The inferred \nprecondition for continuity is also restricted. Using induction and E-d reasoning, we can show that: \n Theorem 2. The inference rules in Figure 3 are sound. Example 7. Consider the program P = P1; P2, where \nP1 is x := y/z and P2 is the program if (x = 0) then r := y else r := z. Let V = {x, y, z, r}, and let \nc be ((x = 0) . (y = z)). To establish the judgment (y = 0) . (z = 0) f Cont(P, V, V ), we .rst prove \nthe judgment (y = 0) . (z = 0) f Cont(P1,V,V ) this requires a continuity proof for the expression y/z, \nand use of the rules ASSIGN, FRAME and WEAKEN. Next we show that c f Cont(P2, V, V ), using, among others, \nthe rule ITE-1. Finally, we apply the rule for sequential composition. 0 5. Continuity analysis of programs \nwith loops In this section, we present our continuity analysis of programs with loops. The main conceptual \ncontributions presented here are: (1) An inductive rule for proving the continuity of loops, based on \na generalization of our rule for loop-free programs. (2) A second rule for loops, based on induction \nwhere the basic step is a sequence of loop iterations (known as an epoch) rather than a single iteration. \nThe latter rule is needed because many important applications cannot be proved continuous by an ordinary \ninductive argument. A soundness proof for it is also sketched. 5.1 Analysis of loops by induction We \nstart with a motivating example: Example 8 (Floyd-Warshall algorithm). Let us consider the Floyd-Warshall \nall-pairs shortest-path algorithm with path recovery (Fig\u00adure 4). Let us call this program FW ; on termination, \nG[i, j] con\u00adtains the weight of the shortest path from i to j, and prev[i, j] con\u00adtains a node such that \nfor some shortest path from i to j, the node right before j is prev[i, j]. We view a graph G as a function \nfrom a set of edges each edge being a pair (u, v) of natural numbers to a set of real-valued edge-weights. \nThus, G is a real-valued array as in Example 1. Let the metric on real-valued arrays be as in Example \n1; the metric on the discrete array prev and the variables i, j is the discrete metric (previously used \non the bool type). Then: (1) if a graph G has a node or edge that G ' does not, then the distance between \nG and G ' is 8, and (2) otherwise, the distance between them is max(u,v).Dom(G){|G[u, v] - G ' [u, v]|}. \nIn other words, a small change to G is de.ned as a small change to edge-weights keeping the node and \nedge structure intact. As the .nal value of G[i, j] gives the weight of the shortest path between i and \nj, the continuity claim true f Cont(FW , {G}, {G}) is sound (however, the claim true f Cont(FW , {G}, \n{prev}) is not a previously valid shortest path may become invalidated due I(c) f Cont(R, X, X) c f \nTerm(P, X) c f Sep(P, X) (Simple-loop) c f Cont(P, X, X) Figure 5. Rule SIMPLE-LOOP. (Here, P = while \nb do (l : R).) to perturbations.) We can establish this property by induction. Let R be the body of the \ninner loop (Line 3). Using an analysis as in the previous section, we have true f Cont(R, {G}, {G}). \nNow let Ri represent i repetitions of R i.e., the .rst i iterations of the loop taken together. Inductively \nassume that Ri is continuous in G i.e., a small change to the initial value of G leads to a small perturbation \nof G at the end of the i-th iteration. By the continuity of R in G, Ri+1 is also continuous in G. Finally, \nobserve that the expressions guarding the two loops here are continuous in G in other words, small changes \nto G do not affect them. Therefore, an execution and its perturbed counter\u00adpart have the same number \nof loop iterations. This establishes that the entire loop represents a continuous function. 0 Let us \nnow systematize the ideas in this example into an induc\u00adtive proof rule. Let P be a program of the form \nwhile b do (l : R). Our goal here is to inductively derive judgments of the form c f Cont(P, X, X), where \nX is an inductively obtained set of variables. However, we need some more machinery. Trace semantics \nand invariants. As our reasoning for loops requires inductive invariants, it requires a trace semantics \nof pro\u00adgrams. Let us de.ne the trace of P from a state sin as the unique sequence p = s0s1 ...sn+1 such \nthat s0 = sin, and for each 0 = i = n, {si}R{si+1} and si satis.es b. Let c be an initial condition (given \nas a logical formula). We de.ne a state s to be reachable from c if it appears in the trace from a state \nsatisfying c (note that in all reachable states, control is implicitly at the label l). The loop invariant \nI(c) of Q is an overapproximation of the set of states reachable from c. Our proofs assume a sound procedure \nto generate loop invariants. The analysis. Let us now proceed to our continuity analysis. As before, \nour strategy is to discharge veri.cation conditions that do not refer to continuity or in.nitesimal changes. \nIn particular, the following conditions are used: Separability. A set of variables X is separable in \na program P if the value of any z . X at the terminal state of P only depends on the values of variables \nin X at the initial state. Formally, we say that X is separable in P under an initial condition c, and \nwrite c f Sep(P, X), if for all states s, s ' reachable from c such that s =X s ', we have [ P ]](s) \n=X [ P ]](s ' ). Under this condition, we have an alternative de.nition of con\u00adtinuity that is equivalent \nto the one that we have been using: P is continuous at a state s in a set X of input variables and the \nsame set X of observable variables if for all E . R+, there exists a d . R+ such that for all s ' satisfying \ns d,X s ' , we have [ P ]](s) E,X [ P ]](s ' ). This de.nition is useful as we use the relation inductively. \n Synchronized termination. P ful.lls the synchronized termina\u00adtion condition with respect to an initial \ncondition c and a set of variables X (written as c f Term(P, X)) if Var(b) . X, and one of the following \nholds: (1) The loop condition b satis.es true f Cont(b, X); (2) Let the formula c ' represent an over\u00adapproximation \nof the set of states reachable from c where b is discontinuous in X. Then we have c ' f R =X skip.  \nIntuitively, this condition handles scenarios where an execution from a perturbed state violates the \nloop condition earlier or later than it would in the original execution. Under synchro\u00adnized termination, \nthe execution that continues does not veer too far from the state where it was when the other execution \nended. Of these, separability can be established using simple slicing. In the absence of nested loops, \nsynchronized termination can be checked using an SMT-solver. If the loop body contains a nested loop, \nit can be checked using an SMT-solver in conjunction with an invariant generator for the inner loop. \nOur rule SIMPLE-LOOP for inductively proving the continuity of P is now as in Fig. 5. Let us now see \nits use in an example: Example 9. We revisit the program FW in Figure 4. (We as\u00adsume it to be rewritten \nas a while-program in the obvious way.) Let X = {G, i, j}. First, we observe that true f Sep(FW ,X). \nAs argued before, letting R be the loop body, we have true f Cont(R, X, X). Finally, the loop guard b \nonly involves discrete variables therefore, by the argument in Example 1, it is always continuous in \nX, which means that true f Term(FW ,X). The sound judgment true f Cont(FW , X, X) follows. Using the \nWEAKEN rule from before, we can now obtain the judgment true f Cont(FW , {G}, {G}). Of course, this example \ndoes not illustrate the subtleties of the synchronized termination condition. For a more serious use \nof this condition, see Examples 11 and 13. 0 As for soundness, we have: Theorem 3. The inference rule \nSIMPLE-LOOP is sound. Proof sketch. Consider an arbitrary trace p = s0s1 ...sm+1 of P starting from a \nstate s0 that satis.es c, and any E . R+. Let us guess a sequence (d0,d1,...,dm+1) such that: (1) dm+1 \n<E, and (2) for all states s, s ' reachable from c, if s and s ' are di \u00adclose (in X), then if t and \nt ' are the states satisfying {s}R{t} and {s ' }R{t ' }, then s ' and t ' are di+1-close (in X). Such \na sequence exists as the loop body R is continuous. Now select d = d0, and consider a state s0 ' such \nthat s0 =d,X s0' . Recall that we assume that all programs in our setting are terminating. Therefore, \nany trace from s0 ' is of the form p ' = s0 ' ...s ' n+1 (without loss of generality, assume that n = \nm). By the continuity of the loop body, we have s1 d1,X s1' . As the synchronized termination condition \nholds, one possibility is that b is continuous at both s1 and s1' , In this case, either both traces \ncontinue execution into the next epoch, or none do. The other possibility is that one of the traces violates \nb early due to per\u00adturbations, but in this case the rest of the other trace is equivalent to a skip-statement. \nGeneralizing inductively, we conclude that sm+1 E,X s ' are arbitrary traces, the program P is n+1. As \np, p ' continuous. 5.2 Continuity analysis by induction over epochs Unsurprisingly, there are many continuous \napplications whose con\u00adtinuity cannot be proved by the rule SIMPLE-LOOP. Pleasantly, many applications \nin this category are amenable to proof by richer form of induction that we have identi.ed, and will now \npresent. We start with two examples: Example 10 (Dijkstra s algorithm). Consider our code for Dijk\u00adstra \ns algorithm (Figure 1; code partially reproduced in Fig. 6) once again. The one output variable is d \nthe array that, on termination, contains the weights of all shortest paths from src. The metric on G \nis as in Example 8. Note that Line 5 in this code selects a node w such that d[w] is minimal, so that \nto implement it, we require a mechanism to break ties on d[w]. In practice, such tie-breaking is implemented \nusing an arbitrary linear order on the nodes. Such implementations, however, are ad hoc and can easily \nbreak inductive reasoning for continuity. To be on the safe side, we conservatively abstract the program \nby replacing the selection in Line 5 by nondeterministic choice. It is 4 while WL = \u00d8 5 do choose node \nw . WL such that d[w] is minimal 6 remove w from WL; 7 for each neighbor v of w ... Figure 6. Dijkstra \ns shortest-path algorithm FRAC-KNAP(v : int . real,c : int . real, Budget : real): @pre: |v| = |c| = \nn 1 for i := 0 to (n - 1) 2 do used[i] := 0 ; 3 curc := Budget; 4 while curc > 0 5 do choose item m such \nthat t =(v[m]/c[m]) is maximal and used[m]=0; 6 used[m] := 1; curc := curc -c[m] 7 totv := totv +v[m]; \n8 if curc < 0 9 then totv := totv -v[m]; 10 totv := totv +(1 + curc /c[m]) * v[m] Figure 7. Greedy Fractional \nKnapsack easy to see that a proof of continuity for this abstraction implies a proof of continuity of \nthe algorithm with a correct, deterministic implementation of tie-breaking. Let us call this abstraction \nDijk. Usually, such abstractions correspond to the pseudocode of the algorithm under consideration, and \nare easily built from code. While the abstraction Dijk may seem to be nondeterministic, in reality it \nis not every initial state here leads to a unique terminal state. Also, as d contains weights of shortest \npaths on termination, the judgment true f Cont(Dijk, {G}, {d}) is sound. Proving it, however, is challenging. \nAssume the inductive hypothesis that d only changes slightly due to a small change to G. Now suppose \nthat before the perturbation, nodes w1 and w2 were tied in the value of d[\u00b7] and we chose w1, and that \nafter the perturbation, we choose w2. Clearly, this can completely change the value of d at the end of \nLine 10. Thus, the continuity of d is not an inductive property. However, consider a maximal set of successive \niterations in an execution processing elements tied on d[\u00b7]. Let us view this collection of iterations \nsubsequently called an epoch as a map from an input state s0 to a .nal value of d. It so happens that \nthis map is robust to permutations i.e., if s0 is .xed, then however we reorder the iterations in the \ncollection, so is the value of the array d at the state s1. Second, small perturbations to s0 can lead \nto arbitrary reorderings of the iterations however, they only lead to small perturbations to the value \nof d in s1 (on the other hand, the value of prev may change completely). This is the insight we use in \nour proof rule. 0 Example 11 (Greedy Fractional Knapsack). Consider the Knap\u00adsack problem from combinatorial \noptimization. We are given a set of items {1,...,n}, each item i being associated with a cost c[i] and \na value v[i] (we assume that c and v are given as arrays of non-negative reals). We are also given a \nnon-negative, real-valued budget. The goal is to identify a subset used .{1,...,n} such P that the constraint \nc[i] = Budget is satis.ed, and the j.used P value of totv = v[i] is maximized. j.used Let the observable \nvariable be totv; as small perturbations can turn previously feasible solutions infeasible, a program \nKnap solv\u00ading this problem correctly is discontinuous in the inputs c and Budget. At the same time, it \nis continuous in the input v. Our analysis can establish the continuity of Knap in v (see Sec\u00adtion 6). \nFor now we focus on the fractional variant of the problem, which has a greedy, optimal, polynomial solution \nand is more in\u00adteresting from the continuity perspective. Here the algorithm can pick fractions of items, \nso that elements of used can be any real P n number 0 = r = 1. The goal is to maximize i=i used[i] \u00b7 \nv[i] P ::= (all syntactic forms in IMP)|(the form Q below) l: while b do . := value u . U such that G[u] \nis minimized; R(., G,U) Figure 8. The language LIMP (P represents programs). P n while ensuring that \ni=i used[i] \u00b7 c[i] = Budget . This algorithm is continuous in all its inputs, as we can adjust the quantities \nin used in.nitesimally to satisfy the feasibility condition even when the inputs change in.nitesimally. \nTo see why proving this is hard, consider a program FracKnap coding the algorithm (Fig. 7). Here, curc \ntracks the part of the bud\u00adget yet to be spent; the algorithm greedily adds elements to used, compensating \nwith a fractional choice when curc becomes nega\u00adtive. Line 5 involves choosing an item m such that (v[m]/c[m]) \nis maximal, and once again, we abstract this choice by nondetermin\u00adism. It is now easy to see that continuity \nof totv is not inductive; one can also see that the observations made at the end of Exam\u00adple 10 apply. \nHowever, one difference is that the the condition of the main loop (Line 4) here can be affected by slight \nchanges to curc. Therefore, proving this program requires a more sophisticated use of the synchronized \ntermination than what we saw before. 0 5.2.1 A language of nondeterministic abstractions Let us now develop \na rule that can handle the issues raised by these examples. To express our conservative abstractions, \nwe extend the language IMP with a syntactic form for loops with restricted nonde\u00adterministic choice. \nWe call this extended language LIMP. Its syntax is as in Figure 8. Here: U is a set the iteration space \nfor the loop in the syntactic form Q. Its elements are called choices.  . is a special variable, called \nthe current choice variable. Every iteration starts by picking an element of U and storing it in .. \n G is a real-valued array with Dom(G) = U. If u is a choice, then G[u] is its weight. The weight acts \nas a selection criterion for choices iterations always select minimal-weight choices. Multiple choices \ncan have the same weight, leading to nonde\u00adterministic execution.  R(., G,U) (henceforth just R) is \nan IMP program that does not write to .. It can read . and read or update the iteration space U and the \nweight array G.  We call a program of form Q an abstract loop henceforth, Q denotes an arbitrary, .xed \nabstract loop. For simplicity, we only consider the analysis of abstract loops an extension to all LIMP \nprograms is easy. Also, we restrict ourselves to programs that terminate on all inputs. The main loops \nin our codes in Figures 1 and 7 are abstract loops. For example, the workset WL, the node w, and the \narray d in Figure 1 respectively correspond to the iteration space U, the choice variable ., and the \nmap G of choice weights. While the weight array is not an explicit variable in Figure 7, it can be added \nas an auxiliary variable. by a simple instrumentation routine. The functional semantics of Q is de.ned \nin a standard way. Due to nondeterminism, Q may have multiple executions from a state; consequently, \n[ Q] comprises mappings of the type s . S, where s is a state, and S the set of states at which Q may \nterminate on starting from s. We skip the detailed inductive de.nition. Continuity. Continuity is de.ned \nin the style of Def. 3: Q is continuous at a state s0 in a set In of input variables and a set Obs of \nobservable variables if for all E . R+, there is a d . R+ such that for all s0 ' satisfying Pertd,In \n(s0,s0' ), all s1 . [ Q]](s0), and all s1 ' . [ Q]](s0' ), we have s0 ' E,Obs s1' . Note that if Q is \ncontinuous, then for states s0, s1, and s2 such that {s1,s2}. [ Q]](s0), we must have s1 =Obs s2. Thus, \nthough Q uses a choice construct, its behavior is not really nondeterministic. Trace semantics and invariants. \nDue to nondeterminism, the trace semantics for abstract loops is richer than that for IMP. Let us denote \nthe body of the top-level loop of Q by B. For u . U, let the parameterized iteration Bu be the program \n(. := u; R) that represents an iteration of the loop with . set to u. For states s, s ' , we say that \nthere is a u-labeled transition from s to s ', and write u s -. s ', if (1) at the state s, G[u] is a \nminimal weight in the array G; and (2) the Hoare triple {s}Bu{s ' } holds. Intuitively, s and s ' are \nstates at the loop header (label l) in successive iterations. Condition (1) asserts that u is a permissible \nchoice for Q at s. Condition (2) says that assuming u is the chosen value for . in a loop iteration, \ns ' is the state at its end. Note that our transition system is deterministic i.e., for .xed s and u, \nthere u is at most one s ' such that s -. s ' . Let ., . ' be nonempty sequences over U, and let u . \nU. We say that there is a .-labeled transition from s to s ' if one of the following conditions holds: \nu . = u and s -. s ' , -. s '' . = u.. ', and there exists a state s '' such that: (1) s u, L . (2) s \n'' satis.es the loop condition b, and (3) s '' -. s ' . A trace of Q from a state sin is now de.ned as \na sequence .0.1.n p = s0 -. s1 -. ...sn -. sn+1, where s0 = sin, and .i for each 0 = i = n, si -. si+1 \nand si satis.es b. .i Here, the transition si -. si+1 represents a sequence of loop iterations leading \nQ from si to si+1. Note that sn+1 may not satisfy b if it does not, then it is the terminal state of \nQ. If each .i is of the form ui . U, then p is said tobe a U-trace. Clearly, Q can have multiple traces \nfrom a given state. At the . same time, if . = u0u1 ...um and there is a transition s0 -. u0um sm+1, \nthen Q has a unique U-trace of the form s0 -. s1 ... -. . sm+1. We denote this trace by Expose(s0 -. \nsm+1). For an initial condition c, a state s is reachable from c if it appears in some trace from a state \nsatisfying c. A transition . s -. s ' is reachable from c if s is reachable from c. The loop invariant \nI(c) of Q is an overapproximation of the set of states reachable from c.  5.2.2 The analysis Now we \npresent our continuity analysis. As in Section 5, our goal is to obtain a continuity judgment c f Cont(Q, \nX, X), where X is an inductively obtained set of variables. As hinted at in Example 10, we perform induction \nover clusters of successive loop iterations parameterized by choices of equal weight. We call these clusters \nepochs. Pleasantly, while the notion of epochs is crucial for our soundness argument, it is invisible \nto the user of the rule, who discharges veri.cation conditions just as before. Veri.cation conditions \nand rule de.nition. We start by de.ning our rule and its veri.cation conditions. Once again, we discharge \nthe conditions of synchronized termination and separability (these are de.ned as before). In addition, \nwe discharge the conditions of G-monotonicity and commutativity. The former property asserts that the \nweight of a choice does not increase during executions of Q. Formally, the program Q is G\u00admonotonic under \nthe initial condition c if for all states s, s ' .I(c) such that there is a transition from s to s ', \nwe have s(G[v]) = s ' (G[v]) for all v . U. The second condition says that parameterized iterations can \nbe commuted. Let us de.ne: u . v . s0 s1 s2 . =Obs . =Obs s0(G[u]) = s0' (G[v]) = v . u . s ' s ' s \n' s1(G[v]) = s1' (G[u]) 012 Figure 9. Commutativity De.nition 6 (Commutativity). The parameterized iterations \nBu and Bv commute under the initial condition c and the set Obs of observable variables if for all states \ns0, s0' , s1, s2 such that: (1) s0 =Obs s0' ; (2) s0, s1, and s0 ' satisfy the loop invariant I(c); (3) \n{s0}Bu{s1} and {s1}Bv{s2}; and (4) s0(G(u)) = s0' (G(v)) = s1(G(v)), there are states s1 ' and s2 ' such \nthat {s0' }Bv{s1' }, {s1' }Bu{s2' }, and s1 ' satis.es I(c) s1' (G(u)) = s0' (G(v)) s2 =Obs s2' . The \nprogram Q is commutative under c and the set Obs of variables (written as c f Comm(Q, Obs)) if for all \nu, v, Bu and Bv commute under c. A commutation diagram capturing the relationship between s0, s1, etc. \nin the above de.nition is given in Fig. 9. Note that given s0' , u, and v, the states s1 ' and s2 ' are \nunique. Also note that the property de.ned here is stronger than commutativity in the usual sense, as \nit asserts properties of weights of choices. Our proof rule for abstract loops is now presented in Fig. \n10. Intuitively, the rule performs induction over sequences of epochs. As we mentioned in Example 10, \nsmall perturbations will reorder loop iterations within an epoch; however, a subtle implication of our \npremises is that such reorderings do not affect continuity at the end of each epoch. Before presenting \na soundness argument and de.ning epochs formally, let us apply the rule to our two examples. Example \n12 (Dijkstra s algorithm). Let us now revisit our imple\u00admentation Dijk of Dijkstra s algorithm, and derive \nthe continuity judgment true f Cont(Dijk, X, X), where X = {G, d, WL}(this can be subsequently weakened \nto judgments like true f Cont(Dijk, {G}, {d})). Here, the array d corresponds to G in the syntax of LIMP, \nand lines 6-10 correspond to the program R. First, we observe that Dijk is d-monotonic (and that the \nreason\u00ading establishing this is simple). Also, X-separability is obvious. As in case of the Floyd-Warshall \nalgorithm, synchronized termination holds as the loop condition, only involving a discrete variable, \nis unconditionally continuous in the set of input variables X. Finally, we observe that lines 6-10 are \nalso commutative by our de.nition. By the rule LOOP, the desired continuity judgment follows. 0 Example \n13 (Fractional Knapsack). Now we consider the pro\u00adgram FracKnap (Fig. 7), recast as a LIMP program using \nan aux\u00adiliary array G such that at the beginning of each loop iteration, we have G[i]= c[i]/v[i]. Let \nus verify the judgment true f Cont(FracKnap, X, X), where X = {G, Items, curc, totv , c, v}. Once again, \nseparability of X is obvious, and G-monotonicity and commutativity can be veri.ed with some effort. The \nsynchro\u00adnized termination condition, however, is more interesting that in the proof of Dijk, as the loop \ncondition (curc > 0) is not al\u00adways continuous in X. To see that the condition holds, let c be the formula \n(curc = 0) capturing the set of states where the loop condition is discontinuous. Under this condition, \nLines 6 10, taken together, are equivalent to a skip-statement. Therefore, we have true f Term(FracKnap,X). \nBy the rule LOOP, we have true f Cont(FracKnap, X, X). 0 Soundness. Now we sketch an argument for the \nsoundness of the rule LOOP. Let us start by de.ning epochs formally: U, G . X I(c) f Comm(Q, X) Q is \nG-monotonic under c c f Sep(Q, X) I(c) f Cont(R, X, X) c f Term(Q, X) (Loop) c f Cont(Q, X, X) Figure \n10. Proof rule LOOP for programs with loops (Q is an abstract loop, and S . ObsQ) . De.nition 7 (Epochs). \nConsider a transition . = s0 -. sm+1, .u0um with Expose(s0 -. sm+1)= s0 -. s1 ... -. sm+1. The transition \n. is an epoch if: 1. For all 0 = j<m, we have sj (G[uj ]) = sj+1(G[uj+1]). um+1 2. Q has no transition \nsm+1 -. sm+2 such that sm(G[um]) = sm+1(G[um+1]). The epoch is said to have weight s0(G(u0)). Intuitively, \nan epoch is a maximal sequence of iterations that agree on choice-weights. For our proofs, we also need \na notion of d-epochs, which are just like epochs, except they allow a margin of error d between the weights \nof the choices made in successive iterations. Formally, for d . R+, a transition . as in De.nition 7 \nis a d-epoch of Q if for some W . R, we have: 1. For all 0 = j<m, |sj (G(uj )) - W | < d. um+1 2. There \nis no transition sm+1 -. sm+2 in Q such that |sm+1(G(um+1)) - W | < d. u0um Note that every U-trace p \n= s0 -. s1 ... -. sm+1 .0.n corresponds to a unique trace Epochize(p)= s0 ' -. s1 ' ... -. sn' +1 such \nthat s0 = 0, sm+1 = s ' i.i s ' n+1, and for each i, s ' -. si' +1 is an epoch. This trace represents \nthe breakdown of p into epochs. For d . R+, the trace Epochized(p), representing the breakdown of p into \nd-epochs, is similarly de.ned. Now we de.ne a notion of continuity for epochs. . De.nition 8 (Continuity \nof epochs). An epoch . = s0 -. s1 of Q is continuous with respect to a set In of input variables and \na set Obs of observable variables if for all E . R+, there exists a d . R+ such that for all states s0 \n' satisfying Pertd,In (s0,s0' ), .L every d-epoch s0 ' -. s1 ' satis.es the property s1 E,Obs s1' . The \ncrux of our soundness argument is that under the premises of the rule LOOP, every epoch of Q is continuous. \nThis is estab\u00adlished by the following theorem: Theorem 4. Suppose the following conditions hold for a \nset of variables X . Var(Q) and an initial condition c: 1. Q is G-monotonic under c 4. U, G . X 2. I(c) \nf Comm(Q, X) 5. c f Sep(Q, X) 3. I(c) f Cont(R, X, X) 6. c f Term(Q, X)  Then every epoch of Q reachable \nfrom c is continuous in input variables X and observable variables X. The proof involves a lemma proving \nthe determinism of epochs: Lemma 1. Suppose the premises of Theorem 4 hold. Then if . = . s0 -. s1 is \nan epoch reachable from c, then for all epochs .L . ' = s0 ' -. s1 ' such that s0 ' =X s0, we have: (1) \n. ' is a permutation of .; and (2) s1 =X s1' . Proof sketch. Let W be the weight of ., and de.ne a variable \nUW whose value at a state is the set of choices in U with weight W . As U . X, UW has the same value \nin X-equivalent states; as epochs are maximal, . terminates only when b is violated or UW is empty. Without \nloss of generality, assume that . and . ' are sequences of distinct choices. Suppose u is the .rst choice \nin . that does not appear in . '; let . = .1u.2. Now we have the following possibilities: (a) the execution \nof .1 added the choice u to UW .0 .1 .2 . . . ... s0 s1 s2 Epochize(p) . d0,X . d1,X . d2,X s ' . s \n' . s ' . ...Epochized(p ' ) 012 . ' . ' . ' 012 Figure 11. Induction over epochs by setting G[u]= W \n; (b) some iteration Bv in . ', where v = u, removed the choice u from UW by setting G[u] >W or removing \nu from U; (c) at some point during the execution of . ' before u could be selected, the loop condition \nb was violated. Each of these scenarios are ruled out by our assumed conditions. We only show how to \nhandle case (b). As Q is G-monotonic, we have the property that if G[u] >W at some point in . ', then \nG[u] >W at all prior points in . ' i.e., u never had the weight W in . '. As for u being removed from \nU in . ' before ever being selected, this violates commutativity. As for postcondition (2), it follows \nfrom commutativity if . is of length two or more. If . = u for some choice u, then the postcondition \nfollows from the separability of X. (As an aside, the above implies that under the premises of the rule \nLOOP, epochs are observationally deterministic: epochs start\u00ading from X-equivalent states always end \nin X-equivalent states.) Now we establish a lemma connecting each d-epoch to an epoch to which it is \nclose. (The proof is quite involved due to lack of space, we only give the intuitions here.) Consider, \nfor suf.ciently u1 small d, an arbitrary d-epoch . such that Expose(.)= s0 -. un s1 ... -. sn+1 and \na state s0 ' such that s0 ' d,X s0. As . is a d\u00adepoch, it is possible to perturb every state appearing \nin Expose(.) by an amount less than d to get a U-trace p such that: (1) p starts with s0' ; and (2) if \nsi ' is the i-th state in p, then for all i, we have si' (G[ui]) = si' +1(G[ui+1]). We can now show that, \nif the premises of Theorem 4 hold, then this trace can be executed by Q and, in fact, is of the form \nExpose(. ' ) for some epoch . ' of Q. Thus we have: Lemma 2. Assume that the premises of Theorem 4 hold. \nThen for all E . R+, there exists a d . R+ such that for all d-epochs . . = s0 -. s1 and all states \ns0 ' such that s0 ' d,X s0, there is an .L epoch . ' = s0 ' -. s1 ' such that: (1) . = . ', and (2) \ns1 E,X s1' . Proof sketch for Theorem 4. Now we can establish Theorem 4. Let . . = s0 -. s1 be any epoch, \nand let E . R+ . Select a d small enough for Lemma 2 to hold. Consider any d-epoch . ' = .L s0 ' -. \ns1 ' such that s0 d,X s0' . By Lemma 2, there is an epoch . '' -. s '' s ' s '' = s0 .L 1 such that \ns1 '' E,X 1. As . and . '' are epochs from the same state, by Lemma 1, we have s1 =X 1 . But this means \nthat s1 E,X s1' . This establishes the continuity of .. Soundness for rule LOOP now follows in a straightforward \nway. The argument is similar to that for Theorem 3 however, this time we use epochs, rather than individual \nloop iterations, as the basic steps of induction. While continuity may be broken inside an epoch, it \nis, by Theorem 4, reinstated at its end. Intuitively, any two traces of Q starting from arbitrarily close \nstates synchronize reaching observationally close states at the ends of epochs (the situation is sketched \nin Figure 11). We have: Theorem 5. The proof rule LOOP is sound. Proof sketch. Consider an arbitrary \nU-trace p of Q starting from a .0.m state s0 that satis.es c, the trace Epochize(p)= s0 -. ... -. sm+1, \nand any E . R+. Select a sequence of di s, with d = d0, just as in Theorem 3, and consider a state s0 \n' such that s0 =d,X s0' . Example Time (s) Simple-loop or No Loop or (U, G(u), .) Term. Expressions \nBubbleSort 0.035 Simple-loop No InsertionSort (Outer) 0.028 Simple-loop Yes InsertionSort (Inner) 0.027 \nSimple-loop Yes Update(A,j+1,z) SelectionSort (Outer) 0.092 Simple-loop No A[s] SelectionSort (Inner) \n0.249 Simple-loop No A[1..i], Array2Set (A[i . . n - 1]) MergeSort 0.739 ({(u, u L) | 1 = u = n, 1 = \nu L = m}, Min(A[u], B[u L]), (i, j)) No Dijkstra 0.177 (Q, dist[u], m) No Bellman-Ford 0.029 Simple-loop \nNo Floyd-Warshall 0.032 Simple-loop No Kruskal 1.069 (Q = Edges(G), W (u, u L), (i, j)) No Prim 0.248 \n({u, u L | u . G - F, u L . F }, W (u, u L), (v, v L)) No Frac. Knapsack 0.38 ({1 . . n}, v[u]/c[u], \nm) Yes Int. Knapsack 3.22 No Loop No Table 1. Benchmark Examples Let p ' be any U-trace from s0' , and \nlet Epochize. (p ' )= .L .L s ' 0n 0 -. ... -. sn' +1 (without loss of generality, assume that n = m). \nBy the continuity of epochs we have s1 d1,X s1' . Generalizing inductively, and using the synchronized \ntermination condition as before, we conclude that sm+1 E,X s ' n+1. As p, p ' are arbitrary traces, Q \nis continuous. 6. Experiments We chose several classic continuous algorithms (mostly from a standard \nundergraduate text on algorithms [3]) to empirically eval\u00aduate the precision and completeness of our \nproof rules. Our rules were able to prove the continuity of 11/13 examples that we tried. An important \nstep before the application of our proof rules LOOP isthe transformation ofloops into abstractloops as \ndescribed in Section 5.2, which requires identifying the iteration space U , the current choice variable \n., and the weight function G. Of course, if the rule SIMPLE-LOOP is applicable, then these steps are \nnot needed. Table 1 describes the rules, and parameters U, ., and G, needed in each of our applications. \nIn some cases, we also needed to introduce some auxiliary variables since our framework tracks continuity \nof program fragments with respect to a set of observation variables. (An alternative would have been \nto de.ne our framework to track continuity of expressions, and in fact, this is another inter\u00adesting \naspect of continuity proofs for programs. Such an extension to our framework is not dif.cult, but we \navoided this to keep the presentation of the framework simpler.) The column Expressions contains the \nexpressions represented by auxiliary variables in the various examples. Transformation of loops into \nabstract loops and introduction of auxiliary variables were performed manually. How\u00adever, there are heuristics \nthat can be used to automate this step. The column Term. denotes whether or not the corresponding example \nrequired establishing the synchronized termination condition. Sorting Algorithms. Consider a sorting \nalgorithm that takes in an array Ain and returns a sorted array Aout. Such an algorithm is continuous \nin Ain a small change to Ain can only result in a small change to Aout[i], for all i. The observation \nrequires a bit of thought as the position of a given element in the output array Aout may change completely \non small perturbations. Indeed, consider an algorithm that returns a representation of the sorted array \nin the form of an array of indices In into the original input array (i.e., i<j . Ain[In[i]] <Ain[In[j]]), \nrather than a sort of the original array. Such an algorithm is discontinuous in Ain. Our proof rules \ncan establish continuity of the three standard iterative sorting algorithms: BubbleSort, InsertionSort, \nSelection-Sort. Our proof rules can also establish continuity for MergeSort, but are unable to establish \nthe continuity of Quicksort. This is because the continuity proof for MergeSort is inductive with re\u00adspect \nto the recursive calls (i.e., continuity holds at every recursive call to MergeSort), but this is not \nthe case with QuickSort. The proof of continuity of QuickSort requires an inductive hypothesis about \nQuickSort that is more powerful than continuity speci.ca\u00adtions. This suggests that an interprocedural \nvariant of continuity analysis would involve developing more powerful techniques. The proof of continuity \nfor each of the sorting algorithms turns out to be quite different from each other (suggesting the fundamentally \ndif\u00adferent ways in which these algorithms operate). We point out some of the interesting aspects of the \ncontinuity proofs for each of these examples. Bubblesort is the simplest of all where the continuity \nproof is inductive for both its loops, and the interesting part is to establish the continuity of the \nloop-body. This involves proving that the Swap operation that swaps two elements of an array is continuous, \nwhich requires an application of proof rule ITE-1. The proof of continuity of InsertionSort is also inductive \nfor both its loops. However, establishing continuity of the inner loop has two interesting aspects. It \nrequires an application of the syn\u00adchronized termination condition, and requires establishing con\u00adtinuity \nof (the auxiliary variable representing) the expression update(A, j +1,z) (note that the loop is actually \ndiscontinu\u00adous in A, but to inductively prove the continuity of the outer loop, we in fact need to prove \ncontinuity of the inner loop with respect to update(A, j +1,z)). The proof of continuity of SelectionSort \nis also inductive for both its loops. The interesting part is to note that the outer loop is not continuous \nin A. It is actually continuous in the expressions A[1..i] and the set Array2Set(A[i, .., n - 1]), which \nsuf.ces to establish the continuity of A[1..n] when the loop terminates since i = n outside the loop. \nSimilarly, the inner loop is continuous in A[s] as opposed to s, but this suf.ces to prove the desired \ncontinuity property of the outer loop. For MergeSort, the challenging part is to establish the continuity \nof the Merge procedure, which is not inductive, and requires using the proof rule LOOP in its generality. \nShortest Path Algorithms. The path returned by any shortest path algorithm is susceptible to small perturbations \nin the edge weights of the input graph. However, the value of the shortest path returned by the shortest \npath algorithms is actually continuous in the input graph. Our proof rules can establish this property \nfor each of the three shortest path algorithms that we considered. Among these, Dijkstra s algorithm \nis the most interesting one, requiring use of the proof rule LOOP in its generality. The continuity proof \nof Bellman-Ford and Floyd Warshall is relatively easy since it is inductive. Minimum Spanning Tree Algorithms. \nThe spanning tree re\u00adturned by minimum spanning tree algorithms can vary widely upon small perturbations \nin the edge weights of the input graph. How\u00adever, the weight of the minimum spanning tree returned by \nthe minimum spanning tree algorithms is actually continuous in the edge weights of the input graph. Our \nproof rules can establish this property for Kruskal and Prim algorithms, but fail for Boruvka s algorithm. \nThe continuity proofs for both Kruskal and Prim algo\u00adrithms are not inductive and require an application \nof the proof rule LOOP in its generality. However, our proof rules are not precise enough to establish \nthe continuity of Boruvka s algorithm. Conti\u00adnuity of Boruvka s algorithm requires merging non-contiguous \niter\u00adations (which is not handled by our epoch framework, which only merges contiguous iterations). A \ngeneral approach to investigate KRUSKAL(G : graph) 1 for each node v in G do C[v] := {v}; 2 Q := set \nof all edges in G; cost := 0; T := \u00d8; 3 while Q= \u00d8 4 do choose edge (v, w) . Q such that G(v, w) is \nminimal; 5 remove (v, w) from Q; 6 if C[v]= C[w] 7 then add edge (v, w) to T ; 8 cost := cost +G(v, w); \n9 C[v] := C[w] := C[v] . C[w]; PRIM(G : graph) 1 for each node v in G 2 do d[v] := .; parent[v] := UNDEF; \n3 s := arbitrary node in G; d[s] := 0; 4 cost := 0; F := {s}; 5 while |F | < |G| 6 do choose node v/. \nF L with a minimal-cost edge (v, v ) into F L 7 F := F .{v}; cost := cost + G(v, v ); 8 for each neighbor \nw of v 9 do if d[w] >d[v]+ G(v, w); 10 then d[w] := d[v]+ G(v, w); 11 parent[w] := v;  MERGESORT(A : \nrealarr) 1 if |A|= 1 2 then return A; 3 m := L|A|/2J; 4 A1 := A[0 ..m]; A2 := A[m +1 .. |A|- 1]; 5 B1 \n:= MERGESORT(A1); 6 B2 := MERGESORT(A2); 7 return MERGE(B2,B2); MERGE(A1 : realarr,A2 : realarr) 1 i \n:= 0; j := 0; k := 0; 2 while k< |A1| + |A2| 3 do if (i =|A1|) or (A1[i] >A2[j]) 4 then result[k] := \nA2[j]; 5 j := j +1; k := k +1; 6 else result[k] := A1[i]; 7 i := i +1; k := k +1; 8 return result; BELLMAN-FORD(G \n: graph, src : node) 1 for each node v in G 2 do d[v] := .; parent[v] := UNDEF; 3 d[src] := 0; 4 for \neach node in G 5 do for each edge (v, w) of G 6 do if d[v]+ G(v, w) <d[w] 7 then d[w] := d[v]+ G(v, w); \n8 parent[w] := v; Figure 12. Pseudocode for experiments KNAPSACK(v : realarr,c : realarr,j : int,W : \nreal,) 1 if j =0 then return 0 2 else if W =0 then return 0 3 else if (c[j] >W ) then return KNAPSACK(v, \nc, j - 1,W ) 4 else z1 := KNAPSACK(v, c, j - 1,W ); 5 z2 := KNAPSACK(v, c, j - 1,W - c[j]); 6 return \nmax{z1,z2} INSERTION-SORT(A : realarr) 1 for i := 1 to (|A|- 1) 2 do z := A[i]; j := i - 1; 3 while j \n= 0 and A[j] >z 4 do A[j + 1] := A[j]; j := j - 1; 5 A[j + 1] := z; SELECTION-SORT(A : realarr) 1 for \ni := 1 to (|A|- 1) 2 do s := i; 3 for j := i +1 to (|A|- 1) 4 do if (A[j] <A[s]) s := j; 5 swap(A[i],A[s]); \n BUBBLE-SORT(A : realarr) 1 for i := 1 to (|A|- 1); 2 do for j := 1 to (|A|- 1); 3 do if (A[i] >A[i + \n1]) 4 then swap(A[i],A[i + 1]); might be to restructure the looping iteration space of the program before \nusing our proof rule for loops based on the epoch frame\u00adwork. Knapsack Algorithms. The integer-knapsack \nalgorithm takes as input a weight array c and a value array v containing the weight and value respectively \nof various objects, and a knapsack capacity Budget and returns the set of items with maximum combined \nvalue totv such that their combined weight is less than the knapsack capacity. The value of totv is discontinuous \nin c and Budget since small perturbations may make an object now no longer .t in the knapsack (or the \nother way round). However, it is interesting to note that totv is actually continuous in v. Our proof-rules \nare able to establish this property inductively across the different recursive calls of Knapsack after \nproving that continuity of the recursion\u00adfree part, which requires an application of proof rule ITE-1. \nAs for fractional knapsack, it was proved as in Example 13. Semi-automated Implementation We have developed \na prototype implementation of our proof-rule based analysis to establish con\u00adtinuity of programs written \nin the abstraction language LIMP. As mentioned earlier, conversion of programs to .t the abstraction \nlan\u00adguage LIMP needs to be done manually. This is a non-trivial step, but can potentially be automated \nand a detailed investigation is left for future work. The application of several proof rules requires \nestablishing equivalences of code-fragments. For example, the rule ITE-1 re\u00adquires the proof of equivalence \nof the two branches at the disconti\u00adnuities of the condition variable. The commutativity and synchro\u00adnization \ntermination condition in the rule LOOP is also a form of code-fragment equivalence check. Our implementation \nuses the Z3 SMT solver to discharge these equivalence obligations when the involved code-fragments are \nloop-free. There are few cases where such code-fragments involves loops. This happens, for example, when \nthe abstract loop to which the proof rule LOOP is applied involves a nested loop. For such cases, we \nmanually summarized the effect of the inner loop as a quanti.ed invariant to enable equiv\u00adalence checking \nby use of an SMT solver. Generation of such in\u00advariants can potentially be automated. An alternative \napproach to investigate might be to use specialized techniques for equivalence checking [19]. By applying \nvarious proof rules in a semi-automated manner as described above, our analysis is able to discover for \neach example the set of input variables under which the example is continuous. The performance results \nreported in table 1 were obtained on a Core2 Duo 2.53 Ghz with 4GB of RAM. 7. Conclusion and future work \nWe have presented a program analysis to automatically determine if a program implements a continuous \nfunction. The practical motiva\u00adtion is the veri.cation of robustness properties of programs whose inputs \ncan have small amounts of error and uncertainty. This work is the .rst in a planned series of papers \non the analysis of robustness and stability of programs, and its applications in the veri.cation of software \nrunning on cyber-physical systems [12]. In particular, we plan to explore the following questions: Quantitative \nanalysis. Rather than knowing whether a program is continuous, what if we want bounds on changes to outputs \non small changes to inputs? We plan to answer this question by developing a quantitative continuity analysis. \nSuch an analysis will be closely related to the problem of differentiating or .nite\u00addifferencing programs. \nSafe handling of discontinuities. Many practical programs are dis\u00adcontinuous but still safe e.g., a controller \nthat is otherwise contin\u00aduous might switch discontinuously from mode to mode. Reasoning about robustness \nin such a program will require, in addition to con\u00adtinuity proofs, speci.cations for how the program \nshould behave at discontinuities. But what would these speci.cations look like in applications of interest? \nCounterexample generation. Can we generate inputs that, when changed slightly, cause large changes in \nthe program s behavior? Modular analysis. What about interprocedural continuity analysis, hinted at in \nSec. 3? Applications outside robustness. Does our proof methodology have applications in a contexts outside \nof robustness? In particular, our proof rule LOOP establishes the observational determinism of non\u00addeterministic \nabstractions expressible in the language LIMP. Can it be used in determinism proofs for concurrent programs \n[22]? Stability. Can we extend the techniques here to do program analysis with respect to control-theoretic \nstability properties e.g., asymp\u00adtotic and Lyapunov stability [16, 17]? References [1] Rajeev Alur, Costas \nCourcoubetis, Thomas A. Henzinger, and Pei-Hsin Ho. Hybrid automata: An algorithmic approach to the speci.\u00adcation \nand veri.cation of hybrid systems. In Hybrid Systems, 1992. [2] Yamine A\u00a8it Ameur, G\u00b4erard Bel, Fr\u00b4ed\u00b4eric \nBoniol, S. Pairault, and Virginie Wiels. Robustness analysis of avionics embedded systems. In LCTES, \npages 123 132, 2003. [3] Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to \nalgorithms. MIT Press and McGraw-Hill, 1990. [4] Patrick Cousot. Proving the absence of run-time errors \nin safety\u00adcritical avionics code. In EMSOFT, pages 7 9, 2007. [5] Patrick Cousot, Radhia Cousot, J\u00b4er \nome Feret, Laurent Mauborgne, Antoine Min\u00b4e, David Monniaux, and Xavier Rival. The ASTRE E\u00b4analyzer. \nIn ESOP, pages 21 30, 2005. [6] Eric Goubault. Static analyses of the precision of .oating-point oper\u00adations. \nIn SAS, pages 234 259, 2001. [7] Eric Goubault, Matthieu Martel, and Sylvie Putot. Asserting the precision \nof .oating-point computations: A simple abstract interpreter. In ESOP, 2002. [8] Joseph Halpern. Reasoning \nabout uncertainty. The MIT Press, 2003. [9] Dick Hamlet. Continuity in sofware systems. In ISSTA, pages \n196 200, 2002. [10] Mats Per Erik Heimdahl, Yunja Choi, and Michael W. Whalen. De\u00adviation analysis: A \nnew use of model checking. Autom. Softw. Eng., 12(3):321 347, 2005. [11] Myron Kayton and Walter R. Fried. \nAvionics navigation systems. Wiley-IEEE, 1997. [12] Edward A. Lee. Cyber physical systems: Design challenges. \nIn ISORC, pages 363 369, 2008. [13] Matthieu Martel. Propagation of roundoff errors in .nite precision \ncomputations: A semantics approach. In ESOP, pages 194 208, 2002. [14] Antoine Min\u00b4e. Relational abstract \ndomains for the detection of .oating-point run-time errors. In ESOP, pages 3 17, 2004. [15] Bradford \nParkinson and James Spiker. The global positioning system: Theory and applications (Volume II). AIAA, \n1996. [16] Stefan Pettersson and Bengt Lennartson. Stability and robustness for hybrid systems. In Decision \nand Control, volume 2, pages 1202 1207, Dec 1996. [17] Andreas Podelski and Silke Wagner. Model checking \nof hybrid sys\u00adtems: From reachability towards stability. In HSCC, pages 507 521, 2006. [18] Mardavij \nRoozbehani, Alexandre Megretski, Emilio Frazzoli, and Eric Feron. Distributed lyapunov functions in analysis \nof graph models of software. In HSCC, pages 443 456, 2008. [19] Ofer Strichman. Regression veri.cation: \nProving the equivalence of similar programs. In CAV, 2009. [20] Wilson Sutherland. Introduction to metric \nand topological spaces. Oxford University Press, 1975. [21] John Taylor. An introduction to error analysis: \nthe study of uncertain\u00adties in physical measurements. University Science Books, 1997. [22] Tachio Terauchi \nand Alex Aiken. A capability calculus for concur\u00adrency and determinism. In CONCUR, pages 218 232, 2006. \n[23] Glynn Winskel. The formal semantics of programming languages. The MIT Press, 1993.    \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We present an analysis to automatically determine if a program represents a continuous function, or equivalently, if infinitesimal changes to its inputs can only cause infinitesimal changes to its outputs. The analysis can be used to verify the <i>robustness </i> of programs whose inputs can have small amounts of error and uncertainty---e.g., embedded controllers processing slightly unreliable sensor data, or handheld devices using slightly stale satellite data.</p> <p>Continuity is a fundamental notion in mathematics. However, it is difficult to apply continuity proofs from real analysis to functions that are coded as imperative programs, especially when they use diverse data types and features such as assignments, branches, and loops. We associate data types with metric spaces as opposed to just sets of values, and continuity of typed programs is phrased in terms of these spaces. Our analysis reduces questions about continuity to verification conditions that do not refer to infinitesimal changes and can be discharged using off-the-shelf SMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation in the value of a variable often leads to divergent control-flow that can lead to large changes in values of variables. Our proof rules identify appropriate ``synchronization points'' between executions and their perturbed counterparts, and establish that values of certain variables converge back to the original results in spite of temporary divergence.</p> <p>We prove our analysis sound with respect to the traditional epsilon-delta definition of continuity. We demonstrate the precision of our analysis by applying it to a range of classic algorithms, including algorithms for array sorting, shortest paths in graphs, minimum spanning trees, and combinatorial optimization. A prototype implementation based on the Z3 SMT-solver is also presented.</p>", "authors": [{"name": "Swarat Chaudhuri", "author_profile_id": "81309496839", "affiliation": "Pennsylvania State University, University Park, PA, USA", "person_id": "P1911039", "email_address": "", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1911040", "email_address": "", "orcid_id": ""}, {"name": "Roberto Lublinerman", "author_profile_id": "81317497568", "affiliation": "Pennsylvania State University, University Park, PA, USA", "person_id": "P1911041", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706308", "year": "2010", "article_id": "1706308", "conference": "POPL", "title": "Continuity analysis of programs", "url": "http://dl.acm.org/citation.cfm?id=1706308"}