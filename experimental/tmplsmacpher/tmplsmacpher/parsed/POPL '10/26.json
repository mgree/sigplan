{"article_publication_date": "01-17-2010", "fulltext": "\n From ProgramVeri.cation to Program Synthesis Saurabh Srivastava Sumit Gulwani JeffreyS. Foster University \nof Maryland, College Park Microsoft Research, Redmond University of Maryland, College Park saurabhs@cs.umd.edu \nsumitg@microsoft.com jfoster@cs.umd.edu Abstract This paper describes a novel technique for the synthesis \nof imper\u00adative programs. Automated program synthesis has the potential to make programming and the design \nof systems easier by allowing programs to be speci.ed at a higher-level than executable code. In our \napproach, which we call proof-theoretic synthesis, the user pro\u00advides an input-output functional speci.cation, \na description of the atomic operations in the programming language, and a speci.ca\u00adtion of the synthesized \nprogram s looping structure, allowed stack space, and bound on usage of certain operations. Our technique \nsynthesizes a program, if there exists one, that meets the input\u00adoutput speci.cation and uses only thegiven \nresources. The insight behind our approach is to interpret program synthe\u00adsis as generalized program \nveri.cation, which allows us to bring veri.cation tools and techniques to program synthesis. Our syn\u00adthesis \nalgorithm works by creating a program with unknown state\u00adments, guards, inductive invariants, and ranking \nfunctions. It then generates constraints that relate the unknowns and enforces three kinds of requirements: \npartial correctness, loop termination, and well-formedness conditions on program guards.We formalize \nthe requirements that program veri.cation tools must meet to solve these constraint and use tools from \nprior work as our synthesizers. Wedemonstratethe feasibilityofthe proposed approachbysyn\u00adthesizing programs \nin three different domains: arithmetic, sorting, and dynamic programming. Using veri.cation tools that \nwe previ\u00adouslybuilt in the VS3 project we are able to synthesize programs for complicated arithmetic \nalgorithms including Strassen s matrix multiplication and Bresenham s line drawing; several sorting algo\u00adrithms; \nand several dynamic programming algorithms. For these programs, the median time for synthesis is 14 seconds, \nand the ra\u00adtioof synthesistoveri.cation time ranges between1\u00d7 to 92\u00d7 (with an medianof7\u00d7), illustrating \nthe potential of the approach. Categories and Subject Descriptors I.2.2[Automatic Program\u00adming]: Program \nSynthesis; F.3.1[Logics and Meanings of Pro\u00adgrams]: Specifying andVerifying and Reasoning about Programs \nGeneralTerms Languages, Theory. Keywords Proof-theoretic program synthesis, veri.cation. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 10, January 17 23, \n2010, Madrid, Spain. Copyright c &#38;#169; 2010ACM 978-1-60558-479-9/10/01... $10.00 1. Introduction \nAutomated program synthesis, despite holding the promise of sig\u00adni.cantly easing the task of programming, \nhas received little atten\u00adtion due to its dif.culty. Being able to mechanically construct pro\u00adgrams has \nwide-ranging implications. Mechanical synthesis yields programs that are correct-by-construction. It \nrelieves the tedium and error associated with programming low-level details, can aid in automated debugging \nand in general leaves the human programmer freetodealwiththehigh-leveldesignofthesystem. Additionally, \nsynthesis could discover new non-trivial programs that are dif.cult for programmers tobuild. In this \npaper, we present an approach to program synthesis that takes the correct-by-construction philosophy \nof program de\u00adsign [14, 18, 38] and shows how it can be automated. Program ver\u00adi.cation tools routinely \nsynthesize program proofs in the form of inductive invariants for partial correctness and ranking functions \nfor termination. We encode the synthesis problem as a veri.ca\u00adtion problembyencoding program guards and \nstatements as logical factsthatneedtobediscovered.Thisallowsustouse certainveri.\u00adcation tools for synthesis. \nThe veri.cation tool infers the invariants and ranking functionsas usual,butinaddition infersthe program \nstatements, yielding automated program synthesis.We call our ap\u00adproach proof-theoretic synthesis because \nthe proof is synthesized alongside the program. We de.ne the synthesis task as requirements on the output \npro\u00adgram: functional requirements, requirements on the form of pro\u00adgram expressions and guards, and requirements \non the resources used. Thekeyto our synthesis algorithm is the reduction from the synthesis task to three \nsets of constraints. The .rst set are safety conditions that ensure the partial correctness of the loops \nin the pro\u00adgram. The second set are well-formedness conditions on the pro\u00adgram guards and statements, \nsuch that the output from the veri.ca\u00adtion tool (facts corresponding to program guards and statements) \ncorrespond to valid guards and statements in an imperative lan\u00adguage. The third set are progress conditions \nthat ensure that the program terminates. To our knowledge, our approach is the .rst that automatically \nsynthesizes programs and their proofs, while previous approaches have either used given proofs to extract \npro\u00adgrams [27] or made no attempt to generate the proof. Some ap\u00adproaches, while not generating proofs, \ndo ensure correctness for a limited class of .nitizable programs [29]. To illustrate our approach, we \nnext show how to synthesize Bresenham s line drawing algorithm. This example is an ideal candidate for \nautomated synthesis because, while the program s requirements are simple to specify, the actual program \nis quite involved. 1.1 Motivating Example As a motivating example, we consider a well-known algorithm \nfrom the graphics community called Bresenham s line drawing algorithm, shown in Figure 1(a). The algorithm \ncomputes (and Figure 1. (a) Bresenham s line drawing algorithm (b) The invariant and ranking function \nthat prove partial correctness and termination, respectively. (c) The algorithm written in transition \nsystem form, with statements as equality predicates, guarded appropriately.  (a) Bresenhams(int X, Y \n) {v1:=2Y -X; y:=0; x:=0; while (x = X) out[x]:=y; if (v1 < 0) v1:=v1+2Y ; else v1:=v1+2(Y -X); y++; \nx++; return out; } (b) Bresenhams(int X, Y ) {[]true . v'1=2Y -X . y'=0 . x'=0 while (x = X) []v1 < 0 \n. out'=upd(out,x,y) . v'1 =v1+2Y . y'=y . x'=x+1 []v1 = 0 . out'=upd(out,x,y) . v'1 =v1+2(Y -X) . y'=y+1 \n. x'=x+1 return out; } (c) Invariant t : 0 < Y = X . v1 = 2(x+1)Y -(2y+1)X . 2(Y -X) = v1 = 2Y . .k : \n0 = k < x . 2|out[k]-(Y/X)k|=1 Ranking function . : X - x writes to the output array out)the discrete \nbest-.t line from(0, 0) to (X, Y ), where the point (X, Y ) is in the NE half-quadrant, i.e., 0 <Y = \nX. The best-.t line is one that does not deviate more thanhalfapixelaway fromthereal line,i.e., |y - \n(Y/X)x|= 1/2. For ef.ciency, the algorithm computes the pixel values (x, y) of this best-.tlineusingonly \nlinear operations,butthe computationis non-trivial and the correctness of the algorithm is also not evident. \nThe speci.cation for this program is succinctly written in terms of its precondition tpre and postcondition \ntpost : tpre :0 <Y = X tpost : .k :0 = k = X . 2|out[k]-(Y/X)k|=1 Notice that in the postcondition, we \nhave written the assertion out\u00adside the loop body for clarity of presentation, but it can easily be rewritten, \nas a quanti.er-free assertion, inside. Bresenham pro\u00adposed the program shownin Figure 1(a) to implement \nthis speci.\u00adcation. The question we answer is whether it is possible to synthe\u00adsize the program given \njust the speci.cation and a description of the available resources (control .ow, stack space and operations). \nLet us stepwisedevelop the idea behind synthesis startingfrom the veri.cation problem for thegiven program. \nObserve that we can write program statements as equality pred\u00adicates and acyclic fragments as transition \nsystems.Forexample, we ' can write x := e as x= e, where x' is a renaming of x to its out\u00adputvalue.Wewill \nwrite statements asequalities betweenthe output (primed) versions of the variables and the expression \n(over the un\u00adprimed versions of the variables). Also, guards that direct control .ow in an imperative \nprogram can now be seen as guards for state\u00admentfactsina transition system. Figure 1(b) shows ourexample \nwrittenin transition system form.To prove partial correctness, one can write down the inductive invariant \nfor the loop and verify that theveri.cation conditionforthe programisinfactvalid.Thever\u00adi.cation condition \nconsists of four implications for the four paths corresponding to the entry, exit, and one each for the \nbranches in the loop. Using standard veri.cation condition generation, with the precondition tpre and \npostcondition tpost , and writing the renamed version of invariant t as t', these are tpre . sentry . \nt' t .\u00acgloop . tpost (1) t . gloop . gbody1 . sbody1 . t' t' t . gloop . gbody2 . sbody2 . where we use \nsymbols for the various parts of the program: gbody1 : v1 < 0 gbody2 : v1 = 0 gloop : x = X ''' sentry \n: v1 =2Y -X . y=0 . x=0 ' sbody1 : out'=upd(out, x, y) . v=v1+2Y . y'=y . x'=x+1 1 ' sbody2 : out'=upd(out, \nx, y) . v=v1+2(Y -X) . y'=y+1 . x'=x+1 1 Withalittlebitofwork, one canvalidate that the invariant t shown \nin Figure 1(c) satis.es Eq. (1). Checking the validity of given in\u00advariants can be automated using SMT \nsolvers [10]. Infact, pow\u00aderful program veri.cation tools now exist that can generate .xed\u00adpoint solutions \ninductive invariants such as t automatically us\u00ading constraint-based techniques [6, 21, 32], abstract \ninterpreta\u00adtion [9] or model checking [3]. There are alsotools that can prove termination [7] by inferring \nranking functions such as . and to\u00adgether with the safety proof provide a proof for total correctness. \n The insight behind our paper is to ask the question, if we can infer t in Eq. (1), then is it possible \nto infer the guards gi s and the statements si s at the same time?We have found that we can in\u00addeed infer \nguards and statements as well,by suitably encoding pro\u00adgrams as transition systems, asserting appropriate \nconstraints, and then leveraging program veri.cation techniques to do a systematic (lattice-theoretic) \nsearch for unknowns in the constraints. Here the unknowns nowrepresent both the invariants and the statements \nand guards. It turns out that a direct solution to the unknown guards and statements may be uninteresting, \ni.e., it may not correspond to real programs. But we illustrate that we can impose additional well-formedness \nconstraints on the unknown guards and statements such that any solution to this new set of constraints \ncorresponds to a valid, real program. Additionally, even if we synthesize valid programs, it may be that \nthe programs are non-terminating. There\u00adfore we need to impose additional progress constraints that ensure \nthat the synthesized programs are ones that we can actually run. We now illustrate the need for these \nwell-formedness and progress constraints over our example. Suppose that the statements sentry , sbody1 \nand sbody2 , are un\u00adknown. A trivial satisfying solution to Eq. (1) may set all these unknowns to false. \nIf we use a typical program veri.cation tool that computes least .xed-points starting from .,then indeed, \nit will output this solution. On the other hand, let us make the conditional guards gbody1 and gbody2 \nunknown.Again, gbody1 = gbody2 = false isa satisfying solution.Wegetuninteresting solutions becausethe \nunknowns are not constrained enough to ensure valid statements V ' and control-.ow. Statement blocks \nare modeled as i x= ei, i ' with one equality for each output variable xi and expressions ei are over \ninput variables. Therefore, false does not correspond to any valid block. Similarly gbody1 = gbody2 = \nfalse does not cor\u00adrespond to any valid conditional with two branches.Forexample, consider if (g) S1 \nelse S2 with two branches. Note how S1 and S2 are guarded by g and \u00acg, respectively, and g .\u00acg holds.For \nevery valid conditional, the disjunction of the guards is always a tautology. In veri.cation, the program \nsyntax and semantics en\u00adsure the well-formedness of acyclic fragments. In synthesis, we will need to \nexplicitly constrain well-formedness of acyclic fragments (Section 3.4). Next, suppose that the loop \nguard gloop is unknown. In this case if we attempt to solve for the unknowns t and gloop , then one valid \nsolution assigns t = gloop = true, which corresponds to an non-terminating loop. In veri.cation, we were \nonly concerned with partial correctness and assumed that the program was termi\u00adnating. In synthesis, \nwe will need to explicitly encode progress by inferring appropriate ranking functions, like . in Figure \n1(c), to prevent the synthesizer from generating non-terminating programs (Section 3.5). Note that our \naim is not to solve the completely general synthe\u00adsis problem for a given functional speci.cation. Guards \nand state\u00adments are unknownsbut theytakevalues fromgiven domains, spec\u00adi.ed by the user as domain constraints, \nso that a lattice-theoretic search can be performed by existing program veri.cation tools. Also notice \nthat we did not attempt to change the number of invari\u00adants or the invariant position in the constraints. \nThis means that we assume a given looping or .owgraph structure, e.g., one loop for our example. Lastly, \nas opposed to veri.cation, the set of program variables is not known, and therefore we need a speci.cation \nof the stackspace available and also a bound on the type of computations allowed. Weusethe speci.cations \nto construct anexpansionthatisapro\u00adgram with unknown symbols and construct safety conditions over the \nunknowns.We then impose the additional well-formedness and progress constraints.We call the new constraints \nsynthesis condi\u00adtions and hope to .nd solutions to them using program veri.cation tools. The constraints \ngenerated are non-standard, and therefore to solvethem we needveri.cation toolsthat satisfy certain properties. \nVeri.cation tools we developed in previous work [32, 21] indeed have those properties. We use them to \nef.ciently solve the syn\u00adthesis conditions to synthesize programs, with a very acceptable slowdown over \nveri.cation. The guards, statements and proof terms for the example in this section come from the domain \nof arithmetic. Therefore, a programveri.cation toolfor arithmeticwouldbe appropriate.For programs whose \nguards and statements are more easily expressed in other domains, a corresponding veri.cation tool for \nthat domain shouldbe used.Infact, wehave employed tools for the domainsof arithmetic and predicate abstraction \nfor proof-theoretic synthesis with great success. Our objective is to reuse existing veri.cation technology \nthat started with invariant validation and progressed to invariant inference and push it further to program \nsynthesis.  1.2 Contributions This paper makes the following contributions: We present a novel way \nof specifying a synthesis task as a triple consisting of the functional speci.cation, the domains of \nexpressions and guards that appear in the synthesized program, and resource constraints that the program \nis allowed to use (Section 2).  We view program synthesis as generalized program veri.ca\u00adtion. We formally \nde.ne constraints, called synthesis condi\u00adtions, that can be solved using veri.cation tools (Section \n3).  We present requirements that program veri.cation tools must meet in order to be used for synthesis \nof program statements and guards (Section 4).  Webuild synthesizers usingveri.cation tools and present \nsyn\u00adthesis results for the three domains of arithmetic, sorting and dynamic programming (Section 5). \n 2. The Synthesis Scaffold andTask We now elaborate on the speci.cations that a proof-theoretic ap\u00adproach \nto synthesis requires and how these also allow the user to specify the space of interesting programs. \nWe describe the synthesis problem using ascaffold of the form (F, D, R) The three components are as \nfollows: 1. Functional Speci.cation The .rst component F of a scaffold describes the desired precondition \nand postcondition of the synthe\u00adsized program. Let vvin and vvout be the vectors containing the input \nand output variables, respectively. Then a functional speci.cation F =(Fpre (vvin ),Fpost (vvin , vvout \n)) is a tuple containing the formu\u00adlaethatholdattheentryandexit program locations.Forexample, . for \nthe program in Figure 1, Fpre (X, Y ) = (0 <Y = X and . Fpost (X, Y, out)= .k :0 = k = X . 2(Y/X)k - \n1 = 2out[k] = 2(Y/X)k +1. 2. Domain Constraints The second component D of the scaffold describesthe domainsforexpressionsand \nguardsinthe synthesized program. The domain speci.cation D=(Dexp ,Dgrd ) isatuple that constrains the \nrespective components: 2a. Program Expressions: Theexpressions manipulatedbythe pro\u00adgram come from the \ndomain Dexp . 2b. Program Guards: The logical guards (boolean expressions) used to direct control .ow \nin the program come from the do\u00admain Dgrd . For example, for the program in Figure 1, the domainsDexp \n,Dgrd are both linear arithmetic. 3. Resource Constraints The third component R of the scaffold describes \nthe resources that the synthesized program can use. The resource speci.cation R =(Rflow ,Rstack ,Rcomp \n) is a triple of resource templates that the user must specify for the .owgraph, stack and computation, \nrespectively: 3a. Flowgraph Template We restrict attention to structured pro\u00adgrams (those that are goto-less, \nor whose .owgraphs are re\u00adducible [22]). The structured nature of such .owgraphs allows us to describe \nthem using simple strings. The user speci.es a string Rflow from the following grammar: T ::= .|*(T ) \n| T ;T (2) where . denotes an acyclic fragment of the .ow graph, *(T ) denotes a loop containing the \nbody T and T ;T denotes the sequential composition of two .ow graphs. For example, for the program in \nFigure 1, Rflow = .;*(.). 3b. Stack Template A map Rstack : type . int indicating the number of extra \ntemporary variables of each type available to the program. For example, for the program in Figure 1, \nRstack =(int, 1). 3c. ComputationTemplate At times it may be important to put an upper bound on the number \nof times an operation is performed insidea procedure.Amap Rcomp : op . int of operations op to the upper \nbound speci.es this constraint. For example, for the program in Figure 1, Rcomp = \u00d8 which indicates that \nthere are no constraints on computation. On the one hand, the resource templates makesynthesis tractable \nbyenablingasystematic lattice-theoretic search, while on the other theyallow the user to specify the \nspace of interesting programs and canbeusedasa feature.For instance,the usermaywishto reduce memory consumption \nat the expense of a more complex .owgraph and still meet the functional speci.cation. If the user does \nnot care, then the resource templates can be considered optional and left unspeci.ed. In this case, the \nsynthesizer can iteratively enumerate possibilities for each resource and attempt synthesis with increas\u00ading \nresources. 2.1 Pickingaproof domain anda solverfor the domain Our synthesis approach is proof-theoretic \nand we synthesize the proof terms, i.e., invariants and ranking functions, alongside the program. These \nproof terms will takevalues fromasuitably chosen proof domain Dprf . Notice that Dprf will be at least \nas expressive as Dgrd and Dexp . The user chooses an appropriate proof domain and also picksa solver \ncapableof handling that domain.We will use programveri.cation tools as solvers and typically, the user \nwill pick the most powerful veri.cation tool available for the chosen proof domain.  2.2 SynthesisTask \nGiven a scaffold (F, D, R), we call an executable program valid with respect to the scaffold if it meets \nthe following conditions. When called with inputs vvin that satisfy Fpre (vvin ) the program terminates, \nand the resulting outputs vvout satisfy Fpost (vvin , vvout ). There are associated invariants and ranking \nfunctions that pro\u00advidea proofof thisfact.  There is a program loop (with an associated loop guard g) \ncorresponding to each loop annotation (speci.ed by * ) in the .owgraph template Rflow . The program contains \nstatements from the following imperative language IML for each acyclic fragment (speci.ed by . ).  S \n::= skip | S;S | x := e | if g then S else S Where x denotes a variable, e denotes some expression, and \ng denotes some predicate. (Memory reads and writes are modeled using memory variables and select/update \nexpressions.) The domainofexpressionsandguardsisas speci.edbythescaffold, i.e., e . Dexp and g . Dgrd \n. The program only uses as manylocal variables as speci.ed by Rstack in addition to the input and output \nvariables vvin , vvout .  Each elementary operation only appears as manytimes as spec\u00adi.ed in Rcomp \n.  EXAMPLE 1 (Square Root). Let us consider a scaffold with func\u00adtional speci.cation F =(x = 1, (i - \n1)2 = x<i2), whichstates that the program computes the integral square root of the input x v , i.e., \ni - 1= LxJ. Also, let the domain constraints Dexp ,Dgrd be limited to linear arithmetic expressions, \nwhich means that the program cannot use any native squareroot or squaring operations. Lastly, let the \nRflow , Rstack and Rcomp be .;*(.);., {(int, 1)} and \u00d8,respectively.Aprogram thatis valid withrespectto \nthis scaffold is the following: IntSqrt(int x) { Invariant t: v:=1;i:=1; v=i2 .x=(i-1)2 .i = 1 whilet,.(v \n= x) v:=v+2i+1;i++; Ranking function .: return i-1; x - (i-1)2 }where v, i are the additional stack \nvariable and loop iteration counter (and reused in the output), respectively. Also, the loop is annotated \nwith the invariant t and ranking function . as shown, and whichprove partial correctness and termination, \nrespectively. In the next two sections, we formally describe the steps of our synthesis algorithm. We \n.rst generate synthesis conditions (Section 3), which are constraints over unknowns for statements, guards,loopinvariantsand \nranking functions.Wethen observethat theyresemble veri.cation conditions, and we can employ veri.ca\u00adtion \ntools, if theyhave certain properties, to solve them (Section 4). 3. Synthesis Conditions In this section, \nwe de.ne and construct synthesis conditions for an input scaffold (F, D, R). Using the resource speci.cation \nR, we .rst generate a program with unknowns corresponding to the fragments we wish to synthesize. Synthesis \nconditions then specify constraints on these unknowns and ensure partial correctness, loop termination \nand well-formedness of control-.ow. We begin our discussion by motivating the representation we use for \nacyclic fragments in the synthesized program. 3.1 UsingTransition Systems to Represent Acyclic Code \nSuppose we want to infer a set of (straight-line) statements that transform a precondition fpre to a \npostcondition fpost , where the relevant program variables are x and y. One approach might be to generate \nstatements that assign unknown expressions ex and ey to x and y, respectively: {fpre }x := ex; y := ey{fpost \n} Then we can use Hoare s axiom for assignment to generate the veri.cation conditionfpre . (fpost [y \n. ey])[x . ex]. However, this veri.cation condition is hard to automatically reason about because it \ncontains substitution into unknowns. Even worse, we have restricted the search space by requiring the \nassignment to y to follow the assignment to x, and by specifying exactly two assignments. Instead we \nwill represent the computation asa transition system which providesamuch cleaner mechanism for reasoning \nwhen pro\u00adgram statements are unknown.A transition in a transition system is a (possibly parallel) mapping \nof the input variables to the output variables.Variableshaveaninputversionandan outputversion(in\u00addicatedby \nprimed names), which allows themto change state.For our example, we can write a single transition: . \n'\u00b8 {fpre } x ' ,y = (ex,ey){fpost ' } Here f ' is the postcondition, written in terms of the output \npost variables, and ex,ey are expressions over the input variables. The veri.cation condition corresponding \nto this tuple is fpre . x ' = ex . y ' = ey . fpost ' . Note that every state update (assignment) can \nalways be written as a transition. We can extend this approach to arbitrary acyclic program frag\u00adments. \nA guarded transition (written []g . s)contains a state\u00adment s that is executed only if the quanti.er-free \nguard g holds.A transition system consists of a set {[]gi . si}i of guarded transi\u00adtions. It is easy \nto see that a transition system can represent any arbitrary acyclic program fragment by suitably enumerating \nthe paths through the acyclic fragment. The veri.cation condition for V {fpre }{[]gi . si}i{f ' } is \nsimply . gi . si . f ' ). post i(fpre post In addition to the simplicity afforded by the lack of anyorder\u00ading, \nthe constraints from transition systems are attractive for syn\u00adthesis as the program statements si and \nguards gi are facts just like the pre-and postconditions fpre and f ' . Given the lack of post differentiation, \nany(or all) can be unknowns in these synthesis con\u00additions. This distinguishes them fromveri.cation conditions \nwhich can only have unknown invariants, or often the invariants must be known as well. Synthesis conditions \ncan thus be viewed as generalizations of veri.cation conditions. Program veri.cation tools routinely \ninfer .xed-point solutions (invariants) that satisfy the veri.cation condi\u00adtions with known statements \nand guards.With our formulation of statements and guards as just additionalfacts in the constraints, \nit is possibleto use(suf.ciently general)veri.cation toolsto inferin\u00advariants and program statements \nand guards. Synthesis conditions serve an analogous purpose to synthesis as veri.cation conditions doto \nveri.cation.Ifaprogramis correct (veri.able),thenitsveri\u00ad.cation condition is valid. Similarly, if a \nvalid program exists for a scaffold, then its synthesis condition has a satisfying solution. 3.2 Expandinga \n.owgraph We synthesize code fragments for each acyclic fragment and loop annotation in the .owgraph template \nas follows:  Acyclic fragments: For each acyclic fragment annotation . , we infer a transition system \n{gi . si}i, i.e., a set of assign\u00adments si, stated as conjunctions of equality predicates, guarded byquanti.er-free \n.rst-order-logic (FOL) guardsgi such that the disjunction of the guards is a tautology. Suitably constructed \nequality predicates and quanti.er-free FOL guards are later translated to executable code assignment \nstatements and con\u00additional guards, respectively in the languageI ML.  Loops: For each loop annotation \n* we infer three elements. The .rst is the inductive loop invariant t, which establishes partial correctness \nof each loop iteration. The second is the ranking function ., which proves the termination of the loop. \nBoth the invariant and ranking function take values from the proof domain, i.e., t,. . Dprf . Third, \nwe infer a quanti.er\u00adfree FOL loop guard g.  Formally,the outputofexpanding.owgraphswillbeaprogram in \nthe transition system language TSL (note the correspondence to the .owgraph grammar from Eq. 2): p ::= \nchoose {[]gi . si}i | whilet,.(g) do {p}| p;p V Here each si isa conjunctionof equality predicates, i.e., \nj (xj = ej ). We will use pvto denote a sequence of program statements in TSL. Note that we model memory \nread and updates using se\u00adlect/update predicates. Therefore, in x = e the variable x could be a memory \nvariable and e could be a memory select or update expression. Given a string for a .owgraph template, \nwe de.ne an expan\u00adsion function Expand : int \u00d7Dprf \u00d7R \u00d7D \u00d7Rflow . TSL that introduces fresh unknowns \nfor missing guards, statements and in\u00ad n,Dprf variants that are to be synthesized. ExpandD,R (Rflow ) \nexpandsa .owgraph Rflow andis parametrizedby an integer n that indicates the number of transition each \nacyclic fragment will be expanded to, the proof domain and the resource and domain constraints. The expansion \noutputsa programin the languageTSL. n,Dprf Expand(.)= choose {[]gi .si}i=1..n gi,si : fresh D,R unknowns \nn,Dprf Expand(*(T )) = whilet,. (g) { t, ., g : fresh D,R n,Dprf ExpandD,R (T ); unknowns } n,Dprf n,Dprf \nn,Dprf Expand(T1;T2)= Expand(T1);Expand(T2) D,RD,RD,R Each unknown g, s, t generated during the expansion \nhas the fol\u00adlowing domain inclusion constraints. t . Dprf |V g . Dgrd |V V s . i xi = ei where xi . \nV, ei . Dexp |V Here V = vvin . vvout . T . L is the set of variables: the input vvin and output vvout \nvariables, the set of temporaries (local variables) T as speci.edby Rstack , and the set of iteration \ncounters and ranking function tracker variables is L (which we elaborate on later), one for each loop \nin the expansion. The restriction of the domains by the variable set V indicates that we are interested \nin the fragment ofthe domainoverthevariablesin V . Also, the set of operations in ei is boundedby Rcomp \n. The expansion has some similarities to the notion of a user\u00adspeci.ed sketch in previous approaches \n[31, 29]. However, the un\u00adknowns in the expansion here are more expressive than the integer unknowns \nconsidered earlier,andthisallowsusto performa lattice search as opposed to the combinatorial approaches \nproposed ear\u00adlier. Notice that the unknowns t, g, s, . we introduce can all be in\u00adterpreted as boolean \nformulae(t,g naturally; s using our transition modeling; and . as .>c, for some constant c), and consequently \nordered in a lattice. EXAMPLE 2. Let us revisit the integral square root computation from Example 1. \nExpanding the .owgraph template .;*(.);. with n =1 yields expsqrt: choose {[]g1 . s1} ; t . Dprf |V \nwhilet,. (g0) { g1,g2,g3 . Dgrd |V V choose {[]g2 . s2} ; s1,s2,s3 . xi = ei }; i xi . V, ei . Dexp \n|V choose {[]g3 . s3} where V = {x, i, r, v}. The variables i and r are the loop iteration counter and \nranking function tracker variable, respectively, and v is the additional local variable. Also, the chosen \ndomains for proofs Dprf , guards Dgrd andexpressions Dexp are FOL factsover quadratic expressions, FOL \nfacts over linear arithmetic and linear arithmetic, respectively. Notice that the expansion encodes everything \nspeci.ed by the do\u00admain and resource constraints and the chosen proof domain. The only remaining speci.cation \nis F, which we will use in the next section to construct safety conditions over the expanded scaffold. \n 3.3 EncodingPartial Correctness: Safety Conditions Now that we have the expanded scaffold we need to \ncollect the constraints (safety conditions) for partial correctness impliedbythe simple pathsin theexpansion. \nSimple paths (straight-line sequence of statements) start at a loop header Fpre and end at a loop header \nor programexit.Theloop headers, programentryand programexit are annotated with invariants, precondition \nFpre and postcondition Fpost , respectively. Let f denote formulae, that represent pre and postconditions \nand constraints. Then we de.ne PathC : f \u00d7 TSL \u00d7 f . f as a function that takes a precondition, a sequence \nof statements and a postcondition and outputs safety constraints that encode the validity of the Hoare \ntriple. Let us .rst describe the simple cases of constraints from a single acyclic fragment and loop: \nPathC(fpre , (choose {[]gi . si}i ),fpost )= V ' (fpre . gi . si . fpost ) PathC(fpre , (whilet,. (g) \n{pvl}),fpost )= fpre . t ' . PathC(t . g, vpl,t ) . (t .\u00acg . fpost ' ) i Here fpost ' and t ' are the \npostcondition fpost and invariant t but with all variables renamed to their output (primed) versions. \nSince the constraints need to refer to output postconditions and invariants the rule fora sequenceof \nstatementsisa bit complicated.For sim\u00adplicity of presentation, we assume that acyclic annotations do \nnot appear in succession. This assumption holds without loss of gener\u00adality because it is always possible \nto collapse consecutive acyclic fragments, e.g., two consecutive acyclic fragments with n transi\u00adtions \neach can be collapsed into a single acyclic fragment with n 2 transitions.Foref.ciency,itis prudenttonotmakethis \nassumption in practice, and the construction here generalizes easily.Fora se\u00adquence of statements in \nTSL, under the above assumptions, there are three cases to consider. First, a loop followed by statements \npv. Second, an acyclic fragment followed by just a loop. Third, an acyclic fragment, followed by a loop, \nfollowed by statements pv. Each of these generates the following, respective, constraints: PathC(fpre \n, (whilet,. (g) {pvl};vp),fpost )= (fpre . t ' ) . PathC(t . g, vpl,t ) . PathC(t .\u00acg, vp, fpost ) PathC(fpre \n, (choose {[]gi . si}i ;whilet,. (g) {vpl}),fpost )= V i(fpre . gi . si . t ' ) . PathC(t . g, vpl,t \n) . (t .\u00acg . fpost ' ) PathC(fpre , (choose {[]gi . si}i ;whilet,. (g) {vpl};pv),fpost )= V i(fpre . \ngi . si . t ' ) . PathC(t . g, vpl,t ) . PathC(t .\u00acg, vp, fpost ) The safety condition for a scaffold \nwith functional speci.ca\u00adtion F =(Fpre ,Fpost ), .owgraph template Rflow , and expansion exp = ExpandD,R \n(Rflow ) is given by: n,Dprf SafetyCond(exp, F)= PathC(Fpre , exp, Fpost ) (3) EXAMPLE 3. Consider the \nexpanded scaffold (from Example 2) and the functional speci.cation F (from Example 1) for integral square \nroot. The loop divides the program into three simple paths, which results in SafetyCond(expsqrt, F): \nx = 1 . g1 . s1 . t ' . t . g0 . g2 . s2 . t ' . t .\u00acg0 . g3 . s3 . (i ' - 1)2 = x ' . x ' <i '2 Notice \nthat gi,si,t are all unknown placeholder symbols.  3.4 EncodingValid Control:Well-formedness Conditions \nWe next construct constraints to ensure the well-formedness of choose statements. In the preceding development, \nwe treated each path through the choose statement as independent. In any ex\u00adecutable program control \nwill always .ow through at least one branch/transition of the statement, and each transition will contain \nwell-formed assignment statements.We .rst describea constraint that encodes this directly and then discuss \nan alternative way of ensuring well-formedness of transition guards. Non-iterative upper bounded search \nDuring the expansion of a scaffold, we can choose n to be greater than the number of transitions expected \nin any acyclic fragment. Any excess transi\u00adtions will have their guards instantiated to false. For any \nstate\u00adment choose {[]gi . si} in the expansion, we impose the well\u00adformedness constraint: `V \u00b4 . WellFormTS({[]gi \n. si}i)= valid(si) Valid transition `iW\u00b4 (4) . i gi Covers space Here the predicate valid(si) ensures \none and only one equality assignment to each variable in si. This condition ensures that each si corresponds \nto a well-formed transition that can be translated to executable statements. The second term constrains \nthe combination oftheguardstobea tautology.Notethatthisis importantto ensure that eachtransitionsystemis \nwell-formedand canbe convertedtoa validexecutable conditional.Forexample, consider theexecutable conditional \nif (G) then x := E1 else x := E2. The correspond\u00ading transition system is {[]g1 . (x ' = E1), []g2 . \n(x ' = E2)}, where g1 = G and g2 = \u00acG and g1 . g2 holds. In every well\u00adformed executable conditional \nthe disjunction of the guards will be a tautology. The second term imposes this constraint. Notice that \nthis construction does not constrain the guards to be disjoint (mutually exclusive). Disjointedness is \nnot required for correctness [11] because if multiple guards are triggered then arbitrarily choosing \nthe body for any one suf.ces. Therefore, without loss of generality, the branches can be arbitrarily \nordered (thus ensuring mutual exclusivity) in the output to get a valid imperative program. Iterative \nlower bounded search Notice that Eq. (4) is non\u00adstandard, i.e., it is not an implication constraint like \ntypical ver\u00adi.cation conditions, and we will elaborate on this in Section 4. For the case when a program \nveri.cation tool is unable to handle such non-standard constraints, we need a technique for ensuring \nwell-formedness of transitions without asserting Eq. (4). We .rst assume that valid(si) holds, and we \nwill show in Section 4.3 the conditions under which it does. Then all we need to ensure well-formedness \nis that .igi is a tautology. Since the transitions of a choose statement represent independent execution \npaths, wecan perform an iterativesearch for the guards gi.Westart by .nding any satisfying guard (and \ncorresponding transition) which canevenbe false.Wethen iteratively ask for another guard (and transition) \nsuch that the space de.ned by the new guard is not entirely contained in the space de.ned by the disjunction \nof the guards already generated. If we ensure that at each step the newly discovered guard covers some \nmore space that was not covered by earlier guards, then eventually the disjunction of all will be a tautology. \n More formally, suppose n such calls result in the transition system {[]gi . si}i=1..n,and.i=1..ngi is \nnot alreadyatautology. Then for the n +1st transition, we assert the constraint \u00ac(gn+1 . (.i=1..ngi)). \nThis constraint ensures that gn+1 will cover some space not covered by .i=1..ngi.We repeat until .igi \nholds. This iterative search for thetransitions also eliminates the need to guess the value of n. Well-formedness \nof an Expanded Scaffold We constrain the well-formedness of each transition system in the expanded scaf\u00adfold \nexp = ExpandD,R (Rflow ) using Eq. (4). n,Dprf ^ WellFormCond(exp)= WellFormTS({[]gi . si}i) (5) choose \n{[]gi.si}i .cond(exp) where cond(exp) recursively examines the expanded scaffold exp and returns the \nset of all choose statements in it. EXAMPLE 4. For the expanded scaffold in Example 2, since each acyclic \nfragment only contains one guarded transition, the well-formedness constraints are simple and state that \neach of g1,g2,g3 = true and valid(s1).valid(s2).valid(s3) holds. 3.5 Encoding Progress: Ranking functions \nUntil now our encoding has focused on safety conditions that, by themselves, only ensure partial correctness \nbut not termination. Next, we add progress constraints to ensure that the synthesized programs terminate. \nTo encode progress for a loop l = whilet,.l (g) do {pv}, we assert the existence of a ranking function \nas an unknown (numer\u00adical) expression .l that is lower bounded and decreases with each iteration of the \nloop. Because .l is an unknownexpression it is dif\u00ad.cultto encode directlythatit decreases. Therefore,we \nintroducea tracking variable rl, such that rl = .l.We use rl to remember the value of the ranking function \nat the head of the loop, and because it is a proof variable no assignments to it can appear in the body \nof the loop. On the other hand, .l changes due to the loop body, and at the end of the iteration we can \nthen check if the new value is strictly less than the old value, i.e., rl >.l.Without loss of gen\u00aderality, \nwe pick a lower bound of 0 for the tracking variable and conservatively assume that the termination argument \nis implied by the loop invariant t, i.e, t . rl = 0. Now that we have asserted the lower bound, what \nremains is to assert that .l decreases in each iteration. Assume, for the time being, that the body does \nnot contain any nested loops. Then we can capture the effect of the loop body using PathC as de.ned earlier, \nwith precondition t . g and postcondition rl >.. Then, the progress constraint for loop l without anyinner \nloop is: . prog(l)=(rl = .l . (t . rl = 0) . PathC(t . g, vp, rl >.l)) Using the above de.nition of \nprogress we de.ne the progress constraint for the entireexpanded scaffold exp = ExpandD,R (Rflow ): n,Dprf \n^ RankCond(exp)= prog(l) (6) l.loops(exp) where loops(exp) recursively examines the expanded scaffold \nexp and returns the set of all loops in it. EXAMPLE 5. Intheexpandedscaffoldof Example2thereisonly one \nloop, whoseranking function we denoteby .l and with tracker rl. Then wegenerate the followingprogress \nconstraint: rl = .l . (t . rl = 0) . (t . g0 . g2 . s2 . rl ' >. l' ) To relax the assumption we made \nearlier about no nesting of loops, we need a simple modi.cation to the progress constraint prog(l). Instead \nof considering the effect of the entire body pv(which nowcontains inner loops), we instead consider the \nfragment end(l) after the last inner loop in pv. Also, let tend denote the invariant for the last inner \nloop. Then, the progress constraint for loop l is: . prog(l)= rl = .l . (t . rl = 0) . PathC(tend , end(l),rl \n>.l) Noticethat becausetheloopinvariantsarenot decidedapriori,i.e., we are not doing program extraction, \nwe may assert that the invari\u00adants should be strong enough to satisfy the progress constraints. Speci.cally, \nwe have imposed the requirement that the intermedi\u00adate loop invariants carry enough information such \nthat it suf.ces to consider only the last loop invariant tend in the assertion.  3.6 Entire Synthesis \nCondition Finally, we combine the constraints from the preceding sections to yield the entire synthesis \ncondition for anexpanded scaffold exp = ExpandD,R (Rflow ). The constraint SafetyCond(exp, F) (Eq.3) \nn,Dprf ensurespartial correctnessofthe programwith respecttothefunc\u00adtional speci.cation. The constraint \nWellFormCond(exp) (Eq. 5) restricts the space to programs with valid control-.ow. The con\u00adstraint RankCond(exp) \n(Eq. 6) restricts the space to terminating programs. The entire synthesis conditionisgivenby sc = SafetyCond(exp, \nF).WellFormCond(exp).RankCond(exp) Notice that we have omitted the implicit quanti.ers for the sake of \nclarity. The actual form is .U.V : sc. The set V denotes the program variables, vvin . vvout . T . L \nwhere T is the set of temporaries (additional localvariables) as speci.edby the scaffold and L is the \nset of iteration counters and ranking function trackers. Also, U is the set of all unknowns of various \ntypes instantiated during the expansion of scaffold. This includes unknowns for the invariants t , the \nguards g and the statements s. EXAMPLE 6. Accumulating the partial correctness, well-formedness ofbranching \nandprogress constraints weget the following synthe\u00adsis condition (where we have removed the trivial guards \ng1,g2,g3 as discussed in Example 4): x = 1 . s1 . t ' . t . g0 . s2 . t ' . t .\u00acg0 . s3 . (i ' - 1)2 \n= x ' . x ' <i '2 . valid(s1) . valid(s2) . valid(s3) . rl = .l . (t . rl = 0) . (t . g0 . s2 . rl ' \n>. ' l) Here is a valid solution to the above constraints: t : v = i2 . x = (i - 1)2 . i = 1 g0 : v = \nx .l : x - (i - 1)2 ' '' (7) s1 : v =1 . i ' =1 . x = x . rl = rl ' '' s2 : v = v +2i +1 . i ' = i +1 \n. x = x . rl = rl ' '' s3 : v = v . i ' = i . x = x . rl = rl Notice how eachof the unknowns satisfy \ntheir domain constraints, i.e., t is from FOL over quadratic relations, .l is a quadratic expression,s1,s2,s2 \nareconjunctions of linear equalities and g0 is from quanti.er-free FOL over linear relations. In the \nnext section we show how suchsolutions can be computed using existing tools. Input:Scaffold(F, D, R), \nmaximum transitions n, proof domain Dprf Output:Executable program orFAIL begin n,Dprf exp := ExpandD,R \n(Rflow ); sc := SafetyCond(exp, F) . WellFormCond(exp) . RankCond(exp); p := Solver(sc); if (unsat(p)) \nthen return FAIL; return Exep(exp); end Algorithm1:The entire synthesis algorithm. Under the assumption \n[13] that every loop with a pre-and post\u00adcondition has an inductive proof of correctness, and every termi\u00adnating \nloop hasaranking function, and thatthe domains chosen are expressive enough, we can prove that the synthesis \nconditions, for the case of non-iterative upper bounded well-formedness, model the programfaithfully: \nTHEOREM 1 (Soundness and Completeness). The synthesis con\u00additions correspondingtoa scaffoldare satis.ableiffthereexistsa \nprogram (with a maximum of n transitions in each acyclic frag\u00adment where n is the parameter to the expansion) \nthat is valid with respect to the scaffold. Additionally, for the alternative approach to discovering \nguards (Section 3.4), we can prove soundness and relative completeness: THEOREM 2 (Soundness and Relative \nCompleteness). (a) Sound\u00adness: If there exists a program that is valid with respect to the scaffold then \nat each step of the iteration the synthesis conditions generated are satis.able.(b) Relative completeness: \nIf the iterative search for guards terminates then it .nds a program that is valid with respect to the \nscaffold. 4. Solving Synthesis Conditions In this section we describe how the synthesis conditions for \nan expanded scaffold can be solved using .xed-point computation tools (program veri.ers). Suppose we \nhave a procedure Solver(sc) that can generate solutions to a synthesis condition sc. Algorithm1is our \nsynthesis algorithm, which expands the given scaffold to exp, constructs synthesis conditions sc, uses \nSolver(sc) to generate a solution p to the unknowns that appear in the constraints and .nally generates \nconcrete programs (whose acyclic fragments are from the language IML from Section 2) using the postprocessor \nExep(exp). The concretization function Exep(exp) takes the solution p computed by Solver(sc) and the \nexpanded scaffold exp, and outputsa program whose acyclic fragments are fromthe language IML. The function \nde.nes a concretization for each statement in TSL and annotates each loop with its loop invariant and \nranking function: Exep(p;pv)= Exep(p);Exep(pv) Exep(whilet,.(g) do {vp})= whilep(t),p(.)(p(g)) { Exep(pv) \n}Exep(choose {[]g . s})= if (p(g)) {Stmt(p(s))} else {skip} Exep(choose {[]gi . si}i=1..n)= (where n> \n1) if (p(g1)) {Stmt(p(s1))} else {Exep(choose {[]gi . si}i=2..n)} where p maps each s to a conjunction \nof equalities and the con\u00adcretization function Stmt(s) expands the equality predicates to their corresponding \nstate updates: ^ . (t1 := e1; .. ;tn := en); Stmt( xi = ei)= (x1 := t1; .. ;xn := tn) i=1..n The above \nis a simple translation that uses additional fresh tem\u00adporary variables t1 ..tn to simulate parallel \nassignment. Alterna\u00adtively, one can use data dependencyanalysis to generate code that uses fewer temporary \nvariables. 4.1 Basic Requirementfor Solver(sc) Our objectiveisto useveri.cation toolsto implement Solver(sc), \nbut we realize that not all tools are powerful enough. For use as a solver for synthesis conditions, \nveri.cation tools require certain properties. Let us .rst recall [32] the notion of the polarity of unknowns \nin a formula. Let f be a FOL formula with unknowns whose occurrences are unique. Notice that all the \nconstraints we gen\u00aderate have unique occurrences as we rename appropriately. We can categorize unknowns \nas either positive or negative such that strengthening (weakening) the positive(negative)unknowns makes \nf stronger. Structurally, the nesting depth under negation in the boolean formula written using the basic \noperators(., ., \u00ac, ., .) de.nes whether an unknown is positive (even depth) or negative (odd depth).Forexample, \nthe formula (a .\u00acb) .\u00ac(\u00acc . d) has positive unknowns {a, c} and negative unknowns {b, d}. In program \nveri.cation we infer loop invariants given veri.ca\u00adtion conditions with known program statements. Let \nus reconsider theveri.cation conditioninEq.(1)withknown program statements and guards. Notice that the \nconstraints can be categorized into three forms, t . f1 . t ' , t . f2 . f3 and f4 . t ', where fi s \nde\u00adnote known formulae. Also, observe that these three are the only forms in which constraints in veri.cation \nconditions can occur. From these, we can see that the veri.cation conditions contain at most one positive \nand one negative unknown (using the disjunctive translation of implication), depending on whether the \ncorrespond\u00ading path ends or starts at an invariant. Program veri.cation tools implementing typical .xed-point \ncomputation algorithms are spe\u00adcialized to work solely with constraints with one positive and one negative \nunknown because there is no need to be more general. In fact, traditional iterative .xed-point computation \nis even more specialized in that it requires support for either just one pos\u00aditive unknown or just one \nnegative unknown.Traditional veri.ers work either in a forward (computing least .xed-point) or back\u00adwards \n(computing greatest-.xed point) direction starting with the approximation . or T, respectively, and iteratively \nre.ning it. Abackwards iterative data .ow analyzer always instantiates the positive unknown to the current \napproximation and uses the result\u00ading constraint (with only one negative unknown) to improve the approximation.Forexample, \nsupposethe current approximationto the invariant t is f5, then a backwards analyzer may instantiate t \n' in the constraint t . f1 . t ' to get the formula t . f1 . f5 ' (with one negative unknown t ). It \nwill then use the formula to improve the approximation by computing a new value for t that makes this \nformula satis.able. Similarly, a typical forwards iterative data .ow analyzer in\u00adstantiates the negative \nunknown to the current approximation and uses the resulting constraint (with only one positive unknown) \nto improve the approximation.Forexample, supposethe current ap\u00adproximation to the invariant t is f6, \nthen a forwards analyzer may instantiate t in the constraint t . f1 . t ' to get the formula f6 . f1 \n. t ' (with one positive unknown t ). It will then use the formula to improve the approximation by computing \na new value for t ' that makes this formula satis.able. In contrast, let us consider the components \n(from Section 3) of the synthesis condition. The component SafetyCond(exp) (Eq. (3)), in addition to \nthe unknowns due to the invariants t , contains unknowns for the program guards g and program state\u00adments \ns. These unknowns appear exclusively as negative un\u00adknowns, and there can be multiple such unknowns in \neach con\u00adstraint.Forexample,inEq.(1),the guardsand statement unknowns appear as negative unknowns. On \nthe other hand, the component WellFormCond(exp) (Eq. (5)) contains the well-formedness con\u00addition on \nthe guards .igi that is a constraint with multiple positive unknowns. Therefore we needaveri.er that \nsatis.es the following. REQUIREMENT 1. Support for multiple positive and multiple neg\u00adative unknowns. \nNotice this requirement is more general than that supported by typical veri.ers we discussed above. Nowconsider, \nanexample safety constraint such as t .g .s . t ' with unknowns t , g and s, that can be rewritten as \nt . t ' . \u00acg.\u00acs. Also, let us rewrite anexample well-formedness constraint .gi as true ..gi. This view \npresents an alternative explanation for Requirement1 in that we needa tool that can infer the right case \nsplit, which in most cases would not be unique and would require maintaining multiple orthogonal solutions. \nIntuitively, this is relatedtoa tool s abilityto infer disjunctivefacts. In the above we implicitly assumed \nthe invariant to be a con\u00adjunction of predicates. In the general case, we may wish to infer more expressive \n(disjunctive) invariants, e.g., of the form u1 . u2 or .k : u3 . u4, where ui s are unknowns. In this \ncase, multi\u00adple negative and positive unknowns appear even in the veri.cation condition and therefore \nthe veri.cation tool must satisfy Require\u00adment 1, which matches the intuition that disjunctive inference \nis required. 4.2 Constraint-basedVeri.ers as Solver(sc) Constraint-based .xed-point computation is a \nrelatively recent ap\u00adproach to program veri.cation that has been successfully used for dif.cult analyses \n[32]. In previous work, we designed ef.cient constraint-based veri.cation tools for two popular domains, \npred\u00adicate abstraction [32, 20] and linear arithmetic [21]. The tools for both domains satisfy Requirement \n1. Constraint-based veri.cation tools reduce a veri.cation condi\u00adtion vc (with invariant unknowns) to \na boolean constraint .(vc) suchthatasatisfying solutiontothe boolean constraint corresponds to valid \ninvariants. The property they ensure is the following: PROPERTY 1. The boolean constraint .(vc) is satis.able \niffthere exists a .xed-point solution for the unknowns corresponding to the invariants. The reduction \ncan also be applied to synthesis condition sc to get boolean constraints .(sc) anda similar property \nholds.Thatis,the boolean constraint .(sc) is satis.able iff there exist statements, guards and invariants \nthat satisfy the synthesis condition. 4.3 IterativeVeri.ers as Solver(sc) Let us nowconsider the case \nwhere theveri.cation tool cannot han\u00addle non-standard constraints, such as Eq. (4). This is the case \nfor typical iterative program veri.cation tools that compute increas\u00adingly better approximationstoinvariants.Weshowthat \ndespitethis lack of expressivity it is still possible to solve synthesis conditions as long as the tool \nsatis.es an additional requirement. The onlynon-implication constraint in the synthesis condition sc \nis WellFormCond(sc). In Section 3.4, we discussed how an it\u00aderative lower-bounded search can discover \nthe transitions {[]gi . si}i without asserting Eq. (5). There we had left the question of ensuring valid(si) \nunanswered. Consider now the case where a valid solution gi,si exists (i.e., si is not false or that \nvalid(si) holds)that satis.esthe constraintset.Asan instance,inExample6, we have a synthesis condition \nfor which a valid solution exists as shown by Eq. (7). Notice that this solution is strictly weaker than \nanother solution that assigns identical values to other unknowns but assignsfalse to anyof s2, s2 or \ns3. Infact, we can observe that if the tool only generates maximally weak solutions then be\u00adtween these \ntwo solutions (which are comparable as we saw), it will always pick the one in which it does not assign \nfalse to state\u00adment unknowns. Therefore, it will always generate si such that valid(si) holds unless \nno such si exists. Therefore, if the pro\u00adgram veri.cation tool satis.es the following requirement, then \nwe canomitEq.(5)fromthe synthesis conditionandstillsolveitusing the tool. REQUIREMENT 2. Solutions are \nmaximally weak. This requirement corresponds to the tool s ability to compute weakest preconditions. \nThe typical approach to weakest precon\u00additions (greatest .xed-point) computation propagates facts back\u00adwards,but \nthisis considered dif.cult and therefore not manytools exist that do this. However, althoughtraditional \niterative data .ow veri.ersfailtomeet Requirements1and2,theredoexistsomeiter\u00adative tools [32] that compute \nmaximally weak solutions and there\u00adfore satisfy the requirements. We have argued that maximally weak \nsolutions for statement unknowns si ensure valid(si),but this comes at the cost of de\u00adgraded performance \nbecause maximally weak solutions are gener\u00adated for guard and invariant unknowns too.We require maximally \nweak solutions only for the statement unknowns, while for syn\u00adthesis we are interested in any solution \nto the guard and invari\u00adant unknowns that satisfy the synthesis condition. In our trials, the constraint-based \nscheme [21, 32] (which computes any.xed-point in the lattice rather than the greatest .xed-point) outperformed \nthe iterative scheme [32]. Infact, our tool based on iterative approxi\u00admations does not terminate for \nmost benchmarks, and we therefore perform the experiments using our constraint-based tool. 5. Experimental \nCase Studies To evaluate our approach, we synthesized examples in three cate\u00adgories: First, easy to specifybut \ntrickyto program arithmetic pro\u00adgrams; second, sorting programs, which all have the same spec\u00adi.cation \nbut yield different sorting strategies depending on the resource constraints; third, dynamic programming \nprograms for which naive solutions yield exponential runtimes, but which can be computed in polytime \nby suitable memoization. 5.1 Implementation We augmented our constraint-based veri.cation tools from \nprior work to build more powerful veri.ers that we use as solvers for synthesis conditions. In this section, \nwe summarize the capabilities of these tools and our extensions to them. The description here is necessarily \nbrief due to lack of space, and more details can be foundin the companion technical report [35].We also \ndescribea technique we use to simplify user input by expanding .owgraphs to be more expressive as required \nsometimes. Veri.cationTools Our synthesis technique relies on an underly\u00ading programveri.cation tool.For \nthiswork, we used tools that are part of the VS3 project[33].Weusedtwotools:an arithmeticveri.\u00adcation \ntool [21], which we call VS3 here; and a predicate abstrac- LIA tion veri.cation tool [32, 34, 20], which \nwe call VS3 here. PA Capabilities Bothveri.cation tools VS3 and VS3 are based on LIA PA the idea of reducing \nthe problem of invariant generation to satis.a\u00adbility solving. Each tool takes as inputaCprogram (annotated \nwith assertions, typically the postcondition; and assumptions, typically the precondition) and hints \nabout the form of the invariants. The tool then generates invariants that suf.ce to prove the assertions. \nIf thetoolfailsto generatetheinvariants,theneitherthe assertionsin the programdonotholdornoinvariantsexistthatare \ninstantiations of the given template form. VS3 works over the theory of linear arithmetic and discovers \nLIA (quanti.er-free) invariants in DNF form with linear inequalities over programvariables as the atomicfacts.As \nhints,itexpects three integer parameters, maxdsj , maxcnj , maxbv. The param\u00adeters maxdsj and maxcnj \nlimit the maximum number of dis\u00adjuncts and conjuncts (in each disjunct) in the invariants. Ad\u00additionally, \nmaxbv is a integer pair (b1,b2) for the size in bits of invariant coef.cients b1 and intermediate computations \nb2. None of our synthesized benchmarks required disjunctive in\u00advariants(maxdsj =1), and we choose appropriate \nvalues for maxcnj and maxbv for different benchmarks.Forexample, sup\u00adpose x, y, z are the program variables \nand x = y . z = x +1 is the unknown program invariant. The user will specify some valuev1 for maxcnj \nand (v2,v3) for maxbv. VS3 will setup and LIA solve constraints for a conjunctive invariant with atoms \nof the form c0 + c1x + c2y + c3z = 0. It will search for solutions to each coef.cient ci assumed to have \nv2 bits and use v3 bits for anyintermediate values in the constraints. In this example, any v1 = 3 and \nany v2 = 2 (one bit for the sign) will work, and we can choose a large enough v3 to avoid over.ows. VS3 \nworks over a combination of the theories of equality PA with uninterpreted functions, arrays, and linear \narithmetic and discovers (possibly) quanti.ed invariants. VS3 expects the PA boolean structure of the \ninvariants, i.e., quanti.cation and dis\u00adjunctions,tobemadeexplicitasa template.As hints,itexpects a boolean \ntemplate T (with holes for conjunctivefacts) anda set of predicates P . VS3 infers the subset of predicates \nfrom PA P the atoms of the conjunct that populate the holes in T . For example, ifx = 0 ..k :0 = k = \nx . A[k] = A[k + 1] is the invariant, then VS3 would discover it when run with any PA T that is at least \n([-] ..k :[-] . [-]) and any P that is a superset of {x = 0,k = 0,x = k, A[k + 1] = A[k]}, where [-] \ndenotes a conjunctive hole. Extensions These tools are powerful and can infer expressive invariants such \nas those requiring quanti.cation and disjunction, butfor someofthe benchmarks,the reasoning requiredwasbeyond \neven their capabilities. We therefore extended the base veri.ers with the following features. Quadraticexpressions \nfor arithmetic For handling quadraticex\u00adpressions in the proofs, we implemented a sound but incom\u00adplete \ntechnique that renames quadraticexpressionsto freshvari\u00adables and then uses linear arithmetic reasoning, \nalready built into VS3 LIA . This suf.ces for most of our benchmarks, except when linear relations need \nto be lifted to quadratic relations, e.g., a = b = 0 . a 2 = b2 = 0. This happens in one isolated step \nin the integral square root binary search case, which we circumventbyexplicitly encoding an assumption.We \ncall this augmented solver VS3 QA . Axiomatization Proposals exist for extending veri.cation tools with \naxioms for theories theydo not natively support, e.g., the theory of reachability for lists [26].We take \nsuch axiomatiza\u00adtion a step further and allow the user to specify axioms over uninterpreted symbols that \nde.ne computations.We implement this in VS3 to specify the meaning of dynamic programming PA programs, \ne.g., the de.nition of Fibonacci. We call this aug\u00admented solver VS3 AX . . (a) specify an acyclic \n.owgraph template Rflow = . and a computa\u00ad (b) . Strassens(int aij ,bij ) { tion template Rcomp = \u00d8 \nthat imposes no constraints.To ensure that SelSort(int A[],n) { . v1:=(a11+a22)(b11+b22) no temporaries \nare used we specify Rstack = \u00d8. The synthesizer i1:=0; v2:=(a21+a22)b11 whilet1,.1 (i1 <n - 1) generates \nvarious versions of the program, e.g., one being v3:=a11(b12-b22) v1:=i1; v4:=a22(b21-b11) Swap(int x, \ny){x := x + y; y := x - y; x := x - y; } i2:=i1+1; v5:=(a11+a12)b22 whilet2,.2 (i2 <n) Strassen s 2 \n\u00d7 2 Matrix Multiplication Consider Strassen s ma\u00ad v6:=(a21-a11)(b11+b12) if (A[i2]<A[v1]) trix multiplication, \nwhich computes the product of two n \u00d7 n ma\u00adv7:=(a12-a22)(b21+b22) v1:=i2; c11:=v1+v4-v5+v7 trices in \nT(n 2.81) time instead of T(n 3).Thekeyto this algorithm i2++; c12:=v3+v5 is an acyclic fragment that \ncomputes the product of two 2 \u00d7 2 input swap(A[i1],A[v1]); c21:=v2+v4 matrices {aij ,bij }i,j=1,2 using7multiplications \ninsteadoftheex\u00ad i1++; c22:=v1+v3-v2+v6 pected 8. Used recursively, this results in asymptotic savings.We \nreturn A;return cij ; } do not attempt to synthesize the full matrix multiplication proce\u00ad } Ranking \nfunctions: dure because it contains no signi.cant insight. Instead, we synthe\u00ad (c) .1 : n - i1 - 2 \nsize the crucial acyclic fragment, which is shown in Figure 2(a). Fib(int n) { .2 : n - i2 - 1 Here \nthe precondition Fpre is true and the postcondition Fpost is v1:=0;v1:=1;i1:=0; Invariant t1: whilet,.(i1 \n= n) the conjunctionoffour equalitiesas(overthe outputs {cij }i,j=1,2): .k1,k2 :0 = k1 <k2 <n \u00ab \u00abv1:=v1+v2;swap(v1,v2); \n. k1 <i1 . A[k1] = A[k2] c11 c12 a11b11 + a12b21 a11b12 + a12b22 i1++; = Invariant t1: c21 c22 a21b11 \n+ a22b21 a21b12 + a22b22return v1; i1 <i2 . i1 = v1 <n } .k1,k2 :0 = k1 <k2 <n The synthesizer also \ngenerates many alternate versions that are Ranking function .: . k1 <i1 . A[k1] = A[k2] functionally \nequivalent to Figure 2(a). x - s .k : i1 = k<i2 . k = 0 As a side note, we also attempted synthesis \nusing 6 multipli- Invariant t: . A[v1] = A[k] cations, which failed. This suggests that possibly no \nasymptoti\u00adcallyfaster solutionexists using simple quadratic computations v1 = Fib(i1) . v2 = Fib(i1 +1) \n Figure 2. Illustrative examples from each of the domains. (a) 2.376 theoretical results up to n are \nknown, but use products that Arithmetic: Strassen s Matrix Multiplication (b) Sorting: Selec\u00adcannot be \neasily be captured in the simple domains considered here. tion Sort (c) Dynamic Programming: Fibonacci. \nFor simplifying the presentation, we omit degenerate conditional branches, i.e. Integral Square Root \nConsider computing the integral square v true/false guards, We name the loop iteration counters L = \nroot L xJ of a positive number x using only linear or quadratic . {i1,i2,..} and the temporary stack \nvariables T = {v1,v2,..}. operations. The precondition is Fpre = x = 1 and the postcon\u00ad . dition, involving \nthe output i, is Fpost =(i - 1)2 = x<i2 . . We provide a single loop .owgraph template Rflow = *(.) \nand Notethat theseextensions aretofacilitateveri.cationandnotsyn- . an empty computation template Rcomp \n= \u00d8. The synthesizer gen\u00adthesis. The synthesis solver is exactly the same as the veri.cation erates different \nprograms depending on the domain constraints and tool.Withouttheextensions mostof our benchmarks cannoteven \nthe stack template: beveri.ed,andthustheirveri.cationcanbeseenasan independent . contribution. Rstack \n= {(int, 0)} and we allow quadratic expressions in Dexp ,Dgrd . The synthesized program does a sequential \nsearch Flowgraphs with Init/Final Phases In practiceafair numberof downwards starting from i = x by \ncontinuously recomputing loops have characteristic initialization and .nalization phases that and checking \n(i - 1)2 against x. exhibit behavior different from the rest of the loop. In theory, ver\u00ad . i.ers should \nbe able to infer loop invariants that capture such se- Rstack = {(int, 1)} and we only allow linear expressions \nin Dexp ,Dgrd . The synthesized program does a sequential search mantically different phases. However, \nthis requires disjunctive rea\u00adbut usesthe additional localvariable rather surprisinglyto track soning, \nwhichisfairlyexpensiveifatall supportedbytheveri.er. thevalue of (i - 1)2 using only linear updates. \nThe synthesized Instead we use an alternate expansion Expand n(T ) that introduces program is Example \n1, from earlier. acyclic fragments for the initialization and .nalization if synthesis . without themfails.For \ninstance, for Example1, the the user only Rstack = {(int, 2)} and we allow quadratic expressions in \nneeds to specify the .owgraph *(.) instead of the more compli-Dexp ,Dgrd . The synthesized program does \na binary search for cated .;*(.);.. Except for the expansion of loops, Expand n(T ) thevalue of i and \nusesthetwo additional localvariablestohold expands all other statements exactly like Expandn(T ) does. \nFor the low and high end of the binary search space. To restrict loops,itbuilds an initialization and \n.nalization phase as follows. reasoning to linear arithmetic we model m = L(s1 + s2)/2J nas the assumption \ns1 = m = s2. Additionally, because of the Expand (*(T ))=Expandn(.); . Added initialization n incompleteness \nin the handling of quadratic expressions, our whilet (g) {Expand (T );} solver cannot derive (s2 + 1)2 \n= s12 from s2 +1 = s1. Thus Expandn(.); . Added .nalization we provide the quadratic inequality as another \nassumption. 5.2 Algorithms that use arithmetic Bresenham s Line Drawing Algorithm Consider Bresenham \ns line drawing algorithm, as we discussed in Section 1.1. For ef.- For this category, we pickDprf to \nbe quadratic arithmetic and use ciency,the algorithm only uses linear updates, which are non-trivial \nas our solver the VS3 QA tool.Wechoseasetof arithmetic benchmarks to verify [16] or even understand \n(let alone discover from scratch). with simple-to-state functional speci.cations but each containing \n. We specify the precondition Fpre =0 <Y = X. The post\u00ad some trickyinsight that human programmers may \nmiss. condition can be written as a quanti.ed assertion outside the loop Swapping withoutTemporaries \nConsider a program that swaps or as a quanti.er-free assertion inside the loop, as mentioned in two integer-valued \nvariables without usinga temporary.The pre-Section1.1.We chooseto annotatethe.owgraphwiththe simpler \n. condition and postcondition to the program are speci.ed as Fpost = quanti.er-free 2|y - (Y/X)x|= 1 \nat the loop header and addi\u00ad . (x = c2 .y = c1) and Fpre =(x = c1 .y = c2),respectively.We tionally \nspecify that the loop iteratesover x =0 ..X. This simpli\u00ad.cation in the implementation allowed us to \nuse VS3 , which only QA supports quanti.er-freefacts. . We specify a single loop .owgraph Rflow = *(.) \nand empty .. stack and computation templates Rstack = \u00d8, Rcomp = \u00d8. The synthesizer generates multiple \nversions, one of which is exactly as shown in Figure 1(b) and can be translated to the program in Figure \n1(a).  5.3 Sorting Algorithms For this category, we pick Dprf to be the theory supported by VS3 The \ncurrent version of our PA , which we use as our solver. toolworks witha user-supplied setof predicates.We \nareworking on predicateinference techniques in the style of CEGAR-based model checkers [23] but for now, \nwegive the toola candidate set of predicates. . The sortedness speci.cation consists of the precondition \nFpre = . true and the postcondition Fpost = .k :0 = k<n . A[k] = A[k + 1]. The full functional speci.cation \nwould also ensure that the output arrayisa permutationof the input,butverifying and thus, synthesizing \nthe full speci.cation is outside the capabilities of most tools today. We therefore usea mechanismto \nlimitthe spaceof programsto desirable sorting algorithms, while still only using Fpost .We limit Dexp \nto those that only involveoperations that maintain elements forexample, swapping elements or moving elements \nto unoccupied locations. Using this mechanism, we ensure that invalid algorithms (that replicate or lose \narray elements) are not considered. Non-recursive sorting algorithms Consider comparison-based sorting \nprograms that are composedof nested loops.We specifya . .owgraph template Rflow = *(*(.)) and a computation \ntemplate Rcomp that limits the operations to swapping of array values. . Rstack = \u00d8: The synthesizer \nproduces two sorting programs that are valid with respect to the scaffold. One corresponds to BubbleSortandthe \notherisa non-standardversionof Insertion Sort. The standard version of Insertion Sort uses a temporary \nvariable to hold the inserted object. Since we do not provide a temporaryvariable, the synthesized program \nmoves the inserted elementbyswappingitwithitsneighbor,whilestill performing operations similar to Insertion \nSort. . Rstack = {(int, 1)}:The synthesizer produces another sorting programthatusesthe temporaryvariabletoholdanarrayindex. \nThis program corresponds to Selection Sort and is shown in Fig\u00adure 2(b). Notice the non-trivial invariants \nand ranking functions that are synthesized alongside for each of the loops. Recursive divide-and-conquer \nsorting Consider comparison\u00adbased sorting programs that use recursion.Wemakeafew simple modi.cationstothe \nsystemto specify recursiveprograms. First,we introducea terminal string \u00ae to the .owgraph template language \n(Eq. 2), representing a recursive call. Let (Fpre (vvin ),Fpost (vvout )) denote the functional speci.cation. \nThen we augment the expan\u00adsion (Section 3.2) to handle the new .owgraph string as follows: Expandn(\u00ae)= \nchoose{[]true . srecur } ' '' where srecur = sargs .(Fpre (vvin ) . Fpost (vvout )).sret setsval\u00adues \nto the arguments of the recursive call (using sargs ),assumes the effect of the recursive call (using \nFpre (vvin ' ) . Fpost (vvout '' ), with the input arguments renamed to vvin ' and the return variables \nre\u00adnamed to vvout '' )and lastly,outputs the returnedvalues into program variables (using sret ). The \nstatements sargs ,sret take the form: V ' sargs = xi = ei where xi . vvin ,ei . Dexp |Vars Vi sret = \ni xi = ei where xi . Vars,ei . Dexp |v.out \"\" Here Vars denote the variables of the procedure (the input, \noutput and local stackvariables).We also tweak the statement concretiza\u00ad tion function (Section4)to \noutputa recursive call statement rec: ' '''' ' Stmt(Fpre (vvin ) .Fpost (vvout )) = vvout := rec(vvin \n) We specify a computation template that allows only swapping or movingof elements.We thentrydifferentvaluesofthe.owgraph \nand stack templates: . Rflow = \u00ae;\u00ae;. (two recursive calls followed by an acyclic . fragment) and Rstack \n= \u00d8:The synthesizer producesa program that recursively sorts subparts and then combines the results. \nThis corresponds to Merge Sort. . Rflow = .;\u00ae;\u00ae (anacyclic fragment followedbytworecursive . calls) \nand Rstack = {(int, 1)}: The synthesizer produces a program that partitions the elements and then recursively \nsorts the subparts. This corresponds to Quick Sort. 5.4 Dynamic Programming Algorithms For this category, \nwe pickDprf tobe the theory supportedby VS3 AX , which we use as our solver. As in the previous section, \nsince our tool does not currently infer predicates, we give it a candidate set. We choose all the textbook \ndynamic programming examples [8] and attempt to synthesize them from their functional speci.cations. \nThe .rst hurdle (even for veri.cation) for these algorithms is that the meaning of the computation is \nnot easily speci.ed. To address this issue, we need support for axioms, which are typically recursive \nde.nitions. De.nitional Axioms The veri.cation tool allows the user to de\u00ad.nethemeaningofacomputationasan \nuninterpretedsymbol,with (recursive)quanti.edfacts de.ningthe semanticsofthe symbol ax\u00adiomatically.Forexample, \nthe semanticsof Fibonacci are de.nedin terms of the symbol Fib and the three axioms: Fib(0) = 0 . Fib(1) \n= 1 .k : k = 0 . Fib(k +2) = Fib(k + 1) + Fib(k)  The tool passes thegiven symbol and its de.nitional \naxiomsto the underlying theorem prover (Z3 [10]), which assumes the axioms beforeevery theoremproving \nquery.Withthis interpretationofthe symbol Fib knowntothe theoremprover,theveri.ercannowpose theorem proving \nqueries involving the symbol Fib.For instance, the iterative program for Fibonacci maintains an invariant \nof the form x = Fib(i), which the veri.er can now infer. This allows the tool to verify dynamic programming \nprograms that are typically iterative, relating them to the recursive de.nitional axioms. Even with veri.cation \nin place, automatic synthesis of these programs involves three non-trivial tasks for the synthesizer. \nFirst, the synthesizer needs to automatically discover a strategy for translating the recursion (in the \nfunctional speci.cation) to non\u00adrecursive iteration (for the actual computation). The functional speci.cations \ndo not contain this information, e.g., in the speci.ca\u00adtion for Fibonacci above, the iteration strategy \nfor the computation is not evident. Second, the synthesizer needs to take the (non\u00addirectional) equalities \nin the speci.cations and impose directional\u00adity suchthat elementsarecomputedintherightorder.Forexample, \nfor Fibonacci the synthesizer needs to automatically discover that Fib(k) and Fib(k + 1) should be computed \nbefore Fib(k + 2). Third, the synthesizer needs to discover an ef.cient memoization strategy for only \nthose results needed for future computations, to .t the computationin the space provided which is one \nof the bene\u00ad.ts of dynamic programming algorithms. For example, Fibonacci can be computed using only \ntwo additional memory locations by suitable memoization.Fortunately, justby specifying the resource constraints \nand using our proof-theoretic approach the synthesizer is able to perform these tasks and synthesize \ndynamic programming algorithms from their recursive functional speci.cations. Also, as in the case of \nsorting, we want to disallow completely arbitrary computations. In sorting, we could uniformly restrict \nthe expression language to only swap and move operations. For dy\u00adnamic programming, the speci.cation \nof the operations is problem\u00adspeci.c.For instance,for shortestpath,weonlywanttoallowthe path matrix updates \nthat correspond to valid paths, e.g., disallow arbitrary multiplication of path weights. Rcomp speci.es \nthese con\u00adstraints by only permitting updates through certain predicates. Dynamic programming solutions \ntypically havean initialization phase (init-loop) and then a phase (work-loop) that .lls the appro\u00adpriate \nentriesin the table. Therefore, we chosea Rflow with an init\u00adloop(*(.))followed by a work-loop. . By \nspecifying a .owgraph template Rflow = *(.);*(.) and a stack template with no additional variables (except \nfor the case of . Fibonacci, where the synthesizer required Rstack = {(int, 2)}), we were able to synthesize \nthe following four examples: Fibonacci Consider computing the nth Fibonacci number from the functional \nspeci.cation as above. Our synthesizer generates a program that memoizes the solutions to the two subproblems \nFib(i1) and Fib(i1 + 1) in the i1th iteration.It maintainsa sliding window for the two subproblems and \nstores their solutions in the twoadditional stackvariables. The synthesizedprogram along with its invariant \nand ranking function is shown in Figure 2(c). Checkerboard Consider computing the least-cost path in \na rect\u00adangular grid (with costs at each grid location), from the bottom row to the top row. The functional \nspeci.cation states the path cost for a grid location in terms of the path costs for possible previous \nlocations (i.e., below left, below or below right). Our synthesizer generates a program that .nds the \nminimum cost paths. Longest Common Subsequence (LCS) Consider computing the longest common substring \nthat appears in the same order in two given input strings (as arrays of characters). The recursive func\u00adtional \nspeci.cation relates the cost of a substring against the cost of substrings with one fewer character. \nOur synthesizer generates a program for LCS. Single Source ShortestPath Consider computing the least-cost \npath from a designated source to all other nodes where the weight of edges is given given as a cost function \nfor each source and destination pair. The recursive functional speci.cation states the cost structure \nfor all nodes in terms of the cost structure of all nodes if one fewer hop is allowed. Our synthesizer \ngenerates a program for the single source shortest path problem. Forthe following twoexamples, synthesisfailed \nwith the sim\u00adpler work-loop, but we synthesize the examples by specifying a .owgraph template *(.);*(*(.)) \nand no additional stackvariables: All-pairs Shortest Path Consider computing all-pairs shortest paths \nusing a recursive functional speci.cation similar to the one we used for single source shortest path. \nOur synthesizer times out for thisexample.We therefore attempt synthesisby(i) specifying the acyclic \nfragments and synthesizing the guards, and (ii) spec\u00adifying the guards and synthesizing the acyclic fragments. \nIn each case, our synthesizer generates the other component, corresponding to Floyd-Warshall s algorithm. \nMatrix Chain Multiply Consider computing the optimal way to multiply a matrix chain. Depending on the \nbracketing, the total number of multiplications varies.We wish to .nd the bracketing that minimizes the \nnumber of multiplications. E.g., if we use the simple n 3 multiplication for twomatrices, then A10\u00d7100B100\u00d71C1\u00d750 \ncan either takes 1,500 multiplications for (AB)C or 55,000 mul\u00adtiplications for A(BC). The functional \nspeci.cation de.nes the cost of multiplying a particular chain of matrices in terms of the cost of a \nchain with one fewer element. Our synthesizer generates a program that computes the optimal matrix bracketing. \nBenchmark maxcnj maxbv Assumes Swap two 0 2, 6 0 Strassen s 0 2, 6 0 Sqrt (linear search) 4 2, 6 0 Sqrt \n(binary search) 3 2, 8 2 Bresenham s 6 2, 5 0  Table 1. Parameters used for synthesis usingVS3 QA .For \neach bench\u00admark, we list the maximum number of conjuncts in anyinvariant (maxcnj )and bit vector sizes(maxbv, \nfor constants and for bit\u00adblasting), respectively. The last column lists any assumes we man\u00adually speci.ed. \nBenchmark Number of Defn. Templates, Annot. or Axioms Predicates Assumes Bubble Sort 0 3, 17 1 Insertion \nSort 0 3, 16 1 Selection Sort 0 3, 20 1 Merge Sort 0 4, 16 3 Quick Sort 0 3, 15 0 Fibonacci 3 1, 12 0 \nCheckerboard 5 2, 8 0 Longest Common Subseq. 6 2, 13 0 Matrix Chain Multiply 7 2, 18 0 Single-Src Shortest \nPath 3 3, 16 0 All-pairs Shortest Path 10 4, 19 0 Table 2. Parameters used for synthesis usingVS3 AX \n.For each bench\u00admark, we list the number of de.nitional axioms required to specify the meaning of the \ncomputation. Additionally, we list the number of invariant templates and size of predicate set given \nto VS3 AX . The last column lists anyannotations or assumes we manually speci.ed. 5.5 Performance Parameters \nfor VS3 and VS3 Table 1 lists the parameters re- QA AX quired to run VS3 over our arithmetic benchmarks. \nThe second QA column lists the maximum number of conjuncts(maxcnj )expected ineachinvariant.The third \ncolumn liststhebitvectorsizes(maxbv) usedforinvariantcoef.cientsand intermediatevalues.Thelastcol\u00adumn \nlists the assumptions we manually provided. We guessed reasonablevalues formaxcnj for the programs with \nloops.We started witha smallvalue and iteratively increaseditif synthesis failed. For the bit-vector \nsizes, we choose two bits for the coef.cients since we were not expecting large coef.cients in theinvariants.We \nthen chose reasonable values (5 8) for the bit\u00advectorsizefor intermediate computations.Therewasonlyonecase, \nsquare root using binary search, where we had to specify a manual assertion, as discussed earlier. In \nall, little user effort was required. Table2lists the parameters required to runVS3 over our sort- AX \n ing and dynamic programming benchmarks. The second column lists the number of de.nitional axioms required \nfor specifying the meaning of the computation and are required even for veri.cation. The third column \nlists the number of templates and predicates used. The last column lists the number of annotations or \nassumptions that were manually provided. The templates contain conjunctive holes andexplicit quanti.ca\u00adtion \nand disjunction.For each benchmark, there is one quanti.er\u00adfree, disjunction-less, template and the remaining \nare universally quanti.ed with a form almost identical to the postcondition, and therefore easy to write \nout. All invariants are conjunctions of the templates with their holes instantiated with a subset (conjunction) \nof the predicate set. The predicates are atomic relations between linear and array expression over program \nvariables and constants. We specifya suitable predicate set for each benchmark.We start withacandidate \nset and then iteratively added predicateswhen syn\u00adTable 3. (a) Arithmetic(b) Sorting(c) Dynamic Programming.For \neach category, we indicate the tool used both for veri.cation and synthesis.For each benchmark, we indicate \nthe time in seconds to solve the veri.cation conditions and the synthesis conditions, and the slowdown \nfor synthesis compared to veri.cation. Benchmark Verif. Synthesis Ratio Arith. (VS 3QA ) Swap two Strassen \ns Sqrt (linear search) Sqrt (binary search) Bresenham s 0.11 0.11 0.84 0.63 166.54 0.12 4.98 9.96 1.83 \n9658.52 1.09 45.27 11.86 2.90 58.00 Sorting (VS 3PA ) Bubble Sort Insertion Sort Selection Sort Merge \nSort Quick Sort 1.27 2.49 23.77 18.86 1.74 3.19 5.41 164.57 50.00 160.57 2.51 2.17 6.92 2.65 92.28 Dynamic \nProg. (VS 3AX ) Fibonacci Checkerboard Longest Common Subseq. Matrix Chain Multiply Single-Src Shortest \nPath All-pairs Shortest Path1 0.37 0.39 0.53 6.85 46.58 112.28 5.90 0.96 14.23 88.35 124.01 (i) 226.71 \n(ii) 750.11 15.95 2.46 26.85 12.90 2.66 (i) 2.02 (ii) 6.68 thesisfailed.We needed at most 20 predicates, \nwhich were easily found for each benchmark. In the interest of space, we omit the exact templates and \npredicates used for each benchmark,but they can be found in the companion technical report [35].For each \nof the sorting programs with a nested loop structure (Bubble, Inser\u00adtionand Selection Sort),the outerloopinvariantisredundantgiven \nthe inner loop invariant. Therefore, we tag the outer loop the one annotation each case and the solver \ndoes not to attempt to gener\u00adate its invariant, improving performance.For Merge Sort, synthe\u00adsisfails \nunless we reduce the search spaceby manually specifying three simple quanti.er-free atomicfactsoftheinvariant.Inall, \nlittle user effort was required. Runtimes Table3presentsthe performanceofa constraint-based synthesizer \nover arithmetic, sorting and dynamic programming benchmarks using the parameters from Tables 1 and 2. \nAll run\u00adtimes are median of three runs, measured in seconds.We measure the time for veri.cation and the \ntime for synthesis using the same tool. The total synthesis time varies between 0.12 9658.52 sec\u00adonds, \ndepending on the dif.culty of the benchmark, with a median runtimeof 14.23 seconds.Thefactorslowdownfor \nsynthesisvaries between 1.09 92.28 with a median of 6.68. The benchmarks we used are considered dif.cult \neven for veri\u00ad.cation. Consequentlythelowaverage runtimesfor proof-theoretic synthesis are encouraging. \nAlso, the slowdown for synthesis com\u00adpared to veri.cation is acceptable, and shows that we can indeed \nexploit the advances in veri.cation to our advantage for synthesis.  5.6 Limitations and FutureWork \nOur synthesis system borrowsthe limitationsofthe underlyingver\u00adi.ers. Speci.cally, we added assumptions \nfor two cases, binary search square root and merge sort, to compensate for the incom\u00adplete handling of \nquadratic expressions by VS3 and inef.ciency QA of VS3 AX , respectively. Similarly, we have to specify \na set of can\u00addidate predicates for VS3 AX an overhead that can be alleviated us\u00ading predicate inference \ntechniques.For ourexperiments, this meant that at times, our guesses were not suf.cient, and synthesis \nonly succeeded aftera .rstfewfailed attempts which were usedto iter\u00adatively re.ne the set of predicates, \nakin to a manual run of CE\u00ad 1These timings are for separately (i) synthesizing the loop guards, and for \n(ii) synthesizing the acyclic fragments. GAR [4]. Aside from these avoidable incompleteness issues of \nveri.ers, there are two major concerns for any synthesis system, namely scalability and relevance. Scalability \nThe synthesis conditions we generate are tractable for current solvers,even though these benchmarks are \nconsidered some of the most dif.cult even to verify. Yet, the synthesis conditions are not trivial, as \nillustrated by the need to annotate three of the sorting benchmarks to omit invariants for their outer \nloops.With the current systemweexpecttobeableto synthesize programsthat havemorelinesofcode,butforwhichthe \nreasoninginvolvedisnot as complicated as in our benchmarks. But to synthesize programs that are larger \nand involve more complicated reasoning, more ef.cient veri.ers are needed. Relevance Any solution to \nthe synthesis conditions is a valid terminating program that satis.es the scaffold speci.cation. But \nthere may be multiple valid ones that may differ, for instance, in cache behavior, runtime performance, \nor readability. Currently, we do not prioritize the synthesized programs in any manner,but for completeness \nwe let the solver enumerate solutions.To get another solution, we assert the negation of a solution generated \nin a step and iteratively ask for the next solution. Notice though that the synthesis times reported \nin Table 3 are for generating the .rst of those solutions. We envision that in the future, we can either \naugment synthesis conditions with constraints about relevance or use a postprocessing step to prioritize \nand pick relevant solutions from those enumerated. 6. RelatedWork Proof-theoretic program synthesis Dijkstra \n[14], Gries [18] and Wirth [38] advocated that programmers write program that are correct-by-construction \nby manually developing the proof of cor\u00adrectness alongside the program. Because techniques for ef.cient \ninvariant inference were unavailable in the past, synthesis was con\u00adsidered intractable [12]. Recently, \nscheme-guided synthesis [17] has been proposed but specialized to the arithmetic domain [5]. Categorizations \nof approaches as constructive/deductive synthesis, schema-guided synthesis and inductive synthesis are \npresented in a recent survey [1]. Our approach can be seen as midway be\u00adtween constructive/deductive \nsynthesis and schema-guided synthe\u00adsis. Some researchers proposed heuristic techniques for automa\u00adtion, \nbut they cater to a very limited schematic of programs are limited in their applicability [15]. In this \npaper, we have shown that veri.cation has reached a point where automatic synthesis is feasi\u00adble. Extracting \nprogram from proofs The semantics of program loops is related to mathematical induction. There, an inductive \nproof of the theorem induced by a program speci.cation can be used to extract a program [27]. Using signi.cant \nhuman input, the\u00adorems proved interactively in the Coq have a computational analog that can be extracted \n[2]. The dif.culty is that the theorem is of the whole program, and proves that an output exists for \nthe spec\u00adi.cation. Such a theorem is much more dif.cult than the simple theorem proving queries generated \nby the veri.cation tool. Sketching Instead of a declarative speci.cation of the desired computation as \nwe use, combinatorial sketching [29, 30, 31] uses an unoptimized program as the speci.cation. A model \nchecker eliminates invalid candidate programs that the synthesizer gener\u00adates. Loops are handledina novelbut \nincomplete manner,by un\u00adrolling, or by using a prede.ned skeleton, or by using domain spe\u00adci.c loop .nitization \ntricks [29] that are not applicable when syn\u00adthesizing true unbounded loops (which our approach synthesizes \nnaturally using safety and liveness constraints). Sketching does not inherently generate the proof, although \npostprocessing steps can ensure correctness [30], while our approach produces the program, and the proof. \n Model checking-based synthesis of automata Seminal work on model checking [3] proposed synthesizing \nsynchronization skeletons a problem that has recently seen renewed interest [36, 37]. Synthesis from \nLTL speci.cation has also been consid\u00adered [28]. For the case of reactive systems, proposals exist that \nreduce the synthesis problem toagame betweenthe environment and the synthesizer where the winning strategy \ncorresponds to the synthesized program. Recently, this approach has also been ap\u00adplied to program repair \n[25, 19], which can be seen as restricted program synthesis. Despite optimizations [24], the practicality \nof these approaches for complete program synthesis remains unclear. 7. Conclusions We have presented \na principled approach to synthesis that treats synthesis as a generalized veri.cation problem. The novelty \nof our approach lies in generating synthesis conditions, which are composed of safety conditions, well-formedness \nconditions, and progress conditions, such that a satisfying solution to the syn\u00adthesis conditions correspondstoa \nsynthesized program.Wehave been able to use veri.cation tools to synthesize programs, and, si\u00admultaneously,their \nproof(invariants, ranking functions).Wehave demonstrated the viability of our approach by synthesizing \ndif.\u00adcult examples in the three domains of arithmetic, sorting, and dy\u00adnamic programming, all in very \nreasonable time. We believe the reason for the practicality of our approach is the interplay between \nthe proof (invariants) and the statements. Speci.cally, by setting up constraints with both statement \nand proof unknowns together, statements that do not have a corresponding proof are ef.ciently eliminated.We \nbelieve this is the .rst proposal that leverages this insight for ef.cient and automatic program synthesis. \nAcknowledgments This work was supported in parts by the grants CCF-0430118 and CCF-0346982.Wewouldalsoliketo \nthankAvik Chaudhuri,El\u00adnatan Reisner, and the other anonymous reviewers, for their invalu\u00adable feedback. \nReferences [1] D. Basin, Y. DeVille, P. Flener, A. Hamfelt, and J. F. NIlsson. Synthesis of programs \nin computational logic. In LNCS 3049. [2] Yves Bertot and Pierre Casteran. Interactive Theorem Proving \nand Program Development. SpringerVerlag, 2004. [3] Edmund M. Clarke and E. Allen Emerson. Design and \nsynthesis of synchronization skeletons using branching-time temporal logic. In Logic of Programs, pages \n52 71. Springer-Verlag, 1982. [4] Edmund M. Clarke, Orna Grumberg, Somesh Jha, Yuan Lu, and HelmutVeith. \nCounterexample-guided abstraction re.nement. In CAV 00, pages 154 169, 2000. [5] Michael Col\u00b4on. Schema-guided \nsynthesisof imperative programsby constraint solving. In LOPSTR, pages 166 181, 2004. [6] Michael Col\u00b4on, \nSriram Sankaranarayanan, and HennySipma. Linear invariant generation using non-linear constraint solving. \nIn CAV 03. [7] Byron Cook, Andreas Podelski, and AndreyRybalchenko. Termina\u00adtion proofs for systems code. \nIn PLDI 06, pages 415 426, 2006. [8] T. Cormen, C. Leiserson, and R. Rivest. Introduction to Algorithms. \n[9] P. Cousot and R. Cousot. Abstract interpretation a uni.ed lattice model for static analysis of programs \nby construction or approxima\u00adtion of .xpoints. In POPL 77. [10] Leonardo de Moura and Nikolaj Bj\u00f8rner. \nZ3, 2008. http:// research.microsoft.com/projects/Z3/. [11] EdsgerW. Dijkstra. Guarded commands, nondeterminacyand \nformal derivation of programs. Communicationsof theACM,18(8):453 457. [12] Edsger W. Dijkstra. A constructive \napproach to the problem of program correctness. BIT Numerical Math., 8(3):174 186, 1968. [13] EdsgerW. \nDijkstra and Carel S. Scholten. Predicate Calculus and Program Semantics. Texts and Monographs in CS. \n1990. [14] EdsgerWybe Dijkstra. ADiscipline of Programming. 1976. [15] JoeW. Duran. Heuristicsfor program \nsynthesisusingloopinvariants. In ACM 78, pages 891 900,NewYork,NY, USA.ACM. [16] Jean-Christophe Filli \nUsing SMT solvers for deductive atre. veri.cationofCandJava programs. In SMT 08.  [17] Pierre Flener,Kung-Kiu \nLau, Mario Ornaghi, and Julian Richardson. An abstract formalization of correct schemas for program synthesis. \nJ. Symb. Comput., 30(1):93 127, 2000. [18] David Gries. The Science of Programming. 1987. [19] Andreas \nGriesmayer,Paul Bloem Roderick, and Byron Cook. Repair of boolean programs with an application to C. \nIn CAV 06. [20] Sumit Gulwani, Saurabh Srivastava, and RamarathnamVenkatesan. Constraint-based invariant \ninference over predicate abstraction. In VMCAI 09. [21] Sumit Gulwani, Saurabh Srivastava, and RamarathnamVenkatesan. \nProgram analysis as constraint solving. In PLDI 08, pages 281 292. [22] Matthew S. Hecht and JeffreyD. \nUllman. Flow graph reducibility. In STOC 72, pages 238 250,NewYork,NY, USA, 1972.ACM. [23] ThomasA. Henzinger,RanjitJhala,Rupak \nMajumdar,andKennethL. McMillan. Abstractions from proofs. In POPL 04, 2004. [24] Barbara Jobstmann and \nRoderick Bloem. Optimizations forLTL synthesis. In FMCAD 06, pages 117 124. IEEE Computer Society. [25] \nBarbara Jobstmann, Andreas Griesmayer, and RoderickPaul Bloem. Program repair asagame. In CAV 05, pages \n226 238. [26] Shuvendu Lahiri and Shaz Qadeer. Back to the future: revisiting precise programveri.cation \nusing SMT solvers. In POPL 08. [27] Zohar Mannaand RichardJ.Waldinger.Toward automatic program synthesis. \nCommunicationsof theACM, 14(3):151 165, 1971. [28] A. Pnueli and R. Rosner. On the synthesis of a reactive \nmodule. In POPL 89, pages 179 190,NewYork,NY, USA.ACM. [29] Armando Solar-Lezama, Gilad Arnold, Liviu \nTancau, Rastislav Bodik, Vijay Saraswat, and Sanjit Seshia. Sketching stencils. In PLDI 07, pages 167 \n178,NewYork,NY, USA.ACM. [30] Armando Solar-Lezama, Christopher Grant Jones, and Rastislav Bodik. Sketching \nconcurrent data structures. In PLDI 08. [31] Armando Solar-Lezama, Rodric Rabbah, Rastislav Bod\u00b4ik, andKemal \nEbcio.glu. Programming by sketching for bit-streaming programs. In PLDI 05. [32] Saurabh Srivastava and \nSumit Gulwani. Programveri.cation using templates over predicate abstraction. In PLDI 09. [33] Saurabh \nSrivastava, Sumit Gulwani, and Jeffrey S. Foster. VS3 . http://www.cs.umd.edu/~saurabhs/pacs/. [34] Saurabh \nSrivastava, Sumit Gulwani, and JeffreyS.Foster. VS3: SMT solvers for programveri.cation. In CAV 09. [35] \nSaurabh Srivastava, Sumit Gulwani, and JeffreyS.Foster. Proof\u00adtheoretic program synthesis: From programveri.cation \nto program synthesis.Technical report, Microsoft Research, Redmond, 2009. [36] MartinVechev, EranYahav, \nand GretaYorsh. Inferring synchroniza\u00adtion under limited observability. In TACAS 09, 2009. [37] MartinVechev, \nEranYahav, and GretaYorsh. Abstraction-guided synthesis. In POPL 10, 2010. [38] NicholasWirth. Systematic \nProgramming: An Introduction. 1973.    \n\t\t\t", "proc_id": "1706299", "abstract": "<p>This paper describes a novel technique for the synthesis of imperative programs. Automated program synthesis has the potential to make programming and the design of systems easier by allowing programs to be specified at a higher-level than executable code. In our approach, which we call proof-theoretic synthesis, the user provides an input-output functional specification, a description of the atomic operations in the programming language, and a specification of the synthesized program's looping structure, allowed stack space, and bound on usage of certain operations. Our technique synthesizes a program, if there exists one, that meets the input-output specification and uses only the given resources.</p> <p>The insight behind our approach is to interpret program synthesis as generalized program verification, which allows us to bring verification tools and techniques to program synthesis. Our synthesis algorithm works by creating a program with unknown statements, guards, inductive invariants, and ranking functions. It then generates constraints that relate the unknowns and enforces three kinds of requirements: partial correctness, loop termination, and well-formedness conditions on program guards. We formalize the requirements that program verification tools must meet to solve these constraint and use tools from prior work as our synthesizers.</p> <p>We demonstrate the feasibility of the proposed approach by synthesizing programs in three different domains: arithmetic, sorting, and dynamic programming. Using verification tools that we previously built in the VS3 project we are able to synthesize programs for complicated arithmetic algorithms including Strassen's matrix multiplication and Bresenham's line drawing; several sorting algorithms; and several dynamic programming algorithms. For these programs, the median time for synthesis is 14 seconds, and the ratio of synthesis to verification time ranges between 1x to 92x (with an median of 7x), illustrating the potential of the approach.</p>", "authors": [{"name": "Saurabh Srivastava", "author_profile_id": "81100062128", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1911103", "email_address": "", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1911104", "email_address": "", "orcid_id": ""}, {"name": "Jeffrey S. Foster", "author_profile_id": "81338488852", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1911105", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706337", "year": "2010", "article_id": "1706337", "conference": "POPL", "title": "From program verification to program synthesis", "url": "http://dl.acm.org/citation.cfm?id=1706337"}