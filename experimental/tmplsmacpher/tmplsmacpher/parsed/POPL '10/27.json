{"article_publication_date": "01-17-2010", "fulltext": "\n Abstraction-Guided Synthesis of Synchronization Martin Vechev Eran Yahav Greta Yorsh IBM Research IBM \nResearch IBM Research Abstract We present a novel framework for automatic inference of ef.cient synchronization \nin concurrent programs, a task known to be dif.\u00adcult and error-prone when done manually. Our framework \nis based on abstract interpretation and can infer synchronization for in.nite state programs. Given a \nprogram, a speci.cation, and an abstraction, we infer synchronization that avoids all (abstract) interleavings \nthat may violate the speci.cation, but permits as many valid interleavings as possible. Combined with \nabstraction re.nement, our framework can be viewed as a new approach for veri.cation where both the program \nand the abstraction can be modi.ed on-the-.y during the veri.\u00adcation process. The ability to modify the \nprogram, and not only the abstraction, allows us to remove program interleavings not only when they are \nknown to be invalid, but also when they cannot be veri.ed using the given abstraction. We implemented \na prototype of our approach using numerical abstractions and applied it to verify several interesting \nprograms. Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]; D.2.4 [Program Veri.cation] \nGeneral Terms Algorithms, Veri.cation Keywords concurrency, synthesis, abstract interpretation 1. Introduction \nWe present abstraction-guided synthesis, a novel approach for syn\u00adthesizing ef.cient synchronization \nin concurrent programs. Our ap\u00adproach turns the one dimensional problem of veri.cation under abstraction, \nin which only the abstraction can be modi.ed (typi\u00adcally via abstraction re.nement), into a two-dimensional \nproblem, in which both the program and the abstraction can be modi.ed un\u00adtil the abstraction is precise \nenough to verify the program. Based on abstract interpretation [10], our technique synthe\u00adsizes a symbolic \ncharacterization of safe schedules for concurrent in.nite-state programs. Safe schedules can be realized \nby modify\u00ading the program or the scheduler: Concurrent programming: by automatically inferring minimal \natomic sections that prevent unsafe schedules, we assist the pro\u00adgrammer in building correct and ef.cient \nconcurrent software, a task known to be dif.cult and error-prone.  Benevolent runtime: a scheduler that \nalways keeps the program execution on a safe schedule makes the runtime system more reliable and adaptive \nto ever-changing environment and safety requirements, without the need to modify the program.  Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation Given a program P , a speci.cation S, and an abstraction func\u00adtion \na, veri.cation determines whether P |=a S, that is, whether P satis.es the speci.cation S under the abstraction \na. When the answer to this question is negative, it may be the case that the pro\u00adgram violates the speci.cation, \nor that the abstraction a is not pre\u00adcise enough to show that the program satis.es it. When P|=a S, abstraction \nre.nement approaches (e.g., [3, 8]) share the common goal of trying to .nd a .ner abstraction aI such \nthat P |= a! S. In this paper, we investigate a complementary approach, of .nding a program P I such \nthat P I |=a S under the original abstraction a and P I admits a subset of the behaviors of P . Furthermore, \nwe combine the two directions re.ning the abstraction, and restricting program behaviors, to yield a \nnovel abstraction-guided synthesis algorithm. One of the main challenges in our approach is to devise \nan al\u00adgorithm for obtaining such P I from the initial program P .In this paper, we focus on concurrent \nprograms, and consider changes to P that correspond to restricting interleavings by adding synchro\u00adnization. \nAlthough it is possible to apply our techniques to other settings, concurrent programs are a natural \n.t. Concurrent programs are of\u00adten correct on most interleavings and only miss synchronization in a few \ncorner cases, which can be then avoided by synthesizing ad\u00additional synchronization. Furthermore, in \nmany cases, constraining the permitted interleavings reduces the set of reachable (abstract) states, \npossibly enabling veri.cation via a coarser abstraction and avoiding state-space explosion. The AGS algorithm, \npresented in Section 4, iteratively elimi\u00adnates invalid interleavings until the abstraction is precise \nenough to verify the program. Some of the (abstract) invalid interleavings it observes may correspond \nto concrete invalid interleavings, while others may be artifacts of the abstraction. Whenever the algorithm \nobserves an (abstract) invalid interleaving, the algorithm tries to eliminate it by either (i) modifying \nthe program, or (ii) re.ning the abstraction. To re.ne the abstraction, the algorithm can use any stan\u00addard \ntechnique (e.g.,[3, 8]). These include moving through a pre\u00addetermined series of domains with increasing \nprecision (and typi\u00adcally increasing cost), or re.ning within the same abstract domain by changing its \nparameters (e.g., [4]). To modify the program, we provide a novel algorithm that gen\u00aderates and solves \natomicity constraints. Atomicity constraints de\u00ad.ne which statements have to be executed atomically, \nwithout an intermediate context switch, to eliminate the invalid interleavings. This corresponds to limiting \nthe non-deterministic choices avail\u00adable to the scheduler. A solution of the atomicity constraints can \nbe implemented by adding atomic sections to the program. Our approach separates the process of identifying \nthe space of solutions (generating the atomicity constraints) from the process of on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute choosing between the possible \nsolutions, which can be based on to lists, requires prior speci.c permission and/or a fee. a quantitative \ncriterion. As we discuss in Section 6, our approach POPL 10, January 17 23, 2009, Madrid, Spain. provides \na solution to a quantitative synthesis problem [5], as it cCopyright &#38;#169; 2009 ACM 978-1-60558-479-9/10/01. \n. . $10.00  T1 { T2 { T3 { f(x) {if (x==1) return 3; 1: x+=z 1:z++ 1:y1=f(x) 2: x+=z 2:z++ 2:y2=x \nelse if (x==2) }} 3: assert return 6; (y1 y2) = else return 5; } } Figure 1. Simple example computing \nvalues of y1 and y2. can compute a minimally atomic safe schedule for a program, a schedule that poses \nminimal atomicity constraints on interleavings, and does not restrict interleavings unnecessarily. Furthermore, \nour approach can be instantiated with different methods for: (i) modifying the program to eliminate invalid \ninter\u00adleavings (ii) re.ning the abstraction (iii) choosing optimal solutions (quantitative criterion) \n(iv) implementing the resulting solution. The problem we address in this paper is closely related to \nthe ones addressed by program repair [12, 14] and controller synthe\u00adsis [20]. However, in contrast to \nthese, our approach focuses on concurrent programs, uses abstract interpretation, and is able to handle \nin.nite-state programs. 1.1 Main Contributions The contributions of this paper can be summarized as \nfollows: We provide a novel algorithm for inferring correct and ef.cient synchronization in concurrent \nprograms. The algorithm infers minimal atomic sections that can be veri.ed under a given abstraction. \n We advocate a new approach to veri.cation where both the program and the abstraction can be modi.ed \non the .y during the veri.cation process. This enables veri.cation of a restricted program where veri.cation \nof the original program fails.  We implemented our approach in a prototype tool called GUARDIAN and \napplied it to synthesize synchronization for sev\u00aderal interesting programs using numerical abstractions. \n  1.2 Limitations Our focus in this paper is on the AGS algorithm (Sec. 4) and on an algorithm for eliminating \ninvalid interleaving by adding atomic sections. While our approach can be instantiated with various abstraction-re.nement \nalgorithms and abstract domains, our cur\u00adrent realization is quite modest: The abstraction-re.nement \napproach we use in the paper is rather simplistic. Using more sophisticated re.nement ap\u00adproaches is \na topic of future work.  We only implement a number of simple numerical abstract domains, which enable \nus to handle in.nite-state numerical programs. To make the approach more widely applicable, we intend \nto integrate additional abstract domains in the future.   2. Overview In this section, we demonstrate \nour technique on a simple illus\u00adtrative example. The discussion in this section is mostly informal, additional \nformal details are provided in Section 4. Additional ex\u00adamples, inspired by real applications, are described \nin Section 7. 2.1 Example Program Consider the example shown in Fig. 1. In this example, the pro\u00adgram \nexecutes three processes in parallel: T1||T2||T3. Different interleavings of the statements executed \nby these processes lead to different values being assigned to y1 and y2. Inevery executionof the program \nthere is a single value assigned to y1 and a single value assigned to y2. The assertion in T3 requires \nthat the values of y1 and y2 are not equal. Initially, the value of all variables are 0. For example, \ny1 gets the value 6,and y2 gets the value 2 in the interleaving z++;x+=z;x+=z;y1=f(x);y2=x;z++;assert.In \nthe interleaving x+=z;x+=z;y1=f(x);y2=x;z++;z++;assert, y1 gets the value 5,and y2 gets the value 0. \nFig. 2 (I) shows the possible values of y1 and y2 that can arise during all possible program executions, \nassuming that the macro f executes atomically. Note that in some interleavings y1 and y2 may be evaluated \nfor different values of x (i.e., x can be incremented be\u00adtween the assignment to y1 and the assignment \nto y2). The point y1= y2=3 (marked in red in Fig. 2 (I)) corresponds to values that violate the assertion. \nThese values arise in the following inter\u00adleaving: z++; x+=z; y1=f(x); z++; x+=z; y2=x;assert. Our goal \nis to add ef.cient synchronization to the program such that the assertion in T3 is not violated in any \nexecution. The AGS algorithm iteratively eliminates invalid interleavings (under an abstraction) by either \nmodifying the program or the ab\u00adstraction. Fig. 2 shows how the algorithm operates on the program of \nFig. 1, and how it can move on both dimensions, choosing to modify either the program, or the abstraction, \non every step. Be\u00adfore we explain Fig. 2, we explain how the algorithm modi.es the program to eliminate \ninvalid iterleavings without any abstraction.  2.2 Inferring Synchronization under Full Information \nWe begin by considering the example program without abstraction. Since this is an illustrative .nite-state \nprogram, we can focus on the aspects of the algorithm related to generating atomicity constraints. The \nalgorithm accumulates atomicity constraints by iteratively eliminating invalid interleavings. Every invalid \ninterleaving yields an atomicity constraint that describes all possible ways to eliminate that interleaving, \nby disabling context-switches that appear in it. Under full information, the program of Fig. 1 has a \nsingle in\u00advalid interleaving z++; x+=z; y1=f(x); z++; x+=z; y2=x; assert. This interleaving can be eliminated \nby disabling either of the context switches that appear in this interleaving: the context switch between \nx+=z and x+=z in T1, between z++ and z++ in T2, and between y1=f(x) and y2=x in T3. This corresponds \nto the following atomicity constraint, generated by AGS algorithm: [y1=f(x),y2=x] .[x+=z,x+=z] .[z++,z++] \nThis constraint is a disjunction of three atomicity predicates, of the form [s1,s2],where s1 and s2 are \nconsecutive statements in the program. Each atomicity predicate represents a context-switch that can \neliminate the invalid interleaving, and the disjunction repre\u00adsents the fact that we can choose either \none of these three to elim\u00adinate the invalid interleaving. For this program, there are no addi\u00adtional \nconstraints, and any satisfying assignment to this constraint yields a correct program. For example, \nadding an atomic section around z++ and z++ in T2 yields a correct program. Since we can obtain multiple \nsolutions, it is natural to de.ne a quantitative criterion for choosing among them. This criterion can \nbe based on the number of atomic sections, their length, etc. Our approach separates the process of identifying \nthe space of solutions (generating the atomicity constraints) from the process of choosing between the \npossible solutions, which can be based on a quantitative criterion. In this example, each of the three \npossible solutions only requires a single atomic section of two statements. Next, we illustrate how AGS \noperates under abstraction. In this example, we use simple numerical domains: parity, intervals, and \noctagon. In Section 7, we show re.nement by increasing the set of variables for which the abstraction \ntracks correlations.  y1 43210654321 654321  y2 (I) interval T1 x+=z; x+=z T2 z++; z++; T3 y1=f(x) \ny2=x assert y1!= y2  0123 4 (II) Figure 2. (I) Values of y1 and y2 that arise in the program of Fig. \n1; (II) Atomic section around the assignments to y1 and y2 under interval abstraction; (a-g) Possible \nsteps of the AGS algorithm: on each step, the algorithm can choose between re.ning the abstraction (down \narrows) and modifying the program by avoiding certain interleavings (right arrows). 2.3 Inferring Synchronization \nunder Parity Abstraction We .rst show how the algorithm works using the parity abstraction over y1 and \ny2. The parity abstraction represents the actual value of a variable by its parity, and only observes \nwhether the value is even or odd. Variables y1 and y2 take abstract values from {.,E,O, T}, with the \nstandard meaning. The starting point, parity abstraction of the original program, is shown in Fig. 2 \n(a). It shows the concrete values of y1 and y2 that can arise during program execution, and their abstraction. \nThe concrete values are shown as full circles and are the same as in Fig. 2 (I). Black circles denote \nthe concrete values that satisfy the assertion, and red circle values that violate the assertion. The \nshaded area denotes the concretization of the abstract values com\u00adputed for y1 and y2. The abstract values \nfor both y1 and y2 are T. As a result, the concretization (the shaded area) covers the en\u00adtire plane. \nIn particular, it covers concrete values that violate the assertion. Values that cannot arise in any \nconcrete execution of the program (false alarms) are shown as hollow red circles in the .gure. The AGS \nalgorithm performs abstract interpretation of the pro\u00adgram from Fig. 1 using parity abstraction. In Fig. \n3 we show part of the abstract transition system constructed by AGS. Fig. 3 only shows abstract states \nthat can reach an error state. Error states are shown as dashed red line circles in the .gure. The values \nof vari\u00adables in a state are shown as a tuple (pc1,pc2,pc3,x,z,y1,y2), where variables y1 and y2 take \nan abstract value from the parity domain. This transition system is very simple and in particular con\u00adtains \nno cycles; however, this is only for illustrative purposes and the AGS algorithm handles all forms of \nabstract transition systems. Under parity abstraction, there are several invalid interleavings. The choice \nwhich of them to eliminate .rst is important, as dis\u00adcussed in Section 5. The AGS algorithm .rst chooses \nto elimi\u00adnate the invalid interleaving: p1 = z++; x+=z; x+=z; z++; y1=f(x); y2=x; assert. This interleaving \nis shown in Fig. 3 by emphasizing its edges (the right emphasized path in the .gure). Under this interleaving, \nand under the parity abstraction, y1= Tand y2= T(due to joins in the abstract transition system). The \nAGS algorithm can now choose whether to try and elimi\u00adnate this by either adding atomicity, or by re.ning \nthe abstraction. Fig. 2 shows these alternatives, which we explain in detail in the rest of this section. \nEliminate p1 by atomicity constraint: To eliminate this interleav\u00ading, the following constraint is generated: \n[z++,z++].This stepis shown as the step from Fig. 2 (a) to Fig. 2 (b). Note that the pro\u00adgram in Fig. \n2 (b) has an atomic section around the statements z++ and z++ in T2. This limits the concrete values \nthat y1 and y2 can take, as shown by the full circles in Fig. 2 (b), compared to those on Fig. 2 (a). \nIn particular, it eliminates the error state in which y1 and y2 both have the value 3 (no red full circle \nin the .gure). However, parity abstraction is not yet precise enough to verify the correctness of the \nresulting program, as shown by the shaded area in Fig. 2 (b). During abstract interpretation of the program, \ny1 takes both the values E and O, and thus goes to T. The concretiza\u00adtion (the shared area) therefore \nspans all possible concrete values of y1. The abstract value of y2 remains E, therefore the concretiza\u00adtion \n(the shaded area) only contains even values of y2. The abstract values represent three points that violate \nthe assertion, shown as hollow red circles in Fig. 2 (b). After eliminating p1 by adding the constraint \n[z++,z++], the following (abstract) interleaving may violate the assertion: p2 = x+=z;z++;z++;x+=z;y1=f(x); \ny2=x; assert.This interleaving yields the abstract values y1= Tand y2= T(due to joins), which may violate \nthe assertion. The interleaving p2 is shown in Fig. 3 as the left emphasized path in the .gure. Eliminate \np2 by atomicity constraint: To eliminate this interleav\u00ading, the following constraint is generated: [x+=z,x+=z].This \nstep is shown as the step from Fig. 2 (b) to Fig. 2 (c). The resulting overall constraint is: [x+=z,x+=z] \n.[z++,z++]  Figure 3. Partial abstract transition system for the program of Fig. 1. Only abstract states \nthat can reach an error state are shown. With this atomicity constraint, under the parity abstraction, \nthere are no further invalid interleavings. This constraint is sat\u00adis.ed by a program that has the statements \nx+=z and x+=z of T1 execute atomically, and the statements z++ and z++ of T2 execute atomically. In this \nprogram, the abstract values are y1= O and y2= E. These abstract values guarantee that the assertion \nis not violated, as shown in Fig. 2 (c). Eliminate p2 by abstraction re.nement: After eliminating the \ninterleaving p1, all remaining concrete interleavings satisfy the assertion, but we could not prove it \nunder parity abstraction. Instead of eliminating interleaving p2 by adding atomicity constraints, as \ndescribed above, we can choose to re.ne the abstraction from parity to interval, moving from Fig. 2 (b) \nto Fig. 2 (e). Interval abstraction is precise enough to prove this program.  2.4 Inferring Synchronization \nunder Interval Abstraction Instead of eliminating interleaving p1 by adding an atomicity con\u00adstraint, \nthe algorithm can choose to try and eliminate p1 by re.ning the abstraction from parity to interval. \nThis corresponds to the step from Fig. 2 (a) to Fig. 2 (d). Under interval abstraction, the abstract \nvalues are y1=[3, 6] and y2=[0, 4], representing two points that may violate the assertion, as shown \nin .gure Fig. 2 (d). The algorithm can again choose to eliminate invalid interleav\u00adings by adding an \natomicity constraint (step from Fig. 2 (d) to Fig. 2 (e)) or by abstraction re.nement (step from Fig. \n2 (d) to Fig. 2 (f)). In the former case, AGS produces the overall constraint: ([x+=z,x+=z] .[z++,z++]) \n.([y1=f(x),y2=x] .[x+=z,x+=z] .[z++,z++]) This constraint requires only one of T1 and T2 to execute atomically. \nFig. 2 (e) shows a program corresponding to one of the solutions, in which T2 is atomic. As apparent \nfrom the constraint above, [y1=f(x),y2=x] is not suf.cient for showing the correctness of the program \nunder the in\u00adterval abstraction. The result of applying interval abstraction to the program implemented \nfrom this constraint is shown in Fig. 2 (II). 2.5 Inferring Synchronization under Octagon Abstraction \nFinally, the octagon abstract domain [18], maintains enough infor\u00admation to only require atomicity as \nin the case with full informa\u00adtion. In particular, it is suf.cient to make y1=f(x) and y2=x ex\u00adecute \natomically for the program to be successfully veri.ed under Octagon abstraction, as shown in Fig. 2 (g). \n  3. Preliminaries Transition System A transition system ts is a tuple (S, T, Init)where S is a set \nof states, T .S \u00d7S is a set of transitions between states, and Init .S are the initial states. For a \ntransition t .T ,we use src(t) to denote the source state of t,and dst(t) to denote its destination state. \nFor a transition system ts, a trace p is a (possibly in.nite) sequence of transitions p0,p1,... such \nthat for every i> 0, pi .T and dst(pi-1)= src(pi). For a .nite trace p, |p|denotes its length (number \nof transitions). We use t.p to denote the trace created by concatenation of a transition t and a trace \np,when dst(t)= src(p0). A complete trace p is a trace that starts from an initial state: src(p0) .Init.We \nuse [[ts]] to denote the (pre.x-closed) set of complete traces of transition system ts. Program Syntax \nWe consider programs written in a simple pro\u00adgramming language with assignment, non-deterministic choice, \nconditional goto, sequential composition, parallel composition, and atomic sections. The language forbids \ndynamic allocation of threads, nested atomic sections, and parallel composition inside an atomic section. \nNote that a program can be statically associated with the maximal number of threads it may create in \nany execution. Assignments and conditional goto statements are executed atomi\u00adcally. All statements have \nunique labels. For a program label l,we use stmt(l) to denote the unique statement at label l. We use \nVar to denote the set of (shared) program variables. To simplify the exposition, we do not include local \nvariables in de.\u00adnitions, although we do use local variables in examples. There is nothing in our approach \nthat prevents us from using local variables, but having local variables makes the formal de.nitions cumber\u00adsome. \nWe assume that all program variables have integer values, initialized to 0. Program Semantics Let P be \na program with variables Var.Let k be the maximal number of threads in P , with thread identi.ers 1,...,k.Astate \ns is a triplet (vals,pcs)where vals : Var .Int is a valuation of the variables, and pcs : {1,...,k}.Int \nis the program counter of each thread, which ranges over program labels in the code executed by the thread. \nWe de.ne a transition system for a program P to be (SP , TP , InitP ), where transitions TP are labeled \nby program statements. For a transition t .TP ,we use stmt(t) to denote the corresponding statement. \nWe use lbl(t) and tid(t) to denote (unique) program label and thread identi.er that correspond to stmt(t), \nrespectively. A transition t is in TP if all of the following conditions hold: (a) the program counter \nof the thread tid(t) in state src(t) is at program label lbl(t), (b) the execution of the statement \nstmt(t) from state src(t) by thread tid(t) results in state dst(t), (c) no other thread is inside an \natomic section in state src(t). We use [[P ]] to denote the set of traces of P , i.e., [[P ]] =[[ts]] \nwhere ts = (SP , TP , InitP ).   Abstraction Our method is based on abstract interpretation [10]. In \nthis section, we quickly review relevant terminology that will be used throughout the paper. An abstract \ndomain is a complete join semilattice A = (A, ., U, .), i.e., a set A equipped with partial order ., \nsuch that for every subset X of A, A contains a least upper bound (or join), denoted UX. The bottom element \n..A is U\u00d8.We use x Uy as a shorthand for U{x, y}. In this paper, we assume that the abstract domain A \nis a pow\u00aderset of abstract states, with (partially) disjunctive join. An abstract state s is ranging \nover an abstract domain B = (B, .B, UB, .B). def For X .SP , the abstraction function a is de.ned by \na(X)= U{\u00df(s) |s .X},where \u00df is the abstraction function for the un\u00adderlying domain of abstract states. \nFor a given \u00df, the abstraction a can vary anywhere on the range between relational and carte\u00adsian , depending \non the de.nition of join. An abstract transformer for a program statement st is denoted by [[st]]a : \nA .A.For a .A, the abstract transformer is de.ned def pointwise: [[st]]a(a)= U{[[st]]\u00df (s) |s .a},where \n[[st]]\u00df is the abstract transformer for the underlying domain of abstract states. We abuse the notation \nslightly and use a to collectively name all the components of an abstract interpreter: its abstract domain, \nincluding the underlying domain of abstract states, abstract trans\u00adformers, and widening operator, if \nde.ned. We de.ne an abstract transition system for P and a to be (SP , TP , Init.),where Init.= a(InitP \n), and a transition (s, sI) PP labeled by a program statement st is in TP if and only if [[st]]\u00df (s) \n.B sI . We use [[P ]]a to denote the set of abstract traces of P , i.e., [[P ]]a =[[ts]] where ts is \nthe abstract transition system for P and a,inwhich SP is the result of abstract interpretation, i.e., \nthe set of abstract states at .xed point. Speci.cation The user can specify a state property S,which \ndescribes a set of program states. This property can refer to program variables and to the program counter \nof each thread (e.g., to model local assertions). Our approach can be extended to handle any temporal \nsafety speci.cations, expressed as a property automaton, by computing the synchronous product of program \ns transition system and the property automaton [9]. Given a (concrete or abstract) state s,we use s |= \nS to denote that the state s satis.es the speci.cation S. We lift it to traces as follows. A trace p \nsatis.es S, denoted by p |= S, if and only if src(p0) |= S and for all i =0, dst(pi) |= S.Aset . of (concrete \nor abstract) traces satis.es S, denoted by . |= S if and only if all traces in it satisfy S.  4. Computing \na Safe Schedule Under Abstraction Algorithm 1 provides a declarative description of abstraction\u00adguided \nsynthesis. The algorithm takes an input program, a spec\u00adi.cation, and an abstraction, and produces a \n(possibly modi.ed) program that satis.es the speci.cation. The main loop of the algorithm selects an \nabstract trace p of the program P such that p satis.es the atomicity formula .,but does not satisfy the \nspeci.cation S. Then, the algorithm attempts to eliminate this invalid interleaving p by either: adding \natomicity constraints: the procedure avoid generates atomicity constraints that disable p. The constraints \ngenerated by avoid for p are accumulated by AGS in the formula ..  re.ning the abstraction: using a \nstandard abstraction re.nement  approach (e.g., [3, 8]) to re.ne the abstraction. On every iteration, \nthe loop condition takes into account the up\u00addated . and a when choosing an invalid interleaving p. Some \nof the (abstract) invalid interleavings may correspond to concrete invalid interleavings, while others \nmay be artifacts of the abstraction. The choice of whether to eliminate an interleaving via abstraction \nre.nement, or by adding atomic sections, is left as non-deterministic choice (denoted by * in the algorithm). \nIn this section, we assume that it makes the right choices (for example, only picks re.nement when it \nis indeed possible to eliminate p using re.nement). In Section 5, we discuss how to implement it. When \nall invalid interleavings have been eliminated, AGS calls the procedure implement to .nd a solution for \nthe constraints accumulated in .. Algorithm 1: Abstraction-Guided Synthesis. 1 2 3 4 5 6 7 8 9 10 11 \n12 13 14 15 Input: Program P , Speci.cation S, Abstraction a Output: Program satisfying S under a . = \ntrue while true do .= {p |p .[[P ]]a n[[.]],p |= S} if . is empty then return implement(P ,.) p = select \ntrace from . if shouldAvoid(p, a) then . = avoid(p) if . = false then . = . .. else abort else aI = refine(a, \np) if aI = a then a = aI else abort end end Function avoid(p) Input: Trace p Output: Atomicity constraint \nfor avoiding p . = false foreach i =0,..., |p|do if exists j>i +1 such that tid(pi)= tid(pj ) and for \nall l such that i<l<j, tid(pi) = tid(pl) then . = . .[lbl(pi),lbl(pj )] end return . Function implement(P, \n.) Input: Program P , atomicity formula . Output: Program with atomic sections satisfying . Find a minimal \nsatisfying assignment G |= . P I = P with adding atomic sections in atomize(G) return P I 4.1 Generating \nAtomicity Constraints The procedure avoid takes a trace p as input, and generates an atomicity constraint \nthat describes all context switches in p,and thus describes all possible ways to eliminate p by adding \natomic sections to the original program. The atomicity constraint generated by avoid is a disjunction \nof atomicity predicates. An atomicity predicate requires that a pair of consecutive program statements \nexecute atomically, without in\u00adterleaving execution of other threads between them. Formally, given a \nprogram P , and a pair of program labels l and lI,we use [l, lI] to denote an atomicity predicate. In \nour examples, we write [stmt(l), stmt(lI)] instead of [l, lI].An atomicity formula is a conjunction of \ndisjunctions of atomicity predicates. Let p be a trace in a (concrete or abstract) transition system \nof P . We say that p satis.es [l, lI], denoted by p |=[l, lI], if and only if for all 0 =i,if lbl(ti)= \nl and i +1 < |p|,then lbl(ti+1)= lI and tid(ti)= tid(ti+1).  A set of traces . satis.es an atomicity \npredicate p, denoted by . |= p, if and only if all the traces in . satisfy p. Similarly, we interpret \nconjunctions and disjunctions of atomicity predicates as intersection and union of sets of traces. The \nset of traces that satisfy an atomicity formula . is denoted by [[.]]. The procedure avoid only generates \natomicity predicates for neighboring locations (locations that appear in the same thread, where one location \nimmediately follows the other), with the intu\u00aditive meaning that no operation is allowed to interleave \nbetween the execution of these neighboring locations. The algorithm identi.es all context switches in \np as follows. A context switch after transition pi occurs if there is another transition pj by the same \nthread later in the trace, but not immediately after pi. Then, if the transition pj is the .rst such \ntransition after pi,we generate the atomicity predicate [lbl(pi),lbl(pj )]. In the case of an invalid \nsequential interleaving, an interleaving in which each thread runs to completion before it context-switches \nto another thread, it is (obviously) impossible to avoid the inter\u00adleaving by adding atomic sections. \nIn such cases, avoid returns false and AGS aborts.  4.2 Implementing Atomicity Constraints The procedure \nimplement takes a program P and an atomicity formula . as input. An atomicity formula can be seen as \na formula in propositional-logic, where the atomicity predicates are treated as propositional (boolean) \nvariables. Note that the atomicity formula is in positive CNF, and thus it is always satis.able. The \nprocedure constructs a program P I by .nding a minimal satisfying assignment for ., i.e., a satisfying \nassignment with the smallest number of propositional variables set to true. The atom\u00adicity predicates \nassigned to true in that assignment are then imple\u00admented as atomic sections in the program. Our approach \nseparates the characterization of valid solutions from their implementation. The atomicity formula . \nmaintained in the algorithm provides a symbolic description of possible solutions. In this paper, we \nchoose to realize these by changing the program and adding atomic sections. However, these could be realized \nusing other synchronization mechanisms, as well as by controlling the scheduler of the runtime environment \n(if such scheduler exists). In general, there could be multiple satisfying assignments for ., corresponding \nto different additions of atomic sections to the input program P . Usually, we are interested in minimal \nsatisfying assignments, as they represent solutions that do not impose redun\u00addant atomic sections. To \nrealize a satisfying assignment G |= . as atomic sections, we de.ne atomize(G) to extract the minimal \n(contiguous) atomic sections from the assignment. Towards this end, we construct the set of program labels \nin which context switches are not permit\u00adted by G: L = {lI |[l, lI] .G}. For every maximally-connected \ncomponent of L in the control-.ow-graph of the original program, we .nd the immediate dominator and postdominator, \nand add (be\u00adgin and end) atomic section at these labels, respectively. This may cause extra statements \nincluded in an atomic section, eliminating additional interleavings. This situation is sometimes unavoidable \nwhen implementing atomicity constraints using atomic sections. It is possible that implementing an assignment \nG results in eliminating additional interleavings even when there are no extra statements in the atomic \nsection. Consider the example of Fig. 4. In this example, T2 cannot interleave with the .rst iteration \nof the loop in T1. But once the .rst iteration is over, it can interleave with any other iteration. However, \nsince we require implementation via atomic sections, the only implementable solution is to add an atomic \nsection around the statements x++ and x++ inside the loop, forcing every iteration of the loop to be \nexecuted atomically. 4.3 Abstraction Re.nement The procedure refine takes an interleaving p as input \nand at\u00adtempts to re.ne the abstraction in order to avoid p. For that to be possible, p has to be an artifact \nof the abstraction, and not cor\u00adrespond to a concrete invalid interleaving. AGS tries to re.ne the abstraction \nby calling refine, but if the abstraction cannot be re\u00ad.ned, and refine returns the same abstraction, \nAGS aborts. In this paper, we focus on the procedure for restricting invalid interleavings, and can leverage \nany standard re.nement scheme (e.g., [3, 4, 8, 22]). In the examples, we use two kinds of simple re.nements: \none that moves to another abstract domain (Section 2), and one that varies the set of variables that \nare abstracted relation\u00adally (Section 7). 4.4 Choosing Interleaving p to Eliminate Since our program \nmodi.cations consist of adding atomic sections, we cannot eliminate sequential executions (which have \nno context switches). It is therefore required that we can verify the correctness of the sequential runs \nof the program under the given abstraction. In fact, for verifying the correctness of interleavings that \ninvolve fewer context-switches, less precise abstractions can be suf.cient. Generally, it is natural \nto consider interleavings in an increasing order of the number of context switches. Atomicity constraints \nobtained for interleavings with a lower number of context switches restrict the space that needs to be \nexplored for interleavings with higher number of context switches.  4.5 Program Modi.cation vs. Abstraction \nRe.nement When an invalid interleaving p is detected, a choice has to be made between re.ning the abstraction \nand adding an atomicity constraint that eliminates p. This choice is denoted by the condition shouldAvoid(p, \na) in the algorithm. Apart from clear boundary conditions outlined below, this choice depends on the \nparticular abstractions with which the algorithm is used. When p is a sequential interleaving, and avoid \nis realized as the addition of atomic sections, it is impossible to add atomicity constraints to avoid \np. Therefore, in this case, the only choice is to re.ne the abstraction (if possible). Hence, the condition \nshouldAvoid(p, a) is set to return false when p is a sequential interleaving. Similarly, depending on \nthe re.nement framework used, it may be impossible to further re.ne the abstraction a. For example, when \nusing a .xed sequence of abstraction with increasing precision (as in Section 2), upon reaching the most \nprecise abstraction in the se\u00adquence, there s no way to further re.ne the abstraction. Therefore, in \nthis case, the only choice is trying to avoid the interleaving p,and the condition shouldAvoid(p, a) \nreturns true when it is known a priori that a cannot be re.ned anymore. For re.nement schemes that use \nsymbolic backwards execu\u00adtion to .nd a concrete counterexample (e.g., [3, 8]), the condition shouldAvoid(p, \na) can be based on the result of the symbolic exe\u00adcution. When the re.nement scheme is able to .nd a \nconcrete coun\u00adterexample, shouldAvoid(p, a) can choose to repair, using the concrete counterexample as \nbasis. If the re.nement scheme fails to .nd a concrete counterexample, but also fails to .nd a spuri\u00adous \npath for abstraction re.nement, shouldAvoid(p, a) can again choose to repair, as re.nement cannot be \napplied. Attempting veri.cation with a re.ned abstraction may fail due to state explosion. In most cases \nthere is no way to check for such failure a priori in the condition shouldAvoid(p, a). Practically, it \nis useful to invoke the veri.cation procedure as a separate task, and implement a backtracking mechanism \nfor the re.nement when ver\u00adi.cation fails to terminate after a certain time. Backtracking the re\u00ad.nement \nmay enable successful veri.cation of a more constrained variant of the program.  T1 {T2 { while (*) \n{if (x==1) {x++ assert false x++ } }} } Figure 4. Limitations of implementability. Correctness only \nre\u00adquires the .rst iteration of the loop in T1 be executed atomically. Implementability forces every \niteration to be executed atomically.   5. Abstraction Guided Synthesis In the previous section, we \ndescribed the AGS algorithm in a declarative manner, and omitted some details that we now address: how \ndo we compute [[P ]]a?  how do we obtain an interleaving p .[[P ]]a n[[.]] and p|= S?  how do we choose, \non every step of the algorithm, whether to add atomicity constraints or to re.ne the abstraction? To \nrealize Algorithm 1, we .rst use standard abstract interpreta\u00ad  tion to compute the set of abstract \nstates SP reachable from InitP under abstraction a. Then, we explore the invalid interleavings and eliminate \nthem. The algorithm is amenable to several optimiza\u00adtions, and we describe them later in this section. \nIn the pseudocode of Algorithm 1, we replace the declarative expression in Line 3 with a call to function \nTraces: .= Traces(InitP ,BadP , SP ,.) in Algorithm 1, where BadP is the set of reachable error states: \n{s .SP |s|= S}. Function Traces(X, Y, V, .) Input: Set of abstract states X, Y, V , Atomicity Formula \n. Output: Set of traces from X to Y passing in V , satisfying . workset = {t |src(t) .V \\X, dst(t) .Y \n}result = {t |src(t) .X, dst(t) .Y }while workset is not empty do p = select and remove interleaving \nfrom workset foreach Statement st and state s .V such that [[st]]\u00df(s) gB src(p0) do t = transition (s, \nsrc(p0)) labeled with st pI = t.p if pI |= . and pI is acyclic then if s .X then result = result .{pI}else \nworkset = workset .{pI}end end end return result The function Traces(X, Y, V, .) enumerates all traces \nthat start in a state in X .V , end in a state in Y .V , go only through states in V and satisfy the \natomicity constraint .. It works by performing a backward exploration starting from states in Y and extending \ninterleaving suf.xes backwards. A suf.x is further extended only as long as it satis.es .. Thus, the \nalgorithm lever\u00adages the constraints that are already accumulated in the atomicity formula . to prune \nthe interleavings that have to be explored. The use of . is critical for the practicality of the approach, \nas shown experimentally in Section 7. Exploring .-enabled statements We say that statement st is .\u00adenabled \nin state s when executing st from s does not contradict .. Formally, given a set of states V , the condition \nenabled(st, s,.,V ) holds if and only if for every pair of transitions t and tI such that src(t) .V , \ndst(tI) .V , dst(t)= src(tI)= s .V and stmt(tI)= st, the partial trace t.tI satis.es .. For example, \nif . is [a,c] then st is not .- ab enabled in s, in the partial state space shown on c st the right. \nHowever, if . is [a,c] .[b,c],then st is .-enabled in s. Algorithm 2: Abstraction-Guided Synthesis. Input: \nProgram P , Speci.cation S, Abstraction a Output: Program satisfying S under a 1 states = workset = InitP \n2 . = true 3 while workset is not empty do 4 s = select and remove state from workset 5 foreach Statement \nst do 6 if enabled(st,s, .,states) then 7 sI =[[st]]\u00df(s) 8 if sI |= S then 9 select p .Traces(InitP \n, {sI}, states\\workset, .) 10 if shouldAvoid(p, a) then 11 . = avoid(p) 12 if . = false then 13 . = . \n.. 14 states = workset = InitP 15 disabled = \u00d8 16 else abort 17 else 18 // refine(p) 19 end 20 else \n21 if {sI}gstates then 22 states = states U{sI} 23 X = {sII .states |sI gB sII} 24 workset = workset \nUX 25 end 26 end 27 end 28 end 29 end 30 return implement(P ,.) Forward Pruning using . Algorithm 2 \nis an optimized version of Algorithm 1. In the optimized algorithm, we focus on the explo\u00adration code, \nand on the code for avoiding an interleaving (Lines 11\u00ad16), the code for re.nement is similar and is \nabbreviated to a com\u00adment in Line 18. The algorithm combines (forward) abstract inter\u00adpretation of the \nprogram, with (backward) exploration of invalid in\u00adterleavings. The main idea of the algorithm is to \nuse the constraints accumulated in . to restrict the space that has to be explored both forward and backward. \nIn particular, this optimization avoids con\u00adstructing the entire (unrestricted) transition system upfront. \nThe abstract interpretation part of the algorithm is standard, and uses a workset to maintain abstract \nstates that should be explored. Once the workset is empty we know a .xed point is reached. At every point, \nforward exploration of new states is restricted by the current constraints accumulated in . (Line 6). \nFor every invalid interleaving p, the formula . represents all the possible ways to eliminate p. This \nmeans that the algorithm only restricts further exploration when the next exploration step contradicts \nall possible ways to eliminate existing invalid interleavings. In the algorithm, we use the join operator \nof the abstract do\u00admain to add new states to the set states of explored abstract states (Line 24). More \ngenerally, the algorithm can use a widening op\u00aderator [10] when required. To determine whether a state \nshould be added to the set of states, we check whether the state is already represented in states (Line \n21).  Rebuilding Parts of the Transition System Instead of rebuild\u00ading the whole transition system whenever \nwe add a constraint to . (Line 14), or whenever we re.ne the abstraction, we can rebuild only the parts \nof the transition system that depend on the modi.ca\u00adtion. Following approaches such as [13], we can invalidate \nonly the parts of the abstract transition system that may be affected by the re.nement, and avoid recomputation \nof other parts. Lazy Abstraction Algorithm 2 need not maintain the same ab\u00adstraction across different \ninterleavings. The algorithm can be adapted to use lazy abstraction re.nement as in [13]. Instead of \nmaintaining a single homogenous abstraction a for the entire pro\u00adgram, we can maintain different abstractions \nfor different parts of the program, and perform lazy re.nement. Simpli.cation of . Rather than taking \nthe conjunction of con\u00adstraints as they are accumulated in ., we preform (propositional) simpli.cation \nof . on-the-.y. This is required in practice, as the number of terms added to . may be large even for \nsmall programs. Multiple Solutions The algorithm as described here only yields a single minimal solution. \nIn practice (and in our implementation, described in Section 7), it is often desirable to present the \nuser with a range of possible solutions and let the user make her own choice.  6. Correctness and Minimality \nIn this section, we show that Algorithm 1 computes a correct program with smallest atomic sections, assuming \nthe abstraction is .xed. At the end, we discuss the effect of abstraction re.nement, and correctness \nof Algorithm 2. 6.1 Correctness In this section, we assume that the abstraction is .xed, i.e., shouldAvoid \nin Algorithm 1 always returns true. The follow\u00ading theorem says that a run of the AGS algorithm terminates \nwith either an abort or a valid program. THEOREM 6.1 (Correctness). A run of the AGS algorithm termi\u00adnates \nwith either an abort or returns a program P I such that (1) P I satis.es S under a, i.e., [[P I]]a |= \nS, and (2) P I admits a subset of interleavings of the original program P ,  i.e., [[P I]]a .[[P ]]a. \nSketch of Proof: In every iteration, the AGS algorithm eliminates at least one simple path to error state \nfrom the abstract transition system. As a result, the abstract transition system may be modi.ed to take \ninto account the updated atomicity formula ..However, the abstract transition system is always modi.ed \nin a way that does not introduce any new paths, in particular it has no new paths to error states. Because \nthe number of simple paths is .nite, the while loop in the AGS algorithm terminates either by eliminating \nall simple paths to error, or .nding a path to error that has no context switches and thus it cannot \nbe eliminated by our method. In the latter case, the algorithm aborts. In the former case, the set of \ntraces . is empty. That is, any execution of P that respects the atomicity formula . satis.es S under \nthe abstraction a.Let P I be the program returned by implement(P,.) in this case. The interleavings of \nP I is a subset of those of P permitted by . under a: [[P I]]a .[[P ]]a n[[.]]. Therefore, P I satis.es \nS under a and AGS algorithm returns P I . The AGS algorithm cannot .x a program whose sequential executions \ndo not satisfy S under a. Otherwise, if there is a way to add atomic sections to P such that the result \nsatis.es S under a, then there exists a run of the AGS algorithm that does not abort, and computes a \nresult. In the worst case, it makes the program always execute sequentially. T1 { 0: if (y==0) y=2 if \n(y==0) goto L 1: x++ 2: L: } T2 { 0: y=2 1: x+=1 2: assert x=y }  (a) (b) Figure 5. Example demonstrating \nthe effect of join and the choice of different abstract traces to eliminate. THEOREM 6.2. If the sequential \nexecutions of P satisfy S under a, then there exists a run of AGS algorithm that does not abort. In Algorithm \n2, at Line 9, we always choose from Traces a trace p that has context switches, if there is one. It guarantees \nthat no run of AGS algorithm aborts if the sequential version of P is valid. The toy example in Fig. \n5(a) has a single invalid interleaving: y=2;if(y==0);x++;x+=1;assert, as shown on Fig. 5(b). How\u00adever, \nunder parity abstraction, there are two invalid interleavings, due to join (shaded area). One of them \nis the abstraction of the concrete invalid interleaving, denoted by p1. The other one is a se\u00adquential \ninterleaving, denoted by p2,in which T1 executes .rst, and then T2. If the AGS algorithm .rst chooses \nto eliminate p2,it will abort, because there are no context switches to disable. How\u00adever, if we chose \np1 .rst, avoid will return the atomicity constraint [y=2,x+=1], and the program will be successfully \nveri.ed under this constraint, using parity abstraction. Similarly, we can construct an example in which \na wrong choice leads to larger atomic sections than necessary.  6.2 Minimality Next, we de.ne the notion \nof a minimally-atomic program, and show how to use the AGS algorithm to compute all minimally\u00adatomic \nprograms for a given input program, speci.cation and ab\u00adstraction. Let G be a set of atomic predicates \nthat refer to a program P . In AGS algorithm, we obtain G as a satisfying assignment to the atomicity \nformula .. Recall from Section 4.2 that there is a unique way to realize G by adding atomic sections \nto P . Let us denote the resulting program by P |G. Let P I be obtained from P by adding atomic sections. \nWe use G(P, P I) to denote the (unique) set of atomic predicates that corresponds to these atomic sections. \nMinimally Atomic Programs A valid program is minimally\u00adatomic when removing or shrinking any atomic section \nin it makes it invalid. DEFINITION 6.3 (Minimally Atomic). Consider a program P and an abstraction a.Let \nP I be a program obtained from P by adding atomic sections. P I is minimally-atomic with respect to a \nif and only if [[P I]]a |= S and for every program P II obtained from P by adding atomic sections, if \nG(P, P II) .G(P, P I),then [[P II]]a|= S. The condition G(P, P II) .G(P, P I) means that the atomic sec\u00adtions \nof P II is a (strict) subset of those of P I . We use MA(P, a) to denote the set of all minimally-atomic \nprograms with respect to a that can be obtained from P .  The programs in MA(P, a) have incomparable \nsets of atomic sections, i.e., for every pair P I,P II .MA(P, a), G(P, P I).G(P, P II). However, they \nmay have the same set of traces under a (and even concrete traces). When the abstraction a is not precise \nenough to prove that all sequential executions of P satisfy S, MA(P, a) is empty. In the rest of this \nsection, we show that every minimally-atomic program can be implemented by AGS algorithm. THEOREM 6.4 \n(Minimality). For every minimally-atomic pro\u00adgram P I .MA(P, a), there exists a run of the AGS algorithm \nthat returns P I . Sketch of Proof: Let P I .MA(P, a) and let G=G(P, P I).We show that G is a satisfying \nassignment to . computed in some run of AGS algorithm, i.e., for some sequence of invalid interleavings \npicked by AGS to be eliminated. Let GI be a maximal subset of G such that P |G! = P I. Because P I is \na minimally-atomic program w.r.t. a, [[P |G! ]]a|= S.There exists an atomicity predicate p such that \n[[P |G!.{p}]]a |= S. Thus, there is an invalid interleaving p in [[P |G! ]]a that is eliminated by p. \nNote that the atomicity predicates in G \\(GI .{p}) are the result of atomize.That is, P I = P |G= P |G!.{p}, \nbecause P I is the result of implement(P,.) which chooses GI .{p}as the minimal assignment it implements. \nAssume that the invalid interleaving p is picked by AGS in the last iteration. Atomicity constraint . \ngenerated by avoid(p) will include p as one of its disjuncts. Suppose that there exist a run of AGS that \nproduces .I such that GI is a minimal satisfying assign\u00adment for .I. Then, GI .{p}is a minimal satisfying \nassignment to . = .I .. and . is produced in the last iteration of AGS. Similarly, we can continue subtracting \natomicity predicates from GI, constructing the sequence of invalid interleavings back\u00adwards, until we \nrun out of atomicity predicates. The following proposition is much stronger than Theorem 6.4, as it requires \nthat a single run of the AGS algorithm yield all minimally-atomic programs. Moreover, the minimally-atomic \npro\u00adgrams exactly correspond to all minimal satisfying assignments of the atomicity formula . computed \nby that run. PROPOSITION 6.5 (Minimality-Strong). If the sequential execu\u00adtions of P satisfy S under \na, then there exists a run of the AGS algorithm that yields atomicity formula . such that for every \nminimal satisfying assignment A to ., the program implement(P, A) .MA(P, a),  for every P I .MA(P, a), \nthere exists a minimal satisfying assignment A for ., such that implement(P, A) returns P I .  Correctness \nand Minimality of Algorithm 2 Correctness of the operational version of the AGS algorithm, given in Algorithm \n2, follows from the fact that an invalid interleaving eliminated by Al\u00adgorithm 2 from a partial transition \nsystem is also an invalid inter\u00adleaving that can be chosen by an iteration of Algorithm 1 from the corresponding \nfull transition system. Minimality follows from the fact that the order of (forward) exploration in Algorithm \n2 can be chosen to discover error states in a way that exhibits any sequence of minimal invalid interleavings. \nAbstraction Re.nement If re.nement is not guaranteed to termi\u00adnate, then AGS algorithm is not guaranteed \nto terminate. The rea\u00adson is that every re.nement step may produce new simple invalid interleavings. \nWhen the re.nement is guaranteed to be monotonic, i.e., abstraction is more precise in every step (e.g., \nparity to inter\u00advals is not monotonic), we can attain minimality under abstraction re.nement, by discarding \nthe atomicity constraints . after each re\u00ad.nement step. When the re.nement is not monotonic, we can de.ne \na minimally-atomic program to respect any of the explored abstrac- Program Re.nement Steps Avoid Steps \nDouble Buffering 1 2 Defragmentation 1 8 3D Update 2 23 Array Removal 1 17 Array Init 1 56 Table 1. Experimental \nResults. tions. In the case of lazy abstraction, which re.nes only part of the state-space, the de.nition \nof minimality is even more involved. Finding a minimally-atomic program requires backtracking and it \nis at least exponential in the size of the abstract transition system of the input program, inline with \nthe known complexity bounds for game-based synthesis [14]. Thus, it is more valuable to invest into a \ngood heuristic. The simple heuristics that we use in the AGS algorithm produce reasonable, and often \nminimal, synchronization in practice, as we show in the next section.   7. Experience Webuiltaprototypetoolnamed \nGUARDIAN basedontheAGSalgo\u00adrithmofSection5.Weapplied GUARDIAN toseveralinterestingpro\u00adgrams, inspired \nby real applications, which we describe next. The abstractions we used are variants of parity and interval \ndomains, where the abstractions differ in what variables are kept relational. Table 1 summarizes our \nexperimental results. Note that all of our example programs are in.nite state, and hence require abstrac\u00adtion \nfor full veri.cation. In our experiments, we were interested in exploring the space of .xes under several \nabstractions. Even when GUARDIAN found a solution with the original abstraction, we still let it explore \nsolutions with .ner abstractions. For every program in the table, we report the number of re.nement steps, \nand number of avoid steps performed by the algorithm. In Table 2, we report the atomicity constraints \nfound by GUARDIAN for programs whose code is shown in the paper (atomicity constraints refer to the code). \nWhen using .-pruning, all experiments ran in less than 10 minutes. Without using . to restrict exploration, \nmost programs went out of memory exploring a hopelessly large (and redundant) space of interleavings. \nTo enumerate minimal assignments for the atomicity constraints constructed by our algorithm, GUARDIAN \nuses a model enumerator [1]. In the rest of this section, we describe some of our examples in more detail. \n 7.1 Abstract Domain In our examples, the abstract domain is a powerset of abstract states. An abstract \nstate s is a tuple (vals,pcs),where val maps program variables to their abstract values, ranging over \nan abstract domain such as parity, sign or interval. For X .SP , the abstraction function a is de.ned \nas follows: def a(X)= U{(.(vals),pcs)|s .X} where . maps concrete values of program variables to their \nabstract values. We use U. and . to denote the join and order of abstract values. The values of program \ncounters are preserved. We use the following join and partial order, parametric on the variables tracked \nrelationally. Let V .Var be a subset of variables. The join Ufor the abstract domain A is de.ned using \na relational join over a subset of variables in V and cartesian join over the rest of the variables: \n. . {s1,s2}if pcs1 = pcs2 or def s1Us2 = exists v .V s.t. vals1 (v)= vals2 (v) . {(vals1 U. vals2 ,pcs1 \n)}otherwise For V = Var this de.nes relational join. For V = \u00d8this de.nes cartesian join. Most of our \nexamples vary the abstraction by varying  Program Abstraction (set of tracked variables V ) Solution \n(atomicity constraints) DBuffer \u00d8 [fill:L1,fill:L2]) .([render:L1,render:L2] {F ill, Render} Defrag {Barrier, \nRegion, F 1,F 2,empty}{Barrier, Region, F 1,F 2,empty,i, j} 3D update {x2,x3,y3,z1,z3}{x2,x3,y3,z1,z3,y2,z2}{x2,x3,y3,z1,z3,y2,z2,x1,y1} \ntrue [D:L1,D:L2] .[U:L1,U:L2] .[U:L2,U:L3] .[U:L3,U:L4] [D:L1,D:L2] .[U:L1,U:L2] [P1:L2,P1:L3] .[P2:L2,P2:L3] \n.[P1:L8,P1:L9] [P1:L2,P1:L3] .[P2:L2,P2:L3]) true Table 2. Abstraction and solutions for some of the \nexample programs int Fill = 1; render() { int Render = 0; L1:if (j < N) { int i=j =0;  L2: write(Im[Render][j]); \n fill() { L3: j+=1; L1:if (i < N) { L4: goto L1; L2: Im[Fill][i] = read(); } L3: i+=1; L4: goto L1; \n L5: j = 0; } L6: goto 1; L5: Fill = 1; } L6: Render = 1; main() { L7: i = 0; fill() || render(); \n L8: goto L1; } } Figure 6. Double Buffering the relationality in the join. Because the join is parametric \non the set V , in the presentation of our examples, we only vary the value of V .The value of V for each \nexample is shown in Table 2. The partial order on A is de.ned as follows: for all Y, Y I .A, YY I if \nand only if for all s1 .Y there exists s2 .Y I such that pcs1 = pcs2 ,and vals1 . vals2 .  7.2 Double \nBuffering This example is motivated by the mechanism of double buffering. Variants of this mechanism \nare used in a variety of settings, from computer graphics to device drivers. This scheme is illustrated \nin Fig. 6. There are two buffers of images (Im)oflength N. The .ller process .lls the buffer indexed \nby the variable Fill, while at the same time the rendering process consumes the buffer indexed by variable \nRender. When the .lling completes, the values of the two variables are swaped and the .lling restarts. \nThe rendering process simply renders the image indexed by variable Render.To avoid clutter, we assume \nthat rendering is at least twice as fast as .lling and hence before Render is changed, the value of its \nbuffer has been written to the screen at least once (writing to the screen is idempotent and hence can \nbe repeated). Speci.cation: We would like to prove that the .ller and renderer processes never access \nthe same location simultaneously. Formally: pc(fill)= L2 .pc(render)= L2 .\u00ac(Fill = Render .i = j) Result: \nOur .rst solution is obtained with a cartesian parity abstrac\u00adtion. This abstraction loses relationship \nbetween variables Fill, Render, i and j when states are joined. Formally, the set V of tracked variables \nis empty (V = \u00d8). Recall that the program coun\u00adters are always kept relational. With a more re.ned abstraction, \nGUARDIAN proves the correct\u00adness of the original program. The key reason why we succeeded in this case \nis that this abstraction maintains the relationship be\u00adtween the values Fill and Render on each iteration \nof the loop and can show that these two variables are never equal. In this ex\u00adample, re.ning the abstraction \nled to proving the program without any necessary .xes. Further re.ning the abstraction (e.g. inserting \nvariable i or j in the set V ) is not necessary. 7.3 Concurrent Defragmentation This example is inspired \nby the problem of defragmentation. De\u00adfragmentation algorithms are used in various storage management \nscenarios (e.g., memory, disk storage) to increase space utilization. In many cases, defragmentation \ntakes place concurrently with an executing application. In Fig. 7, we show a simpli.ed system where one \nprocess called Defragment performs memory compaction concurrently with another process called Update \nwhich allocates new entries in memory. The memory is organized as an array of entries called Pages. The \nsize of the array N is unknown a-priori. Each entry in the array is either occupied (set to true) or \nfree (set to false). In practice, an entry may correspond to a heap object or a .le on a disk drive. \nTypically, each entry will also contain various other data .elds, which we have omitted here for simplicity. \nTo avoid synchronization on each entry, the two processes should always work on disjoint regions of memory. \nTo ensure that, at the start of their operation, the two processes handshake and then each picks a separate \nregion to work with (labels L1-L2 in each process). Note that the handshake is not deterministic, and \nprocesses could select different regions on different handshakes. In our case, there are two regions, \nwith the .rst region contain\u00ading memory locations with an even index and the second region containing \nmemory locations with an odd index. Defragment works by iterating over the array and moving all used \nentries to one side of the page. Update works by selecting a memory location and updating it if some \ncondition holds. Speci.cation: The processes should always access disjoint mem\u00adory locations when at \nthe program points accessing shared memory locations. (We omit the speci.cation as it is long and tedious). \nResult: The resulting constraints are shown in Table 2. The names of the processes have been abbreviated \nusing their .rst letter. Note that the original program is incorrect (variable Region is incre\u00admented \nwithout any synchronization), and with this more re.ned abstraction, the inferred correction is not a \nfalse positive (e.g. it is not due to an imprecision of the abstraction). However, the con\u00adstraint [U:L2,U:L3] \n.[U:L3,U:L4] inferred with the coarser cartesian abstraction is due to the imprecision of the abstraction. \n 7.4 3D Grid Computation Consider a concurrent program that updates values in a 3 dimen\u00adsional grid. \nThe program is shown in Fig. 8. Processes P1 and P2 should always access disjoint memory locations and \nhence no syn\u00adchronization between the processes should be required. P1 starts by reading a value from \nthe input and then begins a loop which adds this value to the locations on the diagonal of the 2D matrix. \nWe it\u00aderate over the diagonal of the 2D (x,y) plane as the value of variable z1 is .xed to 1 and only \nx1 and y1 change. The loop comprises the statements at labels L2 and L6 in P1. After the plane is updated, \nP1 updates a value in another plane (L7-L9). For clarity we have only shown the update of a single location, \nbut this can also be extended to update the diagonal. Similarly, process P2 updates the diagonal  Barrier \n= F1 = F2 = 0; Region = 2; Defragment() { /* Pick a Region */ L1: i = Region; Update() { L2: Region \n= i -1; /* Pick a Region */ L3: empty = i -2; L1: j = Region; L4: if (i >= N) goto L14; L2: Region \n= j -1; /* has free entry? */ L3: b = Pages[j]; L5: b = Pages[i]; /* Update the Page */ L6: if (!b \n&#38;&#38; empty <= 0) L4: if (!b) L7: empty = i; Pages[j] = true; /* Copy Entry */ /* Barrier Sync \n*/  L8: if (b &#38;&#38; empty > 0) { L5: Barrier += 1; F2 = 0; L9: Pages[empty] = true; L6: if (F2 \n== 1) L10: empty += 2; goto L7; L11: Pages[i] = false; if (Barrier == 2) { }Barrier = 0; F1 = 1; L12: \ni += 2; Region = 2; L13: goto L4; goto L7; /* Barrier Synch */ } L14: Barrier += 1; F1 = 0; goto L6; \n L15: if (F1 == 1) L7:  goto L16; } if (Barrier == 2) { Barrier = 0; F2 = 1; main() { Region = 2; Defragment() \n|| Update();  goto L16; } } goto L15; L16: goto L1; } Figure 7. Concurrent Defragmentation x1 = 0; \ny1 = 0; z1 = 1; x2 = 0; y2 = 1; z2 = 1; x3 = 0; y3 = 1; z3 = 0; P2() L1: v = read();  P1() L2: r = \nA[x2][y2][z2]; L1: v = read(); L3: A[x2][y2][z2] = r + v; L2: r = A[x1][y1][z1]; L4: y2 += 1; L3: A[x1][y1][z1] \n= r + v; L5: z2 += 1; L4: x1 += 1; L6: if (y2 < N) L5: y1 += 1; goto L2; L6: if (x1 < N) goto L2; main() \n P1() || P2() L7: v = read(); L8: r = A[x3][y3][z3]; L9: A[x3][y3][z3] = r + v; Figure 8. Concurrent \n3D Updating of a 2D plane but this time in the (z,y) dimension. That is, the value of x2 is .xed and \nonly y2 and z2 are updated. Speci.cation: The two processes should never access the same locations simultaneously. \nThat is, if process P1 is reading or writing shared data (e.g. at labels L2, L3, L8, L9), P2 should not \nbe writing simultaneously (e.g. be at L3), and vice versa for P2,where the indices of the array being \naccessed are equal for both processes. Result: As shown in Table 2, re.ning the abstraction leads to \nweaker atomicity constraints. In this example, we have 3 layers of abstractions, each leading to .ner-grained \nsolutions.   8. Related Work Synthesis from Temporal Speci.cations: Earlyworkby Emerson and Clarke \n[7] uses temporal speci.cations to generate a synchro\u00adnization skeleton. This has been extended by Attie \nand Emerson to synthesize programs with .ner grained atomic sections [2]. Early work by Manna and Wolper \n[16] synthesizes CSP programs. Pnueli and Rosner [20] consider the problem of synthesizing a reactive \nmodule based on an LTL speci.cation. They discuss the problem of implementability in this setting, and \nde.ne necessary and suf.\u00adcient conditions for the implementability of a given speci.cation. Our work \nfocuses on concurrent programs for shared memory and is based on abstract interpretation, handling in.nite-state \nsystems. Program Repair and Game-Based Synthesis: Jobstmann et. al. [14] consider the problem of program \nrepair as a game. In their approach, a game is constructed from (a modi.ed version of) the program to \nbe repaired, and an LTL speci.cation of the correctness property. The problem of repair boils down to \n.nding a winning strategy in that game. This approach has been later extended to provide fault localization \nand .xing [15, 27]. The approach has also been extended to work with predicate abstraction in [12]. In \ncontrast to these, we focus on concurrent programs, use abstract interpretation, and solve the quantitative \nproblem of computing a minimally constrained program. In our previous work [30], we focused on inference \nof CCR guards in .nite-state concurrent programs, where the atomic blocks were not modi.ed. This work \ncan be viewed as the next general step and addresses the more general problem of in.nite-state systems, \nemploys abstract interpretation, and infers atomicity constraints (as opposed to only inferring guards). \nDynamic Approaches: The problem of restricting the program to valid executions can be addressed by monitoring \nthe program at runtime and forcing it to avoid executions that violate the speci\u00ad.cation. However, restricting \nthe executions of a program at run\u00adtime requires a recovery mechanism in case the program already performed \na step that violates the speci.cation, and/or a predictive mechanism to check whether future steps lead \nto a violation. Existing approaches using recovery mechanisms typically re\u00adquire user annotations to \nde.ne a corrective action to be taken when the speci.cation is violated. For example, software transactional \nmemory [23] is a special case of a recovery mechanism in which the user provides atomicity annotations \nde.ning atomic sections. The system then requires the absence of read/write con.icts, and if this property \nis violated, the execution of an atomic section is restarted. Other examples include Tolerace [19] which \ncreates local copies of variables to detect and recover from races, and ISOLATOR [21] which can recover \nfrom violations of isolation. Search-based Synthesis: In previous work [28, 29], we used a semi-automated \napproach for exploring a space concurrent garbage collectors and linearizable data-structures. The work \nused a search procedure and an abstraction speci.cally geared towards the safety property required for \nthe speci.c domain. In sketching [25, 26], the user provides a reference program of the desired implementation \nand some sketches which partially specify certain optimized functions. The sketching compiler auto\u00admatically \n.lls in the missing low-level details to create an optimized implementation. Sketching has been used \nfor bounded programs and in special cases of unbounded domains [24]. In [25], .nding a candidate solution \nis done using a counterexample-guided in\u00adductive synthesis (CEGIS) algorithm that uses a backing bounded\u00adchecking \nprocedure. Candidates are generated iteratively and run through the checker. Counterexamples are used \nto limit the next candidates to be generated. In contrast, rather than generating can\u00addidates and checking \nthem, in our approach, the synthesizer is part of the veri.cation procedure and is based on abstract \ninterpretation. Further, in contrast to sketching, which aims to .nd some solution for the sketch, we \nare interested in .nding a solution with minimal synchronization. Locks for Atomicity: There have been \nseveral works on inferring locks for atomic sections. In the work by McCloskey et. al. [17], a tool called \nAutolocker is presented. The tool takes as input a pro\u00adgram that has been manually annotated with (i) \natomic sections and  (ii) a mapping between locks and memory locations protected by these locks. Autolocker \nproduces a program that implements the atomic sections in (i) with the locks in (ii). Further work by \nEmmi et. al. [11] proposed a technique to automate part (ii) above. The actual assignment of locations \nto locks is solved as an optimization problem where the goal is to minimize the total number of locks \nwhile still achieving minimum interference between the computed locks. The latest work of Cherem et. \nal. [6] proposes another alter\u00adnative to automate (ii) while also computing actual lock placement in \nthe code. Our work is complementary to these approaches, as our focus is not on optimizing the implementation \nof atomic sections, but on inferring minimally atomic synchronization.  9. Conclusions and Future Work \nIn this paper, we presented a novel algorithm for the automatic synthesis of ef.cient synchronization \nin concurrent in.nite-state programs (AGS). The synchronization can be realized by modify\u00ading either \nthe program or the scheduler. Our algorithm is based on abstract interpretation and thus applies to concurrent \nin.nite-state programs. The AGS algorithm leads to a new veri.cation approach: it allows for both the \nabstraction and the program to be modi.ed simultaneously until the abstraction is precise enough to verify \nthe (modi.ed) program. This enables veri.cation of a program in cases where it would have otherwise failed. \nWe implemented the AGS approach in a prototype tool named GUARDIAN,andsuccessfullyapplied ittoseveralsmallbut \ninterest\u00ading concurrent programs. GUARDIAN works with various numerical abstractions. In the future, \nwe intend to investigate its use with .ner abstract domains, such as the trace partitioning domain [22], \nwhich is a natural .t for our setting, as it allows to abstract interleavings with varying degrees of \nprecision. We demonstrated our approach using atomic sections as the syn\u00adchronization primitive, but \navoid and implement can be real\u00adized using other synchronization primitives. In the future, we intend \nto explore extensions of AGS to other synchronization primitives. The AGS algorithm described in this \npaper can also be applied in a dynamic setting, where invalid interleavings are obtained by running the \nprogram driven by test-cases. In such a setting, the constraints obtained from dynamic executions can \nbe used to give the user partial program corrections, or used to limit the space that has to be explored \nstatically.  Acknowledgement The authors wish to thank Mooly Sagiv for many insightful com\u00adments on \nan earlier version of this work.  References [1] The SAT4J SAT solver. available at http://www.sat4j.org/. \n[2] ATTIE,P., AND EMERSON, E. Synthesis of concurrent systems for an atomic read/atomic write model of \ncomputation. In PODC 96 (1996), ACM, pp. 111 120. [3] BALL,T., AND RAJAMANI, S. K. Automatically validating \ntemporal safety properties of interfaces. In SPIN (2001), pp. 103 122. [4] BLANCHET,B., COUSOT,P., COUSOT,R., \nFERET,J., MAUBORGNE,L.,MINE\u00b4,A., MONNIAUX,D., AND RIVAL, X. A static analyzer for large safety-critical \nsoftware. In PLDI (2003), pp. 196 207. [5] BLOEM,R.,CHATTERJEE,K., HENZINGER,T., AND JOBSTMANN, B. Better \nquality in synthesis through quantitative objectives. In CAV (2009), pp. 140 156. [6] CHEREM,S., CHILIMBI,T., \nAND GULWANI, S. Inferring locks for atomic sections. In PLDI (2008), pp. 304 315. [7] CLARKE,E., AND \nEMERSON, E. Design and synthesis of synchro\u00adnization skeletons using branching-time temporal logic. In \nLogic of Programs, Workshop (1982), pp. 52 71. [8] CLARKE,E. M.,GRUMBERG,O., JHA,S.,LU,Y., AND VEITH, \nH. Counterexample-guided abstraction re.nement. In CAV (2000), pp. 154 169. [9] CLARKE,JR., E., GRUMBERG,O., \nAND PELED,D. Model Check\u00ading. The MIT Press, 1999. [10] COUSOT,P., AND COUSOT, R. Abstract interpretation: \nA uni.ed lat\u00adtice model for static analysis of programs by construction of approxi\u00admation of .xed points. \nIn POPL (1977), pp. 238 252. [11] EMMI,M.,FISCHER,J. S., JHALA,R., AND MAJUMDAR,R. Lock allocation. In \nPOPL (2007), pp. 291 296. [12] GRIESMAYER,A., BLOEM,R. P., AND COOK, B. Repair of boolean programs with \nan application to C. In CAV (2006), pp. 358 371. [13] HENZINGER,T. A., JHALA,R., MAJUMDAR,R., AND SUTRE,G. \nLazy abstraction. In POPL (2002), pp. 58 70. [14] JOBSTMANN,B., GRIESMAYER,A., AND BLOEM, R. Program \nrepair as a game. In CAV (2005), pp. 226 238. [15] JOBSTMANN,B., STABER,S., GRIESMAYER,A., AND BLOEM,R. \nFinding and .xing faults. Journal of Computer and System Sciences (JCSS) (2008). [16] MANNA,Z., AND WOLPER, \nP. Synthesis of communicating pro\u00adcesses from temporal logic speci.cations. ACM Trans. Program. Lang. \nSyst. (TOPLAS) 6, 1 (1984), 68 93. [17] MCCLOSKEY,B., ZHOU,F., GAY,D., AND BREWER,E. Au\u00adtolocker: synchronization \ninference for atomic sections. In POPL (2006), pp. 346 358. [18] MINE\u00b4, A. The octagon abstract domain. \nHigher Order Symbol. Comput. 19, 1 (2006), 31 100. [19] NAGPALY,R.,PATTABIRAMANZ,K.,KIROVSKI,D., ANDZORN,B. \nTolerace: Tolerating and detecting races. In STMCS: Second Workshop on Software Tools for Multi-Core \nSystems (2007). [20] PNUELI,A., AND ROSNER, R. On the synthesis of a reactive module. In POPL 89 (New \nYork, NY, USA, 1989), ACM, pp. 179 190. [21] RAJAMANI,S., RAMALINGAM,G., RANGANATH,V.-P., AND VASWANI, \nK. Controlling non-determinism for semantic guarantees. In Exploiting Concurrency Ef.ciently and Correctly \n (EC)2 (2008). [22] RIVAL,X., AND MAUBORGNE, L. The trace partitioning abstract domain. ACM Trans. Program. \nLang. Syst. 29, 5 (2007), 26. [23] SHAVIT,N., AND TOUITOU, D. Software transactional memory. In PODC \n95 (New York, NY, USA, 1995), ACM, pp. 204 213. [24] SOLAR-LEZAMA,A., ARNOLD,G., TANCAU,L.,BOD\u00b4IK,R., \nSARASWAT,V. A., AND SESHIA, S. A. Sketching stencils. In PLDI (2007), pp. 167 178. [25] SOLAR-LEZAMA,A., \nJONES,C. G., AND BODIK, R. Sketching concurrent data structures. In PLDI (2008), pp. 136 148. [26] SOLAR-LEZAMA,A., \nRABBAH,R. M., BOD\u00b4IK,R., AND EBCIOGLU, K. Programming by Sketching for Bit-Streaming Pro\u00adgrams. In PLDI \n(2005), pp. 281 294. [27] STABER,S., JOBSTMANN,B., AND BLOEM, R. Finding and .xing faults. In CHARME \n(2005), pp. 35 49. [28] VECHEV,M., AND YAHAV, E. Deriving linearizable .ne-grained concurrent objects. \nIn PLDI (2008), pp. 125 135. [29] VECHEV,M.T.,YAHAV,E.,BACON,D. F., AND RINETZKY,N. Cgcexplorer: a semi-automated \nsearch procedure for provably correct concurrent collectors. In PLDI (2007), pp. 456 467. [30] VECHEV,M. \nT., YAHAV,E., AND YORSH, G. Inferring synchroniza\u00adtion under limited observability. In TACAS (2009), \npp. 139 154.  \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We present a novel framework for automatic inference of efficient synchronization in concurrent programs, a task known to be difficult and error-prone when done manually.</p> <p>Our framework is based on abstract interpretation and can infer synchronization for infinite state programs. Given a program, a specification, and an abstraction, we infer synchronization that avoids all (abstract) interleavings that may violate the specification, but permits as many valid interleavings as possible.</p> <p>Combined with abstraction refinement, our framework can be viewed as a new approach for verification where both the program and the abstraction can be modified on-the-fly during the verification process. The ability to modify the program, and not only the abstraction, allows us to remove program interleavings not only when they are known to be invalid, but also when they cannot be verified using the given abstraction.</p> <p>We implemented a prototype of our approach using numerical abstractions and applied it to verify several interesting programs.</p>", "authors": [{"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P1911106", "email_address": "", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P1911107", "email_address": "", "orcid_id": ""}, {"name": "Greta Yorsh", "author_profile_id": "81315492721", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P1911108", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706338", "year": "2010", "article_id": "1706338", "conference": "POPL", "title": "Abstraction-guided synthesis of synchronization", "url": "http://dl.acm.org/citation.cfm?id=1706338"}