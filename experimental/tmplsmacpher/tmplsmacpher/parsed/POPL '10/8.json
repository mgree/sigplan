{"article_publication_date": "01-17-2010", "fulltext": "\n A Veri.ed Compiler for an Impure Functional Language Adam Chlipala Harvard University, Cambridge, MA, \nUSA adamc@cs.harvard.edu Abstract We present a veri.ed compiler to an idealized assembly language from \na small, untyped functional language with mutable references and exceptions. The compiler is programmed \nin the Coq proof assistant and has a proof of total correctness with respect to big\u00adstep operational \nsemantics for the source and target languages. Compilation is staged and includes standard phases like \ntranslation to continuation-passing style and closure conversion, as well as a common subexpression elimination \noptimization. In this work, our focus has been on discovering and using techniques that make our proofs \neasy to engineer and maintain. While most programming language work with proof assistants uses very manual \nproof styles, all of our proofs are implemented as adaptive programs in Coq s tactic language, making \nit possible to reuse proofs unchanged as new language features are added. In this paper, we focus especially \non phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect \nhas been a key challenge area in past compiler veri.cation projects, with much more effort expended in \nthe statement and proof of binder-related lemmas than is found in standard pencil\u00adand-paper proofs. We \nshow how to exploit the representation tech\u00adnique of parametric higher-order abstract syntax to avoid \nthe need to prove any of the usual lemmas about binder manipulation, of\u00adten leading to proofs that are \nactually shorter than their pencil-and\u00adpaper analogues. Our strategy is based on a new approach to encod\u00ading \noperational semantics which delegates all concerns about sub\u00adstitution to the meta language, without \nusing features incompatible with general-purpose type theories like Coq s logic. Categories and Subject \nDescriptors F.3.1 [Logics and meanings of programs]: Mechanical veri.cation; D.2.4 [Software Engineer\u00ading]: \nCorrectness proofs, formal methods; D.3.4 [Programming Languages]: Compilers General Terms Languages, \nVeri.cation Keywords compiler veri.cation, interactive proof assistants 1. Introduction Mechanized proof \nabout programming languages is rather new as an engineering discipline. Only a handful of real world \nprojects have been undertaken with users beyond computer science and mathematics researchers. Still, \nprojected practical applications un\u00adderlie most recent work. For example, compiler veri.cation holds \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c . 2010 ACM 978-1-60558-479-9/10/01. . . $10.00 the \npromise of dramatically reducing the costs of quality assur\u00adance in the development, evolution, and maintenance \nof compil\u00aders. Unfortunately, this sort of veri.cation seems today to require epic investments of time \nand cleverness by experts in semantics and theorem-proving. The main message of this paper is that, as \nin more familiar software development, compiler veri.cation admits design patterns that cut down dramatically \non the required grunt work, to the point where it seems plausible that the use of veri.cation can actually \nreduce the overall effort required to build a correct com\u00adpiler, even when the baseline we compare against \nis almost-correct compilers where copious testing has found all but the most obscure bugs. There has \nbeen much important research on verifying compilers for relatively low-level languages like C, including \nin the veri.ed language stack project by Moore (1989) and the more recent Com\u00adpCert project (Leroy 2006). \nIn that domain, languages researchers generally start projects and discover that, when they pick the \nwrong abstractions and proof structuring principles, veri.cation requires much more work than they expected. \nIn contrast, when picking the wrong abstractions in mechanized proofs about languages with nested variable \nbinders (such as most functional languages), the same researchers often .nd themselves buried in details \nthat they thought of as irrelevant. We have heard many stories of knowledge\u00adable semanticists outright \ngiving up on these kinds of proofs. Recently, there has been much progress in research on the repre\u00adsentation \ntechniques that minimize the chances of such defeats. The use of higher-order abstract syntax (HOAS) \nin Twelf (Pfenning and Sch\u00a8 urmann 1999) remains a popular choice, though it seems that a majority of \nlanguages researchers prefer interactive proof assistants that, unlike Twelf, can automate large parts \nof proofs. The de Bruijn index representation (de Bruijn 1972) is another old standard that sees wide \nuse today. Perhaps the most popular methodologies now center around the nominal logic package for Isabelle/HOL \n(Urban and Tasson 2005) and the Penn approach to locally nameless bind\u00ading in Coq (Aydemir et al. 2008). \nWith these techniques, many facts about variable freshness and term well-formedness remain present explicitly \nin proofs, but there are standard recipes for .guring out the right lemmas to prove and when to apply \nthem. These recipes make it likely that a user with enough persever\u00adance will manage to prove his theorem. \nThis is a great improvement over the situation of just a few years ago, but, in this paper, we ar\u00adgue \nthat we should be striving for more. In software engineering, we focus on maximizing programmer productivity, \nand we believe that mechanized proof engineering could stand to see a similar fo\u00adcus. Most parts of most \nproofs about practical programming lan\u00adguages are exercises in stepping through many cases that are proved \nin unenlightening ways, with a handful of cases representing the core insights of a proof. Unfortunately, \nmost mechanized proofs still spend signi.cant amounts of code on the uninteresting cases, with signi.cant \neffort expended to write that code. When it comes time to change a theorem statement, say because a language \nhas been extended with a new construct, the user needs to go back over all of his very manual proofs, \nediting and adding cases. This is especially tedious with traditional manual proofs in Coq, where proofs \nare completely unstructured series of commands that modify proof states. Declarative proof languages \nlike Isabelle s Isar (Wen\u00adzel 1999) help alleviate much of this complication, but they do so arguably \nat the expense of greater verbosity of proofs and greater expenditure in building the .rst version of \na formal development. Is there an even more effective means of structuring proof scripts, such that we \ncan realize evolvability bene.ts similar to those that software engineers have come to expect? In the \ncourse of this paper, we hope to convince the reader that the answer is yes. We will describe our experience \nbuilding a veri.ed compiler for an impure functional language in Coq. Three main contributions underly \nthe implementation. We apply the parametric higher-order abstract syntax tech\u00adnique (Chlipala 2008) \non a larger, more realistic compiler veri\u00ad.cation case study than in previous work. This encoding lets \nus avoid any code dealing with name freshness or index rearrange\u00adment in our compiler pass implementations, \nand that simplicity makes it easier to write correctness proofs.  To avoid the usual deluge of lemmas \nabout substitution, we use a new approach to encoding operational semantics. Substitution does not appear \nexplicitly and is instead delegated to the meta language, as in the classical HOAS approach, but in a \nway compatible with general-purpose type theories like Coq s.  Each of our Coq proof scripts is a program \nthat is able to adapt to changes to language de.nitions and theorem state\u00adments. Such proof scripts express \nwhat are, in our opinion, the real essences of why theorems are true, the insights that could stump someone \ncoming at the proofs from scratch.  Our approach is a synergistic combination of lightweight repre\u00adsentations \nand aggressive theorem-speci.c automation. We imple\u00admented a .rst version of our compiler for a source \nlanguage miss\u00ading several of the features from the .nal version: let expressions, constants, equality \ntesting, and recursive functions. We were able to add these features after-the-fact with minimal alterations \nof and additions to our proof scripts; the extended proofs do not even men\u00adtion the new syntactic constructs \nor the operational semantics rules that govern them. Some of the ideas we present here can be mapped \nback into alternative ways of doing things in pencil-and-paper semantics, but, at the level of detail \nthat is traditional in venues like POPL, our techniques would probably only increase proof length and \ncomplexity. Instead, this paper is focused on how to engineer a veri.ed compiler for a functional language. \nThe trickiest parts of doing this with a proof assistant turn out to have little relation to the trickiest \nparts of doing it on paper. Representations matter a lot, and proof structuring techniques have a serious \nimpact on how easy it is to evolve a veri.ed compiler over time. Past projects have considered verifying \ncompilers for pure func\u00adtional languages. As far as we are aware, ours is the .rst to consider a functional \nsource language with either of mutable references or exceptions. On paper, these features seem straightforward \nto add to a compiler proof. In a proof assistant, when using the most straight\u00adforward proof techniques, \nimpurity infects all of the main theorems and lemmas. It seems a shame to pass up the opportunity to \nauto\u00admate the .ow of these details through our proofs, and we do our best to take advantage of the possibility. \n 1.1 The Case Study Our compiler operates on programs in a kind of untyped Mini-ML, as shown in Figure \n1. We have constants from some unspec- Constants c Variables x, f Expressions e ::= c | e = e | x | e \ne | .x f (x). e | let x = e in e | () | .e, e. | fst(e) | snd(e) | inl(e) | inr(e) | case e of inl(x) \n. e | inr(x) . e | ref(e) | !e | e := e | raise(e) | e handle x . e Figure 1. Source language syntax \nRegisters r ::= r0 | ... | rN-1 Constants n . N Lvalues L ::= r | [r + n] | [n] Rvalues R ::= n | r | \n[r + n] | [n] + Instructions I ::= L := R | r = n | L := R = ? R | jnz R, n Control-.ow instructions \nJ ::= halt R | fail R | jmp R Basic blocks B ::= (I * ,J) Programs P ::= (B * ,B) Figure 2. Target assembly \nlanguage syntax i.ed base types, comparable with a primitive equality operation; recursive functions; \nlet-binding; unit values; products; sums; mu\u00adtable references; and exceptions. This language is meant \nto cap\u00adture the key features of core ML s dynamic semantics, omitting es\u00adsential features only when they \ninvolve variable numbers of argu\u00adments or variable binding structure. In particular, we do not model \nvariable-arity products and sums, mutually-recursive functions, or compound pattern matching. Our target \nlanguage is the idealized assembly language shown in Figure 2. It differs from a real assembly language \nin representing words with natural numbers and in supporting an in.nite memory bank of words. There is \nstill a .nite supply of N registers. Our particular compiler works for any N = 3, allocating some vari\u00adables \nto the additional registers when possible. An assembly pro\u00adgram consists of a list of basic blocks with \none distinguished basic block where execution begins. A basic block is a sequence of in\u00adstructions terminated \nby a control-.ow instruction. The supported varieties of instruction are assignment using different addressing \nmodes (where [\u00b7] operands denote memory accesses), increment of a register by a constant, equality comparison, \nand conditional jump based on whether a value is nonzero. Control-.ow instructions in\u00adclude halt, for \nnormal program termination; fail, for termination on an uncaught exception; and jmp, the standard computed \ngoto. Each halt or fail instruction takes an additional program result code as an argument. The destination \noperands to jnz and jmp are inter\u00adpreted as indices into the program s list of basic blocks. The compiler \nis idealized in another important way. Unlike in our past work on a compiler for basic lambda calculus \n(Chlipala 2007), there is no interface with a garbage collector. The output as\u00adsembly programs allocate \nnew memory but never free any memory. As our present focus is on reasoning about nested binders, we leave \nthe low-level treatment of memory management to future work. We give the source language a standard big-step \noperational semantics. Figure 3 shows a sampling of the rules. We de.ne a separate syntactic class of \nvalues (associated with the metavariable v) in the usual way, taking a restriction of the syntax of expressions \nand adding a form ref(n), standing for an allocated reference cell (h, .x f(x).e) . (h, Ans(.x f(x).e)) \n(h1,e1) . (h2, Ans(.x f(x).e)) (h2,e2) . (h3, Ans(e .)) (h3,e[f .. .x f(x).e][x .. e .]) . (h4,r) (h1,e1 \ne2) . (h4,r) (h1,e1) . (h2, Ex(v)) (h1,e1 e2) . (h2, Ex(v)) (h1,e1) . (h2, Ans(.x f(x).e)) (h2,e2) . \n(h3, Ex(v)) (h1,e1 e2) . (h3, Ex(v)) (h1,e) . (h2, Ans(v)) (h1, ref(e)) . (v :: h2, Ans(ref(|h2|))) \n(h1,e) . (h2, Ans(ref(n))) h2.n = v (h1, !e) . (h2, Ans(v)) (h1,e) . (h2, Ans(v)) (h1, raise(e)) . (h2, \nEx(v)) (h1,e1) . (h2, Ex(v)) (h2,e2[x .. v]) . (h3,r) (h1,e1 handle x . e2) . (h3,r) Figure 3. Sample \nrules from source language semantics at numerical address n. The basic judgment is (h1,e) . (h2,r), where \nh1 and h2 are reference cell heaps represented as lists of values, e is the expression to evaluate, and \nr is the result, which is either Ans(v) for normal termination with expression result v or Ex(v) when \nv was raised as an exception and not caught. There are 38 rules in total, covering all of the points \nin an expression s evaluation where an exception might be raised. We write |h| for the length of a list \nh. Our assembly language also has a big-step operational seman\u00adtics. There are many equivalent ways of \nformalizing such a seman\u00adtics; the speci.cs that we chose will not matter in what follows. The overall \njudgment is of the simple form P . h, r, where h is the .nal heap and r is either halt(n) or fail(n). \nOur .nal theorem says that compiled programs have the same observable behavior as their corresponding \nsource programs. With our limited languages, a program can only exhibit one of three kinds of observable \nbehavior: halting, failing, or diverging. The the\u00adorem we prove in this case study ignores non-termination. \nOur the\u00adorem is enough to translate facts about terminating source programs into facts about assembly \nprograms, which is the main kind of veri\u00ad.cation of interest for a deterministic source language without \nI/O. We plan eventually to strengthen the .nal theorem by applying co\u00adinductive big-step operational \nsemantics (Leroy and Grall 2009) to prove that divergent source programs are also mapped to divergent \nassembly programs. We write .e. for the compilation of expression e. Stating the .nal theorem requires \nformalizing the contract between the com\u00adpiler and the programmer. The compiler writer agrees to follow \ncer\u00adtain data layout conventions, but it is useful to leave some aspects of representation unspeci.ed, \nto avoid unnecessary restrictions on h . .x f (x).e = n = n = n~h . () ~h . ref(n) ~. ~~ h . v1 = h[n] \nh . v2 = h[n + 1] h . c ~h . (v1,v2) ~ = .c. = n ~~ h[n]=0 h . v = h[n + 1] h[n]=1 h . v = h[n + 1] h \n. inl(v) ~h . inr(v) ~ = n = n Figure 4. The compiler s data layout contract optimization. We chose \nto give a full speci.cation for all kinds of data besides functions and references. Other decisions are \npossible, with minimal impact on the proofs. A much more speci.c relation underlies our main inductive \nproofs, and we arrive at the visible contract simply by forgetting some details of the real relation. \n~ The data layout contract is speci.ed as a relation h . v = n, pa\u00adrameterized by the .nal assembly-level \nheap h and relating source\u00adlevel value v to assembly-level word n. Figure 4 gives the details. We overload \nthe notation .c. to denote the compilation of a source\u00adlevel constant into a word. Our development is \nparameterized over an arbitrary injective function of this kind. We lift this relation in the natural \nway to apply to program results. ~~ h . v = nh . v = n h . Ans(v) ~= halt(n) h . Ex(v) ~= fail(n) Now \nour main theorem can be stated succinctly. THEOREM 1 (Semantic correctness of compilation). For any \nsource program e, heap h, and result r, if (\u00b7,e) . (h, r), then there exist h. and r . such that .e.. \nh. ,r . and h. . r ~. . = r  1.2 Outline In the next section, we review the higher-order syntax represen\u00adtation \nscheme that we introduced in prior work. In Section 3, we present a new substitution-free approach to \nencoding operational semantics. Section 4 describes, with discussion of their correct\u00adness proofs, the \nmain phases of our compiler: conversion from .rst\u00adorder to higher-order syntax, CPS translation, closure \nconversion, translation to three-address code, and assembly code generation. Section 5 discusses our \nveri.ed optimizations: common subexpres\u00adsion elimination, dead code elimination, and register allocation. \nSection 6 gives some statistics about our implementation, Section 7 compares with related work, and we \nconclude in Section 8. The case study that we describe in this paper is included in the directory examples/Untyped \nof the latest release of our Lambda Tamer library for compiler veri.cation in Coq, available at http://ltamer.sourceforge.net/ \n 2. Parametric Higher-Order Abstract Syntax In this section, we summarize key elements of our past work \non representing programming language syntax. The following section presents new material that is crucial \nfor scaling up to more realistic languages. To formalize reasoning about languages with nested variable \nbinders, one needs to settle on a binding representation. Such de\u00adtail is often swept under the rug in \npencil-and-paper proofs. Taking many informal presentations literally, we arrive at concrete repre\u00adsentations \nlike the one embodied in this Coq datatype de.nition for the abstract syntax of untyped lambda calculus. \n Inductive exp : Type := | Var : string -> exp | App : exp -> exp -> exp | Abs : string -> exp -> exp. \nA type de.nition like this one does not implement usual conven\u00adtions on its own. At a minimum, we need \nsome well-formedness judgment characterizing when an expression is free of dangling variable references. \nThis means that each of our proofs must in\u00adclude extra premises characterizing which expressions are \nwell\u00adformed with respect to which variable environments. If our formal\u00adization requires a notion of capture-avoiding \nsubstitution, we need to de.ne one manually. We must also prove a fair number of ex\u00adtra lemmas about \nwell-formedness and substitution. These lemmas must have different proofs for different object languages. \nThere are other so-called .rst-order representation schemes that alleviate this burden somewhat, including \nthe de Bruijn index, nom\u00adinal, and locally nameless styles that we mentioned earlier. Sig\u00adni.cant extra \nreasoning about freshness and/or well-formedness remains. An alternative is to use higher-order abstract \nsyntax (HOAS) (Pfenning and Elliot 1988), which represents object lan\u00adguage binders using meta language \nbinders. This pseudo-Coq def\u00adinition captures the way we revise concrete syntax to arrive at HOAS. Inductive \nexp : Type := | App : exp -> exp -> exp | Abs : (exp -> exp) -> exp. For instance, we represent an application \nof the identity func\u00adtion to itself with App (Abs (fun x => x)) (Abs (fun x => x)). We encode the matching-up \nof binders with their uses by bor\u00adrowing our meta language s facility for that kind of matching with \nanonymous functions. We called this de.nition pseudo-Coq because Coq will not accept it. An inductive \ntype is not allowed to be de.ned with a con\u00adstructor that takes as input a function over the same type. \nAllow\u00ading this would be problematic because it would allow the coding of non-terminating programs, even \nwithout use of explicit recur\u00adsive de.nitions, by taking advantage of the opportunity to write exotic \nterms that do not correspond to real lambda terms. Since Coq follows the Curry-Howard Isomorphism in \nidentifying proofs with functional programs, non-termination corresponds to logical inconsistency, where \nany theorem can be proved spuriously with an in.nite loop. Systems like Twelf avoid this problem by using \nweaker meta languages like LF (Harper et al. 1993) that crucially omit features like pattern-matching \nand recursion. Even in general-purpose functional programming languages, HOAS terms are dif.cult to deconstruct \nmanually. It is not gen\u00aderally possible to go under a binder, since languages like ML and Haskell provide \nno way to introspect into closures at runtime. Washburn and Weirich (2008) proposed a technique for .xing \nsome of these de.ciencies by taking advantage of parametric polymor\u00adphism. Guillemette and Monnier (2008) \nshowed how the technique can be combined with generalized algebraic datatypes to do static veri.cation \nof compiler type preservation in GHC Haskell. Washburn and Weirich s encoding still is not accepted literally \nby Coq, but a small variation achieves similar bene.ts. In past work (Chlipala 2008), we showed how to \nuse parametric higher\u00adorder abstract syntax (henceforth abbreviated PHOAS ) to for\u00admalize the syntax \nand semantics of programming languages. We were able to construct very simple, highly-automated proofs \nof the correctness of some transformations on functional programs. Here is the syntax of lambda calculus \nreformulated in PHOAS. Section var. Variable var : Type. Inductive exp : Type := | Var : var -> exp \n| App : exp -> exp -> exp | Abs : (var -> exp) -> exp. End var. Definition Exp : Type := forall var \n: Type, exp var. We use Coq s section mechanism to scope a local variable over a de.nition. Outside of \nthe section, the exp type becomes a type family parameterized by a choice of var type. This de.nition \nis accepted by Coq, and the crucial difference from HOAS is that a binder is represented as a function \nover variables, rather than over expressions. This satis.es Coq s positivity constraint for inductive \nde.nitions. Now the identity function can be written as Abs (fun x => Var x), with some choice of var \n.xed globally. Considering just the part of the above code inside the section, we are using the encoding \nknown as weak HOAS (Honsell et al. 2001). If we treat the type var as an unknown, Coq s type system ensures \nthat every exp corresponds to a real lambda term, since it is not possible for a function over variables \nto do anything interesting based on its input values, which in effect come from an abstract type. The \nparametric part of PHOAS comes in treating var as more than just a global unknown. We de.ne our .nal \nexpression representation type Exp such that an expression is a .rst-class polymorphic function from \na choice of var to an exp that uses that choice. For instance, the .nal PHOAS form of the identity function \nis fun var => Abs var (fun x : var => Var var x). The parametricity of the meta language makes this scheme \nequivalent to treating var as a global constant, but we gain the ability to instantiate Exps with particular \nvar choices to help us write particular functions. For instance, we can implement capture-avoiding substitution \nlike this: Section flatten. Variable var : Type. Fixpoint flatten (e : exp (exp var)) : exp var := match \ne with | Var e => e | App e1 e2 => App (flatten e1) (flatten e2) | Abs e1 => Abs (fun x => flatten (e1 \n(Var x))) end. End flatten. Definition Exp1 := forall var : Type, var -> exp var. Definition Subst (E \n: Exp1) (E : Exp) : Exp := fun var => flatten (E (exp var) (E var)). First, we write a function flatten \nthat .attens an expression where variables are themselves represented as expressions. Every variable \nis replaced by the expression that it holds. We recurse inside an Abs constructor by building a new argument \nfor Abs that itself calls flatten. The recursive call is on the original function body applied to a particular \nlocally-bound variable. We can use flatten to implement substitution easily. We de\u00ad.ne Exp1, the PHOAS \ntype of an expression with one free vari\u00adable. Substitution of E in E is implemented with an anonymous \npolymorphic function over a var choice. For a particular var, we instantiate the one-hole expression \nE to represent variables as ex\u00adpressions and the substitutand E to represent variables with var. Applying \nthe former to the latter, we arrive at an expression whose .attening is the proper result of substitution. \n In past work (Chlipala 2008), we showed how to use PHOAS to give denotational semantics to statically-typed, \nstrongly-normalizing functional languages. The basic trick is to parameterize variables by types and \nde.ne a type denotation function that can be used as a variable representation in implementing the expression \ndenotation function. Using this encoding, we implemented and proved totally correct a number of common \nfunctional language compiler passes. These proofs usually rely on the fact that values of types like \nExp are really parametric. We formalized this property in terms of a judgment that axiomatizes equivalence \nbetween expressions that use different variable representations. This judgment for untyped lambda calculus \nis G . e1 ~ e2 as de.ned below. Where ei rep\u00adresents variables in vari, G is a context of pairs in var1 \n\u00d7 var2. We write #x for the Var constructor applied to x and .f for Abs applied to f. as the one surprise. \nUnfortunately, inductive proofs over paramet\u00adric terms tend to involve just as much administrative overhead \nas we .nd with .rst-order representations. Dealing with instantiated terms (e.g., in exp) frees us to \nleave variables deep within syntax trees annotated with arbitrary meta language values. If we work with \nparametric terms, we must instead represent and apply con\u00adtexts explicitly in our induction hypotheses, \nsince it is impossible to go under a binder without .rst .xing a var choice. Moreover, working with substitution \nexplicitly brings back the same family of lemmas about the interaction of substitution and other functions. \nGenerally we must prove at least one lemma about substitution for each program transformation function \nthat we write. The details of such lemmas are almost always elided in pencil-and-paper proofs, but we \nmust prove them in full detail to satisfy a proof assistant. The alternative that we use here is to de.ne \nan operational se\u00ad (x1,x2) . G G . e1 ~ e 1 G . e2 ~ e 2 mantics over instantiated terms that avoids \nmentioning substitution 1 e2 explicitly. Our encoding has the .avor of a hybrid between a high\u00ad G . \n#x1 ~ #x2 G . e1 e2 ~ e .x1,x2. G, (x1,x2) . f1(x1) ~ f2(x2) G . .f1 ~ .f2 A parametric expression E \nis well-formed if, for any var1 and var2, we have \u00b7. E(var1) ~ E(var2). We conjecture that this statement \nholds true for any E of the proper type, and we asserted that fact as an axiom in our past work. In the \ncase study of this paper, we instead prove that expressions are well-formed as needed. Future theoretical \nwork that proved the consistency of this family of axioms would remove the need for specialized well-formedness \nproofs. In proving the correctness of a program transformation, it is generally the case that we use \none variable choice in evaluating a source program and a different variable choice in translating the \nprogram. We use the well-formedness of the expression to derive that the two instantiated expressions \nare equivalent. Proofs then tend to proceed by induction or inversion on these concrete well\u00adformedness \nderivations. At this point, the reader may want to accuse us of mislead\u00ading advertising, since earlier \nwe complained that .rst-order repre\u00adsentations require too much bookkeeping about well-formedness. The \nkey difference with PHOAS is that (we conjecture) every expression is well-formed by construction. We \nmaterialize well\u00adformedness proofs only as needed, and we never need to prove PHOAS analogues of common \nlemma schemas like substitution, weakening, and permutation. In this case study, we proved well\u00adformedness \nmanually where needed as a kind of due diligence, but we anticipate that the theory will eventually be \nin place to rest easily assuming axioms of universal well-formedness. In any case, PHOAS avoids the need \nto generate fresh names or rearrange ex\u00adisting names in implementing a wide variety of transformations. \nThese administrative operations are the bane of programming and proving with .rst-order representations. \n3. Substitution-Free Operational Semantics level semantics and an abstract machine, where we track closure \nallocation explicitly. We want to write semantics equivalent to ex\u00adamples like the one in Figure 3. For \nbasic lambda calculus, it is tempting to start out by de.ning a type val of values like the fol\u00adlowing, \nsuch that we can instantiate var as val in our semantics. Inductive val : Type := | VAbs : (val -> exp \nval) -> val. This de.nition suffers from the same problem as our earlier HOAS pseudo-de.nition: we try \nto de.ne val in terms of functions over itself. Coq rejects the de.nition as ill-formed, which is a good \nthing, because otherwise we would be able to implement a lambda calculus interpreter in Coq, which gives \nus a trivial way of coding an in.nite loop and thus breaking logical consistency. Our solution is to \nrepresent values as natural numbers that index into a heap of closures, or meta language functions from \nvalues to expressions. The technique bears a resemblance to approaches to making allocation explicit \nin operational semantics, e.g., as in Morrisett et al. (1995). That line of work aims to capture how \nhigh\u00adlevel programs execute on real machines, while keeping at the right level of abstraction. In contrast, \nour use of explicit allocation is aimed at removing the need to reason about explicit substitution. What \nwe have here is really just an instance of a common pattern in semantics of moving to more explicitly \nsyntactic techniques to circumvent circularities in type de.nitions. Definition val : Type := nat. Definition \nclosure : Type := val -> exp val. Definition closures : Type := list closure. Here is a big-step semantics \nin this style for basic lambda calculus. Its Coq type signature could be closures -> exp val -> closures \n-> val -> Prop. (H, #v) . (H, v)(H, .f) . (f :: H, |H|) (H1,e1) . (H2,n)(H2,e2) . (H3,v) H3.n = f (H3,f(v)) \n. (H4,v ) Our past work gives programs semantics by interpretation into (H1,e1 e2) . (H4,v Coq s strongly-normalizing \nlogic CIC; thus, that work cannot be ) applied directly to Turing-complete programming languages like \nour source and target languages here. The main representational in\u00adnovation of our new work is an effective \nway of writing operational semantics over PHOAS terms. Operational semantics has proved its worth in \nthe formalization of a wide variety of languages, so our new encoding expands the effective range of \nPHOAS dramatically. It is tempting to write operational semantics directly over para\u00admetric terms (e.g., \nin Exp from the last section). Doing so is actually fairly straightforward, with the trick for implementing \nsubstitution As with HOAS and its relatives in general, we manage to del\u00adegate the object-language-speci.c \nhandling of substitution to the meta language. This delegation happens in the occurrence of f (v) in \nthe application rule. The rule for variables is also more interest\u00ading than it may appear at .rst. By \nevaluating a variable node #v to its content v, we effectively push the operation of last section s flatten \nfunction into our semantics. The trickiness of usual substitution stems from the need to reason about \nnested binder scopes. We have replaced that kind of reasoning with global reasoning about a closure heap. \nWe can prove a relatively small set of lemmas about lists and reuse it to handle closure heaps in all \ndevelopments that use our encoding. There is no need to prove even a single lemma about substitution \nupon starting with a new object language or transformation. Our technique generalizes to the full source \nlanguage from Fig\u00adure 1. We revise our val de.nition like this, using the illustrative type synonym label \nfor nat from our library: Inductive val : Type := | VFunc : label -> val | VUnit : val | VPair : val \n-> val -> val | VInl : val -> val | VInr : val -> val | VRef : label -> val. The main PHOAS semantics \nfor the source language tracks input and output versions of both a closure heap and a reference heap. \nWe reuse our library of list lemmas to reason about both kinds of heaps. The de.nitions above are about \nexpressions specialized to rep\u00adresent variables as values, but it is now easy to de.ne an evaluation \nrelation for parametric terms. (\u00b7,E(val)) . (H. ,v) E . v We have modi.ed standard operational semantics \nby adding a level of indirection. In a traditional paper proof at the tradi\u00adtional level of detail, our \nchange would only add bureaucratic has\u00adsle. Counterintuitively, the change reduces hassle in mechanized \nproofs, since it helps us delegate to the meta language some details of processing the object language. \nThe substitution-free encoding is more than just a trick to place PHOAS on a level playing .eld with \n.rst-order representations. PHOAS with our new semantic encoding compares very favorably with other known \ncombinations of syntactic and semantic encod\u00adings. As far as we are aware, every competing technique \nis either invalid in a general-purpose proof system or leads to proof over\u00adhead signi.cantly above that \nin our implementation. In verifying all of our translations that represent syntax solely with PHOAS, \nthere is not a single lemma establishing one of the standard object\u00adlanguage-speci.c syntactic properties. \nWe only once use proof by induction over the structure of programs, and that occurs for an auxiliary \nlemma relevant to closure conversion, where explicit rea\u00adsoning about variables is hard to avoid. Substitution-free \noperational semantics has much in common with environment semantics, where an evaluation judgment takes \nan additional input which assigns a value to each free variable of the expression to evaluate. Both approaches \ninvolve explicit .rst\u00adorder treatment of an aspect of evaluation that is implicit in the more common \nvarieties of natural semantics. Where environment semantics treats every variable in a .rst-order way, \nsubstitution\u00adfree semantics does the same with closures. We have found the latter to have serious advantages \nfor proof engineering. None of our translations includes more than one case that has any interesting \neffect on closure allocation sequence. As a result, none of our proofs includes more than one case that \nmust compensate for the effect of such a change on semantics. In contrast, with environment semantics, \nalmost every case of each of our translations would need some accounting for a rearrangement of variable \nbinding structure. Theorem-speci.c proof automation is one of the core tech\u00adniques in our approach to \ncompiler veri.cation, and we will have more to say later on the details of that automation. While .rst-order \nencodings of operational semantics usually mention substitution explicitly, the standard lemmas about \nsubstitution tend to admit simple automated proof strategies. Nonetheless, we feel it is still a signi.cant \nburden to have to state and apply all of these lemmas explicitly. Perhaps an automation package could \ngo further and ap\u00adply substitution properties automatically as needed. In the project described in this \npaper, we have avoided any automatic application of induction, which rules out proofs of the usual syntactic \nlemmas. We view our more elementary automation style as a mark in favor of our proposal. More standard \noperational semantics with explicit substitution is easier to understand and believe, so we chose to \nstate the .nal theorem independently of the new technique. The .rst part of the next section shows how \nwe convert from .rst-order to higher-order syntax and semantics, along with how we justify the soundness \nof the conversion. 4. Main Compiler Phases In this section, we walk through our compiler intermediate \nlan\u00adguages and the phases that translate into them. We will overload notation by writing .\u00b7. for the \ncompilation function being discussed in each subsection. For lack of space, we discuss our proof automation \ntechniques only in the context of CPS translation, in Section 4.2.2. The design patterns introduced there \napply equally well to the other transla\u00adtions. 4.1 PHOASi.cation Our .nal theorem is stated entirely \nin terms of standard encodings of syntax and semantics, with no mention of PHOAS. We achieve this by \nbeginning our compiler with a de Bruijn index (de Bruijn 1972) implementation of the syntax from Figure \n1. The .rst com\u00adpiler phase translates these programs into PHOAS equivalents, where the PHOAS syntax \nis a different encoding of Figure 1. It is not generally possible to cheat in implementing a trans\u00adlation \ninto PHOAS, in the sense that there is no default value to use for variables when it turns out that the \ninput program is ill-formed somehow. Therefore, we use dependent types to enforce that every source program \nis closed by construction. This is a standard tech\u00adnique in the dependent types world, where a type exp \nis indexed by a natural number expressing how many free variables are avail\u00adable. Variables in ASTs are \nrepresented in types fin n, which are isomorphic to sets of natural numbers below n. Our implementation \nof PHOASi.cation uses another standard dependent type family, which we call ilist in our library. For \na type T and a natural number n, an ilist T n value is a length-n list of values in T. The type of the \nmain translation is forall (var : Type) (n : nat), Source.exp n -> ilist var n -> Core.exp var. Besides \nthe expression to translate, .\u00b7. takes in a list repre\u00adsenting a mapping from de Bruijn indices to PHOAS \nvariables. Here are some representative cases from the function s de.nition, where we write s.f for the \nprojection from the ilist s of the value at the position indicated by fin value f. We write . for the \nmeta language s function abstraction. .#x. s = #(s.x) .e1 e2. s = .e1. s .e2. s ..x f(x).e1. s = .f. \n.x( .x. .e1. (x :: f :: s)) In the source language de.nition, we simplify the de.nition of substitution \nby de.ning a type of values and including an exp constructor that allows the injection of any value into \nany type exp n. Thus, the closed nature of a value is apparent from its type, so we avoid needing to \nlift de Bruijn indices in the process of substituting a value in an open term. 4.1.1 Correctness Proof \nWe de.ne two mutually-inductive relations, for characterizing the compatibility of source and PHOAS expressions \nand values. Both relations are parameterized by PHOAS-level closure heaps, and the expression relation \nis also parameterized by an ilist, like in the de.nition of the translation. Here are a few representative \ncases. We write $ for the constructor that injects source values into source expressions, and we write \nH . H. for the fact that H is a suf.x of H.. We use the notation .x F for PHOAS-level application of \nthe AST constructor for recursive functions. H . s.f . v H,s . e1 . e . 1 H, s . e2 . e . 2 Primops p \n::= c | x = x | .x f(x).e | () |.x, x.| fst(x) | snd(x) | inl(x) | inr(x) | ref(x) | !x | x := x Expressions \ne ::= halt(x) | fail(x) | xx | let x = p in e | case x of inl(x) . e | inr(x) . e Figure 5. CPS language \nsyntax .#x. kS kE = kS (x) H, s . #f . #v H,s . e1 e2 . e .. .raise(e). kS kE = x k. e; kE (x) 1 e2 \nkk EE k .let x = e1 in e2. kS kE = x . e1; .e2. kS kE.f,x. H,x :: f :: s . e . F (f)(x) H . v . v . \nH, s . .x f(x).e . .x F H,s . $v . #v . .e1 e2. kS kE H.n = f .x1,x2,H..H . H. . H. ,x2 :: x1 :: \u00b7. e \n. f(x1)(x2) H . Fix(e) . Fix(n) H . v1 . v . H . v2 . v . 12 H . Pair(v1,v2) . Pair(v1. ,v 2. ) We \nprove that both relations are monotone, with respect to replacing a heap H with another heap H. such \nthat H . H.. We also lift the value relation to apply over results Ans(\u00b7) and Ex(\u00b7) in the natural way. \nThere are two main lemmas behind the correctness theorem for this phase. First, we prove that the compilation \nfunction respects expression compatibility. LEMMA 1. For any e and s with compatible type indices, if \ne contains no uses of $, then \u00b7,s . e ..e. s. From this starting point, we track the parallel execution \nof e and its compilation. We use a notion of reference heap compatibility H . h . h., which says that \nh and h. have the same length and that their values belong pairwise to H .\u00b7.\u00b7. LEMMA 2. If: (h1,e) . \n(h2,r) at the source level,  And H, s . e . e . ,  And H . h1 . h. 1,  Then there exist H. , h. 2, \nand r . such that: (H, h. 1,e .) . (H.,h2. ,r .),  And H. . r . r . ,  And H. . h2 . h. 2.  Lemma \n2 appeals to an auxiliary lemma about substitution. We note that this is the only place in our development \nwhere a substitution theorem is proved explicitly. These lemmas together yield the .nal theorem directly. \nTHEOREM 2. If (\u00b7,e) . (h, r) and e is closed and does not use $, then there exist H, h., and r . such \nthat (\u00b7, \u00b7, .e.\u00b7) . (H, h. ,r .) and H . r . r . .  4.2 Conversion to Continuation-Passing Style The \n.rst main compiler phase translates programs into continuation\u00adpassing style. Functions no longer return, \nexplicit exception han\u00addling constructs are eliminated, and expression evaluation is broken up with sequences \nof let bindings of the results of primitive oper\u00adations on variables. Figure 5 shows the syntax of the \ntranslation s target language. = f E E . e1; x . e2; let k. = .r. kS (r) in S let kE = .r. kE(r) in .\u00b8 \nlet p . = kS . ,k. in E . .\u00b8 let p = x, p in fp Figure 6. CPS translation We use a higher-order one-pass \nCPS translation, in the style of Danvy and Filinski (1992). The type of the translation is forall var, \nCore.exp var -> (var -> CPS.exp var) -> (var -> CPS.exp var) -> CPS.exp var. Beyond the input expression, \nthe extra arguments are the current success continuation and the current exception handler, represented \nas meta language functions over result variables. Figure 6 shows some representative cases of the de.nition. \nWe write .x. e as shorthand for .x f (x).e when kE f does not occur free in e. We write x . e1; e2 as \nshorthand for .e1. ( .x. e2)(kE ). The application case demonstrates how the ef\u00adfective domain of each \ncore function is expanded to 3-tuples of a main argument, a success continuation, and an exception handler. \nThis compilation function takes as an argument a choice of var representation. In compiling a parametric \nexpression E, we return an abstraction over var, within which we call the concrete compilation function \nwith var and E(var) as arguments. We pass always-halt and always-fail functions as the success continuation \nand exception handler, respectively. 4.2.1 Correctness Proof As for the last phase, this correctness \nproof is based around a re\u00adlation between core and CPS values. Since both languages use PHOAS, the relation \nis parameterized by a closure heap for each. Here are some representative rules. For a meta language \nfunction f representing a function abstraction body, we write .f. for the way that the main compilation \ntranslates that body. The relation de.ni\u00adtion below depends on a version of the ~ relation from Section \n2, extended to apply to the input language. H.. H.n = f .n = .f.. (.x1,x 1,x2,x 2. G, (x1,x 1), (x2,x \n2) . f(x1)(x2) ~ f .(x1. )(x2. )) (.v, v . . (v, v .) . G . H, H. . v . v .) H, H. . Fix(n) . Fix(n \n.) H, H. . v1 . v1 . H, H. . v2 . v2 . H, H. . Pair(v1,v2) . Pair(v1. ,v 2. ) H, H. . Ref(n) . Ref(n) \n As in the last subsection, we lift this relation in the natural way to pairs of reference heaps and \npairs of results. Our main theorem is with respect to a substitution-free big-step semantics for CPS \nprograms. The signature is the same as for Core but with .nal heaps no longer speci.ed, since the .nal \nresult is all that we care about. THEOREM 3. If: (H1,h1,e) . (H2,h2,r) at the source level,  And G \n. e ~ e . ,  And H1,H1 . . h1 . h. 1,  And, for every (v, v .) . G, H1,H1 . . v . v . ,  Then for \nevery pair of continuations kS and kE, there exist H2. , h. 2, and r . such that: If r . = Ans(v) and \n(H2. ,h. 2,kS (v)) . r .. , then (H1. ,h1. , .e .. kS kE ) . r .. ,  And, if r . = Ex(v) and (H2. ,h. \n2,kE (v)) . r .. , then (H1. ,h1. , .e .. kS kE ) . r .. ,  And H1 . . H2. ,  And H2,H2 . . r . r \n. ,  And H2,H2 . . h2 . h. 2.  This theorem about expressions specialized to values-as-variables makes \nit easy to derive the theorem about parametric expressions when we substitute the initial success and \nexception continuations for kS and kE .  4.2.2 Automating the Proofs We begin by proving that each of \nour compatibility relations is monotone with respect to extension of closure heaps, which follows by \ninduction on derivations. We also give one-liner proofs for .ve more lemmas that massage obvious facts \ninto forms that Coq s automated resolution prover will be able to use. At that point, we are ready to \ntackle the proof of Theorem 3, which proceeds by induction on core evaluation judgments. Figure 7 gives \nthe complete proof script for this theorem, imple\u00admenting in Coq s domain-speci.c tactic language Ltac \n(Delahaye 2000). We will step through the different elements, remarking as appropriate on the design \npatterns they embody. The script begins with hints, which extend Coq s resolution prover, supporting \nhigher-order logic programming in the tradition of Prolog. The .rst hint suggests that proof search should \ntry each of the rules of the evaluation judgment for CPS-level primops. In this way, we avoid having \nto mention the rules explicitly, which makes it possible for the proof to keep working even after we \nadd new kinds of primops. Hint Constructors CPS.evalP. Next, we use the Hint Resolve command to suggest \nsome other rules and lemmas to be applied automatically during reso\u00adlution. Hint Resolve answer_Ans answer_Ex \n.... We use a Hint Extern command to specify a free-form proof search step. We give 1 as an estimate \nof the cost of this rule, which effects the order in which rules are attempted. After that, we write \na pattern to match against a goal. When the goal matches the pattern, we suggest running the proof script \nto the right of the arrow. In this case, our script suggests unfolding some de.nitions, so that we expose \nthe syntactic structure of an expression to evaluate, making it clear which operational semantics rules \napply. Hint Extern 1 (CPS.eval _ _ (cpsFunc _ _) _) => unfold cpsFunc, cpsFunc . Now we are ready for \nthe main body of the proof. We proceed by induction on the .rst hypothesis (H1,h1,e) . (H2,h2,r), and \nwe chain onto our use of induction a script to apply to every inductive case. The semicolon operator \naccomplishes this chaining, and we wrap the per-case script with abstract to prove every case as a separate \nlemma, which saves memory by freeing some temporary data structures after each lemma. induction 1; abstract \n(... We begin every case with an inversion on the expression equiva\u00adlence judgment G . e ~ e ., which \nis the .rst remaining hypothesis. inversion 1; Next, we call a generic simpli.cation tactic from our \nLambda Tamer library. This tactic knows nothing about any particular object language. It relies on a \nnumber of built-in Coq automation tactics and adds some new strategies, combining propositional simpli.\u00adcation, \npartial evaluation, resolution proving, rewriting, and com\u00admon rules for simplifying sets of hypotheses \ndealing with standard datatypes like natural numbers, lists, and optional values. simpler; At this point, \nthe top-level structure of the expressions appear\u00ading in a case is known, and we have gotten as far as \nwe can with generic simpli.cation. The next step is to begin a loop over some theorem-speci.c simpli.cation \nstrategies. Like other tactic\u00adbased proof assistants, Coq supports a number of tacticals, a kind of higher-order \ncombinators for assembling new proof strategies. We use the repeat tactical to structure our loop. The \nargument to repeat is a tactic to attempt repeatedly until it no longer applies. Our argument here uses \na match tactic expression, which general\u00adizes normal pattern matching in the tradition of ML and Haskell. \nA match tactic matches on the form of a proof goal, including both hypotheses and conclusion. repeat \n(match goal with Our heuristics are pattern-matching rules, where each pattern has the form HYPS |-CONC. \nThe HYPS section describes condi\u00adtions on hypotheses and CONC gives a pattern to match against the conclusion \nto be proved. The former section is a comma-separated list of zero or more entries of the form H:p, asserting \nthat there must exist some hypothesis matching pattern p and to whose name the local variable H should \nbe bound. The .rst heuristic looks for a hypothesis asserting some fact H, H. . v1 . v2. In our implementation, \nsuch a fact is written as H &#38; H |--v1 ~~ v2, using an ASCII notation that we register as a Coq syntax \nextension or macro. Thus, the .rst pattern matches any goal with a hypothesis over this judgment. For \neach such hypothesis, we apply the tactic invert 1 2 from our library. This tactic performs inversion \nif and only if it is possible to deduce from the form of the hypothesis that at most two distinct rules \nof the underlying judgment could apply. If more than two rules are possible, the tactic invocation fails, \ntriggering backtracking to try a different choice of H or, if that fails, the next rule in our match \nexpression. By using invert 1 2, we avoid having to specialize this heuristic to the details of our object \nlanguage, for instance by writing one heuristic per case where we can deduce that a particular rule of \n\u00b7, \u00b7.\u00b7.\u00b7 must have been used to conclude H. |[ H: _&#38;_|--_~~_|-_ ]=> invert_1_2H The next heuristic \nfollows the logic of the previous one, but for the judgment for result compatibility instead of value \ncompatibility, which has a different notation. |[ H: _&#38;_|---_ ~~_ |-_]=>invert_1H Hint Constructors \nCPS.evalP. Hint Resolve answer_Ans answer_Ex CPS.EvalCaseL CPS.EvalCaseR EquivRef . Hint Extern 1 (CPS.eval \n_ _ (cpsFunc _ _) _) => unfold cpsFunc, cpsFunc . induction 1; abstract (inversion 1; simpler; repeat \n(match goal with |[ H: _&#38;_|--_~~_|-_ ]=> invert_1_2H |[ H: _&#38;_|---_~~_ |-_]=>invert_1H | [ H \n: forall G e2, Core.exp_equiv G ?E e2 -> _ |-_ ] => match goal with |[ _: Core.eval ?S_E___, _ : Core.eval \n_ _ ?E ?S _ _, _ : forall G e2, Core.exp_equiv G ?E e2 -> _ |-_ ] => fail 1 | _ => match goal with |[ \nk: val->expV, ke : val -> exp val, _: _&#38; ?s|--_~~_, _ : context[VCont] |-_ ] => guessWith ((fun \n(_ : val) x => ke x) :: (fun (_ : val) x => k x) :: s) H | _ => guess H end end end; simpler); try (match \ngoal with | [ H1 : _, H2 : _ |-_ ] => generalize (sall_grab H1 H2) end; simpler); splitter; eauto 9 \nwith cps_eval; intros; try match goal with | [ H : _ &#38; _ |---_ ~~ ?r |-answer ?r _ _ ] => inverter \nH; simpler; eauto 9 with cps_eval end). Figure 7. Complete proof script for Theorem 3 The next and .nal \nheuristic from our main loop chooses when and how to apply induction hypotheses (IHes). The .rst step \nin that direction is to identify some hypothesis H that has the right syntactic structure to be an IH. \nOur Coq development uses the predicate Core.exp equiv for the judgment we write as G . e ~ e . in the \nstatement of Theorem 3, and the code ?E denotes a pattern variable. | [ H : forall G e2, Core.exp_equiv \nG ?E e2 -> _ |-_ ] => It would not be effective to apply the IHes in an arbitrary or\u00adder. Because of \nthe form of Theorem 3, each successful application yields an existentially-quanti.ed conclusion, and \neliminating those quanti.ers gives us new variables to work with. Those new vari\u00adables might be needed \nto instantiate the universal quanti.ers of a different IH. It turns out that a simple heuristic lets \nus choose the right IH order in every case: as each IH is associated with an expres\u00adsion, follow the \norder in which those expressions were evaluated in the original program. We can track evaluation order \nby inspecting the closure heaps that are threaded through evaluation. One evaluation comes after another \nif the former s starting closure heap equals the latter s ending heap. By clearing each IH as we use \nit, we make it possible to use the following pattern match to identify an IH that is not ready yet. In \nparticular, where the current H is for some expression E, there must exist another IH about some expression \nE , such that evaluation of E begins where evaluation of E leaves off, in terms of the .ow of a closure \nheap S. Thus, since E comes after E , and since we have not yet applied the IH for E , we are not yet \nready to apply the IH for E. We use the fail tactic to backtrack to making a different choice of H. \nmatch goal with | [_ :Core.eval?S_E___, _ : Core.eval _ _ ?E ?S _ _, _ : forall G e2, Core.exp_equiv \nG ?E e2 -> _ |-_] =>fail 1 If the last rule .nds no matches, then we know that H is the ap\u00adpropriate \nIH to apply now. We perform a further pattern-match to determine whether we need to apply an instantiation \nstrategy spe\u00adci.c to the case of function application, one of the few interesting cases of the translation. \nSince the general case is simpler, we will discuss it .rst. We simply apply the tactic guess, which comes \nfrom our Lambda Tamer library, to our IH. This generates a new uni.cation variable for every universal \nquanti.er in the statement of H. Additionally, we apply automatic resolution proving to dis\u00adcharge each \nhypothesis of H, in the process learning the values of most of the uni.cation variables that we just \nintroduced. For this theorem, uni.cation variables remain for the continuation variables kS and kE , \nbut the other uni.cation variables are determined im\u00admediately from context. By relying on the versatile \nguess tactic, we avoid almost all object-language-speci.c application of IHes. This is one of the key \ntechniques supporting proof reuse. | _ => guess H In most proofs, guess can handle the uninteresting \ncases that would not be written out in detail in a pencil-and-paper proof. Often a bit more help from \nthe human prover is needed for the cases that lie at the heart of a transformation s purpose. For CPS \ntranslation, the only such case is function application, and we use pattern matching to identify that \ncase and treat it specially. We use the pattern context[VCont] to require that some hypothesis mentions \na continuation value, which turns out to be enough to isolate the case of interest. We also bind local \nnames for the success continuation k and the exception handler ke. Finally, we pattern\u00admatch out the \nonly closure heap s that is mentioned in a value compatibility hypothesis. From these variables, we \ncan construct our piece of advice to guess. More speci.cally, we use the variant guessWith, which lets \nus suggest a value to be used to instantiate any universal quan\u00adti.er of proper type, such that the remaining \nquanti.ers are still in\u00adstantiated with fresh uni.cation variables. We know that each func\u00adtion call \nallocates a new continuation each for the success continu\u00adation and exception handler. The argument we \npass to guessWith re.ects that knowledge, suggesting a closure heap that has the two new continuations \npushed on. match goal with | [ k : val -> expV, ke : val -> exp val, _:_ &#38;?s|--_~~_, _ : context[VCont] \n|-_ ] => guessWith ((fun (_ : val) x => ke x) ::(fun (_:val)x=>kx)::s) H Each iteration of the main loop \nends with a call to simpler, which will take the existentially-quanti.ed conjunctions produced by guess \nand replace them with individual hypotheses that use fresh top-level variables. After we .nish this main \nloop of heuristics, most of the work in the proof is done. Simple resolution proving can handle most \nof the remaining goals. We use one additional pattern match to catch a case where it would be useful \nto add a new hypothesis justi.ed by the library theorem sall grab about heap well-formedness. try (match \ngoal with |[H1: _,H2:_|-_]=> generalize (sall_grab H1 H2) end; simpler); After that, we call the tactic \nsplitter to turn a goal like .x1,...,xn.f1 . ... . fm into separate goals f1, ..., fm, with each xi replaced \nby a fresh uni.cation variable. We solve most of these goals with a call to the resolution prover eauto, \nspecifying a proof tree depth of 9 and an additional hint database cps eval, which includes a rule to \napply as many CPS operational semantics rules as possible, counted as a single proof step. splitter; \neauto 9 with cps_eval; Each remaining goal is solved by case analysis on whether an unknown evaluation \nresult r is normal or represents an uncaught exception. More speci.cally, we .nd a hypothesis stating \na result compatibility fact, we perform inversion on that hypothesis, and we .nish off the resulting \ncases with standard tactics. try match goal with |[H :_ &#38;_ |---_~~?r |-answer ?r_ _] => inverter \nH; simpler; eauto 9 with cps_eval end). Our inductive proof has 38 cases to consider, with one for each \nsemantic rule. Many of these cases need this kind of further split on results of sub-evaluations. By \nusing automation to structure our proof script, we shield the human proof architect from the need to \nconsider these many cases individually.  4.3 Closure Conversion The next compiler phase combines the \ntraditional transformations of closure conversion, which changes all functions to take their Primops \np ...as in last language, minus .x... Expressions e ...as in last language... Programs P ::= e | let \nf = .x f(x). e in P Figure 8. Closed language syntax free variables as explicit arguments; and hoisting, \nwhich moves all function de.nitions to the top level of a program. Since we are using PHOAS, it is easiest \nto combine these phases into one, such that the closed nature of function de.nitions can be apparent \nsyntactically from the fact that they only appear at the top level of a program. Figure 8 shows the syntax \nof this translation s target language. As in our past work on closure conversion with PHOAS (Chli\u00adpala \n2008), this phase is interesting because we implement it by converting higher-order syntax to .rst-order \nsyntax, which is passed to a translation that again produces higher-order syntax. Given a parametric \nexpression E, we instantiate it like E(nat), choosing to represent variables as natural numbers. We also \nfollow a speci.c convention in how we use such a term, which has type CPS.exp nat. Our convention is \nisomorphic to the technique of de Bruijn levels, where bound variables that are not inside nested scopes \nhave level 0, the next binders inside these have level 1, and so on. Compared to the more common de Bruijn \nindices, this tech\u00adnique has the advantage that all occurrences of a given binder s variable use the \nsame level. Therefore, since PHOAS binders are represented as functions, we can descend into a binder \nsimply by calling its function with the appropriate number. We formalize this convention with a well-formedness \njudgment over CPS.exp nat. Here are the key rules in a restriction to un\u00adtyped lambda calculus. f <n \nx<n n +1 . f(n) wf n . fx wf n . .f wf Our closure conversion is dependently-typed, such that it takes \na well-formedness proof as input. We prove a theorem saying that, for any well-formed E, we have 0 . \nE(nat) wf; and we pass an invocation of this theorem in the initial call to the translation function. \nHere is the type of the main translation. forall (var : Type) (n : nat) (e : exp nat), wf n e -> (((env \nvar (freeVars n e) -> Closed.exp var) -> Closed.prog var) -> Closed.prog var The function freeVars calculates \nthe free variable set of an ex\u00adpression. When called like freeVars n e, it returns a length-n list of \nbooleans, indicating which variables up to n appear free in e. The type family env is parameterized by \nsuch a list. An env var fvs is a tuple of one var variable for each entry of fvs that is true. The main \ncomplexity in the translation type comes from a continuation argument. In translating an expression, \nthe form of that expression implies some top-level function de.nitions that we should add. It is critical \nthat none of these de.nitions mentions any local variables, or else we would have an ill-formed program. \nThus, in translating an expression, we bind its functions and then call a continuation which may bind \nadditional functions. That continua\u00adtion then, inside its new de.nitions, calls a sub-continuation with \nan environment giving values to all free variables. We hope that a few representative examples, as shown \nin Figure 9, make the protocol clear. We write wf arguments as f. We rely k( .fx. nfk = .s.let s. .halt(x). \nnfk = .s.halt(get xsf)) k( = fst(get f (.1(s)) (p1(f))) in let f. = snd(get f (.1(s)) (p1(f))) in . \n\u00b8 let p = s. , get x (.2(s)) (p2(f))in f . p) .let p in f. nfk = .kP . .p. n(p1(f))( .f(n). (n + 1)(p2(f))( \n.s. .kE .k( kP (.1(s)) ( .x. kE (x :: .2(s)))))) Figure 9. Representative cases of closure conversion \non a similar primop translation function whose exact type and de.nition we will not discuss further here. \nThere are a few unusual things going on in Figure 9. First, we manipulate well-formedness proofs f as \ndata. When we know from the structure of e that a proof f must be deduced from a rule with two premises, \nwe write pi(f) for the extraction of the ith premise s proof. For a function call fx, the two premises \nsay that the two variables f and x are both less than the current de Bruijn level n. Such less-than proofs \nmay be passed to the get function to enable extracting variables from an environment s. We must also \ndo similar splitting of environments. Several cases of the de.nition of freeVars are implemented by joining \nsets of free variables. When an environment s has a type based on the union of two sets, then the projections \n.1 and .2 translate into environments for those sets. This is always possible to do, since a union of \ntwo sets contains any binding required by either set alone. The case for .x f(x).e, omitted above, is \nwhere new function bindings are created. It relies on two auxiliary functions for packing environments \ninto tuples at closure formation sites and unpacking those tuples in function prologues. Free variable \ninformation is used to choose which variables to pack. 4.3.1 Correctness Proof Reasoning about layers \nof nested continuations is tricky, so we prove the correctness of our translation by de.ning an alternate \ntranslation that does not use continuations. The alternate transla\u00adtion is specialized to the operational \nsemantics of closed programs, where, as in the de.nition of val from Section 3, we represent function \nvalues with natural numbers pointing into a closure heap. By specializing the translation to the semantics, \nwe can refer di\u00adrectly to closures and closure heaps, since function addresses are generated in a predictable \nway. Here is the type of the alternate translation: forall (n : nat) (e : CPS.exp nat), Closed.closures \n-> wf n e -> Closed.closures * (env Closed.val (freeVars n e) -> Closed.exp Closed.val) A call to this \nfunction takes a current closure heap as input, and return values are pairs of extended closure heaps \nand functions from local variable environments to expressions. We did not just use this version as our \ninitial translation because it works with a program representation where types alone do not guarantee \nwell\u00adformedness; any variable might include an out-of-bounds func\u00adtion reference. It is fairly straightforward \nto prove a correctness theorem for the alternate translation. We need to prove about two dozen lem\u00admas \nabout operations on free variables, environments, and closure heaps. As for the last two phases, we de.ne \ncompatibility rela\u00adtions over the values and reference heaps of the source and tar\u00adget languages. With \nthese pieces in place, we can prove the overall correctness theorem with the usual induction on source \nevaluation derivations. A .nal lemma connects the two translations, proved by mutual structural induction \nover CPS expressions and primops.  4.4 Flattening After closure conversion, programs are already almost \nin the form of three-address code, the family of traditional low-level compiler intermediate languages. \nBeyond the use of structured control .ow with case expressions, the only serious difference is that closed \nprograms use let-binding of immutable variables instead of manip\u00adulation of mutable temporaries, of which \neach procedure in three\u00adaddress code has a .xed set. We make this connection precise by de.ning a .at \nlanguage and a translation into it. Every let-bound variable in an input function is assigned a distinct \ntemporary in the function s translation, and the we replace the recursive type of programs P with a simpler \ntype of pairs, where each pair consists of a list of .at functions and a .at main program expression. \nEvery variable reference in the source program becomes either a temporary or a numeric index into the \nglobal list of functions. Flattening works like closure conversion in instantiating PHOAS terms for use \nwith de Bruijn levels. As this translation returns us to .rst-order languages, its correctness proof \nrequires more lemmas about lists and maps, though most are independent of the set of lan\u00adguage constructs. \nThe main inductive theorems are comparable in complexity and proof organization to those for closure \nconversion. 4.5 Code Generation The .nal compiler pass translates .at programs into assembly lan\u00adguage. \nAt this stage, neither source nor target language is novel. We still prove every theorem with an adaptive \ntactic program, but the basic organization of theorems is as one would expect from re\u00adlated projects. \nOur translation uses one register as the heap limit pointer, incrementing it as products, sums, and references \nare al\u00adlocated. Another register doubles as the function argument register and as a place to stash short-lived \nvalues during double memory indirections. Finally, we reserve a register to store a recursive func\u00adtion \ns self pointer. The remaining N - 3 registers are used to store the .rst N - 3 temporaries of each procedure. \nThe remaining temporaries are stored in a global area at the beginning of mem\u00adory. Since we deal only \nwith compiling whole programs, it is easy to choose a size for this region by .nding the highest numbered \ntemporary used in a program. The correctness proof is comparable in complexity to those of previous phases, \nbut with signi.cantly more supporting lemmas. 5. Optimizations To better gauge how our approach scales \nto the algorithms used by real compilers, we also implemented and veri.ed two common optimization passes. \n5.1 Common Subexpression Elimination Between closure conversion and .attening, we perform intrapro\u00adcedural \ncommon subexpression elimination (CSE) on closed pro\u00adgrams. Since our languages have no intraprocedural \niteration con\u00adstructs, there is no need to perform data.ow analysis. Instead, a single recursive traversal \nof a program suf.ces. The optimization still simpli.es cases like application of a known function, where \nit is possible to avoid building a closure. As we descend into a program s structure, we maintain a \nmap\u00adping from variables to symbolic values, as de.ned below. Symbolic values s ::= #n | c | () |.s, s.| \ninl(s) | inr(s) Values not built from the basic constant, unit, product, and sum constructors are represented \nwith symbolic variables #n, where a fresh n is generated for each new input-program variable that cannot \nbe determined to have more speci.c structure. The purpose of CSE is to remove some redundant bindings \nand case analyses. This transformation may sound complicated enough to require conversion of input programs \nto .rst-order form to analyze them. However, it is possible to implement CSE in an elegant higher-order \nway. In translating a parametric program P , we must produce a CSE d version of it for each possible \nvariable representation var. Our solution is to do so by instantiating P at variable type var * sval, \nwhere sval is the type of symbolic values s. Thus, each variable is tagged with a symbolic representation, \nand this representation may be accessed directly at use sites. The main translation maintains a mapping \nfrom symbolic values to vari\u00adables. We use this mapping to simplify case expressions with dis\u00adcriminees \nthat we see statically are either inl or inr. When proceed\u00ading under a let binder, the translation evaluates \nthe bound expres\u00adsion symbolically. If the result is in the map, we avoid creating a new binder in the \ntranslation. Instead, we apply the binder body, which is a function over variable/value pairs, to the \nvariable that our map associates with the appropriate symbolic value, paired with that value. If the \nvalue we are binding is not found in the map, we do create a new binder, and, in the recursive call inside \nthe binder s scope, we add the new variable to the symbolic map. The main correctness theorem for this \ntranslation is proved very similarly to the main theorem for CPS conversion. The proof can be a bit simpler \nbecause we need no value compatibility relation; CSE has no effect on the values that appear during program \nevaluation. We prove the main theorem with about 20 lines of tactic code for performing appropriate case \nanalyses, applying IHes and a lemma about primops, and materializing known facts about variables men\u00adtioned \nin expression equivalence derivations.  5.2 Combined Register Allocation and Dead Code Elimination In \na single pass performed between .attening and code generation, we combine register allocation and dead \ncode elimination. Code generation automatically assigns the lowest-numbered temporaries to registers. \nThus, the task of register allocation is simply to minimize the number of temporaries that each procedure \nuses, by .nding opportunities to combine several mutually non-interfering temporaries into one. We use \nliveness information to calculate in\u00adterference graphs. As with CSE, the lack of intraprocedural itera\u00adtion \nmakes it possible to compute an interference graph in a single traversal of procedure syntax. We use \nthe same liveness informa\u00adtion to eliminate useless assignments to temporaries. Our implementation makes \nuse of the .nite set and map support in Coq s standard library. We represent liveness information with \nsets of temporaries, interference graphs with sets of unordered pairs of temporaries, and temporary reassignments \nwith maps from tem\u00adporaries to temporaries. Coq s library contains functors that build such structures \nfrom modules describing keys, and each functor output contains a set of standard theorems about its data \nstructure. On top of this, we also implement and use an abstract data struc\u00adture for temporary sets whose \ncomplements are .nite, for use in choosing new names for temporaries. As with code generation and many \nother kinds of low-level rea\u00adsoning, this correctness proof is built from many unsurprising lem\u00admas. \nBy relying on the standard theorems about sets and maps, we manage to avoid most proving that is not \nspeci.c to our transfor\u00admation. Component Total Proofs Source language 228 0 Core PHOAS language 266 \n2 PHOASi.cation 28 0 Correctness 390 138 Well-formedness 40 17 CPS language 279 18 CPS translation 94 \n0 Correctness 221 60 Well-formedness 39 12 Closed language 311 21 Closure conversion 303 13 Correctness \n652 238 Well-formedness 261 119 CSE 87 1 Correctness 228 80 Well-formedness 177 70 Flat language 108 \n0 Flattening 63 0 Correctness 524 156 Register allocation 201 49 Correctness 642 310 Assembly language \n105 0 Code generation 153 0 Correctness 1156 491 Overall compiler 13 0 Correctness 89 12 Total 6658 1807 \nFigure 10. Lines of code in different components  6. Statistics Figure 10 breaks down our development \nby the number of lines of code in each component. We include how many lines of code in each component \ncome from proofs, which counts literal proof scripts, resolution hints, and de.nitions of tactic functions. \nMore or less all of the remaining lines come from de.nitions of syntax and semantics, in the .les corresponding \nto languages; from compiler phase implementations, in their .les; or from theorem statements and auxiliary \nde.nitions, in Correctness and Well-formedness .les. Our development depends upon our Lambda Tamer Coq \nlibrary, which contains about 1500 lines of object-language-agnostic theo\u00adrems and tactics. We assert \ntwo axioms from the Coq standard li\u00adbrary: functional extensionality, which says that two functions are \nequal if they map equal inputs to equal outputs; and proof irrel\u00adevance for equality proofs, which says \nthat no equality proposi\u00adtion has more than one distinct proof. This pair of axioms has been proved on \npaper to be consistent with CIC. The Well-formedness components in Figure 10 deal with proofs that transformations \nproduce well-formed PHOAS terms from well-formed inputs. We conjecture that there are CIC-consistent \naxioms stating that all PHOAS terms are well-formed. If this were proved metatheoretically, then we could \nsafely omit the well\u00adformedness proofs. Our .rst .nished compiler was for our .nal source language mi\u00adnus \nlet expressions, constants, equality testing, and recursive func\u00adtions. We also proved a simpler version \nof the .nal correctness theorem, stating only that a compiled program exhibits the same binary success-or-failure \nresult as the original, ignoring details of returned values and thrown exceptions. As we added the enhance\u00adments \nneeded to reach the .nal version, we measured how much effort was required. 6.1 Strengthening the Main \nTheorem Since our source language is Turing-complete, the interesting as\u00adpects of compiler veri.cation \nmust be tackled even if the .nal the\u00adorem only distinguishes between two distinct classes of program \noutcome. Other distinctions may be modeled using tests within the object language. Thus, it was pragmatic \nfor us to develop our initial proofs using this simpli.cation. Later, we went back and adapted the proofs \nto allow us to prove Theorem 1 as we stated it earlier in the paper. This required updating the different \nobject languages so that halt and fail operations take parameters. We added or modi.ed about 100 lines \nof syntax and semantics. We also had to introduce the compiler data layout contract relation and its \nrelatives for the different translation phases, whose de.nitions added about 150 lines of unsurprising \ncode. Additionally, we added about 100 lines of theorem statements, one-liner automated proofs, and resolution \nhints for some new theorems about the contract relations. Beyond that, we modi.ed or added about 80 lines \nof theorem statements and proof script, with most added to deal with new ex\u00adistential quanti.cations \nover program result values. Many of these changes were improvements that occurred to us as we worked \non the upgrade, such that we would say that the changes belonged in the original compiler. Notably, the \ncorrectness proofs of the opti\u00admizations required no changes. We worked on this upgrade over the course \nof two days, with performance of the Coq proof assis\u00adtant being the primary limiting factor. Automation \nof large proofs frequently leads Coq to run out of memory or run for excessively long, and we spent more \ntime than we would like tuning our scripts to skirt these limitations on an old computer with modest \nresources. We believe that relatively straightforward improvements to Coq s proof engine would make this \nkind of upgrade quite reasonable to complete in well under a day of work. Even with the current state \nof the tools, the upgrade did not require us to add any proof code speci.c to a particular case of any \nof our inductive proofs. 6.2 Adding let Expressions Adding let expressions was relatively simple, since \ngeneral let only appears until CPS translation, and since let does not add a new category of runtime \nvalues. Thus, we extended the .rst two translations only and the syntax and semantics of the .rst two \nlanguages. We also added a let case to the expression compatibility relation in the PHOASi.cation correctness \nproof. These changes amounted to about 30 new lines of code, with no old lines modi.ed. All of our proofs \ncontinued working unchanged. We did not need to update a single theorem statement or proof script. The \nwhole process took under half an hour.  6.3 Adding Constants and Equality Testing Next, we added constants \nand equality comparison of constants, which impacts much more of the compiler and its proof. We added \nor changed about 100 lines de.ning syntax, semantics, and transla\u00adtions, and we added about 50 lines \nde.ning new rules for inductive relations used in the proofs. To adapt the old proofs, we had to change \nsome patterns to mention new constructors of datatypes, to placate Coq s limited support for inferring \nwhich datatype is being pattern-matched on. We proved one lemma about the encoding of constants, giving \nit a one-line proof and adding it as a hint, and we added one additional one-line hint that detects when \nconstants are being compared for equality and then performs case analysis on whether they are equal. \nThese proof changes amount to about 10 lines in total, and we also added 5 more lines to improve per\u00adformance \nin a way not related to constants. We spent about half a day on this extension, again with most of that \ntime spent waiting for slow proof search to .nish.  6.4 Adding Recursive Functions Replacing non-recursive \nanonymous functions .x. e with recur\u00adsive anonymous functions .x f(x).e turned out to be the most intensive \nchange. We added or modi.ed about 50 lines to account for additions to syntax, semantics, and translations, \nand we modi\u00ad.ed about 20 lines that de.ne function abstraction rules for further inductive relations. \nWe added or modi.ed about 350 lines of theorem statements and proofs. In each of the earlier phases of \nthe compiler, we mod\u00adi.ed at most one line of proof in a way that is really speci.c to recursive functions. \nThe remaining extra lines come from the fact that .x is our only construct that binds more than one variable \nat once. We had already proved a host of lemmas specialized to the case of one variable at a time, and \nwe needed to duplicate these lemmas, reusing their automated proofs without changes. Other al\u00adterations \nto theorem statements and tactics re.ected only the need to handle a new binding pattern. The exception \nto this trend was code generation, where a change to the calling convention triggered a fair amount of \nchurn in theorem statements. The full upgrade to .x support took us approximately one day. 7. Related \nWork Compiler veri.cation for .rst-order languages has a considerable history, but we will focus on reviewing \nrelated work in compil\u00ading functional languages. See the bibliography by Dave (2003) for pointers into \nthe traditional literature on .rst-order compilers. There have been a variety of investigations into \nverifying compil\u00aders for pure functional languages, drawing on several very different representation \nand proof techniques. Flatau (1992) described a partial veri.cation of a compiler from a subset of the \nNqthm logic to the .rst-order imperative language Piton, performed with the Nqthm prover. The proof was \nhighly au\u00adtomated, in the usual style of Nqthm and ACL2. Since the com\u00adpiler worked in a single pass \nand targeted a .rst-order language, it avoided most of the issues inherent in reasoning about rearranging \nvariable binders. Minamide and Okuma (2003) used Isabelle/HOL and Isar to build proofs of correctness \nfor CPS translations for bare-bones untyped lambda calculus, using concrete encoding of binders with \nvariable names. The simplest translation that they veri.ed (that of Plotkin) had a correctness proof \nof about 250 lines. Their proof for Danvy and Nielsen s translation took about 400 lines, and this .gure \ngrew to about 600 when they added let binding to the object language. Tian (2006) used Twelf with HOAS \nto prove CPS translation correctness for an untyped, pure mini-ML language without recur\u00adsion, which \nis subsumed within our case study source language. His comparable CPS translation theorem took about \n50 lines, in the usual, fully manual Twelf style. Tian s development uses a target language more specialized \nto his particular translation, featuring hardcoded binding of a second-class continuation with any function \nde.nition, rather than building this functionality on top of product types and .rst-class continuations. \nWe expect that the cost of writ\u00ading such proofs in Twelf becomes more apparent when source and target \nlanguages are less tightly coupled, so that some inductive proof cases must build proof trees of non-trivial \ndepth. In past work (Chlipala 2007), we veri.ed a compiler from basic simply-typed lambda calculus to \na target language similar to three\u00adaddress code. We used the dependent de Bruijn representation throughout \nthe early stages of the compiler, and we relied on a metaprogramming component to prove some of the standard \nbinder lemmas for us. By switching to PHOAS, we have avoided the need to state those lemmas explicitly. \nOur present compiler extends our past PHOAS-based work (Chlipala 2008) by treating impurity, recursion, \nand the rest of the compilation pipeline beyond CPS translation and closure conversion. Dargaye and \nLeroy (2007) used Coq to verify CPS translations for another mini-ML language encoded with de Bruijn \nindices. We believe that our CPS translation is comparable in functionality to their optimized translation, \nwith the exception that ours does not do tail call optimization. They give code size statistics for their \nCPS translation, broken into speci.cations and proofs. The proofs size for the dependencies of the optimized \ntransform correctness proof is 4287 lines, an order of magnitude more than the total size of our CPS \ncorrectness .le. The complexities of the languages treated by the two projects are not directly comparable; \nDargaye and Leroy include variable-arity recursive functions and datatype constructors, but they omit \nimpure features. Benton and Hur (2009) take a very different approach to com\u00adpiler veri.cation in Coq, \nstarting with a typed functional language (represented with de Bruijn indices) and using step-indexed \nlogical relations over types to establish correctness. Their compiler works in one pass, so the theorems \ninvolved are very different from those in the early phases of our compiler. Their development uses the \nusual Coq manual proof style and runs to about 4000 lines. Though their source language (basic lambda \ncalculus with recursive func\u00adtions) is less featureful than the source language of this paper, their \n.nal correctness theorem is more general than in any related work, as it facilitates sound linking with \ncode produced by different com\u00adpilers or written by hand. 8. Conclusion Mostly-manual interactive proving \nof theorems about program\u00adming languages and compilers can be a very engaging challenge, and it has been \na crucial tool in our efforts to .gure out the right abstractions for the job. Nonetheless, we do not \nbelieve that this style scales to real-world implementations of high-level languages. We think it is \nnot at all too early to be thinking about the tech\u00adniques that may some day be applied in such a setting. \nOur case study in this paper is a veri.ed compiler whose mechanized cor\u00adrectness proofs are tactic programs \nthat can adapt automatically to speci.cation changes. We believe strongly that, if compiler veri.\u00adcation \never goes mainstream, it will be done more like we propose here than like the styles more commonly employed \nby languages researchers. To avoid getting bogged down in administrative lemmas about binding, we exploit \nthe parametric higher-order abstract syntax encoding, along with a new way of using it to encode substitution\u00adfree \noperational semantics. As a result, some of our compiler phase correctness proofs are shorter even than \nwhat a diligent semanticist would write on paper. Despite our use of novel representations, our .nal \ntheorem is stated only in terms of established encodings and relies on no nonstandard axioms. We hope \nto expand our case study into a veri.ed compiler for core Standard ML. This requires adding a few more \nfeatures to the source language and implementing a (hopefully straightforward) elaboration from the of.cial \nabstract syntax of Standard ML. We would also like to de.ne typing judgments for each language and prove \na type preservation theorem for the .nal compiler, which could output Typed Assembly Language. We conjecture \nthat this extension would require little change to the semantic preservation theorems and proofs. It \nwould also be interesting to try to complete the end-to-end compiler picture with a veri.ed parser, type \ninfer\u00adence engine, and assembler. Acknowledgments We would like to thank Greg Morrisett, Ryan Wisnesky, \nand the anonymous referees for helpful feedback on earlier versions of this paper. This work was supported \nby a gift from Microsoft Research. References Brian Aydemir, Arthur Chargu\u00b4 eraud, Benjamin C. Pierce, \nRandy Pollack, and Stephanie Weirich. Engineering formal metatheory. In Proc. POPL, pages 3 15, 2008. \n Nick Benton and Chung-Kil Hur. Biorthogonality, step-indexing and com\u00adpiler correctness. In Proc. ICFP, \npages 97 108, 2009. Adam Chlipala. A certi.ed type-preserving compiler from lambda calculus to assembly \nlanguage. In Proc. PLDI, pages 54 65, 2007. Adam Chlipala. Parametric higher-order abstract syntax for \nmechanized semantics. In Proc. ICFP, pages 143 156, 2008. Olivier Danvy and Andrzej Filinski. Representing \ncontrol: A study of the CPS transformation. Mathematical Structures in Computer Science,2 (4):361 391, \n1992. Zaynah Dargaye and Xavier Leroy. Mechanized veri.cation of CPS trans\u00adformations. In Proc. LPAR, \npages 211 225, 2007. Maulik A. Dave. Compiler veri.cation: a bibliography. SIGSOFT Softw. Eng. Notes, \n28(6):2 2, 2003. Nicolas G. de Bruijn. Lambda-calculus notation with nameless dummies: a tool for automatic \nformal manipulation with application to the Church-Rosser theorem. Indag. Math., 34(5):381 392, 1972. \nDavid Delahaye. A tactic language for the system Coq. In Proc. LPAR, pages 85 95, 2000. Arthur D. Flatau. \nA Veri.ed Implementation of an Applicative Language with Dynamic Storage Allocation. PhD thesis, University \nof Texas at Austin, November 1992. Louis-Julien Guillemette and Stefan Monnier. A type-preserving compiler \nin Haskell. In Proc. ICFP, pages 75 86, 2008. Robert Harper, Furio Honsell, and Gordon Plotkin. A framework \nfor de.ning logics. J. of the ACM, 40(1):143 184, 1993. Furio Honsell, Marino Miculan, and Ivan Scagnetto. \nAn axiomatic ap\u00adproach to metareasoning on nominal algebras in HOAS. In Proc. ICALP, pages 963 978, 2001. \nXavier Leroy. Formal certi.cation of a compiler back-end or: programming a compiler with a proof assistant. \nIn Proc. POPL, pages 42 54, 2006. Xavier Leroy and Herv\u00b4e Grall. Coinductive big-step operational semantics. \nInf. Comput., 207(2):284 304, 2009. Yasuhiko Minamide and Koji Okuma. Verifying CPS transformations in \nIsabelle/HOL. In Proc. MERLIN, pages 1 8, 2003. J Strother Moore. A mechanically veri.ed language implementation. \nJ. Automated Reasoning, 5(4):461 492, 1989. G. Morrisett, M. Felleisen, and R. Harper. Abstract models \nof memory management. In Proc. FPCA, pages 66 77, 1995. F. Pfenning and C. Elliot. Higher-order abstract \nsyntax. In Proc. PLDI, pages 199 208, 1988. Frank Pfenning and Carsten Sch\u00a8System description: Twelf \na urmann. \u00admeta-logical framework for deductive systems. In Proc. CADE, pages 202 206, 1999. Ye Henry \nTian. Mechanically verifying correctness of CPS compilation. In Proc. CATS, pages 41 51, 2006. C. Urban \nand C. Tasson. Nominal techniques in Isabelle/HOL. In Proc. CADE, pages 38 53, 2005. Geoffrey Washburn \nand Stephanie Weirich. Boxes go bananas: Encoding higher-order abstract syntax with parametric polymorphism. \nJ. Funct. Program., 18(1):87 140, 2008. Markus Wenzel. Isar -a generic interpretative approach to readable \nformal proof documents. In Proc. TPHOLs, pages 167 184, 1999.   \n\t\t\t", "proc_id": "1706299", "abstract": "<p>We present a verified compiler to an idealized assembly language from a small, untyped functional language with mutable references and exceptions. The compiler is programmed in the Coq proof assistant and has a proof of total correctness with respect to big-step operational semantics for the source and target languages. Compilation is staged and includes standard phases like translation to continuation-passing style and closure conversion, as well as a common subexpression elimination optimization. In this work, our focus has been on discovering and using techniques that make our proofs easy to engineer and maintain. While most programming language work with proof assistants uses very manual proof styles, all of our proofs are implemented as adaptive programs in Coq's tactic language, making it possible to reuse proofs unchanged as new language features are added.</p> <p>In this paper, we focus especially on phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect has been a key challenge area in past compiler verification projects, with much more effort expended in the statement and proof of binder-related lemmas than is found in standard pencil-and-paper proofs. We show how to exploit the representation technique of parametric higher-order abstract syntax to avoid the need to prove any of the usual lemmas about binder manipulation, often leading to proofs that are actually shorter than their pencil-and-paper analogues. Our strategy is based on a new approach to encoding operational semantics which delegates all concerns about substitution to the meta language, without using features incompatible with general-purpose type theories like Coq's logic.</p>", "authors": [{"name": "Adam Chlipala", "author_profile_id": "81100341086", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P1911049", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706312", "year": "2010", "article_id": "1706312", "conference": "POPL", "title": "A verified compiler for an impure functional language", "url": "http://dl.acm.org/citation.cfm?id=1706312"}