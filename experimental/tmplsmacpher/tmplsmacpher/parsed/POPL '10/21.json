{"article_publication_date": "01-17-2010", "fulltext": "\n Counterexample-Guided Focus Andreas Podelski University of Freiburg podelski@informatik.uni-freiburg.de \n Abstract The automated inference of quanti.ed invariants is considered one of the next challenges in \nsoftware veri.cation. The question of the right precision-ef.ciency tradeoff for the corresponding pro\u00adgram \nanalyses here boils down to the question of the right treat\u00adment of disjunction below and above the universal \nquanti.er. In the closely related setting of shape analysis one uses the focus op\u00aderator in order to \nadapt the treatment of disjunction (and thus the ef.ciency-precision tradeoff) to the individual program \nstatement. One promising research direction is to design parameterized ver\u00adsions of the focus operator \nwhich allow the user to .ne-tune the fo\u00adcus operator not only to the individual program statements but \nalso to the speci.c veri.cation task. We carry this research direction one step further. We .ne-tune \nthe focus operator to each individual step of the analysis (for a speci.c veri.cation task). This .ne-tuning \nmust be done automatically. Our idea is to use counterexamples for this purpose. We realize this idea \nin a tool that automatically infers quanti.ed invariants for the veri.cation of a variety of heap\u00admanipulating \nprograms. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; \nF.3.1 [Logics and Meaning of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams; F.3.2 \n[Logics and Meaning of Programs]: Semantics of Programming Languages Program Analysis General Terms Algorithms, \nLanguages, Reliability, Veri.cation Keywords Data Structures, Quanti.ed Invariants, Predicate Ab\u00adstraction, \nAbstraction Re.nement, Shape Analysis 1. Introduction There is a considerable interest in the automated \nveri.cation of correctness properties of programs that implement or use heap\u00adallocated data structures \n[6, 10, 19 21, 23, 24, 27, 29, 31, 34, 42, 43, 45, 48 50, 56, 57]. The correctness properties of such \nprograms typically include quanti.ed assertions, e.g., assertions that describe the expected shape of \ndata structures such as the reference count for each internal object of the data structure is 1 and assertions \nthat describe the effect of data structure operations such as all objects stored in the data structure \nare properly initialized .The automated inference of quanti.ed invariants for the veri.cation of quanti.ed \nassertions is considered one of the next challenges in software veri.cation. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 10, January 17 23, 2010, Madrid, Spain. \nCopyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . $10.00 Thomas Wies EPFL and University of \nFreiburg thomas.wies@ep..ch The eternal quest for the right precision/ef.ciency tradeoff in the corresponding \nprogram analyses here boils down to the ques\u00adtion of the right treatment of disjunction below and above \nthe uni\u00adversal quanti.er (speci.cally in the construction of the abstract transformer). To achieve a \nreasonable average tradeoff between precision and ef.ciency, existing program analyses follow a com\u00admon \nrecipe (see, e.g., [19, 42, 48]). The designer of the program analysis starts off with a coarse but ef.cient \ngeneric abstract trans\u00adformer and then manually adapts this abstract transformer to the individual kinds \nof program statements such that disjunctions are introduced prudently. In the closely related setting \nof shape analy\u00adsis this adaptation is referred to as the focus operation [48]. If one has to adapt the \nabstract transformer to individual program state\u00adments, uniformly for all possible uses of the analysis, \nthe designer of the program analysis is obliged to be very conservative with re\u00adgard to the precision \nof the focus operator. As a consequence, the analysis is often too inef.cient. It is therefore desirable \nto .ne-tune the focus operator to a speci.c use of the analysis. A promising research direction is to \ndesign parameterized focus operators that allow the user of the analysis to (manually) do this .ne-tuning \nherself, for each new veri.cation task; the resulting gain of scalability is encouraging; new classes \nof heap-manipulating programs, even concurrent ones, have been successfully analyzed [39 41]. In this \npaper, we carry this research direction one step further (and perhaps to its logical extreme). We propose \nto .ne-tune the focus operator not only to each individual problem instance, but to each individual step \nof the analysis, i.e., each application of the abstract transformer. This .ne-tuning must be done automatically. \nOur idea is to use counterexamples for this purpose. The contri\u00adbution of this paper is to conceptually \nand practically realize the idea and to demonstrate its interesting potential. We present a new method \nand tool that automatically infers quanti.ed invariants for the veri.cation of a variety of heap-manipulating \nprograms. The general idea of using counterexamples for re.ning an abstraction stems from the classical \nscheme of counterexample\u00adguided abstraction re.nement (CEGAR) [3, 14, 25]. A number of software veri.cation \ntools based on the scheme, e.g., [3, 12, 26, 44], are able to synthesize expressive invariants (though, \nnot quanti.ed ones, so far). A spurious counterexample is an error trace that is possible in the abstract \nbut not in the concrete. Adding predicates extracted from a spurious counterexample re.nes the abstract \ndo\u00admain; in a subsequent step, the scheme re-de.nes the abstract trans\u00adformer (as a function on the new \nabstract domain). In contrast, the focus operator re.nes the effect of the abstract transformer without \nchanging the abstract domain and without re-de.ning the function. We propose to use spurious counterexamples \nfor both: for re.ning the abstract domain, and for .ne-tuning a focus operator that adapts the effect \nof the abstract transformer. More precisely, we transcend the idea of lazy abstraction [25], a CEGAR \nscheme which adapts the abstract domain exactly to the point of the execution of the analyzed program. \nWe devise a nested lazy abstraction re.nement loop that adapts both, the abstract do\u00admain and the abstract \ntransformer exactly to the point. The re.ne\u00adment loop consists of two nested re.nement steps. The .rst \nstep re.nes the abstract domain by extracting new predicates from the spurious error trace. If a spurious \nerror trace is not eliminated by merely re.ning the abstract domain then the second step uses the spurious \nerror trace to construct a focus operator that adapts the ef\u00adfect of the abstract transformer on the \ncurrent abstract domain. This adaptation can thus be performed locally and lazily, i.e., anew for each \nsingle application of the abstract transformer. In passing, let us note that our development of the \ncounter\u00adexample-guided focus was originally motivated by the goal to es\u00adtablish the so-called progress \nproperty [25]. The property means that every spurious counterexample encountered during the anal\u00adysis \nis eventually eliminated by a re.nement step. The property is of foremost theoretical interest (a priori, \nit need not improve the chances of convergence in practice since there may be always a new spurious counterexample). \nIn the setting of quanti.ed invariants, however, the theoretical interest of counterexample-guided focus \nis in line with its practical relevance. The progress property holds (and only holds) in the presence \nof counterexample-guided focus. The practical veri.cation does succeed with counterexample-guided fo\u00adcus \nand does not without; the reason is apparently that in many benchmarks, a uniformly precise abstract \ntransformer with feasible cost comes with a too low precision. Summary. To summarize, our work leverages \nthe research on the CEGAR scheme in software veri.cation [3, 12, 14, 25]and the research on the focus \noperator in shape analysis [39 41, 48]. Our contribution is to show that the techniques developed in \nthese two research directions can be fruitfully integrated to enhance one another for the inference of \nquanti.ed invariants: the focus operator can be made effective in a CEGAR setting because it can be \n.ne-tuned lazily and its locality can be driven to the extreme,  the CEGAR scheme can be made effective \nfor quanti.ed in\u00advariants because adding an inner re.nement loop for the focus operator provides the \nprogress property and the precision re\u00adquired on practical examples (without the otherwise prohibitive \ncost for a precise abstract transformer on an abstract domain for quanti.ed assertions).   2. Motivating \nExample In this section, we will use an example to motivate the counter\u00adexample-guided focus and explain \nit in more detail. The program INIT shown in Fig. 1 initializes all entries in a singly-linked list. \nThe assert statement at location f2 checks that all entries in the list are indeed initialized after \ntermination of the while loop. We would like to automatically compute an inductive invariant for the \nloop cutpoint at location f1 that implies the safety of this assertion. In the remainder of this section, \nwe will .rst present the ab\u00adstract domain, then the most precise abstract transformer (which is too costly \nto implement), then a less costly abstract transformer (which is too coarse), and .nally the transformer \nobtained by the counterexample-guided focus. Abstract Domain: Boolean heaps. We use a variation of predi\u00adcate \nabstraction [22] which we call Boolean heap abstraction [45]. Here we do not use state predicates (which \ncan be de.ned by closed .rst-order formulas as, e.g., in the assert statement at location f2). Instead, \nwe use predicates that range over objects in the heap (and which can be de.ned by formulas with free \n.rst-order variables). These predicates are reminiscent of the predicates used in three\u00advalued shape \nanalysis [48] and indexed predicate abstraction [31]. Figure 2 shows three such predicates for Program \nINIT. The predi- C0 : y:= x C1 : while y=null do y.data:= 0 y:= y.next C2 : assert (.v. (x, v) .next \n* .v=null .v.data=0)  Figure 1. Program INIT Cont = {v |(x, v) .next * .v=null } Iter = {v |(y, v) .next \n* .v=null } Init = {v |v.data =0 } Figure 2. Predicates for Program INIT denoting sets of objects cates \nrange over objects in the heap. For notational convenience we write predicates as sets. For instance, \nthe predicate Cont denotes the content of the list pointed to by x, i.e., the set of all non-null objects \nthat are reachable from x by following next pointers in the heap. Data structure .elds such as next and \ndata are modeled as function symbols. The binary relation next * denotes the re.exive transitive closure \nof the function next. Given a program state (i.e., a valuation of next and data as functions), the .nite \nset of predi\u00adcates induces a partition of the heap into .nitely many equivalence classes of heap objects. \nThe abstract domain of Boolean heaps consists of formulas that describe such partitions. Each formula \nin the abstract domain is a disjunction of universally quanti.ed Boolean combinations of the given predicates. \nThe abstract domain is .nite. We call the outer disjuncts in these formulas abstract states. For instance, \nthe formula F given by F =.v. (v . Iter . v . Cont) . (v . Cont . v . Iter . v . Init) is an abstract \nstate for the predicates in Fig. 2. An abstract state is a special case of an element of the abstract \ndomain (a disjunction with only one disjunct). We use elements of the abstract domain to express inductive \ninvariants. For example, the formula F is an inductive invariant for location f1 in program INIT and \nimplies that the assert statement at location f2 does not fail. Most Precise Abstract Transformer. For \nthe purpose of this expo\u00adsition, we represent the most precise abstract transformer using an abstract \nprogram over abstract states; see Fig. 3. The abstraction of the concrete transformer for each basic \nblock in the concrete program is translated to a statement in the abstract program. The abstract program \nhas a program variable for each of the given pred\u00adicates (carrying the same name, e.g., Iter, Cont). \nThe program variables in the abstract program range over sets of objects. Figure 3 shows the Boolean \nheap abstraction of program INIT for the predicates given in Fig. 2. Here * stands for the non\u00addeterministic \nchoice of a Boolean value. The statement havoc x stands for the nondeterministic assignment of program \nvariable x. In the example, for the sake of simplicity, we implicitly assume that all lists are acyclic. \nWe express the updates in the abstract pro\u00adgram through logical formulas over unprimed and primed variables \n(which, as usual, model the pre and the post value for the update). For instance, the abstract transformer \nfor the loop body of program INIT is given by .v. (v . Cont . v . Cont') . (v . Init -v . Init') . (v \n. Iter' -v . Iter) . (v . Init' . v/. Init . v . Iter . v/. Iter').  C0 : havoc Iter ' C0 : havoc Iter \n' C0 : ' ' assume Iter = Cont assume Iter = Cont ' ' Iter:= Iter Iter:= Iter C1 : while *do C1 : while \n*do C1 : havoc Iter ' , Init ' havoc Iter ' , Init ' ' ' assume Init .Init assume Init .Init ' ' assume \nIter .Iter assume Iter .Iter assume Init ' -Init = Iter -Iter ' (Iter, Init):= (Iter ' , Init ' ) (Iter, \nInit):= (Iter ' , Init ' ) assume Iter=\u00d8 assume Iter=\u00d8 C2 : assert Cont .Init C2 : C2 : assert Cont .Init \n ' havoc Iter ' assume Iter = Cont ' Iter:= Iter while *do ' havoc Iter , Init ' ,Y ' assume Init = Init \n.Y ' assume Iter = Iter -Y ' (Iter, Init):= (Iter , Init ' ) assume Iter=\u00d8assert Cont .Init Figure 3. \nProgram ABSINIT:Booleanheapab-Figure 4. Program CARTABSINIT:Carte-Figure 5. Counterexample-guided focus \nstraction of Program INIT sian abstraction of Program ABSINIT applied to Program CARTABSINIT For readability, \nin the abstract program in Fig. 3, we decompose the abstract transformers into several assume statements \nand represent them as set constraints (instead of universally quanti.ed formulas). Also, we omit some \nredundant information. The successor abstract state under the execution of a basic block in the abstract \nprogram is obtained as one expects. First the ab\u00adstract state is conjoined with the logical formula (in \nunprimed and primed variables) that represents the abstract transformer of the ba\u00adsic block. Then the \nunprimed variables are projected and the primed variables renamed by their unprimed version. The (.nite) \nset of reachable abstract states for the abstract program ABSINIT repre\u00adsents an (inductive) invariant \nof the program INIT. Projected to lo\u00adcation f1, this invariant implies the formula F . I.e., the correspond\u00ading \nanalysis (which computes the set of reachable abstract states of ABSINIT) succeeds to prove the correctness \nof the program INIT. Program ABSINIT represents the most precise abstract trans\u00adformer with respect to \nthe abstract domain induced by the given predicates. This abstraction is in general too expensive. First, \nthe construction of the abstract program requires exponentially many theorem prover calls in the number \nof predicates. Second, the most precise abstraction often keeps track of more information than is necessary \nfor proving a speci.c property and causes the analysis to explore unnecessarily large parts of the abstract \ndomain. Abstract Transformer with Cartesian Abstraction. In order to obtain an abstract transformer with \nfeasible cost, one can apply the Cartesian abstraction [2, 16] on top of the Boolean heap ab\u00adstraction. \nCartesian abstraction is originally de.ned on abstract do\u00admains that are power sets of vectors (in our \ncase bitvectors). Its name stems from the fact that it abstracts a set of vectors S by the smallest Cartesian \nproduct (of component sets) that contains S.It applies to our setting because we can represent an abstract \nstate canonically as a set of bitvectors where each bitvector corresponds to an inner disjunct of the \nabstract state. The Cartesian abstraction of the most precise abstract post (with respect to the Boolean \nheap abstraction) can be constructed effec\u00adtively from the concrete program (in practice using only a \npolyno\u00admial number of theorem prover calls) [45]. Furthermore, Cartesian abstraction avoids an explosion \nof the size and number of abstract states that are explored in the .xed point computation by restricting \nthe disjuncts that can appear below and above the universal quanti\u00ad.er in abstract states. The resulting \nanalysis is ef.cient in practice. In general, however, it is too imprecise. This is demonstrated by our \nexample program. Figure 4 shows the Cartesian abstraction of Program ABSINIT. Program CARTABSINIT loses \nthe correlation between the predi\u00adcates Iter and Init in the abstract transformer of the loop body (the \ncorrelation is expressed by the statement assume Init ' - Init = Iter - Iter ' in the program ABSINIT \nin Fig. 3). As a consequence, the invariant F of Program ABSINIT is not an invariant of Pro\u00adgram CARTABSINIT. \nI.e., the corresponding analysis (which com\u00adputes the set of reachable abstract states of CARTABSINIT) \ndoes not succeed to prove the correctness of the program INIT. Counterexample-Guided Focus. The analysis \nof the abstract pro\u00adgram CARTABSINIT produces a spurious error trace that witnesses a violation of the \nassert statement at location f2. It seems tempt\u00ading to try to use the spurious error trace to re.ne the \nabstraction as done in the CEGAR scheme. As mentioned above, in the existing CEGAR scheme, the abstract \ndomain is re.ned; the desired effect is to eliminate the spurious error trace. The underlying assumption \nwhich guarantees this effect is that the re.ned analysis is based on the most precise abstract transformer. \nIn our case, however, the spurious error trace results from an imprecise abstract transformer, rather \nthan an imprecise abstract domain. In fact, the abstract do\u00admain is already able to express an inductive \ninvariant that is suf\u00ad.ciently strong to rule out all spurious error traces. An attempt to extract new \npredicates from the proof of spuriousness of the coun\u00adterexample is therefore pointless. It makes more \nsense to use the spurious error trace for re.ning the abstract transformer, rather than the abstract \ndomain. To do so, we use the above-mentioned focus operator that is in\u00adspired by shape analysis. It re.nes \nthe image of the abstract trans\u00adformer by changing the presentation of its pre-image. We do not change \nthe de.nition of the abstract transformer (with the Carte\u00adsian abstraction). However, when applied to \nthe new presentation, the Cartesian abstraction does not lose as much precision as before. The novelty \nof our approach is that we devise a focus operator that is constructed on-demand and locally from the \nspurious error trace. Figure 5 illustrates the effect of our counterexample-guided fo\u00adcus operator on \nProgram CARTABSINIT. It only affects the body of the while loop. The focus operator uses an additional \npredicate Y = { v | y = v } in order to split disjuncts below the universal quanti.er into multiple disjuncts \nbefore the application of the ab\u00adstract transformer for the loop body. As a consequence, the corre\u00adlation \nbetween variables Iter and Init is not lost in spite of the Cartesian abstraction of the abstract transformer. \nNote the differ\u00adent, but equivalent representation of the correlation by two assume statements. Our method \nand tool automatically infers the split pred\u00adicate Y and the corresponding focus operator from a spurious \ncoun\u00adterexample of the abstract program CARTABSINIT. The inductive invariant obtained by the .xed point \nof the resulting abstract trans\u00adformer implies the assertion at location f2. I.e., the analysis (which \ncomputes the set of reachable abstract states of the abstract pro\u00adgram in Fig. 5) does succeed to prove \nthe correctness of Program INIT.  3. Preliminaries We now formalize the notion of formulas and programs \nused in this paper and formally introduce Boolean heap abstraction. 3.1 Logics and Programs. Logical \nformulas and structures. For reasoning about programs we consider formulas in a sorted logic L. We require \nthat L pro\u00advides the sorts obj (heap objects) and loc (program locations). Fur\u00adthermore, we require that \nL provides logical constructs for equality over both sorts, Boolean connectives, and .rst-order quanti.cation \nover variables of sort obj. Formulas are expressed over a signature S where S consists of the following \nconstant symbols of sort loc: f . Locs (control locations), pc (the program counter), f0 (the ini\u00adtial \nlocation), and fE (the error location), as well as the following symbols of sort obj: unary function \nsymbols f . Flds (data struc\u00adture .elds) and constant symbols x . Vars (program variables). We leave \nout sort annotations in formulas whenever this causes no confusion. In our running examples, the logic \nL is given by .rst\u00adorder logic with transitive closure. We .x non-empty disjoints sets O and L for the \ninterpretations of sort obj and loc.A S-structure A is a .rst-order interpretation that interprets each \nsymbol in S by a function over, respectively, an element in the sets interpreting the associated sorts. \nWe write A(f) for the interpretation of symbol f . S in structure A.We further write A(t) for the denotation \nof a S-term t in structure A. Hereby, A also provides interpretations for the free variables occurring \nin t. We use standard logical notation for satis.ability, validity, and entailment. Finally, for a formula \nF we write [[F ]] to denote the set of all structures that satisfy F . Programs. The set of commands \nComs is de.ned by the following grammar where F is a formula in L, x, y . Vars denote program variables, \nf . Flds a pointer .eld, and f . Locs .{f0,fE} a control location: c ::= c ; c | assume F | pc:= f | \nx:= y | x:= y.f | x.f:= y A program P is a .nite set of commands. Consecutiveness of com\u00admands in a program \nis achieved by composing assume statements on the value of pc, updates of pc, and the actual commands \nusing the sequential composition operator. Program states. A program state is a S-structure. We denote \nby States the set of all program states. We call a state s initial state iff it satis.es s |= pc=f0 and \nwe call it error state iff it satis.es s |= pc=fE.Let init be the formula pc=f0 denoting all initial \nstates and let safe be the formula pc denoting all non-error =fE states. Null pointers and allocation. \nNote that we interpret data structure .elds f as total functions. We treat null as a program variable \nthat can neither be assigned nor dereferenced. We assume that for all .elds f . Flds the equality null.f \n= null holds in all program states. In order to ensure absence of null dereferences, every com\u00admand c \nthat contains a dereference of the form x.f is guarded by a command assume (x=null); pc:= fE that directs \ncontrol to the er\u00adror location if x is not de.ned. Allocation of fresh heap objects can be modelled by \nintroducing a predicate symbol that keeps track of the current set of allocated objects. However, this \nrequires the in\u00adclusion of havoc commands that nondeterministically update the value of a program variable. \nThe techniques presented in this paper carry over to programs extended with havoc commands. For details \nsee [51]. Transition relations. Each command c represents a relation [[c]] on pairs of states (s, s ' \n) that is de.ned recursively on the structure of commands as follows: If c is a sequential composition \nc1; c2 then there must exist a '' '' state s such that (s, s '' ) . [[c1]] and (s ,s ' ) . [[c2]]. If \nc is an assume command assume F , we require that s |= F and s ' = s.  If c is an update of the program \ncounter or a program variable, we have s ' = s[pc .s(f)] for c =(pc:= f), s ' = s[x . s(y)] for c =(x:= \ny) and s ' = s[x . s(f(y))] for c =(x:= f.y).  Finally, if c is a .eld update of the form f.x:= y,we \nhave s ' = s[f . s(f)[s(x) .s(y)]].  Computations and traces. A program computation of a program c0c1 \n P is a (possibly in.nite) sequence s = s0 . s1 . ... of states and commands such that s0 is an initial \nstate and for each pair of consecutive states si and si+1 we have (si,si+1) . [[ci]] for some command \nci . P .A trace is a sequence of commands and we c0c1 call the projection of a computation s = s0 . \ns1 . ... to the sequence of commands c0c1 ... the trace of that computation. A trace is called error \ntrace if it is the trace of some computation that reaches an error state. A program is safe if it has \nno error traces. Predicate transformers. Given a set of states S and a binary relation R on states, we \nde.ne strongest postcondition post and weakest (liberal) precondition wlp as usual: .\u00af def '' post(R)(S)= \ns |.s. (s, s ) . R . s . S def . '' wlp(R)(S)= s |.s ' . (s, s ) . R . s . S \u00af . We further introduce \nsymbolic weakest preconditions on formulas. For any command c and formula F the formula wlp(c)(F ) is \na for\u00admula such that we have wlp([[c] )([[F ]]) = [[wlp(c)(F )]]. Note that we do not require F to be \nclosed. We extend symbolic weakest pre\u00adconditions from commands to sequences of commands as expected. \n 3.2 Boolean Heap Abstraction We formalize the Boolean heap abstraction in terms of an abstract interpretation \n[15]. The concrete domain is given by sets of states ordered by set inclusion. We represent elements \nof the concrete do\u00admain by closed formulas in L. The concrete .xed point functional is the operator post \nfor the transition relation of the concrete pro\u00adgram (i.e., the union of the transition relations of \nall its commands) and the initial states init. The abstract domain is a .nite set of for\u00admulas that forms \na sublattice of the concrete domain. The analysis is to compute the least .xed point of an abstraction \nof post that is de.ned on the abstract domain. The computed least .xed point is an inductive invariant \nof the concrete program. Abstract domain. The abstract domain is parameterized by a .nite set of predicates \nthat denote sets on heap objects in a given state. In the following, we .x a particular .nite set of \npredicates P.We consider P to be given by a set of (closed) lambda terms of the form .v. G where G is \na formula in L, i.e., each predicate p .P denotes a set of objects in a given state. If the formula G \nis itself closed we call the corresponding predicate state predicate. We denote by P(v) the set of formulas \nobtained by beta reduction of all formulas p(v) for p .P. For notational convenience we assume that P \nis such that for all v the set P(v) is closed under negation. The following de.nitions are implicitly \nparameterized by the set P. The abstract domain AbsDom over P consists of all formulas of the form nmi \n.v. Dji (v) i=1 ji=1 where each Dji (v) is a conjunction of formulas in P(v).We call the outer disjuncts \nof these formulas abstract states and the inner disjuncts abstract objects. We identify formulas up to \nlogical equivalence. The partial order on the abstract domain is given by the logical entailment relation \n|= . Note that AbsDom is .nite (modulo logical equivalence) and closed under both conjunction and disjunction. \nThus, it forms a complete lattice. The abstract domain can be easily generalized to formulas with quanti.cation \nover more than one variable and predicates that denote relations on objects rather than just sets [51]. \n Abstraction function. The abstraction function a that maps a set of states represented by a closed \nformula F to a formula in the abstract domain is de.ned as follows no ^ def ## a(F )= F . AbsDom | F \n|= F. The function a is the lower adjoint of a Galois connection (a, .) between the concrete and abstract \ndomain, with . being the identity function. Abstract post operator. The most precise abstract post operator \non the abstract domain of Boolean heaps post# and a command c is BH given by composition of the concrete \npost operator for c with the Galois connection (a, .). The actual abstract post operator that we use \nin the .xed point computation of the analysis is an abstraction of post # We denote this operator by \npost# and call it the BH. C\u00b7BH Cartesian abstract post operator. Formally, the operator post# C\u00b7BH # \nis de.ned as a Cartesian abstraction of the operator postBH.Inthe # following, we only show how postC\u00b7BH \nis computed. For further details see [45, 51]. We allow abstract states in the pre and post-images of \noperator # post C\u00b7BH to range over different sets of predicates P1, respectively, P2.Let c be a command \nand F # an abstract state over predicates P1 of the form m # _ F = .v. Dj (v) j=1 where the disjuncts \nDj are monomials, i.e., each predicate in P1 occurs either positive or negative in each Dj . The operator \npost# C\u00b7BH maps F # to a single abstract state F '# by mapping each disjunct Dj in F # to a single disjunct \nDj ' in F '#. The mapping guarantees that if s . States is a concrete state that satis.es F # and o .O \nan object that satis.es disjunct Dj in s then for every c-successor s ' of s, o satis.es Dj ' in s ' \n. Since this property holds for all objects o,every c-successor s ' of s satis.es F '#. Formally, # the \nimage of F # under the abstract post operator postC\u00b7BH for command c is given by: post# C\u00b7BH[P1, P2](c)(F \n#)= W m V. \u00af .v. p(v) .P2(v) | F # . Dj (v) |= wlp(c)(p(v)) j=1 Thus, the image of the Cartesian abstract \npost is computed by checking entailments between conjunctions of predicates and weakest preconditions \nof predicates. The quanti.ed formula F # in the antecedent of these entailments can be replaced by any \nweaker formula, e.g., a conjunction of .nitely many instantiations of F # . The operator post# C\u00b7BH is \nextended to disjunctions of abstract states as expected.  4. Counterexample-Guided Focus Before we \nformally de.ne the counterexample-guided focus oper\u00adator, it is instructive to fully understand the nature \nof the loss of precision that is induced by Cartesian abstraction. Recall Program Init from Section 2. \nThe left part of Figure 6 shows a program state s that may occur at location f1 during execution of Program \nINIT. The boxes represent abstract objects over the sets Cont, Init,and Iter. Hereby Sc stands for the \nset Figure 6. A reachable program state s of Program INIT and its successor state s ' that is obtained \nafter execution of the loop body.  complement of S. Formally the state s satis.es the abstract state \nF # = .v. v . Cont .v . Iter .v/. Init .v/. Cont .v/. Iter The right part of Figure 6 shows the post-state \ns ' of s that is obtained at location f1 after execution of the loop body. The boxes indicate again the \nabstract objects associated with the concrete objects. The abstract state consisting of the disjunction \nof these abstract objects is the image of the most precise abstract post postBH # of the abstract state \nF #.Let F '# be this abstract post\u00adstate. Note that the concrete objects in s that are represented by \nthe abstract object v . Cont . v . Iter . v/. Init end up in two disjoint abstract objects in F '#. \nThe Cartesian ab\u00ad dat a stract post operator merges these two disjuncts into a single con\u00adjunction that \nonly contains the predicates on which both disjuncts agree, namely, v . Cont. The correlations between \npredicates Init and Iter in the two inner disjuncts of F '# are lost. dat a If we want to adapt the \nprecision of the Cartesian abstract post we need to prevent it from merging disjuncts in the post-image \nof the most precise abstract post. This adaptation is performed by our focus operator. The focus operator \nadapts the precision of the Carte\u00adsian abstract post indirectly. Namely, it re.nes the abstract domain \nof the pre-image and splits disjuncts (i.e., both abstract states and abstract objects in abstract states) \nin the pre-image into more .ne\u00adgrained disjunctions. The splitting ensures that individual disjuncts \nin the re.ned pre-image are mapped to individual disjuncts in the post-image under the most precise abstract \npost. This effectively dat a prevents Cartesian abstraction from losing precision. Both the re\u00ad.nement \nof the abstract domain and the splitting of disjuncts are guided by spurious error traces that are produced \nby the analysis. dat a Counterexample-guided focus. We now formally de.ne the counterexample-guided \nfocus operator. In the following we .x a program P and a set of predicates P.An abstract computation \ns# is a sequence op0op1opn-1 F # F # 0 -. F1# -. ... -. n where the Fi # are elements of the abstract \ndomain AbsDom[P] and the opi are abstract transformers AbsDom[P] . AbsDom[P]. Moreover, the following \ntwo conditions hold: (1) F0# = a[P](init) ## and (2) for all i between 0 and n-1, Fi+1 = opi(Fi ). We \nsay that the abstract computation ends in an error state if Fn# |= safe.We say that s# is generated by \na trace p = c0 ...cn-1 and an operator op . Coms . AbsDom . AbsDom if for all i, opi = op(ci). We say \nthat the generated abstract computation is sound if for all i between 0 and n - 1, F # is an over-approximation \nof the set of i+1 states that are reachable from an initial state by following the trace c0 ...ci. Finally, \nwe say that a trace p is a spurious error trace for op, if the abstract computation generated by p and \nop ends in an error state, yet, p is not an error trace of program P . The counterexample-guided focus \noperator is used to eliminate spurious error traces for the Cartesian abstract post that are not spurious \nerror traces for the most precise abstract post. Let p0 = c0 ...cn-1 be such a trace. Note that concrete \nerror traces can be characterized in terms of symbolic weakest preconditions. LEMMA 1. Atrace p is an \nerror trace iff init |= wlp(p)(safe). Since p0 is not a concrete error trace, we know that p0 satis.es \ninit |= wlp(p0)(safe) (1) Let prefi(p0) be the pre.x of p0 up to command ci-1, respectively, su.i(p0) \nits suf.x starting from command ci.From (1)and the properties of predicate transformers post and wlp \nfollows that for all i between 0 and n - 1 we have post([[prefi(p0)]])([[init]]) . wlp([[su.i(p0)]])([[safe]]) \n(2) In other words, for each i the formula wlp(su.i(p0))(safe) is satis.ed in all states that are reachable \nfrom an initial state by following the trace prefi(p0). The idea of our counterexample\u00adguided focus operator \nis to use the formulas wlp(su.i(p0))(safe) to guide the splitting of disjuncts in the pre-images of the \nCartesian post operator. The counterexample-guided focus operator focus takes a se\u00adquence of commands \np (the suf.x of a spurious error trace p0) and an element of the abstract domain AbsDom[P] as arguments \nand maps the latter to an element of a re.ned abstract domain AbsDom[preds(p)] with P. preds(p). The \noperator focus is de.ned as follows #def # focus(p)(F )= a[preds(p)](wlp(p)(safe)) . F The set of predicates \npreds(p) is the union of the predicates P and predicates that are extracted from the weakest precondition \nof safe with respect to p. More precisely, if p = cp ' then preds extracts all atoms from the formula \nwlp(c)(a[P](wlp(p ' )(safe))) (3) The adapted Cartesian abstract post operator post # (p) for the f\u00b7C\u00b7BH \nsuf.x p of some spurious error trace is obtained by composition of the Cartesian post operator (for the \nre.ned pre-image domain) with the focus operator: # def # post (p)= .c. post[preds(p), P](c) . focus(p) \nf\u00b7C\u00b7BHC\u00b7BH Let s# be the abstract computation generated from path p0 and the sequence of operators [post# \n(su.i(p))(ci)]0=i<n. f\u00b7C\u00b7BH PROPOSITION 2 (Soundness of Focus). The abstract computation s# is sound. \nThe proof of Proposition 2 follows from Property (2)and the # fact that postC\u00b7BH is a sound approximation \nof the most precise abstract post operator post # BH. PROPOSITION 3 (Progress of Focus). The abstract \ncomputation s# does not reach an error state. The proof of Proposition 3 relies on the fact that the \ntrace po of computation s# is not an error trace and not spurious for the most precise abstract post. \nOne can then show that focus performs suf.cient splitting of disjuncts in abstract states of s# before \neach application of the Cartesian abstract post. This splitting ensures that Cartesian abstraction causes \nno loss of information that is crucial for proving that po is safe. Note, however, that the focused Cartesian \nabstract post is not guranteed to compute the most precise abstract post for the given trace. Its precision \nlies between the plain Cartesian abstract post and the most precise abstract post. Practical considerations. \nIn our actual analysis the focus oper\u00adator is always applied in a very speci.c situation, namely, when \nthe abstract domain for the post states of the re.ned abstract trans\u00adformer already contains all the \npredicates that can be extracted from the spurious error trace used for the focus. Therefore the abstrac\u00adtion \nfunction a in (3) can be replaced by the identity function. The resulting focus operator is then polynomial \nin the number of ex\u00adtracted predicates and the size of the representation of the focused abstract states. \n 5. Additional Examples We now illustrate how the counterexample-guided focus adapts the abstract transformers \nfor the abstractions of three example programs. In all of these examples, the analysis without counter\u00adexample-guided \nfocus would not be able to infer a suf.ciently strong invariant that proves the correctness of the program. \nProgram INIT. We .rst revisit Program INIT from Section 2. We explained the nature of the loss of precision \nunder Cartesian abstraction for this example program in the previous section. This loss of precision \ncauses that the analysis of Program INIT produces spurious error traces even though the abstract domain \ncan express a suf.ciently strong inductive invariant. The shortest such spurious error trace is the trace \nthat starts with the commands at location f0 executes the while loop once and then goes to the error \nlocation via the failing assert statement at location f2. The weakest precondition wlp(p)(safe) for the \nsuf.x p of this spurious error trace that starts in location f1 is given by the formula y.next=null . \ny=null . (.v. (x, v) . next * . v=null . v.data=0 . y=v) Using this formula, the focus operator re.nes \nthe pre-image of the abstract transformer for the loop body by adding, among other predicates, the predicate \nY = { v | y=v }. In this particular ex\u00adample, simply adding predicate Y to the pre-image domain is suf\u00ad.cient \nto rule out the spurious error trace. Recall that the image of the Cartesian abstract post operator is \ncomputed by considering a normal form of the pre-image where in each inner disjunct every predicate occurs \neither positively or negatively. Thus, re.ning the abstract domain by adding predicate Y already enables \nthe Carte\u00adsian abstract post to perform the necessary splitting of disjuncts in the original pre-image. \nHowever, this splitting is purely syntactic. Some of the split disjuncts might be unsatis.able in all \nrepresented pre-states but might be mapped to satis.able disjuncts in the post\u00adimage and, thus, cause \nimprecision. Also, without proper focus the image of the Cartesian abstract post will always be a single \nabstract state and never a proper disjunction. Our second example shows that, in general, the local re.nement \nof the abstract domain of the pre-image alone, does not suf.ce to eliminate a spurious error trace for \nthe Cartesian abstract post. Program LISTREVERSE. Consider program LISTREVERSE in Figure 7 that performs \nan in-place reversal of a singly-linked list. The list is pointed to by program variable r. Assume the \nheap is sharing-free before execution of the program (the .rst assume statement at location f0) and assume \nthat r points to the actual root node of the list (the second assume statement at location f0). We would \nlike to verify that under these assumptions r points again to the root node of the reversed list after \ntermination of the program. One part of the inductive invariant for location f1 that is needed to verify \nthe assertion at location f2 is given by the formula e=null . (.v. v.next=e) This formula expresses \nthat e always points to the root of the part of the original list that has yet to be reversed. This formula \ncan be C0 : assume (.uv w. v.next=w .u.next=w .v=u .w=null) assume (r=null .(.v. v.next=r)) e:= r; r:= \nnull C1 : while e=null do t:= e; e:= e.next t.next:= r C2 : assert (r=null .(.v. v.next=r)) Figure 7. \nProgram LISTREVERSE  Figure 8. Two reachable states of Program LISTREVERSE expressed by the disjunction \nof the following two abstract states F1# : .v. e=v . null=v F # 2 : .v. v.next=e Figure 8 shows two states \nthat may occur at location f1 during execution of the while loop. Both states satisfy the abstract state \nF # 2 . Note that after execution of the loop body, the state s1 sat\u00adis.es only abstract state F1# while \nthe second state satis.es only abstract state F2#. The Cartesian abstract post operator will always merge \nthe post states of F2# that result from execution of the loop body into a single abstract state. An analysis \nbased on this opera\u00ad tor therefore cannot infer a suf.ciently strong inductive invariant. The counterexample-guided \nfocus causes F2# to be split into two abstract states, one whose post states are covered by F1# and one \nwhose post states are covered by F2#. Thus, the Cartesian abstract post will not lose precision on the \nfocused pre-image. Re.ning the abstract domain by adding additional predicates will not make up for the \nloss of precision on the unfocused pre-image, unless one adds a state predicate that expresses one of \nthe outer disjuncts in terms of an inner disjunct of the other (e.g., the predicate e=null in the given \nexample). In general, the state predicates that one needs to add to prevent loss of precision under Cartesian \nabstraction can be arbitrarily complex quanti.ed formulas. Program DLISTERASE. The splitting of disjuncts \nthat is per\u00adformed by our counterexample-guided focus operator is closely re\u00adlated to materialization \nin shape analysis. Materialization refers to an intermediate step in the computation of the abstract \npost where a concrete object is extracted from an abstraction of a collection of objects. For instance, \ngiven an abstraction of a list, one needs to extract the head of the list in order to compute a precise \nabstract post-state for a command that iterates over the list. Our third exam\u00adple demonstrates that counterexample-guided \nfocus performs mate\u00adrialization automatically, even in cases that require data-structure\u00adspeci.c manual \nadaptation of the abstract transformers in many ex\u00adisting shape analyses. Consider Program DLISTERASE \nshown in Figure 9.This pro\u00ad gram erases all entries in an acyclic doubly-linked list. The list is pointed \nto by program variable r. The loop that erases the entries in the list iterates backwards over the data \nstructure starting from the last entry l. In each iteration both the forward pointer next and the backward \npointer prev of the current iterate are set to null.The task is to verify that the prev and next .elds \nof all original list entries have indeed been set to null after the loop terminates. This prop\u00aderty is \nexpressed by the assert statement at location f2. The assume statements at location f0 express the precondition \nof the program. C0 : assume (.vw. v.prev =null .w.next=v .v.prev .next = v) assume (.v. v .Cont0 .(r, \nv) .next * .v=null) assume l .Cont0 assume l.next=null C1 : while l=null do l.next:= null t:= l; l:= \nl.prev t.prev := null C2 : assert (.v. v .Cont0 .v.next=null .v.prev =null) Figure 9. Program DLISTERASE \n The .rst assume statement expresses that .eld prev is the inverse of .eld next which implies that the \nlist r is doubly-linked. The sec\u00adond assume statements de.nes the set Cond0,a ghost variable that denotes \nthe set of elements that are originally stored in the list. The third and fourth assume statement together \nensure that l points to the last entry in the list. The important part of the inductive invariant at \nlocation f1 that is strong enough for proving the assertion at location f2 is given by the following \nformula .v. v . Cont0 . (l, v) ./prev * . v.next=null . v.prev=null In order to infer this formula, \nthe abstract transformer for the loop body needs to split some of the inner disjuncts that contain positive \noccurrences of the predicate (l, v) . prev * in order to keep pre\u00adcise information about the object pointed \nto by program variable l in each iteration. This splitting corresponds to materialization from the back \nof the doubly-linked list. Many shape analyses, e.g., those based on separation logic [13, 19, 38], need \nspecial hand-crafted rules to perform materialization for speci.c data structures. With counterexample-guided \nfocus, the abstract transformer is automat\u00adically adapted to perform materialization. The adaptation \nmecha\u00adnism is independent of the data structures that the analyzed pro\u00adgram manipulates and is only applied \nwhen the extra precision is needed to prove a particular property. 6. Lazy Nested Abstraction Re.nement \nWe now present our lazy nested abstraction re.nement loop that integrates lazy counterexample-guided \nre.nement of the ab\u00adstract domain and lazy adaptation of the abstract transformer via counterexample-guided \nfocus. The re.nement loop is shown in Figure 10. The procedure LazyNestedRe.ne takes a program P as input \nand constructs an abstract reachability tree (ART) in the spirit of lazy abstrac\u00adtion [25]. An ART is \na tree where each node r is labeled by a set of predicates r.preds and abstract states r.states in AbsDom[r.preds]. \nThe root node r0 of the ART is labeled by an abstract state denot\u00ading the set of all initial states. \nEach edge in the ART is labeled by a command c in program P and an abstract transformer op for the c,op \n command c. We write r -. r ' to denote that there is an edge in the ART from node r to node r ' which \nis labeled by c and op. . *' Furthermore, we write r pr to indicate that there is a (possibly empty) \npath from r to r ' in the ART that is labeled by the trace p. Each path in the ART starting from the \nroot node corresponds to an abstract computation that is generated from the trace labelling the path. \nThe lazy nested abstraction re.nement algorithm iteratively ex\u00adtends and re.nes the ART until either \na .xed point is reached, i.e., the disjunction of the abstract states contained in all ART nodes is an \ninductive invariant of program P , or until an error trace has been constructed. If a spurious error \ntrace is encountered during the .xed point computation then this trace is used to re.ne the abstraction. \nWe now describe the algorithm in detail. proc LazyNestedRe.ne(P : program) begin let r0 = (preds : {init}, \nstates : init, covered : false)let succ(r)= let Succ = \u00d8for all c .Tdo ' let r = (preds : \u00d8, states : \nfalse, covered : false) # let op = post(c) C\u00b7BH c,op ' add edge r -.r ' Succ:= Succ .{(r, op,r )} return \nSucc let U = succ(r0) while U = \u00d8do choose and remove (r1, op,r2) .U r2.states:= op[r1.preds,r2.preds](r1.states) \nif r2.states |= W{r.states |r = r2 }then r r.covered:= true ' else if r2.states |= safe then U := U .succ(r \n) p else let rs,p such that p is maximal trace with rs .* r2 . rs.states|= wlp(p)(safe) if rs = r0 then \nreturn counterexample(p) c,op else let rp,c, op such that rp -.rs let Pp = preds(wlp(p)(safe)) ' let \nop = if Pp.rs.preds then rs.preds:= rs.preds .Pp op else op .focus(cp) remove subtrees starting from \nrs for all r2 such that r2.covered .r2.states|= false . rs.states older than r2.states do c,op let r1,c, \nop such that r1 -.r2 r2.covered:= false r2.states:= false U := U .{(r1, op,r2)} rs.states:= false rs.covered:= \nfalse I c,op update edge rp -.rs ' U := U .{(rp, op ,rs)} return program is safe end Figure 10. Lazy \nnested abstraction re.nement algorithm The algorithm maintains a work set of unprocessed ART edges U. \nIn each iteration one unprocessed ART edge (r1, op,r2) is selected. Then the image of the abstract states \nin r1 and the abstract transformer op is computed and the resulting abstract states are stored in r2.states. \nIf the computed abstract states are already subsumed by other ART nodes then the node r2 is marked as \ncovered. Otherwise if r2.states contains no error states then the ART is extended with new nodes that \nare the successors of r2 for all the commands in P . The edges to the successor nodes are labeled by \nthe commands c and the initial abstract transformer given by the Cartesian abstract post for the command \nc. Then the new edges are inserted to the set of unprocessed edges. If r2 contains error states then \nthe trace labelling the path from r0 to r2 is a potential error trace. The analysis now determines whether \nthis trace is a spurious error trace. For this purpose, it performs a symbolic backward analysis of the \nerror trace. This backward analysis .nds the oldest ancestor node rs of r2 with p rs . * r2 such that \nrs.states represents some concrete state that can reach an error state by executing the trace p, i.e., \nformally rs is the oldest node on the path that still satis.es rs.states |= wlp(p)(safe) . If rs is \nthe root node of the ART then p is a concrete error trace and the procedure returns the counterexample. \nIf, however, rs is not the root node then the trace is a spurious error trace. In this case we call rs \nthe spurious node of the trace. The algorithm then determines the immediate predecessor node rp of the \nspurious node. We call rp the pivot node of the spurious error trace. The pivot node is the youngest \nnode on the given path in the ART that does not represent any concrete states that can reach an error \nstate by following the commands in the trace. Depending on the re.nement phase either the abstract domain \nof rs is re.ned or the abstract transformer that labels the edge between rs and rp is adapted. The re.nement \nworks as follows. The spurious part of the error trace starts from the spurious node rs. Our abstraction \nre.nement procedure .rst attempts to re.ne the abstract domain of node rs by adding new predicates Pp \nthat are extracted from the spurious part p of the spurious error trace. The predicate extraction function \npreds guarantees that the weakest precondition wlp(p)(safe) of the path is expressible in the abstract \ndomain of the re.ned node rs, i.e., formally the following entailment holds a[Pp](wlp(p)(false)) |= wlp(p)(false) \n. Suppose the analysis was to compute the most precise abstract post operator for each command. Then \nwe were guaranteed that after re.ning the predicate set of node rs and reprocessing the ART edge between \nrp and rs, the node rs would no longer be spurious for this spurious error trace. This would ensure that \nthe spurious error trace would eventually be eliminated. However, our analysis uses the Cartesian abstract \npost operator. Thus, the same spurious error trace might be reproduced after the re.nement of the abstract \ndomain. The re.nement procedure would then fail to derive new predicates for node rs. In this case, the \nsecond re.nement phase re.nes the current abstract transformer op for command c that labels the edge \nbetween rp and rs. The abstract transformer op is re.ned by composition with the counterexample-guided \nfocus for the spurious part cp of the trace. After the abstraction has been re.ned, the spurious subtrees \nbe\u00adlow rs are removed from the ART. In order to ensure soundness, ART nodes that have potentially been \nmarked as covered due to subsumption by nodes in the removed subtrees are uncovered and reinserted into \nthe work set. Finally, the spurious ART edge be\u00adtween rp and rs is updated and also reinserted into the \nwork set. If the set of unprocessed ART edges becomes empty then all outgoing edges of inner ART nodes \nhave been processed and all leaf nodes are covered, i.e., an inductive invariant that proves the absence \nof reachable error states has been computed. Thus, the procedure returns program is safe . THEOREM 4 \n(Soundness). Procedure LazyNestedRe.ne is sound, i.e., for any program P if LazyNestedRe.ne(P ) terminates \nwith program is safe then program P is safe. The proof of Theorem 4 relies on Proposition 2 and follows \nthe argumentation in [25]. The next theorem states that procedure LazyNestedRe.ne has the progress property. \nIn the setting of lazy abstraction, progress means that procedure LazyNestedRe.ne can\u00adnot diverge because \nit gets stuck on re.ning a .nite set of spurious error traces over and over again. THEOREM 5 (Progress). \nArun . of procedure LazyNestedRe.ne terminates, unless the set of spurious error traces encountered in \n. is in.nite.    benchmark checked properties DP time (in s) CGF   List.traverse AC, SF MONA 0.11 \nno List.init AC, SF, PC MONA 0.69 no List.create AC, SF MONA 0.78 yes List.getLast AC, SF, PC MONA 0.53 \nno List.contains AC, SF, PC MONA 0.53 no List.insertBefore AC, SF MONA 2.48 yes List.append AC, SF MONA \n8.95 no List..lter AC, SF MONA 5.31 yes List.partition AC, SF MONA 149.16 yes List.reverse AC, SF MONA \n5.52 yes DList.addLast AC, SF, DL, PC MONA 2.05 yes DList.erase AC, SF, DL, PC MONA 17.98 yes SortedList.add \nAC, SF, SO, PC MONA, Z3 9.88 no SkipList.add AC, SF, PC MONA 10.82 yes Tree.add AC, SF, PC MONA 18.51 \nno ParentTree.add AC, SF, PL, PC MONA 20.48 no ThreadedTree.add AC, SF, TH, SO, PC MONA, Z3 445.93 no \nClient.move CS Z3 3.11 no Client.createMove CS, PC Z3 41.07 yes Client.partition CS, FC, PC Z3 108.15 \nno Properties: CS = call safety, AC = acyclic, SF = sharing free, DL = doubly linked, PL = parent linked, \nTH = threaded, SO = sorted, FC = frame condition, PC = post condition Table 1. Summary of experiments. \nColumn DP lists the used deci\u00adsion procedures. Column CGF indicates whether counterexample\u00adguided focus \nwas required to successfully verify the corresponding program.  7. Implementation and Case Studies We \nimplemented our analysis in our tool Bohne. Bohne is imple\u00admented in Objective Caml and distributed as \npart of the Jahob sys\u00adtem [30, 51, 56, 57]. The input to Bohne are Java programs anno\u00adtated with special \ncomments that specify procedure contracts and representation invariants of data structures. We represent \nelements of the abstract domain as sets of BDDs [11] and use the weaker order of propositional implication \nfor the .xed point test in the abstraction re.nement loop. Abstract transformers are also repre\u00adsented \nas BDDs to enable ef.cient post-image computation. We represent the program counter explicitely and not \nimplicitely in ab\u00adstract states. Our tool uses a few simple heuristics to guess an ini\u00adtial set of predicates \nfrom the input program and its speci.cation. All additional predicates are inferred in the nested abstraction \nre\u00ad.nement procedure. Since we syntactically extract new predicates from weakest preconditions of .nite \ntraces in the program, we can\u00adnot infer reachability predicates (i.e., predicates with transitive clo\u00adsure \noperators) if they do not already occur in the speci.cation. We therefore use a widening technique to \ninfer new reachability predicates from the predicates that are extracted from weakest pre\u00adconditions. \nCase studies. We applied Bohne to verify operations on a diverse set of data structures implementations, \nchecking a variety of prop\u00aderties. No manual adaptation of the abstract domain or the abstract transformers \nwas required for the successful veri.cation of these example programs. In particular, we were able to \nverify preserva\u00adtion of data structure invariants for operations on threaded binary trees (including \nsortedness and the in-order traversal invariant). We are not aware of any other analysis that can verify \nthese proper\u00adties with a comparable degree of automation. Further experiments cover data structures such \nas (sorted) singly-linked lists, doubly\u00adlinked lists, two-level skip lists, trees, and trees with parent \npoint\u00aders. The veri.ed properties include: absence of runtime errors such as null dereferences complex \ndata structure consistency properties, such as preservation of the tree structure and sortedness. Finally, \nwe veri.ed procedure contracts expressing functional correctness  benchmark #appl. of #ref. .nal ART \n#predicates abs. post steps size depth st./loc. total avrg. max. List.traverse 3 0 4 4 1.00 4 2.8 3 List.init \n5 1 5 5 2.00 7 5.4 7 List.create 11 6 6 6 1.67 11 6.7 9 List.getLast 7 1 6 6 2.00 7 6.0 7 List.contains \n5 1 5 5 2.00 6 5.2 6 List.insertBefore 8 2 5 5 13.50 10 7.4 8 List.append 5 1 4 4 1.50 13 8.2 11 List..lter \n31 5 14 5 2.50 12 7.1 10 List.partition 62 21 40 7 3.50 15 10.8 12 List.reverse 9 3 5 5 2.00 11 7.0 9 \nDList.addLast 7 3 5 5 1.50 8 7.2 8 DList.erase 23 6 10 5 5.00 10 7.6 8 SortedList.add 21 3 13 5 1.33 \n9 6.2 9 Skiplist.add 19 4 16 6 3.67 12 9.6 11 Tree.add 11 0 12 5 3.00 11 10.5 11 ParentTree.add 11 0 \n12 5 3.00 11 10.5 11 ThreadedTree.add 151 4 82 6 4.33 17 7.8 17 Client.move 8 0 9 9 1.00 16 8.4 11 Client.createMove \n46 6 21 18 1.00 33 10.1 14     Client.partition 118 18 24 19 1.00 32 11.9 15 Table 2. Analysis \ndetails for experiments. The columns list the number of applications of the abstract post, the number \nof re.ne\u00adment steps, the size and depth of the .nal ART that represents the computed .xed point, the \naverage number of abstract states per lo\u00adcation in the .xed point, the total number of predicates, and \nthe average and maximal number of predicates in a single ART node. properties, e.g., how the set of elements \nstored in a data structure is affected by the procedure. We further performed modular veri.cation of \ndata structure clients that use the interface for sets with iterators from the java.util library. For \nthis purpose, we annotated procedure con\u00adtracts for all set operations and then used our tool to infer \ninvariants for the client. These invariants ensure that all preconditions of the set operations are satis.ed \nat call sites in the client. Furthermore we veri.ed functional correctness properties of the client code. \nTable 1 shows a summary for a collection of benchmarks run\u00adning on a 2.66 GHz Intel Core2 with 3 GB memory \nusing one core. Each program satis.ed all of the checked properties listed for the respective program \nand for each program all of the checked prop\u00aderties have been veri.ed in a single run of the analysis. \nFor reason\u00ading about transitive closure of .elds in tree-like data structures we used MONA [28] in combination \nwith our .eld constraint analysis technique [52] for reasoning about non-tree .elds. We further used \nthe SMT solver Z3 [18] for verifying sortedness properties. For the the data structure clients we used \nonly Z3. The last column in Ta\u00adble 1 indicates that for many of our benchmarks the veri.cation would \nnot succeed without the use of counterexample-guided fo\u00adcus, i.e., without counterexample-guided focus \nthe analysis would not be able to rule out some spurious error trace and get stuck. Note that our examples \nare not limited to stand-alone programs that build and then traverse their own data structures. Instead, \nour examples verify procedures with non-trivial preconditions, post\u00adconditions and representation invariants \nthat can be part of arbi\u00adtrarily large code. Further details of the benchmarks are given in Tables 2 \nand 3.Table 3 gives details on the calls to the validity checker and its underlying decision procedures. \nOne immediately observes that the calls to the validity checker are the main bottleneck of the analysis. \nOn average, 98% of the total running time is spent in the validity checker. The reasons for the high \nrunning times are diverse. First, communication with decision procedures is currently implemented via \n.les which is slower than passing data directly. Second, we use expensive decision procedures such as \nMONA. In   benchmark #VC calls rel. time spent in VC time/DP call     total cache total re.ne. \navrg.     List.traverse 132 189 68 64.02% 95.36% 57.22% 38.14% 0.011 0.016 158 56 64.56% 97.74% \n54.89% 42.86% 0.009 0.028 List.contains 52 54.39% 95.45% 55.30% 40.15% 0.010 0.028 List.insertBefore \nList.append 820 7650 3027 60.43% 99.17% 95.63% 3.54% 0.049 0.088 312 49.27% 98.55% 89.05% 9.50% 0.017 \n0.048 89 44.72% 97.86% 62.57% 35.28% 0.023 0.040 484 35.38% 98.15% 81.42% 16.73% 0.036 0.224 470 190 \n59.57% 97.89% 65.65% 32.24% 0.051 0.120 679 241 64.51% 97.52% 43.84% 53.68% 0.044 0.076 390 124 68.21% \n99.52% 67.59% 31.94% 0.149 0.624 7.85% 0.720 3.816 Table 3. Statistics for validity checker (VC) calls. \nThe columns list the total number of calls to the VC, the number of actual calls to decision procedures \nand the corresponding cache hit ration, the time spent in the VC relative to the total running time, \nand the average and maximal time spent for a single VC call. some of the examples individual calls to \nthese decision procedures can take up to several seconds. Running times can be improved by incorporating \nmore ef.cient decision procedures for reasoning about speci.c data structures [7, 33, 54]. Limitations. \nThe set of data structures that our implementation can handle is limited by the decision procedures that \nwe have currently incorporated into our system. The use of monadic second-order logic over trees as our \nmain logic for reasoning about transitive closure makes it more dif.cult to use our tool for verifying \ndata structures that admit cycles or sharing. Furthermore, our widening technique for inferring new reachability \npredicates only works for .at tree-like structures. It is not appropriate for handling nested data structures \nsuch as lists of lists which may require the analysis to infer nested reachability predicates. Costs \nand gains of automation. In order to estimate the costs and gains of an increased degree of automation, \nwe compared Bohne to TVLA [35], the implementation of three-valued shape analysis [48]. We used TVLA \nversion 3.0 alpha [8] for our comparison. We ran both tools on a set of singly-linked list benchmarks. \nFor each example program we used the same precondition in both tools: heaps that form a forest of acyclic, \nsharing free lists. For TVLA we provided preconditions in the form of sets of three-valued logical structures. \nBohne automatically computed the abstraction of pre\u00adconditions given as logical formulas. We did not \nuse .nite differ\u00adencing [46] to automatically compute predicate updates in TVLA. With .nite differencing \nTVLA was unable to prove preservation of acyclicity of lists in some of the examples. We therefore used \nthe standard abstract domain and abstract transformers for singly\u00adlinked lists that are shipped with \nTVLA (with standard focus as described in [48]). This abstract domain provides high precision for analyzing \nlist-manipulating programs. We checked for proper\u00adties that require such high precision, in order to \nget a meaningful comparison. We checked for absence of null dereferences as well as preservation of acyclicity \nand absence of sharing. All properties where checked in a single run of each analysis. Both tools were \nable to verify these properties for all our benchmarks. The results of our experiments are summarized \nin Table 4. The running times for Bohne are between one and two orders of magnitude higher than for TVLA. \nObserve that almost all time is    benchmark running time (in s) avrg. #abs. states #predicates Bohne \nw/o VC TVLA Bohne TVLA Bohne TVLA   traverse 0.11 0.008 0.179 1.0 8 4 12 create 0.78 0.036 0.133 1.7 \n6 11 12 getLast 0.53 0.012 0.214 2.0 10 7 14 insertBefore 2.48 0.068 0.503 13.5 15 10 18 append 8.95 \n0.048 0.462 1.5 23 13 18 .lter 5.31 0.140 0.600 2.5 19 12 18 partition 149.16 1.238 1.508 3.5 72 15 18 \nreverse 5.52 0.080 0.331 2.0 12 11 14  Table 4. Comparison between Bohne and TVLA. The columns list \ntotal running times, average number of abstract states per location in the .xed point, and total number \nof predicates (we refer to the total number of unary predicates used by TVLA.). The third column shows \nthe running time of Bohne without the time spent in the validity checker, i.e., this would be the total \nrunning time if we had an oracle for checking validity of formulae that would always return instantaneously. \nspent in the decision procedure. Thus, the increase in running time is the price that we pay for automation. \nMore important in the context of this work is the fact that the space consumption of Bohne (measured \nin number of explored ab\u00adstract states) is smaller than TVLA s, in some examples signi.\u00adcantly. TVLA \ns focus operator eagerly splits abstract states (and summary nodes) during .xed point computation in \norder to retain high precision. This potentially leads to an explosion in the num\u00adber of explored abstract \nstates. Instead, our counterexample-guided focus splits abstract states and abstract objects on demand, \nonly if the additional precision is required to rule out some spurious error trace. We believe that this \nis the main reason for the smaller space consumption of Bohne. This believe is supported by our experience \nwith a uniform focus operator similar to TVLA s that we used in an earlier implementation. However, there \nare other factors that play a role, such as the fact that Bohne s abstraction re.nement loop infers a \nsmaller number of predicates, compared to the .xed set of predicates that TVLA uses. A smaller predicate \nset results in a smaller abstract domain. Furthermore, the abstract domains of the two analyses are very \nsimilar, but not equivalent. In particular, abstract objects in our abstract states can be empty. This \nis in contrast to summary nodes in TVLA s three-valued structures which are bound to be non-empty. The \npresence of empty abstract objects can result in more compact abstractions. Hence, a more sturdy conclusion \nas to why the space consumption of Bohne is smaller requires further experiments. 8. Further Related \nWork Shape analysis. Most shape analyses infer quanti.ed invariants of heap programs, either explicitly \nor implicitly. We discuss some of these techniques in the following. Our analysis shares many ideas with \nthree-valued shape anal\u00adysis [48] which inspired our idea of the counterexample-guided focus operator. \nIn the previous section we presented an experi\u00admental comparison of both analyses. We now discuss additional \nrelated work. Recent approaches enable the automatic computation of transfer functions [36, 46] for three-valued \nshape analysis. Some of these approaches are using decision procedures [55]. A method for automated generation \nof predicates using inductive learning has been presented in [37], but is not based on counterexamples. \nA recent direction is the development of parameterized focus oper\u00adators that are .ned-tuned to speci.c \nveri.cation tasks [40, 41, 47]. Our counterexample-guided focus is not only .ne-tuned to a spe\u00adci.c veri.cation \ntask but also to the individual steps of the analysis of a speci.c veri.cation task. Furthermore, this \n.ne-tuning is per\u00adformed automatically. However, the above techniques are explored in the more challenging \nsetting of the veri.cation of concurrent heap programs. Here, user-provided domain-speci.c knowledge \nis often invaluable for obtaining an ef.cient analysis. Shape analyses based on separation logic such \nas [13, 19, 38] are typically tailored towards speci.c data structures and proper\u00adties. This makes them \nscale to programs of impressive size [53], but also limits their application domain. Recent techniques \nintro\u00adduce some degree of parametricity and allow the analysis to auto\u00admatically adapt to speci.c data \nstructure classes [4, 24]. All of these techniques still require manual adaptation of abstract transformers \nto perform materialization for different classes of data structures. Our focus operator performs this \nadaptation automatically. Shape analysis based on abstract regular tree model checking [10] encodes heap \nprograms into tree transducers which can be analyzed using automata-based program analysis techniques. \nThe encoding into tree transducers loses precision if the structures ob\u00adserved in the heap program do \nnot exhibit some regularity. This shape analysis can take advantage of abstraction re.nement tech\u00adniques \nthat have been developed for abstract regular tree model checking [9]. In particular, there is an automata-based \nversion of predicate abstraction that can be combined with abstraction re.ne\u00adment and provide progress \nguarantees. However, these re.nement techniques cannot prevent any loss of precision that is caused by \nthe initial encoding of a heap program into tree transducers. Also, this approach focuses on shape invariants \nof data structures and does not apply to properties such as sortedness. Predicate abstraction. Classical \npredicate abstraction [22] can be seen as an instance of Boolean heap abstraction where all abstraction \npredicates are closed formulas. Our technique of counterexample-guided focus therefore carries over to \nclassical predicate abstraction. The advantages of combining predicate abstraction with shape analysis \nare clearly demonstrated in lazy shape analysis [6]. Lazy shape analysis performs independent runs of \na shape analysis algo\u00adrithm, whose results are then used to improve the precision of predi\u00adcate abstraction. \nThe combined analysis implicitly infers quanti.ed invariants. In contrast, our analysis transcends the \nlazy abstraction technique to the point where it itself becomes effective as a shape analysis. Thus, \nour analysis offers the bene.ts of lazy abstraction (i.e., a high degree of automation and targeted precision) \nalso for the heap-aware analysis. Indexed predicate abstraction [31] uses predicates with free variables \nto infer quanti.ed invariants and is similar to our analysis. However, indexed predicate abstraction \nhas not yet been used for the analysis of heap programs. Heuristics for automatic discovery of indexed \npredicates are described in [32]. The abstract domain of our analysis is more general than the indexed \npredicate abstraction domain, because it contains disjunctions of universally quanti.ed statements. The \npresence of disjunctions avoids loss of precision at join points of the control .ow graph. This is important \nin the context of abstraction re.nement because it enables the analysis to precisely identify spurious \nerror traces in the abstract system. The SLAM tool [3] uses Cartesian abstraction [2] on top of predicate \nabstraction. The loss of precision under Cartesian abstrac\u00adtion is not a decisive factor in standard \npredicate abstraction [1]. However, there are other approximations of abstract transformers that are \nused for performance reasons in actual implementations. A side-effect of such approximations is that \nspurious error traces may not be eliminated by sole re.nement of the abstract domain. SLAM incorporates \nan algorithm developed by Das and Dill [17] as a countermeasure. This algorithm gradually re.nes the \nabstract transformer towards the most precise abstract post for a .xed pred\u00adicate abstraction. In SLAM \nthis algorithm is applied whenever no new predicates can be extracted from a spurious error trace. The \nre.nement of the abstract transformer guarantees that the spurious error trace is eventually eliminated. \nHowever, this algorithm is not appropriate in the context of an abstract domain of quanti.ed asser\u00adtions. \nIt relies on a greedy elimination of spurious abstract transi\u00adtions in the abstract transformer. In predicate \nabstraction these ab\u00adstract transitions are simple conjunctions of predicates. In our set\u00adting they are \nquanti.ed Boolean combinations of predicates. The enumeration of abstract transitions is therefore infeasible. \n Quanti.ed invariants over arrays. For programs over arrays the problem of how to treat disjunctions \nin the inference of quanti.ed invariants is not as accentuated as in the context of heap programs. Techniques \nfor inferring quanti.ed invariants that only apply to programs over arrays include [20, 27, 29, 49]. \nNoticeable excep\u00adtions are [23, 50] which also apply to heap programs. Here the user speci.es predicates \nand templates for the quanti.ed invariants that partially .x the Boolean structure of the inferred invariants. \nThe analysis then automatically instantiates the template parameters. The templates can signi.cantly \nreduce the search-space. However, .nding the right predicates and the right templates is non-trivial \nin the context of heap programs. Extracting predicates from counterexamples. The problem of extracting \ngood predicates from counterexamples for a domain of quanti.ed assertions is orthogonal to the contribution \nof this pa\u00adper. In the setting of quanti.ed invariant inference, the question of how to infer predicates \nfrom a .nite set of spurious counterexam\u00adples that rule out in.nitely many similar spurious counterexamples \n(e.g., resulting from the traversal of a recursive data structure) is open. In our implementation we \nfollowed a practical approach to solve this problem. Techniques for the inference of quanti.ed inter\u00adpolants \n[43] offer a promising alternative. Such techniques could be integrated in our approach following the \nline of interpolation-based abstraction re.nement [5, 27]. 9. Conclusion In this paper, we have addressed \nthe automated inference of quan\u00adti.ed invariants for software veri.cation. The .ne-tuning of the fo\u00adcus \noperator (from the related area of shape analysis) is central to .nding the right ef.ciency-precision \ntradeoff in the underlying pro\u00adgram analysis. We have put forward the idea to use counterexam\u00adples to \nguide the .ne-tuning of the focus operator. We have shown how this idea can be realized in a method and \ntool; preliminary experiments indicate its practical potential. An interesting line of future research \nis the extension of the pre\u00adsented work to the veri.cation of concurrent programs. It seems that here \none should seek the integration of counterexample-guided focus with existing mechanisms for manually \n.ne-tuning parame\u00adterized versions of the focus operator[39]. This would allow the user to incorporate \ndomain-speci.c knowledge (e.g., about syn\u00adchronization) into the analysis.  References [1] T. Ball, \nB. Cook, S. Das, and S. K. Rajamani. Re.ning approximations in software predicate abstraction. In TACAS \n04, pages 388 403, 2004. [2] T. Ball, A. Podelski, and S. K. Rajamani. Boolean and Cartesian abstraction \nfor model checking C programs. In TACAS 01, pages 268 283, 2001. [3] T. Ball and S. K. Rajamani. The \nSLAM project: debugging system software via static analysis. In POPL 02, pages 1 3, 2002. [4] J. Berdine, \nC. Calcagno, B. Cook, D. Distefano, P. O Hearn, T. Wies, and H. Yang. Shape analysis for composite data \nstructures. In CAV 07, pages 178 192, 2007.  [5] D. Beyer, T. A. Henzinger, R. Majumdar, and A. Rybalchenko. \nPath invariants. In PLDI, pages 300 309, 2007. [6] D. Beyer, T. A. Henzinger, and G. Th\u00b4eoduloz. Lazy \nshape analysis. In CAV 06, pages 532 546, 2006. [7] J. D. Bingham and Z. Rakamaric. A logic and decision \nprocedure for predicate abstraction of heap-manipulating programs. In VMCAI 06, pages 207 221, 2006. \n[8] I. Bogudlov, T. Lev-Ami, T. W. Reps, and M. Sagiv. Revamping TVLA: Making Parametric Shape Analysis \nCompetitive. In CAV 07, pages 221 225, 2007. [9] A. Bouajjani, P. Habermehl, A. Rogalewicz, and T. Vojnar. \nAbstract regular tree model checking. ENTCS, 149(1):37 48, 2006. [10] A. Bouajjani, P. Habermehl, A. \nRogalewicz, and T. Vojnar. Abstract regular tree model checking of complex dynamic data structures. In \nSAS 06, pages 52 70, 2006. [11] R. E. Bryant. Graph-Based Algorithms for Boolean Function Manipulation. \nTC, 35(10):677 691, 1986. [12] S. Chaki, E. M. Clarke, A. Groce, S. Jha, and H. Veith. Modular veri.cation \nof software components in C. In ICSE 03, pages 385 395, 2003. [13] B.-Y. E. Chang, X. Rival, and G. C. \nNecula. Shape analysis with structural invariant checkers. In SAS 07, pages 384 401, 2007. [14] E. M. \nClarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith. Counterexample-Guided Abstraction Re.nement. In CAV \n00, pages 154 169, 2000. [15] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice model \nfor static analysis of programs by construction or approxima\u00adtion of .xpoints. In POPL 77, pages 238 \n252, 1977. [16] P. Cousot and R. Cousot. Formal language, grammar and set\u00adconstraint-based program analysis \nby abstract interpretation. In FPCA 95, pages 170 181, 1995. [17] S. Das and D. L. Dill. Successive approximation \nof abstract transition relations. In LICS 01, pages 51 60, 2001. [18] L. de Moura and N. Bj\u00f8rner. Z3: \nAn Ef.cient SMT Solver. In TACAS 08, 2008. [19] D. Distefano, P. O Hearn, and H. Yang. A local shape \nanalysis based on separation logic. In TACAS 06, pages 287 302, 2006. [20] C. Flanagan and S. Qadeer. \nPredicate abstraction for software veri.cation. In POPL 02, pages 191 202, 2002. [21] D. Gopan, T. W. \nReps, and S. Sagiv. A framework for numeric analysis of array operations. In POPL 05, pages 338 350, \n2005. [22] S. Graf and H. Sa\u00a8idi. Construction of Abstract State Graphs with PVS. In CAV 97, pages 72 \n83, 1997. [23] S. Gulwani, B. McCloskey, and A. Tiwari. Lifting abstract interpreters to quanti.ed logical \ndomains. In POPL 08, pages 235 246, 2008. [24] B. Guo, N. Vachharajani, and D. I. August. Shape analysis \nwith inductive recursion synthesis. In PLDI 07, pages 256 265, 2007. [25] T. A. Henzinger, R. Jhala, \nR. Majumdar, and G. Sutre. Lazy Abstraction. In POPL 02, pages 58 70, 2002. [26] T. A. Henzinger, R. \nJhala, R. Majumdar, and G. Sutre. Software veri.cation with BLAST.In SPIN 03: Model Checking of Software, \npages 235 239. 2003. [27] R. Jhala and K. L. McMillan. Array abstractions from proofs. In CAV 07, pages \n193 206, 2007. [28] N. Klarlund and A. M\u00f8ller. MONA Version 1.4 User Manual. BRICS Notes Series NS-01-1, \nUniversity of Aarhus, January 2001. [29] L. Kovacs and A. Voronkov. Finding loop invariants for programs \nover arrays using a theorem prover. In FASE 09, pages 470 485, 2009. [30] V. Kuncak. Modular Data Structure \nVeri.cation. PhD thesis, EECS Department, Massachusetts Institute of Technology, February 2007. [31] \nS. K. Lahiri and R. E. Bryant. Constructing quanti.ed invariants via predicate abstraction. In VMCAI \n04, pages 267 281, 2004. [32] S. K. Lahiri and R. E. Bryant. Indexed predicate discovery for unbounded \nsystem veri.cation. In CAV 04, 2004. [33] S. K. Lahiri and S. Qadeer. Back to the future: revisiting \nprecise program veri.cation using SMT solvers. In POPL 08, pages 171 182, 2008. [34] P. Lam, V. Kuncak, \nand M. Rinard. Hob: A tool for verifying data structure consistency. In CC 05, 2005. [35] T. Lev-Ami. \nTVLA: A Framework for Kleene Based Logic Static Analyses. Master s thesis, Tel-Aviv University, Israel, \n2000. [36] T. Lev-Ami, N. Immerman, and M. Sagiv. Abstraction for shape analysis with fast and precise \ntransformers. In CAV 06, pages 533 546, 2006. [37] A. Loginov, T. Reps, and M. Sagiv. Abstraction re.nement \nvia inductive learning. In CAV 05, 2005. [38] S. Magill, J. Berdine, E. M. Clarke, and B. Cook. Arithmetic \nstrengthening for shape analysis. In SAS 07, pages 419 436, 2007. [39] R. Manevich. Partially Disjunctive \nShape Analysis. PhD thesis, Tel-Aviv University, Israel, 2009. [40] R. Manevich, J. Berdine, B. Cook, \nG. Ramalingam, and M. Sagiv. Shape analysis by graph decomposition. In TACAS 07, pages 3 18, 2007. [41] \nR. Manevich, M. Sagiv, G. Ramalingam, and J. Field. Partially disjunctive heap abstraction. In SAS 04, \npages 265 279, 2004. [42] M. Marron, D. Kapur, D. Stefanovic, and M. V. Hermenegildo. A static heap analysis \nfor shape and connectivity. In LCPC, pages 345 363, 2006. [43] K. L. McMillan. Quanti.ed invariant generation \nusing an interpolating saturation prover. In TACAS 08, volume 4963, pages 413 427, 2008. [44] A. Podelski \nand A. Rybalchenko. ARMC: the logical choice for software model checking with abstraction re.nement. \nIn PADL 07, pages 245 259, 2007. [45] A. Podelski and T. Wies. Boolean Heaps. In SAS 05, pages 267 282, \n2005. [46] T. Reps, M. Sagiv, and A. Loginov. Finite differencing of logical formulas for static analysis. \nIn ESOP 03, pages 380 398, 2003. [47] N. Rinetzky, A. Poetzsch-Heffter, G. Ramalingam, M. Sagiv, and \nE. Yahav. Modular shape analysis for dynamically encapsulated programs. In ESOP 07, pages 220 236, 2007. \n [48] M. Sagiv, T. Reps, and R. Wilhelm. Parametric shape analysis via 3-valued logic. TOPLAS, 24(3):217 \n298, 2002. [49] M. N. Seghir, A. Podelski, and T. Wies. Abstraction re.nement for quanti.ed array assertions. \nIn SAS 09, 2009. [50] S. Srivastava and S. Gulwani. Program veri.cation using templates over predicate \nabstraction. In PLDI 09, 2009. [51] T. Wies. Symbolic Shape Analysis. PhD thesis, University of Freiburg, \nFreiburg, Germany, 2009. [52] T. Wies, V. Kuncak, P. Lam, A. Podelski, and M. Rinard. Field Constraint \nAnalysis. In VMCAI 06, 2006. [53] H. Yang, O. Lee, J. Berdine, C. Calcagno, B. Cook, D. Distefano, and \nP. W. O Hearn. Scalable shape analysis for systems code. In CAV 08, pages 385 398, 2008. [54] G. Yorsh, \nA. M. Rabinovich, M. Sagiv, A. Meyer, and A. Bouajjani. A Logic of Reachable Patterns in Linked Data-Structures. \nIn FOSSACS 06, pages 94 110, 2006. [55] G. Yorsh, T. Reps, and M. Sagiv. Symbolically computing most\u00adprecise \nabstract operations for shape analysis. In TACAS 04, 2004. [56] K. Zee, V. Kuncak, and M. Rinard. Full \nFunctional Veri.cation for Linked Data Structures. In PLDI 08, pages 349 361, 2008. [57] K. Zee, V. Kuncak, \nand M. C. Rinard. An integrated proof language for imperative programs. In PLDI 09, pages 338 351, 2009. \n \n\t\t\t", "proc_id": "1706299", "abstract": "<p>The automated inference of quantified invariants is considered one of the next challenges in software verification. The question of the right precision-efficiency tradeoff for the corresponding program analyses here boils down to the question of the right treatment of disjunction below and above the universal quantifier. In the closely related setting of shape analysis one uses the focus operator in order to adapt the treatment of disjunction (and thus the efficiency-precision tradeoff) to the individual program statement. One promising research direction is to design parameterized versions of the focus operator which allow the user to fine-tune the focus operator not only to the individual program statements but also to the specific verification task. We carry this research direction one step further. We fine-tune the focus operator to each individual step of the analysis (for a specific verification task). This fine-tuning must be done automatically. Our idea is to use counterexamples for this purpose. We realize this idea in a tool that automatically infers quantified invariants for the verification of a variety of heap-manipulating programs.</p>", "authors": [{"name": "Andreas Podelski", "author_profile_id": "81100130920", "affiliation": "University of Freiburg, Freiburg, Germany", "person_id": "P1911086", "email_address": "", "orcid_id": ""}, {"name": "Thomas Wies", "author_profile_id": "81384598627", "affiliation": "Ecole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland", "person_id": "P1911087", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706330", "year": "2010", "article_id": "1706330", "conference": "POPL", "title": "Counterexample-guided focus", "url": "http://dl.acm.org/citation.cfm?id=1706330"}