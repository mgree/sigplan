{"article_publication_date": "01-17-2010", "fulltext": "\n Sequential Veri.cation of Serializability H. Attiya G. Ramalingam N. Rinetzky Technion Microsoft Research \nIndia Queen Mary University of London hagit@cs.technion.ac.il grama@microsoft.com maon@dcs.qmul.ac.uk \n Abstract Serializability is a commonly used correctness condition in concur\u00adrent programming. When a \nconcurrent module is serializable, cer\u00adtain other properties of the module can be veri.ed by considering \nonly its sequential executions. In many cases, concurrent modules guarantee serializability by using \nstandard locking protocols, such as tree locking or two-phase locking. Unfortunately, according to the \nexisting literature, verifying that a concurrent module adheres to these protocols requires considering \nconcurrent interleavings. In this paper, we show that adherence to a large class of lock\u00ading protocols \n(including tree locking and two-phase locking) can be veri.ed by considering only sequential executions. \nThe main consequence of our results is that in many cases, the (manual or automatic) veri.cation of serializability \ncan itself be done using sequential reasoning. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation; D.1.3 [Programming Tech\u00adniques]: Concurrent Programming; F.3.2 [Logics \nand Meanings of Programs]: Semantics of Programming Languages Program analysis General Terms Theory, \nVeri.cation Keywords Concurrency, Veri.cation, Serializability, Reduction, Modular Analysis 1. Introduction \nIn this paper, we study the dual problems of using sequential rea\u00adsoning to verify that a concurrent \nmodule is serializable and the use of serializability to enable sequential reasoning of a concur\u00adrent \nmodule. A concurrent module encapsulates shared data with a set of procedures, which may be invoked by \nconcurrently exe\u00adcuting clients (threads). A key challenge in the design and analysis of concurrent modules \nis dealing with all possible interleavings of concurrently executing threads (i.e., procedure invocations \nwhose executions overlap). Serializability [5, 28] is a commonly desired and important cri\u00adterion for \nconcurrent modules. Informally, a concurrent module is said to be serializable if any (potentially interleaved) \nexecution of a number of procedure invocations is equivalent to some sequential execution of those procedure \ninvocations (one after another). One of the attractions of the serializability property is that it enables \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . \n$10.00 sequential reasoning: when a module is serializable certain proper\u00adties can be veri.ed by considering \nonly non-interleaved executions of the module. This is equivalent to reasoning about the module assuming \nthat it is used by a single-threaded (sequential) client. One well-known way of ensuring serializability \nis to use lock\u00ading protocols such as tree locking (TL)[15], two-phase locking (2PL) [23], or hand-over-hand \nlocking, which is an instance of dy\u00adnamic tree locking (DTL). If a concurrent module adheres to one of \nthese protocols, then it is serializable, implying that a sequential reduction can be applied to the \nveri.cation of other properties. Sequential Reductions for Serializability. To the best of our knowledge, \nno prior work has addressed the question of whether sequential reductions apply to the problem of verifying \nthat a mod\u00adulefollowsalockingprotocolsuchas TL/2PL/DTL.Ourresultsgive a positive answer to this question. \nRoughly stated, we establish that if the execution of every procedure satis.es the TL/2PL/DTL protocol \nin the absence of any interleaving of procedure executions then any interleaved execution of the procedures \nalso satis.es the TL/2PL/DTL pro\u00adtocol. Informally, sequential TL/2PL/ DTL implies concurrent TL/2PL/ \nDTL . In fact, we generalize this result for a class of lo\u00adcal locking protocols LP that ensure con.ict-serializability \n[5]; this class includes TL and 2PL. Our investigation of this question goes through the considera\u00adtion \nof two types of sequential executions: A non-interleaved exe\u00adcution is an execution in which every procedure \ninvocation is con\u00adtiguous: i.e., there is no interleaving of instructions correspond\u00ading to different \nprocedure invocations. An almost-complete non\u00adinterleaved execution is a non-interleaved execution in \nwhich all procedure invocations (except perhaps the last one) complete. Our .rst reduction is to non-interleaved \nexecutions: we show that if the set of all non-interleaved executions of a module sat\u00adisfy LP then the \nset of all executions of the module satisfy LP. Our second, and more complicated reduction, is to almost-complete \nnon-interleaved executions: we prove that if the set of all almost\u00adcomplete non-interleaved executions \nof a module satisfy LP then the set of all executions of the module satisfy LP, under an assump\u00adtion \nthat (roughly) requires each procedure invocation to complete when it executes solo. Sequential Analysis \nof Serializable Modules. We next study the implication of our reductions for sequential analysis of a \nserializ\u00adable module, e.g., to verify properties such as memory safety. While the idea that serializability \n(or atomicity) simpli.es analysis is not new (e.g., see [13]), we establish several new results in this \nregard. We describe a class of properties, called transaction-local, whose veri.cation can exploit a \nsequential reduction. We describe differ\u00adent types of sequential analyses, enabled by the different reduction \ntheorems we establish, with differing preconditions. We also report on a preliminary evaluation of the \neffectiveness of the reduction techniques in verifying a couple of examples involving concurrent lists \nand trees.  Contributions. Our results enable modular and sequential reason\u00ading, whether manual or automated, \nabout concurrent modules both in verifying that a module adheres to a locking protocol and in de\u00advelopment \nof algorithms for such modules. In the veri.cation con\u00adtext, the reduction enables simpler and more ef.cient \nveri.cation algorithms: e.g., it justi.es the use of sequential Hoare Logic or sequential type systems \nor sequential abstract interpretation to ver\u00adify that the module adheres to a locking protocol. Similarly, \nin the development context, a developer wishing to add, e.g.,a new pro\u00adcedure to swap two adjacent elements \nin a list to a module that uses hand-over-hand locking, does not have to worry about concurrent interleaving \nwith other methods. The developer can rely on mod\u00adule invariants and think sequentially in designing \nthe algorithm (as happens when using coarse-grained locking). One of the implica\u00adtions of our result \nis that an automated tool [8] for synthesizing locking code to ensure the atomicity of the methods of \na concur\u00adrent module can also use sequential reasoning if it utilizes locking protocols such as TL/2PL/DTL. \nThe result we present is a fundamental one about commonly\u00adused locking protocols. Readers might wonder \nif it follows directly from classical serializability theory, e.g., [5, 28]. However, the clas\u00adsical \nresult is about a single execution and states that if, in an inter\u00adleaved execution of transactions (procedure \ninvocations, in our ter\u00adminology), every transaction follows, e.g., TL, then this particular execution \nis serializable. We are, however, interested in checking whether a given module follows TL: i.e., we \nwish to check the an\u00adtecedent of the above result for the set of all interleaved executions that can \nbe produced by the module. The classical results do not let us simplify this question. (This was not \na concern in databases, where transactions are governed by a centralized concurrency con\u00adtrol monitor, \nwhich coordinates and controls their actions, at run\u00adtime, making sure they follow the locking protocol.) \nNote. Due to space limitations, several proofs have been omitted from the paper and appear in [2].  \n2. The Challenge We start with three illustrative examples shown in Fig. 1. EXAMPLE 1. Module T, shown \nin Fig. 1(a), follows the TL pro\u00adtocol. Informally, the TL protocol requires that accesses to every shared \nvariable be protected by a lock, and that the locks be ordered in a forest. Locks should be acquired \nand released in a hand-over\u00adhand order; a child lock is acquired before its parent is released. (For \na more precise de.nition, see Sec. 4.2). Module T has a single procedure (t) and three shared variables, \nQ, W,and E. Each variable also functions as its own lock. The tree\u00adordering on the shared variables is \nas follows: Q is the parent of W, which is the parent of E. Let us .rst analyze sequential executions \nof this module: i.e.,we consider executions where procedure t may be invoked multiple times, but a new \ninvocation of t starts only after the previous invocation of t terminates. We can verify that such executions \nsatisfy the TL protocol. This veri.cation, however, depends on certain invariants that hold in such sequential \nexecutions. E.g., verifying that the access to E happens only when the lock on E is held requires establishing \nthat the second assertion (q== w) holds. The latter depends on the fact that Q and W have the same value \nwhen read by procedure t. Concurrency, however, complicates the picture signi.cantly. In\u00advariants of \nsequential executions need not be invariants of concur\u00adrent executions. What do we need to do to verify \nthat this module follows TL even in concurrent executions? Our results show how we can use the sequential \nreasoning de\u00adscribed above to verify that module T follows TL even in concur\u00adrent executions, inferring \nalong the way that, from the viewpoint of every thread, Q== W always holds whenever a thread holds the \nlocks of both variables, and that the other assertions embedded in the module are also correct in concurrent \nexecutions. EXAMPLE 2. Module DT implements a concurrent binary search tree following the hand-over-hand \n(lock-coupling) locking proto\u00adcol. Fig. 1(b) shows only the delete procedure of the module. Informally, \nthe hand-over-hand protocol requires that every ac\u00adcess to a .eld of a dynamically allocated object be \nprotected by acquiring the lock of the object and that the heap be shaped like a tree/forest. (However, \nthe forest shape requirement is allowed to be temporarily violated in the parts of the heap locked by \na thread, which typically happens during heap updates.) Locks should be ac\u00adquired and released in a hand-over-hand \norder. (For a more pre\u00adcise de.nition, see Sec. 6.1). Any sequential execution of module DT correctly \nfollows the locking protocol. Concurrency, again, complicates the picture. The tree ordering of the locks \nis determined by the shape of the heap. Thus verifying that the module follows the locking protocol is \ninherently linked with a shape analysis of the heap. Note, however, that in concurrent executions there \nmay be an unbounded number of threads simultaneously operating on different parts of the data\u00adstructure. \nAs a result, a concurrent shape analysis is typically more expensive and less precise than a sequential \nshape analysis. We show that sequential reasoning can be used to verify that module DT follows DTL even \nin concurrent executions. In particu\u00adlar, this property can be veri.ed by a sequential shape analysis. \nEXAMPLE 3. We now present an example where the use of sequen\u00adtial reasoning leads to results not valid \nfor concurrent executions. Fig. 1(c) shows module M whichhas twosharedvariables X and Y, and two procedures \nm and f. Let us check whether this mod\u00adule follows the 2PL protocol and also check whether the assertion \nY==0 in procedure f is true. Informally, the 2PL protocol re\u00adquires that accesses to every shared variable \nbe protected by a lock, and that after a lock is released, no lock is acquired (see Sec. 4.2). Let us \n.rst consider only sequential executions. The assertion Y==0 is true in the initial state of the module, \nand any complete execution of procedure m or procedure f preserves this invariant. As a result, the module \nsatis.es 2PL. However, this assertion does not hold in concurrent executions of M. Procedure m can, in \nfact, break this conjectured invariant, set\u00adting Y to be 1. When m does this, it also ends up in a non-terminating \nloop. As a result, the broken invariant is never exposed to a call of f in the sequential setting. In \na concurrent setting, however, an in\u00advocation of f by another thread can observe this broken invariant. \nAs a result, this invariant fails to hold for concurrent executions. When this invariant breaks, procedure \nf will no longer satisfy 2PL. Speci.cally, it ends up acquiring a lock after releasing a lock. We show \nthat if all procedures of the module are guaranteed to terminate then the sequential reduction described \nearlier is valid for verifying adherence to locking protocols such as 2PL. Rather than checking for termination \nin all concurrent execu\u00adtions, we show that it suf.ces to check for termination only in se\u00adquential executions. \nThis is weaker than solo-termination [12].  3. Preliminaries In Sec. 3.1, we outline our programming \nmodel. In Sec. 3.2, we formalize the notions of executions and schedules. In Sec. 3.3, we review the \nnotion of con.ict-serializability and state some of its known properties. 3.1 Programming Model We assume \na programming language which allows for writing concurrent modules. For brevity, some of the standard \ntechnical details are omitted and can be found in [2].  module T; int Q=0, W=0, E=0; void t() { int \nq=0, w=0, e=0; acquire(Q); q = Q++; acquire(W); // assert(Q==W+1) release(Q); w = W++; // assert(q==w) \nif (isEven(q)) acquire(E); release(W); if (isEven(w)) { e = E++; release(E); } } module DT; NR; bool \ndelete(int k){Np =R; p.acquire(); N c = p.r; bool ret = false; while (c != null){c.acquire(); if (c.k \n== k) break; p.release(); p= c; if (p.k < k) c = p.l; else c = p.r; } if (c != null){ret = true; N \ncr = c.r; c.r = null; N cl = c.l; c.l = null; Nt =cr; if (cr == null) t = cl; else if (cl != null){ \n N rp = cr; rp.acquire(); N rc = rp.l; while (rc != null){ rp.release(); rp = rc; rp.acquire(); rc = \nrp.l; } rp.l = cl; rp.release(); } } if (p.k < k) p.r=t; else p.l=t; } p.release(); if (c != null) \nc.release(); return ret; } module M; int X=0, Y=0; void m() { acquire(X); if (*) {acquire(Y); Y=1; \nrelease(Y); while (true) skip; } release(X); } void f() {int y; acquire(Y); // assert(Y==0) y= Y; release(Y); \nif (y!=0) acquire(Y); if (y!=0) release(Y); } (a) Module T (b) Module DT (c) Module M Figure 1. Concurrent \nmodules. (*) represents non-deterministic branching. N is the type of a pointer to the user de.ned type \n{N l,r; int k}.The assert statements are used for expository reasons only. They are not intended to be \nexecutable code, and thus remarked. Concurrent modules. A concurrent module encapsulates data that is \nprivate to the module and de.nes a set of procedures that may be invoked by clients of the module, potentially \nconcurrently. The module variables are global to the module, i.e.,sharedby all procedure invocations. \nProcedures may also have local variables, which are private to the invocation of the procedures. Without \nloss of generality, we treat each invocation of a procedure as a thread. Synchronization. Threads communicate \nusing a shared memory comprised of global variables and an (unbounded) heap. Since procedure invocations \nmay execute concurrently and access shared data, they utilize locks for concurrency control. A lock can \nbe a global variable or a heap allocated object. Locks are exclusive, i.e., a lock can be held by at \nmost one thread at a time. The execution of a thread trying to acquire a lock which is held by another \nthread is blocked until a time when the lock is available, i.e., is not held by any thread. 3.1.1 Syntax \nVariables can be either of type boolean, integer,or a pointer to a user-de.ned type. We assume the syntactic \ndomains x .V = VL lVG of variable identi.ers, local variable identi.ers x, y .VL and global variable \nidenti.ers X, Y .VG, f .F of .eld identi.ers, p .P of procedure identi.ers, and . .K of pro\u00adgram points. \nWe assume that variables, procedures, and program points have unique identi.ers in every program. Instead \nof committing ourself to a particular syntax, we assume that body of a procedure is represented in a \nstandard way using a control-.ow graph. We refer to the vertices of a control-.ow graph as program points \n. .K. The edges of a control-.ow graph are annotated with primitive instructions. Besides the following \nas\u00adsumptions, the exact set of instruction is of no particular impor\u00adtance. We use assume statements \nto encode conditionals in the usual way: i.e., a control-.ow edge is annotated with the statement assume \ncond to indicate that the branch is taken only if cond is true. Without loss of generality, we allow \nreferences to global vari\u00adables and heap objects only in load/store instructions that copy val\u00adues between \nthese and local variables and acquire/release v . Val = Loc lZl{tt, . , null} . .E = VL '. Val h .H = \nLoc '.F '. Val l .L = Loc lVG s .S = K\u00d7E\u00d7 2L s . S =(VG '. Val) \u00d7H\u00d7 (T '.S) Figure 2. Semantic domains. \ninstructions. In particular, the condition of an assume statement cannot refer to a global variable. \n 3.1.2 Semantics Memory states. Fig. 2 de.nes the semantic domains of memory states of module m and \nthe meta-variables ranging over them. We assume the semantic domain t .T of thread identi.ers. A memory \nstate s = (g,h, e). S of a concurrent module cm is a triplet: g is the global environment which records \nthe values of the module-global variables. h assigns values to .elds of dynamically allocated objects. \nA value v .VAL can be either a location, an integer, a boolean value, or null. e associates a thread \nt with its thread local state e(t).A thread-local state s = (., ., L). S is a triplet: . is the value \nof the thread s program counter, . records the values of its local variables, and L is the thread s lock \nset which records the locks that the thread holds. A lock can be either a global variable or a location. \nOperational semantics. The behavior of a concurrent module can tr be described by a transition relation \n. S \u00d7T \u00d7 (K\u00d7K) \u00d7 S l{.} that interleaves the execution of different threads. (. is (t,e)tr a specially \ndesignated error state.) A transition s -. s' . represents the fact that s can be transformed into s' \nvia thread t executing the instruction annotating control-.ow edge e.Given a (t,e) transition s -. s', \nwe say that the transition is executed by t. We present the (rather mundane) formal de.nition of transitions \npertaining to a speci.c set of primitive instructions in [2]. The statement acquire Y attempts to acquire \na lock on global vari\u00adable Y. The statement x.acquire() attempts to acquire a lock on the heap object \npointed to by local variable x.The assume(b) instruction acts as skip when executed on a state s which \nsat\u00adis.es b.If b does not hold in s, the corresponding transition is not enabled. A memory fault (e.g., \nnull-dereference) causes a transition to an error state.   3.2 Executions and Schedules Enabled execution \nstep. We refer to a pair (t, e) consisting of a thread identi.er and a control-.ow edge as an execution \nstep. We say that (t, e) is enabled in state s if there exists some transi\u00ad (t,e) s ' program-counter \nof t is at the source of edge e (in state s). Then, (t, e) is disabled in s iff e is labeled with either \nan assume state\u00adment whose condition evaluates to false or an acquire statement of a lock that is currently \nheld by another thread. tion s . and thatitis disabled otherwise. Assume that the t1 Executions. An execution \np is a sequence of transitions s0 . tk s1 ... . sk that starts at the initial state (i.e., s0 = s\u00d8) with \nthe target of every transition being the same as the source of the next transition. (When discussing \nexecutions, we may omit the labels on the transitions if no confusion is likely.) The sub-execution of \nthread t in an execution p is the subse\u00adquence of transitions executed by thread t in p. The transaction \nTi executed by a thread ti in an execution p is the sequence of control-.ow edges annotating the transitions \nexecuted by t. Note that a transaction is a path in the control .ow graph of the procedure p invoked \nby t, starting at p s entry node. NI-executions. An execution is non-interleaved (abbreviated NI\u00adexecution) \nif instructions of different threads are not interleaved, i.e., for every pair of threads ti .tj , either \nall the transitions = executed by ti come before any transition executed by tj ,or vice versa. ACNI-executions. \nA thread is said to have completed in an ex\u00adecution if its program counter in the .nal state is at the \nexit node of the corresponding procedure. An execution is complete if ev\u00adery thread in its initial state \nhas completed. An execution is com\u00adplete non-interleaved (abbreviated CNI-execution) if it is both com\u00adplete \nand non-interleaved. An execution is almost-complete non\u00adinterleaved (abbreviated ACNI-execution) if \nit is non-interleaved, and all but the thread executing last has completed. Schedules. A schedule F= \n(t1,e1),..., (tk,ek) is a se\u00adquence of execution steps. F is valid if for every thread identi\u00ad.er t, \nthe subsequence of edges paired with t comprise a path in a control .ow graph of a procedure p starting \nat p s entry node. F is feasible if it is induced by some execution p, i.e., (t1,e1)(tk,ek) p = s0 -. \ns1 ... -. sk. 3.3 Con.ict Serializability Well locked executions. An execution is well-locked if a thread \nnever accesses a global variable or a .eld of a dynamically allo\u00adcated object without holding its protecting \nlock. (In database ter\u00adminology, well-lockedness is referred to as obeying the access rules [5].) In \nthis paper, we assume that every global variable also acts as its own protecting lock and that every \ndynamically allocated object also acts as the protecting lock of its own .elds. Con.ict serializability. \nGiven an execution, we say that two tran\u00adsitions con.ict if (i) they are executed by two different threads, \n(ii) they access some common global variable or a heap allocated object. The usual de.nition of con.ict \nrequires at least one of the con.icting instruction to be a write (to the global variable or ob\u00adject). \nHowever, this simpler de.nition suf.ces for us, since we use exclusive locks for reading as well. (Note \nthat a lock acquire/release instruction is considered to be a write of the corresponding lock.) Executions \np1 and p2 are con.ict-equivalent if they include the same transactions and they agree on the order between \ncon.ict\u00ading transitions. (I.e., the ith transition of a thread t precedes and con.icts with the jth transition \nof thread t ' in p1 iff the former precedes and con.ict with the latter in p2.) Con.ict-equivalence ensures \nview equivalence [5], i.e., every transaction in con.ict\u00adequivalence executions reads and writes the \nsame sequence of val\u00adues to and from the global variables. An execution is con.ict-serializable if it \nis con.ict-equivalent with a non-interleaved execution. A module is con.ict-serializable if all of its \nexecutions are con.ict-serializable. Since con.ict-equivalent executions produce the same state, we have \nthe next lemma. LEMMA 1. Let p and p ' be con.ict-equivalent (well-locked) exe\u00adcutions. (a) For any thread \nt, the set of locks held by t after p is the same as the set of locks held by t after p ' . (b) The execution \nof (t, e) is enabled after p iff it is enabled after p ' . The next lemma is handy for NI-reduction, \nas it allows to move a step earlier in the execution. LEMMA 2. Let pni = pP ptpS be a well-locked NI-execution, \nwhere pt is the sub-execution by a thread t. If the execution step (t, e) is enabled after pni, then \nit is also enabled after pP pt.  4. NI-Reductions for Static Protocols We wish to verify that a given \nmodule is con.ict-serializable, by verifying that it adheres to a locking protocol, e.g., TL or 2PL, \nthat guarantees con.ict-serializability. However, we would like to do this by considering only the module \ns almost-complete non\u00adinterleaved executions rather than all the executions it can generate. We take \na two-step approach towards our goal. The .rst step is a reduction to non-interleaved executions (NI-reduction): \nwe show for a class of locking protocols that if every NI-execution of the module satis.es the protocol \nthen all executions of the module satis.es the protocol. The second step is a reduction to almost\u00adcomplete \nnon-interleaved executions (ACNI-reduction): we show for a class of locking protocols that if every ACNI-execution \nof the module satis.es the protocol then all executions of the module satis.es the protocol. We .rst \nestablish our reductions in a static setting where we assume that the program does not use the heap (dynamic \nmemory allocation) or pointers. This restriction is just a convenience in the case of 2PL: the reductions \ncarry over even in the presence of dynamic memory allocation. However, extending the reductions to handle \ndynamic memory allocation is more challenging in the case of TL because this requires changes to the \nprotocol itself. Section 6 presents our reductions for Dynamic Tree Locking (DTL). 4.1 Transaction-Local \nProperties Given a memory state s = (g,h,e),the thread-owned state of thread t,de.nedby ownedt (s)= (g|L,h|L,e(t)) \nwhere e(t)= (., ., L), is its own thread-local state in s and the projection of the global state on the \nlocks that t holds. The thread-owned view of a thread t executing a transaction T in an execution p is \nobtained by replacing every state s in the corresponding t s sub-execution by ownedt (s). We now de.ne \na certain class of properties of executions. To avoid restricting ourself to any particular logic we \nsimply identify a property f (of executions) with the set of executions that satisfy the property. Similarly, \nwe identify a property f ' of thread-owned views with the set of thread-owned views that satisfy the \nproperty.  We say that a property f of executions is transaction-local if (a) there is a property f \n' of thread-owned views such that an execution p is in f iff all the thread-owned views of all the threads \nlaunched in p are in f ' ,and (b) f is pre.x-closed: if p is in f, then every pre.x of p is in f. For \nany transaction-local property f of executions, we will use fT to denote the property f ' of thread-owned \nviews guaranteed to exist by the above de.nition.  4.2 Local Con.ict Serializable Locking Protocols \nA local con.ict-serializable locking protocol (abbreviated LCS\u00adlocking-protocol) is a transaction-local \nproperty that guarantees well-lockedness and con.ict-serializability. In other words, a prop\u00aderty LP \nof executions is an LCS-locking-protocol iff (i) LP is transaction-local and (ii) p . LP implies that \np is well-locked and con.ict-serializable. A module follows a locking protocol LP if ev\u00adery possible \nexecution of the module satis.es LP. There are two important instances of LCS-locking protocols. Two-phase \nlocking (2PL) requires each execution p to be well\u00adlocked and to ensure that every thread executes in \ntwo phases: a growing phase, in which the thread may acquire locks but not release any locks, followed \nby a shrinking phase, in which the thread may release locks, but may not acquire locks. If an execution \nfollows 2PL then it is con.ict-serializable [23]. Tree locking (TL) assumes that locks are organized \nin the shape of a forest (collection of trees). An execution p follows the TL policy if it is well locked \nand every thread (i) can acquire a lock only if it holds the lock on its parent in the tree, unless the \nlock is the .rst acquired by the transaction, and (ii) does not acquire a lock that it has previously \nreleased. If an execution follows TL then it is con.ict-serializable [15]. 4.3 NI-Reduction for LCS-Locking-Protocols \nLEMMA 3. Let LP be an LCS-locking-protocol. Let pni = pP ptpS be a non-interleaved execution, where pt \nis the sub-execution of a t,ethread t.Let pni . s be an execution that extends pni by a single t,e transition. \nIf pP ptpS satis.es LP and pP pt . s ' satis.es LP,then t,epP ptpS . s also satis.es LP. THEOREM 4. If \nevery NI-execution of a module satis.es an LCS\u00adlocking-protocol LP, then every execution of the module \nsatis.es LP. Proof: The proof is by induction on the length of p. The base case is when the trace is \nempty, which is immediate. t,e For the induction step assume that p ' = p . s ' ,where s ' is the state \nproduced by a thread t executing an instruction st annotating e. By the induction hypothesis, execution \np follows the locking protocol. The locking protocol guarantees con.ict-serializability. Thus, there \nexists a non-interleaved execution pni which is con.ict\u00adequivalent to p. By Lemma 1, the execution of \n(t, e) is enabled after pni as well. Let pni be of the form pP ptpS where pt is the sub-execution by \nthread t. By Lemma 2, (t, e) is also enabled after pP pt. Since pP pt.(t, e) is an NI-execution, it satis.es \nLP (by assump\u00adtion). By Lemma 3, pni.(t, e) satis.es LP as well. Since p satis.es LP,and p and pni are \ncon.ict-equivalent (and produce the same state), it follows that p.(t, e) satis.es LP as well. Since \nTL and 2PL are LCS-locking-protocols, this theorem shows that a module can be veri.ed to follow TL or \n2PL by con\u00adsidering only its NI-executions.  5. ACNI-Reductions for Static Protocols In this section, \nwe show how to improve upon NI-reduction. We would like to show that all executions of a concurrent module \nsat\u00adisfy an LCS-locking-protocol LP if and only if all ACNI-executions of the module satisfy LP. Unfortunately, \nthis goal is too ambitious, because of the possi\u00adbility of procedure invocations that do not terminate. \nAs the exam\u00adple in Fig. 1(c) shows, it is possible to have a procedure invocation T whose changes to \nshared data are visible to other procedure in\u00advocations even though T will never terminate. This can \nlead to exe\u00adcution behaviors that can never be observed in an almost-complete non-interleaved execution. \nTherefore, we will settle for a slightly weaker result. We say that an ACNI-execution of a module is \ncompletable if it is the pre.x of a complete non-interleaved execution (CNI-execution) of the module. \nWe will show that ACNI-reduction holds for a module as long as every ACNI-execution of the module is \ncompletable. (One way to show that every ACNI-execution is completable is to use a sequential termination \nanalysis to show that the last procedure invocation of an ACNI-execution is guaranteed to terminate in \nthe absence of any other concurrent procedure invocation. Our ACNI\u00adreduction theorem can also be used \ndirectly, without a termination analysis, for any concurrent module where every loop exit edge is labelled \nwith an assume true statement, as discussed in Section 7.) There is a second hurdle we need to consider, \nnamely that of deadlock. Even if all transactions are guaranteed to terminate in isolation, they may \nend up in a deadlock in an interleaved execution. We directly show that this is not a concern for 2PL \nand TL and discuss the general case in Sec. 5.4. 5.1 Proof Strategy We take a two step approach. First, \nwe establish that when all ACNI-executions satisfy LP and are completable then all NI\u00adexecutions satisfy \nLP. Then, our NI-reduction theorem (The. 4) let us conclude that all executions satisfy LP. DEFINITION \n1. Let pni be an NI-execution with a schedule a1 \u00b7\u00b7\u00b7 ak,each ai representing the sub-schedule of a differ\u00adent \nthread ti.Let pcni be a CNI-execution with a schedule a1\u00df1 \u00b7\u00b7\u00b7 ak\u00dfk,each ai\u00dfi representing the sub-schedule \nof a dif\u00adferent thread ti. We say that pcni is an equivalent completion of pni if pcni satis.es the following \ncondition: for all i<j,the setof variables accessed by the execution of \u00dfi and aj are disjoint. The next \nlemma is a simple consequence of the de.nitions. LEMMA 5. Let pcni be an equivalent completion of pni. \nFor any thread t, the thread-owned view of t in pni is a pre.x of the thread\u00adowned view of t in pcni. \nOur overall proof strategy is to take an NI-execution pni and construct an equivalent completion pcni. \nBy Lemma 5, if pcni sat\u00adis.es LP,then pni satis.es LP as well. We now present an iterative scheme that \nwill successfully construct an equivalent completion for some, but not necessarily every, NI-execution \npni. Let the schedule of pni be a1 \u00b7\u00b7\u00b7 ak, each ai representing the sub-schedule of a different thread \nti. We construct a sequence of schedules .h,where 1 = h = k, where each .h is of the form a1\u00df1 \u00b7\u00b7\u00b7 ah\u00dfh \nand is the schedule of an CNI-execution. Further\u00admore, the execution of \u00dfi is guaranteed to not access \nany variable accessed by any aj for j>i. We let .0 be the empty schedule. We construct the schedule .i+1 \nfrom .i as follows. Note that .iai+1 is the schedule of an ACNI-execution. Hence, by assumption, this \nis completable. That is, there exists an CNI-execution pi+1 with a schedule .iai+1\u00dfi+1. If the execution \nof \u00dfi+1 in pi+1 does not access any variable accessed by aj in pni for any j>i +1, then we de.ne .i+1 \nto be .iai+1\u00dfi+1. Otherwise, our algorithm fails.  If the algorithm succeeds at every step i, then we \nde.ne the execution pk produced .nally to be the iterative completion of pni. Otherwise, we say that \nthe iterative completion of pni is not de.ned. LEMMA 6. If the iterative completion of an NI-execution \npni is de.ned, then it is an equivalent completion of pni. It follows from the above discussion that \nthe key open issue in proving ACNI-reduction is handling the case where \u00dfi+1 accesses some variable accessed \nby some aj for j>i +1 in the above algorithm for constructing the iterative completion. There are two \nreasons why this failure case may arise, one illustrated by TL and one illustrated by 2PL. We show how \nto work around them and prove ACNI-reduction for both these protocols. We then com\u00adbine these ideas to \nestablish ACNI-reduction for any LCS-locking\u00adprotocol satisfying a property we call progressive (see \nSec. 5.4). The following lemma shows that extending certain executions by a single transition does not \nviolate an LCS-locking-protocol LP. LEMMA 7. Assume that every ACNI-execution of a module satis\u00ad.es an \nLCS-locking-protocol LP.Let p be an NI-execution that has an equivalent completion. Then, any NI-execution \np ' = pt con\u00adsisting of p following by a single transition t satis.es LP. Proof: Let transition t be \ngenerated by thread t executing instruc\u00adtion e.Since p satis.es LP, the sub-execution of every thread \nin p satis.es LP. Thus, to prove that p ' satis.es LP we just need to verify that the sub-execution of \nt in p ' satis.es LP. Let a denote the sub-execution of t in p. Thus, the sub\u00adexecution of t in p ' is \nat .Let pcni be an equivalent completion of p, which exists by assumption. Consider the ACNI-execution \npt which is the pre.x of pcni that ends with a.(We let pt be pcni if a is empty.) We can show that the \nthread-owned state of t at the end of this ACNI-execution is the same as that at the end of p.Since t \ncan execute instruction e after p, it follows that it can execute the same instruction after pt as well. \n(See Lemma 1.) The sub-execution of t in pt extended by e must satisfy LP (since this extension is an \nACNI-execution). Hence, at must also satisfy LP. It follows that p ' satis.es LP.  5.2 ACNI-reduction \nfor TL We.rstestablishthatACNI-reductionisvalidfor TL.Theinductive proof carries a stronger property, \nto help in the induction step. THEOREM 8. If every ACNI-execution of a module satis.es TL and is completable, \nthen every NI-execution p of the module (a) satis.es TL and (b) has an equivalent completion pcni. Proof: \nAssume that every ACNI-execution of the module satis.es TL and is completable. We prove (a) and (b) by \ninduction on the length of the execution. The base case (an empty execution) is trivial. Assume as the \ninductive hypothesis that p satis.es (a) and (t,e) (b). Consider an NI-execution p ' = p . s ' ,where \ns ' is the state produced by the execution of an instruction e by a thread t after p. (a) follows immediately \nfrom Lemma 7 and the inductive hy\u00adpothesis. We now prove (b). Our strategy is to use the algorithm for \nconstructing the iterative completion of an NI-execution described earlier. However, the iterative completion \nof p ' may not be de.ned, as illustrated by the following simple example. Assume that p ' consists of \na thread t1 acquiring a lock on u, followed by a thread t2 acquiring a lock on v, a child of u (in the \nlock-ordering forest). Suppose the next step of t1 is to acquire a lock on v.If so, the iterative completion \nof p ' is not de.ned. The trick in this case is to reorder the execution of t1 and t2 in p ' to obtain \na con.ict\u00adequivalent execution p '' and construct the iterative completion of p '' . We now show that \nthis can always be done. Let the schedule of p ' be a1 \u00b7\u00b7\u00b7 ak, each ai representing the sub-schedule \nof a different thread ti. We assume, without loss of generality, that every thread ti acquires at least \none lock in p ' .(A thread ti that does not acquire any locks can be easily handled.) Consider the state \nsi produced by the execution pi of a1 \u00b7\u00b7\u00b7 ai.Let lockedi denote the set of variables locked by ti in \nsi.Let lockablei denote the set of children of variables in lockedi that have not already been locked \nby ti (in the execution pi). (These are variables that ti canimmediatelylock,ifavailable,withoutviolating \nTL.)Let lockable * i denote the set of descendants of variables in lockablei (where a vertex is considered \nto be its own descendant). These are the variables that ti can eventually lock without violating TL. \nGiven i<j, we say that a thread ti may-depend-upon tj if the .rst variable locked by tj (in p ' ) is \ncontained in lockable * i .These are the dependences that can create us problem while constructing an \niterative completion. We identify a permutation of the set of threads t1 to tk in which ti occurs after \ntj if ti may-depend-upon tj . We do this iteratively as follows. Initially, we start with an empty sequence \nS0 .Given Sj, a permutation of t1 through tj, we identify Sj+1 as follows. If none of the ti in Sj may-depend-upon \ntj+1, we append tj+1 to Sj to obtain Sj+1. Otherwise, we .nd the .rst occurrence of a ti in Sj such that \nti may-depend-upon tj and insert tj just before ti to obtain Si+1. Let the .nal sequence Sk be tp1 , \n\u00b7\u00b7\u00b7 ,tpk . Consider the schedule ap1 \u00b7\u00b7\u00b7 apk . We claim that this produces an NI-execution p '' that \nis con.ict-equivalent to p ' .Furthermore, we claim that the iterative completion of p '' is de.ned and \nthat this yields an equivalent completion for p ' as well. Proofs omitted due to lack of space.  5.3 \nACNI-Reduction for 2PL We will now present an analogue of the above theorem for 2PL. The challenge in \nconstructing an iterative completion of an NI\u00adexecution that follows 2PL is the possibility of a deadlock. \n(TL,on the other hand, guarantees deadlock-freedom.) Given two threads t1 and t2 in an execution p, we \nsay that t2 depends on t1 if t1 accesses a global variable g during the execution that is accessed later \nby thread t2.We de.ne the schedule slice of p with respect to thread t, to be the subsequence of the \nschedule of p consisting of steps taken by t or any other thread t ' on which t depends (directly or \ntransitively). It follows trivially that the schedule slice is a feasible schedule. We de.ne the slice \nof p with respect to t, denoted p|t, to be the execution induced by the schedule slice. For any thread \nt ' that occurs in p|t, it can be shown that the thread-owned view is the same in p and p|t. THEOREM \n9. If every ACNI-execution of a module satis.es 2PL and is completable, then (a) every NI-execution p \nof the module satis.es 2PL, and (b) for every NI-execution p of the module and every thread t, p|t has \nan equivalent completion pt cni. Proof: The proof is by induction on the length of the execution. (t,e) \nConsider an NI-execution p ' = p . s ' , where we assume that p satis.es (a) and (b). We show that p \n' also satis.es (a) and (b). (a) Since p follows 2PL, we just need to show that t follows 2PL t,e in \np ' . Consider p ' |t which must be of the form p|t . s '' . It follows from Lemma 7 and the inductive \nhypothesis that p ' |t follows 2PL. Therefore, p ' itself satis.es (a) (i.e., follows 2PL). (b) We now \nargue that the iterative completion of p ' |t is de\u00ad.ned. Assume that we take a schedule a1 \u00b7\u00b7\u00b7 ak and \nconstruct the schedule a1\u00df1 \u00b7\u00b7\u00b7 ak\u00dfk using our iterative completion algorithm. We just need to show that \nthe set of variables accessed by \u00dfi is disjoint from the set of variables accessed by aj for j>i.Note \nthat the execution of the thread ti (i<k) is included in p ' |t only if some other thread depends on \nti in p ' . Such a dependence is possible only if thread ti has reached its shrinking phase and has released \nsome lock by the end of ai. In this case, \u00dfi cannot acquire any new lock. Hence, \u00dfi can access only variables \nlocked by ti at the end of ai.But aj can only access variables that are not locked by ti at the end of \nai. The result follows.   5.4 ACNI-Reduction for Progressive Locking Protocols We now generalize the \nresults presented above to show that ACNI\u00adreduction is valid for a class of locking protocols. A thread \n(or a transaction) in an execution is visible if it has released at least one lock. In particular, if \na thread t reads/writes a global variable that is then subsequently accessed in a well-locked execution \nby another thread, then the .rst thread t is visible. An LCS-locking-protocol LP is said to be progressive \nif (i) An execution satisfying LP cannot contain a deadlock involving a visible transaction, and (ii) \nIf an execution p satis.es LP and t is any thread in p, then the abrupt completion of t in p,de.nedto \nconsist of p followed by a release by t of all locks it holds, also satis.es LP. Note that both TL and \n2PL satisfy the above de.nition and that condition (ii) above is satis.ed by any locking protocol that \nallows threads to release a lock at any time. The following theorem (whose proof appears in Appendix \nA) states that ACNI-reduction is valid for any progressive LCS\u00adlocking-protocol. THEOREM 10. Let LP be \na progressive locking protocol. If every ACNI-execution of a module satis.es LP and is completable, then \n(a) every NI-execution p of the module satis.es LP, and (b) for every NI-execution p of the module and \nevery thread t, p|t has an equivalent completion pt cni.  6. Sequential Reductions for Verifying DTL \nIn this section we extend our results to hand-over-hand lock\u00ading protocols, such as the concurrent binary \ntree example shown in Fig. 1(b). This requires us to generalize our results to handle mutable, dynamically \nallocated, pointer-linked data structures with heap-storable locks. More importantly, however, the protocol \nused is a Dynamic Tree Locking protocol. The tree-ordering, which de\u00adtermines the order in which a thread \nshould acquire locks, is deter\u00admined by the data-structure itself and can change over time. This requires \nextensions to the protocol itself. Signi.cantly, the required extension makes the protocol a non-transaction-local \nproperty, re\u00adquiring new proofs of the reduction theorems. 6.1 Dynamic Tree Locking (DTL) Inthissection,wepresent \nDTL,avariantoftheDynamicDagLock\u00ading protocol introduced in [7]. The variations permit coding pat\u00adterns \n(such as temporary violations of structural invariants in the part of the graph locked by a thread) typically \nfound in imple\u00admentations. DTL guarantees con.ict-serializability, though we omit a proof of this due \nto space constraints.1 We simplify the de.nition of the dynamic tree locking proto\u00adcol, the statement \nof the theorems and their proofs by representing the shared program state using a graph. Each global \nvariable and each heap-allocated object is represented by a vertex in the graph. The graph contains the \nedge u . v iff the variable/object repre\u00adsented by u points to the object represented by v.If u represents \na structured variable/object whose f .eld points to v, then this is f represented by a labelled edge \nu . v. In the sequel, we will omit the edge labels when no confusion is likely. We note that the target \nof an edge must always represent a heap object. In the sequel, we 1 The reduction theorems for the Dynamic \nDag Locking protocol can be found in [2]. will often abuse notation and not distinguish between a vertex \nand the variable/object it represents. We de.ne the set of vertices in the scope of a thread t at any \npoint during an execution as follows: Initially, the scope of any thread is empty. Whenever a thread \nt acquires a lock on u,all successors of u (in the graph representation) become part of t s scope. Whenever \na thread t allocates a new object u, u becomes part of t s scope. Whenever a thread releases a lock on \nu,all successors of u are removed from t s scope. Note that in comparing states produced by different \nexecutions, the address of a heap allocated object is irrelevant. Instead, equiv\u00adalence of states is \nestablished with respect to a bijective mapping between the heap allocated objects of the two states, \nin the obvious way. More generally, given two executions p1 and p2,we de.ne the correspondence relation \nbetween heap objects of the two exe\u00adcutions as follows: the object allocated by the i-th execution-step \nof thread t in p1 corresponds to the object allocated by the i-th execution-step of thread t in p2 (when \nboth exist). We use this cor\u00adrespondence to compare states in the two executions. DEFINITION 2. We say \nthat an execution satis.es the Dynamic Tree Locking Protocol (DTL) if it satis.es the following conditions: \n1. The execution is well-locked. 2. A thread never acquires a lock on an object u after it has released \na lock on that object u. 3. The .rst lock a thread acquires is on a global variable. 4. Subsequent \nto acquiring its .rst lock, a thread can acquire a lock on a vertex only if it is in the thread s scope. \n 5. A thread can insert an edge u . v only if v is in the thread s scope. (More precisely, the thread \ncan assign the address of an object represented by vertex v to a global variable or an object s .eld \nonly if v is in the thread s scope.) 6. Whenever a thread releases a lock on u, for every successor \nv of u, u must be v s only predecessor: i.e., u . v and w . v implies u = w. (In other words, whenever \na thread releases v from its scope, v must have a unique predecessor.)  Note that a thread is allowed \nto create a new resource at any point.  6.2 Properties of DTL LEMMA 11. The following properties hold \ntrue at any point during and just after any execution satisfying DTL: 1. A vertex not in the scope of \nany thread has at most one prede\u00adcessor. 2. All predecessors of a vertex in the scope of thread t must \ncurrently be locked by t. 3. A vertex can be in the scope of at most one thread.  Proof: By induction. \nNote that when a new vertex is created it belongs to its creator s scope. When a thread releases u from \nits scope, it ensures that u has at most one predecessor. A vertex u not in the scope of any thread enters \nthe scope of a thread t when t acquires a lock on u s unique predecessor. Once u enters t s scope, no \nother thread t ' can insert any edge x . u or acquire u for its scope: for t ' to insert an edge x . \nu, u must .rst be in t ' s scope, and for u to enter t ' s scope u must .rst have an unlocked predecessor \nthat t ' can lock. The result follows. Since DTL conditions 1 to 5 are transaction-local, we have the \nnext lemma: LEMMA 12. Let the thread-owned views of thread t in two exe\u00adcutions p and p ' be the same. \nLet the execution of step (t, e) be enabled after both p and p ' . Then, the execution of step (t, e) \nafter p satis.es DTL conditions 1 to 5 iff the execution of step (t, e) after p satis.es DTL conditions \n1 to 5.  However, condition 6 of the DTL protocol is not a transaction\u00adlocal property. This is why our \nearlier reductions do not hold for DTL and we must separately establish the following results. LEMMA \n13. Let pni = pP ptpS be a non-interleaved execution, t,e where pt is the sub-execution of a thread t.Let \npni . s be an execution that extends pni by a single transition. If pP ptpS satis.es t,et,e s ' DTL and \npP pt . satis.es DTL,then pP ptpS . s also satis.es DTL. Proof: Note that the thread-owned view of t \nis the same in pP pt and pP ptpS. Since the step (t, e) satis.es DTL after pP pt,it follows from Lemma \n12 we just need to check for condition 6. Assume that step (t, e) releases a lock on u, and we have an \nedge u . v in the state after pP ptpS. Thus, v must be in the scope of t throughout the execution of \npS. Consequently, v must have the same set of predecessors after pP ptpS as after pP pt. t,e Thus condition \n6 is satis.ed by pP ptpS . s iff it is satis.ed t,eby pP pt . s ' . LEMMA 14. Let a\u00dfu.dt denote the \nschedule of an NI-execution, where \u00dfu and dt denote the schedules executed by threads u and t respectively. \nAssume that no transitive con.ict-dependence exists between u and t in this execution. If a\u00dfu. and a.dt \nboth satisfy DTL, then so does a\u00dfu.dt. (In other words, \u00dfu does not affect the correctness of dt.) Proof: \nLet p1 denote the execution of a\u00dfu.dt We need to show that the execution of all instructions in dt in \np1 satisfy DTL.Let p2 denote the execution of a.dt. The absence of any transitive con.ict-dependence \nbetween \u00dfu and dt in p1 means that Lemma 12 applies and that we just need to verify that the execution \nof dt in p1 satis.es condition 6. This follows inductively. The key properties to note are: (a) The subgraph \ninduced by the set of all vertices ac\u00adcessed by dt and their successors at any point during the execution \nof dt in p1 is isomorphic to the corresponding subgraph induced in p2. (b) Furthermore, none of the vertices \nin these subgraphs have any predecessor outside the subgraph. As a result a vertex going out of t s scope \nat some point in p1 can have more than one pre\u00addecessor iff the corresponding vertex going out of t s \nscope at the corresponding point in p2 has more than one predecessor. The re\u00adsult follows.  6.3 NI \nReduction for DTL THEOREM 15. If every NI-execution of a module satis.es DTL then every execution of \nthe module satis.es DTL. Proof: The proof is identical to that of Theorem 4, except for the use of Lemma \n13 in place of Lemma 3.  6.4 ACNI Reduction for DTL We .rst establish that the iterative completion \nalgorithm described in Sec. 5.1 will succeed under a simple condition. LEMMA 16. Let a1a2 \u00b7\u00b7\u00b7 ak be the \nschedule of an DTL NI\u00adexecution p,each ai represents the schedule of a different thread ti. Let a1\u00df1a2\u00df2 \n\u00b7\u00b7\u00b7 ak\u00dfk be the schedule another DTL NI-execution p ' ,where ai\u00dfi represents the schedule of thread ti. \nAssume that ti acquires at least a single lock in execution p. For any i<j,the set of locations accessed \n(read, written, or locked) during the execu\u00adtion of \u00dfi in p ' is disjoint from the set of locations accessed \nduring the execution of aj in p. (Note that we compare heap locations in different executions modulo \nthe correspondence relation described in Sec. 6.1.) Proof: Let si be the state produced by the execution \nof a1 \u00b7\u00b7\u00b7 ai. Let Vi denote the set of vertices in si.Let scopei denote the set of vertices in the scope \nof ti in state si.Let reachablei denote the set of vertices in si that are reachable via some path from \nsome vertex in scopei. For j>i, the execution of aj in p cannot lock any vertex in reachablei. We can \nestablish this inductively. Thread tj cannot lock any vertex in the scope of any other thread tk in si.Further\u00admore, \nany vertex y not in the scope of any thread (in si)must have at most one predecessor x.If tj is unable \nto lock x, it will be unable to lock y as well. Now consider the execution of a1\u00df1a2\u00df2 \u00b7\u00b7\u00b7 ak\u00dfk.Let us \nidentify any vertex allocated in a step s in p with the location allo\u00adcated by the corresponding step \ns ' in p ' . We claim that any vertex in Vi that is locked by ti during the execution of \u00dfi must be in \nreachablei: this follows inductively, since for ti to acquire a lock on any y it must .rst hold a lock \non some predecessor x of y. In particular, this implies that the set of vertices locked by \u00dfi and aj \nmust be disjoint (where i<j). THEOREM 17. If every ACNI-execution of a module satis.es DTL and is completable, \nthen (a) every NI-execution p of the module satis.es DTL, and (b) every NI-execution p of the module \nhas an equivalent completion pcni (which is a CNI-execution, by de.ni\u00adtion). Proof: Assume that every \nACNI-execution of the module satis.es TL and is completable. We prove (a) and (b) by induction on the \nlength of the execution. The base case (an empty execution) is trivial. Assume as the inductive hypothesis \nthat p satis.es (a) and (t,e) (b). Consider an NI-execution p ' = p . s,where s is the state produced \nby the execution of an instruction e by a thread t after p. We assume that t has a transitive con.ict-dependence \non all other threads that execute in p. (Otherwise, we can omit the execution of any other thread u that \nt does not have a dependence on from p ' to get a shorter execution p '' . From the inductive assumption, \np '' must satisfy DTL. By Lemma 14, p ' must also satisfy DTL.) Let pcni be an equivalent completion \nof p. pcni must sat\u00adisfy DTL, by assumption, since it is an ACNI-execution. Let the schedule of p be \na1a2 \u00b7\u00b7\u00b7 ak, and the schedule of pcni be a1\u00df1a2\u00df2 \u00b7\u00b7\u00b7 ak\u00dfk, where each ai (and \u00dfi) represents execution \nby a thread ti. Note that Lemma 16 applies to p and pcni. Case 1: First, consider the case where t . \n= tk. Thus, (t, e) is the .rst step performed by a new thread t. We .rst show that (t, e) is enabled \nafter pcni as well. Suppose that (t, e) tries to acquire a lock on u.Since (t, e) is enabled after p, \nu must be unlocked after p. Furthermore, we assumed the existence of a con.ict-dependence between tk \nand t. This con.ict-dependence can exist only if tk acquired and then released a lock on u (during ak \nin p). The de.nition of an equivalent completion implies that tk acquires and releases a lock on u during \nak in pcni. Hence, tk cannot acquire a lock on u again during \u00dfk. Hence, u must be unlocked after pcni \nas well. Thus, (t, e) is enabled after pcni. t,es ' Now pcni . is an ACNI-execution, which satis.es DTL \nby assumption. So, the execution of (t, e) following pcni satis.es DTL. As a result, the execution of \n(t, e) after p satis.es all local properties of DTL. The only non-local property we need to check is \ncondition 6. But the .rst instruction of a thread t cannot release t,e a lock. (Otherwise, the ACNI-execution \npcni . s ' would violate DTL.) Hence, (a) follows. t,e As for (b), the ACNI-execution pcni . s ' must \nhave a comple\u00adtion by assumption. This gives us an equivalent completion for p ' . (Lemma 16 above ensures \nthat this is an equivalent completion.) Case 2: Now, consider the case where t = tk. Consider the schedule \n. = a1\u00df1a2\u00df2 \u00b7\u00b7\u00b7 ak. (We don t consider \u00dfk since tk continues executing.) We now show that (t, e) is \nenabled after .. Assume that (t, e) acquires a lock on u (following execution p). If u is the .rst lock \nacquired by tk, then the reasoning is the same as in case 1. Otherwise, tk must hold a lock on the unique \npredecessor x of u after p. This implies that none of the \u00dfj,for j<k, can acquire alockon x in the execution \n.. This follows from Lemma 16 (or, equivalently, from the de.nition of an equivalent completion). As \na result, none of the \u00dfj can acquire a lock on u either. Hence, u remains unlocked at the end of .. Hence, \n(t, e) is enabled after ..  Now ..(t, e) is an ACNI-execution, which satis.es DTL by assumption. We \nneed to now show that p.(t, e) satis.es DTL.By Lemma 12, we just need to check for non-local property \n6. This follows just as in the proof of Lemma 14. Hence, (a) follows. As for (b), the ACNI-schedule ..(t, \ne) must have a completion by assumption. This gives us an equivalent completion for p ' .(By Lemma 16 \nthis completion is equivalent.)   7. Sequential Analyses for Concurrent Modules Our reduction theorems \nenable the use of sequential analyses to verify that a module adheres to a locking protocol such as, \ne.g., DTL and is con.ict-serializable. For a con.ict-serializable module, our results also enable the \nuse of sequential analysis to infer or verify other properties of the module. In this section we make \nthese claims more precise by describing different kinds of sequential analyses and the conditions under \nwhich they can be used and formalizing the types of properties that can be veri.ed using these analyses. \n7.1 Veri.able Properties Transaction-Local Properties. We .rst establish that analogues of our earlier \nreduction theorems hold for verifying transaction\u00adlocal properties. Since transaction-local properties \nobserve only what happens within each thread, the next theorem is a straightfor\u00adward consequence of known \nproperties of con.ict-serializability. THEOREM 18. Let M be a con.ict-serializable module and let f be \na transaction-local property. If every NI-execution of module M is well-locked and satis.es property \nf, then every execution of the module satis.es f. The following theorem shows that ACNI-reductions hold \nfor transaction-local properties. THEOREM 19. Let M be a con.ict-serializable module and let f be a transaction-local \nproperty. If every ACNI-execution of module M is well-locked, completable and satis.es f, then every \nexecution of M satis.es f. The above theorems justify the use of sequential analyses (de\u00adscribed later) \nto infer or verify transaction-local properties. As an example of properties that can be so veri.ed, \nconsider the mod\u00adule T shown in Fig. 1(a). The property Q==W +1 in the .rst assert statement is transaction-local \nand can be veri.ed by a se\u00adquential analysis. Similarly, a sequential analysis can also establish the \nsecond assert statement which, as it is concerned only with local variables, is also transaction-local. \nNote, however, that the property Q== W is not a transaction-local property at the program point of the \nsecond assertion, as Q s lock has been released. Module Invariants. A program state s is quiescent if \nthere ex\u00adists a complete execution that produces s. A property f of program states is said to be a module \ninvariant if all quiescent states satisfy f. Our results also justify the use of sequential analyses \nto infer module invariants. For example, a sequential analysis of module T, shown in Fig. 1(a) can determine \nthat Q== W is a module invari\u00adant. Module invariants are of particular interest since one can rea\u00adson \nabout module procedures assuming that they are invoked in a state satisfying the module invariants (when \nthe module is con.ict\u00adserializable and its ACNI-executions are completable). Note that the inferred module \ninvariants do not have to be transaction-local, but can still be used in the reasoning as explained above. \n 7.2 Sequential Modular Analysis We describe several sequential modular analyses and the condi\u00adtions \nunder which these analyses can be used. The term modular analysis refers to analysis of a given module, \nindependent of any speci.c client of the module. The information computed by a mod\u00adular analysis is valid \nfor any possible client of the module. Optimistic Analysis. An optimistic analysis exploits ACNI\u00adreduction \nand can be used when the conditions for ACNI-reduction are met. Brie.y, an optimistic analysis amounts \nto an analysis of the program obtained by combining the module with its most-general sequential client. \nThe most-general sequential client of module DT is shown in Fig. 3(a). Thus, an optimistic analysis iteratively \niden\u00adti.es the module invariants, and analyzes the invocation of every module procedure on any state \nsatisfying the module invariant. We start with a tentative module invariant that characterizes the initial \nstate of the module s data (before any procedure is invoked), and it\u00aderatively analyze the module s procedures \nwhen invoked in a state satisfying the tentative module invariant, and weaken the module invariant to \naccount for the state that can be produced at the end of the module procedure. Completability Analysis. \nThe use of an optimistic analysis or ACNI-reduction requires verifying that every ACNI-execution is completable. \nThis requirement can be proved with termination analysis (e.g. [6, 10, 16]). Note that the termination \nanalysis needs to only establish that a procedure invocation terminates when in\u00advoked in a state satisfying \nthe module invariant and executes in isolation. In other words, the termination analysis itself can be \nop\u00adtimistic. However, completability is a weaker requirement than termination, as illustrated by the \nfollowing examples. Contrast the problematic code snippet from Fig. 1(c) acquire(Y); Y = 1; release(Y); \nwhile true; with acquire(Y); Y = 1; release(Y); while(*);. Both snippets are potentially non-terminating. \nHowever, the second one is completable according to our de.nition. Thus, an optimistic analysis is valid \nin the second, although it is not valid in the .rst case. The two requirements differ only for non-deterministic \npro\u00adgrams. This is quite signi.cant, as many analyses typically do not interpret branching or looping \nconditions. Such analyses can effec\u00adtively use an optimistic approach without requiring a termination\u00adanalysis. \nFor the reduction to hold, we still need to guarantee that every ACNI-execution can be completed. However, \ninstead of ver\u00adifying this, if we weaken conditions on branches (assume state\u00adments) to ensure that this \nholds trivially, we are .ne. Stated differ\u00adently, given a module M, we weaken conditions in M to construct \na module Mw that has more behaviors than M and one that trivially satis.es the completable condition. \nOur theorem justi.es the use of optimistic analysis for Mw. Because Mw has more behaviors than M, a successful \nveri.cation of Mw applies to M also. Pessimistic Analysis. This approach analyzes each procedure of the \nmodule independently once. It assumes nothing about the initial state (of the module s global variables) \nin which the procedure is invoked. However, the analysis will assume that the procedure is executing \nin isolation, without interference from any concurrent execution of other procedure invocations. This \nanalysis relies only on NI-reduction. Hence, it does not require verifying that all ACNI\u00adexecutions are \ncompletable. A pessimistic analysis has the potential to be more ef.cient than an optimistic analysis \nbut is less precise. (E.g., note that pes\u00adsimistic analysis cannot verify either of the assertions of \nmodule T  module MGC-DTL; void mostGeneralClient() {DTL t; t.init(); while (*) { int k = random(); \n if (*) t.insert(k); else if(*) t.delete(k); else t.contains(k); }} module S; int Y, B; void p() {acquire(B); \nB= 0; release(B); acquire(B); int b = B; if (b) Y = 2; release(B); }void q() {acquire(B); B= 1; release(B); \n} (a) The MSGC of module DT. (b) Module S. Figure 3. Additional examples. in Fig. 1(a) and cannot verify \nthat the module follows the TL pol\u00adicy and is, hence, con.ict-serializable. However, both an optimistic \nanalysis or a cautiously optimistic analysis, described below, using, e.g., the octagon domain [22], \ncan verify these properties.) Such an analysis is typically insuf.cient for verifying adherence to DTL \nsince DTL depends on module shape invariants. Veri.cation using type systems are often pessimistic analyses \nand, when they are, can safely be used for con.ict-serializable modules. Cautiously Optimistic Analysis. \nThis middle ground between the two possibilities mentioned above is similar to the optimistic anal\u00adysis \nin computing a module invariant, except that it computes a weaker module invariant, exploiting only the \nNI-reduction. Specif\u00adically, the module invariant computed by this approach describes all states that \ncan be produced by any NI-execution. The condition for applying this approach is the same as for the \npessimistic approach, namely con.ict-serializability. Note. A key advantage of sequential reduction is \nthat it permits analyses to track correlations between variables, including those not locked, without \nhaving to account for the side-effect of other threads on such correlations.  8. Prototype Implementation \nTo measure the value of sequential reduction, we analyzed the most general concurrent client of a lock-coupling \nsorted list implementa\u00adtion with different numbers of threads. The analysis, an adaptation of [1] to \nverify memory safety, tracks properties such as pointed-to by a variable, reachability from a variable, \nheap sharing, cyclicity, and sortedness. Analysis with one thread, corresponding to opti\u00admistic analysis \nobtained by our sequential reduction results, took 10 seconds and consumed 3.6 MB memory. obtained by \na sequen\u00adtial reduction.) With two threads, the analysis took almost 4 hours and consumed 886 MB memory. \nWith three threads, the analysis did not terminate in 8 hours. We implemented an optimistic analysis \nof hand-over-hand (lock-coupling) algorithms for sorted lists and binary search trees. Our analysis was \nderived from the sequential analyses of [17] and [20] and implemented using TVLA [18]. Our analysis success\u00adfully \nveri.ed that these algorithms do not dereference null-valued pointers, do not leak memory, and follow \nthe DTL locking pol\u00adicy. The list analysis also veri.ed that the list remains acyclic and sorted. The \ntree analysis also veri.ed that procedures maintain a forest of trees which are heap-sorted. The analyzer \nalso veri.es that every procedure invocation in sequential execution terminates (as required by ACNI-reduction). \nThe termination argument is based on the size-change argument [3, 16]: It shows that in every itera\u00adtion \nthe set of reachable elements from the procedure variables is Table 1. Experimental Results. Experiments \nperformed on a ma\u00adchine with a 2.4 Ghz Intel Core 2 Duo processor and 2 Gb memory running version 10.5.7 \nof the Mac OS X operating system. a strict subset of the reachable elements in the previous iteration. \nWe are not aware of any other automatic veri.cation tool that is ca\u00adpable of verifying partial and total \ncorrectness of the lock-coupling tree algorithm. We measured the additional cost required by our extensions \nto the sequential analysis of the lock-coupling tree. (The analysis of the lock-coupling list is too \nshort for a meaningful comparison.) Tab. 1(b) compares the cost of our analysis (*) with a vanilla version \nthat veri.es all of the above properties except for adherence to the locking protocol. The results show \nthat the cost of the two analyses is in the same order of magnitude. The difference between the two is \nessentially the cost paid to validate that the sequential invariants are also concurrent invariants. \nTab. 1(a) compares the cost of our analysis (*) to existing anal\u00adyses using the lock-coupling list algorithm \nas a common bench\u00admark. The analyses of [1], [24], and [27] verify linearizability [14]. Our analysis \nveri.es serializability. These problems are similar, but not the same. Nevertheless, we believe the comparison \nshows the effectiveness of sequential reasoning. The analysis of [1] is able to verify that the list \ncoupling algorithm is linearizable for a bounded number of threads. In our experiments, the analysis \ntimed out. However, in [1] it is reported to verify the algorithm for two threads in less than 4 hours. \nThe analysis of [24], which is an op\u00adtimized version of [4, 21], is able to verify that the list coupling \nalgorithm is linearizable for an unbounded number of threads. The analysis of [27] is a separation logic \nbased analysis which uses rely guarantee reasoning. This analysis is the cheapest analysis in our benchmark. \nHowever, it is based on an abstraction that does not track sortedness. As a result, this analysis fails \nto verify lineariz\u00adability in some cases. 9. Discussion and Related Work The idea of simplifying reasoning \nabout concurrent programs using reduction, i.e., by treating code fragments that satisfy appropriate \nconditions atomically, is quite old. However, not many have con\u00adsidered the problem of whether the process \nof reduction itself can be done ef.ciently by considering only the reduced system. One exception is the \nwork by Stoller and Cohen [25]. They consider the aforementioned issue and present suf.cient conditions \nunder which the reduction itself can bene.t from reduction. Our results are in\u00adcomparable with theirs \nand complement their work: The applicabil\u00adity conditions for their result do not hold for the problem \nwe study. For example, the code fragment that can be reduced (i.e., treated atomically) by their approach \ncannot span blocking synchroniza\u00adtion operations (such as an acquire-lock operation). In contrast, we \nare able to treat code fragments containing an arbitrary number of such operations atomically, when they \nsatisfy the locking protocol. On the other hand, their technique, unlike ours, is not limited to lock-based \nsynchronization. One of the early works on reductions is Lipton s theory of D\u00adreductions which permits \ntreating certain blocks of instructions atomically [19]. Lipton s theory discusses the conditions that \nthe instructions in the block need to satisfy (certain commutativity properties of different instructions \nin every interleaved execution) to enable such reduction. Lipton does not discuss the problem of (automatically) \nidentifying if a code fragment satis.es these conditions.  Flanagan and Qadeer [13] present an algorithm \nfor identifying procedures (and code fragments) that behave atomically, exploiting Lipton s theory. The \nFlanagan-Qadeer approach decomposes the atomicity checking problem into two parts: (A) the algorithm \nthey present identi.es atomic regions of code under the assumption that the system does not have any \ndataraces and (B) they use existing algorithms to check if the system has any potential datarace. Our \nresults show that atomicity checking (via checking adher\u00adence to a locking protocol such as 2PL) can \nbe simpler than the sub-problem of checking for dataraces in the following sense. Ab\u00adsence of dataraces \nand the stronger property of well-lockedness (which requires that every shared variable resp. .eld of \na dynami\u00adcally allocated object be accessed by a thread t only when t holds the variable s resp. .eld \ns protecting lock) do not enjoy sequential reduction. In other words, it is possible that all threads \nsatisfy well\u00adlockedness when there is no interleaving of thread executions, but an interleaved execution \nmay not satisfy well-lockedness, as illus\u00adtrated below. EXAMPLE 4. Consider module S shown in Fig. 3(b). \nModule S is well-locked in all NI-executions, but not in all interleaved execu\u00adtions. In any NI-execution, \nthe conditional assignment of 2 to Y in procedure p never executes because the condition b is always \nfalse at this point. However, in an interleaved execution, q might change the value of b to 1, which \ncauses the conditional assignment of 2 to Y to happen (when procedure p does not have a lock on Y), violating \nthe well-lockedness condition. The analogue of our approach, in the Flanagan-Qadeer setting, would betodopart \n(B) assuming atomicity of code identi.ed as atomic by part (A), which can make the second check more \nef.cient and/or more precise. Of course, such a cyclic dependence (assume\u00adguarantee reasoning) would \nhave to be justi.ed as being correct, which is one of the main contributions of our work. Code following \nthe TL or DTL protocol cannot be identi.ed as being atomic by the Flanagan-Qadeer approach. Furthermore, \nthe Flanagan-Qadeer approach does not apply when atomicity depends on other invariants maintained by \nthe module which must be simul\u00adtaneously established to verify atomicity (such as in cases involving \nlist/tree manipulation), unlike our approach. Partial order reduction (POR) techniques [9] combat the \nstate\u00adexplosion problem by exploring only a representative subset of all program executions. In general, \nhowever, verifying that a subset of all executions is representative is hard and requires analysis that \naccounts for all interleaved executions. Elmas et al. [11] present a proof technique for concurrent pro\u00adgrams \nthat combines reduction with abstraction, in an iterative fash\u00adion, to simplify veri.cation. The core \nreduction is based on Lipton s theory of movers. 10. Conclusions and Future Work Verifying that a concurrent \nmodule is serializable is an important and challenging problem. In this paper, we exploit the fact that \noften concurrent modules guarantee serializability through a local locking protocol to establish a reduction \nof the aforementioned con\u00adcurrent veri.cation problem to a sequential veri.cation problem. Our results \nenable (automatic or manual) analysis and veri.ca\u00adtion tools for concurrent modules to perform the analysis \nmore ef\u00ad.ciently by considering only sequential executions. Currently, we assume that all locks are exclusive. \nIn the future, we plan to also consider non-exclusive (shared) read locks as well as other concur\u00adrency \ncontrol mechanisms, like those based on timestamps [26]. Abstractly our work may be thought of as a staged \nrely\u00adguarantee reasoning . We take a speci.cation for the procedures (namely, the locking protocol) and \nwe apply rely-guarantee reason\u00ading to analyze the speci.cations themselves (rather than an imple\u00admentation \nof the speci.cation). The results we so establish guaran\u00adtee that, as a second stage, one can verify \nthat the procedures satisfy their speci.cations (the locking protocols) in a sequential setting and get, \nfor free, the proof that the procedures satisfy their speci\u00ad.cations in a concurrent setting. We believe \nthat generalizing this approach (of staged rely-guarantee reasoning) to other problems is an interesting \ndirection for future research. Acknowledgements: We are grateful to Ahmed Bouajjani, Mooly Sagiv, and \nEran Yahav for collaborating with us in the early stages of this research, and for many interesting discussions. \nWe thank Rachid Guerraoui, Vasu Singh, and the anonymous referees for their useful comments. References \n[1] D. Amit, N. Rinetzky, T. Reps, M. Sagiv, and E. Yahav. Comparison under abstraction for verifying \nlinearizability. In CAV, 2007. [2] H. Attiya, G. Ramalingam, and N. Rinetzky. Sequen\u00adtial veri.cation \nof serializability. Technical report. http://www.dcs.qmul.ac.uk/~maon/pubs/seqser.pdf. [3] J. Berdine, \nA. Chawdhary, B. Cook, D. Distefano, and P. O Hearn. Variance analyses from invariance analyses. In POPL, \n2007. [4] J. Berdine, T. Lev-Ami, R. Manevich, G. Ramalingam, and M. Sagiv. Thread quanti.cation for \nconcurrent shape analysis. In CAV, 2008. [5] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency \nCon\u00adtrol and Recovery in Database Systems. Addison-Wesley, 1987. [6] A. R. Bradley, Z. Manna, and H. \nB. Sipma. The polyranking principle. In ICALP, 2005. [7] V. K. Chaudhri and V. Hadzilacos. Safe locking \npolicies for dynamic databases. In PODS, 1995. [8] S. Cherem, T. M. Chilimbi, and S. Gulwani. Inferring \nlocks for atomic sections. In PLDI, 2008. [9] E. M. Clarke, Jr., O. Grumberg, and D. A. Peled. Model \nchecking. MIT Press, Cambridge, MA, USA, 1999. [10] B. Cook, A. Podelski, and A. Rybalchenko. Termination \nproofs for systems code. In PLDI, 2006. [11] T. Elmas, S. Qadeer, and S. Tasiran. A calculus of atomic \nactions. In POPL, 2009. [12] F. Fich, M. Herlihy, and N. Shavit. On the space complexity of randomized \nsynchronization. JACM, 45(5), 1998. [13] C. Flanagan and S. Qadeer. A type and effect system for atomicity. \nIn PLDI, 2003. [14] M. P. Herlihy and J. M. Wing. Linearizability: a correctness condition for concurrent \nobjects. TOPLAS, 12(3), 1990. [15] Z. M. Kedem and A. Silberschatz. A characterization of database graphs \nadmitting a simple locking protocol. Acta Informatica, 16, 1981. [16] C. S. Lee, N. D. Jones, and A. \nM. Ben-Amram. The size-change principle for program termination. In POPL, 2001. [17] T. Lev-Ami, T. Reps, \nM. Sagiv, and R. Wilhelm. Putting static analysis to work for veri.cation: A case study. In ISSTA, 2000. \n[18] T. Lev-Ami and M. Sagiv. TVLA: A framework for Kleene based static analysis. In SAS. Springer, 2000. \n[19] R. J. Lipton. Reduction: a method of proving properties of parallel programs. CACM, 18(12), 1975. \n[20] A. Loginov, T. W. Reps, and M. Sagiv. Automated veri.cation of the deutsch-schorr-waite tree-traversal \nalgorithm. In SAS, 2006. [21] R. Manevich, T. Lev-Ami, G. Ramalingam, M. Sagiv, and J. Berdine. Heap \ndecomposition for concurrent shape analysis. In SAS, 2008. [22] A. Min\u00b4e. The octagon abstract domain. \nHOSC, 19(1), 2006. [23] C. H. Papadimitriou. The serializability of concurrent database up\u00addates. J. \nACM, 26(4), 1979.  [24] M. Segalov, T. Lev-Ami, R. Manevich, G. Ramalingam, and M. Sagiv. Abstract transformers \nfor thread correlation analysis. In APLAS, 2009. [25] S. D. Stoller and E. Cohen. Optimistic synchronization-based \nstate\u00adspace reduction. FMSD, 28(3), 2006. [26] R. H. Thomas. A majority consensus approach to concurrency \ncontrol for multiple copy databases. ACM Trans. Database Syst., 4(2), 1979. [27] V. Vafeiadis. Shape-value \nabstraction for verifying linearizability. In VMCAI, 2009. [28] G. Weikum and G. Vossen. Transactional \nInformation Systems: The\u00adory, Algorithms, and the Practice of Concurrency Control. Morgan Kaufmann, 2001. \n  A. ACNI-reduction for Progressive Locking Protocols In this section, we show that ACNI-reduction is \nvalid for any progressive LCS-locking-protocol. LEMMA 20. Let LP be any progressive LCS-locking protocol. \nAs\u00adsume that every ACNI-execution of a module satis.es LP and is completable. Let pni be any non-interleaved \nexecution that satis.es LP. If all threads except the one executing last in pni are visible then pni \nhas an equivalent completion. Proof: Let the schedule of the given NI-execution pni be of the form a1a2 \n\u00b7\u00b7\u00b7 ak, where each ai denotes the sub-schedule of a thread ti. We essentially wish to let every thread \nti runtocom\u00adpletion, producing sub-schedule .i = ai\u00dfi. However, while doing so, we need to ensure that \nthe new instructions \u00dfi do not modify (lock) any global variable that is read by aj of any subsequently \nexecuting thread. In order to achieve this, we may need to re-order the sub-schedules of some threads, \nwhile preserving the dependen\u00adcies between them. We now sketch how we can do this. Recall that threads \nti and tj are said to con.ict in pni if they access some com\u00admon global variable. If ti and tj con.ict, \nand i<j, we say that tj is dependent on ti. We now show how to complete the given NI-execution iter\u00adatively. \nWe start with the empty schedule F0, and construct the schedule Fi+1 from Fi by identifying the thread \ntsi to be sched\u00aduled next, constructing a thread-completion .si = asi \u00dfsi ,and de.ning Fi+1 =Fi.si .Let \nSi denote the set {s1, \u00b7\u00b7\u00b7 ,si}.We maintain the following invariant as we construct the schedule: Fi \nis a feasible schedule; Fi preserves dependencies among the sched\u00aduled threads (i.e., if we have thread \ntsq is dependent on thread tsp in pni, then we will ensure that p<q); further, in the execution of Fi, \nthe instructions executed by \u00dfsi will not reference (or lock) any global variable referenced by aj for \nany j not in Si (i.e.,any thread that has not been scheduled yet). Given schedule Fi, we say that a thread \ntj has been completed if j . Si. We say that a thread tj is a candidate (for scheduling next) if all \nthreads it depends on (in execution pni) have completed. We note that at least one candidate must exist \nsince the depen\u00addence edges between threads form an acyclic graph. The invari\u00adants maintained guarantee \nthat for any candidate tj, Fiaj is feasi\u00adble and produces an ACNI-execution where the thread-owned view \nof tj is the same as in pni. Furthermore, our assumption that all ACNI-executions can be completed imply \nthat there exists a CNI\u00adexecution with a schedule Fiaj\u00dfj . Our assumptions also guarantee that this new \nschedule satis.es the locking protocol LP.If \u00dfj (in the above CNI-execution) does not reference (lock) \nany location refer\u00adenced by any thread not in Si .{tj }, we say that tj is a valid candidate. If a valid \ncandidate tj exists, we de.ne si to be j and Fi+1 to be Fiaj \u00dfj . All that remains is to show that at \nleast one valid candidate must exist. We prove this by contradiction. Assume that no valid candi\u00addates \nexist. We will derive a contradiction as below. The informal idea is to show that if all candidates are \nblocked then we can reach a con.guration that the locking protocol is not supposed to allow (either a \ndeadlock among visible transactions or an execution that is not con.ict-serializable). As shown above, \nevery candidate tj has a potential completion aj \u00dfj. Since the candidate is not valid, \u00dfj must have a \npre.x of the form \u00dfj ' ej,where \u00dfj ' does not reference any forbidden variable (any variable referenced \nby a thread not in Si .{tj }), while ej ref\u00aderences a forbidden variable. Assume that ej references a \nforbidden variable also referenced by ah,where th .. Si .{tj }. We say that tj is blocked by th, and \nsay there is a blocking dependence from th to tj. The key to note is that there must exist a cycle involving \nthe union of blocking dependence edges and normal dependence edges. (The cycle can be constructed since \nevery candidate thread has at least one incoming blocking dependence edge, while every non-candidate \n(unscheduled) thread has at least one incoming de\u00adpendence edge.) Let dj denote aj\u00dfj ' .Since aj \u00dfj satis.es \nLP (as mentioned above), it must be well-locked and hence ej must be an ac\u00adquire instruction. (Well-lockedness \nalso implies that no other non\u00adscheduled thread can access a variable for which tj holds the lock at \nthe end of aj ; thus, the forbidden variables cannot include any variable for which tj holds the lock \nat the end of aj .) Let tc1 , \u00b7\u00b7\u00b7 ,tcx be the set of candidate threads. Let td1 , \u00b7\u00b7\u00b7 ,tdy be the set \nof remaining unscheduled threads in an or\u00adder satisfying the dependencies between them. Consider the \nfol\u00adlowing schedule F ' =Fidc1 \u00b7\u00b7\u00b7 dcx ad1 \u00b7\u00b7\u00b7 ady . Thisisafeasible schedule that satis.es LP.Let F \n'' denote the abrupt completion of this schedule with respect to every tdp . This is also a schedule \nthat follows the locking protocol LP. Note that this schedule may not be possible in our concurrent module, \nbut we can still use it in our argument below. From this point on, we iteratively choose a candidate \ntcq that is trying acquire a (forbidden) lock that is not currently held by an\u00adother candidate. We let \ntcq acquire the forbidden lock, and then we force it to terminate abruptly (by releasing all its locks). \nEventu\u00adally, either all candidates would have acquired its forbidden lock or we must get stuck because \nof a deadlock involving the candidates. The latter is not possible according to de.nition of a progressive \nlocking protocol. The former is also not possible because the above process converts every blocking dependence \nthat existed originally into a real dependence. As a result, the .nal execution will have a cycle of \ndependencies among the transactions, while the execution is supposed to be con.ict-serializable (since \nit follows the locking protocol). The result follows. Proof: (Theorem 10) The proof is by induction \non the length of p. The claim is trivial for the base case, when p is empty. Consider the inductive step. \nPart (a) follows from our inductive hypothesis and Lemma 7. From part (a), it follows that p and, hence, \np|t are con.ict-serializable. Let pni be the NI-execution ob\u00adtained by con.ict-serialization of p|t. \nPart (b) follows by applying Lemma 20.   \n\t\t\t", "proc_id": "1706299", "abstract": "<p>Serializability is a commonly used correctness condition in concurrent programming. When a concurrent module is serializable, certain other properties of the module can be verified by considering only its sequential executions. In many cases, concurrent modules guarantee serializability by using standard locking protocols, such as tree locking or two-phase locking. Unfortunately, according to the existing literature, verifying that a concurrent module adheres to these protocols requires considering concurrent interleavings.</p> <p>In this paper, we show that adherence to a large class of locking protocols (including tree locking and two-phase locking) can be verified by considering only <i>sequential </i> executions. The main consequence of our results is that in many cases, the (manual or automatic) <i>verification of serializability can itself be done using sequential reasoning </i>.</p>", "authors": [{"name": "H. Attiya", "author_profile_id": "81453633305", "affiliation": "Technion, Haifa, Israel", "person_id": "P1911031", "email_address": "", "orcid_id": ""}, {"name": "G. Ramalingam", "author_profile_id": "81100519054", "affiliation": "Microsoft Research India, Bangalore, India", "person_id": "P1911032", "email_address": "", "orcid_id": ""}, {"name": "N. Rinetzky", "author_profile_id": "81453621777", "affiliation": "Queen Mary University of London, London, United Kingdom", "person_id": "P1911033", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706305", "year": "2010", "article_id": "1706305", "conference": "POPL", "title": "Sequential verification of serializability", "url": "http://dl.acm.org/citation.cfm?id=1706305"}