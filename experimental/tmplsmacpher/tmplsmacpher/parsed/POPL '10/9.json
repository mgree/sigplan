{"article_publication_date": "01-17-2010", "fulltext": "\n Veri.ed Just-In-Time Compiler on x86 Magnus O. Myreen Computer Laboratory, University of Cambridge \nmagnus.myreen@cl.cam.ac.uk Abstract This paper presents a method for creating formally correct just-in\u00adtime \n(JIT) compilers. The tractability of our approach is demon\u00adstrated through, what we believe is the .rst, \nveri.cation of a JIT compiler with respect to a realistic semantics of self-modifying x86 machine code. \nOur semantics includes a model of the instruction cache. Two versions of the veri.ed JIT compiler are \npresented: one generates all of the machine code at once, the other one is incre\u00admental i.e. produces \ncode on-demand. All proofs have been per\u00adformed inside the HOL4 theorem prover. Categories and Subject \nDescriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation Formal methods, Correct\u00adness \nproofs; F.3.1 [Logics and meanings of programs]: Specifying and Verifying and Reasoning about Programs \nMechanical veri.\u00adcation, Speci.cation techniques, Invariants General Terms Performance, Reliability, \nVeri.cation Keywords Compiler veri.cation, just in time, self-modifying code 1. Introduction Just-in-time \n(JIT) compilation is an effective technique for boosting the speed of program interpreters. The idea \nof JIT compilation, i.e. to dynamically translate input programs into native machine code, then execute \nonly native code, is an old invention which dates back to 1960 [15]. However, the concept is still relevant \ntoday as JIT compilation is considered vital for competitive interpreter-based implementations of modern \nlanguages, Java, C# and ML etc. To date, there seems to be no publications on veri.cation of a full JIT \ncompiler. The reasons for this is likely to lie in the fact that veri.cation of JIT compilers poses a \nnumber of challenges that are generally considered hard: 1. Compiler veri.cation. A JIT compiler needs \nto correctly map its input programs down to concrete machine code. In this case the target must be real \nmachine code (numbers), not assembly code or intermediate code which most other veri.ed compilers seem \nto output. 2. Non-static code. Conventional compilers can treat generated code purely as data, since \nexecution of generated code is not done during compilation. However, JIT compilers switch be\u00adtween execution \nof static code (the compiler) and dynamically generated code; hence some data needs to be treated as \ncode.  Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n10, January 17 23, 2010, Madrid, Spain. Copyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . \n$10.00 3. Self-modifying code. The dynamically generated code might also cause self-modi.cation: the \ngenerated code can, in incre\u00admental JIT compilers, invoke the code generator which may al\u00adter the code \nthat called it (Section 5.1 provides an example). Code that can modify itself poses veri.cation challenges \nthat only few have tackled [8, 6]. 4. Pointer arithmetic and code pointers. Pointer arithmetic and particularly \ncode pointers, both natural parts of JIT compilation and execution of the generated code, have been considered \nhard to deal with in a formal context.  This paper presents a method for creating formally correct JIT \ncompilers. The tractability of our approach is demonstrated through, what we believe is the .rst, veri.cation \nof a JIT com\u00adpiler with respect to a semantics of self-modifying x86 code. The contributions of this \npaper are: a) a semantics suitable for veri.cation of self-modifying x86 code, i.e. an operational semantics \nwhich takes into account the haz\u00adards of a possibly out-of-date instruction cache. b) a Hoare logic that \n.ts on top of (a) and can reason about self\u00admodifying code and code pointers by treating code as data \nand the program counter as a normal register, c) a work.ow, using (b), with which we veri.ed two JIT \ncompil\u00aders: one generates all of the machine code at once, the other one is incremental i.e. produces \ncode only on-demand. The input language of our veri.ed JIT compiler is a simple stack-based bytecode \n(Section 3) with support for a few branch instructions and stack operations: pop, push, subtract and \nswap. Our de.nitions and proofs have been developed inside the HOL4 theorem prover [26], which greatly \nhelped keeping track of all the details involved. Our HOL4 proof scripts are available online [1] together \nwith the veri.ed JIT compilers. A very basic benchmark of how ef.cient the veri.ed JIT compilers are \nwhen executed on real x86 hardware is presented in Section 4.6. 2. Main ideas This section presents a \nhigh-level summary of the main ideas that make veri.cation of JIT compilers possible. Subsequent sections \nexplain the technical details of our proofs (Sections 3, 4, 5) and veri.cation framework (Sections 6, \n7). 2.1 Operational semantics The basis of this work is a detailed and extensively tested seman\u00adtics \nof x86 machine code (Section 6). This semantics is carefully designed to be suitable for veri.cation \nof self-modifying x86 code, i.e. it includes an instruction cache that exposes the intricacies of a possibly \nout-of-date instruction cache. The memory is modelled as a function m and the instruction cache modelled \nas a function i: 1. instruction-fetches read instruction cache i;  2. data-reads and -writes access \nonly memory m, and 3. occasionally i is updated with values from m.  Cache i can be out-of-date: reading \nfrom i might not return values accurate with respect to what is stored in m. The model covers a number \nof 32-bit user-mode x86 instruc\u00adtions together with their precise bit-level fetch and decode.  2.2 Machine-code \nHoare logic We next set up a Hoare logic (informally described here, but for\u00admally de.ned in Section \n7) to ease reasoning about the detailed x86 semantics. We de.ne a Hoare triple {p} c {q}, in Section \n7.4, as an abbreviating statement above the x86 semantics, and then prove each individual Hoare-triple \nstatement as a theorem from the x86 semantics. This approach, from [9], contrasts with the original Hoare \nlogic [11] where certain Hoare triples were axioms. The three key features of this machine-code Hoare \nlogic are: 1. the program counter is treated as a normal register, 2. code is simply safe-to-execute \n(instruction-cache accurate) data 3. updates to the cache are made local using a cache abstraction. \n A few examples will explain these features. The treatment of the program counter as a normal register \ncan be observed in the following Hoare triple. This Hoare-triple theorem describes an x86 instruction \nxchg eax,ebx, encoded as 93, which swaps the value v of register eax with the value w of register ebx, \nand simultane\u00adously adds 1 (the byte length of the instruction) to the value p of program counter pc. \nFor now, read * informally as and . This sep\u00adarating conjunction * is de.ned and explained in Section \n7.1. {eax v * ebx w * pc p} p : 93 {eax w * ebx v * pc (p + 1)} Direct assignments to the program counter \nare performed by jumps to code pointers, e.g. x86 instruction jmp eax, encoded as FFE0, assigns the value \nof eax to the program counter pc: {eax v * pc p} p : FFE0 {eax v * pc v} This treatment of the program \ncounter signi.cantly eases the effort involved in reasoning about code pointers. The fact that code is \nsimply safe-to-execute data can be ob\u00adserved from the following four theorems. First, code need not be \nin the middle of a Hoare triple: {p} c {q} = {p * code c}\u00d8{q * code c} Second, code can always be weakened \nto data in the postcondition: {p} c {q * code Ldj} =.{p} c {q * datax d} Third, data in the precondition \ncan be strengthened into code: {p * datax d} c {q} =.{p * code Ldj} c {q} Fourth, arbitrary data can \nbe turned into code (i.e. data can be made safe to execute) as a side-effect of certain jump instructions, \ne.g. jmp eax, which is encoded as FFE0 and we already saw in a theorem above, also .ts the theorem: {eax \nv * pc p * datax d} p : FFE0 {eax v * pc v * code Ldj} These Hoare triples {p} c {q} satis.es unconventional \nprop\u00aderties. Their formal de.nition is given in Section 7, but for now informally read them as: for any \nstate which satis.es p * code c, execution of the x86 semantics will always reach a state for which q \n* code c holds, and furthermore, no other part of the state was modi.ed, i.e. our Hoare triple satis.es \nthe frame rule from separa\u00adtion logic [24], {p} c {q} =..r. {p * r} c {q * r} which is essential for \nlocal reasoning. In order to support the frame rule, we had to introduce a lightweight cache abstraction \n(Section 7.3) which makes the non\u00adlocal updates to the instruction cache seem local.  2.3 Veri.cation \nof JIT compilers We have used our Hoare logic for self-modifying x86 to construct two JIT compilers based \non the following work .ow. 1. We start by de.ning the syntax and operational semantics of next the input \nbytecode. Let -. be a relation which describes one execnext step of the execution and let -. be sequence \nof -. that end with execution of a special stop instruction. This semantics represents states as tuples \n(xs, l, p, cs) that consist of a stack xs, a natural number l which keeps track of available stack space, \na bytecode program cs and a program counter p. 2. A coupling invariant, jit inv, is then de.ned which \nallows us to relate states in the bytecode semantics to states in the x86 code, given a base address \na: next .sta. s -. t =.{jit inv sa}\u00d8{jit inv ta} Informally, this jit inv assertion requires, for bytecode \nstate (xs, l, p, cs) and base address a, that a stack is located in x86 memory containing xs and free \nspace l, that x86 code equivalent to bytecode program cs is stored in memory from address a onwards and \nthat the x86 program counter contains an x86 address equivalent to bytecode program counter p. The invariant \nfor the incremental version of our JIT compiler further requires that a code generator is present and \nthat a mapping is maintained which keeps track of what and where bytecode instructions have been translated \ninto x86 code. exec 3. It then follows that each successful execution -. in the byte\u00adcode semantics is \nmimicked by the x86 implementation. The x86 code for the special stop instruction exits by jumping to \nan address w given in register edx. Here stack is the stack assertion from inside jit inv. exec (xs, \nl, p, cs) -. (xs2,l2,p2, cs2)=. {jit inv (xs, l, p, cs) a * edx w}\u00d8 {stack (xs2,l2) * pc w * edx w * \nT} This theorem guarantees that any x86 state which satis.es exec jit inv will correctly perform evaluations \naccording to -.. 4. Finally, it remains to produce veri.ed x86 machine code that can establish an appropriate \nstate which satis.es jit inv. For this we needed veri.ed x86 code which implements code gen\u00aderation i.e. \nthe translation from bytecode to x86 code. By ex\u00adploiting backwards compatibility to previously proved \nsynthe\u00adsis tools [22], we were able to easily create this x86 code from veri.ed functional descriptions. \nThe .nal correctness theorem follows as a result of composing the code for establishing jit inv with \nthe code for executing the bytecode program. The resulting correctness theorem states that the JIT compiler \nexec correctly implements -.: given a stack, a string representing the encoding of a bytecode program, \nand enough space to .t the gen\u00aderated x86 code, the JIT compiler will always terminate in a state  exec \nwhere the stack has been updated according to -.. 3. Input language To make this paper readable we chose \na simple input language for our JIT compiler. Syntax. The input language is a stack-based bytecode, supporting \nthe following instructions. Here i is a 7-bit immediate constant. pop pop top element off the stack sub \nsubtract swap swap top two stack elements push i push value i onto stack jump i jump to instruction i \njeq i jump to i, if top two equal jlt i jump to i, if top two less stop halt execution Bytecode programs \nare lists of the above instructions. Concrete encoding. The abstract syntax restricts constants to 7-bit \nvalues in order to make the concrete encoding (almost) readable as a string. Constants are encoded as \na character \"0\" + i. Here ord maps characters to natural numbers, chr is the inverse of ord. imm i = \nchr (ord \"0\" + i) This encoding of immediate constants makes small constants 1, 2, 3 readable strings \n\"1\", \"2\", \"3\", while larger constants are less intuitive, e.g. 41, 42, 43 are encoded as \"X\", \"Z\", \"[\". \nThe concrete encoding of individual instructions is de.ned as the function enc. Here ++ concatenates \nstrings. enc (pop)= \"p\" enc (sub)= \"-\" enc (swap)= \"s\" enc (push i)= \"c\" ++ imm i enc (jump i)= \"j\" ++ \nimm i enc (jeq i)= \"=\" ++ imm i enc (jlt i)= \"<\" ++ imm i enc (stop)= \".\" Bytecode programs are encoded \nas the concatenation of the indi\u00advidual instruction encodings: encode [] = \"\" encode (c :: cs)= enc c \n++ encode cs This concrete encoding makes small bytecode programs almost readable, e.g. the string \"=6<4-j0sj0.\" \nis the encoding of a bytecode program which calculates the greatest common divisor: 0: jeq 6 1: jlt 4 \n2: sub 3: jump 0 4: swap 5: jump 0 6: stop Semantics. The effect of executing a bytecode program is de.ned \nnext. First, let fetch be a function which looks up the next instruc\u00adtion to be executed from a list \nof instructions: fetch n [] = none fetch n (c :: cs)= some c if n =0 fetch n (c :: cs)= fetch (n - 1) \ncs if n> 0 We de.ne the relation next -. to describe the effect of executing the next instruction. Here \nstates are tuples (xs, l, p, cs), where xs is the data stack (a list of 32-bit words), l is a natural \nnumber which keeps track of available stack space, p is the bytecode program counter and cs is the bytecode \nprogram. fetch p cs = some pop next (x :: y :: xs, l, p, cs) -. (y :: xs, l+1,p+1, cs) fetch p cs = some \nsub next (x :: y :: xs, l, p, cs) -. ((x - y) :: y :: xs, l, p+1, cs) fetch p cs = some swap next (x \n:: y :: xs, l, p, cs) -. (y :: x :: xs, l, p+1, cs) fetch p cs = some (push i) next (xs, l+1, p, cs) \n-. (i :: xs, l, p+1, cs) fetch p cs = some (jump i) next (xs, l, p, cs) -. (xs, l, i, cs) fetch p cs \n= some (jeq i) x = y next (x :: y :: xs, l, p, cs) -. (x :: y :: xs, l, i, cs) fetch p cs = some (jeq \ni) x= y next (x :: y :: xs, l, p, cs) -. (x :: y :: xs, l, p+1, cs) fetch p cs = some (jlt i) x<y next \n(x :: y :: xs, l, p, cs) -. (x :: y :: xs, l, i, cs) fetch p cs = some (jlt i) y = x next (x :: y :: \nxs, l, p, cs) -. (x :: y :: xs, l, p+1, cs) nextexec We use -. to de.ne an inductive relation -. that \ndescribes the effect of successfully executing a bytecode program. Each suc\u00adcessful execution must reach \nthe stop instruction. fetch p cs = some stop exec (xs, l, p, cs) -. (xs, l, p, cs) nextexec s -. tt -. \nu exec s -. u 4. Veri.ed JIT compiler version 1 Given the above de.nition of an input language, we can \nconstruct our veri.ed JIT compilers. As mentioned earlier, two JIT compilers will be presented: one JIT \ncompiler generates all of the target x86 code as an initialisation step, the other one produces code \nincrementally on-demand. This section describes the .rst version; the second version is presented in \nSection 5. 4.1 Informal intuition The most basic JIT compilers perform the following steps, they: 1. \ngenerate native machine code from bytecode, and then 2. let the generated code run on bare metal.  \nThe .rst version our JIT compiler implements this separation be\u00adtween code generation and code execution. \nThe code generation, we consider, is also very simple: each bytecode instruction is always translated \nto the same x86 instruc\u00adtions. Before describing the exact mapping from bytecode instruc\u00adtions to x86 \ninstructions, we start with an informal description of how the stack xs in the state of a bytecode program, \nfrom the pre\u00advious section, is represented on x86:  register eax holds the value of the top of the \nstack xs,  register edi points to the rest of the stack; we write [edi] for the location of the .rst \nelement of the rest of the stack.  register edx holds the address to which stop is to jump.  The choice \nof general purpose registers eax, edi, edx is arbitrary, and does not depend on any special x86 features. \nThe mapping from bytecode instructions is informally the fol\u00adlowing. Push and pop, translate to some \npointer arithmetic and move instruction mov, subtraction translates to subtraction on x86, the swap instruction \ntranslates to x86 instruction xchg (exchange), and jump instructions are translated into jump instructions \non x86, conditional jumps require a compare instruction before the jump. pop mov eax,[edi]; add edi,4 \nsub sub eax,[edi] swap xchg [edi],eax push i sub edi,4; mov [edi],eax; mov eax,i jump i jmp offset \njeq i cmp eax,[edi]; je offset jlt i cmp eax,[edi]; jb offset stop jmp edx The tedious part is to \nget the jump offsets correct as x86 instruc\u00adtions vary in length. Fortunately, our proof methodology \nhelped us .nd and remove our off-by-one bugs early in the design.  4.2 Invariant maintained by the generated \nx86 code The previous section described informally a coupling invariant which the generated by x86 code \nmaintains to the bytecode. We start the construction of our JIT compiler by .rst formalising an invariant \njit inv and proving that execution of each bytecode in\u00adstruction respects this invariant: next .sta. \ns -. t =.{jit inv sa}\u00d8{jit inv ta} Invariant jit inv maintains a stack, makes sure that appropriate x86 \ncode is in memory, and also requires that the value of x86 program counter is set correctly. We will \nformalise each part of the invariant separately. The stack consists of an array-like list of 32-bit words \nxs, that grows from address a upwards. Here M ax asserts that 32-bit word x is located at address a, \nwhile emp is just the unit of *. list a [] = emp list a (x :: xs)= M ax * list (a+4) xs We also have \nto be precise about what space is reserved as free stack space. Let space an state that there is space \nfor n instances of M elements below address a. space a 0= emp space a (n+1) = .x. M (a-4) x * space (a-4) \nn The stack maintained by jit inv has the following speci.cation: the stack must not be empty [], the \ntop of the stack is located in eax while the rest is placed in memory from some address a upwards, address \na is stored in register edi. We also add an assertion which requires that a is 32-bit word aligned, i.e.a \n&#38;3=0. stack ([],l)= F stack (x :: xs, l)= .a. eax x * edi a *(a &#38;3=0)* list a xs * space al To \nformalise that appropriate x86 code is in memory, we need to de.ne the exact x86 encoding into which \neach bytecode should be translated. A 32-bit immediate constant w is represented as follows in an instruction \nas a list of 4 bytes. Here w2w translates 32-bit words to 8-bit words, and \u00bb is logical-shift right. \nximm w = [w2w w, w2w (w \u00bb 8), w2w (w \u00bb 16), w2w (w \u00bb 24)] The encoding of each instruction is now de.ned \nas xenc where t is a function which given the address of a bytecode instruction returns the 32-bit address \nfor the corresponding x86 instruction; w2n converts an unsigned n-bit word into a natural number, and \nin this case w2w converts 7-bit words to 8-bit words. xenc t (pop) =[8B, 07, 83, C7, 04] xenc t (sub) \n=[2B, 07] xenc t (swap) =[87, 07] xenc t (stop) =[FF, E2] xenc t (push i)=[83, EF, 04, 89, 07, B8, w2w \ni, 00, 00, 00] xenc t (jump i)= [E9] ++ ximm (t (w2n i) - 5) xenc t (jeq i) =[3B, 07, 0F, 84] ++ ximm \n(t (w2n i) - 5) xenc t (jlt i) =[3B, 07, 0F, 82] ++ ximm (t (w2n i) - 5) Using this encoding function \nwe de.ne the length of each encoding. Here n2w converts a natural number into a 32-bit word. xenc length \nc = n2w (length (xenc (.x. 0) c)) Now we can de.ne a mapping from address p in the bytecode program cs \nto addresses in the corresponding x86 code, given that the x86 code starts at address a: addr cs a 0= \na addr [] ap = a addr (c :: cs) a (p+1) = addr cs (a + xenc length c) p The code assertion, code Lmj, \nwhich jit inv will use, asserts, for each a . domain m, that the memory of the x86 state contains code \nbyte m(a) at address a. Let memb a m bs make sure that bytes bs appear in m from address a onwards. memb \nam [] = T memb am (b :: bs)=(m(a)= b) . memb (a+1) m bs We can now de.ne an assertion which states that \nm contains code for an entire bytecode program cs, from address a onwards: code in mem cs a m = .p c. \nfetch p cs = some c =. let branch =(.i. addr cs a i - addr cs a p) in memb (addr cs a p) m (xenc branch \nc) The value of the x86 program counter should always point at the addr cs a p,i.e. contain the address \nwhere instruction with index p is stored. Using these de.nitions we de.ne jit inv as maintaining a stack, \nprogram counter, and appropriate x86 code. (Here s hides the value of the e.ags that are irrelevant between \ninstructions.) jit inv (xs, l, p, cs) a = stack (xs, l) * pc (addr cs a p) * s * .m. code Lmj*(code in \nmem cs a m) next Proving that each case of -. satis.es jit inv is a simple ex\u00adercise of applying the \nproof rules from Section 7.5 to Hoare triple theorems for each of the instructions that xenc can generate. \nFor example, proving the case for swap starts from theorem: {eax x * edi w * M wy * pc p} p : 8707 {eax \ny * edi w * M wx * pc (p+2)} which then implies the following, (m(p)= 87) . (m(p+1) = 07)=. {eax x * \nedi w * M wy * pc p} Lmj {eax y * edi w * M wx * pc (p+2)}  which in turn leads the following, etc. \ncode in mem cs a m =. {stack (x :: y :: xs, l) * pc (addr cs a p) * code Lmj} \u00d8 {stack (y :: x :: xs, \nl) * pc (addr cs a (p + 1)) * code Lmj} 4.3 Bytecode programs are executed by x86 code The previous \nsection showed how to prove that jit inv is maintained next for each transition of -.: next .sta. s -. \nt =.{jit inv sa}\u00d8{jit inv ta} exec To prove that all successful executions -. are handled correctly, \nwe also proved, in much the same manner, the following result for the stop instruction: fetch p cs = \nsome stop =. {jit inv (xs, l, p, cs) a * edx w} \u00d8 {stack (xs, l) * pc w * edx w * T} The two theorems \nabove are suf.cient for proving that any exec successful execution of a bytecode program, i.e. -., will \nalways be computed by the x86 code inside jit inv: exec (xs, l, p, cs) -. (xs2,l2,p2, cs2)=. {jit inv \n(xs, l, p, cs) a * edx w}\u00d8 {stack (xs2,l2) * pc w * edx w * T} exec This theorem follows by a simple \ninduction on the relation -..  4.4 Implementing code generation The remaining part of the construction \nof the JIT compiler is to produce veri.ed x86 machine code that can establish an appropriate state which \nsatis.es jit inv in the precondition of the theorem above. For this we needed veri.ed x86 code which \nimplements code generation i.e. translation from bytecode to x86 code. By exploiting backwards compatibility \nto previously developed synthesis tools [22], we can create x86 code from functional de\u00adscriptions in \nHOL4. For example, x86 code for calculating addr, given above, is constructed by .rst de.ning, in HOL4, \na function x86 addr which we think might calculate addr. x86_addr (r1,r5,r6,g) = if r1 = 0w then (r5,g) \nelse let r1 = r1 -1w in if (g r6 = n2w (ORD #\"-\")) . (g r6 = n2w (ORD #\"s\")) . (g r6 = n2w (ORD #\".\")) \nthen let r6 = r6 + 1w in let r5 = r5 + 2w in x86_addr (r1,r5,r6,g) else if g r6 = n2w (ORD #\"p\") then \nlet r6 = r6 + 1w in let r5 = r5 + 5w in x86_addr (r1,r5,r6,g) else if g r6 = n2w (ORD #\"c\") then let \nr6 = r6 + 2w in let r5 = r5 + 10w in x86_addr (r1,r5,r6,g) else if g r6 = n2w (ORD #\"j\") then let r6 \n= r6 + 2w in let r5 = r5 + 5w in x86_addr (r1,r5,r6,g) else if (g r6 = n2w (ORD #\"=\")) . (g r6 = n2w \n(ORD #\"<\")) then let r6 = r6 + 2w in let r5 = r5 + 8w in x86_addr (r1,r5,r6,g) else (r5,g) Our synthesis \ntool can from such functions generate x86 code and prove that the generated x86 code executes the input \nfunction. In this case, the tool returns a Hoare-triple theorem which states that x86 addr is executed \nby the generated code: x86 addr pre (eax, ecx, ebx, g)=. {eax eax * ebx ebx * ecx ecx * data g * pc p \n* s} p : 83F8007449 ... {let (ecx, g)= x86 addr (eax, ecx, ebx, g) in eax * ebx * ecx ecx * data g * \npc (p+78) * s} By then manually proving that x86 addr calculates addr correctly, string in memory(a, \nencode cs, g) . i< 256 =. x86 addr pre (n2w i, w, a, g) . x86 addr (n2w i, w, a, g)=(addr csw i,g) we \ncan produce a formal guarantee that the x86 code produced by our synthesis tool indeed calculates addr. \nWe produced the full code generator that establishes the invari\u00adant jit inv by synthesising x86 code \nfrom functions which were manually proved to correctly write x86 instructions into memory with respect \nto jit inv.  4.5 Final correctness theorem The resulting overall correctness theorem states that the \nJIT com\u00ad exec piler correctly implements -.: given a stack (xs, l), a string rep\u00adresenting the encoding \nof a bytecode program cs, and enough space to .t the generated x86 code, this JIT compiler will always \ntermi\u00ad exec nate in a state where the stack has been updated according to -.. exec (xs, l, p, cs) -. \n(xs2,l2,p2, cs2) . string in memory(a, encode cs, g)=. {stack (xs, l) * data g * edx a * enough space \nfor cs} p : 89D789C60F ... {stack (xs2,l2) * pc (p+410) * T}  4.6 Basic benchmark Finally, we conducted \nexperiments to discover how fast or slow this JIT compiler is on real x86 hardware (a 2.6 GHz Intel processor). \nIn order to run our veri.ed machine code, we wrote a small C wrap\u00adper which essentially just jumps to \nthe veri.ed machine code after making the necessary calls to the operating system for allocating heap \nspace with execute permissions enabled. (The veri.ed ma\u00adchine code is supplied to the GNU C compiler \ngcc as an assembly .le consisting of only .byte directives.) Our JIT compiler calculates the greatest \ncommon divisor (GCD) using the bytecode program \"=6<4-j0sj0.\" (Section 3) in very fast times for small \ninputs, e.g. 135 and 345: $ time ./jit \"=6<4-j0sj0.\" 135 345 Top of stack on exit: 5 real 0m0.003s user \n0m0.001s sys 0m0.002s Only for large inputs, e.g. the pair 2 and 200,000,000, do we get more reliable \nmeasurements: $ time ./jit \"=6<4-j0sj0.\" 2 200000000 Top of stack on exit: 2 real 0m0.131s user 0m0.128s \nsys 0m0.003s Incidentally, the time, 0.128 seconds, matches the execution time of a C program which \ncalculates GCD using a loop: while (x != y) { if (x <y) { y= y-x; } else { x =x-y; }} Our C program gcd.c \nis compiled using gcc version 4.0.1 and optimisation .ag -O3:  $ gcc -O3 gcd.c -o gcd $ time ./gcd 200000000 \n2 GCD: 2 real 0m0.129s user 0m0.127s sys 0m0.002s Our JIT compiler makes no optimisations and thus the \ncomparison can easily be made in favour of gcc by simply providing our JIT compiler with less optimal \nbytecode, e.g. \"=8<4sj0s-sj0.\". However, the point which we want to make is that simple JIT compilers \nproduced as described above are not necessarily slow; by providing the JIT compilers with optimised bytecode \nwe can get competitive performance. 5. Veri.ed JIT compiler version 2 incremental The previous section \npresented a JIT compiler which generates all of the native code at once and then jumps to the generated \ncode. This section describes the construction of a veri.ed JIT compiler which only generates code on-demand, \nincrementally. The construction and proof of this incremental JIT compiler fol\u00adlows very closely the \nwork.ow of the simpler non-incremental JIT compiler. Therefore, this section will mainly concentrate \non de\u00adscribing the intuition of how this incremental JIT compiler works, and will point out how the invariant \nused in the proof had to be modi.ed. 5.1 Informal intuition The basic intuition for our incremental JIT \ncompiler is that each non-branching instruction (sub, swap, pop, push,stop) is generated as before, but \nbranch instructions (jump, jlt, jeq) will be generated with calls to the code generator, e.g. bytecode \ninstruction jeq i at location p will always initially result in the following x86 code:1 cmp eax,[edi] \nje L xor ecx, p+1 call ebx L: xor ecx, i call ebx When this code is run one of the call instructions \nwill call the code generator, whose address is in register ebx. The value in register ecx will tell the \ncode generator for which bytecode instruction it should generate x86 code. Suppose for this example that \nthe x86 code calls the code gen\u00aderator using the .rst call, i.e. with p +1 in ecx, then the code generator \nwill perform a look-up in a table it maintains of where and what bytecode instructions have been translated \nto x86 code. If it .nds that p +1 has not yet been generated then it translates the longest sequence \nof non-jump instructions around p +1 into na\u00adtive code and replaces the xor and call instructions with \na jump directly to desired location in the new code, i.e. the code becomes: cmp eax,[edi] je L jmp G \n L: xor ecx, i call ebx ... (new code starts here) G: (code for instruction p +1 in bytecode) (new code \nends here) 1 Here xor ecx,i has the effect of ecx := i since, whenever such xor instructions are encountered, \necx contains zero. We chose to use xor instead of mov because xor allows us to use a shorter instruction \nencoding. In case the code generator had already generated code for instruc\u00adtion p +1, then no new code \nwould have been generated, instead only the new jump instruction would have been inserted. The interesting \naspect of this JIT compiler is that it can generate slightly different x86 code depending on the input, \ni.e. depending on which branches are taken in which sequence. Branches that are never taken will never \nbe replaced by jmp instructions.  5.2 Invariant This incremental JIT compiler maintains an invariant \nwhich, in\u00adstead of having static code corresponding to the bytecode program, states that a code generator \nis present and x86 code for part of the bytecode exists. The interesting part of the invariant describes \nthe state main\u00adtained in between calls to the code generator. Its state consists of a partial mapping \nfrom bytecode instruction indexes (natural num\u00adbers) to locations (32-bit addresses) where that particular \nbytecode instruction is represented as x86 code. We represent this partial map as a total function to \nan option type; the type of this mapping, for which we will use variable j, is: N . word32 option De.ning \nhow code is represented in memory is slightly more complicated now. In particular, each jump can potentially \nbe repre\u00adsented in two ways, either as the combination of xor and call, or as just a direct jmp. The \nencoding of a jump to bytecode location p, in the generated x86 at address a, is either present as xor \necx, p; call ebx (encoded as 83F1[i]FFD3) or, if ji = some w, simply as an unconditional jump jmp (w-a-5) \n(encoded as E9[w-a-5]). We reuse the de.nition of ximm from the Section 4.2. enc jmp apj bs = (bs =[83, \nF1, n2w p, FF, D3]) . .w. (jp = some w) . (bs =[E9] ++ ximm (w-a-5)) The relation between bytecode instructions \nand x86 code is de.ned as xenc inc capj bs: a relation which states that byte list bs, at machine address \na, is a valid x86 code corresponding to bytecode instruction c, at location p. The .rst 5 cases are identical \nto the encoding xenc for the non-incremental version. xenc inc (pop) apj bs =(bs = xenc (.x. 0) pop) \nxenc inc (sub) apj bs =(bs = xenc (.x. 0) sub) xenc inc (swap) apj bs =(bs = xenc (.x. 0) swap) xenc \ninc (stop) apj bs =(bs = xenc (.x. 0) stop) xenc inc (push i) apj bs =(bs = xenc (.x. 0) (push i)) xenc \ninc (jump i) apj bs = enc jmp a (w2n i) j bs xenc inc (jeq i) apj bs = .bs0 bs1 bs2. (bs = bs0 ++ bs1 \n++ bs2) . (bs0 =[3B, 07, 0F, 84, 05, 00, 00, 00]) . enc jmp (a+8) (p+1) j bs1 . enc jmp (a+13) (w2n i) \nj bs2 xenc inc (jlt i) apj bs = .bs0 bs1 bs2. (bs = bs0 ++ bs1 ++ bs2) . (bs0 =[3B, 07, 0F, 82, 05, 00, \n00, 00]) . enc jmp (a+8) (p+1) j bs1 . enc jmp (a+13) (w2n i) j bs2 We can now de.ne what it means for \nbytecode cs to be repre\u00adsented in memory m according to mapping j: whenever bytecode instruction c is \naccording to j stored at an address w, then there ex\u00adists some sequence of bytes bs, which represents \nc, stored in mem\u00adory m at address w. We use memb de.ned in Section 4.2. code in mem csj m = .p c w. (fetch \np cs = some c) . (jp = some w)=. .bs. memb w mbs . xenc inc cw pj bs  We separately assert a well-formedness \nrequirement on map\u00adping j: each x86 representation of a bytecode instruction which is not a jump must, \nif and only if present in the x86 code, be im\u00admediately followed by the x86 representation of the next \nbytecode instruction. Let ilength c be a function which return the length of each instruction encoding, \ne.g. ilength (jump i)=5. is jump c = .i. c .{ jump i, jeq i, jlt i, stop } wellformed cs j = .c p. (fetch \np cs = some c) . fetch (p+1) cs = none . \u00acis jump c =. ((jp = none) .. (j (p+1) = none)) . .w. (jp = \nsome w)=. (j (p+1) = some (w + ilength c)) The incremental JIT compiler s main invariant, i.e. its version \nof jit inv, now states that the stack is maintained as stack (xs, l), de.ned in Section 4.2; the program \ncounter holds a value eip for which there exists some generated x86 code, i.e. jp = some eip; the code \ncs is represented in memory m according to a wellformed mapping j; and in order to make code generator \nwork, we also include that the code generator is present in memory, that ebx holds a pointer to the entry \npoint in the code generator and register ecx is zero (which is necessary for the xor trick to work). \nSome inessential details are elided with . . . . jit inv (xs, l, p, cs) a = .m j eip w. stack (xs, l) \n* pc eip *(jp = some eip)* s * code Lmj*(code in mem csj m . wellformed cs j)* code (codegen w) * ebx \nw * ecx 0 * ... The veri.cation proof can be found in our HOL4 proof scripts [1]. 6. x86 semantics We \nuse an operational semantics for (user-mode) x86 machine code which builds on a previously presented \nsemantics of x86 [25]. The following subsections will detail how the sequential instantiation of our \npreviously developed x86 semantics is extended to include an instruction cache and read/write/execute \nmemory permissions. 6.1 x86 state and memory model The operational semantics represents x86 states as \ntuples which consist of a register .le r, program counter/instruction pointer e, status bits/e.ags s, \nmemory m and instruction cache i: (r, e, s, m, i) The type de.nition makes use of the following data-types: \na option ::= some a | none regs ::= EAX | EBX | ECX | EDX | EDI | ESI | EBP | ESP e.ags ::= CF | PF | \nAF | ZF | SF perm ::= r | w | x The type of each x86 state component is: r : regs . word32 e : word32 \n s : e.ags . bool option m : word32 . (word8 * perm set) option i : word32 . (word8 * perm set) option \n The option type is used in places where the actual value may be missing, e.g. the value of an e.ag \nin s may be any of three values: some T has value true (written T), some F has value false (written F), \nor none has unde.ned/unpredictable value. Memory locations in m can either be absent (none), or present \n(some). Memory locations contain two components: the 8-bits of data that are stored at the address and \na set of read/write/execute permissions describing how this data can be accessed. All memory reads and \nwrites are de.ned in the operational semantics using read mem, write mem and read instr. read mem a (r, \ne, s, m, i)= case ma of none . none | some (w, p) . if {r}. p then some w else none write mem av (r, \ne, s, m, i)= case ma of none . none | some (w, p) . if {w}. p then some (r, e, s, m[a . some (v, p)],i) \nelse none The function which fetches 8-bits of an instruction gives priority to the instruction cache, \nand requires the execute permission x. read instr a (r, e, s, m, i)= case (i a,m a) of  (none, none) \n. none | (none, some (w, p)) . if {r, x}. p then some w else none | (some (w, p), ) . if {r, x}. p then \nsome w else none Our user-mode semantics has no instructions for altering the permissions attached to \nmemory locations; instead permissions re\u00admain static through out execution. In reality operating systems \npro\u00advide user-mode programs with procedures they can call to alter such read/write/execute permissions \non a per-page granularity.  6.2 Instruction-cache update Our semantics executes an instruction-cache \nupdate icache -. before each instruction is performed. Each update deletes some set of old address from \nthe cache and loads, from memory, a new set of addresses into the cache: icache (r, e, s, m, i) -. (r2,e2,s2,m2,i2) \n= .new old. r2 = r . e2 = e . s2 = s . m2 = m . i2 = .addr. if addr . new then m addr else if addr . \nold then none else i addr This cache update transition over approximates the number of different updates \na real instruction cache can perform, e.g. a real cache cannot load the entire memory. Over approximating \npossible cache updates is suf.cient, since we will prove that no cache update can cause unwanted behaviour. \nA noteworthy feature of this cache update transition is that it will never introduce new inaccuracies. \nWe say that the instruction cache is accurate (not out-of-date) for address a if the instruction cache \neither does not contain an entry for address a or memory location a is correctly represented in the cache, \ni.e. accurate a (r, e, s, m, i)= ia = none . ia = ma Automatic cache updates will never introduce inaccurate \nentries: icache .sta. s -. t . accurate as . accurate at Similarly inaccuracies might be removed: icache \n.a s. \u00ac(accurate as) ..t. s -. t . accurate at Address a becomes inaccurate whenever address a is repre\u00adsented \nin the instruction cache and a store instruction successfully modi.es the byte stored at address a using \nwrite mem av, for some 8-bit data v.  6.3 Next-state relation x86 The top level next-state relation \n-. is de.ned as a composition of an instruction cache update, then fetch-and-decode, followed by execution \nof the fetched instruction. The de.nition of execute can be found in our proof scripts [1]. The function \nfetch and decode is explained in Appendix A. x86 s -. u = .t instr len. icache (s -. t) . (fetch and \ndecode t = some (instr, len)) . (execute instr len t = some u) Both the decoder and the execute function \nhave been tested exten\u00adsively against real x86 hardware, as part of previous work [25]. This gives us \ncon.dence that our semantics is, if not completely correct, at least very nearly completely right for \nthe instructions it covers: ADC ADD AND CALL CMOVA CMOVB CMOVE CMOVNA CMOVNB CMOVNE CMOVNS CMOVS CMP \nCMPXCHG DEC DIV INC JA JB JE JMP JNA JNB JNE JNS JS LEA LOOP LOOPE LOOPNE MOV MOVZX MUL NEG NOT OR POP \nPOPAD PUSH PUSHAD RET SAR SBB SHL SHR SUB TEST XADD XCHG XOR 6.4 Clearing the instruction cache The \nx86 instruction set has no instruction for the sole purpose of clearing the instruction cache. However, \nthe Intel Manual (March 2009) [12] states that it is safe to execute self-modifying code (in sequential \nprograms) if the following steps are taken: 1. store modi.ed code 2. jump to new code or intermediate \ncode 3. execute new code  This ambiguous description makes it seem safe to assume that some kind of \njump is enough to erase any instruction cache inaccuracies that might make the new code unsafe to execute. \nIn order to make as few assumptions as possible, we will only assume that one type of jump has the ability \nto clear the instruction cache. We make clearing the instruction cache a side-effect of execution of \njump instructions of the form jmp r32 , i.e. jumps to a code pointer stored in a 32-bit register. Other \njump instructions, such as branch-to-offset, procedure calls, and procedure returns, are not given this \nextra side-effect. The relevant part of the execute de.nition sets i to the empty cache, i.e. .a.none: \nexecute (Xjmp (Xreg d)) len (r, e, s, m, i)= some (r, r(d), s, m, .a.none) All other cases of execute \nleave the instruction cache i untouched.  6.5 Execution sequences In the next section we will quantify \nover all possible x86 execution sequences. For this purpose, we de.ne a valid execution sequence x from \ninitial state s, written x86 seq sx, to be an in.nite sequence of x86 states (with type N . x86 state) \nsuch that x(0) = s and for each natural number n: x86x86 x(n) -. x(n + 1) if .y. x(n) -. y x(n +1) = \nx(n) otherwise The above de.nition of x86 seq makes stuck states repeat for\u00adever. Repeating stuck states \nis reasonable as we will only consider judgements of total-correctness which asserts that each execution \nsequence x will contain a desirable .nal state x(n) satisfying some postcondition post: .x. x86 seq sx \n=..n. post (x(n)) 7. Machine-code Hoare logic This section presents a de.nition of a Hoare triple and \nassociated Hoare proof rules which allow local reasoning in the presence of an instruction cache. The \nwork presented in this section builds on previous experience [19, 21] in adapting separation logic [24] \nto machine languages. 7.1 Separating conjunction: * Conventionally the separating conjunction * is de.ned \nto split par\u00adtial functions. However, for this work, and previous work on Hoare logic for machine languages, \nwe have found that such a separating conjunction is ill suited for machine languages as processor states \nare essentially multiple different mappings (mappings from regis\u00adter names to register values, memory \nlocations to memory values, status-bit names to bit values etc.). We choose to use a set-based separating \nconjunction in order to treat all resources uniformly and hence make the frame rule (presented later) \napply to all types of resources simultaneously. Our set-based separating conjunction * splits a set (of \nstate elements) into two sets: p * q is true for set s if s can be split into two disjoint sets u and \nv such that p is true for u and q is true for v. (p * q) s = .u v. p u . qv . (u . v = s) . (u n v = \n{}) The separating conjunction * is associative and commutative. Its unit is emp and angled brackets \n(...) will be used for carrying pure boolean assertions (.p c s. (p *(c)) s = ps . c): emp s =(s = {}) \n(b) s =(s = {}) . b This separating conjunction requires states to be represented as sets of state components. \nLet the type of an x86 state component be de.ned by a data-type x86 el with the following constructors. \nA boolean is attached to each memory component to indicate whether of not that byte is accurately represented \nin the instruction cache. Eip : word32 . x86 el Reg : regs . word32 . x86 el Status : e.ags . bool option \n. x86 el Mem : word32 . (word8 * perm) option . bool . x86 el This data-type allows us to de.ne a translation \nfunction x86set which maps states represented as tuples x86 state, as described in the previous section, \nto states represented as sets of x86 state elements. Here range f = { y |.x.f x = y }. x86set (r, e, \ns, m, i)= { Eip e }. range (.a. Reg a (ra)) . range (.a. Status a (sa)) . range (.a. Mem a (ma)(accurate \na (r, e, s, m, i))) Let R rw assert that register r has value w, similarly let S ax assert that e.ag \na has value x, and let pc assert the value of the instruction pointer. (R ax) s =(s = {Reg ax}) (S ax) \ns =(s = {Status ax}) (pc x) s =(s = {Eip x})  The above assertions have their intended meaning when \nused to\u00adgether with the translation function x86set,e.g. (R ax * p)(x86set(r, e, s, m, i)) =. (ra = x) \n(S ax * p)(x86set(r, e, s, m, i)) =. (sa = x) (pc x * p)(x86set(r, e, s, m, i)) =. (e = x) but * also \nseparates between assertions of the same kind: (R ax * R by * p)(x86set(r, e, s, m, i)) =. a = b (S ax \n* S by * p)(x86set(r, e, s, m, i)) =. a = b (pc x * pc y * p)(x86set(r, e, s, m, i)) =. F We will often \nabbreviate R EAX w with just eax w, R EBX w with ebx w etc. Another abbreviating assertion is s which \nhides the values of the e.ags: s = .cpaz s. S CF c * S PF p * S AF a * S ZF z * S SF s Assertion s is \nused in theorems where we need to state that the e.ags were modi.ed but we want hide their actual values, \nwhich are frequently not of interest.  7.2 Memory assertions: M, code, data The most basic memory assertion \nB ax is de.ned to state that byte x, which can be read and written but not executed, is located in memory \nat address a, which might or might not be accurately represented in the instruction cache: (B ax) s = \n.acc. s = {Mem a (some (x, {r, w})) acc} A similar assertion M, which states that 32-bit data w is at \naddress a, can be de.ned using four B assertions. Here w[j-i] extracts bits i to j (inclusive) from 32-bit \nword w. M aw = B (a+0) (w[7-0]) * B (a+1) (w[15-8]) * B (a+2) (w[23-16]) * B (a+3) (w[31-24]) Our machine-code \nHoare triple represents code as set of code fragments: each element is a tuple (a, x, p) where a is a \n32-bit address, x is 8-bits of an instruction and p is write permission w iff location a has write permissions \nset. The code assertion makes sure that code set c is stored in memory and accurately represented in \nthe instruction cache, hence T below. (code c) s = (s = { Mem a (some (x, {r, x,p})) T | (a, x, p) . \nc }) The data assertion data m, which informally states that m(a) is stored in memory at location a if \na . domain m, is de.ned as using auxiliary function aux: aux(m, p, acc)= { Mem a (some (m(a), {r, w,p})) \n(acc a) | a . domain m } Now let data m and datax m, respectively, assert that m is non\u00adexecutable or \nexecutable data, regardless of which addresses are accurately represented acc in the instruction cache: \n(data m) s = .acc. s = aux(m, r, acc) (datax m) s = .acc. s = aux(m, x, acc) When turning datax m into \na code assertion code Lmj, the notation Lmj stands for: { (a, m(a), w) | a . domain m } 7.3 Cache abstraction: \n: Separation logic, which we use as an inspiration for our machine\u00adcode Hoare logic, is based on the \nnotion of local reasoning: each action can be described by a Hoare triple that only describes small local \nupdates, such Hoare triples can separately be brought into a grander context using, what is known as, \nthe frame rule, which will be described later. Applying ideas from separation logic naively to our x86 \nmodel does not work as the updates to the instruction cache are non-local and can occur at random; a \nbasic set up that exposes instruction\u00adcache updates would struggle to support the vital frame rule. In \norder to regain local-reasoning, the de.nition of our Hoare triple will assert pre/postconditions p, \nnot just as, p (x86set(s)) but instead using an instruction cache abstraction p : s which allows p to \nbe true for some state t with a less accurate instruction cache but otherwise equivalent to s: icache \np : s = .t. t -. s . p (x86set t) Since all non-local cache updates only introduce new accuracies, a \ndifferent state t can always be chosen in such a way that old inac\u00adcuracies (that p might potentially \ndepend on) can be reintroduced before p is asserted. 7.4 De.nition of Hoare triple: {p} c {q} The previous \nsubsections de.ned the necessary building blocks for our instruction-cache-aware Hoare triple, namely: \n:, code and x86 seq. We .rst de.ne pq to be a total-correctness Hoare triple without any code: if some \npart of the x86 state satis.es p then every possible execution sequence will reach a state which satis.es \nq. Here * r ensures that every resource that is modi.ed is mentioned in precondition p. pq = .s r. (p \n* r) : s . .x. x86 seq sx ..n. (q * r) : x(n) Our machine-code Hoare triple {p} c {q} is an abbreviation \nwhich maintains code c as an invariant separate from pre-and postcondition p and q, respectively: {p} \nc {q} =(p * code c)(q * code c) The relationship between the two judgements can be seen in the following \ntheorems. Here \u00d8 is the empty set. {p}\u00d8{q} = pq {p} c {q} = {p * code c}\u00d8{q * code c} These arise from \nthe fact that: p * code \u00d8 = p * emp = p.  7.5 Proof rules The machine-code Hoare triple, de.ned above, \nsupports a few un\u00adusual proof rules, i.e. theorems proved from the de.nition of our machine-code Hoare \ntriple {p} c {q} and x86 semantics. Sec\u00adtion 2.2 already presented the following proof rules for transform\u00ading \ndata into code and vice versa: {p} c {q} = {p * code c}\u00d8{q * code c} {p} c {q * code Ldj} =.{p} c {q \n* datax d}  {p * datax d} c {q} =.{p * code Ldj} c {q} Section 2.2 also mentioned that jumps to code \npointers, i.e. instruc\u00adtions of the form jmp r32, can turn data into code, e.g. jmp eax, encoded as FFE0, \nturns data into code: {eax v * pc p * datax d} p : FFE0 {eax v * pc v * code Ldj} Other code related \nrules include the rule for code extension: {p} c {q} =..e. {p} (c . e) {q}  which illustrates well that \nthese Hoare triples state only that code c is suf.cient to transform states satisfying p into states \nsatisfying q. Thus any extension e to set c will also be suf.cient for transforming states satisfying \np into states satisfying q. The last code related rule is one which introduces Lmj: {p} c {q} =. ((.a \nw. (a, w) . c =. m(a)= w)=.{p}Lmj{q}) Other more conventional rules are: {p} c {q} =..r. {p * r} c {q \n* r} {p} c1 {q}.{q} c2 {r} =.{p} c1 . c2 {r} {p} c {q}. (.s. q s =. rs)=.{p} c {r} {p} c {q}. (.s. \nr s =. ps)=.{r} c {q}  {.x. p x} c {q} = .x. {px} c {q} {p *(b)} c {q} =(b =.{p} c {q}) 8. Quantitative \ndata Large parts of the basis for this work, the x86 semantics, machine\u00adcode Hoare logic and some proof \nautomation, consist of only minor extensions to proof scripts/automation which were developed in previous \nwork [19, 21, 22, 25]. The following table lists the number of lines of HOL4 code, i.e. proof scripts \nand automation, that were reused (column old) and that are new for this work (column new). old new total \nx86 semantics 2319 150 2469 general Hoare logic 425 7 432 x86 instantiation of Hoare logic 587 430 1017 \nproof automation* 4851 21 4872 input language for JIT compiler 0 99 99 JIT compiler version 1 0 1231 \n1231 JIT compiler version 2 0 2550 2550 total 8182 4488 12670 * includes a proof-producing compiler [22] \nwhich maps functions in the logic of HOL4 to ARM, x86 and PowerPC machine code, used in Section 4.4. \nThis compiler is based on a proof-producing decom\u00adpiler [21] which is also included in the number 4851. \nThe veri.ed JIT compilers can be run on real x86 hardware. A few tests, in Section 4.6, suggest that \nthe JIT compiler is reason\u00adably ef.cient: the JIT compiler evaluates the bytecode program for calculating \nthe greatest common divisor (GCD), from Section 3, for the pair 2 and 200,000,000 in 0.13 seconds a \ntime which matches the execution time of compiled C code for calculating GCD. The C code to which we \ncompare, also listed in Section 4.6, was compiled using the GNU C compiler gcc with optimisation turned \non. 9. Related work This paper has touched on a number of topics: JIT compilation, self-modifying code, \nveri.cation of machine-code programs and compiler veri.cation. Related work on each topic is brie.y dis\u00adcussed \nbelow. JIT compilation. The history of JIT compilation has been surveyed by Aycock [3] who traced back \nthe origins of JIT compilation to McCarthy s LISP paper in 1960 [15]. But the term just-in-time (JIT) \nwas only introduced in connection with attempts to speed up Java, which was slow before using JIT technology: \nJava isn t just slow, it s really slow, surprisingly slow. from Tyma [28] and Aycock [3] Discussions \nsuch as those of Tyma [28] and Aycock [3] make it clear that JIT compilation is vital for implementation \nof ef.cient interpreters for modern languages: Java, C#, ML etc. Self-modifying code. There is very little \npublished work [8, 6] on veri.cation of self-modifying code. To the best of our knowledge, Gerth [8] \nwas the .rst to propose a solution. His solution is to not have code but instead only data. He demonstrates \nthat linear temporal logic can be used to prove functional correctness and termination of self-modifying \ncode in a toy assembly language. Gerth presents only a few small examples, questions whether his approach \nwill scale, and does not include an instruction cache in his model of the assembly language. More recent \nwork by Cai et al. [6] is somewhat more closely related to our work: they use a Hoare logic and target \nlarger case studies in more realistic machine languages. Cai et al. developed an extension, called GCAP, \nto an assembly-level Hoare logic called CAP [29], and formalised GCAP in the Coq theorem prover. They \npresent a long list of small veri.cation examples for MIPS and 16\u00adbit x86 code, some of which generate \ncode dynamically, but none of which is a full JIT compiler. One of the main differences to this paper \nis their approach for mutable code: they introduce the concept of possibly overlapping code blocks that \nmight or might not be present at any given time, but one of which must be present in memory when starting \nexecu\u00adtion of a block at that address. In comparison, our approach seems much simpler since for us code \nis just safe-to-execute data: {p} c {q} = {p * code c}\u00d8{q * code c} Another contrast to the work by Cai \net al. is that our work builds on a semantics of x86 which takes into account hazards of an out\u00adof-date \ninstruction cache, while their work ignores the instruction cache. Also our framework provides total-correctness \nresults, while GCAP produces partial-correctness results. Veri.ed machine code. Construction of the two \nversions of our JIT compiler required veri.ed (functional correctness) implemen\u00adtations of code generation, \ni.e. x86 code that reads the input byte\u00adcode and produces equivalent x86 code. The obvious way of creating \nthis veri.ed code would have been to, .rst, either handcraft or generate the machine code, then apply \npost hoc veri.cation to the proposed machine code. Such an approach could have used: symbolic simulation \nwhich the ACL2 community has empha\u00adsised and successfully applied [4, 17], e.g. Boyer and Yu [5] veri.ed \nof machine code for the Motorola MC68020;  a programming logic directly,e.g. Cai et al. [6] chose this \nmethod, which works for small examples, but is very labour\u00adintensive to apply to larger examples [16]; \n veri.cation condition generation (VCG) applied to machine code: Matthews et al. [14] presented a light-weight \napproach to trustworthy VCG for machine code an approach that Hardin et al. have applied to AAMP7G machine \ncode [10]; or  decompilation into logic a new method proposed by Myreen et al. [21] which maps, via \nproof, machine code into equivalent functions in logic (details and examples in Myreen [19]).  Instead \nof using any of the above methods for post hoc veri.cation of handcrafted code, we chose to synthesis \nappropriate machine code using a proof-producing compiler [22]. Our compiler maps functions from the \nlogic of HOL4 down to functionally equivalent x86 machine code and completely automatically proves a \ncerti.cate theorem. In order to get the desired veri.ed machine code we wrote functions with the appropriate \nbehaviour, proved them correct, and then related the veri.cation result to the automatically generated \nmachine code using the certi.cate theorem produced by our proof\u00adproducing compiler [22].  In.uential \nwork on proof-carrying code (PCC) by Necula [23], typed-assembly language (TAL) by Morrisett et al. [18], \nand foun\u00addational PCC (FPCC) by Appel [2, 27] share the goal of creating trustworthy software, but are \nnot directly applicable, since work in this direction has aimed to automatically ensure safety properties. \nIn contrast, this paper targets much stronger properties of full func\u00adtional correctness and termination. \nCompiler veri.cation. This paper touched on the topic of com\u00adpiler veri.cation: the code generator inside \nthe JIT compilers was proved to correctly translate (without any optimisations) bytecode into x86 machine \ncode. From the point of view of compiler veri\u00ad.cation, this is a very simple and slightly unusual implementation \nto prove correct. Most papers on compiler veri.cation, of which Dave has made a survey [7], concentrate \non proving a few optimis\u00ading transformations correct. An impressive exception to this trend is Leroy \ns recent proof a full end-to-end implementation of an op\u00adtimising C compiler [13]. Another contrast to \nprevious work on compiler veri.cation, is that our simple code generator is implemented in x86 machine \ncode (numbers) and produce concrete x86 code as output (numbers); while majority of other work implement \nthe compiler in small toy languages or functional languages and output either assembly code or code in \nan intermediate language used only inside compilers. 10. Future work The input language of our veri.ed \nJIT compiler is a simple stack\u00adbased bytecode. In the future, we plan to extend this input language to \na bytecode language suitable for reimplementation of our ver\u00adi.ed x86, ARM and PowerPC implementations \nof a LISP inter\u00adpreter [20], i.e. we aim to extend the language to operate over a garbage collected stack \nof s-expressions. Such an extension ought to be largely orthogonal to the developments here as that extension \nshould boil down to replacing the stack assertion in jit inv with a more complicated assertion sexp stack, \nwhile code generation is, in principle, unchanged. Acknowledgments I would like to thank John Matthews \nfor suggesting that I try to tackle the problem of verifying a JIT compiler. I am grateful for comments \nfrom Mike Gordon. This work was funded by the EPSRC, UK. A. Instruction fetch and decode Modelling fetch \nand decode for x86 is made interesting due to the complex instruction encodings that result in variable \nlength instructions. Modelling x86 instruction fetch and decode was part of previous work [25] but did \nnot get a thorough explanation there, so we present fetch-and-decode here, as the current author was \nresponsible for modelling of fetch-and-decode for [25]. Fetch. First, how do we cleanly separate fetching \nfrom decoding? The problem is that fetching needs to know how many bytes to fetch, but the number of \nbytes (the length of the x86 instruction) is only known after decoding. A close inspection of the x86 \nmanual [12] shows that no 32-bit mode x86 instruction is longer than 20 bytes. Our solution is to .rst \nfetch 20 bytes using fetch 20 as, fetch 0 as = [] fetch (n+1) as = read instr as :: fetch n (a+1) s Then \nevaluate decode (outlined later) on a list in which errors, i.e. none elements, have been replaced by \nzero: decode is applied to clean (fetch 20 as) where clean is: clean [] =[] clean (none :: cs) = 0:: \nclean cs clean (some x :: cs)= x :: clean cs A successful application of decode returns some (i, len), \nwhere i is the abstract syntax tree representation of the instruction and len is the length (number of \nbytes) of the encoding. Before returning the result, we check that the .rst len bytes are not none, using \nnot none len: not none 0 cs = T not none (n+1) (none :: cs)= F not none (n+1) (some x :: cs)= not none \nn cs The de.nition of fetch and decode is hence: fetch and decode (r, e, s, m, i)= let cs = fetch 20 \ne (r, e, s, m, i) in case decode (clean cs) of none . none | some (i, len) . if not none len cs then \nsome (i, len) else none Decode. The Intel Manual [12] de.nes instruction encodings using lists of the \nfollowing style: each line has two parts, the .rst part (left of the bar) provides the encoding formats, \nthe second part (right of the bar) provides the assembly instruction. Most assembly instructions have \nmultiple different encodings. Here are some of the encodings for the move instruction mov: \" 89 /r | \nMOV r/m32, r32 \" \" 8B /r | MOV r32, r/m32 \" \" B8+rd id | MOV r32, imm32 \" \" C7 /0 id | MOV r/m32, imm32 \n\" The Intel manual provides a description of what each encoding symbol 89, B8+rd, /r, etc. means. In \norder to minimise errors that can occur when trying to write the equivalent de.nitions in a theorem prover, \nwe decided to copy across these lines from the Intel Manual into HOL4 and then write, in the HOL4 logic, \nan interpreter for this syntax based on Intel s description of what it means. The interpreter .rst splits \neach string into two at the bar character | and then breaks each side up into a list of tokens, e.g. \nthe .rst line from above becomes the pair: [\"89\", \"/r\"] and [\"MOV\", \"r/m32\", \"r32\"] When the interpreter \nreceives a concrete input to decode, e.g. a list of bytes [0x89, 0x07, 0x86, . . . ], it will execute \na match function which attempts to .t the encoding format, in this case [\"89\", \"/r\"], onto the concrete \nbyte list. If a match is found then a separate func\u00adtion constructs the data-type used for representing \nthe assembly in\u00adstruction in our operational semantics. The interpreter also returns the unused tail \nof the input. The example above produces: some (Xmov (Xrm r (Xm none (some EDI) 0) EAX), [0x86,...]) \nThis output from the interpreter means that decoding found x86 instruction mov [edi], eax and that input \n[0x86, . . . ] was left unused, and hence the instruction consisted of the .rst two bytes of the input. \nThe top-level decode function returns the generated data-type together with the number of bytes consumed \nfrom the input list.  Decoder speed-up. Running the decoder inside the theorem prover logic, e.g. evaluating \ndecode [0x89, 0x07, 0x86, ...], to prove de\u00adcoding results about speci.c machine instruction is very \nslow if done naively. It takes nearly 15 minutes for HOL4 to evaluate the above example without helping \nlemmas. (Evaluating decode out\u00adside the logic is not acceptable as we want a theorem certi.ed by the \nlogical core of HOL4.) However, a signi.cant speed up can eas\u00adily be achieved by partially evaluating \nthe decode function for the list of x86 instruction encodings, i.e. the list of strings mentioned above. \nWhen the theorem produced by partial evaluation is supplied to the standard HOL4 s evaluation engine, \nevaluation is performed inside the logic in less than 2 seconds for most instruction, some take up to \na bearable 6 seconds. References [1] HOL4 proof scripts, veri.ed x86 code and other supporting material: \nhttp://www.cl.cam.ac.uk/~mom22/jit/. [2] Andrew W. Appel. Foundational proof-carrying code. In Logic \nin Computer Science (LICS). IEEE, 2001. [3] John Aycock. A brief history of just-in-time. ACM Computing \nSurveys, 35:97 113, 2003. [4] R. S. Boyer and J S. Moore. Proving theorems about pure LISP functions. \nJACM, 22(1):129 144, 1975. [5] Robert S. Boyer and Yuan Yu. Automated proofs of object code for a widely \nused microprocessor. J. ACM, 43(1):166 192, 1996. [6] Hongxu Cai, Zhong Shao, and Alexander Vaynberg. \nCerti.ed self\u00admodifying code. In Jeanne Ferrante and Kathryn S. McKinley, editors, Programming Language \nDesign and Implementation (PLDI), pages 66 77. ACM, 2007. [7] Maulik A. Dave. Compiler veri.cation: a \nbibliography. SIGSOFT Softw. Eng. Notes, 28(6):2 2, 2003. [8] R. Gerth. Formal veri.cation of self modifying \ncode. In Int. Conf. for Young Computer Scientists, pages 305 313. International Academic Publishers, \nChina, 1991. [9] Michael J. C. Gordon. Mechanizing programming logics in higher or\u00adder logic. In Current \nTrends in Hardware Veri.cation and Automated Theorem Proving. Springer, 1989. [10] David S. Hardin, Eric \nW. Smith, and William D. Young. A robust machine code proof framework for highly secure applications. \nIn Panagiotis Manolios and Matthew Wilding, editors, Proceedings of the Sixth International Workshop \non the ACL2 Theorem Prover and Its Applications, 2006. [11] C. A. R. Hoare. An axiomatic basis for computer \nprogramming. Communications of the ACM, 12(10):576 580, 1969. [12] Intel. Intel 64 and IA-32 Architectures \nSoftware Developers Manual. Intel Corporation, March 2009. [13] Xavier Leroy. Formal certi.cation of \na compiler back-end, or: pro\u00adgramming a compiler with a proof assistant. In Principles of Pro\u00adgramming \nLanguages (POPL), pages 42 54. ACM Press, 2006. [14] John Matthews, J. Strother Moore, Sandip Ray, and \nDaron Vroon. Ver\u00adi.cation condition generation via theorem proving. In Logic Program\u00adming and Automated \nReasoning (LPAR), volume 4246 of LNCS, pages 362 376. Springer, 2006. [15] John McCarthy. Recursive functions \nof symbolic expressions and their computation by machine, part I. Communications of the ACM, 1960. [16] \nAndrew McCreight, Zhong Shao, Chunxiao Lin, and Long Li. A gen\u00aderal framework for certifying garbage \ncollectors and their mutators. In Jeanne Ferrante and Kathryn S. McKinley, editors, Proceedings of the \nConference on Programming Language Design and Implementation (PLDI), pages 468 479. ACM, 2007. [17] J \nStrother Moore. Symbolic simulation: An ACL2 approach. In Ganesh Gopalakrishnan and Phillip J. Windley, \neditors, Formal Meth\u00adods in Computer-Aided Design (FMCAD), pages 334 350, 1998. [18] J. Gregory Morrisett, \nDavid Walker, Karl Crary, and Neal Glew. From System F to typed assembly language. In Principles of Programming \nLanguages (POPL), pages 85 97. ACM Press, 1998. [19] Magnus O. Myreen. Formal veri.cation of machine-code \nprograms. PhD thesis, University of Cambridge, 2009. [20] Magnus O. Myreen and Michael J.C. Gordon. Veri.ed \nLISP imple\u00admentations on ARM, x86 and PowerPC. In Stefan Berghofer, To\u00adbias Nipkow, Christian Urban, \nand Makarius Wenzel, editors, Theorem Proving in Higher Order Logics (TPHOLs), LNCS. Springer, 2009. \n[21] Magnus O. Myreen, Konrad Slind, and Michael J. C. Gordon. Machine-code veri.cation for multiple \narchitectures An application of decompilation into logic. In Alessandro Cimatti and Robert B. Jones, \neditors, Formal Methods in Computer Aided Design (FMCAD). IEEE, 2008. [22] Magnus O. Myreen, Konrad Slind, \nand Michael J.C. Gordon. Extensi\u00adble proof-producing compilation. In Michael I. Schwartzbach Oege de \nMoor, editor, Compiler Construction (CC), LNCS. Springer, 2009. [23] George C. Necula. Proof-carrying \ncode. In Principles of Programming Languages (POPL), pages 106 119. ACM, 1997. [24] John Reynolds. Separation \nlogic: A logic for shared mutable data structures. In Proceedings of Logic in Computer Science (LICS). \nIEEE Computer Society, 2002. [25] Susmit Sarkar, Pater Sewell, Francesco Zappa Nardelli, Scott Owens, \nTom Ridge, Thomas Braibant Magnus O. Myreen, and Jade Alglave. The semantics of x86-CC multiprocessor \nmachine code. In Principles of Programming Languages (POPL). ACM, 2009. [26] Konrad Slind and Michael \nNorrish. A brief overview of HOL4. In Otmane A\u00a8it Mohamed, C\u00b4noz, and So.` esar Mu ene Tahar, editors, \nTheorem Proving in Higher Order Logics (TPHOLs), LNCS, pages 28 32. Springer, 2008. [27] Gang Tan and \nAndrew W. Appel. A compositional logic for control .ow. In E. Allen Emerson and Kedar S. Namjoshi, editors, \nProceed\u00adings of Veri.cation, Model Checking and Abstract Interpretation (VM-CAI), LNCS. Springer, 2006. \n[28] Paul Tyma. Why are we using Java again? Commun. ACM, 41(6):38 42, 1998. [29] Dachuan Yu, Nadeem \nA. Hamid, and Zhong Shao. Building certi.ed libraries for PCC: Dynamic storage allocation. Science of \nComputer Programming, 50(1-3):101 127, 2004.    \n\t\t\t", "proc_id": "1706299", "abstract": "<p>This paper presents a method for creating formally correct just-in-time (JIT) compilers. The tractability of our approach is demonstrated through, what we believe is the first, verification of a JIT compiler with respect to a realistic semantics of self-modifying x86 machine code. Our semantics includes a model of the instruction cache. Two versions of the verified JIT compiler are presented: one generates all of the machine code at once, the other one is incremental i.e. produces code on-demand. All proofs have been performed inside the HOL4 theorem prover.</p>", "authors": [{"name": "Magnus O. Myreen", "author_profile_id": "81392605670", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P1911050", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706313", "year": "2010", "article_id": "1706313", "conference": "POPL", "title": "Verified just-in-time compiler on x86", "url": "http://dl.acm.org/citation.cfm?id=1706313"}