{"article_publication_date": "01-17-2010", "fulltext": "\n A Simple, Veri.ed Validator for Software Pipelining (veri.cation pearl) Jean-Baptiste Tristan Xavier \nLeroy INRIA Paris-Rocquencourt INRIA Paris-Rocquencourt B.P. 105, 78153 Le Chesnay, France B.P. 105, \n78153 Le Chesnay, France jean-baptiste.tristan@inria.fr xavier.leroy@inria.fr Abstract Software pipelining \nis a loop optimization that overlaps the execu\u00adtion of several iterations of a loop to expose more instruction-level \nparallelism. It can result in .rst-class performance characteristics, but at the cost of signi.cant obfuscation \nof the code, making this optimization dif.cult to test and debug. In this paper, we present a translation \nvalidation algorithm that uses symbolic evaluation to de\u00adtect semantics discrepancies between a loop \nand its pipelined ver\u00adsion. Our algorithm can be implemented simply and ef.ciently, is provably sound, \nand appears to be complete with respect to most modulo scheduling algorithms. A conclusion of this case \nstudy is that it is possible and effective to use symbolic evaluation to reason about loop transformations. \nCategories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation -Correctness \nproofs; D.3.4 [Programming Languages]: Processors -Optimization General Terms Languages, Veri.cation, \nAlgorithms Keywords Software pipelining, translation validation, symbolic evaluation, veri.ed compilers \n1. Introduction There is one last technique in the arsenal of the software optimizer that may be used \nto make most machines run at tip top speed. It can also lead to severe code bloat and may make for almost \nunreadable code, so should be considered the last refuge of the truly desperate. However, its perfor\u00admance \ncharacteristics are in many cases unmatched by any other approach, so we cover it here. It is called \nsoftware pipelining [. . . ] Apple Developer Connection1 Software pipelining is an advanced instruction \nscheduling opti\u00admization that exposes considerable instruction-level parallelism by overlapping the execution \nof several iterations of a loop. It pro\u00adduces smaller code and eliminates more pipeline stalls than merely \n1 http://developer.apple.com/hardwaredrivers/ve/ software pipelining.html Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 10, January 17 23, 2010, Madrid, Spain. \nCopyright c &#38;#169; 2010 ACM 978-1-60558-479-9/10/01. . . $10.00 unrolling the loop then performing \nacyclic scheduling. Software pipelining is implemented in many production compilers and de\u00adscribed in \nseveral compiler textbooks [1, section 10.5] [2, chapter 20] [16, section 17.4]. In the words of the \nrather dramatic quote above, truly desperate programmers occasionally perform soft\u00adware pipelining by \nhand, especially on multimedia and signal pro\u00adcessing kernels. Starting in the 1980 s, many clever algorithms \nwere designed to produce ef.cient software pipelines and implement them either on stock hardware or by \ntaking advantage of special features such as those of the IA64 architecture. In this paper, we are not \ncon\u00adcerned about the performance characteristics of these algorithms, but rather about their semantic \ncorrectness: does the generated, software-pipelined code compute the same results as the original code? \nAs with all advanced compiler optimizations, and perhaps even more so here, mistakes happen in the design \nand implementa\u00adtion of software pipelining algorithms, causing incorrect code to be generated from correct \nsource programs. The extensive code rear\u00adrangement performed by software pipelining makes visual inspec\u00adtion \nof the generated code ineffective; the additional boundary con\u00additions introduced make exhaustive testing \ndif.cult. For instance, after describing a particularly thorny issue, Rau et al. [23] note that The authors \nare indirectly aware of at least one computer manufacturer whose attempts to implement modulo schedul\u00ading, \nwithout having understood this issue, resulted in a com\u00adpiler which generated incorrect code. Translation \nvalidation is a systematic technique to detect (at compile-time) semantic discrepancies introduced by \nbuggy com\u00adpiler passes or desperate programmers who optimize by hand, and to build con.dence in the result \nof a compilation run or manual optimization session. In this approach, the programs before and after \noptimization are fed to a validator (a piece of software dis\u00adtinct from the optimizer), which tries to \nestablish that the two pro\u00adgrams are semantically equivalent; if it fails, compilation is aborted or \ncontinues with the unoptimized code, discarding the incorrect optimization. As invented by Pnueli et \nal. [20], translation valida\u00ad tion proceeds by generation of veri.cation conditions followed by model-checking \nor automated theorem proving [19, 29, 28, 3, 8]. An alternate approach, less general but less costly \nin validation time, relies on combinations of symbolic evaluation and static anal\u00adysis [18, 24, 6, 26, \n27]. The VCGen/theorem-proving approach was applied to software pipelining by Leviathan and Pnueli [14]. \nIt was long believed that symbolic evaluation with static analyses was too weak to validate aggressive \nloop optimizations such as software pipelining. In this paper, we present a novel translation validation \nalgo\u00adrithm for software pipelining, based on symbolic evaluation. This algorithm is surprisingly simple, \nproved to be sound (most of the  Original loop body B: Prolog P: Steady state S: Epilogue E: x := load(float64, \np); y := x * c; store(float64, p, y); p := p + 8; i := i + 1; p1 := p; p2 := p; x1 := x; x2 := x; x1 \n:= load(float64, p2 := p1 + 8; x2 := load(float64, y := x1 * c; i := i + 2; p1); p2); store(float64, \np1, y); p1 := p2 + 8; y := x2 * c; x1 := load(float64, p1); store(float64, p2, y); p2 := p1 + 8; y := \nx1 * c; x2 := load(float64, p2); i := i + 2; store(float64, y := x2 * c; store(float64, x := x2; p := \np2; p1, p2, y); y); Figure 1. An example of software pipelining proof was mechanized using the Coq proof \nassistant), and infor\u00admally argued to be complete with respect to a wide class of soft\u00adware pipelining \noptimizations. The formal veri.cation of a validator consists in mechanically proving that if it returns \ntrue, its two input programs do behave identically at run time. This is a worthwhile endeavor for two \nrea\u00adsons. First, it brings additional con.dence that the results of vali\u00addation are trustworthy. Second, \nit provides an attractive alternative to formally verifying the soundness of the optimization algorithm \nitself: the validator is often smaller and easier to verify than the op\u00adtimization it validates [26]. \nThe validation algorithm presented in this paper grew out of the desire to add a software pipelining \npass to the CompCert high-assurance C compiler [11]. Its formal veri\u00ad .cation in Coq is not entirely \ncomplete at the time of this writing, but appears within reach given the simplicity of the algorithm. \nThe remainder of this paper is organized as follows. Section 2 recalls the effect of software pipelining \non the shape of loops. Sec\u00adtion 3 outlines the basic principle of our validator. Section 4 de.nes the \n.avor of symbolic evaluation that it uses. Section 5 presents the validation algorithm. Its soundness \nis proved in section 6; its com\u00ad pleteness is discussed in section 7. The experience gained on a pro\u00ad \ntotype implementation is discussed in section 8. Section 9 discusses related work, and is followed by \nconclusions and perspectives in section 10. 2. Software pipelining From the bird s eye, software pipelining \nis performed in three steps. Step 1 Select one inner loop for pipelining. Like in most previous work, \nwe restrict ourselves to simple counted loops of the form i := 0; while (i<N) {B} We assume that the \nloop bound N is a loop-invariant variable, that the loop body B is a basic block (no conditional branches, \nno function calls), and that the loop index i is incremented exactly once in B. The use of structured \ncontrol above is a notational convenience: in reality, software pipelining is performed on a .ow graph \nrepresentation of control (CFG), and step 1 actually isolates a sub-graph of the CFG comprising a proper \nloop of the form above, as depicted in .gure 2(a). Step 2 Next, the software pipeliner is called. It \ntakes as in\u00adput the loop body B and the loop index i, and produces a 5-tuple (P, S, E, \u00b5, d) as result. \nP, S and E are sequences of non-branching instructions; \u00b5 and d are positive integers. S is the new \nloop body, also called the steady state of the pipeline.  P is the loop prolog: a sequence of instructions \nthat .lls the pipeline until it reaches the steady state.  E is the loop epilog: a sequence of instructions \nthat drains the pipeline, .nishing the computations that are still in progress at the end of the steady \nstate.  \u00b5 is the minimum number of iterations that must be performed to be able to use the pipelined \nloop.  d is the amount of unrolling that has been performed on the steady state. In other words, one \niteration of S corresponds to d iterations of the original loop B.  The prolog P is assumed to increment \nthe loop index i by \u00b5; the steady state S, by d; and the epilogue E, not at all. The software pipelining \nalgorithms that we have in mind here are combinations of modulo scheduling [22, 7, 15, 23, 4] fol\u00ad lowed \nby modulo variable expansion [10]. However, the presenta\u00ad tion above seems general enough to accommodate \nother scheduling algorithms. Figure 1 illustrates one run of a software pipeliner. The schedule obtained \nby modulo scheduling performs, at each iteration, the ith store and increment of p, the (i+1)th multiplication \nby c, and the (i + 2)th load. To avoid read-after-write hazards, modulo variable expansion was performed, \nunrolling the schedule by a factor of 2, and replacing variables p and x by two variables each, p1/p2 \nand x1/x2, which are de.ned in an alternating manner. These pairs of variables are initialized from p \nand x in the prolog; the epilogue sets p and x back to their .nal values. The unrolling factor d is therefore \n2. Likewise, the minimum number of iterations is \u00b5 =2. Step 3 Finally, the original loop is replaced \nby the following pseudo-code: i := 0; if (N = \u00b5) { M := ((N - \u00b5)/d) \u00d7 d + \u00b5; P while (i<M) {S } E } \nwhile (i<N) {B} In CFG terms, this amounts to replacing the sub-graph shown at the top of .gure 2 by \nthe one shown at the bottom. The effect of the code after pipelining is as follows. If the number of \niterations N is less than \u00b5, a copy of the original loop is executed. Otherwise, we execute the prolog \nP, then iterate the steady state S, then execute the epilogue E, and .nally fall through the copy of \nthe original loop. Since P and S morally correspond to \u00b5 and d iterations of the original loop, respectively, \nS is iterated n times where n is the largest integer such that \u00b5 + nd = N, namely n =(N - \u00b5)/d. In terms \nof the loop index i, this corresponds to the upper limit M shown in the pseudo-code above. The copy of \nthe original loop executes the remaining N - M iterations.  in: (a) Original sub-CFG i := 0 in: g: \n (b) Replacement sub-CFG after software pipelining Figure 2. The effect of software pipelining on a control-.ow \ngraph Steps 1, 2 and 3 can of course be repeated once per inner loop eligible for pipelining. In the \nremainder of this paper, we focus on the transformation of a single loop. 3. Validation a posteriori \nIn a pure translation validation approach, the validator would re\u00adceive as inputs the code before and \nafter software pipelining and would be responsible for establishing the correctness of all three steps \nof pipelining. We found it easier to concentrate translation validation on step 2 only (the construction \nof the components of the pipelined loop) and revert to traditional compiler veri.cation for steps 1 and \n3 (the assembling of these components to form the transformed code). In other words, the validator we \nset out to build takes as arguments the input (i, N, B) and output (P, S, E, \u00b5, d) of the software pipeliner \n(step 2). It is responsible for establishing conditions suf.cient to prove semantic preservation between \nthe original and transformed codes (top and bottom parts of .gure 2). What are these suf.cient conditions \nand how can a validator establish them? To progress towards an answer, think of complete unrollings of \nthe original and pipelined loops. We see that the run\u00adtime behavior of the original loop is equivalent \nto that of BN , that is, N copies of B executed in sequence. Likewise, the generated code behaves either \nlike BN if N<\u00b5, or like P; S.(N); E; B.(N) if N = \u00b5, where def .(N)=(N - \u00b5)/d def .(N)= N - \u00b5 - d \u00d7 .(N) \nThe veri.cation problem therefore reduces to establishing that, for all N, the two basic blocks def def \nXN = BN and YN = P; S.(N); E; B.(N) are semantically equivalent: if both blocks are executed from the \nsame initial state (same memory state, same values for variables), they terminate with the same .nal \nstate (up to the values of tempo\u00adrary variables introduced during scheduling and unused later). For a \ngiven value of N, symbolic evaluation provides a simple, effective way to ensure this semantic equivalence \nbetween XN and YN . In a nutshell, symbolic evaluation is a form of denotational semantics for basic \nblocks where the values of variables x, y, . . . at the end of the block are determined as symbolic expressions \nof their values x0,y0,... at the beginning of the block. For example, def the symbolic evaluation of \nthe block B = y := x+1; x := y\u00d72 is 8 < x . (x0 + 1) \u00d7 2 a(B)= y . x0 +1 : v . v0 for all v/.{x, y} (This \nis a simpli.ed presentation of symbolic evaluation; section 4 explains how to extend it to handle memory \naccesses, potential run\u00adtime errors, and the fresh variables introduced by modulo variable expansion \nduring pipelining.) The fundamental property of symbolic evaluation is that if two blocks have identical \nsymbolic evaluations, whenever they are ex\u00adecuted in the same but arbitrary initial state, they terminate \nin the same .nal state. Moreover, symbolic evaluation is insensitive to reordering of independent instructions, \nmaking it highly suitable to reason over scheduling optimizations [26]. Therefore, for any given N, our \nvalidator can check that a(XN )= a(YN ); this suf\u00ad.ces to guarantee that XN and YN are semantically equivalent. \nIf N were a compile-time constant, this would provide us with a validation algorithm. However, N is in \ngeneral a run-time quan\u00adtity, and statically checking a(XN )= a(YN ) for all N is not decidable. The \nkey discovery of this paper is that this undecidable property .N, a(XN )= a(YN ) is implied by (and, \nin practice, equivalent to) the following two decidable conditions: a(E; Bd)= a(S; E) (1) a(B\u00b5)= a(P; \nE) (2) The programmer s intuition behind condition 1 is the following: in a correct software pipeline, \nif enough iterations remain to be performed, it should always be possible to choose between 1) execute \nthe steady state S one more time, then leave the pipelined loop and execute E; or 2) leave the pipelined \nloop immediately by executing E, then run d iterations of the original loop body B. (See .gure 3.) Condition \n2 is even easier to justify: if the number Figure 3. A view of an execution of the loop, before (top) \nand after (bottom) pipelining. The dashed vertical arrows provide intuitions for equations 1 and 2. \n B\u00b5 Bd Bd B.(N) E E E P S S of executions is exactly \u00b5, the original code performs B\u00b5 and the pipelined \ncode performs P; E. The translation validation algorithm that we de.ne in section 5 is based on checking \nminor variations of conditions 1 and 2, along with some additional conditions on the evolutions of variable \ni (also checked using symbolic evaluation). Before presenting this algorithm, proving that it is sound, \nand arguing that it is complete in practice, we .rst need to de.ne symbolic evaluation more precisely \nand study its algebraic structure. 4. Symbolic evaluation with observables In this section, we formally \nde.ne the form of symbolic evaluation used in our validator. Similar forms of symbolic evaluation are \nat the core of several other translation validators, such as those of Necula [18], Rival [24], and Tristan \nand Leroy [26]. 4.1 The intermediate language We perform symbolic evaluation over the following simple \nlan\u00adguage of basic blocks: Non-branching instructions: ' I ::= r := rvariable-variable move | r := op(op, \nTr) arithmetic operation | r := load(., raddr) memory load | store(., raddr,rval) memory store Basic \nblocks: B ::= I1; ... ; In instruction sequences Here, r ranges over variable names (a.k.a. pseudo-registers \nor tem\u00adporaries); op ranges over arithmetic operators (such as load inte\u00adger constant n or add integer \nimmediate n or .oat multiply ); and . stands for a memory quantity (such as 8-bit signed integer or 64-bit \n.oat ). 4.2 Symbolic states The symbolic evaluation of a block produces a pair (., s) of a mapping . \nfrom resources to terms, capturing the relationship between the initial and .nal values of resources, \nand a set of terms s, recording the computations performed by the block. Resources: . ::= r | Mem Terms: \nt ::= .0 initial value of . | Op(op,Tt) result of an operation | Load(., taddr,tm) result of a load | \nStore(., taddr,tval,tm) effect of a store Resource maps: . ::= . . t .nite map Computations performed: \ns ::= {t1; ... ; tn} .nite set Symbolic states: A ::= (., s) Resource maps symbolically track the values \nof variables and the memory state, represented like a ghost variable Mem. A resource . that is not in \nthe domain of a map . is considered mapped to .0. Memory states are represented by terms tm built out \nof Mem0 and Store(...) constructors. For example, the following block store(., x, y); z := load(., w) \nsymbolically evaluates to the following resource map: Mem . Store(., x0,y0, Mem0) z . Load(., w0, Store(., \nx0,y0, Mem0)) Resource maps describe the .nal state after execution of a basic block, but do not capture \nall intermediate operations performed. This is insuf.cient if some intermediate operations can fail at \nrun\u00adtime (e.g.integer divisions by zero or loads from a null pointer). Consider for example the two blocks \ny := x and y := load(int32,x); y := x They have the same resource map, namely y . x0, yet the right\u00admost \nblock right can cause a run-time error if the pointer x is null, while the leftmost block never fails. \nTherefore, resource maps do not suf.ce to check that two blocks have the same semantics; they must be \ncomplemented by sets s of symbolic terms, recording all the computations that are performed by the blocks, \neven if these computations do not contribute directly to the .nal state. Continu\u00ading the example above, \nthe leftmost block has s = \u00d8 (a move is not considered as a computation, since it cannot fail) while \nthe right\u00admost block has s' = {Load(int32,x0, Mem0)}. Since these two sets differ, symbolic evaluation \nreveals that the two blocks above are not semantically equivalent with respect to run-time errors. 4.3 \nComposition A resource map . can be viewed as a parallel substitution of symbolic terms for resources: \nthe action of a map on a term t is the term .(t) obtained by replacing all occurrences of .0 in t by \nthe term associated with . in .. Therefore, the reverse composition .1; .2 of two maps is, classically, \ndef .1; .2 = {. . .1(.2(.)) | . . Dom(.1) . Dom(.2)} (3) The reverse composition operator extends naturally \nto abstract states: def (.1,s1); (.2,s2)=(.1; .2,s1 .{.1(t) | t . s2}) (4) Composition is associative \nand admits the identity abstract state def e =(\u00d8, \u00d8) as neutral element.  4.4 Performing symbolic evaluation \nThe symbolic evaluation a(I) of a single instruction I is the ab\u00adstract state de.ned by '' a(r := r)=(r \n. r0, \u00d8) a(r := op(op, Tr)) = (r . t, {t}) where t = Op(op, Tr0) a(r := load(., r')) = (r . t, {t}) ' \nwhere t = Load(., r0, Mem0) a(store(., r, r')) = (Mem . tm, {tm}) ' where tm = Store(., r0,r0, Mem0) \n Symbolic evaluation then extends to basic blocks, by composing the evaluations of individual instructions: \ndef a(I1; ... ; In)= a(I1); ... ; a(In) (5) By associativity, it follows that symbolic evaluation distributes \nover concatenation of basic blocks: a(B1; B2)= a(B1); a(B2) (6)  4.5 Comparing symbolic states We use \ntwo notions of equivalence between symbolic states. The .rst one, written ~, is de.ned as strict syntactic \nequality:2 (., s) ~ (. ' ,s ' ) if and only if .., .(.)= . ' (.) and s = s ' (7) The relation ~ is therefore \nan equivalence relation, and is compat\u00adible with composition: A1 ~ A2 =. A1; A ~ A2; A (8) A1 ~ A2 =. \nA; A1 ~ A; A2 (9) We need a coarser equivalence between symbolic states to ac\u00adcount for the differences \nin variable usage between the code before and after software pipelining. Recall that pipelining may perform \nmodulo variable expansion to avoid read-after-write hazards. This introduces fresh temporary variables \nin the pipelined code. These fresh variables show up in symbolic evaluations and prevent strict equivalence \n~ from holding. Consider: x := op(op,x) and x ' := op(op,x) x := x ' These two blocks have different \nsymbolic evaluations (in the sense of the ~ equivalence), yet should be considered as semantically equivalent \nif the temporary x ' is unused later. Let . be a .nite set of variables: the observable variables. (In \nour validation algorithm, we take . to be the set of all variable mentioned in the original code before \npipelining; the fresh tem\u00adporaries introduced by modulo variable expansion are therefore not in ..) Equivalence \nbetween symbolic states up to the observables . is written . and de.ned as (., s) . (. ' ,s ' ) if and \nonly if .. . . .{Mem},.(.)= . ' (.) and s = s ' (10) The relation . is an equivalence relation, coarser \nthan ~, and compatible with composition on the right: A1 ~ A2 =. A1 . A2 (11) A1 . A2 =. A; A1 . A; A2 \n(12) However, it is not, in general, compatible with composition on the left. Consider an abstract state \nA that maps a variable x . . to a term t containing an occurrence of y ./.. Further assume A1 . A2. The \ncomposition A1; A maps x to A1(t); likewise, A2; A maps x to A2(t). Since y/. ., we can have A1(y)= A2(y) \nand therefore A1(t) A2(t) without violating the hypothesis = A1 . A2. Hence, A1; A . A2; A does not hold \nin general. To address this issue, we restrict ourselves to symbolic states A that are contained in the \nset . of observables. We say that a 2 We could relax this de.nition in two ways. One is to compare symbolic \nterms up to a decidable equational theory capturing algebraic identities such as x \u00d7 2= x + x. This enables \nthe validation of e.g. instruction strength ' reduction [18, 24]. Another relaxation is to require s \n. s instead of s = s ', enabling the transformed basic block to perform fewer computations than the original \nblock (e.g. dead code elimination). Software pipelining being a purely lexical transformation that changes \nonly the placement of computations but not their nature nor their number, these two relaxations are not \nessential and we stick with strict syntactic equality for the time being. Semantic interpretation of \narithmetic and memory operations: op : list val . option val load : quantity \u00d7 val \u00d7 mem . option val \nstore : quantity \u00d7 val \u00d7 val \u00d7 mem . option mem Transition semantics for instructions: r := r ' :(R, \nM) . (R[r . R(r ' )],M) op(R(Tr)) = LvJ r := op(op, Tr):(R, M) . (R[r . v],M) load(., R(raddr),M)= LvJ \nr := load(., raddr):(R, M) . (R[r . v],M) store(., R(raddr),R(rval),M)= LM 'J store(., raddr,rval):(R, \nM) . (R, M ' ) Transition semantics for basic blocks: B : S '* . S '' * I : S . S ' e : S . S . S '' \nI.B : S * Figure 4. Operational semantics for basic blocks symbolic term t is contained in ., and write \nt . ., if all variables x appearing in t belong to .. We extend this notion to symbolic states as follows: \n(., s) . . if and only if .. . . .{Mem},.(.) . . and .t . s, t . . (13) It is easy to see that . is compatible \nwith composition on the left if the right symbolic state is contained in .: A1 . A2 . A . . =. A1; A \n. A2; A (14) The containment relation enjoys additional useful properties: A1 . . . A1 . A2 =. A2 . . \n(15) A1 . . . A2 . . =. (A1; A2) . . (16) 4.6 Semantic soundness The fundamental property of symbolic \nevaluation is that if a(B1) . a(B2), the two blocks B1 and B2 have the same run-time behavior, in a \nsense that we now make precise. We equip our language of basic blocks with the straightforward operational \nsemantics shown in .gure 4. The behavior of arithmetic and memory operations is axiomatized as functions \nop, load and store that return option types to model run-time failures: the result \u00d8 (pronounced none \n) denotes failure; the result LxJ (pro\u00adnounced some x ) denotes success. In our intended application, \nthese interpretation functions are those of the RTL intermediate lan\u00adguage of the CompCert compiler [12, \nsection 6.1] and of its mem\u00ad ory model [13]. The semantics for a basic block B is, then, given as the \nrelation * B B : S . S ', sometimes also written S . S ', where S is the initial state and S ' the .nal \nstate. States are pairs (R, M) of a variable state R, mapping variables to values, and a memory state \nM. We say that two execution states S =(R, M) and S ' = (R '' ,M ) are equivalent up to the observables \n., and write S ~ =. S ', if M = M ' and R(r)= R ' (r) for all r . .. THEOREM 1 (Soundness of symbolic \nevaluation). Let B1 and B2 be two basic blocks. Assume that a(B1) . a(B2) and a(B2) .  * S ' ~ .. If \nB1 : S . and S =. T , there exists T ' such that * ~ B2 : T . T ' and S ' =. T ' . We omit the proof, \nwhich is similar to that of Lemma 3 in [26]. 5. The validation algorithm We can now piece together the \nintuitions from section 3 and the de.nitions from section 4 to obtain the following validation algo\u00ad \n rithm: validate (i, N, B) (P, S, E, \u00b5, d) . = a(B\u00b5) . a(P; E) (A) . a(E; Bd) . a(S; E) (B) . a(B) . \n(C) . a(B)(i) = Op(addi(1), i0) (D) . a(P)(i) = a(B\u00b5)(i) (E) . a(S)(i) = a(Bd)(i) (F) . a(E)(i) = i0 \n(G) . a(B)(N) = a(P)(N) = a(S)(N) = a(E)(N) = N0 (H) The inputs of the validator are the components \nof the loops be\u00adfore (i, N, B) and after (P, S, E, \u00b5, d) pipelining, as well as the set . of observables. \nChecks A and B correspond to equations (1) and (2) of section 3, properly reformulated in terms of equiva\u00ad \nlence up to observables. Check C makes sure that the set . of ob\u00adservables includes at least the variables \nappearing in the symbolic evaluation of the original loop body B. Checks D to G ensure that the loop \nindex i is incremented the way we expect it to be: by one in B, by \u00b5 in P, by d in S, and not at all \nin E. (We write a(B)(i) to refer to the symbolic term associated to i by the resource map part of the \nsymbolic state a(B).) Finally, check H makes sure that the loop limit N is invariant in both loops. The \nalgorithm above has low computational complexity. Using a hash-consed representation of symbolic terms, \nthe symbolic eval\u00aduation a(B) of a block B containing n instructions can be com\u00adputed in time O(n log \nn): for each of the n instructions, the algo\u00adrithm performs one update on the resource map, one insertion \nin the constraint set, and one hash-consing operation, all of which can be done in logarithmic time. \n(The resource map and the constraint set have sizes at most n.) Likewise, the equivalence and containment \nrelations ( . and ) can be decided in time O(n log n). Finally, note that this O(n log n) complexity \nholds even if the algorithm is implemented within the Coq speci.cation language, using persis\u00adtent, purely \nfunctional data structures. The running time of the validation algorithm is therefore O(n log n) where \nn = max(\u00b5|B|,d|B|, |P|, |S|, |E|) is pro\u00adportional to the size of the loop after pipelining. Theoretically, \nsoftware pipelining can increase the size of the loop quadratically. In practice, we expect the external \nimplementation of software pipelining to avoid this blowup and produce code whose size is linear in that \nof the original loop. In this case, the validator runs in time O(n log n) where n is the size of the \noriginal loop. 6. Soundness proof In this section, we prove the soudness of the validator: if it returns \ntrue , the software pipelined loop executes exactly like the origi\u00adnal loop. The proof proceeds in two \nsteps: .rst, we show that the ba\u00adsic blocks XN and YN obtained by unrolling the two loops have the same \nsymbolic evaluation and therefore execute identically; then, we prove that the concrete execution of \nthe two loops in the CFG match. 6.1 Soundness with respect to the unrolled loops We .rst show that the \n.nitary conditions checked by the validator imply the in.nitary equivalences between the symbolic evaluations \nof XN and YN sketched in section 3. LEMMA 2. If validate (i, N, B)(P, S, E, \u00b5, d) . returns true, then, \nfor all n, a(B\u00b5+nd ) . a(P; Sn; E) (17) a(P; Sn ; E) . (18) a(B\u00b5+nd )(i)= a(P; Sn)(i)= a(P; Sn ; E)(i) \n(19) Proof: By hypothesis, we know that the properties A to H checked by the validator hold. First note \nthat .n, a(Bn) . (20) as a consequence of check C and property (16). Conclusion (17) is proved by induction \non n. The base case n = 0 reduces to a(B\u00b5) . a(P; E), which is ensured by check A. For the inductive \ncase, assume a(B\u00b5+nd) . a(P; Sn; E). a(B\u00b5+(n+1)d) . a(B\u00b5+nd ); a(Bd) (by distributivity (6)) . a(P; Sn \n; E); a(Bd) (by left compatibility (14) and ind. hyp.) . a(P; Sn); a(E; Bd) (by distributivity (6)) . \na(P; Sn); a(S; E) (by check B, right compatibility (12), and (20)) . a(P; Sn+1 ; E) (by distributivity \n(6)) Conclusion (18) follows from (17), (20), and property (15). Con\u00ad clusion (19) follows from checks \nE, F and G by induction on n. o THEOREM 3. Assume that validate (i, N, B)(P, S, E, \u00b5, d) . * returns \ntrue. Consider an execution BN : S . S ' of the unrolled initial loop, with N = \u00b5. Assume S ~T . Then, \nthere exists a =. ' |{z} |{z} concrete state T such that P; S.(N); E; B.(N) * : T . T ' (21) S =. T ' \n~' (22) Moreover, execution (21) decomposes as follows: T . T0 P SSE' . \u00b7 \u00b7 \u00b7 . T.(N) . T0 BB' . \u00b7 \u00b7 \n\u00b7 . T.(N) = T ' (23) .(N) times .(N) times and the values of the loop index i in the various intermediate \nstates satisfy Tj (i)= S(i)+ \u00b5 + dj (mod 232) (24) Tj ' (i)= S(i)+ \u00b5 + d \u00d7 .(N)+ j (mod 232) (25) Proof: \nAs a corollary of Lemma 2, properties (17) and (18), we obtain a(BN ) . a(P; S.(N); E; B.(N)) a(P; S.(N); \nE; B.(N)) . since N = \u00b5+d \u00d7.(N)+.(N). The existence of T ' then follows from Theorem 1. It is obvious \nthat the concrete execution of the pipelined code can be decomposed as shown in (23). Check D in the \nvalidator  CFG instructions: Ig ::= (I,e) non-branching instr. | (if(r<r ' ),et,ef ) conditional branch \nCFG: g ::= e . Ig .nite map Transition semantics: g(e)=(I,e' ) I :(R, M) . (R ' ,M ' ) (as in .gure 4) \ng :(e, R, M) . (e ' ,R ' ,M ' ) ( et if R(r) <R(r ' ) g(e)=(if(r<r ' ),et,ef ) e ' = ef if R(r) = R(r \n' ) g :(e, R, M) . (e ' , R, M) Figure 5. A simple language of control-.ow graphs guarantees that Sj+1(i)= \nSj (i)+1 (mod232), which implies Sj (i)= S(i)+ j mod 232. Combined with conclusion (19) of Lemma 2, this \nimplies equation (24): Tj (i)= S\u00b5+dj (i)= S(i)+ \u00b5 + dj (mod 232) as well as T0' (i)= S\u00b5+d\u00d7.(N)(i)= S(i)+ \n\u00b5 + d \u00d7 .(N) (mod 232) Check D entails that Tj ' (i)= T0' (i)+ j mod 232. Equation (25) therefore follows. \no  6.2 Soundness with respect to CFG loops The second and last part of the soundness proof extends the \nsemantic preservation results of theorem 3 from the unrolled loops in a basic-block representation to \nthe actual loops in a CFG (control-.ow graph) representation, namely the loops before and after pipelining \ndepicted in .gure 2. This extension is intuitively obvious but surprisingly tedious to formalize in full \ndetails: indeed, this is the only result in this paper that we have not yet mechanized in Coq. Our .nal \nobjective is to perform instruction scheduling on the RTL intermediate language from the CompCert compiler \n[12, sec\u00ad tion 6.1]. To facilitate exposition, we consider instead the simpler language of control-.ow \ngraphs de.ned in .gure 5. The CFG g is represented as partial map from labels e to instructions Ig. An \ninstruction can be either a non-branching instruction I or a condi\u00adtional branch if(r<r ' ), complemented \nby one or two labels de\u00adnoting the successors of the instruction. The operational semantics of this language \nis given by a transition relation g :(e, R, M) . (e ' ,R ' ,M ' ) between states comprising a program \npoint e, a regis\u00adter state R and a memory state M. The following trivial lemma connects the semantics \nof basic blocks (.gure 4) with the semantics of CFGs (.gure 5). We say that a CFG g contains the basic \nblock B between points e and e ' , and write g, B : e r e ', if there exists a path in g from e to e \n' that spells out the instructions I1,...,In of B: g(e)=(I1,e1) . g(e1)=(I2,e2) .\u00b7 \u00b7\u00b7. g(en)=(In,e' ) \n* LEMMA 4. Assume g, B : e r e '. Then, g :(e, S) . (e ' ,S ' ) if * and only if B : S . S ' . In the \nremainder of this section, we consider two control-.ow graphs: g for the original loop before pipelining, \ndepicted in part (a) of .gure 2, and g ' for the loop after pipelining, depicted in part (b) of .gure \n2. (Note that .gure 2 is not a speci.c example: it describes the general shape of CFGs accepted and produced \nby all software pipelining optimizations we consider in this paper.) We now use lemma 4 to relate a CFG \nexecution of the original loop g with the basic-block execution of its unrolling. LEMMA 5. Assume a(B)(i)= \nOp(addi(1),i0) and a(B)(N)= * N0. Consider an execution g :(in,S) . (out,S ' ) from point in * to point \nout in the original CFG, with S(i)=0. Then, BN : S . S ' where N is the value of variable N in state \nS. Proof: The CFG execution can be decomposed as follows: ** (in,S0) . (x,S0) . (in,S1) . \u00b7 \u00b7\u00b7 . (in,Sn) \n. (out,Sn) with S = S0 and S ' = Sn and Sn(i) = Sn(N) and Sj (i) < Sj (N ) for all j<n. By construction \nof B, we have g, B : x r in. * Therefore, by Lemma 4, B : Sj . Sj+1 for j =0,...,n - 1. * It follows \nthat Bn : S . S '. Moreover, at each loop iteration, variable N is unchanged and variable i is incremented \nby 1. It follows that the number n of iterations is equal to the value N of variable N in initial state \nS. This is the expected result. o Symmetrically, we can reconstruct a CFG execution of the pipelined \nloop g ' from the basic-block execution of its unrolling. LEMMA 6. Let Tinit be an initial state where \nvariable i has value 0 and variable N has value N. Let T be Tinit where variable M is set to \u00b5 + d \u00d7 \n.(N). Assume that properties (23), (24) and (25) hold, as well as check H. We can, then, construct an \nexecution * g ' :(in,Tinit) . (out,T ' ) from point in to point out in the CFG g ' after pipelining. \nProof: By construction of g ', we have g ' , P : c r d and g ' , S : f r d and g ' , E : e r g and g \n' , B : h r g. The result is obvious if N<\u00b5. Otherwise, applying Lemma 4 to the basic\u00ad block executions \ngiven by property (23), we obtain the following CFG executions: * g ' :(in,Tinit) . (c,T ) * g ' :(c,T \n) . (d,T0) * g ' :(f,Tj ) . (d,Tj+1) for j =0,...,.(N) - 1 * g ' :(e,T.(N)) . (g,T 0' ) * '' ' g :(h,T \nj ) . (g,T j+1) for j =0,...,.(N) - 1 The variables N and M are invariant between points c and out: N \nbecause of check H, M because it is a fresh variable. Using properties (24) and (25), we see that the \ncondition at point d is true in states T0,...,T.(N)-1 and false in state T.(N). Likewise, the condition \nat point g is true in states T0' ,...,T .' (N)-1 and false in state T.' (N). The expected result follows. \no Piecing everything together, we obtain the desired semantic preservation result. THEOREM 7. Assume \nthat validate (i, N, B)(P, S, E, \u00b5, d) . returns true. Let g and g ' be the CFG before and after pipelining, \n* respectively. Consider an execution g :(in,S) . (out,S ' ) with S(i)=0. Then, for all states T such \nthat S ~T , there exists a =. * '' ~' state T such that g :(in,T ) . (out,T ' ) and S ' =. T . Proof: \nFollows from Theorem 3 and Lemmas 5 and 6. o 7. Discussion of completeness Soundness (rejecting all incorrect \ncode transformations) is the fun\u00addamental property of a validator; but relative completeness (not re\u00adjecting \nvalid code transformations) is important in practice, since a validator that reports many false positives \nis useless. Seman\u00adtic equivalence between two arbitrary pieces of code being unde\u00adcidable, a general-purpose \nvalidation algorithm cannot be sound and complete at the same time. However, a specialized valida\u00adtor \ncan still be sound and complete for a speci.c class of pro\u00adgram transformations in our case, software \npipelining optimiza\u00adtions based on modulo scheduling and modulo variable expansion. In this section, \nwe informally discuss the sources of potential in\u00adcompleteness for our validator.  Mismatch between \nin.nitary and .nitary symbolic evaluation A .rst source of potential incompleteness is the reduction \nof the in.nitary condition .N, a(XN )= a(YN ) to the two .nitary checks A and B. More precisely, the \nsoundness proof of section 6 hinges on the following implication being true: (A) . (B)=. (17) or in \nother words, a(B\u00b5) . a(P; E) . a(E; Bd) . a(S; E) =..n, a(B\u00b5+nd) . a(P; Sn ; E) However, the reverse \nimplication does not hold in general. Assume that (17) holds. Taking n =0, we obtain equation (A). However, \nwe cannot prove (B). Exploiting (17) for n and n +1 iterations, we obtain: a(B\u00b5+nd ) . a(P; Sn); a(E) \na(B\u00b5+nd ); a(Bd) . a(P; Sn); a(S; E) Combining these two equivalences, it follows that a(P; Sn); a(E; \nBd) . a(P; Sn); a(S; E) (26) However, we cannot just simplify on the left and conclude a(E; Bd) . a(S; \nE). To see why such simpli.cations are incorrect, consider the two blocks x := 1; and x := 1; y := 1 \ny := x We have a(x := 1); a(y := 1) . a(x := 1); a(y := x) with . = {x, y}. However, the equivalence \na(y := 1) . a(y := x) does not hold. Coming back to equation (26), the blocks E; Bd and S; E could make \nuse of some property of the state set up by P; Sn (such as the fact that x =1 in the simpli.ed example \nabove). However, (26) holds for any number n of iteration. Therefore, this hypothetical property of the \nstate that invalidates (B) must be a loop invariant. We are not aware of any software pipelining algorithm \nthat takes advantage of loop invariants such as x =1 in the simpli.ed example above. In conclusion, the \nreverse implication (17) =. (A) . (B), which is necessary for completeness, is not true in general, but \nwe strongly believe it holds in practice as long as the software pipelining algorithm used is purely \nsyntactic and does not exploit loop invariants. Mismatch between symbolic evaluation and concrete executions \nA second, more serious source of incompleteness is the following: equivalence between the symbolic evaluations \nof two basic blocks is a suf.cient, but not necessary condition for those two blocks to execute identically. \nHere are some counter-examples. 1. The two blocks could execute identically because of some prop\u00aderty \nof the initial state, as in y := 1 vs. y := x assuming x =1 initially. 2. Symbolic evaluation fails to \naccount for some algebraic proper\u00adties of arithmetic operators: i := i + 1; vs. i := i +2 i := i +1 3. \nSymbolic evaluation fails to account for memory separation properties within the same memory block: store(int32,p \n+4,y); vs. x := load(int32,p); x := load(int32,p) store(int32,p +4,y) 4. Symbolic evaluation fails to \naccount for memory separation properties within different memory blocks: store(int32, a, y); vs. x := \nload(int32,b); x := load(int32,b) store(int32, a, y) assuming that a and b point to different arrays. \nMismatches caused by speci.c properties of the initial state, as in example 1 above, are not a concern \nfor us: as previously argued, it is unlikely that the pipelining algorithm would take advantage of these \nproperties. The other three examples are more problem\u00adatic: software pipeliners do perform some algebraic \nsimpli.cations (mostly, on additions occurring within address computations and loop index updates) and \ndo take advantage of nonaliasing proper\u00adties to hoist loads above stores. Symbolic evaluation can be \nenhanced in two ways to address these issues. The .rst way, pioneered by Necula [18], consists in comparing \nsymbolic terms and states modulo an equational theory capturing the algebraic identities and good variable \nproperties of interest. For example, the following equational theory addresses the issues illustrated \nin examples 2 and 3: Op(addi(n), Op(addi(m),t)) = Op(addi(n + m),t) Load(. ' ,t1, Store(., t2,tv,tm)) \n= Load(. ' ,t2,tm) if |t2 - t1|= sizeof(.) The separation condition between the symbolic addresses t1 \nand t2 can be statically approximated using a decision procedure such as Pugh s Omega test [21]. In practice, \na coarse approximation, restricted to the case where t1 and t2 differ by a compile-time constant, should \nwork well for software pipelining. A different extension of symbolic evaluation is needed to ac\u00adcount \nfor nonaliasing properties between separate arrays or mem\u00adory blocks, as in example 4. Assume that these \nnonaliasing prop\u00aderties are represented by region annotations on load and store in\u00adstructions. We can, \nthen, replace the single resource Mem that sym\u00adbolically represent the memory state by a family of resources \nMemx indexed by region identi.ers x. A load or store in region x is, then, symbolically evaluated in \nterms of the resource Memx. Continuing example 4 above and assuming that the store takes place in region \na and the load in region b, both blocks symbolically evaluate to Mema . Store(int32,a0,y0, Mema 0 ) x \n. Load(int32,b0, Memb 0) 8. Implementation and preliminary experiments The .rst author implemented the \nvalidation algorithm presented here as well as a reference software pipeliner. Both components are implemented \nin OCaml and were connected with the CompCert C compiler for testing. The software pipeliner accounts \nfor approximately 2000 lines of OCaml code. It uses a backtracking iterative modulo scheduler [2, chapter \n20] to produce a steady state. Modulo variable expan\u00ad sion is then performed following Lam s approach \n[10]. The mod\u00ad ulo scheduler uses a heuristic function to decide the order in which Figure 6. The symbolic \nvalues assigned to register 50 (left) and to its pipelined counterpart, register 114 (right). The pipelined \nvalue lacks one iteration due to a corner case error in the implementation of the modulo variable expansion. \nSymbolic evaluation was performed by omitting the prolog and epilog moves, thus showing the difference \nin register use.  to consider the instructions for scheduling. We tested our software pipeliner with \ntwo such heuristics. The .rst one randomly picks the nodes. We use it principally to stress the pipeliner \nand the valida\u00adtor. The second one picks the node using classical dependencies constraints. We also made \na few schedules by hand. We experimented our pipeliner on the CompCert benchmark suite, which contains, \namong others, some numerical kernels such as a fast Fourier transform. On these examples, the pipeliner \nperforms signi.cant code rearrangements, although the observed speedups are modest. There are several \nreasons for this: we do not exploit the results of an alias analysis; our heuristics are not state of \nthe art; and we ran our programs on an out-of-order PowerPC G5 processor, while software pipelining is \nmost effective on in-order processors. The implementation of the validator follows very closely the al\u00adgorithm \npresented in this paper. It accounts for approximately 300 lines of OCaml code. The validator is instrumented \nto produce a lot of debug information, including in the form of drawings of sym\u00adbolic states. The validator \nfound many bugs during the develop\u00adment of the software pipeliner, especially in the implementation of \nmodulo variable expansion. The debugging information produced by the validator was an invaluable tool \nto understand and correct these mistakes. Figure 6 shows the diagrams produced by our val\u00ad idator for \none of these bugs. We would have been unable to get the pipeliner right without the help of the validator. \nOn our experiments, the validator reported no false alarms. The running time of the validator is negligible \ncompared with that of the software pipeliner itself (less than 5%). However, our software pipeliner is \nnot state-of-the-art concerning the determination of the minimal initiation interval, and therefore is \nrelatively costly in terms of compilation time. Concerning formal veri.cation, the underlying theory \nof sym\u00adbolic evaluation (section 4) and the soundness proof with respect to unrolled loops (section 6.1) \nwere mechanized using the Coq proof assistant. The Coq proof is not small (approximately 3500 lines) \nbut is relatively straightforward: the mechanization raised no un\u00adexpected dif.culties. The only part \nof the soundness proof that we have not mechanized yet is the equivalence between unrolled loops and \ntheir CFG representations (section 6.2): for such an intuitively obvious result, a mechanized proof is \ninfuriatingly dif.cult. Simply proving that the situation depicted in .gure 2(a) holds (we have a proper, \ncounted loop whose body is the basic block B) takes a great deal of bureaucratic formalization. We suspect \nthat the CFG repre\u00adsentation is not appropriate for this kind of proofs; see section 10 for a discussion. \n9. Related work This work is not the .rst attempt to validate software pipelining a posteriori. Leviathan \nand Pnueli [14] present a validator for a soft\u00ad ware pipeliner for the IA-64 architecture. The pipeliner \nmakes use of a rotating register .le and predicate registers, but from a valida\u00adtion point of view it \nis almost the same problem as the one studied in this paper. Using symbolic evaluation, the validator \ngenerates a set of veri.cation conditions that are discharged by a theorem prover. We chose to go further \nwith symbolic evaluation: instead of using it to generate veri.cation conditions, we use it to directly \nestablish semantic preservation. We believe that the resulting val\u00adidator is simpler and algorithmically \nmore ef.cient. However, ex\u00adploiting nonaliasing information during validation could possibly be easier \nin Leviathan and Pnueli s approach than in our approach. Another attempt at validating software pipelining \nis the one of Kundu et al. [9]. It proceeds by parametrized translation valida\u00ad tion. The software pipelining \nalgorithm they consider is based on code motion, a rarely-used approach very different from the mod\u00adulo \nscheduling algorithms we consider. This change of algorithms makes the validation problem rather different. \nIn the case of Kundu et al. , the software pipeliner proceeds by repeated rewriting of the original loop, \nwith each rewriting step being validated. In contrast, modulo scheduling builds a pipelined loop in one \nshot (usually us\u00ading a backtracking algorithm). It could possibly be viewed as a se\u00adquence of rewrites \nbut this would be less ef.cient than our solution. Another approach to establishing trust in software \npipelining is run-time validation, as proposed by Goldberg et al. [5]. Their approach takes advantages \nof the IA-64 architecture to instrument the pipelined loop with run-time check that verify properties \nof the pipelined loop and recover from unexpected problems. This tech\u00adnique was used to verify that aliasing \nis not violated by instruction switching, but they did not verify full semantic preservation. 10. Conclusions \nSoftware pipelining is one of the pinnacles of compiler optimiza\u00adtions; yet, its semantic correctness \nboils down to two simple se\u00admantic equivalence properties between basic blocks. Building on this novel \ninsight, we presented a validation algorithm that is simple enough to be used as a debugging tool when \nimplementing software pipelining, ef.cient enough to be integrated in production compil\u00aders and executed \nat every compilation run, and mathematically ele\u00adgant enough to lend itself to formal veri.cation. Despite \nits respectable age, symbolic evaluation the key tech\u00adnique behind our validator .nds here an unexpected \napplication to the veri.cation of a loop transformation. Several known exten\u00adsions of symbolic evaluation \ncan be integrated to enhance the preci\u00adsion of the validator. One, discussed in section 7, is the addition \nof an equational theory and of region annotations to take nonaliasing information into account. Another \nextension would be to perform symbolic evaluation over extended basic blocks (acyclic regions of the \nCFG) instead of basic blocks, like Rival [24] and Tristan and Leroy [26] do. This extension could make \nit possible to validate pipelined loops that contain predicated instructions or even condi\u00adtional branches. \n From a formal veri.cation standpoint, symbolic evaluation lends itself well to mechanization and gives \na pleasant denotational .avor to proofs of semantic equivalence. This stands in sharp contrast with the \nlast step of our proof (section 6.2), where the low-level operational nature of CFG semantics comes back \nto haunt us. We are led to suspect that control-.ow graphs are a poor representation to reason over loop \ntransformations, and to wish for an alternate representation that would abstract the syntactic details \nof loops just as symbolic evaluation abstracts the syntactic details of (extended) basic blocks. Two \npromising candidates are the PEG representation of Tate et al. [25] and the representation as sets of \nmutually recursive HOL functions of Myreen and Gordon [17]. It would be interesting to reformulate software \npipelining validation in terms of these representations. Acknowledgments This work was supported by Agence \nNationale de la Recherche, project U3CAT, program ANR-09-ARPEGE. We thank Franc\u00b8ois Pottier, Alexandre \nPilkiewicz, and the anonymous reviewers for their careful reading and suggestions for improvements. References \n[1] A. V. Aho, M. Lam, R. Sethi, and J. D. Ullman. Compilers: principles, techniques, and tools. Addison-Wesley, \nsecond edition, 2006. [2] A. W. Appel. Modern Compiler Implementation in ML. Cambridge University Press, \n1998. [3] C. W. Barret, Y. Fang, B. Goldberg, Y. Hu, A. Pnueli, and L. Zuck. TVOC: A translation validator \nfor optimizing compilers. In Computer Aided Veri.cation, 17th Int. Conf., CAV 2005, volume 3576 of LNCS, \npages 291 295. Springer, 2005. [4] J. M. Codina, J. Llosa, and A. Gonz\u00b4 alez. A comparative study of \nmodulo scheduling techniques. In Proc. of the 16th international conference on Supercomputing, pages \n97 106. ACM, 2002. [5] B. Goldberg, E. Chapman, C. Huneycutt, and K. Palem. Software bubbles: Using predication \nto compensate for aliasing in software pipelines. In 2002 International Conference on Parallel Architectures \nand Compilation Techniques (PACT 2002), pages 211 221. IEEE Computer Society Press, 2002. [6] Y. Huang, \nB. R. Childers, and M. L. Soffa. Catching and identifying bugs in register allocation. In Static Analysis, \n13th Int. Symp., SAS 2006, volume 4134 of LNCS, pages 281 300. Springer, 2006. [7] R. A. Huff. Lifetime-sensitive \nmodulo scheduling. In Proc. of the ACM SIGPLAN 93 Conf. on Programming Language Design and Implementation, \npages 258 267. ACM, 1993. [8] A. Kanade, A. Sanyal, and U. Khedker. A PVS based framework for validating \ncompiler optimizations. In SEFM 06: Proceedings of the Fourth IEEE International Conference on Software \nEngineering and Formal Methods, pages 108 117. IEEE Computer Society, 2006. [9] S. Kundu, Z. Tatlock, \nand S. Lerner. Proving optimizations correct using parameterized program equivalence. In Proceedings \nof the 2009 Conference on Programming Language Design and Implementation (PLDI 2009), pages 327 337. \nACM Press, 2009. [10] M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. \nIn Proc. of the ACM SIGPLAN 88 Conf. on Programming Language Design and Implementation, pages 318 328. \nACM, 1988. [11] X. Leroy. Formal veri.cation of a realistic compiler. Commun. ACM, 52(7):107 115, 2009. \n[12] X. Leroy. A formally veri.ed compiler back-end. Journal of Automated Reasoning, 2009. To appear. \n[13] X. Leroy and S. Blazy. Formal veri.cation of a C-like memory model and its uses for verifying program \ntransformations. Journal of Automated Reasoning, 41(1):1 31, 2008. [14] R. Leviathan and A. Pnueli. Validating \nsoftware pipelining optimizations. In Int. Conf. on Compilers, Architecture, and Synthesis for Embedded \nSystems (CASES 2002), pages 280 287. ACM Press, 2006. [15] J. Llosa, A. Gonz\u00b4e, and M. Valero. Swing \nmodulo alez, E. Ayguad\u00b4scheduling: A lifetime-sensitive approach. In IFIP WG10.3 Working Conference on \nParallel Architectures and Compilation Techniques, pages 80 86, 1996. [16] S. S. Muchnick. Advanced compiler \ndesign and implementation. Morgan Kaufmann, 1997. [17] M. O. Myreen and M. J. C. Gordon. Transforming \nprograms into recursive functions. In Brazilian Symposium on Formal Methods (SBMF 2008), volume 240 of \nENTCS, pages 185 200. Elsevier, 2009. [18] G. C. Necula. Translation validation for an optimizing compiler. \nIn Programming Language Design and Implementation 2000, pages 83 95. ACM Press, 2000. [19] A. Pnueli, \nO. Shtrichman, and M. Siegel. The code validation tool (CVT) automatic veri.cation of a compilation \nprocess. International Journal on Software Tools for Technology Transfer, 2(2):192 201, 1998. [20] A. \nPnueli, M. Siegel, and E. Singerman. Translation validation. In Tools and Algorithms for Construction \nand Analysis of Systems, TACAS 98, volume 1384 of LNCS, pages 151 166. Springer, 1998. [21] W. Pugh. \nThe Omega test: a fast and practical integer programming algorithm for dependence analysis. In Supercomputing \n91: Proceedings of the 1991 ACM/IEEE conference on Supercomputing, pages 4 13. ACM Press, 1991. [22] \nB. R. Rau. Iterative modulo scheduling. International Journal of Parallel Processing, 24(1):1 102, 1996. \n[23] B. R. Rau, M. S. Schlansker, and P. P. Timmalai. Code generation schema for modulo scheduled loops. \nTechnical Report HPL-92-47, Hewlett-Packard, 1992. [24] X. Rival. Symbolic transfer function-based approaches \nto certi.ed compilation. In 31st symposium Principles of Programming Languages, pages 1 13. ACM Press, \n2004. [25] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner. Equality saturation: a new approach to optimization. \nIn 36th symposium Principles of Programming Languages, pages 264 276. ACM Press, 2009. [26] J.-B. Tristan \nand X. Leroy. Formal veri.cation of translation validators: A case study on instruction scheduling optimizations. \nIn 35th symposium Principles of Programming Languages, pages 17 27. ACM Press, 2008. [27] J.-B. Tristan \nand X. Leroy. Veri.ed validation of Lazy Code Motion. In Proceedings of the 2009 Conference on Programming \nLanguage Design and Implementation (PLDI 2009), pages 316 326. ACM Press, 2009. [28] L. Zuck, A. Pnueli, \nY. Fang, and B. Goldberg. VOC: A methodology for translation validation of optimizing compilers. Journal \nof Universal Computer Science, 9(3):223 247, 2003. [29] L. Zuck, A. Pnueli, and R. Leviathan. Validation \nof optimizing compilers. Technical Report MCS01-12, Weizmann institute of Science, 2001.   \n\t\t\t", "proc_id": "1706299", "abstract": "<p>Software pipelining is a loop optimization that overlaps the execution of several iterations of a loop to expose more instruction-level parallelism. It can result in first-class performance characteristics, but at the cost of significant obfuscation of the code, making this optimization difficult to test and debug. In this paper, we present a translation validation algorithm that uses symbolic evaluation to detect semantics discrepancies between a loop and its pipelined version. Our algorithm can be implemented simply and efficiently, is provably sound, and appears to be complete with respect to most modulo scheduling algorithms. A conclusion of this case study is that it is possible and effective to use symbolic evaluation to reason about loop transformations.</p>", "authors": [{"name": "Jean-Baptiste Tristan", "author_profile_id": "81342514111", "affiliation": "INRIA Paris-Rocquencourt, Le Chesnay, France", "person_id": "P1911047", "email_address": "", "orcid_id": ""}, {"name": "Xavier Leroy", "author_profile_id": "81100078576", "affiliation": "INRIA Paris-Rocquencourt, Le Chesnay, France", "person_id": "P1911048", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1706299.1706311", "year": "2010", "article_id": "1706311", "conference": "POPL", "title": "A simple, verified validator for software pipelining", "url": "http://dl.acm.org/citation.cfm?id=1706311"}