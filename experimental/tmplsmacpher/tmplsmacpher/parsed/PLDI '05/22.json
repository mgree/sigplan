{"article_publication_date": "06-12-2005", "fulltext": "\n Threads Cannot Be Implemented As a Library Hans-J. Boehm HP Laboratories Palo Alto, CA Hans.Boehm@hp.com \nAbstract In many environments, multi-threaded code is written in a language that was originally designed \nwithout thread support (e.g. C), to which a library of threading primitives was subsequently added. There \nappears to be a general understanding that this is not the right approach. We provide speci.c arguments \nthat a pure library ap\u00adproach, in which the compiler is designed independently of thread\u00ading issues, \ncannot guarantee correctness of the resulting code. We .rst review why the approach almost works, and \nthen ex\u00adamine some of the surprising behavior it may entail. We further illustrate that there are very \nsimple cases in which a pure library\u00adbased approach seems incapable of expressing an ef.cient parallel \nalgorithm. Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably \nwell speci.ed, and does not attempt to ensure type-safety, which would entail even stronger constraints. \nThe issues we raise are not speci.c to that context. Categories and Subject Descriptors D.3.3 [Programming \nLan\u00adguages]: Concurrent programming structures; D.3.4 [Program\u00adming Languages]: Optimization General \nTerms Languages, Performance Keywords Threads, data race, optimization, Pthreads, register promotion \n1. Introduction Multi-threaded programs are rapidly becoming pervasive, driven primarily by two considerations: \n Many programs need to carry on several different logically con\u00adcurrent interactions. For example, they \nmay need to concur\u00adrently serve several different client programs, or provide sev\u00aderal related services \nwhich can progress asynchronously, usu\u00adally in separate windows, to a single user. Threads provide a \nclean structuring mechanism for such programs.  Multiprocessors are .nally becoming mainstream. Many \nof the most popular processors in desktop computers support multiple hardware contexts in a single processor, \nmaking them logically multiprocessors. In addition, essentially every microprocessor  Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 05, June 12 \n15, 2005, Chicago, Illinois, USA. Copyright 2005 ACM 1-59593-056-6/05/0006...$5.00. manufacturer who \nis not already shipping chip-level multipro\u00ad cessors has announced the intention to do so within a year. \nThis even applies to some intended for embedded applications, where multiprocessors can provide more \ncompute throughput with less total power consumption (cf. [4]). In many cases, there is no way to effectively \nutilize the per\u00ad formance of the additional processor cores or hardware threads without writing explicitly \nmulti-threaded programs.1 Most multi-threaded programs communicate through memory shared between the \nthreads. Many such programs are written in a language such as Java, C#, or Ada, which provides threads \nas part of the language speci.cation. Recent experience has shown that it is quite tricky to correctly \nspecify such a language, particularly when type-safety and security considerations are involved. How\u00adever, \nthese issues are becoming reasonably well-understood.[23] Here we focus on another large set of applications \nwhich are written in languages such as C or C++ that do not provide for mul\u00adtiple threads as part of \nthe language speci.cation. Instead thread support is provided by add-on libraries. In order to focus \nthese dis\u00adcussions, we concentrate on what is arguably the best speci.ed rep\u00adresentative of these, Posix[16] \nthreads.2 We argue that these environments are as under-speci.ed as the original Java memory model[27], \nthough for somewhat differ\u00adent reasons. In particular, essentially any application must rely on implementation-de.ned \nbehavior for its correctness. Implementa\u00adtions appear to have converged on characteristics that make \nit pos\u00adsible to write correct multi-threaded applications, though largely, we believe, based on painful \nexperiences rather than strict adher\u00adence to standards. We believe that there is little general understand\u00ading \non what those characteristics are, nor the exact constraints that language implementations need to obey. \nSeveral recent papers sug\u00adgest that these issues are also not fully appreciated by the research community. \nHere we point out the important issues, and argue that they lie almost exclusively with the compiler \nand the language speci.cation itself, not with the thread library or its speci.cation. Hence they 1 Other \nalternatives include running multiple independent processes, auto\u00admatically parallelizing existing sequential \ncode, or using a hardware context to prefetch data for the main sequential thread[9]. The .rst two are \nclearly signi.cant in certain important domains, e.g. to many network server or nu\u00admerical applications, \nbut are generally less applicable to, for example, typ\u00adical desktop or hand-held applications, even when \nthose require substantial CPU resources, such as for image processing. The last applies to a wide va\u00adriety \nof domains, but has much more limited bene.t than explicitly parallel client code. 2 We explicitly do \nnot address the less frequently used environments in which multiple concurrent threads communicate communicate \nprimarily through message passing, such as C programs communicating only through the MPI[34] library, \nor Erlang[11] or Concurrent ML[28] programs. cannot be addressed purely within the thread library or \nits speci.\u00adcation. Most of the pieces we present have been at least super.cially mentioned elsewhere, \nthough we believe that particularly the im\u00adportant relationship between register promotion and thread-safety \nis not widely understood.3 Our contribution is to present these pieces coherently as an argument that \nconcurrency must be ad\u00addressed at the language level.4 Together with several others, we are joining in \nan effort[2] to revise the C++ language standard to better accommodate threads. Our goal here is to describe \nprecisely why that is necessary, and why similar efforts are needed for most other environments relying \non library-based threads. 2. Overview We review the approach to threads exempli.ed by the Pthreads approach, \nexplaining how and why it appears to work. We then discuss three distinct de.ciencies in this approach, \neach of which can lead, and at least two of which have lead, to subtly incorrect code. Each of these \nfailures is likely to be very intermittent, and hard to expose during testing. Thus it is particularly \nimportant to resolve these issues, since they are likely to lead to unreliable production code, and make \nit impossible to guarantee correctness of multi-threaded programs. Library-based approaches to concurrency \nnormally require a very disciplined style of synchronization by multi-threaded pro\u00adgrams. Although we \nagree that this disciplined style is appropriate for perhaps 98% of uses, we argue that it eliminates \nsome low-level programming techniques which, in some cases, may be essential for obtaining any performance \nbene.t from multiple processors. In other cases, such low-level programming techniques can be used to \nimprove the performance of higher-level library primitives, and thus provide pervasive performance improvements \nfor a large col\u00adlection of applications. Thus, although these usage rules are highly desirable guidelines, \nwe argue that they are inappropriate as abso\u00adlute requirements in a world in which we need to rely on \nmultipro\u00adcessors for performance. 3. The Pthreads Approach to Concurrency Any language supporting concurrency \nmust specify the semantics of multi-threaded execution. Most fundamentally, it must specify a memory \nmodel , i.e. which assignments to a variable by one thread can be seen by a concurrently executing thread. \nTraditionally[19] concurrent execution was viewed as simply an interleaving of the steps from the threads \nparticipating in the computation. Thus if we started in an initial state in which all variables are zero, \nand one thread executes: x =1; r1 =y; while another executes y =1; r2 =x; either the assignment to x \nor the assignment to y must be executed .rst, and either r1 or r2 must have a value of 1 when execution \ncompletes.5 3 The only discussion we could .nd is in an HP Technical Brief[15], which addresses this \nissue only very partially and from a different perspective. 4 Peter Buhr [8] makes a similar sounding \nargument, but he focusses on code written under very different assumptions from the Pthreads model, such \nas the implementation of the threads library itself. We concentrate on problems that cannot be isolated \nto the threads library. 5 Many more such examples were discussed as part of the work on the Java memory \nmodel, which is captured in [26] This is probably the most intuitive memory model, though not necessarily \nthe one that is easiest to use in practice.6 It is referred to as sequential consistency. In practice, \nit appears unlikely that such a restrictive memory model can be implemented with reasonable performance \non con\u00adventional architectures. Essentially all realistic programming lan\u00adguage implementations supporting \ntrue concurrency allow both r1 and r2 to contain zero at the end of the above example. There are two \nreasons for this: Compilers may reorder memory operations, if that doesn t vio\u00adlate intra-thread dependencies. \nEach pair of actions in the above threads could be reordered, since doing so does not change the meaning \nof each thread, taken in isolation. And performing loads early may result in a better instruction schedule, \npoten\u00adtially resulting in performance improvements. (Cf. [3].)  The hardware may reorder memory operations \nbased on similar constraints. Nearly all common hardware, e.g. X86 processors, may reorder a store followed \nby a load[21]. Generally a store results immediately in a write-buffer entry, which is later writ\u00adten \nto a coherent cache, which would then be visible to other threads.  Thus it is customary to specify \na much weaker memory model, which allows results such as r1 = r2 = 0 in the above example. Both the original \nJava memory model and the new one described in [23] do so, as does the Pthreads standard[16]. The Pthreads \nstandard intentionally avoids specifying a formal semantics for concurrency. We expect that this was \ndue in part to the fact that this standardization effort did not control the underly\u00ading language standard. \nBut the rationale for the standard also states in part: Formal de.nitions of the memory model were rejected \nas unreadable by the vast majority of programmers. In ad\u00addition, most of the formal work in the literature \nhas concen\u00adtrated on the memory as provided by the hardware as op\u00adposed to the application programmer \nthrough the compiler and runtime system. It was believed that a simple statement intuitive to most programmers \nwould be most effective . Instead, it informally decrees: Applications shall ensure that access to any \nmemory location by more than one thread of control (threads or pro\u00adcesses) is restricted such that no \nthread of control can read or modify a memory location while another thread of con\u00adtrol may be modifying \nit. Such access is restricted using functions that synchronize thread execution and also syn\u00adchronize \nmemory with respect to other threads. The fol\u00adlowing functions synchronize memory with respect to other \nthreads: ..., pthread mutex lock(), ..., ..., pthread mutex unlock(), ... [Many other synchronization \nfunctions listed] 7 6 As was pointed out during the discussions in [26], it has the clear disad\u00advantage \nthat shared variables used for synchronization are not identi.ed by the source. 7 There is an attempt \nat further clari.cation in the rationale. Aside from some insightful discussion of hardware memory reordering, \nprobably the most relevant statements are: All these functions would have to be rec\u00adognized by advanced \ncompilation systems so that memory operations and calls to these functions are not reordered by optimization. \nAll these func\u00adtions would potentially have to have memory synchronization instructions added, depending \non the particular machine. There is no discussion of in\u00ad Unlike in Java, it is acceptable to leave the \nsemantics of pro\u00adgrams with data races, i.e. concurrent reads and writes or concur\u00adrent writes, formally \nunde.ned, and the standard chooses to do so. (In Java, this is unacceptable because the language is designed \nto limit the damage that can be caused by untrusted, and possibly ma\u00adlicious code. Thus the semantics \nof such code need to be speci.ed. We make no guarantees about malicious C/C++ code.) In practice, C and \nC++ implementations that support Pthreads generally proceed as follows: 1. Functions such as pthread \nmutex lock() that are guaranteed by the standard to synchronize memory include hardware in\u00adstructions \n( memory barriers ) that prevent hardware reorder\u00ading of memory operations around the call.8 2. To prevent \nthe compiler from moving memory operations around calls to functions such as pthread mutex lock(), they \nare essentially treated as calls to opaque functions, about which the compiler has no information. The \ncompiler effec\u00adtively assumes that pthread mutex lock() may read or write any global variable. Thus a \nmemory reference cannot simply be moved across the call. This approach also ensures that transitive calls, \ne.g. a call to a function f() which then calls pthread mutex lock(), are handled in the same way more \nor less appropriately, i.e. memory operations are not moved across the call to f() either, whether or \nnot the entire user program is being analyzed at once. This approach clearly works most of the time. \nUnfortunately, we will see that it is too imprecise to allow the programmer to reason convincingly about \nprogram correctness, or to provide clear guid\u00adance to the compiler implementor. As a result, apparently \ncorrect programs may fail intermittently, or start to fail when a new com\u00adpiler or hardware version is \nused. The resulting failures are trig\u00adgered by speci.c thread schedules, and are thus relatively hard \nto detect during testing. These problems all arise from the fact that the underlying lan\u00adguage speci.cation \ndoes not mention threads, and hence does not suf.ciently constrain the compiler. In particular, the current \nspeci\u00ad.cation does not suf.ciently de.ne when a data race exists in the original program, or when the \ncompiler may introduce one. A secondary problem with this approach is that, in some cases, it excludes \nthe best performing algorithmic solutions. As a result, many large systems, either intentionally, or \nunintentionally, violate the above rules. The resulting programs are then even more suscep\u00adtible to the \nabove problems. 4. Correctness issues We list three different issues that we have encountered with the \ncurrent Pthreads approach. We are not aware of cases in which the .rst problem led to an actual failure. \nBut anecdotes abound about failures caused by the second problem, and we have personally encountered \nan intermittent failure caused by the third. 4.1 Concurrent modi.cation The Pthreads speci.cation prohibits \nraces, i.e. accesses to a shared variable while another thread is modifying it. As pointed out by the \nwork on the Java memory model[23], the problem here is direct calls to these functions. The statement \nabout optimization reordering appears to address only the rare case of whole program optimization, and \nas a result does not appear to re.ect actual implementation strategies. 8 It is probably acceptable to \nguarantee only that, e.g. for pthread mutex lock() and pthread mutex unlock(), memory operations not \nmove out of a critical section. Full memory barriers may not be needed. See below.       that whether \nor not a race exists depends on the semantics of the programming language, which in turn requires that \nwe have a properly de.ned memory model. Thus this de.nition is circular. As a concrete example (essentially \n.gure 6 from [17]), consider two threads, each executing one of the following two statements, again in \nan initial state in which x and y are zero: if (x == 1) ++y; if (y == 1) ++x; Does this program contain \na race? Is x == 1 and y ==1an acceptable outcome? Under the sequentially consistent interpretation, there \nis no race, since no variable can become nonzero. Hence we can argue that this is a valid Pthreads program, \nand x == 0 and y == 0 is the only valid outcome. (This is in fact the approach taken in [23].) On the \nother hand, if our compiler is allowed to transform se\u00adquential code not containing calls to pthread \noperations in any way that preserves sequential correctness, the above could be trans\u00adformed to9 ++y; \nif (x != 1) --y; ++x; if (y != 1) --x; This would argue both that there is a race, hence the semantics \nof this program is formally unde.ned, and x == 1 and y ==1isa potential outcome. Indeed, under the implementation \nstrategy we outlined above, in which the compiler is unaware of threads, it is allowed to transform code \nsubject only to sequential correctness constraints and hence could generate the code containing a race. \nThus we believe that the circularity in the de.nition is a real issue, though not one likely to generate \nfrequent practical problems. Resolving it essential requires a programming-language-de.ned and compiler-respected \nmemory model, simply to ensure that the user and compiler can agree on when there is a data race. The \nremaining two issues are much more serious in practice. 4.2 Rewriting of Adjacent Data In our preceding \nexample, a compiler could potentially introduce a race by speculatively executing a store operation early. \nThere is in fact no prohibition against storing into a variable that is never mentioned in the source. \nAnd indeed, for C or C++ (but not Java), that is often unavoidable. Consider a C struct containing bit \n.elds on a little-endian 32\u00adbit machine: struct { int a:17; int b:15 } x; Since very few machines support \na 17-bit-wide store instruc\u00adtion, the assignment x.a= 42 is likely to be implemented as something like \n9 This is probably far-fetched in this example. But it is hard to argue that similar speculative execution \nis never pro.table, especially in the presence of (possibly misleading) pro.le information, and potentially \ncomplex in\u00adstruction scheduling constraints. As was pointed out in earlier discussion ([26] and section \n9.1.1 in [23]), similar issues do arise in practice when moving stores across potentially nonterminating \nloops. And the real ques\u00adtion here is whether the transformation is correct, not whether it is prof\u00aditable. \n{ tmp = x; // Read both fields into // 32-bit variable. tmp &#38;= ~0x1ffff; // Mask off old a. tmp |= \n42; x = tmp; // Overwrite all of x. } Note that this effectively stores into both x.a and x.b normally \nstoring the original value of x.b back into x.b. For sequential code this is completely uncontroversial. \nBut if there is a concurrent update to x.b that occurs between the tmp = x and x = tmp assignments in \nthe above code, we have introduced a race, and the concurrent update to x.b may be lost, in spite of \nthe fact that the two threads operate on completely distinct .elds. On most architectures this is both \nunavoidable and well\u00adrecognized for bit-.elds. For example, the problem is already dis\u00adcussed in the \ncontext of IBM System/370 assembly code in [35]. The resulting behavior is sanctioned by the Pthreads \nstandards, since it prohibits a concurrent write to a memory location (a for\u00admally unde.ned term) not \njust a concurrent write to a program vari\u00adable.10 Unfortunately, this behavior is currently not restricted \nto adja\u00adcent bit-.elds. A compiler may read and rewrite any other .elds sharing the same memory location \nbeing assigned. And it may be quite pro.table for a compiler to take advantage of this. As an admittedly \nextreme example, consider the following structure on a 64-bit machine, where it is know to the compiler \nthat x is 64-bit aligned: struct { char a; char b; char c; char d; char e; char f; char g; char h; } \nx; Assume the programmer intended a to be protected by one lock, and the other .elds by another. If the \ncompiler sees the sequence of assignments: x.b = b ; x.c = c ; x.d = d ; x.e = e ; x.f = f ; x.g = g \n; x.h = h ; It would almost certainly be more ef.cient to compile this into (taking some liberties with \nthe C notation): x = hgfedcb\\0 | x.a; i.e. to compile it into a load of x.a, which is then or ed with \na constant representing the values of the other seven .elds, and stored back as a single 64-bit quantity, \nrewriting all of x. Again, this transformation introduces a potential race, this time with a concurrent \nassignment to x.a, even though the two threads may in fact access disjoint sets of .elds. It would also \nbreak code that accesses all .elds from multiple threads, but chooses to protect x.a with a different \nlock than the other .elds, a fairly common occurrence in practice. The current Pthreads speci.cation \nexplicitly allows this, without any restriction on the .eld types. By our reading, it even allows it \nfor adjacent global variables outside of a struct declaration. Since linkers may, and commonly do, reorder \nglobals, this implies that an update to any global variable may potentially read and rewrite any other \nglobal variable. We do not believe that any interesting Pthreads programs can be claimed to be portable \nunder these rules. Fortunately, the original motivation for this lax speci.cation seems to stem from \nmachine architectures that did not support byte\u00ad 10 This formulation was the subject of a clari.cation \nrequest for the Posix standards[33]. The result makes it clear that this was intentional, and mem\u00adory \nlocation is intended to be implementation de.ned. wide stores.11 To our knowledge, no such architectures \nare still in wide-spread multiprocessor use. And in the case of uniprocessors, restartable atomic sequences[5] \ncan be used to make byte stores appear atomic. The real issue here is that for a language such as C, \nthe lan\u00adguage speci.cation needs to de.ne when adjacent data may be overwritten. We believe that for \nthe language to be usable in a multi-threaded context, this speci.cation needs to be much more restrictive \nthan what a highly optimizing compiler for a single\u00adthreaded language would naturally implement, e.g. \nby restricting implicit writes to adjacent bit-.elds. 4.3 Register promotion There are other optimizing \ntransformations that introduce variable updates were there were none in the source code. Consider the \nfollowing program which repeatedly updates the global shared variable x inside a loop. As is common in \nsome library code, the access to x is protected by a lock, but the lock is acquired conditionally, most \nprobably depending on whether a second thread has been started inside the process: for (...) { ... if \n(mt) pthread_mutex_lock(...); x =... x ... if (mt) pthread_mutex_unlock(...); } Assume the compiler determines \n(e.g. based on pro.le feed\u00adback [30] or on static heuristics as in, for example, [36]) that the conditionals \nare usually not taken, e.g. because this applica\u00adtion rarely creates a second thread. Following the implementation \nstrategy outlined above, and treating pthread mutex lock() and pthread mutex unlock() simply as opaque \nfunction calls, it is bene.cial to speculatively promote x to a register r in the loop[10], using, for \nexample, the algorithms outlined in [31] or [22]. This results in r= x; for (...) { ... if (mt) { x = \nr; pthread_mutex_lock(...); r = x; } r =... r ... if (mt) { x = r; pthread_mutex_unlock(...); r = x; \n} } x= r; The pthread standard requires that memory must be synchronized with the logical program state \nat the pthread mutex lock() and pthread mutex unlock() calls. By a straightforward interpretation \nof that statement, we believe that this requirement is technically satis.ed by the transformation. The \nproblem is that we have introduced extra reads and writes of x while the lock is not held, and thus the \nresulting code is completely broken, in spite of the fact that the implementation seems to satisfy the \nletter of the speci.cation, and is performing transformations that are reasonable without threads. It \nis worth noting that identical problems arise if the above code had called functions named f and g instead \nof pthread mutex lock and pthread mutex unlock, since f and 11 The .rst iteration of the Alpha architecture \nhad this characteristic, as did some earlier word-addressable machines. In the case of the Alpha, this \npart of the architecture was quickly revised. g may (indirectly) call thread library synchronization \nprimitives. Hence, again in this case, thread-safe compilation restricts transfor\u00admations on code that \nmay not be known to invoke thread primitives, and it has implications beyond the semantics of added library \ncalls; speculative register promotion around unknown procedure calls is generally unsafe. This again \nargues that compilers must be aware of the existence of threads, and that a language speci.cation must \naddress thread\u00adspeci.c semantic issues. And this one appears to have profound practical implications. \nWe know of at least four optimizing compil\u00aders (three of them production compilers) that performed this \ntrans\u00adformation at some point during their lifetime; sometimes at least partially reversing the decision \nwhen the implications on multi\u00adthreaded code became known. Unfortunately, we expect that in this case \nthread-safety has a measurable cost in single-threaded performance. Hence confusion about thread-safety \nrules may also make it hard to interpret even single-threaded benchmark performance.  5. Performance \nThe only parallel programming style sanctioned by the Pthreads standard is one in which Pthread library \nmutual exclusion primi\u00adtives are used to prevent concurrent modi.cation of shared vari\u00adables. And it \nis this restriction that allowed the implementation strategy outlined above to almost work. With this \nrestriction, the order in which memory operations become visible is intended to be irrelevant unless \nmemory operations are separated by a call to a Pthreads-library routine. There is much evidence that, \nat least in our context, this was mostly a reasonable choice. Programs that rely on memory ordering without \nexplicit synchronization are extremely dif.cult to write and debug. However, there is a cost involved \nin following this dis\u00adcipline. Operations such as pthread mutex lock() and pthread mutex unlock() typically \nrequire one hardware atomic memory update instruction, such as compare-and-swap, per libary call. On \nsome architectures (e.g. X86 processors), these instructions also implicitly prevent hardware reordering \nof memory references around the call. When they don t, a separate memory barrier instruction may be required \nas well. In addition, dynamic library calling overhead is often involved. The cost of atomic operations \nand memory barriers varies widely, but is often comparable to that of a hundred or more register-to-register \ninstructions, even in the absence of a cache miss. For example, on some Pentium 4 processors, hardware \nin\u00adstructions to atomically update a memory location require well over 100 processor cycles, and these \ncan also double as one of the cheaper mechanisms for ensuring that a store operation becomes visible \nto other threads before a subsequent load. As a result of the high cost of these hardware instructions, \nand the even higher cost of the pthread primitives built on them, there are a small number of cases in \nwhich synchronization performance is critical, and more careful and direct use of the hardware primi\u00adtives, \ntogether with less constrained use of shared variables, is es\u00adsential. In some cases it may also be necessary \nto avoid deadlock issues inherent in lock-based programming[7], or desirable because a different parallel \nprogramming model is preferable for an appli\u00adcation (cf. [32]). The potential practical bene.t of parallel \nalgorithms involving races has long been recognized. A variant of Gauss-Seidel iteration that took advantage \nof data races on a multiprocessor was described by Rosen.eld in 1969[29]. The continued use of the double-checked \nlocking idiom, even in contexts such as ours, where it is both technically incorrect[25], and often dangerous \nin practice, is another indication of at least the perceived need for such techniques.   There is a \nlarge literature on lock-free and wait-free pro\u00adgramming techniques (cf. [35, 12, 13]) that addresses \nprogram\u00adming techniques which rely directly on atomic memory opera\u00adtions, in addition to simple atomic \nloads and stores, but avoid locks. Java recently added a facility for supporting this kind of programming[20]. \nThese all involve races in our present sense. Although these techniques are currently only appropriate \nfor a small fraction of multi-threaded code, they are often desirable in lower level libraries, and hence \nmay affect the performance of many programs whose authors are unaware of them. For example, it is quite \ncommon to use atomic increment operations in the implemen\u00adtation of reference counting in the standard \nC++ string library.12 These techniques generally rely on the ability to access shared variables with \nordinary load and store instructions. In practice, some control over reordering memory references is \nneeded as well, but much of the performance of these techniques is attributable to minimizing such restrictions \non reordering. 5.1 Expensive Synchronization: An Example There are many examples in the literature in \nwhich lock-free code13 provides a performance advantage. See for example [24] for a recent and particularly \ninteresting one. What follows is a particularly simple example, which we be\u00adlieve more clearly illustrates \nthe issues. It takes the form of a simple parallel Sieve of Eratosthenes algorithm, implemented on shared \nmemory machines, for which shared variable access without order\u00ading or synchronization overhead appears \nto be critical. Although this problem appears contrived, it was extracted from a similar issue which \noccurred in our mark-sweep garbage collec\u00adtor. And indeed, any graph traversal algorithm using mark bits \nto track visited nodes will incur similar overheads, though it would probably represent a smaller fraction \nof the entire algorithm. Consider the following Sieve of Eratosthenes implementation: for (my_prime = \nstart; my_prime < 10000; ++my_prime) if (!get(my_prime)) { for (multiple = my_prime; multiple < 100000000; \nmultiple += my_prime) if (!get(multiple)) set(multiple); } where get and set operations implement a Boolean \narray A containing 100 million elements. In the simplest case, we might declare A as a suf.ciently large \narray initialized to false values, and implement get(i) as A[i] and set(i) as A[i] = true. For all values \ni between 10,000 and 100,000,000, this simple algorithm arranges that on completion get(i) is false if \nand only if i is prime.14 12 The GNU C++ library currently does so. There is a strong argument that, \nespecially in a multi-threaded context, the reference counting here is actually counter-productive. But \nreference counting implemented with atomic operations greatly outperforms the same algorithms implemented \nin terms of locks. 13 In some cases, this code is lock-free in the technical sense of ensuring progress \nif there is at least one runnable thread, and in other cases (such as [14]) it provides weaker guarantees. \nIn all cases, the performance is gained by allowing concurrent data access without locks for mutual exclusion. \n14 As a sacri.ce to simplicity, this algorithm does have the minor de.ciency that, as stated, it fails \nto compute primes smaller than 10,000. But even computing those with the normal sequential algorithm \nwould take a trivial amount of time. 60  60 50 50 40 40 30 30 20 20 10 10 0 0 Mutex spinl. at. upd. \nunsafe  Figure 1. Sieve execution time for byte array (secs) Figure 2. Sieve execution time for bit \narray (secs) Interestingly, it continues to do so if we run multiple copies of this program concurrently \nin multiple threads, assuming get and set operate on the same array, in such a way that the entry corresponding \nto a set argument becomes true sometime before program completion, and get returns false for entries \nthat have never been set. (Clearly set(i) is called only for values iwhich are either smaller than 10,000, \nor a multiple of such a number. Thus it is never called on a prime in our range of interest. Consider \nthe smallest composite number greater than or equal to 10,000 on which it is not called. This is a multiple \nof some number j<10,000. For any thread not to invoke set on all multiples of j, get(j) must have returned \ntrue. But then some other thread must have called set(j). That same thread would have invoked set on \nall multiples of j.) Thus N copies of the above program running in N threads, correctly compute primes \nunder extremely weak assumptions about the order in which set operations become visible to other threads. \nIf updates by one thread become visible to another before program termination, one thread will be able \nto take advantage of work done by another, and we will see speed-ups due the additional threads.15 Perhaps \nmore interestingly, there appears to be no good way to take advantage of this with proper synchronization \nto prevent concurrent modi.cation of array elements. Figure 1 gives running times of the above program, \nusing 1, 2, or 4 concurrent threads, on a 4-way multiprocessor with relatively low hardware synchronization \noverhead (1 GHz Itanium 2, Debian Linux, gcc3.3)16. Here the bit-array Ais implemented as an array of \nbytes. The .rst set of bars uses traditional pthread mutex syn\u00adchronization (one lock per 256 entries), \nand the second uses more recently introduced spin-locks, which perform somewhat better in this case.17 \nThe third uses volatile accesses to the array without other synchronization,while the last uses ordinary \narray accesses. The fourth uses ordinary byte loads and stores. Only the .rst two are compatible with \npthread programming rules. 15 As written, this algorithm is a poor match for a software distributed shared \nmemory system that minimizes update propagation, such as [18]. It remains correct, but would not result \nin a speed-up. 16 Note that gcc3.3 generally does not pipeline such loops, which is impor\u00adtant on this \narchitecture, for this simple a loop. Hence absolute performance is almost certainly suboptimal with \nthis compiler, and the actual overhead of the synchronization operations is understated. 17 Spin-locks \ntypically have the advantage that they avoid an expensive compare-and-swap-like operation when the lock \nis released. The version we used performs worse than the mutex implementation if processors are heavily \nover-committed, as with 20 threads. Hence it is often undesirable in practice. But none of our tests \nreported here exercise that case. The lock\u00adfree implementations are robust against processor over-commitment. \n80 70 60 50 40 30 20 10 0 Mutex spinl. at. upd. unsafe Figure 3. HT P4 execution time for byte array \n(secs) Figure 2 presents similar data, but with Aimplemented as a bit array. In this case the third set \nof bars uses a hardware cmpxchg instruction in the set implementation to atomically update a bit in the \narray without risk to the adjacent bits. The fourth set of bars re.ects the performance of the program \nwhich implements set with an ordinary or operation into the bit vector. This is incorrect for more than \none thread. (Like most programs with data races, it rarely fails during simple testing like this. Thus \ntime measurements are no problem.) Note that in either case, we obtain no speed-up over the synchronization-free \nsingle-threaded version by using the Pthread mutex primitives, but we see substantial speed-ups (and \nhence ef\u00adfective multiprocessor use) for the lock-free implementations. Repeating the byte-array experiment \non a hyper-threaded Pen\u00adtium 4 (2GHz, 2 processors with 2 threads each, Fedora Core 2 Linux), with relatively \nhigher synchronization costs, we see even less promising results for the fully synchronized versions \nin .g\u00adures 3. Here the single-threaded version seems essentially optimal, perhaps because it already \nsaturates the memory system. But for more realistic uses of a shared bit array, we again return to a \npicture more similar to the Itanium results. Figure 4 gives the time (in milliseconds) required for our \ngarbage collector[6] to trace a heap containing slightly more than 200MB of 24-byte objects, with byte \narrays used to represent mark bits. We again vary the number of threads participating in the trace. (For \nthe fully synchronized version, we use a lock per page in the heap.) We see reasonable scaling with thread \ncount, since the number of threads is less than the number of hardware threads. But even with 4 threads, \nthe properly synchronized code only barely exceeds the 1 thread 111 2 threads 111 4 threads 111 111 2000 \n111 111 111 111 111 111 111 111 111 111 1500 111 111 111 111 111 111 111 111 111 111 111 111 111 111 \n111 111 111 111 111 111 111 1000 111 111 111 111 111 1111 111 111 111 111 111 111 111 111 111 111 111 \n111 1111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 \n111 111 111 111 1111111 111 111 111 500 111 111 111 111 111 111 111 111111 111 111 111 111 111 111 111111 \n111 111 111 111 111 111 111111 111 111 111 111 111 111 111111 111 111 111 111 111 111 111 111111 111 \n111 111 111 111 111 111 111111 111 111 111 111 111 111 111 111111 111 111 111 111 111 111 111 111111 \n111 111 0 111 1111 1111 1111 111 Mutex spinl. at. upd. Figure 4. HT P4 time for tracing 200 MB (msecs) \nperformance of a single synchronization-free thread, and that only with the use of spin-locks.  5.2 \nConsequences of allowing data races As the above argues, there are cases in which it appears impossible \nto gain bene.t from a multiprocessor without direct .ne-grained use of atomic operations on shared variables. \nThis is impossible in a purely library-based threads implementation in which synchro\u00adnization is required \nfor concurrent data modi.cation. Once we al\u00adlow concurrent updates, it falls on the language speci.cation \nto give their semantics, and on the compiler itself to implement them. It is still possible to encapsulate \nthe primitives in something that looks to the programmer like a library for accessing shared vari\u00adables. \nSome of the recent Java extensions [20] take this route. But to retain full performance, the implementation \nneeds to understand that some of these primitives impose special memory ordering con\u00adstraints. The presence \nof unprotected concurrent accesses to shared vari\u00adables also implies that additional properties of a \npthread-like im\u00adplementation become visible, and should be addressed by the spec\u00adi.cation. Consider the \nsequence x=1; pthread_mutex_lock(lock); y=1; pthread_mutex_unlock(lock); Some implementations of pthread \nmutex lock() only in\u00adclude a one-way acquire barrier. Thus the above may be executed as pthread_mutex_lock(lock); \ny=1; x=1; pthread_mutex_unlock(lock); with the two assignments reordered. A direct reading of the pthread \nstandard appears to preclude that, but the transformation is undetectable in the absence of races. On \nsome architectures it has a signi.cant performance impact, and is thus desirable. In an environment in \nwhich data races are allowed, and this distinction is thus observable, locking operations should probably \nnot be speci.ed as preventing all reordering around them. Indeed, the Java memory model does not.  6. \nTowards a solution Several of us are trying to address these problems in the context of the C++ standard.[2, \n1] Other participants in this effort include Andrei Alexandrescu, Kevlin Henney, Ben Hutchings, Doug \nLea, Maged Michael, and Bill Pugh. We currently expect all of the problems to be solvable by a solution \nbased on the approach of the Java Memory Model[23], but adapted to the differing language design goals \nin a number of ways: 1. In the absence of type-safety guarantees, it appears unnecessary to fully de.ne \nthe semantics of all data races. It may be reason\u00adable to restrict data races to volatile accesses or \nto shared variable access made through certain library routines, thus par\u00adtially preserving the spirit \nof the Pthreads approach. This would still require that we much more carefully de.ne when a data race \nexists, so that we avoid the issues in section 4. 2. Some type-safety and security motivated issues \nbecome far less critical. In particular, we expect that much of the work on causality in [23] is not \nneeded in the context of a type-unsafe language. 3. The Java memory model traded performance for simplicity \nin a few cases (e.g. the prohibition against reordering a volatile store followed by a volatile load), \nwhich may be inappropriate in this context. 4. In the case of at least C++ bit-.elds, the compiler must \nintro\u00adduce stores, and hence the possibility of races, that were not present in the source. It seems \nlikely that on modern architec\u00adtures this can be limited to adjacent bit-.elds.  7. Acknowledgements \nI would like to thank Doug Lea, Peter Buhr, and the anonymous reviewers for very useful comments on an \nearlier draft.  References [1] A. Alexandrescu, H.-J. Boehm, K. Henney, B. Hutchings, D. Lea, and B. \nPugh. Memory model for multithreaded C++: Issues. http://www.open-std.org/JTC1/SC22/WG21/docs/papers/ \n2005/n1777.pdf. [2] A. Alexandrescu, H.-J. Boehm, K. Henney, D. Lea, and B. Pugh. Memory model for multithreaded \nC++. http://www.open-std. org/JTC1/SC22/WG21/docs/papers/2004/n1680.pdf. [3] M. Auslander and M. Hopkins. \nAn overview of the PL.8 compiler. In Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction, \npages 22 31, 1982. [4] A. Bechini, P. Foglia, and C. A. Prete. Fine-grain design space exploration for \na cartographic SoC multiprocessor. ACM SIGARCH Computer Architecture News (MEDEA Workshop), 31(1):85 \n92, March 2003. [5] B. N. Bershad, D. D. Redell, and J. R. Ellis. Fast mutual exclusion for uniprocessors. \nIn ASPLOS-V: Fifth International Conference on Architectural Support for Programming Languages and Operating \nSystems, pages 223 233, October 1992. [6] H.-J. Boehm. A garbage collector for C and C++. http: //www.hpl.hp.com/personal/Hans_Boehm/gc/. \n [7] H.-J. Boehm. An almost non-blocking stack. In Proceedings of the Twenty-third Annual ACM Symposium \non Principles of Distributed Computing, pages 40 49, July 2004. [8] P. A. Buhr. Are safe concurrency \nlibraries possible. Communications of the ACM, 38(2):117 120, February 1995. [9] J. D. Collins, H. Wang, \nD. M. Tullsen, C. Hughes, Y.-F. Lee, D. Lavery, and J. P. Shen. Speculative precomputation: Long\u00adrange \nprefetching of delinquent loads. In Proceedings of the 28th International Symposium on Computer Architecture, \npages 14 15, 2001. [10] K. D. Cooper and J. Lu. Register promotion in c programs. In Proceedings of the \nACM SIGPLAN 1997 Conference on Programming Language Design and Implementation, pages 308 319, 1997. [11] \nEricsson Computer Science Laboratory. Open source Erlang. http://www.erlang.org. [12] M. Herlihy. Wait-free \nsynchronization. ACM Transactions on Programming Languages and Systems, 13(1):123 149, 1991. [13] M. \nHerlihy. A methodology for implementing highly concurrent data structures. ACM Transactions on Programming \nLanguages and Systems, 15(5):745 770, 1993. [14] M. Herlihy, V. Luchangco, and M. Moir. Obstruction-free \nsynchro\u00adnization: Double-ended queues as an example. In Proc. 23rd Interna\u00adtional Conference on Distributed \nComputing Systems (ICDCS), pages 522 529, 2003. [15] HP Technical Brief. Memory ordering optimization \nconsiderations. http://h21007.www2.hp.com/dspp/files/unprotected/ ddk/Optmiztn.pdf. [16] IEEE and The \nOpen Group. IEEE Standard 1003.1-2001. IEEE, 2001. [17] JSR 133 Expert Group. Jsr-133: Java memory model \nand thread spec\u00adi.cation. http://www.cs.umd.edu/~pugh/java/memoryModel/ jsr133.pdf, August 2004. [18] \nP. Keleher, A. L. Cox, and W. Zwaenepoel. Lazy release consistency for software distributed shared memory. \nIn Proceedings of the 19th Annual Symposium on Computer Architecture (ISCA 92), pages 13 21, May 1992. \n[19] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. \nIEEE Transactions on Computing, C-28(9):690 691, 1979. [20] D. Lea. Concurrency jsr-166 interest site. \nhttp://gee.cs.oswego. edu/dl/concurrency-interest. [21] D. Lea. The JSR-133 cookbook for compiler writers. \nhttp: //gee.cs.oswego.edu/dl/jmm/cookbook.html. [22] R. Lo, F. Chow, R. Kennedy, S.-M. Liu, and P. Tu. \nRegister promotion by sparse partial redundancy elimination of loads and stores. In Proceedings of the \nACM SIGPLAN 1998 Conference on Programming Language Design and Implementation, pages 26 37, 1998. [23] \nJ. Manson, W. Pugh, and S. Adve. The java memory model. In Conference Record of the Thirty-Second Annual \nACM Symposium on Principles of Programming Languages, pages 378 391, January 2005. [24] M. M. Michael. \nScalable lock-free dynamic memory allocation. In Proceedings of the ACM SIGPLAN 2004 Conference on Programming \nLanguage Design and Implementation, pages 35 46, 2004. [25] B. Pugh. The double-checked locking is broken \ndeclara\u00adtion. http://www.cs.umd.edu/~pugh/java/memoryModel/ DoubleCheckedLocking.html. [26] B. Pugh. \nThe java memory model. http://www.cs.umd.edu/ ~pugh/java/memoryModel/. [27] W. Pugh. The java memory \nmodel is fatally .awed. Concurrency -Practice and Experience, 12(6):445 455, 2000. [28] J. H. Reppy. \nCml: A higher-order concurrent language. In Proceedings of the ACM SIGPLAN 1991 Conference on Programming \nLanguage Design and Implementation, pages 293 305, 1991. [29] J. L. Rosen.eld. A case study in programming \nfor parallel processors. Communications of the ACM, 12(12):645 655, December 1969. [30] V. Sarkar. Determining \naverage program execution times and their variance. In Proceedings of ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, Portland, Oregon, January 1989. [31] A. V. S. Sastry and R. D. C. \nJu. A new algorithm for scalar register promotion based on ssa form. In Proceedings of the ACM SIGPLAN \n1998 Conference on Programming Language Design and Implementation, pages 15 25, 1998. [32] N. Shavit \nand D. Touitou. Software transactional memory. In Proceedings of the Fourteenth Annual ACM Symposium \non Principles of Distributed Computing, pages 204 213, 1995. [33] A. Terekhov and D. Butenhof. The austin \ncommon standards revision group: Enhancement request 9 (austin/107): Clari.cation of memory location \n. http://www.opengroup.org/austin/docs/austin_ 107.txt, May 2002. [34] The MPI Forum. The message passing \ninterface (MPI) standard. http://www-unix.mcs.anl.gov/mpi/. [35] R. Treiber. Systems programming: Coping \nwith parallelism. Technical Report RJ5118, IBM Almaden Research Center, 1986. [36] Y. Wu and J. R. Larus. \nStatic branch frequency and program pro.le analysis. In Proceedings of the 27th Annual International \nSymposium on Microarchitecture, pages 1 11, 1994. \n\t\t\t", "proc_id": "1065010", "abstract": "In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure library-based approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure type-safety, which would entail even stronger constraints. The issues we raise are not specific to that context.", "authors": [{"name": "Hans-J. Boehm", "author_profile_id": "81423595101", "affiliation": "HP Laboratories, Palo Alto, CA", "person_id": "PP39052937", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065042", "year": "2005", "article_id": "1065042", "conference": "PLDI", "title": "Threads cannot be implemented as a library", "url": "http://dl.acm.org/citation.cfm?id=1065042"}