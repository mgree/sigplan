{"article_publication_date": "06-12-2005", "fulltext": "\n Demystifying On-the-Fly Spill Code Alex Alet`a Josep M. Codina David Kaeli UPC, Barcelona Antonio Gonz`alez \nNortheastern Univ., Boston, MA aaleta@ac.upc.edu UPC -Intel Labs, Barcelona kaeli@ece.neu.edu josex.m.codina,antonio.gonzalez@intel.com \nAbstract Modulo scheduling is an effective code generation technique that exploits the parallelism in \nprogram loops by overlapping iterations. One drawback of this optimization is that register requirements \nincrease signi.cantly because values across different loop iterations can be live concurrently. One possible \nsolution to reduce register pressure is to insert spill code to release registers. Spill code stores \nvalues to memory between the producer and consumer instructions. Spilling heuristics can be divided into \ntwo classes: 1) a posteri\u00adori approaches (spill code is inserted after scheduling the loop) or 2) on-the-.y \napproaches (spill code is inserted during loop scheduling). Recent studies have reported obtaining better \nresults for spilling on-the-.y. In this work, we study both approaches and propose two new techniques, \none for each approach. Our new algo\u00adrithms try to address the drawbacks observed in previous proposals. \nWe show that the new algorithms outperform previous techniques and, at the same time, reduce compilation \ntime. We also show that, much to our surprise, a posteriori spilling can be in fact slitghtly more effective \nthan on-the-.y spilling. Categories and Subject Descriptors D.3.4 [Processors]: Code generation/Compilers \nGeneral Terms Algorithms, Languages Keywords Modulo scheduling, register allocation, spill code 1. Introduction \nAs the complexity of microprocessors continues to increase with each new generation, power consumption \nbecomes a growing is\u00adsue. One possible solution is to use a simpler processor, such as a very long instruction \nword (VLIW) architecture. These architec\u00adtures have been shown to provide a good compromise between per\u00adformance \nand power consumption. VLIW processors execute in\u00adstructions in order. Therefore, performance relies \nheavily on the compiler, and in particular, ef.cient code generation. In recent years, VLIW architectures \nhave become very popular in the embedded DSP domain. In this context, multimedia appli\u00adcations and numerical \ncode are the most commonly executed pro\u00adgrams. For these applications, loops represent the majority of \nexe- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n05 June 12-15, 2005, Chicago, Illinois, USA Copyright c &#38;#169; 2005 ACM 1-59593-056-6/05/0006. . \n. $5.00. cution time. Hence, code generation for cyclic code becomes a key issue when considering compilers \nfor VLIW-based systems. In this area, software pipelining techniques have been shown to be very useful \nto boost the execution performance of loops. To effectively schedule loop bodies, modulo scheduling (MS) \n[15] has been shown to achieve very good results. MS increases parallelism by overlapping the execution \nof mul\u00adtiple iterations of a loop. New iterations can start without waiting for the previous iterations \nto .nish. A new iteration can start af\u00adter a constant number of cycles. This constant number of cycles \nis known as the initiation interval, or II. Since many iterations are executed concurrently, register \npressure increases because many in\u00adstances of the same instruction from different loop iterations may \nbe live concurrently. In this work we have assumed a .le that uses rotating registers [6] to deal with \nthese lifetimes. The use of cluster architectures is spreading to overcome de\u00adlays that arise due to \nthe transmission of signals with respect to the clock frequency [1]. For clustered processors, register \npressure is even higher because of several issues. First, some values may be live in various clusters \nconcurrently, therefore consuming one reg\u00adister in each cluster. Moreover, computation may not be balanced \nacross clusters, thus some of the clusters will be responsible for larger portion of the computation. \nThis imbalance may result in increased register pressure. Finally, inter-cluster communications may increase \nthe lifetime of some values. Spill code reduces register requirements at the expense of in\u00adcreasing memory \ntraf.c. Spilling a value means storing that value to memory. Then, the register where the value was being \nmaintain can be released between consecutive uses. Before the next use, the value is re-loaded from memory. \nBased on the phase of instruction scheduling where spill code is inserted, spill algorithms can be divided \ninto two main categories: spill code a posteriori and spill code on-the-.y. The former in\u00adserts spill \ninstructions after scheduling the loop. The later adds spill code during loop scheduling. On-the-.y spilling \nhas more freedom to spill. Besides, scheduling decisions can also take advantage of register spilling. \nTherefore, it seems to have a bigger potential. In fact, current state-of-the-art modulo scheduling techniques \ninsert on-the-.y spill code [5, 20]. We performed a thorough study of previous on-the-.y spilling schemes. \nThis study has motivated the design of the new scheme described in this paper that overcomes many of \nthe drawbacks we found. At the same time, we have stud\u00adied previous a posteriori spill algorithms and \nidenti.ed their main limitations. Based on this study, we then developed an alternative spilling scheme \nfor a posteriori also. In this paper we present an evaluation of both techniques and compare them against \nstate-of-the-art spilling algorithms. Results show that both schemes outperform previous approaches and, \nat the same time, they reduce compile time. However, one surprising result was that a posterori and on-the-.y \ntechniques turned out to achieve very similar results. In fact, a posteriori achieves even slightly better \nperformance. In this work, we describe in detail both algorithms and report the results obtained. We \nalso analyze the the cause of why spill a posteriori performs better than on-the-.y, and present an explana\u00adtion. \nThe rest of this paper is organized as follows: In section 2 we give some back-ground on spill code, \nwe de.ne some basic concepts and relate them with previous work. In sections 3 and 4 we propose two new \nspilling techniques, one a posteriori and one on-the-.y based on our study of previous schemes. In section \n5 we evaluate the new algorithms and compare them with stata-of-the-art heuristics. Finally, in section \n6 we present the conclusions of this work.  2. Spill code basics For a given modulo schedule S and a \ncycle c, we will let alive(c) denote the number of values that are alive in cycle c. We then de.ne: maxLive \n= max {alive(c)} 0=c=II-1 and the cycle c where alive(c)= maxLive is de.ned as the critical cycle, denoted \nCC. As was shown in [14], maxLive is an accurate approximation of the number of registers required by \nthe schedule. Hence, if the value for maxLive is greater than the number of registers in the microarchitecture \n(referred as n of reg hereafter), then the schedule is not valid. In order to reduce register pressure \nwe can add spill code, which involves storing values to memory in order to release a register for a number \nof cycles. These values are reloaded when a consumer instruction needs them. We will refer to the store \ninstructions inserted for spill purposes as spill-stores or s-stores and to new loads as spill-loads \nor s-loads. There have been a number of approaches proposed to perform register allocation and spilling \nfor acyclic code. Heuristics based on graph coloring [4] have been shown to be very effective. Unfortu\u00adnately, \nthese schemes cannot be applied to modulo-scheduled loops because values produced by multiple instances \nof the same instruc\u00adtion in a loop may be live concurrently. There have been some solu\u00adtions proposed \nto overcome these limitations [14], though they do not allow for the insertion of spill code. In this \npaper, we focus on spill code generation for modulo-scheduled code. An alternative scheme to register \nspilling for modulo schedules is to increase the II. In this case, the amount of overlap between it\u00aderations \ndecreases and therefore fewer values are live concurrently. However, reducing overlap also reduces the \namount of parallelism. In fact, the goal of modulo scheduling is to try to extract paral\u00adlelism by overlapping \nloop iterations. In addition, increasing the II does not guarantee that we can produce a valid schedule. \nThis is es\u00adpecially true for graphs with recurrences. On other hand, if we can produce a valid schedule \nby inserting spill code, we can maintain the II, but at the expense of increasing memory traf.c. In general, \nadding spill code is a better solution as has been shown in [12]. We can choose to generate spill code \nfor variables or for uses. Spilling for variables means storing a value to memory just after producing \nthe variable value and reloading it to a register for each consumer instruction (see .gure 1 c). Alternatively, \nspilling for uses means that the value is only loaded for a subset of its consumers (see .gure 1 b). \nTherefore, the value has to be kept in a register for some number of cycles. Both spill options are compared \nin [19] where it is shown that spilling of uses outperforms spilling of variables. Moreover, spilling \nof variables increases memory traf.c more than spilling of uses. We de.ne the time between two subsequent \nuses of a variable as the inter-use time. This quantity will be used to help decide which uses we will \ngenerate spill code for. Figure 1 shows more clearly how we de.ne inter-use times. Some other important \ncharacteristics of a spill heuristic are: how to select the values to spill,  how many values to spill, \nand  how to schedule spill instructions.  All these features are heavily dependent on when spilling \nis per\u00adformed. We can generate spill code after scheduling (i.e., spill a posteriori), or during scheduling \n(i.e., spill on-the-.y). We describe these two different approaches, as well as the other characteristics, \nnext. 2.1 Spill code a posteriori Spill code a posteriori is the name of a family of spill heuristics. \nThe main feature of these schemes is that spilling is considered only after a loop is scheduled. These \napproaches can take full advantage of the information provided by the schedule in order to identify for \nwhich uses spills will be inserted for. On the other hand, modulo scheduling produces very compact code, \nso it may be dif.cult to .t the new spill instructions within the computed schedule. To address this \nissue, previous a posteri\u00adori approaches compute a new schedule once spill code has been added. Usually, \nspill code is inserted for the consumer instruction pos\u00adsessing the longest inter-use lifetime (based \non the schedule before adding spill code). We can further tune the accuracy of this heuristic if we weight \nthe inter-use lifetime by dividing it by the number of memory instructions that had to be added for the \nspill (denoted as newMemTraf). In [19], it is also required that the selected inter-use spans the CC. \nAnother important feature of an a posteriori spill code algo\u00adrithm is when to stop adding spills (i.e., \nhow many spill candidates are selected at a time). Since the new spill instructions have not been scheduled \nyet, we do not know the actual register pressure. Therefore, we have to estimate how many values need \nto be spilled. One option is to add only one spill, then re-schedule the graph and repeat the process \nuntil no further spills can be added or until a valid schedule is found [12]. This approach prevents \nover-spilling at the expense of more computation time. The opposite option is to add as many spill instructions \nas possible, that is, until we saturate mem\u00adory bandwidth [18]. This scheme is faster because re-scheduling \nis performed only once. However, it over-spills (that is, it introduces more spill code than it is really \nrequiered) so memory traf.c in\u00adcreases signi.cantly. For that reason, the resulting schedules are not \nalways bene.cial. An hybrid option is to spill 2k candidates at a time (where k is the number of times \nthat we have re-scheduled the graph) [16]. This approach is faster than spilling a candidate at a time \nand does not introduce as many spill instructions as saturat\u00ading memory bandwidth. Finally, a different \napproach is to assume that the spill instructions will free a register during all the inter\u00aduse times. \nUsing this assumption, a new approximation of maxLive is achieved. Then spilling will stop when the estimated \nmaxLive is low enough (though when re-scheduling, the real maxLive may turn out to be bigger) [19]. As \ndiscussed above, previous a posteriori spill techniques re\u00adschedule the loop after inserting spill code. \nWhen performing re\u00adscheduling, register pressure can be further reduced if we schedule the s-stores as \nclose as possible to the value-producer instruction and s-loads as close as possible to the consumers. \nTo achieve this goal, previous a posteriori schemes scheduled spill instructions and their associated \nproducers/consumers as a single instruction-pair. These instruction-pairs are more dif.cult to schedule, \nwhich can lead to a longer schedule. In some cases, these instructions cannot  Figure 1. Example: spilling \nfor uses and spilling for variables. be scheduled due to resource con.icts and then the II must be increased. \n 2.2 On-the-.y spill code On-the-.y spill code heuristics take into account register require\u00adments during \nloop scheduling. If, at a particular point of the parital schedule, it is detected that register pressure \nis high, these schemes can insert and schedule spill instructions on-the-.y, that is, at the same time \nthat they schedule the original loop instructions. The main advantage to using this approach is that \nwe may have more .exibility scheduling spill instructions. A positive side-effect will be that instructions \nin the original loop can be scheduled while tak\u00ading into account the already scheduled spill instructions. \nTherefore, instruction scheduling and register spilling can interact to produce a better schedule. To \nthe best of our knowledge, there are only two techniques that insert spill code on the .y for modulo \nscheduled loops, URA-CAM [5] and MIRS [20]. URACAM is an approach to performing instruction scheduling, \ncluster assignment and register allocation in a single phase. The main obective of this approach is to \nmaintain a balance with critical resources (i.e., communications, register and memory) at each scheduling \npass. In order to achieve this balance, a .gure of merit is de.ned which consists on a set of percentages. \nEach percentage in the .gure of merit is associated with a resource and is assigned based on the pressure \nplaced on that resource during each scheduling step. This .gure of merit is used to compare partial schedules, \nselecting the most bene.cial. After an instruction has been scheduled, URACAM studies the bene.ts of \napplying spill code for each inter-use already in the partial schedule by evaluating the .gure of merit. \nThe amount of spill added at each step is only limited by the bene.t that is obtained (i.e., any inter-use \nlifetime that produces bene\u00ad.ts is spilled). Unlike traditional approaches, URACAM allows spill instructions \nto be scheduled at any distance from their pro\u00adducer/consumers.  MIRS is a modulo scheduling algorithm \nwith integrated on\u00adthe-.y spill code generation and back-tracking. On-the-.y spill code is inserted whenever \nthe partial schedule reaches a point where maxLive > 2 \u00b7 n of reg. Regarding spill can\u00addidates, they \nare selected following the heuristics described in [19]. However, unlike in [19], the new spill instructions \nare not scheduled together with their producers/consumers as instruction-pairs. They have some freedom \nto be scheduled fur\u00adther apart. In particular, a spill instruction can be scheduled up to 4 cycles away \nfrom its producer/consumer. If spill code does   a) b)  c) Figure 2. Problems with uracam not reduce \nregister pressure suf.ciently, then back-tracking is used and selected instructions are un-scheduled \nin order to re\u00adduce register pressure. In particular, the cycles in which the un-scheduled instruction \nis executed have to span CC. Thus, maxLive can be reduced by moving instructions, instead of by spilling \na value to memory.  3. On-the-.y spill code generation In this section we describe a new technique to \ngenerate spill code on-the-.y based on our study of previous approaches. In this paper, we will use URACAM \nas our reference technique. For this reason, we .rst discuss some areas for improvement in URACAM. The \nreason why we chose to start with URACAM as our base case is because its on-the-.y spilling strategy \nis less restrictive, and thus allows us to apply optimizations such as inserting spill code, even before \nwe detect an actual need for it. 3.1 Areas for improving URACAM When inserting spill code on-the-.y with \nURACAM we try to anticipate register constraints and insert spill operations in ad\u00advance. Thus, spill \ninstructions can be added even if maxLive < n of reg. But anticipating a spill can have some drawbacks. \nSince the .nal schedule has not been computed yet, spilling is performed with limited information which \ncan lead to making uninformed decisions. We can see an example of a poor decision being made in .gure \n2. The graph on the left represents the DDG of a loop. According to [11], nodes in this graph will be \nscheduled in al\u00adphabetical order. When node E is scheduled, the .gure of merit of URACAM may suggest \nthat introducing spill code for the edge E . B (graph in the center) leads to a more balanced sched\u00adule. \nThis may saturate memory ports. Then, when instruction F is scheduled, it is not possible to introduce \na spill for edge F . A. This may result in an invalid schedule because edge F . A has the longest inter-use \nlifetime.  In fact, this may be a critical drawback. In loops with high reg\u00adister requirements, there \nare often instructions at the end of the loop that depend on values generated at the beginning of the \nloop. How\u00adever, these dependencies are not visible in a partial schedule until both the producer and \nthe consumer have been scheduled. Since all intermediate instructions would have been scheduled before, \nthis long dependency is not visible until a major portion of the loop has been scheduled. Thus, the memory \nslots may have already been used to schedule spill for uses that possess shorter inter-use life\u00adtimes. \n 3.2 New on-the-.y spill algorithm In this section we present a new on-the-.y spill approach that overcomes \nthe problems we have highlighted above. The proposed scheme makes two scheduling passes. The .rst pass \ntries to sched\u00adule the loop without adding spill code. If maxLive = n of reg, then the schedule is already \nvalid and we save compile time. If instead maxLive > n of reg, we re-schedule the loop with on\u00adthe-.y \nspill code generation. During this second scheduling pass, the spill heuristics are guided by information \nobtained from the .rst computed schedule. For that reason, instructions are scheduled following the same \nstrategy used in the .rst pass so that both sched\u00adules are as similar as possible. The difference with \nthe .rst pass is that after scheduling an instruction, adding spill code is considered. More speci.cally, \nspill instructions can be inserted in two cases: 1. maxLive > n of reg: If, during a partial schedule, \nwe detect that maxLive > n of reg, we look for spill candidates within the current partial schedule. \nThe fact is that the to-be scheduled instructions can only increase register pressure, so sooner or later \nwe will need to spill some of the already scheduled values to memory. To schedule these spills, we use \nthe a posteriori spilling scheme described in section 4 (without re-scheduling), but ap\u00adply it to the \nalready-scheduled subgraph instead of to the whole graph. Our objective here is to try to strike a compromise \nbe\u00adtween inserting spill instructions late during scheduling (so that the s-loads and s-stores do not \ninterfere with the original mem\u00adory instructions). We can then insert spill instructions early dur\u00ading \nscheduling (so that spill instructions can be scheduled in the most suitable slots and then the rest \nof the instructions to be scheduled can take advantage of this knowledge). Note that if register pressure \ncannot be decreased enough such that maxLive = n of reg, then we can stop trying to schedule. We then \nwill increase the II and re-start the schedul\u00ading process. This helps to reduce compile time. 2. Estimation \nof register requirements An important feature of on-the-.y spill code generation is that it allows for \nthe insertion of spill code even before detecting an actual need for it so that instruction scheduling \ncan adapt to the new situation. For that purpose, we need to anticipate that the .nal schedule will be \nregister constrained. Otherwise, we could be over-spilling. Next, we will de.ne some concepts that we \nuse to anticipate that the .nal schedule will be register constrained. We de.ne the lifetime of a schedule \nS, lftms(S), as the sum of the individual lifetimes for each value: X lftms(S)= lftm(v) v.S Note that \nif lftms(S)> II \u00b7 n of regs then maxLive > n of regs In our spilling heuristic, we estimate lftms(S) \nfor the .\u00adnal schedule. Whenever this estimation satis.es the previously mentioned relation: lftms(S)> \nII \u00b7 n of regs we will antic\u00adipate that the .nal schedule will be register constrained and we will insert \nspill code. Hence, we need to estimate lftms(S) for the .nal schedule. However, some instructions have \nnot been scheduled yet. For these instructions, we will assume that they will be scheduled in the same \ncycle where they were placed in the .rst pass schedule. We will refer to this cycle as the estimated \ncycle. Now, we can estimate the lifetimes of all instructions and therefore we can also estimate lftms(S) \nfor the .nal schedule. Then, if lftms(S)> II\u00b7 n of regs we will anticipate spill code for the consumers \nwith the longest inter-use lifetimes. If both the producer and the consumer of the value are scheduled, \nwe can schedule the spill instructions and reduce the estimation of lftms(S). If any of them is not scheduled \nyet, we will only reserve the memory slots required to introduce the spill instruc\u00adtions. In that case, \nlftms(S) is reduced assuming that the inter\u00aduse will be spilled for its whole estimated lifetime. When \nboth the producer and the consumer are scheduled, we will schedule the spill instructions (if still required). \nWe iterate through this process until the estimation lftms(S)= II \u00b7 n of regs or until no further spill \ncan be introduced. Note that this algorithm will tend to insert less spill code than may be needed since \nit only estimates lftms(S). Since it is only an estimate, we are conservative in order to prevent over\u00ad \nspilling. After inserting spill code on-the-.y we proceed with the loop schedule. The next scheduled \ninstructions will have more free\u00addom in order to take advantage of the new spill code operations. More \nspeci.cally, when scheduling a node we consider holding off scheduling the instruction so that it can \nobtain its operands from an s-load instead of from the producer operation. Then, we select the cycle \nthat further reduces register requirements. If some node is then scheduled far from the estimated cycle, \nthe estimation pro\u00adduced for the remaining nodes can also be imprecise. To prevent this from happening \n, if: |real cycle(ins)-estimated cycle(ins)|> II we re-compute the estimated cycle for the instructions \nthat have not yet been scheduled. However, this occurs for less than 10% of the graphs that need spill \ncode.  4. NoRPS As was discussed in section 2.1, previously proposed a posteriori spilling schemes re-schedule \na whole loop after inserting spill in\u00adstructions. The reason for re-scheduling is that modulo schedules \ntend to be compact and so it may be dif.cult to .nd candidate slots for the insertion of new memory instructions. \nIn this work we propose a new scheme that adds spill code a posteriori without re-scheduling the loop. \nSpill instructions are B[i]=A[i] / B[i]; Register requirements a) No spill cycle aliv sp A[i]=B[i]+k; \n0 1 2 A++ B++ ldA,B /,+ k b) No re-schedule 0 1 2 A++/LSp B++ ldA,B /,+ k c) Re-schedule 0 1 2 A++/LSp \nB++ ldA,B /,+ k LOOP Figure 3. Example for spill code a posteriori. scheduled at the point in time that \nthey are added to the schedule (NoRPS stands for No Re-scheduling Posteriori Spill). In order to reduce \nregister requirements, spill code is usually inserted for con\u00adsumers whose inter-use lifetime is long \n(in terms of the number of cycles). Since there is considerable space between two consumers, it is almost \nalways possible to schedule the spill instructions with\u00adout re-scheduling the loop. In fact, scheduling \nspills without re\u00adscheduling the loop has some advantages, as we will see. Next, we will describe an \nexample that illustrates some of these bene.ts. 4.1 Motivating example Assume we have an architecture \nwith two memory ports and 12 reg\u00adisters (for illustration purposes only). Assume also that instructions \nin our example system incur the following latencies: one cycle for each add, two cycles for each memory \ninstruction and eight cycles for each division. At the top left corner of .gure 3 we show the code for \na simple loop. Beneath it we present the associated data depen\u00addence graph (DDG). Each node in the DDG \nrepresents one instruc\u00adtion in the loop, and each edge represents a dependency between two instructions. \nInside each node we describe the instruction ex\u00adecuted, and in parentheses, the cycle where it has been \nscheduled (assuming an II =3). In table a in .gure 3, we show the lifetimes of all of the values for \nthe schedule, before considering spill code. The second column titled aliv represents the number of values \nthat are alive during each cycle. For this schedule, maxLive = 14 (which is greater than the number of \nregisters available), so we need to add spill code. We will select the edge A ++ . StA because it has \nthe longest inter-use lifetime. We denote loads and stores associated with spills as sld and sSt. Next, \nwe schedule the spill instructions without re\u00adscheduling the whole loop. We want to schedule the sld \nas close as possible to StA, which in this example is cycle 10 (= 1 mod 3). However, both memory ports \nare used at modulo cycle 1 by the two loads of the original loop code iteration. Therefore, we have to \nmove the load one cycle back in the schedule to cycle 9. For the associated sSt, we want to schedule \nit as close as possible to A++. Again, the .rst candidate is cycle 1, but we have to move sSt one cycle \nforward to cycle 2 due to resource con.icts. The resulting register requirements of each instruction \nafter in\u00adserting the spill instructions (without re-scheduling the loop) are shown in table b of .gure \n3. Since we have not re-scheduled the loop, the only differences correspond to the value spilled, A++. \nIn cycle 2, we store A++ to memory. However, we still need to keep the value in a register an additional \ncycle longer due to the self\u00addependency. Therefore, A++ is live from cycle 0 to cycle 3. Note that moving \nthe sSt from cycle 1 (where we would have liked to have placed it) to cycle 2 (where we .nally placed \nit) has no ef\u00adfect on register pressure. We have also scheduled the sld in cycle 9. Therefore, the value \nproduced by A++ does not need to be in a reg\u00adister from cycle 3 to cycle 9, saving 6 lifetimes, 2 per \ncycle of the modulo schedule. Hence, we obtain maxLive = n of reg. Note that for sld, a register is required \nfrom cycle 9 (where it is sched\u00aduled), to cycle 12 where StA is. For this instruction, scheduling it \none cycle earlier (cycle 9) the ideal cycle (cycle 10) increases its lifetime by one cycle. Next we will \nsee what would happen if we re-schedule the whole loop with the new spill instructions. According to \nthe order\u00ading proposed in [11], priority is based on path length. The schedul\u00ading order works in a bottom-up \nfashion. Therefore, the ordering for the original instructions would be StA, +, /, ldA, A++, ldB, B++, \nStB. As we mentioned in section 2.1, when re-scheduling the loop, spill instructions are scheduled together \nwith consumer/producer instructions as instruction-pairs. Thus, the StA would be sched\u00aduled again in \ncycle 12. The sld would take advantage of the re\u00adscheduling and it would be placed closer to its consumer \nin cy\u00adcle 10, reducing its lifetime one cycle with respect to the non-re\u00adscheduling approach. The / would \nbe scheduled again in cycle 3, and the ldA in cycle 1. The A++ could be scheduled in cycle 0, but it \nhas to be scheduled together with its store. Since both memory ports are already occupied in cycle 1, \nwe have to move A++ one cycle back to cycle -1 1, such that the sSt can be scheduled in cycle 0. Then \nthe ldB has to be scheduled in cycle -1 because memory ports are saturated in cycles 1 and 0. Therefore, \nB++ also needs to be moved earlier in the schedule to cycle -2. Finally, the StB would be placed in cycle \n11. We present the register requirements of the new schedule in table c. As we can see, maxLive > n of \nreg. Since memory ports would be saturated, no further spilling would be possible. As a result, the II \nwould have to be increased. 4.2 Analysis of the motivating example Next, we will look in more detail \nat the differences between the two different a posteriori approaches used in the example: the re\u00adscheduling \nand the non-re-scheduling approaches. 1. If the loop is re-scheduled, instruction sld is scheduled together \nwith its consumer StA, which reduces sld s lifetime by one cycle with respect to the non-re-scheduling \napproach. This is one of the positive consequences of re-scheduling. Spill loads are placed closer to \nits consumers. 2. When instruction A++ is re-scheduled, it is moved one cycle earlier in the schedule \nbecause it has to be scheduled together with sSt as an instruction pair. However, placing sSt closer \nto A++ does not produce any bene.t on register pressure because A++ is alive for 3 cycles anyway due \nto its self-dependency. In this case, re-scheduling has not reduce register pressure. On the other hand, \nmoving A++ one cycle earlier increases the length of the schedule. Hence, in this case re-scheduling \nhas a negative effect. 3. During re-scheduling, instruction ldB has to be moved 2 cy\u00adcles earlier from \nits original scheduling position because there are other memory instructions (the spill instructions) \nalready scheduled. This increases ldB s lifetime by 2 cycles. In addition, instruction B++ has to be \nmoved 2 cycles later in the schedule  1 The cycle can be negative because we are doing a modulo schedule. \n  too, which in turn increases B++ s lifetime (and the length of the schedule) by an additional two \ncycles. These 3 points have illustrated the main differences between re\u00adscheduling and not re-scheduling. \nFirst, when a re-scheduling step is performed, spill instructions are scheduled closer to their pro\u00adducers/consumers. \nIn the case of an s-load, this often helps reduce their lifetimes. In the case of a store, it does not \nalways reduce the producer-instruction lifetime because there may be another con\u00adsumer scheduled between \nthe store and the consumer for which we have spilled. Second, when re-scheduling the original memory \nin\u00adstructions of the graph, there may already be some spill instructions scheduled, which can cause the \noriginal memory instructions to be scheduled further from their producers/consumers. This policy may \nbe worse than moving spill instructions because it also affects other original loop instructions, and \ncan increase the length of the sched\u00adule. The last important difference between both schemes is related \nto the strategy used to schedule the spill instructions during the re-scheduling step. These instructions \nare scheduled as instruction\u00adpairs with its associated producers/consumers. These instruction\u00adpairs are \nmore complex to schedule. Therefore, re-scheduling can increase the length of the schedule and also the \nII. In addition, by not re-scheduling the loop, we can also enjoy other advantages. For instance, since \nall of the original loop in\u00adstructions are already scheduled and they will not be moved, we can compute \nthe exact bene.t of spilling a certain candidate. Therefore, we can more precisely select the instances \nof where to insert spill code. We de.ne our selection heuristic in the next subsection. 4.3 Selecting \nspilling candidates Before describing our selection policy, we .rst introduce some new terms to help \nquantify spill characteristics. Given a particular cycle c of the schedule, we de.ned alive(c) as the \nnumber of live values in cycle c. In our previous example in .gure 3, we show the value for alive(c) \nin the second column from the right in tables a,b,c. If alive(c)> n of regs then: n of spills(c)= alive(c)-n \nof regs which indicates the number of values that need to be spilled. In the example we show the n of \nspills in the right-most column of tables a,b,c. Given a schedule S, n of spills(S) will be sum of the \nnumber spills found in each cycle of S: II-1 X n of spills(S)= spills(c) c=0 Therefore, for the example \nshown in .gure 3, we have 3 spills for the original schedule, 0 spills after inserting spill code without \nre\u00adscheduling, and 2 spills for the re-schedule. Now we can describe the metric that will be used to \ndetermine the spill priority. First, we compute the total number of spills in the schedule S. Then, for \neach register dependency, we compute the cycle where a s-load could be scheduled and the cycle where \na s-store could be scheduled. Then, for each register dependency we assume that the corresponding spill \nstore and load have been scheduled in the previously mentioned cycles and we compute the new number of \nspills that would remain in the resulting schedule S. . We select the candidate that further reduces \nthe number of spills, divided by the number of new memory instructions added: j. n of spills(S)-n of \nspills(S.) max newMemTraf.c Let us illustrate this metric with the example presented in .gure 3. As mentioned \nabove, the sld can be placed in cycle 9 and the sSt placed in cycle 2. Thus, we can save the lifetime \nbetween cycles 3 and 9, that is, we can reduce alive(c) by two in each cycle. Thus, the resulting schedule \nwould have 10, 12, 11 live values in each cycle (table b), which means 0 spills. The reduction in the \nnumber of spills is therefore 3. Since we have to add both sSt and sld instructions, we assign the value \nof the metric to be 3/2. In case of a tie in this .rst metric (as would be the case between dependencies \nA ++ . StA and B ++ . StB, we select the use with the longer inter-use lifetime. Once the spill instructions \nare scheduled, we update the cycles where s-stores and s-loads can be scheduled and the metric for all \nuses. We iterate over this process until maxLive = n of reg. If we .nd we cannot further improve the \nschedule during this process, we re-schedule the loop with the new spill instructions. During re\u00adscheduling, \nwe try to schedule the spill instructions as close as pos\u00adsible to their producer/consumer. To achieve \nthis goal, spill instruc\u00adtions are scheduled just after their associated producer/consumer have been \nscheduled. However, they do not have to be scheduled together as an instruction-pair. Therefore, in some \ncases they are separated. In the case where re-scheduling does not produce a valid sched\u00adule, we have \nto increase the II. Then, all the spill instructions added are removed in the hope that by using higher \nII, the loop can be scheduled without adding spill code.  5. Experimental Evaluation In this section, \nwe evaluate the spill techniques described in this paper: MIRS, URACAM, NoRPS and newOF (newOF denotes \nour new scheme that inserts spill code on-the-.y). In our evaluation, we use more than 4000 loops taken \nfrom the SPECFP2000 suite. In particular, we have selected only the loops present in the 10 for\u00adtran \nbenchmarks in the suite. We chose not to evaluate the C pro\u00adgrams because, due to C s complex memory \ndisambiguation, there are a signi.cant number of memory dependencies that cause recur\u00adrences. Hence, \nfor these programs, recurrences limit the bene.ts of modulo scheduling, independent of the particular \nregister alloca\u00adtion and spilling technique used. To generate the DDGs for these loops, we have used \nthe ORC compiler [10], with unrolling dis\u00adabled, and level O3 optimization. We have also used ORC to \nobtain loop execution frequencies and the average number of iterations executed for each loop. Clustered \narchitectures have been used in some commercial sys\u00adtems [17, 8, 13, 7, 9]. In this paper we consider \nthree different clustered architecture con.gurations (other con.gurations that we tested exhibited similar \ntrends): two that can issue up to 6 instruc\u00adtions per cycle and two that can issue up to 12 instructions \nper cy\u00adcle. We also study a non-clustered (i.e., uni.ed) con.guration. In the clustered con.gurations, \neach cluster is supplied with 1 integer functional unit, 1 .oating point unit, 1 memory port and 16 regis\u00adters. \nIn table 1, we present the latencies assumed for the different instructions. Next we describe the four \ncon.gurations tested: 1. uni.ed32r: A uni.ed con.guration with 2 integer functional units, 2 .oating \npoint units, 2 memory ports and 32 registers. (Issue 6). 2. 2c1b1l32r: A 2-cluster con.guration with \none 1-cycle latency bus for inter-cluster communication and 32 registers. (Issue 6). 3. 4c1b1l64r: A \n4-cluster con.guration with one 1-cycle latency bus for inter-cluster communication and 64 registers. \n(Issue 12). 4. 4c2b1l64r: A 4-cluster con.guration with two 1-cycle latency buses for inter-cluster \ncommunication and 64 registers. (Issue 12).  All spill algorithms are evaluated using the same instruction \nsched\u00aduler. More speci.cally, we utilize the swing modulo scheduler [11], though other approaches could \nalso be used. For the clustered archi\u00adtectures, before scheduling the loop, the DDG is partitioned using \n2c1b1l32r     4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Unified 32r 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 \n7.0 7.0 4c1b1l64r    4c2b1l64r    6. 5. 4. 3. 2. 1. 0. 0 0 0 0 0 0 0 6 5 4 3 2 1 0 . . . . . \n. . 0 0 0 0 0 0 0 Figure 4. IPC for the different spill techniques and different con.gurations. II sum \ninteger fp arith 1 3 mem 2 2 mul/abs 2 6 div/sqrt 6 18 Table 1. Latencies assumed for the instructions. \na multi-level strategy oriented towards modulo scheduling [2, 3]. clusion is that the two heuristics \npresented in this paper outperform previously proposed spilling schemes for all con.gurations and for \nall programs (except for 200.sixtrack, which is recurrence con\u00adstrained and obtains almost the same performance \nfor all spilling techniques). With respect to URACAM, we only report results for the uni\u00ad.ed architecture \nwith 32 registers. The reason why we choose not present results for the clustered con.gurations is that \nURACAM ap\u00adplies additional transformations (besides spilling) for clustered ar\u00adchitectures that may lead \nto very different schedules. Hence, the differences reported may not necessarily be related to more ef.-For \nimplementing MIRS, there are some user-de.ned parameters speci.ed. For these parameters, we have used \nthe values reported cient spill code generation. Therefore, the comparisons would be unfair and inconsistent. \nFor URACAM equipped with 32 registers, by Zalamea et al. [20]. the average speed-up of the proposed schemes \n(with respect to 5.1 Performance Evaluation URACAM) is close to 5%. As we anticipated in section 2.2, \nthe spilling algorithm used by URACAM often fails to spill the longest In .gure 4, we show the IPC (average \nnumber of instructions com\u00adinter-uses because it has already .lled all memory slots with other mitted \nper cycle) obtained for the 10 fortran SPECFP2000 bench\u00adspill instructions. Moreover, this approach sometimes \ninserts spill marks using the different spill techniques. The .rst important con\u00ad II sum uni.ed32r 2c1b1l32r \n4c1b1l64r 4c2b1l64r mirs 23786 25380 18749 17915 newOF 23695 25282 18494 17737 norps 23548 25093 18503 \n17732 Table 2. Sum of the II of all the loops for the different spill techniques and the different con.gurations. \nSL sum uni.ed32r 2c1b1l32r 4c1b1l64r 4c2b1l64r mirs 81653 87110 77005 76876 newOF 77196 82824 75675 75722 \nnorps 77523 82358 75857 75618 Table 3. Sum of the length of the schedules of all the loops for the different \nspill techniques and the different con.gurations. instructions before they are actually needed. For this \ntechnique, ag\u00adgressive spill insertion sometimes results in over-spilling, and sig\u00adni.cantly increasing \nmemory traf.c. With respect to MIRS, the differences are smaller. The average speedup of the proposed \ntechniques is 4% for the 6-issue con.g\u00adurations, and 3% for the 12-issue con.gurations. For the 4-cluster \ncon.gurations, the speed-up is slightly lower for two reasons. First, register pressure is not as critical \nbecause we use 64 registers. Sec\u00adond, the 4-cluster con.gurations also suffer from communication constraints. \nSince we use the same graph partition algorithm for all schedules, the differences are reduced. Another \nimportant issue to consider is that MIRS uses back-tracking to produce the sched\u00adule, while the two proposed \ntechniques and URACAM do not. This fact improves MIRS schedules, at the expense of a higher compi\u00adlation \ntime. In particular, we have measured that compilation time for MIRS and .nd that it approximately doubles \ncompilation time versus the other two schemes. The main reason why the two new schemes outperform MIRS \nis due to the metric used to select the spill candidates. In MIRS, spill candidates are selected according \nto the number of cycles between two successive consumers of the same value (without taking into account \nthe cycle in which the spill instructions can be scheduled). Hence, this approach does not take into \naccount the memory con.icts that will arise when schedul\u00ading the new spill instructions and relies on \nback-tracking to sched\u00adule them in an appropriate cycle. On the other hand, NoRPS and newOF insert spill \ncode while taking into account the impact of the scheduling of the new instructions. Another difference \nis that MIRS only requires the spilled value to be spilled to span CC, whereas NoRPS and newOF use a \nmore precise estimate, based on the im\u00adpact that spilling a particular candidate will have. Finally, \nwhen we compare the two proposed spill heuristics (newOF and NoRPS), we see that the performance differences \nare very small. The IPC obtained by the approaches is almost the same. For these two approaches, the \nspill candidates are chosen based on the same metric. Therefore, the spill instructions inserted are \noften the same. The main difference is that NoRPS inserts spill instruc\u00adtions a posteriori, whereas newOF \ninserts spill instructions on-the\u00ad.y. Hence, in most cases the differences between the two sched\u00adules \nare tied to the cycles where particular instructions are sched\u00aduled. This results in very small schedule \ndifferences. In some cases (where maxLive is only slightly bigger than n of regs), this small difference \nmay lead to a better schedule for one of the techniques. In general, NoRPS is slightly better because, \nas we explained in section 4, it assigns higher priority to original memory instructions than to memory \nspill instructions. Nevertheless, the .nal IPC dif\u00adfers less than 1% on average. Comparing the sum of \nthe II s and  % spill instructions added 100% 80% 60% 40% 20% 0% unified32r 2c1b1l32r 4c1b1l64r 4c2b1l64r \nFigure 5. Percentage of spill instructions added on-the-.y and a posteriori with MIRS for the different \ncon.gurations. the sum of the lengths of the schedules obtained with newOF and NoRPS (tables 2 and 3, \nrespectively), we can see that the differ\u00adences are small. In fact, for the 4c1b1l64r con.guration, newOF \nob\u00adtains better results. However, NoRPS performs better in some loops that have a bigger impact on IPC \nand so performance is higher for NoRPS. The main conclusion here is that a posteriori spilling can pro\u00advide \na spill code picture as accurate as on-the-.y spilling. This re\u00adsult was a bit unexpected. If we carefully \nstudy the MIRS approach, we will see that it is very similar to an a posteriori spill technique. To further \nsup\u00adport this conclusion, we present .gure 5. We will refer to spill instructions inserted during loop \nscheduling as on-the-.y spill in\u00adstructions, whereas the spill instructions inserted after scheduling \nthe original loop instructions will be referred to as a posteriori spill instructions. (Since MIRS includes \nback-tracking, some origi\u00adnal loop instructions may later be un-scheduled. However, we con\u00adsider a spill \ninstruction to be an a posteriori spill instruction if it is inserted after having produced a schedule \nwhere all the origi\u00adnal instructions of the loop are scheduled). As we can see in .g\u00adure 5, the percentage \nof on-the-.y spill instructions is low; approx\u00adimately 90% of the spill instructions are inserted a posteriori. \nIn order to introduce on-the-.y spill instructions, MIRS requires that maxLive > 2\u00b7 n of reg for a given \npartial schedule. This means that when spill instructions are added on-the-.y, register pressure is very \nhigh for the partial schedule. With added register pressure, it may be more dif.cult to produce a valid \nschedule, even if we insert spill code. In fact, when we reach maxLive > 2 \u00b7 n of reg, we often have \nto increase the II. Therefore, most of the spill instruc\u00adtions added by MIRS are added once a .rst complete \nschedule of the original loop has been produced. Previous a posteriori approaches did not perform as \nwell as on\u00adthe-.y schemes. We will further explore the causes for this in the next sub-section. 5.2 Further \nanalysis of a posteriori spilling As previously discussed in section 4, the key novelty of NoRPS with \nrespect to other a posteriori approaches is that a loop is not re\u00adscheduled after inserting spill code \ninstructions. We maintain that accesses relative to norps 100% 99% 98% 97% 96% 95% 94% 93% 92%  Figure \n6. Average for all the programs of the ratio of dynamic memory accesses with respect to NoRPS for the \ndifferent spill schemes and the different con.gurations. most of the time it is possible to schedule \nspill instructions without re-scheduling the whole loop because the spill instructions are usually inserted \nfor long lifetimes. To check whether this intuition holds, we computed the percentage of loops for which \na valid schedule was obtained without re-scheduling over the total number of loops that needed spill \ncode. We have observed that for 90% of the instances where spill code was inserted, the loop did not \nneed to be re-scheduled. Therefore, re-scheduling is not essential. We obtain a few advantages by not \nre-scheduling. First, the re\u00adsulting schedules are shorter because the instructions have been scheduled \nwith fewer con.icts. However, for modulo scheduling, the length of the schedule has a limited impact \non performance. Another positive consequence of not re-scheduling was in the ex\u00adample shown in .gure \n3 in section 4. If the insertion of spill in\u00adstructions cause non-spill memory instructions present in \nthe loop to be scheduled later, this can increase lifetimes for the original memory instructions, and \nmay also increase the lifetimes of other instructions in the loop. Finally, the metrics used to insert \nspills are more precise when no re-scheduling is performed because they can exactly measure the impact \nof a particular spill insertion. On the other hand, if we do not re-schedule, we may .nd it more dif.cult \nto .nd a suitable slot to schedule a spill instruction. For that reason, in case a valid schedule has \nnot been found, NoRPS allows for re-scheduling to be performed. However, the re-scheduling al\u00adgorithm \nis different from previous a posteriori approaches. In pre\u00advious schemes, spill instructions were scheduled \ntogether with their producers/consumers as instruction-pairs. This makes .nding an appropriate slot more \ndif.cult and the II may have to be increased due to resource con.icts. Using our modi.ed re-scheduling \nalgo\u00adrithm, we try to place spill instructions as close as possible to their producers/consumers, but \nnot necessarily back-to-back. 5.3 Memory traf.c In .gure 6, we compare the memory traf.c of the schedules \ngener\u00adated with the different spill schemes for each con.guration. In par\u00adticular, we have measured the \nratio of dynamic memory accesses with respect to the number of dynamic memory accesses produced with \nthe NoRPS approach. In the .gure we present the averaged ratios of the programs evaluated. The main conclusion \nis that the differences are small (in most of the cases around 1% and for all the cases under 3%). The \nspill technique that produces the greatest number of mem\u00adory accesses is NoRPS. This is due to the fact \nthat, in general, this spill heuristic achieves a lower II. This comes at the expense of in\u00adserting more \nspills. However, for the 4c1b1l64r con.guration, the sum of the II of newOF was lower than for NoRPS, \nand the number of memory accesses is still smaller for newOF. The newOF tech\u00adnique produces fewer memory \naccesses because when spills are inserted on-the-.y, the spill instructions can be scheduled closer to \ntheir producer/consumers. Therefore, shorter lifetimes are pro\u00adduced and fewer spills are required. In \nfact for 3 of the 4 con.gu\u00adrations reported, newOF generates the least memory traf.c.  6. Conclusions \nIn this work, we have described two new spill code generation schemes. Our new proposals are based on \na thorough study of the bene.ts and drawbacks of previous techniques for both on-the-.y and a posteriori \nspill approaches. Our new schemes outperform previous state-of-the-art techniques for register spilling. \nHowever, one unexpected result in this work is that we found that our scheme that inserts spill code \na posteriori and our scheme that inserts spill on-the-.y obtain almost the same performance. In recent \nwork, on\u00adthe-.y techniques were reported to obtain better performance. We have studied the reasons why \nthe proposed a posteriori scheme achieve similar schedules than on-the-.y, while previous proposals did \nnot. The key difference is that a loop is not re-scheduled after spill code is inserted. We have shown \nthat this has some advantages. The most important bene.t is that it allows for a very precise esti\u00admation \nof the effects that spilling a selected use can have. Hence, the metric used provides more accurate spilling. \nThe proposed on\u00adthe-.y algorithm uses the same metric. However its performance is slightly lower than \nthe a posteriori scheme, due mainly to the bene.ts obtained by scheduling spill instructions at the end. \n 7. Acknowledgements This work was partially supported by the Spanish Ministry of Education and Science \nunder contract TIN2004-03072 and Feder funds, by Intel, by CenSSIS the Center for Subsurface Sensing \nand Imaging Systems, under the Engineering Research Centers Pro\u00adgram of the NSF (Award Number EEC-9986821), \nand by the In\u00adstitute for Complex Scienti.c Software at Northeastern University.  References [1] V. \nAgarwal, M. S. Hrishikesh, S. W. Keckler, and D. Burger. Clock rate versus ipc: the end of the road for \nconventional microarchitec\u00adtures. In Proceedings of the 27th Annual International Symposium on Computer \nArchitecture, pages 248 259, June 2000. [2] A. Alet`a, J. M. Codina, J. S\u00b4anchez, and A. Gon\u00b4alez. Graph \npar\u00adtitioning based instruction scheduling for clustered processors. In Proceedings of the 34th International \nSymposium on Microarchitec\u00adture, pages 150 159, December 2001. [3] A. Alet`a, J. M. Codina, J. S\u00b4anchez, \nA. Gon\u00b4alez, and D. Kaeli. Exploiting pseudo-schedules to guide data dependence graph partitioning. In \nProceedings of the International Conference on Parallel Architectures and Compiler Techniques, pages \n281 290, September 2002. [4] G. Chaitin. Register allocation and spilling via graph coloring. ACM SIGPLAN \nNotices, 39(4):66 74, April 2004. [5] J. M. Codina, J. S\u00b4anchez, and A. Gonz\u00b4alez. A uni.ed modulo scheduling \nand register allocation technique for clustered proces\u00adsors. In Proceedings of the International Conference \non Parallel Architectures and Compiler Techniques, pages 175 184, September 2001. [6] J. Dehnert, P. \nHsu, and J. Bratt. Overlapped loop support in the cydra 5. In Proceedings of the 3rd International conference \non architectural support for programming languages and operating systems, pages 26 38, April 1989. [7] \nP. Faraboschi, G. Brown, J. Fisher, G. Desoli, and F. Homewood. Lx: a technology platform for customizable \nvliw embedded processubg. In Proceedings of the 27th international symposium on computer architecutre, \nJune 2000. [8] J. Fridman and Z. Green.eld. The tigersharc dsp architecture. IEEE Micro, pages 66 76, \nJanuary-february 2000. [9] P. Glaskowsky. Map1000 unfolds at equator. Microprocessor report, 12(16), \nDecember 1998. [10] R. Ju, S. Chan, T.-F. Ngai, C. Wu, Y. Lu, and J. Zhang. Open research compiler (orc) \n2.0 and tuning performance on itanium. Presented at the 35th International Symposium on Microarchitecture, \nDecember 2002. [11] J. Llosa, E. Ayguad\u00b4e, A. Gonz\u00b4alez, and M. Valero. Swing modulo scheduling. In Proceedings \nof the International Conference on Parallel Architectures and Compiler Techniques, September 1996. [12] \nJ. Llosa, M. Valero, and E. Ayguad\u00b4e. Heuristics for register\u00adconstrained software pipelining. In Proceedings \nof the 29th International Symposium on Microarchitecture, pages 250 261, December 1996. [13] G. G. Pechanek \nand S. Vassiliadis. The manarray embedded processor architecture. In Proceedings of the 26th Euromicro \nConference: Informatics: inventing the future, September 2000. [14] B. Rau, M. Lee, P. P. Tirumalai, \nand M. S. Schlansker. Register allocation for software pipelined loops. In Proceedings of the conference \non Programming language design and implementation, pages 283 299, June 1992. [15] B. R. Rau. Iterative \nmodulo scheduling: an algorithm for software pipelining loops. In Proceedings of the 27th annual international \nsymposium on Microarchitecture, pages 63 74, November 1994. [16] J. Ruttenberg, G. R. Gao, A. Stoutchinin, \nand W. Lichtenstein. Software pipelining showdown: optimal vs. heuristic methods in a production compiler. \nIn Proceedings of the conference on programming language design and implementation, pages 1 11, May 1996. \n[17] Texas Instruments Inc. TMS320C62x/67x CPU and instruction set reference guide, 1998. [18] J. Wand, \nA. Krall, M. A. Ertl, and C. Eisenbeis. Software pipelining with register allocation and spilling. In \nProceedings of the 27th International Symposium on Microarchitecture, pages 95 99, November 1994. [19] \nJ. Zalamea, J. Llosa, E. Ayguad\u00b4e, and M. Valero. Improved spill code generation for software pipelined \nloops. In Proceedings of the conference on programming language design and implementation, pages 134 \n144, June 2000. [20] J. Zalamea, J. Llosa, E. Ayguad\u00b4e, and M. Valero. Mirs: Modulo scheduling with integrated \nregister spilling. In Proceedings of the 14th workshop on languages and compilers for parallel computing, \naugust 2001. \n\t\t\t", "proc_id": "1065010", "abstract": "Modulo scheduling is an effective code generation technique that exploits the parallelism in program loops by overlapping iterations. One drawback of this optimization is that register requirements increase significantly because values across different loop iterations can be live concurrently. One possible solution to reduce register pressure is to insert spill code to release registers. Spill code stores values to memory between the producer and consumer instructions.Spilling heuristics can be divided into two classes: 1) <i>a posteriori</i> approaches (spill code is inserted after scheduling the loop) or 2) <i>on-the-fly</i> approaches (spill code is inserted during loop scheduling). Recent studies have reported obtaining better results for spilling <i>on-the-fly</i>. In this work, we study both approaches and propose two new techniques, one for each approach. Our new algorithms try to address the drawbacks observed in previous proposals. We show that the new algorithms outperform previous techniques and, at the same time, reduce compilation time. We also show that, much to our surprise, <i>a posteriori</i> spilling can be in fact slitghtly more effective than <i>on-the-fly</i> spilling.", "authors": [{"name": "Alex Alet&#224;", "author_profile_id": "81100280429", "affiliation": "UPC, Barcelona", "person_id": "P728826", "email_address": "", "orcid_id": ""}, {"name": "Josep M. Codina", "author_profile_id": "81100152094", "affiliation": "UPC - Intel Labs, Barcelona", "person_id": "P349113", "email_address": "", "orcid_id": ""}, {"name": "Antonio Gonz&#224;lez", "author_profile_id": "81100615797", "affiliation": "UPC - Intel Labs, Barcelona", "person_id": "P599965", "email_address": "", "orcid_id": ""}, {"name": "David Kaeli", "author_profile_id": "81100454796", "affiliation": "NEU, Boston, MA", "person_id": "PP14159837", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065032", "year": "2005", "article_id": "1065032", "conference": "PLDI", "title": "Demystifying on-the-fly spill code", "url": "http://dl.acm.org/citation.cfm?id=1065032"}