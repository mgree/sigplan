{"article_publication_date": "06-12-2005", "fulltext": "\n Shangri-La: Achieving High Performance from Compiled Network Applications while Enabling Ease of Programming \n Michael K. Chen1, Xiao Feng Li2, Ruiqi Lian3, Jason H. Lin2, Lixia Liu2, Tao Liu3, Roy Ju1 1. Microprocessor \nTechnology Labs 2. Intel China Research Center Ltd. 3. Institute of Computing Technology Intel Corporation \nBeijing, China China Academy of Sciences Santa Clara, CA, USA {xiao.feng.li, jason.h.lin, Beijing, China \n{michael.k.chen, roy.ju}@intel.com lixia.liu}@intel.com {lrq, liutao}@ict.ac.cn Abstract Programming \nnetwork processors is challenging. To sustain high line rates, network processors have extremely tight \nmemory access and instruction budgets. Achieving desired performance has traditionally required hand-coded \nassembly. Researchers have recently proposed high-level programming languages for packet processing, \nbut the challenges of compiling these languages into code that is competitive with hand-tuned assembly \nremain unanswered. This paper describes the Shangri-La compiler, which accepts a packet program written \nin a C-like high-level language and applies scalar and specialized optimizations to generate a highly \noptimized binary. Hot code paths identified by profiling are mapped across processing elements to maximize \nprocessor utilization. Since our compilation target has no hardware caches, software-controlled caches \nare generated for frequently accessed application data structures. Packet handling optimizations significantly \nreduce per\u00adpacket memory access and instruction counts. Finally, a custom stack model maps stack frames \nto the fastest levels of the target processor s heterogeneous memory hierarchy. Binaries generated by \nthe compiler were evaluated on the Intel IXP2400 network processor with eight packet processing cores \nand eight threads per core. Our results show the importance of both traditional and specialized optimization \ntechniques for achieving the maximum forwarding rates on three network applications, L3-Switch, MPLS \nand Firewall. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications \n data-flow languages, specialized application languages; D.3.4 [Programming Languages]: Processors code \ngeneration, compilers, optimization. General Terms Algorithms, Performance, Design, Languages. Keywords \nPacket processing, network processors, chip multiprocessors, throughput-oriented computing, program partitioning, \ndataflow programming. 1. Introduction In spite of the time and effort required, network processors like \nthe Intel IXP [16], IBM PowerNP [15], Broadcom BCM1250 [4] and PMC Sierra RM9000 [29] have been mostly \nprogrammed using Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 05, June 12 15, 2005, Chicago, Illinois, USA. Copyright 2005 ACM 1-59593-056-6/05/0006 $5.00. \nhand-coded assembly. Networks have mostly relied on the widespread use of a few core packet programs. \nTo achieve high line rates, though, a network program usually has very tight memory access and instruction \ncount budgets. In the past, careful hand\u00adoptimization of assembly code has been the most effective means \nof achieving the required performance given the small kernels and difficult resource constraints. As \nnetworks have evolved, so has the code running them, becoming larger, more diverse and complex. More \nand more network protocols are being developed for specialized applications (e.g. wireless, VoIP, proxies, \nnetwork-attached storage). Enhancements to base protocols have been implemented to satisfy load balancing, \nsecurity and reliability requirements. Shipped network hardware must operate correctly in an increasing \nnumber of different configurations. Hand-coded assembly has become a hindrance to developing new network \napplications and updating existing applications. Reusing common routines written in assembly in different \ncontexts is difficult, debugging assembly code is extremely tedious and maintaining assembly code is \na time-consuming effort. Even state\u00adof-the-art tools that abstract some of the assembly programming details \nstill expose to programmers the multi-threaded packet processing cores, heterogeneous memories and custom \ncommunication topologies found on network processors. Shangri-La, which consists of a programming language, \na compiler and a runtime system, simplifies development and accelerates performance tuning of network \napplications. The Shangri-La compiler accepts a program written in Baker, a high\u00adlevel, domain specific \nlanguage for designing modular network applications. Aggressive optimizations are applied so that code \nwritten in the high-level language can achieve performance comparable to hand-tuned assembly code. Although \nthe compiler currently targets the Intel IXP multi-core network processor, many of the techniques we \ndescribe are generally applicable since they deal with the difficulties of targeting heterogeneous multiprocessors \nand heterogeneous memories, and of optimizing network application constructs. The primary contributions \nof this paper are: . Demonstrated ability to achieve comparable hand-tuned performance on highly resource-constrained \nnetwork processors from code compiled from a high-level language. . A complete framework for aggressively \ncompiling network programs using both traditional and specialized optimizations techniques. Hot code \npaths identified from profiling are mapped across processing elements to maximize packet forwarding rates. \nDelayed-update software-controlled caches are automatically generated for unprotected, error-tolerant \napplication data structures, useful for network processors that have no hardware caches. Packet handling \noptimizations help reduce per-packet memory access and instruction counts. A custom stack model maps \nprogram stack frames to the fastest levels of the target processor s heterogeneous memory hierarchy. \n. Detailed performance evaluation of compiler generated code on real IXP hardware of three popular network \napplications, L3-Switch, MPLS and Firewall. The results show the importance of traditional and specialized \noptimization techniques in achieving maximal packet forwarding rates. The remainder of this paper details \nthe major components of the Shangri-La system and provides results from running our compiler-generated \ncode on real hardware. Section 2 introduces the Baker network programming language. Section 3 presents \nthe architecture and performance characteristics of Shangri-La s target hardware, the Intel IXP. Section \n4 introduces the Shangri-La compilation framework and the runtime support components. Section 5 details \nimportant optimization techniques implemented in the Shangri-La compiler. Section 6 presents the results \nof running three compiled network applications on real hardware. Section 7 describes related work in \ncompiling for network processors and Section 8 summarizes our findings. 2. Baker Programming Language \nCurrently, most network processors are programmed by hand using assembly. Assembly programming is undesirable \nfor many reasons. It is not portable between different ISAs. Debugging and performance-tuning assembly \ncode is a tedious and time-consuming effort. Finally, assembly programming requires extremely skilled \nprogrammers, since it exposes all the hardware resources. An IXP programmer must deal explicitly with \nheterogeneous memories, inter-processor communication, multi-processing and multi\u00adthreading in addition \nto the usual complexities. Even in state-of-the-art tools that allow network processor code to be written \nin a C dialect [17][18], these aspects of the hardware are configured by the programmer through manual \ninsertion of directives and keywords. For example, keywords indicate physical memory levels for referred \nvariables and intrinsics represent accesses to specialized inter-processor communication hardware. Likewise, \ndata packing and alignment optimizations are contingent upon input from the programmer. These language \nextensions impose significant responsibilities on the programmer and limit portability of the written \nprograms. Baker [13] is a platform-independent language for network application development. A programmer \nwrites packet-processing applications to an abstract machine with a single level of memory. Baker looks \nlike C, but includes constructs to enable development of large applications from reusable, modular components \nand to simplify the expression of network programs. Aside from this basic model, a programmer must only \nunderstand that the generated code may run on a multi-core processor and must explicitly identify any \ncritical sections. 2.1 Modular framework Baker programs are structured as a dataflow of packets from \nreceiver (Rx) to transmitter (Tx), as shown in Figure 1. A module is a container for holding related \npacket processing functions (PPFs), wirings, support code and shared data. PPFs contain the C-like code \nthat performs the actual packet processing. PPFs can hold temporary local state and access global data \nstructures. Packets enter and exit PPFs through channel endpoints. The input and output channel endpoints \nof PPFs wired together form communication channels (CCs), shown as directed arrows in the figure. CCs \nare asynchronous and can be thought of as queues or FIFOs. Output endpoints are also immediate-release. \nThis means that when a PPF places data on an output, the data is released onto the CC and is no longer \naccessible to the PPF. A piece of sample Baker code is shown in Figure 2. l3_switch.m Figure 1 The \nL3-Swtich Baker program containing Modules (.m), PPFs (.p) and CCs (directed arrows). L3-Switch bridges \nand forwards IP packets. l3_switch.l2_clsfr.ppf( ether_pkt * ph ) { int is_arp = ( ph->type == ETH_TYPE_ARP \n); int forward = ( ph->dst == mac_addrs[ph->metadata.rx.port] ); if( is_arp ){ channel_put( arp_cc, \npacket_copy( ph )); } if( forward ){ ipv4_pkt iph = packet_decap( ph ); channel_put( l3_forward_cc, \niph ); } else{ channel_put( l2_bridge_cc, ph ); } } Figure 2 l2_clsfr PPF in the l3_switch module \ndemonstrating packet (ph->protocol_field) and metadata (ph->metadata.metadata_field) accesses, use of \npacket primitives (packet_decap() &#38; packet_copy()) and placement of packets on CCs (channel_put()). \n 2.2 Packet support Designing packet and packet protocol support is especially tricky. Protocol developers \nmust be able to add new protocol types easily. There must also be a way to associate state with a packet \nfor use by the network application that is not stored in the packet. Component developers must be insulated \nfrom lower-layer protocol encapsulations and packet representations. Finally, manipulations on packets \nmust be computationally efficient. A packet s data can be thought of as a string of bits. How those bits \nare interpreted is determined by protocols. A protocol developer creates new protocols by describing \nthem using Baker s protocol construct, illustrated in Figure 3. The demux pseudo field specifies the \nprotocol s size in a particular packet. Packet metadata can be used to store state associated with a \npacket, but not contained within a packet. It is particularly useful to a network programmer for storing \nstate associated with a packet generated in one PPF and used later by another PPF. protocol ether { protocol \nipv4 { dst: 48; ver: 4; src : 48; length : 4; type : 16; ...(fields omitted) demux{ 14 }; src : 32; \n}; dst: 32; demux{ length << 2 }; }; packet_encap() packet_shorten()  DRAM (data) SRAM (metadata) \npacket_handle Figure 3 Example protocols and illustrations describing the effects of packet primitives \non a packet_handle. Baker programs manipulate packets through a packet_handle. The packet_handle points \nto packet metadata in SRAM. The packet metadata holds user-defined metadata and pointers to the actual \npacket data in DRAM. Robust encapsulation primitives make it easy to layer different packet protocols. \npacket_decap(), which internally uses the result of the demux field, is used to access the encapsulated \npayload. Equivalently, one can encapsulate one protocol inside another using packet_encap(). packet_add_tail() \nand packet_remove_tail() are primitives used to append data to a packet. The effects of these primitives \non a packet_handle are shown in Figure 3. There are other features of Baker used to support various packet \nprotocols [13] that are beyond the scope of this paper.  2.3 Language restrictions Baker imposes several \nlanguage restrictions to reduce analysis complexity and simplify code generation. Requiring type-alias-free \npointers, which prevent explicit typecasts to change the interpretation of a given memory location [11], \nsimplifies alias analysis. Recursion for code within a PPF is not supported for two reasons: our survey \nof network applications indicates recursion is not required; and it would complicate and add overhead \nto runtime stacks on processors like the Intel IXP with uncached, heterogeneous memories.  3. IXP Network \nProcessor While the optimizations presented in the paper may be applicable to other network processors, \nembedded systems or multiprocessors, the Shangri-La compiler currently targets the Intel IXP family of \nnetwork processors [16]. This section highlights the hardware s many exposed complexities that make it \ndifficult to program or target with a high-level compiler. 3.1 Processor cores Intel IXP processors are \nspecialized chip-multiprocessors, with one Intel XScale core and multiple microengines (MEs) [16]. The \nXScale processor is used to process control packets, execute non\u00adcritical application code, and handle \ninitialization and management of the network processor. The ME cores are primarily responsible for processing \npackets. The IXP2400 used for experiments in the paper, shown in Figure 4, has eight MEs. Other IXP processors \n(e.g. IXP1200 or IXP2800) that have different clock speeds and number of cores can also be targeted by \nShangri-La. MEs are lightweight, multi-threaded, pipelined processors running a special instruction set \ndesigned for processing packets. Each ME has its own instruction store, independent of other data memory, \nthat holds all the code for threads running on that core. This fast, uncached memory can only hold 4096 \n40-bit instructions, but it is large enough to hold critical instruction paths for most network applications. \n Figure 4 Block diagram of the IXP2400 showing characteristics of the XScale core, MEs and various memories. \nOne ME has support for four or eight hardware-assisted threads of execution. Threads on an ME are non-pre-emptive: \nexecuting code must give up control explicitly, on a memory instruction or a context switch instruction, \nbefore another thread can run. A thread arbiter swaps between ready threads in a round-robin order. This \nthread model simplifies synchronization within an ME and eliminates additional interrupt-handling hardware, \nbut burdens the programmer with the additional responsibility of handling context switches. 3.2 Memory \narchitecture Intel IXPs have heterogeneous and uncached memory hierarchies. The four separate levels \nof memory in the IXP are Local Memory, Scratch Memory, SRAM and DRAM. The sizes and access latencies \nof the memories are shown in Figure 4. With current IXP programming tools, each different level of memory \nmust be accessed explicitly by the programmer. Memory access instructions include a ref_cnt parameter \nthat facilitates wide memory accesses. With the ref_cnt parameter, 4B to 32B of Scratch Memory or SRAM, \nand 8B to 64B of DRAM can be accessed with only one memory instruction. These wide memory accesses are \nimplemented as reads and writes to a contiguous sequence of registers. Local Memory is treated differently \nfrom the other levels of memory. Local Memory is very fast, but is private to a given ME. A subset of \nLocal Memory can be accessed directly from any instruction without a load delay using 8-or 16- word offset \naddressing. The entire 640 words of Local Memory can be accessed from any thread in an ME with three \ncycles of latency. Notably absent in the IXP are any caches for the MEs. In general, network processor \narchitects have used die area and power budgets for multithreading and more processing cores instead \nof cache memory. This has been motivated by the observation that packets have little or no temporal or \nspatial memory locality. Additionally, caches are undesirable in embedded systems and in network applications \nbecause they introduce non-deterministic timing effects into a program s behavior. For example, it is \ndifficult to guarantee a minimum line rate for a running network application if it is affected by a cache \nhit rate. 3.3 Specialized hardware Although MEs don t contain traditional caches, they do have hardware \nwhich can be used to implement a software-controlled cache. Each ME contains a small 16-entry content-addressable \nmemory (CAM). Each entry of the CAM consists of a 32-bit address tag and a 4-bit result. A cam_lookup \nwith an address key will return the matching tag s entry number and the 4-bit result on a hit, or the \nLRU entry number on a miss. An implementation of a software-controlled cache using the CAM will be described \nin Section 5.2. The IXP network processor includes many other application\u00adspecific features [16] that \nare beyond the scope of this paper.  4. Shangri-la Shangri-La is a research project exploring new technologies \nfor improving network processor performance, programmability and usability. Shangri-La consists of the \nBaker programming language, a compiler, a runtime system and support libraries. Compiler development \nhas focused on innovating and identifying effective optimization techniques, while runtime efforts have \nexplored adaptation techniques to improve performance or reduce power consumption. 4.1 Compiler The Shangri-La \ncompiler leverages the large code base of the ORC [19][1] project. Extensive changes to the base compiler \nwere made to support the complexities of compiling to the Intel IXP. An overview of the compiler flow \nis shown in Figure 5. Right after the source program is converted into nodes of the WHIRL intermediate \nrepresentation (IR), the Function Profiler, which takes a user-supplied packet trace, simulates the network \napplication by interpreting the IR nodes. During simulation, the Functional profiler collects global \ndata structure access frequencies, CC utilizations and relative PPF execution times. The Functional profiler \nis immediately followed by Inter\u00adprocedural analysis (IPA) and global optimizer. IPA is primarily responsible \nfor forming aggregates, collections of PPFs that are mapped to one processing element. The partitioning \nstrategy tries to maximize packet forwarding rates, using Functional profiler statistics to identify \nhot CCs to eliminate and frequently executed PPFs to duplicate. The aggregation methodology will be described \nfurther in Section 5.1. The formed aggregates are then dumped into separate WHIRL IR files with each \nfile representing one aggregate. The combined IPA and global optimizer is also responsible for managing \nglobal memory. While most global application data structures are mapped to SRAM, some data can be placed \nin Scratch Memory, though, which has about half the latency of SRAM, but is significantly smaller. Possible \ndata structures to be placed in Scratch Memory can be identified with Functional profiler access frequency \nstatistics. Stage Operations Front-end Parse Baker parser Functional Simulate application profiler \nCollect execution frequencies and access frequencies  Perform inter-procedural  IPA analysis (IPA) \nand Estimate code sizes and global execution frequencies optimizer Map global data to memory  Choose \nSW caching candidates  Analyze packet primitives  Form aggregates  Code Perform WOPT generator Lower \nWHIRL to CGIR  Perform CG optimizations  Layout program stack   Figure 5 Overview of Shangri-La \ncompilation stages. Two different paths in the Code Generator target the different IXP cores. Infrequently \nexecuted control, management and initialization code is mapped to the XScale core. To generate code for \nthe XScale, the WHIRL nodes are transformed back to C and then compiled by gcc. We will not discuss this \npath further since it is not critical to performance. Code mapped to the MEs is extensively optimized. \nFirst, the compiler applies SSA-based optimizations like dead code elimination, copy propagation and \nredundancy elimination [7]. Afterwards, the WHIRL nodes are lowered into a code generation intermediate \nrepresentation (CGIR) adapted for the ME instruction set in which low-level optimizations like global \nregister allocation and instruction scheduling are performed. Significant effort was spent dealing with \ntricky aspects of the ME instruction set. For example, the 32 general-purpose registers available to \neach ME thread are divided into two banks so that instructions with two source operands must have each \noperand originate from a different register bank. This additional constraint must be considered during \nglobal register allocation. Special handling was also required to support wide memory accesses. As mentioned \nearlier, wide Scratch Memory, SRAM and DRAM reads and writes access a sequence of adjacent 32-bit registers. \nTo properly handle the register dependencies for these memory instructions that read or write multiple \nregisters, the CGIR implements aggregate pseudo-registers which can associate dependencies to individual \nregisters or to the entire set of registers. 4.2 Runtime system Code generated by the Shangri-La compiler \nis run on the Intel IXP atop a thin, custom runtime system (RTS). The RTS includes libraries that abstract \ncommonly found network processor hardware resources to facilitate runtime reconfiguration of the system. \nFor example, the RTS can dynamically decide how to map CCs to one of many possible hardware-specific \nimplementations (e.g. the IXP has ME next-neighbor registers or Scratch Memory rings for this purpose). \nThe most promising uses of runtime reconfiguration being explored include dynamically turning ME cores \noff to reduce power consumption when network traffic is low and dynamic code reconfiguration to adapt \nto changes in processed packet characteristics [36]. This part of Shangri-La is under development and \nnot the focus of this paper.  5. Key Optimizations At high line rates, network processors have extremely \ntight instruction and memory access budgets. For OC-48 (2.5Gbps) with minimum size 64B packets, less \nthan 700 instructions can be dedicated to processing a given packet on the IXP2400 (which is designed \nfor the OC-48 configuration) with six MEs (two of the eight MEs are dedicated to Rx and Tx, respectively). \nAvailable memory bandwidth also is a precious resource on IXPs. We performed a simple memory access experiment \non our IXP setup to characterize the estimated maximum possible forwarding rates for different numbers \nof memory accesses per packet, as shown in Figure 6. In this experiment, all six programmable MEs are \nexecuting a simple tight loop issuing only memory accesses. The numbers of memory accesses in the tight \nloop are plotted on the x axis and the achieved forwarding rates are plotted on the y axis. This figure \nsuggests that to achieve 2.5Gbps for minimum sized packets (64B), there can be no more than two DRAM \naccesses, eight SRAM accesses or 64 Scratch memory accesses for each packet. Also evident in the figure \nare the fractionally lower forwarding rates that result from issuing wider accesses to a given memory \nlevel. 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Forwading rate (Gbps) 1 2 4 8 16 32 64 128 Memory a ccesses / 64B \npacket Scratch (4B) Scratch (32B) SRAM (4B) SRAM (32B) DRAM (8B) DRAM (64B) Figure 6 Illustrates the \neffects of memory accesses per packet (1-128), memory level (DRAM, SRAM or Scratch) and memory access \nwidth (8B-64B) on the forwarding rate of a simple application running on the IXP2400. While the application \nof traditional scalar inter-procedural and intra-procedural optimizations are critical to meeting the \ntight instruction and memory budgets, the optimizations highlighted in this section describe special \ntechniques we applied to meet our performance goals. 5.1 Aggregation Aggregation attempts to map the \napplication onto available processor resources in order to maximize packet forwarding rates. Aggregation \nmakes use of a throughput model, statistics from the Functional profiler and an algorithm for creating \naggregates from multiple PPFs. n Equation 1 t .\u00d7 k p The throughput relationship shown in Equation 1 \ndescribes packet throughput (t) in terms of the number of MEs (n), the pipeline stage with the lowest \nthroughput (k) and total number of pipeline stages (p). For a given IXP processor, n is fixed, leaving \np and k for optimization. k is derived primarily from estimated PPF execution times and CC overhead statistics \ncollected by the Functional profiler. The goal of aggregate formation is to maximize packet forwarding \nrates. Compared to traditional multi-processor optimization where the goal is to minimize total execution \ntime, this relationship: better reflects how packet pipelines behave; describes the optimization goal \naccording to how performance is evaluated for packet processing; and eliminates system latency from the \noptimization criteria. The last two points differentiate network applications from normal program optimization \nin that latency of a packet through the system can be tolerated in many cases, but minimum forwarding \nrates must be guaranteed. The throughput-driven cost model can result in significantly different code \nfrom traditional program parallelization that focuses on latency reduction. When parallelizing a fixed \nworkload, all the system resources are consumed toward a single goal to minimize total execution time. \nIn throughput-oriented systems, the available resources are used to maximize the number of inputs that \ncan be simultaneously processed. Unlike workload optimization, long latency communication and memory \naccesses are tolerable as long as they can be hidden with work from processing other inputs and memory \nbandwidth is not saturated. This cost model is used to drive aggregate formation, as shown in Figure \n7. To maximize throughput, our compiler can choose to pipeline or duplicate code across multiple processing \nelements. In pipelining, a packet processing task is divided into aggregate pipe stages (each representing \none stage in the pipeline) that are mapped to different processing elements connected via CCs. During \nexecution, packets are passed between the aggregate pipe stages in an assembly-line fashion. Our model \ncorrectly indicates that with n fixed, adding pipe stages (increasing p) requires a proportional decrease \nin k to maintain the same throughput. done . false while ! done do done . true {dom, next_dom} . FIND_DOMINATING( \naggregates ) if EXEC_TIME( dom ) >> EXEC_TIME( next_dom ) then if DUPLICATE_IMPROVES_THROUGHPUT( dom, \ntarget_throughput ) DUPLICATE( dom ) done . false continue aggregate_pairs . FORM_PAIRS( aggregates ) \nSORT_BY_HIGHEST_CHANNEL_COST( aggregate_pairs ) foreach pair in aggregate_pairs do if MERGE_IMPROVES_THROUGHPUT( \npair, target_throughput ) &#38;&#38; MERGE_SATISFIES_CODESIZE_LIMIT( pair ) then MERGE( pair ) done . \nfalse break if done &#38;&#38; NUM( aggregates ) > num_processors then RELAX_CONSTRAINT( target_throughput \n) done . false foreach aggr in aggregates do if( ! SATISFIES_CODESIZE_LIMIT( aggr ) | | INFREQUENTLY_EXECUTED( \naggr ){ MAP_TO_XSCALE( aggr ) remove aggr from aggregates } duplication_factor . num_MEs / NUM( aggregates \n) MAP_TO_MES( aggregates, duplication_factor ) Figure 7 Pseudo-code for forming aggregates. An individual \naggregate pipe stages or the entire pipeline can also be duplicated to run on multiple processors. Duplicating \nan aggregate pipe stage effectively doubles its throughput. The duplicated pipe stage can now handle \ntwice as many packets in steady-state flow, even though the latency through it remains unchanged. If \nthe entire packet pipeline is duplicated instead, floor(n / p) is the pipeline replication factor. On \nreal network programs and IXP hardware, the throughput model biases against pipelining and favors duplication \nfor two reasons. Firstly, to maximize throughput of the slowest pipe aggregate, work must be evenly partitioned \nacross all pipeline aggregates, a challenging task in practice. Secondly, pipelining naturally adds overhead \nfor communicating data over CCs between each pipe aggregate compared to an equivalent aggregation without \npipelining. Supporting pipelining, though, is necessary for several reasons. MEs have very limited code \nstore. If a network application s critical path cannot fit into the code store of a single ME, there \nis no choice but to utilize pipeline stages. Pipelining may also have beneficial secondary efforts. Pipelining \naccess to multiple locks might result in less contention than duplicating one aggregate with multiple \nlocks. It might also reduce capacity misses for software-controlled caching (Section 5.2). Aggregates \nare formed heuristically by merging or duplicating PPFs to maximize system throughput. When merging, \nthe goal is to improve throughput by reducing communication costs. Here, pairs of PPFs with the highest \ncommunication costs are placed on the same aggregate. Pipeline aggregate duplication is used to improve \nthe throughput of the slowest pipe aggregate if its throughput is much less than the other pipeline aggregates. \nAfter the aggregates have been formed, frequently executed aggregates representing the core packet processing \nfunctions are mapped to the MEs while infrequently executed aggregates representing support, control \nand initialization functions are mapped to the XScale processor. An important consideration in real-time \napplications like software-controlled cache entries would have to be protected by critical sections or \nthe home location would have to be checked on every access, both of which would be expensive and eliminate \nany caching benefits. Shangri-La generates a novel delayed-update software cache that can be used in \nerror-tolerant applications like packet processing. Suitable caching candidates are frequently read data \nstructures that have high hit rates, but are infrequently written. A frequently found pattern are structures \nthat are frequently read by the packet processing cores, but infrequently written by maintenance, control \nor initialization code. Updates to these structures are not protected by critical sections in the original \ncode, but rely on the coherency of a single atomic write to guarantee correctness of an update. A delayed-update \ncache only checks on every ith packet for updates at a cache line s home location, as shown in Figure \n8. This significantly reduces the frequency and cost of coherency checks, but causes updates to cached \nentries to be delayed relative to changes in the home location (e.g. in SRAM). While incoherency is undesirable \nin normal applications, delayed updates in network programs only causes packet delivery errors. Fortunately, \nnetwork protocols are tolerant of packet delivery errors. For example, TCP, used for most connection-oriented \ninternet messages, can request retransmission of lost frames in a stream [34]. Quality of service (QoS) \nrouters explicitly drop frames on selected packet streams to throttle bandwidths, and firewalls drop \nselected packets to secure internal networks from the internet. Code on store path Code on load path \n(infrequently executed) (frequently executed) dataglobal . val tmp . dataglobal Optimized Original Access \nAccess ... tmp ... packet processing is worst case execution time (WCET) analysis. Computing bounds \non task execution in the system ensures that the network processor can maintain a minimum line rate. \nThis analysis dataglobal . val updateddata . true can be incorporated into our current compilation framework \nthrough an iterative compilation design. Results of WCET analysis on code if count > check_limit then \ncount . 0 if updateddata then CLEAR_CACHE() produced by the Code generator can be fed back into the IPA \nand global optimizer to modify compilation decisions or to notify the programmer that the current program \nwill be unable to achieve the updateddata . false count++ if CACHE_HIT( data) then tmp . datacached \n user-specified minimum performance targets. else 5.2 Delayed-update software-controlled caching Packet \nprocessing cores in the Intel IXP do not have hardware caches. The common belief is that packet applications \nlack enough interesting locality to dedicate die-area for caches. For example, little locality exists \nin packets stored in DRAM since packets are usually processed by one thread and then leave the system \ncompletely. Recent studies, though, have shown packet programs have locality in the application data \nstructures. For example, Baer et al [3] as well as Chiueh and Pradhan [6] demonstrated architectures \nwhere caching can improve the forwarding rate of packet route lookups. The Shangri-La compiler utilizes \nexisting IXP hardware to implement a software-controlled cache that tries to exploit available application \ncaching opportunities without hardware caches. On the IXP, the CAM (Section 3.3) can be used to do fast \nlookups for available cache entries and cache lines can be stored in an ME s Local Memory which is available \nto all its threads. To identify good caching candidates, expected hit rates and access frequencies for \nglobal data access statistics from the Functional profiler are used. To correctly maintain strict cache \ncoherency, access to tmp . dataglobal ... tmp ... Figure 8 Code for store and load paths for delayed-update \ncache to dataglobal. Updates to the shared global are detected by changes in the compiler-generated updateddata. \nFor a given application, a minimum per-packet load update check rate (rload_check) can be calculated \nfrom a user-specified per\u00adpacket maximum tolerable packet delivery error rate (rerror), the per\u00adpacket \nrate of expected stores to the variable (rstore), and the per\u00adpacket rate of expected loads to the variable \n(rload), as shown in Equation 2. As expected, this equation suggests reductions in expected stores or \nloads can reduce the minimum update check rate. rstore \u00d7 rload Equation 2 load _ check r = r error 5.3 \nOptimized packet handling Packet encapsulation (packet_encap(), packet_decap(), packet_extend, packet_shorten), \npacket data access (data_read, data_write) and metadata access (meta_read, meta_write) primitives all \nsignificantly impact instruction and memory counts. For example, each packet read and write requires \nup to {38 + 5 * access_size_in_words} instructions and involves at least one SRAM and one DRAM access. \nGiven the frequency of packet reads and writes in packet processing code, this overhead can be significant \nrelative to the 700 instructions / packet budget for achieving 2.5Gbps on the Intel IXP2400. Given the \nfrequency packet handling operations occur in real application code and the limited per-packet instruction \nand memory access budget available, optimizing them can result in significant performance improvements. \n5.3.1 Packet access combining (PAC) Network applications are naturally expected to access fields of a \npacket during processing. Packet data are always stored in DRAM memory on the IXP because packets can \nbe extremely large and in most cases, only the header of the packet is accessed by a network application. \nAccording to Figure 6, though, if we simply map every packet access to a DRAM access, packet forwarding \nrates would be quickly limited by DRAM bandwidth. To prevent this, an analysis incorporated in the IPA \nand global optimizer and the Code Generation stages of the Shangri-La compiler aggressively combines \nmultiple protocol field accesses into a single, wide DRAM access. For example, in Figure 2, the packet \nfields dst and type can be accessed together using only one DRAM access. This optimization can also be \napplied to combine packet SRAM metadata accesses. Packet accesses to be combined must satisfy three criteria: \n. packet_handles must be equal. . The address ranges of the packet data accesses must be adjacent or \nwithin a specified bounded range. For the IXP, which is optimized for wide memory accesses, even accesses \nseparated by 32- or 64-bits can benefit from combining. . The combined data width can not exceed the \nwidth that can be accessed by one memory instruction. Packet access combining is performed in four major \nsteps: 1. Use the criteria above to find the candidates among all packet accesses in an aggregate. 2. \nCompute dominator and post-dominator graph. Packet accesses to be combined must satisfy the dominance \nrelationship (e.g. only dominated reads may be combined). 3. Combined packet accesses must not violate \nany data dependencies. A data-flow analysis identifies any dependencies between protocol field accesses. \nIn this analysis, a read access is considered a use, and a write access is considered a definition. Two \nread accesses can be combined if there is no intervening definition to the first field before the second \nread access. Two write accesses can be combined if there is no intervening use of the first field before \nthe second write access. 4. Combine the packet accesses and eliminate the redundant ones. The remaining \npacket reads and writes are updated with new memory access offsets and sizes. The removed packet access \nlocations now read temporaries containing the pre-loaded packet data or write temporaries that buffer \ndata to be written out the packet.  5.3.2 Static offset and alignment resolution (SOAR) Statically \ndetermining packet access offsets is almost as important to performance as packet access combining. In \nnetwork applications, the location and alignment of a given protocol s field is application-context specific. \nConsider the MPLS over Ethernet packet [28] shown in Figure 9. These packets can have an arbitrary number \nof MPLS headers attached to the payload (e.g. IPv4 header and data). Consequentially, in applications \nthat process MPLS packets, the locations of the MPLS and IPv4 protocol fields relative to the start of \nthe packet cannot be determined statically. When offsets of protocol fields are not static, the alignment \nof fields may also be application dependent. Many processor architectures, including IXP, can only perform \nword-aligned memory accesses. MPLS over ipv4 Ethernet payload packet 14B 4B 4B 20B IPv4 over ipv4 \nEthernet payload packet 14B 20B Figure 9 Illustrates when the offset and alignment of packet fields \ncan (normal IPv4) and cannot (MPLS) be resolved statically. Handling both unknown field offsets and alignments \ndynamically at runtime adds significant overheads to packet access primitives. While static alignment \nresolution can remove only a few instructions, more than half of the 40+ instructions in a packet data \naccess can be removed with static offset resolution. Fortunately, static offsets and alignments can be \ndetermined in many instances, but they can only be determined by analyzing the entire packet processing \napplication. -offset 0123 n  -offset Figure 10 Lattice for static offset determination (SOD). We \ndeveloped a full-program analysis to determine, when possible, static protocol field offsets and alignments \nfor packet accesses in a given packet application. Static offset and alignment resolution (SOAR) is performed \nin eight major steps. The purpose of the analysis is to statically determine the value of the head_ptr \n(see Figure 3) at all packet access locations: 1. Identify all packet encapsulation (e.g. packet_encap() \nand packet_decap()), packet data accesses (e.g. ph\u00ad>protocol_field) and packet_handle assignments in \nthe application. 2. Initialize lattice values for static offset determination (SOD). SOD lattice values, \nshown in Figure 10, correspond to current protocol offset (c_offset) of a live packet_handle relative \nto the initial head_ptr:  At packet_handles entering the receive module (Rx), c_offset . 0 At all other \nprogram locations, c_offset . - offset 3. Perform global (inter-procedural and intra-procedural) forward \nflow analysis of lattice values for SOD. Computed values for c_offset should be recorded at all packet \naccess program points. The monotonic flow function is described below: At packet_encap(), c_offsetout \n. c_offsetin BIT_OFFSET( packet_encap() ) At packet_decap(), c_offsetout . c_offsetin + BIT_OFFSET( packet_decap() \n) At control flow joins, c_offsetout . n if all c_offsetin(i) = n | - offset . -offset otherwise 4. \nPerform global backward flow analysis of lattice values using the previous flow function, but only apply \nanalysis at program points where c_offset = - offset. Updated values for c_offset should be recorded \nat all packet access program points. This backward path is used to propagate static offsets to packets \nnot entering via Rx (e.g. at packet_create() or packet_copy()). -alignment quadword doubleword word \n short byte -alignment Figure 11 Lattice for static alignment determination (SAD). 5. Initialize lattice \nvalues for static alignment determination (SAD). Lattice values correspond to current protocol alignment \n(c_alignment) of a live packet_handle relative to the head_ptr: At packet_handles entering the receive \nmodule (Rx), c_alignment . quadword At all other program locations, c_alignment . - alignment 6. Perform \nglobal forward flow analysis of lattice values for SAD. Computed values for c_alignment should be recorded \nat all packet access program points. The monotonic flow function is described below: At a packet_encap(), \nc_alignmentout . ALIGNMENT( c_alignmentin -BIT_OFFSET( packet_encap() ) ) At a packet_decap(), c_alignmentout \n. ALIGNMENT( c_alignmentin + BIT_OFFSET( packet_decap() ) ) At control flow joins, c_alignmentout . \na if all c_alignmentin(i) = a | - alignment . MIN_ALIGNMENT( all c_alignmentin(i) ) otherwise 7. Perform \nglobal backward flow analysis of lattice values using the previous flow function, but only apply analysis \nwhere c_alignment = - alignment. Updated values for c_alignment should be recorded at all packet access \nprogram points. This backward path is used to propagate static offsets to packets not entering via Rx \n(e.g. at packet_create() or packet_copy()). 8. The results of these two dataflow analyses can be used \nto optimize packet accesses and packet encapsulation in the generated code:  For all packet field accesses \nwith a statically resolved constant offset (c_offset != -offset), an optimized packet access sequence \nfor a fixed offset can be used in place of a one that must handle unknown offsets: OFFSET( field ) . \nc_offset + OFFSET_IN_PROTOCOL( field ) For all packet field accesses with a statically resolved constant \nalignment (c_alignment != -alignment) and an unknown offset (c_offset = -offset), an optimized packet \naccess sequence for a fixed alignment can be used in place of one that must handle unknown offsets and \nalignments: ALIGNMENT( field ) . ALIGNMENT( c_alignment + PROTOCOL _ALIGNMENT ( field ) ) For all packet \nencapsulations (packet_encap() and packet_decap()) with a statically resolved constant offset, code does \nnot need to be generated to update the head_ptr relative to the size of the current encapsulation. Prior \nto join points where a static offset cannot be resolved (c_offset = -offset), code must be inserted that \nupdates the value of head_ptrto reflect its current offset. Applying this transformation eliminates instructions \nand memory accesses resulting from unnecessary updates of head_ptr. 5.3.3 Eliminating packet access \nprimitives In this section, we describe situations where program analysis can be used to identify packet \naccess primitives from a compiled source application that can be completely eliminated in the generated \ncode. The metadata construct is useful for packet processing because it allows state to be attached to \na packet as it flows through different PPFs and modules. For example, in Figure 1, the l3_fwdr module \ncan attach a next hop ID to a packet, which the eth_encap module uses to find the correct Ethernet header \ninformation to encapsulate the packet with. Performance may suffer, though, if metadata accesses are \nalways converted into actual reads and writes of metadata stored into SRAM (see Figure 3). Writes to \nSRAM are only necessary if the metadata field might be accessed by another ME. In many cases, after aggregation \nand extensive inlining, a given metadata field may only be live within one PPF or aggregate. In this \ncase, the metadata access can be simply treated as a local variable. packet_encap() and packet_decap() \nallow arbitrary layering of packet protocols and allow modular packet applications to be written independent \nof how it may be encapsulated within another application. For example, IPv4 applications can be written \nto run on Ethernet or to run on any other physical layer protocol. The encapsulation functions update \nthe current head_ptr (stored in the packet SRAM metadata) to reflect data prepended to a packet. These \nencapsulation primitives add memory and instruction overheads. Full support of these primitives is required \nto enable handling and layering of arbitrary protocols (like the MPLS application where an arbitrary \nnumber of MPLS packet headers can be prepended), but compiler analysis can identify instances when code \ngenerated for the primitive can be completely omitted: Function A (3 words) Function B (10 words) Function \nC (6 words)  . packet_encap() and packet_decap() can be eliminated in conjunction with results of the \nSOAR analysis. If a value of head_ptr has been statically determined at a packet_encap() and packet_decap() \nlocation, the head_ptr to the current protocol does not need to be maintained and these primitives do \nnot need to be represented at all in the code. . Paired encapsulation calls (packet_encap() . packet_decap() \nor packet_decap() . packet_encap()) between two protocols can be eliminated if they are paired for every \npath between them and are called within the same aggregate. In this case, the net result relative to \nother aggregates is that the head_ptr remains unchanged.  5.4 Stack layout optimization Implementing \na normal program stack is not straightforward due to the Intel IXP s partitioned memory hierarchy and \nexplicit memory instructions for accessing each memory level. Local program variables and spilled register \ntemporaries are traditionally stored in a frame of the program stack. Since Baker does not support recursion \nand a static call graph can be constructed at compile time, program stack locations could easily be assigned \nstatically to different memory locations. In Shangri-La, though, the primary goal of stack layout optimization \nis to allocate as many stack frames as possible to the limited amount of fast memory. Only 48 words of \nLocal Memory are available to each of the eight threads for stack memory (the remaining memory is reserved \nfor other uses like software\u00adcontrolled caching). To accommodate programs with larger stacks, the stack \ncan grow into SRAM, but its high latency and the consumed bus bandwidth would significantly impact performance \nif used extensively for the program stack. Since explicit instructions access each level of memory, the \ncompiler can, for every stack access, either generate instructions and associated control to store in \nboth types of memory, or statically assign it to only one memory level. Since any control overhead would \nadd a significant number of dynamic instructions for every stack access, we opted for the later solution. \nIn Shangri-La, an aggregate s dispatch loop calls PPFs (procedures in this discussion) that have packets \narriving on its input CCs (the procedure s inputs), resulting in a very flat call graph. Given this runtime \nmodel, we expect top level procedures in the call graph to be executed most frequently. Hence, the basic \nstack allocation strategy is to assign Local Memory to procedures higher in the program call graph and \nassign SRAM memory when Local Memory has been completely exhausted. If a procedure is called from more \nthan one place, its call stack is assigned to the minimum stack location (in Local Memory or SRAM) that \nwill never collide with possibly live stack entries, depending on where it is called from. Our experiments \nso far suggest stack locations in SRAM can significantly degrade performance. In initial implementations, \nthe L3-Switch application included over 100 dynamic SRAM accesses per packet that came from the stack. \nAlthough stack space in Local Memory is small, the call stack in this application also did not exceed \n5 frames. It was soon discovered that stack accesses were being mapped to SRAM because the Local Memory \nstack locations were poorly utilized. One problem was that initially, to easily accommodate the IXP s \noffset addressing mode, the stack has a minimum 64B (16 words) frame size. On the IXP, stack entries \ncan be accessed in the same cycle only by using offset-based addressing. In offset-based addressing, \nthe address pointer used to access Local Memory must to be aligned so that the offset can simply be OR \ned to the address pointer (e.g. $SP[3] is equivalent to *($SP | (3 << 2)). An improved stack layout was \nimplemented that eliminated this minimum stack frame size. Here, the compiler maintains two stack pointers, \nthe physical ($pSP) and virtual ($vSP) stack pointers, as shown in Figure 12. The physical stack pointer \nis always properly aligned and the virtual one is sized to the procedure s required minimum. In the final \ncode, only the physical stack pointer is generated, but the virtual stack pointer is used to calculate \nthe correct offset for a stack access relative to the physical stack pointer. We also confirmed that \naggressive inlining improved utilization of the stack. Merging stack frames together eliminates frame \nboundaries and stack slots reserved for call actual parameters. It also increases global optimization \nopportunities, which decreases the number of stack slots reserved for temporaries. Both frame size optimizations \nand aggressive inlining are essential for keeping the runtime stack completely in Local Memory and for \nachieving good performance on larger network applications. Call graph Stack layout 16-word aligned stack \n 32 24 16 8 0  6. Experimental Results To evaluate code generated from the Shangri-La compiler, we \nperformed experiments on an Intel IXP2400 evaluation board with 8MB SRAM, 64MB DRAM and three 1Gbps optical \nports. An IXIA packet generator with three 1Gbps optical ports (to support a maximum of 3Gbps throughput) \nwas used to transmit packets and collect statistics. 6.1 Benchmark applications Three network applications \nwritten in Baker were evaluated, L3-Switch (3126 lines), Firewall (2784 lines) and MPLS (4331 lines). \nL3-Switch [27] bridges and routes IP packets. The critical path for L3-Switch is the route lookup for \nthe next hop router ID. Next hop IDs are found by traversing a tree data structure (often called a trie) \nto match the longest matching string of bits from the most significant bits of the destination IP address \nand retrieving the next hop ID associated with that match. Firewall sits between an internal network \nand an external network and prevents selected packets from passing. A classifier attaches flow IDs to \npackets by matching several packet fields (e.g. source and destination IPs, source and destination ports, \nprotocol and type of service (TOS)) to an ordered list of user-defined patterns. Selected flow IDs are \nthen dropped by the Firewall. Multiprotocol Label Switching (MPLS) [28] routes according to labels, instead \nof destination IPs, attached to packets entering the domain. Routing with labels reduces hardware requirements \nfor routing and facilitates high-level traffic control that cannot be achieved by per-hop IP routing. \nL3-Switch and MPLS were evaluated using NPF packet traces [27][28]. We developed our own packet traces \nfor evaluating Firewall. 6.2 Performance Evaluation Packet forwarding rates and dynamic memory accesses \nfor each application were collected as optimizations were successively enabled. Optimization ordering \nwas done in a way to highlight each optimization, since the benefits of some of the optimizations depend \non each other. All optimizations are disabled in the BASE configuration, -O1 adds typical scalar optimizations, \n-O2 inlines base packet handling routines, PAC enables packet access combining, SOAR enables static offset \nand alignment resolution, PHR removes unnecessary packet handling support code and SWC enables software-controlled \ncaching. Table 1 Dynamic memory accesses per packet. P acket A pplicatio n Scratch SRAM DRAM Scratch \nSRAM Total L3-Switch + SWC 2.0 3.0 2.0 0.0 8.0 15.0 + PHR 2.0 3.0 2.0 1.0 11.0 19.0 + PAC 2.0 9.0 3.0 \n1.0 11.0 26.0 + -O1 2.0 35.0 29.0 1.0 12.0 79.0 BASE 2.0 35.0 29.0 1.0 22.0 89.0 Firewall + SWC 2.0 1.0 \n1.0 0.3 14.0 18.3 + PHR 2.0 1.0 1.0 0.3 14.0 18.3 + PAC 2.0 5.0 1.0 0.3 14.0 22.3 + -O1 2.0 40.6 25.6 \n0.3 30.8 99.3 BASE 2.0 40.6 25.6 0.6 32.5 101.3 MP LS + SWC 2.0 7.0 2.0 0.0 5.0 16.0 + PHR 2.0 7.0 2.0 \n2.0 9.0 22.0 + PAC 2.0 14.0 3.0 2.0 8.0 29.0 + -O1 2.0 23.0 16.0 2.0 9.0 52.0 BASE 2.0 23.0 16.0 2.0 \n14.0 57.0 For all configurations except three, the program s entire critical packet pipeline was mapped \nto one ME and then replicated up to five times on the other MEs. The MPLS O1 pipeline and the L3-Switch \nand MPLS BASE pipelines had to be mapped to two MEs due to ME code size constraints. This pipeline was \nthen replicated two more times on the remaining four MEs. In the most optimized case, we have been unable \nso far to map the critical path of the benchmark applications to more than one ME. This is due in part \nto the fact that today, network forwarding applications are still designed to be simple and have short \ncritical paths so that they can handle high packets rates. Forwarding rate (Gbps) Forwarding rate (Gbps) \n Forwarding rate (Gbps) 3.0 2.5 2.0 1.5 1.0 0.5 0.0 + SWC  + PHR  + SOAR  + PAC + -O2 + -O1 BASE \n Figure 13 Packet forwarding rates for L3-Switch. 3.0 2.5 2.0 1.5 1.0 0.5 0.0 + SWC  + PHR  + SOAR \n + PAC + -O2 + -O1 BASE 3.0 2.5 2.0 1.5 1.0 0.5 0.0 MEs 123456 + SWC  + PHR  + SOAR  + PAC \n + -O2 + -O1 BASE Figure 15 Packet forwarding rates for MPLS. Table 1 shows the average per-packet \ndynamic memory accesses for each application as relevant optimizations are enabled (-O2 and SOAR only \naffect dynamic instruction counts and have no effect on memory access counts). The effects of stack layout \noptimization described in Section 5.4 are already included in these reported numbers. Without stack layout \noptimization, even simple programs would generate too many SRAM accesses to achieve respectable packet \nforwarding rates. The significant impact of PAC is evident in this table from the large reduction in \npacket handling SRAM and DRAM accesses. In the case of Firewall, PAC even aids the scalar optimizer by \nexposing additional opportunities to eliminate application SRAM accesses. Figures 13-15 display packet \nforwarding rates on minimum sized 64B packets. Each curve represents the effect of successive optimizations \nfor one to six MEs enabled. Code generated by Shangri-La for all three applications have successfully \nachieved 100% forwarding rates at 2.5Gbps, which is what the IXP2400 designed for and is the same throughput \ntarget achieved by hand\u00adcoded assembly versions of the applications written specifically for the processors. \nThe figures further show that PAC improved packet forwarding the most. While PAC eliminates instructions, \nits largest effect is on reducing DRAM and SRAM accesses. As Figure 6 suggested earlier, having more \nthan just a few DRAM accesses limits the theoretical forwarding rate of the system by saturating the \nmemory bandwidth. This saturation is evidenced by the non-linear increase and flattening in forwarding \nrates with increasing numbers of MEs enabled for each of the optimization curves. Without memory bandwidth \neffects, forwarding rates should always increase linearly with more MEs enabled and eliminated instructions \nshould always improve performance with a constant proportion with more MEs enabled. In the BASE, -O1 \nand O2 configurations, forwarding rate flattening occurs with fewer MEs because there are more memory \naccesses per packet. The PAC, SOAR, PHR and SWC configurations generate fewer memory accesses per packet \nand only saturate memory bandwidth with at least four MEs enabled. -O1 and SOAR are important to reducing \nper-packet instruction counts. Reduced instruction count is important because its effect is multiplied \nas more MEs are enabled. O1 optimizations enable MPLS s and L3-Switch s critical path to fit on one ME \ninstead of two. SOAR significantly improves forwarding rates on L3-Switch and MPLS. Instruction count \nreductions can be best seen with only one or two MEs enabled on the L3-Switch and MPLS applications. \nAt these points, memory bandwidth limits are not being hit and improvements in forwarding rates are purely \ndue to instruction count reductions. The effects of O2, PHR and SWC appear to have limited effects on \napplication forwarding rates, but PHR s and SWC s ability to reduce dynamic SRAM and Scratch Memory accesses \nare clearly evident in Table 1. SWC successfully caches two small frequently\u00adaccessed data structures \nin L3-Switch and MPLS. Our experiments suggest only a rough relationship between the number of memory \naccesses and the IXP2400 s maximum achievable packet forwarding rates. For example, Figure 6 showed that \nthe hardware could achieve 2.5Gbps only if there was 1 DRAM access. Both L3-Switch and Firewall achieve \napproximately 2.7Gbps, and MPLS achieves 3Gbps, even though all these applications have approximately \nthe same number of DRAM and SRAM accesses in the most optimized configuration. These inconsistencies \nsuggest that although there is a clear trend between memory accesses and achievable packet forwarding \nrates, there are also important secondary factors. One secondary factor is the impact of the memory access \nwidth on packet forwarding rates, shown earlier in Figure 6. MPLS probably achieves higher packet forwarding \nrates because it issues narrower memory accesses to DRAM than the other two applications (24B vs. 40B). \nAnother possible factor for the discrepancy is the balance between computation and memory accesses. For \nexample, the experiment in Figure 6 had almost no computation, but achieved lower packet forwarding rates \nthan real applications. In this case, the amount of computation and memory access overlap between threads \non the same ME may be reduced because all the threads are waiting on memory.  7. Related Work Click \n[20] is the most relevant and established academic C++ programming model and environment for building \npacket processing applications on a single, general-purpose, processor. Baker bears many similarities \nto Click, especially in regards to its modeling of communication channels (CCs). The original Click project \nfocused more on the language design than performance: they used a standard C++ compiler and were targeting \na general-purpose uniprocessor. Due to architectural and technology differences, it is difficult to make \nany performance comparison between our system and theirs. Kohler, Morris and Chen [21] later described \na source\u00adto-source tool for optimizing Click module configurations. Most of the optimizations they implemented \nto eliminate modular inefficiencies in a L3-Switch resembled traditional scalar compiler optimizations. \nThe click-align optimizer addressed similar packet data alignment issues faced by our system. Additional \nwork has also been done by other researchers to extend the performance of Click. NP-Click [31] was a \nproject to implement Click on the Intel IXP by replacing code in Click modules with ME instructions. \nThis modularization resulted in a 35% reduction of the packet forwarding rate on minimum sized 64B packets \ncompared to a hand-coded implementation. SMP Click extended the Click runtime system to run an unmodified \nClick configuration on a SMP [5]. There has been a lot of research recently specifically on programming \nthe IXP, although it has mostly focused on low-level compilation issues. George and Blume [12] developed \na network programming framework and a network application language, Nova, but their language is less \nambitious and the compiler has mostly focused on scalar optimizations for the IXP. Li and Gupta [24] \ndeveloped an algorithm that lays out local variables based on access patterns to take better advantage \nof autoincrement/autoincrement addressing modes available on the IXP. Zhuang and Pande [38] described \nthree different approaches for resolving ME register bank conflicts during register allocation. In a \nlater paper [37], they described how to share registers across threads in a ME to make better use of \navailable architectural registers. Kim et al. [22] described a retargetable compiler infrastructure for \nnetwork processors, but their target processor was the Paion PPII. Much like our work, they concluded \nthat aggressive reduction of memory accesses is critical in packet processors that do not have caches. \nIn addition to assemblers, Intel currently has a product toolkit for developing network applications \nin a C-like language [18]. A newer version of the toolkit is also being developed that supports an auto-partitioning \nmode that can automatically construct pipeline stages from a program [10]. This compiler achieves similar \noptimization goals, but assumes a different starting point for the programmer. While the Shangri-La compiler \nencourages programmers to write small PPFs, which are merged or duplicated by the compiler, they assume \nprogrammers write large procedures that are partitioned into stages by the compiler. Both of these commercial \ncompilers [10][18] remove scheduling and register allocation challenges of programming in assembly, but \nmapping data to memory levels, managing threads and accessing specialized hardware (e.g. hardware queues \nand CAM) are still the programmer s responsibility. Non-inlined function calls are converted into branches \nand then registers are globally allocated. Automatic spilling of live registers is supported, but only \nto one level of memory specified by the programmer [17]. There are a few publications worth mentioning \ndescribing work relating to the optimizations highlighted in this paper. Udayakumaran and Barua [35] \nalso proposed a form of software\u00adcontrolled caching. In their scheme, the software-controlled cache is \nused to store register spills to the program stack and prevent it from polluting the hardware data cache. \nBecause our scheme selectively caches global data, it requires a more complex scheme to identify good \ncaching candidates and selectively generate caching code. Additionally, our software-controlled cache \nalso implements a delayed-update coherence mechanism. In comparison, they can completely ignore coherency \nbecause they only cache a thread s private stack. Davidson and Jinturkar [9] described a memory coalescing \nalgorithm for general purpose processors similar to our packet access combining (PAC). This algorithm \nreplaced narrow array access with doubleword accesses in unrolled loops. Memory coalescing implemented \nextensive profitability checks to factor the realignment costs and limited packing width. Packet access \ncombining is almost always profitable given the high cost of DRAM access on the IXP. Both algorithms \nperform similar scalar safety checks, but memory coalescing must also handle potential array aliasing. \nGupta, Mehofer and Zhang [14], and Stephenson, Babb and Amarasinghe [33] described frameworks for bit-level \nanalysis and optimization that may be useful for analyzing network packet accesses, but neither of these \nworks describe any ideas that bear any resemblance to our optimizations of packet access primitives. \nFinally, Avissar, Barua and Stewart [2] discussed techniques for mapping a program stack to heterogeneous \nmemories. Our work is similar to their work in that both have static, not dynamic, mappings to memory \nlevels. In their approach, both global and stack memories are allocated by solving a large linear programming \nsystem that incorporates profiling. We also use profiling data for mapping global data structures, but \nwe allocate stack memory and global data separately, and our stack allocation strategy primarily relies \non the static program call graph. Avissar, Barua and Stewart s work also does not need to deal with the \ncomplexities of stack frame alignment. 8. Conclusions This paper addresses the challenges of achieving \nhand-tuned performance on highly resource-constrained network processors on code compiled from high-level \nlanguages. We presented a complete framework for aggressively compiling network programs using both traditional \nand specialized optimizations techniques to aggressively reduce both instruction and memory access counts. \nDetailed performance evaluations of compiler generated code of three popular network applications on \nreal hardware show the importance of these optimization techniques in achieving 100% packet forwarding \nrates at 2.5Gbps. For future work, we will continue with efforts to improve compiled program performance \nand to try more network applications on our system. We are also considering if some of the highlighted \noptimizations can be applied to deal with the difficulties of compiling for future general-purpose chip \nmultiprocessors with heterogeneous cores and memories. 9. Acknowledgements This work would not be possible \nwithout significant contributions from Erik Johnson, Aaron Kunze, Steve Goglin, Vinod Balakrishnan, Arun \nRaghunath and Robert Odell at Intel Communications Technology Lab (CTL); Institute of Computing Technology \n(ICT) at Chinese Academy of Science; Prof. Harrick Vin and his research group at UT Austin; and our intern \nAstrid Wang. 10. References [1] Amaral, J.N., Gao, G.R., Dehnert, J. and Towle, R. The SGI Pro64 Compiler \nInfrastructure: A Tutorial. In PACT 00, Philadelphia, PA, October 2000. [2] Avissar, O., Barua, R. and \nStewart., D. An optimal memory allocation scheme for scratch-pad-based embedded systems. In ACT Transactions \non Embedded Computing Systems (TECS), 1(1) pp. 6-26, November 2002. [3] Baer, J.L., Low, D., Crowley, \nP. and Sidhwaney, N. Memory Hierarchy Design for a Multiprocessor Look-up Engine. In PACT 03, New Orleans, \nLA, September 2003. [4] Broadcom Corporation. The Sibyte BCM1250 Processor. http://sibyte.broadcom.com/public/index.html \n[5] Chen, B. and Morris, R. Flexible Control of Parallelism in a Multiprocessor PC Router. In USENIX \n2001 Annual Technical Conference, Boston, MA, June 2001. [6] Chiueh, T. and Pradhan, P. High-performance \nIP routing table lookup using CPU caching. In IEEE Infocom 99, New York, NY, March 1999. [7] Chow, F., \nChan, S., Kennedy, R., Liu, S.M., Lo, R. and Tu, P. A new algorithm for partial redundancy elimination \nbased on SSA form. In PLDI 97, Las Vegas, NV, June 1997. [8] Cooper, K. and Harvey, T. Compiler-Controlled \nMemory. In ASPLOS-VIII, San Jose, CA, October 1998. [9] Davidson, J. and Jinturkar, S. Memory Access \nCoalescing: A Technique for Eliminating Redundant Memory Accesses. In PLDI 94, Orlando, FL, June 1994. \n[10] Dai, J., Huang, B., Li, L. and Harrison, L. Automatically Partitioning Packet Processing Applications \nfor Pipelined Architectures. To appear in PLDI 05, Chicago, IL, June 2005. [11] Diwan, A., McKinley, \nK. and Moss, E. Type-Based Alias Analysis. In PLDI 98, Montreal, Canada, June 1998. [12] George, L. and \nBlume, M. Taming the IXP Network Processor. In PLDI 03, San Diego, CA, June 2003. [13] Goglin, S., Johnson, \nE.J. and Vin, H. Baker: A Packet Processing Programming Language for Highly Concurrent Hardware. Under \npreparation for submission. [14] Gupta, R., Mehofer, E. and Zhang, Y. A Representation for Bit Section \nbased Analysis and Optimization. In International Conference on Compiler Construction, Grenoble, France, \nApril 2002. [15] IBM. The PowerNP architecture. http://www.hifn.com/products/5np4g.html. [16] Intel Corporation. \nIntel IXP2400 Network Processor: Hardware Reference Manual. October 2002. [17] Intel Corporation. Microengine \nVersion 2 (MEv2): Microengine C Compiler Coding Considerations. June 2003. [18] Johnson, E.J. and Kunze, \nA. IXP2400/2800 Programming: The Complete Microengine Coding Guide. Intel Press, Hillsboro, OR, April \n2003. [19] Ju, R., Chan, S. and Wu, Chengyong. Open Research Compiler for Itanium Processor Family. Tutorial \nin MICRO\u00ad34, Austin, TX, December 2001. [20] Kohler, E., Morris, R., Chen, B., Jannotti, J. and Kaashoek, \nM.F. The Click Modular Router. In ACM TCS, 18(3) pp. 263\u00ad297, August 2000. [21] Kohler, E., Morris, R. \nand Chen, B. Programming language optimizations for modular router configurations. In ASPLOS-X, San Jose, \nCA October 2002. [22] Kim, J., Jung, S. and Park, Y. Experiences with a Retargetable Compiler for a Commercial \nNetwork Processor. In CASES 02, Grenoble, France, October 2003. [23] Kulkarni, C., Gries, M., Sauer, \nC. and Keutzer, K. Programming Challenges in Network Processor Deployment. In CASES 03, San Jose, CA, \nOctober 2003. [24] Li, B. and Gupta, R. Simple Offset Assignment in Presence of Subword Data. In CASES \n03, San Jose, CA, October 2003. [25] Narlikar, G. and Zane, F. Performance Modeling for Fast IP Lookups. \nIn SIGMETRICS 01, Cambridge, MA, June 2001. [26] Intel Corporation. Microengine Version 2 (MEv2): Microengine \nC Compiler Coding Considerations. June 2003. [27] Network Processing Forum. IP Forwarding Application \nLevel Benchmark. http://www.npforum.org/techinfo/ipforwarding_bm.pdf [28] Network Processing Forum. MPLS \nForwarding Application Level Benchmark and Annex. http://www.npforum.org/techinfo/MPLSBenchmark.pdf [29] \nPMC-Sierra. MIPS-based Processors. http://pmc-sierra.com/processors/ [30] Rosen, E., Viswanathan, A. \nand Callon, R. RFC 3031 Multiprotocol Label Switching Architecture. IETF, January 2001. [31] Shah, N., \nPlishker, W. and Keutzer, K. NP-Click: A Programming Model for the Intel IXP1200. In 2nd Workshop on \nNetwork Processors (NP-2), Anaheim, CA, February 2003. [32] Shah, N., Plishker, W. and Keutzer, K. Comparing \nNetwork Processor Programming Environments: A Case Study. In 2004 Workshop on Productivity and Performance \nin High-End Computing (P-PHEC), HPCA-10, Madrid, Spain, February 2004. [33] Stephenson, M., Babb, J. \nand Amarasinghe, S. Bitwidth Analysis with Application to Silicon Compilation. In PLDI 00, Vancouver, \nBC, June 2000. [34] W.R. Stevens. TCP/IP Illustrated, Volume 1: The Protocols. Addison-Wesley, Boston, \nMA, 1994. [35] Udayakumaran, S. and Barua, R. Compiler-Decided Dynamic Memory Allocation for Scratch-Pad \nBased Embedded Systems. In CASES 03, San Jose, CA, October 2003. [36] Vin, H., Mudigonda, J., Jason, \nJ., Johnson, E.J., Ju, R., Kunze, A. and Lian, R. A Programming Environment for Packet\u00adprocessing Systems: \nDesign Considerations. In 3rd Workshop on Network Processors &#38; Applications, Madrid, Spain, February \n2004. [37] Zhuang, X. and Pande, S. Balancing Register Allocation Across Threads for a Multithreaded \nNetwork Processor. In PLDI 04, Washington, DC, June 2004. [38] Zhuang, X. and Pande, S. Resolving Register \nBank Conflicts for a Network Processor. In PLDI 03, New Orleans, LA, June 2004.   \n\t\t\t", "proc_id": "1065010", "abstract": "Programming network processors is challenging. To sustain high line rates, network processors have extremely tight memory access and instruction budgets. Achieving desired performance has traditionally required hand-coded assembly. Researchers have recently proposed high-level programming languages for packet processing, but the challenges of compiling these languages into code that is competitive with hand-tuned assembly remain unanswered.This paper describes the Shangri-La compiler, which accepts a packet program written in a C-like high-level language and applies scalar and specialized optimizations to generate a highly optimized binary. Hot code paths identified by profiling are mapped across processing elements to maximize processor utilization. Since our compilation target has no hardware caches, software-controlled caches are generated for frequently accessed application data structures. Packet handling optimizations significantly reduce per-packet memory access and instruction counts. Finally, a custom stack model maps stack frames to the fastest levels of the target processor's heterogeneous memory hierarchy.Binaries generated by the compiler were evaluated on the Intel IXP2400 network processor with eight packet processing cores and eight threads per core. Our results show the importance of both traditional and specialized optimization techniques for achieving the maximum forwarding rates on three network applications, <i>L3-Switch, MPLS</i> and <i>Firewall</i>.", "authors": [{"name": "Michael K. Chen", "author_profile_id": "81100109668", "affiliation": "Intel Corporation, Santa Clara, CA", "person_id": "PP14048508", "email_address": "", "orcid_id": ""}, {"name": "Xiao Feng Li", "author_profile_id": "81388599257", "affiliation": "Intel China Research Center Ltd., Beijing, China", "person_id": "PP18001864", "email_address": "", "orcid_id": ""}, {"name": "Ruiqi Lian", "author_profile_id": "81377591007", "affiliation": "China Academy of Sciences, Beijing, China", "person_id": "P511529", "email_address": "", "orcid_id": ""}, {"name": "Jason H. Lin", "author_profile_id": "81451598843", "affiliation": "Intel China Research Center Ltd., Beijing, China", "person_id": "P728833", "email_address": "", "orcid_id": ""}, {"name": "Lixia Liu", "author_profile_id": "81546732556", "affiliation": "Intel China Research Center Ltd., Beijing, China", "person_id": "PP95042886", "email_address": "", "orcid_id": ""}, {"name": "Tao Liu", "author_profile_id": "81452600026", "affiliation": "China Academy of Sciences, Beijing, China", "person_id": "PP14182399", "email_address": "", "orcid_id": ""}, {"name": "Roy Ju", "author_profile_id": "81451594942", "affiliation": "Intel Corporation, Santa Clara, CA", "person_id": "P520418", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065038", "year": "2005", "article_id": "1065038", "conference": "PLDI", "title": "Shangri-La: achieving high performance from compiled network applications while enabling ease of programming", "url": "http://dl.acm.org/citation.cfm?id=1065038"}