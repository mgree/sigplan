{"article_publication_date": "06-12-2005", "fulltext": "\n Scalable Statistical Bug Isolation Ben Liblit Mayur Naik Alice X. Zheng Computer Sciences Department \nComputer Science Department Department of Electrical Engineering University of Wisconsin-Madison Stanford \nUniversity and Computer Science <liblit@cs.wisc.edu> <mhn@cs.stanford.edu> University of California, \nBerkeley <alicez@cs.berkeley.edu> Alex Aiken Michael I. Jordan Computer Science Department Department \nof Electrical Engineering Stanford University and Computer Science Department of Statistics University \nof California, Berkeley <aiken@cs.stanford.edu>  <jordan@cs.berkeley.edu>  Abstract We present a statistical \ndebugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical \nalgorithms that focus solely on identifying predictors that corre\u00adlate with program failure perform poorly \nwhen there are multiple bugs. Our new technique separates the effects of different bugs and identi.es \npredictors that are associated with individual bugs. These predictors reveal both the circumstances under \nwhich bugs occur as well as the frequencies of failure modes, making it easier to pri\u00adoritize debugging \nefforts. Our algorithm is validated using several case studies, including examples in which the algorithm \nidenti.ed previously unknown, signi.cant crashing bugs in widely used sys\u00adtems. Categories and Subject \nDescriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation statistical methods; D.2.5 [Software \nEngineering]: Testing and Debugging debugging aids, distributed debugging, monitors, tracing; I.5.2 [Pattern \nRecogni\u00adtion]: Design Methodology feature evaluation and selection General Terms Experimentation, Reliability \nKeywords bug isolation, random sampling, invariants, feature se\u00adlection, statistical debugging * This \nresearch was supported in part by NASA Grant No. NAG2-1210; NSF Grant Nos. EIA-9802069, CCR-0085949, \nACI-9619020, and ENG\u00ad0412995; DOE Prime Contract No. W-7405-ENG-48 through Memoran\u00addum Agreement No. \nB504962 with LLNL; DARPA ARO-MURI ACCLI-MATE DAAD-19-02-1-0383; and a grant from Microsoft Research. \nThe information presented here does not necessarily re.ect the position or the policy of the Government \nand no of.cial endorsement should be inferred. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 05, June 12 15, 2005, Chicago, Illinois, USA. Copyright 2005 \nACM 1-59593-056-6/05/0006...$5.00. 1. Introduction This paper is about statistical debugging, a dynamic \nanalysis for identifying the causes of software failures (i.e., bugs). Instrumented programs monitor \ntheir own behavior and produce feedback re\u00adports. The instrumentation examines program behavior during \nex\u00adecution by sampling, so complete information is never available about any single run. However, monitoring \nis also lightweight and therefore practical to deploy in a production testing environment or to large \nuser communities, making it possible to gather informa\u00adtion about many runs. The collected data can then \nbe analyzed for interesting trends across all of the monitored executions. In our approach, instrumentation \nconsists of predicates tested at particular program points; we defer discussing which predicates are \nchosen for instrumentation to Section 2. A given program point may have many predicates that are sampled \nindependently during program execution when that program point is reached (i.e., each predicate associated \nwith a program point may or may not be tested each time the program point is reached). A feedback report \nR consists of one bit indicating whether a run of the program succeeded or failed, as well as a bit vector \nwith one bit for each predicate P.If P is observed to be true at least once during run R then R(P)=1, \notherwise R(P)=0. Let B denote a bug (i.e., something that causes incorrect behav\u00adior in a program). \nWe use B to denote a bug pro.le, i.e., a set of failing runs (feedback reports) that share B as the cause \nof failure. The union of all bug pro.les is exactly the set of failing runs, but note that Bi n Bj .0 \nin general; more than one bug can occur in/ = some runs. A predicate P is a bug predictor (or simply \na predictor)of bug B if whenever R(P)=1 then it is statistically likely that R . B (see Section 3.1). \nStatistical debugging selects a small subset S of the set of all instrumented predicates P such that \nS has predictors of all bugs. We also rank the predictors in S from the most to least important. The \nset S and associated metrics (see Section 4) are then available to engineers to help in .nding and .xing \nthe most serious bugs. In previous work, we focused on techniques for lightweight instrumentation and \nsampling of program executions, but we also studied two preliminary algorithms for statistical debugging \nand presented experimental results on medium-size applications with a single bug [10, 16]. The most general \ntechnique we studied is regularized logistic regression, a standard statistical procedure that tries \nto select a set of predicates that best predict the outcome of every run. As we worked to apply these \nmethods to much larger programs under realistic conditions, we discovered a number of serious scalability \nproblems: For large applications the set P numbers in the hundreds of thousands of predicates, many \nof which are, or are very nearly, logically redundant. In our experience, this redundancy in P causes \nregularized logistic regression to choose highly redun\u00addant lists of predictors S. Redundancy is already \nevident in prior work [10] but becomes a much more serious problem for larger programs.  A separate \ndif.culty is the prevalence of predicates predicting multiple bugs. For example, for many Unix programs \na bug is more likely to be encountered when many command line .ags are given, because the more options \nthat are given non-default settings the more likely unusual code paths are to be exercised. Thus, predicates \nimplying a long command line may rank near the top, even though such predicates are useless for isolating \nthe cause of individual bugs.  Finally, different bugs occur at rates that differ by orders of magnitude. \nIn reality, we do not know which failure is caused by which bug, so we are forced to lump all the bugs \ntogether and try to learn a binary classi.er. Thus, predictors for all but the most common bugs have \nrelatively little in.uence over the global optimum and tend to be ranked very low or not included in \nS at all.  These problems with regularized logistic regression persist in many variations we have investigated, \nbut analysis of this body of experimental work yielded some key technical insights. In addition to the \nbug predictors we wish to .nd among the instrumented predicates, there are several other kinds of predicates. \nFirst, nearly all predicates (often 98% or 99%) are not predictive of anything. These non-predictors \nare best identi.ed and discarded as quickly as possible. Among the remaining predicates that can predict \nfailure in some way, there are some bug predictors. There are also super-bug predictors: predicates that, \nas described above, predict failures due to a variety of bugs. And there are sub-bug predictors: predicates \nthat characterize a subset of the instances of a speci.c bug; these are often special cases of more general \nproblems. We give the concepts of super-and sub-bug predictors more precise technical treatment in Section \n3.3. The dif.culty in identifying the best bug predictors lies in not being misled by the sub-or super-bug \npredictors and not being overwhelmed by the sheer number of predicates to sift through. This paper makes \na number of contributions on these problems: We present a new algorithm for isolating multiple bugs \nin com\u00adplex applications (Section 3) that offers signi.cant improve\u00ad ments over previous work. It scales \nmuch more gracefully in all the dimensions discussed above and for each selected predicate P it naturally \nyields information that shows both how important (in number of explained program failures) and how accurate \na predictor P is.  We validate the algorithm by a variety of experiments. We show improved results for \npreviously reported experiments [10]. In a controlled experiment we show that the algorithm is able to \n.nd a number of known bugs in a complex application. Lastly, we use the algorithm to discover previously \nunknown serious crashing bugs in two large and widely used open source applications.  We show that relatively \nfew runs are suf.cient to isolate all of the bugs described in this paper, demonstrating that our  approach \nis feasible for in-house automatic testing as well as for deployment to end users (see Section 4.3). \n We report on the effectiveness of the current industry practice of collecting stack traces from failing \nruns. We .nd that across all of our experiments, in about half the cases the stack is useful in isolating \nthe cause of a bug; in the other half the stack contains essentially no information about the bug s cause. \n Finally, we show that, in principle, it is possible for our ap\u00adproach to help isolate any kind of failure, \nnot just program crashes. All that is required is a way to label each run as either successful or unsuccessful. \n With respect to this last point, perhaps the greatest strength of our system is its ability to automatically \nidentify the cause of many different kinds of bugs, including new classes of bugs that we did not anticipate \nin building the tool. By relying only on the distinction between good and bad executions, our analysis \ndoes not require a speci.cation of the program properties to be analyzed. Thus, statis\u00adtical debugging \nprovides a complementary approach to static anal\u00adyses, which generally do require speci.cation of the \nproperties to check. Statistical debugging can identify bugs beyond the reach of current static analysis \ntechniques and even new classes of bugs that may be amenable to static analysis if anyone thought to \ncheck for them. One of the bugs we found, in the RHYTHMBOX open source music player, provides a good \nillustration of the potential for posi\u00adtive interaction with static analysis. A strong predictor of failure \nde\u00adtected by our algorithm revealed a previously unrecognized unsafe usage pattern of a library s API. \nA simple syntactic static analysis subsequently showed more than one hundred instances of the same unsafe \npattern throughout RHYTHMBOX. The rest of the paper is organized as follows. After providing background \nin Section 2, we discuss our algorithm in Section 3. The experimental results are presented in Section \n4, including the advantages over our previous approach based on regularized logis\u00adtic regression. Section \n5 considers variations and extensions of the basic statistical debugging algorithm. We discuss related \nwork in Section 6 and offer our conclusions in Section 7. 2. Background This section describes ideas \nand terminology needed to present our algorithm. The ideal program monitoring system would gather complete \nexecution traces and provide them to an engineer (or, more likely, a tool) to mine for the causes of \nbugs. However, complete tracing of program behavior is simply impractical; no end user or tester would \naccept the required performance overhead or network bandwidth. Instead, we use a combination of sparse \nrandom sampling, which controls performance overhead, and client-side summariza\u00adtion of the data, which \nlimits storage and transmission costs. We brie.y discuss both aspects. Random sampling is added to a \nprogram via a source-to-source transformation. Our sampling transformation is general: any col\u00adlection \nof statements within (or added to) a program may be desig\u00adnated as an instrumentation site and thereby \nsampled instead of run unconditionally. That is, each time instrumentation code is reached, a coin .ip \ndecides whether the instrumentation is executed or not. Coin .ipping is simulated in a statistically \nfair manner equivalent to a Bernoulli process: each potential sample is taken or skipped randomly and \nindependently as the program runs. We have found that a sampling rate of 1/100 in most applications1 \nkeeps the perfor\u00admance overhead of instrumentation low, often unmeasurable. 1 Some compute-bound kernels \nare an exception; we currently sometimes resort to simply excluding the most performance critical code \nfrom instru\u00admentation. Orthogonal to the sampling transformation is the decision about what instrumentation \nto introduce and how to concisely summarize the resulting data. Useful instrumentation captures behaviors \nlikely to be of interest when hunting for bugs. At present our system offers the following instrumentation \nschemes for C programs: branches: At each conditional we track two predicates indicating whether the \ntrue or false branches were ever taken. This applies to if statements as well as implicit conditionals \nsuch as loop tests and short-circuiting logical operators. returns: In C, the sign of a function s return \nvalue is often used to signal success or failure. At each scalar-returning function call site, we track \nsix predicates: whether the returned value is ever < 0, = 0, > 0, = 0, =0, or . =0. scalar-pairs: Many \nbugs concern boundary issues in the relation\u00adship between a variable and another variable or constant. \nAt each scalar assignment x = ..., identify each same-typed in-scope variable yi and each constant expression \nc j. For each yi and each c j, we track six relationships to the new value of x: <, =,>,=,=,.=. Each \n(x,yi)or (x, c j)pair is treated as a distinct instrumentation site in general, a single assignment statement \nis associated with multiple distinct instrumentation sites. All predicates at an instrumentation site \nare sampled jointly. To be more precise, an observation is a single dynamic check of all predicates at \na single instrumentation site. We write P observed when the site containing P has been sampled, regardless \nof whether P was actually true or false. We write P observed to be true or merely P when the site containing \nP has been sampled and P was actually found to be true. For example, sampling a single negative return \nvalue means that all six returns predicates (< 0, = 0, > 0, = 0, =0, and =.0) are observed. However only \nthree of them (< 0, = 0, and . =0) are observed to be true. 2 These are natural properties to check and \nprovide good coverage of a program s scalar values and control .ow. This set is by no means complete, \nhowever; in particular, we believe it would be useful to have predicates on heap structures as well (see \nSection 4).  3. Cause Isolation Algorithm This section presents our algorithm for automatically isolating \nmul\u00adtiple bugs. As discussed in Section 2, the input is a set of feedback reports from individual program \nruns R, where R(P)=1 if predi\u00adcate P is observed to be true during the execution of R. The idea behind \nthe algorithm is to simulate the iterative manner in which human programmers typically .nd and .x bugs: \n1. Identify the most important bug B. 2. Fix B, and repeat.  For our purposes, identifying a bug B \nmeans selecting a predi\u00adcate P closely correlated with its bug pro.le B. The dif.culty is that we know \nthe set of runs that succeed and fail, but we do not know which set of failing runs corresponds to B,orevenhow \nmanybugs there are. In other words, we do not know the sizes or membership of the set of bug pro.les \n{Bi}. Thus, in the .rst step we must infer which predicates are most likely to correspond to individual \nbugs and rank those predicates in importance. For the second step, while we cannot literally .x the bug \ncorre\u00adsponding to the chosen predictor P, we can simulate what happens if the bug does not occur. We \ndiscard any run R such that R(P)=1 and recursively apply the entire algorithm to the remaining runs. \n2 In reality, we count the number of times P is observed to be true, but the analysis of the feedback \nreports only uses whether P is observed to be true at least once. Discarding all the runs where R(P)=1 \nreduces the importance of other predictors of B, allowing predicates that predict different bugs (i.e., \ncorresponding to different sets of failing runs) to rise to the top in subsequent iterations. 3.1 Increase \nScores We now discuss the .rst step: how to .nd the cause of the most important bug. We break this step \ninto two sub-steps. First, we eliminate predicates that have no predictive power at all; this typ\u00adically \nreduces the number of predicates we need to consider by two orders of magnitude (e.g., from hundreds \nof thousands to thou\u00adsands). Next, we rank the surviving predicates by importance (see Section 3.3). \nConsider the following C code fragment: f = ...; (a) if (f == NULL) { (b) x = 0; (c) *f; (d) } Consider \nthe predicate f == NULL at line (b), which would be captured by branches instrumentation. Clearly this \npredicate is highly correlated with failure; in fact, whenever it is true this program inevitably crashes.3 \nAn important observation, however, is that there is no one perfect predictor of failure in a program \nwith multiple bugs. Even a smoking gun such as f == NULLat line (b) has little or no predictive power \nfor failures due to unrelated bugs in the same program. The bug in the code fragment above is deterministic \nwith respect to f == NULL:if f == NULL is true at line (b), the program fails. In many cases it is impossible \nto observe the exact conditions causing failure; for example, buffer overrun bugs in a C program may \nor may not cause the program to crash depending on runtime system decisions about how data is laid out \nin memory. Such bugs are non-deterministic with respect to every observed predicate; even for the best \npredictor P, it is possible that P is true and still the program terminates normally. In the example \nabove, if we insert before line (d) a valid pointer assignment to f controlled by a conditional that \nis true at least occasionally (say via a call to read input) if (...) f = ... some valid pointer ...; \n *f; the bug becomes non-deterministic with respect to f == NULL. To summarize, even for a predicate \nP that is truly the cause of a bug, we can neither assume that when P is true that the program fails \nnor that when P is never observed to be true that the program succeeds. But we can express the probability \nthat P being true implies failure. Let Crash be an atomic predicate that is true for failing runs and \nfalse for successful runs. Let Pr(A|B)denote the conditional probability function of the event A given \nevent B.We want to compute: Failure(P)= Pr(Crash|P observed to be true) for every instrumented predicate \nP over the set of all runs. Let S(P) be the number of successful runs in which P is observed to be true, \nand let F(P)be the number of failing runs in which P is observed to be true. We estimate Failure(P)as: \nF(P)Failure(P)= S(P)+F(P) 3 We also note that this bug could be detected by a simple static analysis; \nthis example is meant to be concise rather than a signi.cant application of our techniques. Notice that \nFailure(P)is unaffected by the set of runs in which P is not observed to be true. Thus, if P is the cause \nof a bug, the causes of other independent bugs do not affect Failure(P). Also note that runs in which \nP is not observed at all (either because the line of code on which P is checked is not reached, or the \nline is reached but P is not sampled) have no effect on Failure(P). The de.nition of Failure(P) generalizes \nthe idea of deterministic and non-deterministic bugs. A bug is deterministic for P if Failure(P)= 1.0, \nor equivalently, P is never observed to be true in a successful run (S(P)=0) and P is observed to be \ntrue in at least one failing run (F(P)> 0). If Failure(P)< 1.0 then the bug is non-deterministic with \nrespect to P. Lower scores show weaker correlation between the predicate and program failure. Now Failure(P)is \na useful measure, but it is not good enough for the .rst step of our algorithm. To see this, consider \nagain the code fragment given above in its original, deterministic form. At line (b)we have Failure(f==NULL)=1.0, \nso this predicate is a good candidate for the cause of the bug. But on line (c) we have the unpleasant \nfact that Failure(x==0)=1.0 as well. To understand why, observe that the predicate x==0is always true \nat line (c)and, in addition, only failing runs reach this line. Thus S(x==0)=0, and, so long as there \nis at least one run that reaches line (c)at all, Failure(x==0)at line (c)is 1.0. As this example shows, \njust because Failure(P)is high does not mean P is the cause of a bug. In the case of x==0, the deci\u00adsion \nthat eventually causes the crash is made earlier, and the high Failure(x==0)score merely re.ects the \nfact that this predicate is checked on a path where the program is already doomed. A way to address this \ndif.culty is to score a predicate not by the chance that it implies failure, but by how much difference \nit makes that the predicate is observed to be true versus simply reaching the line where the predicate \nis checked. That is, on line (c), the probability of crashing is already 1.0 regardless of the value \nof the predicate x==0, and thus the fact that x==0is true does not increase the probability of failure \nat all. This fact coincides with our intuition that this predicate is irrelevant to the bug. Recall that \nwe write P observed when P has been reached and sampled at least once, without regard to whether P was \nactually true or false. This leads us to the following de.nition: Context(P)= Pr(Crash| P observed) Now, \nit is not the case that P is observed in every run, because the site where this predicate occurs may \nnot be reached, or may be reached but not sampled. Thus, Context(P)is the probability that in the subset \nof runs where the site containing predicate P is reached and sampled, the program fails. We can estimate \nContext(P) as follows: F(P observed) Context(P)= S(P observed)+F(P observed) The interesting quantity, \nthen, is Increase(P)= Failure(P)- Context(P) which can be read as: How much does P being true increase \nthe probability of failure over simply reaching the line where P is sampled? For example, for the predicate \nx==0on line (c),we have Failure(x==0)=Context(x==0)=1.0 and so Increase(x==0)=0. In most cases, a predicate \nP with Increase(P)= 0 has no predic\u00adtive power and can safely be discarded. (See Section 5 for possible \nexceptions.) Because some Increase(P) scores may be based on few observations of P, it is important to \nattach con.dence intervals to the scores. In our experiments we retain a predicate P only if the 95% \ncon.dence interval based on Increase(P) lies strictly above zero; this removes predicates from consideration \nthat have high in\u00adcrease scores but very low con.dence because of few observations. Pruning predicates \nbased on Increase(P)has several desirable properties. It is easy to prove that large classes of irrelevant \npredi\u00adcates always have scores = 0. For example, any predicate that is un\u00adreachable, that is a program \ninvariant, or that is obviously control\u00addependent on a true cause is eliminated by this test. It is also \nworth pointing out that this test tends to localize bugs at a point where the condition that causes the \nbug .rst becomes true, rather than at the crash site. For example, in the code fragment given above, \nthe bug is attributed to the success of the conditional branch test f== NULLon line (b)rather than the \npointer dereference on line (d). Thus, the cause of the bug discovered by the algorithm points di\u00adrectly \nto the conditions under which the crash occurs, rather than the line on which it occurs (which is usually \navailable anyway in the stack trace). 3.2 Statistical Interpretation We have explained the test Increase(P)> \n0 using programming terminology, but it also has a natural statistical interpretation as a simpli.ed \nlikelihood ratio hypothesis test. Consider the two classes of trial runs of the program: failed runs \nF and successful runs S. For each class, we can treat the predicate P as a Bernoulli random variable \nwith heads probabilities pf (P) and ps(P), respectively, for the two classes. The heads probability is \nthe probability that the predicate is observed to be true. If a predicate causes a set of crashes, then \np f should be much bigger than ps. We can formulate two statistical hypotheses: the null hypothesis H0: \np f = ps, versus the alternate hypothesis H1: pf > ps. Since pf and ps are not known, we must estimate \nthem: p f (P)= F(P) F(P observed) p s(P)= S(P) S(P observed) Although these proportion estimates of \npf and ps approach the actual heads probabilities as we increase the number of trial runs, they still \ndiffer due to sampling. With a certain probability, using these estimates instead of the actual values \nresults in the wrong answer. A likelihood ratio test takes this uncertainty into (p f -p s) account, \nand makes use of the statistic Z = , where Vf ,s is Vf ,s a sample variance term [8]. When the data size \nis large, Z can be approximated as a standard Gaussian random variable. Performed independently for each \npredicate P, the test decides whether or not p f (P)= ps(P) with a guaranteed false-positive probability \n(i.e., choosing H1 when H0 is true). A necessary (but not suf.cient) condition for choosing H1 is that \np f (P)> p s(P). However, this is equivalent to the condition that Increase(P)> 0. To see why, let a \n=F(P), b =S(P), c =F(P observed), and d =S(P observed). Then Increase(P)> 0 .. Failure(P)> Context(P) \nac .. > .. a(c +d)> (a +b)c a +bc +d ab .. ad > bc .. > .. p f (P)> p s(P) cd  3.3 Balancing Speci.city \nand Sensitivity We now turn to the question of ranking those predicates that sur\u00advive pruning. Table \n1 shows the top predicates under different rank\u00ad ing schemes (explained below) for one of our experiments. \nDue to space limitations we omit additional per-predicate information, such as source .le and line number, \nwhich is available in the inter\u00adactive version of our analysis tools. We use a concise bug thermometer \nto visualize the information for each predicate. The length of the thermometer is logarithmic in the \nnumber of runs in which the predicate was observed, so Table 1. Comparison of ranking strategies for \nMOSS without redundancy elimination (a) Sort descending by F(P) Thermometer Context Increase S F F + \nS Predicate 0.176 0.007 \u00b1 0.012 0.176 0.007 \u00b1 0.012 0.176 0.007 \u00b1 0.012 0.176 0.007 \u00b1 0.013 0.176 0.007 \n\u00b1 0.013 0.176 0.008 \u00b1 0.013 0.177 0.008 \u00b1 0.014 0.176 0.261 \u00b1 0.023 ............................................ \n 22554 22566 22571 18894 18885 17757 16453 4800 5045 27599 files[filesindex].language != 15 5045 27611 \ntmp == 0 is FALSE 5045 27616 strcmp != 0 4251 23145 tmp == 0 is FALSE 4240 23125 files[filesindex].language \n!= 14 4007 21764 filesindex >= 25 3731 20184 new value of M < old value of M 3716 8516 config.winnowing_window_size \n!= argc 2732additionalpredictorsfollow ............................................ (b) Sort descending \nby Increase(P) Thermometer Context Increase S F F + S Predicate 0.065 0.935 \u00b1 0.019 0 0.065 0.935 \u00b1 \n0.020 0 0.071 0.929 \u00b1 0.020 0 0.073 0.927 \u00b1 0.020 0 0.071 0.929 \u00b1 0.028 0 0.075 0.925 \u00b1 0.022 0 0.076 \n0.924 \u00b1 0.022 0 0.077 0.923 \u00b1 0.023 0  ............................................  23 23 ((*(fi + \ni)))->this.last_token < filesbase 10 10 ((*(fi + i)))->other.last_line == last 18 18 ((*(fi + i)))->other.last_line \n== filesbase 10 10 ((*(fi + i)))->other.last_line == yy_n_chars 19 19 bytes <= filesbase 14 14 ((*(fi \n+ i)))->other.first_line == 2 12 12 ((*(fi + i)))->this.first_line < nid 10 10 ((*(fi + i)))->other.last_line \n== yy_init 2732additionalpredictorsfollow ............................................ (c) Sort descending \nby harmonic mean Thermometer Context Increase S F F + S Predicate 0.176 0.824 \u00b10.009 0 0.176 0.824 \u00b10.009 \n0 0.176 0.824 \u00b10.009 0 0.176 0.824 \u00b10.009 0 0.176 0.824 \u00b10.009 0 0.176 0.824 \u00b10.009 0 0.116 0.883 \u00b10.012 \n1 0.116 0.883 \u00b10.012 1  1585 1585 files[filesindex].language > 16 1584 1584 strcmp > 0 1580 1580 strcmp \n== 0 1577 1577 files[filesindex].language == 17 1576 1576 tmp == 0 is TRUE 1573 1573 strcmp > 0 774 775 \n((*(fi + i)))->this.last_line == 1 776 777 ((*(fi + i)))->other.last_line == yyleng ........................................... \n2732additionalpredictorsfollow ........................................... small increases in thermometer \nsize indicate many more runs. Each more, the very narrow dark gray bands ( ) in most thermometers thermometer \nhas a sequence of bands. The black band on the left indicate that most Increase scores are very small. \nshows Context(P) as a fraction of the entire thermometer length. Our experience with other ranking strategies \nthat emphasize the The dark gray band ( ) shows the lower bound of Increase(P) number of failed runs \nis similar. They select predicates involved with 95% con.dence, also proportional to the entire thermometer \nin many failing, but also many successful, runs. The best of these length. The light gray band ( ) shows \nthe size of that con.dence predicates (the ones with high Increase scores) are super-bug pre\u00adinterval.4 \nIt is very small in most thermometers, indicating a tight dictors: predictors that include failures from \nmore than one bug. interval. The white space at the right end of the thermometer shows Super-bug predictors \naccount for a very large number of failures by S(P), the number of successful runs in which the predicate \nwas combining the failures of multiple bugs, but are also highly non\u00adobserved to be true. The tables \nshow the thermometer as well as the deterministic (because they are not speci.c to any single cause of \nnumbers for each of the quantities that make up the thermometer. failure) despite reasonably high Increase \nscores. The most important bug is the one that causes the greatest Another possibility is: number of \nfailed runs. This observation suggests: Importance(P)=Increase(P) Importance(P)=F(P) Table 1(b) shows \nthe top predicates ranked by decreasing Increase Table 1(a) shows the top predicates ranked by decreasing \nF(P)af\u00ad score. Thermometers here are almost entirely dark gray ( ), indi\u00adter predicates where Increase(P)= \n0 are discarded. The predicates cating Increase scores that are very close to 1.0. These predicates in \nTable 1(a) are, as expected, involved in many failing runs. How\u00ad do a much better job of predicting failure. \nIn fact, the program al\u00adever, the large white band in each thermometer reveals that these ways fails \nwhen any of these predicates is true. However, observe predicates are also highly non-deterministic: \nthey are also true in that the number of failing runs (column F) is very small. These many successful \nruns and are weakly correlated with bugs. Further\u00ad predicates are sub-bug predictors: predictors for \na subset of the failures caused by a bug. Unlike super-bug predictors, which are 4 If reading this paper \nin color, the dark gray band is red, and the light gray not useful in our experience, sub-bug predictors \nthat account for band is pink. a signi.cant fraction of the failures for a bug often provide valu\u00adable \nclues. However still they represent special cases and may not suggest other, more fundamental, causes \nof the bug. Tables 1(a) and 1(b) illustrate the dif.culty of de.ning impor\u00adtance. We are looking for \npredicates with high sensitivity, mean\u00ading predicates that account for many failed runs. But we also \nwant high speci.city, meaning predicates that do not mis-predict failure in many successful runs. In \ninformation retrieval, the corresponding terms are recall and precision. A standard way to combine sensitiv\u00adity \nand speci.city is to compute their harmonic mean; this measure prefers high scores in both dimensions. \nIn our case, Increase(P) measures speci.city. For sensitivity, we have found it useful to con\u00adsider a \ntransformation f of the raw counts, and to form the normal\u00adized ratio f(F(P))/f(NumF), where NumF is \nthe total number of failed runs. In our work thus far f has been a logarithmic trans\u00adformation, which \nmoderates the impact of very large numbers of failures. Thus our overall metric is the following: 2 Importance(P)= \n11 Increase(P)+log(F(P))/log(NumF) It is possible for this formula to be unde.ned (due to a division \nby 0), in which case we de.ne the Importance to be 0. Table 1(c) gives results using this metric. Both \nindividual F(P) counts and individual Increase(P) scores are smaller than in Tables 1(a) and 1(b), but \nthe harmonic mean has effectively balanced both of these important factors. All of the predicates on \nthis list indeed have both high speci.city and sensitivity. Each of these predictors accurately describes \na large number of failures. As in the case of the pruning based on the Increase score, it is useful to \nassess statistical signi.cance of Importance scores by computing a con.dence interval for the harmonic \nmean. Exact con\u00ad.dence intervals for the harmonic mean are not available, but we can use the delta method \nto derive approximate con.dence inter\u00advals [9]. Computing the means and variances needed for applying \nthe delta method requires computing estimates of underlying bino\u00admial probabilities for the predicates \nand conditioning on the event that the corresponding counts are non-zero.  3.4 Redundancy Elimination \nThe remaining problem with the results in Table 1(c) is that there is substantial redundancy; it is easy \nto see that several of these pred\u00adicates are related. This redundancy hides other, distinct bugs that \neither have fewer failed runs or more non-deterministic predictors further down the list. As discussed \npreviously, beginning with the set of all runs and predicates, we use a simple iterative algorithm to \neliminate redundant predicates: 1. Rank predicates by Importance. 2. Remove the top-ranked predicate \nP and discard all runs R (feed\u00adback reports) where R(P)=1. 3. Repeat these steps until the set of runs \nis empty or the set of predicates is empty.  We can now state an easy-to-prove but important property \nof this algorithm. LEMMA 3.1. Let P1,...,Pn be a set of instrumented predicates, B1,...,Bm a set of bugs, \nand B1,...,Bm the corresponding bug pro.les. Let  Z = {R|R(Pi)=1}. 1=i=nIf for all 1 = j = m we have \nBj nZ .0, then the algorithm chooses =/at least one predicate from the list P1,...,Pn that predicts at \nleast one failure due to B j. Thus, the elimination algorithm chooses at least one predicate predictive \nof each bug represented by the input set of predicates. We are, in effect, covering the set of bugs with \na ranked subset of predicates. The other property we might like, that the algorithm chooses exactly one \npredicate to represent each bug, does not hold; we shall see in Section 4 that the algorithm sometimes \nselects a strong sub-bug predictor as well as a more natural predictor. (As a technical aside, note that \nLemma 3.1 does not guarantee that every selected predicate has a positive Increase score at the time \nit is selected. Even if predicates with non-positive Increase scores are discarded before running the \nelimination algorithm, new ones can arise during the elimination algorithm. However, the Increase score \nof any predicate that covers at least one failing run will at least be de.ned. See Section 5 for related \ndiscussion.) Beyond always representing each bug, the algorithm works well for two other reasons. First, \ntwo predicates are redundant if they predict the same (or nearly the same) set of failing runs. Thus, \nsim\u00adply removing the set of runs in which a predicate is true automati\u00adcally reduces the importance of \nany related predicates in the correct proportions. Second, because elimination is iterative, it is only \nnec\u00adessary that Importance selects a good predictor at each step, and not necessarily the best one; any \npredicate that covers a different set of failing runs than all higher-ranked predicates is selected eventually. \nFinally, we studied an optimization in which we eliminated logically redundant predicates within instrumentation \nsites prior to running the iterative algorithm. However, the elimination algorithm proved to be suf.ciently \npowerful that we obtained nearly identical experimental results with and without this optimization, indicating \nit is unnecessary.  4. Experiments In this section we present the results of applying the algorithm \nde\u00adscribed in Section 3 in .ve case studies. Table 2 shows summary statistics for each of the experiments. \nIn each study we ran the pro\u00adgrams on about 32,000 random inputs. The number of instrumenta\u00adtion sites \nvaries with the size of the program, as does the number of predicates those instrumentation sites yield. \nOur algorithm is very effective in reducing the number of predicates the user must ex\u00adamine. For example, \nin the case of RHYTHMBOX an initial set of 857,384 predicates is reduced to 537 by the Increase(P)>0 \ntest, a reduction of 99.9%. The elimination algorithm then yields 15 pred\u00adicates, a further reduction \nof 97%. The other case studies show a similar reduction in the number of predicates by 3-4 orders of \nmag\u00adnitude. The results we discuss are all on sampled data. Sampling creates additional challenges that \nmust be faced by our algorithm. Assume P1 and P2 are equivalent bug predictors and both are sampled at \na rate of 1/100 and both are reached once per run. Then even though P1 and P2 are equivalent, they will \nbe observed in nearly disjoint sets of runs and treated as close to independent by the elimination algorithm. \nTo address this problem, we set the sampling rates of predicates to be inversely proportional to their \nfrequency of execution. Based on a training set of 1,000 executions, we set the sampling rate of each \npredicate so as to obtain an expected 100 samples of each predicate in subsequent program executions. \nOn the low end, the sampling rate is clamped to a minimum of 1/100; if the site is expected to be reached \nfewer than 100 times the sampling rate is set at 1.0. Thus, rarely executed code has a much higher sampling \nrate than very frequently executed code. (A similar strategy has been pursued for similar reasons in \nrelated work [3].) We have validated this approach by comparing the results for each experiment with \nresults obtained with no sampling at all (i.e., the sampling rate of all predicates set to 100%). The \nresults are identical except for the RHYTHMBOX and MOSS experiments, where we judge the differences to \nbe minor: sometimes a different but logically equivalent predicate is chosen, the ranking of predictors \nof different Table 2. Summary statistics for bug isolation experiments Runs Predicate Counts Lines of \nCode Successful Failing Sites Initial Increase > 0 Elimination MOSS 6001 26,299 5598 35,223 202,998 2740 \n21 CCRYPT 5276 20,684 10,316 9948 58,720 50 2 BC 14,288 23,198 7802 50,171 298,482 147 2 EXIF 10,588 \n30,789 2211 27,380 156,476 272 3 RHYTHMBOX 56,484 12,530 19,431 14,5176 857,384 537 15 bugs is slightly \ndifferent, or one or the other version has a few extra, weak predictors at the tail end of the list. \n 4.1 A Validation Experiment To validate our algorithm we .rst performed an experiment in which we knew \nthe set of bugs in advance. We added nine bugs to MOSS, a widely used service for detecting plagiarism \nin soft\u00adware [15]. Six of these were previously discovered and repaired bugs in MOSS that we reintroduced. \nThe other three were varia\u00adtions on three of the original bugs, to see if our algorithm could discriminate \nbetween pairs of bugs with very similar behavior but distinct causes. The nature of the eight crashing \nbugs varies: four buffer overruns, a null .le pointer dereference in certain cases, a missing end-of-list \ncheck in the traversal of a hash table bucket, a missing out-of-memory check, and a violation of a subtle \ninvariant that must be maintained between two parts of a complex data struc\u00adture. In addition, some of \nthese bugs are non-deterministic and may not even crash when they should. The ninth bug incorrect comment \nhandling in some cases only causes incorrect output, not a crash. We include this bug in our experiment \nin order to show that bugs other than crashing bugs can also be isolated using our techniques, provided \nthere is some way, whether by automatic self-checking or human inspection, to recognize failing runs. \nIn particular, for our experiment we also ran a correct version of MOSS and compared the output of the \ntwo versions. This oracle provides a labeling of runs as success or failure, and the resulting labels \nare treated identically by our algorithm as those based on program crashes. Table 3 shows the results \nof the experiment. The predicates listed were selected by the elimination algorithm in the order shown. \nThe .rst column is the initial bug thermometer for each predicate, showing the Context and Increase scores \nbefore elimi\u00adnation is performed. The second column is the effective bug ther\u00admometer, showing the Context \nand Increase scores for a predicate P at the time P is selected (i.e., when it is the top-ranked predicate). \nThus the effective thermometer re.ects the cumulative diluting ef\u00adfect of redundancy elimination for \nall predicates selected before this one. As part of the experiment we separately recorded the exact set \nof bugs that actually occurred in each run. The columns at the far right of Table 3 show, for each selected \npredicate and for each bug, the actual number of failing runs in which both the selected predicate is \nobserved to be true and the bug occurs. Note that while each predicate has a very strong spike at one \nbug, indicating it is a strong predictor of that bug, there are always some runs with other bugs present. \nFor example, the top-ranked predicate, which is overwhelmingly a predictor of bug #5, also includes some \nruns where bugs #3, #4, and #9 occurred. This situation is not the result of misclassi.cation of failing \nruns by our algorithm. As observed in Section 1, more than one bug may occur in a run. It simply happens \nthat in some runs bugs #5 and #3 both occur (to pick just one possible combination). A particularly interesting \ncase of this phenomenon is bug #7, one of the buffer overruns. Bug #7 is not strongly predicted by any \npredicate on the list but occurs in at least a few of the failing runs of most predicates. We have examined \nthe runs of bug #7 in detail and found that the failing runs involving bug #7 also trigger at least one \nother bug. That is, even when the bug #7 overrun occurs, it never causes incorrect output or a crash \nin any run. Bug #8, another overrun, was originally found by a code inspection. It is not shown here \nbecause the overrun is never triggered in our data (its column would be all 0 s). There is no way our \nalgorithm can .nd causes of bugs that do not occur, but recall that part of our purpose in sampling user \nexecutions is to get an accurate picture of the most important bugs. It is consistent with this goal \nthat if a bug never causes a problem, it is not only not worth .xing, it is not even worth reporting. \nThe other bugs all have strong predictors on the list. In fact, the top eight predicates have exactly \none predictor for each of the seven bugs that occur, with the exception of bug #1, which has one very \nstrong sub-bug predictor in the second spot and another predictor in the sixth position. Notice that \neven the rarest bug, bug #2, which occurs more than an order of magnitude less frequently than the most \ncommon bug, is identi.ed immediately after the last of the other bugs. Furthermore, we have veri.ed by \nhand that the selected predicates would, in our judgment, lead an engineer to the cause of the bug. Overall, \nthe elimination algorithm does an excellent job of listing separate causes of each of the bugs in order \nof priority, with very little redundancy. Below the eighth position there are no new bugs to report and \nevery predicate is correlated with predicates higher on the list. Even without the columns of numbers \nat the right it is easy to spot the eighth position as the natural cutoff. Keep in mind that the length \nof the thermometer is on a log scale, hence changes in larger mag\u00adnitudes may appear less evident. Notice \nthat the initial and effective thermometers for the .rst eight predicates are essentially identical. \nOnly the predicate at position six is noticeably different, indicating that this predicate is somewhat \naffected by a predicate listed ear\u00adlier (speci.cally, its companion sub-bug predictor at position two). \nHowever, all of the predicates below the eighth line have very dif\u00adferent initial and effective thermometers \n(either many fewer failing runs, or much more non-deterministic, or both) showing that these predicates \nare strongly affected by higher-ranked predicates. The visualizations presented thus far have a drawback \nillus\u00adtrated by the MOSS experiment: It is not easy to identify the pred\u00adicates to which a predicate \nis closely related. Such a feature would be useful in con.rming whether two selected predicates represent \ndifferent bugs or are in fact related to the same bug. We do have a measure of how strongly P implies \nanother predicate Pi: How does removing the runs where R(P)=1 affect the importance of Pi? The more closely \nrelated P and Pi are, the more Pi s importance drops when P s failing runs are removed. In the interactive \nversion of our analysis tools, each predicate P in the .nal, ranked list links to an af.nity list of \nall predicates ranked by how much P causes their ranking score to decrease. Table 3. MOSS failure predictors \nusing nonuniform sampling Number of Failing Runs Also Exhibiting Bug #n Initial Effective Predicate #1 \n#2 #3 #4 #5 #6 #7 #9   files[filesindex].language > 16 0 0 28 54 1585 0 0 68 ((*(fi + i)))->this.last_line \n== 1 774 0 17 0 0 0 18 2 token_index > 500 31 0 16 711 0 0 0 47 (p + passage_index)->last_token <= filesbase \n28 2 508 0 0 0 1 29 __result == 0 is TRUE 16 0 0 9 19 291 0 13 config.match_comment is TRUE 791 2 23 \n1 0 5 11 41 i == yy_last_accepting_state 55 0 21 0 0 3 7 769 new value of f < old value of f 3 144 2 \n2 0 0 0 5 files[fileid].size < token_index 31 0 10 633 0 0 0 40 passage_index == 293 27 3 8 0 0 0 2 366 \n((*(fi + i)))->other.last_line == yyleng 776 0 16 0 0 0 18 1 min_index == 64 24 1 7 0 0 1 1 249 ((*(fi \n+ i)))->this.last_line == yy_start 771 0 18 0 0 0 19 0 (passages + i)->fileid == 52 24 0 477 14 24 0 \n1 14 passage_index == 25 60 5 27 0 0 4 10 962 strcmp > 0 0 0 28 54 1584 0 0 68 i > 500 32 2 18 853 54 \n0 0 53 token_sequence[token_index].val >= 100 1250 3 28 38 0 15 19 65 i==50 27 0 11 0 0 1 4 463 passage_index \n== 19 59 5 28 0 0 4 10 958 bytes <= filesbase 1 0 19 0 0 0 0 1 Table 4. Predictors for CCRYPT Initial \nEffective Predicate res == nl line <= outfile Table 5. Predictors for BC Initial Effective Predicate \na_names < v_names old_count == 32  4.2 Additional Experiments We brie.y report here on experiments \nwith additional applica\u00adtions containing both known and unknown bugs. Complete anal\u00adysis results for \nall experiments may be browsed interactively at <http://www.cs.wisc.edu/ liblit/pldi-2005/>. 4.2.1 CCRYPT \nWe analyzed CCRYPT 1.2, which has a known input validation bug. The results are shown in Table 4. Our \nalgorithm reports two predictors, both of which point directly to the single bug. It is easy to discover \nthat the two predictors are for the same bug; the .rst predicate is listed .rst in the second predicate \ns af.nity list, indicating the .rst predicate is a sub-bug predictor associated with the second predicate. \n 4.2.2 BC GNU BC 1.06 has a previously reported buffer overrun. Our results are shown in Table 5. The \noutcome is the same as for CCRYPT: two predicates are retained by elimination, and the second predicate \nlists the .rst predicate at the top of its af.nity list, indicating that the .rst predicate is a sub-bug \npredictor of the second. Both predicates point to the cause of the overrun. This bug causes a crash long \nafter the overrun occurs and there is no useful information on the stack at the point of the crash to \nassist in isolating this bug. 4.2.3 EXIF Table 6 shows results for EXIF 0.6.9, an open source image \nprocess\u00ading program. Each of the three predicates is a predictor of a distinct Table 6. Predictors for \nEXIF Initial Effective Predicate i<0 maxlen > 1900 o + s > buf_size is TRUE and previously unknown \ncrashing bug. It took less than twenty min\u00adutes of work to .nd and verify the cause of each of the bugs \nusing these predicates and the additional highly correlated predicates on their af.nity lists. All bugs \nhave been con.rmed as valid by EXIF project developers. To illustrate how statistical debugging is used \nin practice, we use the last of these three failure predictors as an example, and describe how it enabled \nus to effectively isolate the cause of one of the bugs. Failed runs exhibiting o+s>buf size show the \nfollowing unique stack trace at the point of termination: main exif_data_save_data exif_data_save_data_content \nexif_data_save_data_content exif_data_save_data_entry exif_mnote_data_save exif_mnote_data_canon_save \nmemcpy The code in the vicinity of the call to memcpy in function exif mnote data canon saveis as follows: \nfor (i = 0; i < n->count; i++) { ... memcpy(*buf + doff, (c) n->entries[i].data, s); ... } This stack \ntrace alone provides little insight into the cause of the bug. However, our algorithm highlights o+s>buf \nsize in function exif mnote data canon load as a strong bug predictor. Thus, a quick inspection of the \nsource code leads us to construct the following call sequence: Table 7. Predictors for RHYTHMBOX Initial \nEffective Predicate  tmp is FALSE (mp->priv)->timer is FALSE (view->priv)->change_sig_queued is TRUE \n(hist->priv)->db is TRUE rb_playlist_manager_signals[0] > 269 (db->priv)->thread_reaper_id >= 12 entry \n== entry fn == fn klass > klass genre < artist vol <= (float )0 is TRUE (player->priv)->handling_error \nis TRUE (statusbar->priv)->library_busy is TRUE shell < shell len < 270 main exif_loader_get_data exif_data_load_data \nexif_mnote_data_canon_load exif_data_save_data exif_data_save_data_content exif_data_save_data_content \nexif_data_save_data_entry exif_mnote_data_save exif_mnote_data_canon_save memcpy The code in the vicinity \nof the predicate o+s>buf size in function exif mnote data canon loadis as follows: for(i =0; i < c; i++) \n{ ... n->count = i + 1; ... if (o + s > buf_size) return; (a) ... n->entries[i].data = malloc(s); (b) \n... } It is apparent from the above code snippets and the call sequence that whenever the predicate \no+s>buf sizeis true, the function exif mnote data canon load returns on line (a), thereby skipping the \ncall to malloc on line (b) and thus leaving n->entries[i].data uninitialized for some value of i, and \n the function exif mnote data canon save passes the uninitialized n->entries[i].data to memcpy on line \n(c), which reads it and eventually crashes.  In summary, our algorithm enabled us to effectively isolate \nthe causes of several previously unknown bugs in source code unfamiliar to us in a small amount of time \nand without any explicit speci.cation beyond the program shouldn t crash. 4.2.4 RHYTHMBOX Table 7 shows \nour results for RHYTHMBOX 0.6.5, an interactive, graphical, open source music player. RHYTHMBOX is a \ncomplex, multi-threaded, event-driven system, written using a library provid\u00ading object-oriented primitives \nin C. Event-driven systems use event queues; each event performs some computation and possibly adds more \nevents to some queues. We know of no static analysis today that can analyze event-driven systems accurately, \nbecause no static analysis is currently capable of analyzing the heap-allocated event queues with suf.cient \nprecision. Stack inspection is also of limited utility in analyzing event-driven systems, as the stack \nin the main event loop is unchanging and all of the interesting state is in the queues. We isolated two \ndistinct bugs in RHYTHMBOX. The .rst predi\u00adcate led us to the discovery of a race condition. The second \npredi\u00adcate was not useful directly, but we were able to isolate the bug us\u00ading the predicates in its \naf.nity list. This second bug revealed what turned out to be a very common incorrect pattern of accessing \nthe underlyingobjectlibrary(recallSection1). RHYTHMBOX develop\u00aders con.rmed the bugs and enthusiastically \napplied patches within a few days, in part because we could quantify the bugs as important crashing bugs. \nIt required several hours to isolate each of the two bugs (and there are additional bugs represented \nin the predictors that we did not isolate) because they were violations of subtle heap invariants that \nare not directly captured by our current instrumen\u00adtation schemes. Note, however, that we could not have \neven begun to understand these bugs without the information provided by our tool. We intend to explore \nschemes that track predicates on heap structure in future work. 4.3 How Many Runs Are Needed? Recall \nthat we used about 32,000 runs in each of the .ve case studies. For many of the bugs this number is clearly \nfar more than the minimum required. In this section, we estimate how many runs are actually needed for \nall bug predictors to be identi.ed. Our estimates are computed using the following methodology. We choose \none predictor for each bug identi.ed in the case studies. Where the elimination algorithm selects two \npredictors for a bug, we pick the more natural one (i.e., not the sub-bug predictor). For each chosen \npredictor P, we compute the importance of P using many different numbers of runs. Let ImportanceN (P) \nbe the importance of P using N runs. We are interested in the minimum N such that Importance32,000(P) \n- ImportanceN (P) < 0.2 The threshold 0.2 is selected because we observe that all of the chosen predictors \nin our studies would still be ranked very highly even if their importance scores were 0.2 lower. Table \n8 presents the results of the analysis. In these experiments, the number of runs N ranges over the values \n100, 200, ..., 900, 1,000, 2,000, ..., 25,000. For each study, we list two numbers for each bug with \npredictor P: the minimum number of runs N such that the threshold test is met, and the number of failing \nruns F(P) among those N runs where P is observed to be true. Note that the number of runs N needed for \ndifferent bugs varies by two orders of magnitude. We need 21,000 runs to isolate all of the bug predictors \nin EXIF because the last bug in that study is extremely rare: only 21 failing runs out of our total population \nof 33,000 runs share bug #3 as the cause of failure. If we exclude bug #3, then just 2,000 runs are suf.cient \nto isolate EXIF bugs #1 and #2. Thus, results degrade gracefully with fewer runs, with the predictors \nfor rare bugs dropping out .rst. The number F(P) is independent of the rate at which the dif\u00adferent bugs \noccur and allows us to compare the absolute number of failures needed to isolate different bugs. Notice \nthat we can isolate any bug predictor with between 10 and 40 observations of failing runs caused by the \nbug. How long it takes to get those observations of failing runs depends on the frequency with which \nthe bug occurs and the sampling rate of the bug predictor. Assume F failures are needed to isolate the \npredictor of a bug. If the failing runs where the predictor is observed to be true constitute a fraction \n0 = p = 1of all runs, then about N = F/p runs will be required by our algorithm.  4.4 Comparison with \nLogistic Regression In earlier work we used 1-regularized logistic regression to rank the predicates \nby their failure-prediction strength [10, 16]. Logistic regression uses linearly weighted combinations \nof predicates to classify a trial run as successful or failed. Regularized logistic Table 8. Minimum \nnumber of runs needed Bug #n Runs #1 #2 #3#4#5 #6#9 MOSS F(P) N 18 500 10 3,000 32 2,000 12 800 21 300 \n11 1,000 20 600 CCRYPT F(P) N 26 200 BC F(P) N 40 200 RHYTHMBOX F(P) N 22 300 35 100 EXIF F(P) N 28 2,000 \n12 300 13 21,000 Table 9. Results of logistic regression for MOSS Coef.cient Predicate 0.769379 (p + \npassage_index)->last_line < 4 0.686149 (p + passage_index)->first_line < i 0.675982 i>20 0.671991 i>26 \n0.619479 (p + passage_index)->last_line < i 0.600712 i>23 0.591044 (p + passage_index)->last_line == \nnext 0.567753 i>22 0.544829 i>25 0.536122 i>28 regression incorporates a penalty term that drives most \ncoef.cients towards zero, thereby giving weights to only the most important predicates. The output is \na set of coef.cients for predicates giving the best overall prediction. A weakness of logistic regression \nfor our application is that it seeks to cover the set of failing runs without regard to the orthogo\u00adnality \nof the selected predicates (i.e., whether they represent distinct bugs). This problem can be seen in \nTable 9, which gives the top ten predicates selected by logistic regression for MOSS. The striking fact \nis that all selected predicates are either sub-bug or super-bug predictors. The predicates beginning \nwith p + ... are all sub\u00adbug predictors of bug #1 (see Table 3). The predicates i > ... are super-bug \npredictors: i is the length of the command line and the predicates say program crashes are more likely \nfor long com\u00admand lines (recall Section 1). The prevalence of super-bug predictors on the list shows \nthe dif.culty of making use of the penalty term. Limiting the number of predicates that can be selected \nvia a penalty has the effect of encouraging regularized logistic regression to choose super-bug predictors, \nas these cover more failing runs at the expense of poorer predictive power compared to predictors of \nindividual bugs. On the other hand, the sub-bug predictors are chosen based on their excellent prediction \npower of those small subsets of failed runs.  5. Alternatives and Extensions While we have targeted \nour algorithm at .nding bugs, there are other possible applications, and there are variations of the \nbasic approach we have presented that may prove useful. In this section we brie.y discuss some of these \npossibilities. While we have focused on bug .nding, the same ideas can be used to isolate predictors \nof any program event. For example, we could potentially look for early predictors of when the program \nwill raise an exception, send a message on the network, write to disk, or suspend itself. Furthermore, \nit is interesting to consider applications in which the predictors are used on-line by the run\u00adning program; \nfor example, knowing that a strong predictor of pro\u00adgram failure has become true may enable preemptive \naction (see Section 6). There are also variations on the speci.c algorithm we have proposed that are \nworth investigating. For example, we have chosen to discard all the runs where R(P)=1 when P is selected \nby the elimination algorithm, but there are at least three natural choices: 1. When P is selected, discard \nall runs where R(P)=1. 2. When P is selected, discard only failing runs where R(P)=1. 3. When P is \nselected, relabel all failing runs where R(P)=1as successful runs.  We have already given the intuition \nfor (1), our current choice. For (2), the idea is that whatever the bug is, it is not manifested in the \nsuccessful runs and thus retaining all successful runs is more representative of correct program behavior. \nProposal (3) goes one step further, asserting that even the failing runs will look mostly the same once \nthe bug is .xed, and the best approximation to a program without the bug is simply that the failing runs \nare now successful runs. On a more technical level, the three proposals differ in how much code coverage \nthey preserve. By discarding no runs, proposal (3) preserves all the code paths that were executed in \nthe original runs, while proposal (1) discards the most runs and so potentially renders more paths unreached \nin the runs that remain. This differ\u00adence in paths preserved translates into differences in the Failure \nand Context scores of predicates under the different proposals. In fact, for a predicate P and its complement \n\u00acP, it is possible to prove that just after predicate P is selected by the elimination algorithm, then \nIncrease3(\u00acP)= Increase2(\u00acP)= Increase1(\u00acP)=0 where the subscripts indicate which proposal for discarding \nruns is used and assuming all the quantities are de.ned. Thus, proposal (1) is the most conservative, \nin the sense that only one of P or \u00acP can have positive predictive power, while proposal (3) potentially \nallows more predictors to have positive Increase scores. This analysis reveals that a predicate P with \na negative Increase score is not necessarily useless the score may be negative only because it is temporarily \novershadowed by stronger predictors of different bugs that are anti-correlated with P. It is possible \nto construct examples where both P and \u00acP are the best predictors of different bugs, but the result mentioned \nabove assures us that once one is selected by the elimination algorithm, the other s Increase score is \nnon-negative if it is de.ned. This line of reasoning also suggests that when using proposal (2) or (3) \nfor discarding runs, a predicate P with Increase(P) = 0 should not be discarded in a preprocessing step, \nas P may become a positive predictor once \u00acP is selected by the elimination algorithm. In the case of \nproposal (1), only one of P or \u00acP can ever have a non-negative Increase score, and so it seems reasonable \nto eliminate predicates with negative scores early. 6. Related Work In this section we brie.y survey \nrelated work. There is currently a great deal of interest in applying static analysis to improve software \nquality. While we .rmly believe in the use of static analysis to .nd and prevent bugs, our dynamic approach \nhas advantages as well. A dynamic analysis can observe actual run-time values, which is of\u00adten better \nthan either making a very conservative static assumption about run-time values for the sake of soundness \nor allowing some very simple bugs to escape undetected. Another advantage of dy\u00adnamic analysis, especially \none that mines actual user executions for its data, is the ability to assign an accurate importance to \neach bug. Additionally, as we have shown, a dynamic analysis that does not require an explicit speci.cation \nof the properties to check can .nd clues to a very wide range of errors, including classes of errors \nnot considered in the design of the analysis. The Daikon project [5] monitors instrumented applications \nto discover likely program invariants. It collects extensive trace in\u00adformation at run time and mines \ntraces of.ine to accept or reject any of a wide variety of hypothesized candidate predicates. The DIDUCE \nproject [7] tests a more restricted set of predicates within the client program, and attempts to relate \nstate changes in candidate predicates to manifestation of bugs. Both projects assume complete monitoring, \nsuch as within a controlled test environment. Our goal is to use lightweight partial monitoring, suitable \nfor either testing or deployment to end users. Software tomography as realized through the GAMMA system \n[1, 12] shares our goal of low-overhead distributed monitoring of deployed code. GAMMA collects code \ncoverage data to support a variety of code evolution tasks. Our instrumentation exposes a broader family \nof data-and control-dependent predicates on pro\u00adgram behavior and uses randomized sparse sampling to \ncontrol overhead. Our predicates do, however, also give coverage informa\u00adtion: the sum of all predicate \ncounters at a site reveals the relative coverage of that site. Efforts to directly apply statistical \nmodeling principles to de\u00adbugging have met with mixed results. Early work in this area by Burnell and \nHorvitz [2] uses program slicing in conjunction with Bayesian belief networks to .lter and rank the possible \ncauses for a given bug. Empirical evaluation shows that the slicing compo\u00adnent alone .nds 65% of bug \ncauses, while the probabilistic model correctly identi.es another 10%. This additional payoff may seem \nsmall in light of the effort, measured in man-years, required to dis\u00adtill experts often tacit knowledge \ninto a formal belief network. However, the approach does illustrate one strategy for integrating information \nabout program structure into the statistical modeling process. In more recent work, Podgurski et al. \n[13] apply statistical fea\u00ad ture selection, clustering, and multivariate visualization techniques to \nthe task of classifying software failure reports. The intent is to bucket each report into an equivalence \ngroup believed to share the same underlying cause. Features are derived of.ine from .ne\u00adgrained execution \ntraces without sampling; this approach reduces the noise level of the data but greatly restricts the \ninstrumentation schemes that are practical to deploy outside of a controlled testing environment. As \nin our own earlier work, Podgurski uses logis\u00adtic regression to select features that are highly predictive \nof failure. Clustering tends to identify small, tight groups of runs that do share a single cause but \nthat are not always maximal. That is, one cause may be split across several clusters. This problem is \nsimilar to cov\u00adering a bug pro.le with sub-bug predictors. In contrast, current industrial practice uses \nstack traces to cluster failure reports into equivalence classes. Two crash reports show\u00ading the same \nstack trace, or perhaps only the same top-of-stack function, are presumed to be two reports of the same \nfailure. This heuristic works to the extent that a single cause corresponds to a single point of failure, \nbut our experience with MOSS,RHYTHM-BOX, and EXIF suggests that this assumption may not often hold. In \nMOSS, we .nd that only bugs #2 and #5 have truly unique signa\u00adture stacks: a crash location that is present \nif and only if the corre\u00adsponding bug was actually triggered. These bugs are also our most deterministic. \nBugs #4 and #6 also have nearly unique stack sig\u00adnatures. The remaining bugs are much less consistent: \neach stack signature is observed after a variety of different bugs, and each triggered bug causes failure \nin a variety of different stack states. RHYTHMBOX and EXIF bugs caused crashes so long after the bad \nbehavior that stacks were of limited use or no use at all. Studies that attempt real-world deployment \nof monitored soft\u00adware must address a host of practical engineering concerns, from distribution to installation \nto user support to data collection and warehousing. Elbaum and Hardojo [4] have reported on a limited \ndeployment of instrumented Pine binaries. Their experiences have helped to guide our own design of a \nwide public deployment of ap\u00adplications with sampled instrumentation, presently underway [11]. For some \nhighly available systems, even a single failure must be avoided. Once the behaviors that predict imminent \nfailure are known, automatic corrective measures may be able to prevent the failure from occurring at \nall. The Software Dependability Frame\u00adwork (SDF) [6] uses multivariate state estimation techniques to \nmodel and thereby predict impending system failures. Instrumen\u00adtation is assumed to be complete and is \ntypically domain-speci.c. Our algorithm could also be used to identify early warning predi\u00adcates that \npredict impending failure in actual use. 7. Conclusions We have demonstrated a practical, scalable algorithm \nfor isolating multiple bugs in complex software systems. Our experimental re\u00adsults show that we can detect \na wide variety of both anticipated and unanticipated causes of failure in realistic systems and do so \nwith a relatively modest number of program executions.  References [1] J. Bowring, A. Orso, and M. \nJ. Harrold. Monitoring deployed software using software tomography. In M. B. Dwyer, editor, Proceedings \nof the 2002 ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering (PASTE-02), \nvolume 28, 1 of SOFTWARE ENGINEERING NOTES, pages 2 9. ACM Press, 2002. [2] L. Burnell and E. Horvitz. \nStructure and chance: melding logic and probability for software debugging. Communications of the ACM, \n38(3):31 41, 57, Mar. 1995. [3] T. Chilimbi and M. Hauswirth. Low-overhead memory leak detection using \nadaptive statistical pro.ling. In ASPLOS: Eleventh International Conference on Architectural Support \nfor Programming Languages and Operating Systems, Boston, MA, Oct. 2004. [4] S. Elbaum and M. Hardojo. \nDeploying instrumented software to assist the testing activity. In RAMSS 2003 [14], pages 31 33. [5] \nM. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin. Dynamically discovering likely program invariants \nto support program evolution. IEEE Transactions on Software Engineering, 27(2):1 25, Feb. 2001. [6] K. \nC. Gross, S. McMaster, A. Porter, A. Urmanov, and L. G. Votta. Proactive system maintenance using software \ntelemetry. In RAMSS 2003 [14], pages 24 26. [7] S. Hangal and M. S. Lam. Tracking down software bugs \nusing au\u00adtomatic anomaly detection. In Proceedings of the 24th International Conference on Software Engineering \n(ICSE-02), pages 291 301. ACM Press, 2002. [8] E. Lehmann. Testing Statistical Hypotheses. John Wiley \n&#38; Sons, 2nd edition, 1986. [9] E. Lehmann and G. Casella. Theory of Point Estimation. Springer, 2nd \nedition, 2003. [10] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. Bug isolation via remote program \nsampling. In J. James B. Fenwick and C. Norris, editors, Proceedings of the ACM SIGPLAN 2003 Conference \non Programming Language Design and Implementation (PLDI-03), volume 38, 5 of ACM SIGPLAN Notices, pages \n141 154. ACM Press, 2003. [11] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan. Public deployment \nof cooperative bug isolation. In Proceedings of the Second International Workshop on Remote Analysis \nand Measurement of Software Systems (RAMSS 04), pages 57 62, Edinburgh, Scotland, May 24 2004. [12] A. \nOrso, T. Apiwattanapong, and M. J. Harrold. Leveraging .eld data for impact analysis and regression testing. \nIn Proceedings of the 9th European software engineering conference held jointly with 11th ACM SIGSOFT \ninternational symposium on Foundations of software engineering, pages 128 137. ACM Press, 2003. [13] \nA. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun, and B. Wang. Automated support for classifying \nsoftware failure reports. In Proceedings of the 25th International Conference on Software Engineering \n(ICSE-03), pages 465 477. IEEE Computer Society, 2003. [14] RAMSS 03: The 1st International Workshop \non Remote Analysis and Measurement of Software Systems, May 2003. [15] S. Schleimer, D. S. Wilkerson, \nand A. Aiken. Winnowing: local algorithms for document .ngerprinting. In ACM, editor, Proceedings of \nthe 2003 ACM SIGMOD International Conference on Management of Data 2003, San Diego, California, June \n09 12, 2003, pages 76 85, New York, NY 10036, USA, 2003. ACM Press. [16] A. X. Zheng, M. I. Jordan, B. \nLiblit, and A. Aiken. Statistical debugging of sampled programs. In S. Thrun, L. Saul, and B. Sch\u00a8 olkopf, \neditors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004. \n\t\t\t", "proc_id": "1065010", "abstract": "We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.", "authors": [{"name": "Ben Liblit", "author_profile_id": "81100555854", "affiliation": "University of Wisconsin-Madison", "person_id": "PP38025922", "email_address": "", "orcid_id": ""}, {"name": "Mayur Naik", "author_profile_id": "81100223912", "affiliation": "Stanford University", "person_id": "P195415", "email_address": "", "orcid_id": ""}, {"name": "Alice X. Zheng", "author_profile_id": "81538301756", "affiliation": "University of California, Berkeley", "person_id": "P338603", "email_address": "", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University", "person_id": "PP39041079", "email_address": "", "orcid_id": ""}, {"name": "Michael I. Jordan", "author_profile_id": "81339507945", "affiliation": "University of California, Berkeley", "person_id": "PP42051464", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065014", "year": "2005", "article_id": "1065014", "conference": "PLDI", "title": "Scalable statistical bug isolation", "url": "http://dl.acm.org/citation.cfm?id=1065014"}