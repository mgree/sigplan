{"article_publication_date": "06-12-2005", "fulltext": "\n DART: Directed Automated Random Testing Patrice Godefroid Nils Klarlund Koushik Sen Bell Laboratories, \nLucent Technologies Computer Science Department {god,klarlund}@bell-labs.com University of Illinois at \nUrbana-Champaign ksen@cs.uiuc.edu Abstract We present a new tool, named DART, for automatically testing \nsoft\u00adware that combines three main techniques: (1) automated extrac\u00adtion of the interface of a program \nwith its external environment using static source-code parsing; (2) automatic generation of a test driver \nfor this interface that performs random testing to simulate the most general environment the program \ncan operate in; and (3) dynamic analysis of how the program behaves under random test\u00ading and automatic \ngeneration of new test inputs to direct systemati\u00adcally the execution along alternative program paths. \nTogether, these three techniques constitute Directed Automated Random Testing,or DART for short. The \nmain strength of DART is thus that testing can be performed completely automatically on any program that \ncom\u00adpiles there is no need to write any test driver or harness code. Dur\u00ading testing, DART detects standard \nerrors such as program crashes, assertion violations, and non-termination. Preliminary experiments to \nunit test several examples of C programs are very encouraging. Categories and Subject Descriptors D.2.4 \n[Software Engineer\u00ading]: Software/Program Veri.cation; D.2.5 [Software Engineer\u00ading]: Testing and Debugging; \nF.3.1 [Logics and Meanings of Pro\u00adgrams]: Specifying and Verifying and Reasoning about Programs General \nTerms Veri.cation, Algorithms, Reliability Keywords Software Testing, Random Testing, Automated Test \nGeneration, Interfaces, Program Veri.cation 1. Introduction Today, testing is the primary way to check \nthe correctness of soft\u00adware. Billions of dollars are spent on testing in the software indus\u00adtry, as \ntesting usually accounts for about 50% of the cost of software development [27]. It was recently estimated \nthat software failures currently cost the US economy alone about $60 billion every year, and that improvements \nin software testing infrastructure might save one-third of this cost [31]. Among the various kinds of \ntesting usually performed during the software development cycle, unit testing applies to the indi\u00advidual \ncomponents of a software system. In principle, unit testing plays an important role in ensuring overall \nsoftware quality since its role is precisely to detect errors in the component s logic, check all corner \ncases, and provide 100% code coverage. Yet, in practice, Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. PLDI 05 June 12 15, 2005, Chicago, Illinois, USA. \nCopyright 2005 ACM 1-59593-056-6/05/0006...$5.00. unit testing is so hard and expensive to perform that \nit is rarely done properly. Indeed, in order to be able to execute and test a component in isolation, \none needs to write test driver/harness code to simulate the environment of the component. More code is \nneeded to test functional correctness, for instance using assertions checking the component s outputs. \nSince writing all this testing code manually is expensive, unit testing is often either performed very \npoorly or skipped altogether. Moreover, subsequent phases of testing, such as feature, integration and \nsystem testing, are meant to test the overall correctness of the entire system viewed as a black-box, \nnot to check the corner cases where bugs causing reliability issues are typically hidden. As a consequence, \nmany software bugs that should have been caught during unit testing remain undetected until .eld de\u00adployment. \nIn this paper, we propose a new approach that addresses the main limitation hampering unit testing, namely \nthe need to write test driver and harness code to simulate the external environment of a software application. \nWe describe our tool DART, which com\u00adbines three main techniques in order to automate unit testing of \nsoftware: 1. automated extraction of the interface of a program with its external environment using static \nsource-code parsing; 2. automatic generation of a test driver for this interface that per\u00adforms random \ntesting to simulate the most general environment the program can operate in; and 3. dynamic analysis \nof how the program behaves under random testing and automatic generation of new test inputs to direct \nsystematically the execution along alternative program paths.  Together, these three techniques constitute \nDirected Automated Random Testing,or DART for short. Thus, the main strength of DART is that testing \ncan be performed completely automatically on any program that compiles there is no need to write any \ntest driver or harness code. During testing, DART detects standard errors such as program crashes, assertion \nviolations, and non-termination. We have implemented DART for programs written in the C pro\u00adgramming \nlanguage. Preliminary experiments to unit test several examples of C programs are very encouraging. For \ninstance, DART was able to .nd automatically attacks in various C implementations of a well-known .awed \nsecurity protocol (Needham-Schroeder s). Also, DART found hundreds of ways to crash 65% of the about \n600 externally visible functions provided in the oSIP library, an open\u00adsource implementation of the SIP \nprotocol. These experimental re\u00adsults are discussed in detail in Section 4. The idea of extracting automatically \ninterfaces of software com\u00adponents via static analysis has been discussed before, for model\u00adchecking \npurposes (e.g., [8]), reverse engineering (e.g., [37]), and compositional veri.cation (e.g., [1]). However, \nwe are not aware of any tool like DART which combines automatic interface extraction with random testing \nand dynamic test generation. DART is comple\u00admentary to test-management tools that take advantage of interface \nde.nitions as part of programming languages, such as JUnit [20] for Java, but do not perform automatic \ntest generation. Random testing is a simple and well-known technique (e.g., [4]), which can be remarkably \neffective at .nding software bugs [11]. Yet, it is also well-known that random testing usually provides \nlow code coverage (e.g., [32]). For instance, the then branch of the conditional statement if (x==10) \nthen ... has only one chance to be exercised out of 232 if x is a 32-bit integer program input that is \nrandomly initialized. The contributions of DART com\u00adpared to random testing are twofold: DART makes random \ntesting automatic by combining it with automatic interface extraction (in contrast with prior work which \nis API-speci.c, e.g., [11]), and also makes it much more effective in .nding errors thanks to the use \nof dynamic test generation to drive the program along alternative con\u00additional branches. For instance, \nthe probability of taking the then branch of the statement if (x==10) then ... can be viewed as 0.5 with \nDART. The novel dynamic test-generation techniques used in DART are presented in Section 2. Besides testing, \nthe other main way to check correctness during the software development cycle is code inspection. Over \nthe last few years, there has been a renewed interest in static source-code analysis for building automatic \ncode-inspection tools that are more practical and usable by the average software developer. Examples \nof such tools are Pre.x/Prefast [6], MC [16], Klocwork [22], and Polyspace [33]. Earlier program static \ncheckers like lint [19] usu\u00adally generate an overly large number of warnings and false alarms, and are \ntherefore rarely used by programmers on a regular basis. The main challenge faced by the new generation \nof static analyz\u00aders is thus to do a better job in dealing with false alarms (warn\u00adings that do not actually \ncorrespond to programming errors), which arise from the inherent imprecision of static analysis. There \nare es\u00adsentially two main approaches to this problem: either report only high-con.dence warnings (at \nthe risk of missing some actual bugs), or report all of them (at the risk of overwhelming the user). \nDespite signi.cant recent progress on techniques to separate false alarms from real errors (for instance, \nby using more precise analysis tech\u00adniques to eliminate false alarms, or by using statistical classi.cation \ntechniques to rank warnings by their severity more accurately), an\u00adalyzing the results of static analysis \nto determine whether a warning actually corresponds to an error still involves signi.cant human in\u00adtervention. \nWe believe DART provides an attractive alternative approach to static analyzers, because it is based \non high-precision dynamic analysis instead, while being fully automated as static analysis. The main \nadvantage of DART over static analysis is that every execu\u00adtion leading to an error that is found by \nDART is guaranteed to be sound. Two areas where we expect DART to compete especially well against static \nanalyzers are the detection of interprocedural bugs and of bugs that arise through the use of library \nfunctions (which are usually hard to reason about statically), as will be dis\u00adcussed later in the paper. \nOf course, DART is overall complemen\u00adtary to static analysis since it has its own limitations, namely \nthe computational expense of running tests and the sometimes limited effectiveness of dynamic test generation \nto improve over random testing. In any case, DART offers a new trade-off among existing static and dynamic \nanalysis techniques. The paper is organized as follows. Section 2 presents an overview of DART. Section \n3 discusses implementation issues when dealing with programs written in the C programming lan\u00adguage. \nIn Section 4, experimental results are discussed. We com\u00adpare DART with other related work in Section \n5 and conclude with Section 6.  2. DART Overview DART s integration of random testing and dynamic test \ngeneration using symbolic reasoning is best intuitively explained with an ex\u00adample. 2.1 An Introduction \nto DART Consider the function h in the .le below: int f(int x) { return 2 * x; } int h(int x, int y) \n{ if(x != y) if (f(x) == x +10) abort(); /* error */ return 0; } The function h is defective because \nit may lead to an abort state\u00adment for some value of its input vector, which consists of the input parameters \nx and y. Running the program with random values of x and y is unlikely to discover the bug. The problem \nis typical of random testing: it is dif.cult to generate input values that will drive the program through \nall its different execution paths. In contrast, DART is able to dynamically gather knowledge about the \nexecution of the program in what we call a directed search. Starting with a random input, a DART-instrumented \npro\u00adgram calculates during each execution an input vector for the next execution. This vector contains \nvalues that are the solution of sym\u00adbolic constraints gathered from predicates in branch statements dur\u00ading \nthe previous execution. The new input vector attempts to force the execution of the program through a \nnew path. By repeating this process, a directed search attempts to force the program to sweep through \nall its feasible execution paths. For the example above, the DART-instrumented h initially guesses the \nvalue 269167349 for x and 889801541 for y.As a result, h executes the then-branch of the .rst if-statement, \nbut fails to execute the then-branch of the second if-statement; thus, no error is encountered. Intertwined \nwith the normal execution, the predicates x0 y0 and 2 \u00b7 x0= x0 +10 are formed on-the-.y = according to \nhow the conditionals evaluate; x0 and y0 are symbolic variables that represent the values of the memory \nlocations of vari\u00adables x and y. Note the expression 2 \u00b7 x0, representing f(x):it is de.ned through an \ninterprocedural, dynamic tracing of symbolic expressions. The predicate sequence (x0 = y0,2 \u00b7 x0 x0 +10), \ncalled = a path constraint, represents an equivalence class of input vectors, namely all the input vectors \nthat drive the program through the path that was just executed. To force the program through a different \nequivalence class, the DART-instrumented h calculates a solution to the path constraint (x0 y0,2 \u00b7 x0 \n= x0 +10) obtained by = negating the last predicate of the current path constraint. A solution to this \npath constraint is (x0 =10,y0 = 889801541) and it is recorded to a .le. When the instrumented h runs \nagain, it reads the values of the symbolic variables that have been solved from the .le. In this case, \nthe second execution then reveals the er\u00adror by driving the program into the abort() statement as expected. \n 2.2 Execution Model DART runs the program P under test both concretely, executing the actual program \nwith random inputs, and symbolically, calculat\u00ading constraints on values at memory locations expressed \nin terms of input parameters. These side-by-side executions require the pro\u00adgram P to be instrumented \nat the level of a RAM (Random Access Memory) machine. The memory M is a mapping from memory addresses \nm to, say, 32-bit words. The notation + for mappings denotes updating; ' for example, M:= M+[ m.v]is \nthe same map as M,ex\u00adcept that M' (m)= v. We identify symbolic variables by their addresses. Thus in \nan expression, m denotes either a memory ad\u00address or the symbolic variable identi.ed by address m, depending \non the context. A symbolic expression, or just expression, e can be ' '' of the form m, c (a constant), \n*(e,e) (a dyadic term denoting ''' ' multiplication), =(e,e)(a term denoting comparison), \u00ac(e)(a monadic \nterm denoting negation), *e' (a monadic term denoting pointer dereference), etc. Thus, the symbolic variables \nof an ex\u00adpression e are the set of addresses m that occur in it. Expressions have no side-effects. The \nprogram P manipulates the memory through statements that are specially tailored abstractions of the machine \ninstructions actually executed. There is a set of numbers that denote instruction addresses, that is, \nstatement labels. If e is the address of a statement (other than abort or halt), then e +1is guaranteed \nto also be an address of a statement. The initial address is e0. A statement can be a conditional statement \nc of the form if (e) then goto e ' (where 'e is an expression over symbolic variables and eis a statement \nlabel), an assignment statement a of the form m . e (where m is a memory address), abort, corresponding \nto a program error, or halt, corresponding to normal termination. The concrete semantics of the RAM machine \ninstructions of P is re.ected in evaluate concrete(e, M), which evaluates expres\u00adsion e in context Mand \nreturns a 32-bit value for e. Addition\u00adally, the function statement at(e, M) speci.es the next statement \nto be executed. For an assignment statement, this function calcu\u00adlates, possibly involving address arithmetic, \nthe address m of the left-hand side, where the result is to be stored; in particular, indirect addressing, \ne.g., stemming from pointers, is resolved at runtime to a corresponding absolute address.1 A program \nP de.nes a sequence of input addresses M 0, the addresses of the input parameters of P .An input vector \nIM, which associates a value to each input parameter, de.nes the initial value of M 0 and hence M.2 Let \nC be the set of conditional statements and A the set of assignment statements in P .A program execution \nw is a .nite3 sequence in Execs := (A .C) * (abort |halt). We prefer to view w as being of the form a1c1a2c2 \n... ckak+1s, where ai .A * (for 1=i =k +1), ci .C (for 1=i =k), and s .{abort, halt}. The concrete semantics \nof P at the RAM machine level allows us to de.ne for each input vector IMan execution sequence: the result \nof executing P on IM(the details of this semantics is not relevant for our purposes). Let Execs(P ) be \nthe set of such executions generated by all possible IM. By viewing each statement as a node, Execs(P \n) forms a tree, called the execution tree. Its assignment nodes have one successor; its conditional nodes \nhave one or two successors; and its leaves are labeled abort or halt.  2.3 Test Driver and Instrumented \nProgram The goal of DART is to explore all paths in the execution tree Execs(P ). To simplify the following \ndiscussion, we assume that we are given a theorem prover that decides, say, the theory of integer linear \nconstraints. This will allow us to explain how we handle the transition from constraints within the theory \nto those that are outside. DART maintains a symbolic memory Sthat maps memory ad\u00addresses to expressions. \nInitially, S is a mapping that maps each 1 We do this to simplify the exposition; left-hand sides could \nbe made symbolic as well. 2 To simplify the presentation, we assume that M 0 is the same for all executions \nof P . 3 We thus assume that all program executions terminate; in practice, this can be enforced by \nlimiting the number of execution steps. evaluate symbolic (e, M, S)= match e: case m: //the symbolic \nvariable named m if m .domainSthen return S(m) else return M(m) ' '' case *(e,e): //multiplication ' \nlet f = evaluate symbolic(e, M, S); '' let f = evaluate symbolic(e, M, S); if not one of f' or f'' is \na constant c then all linear =0 return evaluate concrete(e, M) if both f' and f'' are constants then \nreturn evaluate concrete(e, M) if f' is a constant c then return *(f' ,c) else return *(c, f'' ) case \n*e' : //pointer dereference ' let f = evaluate symbolic(e, M, S); if f' is a constant c then if *c .domainSthen \nreturn S(*c) else return M(*c) else all locs de.nite =0 return evaluate concrete(e, M) etc.     \n Figure 1. Symbolic evaluation m . M 0 to itself. Expressions are evaluated symbolically as de\u00adscribed \nin Figure 1. When an expression falls outside the theory, as in the multiplication of two non-constant \nsub-expressions, DART simply falls back on the concrete value of the expression, which is used as the \nresult. In such a case, we also set a .ag all linear to 0, which we use to track completeness. Another \ncase where DART s directed search is typically incomplete is when the program deref\u00aderences a pointer \nwhose value depends on some input parameter; in this case, the .ag all locs de.nite is set to 0and the \nevaluation falls back again to the concrete value of the expression. With this eval\u00aduation strategy, \nsymbolic variables of expressions in Sare always contained in M 0. To carry out a search through the \nexecution tree, our instru\u00admented program is run repeatedly. Each run (except the .rst) is executed with \nthe help of a record of the conditional statements executed in the previous run. For each conditional, \nwe record a branch value, which is either 1 (the then branch is taken) or 0 (the else branch is taken), \nas well as a done value, which is 0 when only one branch of the conditional has executed in prior runs \n(with the same history up to the branch point) and is 1 oth\u00aderwise. This information associated with \neach conditional state\u00adment of the last execution path is stored in a list variable called stack, kept \nin a .le between executions. For i, 0 = i< |stack|, stack[i]=(stack[i].branch, stack[i].done)is thus \nthe record corre\u00adsponding to the i +1th conditional executed. More precisely, our test driver run DART \nis shown in Figure 2. This driver combines random testing (the repeat loop) with directed search (the \nwhile loop). If the instrumented program throws an exception, then a bug has been found. The two completeness \n.ags, namely all linear and all locs de.nite, each holds unless a bad situation possibly leading to incompleteness \nhas occurred. Thus, if the directed search terminates that is, if directed of the inner loop no longer \nholds then the outer loop also terminates provided all of the completeness .ags still hold. In this case, \nDART terminates and safely reports that all feasible program paths have been explored. But if just one \nof the completeness .ags have been turned off at some point, then the outer loop continues forever (modulo \nresource constraints not shown here).    run DART () = all linear, all locs de.nite, forcing ok =1,1,1 \nrepeat stack = (); IM=[] ; directed =1 while (directed) do try (directed, stack, IM)= instrumented program(stack, \nIM) catch any exception . if (forcing ok) print Bug found exit() else forcing ok =1 until all linear \n.all locs de.nite       Figure 2. Test driver instrumented program(stack,IM)= // Random initialization \nof uninitialized input parameters in MM0 for each input x with IM[x]unde.ned do IM[x]= random() Initialize \nmemory Mfrom MM0 and IM // Set up symbolic memory and prepare execution S =[m .m |m .MM0]. e = e0 // \nInitial program counter in P k = 0// Number of conditionals executed // Now invoke P intertwined with \nsymbolic calculations s = statement at(e,M) while (s /  .{abort, halt}) do match (s) case (m .e): \nS= S+ [m .evaluate symbolic(e, M, S)] v = evaluate concrete(e, M) M= M+[m .v]; e = e +1 case (if (e)then \ngoto e ' ): b = evaluate concrete(e, M) c = evaluate symbolic(e, M, S) if b then path constraint = path \nconstraint ^ (c)stack = compare and update stack(1,k,stack) e = e ' else path constraint = path constraint \n^ (neg(c))stack = compare and update stack(0,k,stack) e = e +1 k= k +1 s =statement at(e,M) // End of \nwhile loop if (s==abort) then raise an exception else // s==halt return solve path constraint(k,path \nconstraint,stack) Figure 3. Instrumented program The instrumented program itself is described in Figure \n3 (where ^ denotes list concatenation). It executes as the original pro\u00adgram, but with interleaved gathering \nof symbolic constraints. At each conditional statement, it also checks by calling com\u00adpare and update \nstack, shown in Figure 4, whether the current execution path matches the one predicted at the end of \nthe pre\u00advious execution and represented in stack passed between runs. Speci.cally, our algorithm maintains \nthe invariant that when in\u00adstrumented program is called, stack[|stack|-1].done =0holds.    compare \nand update stack(branch,k,stack)= if k< |stack|then if stack[k].branch =branch then forcing ok =0 raise \nan exception else if k =|stack|-1then stack[k].branch = branch stack[k].done =1 else stack = stack ^ \n((branch, 0))return stack Figure 4. Compare and update stack solve path constraint(ktry ,path constraint,stack)= \nlet j be the smallest number such that for all h with -1=j<h<ktry, stack[h].done =1 if j =-1then return \n(0, , )// This directed search is over else path constraint[j]= neg(path constraint[j]) stack[j].branch= \n\u00acstack[j].branch if (path constraint[0,... ,j]has a solution IM' ) then return (1, stack[0..j], IM+IM' \n) else solve path constraint(j,path constraint,stack) Figure 5. Solve path constraint This value is \nchanged to 1if the execution proceeds according to all the branches in stack as checked by compare and \nupdate stack. If it ever happens that a prediction of the outcome of a conditional is not ful.lled, then \nthe .ag forcing ok is set to 0and an exception is raised to restart run DART with a fresh random input \nvector. Note that setting forcing ok to 0 can only be due to a previous incompleteness in DART s directed \nsearch, which was then (con\u00adservatively) detected and resulted in setting (at least) one of the completeness \n.ags to 0. In other words, the following invariant always holds: all linear .all locs de.nite .forcing \nok. When the original program halts, new input values are generated in solve path constraint, shown in \nFigure 5, to attempt to force the next run to execute the last4 unexplored branch of a conditional along \nthe stack. If such a branch exists and if the path constraint that may lead to its execution has a solution \nIM' , this solution is used to update the mapping IMto be used for the next run; values corre\u00adsponding \nto input parameters not involved in the path constraint are preserved (this update is denoted IM+IM' \n). The main property of DART is stated in the following theorem, which formulates (a) soundness (of error \nfounds) and (b) a form of completeness. THEOREM 1. Consider a program P as de.ned in Section 2.2. (a) \nIf run DART prints out Bug found for P , then there is some input to P that leads to an abort. (b) If \nrun DART terminates without printing Bug found, then there is no input that leads to an abort statement \nin P , and all paths in Execs(P )have been exercised. (c) Otherwise, run DART will run forever. Proofs \nof (a) and (c) are immediate. The proof of (b) rests on the assumption that any potential incompleteness \nin DART s directed search is (conservatively) detected and recorded by setting at least one of the two \n.ags all linear and all locs de.nite to 0. 4 A depth-.rst search is used for exposition, but the next \nbranch to be forced could be selected using a different strategy, e.g., randomly or in a breadth\u00ad.rst \nmanner.           Since DART performs (typically partial) symbolic executions only as generalizations \nof concrete executions, a key difference be\u00adtween DART and static-analysis-based approaches to software \nver\u00adi.cation is that any error found by DART is guaranteed to be sound (case (a) above) even when using \nan incomplete or wrong theory. In order to maximize the chances of termination in case (b) above, set\u00adting \noff completeness .ags as described in evaluate symbolic could be done less conservatively (i.e., more \naccurately) using various op\u00adtimization techniques, for instance by distinguishing incomplete\u00adness in \nexpressions used in assignments from those used in condi\u00adtional statements, by re.ning after each conditional \nstatement the constraints stored in S that are associated with symbolic variables involved in the conditional, \nby dealing with pointer dereferences in a more sophisticated way, etc. 2.4 Example Consider the C program: \nint f(int x, int y) { int z; z=y; if (x == z) if (y ==x +10) abort(); return 0; } M The input address \nvector is M0 = (mx,my) (where mx = my are some memory addresses) for f s input parameters (x,y). Let \nus assume that the .rst value for x is 123456 and that of y is 654321, that is, IM= (123456,654321). \nThen, the initial concrete memory becomes M =[mx . 123456,my . 654321], and the initial symbolic memory \nbecomes S =[mx . mx,my . my]. During execution from this con.guration, the else branch of the outer if \nstatement is taken and, at the time halt is encoun\u00adtered, the path constraint is (\u00ac(mx = my)). We have \nk =1, stack = ((0,0)), S =[mx . mx,my . my,mz . my], M =[mx . 123456,my . 654321,mz . 654321]. The sub\u00adsequent \ncall to solve path constraint results in an attempt to solve (mx = my), which leads to a solution (mx \n. 0,my . 0). The updated input vector IM+ IM' is then (0,0), the branch bit in stack has been .ipped, \nand the assignment (directed, stack, IM)=(1, ((1,0)), (0,0)) is executed in run DART. During the second \ncall of instrumented program, the compare and update stack will check that the actually executed branch \nof the outer if statement is now the then branch (which it is!). Next, the else branch of the inner if \nstatement is executed. Consequently, the path constraint that is now to be solved is (mx = my,my = mx \n+10). The run DART driver then calls solve path constraint with (ktry,path constraint, stack)=(2, (mx \n= my,my = mx+10), ((1,1),(0,0))). Since this path constraint has no solution, and since the .rst conditional \nhas already been covered (stack[0].done =1), solve path constraint returns (0, , ). In turn, run DART \nterminates since all complete\u00adness .ags are still set.  2.5 Advantages of the DART approach Despite \nthe limited completeness of DART when based on linear integer constraints, dynamic analysis often has \nan advantage over static analysis when reasoning about dynamic data. For example, to determine if two \npointers point to the same memory location, DART simply checks whether their values are equal and does \nnot require alias analysis. Consider the C program: struct foo { int i; char c; }bar (struct foo *a) \n{ if (a->c == 0) { *((char *)a + sizeof(int)) = 1; if (a->c != 0) abort(); }} DART here treats the pointer \ninput parameter by randomly initial\u00adizing it to NULL or to a single heap-allocated cell of the appropri\u00adate \ntype (see Section 3.2). For this example, a static analysis will typically not be able to report with \nhigh certainty that abort() is reachable. Sound static analysis tools will report the abort might be \nreachable , and unsound ones (like BLAST [18] or SLAM [2]) will simply report no bug found , because \nstandard alias analysis is not able to guarantee that a->c has been overwritten. In contrast, DART .nds \na precise execution leading to the abort very easily by simply generating an input satisfying the linear \nconstraint a->c == 0. This kind of code is often found in implementations of net\u00adwork protocols, where \na buffer of type char * (e.g., representing a message) is occasionally cast into a struct (e.g., representing \nthe different .elds of the protocol encoded in the message) and vice versa. The DART approach of intertwined \nconcrete and symbolic ex\u00adecution has two important advantages. First, any execution lead\u00ading to an error \ndetected by DART is trivially sound. Second, it al\u00adlows us to alleviate the limitations of the constraint \nsolver/theorem prover. In particular, whenever we generate a symbolic condition at a branching statement \nwhile executing the program under test, and the theorem prover cannot decide whether that symbolic con\u00addition \nis true or false, we simply replace this symbolic condition by its concrete value, i.e., either true \nor false. This allows us to continue both the concrete and symbolic execution in spite of the limitation \nof the theorem prover. Note that static analysis tools us\u00ading predicate abstraction [2, 18] will simply \nconsider both branches from that branching point, which may result in unsound behaviors. A test-generation \ntool using symbolic execution [36], on the other hand, will stop its symbolic execution at that point \nand may miss bugs appearing down the branch. To illustrate this point, consider the following C program: \n1 foobar(int x, int y){ 2 if (x*x*x > 0){ 3 if (x>0 &#38;&#38; y==10) 4 abort(); 5 } else { 6 if (x>0 \n&#38;&#38; y==20) 7 abort(); 8 } 9 } Given a theorem prover that cannot reason about non-linear arith\u00admetic \nconstraints, a static analysis tool using predicate abstrac\u00adtion [2, 18] will report that both aborts \nin the above code may be reachable, hence one false alarm since the abort in line 7 is un\u00adreachable. \nThis would be true as well if the test (x*x*x > 0) is replaced by a library call or if it was dependent \non a con.guration parameter read from a .le. On the other hand, a test-generation tool based on symbolic \nexecution [36] will not be able to generate an in\u00adput vector to detect any abort because its symbolic \nexecution will be stuck at the branching point in line 2. In contrast, DART can generate randomly an \ninput vector where x>0 and y!=10 with al\u00admost 0.5 probability; after the .rst execution with such an \ninput, the directed search of DART will generate another input with the same positive value of x but \nwith y==10, which will lead the program in its second run to the abort at line 4. Note that, if DART \nrandomly generates a negative value for x in the .rst run, then DART will generate in the next run inputs \nwhere x>0 and y==20 to satisfy the other branch at line 7 (it will do so because no constraint is gen\u00aderated \nfor the branching statement in line 2 since it is non-linear); however, due to the concrete execution, \nDART will then not take the else branch at line 6 in such a second run. In summary, our mixed strategy \nof random and directed search along with simulta\u00adneous concrete and symbolic execution of the program \nwill allow us to .nd the only reachable abort statement in the above example with high probability. \n 3. DART for C We now discuss how to implement the algorithms presented in the previous section for \ntesting programs written in the C programming language. 3.1 Interface Extraction Given a program to \ntest, DART .rst identi.es the external inter\u00adfaces through which the program can obtain inputs via uninitialized \nmemory locations MM0. In the context of C, we de.ne the external interfaces of a C program as its external \nvariables and external functions (reported as un\u00adde.ned reference at the time of compilation of the program), \nand  the arguments of a user-speci.ed toplevel function, which is a function of the program called to \nstart its execution.  The main advantage of this de.nition is that the external interfaces of a C program \ncan be easily determined and instrumented by a light-weight static parsing of the program s source code. \nInputs to a C program are de.ned as memory locations which are dynami\u00adcally initialized at runtime through \nthe static external interface. This allows us to handle inputs which are dynamic in nature, such as lists \nand trees, in a uniform way. Considering inputs as uninitialized runtime memory locations, instead of \nsyntactic objects exclusively such as program variables, also allows us to avoid expensive or im\u00adprecise \nalias analyses, which form the basis of many static analysis tools. Note that the (simpli.ed) formalization \nof Section 2.2 assumed that the input addresses MM0 are the same for all executions of program P . However, \nour implementation of DART supports a more general model where multiple inputs can be mapped to a same \naddress m when these are obtained by successively reading m during different successive calls to the \ntoplevel function, as will be discussed later, as well as the possibility of a same input being mapped \nto different addresses in different executions, for instance when the input is provided through an address \ndynamically allocated with malloc(). For each external interface, we determine the type of the input \nthat can be passed to the program via that interface. In C, a type is de.ned recursively as either a \nbasic type (int, .oat, char, enum, etc.), a struct type composed of one or more .elds of other types, \nan array of another type, or a pointer to another type. Figure 6 shows a simple example of C program \nsimulating a controller for an air-conditioning (AC) system. The toplevel func\u00adtion is ac controller, \nand the external interface is simply its ar\u00adgument message, of basic type int. It is worth emphasizing \nthat we distinguish three kinds of C functions in this work. Program functions are functions de.ned \nin the program.  External functions are functions controlled by the environment and hence part of the \nexternal interface of the program; they can nondeterministically return any value of their speci.ed return \ntype.   /* initially, */ int is_room_hot=0; /* room is not hot */ int is_door_closed=0; /* and door \nis open */ int ac=0; /* so, ac is off */ void ac_controller(int message) {if (message == 0) is_room_hot=1; \nif (message == 1) is_room_hot=0; if (message == 2) { is_door_closed=0; ac=0; } if (message == 3) { is_door_closed=1; \nif (is_room_hot) ac=1; } if (is_room_hot &#38;&#38; is_door_closed &#38;&#38; !ac) abort(); /* check \ncorrectness */ } Figure 6. AC-controller example (C code) Library functions are functions not de.ned \nin the program but controlled by the program, and hence considered as part of it. Examples of such functions \nare operating-system functions and functions de.ned in the standard C library. These functions are treated \nas unknown but deterministic black-boxes which we cannot instrument or analyze. The ability of DART \nto handle deterministic but unknown (and arbitrarily complex) library functions by simply executing these \nmakes it unique compared to standard symbolic-execution based frameworks, as discussed in Section 2.4. \nIn practice, the user can adjust the boundary between library and external functions to sim\u00adulate desired \neffects. For instance, errors in system calls can easily be simulated by considering the corresponding \nsystem functions as external functions instead of library functions. 3.2 Generation of Random Test Driver \nOnce the external interfaces of the C program are identi.ed, we generate a nondeterministic/random test \ndriver simulating the most general environment visible to the program at its interfaces. This test driver \nis itself a C program, which performs the random initial\u00adization abstractly described at the beginning \nof the function instru\u00admented program() in Section 2, and which is de.ned as follows: The test driver \nconsists of a function main which initializes all external variables and all arguments of the toplevel \nfunction with random values by calling the function random init de\u00ad.ned below, and then calls the application \ns toplevel function. The user of DART speci.es (using the parameter depth) the number of times the toplevel \nfunction is to be called iteratively in a single run.  The test driver also contains code simulating \neach external function in such a way that, whenever an external function is called during the program \nexecution, a random value of the function s return type is returned by the simulated function.  For \nexample, Figure 7 shows the test driver generated for the AC-controller example of Figure 6. The initialization \nof memory locations controlled by the exter\u00adnal interface is performed using the procedure random init \nshown in Figure 8. This procedure takes as arguments a memory location m and the type of the value to \nbe stored at m, and initializes ran\u00addomly the location m depending on its type. If m stores a value of \n  void main() {for (i=0; i < depth ; i++) {int tmp; random_init(&#38;tmp,int); ac_controller(tmp); }} \nFigure 7. Test driver generated for the AC-controller example (C code) random_init(m,type) {if (type \n== pointer to type2) {if (fair coin toss == head) {*m = NULL; } else { *m = malloc(sizeof(type)); random_init(*m,type2); \n }} else if (type == struct) {for all fields f in struct random_init(&#38;(m->f),typeof(f)); } else if \n(type == array[n] of type3){for (int i=0;i<n;i++) random_init((m+i),type3); } else if (type == basic \ntype) {*m = random_bits(sizeof(type)); }} Figure 8. Procedure for randomly initializing C variables of \nany type (in pseudo-C) basic type, its value *m5 is initialized with the auxiliary procedure random bits \nwhich returns n random bits where n is its argument. If its type is a pointer, the value of location \nm is randomly initial\u00adized with either the value NULL (with a 0.5 probability) or with the address of \nnewly allocated memory location, whose value is in turn initialized according to its type following the \nsame recursive rules. If type is a struct or an array, every sub-element is initial\u00adized recursively \nin the same way. Note that, when inputs are data structures de.ned with a recursive type (such as lists), \nthis general procedure can thus generate data structures of unbounded sizes. For each external variable \nor argument to the toplevel function, say v, DART generates a call to random init(&#38;v,typeof(v)) in \nthe function main of the test driver before calling the toplevel function. For instance, in the case \nof the AC-controller pro\u00adgram, the variable message forming the external interface is of type int, and \ntherefore the corresponding initialization code random init(&#38;tmp,int)6 is generated (see Figure 7). \nSimilarly, if the C program being tested can call an external function, say return type some fun(), then \nthe test driver gen\u00aderated by DART will include a de.nition for this function, which is as follows: return_type \nsome fun(){ return_type tmp; random init(&#38;tmp,return type); return tmp; } 5 In C, *m denotes the \nvalue stored at m. 6 In C, &#38;v gives the memory location of the variable v.      Once the test \ndriver has been generated, it can be combined with the C program being tested to form a self-executable \nprogram, which can be compiled and executed automatically. 3.3 Implementation of Directed Search A directed \nsearch can be implemented using a dynamic instrumen\u00adtation as explained in Section 2. The main challenge \nwhen deal\u00ading with C is to handle all the possible types that C allows, as well as generate and manipulate \nsymbolic constraints, especially across function boundaries (i.e., tracking inputs through function calls \nwhen a variable whose value depends on an input is passed as argument to another program function). This \nis tedious (because of the complexity of C) but conceptually not very hard. In our implementation of \nDART for C, the code instrumentation needed to intertwine the concrete execution of the program P with \nthe symbolic calculations performed by DART as described in function instrumented program() (see Section \n2) is performed using CIL [28], an OCAML application for parsing and analyzing C code. The constraint \nsolver used by default in our implementation is lp solve [26], which can solve ef.ciently any linear \nconstraint using real and integer programming techniques. 3.4 Additional Remarks For the sake of modeling \nrealistic external environments, we have assumed in this work that the execution of external functions \ndo not have any side effects on (i.e., do not change the value of) any previously-de.ned stack or heap \nallocated program variable, in\u00adcluding those passed as arguments to the function. For instance, an external \nfunction returning a pointer to an int can only return NULL or a pointer to a newly allocated int, not \na pointer to a pre\u00adviously allocated int. Note that this assumption does not restrict generality: external \nfunctions with side effects or returning previ\u00adously de.ned heap-allocated objects can be simulated by \nadding interface code between the program and its environment. Another assumption we made is that all \nprogram variables (i.e., all those not controlled by the environment) are properly initialized. Detecting \nuninitialized program variables can be done using other analyzes and tools, either statically (e.g., \nwith lint [19]) or dy\u00adnamically (e.g., with Purify [17]) or both (e.g., with CCured [29]). Instead of \nusing a static de.nition of interface for C programs as done above in this section, we could have used \na dynamic de.\u00adnition, such as considering any uninitialized variable (memory lo\u00adcation) read by the program \nas an input. In general, detecting in\u00adputs with such a loose de.nition can only be done dynamically, \nusing a dynamic program instrumentation similar to one for de\u00adtecting uninitialized variables. Such instrumentations \nrequire a pre\u00adcise, hence expensive, tracking of memory accesses. Discovering and simulating external \nfunctions on-the-.y is also challenging. It would be worth exploring further how to deal with dynamic \ninter\u00adface de.nitions.  4. Experimental Evaluation In this section, we present the results of several \nexperiments per\u00adformed with DART. We .rst compare the ef.ciency of a purely random search with a directed \nsearch using two program examples. We then discuss the application of DART on a larger application. All \nexperiments were performed on a Pentium III 800Mhz proces\u00adsor running Linux. Runtime is user+system time \nas reported by the Unix time command and is always roughly equal to elapsed time. 4.1 AC-controller Example \nOur .rst benchmark is the AC-controller program of Figure 6. If we set the depth to 1, the program does \nnot have any execution leading to an assertion violation. For this example, a directed search explores \nall execution paths upto that depth in 6 iterations and less than a second. In contrast, a random search \nwould thus runs forever without detecting any errors. If we set the depth to 2, there is an assertion \nviolation if the .rst input value is 3 and the second input value is 0. This scenario is found by the \ndirected search in DART in 7 iterations and less than a second. In contrast, a random search does not \n.nd the assertion violation after hours of search. Indeed, if message is a 32-bit integer, the probability \nfor a random search to .nd the speci.c combination of inputs leading to this assertion  32 32 violation \nis one out of 2\u00d7 2=264, i.e., virtually zero in practice! This explains why a directed search usually \nprovides much better code coverage than a simple random search. Indeed, most applications contain input-.ltering \ncode that performs basic sanity checks on the inputs and discards the bad or irrelevant ones. Only inputs \nthat satisfy these .ltering tests are then passed to the core application and can in.uence its behavior. \nFor instance, in the AC-controller program, only values 0to 3are meaningful inputs while all others are \nignored; the directed mode is crucial to identify (iteratively) those meaningful input values. It is \nworth observing how a directed search can learn through trial and error how to generate inputs that satisfy \nsuch .ltering tests. Each way to pass these tests corresponds to an execution path through the input-.ltering \ncode that leads to the core application code. Every such path will eventually be discovered by the directed \nsearch provided it can reason about all the constraints along the path. When this happens, the directed \nsearch will reach and start exercizing (in the same smart way) the core application code. In contrast, \na purely random search will typically be stuck forever in the input-.ltering code and will never exercize \nthe code of the core application.  4.2 Needham-Schroeder Protocol Our second benchmark example is a \nC implementation of the Needham-Schroeder public key authentication protocol [30]. This protocol aims \nat providing mutual authentication, so that two par\u00adties can verify each other s identity before engaging \nin a transac\u00adtion. The protocol involves a sequence of message exchanges be\u00adtween an initiator,a responder, \nand a mutually-trusted key server. The exact details of the protocol are not necessary for the dis\u00adcussion \nthat follows and are omitted here. An attack against the original protocol involving six message exchanges \nwas reported by Lowe in [24]: an intruder I is able to impersonate an initiator A to set up a false session \nwith responder B, while B thinks he is talking to A. The steps of Lowe s attack are as follows: 1. A \n. I : {Na,A}Ki (A starts a normal session with I by sending it a nonce Na and its name A, both encrypted \nwith I s public key Ki) 2. I(A). B : {Na,A}Kb (the intruder I impersonates A to try to establish a false \nsession with B) 3. B . I(A): {Na,Nb}Ka (B responds by selecting a new nonce Nb and trying to return \nit with Na to A) 4. I . A :{Na,Nb}Ka (I simply forwards B s last message to A; note that I does not \nknow how to decrypt B s message to Asince it is encrypted with A s key Ka) 5. A . I :{Nb}Ki (A decrypts \nthe last message to obtain Nb and returns it to I) 6. I(A). B : {Nb}Kb (I can then decrypt this message \nto obtain Nb and returns it to B; after receiving this message, B believes that Ahas correctly established \na session with it)  The C implementation of the Needham-Schroeder protocol we considered7 is described \nby about 400 lines of C code and is more 7 We thank John Havlicek for providing us this implementation. \ndepth error? Random search Directed search 1 no - 69 runs (<1 second) 2 yes - 664 runs (2 seconds)  \nFigure 9. Results for Needham-Schroeder protocol with a possi\u00adbilistic intruder model depth error? Iterations \n(runtime) 1 no 5 runs (<1 second) 2 no 85 runs (<1 seconds) 3 no 6,260 runs (22 seconds) 4 yes 328,459 \nruns (18 minutes)  Figure 10. Results for Needham-Schroeder protocol with a Dolev-Yao intruder model \ndetailed than the protocol description analyzed in [24]. The C pro\u00adgram simulates the behavior of both \nthe initiator A and responder B according to the protocol rules. It can be executed as a single Unix \nprocess simulating the interleaved behavior of both protocol entities. It also contains an assertion \nthat is violated whenever an attack to the protocol occurs.8. In the C program, agent identi.ers, keys, \naddresses and nounces are all represented by integers. The program takes as inputs tuples of integer \nvalues representing in\u00adcoming messages. Results of experiments are presented in Figure 9. When at most \none (depth is 1) message is sent to the initiator or responder, there is no program execution leading \nto an assertion violation. The table indicates how many iterations (runs) of the program are needed by \nDART s directed search to reach this conclusion. This number thus represents all possible execution paths \nof this protocol implementation when executed once. When two input messages are allowed, DART .nds an \nassertion violation in 664 iterations or about two seconds of search. In contrast, a random search is \nnot able to .nd any assertion violations after many hours of search. An examination of the program execution \nleading to this asser\u00adtion violation reveals that DART only .nds part of Lowe s attack: it .nds the projection \nof the attack from B s point of view, i.e., steps 2 and 6 above. In other words, DART .nds that, when \nplaced in its most general environment, there exists a sequence of two input messages that drives this \ncode to an assertion violation. However, the most general environment, which can generate any valid input \nat any time, is too powerful to model a realistic intruder I: for in\u00adstance, given a conditional statement \nof the form if (input == my secret) then ... , DART can set the value of this input to my secret to direct \nits testing, which is as powerful as being able to guess encryption keys hard-coded in the program. (Such \na most powerful intruder model is sometimes called a possibilistic attacker model in the literature.) \nTo .nd the complete Lowe s attack, it is necessary to use a more constrained model of the environment \nthat models more precisely the capabilities of the intruder I, namely I s ability to only decrypt messages \nencrypted with its own key Ki, to compose messages with only nonces it already knows, and to forward \nonly messages it has previously seen. (Such a model is called a Dolev-Yao attacker model in the security \nliterature.) We then augmented the original code with such a model of I. We quickly discovered that there \nare many ways to model I and that each variant can have a signi.cant impact on the size of the resulting \nsearch space. Figure 10 presents the results obtained with one of these models, which is at least as \nunconstrained as the original intruder model of [24] yet results in the smallest state space we could \nget. In this 8 A C assertion violation (as de.ned in <assert.h>) triggers an abort(). version, the intruder \nmodel acts as an input .lter for entities A and B. As shown in the Figure, the shortest sequence of inputs \nleading to an assertion violation is of length 4 and DART takes about 18 minutes of search to .nd it. \nThis time, the corresponding execution trace corresponds to the full Lowe s attack: (After no speci.c \ninput) A sends its .rst message as in Step 1 (depth 1).  B receives an input and sends an output as \nin Step 3 (depth 2).  A receives an input and sends an output as in Step 5 (depth 3).  B receives an \ninput as in Step 6, which then triggers an assertion violation (depth 4).  Note that, since the initiator \nI is modeled as an input .lter, Steps 2 and 4 are not represented explicitly by additional messages. \nThe original code we started with contains a .ag which, if turned on, implements Lowe s .x to the Needham-Schroeder \npro\u00adtocol [25]. By curiosity, we also tested this version with DART and, to our surprise, DART found \nagain an assertion violation after about 22 minutes of search! After examining the error trace produced \nby DART, we discovered that the implementation of Lowe s .x was incomplete. We contacted the author of \nthe original code and he con.rmed this was a bug he was not aware of. After .xing the code, DART was \nno longer able to .nd any assertion violation. It is interesting to compare these results with the ones \nre\u00adported in [13] where the same C implementation of the Needham-Schroeder protocol was analyzed using \nstate-space exploration techniques. Speci.cally, [13] studied the exploration of the (very large) state \nspace formed by the product of this C implementation in conjunction with a nondeterministic C model of \nthe intruder. The tool VeriSoft [12] was used to explore the product of these two interacting Unix processes. \nSeveral search techniques were experimented with. To summarize the results of [13], neither a sys\u00adtematic \nsearch nor a random search through that state space were able to detect the attack (within 8 hours of \nsearch). But a random search guided using application-independent heuristics (essentially maximizing \nthe number of messages exchanged between the two processes) was able to .nd the attack after 50 minutes \nof search on average, on a comparable machine. So far, we have not explored the use of heuristics in \nthe context of DART. Because the intruder model in the implementations of the Needham-Schroeder protocol \nconsidered here and in [13] are dif\u00adferent, a direct comparison between our results and the results of \n[13] is not possible. Yet, DART was able to .nd Lowe s attack using a systematic search (and to discover \na previously-unknown bug in the implementation of Lowe s .x), while the experimental setup of [13] was \nnot. This performance difference can perhaps be explained intuitively as follows. A standard model checking \nap\u00adproach as taken in [13] (at a protocol implementation level) and also in [24] (at a more abstract \nprotocol speci.cation level) repre\u00adsents the program s environment (here the intruder) by a nondeter\u00administic \nprocess that blindly guesses possible sequences of inputs (attacks), and then checks the effect of these \non the program by performing state-space exploration. In contrast, a directed search as implemented in \nDART does not treat the program under test as a black-box. Instead, the directed search attempts to partition \niteratively the program s input space into equivalence classes, and generate new inputs in order to exhibit \nnew program responses in\u00adputs that trigger a previously considered program behavior are not generated \nand re-tested over and over again. In this sense, a directed search can be viewed as a more white-box \napproach than traditional model checking since the observation of how the program reacts to speci.c inputs \nis used to generate the next test inputs. Since a directed search exploits more information about the \nprogram being tested, it is not surprising that it can (and should) be more effective.  4.3 A Larger \nApplication: oSIP In order to evaluate further the effectiveness and scalability of DART, we applied \nit to test a large application of industrial rele\u00advance: oSIP, an open-source implementation of the Session \nInitia\u00adtion Protocol. SIP is a telephony protocol for call-establishment of multi-media sessions over \nIP networks (including Voice-over-IP). oSIP is a C library available at http://www.gnu.org/software/osip/osip.html. \n The oSIP library (version 2.0.9) consists of about 30,000 lines of C code describing about 600 externally \nvisible functions which can be used by higher-level applications. Two typical such applications are SIP \nclients (such as softphones to make calls over the internet from a PC) and servers (to route internet \ncalls). Our experimental setup was as follows. Since there is very little documentation on the API provided \nby the oSIP library other than the code itself, we considered one-by-one each of the about 600 externally \nvisible functions as the toplevel function that DART calls. These function names were automatically extracted \nfrom the library using scripts. For each toplevel function, the inputs controlled by DART were the arguments \nof the function, and the search was limited to a maximum of 1,000 iterations (runs). In other words, \nif DART did not .nd any errors after 1,000 runs, the script would then move on to the next toplevel function, \nand so on. Since the oSIP code does not contain assertions, the search was limited to .nding segmentation \nfaults (crashes) and non-termination.9 The results obtained with DART were surprising to us: DART found \nhundreds of ways to crash externally visible oSIP functions. In fact, DART found a way to crash 65% of \nthe oSIP functions within 1,000 attempts for each function. A closer analysis of the results revealed \nthat most of these crashes share the same basic pattern: an oSIP function takes as argument a pointer \nto a data structure and then de-references later that pointer without checking .rst whether the pointer \nis non-NULL. It is worth noticing that some oSIP functions do contain code to test for NULL pointers, \nbut most do not perform such tests consistently (i.e., for all execution paths), and the the documentation \ndoes not distinguish the former category of functions from the latter. Also note that a simple visual \ncode inspection would have revealed most of these problems. Because DART reported so many errors and \nbecause of the lack of a speci.cation for the oSIP API, it is hard to evaluate how severe these problems \nreally are. Perhaps the implicit assumption for higher-level applications is that they must always pass \nnon-NULL pointers to the oSIP library, but then it is troubling to see that some of the oSIP functions \ndo check their arguments for NULL pointers. All we can conclude with good certainty is that, from the \npoint of view of a higher-level application developer, there are many ways to misuse the API, and that \nprogramming errors in higher-level code (such as mistakenly passing a NULL pointer to some unguarded \noSIP function) could result in dramatic failures (crashes). Overwhelmed by the large number of potential \nproblems re\u00adported by DART, we decided to focus on the oSIP functions called in a test driver provided \nwith the oSIP library, and to analyze in detail the results obtained for these functions. In the process, \nwe discovered what appears to be a signi.cant security vulnerability in oSIP: we found an externally \ncontrollable way to crash the oSIP parser. Speci.cally, the attack is as follows: Build an (ASCII) SIP \nmessage containing no NULL (zero) or | characters, and of more than 2.5 Megabytes (for a cygwin environment \n the size may vary on other platforms). 9 Non-termination is reported by DART after a timer expiration \ntriggered when the program under test does not call any DART instrumentation within a speci.c time delay. \n  Pass it to oSIP parser using the oSIP function osip message parse .  One of the .rst thing this \nfunction does is to copy this packet in stack space using the system call alloca(size). This system call \nreturns a pointer to size bytes of uninitialized local stack space, or NULL if the allocation failed. \nSince 2.5 Megabytes is larger than the standard stack space available for cygwin processes, an error \nis reported and NULL is returned.  The oSIP code does not check success/failure of the call to alloca, \nand pass the pointer blindly to another oSIP function, which does not check this input argument and then \ncrashes because of the NULL pointer value.  By modifying the test driver that comes with oSIP and generat\u00ading \nan input SIP message that satis.es these constraints, we were able to con.rm this attack. This is a potentially \nvery serious .aw in oSIP: it could be possible to kill remotely any SIP client or server relying on the \noSIP library for parsing SIP messages by sim\u00adply sending it a message satisfying the simple properties \ndescribed above! However, we do not know whether existing SIP clients or servers (i.e., higher-level \napplications) built using oSIP are vulner\u00adable to this attack. Note that, as of version 2.2.0 of the \noSIP library (December 2004), this code has been .xed (see comments in the ChangeLog .le).  5. Other \nRelated Work Automatically closing an open reactive program with its most gen\u00aderal environment to make \nit self-executable and to systematically explore all its possible behaviors was already proposed in [8]. \nHow\u00adever, the approach taken there is to use static analysis and code transformation in order to eliminate \nthe external interface of the open program and to replace with nondeterministic statements all conditional \nstatements whose outcome may depend on an input value. The resulting closed program is a simpli.ed version \n(abstrac\u00adtion) of the original open program that is guaranteed to simulate all its possible behaviors. \nIn comparison, DART is more precise both because it does not abstract the program under test and because \nit does not suffer from the inherent imprecision of static analysis. The article [35] explores how to \nachieve the same goal as [8] by partitioning the program s input domain using static analysis. Be\u00adcause \n[35] does not rely on abstraction (program simpli.cations), it can be more precise than [8], but it still \nsuffers from the cost and imprecision of static analysis compared to DART. There is a rich literature \non test-vector generation using sym\u00adbolic execution (e.g., see [21, 27, 10, 3, 36, 38, 9]). Symbolic \nex\u00adecution is limited in practice by the imprecision of static analysis and of theorem provers. As illustrated \nby the examples in Section 2, DART is able to alleviate some of the limitations of symbolic exe\u00adcution \nby exploiting dynamic information obtained from a concrete execution matching the symbolic constraints, \nby using dynamic test generation, and by instrumenting the program to check whether the input values \ngenerated next have the expected effect on the pro\u00adgram. The ability of DART to handle complex or unknown \ncode segments (including library functions) as black-boxes by simply executing these makes it unique \ncompared to standard symbolic\u00adexecution based frameworks, which require some knowledge (min\u00adimally regarding \ntermination) about all program parts or are incon\u00adclusive otherwise. Since most C code usually contains \na system or library call every 10 lines or so on average, this distinguishing fea\u00adture of DART is a signi.cant \npractical advantage. The directed search algorithm introduced in Section 2 is closely related to prior \nwork on dynamic test generation (e.g., [23, 15]). The algorithms discussed in these papers generate test \ninputs to exercise a speci.c program path or branch (to determine if its exe\u00adcution is feasible), starting \nwith some (possibly random) execution path. In contrast, DART attempts to cover all executable program \npaths, in a style similar to systematic testing and model checking (e.g., [12]). It therefore does not \nuse branch/predicate classi.cation techniques as in [23, 15]. Also, prior work on dynamic test gener\u00adation \ndoes not deal with functions calls, unknown code segments (such as library functions), how to check at \nrun-time whether pre\u00addictions about new test inputs are matched in the next run, and does not discuss \ncompleteness. Finally, to the best of our knowledge, dy\u00adnamic test generation has never been implemented \npreviously for a full-.edged programming language like C nor applied to large ex\u00adamples like the Needham-Schroeder \nprotocol and the oSIP library. DART is also more loosely related to the following work. QuickCheck [7] \nis a tool for random testing of Haskell programs which supports a test speci.cation language where the \nuser can assign probabilities to inputs. Korat [5] is a tool that can analyze a Java method s precondition \non its input and automatically gener\u00adate all possible non-isomorphic inputs up to a given (small) size. \nContinuous testing [34] uses free cycles on a developer s machine to continuously run regression tests \nin the background, providing feedback about test failures as source code is edited. Random in\u00adterpretation \n[14] is an approximate form of abstract interpretation where code fragments are interpreted over a probabilistic \nabstract domain and their abstract execution sampled via random testing. 6. Conclusions With DART, we \nhave turned the conventional stance on the role of symbolic evaluation upside-down: symbolic reasoning \nis an adjunct to real execution. Randomization helps us where automated rea\u00adsoning is impossible or dif.cult. \nFor example, when we encounter malloc s we use randomization to guess the result of the alloca\u00adtion. \nThus symbolic execution degrades gracefully in the sense that randomization takes over, by suggesting \nconcrete values, when au\u00adtomated reasoning fails to suggest how to proceed. DART s ability to execute \nand test any program that compiles without writing any test driver/harness code is a new powerful paradigm, \nwe believe. Running a program for the .rst time usu\u00adally brings interesting feedback (detects bugs), \nand DART makes this step almost effortless. We wrote almost because in prac\u00adtice, the user is still responsible \nfor de.ning what a suitable self\u00adcontained unit is: it makes little sense to test in isolation functions \nthat are tightly coupled; instead, DART should be applied to pro\u00adgram/application interfaces where pretty \nmuch any input can be ex\u00adpected and should be dealt with. The user can also restrict the most general \nenvironment or test for functional correctness by adding interface code to the program in order to .lter \ninputs (i.e., enforce pre-conditions) and analyze outputs (i.e., test post-conditions). We plan to explore \nhow to effectively present to the user the interface identi.ed by DART and let him/her specify constraints \non inputs or outputs in a modular way.  Acknowledgments We thank Dennis Dams, Cormac Flanagan, Alan \nJeffrey, Rupak Majumdar, Darko Marinov, Kedar Namjoshi and Vic Zandy for helpful comments on this work. \nWe are grateful to the anonymous reviewers for their comments on a preliminary version of this paper. \nThis work was funded in part by NSF CCR-0341658. The work of Koushik Sen was done mostly while visiting \nBell Laboratories, and we thank his advisor, Gul Agha, for the additional time and resources needed to \ncomplete this work. References [1] R. Alur, P. Cerny, G. Gupta, P. Madhusudan, W. Nam, and A. Sri\u00advastava. \nSynthesis of Interface Speci.cations for Java Classes. In Proceedings of POPL 05 (32nd ACM Symposium \non Principles of Programming Languages), Long Beach, January 2005. [2] T. Ball and S. Rajamani. The SLAM \nToolkit. In Proceedings of CAV 2001 (13th Conference on Computer Aided Veri.cation), volume 2102 of Lecture \nNotes in Computer Science, pages 260 264, Paris, July 2001. Springer-Verlag. [3] D. Beyer, A. J. Chlipala, \nT. A. Henzinger, R. Jhala, and R. Majumdar. Generating Test from Counterexamples. In Proceedings of ICSE \n2004 (26th International Conference on Software Engineering). ACM, May 2004. [4] D. Bird and C. Munoz. \nAutomatic Generation of Random Self-Checking Test Cases. IBM Systems Journal, 22(3):229 245, 1983. [5] \nC. Boyapati, S. Khurshid, and D. Marinov. Korat: Automated testing based on Java predicates. In Proceedings \nof ISSTA 2002 (International Symposium on Software Testing and Analysis), pages 123 133, 2002. [6] W. \nBush, J. Pincus, and D. Sielaff. A static analyzer for .nding dynamic programming errors. Software Practice \nand Experience, 30(7):775 802, 2000. [7] K. Claessen and J. Hughes. QuickCheck: A Lightweight Tool for \nRandom Testing of Haskell Programs. In Proceedings of ICFP 2000, 2000. [8] C. Colby, P. Godefroid, and \nL. J. Jagadeesan. Automatically Closing Open Reactive Programs. In Proceedings of PLDI 98 (1998 ACM SIGPLAN \nConference on Programming Language Design and Implementation), pages 345 357, Montreal, June 1998. ACM \nPress. [9] C. Csallner and Y. Smaragdakis. Check n Crash: Combining Static Checking and Testing. In Proceedings \nof ICSE 2005 (27th International Conference on Software Engineering). ACM, May 2005. [10] J. Edvardsson. \nA Survey on Automatic Test Data Generation. In Proceedings of the 2nd Conference on Computer Science \nand Engineering, pages 21 28, Linkoping, October 1999. [11] J. E. Forrester and B. P. Miller. An Empirical \nStudy of the Robustness of Windows NT Applications Using Random Testing. In Proceedings of the 4th USENIX \nWindows System Symposium, Seattle, August 2000. [12] P. Godefroid. Model Checking for Programming Languages \nusing VeriSoft. In Proceedings of POPL 97 (24th ACM Symposium on Principles of Programming Languages), \npages 174 186, Paris, January 1997. [13] P. Godefroid and S. Khurshid. Exploring Very Large State Spaces \nUsing Genetic Algorithms. In Proceedings of TACAS 2002 (8th Conference on Tools and Algorithms for the \nConstruction and Analysis of Systems), Grenoble, April 2002. [14] S. Gulwani and G. C. Necula. Precise \nInterprocedural Analysis using Random Interpretation. In To appear in Proceedings of POPL 05 (32nd ACM \nSymposium on Principles of Programming Languages), Long Beach, January 2005. [15] N. Gupta, A. P. Mathur, \nand M. L. Soffa. Generating test data for branch coverage. In Proceedings of the 15th IEEE International \nConference on Automated Software Engineering, pages 219 227, September 2000. [16] S. Hallem, B. Chelf, \nY. Xie, and D. Engler. A System and Language for Building System-Speci.c Static Analyses. In Proceedings \nof PLDI 02 (2002 ACM SIGPLAN Conference on Programming Language Design and Implementation), pages 69 \n82, 2002. [17] R. Hastings and B. Joyce. Purify: Fast Detection of Memory Leaks and Access Errors. In \nProceedings of the Usenix Winter 1992 technical Conference, pages 125 138, Berkeley, January 1992. [18] \nT. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy Abstraction. In Proceedings of the 29th ACM Symposium \non Principles of Programming Languages, pages 58 70, Portland, January 2002. [19] S. Johnson. Lint, a \nC program checker, 1978. Unix Programmer s Manual, AT&#38;T Bell Laboratories. [20] Junit. web page: \nhttp://www.junit.org/. [21] J. C. King. Symbolic Execution and Program Testing. Communica\u00adtions of the \nACM, 19(7):385 394, 1976. [22] Klocwork. web page: http://klocwork.com/index.asp. [23] B. Korel. A dynamic \nApproach of Test Data Generation. In IEEE Conference on Software Maintenance, pages 311 317, San Diego, \nNovember 1990. [24] G. Lowe. An Attack on the Needham-Schroeder Public-Key Authentication Protocol. Information \nProcessing Letters, 1995. [25] G. Lowe. Breaking and Fixing the Needham-Schroeder Public-Key Protocol \nusing FDR. In Proceedings of TACAS 1996 ((Second International Workshop on Tools and Algorithms for the \nConstruction and Analysis of Systems), volume 1055 of Lecture Notes in Computer Science, pages 147 166. \nSpringer-Verlag, 1996. [26] lp solve. web page: http://groups.yahoo.com/group/lp solve/. [27] G. J. Myers. \nThe Art of Software Testing. Wiley, 1979. [28] G. C. Necula, S. McPeak, S. P. Rahul, and W. Weimer. CIL: \nIntermediate Language and Tools for Analysis and transformation of C Programs. In Proceedings of Conference \non compiler Construction, pages 213 228, 2002. [29] G. C. Necula, S. McPeak, and W. Weimer. CCured: Type-Safe \nRetro.tting of Legacy Code. In Proceedings of POPL 02 (29th ACM Symposium on Principles of Programming \nLanguages), pages 128 139, Portland, January 2002. [30] R. Needham and M. Schroeder. Using Encryption \nfor Authentication in Large Networks of Computers. Communications of the ACM, 21(12):993 999, 1978. [31] \nThe economic impacts of inadequate infrastructure for software testing. National Institute of Standards \nand technology, Planning Report 02-3, May 2002. [32] J. Offut and J. Hayes. A Semantic Model of Program \nFaults. In Proceedings of ISSTA 96 (International Symposium on Software Testing and Analysis), pages \n195 200, San Diego, January 1996. [33] Polyspace. web page: http://www.polyspace.com. [34] D. Saff and \nM. D. Ernst. Continuous testing in Eclipse. In Proceedings of 2nd Eclipse Technology Exchange Workshop \n(eTX), Barcelona, March 2004. [35] S. D. Stoller. Domain Partitioning for Open Reactive Programs. In \nProceedings of ACM SIGSOFT ISSTA 02 (International Symposium on Software Testing and Analysis), 200. \n[36] W. Visser, C. Pasareanu, and S. Khurshid. Test Input Generation with Java PathFinder. In Proceedings \nof ACM SIGSOFT ISSTA 04 (International Symposium on Software Testing and Analysis), Boston, July 2004. \n[37] J. Whaley, M. C. Martin, and M. S. Lam. Automatic Extraction of Object-Oriented Component Interfaces. \nIn Proceedings of ACM SIGSOFT ISSTA 02 (International Symposium on Software Testing and Analysis), 2002. \n[38] T. Xie, D. Marinov, W. Schulte, and D. Notkin. Symstra: A framework for generating object-oriented \nunit tests using symbolic execution. In Proceedings of TACAS 05 (11th Conference on Tools and Algorithms \nfor the Construction and Analysis of Systems), volume 3440 of LNCS, pages 365 381. Springer, 2005.  \n \n\t\t\t", "proc_id": "1065010", "abstract": "We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) <i>automated</i> extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs <i>random</i> testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to <i>direct</i> systematically the execution along alternative program paths. Together, these three techniques constitute <i>Directed Automated Random Testing</i>, or <i>DART</i> for short. The main strength of DART is thus that testing can be performed <i>completely automatically</i> on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.", "authors": [{"name": "Patrice Godefroid", "author_profile_id": "81100504535", "affiliation": "Bell Laboratories, Lucent Technologies", "person_id": "PP40027996", "email_address": "", "orcid_id": ""}, {"name": "Nils Klarlund", "author_profile_id": "81100072411", "affiliation": "Bell Laboratories, Lucent Technologies", "person_id": "PP39025867", "email_address": "", "orcid_id": ""}, {"name": "Koushik Sen", "author_profile_id": "81100399070", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL", "person_id": "PP38024992", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065036", "year": "2005", "article_id": "1065036", "conference": "PLDI", "title": "DART: directed automated random testing", "url": "http://dl.acm.org/citation.cfm?id=1065036"}