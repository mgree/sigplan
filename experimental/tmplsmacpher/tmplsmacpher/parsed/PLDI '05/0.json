{"article_publication_date": "06-12-2005", "fulltext": "\n A Serializability Violation Detector for Shared-Memory Server Programs Min Xu Rastislav Bod\u00edk Mark \nD. Hill Electrical &#38; Computer Engr. Dept. Computer Science Division, EECS Computer Sciences Dept. \nUniversity of Wisconsin-Madison University of California, Berkeley University of Wisconsin-Madison mxu@cae.wisc.edu \nbodik@eecs.berkeley.edu markhill@cs.wisc.edu Abstract We aim to improve reliability of multithreaded \nprograms by pro\u00adposing a dynamic detector that detects potentially erroneous pro\u00adgram executions and \ntheir causes. We design and evaluate a Serializability Violation Detector (SVD) that has two unique goals: \n(I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply \nalerting users that a software error may have occurred; and (II) helping debug programs by revealing \ncauses of error symptoms. Two properties of SVD help in achieving these goals. First, to detect only \nerroneous executions, SVD checks serializability of atomic regions, which are code regions that need \nto be executed atomically. Second, to improve usability, SVD does not require a priori annotations of \natomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widely-used \nmultithreaded server programs show that SVD .nds real bugs and reports modest false positives. Categories \nand Subject Descriptors. D.2.5 [Software Engineer\u00ading]: Testing and Debugging Diagnostics; D.2.4 [Software \nEngineering]: Software/Program Veri.cation Reliability; General Terms. Algorithms, Languages, Reliability \nKeywords. Multithreading, Serializability, Race Conditions  1. Introduction 1.1. Objective In shared-memory \nprograms, bugs often manifest themselves only under speci.c thread interleavings, and sometimes at the \nworst time. For example, the 2003 U.S.-Canada power outage went unnoticed for a crucial period of time; \nthe reason was a data race in the grid-monitoring system [28,35]. Fortunately, the non-deterministic \nnature of these timing-depen\u00addent bugs can be exploited. If one can detect erroneous executions on-the-.y, \nthen backward error recovery (BER) can be used to roll back a part of the erroneous execution; subsequent \nreexecution with a conservative thread scheduling may avoid recurrence of the software error. When BER \nis not available, detecting erroneous executions can alert that a software error may have happened. In \nthe 2003 blackout, an early warning may have prompted the power Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distrib\u00aduted for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, or republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 05, June 12-15, 2005, Chicago, Illinois, \nUSA. Copyright 2005 ACM 1-59593-056-6/05/0006...$5.00. grid operators to reboot the grid-monitoring system, \nwhich would then warn about the power outage before it had become a disaster. The goal of this paper \nis to develop a detector suitable for (I) BER\u00adbased avoidance of erroneous program executions; and (II) \nalerting users as software errors occur. We argue that such a detector should have the following two \nproperties. Detect only erroneous executions. Typical detectors strive for best coverage by detecting \npotential bugs, i.e., bugs that may man\u00adifest in executions not directly examined. In contrast, our detector \nshould detect only erroneous program executions, because it is to be deployed in the following scenarios: \n Bug avoidance with BER. Imagine a detector with low overhead, perhaps implemented in hardware. When \nan erroneous execu\u00adtion is detected, the execution rolls back to a safe checkpoint and reexecutes (more) \nserially [30,34]. In this scenario, detect\u00ading only erroneous executions helps reducing the performance \nlost in unnecessary rollbacks. Similarly, detecting only mani\u00adfested errors helps avoid overloading operators \nwith false alarms. Because unnecessary rollbacks or alarms occur on each instance of a false positive, \nthe detector should strive to reduce dynamic false positives, which include dynamic instances of identical \nwarnings.  From symptoms to bugs. Imagine we have captured a failing multithreaded execution with a \ndeterministic recorder [4,29,38]; how do we now .nd the bug in the execution? Replaying the execution \nwith the detector will point to an error that actually happened in this execution; this error is likely \nthe cause of the failure. Detecting causes of erroneous executions avoids report\u00ading errors that may \nexist in some other executions, thus improv\u00ading understanding of the execution at hand. In this scenario, \nthe detector should strive to reduce static false positives, which are false warnings related to the \nsame piece of code.  Do not require a priori program annotation. Typical detectors require a priori \nprogram annotations, such as identi.cation of syn\u00adchronization constructs or annotations of atomic regions, \nwhich are code regions that need to be executed atomically. The annota\u00adtion effort is non-trivial for \nserver programs, because the source code is large and sometimes not fully available, especially for commercial \nsoftware. Instead of requiring a priori annotations, we believe that a dynamic detector that enables \na posteriori examina\u00adtions is more applicable to server programs, because the examina\u00adtion is limited \nto the program trace of a single execution.  1.2. Our Solution We propose a new detector, called Serializability \nViolation Detec\u00adtor (SVD), that seeks to detect only erroneous program executions and does not require \na priori program annotation. Three ideas underlie the design of SVD. 1) Computational units. SVD automatically \ninfers approxima\u00adtions of atomic regions. We call the inferred atomic regions Computational Units (CU \ns), because the inference heuristic relies on computational patterns, namely data and control dependences. \nBecause our heuristic inference does not rely on any synchronization constructs, it identi.es atomic \nregions even when locks are mistakenly omitted in the example shown in Section 2.1. Thanks to the inference, \nno a priori annotation is needed. 2) Serializability. SVD detects erroneous executions by deter\u00admining \nwhether the execution violated serializability of the inferred CU s. Traditionally, serializability is \nde.ned for data\u00adbase transactions. In databases, serializability means that the execution of a group \nof transactions is logically equivalent to a serial execution of the same group of transactions. In shared\u00admemory \nprograms, we apply serializability to atomic regions, which are approximated by inferred CU s. A serializable \nexe\u00adcution is correct, because it appears that each atomic region executed atomically. Executions that \nare not serializable are often erroneous. We use an ef.cient yet approximate serializ\u00adability test, which \nensures that the memory locations read by a CU are not overwritten by another thread before the CU ends. \n3) A posteriori examination. Because inferred CU s may differ from the atomic regions, SVD is able to \nproduce a log of CU s for the programmer to examine a posteriori. This way, pro\u00adgrammers can discover \nerroneous executions that are missed online by SVD due to inherent limitations of inferring CU s without \nany a priori annotations. We design and evaluate SVD in a post-mortem debugging scenario with deterministic \nreplay. Our results show that SVD helps .nd erroneous executions caused by timing-dependent bugs in Apache \n[3] and MySQL [23], without requiring a priori program annotations. Experimental results on these two \nserver programs show that SVD reports far fewer dynamic false positives and mod\u00adestly fewer static false \npositives than a data race detector. Experi\u00admental results on PostgreSQL [27], a relatively mature server \nprogram, show that although SVD reports more false positives than the data race detector, the absolute \nfalse positives rate is low. We contribute in the following aspects. We propose a novel detector that \nseeks to detect only erroneous program executions caused by timing-dependent bugs, without requiring \na priori program annotations.  We propose a novel method for approximately inferring atomic regions. \n By applying the detector to large shared-memory server pro\u00adgrams, we show the new detector is suitable \nfor avoiding (unknown) bugs with BER.  The rest of this paper is organized as follows. In Section 2, \nwe use three examples to illustrate the key ideas of SVD. In Section 3 we formalize SVD. In Section 4, \nwe present two versions of SVD algorithms and outline a hardware implementation. We qualita\u00adtively analyze \nSVD in Section 5. After describing our evaluation methodology in Section 6, we evaluate SVD in Section \n7. We present related work in Section 8 and conclude in Section 9.  2. Overview of SVD 2.1. Inferring \nComputational Units To understand how SVD operates, it is useful to pretend .rst that programs come with \na priori annotations that specify code regions to be executed atomically. We call these code regions \natomic regions. The programmer implements atomic regions with lock\u00adbased critical sections or by means \nof some other synchronization mechanisms, such as signal, monitor or thread fork. A program is buggy \nwhen some atomic regions are not implemented correctly, e.g., when their critical sections are placed \nincorrectly or are entirely missing. SVD seeks to detect if a bug in an incorrect implementation of an \natomic region manifested itself in a given execution. SVD performs the detection by verifying that atomic \nregions were executed in a serialized way. Because we assume that a priori annotations of atomic regions \nare actually not available, our approach is to infer these regions. The key feature of our inference \nis that it does not rely on the synchro\u00adnization mechanisms used in the program; such inference would \nlikely infer atomic regions that were as buggy as the synchroniza\u00adtions, whose bugs we want to identify \nin the .rst place. Instead, SVD infers atomic regions from how shared variables are used and from data \nand control dependences. The result of SVD inference are computational units (CU s), which approximate \ndynamic instances of atomic regions. That is, CU s are execution paths through atomic regions. (Henceforth, \natomic regions refer to dynamic instances of atomic regions.) As we shall show, SVD computes CU s online, \nautomatically, without requir\u00ading a priori program annotations, and without being affected by (incorrect) \nsynchronization constructs. Informally, a CU is the largest group of dynamic program state\u00adments that \nfollow the following two-part region hypothesis. 1) A shared variable written in an atomic region is \nnot read again in the same atomic region. In other words, true (read-after\u00ad write) dependences through \nshared variables between state\u00ad ments happen only across atomic regions, not within them. 2) Program \nstatements are related through true dependences or control dependences in atomic regions. In other words, \nan atomic region does not perform multiple unrelated computa\u00ad tions. The precise meaning of related is \ngiven in Section 3.2. To evaluate the power of region hypothesis, we manually exam\u00adined 14 atomic regions \nfrom real programs. We observed that region hypothesis held on the common paths of all 14 regions. For \nexample, it holds for the atomic region in JDK 1.4 StringBuffer class, which contains a subtle synchronization \nbug [16]. It did not hold on some rare paths. We discuss in Section 2.3 how to deal with these limitations \nusing a posteriori examinations. Figure 1 shows an example of a CU. The simpli.ed code in Figure 1(a) \nis taken from the MySQL database server, which uses .le system locks to guard database tables. In the \n.gure, one such lock is info.internal_lock. SVD correctly infers the atomic region, which is implemented \ncorrectly as a critical section guarded by info.internal_lock. The inferred CU contains four statements, \nrepresented in the .gure as an oval. To understand the inference, observe .rst that the shared variable \ninfo.tot_lock is read and subsequently written in the CU, but it is not read in the CU again. Second, \nnote that the four statements are related via dependences as shown in Figure 1(b). Statements 1.05, 1.06 \nand 1.07 are control-dependent on 1.03, and statement 1.06 is true\u00ad  that are reported by race detectors. \n(a) MySQL table locking code. Inferred CU s are big enough to cover atomic regions implemented correctly \nwith the lock. The code also contains a harmless data race on shared variable info.tot_lock. While race \ndetectors will report this false positive, SVD will not, because CU s in the execution are serializable. \n(b) and (c) Data and control dependences between statements within a CU. Note that only statements that \nare executed are included in a CU. dependent on 1.05 via a local variable register1. (The true-depen\u00addence \npredecessor of statement 1.03 is not in the CU because it belongs to the preceding CU; see Section 3.2). \nNote that SVD per\u00ad forms the inference without using the lock info.internal_lock. (The shared variable \ninfo.tot_lock is not a lock, despite what its name may suggest.) To illustrate how SVD computes a CU \nin a buggy program, con\u00adsider the example in Figure 2, where a lock is mistakenly omitted. The simpli.ed \ncode shown in Figure 2(a) is taken from the log_con.g module of the Apache web server. The log_con.g \nmod\u00adule buffers log messages, generated from multiple threads, in a shared memory buffer before writing \nthem to a .le. In this shown execution, two threads execute function ap_buffered_log_writer() simultaneously. \nVariable len is thread\u00adlocal. Thread-local pointer buf points to a shared memory buffer (buf.bufout) \nand a shared index variable (buf.outcnt). In order to avoid corrupting data in buf.bufout, the memcpy() \noperation (3.08) and the update to buf.outcnt (3.09) should be guarded within a critical section, which \nis not implemented. Figure 2(a) shows the CU s of the two threads. (The broken oval shows the serializability \nis violated.) The shared variables are shown in bold. Figure 2(b) shows that statements 3.05, 3.08 and \n3.09 are true-dependent on statement 3.04 via the thread-local vari\u00adable len. Figure 2(c) shows a similar \nCU of thread 2. Note that the inferred CU s include statements from the atomic region, even though the \nprogrammer did not implement them correctly (the locking is missing). So far, we have not discussed the \nactual inference algorithm. SVD partitions a thread execution into CU s by following dependences in execution \norder. It groups related instructions into a CU, starting Figure 2: SVD computes CU s via dependences \neven when buggy code contains incorrect synchronization; SVD detects erroneous program executions. (a) \nApache s log_con.g module contains a data race that corrupts the log messages stored in a shared buffer. \nSVD detects when corruptions happen by observing serializability of CU s is violated. (b) and (c) Data \nand control dependences between statements within a CU. Note that only statements that are executed are \nincluded in a CU. Figure 3: SVD can miss erroneous executions (i.e. false negatives), but it generates \na log for an a posteriori examination to mitigate the problem. The buggy code is taken from MySQL, which \nexecutes prepared SQL queries. (a) SVD stops growing CU s when shared dependences are observed. Variables \nfield.query_idand join_tab.used_fieldsare mistakenly shared. Because true dependences are observed on \nthe shared variables, several small CU s are computed according to the second rule of region hypothesis. \nTherefore, SVD fails to detect this erroneous execution online. We also show a log generated by SVD that \nenabled an a posteriori examination, which discovered this bug. (b) and (c) Data and control dependences \nbetween statements within a CU. Note that only statements that are executed are included in a CU. a new \nCU whenever it encounters a read from a shared variable. SVD uses a heuristic to .nd shared variables. \nA variable is shared if it is accessed by more than one thread after it is accessed by a CU and before \nthe CU ends. Figure 1 and Figure 2 do not show shared dependences that end CU s. Yet one can imagine \nwhen one of the shared variables (info.tot_lock and buf.bufout, buf.bufcnt) is read back by a statement \ns, SVD concludes that a CU ends just before s. The end of the CU is conservative, because the atomic \nregion may have ended earlier than s. Figure 3 in Section 2.3 shows some examples of shared dependences. \n 2.2. Detecting Serializability Violations After SVD computes CU s, it reports erroneous program execu\u00adtions \nwhenever CU s are not serializable. To detect serializability violations, SVD observes the temporal order \nbetween memory accesses from different threads (i.e., thread interleaving) and con\u00ad.icts between the \naccesses. Two accesses con.ict if and only if they access the same variable from different threads and \nat least one access is a write. 2.2.1 Finding Erroneous Program Executions To check serializability, \nSVD uses a heuristic that tests if con.icts have happened on input variables of a CU. Input variables \nare locations not written within the same CU before its .rst read by the CU. This check is performed \nwhenever a CU performs a write. This heuristic is a relaxation of a conservative serializability detec\u00adtion \nalgorithm presented in Section 3.3. It helps SVD achieve low false positive rate by allowing unlikely \nfalse negatives (Section 4). Figure 2(a) shows an erroneous execution of the Apache web server found \nby SVD. The diamond and dot patterns shows a logi\u00adcal interleaving of statements from the two threads. \nNote that we can freely reorder those statements that do not con.ict, but not those statements that con.ict \non shared variables buf.bufout and buf.outcnt. Due to the con.icts on shared variables, the execu\u00adtion \nof the CU of thread 3 is broken by thread 4. SVD detects the serializability violation when 3.09 is writing \nbuf.outcnt by observing a con.ict (3.05 vs. 4.09). 2.2.2 Avoiding False Positives w.r.t. Race Detection \nAnother method for detecting erroneous executions is data race detection. A data race occurs when two \nthreads access the same variable with no synchronization between the accesses, where at least one of \nthe accesses is a write [24]. Serializability helps SVD avoid some false positives reported by race detectors, \nbecause a program execution can be serializable and at the same time contain data races. Although SVD \ndoes report false positives that are not reported by race detectors, we found SVD usually reports fewer \nfalse positives than race detectors (Section 7.2). Figure 1(a) shows a correct execution of the table \nlocking code in MySQL that contains data races. Thread 1 updates info.tot_lock within a critical section \n(guarded by mutex info.internal_lock). Thread 2 reads info.tot_lock without .rst synchronizing with thread \n1. Data races occur due to statements 1.06, 1.11 and 2.03. Existing race detectors would conclude the \nexecution shown in Figure 1(a) is erroneous because of the data races. However, the data races do not \nindicate an erroneous MySQL exe\u00adcution; i.e., existing race detectors would report false positives. The \ncode in thread 1 implies info.tot_lock is never zero for shared tables, because shared tables must be \nlocked before they are used and info.tot_lock is initially zero. In other words, the predi\u00adcate of statement \n2.03 is never true for shared tables.1 Therefore, the data races are not harmful. We found that it requires \nnon-trivial time and effort, even by a programmer who is familiar with MySQL, to determine the races \ndo not indicate a bug. SVD avoids reporting the false positives by observing that serializ\u00adability is \nnot violated in Figure 1(a). In this execution, both CU s are serializable.  2.3. Logging for A posteriori \nExamination Accuracy of SVD is largely determined by how closely CU s approximate atomic regions. When \nCU s are larger than atomic regions, SVD may report false positives. When CU s are smaller, SVD may miss \nerroneous executions (false negatives). This section describes how we mitigate the false negative problem. \nSVD logs each statement s that reads a variable last written by another thread; it also logs the remote \nwrite rw and the immedi\u00adately preceding thread-local write lw to the same variable. The tri\u00adple (s, rw, \nlw) records that rw overwrote the value that lw may have intended to communicate to s. If this local \ncommunication is indeed intended, then we .nd a likely bug. The programmer exam\u00adines the log in a posteriori \nexamination. The examination allows discovering, in a post-mortem fashion, more erroneous executions \nthan SVD can do online. The statement s is an input to the CU, so the log effectively records shapes \nof inferred CU s. For example, Figure 3(a) shows an erroneous execution caused by a MySQL bug that we \nfound during an a posteriori examination. Each MySQL query is carried on by a single thread. During execu\u00adtions \nof MySQL prepared SQL queries, two variables (.eld.query_id and join_tab.used_.elds), which are intended \nto be thread-local, are shared between threads by mistake. The variable .eld.query_id is intended to \ndistinguish those .elds of a database table (join_tab) that are used by a SQL query. The variable join_tab.used_.elds \nis used to record the total number of .elds used by the query. During the execution, thread 5 initially \ncomputes number of .elds that it uses (5.08). Then, thread 6 updates the values of .eld.query_id (6.03) \nand join_tab.used_.elds (6.08), which later causes the loop of thread 5 (5.12) to go out-of-bounds based \non an inconsistent value of join_tab.used_.elds. This bug crashes MySQL server with a segmentation fault. \nBecause the thread-local variables are mistakenly shared, in order to detect erroneous executions of \nMySQL, we must detect serializ\u00adability violations for those atomic regions that provide mutual exclusion \nto the accesses to these variables. However, variables like .eld.query_id and join_tab.used_.elds are \nread back within the atomic regions, violating the second rule of region hypothesis. SVD fails to report \nerroneous executions caused by this bug, because it forms CU s smaller than the atomic regions. Unfortunately, \nserializability for these small CU s is not violated during the execution as shown in Figure 3(a). Figure \n3(a) shows a log that contains statements 5.12 and 5.14. When a programmer 1. The predicate of statement \n2.03 can be true for thread-local temporary tables, which need not be locked. examines the log, he can \ndiscover this bug by noticing that join_tab.used_.elds and .eld.query_id are mistakenly shared. After \nhe .xes the bug, SVD will no longer break the atomic regions into small CU s in future executions.  \n3. De.nitions of CU s and Serializability This section formalizes the two key ideas behind SVD: inferring \natomic regions and detecting serializability violations. 3.1. Dynamic Program Dependence Graph We de.ne \ncomputational units using dynamic program depen\u00addence graph (d-PDG). A d-PDG represents dependences on \na pro\u00adgram trace of a multithreaded program execution. The program trace is a sequence of all dynamic \nstatements executed by all the threads, listed in execution order, a total order denoted .. We say that \na . b if dynamic statement a is executed before dynamic statement b or if a = b .A thread trace r of \nthread t is a subse\u00adquence of the program trace such that all dynamic statements in r were executed by \nt . A d-PDG is a directed acyclic graph (VE) , whose vertices are , dynamic statements and arcs are dependences \npartitioned into true ( ), control (E), or con.ict ( ) dependences. True and control Et cEh dependences \nexist only between vertices of the same thread, while con.ict dependences exists only between vertices \nof different threads. A true dependence arc (ab) exists whenever (I) , b . a ; (II) a location v de.ned \nin b is used in a ; and (III) v is not de.ned on the thread trace between b and a in a vertex other than \n b . Note that v may be de.ned by some other thread. A control dependence arc (ab) exists whenever (I) \nmodifying the predicate  , value of the conditional branch b would result in a not being exe\u00adcuted; \nand (II) no other conditional branch on the thread trace between b and a could be used to bypass the \nexecution of a .A con.ict dependence arc (ab) exists whenever (I) , b . a ; (II)a location v is written \nby b and read or written by a or v is read by b and written by a ; (III) v is not written on the program \ntrace between b and a by a vertex other than a and b ; (IV) a and b were executed by different threads. \nOur d-PDG de.nition is slightly different from the de.nition given by Miller and Choi [21]. In particular, \nwe include con.ict dependence arcs and omit synchronization dependence arcs; we use the former to detect \nserializability violations (Section 3.3), while we do not rely on the identi.cation of the latter in \nthe SVD algorithm. To ensure that d-PDG is acyclic, we assume that vertices corre\u00adspond to atomic operations \nand are .ne-grain enough such that for all arcs(ab) we have the property , b . a . In contrast to d-PDG, \na thread d-PDG (td-PDG) represents depen\u00addences in a thread trace. A td-PDG thus contains all true and \ncon\u00adtrol dependences of a given thread trace and omits all con.ict dependences. In other words, td-PDG \nis a result of partitioning a d-PDG by thread membership of vertices. A td-PDG is identical to a dynamic \ndependence graph de.ned by Agrawal and Horgan [1]. In order to de.ne a computational unit, we partition \ntrue depen\u00addences according to the nature of their locations. Dependences on variables not shared among \nthreads are called true-local depen\u00addences, while remaining true dependences are called true-shared dependences. \nThese two sets of arcs are denoted and E, El s respectively. Note that true-shared dependences are intra-thread \ndependences, even though they involve shared variables. Figure 4: After the crossing arc (ba) is removed, \nthe shared , arc yx) is no longer in the same weakly (, connected component (along local and control \ndependences). 3.2. Computational Units (CU) A computational unit (CU) is an approximation of an atomic \nregion. (Recall that a CU is neither an over-nor an under-approxi\u00admation of an atomic region.) Speci.cally, \na CU is a partition of a td-PDG determined by region hypothesis. Recall from Section 2 that region hypothesis \nplaces two constraints on CU s. 1) Within an atomic region, a write of a shared location v must not be \nfollowed by a read of v . In other words, a CU must not contain any e . E. s 2) An atomic region does \nnot perform multiple independent computations. In other words, vertices of each CU must be weakly connected. \n(A directed graph is weakly connected if and only if its underlying undirected graph is connected.) Unfortunately, \nthe two constraints do not produce a unique parti\u00adtion of a td-PDG, even under the natural goal that \neach CU is max\u00adimal. This is because there is, in general, a choice as to where to cut a weakly connected \ngraph to satisfy the .rst constraint. To make CU s unique, we de.ne a heuristic that selects unique cuts. \nThese cuts will be determined in execution order, so that vertices close in the program are grouped into \nthe same CU. Our plan is to de.ne a set of crossing arcs whose removal will par\u00adtition a weakly connected \ncomponent such that a shared arc will be removed from it. After removing all crossing arcs and all shared \narcs, the largest weakly connected components of td-PDG will yield a unique set of CU s. ,(yx) is a cross- \nDe.nition 1: Let (yx). E. El . We say that , c ing arc of (ba) iff there exists (ba). Esuch that (I) \ny . b ; ,, s and (II)x anda are weakly connected along arcs of E. El . c Figure 4 shows an example crossing \narc. The control dependence arc (ba) is a crossing arc, because there is a shared dependence , (yx) such \nthat the vertices a and , x are weakly connected along arcs of . E, and y precedes b in execution order. \nAfter the El c crossing arc is removed, (yx) is no longer in the same weakly , connected component (along \nlocal and control arcs). The following de.nition operationally de.nes a unique set of crossing arcs to \nremove, thus de.ning the partitioning of a thread trace into CU s. De.nition 2: A reduced dependence \ngraph of a td-PDG t is obtained by removing fromt arcs as follows. 1) Find an earliest arc2 e . E. An \narc (ba), is de.ned to be s earlier than arc(yx) if , b . y . 2) Remove all crossing arcs of e . 3) Remove \narc e . 4) Repeat step 1, 2, 3 untilE is empty. s Informally, as illustrated in Figure 4, we cut a td-PDG \njust before the execution reaches the source vertex of each shared arc. De.nition 3: Given a td-PDG t \n, the computational unit of a vertex x is the set of vertices that are weakly connected with x in the \nreduced dependence graph of t . CU s de.ned by De.nition 3 can overlap in a thread trace. In other words, \nvertices of a CU are not always adjacent in the thread trace. In the following, however, we assume non-overlapping \nCU s so that we can derive a heuristic to detect serializability violations by drawing results from database \nserializability theory as if CU s are database transactions.  3.3. Serializability and Strict 2PL In \nthis subsection, we de.ne CU serializability and we derive a heuristic to detect serializability violations. \nIn shared-memory programs, CU s from different threads execute concurrently. However, correct and incorrect \nprogram executions differ in whether CU s are serializable. De.nition 4: CU s of a program trace are \nserializable iff there exists an equivalent program trace where all statements of each CU are adjacent \nto each other. We say two program traces are equiva\u00adlent if they have identical d-PDGs. If a thread \ntrace contains only non-overlapping CU s, then at any given time the thread is executing at most one \nCU. All other CU s of the thread are either .nished or not started. This model is identi\u00adcal to the serializability \nmodel of databases if CU s are replaced by database transactions [25]. In this case, CU serializability \nproblem is equivalent to the transaction serializability problem in databases. In databases, a popular \nmethod to guarantee transaction serializ\u00adability is the 2-Phase Locking (2PL) protocol [14]. Strict 2PL \npro\u00adtocol is an important variation of 2PL protocol. In Strict 2PL, a transaction must gain exclusive \naccess to the shared data between its initial access to the data and the end of the transaction. Exclu\u00adsive \naccess means (I) no other transaction can write a datum if the transaction is exclusively reading the \ndatum, (II) no other transac\u00adtion can read or write a datum if the transaction is exclusively writ\u00ading \nthe datum. Not violating strict 2PL is suf.cient yet not necessary for serializability. Here, we draw \nthe results from strict 2PL to detect CU serializabil\u00adity violations. We check program traces for strict \n2PL violations. In particular, we check whether any statement of a CU k has con\u00ad.icted with a statement \nfrom a different thread before k .nishes. A con.ict before k .nishes is equivalent to failing to have \nexclusive access to a shared datum. We report a serializability violation if strict 2PL is violated. \nThe detection based on strict 2PL violation is conservative, because not violating strict 2PL is suf.cient \nyet not necessary for serializability. We choose this heuristic, because detecting strict 2PL violations \ndoes not require exchanging exces\u00ad 2. An earliest arc is unique if each vertex read at most one shared \nvariable (possible if dynamic statements are .ne-grain enough). Data Structures // S_T includes memory \nvalues (variables) that are being loaded and stored by a dynamic statement. The set s.dep-Pred includes \nstatements that s is directly true-dependent or control-dependent upon. The reference cu points to the \nCU that contains this statement. S_T ::= structure (VAL leftValue, SET<VAL> rightValues, SET<S_T> depPred, \nCU_T cu) // CU_T includes a set of S_T, a set of values and a boolean .ag. Variables in shVars are shared \nand have been writ\u00adten by this CU. CU_T ::= structure (SET<S_T> stmts, SET<VAL> shVars, bool active) \n // T_T is a set of S_T comprising a thread trace T_T ::= SET<S_T> Algorithm 1 forall t in all threads \nof the program trace { S_T s while ((s := next_dyn_stmt_in_T_exec(t,s)) != NIL) { forall v in s.rightValues \n{ 5 forall st in s.depPred { if (st.cu.active==TRUE &#38;&#38; v in st.cu.shVars)// shared dependence \nst.cu.active := FALSE }} 10 s.cu.stmts := . st \u00b7 cu \u00b7 stmts st . s \u00b7 depPred . st \u00b7 cu \u00b7 active = TRUE \nforall st in s.cu.stmts { st.cu := s.cu } // update cu s.cu.stmts := s.cu.stmts + s s.cu.active := TRUE \n15 if (s.leftValue.shared) s.cu.shVars := s.cu.shVars + s.leftValue } // end of while // close CU s, \nafter .nished scanning a thread trace forall cu in t { 20 if (cu.active) { cu.active := FALSE } }} Figure \n5: The of.ine algorithm scans each thread trace and computes CU s. sive information, such as timestamps, \nbetween threads. More accu\u00adrate detection of serializability violations is possible with higher detection \ncost. We leave exploring this direction to future work. 4. Serializability Violation Detector This section \ndescribes Serializability Violation Detector (SVD),a software-based online detector. To simplify the \npresentation, we .rst give a simple of.ine, multi-pass algorithm in Section 4.1. Section 4.2 presents \nan online algorithm. SVD uses several heuris\u00ad tics to achieve one-pass detection. Section 4.3 presents \nsome prag\u00ad matic considerations when we implement SVD. Finally, Section 4.4 brie.y discusses a potential \nhardware version of SVD. 4.1. An Of.ine Algorithm In this section, we describe an of.ine, multi-pass \nalgorithm. The of.ine algorithm detects serializability violations of a program execution by scanning \nthe program trace multiple times. To help understand the basic principles of our detector, we keep the \nof.ine algorithm as simple as possible. Later on in Section 4.2, we give an online, one-pass algorithm. \n4.1.1 Program Traces and Dependence Predecessors The of.ine algorithm operates on program traces where \n(I) true\u00addependent and control-dependent predecessors of a dynamic state\u00adment s are known and stored \nin a data structure s.depPred, and (II) a boolean .ag v.shared indicates whether a variable v is shared. \nIt is important to note that these two types of information are required only by the of.ine algorithm. \nThe online algorithm in Section 4.2 does not require them. Instead, the online algorithm employs heuristics \nto compute them on-the-.y. We assume two selection functions that let us scan either the whole program \ntrace or each individual thread trace. Function next_dyn_stmt_in_P_exec(r) returns the next statement \ns that fol\u00adlows r regardless of whether s is executed by the same thread that executed r. Function next_dyn_stmt_in_T_exec(r,t) \nreturns the next statement s that follows r, with the restriction that s and r must be executed by the \nsame thread t. 4.1.2 Three Passes of the Of.ine Algorithm The of.ine algorithm operates in three passes. \nThe .rst pass scans each thread trace and computes CU s. The second pass assigns a unique sequence number \nto each dynamic statement in the pro\u00adgram trace, which de.nes a total order. The second pass also records \nwhere a CU .nishes its execution. Finally, the third pass scans the program trace and checks for strict \n2PL violations. By doing so, serializability violations are detected and reported. Figure 5 shows how \nthe of.ine algorithm computes CU s from thread traces. The key for the algorithm is to .nd and remove \ncrossing arcs from a td-PDG as a thread is executing. Using a bool\u00adean variable, active, the of.ine algorithm \ndistinguishes those CU s that are still connecting to future statements and those CU s that are cut by \nshared dependence arcs. The of.ine algorithm is able to compute CU s in one pass, which is an important \nfeature for the online algorithm later. In Figure 5, the of.ine algorithm starts by iterating through \nall statements for each thread trace. For each variable v that a state\u00adment s reads, we check whether \nany statement in s.depPred wrote v and v is shared (line 4-7). If so, then the CU that contains the pre\u00addecessor \nis marked inactive (line 8). Therefore, all crossing arcs that connect to the CU from a later dynamic \nstatement are cut . Next, all active CU s that contain any of s s dependence predeces\u00adsors are merged \n(line 10), i.e. we build a larger CU that is weakly connected. Finally, s is added to the resulting CU \n(line 13) and the dependences in the CU are allowed to propagate further by leaving the CU active (line \n14). Once CU s are computed, the information about which statements belong to which CU is stored to be \nused by future passes of the algorithm. Figure 6 shows how the of.ine algorithm detects and reports potential \nserializability violations by .rst assigning a total order to all statements in a program trace. After \nthat, it checks for strict 2PL violations. The second pass iterates through all dynamic state\u00adments of \na program trace and assigns a unique sequence ID to each of them (line 4). Because we have already computed \nCU s in the .rst pass, we can record the sequence ID of the last statement of a CU (line 5), i.e. when \na CU .nishes its execution. In the third pass, the of.ine algorithm checks for con.icts due to a statement \ns0 from a thread t0 and a statement s1 in a CU of a thread other than t0 (line 13-15). If so, line 16 \nchecks for strict 2PL violations. Finally, line 17 reports serializability violations. Data Structures \n// S_T records information of a statement S_T ::= structure (int seqId, T_T thread, CU_T cu, VAL leftValue, \nSET<VAL> rightValues) // CU_T records information about a CU CU_T ::= structure (SET<S_T > stmts, int \nmaxSeqId) // T_T is a set of CU s of a thread trace T_T ::= SET<CU_T > Algorithm 1 int gSeqId := 0 \nS_T s while ((s := next_dyn_stmt_in_P_exec (s)) != NIL) { s.seqId := gSeqId++ // generate a total order \n5 s.cu.maxSeqId := (s.cu.maxSeqId > s.seqId)? s.cu.maxSeqId : s.seqId } while ((s := next_dyn_stmt_in_P_exec \n(s)) != NIL) { forall t in all threads in a program trace { 10 if (t != s.thread) { forall cu in t { \nforall st in cu.stmts { if ((s.leftValue == st.leftValue || s.leftValue in st.rightValues || 15 st.leftValue \nin st.rightValues) &#38;&#38; (cu.maxSeqId > s.seqId > st.seqId)) { report_violation (s, st) }}}}}} Figure \n6: The of.ine algorithm scans the total order of a program trace and records where a CU .nishes its execution. \nIt scans again the program trace and checks for strict 2PL violations to detect serializability violations. \nWhile the of.ine algorithm is simple, it cannot be used online and requires program traces annotated \nwith extra information. Next, we approximate the of.ine algorithm with an online algorithm, which includes \nseveral heuristics. 4.2. An Online Algorithm Unlike the of.ine algorithm, SVD computes CU s and detects \nseri\u00adalizability violations in one-pass. Therefore, SVD can be used online during program execution although \nits performance over\u00adhead may be high. Furthermore, SVD infers which variables are shared as well as \ntrue and control dependences automatically. Recall that the of.ine algorithm requires this extra information. \nOne of the goals of SVD is to detect erroneous program executions regardless of whether program source \ncode is available. Therefore, SVD uses only information that is available from program bina\u00adries. In \nparticular, SVD uses dynamic instructions instead of dynamic program statements as the unit of computation \nand SVD uses .xed-sized memory blocks instead of variables as the unit of memory accesses. As shown in \nFigure 7, the online detection algorithm of SVD observes a stream of events . An event is either a dynamic \ninstruction or a remote access message from other threads. Because threads execute in parallel, multiple \ninstances of this algo\u00adrithm are running simultaneously. Like the of.ine algorithm, this algorithm needs \na data structure to represent a CU (CU_T ). In addition, because dynamic instructions operate on both \nmemory blocks and registers, we de.ne a memory block data structure Data Structures // FSM_STATE as \nde.ned in Figure 8 FSM_STATE ::= enum (Idle, Loaded, Loaded_Shared, Stored, Stored_Shared, True_Dep) \n// CU type CU_T ::= structure (SET<BLK_T> rs, SET<BLK_T> ws) // memory block type BLK_T ::= structure \n(CU_T cu, FSM_STATE state) // register type REG_T ::= structure (SET<CU_T > cuSet) Subroutines check_violations() \n// check serializability violations merge_and_update() // merge units in a cuSet deactivate_log_CU() \n// stop growing a CU &#38; generate log ctrl_dep_from_stack() // aggregate control dependences push_ctrl_cu(), \npop_ctrl_cu() // control dependence stack is_instr() // check if an event is an instruction or a message \n Algorithm 1 repeat until no more events { currrentTarget := 0x0 // control reconvergence point switch \n(event) { case (LOAD) args (block, destReg): 5 if (block.state == Stored_Shared ) deactivate_log_CU(block.cu) \nblock.cu.rs := block.cu.rs + block dest_reg.cuSet := { block.cu } break 10 case (ALU) args (srcR1, srcR2, \ndestR): destR.cuSet := srcR1.cuSet. srcR2.cuSet break case (STORE) args (srcReg1, srcReg2, block): // \nreg 1, reg2 contain data and addr, respectively 15 dataCuSet := srcReg1.cuSet addrCuSet := srcReg2.cuSet \nctrlCuSet := ctrl_dep_from_stack() check_violations(dataCuSet. addrCuSet . ctrlCuSet) 20 block.cu := \nmerge_and_update(dataCuSet) block.cu.ws := block.cu.ws + block break case (BRANCH) args (srcReg, target): \nif (target.op == BA ) // BA is Branch-Always 25 currentTarget := target.target.pc // if...else... else \ncurrentTarget := target.pc // if... push_ctrl_cu(srcReg.cuSet, currentTarget) break case (REMOTE_ACCESS) \nargs (block): 30 if (block.state == True_Dep ) deactivate_log_CU(block.cu)  break } // end of switch \nif (is_instr(event) &#38;&#38; event.pc == currentTarget) 35 currentTarget := pop_ctrl_cu() } // end \nof repeat until Figure 7: SVD s online detection algorithm. (BLK_T) and a register data structure (REG_T). \nIn the following, we describe some key heuristics that enable this online algorithm. Infer true dependences \nvia CU reference propagation. SVD automatically infers true dependences online by propagating unique \nCU references along program data .ow graph as programs execute. First, SVD keeps a CU reference for each \nmemory block in BLK_T.cu. SVD also maintains a set of CU references for each machine register in REG_T.cuSet. \nWhen a memory block is loaded into a register, the CU reference of the memory block is stored in the \nset of CU references of the register (line 8). This tags the register to be true-dependent on the memory \nblock that is rep\u00adresented by the unique CU reference. When an arithmetic instruc\u00adtion is executed, SVD \nobtains the union of the two sets of CU references of the two source registers and stores the resulting \nset in the CU reference set of the destination register (line 11). Similarly, this tags the destination \nregister to be true-dependent on the set of memory blocks that affect the source registers. Finally, \nwhen it comes to store instructions, SVD needs to store the dependences that are represented by the set \nof CU references of the source regis\u00adter into a single CU reference of the destination memory block. \nTherefore, SVD consolidates the dependences by merging all CU s pointed to by the set of CU references \nof the source register (line 20). Function merge_and_update() .rst merges all CU s, then updates old \nCU references stored in memory blocks and registers to a reference that is pointing to the new CU. Note \nthat SVD never explicitly stores the true dependences between td-PDG vertices. Instead, SVD records enough \ninformation so that we know which CU will contain td-PDG vertices that are weakly connected via true \ndependences. Infer partial control dependences via Skipper heuristic. SVD infers partial control dependences \nusing a simple heuristic pro\u00adposed in Skipper [7]. SVD infers only the control dependences of the if-then-else \ntype of control .ows. SVD does not infer control dependences of the loop type control .ows. In particular, \nSVD keeps a stack of branch instructions and their control .ow recon\u00advergence points [11]. When a branch \ninstruction is executed, SVD .rst probes the branch target of the instruction and determines the control \n.ow reconvergence point of the branch instruction (line 24\u00ad26). SVD then pushes the set of CU references \nthat affect the branch instruction s outcome and the reconvergence point onto the stack. Later on, when \nthe control reconvergence point is reached, SVD pops the top of the stack and updates the variable current-Target \nfor next reconvergence point (line 34-35). On line 17, SVD queries control dependences when store instruc\u00adtions \nare executed. Function contrl_dep_from_stack() aggregates all sets of CU references currently stored \nin the control depen\u00addence stack. The resulting set of CU references are used in check\u00ading serializability \nviolations, which will be discussed shortly. Infer shared memory blocks. In multithreaded programs, mem\u00adory \nblocks are allocated, freed, and reallocated. Therefore, a mem\u00adory block can change between being thread-local \nor shared in the life time of the program. SVD keeps a state (BLK_T.state) for each memory block to infer \nif a block is in thread-local or shared state. Note that although memory blocks are shared by all threads, \nSVD s data structures are privately maintained for each individual thread, i.e. different threads have \nseparate BLK_T.state for the same memory block. SVD maintains BLK_T.state using a .nite state machine \nas shown in Figure 8. The state changes according to the sequence of load, store, and remote access events \nthat happen to a block. Among the six states, two states (Loaded_Shared and Stored_Shared) rep\u00adresent \nthat a shared block. When SVD detects that a CU is .n\u00adished, SVD resets the block state of all blocks \nbelonging to the CU to Idle one of the states that represent a thread-local block. Detect shared dependence. \nThe purpose of maintaining BLK_T.state is that SVD can detect when a CU .nishes its execu\u00adtion as shared \ndependences happen. SVD detects shared depen\u00addences on a memory block through two state transitions in \nFigure 8: (I) a load happens on a block in Stored_Shared state (also shown by line 5-6 in Figure 7) or \n(II) a remote access happens on a block in True_Dep state (line 30-31 in Figure 7). In both cases, the \nblock state is changed to Idle as SVD detects the end of a CU. Function deactivate_log_CU() not only \nremoves references of a CU from the SVD data structures (BLK_T, REG_T, and con\u00adtrol dependence stack), \nbut also changes the state of all blocks of a CU to Idle and generates proper entries in the CU log for \nthe a posteriori examination. Check for strict 2PL violations. SVD checks for strict 2PL vio\u00adlations \nwhenever a store instruction is executed (line 18). From the three sets of CU references (Section 4.3 \nexplains addrCuSet), SVD builds a list of memory blocks that the store instruction is control-, true-, \nor address-dependent upon. For each of these mem\u00adory blocks, SVD checks if any con.ict has happened after \nthe CU has accessed the block. For brevity, we do not show how SVD keeps track of con.icts. However, \nthe basic idea is straightforward: SVD keeps a BLK_T.con.ict .ag for each memory block, and sets it as \nload, store and remote access events are observed, and .nally resets it when a CU ends. 4.3. Pragmatic \nConsiderations This section presents several pragmatic considerations to make SVD easier to implement \nand achieve a better trade-off between false negatives and false positives. Represent CU with memory \nblocks, not dynamic instructions. Instead of using a set of dynamic instructions to represent a CU (which \nis similar to the of.ine algorithm that uses a set of dynamic program statements), SVD uses two sets \nof memory blocks: a read set and a write set (CU_T .rs and CU_T .ws) to represent a CU. This is easier \nto implement, because SVD does not need to remem\u00adber an arbitrary number of dynamic instructions. In \nour implemen\u00adtation, because each dynamic instruction accesses at most one memory block, representing \nCU with memory blocks is strictly cheaper than representing CU with dynamic instructions. This approximation, \nhowever, introduces aliases when multiple dynamic instructions access the same memory block. Aliasing \nmakes SVD infer CU s more conservatively, because more dynamic instructions may be included in a CU. \nWe leave studying the impact of this approximation to future work. CU s are weakly connected via only \ntrue dependences. By de.\u00adnition, vertices of a CU should be connected by either true or con\u00adtrol dependence \narcs. However, due to implementation constraints, we have only implemented connecting memory blocks of \na CU through true dependence arcs. As shown on line 20, function merge_and_update() merges only CU references \nfrom data-CuSet. SVD stores the resulting new CU for further dependence propagation. On the other hand, \nSVD does check control depen\u00addences for serializability violations. We leave weakly connecting CU vertices \nvia control dependence arcs to future work. Handle vector, pointer data types (address dependences). \nSVD extends the of.ine algorithm to support vector and pointer data types by checking con.icts on address-dependent \nmemory blocks when store instructions are executed. One of the source registers of a store instruction \ncontains true dependences that affect the address computation of the store instruction (line 16). SVD \ncomputes the address dependences (addrCuSet) and reports serializability vio\u00adlations if any con.ict happens \nto any of the memory blocks that affect the address computation of the store instruction. We do not propagate \naddress dependences after variables are written to mem\u00adory, because we .nd doing so causes more false \npositives. Check only input blocks of a CU. Function check_violations() checks only the input blocks \nof a CU (CU_T .rs) for con.icts (line 18). This heuristic is not suf.cient to detect all serializability \nviolations. However, because SVD is conservative in detecting serializability violations, it is likely \nSVD reports false positives. In practice, we found employing this heuristic is more likely to .nd erroneous \nexecutions that are not serializable, hence, reduces SVD s false positives. Approximate threads with \nprocessors. Finally, in our evaluation infrastructure (Section 6), threads may migrate from one processor \nto another. SVD does not have the ability to detect thread migra\u00adtion. Therefore, SVD approximates threads \nwith processors, i.e. the algorithm shown in Figure 7 has a running copy for each pro\u00ad cessor in a simulated \nmultiprocessor system. 4.4. Potential Hardware SVD As more transistors become available on-chip, we believe \nthat the overhead of the software version SVD can be dramatically reduced if some parts of it are implemented \nin hardware. First, hardware can help SVD infer true and control dependences if we piggyback CU references \npropagation to existing hardware data paths. Sec\u00adond, multiprocessor caches can help store CU s. Finally, \ncache coherence protocols can help detect serializability violations. We leave the detailed design and \nevaluation of hardware SVD to future work. 5. Qualitative Analysis SVD relies on many heuristics to detect \nerroneous program execu\u00adtions. When these heuristics fail, SVD either fails to report errone\u00adous executions \n(false negatives) or mis-reports correct executions (false positives). This section analyzes the most \nimportant causes of false negatives and false positives. 5.1. False Negatives Atomic regions contain \nshared dependences. As shown in Figure 3, a read to a shared variable (5.14) that follows a write to \nthe same variable (5.03) may exist in an atomic region. In other words, atomic regions of a program may \ncontain shared depen\u00addences. When such behavior exists in a program, SVD can cut weakly connected components \nof td-PDG to infer CU s that are smaller than the atomic regions. These small CU s can cause false negatives, \nbecause small CU s may be serializable while the atomic regions are not serializable. Although we mitigate \nthis problem by allowing a posteriori examination to CU s inferred by SVD, these false negatives are \nstill harmful, because SVD can not use BER to avoid them online. A better solution is to detect when \na shared variable is not meant to be shared by the programmer. We leave exploring this direction to future \nwork. Atomic regions contain independent computations. Not all statements in an atomic region are weakly \nconnected. As shown in Figure 9, an atomic region contains code to dequeue and .ll a shared data queue. \nIf both .eld_a and .eld_b are read from pro\u00adgram inputs, i.e. not dependent, then the statements in this \natomic region are not weakly connected. Again, small CU s inferred by SVD can cause false negatives. \nSVD mitigates the problem by checking address dependences (on variable head) before a vari\u00adable is written \nto memory. In our experiments with shared-memory server programs, we have not observed false negatives \n(by compar\u00ading with another race detector) caused by atomic regions that con\u00adtain independent computations. \n5.2. False Positives CU s that are too large. CU s that are larger than the atomic regions can lead to \nfalse positives. CU s can be too large, because SVD cuts CU s when shared dependences are observed, which \nis often after atomic regions have .nished. Strict 2PL violation. SVD can report spurious serializability \nvio\u00adlations when an execution violates strict 2PL but is still serializ\u00adable. Finally, our SVD implementation \nemploys other heuristics, such as representing CU s with memory blocks, using .xed-size memory blocks, \nand checking only input blocks for a CU, etc. These heuris\u00adtics also can cause false negatives and false \npositives. We leave detailed studies of their impacts to future work. Table 1: Test Programs Name Description \nThe Erroneous Execution Apache Apache is a multithreaded open source web server. We use SURGE [5] to \ngenerate web requests that fetches a total of 500MB static web pages. We use apache 2.0.48 and enable \na feature that buffers the access log in memory. Apache has estimated 285,000 lines of code. Apache silently \ncorrupts its access log with this setup. The bug was reported and a patch was available before we applied \nSVD. MySQL MySQL is a multithreaded open source database management system (DBMS). We use MySQL 4.1.1-alpha \nand an in-house query generator to continuously issue SELECT queries using the new prepared query interface \nof MySQL. MySQL has estimated 728,000 lines of code. MySQL crashes non-deterministically with this setup. \nThe crash was reported to MySQL developers. However, the root cause and a patch was not known before \nwe applied SVD. PgSQL PostgreSQL (PgSQL) is a multiprocessed open source DBMS. PgSQL signi.cantly differs \nfrom MySQL in database architecture and coding styles. We use PgSQL 8.0.0 beta3 and OSDL s DBT-2 benchmark \nto emulate a medium sized On-Line Transaction Processing (OLTP) workload that has total 1.4GB database \nand 20 warehouses. Each warehouse has one database connection and 15 terminals. PgSQL has estimated 659,000 \nlines of code. There are no known errors with this setup. Our purpose is to study how well SVD performs \non error-free execution such as these of PgSQL. Next, we quantitatively evaluate SVD using shared-memory \nserver programs (Section 6 and Section 7). 6. Evaluation Methods The major premise of SVD is that it \ncan be integrated with BER to avoid erroneous executions transparently. Our methodology mea\u00adsures the \nsuccess of SVD indirectly by comparing SVD with a happens-before race detector that we have developed. \nBy using both detectors on identical executions of three large shared-memory programs, we measure apparent \nfalse negatives and dynamic false positives of SVD. Apparent false negatives are those erroneous program \nexecutions that are found by the happens\u00adbefore detector but not by SVD. SVD is as effective as the hap\u00adpens-before \ndetector if no apparent false negatives are found. Dynamic false positives are those dynamic instances \nof false posi\u00adtives reported by SVD. Reporting few dynamic false positives is important for a detector \nthat is used in BER, because the number of dynamic false positives is proportional to the performance \nloss due to unnecessary rollbacks. Our methodology favors the happens-before detector, because the required \na priori annotation is available to the happens-before detector only. We also evaluate the static false \npositives and how well a detector helps programmers understand bugs. These metrics are important when \nSVD is used with a post-mortem debugger. Static false posi\u00adtives do not include multiple warnings from \nthe same static piece of code. Reporting few static false positives keeps programmers from being distracted. \nHelping programmers understand bugs means they can .x the bugs more quickly. We also report perfor\u00admance \noverhead of SVD. Next, we describe our evaluation infrastructure and the happens\u00adbefore race detector \nthat is used to compare against SVD. 6.1. Simulation-based Deterministic Replay We use Simics [19], a \nfull-system simulator, at the center of our evaluation infrastructure. First, full-system simulation \nprovides a deterministic and .exible execution environment for shared-mem\u00adory programs. Starting from \nthe same simulation checkpoint each time a program executes in Simics, the thread/process interleaving \nis solely determined by an initial random seed. By specifying the same seed, we can replay the same execution \nin Simics. Second, Simics provides a .exible implementation platform for SVD, which needs to capture \nvarious execution events, such as register reads/writes. Not only does Simics enable us to apply SVD \nto a wide range of programs, but also it allows us to study a hardware design of SVD in future work. \nTo obtain realistic program execution behavior, we use Wisconsin SMP Performance Model [2,20,37] to model \nthe timing of a simu\u00adlated SMP system, which contains four cache-coherent, 1 GHz, 4\u00adway issue, out-of-order, \nSPARC processors running Solaris 9. SVD is entirely hidden from the simulated programs and OS and therefore \ndoes not perturb the simulated program executions. The disadvantage of the simulator is its slowdown \ncompared to the native execution. We overcome this problem by fast-forwarding and sampling the simulated \nexecutions. Fast-forwarding turns off the detailed timing simulation and helps us simulate only the part \nof the program execution that contains the actual bug manifesta\u00adtion. Sampling helps us study how long-running \nprograms may impact SVD. Table 1 summarizes the three server programs we use to test SVD. Programs are \n285,000 lines of source code or greater. Our setups are derived from actual bug reports to these server \nprograms. The bug causing crashes in this MySQL setup was not known prior to running SVD. Our setup adequately \ntests the scalability of SVD. 6.2. The Frontier Race Detector To compare with SVD, we implemented a happens-before \nrace detector called the Frontier Race Detector (FRD). A happens\u00adbefore detector detects a race if two \nthreads access a shared mem\u00adory location and the accesses are causally unordered in a precise sense as \nde.ned by Lamport [18]. The original happens-before detectors compute the causal relationships between \nmemory accesses based on known synchronization. FRD detects data races in two passes. In the .rst pass, \nwithout knowing synchronization, FRD .rst computes the tightest races, i.e. those con.icting accesses \nthat are not causally ordered by any other con.icting accesses. These tightest races are called the frontier \nraces [9]. Then FRD asks the programmer to annotate the frontier races as either data or synchronization \nraces. After that, the frontier race detector scans the same program trace with the known synchronization \naccesses and .nds data races just like a standard happens-before race detector. We chose to use the frontier \nrace detector instead of a standard happens-before detector, because we wanted to avoid annotating the \nmassive amount of source code of the server programs. The frontier detector reports the same set of data \nraces as a happens\u00adbefore detector. However, it requires us to annotate only the syn\u00adchronization operations \nthat actually exist in the program trace. To avoid false sharing, we use word-size blocks in SVD and \nFRD. Table 2: Evaluation Results denotes Segment Million Samples Apparent False Static False Positives \nDynamic False Positives Per Million Insts (Total) SVD s Computational Units abuggy Insts Across Negatives \na posteriori Dynamic CU s Per exec. 4 CPUs SVD FRD SVD FRD Examinations Million Insts (Total) Apache \n 16 1 0 1 2 0.2 (3) 1.3 (20) 2 324 (5183) Apache 16 4 N/A 2 3 0.1 (7) 0.3 (16) 48 47 (2976) MySQL 40 \n1 0 44 91 5.8 (233) 140 (5620) 50 77 (3080) MySQL 40 6 N/A 60 76 8 (1924) 29(6841) 97 77(18399) PgSQL \n16 16 N/A 46 4 1.8 (456) 0.03 (7) 87 8.6 (2194) 7. Evaluation Results Table 2 summarizes our experimental \nresults. We sample multiple execution segments for the three server programs. We report the results from \nboth the erroneous execution samples and bug-free execution samples of Apache and MySQL. 7.1. Apparent \nFalse Negatives In our experiments on Apache and MySQL, we found that SVD exhibits no apparent false \nnegatives. Both SVD and FRD found two timing-dependent bugs in the two programs. Both SVD and FRD found \na known timing-dependent bug in Apache. SVD reported a serializability violation and FRD reported several \ndata races related to this bug. We have examined the details of the bug in Section 2 (Figure 2). SVD \nhelped the programmers to understand the bug by showing that the computation on the buffer index is broken \n, i.e. the input to the computation is changed by other threads before the output of the computation \nis written. Both SVD and FRD found a bug in MySQL whose root cause was previously unknown. SVD also helped \nus understand the bug. In Section 2, we have described the bug, which caused sever crashes (Figure 3). \nWe have con.rmed the root cause of the bug with the MySQL developers. SVD found the root cause of the \nbug by pre\u00adsenting the log of CU inputs and their last thread-local producers to the programmer. FRD \nalso found this bug through data races on the mistakenly shared variables. However, it was not easy for \nus to .nd the race and identify it as a bug, because it was reported by FRD among many other static false \npositives. 7.2. Static and Dynamic False Positives Table 2 also compares the reported false positives \nby both detec\u00ad tors. False positives include both static and dynamic false posi\u00adtives. The ratio of the \nstatic false positives and the actual bugs found indicates how many distractions the programmers would \nhave to deal with when using a detector. The frequency of dynamic false positives (per million instructions \nper CPU) indicates how much performance would be lost in unnecessary rollbacks (BER). For Apache and \nMySQL, SVD improves the race detection accu\u00adracy compared to FRD. With fewer static false positives, \nthe pro\u00adgrammer can .nd timing-dependent bugs more quickly. With fewer dynamic false positives (order \nof magnitude fewer for MySQL), SVD can reduce the unnecessary rollbacks. For PgSQL, SVD reported more \nstatic and dynamic false positives than FRD. PgSQL has a different shared memory architecture than other \nmultithreaded programs we have tested. We speculate that PgSQL developers may have spent more effort \nmaking it data race free. Considering that SVD does not require a priori annotations and the low frequency \nof the dynamic false positives for PgSQL, SVD performs reasonably well for PgSQL. 7.3. Overheads SVD \nhas signi.cant space and time overheads. The overheads mainly come from the following: Algorithm Complexity. \nSVD performs dependence calculations on every instruction of the execution. This incurs a signi.cant \ntime overhead.  Recording CU s. SVD records CU pointer for each memory block, which means the space \noverheads is proportional to total memory footprint of a program.  Debugging Support. SVD collects detailed \ndebugging informa\u00adtion, such as virtual PC and stack traces, which introduces both time and space overheads. \n In our simulator, SVD incurs a signi.cant slowdown, as high as a factor of 65. For some programs, such \nas Apache, SVD doubles the memory usage of the simulator. Despite the high overhead, we found SVD is \nscalable, because its performance overhead did not increase as the program size increases. The scalability \nof SVD is a result of focusing only on particular dynamic executions. Finally, we sampled long executions \n(10 seconds in the steady state) to study if the long executions make SVD report more false positives. \nWe found the number of static false positives grow slowly as the length of the execution increases, which \nmeans the main parameter to the number of static false positives is the exer\u00adcised code size during the \nexecution, not the length of the execu\u00adtion. On the other hand, dynamic false positives approximately \nincreased linearly with the increase of the execution length. 8. Related Work Another project that proposes \nonline bug avoidance is ReEnact [29]. Using hardware available for thread level specula\u00adtion, ReEnact \nstrives to be both a deterministic debugger for data races and an on-the-.y race avoidance mechanism. \nHowever, ReEnact differs from SVD in that it requires an a priori program annotation to perform race \ndetection and requires a prede.ned bug database to avoid of known bugs. SVD, on the other hand, does \nnot require a priori annotations and can help avoid unknown bugs if it is integrated with a BER mechanism. \nExisting bug-detectors for shared-memory programs, such as data race detectors and atomicity violation \ndetectors, strive to detect more bugs, even when a program execution is correct. SVD com\u00adplements these \ntechniques by being a dynamic detector that distin\u00ad guishes erroneous program executions from correct \nexecutions. Netzer and Miller [24] formalize races and point out it is NP-hard to detect data races without \nfalse negatives and false positives. Practical race detectors often sacri.ce detection accuracy in allow\u00ading \nfalse positives, but not allowing false negatives. One type of race detector uses the lockset algorithm. \nThe lockset algorithm checks whether each shared variable in a program is consistently guarded by at \nleast one lock. Eraser [33] uses the lockset algorithm during program execution to .nd data races. RacerX \n[13] uses the lockset algorithm in compile time to .nd data races. Another type of race detector uses \nthe happens-before algorithm. The happens-before algorithm checks whether con.icting accesses to shared \nvariables in a program are ordered by explicit synchroni\u00adzation. Many false positives reported by the \nlockset algorithm can be avoided, because the happens-before algorithm can .nd syn\u00adchronization that \norders the unlocked accesses found by the lockset algorithm. Many dynamic race detectors implement the \nhappens\u00adbefore algorithm in software [10,17,21,32]. Hardware [22,29] and Distributed-Shared-Memory [26,31] \nimplementations were also proposed to reduce the runtime overhead of the detectors. It is also possible \nto combine these two algorithms [12]. Choi et al. have proposed hybrid detectors [8,36] that have both \nlow overhead (lockset) and high accuracy (happens-before). SVD differs from both the lockset and the \nhappens-before algorithms in that it does not require a priori annotations. Recently, researchers have \nnoticed that race detectors cannot .nd all timing-dependent bugs. For example, the stale-value detector \n[6] .nds where stale values are used after critical sections have ended, because this type of program \nbehavior may be an indi\u00adcator of timing-dependent bugs. More generally, atomicity based detectors .nd \natomicity violations for prede.ned program code regions. The static atomicity detector uses a type system \nto allow programmers to specify atomic regions, hereby referred to as atomicity annotations. Therefore, \npotential bugs of atomicity violations can be found statically [16]. The dynamic atomicity detector tries \nto automatically infer atom\u00adicity annotations in Java programs and detects atomicity violations while \nmonitoring the program executions [15]. SVD differs from atomicity detectors in that they use two different \nprogram safety properties serializability versus atomicity. Ato\u00admicity requires that the program codes \nwith atomicity annotations always execute in series. Atomicity detectors check how synchro\u00adnization is \ndone in programs. On the other hand, serializability is concerned with particular program executions. \nAtomic regions inferred by SVD may execute in series in certain executions, but not in others. SVD essentially \nignores how synchronization is done in programs. 9. Conclusions and Future Work We propose a serializability \nviolation detector (SVD) that can detect erroneous executions of shared-memory programs without requiring \na priori program annotations. Such a detector is poten\u00adtially useful in alerting software users as software \nerrors happen online or in triggering recovery to avoid erroneous executions with a backward error recovery \n(BER) mechanism. SVD reports few dynamic false positives, which makes it particularly suitable to be \nused in avoiding erroneous executions caused by unknown bugs. One of our most important contributions \nis to propose a new infer\u00adence method of atomic regions that removes the a priori annota\u00adtion requirement \nexists in typical detectors. We exploit region hypothesis to infer atomic regions dynamically as programs \nexe\u00adcute, without program source code. Experimental results shows that the inference method is effective \non real server programs. In the future, we plan to implement SVD in simulated hardware so that its space \nand time overheads are reduced. With low overhead, SVD can be integrated with a BER mechanism to avoid \n(on-the\u00ad.y) erroneous executions of server programs. 10. Acknowledgements This work is supported in part \nby the National Science Foundation (NSF), with grants CCF-0085949, CCR-0093275, CCR-0105721, EIA/CNS-0103670, \nCCR-0105721, EIA/CNS-0205286, CNS\u00ad0225610, CCR-0243657, CCR-0324878, CCR-0326577 and an award from University \nof California MICRO program, the Okawa Research Award, as well as donations from IBM, Intel, Microsoft, \nand Sun Microsystems. This work has also been supported in part by the Defense Advanced Research Projects \nAgency (DARPA) under contract No. NBCHC020056. Hill has a signi.cant .nancial interest in Sun Microsystems. \nThe views expressed herein are not necessarily those of DARPA, IBM, Intel, Microsoft, NSF, or Sun Microsystems. \nWe would like to thank Remzi Arpaci-Dusseau, Mikko Lipasti, Barton Miller, and David Wood for useful \ncomments based on an earlier version of the paper. We thank Jong-Deok Choi, Ravi Rajwar, Milo Martin \nand Daniel Sorin for discussions. We are grateful to Ben Liblit, Alaa Alameldeen, Kevin Moore, Mike Marty, \nBradford Beckmann, Luke Yen for proofreading the paper. Xu is thankful to Jichuan Chang, Hongfei Guo, \nShiliang Hu, Yun\u00adpeng Li, Yuan Wang and Su Zhang for feedback from different perspectives. We thank all \nanonymous PLDI reviewers for their detailed reviews that were extremely helpful. We thank UW Con\u00addor \ngroup for simulation support. Finally, we are thankful to devel\u00adopers on the Apache/MySQL/PgSQL mailing \nlists for answering questions. References [1] H. Agrawal and J. R. Horgan. Dynamic Program Slicing. In \nProceedings of the SIGPLAN 1990 Conference on Programming Language Design and Implementation, pages 246 \n256, June 1990. [2] A. R. Alameldeen, M. M. K. Martin, C. J. Mauer, K. E. Moore, M. Xu, D. J. Sorin, \nM. D. Hill, and D. A. Wood. Simulating a $2M Commercial Server on a $2K PC. IEEE Computer, 36(2):50 57, \nFeb. 2003. [3] Apache HTTP Server Project. http://www.apache.org/. [4] D. F. Bacon and S. C. Goldstein. \nHardware-Assisted Replay of Multiprocessor Programs. Proceedings of the ACM/ONR Workshop on Parallel \nand Distributed Debugging, published in ACM SIGPLAN Notices, pages 194 206, 1991. [5] P. Barford and \nM. Crovella. Generating Representative Web Workloads for Network and Server Performance Evaluation. In \nProceedings of the 1998 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, pages \n151 160, June 1998. [6] M. Burrows and K. R. M. Leino. Finding stale-value errors in concurrent programs. \nTechnical report, Compaq Systems Research Center Technical Note (2002-04), May 2002. [7] C.-Y. Cher and \nT. N. Vijaykumar. Skipper: a microarchitecture for exploiting control-flow independence. In Proceedings \nof the 34th Annual IEEE/ACM International Symposium on Microarchitecture, pages 4 15, Dec. 2001. [8] \nJ.-D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Sridharan. Efficient and precise datarace \ndetection for multithreaded object-oriented programs. In Proceedings of the SIGPLAN 2002 Conference on \nProgramming Language Design and Implementation, pages 258 269, June 2002. [9] J.-D. Choi and S.-L. Min. \nRace Frontier: Reproducing Data Races in Parallel-Program Debugging. In Proceedings of the 3rd ACM SIGPLAN \nSymposium on Principles and Practice of Parallel Programming (PPOPP), pages 145 154, July 1991. [10] \nM. Christiaens and K. D. Bosschere. TRaDe, a topological approach to on-the-fly race detection in java \nprograms. In Proceedings of the Java Virtual Machine Research and Technology Symposium (JVM 01), 2001. \n[11] J. D. Collins, D. M. Tullsen, and H. Wang. Control Flow Optimization Via Dynamic Reconvergence Prediction. \nIn Proceedings of the 37th Annual IEEE/ACM International Symposium on Microarchitecture, pages 129 140, \nDec. 2004. [12] A. Dinning and E. Schonberg. The Empirical Comparison of Monitoring Algorithms for Access \nAnomaly Detection. In Proceedings of the 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel \nProgramming (PPOPP), pages 1 10, Mar. 1990. [13] D. Engler and K. Ashcraft. RacerX: effective, static \ndetection of race conditions and deadlocks. In Proc. 19th Symposium on Operating System Principles, pages \n237 252, Oct. 2003. [14] K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger. The notions of consistency \nand predicate locks in a database system. Communications of the ACM, 19(11):624 633, 1976. [15] C. Flanagan \nand S. N. Freund. Atomizer: a dynamic atomicity checker for multithreaded programs. In Proceedings of \nthe 31st ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 256 267, 2004. [16] \nC. Flanagan and S. Qadeer. A Type and Effect System for Atomicity. In Proceedings of the SIGPLAN 2003 \nConference on Programming Language Design and Implementation, June 2003. [17] Y.-K. Jun and K. Koh. On-the-Fly \nDetection of Access Anomalies in Nested Parallel Loops. In Proceedings of the ACM/ONR Workshop on Parallel \nand Distributed Debugging (PADD), pages 107 117, 1993. [18] L. Lamport. Time, Clocks and the Ordering \nof Events in a Distributed System. Communications of the ACM, 21(7):558 565, July 1978. [19] P. S. Magnusson \net al. Simics: A Full System Simulation Platform. IEEE Computer, 35(2):50 58, Feb. 2002. [20] C. J. Mauer, \nM. D. Hill, and D. A. Wood. Full System Timing-First Simulation. In Proceedings of the 2002 ACM Sigmetrics \nConference on Measurement and Modeling of Computer Systems, pages 108 116, June 2002. [21] B. P. Miller \nand J.-D. Choi. A Mechanism for Efficient Debugging of Parallel Programs. In Proceedings of the SIGPLAN \n1988 Conference on Programming Language Design and Implementation, pages 135 144, June 1988. [22] S. \nL. Min and J.-D. Choi. An Efficient Cache-based Access Anomaly Detection Scheme. In Proceedings of the \nFourth International Conference on Architectural Support for Programming Languages and Operating Systems, \npages 235 244, Apr. 1991. [23] MySQL AB. http://www.mysql.com/. [24] R. H. B. Netzer and B. P. Miller. \nWhat are Race Conditions?: Some Issues and Formalizations. ACM Letters on Programming Languages and Systems, \n1(1):74 88, Mar. 1992. [25] C. Papadimitriou. The Theory of Database Concurrency Control. Computer Science \nPress, Rockville, Maryland, 1986. [26] D. Perkovic and P. Keleher. A Protocol-Centric Approach to On-The-Fly \nRace Detection. IEEE Transactions on Parallel and Distributed Systems, 11(10):1058 1072, Oct. 2000. [27] \nPostgreSQL Global Development Group. http://www.postgresql.org/. [28] K. Poulsen. SecurityFocus News: \nTracking the blackout bug. http://www.securityfocus.com/news/8412. [29] M. Prvulovic and J. Torrellas. \nReEnact: Using Thread-Level Speculation Mechanisms to Debug Data Races in Multithreaded Codes. In Proceedings \nof the 30th Annual International Symposium on Computer Architecture, pages 110 121, June 2003. [30] M. \nPrvulovic, Z. Zhang, and J. Torrellas. ReVive: Cost-Effective Architectural Support for Rollback Recovery \nin Shared-Memory Multiprocessors. In Proceedings of the 29th Annual International Symposium on Computer \nArchitecture, pages 111 122, May 2002. [31] B. Richards and J. R. Larus. Protocol-based Data-race Detection. \nIn SIGMETRICS symposium on Parallel and Distributed Tools, pages 40 47, 1998. [32] M. Ronsse and K. D. \nBosschere. Non-intrusive On-the-fly Data Race Detection using Execution Replay. In Automated and Algorithmic \nDebugging, Nov. 2000. [33] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson. Eraser: \nA Dynamic Data Race Detector for Multithreaded Programs. ACM Transactions on Computer Systems, 15(4):391 \n411, Nov. 1997. [34] D. J. Sorin, M. M. K. Martin, M. D. Hill, and D. A. Wood. SafetyNet: Improving the \nAvailability of Shared Memory Multiprocessors with Global Checkpoint/Recovery. In Proceedings of the \n29th Annual International Symposium on Computer Architecture, pages 123 134, May 2002. [35] U.S.-Canada \nPower System Outage Task Force. Final Report on the August 14th Blackout in the United States and Canada. \nTechnical report, Department of Energy, 2004. [36] C. von Praun and T. Gross. Object-Race Detection. \nIn Proceedings of the Conference on Object-Oriented Programming, Systems, Languages and Application (OOPSLA), \nOct. 2001. [37] Wisconsin Multifacet GEMS Simulator. http://www.cs.wisc.edu/gems/. [38] M. Xu, R. Bod\u00edk, \nand M. D. Hill. A Flight Data Recorder for Enabling Full-system Multiprocessor Deterministic Replay. \nIn Proceedings of the 30th Annual International Symposium on Computer Architecture, pages 122 133, June \n2003.   \n\t\t\t", "proc_id": "1065010", "abstract": "We aim to improve reliability of multithreaded programs by proposing a dynamic detector that detects potentially erroneous program executions and their causes. We design and evaluate a <i>Serializability Violation Detector</i> (SVD) that has two unique goals: (I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply alerting users that a software error may have occurred; and (II) helping debug programs by revealing causes of error symptoms.Two properties of SVD help in achieving these goals. First, to detect only erroneous executions, SVD checks serializability of atomic regions, which are code regions that need to be executed atomically. Second, to improve usability, SVD does not require <i>a priori</i> annotations of atomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widely-used multithreaded server programs show that SVD finds real bugs and reports modest false positives. The goal of this paper is to develop a detector suitable for (I) BER-based avoidance of erroneous program executions; and (II) alerting users as software errors occur. We argue that such a detector should have the following two properties.", "authors": [{"name": "Min Xu", "author_profile_id": "81100195436", "affiliation": "University of Wisconsin-Madison", "person_id": "PP43117363", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "University of California, Berkeley", "person_id": "P517421", "email_address": "", "orcid_id": ""}, {"name": "Mark D. Hill", "author_profile_id": "81100455115", "affiliation": "University of Wisconsin-Madison", "person_id": "PP40027462", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065013", "year": "2005", "article_id": "1065013", "conference": "PLDI", "title": "A serializability violation detector for shared-memory server programs", "url": "http://dl.acm.org/citation.cfm?id=1065013"}