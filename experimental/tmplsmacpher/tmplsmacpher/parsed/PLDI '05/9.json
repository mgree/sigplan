{"article_publication_date": "06-12-2005", "fulltext": "\n Code Placement for Improving Dynamic Branch Prediction Accuracy Daniel A. Jim\u00b4enez Deptartment of Computer \nScience Departamento de Arquitectura de Computadores Rutgers University and Universidad Polit\u00b4ecnica \nde Catalu na Piscataway, New Jersey, USA Barcelona, Catalu na, Spain djimenez@cs.rutgers.edu Abstract \nCode placement techniques have traditionally improved instruction fetch bandwidth by increasing instruction \nlocality and decreasing the number of taken branches. However, traditional code placement techniques \nhave less bene.t in the presence of a trace cache that alters the placement of instructions in the instruction \ncache. More\u00adover, as pipelines have become deeper to accommodate increasing clock rates, branch misprediction \npenalties have become a signif\u00adicant impediment to performance. We evaluate pattern history ta\u00adble partitioning, \na feedback directed code placement technique that explicitly places conditional branches so that they \nare less likely to interfere destructively with one another in branch prediction tables. On SPEC CPU \nbenchmarks running on an Intel Pentium 4, branch mispredictions are reduced by up to 22% and 3.5% on \naverage. This reduction yields a speedup of up to 16.0% and 4.5% on av\u00aderage. By contrast, branch alignment, \na previous code placement technique, yields only up to a 4.7% speedup and less than 1% on average. Categories \nand Subject Descriptors D.3.4 [Processors]: Compil\u00aders, Code generation, Optimization General Terms Performance, \nExperimentation Keywords Compilers, Branch prediction 1. Introduction As pipeline depths increase to \nsupport higher clock rates, the penalty for a mispredicted conditional branch also increases, reduc\u00ading \nthe number of instructions executed per cycle. Improvements in branch predictor accuracy can have a signi.cant \npositive impact on performance. Almost all modern microprocessors use a pattern history table (PHT) to \npredict the outcomes of conditional branches by tracking the tendency of a branch to be taken or not \ntaken given a branch address and histories of branch outcomes. Because of strict timing and area limitations, \nPHTs have relatively few en- Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 05, June 12 15, 2005, Chicago, Illinois, USA. Copyright 2005 ACM 1-59593-056-6/05/0006...$5.00. \ntries compared with the number of branches and branch histories. Thus, mispredictions caused by con.icts \nin PHTs are inevitable. In this paper, we evaluate pattern history table partitioning (PHT partitioning), \na code placement technique that places conditional branches at addresses such that destructive interference \nin the PHT is reduced. The compiler divides an abstract model of the PHT into 2 or more partitions, each \nintended to predict branches with a particular kind of behavior (e.g. branches that are strongly biased \nto be taken). Conditional branch instructions are explicitly placed such that their addresses tend to \nmap to the most appropriate PHT partition. This paper makes the following contributions: 1. We present \nthe .rst experimental study of an actual implemen\u00adtation of pattern history table partitioning. We evaluate \nthis technique on the Intel Pentium 4, a microprocessor that has an instruction trace cache that signi.cantly \nreduces the bene.t of traditional code placement techniques such as branch align\u00adment [4]. On SPEC CPU \ninteger benchmarks, our technique reduces branch mispredictions by up to 22% and 3.5% on aver\u00adage. This \nreduction yields a speedup of up to 16.0% and 4.5% on average. By contrast, branch alignment yields only \nup to a 4.7% speedup and less than 1% speedup on average. 2. We compare our algorithm with a modi.ed \nversion that be\u00adhaves similarly to address adjustment with branch classi.\u00adcation, a previous technique \n[6]. Address adjustment reduces branch mispredictions but tends to increase instruction counts. When \nthe algorithm is constrained to insert fewer no-ops, it im\u00adproves average speedup by only 1.5%. When \nmore no-ops are inserted, the technique results in a slight slowdown on average. By contrast, our PHT \npartitioning achieves the same reduction in branch mispredictions as aggressive address adjustment but \ndelivers a signi.cant speedup.  1.1 Motivation A .rst version of our technique builds on the observation \nthat branches tend to be highly biased to be either taken or not taken. It makes sense to partition the \nPHT into two halves: one for biased taken branches and one for biased not taken branches. This way, branch \ninstructions mapping to the same PHT entry will tend to have similar behavior and will not interfere \ndestructively with one another. This observation has been made in previous work [23] and generalized \nand exploited in microarchitectural simulation [6]. Figure 1 uses greyscale intensities to show the average \nvalue of PHT entries before and after a transformation that maps biased taken branches to even indices \nand biased not taken branches to odd indices. These results are gathered from trace-based simula\u00ad  Figure \n1. Average PHT entries before and after compiler-based alignment. tion facilitated by SimpleScalar / \nAlpha [2]1. Each pixel represents the average value of an entry of the PHT for the SPEC CPU 2000 benchmark \n176.gcc and a 4,096-entry GAs (Global Adaptive set-based) [31] branch predictor. This predictor concatenates \nthe 4 lower bits of the branch address with the last 8 branch outcomes to form an index into the PHT. \nThe PHT entries are two-bit saturating counters that are incremented or decremented if the corresponding \nbranch was taken or not taken, respectively. Before the optimization the pattern history table is a jumbled \nmess of counters with a sig\u00adni.cant amount of destructive interference. After the optimization, the pattern \nhistory table is divided into entries that predict mostly taken on the left and mostly not taken on the \nright. The average PHT entry is brighter on the right and darker on the left, illustrating a reduction \nin destructive interference. The result of this reduction in interference is that the misprediction rate \nfor 176.gcchas been cut almost in half, from 13.2% to 7.7%. This nice result comes from the world of \nsimulation, in which we completely specify the design of the branch predictor. The chal\u00adlenge we address \nin this paper is to develop a similar optimization for an existing microprocessor whose branch predictor \nis complex and not well documented: Intel s Pentium 4.  1.2 Paper Organization This paper is organized \nas follows. In Section 2, we explore pre\u00advious related work in optimizing for instruction fetch and branch \nprediction. In Section 3 we give background into two-level adap\u00adtive branch prediction. In Section 4 \nwe describe our optimization. In Section 5 we describe our experimental methodology for eval\u00aduating the \nnew optimization for the Intel Pentium 4 microproces\u00adsor. In Section 6 we give the results of our experiments \nshowing speedups and reductions in branch mispredictions. In Section 7 we conclude and give directions \nfor future research.  2. Related Work In this section we discuss related work in code placement tech\u00adniques. \n2.1 Pro.le Guided Code Placement Hat.eld and Gerald [8], McFarling [20], Pettis and Hanson [25], and \nGloy and Smith [11] have presented methods to reorder proce\u00addures to improve locality based on pro.le \ndata. McFarling presents an algorithm for placing code so as to minimize con.ict misses in a direct-mapped \ninstruction cache [19]. 1 We use simulation only to generate Figures 1 and 7. All other results in this \npaper are from a real Intel Pentium 4. 2.2 Code Placement for Reducing Branch Costs Without respect \nto branch prediction concerns, it is generally better for a frequently executed branch to be laid out \nsuch that it is usually not taken. This way, the instruction cache is used more ef.ciently as the hot \npath through the code is laid out sequentially. Also, machines that fetch multiple instructions in a \nsingle cycle bene.t from not taken branches because they do not prematurely terminate fetching and thus \nmay exploit the full fetch width on every cycle for which there is an instruction cache hit. Calder and \nGrunwald present branch alignment, an algorithm that seeks to minimize the number of taken branches by \nreordering code such that the hot path through a procedure is laid out in a straight line [4]. Their \ntechnique improves performance by an average of 5% on an Alpha AXP 21064. We make use of the greedy version \nof branch alignment to compare against our proposed technique on a modern machine. Young et al. present \na version of branch alignment [32] based on an algorithm for solving the Directed Traveling Salesman \nProblem and show that both their algorithm and the greedy branch alignment algorithm presented by Calder \nand Grunwald are close to optimal with respect to minimizing the number of taken branches. Software Trace \nCache is a code-reordering optimization intro\u00adduced by Ramirez et al. that packs frequently used code \nin a section of memory so as to minimize con.ict misses in a direct-mapped in\u00adstruction cache [26]. The \nSoftware Trace Cache idea also lays out hot paths so that most branches are not taken. Ramirez et al. \nstudy the effect of such code reordering techniques on branch prediction, showing that branch prediction \naccuracy can be positively or neg\u00adatively affected by code reordering depending on the details of the \nbranch predictor [27]. For instance, by increasing the number of not-taken branches, branch alignment \ntechniques can skew the dis\u00adtribution of accesses to PHT entries. Software Trace Cache is eval\u00aduated \nthrough detailed microarchitectural simulation and assumes a direct-mapped instruction cache. 2.3 Code \nPlacement for Improving Branch Prediction Accuracy 2.3.1 Address Adjustment Chen and King .rst propose \nthe idea of using code placement to re\u00adduce interference in pattern history tables [6]. They propose \nwalk\u00adtime address adjustment, a technique that adjusts branch addresses at link-time to avoid interference \nin branch prediction tables. As in our optimization, no-op instructions are inserted to move branch addresses \nto positions less likely to interfere with one another and the PHT is divided into partitions representing \ndifferent kinds of branch behavior. That research considered simple branch predic\u00adtors evaluated through \nsimulating the .rst 30 million branches of 8 SPEC CPU benchmarks. The research did not demonstrate an \nimprovement in performance for large or complex branch predic\u00adtors such as the ones encountered in modern \nmicroarchitectures. The address adjustment technique yielding the best improvement in branch prediction \naccuracy also places many extra no-op instruc\u00adtions on the critical path, a fact whose impact is not \nrealized through simulations that measure only misprediction rate. Extra inserted no\u00adops with this technique \ncan also hinder performance by violating alignment heuristics. For instance, Chen and King s address \nad\u00adjustment can move branch targets off of fetch block boundaries and cause a decrease in the instruction \nfetch rate and an increase in in\u00adstruction cache miss rates. Our technique minimizes both mispre\u00addiction \nrate and extra instruction count and is the .rst such tech\u00adnique to demonstrate signi.cantly improved \nperformance on a real machine. Our technique also respects compiler alignment heuristics designed to \nmaximize the instruction fetch rate. In Section 6 we con.gure our algorithm to behave similarly to that \nof Chen and King s best performing technique, branch classi\u00ad.cation. It shows the negative impact on \nperformance of improving misprediction rate without proper consideraton for keeping instruc\u00adtion count \nfrom increasing too much. The practical impact of Chen and King s work is to suggest that address adjustment \nbe taken into account by microarchitects when designing a branch predictor. That is, address adjustment \nenables good accuracy with smaller branch predictor. A smaller predictor is desirable because it will \nhave a lower access latency and thus mitigate the negative impact on performance of a high\u00adlatency predictor \n[14]. Our work addresses a different issue: how to optimize for a .xed branch predictor in a real microarchitecture. \nIn the same spirit as our work and the work of Chen and King, Milenkovic et al. suggest that compiler \ntechniques could be used to avoid destructive interference in branch predictors [23]. However, they do \nnot provide an algorithm or an implementation.  2.4 Branch Allocation Our idea is similar to branch \nallocation [16]. Branch allocation uses the working set characteristics of branches to explicitly assign \neach conditional branch a set of branch history table resources at compile time. The analysis forms a \ncon.ict graph between branches and uses a technique similar to register allocation to al\u00adlocate branch \nhistory table resources among branches such that destructive aliasing is reduced. However, branch allocation \nmod\u00adi.es the instruction set to allow branch instructions to explicitly specify an index into the pattern \nhistory table. Branch allocation is shown to improve misprediction rates through simulation. By contrast, \nour research implicitly guides branches to destinations in the PHT and thus requires no modi.cation to \nthe instruction set. 2.4.1 Static Correlated Branch Prediction Young and Smith introduce static correlated \nbranch prediction [33], a technique that duplicates basic blocks with branches whose out\u00adcomes are highly \ndependent on the immediately preceding program path. This technique seeks to improve branch prediction \naccuracy for machines where branches are statically predicted, i.e., the pre\u00addiction is determined at \ncompile-time. This technique requires path pro.ling which is more expensive than the edge pro.ling required \nby our technique. Static correlated branch prediction duplicated basic blocks resulting in signi.cant \ncode expansion while our tech\u00adnique does not. Most importantly, our technique addresses dynamic branch \nprediction accuracy used in modern microprocessors, and not older static branch prediction techniques. \n 2.5 Other Compiler-Related Branch Prediction Some processors implement static branch prediction either \nimplic\u00aditly through heuristics or explicitly through compiler-provided hint bits that are set to indicate \nthe likely direction of each branch. Some branches can be predicted with a static bias bit, while others \nwith less biased behavior can use the dynamic predictor. Since the eas\u00adily predictable branches are .ltered \nout, aliasing in the dynamic predictor is alleviated and accuracy is improved. This technique, along \nwith a methodology for choosing the bias bits, was intro\u00adduced by Chang et al. [5]. Patil and Emer study \nthe technique, measuring its utility in reducing destructive aliasing and re.ning the heuristics used \nto decide which branches should be predicted statically [24]. The Intel Pentium 4 instruction set includes \nhint bits for conditional branches, but these hints are used only during trace construction [13] and \nare unlikely to have an impact on dynamic branch prediction accuracy.  3. Branch Prediction Background \nIn this section, we give background into two-level adaptive branch prediction that is used in many modern \nmicroprocessors. 3.1 The Need for Branch Prediction Modern pipelined microprocessors consult branch predictors \nto speculatively fetch and execute instructions beyond conditional branches. When a conditional branch \nis fetched, its outcome and target may not be computed for many cycles. Nevertheless, instruc\u00adtion fetch \nmust continue to feed the rest of the execution engine and sustain performance. Branch predictors quickly \npredict the likely direction of a branch and allow the processor to continue to fetch instructions down \nthe predicted path. Processors that implement speculative execution execute instructions on the predicted \npath and have a mechanism for rolling back and restarting execution if a branch is mispredicted. A branch \nmisprediction can be very costly in terms of the number of wasted clock cycles and wasted energy processing \nwrong-path instructions. As pipelines become deeper to support higher clock rates, the penalty of a mispredicted \nbranch also increases and has a higher negative impact on performance. 3.2 Two-Level Adaptive Branch \nPrediction Yeh and Patt observed that the outcome of a given branch is often highly correlated with the \noutcomes of other recent branches [31]. This history of branch outcomes forms a pattern that can be used \nto provide a dynamic context for prediction. In the branch prediction scheme of Yeh and Patt, every time \na branch outcome becomes known, a single bit (0 for not taken, 1 for taken) is shifted into a pattern \nhistory register. A pattern history table of two-bit saturating counters is indexed by a combination \nof branch address and his\u00adtory register. The high bit of the counter is taken as the prediction. Once \nthe branch outcome is known, the counter is decremented if the branch is not taken, or incremented otherwise, \nand the pattern history is updated. Much research in the 1990s focused on re.n\u00ading this scheme of Yeh \nand Patt. For instance, hybrid predictors that combine branch predictors to improve accuracy have been \npro\u00adposed [21, 10] and implemented [15]. 3.3 Destructive Interference in Pattern History Tables Destructive \ninterference occurs when two distinct branches with opposite behavior coincidentally use the same counter \nin the pat\u00adtern history table. Destructive interference will lead to one of the branches being predicted \nincorrectly. This situation is somewhat similar to con.ict misses in data caches where multiple items \nin memory map to the same set in the cache resulting in cache misses [22]. Several predictor organizations \nhave been proposed to deal with the problem of destructive interference [17, 29, 9], but the problem \ncannot be entirely eliminated because the number of counters in pattern history tables is much less than \nthe number of branches and branch histories in typical programs. In this paper, we present a software \napproach to reducing destructive interference. Almost all approaches to reducing the impact of destructive \nin\u00adterference have been studied at the microarchitectural level through simulation. The agree predictor \nuses a PHT to predict whether branch outcome will agree with a bias bit either provided stati\u00adcally or \nlearned online [29]. In this way, the agree predictor trans\u00adforms possibly destructive interference into \nconstructive interfer\u00adence since most branches are likely to agree with their bias. Our technique is \nsimilar to this approach in that we statically group branch instructions in a partition of the PHT with \nother branch in\u00adstructions with similar behavior. Of course, our technique is a soft\u00adware technique applied \nto a real machine, not a microarchitectural innovation. One hardware technique in particular is closely \nrelated to our approach. The BiMode predictor divides the predictor into three tables: one PHT for biased \ntaken branches, one PHT for biased not taken branches, and a third chooser PHT that tracks the biases \nof branches to decide which of the .rst two PHTs to use for predicting a given branch [17]. This technique \ndoes in hardware what our technique does in software. Again, note that our technique is a more feasible \napproach since it does not require a change to the microarchitecture and does not introduce a level of \nindirection and delay into the critical path of the branch predictor. 3.4 Branch Prediction in the Intel \nPentium 4 For this study, we choose the Intel Pentium 4 as our optimization target. This very popular \nmicroprocessor is widely used in a range of applications. It has several features that make it an interesting \ncandidate for our technique: 1. It has a 20-stage pipeline to support a high clock frequency. This means \nthat the Intel Pentium 4 will be sensitive to branch predictor accuracy, as a mispredicted branch will \nincur a sub\u00adstantial cycle penalty. 2. It has an instruction trace cache [28] that stores decoded in\u00adstructions \nin the order they were fetched rather than the order they appear in the program text. This tends to reduce \nthe im\u00adprovement provided by traditional code placement techniques that seek to improve instruction locality \nsince the trace cache has the effect of dynamically reordering frequently used sec\u00adtions of code. Our \ncode placement technique is still able to im\u00adprove performance because the instruction fetch engine uses \nthe same branch outcome predictor to guide speculation whether it is fetching from the second level cache \nor the trace cache [23] (although a different predictor is used to predict the next trace to fetch). \nThis means that even when the Pentium 4 is fetching from the trace cache it is still driving speculation \nby predict\u00ading branches using branch addresses and those predictions are improved by our technique. \n3. The branch predictor of the Intel Pentium 4 is not well doc\u00adumented. From published Intel documentation \n[12], we know that the branch predictor has a 4,096-entry branch target buffer (BTB) with presumably \ncorrespondingly many entries in a PHT. The predictor is able to fully predict loop back edges with a \ntrip count of between 1 and 16. It can correctly predict branches with taken/not-taken pattern lengths \nbetween 1 and 4. Beyond that information, what little is known publicly about the Intel Pentium 4 branch \npredictor has been gathered through reverse-engineering [23]. We hypothesize that the branch pre\u00addictor \nuses some bits from the conditional branch fetch address as part of an index into a PHT to make a prediction. \nWe use trial and error to determine which bits are the most likely to  both be used by the predictor \nand are feasible to be used by our optimization.  4. Pattern History Table Partitioning In this section, \nwe describe the pattern history table partitioning (PHT partitioning) technique. We .rst describe bimodal \nPHT par\u00adtitioning that divides the PHT into two partitions, one for biased taken branches and the other \nfor biased not taken branches. We then describe a variation called 4-way PHT partitioning that di\u00advides \nthe PHT into quadrants for branches that are combinations of strongly and weakly biased taken or not \ntaken. Figure 2 graphi\u00adcally illustrates the manner in which the PHT is partitioned in the two versions. \n 4.1 The Basic Idea Behind Bimodal PHT Partitioning The main idea for bimodal PHT partitioning is to \nplace branches with similar biases (i.e. taken or not taken) such that they use the same half of the \nPHT. We assume that the branch prediction algorithm uses at least one lower-order bit from the branch \naddress, along with branch history, for indexing the PHT. PHT partitioning ensures that this bit is usually \none value, e.g. 0, for biased not taken branches and usually the other value for biased taken branches. \nThis task is accomplished by inserting multiple no operation (no\u00adop) instructions between regions in \nthe code identi.ed by the PHT partitioning algorithm. This padding with no-ops changes the bits in the \naddresses of every instructions; the goal is to maximize the number of conditional branch instructions \nwhere the relevant bit matches the branch bias. 4.1.1 Tracking Instruction Address Bits The PHT partitioning \nalgorithm performs an assembly of each procedure to keep track of the sizes of all instructions so that \nthe algorithm can always compute the lower-order bits address of a given instruction. This tracking of \ninstruction sizes is non-trivial in the Intel Pentium 4 ISA, where jump instructions may change sizes \ndepending on the magnitude of the distance to their targets; after each proposed placement, the offsets \nare recomputed and the sizes of jump instructions are adjusted accordingly. In addition, the front-end \ncompiler inserts alignment pseudo-ops so that e.g. branch targets are aligned on 16-byte boundaries whenever \nsuch alignment results in the insertion of fewer than 8 no-op instructions; our algorithm tracks the \naddress bits generated by this alignment. There is no documentation explaining which address bits in \nthe Intel Pentium 4 are used in indexing the PHT, but we hypothe\u00adsize that several of the lower order \nbits are used. In practice, we .nd that using the fourth bit of the branch address provides the best \ntrade-off between improved branch prediction accuracy and reduced instruction cache locality. Thus, we \nneed only keep track of addresses modulo 24 =16. The beginning of each procedure is aligned on a 16-byte \nboundary so that it begins at address 0 modulo 16. However, for the purpose of describing a general algorithm, \nlet us say that the goal of PHT partitioning is to assign the kth bit of the branch address, where k \nis a parameter chosen for the given mi\u00adcroarchitecture. Thus, our algorithm needs to keep track of branch \naddress bits modulo 2k .  4.1.2 Pro.ling Requirements The PHT partitioning algorithm requires edge pro.les, \ni.e., infor\u00admation from a training run about how often any edge in the control .ow graph has been traversed. \nThis way the algorithm can infer the biases of branches to be taken or not taken. Not every branch can \nbe placed with the most appropriate PHT partition, so the fre\u00adquency information from the edge pro.les \nis used to prioritize the frequently executed branches for placement. All Branches Biased Not Taken \nBiased Taken Branches Branches Strongly Biased Strongly Biased Not Taken Taken Branches Branches Weakly \nBiased Weakly Biased Not Taken Taken Branches Branches  Unpartitioned PHT Bimodal PHT Partitioning 4-Way \nPHT Partitioning Figure 2. Each box represents the PHT. Bimodal PHT partitioning divides the PHT into \ntwo halves; 4-Way PHT Partitioning divides it into four quadrants. 4.1.3 Choosing Regions Placing multiple \nno-op instructions in arbitrary locations of a pro\u00adgram can have a negative impact on performance because \nit can de\u00adcrease useful fetch bandwidth. Thus, PHT partitioning places no\u00adop instructions only where \nit is unlikely that they will hurt per\u00adformance. Each procedure is divided into a number of regions be\u00adtween \nwhich no-ops may be inserted by PHT partitioning. The ba\u00adsic blocks are scanned in the sequence they \nappear in the procedure to build regions. The .rst region begins with the .rst basic block in the procedure. \nRegions may be ended by a basic block if it satis.es one of the following rules: 1. The basic block does \nnot fall through. For instance, no-ops inserted after a basic block ending in a jump or a return will \nnot be executed and thus will have no impact on fetch bandwidth. 2. The basic block ends in a conditional \nbranch that is taken at least 99.9% of the time and at least 100 times. This way, no\u00adops on the fall-through \npath will be executed relatively infre\u00adquently. We require that the branch be taken at least 100 times \nso that we do not needlessly increase code size by padding very infrequent basic blocks that are unlikely \nto have any impact on performance. We refer to this rule as the frequency heuristic.  4.1.4 Padding \nBetween Regions Once regions are chosen for a given procedure, the PHT partition\u00ading algorithm visits \neach region in sequence. It determines the ef\u00adfect of inserting each possible number of one-byte no-op \ninstruc\u00adtions from 0 through 2k - 1on assigning branches to their proper partitions. For each proposed \nnumber of no-ops to insert, the algo\u00adrithm recomputes the addresses of every branch instruction in the \nregion, taking into account changes in branch instruction lengths and compiler-inserted alignment pseudo-ops. \nIt inserts the number of no-ops that maximizes the number of branches assigned to their proper PHT partitions, \nweighted by edge frequency. That is, it in\u00adserts a number of no-ops that maximizes the number of dynamic \nbranches in the pro.led run assigned to their proper partition. It is important to note that inserting \nno-ops between regions composed of multiple basic blocks provides only coarse control over the ad\u00addresses \nof branches. For example, in the SPEC CPU 2000 bench\u00admark 254.gap, 85% of the static branches are assigned \nto their proper PHT partition, versus 53% before applying PHT partition\u00ading.  4.2 Four-Way PHT Partitioning \nBimodal PHT partitioning uses one address bit to divide the PHT into two partitions. Since the branch \npredictor presumably uses more than one address bit, it behooves us to investigate using more than one \nbit to divide the PHT into more than two partitions. But what behavior should each of the new partitions \nseek to capture? Four-way PHT partitioning divides the PHT into 4 regions, separating branches into 4 \nclasses of behavior: strongly biased taken, strongly biased not taken, weakly biased taken, and weakly \nbiased not taken. We de.ne strong branch behavior to mean that the behavior is encountered at least 95% \nof the time, e.g. we say a branch is strongly biased taken if it is taken at least 95% of the time it \nis executed. We decided on the 95% threshold based on behavior we observed in the pro.les we gathered. \nOn average, approximately 50% of the dynamic branches in our benchmark suite were either taken or not \ntaken 95% of the time. Thus, choosing 95% as our threshold for strong behavior should ensure a balanced \ndistribution of branches to PHT partitions. The details of 4-way PHT partitioning are the same as bimodal \nPHT partitioning except for the procedure for determining the best number of no-ops to insert between \nregions. We choose a number of no-ops to insert that maximizes a .tness function de.ned as follows. The \n.tness function is the sum over all branches in the region of: 1. The frequency with which the branch \nagrees with its bias if both the strength and bias match the PHT partition to which it is mapped, 2. \nHalf of the frequency with which the branch agrees with its bias if only the bias matches the PHT partition, \n 3. 0 if neither bias nor strength match.  Thus, branches that agree in both strength and bias with \ntheir assigned PHT partition are most preferred, branches that agree in only bias are preferred less, \nand branches that agree in neither strength nor bias are avoided. We found that, for the Intel Pentium \n4, using the third and sixth bits of the branch address yielded the best trade-off between im\u00adprovement \nin branch prediction and reduced instruction cache lo\u00adcality due to the inserted no-ops. Thus, for 4-way \nPHT partitioning, from 0 through 26 - 1=63no-op instructions may be inserted be\u00adtween regions.  5. Methodology \nIn this section, we explain our experimental methodology for eval\u00aduating the effect of PHT partitioning. \n 5.1 Benchmarks Table 1 shows the benchmarks used for this study. We used the following criteria for \nchoosing the benchmarks for this study: 1. We chose SPEC CPU [30] benchmarks from the 1995 and 2000 suites. \n 2. We chose the integer benchmarks from SPEC CPU. It is cus\u00adtomary to use the integer benchmarks in \nbranch prediction stud\u00adies because they represent a class of applications sensitive to branch predictor \naccuracy. 3. We omitted the benchmarks from SPEC CPU 95 that are sub\u00adstantially duplicated in SPEC CPU \n2000, e.g. 126.gcc. 4. We omitted 124.m88ksimand 252.eonbecause they failed to compile and run correctly \nwith our compiler infrastructure. 5.2 The Camino Compiler  In this paper we introduce the Camino Compiler \nInfrastructure. Camino was developed to study code placement optimizations at the level of assembly language. \nCamino uses GCC as its front-end to compile C programs to x86 assembly language. Camino reads the assembly \nlanguage into an internal representation consisting of a list of procedures with control .ow graphs (CFG). \nCamino transforms the code, then uses GCC as a back-end for assembling and linking. Camino is capable \nof instrumenting code to gather ba\u00adsic block counts as well as edge and path pro.les; only edge pro\u00ad.les \nwere gathered for this research. Camino is the infrastructure in which we implement the code placement \noptimizations mentioned in this paper: greedy branch alignment and pattern history parti\u00adtioning. Camino \nwas developed using GCC 2.95.4 as the front-end and back-end; however, for this study we use the latest \nversion of GCC available as of this writing, GCC 3.4.2, for most of the bench\u00admarks. We revert to GCC \n2.95.4 for 176.gcc, 253.perlbmk, and 255.vortexbecause the combination of Camino and GCC 3.4.2 results \nin the failure of those programs to compile correctly. 5.2.1 Optimizations in Camino Camino implements \nthe greedy branch alignment as described by Calder and Grunwald [4]. The point of the algorithm is to \nlay out basic blocks in a procedure such that basic blocks incident on fre\u00adquently executed edges are \nplaced adjacent to one another. Basi\u00adcally, a priority queue of control .ow graph edges is generated \nin order of the frequency with which each edge is traversed accord\u00ading to the edge pro.les from a training \nrun. The algorithm removes edges from the queue in descending order of edge frequency. For each edge \nremoved, the basic blocks incident on that edge are laid out adjacent to one another in the program text \nif no previously cho\u00adsen edge succeeds the predecessor block or precedes the successor block. Once all \nbasic blocks have been laid out, conditional branch senses are modi.ed so that branches between adjacent \nbasic blocks fall through. Camino also implements GCC s heuristics for aligning code for maximum performance, \ne.g. aligning certain branch targets on 16\u00adbyte boundaries. 5.3 Compiling and Running the Benchmarks \nWe compile each benchmark using the edge pro.ling option of Camino. We run each benchmark on train inputs \nprovided by SPEC to gather pro.les. We use refinputs to measure the program behavior we report in Section \n6. The programs are linked statically. We subsequently compile several versions of each benchmark: A \nbaseline using only the GCC optimization .ags recom\u00admended by SPEC for x86 Linux: -O3 -fomit-frame-pointer. \n A version using the baseline optimizations and greedy branch alignment.  A version using the baseline \noptimizations and bimodal pattern history table partitioning.  A version using the baseline optimizations \nand 4-way pattern history table partitioning.  Two versions (explained in Section 6.3) in which PHT \npartition\u00ading is con.gured to behave similarly to other previous work.  We run the programs on a Dell \nworkstation with 2GB SDRAM featuring an Intel Pentium 4 at a clock frequency of 2.8 GHz. The computer \nruns the Fedora Core 2 Linux operating system. We kill as many background daemon processes as possible \nto provide a quiescent system for our runs. We use the Unix /usr/bin/time command to measure execution \ntime. We use the OPro.le pro.ling system to measure numbers of branch mispredictions [18]. The reported \ntimes and mispredictions are for the measured user process only, not for the whole system. We run each \nbenchmark 5 times and compute speedups based on the median running times. The OPro.le system uses the \nperformance counters of the Intel Pentium 4 to count branch mispredictions. We collect branch misprediction \nstatistics in separate runs because the method by which branch mispredictions are counted involves somewhat \nfrequent interrupts of the user process and might have the effect of perturbing timing results. 5.3.1 \nCode Expansion Each code placement technique resulted in a very slight expansion of the generated executable. \nThis expansion never exceeded 1.5% of the total size of the executable and was usually much less. 5.3.2 \nCompilation Times The PHT partitioning algorithm exhaustively computes a .tness function for each proposed \nnumber of no-op instructions to insert for each region. Each evaluation of this .tness function requires \nthat part of the job of assembling the region be done in order to .nd the addresses (modulo 64) of each \nbranch instruction in the region. We implemented a number of algorithmic optimizations to keep this extra \noverhead to a minimum. On average, compilation of programs using the PHT partitioning technique takes \n16% more time than compilation using only the baseline optimizations. 6. Results In this section, we \ngive the results of experiments showing the bene.t of PHT partitioning. We begin by showing the speedups \nachieved on the benchmark. We then give insight into how these speedups were achieved by showing the \neffect of the technique on reducing mispredictions. 6.1 Speedup Baseline Optimization (-O3 -fomit-frame-pointer) \n1.20 Greedy Branch Alignment Bimodal Pattern History Table Partitioning 1.15 4-way Pattern History Table \nPartitioning 1.10 1.05 1.00 0.95 0.90 0.85 0.80 Benchmark Speedup Figure 3. Speedup for SPEC CPU integer \nbenchmarks. Table 1. Description of SPEC CPU integer benchmarks with the number of conditional branches \nexecuted at least once. Benchmark Description # Branches 099.go Plays the game of go. Pattern matching. \n9,511 129.compress Compresses .les with Lempel-Ziv adaptive encoding. 205 130.li Lisp interpreter running \nthe Gabriel benchmarks. 967 132.ijpeg Compression/decompression for JPEG images. 1,703 164.gzip Compresses \n.les with Lempel-Ziv coding. 794 175.vpr Placement and routing program for FPGAs. 1,248 176.gcc C compiler \n(gcc 2.7.2.2) for the Motorola 88100. 27,961 181.mcf Single-depot scheduling for mass transportation. \n294 186.crafty Plays chess using alpha-beta search. 3,787 197.parser Parses English text to produce grammar \nanalysis. 3,869 253.perlbmk Stripped-down version of Perl v5.005 03. 9,007 254.gap Language for group-theoretic \ncomputation. 6,263 255.vortex Object-oriented database program. 8,644 256.bzip2 Compresses .les with \nblock-sorting compression. 756 300.twolf Standard-cell placement and routing. 3,052 40 Figure 3 shows \nthe speedups achieved for each benchmark and Mispredictions per 1000 Instructions on average by greedy \nbranch alignment, bimodal PHT partition\u00ad ing, and 4-way PHT partitioning. On average, 4-way PHT par\u00ad \ntitioning performs the best at an arithmetic mean 4.5% speedup. This technique yields a speedup of up \nto 16.0% in the case of 129.compressand 11.4% for 130.li. The speedup is greater than 1.0 on 13 of the \n15 benchmarks. Bimodal PHT partitioning, a simpler technique, yields a 3.4% speedup on average. It speeds \nup 176.gccby 11%. The speedup is greater than 1.0 on 12 of the 15 benchmarks. In a few instances, Greedy \nBranch Alignment Bimodal Pattern History Table Partitioning 28 26 4-way Pattern History Table Partitioning \n24 22 20 18 16 14 12 10  8 6 4 2  0 bimodal PHT partitioning outperforms 4-way PHT partitioning, but \nthe magnitude of the aggregate speedup of 4-way PHT partitioning leads us to prefer it over bimodal. \nGreedy branch alignment, a code reordering technique that lays out basic blocks so that most branches \nare not taken, achieves only a 0.2% speedup on average. The speedup is greater than 1.0 on only 7 of \nthe 15 benchmarks. The lack of improvement from greedy branch alignment is almost certainly due to the \nfact that the Intel Pentium 4 uses an instruction trace cache that has the effect of reordering code \ndynamically, negating much of the bene.tto instruction fetch of branch alignment. 6.2 Reduction in Branch \nMispredictions Figure 4 shows the number of branch mispredictions per 1000 instructions (MPKI) for each \nbenchmark. Clearly, there is a wide variation in branch prediction accuracy over all the benchmarks. \nFor instance, 099.go incurs about 37 mispredictions for every 1000 instructions. On the other hand, 255.vortex \nincurs less than one misprediction for every 1000 instructions. Note: the MPKI .gures are all computed \nwith respect to the instruction count of the original program, so e.g. the extra no-ops introduced by \nPHT partitioning do not arti.cially decrease MPKI. The highly variable MPKI can obscure the effect on \nmispre\u00addictions of the various techniques. Figure 5 shows the MPKI for each benchmarks normalized to \nthe MPKI for the baseline opti\u00admizations. This gives a more clear and balanced picture to explain why \nPHT partitioning provides superior performance. For instance, when 4-way PHT partitioning is used for \n130.li, this benchmark incurs 16.2% fewer mispredictions that it does with the baseline optimizations. \nOn average, normalized MPKI is reduced by 3.5%. Note that the magnitude of the improvement in MPKI does \nnot always correspond to the magnitude of the performance improve\u00adment. This is due to the fact that \nthere are many other components to the microarchitecture than just branch prediction, and the extent \nBenchmark Figure 4. Mispredictions per 1000 instructions (MPKI) for SPEC CPU integer benchmarks. to which \nthese factors affect performance varies from one bench\u00admark to another. For instance, a benchmark with \nvery few cache misses might be very sensitive to branch prediction accuracy, while another with a high \nnumber of misses might not be improved at all by branch prediction optimizations. Also, some branches \nare more important than others in terms of their ability to affect performance. The minimum penalty of \na mispredicted branch on the Intel Pen\u00adtium 4 is 20 cycles, but the maximum penalty is much higher due \nto wrong-path effects such as cache pollution and resource contention with right-path instructions. Thus, \nit might be more important to predict certain branches correctly than others; we plan to research this \nissue in future work. For instance, although the best speedup for 4-way PHT parti\u00adtioning is achieved \non 129.compress, this benchmark incurs slightly more mispredictions that it does with the baseline opti\u00admizations. \nWe hypothesize that this apparent paradox is due to the fact that 6 static branches account for over \n50% of all the branches executed during a run of the program. With so few branches domi\u00adnating the program, \na small variance in the misprediction penalty in one of these branches can have a large impact on performance. \nThat is, it is likely that one or more of these 6 branches has a high mis\u00adprediction penalty, and our \noptimization removes this high penalty by removing the mispredictions. In retrospect, 129.compress might \nbe considered a poor benchmark for branch prediction stud\u00adies because of its very small working set of \nbranches, but we in\u00adclude it in this study for completeness. Nevertheless, we are con.\u00addent that we have \ndemonstrated that our technique achieves a sig\u00adni.cant speedup: when we exclude 129.compress as well \nas 181.mcf which is the outlier with the lowest speedup, the arith\u00admetic mean speedup is 4.2% and the \naverage improvement in nor\u00admalized MPKI is 4. 1%. imperative that a PHT partitioning algorithm avoid \nplacing no-op instructions on frequently executed paths. 6.4 Potential for In.uencing Branch Predictor \nDesign PHT partitioning has the potential to simplify the design of future branch predictors. A branch \npredictors must supply its prediction within a single clock cycle [14]. However, as clock rates increase, \nNormalized Mispredictions per 1000 Instructions 1.20 1.15 1.10 1.05 1.00 0.95 0.90 0.85 0.80 0.75 Baseline \nOptimization (-O3 -fomit-frame-pointer) Greedy Branch Alignment Bimodal Pattern History Table Partitioning \n4-way Pattern History Table Partitioning the amount of time available to make a prediction becomes shorter. \nOne option available to microarchitects is to reduce the size of the PHT and thus decrease its access \nlatency, allowing the branch pre\u00addictor latency to .t in a single clock period. For instance, the ag\u00adgressively \nclocked AMD Athlon uses a small 2K-entry GAs pre\u00addictor, four times smaller than the 8K-entry GAs used \nin the previ\u00adous generation AMD K6 core [7]. However, the resulting decrease in branch predictor accuracy \ncan decrease some of the performance advantage gained by increasing the clock rate. PHT partitioning \ncan allow designers to use smaller tables but sustain the same mispre\u00addiction rates as previous generation \nprocessors. Figure 7 shows the effect on misprediction rate of PHT partitioning with PHTs varying from \n256 to 64K entries for the 176.gccbenchmark using a sim\u00adulated GAs predictor with an 8-bit history length. \nFor reference, the Intel Pentium 4 branch predictor has 4,096 entries [12]. A predictor Benchmark Figure \n5. Normalized mispredictions per 1000 instructions for with only 256 entries and bimodal partitioning \nachieves a mispre- SPEC CPU integer benchmarks. diction rate of 4.3%, while a predictor with 8,192 entries \nwith no partitioning achieves a higher misprediction rate of 4.8%. Thus, accuracy can be sustained or \neven improved even though techno\u00adlogical contraints may force the PHT to become smaller. 6.3 Impact of \nMinimizing Extra No-Op Instructions 5 There is an important trade-off between reducing the number of \nmispredicted branches and increasing the number of no-op instruc\u00adtions fetched. The previous work of \nChen and King achieved a sig\u00adni.cant reduction in misprediction rate using an optimization sim\u00adilar to \nPHT partitioning; however, this reduction comes at the cost of many extra no-op instructions being executed. \nChen and King de.ne the Maximum Motion Distance (MMD) as the maximum number of no-op instructions that \nmay be inserted after a branch instruction [6]. A lower MMD will tend to reduce the number of extra no-ops \nfetched while a higher MMD provides more .exibility for placing branches to reduce destructive interference. \nOur technique uses a frequency heuristic to decide whether to insert no-ops after a conditional branch: \nif the branch is taken less than 99.9% of the time, then we do not place no-ops after it because they \nhave the potential to decrease fetch bandwidth with extra fetched no-ops. Figure 6 illustrates the impact \nof extra no-op instructions. For this experiment, we modify the PHT paritioning algorithm to ignore the \nfrequency heuristic and accept an MMD parameter that will be used to determine how many no-ops may be \nplaced after a branch. The resulting algorithm is similar to the link-time algorithm of Chen and King. \n(However, the modi.ed algorithm still respects alignment heuristics as described before.) Figure 6 (a) \nshows the normalized MPKI for the unmodi.ed and modi.ed PHT partitioning algorithms. With an MMD of 4, \nthe modi.ed algorithm yields an average normalized MPKI of 0.991, a reduction of less than 1% in the \nnumber of mispredictions. With an MMD of 32, the modi.ed algorithm yields a normalized MPKI of 0.966, \na reduction of 3.5%. The original algorithm also yields a normalized MPKI of 0.966. However, the original \nalgorithm yields a much better speedup than the modi.ed versions because it uses the frequency heuristic. \nFigure 6 (b) shows that the average speedup of the original algo\u00adrithm is 4.3%, while the speedups for \nthe modi.ed algorithm with MMDs of 4 and 32 are 1.5% and -0.4% respectively. Indeed, the most aggressive \nform of the modi.ed algorithm with the highest reduction in mispredictions yields a negative speedup. \nThus, it is Percent Mispredicted 4 3 Without Bimodal Partitioning With Bimodal Partitioning2 1 0 Figure \n7. Simulated Misprediction Rates for Various Sized PHTs on 176.gcc. 7. Conclusion This paper has introduced \na technique for pattern history table partitioning for reducing branch mispredictions. By dividing the \nbranch predictor s pattern history table into partitions and grouping branches with similar behaviors \ninto the same partition, our tech\u00adnique reduces destructive interference in the PHT and decreases branch \nmispredictions, resulting in an improvement in perfor\u00admance. Our technique requires only edge pro.les \nwhich can be collected by many popular compilers. Thus, our technique can be readily incorporated into \ncompilers that exploit feedback directed optimization. We plan to extend our work in PHT partitioning \nin several ways. We believe that the full potential of our technique has not been demonstrated due to \nthe limited public knowledge of branch pre\u00addictor implementations in industrial CPUs. We are attempting \nto reverse-engineer branch predictors of several platforms so that we Number of Table Entries Normalized \nMispredictions per 1000 Instructions 1.20 1.15 1.10 1.05 1.00 0.95 0.75 Baseline Optimization (-O3 -fomit-frame-pointer) \n Baseline Optimization (-O3 -fomit-frame-pointer) 1.20 4-Way PHT Partitioning (unmodified) 4-Way PHT \nPartitioning (unmodified) 4-Way PHT Partitioning without Frequency Heuristic, MMD=4 4-Way PHT Partitioning \nwithout Frequency Heuristic, MMD=4 1.15 4-way PHT Partitioning without Frequency Heuristic, MMD=32 4-way \nPHT Partitioning without Frequency Heuristic, MMD=32 1.10 1.05  Speedup 1.00 0.95 0.90 0.90 0.85 0.85 \n0.80 0.80 Benchmark Benchmark (a) (b) Figure 6. can better target our optimizations for the characteristics \nof those microarchitectures. We are adapting our algorithm to microarchi\u00adtectures that use global history \nbut not branch address in indexing the PHT; this entails controlling branch senses (e.g. jge vs. je) \nrather than branch addresses to partition the PHT. By predicting more branches correctly, our technique \nreduces the amount of en\u00adergy wasted on wrong-path computations. We are planning to mea\u00adsure the bene.ts \nto energy reduction of branch prediction optimiza\u00adtions. We also plan to explore the use of static branch \nprediction heuristics [1, 3] to avoid the costly pro.ling step. Although microarchitectural research \nin improving branch pre\u00addiction accuracy has been heavily explored, relatively little work has been done \nto improve accuracy with software-only techniques. We believe that this approach has great potential \nto improve perfor\u00admance in current and future microprocessors. 8. Acknowledgements This research is supported \nby a grant from the Ministerio de Ed\u00aducaci\u00b4 on y Ciencia (Spanish Ministry of Education and Science), \nSB2003-0357. References [1] T. Ball and J. Larus. Branch prediction for free. In Proceedings of the SIGPLAN \n93 Conference on Programming Language Design and Implementation, pages 300 313, June 1993. [2] D. Burger \nand T. M. Austin. The SimpleScalar tool set version 2.0. Technical Report 1342, Computer Sciences Department, \nUniversity of Wisconsin, June 1997. [3] B. Calder, D. Grunwald, M. Jones, D. Lindsay, J. Martin, M. Mozer, \nand B. Zorn. Evidence-based static branch prediction using machine learning. ACM Transactions on Programming \nLanguages and Systems, 19(1):188 222, 1997. [4] B. Calder and D. Grunwald. Reducing branch costs via \nbranch alignment. In Proceedings of the 6th International Conference on Architectural Support for Programming \nLanguages and Operating Systems, October 1994. [5] P.-Y. Chang and U. Banerjee. Branch classi.cation: \na new mechanism for improving branch predictor performance. In Proceedings of the 27th International \nSymposium on Microarchitecture, November 1994. [6] C.-M. Chen and C.-Ta King. Walk-time address adjustment \nfor improving the accuracy of dynamic branch prediction. IEEE Transactions on Computers, 48(5):457 469, \nMay 1999. [7] K. Diefendorff. K7 challenges Intel. Microprocessor Report, 12(14), October 1998. [8] D. \nJ. Hat.eld and J. Gerald. Program restructuring for virtual memory. IBM Systems Journal, 10(3):168 192, \n1971. [9] A. N. Eden and T. N. Mudge. The YAGS branch prediction scheme. In Proceedings of the 31st Annual \nACM/IEEE International Symposium on Microarchitecture, pages 69 80, November 1998. [10] M. Evers, P.-Y. \nChang, and Y. N. Patt. Using hybrid branch predictors to improve branch prediction accuracy in the presence \nof context switches. In Proceedings of the 23rd International Symposium on Computer Architecture, May \n1996. [11] N. Gloy and M. D. Smith. Procedure placement using Temporal-Ordering information. ACM Transactions \non Programming Lan\u00adguages and Systems, 21(5):977 1027, September 1999. [12] B. Hayes. Differences in \noptimizing for the Pentium 4 pro\u00adcessor vs. the Pentium III processor. Intel Developer Services, http://www.intel.com/ \ncd/ ids/ developer/ asmo-na/eng/44010.htm. [13] Intel Corporation. Intel Pentium 4 processor optimization. \nTechnical Report Order Number: 248966, Intel Corporation, 2001. [14] D. A. Jim\u00b4enez, S. W. Keckler, and \nC. Lin. The impact of delay on the design of branch predictors. In Proceedings of the 33rd Annual International \nSymposium on Microarchitecture (MICRO-33), pages 67 76, December 2000. [15] R. E. Kessler. The Alpha \n21264 microprocessor. IEEE Micro, 19(2):24 36, March/April 1999. [16] S. P. Kim and G. S. Tyson. Analyzing \nthe working set characteristics of branch execution. In Proceedings of the 31rd Annual International \nSymposium on Microarchitecture, November 1998. [17] C.-C. Lee, C.C. Chen, and T.N. Mudge. The bi-mode \nbranch predictor. In Proceedings of the 30th Annual International Symposium on Microarchitecture, pages \n4 13, November 1997. [18] J. Levon. Opro.le -a system pro.ler for linux. Technical report, http://opro.le.sourceforge.net/ \n(Current on September 23, 2004). [19] S. McFarling. Program optimization for instruction caches. In Proceedings \nof the third international conference on Architectural support for programming languages and operating \nsystems, pages 183 191, 1988. [20] S. McFarling. Program optimization for instruction caches. In Proceedings \nof the Third International Conference on Architectural Support for Programming Languages and Operating \nSystems, pages 183 191. ACM, 1989. [21] S. McFarling. Combining branch predictors. Technical Report TN\u00ad36m, \nDigital Western Research Laboratory, June 1993. [22] P. Michaud, A. Seznec, and R. Uhlig. Trading con.ict \nand capacity aliasing in conditional branch predictors. In Proceedings of the 24th International Symposium \non Computer Architecture, pages 292 303, June 1997. [23] M. Milenkovic, A. Milenkovic, and J. Kulick. \nMicrobenchmarks for determining branch predictor organization. Software Practice and Experience, 34:465 \n487, April 2004. [24] H. Patil and J. Emer. Combining static and dynamic branch prediction to reduce \ndestructive aliasing. In Proceedings of the 6th International Symposium on High Performance Computer \nArchitecture, January 2000. [25] K. Pettis and R. C. Hansen. Pro.le guided code positioning. In Proceedings \nof the ACM SIGPLAN 90 Conference on Programming Language Design and Implementation, pages 16 27, June \n1990. [26] A. Ram\u00b4irez, J. Larriba-Pey, C. Navarro, J. Torrellas, and M. Valero. Software trace cache. \nIn Proceedings of the 13th International Conference on Supercomputing, Rhodes, Greece, June 1999. [27] \nA. Ramirez, J. L. Larriba-Pey, and M. Valero. The effect of code reordering on branch prediction. In \nProceedings of the 2000 International Conference on Parallel Architectures and Compilation Techniques. \nIEEE Computer Society Press, October 15 19, 2000. [28] E. Rotenberg, S. Bennett, and J. E. Smith. Trace \ncache: A low latency approach to high bandwidth instruction fetching. In Proceedings of the 29th International \nSymposium on Microarchitecture, December 1996. [29] E. Sprangle, R.S. Chappell, M. Alsup, and Y. N. Patt. \nThe Agree predictor: A mechanism for reducing negative branch history interference. In Proceedings of \nthe 24th International Symposium on Computer Architecture, pages 284 291, June 1997. [30] Standard Performance \nEvaluation Corporation. SPEC CPU 2000, http://www.spec.org/osg/cpu2000, April 2000. [31] T.-Y. Yeh and \nY. N. Patt. Two-level adaptive branch prediction. In Proceedings of the 24th ACM/IEEE International Symposium \non Microarchitecture, pages 51 61, November 1991. [32] C. Young, D. S. Johnson, D. R. Karger, and M. \nD. Smith. Near\u00adoptimal intraprocedural branch alignment. In Proceedings of the SIGPLAN 97 Conference \non Program Language Design and Implementation, June 1997. [33] C. Young and M. D. Smith. Static correlated \nbranch prediction. ACM Transactions on Programming Languages and Systems, May 1999.   \n\t\t\t", "proc_id": "1065010", "abstract": "Code placement techniques have traditionally improved instruction fetch bandwidth by increasing instruction locality and decreasing the number of taken branches. However, traditional code placement techniques have less benefit in the presence of a trace cache that alters the placement of instructions in the instruction cache. Moreover, as pipelines have become deeper to accommodate increasing clock rates, branch misprediction penalties have become a significant impediment to performance. We evaluate <i>pattern history table partitioning</i>, a feedback directed code placement technique that explicitly places conditional branches so that they are less likely to interfere destructively with one another in branch prediction tables. On SPEC CPU benchmarks running on an Intel Pentium 4, branch mispredictions are reduced by up to 22% and 3.5% on average. This reduction yields a speedup of up to 16.0% and 4.5% on average. By contrast, branch alignment, a previous code placement technique, yields only up to a 4.7% speedup and less than 1% on average.", "authors": [{"name": "Daniel A. Jim&#233;nez", "author_profile_id": "81100242990", "affiliation": "Rutgers University, Piscataway, New Jersey & Universidad Polit&#233;&#233;cnica de Catalu&#241;a, Barcelona, Spain", "person_id": "P728832", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065025", "year": "2005", "article_id": "1065025", "conference": "PLDI", "title": "Code placement for improving dynamic branch prediction accuracy", "url": "http://dl.acm.org/citation.cfm?id=1065025"}