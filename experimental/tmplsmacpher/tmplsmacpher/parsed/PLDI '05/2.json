{"article_publication_date": "06-12-2005", "fulltext": "\n VYRD: VerifYing Concurrent Programs by Runtime Re.nement-Violation Detection Tayfun Elmas Serdar Tasiran \nShaz Qadeer Koc\u00b8 University, Istanbul, Turkey Microsoft Research, Redmond, WA {telmas,stasiran}@ku.edu.tr \nqadeer@microsoft.com Abstract We present a runtime technique for checking that a concurrently\u00adaccessed \ndata structure implementation, such as a .le system or the storage management module of a database, conforms \nto an executable speci.cation that contains an atomic method per data structure operation. The speci.cation \ncan be provided separately or a non-concurrent, atomized interpretation of the implementa\u00adtion can serve \nas the speci.cation. The technique consists of two phases. In the .rst phase, the implementation is instrumented \nin or\u00adder to record information into a log during execution. In the second, a separate veri.cation thread \nuses the logged information to drive an instance of the speci.cation and to check whether the logged \nex\u00adecution conforms to it. We paid special attention to the general ap\u00adplicability and scalability of \nthe techniques and to minimizing their concurrency and performance impact. The result is a lightweight \nveri.cation method that provides a signi.cant improvement over testing for concurrent programs. We formalize \nconformance to a speci.cation using the notion of re.nement: Each trace of the implementation must be \nequivalent to some trace of the speci.cation. Among the novel features of our work are two variations \non the de.nition of re.nement appropri\u00adate for runtime checking: I/O and view re.nement. These de.\u00adnitions \nwere motivated by our experience with two industrial-scale concurrent data structure implementations: \nthe Boxwood project, a B-link tree data structure built on a novel storage infrastructure [10] and the \nScan .le system [9]. I/O and view re.nement checking were implemented as a veri.cation tool named VYRD \n(VerifYing concurrent programs by Runtime Re.nement-violation Detection). VYRD was applied to the veri.cation \nof Boxwood, Java class li\u00adbraries, and, previously, to the Scan .lesystem. It was able to detect previously \nunnoticed subtle concurrency bugs in Boxwood and the Scan .le system, and the known bugs in the Java \nclass libraries and manually constructed examples. Experimental results indicate that our techniques \nhave modest computational cost. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program \nVeri.cation formal methods, validation; F.3.1 [Logics and Meanings of Programs]: Specifying and Veri\u00adfying \nand Reasoning about Programs mechanical veri.cation, Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 05, June 12 15, 2005, Chicago, Illinois, USA. Copyright 2005 \nACM 1-59593-056-6/05/0006...$5.00. speci.cation techniques; D.2.5 [Software Engineering]: Testing and \nDebugging debugging aids, diagnostics, monitors, tracing General Terms Algorithms, Veri.cation Keywords \nRuntime Veri.cation, Re.nement, Concurrent Data Structures, 1. Introduction Many widely-used software \nsystems, such as .le systems, databases, internet services, and standard Java and C# class libraries, \nhave concurrently-accessed data structures at their core [9, 10]. Perfor\u00admance requirements force these \nsystems to use intricate synchro\u00adnization mechanisms, which makes them prone to concurrency errors. Functional \nerrors in these systems may have serious conse\u00adquences such as data loss, corruption, or system crash. \nTherefore, functional correctness is just as important as performance. Concur\u00adrency bugs are notoriously \ndif.cult to detect and reproduce through testing. This paper introduces a new scalable runtime analysis \ntech\u00adnique called re.nement checking for .nding concurrency-related errors in industrial-scale software \nimplementations. Checking re.nement consists of verifying that each execution trace of a concurrent implementation \nis equivalent to a trace of its speci.cation. This criterion provides more thorough validation than checking \nmethod-local assertions during program execution. We require that speci.cations execute each method atomically. \nThus, if an implementation re.nes a speci.cation, each implementation trace, in which portions of method \nexecutions are interleaved with others, is equivalent to a speci.cation trace with atomic method executions. \nAtomicity, a more widely-studied correctness crite\u00adrion [7, 6] requires that each implementation trace \nto be equiva\u00adlent to some atomic execution of the implementation. The distinc\u00adtion can be important for \nconcurrent systems and makes atomic\u00adity unnecessarily restrictive in some cases. Consider, for example, \nan implementation in which a method may terminate exceptionally because resource contention between concurrent \nthreads prevents it from completing its job. In an atomic execution of this implementa\u00adtion, there is \nonly one thread in the middle of a method execution at any given time and thus no resource contention \nbetween methods, therefore, method executions never terminate exceptionally. There\u00adfore, executions of \nthis implementation containing exceptional ter\u00adminations of this method are not equivalent to any atomic \nexecu\u00adtion of the system and will be declared erroneous according to the atomicity criterion. As this \nexample shows, since there is not a (possibly more permissive) speci.cation separate from the imple\u00admentation, \natomicity is sometimes too stringent and may declare certain acceptable system behavior as erroneous. \nIf a speci.cation that allows exceptional method terminations had been used for re\u00ad.nement checking, \nthis execution would not have caused a re.ne\u00adment violation. The ability to use speci.cations that more \nclosely model concurrent executions makes re.nement a more appropriate correctness criterion for concurrent \nsystems. In this work, we present a technique for checking re.nement at runtime. The speci.cation can \nbe provided separately or a non\u00adconcurrent, atomized interpretation of the implementation can serve as \nthe speci.cation. We have chosen to investigate runtime checking and sacri.ce completeness because of \nthe computational cost and practical dif.culty of exhaustively verifying re.nement for concurrent software, \nwhich requires reasoning about the entire state space of the implementation. The runtime re.nement check\u00ading \ntechniques presented in this paper are very scalable, require rel\u00adatively little manual effort, and provide \nsigni.cantly more thorough checking than testing. A key contribution of this paper is the de.nition and \nuse of two variations on the notion of re.nement appropriate for run\u00adtime checking. These de.nitions, \ncalled I/O and view re.nement, are motivated by our experience with verifying two industrial-scale concurrent \nprograms: Boxwood, a B-link tree data structure built on a novel storage infrastructure [10] and the \nScan .le system [9, 13]. Both of these systems aim to provide the illusion of atomic, linearly ordered \nmethod executions, but are highly concurrent implementa\u00adtions and use intricate synchronization mechanisms \nand caching to improve performance. Interestingly, neither program s methods can be shown to be atomic \nusing simpler correctness criteria and analysis methods such as reduction [7], commit atomicity [4], \nor purity [5]. We provide a more thorough discussion of the related work and a comparison of re.nement \nwith other correctness crite\u00adria in Section 8. Runtime checking of our simpler correctness criterion, \nI/O re\u00ad.nement, requires very little instrumentation and logging while still providing a more rigorous \ncheck than pure testing. Intuitively, I/O re.nement stipulates that for each execution of the implementation, \nthere is an atomic, sequential run (the witness interleaving ) of the speci.cation consisting of the \nsame method calls (and arguments) and return actions (and values). This correctness criterion can be \nviewed as a variant of linearizability [8] as elaborated on in Sec\u00adtion 8. To enable runtime checking \nof I/O re.nement, the programmer annotates the implementation code so that in every method execu\u00adtion, \na unique action is marked as the commit action . Our analy\u00adsis uses the order of occurrence of commit \nactions during execution to generate the witness interleaving. This novel use of commit ac\u00adtions distinguishes \nI/O re.nement from testing. Given the witness interleaving, we can determine exactly which return values \nare al\u00adlowed for a particular invocation of a method. In the absence of this information, pure testing \nis forced to be overly permissive. Our second correctness criterion, view re.nement, provides more thorough \nchecking than I/O re.nement by means of more visibility into program state. View re.nement augments I/O \nre\u00ad.nement by requiring a particular correspondence between the implementation and speci.cation states \nwhen a commit action is taken. Hypothetical view variables viewI and viewS are added to the implementation \nand speci.cation, respectively. Intuitively, the value of the view variable is a canonical representation \nof the ab\u00adstract data structure state when the commit action is taken. View re.nement requires that the \nvalue of viewI matches viewS for each method execution. The programmer speci.es how viewI and viewS are \nto be computed as a function of the implementation and speci.cation state, respectively. The visibility \nview provides into program state makes pos\u00adsible the early detection of discrepancies of the implementation \nstate from the speci.cation state. In contrast, in testing and I/O re\u00ad.nement, an implementation error \nis detected only if and when it causes a discrepancy in the return value of a method. In a particu\u00adlar \nrun, a triggered error may lead to an observed discrepancy long Insert(x, returnValue) 1 if (status == \nsuccess) 2 M=MU{x} 3 return returnValue; LookUp(x) 1 return x in M Figure 1. Speci.cations of multiset \noperations after it occurs, or not at all, whereas, in a different run, the same error would have caused \na more serious and easily observable out\u00adcome. view enables the detection of the error even in the former \ncase. This strength of view re.nement comes at the cost of more detailed instrumentation, logging, and \nprogrammer effort put into specifying how viewS and viewI should be computed. We implemented I/O and \nview re.nement checking as a ver\u00adi.cation tool called VYRD (VerifYing concurrent programs by Runtime \nRe.nement-violation Detection). VYRD can perform re\u00ad.nement checking of industrial-scale concurrent data \nstructure im\u00adplementations. We used the Boxwood project to drive our devel\u00adopment of the VYRD tool. VYRD \nis very effective in detecting concurrency errors. Using it, we were able to detect a subtle er\u00adror in \na cache module in Boxwood that had previously gone un\u00addetected1. VYRD was also able to catch known concurrency \nbugs in java.util.StringBuffer and java.util.Vector. An ear\u00adlier prototype of VYRD caught several subtle \nconcurrency errors in a Windows NT .lesystem [9]. In addition to extensive exper\u00adimental results about \nthe effectiveness and computational cost of using VYRD, we report the issues identi.ed and solutions \nim\u00adplemented while verifying industrial-scale systems. Chief among these are minimizing performance and \nconcurrency impact on the program being veri.ed and incremental computation and compari\u00adson of viewI \nin order to avoid re-traversing the entire program state at each veri.cation step. The organization of \nthe paper is as follows. We illustrate our runtime veri.cation technique on a concurrent implementation \nof a multiset described in Section 2. Section 3 formalizes state transition systems and our notion of \nre.nement. Sections 4 and 5 present I/O and view re.nement and our technique for checking them at runtime. \nThe re.nement checking tool VYRD is described in Section 6. Experimental results from the application \nof VYRD to Boxwood and other programs are described in Section 7. We discuss related work in Section \n8.  2. Example We will use a multiset data structure as our running example throughout the paper. The \n.rst, simple version of multiset supports two operations: Insert(x) to insert an element x into the multi\u00adset, \nand LookUp(x) to check if x is an element of the multiset. The speci.cations of these operations are \npresented in Fig. 1 where M is a state variable that represents the multiset contents. Any invoca\u00adtion \nof the Insert operation is allowed to terminate successfully or exceptionally, but exceptionally-terminating \nInsert operations are required to leave the multiset state unchanged. The multiset im\u00adplementation (Fig. \n2) uses an array A[0..n-1] to store the multiset elements. The .eld A[i].elt denotes the element stored \nin A[i], and initially A[i].elt = null for all i. The FindSlot subroutine looks for an available slot \nin the array for a single element x.Ifit .nds one, it reserves the slot by setting its content to x and \nreturns its index. Otherwise it returns -1. 1 For reasons that are not related to this error, the cache \nmodule had been re-written in a later version of Boxwood. The new cache module does not contain this \nerror. FindSlot(x) Delete(x) LookUp(x) 1 for (i=0; i<n; i++) 1 for (i=0; i<n; i++) 1 for (i=0; i<n; i++) \n2 synchronized(A[i]) { Insert(x) 2 synchronized(A[i]) { 2 synchronized(A[i]) { 3 if (A[i].elt == null) \n{ 1 i = FindSlot(x); 3 if (A[i].elt = x) { 3 if (A[i].elt = x) 4 A[i].elt = x; 2if(i ==-1) 5 return \ni; 3 return failure; 6 } 4 return success; 6 } 6 return false; 7} 7} 8 return -1; Figure 2. Implementation \nof the multiset operations The idea of I/O re.nement is illustrated in Fig. 3. The imple\u00admentation s \nactions are shown on the left half of the .gure from top to bottom in the order they happen in time. \nThe colors of the boxes representing actions indicate different threads performing them. The execution \nshows four method calls LookUp(3), Insert(3), Insert(4) and Delete(3) being concurrently executed by \nfour different threads. Since the four method executions overlap with each other, they could be serialized \nin any one of 4! ways. A simple but naive method for determining the correctness of the return value \nof LookUp(3) would require evaluating 4! serializations. Clearly, this method would not scale as the \nnumber of methods being exe\u00adcuted concurrently increases. Our solution to this problem has two parts. \nFirst, we require that the programmer specify a unique commit action for each method execution. The concurrent \nmethod executions are then deemed to have been serialized in the order of occurrence of the commit ac\u00adtions. \nSecond, we use the sequence of commit actions in an exe\u00adcution of the implementation to drive the execution \nof the speci\u00ad.cation. Figure 3 also shows the state transitions of the speci.ca\u00adtion in the right half. \nAt each commit action, the method that is committing and its return value (derived by looking ahead in \nthe implementation s execution) are used to execute the speci.cation. From its current state the speci.cation \nmust take a transition asso\u00adciated with the committing method and its return value. If this is not possible, \ne.g., if a call to LookUp(y) method has a return value of false when the speci.cation state indicates \nthat y is in the multi\u00adset, a re.nement-violation is detected. Observe that, in the example in Fig. 3, \nalthough the execution of LookUp(3) by the gray thread starts before the execution of Insert(3) and ends \nbefore the ex\u00adecution of Insert(3) ends, LookUp(3) returns true since its commit action comes after that \nof Insert(3). To see how the notion of I/O re.nement improves upon existing testing techniques for concurrent \nsoftware, consider a veri.cation thread consisting of calls to LookUp run after the termination of the \nthreads in Fig. 3. Since the executions of Insert(3) and Delete(3) overlap, the veri.cation thread would \nhave to consider both possible return values of LookUp(3) to be correct. Performing I/O re.nement using \nthe order of commit points to obtain a witness serialization, however, we are able to determine that \nDelete(3) occurs after Insert(3) and a LookUp(3) that occurs after the methods in Fig. 3 should return \nfalse. 2.1 Inserting a pair We now present a more complicated version of the multiset exam\u00adple that supports \na new operation InsertPair(x,y), whose spec\u00adi.cation is given below. Here, M denotes the multiset contents. \nInsertPair(x, y, returnValue) 1 if (returnValue == success) 2 M=M .{x,y} 3 return returnValue; When \nInsertPair(x,y) terminates successfully, it adds x and y to the multiset. If it terminates exceptionally, \nthe multiset state Figure 3. Re.nement of multiset. Some actions are not shown to keep the .gure simple. \n should remain unchanged. In particular, it will be considered a re.nement violation if only one of x \nor y is inserted into the multiset. We present this new multiset implementation for two reasons. First, \na variety of concurrent data structures, such as .le systems and storage systems, implement operations \nthat require multiple resources for successful completion. This feature adds a signi.cant amount of complexity \nto the implementation. The InsertPair op\u00aderation mimics such operations and is therefore particularly \nsuited for illustrating the power of our approach. Second, this example shows the generality of our approach \ncompared to existing tech\u00adniques based on reduction (e.g., [7, 6, 5]) which cannot prove the correctness \nof the implementation of InsertPair. The implementation of InsertPair is given in Figure 4. The implementation \nadds a boolean .eld A[i].valid to each array element to indicate whether the element A[i] should be considered \na member of the multiset. InsertPair .rst makes sure that there is room for x and y. If this is not the \ncase, it terminates exceptionally, and frees space it may have allocated. Otherwise, the code block in \nlines 9-14 atomically adds x and y to the multiset by setting their valid bits. Line 3 of the LookUp \nand Delete methods in Fig. 2 must now be modi.ed to read 3 if ((A[i].elt == x) &#38;&#38; (A[i].valid)) \nInsertPair(x,y) 1 i = FindSlot(x); 2 if (i==-1) 3 return failure; 4 j = FindSlot(y); 5 if (j==-1) { 6 \nA[i].elt = null; 7 return failure; 8} 9 synchronized (A[i]) { // begin commit block 10 synchronized (A[j]) \n{ 11 A[i].valid = true; 12 A[j].valid = true; 13 } // end commit block 14 } 15 return success; Figure \n4. The implementation of InsertPair. The commit point in an execution of the InsertPair method that returns \nsuccess is selected to be line 13. The intuition behind this is as follows. Suppose an application thread \ntis in the process of executing an invocation of InsertPair that will succeed. Let p and q be the locations \nwhere this invocation inserts x and y respectively. Another thread t. can access A[p] and A[q] either \n(1) before thread t executes line 10, or (2) after thread t executes line 13. Consider the .rst case. \nThread t holds the locks for A[p] and A[q] while it sets A[p].valid and A[q].valid to true. Therefore, \nif thread t. reads the valid bits before thread texecutes line 10, it reads them as false. Consequently, \nthread t. observes x and y as not having been inserted into the multiset. In the second case, since A[p].elt \n= x, A[q].elt = y, and A[p].valid and A[q].valid are true, thread t. observes both x and y as having \nbeen inserted into the multiset. Thus, line 13 is where the modi.ed view of the data structure becomes \nvisible to other threads. To demonstrate re.nement checking, a buggy implementation of FindSlot is given \nin Fig. 5. It is possible for two concurrently executing FindSlot operations to both conclude at line \n2 that the same index i is available, since neither have to hold the lock for A[i] in line 2. Fig. 6 \nshows an I/O re.nement violation that is caused by this bug. Thread T2 overwrites the value 5 that thread \nT1 inserted into A[0]. If the test program included a LookUp(5) after both InsertPair operations complete, \nthe speci.cation state would be {5,6,7,8} and require that the return value be true while, in the implementation, \nthe return value would be false. FindSlot(x) 1 for (i=0; i<n; i++) { 2 if (A[i].elt == null) { // A[i] \nshould be locked 3 synchronized(A[i]) { 4 A[i].elt = x; 5 return i; 6} 7} 8 return -1; Figure 5. Buggy \nimplementation of FindSlot.  3. De.nitions In this section, we de.ne the notion of I/O re.nement and \nview re.nement. This section is not a prerequisite for an intuitive under\u00adstanding of the remainder of \nthe paper and it can be skipped on a .rst reading.  Figure 6. Re.nement violations in the buggy version \nof multiset. We focus on concurrently accessible implementations of data structures written in object-oriented \nlanguages. The data structure makes available a set of operations each of which is implemented as a public \nmethod. M denotes the set of public methods. Methods return a single value, and exceptional terminations \nfor methods are modeled by special return values. A method \u00b5 .M is called an observer if \u00b5 s speci.cation \ndoes not allow it to modify data structure state. All other methods are called mutators. The domain Tid \nrepresents the set of thread identi.ers and is the union of two disjoint sets, Tidapp and Tidds. Tidapp \ncontains identi.ers of application threads that call the public methods and Tidds contains identi.ers \nof worker threads that perform tasks internal to the data structure. 3.1 State transition systems We \nuse state transition systems as the formal semantics of both speci.cation and implementation programs. \nA state transition sys\u00adtem is a tuple (V ,S,s0,d): V is the set of program variables.  S is the set \nof states. Each state is an assignment of a value of the correct type to each variable in V . s0 . Sis \nthe initial state.  d is the transition function from S \u00d7 Actions to S, where Actions is the set of \nactions that the system can perform. Each distinct method call, return, and atomic update of a set of \nstate variables is modeled by a unique action. If d(s,a)= s e, the transition system may perform the \naction ain state sto change  a the state to s e. We denote such a transition by s-. s . . a1 A run of \nthe state transition system is a .nite sequence r =s0 -. a2anai s1 -. \u00b7\u00b7\u00b7 -. sn for some n = 0such that \nsi -. si+1 for all 0= i<n. A call action is a tuple acall =(t,\u00b5,.), where tis the identi.er of the thread \nperforming the method call, \u00b5 .M is the public method invoked, and . is the list of actual method arguments. \nA return action is a tuple aret =(t, \u00b5, .), where t is the identi.er of the thread performing the method \nreturn, \u00b5 .Mis the returning public method, and . is the value returned by the method. A state in a run \nis quiescent if it does not lie between the call and return action of any method. 3.2 Traces and well-formedness \nTraces are de.ned by designating a subset ..Actions of actions as visible. Call and return actions of \npublic methods are required to be visible. Given a run r of the state transition system, the .-trace \ncorresponding to r is the sequence of visible actions, i.e., actions belonging to .that take place during \nthat run. When it is clear from the context, we simply refer to a trace and omit mention of .. The sequence \nof actions associated with a thread t and lying between the call and return action for a public method \nis called an execution . of that method. Formally, . =(\u00b5, t, s)where \u00b5 is the method executed by thread \nt .Tid and s is the sequence of the actions that belong to the particular invocation. . is said to have \na signature Sign(.)=(t, \u00b5, ., .), where t .Tid is the thread that executed method \u00b5 .M, . is the set \nof actual parameters and . is the return value for this execution. For two method executions . and .e \nin a trace t , we say . .t .e if the return action of . comes before the call action of .e in t . A sequence \nof actions by an application thread t is well-formed if (i) each call action acallp is eventually followed \nby a matching re\u00adturn action aret pq p , and (ii) if acall takes place after acall but before aret pq \np , then aret takes place before aret as well. A trace t is well\u00adformed if for every application thread \nt, the subsequence of t cor\u00adresponding to actions of thread t, denoted by t |t, is well-formed. A run \nis well-formed if its corresponding trace is well-formed. In this paper, we restrict our analysis to \nwell-formed runs of state tran\u00adsition systems. A well-formed run is method-atomic if after every call \naction acall(t, \u00b5, .)to a public method \u00b5 by an application thread t, no thread other than thread t performs \nan action until the corresponding return action aret(t, \u00b5, .)has occurred. A state tran\u00adsition system \nis method-atomic if all of its runs are method-atomic. Each portion of such a run between (and including) \ncorrespond\u00ading call and return actions is called an atomic fragment of the run. A method-atomic state \ntransition system is deterministic if when\u00adever two atomic fragments of any two runs have the same starting \nstate and the same method execution signature, they have the same .nal state as well. Thus, every trace \nof a method-atomic and deter\u00administic state transition system is produced by a unique run. Note that \nthis de.nition of determinism allows different possible return values for a method invocation at a given \nstate. It only requires that given the return value, the .nal state be uniquely de.ned. We require our \nspeci.cations to be method-atomic and deterministic state transition systems. 3.3 Re.nement between \nstate transition systems A re.nement relation .. between state transition systems I and S designates \na subset . of actions common to I and S to be visible. Let the implementation and speci.cation of a concurrent \ndata structure be given by the state transition systems I and S, respectively. I .-re.nes S, denoted \nI .. S, if for every . trace t of I , there is a .trace t e of S such that (i) t |t = t e|t e for all \nt . Tidapp, and (ii) for all method executions . and ., . .t .e implies . .t' .e. In our simplest notion \nof re.nement, I/O\u00adre.nement, call and return actions are the only visible actions.  4. Checking I/O \nre.nement at runtime This section gives more details of our method for checking I/O re\u00ad.nement at runtime. \nSuppose I has a run r that has been annotated with information about the commit action for each method \nexe\u00adcuted by each thread. Recall that only call and return actions are visible for I/O re.nement. Therefore, \nthe corresponding trace t has the property that for each thread t, t |t is a sequence of pairs of matching \ncall and return actions. We construct a method-atomic in\u00adterleaving by arranging these call-return pairs \nin the order of their commit actions. Finally, we check whether the resulting interleav\u00ading is a trace \nof S. Recall that, since S is required to be atomic and deterministic, this ordered sequence of call-return \npairs corre\u00adsponds to a unique run, i.e., uniquely determines the sequence of states that the speci.cation \ngoes through. Therefore, it is straight\u00adforward to check whether a given method-atomic interleaving cor\u00adresponds \nto a trace of S. We simply execute the speci.cation one method call at a time in the order given by the \ninterleaving. For each method execution ., the signature Sign(.)(which includes the return value) derived \nfrom t is used to drive the speci.cation. If at any point, it is not possible to execute the speci.cation \nwhile conforming to the return action as speci.ed in Sign(.), the re.ne\u00adment check is said to fail. We \nnow elaborate on the details of our checking method. 4.1 Selecting commit actions The order of the commit \nactions in time must coincide with the ap\u00adplication s view of how the state of the data structure transforms \nover time. Intuitively, the commit action in an execution of a mu\u00adtator method by a thread is the .rst \naction which makes visible to other threads the modi.ed abstract data structure state. An exam\u00adple was \nprovided in Section 2.1. Selection of commit actions for a method from the Boxwood project is given in \nSection 7.2.5. In practice, commit actions are speci.ed by designating certain lines in the implementation \ncode to be commit points. The program\u00admer must make sure that for each method, exactly one action is \nmarked as the commit action for every execution path through the method code. This is accomplished by \nassociating with each com\u00admit point annotation a condition under which it is the commit point. This condition \ncan be typically expressed in terms of thread-local variables. Commit actions are really hints to re.nement \nchecking tools and there is no formal procedure for selecting them. If a certain design pattern is followed, \na method for selecting them can be devised. If the user has de.ned an abstraction function (a view , \nas explained in Section 5), a good guess for the commit action is the .rst lock release following the \nlast modi.cation to a variable in the support of view . The examples of commit point selection referred \nto in Sections 2.1 and 7.2.5 above .t this description. The runtime re.nement check described could fail \neither be\u00adcause the implementation truly does not re.ne the implementation or because the witness interleaving \nobtained using the commit ac\u00adtions is wrong. Comparing the witness interleaving with the imple\u00admentation \ntrace reveals which one of these is the case. If the wit\u00adness interleaving is wrong, the programmer must \nre-examine and modify the commit point selection or must re.ne the conditions associated with the commit \npoint. We have found this iterative pro\u00adcess very useful for debugging code that is in development and \nfor improving our understanding of method implementations.  4.2 Off-line re.nement checking using a \nlog It is desirable for a runtime veri.cation tool not to modify the con\u00adcurrency characteristics of \nthe implementation signi.cantly. This could happen if a large amount of instrumentation overhead is in\u00adtroduced \nor application threads block waiting for access to the in\u00adstrumentation module. To interfere minimally \nwith the implemen\u00adtation, we run re.nement checking on a separate thread which is informed about the \nimplementation s actions through a log. For\u00admally, a log Lis a .nite sequence of actions L= a0, ..., \nan.In practice, the log is a .le whose tail is kept in memory for faster acmt acall acmt acmt acmt acmt \naret Actions in the log ... 0 ... ... 1 ... 2 ... n-1 ... n ... Speci.cation state s0 s1 s2 sn-1 sn Figure \n7. Checking re.nement for observer methods. aret must be consistent with si for some 0 =i =n. access. \nThe implementation threads write entries to the log as they run; the veri.cation thread reads these entries \nand performs re.ne\u00adment checking. For I/O re.nement checking, the entries of the log are required to \ncontain all call, return, and commit actions. Actions must appear in the log in the order they occur \nin the execution. One way to achieve this is to require that each logged action be performed atomically \nwith the corresponding log update.  4.3 Verifying observer methods While the commit action of a mutator \nmethod is typically associ\u00adated with an atomic write to a shared variable, the commit action of an observer \nmethod is typically associated with a shared variable read. A subset of shared variable reads performed \nby an observer method determine its return value, as is the case, for example, with a LookUp call that \nreturns true. However, it is not known before the read is performed whether its outcome will determine \nthe re\u00adturn value. Therefore, precisely marking the commit action of an observer method requires that \nalmost all reads performed by it be logged. This would result in large overhead and signi.cant impact \non the concurrency among the operations. Instead, we obviate the need for annotating and logging the \nexecution of observer meth\u00adods as follows. We only record the call and return actions of ob\u00adserver methods \ninto the log. For one such execution . of an observer method \u00b5, let acall and aret be the call and return \naction entries ,acmt ,acmt in the log, and acmt , ..., acmt the commit actions of 123 n mutator methods \nthat lie between acall and aret in the log. Let acmt 0 be the last commit action before acall. We denote \nby si the speci.cation state obtained after atomically executing the method associated with acmt i for \nall 0 =i =n. If the return value . in . of the method \u00b5 is consistent with what the speci.cation allows \nas a valid return value for \u00b5 at any of the states s0,s1, ..., sn,we declare . correct. This amounts \nto allowing the real commit action of . to be anywhere between (and including) acall and aret. Oth\u00aderwise, \na re.nement violation is signaled since a return value of . is not consistent with any choice of commit \naction for .. Observe that this check does not result in a combinatorial blow-up in the number of linearizations \nconsidered, since the witness interleaving provides a unique ordering of commit points of mutator methods. \nBecause observer methods do not modify the data structure state, the choice of the commit action for \nan observer method does not in.uence the checks performed for other methods. 4.4 Using the atomized \nimplementation as the speci.cation If a separate speci.cation does not exist, our technique enables the \nuse of an atomized version of the same implementation code as the speci.cation. In an atomized version, \nthe program is forced to have only method-atomic executions. This is accomplished by de.ning a global \nlock and requiring that methods must hold this lock during execution and must release it upon termination. \nThe atomized version s methods are also modi.ed so that, in addition to the original set of arguments, \nthey take the return value as an argument and compute the new speci.cation state, in a fashion similar \nto Figure 1. The idea of using an atomized version as a speci.cation for re.nement veri.cation was introduced \nearlier by us [14] and Flanagan [4] independently. An alternative way to view this veri.cation approach \nis to decompose the re.nement veri.cation task into two: Verifying that the implementation re.nes the \natomized version, and that the atomized version, a sequential program, re.nes a higher-level speci.cation. \nThe latter problem can then be addressed using methods developed for that purpose.  5. Improving I/O \nre.nement using view variables As an extreme example of the limited visibility provided by testing and \nI/O re.nement, consider test programs for the multiset example that only call InsertPair. Since no observer \nmethods are called, the runtime checks for I/O-re.nement would pass trivially for all such tests. For \nuseful testing or I/O re.nement checking, frequent calls to observer methods must be performed. But this \nmight reduce the degree to which mutator methods are executed concurrently and make it less likely that \nan existing error will get triggered. Further\u00admore, calls to the observer methods might not get scheduled \nat the most interesting points. This section presents view re.nement, a correctness condition for concurrent \nprograms which augments I/O-re.nement with more visibility into the data structure state. This extra \nvisibility enables us to get more thorough checking from each test run. The key idea in view re.nement \nis to introduce hypotheti\u00adcal view variables viewI and viewS into the implementation and speci.cation, \nrespectively. Intuitively, the values of viewI and viewS extract the contents of data structure from \nthe implemen\u00adtation and speci.cation states. These variables are computed as a function of the data structure \nstate and abstract away information that is not relevant to what applications can observe. For example, \nfor a binary search tree, viewI might be de.ned as the list of the (key, value) pairs, thus abstracting \naway the structure of the tree. If a hashtable is given as the speci.cation for the binary tree, viewS \nmight again be the set of (key, value) pairs while the hash func\u00adtion and the collision resolution mechanism \nare abstracted away. The programmer is asked to specify how the view variables are to be computed. We \nhave found the de.nition of view tobean it\u00aderative process which leads to a better understanding of why \nthe program guarantees atomicity of methods. 5.1 Computing view using a log The implementation and speci.cation \nare not actually modi.ed to perform view re.nement checking. Instead, the veri.cation thread separately \n(possibly off-line) constructs the values of viewI and viewS using the log and the speci.cation run driven \nby the wit\u00adness interleaving, respectively. The variables viewI and viewS are initialized to the same \nvalue. Each mutator method updates viewI once, atomically with its commit action. viewS is updated once \natomically anytime between the call and return of each mutator method. Observer methods do not modify \nthe view variables. Dur\u00ading runtime veri.cation, we now also check that the same sequence of updates \nare performed to viewI and viewS . Formally, we de\u00adclare as visible all commit actions and annotate each \nof them with the corresponding updated value of viewI or viewS . We now illustrate this method on the \nmultiset example. viewS is selected to be the entire multiset contents M. viewI is updated atomically \nwith the commit actions of mutator methods as follows: 1 viewI = \u00d8 2 for (i=0; i<n; i++) 3 if A[i].valid \n4 viewI = viewI .{A[i].elt}; With the addition of the auxiliary variable viewI to the imple\u00admentation, \nwe get useful checking even with a test program that has no calls to LookUp. This stronger correctness \ncriterion is more likely to expose errors and provide early warnings as the follow\u00ading examples demonstrate. \nFirst consider the bug scenario in Fig. 6. The .rst call to LookUp(5) by thread T1 returns the expected \nvalue of true. If a second call to LookUp(5) had not been performed at the end of the test program by \nany thread, the error would have gone undetected. This is a likely scenario since T2 is likely to only \ncheck the results of its own operations. Computing viewI at the commit point of InsertPair(6,8) and comparing \nit with viewS would have revealed the bug immediately. Suppose that a thread in the test program calls \nInsertPair(x,x) to insert two copies of x into the multiset, but, because of an error in the implementation, \nonly one x gets inserted into the array A. To expose the error through testing or I/O re.nement checking, \nwe need an execution that fol\u00adlows this call with Delete(x), followed by Lookup(x) with no other calls \nbetween them to mutators with x as the argument. The probability of this during a test program not written \nto target this er\u00adror is low. Even if such a test scenario were exercised, if the insert, delete and \nlookup operations were far apart in the execution, by the time Lookup(x) returns an unexpected result, \nit would be dif.cult to locate the original error. View re.nement checking detects this error immediately \nat the commit action of InsertPair(x,x). 5.2 Commit blocks The computation of viewI given above is a \nbit oversimpli.ed. Consider the scenario where a thread t1 that is executing an InsertPair(x1 ,y1) operation \ncompletes line 11 (A[i].valid = true). Then, a context switch occurs to thread t2 which is exe\u00adcuting \nInsertPair(x2 ,y2). Suppose that t2 executes its commit action (line 13). At this point, the viewI computation \nwill include x1 into viewI but not y1, since its valid bit is not yet set. Since lines 11 and 12 of Fig. \n4 are protected by locks, it is not possible for another thread to see this dirty state where x1 is in \nthe multi\u00adset but y1 is not. To handle such cases, intuitively, the computation of viewI needs to roll \nback some shared variable modi.cations that uncommitted operations have performed. To ease the task of \nde.ning and computing viewI , we introduce the concept of com\u00admit blocks. Lines 9 and 13 in Fig. 4 indicate \nthe beginning and the end of the commit block for InsertPair. The intent of the commit block is that \n(i) it can be easily veri.ed to be atomic (by inspection, statically, using a reduction argument, or \nbe checked at runtime using a tool based on reduction, such as Atomizer [6]), and (ii) the task of computing \nviewI is simpli.ed by assuming that only the committing thread is in the process of executing a commit \nblock. In an actual execution t , (ii) does not hold, but by using (i), the execution can easily be converted \nto an equivalent execution t where (ii) holds. Then, view can be written under the assumption that, at \nany given commit point, only one thread is in the process of executing its commit block. Conceptually, \nour re.nement checking algorithm .rst computes t from the actual execution t described in the log, and \nthen performs re.nement checking on t . In fact, rele\u00advant portions of t are constructed incrementally, \non-the-.y during the re.nement check. The choice of the commit block must strike a compromise between \nsimplifying the computation for viewI and verifying that the commit block is atomic. Marking too large \na block, for instance, the entire InsertPair method as the commit block would make it impossible or too \nexpensive to check that it is atomic. Marking too small a block, for instance, only the commit action, \nas the commit block does not simplify the computation of viewI . The granularity of checking done using \nview variables is inter\u00admediate between two extremes. At one extreme, one can require a state correspondence \nonly at quiescent states of the implementa\u00adtion as in [13, 4]. But most industrial-scale concurrent data \nstruc\u00adtures are built to be used by large numbers of threads continuously and during any realistic execution, \nquiescent points are very rare. Checking only at these points might cause errors to be overwritten or \nto be discovered too late. At the other extreme, asking the pro\u00adgrammer write a re.nement map that relates \nimplementation and speci.cation states after each action is impractical. Our choice of view variables \nthat are updated only with commit actions strikes a good compromise: A check is performed for each method \nex\u00adecution, and the re.nement map is easier to write, particularly if commit blocks are used as described \nearlier. Computing the value of viewI in the multiset implementation requires an atomic snapshot of the \ncontents of the entire array A.We use the log to solve the problem of taking this atomic snapshot and \nconstructing the updated value of viewI . Let supp(view) denote the set of program variables that in.uence \nthe computation of viewI . The set supp(viewI ) can be computed by a simple static analysis of the code \nfor updating viewI . In addition to all log entries required for I/O-re.nement checking, we insert an \nentry into the log recording each update to a variable in supp(viewI ).  6. The Runtime Re.nement Checking \nTool We implemented our re.nement checking techniques as a reusable library called VYRD. VYRD implements \nthe functionality of a generic re.nement checker and has components for instrumenting implementation \ncode for logging and for checking I/O and view re.nement using the log. 6.1 Logging issues The implementation \ncode is instrumented using the helper classes in VYRD to save actions performed and related data to the \nlog at runtime. The logging mechanism of VYRD uses the binary object serialization mechanism of the .Net \nplatform in order to restore record objects as they are saved at runtime. This gives a transparent view \nof logging and restoring the implementation trace and relieves the programmers from considerable programming \neffort. In the industrial systems we applied VYRD to, storage and access for shared variables were managed \nand their sequential consistency and atomicity of updates was ensured by a separate module. This allowed \nus to interpret log entries using a sequential memory model. We set aside the veri.cation of the lower-level \nstorage modules as a future task. 6.2 Logging at different levels of granularity It is possible to log \nexecutions at different levels of granularity. An example of .ne-grained logging is to record all writes \nto shared variables, e.g., assignments to an integer-typed variable, inserts to a hashtable, or locking/unlocking \nusing a semaphore. No knowledge of the data structure is necessary to re-construct data structure state \nusing the log in this case. Coarse-grained logging is data-structure speci.c. If the programmer can ensure \nstatically that a group of actions are performed atomically by a method, the group of actions can be \nlogged as a single entry, which reduces logging contention and overhead. Such a group of actions typically \nis a lower-level task than a public method, e.g., re-distributing data between two tree nodes, but higher-level \nthan a single shared variable update. Since such tasks are data structure-speci.c, replay methods that \nre-construct data structure state from the log must be provided by the programmer for each such kind \nof log entries. 6.3 Programmer-de.ned view construction De.ning a view for a concurrently accessed data \nstructure requires knowledge of internals of the implementation. Roughly speaking, view variables must \nextract the abstract data structure contents by abstracting away the rest of the program state. Since \nthe way abstraction is done is speci.c to the data structure, the programmer is asked to describe how \nview is to be computed from program state. Currently, view is de.ned by writing a method that computes \nit using the log. In many examples, including Boxwood, view can be written using shared state only. Once \nwe understood a data structure implementation, even for the industrial examples we studied, we found \nthat it only took a few days of effort to de.ne view. 6.4 Incremental computation and comparison of \nviews The size of the program state for an industrial-scale concurrent data structure could be huge, \ne.g., the entire contents of a hard disk for a .le system. To avoid re-traversing the entire program \nstate at each commit action we compute viewI incrementally. Typically, since viewI represents data structure \ncontents, its value has a certain structure, and one can speak of portions of that structure, e.g., a \nsubset of linked-list entries or .les that are modi.ed between two commit points. As we replay the actions \nof the implementation and the methods in the speci.cation, we perform a dependency analysis of the portions \nof view variables on the variable updates that took place between two commit actions. We then only re-compute \nand compare only the relevant parts of the view variables.  7. Experience with Vyrd We ran VYRD on \nseveral programs to test its ef.cacy in catching errors. In the experiments reported in this section, \nlogging was done at the .ner, shared variable write level. 7.1 Test harnesses To produce traces for re.nement \nchecking, we developed test pro\u00adgrams for each data structure implementation intuitively aiming to trigger \nconcurrency errors. All test cases on a particular data struc\u00adture start from the same initial state. \nEach test program .rst gener\u00adates a random pool of keys to be shared by all threads as arguments for \nmethod calls. Then the program creates a number of threads (re\u00adported in the tables of experimental results) \neach of which, using arguments randomly chosen from the pool, issues a given number (also reported in \ntables) of random method calls to the same data structure instance concurrently. The pool is reduced \ngradually over time to focus more concurrent method calls on a smaller region of the data structure. \nIn implementations with compression mecha\u00adnisms, the compression thread is either triggered automatically \nby mutator methods, or, otherwise, it is run continuously.  7.2 Boxwood Boxwood [10] provides scalable \nstorage infrastructure for appli\u00adcations. Boxwood is written in C# and consists of roughly 30K lines \nof code (LOC). A block diagram for Boxwood is given in Fig. 10. A B-link tree module (BLinkTree) is built \non Boxwood s data store abstraction: Each shared variable is a byte-array identi\u00ad.ed by a unique handle, \nand is stored and managed by the Chunk Manager module. Shared variables have version numbers that are \nincremented after each write. The Cache module improves the per\u00adformance of BLinkTree and is intermediate \nbetween BLinkTree and Chunk Manager. We concentrated on verifying the BLinkTree ( 3KLOC) and Cache ( \n1KLOC) modules, assuming that Chunk Manager was implemented correctly. We followed a modular approach \nto verify\u00ading BLinkTree and Cache. We treated Cache as a separate data structure that works in collaboration \nwith Chunk Manager and has BLinkTree as its client. The veri.cation of BLinkTree was performed assuming \nthat the Cache+Chunk Manager combination works correctly. 7.2.1 Veri.cation of Cache + Chunk Manager \nCache has public methods for reading and writing shared variables, .ushing all dirty variables in the \ncache to Chunk Manager, and a revoke method for writing a single variable from Cache to Chunk WRITE(handle, \nbuffer) 1 RECLAIMLOCK.BEGINREAD() 2 LOCK(clean) 3 ce . GET-CLEAN-ENTRY(handle) 4 de . GET-DIRTY-ENTRY(handle) \n5 UNLOCK(clean) 6 if ce = null and de = null 7 then 8 RECLAIMLOCK.ENDREAD() 9 te . MAKE-NEW-ENTRY(handle) \n 10 RECLAIMLOCK.BEGINREAD() 11 COPY-TO-CACHE(buffer, te) 12 LOCK(clean) 13 ADD-TO-DIRTY-LIST(handle, \nte) . Commit point 1 14 UNLOCK(clean) 15 elseif ce = null 16 then 17 LOCK(clean) 18 REMOVE-FROM-CLEAN-LIST(handle) \n19 COPY-TO-CACHE(buffer, ce) 20 ADD-TO-DIRTY-LIST(handle, ce) . Commit point 2 21 UNLOCK(clean) 22 else \n23 COPY-TO-CACHE(buffer, de) . Commit point 3 24 RECLAIMLOCK.ENDREAD() COPY-TO-CACHE(buffer, entry) \n1 for i . 0 to LENGTH-OF(buffer) - 1 2 do 3 entry.data[i] . buffer[i] FLUSH() 1 LOCK(clean) 2 while \nmore entry in dirty list 3 do 4 te . NEXT-DIRTY-ENTRY() 5 if te is old enough to .ush 6 then 7 BOXWOOD-ALLOCATOR-WRITE(te.handle, \nte.data) 8 ADD-TO-VICTIMS-LIST(te) 9 while more entry in victims list 10 do 11 te . NEXT-VICTIM-ENTRY() \n12 REMOVE-FROM-DIRTY-LIST(te) 13 ADD-TO-CLEAN-LIST(handle, te) 14 UNLOCK(clean) . Commit point Figure \n8. Pseudocode fragment for the Boxwood Cache. Manager. Each variable accessed via Cache is addressed \nby the same unique handle that is also recognized by Chunk Manager. We veri.ed the correctness of the \nabstract data store provided by the Cache+Chunk Manager combination assuming that Chunk Manager is implemented \ncorrectly. We speci.ed viewI for Cache+Chunk Manager as the set of (handle,byte-array) pairs stored in \nthem. To extract viewI , for each handle, if there exists a cache entry associated with handle, byte\u00adarray \nis taken from the cache entry, otherwise, it is taken from Chunk Manager. We also veri.ed the following \ninvariants about Cache at runtime: (i) If a clean cache entry exists for handle, Cache and Chunk Manager \nmust contain the same corresponding byte\u00adarray. (ii) a cache entry must be in either the clean or dirty \nentries list. For Cache we logged add and delete operations to hashtables and byte-array-copy operations \nfrom/to the cache entry buffers. Logging at this level of granularity was necessary for detecting the \nconcurrency error described below.  7.2.2 Bug caught in Cache The error is due to a method call (the \ncall to COPY-TO-CACHE in line 23 of Fig. 7.2) not being protected by the proper lock (LOCK(clean)). As \na result, while a byte array in the cache is being overwritten in-place (the COPY-TO-CACHE method), it \nis possible for a cache .ush to be triggered. The cache .ush may be interleaved so that a corrupted byte-array \nthat contains partly old and partly new data is written to Chunk Manager. At this point, the cache entry \nis marked clean since it is believed to have been written to Chunk Manager. Invariant (i) for Cache is \nviolated at this point, and a view re.nement violation occurs. It is a lot harder for I/O re.nement and \ntesting to detect this error: The corrupted state must lead to an error in the return value of a method \nas described in the following scenario: Before another write to this cache entry occurs, it is evicted \nfrom Cache but is not written back to Chunk Manager since it is marked clean . Then the block is read \nback into the cache, which retrieves the corrupted buffer from Chunk Manager. In our experiments, both \nI/O re.nement and view re.nement detected this error but I/O re.nement required a much longer test run. \n 7.2.3 Veri.cation of BLinkTree The BLinkTree module in Boxwood is a highly concurrent imple\u00admentation \nof a B-link tree data structure [12]. A B-link tree stores a set of (key, value) pairs and provides the \nINSERT,DELETE, and LOOKUP methods. An internal compression thread works concur\u00adrently with data structure \noperations and re-arranges the tree struc\u00adture without modifying the set of (key, value) pairs. In addition \nto the data structure operations, we also checked that atomic up\u00addates that the compression thread performs \nto the program state do not modify view. We found no errors in the original code for BLinkTree. Bugs \nwere inserted manually in order to collect ex\u00adperimental data for performance measurements. 7.2.4 De.ning \nviewI for BLinkTree viewI was de.ned to be the sorted list of all the (key, data) pairs in the tree, \nalong with their version numbers. Each (key, data) pair is stored at a data node pointed to by a leaf \npointer node (Fig. 10). The list was computed by a left to right traversal of the leaf pointer nodes \nand adding to the list each data node they point to. The non\u00addata nodes form an indexing structure for \nthe BLinkTree and are used for accessing the data nodes ef.ciently but their structure is abstracted \nin the computation of viewI . Although the indexing structure is manipulated arbitrarily by many threads \nconcurrently, it keeps all the data nodes always visible and accessible even when uncompleted operations \nare modifying the indexing structure. 7.2.5 Selecting commit actions The BLinkTree mutator methods described \nin [12] follow a pattern in which the effect of each method is re.ected in the data struc\u00adture state \nby a single write to a particular data or leaf node vari\u00adable, while remaining writes re-structure the \ntree. For INSERT (See Figure 8) and DELETE, we identify this single write and designate it as the commit \naction. In INSERT, there are four possible com\u00admit points for different executions of INSERT depending \non what exit path is taken through the method code. These paths are distin\u00adguished by whether the key \nbeing inserted was already in the tree (line 15), whether the leaf node that is being inserted into needs \nto be split into two nodes (lines 39 and 46) , and whether the leaf node is also the root node of the \ntree (line 52). Each of these cases is identi.ed using a boolean expression in terms of method-local \nvariables. The appropriate leaf or data node write operation is then marked as the commit action. INSERT(key, \ndata) 1 . ptr is the pointer to the current parent node 2 . p is the pointer to the current node 3 p \n. null 4 completed . false 5 stack . MOVE-DOWN-AND-STACK() 6 repeat 7 ptr . POP(stack) 8 repeat 9 found \n. true 10 LOCK(ptr) 11 A . READ-NODE(ptr) 12 if A is a leaf pointer node and key is in A 13 then 14 \nOVERWRITE(A, data) 15 . Commit point 1 16 UNLOCK(ptr) 17 RETURN 18 if key > HIGHVALUE(A) 19 then 20 UNLOCK(ptr) \n21 found . false 22 ptr . GET-RIGHT-POINTER(A) 23 elseif key < LOWVALUE(A) 24 then 25 UNLOCK(ptr) 26 \nfound . false 27 ptr . GET-LEFT-POINTER(A) 28 until found 29 if p = null 30 then 31 . this is the .rst \nwrite and we 32 . are going to write a data node 33 p . CREATE-DATA-NODE(key, data) 34 if A is safe to \ninsert d 35 then 36 p . INSERT-INTO-SAFE(p, A) 37 if A is a leaf pointer node 38 then 39 . Commit point \n2 40 completed . true 41 elseif A is not the root 42 then 43 p . INSERT-INTO-UNSAFE(p, A) 44 if A is \na leaf pointer node 45 then 46 . Commit point 3 47 else 48 p . INSERT-INTO-UNSAFE-ROOT(p, A) 49 completed \n. true 50 if A is a leaf pointer node 51 then 52 . Commit point 4 53 until completed Figure 9. The implementation \nof the Insert subroutine in BLink-Tree and the conditional commit point annotations.       7.3 \nThe Scan .le system A previous prototype of VYRD had been applied to the Scan .le system [13] which consisted \nof some 5KLOC. It detected several previously undetected, serious concurrency bugs while running Scan \ns own performance benchmarks. Interestingly, these bugs were also in the cache module of Scan and were \nvery similar to those found in Boxwood s Cache. More details on this example can be found in [13]. Table \n1. Time to detection of error  Moving acquire in 4 1308 25 1.03 Multiset-Vector 8 773 21 FindSlot \n16 758 10 32 820 6 Multiset- Unlocking parent 4 3648 736 1.38 BinaryTree 8 930 217 before insertion \n16 330 76 32 262 78 Taking length 4 219 219 2.83 java.util.\u00ad non-atomically in 8 58 58 Vector lastIndexOf() \n16 52 52 32 25 25 Copying from an 4 195 90 3.46 java.util.\u00ad unprotected 8 152 63 StringBuffer StringBuffer \n16 124 19 32 29 17 BLinkTree Allowing 2 2198 405 1.27 duplicated data 4 4450 483 nodes 8 3332 611 \n10 2763 342 16 1069 301 25 3692 515 32 2111 715 Cache Writing an 4 521 14 16.9 unprotected dirty \n8 805 8 cache entry 10 599 10 16 302 29 25 539 26 32 311 34 7.4 Microbenchmarks 7.4.1 Multithreaded \nJava libraries We worked on the java.util.Vector and StringBuffer classes, concurrency errors in which \nhad been reported previously [6, 7]. Both examples consisted of approximately 1KLOC. Both I/O and view \nre.nement were able to easily detect the errors. 7.4.2 Multiset We developed two different Java implementations \nof Multiset: one with an Vector-based representation ( 300 LOC), the other based on a binary search tree \n( 1KLOC). We included com\u00adpression threads and a delete operation in both implementations. VYRD was a \nvaluable debugging aid during the development of the multiset implementations. The concurrency errors \nused in run\u00adtime measurements were inserted manually and were all detected.  7.5 I/O re.nement vs. view \nre.nement To compare the effectiveness of I/O and view re.nement checking, on the examples described \nabove, we recorded the number of meth\u00adods executed before the .rst error was detected by each technique. \nTable 1 reports the average of results from large numbers of repe\u00adtitions of the same experiment. The \nlast column presents the ratio of the CPU time required for running VYRD in view re.nement mode to that \nof running it in I/O re.nement only mode on the same trace. View re.nement s superiority in early detection \nis apparent from these results. The additional cost of checking view re.nement must be traded off with \nits ability to detects errors much earlier in the trace. The only error in java.util.Vector was in an \nobserver method and typically manifests itself without corrupting data struc\u00adture state. Therefore view \nre.nement was no better at detecting it than I/O re.nement in our tests. 7.6 Computational overhead \nmeasurements Table 2 compares the overhead during runtime due to logging only with the runtime of the \nunmodi.ed program, assuming I/O or view re.nement will be run using the log later. All numbers reported \nare CPU seconds on a 2.4 GHz Intel Pentium PC with 1GB of RAM. In the .rst three examples and in Cache, \nin which the mutator meth\u00adods perform a large number of shared variable writes, the overhead of logging \ninformation required for view re.nement is signi.cantly larger than that for I/O re.nement. This is because \nthese examples require logging at very .ne granularity. In java.util.Vector, java.util.StringBuffer and \nBLinkTree, the difference be\u00adtween logging overhead for I/O and view re.nement is much less. Table 3 \npresents results on the computational cost of checking view re.nement and compares it with the computational \ncost of running the program and logging information. All running times are CPU seconds on the same machine. \nThe impact of instrumen\u00adtation and logging, and the computational cost of checking view re.nement are \ntolerable, especially for the industrial examples.  8. Related work Many correctness criteria for concurrent \nsystems (e.g., atomicity in the work of Flanagan and Qadeer [7] and Wang and Stoller [16] require that \nnon-atomic (interleaved) executions of the implemen\u00adtation be equivalent to an atomic, sequential run \nof the implemen\u00adtation. The de.nition of equivalence used in these criteria does not make reference to \nthe speci.cation for the system. I/O-re.nement and view re.nement are different from these criteria in \nthat they do not require the existence of such an atomic, sequential run of the implementation. Therefore, \nwe believe that they rule out fewer useful practical implementations. The choice of the de.nitions for \nI/O and view re.nement were motivated by our experience with verifying two industrial-scale concurrent \nprograms, neither of which could be proven using using reduction (e.g. as in [7]), the concept of commit \natomicity as in [4], or by proper selection of pure code blocks as de.ned in [5]. The intuition behind \nthis fact is illustrated by the following simple example inspired by Boxwood. Consider a tree implementation \nof a set data structure in which data is stored at the leaves only, Table 2. Overhead of logging Implementation \nProgram I/O Ref. View Ref. Multiset-Vector 15.4 0.39 3.69  Vector 0.20 0.09 0.12 StringBuffer \n0.92 0.18 0.24 BLinkTree 56.2 2.42 2.63 Cache 1.8 1.67 3.31 Table 3. Running time breakdown Program \n#Thrd/ Prog. Prog.+ Prog.+ #Mthd alone logging logging only and VYRD VYRD alone (off-line)  Vector 20/200 \n0.2 0.32 2.46 2.03 StringBuffer 10/30 0.92 1.16 2.1 1.85 BLinkTree 10/600 56.2 58.9 213.18 157.32 Cache \n10/500 1.8 5.11 9.5 4.45 while internal nodes (the indexing structure ) enable search. In the following, \nlet p denote a shared variable a write to which changes the abstract data structure contents, i.e., a \nwrite which changes the contents of a leaf. Let q denote a shared variable that represents an internal \nnode of the tree. In the example below, writes to q re-arranges the shape of the data structure, for \nexample, by re-balancing the tree. Suppose all writes to p and q are lock protected. Consider the interleaving \nof actions by two threads t1 and t2 concurrently executing two methods \u00b51 and \u00b52 respectively: t1: W \n(p) W (q) t2: W (p) W (q) Because t1 and t2 perform multiple, lock-protected writes to the same set of \nshared variables that do not commute with each other, it is not possible to convert this execution using \nreduction or purity to a method-atomic execution of the implementation. However, since only the writes \nto p modify the abstract data structure state, re.nement checking can be performed using the witness \nordering of \u00b51 followed by \u00b52 and will not report any re.nement violations. I/O re.nement can be viewed \nas a variant of linearizability [8], where the sequential speci.cation for the data structure imple\u00admentation \nis given in the form of an executable speci.cation, and may contain non-determinism in order to allow \ntermination scenar\u00adios not possible in a sequential execution. Checking re.nement as a veri.cation approach \nis well-studied (See [1, 11] among many others). Runtime checking of confor\u00admance to a state invariant \nderived from an object model has been investigated [3]. Runtime checking of property annotations inserted \ninto implementation code has been studied [2]. Runtime analysis methods for correctness criteria such \nas atomicity have been de\u00adveloped [6, 16]. Re.nement checking has recently been integrated with simulation-based \nvalidation of hardware designs [15]. The commit-atomicity work of Flanagan [4], published simultaneously \nwith an earlier version of the work described here [14], is closely related but is a very restricted \ncase of the work presented here. In commit atomicity, the speci.cation is required to be the atomized \nversion of the implementation, re.nement checking is done only at quiescent points rather than at each \ncommit point, and a complete match between speci.cation and implementation states is required whereas \nwe, by using view variables, can declare more intuitively equivalent program states as matches. Our work \nis the .rst general tool that can check at runtime that an industrial-scale concurrent data structure \nimplementation re.nes a speci.cation. 9. Conclusion Run-time checking of re.nement promises to be a \npowerful veri.\u00adcation approach with reasonable computational cost. In this paper, we presented two notions \nof re.nement and techniques for check\u00ading them. We applied these techniques to the Boxwood project and \npresented experimental results demonstrating the ef.cacy of and the trade-offs offered by our techniques. \n  Acknowledgments The work reported in this paper was supported by Microsoft Re\u00adsearch, Redmond and \nSilicon Valley. We would like to thank Jim Larus, Roy Levin and Sriram Rajamani for making our collabo\u00adration \npossible. We also thank the Boxwood group, particularly Chandu Thekkath and Lidong Zhou for their support \nwith under\u00adstanding and running Boxwood. Serdar Tasiran would like to thank Martin Abadi for stimulating \ndiscussions. References [1] M. Abadi and L. Lamport. The existence of re.nement mappings. In Proc. 3rd \nAnnual Symposium on Logic in Computer Science, pp. 165 175. IEEE Computer Society Press, 1988. [2] F. \nChen and G. Rosu. Towards monitoring-oriented programming: A paradigm combining speci.cation and implementation. \nIn Electronic Notes in Theoretical Computer Science, Vol. 89. Elsevier, 2003. [3] M. L. Crane and J. \nDingel. Runtime conformance checking of objects using Alloy. In Electronic Notes in Theoretical Computer \nScience, Vol. 89. Elsevier, 2003. [4] C. Flanagan. Verifying Commit-Atomicity Using Model-Checking. In \nSPIN 04: The SPIN Workshop on Model Checking of Software. Springer-Verlag, 2004. [5] C. Flanagan, S. \nFreund, and S. Qadeer. Exploiting purity for atomicity. In Proceedings of the International Symposium \non Software Testing and Analysis (ISSTA 2004). ACM Press, 2004. [6] C. Flanagan and S. N. Freund. Atomizer: \nA dynamic atomicity checker for multithreaded programs. In Proc. 31st ACM Symposium on Principles of \nProgramming Languages, pp. 256 267, 2004. [7] C. Flanagan and S. Qadeer. A type and effect system for \natomicity. In Proc. ACM SIGPLAN 2003 Conf. on Programming language design and implementation, pages 338 \n349. ACM Press, 2003. [8] M. P. Herlihy and J. M. Wing. Linearizability: A correctness condition for \nconcurrent objects. ACM Trans. on Programming Languages and Systems, 12(3):463 492, 1990. [9] M. Ji and \nE. Felten. Scan-based scheduling and layout in a reliable write-optimized .le system. Technical Report \nTR-661-02, Princeton University, Department of Computer Science, 2002. [10] J. MacCormick, N. Murphy, \nM. Najork, C. A. Thekkath, and L. Zhou. Boxwood: Abstractions as the foundation for stor\u00adage infrastructure. \nIn Proceedings of the 6th Symposium on Operating Systems Design and Implementation (OSDI 2004), San Francisco, \nCA, USA, December 2004, pages 105-120. http://research.microsoft.com/research/sv/Boxwood [11] S. Park \nand D. L. Dill. Veri.cation of cache coherence protocols by aggregation of distributed transactions. \nTheory of Computing Systems, 31(4):355 376, 1998. [12] Y. Sagiv. Concurrent operations on b-trees with \novertaking. Journal of Computer and System Sciences, 3(2), Oct. 1986. [13] S. Tasiran, A. Bogdanov, and \nM. Ji. Detecting concurrency errors in .le systems by runtime re.nement checking. Technical Report HPL-2004-177, \nHP Laboratories, 2004. [14] S. Tasiran and S. Qadeer. Runtime re.nement veri.cation of concurrent data \nstructures. In Proc. Runtime Veri.cation 04 (ETAPS 04). Electronic Notes in Theoretical Computer Science. \nElsevier, 2004. [15] S. Tasiran, Y. Yu, and B. Batson. Using a formal speci.cation and a model checker \nto monitor and guide simulation. In Proceedings of the 40th Design Automation Conference, pages 356 361. \nACM, 2003. [16] L. Wang and S. D. Stoller. Run-time analysis for atomicity. In Electronic Notes in Theoretical \nComputer Science, Vol. 89. Elsevier, 2003.  \n\t\t\t", "proc_id": "1065010", "abstract": "We present a runtime technique for checking that a concurrently-accessed data structure implementation, such as a file system or the storage management module of a database, conforms to an executable specification that contains an atomic method per data structure operation. The specification can be provided separately or a non-concurrent, \"atomized\" interpretation of the implementation can serve as the specification. The technique consists of two phases. In the first phase, the implementation is instrumented in order to record information into a log during execution. In the second, a separate verification thread uses the logged information to drive an instance of the specification and to check whether the logged execution conforms to it. We paid special attention to the general applicability and scalability of the techniques and to minimizing their concurrency and performance impact. The result is a lightweight verification method that provides a significant improvement over testing for concurrent programs.We formalize conformance to a specification using the notion of refinement: Each trace of the implementation must be equivalent to some trace of the specification. Among the novel features of our work are two variations on the definition of refinement appropriate for runtime checking: I/O and \"view\" refinement. These definitions were motivated by our experience with two industrial-scale concurrent data structure implementations: the Boxwood project, a B-link tree data structure built on a novel storage infrastructure [10] and the Scan file system [9]. I/O and view refinement checking were implemented as a verification tool named VRYD (VerifYing concurrent programs by Runtime Refinement-violation Detection). VYRD was applied to the verification of Boxwood, Java class libraries, and, previously, to the Scan filesystem. It was able to detect previously unnoticed subtle concurrency bugs in Boxwood and the Scan file system, and the known bugs in the Java class libraries and manually constructed examples. Experimental results indicate that our techniques have modest computational cost.", "authors": [{"name": "Tayfun Elmas", "author_profile_id": "81100211898", "affiliation": "Ko&#231; University, Istanbul, Turkey", "person_id": "P728840", "email_address": "", "orcid_id": ""}, {"name": "Serdar Tasiran", "author_profile_id": "81100391292", "affiliation": "Ko&#231; University, Istanbul, Turkey", "person_id": "P261829", "email_address": "", "orcid_id": ""}, {"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "PP14106781", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065015", "year": "2005", "article_id": "1065015", "conference": "PLDI", "title": "VYRD: verifYing concurrent programs by runtime refinement-violation detection", "url": "http://dl.acm.org/citation.cfm?id=1065015"}