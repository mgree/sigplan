{"article_publication_date": "06-12-2005", "fulltext": "\n Mitosis Compiler: An Infrastructure for Speculative Threading Based on Pre-Computation Slices Carlos \nGarc\u00eda Qui\u00f1ones , Carlos Madriles , Jes\u00fas S\u00e1nchez , Pedro Marcuello , Antonio Gonz\u00e1lez and Dean M. Tullsen \n1 Intel Barcelona Research Center Dept. of Computer Science and Engineering Intel Labs, Universitat \nPolit\u00e8cnica de Catalunya, Barcelona University of California, San Diego e-mail: {carlos.garcia.quinones, \ncarlos.madriles.gimeno, f.jesus.sanchez, pedro.marcuello, antonio.gonzalez}@intel.com, tullsen@cs.ucsd.edu \nAbstract Speculative parallelization can provide significant sources of additional thread-level parallelism, \nespecially for irregular applications that are hard to parallelize by conventional approaches. In this \npaper, we present the Mitosis compiler, which partitions applications into speculative threads, with \nspecial emphasis on applications for which conventional parallelizing approaches fail. The management \nof inter-thread data dependences is crucial for the performance of the system. The Mitosis framework \nuses a pure software approach to predict/compute the thread s input values. This software approach is \nbased on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added \nat the beginning of the speculative thread. P-slices must compute thread input values accurately but \nthey do not need to guarantee correctness, since the underlying architecture can detect and recover from \nmisspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce \ntheir overhead. The most important optimizations included in the Mitosis compiler and presented in this \npaper are branch pruning, memory and register dependence speculation, and early thread squashing. Performance \nevaluation of Mitosis compiler/architecture shows an average speedup of 2.2. Categories and Subject Descriptors \nC.1.4 [Processor Architectures]: Parallel Architectures, D.3.4 [Programming Languages] Processors compilers, \ncode generation, optimization. General Terms Performance, Design Keywords Speculative multithreading; \nthread-level parallelism; automatic parallelization; pre-computation slices. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advantage and that copies bear this \nnotice and the full citation on the first page. To copy otherwise, or republish, to post on servers or \nto redistribute to lists, requires prior specific permission and/or a fee. PLDI 05, June 12 15, 2005, \nChicago, Illinois, USA. Copyright 2005 ACM 1-59593-056-6/05/0006 $5.00. 1. Introduction Several microprocessor \nvendors have recently introduced single chip architectures that can execute multiple threads in parallel, \nexploiting thread-level parallelism. Two different approaches have been used to architect these systems: \nsimultaneous multithreading [25][7] and multiple cores [24][22][16]. These architectures increase throughput \nby executing independent jobs in parallel, or reduce execution time by parallelizing applications. This \nlatter case has proved to be successful for regular numerical applications, but less so for non-numerical, \nirregular applications, for which the compiler usually fails to discover a significant amount of thread-level \nparallelism. Speculative multithreading (SpMT for short) attempts to speed up the execution of applications \nthrough speculative thread-level parallelism. Threads are speculative in the sense that they may be data \nand control dependent on previous threads (that have not completed) and their execution may be incorrect. \nThere are two main strategies for speculative thread-level parallelism: (1) use helper threads to reduce \nthe execution time of high-latency instructions/events through side effects, and (2) parallelize applications \ninto speculative parallel threads, each of which contributes by executing a part of the original application. \nHelper Threads [6][5][18][27] attempt to reduce the execution time of the application by using speculative \nthreads to reduce the cost of high-latency operations (such as load misses and branch mispredicts). For \ninstance, in [5][27] this is done by executing a subset of instructions from the original code to pre-compute \nload addresses or branch directions. Instructions executed by speculative threads do not compute/modify \nany architectural state of the processor, and thus, all architectural state must still be computed by \nthe main, conventional thread. With speculative parallelization ([10][1][13] among others), each of the \nspeculative threads executes a different part of the program. This partitioning is based on relaxing \nthe parallelization constraints and allowing the spawning of speculative threads even when the compiler \ncannot guarantee correct execution. When a speculative thread finishes, the speculation is verified. \nUnlike helper threads, the values produced by a speculative thread are 1 This work was done while he \nwas a visiting professor at Intel Barcelona Research Center SP CQ I P SP SP CQ I P IN IT P- S L I \nC E BO D Y WA I T C O MMIT IN I T P- S L I C E BODY I T CO M M IT  CQ IP S equent i a l E x e c u \nt i o n T i m e  CQ IP P a ra lle l E x e c u t io n T i m e Sp a w n St ar t En d Co m m i t ti me \nti m e ti me ti m e (a ) S e qu e n t i a l ve rs io n (b) Sp MT v e rs i o n Figure 1.Sequential vs \nSpMT parallel execution committed. In case of amisspeculation (control or data), the work doneby the \nspeculative thread is discarded. The Mitosis architecture used in this work follows this approach. A \npotential speculative thread is defined by aspawning pair, which consists of the point in which the spawning \ninstruction is inserted and the point where the speculative threadwill start execution when it is spawned. \nPrior work has demonstrated that well chosen speculative threads (or spawning pairs) can result in significant \nspeedups[15]. Conversely, a poorly chosen pair can hurt performance. A key point in any SpMT system is \nhow to deal with inter-thread data dependences. Two mechanisms have been studied so far: (1) synchronization \nmechanisms and (2) value prediction. The synchronization approach imposes a high overheadwhen dependences \nare frequent, as in the workloadpresented here. Value prediction has more potential if the values that \nare computed by one thread andconsumed by another canbe predicted, the consumer threadcan be executed \ninparallel with the producer thread since these values are only neededfor validation at a later stage. \nIt is typically assumed that these value predictions are computed in hardware.The Mitosis system presents \na novel approach, which adds code (derived from the originalprogram) to predict in software the live-ins \n(values consumed, but not produced by, the thread) for each speculative thread. Because mechanisms for \nrecovery of incorrect threads are already in place, the code toproduce the values need not always be \ncorrect, and can be highly optimized. We refer to this code as pre-computation slices (p-slices). The \nmain advantages of p-slices are: (1) they are potentiallymore accurate in the predictionof live-ins than \na hardware-based predictor, since it is derived from the original code, (2) they can encapsulate multiple \ncontrol flows that contribute to the prediction of live-ins, and (3) they can accelerate the detection \nof incorrectly spawned threads. In this work, we present the Mitosis compiler. One objective of the Mitosis \ncompiler is to identify themost effective points in any program to spawn speculative threads. This entails, \namong other tasks, locating regionsof code where the live-ins can be accurately predicted throughp-slices \nwith a low overhead. The Mitosis compiler provides thefollowingfeatures: (1) it identifies effective \nspawning pairs, (2) generates pre-computation slices, (3) optimizes pre-computation slices to minimize \ntheir overhead, and (4) maximizes the accuracy of pre-computation slices. Themain contributionsof this \nwork are: A general compilation framework to analyze and insert spawning pairs at any point of any program. \n The use of pre-computationslices to predict values corresponding to inter-threaddependences.  Figure \n2.Stages in the life of a speculative thread Amechanism to build and optimize (in terms of both accuracy \nand overhead) pre-computation slices.  Amodel to estimate the benefit ofany set ofspawning pairs for \na given SpMT configuration, and a scheme to select the most effective set.  Evaluation of the proposed \ncompilation technique showsvery encouraging results. Performance results reported for the Mitosis compiler \nshow aspeedup of about 2.2 for asubset of the Olden benchmark suite. This is code that state-of-the-art \nparallelizing compilers/architecturescannot parallelize. The rest of the paper is organized as follows. \nSection 2 describes basic concepts of the Mitosis SpMT architecture that are relevant to the Mitosis \ncompiler. In Section 3, the Mitosis compiler infrastructure is presented. The scheme to build and optimize \nspeculative pre-computation slices is further detailed in Section 4. Section 5 evaluates the Mitosis \ncompiler. Finally, Section 6 discusses some relatedwork and Section 7 summarizes the main conclusionsof \nthis work.  2. Overview ofthe Mitosis Architecture This section presents an overview of the underlying \nspeculative multithreaded architecture, with special emphasis on those features that are relevant to \nthe compiler for generating effective speculative threads. 2.1. Features The Mitosis SpMT architecture \nis composed of severalthread units(TUs), each able to execute a thread. A thread unit contains its own \nregister file, instruction/data caches, functionalunits and program counter. This means that the compiler \ncan see a thread unit as an independent entity of execution. The threadunits can be organized in various \nways. For instance, one can implement this architecture in a SMT (Simultaneous Multithreaded) or a CMP \n(Chip Multiprocessor) fashion. For this study, weassumea CMP\u00adlike design for simplicity and scalability. \n 2.2. Execution Model In the Mitosis architecture there is always one (and only one) non\u00adspeculative \nthread, which is theonly one allowed to commit its results. All other threads are speculative. Aspeculative \nthread is created whena spawn instruction is found. Aspawn instruction basically describesa spawning \npair. Aspawning pair is defined as a set of two points in the program (each at the beginning of a basic \nblock). The former is called the spawning point (SP), marked by the spawn instruction, and identifies \nwhen a new speculative thread is created. The latter is called thecontrol quasi\u00adindependent point (CQIP) \nand representswhere the speculative thread starts executing (after some initialization), and is identified \nas an operand of thespawn instruction. Figure 1 shows the 0. 23 8 0 . 7 62  (c) Lo o p m a cr ono de \n (a ) Con t r o l - fl o w gr aph of th e a l o o p TYPE PATH PROB NEXT (b ) Lo op p a t h s Figure \n3. Construction of a loop macronode difference between sequential and SpMT parallel execution of code. \nAny thread can spawn a new thread. The requirements to spawn a thread are: (i) there is a free thread \nunit, or (ii) there is at least one running thread more speculative (further in sequential time) than \nthe thread is to be created. In the latter case, the most speculative thread is cancelled and the freed \nthread unit is assigned to the spawned thread. Correct execution of the code following the CQIP requires \nthat the future state (memory and register values) of the processor at the CQIP is correctly predicted. \nOur SpMT model differs from prior proposals in that we assume that each speculative thread includes a \npre-computation slice (p-slice for short) that computes the live\u00adins of the thread. A speculative thread \nhas two operation modes, depending on whether it is executing code from the pre\u00adcomputation slice or \nthe body of the thread. In particular, data produced while in p-slice mode is stored in a special buffer \n(called the slice buffer) and will be used as input for the body of the speculative thread. Data produced \nby the speculative thread is kept in the regular structures of the thread unit (register file and memory) \nand will be committed once the thread becomes non\u00adspeculative. This distinction arises from the fact \nthat the p-slice only predicts machine state, while the speculative thread body (following the p-slice) \ncalculates actual machine state; thus, data produced by the p-slice must be confirmed, but never committed. \nThreads commit in sequential order. This means that a thread must wait to become the oldest one (i.e., \nthe non-speculative thread) to commit. A thread finishes when it reaches the start (CQIP) of another \nthread that is active. Then, the latter thread is validated. If the validation is correct, the latter \nthread is allowed to commit once it is the oldest thread. Otherwise, it is squashed (as well as its successors) \nand the former thread proceeds to execute the instructions beyond the CQIP. Figure 2 shows a scheme with \nthe different stages in the life of a speculative thread.  3. Selecting Spawning Pairs The Mitosis \ncompiler for this architecture performs the following tasks: (1) generate the p-slices for each pair, \n(2) optimize the p\u00adslices to minimize overhead, and (3) select the best candidate spawning pairs. These \ntasks are heavily inter-related; however, we will discuss each separately. This section describes the \nselection of spawning pairs. Generation and optimization of p-slices are described in Section 4. The \nproposed scheme has been implemented in the code generation phase (after optimizations) of the ORC compiler \n[11]. The compiler makes use of the information provided by an edge profile. This information includes \nthe probability of going from any basic block to each of its successors and the execution count of each \nbasic block. Selecting the best set of spawning pairs requires assessing the benefit of any given candidate \npair. However, determining the benefits of a particular spawning pair is not straightforward. The effectiveness \nof a pair depends on the control flow between the spawning point and the start of the thread, the control \nflow after the start of the thread, the accuracy of the p-slice, the overhead of the p-slice, the number \nof hardware contexts available to execute speculative threads, and interactions with other speculative \nthreads running at the same time. This analysis requires a model of program execution. To avoid capturing \nand repeatedly traversing a full path trace of the program, we generate a (much smaller) synthetic trace \nof execution that captures the dynamic behavior. The key idea is to traverse this trace while keeping \ntrack of the threads that are active at any time. For each thread, its state (see Figure 2) is maintained \nto emulate its evolution during its lifetime. This analysis emulates the timing behavior of the speculative \nthreads, assuming a simple model where each instruction takes a fixed time. Based on this, the compiler \ncan estimate the expected benefits of any set of spawning pairs, and select those that are expected to \nminimize total execution time. 3.1. Building the Synthetic Trace We build a synthetic trace of the program \nto translate the edge profile into path information, without having to capture and maintain a full path \nprofile. The synthetic trace is built based on edge profiling information at the basic block level. The \nanalysis performs a reverse topological traversal of the call graph. This means that callee routines \nare analyzed before callers. For each routine, we compute a set of paths. A routine path is defined as \na list of connected nodes in the CFG from the entry node to an exit node. We assume here that a routine \nhas only one entry. A path node can be one of these types: basic block (BB), loop, or call. Loop and \ncall nodes are macronodes in the sense that they include more than a single basic block. A loop node \nconsists of the header of a given loop and contains all the nodes (basic blocks, inner loops or calls) \nthat belong to that loop. A call node is just a basic block that ends with a call to a particular function. \nThe exit node of a routine path could be either a return (node with no successor) or a call to the exit \nfunction. Thus, there are two types of routine paths: return paths and exit paths. Each routine path \nis characterized by its total length (in instructions) and its probability (using the edge profiling \ninformation). This is summarized in the following expressions: ROUTINE = SET OF { ROUTINE PATH } ROUTINE \nPATH = LIST OF { NODE } + TYPE + LENGTH + PROB ROUTINE PATH TYPE = { RETURN | EXIT } NODE = { BB | LOOP \n| CALL } A loop node requires more analysis. For each loop, we compute a set of loop paths. A loop path \nis defined as a sequence of nodes in the CFG of the loop from the head of the loop to a possible loop \n [ 1] t_exec = SeqExecTime;  [ 2] Selected_Pairs = \u00d8; [ 3] exit = FALSE;  [ 4] while (!exit) {  [ \n5] select = NULL;  [ 6] for (cand=First_Cand(Candidate_Pairs); cand; cand = Next_Cand(cand)) {  [ \n7] Analized_Pairs = Selected_Pairs + cand;  [ 8] t_exec_tmp = Model_Set_of_Pairs(Analyzed_Pairs, Trace, \nN_TUs);  [ 9] if (t_exec_tmp < t_exec) {   [10] t_exec = t_exec_tmp; [11] select = cand; [12] \n} [13] } [14] if (select == NULL) [15] exit = TRUE; [16] else {  [17] Candidate_Pairs = Candidate_Pairs \n select; [18] Selected_Pairs = Selected_Pairs + select; [19] } [20] }  Figure 4. Greedy algorithm \nto select spawning pairs exit node. A loop exit node can be a node with an edge to the loop head (continue \npath), an edge outside the loop (break path) or a call to a function that may call the exit function \n(exit path). We are assuming here that a loop has a single header. As in the case of routines, each loop \npath has its length (in instructions) and probability. This is summarized with the following expressions: \nLOOP PATH = LIST OF { NODE } + TYPE + LENGTH + PROB LOOP PATH TYPE = { CONTINUE | BREAK | EXIT } Next, \na set of synthetic traces are built for each loop. A loop synthetic trace consists of selecting NITER \n 1 loop paths of type continue and 1 loop path of type break or exit, where NITER is the average trip \ncount. For large loops, we build the trace assuming a fixed maximum value for the trip count. The rest \nof the iterations are considered to have the same behavior as the analyzed ones. The selection of the \npaths inside the loop consists of a random weighted model according to each path probability. The number \nof loop synthetic traces that are built per loop is a parameter of the tool and can be adjusted depending \non time and memory space requirements. We call each loop s synthetic trace a loop instance. For each \nloop node the compiler keeps a list of instances and a pointer to one of them (initially the first one), \nthat is updated in a circular fashion. LOOP = CIRCULAR LIST OF { LOOP INSTANCE } + POINTER + TRIP COUNT \n+ LENGTH LOOP INSTANCE = LIST OF { LOOP PATH } The length of a loop node is computed as the weighted \naverage length of the loop instances. Figure 3 shows an example of how a given loop is split in its different \npaths and the shape of the resultant loop macronode. A call node also has attached a list of instances. \nA call instance refers to a possible path in the callee routine. The compiler builds this list of instances, \nsimilarly to loops, by randomly selecting (based on the probabilities) routine paths from the callee \nroutine. In the same way, a pointer is also attached. CALL = CIRCULAR LIST OF { CALL INSTANCE } + POINTER \n+ LENGTH CALL INSTANCE = ROUTINE PATH The length of a call node is computed as the weighted length \nof routine paths in the callee function. The synthetic trace will be traversed by starting the path at \nthe main routine of the program. As the main routine is called only once, there is only one routine path \nfor that function. When a loop node is found, the traversal proceeds through the loop instance pointed \nby the loop instance pointer and the pointer is set to the next instance. In the case of a call node, \nthe traversal proceeds through the path in the callee routine described by the call pointer and the pointer \nis set to the next one in the same way as for loops. When an exit path (or the only path in the main \nroutine) is completely traversed, the program traversal finishes.  3.2. Candidate Pairs A key feature \nof the proposed compilation tool is its generality, in the sense that it can discover speculative thread-level \nparallelism in any region of the program. The tool is not constrained to analyze potential spawning pairs \nat loop or subroutine boundary, but practically any pair of basic blocks is considered a candidate spawning \npair. To reduce the search space we first apply the following filters to eliminate candidate pairs that \nare likely to have little potential: 1) Spawning pairs in routines whose contribution to the total execution \nof the program is lower than a threshold are discarded. 2) Both basic blocks of the spawning pair must \nbe located in the same routine and at the same loop level. 3) The length of the spawning pair (as the \naverage length of all the paths from the SP to the CQIP) must be higher than a certain minimum size in \norder to overcome the initialization overhead when a speculative thread is created. It must also be lower \nthan a certain maximum size in order to avoid very large speculative threads and avoid stalls due to \nthe lack of space to store speculative state. 4) The probability of reaching the CQIP from the SP must \nbe higher than a certain threshold. 5) Finally, the ratio between the length of the p-slice and the estimated \nlength of the speculative thread must be lower than a threshold. This ratio is a key factor for the benefits \nof the thread. In Section 4 we describe in detail how the pre\u00adcomputation slices are built and optimized \nto reduce this overhead. This step analyzes the different routines in the program one by one. For each \nroutine, all combinations of basic blocks are considered and passed through the different filters. The \nresult of YES  this process is a set of candidate spawning pairs (candidate pairs for short) of the \nwhole program. For each candidate pair the following information is kept: (i) basic block for the spawning \npoint, (ii) basic block for the CQIP, (iii) probability that the p\u00adslice reaches the CQIP and average \nlength in this case, and (iv) average length of the p-slice when the CQIP is not reached (in this case, \nthe speculative thread is cancelled before its body is started).  3.3. Pair Selection Once the set of \ncandidate pairs is built, the selection of pairs from it follows the greedy algorithm shown in Figure \n4. The basic idea is to include pairs in the selected set until negligible benefit is obtained. Among \nall candidate pairs, the new pair chosen (if any) is the one that provides the best improvement among \nall pairs in the candidate set. The benefit is computed using a model that estimates the execution behavior \nof a set of pairs for a given number of thread units. The inputs to this algorithm are: (i) the program \ntrace (see Section 3.1), (ii) the set of candidate pairs (see Section 3.2) and (iii) the number of thread \nunits. Initially, the execution time is set to the equivalent execution time when no spawning pairs are \nconsidered (SeqExecTime) and the set of selected pairs is empty (lines 1 and 2 in Figure 4). Then, the \ngreedy loop begins (line 4). At each iteration, all individual pairs in the candidate set are tried one \nby one, in conjunction with the pairs already selected (line 7). The execution time of the program for \neach of these new sets of spawning pairs is estimated. The model for this estimation is explained in \ndetail in Section 3.3.1. If any new set of pairs is better than the current one (line 9), the new pair \nis kept in the SELECT variable and execution time is updated. If no new set results in significant improvement, \nthe greedy algorithm finishes (lines 14 and 15). However, if a given combination improved the previous \nexecution time, the SELECTpair, which contains the best pair in the candidate set, is removed from the \ncandidate set (line 17) and added to the selected set (line 18). Depending on the parameterization of \nthe filters, this exhaustive search can still be a lengthy process. However, this is a research compiler \noptimizing code for, and facilitating the understanding of, a research architecture. As we achieve success \nand experience with finding effective pairs, we expect to be able to refine the search process significantly. \n3.3.1. SpMT Estimation Model As we have seen before, the core of the selection algorithm is a function \nable to estimate the execution time of a program for a given set of spawning pairs (line 8 in Figure \n4). The inputs of that model are: (i) the number of thread units, (ii) a program trace, and (iii) a set \nof spawning pairs. The goal of this model is to analyze the behavior and interactions of the set of spawning \npairs when the program is executed on the given SpMT processor. The output of the model is the SpMT execution \ntime. For the sake of simplicity we assume below that the execution of any instruction takes a unit of \ntime. However, the model can be extended in a straightforward manner to include different execution times \nfor each static instruction (e.g., using average memory latencies obtained through profiling). The model \nanalyzes the evolution of threads during execution. The model works as follows: the program trace is \nanalyzed sequentially. Just key basic blocks need to be analyzed. Key basic blocks are: the first and \nlast basic blocks in the trace, and SP and CQIP basic blocks. During the trace traversal, two global \nvariables are being updated: Current time: the time at which the current basic block instance is being \nexecuted.  Current thread: the thread that executes the current basic  block instance under analysis. \nThe core of the model is shown in Figure 5. Initially, a single non\u00adspeculative thread is assumed. This \nthread is allocated to any thread unit and is supposed to be responsible for the execution of the whole \ntrace, so its end time is accordingly initialized. When a basic block that corresponds to an SP in the \nset of pairs is found, some actions are required. If the basic block corresponding to the CQIP does not \nexist in the remainder of the trace or the given CQIP has been already executed by another thread, the \nthread is marked as a CANCELthread (a thread that exits before the CQIP is reached); otherwise, we mark \nit as a NORMAL thread. In either case we look for a thread unit. If a thread unit is free we assign it \nto the new thread. Otherwise, we check if the most speculative thread (the one whose next thread is NULL) \nis further in program order (more speculative) than the new thread. If so, the most speculative thread \nis cancelled and the freed unit is allocated to the new thread. Otherwise, the spawn is discarded. When \na basic block that corresponds to a CQIP in the set of pairs is found, it is checked whether any more \nspeculative active thread was started at this basic block instance. If this is the case, the current \nthread is terminated and current thread and current time variables are updated accordingly. Finally, \nthe last basic block of the trace just terminates the current thread. The commit time of this last thread \nrepresents the SpMT execution time of the program. Spawning a new thread requires the following actions: \n1) Identify the order of this new thread with respect to the current ones. Its previous thread is the \nthread that contains the CQIP of the new thread (i.e., the linked list of threads starting at the current \nthread is traversed until the first that contains the CQIP is found). Its next thread is the thread that \nwas the successor (before spawning) of its previous thread. 2) Decide whether this is a CANCEL thread \nor a NORMAL one: this is randomly selected based on the cancel probability of each type for this particular \nthread. 3) Record the start and end basic blocks of the thread. The former is the current CQIP and the \nlater is the start of the next thread. Finally, canceling a thread requires the following actions: (1) \nidentify previous and next threads, and (2) update links and end information of the previous thread. \n  4. Speculative P-Slices This architecture handles inter-thread dependences through the execution of \na pre-computation slice inserted at the beginning of every speculative thread. The goal of the p-slice \nis to calculate the live-ins of the new speculative thread very quickly. Regardless of the code we generate \nfor a given p-slice, the architecture guarantees a functionally correct execution of the program. Thus \naccuracy of p-slices only affects performance, not correctness. This is a key observation, since it allows \nthe compiler to perform aggressive, unsafe optimizations when generating these p-slices. The steps to \nbuild a p-slice for a given spawning pair are: 1) Identify live-ins. 2) Generate conservative p-slice. \n3) Optimize the p-slice. 4.1. Identifying Live-ins Identifying the live-ins of a speculative thread requires \na top\u00addown traversal of its control-flow graph starting at the CQIP to identify register and memory values \nread before being written by the speculative thread. Each path is explored until a certain length. This \nlength represents the time that previous threads take to compute and commit these values. This is because \nonce the previous thread commits, the speculative thread need no longer rely on predicted values, but \ncan read committed values. This time is estimated as the time it takes to sequentially execute all the \ncode between the SP and CQIP minus the thread spawn overhead. 4.2. Generating Conservative P-Slices \nThe p-slice for a spawning pair is built by traversing the control\u00adflow graph backwards from the CQIP \nuntil the SP. The input to that step is the set of live-ins, both register and memory values. The first \ninstructions included in the slice are those that directly produce the thread live-ins. Then the process \ninserts ancestors of these instructions, taking into account both data and control dependences, provided \nthat they are below the SP. Initially, all dependences among instruction as given by the compiler in \nthe conventional, conservative way are considered. This means that the only reason why a p-slice may \nbe incorrect is if not all the live-ins of the thread are being considered (e.g. the length of the thread, \nas described in the previous section, may have been incorrectly estimated). Upon finding a call to a \nsubroutine, the side-effects of that subroutine as well as the use of the returned value(s) are analyzed, \nand if there is any dependence, the call instruction to the subroutine is included into the slice. This \nmeans that the slice includes the whole subroutine, although the full code of that subroutine may not \nbe needed. A possible optimization (not considered in the results presented in this paper) would be the \nin\u00adlining or specialization of some functions. 4.2.1. Early Cancellation A safe optimization that we \nhave implemented is called early cancellation. Starting from the SP, we can analyze whether a path in \nthe control-flow graph will reach the CQIP or not. In the latter case, the thread would keep executing \nuseless instructions (wasting power and keeping a thread unit busy) until it is squashed by a less speculative \nthread. A reachability analysis of the CFG from the SP to the CQIP is used to identify the points in \nthe program where we can guarantee that the CQIP will never be reached. The compiler inserts a cancel \ninstruction in each of these points, which will squash the thread when executed. 4.3. Speculative Optimizations \nThe p-slices built using the conservative assumptions of the compiler, as described above, are normally \nvery large. Large p\u00adslices significantly constrain the benefits of speculative threads. However, a key \nfeature of the Mitosis SpMT architecture is that it can detect and recover from misspeculations. This \nopens the door to new types of aggressive/unsafe optimizations that otherwise could not be applied by \nthe compiler, and which have the potential to significantly reduce the overhead of p-slices. In the following \nsubsections, we describe the set of speculative optimizations currently included in the Mitosis compiler. \nSpeculative optimizations require a new factor in the analysis: the misspeculation probability. This \nfactor represents the probability that a given p-slice is incorrect. This happens when some live-ins \nare not computed or they are incorrect. This probability is attached to each candidate pair and used \nby the model described in Section 3.3.1 when deciding whether a spawned pair is NORMAL or CANCEL. 4.3.1. \nMemory Dependence Speculation Modern compilers often fail to parallelize applications because of ambiguous \nmemory dependences. Many memory dependences are only included because the compiler cannot prove that \nthe corresponding instructions are independent, but in fact they are. In many other cases, two static \ninstructions do have a memory  (b ) C o n s e r v a tiv e s l i c e ( d ) O p timi z e d s l ic e a \n. b,c CTRL b . d REG c . g REG f . l REG d . j MEM (p=0.01) g . o REG d . n MEM (p=1.00) h . k REG (c \n) S o m e D D G d e p e n d e n c e s Figure 6. Example of p-slice dependence, but this dependence only \nhappens for a very few dynamic instances of these instructions. We have implemented a memory dependence \nprofile to minimize the number of unnecessary dependences considered when generating p-slices. The profiler \ncomputes the dependence frequency between any pair of store-load, store-call, call\u00adload or call-call \ninstructions for each routine (the SP and CQIP of a spawning pair are in the same routine). Dependences \nfor calls refer to dependences due to any memory reference inside the called routine. The compiler only \nconsiders that two instructions have a dependence whenever this dependence has happened with a frequency \nabove a given threshold. In other words, dependences that never occur in practice, or occur very infrequently, \nare discarded. 4.3.2. Branch Pruning Branch pruning consists of ignoring those paths that exhibit low \nprobability of being taken when generating p-slices. These paths may belong to either the body of a speculative \nthread or its p\u00adslice, with different consequences in each case. Pruning branches of the body of speculative \nthreads is done during the process of identifying the live-ins of the threads (Section 4.1). A pruned \nbranch is still included in the thread body code, but any live-in in the pruned path is ignored when \nthe p\u00adslice is generated, which reduces the size of the p-slice. On the other hand, pruning a branch \nin the p-slice removes all the instructions of the pruned path from the p-slice. Additionally, predecessors \nof these removed instructions are also removed if their output is not used elsewhere. In the place of \na pruned path, a cancelinstruction is inserted; if this path happens to be taken, the thread input values \nwill likely be miscomputed, and it is preferable to cancel the thread and free this hardware context \nfor another thread. We call this optimization speculative early cancellation. Examples of both types \nof branch pruning are shown in Figure 6. On the left, (a) shows a control-flow graph that includes a \nSP and a CQIP. Each edge is annotated with its probability of being taken (edges without label have probability \n1.0). On the right (b), the control-flow graph of the conservative p-slice is shown. Basic blocks are \nlabeled with prime letters to indicate that they contain just a subset of the instructions of the original \nbasic blocks. Some data dependences among instructions in some basic blocks are also listed in Figure \n6(c) (lower case letters represent instructions in basic blocks with the corresponding capital letter, \ne.g., instruction a is in basic block A ). A possible edge (i.e., branch) to be pruned in the speculative \nthread is L.N, which will remove a live-in (data dependence d.n) and then some instructions in the slice \n(dependence b.d is not needed). On the other hand, an example of pruning in the p-slice would be for \nedge E.G, which will remove the data dependence g.o. This will remove in turn the need for dependence \nc.g in the p-slice. In this example, as no instructions are needed from basic blocks B and C (since their \ndependences have been removed), the control dependence a.b,c can also be removed from the slice. The \nresulting optimized p\u00adslice is show in Figure 6(d). 4.3.3. Dependence Pruning Data dependences that \nare infrequent can also be ignored. For memory dependences, the profiler described in Section 4.3.1 is \nused for this purpose. In the case of register dependences, the probability of reaching the producer \nonce the spawn has been executed is computed and multiplied by the probability of reaching the consumer \nafter executing the producer. Note that a consumer can be located either in the slice or the speculative \nthread body. As in the case of memory dependences, if this probability is lower than a threshold, the \ndependence is ignored for the purpose of generating the final p-slice. 4.3.4. Cancel Elimination As \npreviously discussed, cancel instructions are inserted at points where the compiler can guarantee that \nthe speculative thread is incorrect or the flow cannot reach the CQIP. This allows the processor to squash \nearly a speculative thread in order to free the thread unit for other threads. However, this means that \nthe branch instruction leading to the pruned code must be preserved (and all its ancestors in the dependence \ngraph). This overhead may be large in some cases, which significantly impacts the effectiveness of the \nspeculative thread. In these cases, it may be more effective just to remove the cancel operation and \nthe associated branch instruction (which will also remove some of its ancestors). This will make the \nslice always follow the frequent path, which can be incorrect in some infrequent cases. The architecture \nwill still detect these misspeculations, and squash the thread.  5. Experimental Evaluation 5.1. Framework \nThe Mitosis compiler has been implemented on top of the ORC compiler [11] to generate IPF code. The performance \nof the Mitosis compiler/architecture has been evaluated through a detailed, execution-driven microarchitectural \nsimulator built on top of SMTSIM [25]. The modeled Mitosis processor is a research Itanium\u00ae CMP processor \nwith 4 hardware contexts. Each hardware context is a 6-way issue, in-order core. The main parameters \nof the processor configuration are shown in Table 1. The figures in the table are per thread unit. Table \n1. Mitosis processor configuration Fetch, in-order issue and commit bandwidth 2 bundles (6 instructions) \nI-Cache 64KB L0-Cache 4-way associative 16KB hit latency: 1 cycle L1-Cache 4-way associative 1MB hit \nlatency: 4 cycles L2-Cache (share) 4 way associative 8 MB hit latency: 8; miss latency: 250 Local Register \nFile Latency = 1 cycle Remote Register File Latency = 6 cycles Spawn overhead 5 cycles Validation overhead \n15 cycles Commit overhead 5 cycles To evaluate the potential performance of the Mitosis architecture, \na set of non-automatic parallelizable codes have been used. These benchmarks correspond to a subset of \nthe Olden benchmark suite. The benchmarks used are bh, em3d, health, mst and perimeter. We have used \na train input for profiling of around 10M instructions per benchmarks, and a different input set that \non average executes around 300M instructions for the simulation. Statistics in the next section correspond \nto the whole execution of these programs. The rest of the suite has not been considered due to the recursive \nnature of the programs. The Mitosis compiler is not currently able to extract speculative thread-level \nparallelism in recursive routines. This feature will be targeted in future work. The ORC compiler has \nbeen used with full optimizations enabled (-O3) except software pipelining and if-conversion. For the \nMitosis optimizations, we have considered a 5% threshold for dependence pruning and 15% for branch pruning. \nWe have not paid special attention to the compilation time. Our first attempt at using filters to trim \nthe search space is very promising, but timing aspects require further work in the future. Olden benchmarks \nhave been chosen since they are pointer intensive programs for which automatic parallel compilers are \nunable to extract thread-level parallelism. To corroborate this, we have compiled the Olden suite with \nthe Intel\u00ae C++ production compiler which produces parallel code. Almost no part of the code was parallelized \nfor any benchmark. 5.2. Results The first results we will show focus on the benefit of the proposed \noptimization for the p-slices. For that, we will use a metric that we call average benefit per pair. \nIt is an approximation of the number of parallelized instructions by each instance of the pair. The expected \nbenefit of a single pair is computed as follows: Overlap = PairLength (SliceLength + Init) ProbCorrect \n= (1-Cancel) * (1-Misspec) Benefit = Overlap * Count * ProbCorrect PairLengthand SliceLength show the \naverage length of the pair and the slice, respectively. Init, as shown in Figure 2, represents the latency \nof the spawn instruction. Cancel shows the probability that the slice is cancelled, and Misspec is the \nprobability that the slice is incorrect due to speculative optimizations. Finally, Count is the number \nof times the spawning instruction is executed. From those expressions, the average benefit per pair is \ncomputed as: AvgBenefitPerPair = SUM(Benefiti) / SUM(Counti), for all PAIRi To quantify the effect of \nthe different optimizations applied to p\u00adslices, Table 2 shows the average benefit per pair for all pairs \nafter filtering (that is, the set of candidate pairs that will be later considered by the pair selection \nscheme). We show in the different columns the proposed metric without any speculative optimization (DFL), \nafter dependence pruning (DPR), after branch pruning (BPR) and finally after cancel elimination (CCL). \nEach optimization is added on top of previous ones. Table 2. Benefit of p-slice optimizations on all \ncandidate pairs Default Dependence pruning Branch pruning Cancel elimination 1.9 106.5 106.5 287.6 We \ncan observe that all optimizations significantly improve the quality of p-slices, with the exception \nof branch pruning. However branch pruning is necessary for cancel elimination, which is shown to be quite \neffective. Figure 7 shows the estimated speedup (using the model proposed in Section 3.3.1) for the different \noptimizations. As in the previous table, each optimization is applied on top of the previous one. We \ncan observe that the improvement in the p-slice overheads shown in Table 2 actually translate into speedup. \nWe can observe that, on average, the expected speedup grows from 1.15x without optimizations up to almost \n2x when all optimizations are set. Table 3 presents some statistics of the speculative threads generated \nby the Mitosis system with fully optimized p-slices. The last row shows the arithmetic mean for the evaluated \nbenchmarks. The second column shows the number of spawned threads by benchmark and the second column \nthe average number of speculative instructions executed by speculative threads. It can be observed that \nbh spawns the fewest threads but their average size is about 30 times larger than for the rest of benchmarks. \nOn the other hand, mst spawns the most but the average size of its speculative threads is the lowest. \nThe fourth column shows the average dynamic size of the p-slices and the fourth column the relationship \nbetween the sizes of the speculative threads and their corresponding p-slices. This percentage is consistently \nquite low for all the studied benchmarks and on average represents less than 3%. The sixth column shows \nthe average number of thread input values that are computed by the p-slice, which is only three values \non average. The seventh column represents the percentage of threads that are squashed. This percentage \nis rather low for all the benchmarks except for health, for which about one out of every four threads \nis squashed. We have observed that for this particular benchmark, memory dependences for the profiling \nand simulated inputs are significantly different, which result in many memory dependence misspeculations. \nFinally, the right-most column shows the degree of speculative thread-level parallelism that is exploited \nby Mitosis. This column represents the average number per cycle of active threads that are executing \ncorrect code. It can be observed that, even though parallelizing compilers cannot find parallelism in \nthese benchmarks, there is still a high degree of thread-level parallelism that is exploited by the Mitosis \ncompiler/architecture. On average, the number of active and correct threads per cycle is slightly higher \nthan 2.5. Table 3. Characterization of the Olden benchmarks  Squash Pctg Active Threads/cycle bh 422 \n15543 196 1.3% 4.4 0.7% 2.68 em3d 396638 422 9 2.1% 1.0 0.3% 2.87 health 198497 1112 41 3.7% 2.7 26.9% \n2.35 mst 1367114 271 5 2.1% 2.3 0.8% 2.62 perimeter 493725 576 24 4.2% 3.6 1.0% 2.08 6.0% 2.52 Figure \n8 shows the performance of the Mitosis processor compared to other single- and multi-threaded architectures. \nPerformance is reported as speedup over execution on a single Mitosis thread unit. The compared architectures \nare: a) an out-of\u00adorder superscalar processor, with twice the resources of an in\u00adorder Mitosis thread \nunit, and b) a single thread unit with perfect memory. This represents an upper bound on the performance \nthat can be achieved by helper threads that target memory latency [5]. The main conclusion of this study \nis that the Mitosis system is very effective at exploiting thread-level parallelism for irregular applications. \nAn average speedup of 2.2 is observed, and significant speedup is achieved for all benchmarks. It can \nbe observed that the Mitosis processor clearly outperforms the other architectures. Average speedups \nfor the big out-of-order core and perfect memory are 1.26 and 1.23 respectively. The last bar in each \ngroup of bars in Figure 8 shows the speedup estimated by our model with full optimizations for the selected \npairs. In three of the five benchmarks (em3d, health and perimeter) the speedup predicted by the model \nis relatively close to that of the simulation. In the case of mst we have observed that the difference \nis due to many high-latency instructions. Note that for mst the Perfect Memory scheme performs better \nthan Mitosis. We expect that including a more accurate latency for each instruction in the model (instead \nof the fixed 1-cycle currently assumed) will significantly improve performance in these cases. In the \ncase of bh, the main source of discrepancy between simulated and estimated speedups are due to the use \nof average lengths to estimate the timing of the p-slices and speculative threads. We have observed that \nfor this program, these lengths experience a significant variability, and thus, the selected threading \nscheme is not optimal for the cases that significantly depart from the average. Looking at particular \nbenchmarks, we can observe that the big out-of-order core is comparable to Mitosis only for em3d. This \nis due to the fact that this program has abundant ILP, which could also benefit more aggressive configurations \nof Mitosis, for instance based on out-of-order cores. Perfect memory is comparable to Mitosis only for \nmst. For this program, the performance of the memory system is rather poor; for a single\u00adthreaded execution, \nthe L0 and L1 miss ratios are around 50% and 70% respectively. This clearly points out that memory is \nthe main bottleneck for this program, and any technique that tries to accelerate it should focus on memory. \nObviously, perfect memory attacks this problem but the results show that Mitosis solves it effectively \ntoo. To summarize, we find the Mitosis architecture and compiler to represent a highly flexible parallel \narchitecture. Whether the code contains traditional thread-level parallelism (not shown in these benchmarks, \nbut easily handled by this system), instruction-level parallelism (em3d), or memory-level parallelism \n(mst), Mitosis exploits it effectively. Additionally, codes that exhibit none of the above also experience \nhigh speedups.  6. Related Work Several speculative multithreaded architectures have been proposed, \nalong with hardware and compiler techniques to extract speculative threads. In this section we review \nthe main works, with regard to the schemes used to identify speculative threads and to manage inter-thread \ndata dependences, which are the topic of this paper. The Expandable Split Window Paradigm [10] and the \nfollow-up work, the Multiscalar processor [19][26] were pioneering works in the area of SpMT. Speculative \nthreads (called tasks) are created by the compiler based on several heuristics that tried to minimize \nthe data dependences among threads as well as maximize the workload balance, among other compiler criteria. \nThe process consists of walking the control-flow graph and accumulating basic blocks into tasks. Inter-thread \ndata dependences are managed differently depending on whether they are through memory or registers. For \nregister dependences, the compiler is responsible for detecting the instruction that performs the last \nwrite on this register in order to bypass the value to the consumer thread. Memory dependences are handled \nthrough the ARB mechanism. Several studies propose architectures and schemes to create speculative threads \nbased on well-known program constructs such as loop iterations, loop continuations and subroutine continuations \n([23][9][1][4][13][2][20][17] among others) either through hardware or software mechanisms. The Superthreaded \n[23] and the SPSM [9] system are two examples where the loop parallelization task is performed by the \ncompiler. These schemes differ in the mechanism used to deal with inter\u00adthread data dependences. Most \ndetect memory dependence violations based on modifications of traditional snoop-based cache coherence \nprotocols. Memory data value prediction has also been proposed, but these values usually show lower predictability \n[3][21]. Register dependent values are either synchronized or hardware predicted. Some compiler-based \nschemes, such as the Superthreaded architecture [23], reorder the code in order to compute the dependent \nvalue earlier. Du et al. [8] have recently proposed a cost-driven compilation framework that statically \ndetermines which loops are good candidates to parallelize. They compute a cost graph from the CFG and \nDDG and estimate the probability of misspeculations. Inter-thread data dependences are handled by moving \nproducer instructions before the spawn of the next iteration. A more complex scheme to partition the \nprogram into speculative threads is presented in a recent work [15]. That scheme is based on profiling \ninformation. As in Mitosis, any combination of basic blocks is considered a candidate spawning pair. \nIn that work, inter-thread data dependences are handled by means of hardware value prediction. Our work \ndiffers from that in the fact that we use a software approach to predict thread inputs, which implies \na significantly different microarchitecture, and the compiler support presented in this paper. The use \nof Helper Threads, which speculatively execute a subset of the original code to reduce the latency of \nhigh-cost instructions, has been thoroughly studied [5][12][18][27]. This research borrows some concepts \nfrom that body of work to create the pre\u00adcomputation slices for thread live-ins. However, the need of \nMitosis to pre-compute a set of values accurately (as opposed to a single load address or branch result), \nand an increased cost of misspeculation, requires significantly more careful creation of slices, and \nthe inclusion of more accurate control flow in the slice previous work on helper threads typically followed \nonly a single control flow path in a slice. Finally, Zilles s et al. Master/Slave Speculative Parallelization \n(MSSP) [28] represents a different scheme to exploit speculative thread-level parallelism via distilled \nprograms. Distilled programs are a small subset of instructions of a given program that compute the input \nvalues of the speculative threads. In that execution model, the distilled program runs as a master thread \nand when all the input values for a speculative thread are computed, it is spawned on an idle context \nwhile the master starts computing new input values for the next thread. Our execution model differs from \nthat previous work in the fact that the computation of the thread live-in values are done by speculative \nthreads, which allows the processor to spawn threads out of the program order, and to often compute the \nlive-ins for speculative threads in parallel. 7. Conclusions In this work we have presented and evaluated \nthe Mitosis compiler for exploiting speculative thread-level parallelism. This compiler includes a mechanism \nto build a synthetic trace, a scheme to generate and speculatively optimize pre-computation slices, a \nmodel to estimate the benefits of any set of spawning pairs for a given SpMT configuration, and a greedy \nalgorithm to select the best set of pairs. The two major novelties of the proposal are: (1) the use of \npre-computation slices (i.e., software value prediction) to handle inter-thread data dependences, and \n(2) a model of the whole system that helps the compiler to identify which parts of the program will provide \nthe highest benefit when parallelized through speculative threads. This model takes into account possible \nmisspeculations, overheads, and load balancing. A key contribution of this work is a set of compiler \noptimizations that reduce the length (and thus the overhead) of pre-computation slices. Branch pruning, \nmemory and register dependence speculation, and early thread squashing are the main techniques proposed \nin this paper. The results obtained by the Mitosis compiler/architecture for a subset of the Olden benchmarks \nare impressive. It outperforms single-threaded execution by 2.2x. When compared with a big out-of-order \ncore, the speedup is close to 2x. We have also shown that the benefits of Mitosis do not come only from \nreducing memory latency since it outperforms an ideal system with perfect memory by about 60%. Overall, \nthis work shows that significant amounts of thread-level parallelism can be exploited in irregular codes, \nwith a rather low overhead in terms of extra/wasted activity.  Acknowledgments We would like to thank \nPeter Rundberg (currently at Gridcore, Sweden) for his valuable collaboration at the first stages of \nthis work. Also, we would like to thank John Shen and Hong Wang (from MRL, Santa Clara) for their collaboration \nin the definition of the Mitosis architecture and the ORC team for their support in the compiler implementation. \nThis work has been partially supported by the Spanish Ministry of Education and Science under contract \nTIN2004-03072 and Feder funds. Finally, we would like to thank the reviewers for their helpful and constructive \ncomments. References [1] H. Akkary and M.A. Driscoll, A Dynamic Multithreading Processor , in Proc. \nof the 31st Int. Symp. on Microarchitecture, 1998 [2] M. Cintra, J.F. Martinez and J. Torrellas, Architectural \nSupport for Scalable Speculative Parallelization in Shared-Memory Systems , in Proc. of the 27th Int. \nSymp. on Computer Architecture, 2000 [3] M. Cintra and J.Torrellas, Eliminating Squashes through Learning \nCross-thread Violations in Speculative Parallelization for Multiprocessors , in Proc. of the 8th Int. \nSymp. on High Performance Computer Architecture, 2002 [4] L. Codrescu and D. Wills, On Dynamic Speculative \nThread Partitioning and the MEM-Slicing Algorithm , in Proc. of the Int. Conf. on Parallel Architectures \nand Compilation Techniques, pp. 40-46, 1999 [5] J.D. Collins, H. Wang, D.M. Tullsen, C. Hughes, Y-F. \nLee, D. Lavery and J.P. Shen, Speculative Precomputation: Long Range Prefetching of Delinquent Loads \n, in Proc. of the 28th Int. Symp. on Computer Architecture, 2001 [6] R.S. Chapel, J. Stark, S.P. Kim, \nS.K. Reinhanrdt and Y.N. Patt, Simultaneous Subordinate Microthreading (SSMT) , in Procs. of the 26th \nInt. Symp. on Computer Architecture, pp. 186-195, 1999 [7] K. Diekendorff, \u00a8Compaq Chooses SMT for Alpha\u00a8, \nMicroprocessor Report, December, 1999 [8] Z.-H. Du, C-Ch. Lim, X.-F. Li, Q. Zhao and T.-F. Ngai, A Cost-Driven \nCompilation Framework for Speculative Parallelization of Sequential Programs , in Procs. of the Conf. \non Programming Language Design and Implementation, June 2004 [9] P.K. Dubey, K. O Brien, K.M. O Brien \nand C. Barton, Single-Program Speculative Multithreading (SPSM) Architecture: Compiler-Assisted Fine-Grained \nMultithreading , in Proc. of the Int. Conf. on Parallel Architectures and Compilation Techniques, 1995 \n[10] M. Franklin and G.S. Sohi, The Expandable Split Window Paradigm for Exploiting Fine Grain Parallelism \n, in Proc. of the 19th Int. Symp. on Computer Architecture, 1992 [11] R. Ju, S. Chan and C. Wu, Open \nResearch Compiler for the ItaniumTM Family , in Tutorial in the 34th Int. Symp. on Microarchitecture, \n2001 [12] C. Luk, Tolerating Memory Latency through Software-Controlled Pre-Execution in Simultaneous \nMultithreading Processors , in Proc. of the 28th Int. Symp. on Computer Architecture, pp. 40-51, 2001 \n[13] P, Marcuello and A. Gonz\u00e1lez, Clustered Speculative Multithreaded Processors , in Proc. of the 13th \nInt. Conf. on Supercomputing, pp. 365-372, 1999 [14] P. Marcuello, J. Tubella and A. Gonz\u00e1lez, Value \nPrediction for Speculative Multithreaded Architectures , in Proc. of the 32nd. Int. Conf,. on Microarchitecture, \npp. 203-236., 1999 [15] P. Marcuello and A. Gonz\u00e1lez, Thread-Spawning Schemes for Speculative Multithreaded \nArchitectures , in Proc. of the 8th Int. Symp, on High Performance Computer Architectures, 2002 [16] \nT. Marr et al., Hyper-threading Technology Architecture and Microarchtiecture , Intel technology Journal, \n6(1), 2002 [17] J. Oplinger et. al., Software and Hardware for Exploiting Speculative Parallelism in \nMultiprocessors , Technical Report CSL-TR-97-715, Stanford University, 1997 [18] Roth and G.S. Sohi, \nSpeculative Data-Driven Multithreading , in Proc. of the 7th. Int. Symp. On High Performance Computer \nArchitecture, pp. 37-48, 2001 [19] G.S. Sohi, S.E. Breach and T.N. Vijaykumar, Multiscalar Processors \n, in Proc. of the 22nd Int. Symp. on Computer Architecture, pp.414-425, 1995 [20] J. Steffan and T. Mowry, \nThe Potential of Using Thread\u00adlevel Data Speculation to Facilitate Automatic Parallelization , in Proc. \nof the 4th Int. Symp. on High Performance Computer Architecture, pp. 2-13, 1998 [21] J. Steffan, C. Colohan, \nA. Zhai and T. Mowry, Improving Value Communication for Thread-Level Speculation , in Proc. of the 8th \nInt. Symp. on High Performance Computer Architecture, pp. 58-62, 1998 [22] S. Storino an dJ. Borkenhagen, \nA Multithreaded 64-bit PowerPC Commercial RISC Processor Design , in Proc. Of the 11th Int. Conf. on \nHigh Performance Chips, 1999 [23] J.Y. Tsai and P-C. Yew, The Superthreaded Architecture: Thread Pipelining \nwith Run-Time Data Dependence Checking and Control Speculation , in Proc. of the Int. Conf. on Parallel \nArchitectures and Compilation Techniques, 1995 [24] M. Tremblay et al., The MAJC Architecture, a synthesis \nof of Parallelism and Scalability , IEEE Micro, 20(6), 2000 [25] D. M. Tullsen, S.J. Eggers and H.M. \nLevy, Simultaneous Multithreading: Maximizing On-Chip Parallelism , in Proc. of the 22nd Int. Symp. on \nComputer Architecture, pp. 392\u00ad403, 1995 [26] T.N. Vijaykumar, Compiling for the Multiscalar Architecture \n, Ph.D. Thesis, Univ. of Wisconsin-Madison, 1998 [27] C.B. Zilles and G.S. Sohi, Execution-Based Prediction \nUsing Speculative Slices , in Proc. of the 28th Int. Symp. on Computer Architecture, 2001 [28] C.B. Zilles \nand G.S. Sohi, Master/Slave Speculative Parallelization , in Proc. of the 35th Int. Symp. on Microarchitecture, \n2002  \n\t\t\t", "proc_id": "1065010", "abstract": "Speculative parallelization can provide significant sources of additional thread-level parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail.The management of inter-thread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. P-slices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing.Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2.", "authors": [{"name": "Carlos Garc&#237;a Qui&#241;ones", "author_profile_id": "81100210163", "affiliation": "Universitat Polit&#232;cnica de Catalunya, Barcelona", "person_id": "P728831", "email_address": "", "orcid_id": ""}, {"name": "Carlos Madriles", "author_profile_id": "81100338789", "affiliation": "Universitat Polit&#232;cnica de Catalunya, Barcelona", "person_id": "P728830", "email_address": "", "orcid_id": ""}, {"name": "Jes&#250;s S&#225;nchez", "author_profile_id": "81100532891", "affiliation": "Universitat Polit&#232;cnica de Catalunya, Barcelona", "person_id": "P728835", "email_address": "", "orcid_id": ""}, {"name": "Pedro Marcuello", "author_profile_id": "81100122060", "affiliation": "Universitat Polit&#232;cnica de Catalunya, Barcelona", "person_id": "P222915", "email_address": "", "orcid_id": ""}, {"name": "Antonio Gonz&#225;lez", "author_profile_id": "81100615797", "affiliation": "Universitat Polit&#232;cnica de Catalunya, Barcelona", "person_id": "P679162", "email_address": "", "orcid_id": ""}, {"name": "Dean M. Tullsen", "author_profile_id": "81100552297", "affiliation": "University of California, San Diego, CA", "person_id": "P64792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1065010.1065043", "year": "2005", "article_id": "1065043", "conference": "PLDI", "title": "Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices", "url": "http://dl.acm.org/citation.cfm?id=1065043"}